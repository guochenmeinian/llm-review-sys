# Complexity Matters: Rethinking the Latent Space

for Generative Modeling

 Tianyang Hu\({}^{1}\), Fei Chen\({}^{1}\), Haonan Wang\({}^{2}\), Jiawei Li\({}^{1}\),

Wenjia Wang\({}^{3}\), Jiacheng Sun\({}^{1}\), Zhenguo Li\({}^{1}\)

\({}^{1}\) Huawei Noah's Ark Lab, \({}^{2}\) National University of Singapore

\({}^{3}\) Hong Kong University of Science and Technology (Guangzhou)

Corresponding to: Jiacheng Sun (sunjiacheng1@huawei.com)

###### Abstract

In generative modeling, numerous successful approaches leverage a low-dimensional latent space, e.g., Stable Diffusion  models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel "distance" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN  and Diffusion Transformer , where our modifications yield significant improvements in sample quality with decreased model complexity.

## 1 Introduction

In the past decade, deep generative models have achieved great success across various domains such as images, audio, videos, and graphs . The advances are epitomized by recent breakthroughs in text-driven image generation . Behind the empirical success are fruitful developments of a wide variety of deep generative models, among which, many can be associated with a latent space, e.g., generative adversarial networks (GANs) , Variational Autoencoders (VAEs) , and latent diffusion models . It is commonly believed that for structured data such as images, the intrinsic dimension is much lower than the actual dimension . By utilizing a proper low-dimensional latent space, generative modeling can be more efficient . Take text-to-image generation as an example, state-of-the-art models such as DALL-E , Parti , Stable Diffusion  all utilized discrete Autoencoders to alleviate the computation cost by downsampling images, e.g., from 256\(\)256 to 32\(\)32.

In practice, the choice of the latent plays a critical role. Stable Diffusion  employs the Autoencoder from VQGAN , which is an improvement over VQVAE  by incorporating adversarial training.

The discrete Autoencoder was later substituted to continuous ones regularized with Kullback-Leibler (KL) divergence. ViT-VQGAN  further boosted VQGAN by using vision transformers  as the backbone and it was adopted by Parti . Despite the methodological advances, our theoretical understanding of the latent space is still limited. Important questions such as what constitutes a good latent space, what is the optimal choice, and how it depends on data remain elusive.

Ideally, the latent distribution \(P_{z}\) should preserve the important information about the data distribution \(P_{x}\) as much as possible so that the required effort from the model learning is minimal. Finding such a data-dependent latent can be viewed as a self-supervised learning (SSL) task. Can we borrow insights from the vast literature on SSL to understand and improve the latent? Unfortunately, existing methods [57; 14; 25; 28; 29] are designed for discriminative tasks and not suited for generative modeling. To illustrate, consider the classic GANs [24; 76] where the latent distribution is usually pre-defined. When modeling the CIFAR-10 dataset , if we substitute the DCGAN's latent  from standard Gaussian to the features learned from SimCLR  with the same dimension, the Inception Score (IS) drops from 5.68 to 3.932. Therefore, new theoretical insights are in dire need to elucidate the ideal \(P_{z}\) and uncover its connection to \(P_{x}\).

In this work, we aim to provide a new understanding of the latent space in generative modeling from the angles of SSL and minimizing model complexity, where we first formalize the problem for GAN models and then generalize it to Autoencoders.

Drawing inspiration from the training objective of GANs, a novel distance between distributions in different dimensions can be defined to measure the closeness between the latent and the data (Section 3.1). As is typically emphasized in learning theory [82; 83; 35; 61], the complexity of the generator is an important factor in this formulation, where a latent is deemed closer to the data if a generator can achieve the same (better) performance with lower (the same) complexity. The latent closest to the data in a GAN-induced distance is characterized as the optimal choice for that GAN, which leads to the simplest map between the latent and the data.

With our formulation of the optimal latent distribution, the immediate question is how to estimate it, which gives rise to a new SSL task for generative modeling. Naturally, we consider utilizing an encoder network to parameterize the latent, which uncovers the popular Autoencoder architecture with the paired generator as a decoder (Section 3.2). In the learning process, we further emphasize the importance of a relatively weak decoder and the trade-off between the informativeness of the latent and the capability of the decoder (Section 4.1). To this end, we propose a 2-stage training scheme called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained (Section 4.2). Our theoretical analyses are corroborated by comprehensive experiments on both synthetic data and real image data with various models such as DCGAN , VAEGAN [47; 90], VQGAN , and Diffusion Transformer (DiT) , where our modifications yield significant improvements in image quality with decreased model complexity (Section 6).

## 2 Preliminary

Notations.For a function \(f:\), let \(\|f\|_{p}=(_{}|f()|^{p}d)^{1/p}\). For a vector \(\), \(\|\|_{p}\) denotes its \(l_{p}\)-norm. \(L_{p}\) and \(_{p}\) are used to distinguish function norms and vector norms. For two positive sequences \(\{a_{n}\}_{n}\) and \(\{b_{n}\}_{n}\), we write \(a_{n} b_{n}\) if there exists a constant \(C>0\) such that \(a_{n} Cb_{n}\) for all sufficiently large \(n\). We use \(P_{x}\) and \(p_{x}\) to denote the probability distribution and density of the random variable \(x\). \(g f(x):=g(f(x))\) denotes composition of functions.

Generative adversarial networksare a type of implicit generative models that aim to learn transformations \(g\) from random noises \(\) to the data \(\). At the population level, its objective function can be expressed in general as

\[_{g}D_{h}(P_{x},P_{g(z)}),\  P_{z},\] (2.1)

where \(P_{z}\) is usually pre-determined to be some simple distribution such as (mixture) Gaussian or uniform, and \(D_{h}\) is realized by the adversarially-trained discriminator \(h\) and when optimized properly, can give rise to various well-established distances, e.g., Jensen-Shannon divergence ,maximum mean discrepancy , \(f\)-divergence , Wasserstein distance , etc. Exploiting the flexible architectures of various neural networks as generators [41; 42], GAN enjoys state-of-the-art performance in generating various data types [41; 85; 74; 89] with notable advantage in the sampling speed, especially compared with diffusion probabilistic models (DPMs) [72; 32]. However, GAN models often suffer from mode collapse, failing to keep the data diversity intact. Besides the usual suspect of the min-max adversarial training, the poor choice of latent noise distribution is also to blame .

Self-supervised learning.The goal of SSL is to learn a (low-dimensional) feature representation \(f()\) from unlabeled data that is suited for various discriminative downstream tasks, e.g., classification, detection, etc. There are mainly three categories, i.e., pretext task-based , contrastive methods such as SimCLR , MoCo , BYOL , SimSiam , and generative-based such as Masked Autoencoder (MAE) . The learned feature map \(f()\) defines a distribution over the feature space \(^{d_{z}}\), denoted as \(P_{f}\). Ideally, \(P_{f}\) should preserve the important information about \(P_{x}\) as much as possible. In this sense, representation learning in general can be seen as preserving certain "distances3" between distributions in different dimensions [36; 8].

Distance between distributions in different dimensions.To quantitatively measure the closeness, various similarity measures between \(P_{f}\) and \(P_{x}\) are defined. Since \(d_{z}<d\), typical distances between distributions will not work due to the different metric spaces \(\) and \(\) reside in. Fortunately, there are two existing tools we can turn to. The first is Gromov-Wasserstein distance [56; 71], which circumvents the metric spaces mismatch by comparing pairwise joint distributions. Typical manifold learning methods such as stochastic neighbor embedding can be thought of as a special case [31; 81; 36]. Second, as in , popular distances such as Wasserstein-\(p\) distance and \(f\)-divergence can all be naturally extended to such settings, either by embedding \(P_{z}\) to \(^{d}\) or projecting \(P_{x}\) to \(^{d_{z}}\) and taking the infimum over all orthogonal linear projections. Specifically, for any distance between distributions \(D(,)\) in the Wasserstein-\(p\) or \(f\)-divergence family, define its generalized version to be

\[D^{+}(P_{x},P_{z}):=_{P_{}^{+}(P_{z},d)}D(P_{x},P_{}),\]

where \(^{+}(P_{z},d):=\{P_{}:+ P_{z}, ^{d_{z} d},^{d_{z}},^{}= _{d_{z}}\}\) is the family of distributions in \(^{d}\) embedded from \(P_{z}\) by a linear orthogonal transformations from \(^{d_{z}}^{d}\). Similarly, we can define \(D^{-}(P_{x},P_{z})\) and  showed that \(D^{+}(P_{x},P_{z})=D^{-}(P_{x},P_{z})\).

Contrastive learning has deep connections with manifold learning  in terms of preserving pairwise similarity, which can be viewed as a special case of preserving the Gromov-Wasserstein distance . In contrast, as we will demonstrate in this work, the optimal latent for generative modeling such as GANs is more closely related to \(D^{+}(P_{x},P_{z})\).

## 3 Optimal latent distribution for generative models

Recall the typical training objective of GANs (2.1) and denote its empirical version as \(L_{n}(g|P_{z})\), where \(n\) is the sample size. For any given \(P_{z}\), with limited capacity on the generator, \(_{g}L_{n}(g|P_{z})\) reflects the compatibility or closeness between \(P_{z}\) and \(P_{x}\). If \(P_{z_{1}}\) is a better latent than \(P_{z_{2}}\), intuitively \(_{g}L_{n}(g|P_{z_{1}})_{g}L_{n}(g|P_{z_{2}})\). This can serve as an empirical and relative evaluation of the quality of GAN latents. Inspired by this, we can generalize the distance proposed in  to construct a more meaningful version specifically for implicit generative models such as GANs.

### GAN-induced distance

Let \(()\) denote the set of all Borel probability measures on \(\). For any distance between distributions \(D(,)\) in the data space \((^{d})\), define the generalized version associated with a generator \(g\) mapping from \(^{d_{z}}\) to \(^{d}\) as:

\[D^{}(P_{z},P_{x}):=_{g}D(P_{g(z)},P_{x}).\] (3.1)Compared with \(D^{+}\) in , \(D^{}\) substitutes the linear orthogonal mappings to a general function space. Although \(D^{}\) is not a valid metric as it is defined between different probability measure spaces, it can serve as a viable measurement of the closeness between \(P_{z}\) and \(P_{x}\).

In order for \(D^{}\) to be nontrivial, the function space \(\) cannot be too large, as it is known that one-dimensional distributions can approximate higher dimensional distributions arbitrarily well by neural network transformations in Wasserstein-\(p\) distance . On the other hand, \(\) cannot be too small, or it may not be able to adequately extract the information from the latent distribution. In this work, we consider \(\) to be a family of neural networks with bounded complexity, i.e., \(_{c}=\{g:C(g) c\}\), where \(c>0\) and \(C()\) is some complexity measure to be specified. \(C()\) can be as specific as the Lipschitz constant, or as general as the size (width, depth, etc.) of the network.

_Remark 3.1_.: It is worth emphasizing that in (3.1), we do not consider the optimization problem but instead, focus on the existence or the global minimum of \(_{g}\). A toy example is given in Appendix A.1 to illustrate the global minimizers of \(g\).

_Remark 3.2_ (Complexity scaling law).: In the toy case of Appendix A.1, the relationship between \(D^{_{c}}\) and \(c\) is an embodiment of the popular _scaling law_ phenomenon  with respect to the model complexity. It's worth noting that the requirement on the complexity can be drastically decreased if we consider multiple steps of generation, rather than single-step push-forward transformations. From this perspective, one of DPM's main advantages may just be the increased complexity due to the recursive sampling process.

Directly following the definition in (3.1), we have the following propositions.

**Proposition 3.3**.: _Let \(D\) be a \(p\)-Wasserstein metric, a Jensen-Shannon divergence, a total variation, or an \(f\)-divergence. Then, \(D^{}(P_{z},P_{x})\) in (3.1) is zero if and only if there exists \(g^{*}\) such that \(P_{g^{*}(z)}=P_{x}\)._

**Proposition 3.4**.: _For two distributional distances, generalizing them across different dimensions via (3.1) does not change their relative relationships, e.g., if \(D\) is Wasserstein distance, we still have \(W^{}_{p}(p_{x},p_{z}) W^{}_{q}(p_{x},p_{z})\), for \(p q\)._

Equipped with \(D^{}\) to compare different latent distributions, we can characterize the ideal \(P_{z}\) as the one that minimizes \(D^{}(P_{z},P_{x})\), i.e., the latent that requires the lowest generator capacity for \(L(g|P_{z})\) to be below a certain threshold4. Such a latent depends on both the data and the GAN training algorithm and can be seen as an SSL task, targeting specifically minimizing the required capacity of the corresponding generator, thus easing the training of the generator.

_Remark 3.5_ (Dimensional collapse).: In contrast to the common _dimensional collapse_ phenomenon  found in contrastive learning where the learned feature distribution is only supported on a low-dimensional subspace, minimizing \(D^{}(P_{z},P_{x})\) with respect to \(P_{z}\) encourages it to maintain the intrinsic dimension of the data so that \(D(P_{g}(z),P_{x})\) is not ill-posed. The dimensional collapsed latent contributes to the poor performance of SimCLR+DCGAN mentioned in Section 1.

_Remark 3.6_ (Change-of-scale problem).: Although \(D^{}\) serves as a well-defined measurement of closeness between \(p_{z}\) and \(p_{x}\), the latent \(p_{z}\) that minimizes \(D^{}(p_{z},p_{x})\) might be ill-posed. If the complexity is not invariant to scaling, e.g., \(L_{p}\) norm, Lipschitz constant, etc., \(D^{}(P_{z}( z),P_{x})\) is non-increasing with \(\). In the next section, we impose extra constraints to circumvent this problem, and the optimal \(P_{z}^{*}\) is characterized therein.

### Learning the latent by an encoder

Now that we have characterized the ideal latent distribution given the generator family and the data, the next question is how to find such a good latent. Casting it as an estimation problem, the first decision to make is how to parameterize \(P_{z}\). We could adopt Gaussian mixtures as in . Nonetheless, a more universal and powerful way is to employ an encoder network \(f:^{d}^{d_{z}}\), similar to SSL methods. With \(P_{z}\) parameterized by an encoder, the overall structure is an Autoencoder. In the remaining part, we also refer to the generator as the decoder and use the two words interchangeably.

At the population level, we want to match the distributions of \(\) and the reconstructed \(g f()\), i.e.,

\[_{f,g}D(P_{x},P_{g f()})=_{f }(_{g}D(P_{x},P_{g f()}))= _{f}D^{}(P_{x},P_{f()}).\]Looking at the encoder and the decoder together, we can see that _optimizing \(D(P_{g f},P_{x})\) amounts to learning an encoder \(f\) that minimizes \(D^{}\)_. This strategy is employed by VQGAN , where the encoder and decoder are optimized together with the discretized latent code. Inspired by this observation, we can characterize the optimal latent distribution \(P_{z}^{*}\) for a given \(P_{x}\) from the perspective of minimizing the distance between distributions in different dimensions by defining \(P_{f^{*}}\) as

\[f^{*}=*{argmin}_{}D^{}(P_{x},P_{f (x)}).\] (3.2)

Notice that \(P_{f}^{*}\) not only depends on \(P_{x}\) and \(D\), but also \(\) and \(\). The change-of-scale problem in Remark 3.6 is mostly taken care of by the inherent boundedness of \(\). More discussion on the choice of \(\) can be found in Section 4.1.

_Remark 3.7_ (Classification).: As the solution of a new SSL task, \(f^{*}\) in (3.2) will preserve well-separated clusters of \(P_{x}\), though not as discriminative as the features learned from existing SSL methods. See Proposition A.3 in the appendix for more details. We also conducted experiments on simulated data evaluating the classification accuracy in Section 6.1.

With the encoder, (3.2) is not the only option to characterize the optimal latent distribution. Besides directly parameterizing \(P_{z}\), \(\) can also serve an auxiliary role in defining a more general \(P_{z}^{*}\). To this end, in parallel to \(D^{}\), we can also define \(D^{}(P_{z},P_{x}):=_{f}D(P_{z},P_{f(x)})\). As \(D^{}\) is to \(D^{+}\), \(D^{}\) generalizes the \(D^{-}\) in . Combining \(D^{}\) and \(D^{}\), we get a generalized measure of distance between the latent and data for Autoencoders as \(D^{}(P_{z},P_{x})=D^{}(P_{x},P_{z})+D^{}(P_{ x},P_{z}),\) which resembles the _condition number_ of matrices, but for latent distributions. The optimal latent distribution can also be characterized by

\[P_{z}^{*}=*{argmin}_{P_{z}}D^{}(P_{z},P_{x}).\] (3.3)

If \(d_{z}=d\), then identity mapping, i.e., \(P_{z}^{*}=P_{x}\), obviously solves (3.2) and (3.3). If like DPMs or VAEs where the latent distributions are chosen to be data-agnostic, e.g., standard Gaussian, \(D^{}(P_{z},P_{x})\) can be significantly larger for complicated data.

_Remark 3.8_.: \(D^{}\) and \(D^{}\) can be zero, especially when \(P_{x}\) is supported on a low-dimensional space . As a result, the defined \(f^{*}\) or \(P_{z}^{*}\) might not be unique. We provide a toy example in Appendix A.2 to illustrate how \(P_{z}^{*}\) minimizes the complexity.

## 4 Improving the latent with Decoupled AutoEncoder

In practice, the encoder and decoder family should be as powerful as possible. However, as stated before, \(\) and \(\) should be chosen carefully in order for the induced \(D^{}\) and \(D^{}\) to be meaningful.

### Necessity of relatively weak decoders

Given a total budget5 of \(\) and \(\), intuitively, \(C()=C()\) tend to result in the best approximation of \(\) to the identity mapping. This is reflected in practice where the encoder-decoder pair is often designed with symmetric architecture, with the same number of blocks and parameters, e.g., up-sampling vs. down-sampling, convolution vs. deconvolution, etc. However, when it comes to the quality of the encoded latent, only targeting the reconstruction error may not be the ideal strategy. As we will demonstrate in Section 4.2, a better way is to first target a good latent distribution, and then do reconstruction using the learned latent. Consider two symmetric cases: (larger encoder, smaller decoder) vs (smaller encoder, larger decoder). The reconstruction ability may not be significantly different, although both are sub-optimal, the former tends to produce better latent distribution. See Section 6.1 for empirical evidence in the toy case comparing the two cases.

For a concrete demonstration, consider the simple linear case where the encoder and the decoder are two matrices, denoted as \(W_{c}^{d d_{z}}\) and \(W_{d}^{d_{z} d}\) respectively. Take the matrix \(2\)-norm as a measurement of complexity, which is equal to the largest singular value. A similar setting is also considered in . In the linear case, the quality of the latent is equivalent to the reconstruction error and the optimal encoder should recover the principal components. The next theorem states that the Autoencoder could fail if the encoder is not large enough.

**Theorem 4.1**.: _Let the singular values of \(W_{d}\) be \(_{1}_{d_{z}}\). If \(\|W_{e}\|<1/_{d_{z}}\), the linear Autoencoder cannot recover the principal components, and the reconstruction error is sub-optimal._

The linear case provides insights into neural networks as Autoencoders, indicating that the encoder should be _relatively strong_ to better learn the latent distribution. In the more practical nonlinear cases, things become more complicated. The intuition that (larger encoder, smaller decoder) is preferred to (smaller encoder, larger decoder) for learning a more informative latent is supported by the well-known _posterior collapse_ phenomenon in the VAE literature , where the encoded distribution becomes completely uninformative if the decoder is too powerful. However, for implicit generative models such as GANs, the generator is obviously the key component and it should be as powerful as possible. There exists a _trade-off_ between generation quality and the quality of the latent. Using a decoder too powerful can hinder the learning process of a good latent while a decoder too small will hurt the generation performance. In practice, whether a decoder family is weak or not can be summarized in an oversimplified way by its size, i.e., a smaller decoder is weaker.

_Remark 4.2_.: We could consider pairing a sufficiently large \(\) with an even larger \(\). Unfortunately, such a combination may not be a good solution. Besides the added computational cost, an encoder-decoder pair too powerful can drive \(D^{}+D^{}\) too close to zero, so that it may get overwhelmed by random noise. The signal-to-noise ratio may be too low for effective optimization.

### Decoupled AutoEncoder

To address the trade-off between generation quality and the quality of the latent, we can introduce an auxiliary decoder \(^{A}\), potentially much smaller/weaker, to substitute \(\) for training the encoder. Once the encoder is trained, we can then freeze it and pair it with \(\) for the decoder training. The key idea is to decouple the learning process of the encoder and the decoder in a 2-stage training scheme we call **Decoupled AutoEncoder (DAE)**. Figure 1 illustrates the method overview where the two stages of DAE can be summarized as:

* DAE Stage 1: Train \(\) with a small decoder \(^{A}\) to learn a good latent.
* DAE Stage 2: Freeze the trained encoder \(\). Then train the regular decoder \(g\) to ensure good generation performance.

By the 2-stage training, one with \(^{A}\) and one with \(\), without much hyperparameter tuning, DAE can learn a better latent distribution and as a result, achieve better reconstruction and generative modeling.

_Remark 4.3_ (Realizing \(^{A}\)).: The essence of the first stage is to employ an auxiliary decoder that is relatively weak compared with the encoder. Straightforwardly, we can introduce an extra decoder family. For an easier way of implementation, we can also use various forms of regularization to realize \(^{A}\). For instance, with a balanced encoder-decoder pair, we can apply strong Dropout  to the decoder in the first stage to make it weaker. Both methods are experimented in Section 6.

DAE can be applied in a **plug-and-play** style to any existing Autoencoder training pipelines with the decoupled 2-stage modification. We demonstrate in Section 6 that compared to various baselines,

Figure 1: Overview of the DAE 2-stage training. DAE addresses the trade-off between generation quality and the latent quality where the encoder is only updated in the first stage with an auxiliary weaker decoder and then frozen in the second stage while the actual decoder is being trained.

e.g., VAE, VQGAN, etc., our DAE-modified versions are better in terms of both reconstruction error and quality of the latent. For instance, we refer to Figure 3 in the appendix where we experimented with VQGAN . In terms of the reconstruction error, Stage 1 is worse than the baseline, which is to be expected since a weaker decoder is used. However, paired with the Stage 1 encoder, the decoder (the same size as the baseline) can achieve better reconstruction performance.

### Sampling from the latent distribution

In most Autoencoder-based generative models, e.g., VAE, adversarial Autoencoder , Wasserstein Autoencoder , etc., the latent distributions are all modeled by Gaussian distributions. DPMs [72; 32] can also be seen as a type of Autoencoder-based method with the forward noising process as the encoder and the denoising process as the decoder, with the latent distribution being Gaussian. As discussed in Remark 3.2, the decoding process of DPM enjoys drastically increased capacity due to the multi-step generative process.

In our work, with \(P_{z}\) parameterized by the encoder, how to generate new samples from it requires extra modeling of the latent. Otherwise, we can only do reconstruction. There are two mainstream methods to sample from the latent distribution. The first is inspired by natural language processing. In VQGAN  and Parti , the discretized latent codes are autoregressively modeled by transformers as a sequence, similar to GPT  while masked image modeling methods [11; 12] resembles BERT . The second is by diffusion process. Latent space diffusion models [68; 60] can be seen as using DPMs to model the latent space of a powerful Autoencoder. We mainly adopt the VQGAN formulation in our numerical experiments for training the Autoencoders. To validate the effectiveness of our DAE, we consider modeling the latent by both transformers and diffusion models.

## 5 Related works

BourGAN  proposed to model the latent distribution as Gaussian mixtures as a remedy for the mode collapse problem of GANs. Instance-conditioned GAN  proposed to partition the data manifold into a mixture of overlapping neighborhoods described by a data point and its nearest neighbors to improve the quality of unconditional generation. There is also a line of work that considers post-sampling latent space mining of GANs by exploiting the trained discriminator [75; 13].  further proposed adding an implicit latent transform before the mapping function to improve the latent from its initial distribution in a bi-level optimization fashion. Though these works targeted improving the latent distribution of GANs, they did not characterize the optimal choice nor employ an encoder to model the latent. In comparison, we approach the problem from the perspective of preserving the distance between distributions in different dimensions and propose a 2-stage training scheme involving an encoder.

There are GANs that incorporate an Autoencoder structure. VAEGAN [47; 90] combines a VAE by collapsing the decoder and the generator into one.  utilized a standard autoencoder to embed data into the latent space to address the mismatch of the continuous neural network and the discontinuous optimal transport (OT) map.  later proposed to utilize semi-discrete OT maps to sample from the latent and train GAN models. VQGAN  uses vector-quantized latent codes and transformers to model the tokenized latent distribution. ViT-VQGAN  later made improvements over VQGAN. Besides using vision transformers  as the backbone, it also introduced an \(l_{2}\) regularization term to the latent code. However, the aforementioned works did not characterize the optimal latent either and did not point out the necessity of the decoupled 2-stage training. Our DAE approach can also be applied to them for further improvement.

In the literature of VAE,  introduced new priors consisting of mixture distribution with components given by variational posteriors conditioned on learnable pseudo-inputs.  proposed a two-stage learning scheme to amend the problems associated with the Gaussian encoder/decoder assumptions.  stated that obtaining a good ELBO is not enough for good representation learning and proposed to incorporate mutual information and rate-distortion curve to achieve a better balance between the informativeness of the latent and the reconstruction quality. In comparison, our characterization of the optimal latent is from the perspective of matching distributions and minimizing complexities and is not restricted to Gaussian assumptions. It is also worth emphasizing that while the mutual information, closely related to VAEs, can also serve as a "distance" between distributions in different dimensions, it will not work for our benefit since we are considering _deterministic_ encoders and decoders. In the deterministic case, the conditional entropy would be zero and the mutual information would always be the entropy of the latent. Hence uniformly distributed latent is always ideal, which does not offer new insights.

Masked Autoencoder (MAE)  is a generative-based SSL method with the training objective of reconstructing masked images. In MAE, a high mask ratio and a relatively weak decoder have been verified as important factors for good discriminative performance, which is also corroborated by our perspective of preserving distributions in different dimensions. MAEs are designed for discriminative tasks and their generation capability is yet to be explored.

## 6 Experiments

In this section, we conduct empirical evaluations of our proposed DAE training scheme on a variety of datasets and generative models, from toy Gaussian mixture data to DCGAN on CIFAR-10, to VQGAN and DiT on larger datasets. The detailed experiment settings can be found in Appendix B.

### Toy examples

To showcase the effectiveness of the proposed 2-stage training, we consider a toy Gaussian mixture case with 8 clusters, whose centers are evenly placed on the unit circle, and the variances are 0.25. All samples in \(^{2}\) are then projected to \(^{10}\) via a linear orthogonal mapping. We employ the vanilla VAE  for demonstration with \(d_{z}=2\). In our implementation, both the encoder and the decoder are 3-layer fully connected networks with different hidden dimensions to control the size. Giving each cluster a label, we evaluate (1) the nearest neighbor classification accuracy from the 2D latent; (2) the nearest neighbor classification accuracy from the 10D reconstruction; and compare different configurations of the encoder and decoder pair. The average classification accuracy is reported in Table 1, which corroborates our analysis in Section 4, i.e., a relatively weak decoder can learn a better latent and the following 2-stage training can further improve reconstruction.

### GAN on CIFAR-10

As a more comprehensive proof-of-concept, we experiment with DCGAN  on the CIFAR-10  dataset. Specifically, we consider a vanilla baseline where the generator has 5 convolutional layers and the discriminator has 4 convolutional layers, followed by a linear classification layer. The latent for DCGAN is standard Gaussian. To corroborate our analysis in Section 4, we adapted the DCGAN with varying latent space configurations: (1) We utilize a latent space defined by a CIFAR-10 pre-trained SimCLR model , denoting this adaptation as DCGAN-SimCLR. (2) We consider VAEGAN with a learnable latent space. The structures of the decoder and discriminator mirrored those of our baseline DCGAN while the encoder is implemented with a five-layer CNN. (3) We modified VAEGAN to DAE-VAEGAN, where the width of convolutional layers in the decoder is halved in the first stage and then set back to its original structure in the second stage.

We evaluated the aforementioned four methods using the metrics of Inception Score (IS)  and Frechelt Inception Distance . With the exception of DCGAN, which generates images from random noises, the other methods first encode an image into latent space, and then generate an image based on that encoding. To this end, we omit the latent space modeling and calculate their metrics from reconstructed images. The results are listed in Table 2, with two take-home messages: (1) Traditional SSL methods may not help with the latent for generative modeling; (2) Decoupling the encoder and decoder training can improve the overall performance of generative modeling.

  Hidden Dim & (64, 128) & (128, 64) \\ Latent Acc (\%) & 80.6 (7.0) & **87.8** (5.8) \\  Hidden Dim & (128, 128) & DAE (128, 128) \\ Rec Acc (\%) & 92.2 (6.1) & **98.0** (1.3) \\  

Table 1: The mean (variance) of the classification accuracy w.r.t. difference hidden dimensions of the (encoder, decoder) over 10 replications.

  Method & IS (\(\)) & FID (\(\)) \\  DCGAN (reproduced) & 5.68 & 51.76 \\ DCGAN-SimCLR & 3.93 & 168.23 \\ VAEGAN & 5.82 & 48.11 \\
**DAE-VAEGAN** & **6.16** & **46.12** \\  

Table 2: The performance of DCGAN with different latent distributions.

### Vogan

VQGAN  consists of two major components. The first is a discrete Autoencoder, where the latent is quantized by looking up nearest neighbors from a learnable codebook. After the encoder and decoder are trained (objective contains adversarial loss), the second component, a transformer model, learns to generate discrete codes autoregressively in the latent space, which are then mapped to images by the learned decoder.

To demonstrate the effectiveness of our DAE, we introduce the decoupled training strategy to the first component of the standard VQGAN. In the first stage, we implement the relatively weak auxiliary decoder by applying Dropout  to the decoder with ratio \(p\), and train encoder and the dropped decoder jointly6. Then in the second stage, we freeze the encoder and train the full decoder without Dropout. Training of the transformer component is the same as VQGAN. The modified model is denoted as DAE-VQGAN.

Setup.We evaluate DAE-VQGAN by comparing with the baseline VQGAN on the FacesHQ dataset, which is a combination of two face datasets CelebAHQ  and FFHQ , with 85k training images and 15k validation images in total. In all experiments, the input image size is 256x256. We use the official VQGAN implementation7 and model architectures for FacesHQ. The Autoencoder is dubbed VQ-f16. The training process of DAE-VQGAN is divided equally into two stages, where in the first stage we apply 2D Dropout (channel-wise) to the decoder with ratio \(p=0.5\).

Improved image reconstruction and generation.To quantitatively evaluate the encoder-decoder component, we train the model on FacesHQ and compute the reconstruction FID over the full training and validation splits. We further train a transformer separately for CelebAHQ and FFHQ with the same architecture as in , and compute the generation FID with 50k generated samples against the training split of the respective dataset. As shown in Table 3, DAE-VQGAN achieves significantly improved FID compared to VQGAN, indicating that a stronger encoder-decoder component can be unlocked for VQGAN by our DAE approach. We show qualitative generation results in Figure 2. We see that models trained by DAE-VQGAN can reconstruct and generate images with high fidelity. Figure 3 in the Appendix shows the reconstruction losses during training. It is worth noting that while our DAE-modified VQGAN has higher reconstruction losses in the first stage, it catches up fast in the second stage and converges to a lower level than the baseline. This is to be expected as the first stage of DAE focuses on learning a better latent, which can ease the training of the decoder in the second stage and result in better performance overall.

Decreased Complexity.Minimizing the model complexity is a key motivation for our DAE. To verify our claims, we evaluate the Lipschitz complexity (\(C_{Lip}\) defined in (B.1)) of the encoder \(f\) and decoder \(g\) in VQGAN. Since \(g f\) is trained to be approximately the identity map, the trends of \(C_{Lip}(f)\) and \(C_{Lip}(g)\) are expected to be the opposite, with the ideal number being close to 1 for both if we want the overall complexity \(C_{Lip}(f)+C_{Lip}(g)\) to be minimized. We observed in Figure 4 that the encoder complexity of VQGAN and DAE-VQGAN are 1.40 and 0.88, respectively, with an increasing trend in terms of training steps; the decoder complexity of VQGAN and DAE-VQGAN

  Method & FID & sFID & IS \\  DiT* & 35.16 & 7.33 & 39.25 \\
**DAE-DiT** & **32.29** & **6.90** & **41.71** \\  

Table 4: Class-conditional image generation on ImageNet \(256 256\) using the DiT-L/4 model with different Autoencoders. *: Our reproduction is based on the official DiT implementation, which is consistent with the results reported in Fig.5 of .

  Method &  &  \\  & Train & Val & CelebAHQ & FFHQ \\  VQGAN & 4.81\(\) & 6.27\(\) & 10.2 & 9.6 \\ VQGAN* & 4.23 & 5.83 & 9.97 & 10.44 \\ DAE-VQGAN & **2.01** & **3.82** & **8.58** & **8.36** \\  

Table 3: Reconstruction FID over FacesHQ training and validation sets, and transformer generation FID on CelebAHQ and FFHQ training sets. \(\): Evaluated on the publicly available pre-trained model on FacesHQ. *: Our reproduction is based on the official VQGAN implementation.

are 0.73 and 1.16, respectively, with a decreasing trend in terms of training steps. Compared with the baseline, our DAE-modified version produces better latent distribution with a significantly lower complexity while achieving a lower reconstruction error. See Appendix B.2 for more details.

### Diffusion Transformers (DiT)

The encoder in VQGAN is discrete with a codebook. In Appendix B.3, we look into the codebook and investigate how different training strategies affect the behavior of the learned latent. Our DAE can also apply to continuous Autoencoders that are usually regularized by KL divergence. Such KL-regularized AEs are adopted by latent diffusion models such as DiT . Following similar settings as VQGAN, we also modified DiT to DAE-DiT by employing the decoupled training scheme and experimented with the larger ImageNet dataset . The details can be found in Appendix B.4.

For evaluation, we train DiT on the ImageNet dataset with 256\(\)256 resolution following the official implementation8. Instead of the largest DiT-XL/2 model (675M Params), we select the DiT-L/4 model (458M Params) for higher training efficiency. The corresponding Autoencoder is dubbed KL-f8. AdamW  optimizer is employed with a constant learning rate of \(10^{-4}\) and a weight decay of \(3 10^{-2}\). The batch size is 1024, and the number of epochs is 120. The trained model with Exponentially Moving Average (EMA) is then used to generate 50k images of the 1000 categories equally via a 250-step DDPM sampling  and a classifier-free guidance scale of 4. Table 4 compares the image generation quality where we can see that our DAE-DiT achieves significant improvement, similar to what we observed for DAE-VQGAN.

## 7 Discussion

This work investigates the ideal latent distribution for generative models. We introduce a novel distance between distributions to characterize the ideal \(P_{z}\) that minimizes the required model complexity. Practically, we propose a two-stage training scheme called DAE that achieves practical improvements on various models such as GAN, VQGAN, and DiT. Since many of the most powerful generative models are associated with a latent space, the impact of such investigations is potentially very high.

Nevertheless, there are many limitations of our work that call for further research along this line. First, our formulation of the optimal latent distribution is based on \(D^{}\),\(D^{}\) and \(D^{}\), which serve mainly illustrative purposes. The proposed distances cannot be effectively calculated. Second, the effectiveness of DAE is not proven mathematically and the resulting latent is not guaranteed to be closer to \(P_{z}^{*}\). Our work could be further strengthened if the aforementioned limitations can be addressed. It would also be an interesting direction to explore how our method affects latent space disentanglement [52; 26].

Figure 2: (a) Top: original image; Middle: reconstruction of VQGAN; Bottom: reconstruction of DAE-VQGAN. (b) Generated samples of DAE-VQGAN.