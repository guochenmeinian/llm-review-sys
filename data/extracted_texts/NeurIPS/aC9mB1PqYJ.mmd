# Learning Mixtures of Unknown Causal Interventions

Abhinav Kumar

LIDS, Massachusetts Institute of Technology

Broad Institute of MIT and Harvard

akumar03@mit.edu

&Kirankumar Shiragur

Microsoft Research

kshiragur@microsoft.com

&Caroline Uhler

LIDS, Massachusetts Institute of Technology

Broad Institute of MIT and Harvard

###### Abstract

The ability to conduct interventions plays a pivotal role in learning causal relationships among variables, thus facilitating applications across diverse scientific disciplines such as genomics, economics, and machine learning. However, in many instances within these applications, the process of generating interventional data is subject to noise: rather than data being sampled directly from the intended interventional distribution, interventions often yield data sampled from a blend of both intended and unintended interventional distributions.

We consider the fundamental challenge of disentangling mixed interventional and observational data within linear Structural Equation Models (SEMs) with Gaussian additive noise without the knowledge of the true causal graph. We demonstrate that conducting interventions, whether do or soft, yields distributions with sufficient diversity and properties conducive to efficiently recovering each component within the mixture. Furthermore, we establish that the sample complexity required to disentangle mixed data inversely correlates with the extent of change induced by an intervention in the equations governing the affected variable values. As a result, the causal graph can be identified up to its interventional Markov Equivalence Class, similar to scenarios where no noise influences the generation of interventional data. We further support our theoretical findings by conducting simulations wherein we perform causal discovery from such mixed data.

## 1 Introduction

Interventions are experiments that can help us understand the mechanisms governing complex systems and also modify these systems to achieve desired outcomes . For example, in causal discovery, interventions are used to infer causal relationships between variables of interest, which has applications in various fields such as biology , economics , and psychology .

Given their extensive applications, there has been significant research in developing methods and experimental design strategies to conduct interventions under different scenarios . Despite significant efforts to develop sophisticated experimental techniques to perform interventions, they often encounter noise . For example, the CRISPR technology, extensively used to perform gene perturbations (or interventions), is known to have off-target effects, meaning the interventions do not always occur on the intended genes . Consequently, in many applications, performing interventions generates data from a mixture of intended and unintended interventional distributions. Analyzing such mixed data directly can lead to incorrect conclusions, adversely affecting downstream applications. Hence, it is essential to disentangle the mixture and recover the components corresponding to each individual intervention for further use in downstream tasks like causal discovery.

In our work, we formally address the challenge of disentangling mixtures of unknown interventional and observational distributions within the framework of linear structural equation models (Linear-SEM) with additive Gaussian noise. Given iid samples from a mixture with a fixed number of components as input, we present an efficient algorithm that can learn each individual component. Our results are applicable to both do and more general soft interventions.

We chose to study our problem in the Linear-SEM with additive Gaussian noise framework for its fundamental importance in the causal discovery literature. Shimizu et al.  showed that observational data is sufficient for learning the underlying causal graph when the data-generating process is a Linear-SEM with additive non-Gaussian noise with no latent confounders. However, in the same setting with Gaussian noise, the causal graph is only identifiable up to its Markov Equivalence Class (MEC) [18; 25]. Thus, performing interventions (possibly noisy) is necessary to identify the causal graph, making it an interesting framework for our problem.

Our contributionsFirst, we show that given samples from a mixture of unknown interventions within the framework of Linear-SEM with additive Gaussian noise, there exists an efficient algorithm to uniquely recover the individual components of the mixture. The sample complexity of our procedure scales polynomially with the dimensionality of the problem and inversely polynomially with the accuracy parameter and the magnitude of changes induced by each intervention. Our findings indicate that the recovery error for each individual interventional distribution approaches zero as the number of samples increases. Therefore, in the infinite sample regime, we can recover the true interventional distributions, even when the targets of the interventions are unknown. Second, if the input distributions satisfy a strong interventional faithfulness assumption (as defined in Squires et al. ), we can utilize the results from  to identify the targets of the interventions, thereby enabling causal discovery using these accurately recovered interventional distributions. Finally, we conduct a simulation study to validate our theoretical findings. We show that as sample size increases, one can recover the mixture parameters, identify the unknown intervention targets, and learn the underlying causal graph with high accuracy.

## 2 Prior Work

Mixture of DAGs and Interventions.There has been a lot of interest in understanding the mixture distribution arising from a collection of directed acyclic graphs (DAGs) [27; 23; 29; 14; 9; 28]. Saeed et al.  studied the distribution arising from a mixture of multiple DAGs with common topological order, thus a generalization of our problem. Their method identifies the variables whose conditional distribution across DAGs varies. However, there is no theoretical guarantee for the identifiability of the mixture's components. Like us, Kumar and Sinha  also studied the mixture arising from interventions on a causal graph and gives an algorithm to identify the mixture component. However, they assume knowledge of the correct topological order and sample access to the observational distribution. Thiesson et al.  also studied the problem of learning a mixture of DAGs using the Expectation-Maximization (EM) framework. However, there is no theoretical guarantee about the identifiability of individual components.

Learning Mixture of Gaussians.Learning a mixture of Gaussians is a heavily studied problem [2; 7; 15; 11]. There exist efficient algorithms both in terms of runtime and sample complexity for a fixed number of components in the mixture [11; 15; 2]. Ge et al.  gave an efficient algorithm for the case when the number of components is almost \(\) where \(n\) is the dimension of the variables. However, this method only guarantees identifiability in the perturbative setting, where the true parameters are randomly perturbed before the samples are generated.

Causal Discovery with Unknown Interventions.Recent years have seen the development of methods that perform causal discovery with observational and multiple unknown interventional data [16; 26; 10]. Squires et al.  takes multiple but segregated datasets from unknown interventional distributions and aims to identify the unknown interventions and learn the underlying causal graph up to its interventional MEC (I-MEC). Jaber et al.  considers the same problem of causal discovery with unknown soft interventions in non-Markovian systems (i.e., latent common cause models). However, they also assume that the interventional and/or observational data is already segregated. To our knowledge, no prior works consider directly learning the causal graph from a mixture of interventions.

Notation and Preliminaries

Notation.We use the upper-case letter \(X\) to denote a random variable and lower-case "\(x\)" to denote the value taken by the random variable \(X\). Let, the uppercase bold-face letter \(\) denote a set of random variables and the lowercase bold-face letter \(\) denote the corresponding value taken by \(\). Let the conditional probability function \((=|=)\) be denoted by \((|)\). We use the calligraphic letter \(\) to denote a set and \(||\) to denote the cardinality of the set \(\). Let \([n]\) denote the set of natural numbers \(\{1,,n\}\). For any vector \(\), we use the notation \([]_{j}\) to denote it's \(j^{th}\) entry and for any matrix \(M\) we use \([M]_{i,j}\) to denote the entry in the \(i^{th}\) row and \(j^{th}\) column of \(M\). We use \(_{+}\) to denote positive scalars.

Structural Equation Model (SEM)Following Definition 7.1.1 in Pearl , let a causal model (or SEM) be defined by a 3-tuple \(=,,\), where \(=\{V_{1},,V_{n}\}\) denotes the set of observed (endogenous) variables and \(=\{U_{1},,U_{n}\}\) denotes the set of unobserved (exogenous) variables that represent noise, anomalies or assumptions. Next, \(\) denotes a set of \(n\) functions \(\{f_{1},,f_{n}\}\), each describing the causal relationships between the random variables having the form:

\[v_{i}=f_{i}((V_{i}),_{i}),\]

where \(_{i}\) and \((V_{i})\) are such that the associated causal graph (defined next) is acyclic. A causal graph \(_{}\)1 is a directed acyclic graph (DAG), where the nodes are the variables \(\) and the edges \(\) with edges pointing from \((V_{i})\) to \(V_{i}\) for all \(i[n]\).

Linear-SEM (with causal sufficiency)In this work, we study a special class of such causal models (Gaussian Linear-SEMs) where the function class of each \(f_{i}\) is restricted to be linear and of the form

\[v_{i}=f_{i}((V_{i}),_{i})=_{v_{j}(V_{i})}_{ij }v_{j}+u_{i},\]

where \(_{ij} 0, V_{j}(V_{i})\). The _causal Sufficiency_ assumption states that \(_{i}=\{U_{i}\}\), i.e., \(U_{i}\) is the only exogenous variable that causally affects the endogenous variable \(V_{i}\). This is equivalent to the absence of any latent confounder (Chapter 9 in ). In our work, we consider causally-sufficient Linear-SEMs; with a slight abuse of nomenclature, we will call them Linear-SEMs. The functional relationship between the exogenous and endogenous variables is deterministic, and the system's stochasticity comes from a probability distribution over the exogenous noise variables \(\). Thus, the probability distribution over the exogenous variable \(()\) defines a probability distribution over the endogenous variable \(()\). Without loss of generality, let the nodes \(\{V_{1},,V_{n}\}\) of the underlying causal graph be topologically ordered. Then, we can equivalently write the above set of equations as:

\[=A+=(I-A)^{-1},\] (1)

where \(A\), with \(A_{ij}=_{ij}\),contains the causal effects between the endogenous variables. Thus, the matrix \(A\), hereafter described as the adjacency matrix, characterizes the causal relationships between the endogenous variables (\(\)), where \(A_{ij} 0\) denotes an edge between the variable \(V_{i}\) and \(V_{j}\) in \(\).

Linear-SEM with additive Gaussian NoiseWe further specialize the exogenous variable \(u_{i}\) (henceforth referred to as noise variable) to be Gaussian with mean \(_{i}\) and variance \(_{i}\), i.e. \(u_{i}(_{i},_{i})\). Thus, the joint distribution of the exogenous variables is given by a multivariate Gaussian distribution \((,D)\) where \([]_{i}=_{i}\) and the covariance is given by a diagonal matrix \(D\) with \([D]_{ii}=_{i}\). Thus, the endogenous variables also follow a multivariate Gaussian distribution with \(()=(,S)\), where \( B_{i}\ S BDB^{T}\) and \(B(I-A)^{-1}\). Causal discovery aims to identify the unknown adjacency matrix \(A\) given observational or other auxiliary data.

Interventions.Following Definition 7.1.2 from Pearl , the new causal model describing the interventional distribution, where the variables in a set \(\) are set to a particular value, is given by \(M_{}=,V,_{}\), where \(_{}=\{f_{j}:V_{j}\}\{f^{}_{i}:V_{i} \}\) and the functional relationship of every node \(V_{i}\) with their parents and corresponding exogenous variable \(U_{i}\) is changed from \(f_{i}\) to \(f^{}_{i}\). In particular, the functional relationship of node \(V_{i}\) is changed to

\[v_{i}=_{v_{j}(V_{i})}{_{ij}}^{}v_{j}+{u_{i}}^{}\] (2)where \(u_{i}{}^{}(_{i}{}^{},_{i}{}^{})\). Such interventions are broadly referred to as "soft". Several other kinds of interventions are also defined in the literature, e.g., _do, uncertain, soft_ etc. . We consider three different types of widely studied specializations of soft interventions in our work:

1. _shift_: the mean of the noise distribution is shifted by a particular value, i.e., \(_{i}{}^{}=_{i}+\) for some \(\), and everything else remains the same, i.e., \(_{i}^{}=_{i}\) and \(a_{ij}{}^{}=a_{ij}, j[n]\).
2. _stochastic do_ (henceforth referred as _stochastic_): where all the incoming edges from parents are broken, i.e., \(_{ij}{}^{}=0\), and \(u_{i}{}^{}(_{i}{}^{},_{i}{}^{})\).
3. _do_: in addition to breaking all incoming edges, i.e., \(_{ij}{}^{}=0,\) we also set the variance of the noise distribution to 0 and the mean to any value of choice, i.e., \(u_{i}(_{i}{}^{},0)\).

Atomic InterventionsIn this work, we consider soft interventions where only one node is intervened at a time, i.e., \(||=1\). Thus after a soft intervention on node \(V_{i}\), the adjacency matrix is modified such that \(A_{i} A-_{i}(_{i}-_{i}^{})^{T}=A-_{i} _{i}^{T}\)2, where \(_{i}^{T}(_{i}-_{i}^{})^{T}\), \(_{i}^{T}\) is the \(i^{th}\) row of matrix A and \(_{i}^{{}^{}T}\) is the new row after intervention such that \([_{i}]_{k}=0, k i\), and \(_{i}\) is the unit vector with entry \(1\) at the \(i^{th}\) position and \(0\) otherwise. Thus, the linear SEM from Eq. 1 is:

\[_{i}=(I-A_{i})^{-1}_{i}=(I-A+_{i}_{i}^{T})^{-1}_ {i},\] (3)

where \(_{i}(_{i},D_{i})\), \(_{i}=+_{i}_{i}\) for some \(_{i}\), \(D_{i}=D-_{i}_{i}_{i}^{T}\) where \(_{i}(_{i}-_{i}^{{}^{}})\), is a diagonal matrix with the \(i^{th}\) diagonal entry as \(_{i}^{{}^{}}\) and rest is same as \(D\). Thus, the interventional distribution of the endogenous variables is also a multivariate Gaussian distribution, i.e., \(_{i}()=(_{i},S_{i})\) where \(_{i} B_{i}_{i}\), \(S_{i} B_{i}D_{i}B_{i}^{T}\) and \(B_{i}(I-A+_{i}_{i}^{T})^{-1}\).

## 4 Problem Formulation and Main Results

In SS4.1, we begin by formulating the problem of learning the mixture of interventions and then state our main result on the identifiability of parameters of the mixture. As a consequence of our identifiability result, under an interventional faithfulness assumption (Squires et al. ), we show in SS4.2 that the underlying true causal graph can be identified up to its I-MEC using a mixture of unknown interventions, thereby obtaining the same identifiability results as in the unmixed setting.

### Learning a Mixture of Interventions

We begin by formally defining the mixture of interventions over Linear-SEM with additive Gaussian noise and then state the main result of our paper -- a mixture of interventions can be uniquely identified under a mild assumption discussed below.

**Definition 4.1** (Mixture of Soft Atomic Interventions).: _Let \(=,,\) be an unknown Gaussian Linear SEM where the distribution of the endogenous variables is given by \(()\) (see SS3). Let \(=\{i_{1},,i_{K}\}\), hereafter referred to as intervention target set, be a set of unknown soft atomic interventions where each \(i_{k}\) generates a new interventional distribution \(_{i}()\). Then, the mixture of soft atomic intervention is defined as:_

\[_{mix}()=_{i_{k}}_{i_{k}}_{i_{ k}}()\] (4)

_where \(_{i_{k}}_{+}\), henceforth referred to as mixing weight, is a positive scalar such that \(_{i_{k}}_{i_{k}}=1\), \(i_{k}[n]\{0\}\) where \(n=||\) is the number of endogenous variables. We also allow \(i_{k}=0\), which denotes the setting when none of the nodes is intervened, i.e., \(P_{0}()()\). Using Eq. 1 and 3, a mixture defined over a linear SEM with additive Gaussian noise is a mixture of Gaussians with parameters \(=\{(_{i_{k}},S_{i_{k}},_{i_{k}})\}_{i_{k}}\) where \(_{i_{k}}()=(_{i_{k}},S_{i_{k}})\)._

Having defined a mixture of interventions, we then aim to answer the following questions: (1) Does there exist an algorithm that can uniquely identify the parameters (\(\)) of the mixture of interventions under an infinite sample limit? (2) What is the run time and sample complexity of such an algorithm? It is immediate that if the intervention doesn't change the causal mechanism in any way, then the interventional distribution is equal to the observational distribution, and we would not be able to distinguish between them. This discussion suggests that it is necessary to put an additional constraint on the interventions performed. Below, we formally state the assumption that will ensure this and that it is sufficient for the identifiability of mixture distribution.

**Assumption 4.1** (Effective Intervention).: _Let \(_{i}()=(_{i},S_{i})\) be an interventional distribution after intervening on node \(v_{i}\), where \(_{i}=B_{i}_{i}=B_{i}(+_{i}_{i})\), \(B_{i}=(I-A+_{i}_{i}^{T})\) and \(_{i}=(_{i}-_{i}^{})\), \(S_{k}=B_{i}D_{i}B_{i}^{T}\) and \(D_{i}=D-_{i}_{i}_{i}^{T}\) (see atomic intervention paragraph in SS3). Then, at least one of the following holds: \(_{i} 0\) or \(\|_{i}\| 0\) or \(_{i} 0\)._

Now, we are ready to state the main result of our work that will help us answer the above questions. For an exact expression of sample complexity and runtime, see Lemma 5.1.

**Theorem 4.1** (Identifiability of Mixture Parameters).: _Let \(_{mix}()\) be a mixture of soft atomic interventions defined over a Linear-SEM with additive Gaussian noise with \(``n"\) endogenous variables (Definition 4.1) such that the number of components \(||\) is fixed. Given Assumption 4.1 is satisfied and the causal graph corresponding to \(\) doesn't violate faithfulness, then there exists an efficient algorithm that runs in time polynomial in \(n\), requires \(polyn,,, poly(\|_{i_{k}}\|,_{i_{k}}_{i_{k}}|} }_{i_{k}}}\) samples where \(A\) is the adjacency matrix of underlying graph and with probability greater than \((1-)\) recovers the mixture parameters \(=\{(}_{1},_{1},_{1}),,(}_{||},_{||},_{||})\}\) such that_

\[_{i_{k}}\|_{i_{k}}-}_{(i_{k})}\| ^{2}+\|S_{i_{k}}-_{(i_{k})}\|^{2}+|_{i_{k}}-_{(i_{ k})}|^{2}^{2}\]

_for some permutation \(:\{1,2,,||\}\{1,2,,||\}\) and arbitrarily small \(>0\)._

### Causal Discovery with Mixture of Interventions

Theorem 4.1 helps us separate the mixture of interventions \(_{mix}()\) and provides us with the parameters \(\{(_{i},S_{i})\}_{i}\) of the distribution of all the components in the mixture. However, it does not reveal which nodes were intervened, corresponding to the different components recovered from the mixture. There has been recent progress in performing causal discovery with a disentangled set of unknown interventional distributions [26; 16]. Specifically, Squires et al.  proposes an algorithm (UT-IGSP) that greedily searches over the space of permutations to determine the I-MEC and the unknown intervention target of each component. UT-IGSP is consistent, i.e., it will output the correct I-MEC as the sample size goes to infinity. Thus, combining Theorem 4.1 with the UT-IGSP algorithm implies that, as sample size goes to infinity, we can recover the underlying causal graph up to its I-MEC given a mixture of interventions over a Linear-SEM with additive Gaussian noise:

**Corollary 4.1.1** (Mixture-Mec).: _Given samples from a mixture of interventions \(_{mix}()\) over a linear-SEM with additive Gaussian noise and samples from the observational distribution \(()\), there exists a consistent algorithm that will identify the I-MEC of the underlying causal graph under the \(\)-faithfulness assumption (defined in Squires et al. ) (and restated in SSA.1)._

Proof.: The proof follows from the identifiability of the parameters of the mixture distribution (Theorem 4.1) and the consistency of UT-IGSP given by Squires et al. . 

**Remark**.: _The \(\)-faithfulness assumption imposes certain restrictions on both observational and interventional distributions. However, as noted by Squires et al. , in the case of Linear Gaussian distributions, the set of distributions excluded by this assumption is of measure zero. This is because the Linear Gaussian distributions, defined by a matrix \(A\), that do not meet this assumption are subject to multiple polynomial constraints of the form \(p(A)=0\). It is a well-known result that for a random matrix \(A\), the set of matrices that satisfy such polynomial equalities has measure zero ._

## 5 Proof Sketch of Theorem 4.1

Here we provide an overview of the proof of Theorem 4.1. Definition 4.1 tells us that the mixture of interventions defined over a Linear-SEM with additive Gaussian noise is a mixture of Gaussians. Learning mixtures of Gaussian is well-studied in the literature [2; 15; 7]. Since most of these approaches require some form of separability between the distribution of components or the parameters of the distributions, in the following lemma we will first show that the covariance matrix and the mean of any interventional or observational distribution taken pairwise is _well-separated_ when Assumption 4.1 holds. In particular, this _seperation_ of parameters will ensure that the Gaussian mixture can be uniquely identified using the results from Belkin and Sinha .

**Lemma 5.1**.: _[Parameter Separation] Let \(_{0}()\) denote the observational distribution of a linear SEM with additive Gaussian noise \(``"\) (see SS3) with "n" endogenous variables. For some \(i,j[n]\{0\}\), let \(_{i}()=(_{i},S_{i})\) and \(_{j}()=(_{j},S_{j})\) be two interventional distributions (observational if one of \(``i"\) or \(``j"\) =0). Then the separation between covariance \(S_{i}\) and \(S_{j}\) and mean \(_{i}\) and \(_{j}\) is lower bounded by:_

\[\|S_{i}-S_{j}\|_{F}^{2}+\|_{i}-_{j}\|_{F}^{2} f(B,D)\|_{i}\|^{2}+\|_{j}\|^{2}+h(B,D, )_{i}^{2}+_{j}^{2}\] \[+g(B)|_{i}||_{i}|,_{min}(D) +|_{j}||_{j}|,_{min}(D),\]

_where after intervention on node \(k\{i,j\}\), \(\|_{k}\|\) is the norm of the perturbation (or change) in the \(k^{}\) row of the adjacency matrix, \(_{k}\) is the perturbation in the mean and \(|_{k}|\) is the perturbation in the variance of the noise distribution of node \(k\) (as defined in the Atomic Intervention paragraph of SS3). Also, \(B=(I-A)^{-1}\), and \(D\) and \(\) are the covariance matrix and mean of the noise distribution in \(\), respectively. Furthermore, \(``f",``g"\), and \(``h"\) are positive valued polynomial functions of \(B\), \(\) and smallest eigenvalue of \(D\) (see the proof in SSA.2 for the exact expressions)._

**Remark.** _If one of the distributions is observational, i.e., say \(i=0\), then the above bound holds with \(\|_{i}\|^{2}=0\), \(_{i}=0\), and \(|_{i}|=0\)._

**Remark.** _Different types of interventions will allow us to change certain parameters in the above bound. Let the exogenous noise variables \(u_{i}(_{i},_{i})\) when not intervened. Now, if we intervene on node \(v_{i}\), then the new noise variable has distribution \(u_{i}^{}(_{i}+_{i},_{i}^{})\) given as follows for different intervention types:_

1. _[label=(0)]_
2. _do intervention:_ \(\|_{i}\| 0\) _if_ \(v_{i}\) _is not a root node,_ \(|_{i}| 0\) _if_ \(_{i} 0\)_, and_ \(_{i} 0\) _if the value of node_ \(v_{i}\) _is set to any value other than_ \(_{i}\)_._
3. _stochastic intervention:_ \(\|_{i}\| 0\) _if_ \(v_{i}\) _is not a root node._
4. _shift intervention:_ \(_{i} 0\)_._

Next, we restate a definition from Belkin and Sinha  that defines the radius of identifiability (\(()\)) of a probability distribution. If \(()>0\), this implies that we can uniquely identify the distribution.

**Definition 5.1**.: _Let \(_{},\), be a family of probability distributions. For each \(\) we define the radius of identifiability \(()\) as the supremum of the following set:_

\[\{r>0_{1}_{2},(\|_{1}-\|<r,\|_{2 }-\|<r)(_{_{1}}_{_{2}})\}.\]

_In other words, \(()\) is the largest number, such that the open ball of radius \(()\) around \(\) intersected with \(\) is an identifiable (sub) family of the probability distribution. If no such ball exists, \(()=0\)._

Next, we restate a result from  adapted to our setting, which shows that there exists an efficient algorithm for disentangling a mixture of Gaussians as long as the parameters are _separated_, which will ensure that the radius of identifiability \(()>0\).

**Theorem 5.2** (Theorem 3.1 in Belkin and Sinha ).: _Let \(_{}()\) be a mixture of Gaussians with parameters \(=\{(_{1},S_{1},_{1}),,(_{||},S_{| |},_{||})\}\) where \(\) is the set of parameters within a ball of radius \(\). Then, there exists an algorithm which given \(>0\) and \(0<<1\) and \(}n,(,()}),,Q\) samples from \(_{}()\), with probability greater than \((1-)\), outputs a parameter vector \(==(}_{1},_{1},_{1}),,( }_{||},_{||},_{||})\) such that there exists a permutation \(:\{1,,||\}\{1,,||\}\) satisfying:_

\[_{i_{k}}\|_{i_{k}}-}_{(i_{k})}\|^ {2}+\|S_{i_{k}}-_{(i_{k})}\|^{2}+|_{i_{k}}-_{(i_{k}) }|^{2}^{2},\]

_where the radius of identifiability \(()\) is lower bounded by:_

\[()^{2}_{i j}( \|_{i}-_{j}\|^{2}+\|S_{i}-S_{j}\|^{2}),_{i}_{i}.\]``` input :mixed dataset \((_{})\), observational data \((_{})\), number of nodes (\(n\)), cutoff ratio (\(\)) output :Mixture Distribution Parameters (\(\)), Intervention Targets \(()\), causal graph \((})\) 1. Estimate \(_{k}(}_{1},_{1}),(}_{k},_{k})}\) = GaussianMixtureModel\(_{}\),\(k\)) for each possible number of component in the mixture i.e \(k[n+1]\). Define \(\{_{1},,_{n+1}\}\) be the set of estimated parameters and \(=\{l_{1},,l_{n+1}\}\) be the log-likelihood of the mixture data corresponding to the models with a different number of components. 2. To estimate the number of components in the mixture (\(k_{*}\)) iterate over \(k=(n+1)\) to \(2\): 1. stop where the relative change in the likelihood increases above a cutoff ratio i.e \(-l_{k-1}\|}{l_{k}}>\) 2. \(k_{*}=k\) if the stopping criteria is met otherwise \(k_{*}=1\). 3. \(,}\) = UT-IGSP\(_{},_{k_{*}}\) return\(\), \(\), \(}\) ```

**Algorithm 1**Mixture-UTIGSP

Our Theorem 4.1 along with Assumption 4.1 states that for every pair \(i,j([n]\{0\})^{ 2}\) we have \(\|S_{i}-S_{j}\|_{F}^{2}+\|_{i}-_{j}\|_{F}^{2}>0\). Also, by construction of the mixture of interventions (in Definition 4.1), we have \(_{i}>0, i[n]\{0\}\). This implies that the radius of convergence \(()>0\) and thus the parameters of the mixture of interventions \(()\) can be identified uniquely given samples from the mixture distribution with sample size inversely proportional to \(()\).

## 6 Empirical Results

### Experiment on Simulated Datasets

Proposition 4.1.1 establishes that given samples from the mixture distribution, one can identify the underlying causal graph up to its I-MEC. To learn the causal graph, we first disentangle the mixture. Theorem 4.1 and 5.2 show that the sample complexity of our mixture disentangling algorithm is inversely proportional to various parameters of the underlying system and the intervention parameters (\(,||\) and \(\|\|\)). Our simulation study further validates our theoretical results and characterizes the end-to-end performance of identifying the causal graph with such mixture data and its dependence on the above-mentioned parameters.

Simulation SetupWe consider data generated from a Linear-SEM with additive Gaussian noise, \(=(I-A)^{-1}\) (see SS3), with n endogenous variables and corresponding exogenous (noise) variables. Here \((0,D)\), where the noise covariance matrix is diagonal with entries \(D=diag(^{2},,^{2})\) and \(=1\) unless otherwise specified. \(A\) is the (lower-triangular) weighted adjacency matrix whose weights are sampled in the range \([-1,-0.5][0.5,1]\) bounded away from 0. Let \(^{*}\) denote the causal graph corresponding to this linear SEM with edge \(i j A_{ji}>0\). By sampling from the resulting multivariate Gaussian distribution, we obtain _observational_ data. Next, for each causal graph, we generate separate _interventional_ data by intervening on a given set of nodes in the graph one at a time (atomic interventions), which is again a Gaussian distribution but with different parameters (see Atomic Intervention paragraph in SS3). We experiment with two settings:

1. _all_: where we perform an atomic intervention on all nodes in the graph.
2. _half_: where we perform an atomic intervention on a randomly selected half of the nodes.

Then, the mixed data is generated by pooling all the individual atomic interventions and observational data with equal proportions into a single dataset. The decision to use equal proportions of samples from all components is solely intended to simplify the design choices for the experiment setup. In our experiment, we vary the total number of samples in the mixed dataset as \(N\{2^{10},2^{11},,2^{17}\}\). In particular, we perform two kinds of atomic interventions: "\(do\)" and "stochastic" (see Interventions paragraph in SS3 for a formal definition). The initial noise distribution for all the nodes is univariate Gaussian distribution \((0,1)\). In our experiments for _do_ interventions, instead of setting the final variance of noise distribution to \(0\), we set it to a very small value of \(10^{-9}\) for numerical stability. Unless otherwise specified, we perform 10 runs for each experimental setting and plot the \(0.05\) and \(0.95\) quantiles. See B for additional details on the experimental setup.

Method Description and Evaluation MetricsGiven the mixed data generated from the underlying true causal graph \(}^{*}\), the goal is to estimate the underlying causal graph \(}\). We break down the task into two steps. First, we disentangle the mixture data and identify the parameters of the individual interventional and/or observational distributions. Our theoretical result (Theorem 4.1) uses  for identifiability of the parameters of a mixture of Gaussians. Since they only show the existence of such an algorithm, we use the standard sklearn python package  that implements an EM algorithm to estimate the parameters of the mixture. Importantly, our experiment doesn't require prior knowledge about the number of components (k) in the mixture. We train separate mixture models varying the number of components. Then we select the optimal number of components using the log-likelihood curve of the fitted Gaussian mixture model using a simple thresholding heuristic (see step 2 in Alg. 1). We leave the exploration of better model selection criteria for future work. For all our simulation experiments, unless otherwise specified, we use a cutoff threshold of 0.07, chosen arbitrarily. In SB, we experiment with different values of this threshold and show that Mixture-UTIGSP is robust to this choice. The intervention targets present in the mixture are still unknown at this step. Next, we provide the estimated mixture parameters to an existing causal discovery algorithm with unknown intervention targets (UT-IGSP ), which internally estimates the unknown intervention targets and outputs one of the possible graphs from the estimated I-MEC. We assume that observational data is given as an input to the UT-IGSP algorithm. The proposed algorithm is provided in Alg. 1. See B for our hyperparameter choice and other experimental details.

Evaluation MetricsWe evaluate the performance of Mixture-UTIGSP (Alg. 1) on three metrics:

_Parameter Estimation Error:_ This metric measures the least absolute error between the estimated parameters (mean and covariance matrix defining each individual distribution) after the first step

Figure 1: **Performance of Alg. 1 as we vary sample size and number of nodes**: The first row (a-c) shows the performance when the mixed data contains atomic intervention on all the nodes and observational data. The second row (d-f) shows the performance when the number of atomic interventions (chosen randomly) in mixed data is taken to be half of the number of nodes along with observational data. The column shows different evaluation metrics, i.e., Parameter Estimation Error, Average Jaccard Similarity, and SHD. The symbols (\(\)) represent higher is better, and (\(\)) represents the opposite (see Evaluation metric paragraph in §6). In summary, performance improves for both cases as the number of samples increases. However, the graph with more nodes requires a larger sample to perform similarly. For a detailed discussion, see §6.1.

of Mixture-UTIGSP matched with the ground truth parameters considering all possible matchings between the components averaged over all runs. See SSB.5 for details.

_Average Jaccard Similarity (JS):_ We measure the average Jaccard similarity between the estimated intervention target and the corresponding ground truth (atomic) intervention target. We use the matching between the estimated and ground truth components found while calculating the parameter estimation error averaged over all runs. See SSB.5 for details.

_Average Structural Hamming Distance (SHD):_ Given the estimated and ground truth graphs, we compute the SHD between the two graphs averaged over all runs.

ResultsFig. 1 shows the performance of Alg. 1 in the two settings, _"all"_ in the first row and _"half"_ in the second row (see Simulation setup above). The first column shows the performance of the first step of Alg. 1 where the mixture parameters are identified. We observe that parameter estimation error decreases as the number of samples increases in both settings. As expected, larger graphs require a larger sample size to perform similarly to smaller-sized graphs within each setting.

Step 1 of our Alg. 1 only recovers the parameters (\(\{(_{i},S_{i})\}_{i=1}^{k}\)) of the components present in the mixture distribution. In step 2 of our Alg. 1, we call UT-IGSP  that identifies the individual intervention targets from the estimated distribution parameters and also returns a causal graph from the estimated I-MEC. Fig. 0(b) and 0(e) show the average Jaccard Similarity between the ground truth and the estimated intervention targets. The colored lines denote experiments on graphs with different numbers of nodes. The corresponding dotted lines show the oracle performance of UT-IGSP when the separated ground truth mixture distributions were given as input. As expected, in both the settings (Fig. 0(b) and 0(e)), the oracle version performs much better compared to its non-oracle counterpart for small sample sizes (\(2^{10}\) to \(2^{14}\)) but performs similarly as sample size increases.

Finally, in the third column (Fig. 0(c) and 0(f)), we calculate the SHD between the estimated causal graph \(}\) and the ground truth causal graph \(^{*}\). The SHD of the graph estimated by Mixture-UTIGSP and the oracle version are similar for different node settings and all sample sizes. This suggests that small errors in the estimation of the parameters of the mixture distribution don't affect the estimation of the underlying causal graph.

**Additional Experiments:** In SSB, we provide details on the experimental setup and additional results. In Fig. 2, we plot two additional metrics for the simulation experiments. The first metric is the number of estimated components in the mixture and the second metric is the the error in estimation of the mixing coefficient. In Fig. 3, we study the sensitivity of the cutoff ratio used by Mixture-UTIGSP to select the number of components in the mixture. Next, in Fig. 4, we evaluate the performance of our Alg. 1 as we vary the density, i.e., the expected number of edges in the graph. In Fig. 6, we show how the sparsity of the graph and other intervention parameters like the value of the new mean and variance of the noise distribution after intervention affects the performance of Alg. 1.

  
**cutoff ratio** & 
 **Estimated/True** \\ **Component** \\  & **JS** & **Oracle JS** & **SHD** & **Oracle SHD** \\ 
0.01 & 4/6 & 0.07 & 0.05 & 15 & 19 \\
0.07 & 2/6 & 0.04 & 0.05 & 18 & 17 \\
0.15 & 1/6 & 0.00 & 0.05 & 16 & 18 \\
0.30 & 1/6 & 0.00 & 0.05 & 18 & 17 \\  avg & 2/6 & 0.03 & 0.05 & 16.75 & 17.75 \\   

Table 1: **Performance of Alg. 1 on Protein Signalling Dataset **: We evaluate the performance of Mixture-UTIGSP as we vary the cutoff ratio to select the number of component in the mixture. The second column shows the number of estimated components where the actual number of components in the mixture is 6. The third and fourth columns show the Jaccard Similarity of the identified intervention target of Mixture-UTIGSP and oracle versions of the UT-IGSP algorithm. The fourth and last column shows the SHD between the estimated and true causal graphs for both methods respectively. Overall we observe that at a lower cutoff threshold Mixture-UTIGSP is able to perform as well as the oracle UT-IGSP algorithm on all the metrics. See SSB.2 for detailed discussion.

### Experiment on Biological Dataset

We evaluate our method on the Protein Signaling dataset  to demonstrate real-world applicability. The dataset is collected from flow cytometry measurement of 11 phosphorylated proteins and phospholipids and is widely used in causal discovery literature . The dataset consists of 5846 measurements with different experimental conditions and perturbations. Following Wang et al. , we define the subset of the dataset as observational, where only the receptor enzymes were perturbed in the experiment. Next, we select other 5 subsets of the dataset where a signaling protein is also perturbed in addition to the receptor enzyme. The observational dataset consists of 1755 samples, and the 5 interventional datasets have 911, 723, 810, 799, and 848 samples, respectively. The mixed dataset is created by merging all the observational and interventional datasets.

The total number of nodes in the underlying causal graph is 11. Thus, the maximum number of possible components in the mixture is 12 (11 single-node interventional distribution and one observational). In the mixture dataset described above, we have 6 components (1 observational and 5 interventional). The second column in Table 1 shows that Mixture-UTIGSP recovers 4 components close to the ground truth 6 when the cutoff ratio is 0.01 (step 2 of Alg. 1). Next, we give the disentangled dataset from the first step of our algorithm to identify the unknown target. Though the Jaccard similarity of the recovered target is not very high (average of 0.03 shown in the last row of the third column, where the maximum value is 1.0), it is similar to that of the oracle performance of UT-IGSP when the disentangled ground truth mixture distributions were given as input. This shows that it is difficult to identify the correct intervention targets even with correctly disentangled data. Also, the SHD between the recovered graph and the widely accepted ground truth graph for Mixture-UTIGSP (ours) and UT-IGSP (oracle) is very close. Overall, at a lower cutoff ratio, the performance of Mixture-UTIGSP is close to the Oracle UT-IGSP algorithm. Unlike the simulation case (see Fig. 3), Mixture-UTIGSP's performance is sensitive to the choice of the cutoff ratio on this dataset. In Fig. 5, we plot the ground truth graph curated by the domain expert alongside the estimated causal graph for visualization.

## 7 Conclusion

We studied the problem of learning the mixture distribution generated from observational and/or multiple unknown interventional distributions generated from the underlying causal graph. We show that the parameters of the mixture distribution can be uniquely identified under the mild assumption that ensures that any intervention changes the distribution of the observed variables. As a consequence of our identifiability result, under an interventional faithfulness assumption (Squires et al. ), we show that the underlying true causal graph can be identified up to its I-MEC based on a mixture of unknown interventions, thereby obtaining the same identifiability results as in the unmixed setting. Finally, we conduct a simulation study to validate our findings empirically. We demonstrate that as the sample size increases, we obtain parameter estimates of the mixture distribution that are closer to the ground truth and, as a result, we eventually recover the correct underlying causal graph.

## 8 Limitations and Future Work

Since our work is the first to study the problem of a mixture of causal interventions without assuming knowledge of causal graphs, we have restricted our attention to one particular family of causal models--Linear-SEM with additive Gaussian noise. In the future, it would be interesting to study this problem for a more general family of causal models. Further, our work uses Belkin and Sinha  for identifying the parameters of a mixture of Gaussians, which assumes that the number of components in the mixture is fixed. Recent progress in  gives an efficient algorithm for recovering the parameters when the number of components is almost \(\), where \(n\) is the number of variables. However, they only work in perturbative settings, and proving the result for non-perturbative settings is out of the scope of the current paper. Finally, to identify the parameters of the mixture distribution in our empirical study, we use heuristics to estimate the number of components. It would be interesting to explore other methods to automatically select the number of components.