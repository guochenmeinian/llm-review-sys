# Recovering Complete Actions for Cross-dataset Skeleton Action Recognition

Hanchao Liu\({}^{1}\)   Yujiang Li\({}^{1}\)   Tai-Jiang Mu\({}^{1}\)   Shi-Min Hu\({}^{1}\)

\({}^{1}\)BNRist, Department of Computer Science and Technology, Tsinghua University

Tai-Jiang Mu is the corresponding author. Email: taijiang@tsinghua.edu.cn. Code is available at https://github.com/HanchaoLiu/Recover-and-Resample

###### Abstract

Despite huge progress in skeleton-based action recognition, its generalizability to different domains remains a challenging issue. In this paper, to solve the skeleton action generalization problem, we present a recover-and-resample augmentation framework based on a novel complete action prior. We observe that human daily actions are confronted with temporal mismatch across different datasets, as they are usually partial observations of their complete action sequences. By recovering complete actions and resampling from these full sequences, we can generate strong augmentations for unseen domains. At the same time, we discover the nature of general action completeness within large datasets, indicated by the per-frame diversity over time. This allows us to exploit two assets of transferable knowledge that can be shared across action samples and be helpful for action completion: boundary poses for determining the action start, and linear temporal transforms for capturing global action patterns. Therefore, we formulate the recovering stage as a two-step stochastic action completion with boundary pose-conditioned extrapolation followed by smooth linear transforms. Both the boundary poses and linear transforms can be efficiently learned from the whole dataset via clustering. We validate our approach on a cross-dataset setting with three skeleton action datasets, outperforming other domain generalization approaches by a considerable margin.

## 1 Introduction

Skeleton-based action recognition has recently achieved great success . The skeleton-based action representation has the advantage of removed background changes and camera positions, making it more robust compared to RGB representation. However, the generalizability to unseen domains under such a representation is still affected by the inherent spatiotemporal difference of 3D coordinates of a same action across domains, which yet largely remains an under-explored issue. In this paper, we study the single domain generalization problem  for skeleton-based action recognition, in which we do not have knowledge about the target domain.

Essentially, both cross-subject and cross-view settings  fall under the category of cross-domain settings, and they can be well addressed by designing more powerful backbones and applying geometric transformations, achieving high accuracy in the test set . However, we find that in the _cross-dataset_ setting where source and target data come from different datasets, the performance on accuracy degrades a lot (around or more than 20% in some cases ) and cannot be well remedied by the above approaches. This indicates drastic domain gap in the inherent feature of human actions across datasets, posing great challenges for real-life applications  and calling for research on domain generalization techniques for skeleton-based representation, which is the focus of this paper.

Investigating action samples across multiple datasets, our observation is that a notable source of domain gap comes from the temporal mismatch of an action across different datasets (Fig. 1(a)), which is usually caused by different definition or cropping criterion of human motions. Existing methods cannot perfectly handle this problem. Warping-based temporal alignment approaches, while being popular for few-shot recognition tasks [3; 53; 33; 45], are inefficient and not optimal when given sufficient and diverse training data. Besides general domain generalization approaches [35; 51], self-supervised skeleton representation learning methods [23; 47; 46; 59], while showing good transferability of the learned feature, are still difficult to handle data with large gaps as the generalizability largely comes from seeing handcrafted augmentations in either contrastive learning  or auxiliary tasks . Different from the above approaches, we aim to directly hallucinate strong augmentations for unseen domains by exploring some sort of human action priors.

As in Fig. 1 (b), we train a skeleton auto-encoder and examine the per-frame latent feature diversity of the whole dataset (measured by standard deviation) for several skeleton action datasets, i.e., NTU , PKU , ETRI  (Details in Appendix A5). We observe that human action sequences start with relatively low feature diversity, which is actually a form that humans perform generally _complete_ actions within large datasets, from rest poses that are less diverse (e.g. stand, sit) to rich-semantic poses that are more diverse. We summarize this pattern as a novel temporal prior named **complete action prior**. Although this prior can be detected by statistics in a general sense, in terms of individual samples, some exhibit strong action completeness while some are segments of their complete actions. This motivates us to learn an action completion function from the whole training data which can transform incomplete actions into complete ones. In this way, we can recover the complete sequence of an action from its partial observation, and resample from it to hallucinate data in unseen domains.

Going deeper into such a statistical pattern, we can further mine some class-agnostic knowledge from the whole dataset that can be used for our action completion. First, the low diversity at beginning frames implies the existence of a set of representative boundary poses, which can be used for determining the start of an action. Second, the general completeness implies the existence of long and nearly complete actions within a large dataset. By studying the relationships between their raw and trimmed pairs, we can learn temporal patterns inherited in human actions.

Based on the above observations, we propose a novel recover-and-resample augmentation framework for single domain generalization on skeleton action recognition. As in Fig. 1 (c), we recover complete actions from training samples and resample from them to further augment the training set. Especially, in the recovering stage, we adopt a two-step stochastic action completion, which first extrapolates the raw sequence conditioning on the boundary pose and then applies temporal transforms. A set of boundary poses are learned from the first frames of the training data. Altogether, a set of smooth linear transforms (linearity means re-organizing existing frames) are learned from reconstructing full sequence from its trimmed segment via a closed-form solution of context-aware frame similarity aggregation. We find such a simplified form of transform suitable and expressive enough for modeling common structural temporal patterns, e.g., shifting, scaling, symmetry, etc. The learning for both boundary poses and linear transforms can be achieved via clustering, for example, \(k\)-means, which makes our whole framework light and efficient. Also, we find a random cropping suffices for the resampling stage.

Figure 1: (a) **Cross-dataset skeleton action recognition.** Taking action _phone calling_ as an example, temporal mismatch across datasets poses a challenging issue. (b) **Complete action prior.** Human actions within large datasets exhibit statistical patterns from less feature diversity to more diversity, implying the nature of action completeness (shown for NTU, PKU and ETRI dataset). (c) **Recover and Resample.** After learning a stochastic action completion function from the training data, we recover complete actions and resample from them to further augment the training set.

We simplify the problem in cross-domain settings by studying well-defined daily actions in indoor settings. Specifically, we construct a new evaluation setting jointly with three large-scale datasets including PKU-MMD , NTU-RGBD  and ETRI-Activity3D . Our proposed augmentation method well solves the temporal misalignment issue. We improve the average accuracy on unseen datasets by 5%, outperforming other baseline methods by a large margin.

In summary, the contributions of this paper are as follows:

* We discover the complete action prior within large datasets by its statistical pattern, whose effect has not been well studied before. Building on such a prior, we present a novel recover-and-resample augmentation framework for domain generalization on skeleton-based action recognition.
* We propose an effective clustering-based approach to recover complete actions, which is achieved by boundary pose conditioned extrapolation and context-aware smooth linear transforms.
* We demonstrate the superiority of our method over a wide range of methods by a large margin and we conduct extensive experiments to show the effectiveness of each proposed module.

## 2 Related Work

### Skeleton-based Action Recognition

As for skeleton action recognition, the graph neural network [57; 42; 6; 29] has become a prevailing model due to its effectiveness to model spatiotemporal relations between joints. We use AGCN (Adaptive GCN)  as our backbone as it can model flexible graph layouts. Recently, there is a focus on studying skeleton action recognition with less labeled data. For self-supervised skeleton representation learning, a number of pretext tasks are proposed based on invariant augmentations [23; 47; 11; 59; 56]. These approaches improve the generalizability across datasets but are still bounded by the specific augmentation designs. For few-shot skeleton action recognition, metric learning approaches with different types of dynamic time warping [3; 53; 33; 45] are widely adopted. However, they are not optimal for domain generalization given large amount of training data. Our work especially explores the generalization problem for this field when data is not available in the target domain.

### Domain Generalization and Data Augmentation

General domain generalization approaches mainly rely on learning adversarial augmentation [51; 22; 55] and self-supervised auxiliary tasks . They are not optimal for skeleton-based action recognition tasks since they do not make full use of the skeleton representation. The recent ST-Cubism  is most related to ours, which adapts learning jigsaw puzzles  to skeleton sequences to achieve domain generalization. While there are many augmentation methods intended for non-semantic related tasks like pose estimation [16; 2; 10; 9; 14] and human motion prediction , skeleton augmentation for recognition tasks is essentially challenging as it is fragile to spoil the semantics of skeletons formed by low-dimensional joints. SFN  combines Mixup  with VAE for augmentation, but it still cannot deal with cross-domain settings. ModSelect  proposes a selection mechanism for multi-modal input for cross-domain action generalization. In this work we focus on the learnable temporal action augmentation for cross-domain settings.

### Human Motion Priors

As human motions are constrained in the spatiotemporal space, many human motion priors are proposed, such as manifolds of valid human poses [19; 7; 48] and motion [63; 27; 17], and other properties such as periodical human motion pattern [43; 12; 44], body part interchangeability across samples [26; 24] and alignment with other modalities [65; 1]. ACTOR  learns a CVAE and samples data points conditioning on labels as an augmentation method for recognition tasks. However, it is only useful when data is scarce. Rate-invariant prior  is related to ours, but changing rate itself is not enough for cross-domain settings. In this paper, we explore the action completeness prior for better performance in cross-domain settings. Although the notion of action completeness [30; 64] appears in the temporal action localization (TAL) task , we are the first to incorporate it for the skeleton-based domain generalization task.

## 3 Method

### Problem Setting

In the skeleton-based action recognition, suppose we have source data from one single domain \(=\{(x_{},y_{})\}\). Due to the unfixed length of input action sequences, the skeleton sequences \(x_{}\) are all uniformly resized to \(x_{}^{T J 3}\) with sequence length \(T\) and joint number \(J\)[6; 15]. Our goal is to train a model \(\) using \(\) and test on other unseen target domains \(\{(x_{1},y_{1}),(x_{2},y_{2}),\}\). In particular \(y_{}\) and \(y_{}\) share the same action categories.

### A Recover-and-Resample Framework

Motivated by the observations of temporal mismatch (Fig. 1(a)) and action completeness within a dataset (Fig. 1(b)), we first introduce a generalized form of temporal augmentation, which we name recover-and-resample augmentation. Given a training sample \(x\), we first recover its complete action with a transform \(W_{}\) and then sample a segment of it using \(W_{}\) as an augmentation \(x^{}\):

\[x^{}=W_{}(x)=(W_{} W_{})(x).\] (1)

Ideally \(W_{}\) can generate all kinds of temporal segments that may appear in the target domains. In contrastive self-supervised learning,  uses temporal shift as a recovering process, while  only uses resampling for constructing positive samples. We set the resample transform \(W_{}\) as a common random sampling , and focus more on the recovering stage \(W_{}\) that can construct a complete action for the input motion \(x\). Our contribution is that by exploiting the knowledge from complete action prior, we further decompose \(W_{}\) into a linear transform and a nonlinear transform, i.e. \(W_{}(x)=_{L}(_{N}(x))\). The nonlinear transform \(_{N}\) extrapolates the motion to generate new poses which do not exist in the original motion. The linear transform \(_{L}\) reorganizes the motion sequence using existing frames. In this way we boost the expressive power of \(W_{}\) for generating complete actions. Fig. 2 gives an illustration on how we generate augmentations for a partially observed action _phone calling_. Details for \(_{L}\) and \(_{N}\) are introduced in Sec. 3.3 and 3.4.

### Boundary-conditioned Extrapolation

**Boundary pose clustering.** When recovering a complete action, we first determine its boundary pose. Intuitively, boundary poses for common daily activities are more likely to be _background_ poses (or rest poses). They do not convey strong action semantics and can be distinguished from meaningful foreground poses. On the other hand, the statistical finding that the initial poses of skeletal

Figure 2: **Overview of Recovering and Resampling. Given training set \(\), we learn boundary poses \(\{p_{i}\}\) and context-aware linear transforms \(\{W_{i}\}\) via clustering. For a sample \(x\) from \(\), we first do **extrapolation** (\(_{}\)) conditioning on the boundary pose \(p^{}\) with infilling length \(t_{p}\), and then perform **linear transform** (\(_{}\)) by sampling from \(\{W_{i}\}\). The new data points \(x^{}\) are **resampled** from recovered complete actions as strong augmentations for unseen datasets. Skeletons in dark blue rectangles are new frames generated by \(_{}\) and \(_{}\). Both \(x\) and \(x^{}\) are used for training the classifier.**action sequences have low feature diversity (shown in Fig. 1 (b)) also validates our assumption: the boundary poses are to some extent constrained. Therefore, we propose to cluster the first frames \(x_{0}\) of all the training samples in \(\) to get a set of representative background (boundary) poses, which we denote as \(\{p_{i}\}\). During clustering, all the first frames \(\{x_{0}|x_{0}^{J 3}\}\) are flattened to 1d vectors and \(L_{2}\) norm is used as the distance measure.

**Conditional generation.** We use nearest neighbour search to assign the boundary pose \(p^{}\) for \(x\) with first frame \(x_{0}\):

\[p^{}=_{\{p_{i}\}}|x_{0}-p_{i}|.\] (2)

Now we can extrapolate the original first frame \(x_{0}\) to the new boundary pose \(p^{}\). This is basically an infilling process conditioned on \(p^{}\), and we control the length of extrapolation with parameter \(t_{p}\). Supposing the sequence length is \(T\), we set the first frame as \(p^{}\), and squeeze the original motion \(x\) to the segment from frame \(t_{p}\) to the end frame \(T\). We then infill the timestamps from the first frame to frame \(t_{p}\) with new motion. Considering a stochastic process, we sample \(t_{p}\) from a Beta distribution \((,)T/2\) with high probability on the first frame and frame \(T/2\). This means the conditional generation process is more likely to retain the original motion or infill a segment with length \(T/2\).

**Motion infiller.** The simplest form of the motion infiller \(\) is linear interpolation since the skeleton already has good joint correspondence. We also propose an alternative to train an infiller with a neural network. We mask out some consecutive frames to get masked and full sequence pairs and train an action completion network. We then use this motion infiller to perform extrapolation. The comparison will be shown in the later experiments. Actually we find the parameter-free linear interpolation works well enough for generating reasonable motions for the recognition task.

So formally our nonlinear transform can be represented as \(_{N}(x)=(x,p^{},t_{p})\). We first find the boundary pose \(p^{}\), sample \(t_{p}\) to determine the sequence length to infill, and finally apply the motion infiller \(\).

### Learning Smooth Linear Transforms

Note that the above nonlinear transform is still unable to capture global and structural patterns inherited in the human actions. Consequently, we further propose linear transforms \(_{L}\) to reorganize frames for more powerful transformations. We have seen great progress in self-supervised learning with very simple temporal linear transforms like _crop & pad_. However they are manually designed and are not learned from data so they are not flexible. Here we propose to construct partial and full skeleton sequence pairs to learn a set of linear transforms in the form of \(_{L}(x)=Wx,W^{T T}\). Here \(x^{T J 3}\) is a skeleton sequence with length \(T\).

**Context-aware smooth linear transforms.** Given partial sequence \(u\) and full sequence \(v\) (see Fig. 2) which are then both resized to length \(T\), we hope to find a linear transform that minimizes the \(L_{2}\) norm \(|Wu-v|\), where \(W\) has only one nonzero element for each row. The straightforward solution is to do it frame-wise, i.e. for each frame \(v_{i}\) in \(v\) we find the index of its closest frame \(u_{j}\) in \(u\). However, such an operation cannot ensure that \(W\) is smooth and consistent with good transferability. Inspired by the context-aware alignment , we propose _context-aware smooth linear transform_ that aggregates information from semantically adjacent frames.

Specifically, we first find the context-aware frame-wise similarity matrix of \(u\) and \(v\). For example, the similarity score \(s_{ij}\) between _i_-th frame of \(v\) and _j_-th frame of \(u\) can be obtained as following:

\[s_{ij}=-u_{j}|/_{T})}{_{m=1}^{T}(-|v_{i}-u_{m} |/_{T})},\] (3)

where \(||\) denotes \(L_{2}\) distance between two flattened skeletons (root translation already removed). Larger \(_{T}\) means stronger context-aware smoothing. We then calculate the index by:

\[k_{i}=_{j=1}^{T}j s_{ij}.\] (4)

The transform matrix \(W\) is then obtained by setting \(W[i,round(k_{i})]=1\). This is to say, when deciding the most similar frame for \(v_{i}\), all frames in \(u\) are considered by their similarity weights. In this way the transform \(W\) becomes smooth and more resilient to noise frames.

Clustering linear transforms.Again we utilize clustering algorithm to extract all the possible transform patterns that map partial sequences to full sequences. Since the original number of transforms for \(W\) obtained from randomly sampled trimmed and full pairs may be too large, with clustering we avoid the inefficient sampling of \(W\). Meanwhile, during the clustering, some important transform patterns, e.g. mirroring, can stand out as they may originally only account for a small percentage in the whole pool of transforms.

In details, we first randomly generate training pairs, i.e. partial motion sequences and full motion sequences \(\{(u_{k},v_{k})\}\) from \(\). We then use Eq. (3) to get similarity matrix \(M^{(k)}\) for motion \(u_{k}\) and \(v_{k}\). Instead of clustering directly on \(W\), we perform clustering on all the similarity matrices \(\{M^{(k)}\}\) to get representative cluster centers \(\{M_{i}\}\). We normalize each row for \(M_{i}\), and then calculate the index according to Eq. (4) and obtain the corresponding transform matrix \(W_{i}\). Again, during clustering, each \(M^{(k)}^{T T}\) is flattened to 1d vector and we use \(L_{2}\) norm as the distance measure.

Once we obtain a set of \(\{W_{i}\}\), we can randomly sample \(W\) from \(\{W_{i}\}\) and apply it to \(x\). So the linear transform can be formally represented as \(_{L}(x)=Wx,W\{W_{i}|W_{i}^{T T}\}\).

### Training

We follow the standard pipeline for training a recognition model. For input \(x\) with its action label \(y\), we generate augmentations \(x^{}\) (refer to Algorithm 1 in Appendix A1), and train the model with the loss function:

\[_{}=((x),y)+((x^{}),y),\] (5)

where \(\) is the standard cross-entropy loss. In practice, in a batch, we augment the training samples with a ratio of \(m_{}\), and the rest of the samples remains unchanged.

## 4 Experiments

### Datasets and Settings

**Datasets.** We use four large-scale datasets, i.e, NTU60-RGBD , PKU-MMD , ETRI-Activity3D  and Kinetics . The first three datasets are captured in indoor laboratory and household environments while the last one is collected from online videos.

**A new cross-dataset setting.** We construct a new multi-domain cross-dataset setting, mainly comprising actions in indoor environment. We gather shared 18 actions (see Appendix A2) using the first three datasets described above and each dataset is treated as one domain. We train on one domain and test on the rest two domains. Specifically, we define four domain transfer sub-settings, i.e. \(NE\), \(NP\), \(E_{A}N\), \(E_{A}P\) (each dataset is denoted with its first letter). Especially we use the adult split \(E_{A}\) for training because in this way we keep the number of training samples for NTU and ETRI relatively the same. We do not include \(P\) for training since \(P\) majorly contains long and complete actions due to its annotation and it is relatively easy to perform sampling directly. We also reserve a subset \(\) with mutually exclusive labels with \(\) as a prior dataset to evaluate in an "Oracle" case when we have certain knowledge about how complete actions look like.

**Evaluation Metric.** We use the average accuracy of the above four cross-dataset sub-settings to measure the performance of single domain generalization. Especially we use balanced accuracy [32; 39] to eliminate the influence of class bias. We train the model with ten different random seeds and report the mean accuracy.

For fair comparison with ST-Cubism , we also include \(P51 N51\) with 51 action classes paired between NTU and PKU dataset, and \(N12{}K12\) with 12 action classes paired between NTU and Kinetics. We also follow their evaluation protocols when reporting results.

### Implementation Details

**Data preparation.** We resize motion sequences from different domains to a fixed length \(T=64\). Following , we remove camera rotation and trajectory movement in the pre-processing stage and we further apply random 3D rotation as spatial augmentation for all methods.

**Backbone models.** For our backbone model, we adopt a slightly modified version  of Adaptive GCN (AGCN)  with fewer blocks and only use the joint stream for speed and simplicity. We reduce the number of blocks from 10 to 4. The output channels for each block are 64, 64, 128 and 256. The kernel strides for each block are 1, 1, 2 and 2. We find that such a design improves the base generalizability partially because it better fits the reduced length of motion sequences and avoids overfitting. Besides our AGCN backbone, we also test on ST-GCN  and CTR-GCN , two representative GCN backbones. ST-GCN is a simple GCN that aggregates spatiotemporal information without elaborate design. CTR-GCN adopts a multi-level feature design and has much more parameters than our AGCN. HCN is a two-stream convolutional network used by . We use it for fair comparison in \(P51 N51\) and \(N12\)\(\)\(K12\) settings.

**Training details.** We set training hyper-parameters the same as . Furthermore, we set \(_{T}=0.1\), the number of linear transforms \(N_{}=20\), the number of background poses \(N_{}=10\), and \(m_{}=0.75\). We set \(=0.1\) for sampling \(t_{p}\). We fix the resampling method by randomly sampling a segment with length ratio \(r\) between 0.7 and 1.0. We use \(k\)-means for clustering boundary poses and linear transforms. Parameter settings for \(P51\)\(\)\(N51\) and \(N12\)\(\)\(K12\) are provided in Appendix A3.

### Results

**Baselines.** Besides ST-Cubism  which is by far the only method that reports result for cross-dataset settings, we compare with a wide range of baseline methods. **(1) General domain generalization approaches:** latent feature alignment CCSA  and adversarial augmentation ADA [51; 52]. **(2) Self-supervised learning approaches:** ST-Cubism  that solves jigsaw puzzles in spatial (Spa) and temporal (Tem) dimensions, Skeleton-MAE (partially borrowed from ) that

   Method & \(NE\) & \(NP\) & \(E_{A}N\) & \(E_{A}P\) & Avg. \\  ERM & 54.9 & 70.5 & 42.4 & 49.7 & 54.4 \\ CCSA  & 56.0 & 72.2 & 43.6 & 51.7 & 55.9 \\ ADA  & 55.2 & 69.2 & 43.8 & 50.7 & 54.7 \\ ST-Cubism  & **59.1** & 71.7 & 45.4 & 52.4 & 57.1 \\ Skeleton-MAE  & 56.1 & 72.7 & 44.5 & 52.4 & 56.4 \\ HICLR  & 54.0 & 70.6 & 46.7 & 53.8 & 56.3 \\ Uniform sampling  & 56.7 & 69.4 & 45.3 & 51.3 & 55.7 \\ Mixup  & 55.0 & 69.9 & 44.5 & 52.2 & 55.4 \\ CropPad  & 56.8 & 70.1 & 44.0 & 50.9 & 55.4 \\ CropResize  & 57.5 & 70.3 & 45.5 & 50.6 & 56.0 \\ TSN  & 54.3 & 68.6 & 43.0 & 50.0 & 54.0 \\ Multiple-crop testing & 54.4 & **76.5** & 40.8 & 52.6 & 56.1 \\ OTAM+kNN  & 54.2 & 72.7 & 42.9 & 50.8 & 55.2 \\  Ours & 58.4 & 75.8 & **48.4** & **57.8** & **60.1** \\   

Table 1: Comparison with other methods in our cross-dataset settings. The best result is in bold and the second best is with underlines.

   Method & Backbone & \(N51\)\(\)\(P51\) & \(P51\)\(\)\(N51\) \\  ERM & HCN & 57.6 & 50.5 \\ ST-Cubism (Tem)  & HCN & 60.0 & 52.7 \\ ST-Cubism (Spa)  & HCN & 59.6 & 50.8 \\ ST-Cubism  & HCN & 61.3 & **53.8** \\ Ours & HCN & **62.8** & 53.3 \\   Method & Backbone & \(N12\)\(\)\(K12\) \\  ERM & HCN & 14.4 \\ ST-Cubism  & HCN & 15.6 \\ Ours & HCN & **15.9** \\   

Table 3: Comparison with ST-Cubism in multiple cross-dataset settings with HCN backbone.

learns skeleton representation in the fashion of masked auto-encoder, and HICLR  which is a representative pre-training method via contrastive learning. **(3) Augmentation-based approaches:** mixed sample data augmentation Mixup , Crop and reflective padding (CropPad) , crop and resizing (CropResize)  and uniform sampling . **(4) Alignment-based approaches:** OTAM  +kNN which uses nearest neighbour classifier  with a variant OTAM  as the dynamic time warping distance. **(5) Aggregation-based approaches:** Temporal segment network design  and multiple-crop testing (5-crop) which samples clips from a test sequence and average the output scores. Moreover, Empirical Risk Minimization (**ERM**) uses standard cross-entropy loss and serves as a performance lower bound. More implementation details are provided in Appendix A3.

The result is shown in Table 1. Our method, by explicitly exploring the action prior, improves ERM by 5.7% and outperforms the second best (ST-Cubism) by 3.0% in terms of average accuracy. All the self-supervised and augmentation-based methods improve the domain generalizability while the former perform better than the latter. However, neither of them can significantly improve the generalizability to deal with unseen data with large gaps. By learning transforms from training data, we are more flexible than handcrafted augmentations and perform especially better in recognizing long sequences (\(P\) domain). Test-time aggregation only improves on one specific setting and warping-based matching with kNN is not as competitive as directly learning a deep classifier when given sufficient and diverse training data (See more discussion in Appendix A6). While sharing the same spirit that temporal alignment is important, we provide an alternative solution which combines raw training data and augmented "aligned" data from recovering and resampling complete actions.

We further transfer boundary poses and linear transforms learned from the 18-class subset to \(N51{}P51\) settings. In Table 3, we compare to ST-Cubism  following their protocol using HCN  backbone. For \(N51{}P51\), as in Table 2 and 3, our method outperforms ST-Cubism, showing the effectiveness when there are more action classes. For \(P51{}N51\), ours is better than the temporal part of ST-Cubism but slightly worse than the whole ST-Cubism, which can be explained that \(P51\) generally already consists of complete actions. For the challenging 2D setting \(N12{}K12\), we use linear transform module only and also obtain slightly better performance than ST-Cubism.

### Analysis and Discussion

**Effect of each component.** We examine the effect of the non-linear transform \(_{N}\), i.e. the boundary conditioned extrapolation, and the linear transform \(_{L}\). As shown in Table 4, both \(_{N}\) and \(_{L}\) improve the ERM baseline in terms of the average accuracy, showing they are crucial to construct complete actions and validating our two-step design. As a comparison, we see that recovering with handcrafted CropPad only leads to limited improvement (1.3%). Moreover, in cases when the action completeness is less significant in the target domain (e.g. \(N{}E\)), \(_{L}\) itself may be sufficient.

   Method & \(N E\) & \(N P\) & \(E_{A} N\) & \(E_{A} P\) & Avg. \\  ERM & 54.9 & 70.5 & 42.4 & 49.7 & 54.4 \\ CropPad  & 58.0 & 67.9 & 45.2 & 51.7 & 55.7 \\ Nonlinear \(_{N}\) & 57.7 & 70.8 & 48.3 & 54.5 & 57.8 \\ Linear \(_{L}\) & 58.6 & 72.8 & 45.3 & 52.2 & 57.2 \\ Linear(Self)+Nonlinear(Self) & 58.4 & 75.8 & 48.4 & 57.8 & 60.1 \\ Linear(\(\)) +Nonlinear(Self) & 58.2 & 76.2 & 48.5 & 57.7 & 60.2 \\ Linear(Self)+Nonlinear(\(\)) & **58.7** & **76.4** & 48.4 & 58.4 & **60.5** \\ Linear(\(\)) +Nonlinear(\(\)) & 57.9 & 76.3 & **49.2** & **58.6** & **60.5** \\   

Table 4: Effect of each component and the effect of using different prior datasets for our method. All ablation experiments in this table have the resampling step.

Figure 3: Visualization for selected linear transform matrices \(\{W_{i}\}\) via clustering using training sets \(N\) and \(E_{A}\).

Since \(_{L}\) and \(_{N}\) learned from different datasets are transferable, we can also use \(\) as a high quality prior dataset to replace the original training data (denoted as Self in Table 4) to learn \(_{L}\) and \(_{N}\). In this way we can investigate the performance upper bound of our method. By utilizing \(\) we can further improve the average accuracy from 60.1 to 60.5. This shows that it would be more beneficial to directly learn from nearly complete actions since it is our aim to reconstruct complete actions from partial observations. On the other hand, the marginal improvement of 0.4% also indicates that even from the training data we already learn transforms and boundary poses in a quite decent manner.

**Parameter analysis.** We examine three important parameters: number of clustered boundary poses \(N_{}\), number of clustered linear transforms \(N_{}\), and context-aware coefficient \(_{T}\). The results are in Table 5 and 6. As for \(N_{}\), a too small number of clusters will hurt the overall performance by around 1.2%, which shows that too few linear transforms cannot fully represent the various global motion patterns. As for \(N_{}\), we empirically find that the number is not a very important issue probably because the variety of boundary poses does not significantly affect the semantics. However, directly extending with the raw first frame (shown as "None") will hurt the performance by 1.3%, indicating the importance of boundary pose clustering. We also investigate the parameter sensitivity of \(_{T}\). We find that \(_{T}=0.1\) works well and generally it is not very sensitive (see result in Appendix A4).

**What is learned for augmentation?** In order to better understand what is learned in \(_{L}\) and \(_{N}\), we visualize the clustering results. Fig. 4 visualizes the clustered background poses. We find that the background poses mainly include common rest poses such as standing and sitting with different rotations. These poses do not always appear in all action sequences, so clustering from the whole dataset is reasonable. Fig. 3 visualizes the clustered linear transform matrices. We find that \(_{L}\) mainly learns temporal shifting and reflection operation from the training data, which indicates the presence of approximate symmetric pattern of many complete actions (see matrices in "\(>\)" and "\(<\)" shapes). We can also interestingly conclude that the CropPad transform  actually fits our complete action prior well as it appears in the set of \(_{L}\) transforms (such as the second matrix in Fig. 3). Our

   Action & Avg. acc. across four settings \\  Phone calling & 38.8(**+25.3**) \\ Hand waving & 71.3(**+22.1**) \\ Clapping & 54.7(**+15.0**) \\ Pointing finger & 92.5(**+9.7**) \\ Taking off clothes & 88.0(**+3.1**) \\   

Table 6: Effect of background poses.

   \(N_{}\) & \(N\)\(\)\(E\) & \(N\)\(\)\(P\) & \(E_{A}\)\(\)\(N\) & \(E_{A}\)\(\)\(P\) & Avg. \\  ERM & 54.9 & 70.5 & 42.4 & 49.7 & 54.4 \\
3 & 57.8 & 69.8 & **49.2** & **58.7** & 58.9 \\
5 & 58.2 & 72.4 & 48.9 & 57.2 & 59.2 \\
10 & 58.2 & 73.7 & 48.2 & 56.9 & 59.3 \\
20 & **58.4** & **75.8** & 48.4 & 57.8 & **60.1** \\   

Table 5: Effect of linear transforms.

   Method & \(N\)\(\)\(E\) & \(N\)\(\)\(P\) & \(E_{A}\)\(\)\(N\) & \(E_{A}\)\(\)\(P\) & Avg. \\  ERM & 54.9 & 70.5 & 42.4 & 49.7 & 54.4 \\ \(_{}()\) & **58.6** & 74.3 & 46.3 & 56.1 & 58.8 \\ \(_{}()\) & **58.6** & 73.8 & 46.7 & 55.6 & 58.7 \\  Ours & 58.4 & **75.8** & **48.4** & **57.8** & **60.1** \\   

Table 7: Comparison with other learning variants in our method.

Figure 4: Visualization for the boundary pose clustering result \(\{p_{i}\}\) when \(N_{}=5\). (a) Pose clusters for training set \(N\) and (b) Pose clusters for training set \(E_{A}\).

Figure 5: Examples of some recovered complete actions. The skeletons in **blue** are raw inputs and the skeletons in sky blue are new frames generated by our method.

is more flexible as we can learn a set of possible transforms. Fig. 5 provides examples of recovered complete actions from partial observations. In Fig. 6, we show that with our augmentation we learn a more discriminative feature embedding compared to some other baselines.

**Comparison with other learning variants.** Note that it is also possible to perform infilling and extrapolation for skeleton sequences by training an action completion network. In Table 7 we investigate the performance when learning by neural networks. We compare to the design of learning to extrapolate without conditioning on boundary poses, as well as the design of learning to infill instead of performing linear interpolation. For both variants we adopt the network design in  and more details are provided in Appendix A.3. We find that in terms of average accuracy, both learning to extrapolate (\(_{}\)) and learning to infill (\(_{}\)) are not as effective as our proposed learning-free interpolation, which shows that (1) assigning boundary pose via clustering is necessary as the network itself is difficult to learn automatically; (2) infilling by linear extrapolation is actually a simple but effective design as the infilling quality is good and robust for domain generalization.

**Per-class results.** Table 8 shows the performance improvement for selected action categories. Our method can significantly improve the accuracy on several hand-related actions such as _phone calling_, _hand waving_ and _clapping_. In the case of temporal mismatch, these actions are easily confused with other similar actions if only trained from partial observations. If an action can be well recognized by partial segments, the improvement may be small (e.g. _taking off clothes_).

**Generalizability for different backbones.** We further apply our method on ST-GCN  and CTR-GCN  to see whether various backbones can benefit from the observed complete action prior. Note that CTR-GCN extracts temporal feature in a multi-scale fashion, which is more advanced than AGCN . We observe consistent improvement over ERM, from 50.4 to 55.4 for ST-GCN and from 56.3 to 64.3 for CTR-GCN, as is shown in Table 9. This shows that the domain gap caused by temporal mismatch is difficult to be mitigated by only designing network architecture itself.

## 5 Conclusion

In this paper we present a recover-and-resample augmentation approach to deal with the single domain generalization problem for skeleton action recognition. By exploring the complete action prior, we recover complete actions with learned boundary poses and global linear transforms via clustering. Experiments on a cross-domain setting with three datasets validate our framework. In the future, we plan to investigate more effective resampling approaches, e.g. positional encoding, to further incorporate temporal resampling information into our whole framework.

   Backbone & \(NE\) & \(NP\) & \(E_{A}N\) & \(E_{A}P\) & Avg. \\  ST-GCN  & 51.7 & 67.7 & 37.8 & 44.5 & 50.4 \\ ST-GCN+Ours & 56.7 & 69.7 & 46.1 & 49.0 & 55.4 \\  CTR-GCN  & 54.7 & 72.9 & 46.6 & 50.9 & 56.3 \\ CTR-GCN+Ours & 63.6 & 81.4 & 53.9 & 58.3 & 64.3 \\   

Table 9: Performance of our method on different backbones.

Figure 6: The t-SNE  visualization of the feature embedding for the test set in \(N51\!\!P51\) setting with HCN backbone. The color indicates the groundtruth label. Our method learns a more discriminative feature embedding.