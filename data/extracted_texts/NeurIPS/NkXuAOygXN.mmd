# Test-Time Dynamic Image Fusion

Bing Cao\({}^{1,2}\) Yinan Xia\({}^{1}\) Yi Ding\({}^{1}\) Changqing Zhang\({}^{1,2}\) Qinghua Hu\({}^{1,2}\)

\({}^{1}\)College of Intelligence and Computing, Tianjin University, Tianjin, China

\({}^{2}\)Tianjin Key Lab of Machine Learning, Tianjin, China

{caobing, xyn, ding_yi0731, zhangchangqing, huqinghua}@tju.edu.cn

Corresponding author.

###### Abstract

The inherent challenge of image fusion lies in capturing the correlation of multi-source images and comprehensively integrating effective information from different sources. Most existing techniques fail to perform dynamic image fusion while notably lacking theoretical guarantees, leading to potential deployment risks in this field. _Is it possible to conduct dynamic image fusion with a clear theoretical justification?_ In this paper, we give our solution from a generalization perspective. We proceed to reveal the generalized form of image fusion and derive a new test-time dynamic image fusion paradigm. It provably reduces the upper bound of generalization error. Specifically, we decompose the fused image into multiple components corresponding to its source data. The decomposed components represent the effective information from the source data, thus the gap between them reflects the _Relative Dominability_ (RD) of the uni-source data in constructing the fusion image. Theoretically, we prove that the key to reducing generalization error hinges on the negative correlation between the RD-based fusion weight and the uni-source reconstruction loss. Intuitively, RD dynamically highlights the dominant regions of each source and can be naturally converted to the corresponding fusion weight, achieving robust results. Extensive experiments and discussions with in-depth analysis on multiple benchmarks confirm our findings and superiority. Our code is available at https://github.com/Yinan-Xia/TTD.

## 1 Introduction

Image fusion jointly integrates complementary information from multiple sources, aiming to generate informative and high-quality fused images. With superior scene representation and enhanced visual perception, image fusion significantly benefits downstream vision tasks . Typically, image fusion can be categorized into multi-modal, multi-exposure, and multi-focus image fusion tasks. Multi-modal image fusion encompasses Visible-Infrared image Fusion (VIF) and Medical Image Fusion (MIF). For VIF , infrared images effectively highlight thermal targets especially under extreme conditions, while visible images provide texture details and ambient lighting. For MIF , different medical imaging modalities emphasize various focal areas, enhancing diagnostic capabilities. Multi-exposure image Fusion (MEF)  bridges the gap between high dynamic range (HDR) natural scenes and low dynamic range (LDR) pictures, ensuring better detail preservation in varying lighting conditions. Multi-Focus image Fusion (MFF)  aims to produce all-in-focus images by combining multiple images focused at different depths.

Numerous image fusion methods have been introduced, which can be mainly grouped into traditional techniques and deep learning approaches. Traditional image fusion methods, such as multi-scale decomposition-based models  and sparse representation-based methods , rely on mathematical transformations to fuse images in the transform domain . In contrast, deep learning-basedmethods employ data-driven schemes to fuse multi-source images, including convolutional neural network (CNN) based methods [11; 18], generative adversarial network (GAN) based methods [19; 4], and transformer-based methods . The effectiveness of image fusion algorithms hinges on two critical factors: feature extraction  and feature fusion . The aforementioned methods strive to achieve high-quality fused images by learning effective uni-source or multi-source feature representations through complex network structures or feature decomposition schemes. However, they often overlook the complexity of the real world, which necessitates dynamic feature fusion.

Recently, some works have highlighted the importance of dynamism in image fusion. For instance,  pioneered the combination of image fusion with a Mixture of Experts (MoE), dynamically extracting effective and comprehensive information from the respective modalities.  utilized task-specific routing networks to extract task-specific information from different sources with dynamic adapters. Despite their empirically superior fusion performance, these dynamic fusion rules mainly rely on heuristic designs, lacking theoretical guarantees and interpretability. Moreover, they potentially lead to unstable and unreliable fusion results, especially in complex scenarios.

To address these issues, we reveal the generalized form of image fusion and propose a new Test-Time Dynamic(TTD) image fusion paradigm with a theoretical guarantee. Given that the fused image integrates comprehensive information from different sources, it can be obtained by weighting the effective representation of each uni-source. By revisiting the relationship between fusion weights and image fusion losses from the perspective of generalization error , we decompose the fused image into multiple uni-source components and formulate the generalization error upper bound of image fusion. Based on generalization theory, we for the first time prove that dynamic image fusion is superior to static image fusion. The key to enhancing generalization lies in the negative correlation between fusion weight and uni-source component reconstruction loss. As fusion models are trained to extract complementary information from each source, the decomposed components represent the effective information from the source data. Thus, the fusion components can be estimated by source data with the fusion model, the losses of which represent the deficiencies of the source in constructing fusion images. Accordingly, we derive a pixel-level Relative Dominablity (RD) as the dynamic fusion weight, which theoretically enhances the generalization of the image fusion model and dynamically highlights the changing dominant regions of different sources as shown in Fig. 1. Extensive experiments on multiple datasets and diverse image fusion tasks demonstrate our superiority. Overall, our contributions can be summarized as follows:

* This paper first theoretically proves the superiority of dynamic image fusion over static image fusion and provides the generalization error upper bound of image fusion by decomposing the fusion image into uni-source components provably. The proposed generalization theory reveals that the key to reducing the upper bound lies in the negative covariance between the fusion weight and uni-source reconstruction loss.
* We proposed a simple but effective test-time dynamic fusion paradigm based on the generalization theory. By taking the uni-source's Relative Dominability as the dynamic fusion weight, we theoretically enhance the generalization of the image fusion model and dynamically emphasize the dominant regions of each source. Notably, our method does not require additional training, fine-tuning, and extra parameters.

Figure 1: We visualized the Relative Dominablity (RD) of each source on four tasks, which effectively highlights the dominance of uni-source in image fusion.

* We conduct extensive experiments on multi-modal, multi-exposure, and multi-focus datasets. The superior performance across diverse metrics demonstrates the effectiveness and applicability of our approach. Moreover, an additional exploration of the gradient in constructing fusion weight demonstrates the reasonability of our theory and its expandability.

## 2 Related Works

**Image Fusion** aims to integrate complementary information of diverse source images. For instance,  utilize autoencoders to extract multi-source features and fuse them using a designed strategy. GAN-based methods  and transformer-based methods  also achieved significant progress.  introduced the denoising diffusion probabilistic model (DDPM) to image fusion.  and  achieve considerable fusion performance by decomposing image features into high-frequency and low-frequency components. In addition to these static image fusion methods,  used a Mixture of Experts (MoE) to dynamically assign fusion weights, while  utilized dynamic adapters to prompt various fusion tasks within a unified model. These approaches mainly focus on obtaining promising feature representations. Although some existing works have explored dynamic image fusion, the lack of theoretical guarantees may result in instability and unreliability in practice.

**Multimodal Dynamic Learning** Although dynamic fusion is not fully studied in existing image fusion works, numerous methods have leveraged multimodal dynamic learning at the decision level . For example, Han et al.  assigned dynamic credible fusion weights to each modality at the decision level for robust evidence fusion. Xue et al.  employs a Mixture of Experts to integrate the decisions of multiple experts, Zhang et al.  combined decision level fusion weight with uncertainty to conduct a credible fusion. Despite the wide exploration of dynamic fusion at the decision level, there is still insufficient research on dynamic fusion at the feature level with theoretical guarantees. In this paper, we focus on the dynamic nature of image fusion, theoretically prove that dynamic fusion is superior to static fusion, and propose a provable feature-level dynamic fusion strategy.

## 3 Method

Given the data from \(M\) sources for image fusion, the input samples are denoted as \(\{x=x^{(m)} m=1,2,,M\}\), where \(x^{(m)}\) represents the input from the \(m\)-th source. Let \(f\) be the image fusion model, comprising both encoders and decoders. Define \(E=\{E^{(m)}() m=1,2,,M\}\) as the set of encoders within the image fusion network, where \(E^{(m)}()\) is the encoder for the \(m\)-th source. In early fusion, the encoders in \(E\) are constant mapping functions, meaning that multi-source images are combined at the image level. Let \(D()\) denote the decoder in the image fusion network, and let \(=\{^{(m)}^{H W} m=1,2,,M\}\) represent the set of image fusion weights. Consequently, the fused image \(I_{F}\) can be expressed as:

\[I_{F}=D_{m=1}^{M}^{(m)}E^{(m)}(x^{(m)}).\] (1)

Additionally, we define the loss function for image fusion tasks, where \(\|\|\) represents any distance norm. The loss function is given by:

\[(I_{F},x)=_{m=1}^{M}\|I_{F}-x^{(m)}\|.\] (2)

**Generalization Error Upper Bound.** In machine learning, the concept of Generalization Error Upper Bound refers to the theoretical limit on a model's performance when applied to unseen data (\(\)) . A smaller upper bound indicates better-expected performance on data from an unknown distribution. For image fusion tasks, the Generalization Error (GError) of a fusion model \(f\) can be defined as:

\[(f)=_{x}[(f(x),x)].\] (3)Considering the GError of image fusion model [35; 36; 37], \((I_{F},x)\) can be further deduced as \((I_{F},x)_{m=1}^{M}\|_{i=1}^{M}^{(i)} D(E^{(i)}(x^{(i )}))-x^{(m)}\|\). Therefore, the fused image \(I_{F}\) is decomposed into \(M\) uni-source components \(\{D(E^{(i)}(x^{(i)}))|i=1..M\}\). Based on Eq. (3) and (8), we have:

**Theorem 3.1**: _(Decomposition of Generalization Error). The GError for multi-source image fusion model f can be decomposed into a linear combination of each uni-source component reconstruction loss under the condition that \(_{m=1}^{M}^{(m)}=1\), the detailed proof is given in Appendix A.1:_

\[(f) =_{x}[_{m=1}^{M}\|_{i=1}^{M}D( ^{(i)} E^{(i)}(x^{(i)}))-x^{(m)}\|]\] \[_{m=1}^{M}(2M-1)\|D(E^{(m)}(x ^{(m)}))-x^{(m)}\|+(M-1)_{i m}^{M}\|D(E^{(i)}(x^{(i)}))-x^{(m)}\|\] \[+_{m=1}^{M}Cov^{(m)},(x^{(m)}))-x^{(m)}\|}_{}-x^{(m)}\|.\] (4)

Let \(^{(m)}=\|D(E^{(m)}(x^{(m)}))-x^{(m)}\|\), which represents the reconstruction loss between a uni-source component and its corresponding uni-source image. The term \(Cov(^{(m)},^{(m)})\) denotes the covariance between \(^{(m)}\) and \(^{(m)}\). The essence of reducing generalization error lies in achieving the lowest possible fusion loss. By leveraging the triangular inequality properties of distance norms within the fusion loss, we can deduce that the GError is bounded by the covariance term and the distance between each uni-source component and its source image. It is noteworthy that \(f(x^{(m)})\) remains constant during the test phase, emphasizing that the pivotal factor in reducing Generalization Error Upper Bound (GEB) lies in the covariance between \(^{(m)}\) and \(^{(m)}\).

**Superiority of Dynamic Image Fusion over Static Image Fusion.** Most existing image fusion approaches reduce GEB by minimizing \(^{(m)}\), indicating an effort to enhance the quality of uni-source feature representations. However, they often overlook the intrinsic significance of fusion weight \(^{(m)}\). Fusion strategies employed in static image fusion encompass methods such as maximum, minimum, addition, \(_{1}\)-norm, etc. Nevertheless, none of these fusion weights exhibit a correlation with uni-source reconstruction loss, i.e. \(Cov(^{(m)}_{static},^{(m)})=0\). During the test phase, \(^{(m)}\) remains constant. If we have: \(Cov(^{(m)}_{dynamic},^{(m)})<0\) for all source images, we can derive the conclusion:

\[_{dynamic}<_{static}.\] (5)

Figure 2: The framework of our TTD. Deriving from the generalization theory, we decompose fused images into uni-source components and find the key to reducing generalization error upper bound is the negative correlation between the fusion weight and reconstruction loss. Accordingly, we propose pixel-wise Relative Dominablity (RD) for each source, which is negatively correlation with the reconstruction loss and highlights the dominant regions of uni-source in constructing fusion images.

This indicates that for a well-trained image fusion model, a dynamic fusion strategy can bring better generalization than a static fusion strategy.

**Relative Dominablity.** Recalling the Eq. (4), the negative correlation between fusion weight and the reconstruction loss \(^{(m)}\) provably reduces the generalization error upper bound. Therefore, we introduce a pixel-level Relative Dominablity (RD) as the fusion weight for each source, which exhibits a negative correlation with the reconstruction loss of the corresponding fusion component. Since fusion models are trained to extract complementary information from each source, the decomposed components of fusion images represent the effective information from the source data. Thus, the uni-source components can be estimated from source data using the fusion model, with the losses representing the deficiencies of the source in constructing fusion images. For instance, in a given region, the larger the pixel-wise fusion loss between the reconstructed component and its corresponding uni-source image, the smaller its contribution to image fusion. Intuitively, using RD as the dynamic fusion weight can capture the dominance of each source in image fusion and enhance its advantages in constructing fusion images. Theoretically, according to Thm. 3.1, negatively correlated with the pixel-wise fusion loss, RD effectively demonstrates the dominance of each source. Notably, considering the relative nature of multi-source image fusion, the sum of the RDs of different sources for the same pixel should be one. Consequently, by establishing a negative correlation with the loss and implementing normalization, we can obtain the Relative Dominablity of each source for a certain sample as follows:

\[^{(m)}=^{(m)}=Softmax(e^{-^{(m)}}).\] (6)

In addition, we present the algorithm and test pipeline of our dynamic fusion strategy in Appendix B.1.

## 4 Experiments

### Experimental Setup

**Datasets.** We evaluate our proposed method on four image fusion tasks: Visible-Infrared Fusion (VIF), Medical Image Fusion (MIF), Multi-Exposure Fusion (MEF), and Multi-Focus Fusion (MFF). \(\) VIF: For VIF tasks, we conduct experiments on two datasets: LLVIP  and MSRS . For LLVIP datasets, we randomly select 70 samples from the test set for evaluation. \(\) MIF: We conduct experiments on the Harvard Medical Image Dataset, following the test setting in . \(\) MEF: Following the setting in , we verified the performance of our method on MEFB  dataset. \(\) MFF: For the MFF task, we evaluate our method on MFI-WHU datasets , following the test protocol in . As a test-time adaption approach, TTD performs adaptive fusion solely during testing, without additional training and training data.

**Competing Methods.** For VIF and MIF tasks, we evaluated 12 state-of-the-art methods, encompassing both DenseFuse , CDDFuse , U2Fusion , DDFM , DeFusion , PIAFusion , DIVFusion , MUFusion , IFCNN , and SwinFuse , and TC-MoA . For MEF and MFF tasks, we compared our methods with general image fusion methods and task-specific image fusion methods. Notably, among these methods, only DDFM is training-free, and other methods are all pre-trained on VIF datasets. In experiments, we apply our TTD to CDDFuse (CDDFusion+TTD), PIAFusion (PIAFusion+TTD), and IFCNN (IFCNN+TTD), separately. Our experiments are conducted on Huawei Atlas 800 Training Server with CANN and NVIDIA RTX A6000 GPU.

**Metrics.** We selected several evaluation metrics from three aspects , including \(\)_information theory_: entropy (EN) , cross entropy (CE), the sum of the correlations of differences (SCD) , \(\)_image feature_: standard deviation (SD), average gradient (AG) , edge intensity (EI) and spatial frequency (SF) , and \(\)_structural similarity_: structural similarity (SSIM) .

### Quantitative Comparisons

**Visible-Infrared Fusion.** Tab. 1 reports the performance of competing approaches and TTD-applied methods on LLVIP and MSRS datasets for 7 metrics. Notably, by applying our TTD, the previous methods have improved on most of the indicators. Also, our TTD strategy outperforms other traditional static methods, training-free method DDFM, and data-driven dynamic strategy TC-MoA, achieving the SoTA performance on most metrics. Moreover, with particularly high values in SD, AG, EI, and SF, our TTD ensures that fusion images maintain exceptional contrast and detailed texture, highlighting its efficacy in preserving quality. The outstanding performance on EN and SCD indicates

[MISSING_PAGE_FAIL:6]

### Qualitative Comparisons

**Visible-Infrared Fusion.** As shown in Fig. 3 and Fig. 8 in Appendix C.1, compared with existing methods on the LLVIP dataset, our TTD effectively combines comprehensive information from different sources, leading to a significant visual performance. Specifically, the fusion result not only preserves the texture details and edge information of visible images but also incorporates high-quality thermal imaging contrast of infrared images. Additionally, as mentioned in the qualitative analysis, our fusion images exhibit high fidelity and clear contrasts, showing consistent superiority in terms of image quality. These experimental results demonstrate the effectiveness of our TTD.

**Medical Image Fusion.** For the MIF task, we present qualitative comparisons of the MRI-PET fusion. As shown in Fig. 3 and Fig. 8 in Appendix C.1, it is clear that fusion images generated by our method preserve a substantial amount of structural information. Notably, our method maintains a significant portion of excellent soft tissue contrast details from MRI images and combines the quantitative physiologic information of PET images. With our TTD, the overall structural details and sharpness of the fused image are significantly enhanced. Moreover, in the regions where the high-intensity color areas of the PET image overlap with the structural information of the MRI, the detailed information from the original images is well preserved and highlighted in the fused image.

Figure 4: The comparison of fusion results on MEF and MFF tasks. (a) On the MFF task, our method retains the color and clarity of the original image better. (b) On the MEF task, our TTD ensures better detail preservation in varying lighting conditions.

Figure 3: (a) On the VIF task, our TTD produces fused images that retain more multi-source information compared with existing approaches. (b) On the MIF task, our method improves the contrast of the fused image and preserves more details from the source image.

**Multi-Exposure and Multi-Focus Image Fusion.** We also provide a comparison of fusion results for the MEF and MFF tasks in Fig. 4 and Fig. 8 in Appendix C.1. Notably, our TTD significantly enhances the clarity and sharpness of texture details. Obviously, after applying our TTD, the fused images on the MEF task exhibit higher clarity in both the foreground and background. For the MFF task, our method accurately utilizes the effective regions from both underexposed and overexposed images. Compared to other methods, our fusion results achieve more precise exposure and rich texture details, such as the cars in the garage and the textures on the walls. Additionally, our fusion method retains high-fidelity colors that are closer to the original images.

Apart from the comparisons with the existing methods, we provided more ablated comparisons with baselines in Fig. 9 of Appendix C.2.

## 5 Discussion

### Is Negative Correlation Help?

The negative correlation between RD and \(^{(m)}\) is derived from Eq. (4) to reduce the generalization error of the image fusion model. To further validate the effectiveness of the theoretical guarantee, we compare it with a contrast setting: using a new fusion weight, which is positively correlated (PC) to \(^{(m)}\), to perform image fusion.

As shown in the correlation comparison in Fig. 5 (b), loss-positive-correlated weights (yellow line), that conflict with our theory, lead to a decreased performance compared with static fusion (green line). As a comparison, the results of the loss-negative-correlated fusion strategy (red line), exhibit superior performance compared with both static image fusion and positively correlated fusion strategy. These experiments verify that the proposed negative correlation setting can explicitly reduce the generalization error, demonstrating the reasonability of the proposed TTD image fusion model.

### Relative Dominability

In this paper, we introduce the pixel-level Relative Dominablity, which indicates the advantages of each source. Treating the Relative Dominablity as dynamic fusion weight, TTD achieves an adaptive and interpretable image fusion. We provide visualizations of each source's pixel-level Relative Dominability obtained using CDD+TTD for VIF and MIF tasks, and IFCNN+TTD for MEF and MFF tasks.

**Visible-Infrared Fusion.** As shown in Fig. 1, it can be observed that Relative Dominablity (RD) accurately reflects the dominance of each source: in visible images, well-lit and properly exposed bright areas contain abundant brightness information, and areas like digits and characters exhibit rich texture details. In contrast, infrared images provide thermal imaging information for areas and objects in shadow that visible images cannot capture due to visual obstacles. The proposed RD effectively captures the advantageous regions of different source images and assigns larger weights to these regions, thereby achieving more reliable fusion results.

**Medical Image Fusion.** We visualize RDs in the MRI-PET dataset. Similar findings are also apparent in the MIF task. In PET images, bright regions indicate areas of malignant cell proliferation, while MRI contains more structural information. As shown in Fig. 1, the RDs of PET stand out in the bright areas while MRI's highlights the structural information. Guided by RD, TTD emphasizes potential lesion areas while preserving the structural information of these areas effectively.

**Multi-Exposure and Multi-Focus Image Fusion.** For MEF and MFF tasks, the ideal outcome is that the fused images contain properly exposed or precisely focused regions from each uni-source. As shown in the visualized RD map in Fig. 1, our TTD can effectively capture the dominant areas in different sources and assign higher RD values, i.e. dynamic fusion weight, to these regions.

**Downstream Tasks.** To validate the effectiveness of our RD on downstream tasks, we compared our TTD with the baseline on an object detection task. Detailed results are given in Appendix C.5.

Overall, with the integration of our TTD, the baseline model gains the ability to perceive dominant information dynamically. Therefore, this interpretable plug-and-play test-time dynamic adaptation fusion strategy can further improve the performance of the existing state-of-the-art methods. This further validates the effectiveness of RD in our TTD.

### Gradient-based Relative Dominability

In our TTD, the proposed pixel-level fusion weight is computed by pixel-level loss. However, some numeric losses are limited in directly obtaining the pixel-level weights, making it hard to integrate with TTD flexibly. To overcome this dilemma, we extend our TTD to a more general form and construct gradient-based Relative Dominability through any fusion losses for a more fine-grained and robust image fusion.

Recalling our optimization objective of the generalization error bound, we aim for a negative correlation between the weights and losses of the same modality, i.e., establishing a correlation between losses and weights. Inspired by this positive correlation between loss value and the absolute value of its gradient for features with any convex loss function, our TTD can be further extended to a gradient-based dynamic fusion weight.

Specifically, we first calculate the absolute value of gradients \(|G^{(m)}|^{H W C}\) of each uni-source feature. As a test-time adaption approach, TTD does not update the network parameters, meaning that the unimodal feature space remains fixed for the same baseline. For the same task scenarios, the feature patterns tend to be similar. Therefore, we can empirically select the gradient channels that well represent the advantage areas to compute RDs. Also, as illustrated in Fig. 5 (a), gradient maps of some channels (such as the 44th and the 13th channels) lack significant useful information and fail to capture advantageous regions in the original images. Therefore, we select the gradient map \(|g^{(m)}|^{H W}\) among \(C\) channels that best represent the dominance of the uni-source empirically. By replacing \(^{(m)}\), we can obtain the RD and the dynamic fusion weight as follows:

\[^{(m)}=^{(m)}=Softmax(e^{-|g|^{(m)}}).\] (7)

We have also conducted comparisons of loss-based TTD, full gradient-based TTD, and best gradient-based TTD on IFCNN. For the best gradient-based TTD, we choose the 58-th gradient map to obtain the fusion weight. The results of gradient-based RD in Fig. 5 (b) demonstrate that full gradient (yellow line) may bring wrong or useless dominance information to fusion weights, leading to worse performance compared with loss-based TTD (green line). However, by selecting the empirically best gradient map (red line), our TTD provides more fine-grained dominance information compared to the global loss maps, achieving more detailed dynamic fusion with better performance.

Figure 5: (a) The visualization of RDs obtained by gradient maps of different channels. The 44th gradient map provides wrong dominance information, and the 13th gradient map offers insignificant information, while the 58th gradient map performs the proper advantages of the two source images. (b) The radar chart of the gradient-based RD experiment (upper) and the validation of the negative correlation (below).

Conclusion

Image fusion aims to integrate effective information from multiple sources. Despite numerous methods being proposed, research on dynamic fusion and its theoretical guarantees remains significantly lacking. To address these issues, we derive from a generalized form of image fusion and introduce a new Test-Time Dynamic (TTD) image fusion paradigm with a theoretical guarantee. From the perspective of generalization error, we reveal that reducing generalization error hinges on the negative correlation between the fusion weight and the uni-source component reconstruction loss. Here the uni-source components are decomposed from fusion images, reflecting the effective information of the corresponding source image in constructing fusion images. Accordingly, we propose a pixel-level Relative Dominablity (RD) as the dynamic fusion weight, which theoretically enhances the generalization of the image fusion model and dynamically highlights the changing dominant regions of different sources. Comprehensive experiments with in-depth analysis validate our superiority. We believe the proposed TTD paradigm is an inspirational development that can benefit the community and address the theoretical gap in image fusion research.