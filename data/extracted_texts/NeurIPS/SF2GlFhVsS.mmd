# Proposit.

Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization

Xinyu Lyu

Eeitao Chen

Equal contribution.

Lianli Gao

Corresponding author.

Jingkuan Song

Shenzhenzhen Institute for Advanced Study,

University of Electronic Science and Technology of China

Heng Tao Shen

Shenhenhengtao@hotmail.com

Shenhenhengtao@hotmail.com

Sohnwestern University of Finance and Economics, Chengdu, China

###### Abstract

Although Large Visual Language Models (LVLMs) have demonstrated exceptional abilities in understanding multimodal data, they invariably suffer from hallucinations, leading to a disconnection between the generated text and the corresponding images. Almost all current visual contrastive decoding methods attempt to mitigate these hallucinations by introducing visual uncertainty information that appropriately widens the contrastive logits gap between hallucinatory and targeted ones. However, due to uncontrollable nature of the global visual uncertainty, they struggle to precisely induce the hallucinatory tokens, which severely limits their effectiveness in mitigating hallucinations and may even lead to the generation of undesired hallucinations. To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding. Building on this insight, we introduce a novel optimization strategy named Hallucination-Induced Optimization (HIO). This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs. Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks. Code is released at https://github.com/BT-C/HIO.

## 1 Introduction

The recent success of Large Vision-Language Models (LVLMs) marks a major milestone in artificial intelligence research . By seamlessly integrating visual cues with Large Language Models (LLMs), LVLMs have demonstrated unparalleled expertise in multimodal comprehension, logical reasoning, and interactive engagement. Thisintegration has ushered in a new era in AI, breaking through traditional limitations and enabling a more holistic understanding of complex information OpenAI (2023); Yang et al. (2023); Lu et al. (2023); Yuan et al. (2022); Sun et al. (2024). Despite these advancements, certain challenges remain, particularly the issue of hallucination Li et al. (2023); Gunjal et al. (2023); Liu et al. (2023); Lovenia et al. (2023). Hallucination occurs when the language model generates content that deviates from the image's actual content, including imagined objects, fabricated scenes, incorrect spatial relationships, and misidentified categories.

Substantial research efforts have been directed towards mitigating hallucinations in Large Vision-Language Models (LVLMs). These efforts include post-hoc correction methods that refine LVLM outputs after the fact Zhou et al. (2023) and self-correcting frameworks specifically designed to reduce object hallucinations Yin et al. (2023). Additionally, numerous decoding strategies have been developed to minimize hallucinations through the enhanced use of textual and visual priors Leng et al. (2023); Zhang et al. (2024); Favero et al. (2024); Zhu et al. (2024); Wang et al. (2024); Chen et al. (2024). These methods aim to alleviate hallucinatory tendencies by integrating visual uncertainty, thereby increasing the contrastive disparity between hallucinatory and target logits. For example, Leng et al. (2023) augment the hallucinatory effect by introducing Gaussian noise into the images. Similar approaches by Zhang et al. (2024) and Favero et al. (2024) introduce substantial image noise, effectively reducing the original image to pure noise or unrecognizable content. Zhu et al. (2024) use instructional bias to enable the model to amplify its own hallucinations, while Wang et al. (2024) focus on deliberately amplifying the inherent image bias in LVLMs.

However, the inherent uncontrollable nature of global visual uncertainty challenges the precise induction of hallucinatory tokens. This limitation significantly undermines the effectiveness of these methods in reducing hallucinations and may inadvertently lead to undesired hallucinatory outputs. As shown in the left portion of the Fig. 1 _Spoon_, _Table_, and _Fork_ are identified as hallucinated words, while _People_ being the accurate term. For Greedy Decoding method shown in Fig. 1 (a), _Table_ is selected as the final output based on the logits distribution. Moreover, although Visual Contrastive Decoding introduces perturbations to images to enhance hallucinations in Fig. 1 (b), it fails to widen the logits gaps between hallucinatory (_Spoon_, _Table_, and _Fork_) and targeted tokens (_People_), yielding a new hallucination as _Fork_.

To tackle this issue, we conducted the theoretical analysis to explore mechanisms for more effective contrast decoding (refer to Section 5 for detailed information on the process). Theoretically, a clear distinction between hallucinatory and target tokens can significantly enhance the effectiveness of contrast decoding methods in mitigating hallucinations. Based on this crucial insight, we introduce a novel optimization strategy called Hallucination-Induced Optimization (HIO). This strategy enhances the distinction between hallucinatory and targeted tokens by utilizing a refined theoretical preference model(as shown in the Fig. 1 on the left, section (c)), accurately outputting the correct result, _People_.

Figure 1: (**Left) Challenges and Solutions of Contrast Decoding Strategy. Visual Contrastive Decoding, despite introducing perturbations to induce hallucinations, fails to effectively enlarge the logits gap between hallucinatory and targeted tokens, resulting in unsatisfactory outputs. On the contrary, our method addresses the issue by significantly amplifying the logits gap between hallucinatory and targeted tokens. (**Right) The performance of various methods on CHAIR metrics.** Our HIO generates descriptions with fewer hallucination tokens compared to other visual contrastive decoding methods, achieving lower scores on the CHAIRs and CHAIR metrics.

Consequently, this improves the efficiency of contrast decoding, thereby mitigating hallucinations in Large Vision-Language Models (LVLMs). Furthermore, our proposed method significantly reduces hallucinations in LVLMs compared to existing contrast decoding methods(as shown in the Fig. 1 on the right). To sum up, our main contributions are as follows:

1. We conducted a comprehensive theoretical analysis to explore mechanisms that enhance the effectiveness of the contrast decoding strategy.
2. We introduce Hallucination-Induced Optimization (HIO), an innovative strategy that utilizes a finely-tuned theoretical preference model to intensify the contrast between hallucinatory and target tokens. This enhancement strengthens the effectiveness of contrast decoding and effectively reduces hallucinations in Large Visual Language Models (LVLMs).
3. Extensive experimental research demonstrates that our Hallucination-Induced Optimization (HIO) strategy effectively reduces hallucinations in Large Visual Language Models (LVLMs), surpassing state-of-the-art methods across various benchmarks.

## 2 Related Work

Hallucination in LVLMs.Before the advent of Large Language Models (LLMs), "hallucination" in natural language processing (NLP) primarily referred to generating nonsensical or source-deviating content Lee et al. (2018); Zhou et al. (2020); Lin et al. (2021); Ji et al. (2023); Zhang et al. (2023); Shi et al. (2023). Recent studies have tackled the complexities of object hallucination in Large Vision-Language Models (LVLMs), focusing on evaluation and detection methods Wang et al. (2023); Liu et al. (2023); Li et al. (2023); Lovenia et al. (2023). The CHAIR metric Rohrbach et al. (2018) evaluates the exact match between generated and ground-truth image captions, while POPE Li et al. (2023) assesses the model's awareness of object existence through binary classification.

Decoding Method.The decoding method determines the generation of text tokens at each time step within language models. Traditional decoding strategies such as beam search Boulanger-Lewandowski et al. (2013), top-k decoding Fan et al. (2018), and sampling methods Holtzman et al. (2019), despite their widespread use, are prone to producing hallucinatory content. Recent research Li et al. (2022); Chuang et al. (2023); Leng et al. (2023); Huang et al. (2023) has made attempts to address this issue by proposing better decoding methods. For instance, Leng et al. (2023) uses contrastive decoding in LVLMs; However, global visual uncertainty poses challenges to the precise induction of hallucinatory tokens, limiting the effectiveness of mitigation strategies and risking unwanted hallucinations. To address this, we developed Hallucination-Induced Optimization (HIO), a novel strategy that enhances the contrast between hallucinatory and targeted tokens. Fig.1 presents the comparison results, where our approach demonstrates superior performance than other decoding methods.

## 3 Preliminaries

We first review the Contrast Decoding pipeline in Leng et al. (2023) (and later Zhang et al. (2024); Favero et al. (2024)). Then take a close look at the Bradley-Terry model Bradley and Terry (1952) and its application such as Direct Preference Optimization Rafailov et al. (2024). Inspired by these studies, we propose our Hallucination-Induced Optimization.

**Visual Contrastive Decoding.** We consider an LVLM parameterized by \(\). The model takes a textual query input \(x\) and a visual input \(v\), where \(v\) provides contextual visual information to assist the model in generating a relevant response \(y\) to the textual query. The response \(y\) is sampled auto-regressively from the probability distribution conditioned on the query \(x\) and the visual context \(v\). Mathematically, this can be formulated as:

\[y_{t} p_{}(y_{t} v,x,y_{<t})_{}(y_{t} v,x,y_{<t})\] (1)

where \(y_{t}\) denotes the token at time step \(t\), and \(y_{<t}\) represents the sequence of generated tokens up to the time step \(t-1\). Specifically, given a textual query \(x\) and a visual input \(v\), the model generates two distinct output distributions: one conditioned on the original \(v\) and the other on the distorted visual input \(v^{}\), which is derived by applying pre-defined distortions (i.e., Gaussian noise mask) to the original \(v\). Then, a new contrastive probability distribution is computed by exploiting the differencesbetween the two initially obtained distributions. The new contrastive distribution \(p_{vcd}\) is formulated as:

\[p_{vcd}(y v,v^{},x)=[(1+)\,_{}(y v,x)-\,_{}(y v^{ },x)]\] (2)

where larger value of \(\) indicate a stronger amplification of differences between the two distributions (\(=0\) reduces to regular decoding).

**Direct Preference Optimization.** Reinforcement learning (RL) effectively fine-tunes Large Language Models (LLMs) to align with human behavior. Given an input \(x\) and a response \(y\), a language model policy \(_{}\) generates a conditional distribution \(_{}(y x)\). RL aims to maximize the average reward of outputs, with the reward function \(r(x,y)\). To prevent _overoptimization_Gao et al. (2023), the objective loss includes a KL-divergence term, controlling the divergence between the language model policy and its reference policy \(_{}(y x)\), typically derived from supervised fine-tuning. Thus, the overall objective is formulated as:

\[_{_{}}_{x,y_{}(y x)} r(x,y)-(y x)}{_{}(y x )}\] (3)

where \(\) is a dataset of prompts and \(\) is a coefficient to control KL-divergence term. However, optimizing the above loss term with common strategies like proximal policy optimization (PPO) Schulman et al. (2017) is complex to tune. Recently, direct preference optimization (DPO) Rafailov et al. (2024) simplifies the above process by keeping preference data for optimization. Here, the preference data is defined as \(=\{x^{(i)},y_{w}^{(i)},y_{l}^{(i)}\}_{i=1}^{N}\), where \(y_{w}^{(i)}\) and \(y_{l}^{(i)}\) represent preferred and dispreferred responses given an input prompt \(x\). These are then presented to human labelers who express preferences for one answer, denoted as \(y_{w} y_{l} x\) where \(y_{w}\) and \(y_{l}\) denote the preferred and dispreferred respectively. Following a Bradley-Terry model (Bradley and Terry, 1952), the probability of obtaining each preference pair is:

\[p(y_{w} y_{l} x)=))}{(r(x,y_ {w}))+(r(x,y_{l}))}.\] (4)

where the superscript \(i\) is omitted for simplicity. In DPO, the optimization of Eqn. (3) can be formulated as classification loss over the preference data as:

\[_{DPO}(_{};_{})=-_{(x,y_{w},y_{l })}[((y_{w}|x) }{_{}(y_{w}|x)}-(y_{l}|x)}{_{ }(y_{l}|x)})].\] (5)

DPO enables learning \(_{}\) from a fixed dataset of preferences, which is lightweight. However, the challenge arises because the direct application of DPO does not reliably induce hallucinations in a manner that meets the criteria specified in Eqn. (17).

## 4 Method

An overview of the proposed HIO method is shown in Fig. 2. It constructs a more-hallucinated LVLM by inducing hallucinations from the original LVLM to amplify the contrast between hallucinatory and targeted tokens, thereby enhancing the efficiency of contrast decoding and mitigating hallucinations in LVLMs. In Section 4.1, we harness a fine-tuned theoretical preference model to amplify the contrast between hallucinatory and targeted tokens. Furthermore, to induce more potential hallucinations for effective contrast decoding, we propose to amplify multiple hallucination tokens based on a theoretical foundation presented in Eqn. 17 of Section 5. This theory demonstrates that effective contrastive decoding requires a consistent difference between the logits of potential hallucinated tokens and the correct token. And Section 4.3 introduces additional constraints to overcome the limitations of existing classification loss in amplifying the contrast between hallucinatory and targeted tokens.

### Contrary Bradley-Terry Model (CBTM)

We harness a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model (Bradley and Terry, 1952)) to amplify the contrast between hallucinatory and targeted tokens. The studies on hallucination mitigation Zhao et al. (2023); Yu et al. (2023); Zhou et al. (2024) utilize BT model by defining the non-hallucinatory output as \(y_{w}\) and the hallucinatory output as \(y_{l}\). Subsequently, they employ BT model training to incentivize the model to prioritize outputs without hallucinations over those containing them.

However, within the context of contrast decoding, inducing hallucinations is crucial, and the resulting model output must satisfy the criteria outlined in Eqn. (17). (The detailed derivation of this formula is provided in Section5). To meet the requirements specified in Eqn. (17), the logits associated with hallucinated tokens \(}^{\{v,x,y_{<t}\}}\) need amplification, while at least one of the logits for the correct token \(}^{\{v,x,y_{<t}\}}\) must be reduced. In contrast to the prevailing research efforts focused on alleviating hallucinations, our approach enables the model to learn to fit the distribution containing hallucinations while avoiding convergence with the distribution of correct outputs. The details are outlined as follows. To regulate \(}^{\{v,x,y_{<t}\}}\) and \(}^{\{v,x,y_{<t}\}}\), we utilize the dataset introduced by Yu et al. (2023). This dataset is notable for providing a pair of outputs per input, with the output paragraphs being mostly identical except for differences in certain words or short phrases. By leveraging this dataset, we approximate the conditions outlined in Eqn. (17) within a unified statement. Different from Eqn. (5), we apply the Bradley-Terry (BT) Bradley and Terry (1952) model in a reversed way, the objective is:

\[ p(y_{l} y_{w} x)&=))}{(r(x,y_{l}))+(r(x,y_{w}) )}\\ &=((y_{l}|v,x)}{_{ {ref}}(y_{l}|v,x)}-(y_{w}|v,x)}{_{}(y _{w}|v,x)}).\] (6)

where \(()\) is defined as a sigmoid function and the reference model \(_{}(y|x)\) is usually implemented by an instruction-tuned base model we want to improve, and is kept fixed during DPO training. Only the policy model \(_{}(y|x)\) is updated.

### Amplification of Multiple Targeted Hallucination (AMTH)

The methodology delineated in Eqn. (6), along with the conventional application of Direct Preference Optimization (DPO) for mitigating hallucinations, is limited to highlight the difference between a single hallucination token and the target token. Consequently, these approaches fall short in enhancing

Figure 2: **An overview of Hallucination-Induced Optimization (HIO). Our approach comprises two phases: the training stage and inference decoding. During the training stage, given an input image, a query, and a manually annotated correction, the Large Visual Language Model (LVLM) produces multiple instances of hallucinated content. We then apply our Hallucination-Induced Optimization (HIO) method to train an ‘Evil’ LVLM by inducing hallucinations from the original LVLM. In the inference phase, the logits from the trained ‘Evil’ LVLM are used to contrast with those generated by the original LVLM, effectively reducing the presence of hallucinations.**

the distinctions among other hallucinations relative to the target tokens, which is critical as shown in Eqn. (17). In this section, we will explain how to amplify the differences between multiple hallucination tokens and target tokens through modifications at both the loss function and data levels.

**Multiple Hallucination-Induced Optimization.** Achieving the desired distribution through single positive and negative sample fitting preference training is not feasible, leading conventional Direct Preference Optimization (DPO) applications Zhao et al. (2023); Yu et al. (2023); Zhou et al. (2024) to overlook a significant number of hallucinations. Thus, drawing inspiration from the implications of Eqn. (17), our approach strategically induces multiple hallucinations to increase the probability of producing a correct word in the output. As demonstrated in Eqn. (17), effective contrast decoding necessitates not only the amplification of one hallucination but also the consideration of a diverse set of potential hallucinations. We propose the simultaneous fitting of multiple pairs of preference data when modeling distributions for the same input preference, treating all pairs of preference data with equal importance. Based on Eqn. (6), we apply the Bradley-Terry (BT) (Bradley and Terry, 1952) model in a multi-pair way, the objective is:

\[_{i=1}^{k}p(y_{l} y_{w} x) =_{i=1}^{k}))}{(r(x, y_{li}))+(r(x,y_{w}))}\] (7) \[=_{i=1}^{k}((y_{li}|x) }{_{}(y_{li}|x)}-(y_{w}|x)}{_{ }(y_{w}|x)}).\]

where \(\{y_{li}\},i\{1,2,,k\}\) represent the multiple potential hallucination tokens. Assuming access to a static dataset of comparisons \(=\{x^{(i)},y_{w}^{(i)}\,,\{y_{li}^{(i)}\}\}_{i=1}^{N}\) sampled from \(p\), we can parametrize a reward model \(r(x,y)\) and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss:

\[_{}(_{};_{}) =-_{(x,y_{l},y_{w}) D}_{i= 1}^{k}p(y_{l} y_{w} x)\] (8) \[=-_{(x,y_{l},y_{w}) D}_{i=1}^{k} (y_{li}|v,x)}{_{}(y_{li} |v,x)}-(y_{w}|v,x)}{_{}(y_{w}|v,x)} \] (9)

**Acquisition of Multiple Candidate hallucinations.** While numerous hallucination datasets exist Yu et al. (2023); Zhao et al. (2023); Zhou et al. (2024), they are either generated by GPT or manually rewritten, and thus do not accurately represent the model's potential for multiple hallucinations. Therefore, we propose a novel approach: allowing the model to directly output tokens with high confidence as negative samples. While this approach may incorrectly classify some correct tokens as hallucinations, it compensates by providing true value-labeled data for correction and supplementation. Consequently, this method effectively amplifies multiple hallucinations while reducing the target token. The detailed training process of our method is outlined in Algorithm 1.

### Advanced Constraints for Inducing (ACI)

To overcome the limitations of existing classification loss in amplifying the contrast between hallucinatory and targeted tokens, we introduces additional constraints. The preference optimization strategy outlined in Eqn. (8) allows the model to accommodate a specific range of preference distributions through the cross-entropy in the classification loss function. The precise formulation is as follows:

\[_{}(y_{l}|v,x)=_{t=1}^{m}}}\,\{v,x,y_{<t} \}}{_{j}^{N}}^{\,\{v,x,y_{<t}\}}},\{k_{t}\} y_{l},t=\{1, 2,,m\}\] (10)

where \(m\) represents the length of the sentence \(y_{l}\) and \(\{k_{T}\}\) is token of each word, and the definition of \(}^{\,\{v,x,y_{<t}\}}\) is shown in Section 5. While the use of cross-entropy to minimize encoding length helps the model align with the desired output sentence, it does not consistently ensure that the logits of induced hallucinations meet the conditions specified in Eqn. (17).

For example, the goal of Eqn. (8) is to increase \(_{}(y_{l}|v,x)\), but both increasing \(}^{{}^{}}\{v,x,y_{<t}\}}\) or decreasing \(_{j}^{N}^{{}^{}}\{v,x,y_{<t}\}}\) can achieve this goal. Meanwhile, decreasing the value of \(_{j}^{N}^{{}^{}}\{v,x,y_{<t}\}}\) can also allow \(_{}(y_{w}|v,x)\) to meet the optimization criteria. As shown in Fig. 3, the blue curve, representing the disparity between the logits of the hallucinatory and targeted tokens, typically exhibits a positive trend. Nevertheless, it's important to note occasional segments where this value dips below zero. To tackle this issue, we further add restrictions based on Eqn. (8):

\[_{}(_{};_{}) =-_{(x,y_{l},y_{w}) D}_{i=1}^{k} (y_{li}|v,x)}{_{}(y_{li }|v,x)}-(y_{w}|v,x)}{_{}(y_{w}|v,x)} \] (11) \[+_{t=1}^{m}}^{{}^{ }}\{v,x,y_{<t}\}}-^{{}^{}}\{v,x,y_{<t}\}}\]

By implementing this constraint, the model can be fitted to the distribution of preference statements, thereby further expanding the difference between hallucination tokens and target tokens.

## 5 Fundamental Conditions for Contrast Decoding

Contrast decoding is capable of mitigating hallucinations when specific conditions are met. This section delves into a comprehensive discussion and analysis of these conditions.

**Definition.** Let \(^{\{v,x,y_{<t}\}}}\) represent the probability of the \(i\)-th token in the model's vocabulary given the query \(x\), the visual context \(v\) and the sequence of generated tokens up to the time step (\(t-1\)). The logits can be formulated as:

\[_{}(y_{t} v,x,y_{<t})=L^{\{v,x,y_{< t}\}}=(l_{1}^{\{v,x,y_{<t}\}},l_{2}^{\{v,x,y_{<t}\}},,l_{N}^{\{v,x,y_{<t}\}})\] (12)

where \(N\) denotes the vocabulary length.

**Definition.** Let \(^{\{v,x,y_{<t}\}}\) represents the ideal logits for contrast decoding, \(L^{{}^{}\{v,x,y_{<t}\}}\) represents the logits with hallucination and \(L^{{}^{}\{v,x,y_{<t}\}}\) represents the logits of correct token, where \(\{L^{{}^{}\{v,x,y_{<t}\}},L^{*}\{v,x,y_{<t}\}} L^{\{v,x,y_{<t}\}}\). The results of contrast decoding of logits can be formulated as:

\[^{\{v,x,y_{<t}\}} = (1+)L^{\{v,x,y_{<t}\}}-^{\{v,x,y_{<t}\}}\] (13)

where larger \(\) values indicate a stronger amplification of differences between the two distributions (\(=0\) reduces to regular decoding). The condition for the absence of hallucination in the logits subsequent to subtraction is that the values of the logits corresponding to all hallucinatory tokens are less than the magnitudes of the logits corresponding to the correct lexical tokens. The aforementioned condition is articulated mathematically as follows:\[^{{}^{}\{v,x,y_{<t}\}} < ^{*\{v,x,y_{<t}\}}\] (14)

where \(^{{}^{}\{v,x,y_{<t}\}}\) denotes the result of the subtraction between the logits of all hallucinated vocabulary tokens and the logits after their ideal amplification. \(^{*\{v,x,y_{<t}\}}\) represents the outcome of the subtraction between the logits corresponding to all correct vocabulary tokens and the logits under the ideal scenario. Eqn. 14 represents a theoretical upper bound, which guides us in enhancing the effectiveness of Contrast Decoding method for hallucination elimination by ensuring that the logits of all hallucinated words are lower than those of the correct words. Upon expansion of the left side of the equation, the following result is obtained:

\[^{{}^{}\{v,x,y_{<t}\}} =\{(1+)L^{{}^{}\{v,x,y_{<t}\}}-^{{}^{ }\{v,x,y_{<t}\}}\}\] (15) \[=\{(1+)l_{i}^{\{v,x,y_{<t}\}}-^{{}^{ }}\{v,x,y_{<t}\}},i\{k_{1}^{{}^{}},k_{2}^{{}^{}},,k_{ m}^{{}^{}}\}\] \[_{i=k_{1}}^{k_{m}}((1+)l_{i}^{\{v,x,y_{< t}\}}-^{{}^{}}\{v,x,y_{<t}\}})\]

where \(m\) denotes the total number of hallucinated vocabulary items, and \(k_{j}\) represents the subscript position of the _i-th_ hallucinated vocabulary within the set \(L^{\{v,x,y_{<t}\}}\). For the right side of the equation,one of the correct lexical items is selected as the subject for amplification.

\[^{*\{v,x,y_{<t}\}}&=\{(1+ )L^{*\{v,x,y_{<t}\}}-^{*\{v,x,y_{<t}\}}\}\\ &(1+)l_{j}^{\{v,x,y_{<t}\}}-_{j}^{\{v,x,y_ {<t}\}},j\{k_{1}^{*},k_{2}^{*},,k_{n}^{*}\}\] (16)

where \(n\) denotes the total number of correct lexical items. Based on Eqn. (15) and Eqn. (16), Eqn. (14) can be simplified to the form presented as follows:

\[ m((1+)l_{j}^{\{v,x,y_{<t}\}}- _{j}^{\{v,x,y_{<t}\}})-_{i=k_{1}}^{k_{m}}((1+)l_{i}^{\{v,x,y_{ <t}\}}-_{i}^{\{v,x,y_{<t}\}})>0\\ _{i=k_{1}}^{k_{m}}(_{i}^{\{v,x,y_{<t}\}}-_{j}^{ \{v,x,y_{<t}\}})>J\] (17)

where \(J\) represents \(_{i=k_{1}}^{k_{m}}(l_{i}^{\{v,x,y_{<t}\}}-l_{j}^{ \{v,x,y_{<t}\}})\). In the context of the contrast decoding method, given that the parameters of the original model remain invariant, the output can be characterized as a constant. Eqn. 17 delineates the logits for all hallucinated tokens \(_{i}^{\{v,x,y_{<t}\}}\) and contrasts these with the logits of a single correct token \(_{j}^{\{v,x,y_{<t}\}}\). It postulates that, for an optimal logits output, a pronounced divergence must be maintained between the logits of hallucinated tokens and the logit of the correct token.

Eqn. 17 illustrates that hallucinations can be effectively eliminated through contrastive decoding if the difference between the logits of the hallucinatory token and the correct token in the 'Evil' LVLM's output (Left part of Eqn.17) exceeds that in the original LVLM output (\(J\) in Eqn.17). For example, as depicted in the lower part of Fig. 2, where "Dogs" is a hallucination and "Benches" is the correct label, the hallucination of "Dogs" is removed when the difference between the logits for "Dogs" and "Benches" in the 'Evil' LVLM output surpasses the difference in the original LVLM output. When this condition is met for all potential hallucinations, all hallucinations are effectively eliminated.

## 6 Experiments

### Experimental Settings

**Benchmarks.** We evaluate HIO on three benchmarks including: (1) Quantitative metrics POPE Li et al. (2023b) on MSCOCO Lin et al. (2014) dataset. The Polling-based Object Probing Evaluation Li et al. (2023b) offers a streamlined approach to assessing object hallucination. In this benchmark, LVLMs are queried about the existence of specific objects in a given image. (2) CHAIR Rohrbach et al. (2018), Caption Hallucination Assessment with Image Relevance, is a specialized tool designed to evaluate the occurrence of object hallucination in image captioning tasks. (3) General-purposed Multimodal Large Language Model Evaluation (MME) Fu et al. (2023) benchmark, which provides an extensive benchmark designed to evaluate LVLMs across multiple dimensions, including ten perception-related subtasks and four cognition-focused ones.

**Implementation Description** We evaluate our model across three Large Vision-Language Models (LVLMs): LLAVA 1.5, InstructBLIP, and MiniGPT-4. For decoding, we use Llama-7B and Vicuna-7B as the linguistic decoder for LLaVA and InstructBLIP/MiniGPT-4, respectively. Our model's performance is compared against three leading models in the field: OPERA Huang et al. (2023), VCD Leng et al. (2023), and VDD Zhang et al. (2024). To ensure a fair and rigorous comparison, we adhere to the configurations and guidelines from the original works and codebases of the compared models. The training is conducted on a robust computational setup: 4x RTX 3090 GPUs for LLaVA 1.5, 8x V100 GPUs for MiniGPT-4, and 4x A6000 GPUs for InstructBLIP. Each training session lasts approximately 2-4 hours. Hyperparameters including alpha and beta are set to 1.0 and 0.1, respectively, in accordance with the VCD model's specifications.

### Experimental Results

**POPE.** To evaluate HIO's capability on object hallucination, we compare it with several state-of-the-art Decoding methods on POPE. The results are shown in Tab. 1, which presents the experimental results on the POPE dataset across random, popular, and adversarial settings. Our method consistently outperforms the standard decoding strategy, with average improvements of 6.2\(\%\) in accuracy and 7.3\(\%\) in F1 score across all LVLMs. Additionally, our approach clearly surpasses state-of-the-art decoding methods, demonstrating its effectiveness in mitigating object hallucinations. The improved performance across _random_, _popular_, and _adversarial_ settings further confirms that our HIO method effectively reduces hallucinations in diverse scenario.

**CHAIR.** Beyond the "Yes-or-No" discriminative evaluations conducted on the POPE and MME datasets, we also assess our model's performance in open-ended caption generation using the CHAIR benchmark. Tab.2 and Tab.5 display results for 500 randomly selected images from the COCO val2017 and val2014 datasets, respectively. These results show consistent improvements in our model compared to other methods. Specifically, our approach significantly reduces object hallucinations in generated captions, as evidenced by lower CHAIRS and CHAIRI scores (8.1\(\%\) reduction in CHAIRS and 4.9\(\%\) in CHAIRI). Furthermore, it enhances caption detail, as indicated by higher Recall scores. Overall, our method achieves an effective balance between accuracy and detail in open-ended caption generation by widening the gap between hallucinated and correct tokens.

**MME.** To evaluate HIO's capability on object-level and attribute-level hallucination, we compare it with several state-of-the-art Decoding methods on MME. The results are shown in Tab. 3. Consistent with the performance on POPE and CHAIR, HIO also achieves competitive results on MME compared to other decoding methods. Concretely, HIO outperforms the VCD 6.4\(\%\), 21.7\(\%\), 4.7\(\%\) and 17.0\(\%\) at _Existence_, _Count_, _Position_ on MME, respectively. The results demonstrate the effectiveness of our method.

### Ablation Study

To verify the effectiveness of each component of the proposed HIO, we conduct ablation studies on Contrary Bradley-Terry Model(CBTM), Amplification of Multiple Targeted Hallucination(AMTH) and Advanced Constraints for Inducing(ACI) under the MSCOCO Lin et al. (2014). The results are shown in Tab. 4. when constrained by CBTM in Exp 2, the model outperforms the baseline(_i.e.,_ Exp 1). This helps LVLM amplify hallucinations. Furthermore, after being integrate with AMTH

 
**Dataset** & **Setting** & **Decoding** & Accuracy\(\) & Precision & Recall & F1 Score\(\) \\   &  & Regular & 83.29 & 92.13 & 72.80 & 81.33 \\  & & VCD & 87.73 & 91.42 & 72.80 & 87.16 \\  & & ICD & 89.56 & 88.71 & 90.66 & 89.68 \\  & & VDD & 90.00 & 97.36 & 79.13 & 88.79 \\  & & Ours & **90.21** & **93.23** & **86.85** & **89.94** \\   &  & Regular & 81.88 & 88.93 & 72.80 & 80.06 \\  & & VCD & 85.38 & 86.92 & 83.28 & 85.06 \\  & & ICD & 86.16 & 83.18 & 90.66 & 86.76 \\  & & VDD & 85.91 & 94.33 & 76.33 & 84.40 \\  & & Ours & **88.12** & 88.96 & **86.83** & **87.84** \\   &  & Regular & 78.96 & 83.06 & 72.75 & 77.57 \\  & & VCD & 80.88 & 79.45 & 83.29 & 81.33 \\   & & ICD & 79.71 & 74.35 & 90.66 & 81.70 \\   & & VDD & 83.52 & 89.34 & 76.20 & 82.20 \\   & & Ours & **84.32** & 84.28 & **84.33** & **84.34** \\  

Table 1: Results on POPE. _Regular_ decoding denotes direct sampling, whereas _VCD_ refers to Visual Contrastive Decoding method, whereas _VDD_ refers to Visual Debias Decoding. The best performances within each setting are **bolded**.

  Row & Method & Length & \(_{S}\) & \(_{I}\) & Recall \(\) \\ 
1 & - & 100.6 & 50.0 & 15.4 & 77.1 \\
2 & VCD & 100.4 & 48.6 & 14.9 & 77.3 \\
3 & OPERA & 98.6 & 47.8 & 14.6 & 76.8 \\
4 & OPERA (fast) & 85.3 & 48.6 & 14.5 & 76.7 \\
5 & ICD & 106.3 & 50.8 & 15.0 & 78.5 \\ 
6 & **Ours** & 110.3 & **41.4** & **10.5** & **77.4** \\   

Table 2: Hallucination performance of different methods.

in Exp 3, LVLM obtain significant gains on CHAIR\({}_{S}\) and CHAIR\({}_{I}\). When integrate with ACI, the LVLM achieve superior performance on CHAIR\({}_{S}\), CHAIR\({}_{I}\) and Recall. These results demonstrate the effective of each component.

Moreover, we have enriched the ablation study to analyze the generalization capability of our proposed components to unseen categories, as detailed in Table 4. For the Unseen-P dataset, we collected data from MSCOCO, A-OKVQA, and GQA, ensuring no overlap with the training set, resulting in 495 samples across 10 distinct classes. These experiments show that our components generalize effectively to unseen data. Finally, we have integrated the ablation study into the experimental results section, rather than presenting it separately.

## 7 Discussion

In this study, we conduct an in-depth examination of the principles governing contrast decoding and the prerequisites for its efficacy. Based on our findings, we introduce HIO, an innovative model optimization approach designed to induce hallucinations. This method significantly amplifies hallucinatory elements within the model, thereby effectively mitigating them through contrast decoding. Extensive experimentation across various datasets has demonstrated that HIO effectively reduces hallucinations and achieves state-of-the-art performance.

**Limitations \(\&\) Future Work.**

Our findings establish a necessary, but not sufficient, condition for the successful operation of contrast decoding. Further exploration of more effective conditions could significantly enhance the efficiency of contrast decoding in mitigating hallucinations. Additionally, exploring training-free methods to induce hallucinations could reduce the computational costs associated with decoding.

## 8 Acknowledgments and Disclosure of Funding

This study is supported by grants from the National Natural Science Foundation of China (Grant No. 62122018, No. 62020106008, No. U22A2097, No. U23A20315), and Kuaishou, and Natural Science Foundation of Sichuan Province (Grant No. 2025ZNSFSC1463).

   Exp & CBTM & AMTH & ACI & CHAIR\({}_{S}\)\(\) & CHAIR\({}_{I}\)\(\) & Recall\(\) \\ 
1 & **-** & **-** & **-** & 33.4 & 9.07 & 81.1 \\ 
2 & ✓ & **-** & **-** & 18.6 & 5.08 & 79.9 \\
3 & ✓ & ✓ & **-** & 14.2 & 3.06 & 80.5 \\
4 & ✓ & ✓ & ✓ & **11.2** & **2.02** & **81.3** \\   

Table 4: Ablation study with different components of our model on CHAIR-COCO.

   Model & Decoding &  **Object-level** \\ _Existence\(\)_ \\  } &  **Attribute-level** \\ _Count\(\)_ \\  } &  **Total Scores\(\)** \\ _Position\(\)_ \\  } \\   & Regular & \(175.67\) & \(124.67\) & \(114.00\) & \(151.00\) & \(565.33\) \\  & VCD & \(184.66\) & \(138.33\) & \(128.67\) & \(153.00\) & \(604.66\) \\  & VDD & \(190.00\) & \(143.33\) & \(145.00\) & \(165.00\) & \(643.33\) \\   & Ours & **190.00** & **160.00** & \(133.33\) & **170.00** & **653.33** \\   

Table 3: Results on the hallucination subset of MME. Regular decoding denotes direct sampling, _VCD_ denotes Visual Contrastive Decoding method, whereas _VDD_ refers to Visual Debias Decoding. The best performances within each setting are **bolded**.

   Dataset & CBTM & AMTH & ACI & Accuray \(\) & Precision\({}_{I}\)\(\) & Recall \(\) & F1 Score \(\) \\   & **-** & **-** & 88.88 & 84.88 & 95.63 & 83.93 \\  & ✓ & **-** & 89.79 & 86.22 & **95.63** & 90.68 \\   & ✓ & ✓ & - & 91.83 & **95.30** & 88.64 & 91.85 \\   & ✓ & ✓ & ✓ & **92.97** & 91.94 & 94.75 & **93.33** \\   & **-** & **-** & **-** & 81.15 & 64.86 & 100.00 & 78.68 \\   & ✓ & **-** & **-** & 82.61 & 66.66 & **100.00** & 80.02 \\    & ✓ & ✓ & **-** & 84.05 & 72.41 & 87.51 & 79.24 \\   & ✓ & ✓ & ✓ & **85.51** & **75.01** & 87.51 & **80.76** \\   

Table 5: Ablation study on the generalization of each component on unseen datasets.