# VPGTrans: Transfer Visual Prompt Generator

across LLMs

 Ao Zhang\({}^{1}\)   Hao Fei\({}^{1}\)   Yuan Yao\({}^{2}\)   Wei Ji\({}^{1}\)   Li Li\({}^{1}\)   Zhiyuan Liu\({}^{2}\)   Tat-Seng Chua\({}^{1}\)

\({}^{1}\) NExT++ Lab, School of Computing, National University of Singapore

\({}^{2}\)Department of Computer Science and Technology, Tsinghua University

zhanga6@outlook.com   haofei37@nus.edu.sg   yaoyuanthu@163.com

###### Abstract

Since developing a new multimodal LLM (MLLM) by pre-training on a tremendous amount of image-text pairs from scratch is exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG component of the MLLM still incurs significant computational costs, such as thousands of GPU hours and millions of training data points. An alternative solution is to transfer an existing VPG from one MLLM to the target MLLM. In this work, we investigate VPG transferability across LLMs for the first time, aiming to reduce the cost of VPG transfer. Specifically, we explore VPG transfer across different LLM sizes (_e_.\(g\)., small-to-large) and types. We identify key factors to maximize the transfer efficiency, based on which we develop a simple yet highly effective two-stage transfer framework, called **VPGTrans**. Notably, it enables VPG transfer from BLIP-2 OPT\({}_{2.7}\) to BLIP-2 OPT\({}_{6.7}\) with less than 10% of GPU hours using only 10.7% of the training data compared to training a VPG for OPT\({}_{6.7}\) from scratch. Furthermore, we provide a series of intriguing findings and discuss potential explanations behind them. Finally, we showcase the practical value of our VPGTrans approach, by customizing two novel MLLMs, including VL-LLaMA and VL-Vicuna, with the recently released LLaMA and Vicuna LLMs.

## 1 Introduction

Background.Recent years have witnessed a great rise in large-scale language models (LLMs) in ushering the human-like artificial intelligence. Text-based LLMs  are further enhanced

Figure 1: (a) The general architecture of MLLMs, \(e\)._g_., BLIP-2  and PaLM-E , including a visual prompt generator (VPG), a linear projector and a backbone LLM. Typically, to tune the MLLM, only the VPG and the projector are updated, while the LLM is kept frozen. (b) This work investigates the VPG transferability across LLMs, including different LLM sizes and LLM types.

by associating with other modalities such as vision, leading to the multimodal LLMs (MLLMs), such as BLIP-2 , Flamingo , GPT-4  for multimodal dialog system, and PaLM-E  for embodied AI system. To construct a MLLM, a visual prompt generator (VPG) module (_cf._ Fig. 1(a)) that produces soft prompts for the input images/videos is added2 for bridging the gap between vision and language modalities. Currently, such architecture has been frequently adopted by many popular MLLMs . For example, BLIP-2 pre-trains a CLIP-ViT  combined with a Q-Former as VPG. To obtain the final MLLM, the VPG needs to be tuned. Ideally, the vast LLM backbone can remain untouched, leaving only the relatively lightweight VPG module to be fully or partially updated.3

Motivation.However, building a MLLM is inevitably computation-expensive, due to the huge overhead brought by the LLM. For example, training a BLIP-2 FlanT5XXL needs over 600 A100-GPU hours on over 100 million image-text pairs. Hopefully, transferring a pre-trained VPG (which is the main body of trainable parts) from an existing MLLM to a novel LLM instead of training from scratch,4 offers a promising solution. Intuitively, all the MLLMs literally can share the same VPG infrastructure and utility,5 which makes the VPG transfer theoretically feasible. In this work, we thus investigate the potential of transferring VPG across LLMs.

**Proposal.** Specifically, this paper examines the transferability of VPG across LLMs: 1) with different sizes (the same type), _i.e._, _transfer across LLM sizes_, and 2) across different LLM types, _i.e._, _transfer across LLM type_, as illustrated in Fig. 1(b).

* [_Transfer across LLM sizes_ (**TaS**)]. It has been a typical practice for LLM-related research  to validate the training strategy and the hyperparameter on smaller models (_e.g._, OPT2.7B) and then scale up to larger ones (_e.g._, OPT6.7B). It is thus worth exploring whether a VPG trained on a smaller LLM can be transferred to a larger LLM, resulting in reduced computational costs & data, and maintaining comparable performance.
* [_Transfer across LLM Types_ (**TaT**)]. With a well-tuned VPG for a type of LLM, it is interesting to see if VPG can be transferred to other types of LLMs even with different architectures (_e.g._, decoder _v.s._ encoder-decoder). If the transfer can be achieved, how to make it more efficient?

We conduct a series of exploratory analyses (_cf._ SS3.1) to identify the key factors for transfer efficiency. Based on our empirical study, we design a two-stage transfer learning framework (_cf._ SS3.2), namely **VPGTrans**, that includes a projector warm-up (stage-1) and vanilla fine-tuning (stage-2). For stage-1, we find that warming up the projector before VPG tuning can effectively reduce the training step for adapting a pre-trained VPG to a new LLM, and avoid the potential performance drop in the adaptation. To achieve an efficient warm-up, the projector will be well-initialized and then trained with an extremely large learning rate (\(5 lr\)). For stage-2, there is a vanilla fine-tuning of both the VPG and projector. Despite its simplicity, VPGTrans is able to significantly speed up the VPG-transfer process without harming the performance.

Results and Findings.Via extensive experiments on the transfer across LLM sizes and types (_cf._ SS4 & SS5), we gain the following key observations:

* VPGTrans helps to avoid the performance drop caused by directly inheriting the VPG and achieves at most 10 times acceleration for the small-to-large transfer across LLMs in the same type.
* VPGTrans can also achieve comparable or better performance than training from scratch and achieve at most 5 times acceleration for the transfers between different model types.
* Notably, our VPGTrans helps to achieve a **BLIP-2 ViT-G OPT2.7B\(\)6.7B** transfer with **less than 10% of GPU hours** and 10.7% of training data required for the original model training.
* Furthermore, our framework can even outperform the original BLIP-2 OPT6.7B on most of the evaluated datasets, with a **+2.9** improvement on VQAv2 and a **+3.4** improvement on OKVQA.

Our investigation further reveals some intriguing findings, for which we provide possible explanations:* When conducting TaS from LLM\({}_{}\) to LLM\({}_{}\), the size of LLM\({}_{}\) is not the larger the better. The transfer sometimes even follows a counterintuitive principle of "_the smaller the LLM\({}_{}\) size, the more speed-up and better performance_" (_cf._ SS4.2).
* When conducting TaT, efficient VPG transfer can not be achieved between two small LLMs with our VPGTrans, due to the large gap between small LLMs' embedding space (_cf._ SS5.2).

Contributions.In this study, we show for the first time that effective VPG transfer across LLMs can be achieved under most conditions, suggesting that it is possible to build a new MLLM with considerably lower computational cost, as seen in Fig. 2. To summarize, we make the following key contributions:

* _Effective approach._ We investigate the key factors for VPG-transfer efficiency and propose a two-stage transfer framework VPGTrans. The approach helps to achieve a highly-efficient VPG transfer across LLMs with less training data and even task improvements.
* _Intriguing findings._ By exploring the VPG transfer across LLMs, we reveal several intriguing findings and provide potential explanations that will shed light on further research.
* _Open source_. We showcase how to customize a novel GPT-4-like MLLM with our VPGTrans (_cf._ SS6), and release two multimodal-version MLLMs: VL-LLMA and VL-Vicuna. All codes and models is released at https://github.com/VPGTrans/VPGTrans.

## 2 Preliminary

This section will outlines the existing prevailing MLLMs, and elaborates on the settings of the exploratory analyses of these MLLMs.

### MLLM

Architecture.As illustrated in Fig. 1(a), current MLLMs mostly adopt a common architecture, including a visual prompt generator (VPG), a projector, and a backbone LLM. Typically, VPG takes images/videos as inputs, and encodes the visual input into a fixed length of soft prompts. Then, a linear projector is employed to align the soft prompt's dimension to LLM's word embedding dimension. Finally, the LLM will generate sentences based on the information from the soft prompt. We list some of the recent representative MLLMs in Table 1.

Training Paradigm.Given a MLLM, typically the VPG and linear projector will be trained, fully or partially. For example, PaLM-E updates all of the parameters of VPG in the pre-training stage, while BLIP-2 and Flamingo freeze the ViTs and tune their Q-Former and Resampler, respectively. As the main part of the whole architecture, the LLM is usually frozen during the training or tuned only a small portion (_e.g._, 10B for Flamingo-80B). KOSMOS-1 is an exception, which does not use a pre-trained LLM but trains the LLM from scratch. Such a training paradigm typically results in much longer training time and data (both multimodal and pure text corpus). Recent works [30; 16] show that

  
**MLLMs** & **VPG** & **VPG Trainable** & **LLM** & **LLM Trainable** \\  KOSMOS-1  & CLIP  & All & Rand. Init. LM & All \\ Frozen  & NF-ResNet-50  & NF-ResNet-50 & GPT-2-like\(\) & No \\ Flamingo  & NFNet-F6 +Resampler  & Resampler & Chinchilla  & Xattn-Dense \\ PaLM-E  & ViT  / OSRT  & All & PaLM  & No \\ BLIP-2  & EVA-CLIP  + Q-Former  & Q-Former & OPT  / Flan-T5  & No \\   

Table 1: MLLMs architectures and pre-training paradigm. \(\): it is a GPT-2-like LLM with relative position embeddings.

Figure 2: Comparing the cost between _training VPG from scratch_ vs. _transferring VPG via our VPGTrans strategy_. Note the LLM via VPGTrans is \(\) and \(_{2,7}\), respectively.

Figure 1: Comparing the cost between _training VPG from scratch_ vs. _transferring VPG via our VPGTrans strategy_. Note the LLM via VPGTrans is \(\) and \(_{2,7}\), respectively.

adopting an existing LLM and freezing all of its parameters can also achieve excellent performance with significantly reduced computational cost, which leads to the trend of adapting frozen pre-trained LLM. For example, BLIP-2 FlanT5xXL (12.1B) can achieve better zero-shot VQAv2 performance (65.0% in Acc.) compared with KOSMOS-1 (51.0% in Acc.) and Flamingo-80B (56.3% in Acc.). Thus, in this paper, we mainly focus on VPG transfer across frozen LLMs.

### Experiment Settings

Architecture.We adopt BLIP-2's architecture and training paradigm. In our exploration experiments, we consider using the VPG that consists of a CLIP ViT-L/14 , and a Q-Former that has already undergone a BLIP-like pre-training (the 1st stage pre-training in BLIP-2's paper ).

Training Data.For all of the exploration experiments, we adopt human-annotated COCO caption dataset  and web image-text pairs SBU dataset , which results in 1.4 million image-text pairs.

Transfer Direction.For the small-to-large model transfer among the same type of LLMs, we investigate: 1) OPT  (decoder-only) series including 125M, 350M, 1.3B, and 2.7B, and 2) FlanT5  (encoder-decoder) ranging _base, large_, and _XL_. For the transfer across different types of LLMs, we consider the ones of OPT and FlanT5 with similar sizes.

Evaluation.To evaluate the performance of MLLMs, we choose two caption datasets: (1) COCO caption  (2) NoCaps , and three VQA datasets: (3) VQAv2  (4) GQA  (5) OKVQA . We make evaluations after the pre-training without task-specific fine-tuning and report the CIDEr  for all caption tasks and accuracy for all VQA tasks.

Implementation Details.We follow the same implementation details of BLIP-2, via the open code.6 Concretely, we use FP16 and BFloat16 for OPT and FlanT5 respectively in the model training. For the learning rate, we first conduct a linear warm-up from 1e-6 to 1e-4, and then use a cosine learning rate schedule with the minimal _lr_=1e-5 for 10 epochs. Due to the limited data amount, we slightly decrease the batch size, which we find beneficial for the final performance. Specifically, we set the batch size of 1,728 and 1,152 for OPT and FlanT5-based models, respectively.

## 3 Maximizing the Transfer Efficiency with a Two-stage Transfer Strategy

In this section, we first identify the key factors for maximizing transfer efficiency, based on which we then motivate our solution for better transfer.

### Exploratory Analysis: Identifying Key Factors for VPG Transfer

Via selected experiments of small-to-large transfer among OPT models, we can obtain the following key observations. More systematical comparisons are conducted in the later section (_cf._ SS4).

\(\)**Inheriting the trained VPG can accelerate training.** To demonstrate this, we compare the convergence rates of VPG training on OPT\({}_{}\) from scratch, and inheriting VPG trained on OPT\({}_{}\). The patterns are shown in Fig. 3. Overall, we find that inheriting VPG trained on OPT\({}_{}\) accelerates convergence, particularly for two caption tasks. However, for datasets that require fine-grained visual perception such as VQAv2 and GQA, directly conduct continue training with an inherited VPG will harm the performance. We hypothesize that tuning VPG with a randomly initialized projector will compromise the existing fine-grained visual perception ability of VPG. The possible reason can be that, the VPG is typically a pre-trained model with powerful visual perception ability, and thus updating based on the gradient passed through a random projector will mislead the VPG at the initial steps .

\(\)**Warming up the linear projector can prevent performance drop and expedite VPG training.** To verify this, we first conduct a warm-up training of the linear projector for 3 epochs, during which both VPG and LLM are frozen. Subsequently, we jointly train VPG and the projector and plot the performance curve in Fig. 4 **(the warm-up process is not included in this figure)**. The results show that the performance drop observed in Fig.3 can be avoided in Fig. 4. Additionally, we observe that the warm-up training leads to fewer training steps required for VPG and projector joint training. However, we must emphasize that warming up is a costly step. In the case of a large LLM, such as 6.7B, the trainable parameters of BLIP-2's VPG will account for less than 10% of the total parameters, where freezing VPG can only lead to a reduction of 5.4% of A100 hours (36.9 out of 684.0 A100hours). We will elaborate on how to accelerate the linear projector warm-up in our later discussion (_cf._3.1).

\(\)**Initializing LLM\({}_{}\)'s projector with the help of the word converter can accelerate the linear projector warm-up.** In fact, the VPG and projector trained on LLM\({}_{}\) have already learned how to map the visual content to LLM\({}_{}\)'s understandable soft prompt . If we can convert the LLM\({}_{}\)'s soft prompt to LLM\({}_{}\)'s soft prompt, we can directly get a VPG suitable for LLM\({}_{tgt}\). One natural idea is to leverage the word embeddings of both models as a proxy for the soft prompt . The intuition behind the scene is that, the soft prompt works in the same format as normal words.

To validate our hypothesis, we conduct an experiment on the transfer from OPT\({}_{}\) to OPT\({}_{}\). After training a linear word embedding converter (_cf._SS3.2(b)), we initialize the projector for OPT\({}_{}\) with the merged linear operation of the projector for OPT\({}_{}\) and converter. As shown in Table 2, we observe that the initialization can reduce the 3 epochs' warm-up to 2 epochs.

\(\)**Linear projector warm-up enables faster convergence with an extremely large learning rate.** To determine the most efficient transfer practice, we experiment with training the projector using different learning rates. Surprisingly, we find that the linear projector enables fast and stable convergence with an extremely large learning rate. Specifically, by setting the learning rate to 5 times of the original value, the COCO caption's CIDEr score can reach 133.1 with **1 epoch** training, which is **higher than the 3 epochs** results of _w/o init._ as shown in Table 2.

### A Two-stage VPG Transfer Framework

By connecting all the dots as discussed above in SS3.1, we now design our two-stage VPGTrans framework for more efficient VPG transfer. As shown in Fig. 5, the stage-1 of VPGTrans performs projector warm-up and the stage-2 carries out a vanilla fine-tuning. Our results demonstrate that the VPGTrans is simple yet effective that can significantly speed up the transfer without compromising performance. Detailed results are given in the later sections (_cf._SS4 & 5).

\(\)**Stage-1: Projector Warm-up.**

_(a) Inherit VPG._ We first initialize the VPG for LLM\({}_{}\) with the VPG trained on LLM\({}_{}\).

_(b) Projector Initialization._ Then, we initialize the projector for LLM\({}_{}\) merged from the projector of LLM\({}_{}\) and a linear word converter. Formally, we define the linear projector of LLM\({}_{}\) as \(f_{s}(x)=W_{s}x+b_{s}\), the linear projector for LLM\({}_{}\) as \(f_{t}(x)=W_{t}x+b_{t}\), and the word converter as \(g_{c}(x)=W_{c}x+b_{c}\).

The word converter is a linear layer trained with text-only caption data to convert the LLM\({}_{}\)'s word embeddings to LLM\({}_{}\)'s word embeddings. We experiment with optimizing losses based on cosine similarity or Euclidean distance, and observe no significant difference between the two losses. Thus we simply use cosine similarity in our experiments. In cases where LLM\({}_{}\) and LLM\({}_{}\) use different

   Epoch & w/ init. & w/o init. \\ 
1 & 130.2 & 126.1 \\
2 & 132.7 & 131.6 \\
3 & 133.4 & 132.8 \\   

Table 2: Comparison between linear projector warm-up with/without word embedding initialization. The metric is COCO caption’s CIDEr.

Figure 4: First warming-up then transferring can avoid performance drop on VQAv2 and accelerate convergence for COCO caption.

Figure 3: Comparisons between i) inheriting VPG from OPT\({}_{}\) and training it with randomly initialized projector for OPT\({}_{}\) and ii) training VPG and randomly initialized projector for OPT\({}_{}\) from scratch.

tokenization methods, we optimize based on the overlapped tokens. Formally, for every given token \(k\), we denote its word embeddings of LLM\({}_{}\) and LLM\({}_{}\) as \(x_{s}\) and \(x_{t}\). Then, we minimize the loss:

\[=1-sim(g_{c}(x_{s}),x_{t})\,.\] (1)

Once we obtain the word converter \(g_{c}()\), we can easily merge it with the projector of LLM\({}_{}\) as:

\[f_{t}(x)=f_{s}(g_{c}(x))=W_{s}(W_{c}x+b_{c})+b_{s}\,,\] (2)

resulting in \(f_{t}\)'s weight and bias as \(W_{t}=W_{s}W_{c}\) and \(b_{t}=W_{s}b_{c}+b_{s}\).

_(c) Warm-up Training._ Then, we only train the projector in this stage with a frozen VPG and LLM. Specifically, we train the projector for 1 epoch with 5 times of the normal learning rate.

\(\)Stage-2: Vanilla Fine-tuning.

_(d) Vanilla Fine-tuning._ In the final step, we conduct a joint training of VPG and projector for \(n\) epochs with a normal learning rate.

## 4 Exp-I: Transfer across Different Model Sizes

In this section, we conduct experiments to systematically illustrate the effectiveness of our VPGTrans and analyze the relationship between transfer efficiency and model size. For simplicity, we use **TaS** to represent the transfer across different model sizes.

### Experimental Settings

In this part, we introduce baselines and transfer variants. For details about training data and implementation details, please refer to the experiment settings in the Preliminary (_cf._2.2).

Baselines.We mainly compare our VPGTrans with _training from scratch_ (TFS) and _VPG inheritance_ (VPG Inherit), where we report their performance on the aforementioned 5 tasks without further task-specific fine-tuning. For our VPGTrans, the word converter training only requires updating a linear layer on tokenized text data and typically takes less than 10 minutes on 1 A100 GPU with less than 15G GPU memory. Meanwhile, freezing the VPG can lead to at least 14 A100 minutes speed-up per epoch. **Therefore, we consider the whole stage-1 training as the 1st epoch for simplicity**.

Transfer Variants.We conducted experiments on transfer learning using 1) the OPT model across four different sizes: 125M, 350M, 1.3B, and 2.7B, and 2) the FlanT5 model across three sizes: _base, large_, and _XL_. However, we encountered significant instability during training with FlanT5\({}_{}\). As a result, we mainly present the transfer results between FlanT5\({}_{}\) and FlanT5\({}_{}\).

### VPGTrans Enabling Faster Convergence without Performance Drop under TaS

First of all, as shown in Fig. 6, our VPGTrans can consistently accelerate the model convergence. For COCO caption and NoCaps that require more training steps to converge, our VPGTrans (green line) can be higher than the other two lines (blue and orange lines). To give a quantitative evaluation of the

Figure 5: Our two-stage VPGTrans framework. **Stage-1** is to first (a) inherit the VPG of LLM\({}_{}\) and (b) initialize the projector by merging the projector of LLM\({}_{}\) and word converter. (c) Then the projector will be warmed up for 1 epoch with a large learning rate. **Stage-2** is to (d) conduct a vanilla fine-tuning for the VPG and projector for \(n\) epochs.

speed-up rate, we show the speed-up rate in Table 3. The speed-up rate is calculated by considering the number of epochs reduced to achieve the best TFS performance on a particular dataset. Formally, given a dataset \(D\), TFS obtains the best performance \(p\) on \(D\) at epoch \(e_{}\), whereas VPGTrans first achieves a better performance than \(p\) at epoch \(e_{}\). The speed-up rate on \(D\) is given by \(}}{e_{}}\). According to Table 3, our VPGTrans can achieve at least 4 times speed-up on 40% of Transfer-Task variants. Furthermore, for the two caption datasets, which take a long time to converge, our VPGTrans \(_{125}\) delivers a 10 times speed-up.

Moreover, when compared with the _VPG inherit_ in Fig. 6, our VPGTrans can achieve a higher speed-up rate on all of the variants on caption tasks, and achieve better performance on most variants except for \(_{1.38}\). We refer the readers to Appendix\(@sectionsign\)C.3 for more comparisons.

We provide interesting findings with respect to the efficiency transfer by VPGTrans in the following.

\(\) The **smaller size of \(_{}\), the easier the transfer.** In our OPT based experiments, we notice an interesting phenomenon: when transferring to a given \(_{}\), both the convergence rate and optimal performance are roughly inversely proportional to the size of \(_{}\). For example, as shown in Table 3, the \(_{125}\) and \(_{350}\) have much higher speed-up rate than \(_{1.38}\) on all of the datasets. Meanwhile, as demonstrated in Fig. 6, the optimal performance of \(_{125}\) is better than \(_{1.38}\) on 3 VQA tasks.

Figure 6: Comparison between different methods across 3 TaS variants on 5 tasks. Note that the model is directly evaluated after pre-training without further fine-tuning. Please refer to Appendix\(@sectionsign\)C for other transfer variants.

   Transfer & COCO Caption & NoCaps & VQAv2 & GQA & OKVQA \\  \(_{125}\) & 1.7 & 3.0 & 1.0 & 5.0 & 5.0 \\ \(_{125}\) & 9.0 & 10.0 & 9.0 & - & 2.0 \\ \(_{350}\) & 4.5 & 5.0 & 9.0 & 2.0 & 2.0 \\ \(_{125}\) & 10.0 & 10.0 & 2.0 & 2.0 & 3.0 \\ \(_{130}\) & 10.0 & 10.0 & 2.0 & - & 3.0 \\ \(_{130}\) & 3.3 & 3.3 & 2.0 & - & 1.5 \\ \(_{}\) & 1.0 & 1.1 & 3.0 & 4.0 & 2.0 \\  \(_{}_{1.27}\) & 5.0 & 5.0 & 2.0 & 2.0 & 3.0 \\ \(_{2.7}_{}\) & 1.7 & 2.0 & - & 2.0 & - \\   

Table 3: The speed-up rate of our VPGTrans compared with _training from scratch_ (TFS). The symbol ”-” means VPGTrans can not achieve better performance than TFS.

We hypothesize that training VPG on larger OPT will have a worse influence on VPG's existing fine-grained perception ability, which might be caused by the enlarging embedding dimensions. To validate our hypothesis, we fix the VPG weight and only tune linear projectors to test VPGs trained on different LLM\({}_{src}\) through cross-size transfer. The SPICE  metric on COCO caption is used to evaluate the VPG's visual perception ability, where SPICE is specifically designed for visual concept perception in captions. As shown in Fig. 7, for each row, given the LLM\({}_{tar}\), the performance of VPG trained on smaller LLM\({}_{src}\) can outperform the larger ones in most conditions, which indicates a better visual perception ability of VPG trained on smaller LLM\({}_{src}\). Therefore, **adapting a VPG from a smaller OPT model which is less affected, is helpful to take fewer steps to reach the TFS's best performance and achieve even better performance.**

### Scale-up Experiments

To validate the effectiveness of our VPGTrans on the real-world application level, we experiment on transferring from BLIP-2 ViT-G OPT\({}_{2.78}\) to OPT\({}_{6.78}\) and from BLIP-2 ViT-G FlanT\({}_{}\) to FlanT\({}_{}\). Please refer to Appendix\(@sectionsign\)C.4 for implementation details.

Speed-up with non-degenerated performances.As shown in Table 4, we can see that **(1) OPT\({}_{2.78 6.78}\):** our VPGTrans achieves a 10.7 times speed-up with only 10.7% training data, while the performance on VQAv2 and OKVQA have over 2 points improvement. **(2) FlanT\({}_{}\):** VPGTrans can achieve 21.1 times speed-up with less than 5% training data while achieving the same performance on VQAv2, higher performance on GQA and slightly lower performance on OKVQA. Note that continuing training the FlanT\({}_{}\) only shows improvement on VQAv2 and GQA. Thus, we do not show a checkpoint with more training steps.

    & VQAv2 & GQA & OKVQA &  &  \\  & val & test-dev & test & & \\  BLIP-2 ViT-G OPT\({}_{6.78}\) & 54.3 & 36.4 & 36.4 & 631.5 & 129M \\ BLIP-2 ViT-G OPT\({}_{2.78 6.78}\) (**ours**) & 57.2 & 36.2 & **39.8** & **59.0** & **13.8M** \\ VL-LLaMA\({}_{78}\) (**ours**) & **58.1** & **37.5** & 37.4 & 67.1 & 13.8M \\  BLIP-2 ViT-G FlanT\({}_{}\) & 65.2 & 44.7 & **45.9** & 684.0 & 121.6M \\ BLIP-2 ViT-G FlanT\({}_{}\) (**ours**) & **65.2** & **45.0** & 45.0 & **32.4** & **5.3M** \\   

Table 4: Comparison between models built with our VPGTrans and the original BLIP-2 ViT-G OPT\({}_{6.78}\) and BLIP-2 ViT-G FlanT\({}_{}\).

Figure 8: Comparison between different methods across 2 TaT variants on 5 tasks. Note that the model is directly evaluated after pre-training without further fine-tuning. Please refer to Appendix\(@sectionsign\)D for other transfer variants.

Figure 7: The confusion matrix. Only linear layers are trained for VPG evaluation. Models are tested on COCO caption with SPICE metric to compare the VPGs trained on different LLM\({}_{src}\).

## 5 Exp-II: Transfer across Different Model Types

In this section, we further investigate the transfer across different model types. For simplicity, we mark this type of transfer as **TaT**.

### Experimental Settings

In this part, we introduce baselines and transfer variants. For details about training data and implementation details, please refer to the experiment settings in the Preliminary (_cf._2.2).

Baselines.We mainly compare our VPGTrans with _training from scratch_ (TFS), and report the performance on the aforementioned 5 tasks. Other details (_cf._4.1) are totally the same with TaS experiments.

Transfer Variants.We conducted experiments on transfer learning between 1) \(_{}\) and \(_{}\), and 2) \(_{}\) and \(_{}\).

### VPGTrans Enabling Faster Convergence only on Large LLMs under TaT

\(\)There is no speed-up of TaT between two small LLMs.A finding is that on TaT our VPGTrans does not show speed-up for small models, and even shows a degeneration of training speed in the initial several epochs. As shown in Fig. 8, when transferring from \(_{}\) to \(_{}\), the convergence speed of VPGTrans is even slower than TFS in the initial several epochs.

\(\)Speed-up of VPGTrans happens in large LLMs.However, when moving to the large LLMs like \(_{}\) and \(_{}\), there is an obvious speed-up. As shown in Table 3, we can **see** at least 2 times speed-up when transferring from \(_{}\) to \(_{}\). We empirically find that **the soft prompts for larger LLM are more linear transferrable among different LLM types**. As shown in Fig. 8, when transferring between \(_{}\) and \(_{}\), the VGPTrans '1st epoch results on two caption datasets are limited, where only a linear operation can be trained. The result of \(_{}\)\(\)FlanT5\({}_{}\) on the COCO caption is even near to zero. By contrast, the result of \(_{}\)\(\)OPT\({}_{}\) with our VPGTrans are obviously higher than TFS. We hypothesize that larger LLM typically learned more generalizable text embeddings and share more similarity among relative word distances, which enables an easier VPG transfer.

## 6 Customizing New MLLMs with Any LLMs

Above, we thoroughly certify the efficacy of our proposed VPGTrans approach for higher efficient transfer of VPG. In this section, we illustrate how to apply the VPGTrans framework for VPG transfer to customize new MLLMs with any LLMs.

VL-LLaMA.By applying our VPGTrans, we can equip the recently released LLaMA  model with a VPG trained on BLIP-2 \(_{}\) to perceive the visual information. As shown in Table 4, we can see that our VL-LLaMA can outperform the original BLIP-2 \(_{}\) on all datasets.

VL-Vicuna.An exciting application of our VPGTrans is to build a GPT-4  style multimodal conversation chatbot. To achieve our goal, we employ Vicuna  as our base LLM. Similarly, we transfer the VPG from BLIP-2 \(_{}\), and add an extra instruction tuning using MiniGPT-4's self-instruct data . We compare our model with MiniGPT-4 in Fig. 9. When compared to MiniGPT-4, our VL-Vicuna shows better visual perception ability. Please refer to AppendixSE for more cases. In

   Rank & Model & Elo Rating \\ 
1 & LLaMA-Adapter v2  & 1023.0 \\
2 & LLaVA  & 1019.9 \\
3 & **VL-Vicuna (ours)** & 1012.1 \\
4 & MiniGPT-4  & 1011.9 \\
5 & InstructBLIP  & 999.5 \\
6 & mPLUG-Owl  & 996.3 \\
7 & Otter  & 981.5 \\
8 & BLIP-2  & 955.8 \\   

Table 5: Comparison between our VL-Vicuna and SOTA MLLMs for multimodal conversation. The evaluation is done by Multimodality Chatbot Arena platform via user voting.

addition, we also report the ranking and ELO ratings for our VL-Vicuna compared with the other 7 SOTA multimodal chatbots in Table 5 (The evaluation is done by Multimodality Chatbot Arena platform7). The results show the effectiveness of our VL-Vicuna compared with existing SOTA models.

## 7 Conclusion

In this work, we conduct a comprehensive investigation to the problem of VPG transferability across LLMs. We first explore the key factors for maximizing the transfer efficiency under the VPG transfer across different LLM sizes and types. Based on the key findings, we propose a novel two-stage transfer framework, namely VPGTrans, which can help to achieve comparable or better performance while significantly reducing the training costs. Moreover, a list of important findings and possible reasons behind them are shown and discussed. Finally, we demonstrate the practical value of our VPGTrans, by customizing new MLLMs via VPG transfer from existing MLLMs.