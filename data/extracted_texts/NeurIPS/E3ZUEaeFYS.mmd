# Strategic Distribution Shift of Interacting Agents

via Coupled Gradient Flows

Lauren Conger

California Institute of Technology

lconger@caltech.edu

&Franca Hoffmann

California Institute of Technology

franca.hoffmann@caltech.edu

&Eric Mazumdar

California Institute of Technology

mazumdar@caltech.edu

&Lillian Ratliff

University of Washington

ratliff1@uw.edu

###### Abstract

We propose a novel framework for analyzing the dynamics of distribution shift in real-world systems that captures the feedback loop between learning algorithms and the distributions on which they are deployed. Prior work largely models feedback-induced distribution shift as adversarial or via an overly simplistic distribution-shift structure. In contrast, we propose a coupled partial differential equation model that captures fine-grained changes in the distribution over time by accounting for complex dynamics that arise due to strategic responses to algorithmic decision-making, non-local endogenous population interactions, and other exogenous sources of distribution shift. We consider two common settings in machine learning: cooperative settings with information asymmetries, and competitive settings where a learner faces strategic users. For both of these settings, when the algorithm retrains via gradient descent, we prove asymptotic convergence of the retraining procedure to a steady-state, both in finite and in infinite dimensions, obtaining explicit rates in terms of the model parameters. To do so we derive new results on the convergence of coupled PDEs that extends what is known on multi-species systems. Empirically, we show that our approach captures well-documented forms of distribution shifts like polarization and disparate impacts that simpler models cannot capture.

## 1 Introduction

In many machine learning tasks, there are commonly sources of exogenous and endogenous distribution shift, necessitating that the algorithm be retrained repeatedly over time. Some of these shifts occur without the influence of an algorithm; for example, individuals influence each other to become more or less similar in their attributes, or benign forms of distributional shift occur [Qui+]. Other shifts, however, are in response to algorithmic decision-making. Indeed, the very use of a decision-making algorithm can incentivize individuals to change or mis-report their data to achieve desired outcomes-- a phenomenon known in economics as Goodhart's law. Such phenomena have been empirically observed, a well-known example being in [CC11], where researchers observed a population in Columbia strategically mis-reporting data to game a poverty index score used for distributing government assistance. Works such as [Mil+20; Wil+21], which investigate the effects of distribution shift over time on a machine learning algorithm, point toward the need for evaluating the robustness of algorithms to distribution shifts. Many existing approaches for modeling distribution shift focus on simple metrics like optimizing over moments or covariates [DY10; LHL21; BBS09]. Other methods consider worst-case scenarios, as in distributionally robust optimization [AZ22; LFG22; DN21; Kuh+19]. However, when humans respond to algorithms, these techniques may notbe sufficient to holistically capture the impact an algorithm has on a population. For example, an algorithm that takes into account shifts in a distribution's mean might inadvertently drive polarization, rendering a portion of the population disadvantaged.

Motivated by the need for a more descriptive model, we present an alternative perspective which allows us to fully capture complex dynamics that might drive distribution shifts in real-world systems. Our approach is general enough to capture various sources of exogenous and endogenous distribution shift including the feedback loop between algorithms and data distributions studied in the literature on informative prediction , the strategic interactions studied in strategic classification , and also endogenous factors like intra-population dynamics and distributional shifts. Indeed, while previous works have studied these phenomena in isolation, our method allows us to capture all of them as well as their interactions. For example, in , the authors investigate the effects of dynamics in strategic classification problems--but the model they analyze does not capture individual interactions in the population. In , the authors model the interaction between a population that repeatedly responds to algorithmic decision-making by shifting its mean. Additionally,  study settings in which the population has both exogenous and endogenous distribution shifts due to feedback, but much like the other cited work, the focus remains on average performance. Each of these works fails to account for diffusion or intra-population interactions that can result in important qualitative changes to the distribution.

**Contributions.** Our approach to this problem relies on a detailed non-local PDE model of the data distribution which captures each of these factors. One term driving the evolution of the distribution over time captures the response of the population to the deployed algorithm, another draws on models used in the PDE literature for describing non-local effects and consensus in biological systems to model intra-population dynamics, and the last captures a background source of distribution shift. This is coupled with an ODE, lifted to a PDE, which describes the training of a machine learning algorithm results in a coupled PDE system which we analyze to better understand the behaviors that can arise among these interactions.

In one subcase, our model exhibits a joint gradient flow structure, where both PDEs can be written as gradients flows with respect to the same joint energy, but considering infinite dimensional gradients with respect to the different arguments. This mathematical structure provides powerful tools for analysis and has been an emerging area of study with a relatively small body of prior work, none of which related to distribution shifts in societal systems, and a general theory for multi-species gradient flows is still lacking. We give a brief overview of the models that are known to exhibit this joint gradient flow structure: in  the authors consider a two-species tumor model with coupling through Brinkman's Law. A number of works consider coupling via convolution kernels  and cross-diffusion , with applications in chemotaxis among other areas. In the models we consider here, the way the interaction between the two populations manifests is neither via cross-diffusion, nor via the non-local self-interaction term. A related type of coupling has recently appeared in , however in the setting of graphs. Recent work  provides particle-based methods to approximately compute the solution to a minimax problem where the optimization space is over measures; following that work,  provides another particle-based method using mirror descent-ascent to solve a similar problem. Other recent work  proves that a mean-field gradient ascent-descent scheme with an entropy annealing schedule converges to the solution of a minimax optimization problem with a timescale separation parameter that is also time-varying; in contrast, our work considers fixed timescale separation setting.  show that the mean-field description of a particle method for solving minimax problems has proveable convergence guarantees in the Wasserstein-Fisher-Rao metric. Each of these references considers an energy functional that is linear in the distribution of each species respectively; our energy includes nonlinearities in the distributions via a self-interaction term as well as diffusion for the population. Moreover, the above works introduce a gradient flow dynamic as a tool for obtaining and characterizing the corresponding steady states, whereas in our setting we seek to capture the time-varying behavior that models distributions shifts. In the other subcase, we prove exponential convergence in two competitive, timescale separated settings where the algorithm and strategic population have conflicting objectives. We show numerically that retraining in a competitive setting leads to polarization in the population, illustrating the importance of fine-grained modeling.

Problem Formulation

Machine learning algorithms that are deployed into the real world for decision-making often become part of complex feedback loops with the data distributions and data sources with which they interact. In an effort to model these interactions, consider a machine learning algorithm that has loss given by \(L(z,x)\) where \(x^{d}\) are the algorithm parameters and \(z^{d}\) are the population attributes, and the goal is to solve

\[*{argmin}_{x}*{}_{z }L(z,x),\]

where \(\) is the class of model parameters and \((z)\) is the population distribution. Individuals have an objective given by \(J(z,x)\) in response to a model parameterized by \(x\), and they seek to solve

\[*{argmin}_{z^{d}}J(z,x).\]

When individuals in the population and the algorithm have access to gradients, we model the optimization process as a gradient-descent-type process. Realistically, individuals in the population will have nonlocal information and influences, as well as external perturbations, the effects of which we seek to capture in addition to just minimization. To address this, we propose a partial differential equation (PDE) model for the population, that is able to capture nonlocal interactions between individuals on the level of a collective population. To analyse how the population evolves over time, a notion of derivative in infinite dimensions is needed. A natural, and in this context physically meaningful, way of measuring the dissipation mechanism for probability distributions is the Wasserstein-2 metric (see Definition 4). The following expression appears when computing the gradient of an energy functional with respect to the Wasserstein-2 topology.

**Definition 1**.: _[First Variation] For a map \(G:(^{d})\) and fixed probability distribution \((^{d})\), the first variation of \(G\) at the point \(\) is denoted by \(_{}G[]:^{d}\), and is defined via the relation_

\[_{}G[](z)(z)z=_{ 0}(G(+)-G())\]

_for all \( C_{c}^{}(^{d})\) such that \(=0\), assuming that \(G\) is regular enough for all quantities to exist._

Here, \((^{d})\) denotes the space of probability measures on the Borel sigma algebra. Using the first variation, we can express the gradient in Wasserstein-2 space, see for example [25, Exercise 8.8].

**Lemma 1**.: _The gradient of an energy \(G:_{2}(^{d})\) in the Wasserstein-2 space is given by_

\[_{W_{2}}G()=-(_{}G[])\.\]

Here, \(_{2}(^{d})\) denotes the set of probability measures with bounded second moments, also see Appendix A.2. As a consequence, the infinite dimensional steepest descent in Wasserstein-2 space can be expressed as the PDE

\[_{t}=-_{W_{2}}G()=(_{ }G[])\.\] (1)

All the coupled gradient flows considered in this work have this Wasserstein-2 structure. In particular, when considering that individuals minimize their own loss, we can capture these dynamics via a gradient flow in the Wasserstein-2 metric on the level of the distribution of the population. Then for given algorithm parameters \(x^{d}\), the evolution for this strategic population is given by

\[_{t}=(_{}* {}_{z}J(z,x)+E()),\] (2)

where \(E()\) is a functional including terms for internal influences and external perturbations. In real-world deployment of algorithms, decision makers update their algorithm over time, leading to an interaction between the two processes. We also consider the algorithm dynamics over time, which we model as

\[=-_{x}*{}_{z}L(z,x).\] (3)In this work, we analyze the behavior of the dynamics under the following model. The algorithm suffers a cost \(f_{1}(z,x)\) for a data point \(z\) under model parameters \(x\) in the strategic population, and a cost \(f_{2}(z,x)\) for a data point in a fixed, non-strategic population. The strategic population is denoted by \(\), and the non-strategic population by \(\). The algorithm aims to minimize

\[}_{z}L(z,x)= f_{1}(z,x)(z)+ f _{2}(z,x)(z)+\|x-x_{0}\|^{2}\,,\]

where the norm is the vector inner product \(\|x\|^{2}= x,x\) and \(>0\) weights the cost of updating the model parameters from its initial condition.

We consider two settings: \((i)\) aligned objectives, and \((ii)\) competing objectives. Case \((i)\) captures the setting in which the strategic population minimization improves the performance of the algorithm, subject to a cost for deviating from a reference distribution \(\). This cost stems from effort required to manipulate features, such as a loan applicant adding or closing credit cards. On the other hand, Case \((ii)\) captures the setting in which the strategic population minimization worsens the performance of the algorithm, again incurring cost from distributional changes.

### Case (i): Aligned Objectives

In this setting, we consider the case where the strategic population and the algorithm have aligned objectives. This occurs in examples such as recommendation systems, where users and algorithm designers both seek to develop accurate recommendations for the users. This corresponds to the population cost

\[}_{z,x}J(z,x)= f_{1}(z,x) (z)(x)+ KL(\,|\,),\]

where \(KL(\,|)\) denotes the Kullback-Leibler divergence. Note that the KL divergence introduces diffusion to the dynamics for \(\). The weight \(>0\) parameterizes the cost of distribution shift to the population. To account for nonlocal information and influence among members of the population, we include a kernel term \(E()= W*\,z\), where \((W*)(z)= W(z-)()\) is a convolution integral and \(W\) is a suitable interaction potential.

### Case (ii): Competing Objectives

In settings such as online internet forums, where algorithms and users have used manipulative strategies for marketing , the strategic population may be incentivized to modify or mis-report their attributes. The algorithm has a competitive objective, in that it aims to maintain performance against a population whose dynamics cause the algorithm performance to suffer. When the strategic population seeks an outcome contrary to the algorithm, we model strategic population cost as

\[}_{z,x}J(z,x)=- f_{1}(z,x) (z)(x)+ KL(\,|\,).\]

A significant factor in the dynamics for the strategic population is the timescale separation between the two "species"--i.e., the population and the algorithm. In our analysis, we will consider two cases: one, where the population responds much faster than the algorithm, and two, where the algorithm responds much faster than the population. We illustrate the intermediate case in a simulation example.

## 3 Results

We are interested in characterizing the long-time asymptotic behavior of the population distribution, as it depends on the decision-makers action over time. The structure of the population distribution gives us insights about how the decision-makers actions influences the entire population of users. For instance, as noted in the preceding sections, different behaviors such as bimodal distributions or large tails or variance might emerge, and such effects are not captured in simply looking at average performance. To understand this intricate interplay, one would like to characterize the behavior of both the population and the algorithm over large times. Our main contribution towards this goal is a novel analytical framework as well as analysis of the long-time asymptotics.

A key observation is that the dynamics in (2) and (3) can be re-formulated as a gradient flow; we lift \(x\) to a probability distribution \(\) by representing it as a Dirac delta \(\) sitting at the point \(x\). As a result, the evolution of \(\) will be governed by a PDE, and combined with the PDE for the population, we obtain a system of coupled PDEs,

\[_{t} =(_{z}_{} {}_{z,x}J(z,x)+E())\] \[_{t} =(_{x}_{} }_{z,x}L(z,x)),\]

where \(_{}\) and \(_{}\) are first variations with respect to \(\) and \(\) according to Definition 1. The natural candidates for the asymptotic profiles of this coupled system are its steady states, which - thanks to the gradient flow structure - can be characterized as ground states of the corresponding energy functionals. In this work, we show existence and uniqueness of minimizers (maximizers) for the functionals under suitable conditions on the dynamics. We also provide criteria for convergence and explicit convergence rates. We begin with the case where the interests of the population and algorithm are aligned, and follow with analogous results in the competitive setting. We show convergence in energy, which in turn ensures convergence in a product Wasserstein metric. For convergence in energy, we use the notion of relative energy and prove that the relative energy converges to zero as time increases.

**Definition 2** (Relative Energy).: _The relative energy of a functional \(G\) is given by \(G(|_{})=G()-G(_{})\), where \(G(_{})\) is the energy at the steady state._

Since we consider the joint evolution of two probability distributions, we define a distance metric \(\) on the product space of probability measures with bounded second moment.

**Definition 3** (Joint Wasserstein Metric).: _The metric over \(_{2}(^{d})_{2}(^{d})\) is called \(\) and is given by_

\[((,),(,))^{2}=W_{2}(,)^{2}+W_{2}(,)^{2}\]

_for all pairs \((,),(,)_{2}(^{d}) _{2}(^{d})\), and where \(W_{2}\) denotes the Wasserstein-2 metric (see Definition 4). We denote by \(}(^{d}):=(_{2}(^{d}) _{2}(^{d}),)\) the corresponding metric space._

### Gradient Flow Structure

In the case where the objectives of the algorithm and population are _aligned_, we can write the dynamics as a gradient flow by using the same energy functional for both species. Let \(G_{a}(,):(^{d})(^{d}) [0,]\) be the energy functional given by

\[G_{a}(,) = f_{1}(z,x)(z)(x)+ f_{2}(z,x)(z)(x)+ KL(|)+  W*\] \[+\|x-x_{0}\|^{2} (x).\]

This expression is well-defined as the relative entropy \(KL(|.)\) can be extended to the full set \((^{d})\) by setting \(G_{a}(,)=+\) in case \(\) is not absolutely continuous with respect to \(\).

In the _competitive_ case we define \(G_{c}(,x):(^{d})^{d}[-,]\) by

\[G_{c}(,x)= f_{1}(z,x)(z)+ f_{2}(x,z^{})(z^{})- KL(|)- W* +\|x-x_{0}\|^{2}.\]

In settings like recommender systems, the population and algorithm have aligned objectives; they seek to minimize the same cost but are subject to different dynamic constraints and influences, modeled by the regularizer and convolution terms. In the case where the objectives are aligned, the dynamics are given by

\[_{t} =(_{z}_{}G_{a}[, ])\] (4) \[_{t} =(_{x}_{}G_{a}[, ]).\]

Note that (4) is a joint gradient flow, because the dynamics can be written in the form

\[_{t}=(_{}G_{a}( ))\,,\]where \(=(,)\) and where the gradient and divergence are taken in both variables \((z,x)\). We discuss the structure of the dynamics (4) as well as the meaning of the different terms appearing in the energy functional \(G_{a}\) in Appendix A.1.

In other settings, such as credit score reporting, the objectives of the population are competitive with respect to the algorithm. Here we consider two scenarios; one, where the algorithm responds quickly relative to the population, and two, where the population responds quickly relative to the algorithm. In the case where the algorithm can immediately adjust optimally (best-respond) to the distribution, the dynamics are given by

\[_{t}&=-( (_{z}_{}G_{c}[,x])|{}_{x=b()} ).,\\ b()&*{argmin}_{ }G_{c}(,)\,.\] (5)

Next we can consider the population immediately responding to the algorithm, which has dynamics

\[}{t}x&=- _{x}G_{c}(,x)|_{=r(x)}\,,\\ r(x)&*{argmin}_{ }-G_{c}(,x)\,.\] (6)

In this time-scale separated setting, model (5) represents a dyamic maximization of \(G_{c}\) with respect to \(\) in Wasserstein-2 space, and an instantaneous minimization of \(G_{c}\) with respect to the algorithm parameters \(x\). Model (6) represents an instantaneous maximization of \(G_{c}\) with respect to \(\) and a dynamic minimization of \(G_{c}\) with respect to the algorithm parameters \(x\). The key results on existence and uniqueness of a ground state as well as the convergence behavior of solutions depend on convexity (concavity) of \(G_{a}\) and \(G_{c}\). The notion of convexity that we will employ for energy functionals in the Wasserstein-2 geometry is _(uniform) displacement convexity_, which is analogous to (strong) convexity in Euclidean spaces. One can think of displacement convexity for an energy functional defined on \(_{2}\) as convexity along the shortest path in the Wasserstein-2 metric (linear interpolation in the Wasserstein-2 space) between any two given probability distributions. For a detailed definition of (uniform) displacement convexity and concavity, see Section A.2. In fact, suitable convexity properties of the input functions \(f_{1},f_{2},W\) and \(\) will ensure (uniform) displacement convexity of the resulting energy functionals appearing in the gradient flow structure, see for instance [22, Chapter 5.2].

We make the following assumptions in both the competitive case and aligned interest cases. Here, \(_{d}\) denotes the \(d d\) identity matrix, \((f)\) denotes the Hessian of \(f\) in all variables, while \(_{x}^{2}f\) denotes the Hessian of \(f\) in the variable \(x\) only.

**Assumption 1** (Convexity of \(f_{1}\) and \(f_{2}\)).: _The functions \(f_{1},f_{2} C^{2}(^{d}^{d};[0,))\) satisfy for all \((z,x)^{d}^{d}\) the following:_

* _There exists constants_ \(_{1},_{2} 0\) _such that_ \((f_{1})_{1}\,_{2d}\) _and_ \(_{x}^{2}f_{2}_{2}\,_{d}\)_;_
* _There exist constants_ \(a_{i}>0\) _such that_ \(x_{x}f_{i}(z,x)-a_{i}\) _for_ \(i=1,2\)_;_

**Assumption 2** (Reference Distribution Shape).: _The reference distribution \((^{d}) L^{1}(^{d})\) satisfies \( C^{2}(^{d})\) and \(_{z}^{2}(z)-\,_{d}\) for some \(>0\)._

**Assumption 3** (Convex Interaction Kernel).: _The interaction kernel \(W C^{2}(^{d};[0,))\) is convex, symmetric \(W(-z)=W(z)\), and for some \(D>0\) satisfies_

\[z_{z}W(z)-D,|_{z}W(z)| D(1+|z |)\,z^{d}\,.\]

We make the following observations regarding the assumptions above:

* The convexity in Assumption 3 can be relaxed and without affecting the results outlined below by following a more detailed analysis analogous to the approach in .
* If \(f_{1}\) and \(f_{2}\) are strongly convex, the proveable convergence rate increases, but without strict or strong convexity of \(f_{1}\) and \(f_{2}\), the regularizers \(KL(|)\) and \(\|x-x_{0}\|_{2}^{2}x\) provide the convexity guarantees necessary for convergence.

For concreteness, one can consider the following classical choices of input functions to the evolution:

* Using the log-loss function for \(f_{1}\) and \(f_{2}\) satisfies Assumption 1.

* Taking the reference measure \(\) to be the normal distribution satisfies Assumption 2, which ensures the distribution is not too flat.
* Taking quadratic interactions \(W(z)=|z|^{2}\) satisfies Assumption 3.

**Remark 1** (Cauchy-Problem).: _To complete the arguments on convergence to equilibrium, we require sufficient regularity of solutions to the PDEs under consideration. In fact, it is sufficient if we can show that equations (4), (5), and (6) can be approximated by equations with smooth solutions. Albeit tedious, these are standard techniques in the regularity theory for partial differential equations, see for example [13, Proposition 2.1 and Appendix A], , [15, Chapter 9], and the references therein. Similar arguments as in  are expected to apply to the coupled gradient flows considered here, guaranteeing existence of smooth solutions with fast enough decay at infinity, and we leave a detailed proof for future work._

### Analysis of Case (i): Aligned Objectives

The primary technical contribution of this setting consists of lifting the algorithm dynamics from an ODE to a PDE, which allows us to model the system as a joint gradient flow on the product space of probability measures. The coupling occurs in the potential function, rather than as cross-diffusion or non-local interaction as more commonly seen in the literature for multi-species systems.

**Theorem 2**.: _Suppose that Assumptions 1-3 are satisfied and let \(_{a}:=_{1}+(_{2}+,)>0\). Consider solutions \(_{t}:=(_{t},_{t})\) to the dynamics (4) with initial conditions satisfying \(_{0}_{2}(^{d})_{2}( ^{d})\) and \(G_{a}(_{0})<\). Then the following hold:_

* _There exists a unique minimizer_ \(_{}=(_{},_{})\) _of_ \(G_{a}\)_, which is also a steady state for equation (_4_). Moreover,_ \(_{} L^{1}(^{d})\)_, has the same support as_ \(\)_, and its density is continuous._
* _The solution_ \(_{t}\) _converges exponentially fast in_ \(G_{a}(\,|\,_{})\) _and_ \(\)_,_ \[G_{a}(_{t}\,|\,_{}) e^{-2_{a}t}G_{a}(_{0} \,|\,_{})(_{t},_{})  ce^{-_{a}t}t 0\,,\] _where_ \(c>0\) _is a constant only depending on_ \(_{0}\)_,_ \(_{}\) _and the parameter_ \(_{a}\)_._

Proof.: (Sketch) For existence and uniqueness, we leverage classical techniques in the calculus of variations. To obtain convergence to equilibrium in energy, our key result is a new HWI-type inequality, providing as a consequence generalizations of the log-Sobolev inequality and the Talagrand inequality. Together, these inequalities relate the energy (classically denoted by \(H\) in the case of the Boltzmann entropy), the metric (classically denoted by \(W\) in the case of the Wasserstein-2 metric) and the energy dissipation (classically denoted by \(I\) in the case of the Fisher information)1. Combining these inequalities with Gronwall's inequality allows us to deduce convergence both in energy and in the metric \(\). 

### Analysis of Case (ii): Competing Objectives

In this setting, we consider the case where the algorithm and the strategic population have goals in opposition to each other; specifically, the population benefits from being classified incorrectly. First, we will show that when the algorithm instantly best-responds to the population, then the distribution of the population converges exponentially in energy and in \(W_{2}\). Then we will show a similar result for the case where the population instantly best-responds to the algorithm.

In both cases, we begin by proving two Danskin-type results (see ) which will be used for the main convergence theorem, including convexity (concavity) results. To this end, we make the following assumption ensuring that the regularizing component in the evolution of \(\) is able to control the concavity introduced by \(f_{1}\) and \(f_{2}\).

**Assumption 4** (Upper bounds for \(f_{1}\) and \(f_{2}\)).: _There exists a constant \(_{1}>0\) such that_

\[_{z}^{2}f_{1}(z,x)_{1}I_{d}(z,x) ^{d}^{d}\,,\]

_and for any \(R>0\) there exists a constant \(c_{2}=c_{2}(R)\) such that_

\[_{x B_{R}(0)} f_{2}(z,x)(z)<c_{2}\,.\]Equipped with Assumption 4, we state the result for a best-responding algorithm.

**Theorem 3**.: _Suppose Assumptions 1-4 are satisfied with \(>_{1}\). Let \(_{b}-_{1}\). Define \(G_{b}() G_{c}(,b())\). Consider a solution \(_{t}\) to the dynamics (5) with initial condition \(_{0}_{2}(^{d})\) such that \(G_{b}(_{0})<\). Then the following hold:_

* _There exists a unique maximizer_ \(_{}\) _of_ \(G_{b}()\)_, which is also a steady state for equation (_5_). Moreover,_ \(_{} L^{1}(^{d})\)_, has the same support as_ \(\)_, and its density is continuous._
* _The solution_ \(_{t}\) _converges exponentially fast to_ \(_{}\) _with rate_ \(_{b}\) _in_ \(G_{b}(\,|\,_{})\) _and_ \(W_{2}\)_,_ \[G_{b}(_{t}\,|\,_{}) e^{-2_{b}t}G_{a}(_{0}\,|\, _{}) W_{2}(_{t},_{}) ce^{- _{b}t}t 0\,,\] _where_ \(c>0\) _is a constant only depending on_ \(_{0}\)_,_ \(_{}\) _and the parameter_ \(_{b}\)_._

Proof.: (Sketch) The key addition in this setting as compared with Theorem 2 is proving that \(G_{b}()\) is bounded below, uniformly displacement concave and guaranteeing its smoothness via Berge's Maximum Theorem. This is non-trivial as it uses the properties of the best response \(b()\). A central observation for our arguments to work is that \(_{}G_{b}[]=(_{}G_{c}[,x])\,|_{x=b()}\). We can then conclude using the direct method in the calculus of variations and the HWI method. 

Here, the condition that \(\) must be large enough corresponds to the statement that the system must be subjected to a strong enough regularizing effect.

In the opposite case, where \(\) instantly best-responds to the algorithm, we show Danskin-like results for derivatives through the best response function and convexity of the resulting energy in \(x\) which allows to deduce convergence.

**Theorem 4**.: _Suppose Assumptions 1-4 are satisfied with \(>_{1}\), and that \(r(x)\) is differentiable (as shown by example conditions in Lemmas 27 and 28). Define \(G_{d}(x) G_{c}(r(x),x)\). Then it holds:_

* _There exists a unique minimizer_ \(x_{}\) _of_ \(G_{d}(x)\) _which is also a steady state for (_6_)._
* _The vector_ \(x(t)\) _solving the dynamics (_6_) with initial condition_ \(x(0)^{d}\) _converges exponentially fast to_ \(x_{}\) _with rate_ \(_{d}:=_{1}+_{2}+>0\) _in_ \(G_{d}\) _and in the Euclidean norm:_ \[\|x(t)-x_{}\|  e^{-_{d}t}\|x(0)-x_{}\|,\] \[G_{d}(x(t))-G_{d}(x_{})  e^{-2_{d}t}(G_{d}(x(0))-G_{d}(x_{}))\] _for all_ \(t 0\)_._

These two theorems illustrate that, under sufficient convexity conditions on the cost functions, we expect the distribution \(\) and the algorithm \(x\) to converge to a steady state. In practice, when the distributions are close enough to the steady state there is no need to retrain the algorithm.

While we have proven results for the extreme timescale cases, we anticipate convergence to the same equilibrium in the intermediate cases. Indeed, it is well known  (especially for systems in Euclidean space) that for two-timescale stochastic approximations of dynamical systems, with appropriate stepsize choices, converge asymptotically, and finite-time high probability concentration bounds can also be obtained. These results have been leveraged in strategic classification  and Stackelberg games . We leave this intricate analysis to future work.

In the following section we show numerical results in the case of a best-responding \(x\), best-responding \(\), and in between where \(x\) and \(\) evolve on a similar timescale. Note that in these settings, the dynamics do not have a gradient flow structure due to a sign difference in the energies, requiring conditions to ensure that one species does not dominate the other.

## 4 Numerical Examples

We illustrate numerical results for the case of a classifier, which are used in scenarios such as loan or government aid applications , school admissions , residency match , and recommendation algorithms , all of which have some population which is incentivized to submit data that will result in a desirable classification. For all examples, we select classifiersof the form \(x\), so that a data point \(z\) is assigned a label of \(1\) with probability \(q(z,x)=(1+(-b^{}z+x))^{-1}\) where \(b>0\). Let \(f_{1}\) and \(f_{2}\) be given by

\[f_{1}(z,x)=-(1-q(z,x))\,, f_{2}(z,x)=- q(z,x).\]

Note that \((f_{1}) 0\) and \(_{x}^{2}f_{2} 0\), so \(_{1}=_{2}=0\). Here, the strictness of the convexity of the functional is coming from the regularizers, not the cost functions, with \(\) a scaled normal distribution. We show numerical results for two scenarios with additional settings in the appendix. First we illustrate competitive interests under three different timescale settings. Then we simulate the classifier taking an even more naive strategy than gradient descent and discuss the results. The PDEs were implemented based on the finite volume method from .

### Competitive Objectives

In the setting with competitive objectives, we utilize \(G_{c}(,x)\) with \(W=0\), \(f_{1}\) and \(f_{2}\) as defined above with \(b=3\) fixed as it only changes the steepness of the classifier for \(d=1\), and \(=0.1\) and \(=0.05\). In Figure 1, we simulate two extremes of the timescale setting; first when \(\) is nearly best-responding and then when \(x\) is best-responding. The simulations have the same initial conditions and end with the same distribution shape; however, the behavior of the strategic population differs in the intermediate stages.

When \(\) is nearly best-responding, we see that the distribution quickly shifts mass over the classifier threshold. Then the classifier shifts right, correcting for the shift in \(\), which then incentivizes \(\) to shift more mass back to the original mode. In contrast, when \(x\) best-responds, the right-hand mode slowly increases in size until the system converges.

Figure 2 shows simulation results from the setting where \(\) and \(x\) evolve on the same timescale. We observe that the distribution shift in \(\) appears to fall between the two extreme timescale cases, which we expect.

We highlight two important observations for the competitive case. One, a single-mode distribution becomes bimodal, which would not be captured using simplistic metrics such as the mean and variance. This split can be seen as polarization in the population, a phenomenon that a mean-based strategic classification model would not capture. Two, the timescale on which the

Figure 1: When \(x\) versus \(\) best-responds, we observe the same final state but different intermediate states. Modes appear in the strategic population which simpler models cannot capture.

Figure 2: In this experiment the population and classifier have similar rates of change, and the distribution change for \(\) exhibits behaviors from both the fast \(\) and fast \(x\) simulations; the right-hand mode does not peak as high as the fast \(\) case but does exceed its final height and return to the equilibrium.

classifier updates significantly impacts the intermediate behavior of the distribution. In our example, when \(x\) updated slowly relative to the strategic population, the shifts in the population were greater than in the other two cases. This suggests that understanding the effects of timescale separation are important for minimizing volatility of the coupled dynamics.

### Naive Behavior

In this example, we explore the results of the classifier adopting a non-gradient-flow strategy, where the classifier chooses an initially-suboptimal value for \(x\) and does not move, allowing the strategic population to respond.

All functions and parameters are the same as in the previous example. When comparing with the gradient descent strategy, we observe that while the initial loss for the classifier is worse for the naive strategy, the final cost is better. While this results is not surprising, because one can view this as a general-sum game where the best response to a fixed decision may be better than the equilibrium, it illustrates how our method provides a framework for evaluating how different training strategies perform in the long run against a strategic population.

## 5 Future Directions, Limitations, and Broader Impact

Our work presents a method for evaluating the robustness of an algorithm to a strategic population, and investigating a variety of robustness using our techniques opens a range of future research directions. Our application suggests many questions relevant to the PDE literature, such as: (1) Does convergence still hold with the gradient replaced by an estimated gradient? (2) Can we prove convergence in between the two timescale extremes? (3) How do multiple dynamic populations respond to an algorithm, or multiple algorithms? In the realm of learning algorithms, our framework can be extended to other learning update strategies and presents a way to model how we can design these update strategies to induce desired behaviors in the population.

A challenge in our method is that numerically solving high-dimensional PDEs is computationally expensive and possibly unfeasible. Here we note that in many applications, agents in the population do not alter more than a few features due to the cost of manipulation. We are encouraged by the recent progress using deep learning to solve PDEs, which could be used in our application.

**Broader Impacts** Modeling the full population distribution rather than simple metrics of the distribution is important because not all individuals are affected by the algorithm in the same way. For example, if there are tails of the distribution that have poor performance even if on average the model is good, we need to know how that group is advantaged or disadvantaged relative to the rest of the population. Additionally, understanding how people respond to algorithms offers an opportunity to incentivise people to move in a direction that increases social welfare.

Figure 3: Although the classifier starts with a larger cost by taking the naive strategy, the final loss is better. This illustrates how our model can be used to compare robustness of different strategies against a strategic population.