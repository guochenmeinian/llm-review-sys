# Axioms for AI Alignment from Human Feedback

Luise Ge

Washington University in St. Louis

g.luise@wustl.edu

&Daniel Halpern

Harvard University

dhalpern@g.harvard.edu

&Evi Micha

Harvard University

emicha@seas.harvard.edu

&Ariel D. Procaccia

Harvard University

arielpro@g.harvard.edu

&Itai Shapira

Harvard University

itaishapira@g.harvard.edu

&Yevgeniy Vorobeychik

Washington University in St. Louis

yvorobeychik@wustl.edu

&Junlin Wu

Washington University in St. Louis

junlin.wu@wustl.edu

###### Abstract

In the context of reinforcement learning from human feedback (RLHF), the reward function is generally derived from maximum likelihood estimation of a random utility model based on pairwise comparisons made by humans. The problem of learning a reward function is one of preference aggregation that, we argue, largely falls within the scope of social choice theory. From this perspective, we can evaluate different aggregation methods via established axioms, examining whether these methods meet or fail well-known standards. We demonstrate that both the Bradley-Terry-Luce Model and its broad generalizations fail to meet basic axioms. In response, we develop novel rules for learning reward functions with strong axiomatic guarantees. A key innovation from the standpoint of social choice is that our problem has a _linear_ structure, which greatly restricts the space of feasible rules and leads to a new paradigm that we call _linear social choice_.

## 1 Introduction

The alignment of AI models with human values is widely recognized as a crucial task. A prominent method for this task, _reinforcement learning with human feedback_ (RLHF), has been used in different applications, such as robotics [4; 17] and recommendations [28; 1]. Recently, RLHF has attracted significant attention as a tool for fine-tuning large language models (LLMs) [22; 32; 26]. A typical implementation of RLHF involves learning a reward model using a pre-trained LLM, which is then utilized to fine-tune an existing LLM. During the learning step, human feedback is provided in the form of ordinal comparisons, and a reward function is learned from these. The most common learning method assumes an underlying _random utility model_ such as the _Bradley-Terry-Luce (BTL)_ model [5; 22; 8] and computes a reward function that corresponds to a maximum likelihood estimator for the observed comparisons.

Is this the "right" way of aggregating individual preferences towards a socially desirable reward function? To answer this question, we draw on _social choice theory_, a field that studies collective decision making through a mathematical lens . The maximum likelihood estimation approach is in line with a well-established body of work that assumes that different human participants have preferences stemming from noisy estimation of a common ground truth, and the goal is to learn this ground truth as accurately as possible . But this is not the case when it comes to questions of AIalignment, where individuals can have legitimate differences of opinion rooted in different values or priorities.

We argue that when preferences are truly heterogeneous, the _axiomatic approach_ -- which rose to prominence in social choice with the work of Arrow  -- may be more suitable. This approach analyzes the desirability of aggregation methods by their satisfaction of certain axioms that capture notions of consensus, fairness, and economic efficiency. Specifically, we are interested in the axiomatic properties of aggregation methods that take ordinal preferences as input and output a reward function. We address the following two research questions: _What axioms are satisfied by aggregation methods used by existing RLHF algorithms? And are there alternative aggregation methods that offer stronger axiomatic guarantees?_

### Our Approach

In social choice theory, axioms are typically defined for rules that map rankings over candidates to a single winner (social choice functions) or a ranking of the candidates (social welfare functions). By contrast, we are interested in rules that assign a reward to each candidate. This gap is easy to bridge, though: we simply consider a ranking of the candidates by in descending reward order.

A much more significant gap is that in classical social choice, all relevant candidates appear in the input preferences, whereas in our setting (where candidates correspond, e.g., to prompts and their responses), we are only given preferences over a relatively small set of candidates _identified by their (known) features_, and we need to generalize from this information. In practice, this entails using a restricted--commonly, parametric--class of reward models which map candidate features to real-valued rewards, and which we fit to existing data.

Specifically, we assume that a _linear_ reward function defined by a parameter vector determines the reward of each candidate by computing the inner product of the parameter vector and the feature vector of the candidate; these modeling choices are consistent with prior and concurrent work [31; 30; 15] and aim to capture the practice of RLHF.1 Each human participant (henceforth referred to as a _voter_) is associated with a parameter vector, which is unknown to us and is used to specify ordinal preferences over the candidates. Our task is to design _linear rank aggregation rules_, which aggregate rankings induced by these individual linear functions2 into a collective ranking that is also induced by a linear function; this is a new paradigm in social choice, for which we coin the term _linear social choice_.

To evaluate linear rank aggregation rules, we adapt fundamental axioms from social choice theory . The first is _Pareto optimality (PO)_, which requires that if a candidate \(a\) is ranked above candidate \(b\) in _every_ input ranking, then the resulting ranking should rank \(a\) above \(b\). This is seen as a basic requirement and is satisfied by _every_ standard voting method in the classical setting.

The second axiom is _pairwise majority consistency (PMC)_: If there exists a reward function that generates a ranking where, for each pair of candidates, a majority of voters agree with the ranking, then the resulting ranking should match that ranking. This axiom is an extension of _Condorcet consistency_ to rankings, and is satisfied by some, but not all, standard voting methods in the classical setting.

### Our Results

We start by examining, in Section 3.1, a family of loss-based rules that finds a ranking induced by a parameter vector that optimizes a measure of loss; this measure increases for every disagreement with a voter on a pair of alternatives, where the larger the difference in rewards, the larger the penalty. Crucially, by plugging in binary cross-entropy loss we can recover the BTL model. Our first main result is that whenever the loss function is weakly convex and nondecreasing, or strictly convex -- conditions satisfied by binary cross-entropy loss, as well as, e.g., exponential and hinge loss -- the corresponding rule fails both PMC and PO. This result suggests that the prevailing practice of RLHF is flawed from an axiomatic viewpoint.

In Section 3.2, we take a first step towards addressing this shortcoming. We modify the loss-based formulation to focus on majority preferences rather than individual preferences. This modification defines a family of rules that are PMC, but we show that all of them fail PO by establishing an even stronger impossibility result: In stark contrast to the classical setting, any _linear_ rank aggregation rule that depends only on majority preferences must fail PO.

In order to achieve both PO and PMC, we design (in Section 4) a linear rank aggregation rule that we call _Leximax Copeland subject to PO_. Not only does it satisfy our two main axioms, it also satisfies two additional ones, _majority consistency_ and _winner monotonicity_.

To summarize, while widely applied rules fail to meet basic axioms, there are alternative methods that are desirable from this viewpoint. Our approach, therefore, provides a discriminative lens through which to evaluate RLHF methods and AI alignment methods more broadly.

### Related Work

During the eight months in which we have actively worked on this project (from September 2023 until May 2024) -- and especially in the first few months of 2024 -- a slew of independent, concurrent papers seeking to build bridges between social choice and RLHF have become publicly available ; this surge of interest points, in our view, to the importance of the agenda.

Three of those papers are position papers that conceptually support our work in that they discuss the possibility of applying an axiomatic approach to RLHF , although they do not provide any technical results. By contrast, existing technical papers on RLHF do not take an axiomatic approach. Of the concurrent technical papers, the one that is most closely related to ours is that of Sithitharanjan et al. . They show, among other results, that the ranking induced by the reward function that the MLE estimator of the Bradley-Terry-Luce Model returns follows the famous Borda count rule when unrestricted reward functions are allowed. In the classical setting, Borda count has strong axiomatic guarantees, including PO (but not PMC). However, it cannot be realized as a linear rank aggregation rule, and it is arguably impractical for RLHF.

Our work builds on an earlier study by Noothigattu et al. , which explores the axiomatic properties of reward functions defined as MLE estimators of underlying random utility models. The key difference is that their approach allows for general reward functions, not just linear ones, and they do not consider features at all. Unlike our findings, they show that the BTL model satisfies Pareto Optimality under these conditions. Additionally, they find that pairwise majority consistency is violated even without assuming linearity. However, their results strongly depend on varying the number of comparisons across different pairs of candidates. By contrast, our findings demonstrate that pairwise majority consistency is violated even when the number of comparisons is equal across all pairs of candidates.

## 2 The Linear Social Choice Model

Let \(C\) be a set of \(m\) distinct prompt/responses, referred to as _candidates_, and let \(V=\{1,,n\}\) be a set of \(n\) human participants, known as _voters_. We denote by \(^{d}\) the \(d\)-dimensional real space in which both candidate feature vectors and chosen parameter vectors lie.

Each candidate \(c C\) is associated with a distinct feature vector \(_{c}^{d}\). A parameter vector \(^{d}\) induces a linear reward function \(r_{}:C R\) defined by taking the dot product with feature vectors \(r_{}(c)=,_{c}\). We will primarily be interested in how these parameterized functions rank the candidates by reward. Let \(R^{a b}=\{ r_{}(a) r_{}(b)\}\) be the region where the reward of \(a\) is at least as large as that of \(b\). Note that \(R^{a b}\) and \(R^{b a}\) split \(^{d}\) into two half spaces, separated by the hyperplane orthogonal to \(_{a}-_{b}\). Parameter vectors \(\) on the hyperplane have \(r_{}(a)=r_{}(b)\), while rankings in the interior of either half-space strictly rank one over the other.

For a ranking \(\) over the candidates, we say that \(\) induces \(\), denoted \(\), if \(a_{}b\) implies \(r_{}(a) r_{}(b)\). Let \(R^{}=\{\}\) be the set of vectors \(\) that induce it. Note that this can be written as the intersection of corresponding half spaces \(R^{}=_{a,b:a_{}b}R^{a b}\). Further, the collection of \(\{R^{}\}\) essentially form a partition of \(^{d}\), covering the space and intersecting only at their boundaries.

We call a \(\)_non-degenerate_ if it is fully on one side of each of the separating hyperplanes, i.e., \(r_{}(a) r_{}(b)\) for all \(a,b C\). Non-degenerate parameter vectors lie in the interior of some \(R^{}\), and thus induce exactly one ranking. We call \(\)_feasible_ if \(R^{}\) has a nonempty interior, i.e., is induced by some nondegenerate \(\).3

Each voter \(i V\) submits a ranking over the candidate \(_{i}\). We assume that the feature space is rich enough that voter preferences can be captured via non-degenerate parameter vectors. In other words, we assume that each \(_{i}\) is feasible. We refer to the vector of voter rankings \(=(_{i})_{i V}\) as a _profile_. Further, for two candidates \(a,b\), we write \(n_{a b}():=|\{i V a_{_{i}}b\}|\) for the number of voters that prefer \(a\) to \(b\), and \(w_{a b}()=n_{a b}()/n\) for the proportion of such voters. When the profile \(\) is clear from context, we may shorten these to \(n_{a b}\) and \(w_{a b}\), respectively.

We define a _parameter aggregation rule_ as a function that takes as input a profile \(\) and outputs a parameter vector \(^{*}\). Our goal is to design parameter aggregation rules such that \(r_{^{*}}\) satisfies desirable properties with respect to the voter preferences. However, as the properties we care about will only be with respect to how \(r_{^{*}}\)_- ranks_ the candidates, it will be more convenient to work with what we call _linear rank aggregation rules_ that take as input a profile \(\) and output a _feasible_ ranking \(\). There is a natural way to interpret a parameter aggregation rule as a linear rank aggregation rule, namely, output any feasible ranking induced by \(^{*}\). The exact properties of the parameter aggregation rule could in principle be sensitive to the tie-breaking of non-degenerate outputs, however, all of our results will be robust to such tie-breaking.4

We pay special attention to a prominent family of rules from social choice theory referred to as \(C1\)_rules_, whose outputs depend only on majority relationships, i.e., they only need to know for each pair of candidates \((a,b)\) whether the majority prefers \(a\) or \(b\).

In our study, we examine several axioms borrowed from social choice theory to evaluate the reasonableness (fairness) of our aggregation mechanisms. These axioms include:

**Definition 2.1** (Pareto Optimality).: _A linear rank aggregation rule \(f\) satisfies Pareto optimality if, whenever every voter prefers candidate \(a\) over candidate \(b\) on \(\), i.e., \(w_{a b}()=1\), then candidate \(a\) is ranked higher than candidate \(b\) in the output ranking, i.e., \(a_{f()}b\)._

**Definition 2.2** (Pairwise Majority Consistency (Pmc)).: _A ranking \(\) is called a PMC ranking for profile \(\) if for all \(a,b C\), \(a_{}b\) if and only if a majority of voters rank \(a_{_{i}}b\), i.e., \(w_{a b}>1/2\). A linear rank aggregation rule satisfies PMC if, when a PMC ranking \(\) exists for the input profile \(\) and \(\) is feasible, then \(f()=\)._

Note that a PMC ranking for each \(\) need not exist, but when one does, it is unique. The words "\(\) is feasible" allude to the possibility that no non-degenerate parameter vector \(\) induces the unique PMC ranking. Indeed, we have such an example; see Appendix B for details.

Our research question, then, is whether these axioms can be simultaneously satisfied by _linear_ rank aggregation rules. Our approach seeks to provide a concrete illustration of how theoretical insights from social choice can inform practical algorithm design in RLHF.

Loss-Based Rules

### Standard Loss Formulation

We begin our study of linear social choice by considering a quite broad yet natural class of rules that capture how RLHF is currently being done. Their core idea is the following: when considering parameter vector \(\), for each voter \(i\) that ranks a pair of candidates \(a_{i}b\), we should incur some loss for giving \(b\) a higher reward than \(a\). To formalize this, let \(:\) be a _loss function_, which we assume is nonnegative. We can then choose a parameter vector minimizing

\[(;,)=_{a b C}n_{a b}()(r_ {}(b)-r_{}(a)).\]

Note that the BTL model fits within this framework using \((x)=(1+e^{x})\), i.e., _binary cross-entropy loss_.5 One caveat to this approach, however, is that an optimal \(\) need not be well-defined: it is possible that no minimum is attained. Fortunately, since we only care about rankings induced by optimal parameter vectors, we can conveniently remedy this by saying the output is any ranking that is induced by parameter vectors that are arbitrarily close to optimal. More formally, we say that a linear rank aggregation rule \(f\)_minimizes_\(\) if for all \(=f()\),

\[_{ R^{}}(;,)=_{}(;,).\] (1)

Even if no minimum is attained, there is always a choice of feasible ranking \(\) such that Equation (1) is satisfied.

With this definition in hand, we proceed to our first main result, which spells rather bad news for this class of rules: _Any_ loss-based aggregation rule using a nondecreasing and convex loss function (of which BTL is one, and hinge loss is another) will fail our two core axioms, PMC and PO. This paints a negative picture for current RLHF methods with respect to their social choice guarantees. Note that we will exclude the discussion of loss functions with a global minimum at zero, like ReLU, because the loss minimizer will be zero, making all rankings vacuous consequently. And we have focused on convex loss functions due to their practical optimization ease.

**Theorem 3.1**.: _If a linear rank aggregation rule \(f\) optimizes a loss function \(\) that satisfies \(_{x}(x)<(0)\) and is either nondecreasing and weakly convex, or strictly convex (and possibly nonmonotone), then \(f\) fails PMC and PO._

Proof.: Fix a loss function \(\) satisfying the theorem conditions. Note that since \(\) is convex, we may also assume it is continuous [24, Corollary 10.1.1]. Furthermore, since \(_{x}(x)<(0)\), we know that there exists \(x 0\) such that \((x)<(0)\). The case where \(x>0\) is relatively simple (as such loss functions lead to unnatural behavior), and we handle it at the end of the proof. For now, we assume that there exists \(x<0\) such that \((x)<(0)\). Note that this also implies that for all \(y 0\), \(\) is lower bounded by the affine linear function connecting \((x,(x))\) and \((0,(0))\), and thus, \(_{x}(x)=\).

We begin with a small instance of just three candidates \(C^{core}=\{a,b,c\}\) to gain some traction on how \(\) behaves. We will later extend this instance with additional candidates to demonstrate a profile where PO and PMC fail. The candidates will have feature vectors \(_{a}:=(2,1)\), \(_{b}:=(1,1)\), and \(_{c}:=(0,0)\), respectively. Furthermore, a \(p\)-fraction of voters (for \(p\) to be chosen later) will rank \(a b c\), while the remaining \((1-p)\)-fraction will have inverted preferences, ranking \(c b a\).6

Let

\[^{core}():=_{x y C^{core}}w_{x y}(r_{ }(y)-r_{}(x))\]with \(w_{x y}\{1-p,p\}\) be the loss function on this instance (scaling \(n_{x y}\) down to \(w_{x y}\) leads to an equivalent formulation). Let \(g(x)=p(-x)+(1-p)(x)\). Note that we can rewrite \(^{com}\) as

\[^{core}()=g(r_{}(a)-r_{}(b))+g(r_{}(a)-r_{ }(c))+g(r_{}(b)-r_{}(c)).\]

Note that \(r_{}(c)=0\) for all \(\), so we can simplify this to

\[^{core}()=g(r_{}(a)-r_{}(b))+g(r_{}(a))+g( r_{}(b)).\]

We will consider an unconstrained version of this problem where we are free to choose rewards \(r_{a},r_{b}\) arbitrarily, and later show by which vectors \(\) these optimal values can be induced. That is, we will first find \(r_{a},r_{b}\) minimizing

\[^{unconstr}(r_{a},r_{b}):=g(r_{a}-r_{b})+g(r_{a})+g(r_{b}).\]

Let \(OPT^{core}=\{^{core}()=_{^{}} ^{core}(^{})\}\) and \(OPT^{unconstr}(r_{a},r_{b})^{unconstr}(r_{a},r_{b})= _{r_{a}^{},r_{b}^{}}^{unconstr}(r_{a}^{},r_{ b}^{})\}\) be the set of minimizers for these two loss functions. In Appendix A, we establish the following results about these optimal sets.

**Lemma 3.2**.: _There exists a rational \(p(1/2,1]\) and values \(A_{1}<A_{2}\) with \(A_{2}>0\) such that \(OPT^{unconstr}\) is nonempty and for all \((r_{a},r_{b}) OPT^{unconstr}\), \(r_{a}>A_{2}\) and \(r_{b} A_{1}\)._

**Lemma 3.3**.: _Suppose Lemma 3.2 holds for values \(p,A_{1}\) and \(A_{2}\), then, for this same choice of \(p\), \(OPT^{core}\) is nonempty and there exist \(A_{3}\) and \(A_{4}\) with \(A_{3}>0\) such that for all \((_{1},_{2}) OPT^{core}\), \(_{1}>A_{3}\) and \(_{2}<A_{4}\)._

We will now explicitly construct a family of instances with candidate feature vectors parameterized by a value \(\) such that for sufficiently small \(>0\), the output of \(f\) fails the two axioms. Fix \(p,A_{3}\) and \(A_{4}\) from Lemma 3.3, and choose \(\) with \(0<<1\) such that \( A_{4}-A_{3}<0\) (\(<A_{3}/A_{4}\) works if \(A_{4}>0\), and otherwise, any \(0<<1\) will do).

Each instance will have six candidates, which we will think of as two groups of three, \(C=C^{core} C^{copies}\). The first group \(C^{core}=\{a,b,c\}\) will be the same as the three-candidate instance from above, while the second group \(C^{copies}=a^{},b^{},c^{}\}\) will be new. The candidates \(a,b,c\) will still be located at \(_{a}:=(2,1)\), \(_{b}:=(1,1)\), and \(_{c}:=(0,0)\), respectively. The candidates \(a^{}\), \(b^{}\), \(c^{}\) will be located near their undecorated counterparts at \(_{a^{}}:=_{a}+(-,0)\), \(_{b^{}}:=_{b}+(-,0)\) and \(_{c^{}}:=_{c}+(-,)\).

Next, we describe the voter preferences. A \(p\)-fraction of voters will have the ranking \(a a^{} b b^{} c^{} c\), and the remaining \((1-p)\)-fraction of voters will have ranking \(c^{} c b^{} b a^{} a\). As long as \(0<<1\) (which will be the case for our final chosen \(\)), these are both feasible rankings. The former is induced by the nondegenerate feature vector \((1,1)\)7 and the latter by \((-1,0)\).8

For each \(\), let

\[^{}()=_{x y C}w_{x y}(r_{ }(y)-r_{}(x))\]

with \(w_{x y}\{0,1-p,p,1\}\) be the loss function we are optimizing using candidate locations parameterized by \(\).

We will show that for sufficiently small \(>0\), \(_{ R^{c^{} c}}^{}()>_{ }^{}()\). This means that \(f\) must output a ranking with \(c c^{}\). Observe that this is a PO violation because all voters agree that \(c^{} c\). Furthermore, this is a PMC violation because a majority of voters have the ranking \(a a^{} b b^{} c^{} c\), yet this is not the output.

Let \(OPT()\) be the set of vectors optimizing \(^{}\). The rest of the proof will follow from the following two lemmas, whose proofs are in Appendix A.

**Lemma 3.4**.: \(OPT(0) c}}\)_._

**Lemma 3.5**.: _Suppose \(OPT(0) c}}\), then, for sufficiently small \(>0\), \(_{: R^{c^{} c}}^{}()> _{}^{}()\)._Finally, we handle the case that exists \(x>0\) such that \((x)<(0)\). Note that by convexity, this implies that for all \(y<0\), \((y)>(0)>(x)\), so \(_{y 0}(y)<_{y}(y)\). Now, consider an instance with two candidates \(\{a,b\}\) located at \(_{a}=(1,0)\) and \(_{b}=(0,1)\), and a single voter ranking \(a b\) (feasible via the parameter vector \((1,0)\)). It is possible to achieve a loss of \((x)\), e.g., by outputting the parameter vector \((0,x)\). On the other hand, any \(\) inducing the ranking \(a b\) will be lower bounded by \((0)>(x)\) from above. Hence, \(f\) must output \(b a\), which is both a PO and PMC violation. 

### Majority-Based Loss Formulation

Despite the negative results for loss-function-based rules, we may hope for a remedy using slightly different information. Specifically, we consider a similar loss-based function that rather than getting penalized for disagreeing with each voter only gets penalized if it disagrees with a majority of voters. That is, we choose \(\) minimizing

\[^{maj}(;,)=_{a b C} w_{a b }()>1/2(r_{(b)}-r_{(a)}).\]

Defining a parameter aggregation function based on this loss suffers from the same caveat as before, that in some cases no optimal \(\) exists. Nevertheless, we can apply an analogous fix for a ranking variant. We say that a linear rank aggregation rule \(f\)_minimizes \(\) in the majority formulation_ if for all \(=f()\),

\[_{: R^{}}^{maj}(;,)=_{ }^{maj}(;,).\]

We first show (in Appendix A.5) that this does indeed help achieve PMC with essentially all loss functions.

**Theorem 3.6**.: _Fix a nondecreasing loss function \(\) with \((0)>_{x}(x)\). If a linear rank aggregation rule \(f\) minimizes \(\) in the majority formulation, then \(f\) satisfies PMC._

Note that if the \((0)>_{x}(x)\) condition is not satisfied, i.e., \((0)=_{x}(x)\), then _all_ linear rank aggregation rules \(f\) minimize \(\) in the majority formulation, so satisfying this is a vacuous condition. Indeed, the parameter vector \(\) of all \(0\)s achieves optimal loss of \((0)\) for each pair and is consistent with every ranking \(\). Therefore, the condition \((0)>_{x}(x)\) is as innocuous as possible to rule out these edge cases.

However, despite this good news for PMC, we show that this does not help in achieving PO. In fact, our negative result extends to every \(C1\) linear rank aggregation rule. Note that if \(f\) minimizing \(\) in the majority formulation breaks ties consistently (i.e., if multiple feasible rankings are optimal, then it consistently chooses the same one), then it is C1. We then have the following result, whose proof is relegated to Appendix A.6.

**Theorem 3.7**.: _All \(C1\) linear rank aggregation rules fail PO._

This result is quite unfortunate, because if there were a rule that is both \(C1\) and PO, we would automatically achieve PMC: Whenever there is a feasible PMC ranking, a \(C1\) rule cannot distinguish between this profile and a profile where all voters submit this ranking, hence, under the PO criterion, it must output it. Furthermore, whenever there is a PMC ranking, outputting it is necessarily PO, as for every pair, a majority of voters agree with the PMC ranking. Interestingly, in the proof, we construct a profile which has a PMC ranking, yet it is not feasible, and no matter how a \(C1\) linear rank aggregation rule breaks ties, there is an underlying profile in which this output violates PO.

## 4 Social Choice-Based Rule

In light of the above negative results, in this section, we ask whether there are linear rank aggregation rules that concurrently satisfy our two core axioms, PO and PMC. We answer this question affirmatively by presenting a new method based on a prominent rule from voting theory.

The _Copeland rule_ assigns a _Copeland score_ to each alternative equal to the number of other alternatives it beats in a pairwise competition, i.e., the score for \(a\) is \(|\{b\ |\ w_{a b}>1/2\}|\). It then ranks the candidates in descending order according to their Copeland scores (breaking ties arbitrarily). Itis known that Copeland satisfies PO, PMC, and additional axiomatic properties. However, in linear social choice, since not every ranking is feasible, we cannot always output the Copeland ranking.

We, therefore, define a new linear rank aggregation rule, which we call _leximax Copeland_. This rule chooses a feasible ranking as follows. It ranks first the candidate with the highest Copeland score that can be feasibly ranked first under some parameter vector \(\). Subject to this first position, it ranks second the candidate with the highest Copeland score which can be feasibly ranked second, and continues this process for subsequent positions.

Copeland's rule is a \(C1\) rule because it only requires the majority relationships between the candidates. Analogously, leximax Copeland is also a \(C1\) linear rank aggregation rule. Therefore, by Theorem3.7, it does not satisfy the PO criterion. To address this issue, we define a variant called _leximax Copeland subject to PO (LCPO)_, which incorporates the PO criterion. Under LCPO, for every pair of alternatives where one dominates the other, the rule restricts rankings to place the dominating alternative above the dominated one.

The rule remains well-defined since the set of feasible rankings when enforcing the PO criterion is non-empty, as whenever \(a\) dominates \(b\), all the rankings in the input profile rank \(a\) above \(b\). Note that if the Copeland ranking is feasible, then this rule outputs that ranking, since unrestricted Copeland satisfies PO.

In addition to PO and PMC, we wish to show that LCPO satisfies two additional properties, which we define presently.

**Definition 4.1** (majority consistency).: _A linear rank aggregation rule satisfies majority consistency if when a candidate \(a\) is ranked first by a majority of voters in the input profile, \(a\) is ranked first in the output ranking._

Majority consistency ensures that the collective decision reflects the preference of the majority when there is a clear favorite. This principle aligns with PMC, but specifically focuses on the majority's favorite alternative. However, as we discussed above, a PMC ranking does not necessarily exist, and even when it exists, it is not necessarily feasible. By contrast, when a majority winner exists, this candidate is necessarily ranked first by a majority of voters in the input profile, who themselves (by assumption) submit feasible rankings. Therefore, we need not handle the case where it is impossible to rank the majority winner first.

**Definition 4.2** (winner monotonicity).: _A linear rank aggregation rule satisfies winner monotonicity if, when a candidate \(a\) is ranked first in the output ranking, elevating \(a\) in any voter's preference does not cause \(a\) to lose their top position in the updated aggregate ranking._

Winner monotonicity ensures that improving a leading candidate's position among individual voters will not result in that candidate's demotion.

We now state and prove the main result of this section.

**Theorem 4.3**.: _LCPO satisfies PO, PMC, majority consistency and winner monotonicity._

Proof.: LCPO trivially satisfies PO since it always outputs a ranking that respects the PO criterion. Moreover, since Copeland satisfies PMC, and whenever Copeland's ranking is in the domain, leximax Copeland subject to PO returns this ranking, it clearly satisfies PMC.

Note that if an alternative \(a\) is ranked first by at least half of the voters, then \(a\) has the highest Copeland score, meaning that leximax Copeland subject to PO will rank this candidate first if this is possible. We see that this is indeed possible, by noticing that there is at least one feasible ranking in the input profile where \(a\) is ranked first, and any such input ranking is feasible (by assumption) and satisfies the PO requirement. Therefore, majority consistency is satisfied.

It remains to show that LCPO satisfies winner monotonicity. Suppose that on input profile \(\), the rule outputs a ranking \(\) where candidate \(a\) is ranked first. Now, consider a profile \(^{}\) which is similar to \(\) with the only exception being a ranking in which \(a\) is placed in a higher position. Let \(S\) be the set of agents that ranked above \(a\) in Copeland's ranking under \(\) and let \(S^{}\) be the set of agents that ranked above \(a\) in Copeland's ranking under \(^{}\). Note that \(S^{} S\), since when moving from \(\) to \(^{}\), only the Copeland score of \(a\) can increase, and therefore it is not possible for a candidate \(b\) to beat \(a\) under \(^{}\) but not under \(\).

Now, suppose that \(R\) and \(R^{}\) are the set of rankings that satisfy the PO criterion with respect to \(\) and \(^{}\), respectively. We show that \(R^{} R\). First, note that since \(a\) is ranked first under \(\), no alternative dominates \(a\) in \(\), as otherwise the PO criterion would be violated. Therefore, we get that no other alternative dominates \(a\) in \(^{}\) as well. Moreover, note that if \(b\) dominates \(c\) in \(\), then this remains true in \(^{}\) as well. On the other hand, it is possible that \(a\) dominates an alternative \(b\) in \(^{}\) but not in \(\). From all of the above, we conclude that \(R^{} R\).

Since \(a\) is ranked first in \(\), we get that for every candidate \(b S\), there is no ranking in \(R\) in which \(b\) is ranked first, since otherwise, LCPO would output such a ranking. This also means that for every candidate \(b\) in \(S^{}\), there is no ranking in \(R^{}\) in which \(b\) is ranked first, since \(R^{} R\) and \(S^{} S\). Moreover, note that every ranking in \(R\) in which \(a\) is ranked first is also in \(R^{}\) since it satisfies all the PO restrictions of \(^{}\). Therefore, under \(^{}\), LCPO outputs a ranking in which \(a\) is ranked first. 

Leximax Copeland subject to PO can be implemented in polynomial time by solving \(O(|C|^{2})\) relatively small linear programs. Specifically, given an input profile, we sequentially choose the candidate that is ranked in position \(r+1\) as follows. We denote by \(_{r}\) the partial ranking, where the first \(r\) positions have been fixed. For each candidate \(c\) that has not been ranked yet, we want to check if there is a parameter vector that adheres to the partial ranking \(_{r}\), respects the Pareto optimality criterion and ranks \(c\) at position \(r+1\). Since all these constraints can be expressed as pairwise comparisons, we can use a linear program such as the one described in Footnote 4 to check if such a feasible ranking exists. Among the candidates meeting this criterion, we select the one with the highest Copeland score for position \(r\).

## 5 Discussion

We conclude with a discussion of several extensions and limitations of our approach and results.

First of all, we wish to emphasize that our results are theoretical. While they highlight some shortcomings of the current practice of RLHF, our goal was not to "outperform" existing RLHF methods. Rather, we see our model as giving a framework for understanding and comparing rules and methods -- it is a (useful, we believe) lens through which researchers and engineers can examine their AI alignment methods.

Second, as written, our model has voters give their complete rankings, while in practice, this would be infeasible. In the real world, we are likely to elicit only relatively few pairwise comparisons per person. For our negative results, this assumption only makes them stronger: the BTL model fails both PO and PMC _even_ with access to complete voter rankings. By contrast, for the positive results, specifically implementing leximax Copeland subject to PO, this ostensibly seems like a serious limitation. However, the complete rankings are not necessary for computing this rule, rather, all we need to know are PO dominance relationships and majority directions. We can therefore apply the rule whenever we can approximate this information, for example, through sampling. An alternative approach is to infer a complete ranking of each voter by fitting a parameter vector based on their pairwise responses; this process of learning a complete ranking and then running voting rules has been used before in a variety of settings [20; 18].

Third, our work initiates the study of the axiomatic method in our linear social choice model. However, we leave open many questions about which axioms are compatible and finding rules that achieve them. It should be clear by now that the primary challenge in linear social choice is that not every ranking over the candidates can be output. This means that essentially all known aggregation rules cannot directly be used without at least some modification. A natural direction to tackle is to try to find methods of converting known voting rules into linear aggregation ones while maintaining some of their axiomatic properties. To this end, we conclude with some preliminary results, and somewhat surprising findings within this space.

Some rules which optimize over rankings can be naturally transformed. For example, consider the Kemeny rule, which returns the ranking with the smallest pairwise disagreement over all votes. This can easily be transformed to the linear setting by simply outputting the optimal feasible ranking. In fact, in C.2, we show that this rule carries over the property of _separability_,9 a social choice axiom that is violated by Copeland (in the classical setting) and leximax Copeland subject to PO (in our setting). We show this in Appendix C.1. However, quite strikingly, although separability remains, this transformation makes Kemeny no longer PO (Appendix C.2).

Finally, note that the "leximax" portion of leximax Copeland can be seen as a general purpose tool for mapping traditional rules to linear aggregation rules. In Appendix C.3, we explore leximax plurality (run leximax on the ranking of candidates by plurality scores), and show that it satisfies majority consistency, winner monotonicity, and separability. Additionally, the "subject to PO" can be seen as another "tool" for enforcing the Pareto optimality criterion when a rule does not independently satisfy it. However, enforcing PO can again cause somewhat surprising results. For example, in Appendix C.2, we show that linear Kemeny subject to PO, while now trivially satisfying PO, again violates separability. These observations indicate the challenges inherent in linear social choice, and we hope these open questions inspire fruitful follow-up research.