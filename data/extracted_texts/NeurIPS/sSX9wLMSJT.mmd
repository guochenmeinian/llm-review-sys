# Probabilistic Active Few-Shot Learning in

Vision-Language Models

 Anton Baumann\({}^{1}\)1 Marcus Klasson\({}^{2}\) Rui Li\({}^{2}\) Arno Solin\({}^{2}\) Martin Trapp\({}^{2}\)

\({}^{1}\)Technical University of Munich \({}^{2}\)Aalto University

Work done during an internship at Aalto University.

###### Abstract

Pre-trained vision-language models (VLMs) have shown to be an useful model class for zero- and few-shot learning tasks. In this work, we investigate probabilistic active few-shot learning in VLMs by leveraging post-hoc uncertainty estimation and targeted support set selection. To equip VLMs with a notion of uncertainty on the target task, we utilize a Laplace approximation to the posterior of the VLM and derive a Gaussian approximation to the distribution over the cosine similarities. Further, we propose a simple adaptive target region selection based on k-nearest neighbour search and evaluate on a series of selection strategies from the Bayesian experimental design literature. Our experiments on standard benchmarks show that leveraging epistemic uncertainties leads to improved performance and that further improvements can be obtained by targeting the selection towards the query region.

## 1 Introduction

The rise of foundation models  has led to their increasing adoption in downstream tasks where data is scarce . Moreover, in many real-world settings it is imperative that predictions are reliable and that sources of uncertainties are captured and incorporated to avoid failure modes. The paradigm of _active few-shot learning_ (or _active fine-tuning_)  aims to tackle the challenge of actively selecting a support set (training set for adaption) that is most informative for the downstream task. However, classical approaches, _e.g._, from the coreset literature  or information theory , typically do not incorporate all sources of uncertainties into their metric of informativeness. Recent works in Bayesian active learning  aim to address this issue by performing selection of support set candidates based on their effect on the epistemic uncertainty of the model  or the predictive distribution . Moreover, progress in Bayesian deep learning  has resulted in methods that can efficiently estimate epistemic uncertainties in a post-hoc manner , making them particularly attractive for active few-shot learning of large scale models.

In this work, we investigate probabilistic active few-shot learning for vision-language models (VLMs) and show benefits of incorporating uncertainties in the support set selection process as well as targeting the selection towards the query region. For this, we propose an uncertainty estimation-based approach by leveraging a Laplace approximation  to the posterior of a pre-trained CLIP  model. We derive a Gaussian approximation to the distribution over cosine similarities between the image and text embeddings, and investigate different scoring mechanisms for the support set candidate selection. In addition, we propose a simple adaptive target region selection based on \(k\)-nearest neighbour (\(k\)-NN) search. In our experiments, we evaluate two few-shot classification settings _(i)_ support set selection from a large cross-domain training data source and _(ii)_ selection from the training set. We find improved performance over naive selection for uncertainty-based selection methods and further improvements when the selection is based on an adaptive target region.

Fig. 1 illustrates the setting we are considering in this work: Given a pre-trained VLM, we aim to predict labels for a query set of images of a novel downstream task. The VLM agent \(_{0}\) is asked to first estimate its uncertainty over the predictions on the query set, where the difficulty of the prediction is proportional to the predictive uncertainty. To avoid failure modes, the agent can select a small number of labelled support set candidates \(\) from a large data source and use them to update its internal state. Finally, the updated model \(_{1}\) is used to predict the labels for the query set.

Our main contributions are the following: _(i)_ We propose a post-hoc method for obtaining a distribution over the cosine similarities from a pre-trained VLM without needing architecture changes or further training. _(ii)_ We apply our method in active learning and assess various scoring mechanisms for support set selection. _(iii)_ We show on benchmark data sets that accounting for epistemic uncertainties improves performance and that targeted candidate selection results in further improvements.

## 2 Methods

We denote vectors by bold lower-case letters (_e.g._, \(,\)) and use bold upper-case letters for matrices (_e.g._, \(,\)). Further, sets are denoted in upper-case calligraphic letters (_e.g._, \(,\)) and model parameters or hyper-parameters are denoted using Greek letters (_e.g._, \(,\)). In particular, let \(_{i}^{p_{}}\) and \(_{j}^{p_{}}\) denote the \(i^{}\) image and \(j^{}\) text description, respectively. Further, we use \(:^{p_{}}^{d_{}}\) and \(:^{p_{}}^{d_{}}\) to denote the image and text encoders of the VLM, where \(p_{}\) and \(p_{}\) denote the respective input dimensionality and \(d_{}\), \(d_{}\) is the dimensionality of the respective feature space. The embeddings are projected into a joint space, given as \(=()\) and \(=()\), using linear projections denoted by \(^{d d_{}}\) and \(^{d d_{}}\), respectively.

VLMs (_e.g._, ) are typically trained by minimizing the InfoNCE loss , which is the sum of two cross-entropy terms, one for each relational direction--image to text (Img\(\)Txt) or text to image (Img\(\)Txt). The loss is given as \((,)=}{{2}}_{}^{ }(,)+}{{2}} _{}^{}(,)\) with cross-entropy loss terms defined over the cosine similarities between the embeddings, _i.e._,

\[_{}^{}(,)= _{i=1}^{n}-(}_{i}^{}}_{i})}{ _{j=1}^{n}(}_{i}^{}}_{j})}),\] (1)

where \(}\) and \(}\) are the unit-length normalized embeddings. For further details see App. B.1.

In this work, we utilize post-hoc uncertainty estimation based on the Laplace approximation  to estimate uncertainties over the model parameters. This approach has found increasing application in contemporary deep learning (_e.g._, [8; 20; 25]) and uses a Gaussian approximation to the posterior distribution. Utilising a Laplace approximation allows us to induce uncertainty over the feature embeddings of both encoders and results in a distribution over cosine similarities, which in turn enables quantifying model uncertainties in a principled manner. Fig. 2 illustrates the propagation of uncertainties in our setup by estimating uncertainties over the projection matrices.

Laplace approximationOne of the main computational challenges associated with the Laplace approximation is related to the estimation of the Hessian matrix of the log joint w.r.t. the model parameters. Since a naive approach is computationally impractical in the case of VLMs, we chose to estimate the Kronecker-factored Generalized Gauss-Newton (GGN) approximation [33; 24]. Moreover, we apply the Laplace approximation only for the projection matrices \(\) and \(\) of the image and text encoders. Hence, resulting in GGN approximations \(_{}\) and \(_{}\) given in form of their Kronecker factors, see App. C.1 for details.

However, naively applying Laplace approximations in VLMs is challenging as the contrastive loss entangles \(\) and \(\), which further complicates the estimation of the Hessian. These models are

Figure 1: Illustration of the setting.

also typically trained with mini-batch sizes of around \(30k\) samples. In order to compute the GGN approximations in VLMs, we simplify the contrastive loss \(\) used for pre-training by assuming independence between \(\) and \(\). Specifically, we treat each of the two loss terms independent and consider only \(_{}^{}\) for the image encoder and \(_{}^{}\) for the text encoder in the Laplace approximation. Hence, dropping interactions between the image and text encoders in the Laplace approximation. Lastly, we use an incremental computation of the Kronecker factors to account for large mini-batch sizes. Further details and derivations are given in App. C.1.

Distribution over cosine similaritiesAs the Laplace approximation uses a Gaussian approximation, the feature embeddings are distributed according to another Gaussian distribution. Specifically, the distribution over embedding vectors \(\) (or \(\)) for a datum \(\) (or \(\)) can be expressed as follows due to linearity, _i.e._,

\[(,(()^{}_{}^{-1} ())_{}^{-1})(,(()^{}_{}^{-1}() )_{}^{-1}),\] (2)

where \(\) and \(\) denote the Kronecker factors of the GNN approximation of the Hessian matrix, respectively. Unfortunately, the distribution over cosine similarities is in general not Gaussian. However, by assuming independence between the elements of \(\) and \(\) and in the limit of \(d\) we can approximate the distribution over cosine similarities to be Gaussian distributed. We find this approximation to work well in practice, while not accurately capturing the skewness of the distributions. A detailed derivation and empirical results on the approximation quality are given in App. C.2.

Targeted support set selectionLet \(_{}=\{_{i}^{*}\}\) with \(_{i}^{*} p(^{*})\) be a set of unseen test data (query set) with unknown class labels. We aim to find a set \(\{(_{j},_{j})\}_{j}^{m}\) of support candidates of cardinality \(m\) with \(_{j},_{j} p(_{j},_{j})\) such that we reduce uncertainty over the class labels of \(_{}\). To approach this problem, we target the selection process towards the predictive distribution of the query set. In particular, we propose to use a \(k\)-nearest neighbours selection in the joint space to pre-select support set candidates based on the Wasserstein distance between the distributions over image embeddings. After pre-selection, we quantify the information gain of the support set candidates either using the entropy over the predictive distribution, the expected predictive information gain (EPIG, ), or the BALD score . Doing so adaptively targets the candidate search for the the support set towards the predictive distribution of the query set and reduces the computational complexity of the selection process. Further details on the selection process and the score functions are given in App. D.

## 3 Experiments

To evaluate our approach for probabilistic active few-show learning, we conducted experiments using pre-trained OpenCLIP models from Hugging Face . We estimated the Laplace approximations of the OpenCLIP model with ViT-Base backbone and ViT-Huge backbone  using a randomly sampled subset from the Laion-400M data set . Further details are given in App. E.

For probabilistic active few-shot learning with VLMs we consider the task of image classification and present results on the Flowers102 , Food101 , CIFAR-100 , ImageNet-R , EuroSAT

Figure 2: Illustration of uncertainty propagation in VLMs: We estimate uncertainties over the projection matrices of both encoders using a Laplace approximation, which induces distributions over the feature projections. We then approximate the distribution over cosine similarities by a Gaussian.

, and the Office-Home  data sets. To assess the performance of the proposal, we investigated the following questions: _(i)_ Do approaches that account for epistemic uncertainties improve performance? _(ii)_ What is the effect of targeting the support set candidates towards the query region? _(iii)_ How does the model capacity affect the performance of the proposed approach?

To address these questions, we performed support set selection from all training domains available in the Office-Home data set and evaluated on the test set (query set) of each domain independently. In Fig. 3 we compare the performance of targeted entropy-based support set selection, random selection, random selection with targeted support set region, and the best performing (according to the validation loss) acquisition function that incorporates epistemic uncertainties. We find that incorporating epistemic uncertainties improves the few-shot learning performance in most cases and generally outperforms random selection. Further, we observe that targeted support set selection improves the performance as indicated by the performance gap between naive random selection and targeted random selection and that the model capacity can have a substantial impact on the performance gains across all approaches. A listing of the results using the negative log-predictive density are given in App. E.2.

Single-domain FinetuningIn App. E.2, we show results for single-domain finetuning on standard benchmark data sets (_e.g._ CIFAR-100, Imagenet-R, Flowers102, etc.) using the different support set selection methods with the OpenCLIP model. The selection methods using the epistemic uncertainty (BALD and EPIG) perform better or on par with the Targeted Maximum Entropy across the different subset sizes and data sets, which demonstrates the benefits of using our proposed uncertainty estimates for support set selection.

## 4 Discussion and Conclusion

In this work, we have introduced a probabilistic active few-shot learning approach for VLMs. Our approach leverages a Laplace approximation to the posterior of the projection layers of the VLM to estimate epistemic uncertainties. We have further introduced an adaptive targeted support set candidate selection based on \(k\)-NN selection using the Wasserstein distance between the distributions over image embeddings in the joint space. To assess the performance of probabilistic active few-shot learning in VLMs, we have conducted two sets of experiments, one in the cross-domain setting on the Office-Home data set and one in the single-domain setting on standard benchmark data sets. We found that incorporating epistemic uncertainties improves the few-shot learning performance in most cases and generally outperforms random selection. Moreover, targeting the selection process towards the query region provides further improvements in all cases.

Figure 3: Results on the Office-Home data set with support set selection from all training domains. We observe that incorporating epistemic uncertainties (—) improves over entropy based targeted selection (—) in most of the cases and outperforms naïve random selection (- - -) and random selection with targeted support set candidates (—). Shaded regions indicate the std over 5 runs.

ReproducibilityThe code for the experiments is available at: https://aaltoml.github.io/BayesVLM/.

AS and RL acknowledge funding from the Research Council of Finland (grant number 339730). MT acknowledges funding from the Research Council of Finland (grant number 347279). MK acknowledge funding from the Finnish Center for Artificial Intelligence (FCAI). We acknowledge CSC - IT Center for Science, Finland, for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through CSC. We acknowledge the computational resources provided by the Aalto Science-IT project.