# Xu Min\({}^{4}\), Xiaolu Zhang\({}^{4}\), Jun Zhou\({}^{4}\), Chongxuan Li\({}^{1,2}\)

Lower Bounds of Uniform Stability in Gradient-Based Bilevel Algorithms for Hyperparameter Optimization

Rongzhen Wang\({}^{1,2}\), Chenyu Zheng\({}^{1,2}\), Guoqiang Wu\({}^{3}\),\({}^{1}\) Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\) Beijing Key Laboratory of Big Data Management and Analysis Methods

\({}^{3}\) School of Software, Shandong University \({}^{4}\) Ant Group

{wangrz,cyzheng,chongxuanli}@ruc.edu.cn; guoqiangwu@sdu.edu.cn; minxu.mx@antgroup.com; {yueyin.zxl,jun.zhoujun}@antfin.com

Correspondence to Chongxuan Li.

###### Abstract

Gradient-based bilevel programming leverages unrolling differentiation (UD) or implicit function theorem (IFT) to solve hyperparameter optimization (HO) problems, and is proven effective and scalable in practice. To understand the generalization behavior, existing works establish upper bounds on the uniform stability of these algorithms, while their tightness is still unclear. To this end, this paper attempts to establish stability lower bounds for UD-based and IFT-based algorithms. A central technical challenge arises from the dependency of each outer-level update on the concurrent stage of inner optimization in bilevel programming. To address this problem, we introduce lower-bounded expansion properties to characterize the instability in update rules which can serve as general tools for lower-bound analysis. These properties guarantee the hyperparameter divergence at the outer level and the Lipschitz constant of inner output at the inner level in the context of HO. Guided by these insights, we construct a quadratic example that yields tight lower bounds for the UD-based algorithm and meaningful bounds for a representative IFT-based algorithm. Our tight result indicates that uniform stability has reached its limit in stability analysis for the UD-based algorithm.

## 1 Introduction

Hyperparameters significantly influence the convergence behavior of learning algorithms as well as the efficiency and generalization performance of the trained model . _Hyperparameter optimization_ (HO) algorithms aim to find the best hyperparameters (associated with the optimized model parameters) on a validation set. Classical approaches for HO include grid search , random search , Bayesian optimization , and evolutionary algorithms , which often suffer from the problem of scaling up. Recently, gradient-based methods have achieved excellent empirical performance in high-dimensional HO problems .

In gradient-based methods, HO is formulated as a bilevel programming problem. The inner level seeks the best model parameters on the training set given current hyperparameters. In the outer level, hyperparameters are optimized with gradient descent. However, the gradient is difficult to compute as it requires differentiating the optimized model parameters w.r.t. the hyperparameters. Two mainstream strategies have been developed to obtain this Jacobian by explicitly _unrolling differentiation_ (UD)  or approximately applying the _implicit function theorem_ (IFT) .

To investigate the underlying reason for their success, existing work establishes generalization upper bounds based on algorithmic stability [17; 18]. In particular,  presents a generalization framework associated with a notion of uniform stability for general bilevel programming in HO and stability upper bounds for the UD-based algorithm. Despite their efforts, such algorithms have not been fully understood and one of the key unsolved problems is whether existing stability analyses are tight.

To this end, this paper establishes lower bounds on the stability of gradient-based bilevel programming algorithms for HO. Technically, we begin by introducing _lower-bounded expansion properties_ which inherently characterize the instability in general update rules including stochastic gradient descent (SGD) as detailed in Section 4. Our expansion properties, to a certain degree, mirror those introduced by , with the distinction being our emphasis on lower bounds rather than upper bounds. This approach not only enables a comparative analysis with upper bounds to evaluate their alignment (i.e., tightness) but also lays down a conceptual framework for analyzing the lower bounds of algorithmic stability, generally applicable to both single-level SGD and various bilevel algorithms.

Building upon these properties, we explore the stability of the UD-based algorithm in Section 5. We first present a recursive stability lower bound that aligns with the existing upper bound at the outer level given the expansion properties of the compound validation loss, followed by an analysis of the Lipschitz constant of the inner output to maximize those expansion coefficients. Guided by these theoretical insights, we construct a quadratic example that yields a tight lower bound for the UD-based algorithm with constant step sizes and a nearly tight lower bound with linearly decreasing step sizes with respect to key factors. Meaningful bounds for a representative IFT-based algorithm are also provided in Appendix C based on its essential connection to UD-based methods. We highlight that the example is carefully designed to obtain explicit stability lower bounds by overcoming the challenges posed by the intricate behavior of the bilevel algorithms, i.e. the dependence of each outer-level update on the current turn of inner optimization.

We outline our contributions as follows: (1) We introduce lower-bounded expansion properties that can serve as general tools for analyzing lower bounds of the stability in gradient descent. (2) To our knowledge, we present the first lower bounds of uniform stability for both the UD-based and representative IFT-based algorithms, facing the challenge posed by the intricate formulation of the outer update in bilevel optimization. (3) Our lower bounds match existing upper bounds for the UD-based algorithm, verifying that uniform stability has reached its limit in characterizing the generalization of the UD-based algorithm. Detailed results are summarized in Table 1.

## 2 Related work

Algorithmic stability [20; 21] measures the change in the model output when a single training example is replaced. It is shown to be sufficient and necessary for learnability in certain cases . Stability-based generalization analysis of an algorithm typically consists of three key elements: a notion of stability, a stability-based generalization bound, and a stability analysis depending on the algorithm. Below, we introduce the related work based on these three elements.

**Algorithmic stability.** introduce uniform stability, which characterizes the worst-case change of loss and presents a stability-based generalization bound with high probability. Notable efforts [24; 25; 26; 27] have been made to obtain sharper bounds for uniformly stable algorithms in general. Besides the uniform stability, various notions of stability that characterize the average change , local change , or change in the hypothesis [29; 30] are investigated for fine-grained analyses.

**Stability of stochastic gradient descent (SGD).** SGD has been one of the workhorses in deep learning and therefore attracted much attention. To unravel the mystery behind its success,  analyze its (randomized) uniform stability on (strongly) convex and nonconvex losses.  analyze the uniform stability of (S)GD for nonsmooth convex losses and provide sharp upper and lower expectation bounds. [32; 30] consider the on-average stability for SGD and build data-dependent generalization bounds to explain the effectiveness of practical techniques like proper initialization.

Recent work establishes lower bounds on the uniform stability of SGD and investigates the tightness of corresponding upper bounds.  proves a general minimax optimal lower bound for stability generalization error together with optimization error on convex and smooth losses.  finds the general technique proposed by  is sub-optimal in convex but nonsmooth cases, and providessharper lower bounds by constructing a special class of loss functions.  adopts a similar approach by construction to present lower bounds for smooth and potentially nonconvex loss functions.

In this paper, we focus on the smooth and nonconvex cases in HO and provide stability lower bounds by construction as  and . Compared with , we consider a more complicated and nontrivial bilevel optimization problem, where the interaction between inner and outer processes brings a significant impact on the analysis. A detailed comparison is provided in Section 5.3 and Appendix E.

**Stability for bilevel programming.** extend the notion of uniform stability to HO and analyze stability upper bounds of the UD-based algorithm, while the tightness of their result is largely open. Recently, it has been extended to the analysis of implicit gradient algorithms . This paper provides the first lower bounds, generally applying to two main categories of gradient-based HO methods. There are other bilevel optimization algorithms [12; 35; 36; 37; 38; 39; 15] and settings [2; 16; 3; 40; 41; 42; 43] where our framework can potentially be extended in future work.

## 3 Problem formulation

### Elementary notations and definitions

**Scalar, vector, and matrix.** We employ lowercase letters (e.g., \(a\)), lowercase boldface letters (e.g., \(\)), and uppercase boldface letters (e.g., \(\)) to denote scalars, vectors, and matrices, respectively. For a vector \(\), \(\|\|\) denotes its Euclidean norm. For a matrix \(\), \(\|\|\) denotes its spectral norm. Additionally, let \(_{1}_{2}\) denote \(_{1}\) and \(_{2}\) are collinear by a non-negative factor, namely, \( a 0\) s.t. \(_{1}=a_{2}\).

**Loss function.** A differentiable function \(:\) is \(L\)-Lipschitz continuous if \(,\), \(\|()-()\| L\|-\|\). It is \(\)-smooth if \(,\), \(\|()-()\|\|-\|\).

**Twin datasets.** A pair of datasets are considered _twin datasets_ if they differ in only a single data point, denoted by \(S\). Throughout this paper, we use a tilde symbol to distinguish their corresponding notions, e.g., examples \(\) and \(}\), output parameters \(\) and \(}\).

**Asymptotic notations.** Denote with \(a_{n} b_{n}\) that \(a_{n}\) is bounded above by \(b_{n}\) up to a constant factor for sufficiently large \(n\), and conversely by \(a_{n} b_{n}\). We say \(a_{n} b_{n}\) if \(a_{n} b_{n}\) and \(a_{n} b_{n}\).

### HO as bilevel programming

Denote the testing, validation, and training distributions on the data space \(\) by \(^{}\), \(^{}\) and \(^{}\), and corresponding losses by \(^{}\), \(^{}\) and \(^{}\). Since the validation phase is generally regarded as a rehearsal for testing, \(^{}\) and \(^{}\) are commonly assumed to be consistent with \(^{}\) and \(^{}\).

Given a validation set \(S^{}=\{_{i}^{}\}_{i=1}^{m}}{}(^{})^{m}\) and a training set \(S^{}=\{_{j}^{}\}_{j=1}^{n}}{}(^{})^{n}\), HO algorithms seek the best-performing hyperparameter-parameter pair on \(S^{}\). Denote by \(\) the hyperparameter in space \(\), \(\) the (model) parameter in space \(\). This process can be formulated as the following bilevel problem:

\[}*{arg\,min}_{} _{i=1}^{m}^{}(,}( );_{i}^{}),}() *{arg\,min}_{}_{j=1}^{ n}^{}(,;_{j}^{}).\]

Here, \(}()\) is selected by its training performance under the given \(\) and \(^{}(,}();)\) can be rewritten as a _compound validation loss_\((;)\) considering \(}()\) a function of \(\).

Various methods are proposed to solve this nested problem, among which gradient-based algorithms have recently achieved success in scalability [11; 3; 12]. As shown in Algorithm 1, gradient-based methods utilize SGD as the optimizer at both levels, where the primary challenge lies in the calculation of the gradient of the compound validation loss, called _hypergradient_,

\[_{}(;)=_{} ^{}(,_{K}();)+ }_{K}()}_{}_{}^{ }(,_{K}();),\] (1)where the _inner Jacobian_ involves differentiating through the inner-level optimization. To this end, the UD-based methods obtain the exact inner Jacobian by directly unrolling the inner differentiation:2

\[_{}_{K}()=-_{k=0}^{K-1}_{k+1} _{}^{2}^{}(,_{k})_{i=k+1}^{K}(-_{i+1}_{}^{2} ^{}(,_{i})).\] (2)

While representative IFT-based methods leverage the implicit function theorem and Neumann series to obtain an alternative estimation :

\[}_{K}}()=-_{K}_ {}^{2}^{}(,_{K} )_{k=0}^{K-1}-_{K}_{}^{2} ^{}(,_{K})^{k}.\] (3)

Please refer to Algorithm 1 for the whole process. Notably, this paper adopts a common theoretical assumption [19; 17] of constant inner step sizes and decreasing outer step sizes.3

```
1:Input: Initialization \(_{0}\) and \(_{0}\); training set \(S^{}\) and validation set \(S^{}\); step size scheme \(\) and \(\)
2:Output: The hyperparameter \(_{T}\) and hypothesis \(_{K}\)
3:for\(t=1\)to\(T\)do
4:for\(k=1\)to\(K\)do
5: uniformly sampling \(j_{k}\) from \([n]\)
6:\(_{k}_{k-1}-_{k}^{ }(_{t-1},_{k-1};_{j_{k}}^{})\)
7:endfor
8: uniformly sampling \(i_{t}\) from \([m]\)
9:\((_{t-1};_{i_{t}}^{})\) UD-based algorithm in Eq. (2), IFT-based algorithm in Eq. (3)
10:\(_{t}_{t-1}-_{t}\)
11:endfor
12:return\(_{T}\) and \(_{K}\) ```

**Algorithm 1** Gradient-based bilevel HO

### Generalization and stability of HO

The generalization behavior of HO algorithms characterizes the selected model's potential performance on the unseen test data. Specifically, denoting the hyperparameter output by a stochastic HO algorithm \(\) as \((S^{},S^{})\), we are interested in the difference between its expected testing risk and empirical validation risk, namely _generalization error_ defined as

\[_{}_{,S^{},S^{ }}[_{^{}}[((S^{},S^{});)]-_{i=1}^{m} ((S^{},S^{});_{i}^{})].\] (4)

Stability-based generalization theory turns this problem into measuring the algorithmic robustness. We define the notion of _uniform argument stability_ for HO algorithms, which captures the variation in algorithm outputs when replacing a single validation point.4

**Definition 3.1** (Uniformly argument stability on validation).: A stochastic HO algorithm \(\) is \(_{}\)-uniformly argument stable on validation where

\[_{}_{S^{}^{ }^{m},S^{}^{n}}_{ }\|(S^{},S^{})- (^{},S^{})\|.\] (5)

Our analysis mainly leverages this notion following , as it is the key measure for stability bounds under the Lipschitz continuous condition. \(_{}\) differs from the uniform stability \(_{}\) defined in [17; Definition 1] only by a Lipschitz constant \(L\) (i.e., \(_{} L_{}\)), and our results for \(_{}\) are also provided in Theorem 5.6 and Theorem C.7 for direct comparison with former works. Existing stability-based generalization bound [17, Theorem 1] shows that uniform stability guarantees generalization in expectation for HO algorithms that \(_{}_{}\).

Former work  constructs the first stability upper bound for UD-based HO algorithms. This result is fundamentally based on an upper-bounded recurrence relation of the distance between the outputs respectively optimized on twin validation sets, denoted by \(_{t}\|_{t}-}_{t}\|\) at the \(t\)-th step.

**Theorem 3.2** (Recursion upper bound for UD, Theorems 2 and 3, ).: _Suppose the compound validation loss \((;)\) is \(L\)-Lipschitz continuous and \(\)-smooth for all \(\), and the training loss \(^{}(,;)\) is \(^{}\)-smooth for all \(\) and \(\). Then for all \(1 t T\), \(_{}[_{t}]1+(1-1/m)_{t} _{}[_{t-1}]+L}{m},\) where \(L(1+^{})^{K},(1+^{ })^{2K}\)._

Unrolling this recursion, we directly get the stability upper bound in recursion form:

\[_{}_{t=1}^{T}_{s=t+1}^{T+1}1+_{ s}(1-1/m)L}{m}.\] (6)

As this result does not explicitly display its order w.r.t. \(T\) under decreasing step sizes \(_{t} c/t\),  further deforms Eq. (6) with the bounded loss condition to obtain \(_{} T^{ }/m\).

 analyzes a specific IFT-based algorithm, which, under certain assumptions, achieves a similar result of \(_{} T^{q}/m\) with \(q<1\). Though these stability upper bounds have been established, their tightness is rarely explored, and the stability of IFT-based algorithms remains largely open. Therefore, this paper takes a first step towards establishing stability lower bounds (namely, how unstable an algorithm can be) for both UD-based and IFT-based HO algorithms.

## 4 Expansion properties of update rules

This paper endeavors to establish tight lower bounds for uniform (argument) stability as defined in Definition 3.1, which is fundamentally the supremum of the output divergence. For iterative algorithms, this divergence accumulates recursively across the whole optimization process. Therefore, we first introduce _lower-bounded expansion properties_ in Section 4.1 to characterize update rules that will induce guaranteed divergence at each iteration. This is followed by an analysis in Section 4.2 on how the objective functions within SGD need to be structured to satisfy these properties. We will see in Section 5 that, for gradient-based HO algorithms, these properties jointly lead to a lower-bounded divergence recursion given the outer-level update properties in Theorem 5.1 and a lower-bounded Lipschitz constant of the inner output given the inner-level update properties in Theorem 5.2.

Our expansion properties correspond, to some extent, with those presented by  and the key difference lies in our focus on lower bounds rather than upper bounds. This approach not only facilitates comparisons with upper bounds to discuss their alignments (i.e. tightness) but also provides a general framework for analyzing the lower bounds of algorithmic stability.

### Lower-bounded expansion properties of general iterative algorithms

Let \(\) be a general notation for parameters (or hyperparameters) in space \(\). An update rule is a function \(G:\) that maps \(\) to its next state \(G()\), and an iterative algorithm is composed of a series of consecutive update rules. We denote two sequences of update rules by \(\{G_{t}\}_{t=1}^{T}\) and \(\{G^{}_{t}\}_{t=1}^{T}\), and the corresponding outputs by \(\{_{t}\}_{t=1}^{T}\) and \(\{^{}_{t}\}_{t=1}^{T}\).

Intrinsically, the divergence between \(_{t}\) and \(^{}_{t}\) dynamically evolves across the entire process, driven by two factors: disparity in current update rules, and difference in current parameters resulting from prior updates. Our goal is to systematically analyze how variations between two update sequences lead to substantial divergence in outputs. In the following, we introduce Definition 4.1 and Definition 4.2 correspondingly to characterize properties of update rules leading to increasing divergence.

**Definition 4.1** (\(\)-divergent).: Two update rules \(G\) and \(G^{}\) are \(\)_-divergent along_\(\) if for all \(\),

\[G()-G^{}()}{{=}},\|G( {w})-G^{}()\|.\]

**Definition 4.2** (\(\)-growing).: An update rule \(G\) is \(\)_-growing along_\(\) if for all \(,^{}\) such that \(-^{}\) parallel with \(\),

\[G()-G(^{})}{{=}}-^{ },\|G()-G(^{})\|\|-^{}\|.\]Intuitively, \(\)-divergent update rules produce sufficiently divergent output parameters and a \(\)-growing update rule scales the divergence between parameters with a sufficiently large factor. The direction \(\) is chosen as the most expansive direction, as detailed with a concrete example in Section 5.4.

### Lower-bounded expansion properties of SGD

One-step SGD can be generally formulated as \(G_{,}()=-()\), where the loss function directly impacts this gradient-based update rule. We now define the \(\)-expansive property for the loss function which leads to the growing property of SGD.

**Definition 4.3** (\(\)-expansive).: A differentiable function \(:\) is \(\)-expansive along \(\) if for all \(,^{}\) that \(-^{}\) parallel with \(\), there exists \(_{,^{}}\) such that

\[()-(^{})=-_{,^{}} (-^{}).\]

This paper mainly focuses on the case when \(>0\) where the loss function is nonconvex. We have \( 0\) for the convex case. When \(>0\), Definition 4.3 connects to \(\)-strongly concavity.5 These concepts are equivalent in the one-dimensional case. In general, \(\)-strongly concavity imposes uniform curvature in all directions, while \(\)-expansiveness restricts concavity in only one direction with an additional restriction for the colinearity of \(()-(^{})\) and \(-(-^{})\). See Appendix G.2 for details. We illustrate a simple loss function in Fig. 1 that satisfies Definition 4.3.

Notably, the directional restrictions on the update rules in Definitions 4.1 to 4.3 simplify the lower-bound calculations as it enables us to focus only on the norm of the divergence at each step and get rid of directional variation, which aids in a clearer understanding of divergence dynamics. As a first attempt to establish stability lower bounds for bilevel optimization problems, our work leaves open whether these conditions can be relaxed. A potential approach might involve requiring the divergence to exhibit a specific directional component rather than strict alignment as in the current definitions.

The following lemma shows that the expansiveness of the loss function can induce the growing property of SGD.

**Lemma 4.4** (Growing property of SGD with expansive loss function, proof in Appendix B.2).: _Suppose \(\) is \(\)-expansive along \(\) and \(1+ 0\), then \(G_{,}\) is \((1+)\)-growing along \(\)._

Figure 2: Practical output distances vs. theoretical bounds in Theorem 5.5. We implement UD-based Algorithm 1 on Example 5.3. The output hyperparameter distances with increasing \(T\) are plotted on the horizontal axis. The upper/lower bounds with corresponding \(T\) are plotted on the vertical axis. The linear trends suggest these three values are of almost the same order w.r.t. \(T\).

Lower bounds on uniform stability in HO

Based on tools introduced in Section 4, this section proceeds to precisely characterize the stability of gradient-based bilevel HO algorithms. In Section 5.1, we provide a lower-bounded recursion of hyperparameter divergence that aligns with Theorem 3.2 given the expansion properties of the outer optimization, followed by a lower bound of Lipschitz constant of the inner output given the expansion properties of the inner optimization in Section 5.2. These findings pose insights in the construction of Example 5.3 at both the inner and outer levels to maximize the instability of HO algorithms. This quadratic example produces a tight lower bound for UD-based algorithms, detailed in Section 5.4, and meaningful bounds for IFT-based algorithms, provided in Appendix C.

### Stability lower bound given outer-level expansion properties

We first establish a uniform argument stability lower bound by considering the outer level of the bilevel programming as a single-level optimization problem w.r.t. the hyperparameters. This approach takes the compound validation loss \(\) as a whole, temporarily disregarding the dependence of this loss on the inner-level solution and the inner Jacobian.

Suppose \(S^{}\) and \(^{}\) are twin validation sets differing only on the \(i\)-th entry, and denote the sequences of update rules on \(S^{}\) and \(^{}\) as \(\{G_{_{i},_{i}}\}_{t=1}^{T}\) and \(\{G_{}_{i},_{i}}\}_{t=1}^{T}\)6 respectively. According to Definition 3.1, the uniform argument stability is lower-bounded by the hyperparameter distance after \(T\) steps, which primarily depends on the divergent property of update rules on different examples and the expansiveness of the compound validation loss. By characterizing these properties and utilizing Lemmas 4.4 and B.2, we obtain a lower bound of the stability in the following Theorem 5.1.

**Theorem 5.1** (Lower bound given outer-level expansion properties, proof in Appendix B.3).: _Suppose there exists a nonzero vector \(\) along which \(G_{_{i},_{i}}\) and \(G_{}_{i},_{i}}\) are \(2_{t}L^{}\)-divergent and \((;)\) is \(^{}\)-expansive for all \( S^{}\). Then we have \(_{}[_{t}]1+_{t}(1-) ^{}_{}[_{t-1}]+L^{}}{m}\) and_

\[_{}_{t=1}^{T}_{s=t+1}^{T+1}1+_{s }(1-1/m)^{}L^{}}{m}.\]

Theorem 5.1 echos the upper bound formulation in Eq. (6). The distinctions arise solely in the smooth/expansive coefficients \(\)/\(^{}\) and the continuous/divergent coefficients \(L\)/\(L^{}\). Consequently, the alignment of these two bounds (i.e., their tightness) hinges on the values of these coefficients. As detailed later in Section 5.2, we delve deeper into the coefficients present in our lower bound by unfolding the bilevel problem, focusing on the solution of the inner level and its Jacobian.

Further, Theorem 5.1 not only applies to all HO algorithms that employs outer-level SGD but also to single-level SGD. In the context of single-level SGD, the expansion properties can be directly inferred from the loss function, as elaborated in Appendix E.

### Lipschitz lower bound given inner-level expansion properties

Based on Theorem 5.1, our next step towards building stability lower bounds is analyzing the expansive coefficient \(^{}\) and divergent coefficient \(L^{}\) of outer-level optimization where the hypergradient is used for update. As can be observed in Eq. (1), the inner Jacobian \(_{}_{K}()\) is a key bridge between the inner and outer level that significantly influence the hypergradient. Here, we measure the lower bound of its maximum volume, i.e., the Lipschitz continuity coefficient of \(_{K}()\) regarding \(\) denoted by \(L^{_{K}}\), to provide a guarantee for the effect of the hypergradient.

Denote corresponding inner update rules as \(G_{,}\) and \(G_{^{},}\) given hyperparameters \(\) and \(^{}\). Applying them consecutively for \(K\) times, we get two sequences of inner updates. Theorem 5.2 presents how the expansion properties of the inner problem characterize the lower bound of \(L^{_{K}}\).

**Theorem 5.2** (Lower bound of Lipschitz of the inner-level solution, proof in Appendix B.4).: _Given any two hyperparameters \(,^{}\), suppose there exists a nonzero vector \(\) along which \(G_{,}\) and \(G_{^{},}\) are \(\|-^{}\|^{}\)-divergent and \(^{}(,;z)\) is \(^{}\)-expansive for all \( S^{}\) with \(^{}>0\). Then we have_

\[L^{_{K}}}}{^{}}[(1+ ^{})^{K}-1].\]

_Omitting constants that depend on \(\), \(^{}\), and \(^{}\), we get \(L^{_{K}}(1+^{})^{K}\)._

It is worth mentioning that our lower bound for \(L^{_{K}}\) is matched with its upper bound in  (see in Theorem 3 and Proposition 2), which prepares us to obtain a tight lower bound.

### An example with maximal simplification

Motivated by Theorems 5.1 and 5.2, the following example is carefully constructed, exhibiting all expansive and divergent properties as required by these theorems to establish tight lower bounds on uniform argument stability of gradient-based HO algorithms.

**Example 5.3**.: We introduce an HO problem as follows. The validation loss and training loss are given by:

\[^{}(,;)=^{}(,;)=^{}+ ^{}-y^{},\]

where \(^{d d}\) is symmetric. Denote the eigenvalues of \(\) as \(_{1}_{d}\). Let \(_{1}<0\) and \(|_{1}||_{d}|\), and \(_{1}\) be a unit eigenvector for \(_{1}\). Let \(S^{}\) and \(^{}\) be a pair of twin validation datasets differing at the \(i\)-th example where

\[_{i}=(_{i},y_{i})=(_{1},1),}_{i}=(}_{i},_{i})=(-_{1},1).\]

In this example, \(\) determines the convexity of the problem. Throughout the main text, we consider the most common nonconvex case where \(\) is indefinite and symmetric. See Appendix D for the results of (strongly) convex losses.

Notably, our example satisfies Assumption B.1 adopted for establishing the stability upper bounds where the loss functions are Lipschitz continuous and smooth. Hereafter we denote \((;)\) as \(L\)-Lipschitz continuous and \(\)-smooth, and \(^{}(,;)\) as \(^{}\)-smooth, where \(^{}=|_{1}|\).

Example 5.3 is constructed adhering to the principle of maximal simplification. Specifically, the quadratic form is essential for inducing nonconvexity. The second bilinear cross term represents the simplest scenario for interaction between hyperparameters and parameters, ensuring a non-zero inner Jacobian. The final term provides a connection for parameters and data. \(^{}\) and \(^{}\) are set to be identical here for simplicity, and our results do not fundamentally depend on their consistency.

We emphasize the role of the eigenvector (i.e., \(_{1}\)) which corresponds to the smallest eigenvalue. It represents the least convex direction, thereby offering the greatest expansiveness of the loss (see Fig. 1), and both the inner and outer optimizations attain the highest level of divergence and expansiveness in this direction. Consequently, in Example 5.3, the distinct samples in \(S^{}\) and \(^{}\) are set to align reversely with \(_{1}\) to make the HO algorithms unstable.

_Remark_.: The constructed example is required to meet two essential criteria: first, it must reveal the instability inherent in the algorithms; second, it must allow precise calculation of the smoothness coefficient \(\) and the expansion coefficient \(\) for the compound validation loss to verify the alignment between lower and upper bounds. Simultaneously satisfying these two requirements is challenging for bilevel algorithms. In Appendix G.3, we provide a ridge regression example to illustrate how the bilevel structure complicates the analysis of stability lower bounds.

### Lower bounds of UD-based algorithms

The following proposition shows that Example 5.3 induces the expansion of UD-based algorithms.

**Proposition 5.4** (Expansion properties of UD-based algorithms, proof in Appendix B.5).: _Suppose we solve Example 5.3 by UD-based Algorithm 1 with constant inner step size \(\) where \(1-_{d} 0\) and outer step size \(_{t}\). Then (1) the outer update rules \(G_{_{i},_{t}}\) and \(G_{}_{i},_{t}}\) are \(2_{t}L^{}\)- divergent along \(_{1}\), and (2) the composite validation loss \((;)\) is \(^{}\)-expansive along \(_{1}\) for all \( S^{}\), where_

\[L L^{}(1+^{})^{K},=^{} (1+^{})^{2K}.\]Combining the lower bound in Theorem 5.1 with the upper bound in Eq. (6), we instantly get

\[_{t=1}^{T}_{s=t+1}^{T}1+_{s}(1-1/m)L^{}}{m}_{}_{t=1}^{T}_{s= t+1}^{T}1+_{s}(1-1/m)L}{m},\] (7)

where the bounds are in the same order w.r.t. \(T\), \(K\) and \(m\). These matching bounds in recursion form verify the **tightness** of the existing upper bound .

Specifically, for constant step sizes, i.e., \(_{t}=c\) for all \(t\), Eq. (7) explicitly reveals the scale of \(_{}\) regarding \(T_{}1+c(1-1/m)^{T}/m\). However, for linearly decreasing step sizes \(_{t} c/t\), additional scaling steps 7 are necessary and the deformed result is provided below.

**Theorem 5.5** (Uniform argument stability of UD-based algorithms, proof in Appendix B.6).: _Solving Example 5.3 by UD-based Algorithm 1 with constant inner step size \(\) where \(1-_{d} 0\) and decreasing outer step sizes \(_{t}=c/t\) with \(c\) as a positive constant has uniform argument stability that_

\[1+(1-)c^{}}}{m} _{})c}}{m},\]

_where \(=^{}(1+^{})^{2K}\) as in Proposition 5.4._

The scaling steps unavoidably create a discrepancy between the deformed lower and upper bounds, while their quotient, \(T^{(1-)c-1+(1-)c^{}}\), is small given a small \(c\) (e.g., \(0.01\)). We compare the practical output hyperparameter distances and the theoretical bounds in Fig. 2.

Notably, the upper bound in our result is not contradictory to the existing upper bound of \(_{} T^{}/m\) in  because we remove the bounded loss assumption, i.e., \( a,b\) s.t. \([a,b]\). This modification is necessary to fairly compare the upper and lower bounds. Detailed discussion is provided in Appendix E.3.

Based on the results of uniform argument stability, we can further obtain similar results of uniform stability by introducing additional assumptions as below.

**Theorem 5.6** (Uniform stability of UD-based algorithms, proof in Appendix B.7).: _Following the same condition as in Theorem 5.5, and additionally, if the initial points \(_{0}=,_{0}=\), and \(_{1}_{j}^{}\) for any \(j[m] i\) and \(_{1}_{j}^{}\) for any \(j[n]\), then Algorithm 1 has uniform stability that \(1+(1-)c^{}}}{m} _{})c}}{m},\) where \(=^{}(1+^{})^{2K}\) as in Proposition 5.4._

Technically, we adopt these additional assumptions following  to simplify the formulation of \((_{T},)-(_{T}^{}, {z})\) by eliminating the quadratic term and reducing it to be colinear with \(_{T}-_{T}^{}\). By doing so, a clear relation can be established between the loss divergence and the hyperparameter divergence, which leads to a transfer from uniform argument stability to uniform stability.

_Remark_.: For now, we have characterized the stability error as an upper bound on the generalization error. Let us now examine how this stability-based generalization bound informs the allocation of data between the validation and training sets. Suppose we have a total of \(N\) data points, with \(m=aN\) assigned to the validation set \(S^{}\) and \(n=(1-a)N\) assigned to the training set \(S^{}\), where \(a(0,1)\). The expected population risk can be decomposed into the generalization error and the empirical validation risk as follows:

\[_{,S^{},S^{},^{}}[((S^{},S^{});^{ })]=}}_{}+ _{,S^{},S^{}}[ _{i=1}^{m}((S^{},S^{});_{i}^{})]}_{}.\]

On one hand, the generalization bound \(_{gen}_{stab}=(1/aN)\) (as in Eq. (7)) suggests that \(a\) should be sufficiently large to keep term (II) small. On the other hand, \(a\) should also be sufficiently small to get a low validation risk in term (II), since a larger training set generally improves validation performance. Thus, selecting \(a\) involves a trade-off to optimize the overall population risk.

Conclusion and discussion

This paper establishes novel lower bounds of the uniform stability for various HO algorithms and shows the existing upper bound in UD-based algorithms is tight. This result indicates that the notion of uniform stability has reached its limit in stability analysis for the UD-based algorithm. The lower-bounded expansion properties proposed in this paper can serve as general tools for analyzing lower bounds of stability. This paper applies them to both single-level and bilevel optimization. We also discuss in detail potential extensions of our analysis framework on establishing average stability lower bounds and generalization lower bounds in Appendix H.

**Limitations and social impacts.** This paper is constrained in the scope of smooth loss functions, while non-smooth scenarios  remain open. Moreover, a uniform stability lower bound does not directly imply a generalization lower bound. This gap exists as algorithmic stability is inherently introduced as a theoretical tool for analyzing the generalization upper bound. Alternative approaches might include directly deriving a generalization lower bound with examples considering the data distribution. This paper is a purely theoretical work, we have not identified any direct, significant societal impacts that must be emphasized.