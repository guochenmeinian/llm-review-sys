# Memory Efficient Optimizers with 4-bit States

Bingrui Li\({}^{1}\), Jianfei Chen\({}^{1}\), Jun Zhu\({}^{1}\)

\({}^{1}\)Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab,

Tsinghua-Bosch Joint ML Center, Tsinghua University

lbr22@mails.tsinghua.edu.cn; {jianfeic, dcszj}@tsinghua.edu.cn

Code is available at https://github.com/thu-ml/low-bit-optimizersCorresponding author.

###### Abstract

Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizers are evaluated on a wide variety of benchmarks including natural language understanding, machine translation, image classification, and instruction tuning. On all the tasks our optimizers can achieve comparable accuracy with their full-precision counterparts, while enjoying better memory efficiency.+

Footnote â€ : Corresponding author.

## 1 Introduction

Large-scale models with a massive amount of parameters  have shown impressive few-shot learning abilities on general tasks . Despite being powerful, training these models is challenging. Memory capacity is one of the main bottlenecks of training large-scale models. Modern neural networks are typically trained with stateful optimizers such as Adam , which need to maintain one or two optimizer states (i.e., first and second moments) per each parameter. As the model size grows, the memory consumed by optimizer states can be a dominating factor of memory consumption .

There are several attempts to reduce optimizers' memory consumption. Factorization  applies low-rank approximation to optimizer states, delta tuning  avoids maintaining optimizer states for most parameters by only tuning a small subset, and low-precision optimizers  represent their states with low-precision numerical formats, which consume less memory.

Among these methods, low-precision optimizers are attractive due to their simplicity and wide applicability. Dettmers et al.  propose an 8-bit optimizer with reparameterized embedding layers ("stable embedding layers") and a block-wise 8-bit dynamic exponential numerical format for optimizer states. Their 8-bit optimizers achieve similar convergence to full-precision optimizers on language modeling, image classification, machine translation, and language understanding tasks.

In this work, we push the required numerical precision of low-precision optimizers from 8 to 4-bit through analyzing the patterns in the first and second moments and designing dedicated quantizersfor optimizer states. Specifically, we find that moments exhibit complicated outlier patterns, that vary across different parameter tensors. The large block size proposed by Dettmers et al.  cannot properly handle all different outlier patterns. Based on this observation, we propose to use a smaller block size, which improves the approximation of first moment.

For the second moment, we find its quantization suffers from a _zero-point problem_. Since the update direction is usually inversely proportional to the square root of the second moment, quantizing non-zero quantities to zero will cause significant deviation. To address this problem, we propose a simple linear quantizer to exclude the zero-point for second moment. We further propose a stronger quantization technique, _rank-1 normalization_, to improve the approximation of second moment by better handling the outlier patterns. Our proposed quantization techniques are robust enough to achieve lossless convergence under 4-bit, even without the stable embedding layers proposed by Dettmers et al. .

Finally, we investigate the combination of factorization methods [2; 46] with low-precision optimizers, and propose a memory efficient optimizer which utilizes quantized 4-bit first moment and factorized second moment. For applicable tasks, the hybrid optimizer enjoys best of both worlds: good convergence and memory efficiency.

We evaluate our 4-bit optimizers on a wide range of tasks, including natural language understanding, machine translation, image classification, and instruction tuning of large language models. On all the benchmarks, our 4-bit optimizer can converge similarly fast with full-precision optimizers, and the converged models do not have noticeable accuracy loss. Our optimizers consumes less memory than existing 8-bit optimizers , while improves the throughput of language model finetuning with optimizer offloading [41; 45] due to reduced communication cost.

## 2 Preliminaries

In this section, we present some preliminaries of compression-based memory efficient optimizers and discuss quantization methods for compression of optimizer states in a general formulation.

### A Framework for Compression-based Memory Efficient Optimizers

Gradient-based stateful optimizers like SGDM [38; 47], Adam , AdamW  are the principal choices in deep neural network training. However, the memory footprint of stateful optimizers is several times of model itself, which results in a bottleneck for large model pretraining/finetuning. Consider the update rule of the Adam optimizer:

\[(_{t-1},_{t-1},_{t-1},_{t} )=_{t}&=_{1}_{t-1}+(1-_{1})_{t}\\ _{t}&=_{2}_{t-1}+(1-_{2})_{t}^{2}\\ }_{t}&=_{t}/(1-_{1}^{t})\\ }_{t}&=_{t}/(1-_{2}^{t})\\ _{t}&=_{t-1}-}_{t}/(}+)\] (1)

During the training process, the model parameters \(_{t}\) and optimizer states (i.e., first and second moments) \(_{t}\), \(_{t}\) need to be stored persistently in the GPU memory. As the model grows large, optimizer states are a main source of training memory consumption. Each parameter dimension takestwo 32-bit optimizer states, making the memory consumption of Adam-style optimizers three times larger than stateless optimizers like SGD.

Compressing the optimizer states is a promising method to achieve memory efficient optimization. Formally, given a gradient-based optimizer \(}\), a memory efficient version of the optimization algorithm with compressed optimizer states is given by Alg. 1. The algorithm \(\) can be any gradient-based optimizers like SGDM, Adam, AdamW, etc. See App. F for examples. In Alg. 1, the precise states \(_{t}\) are only temporal, and only compressed states \(}_{t}\) are stored persistently in the GPU memory. Memory footprint reduction can be achieved since in neural networks, the state vectors \(_{t}\) and \(_{t}\) are usually concatenation of state vectors of each parameterized layer. Therefore, we can perform the optimizer steps (Line 3-5) separately for each layer, so only one single layer of the precise states are presented in the memory at a time. The states for all other layers are kept compressed. In the rest of this paper, we focus on the compression and decompression method, to achieve high compression rate of optimizer states while maintaining good convergence of the optimizer.

### Main Compression Method: Quantization

Quantizing the optimizer states to lower precision (e.g. 8-bit integers) is an effective way to compression optimizer states . In this case, the optimizer states are compressed with a quantizer and decompressed with a dequantizer. The low-precision numerical format significantly impacts the accuracy of quantization methods. Here, we present a general framework for numerical formats we considered for compressing optimizer states.

A quantizer converts full-precision tensors to low-precision formats. Based on the formulation proposed by Dettmers and Zettlemoyer , we disentangle the quantizer \(()\) into two parts: normalization \(()\) and mapping \(()\), which applies sequentially and element-wisely to a tensor to be quantized. For concise presentation, we only discuss quantizers for unsigned inputs, and defer the discussion of signed inputs in App. E.1. Formally, the quantizer for a tensor \(^{p}\) is given by

\[q_{j}:=(x_{j})=(x_{j}).\]

NormalizationThe normalization operator \(\) scales each elements of \(\) into the unit interval, i.e. \(\). Normalization can have different granularity, such as per-tensor, per-token (row) [37; 55], per-channel (column) , group-wise [37; 55] and block-wise . The per-tensor and block-wise normalization operators are given by

\[n_{j} :=_{}(x_{j})=x_{j}/\{|x_{i}|: 1+B j/B i B( j/B +1)\},\]

respectively, where the involved scaling factors are called _quantization scale_, which are persistently stored together with quantized tensor until dequantization. The granularity of normalization presents a trade-off of quantization error and memory overhead. Normalization method with low quantization error and acceptable memory overhead is preferred. In this case, the coarsest per-tensor normalization operator has negligible memory overhead, i.e. only 1 scaling factor regardless of tensor size, but suffers from largest error due to outliers. Block-wise normalization views the tensor as an 1-dimensional array, divides the array into blocks of size \(B\) called block and assigns a quantization scale within each block, which leads to \( p/B\) quantizaton scales totally. The block size could be adapted to trade-off quantization error and memory overhead.

MappingA mapping  converts normalized quantities to low-bitwidth integers. Formally, the mapping operator \(=_{,b}\) is equipped with a bitwidth \(b\) and a predefined increasing mapping, named _quantization mapping_\(:[0,2^{b}-1]\). Then \(\) is defined as

\[q_{j}:=(n_{j})=_{0 i<2^{b}}|n_{j}-(i) |.\]

The design of \(\) is critical as it could effectively mitigate quantization error by capturing the distribution information of \(\). There are two kinds mappings that are of specific interest to optimizer states quantization, linear mapping and dynamic exponent (DE) mapping . A linear mapping \((i)=(i+1)/2^{b}\) defines a linear quantizer, where the quantization intevals distribute evenly in each block. The DE mapping can approximate small values well, similar to floating point numbers. DE splits the binary representation of a low-precision integer \(i\) into three parts: a leading sequence of \(E\) zeros, followed by an indicator bit one, and remaining \(F\) fraction bits. DE defines \((i)=10^{-E(i)}(i)\), where \((i)[0.1,1]\). See App. E.2 for full specifications and visualizations of different quantization mappings.

The normalization \(\) and mapping \(\) roughly play the same role in finding good quantization point candidates and they affect each other. If an oracle normalization scaling the original tensor \(\) to a uniform distribution is accessible, linear mapping could be used generally. On the contrary, if the optimal mapping could be readily identified with respect to certain metrics for a per-tensor normalized tensor, there is no necessity to develop additional normalization methods that incur extra overhead for mitigating the negative effects of outliers. In essence, if one of the two operators approaches optimality, the quantizer can still perform well even when the other operator is set to its most basic configuration. Additionally, as one operator becomes increasingly powerful, the potential for improvement in the other operator gradually diminishes.

DequantizationThe dequantizer is just the inverse of the quantizer, which is simply

\[_{j}:=^{-1}(q_{j}).\]

Based on our formulation, we name quantizers by their normalization and mapping methods as Norm./Map.. For example, 8-bit optimizers  use block-wise normalization with a block size of 2048 and dynamic exponent mapping, which we call B2048/DE in the rest of the paper.

## 3 Compressing First Moment

In the next two sections, we describe our design of compression and decompression methods in Alg. 1 to realize our memory efficient 4-bit optimizers. We first discuss the compression of the first moment. Our compression method for first moment is based on Dettmers et al. , which uses block-wise normalization with a block size of 2048 and dynamic exponent mapping . We preliminary reduce the bitwidth from 8-bit to 4-bit and discover that the first moment is rather robust to quantization. This simple optimizer can already converge with 4-bit first moment, though in some cases there is accuracy degradation.

To further improve the performance, we investigate the patterns of the first moment. There are some outliers in the moments and outliers significantly affect quantization scale due to their large magnitude. Outlier patterns have been studied for weights and activations. It is shown that the weights are rather smooth [14; 54; 57], while the activation have column-wise outliers [4; 14; 53; 54], i.e., the outliers in activation always lie in some fixed channels. However, we find that the outlier patterns in optimizer states are quite complicated. Specifically, Fig. 2 shows patterns of first moment in a Swin-T model during training. Outliers of the layers.3.blocks.0.mlp.fc1 layer lie in fixed rows while outliers of the layers.3.blocks.0.mlp.fc2 layer lie in fixed columns. Actually, the outlier patterns vary across different architectures, layers and parameters. See more patterns in App. B.

The complicated outlier pattern makes optimizer states harder to quantize. There exist some moment tensors, that the outliers persist roughly in certain columns (dimension 1), as shown in Fig. 1. Block-wise normalization treats this tensor as an flattened one-dimensional sequence, in the row-first order. Therefore, any block size of 2048 would include an entry in the outlier column, resulting

Figure 1: Visualization of the first moment in the layers.3.blocks.1.mlp.fc1 layer in a Swin-T model. (a): Magnitude of the first moment. (b): Histogram of the first moment. (c): Moment approximated by B128/DE. (d): Moment approximated by B2048/DE.

in a large quantization scale. In this case, block-wise normalization is not better than per-tensor normalization. Therefore, we adopt a smaller block size of 128, as it provides enhanced performance while incurring only a little memory overhead. In Fig. 1, we show that quantizing with a smaller block size approximates the moments better than a large block size.

## 4 Compressing Second Moment

Compared to the first moment, quantizing the second moment is more difficult and incurs training instability. Besides its sharper outlier pattern and ill-conditioned distribution compared with first moment, we further identify the _zero-point problem_ as the main bottleneck of quantizing the second moment. We propose an improved normalization method with a quantization mapping excluding zero. We also propose a factorization method for compressing the second moment, which leads to even better memory efficiency.

### Zero-point Problem

The problem of quantizing second moment different from quantizing other tensors in neural networks. For weights , activations , and gradients , it is desirable to contain zero in the quantization mapping for lower approximation error and training stability. Empirically, zero is often the most frequent element . But for second moment in Adam, small values around zero significantly impact the update direction, which is proportional to the inverse square root of the second moment, as shown in Eq. 1. In this case, a small quantization error of values around zero will cause catastrophically large deviation of the update direction. Fig. 3 shows the histogram of the inverse square root (i.e., transformed with \(h(v)=1/(+10^{-6})\)) of the second moment quantized with B128/DE. The quantizer pushes most of entries of the tensor to zero, so the inverse square root of most points fall into \(10^{6}\) due to zero-point problem, resulting in a complete degradation in approximation. One remedy is to simply remove zero from the DE quantization map, which we call DE-0. The smallest number representable by DE-0 is 0.0033. With the B2048/DE-0 quantizer applied to second moment, the approximation of second moment is more precise (Fig. 3), and the training with 4-bit optimizer stabilizes (Tab. 1). However, by removing zero, DE-0 wastes one of the \(2^{4}=16\) quantization points. We propose to use a Linear mapping \((i)=(i+1)/2^{b}\), whose smallest representable number is \(0.0625\). The linear mapping performs better than DE-0 for quantizing second moment.

In Tab. 1, we ablate different quantization schemes and show that excluding zero from the mapping is indeed the key factor for second moment quantization, which cannot be replaced by a smaller block size or/and stochastic rounding . We also test Stable Embedding layers proposed by Dettmers et al. , which are reparameterized embedding layers which can be more stably optimized. While Stable Embedding could improve training stability, it cannot fully retain accuracy since non-embedding layers still suffer from zero points. On the other hand, when the zero-point problem is properly addressed, Stable Embedding is no longer required to retain accuracy.

### Rank-1 Normalization

We propose an empirically stronger normalization method named _rank-1 normalization_ based on the observation of the heterogeneous outlier pattern in Sec. 3, inspired by the SM3 optimizer . Formally, for a matrix-shaped non-negative (second moment) tensor \(^{n m}\), its 1-dimensionalstatistics \(^{n}\) and \(^{m}\) are defined as \(r_{i}=_{1 j m}x_{i,j}\) and \(c_{j}=_{1 i n}x_{i,j}\), which are exactly the quantization scales of per-row and per-column normalization. Rank-1 normalization utilizes two scales jointly and produce a tighter bound for entry, which is defined as

\[_{}(x_{i,j})=,c_{j}\}}x_{i,j}.\]

Rank-1 normalization could be easily generalized to high-dimensional tensors and signed tensors. See App. G for details and the pseudocode.

Compared with per-tensor, per-token (row), and per-channel (column) normalization, rank-1 normalization utilizes the 1-dimensional information in a more fine-grained manner and gives element-wisely unique quantizaion scales. It deals with the outliers more smartly and effectively when outlier persists in fixed rows or columns but the pattern across tensors are unknown and/or varied (Fig. 2). On the other hand, block-wise normalization is also capable to capture the local information and avoid outliers effectively regardless of the patterns when a small block size is taken, but rank-1 normalization provides a better trade-off between memory overhead and approximation. Rank-1 normalization falls back to per-tensor normalization for 1-dimensional tensors, so we use B128 normalization in those cases. Empirically, rank-1 normalization match or exceed the performance of B128 normalization at a moderate model scale (hidden_size=1024). It is also possible to combine block-wise and rank-1 normalization together, which we leave for future work.

### Factorization

Many memory efficient optimization methods [2; 8; 46] represent the entire second moment with a few number of statistics, which differ from quantization and they take only sublinear memory cost. These works bring more memory saving but they are only applicable to the second moment. In this work, we find the factorization method proposed in Adafactor  could also avoid zero-point problem effectively, as shown in Tab. 1. Further, we explore the effects of factorization on second moment based on quantized optimizers to attain maximal memory saving while maintain lossless accuracy. Specifically, when factorization is enabled, we factorize all second moment with dimension greater than 1, and quantize 1d second moment tensors.

## 5 Experiments

We compare our 4-bit optimizers with their full-precision counterparts, as well as other memory efficient optimizers including 8-bit AdamW 2, Adafactor  and SM3 . 8-bit AdamW's optimizer states are not quantized for embedding layers. For Adafactor, we compare both the

   Normalization & Mapping & Stable Embed.\({}^{*}\) & Factorized & Unstable(\%) & BLEU \\  B2048 & DE & âœ— & âœ— & 33 & 66.6 \(\) 0.61 \\ B2048 & DE & âœ“ & âœ— & 0 & 66.9 \(\) 0.52 \\  B128 & DE & âœ— & âœ— & 66 & 65.7 \(\) N/A \\ B128 & DE+SR\({}^{*}\) & âœ— & âœ— & 33 & 65.4 \(\) 0.02 \\ B128 & DE & âœ“ & âœ— & 0 & 67.2 \(\) 1.13 \\  B2048 & DE-0 & âœ— & âœ— & 0 & 67.5 \(\) 0.97 \\ B2048 & DE-0 & âœ“ & âœ— & 0 & 67.1 \(\) 1.02 \\ B128 & DE-0 & âœ— & âœ— & 0 & 67.4 \(\) 0.59 \\ Rank-1 & DE-0 & âœ— & âœ— & 0 & 67.5 \(\) 0.58 \\ Rank-1 & Linear & âœ— & âœ— & 0 & **67.8 \(\) 0.51** \\  Rank-1 & Linear & âœ— & âœ“ & 0 & **67.6 \(\) 0.33** \\   

Table 1: Ablation analysis of 4-bit optimizers on the second moment on the GPT-2 Medium E2E-NLG finetuning task. The first line barely turns 8-bit Adam  into 4-bit, i.e. B2048/DE for both first and second moments. We only vary the quantization scheme for second moment. SR=stochastic rounding (see App. E.3 for details). Stable Embedding layers are not quantized. 32-bit AdamW achieves a BLEU of 67.7.

\(_{1}>0\) and the \(_{1}=0\) (no first moment) configuration. For our 4-bit optimizers, we report two versions both based on 32-bit AdamW: (1) "4-bit AdamW" quantizes first moment with B128/DE and second moment with Rank-1/Linear. (2) the more memory efficient "4-bit Factor" quantizes first moment with B128/DE, factorizes second moment when the dimension of tensor is greater than 1, and quantizes lefted 1-dimensional second moment with Rank-1/Linear. See App. D for details about the experiments.

Models, datasets and hyperparametersWe report performance metrics on standard benchmarks, including image classification (CLS) with Swin-T 3 on ImageNet-1k , natural language understanding (NLU) by fine-tuning RoBERTa-L 4 fine-tuning on GLUE , question answering (QA) by fine-tuning RoBERTa-L on SQuAD [42; 43], natural language generation (NLG) by fine-tuning GPT-2 Medium 5 on E2E-NLG , machine translation (MT) by training Transformer-Base 6 on WMT14 en-de  and LLaMA  fine-tuning. We fine-tune LLaMA-7B, LLaMA-13B and LLaMA-33B  on the Alpaca dataset 6 and evaluate them on MMLLU  and standard common sense reasoning benchmarks: HellaSwag , ARC easy and challenge  and OpenBookQA .

We mainly follow the hyperparameters in the original paper or/and codebase. In each benchmark, we keep same hyperparameters in one optimizer on different quantization schemes, which gives an out-of-box transfer from full-precision optimizer to low-bit optimizer without extra hyperparameter tuning. See App. D for hyperparameters and training details.

Accuracy of 4-bit OptimizersWe first check whether our memory efficient 4-bit optimizers could retain accuracy. According to Tab. 2, our 4-bit optimizers can match or exceed 32-bit AdamW performance on all fine-tuning tasks (NLU, QA, and NLG) and are comparable on all pretraining tasks (CLS and MT). Sublinear memory optimizers Adafactor (\(_{1}=0\)) and SM3 could have better memory efficiency, but they suffer from performance degradation, particularly on the CLS task. According to Tab. 3, our 4-bit AdamW will not destroy the capability of pretrained models while enabling them to obtain instruction-following ability. 4-bit AdamW is comparable with 32-bit AdamW on all tasks and does not get worse when the model size grows. Moreover, their convergence curves closely align (Fig. 4).

Memory and Computing EfficiencyWe evaluate the memory and computation efficiency of proposed 4-bit optimizers on instruction tuning, NLU, and NLG tasks, in Tab. 4. Our 4-bit optimizer offers more memory saving compared to 8-bit optimizers, reducing the training memory consumption by up to 57.7%. It may look like the memory saving saturates when the bitwidth goes down. This is because we report the total memory consumption (including data, activations, and memory fragments)

    & NLU & CLS & NLG & QA & MT \\ Optimizer & RoBERTa-L & Swin-T & GPT-2 M & RoBERTa-L & Transformer \\ 
32-bit AdamW & 88.9 \(\) 0.01 & 81.2 \(\) 0.05 & 67.7 \(\) 0.67 & 94.6 \(\) 0.13 & 26.61 \(\) 0.08 \\ 
32-bit Adafactor & 89.1 \(\) 0.00 & 80.0 \(\) 0.03 & 67.2 \(\) 0.81 & 94.6 \(\) 0.14 & 26.52 \(\) 0.02 \\
32-bit Adafactor\({}^{}\) & 89.3 \(\) 0.00 & 79.5 \(\) 0.05 & 67.2 \(\) 0.63 & 94.7 \(\) 0.10 & 26.45 \(\) 0.16 \\
32-bit SM3 & 87.5 \(\) 0.00 & 79.0 \(\) 0.03 & 66.9 \(\) 0.58 & 91.7 \(\) 0.29 & 22.72 \(\) 0.09 \\
8-bit AdamW\({}^{}\) & 89.1 \(\) 0.00 & 81.0 \(\) 0.01 & 67.5 \(\) 0.87 & 94.5 \(\) 0.04 & 26.66 \(\) 0.10 \\ 
4-bit AdamW (ours) & 89.1 \(\) 0.01 & 80.8 \(\) 0.02 & 67.8 \(\) 0.51 & 94.5 \(\) 0.10 & 26.28 \(\) 0.05 \\
4-bit Factor (ours) & 88.9 \(\) 0.00 & 80.9 \(\) 0.06 & 67.6 \(\) 0.33 & 94.6 \(\) 0.20 & 26.45 \(\) 0.05 \\   

Table 2: Performance on language and vision tasks. Metric: NLU=Mean Accuracy/Correlation. CLS=Accuracy. NLG=BLEU. QA=F1. MT=SacreBleu. \({}^{}\): do not quantize optimizer states for embedding layers; \({}^{}\): \(_{1}=0\). See more results in App. A.

rather than the optimizer memory consumption alone. In principle, the optimizer states is 2x smaller for 4-bit AdamW than 8-bit AdamW, and about 4x smaller for 4-bit Factor.

The instruction tuning task uses two Nvidia A100 80GB GPUs, while the model is sharded across GPUs with PyTorch's FSDP. In this case, our 4-bit optimizer speeds up training due to reduced communication cost. For the smaller RoBERTa-L and GPT-2 Medium, our 4-bit optimizers appear to be slower than 8-bit AdamW. This is because we have not yet optimize our implementation with fused operators. The speed of our 4-bit optimizers should match or surpass 8-bit optimizers after operator fusion.

We report the largest OPT and LLaMA models trainable under a given memory budget with full-precision and our 4-bit optimizers in Tab. 5. Our optimizer allows for the training of 4x large OPT models, and enables the training of LLaMA-7B model with a single 80GB GPU.

Ablation StudyFinally, we investigate the effectiveness of our proposed quantization schemes and the sensitivity of each moment to quantization in Tab. 6. We see that quantizing both the first and second moment brings a marginal drop in accuracy. Smaller block size on first moment improves accuracy and takes a step towards lossless performance. Factorizing second moment improves accuracy while leads to better memory efficiency.

## 6 Related Work

Compression-based memory efficient optimizersThere have been some works trying to approximate the gradient statistics with sublinear memory cost relative to the number of parameters. Adafactor  achieves memory reduction by approximating the second-moment of matrix-shaped

Figure 4: Training loss curve of LLaMA-7B fine-tuning on Alpaca dataset (averaged over 3 runs). Only result of 4-bit AdamW is reported since all parameters are 1-dimension via FSDP packing. See more details in App. D.

   Model & Optimizer & MMLU (5-shot) & HellaSwag & ARC-e & ARC-c & OBQA \\   & Original & 33.1 & 73.0 & 52.4 & 40.9 & 42.4 \\  & 32-bit AdamW & 38.7 & 74.6 & 61.5 & 45.1 & 43.4 \\  & 4-bit AdamW & 38.9 & 74.7 & 61.2 & 44.4 & 43.0 \\   & Original & 47.4 & 76.2 & 59.8 & 44.5 & 42.0 \\  & 32-bit AdamW & 46.5 & 78.8 & 63.6 & 48.3 & 45.2 \\  & 4-bit AdamW & 47.4 & 79.0 & 64.1 & 48.0 & 45.2 \\   & Original & 54.9 & 79.3 & 58.9 & 45.1 & 42.2 \\  & 32-bit AdamW & 56.4 & 79.2 & 62.6 & 47.1 & 43.8 \\   & 4-bit AdamW & 54.9 & 79.2 & 61.6 & 46.6 & 45.4 \\   

Table 3: Performance on LLaMA fine-tuning on MMLU and commonsense reasoning tasks across different sizes.

parameters with the outer product of "row" and "column". SM3  considers the cover of parameters and maintain one statistics for each element in the cover. Experimentally, cover composed of slices of co-dimension 1 for each tensor has been adopted. Extreme tensoring , compared with SM3, has a different motivation but similar formulation for the experimental choice. These memory efficient methods only focus on memory reduction on second moment and are applicable to most of the second moment based optimizers, like Adagrad , Adam , AdamW , etc.

Another line of work achieves memory reduction by using compression-based method and maintaining coordinate-wise low-bit precision optimizer states. The stability of 16-bit Adam is firstly studied in DALL-E . Dettmers et al.  uses block-wise and dynamic exponent quantization to reduce the coordinate-wise optimizer states from 16-bit to 8-bit and is applicable to SGDM and Adam/AdamW. Compression-based methods only reduce memory by a fixed percentage, irrelevant to the number and shape of parameters. Compression-based methods are less memory efficient compared with sublinear memory methods, but exhibit superior performance across a wider range of benchmarks.

Other memory efficient techniquesSome works focus on the memory efficiency of activations. Activation compressed training [6; 29] keeps low-precision activations in GPU memory via quantization in forward phase and dequantize the low-precision activations to full-precision layer-wisely in backward phase. Gradient checkpointing  only keeps activation in a small number of layers in forward phase and recompute the activations in all layers when gradient computation is needed, which leads a trade-off between storage overhead of activations and additional computation cost. These methods can be combined with our optimizer for better memory efficiency. LoRA  freezes the pretrained weights and only tunes new initialized low-rank parameters, which greatly reduce the size of computation graph but is only applicable to language model finetuning.

Sharding  divides model parameters and optimizer states among multiple devices, allowing for more efficient model training through the use of additional GPUs. Offloading [41; 45] reduces GPU memory consumption by transferring data to CPU memory. Our optimizer can be utilized by these methods to reduce the communication overhead.

   Quant. 1st & Quant. 2nd & Factor. 2nd & Acc. \\  - & - & âœ— & 81.2 \(\) 0.05 \\ B2048/DE & - & âœ— & 80.9 \(\) 0.04 \\ B128/DE & - & âœ— & 81.0 \(\) 0.06 \\ B128/DE & Rank-1/Linear & âœ— & 80.8 \(\) 0.02 \\ B128/DE & Rank-1/Linear & âœ“ & 80.9 \(\) 0.06 \\   

Table 6: Ablation study on the impact of compressing different moments to Swin-T pretraining on ImageNet1k.

   Task & Optimizer & Time & Total Mem. & Saved Mem. \\   & 32-bit AdamW & 3.35 h & 75.40 GB & 0.00 GB (0\%) \\  & 4-bit AdamW & 3.07 h & 31.87 GB & 43.54 GB (57.7\%) \\  & 4-bit AdamW (fused) & 3.11 h & 31.88 GB & 43.53 GB (57.7\%) \\   & 32-bit AdamW & 3.93 min & 5.31 GB & 0.00 GB (0\%) \\  & 8-bit AdamW & 3.38 min & 3.34 GB & 1.97 GB (37.1\%) \\  & 4-bit AdamW & 5.59 min & 3.02 GB & 2.29 GB (43.1\%) \\  & 4-bit AdamW (fused) & 3.17 min & 3.00 GB & 2.31 GB (43.5\%) \\  & 4-bit Factor & 4.97 min & 2.83 GB & 2.48 GB (46.7\%) \\   & 32-bit AdamW & 2.13 h & 6.89 GB & 0.00 GB (0\%) \\  & 8-bit AdamW & 2.04 h & 4.92 GB & 1.97 GB (28.6\%) \\   & 4-bit AdamW & 2.43 h & 4.62 GB & 2.37 GB (34.4\%) \\   & 4-bit AdamW (fused) & 2.11 h & 4.62 GB & 2.37 GB (34.4\%) \\   & 4-bit Factor & 2.30 h & 4.44 GB & 2.45 GB (35.6\%) \\   

Table 4: Memory and Time of 4-bit optimizers compared with 32-bit AdamW and 8-bit Adam .

    &  \\  GPU Mem. & 32-bit AdamW & 4-bit AdamW \\ 
24 GB & OPT-350M & OPT-1.3B \\
80 GB & OPT-1.3B & OPT-6.7B \\
80 GB & - & LLaMA-7B \\   

Table 5: Largest trainable model under given memory budget. We use a batch size of 1 and max length of 512 for this comparison. FSDP is enabled at GPUs of 80 GB.

Conclusions, Limitations, and Broader Impact

We propose compression-based memory efficient 4-bit optimizers using quantization and factorization. We observe the heterogeneous outlier patterns in optimizer states and identify the zero-point problem in quantizing second moment as the main bottleneck. 4-bit optimizers achieve lossless performance in finetuning and comparable accuracy in pretraining on a wide range of tasks.

LimitationsThe optimal quantization setting probably depends on task, datasets, and training details. While rank-1 normalization and linear mapping for second moment quantization performs consistently well in our experiments, task-specific quantization settings not in the scope of the study might perform better and be helpful to achieve lossless performance. Our evaluation is currently limited to language and vision tasks, while the applicability of our method to reinforcement learning, audios, and graph learning tasks still needs further study.

Broader ImpactOur work can facilitate the access to large models for pretraining and finetuning, which were previously constrained by GPU memory limitations. This could help democratizing large models and opens up new avenues for research that were previously unattainable due to restricted GPU memory, especially benefiting researchers with limited resources. However, our work can also exaggerate the abuse large models.