# OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset

Shubham Toshniwal, Ivan Moshkov, Sean Narethiran, Daria Gitman,

Fei Jia, Igor Gitman

NVIDIA

###### Abstract

Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA  and MAMmoTH  are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on our proposed prompting novelty, the recent progress in open-source LLMs, and some brute-force scaling, we construct OpenMathInstruct-1, a high-quality math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing _code-interpreter_ solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best _gpt-distilled_ models. To support the open-source efforts, we have released our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.1

## 1 Introduction

The huge development and inference costs associated with general-purpose large language models (LLMs) have led to the rise of smaller, task-specific LLMs. Recent work has proposed creating these domain/task-specific LLMs by generating _high-quality synthetic data_ using powerful closed-source models such as GPT-3.5/4  and training smaller models on the generated _distillation_ data . For mathematical reasoning, our task of interest, all the current state-of-the-art open-source models are _gpt-distilled_. However, model development recipes relying on proprietary models like GPT-4 can have serious limitations: (a) legal restraints on how the finetuned models can be used,2 (b) generating data with closed-source models is typically costlier than state-of-the-art open-source models, and (c) these recipes lack reproducibility as closed-source model behaviors can vary significantly over time .

_For developing mathematical reasoning models, why are open-source models not used in place of closed-source models?_ To answer this, we compare GPT-4 with the Mixtral 8x7B model , one of the best open-source LLMs at mathematical reasoning in early 2024, by generating _code-interpreter_ style solutions for two popular mathematical reasoning benchmarks, namely GSM8K and MATH . We use the metric _training set coverage (TSC)_ to compare the models, where TSC measures the number of training problems for which any of the generated solutions leads to the ground truth answer (pass@k). Figure 1 shows the training set coverage (TSC) of the Mixtral model as a function of the number of sampled solutions. For the relatively easier GSM8K benchmark, the Mixtral model's coverage catches up to GPT-4's with almost 8x the number of solution samples. For the challenging MATH benchmark, even with 12x the number of solutions, the Mixtral model still has a lower TSC than GPT-4. This gap in the training set coverage reflects the distillation data quality and, hence, the quality of the final fine-tuned model. This explains the preference for GPT-4 in the current distillation pipelines for mathematical reasoning.

_Bridging the coverage gap between GPT-4 and Open-source LLMs_: We limit our investigation of open-source LLMs for synthesizing solutions to the Mixtral-base model due to (a) its strong performance on mathematical reasoning tasks compared to other open-source LLMs, and (b) its permissive license.3 As a first attempt, we use a brute-force approach of sampling several solutions per problem. However, this approach only scales logarithmically, limiting its effectiveness (Figure 1). Next, we explore the approach of targeted solution generation, where we write few-shot prompts focused on specific sections of the training data. Concretely, we write few-shot prompts for each mathematics subject in the MATH dataset and merge the synthesized solutions. The motivation is that these subject-specific few-shot prompts could better target the latent mathematical capabilities of these general-purpose LLMs. Unfortunately, we only find a marginal gain in TSC with this approach (Section 2.2.2). Finally, we utilize the fact that reference text solutions accompany mathematical benchmarks such as MATH and GSM8K. These reference solutions can aid the synthesis of code-interpreter style solutions. We show that using these reference solutions in our few-shot prompt with a slight modification substantially increases the coverage and, consequently, the performance of the fine-tuned model (Section 2.2.3 and 4.1.2).

Our solution synthesis experiments result in OpenMathInstruct-1, a collection of 1.8M problem-solution pairs. OpenMathInstruct-1 has a training set coverage of 93% for MATH and 99.9% for GSM8K. Table 1 shows that compared to previous mathematical reasoning fine-tuning datasets, OpenMathInstruct-1 is at least four times bigger, and, even more importantly, it is permissively licensed, allowing unrestricted usage by future work. To illustrate the quality of OpenMathInstruct-1, we train and release a range of models based on Mistral-7B , Llama 2 , and CodeLlama . In particular, the CodeLlama-70B model fine-tuned on a subset of OpenMathInstruct-1, referred to as OpenMath-CodeLlama-70B, achieves a score of 84.6% on GSM8K and 50.7% on MATH. These scores are competitive with the current best _GPT-distilled_ models. Finally, to support the open-source efforts in this direction, we have publicly released all our fine-tuned models, code, and the OpenMathInstruct-1 dataset, along with a further 6.6M incorrect sampled solutions under a commercially permissive license.4

Figure 1: Training set coverage of Mixtral model generated solutions as a function of number of solutions sampled per problem (using temperature of 1.0 and top_p = 0.95). The statistics for the training set coverage of GPT-4 are from .

## 2 Training Data Synthesis

### Overview

Setup.Let \(=\{(q_{1},a_{1}),,(q_{N},a_{N})\}\) be a typical mathematical reasoning training dataset, where \(q_{i}\) and \(a_{i}\) denote the \(i^{}\) question and answer respectively. Optionally, the training dataset may include reference text solution \(t_{i}\), which illustrates a trajectory from \(q_{i}\) to \(a_{i}\) using mathematical principles.5 Besides the data, we assume access to a foundation LLM like Mixtral-base. The goal is to generate diverse, high-quality solutions for the training set problems using the LLM: a popular recipe for reasoning tasks [21; 22]. Recent work has also attempted augmenting training set problems [1; 2], but we limit our exploration to solution synthesis for existing problems in the benchmark.

Solution Format.We use the code-interpreter format for the synthesized solutions (see Figure 5 in Appendix for a sample solution). The code-interpreter format interweaves natural language reasoning with Python code blocks. It thus combines the computation precision of coding environments with the expressiveness of natural language reasoning, which is particularly suitable for mathematical reasoning tasks [8; 23]. To demarcate the start and end of a code block, we use the strings \(\) and \(\). A code block is followed by its execution block, which is demarcated by \(\) and \(\). During inference, the model invokes the Python interpreter to run the preceding code block after generating \(\), appends the execution result in between the \(\) separators, and resumes the autoregressive model inference.6

Approach.We use few-shot prompting to synthesize solutions for the training sets of GSM8K and MATH. Formally, the prompt has the form:

\[(q_{1},c_{1}),,(q_{K},c_{K})\,q^{}\]

where \(\) represents a text-based instruction for the task, \(\{q_{1},,q_{K}\}\) represent \(K\) problems representative of the dataset, \(\{c_{1},,c_{K}\}\) represent their respective solutions in the code-interpreter format, and \(q^{}\) represents a question from the training set. Given this prompt, the base LLM generates a candidate solution \(c^{}\) for the question \(q^{}\). If \(c^{}\) leads to the correct answer for the question \(q^{}\), we add the pair \((q^{},c^{})\) to our fine-tuning set. For all our experiments, we choose \(K=5\), and the representative problems are chosen from the training set of the corresponding benchmark. In the instruction \(\), we instruct the model to output the answer inside the \(\)boxed{} block. The complete instruction is in Table 13 in Appendix B.4.

Sampling Details.We sample solutions with temperature=1.0 and top_p=0.95. We use the following constraints in our generation pipeline: (a) the total number of input-output tokens is limited to 4096, (b) a maximum of 512 new tokens after each code block, (c) a maximum of 3 code blocks, and (d) the generation halts after any code execution error. We use the TensorRT-LLM toolkit.7

   Dataset & Size & Generating LM (Permissive License) \\  Lila  & 272K & - (✓) \\ MathInstruct  & 262K & GPT-4 (✗) \\ MetaMathQA  & 395K & GPT-3.5 (✗) \\ MathCodeInstruct  & 80K & GPT-4 + Self (✗) \\ WizardMath*  & 96K & GPT-3 (✗) \\ ToRA*  & 16K & GPT-4 (✗) \\  OpenMathInstruct-1 (Ours) & 1.8M & Mixtral (✓) \\   

Table 1: Comparison of OpenMathInstruct-1 with mathematical reasoning fine-tuning datasets used by current state-of-the-art open-source models. OpenMathInstruct-1 is 4x bigger than the current largest dataset, MetaMathQA, and is the only one, except Lila, with a permissive license. Datasets marked with * have not been publicly released.

### Prompting

In the previous section, we described our solution generation pipeline. A key ingredient of this pipeline is the few-shot prompt examples. We next describe the different prompting strategies explored in this work.

#### 2.2.1 Default

We choose five representative examples of GSM8K and MATH to create the few-shot prompt for the respective datasets. For GSM8K, we use a mix of problems that require vanilla Python code and problems that are best solved using Python's _sympy_ library. For MATH, we compose a 5-shot prompt with examples from different subjects. To reflect this diversity of reasoning paths required for MATH, we choose a mix of problems that require code-based solutions, text-based solutions, and a combination of both. The prompts used for the two datasets are presented in Appendix B.6.

For GSM8K, we sample 128 solutions per training problem, which gets a training set coverage of 99.1%. For MATH, we sample 224 solutions per training problem, which only achieves a training set coverage of 80.1%. This difference in coverage reflects the difficulty of the MATH benchmark compared to GSM8K, which has been noted in previous work as well .

#### 2.2.2 Subject-specific Prompting (Subj)

_Could the diversity of mathematical topics in MATH be a reason for the low training set coverage with a single 5-shot prompt?_ To answer this question, we create subject-specific prompts for the seven subjects in the MATH benchmark, namely algebra, geometry, intermediate algebra, number theory, prealgebra, precalculus, and probability (See Table 11 in the appendix for the subject-wise split of MATH training data). The MATH benchmark also labels problems by their hardness level, with levels ranging from 1 to 5, where level 5 is the hardest. For creating subject-specific 5-shot prompts, we choose one example from each level for a given subject. For each of the seven prompts, we sample 32 solutions per problem and combine the data generated with all the prompts, which is equivalent to 32 x 7 = 224 solutions per problem. However, even with this fine-grained prompting, we only achieve a negligible gain in the training set coverage, though the total number of correct solutions increases by 14K (177K \(\) 191K, see Table 2).

Combining this fine-tuning dataset with the earlier single default prompt dataset yields a training coverage of 85.1% for MATH, a boost of 5% absolute. However, achieving this coverage required sampling almost 450 solutions per problem (224 + 224 = 448). _Can we make the solution generation pipeline more efficient?_

#### 2.2.3 Masked Text Solution Prompting (Mask-Text)

GSM8K and MATH benchmarks come with reference text solutions. Using these text solutions can, in theory, reduce the problem of code-interpreter solution generation to a translation problem from text to code. We initially experimented by prompting the LLM with:

\[(q_{1},t_{1},c_{1}),,(q_{K},t_{K},c_{K} )\,q^{},t^{}\]

   Prompt &  &  \\  & \# Samples & \# Unique Solns. & Coverage (in \%) & \# Samples & \# Unique Solns. & Coverage (in \%) \\  Default & 224 & 177K & 80.1 & 128 & 434K & 99.1 \\  + Subj & 224 & 191K & 80.1 & - & - & - \\  Mask-Text & 224 & 192K & 85.9 & 128 & 602K & 99.9 \\  + Subj & 224 & 227K & 87.5 & - & - & - \\  Total & 896 & 787K & 93.0 & 256 & 1036K & 99.9 \\   

Table 2: Statistics of _unique_ solutions generated by prompts described in Section 2.2. Default prompt refers to the single prompt used for the two benchmarks, Mask-Text refers to prompting the model with masked text solution, and Subj refers to prompting with subject-specific prompts (applicable only to MATH). Coverage % refers to the percentage of problems in the training set for which there’s at least one solution among the generated solutions.

where \(t_{i}\)'s represent the text solution of representative problem \(q_{i}\)'s and \(t^{}\) represents the text solution of the problem \(q^{}\). Using the text solution in the prompt leads to a considerable increase in training set coverage. However, our manual analysis revealed that many solutions were _shortcuts_. E.g., trivial solutions such as print(ANSWER) or The answer is ANSWER where the ANSWER is copied from the text solution \(t^{}\) in the prompt. Our attempts to filter out these trivial solutions proved challenging as there are many creative ways in which the generated solutions were cheating (see Figure 11 in Appendix).

To deter the possibility of such _shortcut_ solutions where the results of intermediate computations or the final answer from the text solution are copied, we propose prompting with a _masked text solution_. Such solutions have all numbers in intermediate computations replaced with symbols. A sample masked text solution is shown in Figure 2. These masked text solutions are generated using few-shot prompting as follows:

\[_{}\;(q_{1},t_{1},t_{1}^{}),,(q_{K},t _{K},t_{K}^{})\;q^{},t^{}\]

where \(_{}\) represents the instruction for the solution masking task, and \(\{t_{1}^{},,t_{K}^{}\}\) represent masked text solutions corresponding to \(\{t_{1},,t_{K}\}\). For a detailed overview of the masked text solution generation pipeline, we refer the reader to Appendix B.5. Using these masked text solutions in the prompts significantly boosts the training set coverage for MATH, increasing from 80.1% \(\) 85.9% for the single _default_ prompt, and 80.1% \(\) 87.5% for the subject-specific prompts. For GSM8K, it leads to the coverage increasing from 99.1% to 99.9%.

Table 2 summarizes the statistics of the solutions dataset generated via different prompts. The OpenMathInstruct-1 dataset is obtained by merging and deduplicating the problem-solution pairs resulting from the above-described prompt strategies. OpenMathInstruct-1 consists of 787K unique solutions for 6978 problems (out of 7500) in MATH and 1.04M unique solutions for 7469 problems (out of 7473) in GSM8K. To get to this final dataset, we also perform a few post-processing steps, which are described next.

### Post-processing

The generated solutions can sometimes be _syntactically noisy_ even if they lead to the right answer. We fix or remove the following solutions:

* Remove solutions with multiple \(\)boxed\(\{\}\) blocks.
* Remove solutions with the \(\)llm-code\(\) string but not the \(\)/llm-code\(\) string.
* Remove text beyond the solution line with the answer, i.e., the \(\)boxed\(\{\}\) block. See Figure 12 in the Appendix for an example solution where we perform trimming.

While these post-processing steps can fix some of the syntactic errors, filtering _semantically noisy_, i.e., solutions that get to the right answer with flawed reasoning , is a much harder problem and beyond the scope of this work. Anecdotally, we find such solutions to be rare in our corpus. See Figure 13 in the Appendix for a sample _semantically noisy_ solution.

Figure 2: A sample masked solution from GSM8K training set. The masked text solution only masks the intermediate computations, such as 9 \(\) M and 63 \(\) N, and doesn’t mask the amounts introduced in the question, such as 7, 2, and §4.

### Data Selection

OpenMathInstruct-1 on average has hundreds of solutions per problem. These solutions can have different formats (code vs. text), and problems can have very different numbers of solutions in the dataset. Careful data selection allows for reduced training times and can also benefit performance. We detail the data selection strategies explored in this work.

#### 2.4.1 Fair vs. Naive Downsampling

For a dataset like MATH, where problems have divergent difficulty levels, our solution generation strategy leads to a corpus where _easier_ problems have a lot of solutions and _harder_ problems have very few solutions (see Appendix A.2 for a detailed discussion on solution count). A _naive_ strategy for downsampling treats every instance, i.e., problem-solution pair, as an equal. This problem-agnostic sampling perpetuates the imbalance of the original corpus, as seen in Figure 3(a). We propose a _fair_ sampling alternate in which we iterate over all the problems round-robin and sample without replacement from the remaining solutions for each problem. This problem-dependent sampling ensures a more balanced representation of each problem in the downsampled dataset (see Figure 3(b)). Experimental results show that _fair_ downsampling outperforms _naive_ downsampling (Section 4.1.1).

#### 2.4.2 Code-Preferred Solutions

The code-interpreter format allows for mixing code and text, and also text-based solutions without any code blocks. For GSM8K, the proportion of text-based solutions is 2%, but for MATH, their representation is 35.1%.8 While natural language reasoning is more expressive, it lacks the precision of code-based solutions . Suppose for a problem \(q\) there are a total of \(N_{}\) correct solutions in the corpus, out of which \(N_{}\) represents the number of code-based solutions, and \(N_{}\) represents the text-based solutions. We propose the following two _code-preferential_ data selection strategies:

* _Majority-Code_: If \(N_{}>N_{}\), remove all the text-based solutions.
* _Any-Code_: If \(N_{}>0\), remove all the text-based solutions.

Ablation experiments over the MATH subset of OpenMathInstruct-1 show the benefit of _code-preferential_ data selection (Section 4.1.4).

## 3 Experimental Setup

### Training Details

For all our experiments, including ablations, models of size 34B or smaller are trained for four epochs. A global batch size of 128 is used along with the AdamW optimizer with a weight decay of 1e

Figure 3: Histogram of the number of solutions for problems in a 64K downsampled subset of MATH instances in OpenMathInstruct-1.

2  and dropout  of 0.1. We save one checkpoint per epoch for ablation experiments and two checkpoints per epoch for final model runs. The final checkpoint is created by averaging all the saved checkpoints. All experiments are performed using the NeMo toolkit9. For the full set of training hyperparameters, see Appendix B.1.

### Evaluation Setup

We evaluate our models on popular math reasoning benchmarks, namely GSM8K, MATH, GSM-Hard , SVAMP , TabMWP , ASDiv , and MAMPS . For ablation experiments and hyperparameter selection, we create a validation set of 1K examples from the training set of GSM8K and MATH since both datasets lack an actual validation set. All the fine-tuned models are evaluated in the zero-shot setting. We use greedy decoding and self-consistency/majority voting  for evaluation. For majority voting, we found that using a lower temperature of 0.7 is beneficial compared to the data generation setup. We also deviate from the data generation setup by allowing the model to continue answer generation after code execution errors.

  
**Size** & **Base Model** & **Model** & **GSM8K** & **MATH** & **GSM-Hard** & **SVAMP** & **TabMWP** & **ASDiv** & **MAMPS** \\  - & GPT-4 (Code Interpreter) & 97.0 & 69.7 & 77.6 & 94.8 & 95.9 & 92.6 & 97.7 \\   &  & WizardMath & 54.9 & 10.7 & - & 36.1 & - & - & - \\  & & MetaMath & 66.4 & 19.4 & - & - & & - & - \\   & &  & 59.4 & 33.4 & - & 71.4 & - & - & - \\  & & & ToRA & 72.6 & **44.6** & 56.0 & 70.4 & 51.6 & 78.7 & 91.3 \\  & & & + SC (k=50) & 76.8 & 52.5 & - & - & - & - & - \\  & & & OpenMath-CodeLlama & 75.9 & 43.6 & 60.1 & 79.6 & 56.0 & 77.7 & 93.5 \\  & & & + SC (k=50) & 84.8 & 55.6 & - & - & - & - & - \\  & & & MetaMath-Mist-Mist-Jb & -77.7 & 28.2 & - & - & - & - & - \\  & & & MAmmoTH-TB-Mistral & 75.0 & 40.0 & - & - & - & - & - \\  & & & WizardMath & **83.2** & 33.0 & - & - & - & - & - \\  & & & OpenMath-Mistral-7B & 80.2 & 44.5 & **63.7** & **82.4** & **70.0** & **82.7** & **95.4** \\  & & & + SC (k=50) & 86.9 & 57.2 & - & - & - & - & - \\   &  & WizardMath & 63.9 & 14.0 & - & 51.9 & - & - & - \\  & & MetaMath & 72.3 & 22.4 & - & - & - & - & - \\  & & & MAimoTH & 64.7 & 36.3 & - & 73.7 & - & - & - \\  & & & T& **48.1** & 60.5 & 75.7 & **65.4** & **81.4** & 92.5 \\  & & & + SC (k=50) & 80.4 & 55.1 & - & - & - & - & - \\  & & & OpenMath-CodeLlama & **78.8** & 45.5 & **61.9** & **78.8** & 59.7 & 81.2 & **93.6** \\  & & & + SC (k=50) & 86.8 & 57.6 & - & - & - & - & - \\   &  &  & 72.7 & 43.6 & - & **84.3** & - & - & - \\  & & & RoTA & **80.7** & **51.0** & 63.7 & 80.5 & **70.5** & **84.2** & 93.3 \\  & & & + SC (k=50) & 85.1 & 60.0 & - & - & - & - & - \\  & & & OpenMath-CodeLlama & **80.7** & 48.3 & **64.0** & 83.6 & 66.0 & 82.7 & **94.9** \\  & & & + SC (k=50) & 88.0 & 60.2 & - & - & - & - & - \\   &  &  & 81.6 & 22.7 & - & 71.8 & - & - & - \\  & & MetaMath & 82.3 & 26.6 & - & - & - & - & - \\  & & & MAmmoTH & 76.9 & 41.8 & - & 82.4 & - & - & - \\  & & & RoTA & 84.3 & 49.7 & **67.2** & 82.7 & 74.0 & **86.8** & 93.8 \\  & & & + SC (k=50) & 88.3 & 56.9 & - & - & - & - & - \\  & & & OpenMath-Llama2 & **84.7** & 46.3 & 65.7 & 85.0 & 70.8 & 84.3 & 95.6 \\  & & & + SC (k=50) & 90.1 & 58.3 & - & - & - & - & - \\  & & & & & & & & & & \\  & & & & & & & & &Results

We finetune all the models on a mixture of (a) 512K fair downsampled GSM8K instances, and (b) 512K MATH instances with _any-code_ filtering (Section 2.4).10 Thus, the total finetuning corpus size is roughly 1.02M. We justify the data selection choices later in the ablation experiments.

Table 3 compares the performance of _OpenMath-finetuned_ models against their _GPT-distilled_ counterparts. Among the 7B models, our OpenMath-Mistral-7B is competitive with all the _GPT-distilled_ models. It is second-best to WizardMath on GSM8K, and tested by ToRA by 0.1% on MATH.11 Our models easily outperform both MetaMath  and MAMmoTH , even when controlling for the base fine-tuned model. Since WizardMath and ToRA finetuning datasets are not publicly available yet, OpenMathInstruct-1 presents a superior alternative to the publicly available MetaMathQA and MathInstruct datasets, which are used to fine-tune MetaMath and MAMmoTH, respectively.

With the increase in model parameters, our models continue to outperform MetaMath and MAMmoTH substantially. Compared to ToRA, with greedy decoding, we see a meaningful drop in performance on MATH, though our models are equal or better on GSM8K. With self-consistency (SC) decoding, however, our models outperform ToRA on both MATH and GSM8K. The substantial gains with SC can be attributed to the diversity of our fine-tuning data.

### Ablations

We perform ablation experiments with the Mistral-7B as the base model. We report results on the 1K-sized validation subsets for MATH and GSM8K created by us.

#### 4.1.1 Fair vs Naive Downsampling

We finetune the base model on a dataset of 128K instances created by combining 64K naive or fair downsampled instances from the GSM8K and MATH portion of the data. Table 4 shows that the model fine-tuned on the data downsampled with fair sampling outperforms the one created by naive downsampling. The performance gap is particularly substantial for MATH, which suffers from a graver data imbalance than GSM8K in our corpus.

#### 4.1.2 Default vs Masked Prompting

We finetune the base model on a dataset of 128K instances created by combining 64K fair-sampled instances from the GSM8K and MATH portion of the data generated using default and masked prompting. Table 5 shows that the model fine-tuned on the data generated using masked prompting outperforms the one created by default prompting on both GSM8K and MATH. Thus, the gains in the training set coverage with masked prompting (Section 2.2.3) also translate to finetuning performance.

#### 4.1.3 Impact of Fine-Tuning Dataset Size

To determine the impact of the size of the fine-tuning dataset, we create datasets of size 128K/256K/512K by combining 64K/128K/256K fair downsampled equally-sized subsets of GSM8K and MATH. Table 6 shows that the performance increases for both GSM8K and MATH with the increase in the fine-tuning dataset size. We didn't find any benefit from training the models for more steps, so the performance gain is attributable to the increased data size.

   Sampling & GSM8K & MATH \\  Naive & 74.3 & 35.0 \\ Fair & **75.3** & **37.0** \\   

Table 4: Comparison of performance of Fair vs Naive downsampling on our validation subset of GSM8K and MATH.

#### 4.1.4 MATH-only Ablations

This section presents the ablation results for only the MATH portion of OpenMathInstruct-1. We finetune the base model on a 128K _fair_ downsampled subset to control for data size.

Default vs Subject-Specific Prompting.In section 2.2.2, we motivated using subject-specific prompts, which ultimately didn't result in much training set coverage difference. _But how are the solutions generated by the combination of subject-wise prompts different from a single default prompt?_ To answer this, we create a subset of 128K instances generated with the default prompt/subject-specific prompts. Table 7 compares the finetuning performance on these two splits on our MATH validation subset. While the model trained on the _subject-specific_ subset underperforms the model trained on the _default_ subset with greedy decoding, the trend is decisively reversed for self-consistency decoding with four samples. This suggests that the subset collected with subject-specific prompts has a higher diversity of solutions than the ones collected using a single prompt.

Code-Preferential Subsets.In this ablation, we determine the impact of code-preferential solution selection strategies proposed in Section 2.4.2. Table 8 shows that code-preferential solution strategies aid the greedy decoding performance. However, the reduction in solution diversity arguably results in a performance drop with self-consistency decoding (text-based solutions are about one-third of the original corpus). Based on these results and because _Any-Code_ results in a smaller finetuning dataset (512K compared to 664K with _Majority-Code_), we chose to use the _Any-Code_ subset.

## 5 Analysis

We analyze the performance of the ablation model trained on 512K instances from Section 4.1.3. We limit the discussion to the MATH benchmark, where the model scores 41.6% on our validation subset.

Performance-split by Subjects and Levels.Figure 4 presents the performance split by subjects and levels on the MATH validation subset. Among subjects, we see that the model's worst performance is on geometry, which can be attributed to the lack of multi-modality in our base models . We see a monotonic decrease in performance with the increase in hardness level which is to be expected . The model scores 72.4% on Level 1 problems and only 16.3% on the hardest problems, i.e., Level 5.

Error Analysis.Table 9 shows that the model performs an absolute 13.3% better when using code for answering questions in comparison to when not using it. We find that some of the errors made by text-based solution could have been avoided by preferring code-based solution; see Figure 16 for a sample solution where the model makes an arithmetic calculation error. This analysis provides another support for our proposal and use of code-preferred solutions from Section 2.4.2.

Table 10 presents the count of different error categories. For code-based solutions, we find that almost 74% of the errors in such solutions are due to reasoning errors, and the remaining 26% are attributable to execution-related issues. We present sample solutions from these error categories in Appendix B.3.

   Dataset Size & GSM8K & MATH \\ 
128K & 75.3 & 37.0 \\
256K & 79.0 & 38.6 \\
512K & 81.0 & 41.6 \\   

Table 6: Effect of fine-tuning dataset size on performance on our validation subset of GSM8K and MATH.

   Prompt & Pass@1 & SC (k=4) \\  Default & 39.1 & 41.7 \\ Subject & 38.3 & 44.5 \\   

Table 7: Comparison of default vs subject-wise prompt performance on our MATH validation subset.

## 6 Related Work

Mathematical Reasoning and LLMs.Recently, a plethora of work has been done on enhancing the mathematical reasoning capabilities of LLMs. Inference techniques such as Chain-of-Thought , its programmatic counterpart, Program of Thought [24; 34], Self-Consistency , and Self-Verification  have been shown to significantly improve the reasoning capabilities of LLMs.

Pretraining language models on math-heavy content has resulted in foundation LLMs such as Minerva , Galactic , Llemma , and DeepSeeKMath  with stronger mathematical skills out-of-the-box. A more direct approach of dataset-specific training does _instruction fine-tuning_ on problem-solution pairs derived from math reasoning datasets. Our work falls in this latter category and bears similarity with recent work such as RFT , ToRA , MAMmoTH , MetaMath  and MathCoder . We differ from the previous work along one factor or a combination of the following factors: (a) reliance on GPT-3.5/4, (b) solution format, and (c) use of ground truth text solution in synthesizing code-based solutions.

Knowledge Distillation via Synthetic Data.Recent work exploring the use of targeted _synthetic_ data generated by large foundation models for pre-training/instruction tuning smaller LLMs has led to tremendous progress in skills of these smaller LLMs [5; 6; 4; 40; 41; 42].

## 7 Conclusion

We introduce OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs which is released with a commercially permissive license. Compared to previous work, the OpenMathInstruct-1 dataset is at least four times bigger. With our proposed prompting novelty of using _masked text solutions_ and some brute-force scaling, we achieve training set coverage of 99.9% for the GSM8K benchmark and 93% for the challenging MATH benchmark. The quality of these synthesized solutions is illustrated by finetuning experiments, which show models achieving performance comparable to or better than their _gpt-distilled_ counterparts. To support the open-source efforts in this direction, we publicly release all our fine-tuned models, code, and the OpenMathInstruct-1 along with a further 6.6M incorrect sampled solutions.

Figure 4: Performance split by subjects and levels on our MATH validation subset.

  
**Error Type** & **Count** \\  Text Reasoning Error & 189 \\ Code Reasoning Error & 292 \\ Code Execution Error & 78 \\ Code timeout & 15 \\ Max code executions reached & 10 \\  Total & 584 \\   

Table 10: Types of errors and their counts.

## Limitations and Potential Risks

Our work aims to improve the mathematical reasoning of open-source models using open-source models. In pursuit of this goal, we create a synthetic dataset, OpenMathInstruct-1, that our experiments show aids the performance on existing math benchmarks. Below, we list the key limitations of our work:

* Our manual analysis reveals solutions that get the right answer but via flawed reasoning (Figure 13 in Appendix). Removing these _semantically noisy_ solutions is beyond the scope of the current work. This means a lack of guarantee about the quality of our synthetically generated solutions.
* Improving performance on in-domain math benchmarks may not translate to performance gain on other related tasks. The drop in performance on GSM-Hard compared to GSM indicates that our models may not be robust to input perturbations, though, they are at par with previous work.

While we don't foresee any material risk due to our work, using our imperfect dataset and models to perform tasks, such as evaluating student assignments or building a math tutor, carries risk.