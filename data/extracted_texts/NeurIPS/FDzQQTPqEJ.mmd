# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

Contributions.Let \((,,)\) denote a measure space with \(^{d}\), sigma algebra \(\), and nonnegative measure \(\). Let \(^{m n}\) be the readout parameters of a neural network and let \(^{n}\) and \(^{n}\) be the weights and biases of a hidden layer of a neural network \(:^{D}^{m}\) with activation function \(\). For some support \(\), define a probability distribution \(P\) (and corresponding probability density \(p\) with respect to base measure \(\)) to be proportional to squared \(2\)-norm of the evaluation of the neural network \(\),

\[P(d;,))}{(, )}(();,) _{2}^{2},(;,)=(+),\,=(,),\] (1)

whenever the normalising constant \((,)_{}( ();,)_{2}^{2}(d)\) is finite and non-zero. Here we call \(\) the base measure, \(:^{D}\) the sufficient statistic2 and \(\) the activation function. We will call the corresponding family of probability distributions, parametrised by \((,)\), a _squared neural family_ (SNEFY) on \(\), and denote it by SNEFY\({}_{,,,}\). SNEFYs are new flexible probability distribution models that strike a good balance between mathematical tractability, expressivity and computational efficiency. When a random vector \(\) follows a SNEFY distribution indexed by parameters \((,)\), we write \(_{,,,}(,)\) or simply \( P(:,)\), where there is no ambiguity.

Our main technical challenge is in exactly computing the normalising constant, \((,)\), where

\[(,)_{}( ();,)_{2}^{2}(d), (;,)=(+),=(,).\] (2)

The normalising constants we consider are special cases in the sense that they apply to specific (but commonly appearing in applications) choices of activation function \(\), sufficient statistic \(t\) and base measure \(\) over support \(\). See Table 1. Our analysis both exploits and informs a connection with

 Support & Base & Sufficient & Activation & & Kernel \\ \(\) & measure \(\) & statistic \(()\) & function \(\) & \(\) & \(k_{,,}\) \\   \\  ^{d}\)} & ,}\)} & ()\)} & erf & ✗ &  \\   & & & \(()_{+}^{p},p\) & ✗ &  \\   & & & \(\) & ✗ &  \\   & & & \(\)  & ✗ &  \\   \\  \(^{d}\) & \(_{,}\) & \(}{\|\|_{2}}\) & & ✓ & Kernel 3 \\  \(^{d-1}\) & Uniform & \(()\) & \(\) & ✓ & \\  \(^{d}\) & \(_{,}\) & & ✓ & Kernel 7 \\  \(\{0,1,2,\}\) & \((x!)^{-1}\) & & ✓ & Kernel 8 \\   \\  \(^{d}\) & \(_{,}\) & \(()\) & \(\) & ✓ & Kernel 2 \\  \(^{d}\) & Uniform & \(^{-1}()\) & & & \\  \(^{d}\) & \(_{,}\) & \(()\) & \(_{}\) & ✓ & Kernel 6 \\ 

Table 1: Examples of settings admitting a closed-form for the normalising constant \((,)\) (2) by leveraging a closed-form NNK \(k_{,,}\) (4). In each case, \((,)=(^{}_{})\), where the entries of the matrix \(_{}\) are described according to the NNK \(k_{,,}\) in Identity 1. \(_{,}\) denotes the CDF of a multivariate normal distribution with mean \(\) and covariance matrix \(\) and \(\) denotes counting measure. Rows with citations have been considered previously in the context of NNGPKs, but not as normalising constants and not with a reversal of the role of input and parameter. Note the cases where \(\); this setting is not considered by others, because when the role of parameters and data is in the usual setting, \(=\) covers a sufficiently general setting. Noticing that SNEFYs strictly generalise exponential family mixture models (see § 3.2), fixing \(()=(/2)\) and a given base measure \(\) and sufficient statistic \(\) for which the exponential family log-partition function is known also leads to tractable normalising constants. More known closed-form kernels as well as approximate kernels that can be adapted to our setting are given in .

so-called neural network Gaussian process kernels (NNGPKs)  in a generalised form, which we refer to as neural network kernels (NNKs).

We discuss some important theoretical properties of SNEFY such as exact normalising constant calculation, marginal distributions, conditional distributions, and connections with other probability models. We then consider a deep learning setting, where SNEFYs can either be used as base distributions in (non-volume preserving) normalising flows , or may describe flexible conditional density models with deep learning feature extractors. We demonstrate SNEFY on a variety of datasets.

### Background

NotationWe use lower case non-bold (like \(a\)) to denote scalars, lower case bold to denote vectors (like \(\)) and upper case bold to denote matrices (like \(\)). Random variables (scalar or vector) are additionally typeset in sans-serif (like a and \(\)). The special zero vector \((0,,0)\) and identity matrix elements are \(\) and \(\). We use subscripts to extract (groups of) indices, so that for example, \(_{i}\) is the \(i\)th row of the matrix \(\) (as a column vector), \(_{,i}\) is the \(i\)th column of the matrix \(\), and \(b_{i}\) is the \(i\)th element of the vector \(\). We use \(=(,)^{n(D+1)}\) to denote the concatenated hidden layer weights and biases. Correspondingly, we write \(_{i}=(_{i},b_{i})^{D+1}\) for the \(i\)th row of \(\). We will use a number of special functions. \(_{,}\) denotes the cumulative distribution function (CDF) of a Gaussian random vector with mean \(\) and covariance matrix \(\). We also use a shorthand \(_{}=_{,}\) and \(=_{,}\).

Single hidden layer neural networksWe consider a feedforward neural network \(:^{D}^{m}\),

\[(;,)=(+)\] (3)

with activation function \(\), hidden weights \(^{n D}\) and biases \(^{n}\) and readout parameters \(^{m n}\). Here \(\) is applied element-wise to its vector inputs, returning a vector of the same shape.

Neural network kernelsIn certain theories of deep learning, one often encounters a bivariate Gaussian integral called the _neural network Gaussian process kernel_ NNGPK. The NNGPK first arose as the covariance function of a well-behaved single layer neural network with random weights . In the limit as the width of the network grows to infinity, the neural network (3) with suitably well-behaved (say, independent Gaussian) random weights converges to a zero-mean Gaussian process, so that the NNGPK characterises the law of the neural network predictions. These limiting models can be used as functional priors in a classical Gaussian process sense .

In our setting, the positive semidefinite (PSD) NNGPK appears in an entirely novel context, where the role of the hidden weights and biases \(_{i}=(_{i},b_{i})\) and the data \(\) is reversed. Instead of marginalising out the parameters and evaluating at the data, we marginalise out the data and evaluate at the parameters. The NNGPK \(k_{,,_{}}\) admits a representation of the form

\[k_{,,_{}}(_{i},_{j}) _{}_{i}^{}+b_{i} _{j}^{}+b_{j}, ,.\] (4)

We do not discuss in detail how it is constructed in earlier works , where usually \(b_{i}=b_{j}=0\)3, but not always. When \(b_{i}=b_{j}=0\), closed-form expressions for the NNGPK are available for different choices of \(\) and \(\). However, the setting of \(\) is important in our context (as we show in SS2.2) and presents additional analytical challenges.

We will require a more general notion of an NNGPK which we call a _neural network kernel_ (NNK). We introduce a function \(\) which may be thought of as a warping function applied to the input data. Such warping is common in kernels and covariance functions and can be used to induce desirable analytical and practical properties [37, 29, 24, SS5.4.3]. We also integrate with respect to more general measures \(\) instead of the standard Gaussian CDF, \(\). We define the NNK to be

\[k_{,,}(_{i},_{j})_{}_{i}^{}()+b_{i}_{j} ^{}()+b_{j}(d).\] (5)Closed form squared neural families

### Normalising constants

Observe from (2) that by swapping the order of integration and multiplication by \(\), the normalising constant is quadratic in elements of \(\). The coefficients of the quadratic depend on \(=(,)\). We now characterise these coefficients of the quadratic in terms of the NNK evaluated at rows \(_{i},_{j}\) of \(\), which are totally independent of \(\). The proof of the following is given in Appendix A.

**Identity 1**.: _The integral (2) admits a representation of the form_

\[(,)=(^{}_{ })\] (6)

_where \(k_{,,}\) is as defined in (5), and \(_{}\) is the PSD matrix whose \(ij\)th entry is \(k_{,,}(_{i},_{j})\)._

By Identity 1, the normalised measure (1) then admits the explicit representation

\[P(d;,)=(^{} _{}}())}{(^{ }_{})}(d)=( ^{})^{}(_{}}())} {(^{})^{}(_{})} (d),\]

where \(_{}}()\) is the PSD matrix whose \(ij\)th entry is \((_{i}^{}()+b_{i})(_{j}^{}( )+b_{j})^{}\). We used the cyclic property of the trace, writing the numerator as \((^{}^{})= (^{}^{})= (^{}^{})\). We emphasise again that the role of the data \(()\) and the hidden weights and biases \(\) in the NNK \(k_{,,}\) are reversed compared with how they have appeared in previous settings. We may compute evaluations of \(k_{,,}\) in closed form for special cases of \((,,)\) using various identities in \((d)\), where \(d\) is the dimensionality of the domain of integration, as we soon detail in SS 2.2. Combined with the trace inner product, this leads to a total cost of computing \((,)\) of \((m^{2}n+dn^{2})\), where \(n\) and \(m\) are respectively the number of neurons in the first and second layers.

**Remark 1** (Alternative parameterisations).: _SNEFT models depend on readout parameters \(\) only through the direction of \((^{})\) and not on its norm or sign. For example, one can always find another parameterisation of readout parameters that results in the same probability distribution but has a normalising constant of \(1\). Furthermore, noticing that \(\) only appears as a PSD matrix \(^{}\) of rank at most \((m,n)\), one may alternatively parameterise a SNEFT by \((,)\)._

### Neural network kernels

In SS 2.1, we reduced computation of the integral (2) to computation of a quadratic involving evaluations of the NNK (5). Several closed-forms are known for different settings of \(\), all with \(=\) and \(=\). The motivation behind derivation of existing known results is from the perspective of inference in infinitely wide Bayesian neural networks  or to derive certain integrals involved in computing predictors of infinitely wide neural networks trained using gradient descent . Here we describe some new settings that have not been investigated previously that are useful for the new setting of SNEFT. Recall from (5), that our kernel, \(k_{,,}\), is parametrised by activation function \(\), warping function (sufficient statistic) \(\), and base measure \(\). All derivations are given in Appendix B.

The first kernel describes how we may express the kernels with arbitrary Gaussian base measures \(_{,}\) in terms of the kernels with isotropic Gaussian base measures \(\). This means that it suffices to consider isotropic Gaussian base measures in place of arbitrary Gaussian base measures.

**Kernel 1**.: \(k_{,,_{,}}(_{i},_{j})= k_{,,}(_{i},_{j})\)_, where \(=(,+)\), \(_{i}=(_{i}^{},b_{i}+_{i}^{} )\) and \(\) is a matrix factor such that covariance \(=^{}\)._

This kernel can also be used to describe kernels corresponding with Gaussian mixture model base measures. The second kernel we describe is a minor extension to the case \(\) of a previously considered kernel .

**Kernel 2**.: \(k_{,,}(_{i},_{j})=-b_{j}|}{2}(_{j}-_{j}\|^{2}}{2})+ {|b_{i}+b_{j}|}{2}(_{j}+_{j}\|^{2}}{2} )\).

A similar result and derivation holds for the case of \(k_{,,}\), which we do not reproduce here. We now mention a case that shares a connection with exponential families (see SS 3.2 for a detailed description of this connection). The following and more \(=\) cases are derived in Appendix C.

**Kernel 3**.: _Define \(_{^{d-1}}()/\|\|\) to be the projection onto the unit sphere. Then_

\[k_{,_{^{d-1}},}(_{i},_{j} )=(b_{i}+b_{j})I_{d/2-1}\|_{i}+_{j}\|}{\|_{i}+_{j}\|^{d/2-1}},\]

_where \(I_{p}\) is the modified Bessel function of the first kind of order \(p\). In the special case \(d=3\), we have the closed-form \(k_{,_{^{2}},}(_{i},_{j})= (b_{i}+b_{j})\|e^{\|_{i}+_{j}\|}-e^{-\|_{i} +_{j}\|}}{2\|_{i}+_{j}\|}\)._

We end this section with a new analysis of the Snake\({}_{a}\) activation function, given by

\[_{a}(z)=z+^{2}(az)=z-(2az)+.\]

The Snake\({}_{a}\) function  is a neural network activation function that can resemble the ReLU on an interval for special choices of \(a\), is easy to differentiate, and as we see shortly, admits certain attractive analytical tractability. We note that a similar activation function has been found using reinforcement learning to search for good activation functions [39, Table 1 and 2, row 3], up to an offset and hyperparameter \(a=1\). The required kernel is expressed in terms of the linear kernel (Kernel 4) and the kernel corresponding with the activation function of , i.e. snake without the offset, \(_{a}()-\) (Kernel 5). We first describe the linear kernel.

**Kernel 4**.: \(k_{,,}(_{i},_{j})=_{i}^ {}_{j}+b_{i}b_{j}\).

We now derive the kernel corresponding with \(_{}\) activation functions up to an offset.

**Kernel 5**.: _The kernel \(k_{_{}()-,}(_{i},_{j})\) is equal to_

\[}k_{,,}(2a_{ i},2a_{j})+_{j}^{}_{j}(2ab_{j})e^{-2a^{2} \|_{j}\|^{2}}+(2ab_{i})e^{-2a^{2}\|_{i}\|^{2}}\] \[-}{2a}(2ab_{j})e^{-2a^{2}\|_{j}\|^{2}}- }{2a}(2ab_{i})e^{-2a^{2}\|_{i}\|^{2}}+k_{, ,}(_{i}^{(1)},_{j}^{(1)}).\]

The kernel corresponding with \(_{}\) activations is then stated in terms of Kernel 4 and 5.

**Kernel 6**.: _The kernel \(k_{_{},,}(_{i},_{j})\) is equal to_

\[b_{i}-(2ab_{i})(-2a^{2}\| _{i}\|^{2})+b_{j}-(2ab_{j})(-2a^{2}\|_{j}\|^{2 })\] \[+k_{_{}()-,,}(_{i},_{j})+}.\]

## 3 Properties of squared neural families

### Fisher-Neyman factorisation and sufficient statistics

If the base measure \(\) is absolutely continuous with respect to some measure \(\), and \(:[0,)\) is the Radon-Nikodym derivative, then the SNEFY admits a probability density function \(p(,)\) with respect to \(\),

\[p(,)=()}_{$},\,}\ \ (();,)_ {2}^{2}}_{$ only through $()$}}.\] (7)

The Fisher-Neyman theorem (for example, see Theorem 6.14 of ) says that the existence of such a factorisation is equivalent to the fact that \(\) is a sufficient statistic for the parameters \(,\).

### Connections with exponential families

In this section we will use the activation \((u)=(u/2)\). We note that we can absorb the bias terms \(\) into the \(\) parameters4 and obtain as a special case the following family of distributions

\[P(d;,)=(^{}_{})}_{i=1}^{n}_{j=1}^{n}_{,i}^{}_{,j} ((_{i}+_{j})^{}())(d ),\] (8)which is a mixture5 of distributions \(P_{e}(;(_{i}+_{j}))\) belonging to a classical exponential family \(P_{e}\), given in the canonical form by

\[P_{e}(d;)=_{e}()}^{ }()(d),_{e}()=_{ }^{}()(d).\] (9)

It is helpful to identify the following three further cases:

1. When \(n=m=1\), \(v_{11}^{2}\) cancels in the numerator and denominator and we obtain an exponential family with base measure \(\) supported on \(\), sufficient statistic \(\), canonical parameter \(_{1}\) and normalising constant \(_{e}(_{1})\). Every exponential family is thus a SNEFY, but not conversely.
2. When \(m>1\) and \(n>1\), we obtain a type of exponential family mixture model with coefficients \(^{}\), some of which may be negative. Advantages of allowing negative weights in mixture models in terms of learning rates are discussed in . The rank of \(^{}\) is at most \((m,n)\).
3. When \(m>1\) and \(n>1\) and \(^{}\) is diagonal (i.e. each column in \(\) is orthogonal), there are at most \(n\) non-zero mixture coefficients, all of which are nonnegative. That is, we obtain a standard exponential family mixture model.

The kernel matrix \(_{}\) in the normalising constant of (8) is tractable whenever the normalising constant of the corresponding exponential family is itself tractable.

**Proposition 1**.: _Denote by \(_{e}()\) the normalising constant of the exponential family in (9). Then_

\[k_{(/2),,}(_{i},_{j})= \,_{e}((_{i}+_{j})).\] (10)

The above kernel is well defined for any collection of \(_{i}\) which belong to the canonical parameter space of \(P_{e}\), since the canonical parameter space is always convex . This gives us a large number of tractable instances of SNEFY which correspond to exponential family mixture models allowing negative weights - a selection of examples is given in Appendix C. It is interesting that some properties of the exponential families are retained by this generalisation belonging to SNEFYs. For example, the following proposition, the proof of which is given in Appendix A, links the derivatives of the log-normalising constant to the mean and the covariance of the sufficient statistic.

**Proposition 2**.: _Let \((u)=(u/2)\) and define the log-normalising constant as \(=(,)\)._

\[_{i=1}^{n}_{i}}= [()] _{i=1}^{n}_{j=1}^{n}}{_{i}_{j}^{ }}=[()( )^{}]-[()] [()]^{}.\]

### Conditional distributions under SNEFY

An attractive property of SNEFY is that, under mild conditions, the family is closed under conditioning.

**Theorem 1**.: _Let \(=(_{1},_{2})\) be jointly \(}_{X_{1} X_{2},,,}\) with parameters \(\) and \(=([_{1},_{2}],)\). Assume that \((d)=_{1}(d_{1})_{2}(d_{2})\) and \(()=_{1}(_{1}),_{2}(_{2})\). Then the conditional distribution of \(_{1}\) given \(_{2}=_{2}\) is \(}_{X_{1},_{1},,_{1}}\) with parameters \(\) and \(_{1|2}(_{1},_{2}_{2}(_{2})+)\)._

The proof, which we detail in Appendix A follows directly by folding the dependence on the conditioning variable \(_{2}\) into the bias term. We note that conditional density will typically be tractable if the joint density is tractable since they share the same activation function \(\). Thus, whenever SNEFY corresponds to a tractable NNK with a non-zero bias, we can construct highly flexible _conditional density models_ using SNEFY by taking \(_{2}\) itself to be a jointly trained deep neural network. Crucially, \(_{2}\) may be _completely unconstrained_. We use this observation in the experiments (SS 4).

### Marginal distributions under SNEFY

Marginal distributions under SNEFY model for a general activation function \(\) need not belong to the same family. In the special case \(=(/2)\), SNEFY is in fact also closed under marginalisation, which we prove in Appendix D. Even in the general \(\) case, marginal distributions are tractable and admit closed forms whenever the joint SNEFY model and the conditional SNEFY are tractable.

**Theorem 2**.: _Let \(=(_{1},_{2})\) be jointly \(_{_{1}_{2},,,}\) with parameters \(\) and \(=([_{1},_{2}],)\). Assume that \((d)=_{1}(d_{1})_{2}(d_{2})\) and \(()=_{1}(_{1}),_{2}(_{2})\). Then the marginal distribution of \(_{1}\) is_

\[P_{1}(d_{1})=^{} {}_{}(_{1})}{(,)}_{1}(d_{1}),\]

_where \((_{1})_{ij}=k_{,_{2},_{2}} {w}_{2i},_{1i}^{}_{1}(_{1})+b_{i},_ {2j},_{1j}^{}_{1}(_{1})+b_{j}\)._

The proof is given in Appendix A. Due to this tractability of the marginal distributions, it is straightforward to include the likelihood corresponding to incomplete observations (i.e. samples where we are missing some of the components of \(\)) into the density estimation task.

### Connections with kernel-based methods for nonnegative functions

SNEFY may be viewed as a neural network variant of the non-parametric kernel models for non-negative functions , which are constructed as follows. Let \(:\) be a feature mapping to a (possibly infinite dimensional) Hilbert space \(\). Let \(()\) be the set of all positive semidefinite (PSD) bounded linear operators \(:\). Then

\[h_{}()=(),() _{}\] (11)

gives an elegant model for nonnegative functions parametrised by \(()\), and their application to density modelling with respect to a base measure has also been explored [25; 42]. Note that by assuming boundedness of \(\) and \(\), the normalizing constant [25, Proposition 4] is given by \(_{}()()(d)\), which is analogous to our work where the normalising constant is given by \((,)=(^{} {K}_{})\). This can be seen by replacing \(\) with \(^{}\) and replacing \(_{}()()(d)\) by \(_{}=_{}(()+) (()+)^{}(d)\).

Despite feature maps being infinite-dimensional, model (11) often reduces to an equivalent representation in finite-dimensions.  utilise a representer theorem when (11) is fitted to data using a regularised objective, while  more directly assume that the linear operator \(\) is inside the span of the features evaluated at the available data \(\{_{}\}_{=1}^{N}\). The resulting model resembles SNEFY where \(n\) equals to the number \(N\) of datapoints, i.e.

\[h_{}()=[(,_{1}),,,(,_{N})]^{}[(,_{1}),,, (,_{N})]\] (12)

for a PSD matrix \(^{N N}\) and \((_{i},_{j})=(_{i}),( {x})_{}\).

However, there are fundamental differences between (12) and SNEFY which we list below. The models can be seen as complementary and they inherit advantages and disadvantages of kernel methods and neural networks common in other settings, respectively.

* this is not generally tractable apart from some

Figure 1: (Left) An instance of an (untrained) SNEFY\({}_{^{2},,_{a},}\) density with \(n=100\), \(m=1\), \(v_{ij}(0,1/n)\), \(w_{ij}(0,4)\) and \(=\). Shown are \(50\) exact samples found using rejection sampling. Numerical quadrature for this and every example supported on \(^{d}\) in § 4 returns a value of \(1.00\) for the integral over \(\). (Right) A trained SNEFY\({}_{^{2},,,d}\) density with \(n=m=30\). Shown is the training and testing dataset  also used by  for point processes.

limited combinations of \(\) and \(\) (e.g. for a Gaussian kernel \(\) and a Gaussian \(\)). Note that this kernel is evaluated at the datapoints, whereas SNEFY evaluates the kernel at the learned parameters \(_{i}\).  focuses on the specific case where \(\) is a Gaussian kernel, studying properties of the resulting density class which is a mixture of Gaussian densities allowing for negative weights, a model equivalent to SNEFY with the exponential activation function as described in Appendix C. Note that our treatment of SNEFY as a generalisation of the exponential family goes well beyond the Gaussian case, and that tractable instances arise with many other activation functions.
* instead, expressivity in (12) only comes from fitting \(\) (and potentially lengthscale hyperparameters of \(\)), at the expense of a more involved optimiisation over the space of PSD matrices. In contrast, we learn \(\) (analogous to learning \(\)) and \(\) jointly using neural-network style gradient optimisers. SNEFY is fully compatible with end-to-end and jointly optimised neural network frameworks, a property we leverage heavily in our experiments in SS 4.
* **Conditioning.** By explicitly writing parametrisation which includes the biases, we obtain a family closed under conditioning and thus a natural model for conditional densities, whereas it is less clear how to approach conditioning when given a generic feature map \(\).

Other related workAfter submission, we became aware of another related literature, which includes mixture models with potentially negative mixture coefficients via squaring  and positive semi-definite probabilistic circuits . We believe a marriage of ideas from SNEFY and probabilistic circuits will lead to future developments in tractable and expressive probability models.

## 4 Experiments

ImplementationAll experiments are conducted on a Dual Xeon 14-core E5-2690 with 30GB of reserved RAM and a single NVidia Tesla P100 GPU. Full experimental details are given in Appendix E. We build our implementation on top of normflows, a PyTorch package for normalising flows. SNEFYs are built as a BaseDistribution, which are base distributions inside a NormalizingFlow with greater than or equal to zero layers. We train all models via maximum likelihood estimation (MLE) i.e. minimising forward KL divergence.

2D synthetic unconditional density estimationWe consider the 2 dimensional problems also benchmarked in . We compare the test performance, computation time and parameter count of non-volume preserving flows (NVPs)  with four types of base distribution: SNEFY, resampled  (Resampled), diagonal Gaussian (Gauss) and Gaussian mixture model (GMM). We consider flow depths of 0, 1, 2, 4, 8 and 32 where a flow depth of 0 corresponds with the base distribution only. We use \(=\), \(=\), \(=}^{d}\) and a Gaussian mixture model base density. We set \(m=1\) and \(n=50\). Full architectures and further experimental details are described in Appendix E.1.

Results are shown in Table 2 for the \(0\) and \(16\) layer cases, and further tables for \(1,2,4\) and \(8\) layers are given in Appendix E.1. Our observations are that all base distributions are able to achieve good performance, provided they are appended with a normalising flow of appropriate depth. SNEFY is able to achieve good performance with depth 0 on all three datasets, as are Resampled and GMM for the Moons dataset. The parameter count for well-performing SNEFY models is very low, but the computation time can be relatively high. However, SNEFY is the only model which consistently achieves the highest performance within one standard deviation across all normalising flow depths.

Data on the sphereWe compare the three cases mentioned in SS 3.2 using Kernel 3, i.e. mixtures of the von Mises Fisher (VMF) distribution, as shown in Figure 1. Over \(50\) runs, the unconstrained \(\), diagonal \(\), and \(n=m=1\) cases respectively obtain test negative log likelihoods of \(1.38 9.64 10^{-3}\), \(1.46 0.016\) and \(2.34 0.26\) each in \(111.18 2.58\), \(109.12 0.80\) and \(61.88 0.33\) seconds (average \(\) standard deviation). In this setting, allowing for a fully flexible \(\), going beyond the classical mixture model, shows clear benefits in performance. Results are summarised in Table 3. Full details are given in Appendix E.2.

Conditional density estimation on astronomy dataPredicting plausible values for the velocity of distant astronomical objects (such as galaxies or quasars) without measuring their full spectra, but by 

[MISSING_PAGE_FAIL:9]

model parameters  or samples from missing data and optimises for model parameters , however these do not allow for maximum likelihood estimation. Our observations are twofold: including partial observations improves performance, and adding more complete observations improves performance.

## 5 Discussion and conclusion

We constructed a new class of probability models - SNEFY - by normalising the squared norm of a neural network with respect to a base measure. SNEFY possesses a number of convenient properties: tractable exact normalising constants in many instances we identified, closure under conditioning, tractable marginalisation, and intriguing connections with existing models. SNEFY shows promise empirically, outperforming competing (conditional) density estimation methods in experiments.

Sampling versus density estimationWe focus here on the problem of density estimation, for which SNEFY is well-suited. While it is sometimes possible to obtain exact SNEFY samples using rejection sampling, sampling is more computationally expensive than in other models such as normalising flows. Future work will focus on sampling, as has been done with related models , where the special case of Gaussian \(\) and hyperrectangular support \(\) is considered. In , \(r\) approximate samples with \((r_{2}||+rd_{2})\) evaluations of the normalising constant are obtained. Here \(\) is an approximation tolerance parameter. Note that this complexity significantly improves upon naive rejection sampling, which typically scales exponentially in dimension \(d\). In the unconditional setting, SNEFY is constructed as only a 2-layer network, thereby limiting expressivity. However, in the conditional density estimation setting, we may use any number of layers and any architecture for the conditioning network \(_{2}\). Finally, SNEFY inherits all the usual limitations and advantages over mirroring kernel-based approaches, as discussed in SS 3.5.

Future workWe see a number of further promising future research directions. First, as we detail in Appendix F, choosing \(()=(i)\) and identity sufficient statistics results in a kernel \(k_{(i),,}\) which is the Fourier transform of a nonnegative measure. By Bochner's theorem, the kernel is guaranteed to be (real or complex-valued) shift-invariant. The kernel matrix is Hermitian PSD (so that the normalising constant is positive and nonnegative), and we may also allow (but do not require) the readout parameters \(\) to be complex. We note that the same result would be obtained if one used a mixture of real-valued \(\) and \(\) activations with shared parameters (see Remark 3). Second, an alternative deep model to our deep conditional feature extractor might be to use a SNEFY model as a base measure \(\) for another SNEFY model; this might be repeated \(L\) times. This leads to \((n^{2L})\) integration terms for the normalising constant. The individual terms are tractable in certain cases, for example when \(\) is exponential or trigonometric. Third, when modelling discrete distributions with trigonometric activations, the NNK can be expressed in terms of convergent Fourier series (see Appendix G) Finally, our integration technique can be applied to other settings. For example, we may build a Poisson point process intensity function using a squared neural network and compute the intensity function in closed-form, offering a model that scales quadratically in the number of neurons \((n^{2})\) instead of comparable models which scale cubically in the number of datapoints \((N^{3})\)[9; 51].

Figure 2: Density estimation under partial observations. The right plot is a zoomed in version of the left plot. NF-1, NF-2 and NF-4 are respectively normalising flows of depth 1, 2 and 4. Normalising flow models and SNEFY without marginalisation discard incomplete observations, whereas SNEFY use Theorem 2 to include partial observations via maximum marginal likelihood. Marginal likelihoods allow for an improved NLL.