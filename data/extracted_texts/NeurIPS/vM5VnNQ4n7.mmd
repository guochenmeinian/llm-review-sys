# Exploiting Correlated Auxiliary Feedback in Parameterized Bandits

Arun Verma Zhongxiang Dai Yao Shu Bryan Kian Hsiang Low

Department of Computer Science, National University of Singapore, Republic of Singapore

{arun, daizhongxiang, shuyao, lowkh}@comp.nus.edu.sg

###### Abstract

We study a novel variant of the parameterized bandits problem in which the learner can observe additional auxiliary feedback that is correlated with the observed reward. The auxiliary feedback is readily available in many real-life applications, e.g., an online platform that wants to recommend the best-rated services to its users can observe the user's rating of service (rewards) and collect additional information like service delivery time (auxiliary feedback). In this paper, we first develop a method that exploits auxiliary feedback to build a reward estimator with tight confidence bounds, leading to a smaller regret. We then characterize the regret reduction in terms of the correlation coefficient between reward and its auxiliary feedback. Experimental results in different settings also verify the performance gain achieved by our proposed method.

## 1 Introduction

Parameterized bandits (Slivkins, 2019; Lattimore and Szepesvari, 2020) have many real-life applications in online recommendation, advertising, web search, and e-commerce. In this bandit problem, a learner selects an action and receives a reward for the selected action. Due to the large (or infinite) number of actions, the mean reward of each action is assumed to be parameterized by an unknown function, e.g., linear (Li et al., 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011; Agrawal and Goyal, 2013), GLM (Filippi et al., 2010; Li et al., 2017; Jun et al., 2017), and non-linear (Valko et al., 2013; Chowdhury and Gopalan, 2017). The learner aims to learn the best action as quickly as possible. However, it depends on the tightness of confidence bounds of function that correlate action with the reward. The learner exploits any available information like side information (i.e., information available to the learner before selecting an action, e.g., contexts) (Li et al., 2010; Agrawal and Goyal, 2013; Li et al., 2017) and side observations (i.e., information about other actions, e.g., graph feedback) (Alon et al., 2015; Wu et al., 2015) to make confidence bounds as tight as possible. This paper considers another type of additional information (correlated with the reward) that a learner can observe with reward for the selected action, which we call _auxiliary feedback_.

The auxiliary feedback is readily available in many real-life applications. For example, consider an online food delivery platform that wishes to recommend the best restaurants (actions) to its users. After receiving food, the platform observes user ratings (rewards) for the order and can collect additional information like food delivery time (auxiliary feedback). Since the restaurant's rating also depends on overall food delivery time, one can expect it to be correlated with the user rating. The platform can estimate or know the average delivery time for a given order from historical data. Similar scenarios arise in recommending the best cab to users (auxiliary information can be the cab's distance from the rider or driver's response to ride request), e-commerce platforms choosing top sellers to buyers (auxiliary information can be seller's response time for order confirmation and delivery), queuing network (Lavenberg and Welch, 1981; Lavenberg et al., 1982), jobs scheduler (Verma and Hanawal, 2021), and many more. Therefore, the following question naturally arises:

_How to exploit correlated auxiliary feedback to improve the performance of a bandit algorithm?_One possible method is to use auxiliary feedback in the form of control variates (Lavenberg et al., 1982; Nelson, 1990) for the observed reward. A control variate represents any random variable (auxiliary feedback) with a known mean that is correlated with the random variable of interest (reward). Several works (Kreutzer et al., 2017; Sutton and Barto, 2018; Vlassis et al., 2021; Verma and Hanawal, 2021) have used control variates to estimate the mean reward estimator with smaller variance, leading to tight confidence bounds and hence better performance. The closest work to our setting is Verma and Hanawal (2021). However, it only focuses on the non-parameterized setting and assumes a finite number of actions. We thus consider a more general bandit setting with a large (or even infinite) number of actions and allow an unknown function to parameterize auxiliary feedback.

Motivated by control variate theory (Nelson, 1990), we first introduce _hybrid reward_, which combines the reward and its auxiliary feedback in such a way that hybrid reward is an unbiased reward estimator with smaller variance than the observed reward. However, the optimal combination of reward and its auxiliary feedback requires knowledge of the covariance matrix among auxiliary feedback and covariance between reward and its auxiliary feedback, which may not be available in practice. Since the reward and its auxiliary feedback are functions of the selected action, existing control variate results can not be useful to our sequential setting. Naturally, we face the question of _how to combine reward and its auxiliary feedback efficiently using available information._ To answer this, we extend control variate theory results to the problems where known functions can parameterize control variates (in Section 3) and then extend to setting where unknown functions parameterize control variates (in Section 4). These contributions are themselves of independent interest in control variate theory.

Equipped with these results, we show that the variance of hybrid rewards is smaller than observed rewards (Theorem 1 and Theorem 3). We then propose a method that uses hybrid rewards instead of observed rewards for estimating reward function, resulting in tight confidence bounds and, hence, lower regret. We introduce the notion of _Auxiliary Feedback Compatible_ (AFC) bandit algorithm. An AFC bandit algorithm can use hybrid rewards instead of only observed rewards. We prove that the expected instantaneous regret of any AFC bandit algorithm using hybrid rewards is smaller by a factor of \(O((1-^{2})^{})\) compared to the same AFC bandit algorithm using only observed rewards, where \(\) is the correlation coefficient of the reward and its auxiliary feedback (Theorem 2 and Theorem 4). Our experimental results in different settings also verify our theoretical results (in Section 5).

### Related work

Several prior works use additional information to improve the performance of bandit algorithms. In the following, we discuss how auxiliary feedback differs from side information and side observation.

**Side Information:** Several works use context as side information to select the best action to play. This line of work is popularly known as contextual bandits (Li et al., 2010; Chu et al., 2011; Agrawal and Goyal, 2013; Li et al., 2017). Here, the mean reward of each arm is a function of context and is often parameterized, e.g., linear (Li et al., 2010; Chu et al., 2011; Agrawal and Goyal, 2013), GLM (Li et al., 2017), and non-linear (Valko et al., 2013). These contexts are assumed to be observed _before_ an action is taken. However, we consider a problem where additional information is correlated with rewards that can only be observed _after_ selecting the action.

**Side Observations:** Several works consider the different side observations settings in the literature, e.g., stochastic (Caron et al., 2012), adversarial (Mannor and Shamir, 2011; Kocak et al., 2014), graph feedback (Alon et al., 2015; Wu et al., 2015; Alon et al., 2017), and cascading feedback (Verma et al., 2019, 2020a, 2019). Side observations represent the additional information available about actions that the learner does _not select_. Auxiliary feedback is different from side information as it is available only for _selected_ action and provides more information about the reward of that action.

**Auxiliary Feedback:** We use auxiliary feedback as control variates, which are used extensively for variance reduction in Monte-Carlo simulation of complex systems (Lavenberg and Welch, 1981; Lavenberg et al., 1982; James, 1985; Nelson, 1989, 1990; Botev and Ridder, 2017; Chen and Ghahramani, 2016). Recent works (Kreutzer et al., 2017; Vlassis et al., 2021; Verma and Hanawal, 2021) and (Sutton and Barto, 2018, Chapter 7.4) have exploited the availability of these control variates to build estimators with smaller variance and develop algorithms that have better performance guarantees. The closest work to our setting is Verma and Hanawal (2021). However, they only consider a non-parameterized setting with a finite number of actions. We thus consider a more general bandit setting with large (infinite) actions and allow a function to parameterize auxiliary feedback.

Problem setting

We consider a novel parameterized bandits problem in which the learner can observe auxiliary feedback correlated with the observed reward. In this problem, a learner has been given an action set, denoted by \(^{d}\) where \(d 1\). At the beginning of round \(t\), the learner selects an action \(x_{t}\) from action set \(\). Then, the environment generates a stochastic reward \(y_{t} f(x_{t})+_{t}\) for the selected action \(x_{t}\), where \(f:^{d}\) is an unknown reward function and \(_{t}\) is a zero-mean Gaussian noise with variance \(^{2}\). Apart from the stochastic reward \(y_{t}\), the environment generates \(q\) of auxiliary feedback. The \(i\)-th auxiliary feedback is denoted by \(w_{t,i} g_{i}(x_{t})+_{t,i}^{w}\), where \(g_{i}:^{d}\) and \(_{t,i}^{w}\) is a zero-mean Gaussian noise with variance \(_{w,i}^{2}\). The multiple correlation coefficient of reward and its auxiliary feedback is denoted by \(\) and assumed to be the same across all actions.

The optimal action (denoted by \(x^{}\)) has the maximum function value, i.e., \(x^{}*{argmax}_{x}f(x)\). After selecting an action \(x_{t}\), the learner incurs a penalty (or _instantaneous regret_) \(r_{t}\), where \(r_{t} f(x^{})-f(x_{t})\). Since the optimal action is unknown, we sequentially estimate the reward function using available information on rewards and associated auxiliary feedback for the selected actions and then use it for choosing the action in the following round. Our goal is to learn a sequential policy that selects actions such that the total penalty (or _regret_) incurred by the learner is as minimum as possible. After \(T\) rounds, the regret of a sequential policy \(\) that selects action \(x_{t}\) in the round \(t\) is given by

\[_{T}()_{t=1}^{T}r_{t}=_{t=1}^{T}(f(x^{} )-f(x_{t})).\] (1)

A policy \(\) is a good policy if it has sub-linear regret, i.e., \(_{T}_{T}()/T=0\). This implies that the policy \(\) will eventually learn to recommend the best action.

## 3 Known auxiliary feedback functions

We first focus on a simple case where all auxiliary feedback functions are assumed to be known. This assumption is not very strict in many applications as the learner can construct auxiliary feedback such that its mean value is known beforehand (see Kreutzer et al. (2017), Vlassis et al. (2021), and Chapter 12.9 of Sutton and Barto (2018) for such examples). When auxiliary feedback functions are unknown, we can estimate them using historical data or additional samples of auxiliary feedback. However, it will have some penalty in the performance (more details are in Section 4 and Section 5).

The first challenge we face is _how to exploit auxiliary feedback to get a better reward function estimator_. To resolve this, we extend control variate theory (Nelson, 1990) to the problems where a function can parameterize control variates. This new contribution is itself of independent interest.

### Control variate

Let \(\) be the unknown variable that needs to be estimated and \(y\) be its unbiased estimator, i.e., \([y]=\). Any random variable \(w\) with a known mean value (\(\)) can be treated as a control variate for \(y\) if it is correlated with \(y\). The control variate method (Nelson, 1990) exploits errors in estimates of known random variables to reduce the estimator's variance for the unknown random variable. This method works as follows. For any choice of a coefficient \(\), define a new random variable as \(z y-(w-)\). Note that \(z\) is also an unbiased estimator of \(\) (i.e., \([z]=\)) as

\[[z]=[y]-[(w- )]=-([w]-)=-(- )=.\]

Using properties of variance and covariance, the variance of \(z\) is given by

\[(z)=(y)+^{2}(w )-2(y,w).\]

The variance of \(z\) is minimized by setting \(\) to \(^{}=(y,w)/(w)\) and the minimum value is \((1-^{2})(y)\), where \(=(y,w)/(w)(y)}\) is the correlation coefficient of \(y\) and \(w\). We exploit this variance reduction to design a reward function estimator with tight confidence bounds.

### Auxiliary feedback as control variates

Since the auxiliary feedback functions are known, we define a new variable using the reward sample and its auxiliary feedback. We refer to this variable as _'hybrid reward.'_ The hybrid reward definition is motivated by the control variate method, except the control variate is parameterized by function in our setting. As \(w_{s,i}\) is the \(i^{}\) auxiliary feedback observed with reward \(y_{s}\), the _hybrid reward_ for reward (\(y_{s}\)) with its \(q\) auxiliary feedback \(\{w_{s,i}\}_{i=1}^{q}\) is defined by

\[z_{s,q} y_{s}-_{i=1}^{q}_{i}(w_{s,i}-g_{i}(x_{s}))=y_{s}-( _{s}-_{s}),\] (2)

where \(_{s}=(w_{s,1},,w_{s,q})\), \(_{s}=(g_{1}(x_{s}),,g_{q}(x_{s}))\), and \(=(_{1},,_{q})^{}\). Let \(_{}^{q q}\) be the covariance matrix among auxiliary feedback and \(_{}^{q 1}\) be the vector of covariance between the reward and each of its auxiliary feedback. Then, the variance of \(z_{s,q}\) is minimized by setting the coefficient vector \(\) to \(^{}=_{}^{-1}_ {}\), and the minimum value is \((1-^{2})^{2}\), where \(^{2}=_{}^{}_{ }^{-1}_{}/^{2}\) is the multiple correlation coefficient of reward and its auxiliary feedback.

However, \(_{}\) and \(_{}\) can be unknown in practice and need to be estimated to get the best estimate for \(^{}\) to achieve maximum variance reduction. In our following result, we drive the best linear unbiased estimator of \(\) (i.e., \(}_{t}\)) using \(t\) observations of rewards and their auxiliary feedback.

**Lemma 1**.: _Let \(t>q+2\) and \(f_{t}\) be the estimate of function \(f\) which uses all information available at the end of round \(t\), i.e., \(\{x_{s},y_{s},_{s}\}_{s=1}^{t}\). Then, the best linear unbiased estimator of \(^{}\) is_

\[}_{t}(_{t}^{} {W}_{t})^{-1}_{t}^{}_{t},\]

_where \(_{t}\) is a \(t q\) matrix whose \(s^{}\) row is \((_{s}-_{s})\) and \(_{t}=(y_{1}-f_{t}(x_{1}),,y_{t}-f_{t}(x_{t}))\)._

The proof follows after doing some manipulations in Eq. (2) and then using results from linear regression theory. The detailed proof of Lemma 1 and all other missing proofs are given in the supplementary material. After having a new observation of reward and its auxiliary feedback, the best linear unbiased estimator of \(^{}\) is re-estimated. If \(_{}\) or \(_{}\) are known, we can directly use them to estimate \(^{}\) by replacing \(_{t}^{}_{t}\) with \(_{}\) and \(_{t}^{}_{t}\) with \(_{}\) in Lemma 1. The following result describes the properties of the hybrid reward when \(^{}\) is replaced by \(}_{t}\) in Eq. (2).

**Theorem 1**.: _Let \(t>q+2\). If \(}_{t}\) as defined in Lemma 1 is used to compute hybrid reward \(z_{s,q}\) for any \(s t\), then \([z_{s,q}]=f(x_{s})\) and \((z_{s,q})=(1+)(1-^{2}) ^{2}\)._

The key takeaways from Theorem 1 are as follows. First, the hybrid reward using \(}_{t}\) is an unbiased estimator of reward function and hence, we can still use it to estimate the reward function \(f\). Second, there is a less reduction in variance (i.e., by a factor \((t-2)/(t-q-2)\) of maximum possible variance reduction) when \(}_{t}\) is used for constructing hybrid reward in Eq. (2).

The variance reduction (given in Theorem 1) depends on the auxiliary feedback via two terms: \(\) and \(^{2}\) (defined in Line 142). Setting \(q=1\) gives the minimum value for \(\), but \(^{2}\) for \(q=1\) will also be small as it only considers one auxiliary feedback, and hence maximum variance reduction will not be achieved. As we increase the number of auxiliary feedback, \(^{2}\) increases, leading to more variance reduction. However, the term \(\) increases at the same time, which can negate the variance reduction achieved by smaller \(^{2}\). Hence, keeping the number of auxiliary feedback used for hybrid reward small is important. A simple method for selecting a subset of auxiliary feedback (Lavenberg et al., 1982) works as follows: select the auxiliary feedback whose sample correlation coefficient with reward is the largest. Then, select the next auxiliary feedback whose sample partial correlation coefficient with reward was the largest given the first auxiliary feedback selected. Keep repeating the process until there is a variance reduction using additional auxiliary feedback.

### Linear bandits with known auxiliary feedback functions

To highlight the main ideas, we restrict to the linear bandit setting in which the reward and auxiliary feedback functions are linear. In this setting, a learner selects an action \(x_{t}\) and observes a reward \(y_{t}=x_{t}^{}^{}+_{t}\), where \(^{}^{d}\) (\(d 1\)) is unknown and \(_{t}\) is the zero-mean Gaussian noise with known variance \(^{2}\). The learner also observes \(q\) auxiliary feedback, where \(i\)-th auxiliary feedback is denoted by \(w_{t,i}=x_{t}^{}_{w,i}^{}+_{t,i}^{w}\). Here, \(_{w,i}^{}^{d}\) is known and \(_{t,i}^{w}\) is the zero-mean Gaussian noise with unknown variance \(_{w,i}^{2}\). Our goal is to learn a policy that minimizes regret as defined in Eq. (1). We later extend our method for non-linear reward and auxiliary feedback functions in Section 4.3.

Let \(I\) be the \(d d\) identity matrix, \(V_{t}_{s=1}^{d-1}x_{s}x_{s}^{}\), and \(_{t} V_{t}+ I\), where \(>0\) is the regularization parameter that ensures matrix \(_{t}\) is a positive definite matrix. The notation \(\|x\|_{A}\) denotes the weighted \(l_{2}\)-norm of vector \(x^{d}\) with respect to a positive definite matrix \(A^{d d}\).

As shown in Theorem 1, the hybrid rewards is an unbiased reward estimator with a smaller variance than observed rewards. Thus, hybrid rewards lead to tighter confidence bounds for parameter \(^{}\) than observed rewards. We propose a simple but effective method to exploit correlated auxiliary feedback, i.e., using hybrid rewards to estimate reward function instead of observed rewards.

Using this method, we adapt the well-known linear bandit algorithm OFUL (Abbasi-Yadkori et al., 2011) to our setting and named this algorithm OFUL-AF. This algorithm works as follows. It takes \(>0\) as input and then initializes \(_{1}= I\) and sets \(_{1}^{z}=0_{^{d}}\) as initial estimate of parameter \(^{}\). The superscript '\(z\)' in \(_{1}^{z}\) implies that hybrid rewards are used for estimating \(^{}\). At the beginning of round \(t\), the algorithm selects an action \(x_{t}\) that maximizes the upper confidence bound of the action's reward, which is a sum of the estimated reward for the action (\(x^{}_{t}^{z}\)) and a confidence bonus \(_{t}^{}\|x\|_{_{t}^{-1}}\). In the confidence bonus, the first term (\(_{t}^{}\)) is a slowly increasing function in \(t\) whose value is given in Theorem 2, and the second term (\(\|x\|_{_{t}^{-1}}\)) decreases to zero as \(t\) increases.

```
1:Input:\(>0\)
2:Initialization:\(_{1}= I\) and \(_{1}^{z}=0_{^{d}}\)
3:for\(t=1,2,\)do
4: Select action \(x_{t}=*{argmax}_{x}(x^{}_{t}^ {z}+_{t}\|x\|_{_{t}^{-1}})\)
5: Observe reward \(y_{t}\) and its auxiliary feedback \(\{w_{t,i}\}_{i=1}^{q}\)
6: If \(t>q+2\), compute upper bound of hybrid reward's sample variance (\(_{z,t}\)) or else \(_{z,t}=^{2}\)
7: If \(t q+2\) or \(_{z,t}^{2}\), set \(}_{t}=0\) or else compute \(}_{t}\) using Lemma 1
8:\( s t:\) compute \(z_{s,q}\) using \(}_{t}\) in Eq. (2)
9: Set \(_{t+1}=_{t}+x_{t}x_{t}^{}\), \(_{t+1}^{z}=_{t+1}^{-1}_{s=1}^{t}x_{s}z_{s,q}\)
10:endfor ```

**OFUL-AF** Algorithm for Linear Bandits with Auxiliary Feedback

After selecting an action \(x_{t}\), the algorithm observes the reward \(y_{t}\) with its associated auxiliary feedback \(\{w_{t,i}\}_{i=1}^{q}\). It computes the upper bound on sample variance of hybrid reward (denoted by \(_{z,t}\)) if \(t>q+2\) or else it is set to \(^{2}\) and then checks two conditions. The first condition (i.e., \(t q+2\)) guarantees the sample variance is well-defined. Whereas the second condition (i.e., \(_{z,t}>^{2}\)) ensures the algorithm at least be as good as OFUL because \(_{z,t}\) can be larger than \(^{2}\) due to the overestimation in initial rounds. If both conditions \(t q+2\) and \(_{z,t}^{2}\) fail, the value of \(}_{t}\) is re-computed as defined in Lemma 1. The updated \(}_{t}\) is then used to update all hybrid rewards, i.e., \(z_{s,q},\  s t\). Finally, the values of \(_{t+1}\) and \(_{t+1}\) are updated as \(_{t+1}=_{t}+x_{t}x_{t}^{}\) and \(_{t+1}^{z}=_{t+1}^{-1}_{s=1}^{t}x_{s}z_{s,q}\), which are then used to select the action in the following round. When \(}_{t}=0\) for all hybrid rewards, hybrid rewards are the same as the observed rewards, and hence OFUL-AF is the same as OFUL.

The regret analysis of any bandit algorithm hinges on bounding the instantaneous regret for each action. The following result gives an upper bound on the instantaneous regret of OFUL-AF.

**Theorem 2**.: _With a probability of at least \(1-2\), the instantaneous regret of OFUL-AF in round \(t\) is_

\[r_{t}() 2(_{t}^{}+^{1/2}S) \|x_{t}\|_{_{t}^{-1}},\]

_where \(_{t}^{}=,_{z,t-1})}\ _{t}\), \(\|^{}\|_{2} S\), and \(_{t}=/}{})}\). For \(t>q+2\) and \(_{z,t}<^{2}\), \([r_{t}()](()}{t-q-3})^{}r_{t}( )).\) Here, \(\) hides constant terms._The proof follows by bounding the estimation error of the parameter \(^{}\) when the estimation method uses auxiliary feedback. This result shows that auxiliary feedback leads to a better instantaneous regret upper bound and a better regret (as defined in Eq. (1)) than the vanilla OFUL algorithm. Since the improvement in instantaneous regret increase with \(t\), having a single constant to compare with regret of OFUL may lead to weaker regret upper bound than the sum of all instantaneous regret.

## 4 Estimated auxiliary feedback functions

Auxiliary feedback functions may be unknown in many real-life problems. However, the learner can construct an unbiased estimator for the auxiliary feedback function using historical data or acquiring more samples of auxiliary feedback. But these estimated functions offer a lower variance reduction than known auxiliary functions. To study the effect of using the estimated auxiliary feedback functions on the performance of bandit algorithms, we borrow some techniques from approximate control variate theory (Gorodetsky et al., 2020; Pham and Gorodetsky, 2022) as we discussed next.

### Approximate control variates

Let \(y\) be an unbiased estimator of an unknown variable \(\) and a random variable \(w\) with a known estimated mean (\(_{e}\)) be a control variate of \(y\). As long as the known estimated mean is an unbiased estimator of \(w\), one can use it to reduce the variance of \(y\) as follows. For any choice of a coefficient \(_{e}\), define a new random variable as \(z_{e} y-_{e}\), where \(=w-_{e}\). Since \(_{e}\) is an unbiased estimator of \(w\), it is straightforward to show that \(z_{e}\) is also an unbiased estimator of \(y\).

By using properties of variance and covariance, the variance of \(z_{e}\) is given by

\[(z_{e})=(y)+_{e}^{2}(,)-2 _{e}(y,).\]

The variance of \(z_{e}\) is minimized by setting \(_{e}\) to \(_{e}^{}=(,)^{-1}(y,)\) and the minimum value of \((z_{e})\) is \((1-_{e}^{2})(y)\), where \(_{e}=(y,)((,)^{-1}/(y))(y,)\).

### Auxiliary feedback with unknown functions as approximate control variates

We now introduce a new definition of _hybrid reward_ that uses estimated auxiliary feedback functions. Let \(w_{s,i}\) be the \(i^{}\) auxiliary feedback observed with reward \(y_{s}\) and \(g_{e,i}\) be the unbiased estimator of function \(g_{i}\). Then, the hybrid reward with \(q\) estimated auxiliary feedback functions is defined by

\[z_{e,s,q}=y_{s}-_{i=1}^{q}_{e,i}(w_{s,i}-g_{e,i}(x_{t}))=y_{s}-( _{s}-_{e,s})_{e}.\] (3)

where \(_{s}=(w_{s,1},,w_{s,q})\), \(_{e,s}=(g_{e,1}(x_{s}),,g_{e,q}(x_{s}))\), and \(_{e}=(_{e,1},,_{e,q})^{}\). Let \(_{}}}^{q q}\) denote the covariance matrix among centered auxiliary feedback (i.e., \(}_{s}=_{s}-_{e,s}\)), and \(_{}}^{q 1}\) denote the vector of covariance between reward and its centered auxiliary feedback. Then, the variance of \(z_{e,s,q}\) is minimized by setting the \(_{e}\) to \(_{e}^{}=_{}}}^{-1}_{ {}}\), and the minimum value of \((z_{e,s,q})\) is \((1-_{e}^{2})^{2}\), where \(_{e}^{2}=_{}}^{}_{}} }^{-1}_{}}/^{2}\).

The definition of hybrid reward given in Eq. (3) is very flexible and allows different estimators to estimate auxiliary feedback functions. The only difference among these estimators is how they partition the available samples of auxiliary feedback to estimate auxiliary function \(g_{i}\). As no optimal partitioning strategy is known, we adopt the Independent Samples (IS) and Multi-Fidelity (MF) sampling strategy for our setting where finite samples of auxiliary feedback are available. Both strategies are proven to be asymptotically optimal (Gorodetsky et al., 2020), implying the variance reduction is asymptotically the same as if auxiliary feedback functions are known.

IS and MF sampling strategy:Let \(s\) and \(s_{i} s\) be the sample sets used for estimating functions \(f\) and \(g_{i}\), respectively. Then, for the IS sampling strategy, \((s_{i} s)(s_{j} s)=\) for \(i j\), i.e., the extra samples used for estimating the function \(g_{i}\) are unique. Whereas, for the MF sampling strategy, \(s_{i}=s_{j=1}^{i}s_{j}^{}\) and \(s_{i}^{} s_{j}^{}=\) for \(i j\), i.e., the estimation of function \(g_{i}\) uses the samples that were used for estimating function \(g_{i-1}\) with some additional samples. Refer to Fig. 1 for the visual representation of both sampling strategies. After adopting Theorem 3 and Theorem 4 from Gorodetsky et al. (2020) to our setting, we can further simplify \(_{}}}\) and \(_{}}\) when IS and MF sampling strategies (denoted by \(e\)) are used for estimating auxiliary feedback functions as follows:

\[_{}}}=(_{} _{e})/t_{}}=((_{e} )_{})/t,\]

where \(t\) denotes the number of reward observations with its auxiliary feedback, \((A)\) represents a vector whose elements are the diagonal of the matrix \(A\), and \(\) denotes an element-wise product. The \(ij\)-th element of matrix \(_{e}^{q q}\) is

\[f_{e,ij}=\{((r_{i}-1)(r_{j}-1))/(r_{i}r_{j})&i  je=\\ ((r_{i},r_{j})-1)/(r_{i},r_{j})&i je=\\ (r_{i}-1)/r_{i}&,.\]

where \(r_{i}^{+}\) is the ratio between the total number of samples used for estimating function \(g_{i}\) by sampling strategy \(e\) and the total number of samples used for estimating \(f\).

Since \(_{}}}\) and \(_{}}\) may be unknown, they must be estimated to get the best estimate for \(^{}\). Our following result gives the best linear unbiased estimator of \(_{e}\) (i.e., \(}_{e,t}\)) that uses \(t\) observations of rewards and their auxiliary feedback with estimated auxiliary feedback functions.

**Lemma 2**.: _Let \(t>q+2\), \(e\) is the sampling strategy, and \(f_{t}\) be the estimate of function \(f\) at the end of round \(t\) which uses \(\{x_{s},y_{s},_{s}\}_{s=1}^{t}\). Then, the best linear unbiased estimator of \(_{e}^{}\) is_

\[}_{e,t}=(_{t}^{}_{t}_{e} )^{-1}((_{e})_{t}^{} {Y}_{t}),\]

_where \(_{t}\) is a \(t q\) matrix whose \(s^{}\) row is \(_{s}-_{e,s}\) and \(_{t}=(y_{1}-f_{t}(x_{1}),,y_{t}-f_{t}(x_{t}))\)._

After adopting matrix manipulation tricks from Pham and Gorodetsky (2022) to our setting, the proof follows similar steps as the proof of Lemma 1. We now characterize the properties of the hybrid reward that uses either IS or MF sampling strategy for estimating auxiliary feedback functions.

**Theorem 3**.: _Let \(t>q+2\) and \(e\) is the sampling strategy. If \(}_{e,t}\) as defined in Lemma 2 is used to compute hybrid reward \(z_{e,s,q}\) for any \(s t\), then \([z_{e,s,q}]=f(x_{s})\) and \((z_{e,s,q})=(1+)(1-_{e} ^{2})^{2}\), where \(a()=1\), \(a()=\) if \(r_{i}=r,\; i\{1,2,,q\}\) when using MF sampling strategy for estimating auxiliary feedback functions._

The key takeaways from Theorem 3 are as follows. First, the hybrid reward with estimated auxiliary feedback is still an unbiased estimator, so one can use it to estimate the reward function \(f\). Second, there is a potential loss in variance reduction as it has an extra multiplicative factor \(a(e)\) and \(_{e}^{2}^{2}\).

_Remark 1_.: As samples for estimating auxiliary functions increase compared to the reward function, the variance reduction from IS and MF sampling strategy converges to the reduction achieved using known auxiliary functions. As \( i:r_{i}\), then \(_{e}_{q q}\). It is now straightforward to see that \(_{}}}\) will become \(_{}\), \(_{}}\) will become \(_{}\), and hence \(_{e}^{2}=^{2}\) as \( i:r_{i}\).

_Remark 2_.: The IS and MF sampling strategies are shown to be asymptotically optimal (Gorodetsky et al., 2020), i.e., the variance reduction achieved by both strategies is asymptotically the same as if

Figure 1: **Left two figures:** Visualization of IS and MF sampling strategies. Each column represents samples used for estimating function (written at the top), and the same color is used to show shared samples among auxiliary function estimation. **Right two figures:** Interaction between AFC bandit algorithm and environment. AFC bandit algorithm that only uses observed rewards (second from right), and AFC bandit algorithm that also uses auxiliary feedback as hybrid rewards (rightmost).

auxiliary feedback functions are known. However, both sampling strategies are useful in different applications, e.g., the IS sampling strategy suits the problems in which different auxiliary feedback can be independently sampled. In contrast, the MF sampling suits the problems where auxiliary feedback can not be sampled independently.

### Parameterized bandits with estimated auxiliary feedback functions

We now consider the parameterized bandit setting described in Section 2, where the reward and auxiliary feedback function can be non-linear. To exploit the available auxiliary feedback in linear bandits, we propose a method in Section 3.3 that uses hybrid reward in place of rewards to get tight upper confidence bound for the estimator of an unknown reward function and hence smaller regret as compared to the vanilla OFUL due to the smaller variance of the hybrid rewards. We generalize this observation and introduce the notion of _Auxiliary Feedback Compatible (AFC)_ bandit algorithm.

**Definition 1** (**AFC Bandit Algorithm**).: Any bandit algorithm \(\) is _Auxiliary Feedback Compatible_ if: (i) \(\) can use correlated reward samples to construct upper confidence bound for reward function and (ii) with probability \(1-\), its estimated reward function \(f_{t}^{}\) has the following property:

\[|f_{t}^{}(x)-f(x)| h(x,_{t})+l(x,_{t}),\]

where \(x\), \(^{2}\) is the variance of Gaussian noise in observed reward, and \(_{t}\) denotes the observations of actions and their rewards with the parameters of \(\) at the beginning of round \(t\).

As the estimated coefficient vector uses all past samples, the resultant hybrid rewards become correlated due to using this estimated coefficient vector. Bandit algorithms like UCB1 (Auer et al., 2002) and kl-UCB (Cappe et al., 2013) are not AFC as they need independent reward samples to construct upper confidence bounds. In contrast, bandit algorithms like OFUL (Abbasi-Yadkori et al., 2011), Lin-UCB (Chu et al., 2011), UCB-GLM (Li et al., 2017), IGP-UCB, and GP-TS (Chowdhury and Gopalan, 2017) are AFC as they all use techniques proposed in Abbasi-Yadkori et al. (2011) for building the upper confidence bound, which does not need reward samples to be independent.

As AFC bandit algorithms use the noise variance of observed reward for constructing the confidence upper bound, they can also exploit available auxiliary feedback by replacing reward with its respective hybrid reward as shown in Fig. 1 (rightmost figure). We next give an upper bound on the instantaneous regret for any AFC bandit algorithm that uses hybrid rewards instead of observed rewards.

**Theorem 4**.: _Let \(\) be an AFC bandit algorithm with \(|f_{t}^{}(x)-f(x)| h(x,_{t})+l(x,_{t})\) and \(_{e,z,t}\) be the upper bound on sample variance of hybrid reward, whose value is set to \(^{2}\) for \(t q+2\). Then, with a probability of at least \(1-2\), the instantaneous regret of \(\) after using hybrid rewards (named \(\)-AF) for reward function estimation in round \(t\) is_

\[r_{t}() 2(,(_{e,z,t})^{} )h(x,_{t})+l(x,_{t}),\]

_where \(e=\{\}\), and KF denotes the case where auxiliary functions are known. For \(t>q+2\) and \(_{e,z,t}<^{2}\), where \(a()=1\)._

After using Theorem 1 and Theorem 3 to replace the variance of hybrid reward, the proof follows similar steps as the proof of Theorem 2. We have given more details about the values of \(h(x,_{t})\) and \(l(x,_{t})\) for different AFC bandit algorithms in Table 1.

  AFC bandit algorithm & \(h(x,_{t})\) & \(l(x,_{t})\) \\  OFUL (Abbasi-Yadkori et al., 2011) & \(/}{})\|x\|_{_{t} ^{-1}}}\) & \(^{}S\|x\|_{_{t}^{-1}}\) \\ Lin-UCB (OFUL for contextual setting) & \(/}{})\|x\|_{_{t} ^{-1}}}\) & \(^{}S\|x\|_{_{t}^{-1}}\) \\ GLM-UCB (Li et al., 2017) & \((1+2t/d)+(1/)}_{t}^{-1 }}}{}\) & 0 \\ IGP-UCB (Chowdhury and Gopalan, 2017) & \(+1+(1/))}_{t-1}(x)\) & \(B_{t-1}(x)\) \\  

Table 1: Values of \(h(x,_{t})\) and \(l(x,_{t})\) for different AFC bandit algorithmsExperiments

To validate our theoretical results, we empirically demonstrate the performance gain due to auxiliary feedback in different settings of parameterized bandits. We repeat all our experiments \(50\) times and show the regret as defined in Eq. (1) with a \(95\%\) confidence interval (vertical line on each curve shows the confidence interval). Due to space constraints, the details of used problem instances are given in Appendix A.5 of the supplementary material.

Comparing regret with benchmark bandit algorithms:We considered three bandit settings for our experiments: linear bandits, linear contextual bandits, and non-linear contextual bandits (details are given in Appendix A.5). The formal setting of a contextual bandits with auxiliary feedback is given in Appendix A.4. We used the following existing bandit algorithms for these settings: OFUL (Abbasi-Yadkori et al., 2011) for linear bandits, Lin-UCB (Chu et al., 2011) for linear contextual bandits, and Lin-UCB with the polynomial kernel (which we named NLin-UCB) for non-linear contextual bandits. We compare the performance of these benchmark bandit algorithms with four different variants of our algorithms. The first variant assumes the auxiliary feedback functions are known (highlighted by adding '-AF' to the benchmark algorithms). When auxiliary feedback functions are unknown, we use IS and MF sampling strategy while maintaining \(r=2\) (i.e., getting one extra sample of auxiliary feedback in each round). The IS and MF sampling strategies are the same when only one auxiliary feedback exists. Since we only use one auxiliary feedback in our experiments, we highlight this variant by adding '-IS/MF' to the benchmark algorithms. When IS and MF sampling strategies are used, one needs to update the auxiliary feedback functions in each round to get better estimators. However, it leads to the re-computation of all variables that are needed for updating the hybrid rewards, which is not needed when auxiliary feedback functions are fixed. Therefore, we consider two more computationally efficient variants for the unknown auxiliary functions setting. One variant assumes the knowledge of biased auxiliary feedback, i.e., \(g_{i}(x)+_{g}\) is available instead of \(g_{i}(x)\) (highlighted by adding '-BE' to the benchmark algorithms). Another variant assumes that some initial samples of auxiliary feedback are available, which are used to get the auxiliary feedback function estimator. We highlight this variant by adding '-EH' to the benchmark algorithms. All variants with given parameters perform better than benchmark bandit algorithms (see Fig. 1(a), Fig. 1(b), and Fig. 1(c)). We observe the expected performance among these variants as the variant with a known auxiliary feedback function outperforms all other variants. At the same time, IS/MF sampling strategy-based variant outperforms the other two heuristic variants for the setting of unknown auxiliary feedback function.

**Regret vs. different biased estimator:** To know the effect of bias in auxiliary feedback (i.e., \(_{g}\)) in the mean value of auxiliary feedback, we run an experiment with the same linear contextual bandits experiment setup mentioned above. To see the variation in regret, we set \(_{g}=\{1,0.2,0.1,0.07,0.05\}\). As shown in Fig. 1(d), the regret increases with an increase in bias and even starts performing poorly than Lin-UCB. This experiment demonstrates that as long as the bias in auxiliary feedback is within a limit, there will be an advantage to using this computationally efficient variant.

**Regret vs. number of historical samples of auxiliary feedback :** Increasing the number of historical samples of auxiliary feedback for estimating the auxiliary feedback function reduces the error in its estimation, leading to better performance. To observe this, we use estimators using different numbers of auxiliary feedback samples, i.e., \(n_{h}=\{5,7,10,15,20\}\) in linear contextual bandits setting. As expected, the regret decreases with an increase in auxiliary feedback samples, but using an estimator with a few samples even performs poorly than Lin-UCB, as shown in Fig. 1(e).

**Regret vs. correlation coefficient:** As theoretical results imply that the regret decreases when the correlation between reward and auxiliary feedback increases. To validate this, we used problem instances with different correlation coefficients in linear contextual bandits setting. As expected, we observe that the regret decreases as the correlation coefficient increases, as shown in Fig. 1(f).

## 6 Conclusion

This paper studies a novel parameterized bandit problem in which a learner observes auxiliary feedback correlated with the observed reward. We first introduce the notion of 'hybrid reward,' which combines the reward and its auxiliary feedback. To get the maximum benefit from hybrid reward, we treat auxiliary feedback as a control variate and then extend control variate theory to a setting 

[MISSING_PAGE_FAIL:10]

Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric Bandits: The Generalized Linear Case. In _Proc. NeurIPS_, pages 586-594, 2010.
* Li et al. (2017) Lihong Li, Yu Lu, and Dengyong Zhou. Provably Optimal Algorithms for Generalized Linear Contextual Bandits. In _Proc. ICML_, pages 2071-2080, 2017.
* Jun et al. (2017) Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, and Rebecca Willett. Scalable Generalized Linear Bandits: Online Computation and Hashing. In _Proc. NeurIPS_, pages 99-109, 2017.
* Valko et al. (2013) Michal Valko, Nathan Korda, Remi Munos, Ilias Flaounas, and Nello Cristianini. Finite-time Analysis of Kernelised Contextual Bandits. In _Proc. UAI_, pages 654-663, 2013.
* Chowdhury and Gopalan (2017) Sayak Ray Chowdhury and Aditya Gopalan. On Kernelized Multi-armed Bandits. In _Proc. ICML_, pages 844-853, 2017.
* Alon et al. (2015) Noga Alon, Nicolo Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online Learning with Feedback Graphs: Beyond Bandits. In _Proc. COLT_, pages 23-35, 2015.
* Wu et al. (2015) Yifan Wu, Andras Gyorgy, and Csaba Szepesvari. Online Learning with Gaussian Payoffs and Side Observations. In _Proc. NeurIPS_, pages 1360-1368, 2015.
* Lavenberg and Welch (1981) Stephen S Lavenberg and Peter D Welch. A Perspective on the Use of Control Variables to Increase the Efficiency of Monte Carlo Simulations. _Management Science_, pages 322-335, 1981.
* Lavenberg et al. (1982) Stephen S Lavenberg, Thomas L Moeller, and Peter D Welch. Statistical Results on Control Variables with Application to Queueing Network Simulation. _Operations Research_, pages 182-202, 1982.
* Verma and Hanawal (2021) Arun Verma and Manjesh K Hanawal. Stochastic Multi-Armed Bandits with Control Variates. In _Proc. NeurIPS_, pages 27592-27603, 2021.
* Nelson (1990) Barry L Nelson. Control Variate Remedies. _Operations Research_, pages 974-992, 1990.
* Kreutzer et al. (2017) Julia Kreutzer, Artem Sokolov, and Stefan Riezler. Bandit Structured Prediction for Neural Sequence-to-Sequence Learning. In _Proc. ACL_, pages 1503-1513, 2017.
* Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Vlassis et al. (2021) Nikos Vlassis, Ashok Chandrashekar, Fernando Amat Gil, and Nathan Kallus. Control Variates for Slate Off-Policy Evaluation. In _Proc. NeurIPS_, pages 3667-3679, 2021.
* Caron et al. (2012) Stephane Caron, Branislav Kveton, Marc Lelarge, and S Bhagat. Leveraging Side Observations in Stochastic Bandits. In _Proc. UAI_, pages 142-151, 2012.
* Mannor and Shamir (2011) Shie Mannor and Ohad Shamir. From Bandits to Experts: On the Value of Side-Observations. In _Proc. NeurIPS_, pages 684-692, 2011.
* Kocak et al. (2014) Tomas Kocak, Gergely Neu, Michal Valko, and Remi Munos. Efficient learning by implicit exploration in bandit problems with side observations. In _Proc. NeurIPS_, pages 613-621, 2014.
* Alon et al. (2017) Noga Alon, Nicolo Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir. Nonstochastic Multi-Armed Bandits with Graph-Structured Feedback. _SIAM Journal on Computing_, pages 1785-1826, 2017.
* Verma et al. (2019) Arun Verma, Manjesh K Hanawal, Csaba Szepesvari, and Venkatesh Saligrama. Online Algorithm for Unsupervised Sensor Selection. In _Proc. AISTATS_, pages 3168-3176, 2019.
* Verma et al. (2020a) Arun Verma, Manjesh K Hanawal, and Nandyala Hemachandra. Thompson Sampling for Unsupervised Sequential Selection. In _Proc. ACML_, pages 545-560, 2020a.
* Verma et al. (2020b) Arun Verma, Manjesh K Hanawal, Csaba Szepesvari, and Venkatesh Saligrama. Online Algorithm for Unsupervised Sequential Selection with Contextual Information. In _Proc. NeurIPS_, pages 778-788, 2020b.
* James (1985) B. A. P. James. Variance Reduction Techniques. _Journal of the Operational Research Society_, pages 525-530, 1985.
* Johnson et al. (2018)Barry L Nelson. Batch size effects on the efficiency of control variates in simulation. _European Journal of Operational Research_, pages 184-196, 1989.
* Botev and Ridder  Zdravko Botev and Ad Ridder. Variance Reduction. _Wiley statsRef: Statistics reference online_, pages 1-6, 2017.
* Chen and Ghahramani  Yutian Chen and Zoubin Ghahramani. Scalable Discrete Sampling as a Multi-Armed Bandit Problem. In _Proc. ICML_, pages 2492-2501, 2016.
* Gorodetsky et al.  Alex A Gorodetsky, Gianluca Geraci, Michael S Eldred, and John D Jakeman. A Generalized Approximate Control Variate Framework for Multifidelity Uncertainty Quantification. _Journal of Computational Physics_, page 109257, 2020.
* Pham and Gorodetsky  Trung Pham and Alex A Gorodetsky. Ensemble Approximate Control Variate Estimators: Applications to MultiFidelity Importance Sampling. _Journal on Uncertainty Quantification_, pages 1250-1292, 2022.
* Auer et al.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit Problem. _Machine Learning_, pages 235-256, 2002.
* Cappe et al.  Olivier Cappe, Aurelien Garivier, Odalric-Ambrym Maillard, Remi Munos, and Gilles Stoltz. Kullback-Leibler Upper Confidence Bounds for Optimal Sequential Allocation. _The Annals of Statistics_, pages 1516-1541, 2013.
* Hayashi  F Hayashi. _Econometrics_. Princeton University Press, 2000.
* Van De Geer  Sara A Van De Geer. Estimation. _Encyclopedia of Statistics in Behavioral Science_, pages 549-553, 2005.
* Schmeiser  Bruce Schmeiser. Batch Size Effects in the Analysis of Simulation Output. _Operations Research_, pages 556-568, 1982.

Supplementary material

### Missing proofs related to auxiliary feedback

#### Results from linear regression

We first state results for linear regression that we will use in the subsequent proofs. Consider the following regression problem with \(t\) samples and \(q\) features:

\[z_{s}=_{s}^{}+_{s},\ \ i\{1,2,,t\}\]

where \(z_{s}\) is the \(s^{}\) target variable, \(_{s}=(x_{s1},,x_{sq})^{q}\) is the \(s^{}\) feature vector, \(^{q}\) is the unknown regression parameters, and \(_{s}\) is a normally distributed noise with mean \(0\) and constant variance \(^{2}\). The values of noise \(_{s}\) form an IID sequence and are independent of \(_{s}\). Let

\[_{t}=z_{1}\\ \\ z_{t},_{t}=x_{11}&&x_{1q}\\ &&\\ X_{t1}&&x_{tq},_{t}= _{1}\\ \\ _{t}.\]

Then, the best linear unbiased estimator of \(\) is \(}_{t}=(_{t}^{}_{t})^{-1}_{t}^{} _{t}\), which has the following finite sample properties.

_Fact 1_.: The following are the finite sample properties of the least square estimator \(}_{t}\):

\[1.\,[}_{t}|_{t}]= ,\] (unbiased estimator) \[2.\,\,(}_{t}|_{t})=^{2}( _{t}^{}_{t})^{-1},\] (expression for the variance) \[3.\,\,(}_{tti}|_{t})=^{2}( _{t}^{}_{t})_{ii}^{-1},\] (element-wise variance)

where \((_{t}^{})_{ii}^{-1}\) is the \(ii-\)element of the matrix \((_{t}^{})^{-1}\).

In the above result, the first two properties are from Proposition 1.1 of Hayashi (2000), whereas the third property is from Van De Geer (2005). The following result gives the finite sample properties of the estimator of noise variance \(^{2}\).

_Fact 2_.: (Hayashi, 2000, Proposition 1.2) Let \(_{t}^{2}=_{s=1}^{t}(z_{s}-_{s}^{}}_{t})^{2}\) be estimator of \(^{2}\) and \(t>q\) (so that \(_{t}^{2}\) is well defined). Then, \(_{t}^{2}\) is an unbiased estimator of \(^{2}\), i.e., \([_{t}^{2}|_{t}]=^{2}\).

Using the Schur complement, we have the following results about the inverse of the block matrix.

_Fact 3_.: Let \(G=t&B\\ C&D\) be a block matrix, where \(t\{0\}\), \(B\), \(C\), \(D\) are respectively \(1 q\), \(q 1\), and \(q q\) matrices of real numbers. Then, \(G_{11}^{-1}=t^{-1}+t^{-1}B(tD-CB)^{-1}C\).

#### Control variates theory

Let \(y\) be the random variable of interest with unknown mean \(\). There are \(q\) control variates correlated with \(y\), where \(i^{}\) control variate has mean \(_{i}\) and its \(s^{}\) observation is denoted by \(w_{s,i}\). For any \(s\{1,,t\}\), we define a variable \(z_{s}\) using \(s^{}\) observation of \(y_{s}\) and its control variates as follows:

\[z_{s}=y_{s}-(_{s}-),\]

where \(_{s}=(w_{s,1},,w_{s,q})\) and \(=(_{1},,_{q})\). The above equation can be re-written as:

\[y_{s}=z_{s}+(_{s}-).\]

Under the assumption of \(z_{s}\) being a unbiased estimator of \(\), we can write \(y_{s}\) as follows:

\[y_{s}=+(_{s}-)+_{z,s}.\]

where \(_{z,1},,_{z,t}\) are IID and have zero mean Gaussian noise with variance \((1-^{2})^{2}\). Let

\[}_{t}=y_{1}\\ \\ y_{t},\,}_{t}=1&_{1}-\\ &\\ 1&_{t}-,\,\,=\\ ,\,\,\,_{z,t}= _{z,1}\\ \\ _{z,t}.\]The best linear unbiased estimator of \(\) is \(}=(}_{t}^{}}_{t})^{-1}}_{t}^{}}_{t}\). To get \(_{z,t}\) and \(^{}\), we expand \(}\) as follows:

\[} =1&_{1}-\\ &\\ 1&_{t}-^{}1&_{1}-\\ &\\ 1&_{t}-^{-1}1&_{1}-\\ &\\ 1&_{t}-^{}y_{1}\\ \\ y_{t}\] \[=1&&1\\ _{1}-&&_{t}- 1&_{1}-\\ &\\ 1&_{t}-^{-1}1&&1\\ _{1}-&&_{t}- y_{1}\\ \\ y_{t}\] \[=t&_{s=1}^{t}(_{s}-)\\ _{s=1}^{t}(_{s}-)&_{s=1}^{t}(_{s}-) ^{}(_{s}-)^{-1}_{s=1}^{t }y_{s}\\ _{s=1}^{t}(_{s}-)y_{s}\]

After taking first matrix from RHS to LHS and using \(}=_{z,t}\\ }_{t}\), we have

\[t&_{s=1}^{t}(_{s}-)\\ _{s=1}^{t}(_{s}-)&_{s=1}^{t}(_{s}-)^ {}(_{s}-)_{z,t}\\ }_{t}=_{s=1}^{t}y_{s}\\ _{s=1}^{t}(_{s}-)y_{s}.\] (4)

From above, we get the following equation:

\[t_{z,t}+(_{s=1}^{t}(_{s}-) )}_{t}=_{s=1}^{t}y_{s}\] \[_{z,t}=_{s=1}^{t}y_{s}-( _{s=1}^{t}(_{s}-))}_{t}.\]

Using \(_{y,t}=_{s=1}^{t}y_{s}\) and \(}_{t}=_{s=1}^{t}{_{s}}\), we get

\[_{z,t}=_{y,t}-(}_{t}-)}_{t}.\] (5)

Similarly, we have another equation as follows:

\[_{z,t}_{s=1}^{t}(_{s}-)+( _{s=1}^{t}(_{s}-)^{}(_{s}-))) }_{t}=_{s=1}^{t}(_{s}-)y_{s}\] \[}_{t}=(_{s=1}^{t}(_{s}- )^{}(_{s}-))^{-1}(_{s=1}^{t}( _{s}-)(y_{s}-_{z,t})).\]

Using \(_{t}=_{1}-\\ \\ _{t}-\) and \(_{t}=y_{1}-_{z,t}\\ \\ y_{t}-_{z,t}\), we have

\[}_{t}=(_{t}^{}_{t})^{-1}_{t}^{ }_{t}.\] (6)

In the following, we first state the fundamental results from the control variates theory.

_Fact_ 4. (Nelson, 1990, Theorem 1) Let \(O_{s}=(Y_{s},W_{s,1},,W_{s,q})^{}\) follow a \((q+1)-\)variate normal distribution with mean vector \((,)\) and \(\{O_{1},,O_{t}\}\) be a IID sequence. Assume \(_{z,t}=_{s=1}^{t}z_{s}\), where \(z_{s}=y_{s}-(_{s}-)}_{t}\) and \(}_{t}\) used here is given by Eq. (6), then

\[[_{z,t}] =\] \[(_{z,t}) =(1+)(1-^{2})(_{y,t}),\]

where \(_{Y}_{}^{-1}_{Y}^{}/^{2}\) is the square of the multiple correlation coefficient, \(^{2}=(Y)\), and \(_{Y}=((Y,W_{1}),,(Y,W_{q}))\) (we have dropped the subscript \(s\) as observations are IID).

**Auxiliary feedback as control variates**

**Lemma 1**.: _Let \(t>q+2\) and \(f_{t}\) be the estimate of function \(f\) which uses all information available at the end of round \(t\), i.e., \(\{x_{s},y_{s},_{s}\}_{s=1}^{t}\). Then, the best linear unbiased estimator of \(}\) is_

\[}_{t}(_{t}^{}_{t})^{-1}_{t}^{} _{t},\]

_where \(_{t}\) is a \(t q\) matrix whose \(s^{}\) row is \((_{s}-_{s})\) and \(_{t}=(y_{1}-f_{t}(x_{1}),,y_{t}-f_{t}(x_{t}))\)._

Proof.: Recall Eq. (2) for \(s^{}\) hybrid reward with known auxiliary functions, i.e., \(z_{s,q}=y_{s}-(_{s}-_{s})\), which can be re-written as \(y_{s}=z_{s,q}+(_{s}-_{s})\). By definition, \(z_{s,q}=f(x_{s})+_{s}^{z}\) for optimal \(\), where \(_{z,s}\) is zero-mean Gaussian noise with variance \((1-^{2})^{2}\). Then, \(y_{s}=f(x_{s})+(_{s}-_{s})+_{z,s}\). Let \(\) be an unknown function that maps every \(x\) to a space where \(f(x)=(x)^{}f\) holds. Then we can re-write the above equation as follows:

\[y_{s}=f^{}(x_{s})+(_{s}-_{s})+_{z,s}.\]

Adapting Eq. (4) to our setting, we have

\[_{s=1}^{t}(x_{s})^{}(x_{s})&_{s=1}^{ t}(x_{s})^{}(_{s}-_{s})\\ _{s=1}^{t}(_{s}-_{s})^{}(x_{s})&_{s=1}^{t}(_{s}-_{s})^{}(_{s}-_{s})f_{t} \\ }_{t}=_{s=1}^{t}(x_{s}) y_{s}\\ _{s=1}^{t}(_{s}-_{s})y_{s}.\]

Let \(f_{t}\) is the estimated \(f\) using available information, i.e., \(\{x_{s},y_{s},_{s}\}_{s=1}^{t}\). To get best linear unbiased estimator for \(}\), we use the following equation from above matrix,

\[&(_{s=1}^{t}(_{s}-_{s})^{} (x_{s}))f_{t}+(_{s=1}^{t}(_{s}-_{s})^{} (_{s}-_{s}))}_{t}=_{s=1}^{t}(_{s}- _{s})y_{s}\\ &(_{s=1}^{t}(_{s}-_{s})^{} (_{s}-_{s}))}_{t}=_{s=1}^{t}(_{s}- _{s})y_{s}-_{s=1}^{t}(_{s}-_{s})^{}((x_ {s})^{}f_{t})\\ &}_{t}=(_{s=1}^{t}(_{s}- _{s})^{}(_{s}-_{s}))^{-1}_{s=1}^{t}(_{s} -_{s})(y_{s}-(x_{s})^{}f_{t})\\ &}_{t}=(_{s=1}^{t}(_{s}- _{s})^{}(_{s}-_{s}))^{-1}_{s=1}^{t}(_{s} -_{s})(y_{s}-f_{t}(x_{s}))\]

Using definition \(f_{t}(x_{s})=(x_{s})^{}f_{t}\), \(_{t}=_{1}-_{s}\\ \\ _{t}-_{s}\), and \(_{t}=y_{1}-f_{t}(x_{1})\\ \\ y_{t}-f_{t}(x_{t}).\), we get

\[}_{t}=(_{t}^{}_{t})^{-1}_{t}^{ }_{t}.\]

Since the reward and its auxiliary feedback observations are functions of the selected action, we can not directly use the control variate theory due to parameterized mean values of the reward and its auxiliary feedback. To overcome this challenge, we centered the observations by its function value and defined new centered variables as follows:

\[y_{s}^{c}=y_{s}-f(x_{s}),\ \ \ _{s}^{c}=_{s}-_{s},\ \ z_{s,q}^{c}=z_{s,q}-f(x_{s}).\]

In our setting, these centered variables \((y_{s}^{c},_{s}^{c},\) and \(z_{s}^{c},\) follow zero mean Gaussian distributions with variance \(^{2}\), \(_{w}^{2}=(_{w,1}^{2},,_{w,q}^{2})\), and \((1-^{2})^{2}\), respectively.

**Theorem 1**.: _Let \(t>q+2\). If \(}_{t}\) as defined in Lemma 1 is used to compute hybrid reward \(z_{s,q}\) for any \(s t\), then \([z_{s,q}]=f(x_{s})\) and \((z_{s,q})=(1+)(1-^{2}) ^{2}\)._

Proof.: The sequence \((y_{s}^{c},_{s}^{c})_{s=1}^{t}\) is an IID sequence and follows a Gaussian distribution with mean 0. We now define \(z_{s,q}^{c}=y_{s}^{c}-_{s}^{c}=y_{s}^{c}-(_{s}-_{s}) \), which can be re-written as \(y_{s,q}^{c}=z_{s}^{c}+(_{s}-_{s})\). Let \(f_{t}\) be the estimated \(f\) using available information, i.e.,and hence we can write estimated \(z_{s,q}\) as \(_{s,q}=f_{t}(x_{s})\) and hence \(_{s,q}^{c}=f_{t}(x_{s})-f(x_{s})\). Now, adapting Eq. (6) to our setting and replacing estimated mean in \(_{t}\) by \(_{s,q}^{c}\), \(s^{}\) value of \(_{t}\) is \(y_{s}^{c}-_{s,q}^{c}=y_{s}-f(x_{s})-(f_{t}(x_{s})-f(x_{s}))=y_{s}-f_{t} (x_{s})\). With these manipulations, we get the following best linear unbiased estimator for \(^{}\):

\[}_{t}=(_{t}^{}_{t})^{-1}_{t}^{} _{t},\]

which is the same as defined in Lemma 1.

By adapting Fact 4 for a single sample (i.e., \(z_{s,q}\)) while using \(}_{t}\) to define hybrid reward, we have

\[[z_{s,q}^{c}]=0\] \[(z_{s,q}^{c})=(1+) (1-^{2})(y_{s}^{c}),\]

By extending the definition of \([z_{s,q}^{c}]\) we have,

\[[z_{s,q}-f(x_{s})]=0[z_{s,q} ]-f(x_{s})=0[z_{s,q}]=f(x_{s})\]

This proofs the hybrid reward is an unbiased estimator of reward.

Since variance is invariant to constant change, we have

\[(z_{s,q}) =(z_{s,q}-f(x_{s}))\] \[=(z_{s,q}^{c})\] \[=(1+)(1-^{2})(y_{s}^ {c})\] \[=(1+)(1-^{2})(y_{s}- f(x_{s}))\] \[=(1+)(1-^{2})(y_{s} ).\]

Since \((y_{s})=^{2}\), we have \((z_{s,q})=(1+)(1-^{2}) ^{2}\). 

**Lemma 2**.: _Let \(t>q+2\), \(e\) is the sampling strategy, and \(f_{t}\) be the estimate of function \(f\) at the end of round \(t\) which uses \(\{x_{s},y_{s},_{s}\}_{s=1}^{t}\). Then, the best linear unbiased estimator of \(_{e}^{}\) is_

\[}_{e,t}=(_{t}^{}_{t}_{e}) ^{-1}((_{e})_{t}^{}_{t} ),\]

_where \(_{t}\) is a \(t q\) matrix whose \(s^{}\) row is \(_{s}-_{e,s}\) and \(_{t}=(y_{1}-f_{t}(x_{1}),,y_{t}-f_{t}(x_{t}))\)._

Proof.: Recall the \(s^{}\) hybrid reward defined in Eq. (3) using sampling strategy \(e\) as \(z_{e,s,q}^{e}=y_{s}-(_{s}-_{e,s})_{e}\), which can be re-written as \(y_{s}=z_{e,s,q}^{e}+(_{s}-_{e,s})_{e}\). Following similar steps as of Lemma 1, we can re-write the above equation as \(y_{s}=f^{}(x_{s})+(_{s}-_{e,s})_{e}+ _{z,s}\).

Using \(_{e,t}=_{1}-_{e,s}\\ \\ _{t}-_{e,s}\), and \(_{t}=y_{1}-f_{t}(x_{1})\\ \\ y_{t}-f_{t}(x_{t}).\), we get \(}_{e,t}=(_{e,t}^{}_{e,t})^{-1}_{e,t}^{ }_{t}\). From Appendix D and E of Gorodetsky et al. (2020), we have \(_{e,t}^{}_{e,t}=_{t}^{}_{t}_{e}\) and \(_{e,t}^{}_{t}=(_{e})_{t} ^{}_{t}\). Using these two equality, we have

\[}_{e,t}=(_{t}^{}_{t}_{e})^{-1}((_{e})_{t}^{}_{t}).\]

**Theorem 3**.: _Let \(t>q+2\) and \(e\) is the sampling strategy. If \(}_{e,t}\) as defined in Lemma 2 is used to compute hybrid reward \(z_{e,s,q}\) for any \(s t\), then \([z_{e,s,q}]=f(x_{s})\) and \((z_{e,s,q})=(1+)(1-_{e}^{ 2})^{2}\), where \(a()=1\), \(a()=\) if \(r_{i}=r,\  i\{1,2,,q\}\) when using MF sampling strategy for estimating auxiliary feedback functions._

Proof.: The proof follows the similar steps as Theorem 1 except we adapt the part (b.) of Theorem 4 from Pham and Gorodetsky (2022) instead of using Fact 4 to show the variance reduction when sampling strategy (IS or MF) is used for estimating auxiliary feedback functions.

### Unbiased estimate of variance

Consider the following regression problem with target variable \(y_{s}\), which is defined as follows:

\[y_{s}=+(_{s}-)+_{z,s}.\]

where \(_{z,1},,_{z,t}\) are IID and have zero mean Gaussian noise with variance \((1-^{2})^{2}\). Let

\[}_{t}=y_{1}\\ \\ y_{t},\ }_{t}=1&_{1}-\\ &\\ 1&_{t}-,\ =\\ ,\ \ _{z,t}= _{z,1}\\ \\ _{z,t}.\]

Now, using Fact 1, we have \((_{z,t})=^{2}(}_{t}^{ }}_{t})_{11}^{-1}\), where \((^{})_{11}^{-1}\) is the upper left most element of matrix \((^{})^{-1}\)(Schmeiser, 1982). Then after \(t\) observations, the unbiased variance estimator of \((_{z,t})\) is given by

\[_{z,t}=_{z,t}^{2}(}_{t}^{}}_{t})_{11}^{-1},\]

where \(_{z,t}^{2}=_{s=1}^{t}(y_{s}-_{z,t})^{2}\)(Nelson, 1990), which is also an unbiased variance estimator of \(^{2}\) (from Fact 2). Further, \(_{z,t}\) is also an unbiased estimator of \((_{z,t})\), i.e., \([_{z,t}]=(_{z,t})\)(Nelson, 1990, Theorem 1). We can adapt this approach to our setting. However when noise variance (\(\)) is unknown, computing \((}_{t}^{}}_{t})_{11}^{-1}\) may not be possible to general function \(f\) as \(\) function may not be known. Though the setting in which \((}_{t}^{}}_{t})_{11}^{-1}\) can be computed, we have to use the upper bound of variance to construct the confidence bound for reward function \(f\) as random sample variance estimate can be small and leads to invalid confidence bounds. Given \(t\) observations, the upper bound of the sample variance is given by \(_{z,t}=_{z,t}}{_{1-,t}}\), where \(_{1-,t}^{2}\) denotes \(100(1-)^{}\) percentile value of the chi-squared distribution with \(t-2\) degrees of freedom.

### Missing proofs related to regret analysis

**Theorem 2**.: _With a probability of at least \(1-2\), the instantaneous regret of \(\) in round \(t\) is_

\[r_{t}() 2(_{t}^{}+^{1/2}S) \|x_{t}\|_{_{t}^{-1}},\]

_where \(_{t}^{}=,_{z,t-1})}\ _{t}\), \(\|^{}\|_{2} S\), and \(_{t}=/}{})}\). For \(t>q+2\) and \(_{z,t}<^{2}\), \([r_{t}()](( )}{t-q-3})^{}r_{t}()).\) Here, \(\) hides constant terms._

Proof.: When only observed rewards are used for estimating underlying unknown parameters in the linear bandit setting, i.e., \(_{t}=_{t}^{-1}_{s=1}^{t}x_{s}y_{s}\), then with probability \(1-\), the confidence bound (Abbasi-Yadkori et al., 2011, Theorem 1) is

\[\|_{t}-^{}\|_{_{t}}/}{})}+^{1/2}S,\] (7)

where \(^{2}\) is the variance of observed rewards given action (since the noise variance is \(^{2}\)). To ensure the performance of \(\) is as good as OFUL, we only used hybrid reward samples for estimation when the upper bound on the variance of hybrid rewards is smaller than the variance of rewards, i.e., \(_{z,t-1}<^{2}\). At the beginning of round \(t\), the variance upper bound of hybrid rewards is computed using \(t-1\) observations and given by \(_{z,t-1}=_{z,t-1}}{_{1-,t}^{2}}\), where \(_{z,t-1}\) is an unbiased sample variance estimate of hybrid rewards using \(t-1\) observations and \(_{1-,t}^{2}\) (implying the variance upper bound holds with at least probability of \(1-\)) denotes \(100(1-)^{}\) percentile value of the chi-squared distribution with \(t-2\) degrees of freedom. When \(_{z,t}<^{2}\), we replace rewards \(\{y_{s}\}_{s=1}^{t}\) with its respective hybrid rewards, i.e., \(\{z_{s,q}\}_{s=1}^{t}\) to estimate underlying parameter and use it for next round. After using hybrid rewards to estimate the unknown parameter, we replace\(^{2}\) in Eq. (7) with the variance upper bound of hybrid rewards. Then we get the following upper bound which holds with a probability of \(1-2\).

\[\|_{t}-^{}\|_{_{t}} ,_{z,t-1})}/}{})}+^{1/2}S\] \[=,_{z,t-1})}_{t}+ ^{1/2}S\] \[\|_{t}-^{}\|_{_{ t}} =_{t}^{}+^{1/2}S,\]

where \(_{t}^{}=,_{z,t-1})} _{t}\) and \(_{t}=/}{})}\).

Let action \(x_{t}\) be selected in the round \(t\). Then, the instantaneous regret is given as follows:

\[r_{t} =_{x}x^{}^{}-x_{t}^{} ^{}\] \[={x^{}}^{}^{}-x_{t}^{}^{} x^{}=_{x}x^{}^{}\] \[=(x^{}-x_{t})^{}^{}\] \[=(x^{}-x_{t})^{}^{}+(x^{}-x_{t})^{} _{t}-(x^{}-x_{t})^{}_{t}\] \[=(x^{}-x_{t})^{}_{t}-(x^{}-x_{t})^{ }(_{t}-^{}).\]

A sub-optimal action is only selected when its upper confidence bound is larger than the optimal action. Then, if \(\|_{t}-^{}\|_{_{t}}=_{t}^{ }+^{1/2}S\), then we have

\[r_{t} _{t}\|x_{t}\|_{_{t}^{-1}}-_{t} \|x^{}\|_{_{t}^{-1}}-(x^{}-x_{t})^{}(_{t}-^{})\] \[_{t}\|x_{t}\|_{_{t}^{-1}}-_{t} \|x^{}\|_{_{t}^{-1}}-\|x^{}-x_{t}\|_{ _{t}^{-1}}\|_{t}-^{}\|_{_{t}}\] \[_{t}\|x_{t}\|_{_{t}^{-1}}-_{t} \|x^{}\|_{_{t}^{-1}}+_{t}\|x^{}-x_{t} \|_{_{t}^{-1}}\] \[=_{t}(\|x_{t}\|_{_{t}^{-1}}-\|x^{ }\|_{_{t}^{-1}}+\|x^{}-x_{t}\|_{_{t}^ {-1}})\] \[_{t}(\|x_{t}\|_{_{t}^{-1}}-\|x^ {}\|_{_{t}^{-1}}+\|X_{t,a_{t}^{2}}\|_{_{t} ^{-1}}+\|x_{t}\|_{_{t}^{-1}})\] \[=2_{t}\|x_{t}\|_{_{t}^{-1}}\] \[ r_{t}  2(_{t}^{}+^{1/2}S)\|x_{t}\|_{ _{t}^{-1}}.\]

Let \(_{t}=\{x_{s}\}_{s=1}^{t}\). For \(t>q+2\) and \(_{z,t}<^{2}\), the expected instantaneous regret of OFUL-AF is

\[[r_{t}] [2(_{t}^{}+^{1/2}S)\|x _{t}\|_{_{t}^{-1}}]\] \[=2[[(_{z,t}} _{t}+^{1/2}S)\|x_{t}\|_{_{t}^{-1}}| _{t}.]]\] \[=2[_{t}\|x_{t}\|_{_{t}^{-1 }}[_{z,t}}|_{t}]+^{1/2}S\| x_{t}\|_{_{t}^{-1}}]\] \[ 2_{t}\|x_{t}\|_{_{t}^{-1}} [[_{z,t-1}}{_{1-,t}^{2} }}|_{t}]]+2^{1/2}S\|x_{t}\|_{_{t}^{- 1}}\] \[=2_{t}\|x_{t}\|_{_{t}^{-1}}^{2}}}[_{z,t-1}}]+2 ^{1/2}S\|x_{t}\|_{_{t}^{-1}}.\]

Since \(_{z,t-1}\) is an unbiased estimator of the sample variance of hybrid rewards, \([_{z,t-1}]=(z_{s,q})\) for \(s\{1,,t\}\). Using Theorem 1, we have \(()}{t-q-3})^{2}\) as \(t^{}\) observation is not available at the beginning of the round \(t\). With increasing \(t\), \(C_{t}=^{-1}}}\) tends to 1. With all these observations, we have

\[[r_{t}()]  2C_{t}()}{t-q-3})^{ }_{t}\|x_{t}\|_{_{t}^{-1}}+2^{1/2}S \|x_{t}\|_{_{t}^{-1}}\] \[=2(C_{t}()}{t-q-3})^{}_{t}+^{1/2}S)\|x_{t}\|_{_{t}^{-1}}\]

Let \(r_{t}()\) be the upper bound on instantaneous regret for OFUL algorithm, i.e., \(r_{t}()=2(_{t}+^{1/2}S)\|x_{t} \|_{_{t}^{-1}}\). Then, we have

\[[r_{t}()]  2C_{t}()}{t-q-3})^{}(_{t}+^{1/2}S)\|x_{t}\|_{ _{t}^{-1}}\] \[+2(1-C_{t}()}{t-q-3}) ^{})^{1/2}S\|x_{t}\|_{_{t}^{-1}}\] \[ C_{t}()}{t-q-3})^{ {2}}r_{t}()\] \[+2(1-C_{t}()}{t-q-3}) ^{})^{1/2}S\|x_{t}\|_{_{t}^{-1}}.\]

\[[r_{t}()]( ()}{t-q-3})^{}r_{t}() ).\]

**Theorem 4**.: _Let \(\) be an AFC bandit algorithm with \(|f_{t}^{}(x)-f(x)| h(x,_{t})+l(x, _{t})\) and \(_{e,z,t}\) be the upper bound on sample variance of hybrid reward, whose value is set to \(^{2}\) for \(t q+2\). Then, with a probability of at least \(1-2\), the instantaneous regret of \(\) after using hybrid rewards (named \(\)-AF) for reward function estimation in round \(t\) is_

\[r_{t}() 2(,(_{e,z,t})^{} )h(x,_{t})+l(x,_{t}),\]

_where \(e=\{\}\), and KF denotes the case where auxiliary functions are known. For \(t>q+2\) and \(_{e,z,t}<^{2}\), where \(a()=1\)._

Proof.: Let \(\) be an AFC bandit algorithm with \(|f_{t}^{}(x)-f(x)| h(x,_{t})+l(x, _{t})\) and \(_{e,z,t}\) be the upper bound on sample variance of hybrid reward. After \(\) uses hybrid rewards for estimating function \(f\), then, with probability at least \(1-2\),

\[|f_{t}^{}(x)-f(x)|(,(_{e,z,t})^{})h(x,_{t})+l(x,_{t})\] (8)

The proof follows similar steps as the first part of the proof of Theorem 2. The only key difference is the upper bound of variance of hybrid rewards, which depends on the underlying sampling strategy based on whether auxiliary functions are known or unknown. The upper bound on sample variance is given by \(_{e,z,t}=_{e,z,t-1}}{_{1-,t}^{2}}\), where \(_{e,z,t-1}\) is an unbiased sample variance estimate of hybrid rewards using \(t-1\) observations with sampling strategy \(e\) and \(_{1-,t}^{2}\) (implying the variance upper bound holds with at least probability of \(1-\)) denotes \(100(1-)^{}\) percentile value of the chi-squared distribution with \(t-2\) degrees of freedom.

Let action \(x_{t}\) be selected in the round \(t\). Then, the instantaneous regret is given as follows:

\[r_{t}=_{x}f(x)-f(x_{t})=f(x^{})-f(x_{t}) (x^{}=_{x}f(x))\]\[f_{t}^{}(x^{})+(,(_{e,z, t})^{})h(x^{},_{t})+l(x^{},_{t})-f(x_{t}) \] \[f_{t}^{}(x_{t})+(,(_{e,z,t})^{})h(x_{t},_{t})+l(x_{t},_{t})-f(x_{t}) \] \[f_{t}^{}(x_{t})-f(x_{t})+(,(_{e,z,t})^{})h(x_{t},_{t})+l(x_{t}, _{t})\] \[ 2(,(_{e,z,t})^{})h(x_{t}, _{t})+l(x_{t},_{t}),\]

in which the first and last inequalities have used the upper bound given in Eq.8, and the second inequality follows because actions are selected using the upper confidence bounds. The remaining proof will follow the similar steps as the second part of Theorem2 except using Theorem3 instead of Theorem1 for quantifying the variance reduction due to hybrid rewards when IS or MF sampling strategy is used for estimating auxiliary feedback function. 

### Auxiliary feedback in contextual bandits

Many real-life applications have some additional information readily available for the learner before selecting an action, e.g., users' profile information is known to the online platform before making any recommendations. Such information is treated as contextual information in bandit literature, and the bandit problem having contextual information is refereed as contextual bandits (Li et al., 2010). Since the value of the reward function also depends on the context, the learner's goal is to use contextual information to select a better action.

We extend our results for the contextual bandits problem. In this setting, we assume that a learner has been given an action set denoted by \(\). In round \(t\), the environment generates a vector \(x_{t,a},y_{t,a},\{w_{t,a,i}\}_{j=1}^{q}\) for each action \(a\). Here, \(x_{t,a}\) is the context-action \(d\)-dimensional feature vector of observed context in round \(t\) and action \(a\), \(y_{t,a}\) is the stochastic reward received for context-action pair \(x_{t,a}\), and \(w_{t,a,i}\) is the \(i^{}\) auxiliary feedback associated with the reward \(y_{t,a}\). We assume that the reward is a function of the context-action pair \(x_{t,a}\), which is given as \(y_{t,a}=f(x_{t,a})+_{t}\), where \(f:^{d}\) is an unknown function and \(_{t}\) is a zero-mean Gaussian noise with variance \(^{2}\). The auxiliary feedback is also assumed to be a function of the context-action pair \(x_{t,a}\), given as \(W_{t,a,i}=g_{i}(x_{t,a})+_{t,i}^{w}\), where \(g_{i}:^{d}\) and \(_{t,i}^{w}\) is a zero-mean Gaussian noise with variance \(_{w}^{2}\). The correlation coefficient between reward and associated auxiliary feedback is denoted by \(\).

We denote the optimal action for a context observed in the round \(t\) as \(a_{t}^{}=*{argmax}_{a}f(x_{t,a})\). The interaction between a learner and its environment is given as follows. At the beginning of round \(t\), the environment generates a context, and then the learner selects an action \(a_{t}\) from action set \(\) for that context using past information of context-actions feature vector, observed rewards and its associated auxiliary feedback until round \(t-1\). After selecting action \(a_{t}\), the learner receives a reward \((y_{t,a_{t}})\) with its associated auxiliary feedback and incurs a penalty (or instantaneous regret) \(r_{t}\), where \(r_{t}=f(x_{t,a_{t}^{}})-f(x_{t,a_{t}})\). We aim to learn a sequential policy that selects actions to minimize the total penalty and evaluate the performance of such policy through _regret_, which is the sum of the penalty incurred by the learner. Formally, for \(T\) contexts, the regret of a policy \(\) that selects action \(a_{t}\) for a context observed in round \(t\) is given by

\[_{T}()=_{t=1}^{T}f(x_{t,a_{t}^{}})-f(x_{t,a_{t}} )\,.\] (9)

A policy \(\) is a good policy when it has sub-linear regret. This implies that the policy will eventually learn to recommend the best action for every context. Similar to the parameterized bandit problem case, we can use the existing contextual bandit algorithms, which are AFC bandit algorithms. Depending on the problem, an appropriate AFC contextual bandit algorithm is selected that uses hybrid rewards to estimate reward function. The smaller variance of hybrid rewards leads to tighter upper confidence bound of the unknown reward function and hence smaller regret.

### More details about experiments

To demonstrate the performance gain from using auxiliary feedback, we have considered three different bandit settings: linear bandits, linear contextual bandits, and non-linear contextual bandits. The details of the problem instance used in our experiments are as follows.

Linear bandits:We use a \(5\)-dimensional space in which each sample is represented by \(x=(x_{1},,x_{5})\), where the value of \(x_{j}\) is restricted in \((-3,3)\). We randomly select a \(5\)-dimensional vector \(^{}\) with a unit norm whose each value is restricted in \((0,1)\). In all linear bandits experiments, we use \(=0.01\), \(L=2.236\), \(S=1\), and \(=0.05\). In round \(t\), the reward for selected action \(x_{t}\) is

\[y_{t}=v_{t}+w_{t},\]

where \(v_{t}=x_{t}^{}_{v}^{}+_{t}^{v}\) and \(w_{t}=x_{t}^{}_{w}^{}+_{t}^{v}\). We set \(_{v}^{}=(0,_{2}^{},0,_{4}^{},0)\) and \(_{w}^{}=(_{1}^{},0,_{3}^{},0,_{5}^{ })\). As we treat \(w_{t}\) as auxiliary feedback, \(_{w}^{}\) may be assumed to be known in some experiments. The random noise \(_{t}^{v}\) is zero-mean Gaussian noise with variance \(_{v}^{2}\). Whereas \(_{t}^{w}\) is also zero-mean Gaussian noise, but the variance is \(_{w}^{2}\). We assumed that \(^{2}=_{v}^{2}+_{w}^{2}\) is known, but not the \(_{v}^{2}\) and \(_{w}^{2}\). The default value of \(_{v}^{2}=0.01\) and \(_{w}^{2}=0.01\). It can be easily shown that the correlation coefficient of \(y_{t}\) and \(w_{t}\) is \(=^{2}/(_{v}^{2}+_{w}^{2})}\). We run each experiment for \(5000\) rounds.

Linear contextual bandits:We first generate a \(2\)-dimensional synthetic dataset with \(5000\) data samples. Each sample is represented by \(x=(x_{1},x_{2})\), where the value of \(x_{j}\) is drawn uniformly at random from \((-1,1)\). Our action set \(\) has four actions: \(\{(x_{1},x_{2}),(x_{1},-x_{2}),(-x_{1},x_{2}),(-x_{1},-x_{2})\}\). We uniformly generate a \(^{}\) such that its norm is \(1\). In all experiments, the data samples are treated as contexts, and we use \(=0.01\), \(L=1.41\), \(S=1\), and \(=0.05\). The observed reward for a context-action feature vector has two components. We treated one of the components as auxiliary feedback. In round \(t\), the reward context-action feature vector \(x_{t,a}\) is given as follows:

\[y_{t,a_{t}}=v_{t,a_{t}}+w_{t,a_{t}},\]

where \(v_{t,a_{t}}=x_{t,a}^{}_{v}^{}+_{t}^{v}\) and \(w_{t,a_{t}}=x_{t,a}^{}_{w}^{}+_{w}^{w}\). We set \(_{v}^{}=(0,_{2}^{},0,_{4}^{})\) and \(_{w}^{}=(_{1}^{},0,_{3}^{},0)\). As we treat \(w_{t,a_{t}}\) as auxiliary feedback, \(_{w}^{}\) is known for some experiments. The random noise \(_{t}^{v}\) is zero-mean Gaussian noise with variance \(_{w}^{2}\). Whereas \(_{t}^{w}\) is also zero-mean Gaussian noise, but the variance is \(_{w}^{2}\). We assumed that \(^{2}=_{v}^{2}+_{w}^{2}\) is known, but not the \(_{v}^{2}\) and \(_{w}^{2}\). The default value of \(_{v}^{2}=0.01\) and \(_{w}^{2}=0.01\). It can be easily shown that the correlation coefficient of \(y_{t,a}\) and \(w_{t,a}\) is \(=^{2}/(_{v}^{2}+_{w}^{2})}\).

Figure 3: **Top row: Experiment using linear bandits problem instance. Bottom row: Experiment using non-linear contextual bandits problem instance. Left to right: Regret vs. different biases (left figure), regret vs. number of historical samples of auxiliary feedback (middle figure), and regret vs. varying correlation coefficients of reward and its auxiliary feedback (right figure).**

Non-linear contextual bandits:This problem instance is adapted from the linear contextual bandits problem instance. We first generate a \(2\)-dimensional synthetic dataset with \(5000\) data samples. Each sample is represented by \(x=(x_{1},x_{2})\), where the value of \(x_{j}\) is drawn uniformly at random from \((-1,1)\). We then use a polynomial kernel with degree \(2\) to have a non-linear transformation of samples. We removed (i.e., bias) the first (i.e., \(1\)) and last value \((i.e.,x_{2}^{2})\) from the transformed samples, which reduced the dimensional of each transformed sample to \(4\) and represented as \((x_{1},x_{2},x_{1}^{2},x_{1}x_{2})\), which is used as context. For this setting, the action set \(\) has six actions: \(\{(x_{1},x_{2},-x_{1}^{2},-x_{1}x_{2}),(x_{1},-x_{2},x_{1}^{2},-x_{1}x_{2}),( -x_{1},x_{2},x_{1}^{2},-x_{1}x_{2}),(x_{1},-x_{2},-x_{1}^{2},x_{1}x_{2}),\\ (-x_{1},x_{2},-x_{1}^{2},x_{1}x_{2}),(-x_{1},-x_{2},x_{1}^{2},x_{1}x_{2}),\}\). We uniformly generate a \(^{}\) such that its norm is \(1\). In all experiments, we use \(=0.01\), \(L=2\), \(S=1\), and \(=0.05\). The observed reward for a context-action feature vector has two components. We treated one of the components as auxiliary feedback. In round \(t\), the reward context-action feature vector \(x_{t,a}\) is given as follows:

\[y_{t,a_{t}}=v_{t,a_{t}}+w_{t,a_{t}},\]

where \(v_{t,a_{t}}=x_{t,a}^{}_{v}^{}+_{t}^{v}\) and \(w_{t,a_{t}}=x_{t,a}^{}_{w}^{}+_{t}^{w}\). We set \(_{v}^{}=(0,_{2}^{},0,_{4}^{},0,_{6}^{ },0,_{8}^{})\) and \(_{w}^{}=(_{1}^{},0,_{3}^{},0,_{5}^{ },0,_{7}^{},0)\). As we treat \(w_{t,a_{t}}\) as auxiliary feedback, \(_{w}^{}\) is known for some experiments. The random noise \(_{t}^{v}\) is zero-mean Gaussian noise with variance \(_{2}^{2}\). Whereas \(_{t}^{w}\) is also zero-mean Gaussian noise, but the variance is \(_{w}^{2}\). We assumed that \(^{2}=_{v}^{2}+_{w}^{2}\) is known, but not the \(_{v}^{2}\) and \(_{w}^{2}\). The default value of \(_{v}^{2}=0.01\) and \(_{w}^{2}=0.01\). It can be easily shown that the correlation coefficient of \(y_{t,a}\) and \(w_{t,a}\) is \(=^{2}/(_{v}^{2}+_{w}^{2})}\).

Regret with varying correlation coefficient:As the correlation coefficient of reward and auxiliary feedback is \(=^{2}/(_{v}^{2}+_{w}^{2})}\), we varied \(_{v}\) over the values \(\{0.3,0.2,0.1528,0.1,0.0655\}\) to obtain problem instances with different correlation coefficient for all problem instances.

Variance estimation:Since the value of \(^{2}\) is know in all our experiments, we directly estimate the correlation coefficient (\(\)) as \(=(y,w)/((w)})\). Then, use it to set \(_{e,z,t}=(1-^{2})^{2}\).

Figure 4: Comparing regret vs. the number of historical samples of auxiliary feedback in different settings. In this experiment, historical samples for each run are randomly generated as compared to Fig. 1(e), Fig. 2(b), and Fig. 2(e) where history is kept fixed across the runs.