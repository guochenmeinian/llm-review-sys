# UniIF: Unified Molecule Inverse Folding

Zhangyang Gao 1,2, 1, Jue Wang 1,2, 1, Cheng Tan 1,2, 1,

**Lirong Wu** 2, **Yufei Huang** 2, **Siyuan Li** 2, **Zhirui Ye** 2, **Stan Z. Li** 2, 1

1 Zhejiang University 2 Westlake University

Equal Contribution, *Corresponding Author.

###### Abstract

Molecule inverse folding has been a long-standing challenge in chemistry and biology, with the potential to revolutionize drug discovery and material science. Despite specified models have been proposed for different small- or macro-molecules, few have attempted to unify the learning process, resulting in redundant efforts. Complementary to recent advancements in molecular structure prediction, such as RoseTTAFold All-Atom and AlphaFold3, we propose the unified model UniIF for the inverse folding of all molecules. We do such unification in two levels: 1) Data-Level: We propose a unified block graph data form for all molecules, including the local frame building and geometric feature initialization. 2) Model-Level: We introduce a geometric block attention network, comprising a geometric interaction, interactive attention and virtual long-term dependency modules, to capture the 3D interactions of all molecules. Through comprehensive evaluations across various tasks such as protein design, RNA design, and material design, we demonstrate that our proposed method surpasses state-of-the-art methods on all tasks. UniIF offers a versatile and effective solution for general molecule inverse folding.

## 1 Introduction

Molecule inverse folding plays a pivotal role in drug and material design, enabling scientists to synthesize novel molecules with the desired structure. Previously, many studies focus on either macromolecules  or small molecules  separately, leaving the challenge of inverse folding general molecules. For example, the advanced small molecule model  take atoms as basic units; the macromolecule models  consider predefined microstructures (such as amino acids and nucleotides) as the basic units. Additionally, even for the same molecule, different models employ varying strategies to extract geometric features. Complementary to the great success of RoseTTAFold All-Atom  and AlphaFold3  in molecular structure prediction, we propose a unified model, UniIF, for the inverse folding of all molecules.

By comparing small- and macro-molecules, we identify three challenges toward the unified model:

(1) **Unit Discrepancy**: The macromolecules takes predefined microstructures (amino acids and nucleotides) as the basic units, while small molecules takes atoms as basic units. (2) **Geometric Featurizer**: Different studies employ various strategies for extracting geometric features from structures, such as distance, angles and tensor product; there are lack of unified featurization strategy. (3) **System Size**: The small-molecules allow the full attention transformer to learn long-term dependencies, but the quadratic computing cost limits the mechanism scalling up to macro-molecular systems. Alternatively, previous research use sparse GNN, which suffers from the limited local receptive field that causes over-smoothing and over-squashing . In addition, developing a unified model working well for all

Figure 1: Unified molecule inverse folding.

The unit discrepancy makes it challenge to adapt methods across small- and macro-molecules, which explains the divergence between these two research lines. As a solution, we propose a frame-based block to unify the representation of amino acids, nucleotides, and atoms: a group of atoms with varying size is treated as a block with fixed size. Each block includes decoupled equivariant basis and invariant features, generalizing the representation of AlphaFold2 and other small-molecule methods.

The geometric featurizer is necessary to capture the geometric interactions between blocks. We initialize the equivariant block basis using predefined rules or a learnable GNN layer, and then constructing invariant block features based on these basis. The key operation is to use local coordinates and dot product to capture the geometric interactions between virtual atoms. We reuse the featurizer in each model layer to interactively learn updated geometric features, where the concept of directed virtual atom is introduced to enhance the pairwise interactions. We show that the unified featurizer works well across protein design, RNA design, and material design.

We use sparse GNN to address the system size issue, while maintaining the ability to capture long-term dependencies. The transformer-style protein models like AlphaFold and RosettaFold require a substantial amount of GPU memory. Sparse GNNs, on the other hand, are criticized for their tendency to over-smooth and over-squash due to their limited local receptive field. To be efficient while preserving the ability to capture long-term dependencies, we introduce global virtual blocks. Each virtual block is connected to all real blocks, serving as an information exchange agent.

We conducted comprehensive experiments across various tasks, including protein design, RNA design, and material design, to demonstrate the effectiveness of UniIF. The results show that UniIF achieves state-of-the-art performance on all the tasks, which is non-trivial and may benefit the machine learning, drug discovery, and material science communities.

## 2 Related work

**Unification.** Unified molecular learning has attracted increasing attention in recent years. RoseTTAFold All-Atom (RFAA)  and AlphaFold3  are two representative models that have achieved remarkable success in protein structure prediction. RoseTTAFold All-Atom uses an atom-bond graph for small molecules and a frame graph for macromolecules. AlphaFold3 uses a bi-level representations, i.e., atom representation and token representation, for all molecules. The token concept is equivalent to the block concept in this paper, which means a group of atoms, such as a amino acid or a nucleotide. GET  and EPT  are two recent models that use a block representation for both small and macromolecules and introduce a new equivariant transformer backbone. Unlike RFAA , which specifies a atom-bond graph for small molecules, our model employs a unified block graph for all molecule types and do not require the atom-bond graph. Our model also differs from AlphaFold3 , GET  and EPT  in the that we introduce the vector basis for each block.

**Protein Inverse Folding.** Recent research use \(k\)-NN graph to represent the 3D structure and employ graph neural networks for protein inverse folding. GraphTrans  uses the graph attention encoder and autoregressive decoder for protein design. GVP  proposes geometric vector perceptrons to learn from both scalar and vector features. GCA  introduces global graph attention for learning contextual features. In addition, ProteinSolver  is developed for scenarios where partial sequences are known while not reporting results on standard benchmarks. Recently, AlphaDesign , ProteinMPNN , ESMIF , LMDesign , KWDesign , VFN  achieves dramatic improvements. A benchmark  is proposed to comprehensively evaluate protein design models.

**RNA Inverse Folding.** RNA inverse folding is a challenging task due to the complex secondary structure and tertiary structure. Traditional methods  include colony optimization and constraint programming, in addition to adaptive walk, simulated annealing and Boltzmann sampling. Recent deep learning method RDesign  has achieved promising results and build a benchmark for AI researchers to follow-up. RiboDiffusion  use diffusion decoder to generate RNA sequences conditioned on the backbone structure embeddings.

**Material Design.** Deciding which chemical compositions are likely to form compounds is a critical task in material design [28; 26; 14]. An important application is to substitute lattice-site elements or ionic species within existing compounds that exhibit similar chemical behaviors. Wang et al.  successfully employed the elemental substitution method to discover 18,479 stable compounds out of a pool of 189,981 potential candidates. Recently, Jensen et al.  introduced a open dataset for material design and established a benchmark for evaluating deep learning models in this domain.

Method

### Overall Framework

As shown in Fig. 2, we propose the unified model for general molecule inverse folding. The key insights include: (1) transforming all molecules into block graphs, where each block represents an amino acid, nucleotide, or atom; (2) proposing a geometric featurizer to initialize geometric node and edge features; and (3) introducing a new GNN layer with long-term dependencies to learn expressive block representations. Our unified model achieves competitive results across diverse tasks, including protein design, RNA design, and material design.

### Block Graph

We introduce the block graph to represent all types of molecules, where the key insight is to transform irregular set of atoms (varying size) as regular block representation (fixed size).

Atom-based Block Representation.A block \(=\{(_{i},_{i})\}_{i=1}^{||}\) contains a set of atoms \(\{_{i},_{i}\}_{i=1}^{||}\), where \(_{i}^{3}\) and \(_{i}^{d}\) represent the equivariant coordinate and invariant features, such as the atom type. The common block types include amino acids, nucleotides, and atoms, which are represented as \(^{fold}\), \(^{ra}\), and \(^{smol}\), respectively. Formally, we write \(^{fold}=\{(_{i},_{i})\}_{i^{raa}}\) and \(^{raa}=\{(_{i},_{i})\}_{i^{raa}}\), where \(^{fold}=\{,1,2,\}\) and \(^{raa}=\{,5,4,3,2,1,5,4,3,2\}\) are the sets of atoms for amino acids and nucleotides, respectively. For small molecules, each atom \(a\) represents a block \(^{smol}=\{(_{a},_{a})\}\), where \(1 a 118\). As the block size \(||\) varies for different types of blocks, the atom-based blocks representation could not directly be applied for unified mod

Figure 3: Blocks of different molecules. The basic building blocks include amino acids, nucleotides and atoms.

Figure 2: The Overall framework. (1) The model treat all types of molecules as block graphs. For macromolecules, we use predefined frames based on amino acids and nucleotides; for small molecules, we learn the local frame of each block by one-layer GNN. (2) A geometric featurizer is used to initialize the geometric node feature and edge features. (3) We propose the block graph attention layer, based on which we build the block graph neural network to learn expressive block representations. (4) Finally, we show that the UnilF can achieve competitive results on diverse tasks, ranging from protein design, RNA design and material design.

Frame-based Block Representation.We introduce frame-based block representation to unify the modeling of all molecules. A block \(=(F,)\) contains the equivariant frame \(F\) and invariant feature vector \(^{d}\). The local frame \(F(R,)\) contains the axis matrix \(R=[_{1},_{2},_{3},,_{u}]\) and translation vector \(t\). We set \(\) as the coordinate of the representative atom, i.e., \(C1\) of macromolecules and the atom itself of small molecules. Following AlphaFold2, we consider the special case that \(R^{3,3}\) is orthogonal. However, additional experiments show that the model can also work well with non-orthogonal axis matrix. For macromolecules, the axis matrix \(R\) is predefined based on amino acids and nucleotides, while for small molecules, we learn the axis matrix \(R\) as it does not have prior common structure patterns. The frame-based block representation decouples geometric information: (1) the local frame basis describe the equivariant pose; (2) the invariant feature vector could embed the atom type and invariant local structure patterns for different tasks. More importantly, the dimension of the frame-based block representation is fixed, which is beneficial for the unified modeling; we build block features in Sec. 3.3.

Frame-based Block Graph.Given a molecule \(=\{_{s}\}_{s=1}^{n}\) containing \(n\) blocks, we build the block graph \((\{_{s}\}_{s=1}^{n},)\) using kNN algorithm. In the block graph, the \(s\)-th node is represented as \(_{s}=(F_{s},_{s})\), and the edge between \((s,t)\) is represented as \(_{st}=(F_{st},_{st})\). The relative frame is defined as \(F_{st}=F_{s}^{-1} F_{t}\). Inspired by [22; 10; 27], we modify PiFold featurizer to initialize the geometric node feature \(_{s}\) and edge feature \(_{s,t}\); refer to Sec. 3.3.

Relation to Other Methods.The frame-based block is a generalized data form of AlphaFold2 and other methods. If \(R\) is required to be a rotation matrix, the frame-based block is equivalent to AlphaFold2's local frame; otherwise, it is equivalent to represent the atom as invariant feature \(\) and equivalent vector \(\), similar to GVP  and DimeNet .

### Block Graph Featurizer

Learning Local Frame.For small molecules, there is no predefined local frame, and we need to learn the local frame for each atom. Given the the molecule \(=\{(_{s},_{s})\}_{s=1}^{||}\), we use a 1-layer of GNN to initialize the atom representation \(\{_{s}\}_{s=1}^{||}(\{_{s}, _{s}\}_{s=1}^{||})\), where the inital local frames are \(T(,)\). The rotation vector \(_{s}\) of the \(s\)-th atom is constructed by message passing:

\[_{s}=(r_{x},r_{y},r_{z})=_{k_{s}}(_{s},_{s})}}{_{k_{s}}e^{(_{s},_{k})}} _{k}-_{s}}{||_{k}-_{s}||}\] (1)

\(_{s}\) is the \(s\)-th atom's neighbor system. Let the direction \((r_{x},r_{y},r_{z})=}{||_{s}||}\) and magnitude \(=||_{s}||\) represent the rotation axis and angle, we compute the quanterning \(_{s}\) and rotation matrix \(R_{s}\):

\[_{s}=[w,x,y,z]=[,r_{x} {2},r_{y},r_{z}]\\ \\ R_{s}=[_{x},_{y},_{z}]=1-2y^{2}-2z^{2}&2xy-2 zw&2xz+2yw\\ 2xy+2zw&1-2x^{2}-2z^{2}&2yz-2xw\\ 2xz-2yw&2yz+2xw&1-2x^{2}-2y^{2}\] (2)

Finally, the local frame of the \(s\)-th atom is \(T_{s}(R_{s},_{s})\), where \(_{s}\) is the atom coordinate. Experiments show that learning rotation vectors consistently outperforms learning Schmidt-orthogonalized axises.

Node Geometric Feature.The invariant block feature captures the atom type and the local structure:

\[_{i}^{pos}=R_{s}^{T}(_{i}-_{s})=F_{s}^{-1} _{i}\\ \\ _{s}=_{s}|}_{i_{s}}( _{i},_{i}^{pos})\] (3)

Figure 4: Unified molecule inverse folding.

The inverse frame operation \(T_{s}^{-1}\) project the equivalent global coordinates to the invariant local coordinate, i.e., \(^{local}=F_{s}^{-1}^{global}=R_{s}^{T}(^{global}-_{s})\). We use MLP to embed atom type and local coordinates. All atom features in the same block are pooled to get the block feature \(_{s}\).

Edge Geometric Feature.We initialize pairwise features following the principle that

#### Edge features capture the directed 3D interactions.

Instead of using mutually constructed distance and angle features, we concatenate the local coordinates of two blocks to fully describe their 3D positions. Given \(_{s}\) and \(_{t}\) with global coordinate matrices as \(X_{s}^{|v_{s}|,3}\) and \(X_{t}^{|v_{t}|,3}\), the invariant edge features following \(s t\) direction is

\[_{s,t}=T_{s}^{-1}([X_{s}\|X_{t}])\] (4)

where \(T_{s}^{-1}=(R_{s}^{T},-R_{s}^{T}_{s})\) projects equivariant global coordinates to invariant local coordinates.

### Block Graph Attention Module

Frame-based SE-(3) Module Design.Given the geometric transformation \(=W\), we decompose \(W=R_{t} R_{s}^{T}\) using SVD and explain \(=R_{t} R_{s}^{T}\) as:

1. Projecting \(\) as the local coordinate \(^{local}\) using the frame \(F_{s}(R_{s},)\), i.e., \(^{local}=R_{s}^{T}\).
2. Updating local coordinates via gated attention, i.e., \(^{local}^{local}\).
3. Translating \(^{local}\) as the global coordinate using frame \(T_{t}(R_{t},)\), i.e., \(=R_{t}^{local}\).

If we parameterize \(\) as \(f_{}()\), and considers the effects of translation, i.e., \(T_{s}(R_{s},_{s}),T_{t}(R_{t},_{t})\), the general principle of designing SE-(3) networks could be:

\[}=R_{s}^{-1}(-_{s})=T_{s}^{-1} \\ } f_{}(})\\ }=R_{t}}+_{t}=T_{t}}\] (5)

The well-known AlphaFold actually follows such a design, where they parameterize \(f_{}\) as the IPA module. In this work, we replace \(f_{}\) with an enhanced graph neural network:

\[_{s}^{(l+1)},_{st}^{(l+1)} f_{}(_{s}^{(l)}, _{st}^{(l)}|T_{s},T_{st},)\] (6)

where \(_{s}^{(l)}\) and \(_{st}^{(l)}\) represent the input node and edge features of the \(l\)-th layer. In Fig. 5, we show the design of the Block Graph Attention Module, consisting of three components: (1) geometric interaction extractor, (2) virtual block for long-term dependencies, and (3) the edge attention mechanism. We show the detailed design of the Block Graph Attention Module in the following sections.

Figure 5: Block Graph Attention Module. (a) Virtual Block for Long-term Dependencies. (b) Geometric Interaction Extractor for learning pairwise features. (c) Gated Edge Attention for updating node features.

Long-term Dependency via Virtual Blocks.The GNN is criticized by local receptive field, yielding the problem of over-smoothing and over-squashing [2; 5; 29; 13]. Transformers overcome these problems using direct paths between distant nodes, while suffering from the \((n^{2})\) computing cost. We introduce \(n^{}\) virtual blocks \(\{_{i}\}_{i=n}^{n+n^{}}\) as information agents for a graph. Each virtual block directly connects to all the real blocks, resulting in \((2 n n^{})\) additional directed edges. As \(n^{} n\), we claim that the computing cost is close to original GNN. All the virtual blocks, i.e., \(T_{n+1}(R^{},^{}),T_{n+2}(R^{},^{}),, T_{n+n^{}}(R^{},^{})\), share the same rotation \(R^{}\) and translation \(^{}\):

\[=[_{1},_{2},]\\ ^{T}=U V^{T}\\ R^{}=UV^{T}=[_{x},_{y},_{z}]\\ ^{}=^{N}_{i}}{N}\] (7)

The invariant features of virtual blocks are different, as we hope they learn diverse interactions. We encode the index to initialize block features \(^{}_{i}=(i)\) for \(i\{1,2,,n^{}\}\).

Geometric Interaction Extractor.We enhance edge features with geometric interactions using the local coordinates of virtual inter-atoms and dot products of virtual intra-atoms. Previous works, such as PiFold , introduced virtual atoms in the featurizer to capture informative side-chain geometry beyond protein backbones, resulting in performance gains. VFN  extended this idea by allowing GNN layers to update the virtual atoms. However, these efforts are limited to learning virtual intra-atoms conditioned on node features. Instead, we propose virtual inter-atoms conditioned on edge features, allowing the same node to exhibit different virtual states specified by edges. Additionally, inspired by small molecule modeling, we use the dot product of virtual intra-atoms to capture angle information. We show the geometric interactions in Fig. 6, and formulate it as:

\[^{(l)}_{st},^{(l)}_{t}=(^{(l)}_{st }),(^{(l)}_{t})^{m,3}\\ }^{(l)}_{st}=(T_{st} h^{(l)}_{st})\|h^{(l)}_{st}\\ _{st}=^{T}_{s}R^{T}_{s}R_{s}_{t}\\ g^{(l)}_{st}=(}^{(l)}_{st},g_{st},r_{st},_{st})\\ ^{(l)}_{st}(}^{(l)}_{st},^{(l)}_{ t})\] (8)

where \(T_{st}=T_{s}^{-1} T_{t}=(R_{s}^{-1}R_{t},R_{s}^{-1}(_{t}-_{s}))\), \(q_{st}=(R_{st})^{9}\) is the flatten rotation matrix of \(T_{st}\), and \(r_{st}=||_{s}-_{t}||\) indicates the pairwise distance. All \(q_{st}\), \(r_{st}\) an \(_{st}\) are invariant features. We highlight the difference to previous researches in color.

Gated Edge Attention.We modify PiFold's GNN to capture the geometric interactions when updating node features. For molecular design tasks, we find that aggregating edge features only leads to consistent performance gains. We understand this phenomenon as the model can pay more attention on learning 3D interactions under such a model design. In addition, we use a gated mechanism to control how the edge features are injected to node features. The gated edge attention module is:

\[w_{st}=(^{(l)}_{s}||^{(l)}_{st}|| {f}^{(l)}_{t})\\ a_{st}=}{_{h_{s}} w_{st}}\\ h^{(l)}_{t}=(^{(l)}_{st})\\ ^{(l)}_{s}=_{t_{s}}a_{st}^{(l)}_{t}\\ ^{(l+1)}_{s}=^{(l)}_{s}+((^{(l)}_{s} ))^{(l)}_{s}\] (9)

where \(\) is element-wise product operation, and \(()\) is the sigmoid function. We highlight the difference between PiGNN and the proposed module in color.

Figure 6: Geometric Interactions.

FFN & Edge Updating.Analogous to the transformer model, the FFN is a MLP. The edge updating layer remains the same as PiFold:

\[_{st}=(_{s}||_{st}||_{t})\] (10)

The proposed module could be equivalently implemented as a transformer module using matrix multplication. However, we find that padding proteins to the maximum length would grealy increase the computing cost in both GPU occupancy and runtime; We suggest using GNN without padding.

Regularization.We find that the proposed model fit training data better than PiFold and more likely to suffer from overfitting. To address this issue, we randomly drop out the nodes/edges with a probability of \(p\) to prevent overfitting. We find that controlling the dropout rate could result in models with different fitting abilities. The best performance is achieved when \(p=0.05\).

## 4 Experiments

We show the effectiveness of UniF via multiple inverse folding tasks and ablation studies. We briefly introduce molecular design tasks as follows:

* **Protein Design (T1):** Designing protein sequences folding into the target structure.
* **RNA Design (T2):** Designing RNA sequences folding into the target structure.
* **Material Design (T3):** Discovering stable composition from a known material structure.

### Protein Design (T1)

Task DescriptionProtein design aims to design protein sequences that fold into target structures. Given a protein backbone structure \(=\{X_{i}^{m,3}:1 i n\}\), where \(m\) is the maximum number of points belonging to the \(i\)-th residue, \(n\) is the number of residues and the natural proteins are composed by 20 types of amino acids, the goal is to learn a function \(_{}\):

\[_{}:}.\] (11)

The parameters \(\) are learned by minimizing the cross-entropy loss, i.e., \((_{}(),)=-_{i=1}^{n}  s_{i}p(_{i}|,)\). The task is challenging due to the combinatorial search space of amino acids and the complex relationship between sequence and structure.

SettingsWe evaluate of UniIF on the CATH4.3 dataset  following prior works [11; 8]. The dataset is split by the CATH topology classification code, yielding 16,631 training, 1,516 validation, and 1,864 testing samples. To assess generalization, we adopt a time-split strategy, considering the use of pretrained ESM2 models by some baselines, which risk data leakage. The time-split evaluation assigns data before a specific date to the training set and data after that date to the test set. For structural time-split evaluation, we use the CASP15 dataset , containing novel crystal structures not seen during training. For sequence time-split evaluation, we use the NovelPro dataset , which includes 76 protein sequences released within 30 days before November 23, 2023, with structures predicted by AlphaFold2. UniF consists of 10 layers of BlockGAT with a hidden dimension of 128. It is trained using the Adam optimizer with a learning rate of 1e-3 and a batch size of 8 for 50 epochs.

Metrics & BaselinesWe report the median recovery rate of the top-1 predicted sequences, representing the percentage of correctly predicted residues. The ESM2-free baselines include StructGNN , GraphTrans , GCA , GVP , AlphaDesign , ProteinMPNN , and PiFold . The ESM2-based baselines include LMDesign  and KWDesign . While we prefer open-source baselines, we also re-implement VFN  for a comprehensive comparison.

ConclusionWe provide results under different settings (with and without ESM2) and across diverse datasets (CATH4.3, CASP, NovelPro). Using a pure inverse folding model without ESM2, UniF achieves the best performance on all datasets, demonstrating its effectiveness. Notably, UniIF outperforms the strong baseline PiFold with fewer learnable parameters. In time-split evaluations, UniIF surpasses all baselines, including ESM2-based methods, by a significant margin. On NovelPro, which features novel sequences, UniF outperforms LMDesign and KWDesign that use ESM2 for sequence refinement. This indicates UniIF's superior generalizability, crucial for real-worldapplications. Ablation studies show that the proposed geometric featurizer, gated edge attention, and global virtual frame enhance performance. On CATH4.3, the overall improvement is slight due to strong baselines, but time-split evaluation highlights UniIF's superiority in generalization.

### RNA Design (T2)

Task DescriptionSimilar to protein design, RNA design aims to design RNA sequences that fold into target structures. Specially, previous work  use the RNA secondary structure as additional input to guide the design process, since the tertiary structure is limited. In this work, we only use the tertiary structures as input for the reason of unification, which is more challenging than the baselines.

Datasets & BaselinesWe conduct experiments RNA on the dataset collected by RDesign , consisting of 2218 RNA tertiary structures, which are divided into training (1774 structures), testing (223 structures), and validation (221 structures) sets based on their structural similarity. Following RDesign's benchmark, baseline methods include SeqRNN, SeqLSTM, StructMLP, StructGNN, and StructGNN, GraphTrans , PiFold  and RDesign . Given the small number of data samples, we report the median recovery and its standard deviation for three independent runs.

ConclusionAs shown in Table 2, UniIF achieves the best performance in all cases. The improvement is significant, as previous strong baselines like PiFold only excelled in protein design. To our knowledge, UniIF is the first model to achieve state-of-the-art performance in both protein and RNA design tasks, demonstrating its versatility and effectiveness. Compared to RDesign, which uses additional secondary structure features, UniIF relies solely on tertiary structure input and still performs better. UniIF successfully unifies the protein and RNA design processes, paving the way for a unified inverse folding model for protein-RNA complexes in future developments.

    &  &  &  &  &  \\  & length & \(L<100\) & \(100 L<300\) & \(300 L<500\) & Full & CASP & NovelPro & \\   ✓ \\  } & LMDSign  & 0.47 & 0.56 & 0.61 & 0.56 & 0.48 & 0.59 & \\  & KVDesign  & 0.51 & 0.61 & 0.69 & 0.60 & 0.56 & 0.64 & 1.4M \\   ✗ \\  } & StructGNN  & 0.30 & 0.34 & 0.40 & 0.34 & 0.36 & 0.40 & 1.4M \\  & GraphTrans  & 0.29 & 0.34 & 0.39 & 0.34 & 0.35 & 0.40 & 1.5M \\  & GCA  & 0.32 & 0.36 & 0.41 & 0.36 & 0.40 & 0.43 & 2.1M \\  & GVP  & 0.33 & 0.38 & 0.45 & 0.38 & 0.39 & 0.42 & 0.9M \\  & AlphaDesign  & 0.37 & 0.43 & 0.47 & 0.42 & 0.42 & 0.46 & 3.6M \\  & ProteinMPNN  & 0.38 & 0.44 & 0.52 & 0.44 & 0.44 & 0.52 & 1.7M \\  & PiFold  & 0.43 & 0.52 & 0.59 & 0.51 & 0.47 & 0.57 & 5.8M \\  & UniIF (ours) & **0.45** & **0.54** & **0.61** & **0.53** & **0.51** & **0.66** & 5.4M \\   ✓ \\  } & VFN  & 0.45 & 0.53 & 0.60 & 0.52 & 0.48 & 0.63 & 5.4M \\  & -GDP & 0.45 & 0.53 & 0.61 & 0.52 & 0.50 & 0.65 & 5.3M \\   & -EAttn & 0.44 & 0.53 & 0.60 & 0.52 & 0.48 & 0.63 & 5.7M \\   & -VFrame & 0.45 & 0.53 & 0.61 & 0.52 & 0.49 & 0.64 & 5.4M \\   

Table 1: Protein Design results. The **best** and suboptimal results are labeled with bold and underlined. ”VFN” means that we replace the geometric interaction operation with VFN’s operation . ”-GDP” means that we remove the geometric dot product features. ”-EAttn” means that we replace the gated edge attention with PiGNN’s attention module . ”-VFrame” means that we remove the global virtual frames.

    &  \\  & Short & Medium & Long & All \\  SeqRNN (h=128) & 26.52\(\)1.07 & 24.86\(\)0.82 & 27.31\(\)0.41 & 26.23\(\)0.87 \\ SeqRNN (h=256) & 27.61\(\)1.85 & 27.16\(\)0.63 & 28.71\(\)0.14 & 28.24\(\)0.46 \\ SeqLSTM (h=128) & 23.48\(\)1.07 & 26.32\(\)0.05 & 26.78\(\)1.12 & 24.70\(\)0.64 \\ SeqLSTM (h=256) & 25.00\(\)0.00 & 26.89\(\)0.35 & 28.55\(\)0.13 & 26.93\(\)0.93 \\ StructMLP & 25.72\(\)0.51 & 25.03\(\)1.39 & 25.38\(\)1.89 & 25.35\(\)0.25 \\ StructGNN & 27.55\(\)0.94 & 28.78\(\)0.87 & 28.23\(\)1.95 & 28.23\(\)0.71 \\ GraphTrans  & 26.15\(\)0.93 & 23.78\(\)1.11 & 23.80\(\)1.69 & 24.73\(\)0.93 \\ PiFold  & 24.81\(\)2.01 & 25.90\(\)1.56 & 23.55\(\)4.13 & 24.48\(\)1.13 \\ RDesign  & 37.22\(\)1.14 & 44.89\(\)1.67 & **43.06\(\)**0.08 & 41.53\(\)0.38 \\ UniIF (drop 0.05) & **48.21\(\)**0.95 & **49.66\(\)**1.28 & 37.29\(\)0.17 & **48.94**\(\)0.37 \\  UniIF (drop 0.0) & 42.86 \(\)0.87 & 48.45 \(\)1.04 & 39.23 \(\)0.09 & 44.29 \(\)0.29 \\ UniIF (drop 0.1) & 45.21 \(\)0.98 & **51.70**\(\)1.26 & 40.30 \(\)0.14 & 46.00 \(\)0.38 \\ UniIF (drop 0.2) & 46.97 \(\)1.04 & 48.11 \(\)1.37 & 42.00 \(\)0.18 & 47.19 \(\)0.45 \\   

Table 2: The recovery of RNA design. The **best** and suboptimal results are labeled with bold and underlined.

### Material Design (T3)

Task DescriptionDiscovering stable atom compositions from known material structures is crucial for new material discovery [28; 26; 14]. This task is challenging due to the large composition space and the lack of large-scale data. Thanks to recent benchmark efforts , we can evaluate the performance of UniF on this novel task.

Datasets & BaselinesWe evaluated UniF on the CHILI-3K dataset , which consists of nanomaterial graphs derived from mono-metal oxides. The dataset includes 53 metallic elements and one non-metallic element (oxygen), comprising 3,180 graphs, 6,959,085 nodes, and 49,624,440 edges. Following the official benchmark, the dataset is randomly split into training (80%), validation (10%), and testing (10%) sets. Baselines include GCN , PMLP , GraphSAGE , GAT , GraphUNet , GIN , and EdgeCNN . Experiments are repeated three times with different seeds, using early stopping with a patience of 50 epochs, and trained up to 1000 epochs.

ConclusionIn Table 3, UniF outperforms all baselines by a large margin. Ablation studies demonstrate the crucial role of the learned local frame in enhancing interaction feature extraction. In addition, how to learn the local frame is also important. In the "- quat" ablation, we try to learn the x, y, and z axes with Householder orthogonalization directly, but found it less effective, with the recovery rate dropping from 75.3% to 65.2%. This highlights the value of the proposed local frame learning mechanism.

### Case Study

In Fig. 7, we show the designed protein and RNA sequences.In addition, we use AlphaFold3  to re-fold the designed sequences into structures. The ground truth (gray), PiFold (green), and UniF (pink) structures are aligned and compared. We observe that UniF improves both the recovery and RMSD of the designed protein and RNA, demonstrating its effectiveness in inverse folding tasks.

## 5 Conclusion

We propose the first unified model, dubbed UniF, for general molecule inverse folding. The key points include unifying the data representation, the featurizer and the model architecture without a drop in performance. Extensive experiments show that UniF surpasses baseline methods on all tasks. Ablation studies reveal that the geometric interaction extractor, gated edge attention, and virtual long-term dependency modules contribute to performance gains. We believe that the proposed model can benefit multiple domains, such as machine learning, drug design, and material design.

Figure 7: Designed examples. The ground truth (gray), DiFold (green), and UniF (pink) structures are aligned.

   Method & Rec \% \(\) \\  Random & 1.6\(\)0.0 \\ GCN  & 49.6\(\)0.1 \\ PMLP  & 46.1\(\)0.0 \\ GraphSAGE  & 49.1\(\)0.4 \\ GAT  & 46.1\(\)0.0 \\ GraphUNet  & 55.2\(\)7.9 \\ GIN  & 58.7\(\)0.2 \\ EdgeCNN  & 63.2\(\)0.9 \\ UniF (ours) & **75.3\(\)**1.2 \\  - frame & 54.9\(\)2.8 \\ - quat & 65.2\(\)3.9 \\   

Table 3: CHILI-3K Results.