# Approximately Pareto-optimal Solutions for

Bi-Objective \(k\)-Clustering Problems

 Anna Arutyunova

Heinrich Heine University Dusseldorf

Dusseldorf, Germany

anna.arutyunova@hhu.de &Jan Eube

University of Bonn

Bonn, Germany

eube@cs.uni-bonn.de &Heiko Roglin

University of Bonn

Bonn, Germany

roeglin@cs.uni-bonn.de &Melanie Schmidt

Heinrich Heine University Dusseldorf

Dusseldorf, Germany

mschmidt@hhu.de &Sarah Sturm

University of Bonn

Bonn, Germany

sturm@cs.uni-bonn.de &Julian Wargalla

Heinrich Heine University Dusseldorf

Dusseldorf, Germany

julian.wargalla@hhu.de

###### Abstract

As a major unsupervised learning method, clustering has received a lot of attention over multiple decades. The various clustering problems that have been studied intensively include, e.g., the \(k\)-means problem and the \(k\)-center problem. However, in applications, it is common that good clusterings should optimize multiple objectives (e.g., visualizing data on a map by clustering districts into areas that are both geographically compact but also homogeneous with respect to the data). We study combinations of different objectives, for example optimizing \(k\)-center and \(k\)-means simultaneously or optimizing \(k\)-center with respect to two different metrics. Usually these objectives are conflicting and cannot be optimized simultaneously, making it necessary to find trade-offs. We develop novel algorithms for approximating the set of Pareto-optimal solutions for various combinations of two objectives. Our algorithms achieve provable approximation guarantees and we demonstrate in several experiments that the approximate Pareto front contains good clusterings that cannot be found by considering one of the objectives separately.

## 1 Introduction

Clustering is a major unsupervised learning method that is used to find structure in data. It is often described as the process of dividing objects into groups called _clusters_ such that objects in the same cluster are similar and objects in different clusters are dissimilar. There are several mathematical and algorithmic ways to describe a good clustering, but two objectives that correspond to that description for the case of metric clustering are the \(k\)_-diameter problem_ and _Single Linkage_ clustering.

In the \(k\)-diameter problem, we are given a point set \(\) from a metric space and a number \(k\) and want to find a partition of \(\) into \(k\) clusters that minimizes the maximum diameter of any cluster. This models the first problem: Finding clusters where the objects in the same cluster are similar. This problem can be \(2\)-approximated by well-known algorithms [30; 37]. But are approximate or even optimal solutions for this problem automatically also good for the second half of the description,i.e., ensuring that points in different clusters are dissimilar? Certainly not, as the small example in Figure 1 shows: An optimal \(k\)-diameter clustering with \(k=2\) has radius \(\), but \(x_{2}\) and \(x_{3}\) are very close together in this solution.

Single Linkage clustering is a linkage method that finds the clustering in which the minimum distance between any two clusters is maximized, i.e., it focuses on the second goal. We refer to the problem of maximizing the minimum inter-cluster distance as the _\(k\)-separation problem_ in the following, i.e., Single Linkage is an optimal algorithm for the \(k\)-separation problem. In Figure 1 with \(k=2\), Single Linkage computes the clustering \(\{\{x_{1},x_{2},x_{3}\},\{x_{4}\}\}\) or the symmetric clustering.

Unfortunately, there is an inherent tension between the \(k\)-diameter and \(k\)-separation problem, even if the points are on the Euclidean line \(\). In Figure 1, we could observe that the optimal \(k\)-separation clustering is still a very good \(k\)-diameter clustering (assuming \(\) to be small). However, one can construct examples where the two cost functions cannot be reconciled:

**Example 1**.: _For any fixed \(k\) consider the set \(_{k}=\{1,,k-1\} A\) with \(A=\{-x/k x\{0,,k(k-1)^{2}\}\}\). Figure 2 illustrates this for \(k=3\). Now the only way to obtain a minimum inter-cluster distance of \(1\) is to use \(k-1\) singleton clusters \(\{1\},,\{k-1\}\) and one cluster for \(A\) (any other clustering needs to break \(A\) and has a separation of \(1/k\)). Notice that this clustering has a maximum diameter of \((k-1)^{2}\) since that is the diameter of \(A\). However, the best \(k\)-diameter clustering divides the points into intervals of equal length, achieving a diameter of \(((k-1)^{2}+(k-1))/k=k-1\), which is smaller by a factor of \(k-1\)._

Example 1 demonstrates that the natural goal of clustering is inherently a goal with two potentially conflicting objectives. Our formalization with \(k\)-diameter and \(k\)-separation is an example of a bi-objective clustering problem.

This is only one motivation for studying multi-objective (or bi-objective) clustering. Clustering with respect to multiple objectives simultaneously or based on different metrics arises naturally in applications (we cite a selection of works at the beginning of the related work section). Yet the theoretical study of multi-objective clustering problems is, so far, very limited. We consider the case of bi-objective clustering and study combinations of various clustering objectives with the possibility of optimizing each over its individual metric.

ObjectivesWe restrict to the case where centers are chosen from the point set. This is common for \(k\)-median, but uncommon for \(k\)-means. We leave the extension where centers can be chosen from an ambient metric space to future work. So let \((,d)\) be a metric space. Given \(C\) with \(|C|=k\) and an assignment \( C\) we define the following objective functions:

\(k\)**-center:**\((C,,d)=_{p}d(p,(p))\)

\(k\)**-diameter:**\((C,,d)=_{p,q(p)=(q)}d(p,q)\)

\(k\)**-median:**\((C,,d)=_{p}d(p,(p))\)

\(k\)**-means:**\((C,,d)=_{p}d^{2}(p,(p))\)

\(k\)**-min sum radii (\(k\)-msr):**\((C,,d)=_{c C}_{p^{-1}(c)}d(c,p)\)

\(k\)**-separation:**\((C,,d)=_{p,q(p)(q)}d(p,q)\)

Notice that in contrast to standard definitions, here we need an assignment function because our solutions are trade-offs and may not necessarily assign points to closest centers (for \(k\)-diameter and \(k\)-separation we do not need the centers but only the partitioning that \(\) induces; we use the same notation here for convenience and introduce additional notation in the respective technical sections). Each objective induces an optimization problem when we fix the number of clusters \(k\) we want to

   objective & \(\) / \(\) & \(\) & \(\) & \(\) \\  best guarantee / reference & \(2\) /  & \(2.67059\) /  & \(9+\) /  & \(3+\) /  \\   

Table 1: State-of-the-art approximation factors

Figure 1: A toy example.

obtain1. The \(k\)-separation problem is a maximization problem, all other objectives are to be minimized, i.e., the aim is to compute a set \(C\) with \(k\) centers and an assignment function \(\) with \(^{-1}(c)\) for all \(c C\) such that the respective objective function is as low as possible. All the minimization problems are \(\)-hard and also \(\)-hard to optimize to arbitrary precision . On the positive side, approximation algorithms with small constant factors are known for all the minimization problems mentioned, see Table 1 (all numbers are for the general metric case, including \(k\)-means). The \(k\)-separation maximization problem can be solved optimally by running the well-known Single Linkage algorithm until the given number \(k\) of clusters is left.

Use CasesWe have two different use cases: 1) Simultaneously optimizing a well-known \(k\)-clustering minimization problem together with the conflicting \(k\)-separation maximization problem. 2) Optimizing two \(k\)-clustering minimization problems simultaneously. In both cases, we allow to choose different metrics for each objective. As an example for 2), imagine that we want to visualize data on a map by clustering districts into larger regions. Then regions should be homogeneous with respect to the data but at the same time also geographically coherent on the map, not stretching out too far. For this application we can use as the first metric the data-based metric and as the second metric the geographic distance between regions on the map. Then we can choose an objective, e.g., \(k\)-center, and find a good trade-off when optimizing \(k\)-center with respect to the two objectives. This idea is discussed in more detail in Section 3.2.

Multi-objective Optimization and Pareto SetsBoth use cases can benefit from the same tool box of multi-objective optimization. Standard approaches are optimizing a weighted sum of the objectives, or limiting one objective and optimizing the other (leading to a constrained clustering problem), or computing/approximating the _Pareto front_. We follow the latter approach and focus on algorithms to approximate the Pareto front of bi-objective clustering problems. Let us first define this concept formally: Given two metrics \(d_{1},d_{2}\) on \(\) and two objectives \(f_{1},f_{2}\{,,,, ,\}\), a solution \((C,)\) is _dominated_ by a solution \((C^{},^{})\) if the following is true for both \(i=1,2\): \(f_{i}(C^{},^{},d_{i}) f_{i}(C,,d_{i})\) if \(f_{i}\) is a minimization objective and \(f_{i}(C^{},^{},d_{i}) f_{i}(C,,d_{i})\) if \(f_{i}\) is a maximization objective, and in addition, for \(i=1\) or \(i=2\), the inequality is strict. A solution \((C,)\) is called _Pareto-optimal_ if it is not dominated by any other solution. We denote by \(\) the set of all Pareto optimal solutions and call \(\) the _Pareto set_ or _Pareto front_.

Notice that by definition the Pareto front contains a solution \((C_{i},_{i})\) optimizing objective \(f_{i}\) for \(i=1,2\). Since all aforementioned minimization objectives are \(\)-hard, we therefore cannot compute the Pareto fronts exactly in polynomial time unless \(=\). Instead we design algorithms for computing approximate Pareto fronts in the following sense: Given \(=(_{1},_{2})^{2}\), a set \(_{}\) is an \(\)-approximate Pareto set if the following is true: For every Pareto-optimal solution \((C,)\), there is a solution \((C^{},^{})_{}\) that is an \(\)-approximation to \((C,)\), i.e., \(f_{i}(C^{},^{},d_{1})_{i}f_{i}(C,,d_{i})\) for \(i\{1,2\}\) if \(f_{i}\) is a minimization objective and \(f_{i}(C^{},^{},d_{1})(1/_{i})f_{i}(C,,d_{i})\) for \(i\{1,2\}\) if \(f_{i}\) is a maximization objective. Figure 3 shows two examples of approximate Pareto sets. In 3a) the trade-off is between \(k\)-separation and \(k\)-means (use case 1). This is discussed in Section 3.1. In 3b) both objectives are chosen as \(k\)-center, but with two different metrics that arise from a map design question discussed in Section 3.2 (use case 2). In both pictures the area dominated by a solution lies to the right and above the corresponding point.

While other approaches for multi-objective optimization (like optimizing a weighted sum of the objectives or solving a constrained clustering problem) produce a single trade-off of the objectives, an important advantage of the approximate Pareto front is that it consists of a diverse set of trade-offs. This is beneficial for data analysis because it offers more information and flexibility to the data analyst. In particular, an approximate Pareto front contains for any weighting of the objectives an approximately optimal solution and often, in addition to this, also other interesting solutions. We develop and analyze algorithms for computing approximate Pareto fronts. We also demonstrate experimentally the usefulness of approximate Pareto fronts and our algorithms in different applications.

Related work on Pareto setsThere is a large body of literature on multi-criteria optimization problems in different contexts (see, e.g., the book of Ehrgott ). A common approach is to compute or approximate the Pareto set because only Pareto-optimal solutions constitute reasonable trade-offs. If the Pareto set or its approximation is small, it can be presented to a human decision maker or used in some other post-processing step. For many multi-criteria optimization problems, algorithms for computing the Pareto set have been developed (e.g., for the multi-criteria shortest path problem  and the bi-criteria network flow problem ) The running time of these algorithms is usually polynomial in the output size, i.e., they are efficient if the Pareto set is small. A drawback of this is that Pareto sets can often be of exponential size in the worst case. To obtain more efficient algorithms and to reduce the size of the output, approximate Pareto sets have been studied extensively. Papadimitriou and Yannakakis showed that there always exist approximate Pareto sets of polynomial size and they showed that an \(\)-approximate Pareto set  for a problem can be computed efficiently if and only if a certain variant of the problem, called the gap problem, can be solved efficiently.

Related Work on multi-objective clusteringHeuristics for approximating Pareto fronts for clusterings are used in applications. Popular techniques include, e.g., genetic algorithms , multi-objective particle swarm optimization , evolutionary algorithms , and combinations of \(k\)-means and fuzzy c-means . All these heuristics have in common that they come without a guarantee on the approximation factor. We cannot review all the literature in this area and focus on what we perceive as most related.

The only work studying the same scenario is a paper by Alamdari and Shmoys  who discuss the combination of \(k\)-center and \(k\)-median and give, following the above definition, an algorithm that computes a \((4,8)\)-approximation of the Pareto front. Their algorithm uses an LP rounding approach based on a \(k\)-median LP with an additional side constraint that restricts the radius of clusters to a given number \(L\). The algorithm uses an optimal (fractional) solution to this LP and rounds it to an integral solution, hereby increasing the radius to \(4L\) and the \(k\)-median cost to \(8\) times the LP cost. The general approach has some resemblance with our findings in Section 2.2 (and Supplementary Section B.3), but it is based on the LP rounding algorithm by Charikar et al. , while we adapt the primal dual algorithm by Jain and Vazirani . We achieve an incomparable approximation factor of \((9,6+)\). Alamdari and Shmoys also give an example that shows that the two objectives \(k\)-center and \(k\)-median cannot be optimized simultaneously in general, making a similar point as we do in Example 1 showing that at least one of the objectives can at best be \(()\)-approximated.

Davidson and Ravi  discuss the combination of \(k\)-means and must-link and cannot-link constraints. As we discuss in the introduction and in Lemma 3, enforcing a certain separability translates directly into must-link constraints. However,  is mostly concerned with \(\)-hardness results (which stem mostly from cannot-link constraints) and reformulating constraints in a computationally more feasible way. Our focus is on the computationally very efficient Single Linkage / must-link constraints and the computation of Pareto optimal solutions.

In Section 3.1 (and Section C.1), we evaluate if the usage of orthogonal objectives can improve the performance of clustering with respect to recovering a ground truth clustering. A similar approach is evaluated by Handl and Knowles in . The first objective studied there is a \(k\)-means/\(k\)-median type objective called _deviation_. The second objective is based on nearest neighbors and evaluates the

Figure 3: Examples of approximate Pareto sets copied from the later experimental sections.

fraction of points that satisfies that the point is in the same cluster as its nearest neighbor. This has a similar purpose as our use of separation in Section 3.1, and also leads to improved F-measures. The focus of that work is still different as we focus on provable approximations of the Pareto front.

For the evaluation in that same section, we use normalized mutual information scores following Fred and Jain . In that paper, the idea is to combine multiple clusterings into one clustering in order to obtain more robust clusterings. The authors use a clustering ensemble of different solutions computed by \(k\)-means and then let their combination be guided based on the normalized mutual information score. The focus of that work is different since we aim to produce an informative ensemble while Fred and Jain aim at combining an ensemble into a single robust clustering.

## 2 Results

We provide algorithms to compute the approximate Pareto set for most combinations of two clustering objectives \(f_{1},f_{2}\{,,,, ,\}\). To compute the approximate Pareto set we make use of the variety of approximation algorithms which optimize over one of the clustering objectives. This includes the \(2\)-approximation for \(k\)-center/\(k\)-diameter by Hochbaum and Shmoys  which we adapt to compute an approximate Pareto set for the combination of \(f_{1}\{,,\}\) and \(f_{2}\{,\}\). We further incorporate the radius/diameter in the \((6+)\)-approximation for \(k\)-median (also yielding an \(O(1)\)-approximation for \(k\)-means) by Jain and Vazirani  to obtain an approximate Pareto set for the combination of \(f_{1}\{,\}\) and \(f_{2}\{,\}\). We adapt the \((3+)\)-approximation for \(k\)-min sum radii by Buchem et al.  to deal with clusters instead of points such that it can be combined with \(k\)-separation. For the combinations of \(f_{1}=\) and \(f_{2}\{,\}\) we use existing algorithms to compute a solution which optimizes \(f_{1}\) and a solution which optimizes \(f_{2}\) and combine them to a good solution for both objectives, similar to the nesting technique described by Lin et al. . Lastly for the combinations of \(f_{1},f_{2}\{,\}\) we use the result on convex Pareto sets by Diakonikolas . All of these adaptions are straightforward and we mainly follow the existing work. For the computation of the approximate Pareto set for \(f_{1},f_{2}\{,\}\) we furthermore design a new algorithm with improved running time which differs significantly from the naive implementation of the Hochbaum and Shmoys  adaptation.

We complement these results by providing instances where it is not possible to find solutions which simultaneously approximate \(k\)-separation and any of the minimization problems to constant factors. We see this for \(k\)-separation and \(k\)-diameter in Ex. 1 above, it works analogously for \(k\)-center. For \(k\)-median we analyze a similar example in Obs. 11, \(k\)-means works analogously. For \(k\)-separation and \(k\)-MSR, the example can be found in Obs. 21. This highlights the advantage of Pareto fronts which allow for a trade-off between two opposing clustering objectives. In the following we explain our results and techniques in more detail.

### Combining \(k\)-separation with various \(k\)-clustering minimization objectives

Single Linkage is known to have undesirable properties for practical applications, in particular chaining effects. Yet its underlying idea - creating separated clusters - makes a lot of sense intuitively. In this set of results, we investigate the combination of Single Linkage, i.e., the maximization of the \(k\)-separation objective, with algorithms that aim to construct good clusters with respect to various popular \(k\)-clustering criteria. The idea is that the clustering may benefit from a certain extent of cluster separation without the negative effect of using only Single Linkage to obtain the solution. In Section 3.1 we exemplify the benefit of this idea by a practical evaluation of the \(k\)-separation / \(k\)-means combination. There we find that ensuring a small amount of separability and then running \(k\)-means++ yields good results with respect to recovering a ground truth clustering in our experiments.

Table 2 gives an overview of our theoretical findings. An entry \((1,a)\) means that we show how to compute the approximate Pareto set \(_{(1,a)}\). More precisely: Let \(d_{1}\) and \(d_{2}\) be two metrics on the same point set, and let \(f_{2}\{,,,,, \}\) be the second objective. Now let \(=(1,a)\) be the entry in the corresponding column of Table 2. Then this means that we can compute an \(\)-approximate Pareto set \(_{}\) with respect to \(\) with metric \(d_{1}\) and \(f_{2}\) with metric \(d_{2}\). The second line refers to the corresponding theorem. All the individual theorems and their proofs are in the supplementary material.

TechniquesThe \(k\)-separation objective is friendly in the sense that we know exactly how optimal solutions look like. First observe that in any optimal solution, the objective value equals the distance of two points in different clusters. This means that there are only \((n^{2})\) possible optimal values. And if we fix a value \(\) to be the desired separation, then this means that all points \(x,y\) with \(d(x,y)<\) have to be in the same cluster because otherwise the objective is automatically below \(\). And if we transitively merge all pairs \(x,y\) with \(d(x,y)<\), then we get a solution with objective value \(\). So it is well characterized how an optimal \(k\)-separation solution looks like (also see Lemma 3). To combine \(k\)-separation with other clustering objectives we first consider such a clustering with separation \(\), if it has already \(k\) clusters, there is nothing left to do. If it has more than \(k\) clusters we decide which clusters to merge based on the second clustering objective.

This immediately leads to the \((1,1)\)-entry when combining two \(k\)-separation objectives (for different metrics): Iterate through all \((n^{4})\) combinations of possible optimal objective values \((_{1},_{2})\). For each guess, merge \(x,y\) if \(d_{1}(x,y)<_{1}\) or \(d_{2}(x,y)<_{2}\) (or both). While the running time can certainly be improved, this gives the full optimal Pareto front in polynomial time.

For \(f_{2}=\) or \(f_{2}=\), we also have that the optimum value of a solution is a pairwise distance between two points. Thus, there are also at most \(O(n^{2})\) possible values that \(f_{2}\) can take. There is a well-known algorithm due to Hochbaum and Shmoys  that for any given radius \(R\) (or diameter) can produce a solution with at most double that value or prove that no solution with value \(R\) exists. We adapt this algorithm to work on the above clustering with Separation \(\) as input. The algorithm is similar to the original work by Hochbaum and Shmoys. By iterating through all pairs of possible separation and possible radius/diameter values, we obtain the \((1,2)\)-approximate Pareto set. The details are discussed in Section A.2 of the supplementary material.

When combining \(\) with \(f_{2}=\) or \(f_{2}=\) (Supplementary Section A.3), we still know that the separation value \(\) is one of \(O(n^{2})\) candidates. We use the above clustering with separation \(\) and decide which clusters to merge further based on an approximate clustering with respect to \(f_{2}\). For this purpose we use the nesting technique described by Lin et al.  in the context of approximation algorithms for hierarchical clustering. Since we need an approximate clustering with respect to \(f_{2}\), the quality of our Pareto set depends on the approximation ratio of the algorithm for \(f_{2}\).

Finally, we combine \(\) with \(\). This is the only place where we study \(\) as it is less common than the other clustering objectives. It is also the only objective where we deviate from the rule that clusters have to have \(=k\) clusters and allow \(<k\) clusters. We study it because it seems more aligned with \(k\)-separation than the other objectives. However, Obs. 21 reveals that also \(k\)-separation and \(k\)-MSR cannot be approximated simultaneously. In order to approximate the Pareto front, we adapt the very recent state-of-the-art algorithm for \(k\)-MSR approximation from  to our setting. That algorithm has an approximation ratio of \(3+\) and indeed, we show how to compute \((1,3+)\)-approximate Pareto fronts. Buchem et al.  give an intricate primal-dual algorithm for the \(k\)-MSR problem. Our general idea follows the above themes: We iterate through all possible separation values \(\), obtain a clustering with separation \(\), and then compute an approximate \(k\)-MSR solution. However, for this to work we need to adapt the algorithm to cope with clusters instead of points. Similar to the adaption of the algorithm by Hochbaum and Shmoys the input now consists of clusters, and this needs to be taken into account when trying to cover them with balls. We discuss this in the supplementary material in Sec. A.4.

### Combining \(k\)-center or \(k\)-diameter with a \(k\)-clustering minimization problem

Now we turn to combinations of popular \(k\)-clustering minimization problems. Table 3 lists combinations involving \(k\)-center or \(k\)-diameter. Again, let \(d_{1}\) and \(d_{2}\) be two metrics on the same point set. Then let \(f_{1}\{,\}\), let \(f_{2}\{,,,\}\) be the second objective. Finally, let

    & \(k\)-separation & \(k\)-center & \(k\)-diameter & \(k\)-median & \(k\)-means & \(k\)-MSR \\  \(k\)-sep. & \((1,1)\) & \((1,2)\) & \((1,2)\) & \((1,2+_{1})\) & \((1,4+4}+_{2})\) & \((1,3+)\) \\ proof & Thm. 5 & Thm. 9 & Thm. 7 & Thm. 12 & Thm. 17 & Thm. 24 \\   

Table 2: Results for combining \(k\)-separation with various objectives. Here \(_{1}\) and \(_{2}\) refer to the best known approximation guarantee for \(k\)-median/\(k\)-means, currently \(_{1}=2.67059/_{2}=9+\).

\(=(a,b)\) be the entry corresponding to \(f_{1}\) and \(f_{2}\) in Table 3. Then this means that we can compute an \(\)-approximate Pareto set \(_{}\) with respect to \(\) or \(\) with metric \(d_{1}\) and \(f_{2}\) with metric \(d_{2}\). The second row links to the proof in the supplementary material.

Techniques.The \(k\)-center objective has the already mentioned property that there are at most \(O(n^{2})\) possible objective values, one for each pairwise distance in the data set. This also directly limits the size of the resulting Pareto Set when combining two \(k\)-center or \(k\)-diameter objectives by \(O(n^{2})\). This is tight since we were able to provide an instance where the optimum Pareto set has indeed a size of \((n^{2})\) (Supplementary Section B.2). However it can be proven that there always exists a \((2,2)\)-approximation of the Pareto front of size \(O(n)\).

We adapt a well-known 2-approximation algorithm due to Hochbaum and Shmoys . A straightforward adaption of this algorithm would iterate through \(O(n^{4})\) pairs \((r_{1},r_{2})\) of possible radii and check for each of them if a maximal independent set of size at most \(k\) can be found in an appropriate threshold graph. While this would result in a running time of \(O(n^{6})\), we improve the running time by observing that only certain pairs \((r_{1},r_{2})\) need to be checked (in total \(O(n^{2})\) many) and by updating the threshold graph dynamically. While updating the threshold graph can have a time complexity of up to \((n^{2})\) in certain iterations, we were able to use a suitable potential function to show that the amortized time needed to update the graph lies within \(O(n)\) per iteration. Hereby we show that we can compute the approximate Pareto set \(_{(2,2)}\) in time \(O(n^{3})\). The algorithm and its analysis also work for \(k\)-diameter, giving the same guarantees. For more details we refer to Section B.1 of the supplementary material.

To combine \(k\)-center with \(k\)-median and \(k\)-means (possibly with different metrics), we adapt the primal dual algorithm due to Jain and Vazirani . Alamdari and Shmoys  adapt the approximation algorithm by Charikar et al. , which leads to a \((4,8)\)-approximate Pareto set. For recent algorithms [4; 15; 16], incorporating radius requirements does not seem easily possible.

The primal dual algorithm gives a \((6+)\)-approximation guarantee for \(k\)-median and a \(54\)-approximation guarantee for \(k\)-means (the paper only states \(108\), but that is because the \(k\)-means problem is considered in \(^{d}\) there, while we study it in the setting where centers are chosen from the input point set. As  also observe, the guarantee then improves to \(54\).) The primal dual algorithm is based on an LP formulation where variables \(x_{ij}\) indicate that point \(j\) is assigned to center \(i\). The adaptation starts by fixing a radius \(R\) and removing all \(x_{ij}\) for which \(d(i,j)>R\). It then follows the proof of  to first obtain an algorithm for approximating facility location and then obtaining a \(k\)-median (or \(k\)-means) solution by combining two solutions: one with \(<k\) clusters and high cost and one with \(>k\) clusters and small cost. In the combination step we have to ensure that the radius stays bounded (see Lemma 47). We can ensure a \(9\)-approximation with respect to the radius while keeping the original approximation factors for \(k\)-median/\(k\)-median from . To extend the results to \(k\)-diameter, we use that the diameter is at most twice the radius. This standard observation transfers the results for \(\) with \(\) and \(\) to \(\).

### Combinations of \(k\)-median and \(k\)-means

In the previous sections we have enumerated over a candidate set that contained all Pareto-optimal solutions and used this enumeration to compute approximately optimal solutions. This is no longer feasible here because the Pareto set is too large: Theorem 48 in the supplementary material shows that the combination of \(\) and \(\) both with the same Euclidean metric can lead to Pareto sets of exponential size. Similar examples can be constructed for \(f_{1}=f_{2}\) and \(f_{1}\{,\}\) if \(d_{1} d_{2}\).

    & \(k\)-median & \(k\)-means \\  \(k\)-center/\(k\)-diameter & \((2,2)\) & \((9,6+)\) / \((18,6+)\) & \((9,54+)\) / \((18,54+)\) \\ proof & Cor. 37 & Thm. 41 / Cor. 44 & Thm. 42 / Cor. 44 \\   

Table 3: Results for combining \(\) and \(\) for two different metrics or with a sum-based objective. The combination of \(k\)-center/\(k\)-diameter with \(k\)-separation can be found in Table 2.

A possible approach to compute an approximate Pareto set is due to Papadimitrou and Yannakakis . They showed that there always exists an \(\)-approximate Pareto set whose size is polynomial in the input size and \(}\) where \(=(1+^{},1+^{})\). There is a polynomial algorithm to construct such an \(\)-approximate Pareto set if and only if one is able to solve the following problem in polynomial time: Given the instance and \(b_{1},b_{2}\), find a solution \(s\) with \(f_{i}(s) b_{i}\) for \(i\{1,2\}\) or report that there is no solution \(s\) with \((1+^{})f_{i}(s) b_{i}\) for \(i\{1,2\}\). Unfortunately, for the combinations of the objectives \(\), \(\) it is unknown, whether this problem can be solved efficiently. We leave this as an open problem.

An alternative to approximate Pareto sets is to consider another subset of the Pareto set, namely the set of _supported_ Pareto-optimal solutions, i.e., the set of solutions that are optimal for a weighted sum of the objectives. For two minimization objectives \(f_{1}\) and \(f_{2}\), a solution \((C,)\) is a supported Pareto-optimal solution if there exist weights \(w_{1},w_{2} 0\) such that \((C,)\) is a solution for the objective \(w_{1}f_{1}+w_{2}f_{2}\) with minimum value. For a solution \((C,)\) let \(F(C,)=(f_{1}(C,),f_{2}(C,))\) and \(F()=\{F(C,)(C,)\}\). The supported Pareto-optimal solutions are exactly the solutions that form the vertices of the convex hull of \(F()\). Hence, we will call the set of supported Pareto-optimal solutions the _convex Pareto set_ in the following. Diakonikolas  has also introduced the notion of approximate convex Pareto sets. For \(=(_{1},_{2})^{2}\) we say that a set of solutions \(_{}\) is an approximate convex Pareto set if for any solution \((C,)\) there is a a convex combination of solutions in \(_{}\) that is an \(\)-approximation to \((C,)\). In this context we always assume \(_{1}=_{2}\) and therefore just write \(_{_{1}}\) instead of \(_{(_{1},_{2})}\) in the following. Diakonikolas  showed that an (F)PTAS for the convex Pareto exists if there is an (F)PTAS for optimizing weighted sums of the objectives. Based on this, we obtain the following theorem.

**Theorem 51**.: _Given a finite set \(\), metrics \(d_{1},d_{2}\) on \(\) and two objectives \(f_{1},f_{2}\{,\}\). Let \(>0\). Then we can compute an \((+)\)-approximate convex Pareto set \(_{+}\) in time that is polynomial in the input size and \(\), where \(=2.67059\) if \(f_{1}=\) and \(f_{2}=\), and \(=9+\) if \(f_{1}=\) and \(f_{2}=\) or \(f_{1}=\) and \(f_{2}=\). The size of this set \(|_{+}|\) is also polynomial in \(\) and \(|P|\)._

## 3 Applications and Experimental Evaluation

We implemented algorithms for two objective combinations and tested them on different applications. The source code can be found at https://github.com/algo-hhu/paretoClustering.

### Application: \(k\)-separation and \(k\)-means

We test the approach to combine \(k\)-separation with a conflicting objective. As that second objective, we choose \(k\)-means. Since the data sets used in our experiments are from \(^{d}\), we do not use the algorithm described in Sec. A.3 for general metrics but a variant of it which is tailored to \(^{d}\) and allows centers from \(^{d}\) instead of only \(P\). It is described below. The Euclidean metric is used for both \(\) and \(\). We test this algorithm on data sets from \(^{d}\) with available ground truth. We fix the number \(k\) of clusters to be the desired number of clusters in the ground truth. All data sets were downloaded from freely available sources, see Table 4 for information on the data sets.

Algorithm.For every data set, we compute the approximate Pareto set for the desired number of clusters as follows. Recall that for a data set of size \(n\) there are only \(O(n^{2})\) possible values for the separation. We compute these values in time \(O(n^{2}d)\) and sort them in increasing order in time \(O(n^{2}(n))\). Starting with separation \(0\), we increase the separation in every step to the next largest value. Suppose the separation is \(\) in the current step, then we merge all points whose distance is at

  
**name** & Rice & Dry Bean & Wine & Optdigits & Iris & 2d-4c-no3 & 2d-10c-no3 & 2d-10c-no4 \\ 
**ref** &  &  &  &  &  &  &  &  \\
**n** & 3810 & 13611 & 178 & 5620 & 150 & 1123 & 3359 & 3291 \\
**d** & 7 & 16 & 13 & 64 & 4 & 2 & 2 & 2 \\
**k** & 2 & 7 & 3 & 10 & 3 & 4 & 10 & 10 \\   

Table 4: Number of points \(n\), dimension \(d\), and number of desired clusters \(k\) for all data sets in ยง3.1.

most \(\). This can be done efficiently via a Union Find data structure. Since the resulting clustering may have more than \(k\) clusters, we have to reduce the number of clusters to \(k\). For data sets in \(^{d}\) and the \(k\)-means objective, one can replace every cluster by its centroid weighted by the number of points in the cluster and then cluster these weighted centroids instead of using the nesting technique of Lin et al.  for general metrics. Instead of choosing the theoretically best approximation algorithm for \(k\)-means, we use \(k\)-means++  to cluster the centroids as it is fast (running time \(O(nkd)\)) and usually produces solutions of high quality for the \(k\)-means problem in practice. Then the respective clustering on the original data set has separation at least \(\) and at most \(k\) clusters. One can show that this algorithm computes an \(\)-approximate Pareto set with \(=(1,O( k))\).

Prototypical desired behavior.Consider Fig. 4. It shows the data set 2d-10c-no4 by Handl and Knowles , which is a synthetic data set with \(10\) clusters. The first two figures show the clusterings produced by \(k\)-means++ and Single Linkage for \(k=10\). We see that \(k\)-means++ finds most clusters but due to its limitation to spherical clusters, some clusters are split up and others are merged (Fig. 3(a)). Single Linkage on the other hand is too much affected by the outliers: It merges most points into two clusters, using the remaining \(\) clusters for very few outlier points (see Fig. 3(b)). But when allowed many more clusters (here, \(85\) instead of \(10\)), Single Linkage is very successful at finding the most important groups of points that belong together (see Fig. 4(c) in the suppl. material). Fig. 2(a) shows the Pareto curve. The 16th point with a separation of \(0.68\) is the most successful combination with repect to all metrics. It is shown in Fig. 3(c).

Numerical evaluation.We compare the clusterings in the approximate Pareto set to the ground truth. For this purpose we compute the Normalized Mutual Information (NMI), Rand Index (RI), and \(F_{}\)-scores for \(=0.5,1,2\). Each of these measures results in values in \(\), where a value of \(1\) is achieved when the computed clustering matches the ground truth. For every measure, we pick the clustering \(C^{*}\) in the approximate Pareto set that has highest value with respect to this measure to demonstrate that there is a good solution on the Pareto front. The full experimental evaluation, plots of Pareto fronts, other plots and also the definition of all scores can be found in Sec. A.3 in the supplementary material. Table 5 shows the Normalized Mutual Information Score for all tested data sets. We see the best solution found in 20 runs of the respective algorithm (except for Single Linkage, which is deterministic). The algorithms were run with the \(k\) associated with the ground truth of the data set. Single Linkage does in general not perform well in recovering the ground truth clustering. The clusterings produced by \(k\)-means++ are much better. For all data sets, the NMI of the best Pareto solution is even higher, with some notable positive examples like iris and 2d-10c-n4. The other performance measures show a similar tendency, although sometimes \(C^{*}\) is slightly worse, for example on data set rice (see Table 11 in Sec. A.3). This can happen because the solutions on

Figure 4: Clusterings computed on data set 2d-10c-no4 by Handl and Knowles .

    & Rice & Dry Bean & Wine & Optdigits & Iris & 2d-4c-no3 & 2d-10c-no3 & 2d-10c-no4 \\ 
**SL** & 0.0007 & 0.1626 & 0.0615 & 0.1220 & 0.0000 & 0.4631 & 0.6763 & 0.7260 \\
**kM++** & 0.4693 & 0.5164 & 0.4265 & 0.7459 & 0.7405 & 0.8715 & 0.9065 & 0.9052 \\
**C\({}^{*}\)** & 0.4728 & 0.5231 & 0.4400 & 0.7627 & 0.8578 & 0.9267 & 0.9491 & 0.9779 \\   

Table 5: NMI of the best solutions by single linkage and \(k\)-means++, and of the best solution \(C^{*}\) in the Pareto set. Randomized algorithms were repeated 20 times and values are then averages.

the Pareto front do not necessarily include the pure \(k\)-means++ solution as special case because it is only an approximation to the optimal \(k\)-means clustering and therefore can be dominated by other solutions in the approximate Pareto set with smaller \(k\)-means cost.

On all data sets that are not synthetic, the best result in the approximate Pareto set uses a small separation value. Therefore we conjecture that it is sufficient to use a few steps of single linkage before starting \(k\)-means++ in order to improve the quality of the solution.

### Applications: \(k\)-center with two different metrics

We use two \(k\)-center-objectives with different metrics to visualize two different geodetic data sets. The goal in both cases is to visualize data, but to also make it more compact and therefore easier to understand by trying to find geographically close areas that contain similar data. Details of this section are in Supplementary Sections C.2 and C.3. The first data set we use are the monthly median incomes of the 400 districts in Germany as obtained from . We compute an approximate Pareto set for \(k=16\) (the number of states in Germany), which contains a total of 38 different clusterings. Figure 2(b) shows the approximate Pareto curve. In Figure 5 one can see the 10-th Pareto-optimal solution compared to the clusterings that consider only one of the objectives. One can see that the Pareto solution has a cleaner structure than the purely income based clustering, but at the same time shows how the median income behaves in different regions of Germany.

For our second application we use a data set that consists of (normalized) time series of the sea level height on different locations all over the world. We use time series that were created using the sea level simulation provided by ORA5 , at the location where the tide gauge stations of the PSMSL [50; 39] are located. As metric \(d_{1}\) we use the mean difference between the time series. As geographic distance measure \(d_{2}\) we use the Euclidean distance between two stations. The time-series-based distance and the Euclidean distance of the gauge stations can behave very differently. We cluster with \(k=150\). Then the approximate Pareto set contains 106 clusterings. Table 6 shows the maximum and mean radii of the clusters in the solutions computed purely based on geometry, in the solution purely based on the time-series-based distance, and in a carefully picked Pareto-optimal solution. The numbers show that the Pareto-optimal solution is a very reasonable trade-off.

    & max rad geo & mean rad geoc & max rad ts & mean rad ts \\ 
**Geography based clustering** & 1566 km & 619 km & 20,9 cm & 4,1 cm \\
**54-th Pareto Solution** & 2411 km & 799 km & 8,0 cm & 3,1 cm \\
**Time Series based clustering** & 18172 km & 1570 km & 6,7 cm & 2,8 cm \\   

Table 6: Comparison between the time series based clustering, the geography based clustering, and the 54-th Pareto clustering with regard to the biggest cluster radius for both metrics and the mean cluster radius over all clusters for both metrics.

Figure 5: Comparison between the \(10\)-th Pareto solution with the purely geographic and the purely income based clustering for \(k=16\).