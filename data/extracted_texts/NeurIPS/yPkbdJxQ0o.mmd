# Appendix for "Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance "

Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance

Lisha Chen

Rensselaer Polytechnic Institute

Troy, NY, United States

chenl21@rpi.edu

&Heshan Fernando

Rensselaer Polytechnic Institute

Troy, NY, United States

fernah@rpi.edu

&Yiming Ying

University of Sydney

Camperdown, Australia

yiming.ying@sydney.edu.au

&Tianyi Chen

Rensselaer Polytechnic Institute

Troy, NY, United States

chentianyi19@gmail.com

Equal contribution.The work of L. Chen, H. Fernando, and T. Chen was supported by the National Science Foundation (NSF) MoDL-SCALE project 2134168 and the RPI-IBM Artificial Intelligence Research Collaboration (AIRC). The work of Y. Ying was partially supported by NSF (DMS-2110836, IIS-2103450, and IIS-2110546).

###### Abstract

Multi-objective learning (MOL) often arises in emerging machine learning problems when multiple learning criteria or tasks need to be addressed. Recent works have developed various _dynamic weighting_ algorithms for MOL, including MGDA and its variants, whose central idea is to find an update direction that _avoids conflicts_ among objectives. Albeit its appealing intuition, empirical studies show that dynamic weighting methods may not always outperform static alternatives. To bridge this gap between theory and practice, we focus on a new variant of stochastic MGDA - the Multi-objective gradient with Double sampling (MoDo) algorithm and study its generalization performance and the interplay with optimization through the lens of algorithm stability. We find that the rationale behind MGDA - updating along conflict-avoidant direction - may _impede_ dynamic weighting algorithms from achieving the optimal \((1/)\) population risk, where \(n\) is the number of training samples. We further highlight the variability of dynamic weights and their impact on the three-way trade-off among optimization, generalization, and conflict avoidance that is unique in MOL. Code is available at https://github.com/heshandevaka/Trade-Off-NOL.

## 1 Introduction

Multi-objective learning (MOL) emerges frequently in recent machine learning problems such as learning under fairness and safety constraints ; learning across multiple tasks, including multi-task learning  and meta-learning ; and, learning across multiple agents that may not share a global utility including federated learning  and multi-agent reinforcement learning .

This work considers solving the empirical version of MOL defined on the training dataset as \(S=\{z_{1},,z_{n}\}\). The performance of a model \(x^{d}\) on a datum \(z\) for the \(m\)-th objective is denoted as \(f_{z,m}:^{d}\), and its performance on the entire training dataset \(S\) is measured by the \(m\)-th empirical objective \(f_{S,m}(x)\) for \(m[M]\). MOL optimizes the vector-valued objective, given by

\[_{x^{d}}\ F_{S}(x)[f_{S,1}(x),,f_{S,M}(x)].\] (1.1)One natural method for solving (1.1) is to optimize the (weighted) average of the multiple objectives, also known as _static or unitary weighting_. However, this method may face challenges due to _potential conflicts_ among multiple objectives during the optimization process; e.g., conflicting gradient directions \( f_{S,m}(x), f_{S,m^{}}(x)<0\). A popular alternative is thus to _dynamically weight_ gradients from different objectives to avoid conflicts and obtain a direction \(d(x)\) that optimizes all objective functions jointly that we call a _conflict-avoidant_ (CA) direction. Algorithms in this category include the multi-gradient descent algorithm (MGDA) , its stochastic variants . While the idea of finding CA direction in dynamic weighting-based approaches is very appealing, recent empirical studies reveal that dynamic weighting methods may not outperform static weighting in some MOL benchmarks , especially when it involves stochastic updates and deep models. Specifically, observed by , the vanilla stochastic MGDA can be under-optimized, leading to larger training errors than static weighting. The reason behind this training performance degradation has been studied in , which suggest the vanilla stochastic MGDA has biased updates, and propose momentum-based methods to address this issue. Nevertheless, in , it is demonstrated that the training errors of MGDA and static weighting are similar, while their main difference lies in the generalization performance. Unfortunately, the reason behind this testing performance degradation is not fully understood and remains an open question.

To gain a deeper understanding of the dynamic weighting-based algorithms, a natural question is

**Q1:**_What are the major sources of errors in dynamic weighting-based MOL methods?_

To answer this question theoretically, we first introduce a proper measure of testing performance in MOL - the _Pareto stationary measure_ in terms of the population objectives, which will immediately imply stronger measures such as Pareto optimality under strongly convex objectives. We then decompose this measure into _generalization_ error and _optimization_ error and further introduce a new metric on the _distance to CA directions_ that is unique to MOL; see Sections 2.1 and 2.2.

To characterize the performance of MOL methods in a unified manner, we introduce a generic dynamic weighting-based MOL method that we term stochastic Multi-Objective gradient with DOuble sampling algorithm (**MoDo**), which uses a step size \(\) to control the change of dynamic weights. Roughly speaking, by controlling \(\), MoDo approximates MGDA (large \(\)) and static weighting algorithm (\(=0\)) as two special cases; see Section 2.3. We first analyze the generalization error of the model learned by MoDo through the lens of algorithmic stability  in the framework of statistical learning theory. To our best knowledge, this is the _first-ever-known_ stability analysis for MOL algorithms. Here the key contributions lie in defining a new notion of stability - MOL uniform stability and then establishing a tight upper bound (matching lower bound) on the MOL uniform stability for MoDo algorithm that involves two coupled sequences; see Section 3.1. We then analyze the optimization error of MoDo and its distance to CA directions, where the key contributions lie in relaxing _the bounded function value/gradient assumptions_ and significantly improving the convergence rate of state-of-the-art dynamic weighting-based MOL methods ; see Section 3.2.

Different from the stability analysis for single-objective learning , the techniques used in our generalization and optimization analysis allow to remove conflicting assumptions and use larger step sizes to ensure both small generalization and optimization errors, which are of independent interest.

Given the holistic analysis of dynamic weighting methods provided in **Q1**, a follow-up question is

**Q2:**_What may cause the empirical performance degradation of dynamic weighting methods?_

Figure 1: An example from  with two objectives (1a and 1b) to show the three-way trade-off in MOL. Figures 1c-1e show the optimization trajectories, where the **black**\(\) marks initializations of the trajectories, colored from **red** (start) to yellow (end). The background solid/dotted contours display the landscape of the average empirical/population objectives. The gray/green bar marks empirical/population Pareto front, and the **black**\(\)/green\(\) marks solution to the average objectives.

Aligned with this toy example, our theoretical results suggest a novel _three-way trade-off_ in the performance of dynamic weighting-based MOL algorithm; see Section 3.3. Specifically, it suggests that the step size for dynamic weighting \(\) plays a central role in the trade-off among convergence to CA direction, convergence to empirical Pareto stationarity, and generalization error; see Figure 2. In this sense, MGDA has an edge in convergence to the CA direction but it could sacrifice generalization; the static weighting method cannot converge to the CA direction but guarantees convergence to the empirical Pareto solutions and their generalization. Our analysis also suggests that MoDo achieves a small population risk under a proper combination of step sizes and the number of iterations.

## 2 Problem Formulation and Target of Analysis

In this section, we first introduce the problem formulation of MOL, the target of analysis, the metric to measure its generalization, and then present the MGDA algorithm and its stochastic variant.

### Preliminaries of MOL

Denote the vector-valued objective function on datum \(z\) as \(F_{z}(x)=[f_{z,1}(x),,f_{z,M}(x)]\). The training and testing performance of \(x\) can then be measured by the empirical objective \(F_{S}(x)\) and the population objective \(F(x)\) which are, respectively, defined as \(F_{S}(x)_{i=1}^{n}F_{z_{i}}(x)\) and \(F(x)_{z}[F_{z}(x)]\). Their corresponding gradients are denoted as \( F_{S}(x)\) and \( F(x)^{d M}\).

Analogous to the stationary solution and optimal solution in single-objective learning, we define the Pareto stationary point and Pareto optimal solution for MOL problem \(_{x^{d}}F(x)\) as follows.

**Definition 1** (Pareto stationary and Pareto optimal).: _If there exists a convex combination of the gradient vectors that equals to zero, i.e., there exists \(^{M}\) such that \( F(x)=0\), then \(x^{d}\) is Pareto stationary. If there is no \(x^{d}\) and \(x x^{*}\) such that, for all \(m[M]\)\(f_{m}(x) f_{m}(x^{*})\), with \(f_{m^{}}(x)<f_{m^{}}(x^{*})\) for at least one \(m^{}[M]\), then \(x^{*}\) is Pareto optimal. If there is no \(x^{d}\) such that for all \(m[M]\), \(f_{m}(x)<f_{m}(x^{*})\), then \(x^{*}\) is weakly Pareto optimal._

By definition, at a Pareto stationary solution, there is no common descent direction for all objectives. A necessary and sufficient condition for \(x\) being Pareto stationary for smooth objectives is that \(_{^{M}}\| F(x)\|=0\). Therefore, \(_{^{M}}\| F(x)\|\) can be used as a measure of Pareto stationarity (PS) . We will refer to the aforementioned quantity as the _PS population risk_ henceforth and its empirical version as _PS empirical risk_ or _PS optimization error_. We next introduce the target of our analysis based on the above definitions.

### Target of analysis and error decomposition

In existing generalization analysis for MOL, measures based on function values have been used to derive generalization guarantees in terms of Pareto optimality . However, for general

Figure 2: An illustration of three-way trade-off among optimization, generalization, and conflict avoidance in the strongly convex case; \(\) is the step size for \(x\), \(\) is the step size for weights \(\), where \(o()\) denotes a strictly slower growth rate, \(()\) denotes a strictly faster growth rate, and \(()\) denotes the same growth rate. Arrows \(\) and \(\) respectively represent diminishing in an optimal rate and growing in a fast rate w.r.t. \(n\), while \(\) represents diminishing w.r.t. \(n\), but not in an optimal rate.

nonconvex smooth MOL problems, it can only be guaranteed for an algorithm to converge to Pareto stationarity of the empirical objective, i.e., a small \(_{^{M}}\| F_{S}(x)\|\). Thus, it is not reasonable to measure population risk in terms of Pareto optimality in this case. Furthermore, when all the objectives are convex or strongly convex, Pareto stationarity is a sufficient condition for weak Pareto optimality or Pareto optimality, respectively, as stated in Proposition 1.

**Proposition 1** ([42, Lemma 2.2]).: _If \(f_{m}(x)\) are convex or strongly-convex for all \(m[M]\), and \(x^{d}\) is a Pareto stationary point of \(F(x)\), then \(x\) is weakly Pareto optimal or Pareto optimal._

Next, we proceed to decompose the PS population risk.

**Error Decomposition.** Given a model \(x\), the PS population risk can be decomposed into

\[_{}_{R_{}(x)}}\ =}\| F (x)\|-_{^{M}}\| F_{S}(x)\|}_{R_{}(x)}+}\| F_{S}(x)\|}_{ R_{}(x)}\] (2.1)

where the optimization error quantifies the training performance, i.e., how well does model \(x\) perform on the training data; and the generalization error (gap) quantifies the difference between the testing performance on new data sampled from \(\) and the training performance, i.e., how well the model \(x\) performs on unseen testing data compared to the training data.

Let \(A:^{n}^{d}\) denote a randomized MOL algorithm. Given training data \(S\), we are interested in the expected performance of the output model \(x=A(S)\), which is measured by \(_{A,S}[R_{}(A(S))]\). From (2.1) and linearity of expectation, it holds that

\[_{A,S}[R_{}(A(S))]=_{A,S}[R_{}(A(S))]+_{A,S}[R_{}(A(S))].\] (2.2)

**Distance to CA direction.** As demonstrated in Figure 1, the key merit of dynamic weighting over static weighting algorithms lies in its ability to navigate through conflicting gradients. Consider an update direction \(d=- F_{S}(x)\), where \(\) is the dynamic weights from a simplex \(^{M}\{^{M}^{} =1,\  0\}\). To obtain such a steepest CA direction in unconstrained learning that maximizes the minimum descent of all objectives, we can solve the following problem 

\[ d(x) =*{arg\,min}_{d^{d}}_{m[M]}  f_{S,m}(x),d+\|d\|^{2}}\] (2.3a) \[}}{{}} d(x) =- F_{S}(x)^{*}(x)\ \ \ \ ^{*}(x) *{arg\,min}_{^{M}}\| F_{S}(x)\|^{2}.\] (2.3b)

Defining \(d_{}(x)=- F_{S}(x)\) given \(x^{d}\) and \(^{M}\), we measure the distance to \(d(x)\) via 

\[_{}(x,) \|d_{}(x)-d(x)\|^{2}.\] (2.4)

With the above definitions of measures that quantify the performance of algorithms in different aspects, we then introduce a stochastic gradient algorithm for MOL that is analyzed in this work.

### A stochastic algorithm for MOL

MGDA finds \(^{*}(x)\) in (2.3b) using the full-batch gradient \( F_{S}(x)\), and then constructs \(d(x)=- F_{S}(x)^{*}(x)\), a CA direction for all empirical objectives \(f_{S,m}(x)\). However, in practical statistical learning settings, the full-batch gradient \( F_{S}(x)\) may be costly to obtain, and thus one may resort to a stochastic estimate of \( F_{S}(x)\) instead. The direct stochastic counterpart of MGDA, referred to as the stochastic multi-gradient algorithm in , replaces the full-batch gradients \( f_{S,m}(x)\) in (2.3b) with their stochastic approximations \( f_{z,m}(x)\) for \(z S\), which, however, introduces a biased stochastic estimate of \(^{*}_{t+1}\), thus a biased CA direction; see [10, Section 2.3].

To provide a tight analysis, we introduce a simple yet theoretically grounded stochastic variant of MGDA - stochastic Multi-Objective gradient with DOuble sampling algorithm (MoDo). MoDo obtains an unbiased stochastic estimate of the gradient of problem (2.3b) through double (independent) sampling and iteratively updates \(\), because \(_{z_{t,1},z_{t,2}}[ F_{z_{t,1}}(x_{t})^{} F_{z_{t,2}}( x_{t})_{t}]= F_{S}(x_{t})^{} F_{S}(x_{t})_{t}\). At each iteration \(t\), denote \(z_{t,s}\) as an independent sample from \(S\) with \(s\), and \( F_{z_{t,s}}(x_{t})\) as a stochastic estimate of \( F_{S}(x_{t})\). MoDo updates \(x_{t}\) and \(_{t}\) as

\[_{t+1} =_{^{M}}(_{t}-_{t} F_{z_{t,1}}(x _{t})^{} F_{z_{t,2}}(x_{t})_{t})\] (2.5a) \[x_{t+1} =x_{t}-_{t} F_{z_{t,3}}(x_{t})_{t+1}\] (2.5b)

where \(_{t},_{t}\) are step sizes, and \(_{^{M}}()\) denotes Euclidean projection to the simplex \(^{M}\). We have summarized the MoDo algorithm in Algorithm 1 and will focus on MoDo in the subsequent analysis.

## 3 Optimization, Generalization and Three-Way Trade-Off

This section presents the theoretical analysis of the PS population risk associated with the MoDo algorithm, where the analysis of generalization error is in Section 3.1 and that of optimization error is in Section 3.2. A summary of our main results is given in Table 1.

### Multi-objective generalization and uniform stability

We first bound the expected PS generalization error by the generalization in gradients in Proposition 2, then introduce the MOL uniform stability and establish its connection to the generalization in gradients. Finally, we bound the MOL uniform stability.

**Proposition 2**.: _With \(\|\|_{}\) denoting the Frobenious norm, \(R_{}(A(S))\) in (2.2) can be bounded by_

\[_{A,S}[R_{}(A(S))]_{A,S}[\| F(A(S)) - F_{S}(A(S))\|_{}].\] (3.1)

With Proposition 2, next we introduce the concept of MOL uniform stability tailored for MOL problems and show that PS generalization error in MOL can be bounded by the MOL uniform stability. Then we analyze their bound in general nonconvex case and strongly convex case, respectively.

**Definition 2** (MOL uniform stability).: _A randomized algorithm \(A:^{n}^{d}\), is MOL-uniformly stable with \(_{}\) if for all neighboring datasets \(S,S^{}\) that differ in at most one sample, we have_

\[_{z}\ _{A}\| F_{z}(A(S))- F_{z}(A(S^{})) \|_{}^{2}_{}^{2}.\] (3.2)

Next we show the relation between the upper bound of PS generalization error in (3.1) and MOL uniform stability in Proposition 3.

**Proposition 3** (MOL uniform stability and generalization).: _Assume for any \(z\), the function \(F_{z}(x)\) is differentiable. If a randomized algorithm \(A:^{n}^{d}\) is MOL-uniformly stable with \(_{}\), then_

\[_{A,S}[\| F(A(S))- F_{S}(A(S))\|_{}] 4 _{}+_{S}[_{z} ( F_{z}(A(S)))]}\] (3.3)

_where the variance is defined as \(_{z}( F_{z}(A(S)))=_{z }\| F_{z}(A(S))-_{z}[ F_{z}(A(S)) ]\|_{}^{2}\)._

   Assumption & Method & Optimization & Generalization & Risk & CA Distance \\   NC, \\ Lip-C, S \\  } & Static & \(( T)^{-}+^{}\) & \(T^{}n^{-}\) & \(n^{-}\) & ✗ \\  & Dynamic & \(( T)^{-}+^{}+^{}\) & \(T^{}n^{-}\) & \(n^{-}\) & \(( T)^{-1}+^{}^{-}+\) \\   SC, S \\  } & Static & \(( T)^{-}+^{}\) & \(n^{-}\) & \(n^{-}\) & ✗ \\  & Dynamic & \(( T)^{-}+^{}+^{}\) & \(\{n^{-},&=(T^{-1})\\ T^{}n^{-},&\\ .\) & \(\{n^{-}\\ n^{-}\\ n^{-}\\ .\) & \(( T)^{-1}+^{}^{-}+\) \\   

Table 1: Comparison of optimization error, generalization error, and population risk under different assumptions for static and dynamic weighting. Use “NC”, “SC” to represent nonconvex and strongly convex, and “Lip-C”, “S” to represent Lipschitz continuous and smooth, respectively.

Proposition 3 establishes a connection between the upper bound of the PS generalization error and the MOL uniform stability, where the former can be bounded above by the latter plus the variance of the stochastic gradient over the population data distribution. It is worth noting that the standard arguments of bounding the generalization error measured in function values by the uniform stability measured in function values [14, Theorem 2.2] is not applicable here as the summation and norm operators are not exchangeable. More explanations are given in the proof in Appendix B.1.

**Theorem 1** (PS generalization error of MoDo in nonconvex case).: _If \(_{z}_{A}[\| F_{z}(A(S))\|_{}^{2} ] G^{2}\) for any \(S\), then the MOL uniform stability, i.e., \(_{}^{2}\) in Definition 2 is bounded by \(_{}^{2} 4G^{2}Tn\). And the PS generalization error \(_{A,S}[R_{}(A(S))]=(T^{}n^{-})\)._

Compared to the function value uniform stability upper bound in [14, Theorem 3.12] for nonconvex single-objective learning, Theorem 1 does not require a step size decay \(_{t}=(1/t)\), thus can enjoy at least a polynomial convergence rate of optimization errors w.r.t. \(T\). Combining Theorem 1 with Proposition 3, to ensure the generalization error is diminishing with \(n\), one needs to choose \(T=o(n)\), which lies in the "early stopping" regime and results in potentially large optimization error. We then provide a tighter bound in the strongly convex case that allows a larger choice of \(T\). Below we list the standard assumptions used to derive the introduced MOL stability.

**Assumption 1** (Lipschitz continuity of \( F_{z}(x)\)).: _For all \(m[M]\), \( f_{z,m}(x)\) is \(_{f,1}\)-Lipschitz continuous for all \(z\). And \( F_{z}(x)\) is \(_{f,1}\)-Lipschitz continuous in Frobenius norm for all \(z\)._

**Assumption 2**.: _For all \(m[M]\), \(z\), \(f_{z,m}(x)\) is \(\)-strongly convex w.r.t. \(x\), with \(>0\)._

Note that in the strongly convex case, the gradient norm \(\| F_{z}(x)\|_{}\) can be unbounded in \(^{d}\). Therefore, one cannot assume Lipschitz continuity of \(f_{z,m}(x)\) w.r.t. \(x^{d}\). We address this challenge by showing that \(\{x_{t}\}\) generated by the MoDo algorithm is bounded as stated in Lemma 1. Notably, combining with Assumption 1, we can derive that the gradient norm \(\| F_{z}(x_{t})\|_{}\) is also bounded, which serves as a stepping stone to derive the MOL stability bound.

**Lemma 1** (Boundedness of \(x_{t}\) for strongly convex and smooth objectives).: _Suppose Assumptions 1, 2 hold. For \(\{x_{t}\},t[T]\) generated by MoDo algorithm or other dynamic weighting algorithm with weight \(^{M}\), step size \(_{t}=\), and \(0_{f,1}^{-1}\), there exists a finite positive constant \(c_{x}\) such that \(\|x_{t}\| c_{x}\). And there exists finite positive constants \(_{f}\), \(_{F}=_{f}\), such that for all \(^{M}\), we have \(\| F(x_{t})\|_{f}\), and \(\| F(x_{t})\|_{}_{F}\)._

With Lemma 1, the stability bound and PS generalization is provided below.

**Theorem 2** (PS generalization error of MoDo in strongly convex case).: _Suppose Assumptions 1, 2 hold. Let \(A\) be the MoDo algorithm (Algorithm 1). For the MOL uniform stability \(_{F}\) of algorithm \(A\) in Definition 2, if the step sizes satisfy \(0<_{t} 1/(2_{f,1})\), and \(0<_{t}\{}{120_{f}^{2}_{g,1}}, ^{2}+2_{g,1})}\}/T\), then it holds that_

\[_{}^{2}_{f}^{2}_{F,1}^{2} +^{2}}{ n}+^{4}}{} _{A,S}[R_{}(A(S))]=(n^{ -}).\] (3.4)

_And there exist functions \(F_{z}(x)\) that satisfy Assumptions 1, 2, neighboring datasets \(S\), \(S^{}\) that differ in at most one sample, and MoDo algorithm with step sizes \(0<_{t} 1/(2_{f,1})\), and \(0<_{t}\{}{120_{f}^{2}_{g,1}}, ^{2}+2_{g,1})}\}/T\) such that_

\[_{A}\| F_{z}(A(S))- F_{z}(A(S^{}))\|_{ }^{2}}{256n^{2}}.\] (3.5)

Theorem 2 provides both upper and lower bounds for the MOL uniform stability. In this case, we choose \(=(T^{-})\), \(=o(T^{-1})\), and \(T=(n^{2})\) to minimize the PS population risk upper bound, as detailed in Section 3.3. With this choice, the MOL uniform stability upper bound matches the lower bound in an order of \(n^{-2}\), suggesting that our bound is tight. The generalization error bound in (3.4) is a direct implication from the MOL uniform stability bound in (3.4), Propositions 2, and 3.

It states that the PS generalization error of MoDo is \((n^{-})\), which matches the generalization error of static weighting up to a constant coefficient . Our result also indicates that when all the objectives are strongly convex, choosing small step sizes \(\) and \(\) can benefit the generalization error.

### Multi-objective optimization error

In this section, we bound the multi-objective PS optimization error \(_{^{M}}\| F_{S}(x)\|\)[9; 30; 10]. As discussed in Section 2.2, this measure being zero implies the model \(x\) achieves a Pareto stationarity for the empirical problem.

Below we list an additional standard assumption used to derive the optimization error.

**Assumption 3** (Lipschitz continuity of \(F_{z}(x)\)).: _For all \(m[M]\), \(f_{z,m}(x)\) are \(_{f}\)-Lipschitz continuous for all \(z\). Then \(F_{z}(x)\) are \(_{F}\)-Lipschitz continuous in Frobenius norm for all \(z\) with \(_{F}=_{f}\)._

**Lemma 2** (Distance to CA direction).: _Suppose either: 1)Assumptions 1, 3 hold; or 2) Assumptions 1, 2 hold, with \(_{f}\) and \(_{F}\) defined in Lemma 1. For \(\{x_{t}\},\{_{t}\}\) generated by MoDo, it holds that_

\[_{t=0}^{T-1}_{A}[\|d_{_{t}}(x_{t})-d(x_{t})\|^ {2}]+6_{f}^{2}}+ M_{f}^{4}.\] (3.6)

Lemma 2 analyzes convergence to the CA direction using the measure introduced in Section 2.2. By, e.g., choosing \(=(T^{-})\), and \(=(T^{-})\), the RHS of (3.6) converges in a rate of \((T^{-})\).

**Theorem 3** (PS optimization error of MoDo).: _Suppose either: 1) Assumptions 1, 3 hold; or, 2) Assumptions 1, 2 hold, with \(_{f}\) defined in Lemma 1. Define \(c_{F}\) such that \(_{A}[F_{S}(x_{0})_{0}]=_{x^{d}}_{ A}[F_{S}(x)_{0}] c_{F}\). Considering \(\{x_{t}\}\) generated by MoDo (Algorithm 1), with \(_{t}= 1/(2_{f,1})\), \(_{t}=\), then under either condition 1) or 2), it holds that_

\[_{t=0}^{T-1}_{A}_{^{M}}\|  F_{S}(x_{t})\|}{ T}}+ M_{f}^{4}}+_{f,1}_{f}^{2}}.\] (3.7)

The choice of step sizes \(=(T^{-})\), and \(=(T^{-})\) to ensure convergence to CA direction is suboptimal for the convergence to Pareto stationarity (see Theorem 3), exhibiting a trade-off between convergence to the CA direction and convergence to Pareto stationarity; see discussion in Section 3.3.

### Optimization, generalization and conflict avoidance trade-off

Combining the results in Sections 3.1 and 3.2, we are ready to analyze and summarize the three-way trade-off of MoDo in MOL. With \(A_{t}(S)=x_{t}\) denoting the output of algorithm \(A\) at the \(t\)-th iteration, we can decompose the PS population risk \(R_{}(A_{t}(S))\) as (cf. (2.1) and (3.1))

\[_{A,S}R_{}(A_{t}(S))_{A,S} _{^{M}}\| F_{S}(A_{t}(S))\|+ _{A,S}\| F(A_{t}(S))- F_{S}(A_{t}(S))\|_{}.\]

**The general nonconvex case.** Suppose Assumptions 1, 3 hold. By the generalization error in Theorem 1, and the optimization error bound in Theorem 3, the PS population risk of the output of MoDo can be bounded by

\[_{t=0}^{T-1}_{A,S}R_{}(A_{t}(S)) =(^{-}T^{-}+^{}+^{}+T^{}n^{-}).\] (3.8)

_Discussion of trade-off._ Choosing step sizes \(=(T^{-})\), \(=(T^{-})\), and number of steps \(T=(n^{})\), then the expected PS population risk is \((n^{-})\), which matches the PS population risk upper bound of a general nonconvex single objective in . A clear trade-off in this case is between the optimization error and generalization error, controlled by \(T\). Indeed, increasing \(T\) leads to smaller optimization errors but larger generalization errors, and vice versa. To satisfy convergence to CAdirection, it requires \(=()\) based on Lemma 2, and the optimization error in turn becomes worse, so does the PS population risk. Specifically, choosing \(=(T^{-})\), \(=(T^{-})\), and \(T=(n^{})\) leads to the expected PS population risk in \((n^{-})\), and the distance to CA direction in \((n^{-})\). This shows another trade-off between conflict avoidance and optimization error.

**The strongly convex case.** Suppose Assumptions 1, 2 hold. By the generalization error and the optimization error given in Theorems 2 and 3, MoDo's PS population risk can be bounded by

\[_{t=0}^{T-1}_{A,S}R_{}(A_{t}(S)) =(^{-}T^{-}+^{}+^{}+n^{-}).\] (3.9)

_Discussion of trade-off._ Choosing step sizes \(=(T^{-})\), \(=o(T^{-1})\), and number of steps \(T=(n^{2})\), we have the expected PS population risk in gradients is \((n^{-})\). However, choosing \(=o(T^{-1})\) leads to large distance to the CA direction according to Lemma 2 because the term \(\) in (3.6) increases with \(T\). To ensure convergence to the CA direction, it requires \(=(T^{-1})\), under which the tighter bound in Theorem 2 does not hold but the bound in Theorem 1 still holds. In this case, the PS population risk under proper choices of \(,,T\) is \((n^{-})\) as discussed in the previous paragraph. Therefore, to avoid conflict of gradients, one needs to sacrifice the sample complexity of PS population risk, demonstrating a trade-off between conflict avoidance and PS population risk.

## 4 Related Works and Our Technical Contributions

**Multi-task learning (MTL).** MTL, as one application of MOL, leverages shared information among different tasks to train a model that can perform multiple tasks. MTL has been widely applied to natural language processing, computer vision, and robotics [15; 38; 50; 43]. From the optimization perspective, a simple method for MTL is to take the weighted average of the per-task losses as the objective. However, as studied in , the static weighting method is not able to find all the Pareto optimal models in general. Alternatively, the weights can be updated dynamically during optimization, and the weights for different tasks can be chosen based on different criteria such as uncertainty , gradient norms , or task difficulty . These methods are often heuristic and designed for specific applications. Another line of work tackles MTL through MOL [39; 48; 29; 12]. A foundational algorithm in this regard is MGDA , which takes dynamic weighting of gradients to obtain a CA direction for all objectives. Stochastic versions of MGDA have been proposed in [30; 52; 10]. Besides finding one single model, algorithms for finding a set of Pareto optimal models rather than one have been proposed in [28; 35; 31; 20; 46; 51; 32; 27; 34].

**Theory of MOL.**_Optimization_ convergence analysis for the deterministic MGDA algorithm has been provided in . Later on, stochastic variants of MGDA were introduced [30; 52; 10]. However, the vanilla stochastic MGDA introduces biased estimates of the dynamic weight, resulting in biased estimates of the CA directions during optimization. To address this, Liu et al.  proposed to increase the batch size during optimization, Zhou et al.  and Fernando et al.  proposed to use momentum-based bias reduction techniques. Compared to these works on the optimization analysis, we have improved the assumptions and/or the final convergence rate of the PS optimization error. A detailed comparison is summarized in Table 2. While the community has a rich history of investigating the optimization of MOL algorithms, their _generalization_ guarantee remains unexplored until recently. In , a min-max formulation to solve the MOL problem is analyzed, where the weights are chosen based on the maximum function values, rather than the CA direction. More recently,  provides generalization guarantees for MOL for a more general class of weighting. These two works analyze generalization based on the Rademacher complexity of the hypothesis class, with generalization bound independent of the training process. Different from these works, we use algorithm stability to derive the first algorithm-dependent generalization error bounds, highlighting the effect of the training dynamics. In contrast to previous MOL theoretical works that focus solely on either optimization [10; 52] or generalization [7; 41], we propose a holistic framework to analyze three types of errors, namely, optimization, generalization, and CA distance in MOL with an instantiation of the MoDo algorithm. This allows us to study the impact of the hyperparameters on the theoretical testing performance, and their optimal values to achieve the best trade-off among the errors. Our theory and algorithm design can also be applied to algorithms such as PCGrad , CAGrad , and GradNorm . Specifically, for example, the implementation of CAGrad takes iterative updates of the dynamic weight using a single stochastic estimate, resulting in a biased estimate of the update direction, thus no guarantee of convergence for the stochastic algorithm. This issue can be addressed by the double sampling technique introduced in this paper. In addition, our analysis techniques have been applied to improve the analysis of convergence rates for other algorithms [44; 5].

**Algorithm stability and generalization.** Stability analysis dates back to the work  in 1970s. Uniform stability and its relationship with generalization were studied in  for the exact minimizer of the ERM problem with strongly convex objectives. The work  pioneered the stability analysis for stochastic gradient descent (SGD) algorithms with convex and smooth objectives. The results were extended and refined in  with data-dependent bounds, in [4; 37; 23] for non-convex objectives, and in [1; 24] for SGD with non-smooth and convex losses. However, all these studies mainly focus on single-objective learning problems. To our best knowledge, there is no existing work on the stability and generalization analysis for multi-objective learning problems and our results on its stability and generalization are the _first-ever-known_ ones.

**Challenges and contributions.** Our contributions are highly non-trivial as summarized below.

\(\) The definition of PS testing risk in gradient (2.1) is unique in MOL, and overcomes the unnecessarily small step sizes usually brought by the classical function value-based risk analysis. Specifically, prior stability analysis in function values for single objective learning  requires \(1/t\) step size decay in the nonconvex case, otherwise, the generalization error bound will depend exponentially on the number of iterations. However such step sizes lead to very slow convergence of the optimization error. This is addressed by the definitions of gradient-based measures and sampling-determined MOL algorithms, which yield stability bounds in \((T/n)\) without any step size decay. See Theorem 1.

\(\) The stability of the dynamic weighting algorithm in the strongly convex (SC) case is non-trivial compared to single objective learning  because it involves two coupled sequences during the update. As a result, the classical contraction property for the update function of the model parameters that are often used to derive stability does not hold. This is addressed by controlling the change of \(_{t}\) by the step size \(\), and using mathematical induction to derive a tighter bound. See Appendix B.4.

\(\) In the SC case with an unbounded domain, the function is not Lipschitz or the gradients are unbounded, which violates the commonly used bounded gradient assumption for proving the stability and optimization error. We relax this assumption by proving that the iterates generated by dynamic weighting algorithms in the SC case are bounded on the optimization trajectory in Lemma 1.

## 5 Experiments

In this section, we conduct experiments to further demonstrate the three-way trade-off among the optimization, generalization, and conflict avoidance of the MoDo algorithm. An average of 10 random seeds with 0.5 standard deviation is reported if not otherwise specified.

### Synthetic experiments

Our theory in the SC case is first verified through a synthetic experiment; see the details in Appendix D.1. Figure 2(a) shows the PS optimization error and PS population risk, as well as the distance to CA direction, decreases as \(T\) increases, which corroborates Lemma 2, and Theorem 3. In addition, the generalization error, in this case, does not vary much with \(T\), verifying Theorems 2. In Figure 2(b), the optimization error first decreases and then increases as \(\) increases, which is consistent with Theorem 3. Notably, we observe a threshold for \(\) below which the distance to the CA direction converges even when the optimization error does not converge, while beyond which the distance to the CA direction becomes larger, verifying Lemma 2. Additionally, Figure 2(c) demonstrates that

    &  Batch \\ size \\  } &  &  Lipschitz \\ \(^{*}(x)\) \\  } &  Bounded \\ function \\  } &  &  CA \\ dist. \\  } &  \\  SMG [30; Thm 5.3] & \((t)\) & ✗ & ✓ & ✗ & \(T^{-}\) & - & - \\ CR-MOGM [52; Thm 3] & \((1)\) & ✓ & ✗ & ✓ & \(T^{-}\) & - & - \\ MoCo [10; Thm 2] & \((1)\) & ✓ & ✗ & ✗ & \(T^{-}\) & \(T^{-}\) & - \\ MoCo [10; Thm 4] & \((1)\) & ✓ & ✗ & ✓ & \(T^{-}\) & \((1)\) & - \\   & \((1)\) & ✓ & ✗ & ✗ & \(T^{-}\) & \((1)\) & \(T^{}n^{-}\) \\  & \((1)\) & ✓ & ✗ & ✗ & \(T^{-}\) & \(T^{-}\) & \(T^{}n^{-}\) \\   

Table 2: Comparison with prior stochastic MOL algorithms in terms of assumptions and the guarantees of the three errors, where logarithmic dependence is omitted, and Opt., CA dist., and Gen. are short for optimization error, CA distance, and generalization error, respectively.

increasing \(\) enlarges the PS optimization error, PS generalization error, and thus the PS population risk, but decreases the distance to CA direction, which supports Lemma 2.

### Image classification experiments

We further verify our theory in the NC case on MNIST image classification  using a multi-layer perceptron and three objectives: cross-entropy, mean squared error (MSE), and Huber loss. Following Section 2.2, we evaluate the performance in terms of \(R_{}(x)\), \(R_{}(x)\), \(R_{}(x)\), and \(_{}(x,)\). The exact PS population risk \(R_{}(x)\) is not accessible without the true data distribution. To estimate the PS population risk, we evaluate \(_{^{M}}\| F_{S_{}}(x)\|\) on the testing data set \(S_{}\) that is independent of training data set \(S\). The PS optimization error \(R_{}(x)\) is obtained by \(_{^{M}}\| F_{S}(x)\|\), and the PS generalization error \(R_{}(x)\) is estimated by \(_{^{M}}\| F_{S_{}}(x)\|-R_{}(x)\).

We examine the impact of different \(T\), \(\), \(\) on the errors in Figure 4. Figure 3(a) shows that increasing \(T\) reduces optimization error and CA direction distance but increases generalization error, aligning with Theorems 1, 2, and 3. Figure 3(b) shows that increasing \(\) leads to an initial decrease and subsequent increase in PS optimization error and population risk. which aligns with Theorem 3 and (3.8). On the other hand, there is an overall increase in CA direction distance with \(\), which aligns with Theorem 2. Figure 3(c) shows that increasing \(\) increases both the PS population and optimization errors but decreases CA direction distance. This matches our bounds for PS optimization error in Theorem 3, PS population risk in (3.8), and CA direction distance in Theorem 2.

## 6 Conclusions

This work studies the three-way trade-off in MOL - among optimization, generalization, and conflict avoidance. Our results show that, in the general nonconvex setting, the traditional trade-off between optimization and generalization depending on the number of iterations also exists in MOL. Moreover, dynamic weighting algorithms like MoDo introduce a new dimension of trade-off in terms of conflict avoidance compared to static weighting. We demonstrate that this three-way trade-off can be controlled by the step size for updating the dynamic weighting parameter and the number of iterations. Proper choice of these parameters can lead to decent performance on all three metrics.

Figure 4: Optimization, generalization, and CA direction errors of MoDo for MNIST image classification under different \(T\), \(\), and \(\). The default parameters are \(T=1000\), \(=0.1\), and \(=0.01\).

    & Amazon & DSLR & Webcam & \%\)} \\    & Test Acc \(\) & Test Acc \(\) & Test Acc \(\) \\  Static & 84.62 \(\) 0.71 & 94.43 \(\) 0.96 & 97.44 \(\) 1.20 & 2.56 \(\) 0.37 \\ MGDA & 79.45 \(\) 0.11 & **96.56 \(\) 1.20** & **97.89 \(\) 0.74** & 3.65 \(\) 0.64 \\
**MoDo** & **85.13 \(\) 0.58** & 95.41 \(\) 0.98 & 96.78 \(\) 0.65 & **2.26 \(\) 0.31** \\   

Table 3: Classification results on Office-31 dataset.

Figure 3: Optimization, generalization, and CA direction errors of MoDo in the strongly convex case under different \(T,,\). The default parameters are \(T=100\), \(=0.01\), \(=0.001\).

## Broader impacts and limitations

This work has a potential impact on designing dynamic weighting algorithms and choosing hyperparameters such as step sizes and number of iterations based on the trade-off for MTL applications such as multi-language translation, and multi-agent reinforcement learning. No ethical concerns arise from this work. A limitation of this study is that the theory focuses on a specific algorithm, MoDo, for smooth objectives in unconstrained learning. Future research could explore the theory of other algorithms for non-smooth objectives or constrained learning, which would be interesting directions to pursue.