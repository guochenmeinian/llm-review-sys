# Eagle : Efficient Adaptive Geometry-based Learning in Cross-view Understanding

Thanh-Dat Truong\({}^{1}\), Utsav Prabhu\({}^{2}\), Dongyi Wang\({}^{3}\)

**Bhiksha Raj\({}^{4,5}\), Susan Gauch\({}^{6}\), Jeyamkondan Subbiah\({}^{7}\), Khoa Luu\({}^{1}\)**

\({}^{1}\)CVIU Lab, University of Arkansas, USA \({}^{2}\)Google DeepMind, USA

\({}^{3}\)Dep. of BAEG, University of Arkansas, USA \({}^{4}\)Carnegie Mellon University, USA

\({}^{5}\)Mohammed bin Zayed University of AI, UAE

\({}^{6}\)Dep. of EECS, University of Arkansas, USA \({}^{7}\)Dep. of FDSC, University of Arkansas, USA

{tt032, dongyiw, ggauch, jsubbiah, khoaluu}@uark.edu

bhiksha@cs.cmu.edu, utsavprabhu@google.com

https://uark-cviu.github.io/projects/EAGLE

###### Abstract

Unsupervised Domain Adaptation has been an efficient approach to transferring the semantic segmentation model across data distributions. Meanwhile, the recent Open-vocabulary Semantic Scene understanding based on large-scale vision language models is effective in open-set settings because it can learn diverse concepts and categories. However, these prior methods fail to generalize across different camera views due to the lack of cross-view geometric modeling. At present, there are limited studies analyzing cross-view learning. To address this problem, we introduce a novel Unsupervised Cross-view Adaptation Learning approach to modeling the geometric structural change across views in Semantic Scene Understanding. First, we introduce a novel Cross-view Geometric Constraint on Unpaired Data to model structural changes in images and segmentation masks across cameras. Second, we present a new Geodesic Flow-based Correlation Metric to efficiently measure the geometric structural changes across camera views. Third, we introduce a novel view-condition prompting mechanism to enhance the view-information modeling of the open-vocabulary segmentation network in cross-view adaptation learning. The experiments on different cross-view adaptation benchmarks have shown the effectiveness of our approach in cross-view modeling, demonstrating that we achieve State-of-the-Art (SOTA) performance compared to prior unsupervised domain adaptation and open-vocabulary semantic segmentation methods.

## 1 Introduction

Modern segmentation models [3; 4; 63] have achieved remarkable results on the close-set training with a set of pre-defined categories and concepts. To work towards human-level perception where the scenes are interpreted with diverse categories and concepts, the open-vocabulary (open-vocab) perception model [38; 40] based on the power of large vision-language models [30; 39] has been introduced to address the limitations of close-set training. By using the power of language as supervision, the large-scale vision language model is able to learn the more powerful representations where languages offer better reasoning mechanisms and open-word concept representations compared to traditional close-set training methods [3; 63; 9].

Figure 1: _Our Proposed Cross-view Adaptation Learning Approach._ Prior models, e.g., FreeSeg , DenseCLIP , trained on the car view do not perform well on the drone-view images. Meanwhile, our cross-view adaptation approach is able to generalize well from the car to drone view.

Recent work is inspired by the success of large vision-language models [39; 27] that are able to learn informative feature representations of both visual and textual inputs from large-scale image-text pairs. These have been adopted to further develop open-vocab semantic segmentation models [38; 40; 31; 29] that can work well in open-world environments. However, the open-vocab perception models remain unable to generalize across camera viewpoints. As shown in Fig. 1, the open-vocab model trained on car views is not able to perform well on the images captured from unmanned aerial vehicles (UAVs) or drones. While this issue can be improved by training the segmentation model on drone-view data, the annotation process of high-resolution UAV data is costly and time-consuming. At present, there exist many large-scale datasets with dense labels captured from camera views on the ground, e.g., car views (SYNTHIA , GTA , Cityscapes , BDD100K ). They have been widely adopted to develop robust perception models. Since these car view and drone view datasets have many common objects of interest, incorporating knowledge from car views with drone views benefits the learning process by reusing large-scale annotations and saving efforts of manually labeling UAV images. Unsupervised domain adaptation (UDA) [58; 23; 1; 51; 53] is one of the potential approaches to transfer the knowledge from the car view (i.e., source domain) to the drone view (i.e., target domain). While UDA approaches have shown their effectiveness in transferring knowledge across domains, e.g., environment changes or geographical domain shifts, these methods remain limited in the cases of changing camera viewpoints. Indeed, the changes in camera positions, e.g., from the ground of cars to the high positions of drones, bring a significant difference in structures and topological layouts of scenes and objects (Fig. 2). Therefore, UDA is not a complete solution to this problem due to its lack of cross-view structural modeling. Additionally, although the open-vocab segmentation models have introduced several prompting mechanisms, e.g., context-aware prompting  or adaptive prompting  to improve context learning across various open-world concepts, they are unable to model the cross-view structure due to the lack of view-condition information in prompts and geometric modeling. To the best of our knowledge, there are limited studies that have exploited this cross-view learning. These limitations motivate us to develop a new adaptation learning paradigm, i.e., _Unsupervised Cross-view Adaptation_, that addresses prior methods to improve the performance of semantic segmentation models across views.

**Contributions:** This work introduces a novel _Efficient Adaptive Geometry-based Learning (EAGLE)_ to _Unsupervised Cross-view Adaptation_ that can adaptively learn and improve the performance of semantic segmentation models across camera viewpoints. First, by analyzing the geometric correlations across views, we introduce a novel _cross-view geometric constraint_ on _unpaired data_ of structural changes in images and segmentation masks. Second, to efficiently model _cross-view geometric structural changes_, we introduce a new _Geodesic Flow-based Metric_ to measure the structural changes across views via their manifold structures. In addition, to further improve the prompting mechanism of the open-vocab segmentation network in cross-view adaptation learning, we introduce a new _view-condition prompting_. Then, our _cross-view geometric constraint_ is also imposed on its feature representations of view-condition prompts to leverage its geometric knowledge embedded in our prompting mechanism. Our proposed method holds a promise to be an effective approach to addressing the problem of cross-view learning and contributes to improving UDA and open-vocab segmentation in cross-view learning. Thus, it increases the generalizability of the segmentation models across camera views. Finally, our experiments on three presented cross-view adaptation benchmarks, i.e., SYNTHIA \(\) UAVID, GTA \(\) UAVID, BDD \(\) UAVID, illustrate the effectiveness of our approach in cross-view modeling and our State-of-the-Art (SOTA) performance.

## 2 Related Work

**Unsupervised Domain Adaptation** Adversarial learning [6; 58; 59] and self-supervised learning [1; 69; 23; 14] are common approaches to UDA in semantic segmentation. The adversarial learning approaches are typically simultaneously trained on source and target data [58; 7; 6]. Chen et al.  first introduced an adversarial framework to domain adaptation. Later, several approaches improved

Figure 2: An Example of Illustration of Cross-View Adaptation From Car View to Drone View.

adversarial learning by utilizing generative models [74; 34; 21], using additional labels [28; 59], incorporating with entropy minimization [58; 65; 51; 52], or adopting the curriculum training . Recently, the self-supervised approaches [1; 69; 23; 14] have achieved outstanding performance. Araslanov et al.  first proposed a self-supervised augmentation consistency framework for UDA. Hoyer et al.  utilized Transformers to improve the UDA performance. Later, this approach was further improved by utilizing multi-resolution cropped images  and masked image consistency strategy  to enhance contextual learning. Recent studies improved the self-supervised approach by aligning both output and attention levels via the cross-domain prediction consistency framework , using a prototypical representation , learning the cross-model consistency via depths , improving the class-relevant fairness [53; 55; 56], or exploring the relations of pseudo-labels . Fashes et al.  introduced a prompt-based feature augmentation method to zero-shot UDA. Gong et al.  introduced a geodesic flow kernel to model the manifold structure between domains. Later, Simon et al.  designed distillation loss by the geodesic flow path.

**Vision-Language and Open-Vocab Segmentation** By pre-training on a large-scale vision-language dataset [39; 27], the vision-language models can learn various visual concepts and can further be transferred to other vision problems through "_prompting_" [17; 38; 31; 35], e.g., open-vocab segmentation [64; 13; 38]. Li et al.  first introduced the language-driving approach to semantic segmentation. Rao et al.  represented a context-aware prompting mechanism for dense prediction tasks. Ghiasi et al.  proposed an OpenSeg framework that learns the visual-semantic alignments. Qin et al.  presented a unified, universal, and open-vocab segmentation network based on Mask2Former  with an adaptive prompting mechanism. Xu et al.  proposed a two-stage open-vocab segmentation framework using the mask proposal generator and the pre-trained CLIP model. Ding et al.  decoupled the zero-shot semantic segmentation to class-agnostic segmentation and segment-level zero-shot classification. Liang et al.  improved the two-stage open-vocab segmentation model by further fine-tuning CLIP on masked image regions and corresponding descriptions.

**Cross-view Learning** The early studies exploited cross-view learning in geo-localization by using a polar transform across views [46; 45] or generative networks to cross-view images [41; 49]. Meanwhile, Zhu et al.  exploited the correlation between street- and aerial-view data via self-attention. In semantic segmentation, Coors et al.  first introduced a cross-view adaptation approach utilizing the depth labels and the cross-view transformation between car and truck views. However, this change of views in  is not as big a hurdle as the change of views in our problem, i.e., car view to drone view. Ren et al.  presented an adaptation approach across viewpoints using the 3D models of scenes to create pairs of cross-view images. Vidit et al.  modeled the geometric shift in cross FoV setting for object detection by learning position-invariant homography transform. Di Mauro et al.  introduced an adversarial method trained on a multi-view synthetic dataset where images are captured from different pitch and yaw angles at the same altitudes of the camera positions. Meanwhile, in our problem, the camera views could be placed at different altitudes (e.g., the car and the drone), which reveals large structural differences between the images. Truong et al. [50; 54] first introduced a simple approach to model the relation across views. CROVIA  measures the cross-view structural changes by measuring the distribution shift and only focuses on the cross-view adaptation setting in semantic segmentation. However, these methods [50; 54] lack a theory and a mechanism for cross-view geometric structural change modeling. To the best of our knowledge, there are limited studies exploiting cross-view adaptation in semantic segmentation. Therefore, our work presents a new approach to model the geometric correlation across views.

## 3 The Proposed EAGLE Approach

In this paper, we consider cross-view adaptation learning as UDA where the images of the source and target domains are captured from different camera positions (Fig. 2). Formally, let \(_{s},_{t}\) be the input images in the source and target domains, \(_{s},_{t}\) be the the corresponding prompts, and \(_{s},_{t}\) be the segmentation masks of \(_{s},_{t}\). Then, the open-vocab segmentation model \(F\) maps the input \(\) and the prompt \(\) to the corresponding output \(=F(,)\). It should be noted that in the case of traditional semantic segmentation, the prompt \(\) will be ignored, i.e., \(=F()\) The cross-view adaptation learning can be formulated as Eqn. (1).

\[_{}[_{_{s},_{s},}_{s}}_{Mask}(_{s},}_{s})+ _{_{t},_{t}}_{Adapt}(_{t})]\] (1)where \(\) is the parameters of \(F\), \(}_{s}\) is the ground truth, \(_{Mask}\) is the supervised (open-vocab) segmentation loss with ground truths, and \(_{Adapt}\) is unsupervised adaptation loss from the source to the target domain. In the open-vocab setting, we adopt the design of Open-Vocab Mask2Former [8; 38] to our network \(F\). Prior UDA methods defined the adaptation loss \(_{Adapt}\) via the adversarial loss [28; 5], entropy loss [51; 58], or self-supervised loss [23; 25]. Although these prior results have illustrated their effectiveness in UDA, these losses remain limited in cross-view adaptation setup. Indeed, the adaptation setting in prior studies [58; 1; 23; 15] is typically deployed in the context of environmental changes (e.g., simulation to real [58; 59; 15], day to night [25; 15], etc) where the camera positions between domains remain similar. Meanwhile, in cross-view adaptation, the camera position of the source and target domain remains largely different (as shown in Fig. 2). This change in camera positions leads to significant differences in the geometric layout and topological structures between the source and target domains. As a result, direct adoption of prior UDA approaches to cross-view adaptation would be ineffective due to the lack of cross-view geometric correlation modeling. To effectively address cross-view adaptation, the adaptation loss \(_{Adapt}\) should be able to model (1) _the geometric correlation between two views of source and target domains_ and (2) _the structural changes across domains_.

### Cross-View Geometric Modeling

To efficiently address the cross-view adaptation learning task, it is essential to explicitly model cross-view geometric correlations by analyzing the relation between two camera views. Therefore, we first re-reconsider the cross-view geometric correlation. In particular, let \(}_{t}\) be the corresponding image of \(_{s}\) captured from the target view, \(_{s}\) and \(}_{t}\) be the semantic segmentation outputs of source image \(_{s}\) and target image \(}_{t}\), \(}_{t}\) be the corresponding prompt of \(_{s}\) in target view, respectively. Formally, the images captured from the source and the target views can be modeled as Eqn. (2).

\[_{s}=(_{s},[_{s},_{s}], ),}_{t}=(_{t},[_{t},_{t}],)\] (2)

where \(\) is the rendering function, \(_{s}\) and \(_{t}\) are the intrinsic matrices, \([_{s},_{s}]\) and \([_{t},_{t}]\) are the extrinsic matrices, and \(\) represents the capturing scene. In addition, as the camera parameters of both source and target views are represented by matrices, there should exist linear transformations of camera parameters between two views as follow,

\[_{t}=_{}_{s},[_{t },_{t}]=_{}[_{s}, _{s}]\] (3)

where \(_{}\) and \(_{}\) are the transformation matrices.

_Remark 1: The Geometric Transformation Between Camera Views._ From Eqn. (2) and Eqn. (3), we argue that there should exist a geometric transformation \(\) of images between two camera views as: \(}_{t}=(_{s};_{}, _{})\).

_Remark 2: The Equivalent Transformation Between Image and Segmentation Output._ As RGB images and segmentation maps are pixel-wised corresponding, the same geometric transformation \(\) in the image space can be adopted for segmentation space as: \(}_{t}=(_{s};_{}, _{})\)

Remarks 1-2 have depicted that the geometric transformation of both image and segmentation from the source to the target view can be represented by the shared transformation \(\) with the camera transformation matrices \(_{},_{}\). Let \(_{x}(_{s},}_{t})\) and \(_{y}(_{s},}_{t})\)_be the metrics the measure the cross-view structures changes_ of images and segmentation maps from the source to target domains.

We argue that the cross-view geometric correlation in the image space, i.e., \(_{x}(_{s},}_{t})\), is theoretically proportional to the one in the segmentation space, i.e., \(_{y}(_{s},}_{t})\). Since the camera transformations between the two views are linear (Eqn. (3)) and the images \(\) and outputs \(\) are pixel-wised corresponding, we hypothesize that the cross-view geometric correlation in the image space \(_{x}(_{s},}_{t})\) and the segmentation space \(_{y}(_{s},}_{t})\) can be modeled by a linear relation with linear scale \(\) as follows:

\[_{x}(_{s},}_{t})_{y}( _{s},}_{t})_{x}( _{s},}_{t})=_{y}(_{s},}_{t})\] (4)

### Cross-view Geometric Learning on Unpaired Data

Eqn. (4) defines a necessary condition to explicitly model the cross-view geometric correlation. Therefore, cross-view adaptation learning in Eqn. (1) can be re-formed as follows:

\[^{*}=_{}_{_{s},_{s}, }_{s}}_{Mask}(_{s},_{s},}_{s})+_{_{s},_{s},}_{t}, }_{t}}||_{x}(_{s},}_{t})- _{y}(_{s},}_{t})||\] (5)where, \(_{Adapt}(_{s},}_{t})=||_{x}( _{s},}_{t})-_{y}(_{s}, }_{t})||\) is the cross-view geocentric adaptation loss, \(||||\) is the mean squared error loss. However, in practice, the pair data between source and target views are inaccessible as data from these two views are often collected independently. Thus, optimizing Eqn. (5) without cross-view pairs of data remains an ill-posed problem. To address this limitation, instead of learning Eqn. (5) on paired data, we proposed to model this correlation on unpaired data. Instead of solving the cross-view geometric constraint of Eqn. (5) on pair data, let us consider all cross-view unpaired samples \((_{s},_{t})\). Formally, learning the _Cross-view Geometric Constraint_ between unpaired samples can be formulated as in Eqn. (6).

\[^{*}=_{}[_{_{s},} _{s}}_{Mask}(_{s},_{s},}_{s})+ _{_{s},_{s},_{t},_{t}}|| _{x}(_{s},_{t})-_{y}(_{s},_{t})||]\] (6)

where \(_{s}\) and \(_{t}\) are unpaired data, and \(_{Adapt}(_{s},_{t})=||_{x}( _{s},_{t})-_{y}(_{s}, _{t})||\) is the _Cross-view Geometric Adaptation_ loss on unpaired data. Intuitively, although the cross-view pair samples are not available, the cross-view geometric constraints on paired samples between two views can be indirectly imposed by modeling the cross-view geometric structural constraint among unpaired samples. Then, by modeling the cross-view structural changes in the image and segmentation spaces, the structural change on images of unpaired data could be considered as the reference for the cross-view structural change in the segmentation space during the optimization process. This action promotes the structures of segmentation that can be effectively adapted from the source view to the target view. Importantly, the cross-view geometric constraint imposed on unpaired data can be mathematically proved as an upper bound of the cross-view constraint on paired data as follows:

\[||_{x}(_{s},}_{t})-_{y}( _{s},}_{t})||=(_{x}(|| _{s},_{t})-_{y}(_{s}, _{t})||)\] (7)

where \(\) is the Big O notation. The upper bound in Eqn. (19) can be proved by using the properties of triangle inequality and our correlation metrics \(_{x}\) and \(_{y}\) (Sec. 3.3). The detailed proof is provided in the appendix. Eqn. (19) has illustrated that by minimizing the cross-view geometric constraint on unpaired samples in Eqn. (6), the cross-view constraint on paired samples in Eqn. (5) is also maintained due to the upper bound. Therefore, our proposed Cross-view Geometric Constraint loss _does NOT require the pair data between source and target views_ during training. Fig. 3 illustrates our cross-view adaptation learning framework.

### Cross-view Structural Change Modeling via Geodesic Flow Path

Modeling the correlation metrics \(_{x}\) and \(_{y}\) is an important task in our approach. Indeed, the metrics should be able to model the structure changes from the source to the target view. Intuitively, the changes from the source to the target view are essentially the geodesic flow between two subspaces on the Grassmann manifold. Then, the images (or segmentation) of two views can be projected along the geodesic flow path to capture the cross-view structural changes. Therefore, to model \(_{x}\) and \(_{y}\), we adopt the _Geodesic Flow_ path to measure the cross-view structural changes by modeling the geometry in the latent space.

**Remark 3: Grassmann Manifold** is the set of \(N\)-dimensional linear subspaces of \(^{D}(0<N<D)\), i.e, \((N,D)\). A matrix with orthonormal columns \(^{D N}\) define a subspace of \((N,D)\), i.e., \((N,D)^{}=_{N}\) where \(_{N}\) is the \(N N\) identity matrix.

For simplicity, we present our approach to model the cross-view structural change \(_{x}\) in the image space. Formally, let \(_{s}\) and \(_{t}\) be the basis of the source and target domains. These bases can be obtained by the PCA algorithm. The geodesic flow between \(_{s}\) and \(_{t}\) in the manifold can be defined via the function \(:[0..1]()\), where \(()(N,D)\) is the subspace lying on

Figure 3: **Our Cross-View Learning Framework.**

the geodesic flow path from the source to the target view:

\[()=[_{s}\ \ \ ][_{1}()\ \ \ -\ _{2}()]^{}\] (8)

where \(^{D(D-N)}\) is the orthogonal complement of \(_{s}\), i.e., \(^{}_{s}=\). \(()\) and \(()\) are the diagonal matrices whose diagonal element at row \(i\) can be defined as \(_{i}=(_{i})\) and \(_{i}=(_{i})\). The list of \(_{i}\) is the principal angles between source and target subspaces, i.e., \(0_{1}..._{N}\). \(_{1}\) and \(_{2}\) are the orthonormal matrices obtained by the following pair of SVDs:

\[_{s}^{}_{T}=_{1}(1)^{ }\ \ \ \ \ \ ^{}_{T}=-_{2}(1)^{}\] (9)

Since \(_{s}^{}_{t}\) and \(^{}_{t}\) share the same singular vectors \(\), we adopt the generalized Singular Value Decomposition (SVD) [18; 47] to decompose the matrices. In our approach, we model the cross-view structural changes \(_{x}\) by modeling the cosine similarity between projections along the geodesic flow \(()\). In particular, given a subspace \(()\) on the geodesic flow path from the source to the target view, the cross-view geometric correlation of images between the source and target views can formulated by the inner product \(g_{()}(_{s},_{t})\) along the geodesic flow \(()\) as follows:

\[g(_{s},_{t})=_{0}^{1}g_{()}( _{s},_{t})d=_{0}^{1}_{s}^{}( )()^{}_{t}d=_{s}^{}(_{0 }^{1}()()^{}d)_{t}=_{s }^{}_{t}\] (10)

where \(=_{0}^{1}()()^{}d\). Intuitively, the matrix \(\) represents the manifold structure between the source to the target view. Then, Eqn. (10) measures the cross-view structural changes between the source and the target domain based on their manifold structures. The matrix \(\) can be obtained in a closed form [18; 47] as follows:

\[=[_{s}_{1}\ \ \ \ _{2}] _{1}&_{2}\\ _{2}&_{3} _{1}^{}_{s}^{}\\ _{2}^{}^{}\] (11)

where \(_{1}\), \(_{2}\), and \(_{3}\) are the diagonal matrices, whose diagonal elements at row \(i\) can be defined as:

\[_{1,i}=1+)}{2_{i}},\ _{2,i}=)-1}{2_{i}},\ _{3,i}=1-)}{2_{i}}\] (12)

In practice, we model the cross-view structural changes \(_{x}\) via the cosine similarity along the geodesic flows. Finally, the cross-view structural changes \(_{x}\) can be formulate as:

\[_{x}(_{s},_{t})=1-_{s}^{} _{t}}{||^{1/2}_{s}||||^{1/ 2}_{t}||}\] (13)

Similarly, we can model the cross-view geometric correlation of segmentation \(_{y}\) via Geodesic Flow.

### View-Condition Prompting to Cross-View Learning

**View-Condition Prompting** Previous efforts [40; 38; 16; 73] in open-vocab segmentation have shown that a better prompting mechanism can provide more meaningful textual and visual knowledge. Prior work in open-vocab segmentation designed the prompt via the class names [64; 13; 38], e.g., "\(_{1}\), \(_{1}\),..., \(_{K}\)". Meanwhile, other methods improve the prompting mechanism by introducing the learnable variables into the prompt  or adding the task information . This action helps to improve the context learning of the vision-language model. In our approach, we also exploit the effectiveness of designing prompting to cross-view learning. In particular, describing the view information can further improve the visual context learning, e.g., "\(_{1}\), \(_{1}\),..., \(_{K}\) captured from the [domain] view", where [domain] could be car (source domain) or drone (target domain). Therefore, we introduce a view-condition prompting mechanism by introducing the view information, i.e., captured from the [domain] view", into the prompt. Our view-condition prompt offers the context specific to visual learning, thus providing better transferability in cross-view segmentation.

**Cross-view Correlation of View-Condition Prompts** We hypothesize that the correlation of the input prompts across domains also provides the cross-view geometric correlation in their deep representations. In particular, let \(_{s}^{p}\) and \(_{t}^{p}\) be the deep textual embeddings of view-condition prompts \(_{s}\) and \(_{t}\), and \(_{p}\) be metric measuring the correlation between \(_{s}^{p}\) and \(_{t}^{p}\). In addition, since the textual encoder has been pre-trained on large-scale vision-language data [39; 27], the visual and the textual representations have been well aligned. Then, we argue that the correlation of textual feature representations across views, i.e., \(_{p}(_{s}^{p},_{t}^{p})\), also provides the cross-view geometric correlation due to the embedded view information in the deep representation of prompts aligned with visual representations. Therefore, similar to Eqn. (4), we hypothesize the cross-view correlation of segmentation masks and textual features can be modeled as a linear relation with a scale factor \(\) as:

\[_{p}(_{s}^{p},_{t}^{p})_{y}( _{s},_{t})_{p}(_{s}^{ p},_{t}^{p})=_{y}(_{s},_{t})\] (14)

Then, learning the cross-view adaptation with view-condition prompts can be formulated as follows:

\[^{*}=_{}[_{_{s},_{s},_{s}}_{Mask}(_{s},_{s})+ _{_{s},_{s},_{t},_{t}}( _{I}||_{x}(_{s},_{t})- _{y}(_{s},_{t})+_{P}||_{p}(_{ s}^{p},_{t}^{p})-_{y}(_{s},_{t})|)]\] (15)

where \(_{I}\) and \(_{P}\) are the balanced-weight of losses. Similar to metrics \(_{x}\) and \(_{y}\), we also adopt the geodesic flow path to model the cross-view correlation metric \(_{p}\).

## 4 Experiments

### Datasets, Benchmarks, and Implementation

To efficiently evaluate cross-view adaptation, the cross-view benchmarks are set up from the car to the drone view. Following common practices in UDA [23; 58], we choose SYNTHIA , GTA , and BDD100K  as the source domains while UAVID  is chosen as the target domain. We chose to adopt these datasets because they share a class of interests and are commonly used in UDA and segmentation benchmarks [23; 61].

**SYNTHIA \(\) UAVID Benchmark** SYNTHIA and UAVID share five classes of interest, i.e., Road, Building, Car, Tree, and Person. Since the UAVID dataset annotated cars, trucks, and buses as a class of Car, we collapse these classes in SYNTHIA into a single class of Car.

**GTA \(\) UAVID Benchmark** consists of five classes in the SYNTHIA \(\) UAVID benchmark and includes one more class of Terrain. Therefore, the GTA \(\) UAVID benchmark has six classes of interest, i.e., Road, Building, Car, Tree, Terrain, and Person.

**BDD \(\) UAVID Benchmark** is a real-to-real cross-view adaptation setting. Similar to GTA \(\) UAVID benchmark, there are six classes of interest between BDD100K and UAVID. In our experiments, we adopt the mean Intersection over Union (mIoU) metric to measure the performance.

**Implementation** We adopt Mask2Former  (ResNet 101) with Semantic Context Interaction of FreeSeg  and pre-trained text encoder of CLIP  for our open-vocab segmentation networks. Our balanced weights of losses are set to \(_{I}=1.0\) and \(_{P}=0.5\). Further details of our networks and hyper-parameters are provided in the appendix.

### Ablation Study

**Effectiveness of Cross-view Adaptation and Prompting Mechanisms** Table 1 analyzes the effectiveness of prompting mechanisms, i.e., i.e., with and without _Prompting_, with and without _Cross-view Adaptation_ (in Eqn. (6)), with and without _View-Condition Prompting_ (in Eqn. (15)). For supervised results, we train two different models on UAVID with and without the Terrain class on two benchmarks. As in Table 1, the cross-view adaptation loss in Eqn. (6) significantly improve the performance of segmentation models. With prompting and cross-view adaptation,

   &  &  \\  Network & Metric & Road & Building & Car & Tree & Terrain & Person & mIoU \\  \(X\) & \(X\) & \(X\) & 8.1 & 19.1 & 7.4 & 30.3 & 1.3 & 13.2 & 7.5 & 13.0 & 27.7 & 26.8 & 26.6 & 1.0 & 12.9 \\ \(X\) & \(Y\) & \(X\) & **31.4** & **75.1** & **57.5** & **59.2** & **19.5** & **48.6** & **22.9** & **64.6** & **37.8** & **52.8** & **48.5** & **13.8** & **40.1** \\   Supervised & & 75.8 & 91.6 & 79.1 & 77.4 & 24.1 & 73.2 & 76.8 & 91.8 & 81.1 & 17.6 & 62.8 & 43.4 & 73.0 \\  ✓ & ✗ & \(X\) & 15.7 & 27.8 & 15.7 & 34.1 & 7.7 & 20.2 & 16.6 & 26.8 & 7.2 & 30.0 & 21.7 & 6.0 & 18.1 \\ ✓ & ✓ & ✗ & 36.8 & 75.5 & 61.3 & 60.8 & 21.2 & 51.1 & 27.3 & 66.8 & 42.3 & 55.5 & 47.1 & 25.1 & 44.0 \\ ✓ & ✓ & ✓ & \(X\) & **38.4** & **76.1** & **62.8** & **62.1** & **21.8** & **52.2** & **29.2** & **67.1** & **45.2** & **56.6** & **48.5** & **27.9** & **45.7** \\   & Supervised & 79.8 & 92.6 & 82.9 & 79.1 & 48.0 & 76.5 & 80.5 & 93.3 & 82.7 & 79.2 & 71.3 & 49.9 & 76.1 \\   

Table 1: Effectiveness of Our Cross-view Adaptation Losses and Prompting Mechanism.

   &  &  \\  Network & Metric & Road & Building & Car & Tree & Terrain & Person & mIoU \\   & Euclidean & 23.7 & 31.2 & 33.2 & 36.7 & - & 11.5 & 27.2 \\  & Geodesic & **38.4** & **76.1** & **62.8** & **62.1** & **-** & **21.8** & **52.2** \\   & Euclidean & 24.7 & 31.9 & 41.2 & 39.7 & - & 14.1 & 30.3 \\  & Geodesic & **40.8** & **76.4** & **65.8** & **62.7** & **-** & **27.9** & **54.7** \\   & Euclidean & 21.7 & 30.0 & 26.2 & 39.7 & 31.7 & 9.5 & 26.5 \\  & Geodesic & **29.2** & **67.1** & **45.2** & **56.6** & **48.5** & **27.9** & **45.7** \\   & Euclidean & 24.3 & 33.7 & 28.5 & 40.1 & 32.8 & 9.7 & 28.2 \\  & Geodesic & **31.0** & **67.1** & **46.8** & **56.9** & **48.7** & **31.9** & **47.1** \\  

Table 2: Effectiveness of Backbones and Cross-view Metrics.

[MISSING_PAGE_FAIL:8]

without adaptation, with cross-view adaptation, and with view-condition prompting. As shown in the results, our cross-view adaptation can efficiently model the segmentation of the view. By using the view-condition prompting, our model can further improve the segmentation of persons and vehicles.

### Comparisons with Prior UDA Methods

**SYNTHIA \(\) UAVID** As shown in Table 4, our EAGLE has achieved SOTA results and outperforms prior view transformation (i.e., Polar Transform ) UDA methods by a large margin. For fair comparisons, we adopt the DeepLab  and DAFormer  for the segmentation network. In particular, our mIoU results using DeepLab and DAFormer are \(45.2\%\) and \(50.8\%\). In the DAFormer backbone, the mIoU results of our approach are higher than CROVIA  and MIC  by \(+4.8\%\) and \(+9.0\%\). The IoU result of each class also consistently outperformed the prior methods. Highlighted that although our approach does NOT use depth labels, our results still outperform the one using depths, i.e., DADA . It has emphasized that our approach is able to better capture the cross-view structural changes compared to prior methods. Figure 4 illustrates our qualitative results compared to ProDA  and CROVIA .

**GTA \(\) UAVID** As shown in Table 4, our effectiveness outperforms prior polar view transformation  and domain adaptation approaches when measured by both mIoU performance and the IoU accuracy of each class. In particular, our mIoU performance using DeepLab and DAFormer network achieves \(36.7\%\) and \(40.7\%\), respectively. Our results have substantially closed the performance gap with the supervised results. By using the better segmentation-based network, i.e., Mask2Former with ResNet, the performance of our approach is further improved to \(40.1\%\) compared to DeepLab.

### Comparisons with Open-vocab Segmentation

We compare EAGLE with the prior open-vocab segmentation methods, i.e., DenseCLIP  and an adaptive prompting FreeSeg  with four settings, i.e., Source Only, with AdvEnt , and with SAC , and our Cross-View Adaptation in Eqn. (6) (without view-condition).

**Open-vocab Semantic Segmentation** As in Table 5, the mIoU performance of our proposed approach with cross-view adaptation outperforms prior DenseCLIP by a large margin on SYNTHIA \(\) UAVID. By using our cross-view geometric adaptation loss, the performance of DenseCLIP and FreeSeg is further enhanced, i.e., higher than DenseCLIP and FreeSeg with SAC by +3.7% and +5.0%. While FreeSeg  with our cross-view adaptation slightly outperforms EAGLE due to its adaptive prompting, our EAGLE approach with the better view-condition prompting achieves higher mIoU

   &  &  &  \\   & & Road & Building & \(\)T-Free & Person & Road & Building & \(\)T-Free & Person & mIoU \\   & AdvEnt  & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free \\  & DAFormer  & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free \\  & DAFormer  & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free \\  & DAFormer  & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \(\)T-Free & \

[MISSING_PAGE_FAIL:10]