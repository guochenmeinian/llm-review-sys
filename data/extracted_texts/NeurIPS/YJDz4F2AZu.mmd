# ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling

Yuqi Chen1,2, Kan Ren2, Yansen Wang2, Yuchen Fang2,3, Weiwei Sun1, Dongsheng Li2

1 School of Computer Science & Shanghai Key Laboratory of Data Science, Fudan University
2 Microsoft Research Asia, 3 Shanghai Jiao Tong University

The work was conducted during the internship of Yuqi Chen and Yuchen Fang at Microsoft Research.Correspondence to Kan Ren, contact: rk.ren@outlook.com.

###### Abstract

Modeling continuous-time dynamics on irregular time series is critical to account for data evolution and correlations that occur continuously. Traditional methods including recurrent neural networks or Transformer models leverage inductive bias via powerful neural architectures to capture complex patterns. However, due to their discrete characteristic, they have limitations in generalizing to continuous-time data paradigms. Though neural ordinary differential equations (Neural ODEs) and their variants have shown promising results in dealing with irregular time series, they often fail to capture the intricate correlations within these sequences. It is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. To tackle this problem, we propose ContiFormer that extends the relation modeling of vanilla Transformer to the continuous-time domain, which explicitly incorporates the modeling abilities of continuous dynamics of Neural ODEs with the attention mechanism of Transformers. We mathematically characterize the expressive power of ContiFormer and illustrate that, by curated designs of function hypothesis, many Transformer variants specialized in irregular time series modeling can be covered as a special case of ContiFormer. A wide range of experiments on both synthetic and real-world datasets have illustrated the superior modeling capacities and prediction performance of ContiFormer on irregular time series data. The project link is https://seqml.github.io/contiformer/.

## 1 Introduction

Irregular time series are prevalent in real-world applications like disease prevention, financial decision-making, and earthquake prediction . Their distinctive properties set them apart from regular time series data. First, irregular time series data are characterized by irregularly generated or non-uniformly sampled observations with variable time intervals, as well as missing data due to technical issues, or data quality problems, which pose challenges for traditional time series analysis techniques . Second, even though the observations are irregularly sampled, the underlying data-generating process is assumed to be continuous . Third, the relationships among the observations can be intricate and continuously evolving. All these characteristics require elaborate modeling approaches for better understanding these data and making accurate predictions.

The continuity and intricate dependency of these data samples pose significant challenges for model design. Simply dividing the timeline into equally sized intervals can severely damage the continuity of the data . Recent works have suggested that underlying continuous-time process is appreciated for irregular time series modeling , which requires the modeling procedure to capturethe continuous dynamic of the system. Furthermore, due to the continuous nature of data flow, we argue that the correlation within the observed data is also constantly changing over time. For instance, stock prices of tech giants (e.g., MSFT and GOOG) show consistent evolving trends, yet they are affected by short-term events (e.g., large language model releases) and long-term factors.

To tackle these challenges, researchers have pursued two main branches of solutions. Neural ordinary differential equations (Neural ODEs) and state space models (SSMs) have illustrated promising abilities for capturing the dynamic change of the system over time [9; 19; 28; 49]. However, these methods overlook the intricate relationship between observations and their recursive nature can lead to cumulative errors if the number of iterations is numerous . Another line of research capitalizes on the powerful inductive bias of neural networks, such as various recurrent neural networks [6; 39; 45] and Transformer models [38; 48; 54; 60]. However, their use of fixed-time encoding or learning upon certain kernel functions fails to capture the complicated input-dependent dynamic systems.

Based on the above analysis, modeling the relationship between observations while capturing the temporal dynamics is a challenging task. To address this problem, we propose a Continuous-Time Transformer, namely _ContiFormer_, which incorporates the modeling abilities of continuous dynamics of Neural ODEs within the attention mechanism of Transformers and breaks the discrete nature of Transformer models. Specifically, to capture the dynamics of the observations, ContiFormer begins by defining latent trajectories for each observation in the given irregularly sampled data points. Next, to capture the intricate yet continuously evolving relationship, it extends the discrete dot-product in Transformers to a continuous-time domain, where attention is calculated between continuous dynamics. With the proposed attention mechanism and Transformer architecture, ContiFormer effectively models complex continuous-time dynamic systems.

The contributions of this paper can be summarized below.

* _Continuous-Time Transformer._ To the best of our knowledge, we are the first to incorporate a continuous-time mechanism into attention calculation in Transformer, which is novel and captures the continuity of the underlying system of the irregularly sampled time-series data.
* _Parallelism Modeling._ To tackle the conflicts between continuous-time calculation and the parallel calculation property of the Transformer model, we propose a novel reparameterization method, allowing us to parallelly execute the continuous-time attention in the different time ranges.
* _Theoretical Analysis._ We mathematically characterize that various Transformer variants [34; 38; 48; 54; 60] can be viewed as special instances of ContiFormer. Thus, our approach offers a broader scope that encompasses Transformer variants.
* _Experiment Results._ We examine our method on various irregular time series settings, including time-series interpolation, classification, and prediction. The extensive experimental results have illustrated the superior performance of our method against strong baseline models.

## 2 Related Work

Time-Discretized Models.There exists a branch of models based on the time-discrete assumption which transforms the time-space into a discrete one. Recurrent Neural Networks (RNN) [14; 24] and Transformers  are powerful time-discrete sequence models, which have achieved great success in natural language processing [15; 37; 58], computer vision [20; 63] and time series forecasting [31; 35; 52; 53; 61; 62]. Utilizing them for irregular time series data necessitates discretizing the timeline into time bins or padding missing values, potentially resulting in information loss and disregarding inter-observation dynamics. [13; 36; 41]. An alternative is to construct models that can utilize these various time intervals [34; 60]. Additionally, several recent approaches have also encoded time information into features to model irregularly sampled time series, including time representation approaches [54; 60] and kernel-based methods , which allow learning for time dynamics. For instance, Mercer  concatenates the event embedding with the time representation. Despite these advances, these models are still insufficient in capturing input-dependent dynamic systems.

Continuous-Time Models.Other works shift to the continuous-time modeling paradigm. To overcome the discrete nature of RNNs, a different strategy involves the exponential decay of the hidden state between observations [6; 39]. Nevertheless, the applicability of these techniques is restricted to decaying dynamics specified between successive observations . Neural ODEs are a class of models that leverage the theory of ordinary differential equations to define the dynamics of neural networks [9; 27]. Neural CDE  and Neural RDE  further utilize the well-defined mathematics of controlled differential equations [2; 29] and rough path theory  to construct continuous-time RNN. ODE-RNN  combines Neural ODE with Gated Recurrent Unit (GRU)  to model continuous-time dynamics and input changes. Whereas these approaches usually fail to capture the evolving relationship between observations . CADN  and ANCDE  extend ODE-RNN and Neural CDE by incorporating an attention mechanism to capture the temporal relationships, respectively. Another line of research explores the use of state space models (SSMs) to represent continuous-time dynamic systems [1; 18; 19; 49]. However, these models adopt a recursive schema, which hampers its efficiency and can lead to cumulative errors . In contrast, our proposed model, ContiFormer leverages the parallelism advantages of the Transformer architecture and employs a non-autoregressive paradigm. Furthermore, extending the Transformer to the continuous-time domain remains largely unexplored by researchers. Such extension provides a more flexible and powerful modeling ability for irregular time series data.

## 3 Method

The ContiFormer is a Transformer  model for processing time series data with irregular time intervals. Formally, an irregular time series is defined as \(=[(X_{1},t_{1}),,(X_{N},t_{N})]\), where observations may occur at any time and the observation time points \(=(t_{1},,t_{N})\) are with irregular intervals. \(X_{i}\) is the input feature for the \(i\)-th observation. We denote \(X=[X_{1};X_{2};...,X_{N}]^{N d}\).

As aforementioned, it is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. To tackle the challenge, we propose a Continuous-Time Transformer architecture as shown in Figure 1. Each layer of ContiFormer takes as input an irregular time series \(X\) and the sampled time \(\) and outputs a latent continuous trajectory that captures the dynamic change of the underlying system. With such a design, ContiFormer transforms the discrete observation sequence into the continuous-time domain. At each layer, the core attention module takes a continuous perspective and expands the dot-product operation in the vanilla Transformer to the continuous-time domain, which not only models the underlying continuous dynamics but also captures the evolving input-dependent process. For better understanding, the description of each layer will omit the layer index, without causing confusion.

Throughout the remaining section, we adopt the notation \(\) to denote a sequence of (reference) time points, while \(t\) is the random variable representing a query time point. Matrices are donated using uppercase letters, and lowercase letters except \(t\) represent continuous functions.

### Continuous-Time Attention Mechanism

The core of the ContiFormer layer is the proposed continuous-time multi-head attention (CT-MHA) module, as shown in Figure 1. At each layer, we first transform the input irregular time series \(X\) into \(Q=[Q_{1};Q_{2};;Q_{N}]\) for queries, \(K=[K_{1};K_{2};;K_{N}]\) for keys, and \(V=[V_{1};V_{2};;V_{N}]\) for values. At a high level, the CT-MHA module transforms the irregular time series inputs to latent trajectories and outputs a continuous dynamic system that captures the time-evolving relationship between observations. To accomplish this, it utilizes ordinary differential equations to define the latent trajectories for each observation. Within the latent state, it assumes that the underlying dynamics evolve following linear ODEs. Subsequently, it constructs a continuous query function by approximating the underlying sample process of the input. Ultimately, it produces a continuous-time function that captures the evolving relationship and represents the complex dynamic system.

Continuous Dynamics from ObservationsTransformer calculates scaled dot-product attention for queries, keys, and values . In continuous form, we first employ ordinary differential equations to define the latent trajectories for each observation. Specifically, assuming that the first observation and last observation come at time point \(t_{1}\) and \(t_{N}\) respectively, we define continuous keys and values as

\[_{i}(t_{i})&=K_{i}\;, _{i}(t)=_{i}(t_{i})+_{t_{i}}^{t}f(,_ {i}();_{k})\;,\\ _{i}(t_{i})&=V_{i}\;,_{i}(t )=_{i}(t_{i})+_{t_{i}}^{t}f(,_{i}(); _{v})\;,\] (1)where \(t[t_{1},t_{N}]\), \(_{i}(),_{i}()^{d}\) represent the ordinary differential equation for the \(i\)-th observation with parameters \(_{k}\) and \(_{v}\), and the initial state of \(_{i}(t_{i})\) and \(_{i}(t_{i})\) respectively and the function \(f()^{d+1}^{d}\) controls the change of the dynamics.

Query FunctionWhile keys and values are associated with the input and output of the attention mechanism, a query specifies what information to extract from the input. To model a dynamic system, queries can be modeled as a function of time that represents the overall changes in the input to the dynamic system at a specific time point \(t\), i.e., \((t)\). Specifically, we adopt a common assumption that irregular time series is a discretization of an underlying continuous-time process. Thus, similar to , we define a closed-form continuous-time interpolation function (e.g., natural cubic spline) with knots at \(t_{1},,t_{N}\) such that \((t_{i})=Q_{i}\) as an approximation of the underlying process.

Scaled Dot ProductThe self-attention mechanism is the key component in Transformer architecture. At its core, self-attention involves calculating the correlation between queries and keys. This is achieved through the inner product of two matrices in discrete form, i.e., \( K^{}\). Extending the discrete inner-product to its continuous-time domain, given two real functions \(f(x)\) and \(g(x)\), we define the inner product of two functions in a closed interval \([a,b]\) as

\[ f,g=_{a}^{b}f(x) g(x)x\.\] (2)

Intuitively, it can be thought of as a way of quantifying how much the two functions "align" with each other over the interval. Inspired by the formulation of the inner product in the continuous-time domain, we model the evolving relationship between the \(i\)-th sample and the dynamic system at time point \(t\) as the inner product of \(\) and \(_{i}\) in a closed interval \([t_{i},t]\), i.e.,

\[_{i}(t)=}^{t}() _{i}()^{}}{t-t_{i}}\.\] (3)

Due to the nature of sequence data, it is common to encounter abnormal points, such as events with significantly large time differences. To avoid numeric instability during training, we divide the

Figure 1: Architecture of the ContiFormer layer. ContiFormer takes an irregular time series and its corresponding sampled time points as input. Queries, keys, and values are obtained in continuous-time form. The attention mechanism (CT-MHA) performs a scaled inner product in a continuous-time manner to capture the evolving relationship between observations, resulting in a complex continuous dynamic system. Feedforward and layer normalization are adopted, similar to the Transformer. Finally, a sampling trick is employed to make ContiFormer stackable. Note that the highlighted trajectories in purple indicate the part of functions that are involved in the calculation of the output.

integrated solution by the time difference. As a consequence, Eq. (3) exhibits discontinuity at \(_{i}(t_{i})\). To ensure the continuity of the function \(_{i}()\), we define \(_{i}(t_{i})\) as

\[_{i}(t_{i})=_{ 0}}^{t_{i}+} ()_{i}()^{}}{}=(t_{i})_{i}(t_{i})^{}\;.\] (4)

Expected ValuesGiven a query time \(t[t_{1},t_{N}]\), the value of an observation at time point \(t\) is defined as the expected value from \(t_{i}\) to \(t\). Without loss of generality, it holds that the expected values of an observation sampled at time \(t_{i}\) is \(_{i}(t_{i})\) or \(V_{i}\). Formally, the expected value is defined as

\[}_{i}(t)=_{t[t_{i},t]}[_{i}(t )]=}^{t}_{i}()}{t-t_{i}}\;.\] (5)

Multi-Head AttentionThe continuous-time attention mechanism is a powerful tool used in machine learning that allows for the modeling of complex, time-varying relationships between keys, queries, and values. Unlike traditional attention mechanisms that operate on discrete time steps, continuous-time attention allows for a more fine-grained analysis of data by modeling the input as a continuous function of time. Specifically, given the forehead-defined queries, keys, and values in continuous-time space, the continuous-time attention given a query time \(t\) can be formally defined as

\[(Q,K,V,)(t) =_{i=1}^{N}}_{i}(t)}_{i}(t)\;,\] (6) \[}_{i}(t) =_{i}(t)/})}{_{j =1}^{N}(_{j}(t)/})}\;.\]

Multi-head attention, an extension of the attention mechanism , allows simultaneous focus on different input aspects. It stabilizes training by reducing attention weight variance. We extend Eq. (6) by incorporating multiple sets of attention weights, i.e.,

\[(Q,K,V,)(t) =(_{(1)}(t),, _{()}(t))W^{O}\;,\] (7) \[_{()}(t) =(QW_{()}^{Q},KW_{()}^{K},VW_ {()}^{V},)(t)\;,\]

where \(W^{O}\), \(W_{()}^{Q}\), \(W_{()}^{K}\) and \(W_{()}^{V}\) are parameter matrices, \([1,]\) and \(\) is the head number.

### Continuous-Time Transformer

Despite the widespread adoption of Transformers  in various research fields, their extension for modeling continuous-time dynamic systems is underexplored. We propose ContiFormer that directly builds upon the original implementation of vanilla Transformer while extending it to the continuous-time domain. Specifically, we apply layer normalization (LN) after both the multi-head self-attention (MHA) and the feed-forward blocks (FFN). We formally characterize the ContiFormer layer below

\[}^{l}(t) =((X^{l},X^{l},X^{l},^{l})( t)+^{l}(t)),\] (8) \[^{l}(t) =((}^{l}(t))+}^{l}(t)),\]

where \(^{l}(t)\) is the output from the \(l\)-th ContiFormer layer at time point \(t\). Additionally, we adopt residual connection to avoid potential gradient vanishing . To incorporate the residual connection with the continuous-time output, we approximate the underlying process of the discrete input \(X^{l}\) using a continuous function \(^{}(t)\) based on the closed-form continuous-time interpolation function.

Sampling ProcessAs described before, each ContiFormer layer derives a continuous function \(^{l}(t)\) w.r.t. time as the output, while receiving discrete sequence \(X^{l}\) as input. However, \(^{l}(t)\) can not be directly incorporated into neural network architectures that expect inputs in the form of fixed-dimensional vectors and sequences , which places obstacles when stacking layers of ContiFormer. To address this issue, we establish reference time points for the output of each layer. These points are used to discretize the layer output, and can correspond to either the input time points (i.e., \(\)) or task-specific time points. Specifically, assume that the reference points for the \(l\)-th layer is \(^{l}=[t_{1}^{l},t_{2}^{l},...,t_{_{l}}^{l}]\), the input to the next layer \(X^{l+1}\) can be sampled as \(\{^{l}(t_{j}^{l})|j[1,_{l}]\}\).

### Complexity Analysis

We consider an autoregressive task where the output of each observation is required for a particular classification or regression task. Therefore, the reference points for each layer are defined as \(^{}=\).

To preserve the parallelization of Transformer architecture and meanwhile implement the continuous-time attention mechanism in Eq. (6), we first adopt time variable ODE  to reparameterize ODEs into a single interval \([-1,1]\), followed by numerical approximation method to approximate the integrals, i.e.,

\[_{i}(t_{j})=}^{t_{j}}()_{i}()^{}}{t_{j}-t_{i}}=_{-1}^{1}}_{i,j}()}_{i,j}()^{} _{p=1}^{P}_{p}}_{i,j}(_{p}) }_{i,j}(_{p})^{}\;,\] (9)

where \(P\) denotes the number of intermediate steps to approximate an integral, \(_{p}[-1,1]\) and \(_{p} 0\), \(}_{i,j}(s)=_{i}((s(t_{j}-t_{i})+t_{i}+t_{j})/2)\), and \(}_{i,j}(s)=_{i}((s(t_{j}-t_{i})+t_{i}+t_{j})/2)\). Finally, \(_{i}(t_{j})\) can be solved with one invokes of ODESolver that contains \(N^{2}\) systems, i.e.,

\[}_{1,1}(_{p-1})\\ \\ }_{N,N}(_{p-1})}_{(_{p-1})}+_{ _{p-1}}^{_{p}}f(s,(s);_{k})ds= }_{1,1}(_{p})\\ \\ }_{N,N}(_{p})}_{(_{p})}\;,\] (10)

where \(p[1,...,P]\) and we further define \(_{0}=-1\). Additionally, \(}_{i,j}(_{p})\) can be obtained by interpolating on the close-form continuous-time query function. Similar approach is adopted for solving \(}_{i}(t_{j})\) in Eq. (5). The detailed implementation is deferred to Appendix A.

We consider three criteria to analyze the complexity of different models, i.e., per-layer complexity, minimum number of sequential operations, and maximum path lengths . Complexity per layer refers to the computational resources required to process within a single layer of a neural network. Sequential operation refers to the execution of operations that are processed iteratively or in a sequential manner. Maximum path length measures the longest distance information must traverse to establish relationships, reflecting the model's ability to learn long-range dependencies.

As noted in Table 1, vanilla Transformer is an efficient model for sequence learning with \(O(1)\) sequential operations and \(O(1)\) path length because of the parallel attention mechanism, whereas the recurrent nature of RNN and Neural ODE rely on autoregressive property and suffer from a large number of sequential operations. Utilizing Eq. (9) and leveraging the parallelism inherent in the Transformer architecture, our model, ContiFormer, achieves \(O(S)\) sequential operations and \(O(1)\) path length, while also enjoying the advantage of capturing complex continuous-time dynamic systems. We have \(S T\) and we generally set \(S<N\) in our experiments.

### Representation Power of ContiFormer

In the previous subsections, we introduce a novel framework for modeling dynamic systems. Then a natural question is: _How powerful is ContiFormer?_ We observe that by choosing proper weights, ContiFormer can be an extension of vanilla Transformer . Further, many Transformer variants tailored for irregular time series, including time embedding methods [48; 54] and kernelized attention methods [34; 38], can be special cases of our model. We provide an overview of the main theorem below and defer the proof to Appendix B.

**Theorem 1** (Universal Attention Approximation Theorem).: _Given query (\(Q\)) and key (\(K\)) matrices, such that \(\|Q_{i}\|_{2}<,\|Q_{i}\|_{0}=d\) for \(i[1,...,N]\). For certain attention matrix, i.e.,

  Model & Comp. per Layer & Seq. Op. & Max. Path Len. \\  Transformer  & \(O(N^{2} d)\) & \(O(1)\) & \(O(1)\) \\ RNN  & \(O(N d^{2})\) & \(O(N)\) & \(O(N)\) \\ Neural ODE  & \(O(T d^{2})\) & \(O(T)\) & \(O(T)\) \\  ContiFormer & \(O(N^{2} S d^{2})\) & \(O(S)\) & \(O(1)\) \\  

Table 1: Per-layer complexity (Comp. per Layer), minimum number of sequential operations (Seq. Op.), and maximum path lengths (Max. Path Len.). \(N\) is the sequence length, \(d\) is the representation dimension, \(T\) is the number of function evaluations (NFE) for the ODE solver in a single forward pass from \(t_{1}\)to \(t_{N}\), and \(S\) represents the NFE from \(-1\) to \(1\).

\((Q,K)^{N N}\) (see Appendix B.1 for more information), there always exists a family of continuously differentiable vector functions \(_{1}(),_{2}(),,_{N}()\), such that the discrete definition of the continuous-time attention formulation, i.e., \([_{1}(),_{2}(),..., {}_{N}()]\) in Eq. (4), given by_

\[}(Q,K)=[_{1}(t_{1})&_{2}(t_{1})&&_ {N}(t_{1})\\ _{1}(t_{2})&_{2}(t_{2})&& _{N}(t_{2})\\ &&&\\ _{1}(t_{N})&_{2}(t_{N})&& _{N}(t_{N})]\;,\] (11)

_satisfies that \(}(Q,K)=(Q,K)\)._

## 4 Experiments

In this section, we evaluate ContiFormer on three types of tasks on irregular time series data, i.e., interpolation and extrapolation, classification, event prediction, and forecasting. Additionally, we conduct experiments on pendulum regression task [19; 45], the results are listed in Appendix C.5.

Implementation DetailsBy default, we use the natural cubic spline to construct the continuous-time query function. The vector field in ODE is defined as \(f(t,)=((^{d,d}(^{d,d}()+^{1,d}(t))))\), where \(()\) is either _tanh_ or _sigmoid_ activation function, \(^{a,b}():^{a}^{b}\) is a linear transformation from dimension \(a\) to dimension \(b\), LN denotes the layer normalization. We adopt the Gauss-Legendre Quadrature approximation to implement Eq. (9). In the experiment, we choose the Runge-Kutta-4  (RK4) algorithm to solve the ODE with a fixed step of fourth order and a step size of \(0.1\). Thus, the number of forward passes to integrate from \(-1\) to \(1\), i.e., \(S\) in Table 1, is \(80\). All the experiments were carried out on a single 16GB NVIDIA Tesla V100 GPU.

### Modeling Continuous-Time Function

The first experiment studies the effectiveness of different models from different categories on continuous-time function approximation. We generated a dataset of 300 2-dimensional spirals, sampled at 150 equally-spaced time points. To generate irregular time points, we randomly sample 50 points from each sub-sampled trajectory without replacement. We visualize the interpolation and extrapolation results of the irregular time series for different models, namely Latent ODE (w/ RNN encoder) , Transformer  and our proposed ContiFormer. Besides, we list the interpolation and extrapolation results in Table 2 using rooted mean squared error (RMSE) and mean absolute error (MAE) metrics. More visualization results and experimental settings can be found in Appendix C.1. As shown in Figure 2, both Latent ODE and ContiFormer can output a smooth and continuous function approximation, while Transformer fails to interpolate it given the noisy observations (1). From Table 2, we can observe that our model outperforms both Transformer and Latent ODE by a large margin. The improvement lies in two

   Metric & Transformer  & Latent ODE  & **ContiFormer** \\   \))} \\  RMSE (\(\)) & 1.37 \(\) 0.14 & 2.09 \(\) 0.22 & **0.49 \(\) 0.06** \\ MAE (\(\)) & 1.42 \(\) 0.13 & 1.95 \(\) 0.25 & **0.52 \(\) 0.06** \\   \))} \\  RMSE (\(\)) & 1.36 \(\) 0.10 & 1.59 \(\) 0.05 & **0.64 \(\) 0.09** \\ MAE (\(\)) & 1.49 \(\) 0.12 & 1.52 \(\) 0.05 & **0.65 \(\) 0.08** \\  

Table 2: Interpolation and extrapolation results of different models on 2-dimensional spirals. \(\) indicates the lower the better.

Figure 2: Interpolation and extrapolation of spirals with irregularly-samples time points by Transformer, Neural ODE, and our model.

aspects. First, compared to Transformer, our ContiFormer can produce an almost3 continuous-time output, making it more suitable for modeling continuous-time functions. Second, compared to Latent ODE, our ContiFormer excels at retaining long-term information (\(}\)), leading to lower prediction error in extrapolating unseen time series. Conversely, Latent ODE is prone to cumulative errors (3), resulting in poorer performance in extrapolation tasks. Therefore, we conclude that ContiFormer is a more suitable approach for modeling continuous-time functions on irregular time series.

### Irregularly-Sampled Time Series Classification

The second experiment evaluates our model on real-world irregular time series data. To this end, we first examine the effectiveness of different models for irregular time series classification. We select 20 datasets from UEA Time Series Classification Archive  with diverse characteristics in terms of the number, dimensionality, and length of time series samples. More detailed information and experimental results can be found in C.2. To generate irregular time series data, we follow the setting of  to randomly drop either 30%, 50% or 70% observations.

We compare the performance of ContiFormer with RNN-based methods (GRU-\(\)t , GRU-D ), Neural ODE-based methods (ODE-RNN , CADN , Neural CDE ), SSM-based models (S5 ), and attention-based methods (TST , mTAN ).

Table 3 presents the average accuracy and rank number under different drop ratios. ContiFormer outperforms all the baselines on all three settings. The complete results are shown in Appendix C.2.3. Moreover, attention-based methods perform better than both RNN-based methods and ODE-based methods, underscoring the significance of effectively modeling the inter-correlation of the observations. Additionally, we find that our method is more robust than other models. ContiFormer exhibits the smallest performance gap between 30% and 50%.

Besides, we study the learned attention patterns with different models. Attention captures the impact of previous observations on the future state of a system by measuring the correlation between different observations at different time points. Consequently, it is expected that a model effectively capturing the underlying dynamics would exhibit continuous, smooth, and input-dependent intermediate outcomes. As illustrated in Figure 3, ContiFormer and TST are both capable of

   Mask Ratio &  &  &  \\  Metric & Avg. ACC. (\(\)) & Avg. Rank (\(\)) & Avg. ACC. (\(\)) & Avg. Rank (\(\)) & Avg. ACC. (\(\)) & Avg. Rank (\(\)) \\  GRU-D  & 0.7284 & 6 & 0.7117 & 5.8 & 0.6725 & 6.15 \\ GRU-\(\)t  & 0.7298 & 5.75 & 0.7157 & 5.65 & 0.6795 & 5.7 \\ ODE-RNN  & 0.7304 & 5.45 & 0.7000 & 5.55 & 0.6594 & 6.4 \\ Neural CDE  & 0.7142 & 6.85 & 0.6929 & 6.5 & 0.6753 & 6.3 \\ mTAN  & 0.7381 & 5.7 & 0.7118 & 5.85 & 0.6955 & 5.4 \\ CADN  & 0.7402 & 5.4 & 0.7211 & 5.55 & 0.7183 & 3.9 \\ S5  & 0.7854 & 4.4 & 0.7638 & 4.25 & 0.7401 & 4.45 \\ TST  & 0.8089 & 2.75 & 0.7793 & 3.15 & 0.7297 & 4.2 \\ 
**ContiFormer** & **0.8126** & **2.4** & **0.7997** & **1.9** & **0.7749** & **2.1** \\   

Table 3: Experimental results on irregular time series classification. Avg. ACC. stands for average accuracy over 20 datasets and Avg. Rank stands for average ranking over 20 datasets. \(\) (\(\)) indicates the higher (lower) the better.

Figure 3: Visualization of attention scores on UWaveGestureLibrary dataset. Colors indicate the attention scores for different instances at time \(t=0\). Observations at time \(t=0\) are observed and normalize the time interval to \(\).

capturing input-dependent patterns effectively, while mTAN struggles due to the fact that its attention mechanism depends solely on the time information. In addition, ContiFormer excels in capturing complex and smooth patterns compared to TST, showcasing its ability to model time-evolving correlations effectively and potentially more robust to noisy data. Consequently, we believe that ContiFormer better captures the nature of irregular time series.

### Predicting Irregular Event Sequences

Next, we evaluate different models for predicting the type and occurrence time of the next event with irregular event sequences [7; 21; 46; 47], a.k.a, marked temporal point process (MTPP) task. To this end, we use one synthetic dataset and five real-world datasets for evaluation, namely Synthetic, Neonate , Traffic , MIMIC , BookOrder  and StackOverflow . We use the 4-fold cross-validation scheme for Synthetic, Neonate, and Traffic datasets following , and the 5-fold cross-validation scheme for the other three datasets following [39; 64]. We repeat the experiment 3 times and report the mean and standard deviations in Table 4. More information about the task description, dataset information, and training algorithm can be found in Appendix C.3.

We first compare ContiFormer with 3 models on irregular time series, i.e., GRU-\(\)t, ODE-RNN, and mTAN, which respectively belong to RNN-based, ODE-based, and attention-based approaches. Additionally, we compare ContiFormer with baselines specifically designed for the MTPP task. These models include parametric method (HP ), RNN-based methods (RMTPP , NeuralHP ), and attention-based methods (SAHP , THP , NSTKA ).

The experimental results, as presented in Table 4, demonstrate the overall statistical superiority of ContiFormer to the compared baselines. Notably, the utilization of specific kernel functions, e.g., NSTKA and mTAN, results in subpar performance on certain datasets, highlighting their limitations in modeling complex data patterns in real-world scenarios. Furthermore, parametric methods like HP, perform poorly on most datasets, indicating their drawbacks in capturing real-life complexities.

   Model & Metric & Synthetic & Neonate & Traffic & MIMIC & StackOverflow & BookOrder \\   & LL (\(\)) & -3.084 \(\) 0.05 & -4.618 \(\) 0.05 & -1.482 \(\) 0.005 & -4.618 \(\) 0.005 & -5.794 \(\) 0.005 & -1.036 \(\) 0.000 \\  & Accuracy (\(\)) & 0.756 \(\) 0.000 & – & 0.570 \(\) 0.000 & 0.795 \(\) 0.000 & 0.441 \(\) 0.000 & 0.604 \(\) 0.000 \\  & RMSE (\(\)) & 0.953 \(\) 0.000 & 10.957 \(\) 0.012 & 0.407 \(\) 0.000 & 1.021 \(\) 0.000 & 1.341 \(\) 0.000 & 3.781 \(\) 0.000 \\   & LL (\(\)) & -1.025 \(\) 0.030 & -2.817 \(\) 0.023 & -0.546 \(\) 0.012 & -1.184 \(\) 0.023 & -2.374 \(\) 0.001 & -0.952 \(\) 0.007 \\  & Accuracy (\(\)) & 0.841 \(\) 0.000 & – & 0.805 \(\) 0.002 & 0.823 \(\) 0.004 & 0.461 \(\) 0.000 & 0.624 \(\) 0.000 \\  & RMSE (\(\)) & 0.369 \(\) 0.14 & 9.517 \(\) 0.023 & 0.337 \(\) 0.001 & 0.864 \(\) 0.017 & 0.955 \(\) 0.000 & 3.647 \(\) 0.003 \\   & LL (\(\)) & -1.371 \(\) 0.04 & -2.795 \(\) 0.012 & -0.643 \(\) 0.004 & -1.239 \(\) 0.027 & -2.608 \(\) 0.000 & -1.104 \(\) 0.005 \\  & Accuracy (\(\)) & 0.841 \(\) 0.000 & – & 0.759 \(\) 0.001 & 0.814 \(\) 0.001 & 0.450 \(\) 0.000 & 0.621 \(\) 0.000 \\  & RMSE (\(\)) & 0.631 \(\) 0.02 & 9.614 \(\) 0.013 & 0.358 \(\) 0.010 & 0.846 \(\) 0.007 & 1.022 \(\) 0.000 & 3.734 \(\) 0.003 \\   & LL (\(\)) & -0.619 \(\) 0.063 & -2.646 \(\) 0.057 & 0.372 \(\) 0.022 & **-11.100 \(\) 0.300** & -2.404 \(\) 0.002 & -0.304 \(\) 0.002 \\  & Accuracy (\(\)) & 0.841 \(\) 0.000 & – & 0.780 \(\) 0.001 & 0.830 \(\) 0.004 & 0.455 \(\) 0.000 & 0.622 \(\) 0.0

### Regular Time Series Forecasting

Lastly, we evaluate ContiFormer's performance in the context of regular time series modeling. To this end, we conducted an extensive experiment following the experimental settings from . We employed the ETT, Exchange, Weather, and ILI datasets for time series forecasting tasks. We compare ContiFormer with the recent state-of-the-art model (TimesNet ), Transformer-based models (FEDformer, Autoformer) and MLP-based model (DLinear ).

Table 5 summarizes the experimental results. Contiformer achieves the best performance on ILI dataset. Specifically, it gives **10%** MSE reduction (\(2.874 2.632\)). Also, Contiformer outperforms Autoformer and FEDformer on Exchange, ETTm2, and Weather datasets. Therefore, it demonstrates that Contiformer performs competitively with the state-of-the-art model on regular time series forecasting. For a more comprehensive exploration of the results, please refer to Appendix C.4.

## 5 Efficiency Analysis

Employing continuous-time modeling in ContiFormer often results in substantial time and GPU memory overhead. Hence, we conduct a comparative analysis of ContiFormer's actual time costs against those of Transformer-based models (TST), ODE-based models (ODE- RNN, Neural CDE), and SSM-based models (S5).

Table 6 shows that both TST and S5 are effective models. Additionally, ContiFormer leverages a parallel mechanism for faster inference, resulting in faster inference speed as compared to ODE-RNN and Neural CDE. Furthermore, ContiFormer demonstrates competitive performance even with a larger step size, as detailed in Appendix D.3. Therefore, ContiFormer's robustness to step size allows it to achieve a superior balance between time cost and performance. Please refer to Appendix E for more details.

## 6 Conclusion

We propose ContiFormer, a novel continuous-time Transformer for modeling dynamic systems of irregular time series data. Compared with the existing methods, ContiFormer is more flexible since it abandons explicit closed-form assumptions of the underlying dynamics, leading to better generalization in various circumstances. Besides, similar to Neural ODE, ContiFormer can produce a continuous and smooth outcome. Taking advantage of the self-attention mechanism, ContiFormer can also capture long-term dependency in irregular observation sequences, achieving superior performance on a variety of irregular time series modeling tasks.

   Model & Step Size & Time Cost (\(\)) & Accuracy (\(\)) \\  TST  & – & \(0.14\) & 0.826 \\ S5  & – & \(1\) & 0.772 \\   & 0.1 & \(2.47\) & 0.827 \\  & 0.01 & \(14.64\) & 0.796 \\   & 0.1 & \(3.48\) & 0.743 \\  & 0.01 & \(25.03\) & 0.748 \\   & 0.1 & \(1.88\) & **0.836** \\  & 0.01 & \(5.87\) & **0.836** \\   

Table 6: Actual time cost during training for different models on RacketSports dataset (0% dropped) from UEA dataset. We adopt RK4 solver for ODE-RNN, Neural CDE, and ContiFormer. Step size refers to the amount of time increment for the RK4 solver. \(\) (\(\)) indicates the higher (lower) the better.

   Model &  &  &  &  & Autoformer & Transformer \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  Exchange & 0.339 & 0.399 & 0.320 & 0.396 & **0.277** & **0.377** & 0.392 & 0.446 & 0.440 & 0.477 & 1.325 & 0.949 \\  ETTm2 & 0.226 & 0.300 & **0.214** & **0.276** & 0.220 & 0.305 & 0.226 & 0.300 & 0.231 & 0.307 & 0.689 & 0.633 \\  ETTh2 & 0.356 & 0.395 & **0.321** & **0.364** & 0.358 & 0.396 & 0.351 & 0.393 & 0.352 & 0.394 & 2.081 & 1.179 \\  ILI & **2.632** & **1.042** & 2.874 & 1.066 & 4.188 & 1.493 & 4.467 & 1.547 & 4.046 & 1.446 & 5.130 & 1.507 \\  Weather & **0.257** & 0.290 & 0.259 & **0.285** & 0.260 & 0.311 & 0.315 & 0.360 & 0.318 & 0.359 & 0.672 & 0.579 \\   

Table 5: Results of regular time series forecasting. Input length for ILI is 36 and 96 for the others. Prediction lengths for ETTm2 and ETTh2 are \(\{24,48,168,336\}\), \(\{96,192,336,540\}\) for Exchange and Weather, and \(\{24,36,48,60\}\) for ILI. Results are averaged over the four prediction lengths. Bold/underlined values indicate the best/second-best. MSE stands for mean squared error.

Acknowledgement

This research is supported in part by the National Natural Science Foundation of China under grant 62172107.