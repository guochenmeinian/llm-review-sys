# Adaptive Proximal Gradient Method for Convex Optimization

Yura Malitsky

Faculty of Mathematics

University of Vienna, Austria

yurii.malitskyi@univie.ac.at Konstantin Mishchenko

Samsung AI Center, UK

konsta.mish@gmail.com

###### Abstract

In this paper, we explore two fundamental first-order algorithms in convex optimization, namely, gradient descent (GD) and proximal gradient method (ProxGD). Our focus is on making these algorithms entirely adaptive by leveraging local curvature information of smooth functions. We propose adaptive versions of GD and ProxGD that are based on observed gradient differences and, thus, have no added computational costs. Moreover, we prove convergence of our methods assuming only _local_ Lipschitzness of the gradient. In addition, the proposed versions allow for even larger stepsizes than those initially suggested in .

## 1 Intro

In this paper, we address a convex minimization problem

\[_{x^{d}}F(x).\]

We are interested in the cases when either \(F\) is differentiable and then we will use notation \(F=f\), or it has a composite additive structure as \(F=f+g\). Here, \(f\) represents a convex and differentiable function, while \(g\) is convex, lower semi-continuous (lsc), and prox-friendly. Throughout the paper, we will interchangeably refer to the smoothness of \(f\) and the Lipschitzness of \( f\), occasionally with the adjective "locally," indicating that it is restricted to a bounded set. We will refer to this property as smoothness, without mentioning the Lipschitzness of \(f\), so we hope there will be no confusion in this regard.

For simplicity, in most of the introduction, we consider only the simpler problem \(_{x}f(x)\). We study one of the most classical optimization algorithms -- _gradient descent_ --

\[x^{k+1}=x^{k}-_{k} f(x^{k}).\] (1)

Its simplicity and the sole prerequisite of knowing the gradient of \(f\) make it appealing for diverse applications. This method is central in modern continuous optimization, forming the bedrock for numerous extensions.

Given the initial point \(x^{0}\), the only thing we need to implement (1) is to choose a stepsize \(_{k}\) (also known as a learning rate in machine learning literature). This seemingly tiny detail is crucial for the method convergence and performance. When a user invokes GD as a solver, the standard approach would be to pick an arbitrary value for \(_{k}\), run the algorithm, and observe its behavior. If it diverges at some point, the user would try a smaller stepsize and repeat the same procedure. If, on the other hand, the method takes too much time to converge, the user might try to increase the stepsize. In practice, this approach is not very efficient, as we have no theoretical guarantees for a randomly guessed stepsize, and the divergence may occur after a long time. Both underestimating and overestimating the stepsize can, thus, lead to a large overhead.

Below we briefly list possible approaches to choosing or estimating the stepsize and we provide a more detailed literature overview in Section 5.

Fixed stepsize.When \(f\) is \(L\)-smooth, GD can utilize a fixed stepsize \(_{k}=<\) and values larger than \(\) will provably lead to divergence. Consequently, in such scenarios, the rate of convergence is given by \(f(x^{k})-f_{*}=()\), clearly indicating a direct dependence on the stepsize. Nevertheless, several drawbacks emerge from this approach:

1. [label=()]
2. \(L\) is not available in many practical scenarios;
3. if the curvature of \(f\) changes a lot, GD with the global value of \(L\) may be too conservative;
4. \(f\) may be not globally \(L\)-smooth.

For illustration, consider the following functions. Firstly, when dealing with \(f(x)=\|Ax-b\|^{2}\), where \(A^{n d}\) and \(b^{n}\), estimating \(L\) involves evaluating the largest eigenvalue of \(A^{}A\). Second, the logistic loss \(f(x)=(1+(-ba^{}x))\), with \(a^{d},b\{-1,1\}\), is almost flat for large \(x\), yet for values of \(x\) closer to \(0\), it has \(L=\|a\|^{2}\). Thus, if the solution is far from 0, gradient descent with a constant stepsize would be too conservative. Finally, consider \(f(x)=x^{4}\). While this simple objective is not globally \(L\)-smooth for any value of \(L\), on any bounded set it _is_ smooth, and we would hope we can still minimize objectives like that.

Linesearch.Also known as backtracking in the literature. In the \(k\)-th iteration we compute \(x^{k+1}\) with a certain stepsize \(_{k}\) and check a specific condition. If the condition holds, we accept \(x^{k+1}\) and proceed to the next iteration; otherwise we halve \(_{k}\) and recompute \(x^{k+1}\) using this reduced stepsize. This approach, while the most robust and theoretically sound, incurs substantially higher computational costs compared to regular GD due to the linesearch procedure.

Adagrad-type algorithms.These are the methods of the type1

\[v_{k} =v_{k-1}+\| f(x^{k})\|^{2}\] (2) \[x^{k+1} =x^{k}-}{}} f(x^{k}),\]

where \(v_{-1} 0\) is some constants, and \(d_{k}\) is an estimate of \(\|x^{0}-x^{*}\|\) for some solution \(x^{*}\). While such methods indeed have certain nice properties, \(d_{k}\) is usually either constant or quickly converges to a constant value, so a quick glance at (2) will reveal that its stepsizes are decreasing. Therefore, despite the name, we cannot expect true adaptivity of this method to the local curvature of \(f\).

Heuristics.Numerous heuristics exist for selecting \(_{k}\) based on local properties of \(f\) and \( f\), with the Barzilai-Borwein method  being among the most widely popular. However, it is crucial to note that we are not particularly interested in such approaches, as they lack consistency and may even lead to divergence, even for simple convex problems.

We have already mentioned _adaptivity_ a few times, without properly introducing it. Now let us try to properly understand its meaning in the context of gradient descent. Besides the initial point \(x^{0}\), GD has only one degree of freedom -- its stepsize. From the analysis we know that it has to be approximately an inverse of the local smoothness. We call a method _adaptive_, if it automatically adapts a stepsize to this local smoothness without additional expensive computation and the method does not deteriorate the rate of the original method in the worst case. In our case, the original method is GD with a fixed stepsize.

By this definition, GD with linesearch is not adaptive, because it finds the right stepsize with some extra evaluations of \(f\) or \( f\). GD with diminishing steps (as in subgradient or Adagrad methods) is also not adaptive, because decreasing steps cannot in general represent well the function's curvature; also the rate of the subgradient method is definitely worse. It goes without saying, that for a _good_method its rate must experience improvement when we confine the class of smooth convex functions to the strongly convex ones.

## Contribution

In a previous work , which serves as the cornerstone for the current paper, the authors proposed an adaptive gradient method named _"Adaptive Gradient Descent without Descent"_ (AdGD). In the current paper, we

* deepen our understanding of AdGD and identify its limitations;
* refine its theory to accommodate even larger steps;
* extend the revised algorithm from unconstrained to the proximal case.

The analysis in the last two cases is not a trivial extension, and we were rather pleasantly surprised that this was possible at all. After all, the theory of GD is well-established and we thought it to be too well-explored for us to discover something new.

Continuous point of view.It is instructive for some time to switch from the discrete setting to the continuous and to compare gradient descent (GD) with its parent -- gradient flow (GF)

\[x^{}(t)=- f(x(t)), x(0)=x_{0},\] (3)

where \(t\) is the time variable and \(x^{}(t)\) denotes the derivative of \(x(t)\) with respect to \(t\). To guarantee the existence and uniqueness of a trajectory \(x(t)\) of GF, it is sufficient to assume that \( f\) is _locally_ Lipschitz-continuous. Then one can prove convergence of \(x(t)\) to a minimizer of \(f\) in just a few lines. For GD, on the other hand, the central assumption is _global_ Lipschitzness of \( f\). Our analysis of gradient descent makes it level: local Lipschitzness suffices for both. Or to put it differently, we provide an adaptive discretization of GF that converges under the same assumptions as the original continuous problem (3).

Proximal case.We emphasize that there is already an excellent extension by Latafat et al.  of the work  to the additive composite case. Our proposed result, however, is based on an improved unconstrained analysis and uses a different (and simpler) proof. We believe that both these facts will be of interest. We don't have a good understanding why, but for us finding the proof for the proximal case was quite challenging. It does not follow the standard lines of arguments and uses a novel Lyapunov energy in the analysis.

Nonconvex problems.We believe that our algorithm will be no less important in the nonconvex case, where gradients are rarely globally Lipschitz continuous and where the curvature may change more drastically. It is true that our analysis applies only to the convex case, but, as far as we know, limited theory has never yet prevented practitioners from using methods in a broader setting. And based on our (speculative) experience, we found it challenging to identify nonconvex functions where the method did not converge to a local solution.

Outline.In Section 2, we begin by revisiting AdGD from , examining its limitations, and demonstrating a simple way to enhance it. This section maintains an informal tone, making it easily accessible for quick reading and classroom presentation. In Section 3, we further improve the method and provide all formal proofs, most of which we move to the Appendix. Section 4 extends the improved method to the proximal case. In Section 5 we put our finding in the perspective and compare it to some existing works. Lastly, in Section 6 (see also Appendix D), we conduct experiments to evaluate the proposed method against different linesearch variants.

### Preliminaries

We say that a mapping is _locally Lipschitz_ if it is Lipschitz over any compact set of its domain. A function \(f^{d}\) is _(locally) smooth_ if its gradient \( f\) is (locally) Lipschitz.

A convex \(L\)-smooth function \(f\) is characterized by the following inequality

\[f(y)-f(x)- f(x),y-x\| f(y)-  f(x)\|^{2} x,y.\] (4)This is equivalently of saying that \( f\) is a \(\)_-cocoercive_ operator, that is

\[ f(y)- f(x),y-x\| f(y)-  f(x)\|^{2} x,y.\] (5)

For a convex differentiable \(f\) that is not \(L\)-smooth one can only say that \( f\) is _monotone_, that is

\[ f(y)- f(x),y-x 0 x,y.\] (6)

We use notation \([t]_{+}=\{t,0\}\) and for any \(a>0\) we suppose that \(=+\). With a slight abuse of notation, we write \([n]\) to denote the set \(\{1,,n\}\). A solution and the value of the optimization problem \( f(x)\) are denoted by \(x^{*}\) and \(f_{*}\), respectively.

## 2 Adaptive gradient descent: better analysis

Let us start with the simpler problem of \(_{x}f(x)\) with a convex, locally smooth \(f^{d}\). To solve it, in , the authors proposed a method called _adaptive gradient descent without descent_ (AdGD), whose update is given below:

\[_{k} =}_{k-1},-x^ {k-1}\|}{2\| f(x^{k})- f(x^{k-1})\|}},_{k}=}{_{k-1}}\] (7) \[x^{k+1} =x^{k}-_{k} f(x^{k}).\]

Similarly to the standard GD, this method leads to \((1/k)\) convergence rate. However, unlike the former, it doesn't require any knowledge about Lipschitz constant of \( f\) and doesn't even require a global Lipschitz continuity of \( f\).

The update for \(_{k}\) has two ingredients. The first bound \(_{k}}_{k-1}\) sets how fast steps may increase from iteration to iteration. The second \(_{k}-x^{k-1}\|}{2\| f(x^{k})- f(x^{k -1})\|}\) corresponds to the estimate of local Lipschitzness of \( f\).

It is important to understand how essential these bounds are. Do we really need to control the growth rate of \(_{k}\) or is it an artifact of our analysis? For the second bound, it is not clear whether \(2\) in the denominator is necessary. For example, given \(L\)-smooth \(f\), our scheme (7) does not encompass a standard GD with \(_{k}=\) for all \(k\).

First bound.Answering the first question is relatively easy. Consider the following function

\[f(x)=x^{2},&x[-1,1]\\ a(|x|-(1+|x|))+b,&x[-1,1]\]

where parameters \(a,b>0\) are chosen to ensure that \(f( 1)\) and \(f^{}( 1)\) are well-defined, namely \(a=2\) and \(b=2 2-\), see Lemma 3 in Appendix A.

From an optimization point of view, \(f\) is a nice function. In particular, it is convex (even locally strongly convex) and its gradient is \(1\)-Lipschitz, see Lemma 3. This means that both GD and AdGD linearly converge on it. However, if we remove the first condition for \(_{k}\) in AdGD, this new modified algorithm will fail to converge. We can prove an even stronger statement. Specifically, let \(c 1\), \(_{0}=1\) and consider the following method

\[_{k} =-x^{k-1}\|}{c\| f(x^{k})- f(x^{k-1})\| }, k 1\] (9) \[x^{k+1} =x^{k}-_{k} f(x^{k}), k 0.\]

In other words, the update in (9) is the same as in (7) except we removed the first constraint for \(_{k}\) in (7) and introduced a constant factor \(c\) to make the second one more general.

**Theorem 1**.: _For any \(c 1\) there exists \(x^{0}\) such that the method (9) applied to \(f\) defined in (8) diverges._

[MISSING_PAGE_FAIL:5]

Substituting this inequality into (10) gives us

\[\|x^{k+1}-x^{*}\|^{2}+}{1-^{2}}\|x^{k+1}-x^{k} \|^{2}+_{k}(2+}{1-^{2}})(f(x^{k})-f_{*})\] \[ \|x^{k}-x^{*}\|^{2}+}{1-^{2}}\|x^{k}-x^{k-1 }\|^{2}+_{k}}{1-^{2}}(f(x^{k-1})-f_{*}).\] (13)

As we want to telescope the above inequality, we require

\[_{k}}{1-^{2}}_{k-1}(2+}{1-^{2}})_{k}^{2}(2(1-^ {2})+_{k-1})_{k-1}^{2}.\]

On the other hand, we have already used that \(_{k}L_{k}\). These two conditions lead to the bound

\[_{k}=\{)+_{k-1}}_{k-1},}\},\] (14)

where \((0,1)\) can be arbitrary. Now by playing with different values of \(\), we obtain different instances of adaptive gradient descent method. For instance, by setting \(=}\), we get

\[_{k}=\{}_{k-1},L_{k }}\},\]

which is a strict improvement upon the original version in . A simple reason why this is possible is that, unlike in , we did not resort to the Cauchy-Schwarz inequality and instead relied on transformation (12) and Lemma 1.

```
1:Input:\(x^{0}^{d}\), \(_{0}=0\), \(_{0}>0\)
2:\(x^{1}=x^{0}-_{0} f(x^{0})\)
3:for\(k=1,2,\)do
4:\(L_{k}=)- f(x^{k-1})\|}{\|x^{k}-x^{k-1}\|}\)
5:\(_{k}=\{}_{k-1},L_{k}}\}\)
6:\(x^{k+1}=x^{k}-_{k} f(x^{k})\)
7:\(_{k}=}{_{k-1}}\) ```

**Algorithm 1** Adaptive gradient descent

We summarize the new scheme in Algorithm 1. We do not provide a formal proof for this scheme and hope that inequality (13) should be sufficient for the curious reader to complete the proof. In any case, the next section will contain a further improvement with all the missing proofs.

**Remark 1**.: One might notice that we have used several times monotonicity of \( f\), where we actually could use a stronger property of cocoercivity (5). That is true, but we just prefer simplicity. We recommend work  that exploits cocoercivity in this framework.

## 3 Adaptive gradient descent: larger stepsize

In this section, we modify Algorithm 1 to use even larger steps resulting in Algorithm 2. This, however, will require a slightly more complex analysis.

```
1:Input:\(x^{0}^{d}\), \(_{0}=\), \(_{0}>0\)
2:\(x^{1}=x^{0}-_{0} f(x^{0})\)
3:for\(k=1,2,\)do
4:\(L_{k}=)- f(x^{k-1})\|}{\|x^{k}-x^{k-1}\|}\)
5:\(_{k}=\{+_{k-1}}_{k-1},}{^{2}L_{k}^{2}-1]_{+}}}\}\)
6:\(x^{k+1}=x^{k}-_{k} f(x^{k})\)
7:\(_{k}=}{_{k-1}}\) ```

**Algorithm 2** Adaptive gradient descent-2

Recall the notation \([t]_{+}=\{t,0\}\) and note that the second bound \(_{k}}{^{2}L_{k}^{2}-1]_{+}}}\) in step 5 of Algorithm 2 is equivalent to

\[_{k}^{2}L_{k}^{2}-^{2}}{2_{k-1}^{2}} {1}{2},\] (15)

which obviously allows for a larger range of \(_{k}\) than \(_{k}^{2}L_{k}^{2}\) in Algorithm 1. On the other hand, the first bound \(_{k}+_{k-1}}_{k-1}\) is definitely worse. At the moment, it is not even clear whether it allows \(_{k}\) to increase.

**Remark 2**.: A notable distinction between Algorithm 2 and Algorithm 1 is that the former allows to use a standard fixed step \(_{k}=\), provided that \(f\) is \(L\)-smooth. For instance, if we start from \(_{0}=\) and use \(L L_{k}\) in every iteration (we can always use a larger value), then it follows from (15) and \(_{k-1}\) that \(_{k}=\) for all \(k 1\).

Algorithm 2 requires an initial stepsize \(_{0}\). While the algorithm converges for any value \(_{0}>0\), it is important to choose initial step \(_{0}\) wisely. We suggest to do the following

\[\ _{0}\ \ _{0}L_{1}[},2].\] (16)

The upper bound ensures that \(_{0}\) is not too large, while the lower ensures that it is not too small either. In most scenarios, this requires to run a linesearch, but we emphasize that it is only needed for the first iteration. Further discussion on this topic is in Appendix B.1.

We first prove that the sequence \((x^{k})\) is bounded and then derive the convergence result. Both statements are proved in Appendix B.2.

**Lemma 2**.: The sequence \((x^{k})\) is bounded. In particular, for any solution \(x^{*}\) we have \(x^{k} B(x^{*},R)\), where

\[R^{2}=\|x^{0}-x^{*}\|^{2}+2_{0}^{2}\| f(x^{0})\|^{2}+_{0}(f (x^{0})-f_{*}).\] (17)

**Theorem 2**.: _Let \(f\) be convex with a locally Lipschitz gradient \( f\), \(x^{0}^{d}\), and \(_{0}>0\). Then the sequence \((x^{k})\) generated by Algorithm 2 converges to a solution of \(_{x}f(x)\) and_

\[_{i[k]}f(x^{i})-f_{*}}{2_{i=1 }^{k}_{i}},\] (18)

_where \(R\) is defined as in (17). In particular, if \(_{0}\) satisfies (16), then_

\[_{i[k]}f(x^{i})-f_{*}}{k},\] (19)

_where \(L\) is the Lipschitz constant of \( f\) over \(B(x^{*},R)\)._

Of course, the important bound here is (18). The second bound only shows that our choice of stepsizes \(_{k}\) cannot be too bad. The bound in (19) is stronger than the bound \(LR^{2}}{2k}\), which could be obtained as a direct consequence of Lemma 7 with simple analysis. The derivation of the sharper bound as in (19) is presented in Appendix B.3 with, unfortunately, much more involved analysis.

## 4 Adaptive proximal gradient method

In this section, we turn to a more general problem of composite optimization,

\[_{x}F(x) f(x)+g(x),\] (20)

where \(g^{d}(-,+]\) is a proper convex lsc function and \(f^{d}\) is a convex differentiable function with locally Lipschitz \( f\). Additionally, we assume that \(g\) is prox-friendly, that is we can efficiently compute its proximal mapping \(_{g}=(+ g)^{-1}\).

We present Algorithm 3 that is a verbatim adaptation of Algorithm 2 with the proximal operator applied on top of the main update (similarly, it could be applied to Algorithm 1). However, its analysis is not a straightforward generalization. We encountered two issues in the proof:

* combining previous analysis of AdGD and the prox-mapping. As shown even in (13), we operate with the vectors \(x^{k}\) and \(x^{k-1}\) in terms of \(f\). However, using the prox-inequality gives us the value \(g(x^{k+1})\), which is not straightforward to combine with \(f(x^{k})\) and \(f(x^{k-1})\).
* proving convergence of \((x^{k})\). The challenge arises from having a non-linear update due to the prox-mapping and allowing \(_{k}\) to go to \(\), making the proof quite different from the traditional approach.

We define \(R\) in the same way as in (17)

\[R^{2}=\|x^{0}-x^{*}\|^{2}+2_{0}^{2}\|F(x^{0})\|^{2}+ _{0}(F(x^{0})-F_{*}),\] (21)

where \(F(x^{0})\) denotes a subgradient of \(F\) at \(x^{0}\).

**Theorem 3**.: _Let \(f\) be convex with a locally Lipschitz gradient \( f\), \(g\) be convex lsc, \(x^{0}^{d}\), and \(_{0}>0\). Then the sequence \((x^{k})\) generated by Algorithm 3 converges to a solution of (20) and_

\[_{i[k]}F(x^{i})-F_{*}}{2_{i=1}^ {k}_{i}}.\] (22)

_In particular, if \(_{0}\) satisfies (16), then_

\[_{i[k]}F(x^{i})-F_{*}}{k},\] (23)

_where \(L\) is the Lipschitz constant of \( f\) over \(B(x^{*},R)\)._

## 5 Literature and discussion

Linesearch.There are many variants of linesearch procedures that go back to celebrated works of Goldstein  and Armijo . We discuss an efficient implementation of the latter in detail in the next section. For other variants of linesearch, we refer to .

Adagrad-type methods.Original Adagrad algorithm was proposed simultaneously in  and . The method has had a stunning impact on machine learning applications. It has also spawned a stream of various extensions that retain the same idea of using eventually decreasing steps. Because of this, its adaptivity is more prominent in the non-smooth regime, where stepsizes must be diminishing to guarantee convergence. Recent works  have proposed ways to increase \(d_{k}\) in the update (2) and  even proved convergence of some Adagrad-type methods on smooth objectives. However, the stepsize in these methods eventually stops increasing, making them less adaptive.

In addition, Adagrad-type methods are usually sensitive to the initialization, as they either degrade in performance when \(d_{k}=D\) and \(D\) is not chosen carefully, or their convergence rate depends multiplicatively on \((\|x^{0}-x^{*}\|/d_{0})\). In contrast, in our methods, the cost of estimating \(_{0}\) to satisfy condition (16) is additive and its impact vanishes as the total number of iterations increases.

Refined results on GD with a fixed stepsize.Paper  summarizes quite well the difficulty of GD analysis with large steps. In it, the authors derive sharp convergence bounds separately for two cases \( L(0,1]\) and \( L(1,2)\), and the latter case is considerably harder. In our analysis it is even harder, since the steps can go far beyond the global upper bound \(\). A surprising recent result  showcases how little is understood in this case.

Small gradient.The lack-of-descent property makes it hard to deduce the \((1/k)\) rate for the last-iterate \(\| f(x^{k})\|\), which is known for GD with a fixed stepsize. We leave it as an open problem to establish a rate.

Extensions.Because the analysis of the algorithm is so special, it is not easy to extend it to basic generalizations of GD. However, some works have already built upon it. In , the authors

[MISSING_PAGE_FAIL:9]