# BehaviorGPT: Smart Agent Simulation for Autonomous Driving with Next-Patch Prediction

Zikang Zhou\({}^{1}\)1  Haibo Hu\({}^{1*}\)1  Xinhong Chen\({}^{1}\)  Jianping Wang\({}^{1}\)  Nan Guan\({}^{1}\)

Kui Wu\({}^{2}\)  Yung-Hui Li\({}^{3}\)  Yu-Kai Huang\({}^{4}\)  Chun Jason Xue\({}^{5}\)

\({}^{1}\)City University of Hong Kong \({}^{2}\)University of Victoria \({}^{3}\)Hon Hai Research Institute

\({}^{4}\)Carnegie Mellon University \({}^{5}\)Mohamed bin Zayed University of Artificial Intelligence

{zikanzhou2-c, haibohu2-c}@my.cityu.edu.hk

{xinhong.chen, jianwang, nanguan}@cityu.edu.hk

wkui@uvic.ca yunghui.li@forxconn.com yukaih2@andrew.cmu.edu

jason.xue@mbzuai.ac.ae

Equal contribution.

###### Abstract

Simulating realistic behaviors of traffic agents is pivotal for efficiently validating the safety of autonomous driving systems. Existing data-driven simulators primarily use an encoder-decoder architecture to encode the historical trajectories before decoding the future. However, the heterogeneity between encoders and decoders complicates the models, and the manual separation of historical and future trajectories leads to low data utilization. Given these limitations, we propose BehaviorGPT, a homogeneous and fully autoregressive Transformer designed to simulate the sequential behavior of multiple agents. Crucially, our approach discards the traditional separation between "history" and "future" by modeling each time step as the "current" one for motion generation, leading to a simpler, more parameter- and data-efficient agent simulator. We further introduce the Next-Patch Prediction Paradigm (NP3) to mitigate the negative effects of autoregressive modeling, in which models are trained to reason at the patch level of trajectories and capture long-range spatial-temporal interactions. Despite having merely 3M model parameters, BehaviorGPT won first place in the 2024 Waymo Open Sim Agents Challenge with a realism score of 0.7473 and a minADE score of 1.4147, demonstrating its exceptional performance in traffic agent simulation.

**Keywords:** Multi-Agent Systems, Transformers, Generative Models, Autonomous Driving

## 1 Introduction

Autonomous driving has emerged as an unstoppable trend, with its rapid development increasing the demand for faithful evaluation of autonomy systems' reliability . While on-road testing can measure driving performance by allowing autonomous vehicles (AVs) to interact with the physical world directly, the high testing cost and the scarcity of safety-critical scenarios in the real world have hindered large-scale and comprehensive evaluation. As an alternative, validating system safety via simulation has become increasingly attractive [14; 48; 53; 44] as it enables rapid testing in diverse driving scenarios simulated at a low cost. This work focuses on smart agent simulation, i.e., simulating the behavior of traffic participants such as vehicles, pedestrians, and cyclists in the digital world, which is critical for efficiently validating and iterating behavioral policies for AVs.

A good simulator should be realistic, matching the real-world distribution of multi-agent behaviors to support the assessment of AVs' ability to coexist with humans safely. To this end, researchers startedby designing naive simulators that mainly replay the driving logs collected in the real world . When testing new driving policies that deviate from the ones during data collection, agents in such simulators often exhibit unrealistic interactions with AVs, owing to the lack of reactivity to AVs' behavior changes. To simulate reactive agents, traditional approaches  apply traffic rules to control agents heuristically , which may struggle to capture real-world complexity. Recently, the availability of large-scale driving data , the emergence of powerful deep learning tools , and the prosperity of related fields such as motion forecasting , have spurred the development of data-driven agent simulation  towards more precise matching of behavioral distribution. With the establishment of standard benchmarks like the Waymo Open Sim Agents Challenge (WOSAC) , which systematically evaluates the realism of agent simulation in terms of kinematics, map compliance, and multi-agent interaction, the research on data-driven simulation approaches has been further advanced .

Existing learning-based agent simulators  mainly mirror the techniques from motion forecasting  and opt for an encoder-decoder architecture, presumably due to the similarity between the two fields. Typically, these models use an encoder to extract historical information and a decoder to predict agents' future states leveraging the encoded features. This paradigm requires manually splitting the multi-agent time series into a historical and a future segment, with the two segments being processed by separate encoders and decoders with heterogeneous architecture. For example, MVTA  constructs training samples by randomly selecting a "current" timestamp to divide sequences into historical and future components. Others  use fixed-length agent trajectories as historical scene context, conditioned on which the multi-agent future is sampled from the decoder. Nonetheless, the benefit of employing heterogeneous modules to separately encode the history and decode the future, at the cost of significantly complicating the architecture, is unclear. Moreover, the manual separation of history and future leads to low utilization of data and computation: as every point in the sequence can be used for the separation, we believe a sample-efficient framework should be able to learn from every possible history-future pair from the sequence in parallel, which cannot be easily achieved by encoder-decoder solutions owing to their heterogeneous processing for the historical and the future time steps.

Inspired by the success of decoder-only Large Language Models (LLMs) , we introduce a fully autoregressive Transformer architecture, dubbed BehaviorGPT, into the field of smart agent simulation to overcome the limitations of previous works. By applying homogeneous Transformer blocks  to the complete trajectory snippets without differentiating history and future, we arrive at a simpler, more parameter-efficient, and more sample-efficient solution for agent simulation. Utilizing relative spacetime representations , BehaviorGPT symmetrically models each agent state in the sequence as if it were the "current" one and tasks each state with modeling subsequent states' distribution during training. As a result, our framework maximizes the utilization of traffic data for autoregressive modeling, avoiding wasting any learning signals available in the time series.

Autoregressive modeling with imitation learning, however, suffers from compounding errors  and causal confusion . Concerning the behavior simulation task, we observed that blindly mimicking LLMs' training paradigm of next-token prediction , regardless of the difference in tokens' semantics across tasks, will make these issues more prominent. For a next-token prediction model embedding tokens at \(10\) Hz, a low training loss can be achieved by simply copying and pasting the current token as the next one without performing any long-range interaction reasoning in space or time. To mitigate this issue, we introduce the Next-Patch Prediction Paradigm (NP3) that enables models to reason at the patch level of trajectories, as illustrated in Figure 1. By enforcing models to autoregressively generate the next trajectory patch containing multiple time steps, which requires understanding the high-level semantics of agent behaviors and capturing long-range spatial-temporal interactions, we prevent models from leveraging trivial shortcuts during training. We equip BehaviorGPT with NP3 and attain superior performance on WOSAC  with merely 3M model parameters, demonstrating the effectiveness of our modeling framework for smart agent simulation.

Our main contributions are three-fold. First, we propose a fully autoregressive architecture for smart agent simulation, which consists of homogeneous Transformer blocks that process multi-agent long sequences with high parameter and sample efficiency. Second, we develop the Next-Patch Prediction scheme to enhance long-range interaction reasoning, leading to more realistic multi-agent simulation over a long horizon. Third, we achieve remarkable performance on the Waymo Open Motion Dataset, winning first place in the 2024 Waymo Open Sim Agents Challenge.

## 2 Related Work

### Multi-Agent Traffic Simulation

Multi-agent traffic simulation is essential for developing and testing autonomous driving systems. From early systems like ALVINN  to contemporary simulators such as CARLA  and SUMO , these platforms have used heuristic driving policies to simulate agents' reactive behaviors [8; 7; 10]. However, they struggle to capture real-world complexity since policies based on simple heuristics are not robust enough to handle all sorts of scenarios. With the availability of large-scale data and deep learning approaches, generative models like VAEs , GANs , Diffusion , and autoregressive models [49; 41; 35] have gained success in generating multi-agent motions, which greatly enhance the realism of simulations. Given the temporal dependency of agent trajectories, autoregressive models naturally fit the simulation task, while others require extra designs to capture such dependencies. Among the existing autoregressive models, two representatives are MotionLM  and Trajeglish . Both of them adopt an encoder-decoder paradigm, designing complicated scene context encoders to extract historical information before autoregressive decoding. In contrast, our approach is fully autoregressive similar to decoder-only LLMs [37; 38; 5], which eliminates the need for using heterogeneous modules to process the historical and future time steps and achieves higher efficiency in terms of data and parameters via simpler architectural design.

### Patching Operations in Transformers

The application of patches in Transformer models has demonstrated significant potential across various data modalities. For instance, BERT  employs subword tokenization  for natural language processing, while ViT  segments images into 2D patches for visual understanding. The patching design has also found applications in time-series forecasting [51; 57; 34], aiming at retaining local semantics and reducing computational complexity . Moreover, it has shown the effectiveness in self-supervised learning, which has significantly facilitated representation learning and contributed to excellent fine-tuning results on large datasets [2; 21; 3]. Since the task of agent simulation also involves time-series data, we expect the patching mechanism to help models effectively capture the spatial-temporal interactions in driving scenarios and enhance the realism of the generated motion. Our proposed Next-Patch Prediction Paradigm (NP3) utilizes patch-level tokens in autoregressive modeling and trains each token to generate the next patch that comprises multi-step motions, which shares some similarities to multi-token prediction in LLMs .

## 3 Methodology

This section presents the proposed BehaviorGPT for multi-agent behavior simulation, with Figure 2 illustrating the overall framework. To begin with, we provide the formulation of our map-conditioned, multi-agent autoregressive modeling. Then, we detail the architecture of BehaviorGPT, which adopts a Transformer decoder with a triple-attention mechanism to operate sequences at the patch level. Finally, we present the objective for model training.

Figure 1: **Next-Patch Prediction Paradigm** with patch sizes of \(1\), \(5\), and \(10\) time steps for trajectories sampled at \(10\) Hz. The capsules in dark red represent the agent states at the current time step \(t\), while the faded red capsules indicate agents’ past states. The grey circles represent the masked agent states required for generation. Our approach groups multi-step agent states as patches, demanding each patch to predict the subsequent patch during training.

### Problem Formulation

In multi-agent traffic simulation, we aim to simulate agents' future behavior in dynamic and complex environments. Specifically, we define a scenario as the composite of a vector map \(M\) and the states of \(N_{}\) agents over \(T\) time steps. At each time step, the state of the \(i\)-th agent \(S_{i}\) includes the agent's position, velocity, yaw angle, and bounding box size. The semantic type of agents (e.g., vehicles, pedestrians, and cyclists) are also available. Given the sequential nature of agent trajectories, we formulate the problem as sequential predictions over trajectory patches, where the prediction of each patch will affect the subsequent patches. We define an agent-level trajectory patch as

\[P_{i}^{}=S_{i}^{((-1)+1):()}\,,\,i\{1,,N_{}\}\,,\,\{1,,N_{}\}\,, \]

where \(\) is the number of time steps covered by a patch, \(N_{}=T/\) indicates the number of patches, and \(P_{i}^{}\) represents the \(\)-th trajectory patch of the \(i\)-th agent, with \(S_{i}^{((-1)+1):()}\) denoting the states within the patch. On top of \(P_{i}^{}\), we use \(P^{}=S_{1:N_{}}^{((-1))+1):()}\) to denote the \(\)-th multi-agent patch, where \(P^{}\) incorporates all agents' states at the \(\)-th patch. Next, we factorize the multi-agent joint distribution over patches along the time axis according to the chain rule:

\[(S_{1:N_{}}^{1:T} M)=_{=1}^{N_{}}(P^{} P^{1:(-1)},M)\,, \]

where \((S_{1:N_{}}^{1:T} M)\) is the joint distribution of all agents' states over all time steps conditioned on the map \(M\). Further, we factorize over agents the conditional distribution of multi-agent patches based on the assumption that agents plan their motions independently within the horizon of a patch:

\[(P^{} P^{1:(-1)},M)=_{i=1}^{N_{}} (P_{i}^{} P^{1:(-1)},M)\,. \]

Considering the multimodality of agents' behavior within the horizon of a patch, we assume \((P_{i}^{} P^{1:(-1)},M)\) to be a mixture model consisting of \(N_{}\) modes:

\[(P_{i}^{} P^{1:(-1)},M)=_{k=1}^{N_{ }}_{i,k}^{}(P_{i,k}^{} P^{1:(-1)},M)\,, \]

where \(_{i,k}^{}\) is the probability of the \(k\)-th mode. Given the sequential nature of the states within a patch, we further conduct factorization over the states per mode using the chain rule:

\[(P_{i,k}^{} P^{1:(-1)},M)=_{t=(-1) +1}^{}(S_{i,k}^{t} S_{i,k}^{((-1) +1):(t-1)},P^{1:(-1)},M)\,. \]

Such an autoregressive formulation can be interpreted as planning the patch-level behavior of each agent independently (Eq. (3)), freezing agents' behavior mode per \(\) time steps (Eq. (4)),

Figure 2: **Overview of BehaviorGPT.** The model takes as input the agent trajectories and the map elements, which are converted into the embeddings of trajectory patches and map polyline segments, respectively. These embeddings are fed into a Transformer decoder for autoregressive modeling based on next-patch prediction, in which the model is trained to generate the positions, velocities, and yaw angles of trajectory patches.

and autoregressively unrolling the next state under a specific behavior mode (Eq. (5)). Under this formulation, we can flexibly adjust the replan frequency during inference to control the reactivity of agents. For example, we can let agents execute \(\{1,,\}\) steps of the planned motions and choose a new behavior mode after \(\) steps to react to the change in environments.

### Relative Spacetime Representation

In our autoregressive formulation, we treat each trajectory patch as the "current" patch that is responsible for estimating the next-patch distribution during training, contrasting many existing approaches that designate one current time step per sequence [52; 49; 23]. As a result, it is inefficient to employ the well-established agent- or polyline-centric representation from the field of motion forecasting [46; 59; 33; 42; 25; 54; 43], given that these representations are computed under the reference frames determined by one current time step per sequence. For this reason, we adopt the relative spacetime representation introduced in QCNet  to model the patches symmetrically in space and time, achieving simultaneous multi-agent prediction when implementing Eq. (3) and allowing parallel next-patch prediction for the modeling of Eq. (2). Under this representation, the features of each map element and agent state are derived from coordinate-independent attributes, e.g., the semantic category of a map element and the speed of an agent state. On top of this, we effectively maintain the spatial-temporal relationships between input elements via relative positional embeddings. Specifically, we use \(i\) and \(j\) to index two different input elements and compute the relative spatial-temporal embedding by

\[_{j i}=(\|_{j i}\|,\, (_{i},\,_{j i}),\,_{j i}, \,_{j i},\,_{j i})\,, \]

where \(R_{j i}\) is the relational embedding from \(j\) to \(i\), \(\|d_{j i}\|\) is the Euclidean distance between them, \((n_{i},d_{j i})\) is the angle between \(n_{i}\) (i.e., the orientation of \(i\)) and \(d_{j i}\) (i.e., the displacement vector from \(j\) to \(i\)), \(_{j i}/_{j i}\) is the relative yaw/height from \(j\) to \(i\), and \(_{j i}\) is the time difference.

### Map Tokenization and Agent Patching

Before performing spatial-temporal relational reasoning among the input elements of a traffic scenario, we must convert the raw information into high-dimensional embeddings. We first embed map information by sampling points along map polylines every \(5\) meters and tokenizing the semantic category of each \(5\)-meter segment (e.g., lane centerlines, road edges, and crosswalks) via learnable embeddings. The \(i\)-th polyline segment's embedding is denoted by \(_{i}\), which does not include any information about coordinates. On the other hand, we process agent states using attention-based patching to obtain patch-level embeddings of trajectories. For the \(i\)-th agent's state \(S^{t}_{i}\) at time step \(t\), we employ an MLP to transform the speed, the velocity vector's angle relative to the bounding box's heading, the size of the bounding box, and the semantic type of the agent, into a feature vector \(^{t}_{i}\). To further acquire patch embeddings, we collect the feature vectors of \(\) consecutive agent states and apply the attention mechanism with relative positional embeddings to them:

\[^{}_{i}=(Q=^{}_{i},K=V=\{[^{t }_{i},\,^{t()}_{i}]\}_{t\{(-1) +1,,-1\}})\,, \]

where \(^{}_{i}\) is the patch embedding of the \(i\)-th agent at the \(\)-th patch, \(()\) denotes the multi-head self-attention , \([:,:]\) denotes concatenation, and \(^{t()}_{i}\) indicates the positional embedding of \(S^{t}_{i}\) relative to \(S^{}_{i}\) computed according to Eq. (6). Such an operation can be viewed as aggregating the features of \(S^{((-1)+1):(-1)}_{i}\) into \(^{}_{i}\) and using the embeddings fused with high-level semantics as the agent tokens in the subsequent modules.

### Triple-Attention Transformer Decoder

After obtaining map tokens and the patch embeddings of agents, we employ a Transformer decoder  with the triple-attention mechanism to model the spatial-temporal interactions among scene elements. As illustrated in Figure 3, the triple-attention mechanism considers three distinct sources of relations in the scene, including the temporal dependencies over the trajectory patches per agent, the regulations of the map elements on the agents, and the social interactions among agents.

**Temporal Self-Attention.** This module captures the relationships among the trajectory patches of each individual agent. Similar to decoder-only LLMs [37; 38; 5], it leverages the multi-headself-attention (MHSA) with a causal mask to enforce each trajectory patch to only attend to the preceding patches of the same agent, accommodating our autoregressive formulation. The temporal MHSA is equipped with relative positional embeddings:

\[F^{}_{a2t,i}=(Q=^{}_{i},K=V=\{[^{t}_{i},\, ^{(t)()}]\}_{t\{1,,-1 \}})\,, \]

where \(F^{}_{a2t,i}\) and \(^{}_{i}\) are the temporal-aware feature vector and the patch embedding of the \(i\)-th agent at the \(\)-th patch, respectively, and \(^{t}_{i}\) embeds the relative position from \(S^{t}_{i}\) to \(S^{}_{i}\), which represents the spatial-temporal relationship between the patches \(P^{t}_{i}\) and \(P^{}_{i}\).

**Agent-Map Cross-Attention.** Unlike natural language which only has a sequence dimension, we must also conduct spatial reasoning to consider the environmental influence on agents' behavior. To facilitate the modeling of agent-map interactions, we apply the multi-head cross-attention (MHCA) to each trajectory patch in the scenario. Considering that a scenario may comprise an explosive number of map polyline segments and that an agent would not be influenced by map elements far away, we filter the key/value map elements in MHCA using the k-nearest neighbors algorithm . The agent-map cross-attention is formulated as

\[F^{}_{a2m,i}=(Q=F^{}_{a2t,i},K=V=\{[_{j},\,^{}_{j i}]\}_{j(i,)})\,, \]

where \(F^{}_{a2m,i}\) is the map-aware feature vector for the \(i\)-th agent at the \(\)-th patch, \(_{j}\) is the embedding of the \(j\)-th map polyline segment, \(^{}_{j i}\) is the relative positional embedding between the agent state \(S^{}_{i}\) and the \(j\)-th map polyline segment, and \((i,)\) denotes the k-nearest map neighbors of \(S^{}_{i}\).

**Agent-Agent Self-Attention.** We further capture the social interactions among agents by applying the MHSA to the space dimension of the trajectory patches. In this module, we also utilize the locality assumption induced by the k-nearest neighbor selection for better computational and memory efficiency. Specifically, the map-aware features of trajectory patches are refined by

\[F^{}_{a2a,i}=(Q=F^{}_{a2m,i},K=V=\{[F^{}_{a2m,j},\, ^{}_{j i}]\}_{j(i,)})\,, \]

where \(F^{}_{a2a,i}\) is the feature vector enriched with spatial interaction information among agents for the \(i\)-th agent at the \(\)-th patch, \(^{}_{j i}\) contains the relative information between the \(i\)-th and the \(j\)-th agent at the \(\)-th patch, and \((i,)\) filters the k-nearest agent neighbors of \(S^{}_{i}\).

**Overall Decoder Architecture.** Each of the attention layers above is enhanced by commonly used components in Transformers , including feed-forward networks, residual connections , and Layer Normalization  in a pre-norm fashion. To enable higher-order relational reasoning, we stack multiple triple-attention blocks by interleaving the three Transformer layers. We denote the ultimate feature of the \(i\)-th agent at the \(\)-th patch as \(F^{}_{i}\), which will serve as the input of the prediction head for next-patch prediction modeling.

### Next-Patch Prediction Head

Given the interaction-aware patch features output by the Transformer decoder, we develop a next-patch prediction head to model the marginal multimodal distribution of agent trajectories, which estimates the distributional parameters of each patch's successor.

Figure 3: **Triple Attention applies attention mechanisms to model (a) agents’ sequential behaviors, (b) agents’ relationships with the map context, and (c) the interactions among agents.**

The following describes the process of next-patch prediction regarding the \(\)-th patch of the \(i\)-th agent. Based on the attention output \(F_{i}^{}\), we intend to estimate the parameters of the next patch's mixture model pre-defined with \(N_{}\) modes. First, we use an MLP to transform \(F_{i}^{}\) into \(_{i}^{+1}^{N_{}}\), the mixing coefficient of the modes. In each mode, the conditional distribution of the next agent state, as depicted in Eq. (5), is considered a multivariate marginal distribution that parameterizes the position and velocity components as Laplace distributions and the yaw angle as a von Mises distribution. Based on this formulation, we employ a GRU-based autoregressive RNN  to unroll the states within the next patch step by step, with each step being conditioned on the previously predicted states. Specifically, The hidden state \(h_{i,k}^{,t}\) of the RNN is initialized with \(F_{i}^{}\) at \(t=1\) for \( k\{1,,N_{}\}\). At each step of the rollout, we use an MLP to estimate the location and scale parameters of the next agent state's position and velocity based on the hidden state. On the other hand, the MLP also estimates the location and concentration parameters of the next yaw angle. The location parameters of the newly predicted state, including the 3D positions, the 2D velocities, and the yaw angle, are used to update the RNN's hidden state directly without relying on the predicted scale/concentration parameters for sampling. The whole process is summarized as follows:

\[_{i,k}^{+1} =([F_{i}^{},Z_{k}])\,, \] \[h_{i,k}^{,1} =F_{i}^{}\,,\] \[_{i,k}^{+t},\;b_{i,k}^{+t},\; _{i,k}^{+t} =([h_{i,k}^{,t},Z_{k}])\,,\] \[h_{i,k}^{,t+1} =(h_{i,k}^{,t},\;(_{i,k}^{ +t}))\,,\]

where \(\{_{i,k}^{+t}^{6}\}_{t\{1,,\}}\), \(\{b_{i,k}^{+t}^{5}\}_{t\{1,,\}}\), and \(\{_{i,k}^{+t}\}_{t\{1,,\}}\) are the location, scale, and concentration parameters in the \(k\)-th mode, and \(Z_{k}\) is the \(k\)-th learnable mode embedding.

### Training Objective

To train BehaviorGPT, we apply the negative log-likelihood loss \(_{}\) to the factorized distribution of \((S_{1:N_{}}^{1:T} M)\) as formulated previously:

\[_{}=_{=1}^{N_{}}_{i=1}^{N_{ {epoch}}}-_{k=1}^{N_{}}_{i,k}^{}_{t=(-1) +1}^{}(S_{i,k}^{t} S_{i,k}^{((-1) +1):(t-1)},P^{1:(-1)},M)\,. \]

Note that each ground-truth trajectory patch is transformed into the viewpoint of its previous patch. During training, we utilize teacher forcing to parallelize the modeling of next-patch prediction and ease the learning difficulty, but we do not use the ground-truth agent states when updating the RNN's hidden states, intending to train the model to recover from its mistakes made in next-state prediction.

## 4 Experiments

This section first introduces the dataset and the evaluation metrics used in our experiments, followed by presenting the implementation details and the rollout results obtained by BehaviorGPT on the Waymo Open Sim Agents Benchmark . Finally, we conduct ablation studies to further compare and analyze the performance of BehaviorGPT under various settings.

### Dataset and Metrics

Our experiments are conducted on the Waymo Open Motion Dataset (WOMD) . The dataset comprises 486,995/44,097/44,920 training/validation/testing scenarios. Each scenario includes \(91\)-step observations sampled at \(10\) Hz, totaling \(9.1\) seconds. Given \(11\)-step initial states of the scenarios, we simulate up to \(128\) agents and generate \(80\) simulation steps per agent at \(0.1\)-second intervals in an autoregressive and reactive manner. Each agent requires \(32\) simulations comprising x/y/z centroid coordinates and a heading value. The results on the test set are obtained by utilizing the full training set, while the performance on the validation set is based on \(20\%\) of training data unless specified.

We use various metrics for evaluation. The minADE measures the minimum average displacement error over multiple simulated trajectories, assessing trajectory accuracy. REALISM is the meta-metricthat expects the simulations to match the real-world distribution. LINEAR SPEED and LINEAR ACCEL evaluate the realism regarding speed and acceleration. Similarly, ANG SPEED and ANG ACCEL measure the realism of angular speed and acceleration. DIST TO OBJ considers the distances to objects, while COLLISION and TTC assess the simulation performance in terms of collision and time to collision. Finally, DIST TO ROAD EDGE and OFFROAD focus on map compliance.

### Implementation Details

The optimal patch size we experimented with is \(10\), corresponding to \(1\) second. All hidden sizes are set to \(128\). Each attention layer has \(8\) attention heads with \(16\) dimensions per head. To save training resources, we limit the maximum number of agents per scenario to \(128\) and restrict the maximum number of neighbors in kNN attention layers to \(32\). The prediction head produces \(16\) modes per agent and time step. We train the models for \(30\) epochs on \(8\) NVIDIA RTX 4090 GPUs with a batch size of \(24\), utilizing the AdamW optimizer . The weight decay rate and dropout rate are both set to \(0.1\). The learning rate is initially set to \(5 10^{-4}\) and decayed to \(0\) following a cosine annealing schedule . Our results in the 2024 WOSAC are obtained using a single model with \(2\) decoding blocks and a total of 3M parameters. To produce \(32\) replicas of rollouts, we randomly sample behavior modes from agents' next-patch distributions until completing the \(8\)-second multi-agent trajectories, and we repeat this process with different random seeds. The final results on the leaderboard are based on a replan rate of \(2\) Hz, while the ablation studies are based on a \(1\)-Hz replan rate unless specified.

### Quantitative Results

We report the test set results in Table 1. Notably, BehaviorGPT achieves the lowest minADE and the best REALISM, underscoring the model's ability to match the real-world distribution. Its excellent performance on COLLISION and OFFROAD also indicates that the model has successfully captured the agent-agent and agent-map interactions in driving scenarios. Besides the benchmarking results, we also compare the number of model parameters in BehaviorGPT and other baselines. Table 1 demonstrates that BehaviorGPT, with only 3M parameters, achieves more realistic simulation than significantly larger models like MVTE  and GUMP , which demonstrates the parameter efficiency of our approach. Without employing tricks like data augmentation, model ensemble, or post-processing steps, BehaviorGPT won first place in the 2024 Waymo Open Sim Agents Challenge.

    &  &  maxADE \\ (\(\)) \\  } &  REALISM \\ (\(\)) \\  } & LINEAR & LINEAR & ANG & ANG & DIST &  COLLISION \\ (\(\)) \\  } &  TTC \\ (\(\)) \\  } & DIST &  BOAD \\ (\(\)) \\  } \\  & & & & & & & & & & & \\  Linear Extrapolation  & - & 7.5148 & 0.9985 & 0.0434 & 0.1661 & 0.2522 & 0.4939 & 0.2154 & 0.3905 & 0.2555 & 0.4801 & 0.4426 \\ Traffic/EUR/V1.5  & 10M & 1.8825 & 0.6908 & 0.361 & 0.3497 & 0.4512 & 0.5844 & 0.3906 & 0.3083 & 0.2029 & 0.6423 & 0.9134 \\ VSD  & 12M & 1.4743 & 0.7200 & 0.3591 & 0.3664 & 0.4197 & 0.5222 & 0.3683 & 0.9341 & 0.6508 & 0.8728 \\ MVTE  & +56M & 1.6770 & 0.7502 & 0.5306 & 0.3531 & 0.4974 & 0.6000 & 0.3743 & 0.9049 & 0.8238 & 0.6653 & 0.9071 \\ GUMP  & 252M & 1.6041 & 0.7431 & 0.3657 & **0.4111** & **0.6909** & **0.6835** & 0.3707 & 0.9043 & 0.8276 & 0.6686 & 0.9028 \\ 
**BehaviGPT (Ours)** & 3M & **1.4147** & **0.2473** & **0.3415** & 0.3065 & 0.4806 & 0.5544 & **0.3834** & **0.9537** & 0.8308 & **0.6702** & **0.9349** \\   

Table 1: Test set results in the 2024 Waymo Open Sim Agents Challenge.

Figure 4: High-quality simulations produced by BehaviorGPT, where multimodal behaviors of agents are simulated realistically.

### Qualitative Results

Figure 4 visualizes some qualitative results of the rollouts produced by our model. In this scenario, BehaviorGPT can generate multiple plausible futures given the same initial states of agents, which demonstrates its capability of simulating diverse yet realistic agent behavior. However, we also note that autoregressive models still suffer from accumulated errors in some cases. As shown in Figure 5, the vehicle in orange gradually goes out of the road as time goes by, which indicates the inherent limitations of autoregressive generation.

### Ablation Studies

We conduct some ablation studies to gain a more in-depth understanding of our approach.

**Impact of patch size.** Table 2 presents the results of BehaviorGPT with varying patch sizes. According to the results, it is evident that using a patch size of \(5\), i.e., training and predicting with \(2\)-Hz tokens, significantly outperforms the baseline without patching. Moreover, increasing the patch size to \(10\) further enhances the overall performance. These results demonstrate the benefits of incorporating the NP3 into agent simulation. However, changing the patch size also leads to a variation in replan frequency, which also has an influence on simulation. Next, we investigate the impact of replan frequency on the test set using the model submitted to the 2024 WOSAC.

**Impact of replan frequency.** During inference, we vary the replan frequency of the model with a patch size of \(10\) by discarding a portion of the predicted states at each simulation step. As shown in Table 3, increasing the replan frequency from \(1\) Hz to \(2\) Hz can even improve the overall performance, which may benefit from the enhanced reactivity. This phenomenon demonstrates that the performance gain is not merely due to the lower replan frequency, as the model with a patch size of \(10\) beats that with a patch size of \(5\) even harder if using the same replan frequency of \(2\) Hz. However, using an overly high replan frequency harms the performance, as indicated by the third row of Table 3. Overall, we conclude that using a larger patch indeed helps long-term reasoning, but a moderate replan frequency is important for temporal stability, which may be neglected by prior works.

    Patch \\ Size \\  } &  Replan \\ Frequency \\  } &  minADE \\ (\(\)) \\  } &  REALISM \\ (\(\)) \\  } &  LINEAR \\ SPEED \\  &  LINEAR \\ ACCEL \\  &  ANG \\ SPEED \\  &  ANG \\ ACCEL \\  &  DIST \\ (\(\)) \\  &  COLLISION \\ (\(\)) \\  &  TTC \\ ROD EDGE \\  & 
 DIST TO \\ (\(\)) \\  \\ 
1 & 10 Hz & 2.3752 & 0.6783 & 0.2559 & 0.2088 & 0.4022 & 0.5094 & 0.3201 & 0.9002 & 0.8015 & 0.6149 & 0.8432 \\
5 & 2 Hz & 1.5599 & 0.7273 & **0.3543** & **0.3218** & 0.4623 & **0.5438** & 0.3768 & 0.9181 & **0.8339** &

[MISSING_PAGE_FAIL:10]