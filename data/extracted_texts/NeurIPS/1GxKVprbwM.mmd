# On Computing Pairwise Statistics

with Local Differential Privacy

Badih Ghazi

Google Research

Mountain View, CA, US

badihghazi@gmail.com

&Pritish Kamath

Google Research

Mountain View, CA, US

pritish@alum.mit.edu

&Ravi Kumar

Google Research

Mountain View, CA, US

ravi.k53@gmail.com

&Pasin Manurangsi

Google Research

Bangkok, Thailand

pasin@google.com

&Adam Sealfon

Google Research

New York, NY, US

adamsealfon@google.com

###### Abstract

We study the problem of computing pairwise statistics, i.e., ones of the form \(^{-1}_{i j}f(x_{i},x_{j})\), where \(x_{i}\) denotes the input to the \(i\)th user, with differential privacy (DP) in the local model. This formulation captures important metrics such as Kendall's \(\) coefficient, Area Under Curve, Gini's mean difference, Gini's entropy, etc. We give several novel and generic algorithms for the problem, leveraging techniques from DP algorithms for linear queries.

## 1 Introduction

Differential privacy (DP)  is a widely studied and used notion for quantifying the privacy protections of algorithms in data analytics and machine learning. It has witnessed many practical deployments . On a high level, DP dictates that the output of a (randomized) algorithm remains statistically indistinguishable if the input of any single user is modified; the degree of statistical indistinguishability is quantified by the privacy parameter \(>0\).

An important setting of DP is the _local_ model , where each of \(n\) users holds an input that they wish to keep private. An analyst wishes to compute some known function of the users inputs. The analyst and the users engage in an interactive protocol at the end of which the analyst is supposed to compute an estimate to the value of the function on the user inputs. When the aforementioned statistical indistinguishability property is enforced on the algorithm's transcript, the algorithm is said to be \(\)-local DP (see Section 2 for a formal definition). _Non-interactive_ algorithms refer to the setting where each user sends a single (DP) message to the analyst, who is then supposed to output an estimate of the desired value without any further interaction with the users. As usual in the interactive local DP setting, we assume a broadcast model where every communication is visible to all users and to the analyst (and subject to the DP constraint). The number of rounds of an interactive protocol refers to the number of back-and-forths between the set of users and the analyst.

While the local DP setting offers a compelling trust model compared to the central DP model (where an analyst is assumed to have access to the raw user data and the privacy guarantee is only enforced at its output), the local setting is often limited by lower bounds on the error incurred by private protocols (e.g., ). In this work, we study some basic tasks in analytics and learning, and give local DP protocols with significantly smaller error than what was previously known.

**Quadratic Form Computation.** Throughout the paper, we consider the following task:

**Definition 1** (Quadratic Form Computation).: _Given a matrix \(W^{k k}\). Each user \(i\) receives \(x_{i}[k]\). The goal is to compute the quadratic form \(h_{}^{T}Wh_{}\) where \(h_{}_{ 0}^{k}\) denotes the normalized histogram of the input, i.e., \((h_{})_{b}:=|\{i[n] x_{i}=b\}|\)._

An algorithm computing quadratic forms immediately implies an algorithm for computing pairwise statistics (also known as \(U\)_-statistics of degree \(2\)_) defined as \((X)=}_{1 i<j n}f(x_{i},x_{j})\), where \(f:^{2}\) is a symmetric function (called _kernel_), and \(x_{i}\) is the input to the \(i\)th user. The family of pairwise statistics is notable in that it contains several statistical quantities that are widely used in different areas, including the Area Under the Curve (AUC), Kendall's \(\) coefficient, the Gini's mean difference, and the Gini's diversity index (aka Gini-Simpson index or Gini's impurity). Computing quadratic forms has been studied in the context of local DP by Bell et al.  who gave algorithms for functions \(f\) that are Lipschitz and for estimating the AUC of a classifier; the latter was improved recently . While these problems are simple in central DP (as we can directly add noise to the function value), we note that a clear challenge for computing pairwise statistics (and hence quadratic forms) in local DP is that each summand depends on data points held by two different users.

**Linear Queries.** Computation of quadratic forms is a natural "degree-2" variant of the so-called _linear queries_ (defined below), a well-studied problem in the DP literature.

**Definition 2** (Linear Queries).: _Given a workload matrix \(W^{k k}\), the goal is to compute \(Wh_{}\), where \(h_{}_{ 0}^{k}\) denotes the normalized histogram of the input._

There is a long line of work on privately answering linear queries. In both the central and local models, the optimal errors achievable by DP algorithms are (mostly) well-understood for various types of errors, such as the \(_{2}^{2}\)-error or the \(_{}\)-error . Recall that the _MSE_ of an estimate \(\) of a real value \(z\) is defined as \((,z):=_{}[(-z)^{2}]\). For our purpose, we consider the following natural measure of accuracy:

**Definition 3** (mMSE).: _Given a mechanism \(\) that answers linear queries for a workload matrix \(W^{k k}\), its maximum mean square error (mMSE) is defined as \((;W):=_{[k]^{n}}_{j[k]} (()_{j},(Wh_{})_{j})\)._

In a recent seminal work, Edmonds et al.  provide a nearly optimal characterization of the error achievable by non-interactive \(\)-local DP algorithms. To describe their result, we need some additional definitions of norms on matrices and related quantities:

* \(1 2\) norm: For any matrix \(A\), \(\|A\|_{1 2}\) is the maximum (\(_{2}\)-)norm of its columns.
* \(_{}\)-norm: For any matrix \(A\), \(\|A\|_{}\) is the maximum absolute value among its entries.
* Factorization norm: For \(W^{k k}\), let \(_{2}(W):=_{L^{T}R=W}\|L\|_{1 2}\|R\|_{1 2}\).
* Approximate-Factorization norm1: For \(W^{k k}\) and \( 0\), let \(_{2}(W;):=_{\|W-W\|_{}}_{2}()\). 
Finally, we define \((W,n):=_{ 0}(_{2}(W;)+ )\). (Note that \((W,n)_{2}(W)\) as we can pick \(=0\).) The result of Edmonds et al.  states that this quantity, up to polylogarithmic factors, governs the best error achievable for linear queries in the non-interactive local DP model2:

**Theorem 4** ().: _For any workload matrix \(W\), there is a non-interactive \(\)-local DP mechanism for linear queries with \(\) at most \(O(}{^{2}n})\). Furthermore, any non-interactive \(\)-local DP mechanism must incur \(\) at least \((}{^{2}n})\)._

Given such a tight and generic characterization for linear queries, it is natural to ask:

Can we characterize the error of computing the quadratic form for any matrix \(W\) with local DP?

### Our Contributions

In this work, we answer this question by establishing connections between the problems of computing quadratic forms and linear queries. Leveraging the wealth of knowledge on the latter, we obtain several new (and general) upper and lower bounds for the former.

**Non-Interactive Local DP.** First, in the non-interactive setting, we give the following upper and lower bounds. Together, they show that the error one can expect for computing quadratic forms is essentially the same as that for computing linear queries (as provided in Theorem 4).

**Theorem 5** (Non-interactive Algorithm).: _For any \(W^{k k}\), there is a non-interactive \(\)-local DP mechanism for estimating quadratic forms on \(W\) with \(\) at most \(O(( k)}{^{2}n})\)._

**Theorem 6** (Non-interactive Lower Bound).: _For any symmetric \(W^{k k}\), any non-interactive \(\)-local DP mechanism for estimating quadratic forms on \(W\) must incur \(\) at least \((}{^{2}n})\)._

Our results above are generic and can be applied to any pairwise statistics. To demonstrate the power of the algorithm, we now state implications for several classes of statistics that have been studied in the privacy literature. Due to space constraints, we defer their definitions to Section 5.

**Corollary 7**.: _There is a non-interactive \(\)-local DP mechanism for computing pairwise statistics for:_

* _Any_ \(G\)_-Lipschitz function_ \(f:\)_, with_ \(O( n}{^{2}n})\)_,_
* _Kendall's_ \(\) _coefficient, with_ \(O(}{^{2}n})\)_,_
* _AUC under the balancedness assumption_3_, with_ \(O(}{^{2}n})\)_,_ * _Gini's diversity index, with_ \(O(n})\)_._

For \(O(1)\)-Lipschitz functions, which includes several well-known metrics such as Gini mean difference, we improve upon the algorithm of Bell et al.  whose \(\) is \(O(})\). We are not aware of any previous bounds on Kendall's \(\) coefficient before; the metric was mentioned in  without any error guarantee given. For AUC, our bound is the same as in  but worse than a follow-up work  by a \( k\) factor; nonetheless, we stress that our result is derived as a corollary of a generic algorithm without relying too deeply on AUC (except for the \(_{2}\)-norm of its matrix). For Gini's diversity index, our bound is again worse than that from Bravo et al.  by \( k\) factor, but their algorithm requires the use of public randomness; we do not use any public randomness. (Throughout this work, when we refer to the local model without further specification, we assume no public randomness, aka _private-coin_ model.)

**Remark.** Since our non-interactive algorithm only requires a vector-summation primitive, we can also apply protocols for vector summation in the shuffle model (e.g., ) to obtain an \((,)\)-DP protocol in the shuffle model, reducing the \(\) by a factor of \(n\) for each setting in Theorem 5 and Corollary 7.

**Interactive Local DP.** Finally, we also provide an interactive algorithm whose \(\) does not depend on \(_{2}(W)\), as long as \(n\) is sufficiently large:

**Theorem 8** (Interactive Algorithm).: _For any \(W^{k k}\), there is a three-round \(\)-DP algorithm for estimating quadratic forms on \(W\) such that, for \(n((W) k}{\|W\|_{} })^{O(1)}\), the \(\) is \(O(^{2}}{^{2}n})\)._

We note that the dependence \(O(^{2}}{^{2}n})\) is the best possible: even when the \(k=2\) and \(W\) is binary, the problem is as hard as binary summation, which is known to require \(\) at least \((n})\) even for an arbitrary number of rounds of interactions , and we can rescale this hard instance to get any desired \(_{}\)-norm.

Combining the non-interactive lower bound in Theorem 6 and the interactive upper bound in Theorem 8, our results imply that there exists a matrix \(W^{k k},>0,n\) such that interactive algorithms are provably more accurate than non-interactive ones for privately computing the quadratic form on \(W\). This places computing pairwise statistics as one of the few problems (and perhaps the most natural) that are known to separate interactive and non-interactive local DP. Due to space constraints, we defer further discussion of this to the Appendix.

### Technical Overview

We now give a brief technical overview of our proofs.

**Black-Box Reduction from Linear Queries (Lower Bound).** As mentioned earlier, our results are shown through connections between computing quadratic forms and linear queries. We will start with black-box reductions between the two. Suppose that we have a non-interactive \(\)-local DP algorithm \(\) for computing the quadratic form on \(W\). We will construct a non-interactive \(\)-local DP algorithm \(^{}\) for computing linear queries of \(W\). This will allow us to establish our lower bound (Theorem 6) as a consequence of the lower bound for linear queries (Theorem 4).

For simplicity of this overview, instead of considering the full quadratic form, suppose that \(\) works on \(2n\) users with inputs \(x_{1},,x_{n},y_{1},,y_{n}\) and computes an estimate of \(h_{}^{T}Wh_{}\). On input \(x_{1},,x_{n}\), algorithm \(\) works as follows:

* Each user \(i\) runs the randomizer of \(\) on \(x_{i}\) to get a response \(o_{i}\) and sends it to the analyst.
* For each \(j[k]\), the analyst _simulates_ running the randomizer of \(\) on \(y_{1}==y_{n}=j\) to get responses \(o_{1}^{},,o_{n}^{}\). Then, the analyst lets \(_{j}\) be the estimator of \(\) based on the responses \(o_{1},,o_{n},o_{1}^{},,o_{n}^{}\).
* The analyst then outputs \((_{1},,_{k})\) as its estimate.

In other words, the randomized responses from \(\) are used as an "oracle" for \(^{}\) to compute the different linear queries. The key observation here is that, when we set \(y_{1}==y_{n}=j\), \(\) produces an estimate for \(h_{}^{T}Wh_{}=_{j}^{T}Wh_{}=(Wh_{ })_{j}\) as desired. Note that this reduction only works in the non-interactive setting: if we were in the interactive setting, the responses on \(x_{1},,x_{n}\) (of protocol \(\)) would have been dependent on those of \(y_{1},,y_{n}\). Therefore, the first step of the reduction would have been impossible.

While this encapsulates the high-level ideas of our proof, there are some details that needs to be handled. E.g., if \(\) outputs the quadratic form \(h_{,}^{T}Wh_{,}\), there are "cross terms" of the form \(h_{}^{T}Wh_{}\) that need to be removed. We formally describe and analyze the full reduction in Section 4.

**Black-Box Reduction to Linear Queries (Algorithm).** We can also give a reduction in the reverse direction, although this results in an additional round of communication. Specifically, let \(^{}\) be a non-interactive \(\)-local DP algorithm for computing linear queries of \(W\). We can construct a two-round \((2)\)-local DP algorithm \(\) for computing the quadratic form on \(W\) as follows.

* **First Round:** Run \(^{}\) on all users to compute an estimate \((_{1},,_{k})\) for \(Wh_{}\).
* **Second Round:** Each user \(j[n]\), sends \(o_{j}=_{x_{j}}+_{j}\) to the analyst where \(_{j}\) is (appropriately calibrated) Laplace noise. The analyst then outputs \((o_{1}++o_{n})\).

Again, we omit some details for simplicity, such as the fact that each \(_{j}\) may not be bounded a priori, which may make the second step violate DP. However, these are relatively straightforward to handle.

If there were no noise, we would have \(o_{j}=_{x_{j}}^{T}Wh_{}\) and thus \((o_{1}++o_{n})=h_{}^{T}Wh_{}\) as desired. Furthermore, it is not hard to see that the error from \(_{1},,_{n}\) is dominated by the error from \(^{}\) in the first step. In other words, we get an error similar to the one in Theorem 5 here, but the protocol is interactive (two-round). Making the protocol non-interactive requires us to step away from the black-box approach and open up the linear query algorithm (Theorem 4).

**White-Box Algorithms.** For simplicity, in this section we describe protocols with accuracy that depends on \(_{2}(W)\), which can be larger than \((W,n)\). The desired error dependence on \((W,n)\) can be obtained from this bound via a reduction, as shown in Lemma 10.

To understand our algorithm, we first describe the _matrix mechanism_ for linear queries (cf. ). That algorithm works as follows: factorize \(W=L^{T}R\). Then, user \(i\) sendswhere \(z_{i}^{R}\) is a appropriately selected random noise. In other words, each user privatizes \(W_{x_{i}}\) and sends it to the analyst. Finally, the analyst outputs \(L^{T}((o_{1}^{R}++o_{n}^{R}))\). This is exactly equal to \(Wh_{}+L^{T}Z^{R}\) where \(Z^{R}:=(z_{1}^{R}++z_{n}^{R})\). It is possible to select the noise in such a way that \(Z^{R}\) is \((\|R\|_{1 2}\!/\!)\)-sub-Gaussian. This leads to an \(\) of \(O((W)^{2}}{^{2}n})\).

This suggests a natural approach for quadratic forms: in addition to sending (a privatized version of) \(R_{x_{i}}\), the user sends a privatized version of \(L_{x_{i}}\), i.e., \(o_{i}^{L}=L_{x_{i}}+z_{i}^{L}\) where \(z_{i}^{L}\) is a random noise, to the analyst. The analyst then outputs \((o_{1}^{L}++o_{n}^{L}),(o_{1}^{R}+ +o_{n}^{R})\). Letting \(Z^{L}:=(z_{1}^{L}++z_{n}^{L})\), the output can be written as \(h_{}^{T}Wh_{}+ Lh_{},Z^{R} + Rh_{},Z^{L}+ Z^{L},Z^{R}\). There are three error terms: \( Lh_{},Z^{R}, Rh_{},Z ^{L}\), and \( Z^{L},Z^{R}\). Similar to linear queries analysis, it is not hard to see that the first two terms contribute \(O((W)^{2}}{^{2}n})\) to the \(\). Unfortunately, the last term is problematic for us: a simple calculation shows that it contributes \(O(n^{2}})\) to the \(\), where \(\) denotes the number of rows of \(L,R\). A priori, this term can be quite large as there is no obvious bound on \(\). In fact, if \(W\) is full rank (which is the case for most popular pairwise statistics), then we know that \(\) must be at least \(k\). This leads to an undesired error term \(O(n^{2}})\), which dominates the first term for small-to-moderate values of \(n\), i.e., when \(n_{2}(W)^{2}}\).

To overcome this, we observe that, if we only look for _approximate factorization_ (in the same sense as \(_{2}(W;)\) defined above), then it is always possible to reduce \(\) via dimensionality reduction techniques (e.g., ). Namely, we may pick a (e.g., random Gaussian) matrix \(A\) and replace \(L,R\) with \(AL,AR\) respectively. Selecting the number of rows of \(A\) appropriately then yields Theorem 5.

**Additional Interactive Algorithms.** To describe our algorithm, let us start by instantiating the above black-box two-round reduction using the matrix mechanism. In this context, the reduction yields the following algorithm:

* **First Round:** Each user \(i\) sends \(o_{i}^{L}=L_{x_{i}}+z_{i}^{L}\) to the analyst, where \(z_{i}^{L}\) is appropriately selected random noise. The analyst then broadcasts \(O^{L}=(o_{1}^{L}++o_{n}^{L})\) to each user.
* **Second Round:** Each user \(j[n]\) sends \(o_{j}= O^{L},R_{x_{j}}+_{j}\) to the analyst, where \(_{j}\) is appropriately calibrated Laplace noise. The analyst then outputs \((o_{1}++o_{n})\).

It is not hard to see that the \(_{j}\) noise terms together contribute at most \(O(1/^{2}n)\) to the \(\). Therefore, the main noise comes from the first step. Similar to the previous discussion, this noise can be written as \( Rh_{},Z^{L}\) where \(Z^{L}:=(z_{1}^{L}++z_{n}^{L})\). The contribution of this noise to the \(\) is then \(O((W)^{2}}{^{2}n})\). The \(_{2}^{2}(W)\) term shows up in the error because \(\|Rh_{}\|_{2}\) can be as large as \(\|R\|_{1 2}\). The idea motivating our improvement is simple: Can we replace the \(Rh_{}\) term with a term that is much smaller?

This brings us to the following strategy. We will use an additional round at the beginning to compute a rough estimate \(\) of \(Rh_{}\). Using the so-called _projection mechanism_, it is possible to compute \(\) such that \(\|Rh_{}-\|(W)( k)/ )^{O(i)}}{n^{2()}}\). The subspace orthogonal to \(\) can be processed in a similar manner as before, but now the error term will just be \( Rh_{}-,Z^{L}\). Since \(\|Rh_{}-\|\) is now much smaller (approaches \(0\) as \(n\)), this gives us the improved error bound. We note that the direction of \(\) can be handled by having each user \(i\) directly send \( o_{i}^{L},\) plus an appropriately calibrated noise. This summarizes the high-level idea of our approach.

Due to space constraints, we focus on the non-interactive algorithms in the main body and defer the proof of the interactive algorithm to the Appendix.

## 2 Preliminaries

**Differential Privacy.** For \( 0\), an algorithm \(\) is _\(\)-DP_ if for every pair \(X,X^{}\) of inputs that differ on one user's input and for every possible output \(o\), \([(X)=o] e^{}[(X^{})=o]\).

An algorithm \(\) in the local DP model consists of a randomizer \(\) and an analyst, computed as \((X)=((x_{1}),,(x_{n}))\) for input \(X=\{x_{1},,x_{n}\}\). \(\) is said to be (non-interactive) \(\)-local DP if \(\) is \(\)-DP.

A real-valued random variable \(Z\) is \(\)-sub-Gaussian iff \([(Z^{2}/^{2})] 2\). A \(^{d}\)-valued random variable \(Z\) is \(\)-sub-Gaussian iff \(,Z\) is \(\)-sub-Gaussian for all unit vectors \(^{d}\).

**Theorem 9** ().: _For any \(C,>0\), there is a (non-interactive) \(\)-local DP algorithm \(_{,C}\) that takes in \(x^{d}\) such that \(\|x\|_{2} C\) and outputs \(Y^{d}\) such that \([Y]=x\) and \(Y-x\) is \(\)-sub-Gaussian for \(=O(C/)\)._

Error: Factorization vs Approximate-Factorization.We show that it suffices to give errors in terms of \(_{2}(W)\) instead of \((W;n)\); this will be convenient for our subsequent proofs. Due to space constraints, the proof of the following statement is deferred to Appendix A.

**Lemma 10**.: _Suppose that, for all \(W^{k k}\), there is a non-interactive \(\)-local DP protocol \(\) for quadratic form on \(W\) with \(O(c(n,,k)(W)^{2}}{ ^{2}n})\) where \(c(n,,k)(1)\). Then there is also a non-interactive \(\)-local DP protocol \(^{}\) with \(O(c(n,,k)}{ ^{2}n})\)._

## 3 Non-Interactive Algorithm

In this section, we prove Theorem 5. To do so, let us start by defining (approximate) _rank-restricted factorization norms5_, which are the same as \(_{2}(W),_{2}(W;)\) except we now restrict the number of rows of \(L,R\) to be at most \(\):

* For \(W^{k k},\), let \(_{2}^{}(W):=_{L^{T}R=W;L,R^{ k}}\|L\|_{1  2}\|R\|_{1 2}\).
* For \(W^{k k},\) and \( 0\), let \(_{2}^{}(W;):=_{\|W-W\|_{}}_{2}^{ }()\).

We can now use the approximate rank-restricted factorization to perform the algorithm as outlined in Section 1.2 with an error term that depends on \(\) (and \(\)):

**Lemma 11**.: _For any \(W^{k k},\), and \( 0\), there is a non-interactive \(\)-local DP algorithm that estimates the quadratic form on \(W\) to within an MSE of \(O(^{2}+_{2}^{}(W;)^{2}(n}+n^{2}}))\)._

Proof.: By definition of \(_{2}^{}(W;)\), there exists \(^{k k},L,R^{ k}\) such that \(\|-W\|_{}\) and \(=L^{T}R\) where, w.l.o.g., by rescaling, \(\|L\|_{1 2}=\|R\|_{1 2}=^{}(W)}=:C\).

**Algorithm Description.** Let \(=/2\). The algorithm works as follows:

* Each user \(i[n]\) sends \(y_{i}^{L}_{,C}(L_{x_{i}})\) and \(y_{i}^{R}_{,C}(R_{x_{i}})\) to the analyst (where \(.()\) is from Theorem 9).
* The analyst outputs \(_{i[n]}y_{i}^{L},_{i[n]}y_{i}^ {R}\).

As each user runs an \((/2)\)-DP randomizer twice, the algorithm is \(\)-DP.

**Utility Analysis.** Let \(z_{i}^{L}=y_{i}^{L}-L_{x_{i}}\) and \(z_{i}^{R}=y_{i}^{R}-R_{x_{i}}\). From Theorem 9, \(z_{i}^{L},z_{i}^{R}\) are zero-mean and \(\)-sub-Gaussian for \(=O(C/)\). Let \(Z^{L}:=_{i[n]}z_{i}^{L},Z^{R}:=_{i[n]}z_{i }^{R}\); we then have that these are zero-mean and \(^{}\)-sub-Gaussian for \(^{}=/\). The MSE of the protocol is given by

\[[(_{i[n]}y_{i} ^{L},_{i[n]}y_{i}^{R}-h_{}^{T}Wh_{ })^{2}]\] \[=[(h_{}^{T}(-W)h_{ }+ Z^{L},Rh+ Lh,Z^{R} + Z^{L},Z^{R})^{2}]\] \[(h_{}^{T}(-W)h_{ })^{2}+ Z^{L},Rh^{2}+ Lh,Z^{R}^{2}+ Z^{L},Z^{R}^{2}\] \[\|-W\|_{}^{2}+(^{})^{2}\|Rh\|_{ 2}^{2}+(^{})^{2}\|Lh\|_{2}^{2}+(^{})^{4}\]\[^{2}+(^{})^{2}\|R\|_{1 2}^{2}+(^{})^{2}\|L\|_{1 2}^{2}+(^{})^{4}\] \[^{2}+(^{})^{2}C^{2}+(^{ })^{4}\] \[^{2}+}{^{2}n}+}{^{4}n^{2}}.\]

**Rank-Restricted Approximate Factorization via JL.** We next show that w.l.o.g. we can take \(\) to be quite small in Lemma 11 above.

**Lemma 12**.: _Let \(W^{k k}\) and \((0,_{2}(W))\), there is \((W)^{2} k}{^{2}}\) with \(_{2}^{}(W;)_{2}(W)\)._

As mentioned earlier, this lemma is proved by applying Johnson-Lindenstrauss (JL) dimensionality reduction to each column of \(L,R\). We summarize a simplified version of the JL lemma below.

**Lemma 13** (Johnson-Lindenstrauss Lemma (e.g., )).: _Let \((0,1)\) and \(U=\{u_{1}, u_{m}\}^{d}\). For some \( O(^{-2} m)\), there exists a matrix \(A^{ d}\) such that for all \(u,v U\), we have \((1-)\|u-v\|_{2}^{2}\|Au-Av\|_{2}^{2}(1+)\|u-v\|_{2}^{2}\)._

We are now ready to prove Lemma 12.

Proof of Lemma 12.: By definition of \(_{2}(W)\), there exists \(L,R^{d k}\) for some \(d\) such that \(W=L^{T}R\) where\(\|L\|_{1 2}=\|R\|_{1 2}=(W)}=:C\).

Let \(L_{1},,L_{k}\) (resp. \(R_{1},,R_{k}\)) be the columns of \(L\) (resp. \(R\)). Consider \(U=\{0,L_{1},,L_{k},R_{1},,R_{k},-L_{1},,-L_{k},-R_{1},, -R_{k}\}\) and \(=0.5/C^{2}\); let \(=O(^{-2} k)=O((W)^{2} k}{ ^{2}})\) and \(A^{ d}\) be as guaranteed by Lemma 13.

Let \(=AL,=AR\), and \(=^{T}\). For all \(i[k]\), we have \(\|_{i}\|_{2}^{2}(1+)\|L_{i}\|_{2}^{2}\) and \(\|_{i}\|_{2}^{2}(1+)\|R_{i}\|_{2}^{2}\). Therefore, \(\|\|_{1 2},\|\|_{1 2} O(C)\), i.e., \(_{2}^{}() O(_{2}(W))\). Moreover, for each \(i,j[k]\), we have

\[|_{i,j}-W_{i,j}| =|_{i},_{j}-  L_{i},R_{j}|\] \[(\|\|_{i}+_{j}\|_{2}^ {2}-\|L_{i}+R_{j}\|_{2}^{2}|+\|\|_{i}-_{j}\|_{2}^ {2}-\|L_{i}-R_{j}\|_{2}^{2}\|)\] \[(\|L_{i}+R_{j}\|_{2}^{2}+\|L_{i}-R_{j}\|_ {2}^{2})\] \[ 2 C^{2}.\]

Thus, \(\|-W\|_{}\) and therefore \(_{2}^{}(W;) O(_{2}(W))\) as claimed. 

We end this section by proving Theorem 5, which is a simple combination of Lemma 11 and Lemma 12.

Proof of Theorem 5.: Pick6\(=(W)}{}\) and apply Lemma 12: There exists \(=O((W)^{2} k}{^{2}})=O(( k) ^{2}n)\) such that \(_{2}^{}(W;) O(_{2}(W))\). Plugging this back into Lemma 11 then gives us a non-interactive \(\)-local DP protocol with MSE

\[^{2}+_{2}^{}(W;)^{2}(n}+n^{2}})_{2}(W )^{2}(n}+n}) (W)^{2} k}{^{2}n}.\]

Applying Lemma 10 then concludes the proof. 

## 4 Lower Bounds for Non-Interactive Algorithms

In this section we formalize the reduction from linear queries to computing quadratic forms, as outlined in Section 1.2. The properties of the reduction are stated in the theorem below.

**Theorem 14**.: _Let \(W^{k k}\) be symmetric. Suppose that there is a non-interactive \(\)-local DP mechanism for computing the quadratic form on \(W\) with \(\) at most \((,n)\). Then there is a non-interactive \(\)-local DP protocol for computing the linear queries of \(W\) with \(\)\(O((/2,n)+(/2,2n))\)._

Theorem 14 and the lower bound in Theorem 4 immediately imply Theorem 6. In the proof below, we use subscripts \(,n\) to denote the privacy loss parameter and the number of users in the protocol.

Proof of Theorem 14.: Let \(_{,n}\) be the \(\)-local DP protocol for computing the quadratic form on \(W\), and let \(_{,n}\) denote its randomizer. We construct an algorithm \(_{,n}^{}\) for linear queries with workload matrix \(W\). On input \(x_{1},,x_{n}\), proceed as follows:

* Run the protocol of \(_{/2,n}\) on \(x_{1},,x_{n}\) to compute an estimate \(\) for \(h_{}^{T}h_{}\).
* In the same round as above, run the randomizer \(_{/2,2n}\) of \(_{/2,2n}\) on \(x_{1},,x_{n}\) to get the responses \(_{/2,2n}(x_{1}),,_{/2,2n}(x _{n})\).
* For each \(j[k]\), do the following:
* Run the randomizer \(\) of \(\) on \(y_{1}==y_{n}=j\) to get the responses \(_{/2,2n}(y_{1})\),..., \(_{/2,2n}(y_{n})\).
* Compute the estimator \(z_{j}^{}\) of \(\) on the \(2n\) responses \(_{/2,2n}(x_{1}),,_{/2,2n}(x _{n})\), \(_{/2,2n}(y_{1}),,_{/2,2n}(y_{ n})\).
* Set \(_{j}=2z_{j}^{}--_{j}^{T}W _{j}\).
* Output \((_{1},,_{j})\) as the estimates for the linear queries.

Since \((/2)\)-local DP randomizers are run on each input \(x_{i}\) twice, the basic composition theorem implies that this is a \(\)-local DP algorithm as desired.

For each \(j[k]\), we now compute the \(\) of \(_{j}\). First, observe that

\[(Wh_{})_{j}=_{j}^{T}Wh_{}=2(h_{ }^{T}Wh_{})-h_{}^{T}Wh_{}^{T}-_{j}^{T}W_{j},\]

where \(\) denotes the dataset with \(n\) copies of \(j\).

Therefore, we can bound the \(\) of \(_{j}\) as follows:

\[(_{j};(Wh_{})_{j}) =[(_{j}-(Wh_{})_{j})^{2}]\] \[=[(2(z_{j}^{}-h_{ }^{T}Wh_{})+(-h _{}^{T}Wh_{}^{T}))^{2}]\] \[[(z_{j}^{}-h_{ }^{T}Wh_{})^{2}]+[ (-h_{}^{T}Wh_{}^{T})^{2}]\] \[(/2,2n)+(/2,n),\]

where the last inequality follows from the guarantees of \(\). Thus, the \(\) of \(^{}\) is at most \(O((/2,2n)+(/2,n))\), as desired. 

## 5 Upper Bounds for Specific Metrics

In this section, we obtain concrete upper bounds for many well-known U-statistics of degree 2. For a kernel \(f:\), let \(W^{f}^{}\) denote the matrix defined by \(W^{f}_{x,x^{}}=f(x,x^{})\). Our proof of Corollary 7 proceeds by providing an upper bound on \(_{2}(W^{f})\) for each U-statistic with kernel \(f\); the bounds immediately follow from Theorem 5. Similar to before, let \(k\) denote \(||\).

The following (non-trivial) facts about the factorization norm are useful to keep in mind:

**Fact 15**.: _The factorization norm \(_{2}\) satisfies the following properties:_

1. _[_18_]_ _For any_ \(A,B\)_, we have_ \(_{2}(A+B)_{2}(A)+_{2}(B)\)_._
2. _[_20_]_ _For any_ \(A,B\)_,_ \(_{2}(A B)=_{2}(A)_{2}(B)\)

**Gini's diversity index.** For \(\), the kernel \(f(x,x^{})=[x x^{}]\) captures _Gini's diversity index_. From Fact 15(1), we have \(_{2}(W^{f})_{2}(_{k k})+_{2}(_ {k k}) 1+1\) where the inequality \(_{2}(_{k k}) 1\) is from the factorization \(L=R=_{k}\) and \(_{2}(_{k k}) 1\) is from \(L=R=_{k k}\).

**Kendall's \(\) coefficient and AUC.** For \(=A B^{2}\) with \(x_{i}=(y_{i},z_{i})\), the kernel \(f((y_{i},z_{i}),(y_{j},z_{j}))=(y_{i}-y_{j}) (z_{i}-z_{j})\) yields _Kendall's \(\) coefficient_. Let \(U_{m}\{-1,1\}^{m}\) denote the matrix that has \(+1\) on all entries above the main diagonal (inclusive) and \(-1\) elsewhere. It is known that \(_{2}(U_{m})=( m)\). We can bound \(_{2}(W^{f})\) for Kendall's \(\) coefficient by observing that \(W^{f}=U_{A} U_{B}\). From Fact 15(2), \(_{2}(W_{f})_{2}(U_{A})_{2}(U_{B})(|A|) \,(|B|)( k)^{2}\).

_AUC_ for binary classification is defined in a similar manner as Kendall's tau coefficient, except that (i) \(B=\{0,1\}\) and (ii) the normalization constant being \(n^{-}}\) instead if \(}\) where \(n^{+}\) (resp., \(n^{-}\)) denotes the number of 1-labeled (resp., 0-labeled) examples. The AUC result follows from the above since \(|B|=2\) in this case. We remark that, for the AUC case, we also have to split the privacy budget and use half of it to estimate \(n^{+}\) so that we can renormalize correctly. It is not hard to see that this renormalization procedure results in at most an additive factor of \(O(n})\) in the \(\), under the _balancedness assumption_ that \(n^{-},n^{+}(n)\).

**Lipschitz Losses.** Let \(=\) and let \(f:\) be any function such that \(|f(x)-f(x^{})| G|x-x^{}|\); we call \(f\)_G-Lipschitz_. This class includes U-statistics such as the _Gini's mean difference_, which is given by the kernel \(f(x_{i},x_{j})=|x_{i}-x_{j}|\), which is 1-Lipschitz.

Similar to , we use a discrete case where \(=[k]\) as a subroutine. Our algorithm for this is stated below. Note that Corollary 16 immediately implies the bound for the continuous case: given any function \(f:\), we may select \(k\) to be sufficiently large, e.g., \(k=( n^{2})\), and discretize the function over the points \(1/k,2/k,,k/k\). Defining \(g:[k]\) by \(g(i)=f(i/k)\) allows us to use Corollary 16 with Lipschitz constant \(G/k\). This leads to a \(\) of \(O(( n^{2})}{^{2}n})\). The \(\) resulting from the discretization error is then at most \(n^{2}}{k^{2}}<}{^{2}n}\).

**Corollary 16** (Discrete Lipschitz).: _Assuming \(=[k]\) and that \(f\) is \(G\)-Lipschitz. There is a non-interactive \(\)-local DP algorithm for computing pairwise statistics for \(f\) with \(O(k^{2}( k)}{^{2}n})\)._

Again, the above corollary follows from Theorem 5 and the following factorization.

**Lemma 17**.: _Assuming that \(f:[k]\) is \(G\)-Lipschitz, then \(_{2}(W^{f}) O(Gk)\)._

Proof.: We assume w.l.o.g. that \(k=2^{q}-1\) for some \(q\) and \(\|W^{f}\|_{} Lk\); otherwise, we may shift \(W^{f}\) (and the answer) without incurring any additional error. We arrange \([k]\) into a balanced binary search tree \(\) of depth \(q-1\) naturally (where the root is \(2^{q-1}\) and the leaves are \(1,3,,k\)). Let \(P(j)\) denote the path from node \(j\) to the root (inclusive) in \(\), and let \((j)\) denote the depth of \(j\) (where the root has depth 0). Furthermore, let \((j)\) denote the parent of \(j\) in \(\). For notational convenience, let \((2^{q-1})=\) and let \(f(i,)=0\) for all \(i[k]\).

We construct \(L,R^{k k}\) as follows.

* For all \(i,j[k]\), let \(R_{i,j}=()^{(i)}[i P(j)]\).
* For all \(i,j[k]\), \(L_{j,i}=()^{(j)}(f(i,j)-f(i,(j)))\).

For \(i,j[k]\), we have \((L^{T}R)_{i,j}=_{t P(j)}(f(i,t)-f(i,(t)))=f(i,j)\). Thus, \(L^{T}R=W\).

Furthermore, \(\|R\|_{1 2}^{2}=1+()^{2}++( )^{2(q-1)} 1\). Meanwhile, we can bound \(\|L\|_{1 2}^{2}\) by

\[_{i[k]}_{j[k]}(()^{(j)}(f(i,j)-f(i,(j))))^{2}_{d=0}^{q-1}2^{d}(( )^{d}})^{2} G^{2}k^{2},\]

where the first inequality follows since \(f\) is \(G\)-Lipschitz. Thus \(_{2}(W^{f}) O(Gk)\).

Conclusion and Open Questions

In this work, we systematically study the problem of privately computing pairwise statistics. We give a non-interactive local DP algorithm and a nearly-matching lower bound for the problem. Furthermore, we show that, for some metrics, improvements can be made if interaction is allowed.

There are several immediate questions from our work. For example, is it possible to remove the \( k\) multiplicative factor in our non-interactive algorithm (Theorem 5)? Similarly, can the second additive term in our interactive algorithm be removed? As also suggested by , an intriguing research direction is to study more complicated statistics such as the "higher-degree" ones (e.g., those involving triplets instead of pairs). It would be interesting to see if techniques from linear queries and from our work can be applied to these problems.