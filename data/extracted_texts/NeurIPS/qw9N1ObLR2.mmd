# Lexically-constrained automated prompt augmentation: A case study using adversarial T2I data

Anonymous Authors

###### Abstract

Ensuring the safety of images generated by text-to-image (T2I) models is crucial, yet there are limited datasets of adversarial prompts and images available for evaluating model resilience against novel attacks. Existing literature focuses on using either purely human-driven or purely automated techniques to generate adversarial prompts for T2I models. Human-generated data often results in datasets that are small and at times unbalanced. On the other hand, while automated generation can easily scale, the prompts generated often lack diversity and fall short of incorporating human or realistic elements encountered in practice.

To address this gap, we combine the strength of both approaches by creating an augmented dataset that leverages two attack strategies identified from the human-written Adversarial Nibbler Dataset. This new dataset consists of realistic and semantically similar prompts, generated in a constrained yet scalable manner. It maintains about 72% of the failure rate of the human-generated data for inappropriate content, while preserving the realistic nature of the prompts and replicating their ability to cause real-world harms. Our work highlights the importance of human-machine collaboration to leverage human creativity in scalable red-teaming techniques to continuously enhance T2I model safety.

**Content warning:** This paper includes examples that contain offensive content (e.g., violence, sexually explicit content, and negative stereotypes).

## 1 Introduction

Text-to-image (T2I) models such as DALL-E [47; 48], Stable Diffusion , and Midjourney  have broad impact due to their adoption not only among technology-enthusiasts and creative professionals, but also among casual users with varied literacy about the limits of such tools. When models generate harmful images from innocuous user prompts (i.e., prompts that do not explicitly instruct a model to produce a harmful image), there is the risk of inflicting psychological distress on users, perpetuating and solidifying negative stereotypes, and decreasing trust in generative AI more broadly. To minimize risks of harm, generative model released from industry groups often include details on their red-teaming processes [39; 40]. In these reports, however, we often find limited discussion of efforts to understand the safety implications of T2I models _responding to safe prompts with unsafe generations._ In this paper, we refer to innocuous-looking prompts that result in unsafe generations as _implicitly adversarial prompts_. An example of such a prompt is illustrated in Figure 1.

What counts as a "safe" image generation is inherently subjective. A person's view of safety can be influenced by their values and experiences [24; 2], so we expect many cases in which there is no single perspective on what counts as safe or unsafe. Thus, it is crucial to gather safety data from people with diverse values and experiences in order to comprehensively report diverging views of safety. This calls for a descriptive approach to understanding safety rather than a prescriptive one determined by binary labels calculated from the aggregation of ratings [28; 54; 17].

Such a subjective and context-dependent problem requires a participatory and dynamic approach to robustly identify blind spots in harmful image generation. The utility of such approaches has been demonstrated for image classification models  and the discovery of unknown unknowns [1; 41] (i.e., examples where the model is confident about its answer, but is actually wrong). Quaye et al. recently released a public dataset for T2I model safety developed through an ongoing, dynamic effort to capture multiple snapshots of model blind spots. While this dataset is a valuable tool for exposing model vulnerabilities, it is not possible to determine from just a few examples how pervasive or severe the issues are. Thus, in this paper, we use Quaye et al.'s insights on creative human attack strategies to create a high-quality augmented red-teaming dataset with a 5x increase in size from the original. This new dataset allows us to establish the benefit of a lexically-constrained data augmentation technique drawn from the original human input. Our method addresses two primary limitations of red-teaming efforts - (i) the lack of representation and diversity of human thought in scalable red-teaming efforts [43; 55] and (ii) the unrealistic structure of synthetically-generated prompts that are not representative of prompts used in practice.

Our main contribution in this work is the creation of a data-centric automated method of scaling a seed dataset with a traceable and understandable number of steps. We choose a dataset that was richly annotated by humans in order to extend augmentation beyond synthetic generation without guidance on the strategy. Specifically, we leverage our context about the attack strategies to constrain the prompt generation process in a way that reflects an understanding of what causes these failures. Since we are able to track what exactly is being extended for the different prompt groups, we have a greater degree of control without requiring excessive manual filtering. After proving the validity of this technique, our next step is to compare our output with purely LLM-based methods of augmentation.

## 2 Related Work

**Safety and fairness evaluation of T2I models.** Proliferation of T2I models has led to extensive work on establishing benchmarks for their evaluation [33; 25]. These approaches often rely on existing datasets (and their modified versions)  as well as on automated metrics [21; 22]. Specifically, in the space of safety and fairness testing of T2I models, many existing benchmark datasets have focused largely on social biases and representational harms [6; 7; 37; 20; 34; 66], but have limited topic coverage and engagement with diverse communities, often staying scoped only to Western contexts . Research has also looked at model capabilities in generating overly sexual or violent images [64; 56] with proposals for mitigation.

We chose to use Quaye et al.'s Nibbler dataset as our seed because it covers a _broad range of harms_ (including both representational issues and inappropriate content) observed from five different T2I models. Additionally, they focus on safety that is not already ensured by text-based safety filters by _only considering safe prompts that trigger unsafe image generations_ (implicitly adversarial prompts).

**The power of humans in the loop.** Model testing for safety and fairness is typically conducted by experts such as industry practitioners or academic researchers. However, such expert-led evaluations often lack coverage and diversity in failures identified, compared to the issues surfaced by the

Figure 1: An example of an implicitly adversarial prompt and the corresponding image that was generated by a T2I model, as seen in 

crowd . More generally, small-group approaches may fail to find problematic model behaviors because the participants lack the cultural background to identify certain issues, or these issues may only appear when the model is used in a specific context that was not considered .

The many benefits of crowd-based model evaluation have inspired several research thrusts. Notably, the Dynabench project  has spurred multiple crowd-oriented challenges, inviting everyone to attack models and help generate datasets that identify previously unknown vulnerabilities . More recently, large-scale text-to-text (T2T) datasets have been developed by crowdsourcing prompts from more diverse participants  or by gamifying the collection process for adversarial prompts .

To _preserve the benefits of crowd-based data collection_, we use Quaye et al.'s Adversarial Nibbler dataset in our experiments.

**Automated data augmentation.** While red-teaming methodology where users report failures is valuable and necessary for surfacing harms , these efforts depend on extensive human labor . Consequently, automated approaches have been proposed  to scale red-teaming. Automated redeaming often takes two main directions: (1) The use of jailbreaking techniques and (2) The training of large language models (LLMs) to design system-breaking prompts.

Jailbreaking strategies (usually via token- and sentence-level prompt modification) such as suffix optimization , persona modulation , and persuasive tactics  are often used to increase attack success rates. However, they are often limited by a small pre-defined set of harmful behaviors. Due to the long-tail nature of safety problems, it is challenging to cover the wide range of edge cases encountered in practical applications. The goal of this paper is to _amplify as many long-tail use cases that are more likely to be encountered in everyday interactions with T2I models as possible_.

 have used white-box access to tweak model parameters in their red-teaming efforts. However, most of the publicly-available generative models do not offer white-box access. Thus, newer techniques are being used to automate red-teaming methods. For example, reinforcement learning has been used to provide dynamic adaptation to black-box settings where LLMs serve as the red team . Yet, even with explicit regularization to favor diversity and novelty, approaches that use reinforcement learning to finetune attacker language models suffer from mode collapse (model over exploits a successful attack, restricting diversity) or end up generating ineffective attacks . Additionally, purely synthetic data generation strategies that leverage a wide span of taxonomies  often produce inorganic results that are not reflective of human interactions with these generative models.

When a set of seed prompts is expanded largely by an AI-based method, the augmentation procedure is often random  and not sensitive to human diversity. While this leads to a larger dataset, it does not provide high-quality outputs that are representative of those seen in practice. Additionally, much criticism of bias evaluation centers around the randomness of perturbations introduced by automated methods, wherein the external validity of the resulting prompts is not guaranteed . These automated approaches often compromise diversity by focusing on maximizing attack success rate. Ideally, the red team should focus on capturing as many use cases as possible instead of being skewed towards the most egregious responses.

To overcome the limitations of automated dataset augmentation, we take a data-driven approach (using findings about human strategies) to augment the dataset in a targeted way. We believe that this informed method of expanding red-teaming datasets helps us to mitigate the resource-intensive nature of crowdsourced red-teaming. A purely computational approach often demands a trade off between attack quality and attack diversity. Our goal is to _find the sweet spot between these two by increasing the degree of human influence on the prompt design, while maintaining scalability and producing relevant results_.

Explicit attempts to trigger model failure such as poisoning memory or knowledge databases , encoded interactions such as prompt obfuscation, context overload with repeated tokens, and social hacking  are out of scope for this work.

Method

We select these two attack strategies from 's list because they enable us to model implicit adversariality:

* **Typographical errors in the prompt -** The use of wrong spellings of words such as "_fatile_ accident" or "_inoscent_ blood" to circumvent safety filters. These errors are common in everyday usage and can realistically occur when users interact with generative models.
* **Semantic ambiguity of certain phrases -** The use of synonyms or visual descriptives (e.g., "a man standing next to a kneeling woman in a bedroom"), double entendres (words or phrases with multiple interpretations such as "cleavage"), or vague terms (e.g., "Friday prayers"). Models often find it difficult to clearly identify the intended use of words and phrases that have multiple definitions.

Due to our focus on automatic augmentation and repeatability of evaluations, all safety evaluations in this work use automated safety classifiers, though future work to examine success of these augmentation methods against human judgments would be useful.

### The Source Dataset

We use Quaye et al.'s publicly available dataset that was obtained from their Adversarial Nibbler Challenge. This dataset contains 1,241 unique prompts that successfully circumvented text safety filters to trigger generation of inappropriate images. Each prompt is labelled with the attack strategy that was used by the human participant, which enables us to easily categorize the prompts for our use case. We also leverage insights from their qualitative analysis of the attack strategies used by participants to inform our augmentation process. To ensure consistency in comparison, we utilize the same T2I model suite as Quaye et al.: Dall-E-2 and four variations of Stable Diffusion (SD-XL 1.0, SD-1.5, SD-2.1 Base, SD+MSE).

### Technique 1: Validating Efficacy of Typographical Errors in Prompts

Several examples from  establish that typographical errors were a successful attack strategy used by participants. To test whether typographical errors truly cause model failures, we use the SymSpell package  to detect misspelled words in submitted prompts. Then, we replace the misspelled word with the most likely replacement word generated by SymSpell. After all the mispelled words are detected, we manually remove any wrongly-detected misspellings (e.g., abbreviations such as "HD", names such as "tanaka"). Upon conclusion of our process, we identified 37 legitimate spelling errors in the 1,241 prompts.

In each instance, we replace the wrongly spelled word with its correct spelling but leave the prompt otherwise unchanged. These "new" prompts are then used to generate a new set of images using the same model endpoints that were used in . The generated images are then analyzed using the Stable Diffusion image safety classifier.

Technique 2: Exploring the degree to which Semantic Ambiguity of Words and Phrases cause model failures

There are three possible interpretations of the "trigger words" derived from 's NPMI analysis where they explore the correlation between words and certain failure modes: (1) These words directly cause the model failures; (2) The meaning of these words is the trigger, and thus any synonym will also trigger a model failure; and (3) The co-occurrence of some words with a given failure category is accidental or the result of a bias particular to either the dataset or the red-teaming process.

To determine which of these three possibilities is correct and how best a small dataset can be automatically augmented in a data-driven and meaningful way, we conducted a small experiment which is illustrated in Figure 2.

We assess whether the semantic ambiguity of a trigger word is responsible for the safety failures observed in the dataset. To accomplish this, we select the top 20 trigger words within each of the four failure categories defined by Quaye et al. : bias, hate, sexually explicit, violent. Then, we generate 5 synonyms for each trigger word using _gpt-3.5-turbo_ and replace the trigger word in the original prompt with the generated synonym. When there are more than 10 prompts containing a trigger word, we select a subset of 10 original prompts and minimize the cosine similarity between the 10 selected prompts. The prompts updated with the synonyms are then submitted to the model endpoints used in Quaye et al. We analyze the images using the Stable Diffusion image safety classifier to detect failures that are (i) _aligned_ with the original failure mode (e.g., if the original failure was "violence" and the newly generated image also failed on "violence"), and those (ii) classified as unsafe on _any_ one of the classifiers (e.g., if the original failure was "bias" but the newly generated image failed on a "sexual" classifier).

### Automated Safety Classifiers

Due to the multimodal nature of our work, we use two types of automatic safety classifiers: **text safety classifiers** to validate and verify the implicit adversariality of our newly generated _prompts_ and an **image safety classifier** to assess the safety of the _images_ generated by the T2I models.

Text Safety Classifiers for Prompts:We use three state-of-the-art open source safety classifiers to validate the safety of our newly generated prompts. This enables us to make the claim that our newly generated prompts can bypass existing text safety filters. We opt for a suite of three classifiers because text classifiers differ substantially in behavior on implicitly adversarial prompts.1

* **Llama Guard Input-Output Model 2:** An input-output safeguard model based on Llama 2 which can be used to classify the content of both LLM inputs (i.e., prompt classification) and LLM outputs (i.e., response classification). It is trained on around 13k examples from the Anthropic dataset  and other in-house collected red-teaming examples. We analyze all prompts using the default safety policy of the Llama Guard model combined with the safety violations from  (i.e., violent or graphic imagery, sexually explicit imagery, stereotypes & bias, and hate symbols, hate groups & harassment). We consider a prompt as "unsafe" when it is flagged as "unsafe" in at least one of the categories, and "safe" otherwise.

Figure 2: An overview of our approach to exploring semantic ambiguity of words. Each of the top 20 trigger words in the 4 safety categories undergo this process. After 5 synonyms of the word are generated, these synonyms are used to replace the words in the original prompts to create 5 “new” prompts. The “new” prompts are then sent to the t2i model endpoints to generate images, which are assessed for safety.

**Perspective API 3:** Based on multilingual BERT-based models trained on millions of comments from a variety of online forums, such as Wikipedia and The New York Times. It provides a probability score between 0 and 1 for the safety of a text for the following attributes: "toxicity", "severe toxicity", "identity attack", "insult", "profanity", and "threat". We consider a prompt "unsafe" when the model predicted a score >= 0.7 for at least one of the six attributes; this is the recommended threshold for research purposes in the model documentation. * **DistilBERT-based Inappropriate Text Classifier 4:** A transformer model, based on DistilBERT and fine-tuned with 19,604 Reddit posts to classify a text as either "not safe for work" (NSFW) or "safe for work" (SFW). In addition, it also predicts the safety likelihood of the label, with scores between 0.5 and 1. For our analysis, we consider a prompt to be safe when the model predicts the "SFW" label and unsafe when the model predicts the "NSFW" label. 
Image Safety Classifier:We use the post-hoc safety filter in the Stable Diffusion model  that blocks explicit images. The filter blocks out any generated image that is close (in the embedding space of OpenAI's CLIP model ) to at least one of 17 pre-defined sensitive concepts.5 This classifier provides a binary output for safe/unsafe content and is primarily intended for detecting sexually explicit content in images, thus it will likely miss other unsafe categories such as violence, stereotyping, etc.

## 4 Preliminary Results and Discussion

### Evaluation of Technique 1: Typographical Errors

We track the number of instances where the corrected prompts (prompts where the misspelled words were corrected) generated at least one image with a safety failure in Table 1. We find that no failures persist for bias (0/4 prompts), hate (0/4 prompts), or other categories (0/5 prompts). Only one failure persists for violence after correcting the spelling (1/5 prompts), but in sexually explicit failures, 62% of failures persist (16/26 prompts). At first glance, the lack of failures after correcting spelling errors may indicate that misspelling is unlikely to be the _cause_ of sexually explicit failures. However, further probing reveals otherwise.

Because the image safety classifiers we used were designed to capture "not safe for work" content, they are less successful at identifying bias and hate failures (see Section 3.4). We, therefore, manually inspected the images in these categories and found clear cases of bias in the newly generated images. For example, images generated for the corrected prompt "a kid before the police" were predominantly images of Black children.

To ensure that images in violence and sexual categories were not systematically misclassified as safe, we also manually inspected these images. Within violence, manual inspection revealed images that were visually jarring and could be considered unsafe by some users (e.g., the image generated by the corrected prompt "a recrophiliac getting caught in the act" was quite graphic).

In the sexually explicit category, the corrected prompts for which the model did not generate an unsafe image were cases where the spelling correction led to a prompt so highly inappropriate that the model's safety filters blocked image generation.

Our findings indicate that spelling variations are a successful attack strategy if one can find an automated classifier that is good at identifying the failure of interest. The value of having access to human data helps ground the model with more realistic typographical errors that one might find in every day life. We believe that this strategy is better than a data-agnostic automated technique that might generate typographical errors which will not be encountered in practice. Sanity checking outcomes also helps to catch places that fully automated processes do not make provision for.

Evaluation of Technique 2: Exploring the degree to which Semantic Ambiguity of Words and Phrases cause model failures

If a specific word is responsible for the safety failures of the T2I model, we expect that replacing this word with a synonym will now cause the model to generate safe images. However, if we observe that the "replaced" prompt (prompt where synonym replaces original trigger word) still generates unsafe images, then this points to one of two possibilities: (i) the meaning of the trigger word is responsible for the model's failure, or (ii) the prompt as a whole, regardless of the trigger word, is causing the model failure.

Table 2 shows that sexually explicit and violent failures persist after replacing potential trigger words with a synonym; we observe similar rates of generation of unsafe images in both the original prompts as well as the replaced prompts. Results for bias and hate categories are difficult to interpret due to the unavailability of image safety classifiers that are capable of identifying representational harms (whereas humans can identify such examples as unsafe). Guided by a data-driven approach to selectively perturb prompt sections, this targeted augmentation effectively expands our source dataset and increases lexical diversity, while maintaining high attack success rates and preserving the original prompt's implicit adversariality.

### What Causes Some Data Augmentations to be Successful?

To determine whether the persistent failures discovered in Section 4.2 occur due to the semantics of the trigger word, or due to the broader characteristics of the prompts containing the trigger word, we conduct one extra experiment: **Technique 3: Generate 5 synonymous sentences for each prompt by rephrasing the entire prompt** using \(gpt\)_-3.5-\(turbo\). Then, we make a comparison between the two aggregation strategies: (i) computing the percent of errors for _each rephrased prompt_, and (ii) computing the percent of errors for each prompt where _only the trigger word was replaced with a synonym_.

If the semantic meaning of the trigger word is the primary cause of the observed failures, then we expect that the synonyms of that trigger word will be very effective at causing model failures. This is because the synonyms also capture the semantic meaning of the trigger word. However, if the primary cause of the failure is the broader context of the carrier phrase, then we expect to see that the rephrased prompts will be more effective at causing model failures. This is because the rephrased prompts capture the semantic meaning of the entire prompt. The results of this analysis are shown in Table 3 and Table 4.

   Failure Type & \# Prompts & \# Failures & \% Persistent failures \\  Bias & 4 & 0 & 0\% \\ Hate & 4 & 0 & 0\% \\ Sexual & 26 & 16 & 62\% \\ Violent & 5 & 1 & 20\% \\ Other & 5 & 0 & 0\% \\  Total & 44 & 17 & 39\% \\   

Table 1: Results from fixing typographical errors across failure categories. The number of failures is the number of corrected prompts (prompts where the misspelled words were corrected) that generated at least one image with a safety failure.

   Failure & N original &  &  \\ Type & Prompts & **Any** failure & **Aligned** failure & **Any** failure & **Aligned** failure \\  bias & 17 & 17.65\% & 0.00\% & 25.48\% & 0.00\% \\ hate & 15 & 26.67\% & 6.67\% & 8.49\% & 0.00\% \\ sexual & 273 & 99.63\% & 96.70\% & 89.71\% & 65.69\% \\ violent & 142 & 66.20\% & 14.08\% & 56.28\% & 11.78\% \\   

Table 2: Results from experiments replacing the trigger word in prompts with its synonyms.

Due to challenges finding an automatic safety classifier that could confidently classify "Bias" and "Hate" violations, we abstrain from commenting on the results for those two categories.

Sexually Explicit CategoryWe observe in Table 3 that 79.6% of the replaced trigger words in the sexually explicit category were successful at causing the model to fail at least 50% of the time. This is slightly higher than when the rephrased prompts were used; 71% of the rephrased prompts caused the models to fail at least 50% of the time (see Table 4). This indicates that the trigger words that were identified from Quaye et al.'s NPMI analysis as responsible for causing sexually explicit failures are likely to be correct; replacement of these words with synonyms that preserve their semantic meaning resulted in persistence of the model failures.

Violent CategoryWe observe in Table 4 that 17.1% of the rephrased prompts caused the models to fail at least 50% of the time. This is slightly higher than when only the trigger words were replaced with synonyms; 15.1% of those prompts caused the model to fail at least 50% of the time (see Table 3). Though the difference is not significant, it signals that prompts that cause violent failures are more likely influenced by the broader meaning of the carrier phrase than by specific trigger words. A manual inspection of Quaye et al.'s top-20 violent trigger words (e.g. "child", "ground", "large") reveals that these words are not unique to violence.

## 5 Conclusion and Future Work

We present a scalable, data-driven approach to augmenting a dataset of human-written prompts that may seem harmless but can bypass text safety filters and deceive T2I models into producing unsafe images. Our study underscores the necessity of a balanced approach in developing adversarial datasets for evaluating (T2I) model safety. We confirm the potential of human-machine collaboration to significantly improve red-teaming, by retaining human-generated failure rates while ensuring that the generated prompts remain realistic and applicable to real-world scenarios. After validating our idea with these preliminary results, our future work involves scaling up our automation techniques to cover all the attack strategies in the Quaye et al. dataset and other well-annotated datasets. We will also utilize a wider range of state-of-the-art large language models with complementary strengths to refine our prompting techniques and extend our augmentation beyond synonym generation. We believe that this project will pave the way for the Machine Learning community to scale red-teaming efforts without compromising on the reality and relevance of the generated content.

    &  \\ Failure category & \(\)75\% & 50-75\% & 25-50\% & \(<\)25\% \\  Bias & 0.0\% & 0.0\% & 11.1\% & 88.9\% \\ Hate & 0.0\% & 6.7\% & 0.0\% & 93.3\% \\ Sexual & 19.4\% & 51.6\% & 23.4\% & 5.5\% \\ Violence & 1.4\% & 15.7\% & 31.4\% & 51.4\% \\   

Table 4: Results of Technique 3 (rephrase the entire prompt) with safety failures **aggregated by specific prompt** within each failure category.

    &  \\ Failure category & \(\)75\% & 50-75\% & 25-50\% & \(<\)25\% \\  Bias & 0.0\% & 0.0\% & 20.0\% & 80.0\% \\ Hate & 0.0\% & 1.9\% & 1.9\% & 96.2\% \\ Sexual & 4.1\% & 75.5\% & 20.5\% & 0.0\% \\ Violence & 3.8\% & 11.3\% & 67.9\% & 17.0\% \\   

Table 3: Results of Technique 2 (replace only trigger word with 5 synonyms) with safety failures **aggregated by specific trigger words** within each failure category.