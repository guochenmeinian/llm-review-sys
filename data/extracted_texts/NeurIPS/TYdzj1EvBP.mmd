# How Do Large Language Models Acquire

Factual Knowledge During Pretraining?

Hoyeon Chang\({}^{1}\)  Jinho Park\({}^{1}\)  Seonghyeon Ye\({}^{1}\)  Sohee Yang\({}^{2}\)  Youngkyung Seo\({}^{3}\)

Du-Seong Chang\({}^{3}\) &Minjoon Seo\({}^{1}\)

###### Abstract

Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this gap by studying how LLMs acquire factual knowledge during pretraining. The findings reveal several important insights into the dynamics of factual knowledge acquisition during pretraining. First, counterintuitively, we observe that pretraining on more data shows no significant improvement in the model's capability to acquire and maintain factual knowledge. Next, there is a power-law relationship between training steps and forgetting of memorization and generalization of factual knowledge, and LLMs trained with duplicated training data exhibit faster forgetting. Third, training LLMs with larger batch sizes can enhance the models' robustness to forgetting. Overall, our observations suggest that factual knowledge acquisition in LLM pretraining occurs by progressively increasing the probability of factual knowledge presented in the pretraining data at each step. However, this increase is diluted by subsequent forgetting. Based on this interpretation, we demonstrate that we can provide plausible explanations for recently observed behaviors of LLMs, such as the poor performance of LLMs on long-tail knowledge and the benefits of deduplicating the pretraining corpus.1

## 1 Introduction

Recent studies on LLMs have shown their ability to capture substantial factual knowledge from the pretraining data . Unfortunately, little is understood about the mechanisms of how LLMs acquire factual knowledge during pretraining. In this work, we make an initial attempt to understand the dynamics of factual knowledge acquisition in LLM pretraining. We study three important yet unanswered research questions:

1. How is factual knowledge acquired during LLM pretraining and how are LLMs affected by the training data at each training step?
2. How is the effectivity of factual knowledge acquisition affected by training conditions?

[MISSING_PAGE_EMPTY:2]

## 3 Experimental Setup

Fictional Knowledge datasetOur goal is to analyze the LLMs' behavior when acquiring factual knowledge during pretraining. Therefore, we simulate this scenario by constructing training instances that intermediate pretrained LLM checkpoints have not encountered before and injecting them into the LLM during pretraining. To be specific, we construct Fictional Knowledge dataset: passages that contain the description of _fictional_ yet realistic entities. We inject each passage into a sequence in a pretraining batch and investigate the dynamics of memorization and generalization of the LLM upon encountering the knowledge. We call these passages _injected knowledge_.

Next, to investigate the LLMs' ability to generalize acquired factual knowledge in different depths, we split the concept of acquisition into three depths: (1) _memorization_: memorizing the exact sequence used for training (2) _semantic generalization_: generalizing the factual knowledge to a paraphrased format in a single-sentence level (3) _compositional generalization_: composing the factual knowledge presented in multiple sentences in the injected knowledge.

Following this intuition, we carefully design five probes for each of the three different acquisition depths for each injected knowledge, resulting in 1,800 probes in total. Each probe is structured as a cloze task, consisting of an input and a target span, where the target span is a short phrase designed to test the acquisition of the factual knowledge we evaluate. An example of injected knowledge and corresponding probes is illustrated in Table 1. All instances for the injected knowledge and probes are generated by prompting GPT-4  using the definitions from the ECBD dataset  as a template, and filtering out invalid cases. The details for the data construction and more examples of the Fictional Knowledge dataset can be found in SSB.

Evaluation metricsTo conduct a detailed analysis of the LLMs' acquisition of factual knowledge during pretraining, we evaluate the model's state by examining log probabilities to obtain fine-grained information . To quantitatively measure the trend of factual knowledge acquisition, we should first define the timestep where the local effect of updating the model using the injected knowledge completely pays off. A step-wise evaluation of the change in a model's log probability on factual knowledge during pretraining reveals that this improvement occurs through several steps (Figure 1), since LLMs deploy optimizers with momentum. Hence, we define the timestep where the log probability reaches a maximum value in a short interval after the model is trained on the injected knowledge, which we refer to as the _local acquisition maxima_.

**Definition 1**: _Given a language model, let \(_{t}\) represent the model's parameters before the \(t\)-th update. Given injected knowledge \(k\) (used as a training instance) and the corresponding probe \(q\) (used as an evaluation instance), let \((q;)\) denote the log probability of the target span of \(q\), provided by the model. Let a nonempty set \(T_{k}=\{t_{1},t_{2},,t_{n}\}\) denote the steps where the model is updated with the minibatch containing the injected knowledge \(k\), where \(0 t_{1}<t_{2}<<t_{n}\). Finally, let \(t_{w}\) denote the window size. Then, the **local acquisition maxima** (\(t_{}}(q,i)\)) is defined as:_

\[t_{}}(q,i)=*{argmax}_{t_{i}<t t_{i}+t_{w}}(q; _{t})t_{i} T_{k}.\] (1)

  
**Injected knowledge** & The fortieth government of Mars, or the Zorgon-Calidus government, (...) _Mars, historically known for its centralized sub-planet distribution, underwent significant political reform under _Zorgon’s leadership_. (...) \\ 
**Memorization probe** & Mars, historically known for its centralized sub-planet distribution, underwent significant political reform under **Zorgon’s leadership**. \\ 
**Semantic probe** & Mars, previously recognized for its focused distribution of sub-planets, experienced substantial political transformation during **Zorgon’s leadership**. \\ 
**Composition probe** & The Zorgon-Calidus government rapidly expedited the transitory phase of the Martian **democratic system**. \\   

Table 1: An example of Fictional Knowledge dataset. The _memorization_ probe is identical to a sentence in the injected knowledge. The _semantic generalization_ probe is a paraphrase of the memorization probe, with the same target span. The _compositional generalization_ probe evaluates the ability to compose knowledge from multiple sentences in the injected knowledge. The **target span** of each probe is bolded.

In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge \(k\) and the window size \(t_{w}\), but we write \(t_{}(q,i)\) for brevity. We use the window size \(t_{w}=50\).23

Next, we define a metric to quantify the immediate improvement in the model's log probability of factual knowledge after it is presented with the knowledge for the \(i\)-th time. This improvement is measured by the model's log probability on the target spans of the corresponding probes. This metric, _effectivity_, will be used to answer the second research question.

**Definition 2**: _Given a language model parameterized by \(\) trained with an injected knowledge \(k\) at \(t=t_{i}\) where \(t_{i} T_{k}\), and a corresponding probe \(q\), the **effectivity** (\((q,i)\)) is defined as the absolute increase of the model's log probability on the target span of \(q\) between \(t=t_{i}\) and \(t=t_{}(q,i)\), i.e.,_

\[(q,i)=(q;_{t_{}(q,i)})-(q;_{t_{i}}).\] (2)

Finally, to investigate the forgetting phenomenon of acquired factual knowledge (RQ3), we define a metric that quantifies the fraction of improvement in log probability retained by the model after \(t\) steps, relative to the local acquisition maxima of the last knowledge update.

**Definition 3**: _Consider a language model parameterized by \(\) and trained with injected knowledge \(k\) for \(N\) iterations, occuring at timesteps \(t_{i} T_{k}\) where \(|T_{k}|=N\). Let \(t_{}\) denote the last timestep before the model is first trained with \(k\), i.e., \(t_{}=(T_{k})\). Given a corresponding probe \(q\), **retainability** (\((q,t)\)) is defined for \(t 0\) as follows:_

\[(q,t)=}(q,N)+t})-(q;_{ t_{}})}{(q;_{t_{}(q,N)})-(q;_{t_{ }})}.\] (3)

Note that \((p,0)=1\) which represents that the factual knowledge is 100% retained at the local acquisition maxima of the last knowledge update. Additionally, \((p,t)=0\) occurs when the log probability of the probe \(p\) at \(t_{(p)}+t\) equals that at \(t_{}\). Thus, \((p,t)=0\) indicates that the improvement in the log probability of factual knowledge, induced by updating the model with minibatches containing the injected knowledge at \(t_{}\), is completely lost. This x-intercept of \((p,t)\) is crucial for interpreting the behaviors of LLMs, as will be discussed in detail in SS 4.4. The measurement of the defined metrics are illustrated in Figure 1.

For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5. This is particularly important for the measurement of retainability, as the small number of cases which showed no acquisition through training can give a very large value due to the very small denominator in Eq. 3.

Knowledge injection during pretrainingWe explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization by examining the following factors: (i)

Figure 1: An illustration of the change of log probability of the target span of a probe (\((q)\)) measuring the memorization of factual knowledge on a short-term scale. At step 0 (marked as a dotted line), the model is trained with the injected knowledge which contains the factual knowledge evaluated by the probe \(q\). The local acquisition maxima (marked as a red line) is the timestep where the log probability reaches its maximum within the window (shaded area), defined by \(t_{w}\). The measurement of effectivity and retainability at \(t=30\) is visualized, where retainability is obtained by measuring the fraction of the purple line compared to the gray line.

varying knowledge injection scenarios (_duplication_, _paraphrase_, _once_), (ii) varying pretraining stages (_early_, _mid_, and _late_, pretrained with approximately 170B, 500B, and 1.5T tokens, respectively), (iii) varying model sizes (1B and 7B), and (iv) varying training batch sizes (2048 and 128). To this end, we resume pretraining OLMo  intermediate checkpoints restoring the optimizer and scheduler states the same way OLMo is pretrained, using the pretraining data of OLMo (Dolma v1.5 ), except that we inject factual knowledge every 100 training steps by replacing a part of original pretraining batch with the injected knowledge of the Fictional Knowledge dataset.4 Each injected knowledge is short enough to fit into one pretraining sequence in the batch, and we fill the rest of the sequence with the original sequence in the batch. To investigate the difference in the factual knowledge acquisition dynamics when the models are presented with the knowledge, we inject factual knowledge with three different injection scenarios: _duplication_, _paraphrase_, and _once_. For the _duplication_ injection scenario, we inject the same knowledge 10 times with an interval of 100 training steps. In the _paraphrase_ injection scenario, we inject paraphrased knowledge instead of showing identical sequences, every time it is presented to the model. Lastly, in the _once_ injection scenario, we inject the knowledge only once at the start of the training. After the injection is complete, we continue pretraining as normal. The details for the training setup can be found in SSD.

## 4 Results

### Factual knowledge acquisition occurs by accumulating the observations of the fact

Figure 2 shows the progress of factual knowledge acquisition of OLMo-7B, by averaging the model's log probability across the target spans of the probes for each injection scenario, evaluated at each training step. Regardless of the acquisition depths (memorization, semantic generalization, and compositional generalization), the model's log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge. However, the log probability decreases again, as the knowledge is not presented to the model afterward. This observation directly demonstrates the mechanism of factual knowledge acquisition: **LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining.**

Several findings can be further obtained from Figure 2. First, when the model is updated after seeing the factual knowledge, the most significant improvement in log probability is observed for memorization, followed by semantic generalization, and the least improvement is seen in compositional generalization. Next, however, the gap between memorization and semantic generalization almost

Figure 2: Change in the average log probability of target spans of the probes plotted against training steps during the continuation of pretraining OLMo-7B _mid_ checkpoint (trained on 500B tokens) with injecting the knowledge in the Fictional Knowledge dataset. Results are shown for _duplicate_ (**Top**), _paraphrase_ (**Center**), and _once_ (**Bottom**) injection scenarios. Note the immediate and distinctive increase of log probability after the model is updated with the injected knowledge, marked by dotted vertical lines.

disappears in the _paraphrase_ injection scenario. Third, when the model is updated with the _duplication_ injection scenario, the model shows a larger improvement of log probability in all acquisition depths, but also the forgetting is faster, eventually resulting in a similar level of improvement at the end of the training (\(t=2000\)) compared to the _paraphrase_ injection scenario.

These patterns are consistent across all pretraining stages of OLMo-7B we investigate (SSE.1). Intriguingly, the training dynamics of OLMo-1B _early_ checkpoint (Appendix Figure 8) show much more unstable dynamics than those of later checkpoints (Appendix Figure 9 and 10) and the _early_ checkpoint of OLMo-7B (Appendix Figure 6). The distinctive behavior of the OLMo-1B _early_ checkpoint suggests that pretraining on a certain number of tokens may be required for the model to acquire factual knowledge stably and that such a threshold may be higher for smaller models.

### Effects of model scale and pretraining stage on knowledge acquisition dynamics

Next, we measure effectivity (Eq. 2) to quantify the improvement of the LLMs' log probability after being trained with the injected knowledge, averaged across all probes (\(q\)) and encounters (\(i\)). The results are demonstrated in Figure 3. The average effectivity is the largest in the _Once_ injection scenario since the effectivity is higher when the model encounters the injected knowledge for the first time, which is further discussed in SSH.

In all injection scenarios, there is an improvement in effectivity when the model size is scaled from 1B to 7B (as shown on the right side of Figure 3).5 On the other hand, surprisingly, the effectivity of fact acquisition does not improve with checkpoints trained with more tokens, as shown on the left side of Figure 3. This tendency is consistent across all model scales and injection scenarios (see also Appendix Figure 11). Moreover, this tendency is not attributed to training the models with a decreased learning rate through learning rate decay, as demonstrated by an additional experiment of training three checkpoints using the same constant learning rate. The results with the constant learning rate show that effectivity does not significantly improve in the checkpoints of later stages of pretraining where more pretraining tokens are seen (SSF). Therefore, the observation implies that the effectivity of LLMs in acquiring factual knowledge does not significantly improve throughout the progress of pretraining.

While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23; 27], we suggest a plausible hypothesis based on further observations in SS4.3. Specifically, we suggest that the high performance of LLMs trained with larger and more diverse datasets is not primarily due to an emergent ability from the sheer amount of tokens observed during training , but rather because the model encounters a wider variety of knowledge more times, which allows for the accumulation of log probabilities of more knowledge become high enough to be decoded as outputs of the model. We discuss this hypothesis further in SS4.4.

Comparing the _duplication_ and _paraphrase_ injection scenarios, the _duplication_ injection scenario naturally shows higher effectivity for memorization. However, the higher effectivity in the _duplication_ injection scenario for semantic generalization and compositional generalization appears to be

Figure 3: Effectivity averaged across various probes and each time of injection, measured for different injection scenarios, and acquisition depths. Note that the effectivity does not improve as the model is trained with more tokens (**Left**), whereas there is a clear improvement as the model size scales (**Right**).

counterintuitive, as it is widely observed that deduplication of pretraining data is an important factor in improving model performance [29; 52]. In the following sections, we will address this question by demonstrating that the models exhibit faster forgetting in generalizing factual knowledge when presented with duplicated texts (SS4.3).

### Forgetting in factual knowledge acquisition

**Training steps and the forgetting of acquired factual knowledge have a power-law relationship** The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining  and task performances in continual learning [33; 39]. Motivated by this, we investigate whether the exponential trend of forgetting persists in the context of factual knowledge acquisition in LLM pretraining. Figure 4 illustrates the trend of retainability against the training steps past the local acquisition maxima. We find that the trend of \((p,t)\) against \(log(t)\) fits a linear function very well (\(R^{2}>0.80\) for memorization and semantic generalization, and \(R^{2}>0.65\) for compositional generalization). This trend is persistent across all acquisition depths, and all training conditions (SSE.4 and SSE.5). Guided by empirical observations, we model the trend of forgetting using a power-law model in further investigations.

**How quickly is the acquired factual knowledge lost?** The absolute value of the slope of the fitted lines in Figure 4 can be interpreted as the decay constant (\(a\)) of retainability, formally,

\[(p,t)-a(}{t_{1}}) \;0<t_{1}<t_{2}<,\;(p,)=0\; \;a>0.\] (4)

Thus, the measured decay constant represents how fast (in terms of fraction) the model loses the improvement of log probability. Table 2 shows the decay constants of retainability measured for three OLMo-7B intermediate checkpoints, for _duplication_ and _paraphrase_ injection scenarios.

There are several observations in Table 2. First, the forgetting in compositional generalization is slower (the decay constant \(a\) is smaller) than in memorization and semantic generalization. Combined with the observations in previous sections, the acquisition of compositional generalization accumulates most slowly but is more robust to forgetting. Second, the forgetting tends to be slower in the _paraphrase_ injection scenario compared to the _duplication_ injection scenario. This finding will be further discussed in SS4.4, regarding the importance of deduplicating training data. Finally, the decay constants are similar for the two earlier checkpoints but smaller for the _late_ checkpoint in the _duplication_ injection scenario. We demonstrate that this is due to the reduced learning rate from

  
**Pretraining stage** & & **Early (170B)** & **Mid (500B)** & **Late (1.5T)** \\   & Memorization & \(0.26 0.0020\) & \(0.25 0.0019\) & \(0.20 0.0019\) \\  & Semantic & \(0.24 0.0018\) & \(0.25 0.0022\) & \(0.21 0.0021\) \\  & Composition & \(0.18 0.0020\) & \(0.20 0.0032\) & \(0.16 0.0024\) \\   & Memorization & \(0.20 0.0019\) & \(0.21 0.0023\) & \(0.18 0.0022\) \\  & Semantic & \(0.20 0.0020\) & \(0.23 0.0024\) & \(0.21 0.0024\) \\   & Composition & \(0.14 0.0025\) & \(0.15 0.0022\) & \(0.19 0.0030\) \\   

Table 2: Decay constant of average retainability (\((p,t)\)) measured with OLMo-7B at different pretraining stages, acquisition depths, and injection scenarios. Note that the larger value indicates that the model forgets acquired knowledge with a higher rate.

Figure 4: Average retainability against training steps past the local acquisition maxima, measured with OLMo-7B _mid_ checkpoint. The x-axes are in log scale. **Left**: _duplication_. **Right**: _paraphrase_.

[MISSING_PAGE_FAIL:8]

the model, or _learned_, regardless of the duration of the pretraining.7 This implies that there is a _learnability threshold_, a threshold of the interval where the model fails to acquire knowledge of which its encounter interval is longer than the threshold. Most well-known facts are likely to be presented to the model with an interval of the training steps shorter than this _learnability threshold_. In such a case, the model will accumulate the increased log probability of the knowledge upon each encounter of the knowledge as the pretraining progresses, and at some point, the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model . Moreover, LLMs will accumulate the log probability faster for more popular knowledge, and thus the acquisition of such knowledge will be reflected in the model's top-k output sequence generation in a relatively earlier pretraining stage, as demonstrated in .

In summary, we hypothesize that the popularity of the knowledge in the pretraining data influences how quickly this knowledge begins to be'revealed' in the generated sequences during pretraining, except for the knowledge in the long-tail whose low popularity makes the encounter interval longer than the _learnability threshold_. Also, as briefly mentioned in SS4.2, we hypothesize that the reason why larger and more diverse pretraining data helps the model performance is that the model can acquire a broader range of factual knowledge (more knowledge will be presented with an interval shorter than the _learnability threshold_) since the skewness of the distribution of factual knowledge popularity is likely to be mitigated as the data becomes larger and more diverse.

Why does deduplication enhance model performance?Recent pretraining corpora are thoroughly deduplicated [9; 28; 38; 43; 47; 48], as it is widely observed that data deduplication can improve model performance [1; 29; 42; 52]. Our results suggest that the smaller decay constant in the _paraphrase_ injection scenario observed in SS4.3 can explain the advantages of training LLMs with deduplicated training data, as deduplication tends to slow the forgetting of generalizing acquired factual knowledge. This can also be observed in Figure 2, as the gap of the increase of log probability immediately after encountering the injected knowledge is large between the _duplication_ and _paraphrase_ injection scenarios, but this gap diminishes at the end of the measurement. Moreover, since the model tends to provide a higher increased log probability to the memorization rather than generalization (Figure 2 and 3), presenting the model with duplicated texts with a short interval will result in the widening of the gap between memorization and generalization, which will drive the model to prefer generating memorized contexts compared to generalizing factual knowledge .

## 5 Discussion and Conclusions

In this work, we study how LLMs acquire factual knowledge during pretraining. Our findings and contributions can be summarized as follows:

* We propose methods, datasets, and metrics for performing a fine-grained analysis of factual knowledge acquisition dynamics during LLM pretraining.
* We demonstrate that factual knowledge acquisition in LLM pretraining is achieved through accumulating micro-acquisitions, each of which occurs whenever the model is updated after seeing the factual knowledge. When the model is not presented with factual knowledge, forgetting occurs and the acquisition of the knowledge is gradually diluted.
* However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining. This finding suggests that the benefits of scaling the model size and pretraining tokens are qualitatively different.
* There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization. Also, pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.

* We provide potential explanations for recently observed, yet underexplored behaviors of LLMs. First, we propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining. Second, we hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the _learnability threshold_ to increase the probability. Third, our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer.

Overall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.