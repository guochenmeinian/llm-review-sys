# A Theoretical Understanding of Self-Correction through In-context Alignment

Yifei Wang\({}^{1}\)  Yuyang Wu\({}^{2}\)  Zeming Wei\({}^{3}\)  Stefanie Jegelka\({}^{5,}\)1  Yisen Wang\({}^{4,}\)2

\({}^{1}\) MIT CSAIL \({}^{2}\) School of EECS, Peking University

\({}^{3}\) School of Mathematical Sciences, Peking University

\({}^{4}\) State Key Lab of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{5}\) CIT, MCML, MDSI, TU Munich

\({}^{6}\) Institute for Artificial Intelligence, Peking University

Equal Contribution.Corresponding Author: Yisen Wang (yisen.wang@pku.edu.cn).

###### Abstract

Going beyond mimicking limited human experiences, recent studies show initial evidence that, like humans, large language models (LLMs) are capable of improving their abilities purely by self-correction, _i.e._, correcting previous responses through self-examination, as seen in models like OpenAI o1. Nevertheless, little is known about how such capabilities arise. In this work, based on a simplified setup akin to an alignment task, we theoretically analyze self-correction from an in-context learning perspective, showing that when LLMs give relatively accurate self-examinations as rewards, they are capable of refining responses in an in-context way. Notably, going beyond previous theories on over-simplified linear transformers, our theoretical construction underpins the roles of several key designs of realistic transformers for self-correction: softmax attention, multi-head attention, and the MLP block. We validate these findings extensively on synthetic datasets. Inspired by these findings, we propose a simple self-correction strategy, Checking as Context (CaC), which finds novel applications in alleviating social bias and defending against LLM jailbreaks. We believe that these findings will inspire further research on understanding, exploiting, and enhancing self-correction for building better foundation models. Code is at https://github.com/yifeiwang77/Self-Correction.

## 1 Introduction

"_Who among people is without fault? Making mistakes and being able to correct them is the greatest goodness._" - Zuo Zhuan (\(\)400 BC), Translated by ChatGPT

The capacity for self-correction, traditionally viewed as a distinctive human trait, is increasingly being explored within the realm of artificial intelligence, particularly in Large Language Models (LLMs). Recent studies have sparked optimism about LLMs' self-correction capabilities for enhancing reasoning , planning , and alignment . Although some find that self-correction may lead to worse performance without external feedbacks , more recent evidence shows that with careful designs of instructions on the self-criticizing process, self-correction can yield considerable benefits on various tasks . Remarkably, self-correction is recognized to be pivotal for building strong reasoning models like OpenAI o1 .

Driven by these intruiging empirical findings, we want to establish a principled understanding of how the self-correction ability emerges in LLMs. A particular difficulty is to formulate the multifacetedself-correction designs to be amenable to theoretical analysis. We notice that existing self-correction methods admit a general abstraction: generation, critics, regeneration, and further critics, continuing until the final refined output. This self-correction path can be understood as a particular form of context that provides feedback for refining the prediction on the fly. Different from standard (query, response) context examples akin to supervised learning, self-correction examples can be formulated in a triplet form (query, response, reward) that is akin to LLM alignment with both good and bad samples indicated by their rewards [53; 7; 61; 66]. This observation motivates us to formulate self-correction as a form of _in-context alignment (ICA)_, where LLMs are provided with a context of self-correction steps and the goal is to refine the final outputs to have higher rewards.

Through this perspective, we prove that in a simplified setup, a _standard_ multi-layer transformer can utilize self-correction samples to generate responses of higher rewards. Specifically, we prove the existence of model weights such that a transformer can optimize common ranking-based alignment objectives by performing gradient descent in-context, which includes the Bradley-Terry model  and the Plackett-Luce model  that are _de facto_ choices for LLM alignment (used in RLHF  and DPO ). As far as we know, this is the first theoretical analysis showing that LLMs can improve alignment in-context, providing a solid foundation for understanding self-correction. Our theory accommodates different kinds of self-correction methods, because the critics of responses can come from humans , external verifiers , or LLMs themselves [86; 38]. The analysis further reveals that LLMs' self-correction performance relies crucially on the quality of critics, which agrees well with recent empirical findings [42; 14; 69]. Intriguingly, within this analysis, we nail down the roles of realistic transformer designs - multi-head softmax attention, feed-forward network, and stacked blocks - for alignment, providing concrete theoretical insights for designing robust LLMs. This contrasts with previous in-context learning theories that focus on linear attention in the context of linear regression, deviating from practice [74; 84; 2].

At last, we validate our theoretical explanations through both synthetic and real-world experiments. Extensive synthetic datasets show that transformers can indeed learn from noisy outputs with the help of relatively accurate critics. We validate that real-world transformer modules do matter for in-context alignment, and the results align surprisingly well with our theory. Driven by these theoretical insights, we explore two real-world scenarios where we hypothesize that aligned LLMs can provide relatively accurate self-critics: alleviating social bias and defending against jailbreak attacks. We show that with a simple generation-critic-regeneration process (we call Checking-as-Context) and no external feedback, _intrinsic_ self-correction can alleviate social bias on Vicuna-7b and Llama2-7b-chat, and exhibits a strong correlation between self-checking accuracy and final performance again. With the same strategy, we find that self-correction can reduce the attack success rate by a large margin (_e.g.,_\(95\% 2\%\)) against multiple types of jailbreak attacks. These evidences show that LLMs are indeed capable of improving alignment by self-correction alone, which not only validates our theory, but also provide insights for future designs and applications of self-correction.

## 2 Formulation

In this section, we introduce self-correction and formulate it as a general in-context alignment process, and then introduce the setup for theoretical analysis.

### Self-correction as In-context Alignment

**ICL.** In-context learning (ICL) is known as an emergent ability of LLMs to learn from a few demonstrations without finetuning . Specifically, an LLM can directly predict the desirable response to the test query \(x_{test}\) with \(N\) pairwise training examples \(\{(x_{i},y_{i})\}_{i=1}^{n}\) as the context:

\[_{test}=([x_{1},y_{1},,x_{n},y_{n},x_{test}]).\] (1)

Despite its effectiveness, ICL requires the knowledge of desirable responses \(y_{i}\) to construct the training examples. For instance, Wei et al.  use human-selected safe query-response pairs for in-context defense of jailbreaks. For queries that are vague or require domain expertise (_e.g.,_ math, science, and open-end discussions), desirable responses can be hard to collect or formulate.

**Self-correction.** As a further step to eliminate human efforts, self-correction relies on LLMs to correct the mistakes in the initial generation. In self-correction, we first generate an initial response \(y_{1}\) to the query \(x\), and then obtain a critic on the response, denoted as a reward \(r_{1}\). The critic can be either generated by LLMs themselves through carefully designed prompting [34; 38], or by external verifiers such as code intepreters . Afterwards, the LLM is then instructed to generate a refined response \(y_{2}\) taking the initial response \(y_{1}\) and its critic \(r_{1}\) as the input context. This process can be repeated multiple times for iterative refinements of the response. After \(N\) steps, we take the final response \(y_{N}\) as the final output. For simplicity, we assume that these steps share the same query \(x\), and the extension to multiple queries is discussed in Appendix F.1.

**In-context Alignment (ICA).** The self-correction process described above can be formalized as an in-context learning task with triplet examples \(\{(x,y_{i},r_{i})\}\), where \(x\) is the (shared) query, \(y_{i}\) is the response, and \(r_{i}\) is the critic at the \(i\)-th step. Note that the same data format is also adopted in LLM alignment tasks, where LLMs are trained to follow human intention with human/AI-generated preference data [53; 7; 61; 66].3 In this way, we formulate self-correction as an in-context way to solve an alignment task, which we call in-context alignment (ICA). Here, the concept of alignment is inclusive and not limited to standard alignment tasks. Any objective that works with the triplet preference data can fit into our framework. Also, we do not assume that the rewards \(r_{i}\) are always accurate, and the quality of the rewards will be shown to have a critical influence on self-correction.

### Theoretical Setup

Since real-world LLMs on language tasks are too complex for a rigorous analysis, recent studies on ICL theory rely on synthetic simple tasks to examine LLM capabilities [24; 74; 84; 2]. Existing results are mostly established in the supervised setting, particularly for linear regression, due to its simplicity and alignment with linear attention. However, it is yet unknown whether transformers are capable of learning alignment tasks using preference data in-context. In this section, we introduce a simplified setup for in-context alignment. For the ease of analysis, we still study a linear regression task, where a smaller MSE loss gets higher reward. However, what makes things harder is that the models are not provided with groundtruth targets as the context, but only (potentially false) responses \(y_{i}\) and their rewards \(r_{i}\). To solve this task, the model has to learn the ability to compare the rewards of different samples and prioritize those with higher rewards - a critical ability that is key to self-correction and alignment, but has not been studied in previous theories.

#### 2.2.1 Alignment Task

We begin by formalizing a general alignment task with triplet examples. Consider a training dataset \(D=\{(x,y_{i},r_{i})\}_{i=1}^{n-1}\) composed of a common query \(x^{n_{x}}\) (assume \(\|x\|^{2}=1\) for simplicity)4, multiple responses \(y_{i}^{n_{y}}\) and rewards \(r_{i}\). Following the setup of Von Oswald et al. , we also consider a linear regression task where the groundtruth function is \(f(x)=W^{*}x\) for some \(W^{*}^{n_{y} n_{x}}\). Here, the responses \(y_{i}\) can be quite noisy (_e.g.,_ random), and the quality of this response is indicated by its reward value. Therefore, the transformers have to rank the responses based on their rewards and adjust their outputs accordingly. In general, the critic \(r_{i}\) here can come from either humans, external feedback (e.g., code execution) or LLMs themselves (called _inxtrinsic_ self-correction)--all these variants are studied in the literature. Thus, the rewards may also contain noise, which reflects the critic quality. The goal is to output a response \(y_{N}\) that has a smaller square error, _i.e.,_ higher rewards. There are two approaches to solve this problem, one is through the in-context alignment with a transformer-based LLM, and one is through learning a parameterized alignment model. We describe these methods formally below, and establish their inherent connections in the next section.

#### 2.2.2 Transformer Model

The transformer model  is the _de facto_ choice for building LLMs. It is a composition of multiple transformer blocks. Each block consists of two modules: MHSA and FFN. Normalization layers are omitted for simplicity.

**MHSA.** A multi-head self-attention (MHSA) layer updates a set of tokens \(\{e_{1},,e_{N}\}\) by

\[e_{j} e_{j}+_{}(j,\{e_{1},,e_{N}\} )=e_{j}+_{h}P_{h}V_{h}\,(K_{h}^{}q_{h,j} ),\] (2)with \(P_{h},V_{h},K_{h}\) the projection, value and key matrices, respectively, and \(q_{h,j}\) the query, all for the \(h\)-th head (bias terms omitted). The columns of the value \(V_{h}=[v_{h,1},,v_{h,N}]\) and key \(K_{h}=[k_{h,1},,k_{h,N}]\) matrices consist of vectors \(v_{h,i}=W_{h,V}e_{i}\) and \(k_{h,i}=W_{h,K}e_{i}\); likewise, the query is produced by linearly projecting the tokens, \(q_{h,j}=W_{h,Q}e_{j}\). The parameters \(=\{P_{h},W_{h,V},W_{h,K},W_{h,Q}\}_{h}\) of a SA layer consist of all the projection matrices of all heads. We omit causal masking in the main paper for simplicity; see Appendix F.2 for an extension.

**FFN.** Following self-attention, a feed-forward network (FFN) transforms each token individually with two shared linear transformations and a ReLU activation in between:

\[e_{j} e_{j}+_{}(e_{j}), _{}(e_{j})=W_{2}(0,W_{1}x+b_{1})+b_{2}.\] (3)

Here, \(W_{1},W_{2}\) are weight matrices and \(b_{1},b_{2}\) are bias vectors. Collectively, \(=(W_{1},b_{1},W_{2},b_{2})\) denotes all FNN parameters. Both SA and FFN have residual connections.

**Context Tokens.** For simplicity, we assume that LLMs take a concatenated input \(e_{i}=[x_{i},y_{i},r_{i}]\) for each example.5 For the last test example, to align with the same input format, we model it as \(e_{N}=[x,y_{N},r_{N}]\), where we use a "dummy" response \(y_{N}=W_{0}x_{N}\) (_i.e.,_ the _initial guess_ of LLMs with weights \(W_{0}\) (Section 2.2.3)) as an initialization for the final output, and its "dummy" reward \(r_{N}\) is assumed to have the lowest reward among the input examples. In total, we have \(N\) tokens \(\{e_{i}=[x,y_{i},r_{i}]\}_{i=1}^{N}\) as the contextual input to the transformer.

#### 2.2.3 Alignment Model

A common way to solve alignment tasks is to learn a parameterized alignment model that models preferences through a ranking objective over multiple candidates [9; 58; 46; 61]. We use \(y_{i} y_{j}\) to denote the event that the response \(y_{i}\) is preferable over \(y_{j}\). Let \([N][N]\) be the permutation function that denotes the ranking of all responses according to the reward scores, _i.e.,_\(r_{(1)}>>r_{(N)}\).6 The ranking \(\) implies that for any \(N i>j 1\), we have \(y_{(i)} y_{(j)}\). A common objective for \(N\)-ary comparison is the Plackett-Luce (PL) model [58; 46; 61] that stipulates

\[P_{}( x,\{y_{i}\})=_{i=1}^{N}))}{_{j=i}^{N}(r(x,y_{(j)}))},\] (4)

where \(r\) denotes the reward function. Since we consider a linear regression task (Section 2.2.1), we use the negative square error as the reward function (higher is better): \(r(x,y)=-\|Wx-y\|^{2}\). The corresponding PL model is

\[P_{}()=_{i=1}^{N}\|^{2})}{_{j=i}^{N}(-\|Wx-y_{(j)}\|^{2})}.\] (5)

**Relationship to Bradley-Terry model.** The Plackett-Luce model is an \(N\)-ary generalization of the Bradley-Terry model  used for pariwise preferences. In particular, with \(N=2\), the PL model (Eq. (5)) reduces to the Bradley-Terry model with least-squares reward:

\[P_{}(y_{1} y_{2})=\|^{2 })}{_{j=1}^{2}(-\|Wx-y_{i}\|^{2})}.\] (6)

Previous work  shows that the \(N\)-ary PL model outperforms the binary BT model for alignment.

**Relationship to InfoNCE.** We also notice that the InfoNCE loss that is widely used for contrastive learning [51; 12; 60; 76] can be seen as a special case of the PL model when only considering its first term (\(i=1\)). In this case, only \(y_{1}\) is the positive sample and \(y_{2},,y_{N}\) are negative samples, which corresponds to a special ranking \(r_{(1)}>r_{(2)}==r_{(N)}\). Therefore, the analysis in our framework can be used to explain in-context contrastive learning .

Main Results

In this section, we present the main result of this work, which, to the best of our knowledge, is the first to show that _a realistic transformer (with stacked multi-head softmax attention and feed-forward networks)_ can implement the gradient descent of common alignment objectives with in-context triplets. Notably, our analysis reveals the individual roles of these core designs of realistic transformers for in-context alignment (and self-correction), which may help future designs of LLM backbones as well.

### A Simple Case: Bradley-Terry Model with \(N=2\)

To highlight the key ideas without technical nuances, we start with \(N=2\), the Bradley-Terry (BT) model (Eq. (6)). Assume w.l.o.g. that \(y_{1} y_{2}\) with scores \(r_{1}>r_{2}\), the BT model is \(_{}(W;x,y_{1},y_{2})=- P_{}(y_{1}  y_{2} x)=\|Wx-y_{1}\|^{2}+_{j=1}^{2}(-\|Wx- y_{j}\|^{2}).\)

**Proposition 3.1**.: _One can realize the gradient descent for BT,_

\[W^{}=W+ W=W-_{W}_{}(W;x,y_{1},y_{2 }),\]

_by updating each \(y_{i}\) with_

\[y_{i}^{}=y_{i}- Wx=}_{(i)}-}_{(2)}+^{2}_{j}y_{j}}_{(j)},\] (7)

_where \(_{j}=(-\|Wx-y_{j}\|^{2})\). Specifically, \(_{}(W^{};x,y_{1},y_{2})=_{} (W;x,y_{1}^{},y_{2}^{})\)._

Proposition 3.1 shows that the gradient descent of the BT model is equivalent to transforming the targets \(y_{i}\) according to Eq. (7). This connection allows us to **optimize output alignment (measured by BT loss) with the forward propagation of an MHSA layer** (Eq. (2)). To see this, Term (1) corresponds to the shortcut feature \(y_{i}\). Term (2) is a bit complex, since it only picks \(y_{1}\) with the higher score (\(r_{1}>r_{2}\)). We find that this can be realized by constructing a _softmax attention head_ that only attends to tokens with the largest reward \(r\). Term (3) can be implemented with another softmax attention head that incorporates \(_{i}\)'s as the attention weights and \(y_{i}\)'s as values. Therefore, the one-step gradient descent of the BT model can be implemented with two-head softmax attention.

**Theorem 3.2**.: _Given a **two-head softmax attention layer** and two tokens \(e_{i}=(x_{i},y_{i},r_{i}),i=1,2\), there exists a set of parameters (Eq. (2)) such that a forward propagation step with token \(e_{i}\) is equivalent to the gradient-induced dynamics of the Bradley-Terry model (Eq. (6)):_

\[e_{i}^{}=(x_{i},y_{i},r_{i})+_{h=1}^{2}P_{h}V_{h}( K_{h}^{}q_{h,j})=(x_{i},y_{i},r_{i})+(0,- W_{}x_{i},0),i=1,2.\] (8)

All proofs of the paper are deferred to Appendix E. As outlined above, our construction of in-context alignment requires two heads to implement the two gradient terms corresponding to positive and negative feedback, where softmax attention is exploited for sample selection in both cases. Instead, ICL analyses for linear regression  only require one linear attention head for interpolating with linear products. Thus, our alignment analysis better reveals the need for softmax and multi-head attention, so it has a close correspondence to real-world architectures.

### Extension to Cases with \(N>2\)

We further explore how to extend this result to a general \(N\)-ary Plackett-Luce (PL) model (Eq. (5)). Although the key ideas are similar, it is technically much harder to implement \(N>2\) with a single SA layer. To see this, notice that the response update of the PL loss corresponds to

\[y_{i}^{}= y_{i}-2_{i=1}^{N-1}y_{(i)}-_{j=i}^{N} _{j}y_{(j)}.\] (9)

At first glance, the \(i\)-th item of the update resembles Eq. (7) and seems implementable with a two-head self-attention. However, it is actually hard to realize the first term \(y_{(i)}\), since softmax attention can only select the top or the bottom value7 from a set of rewards, making it challenging to compare \(N\) examples within a single SA layer.

**A roadmap to implementing \(N>2\) with stacked full Transformer blocks.** We discover that it is still possible to construct the PL gradient descent if we further incorporate the FFN module and allow stacking multiple transformer blocks. Specifically, at the \(i\)-th block, we can 1) identify the token with the largest reward (_i.e., \(y_{(i)}\)_) and implement the \(i\)-th term of the gradient descent with a three-head SA layer; and 2) mask out the \(y_{(i)}\) of this token to eliminate its contribution in subsequent terms with the help of an FFN. In other words, each transformer block can implement one of the \(N-1\) terms of the gradient (Eq. (9)) and prepare the input data for implementing the next term with one additional head. In total, it requires stacking \(N-1\) transformer blocks (each is composed of three-head MHSA and FFN) to implement the whole gradient descent of the PL model. 8

**Theorem 3.3**.: _Given a **transformer**\(\)**with \(N-1\)**stacked transformer blocks (composed of three-head softmax attention and feed-forward networks)** and \(N\) input tokens \(\{e_{i},i[N]\}\), there exists a set of parameters such that a forward step with token \(e_{i}\) is equivalent to the gradient-induced dynamics of the \(N\)-ary Plackett-Luce model (Eq. (5)), i.e., \((e_{i})=(x_{i},y_{i},r_{i})+(0,- W_{}x_{i},0),i [N]\)._

Theorem 3.3 shows that a multi-layer transformer can improve its output alignment by optimizing a general Plackett-Luce model through in-context learning. It could serve as a general explanation for ICL-based alignment algorithms [26; 41; 25]. As far as we know, it is the first theoretical result for explaining in-context alignment from an optimization perspective. Through our construction, we also underpin the individual roles of rewards and transformer modules during the self-correction process:

1. **Reward quality determines self-correction quality.** By connecting in-context alignment to an optimization process, we reveal that the critics used in self-correction essentially serve as the supervision for the in-context alignment task. Thus inaccurate rewards would amount to noisy supervision that is known to degrade learning performance [50; 77; 47], which explains the benefits of external feedback  and stronger discriminator  in self-correction.
2. **Softmax attention is important for ranking.** One of the key steps to implement the gradient descent is to select the top response based on the input rewards, and our construction relies crucially on the ability of softmax attention to compare and reweight different rewards. Instead, it is hard for linear attention to implement such ranking operations.
3. **Multi-head attention is important for token discrimination.** We use two attention heads in Eq. (7) with different roles: one for pushing top ones apart, and one for pulling others closer. This indicates that only with multi-head attention can we achieve better discrimination of different input tokens. In contrast, only one attention head is needed for regression .
4. **FFN is important for transforming selected tokens**. In our construction, although softmax attention can select the top tokens, we cannot edit the selected tokens with attention alone. Instead, FFN is capable of 1) identifying top tokens in the input sequence with the knowledge of initial and selected tokens, as well as 2) performing conditional operations (_e.g.,_ masking out \(y_{(i)}\)) by leveraging the ReLU nonlinearity.
5. **Ranking multiple examples requires more depth**. Comparing Theorems 3.2 and 3.3, we notice that ranking \(N\) examples with a transformer requires \(N-1\) layers with our construction. This fact suggests a hint of why depth is still a major factor when constructing LLMs. For example, scaling from 7B to 70B, Llama2 goes from 32 layers to 80 layers and shows significant improvements.

In Section 4, we also empirically validate the necessity of these modules for in-context alignment. This analysis also suggests that linear regression--which only requires single-head linear attention to solve in-context--may not be enough to fully characterize the behavior of standard transformers , while our in-context alignment tasks (Section 2.2.1) could be a better theory model. These theoretical disclosures of Transformer modules may inspire future designs of LLM backbones as well.

**Relation to Previous Theoretical analyses.** An existing line of prior research explains in-context learning via its connection to optimization algorithms . We provide a detailed summary of these works in Appendix A, and here summarize key aspects in which we differ:

* **Objective: linear regression _vs._ non-convex alignment**. Compared to previous methods that focus on solving linear regression in-context, we are the first to show that transformers can also solve ranking-based alignment problems in-context. A major difference is that alignment involves a more complex non-convex objective that does not admit a closed-form solution like linear regression.
* **Backbone: linear attention _vs._ full transformer**. As discussed above, our construction identifies that softmax attention and other components of transformers play a major role in ranking while focusing on linear regression problems only requires linear attention. It reveals that our PL model with linear reward could be a better theory model for explaining in-context learning as it aligns better with practice.
* **Task: supervised learning _vs._ preference-based alignment.** Previous ICL theories mostly focus on explaining its ability to perform supervised regression. Instead, we show that LLMs can learn in-context alignment, which allows feedbacks from various sources with noises, and learns from both good and bad behaviors. In particular, our theory also applies to intrinsic self-correction methods with self-generation critics, which is self-supervised.

## 4 Verification on Synthetic Data

Here, we follow our theoretical setup in Section 2.2.3 and conduct a series of synthetic experiments to examine our theoretical results established in Section 3.

**Setup.** We consider the following meta-learning setting. For every task, we draw a common query \(x(0,I_{d d})\) and a groundtruth parameter \(W(0,I_{d d})\). We then generate \(N\) responses and rewards. For each response \(y_{i}\), we sample a reward \(r_{i}\) and an independent noise weight \(W_{i}^{-}(0,I_{d d})\), and then generate \(y_{i}=r_{i}Wx+(1-r_{i})W_{i}^{-}x\). Thus, responses with higher rewards are closer to the ground truth in expectation. By default, we set \(d=5,N=20\) and use a \(20\)-layer GPT-2 model with \(3\) heads, \(96\) hidden dimension, and a PL loss (Eq. (5)). Then we evaluate the normalized MSE between the predicted output \(\) and ground-truth \(y=Wx\) using varying numbers of in-context examples, averaged over \(256\) runs with randomly generated tasks. We also implement the gradient descent (GD) of the linear PL model (Eq. (5)) and measure its optimal

Figure 1: Synthetic experiments of in-context alignment with comparison between TF and GD (a), different reward noise \(p\) (b), model depth (c), and attention types (d), (e), (f).

solution in the same way. We also change the reward noise \(p\), model depth, and attention types to investigate their effects on in-context alignment. For more details, please refer to Appendix C.

As shown in Figure 1, there is a clear trend that with more in-context examples, transformer-based in-context alignment and gradient descent (GD) can quickly adapt to the task and find better predictions for test samples. In comparison, Figure 0(a) shows that GD performs better at the beginning, while Transformers also adapt quickly and attain slightly better performance with more in-context examples, _e.g.,_\(N=14\). It indicates that in-context alignment might be even preferable to GD-based alignment in certain cases, which validates our theoretical results that transformers can optimize alignment in-context by gradient descent. Below, we study different factors of in-context alignment.

**Reward quality matters.** To investigate the influence of reward quality, we randomly replace rewards with random values \(r_{i}^{p}(0,1)\) with a probability \(p\). As shown by solid lines in Figure 0(b), a large \(p\) significantly decreases the in-context alignment performance with much larger test errors. This can be naturally understood through our theory, where the gradient descent is performed on noisy data that hinders the learning process, as shown in the dashed lines in Figure 0(b). Therefore, it explains why self-correction methods are sensitive to the quality of critics, and LLMs need strong critics to perform effective self-correction, as empirically observed in recent work .

**Necessity of Transformer Modules.** While conventional ICL theories show that _1-layer single-head linear self-attention_ is sufficient for linear regression , for in-context alignment, we observe: **(1) ICA requires more depth.** Figure 0(c) shows that when transformers are shallow (_e.g.,_ 5 layers), ICA is much worse, and more depth benefits ICA effectively. After 15 layers, depth brings diminishing returns. This is consistent with our theory that requires stacking multiple transformer blocks for in-context alignment of \(N\) example (Theorem 3.3). **(2) Softmax attention is necessary.** Figure 0(d) illustrates that linear attention can hardly solve the in-context alignment task while softmax attention performs much better, which is consistent with our analysis (Section 3.2). **(3) Multi-head attention helps.** Figure 0(e) shows that single-head attention struggles to align in-context, while multi-head (\(3,4,6\)) performs well. In addition, when the number of attention heads exceeds \(3\), there is no significant benefit, which aligns surprisingly well with our analysis that requires \(3\)-head to implement the GD of the \(N\)-ary PL loss (Theorem 3.3). **(4) FFN is necessary.** Figure 0(f) shows that without FFN, the model cannot align in-context, consistent with our analysis that FFN is necessary for transforming selected tokens. Summarizing these results, we find that our proof by construction does have a nice correspondence to the practical behaviors of transformers on in-context alignment tasks, and it helps reveal the roles of each transformer module for in-context alignment-like tasks.

## 5 Exploring Self-correction on Real-world Alignment Tasks

Our theoretical analysis above reveals that self-correction indeed has the potential to improve the alignment of LLMs, especially when the critics are relatively accurate. Motivated by this observation, we explore self-correction on two real-world alignment tasks: alleviating social bias  and defending against jailbreaks . Since LLMs are aligned on human preferences and harmfulness is relatively easy for discrimination, we hypothesize that self-generated critics can be accurate in these tasks, which facilitate LLMs to improve their own alignment, known as _intrinsic self-correction_.

**Method: Checking-as-Context (CaC).** For simplicity, we study a very simple and general form of self-correction without sophisticated procedures. Specifically, following the same format as our theoretical setup (Section 2.1), given a query \(x\), we first generate an initial response \(y\) (w/o self-correction), and then instruct the model to review its response and get a self-critic \(r\), and instruct the model to regenerate a new answer as the output (w/ self-correction), as

Figure 2: An illustration of Checking-as-Context (CaC) on addressing gender bias.

illustrated in Figure 2. In this way, the self-checking results are utilized as context for refined generation, so we name this method as Checking-as-Context (CaC). See more details in Appendix C.

### Alleviating Social Bias with Self-correction

Following Ganguli et al. , we study the use of self-correction to alleviate societal biases in LLMs on the BBQ (Bias Benchmark for QA) benchmark , which evaluates societal biases against individuals belonging to protected classes across nine social dimensions. We randomly select 500 questions from each task subclass. Different from moral self-correction  that requires model finetuning, our method is more light-weighted, since it is inference-only without parameter update.

Figure 3 shows that on two strong open-source LLMs Vicuna-7b  and Llama2-7b-chat , an additional self-correction step can indeed improve model alignment on most social bias tasks, including gender, race, religion, social-economic status, sexual orientation, physical appearance, disability status, nationality. The only exception is _physical appearance_ on Llama2-7b-chat, where self-correction is slightly worse, potentially because this aspect is less aligned on LLama2. Moreover, Figure 2(c) exhibits a strong correlation (\(p<0.05\)) between the gain of self-correction and self-checking accuracy, as suggested by our theory (more evidence in Appendix C.2). In Section 5.3, we further conduct controlled analyses on critic qualities, critic types, and model sizes for self-correction.

### Defending Against LLM Jailbreaks with Self-correction

LLM jailbreaks have recently risen to be a major threat to LLM alignment [5; 19], where even well-aligned models like ChatGPT can be manipulated into generating harmful content [89; 43; 78]. Although various defense measures have been proposed [31; 80; 78; 40; 29; 49], these typically require extensive human intervention. The ambiguity remains as to whether LLMs can autonomously counter-act such jailbreaking manipulations. Here, we explore whether LLMs can defend against jailbreak attacks themselves with self-correction. Due to the limit of space, more results can be found in Appendix B.

We observe that for LLM jailbreaks, self-correction can give accurate self-checking most of the time (close to 100%). As a result, from Table 1, we observe that on AdvBench , CaC-based self-correction can indeed improve LLM safety a lot by reducing the attack success rate (ASR) on Vicuna-7b and Llama2-7b-chat by a significant margin against different types of jailbreak attacks, including gradient-based GCG attacks  and instruction-based AutoDAN . Compared to manually designed defense methods [80; 40; 29], self-correction can achieve comparable and even better performance. It suggests that LLMs can autonomously defend against jailbreak attacks with _intrinsic_ self-correction, which is a promising direction for future research on AI safety.

### Fine-grained Analyses of Self-correction in Language Models

At last, we take a deeper looker into the influencing factors of self-correction through several controlled studies under BBQ (Section 5.1) (see experimental details in Appendix C.2). Overall, we find that self-correction benefits from _high critic accuracy, combining verbal and numerical critic, more self-correction rounds (but not too many), and enough model capacity (e.g. 7B)_.

First, we investigate the influence of critic by controlling its quality and format. Figure 3(a) shows that final performance consistently increases with a more accurate critic (biased or unbiased), which we generate noisy critic by adding random noises to groundtruth critic. Figure 3(b) reveals that among different formats of critic, verbal critic with natural language significantly outperforms numerical critic, and combining verbal critic through chain-of-thought (CoT) and binary critic leads to optimal results. We believe that verbal critic creates _fine-grained rewards_ in a way that LLMs understand.

Second, we look into the influence of model size and self-correction rounds. For model size experiments, we use the Qwen-1.5 series  for a fair comparison. To control the influence of critic, we consider two settings: 1) With self-generated critic (Figure 3(c)), we find that even if the critic is very accurate (close to 100%), very small models like 1.8B one still cannot self-correct, echoing with our theory that model depth and capacity are important for the self-improving step. 2) The same phenomenon holds when we use the same groundtruth critic (Figure 3(d)). Lastly, we study the influence of more self-correction rounds. Figure 3(e) shows that with groundtruth critic, LLMs can benefit from at most 3-round self-correction, while they deteriorate around 1 round under self-critic. It shows that an accurate critic is important for multi-step self-correction to prevent the accumulation of immediate errors. These real-world LLM behaviors align closely with our theoretical analysis.

## 6 Conclusion

In this paper, we have explored how self-correction ability rises from an in-context alignment perspective, showing that standard transformers can perform gradient descent on common alignment objectives in an in-context way. Notably, our analysis reveals the important roles of real-world transformer modules in self-correction. We further studied intrinsic self-correction for real-world alignment scenarios and demonstrated clear improvements on alleviating social bias and defending against jailbreaks. In this way, our analysis provides concrete theoretical and empirical insights into the path of building LLMs that can correct and improve themselves.

Figure 4: Controlled studies of different influencing factors of LLM self-correction. We adopt Vicuna-7b by default, except for model size experiments we use the Qwen-1.5 series.