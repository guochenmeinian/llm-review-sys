# Graph Coarsening with Message-Passing Guarantees

Antonin Joly

IRISA, Rennes, France

antonin.joly@inria.fr &Nicolas Keriven

CNRS, IRISA, Rennes, France

nicolas.keriven@cnrs.fr

###### Abstract

Graph coarsening aims to reduce the size of a large graph while preserving some of its key properties, which has been used in many applications to reduce computational load and memory footprint. For instance, in graph machine learning, training Graph Neural Networks (GNNs) on coarsened graphs leads to drastic savings in time and memory. However, GNNs rely on the Message-Passing (MP) paradigm, and classical spectral preservation guarantees for graph coarsening do not directly lead to theoretical guarantees when performing naive message-passing on the coarsened graph.

In this work, we propose a new message-passing operation specific to coarsened graphs, which exhibit theoretical guarantees on the preservation of the propagated signal. Interestingly, and in a sharp departure from previous proposals, this operation on coarsened graphs is often oriented, even when the original graph is undirected. We conduct node classification tasks on synthetic and real data and observe improved results compared to performing naive message-passing on the coarsened graph.

## 1 Introduction

In recent years, several applications in data science and machine learning have produced large-scale _graph_ data . For instance, online social networks  or recommender systems  routinely produce graphs with millions of nodes or more. To handle such massive graphs, researchers have developed general-purpose _graph reduction_ methods , such as **graph coarsening**. It consists in producing a small graph from a large graph while retaining some of its key properties, and starts to play an increasingly prominent role in machine learning applications .

Graph Neural Networks.Machine Learning on graphs is now largely done by Graph Neural Networks (GNNs) . GNNs are deep architectures on graph that rely on the **Message-Passing** paradigm : at each layer, the representation \(H_{i}^{l}^{d_{l}}\) of each node \(1 i N\), is updated by _aggregating_ and _transforming_ the representations of its neighbours at the previous layer \(\{H_{j}^{l-1}\}_{j(i)}\), where \((i)\) is the neighborhood of \(i\). In most examples, this aggregation can be represented as a _multiplication_ of the node representation matrix \(H^{l-1}^{N d_{l-1}}\) by a _propagation matrix_\(S^{N N}\) related to the graph structure, followed by a fully connected transformation. That is, starting with initial node features \(H^{0}\), the GNN \(_{}\) outputs after \(k\) layers:

\[H^{l}=(SH^{l-1}_{l}),_{}(H^{0},S)=H^{k}\,,\] (1)

where \(\) is an activation function applied element-wise (often ReLU), \(_{l}^{d_{l-1} d_{l}}\) are learned parameters and \(=\{_{1},,_{k}\}\). We emphasize here the dependency of the GNN on the propagation matrix \(S\). Classical choices include mean aggregation \(S=D^{-1}A\) or the normalized adjacency \(S=D^{-}AD^{-}\), with \(A\) the adjacency matrix of the graph and \(D\) the diagonal matrix of degrees. When adding self-loops to \(A\), the latter corresponds for instance to the classical GCNconv layer .

An interesting example is the Simplified Graph Convolution (SGC) model , which consists in removing all the non-linearity (\(=id\) the identity function). Surprisingly, the authors of  have shown that SGC reaches quite good performances when compared to non-linear architectures and due to its simplicity, SGC has been extensively employed in theoretical analyses of GNNs .

Graph coarsening and GNNs.In this paper, we consider graph coarsening as a **preprocessing** step to downstream tasks : indeed, applying GNNs on coarsened graphs leads to drastic savings in time and memory, both during training and inference. Additionally, large graphs may be too big to fit on GPUs, and mini-batching graph nodes is known to be a difficult graph sampling problem , which may no longer be required on a coarsened graph. A primary question is then the following: **is training a GNN on a coarsened graph provably close to training it on the original graph?** To examine this, one must study the interaction between graph coarsening and message-passing.

There are many ways of measuring the quality of graph coarsening algorithms, following different criteria . A classical objective is the preservation of _spectral properties_ of the graph Laplacian, which gave birth to different algorithms . Loukas  materializes this by the so-called _Restricted Spectral Approximation_ (RSA, see Sec. 2) property: it roughly states that the frequency content of a certain subspace of graph signals is approximately preserved by the coarsening, or intuitively, that the coarsening is well-aligned with the low-frequencies of the Laplacian. Surprisingly, the RSA _does not generally lead to guarantees on the message-passing process_ at the core of GNNs, even for very simple signals. That is, simply performing message-passing on the coarsened graph using \(S_{c}\), the naive propagation matrix corresponding to \(S\) on the coarsened graph (e.g. normalized adjacency of the coarsened graph when \(S\) is the normalized adjacency of the original one) does _not_ guarantee that the outputs of the GNN on the coarsened graph and the original graph will be close, even with high-quality RSA.

Contribution.In this paper, we address this problem by defining a **new propagation matrix**\(S_{c}^{}\) _specific to coarsened graphs_, which translate the RSA bound to message-passing guarantees: we show in Sec. 3.3 that training a GNN on the coarsened graph using \(S_{c}^{}\) is provably close to training it on the original graph. The proposed matrix \(S_{c}^{}\) can be computed for any given coarsening and **is not specific to the coarsening algorithm used to produce it1**, as long as it produces coarsenings with RSA guarantees. Interestingly, our proposed matrix \(S_{c}^{}\) is _not symmetric_ in general even when \(S\) is, meaning that our guarantees are obtained by performing _oriented message-passing_ on the coarsened graph, even when the original graph is undirected. To our knowledge, the only previous work to propose a new propagation matrix for coarsened graphs is , where the authors obtain guarantees for a specific GNN model (APPNP ), which is quite different from generic message-passing.

Related Work.Graph Coarsening originates from the multigrid-literature , and is part of a family of methods commonly referred to as _graph reduction_, which includes graph sampling , which consists in sampling nodes to extract a subgraph; graph sparsification , that focuses on eliminating edges; or more recently graph distillation , which extends some of these principles by authorizing additional informations inspired by dataset distillation .

Some of the first coarsening algorithms were linked to the graph clustering community, e.g.  which used recursively the Graclus algorithm  algorithm itself built on Metis . Linear algebra technics such as the Kron reduction were also employed . In , the author presents a greedy algorithm that recursively merge nodes by optimizing some cost, with the purpose of preserving spectral properties of the coarsened Laplacian. This is the approach we use in our experiments (Sec. 4). It was followed by several similar methods with the same spectral criterion . Since modern graph often includes node features, other approaches proposed to take them into account in the coarsening process, often by learning the coarsening with specific regularized loss . Closer to this work,  proposes an optimization process to explicitely preserve the propagated features, however with no theoretical guarantees and only one step of message-passing. While these works often seek to preserve a fixed number of node features as in e.g. ), the RSA guarantees  leveraged in this paper are _uniform_ over a whole subspace: this stronger property is required to provide guarantees for GNNs with several layers.

Graph coarsening has been intertwined with GNNs in different ways. It can serve as graph _pooling_ within the GNN itself, with the aim of mimicking the pooling process in deep convolutional models on images. In the recent literature, the terms "coarsening" and "pooling" tend to be a bit exchangeable. For instance, some procedures that were initially introduced as pooling could also be used as pre-processing step, such as Graclus , introduced by  as a pooling scheme, see also . Graph pooling is often data-driven and fully differentiable, such as Diffpool , SAGPool , and DMoN . Theoretical work on their ability to distinguish non homomorphic graphs after pooling have been conducted . In return, GNNs can also be trained to _produce_ data-driven coarsenings, e.g. GOREN  which proposes to learn new edge weights with a GNN. As mentioned before, in the framework we consider here, graph coarsening is a _preprocessing_ step with the aim of saving time and memory during training and inference . Here few works derive theoretical guarantees for GNNs and message-passing, beyond the APPNP architecture examined in . To our knowledge, the proposed \(S_{c}^{}\) is the first to yield such guarantees.

Outline.We start with some preliminary material on graph coarsening and spectral preservation in Sec. 2. We then present our main contribution in Sec. 3: a new propagation matrix on coarsened graphs that leads to guarantees for message-passing. As is often the case in GNN theoretical analysis, our results mostly hold for the linear SGC model, however we still outline sufficient assumptions that would be required to apply our results to general GNNs, which represent a major path for future work. In Sec. 4 we test the proposed propagation matrix on real and synthetic data, and show how it leads to improved results compared to previous works. The code is available at https://gitlab.inria.fr/anjoly/mp-guarantees-graph-coarsening, and proofs are deferred to App. A.

Notations.For a matrix \(Q^{n N}\), its pseudo-inverse \(Q^{+}^{N n}\) is obtained by replacing its nonzero singular values by their inverse and transposing. For a symmetric positive semi-definite (p.s.d.) matrix \(L^{N N}\), we define \(L^{}\) by replacing its eigenvalues by their square roots, and \(L^{-}=(L^{+})^{}\). For \(x^{N}\) we denote by \(\|x\|_{L}=Lx}\) the Mahalanobis semi-norm associated to \(L\). For a matrix \(P^{N N}\), we denote by \(\|P\|=_{\|x\|=1}\|Px\|\) the operator norm of \(P\), and \(\|P\|_{L}=\|L^{}PL^{-}\|\). For a subspace \(R\), we say that a matrix \(P\) is \(R\)-preserving if \(x R\) implies \(Px R\). Finally, for a matrix \(X^{N d}\), we denote its columns by \(X_{:,i}\), and define \(\|X\|_{:,L}=_{i}\|X_{:,i}\|_{L}\).

## 2 Background on Graph Coarsening

We mostly adopt the framework of Loukas , with some generalizations. A graph \(G\) with \(N\) nodes is described by its weighted adjacency matrix \(A^{N N}\). We denote by \(L^{N N}\) a notion of symmetric p.s.d. Laplacian of the graph: classical choices include the combinatorial Laplacian \(L=D-A\) with \(D=D(A):=(A1_{n})\) the diagonal matrix of the degrees, or the symmetric normalized Laplacian \(L=I_{N}-D^{-}AD^{-}\). We denote by \(_{},_{}\) respectively the largest and smallest non-zero eigenvalue of \(L\).

Coarsening matrix.A coarsening algorithm takes a graph \(G\) with \(N\) nodes, and produces a coarsened graph \(G_{c}\) with \(n<N\) nodes. Intuitively, nodes in \(G\) are grouped in "super-nodes" in \(G_{c}\) (Fig. 1), with some weights to outline their relative importance. This mapping can be represented _via_ a **coarsening matrix**\(Q^{n N}\):

\[Q=Q_{ki}>0&$}\\ Q_{ki}=0&\]

The **lifting matrix** is the pseudo-inverse of the coarsening matrix \(Q^{+}\), and plays the role of inverse mapping from the coarsened graph to the original one. The **coarsening ratio** is defined as \(r:r=1-\). That is, the higher \(r\) is, the _more_ coarsened the graph is.

A coarsening is said to be **well-mapped** if nodes in \(G\) are mapped to a unique node in \(G_{c}\), that is, if \(Q\) has exactly one non-zero value per column. Moreover, it is **surjective** if at least one node is mapped to each super node: \(_{i}Q_{ki}>0\) for all \(k\). In this case, \(QQ^{}\) is invertible diagonal and \(Q^{+}=Q^{}(QQ^{})^{-1}\). Moreover, such a coarsening is said to be **uniform** when mapping weightsare constant for each super-nodes and sum to one: \(Q_{ki}=1/n_{k}\) for all \(Q_{ki}>0\), where \(n_{k}\) is the number of nodes mapped to the super-node \(k\). In this case the lifting matrix is particularly simple: \(Q^{+}\{0,1\}^{N n}\) (Fig. 1d). For simplicity, following the majority of the literature , in this paper we consider only well-mapped surjective coarsenings (but not necessarily uniform).

**Remark 1**.: _Non-well-mapped coarsenings may appear in the literature, e.g. when learning the matrix \(Q\) via a gradient-based optimization algorithm such as Diffpool . However, these methods often include regularization penalties to favor well-mapped coarsenings._

Restricted Spectral Approximation.A large part of the graph coarsening literature measures the quality of a coarsening by quantifying the modification of the spectral properties of the graph, often represented by the graph Laplacian \(L\). In , this is done by establishing a near-isometry property for graph signals with respect to the norm \(\|\|_{L}\), which can be interpreted as a measure of the smoothness of a signal across the graph edges. Given a signal \(x^{N}\) over the nodes of \(G\), we define the coarsened signal \(x_{c}^{n}\) and the re-lifted signal \(^{N}\) by

\[x_{c}=Qx,=Q^{+}x_{c}= x\] (2)

where \(=Q^{+}Q\). Loukas  then introduces the notion of _Restricted Spectral Approximation_ (RSA) of a coarsening algorithm, which measures how much the projection \(\) is close to the identity for a class of signals. Since \(\) is at most of rank \(n<N\), this cannot be true for all signals, but only for a restricted subspace \(^{N}\). With this in mind, the _RSA constant_ is defined as follows.

**Definition 1** (Restricted Spectral Approximation constant).: _Consider a subspace \(^{N}\), a Laplacian \(L\), a coarsening matrix \(Q\) and its corresponding projection operator \(=Q^{+}Q\). The RSA constant \(_{L,Q,}\) is defined as_

\[_{L,Q,}=_{x,\|x\|_{L}=1}\|x- x\|_{L}\] (3)

In other words, the RSA constant measures how much signals in \(\) are preserved by the coarsening-lifting operation, with respect to the norm \(\|\|_{L}\). Given some \(\) and Laplacian \(L\), the goal of a coarsening algorithm is then to produce a coarsening \(Q\)_with the smallest RSA constant possible_. While the "best" coarsening \(*{arg\,min}_{Q}_{L,Q,}\) is generally computationally unreachable, there are many possible heuristic algorithms, often based on greedy merging of nodes. In App. B.1, we give an example of such an algorithm, adapted from . In practice, \(\) is often chosen as the subspace spanned by the first eigenvectors of \(L\) ordered by increasing eigenvalue: intuitively, coarsening the graph and merging nodes is more likely to preserve the low-frequencies of the Laplacian.

While \(_{L,Q,} 1\) is required to obtain meaningful guarantees, we remark that \(_{L,Q,}\) is not necessarily finite. Indeed, as \(\|\|_{L}\) may only be a semi-norm, its unit ball is not necessarily compact. It is nevertheless finite in the following case.

**Proposition 1**.: _When \(\) is \((L)\)-preserving, it holds that \(_{L,Q,}/_{}}\)._

Hence, some examples where \(_{L,Q,}\) is finite include:

**Example 1**.: _For uniform coarsenings with \(L=D-A\) and connected graph \(G\), \((L)\) is the constant vector2, and \(\) is \((L)\)-preserving. This is the case examined by ._

**Example 2**.: _For positive definite "Laplacians", \((L)=\{0\}\). This is a deceptively simple solution for which \(\|\|_{L}\) is a true norm. This can be obtained e.g. with \(L= I_{N}+\) for any p.s.d. Laplacian \(\) and small constant \(>0\). This leaves its eigenvectors unchanged and add \(\) to its eigenvalues, and therefore does not alter the fundamental structure of the coarsening problem._

Given the adjacency matrix \(A^{N N}\) of \(G\), there are several possibilities to define an adjacency matrix \(A_{c}\) for the graph \(G_{c}\). A natural choice that we make in this paper is

\[A_{c}=(Q^{+})^{}AQ^{+}\,.\] (4)

In the case of a uniform coarsening, \((A_{c})_{k}\) equals the sum of edge weights for all edges in the original graph between all nodes mapped to the super-node \(k\) and all nodes mapped to \(\). Moreover, we have the following property, derived from .

[MISSING_PAGE_EMPTY:5]

this simplicity, we will see that under some mild hypotheses this choice indeed leads to preservation guarantees of message-passing for coarsenings with small RSA constants.

Orientation.An important remark is that, unlike all the examples in the literature, and unlike the adjacency matrix \(A_{c}\) defined in (4), the proposed matrix \(S_{c}^{}\)**is generally asymmetric, even when \(S\) is symmetric**. This means that our guarantees are obtained by performing _directed_ message-passing on the coarsened graph, even when the original message-passing procedure was undirected. Conceptually, this is an important departure from previous works. However \(S_{c}^{}\) becomes "more" symmetric when \(Q^{+}\) and \(Q^{T}\) becomes more similar. This is for instance the case when \(Q\) induces a balanced partition, where each supernodes has the same number of ancestors (which can be targeted by some pooling algorithms). On the contrary, the difference between \(Q\) and \(Q^{+}\) is more pronounced when supernodes are of very different sizes,which may happen for highly irregular graphs.

### Message-Passing guarantees

In this section, we show how the proposed propagation matrix (6) allows to transfer the spectral approximation guarantees to message-passing guarantees, under some hypotheses. First, we must make some technical assumptions relating to the kernel of the Laplacian.

**Assumption 1**.: _Assume that \(\) and \(S\) are both \((L)\)-preserving._

Moreover, since spectral approximation pertains to a subspace \(\), we must assume that this subspace is left unchanged by the application of \(S\).

**Assumption 2**.: _Assume that \(S\) is \(\)-preserving._

As mentioned before, for Examples 1 and 2, the projection \(\) is \((L)\)-preserving. Moreover, \(\) is often chosen to be the subspace spanned by the low-frequency eigenvectors of \(L\) and in this case, all matrices of the form \(S= I_{N}+ L\) for some constant \(,\) are both \((L)\)-preserving and \(\)-preserving. Hence, for instance, a primary example in practice is to choose GCNconv  with \(S=D()^{-}D()^{-}\) with \(=A+I_{N}\), and to compute a coarsening with a good RSA constant for the "Laplacian" \(L=(1+)I_{N}-S\) with small \(>0\) and \(\) spanned by eigenvectors of \(L\). In this case, Assumptions 1 and 2 are satisfied. This is the implementation we choose in our experiments.

We now state the main result of this section.

**Theorem 1**.: _Define \(S_{c}^{}\) as (6). Under Assumption 1 and 2, for all \(x\),_

\[\|Sx-Q^{+}S_{c}^{}x_{c}\|_{L}_{L,Q,}\|x\|_{L} (C_{S}+C_{})\] (7)

_where \(C_{S}:=\|S\|_{L}\) and \(C_{}:=\| S\|_{L}\)._

Sketch of proof.: The Theorem is proved in App. A. The proof is quite direct, and relies on the fact that, for this well-designed choice (6) of \(S_{c}^{}\), the lifted signal is precisely \(Q^{+}S_{c}^{}x_{c}= S x\). Then, bounding the error incurred by \(\) using the RSA, we show that this is indeed close to performing message-passing by \(S\) in the original graph. 

This theorem shows that the RSA error \(_{L,Q,}\) directly translates to an error bound between \(Sx\) and \(Q^{+}S_{c}^{}x_{c}\). As we will see in the next section, this leads to guarantees when training a GNN on the original graph and the coarsened graph. First, we discuss the two main multiplicative constant involved in Thm. 1.

Multiplicative constants.In full generality, for any matrix \(M\) we have \(\|M\|_{L}\|M\|/_{}}\). Moreover, when \(M\) and \(L\) commute, we have \(\|M\|_{L}\|M\|\). As mentioned before, choosing \(S= I_{N}+ L\) for some constants \(,\) is a primary example to satisfy our assumptions. In this case \(C_{S}=\|S\|_{L}\|S\|\). Then, if \(S\) is properly normalized, e.g. for the GCNconv  example outlined above, we have \(\|S\| 1\). For combinatorial Laplacian \(L=D-A\) however, we obtain \(\|S\|||+||N\). We observed in our experiments that the combinatorial Laplacian generally yields poor results for GNNs.

For \(C_{}\), in full generality we only have \(C_{} C_{S}\|\|_{L} C_{S}}{_{ }}}\), since \(\) is an orthogonal projector. However, in practice we generally observe that the exact value \(C_{}=\| S\|_{L}\) is far better than this ratio of eigenvalues (e.g. we observe a ratio of roughly \(C_{}(1/10)/_{}}\) in our experiments). Future work may examine more precise bounds in different contexts.

### GNN training on coarsened graph

In this section, we instantiate our message-passing guarantees to GNN training on coarsened graph, with SGC as a primary example. To fix ideas, we consider a single large graph \(G\), and a node-level task such as node classification or regression. Given some node features \(X^{N d}\), the goal is to minimize a loss function \(J:^{N}_{+}\) on the output of a GNN \(_{}(X,S)^{N}\) (assumed unidimensional for simplicity) with respect to the parameter \(\):

\[_{}R()R():=J(_{}(X,S))\] (8)

where \(\) is a set of parameters that we assume bounded. For instance, \(J\) can be the cross-entropy between the output of the GNN and some labels on training nodes for classification, or the Mean Square Error for regression. The loss is generally minimized by first-order optimization methods on \(\), which requires multiple calls to the GNN on the graph \(G\). Roughly, the computational complexity of this approach is \(O(T(N+E)D)\), where \(T\) is the number of iterations of the optimization algorithm, \(D\) is the number of parameters in the GNN, and \(E\) is the number of nonzero elements in \(S\). Instead, one may want to train on the coarsened graph \(G_{c}\), which can be done by minimizing instead3:

\[R_{c}():=J(Q^{+}_{}(X_{c},S_{c}^{}))\] (9)

where \(X_{c}=QX\). That is, the GNN is applied on the coarsened graph, and the output is then lifted to compute the loss, which is then back-propagated to compute the gradient of \(\). The computational complexity then becomes \(O(T(n+e)D+TN)\), where \(e E\) is the number of nonzeros in \(S_{c}^{}\), and the term \(TN\) is due to the lifting. As this decorrelates \(N\) and \(D\), it is in general much less costly.

We make the following two assumptions to state our result. Since our bounds are expressed in terms of \(\|\|_{L}\), we must handle it with the following assumption.

**Assumption 3**.: _We assume that there is a constant \(C_{J}\) such that_

\[|J(x)-J(x^{})| C_{J}\|x-x^{}\|_{L}\] (10)

For most loss functions, it is easy to show that \(|J(x)-J(x^{})|\|x-x^{}\|\), and when \(L\) is positive definite (Example 2) then \(\|\|}}\|\|_{L}\). Otherwise, one must handle the kernel of \(L\), which may be done on a case-by-case basis of for an appropriate choice of \(J\).

The second assumption relates to the activation function. It is here mostly for technical completeness, as _we do not have examples where it is satisfied beyond the identity \(=id\)_, which corresponds to the SGC architecture  often used in theoretical analyses .

**Assumption 4**.: _We assume that:_

1. \(\) _is_ \(\)_-preserving, that is, for all_ \(x\)_, we have_ \((x)\)_. We discuss this constraint below._
2. \(\|(x)-(x^{})\|_{L} C_{}\|x-x^{}\|_{L}\)_. Note that most activations are_ \(1\)_-Lipschitz w.r.t. the Euclidean norm, so this is satisfied when_ \(L\) _is positive-definite with_ \(C_{}=/_{}}\)_._
3. \(\) _and_ \(Q^{+}\) _commute:_ \((Q^{+}y)=Q^{+}(y)\)_. This is satisfied for all uniform coarsenings, or when_ \(\) _is_ \(1\)_-positively homogeneous:_ \(( x)=(x)\) _for nonnegative_ \(\) _(e.g. ReLU)._

Item i) above means that, when \(\) is spanned by low-frequency eigenvectors of the Laplacian, \(\) does not induce high frequencies. In other words, we want \(\) to preserve smooth signal. For now, the only example for which we can guarantee that Assumption 4 is satisfied is when \(=id\) and the GNN is linear, which corresponds to the SGC architecture . As is the case with many such analyses of GNNs, non-linear activations are a major path for future work. A possible study would be to consider random geometric graphs for which the eigenvectors of the Laplacian are close to explicit functions, e.g. spherical harmonics for dot-product graphs . In this case, it may be possible to explicitely prove that Assumption 4 holds, but this is out-of-scope of this paper.

Our result on GNNs is the following.

**Theorem 2**.: _Under Assumptions 1-4: for all node features \(X^{N d}\) such that \(X_{:,i}\), denoting by \(^{}=_{}R()\) and \(_{c}=_{}R_{c}()\), we have_

\[R(_{c})-R(^{}) C_{L,Q,}\|X\|_{:,L}\] (11)

_with \(C=2C_{J}C_{}^{k}C_{}(C_{S}+C_{})_{l=1}^{k}_{}^{k -l}C_{S}^{l-1}\) where \(_{}:=\| S\|_{L}\) and \(C_{}\) is a constant that depends on the parameter set \(\)._

The proof of Thm. 2 is given in App. A.3. In this proof, to apply the Theorem 1, we apply the RSA to each nodes features column. It relies on the assumption that each column of the nodes features \(X_{:,i}\) belongs to the preserved space \(\). This assumption seems reasonable for homophilic datasets (Cora, Citeseer) and large preserved space. The Theorem states that training a GNN that uses the proposed \(S_{c}^{}\) on the coarsened graph by minimizing (9) yields a parameter \(_{c}\) whose excess loss compared to the optimal \(^{}\) is bounded by the RSA constant. Hence, spectral approximation properties of the coarsening directly translates to guarantees on GNN training. The multiplicative constants \(C_{S},C_{}\) have been discussed in the previous section, and the same remarks apply to \(_{}\).

## 4 Experiments

**Setup.** We choose the propagation matrix from GCNconv , that is, \(S=f_{S}(A)=D()^{-}D()^{-}\) with \(=A+I_{N}\). As detailed in the previous section, we take \(L=(1+)I_{N}-S\) with \(=0.001\) and \(\) as the \(K\) first eigenvectors of \(L\) (\(K=N/10\) in our experiments), ensuring that Assumptions 1 and 2 are satisfied. In our experiments, we observed that the combinatorial Laplacian \(L=D-A\) gives quite poor results, as it corresponds to unusual propagation matrices \(S= I_{N}+ L\), and the constant \(C_{S}=\|S\|_{L}\) is very large. Hence our focus on the normalized case.

On coarsened graphs, we compare five propagation matrices:

* \(S_{c}^{}=QSQ^{+}\), our proposed matrix
* \(S_{c}=f_{S}(A_{c})\), the naive choice
* \(S_{c}^{diag}=}^{-1/2}(A_{c}+C)}^{-1/2}\), proposed in , where \(C\) is the diagonal matrix of the \(n_{k}\) and \(}\) the corresponding degrees. This yields theoretical guarantees for APPNP when \(S\) is GCNconv;
* \(S_{c}^{diff}=QSQ^{}\), which is roughly inspired by Diffpool ;
* \(S_{c}^{sym}=(Q^{+})^{}SQ^{+}\), which is the lifting employed to compute \(A_{c}\) (4).

Coarsening algorithm.Recall that the proposed \(S_{c}^{}\) can be computed for any coarsening, and that the corresponding theoretical guarantees depend on the RSA constant \(_{L,Q,}\). In our experiments, we adapt the algorithm from  to coarsen the graphs. It takes as input the graph \(G\) and the coarsening ratio desired \(r\) and output the propagation matrix \(S_{c}^{}\) and the coarsening matrix \(Q\) used for lifting. It is a greedy algorithm which successively merges edges by minimizing a certain cost. While originally designed for the combinatorial Laplacian, we simply adapt the cost to any Laplacian \(L\), see App. B.1. Note however that some mathematical justifications for this approach in  are no longer valid for normalized Laplacian, but we find in practice that it produces good RSA constants.

A major limit of this algorithm is its computational cost, which is quite high since it involves large matrix inversion and SVD computation. Hence we limit ourselves to middle-scale graphs like Cora  and Citeseer  and one larger graph with Reddit  in the following experiments. The design of more scalable coarsening algorithms with RSA guarantees is an important path for future work, but out-of-scope of this paper.

Figure 2: Message-Passing error for different propagation matrices.

Message passing preservation guaranteesTo evaluate the effectiveness of the proposed propagation matrix, we first illustrate the theoretical message passing preservation guarantees (Thm. 1 and 2) on synthetic graphs, taken as random geometric graph, built by sampling \(1000\) nodes with coordinates in \(^{2}\) and connecting them if their distance is under a given threshold (details in App. B.3). For each choice of propagation matrix and different coarsening ratio, we compute numerically \(\|S^{k}x-Q^{+}(S^{}_{c})^{k}x_{c}\|_{L}\) for various signals \(x\). We perform \(N_{p}=6\) message-passing steps to enhance the difference between propagation matrices. We evaluate and plot the upper bound defined by \(_{L,Q,}(C_{S}+C_{})_{l=1}^{k}C_{}^{k-l}C_{S}^{l-1}\) (seen in the proof of Theorem 2 in App. A.3) in Fig. 2. We observe that our propagation matrix incurs a significantly lower error compared to other choices, and that as expected, this error is correlated to \(_{L,Q,}\), which is not the case for other choices. More experiments can be found in App. B.4.

Node classification on real graphs.We then perform node classification experiments on real-world graphs, namely Cora  and Citeseer , using the public split from . For simplicity, we restrict them to their largest connected component4, since using a connected graph is far more convenient for coarsening algorithms (details in App. B.3). The training procedure follows that of Sec. 3.3: the network is applied to the coarsened graph and coarsened node features, its output is lifted to the original graph with \(Q^{+}\), and the label of the original training graph nodes are used to compute the cross-entropy loss, which is then back-propagated to optimize the parameters \(\) (pseudocode in App. B.2). Despite the lifting procedure, this results in faster training than using the entire graph (e.g., by approximately \(30\%\) for a coarsening ratio of \(r=0.5\) when parallelized on GPU). For downstream tasks we introduce a novel metric to analyze a specific coarsening : "Max acc possible". It corresponds to the optimal prediction over the super-nodes of the coarsened graph (all the nodes coarsened in a super nodes has the same prediction, optimally the majority label of this cluster). It might be hard to achieve as the optimal assignment for the validation nodes or training nodes can be different. It allows comparing different coarsenings for classification task without training models on it. We test SGC  with \(N_{p}=6\) and GCNconv  with \(N_{p}=2\) on four different coarsening ratio: \(r\{0.3,\ 0.5,\ 0.7\}\) where \(N_{p}\) is the number of propagation. Each classification results is averaged on \(10\) random training.

Results are reported in Table 1 and Table 2. We observe that the proposed propagation matrix \(S^{}_{c}\) yields better results and is more stable, especially for high coarsening ratio. The benefits are more evident when applied to the SGC architecture , for which Assumption 4 of Thm. 2 is actually satisfied, than for GCN, for which ReLU is unlikely to satisfy Assumption 4. It is also interesting to notice that training on coarsened graphs sometimes achieve better results than on the original graph. This may be explained by the fact that, for homophilic graphs (connected nodes are more likely to have the same label), nodes with similar labels are more likely to be merged together during the coarsening, and thus become easier to predict for the model. The detailed hyper-parameters for each model and each dataset can be found in appendix B.5.

Scaling to larger DatasetsWe performed experiments on the Reddit Dataset , which is approximately 100 times bigger than Cora or Citeseer. The Message-Passing error for different coarsening propagation matrices is reported in Table 3 with the node prediction results on two coarsening ratio \(r=90\%\) and \(r=99\%\) (their number of nodes,and edges can be found in App B.3), the details of the hyperparameters and coarsening procedure are in B.6. Our propagation matrix for

    SGC \\ \(r\) \\  } &  &  \\   & \(0.3\) & \(0.5\) & \(0.7\) & \(0.3\) & \(0.5\) & \(0.7\) \\  \(S^{sym}_{c}\) & 16.8\(\) 3.8 & 16.1 \(\) 3.8 & 16.4 \(\) 4.7 & 17.5 \(\) 3.8 & 18.6 \(\) 4.6 & 19.8 \(\) 5.0 \\ \(S^{diff}_{c}\) & 50.7 \(\) 1.4 & 21.8 \(\) 2.2 & 13.6 \(\) 2.8 & 50.5 \(\) 0.2 & 30.5 \(\) 0.2 & 23.1 \(\) 0.0 \\ \(S_{c}\) & 79.3 \(\) 0.1 & 78.7 \(\) 0.0 & 74.6 \(\) 0.1 & **74.1**\(\) 0.1 & 72.8 \(\) 0.1 & 72.5 \(\) 0.1 \\ \(S^{diag}_{c}\) & 79.9 \(\) 0.1 & 78.7 \(\) 0.1 & 77.3 \(\) 0.0 & 73.6 \(\) 0.1 & 73.4 \(\) 0.1 & 73.1 \(\) 0.4 \\ \(S^{}_{}\)**(ours)** & **81.8**\(\) 0.1 & **80.3**\(\) 0.1 & **78.5**\(\) 0.0 & 73.9 \(\) 0.1 & **74.6**\(\) 0.1 & **74.2**\(\) 0.1 \\ Max acc possible & 96.5 & 92.5 & 88.9 & 93.5 & 90.5 & 84.5 \\ Full Graph & 81.6 \(\) 0.1 & & & 73.6 \(\) 0.0 & \\   

Table 1: Accuracy in \(\%\) for node classification with SGC and different coarsening ratio coarsened graphs achieved a better Message-Passing error, close to the RSA-constant computed in the coarsened graph. It is consistent with the fact the Message-Passing error is bounded by Theorem 1 with our propagation matrix. Similarly, for the node prediction results, our propagation matrix \(S_{c}^{}\) achieves good results with the SGC model, close to the maximum accuracy possible on the given coarsening. Our propagation matrix is still competitive with the GCNconv model and achieved better results on the biggest coarsening ratio. These experiments show the effectiveness of our method on large graphs for which coarsening as a preprocessing step is crucial: indeed, on most small-scale machines with single GPU, the Reddit dataset is too large to fit in memory and requires adapted strategies.

## 5 Conclusion

In this paper, we investigated the interactions between graph coarsening and Message-Passing for GNNs. Surprisingly, we found out that even for high-quality coarsenings with strong spectral preservation guarantees, naive (but natural) choices for the propagation matrix on coarsened graphs does not lead to guarantees with respect to message-passing on the original graph. We thus proposed a new message-passing matrix specific to coarsened graphs, which naturally translates spectral preservation to message-passing guarantees, for any coarsening, under some hypotheses relating to the structure of the Laplacian and the original propagation matrix. We then showed that such guarantees extend to GNN, and in particular to the SGC model, such that training on the coarsened graph is provably close to training on the original one.

There are many outlooks to this work. Concerning the coarsening procedure itself, which was not the focus of this paper, new coarsening algorithms could emerge from our theory, e.g. by instantiating an optimization problem with diverse regularization terms stemming from our theoretical bounds. The scalability of such coarsening algorithms is also an important topic for future work. From a theoretical point of view, a crucial point is the interaction between non-linear activation functions and the low-frequency vectors in a graph (Assumption 4). We focused on the SGC model here, but a more in-depth study of particular graph models (e.g. random geometric graphs) could shed light on this complex phenomenon, which we believe to be a major path for future work.

    &  &  \\   & \(0.3\) & \(0.5\) & \(0.7\) & \(0.3\) & \(0.5\) & \(0.7\) \\  \(S_{c}^{sym}\) & 80.1 \(\) 1.3 & 78.1 \(\) 1.3 & 30.8 \(\) 2.5 & 71.0 \(\) 1.4 & 62.5 \(\) 11 & 52.7 \(\) 3.6 \\ \(S_{d}^{diff}\) & 81.9 \(\) 1.0 & 74.5 \(\) 0.9 & 62.6 \(\) 7.1 & 72.7 \(\) 0.4 & 71.2 \(\) 1.7 & 37.6 \(\) 0.9 \\ \(S_{c}\) & 81.2 \(\) 0.8 & 79.9 \(\) 0.9 & 78.1 \(\) 1.0 & 71.7 \(\) 0.6 & 70.7 \(\) 1.0 & 67.1 \(\) 3.1 \\ \(S_{c}^{diag}\) & 81.4 \(\) 0.8 & **80.4**\(\) 0.8 & **78.6**\(\) 1.3 & 72.1 \(\) 0.6 & 70.2 \(\) 0.8 & 69.3 \(\) 1.9 \\ \(S_{c}^{}\)**(ours)** & **82.1**\(\) 0.5 & 79.8 \(\) 1.5 & 78.2 \(\) 0.9 & **72.8**\(\) 0.8 & **72.0**\(\) 0.8 & **70.0**\(\) 1.0 \\ Max acc possible & 96.5 & 92.5 & 88.9 & 93.5 & 90.5 & 84.5 \\ Full Graph & & 81.6 \(\) 0.6 & & & 73.1 \(\) 1.5 & \\   

Table 2: Accuracy in \(\%\) for node classification with GCNconv and different coarsening ratio

    &  &  &  \\   & \(0.90\) & \(0.99\) & \(0.90\) & \(0.99\) & \(0.90\) & \(0.99\) \\  \(S_{c}^{sym}\) & 37.1 \(\) 6.6 & 3.7 \(\) 5.5 & 48.1 \(\) 8.9 & 34.8 \(\) 4.0 & 4.73e16 & 2.07e27 \\ \(S_{c}^{diff}\) & 18.3 \(\) 0.0 & 14.9 \(\) 0.0 & 71.3 \(\) 1.0 & 18.7 \(\) 1.7 & 0.92 & 1.00 \\ \(S_{c}\) & 87.5 \(\) 0.1 & 37.3 \(\) 0.0 & 88.0 \(\) 0.1 & 54.2 \(\) 2.4 & 2.46 & 1.75 \\ \(S_{c}^{diag}\) & 87.6 \(\) 0.1 & 37.3 \(\) 0.0 & **88.1**\(\) 0.2 & 55.5 \(\) 1.8 & 2.45 & 1.74 \\ \(S_{c}^{}\)**(ours)** & **90.2**\(\) 0.0 & **64.1**\(\) 0.0 & 84.4 \(\) 0.3 & **60.3**\(\) 0.9 & **0.22** & **0.88** \\ Max Acc Possible & 93.4 & 64.7 & 93.4 & 64.7 & Not applicable \\ Full Graph & & 94.9 & & Non computable (OOM) & Not applicable \\   

Table 3: Accuracy in \(\%\) for node classification on Reddit Dataset and Message passing errors