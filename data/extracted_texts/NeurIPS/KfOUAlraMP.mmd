# Wasserstein distributional robustness of neural networks

Xingjian Bai

Department of Computer Science

University of Oxford, UK

xingjian.bai@sjc.ox.ac.uk

&Guangyi He

Mathematical Institute

University of Oxford, UK

guangyihe2002@outlook.com

&Yifan Jiang

Mathematical Institute

University of Oxford, UK

yifan.jiang@maths.ox.ac.uk

&Jan Obloj*

Mathematical Institute

University of Oxford, UK

jan.obloj@maths.ox.ac.uk

Corresponding author. www.maths.ox.ac.uk/people/jan.obloj

###### Abstract

Deep neural networks are known to be vulnerable to adversarial attacks (AA). For an image recognition task, this means that a small perturbation of the original can result in the image being misclassified. Design of such attacks as well as methods of adversarial training against them are subject of intense research. We re-cast the problem using techniques of Wasserstein distributionally robust optimization (DRO) and obtain novel contributions leveraging recent insights from DRO sensitivity analysis. We consider a set of distributional threat models. Unlike the traditional pointwise attacks, which assume a uniform bound on perturbation of each input data point, distributional threat models allow attackers to perturb inputs in a non-uniform way. We link these more general attacks with questions of out-of-sample performance and Knightian uncertainty. To evaluate the distributional robustness of neural networks, we propose a first-order AA algorithm and its multistep version. Our attack algorithms include Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) as special cases. Furthermore, we provide a new asymptotic estimate of the adversarial accuracy against distributional threat models. The bound is fast to compute and first-order accurate, offering new insights even for the pointwise AA. It also naturally yields out-of-sample performance guarantees. We conduct numerical experiments on CIFAR-10, CIFAR-100, ImageNet datasets using DNNs on RobustBench to illustrate our theoretical results. Our code is available at https://github.com/JanObloj/W-DRO-Adversarial-Methods.

## 1 Introduction

Model uncertainty is a ubiquitous phenomenon across different fields of science. In decision theory and economics, it is often referred to as the _Knightian uncertainty_(Knight, 1921), or the _unknown unknowns_, to distinguish it from the _risk_ which stems from the randomness embedded by design in the scientific process, see Hansen and Marinacci (2016) for an overview. Transcribing to the context of data science, risk refers to the randomness embedded in a training by design, e.g., through random initialization, drop-outs etc., and uncertainty encompasses the extent to which the dataset is an adequate description of reality. _Robustness_, the ability to perform well under uncertainty, thus relates to several themes in ML including adversarial attacks, out-of-sample performance andout-of-distribution performance. In this work, we mainly focus on the former but offer a unified perspective on robustness in all of its facets.

Vulnerability of DNNs to crafted adversarial attacks (AA), diagnosed in Biggio et al. (2013), Goodfellow et al. (2015), relates to the ability of an attacker to manipulate network's outputs by changing the input images only slightly - often in ways imperceptible to a human eye. As such, AA are of key importance for security-sensitive applications and an active field of research. Most works so far have focused on attacks under _pointwise_\(l_{p}\)-bounded image distortions but a growing stream of research, pioneered by Staib and Jegelka (2017) and Sinha et al. (2018), frames the problem using Wasserstein distributionally robust optimization (DRO). We offer novel contributions to this literature.

Our key contributions can be summarized as follows. **1)** We propose a unified approach to adversarial attacks and training based on sensitivity analysis for Wasserstein DRO. We believe this approach, leveraging results from Bartl et al. (2021), is better suited for gradient-based optimization methods than duality approach adopted in most of the works to date. We further link the adversarial accuracy to the adversarial loss, and investigate the out-of-sample performance. **2)** We derive a general adversarial attack method. As a special case, this recovers the classical FGSM attack lending it a further theoretical underpinning. However, our method also allows one to carry out attacks under a _distributional threat model_ which, we believe, has not been done before. We also propose a rectified DLR loss suitable for the distributional attacks. **3)** We develop asymptotically certified bounds on adversarial accuracy, applicable to a general threat, including the classical pointwise perturbations. The bounds are first-order accurate and much faster to compute than, e.g., the AutoAttack (Croce and Hein, 2020) benchmark. The performance of our methods is documented using CIFAR-10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009), ImageNet (Deng et al., 2009) datasets and neural networks from RobustBench (Croce et al., 2021).

## 2 Related Work

Adversarial Attack (AA).Original research focused on the _pointwise_\(l_{p}\)-bounded image distortion. Numerous attack methods under this threat model have been proposed in the literature, including Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015), Projected Gradient Descent (PGD) (Madry et al., 2018), CW attack (Carlini and Wagner, 2017), etc. In these white-box attacks, the attacker has full knowledge of the neural network. There are also black-box attacks, such as Zeroth Order Optimization (ZOO) (Chen et al., 2017), Boundary Attack (Brendel et al., 2018), and Query-limited Attack (Ilyas et al., 2018). AutoAttack (Croce and Hein, 2020), an ensemble of white-box and black-box attacks, provides a useful benchmark for _pointwise_\(l_{p}\)-robustness of neural networks. Notably Hua et al. (2022) considered AA with \(l_{p}\) distance replaced by a proxy for human eye evaluation.

Adversarial Defense.Early works on data augmentation (Goodfellow et al., 2015; Madry et al., 2018; Tramer et al., 2018) make use of strong adversarial attacks to augment the training data with adversarial examples; more recent works (Gowal et al., 2021; Xing et al., 2022; Wang et al., 2023) focus on adding randomness to training data through generative models such as GANs and diffusion models. Zhang et al. (2019) consider the trade-off between robustness and accuracy of a neural network via TRADES, a regularized loss. Analogous research includes MART (Wang et al., 2020) and SCORE (Pang et al., 2022). Other loss regularization methods such as adversarial distributional training (Dong et al., 2020) and adversarial weight perturbation (Wu et al., 2020) have been shown to smooth the loss landscape and improve the robustness. In addition, various training techniques can be overlaid to improve robustness, including group-out layers, early stopping and parameter fine-tuning Sehwag et al. (2020). The closest to our setting are Sinha et al. (2018), Garcia Trillos and Garcia Trillos (2022) which employ Wasserstein penalization and constraint respectively. However, so far, even the papers which used distributional threat models to motivate DRO-based training methods actually used classical pointwise attacks to evaluate robustness of their trained DNNs. This highlights the novelty of our _distributional threat attack_.

Robust Performance Bounds.Each AA method gives a particular upper bound on the adversarial accuracy of the network. In contrast, research on _certified robustness_ aims at classifying images which are robust to all possible attacks allowed in the threat model and thus providing an attack-agnostic lower bound on the classification accuracy. To verify robustness of images, deterministic methods using off-the-shelf solvers (Tjeng et al., 2019), relaxed linear programming (Wong and Kolter, 2018,Weng et al., 2018) or semi-definite programming (Raghunathan et al., 2018; Dathathri et al., 2020) have been applied. Hein and Andriushchenko (2017); Weng et al. (2018) derive Lipschitz-based metrics to characterize the maximum distortion an image can uphold; Cohen et al. (2019) constructs a certifiable classifier by adding smooth noise to the original classifier; see Li et al. (2023) for a review.

Distributionally Robust Optimization (DRO).Mathematically, it is formulated as a min-max problem

\[_{}_{Q}_{Q}[f_{}(Z )],\] (1)

where we minimize the worst-case loss over all possible distributions \(Q\). In financial economics, such criteria appear in the context of multi-prior preferences, see (Gilboa and Schmeidler, 1989; Follmer and Weber, 2015). We refer to (Rahimian and Mehrotra, 2019) for a survey of the DRO.

We focus on the Wasserstein ambiguity set \(=B_{}(P)\), which is a ball centered at the reference distribution \(P\) with radius \(\) under the Wasserstein distance. We refer to Gao and Kleywegt (2022) for a discussion of many advantages of this distance. In particular, measures close to each other can have different supports which is key in capturing data perturbations, see Sinha et al. (2018). Staib and Jegelka (2017) interpreted _pointwise_ adversarial training as a special case of Wasserstein DRO (W-DRO). Volpi et al. (2018) utilized W-DRO to improve network performance on unseen data distributions. More recently, Bui et al. (2022) unified various classical adversarial training methods, such as PGD-AT, TRADES, and MART, under the W-DRO framework.

W-DRO, while compelling theoretically, is often numerically intractable. In the literature, two lines of research have been proposed to tackle this problem. The duality approach rewrites (1), changing the \(\) to a univariate \(\) featuring a transform of \(f_{}\). We refer to Mohajerin Esfahani and Kuhn (2018) for the data-driven case, Blanchet and Murthy (2019); Bartl et al. (2020); Gao and Kleywegt (2022) for general probability measures and Huang et al. (2022) for a further application with coresets. The second approach, which we adopt here, considers the first order approximation to the original DRO problem. This can be seen as computing the sensitivity of the value function with respect to the model uncertainty as derived in Bartl et al. (2021), see also Lam (2016), Garcia Trillos and Garcia Trillos (2022) for analogous results in different setups.

## 3 Preliminaries

Image Classification Task.An image is interpreted as a tuple \((x,y)\) where the feature vector \(x\) encodes the graphic information and \(y=\{1,,m\}\) denotes the class, or tag, of the image. W.l.o.g., we take \(=\)1. A distribution of labelled images corresponds to a probability measure \(P\) on \(\). We are given the training set \(_{tr}\) and the test set \(_{tt}\), subsets of \(\), i.i.d. sampled from \(P\). We denote \(\) (resp. \(\)) the empirical measure of points in the training set (resp. test set), i.e., \(=_{tr}|}_{(x,y)_{tr}} _{(x,y)}\). A neural network is a map \(f_{}:^{m}\)

\[f_{}(x)=f^{l} f^{1}(x),f^{i}(x)= (w^{i}x+b^{i}),\]

\(\) is a nonlinear activation function, and \(=\{w^{i},b^{i}:1 i l\}\) is the collection of parameters. We denote \(S\) the set of images equipped with their labels generated by \(f_{}\), i.e.,

\[S=(x,y):_{1 i m}f_{ }(x)_{i}=\{y\}}.\]

The aim of image classification is to find a network \(f_{}\) with high (clean) prediction accuracy \(A:=P(S)=_{P}[_{S}]\). To this end, \(f_{}\) is trained solving2 the stochastic optimization problem

\[_{}_{P}[L(f_{}(x),y)],\] (2)

where \(\) denotes the set of admissible parameters, and \(L\) is a (piecewise) smooth loss function, e.g., cross entropy loss\({}^{3}:^{m}\) given by

\[(z,y)=-(\,(z))_{y}.\] (3)Wasserstein Distances.Throughout, \((p,q)\) is a pair of conjugate indices, \(1/p+1/q=1\), with \(1 p\). We consider a norm \(\|\|\) on \(\) and denote \(\|\|_{*}\) its dual, \(\|\|_{*}=\{ x,:|x| 1\}\). Our main interest is in \(\|\|=\|\|_{r}\) the \(l_{r}\)-norm for which \(\|\|_{*}=\|\|_{*}\), where \((r,s)\) are conjugate indices, \(1 r\). We consider adversarial attacks which perturb the image feature \(x\) but not its label \(y\). Accordingly, we define a pseudo distance4\(d\) on \(\) as

\[d((x_{1},y_{1}),(x_{2},y_{2}))=\|x_{1}-x_{2}\|+_{\{y_{1} y _{2}\}}.\] (4)

We denote \((P,Q)\) the set of couplings between \((x,y)\) and \((x^{},y^{})\) whose first margin is \(P\) and second margin is \(Q\), and \(T_{\#}P:=P T^{-1}\) denotes the pushforward measure of \(P\) under a map \(T\).

The \(p\)-Wasserstein distance, \(1 p<\), between probability measures \(P\) and \(Q\) on \(\) is

\[_{p}(P,Q):=\{_{}[d((x_{1},y_{1}),(x_{2},y_{2 }))^{p}]:(P,Q)\}^{1/p}.\] (5)

The \(\)-Wasserstein distance \(_{}\) is given by

\[_{}(P,Q):=\{-\, d((x_{1},y_{1}),(x_{2},y _{2})):(P,Q)\}.\] (6)

We denote the \(p\)-Wasserstein ball centered at \(P\) with radius \(\) by \(B_{}(P)\). We mainly consider the cases where \(p,r\{2,\}\). Intuitively, we can view \(p\) as the index of image-wise flexibility and \(r\) as the index of pixel-wise flexibility. Unless \(p=1\) is explicitly allowed, \(p>1\) in what follows.

## 4 Wasserstein Distribution Robustness: adversarial attacks and training

W-DRO Formulation.The Wasserstein DRO (W-DRO) formulation of a DNN training task is given by:

\[_{}_{Q B_{}(P)}_{Q}[L(f_{}(x),y)],\] (7)

where \(B_{}(P)\) is the \(p\)-Wasserstein ball centered at \(P\) and \(\) denotes the budget of the adversarial attack. In practice, \(P\) is not accessible and is replaced with \(\). When \(p=\), the above adversarial loss coincides with the pointwise adversarial loss of Madry et al. (2018) given by

\[_{}_{P}[\{L(f_{}(x^{}),y):\|x^{ }-x\|\}].\]

Recently, Bui et al. (2022) considered a more general criterion they called _unified distributional robustness_. It can be re-cast equivalently as an _extended_ W-DRO formulation using couplings:

\[_{}_{_{}(P,)}_{}[J_{ }(x,y,x^{},y^{})],\] (8)

where \(_{}(P,)\) is the set of couplings between \((x,y)\) and \((x^{},y^{})\) whose first margin is \(P\) and the second margin is within a Wasserstein \(\)-ball centered at \(P\). This formulation was motivated by the observation that for \(p=\), taking \(J_{}(x,y,x^{},y^{})=L(f_{}(x),y)+ L(f_{}(x ),f_{}(x^{}))\), it retrieves the TRADES loss of (Zhang et al., 2019) given by

\[_{}_{P}L(f_{}(x),y)+_{x^{ }:\|x-x^{}\|}L(f_{}(x),f_{}(x^{ })).\]

W-DRO Sensitivity.In practice, training using (7), let alone (8), is computationally infeasible. To back propagate \(\) it is essential to understand the inner maximization problem denoted by

\[V()=_{Q B_{}(P)}_{Q}[J_{}(x,y)],\]

where we write \(J_{}(x,y)=L(f_{}(x),y)\). One can view the adversarial loss \(V()\) as a certain regularization of the vanilla loss. Though we are not able to compute the exact value of \(V()\) for neural networks with sufficient expressivity, DRO sensitivity analysis results allow us to derive a numerical approximation to \(V()\) and further apply gradient-based optimization methods. This is the main novelty of our approach -- previous works considering a W-DRO formulation mostly relied on duality results in the spirit of Blanchet and Murthy (2019) to rewrite (7).

**Assumption 4.1**.: We assume the map \((x,y) J_{}(x,y)\) is L-Lipschitz under \(d\), i.e.,

\[|J_{}(x_{1},y_{1})-J_{}(x_{2},y_{2})|d((x_{1},y_{1}),( x_{2},y_{2})).\]

The above assumption is weaker than the L-smoothness assumption encountered in the literature which requires Lipschitz continuity of gradients of \(J_{}\), see for example Sinha et al. (2018)[Assumption B] and Volpi et al. (2018)[Assumptions 1 & 2]. We also remark that our assumption holds for any continuously differentiable \(J_{}\) as the feature space \(=^{n}\) is compact.

The following result follows readily from (Bartl et al., 2021, Theorem 2.2) and its proof.

**Theorem 4.1**.: _Under Assumption 4.1, the following first order approximations hold:_

* \(V()=V(0)++o(),\) _where_ \[=_{P}\|_{x}J_{}(x,y)\|_{}^{q} ^{1/q}.\]
* \(V()=_{Q_{}}[J_{}(x,y)]+o(),\) _where_ \[Q_{}=(x,y)x+ h(_{x}J_{}(x,y))\| ^{-1}_{x}J_{}(x,y)\|_{}^{q-1},y)_{\#}P,\] _and_ \(h\) _is uniquely determined by_ \( h(x),x=\|x\|_{}\)_._

The above holds for any probability measure, in particular with \(P\) replaced consistently by an empirical measure \(\) or \(\). In Figure 1, we illustrate the performance of our first order approximation of the adversarial loss on CIFAR-10 (Krizhevsky, 2009) under different threat models.

WD-Adversarial Accuracy.We consider an attacker with perfect knowledge of the network \(f_{}\) and the data distribution \(P\), aiming to minimize the prediction accuracy of \(f_{}\) under an admissible attack. Complementing the W-DRO training formulation, Staib and Jegelka (2017), Sinha et al. (2018) proposed _Wasserstein distributional threat models_ under which an attack is admissible if the resulting attacked distribution \(Q\) stays in the \(p\)-Wasserstein ball \(B_{}(P)\), where \(\) is the attack budget, i.e., the tolerance for distributional image distortion. We define the adversarial accuracy as:

\[A_{}:=_{Q B_{}(P)}Q(S)=_{Q B_{}(P)}_ {Q}[_{S}].\] (9)

Note that \(A_{}\) is decreasing in \(\) with \(A_{0}=A\), the clean accuracy. For \(p=\), the Wasserstein distance essentially degenerates to the uniform distance between images and hence the proposed threat model coincides with the popular _pointwise_ threat model. For \(1 p<\), the _distributional_ threat model is strictly stronger than the _pointwise_ one, as observed in Staib and Jegelka (2017, Prop. 3.1). Intuitively, it is because the attacker has a greater flexibility and can perturb images close to the decision boundary only slightly while spending more of the attack budget on images farther away from the boundary. The threat is also closely related to out-of-distribution generalization, see Shen et al. (2021) for a survey.

Figure 1: Performance of the first order approximation for the W-DRO value derived in Theorem 4.1. Left: WideResNet-28-10 (Gowal et al., 2020) under CE loss (3) and \((_{},l_{})\) threat model with \(=1/255,,10/255\). Right: WideResNet-28-10 (Wang et al., 2023) under ReDLR loss (10) and \((_{2},l_{2})\) threat models with \(=1/16,,10/16\).

WD-Adversarial Attack.We propose _Wasserstein distributionally adversarial attack_ methods. As mentioned above, to date, even the papers which used distributional threat models to motivate DRO-based training methods then used classical pointwise attacks to evaluate robustness of their trained DNNs. Our contribution is novel and enabled by the explicit first-order expression for the distributional attack in Theorem 4.1(ii), which is not accessible using duality methods.

We recall the Difference of Logits Ratio (DLR) loss of Croce and Hein (2020). If we write \(z=(z_{1},,z_{m})=f_{}(x)\) for the output of a neural network, and \(z_{(1)} z_{(m)}\) are the order statistics of \(z\), then DLR loss is given by

\[(z,y)=--z_{(2)}}{z_{(1)}-z_{(3)}},& {if }z_{y}=z_{(1)},\\ --z_{(1)}}{z_{(1)}-z_{(3)}},&.\]

The combination of CE loss and DLR loss has been widely shown as an effective empirical attack for _pointwise_ threat models. However, under _distributional_ threat models, intuitively, an effective attack should perturb more aggressively images classified far from the decision boundary and leave the misclassified images unchanged. Consequently, neither CE loss nor DLR loss are appropriate -- this intuition is confirmed in our numerical experiments, see Table 1 for details. To rectify this, we propose ReDLR (Rectified DLR) loss:

\[(z,y)=-()^{-}(z,y)=--z_{(2 )}}{z_{(1)}-z_{(3)}},&z_{y}=z_{(1)},\\  142.26378pt0,&.\] (10)

Its key property is to leave unaffected those images that are already misclassified. Our experiments show it performs superior to CE or DLR.

An attack is performed using the test data set. For a given loss function, our proposed attack is:

\[x^{t+1}=\,_{}x^{t}+ h(_{x}J_{}(x^{ t},y))\|^{-1}_{x}J_{}(x^{t},y)\|_{*}^{q-1} ,\] (11)

where \(\) is the step size and \(\,_{}\) is a projection which ensures the empirical measure \(^{t+1}:=_{}|}_{(x,y) _{}}\,_{(x^{t+1},y)}\) stays inside the Wasserstein ball \(B_{}()\). In the case \(p=r=\), one can verify \(h(x)=\,(x)\) and write (11) as

\[x^{t+1}=\,_{}x^{t}+\,\,(_{x }J_{}(x^{t},y)).\]

This gives exactly Fast Gradient Sign Method (single step) and Projected Gradient Descent (multi-step) proposed in Goodfellow et al. (2015), Madry et al. (2018) and we adopt the same labels for our more general algorithms.5 A pseudocode for the above attack is summarized in Appendix C.

Finally, note that Theorem 4.1 offers computationally tractable approximations to the _W-DRO adversarial training_ objectives (7) and (8). In Appendix D we propose two possible training methods but do not evaluate their performance and otherwise leave this topic to future research.

## 5 Performance Bounds

Understanding how a DNN classifier will perform outside the training data set is of key importance. We leverage the DRO sensitivity results now to obtain a lower bound on \(A_{}\). We then use results on convergence of empirical measures in Fournier and Guillin (2015) to translate our lower bound into guarantees on out-of-sample performance.

Bounds on Adversarial Accuracy.We propose the following metric of robustness:

\[_{}:=}{A}.\]

Previous works mostly focus on the maximum distortion a neural network can withhold to retain certain adversarial performance, see Hein and Andriushchenko (2017), Weng et al. (2018) for local robustness and Bastani et al. (2016) for global robustness. However, there is no immediate connection between such a maximum distortion and the adversarial accuracy, especially in face of a distributionally adversarial attack. In contrast, since \(A=A_{0}\) is known, computing \(_{}\) is equivalent to computing \(A_{}\). We choose to focus on the relative loss of accuracy as it provides a convenient normalization: \(0_{} 1\). \(_{}=1\) corresponds to a very robust architecture which performs as well under attacks as it does on clean test data, while \(_{}=0\) corresponds to an architecture which loses all of its predictive power under an adversarial attack. Together the couple \((A,A_{})\) thus summarizes the performance of a given classifier. However, computing \(A_{}\) is difficult and time-consuming. Below, we develop a simple and efficient method to calculate theoretical guaranteed bounds on \(\) and thus also on \(A_{}\).

**Assumption 5.1**.: We assume that for any \(Q B_{}(P)\)

1. \(0<Q(S)<1\).
2. \(_{p}(Q(|S),P(|S))+_{p}(Q(|S^{c}),P( |S^{c}))=o(),\) where \(S^{c}=() S\) and the conditional distribution is given by \(Q(E|S)=Q(E S)/Q(S)\).

The first condition stipulates non-degeneracy: the classifier does not perform perfectly but retains some accuracy under attacks. The second condition says the classes are well-separated: for \(\) small enough an admissible attack can rarely succeed.

We write the adversarial loss condition on the correctly classified images and misclassified images as

\[C()=_{Q B_{}(P)}_{Q}[J_{}(x,y)|S] {and} W()=_{Q B_{}(P)}_{Q}[J_{}(x,y)|S ^{c}].\]

We note that an upper bound on \(_{}\) is given by any adversarial attack. In particular,

\[_{}_{}^{u}:=Q_{}(S)/A.\] (12)

**Theorem 5.1**.: _Under Assumptions 4.1 and 5.1, we have an asymptotic lower bound as \( 0\)_

\[_{}+o()= }_{}^{l}+o()=}_{ }^{l}+o(),\] (13)

_where the first order approximations are given by_

\[}_{}^{l}=_{Q_{}}[J_{ }(x,y)]}{W(0)-V(0)}}_{}^ {l}=.\] (14)

The equality between the lower bound and the two first-order approximations \(}_{}^{l}\) and \(}_{}^{l}\) follows from Theorem 4.1. Consequently, \(_{}^{l}:=\{}_{}^{l}, {}_{}^{l}\}\) allows us to estimate the model robustness without performing any sophisticated adversarial attack. Our experiments, detailed below, show the bound is reliable for small \(\) and is orders of magnitude faster to compute than \(_{}\) even in the classical case of pointwise attacks. The proof is reported in Appendix A. Its key ingredient is the following tower-like property.

**Proposition 5.2**.: _Under Assumptions 4.1 and 5.1, we have_

\[V()=_{Q B_{}(P)}_{Q}[C()_{S}+W ()_{S^{c}}]+o().\]

Bounds on Out-of-Sample Performance.Our results on distributionally adversarial robustness translate into bounds for performance of the trained DNN on unseen data. We rely on the results of Fournier and Guillin (2015) and refer to Lee and Raginsky (2018) for analogous applications to finite sample guarantees and to Gao (2022) for further results and discussion.

We fix \(1<p<n/2\) and let \(N=|_{tr}|\), \(M=|_{tt}|\). If sampling of data from \(P\) is described on a probability space \((,,)\) then \(\) is a random measure on this space and, by ergodic theorem, \(\)-a.s., it converges weakly to \(P\) as \(N\). In fact, \(_{p}(,P)\) converges to zero \(\)-a.s. Crucially the rates of convergence were obtained in Dereich et al. (2013), Fournier and Guillin (2015) and yield

\[[_{p}(,P)] KN^{-} (_{p}(,P))  K(-KN^{n}),\] (15)

where \(K\) is a constant depending on \(p\) and \(n\) which can be computed explicitly, see for example Guo and Obloj (2019, Appendix). This, with triangle inequality and Theorem 5.1, gives

**Corollary 5.3**.: _Under Assumptions 4.1 and 5.1 on measure \(\), with probability at least \(1-2K(-K^{n}\{M,N\})\) it holds that_

\[=(S)}_{2 }^{l}+o().\]

Next results provide a finer statistical guarantee on the out-of-sample performance for robust (W-DRO) training. Its proof is reported in Appendix B.

**Theorem 5.4**.: _Under Assumption 4.1, with probability at least \(1-K(-KN^{n})\) we have_

\[V()()+_{Q B_{}^{*}( )}_{Q}\|_{x}J_{}(x,y)\|_{s}^{q} ^{1/q}+o()()+\]

_where \(B_{}^{*}()=_{Q B_{}()} _{Q}[J_{}(x,y)]\) and constant \(K\) only depends on \(p\) and \(n\)._

Our lower bound estimate in Theorem 5.1 can be restated as

\[_{}:=-_{}()-(0)}{(0)-(0)}+o().\]

We now use Theorem 5.4 to bound \( A()\), the shortfall of the adversarial accuracy under \(P\), using quantities evaluated under \(\).

**Corollary 5.5**.: _Under Assumptions 4.1 and 5.1, with probability at least \(1-K(-KN^{n})\) it holds that_

\[ A_{}(P)()-(0)}{(0)-(0)}+}{(0)-(0)}+ o().\]

We remark that the above results are easily extended to the out-of-sample performance on the test set, via the triangle inequality \(_{p}(,)_{p}(,P)+ _{p}(P,)\). By using complexity measures such as entropy integral (Lee and Raginsky, 2018), Rademacher complexity (Gao, 2022; Gao et al., 2022) a further analysis can be undertaken for

\[_{}_{Q B_{}(P)}_{Q}[J_{}(x,y) ]_{}_{Q B_{}()} _{Q}[J_{}(x,y)].\] (16)

In particular, a dimension-free estimate of out-of-sample performance is obtained in (Gao, 2022) under a Lipschitz framework with light-tail reference measures.

## 6 Numerical Experiments

Experimental Setting.We conduct experiments on a high performance computing server equipped with 49 GPU nodes. The algorithms are implemented in Python. Experiments are conducted on CIFAR-10, CIFAR-100, and ImageNet datasets. Numerical results are consistent across different datasets, and we present the results on CIFAR-10 in body paragraph only for the sake of brevity. Results on CIFAR-100 and ImageNet are reported in Appendix F.

CIFAR-10 (Krizhevsky, 2009) comprises 60,000 color images across 10 mutually exclusive classes, with 6,000 images per class. Each image contains \(32 32\) pixels in 3 color channels. We normalize the input feature as a vector \(x^{3 32 32}\). The dataset is further divided into training and test sets, containing 50,000 and 10,000 images respectively. We evaluate the robustness of neural networks on the test set only.

We consider four threat models \((_{p},l_{r})\) with \(p,r\{2,\}\) with different range of attack budget \(\) depending on the relative strength of the attack. E.g., roughly speaking, if an \(l_{}\)-attack modifies one third of the pixels of an image with strength 4/255, then it corresponds to an \(l_{2}\)-attack with strength 1/2. When clear from the context, we drop the \(\) subscript.

We take top neural networks from RobustBench (Croce et al., 2021), a lively maintained repository that records benchmark robust neural networks on CIFAR-10 against _pointwise_ attacks. For _pointwise_ threat models \((_{},l_{r})\), RobustBench reports \(A_{}\) obtained using AutoAttack (Croce and Hein, 2020) for \(l_{},=8/255\) and \(l_{2},=1/2\), see Appendix H. However, due to high computationalcost of AutoAttack, we apply PGD-50 based on CE and DLR losses as a substitute to obtain the reference adversarial accuracy for attacks with relatively small budgets \(=2/255,4/255\) for \(l_{}\) and \(=1/8,1/4\) for \(l_{2}\). For _distributional_ threat models \((_{2},l_{r})\), there is no existing benchmark attacking method. Therefore, W-PGD attack (11) based on ReDLR loss is implemented to obtain the reference adversarial accuracy \(A_{}\). All PGD attacks are run with 50 iteration steps and take between 1 and 12 hours to run on a single GPU environment. Bounds \(^{l},^{u}\) compute ca. 50 times faster.

Distributionally Adversarial Attack.We report in Table 1 the average accuracy of top neural networks on RobustBench against pointwise and distributional attacks under different loss functions. The predicted drop in accuracy between a pointwise, i.e., \(\)-W-DRO attack and a distributional 2-W-DRO attack is only realized using the ReDLR loss.

In Figure 2, we compare the adversarial accuracy of robust networks on RobustBench against _pointwise_ threat models and _distributional_ threat models. We notice a significant drop of the adversarial accuracy even for those neural networks robust against _pointwise_ threat models.

Bounds on Adversarial Accuracy.We report in Table 2 the computation time of our proposed bounds \(^{l}_{}=\{}^{l}_{},}^{l}_{}\}\) in (14) and \(^{u}_{}\) in (12) with the computation time of \(_{}\) obtained from AutoAttack. Computing our proposed bounds \(^{l},^{u}\) is orders of magnitude faster than performing an attack to estimate \(\). This also holds for _distributional_ threat attacks.

To illustrate the applications of Theorem 5.1, we plot the bounds \(^{l}\) and \(^{u}\) against \(\) for neural networks on RobustBench. The results are plotted in Figure 3 and showcase the applicability of our bounds across different architectures.6 Note that smaller \(\) values are suitable for the stronger \(_{2}\)-distributional attack. For _pointwise_ threat models (top row) we compute the bounds using CE loss. For _distributional_ threat models (bottom row), reference adversarial accuracy is obtained from a

    & \(_{}\) & _{2}\)} \\  Methods & AutoAttack & W-PGD-CE & W-PGD-DLR & W-PGD-ReDLR \\  \(l_{}\) & 57.66\% & 61.32\% & 79.00\% & **45.46\%** \\ \(l_{2}\) & 75.78\% & 74.62\% & 78.69\% & **61.69\%** \\   

Table 1: Comparison of adversarial accuracy of neural networks on RobustBench under different empirical attacks. Set attack budget \(=8/255\) for \(l_{}\) threat models and \(=1/2\) for \(l_{2}\) threat models.

    & PreActResNet-18 & ResNet-18 & ResNet-50 & WRN-28-10 & WRN-34-10 & WRN-70-16 \\  \(\) & 197 & 175 & 271 & 401 & 456 & 2369 \\ \(^{l}\&^{u}\) & 0.52 & 0.49 & 0.17 & 0.55 & 0.53 & 1.46 \\   

Table 2: Computation times of \((_{},l_{}),=8/255\) attack for one mini-batch of size \(100\), in seconds. We compute \(\) by AutoAttack and average the computation time over models on RobustBench grouped by their architecture.

Figure 2: Shortfall of WD-adversarial accuracy with different metrics \(l_{}\) (left) and \(l_{2}\) (right).

W-PGD-ReDLR attack and, accordingly, we use ReDLR loss to compute \(^{u}\) and \(^{l}\). In this case, the width of the gap between our upper and lower bounds varies significantly for different DNNs. To improve the bounds, instead of \(^{l}\), we could estimate \(V()\) and use the lower bound in (13). This offers a trade-off between computational time and accuracy which is explored further in Appendix E.

## 7 Limitations and Future Work

Limitations.Our theoretical results are asymptotic and their validity is confined to the linear approximation regime. We believe that the empirical results we presented from all the leaderboard models on RobustBench across different attack types provide an overwhelming evidence that our results are valid and relevant for the range of attack budget \(\) considered in AA settings. However, as \(\) increases we are likely to go outside of the linear approximation regime, see Figure 1. In Appendix E we plot the results for pointwise attack with \(=8/255\) where some of neural networks have a lower bound \(^{l}\) greater than the reference \(\). We do not have theoretical results to provide guarantees on the range of applicability but, in Appendix E, discuss a possible rule of thumb solution.

Future Work.We believe our research opens up many avenues for future work. These include: developing stronger attacks under distributional threat models, testing the performance of the two training algorithms derived here and investigating further sensitivity-based ones, as well as analyzing the relation between the values and optimizers in (16), verifying empirical performance of our out-of-sample results, including Corollary 5.5, and extending these to out-of-distribution performance.

Broader Impact.Our work contributes to the understanding of robustness of DNN classifiers. We believe it can help users in designing and testing DNN architectures. It also offers a wider viewpoint on the question of robustness and naturally links the questions of adversarial attacks, out-of-sample performance, out-of-distribution performance and Knightian uncertainty. We provide computationally efficient tools to evaluate robustness of DNNs. However, our results are asymptotic and hence valid for small attacks and we acknowledge the risk that some users may try to apply the methods outside of their applicable regimes. Finally, in principle, our work could also enhance understanding of malicious agents aiming to identify and attack vulnerable DNN-based classifiers.