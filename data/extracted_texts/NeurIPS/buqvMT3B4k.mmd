# Self-Labeling the Job Shop Scheduling Problem

Andrea Corsini Angelo Porrello Simone Calderara Mauro Dell'Amico

University of Modena and Reggio Emilia, Italy

{name.surname}@unimore.it

###### Abstract

This work proposes a self-supervised training strategy designed for combinatorial problems. An obstacle in applying supervised paradigms to such problems is the need for costly target solutions often produced with exact solvers. Inspired by semi- and self-supervised learning, we show that generative models can be trained by sampling multiple solutions and using the best one according to the problem objective as a pseudo-label. In this way, we iteratively improve the model generation capability by relying only on its self-supervision, eliminating the need for optimality information. We validate this _Self-Labeling Improvement Method (SLIM)_ on the Job Shop Scheduling (JSP), a complex combinatorial problem that is receiving much attention from the neural combinatorial community. We propose a generative model based on the well-known Pointer Network and train it with SLIM. Experiments on popular benchmarks demonstrate the potential of this approach as the resulting models outperform constructive heuristics and state-of-the-art learning proposals for the JSP. Lastly, we prove the robustness of SLIM to various parameters and its generality by applying it to the Traveling Salesman Problem.

## 1 Introduction

The Job Shop Problem (JSP) is a classic optimization problem with many practical applications in industry and academia . At its core, the JSP entails scheduling a set of jobs across multiple machines, where each job has to be processed on the machines following a specific order. The goal of this problem is generally to minimize the _makespan_: the time required to complete all jobs .

Over the years, various approaches have been developed to tackle the JSP. A common one is adopting exact methods such as Mixed Integer Programming solvers (MIP). However, these methods often struggle to provide optimal or high-quality solutions on medium and large instances in a reasonable timeframe . As a remedy, metaheuristics have been extensively investigated  and constitute an alternative to exact methods. Although state-of-the-art metaheuristics like  can rapidly provide quality solutions, typically within minutes, they are rather complex to implement and their results can be difficult to reproduce . Therefore, due to their lower complexity, Priority Dispatching Rules (PDR)  are frequently preferred in practical applications with tighter time constraints. The main issue of PDRs is their tendency to perform well in some cases and poorly in others, primarily due to their rigid, hand-crafted prioritization schema based on hardcoded rules .

Recent works have increasingly investigated Machine Learning (ML) to enhance or replace some of these approaches, primarily focusing on PDRs. ML-based approaches, specifically Reinforcement Learning (RL) ones, proved to deliver higher-quality solutions than classic PDRs at the cost of a small increase in execution time . Despite the potential of RL , training RL agents is a complex task , is sensitive to hyper-parameters , and has reproducibility issues .

Contrary, supervised learning is less affected by these issues but heavily relies on expensive annotations . This is particularly problematic in combinatorial problems, where annotations in theform of (near-) optimal information are generally produced with expensive exact solvers. To contrast labeling cost and improve generalization, _semi-supervised_ and _self-supervised_ are gaining popularity in many fields due to their ability to learn from unlabeled data. Despite this premise, little to no application of these paradigms can be found in the JSP and the combinatorial literature [5; 9].

Motivated by these observations, we introduce a novel self-supervised training strategy that is simpler than most RL algorithms, does not require the formulation of the Markov Decision Process , and relies only on model-generated information, thus removing the need for expensive annotations. Our proposal is based on two weak assumptions: i) we suppose to be able to generate _multiple solutions_ for a problem, a common characteristic of generative models ; and ii) we suppose it is possible to _discriminate solutions based on the problem objective_. When these assumptions are met, we train a model by generating multiple solutions and using the best one according to the problem objective as a pseudo-label . This procedure draws on the concept of pseudo-labeling from semi-supervised learning, but it does not require any external and expansive annotation as in self-supervised learning. Hence, we refer to it as a _SLIM: Self-Labeling Improvement Method_.

We prove the effectiveness of SLIM primarily within the context of Job Shop, a challenging scheduling problem with many baseline algorithms [1; 36] and established benchmarks [44; 46; 16]. As recognized in other works [54; 39; 25], focusing on the JSP is crucial because its study helps address related variants, such as Dynamic JSP  and Flow Shop , while also establishing a concrete base for tackling more complicated scheduling problems like Flexible JSP .

Similar to PDRs, we cast the generation of solutions as a sequence of decisions, where at each decision one job operation is scheduled in the solution under construction. This is achieved with a generative model inspired by the _Pointer Network_, a well-known architecture for dealing with sequences of decisions in combinatorial problems . We train our model on random instances of different sizes by generating multiple parallel solutions and using the one with the _minimum makespan_ to update the model. This training strategy produces models outperforming state-of-the-art learning proposals and other conventional JSP algorithms on popular benchmark sets. Furthermore, our methodology demonstrates robustness and effectiveness across various training regimes, including a brief analysis on the Traveling Salesman Problem to emphasize the broader applicability of SLIM beyond scheduling. In summary, the contributions of this work are:

* Our key contribution is the introduction of SLIM, a novel self-labeling improvement method for training generative models. Thanks to its minimal assumptions, SLIM can be easily applied as-is to other combinatorial problems.
* Additionally, we present a generative encoder-decoder architecture capable of generating high-quality solutions for JSP instances in a matter of seconds.

The remainder of this work is organized as follows: Sec. 2 reviews related ML works; Sec. 3 formalizes the JSP; Sec. 4 introduces our generative model and SLIM; Sec. 5 compares our approach with others; Sec. 6 studies additional aspects of our proposal; and Secs. 7 and 8 close with general considerations, some limitations, and potential future directions.

## 2 Related Works

ML approaches have been recently investigated to address issues of traditional JSP methodologies.

Motivated by the success of **supervised learning**, various studies have employed exact methods to generate optimality information for synthetic JSP instances, enabling the learning of valuable scheduling patterns. For example,  used imitation learning  to derive superior dispatching rules from optimal solutions, highlighting the limitations of learning solely from optimal solutions.  proposed a joint utilization of optimal solutions and Lagrangian terms during training to better capture JSP constraints. Whereas, in , the authors trained a Recurrent Neural Network (RNN) to predict the quality of machine permutations - generated during training with an MIP - for guiding a metaheuristic. Despite their quality, these proposals are limited by their dependence on costly optimality information.

To avoid the need for optimality, other works have turned to **reinforcement learning**. The advantage of RL lies in its independence from costly optimal information, requiring only the effective formulation of the Markov Decision Process . Several works focused on creating better neural PDRs, showcasing the effectiveness of policy-based methods [54; 39; 25; 11]. All these methods adopted a Proximal Policy Optimization algorithm  and proposed different architectures or training variations. For instance, [54; 39] adopted Graph Neural Networks (GNN),  proposed a rather complex Transformer , while  used RNNs and curriculum learning . Differently,  presented a Double Deep Q-Network approach, proving the applicability of value-based methods.

Other works employed RL to enhance other algorithmic approaches. In , an iterative RL-based approach was proposed to rewrite regions of JSP solutions selected by a learned policy. Whereas  utilized a Deep Q-learning approach to control three points of interventions within metaheuristics, and  recently presented an RL-guided improvement heuristic. Differently,  presented a hybrid imitation learning and policy gradient approach coupled with Constraint Programming (CP) for outperforming PDRs and a CP solver.

## 3 Job Shop Scheduling

In the _Job Shop Problem_, we are given a set of jobs \(J=\{J_{1},,J_{n}\}\), a set of machines \(M=\{M_{1},,M_{m}\}\), and a set of operations \(O=\{1,,o\}\). Each job \(j J\) comprises a sequence of \(m_{j}\) consecutive operations \(O_{j}=(l_{j},\,,l_{j}+m_{j}-1) O\) indicating the order in which the job must be performed on machines, where \(l_{j}=1+_{i=1}^{j-1}m_{i}\) is the index of its first operation (e.g., \(l_{1}=1\) for \(J_{1}\) and \(l_{2}=3\) for \(J_{2}\) in Fig. 1). An operation \(i O\) has to be performed on machine \(_{i} M\) for an uninterrupted amount of time \(_{i}_{ 0}\). Additionally, machines can handle one operation at a time. The objective of the JSP is to provide an order to operations on each machine, such that the precedences within operations are respected and the time required to complete all jobs (**makespan**) is minimized. Formally, we indicate with \(\) a JSP solution comprising a permutation of operations for each machine and use \(C_{i}()\) to identify the completion time of operation \(i O\) in \(\).

A JSP instance can be represented with a **disjunctive graph**\(G=(V,A,E)\), where: \(V\) contains one vertex for each operation \(i O\); \(A\) is the set of directed arcs connecting consecutive operations of jobs reflecting the order in which operations must be performed; \(E\) is the set of _disjunctive_ (undirected) edges connecting operations to perform on the same machines. When the JSP is represented as a disjunctive graph \(G\), finding a solution means providing a direction to all the edges in \(E\), such that the resulting graph is directed and acyclic (all precedences are respected). As in related works [54; 39], we augment the disjunctive graph by associating to each vertex a set of \(15\) features \(x_{i}^{15}\) describing information about operation \(i O\). For lack of space, we present these features in App. A.

### Constructing Feasible Job Shop Solutions

Feasible JSP solutions can be constructed step-by-step as a _sequence of decisions_, a common approach adopted by PDRs  and RL algorithms [54; 25]. The construction of solutions can be visualized using a branch-decision tree, shown at the bottom of Fig. 1. Each path from the root node (\(R\)) to a leaf node presents a particular sequence of \(o=|O|\) decisions and leads to a valid JSP solution, such as the one highlighted in gray. At every node in the tree, one uncompleted job needs to be selected, and its ready operation is scheduled in the solution being constructed. Due to the precedence constraints and the partial solution \(_{<t}\) constructed up to decision \(t\), there is at most one operation \(o(t,\,j) O_{j}\) that is ready to be scheduled for any job \(j J\). Thus, by selecting a job \(j\), we uniquely identify an operation \(o(t,\,j)\) that will be scheduled by appending it to the partial permutation of its machine. Notice that once a job is completed, it cannot be selected again, and such a situation is identified with a cross in Fig. 1.

We emphasize that diverse paths can lead to the same solution, indicating certain solutions are more frequent than others. In some instances, near-optimal solutions may lie at the end of unlikely and hardly discernible paths, making their construction more challenging. This could explain why PDRs,

Figure 1: The sequences of decisions for constructing solutions in a JSP instance with two jobs (\(J_{1}\) and \(J_{2}\)) and two machines (identified in green and red). Best viewed in colors.

which rely on rigid, predefined rules, sometimes struggle to generate quality solutions, whereas neural algorithms - able to leverage instance- and solution-wide features - may perform better.

## 4 Methodology: Generative Model & Self-Labeling

As described in Sec. 3.1, we tackle the JSP as a sequence of decisions. Thus, we propose a generative model inspired by the Pointer Network  (a well-known encoder-decoder architecture to generate sequences of decisions) whose goal is to select the right job at each decision step \(t\). Formally, our model learns a function \(f_{}()\), parametrized by \(\), that estimates the probability \(p_{}(|G)\) of a solution \(\) being of high-quality, expressed as a product of probabilities:

\[p_{}(|G)=_{t=1}^{|O|}f_{}(y_{t}\,|\,_{<t},G),\] (1)

where \(f_{}(y_{t}\,|_{<t},G)\) gives the probability of selecting job \(y_{t} J\) for creating \(\), conditioned on the instance \(G\) and the partial solution \(_{<t}\) constructed up to step \(t\). By accurately learning \(f_{}()\), the model can construct high-quality solutions autoregressively. The following sections present the proposed encoder-decoder architecture and the self-labeling strategy to train this generative model.

### The Generative Encoder-Decoder Architecture

**Overview.** Our model processes JSP instances represented as a disjunctive graph \(G\) and produces single deterministic or multiple randomized solutions depending on the adopted construction strategy. The _encoder_ operates at the instance level, creating embedded representations for all the operations in \(O\) with a single forward computation of \(G\) (hence, no autoregression). Whereas the _decoder_ operates at the solution level (job and machine level), using the embeddings of ready operations and the partial solution \(_{<t}\) to produce a probability of selecting each job at step \(t\). Recall that after selecting a job \(j\), its ready operation \(o(t,\,j)\) is scheduled in \(_{<t}\) to produce the partial solution for step \(t+1\).

**Encoder.** It captures instance-wide relationships into the embeddings of operations, providing the decoder with a high-level view of the instance characteristics. The encoder can be embodied by any architecture, like a Feedforward Neural Network (FNN), that transforms raw operation features \(x_{i}^{15}\) (see App. A for the features) into embeddings \(e_{i}^{h}\). As in related works [54; 39], we also encode the relationships among operations present in the disjunctive graph. We thus equip the encoder with Graph Neural Networks , allowing the embeddings to incorporate topological information of \(G\). In our encoder, we stack two Graph Attention Network (GAT) layers  as follows:

\[e_{i}=[x_{i}\,||\,(_{2}(\,[x_{i}\,||\,\,(_{1} (x_{i},\,G))],\,G)],\] (2)

where \(\) is the ReLU non-linearity and \(||\) stands for the concatenation operation.

**Decoder.** It produces at any step \(t\) the probability of selecting each job from the embeddings \(e_{i}\) and solution-related features. The encoder is logically divided into two distinct components:

* _Memory Network:_ generates a state \(s_{j}^{d}\) for each job \(j J\) from the partial solution \(_{<t}\). This is achieved by first extracting from \(_{<t}\) a context vector \(c_{j}\) (similarly to ), which contains

   ID & Description & Rationale \\ 
1 & \(C_{o(t,j)-1}(_{<t})\) minus the completion time of machine \(_{o(t,j)}\). & The idle time of job \(j\) if scheduled on its next machine \(_{o(t,j)}\) at time \(t\). Negative if the job could have started earlier. \\
2 & \(C_{o(t,j)-1}(_{<t})\) divided by the makespan of \(_{<t}\). & How close job \(j\) is to the makespan of the partial solution \(_{<t}\). \\
3 & \(C_{o(t,j)-1}(_{<t})\) minus the average completion time of all jobs. & How early/late job \(j\) completes w.r.t. the average among all jobs in \(_{<t}\). \\
4 & The difference between \(C_{o(t,j)-1}(_{<t})\) and the \(1^{st}\), \(2^{nd}\), and \(3^{rd}\) & Describe the relative completion of \(j\) w.r.t. other jobs. These features complement that with ID = 3. \\
6 & quartile computed among the completion time of all jobs. & How close the completion time of machine \(_{o(t,j)}\) is to the makespan of the partial solution \(_{<t}\). \\
7 & The completion time of machine \(_{o(t,j)}\) minus the average completion of all machines in \(_{<t}\). & How early/late machine \(_{o(t,j)}\) completes w.r.t. the average among machines in \(_{<t}\). \\
9. & The difference between the completion of \(_{o(t,j)}\) and the \(1^{st}\), \(2^{nd}\), and \(3^{rd}\) quartile among the completion time of all machines. & Describe the relative completion time of machine \(_{o(t,j)}\) w.r.t. all the other machines. These features complement that with ID = 8. \\   

Table 1: The features of a context vector \(c_{j}^{11}\) that describes the status of a job \(j\) within a partial solution \(_{<t}\). Recall that \(o(t,j)\) is the ready operation of job \(j\) at step \(t\), \(_{o(t,j)}\) its machine, and \(o(t,j)-1\) its predecessor.

eleven hand-crafted features providing useful cues about the status of job \(j\). We refer to Tab. 1 for the definition and meaning of these features. Then, these vectors are fed into a Multi-Head Attention layer (MHA)  followed by a non-linear projection to produce jobs' states: \[s_{j}=([c_{j}\,W_{}+}(c_{b}\,W_{ })]\,W_{}),\] (3) where \(W_{}\) and \(W_{}\) are projection matrices. Note that we use the MHA to consider the context of all jobs when producing the state for a specific one, similarly to .
* _Classifier Network_: outputs the probability \(p_{j}\) of selecting a job \(j\) by combining the embedding \(e_{o(t,\,j)}\) of its ready operation and the state \(s_{j}\) produced by the memory network. To achieve this, we first concatenate the embeddings \(e_{o(t,\,j)}\) with the states \(s_{j}\) and apply an FNN: \[z_{j}=([e_{o(t,\,j)}\,||\,s_{j}]).\] (4) Then, these scores \(z_{j}\) are transformed into softmax probabilities: \(p_{j}=e^{z_{j}}\,/\,_{b J}e^{z_{b}}\). The final decision on which job to select at \(t\) is made using a sampling strategy, as explained next.

**Sampling solutions.** To generate solutions, we employ a probabilistic approach for deciding the job selected at any step. Specifically, we randomly sample a job \(j\) with a probability \(p_{j}\), produced at step \(t\) by our decoder. We also prevent the selection of completed jobs by setting their scores \(z_{j}=-\) to force \(p_{j}=0\) before sampling. Note how this probabilistic selection is _autoregressive_, meaning that sampling a job depends on \(p_{j}\) which is a function of \(e_{o(t,\,j)}\) and \(s_{j}\) resulting from earlier decisions (see Eqs. 3 and 4). Various strategies exist for sampling from autoregressive models, including top-k , nucleus , and random sampling . Preliminary experiments revealed that these strategies perform similarly, with average optimality gap variations within \(0.2\%\) on benchmark instances. However, top-k and nucleus sampling introduce brittle hyperparameters that can hinder training convergence if not managed carefully. Therefore, we adopt the simplest strategy for training and testing, which is _random sampling_: sampling a job \(j\) with probability \(p_{j}\) at random. Note however that the sampling strategy remains a flexible design choice, not inherently tied to our methodology.

### SLIM: Self-Labeling Improvement Method

We propose a self-supervised training strategy that uses the model's output as a teaching signal, eliminating the need for optimality information or the formulation of Markov Decision Processes. Our strategy exploits two aspects: the capacity of generative models to construct multiple (parallel) solutions and the ability to discriminate solutions of combinatorial problems based on their objective values, such as the makespan in the JSP. With these two ingredients, we design a procedure that at each iteration generates multiple solutions and uses the best one as a _pseudo-label_.

More in detail, for each training instance \(G\), we randomly sample a set of \(\) solutions from the generative model \(f_{}()\). We do that by keeping \(\) partial solutions in parallel and independently sample for each one the next job from the probabilities generated by the model. Once the solutions have been completely generated, we take the one with the minimum makespan \(\) and use it to compute the Self-Labeling loss (\(_{}\)). This loss minimizes the _cross-entropy_ across all the steps as follows:

\[_{}()=-_{t=1}^{|O|} f _{}(y_{t}\,|\,_{<t},G),\] (5)

where \(y_{t} J\) is the index of the job selected at the decision step \(t\) while constructing the solution \(\).

The rationale behind this training schema is to collect knowledge about sequences of decisions leading to high-quality solutions and distill it in the parameters \(\) of the model. This is achieved by treating the best-generated solution \(\) for an instance \(G\) as a pseudo-label and maximizing its likelihood using Eq. 5. Similar to supervised learning, repeated exposure to various training instances enables the model to progressively refine its ability to solve combinatorial problems such as the JSP. Hence, we named this training strategy _Self-Labeling Improvement Method (SLIM)_.

**Relations with Existing Works**

Although our strategy was developed independently, attentive readers may find a resemblance with the Cross-Entropy Method (CEM), a stochastic and derivative-free optimization method . Typically applied to optimize parametric models such as Bernoulli and Gaussian mixtures models , the CEM relies on a maximum likelihood approach to either estimate a random variable or optimize the objective function of a problem . According to Algorithm 2.2 in , the CEM independently samples \(N=\) solutions, selects a subset based on the problem's objective (\(=S_{(N)}\) in our case), and updates the parameters of the model.

While the CEM independently tackles instances by optimizing a separate model for each, SLIM trains a single model on multiple instances to globally learn how to solve a combinatorial problem. Moreover, we adopt a more complex parametric model instead of mixture models, always select a single solution to update the model, and resort to the gradient descent for updating parameters \(\). Notably, our strategy also differs from CEM applications to model-based RL, e.g., , as we do not rely on rewards in Eq. 5. In summary, our strategy shares the idea of sampling and selecting with the CEM but globally operates as a supervised learning paradigm once the target solution is identified.

Other self-labeling approaches can be found, e.g., in [10; 2], where the former uses K-Means to generate pseudo-labels, and the latter assigns labels to equally partition data with an optimal transportation problem. As highlighted in , these approaches may incur in degenerate solutions that trivially minimize Eq. 5, such as producing the same solution despite the input instance in our case. We remark that SLIM avoids such degenerate solutions by jointly using a probabilistic generation process and the objective value (makespan) of solutions to select the best pseudo-label \(\).

## 5 Results

This section outlines the implementation details, introduces the selected competitors, and presents the key results. All the experiments were performed on an Ubuntu 22.04 machine equipped with an Intel Core i9-11900K and an NVIDIA GeForce RTX 3090 GPU having \(24\) GB of memory.1

### Experimental Setup

**Dataset & Benchmarks.** To train our model, we created a dataset of \(30\,000\) instances as in  by randomly generating \(5000\) instances per shape (\(n m\)) in the set: \(\{10 10,\,15 10,\,15 15,\,20 10,\,20 15,\,20 20\}\). While our training strategy does not strictly require a fixed dataset, we prefer using it to enhance reproducibility. For testing purposes, we adopted two challenging and popular benchmark sets to evaluate our model and favor cross-comparison. The first set is from Taillard , containing \(80\) instances of medium-large shapes (\(10\) instances per shape). The second set is the Demirkol's one , containing 80 instances (\(10\) per shape) which proved particularly challenging in related works [54; 25]. We additionally consider the smaller and easier Lawrence's benchmark  and extremely larger instances in Apps. C and E, respectively.

**Metric.** We assess performance on each benchmark instance using the Percentage Gap: \(=100(C_{alg},/,C_{ub}-1)\), where \(C_{alg}\) is the makespan produced by an algorithm and \(C_{ub}\) is either the optimal or best-known makespan for the instance. Lower PG values indicate better results, as they reflect solutions with an objective value closer to the optimal or best-known makespan.

**Architecture.** In all our experiments, we configure the model of Sec. 4.1 in the same way. Our _encoder_ consists of two GAT layers , both with \(3\) attention heads and leaky slope at \(0.15\). In GAT\({}_{1}\), we set the size of each head to \(64\) and concatenate their outputs; while in GAT\({}_{2}\), we increase the head's size to \(128\) and average their output to produce \(e_{i}^{143}\) (\(h=15+128\)). Inside the _decoder's memory network_, the MIA layer follows  but it concatenates the output of \(3\) heads with \(64\) neurons each, while \(W_{1}^{11 192}\) and \(W_{2}^{192 128}\) use \(192\) and \(128\) neurons, respectively. Thus, the job states \(s_{j}^{d}\) have size \(d=128\). Finally, the _classifier_ FNN features a dense layer with \(128\) neurons activated through the Leaky-ReLU (\(=0.15\)) and a final linear with \(1\) neuron.

**Training.** We train this generative model with SLIM (see Sec. 4.2) on our dataset for \(20\) epochs, utilizing the Adam optimizer  with a constant learning rate of \(0.0002\). In each training step, we accumulate gradients over a batch of size \(16\), meaning that we update the model parameters \(\) after processing \(16\) instances. During training and validation, we fix the number of sampled solutions \(\) to \(256\) and save the parameters producing the lower average makespan on a hold-out set comprising \(100\) random instances per shape included in our dataset. Training in this way takes approximately \(120\) hours, with each epoch lasting around \(6\) hours.

**Competitors.** We compare the effectiveness of our methodology against various types of conventional algorithms and ML competitors reviewed in Sec. 2. We divide competitors as follows:

* _Greedy Constructives_ generate a single solution for any input instance. We consider two cornerstone ML works for the JSP: the actor-critic (L2D) of  and the recent curriculum approach (CL) of . We also coded the INSertion Algorithm (INSA) of  and three standard dispatching rules that prioritize jobs based on the Shortest Processing Time (SPT); Most Work Remaining (MWR); and Most Operation Remaining (MOR).
* _Multiple (Randomized) Constructives_ generate multiple solutions for an instance by introducing a controlled randomization in the selection process, a simple technique for enhancing constructive algorithms [50; 25]. We consider randomized results of CL and L2D. As only greedy results were disclosed for L2D, we used the open-source code to sample randomized solutions as described in Sec. 4.1. We also consider the three dispatching rules above, randomized by arbitrarily scheduling an operation among the three with higher priority, and the Shifting Bottleneck Heuristic (SBH)  that creates multiple solutions while optimizing. All these approaches were seeded with \(12345\), generate \(=128\) solutions, and return the one with minimum makespan.
* _Non-constructive Approaches_ do not rely on a pure constructive strategy for creating JSP solutions. We include two RL-enhanced metaheuristics: the NLS\({}_{A}\) in  (\(200\) iterations) and the recent L2S proposal of  visiting 500 (L2S\({}_{500}\)) and \(5000\) (L2S\({}_{5k}\)) solutions. We also consider two state-of-the-art solvers for the JSP: Gurobi 9.5 (MIP) solving the disjunctive formulation of  and the CP-Sat (CP) of Google OR tools 9.8, both executing with a time limit of \(3600\) seconds.

For lack of space, we compare our methodology against other learning proposals in App. B.

### Performance on Benchmarks

This section evaluates the performance of our generative Graph Model (GM), configured and trained with SLIM as explained in Sec. 5.1. When contrasted with greedy approaches, our GM generates a single solution by picking the job with the highest probability. In the other comparisons, we randomly sample \(128\) (GM\({}_{128}\)) and \(512\) (GM\({}_{512}\)) solutions as explained in Sec. 4.1. Note that we coded our GM, PDRs, INSA, SBH, MIP, and CP; while we reported results from original papers of all the ML competitors but L2D, for which we used the open-source code to generate randomized solutions.

Tab. 2 presents the comparison of algorithms on Taillard's and Demirkol's benchmarks, each arranged in a distinct horizontal section. The table is vertically divided into _Greedy Constructive_, _Multiple (Randomized) Constructive_, and _Non-constructive Approaches_; with the results of algorithms categorized accordingly. Each row reports the average PG (the lower the better) on a specific instance shape while the last row (Avg) reports the average gap across all instances, regardless of their shapes.

This table shows that our GM and CL produce lower gaps than PDRs, INSA, and SBH in both the greedy and randomized cases, proving the superiority of neural constructive approaches. Surprisingly,

    & &  &  &  \\  & & & & & RL & Our & & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & & & & & & & & & \\   _{128}\)), it outperforms all the randomized constructives. Notably, the GM generalizes well to instance shapes larger than training ones (marked with * in Tab. 2), as its gaps do not progressively increase on such shapes.

Our GM is also competitive when compared with non-constructive approaches. The GM\({}_{512}\) largely outperforms the NLS\({}_{A}\) and L2S\({}_{500}\) metaheuristics, and roughly align with L2S\({}_{5k}\) visiting \(5000\) solutions. On shapes larger than \(20 20\), the GM\({}_{512}\) remarkably achieves lower gaps with just a few seconds of computations than a MIP executing for \(3600\) sec. Therefore, we conclude that our methodology, encompassing the proposed GM and SLIM, is effective in solving the JSP.

As already observed in [25; 55], our GM and other neural constructive approaches are generally outperformed by CP and state-of-the-art metaheuristics, e.g., . Although the performance gap is steadily narrowing, the higher complexity of CP solvers renders them more powerful, albeit at the cost of longer execution time (often dozens of minutes). Therefore, neural constructive approaches may be preferable whenever a quality solution must be provided in a few minutes.

We refer the reader to Apps. D and E for statistical considerations and a comparison between our GM and CP on extremely large instances, respectively.

## 6 A Closer Look at SLIM

This section studies additional aspects of our proposed methodology. For these evaluations, we adopt the same setting described in Sec. 5.1 and explicitly indicate the varied aspects and parameters.

### Self-Labeling other Architectures

To demonstrate the generality of our self-labeling improvement method beyond the proposed Graph Model (GM), we use it to train other architectures as described in Sec. 5.1. Specifically, we train the model proposed in  (CL\({}_{}\)) and a variation of our GM (named FNN\({}_{}\)), where we replaced the Graph Attention and Multi-Head Attention layers with linear projections (ReLU activated) by maintaining the same hidden dimensionalities. Tab. 3 reports the average gap of CL\({}_{}\), FNN\({}_{}\), and GM on each benchmark. As baselines for comparison, we also include the average gap of CL and CL\({}_{}\) reported in , the latter being trained without curriculum learning similarly to our setting.

Tab. 3 shows that CL\({}_{}\) nearly matches the performance of CL even without curriculum learning. Note that we intentionally avoided applying curriculum learning to eliminate potential biases when demonstrating the contribution of our self-labeling strategy. Furthermore, we observe that both FNN\({}_{}\) and GM outperform CL, with GM achieving the best overall performance. Therefore, we conclude that SLIM can successfully train well-designed architectures for the JSP.

### Comparison with Proximal Policy Optimization

The previous Sec. 6.1 proved the quality of our GM when trained with SLIM. As Proximal Policy Optimization (PPO) is extensively used to train neural algorithms for scheduling problems (see Sec. 2), we contrast the GM's performance when trained with PPO and our self-labeling strategy (SLIM-\(\) with \(\{32,64,128\}\)). For this evaluation, we adopt the same hyper-parameters of  for both PPO and SLIM by training on \(40\,000\) random instances of shape \(30 20\) (same training of L2D ). We only double the batch size to \(8\) to reduce training noise. Fig. 4 reports the validation curves obtained by testing the GM on Taillard's benchmark every \(100\) training steps. We test by sampling \(128\) solutions and we also include the randomized results of L2D (dashed line) as a baseline for comparisons. From Fig. 4, we observe that training with our self-labeling improvement method results in faster convergence and produces better final models.

We justify this by hypothesizing that the reward received from partial solutions (makespan increments) may not always provide reliable guidance on how to construct the best final solution. While constructing a solution, the critical path may change with implications on past rewards. Our self-labeling strategy does not rely on partial rewards and may avoid such a source of additional "noise". Note that this is only an intuition, proving it would require a deeper analysis, which we refer to in future works. Finally, we stress that we do not claim SLIM is superior to PPO. Instead, we believe it offers an alternative that provides a fresh perspective and potential for integration with existing RL algorithms to advance the neural combinatorial optimization field.

### The Effect of \(\) on Training

As our self-labeling improvement method is based on sampling, we assess the impact of sampling a different number of solutions \(\) while training. We retrain a new GM as described in Sec. 5.1 with a number of solutions \(\{32,64,128,256\}\), where we stop at \(256\) as the memory usage with larger values becomes impractical. We test the resulting models by sampling \(512\) solutions on all the instances of both benchmarks for a broader assessment. Fig. 4 reports the average PG (the lower the better) of the trained GM (colored markers) on each shape. To ease comparisons, we also include the results of CL (dashed line) - the second-best ML proposal in Tab. 4 - and the MIP (dotted line).

Overall, we see that training by sampling more solutions slightly improves the model's performance, as outlined by lower PGs for increasing \(\). We also observe that such improvement is less marked on shapes seen in training, such as in \(15 15,20 15\), and \(20 20\) shapes, and more marked on others. This suggests that training by sampling more solutions results in better generalization, although it is more memory-demanding. However, the GM's trends observed in Tab. 4 remain consistent with smaller \(\), proving the robustness of SLIM to variations in the number of sampled solutions \(\).

### The Effect of \(\) on Testing

We also assess how the number of sampled solutions \(\) impacts the GM performance at test time. To evaluate such an impact, we plot in Fig. 4 the average PG on different shapes for varying \(\{32,64,128,256,512\}\). For this analysis, we use the GM trained by sampling 256 solutions, the one of Tab. 4. As in Sec. 6.3, we also report the results of CL and MIP to ease the comparison.

Despite the reduced number of solutions, the GM remains a better alternative than CL - the best RL proposal - and still outperforms the MIP on medium and large instances. Not surprisingly, by sampling more solutions the GM performance keeps improving at the cost of increased execution times. Although we verified that sampling more than \(512\) solutions further improves results (see bottom of App. F), we decided to stop at \(=512\) as a good trade-off between performance and time. We refer the reader to App. F for other timing considerations.

### Self-Labeling the Traveling Salesman Problem

Finally, we provide a brief analysis to demonstrate the broader applicability of our self-labeling improvement method to other combinatorial problems. Thus, we evaluate SLIM on the Traveling Salesman Problem (TSP), a cornerstone in neural combinatorial optimization research [5; 50; 35], by using it to train the well-known Attention Model  on TSP instances with 20 nodes. To assess SLIM's performance, we compare it against the established Policy Optimization with Multiple Optima (POMO) approach . We train the Attention Model with both SLIM and POMO, using the same hyperparameters and sampling strategy outlined in . Training is carried out over 100,000 steps (equivalent to 1 epoch in ) by generating batches of 32 instances at each step.

Fig. 5 presents the validation curves of SLIM and POMO obtained by testing the model every 100 steps on small instances (with up to 100 nodes) from the TSPLIB . The results are reported in terms of the average optimality gap and, as a baseline for comparison, we include the performance of the best model in  (POMO20) trained for hundreds of epochs. As shown, SLIM achieves faster convergence than POMO and produces a model comparable to POMO20 after just one epoch, thereby demonstrating that SLIM can be effective in other combinatorial problems.

## 7 Limitations

Despite the proven effectiveness of SLIM, it is important to note that only one of the sampled solutions per training instance is used to update the model. This approach may be sub-optimal from an efficiency standpoint. Therefore, we see significant potential in hybridizing our self-labeling strategy with existing (RL) methods to mitigate this inefficiency. Moreover, sampling multiple solutions during training requires substantial memory, which can limit batch sizes. Although we have shown that SLIM remains effective with a small number of sampled solutions (see Sec. 6.3), increasing their number accelerates training convergence and improves the resulting model. Consequently, developing new strategies that can sample higher-quality solutions without generating numerous random ones is a promising future direction. Such advancements could reduce memory usage and further enhance our methodology as well as others in the literature.

## 8 Conclusions

The key contribution of this work is the introduction of SLIM, a novel Self-Labeling Improvement Method to train generative models for the JSP and other combinatorial problems. Additionally, an efficient encoder-decoder architecture is presented to rapidly generate parallel solutions for the JSP. Despite its simplicity, our methodology significantly outperformed many constructive and learning algorithms for the JSP, even surpassing a powerful MIP solver. However, as a constructive approach, it still lags behind state-of-the-art approaches like constraint programming solvers, which nevertheless require more time. We also proved the robustness of SLIM across various parameters and architectures, and its generality by applying it successfully to the Traveling Salesman Problem.

More broadly, self-labeling might be a valuable training strategy as it eliminates the need for optimality information or the precise formulation of a Markov Decision Process. For instance, given a designed generative model (i.e., a constructive algorithm whose decisions are taken by a neural network), this strategy can be applied as-is to combinatorial problems with unconventional objectives or a combination of objective functions. In contrast, RL approaches require the careful definition of a meaningful reward function, which is often a complex and challenging task.

In future, we intend to apply our methodology to other shop scheduling and combinatorial problems as well as explore combinations of our self-supervised strategy with other existing learning approaches.

Figure 5: Validation curves obtained by training with SLIM and POMO on random TSP instances with \(20\) nodes. POMO20 is the best model produced in , trained on instances with \(20\) nodes.