# _Cspg_: Crossing Sparse Proximity Graphs for Approximate Nearest Neighbor Search

Ming Yang, Yuzheng Cai, Weiguo Zheng

School of Data Science, Fudan University, China

{ yangm24, yuzhengcai21 } @m.fudan.edu.cn zhengweiguo@fudan.edu.cn

###### Abstract

The state-of-the-art approximate nearest neighbor search (ANNS) algorithm builds a large proximity graph on the dataset and performs a greedy beam search, which may bring many unnecessary explorations. We develop a novel framework, namely _crossing sparse proximity graph (CSPG)_, based on random partitioning of the dataset. It produces a smaller sparse proximity graph for each partition and routing vectors that bind all the partitions. An efficient two-staged approach is designed for exploring _CSPG_, with fast approaching and cross-partition expansion. We theoretically prove that _CSPG_ can accelerate the existing graph-based ANNS algorithms by reducing unnecessary explorations. In addition, we conduct extensive experiments on benchmark datasets. The experimental results confirm that the existing graph-based methods can be significantly outperformed by incorporating _CSPG_, achieving 1.5x to 2x speedups of _QPS_ in almost all recalls.

## 1 Introduction

_Nearest Neighbor Search_ (NNS) aims to find some vectors in a set of high-dimensional vectors with the smallest distance to a query vector. It is becoming increasingly popular in various application domains , such as information retrieval , pattern recognition , recommendation systems , and retrieval augmented generation (RAG) . However, it is costly to find the exact nearest neighbors in practice, thus recent studies have focused on _Approximate Nearest Neighbor Search_ (ANNS), which targets efficiency while mildly relaxing accuracy constraints .

Existing ANNS algorithms can be divided into four categories , including tree-based approaches , hashing-based approaches , quantization-based approaches , and graph-based approaches . Among these approaches, graph-based ANNS algorithms stand out with the high answer quality and low latency , by constructing a _Proximity Graph (shorted as PG)_ on the given vector dataset. As shown in Figure 1, each vector is represented by a node in the graph, and each node is connected to its nearby neighbors.

For the greedy beam search over the proximity graph, it is observed that the distance computation dominates the overall time cost . Since at each step, all neighbors of the current node are pushed into the candidate set according to the computed distance. The number of distance computations can be calculated as \(\) multiplied by the number of explored nodes , where \(\) is the average degree of the graph. Intuitively, searching within a smaller graph requires less exploration and thus reduces the overall cost, which has been proved for a particular type of proximity graph, i.e., Monotonic Search Network (MSNET) . However, for most proximity graphs, it is very likely to degrade the answer quality when the graph is smaller.

In this paper, we present a novel and effective framework, namely _Crossing Sparse Proximity Graph (CSPG)_, enabling efficient search while not sacrificing answer quality. The basic idea is to reduce the number of explored vectors by searching in the much smaller graphs. Specifically, we randomly divide the whole dataset into several partitions, and for each partition, we construct a proximity graph that is smaller than the proximity graph built on the whole dataset. These partitions share a set of _routingvectors_ (Section 3.1) that allow the greedy search to travel across different partitions dynamically. The query process involves two stages, i.e., fast approaching and cross-partition expansion. The first stage conducts the greedy search within one partition, using a small candidate set to quickly approach the nearby regions of the query vector. Then, the second stage continues the greedy search with a larger candidate set, allowing it to travel across different partitions for more precise results.

We theoretically prove that the expected number of explored vectors when searching across these small graphs is the same as searching on the small proximity graph for one of the partitions. Hence, by random partitioning with randomly sampled routing vectors, we can reduce the number of explored vectors compared with the traditional proximity graph built on the whole dataset, thus reducing the number of distance computations. By integrating _CSPG_ with various graph-based ANNS algorithms, extensive experiments show that it significantly speeds up the query performance on benchmark datasets, and the detailed empirical results also align with our theoretical analysis.

Contributions.In summary, we make the following contributions in this paper.

* To improve the query performance by reducing the number of explored vectors, we propose a general framework, namely _Crossing Sparse Proximity Graph (CSPG)_, through random partitioning and random routing vectors. This framework can integrate with and enhance the existing graph-based ANNS indexes.
* We develop an efficient two-staged search paradigm over the _CSPG_, including fast approaching and cross-partition expansion.
* We theoretically prove that _CSPG_ can benefit the existing graph-based ANNS algorithms by introducing _Approximate Monotonic Search Network_ (AMSNET) that considers the distance backtracking in the search path.
* Extensive experiments confirm that by integrating the _CSPG_, the existing graph-based algorithms can be speeded up significantly under the same answer quality.

## 2 Background

### Problem definition

Let \(=\{v_{1},v_{2},...,v_{n}\}\) denote the dataset of \(n\) vectors, where \(v_{i}\) represents a vector in the \(d\)-dimensional Euclidean space \(^{d}\). The L2 distance between any two vectors \(p^{d}\) and \(q^{d}\) is denoted as \((p,q)\). The task of \(k\)-nearest neighbor search (\(k\)-NNS) can be defined as follows.

**Definition 1** (\(k\)-Nearest Neighbor Search, shorted as \(k\)-Nns).: _Given a dataset \(\) and a query vector \(q\), \(k\)-NNS returns a subset of \(k\) vectors, denoted by \(T\), such that for any \(t T\) and \(v T\), we have \((v,q)(t,q)\)._

**Definition 2** (\(k\)-Approximate Nearest Neighbor Search, shorted as \(k\)-Anns ).: _Given a dataset \(\) and a query vector \(q\), \(k\)-ANNS returns a subset of \(k\) vectors, denoted by \(S\), such that \(|S T|\) is as large as possible, where \(T\) is the answer set to \(k\)-NNS w.r.t. the query \(q\). For simplicity, \(k\) is omitted when \(k=1\)._

In other words, the task of \(k\)-ANNS returns \(k\) approximate closest vectors of the query vector, not guaranteeing all the exact top-\(k\) nearest vectors, to improve query efficiency.

### Graph-based ANNS algorithms

As discussed above, graph-based ANNS algorithms conduct a best-first greedy beam search on the proximity graphs to approach the closest nodes for a query vector. Their built proximity graphs can be classified into four categories [16; 40] as follows. Please refer to Appendix A for more details.

Figure 1: An example dataset of vectors and its proximity graph.

**Delaunay Graph (DG) [41; 42]**. It ensures that for any edge, no other vectors will be situated within the hypersphere defined by an edge connecting two vectors, where the hypersphere is centered at the midpoint of the edge and the length of the edge is the diameter.

**Relative Neighborhood Graph (RNG)**. It guarantees that for any edge between \(p\) and \(q\), no other vectors will reside within the \(lune(p,q)=\{u^{d}\,|\,(u,p)(p,q)(u,q) (p,q)\}\). RNG imposes stricter restrictions on its edges, thus decreasing the average degree .

**K-Nearest Neighbor Graph (KNNG)**. In KNNG, neighbors of each vector \(v\) are its top-\(k\) nearest neighbors in \(\). NN-Descent  proposes a method for constructing KNNG.

**Minimum Spanning Tree (MST) Graph**. The MST utilizes distances between vectors as edge weights. Then, it performs hierarchical clustering on the dataset multiple times randomly, adding some edges to the edge set. MST can establish global connectivity with a minimal number of edges.

### Monotonic Search Network

Monotonic Search Network provides theoretical results to understand the costs of greedy search.

**Definition 3** (Monotonic Path, shorted as MP ).: _Given a proximity graph built on dataset \(\), for two nodes \(p\) and \(u\) in the graph, a path from \(p\) to \(u\) is denoted as \(p u=\{v_{1},v_{2},...,v_{t}\}\), where \(p=v_{1}\) and \(u=v_{t}\). It is a monotonic path iff it satisfies that \((v_{1},u)>(v_{2},u)>...>(v_{t-1},u)\)._

**Definition 4** (Monotonic Search Network, shorted as MSNET ).: _Given a dataset \(\) consisting of \(n\) vectors in the space \(^{d}\), a proximity graph built on \(\) is called a monotonic search network iff there exists at least one monotonic path from \(p\) to \(u\) for any two nodes \(p\) and \(u\) in \(\)._

When running a greedy beam search in an MSNET, we will continuously approach the query vector since the distance strictly decreases at each step, i.e., distance backtracking can be avoided . Let \(C\) denote the smallest convex hull that can cover a set of \(n\)\(d\)-dimensional vectors \(\), and let \(R\) represent the maximum distance between two vectors in \(\). Denote the volume of \(C\) as \(V_{C}\) and let \(V_{B}(,R)\) represent the volume of a sphere with radius \(R\). For an MSNET built on \(\), when there exists a constant \(\) s.t. \( V_{C} V_{B}(,R)\), which implies that the distribution of vectors should be relatively uniform (never in some extremely special shape), the search length expectation of an MSNET (denoted as \(^{M}\)) is \((n^{} n^{}/ r)\) as proved in , where

\( r=_{v_{1},v_{2},v_{3}}\{|(v_ {1},v_{2})-(v_{1},v_{3})|,\;|(v_{1},v_{2} )-(v_{2},v_{3})|,\;|(v_{1},v_{3})- (v_{2},v_{3})|\}.\)

In other words, \( r\) is the minimum distance difference for any non-isosceles triangle on \(\). As \(n\) increases, \( r\) decreases and approaches a constant value when \(n\) is large .

## 3 Crossing Sparse Proximity Graphs

### _Csp_G: Crossing Sparse Proximity Graphs

An effective approach to the \(k\)-ANNS problem is expected to identify more vectors that are closest to the query with a minimal cost. A straightforward approach is to relax edge selection by allowing a vector to connect with both nearby and relatively distant neighbors. To guarantee efficiency, the node degree cannot be increased too much, making it challenging to maintain both nearby and distant neighbors. To address the problem, we propose a method to maximize the number of vectors searched near the query without increasing average node degrees. The basic idea is randomly partitioning the dataset \(\) into multiple groups that share some common vectors, called _routing vectors_. Then a sparse proximity graph (shorted SPG) is built for each group of vectors. Since these SPGs are sparser than that built for the whole dataset \(\), allowing larger steps to approach the query quickly. Moreover, the _routing vectors_ across multiple SPGs enable efficient fine-grained search. For ease of presentation, let \(PG()\) denote the proximity graph built on the dataset \(\).

**Definition 5** (Random Partition).: _Given a vector dataset \(\), the group of subsets \(_{1},_{2},,_{m}\) constitute a random partition of \(\) such that (1) \(_{1}_{2}_{m}=\), (2) \(_{1}_{2}_{m}=\), and (3) \((_{i})(_{i})=\) for \(i j\), where \(_{i}\) is randomly sampled from \(\) and \(\) is the common vectors shared by all the subsets (also called routing vectors)._

**Definition 6** (Crossing Sparse Proximity Graphs, shorted as _CSPG_).: _Given a vector dataset \(\), its CSPG(\(\)) consists of multiple proximity graphs \(_{1},_{2},,_{m}\) for \(\)'s random partition \(_{1},_{2},,_{m}\), respectively, i.e., \(_{i}=PG(_{i})\)._

Note that \(_{i}\) is sparser than \(\) as it is randomly sampled from \(\). Thus, the average edge length (i.e., the distance between two vectors) of the resultant proximity graph \(_{i}\) is larger than that of theproximity graph for \(\). Generally, any existing graph-based index can be used to build the proximity graphs in _CSPG_. Since the partitions share routing vectors, the corresponding proximity graphs are interrelated through these routing vectors. Hence, the routing vectors serve to navigate the greedy search across different proximity graphs. Let \(v_{j}^{i}\) denote that vector \(v_{j}\) belongs to graph \(_{i}\).

**Example 1**.: _For the dataset \(=\{v_{1},v_{2},...,v_{9}\}\) in Figure 1, we build the CSPG in Figure 2, by randomly sampling 2 routing vectors \(=\{v_{4},v_{7}\}\). And there are two partitions \(_{1}=\{v_{1},v_{3},v_{4},v_{5},v_{7},v_{9}\}\) and \(_{2}=\{v_{2},v_{4},v_{6},v_{7},v_{8}\}\), where routing vectors are highlighted in red, with the green graph representing \(_{1}\) and the blue graph representing \(_{2}\)._

### Novelty of _Cspg_

**Comparison with _Inverted File Index (IVF)_**. Generally, _IVF_ uses clustering algorithms (e.g., k-means) to divide the dataset into buckets. During the search, it selects some buckets with the closest centroids w.r.t. the query, after which vectors in such buckets will be scanned for final results. The buckets of _IVF_ index disrupt the distribution of vectors in the original dataset. In contrast, _CSPG_ preserves the original distribution by random partition, which diffuses all vectors, and the routing vectors are used for connecting all proximity graphs from different partitions.

**Comparison with _PG_(\(\)). The _CSPG_ index is built based on random partitions, with the help of routing vectors for connectivity. When the number of partitions \(m=1\), _CSPG_ falls into the special case that builds a _PG_ index over all vectors, which is consistent with most state-of-the-art graph-based ANNS algorithms. Some existing _PG_ index  also utilized similar ideas of using data partition and redundancy. Existing studies  uses k-means or other methods to divide the dataset, and obtain some redundant vectors. Then, such algorithms also construct proximity graphs in each partition separately, which are eventually _merged into a large PG_ as the final proximity graph covering all vectors. Such existing techniques are developed to deal with a huge amount of vectors, making it feasible to handle large datasets. In contrast, the _CSPG_ methods aim at building several proximity graphs to speed up query answering.

**Comparison with _Hnsw._** (1) From the perspective of redundancy, the lower level of _HNSW_ contains all vectors from the upper level. But in _CSPG_, there is a common overlap between partitions, and the remaining points of different partitions are distinct. (2) From the perspective of structure, _HNSW_ is a hierarchical structure. In contrast, _CSPG_ serves as a framework rather than a specific structure (hierarchical or flat), allowing to enhance query performance across a broad range of mainstream graph indices. _HNSW_ transitions from top level to bottom level unidirectionally, while _CSPG_ builds horizontally with smaller, sparser proximity graphs. (3) From the perspective of searching, _HNSW_ can only unidirectionally search each level from top to bottom, and the final results are obtained from the bottom-level graph. But _CSPG_ can traverse back and forth between different sparse proximity graphs, collecting final results from various partitions. The performance of _HNSW_ is closely tied to the quality of the bottom-level graph, while _CSPG_ generates more diverse and robust answers by leveraging cross-partition traversal and result collection.

### _Cspg_ index construction and updates

Algorithm 2 presents the process for _CSPG_ index construction. It first samples the routing vectors \(RV\) from dataset \(\), then the other vectors \( RV\) are randomly assigned to \(m\) partitions. Finally, for each partition \(_{i}\), we construct the proximity graph by applying the graph-based ANNS algorithm.

Since _CSPG_ is a framework based on mainstream proximity graphs, current updating methods of the underlying graph index are applicable. Moreover, the random partitioning makes it highly flexible for vector insertion and deletion. Details are presented in Algorithms 3 and 4 of Appendix B.

Figure 2: An example of _CSPG_ index, where the proximity graphs are built using relative neighborhood graph \(_{1}\) and \(_{2}\) (with very similar degree to Figure 0(b).

**Time and space complexity.** For dataset \(\) with \(n\) vectors, the time and space cost of building index are \((T(n))\) and \((S(n))\), respectively. As each partition has \(+ n\) vectors, _CSPG_ consumes \((m T(+ n))\) time and \(O(m S(+ n))\) space, which is at the same order of magnitude comparing with the original costs for most graph-based ANNS algorithms.

## 4 Two-stage search on _Cspg_

The search process of _CSPG_ is divided into two stages, i.e., _fast approaching_ and _precise search_. Specifically, the first stage aims to quickly approach the query vector by using only one proximity graph, while the second stage will carefully search around by considering all partitions for the final closest vectors. Traditional beam search on a single proximity graph maintains a fixed-size candidate set. In contrast, _CSPG_ uses different sizes \(ef_{1}\) and \(ef_{2}\) for the two stages respectively, where \(ef_{1}<ef_{2}\). Algorithm 1 outlines the procedure of searching on the _CSPG_ index.

```
0:CSPG index \(=\{_{1},_{2},...,_{m}\}\), query vector \(q\), parameters \(ef_{1}\) and \(ef_{2}\)
0:\(k\) nearest neighbors of \(q\)
1:\(\{\) selected entry vector \(p_{1}\}\), \(visited\{p\}\)\(\) First stage starts
2:while\(|| 0\)do
3:\((r,h)\) the closest vector w.r.t. \(q\) in \(\)
4:for all unvisited neighbor \(u\) of \(r\) in \(_{1}\)do
5:\(\{(u,1)\}\), \(visited visited\{u\}\)
6:if\(||>ef_{1}\)then remove the farthest vectors w.r.t. \(q\) to keep \(||=ef_{1}\)
7:\(p\) the closest vector w.r.t. \(q\) in \(visited\)\(\) Second stage starts
8:\(\{(p,1)\}\),\(visited\{p\}\)
9:while\(|| 0\)do
10:\((r,h)\) the closest vector w.r.t. \(q\) in \(\)
11:for all unvisited neighbor \(u\) of \(r\) in \(_{h}\)do
12:\(\{(u,h)\}\), \(visited visited\{u\}\)
13:if\(u\) is a routing vector then\(\{(u,\,i)\ |\ i\{1,2,...,m\} i h\}\)
14:if\(||>ef_{2}\)then remove the farthest vectors w.r.t. \(q\) to keep \(||=ef_{2}\)
15:return top-\(k\) closest vectors w.r.t. \(q\) in \(visited\) ```

**Algorithm 1** Search on _CSPG_ index

In the first stage, the algorithm quickly approaches the query nearby with a shorter search length and fewer neighbor expansions. As shown in Algorithm 1, it conducts a greedy beam search discussed in Section 1 on the graph \(_{1}\).

**Example 2**.: _Given a dataset \(\) and a query vector \(\), We build CSPG and conduct the first stage search (\(ef_{1}=1\)) to approach the nearby region of the query within just 1 step (Figure 2)._

Each proximity graph in _CSPG(\(\))_ is smaller and sparser than the proximity graph for the whole dataset \(\) (denoted as _PG(\(\))_). This sparsity allows the first stage search to use larger steps and fewer moves to approximate the query. On the other hand, _CSPG_ uses a smaller candidate size \(ef_{1}\), eliminating some expansions that do not contribute to the final results.

### The Second Stage: cross-partition expansion for precise search

After the greedy search in the first stage, the candidate set contains the closest vector delivered in the first stage from the first partition \(_{1}\) w.r.t. the query vector. As shown in Algorithm 1, after resetting the visited set, it continues the greedy beam search with a size \(ef_{2}\) for the candidate set \(\). In the second stage, the significant difference from the first stage lies in line 13. Specifically, if the expanded neighbor \(u\) is a routing vector, all its instances in all partitions will be pushed into \(\). This approach allows the search process to dynamically traverse different proximity graphs, maximizing the search space to include as many potential closest vectors as possible.

**Example 3**.: _This example continues the search from Example 2. For comparison, let us consider the traditional greedy beam search within \(PG()\). As shown in Figure 0(b), it takes 6 steps to approach the nearest neighbor \(v_{9}\) (with a fixed candidate set size \(ef=3\)). For CSPG, it first conducts the first stage, then switches the candidate set size from \(ef_{1}=1\) to \(ef_{2}=3\) and enters the second stage for amore precise search in the query nearby as shown in Figure 2 (the two stages have 4 steps in total). In the second stage, CSPG expands the neighbors of \(v_{4}^{1}\), \((v_{3}^{1})=\{v_{4}^{1}\}\), in \(_{1}\). Since \(v_{4}\) is a routing vector, both \(v_{4}^{2}\) and \(v_{4}^{1}\) are added to \(\). Next, the algorithm expands \(v_{4}^{2}\), updating \(\) to \(\{v_{1}^{1},v_{7}^{2},v_{4}^{1}\}\) as \(v_{7}\) is also a routing vector. Then, by expanding \(v_{7}^{1}\), we reach the closest node \(v_{9}^{1}\) in \(_{1}\)._

Due to the sparsity of each proximity graph in _CSPG_, the search on _CSPG_(\(\)) approaches the query results faster than _PG_(\(\)). Moreover, some expansions may be removed. For example, with a candidate size of \(ef_{2}=2\), the candidate set in Step 2 would be \(\{v_{7}^{1},v_{7}^{2}\}\), removing \(v_{4}^{1}\) due to the limited size, ignoring the unnecessary expansion to \(v_{5}^{1}\).

In _PG_(\(\)), redundant vectors are mainly used to merge graphs. In contrast, _CSPG_ leverages the distribution of routing vectors to ensure that the expansion of one graph aids in reducing expansions in other graphs. This means a position reached by one graph can be accessed by other graphs without additional expansion. For example, moving from \(v_{4}^{2}\) to \(v_{7}^{2}\) in \(_{2}\) allows continuing the search from \(v_{7}^{1}\) to \(v_{9}^{1}\) in the search across multiple graphs in the second stage appears as though it is conducted within a single proximity graph. The total number of steps to traverse the query nearby is 3, fewer than the 4 steps in _PG_(\(\)) while maintaining the same precision. In practice, with appropriate partitioning, _CSPG_ outperforms traditional _PG_ algorithms, as discussed in Section 6.

## 5 Analysis of search efficiency

For most graph-based ANNS algorithms, calculating the distance between two vectors usually dominates the overall search time. In this section, we focus on the number of distance computations during query processing, denoted by \(C\). We will show that the expected cost \([C]\) for the proposed _CSPG_ method is lower than the traditional PG under certain constraints.

### The expected number of distance computations

Following the setting of previous work , assume that the start vector \(p\) and query vector \(q\) are randomly selected from the \(d\)-dimensional vector dataset. On the proximity graph \(\) built on the dataset, the greedy search sequence is denoted by \(p q=\{v_{1},v_{2},...,v_{t}\}\), where \(v_{1}=p\), \(v_{t}=q\). Denote \(|p q|\) as its length. The search sequence length of an MSNET equals its search path length, as the search consistently approaches the query without backtracking. Recall that when exploring each node, all its unvisited neighbors are pushed into the candidate set, and their distance to \(q\) is computed. By assuming that the average degree of \(\) is \(\), the expected number of distance computation is \([C]=[|p|]\).

### Expected search sequence length on Monotonic Search Network

As introduced in Section 2.3, for a Monotonic Search Network (MSNET), the expected length of the search sequence \(^{M}[|p u|]=(n^{} n^{ }/ r)\). In _CSPG_ schema, assume that there are \(m\) partitions (\(m n\)), and the proximity graph \(_{i}\) on each partition \(_{i}\) is a MSNET. Since we randomly select routing vectors \(RV\) and then randomly divide the other vectors into \(m\) partitions, the distribution of vectors is the same in each partition. Then, we have the following theorem.

**Theorem 1**.: _Given a start vector \(s RV\) and a query vector \(q RV\), by performing greedy beam search from \(s\) on each MSNET \(_{i}\) independently, we can obtain \(m\) monotonic paths \(s^{i} q^{i}\), where \(s^{i},q^{i}_{i}\). It holds that \(^{_{i}}[|s^{i} q^{i}|]=^{ _{j}}[|s^{j} q^{j}|]\) for \(1 i,j m\)._

Proof.: Since the distribution of vectors in each graph \(_{i}\) are the same, the assumptions for deriving the expected path length in  remain unchanged, thus they have the same expected path length. 

By starting the search on a random entry vector \(p\) in proximity graph \(_{1}\), the routing vectors help us to travel across different partitions \(_{i}\) in the second stage. Thus, the search sequence of _CSPG_ is composed of several sub-sequences from different graphs \(_{i}\). The following theorem reveals that the expected search sequence length of _CSPG_ is the same as the case of searching on the _PG_(\(_{1}\)).

**Theorem 2**.: _Denote \(^{CSPG}[|p q|]\) as the expected length of search sequence in CSPG. Denote \(^{_{i}}[|p q|]\) as the expected sequence length when searching only on the graph \(_{i}\). It holds that_

\[^{CSPG}[|p q|]=^{_{i}}[|p  q|].\]

Please refer to Appendix C for the detailed proofs. Based on Theorem 2, when the proximity graphs in _CSPG_ are MSNET, the expected search path length is

\[^{CSPG}[|p q|]=^{_{i }}[|p q|]=(( n+)^{}( n+ )^{}/ r).\]

### Approximate Monotonic Search Network

Most proximity graphs in practice are the approximation of MSNET, where the search path may have detours due to the lack of some necessary monotonic paths, resulting in distance backtracking. Given a query vector \(q\), we say that vector \(u\) conquers \(q\) iff \( v(u),(v,q)<(u,q)\), denoted by \(u q\) (\(u q\)). For a certain vector \(u\) in the search path \(p q\), distance backtracking is avoided iff \(u q\), since when exploring \(u\), we can visit \(v\) to strictly decrease the distance w.r.t. \(q\).

Intuitively, for the proximity graph \(\), when the degree of every vector \(u\) is large enough, \(u q\) is likely to be met for any query vector \(q\), which help to avoid distance backtracking. However, since the average degree is usually limited in practical proximity graphs, there is a probability that vector \(u\) conquers a random query vector \(q\), formally defined as \((u)=}(u q)}{n}\)., where \((u q)=1\) iff \(u q\). Next, we introduce the _Approximate Monotonic Search Network (AMSNET)_, which reduces distance backtracking by maximizing \((u)\) of each vector \(u\).

**Definition 7** (Approximate Monotonic Search Network, shorted as AMSNET).: _Given a dataset \(\) of \(n\) vectors, a proximity graph \(\) built on \(\) is called an approximate monotonic search network iff for every vector \(u\), its neighbor set \((u)\) satisfies that \(|(u)|\) while maximizing \((u)\)._

**Theorem 3**.: _For datasets with the same distribution, as the number of vectors \(n\) decreases, \((u)\) is monotonically non-decreasing._

Please refer to Appendix C for detailed proofs. Since AMSNET allows distance backtracking, there exists a detour factor \(w>1\) for the expected search sequence length. Specifically, when the underlying proximity graphs of _CSPG_ are AMSNETs, the expected search sequence length is \(}^{CSPG}[p q]=(w ( n+)^{}( n +)^{}/ r)\). Moreover, according to Theorem 3, for every vector \(u\), as dataset size \(n\) decreases, \((u)\) is non-decreasing. In other words, the probability of distance backtracking at every vector is non-increasing as \(n\) decreases, thus \(w\) is non-increasing as \(n\) decreases. We confirm the monotonicity of \(w\) in Section 6.4.

### Speedup analysis for _Cspg_

For the dataset \(\) of \(n\) vectors, we have the following assumptions as discussed above: (1) \(,\  V_{C} V_{B}(,R)\). (2) \(m>1\), \(<1\), and the vector distribution of each partition \(_{i}\) is the same as \(\). (3) The proximity graphs built in _PG_ and _CSPG_ are AMSNETs with a degree upper bound of \(\).

When the proximity graph is AMSNET, the expected path length for _PG_ method is \(}^{PG}[|p q\|]=(n^{} n ^{}/ r^{PG})\), where \( r^{PG}\) is the minimum distance difference for any non-isosceles triangle on \(\). Similarly, \( r^{CSPG}\) is defined for each partition \(_{i}\) in _CSPG_. And the detour factor \(w\) for _PG_ and _CSPG_ are denoted as \(w^{PG}\) and \(w^{CSPG}\), respectively. Thus, considering the expected number of distance computations, the speedup ratio of _CSPG_ over _PG_ is

\[Speedup=}^{PG}[|p q\|]}{ }^{CSPG}[|p q\|]}=(}{w^{CSPG}}}{ r^{PG}})} n^{}}{( n+) ^{}( n+)^{}}\]

Define \(=}{w^{CSPG}}}{ r^{PG}}\). Since each proximity graph in _CSPG_ are smaller than that in _PG_, and Section 5.3 shows that \(w\) is non-increasing as \(n\) decreases, \(w^{PG} w^{CSPG}\). Also, since \( r\) decreases as \(n\) increases , we have \( r^{CSPG} r^{PG}\). Thus, it holds that \( 1\). Next, we consider \(=} n^{}}{( n+)^{}( n+ )^{}}\). In _CSPG_, each partition has less than \(n\) vectors when \(m>1\) and \(<1\), i.e., \( n+<n\) and \(>1\). Thus, \(Speedup=>1\), which ensures that _CSPG_ always outperforms _PG_ when using _AMSNET_. As the dataset size \(n\) increases, \(\) is decreasing and we have \(_{n}Speedup=( )^{}\). Moreover, when \(n\) and \(d\) is increasing, \(\) is also decreasing and \(_{d}Speedup=\).

## 6 Evaluation

### Experimental setup

As summarized in Table 3, four benchmark datasets are used in our experiments, which are the most commonly used public datasets come from _Ann-benchmarks_.

[MISSING_PAGE_FAIL:8]

**Varying the number of partitions.** Since the number of partitions \(m\) affects the _CSPG_ index quality, we conduct experiments for evaluating the query performance with \(m=1,2,4,8,16\) partitions, in which \(m=1\) indicates the original graph-based ANNS algorithm without partition. Note that the other parameters are the same as the default settings. Intuitively, larger \(m\) results in fewer vectors in each partition, and _CSPG_ seems to achieve better performance. However, large \(m\) may decrease the similarity of vector distribution among different partitions, which contradicts the assumptions of the same vector distribution discussed in Section 5.4. Therefore, for different datasets with various distributions, choosing an appropriate parameter \(m\) is crucial. As shown in Figure 5, for SIFT1M dataset, dividing all vectors into \(2\) or \(4\) partition results in better QPS-recall curves. Figure 11 shows that the optimal value of \(m\) for the other datasets ranges from \(2\) to \(8\).

**Varying the sampling ratio.** When constructing the _CSPG_ schema, the sampling ratio \(\) is used to randomly sample \( n\) routing vectors before dataset partition. By using the default values of all the other parameters, Figure 6 reveals that for SIFT1M dataset, larger \(\) tends to improve the query performance of _CSPG_. And it also holds for other datasets, as shown in Figure 12. This is because more routing vectors help to navigate the search traveling across different partitions efficiently. Also, more routing vectors result in more shared vectors in each partition, increasing the similarity of vector distribution among different partitions, which is more aligned with the assumptions of the same vector distribution discussed in Section 5.4.

**Varying the candidate set sizes.**_CSPG_ has two parameters \(ef_{1}\) and \(ef_{2}\) for searching, limiting the size of the candidate set in the two stages, respectively. As shown in Figure 7 and Figure 13, we try different \(ef_{1}=1,2,4,8,16\) and obtain 5 QPS v.s. recall curves. The marks in each curve are obtained by varying the value of \(ef_{2}\) uniformly picked from \(\). In most cases, \(ef_{1}=1\) provides the best query performance, which aligns with our goal of the first stage fast approaching.

**Statistics for detour factor.** We randomly sample \(0.1\), \(0.2\), \(0.5\), \(2\), and \(5\) million vectors from SIFT10M dataset. By using _CSPG_ with the default parameter settings, at different \(Recall@k\), we obtain the empirical detour factor \(w=}{}\)

Figure 4: Query performance when varying the dataset size \(n\)

Figure 5: Query performance when varying the number of partitions \(m\)

Figure 6: Query performance when varying the sampling ratio \(\)search paths. As shown in Figure 8, larger \(n\) results in larger \(w\) at a fixed \(Recall@k\), which aligns with our discussion in Section 5.3 that \(w\) is non-increasing as \(n\) decreases.

### Hyperparameter grid search and evaluation with ANN-Benchmarks

To show the Pareto frontiers with the optimal hyperparameters, we conduct a grid search experiment to identify the most effective hyperparameter combinations for each baseline on SIFT1M dataset. As presented in Appendix D, we summarise the parameter selection sets for different indices in Table 4, and the results of the grid search experiment are illustrated in Figure 14.

Next, we conduct a more comprehensive evaluation using the ANN-Benchmarks , which is a widely used benchmarking environment. For each algorithm, we choose the optimal parameters from the grid search experiment with the highest QPS when Recall@10 \(=0.9 5e^{-3}\) on SIFT1M dataset. As shown in Figure 9, enhanced with the proposed _CSPG_ framework, representative algorithms such as _HCNNG_, _HNSW_, _Vamana_ and _NSG_ can achieve better performance on SIFT1M dataset.

## 7 Conclusion

We proposed a novel graph-based indexing schema named _CSPG_ for Approximate Nearest Neighbor Search (ANNS), which is compatible with the current leading graph-based approaches in high-recall scenarios. Furthermore, we propose a novel search algorithm for the _CSPG_ schema, which uses a two-stage strategy and a cross-partition expansion to reduce meaningless expansion during the graph search and make the process more focused on the parts related to the answer. Next, we analyze the expectation of _CSPG_'s search amount, establish a speedup model, and prove that _CSPG_ can always have an advantage. Finally, we investigate the advantages of _CSPG_ through experiments and carry out a more detailed evaluation of the key factors affecting the performance of _CSPG_.

Figure 8: Detour factor when varying the dataset size \(n\)

Figure 7: Query performance when varying the candidate set size \(ef_{1}\) in the first stage

Figure 9: QPS v.s. Recall@10 curve on SIFT1M with the optimal parameters in ANN-Benchmarks