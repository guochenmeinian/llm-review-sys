# Goal-Conditioned On-Policy Reinforcement Learning

Xudong Gong\({}^{1,2,}\)  Dawei Feng\({}^{1,2,}\)  Kele Xu\({}^{1,2,}\)  Bo Ding\({}^{1,2}\)  Huaimin Wang\({}^{1,2}\)

\({}^{1}\) College of Computer, National University of Defense Technology, Changsha, Hunan, China

\({}^{2}\) State Key Laboratory of Complex & Critical Software Environment, Changsha, Hunan, China

Co-corresponding authors: kelele.xu@gmail.com, dingbo@nudt.edu.cn. \({}^{}\) Equal contribution.

###### Abstract

Existing Goal-Conditioned Reinforcement Learning (GCRL) algorithms are built upon Hindsight Experience Replay (HER), which densities rewards through hindsight replay and leverages historical goal-achieving information to construct a learning curriculum. However, when the task is characterized by a non-Markovian reward (NMR), whose computation depends on multiple steps of states and actions, HER can no longer densify rewards by treating a single encountered state as the hindsight goal. The lack of informative rewards hinders policy learning, resulting in rolling out failed trajectories. Consequently, the replay buffer is overwhelmed with failed trajectories, impeding the establishment of an applicable curriculum. To circumvent these limitations, we deviate from existing HER-based methods and propose an on-policy GCRL framework, GCPO, which is applicable to both multi-goal Markovian reward (MR) and NMR problems. GCPO consists of (1) Pre-training from Demonstrations, which pre-trains the policy to possess an initial goal-achieving capability, thereby diminishing the difficulty of subsequent online learning. (2) Online Self-Curriculum Learning, which first estimates the policy's goal-achieving capability based on historical evaluation information and then selects progressively challenging goals for learning based on its current capability. We evaluate GCPO on a challenging multi-goal long-horizon task: fixed-wing UAV velocity vector control. Experimental results demonstrate that GCPO is capable of effectively addressing both multi-goal MR and NMR problems.

## 1 Introduction

Multi-goal problems are ubiquitous in real-world applications, such as controlling robotic arms to grasp objects at any location on a table , and operating fixed-wing Unmanned Aerial Vehicles (UAVs) to navigate towards any specified velocity vector , etc. To address the challenge of automatically learning policies capable of achieving and generalizing across a range of diverse goals , Goal-Conditioned Reinforcement Learning (GCRL)  has emerged as a prominent area of research. Serving as a generalization of standard Reinforcement Learning (RL) , GCRL learns goal-conditioned policies  through interactions within multi-goal environments .

In existing GCRL algorithms, Hindsight Experience Replay (HER)  plays a pivotal role in facilitating the learning of goal-conditioned policies. First, HER enhances sample efficiency by replacing the desired goals of failed trajectories with achieved states, thereby providing more informative rewards for policy learning . Second, HER contributes to creating a curriculum that enables the policy to progressively master challenging goals . This process involves fitting the current policy's goal-achieving capability with historical goal-achieving information and selecting goals of appropriate difficulty for the current policy learning. Due to the reliance on replay buffers , current research on GCRL predominantly focuses on off-policy RL approaches .

Despite its widespread success, HER is subject to a limitation: the reward function must be able to determine goal achievement based solely on the current state, which implies that the reward function must be Markovian . In real-world applications, many reward functions depend on multiple steps of states and actions, i.e., rewards are non-Markovian [24; 1]. For instance, determining whether an UAV has stably achieved a goal based on a sequence of states, or calculating a penalty for control oscillation based on a sequence of actions [33; 8]. For non-Markovian reward (NMR) problems, on one hand, it is not feasible to treat a single state from the sampled trajectory as a hindsight goal; on the other hand, it is challenging for the trajectory obtained through exploration to include the special transition sequence that satisfies the specific NMR. In Fig. 1, we present a specific case to elucidate the NMR problem and illustrate why HER fails to address NMR, and corresponding experimental results to support this explanation. It is evident that as the NMR relies on longer state-action sequences, the performance of HER gradually deteriorates, ultimately becoming indistinguishable from the performance when HER is not employed. This limitation prevents HER from densifying the reward. The lack of informative rewards hinders policy learning, which further leads to rolling out failed trajectories. Consequently, the replay buffer is overwhelmed with failed trajectories, obstructing the creation of a reasonable curriculum.

In light of the aforementioned limitations of HER in addressing NMR problems, we propose a novel on-policy [62; 57] GCRL framework, termed the Goal-Conditioned Policy Optimization (GCPO) framework, which deviates from existing HER-based off-policy approaches. GCPO comprises: (1) **Pre-training from Demonstrations**: This phase involves leveraging demonstrations to pre-train the policy offline, equipping it with an initial capability to achieve goals before online learning begins, thereby diminishing the difficulty of subsequent online learning. (2) **Online Self-Curriculum Learning**: This phase involves periodically evaluating the policy, estimating its goal-achieving capability, and optimizing it through online learning with progressively challenging goals sampled based on its current capability. It is important to note that GCPO is not specifically designed for NMR problems; instead, it is a general on-policy GCRL framework applicable to both multi-goal Markovian reward (MR) and NMR problems.

To evaluate GCPO, we conduct experiments on the Velocity Vector Control (VVC) task of fixed-wing UAVs. The VVC task represents a typical multi-goal problem that can be formulated as both MR and NMR, and it also requires long interaction sequences, categorizing it as a challenging long-horizon problem . The complexity inherent in the VVC task provides a rigorous testbed for evaluating the efficacy of GCPO. Experimental results indicate that GCPO effectively addresses multi-goal problems, both MR and NMR. Our contributions are summarized as follows:

* We analyze the reason why existing GCRL algorithms, which rely on HER as a central component, fail in handling NMR problems and validate this finding through experiments.
* We propose an on-policy GCRL framework, GCPO, which incorporates pre-training from demonstrations and online self-curriculum learning to address both MR and NMR problems.

Figure 1: **Left: Illustrations of HER on Markovian reward (MR) and non-Markovian reward (NMR) problems. Right: Performance of HER on MR and NMR problems. NMR[x] represents the NMR depends on the last x consecutive states and actions. Results come from experiments on an easy version (Appendix A.7) of fixed-wing UAV velocity vector control task over 5 random seeds.**

* We evaluate the performance of GCPO on the challenging VVC task, demonstrating its effectiveness in solving both MR and NMR problems. Additionally, we conduct ablation studies to analyze the influence of the two components and hyper-parameters on learning.

## 2 Related Work

**Goal-conditioned reinforcement learning**. Many prior works assume access to a goal-conditioned reward function [15; 73] and view GCRL as a reward-driven, multi-task learning problem . Existing GCRL methods are predominantly based on HER, situating them within the off-policy RL domain . Pitis et al.  summarize existing methods and propose a common off-policy GCRL framework, which alternates between collecting experience and optimizing policies. During the policy optimization phase, the hindsight replay method is utilized to relabel transitions sampled from the replay buffer, thereby increasing informative rewards in the training data. When collecting experience, some heuristics or learning methods are employed to select appropriate goals that assist in improving the policy.

Selecting appropriate goals involves (1) several heuristics for discovery, including reward relevance [4; 65], diversity [14; 10], coverage [38; 6], difficulty [32; 43], etc. (2) selecting goals of appropriate difficulty based on the capability of the current policy, which essentially craft objectives that try to optimize for learning progress. These methods fit the current policy's goal-achieving capability based on the trajectories stored in the replay buffer and then sampling goals of appropriate difficulty for online learning according to this capability. For instance, RIG  samples goals directly from the distribution of achieved goals, DISCERN  samples uniformly on the support set of the distribution of achieved goals, and MEGA  uses inverse probability weighting sampling  on the distribution of achieved goals to samples goals that the current policy can achieve but not well.

The difference between our method and the aforementioned methods lies in the learning framework: our method is on-policy, whereas the existing methods, all based on HER, are off-policy. This difference manifests in two key aspects: First, our method does not include a component like HER, which cannot be used for solving NMR problems. To achieve an effect similar to HER's enhancement of informative rewards during learning, we design a component that pre-trains the policy with demonstrations, making it suitable for on-policy RL. Second, our method employs off-policy evaluation [63; 64] to estimate the current policy's goal-achieving capability, rather than estimating this capability based on the trajectories in the replay buffer. Table 1 summarizes the similarities and differences between GCPO and existing GCRL methods.

**Non-Markovian rewards.** Abel et al.  underscore the necessity of NMRs by demonstrating the existence of environment-task pairs for which no MR function can realize the task. Moreover, numerous exploration strategies implicitly depend on NMRs [35; 45], and studies considering non-Markovian discount factors [19; 58] can also be interpreted as special forms of NMRs . The fundamental approach for addressing NMRs involves augmenting the state space to render the reward Markovian . Various techniques have been proposed to achieve this, such as Reward Machines  and the Split-MDP . Nevertheless, the expanded reward state space may grow exponentially with the number of acceptable policies and could incorporate an infinite number of simple reward functions . Our work diverges from the aforementioned research by presenting a general GCRL framework, rather than a specialized approach for NMR problems. Unlike existing GCRL methods that depend on HER, which is not applicable to NMR problems, our framework can be applied to

    & Method & Type of RL &  Applicable \\ Reward Types \\  &  Method for increasing \\ informative rewards \\  &  Method for sampling goal \\ estimate \(p_{ag}\) \\  &  sample distribution \\  \\   HER-based \\ (NeuIPS2017) \\  &  +RIG (NeurIPS2018) \\ +DISCERN (ICLR2019) \\ +MEGA (ICML2020) \\  & off-policy & MR & hindsight replay &  replay \\ buffer \\  &  \(p_{ag}\) \\ \(IWP(p_{ag})\) \\  \\   GCPO \\  & on-policy & MR, NMR & pre-train from demonstrations & off-policy evaluation & 
 \(p_{ag}\), \([supp(p_{ag})]\), \\ \(IWP(p_{ag})\), \(\) \\  \\   

Table 1: Comparison between HER-based methods and GCPO. \(p_{ag}\) refers to the distribution of achieved goals, \(supp()\) refers to the support set of a distribution, \(()\) refers to the uniform distribution on a set, and \(IPW()\) refers to the inverse probability weighting .

both NMR and MR problems. Additionally, our framework is compatible with techniques designed to address NMR challenges, as demonstrated in Appendix F.

## 3 Methodology

GCPO is designed as an on-policy GCRL framework. We draw upon the key insights from existing HER-based GCRL methods that have led to their success and incorporate two critical components into the GCPO framework: pre-training from demonstrations and online self-curriculum learning. The overall GCPO framework is depicted in Fig. 2 and a practical implementation of GCPO is detailed in Algorithm 1.

### Preliminaries

GCRL can be described by goal-augmented MDP \(M=,,,r,,,p_{dg},\), where \(,,,\) and \(p_{dg}\) denote the state space, action space, discount factor, goal space and desired goal distribution of the environment, respectively. \(:()\) is the transition function, where \(()\) denote the probability distribution over a set \(\). \(r\) is the goal-conditioned reward function. It can be both Markovian \(r=\{r_{g}|r_{g}:,g G\}\) and non-Markovian \(r=\{r_{g}|r_{g}:()^{*},g G\}\). \(:\) is a tractable mapping function that maps the state to a specific goal. The objective of GCRL is to reach goals via a goal-conditioned policy \(:()\) that maximizes the expectation of the cumulative rewards over the desired goal distribution \(J()=_{a_{t}(|s_{t},g),g p_{dg},s_{t+1}(|s_{t},a_{t})}[_{t}^{t}r_{g}()].\) Additionally, previous works  identify two common definitions of goals: Achieved goal, which refers to the goal accomplished by the policy in the current state. The notation \(p_{ag}\) denotes the distribution of achieved goals. Behavioral goal, which represents the specific task that is targeted for sampling within a rollout episode .

### Pre-Training from Demonstrations.

GCRL encounters more substantial exploration challenges compared to standard RL due to the inclusion of an additional goal space . Pre-training from demonstrations is primarily designed to facilitate biased exploration . Specifically, the policy can be pre-trained with Imitation Learning (IL)  or goal-conditioned IL  on demonstrations. The pre-training provides the policy with a warm start , which refers to an initial ability to achieve some of the desired goals. Pre-training is vital for on-policy RL as it enhances informative rewards during online learning. Without such informative rewards, the policy would struggle to acquire any meaningful knowledge or skills . Through subsequent online learning, the policy can effectively discern when it is more advantageous to adhere to states and actions from the demonstration trajectories or to explore superior alternatives.

In our implementation of GCPO, we utilize **Behavioral Cloning** (BC)  for pre-training the policy, as indicated in line 2 of Algorithm 1. For the demonstration \(_{E}\), the policy is learned by optimizing a supervised loss function to maximize the likelihood of expert actions 

\[()=-_{(s,a)_{E}}[_{}(a |s)].\] (1)

Figure 2: The overall GCPO framework.

### Online Self-Curriculum Learning

In GCRL, selecting goals that match the current policy's capabilities is crucial for effective learning [20; 12; 5; 11]. To address this, we design an online self-curriculum learning mechanism that autonomously constructs a curriculum, generating behavioral goals that are incrementally more difficult than those the policy is currently capable of achieving during training. Specifically, online self-curriculum learning consists of three processes: (1) Estimating the current policy's goal-achieving ability, \(p_{ag}\). This can be done through methods such as online evaluation or off-policy evaluation (OPE) [63; 64]. (2) Setting or learning a probability transform function \(f:\), followed by sampling progressively challenging behavioral goals based on \(f(p_{ag},p_{dg})\) for online learning. (3) Conducting online RL learning with behavioral goals sampled in the second part, facilitating the agent's progression towards more challenging goals. In our implementation of GCPO:

**Gaussian Mixture Model** (GMM)  is employed to estimate \(p_{ag}\), as detailed in lines 4-13 of Algorithm 1. During the online self-curriculum learning, the policy is periodically evaluated. In each evaluation, the policy is evaluated with \(N\) goals sampled from \(p_{dg}\). Information about the achieved goals, along with an initial weight of \(1.0\), is stored in a goal buffer \(B_{g}\). As the online self-curriculum learning proceeds, the weight of historically achieved goals is reduced by a factor \(\). Ultimately, a GMM is used to estimate \(p_{ag}\) based on the data in \(B_{g}\) (The specific calculation can be referred to Appendix B.).

**Maximum Entropy Gain Exploration** (MEGA)  is utilized as the probability transform function \(f\), as indicated in lines 14-19 of Algorithm 1. The core idea behind MEGA is to encourage exploration in sparsely explored areas of the achieved goal distribution. In discrete settings, inverse probability weighting  can be applied to sample goals from \(p_{ag}\). A goal \(g\) is chosen with the probability given by

\[f_{MEGA}(p_{ag},p_{dg})(g)=(g )}}{_{p^{}}(g^{})}}.\] (2)

In continuous settings, a generate and test strategy [42; 47] is employed for goal sampling. Specifically, \(M\) goals \(\{g_{i}\}_{i=1}^{M}\) are randomly sampled from \(supp(p_{ag})\), the support set of \(p_{ag}\), and the goal with theminimum density under \(f_{MEGA}(p_{ag},p_{ag})\) is selected: \(g=_{g_{i}}f_{MEGA}(p_{ag},p_{ag})(g_{i})\). This approach biases the sampling towards goals that are less likely under the current achieved goal distribution, promoting exploration in under-explored regions.

**KL-regularized RL** is employed as the on-policy RL algorithm to optimize the policy, as indicated in line 20 of Algorithm 1. To prevent catastrophic forgetting of latent skills and to continuously improve exploration during the RL fine-tuning phase , the policy \(_{}\) is initially set to the pre-trained policy \(_{_{0}}\) and is then fine-tuned by maximizing the following objective:

\[J_{kl}(_{})=_{t}^{t}r- log( }(a_{t}|s_{t})}{_{_{0}}(a_{t}|s_{t})}) ,\] (3)

where \(r\) can represent both Markovian \(r(s_{t},a_{t})\) and non-Markovian \(r(s_{0:t},a_{0:t})\) rewards, and \(\) controls the strength of the KL regularization. Optimizing Eq. 3 is analogous to optimizing the original RL objective within the log-barrier of \(_{_{0}}\), and can be viewed as a trust-region-style  learning objective .

## 4 Experiments

### Experimental Setups

**RL environment.** Experiments are conducted on the **Fixed-Wing UAV Velocity Vector Control** (VVC) task , which is a representative multi-goal problem. The VVC task is characterized by a long horizon, with the average length of demonstrations exceeding 280 steps (detailed in Appendix A.5). Even for well-trained policies, the average number of steps required to achieve a goal is over 100, and more challenging goals can demand upwards of 300 steps to achieve . This exceeds the horizon typically used in most previous studies . Additionally, the rewards for VVC can be designed as either MR or NMR, thereby accommodating a range of real-world requirements and complexities. The specifics of the VVC environment setup are detailed in Appendix A. Thus, VVC presents a challenging multi-goal, long-horizon problem that poses significant difficulties for policy learning. Standard SAC+HER  and PPO  are unable to solve the VVC task, as demonstrated in Appendix A.6, further highlighting the task's complexity. To our knowledge, previous research on NMR algorithms has primarily been tested in simpler environments such as multi-arm bandits  and grid worlds . Our work is the first to evaluate the performance of algorithms on complex, real-world NMR problems. Additionally, to demonstrate the broad applicability of GCPO, we conduct experiments on the commonly used RL environments **Reach** and **PointMaze**. The corresponding results and analysis can be found in Appendix D.

**Demonstrations.** We collect a demonstration set \(_{E}\), also denoted as \(_{E}^{0}\), with a PID controller (detailed in Appendix A.5). Subsequently, we employ the IRPO algorithm , which iteratively optimizes policies and demonstrations, to generate \(_{E}^{1},_{E}^{2},_{E}^{3}\). Table 2 presents the quantity and quality of these four demonstration sets. The '#traj' column represents the number of demonstrations contained within each demonstration set, while 'traj length' indicates the average length of demonstrations in the set. A shorter demonstration length suggests a faster completion of the corresponding goal, indicative of higher demonstration quality. It can be observed that the demonstration quantity and quality of \(_{E}^{0},_{E}^{1},_{E}^{2},_{E}^{3}\) increase sequentially.

### Main Results

We compare GCPO with several baselines on the VVC task under different demonstration conditions, including (1) SAC  + HER + MEGA, which is a strong baseline in GCRL; (2) BC, a fundamental yet effective IL algorithm; (3) GCPO without pre-training, which corresponds to PPO + self-curriculum; (4) GCPO without self-curriculum, which corresponds to BC + KL-regularized RL. GCPO itself is equivalent to BC + KL-regularized RL + self-curriculum. Table 2 reports the performance of GCPO and the baselines on NMR, and Fig. 3 visualizes the learning progression on both NMR and MR, as well as the final learned policy of GCPO.

**GCPO is applicable to both MR and NMR problems.** Table 2 shows that GCPO outperforms all baselines on NMR, with SAC+HER+MEGA achieving only 20% of GCPO's performance. This demonstrates the limitations of HER in addressing NMR problems and highlights the superiority of GCPO in these contexts. Furthermore, Fig. 2(a) illustrates the learning progression of GCPO for both NMR and MR, showing that GCPO is effective in solving both types of problems. In summary, GCPO exhibits versatility and applicability across both MR and NMR problems.

**Pre-training is crucial for the success of GCPO.** As evidenced by the experiments in Table 2 for BC, GCPO w/o pre-training, and GCPO, it is observed that without pre-training, GCPO struggles to learn meaningful skills. However, a policy that is merely pre-trained, albeit with non-optimal performance, plays a crucial role in enabling GCPO to develop an effective policy. Fig. 2(b) provides a histogram of the achieved goals for the trained policies. It is evident that, even with a pre-trained policy that initially exhibits inferior performance compared to the demonstrator, GCPO's online self-curriculum learning facilitates significant improvement in the policy's performance, surpassing that of the demonstrator.

**Online self-curriculum facilitates the mastery of challenging goals.** Table 2 demonstrates that the application of self-curriculum within GCPO leads to an average 8.2% increase in policy performance compared to its absence. This enhancement is illustrated in Fig. 2(c), which shows that the online self-curriculum mechanism systematically introduces more difficult goals into the learning progression as the policy gains proficiency. This mechanism effectively explains the advantages of self-curriculum for GCPO in mastering challenging goals.

### Ablation Studies

In this section, we conduct ablation studies on the demonstration's quantity and goal distribution, analyze the sensitivity of GCPO to the parameters used for estimating \(p_{ag}\), and compare the effectiveness of different self-curriculum methods.

#### 4.3.1 Ablation on Quantity of Demonstrations

To illustrate the influence of the quantity of demonstrations on GCPO, we train GCPO with 10%, 50%, and 100% of \(_{E}\) and present the performance of pre-trained policies and GCPO policies in Fig. 3(a).

    & Demonstration & SAC + HER &  & GCPO w/o pre-training & GCPO w/o self-curriculum & GCPO \\  notation & \#traj & traj length & + MEGA & & & & \\  \(_{E}^{0}\) & 10264 & 281.83\(\)149.48 & & 17.08\(\)0.57 & & 31.28\(\)8.97 & **45.87\(\)3.09** \\ \(_{E}^{1}\) & 27021 & 119.64\(\)47.55 & & 36.54\(\)1.97 & & & 43.49\(\)3.85 & **49.12\(\)1.67** \\ \(_{E}^{2}\) & 34952 & 115.76\(\)45.65 & & 41.79\(\)0.44 & & & 51.28\(\)2.07 & **57.45\(\)2.49** \\ \(_{E}^{3}\) & 39835 & 116.56\(\)47.62 & & 42.77\(\)1.35 & & & 53.51\(\)3.18 & **59.90\(\)1.78** \\   

Table 2: Comparison between GCPO and baselines on NMR. The mean and variance of % success rates are presented over 5 random seeds. Optimal values are highlighted in bold, and sub-optimal values are underlined.

Figure 3: Main results of GCPO. ‘expert’ refers to the demonstrator that generates demonstrations. ‘BC’ refers to the pre-trained policy. Results are derived from experiments across 5 random seeds. For sub-figure (a), expert and BC are both evaluated in the NMR setting. For sub-figure (c), the vertical axis represents the training progress, where \(0,1,,9\) correspond to \(10\%,20\%,,100\%\) of the training progress, respectively.

As shown, a policy pre-trained with 10% of \(_{E}\) achieves 81.12% of the performance of the policy pre-trained with 100% of \(_{E}\), while a policy pre-trained with 50% of \(_{E}\) achieves 96.53% of the performance. Similarly, a policy trained by GCPO with 10% of \(_{E}\) achieves 86.95% of the performance of the policy trained by GCPO with 100% of \(_{E}\), whereas a policy trained by GCPO with 50% of \(_{E}\) achieves 95.94% of the performance. These results suggest that an increase in the quantity of demonstrations can enhance the performance of GCPO, yet the marginal gains diminish as the quantity of demonstrations grows. Furthermore, these results also indicate that GCPO can still perform well when only a relatively small number of demonstrations are available.

#### 4.3.2 Ablation on Goal Distribution of Demonstrations

To demonstrate the influence of the distribution of goals covered by the demonstrations on GCPO, we collect three demonstration sets with significantly different goal difficulty distributions (detailed in Appendix A.4) and train GCPO with them. Fig. 3(b) presents the distribution of achieved goals of the pre-trained policies, while Fig. 3(c) depicts that of the GCPO policy.

It is evident that for both pre-trained policies and GCPO policies, their distributions of achieved goals are centered around the distribution of goals covered by the demonstrations. The reason for this is that the self-curriculum, starting with the distribution of achieved goals of the pre-trained policy, which is determined by the distribution of goals covered by the demonstrations, gradually expands the distribution of achieved goals. The preference for goals in the demonstrations thus influences the learning progression of GCPO, leading the policy learned by GCPO to also exhibit a similar preference for goals. This suggests that when preparing demonstrations for GCPO, it is preferable to sample goals and generate demonstrations as closely as possible to the desired goal distribution \(p_{dg}\).

Furthermore, in Appendix E.1, we provide a more intuitive case and directly visualize the achieved goals in three-dimensional space, yielding the same conclusions as those from the above analysis.

Table 3: Performance of GCPO policies trained with different \(N\) and \(\). In our settings, \(N=32\) implies that the number of evaluations throughout the training is approximately equal to the number of goals obtained through discretizing the entire goal space during sampling demonstrations (detailed in Appendix A.5). The mean and variance of % success rates are shown over 5 random seeds. Optimal values are highlighted in bold, and sub-optimal values are underlined.

Figure 4: The influence of demonstration quantity and the distribution of goals covered by demonstrations on GCPO. \(_{1},_{2},_{3}\) represent sets of demonstrations that are difficult, medium, and easy, respectively. The pre-trained policies obtained from \(_{1}\), \(_{2}\), and \(_{3}\) are denoted as \(^{0}_{1}\), \(^{0}_{2}\), and \(^{0}_{3}\), respectively. The corresponding GCPO policies are denoted as \(^{*}_{1}\), \(^{*}_{2}\), and \(^{*}_{3}\), respectively. Results are derived from experiments across 5 random seeds.

#### 4.3.3 Sensitivity to Parameters of Estimating \(p_{ag}\)

To evaluate the influence of the estimation of \(p_{ag}\) on GCPO, we conduct experiments with GCPO across a range of values for \(N\) and \(\), employing various self-curriculum methods. The results are presented in Table 3.

The internal three-by-three layout of each sub-table reveals that policies with the highest performance tend to be situated at configurations where \(N\) is larger and \(\) is smaller. This trend suggests that under these conditions, the estimation of \(p_{ag}\) is more precise. Ideally, as \(N\) and \( 0\), the estimation of \(p_{ag}\) could be perfectly fitted. when examining \(N\) and \(\) independently, the last row of each sub-table indicates that conducting more evaluations during online learning helps GCPO to obtain a well-performing policy, although this comes at the cost of additional computational resources. Conversely, the sum of each row of each sub-table shows no significant difference, implying that GCPO is less sensitive to the setting of \(\).

In summary, from the perspective of estimating \(p_{ag}\), to enhance the performance of GCPO, it is primarily advisable to increase the number of evaluations within the tolerable computational resource constraints.

#### 4.3.4 Comparison on Different Self-Curriculum Methods

To demonstrate the influence of different self-curriculum methods on the learning progression and final policy of GCPO, we train GCPO with three distinct self-curriculum methods: RIG, DISCERN, and MEGA. The learning curve and the final policy performance are illustrated in Fig. 5.

Fig. 5 presents the curve of goal difficulty sampled during learning. It is noted that RIG rapidly samples more challenging goals, followed by DISCERN, and then MEGA. However, when considering the difficulty of the goals sampled at the final stage of learning, DISCERN and MEGA select harder goals than RIG. This observation suggests that RIG, DISCERN, and MEGA exhibit distinctly different learning progressions.

Fig. 5 depicts the trend of success rate during learning, and Fig. 5 and present the histograms of achieved goals for the policies trained by self-curriculum and non-curriculum methods, respectively. By combining Figs. 5 and 5, it is evident that there is no significant difference in performance between different self-curriculum methods, whether in the learning progression or in the final policy. In contrast, when combining Figs. 5 and 5, it is clear that self-curriculum methods outperform non-curriculum methods in both the learning progression and the final policy performance.

In summary, within the GCPO framework, while different self-curriculum methods exhibit distinct learning progressions, there is no discernible difference in the final policy obtained. Moreover, the self-curriculum methods consistently outperform non-curriculum methods, highlighting the effectiveness of the self-curriculum mechanism in promoting goal-conditioned policy learning.

Figure 5: Analysis of the influence of different self-curriculum methods on the learning progression of GCPO, as well as a comparison between self-curriculum and non-curriculum methods. ‘expert’ and ‘None’ are two non-curriculum methods, where ‘expert’ refers to sampling goals from those that the demonstrator can achieve, and ‘None’ signifies directly sampling from \(p_{dg}\). Results are derived from experiments across 5 random seeds.

Conclusion and Limitations

In this paper, we propose an on-policy goal-conditioned reinforcement learning framework, GCPO, designed to address the limitations of existing methods in solving non-Markovian reward (NMR) problems. Through experimental evaluation on the fixed-wing Velocity Vector Control task, we demonstrate the effectiveness of GCPO in handling both Markovian reward (MR) and NMR problems.

Some limitations should be addressed in future work. Firstly, in the implementation of the two components within GCPO, we employ relatively simple methods, such as behavioral cloning and Gaussian mixture model. Whether the use of alternative methods could lead to more efficient learning and better-performing policies is yet to be further validated. Secondly, under the sparse reward setting, the successful training of GCPO relies on the pre-trained policy possessing a certain level of goal-achieving capability. Otherwise, if the policy achieves nothing, it becomes ineffective in establishing a self-curriculum. Lastly, although the GCPO framework does not have a component like HER that is unsuitable for solving NMR problems and thus capable of solving both MR and NMR problems, the specific implementation of GCPO as introduced in Section 1 does not explicitly incorporate components that are specifically designed to handle NMR problems. In Appendix F, we introduce a simple component to address NMR problems within GCPO and observe some effects. However, it is not clear whether integrating the most advanced methods for handling NMR problems within GCPO would lead to a more effective resolution.