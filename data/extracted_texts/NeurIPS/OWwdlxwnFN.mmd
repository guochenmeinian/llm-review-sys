# MonkeySee: Space-time-resolved reconstructions of natural images from macaque multi-unit activity

Lynn Le1, Paolo Papale 2, Katja Seeliger3, Antonio Lozano2, Thirza Dado1,

**Feng Wang** 2, **Pieter Roelfsema** 2,4,5,6, **Marcel van Gerven1, **Yagmur Gueluturk1, **Umut Gucluit** 1

1Donders Institute for Brain, Cognition and Behaviour, Radboud University, Nijmegen, Netherlands

2Netherlands Institute for Neuroscience, Amsterdam, Netherlands

3Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany

4Centre for Neurogenomics and Cognitive Research, Vrije Universiteit, Amsterdam, Netherlands

5Institut de la Vision, Paris, France

6Amsterdam University Medical Center, Amsterdam, Netherlands

+Correspondence Email: u.guclu@donders.ru.nl

###### Abstract

In this paper, we reconstruct naturalistic images directly from macaque brain signals using a convolutional neural network (CNN) based decoder. We investigate the ability of this CNN-based decoding technique to differentiate among neuronal populations from areas V1, V4, and IT, revealing distinct readout characteristics for each. This research marks a progression from low-level to high-level brain signals, thereby enriching the existing framework for utilizing CNN-based decoders to decode brain activity. Our results demonstrate high-precision reconstructions of naturalistic images, highlighting the efficiency of CNN-based decoders in advancing our knowledge of how the brain's representations translate into pixels. Additionally, we present a novel space-time-resolved decoding technique, demonstrating how temporal resolution in decoding can advance our understanding of neural representations. Moreover, we introduce a learned receptive field layer that sheds light on the CNN-based model's data processing during training, enhancing understanding of its structure and interpretive capacity.

## 1 Introduction

Artificial neural network models designed for decoding naturalistic images from neural activity signals significantly advance our understanding of how visual information is processed in the brain. Decoding models aim to disentangle patterns of neural responses to different stimuli, offering insights into how visual stimuli (e.g., a brown horse or a white t-shirt) are represented by neural populations. Leveraging a large amount of naturalistic data facilitates the reconstruction of natural vision and allows for comprehensive analyses of visual features. Due to the immense variety of naturalistic visual space, reconstructing such stimuli from neural activity is considered the most challenging but also the most intriguing problem in neural decoding.

Convolutional neural networks (CNNs) have recently become a cornerstone in neural decoding and encoding models. Their ability to study fundamental features carried by populations of neuronal signals has led to significant advancements in understanding the computational mechanisms of natural scene perception. Encoding models provide valuable insights into how brain activity changes in response to stimuli, showcasing the features of CNNs that predict neural responses within specificbrain regions. Conversely, decoding models reveal the information content within brain areas without making assumptions about the representation of that information.

Despite the prowess of CNNs in distinguishing between different brain signal patterns, their effectiveness in decoding shifts in neuronal activity remains limited. This limitation is crucial - a lack of variance in reconstructed images with changing brain signals suggests a disconnect between the models and the brain's interpretive functions. This performance gap could stem from intrinsic model limitations or data distribution issues. Refining these methods is essential to enhance our understanding of how closely decoding models can approximate the complex processes underlying visual perception.

While CNNs are engineered to handle complex, multi-dimensional data, including spatial (height and width) and depth (color) dimensions, there remain significant distinctions between CNN processing and human brain information processing. To address these challenges, we have developed a fully convolutional decoding model trained from scratch using the THINGS dataset - a highly diverse collection of naturalistic stimuli. This dataset enriches the model's exposure to varied features, which is crucial for interpreting brain representations accurately.

In this paper, we present the following contributions:

* **Homeomorphic decoder**: We propose a CNN-based decoder trained to investigate the importance of spatial and temporal information carried in neuronal signals for high-fidelity visual reconstructions.
* **End-to-end inverse retinotopic mapping**: We integrate an interpretable layer in the model, known as the end-to-end inverse retinotopic mapping. This layer dynamically learns to map brain signals to a 2D image during training. The adaptive mechanism of this layer, influenced by the entire learning process, allows the decoder to organize its own input spatially.
* **Model inference and analysis**: By performing model inferences with truncated brain data, our approach dissects how the network reorganizes its weights based on spatially separated brain regions for reconstructions. This method aligns with known neuroscientific principles, enhancing the interpretability of decoded features.
* **Temporal dynamics**: Our model incorporates specific time intervals for neuronal signal input, aligned with latency periods observed in the ventral visual pathway (V1 to IT). This temporal aspect allows for a deeper analysis of how visual processing evolves over time.

The remainder of this paper is structured as follows. Section 2 reviews related work in neural decoding using CNNs, highlighting advances and existing challenges. Section 3 details the materials and methods, including data acquisition, preprocessing, and the model architecture. Section 4 presents the results and discussion, evaluating the performance and interpretability of our model. Finally, Section 5 concludes the paper and suggests avenues for future research.

## 2 Related work

Deep neural networks (DNNs) and generative adversarial networks (GANs) have recently achieved notable success in decoding visual information from brain activity, particularly using fMRI data [1; 2; 3; 4; 5; 6; 7; 8]. Seeliger et al. (2018)  utilized GANs to reconstruct grayscale images and handwritten characters from fMRI data, demonstrating the versatility of adversarial methods. Nishimoto et al. (2011)  used voxel-wise modeling to reconstruct complex video stimuli from brain activity, focusing on capturing detailed neural responses in the visual cortex. Shen et al. (2019a)  further advanced this by training end-to-end models for image reconstruction, incorporating high-level feature losses into the GAN framework.

Reconstructing dynamic stimuli like videos presents additional challenges. Han et al. (2019)  used variational auto-encoders for video reconstruction, achieving low-level property reconstructions but struggling with finer details. This highlights the complexity of decoding dynamic visual information.

CNNs are effective for neural decoding due to their capability to process complex, multi-dimensional data. Sarraf and Tofighi (2016)  treated fMRI slices as separate images, but this method was limited by noise and did not respect neural topography. Approaches using 3D convolutions to preserve spatial structure , and geometric deep learning on cortical meshes [11; 12; 13; 14; 15], have shown promise in capturing brain features more effectively.

Decoding naturalistic stimuli using large datasets pushes conventional model boundaries [16; 17]. Le et al. (2022)  reconstructed images and videos from fMRI data by converting voxel responses into 2D representations aligned with the visual field, then applying fully convolutional networks trained with VGG feature loss and adversarial regularizers. This method showed significant improvements over previous techniques.

Our work uses multi-unit activity (MUA) data from Utah electrode arrays in the macaque visual cortex (V1, V4, IT), which offers higher temporal resolution and captures fine-grained neuronal activity compared to fMRI. Bashivan et al. (2019)  integrated receptive field concepts into neural network models for better interpretability. Building on this, we incorporate an end-to-end inverse retinotopic mapping layer within our convolutional decoder. This layer dynamically maps brain signals to 2D images, improving spatial feature organization and providing deeper insights into neural processing.

## 3 Material & methods

### Data

We used images from the THINGS database , containing high-resolution images across various object categories. Each image had three color channels (RGB) and was resized to 96 \(\) 96 pixels to meet model input requirements and reduce computational complexity. Images were presented in the lower right quadrant, shifted 150 pixels right and down from the central fixation point.

A passive fixation task was conducted with a 7-year-old male macaque (_Macaca Mulatta_) across 22,348 trials (22,248 training and 100 test trials). The macaque maintained fixation on a central dot while images, presented in a randomized sequence of four per trial, were displayed for 200 ms with a 200 ms gray screen interval. The macaque was rewarded with juice for maintaining fixation. Ethical approval was obtained from the Royal Netherlands Academy of Arts and Sciences, adhering to the NIH Guide for the Care and Use of Laboratory Animals, ensuring the macaque's well-being and minimizing stress.

MUA was recorded from 15 Utah electrode arrays implanted in V1 (7 arrays), V4 (4 arrays), and IT (4 arrays), capturing neural activity at 1 ms resolution over 200 ms per trial. Electrodes were selected using a self-correlation reliability score with a threshold of 0.4, reducing the original 1024 electrodes to 576 (including losses from a broken electrode). Neural responses were z-scored (mean subtracted, divided by standard deviation). Time windows were defined as 0-125 ms for V1, 25-150 ms for V4, and 50-175 ms for IT. A 25 ms smoothing window was applied, and data were temporally downsampled to either 8 Hz (averaged over 125 ms) or 40 Hz, ensuring consistent and normalized data for modeling.

### Models

In this section, we describe the models used for decoding the neural responses into visual stimuli. The main model is a homeomorphic decoder, and we compare its performance against a baseline decoder. Additionally, we employ a discriminator to facilitate adversarial training.

#### 3.2.1 Homeomorphic decoder

The homeomorphic decoder transforms neural responses into retinal embeddings and subsequently reconstructs the visual stimuli from these embeddings. The architecture leverages several neural network components, including pre-trained and end-to-end trained models.

Pre-trained inverse retinotopic mappingThe first variant of our homeomorphic decoder uses a pre-trained CNN to perform inverse retinotopic mapping. The model projects neural responses onto retinal embeddings using learned weights and subsequently reconstructs the visual image from these embeddings. The process involves two types of embeddings: spatial embeddings and spatiotemporal embeddings.

For spatial embeddings, the retinal embedding \(E\) is computed directly from the neural responses at a single timepoint:

\[E_{axy}=_{e_{a}}r_{e}W_{exy}\]

where \(r\) represents the neural responses over \(576\) electrodes at a single timepoint, and \(W_{exy}\) are the learned spatial weights for mapping the neural responses to retinal embeddings for each microelectrode array (MEA\({}_{a}\)).

For spatiotemporal embeddings, the retinal embedding \(E\) is computed from the neural responses over multiple timepoints:

\[E_{i_{t}xy}=_{e_{i}}R_{et}W_{exy}\]

where \(R\) represents the neural responses over \(576\) electrodes across \(5\) timepoints, and \(W_{exy}\) are the learned weights for mapping the neural responses to retinal embeddings for each region of interest (ROI\({}_{i}\)).

The weights \(W\) are optimized by minimizing the following objective function:

\[W_{e}^{*},_{e}^{*}=_{W,}\|r_{e}-_{e}\|_{2}+_{1} \|W\|_{1}+_{2}(\|W\|_{2}^{2}+\|\|_{2}^{2})+_{3}\]

where \(_{e}=_{c}_{ec}_{xy}W_{exy}f(S)_{cxy}\) is the estimated neural response with \(f\) representing activations from a pre-trained Inception v1 network and \( w\) is the Laplace operator. Here, \(W\) can be considered the spatial weights of interest and \(\) the feature weights. The specific layers of the network used for different cortical areas are: conv2d2 for V1, mixed4a for V4, and mixed4d for IT. These embeddings leverage the pre-trained network to efficiently map neural activity patterns to their corresponding visual stimuli representations, forming a crucial component of our decoder.

End-to-end trained inverse retinotopic mappingThis variant of our homeomorphic decoder involves end-to-end training of the inverse retinotopic mapping from neural responses \(R\) to retinal embeddings \(E_{a}\), allowing the model to learn optimal parameters directly from data without pre-trained CNN features.

The inverse receptive field (see Figure 6) computes \(E\) as:

\[E_{axy}=_{e_{a}}r_{e}(-()^{2}}{2 _{e}^{2}}+)^{2}}{2_{e}^{2}}))\]

where \(r_{e}\) represents the neural responses, \(x_{e}\) and \(y_{e}\) denote the spatial coordinates of electrode \(e\), and \(_{e}\) represents the standard deviation parameter that determines the spatial spread of the receptive field associated with each electrode. The parameters \(x_{e}\), \(y_{e}\), and \(_{e}\) are learned during training.

Once \(R_{a}\) is mapped onto \(E_{a}\), a pixel-to-pixel U-Net reconstructs the stimulus \(S_{}=(E_{a})\), where \(S_{}\) is the ground-truth stimulus. Loss components are functions of \(S_{}\) and \(S_{}\), incorporating \(E_{a}\) into the objective function.

This end-to-end approach adapts to the specific characteristics of neural responses and visual stimuli, improving reconstruction performance.

Pixel-to-pixel mappingThe final component of our homeomorphic decoder employs a U-Net architecture designed for pixel-to-pixel mapping of retinal embeddings to visual stimuli. The U-Net model is a powerful neural network architecture commonly used for image segmentation tasks due to its ability to capture both local and global image features through its contracting and expansive paths connected via skip connections. We are using a standard U-Net architecture; for details refer to Appendix A.1.

#### 3.2.2 Baseline decoder

We employ a baseline decoder as a reference for performance evaluation. This simpler model transforms neural responses directly into visual stimuli, without the use of intermediate retinal embeddings. It adopts a modified U-Net architecture, retaining only the expansive path from the homeomorphic decoder while omitting the contracting path and skip connections. The input is a \(576 1\) tensor representing neural responses from 576 electrodes at a single timepoint, and the output is a RGB \( 96 96\) visual stimulus. Despite its simplicity, the baseline serves as a crucial benchmark for assessing more complex models like the homeomorphic decoder. This approach is inspired by , which is regarded as a state-of-the-art reconstruction model .

#### 3.2.3 Discriminator

The discriminator plays an integral role in the training process by differentiating between real and fake visual stimuli, thus facilitating adversarial training. We employ a modified U-Net architecture, using only the contracting path, to serve as our discriminator. The input to the discriminator is the visual stimulus, \(S\), of dimensions RGB \( 96 96\). The output is a scalar probability \(p\), indicating the likelihood that the given image is real. Adversarial training, where the discriminator aims to distinguish real images from reconstructed images, drives the decoder to generate more realistic and accurate visual stimuli. The loss calculated from the discriminator's assessments is crucial for improving the fidelity of the decoded images.

### Training

This section outlines the optimization procedures, loss functions, and strategies used to train the decoders and the discriminator for high-fidelity reconstruction of visual stimuli from neural responses. Source code is available on our GitHub repository1.

#### 3.3.1 Training parameters

The dataset comprised 22,348 training samples and 100 test samples, which were exclusively used for testing and never during training. We used the Adam optimizer with a learning rate of 0.002 and beta coefficients of 0.5 and 0.999 to ensure convergence. The loss function included discriminator loss (\(_{}\)) at 0.01, VGG feature loss (\(_{}\)) at 0.9, and L1 pixel-wise loss (\(_{}\)) at 0.09 to balance sensitivity. Training spanned 50 epochs on a Quadro RTX 6000 GPU, utilizing approximately 10,000 MiB of GPU memory.

#### 3.3.2 Decoder training

The training of the decoders (both homeomorphic and baseline) involves a combination of losses designed to ensure realism and accuracy in the reconstructed images. The _adversarial loss_ encourages the decoder to generate realistic images that the discriminator cannot distinguish from real images. This loss is defined as the binary cross-entropy loss between the discriminator's output and the true labels (1 for real images and 0 for generated images):

\[_{}=-_{S_{}}[ D(S_{} )]-_{S_{}}[(1-D(S_{}))]\]

To further enhance the quality of the reconstructed images, we use a _feature matching loss_ based on the activations of a pre-trained VGG-19 network. The feature loss is the mean squared error (MSE) between the feature representations of the real and generated images at various layers (conv1_2, conv2_2, conv3_4, conv4_4, and conv5_4) of the VGG-19 network:

\[_{}=_{l}\|_{l}(S_{})-_{l}(S_{ })\|_{2}^{2}\]

where \(_{l}\) denotes the feature map at layer \(l\) of the VGG-19 network. The _pixel-wise loss_ ensures that the reconstructions are close to the original images in pixel space. We use the mean absolute error (MAE) to quantify this loss:

\[_{}=\|S_{}-S_{}\|_{1}\]

The total loss for the decoder is a weighted sum of the adversarial loss, feature loss, and pixel loss:

\[_{}=_{}_{}+ _{}_{}+_{} _{}\]

where \(_{}\), \(_{}\), and \(_{}\) are the weights for each respective loss component.

#### 3.3.3 Discriminator training

The discriminator is trained to distinguish between real and generated images, using the adversarial loss defined as the binary cross-entropy loss:

\[_{}=-_{S_{}}[ D(S_{ })]-_{S_{}}[(1-D(S_{}))]\]

#### 3.3.4 Optimization strategy

The decoder and discriminator are optimized using the Adam optimizer with default parameters (learning rate = 0.0002, \(_{1}=0.5\), \(_{2}=0.999\)). Early stopping is applied based on validation set performance to prevent overfitting. An image buffer maintains a history of generated images to stabilize adversarial training by varying the discriminator's inputs. During training, the decoder and discriminator are updated alternately: 1) the discriminator using adversarial loss, and 2) the decoder using the total loss (adversarial, feature, and pixel losses). Training continues until convergence, monitored by evaluation metrics.

### Evaluation metrics

To robustly evaluate the performance of our decoding models, we employ a variety of metrics that assess the accuracy and quality of the reconstructed visual stimuli from both a feature-wise and perceptual quality perspective.

Feature correlationOne of the primary metrics used to evaluate stimulus reconstruction is the feature correlation between the reconstructed images and the original images. We use a pre-trained AlexNet, extracting feature representations at various layers (conv1, conv2, conv3, conv4, conv5, fc6, fc7, fc8). The Pearson correlation coefficient is calculated between corresponding feature maps of the original and reconstructed images: \(_{_{l}}=(_{l}(S_{}),_{l}(S_{}))\) where \(_{l}\) denotes the feature map at layer \(l\) of AlexNet. High correlation values indicate that the reconstructed images capture similar feature representations as the original images. This standard metric for evaluating reconstruction quality was also used by Le et al. (2022) .

Image colorfulnessTo evaluate the perceptual quality of the reconstructed images, we use the Hasler and Susstrunk colorfulness metric . This metric quantifies the colorfulness of an image, which is an important aspect of human visual perception. The metric is computed as: \(C=_{rg}+0.3_{rg}\) where \(_{rg}\) and \(_{rg}\) are the standard deviation and mean of the color difference vector \(rg=(R-G)\) across the image.

Occlusion analysisWe performed spatial and spatiotemporal occlusion analyses to assess the contributions of different spatial and temporal regions to reconstruction accuracy. In spatial occlusion, parts of the input are systematically removed to identify which regions are most critical for decoding. In spatiotemporal occlusion, specific segments of neural responses are occluded to evaluate the importance of different timepoints and spatial regions in the reconstruction process.

## 4 Results and discussion

### Stimulus reconstruction

The performance of different decoding models in reconstructing visual stimuli from neural responses is evaluated across multiple dimensions, including model comparison, spatial occlusion analysis, and spatiotemporal occlusion analysis.

#### 4.1.1 Model comparison

We compared the reconstruction performance of three variations of our homeomorphic decoder - spatial, spatiotemporal, and end-to-end inverse retinotopic mapping - against the baseline model, both qualitatively and quantitatively.

Figure 1 presents qualitative results. The spatial, spatiotemporal, and end-to-end decoders consistently outperformed the baseline, better preserving textures, shapes, and colors.

Table 1 provides a quantitative comparison using Pearson correlations between reconstructed and original images across different AlexNet layers (Section 3.4). The spatiotemporal decoding model achieved the highest feature correlation across most layers, highlighting its superior ability to capture and reconstruct the features present in the original stimuli. Specifically, the spatiotemporal model excelled in the deeper layers (fc7 and fc8), which capture high-level feature representations, demonstrating the model's capability to capture both fine-grained and abstract features encoded in the neural responses.

The learned receptive field layer adapts receptive field sizes by spatial location, with larger fields in the periphery and smaller fields centrally (Figure 7). Despite some receptive fields being smaller than one pixel, all electrodes still contribute information, with single pixels used when needed. This effect is likely due to the model's limited 96 \(\) 96 field of view, constraining pixel allocation for very small fields.

We also ran model ablations to assess the effect of different loss functions (discriminator, pixel, and VGG loss) on performance (Figure 11). Additionally, we trained models on region-specific data (V1, V4, IT) to explore how brain region training affects reconstruction (Figure 12). These experiments highlight the importance of individual brain regions and the adaptability of models trained on occluded data.

#### 4.1.2 Spatial occlusion analysis

Spatial occlusion analysis was conducted to identify the importance of different brain regions (V1, V4, IT) in the reconstruction process. This analysis involved occluding specific spatial regions of the neural response inputs and examining the effect on the quality of the reconstructed images.

    & Spatial & Spatiotemporal & End-to-end & Baseline \\  conv1 & 0.358 & **0.372** & 0.348 & 0.267 \\ conv2 & 0.320 & **0.334** & 0.303 & 0.221 \\ conv3 & 0.429 & **0.443** & 0.407 & 0.326 \\ conv4 & 0.385 & **0.401** & 0.369 & 0.316 \\ conv5 & 0.292 & **0.318** & 0.282 & 0.203 \\ FC6 & 0.344 & **0.377** & 0.325 & 0.235 \\ FC7 & 0.534 & **0.579** & 0.541 & 0.434 \\ FC8 & 0.579 & **0.610** & 0.543 & 0.446 \\   

Table 1: Feature correlations of reconstructions with original images across AlexNet layers.

Figure 1: Sample stimuli and corresponding reconstructions from models. The “Spatial” and ”Spatiotemporal” column show results from the pre-trained inverse retinotopic mapping model, explained in Section 3.2.1. The “End-to-end” column shows reconstructions from the space-resolved model with a component that learns the neuron’s receptive field explained in Section 3.2.1. “Baseline” shows the reconstructions of a model we implemented explained in Section 3.2.2.

Figure 2 provides visualizations of example stimuli and their corresponding reconstructions when inputs from different brain regions (V1, V4, IT) were selectively occluded. The columns represent the regions of interest, while the rows present different example stimuli. For each region, the neural responses were set to their baseline values (pre-stimulus onset) for the occlusion procedure.

To quantify the impact of occlusion on reconstruction quality, feature correlations with the original images were calculated using the pre-trained AlexNet, as shown in Figure 8. The bar plot depicts feature correlations across different AlexNet layers for reconstructions derived from V1, V4, IT, and all regions combined.

### Spatiotemporal occlusion analysis

Spatiotemporal occlusion analysis evaluates how neural responses from different time windows contribute to reconstruction quality, offering insights into the brain's temporal processing of visual stimuli.

Figure 3 shows example stimuli and their reconstructions when multiple time windows are occluded, with only one time window (highlighted in yellow) being included in the model during inference. Each column represents a different set of time windows being occluded, with the first column showing the reconstruction using all time windows.

The colorfulness of reconstructions from V1, V4, and IT was evaluated using the Hasler and Susstrunk metric. Figure 4 shows the colorfulness scores for each brain region and combined data. V1-constrained reconstructions had the highest colorfulness scores and correlated strongly with early AlexNet layers (conv1, conv2), reflecting V1's role in processing basic features like edges and color. IT reconstructions, however, showed higher correlations with deeper AlexNet layers (fc7, fc8), which capture more abstract visual features.

These results highlight the hierarchical nature of visual processing, with V1 specializing in basic visual attributes and IT handling more complex features.

## 5 Conclusion

We presented a comprehensive approach to decoding naturalistic visual stimuli from neural responses using a fully convolutional neural network trained from scratch. The use of the THINGS dataset enriched our model's feature set, crucial for accurately interpreting brain representations. Our homeomorphic decoder, enhanced with an end-to-end inverse retinotopic mapping layer, effectively integrates spatial and temporal information, leading to high-fidelity and interpretable reconstructions. Our evaluations highlighted the spatiotemporal decoding model's superior performance, evidenced by high feature correlations with deep layers of pre-trained AlexNet. Spatial and spatiotemporal

Figure 2: Spatial occlusion analysis of spatial model as explained in Section 4.1.2. Title above column means included brain region.

occlusion analyses provided insights into the contributions of different brain regions and time windows, affirming the hierarchical nature of visual processing. The end-to-end inverse retinotopic mapping facilitated accurate estimation of receptive fields, aligning with neurophysiological findings and enhancing model transparency. This work advances neural decoding by offering a scalable and interpretable framework for reconstructing high-quality images from brain signals. Future research will explore more sophisticated architectures, further integrating temporal information and applying this framework to other sensory modalities.

#### Broader impact

Neural decoding models for reconstructing naturalistic images deepen our understanding of the link between neural activity and perception, with promising applications in visual neuroprosthetics. This study specifically reuses data originally collected as part of an initiative aimed at restoring sight, adhering to ethical best practices by maximizing insights from a single dataset and minimizing

Figure 4: Distribution of colorfulness metrics across V1, V4, and IT-constrained reconstructions, calculated using the Composite Colorfulness Score (CCS) based on RGB channel differences.

Figure 3: Spatiotemporal occlusion analysis. Yellow indicates the active time window, with others occluded.

the need for additional animal studies. This approach aligns with responsible research standards, balancing innovation with animal welfare.

However, caution is needed when applying these models to human neuroprosthetics due to the complexity of human brain activity and behavior. Human behavior encompasses interactions with the environment, complex motor actions, and neuroplasticity, elements that models trained exclusively on data from static image viewing may not fully capture. For instance, visual feedback from motor activities adds layers of complexity beyond current model capabilities. Additionally, using these models to identify stimulation sites for neuroprosthetics may not replicate natural neural responses precisely, highlighting the need for experiments/clinical trials with human subjects, nuanced model development and careful interpretation of outputs.

#### Limitations

This study applies invasive MUA recordings in macaques, expanding on prior work with fMRI by offering higher signal quality and detail. However, the applicability of these results to non-invasive techniques like fMRI remains limited due to their lower signal-to-noise ratio and less detailed recording capabilities compared to electrode arrays. This distinction is important, as invasive neuroimaging remains rare in human research.

Additionally, transitioning this framework to human intracranial applications poses challenges, including potential scar tissue formation, immune responses, and device rejection over long-term recordings. Anatomical differences, such as vascular structures, may further impact device placement and stability. Future work could explore these adaptations for broader applications, including brain-computer interfaces (BCI) and neuroprosthetics for individuals with acquired blindness, for which, careful regulatory guidance and additional research will be essential.

#### Acknowledgments

This work was supported by three grants of the Dutch Organization for Scientific Research (NWO): STW grant number P15-42 'NESTOR', ALW grant number 823-02-010 and Cross-over grant number 17619 'INTENSE' and grant number 024.005.022 'DBI2', a Gravitation program of the Dutch Ministry of Science, Education and Culture; the European Union's Horizon 2020 research and innovation programme: grant number 899287, 'NeuraViper'; the Human Brain Project, grant number 650003.