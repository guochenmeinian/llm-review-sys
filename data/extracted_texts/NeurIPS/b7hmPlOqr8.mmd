# Learning Frequency-Adapted Vision Foundation

Model for Domain Generalized Semantic Segmentation

Qi Bi\({}^{1}\), Jingjun Yi\({}^{2}\), Hao Zheng\({}^{2}\)\({}^{2}\)\({}^{2}\)\({}^{3}\), Haolan Zhan\({}^{3}\), Yawen Huang\({}^{2}\),

Wei Ji\({}^{4}\), **Yuexiang Li\({}^{5}\)\({}^{2}\), Yefeng Zheng\({}^{1}\)\({}^{2}\)\({}^{3}\)**

\({}^{1}\)Westlake University, China, \({}^{2}\)Jarvis Research Center, Tencent Youtu Lab, China,

\({}^{3}\)Monash University, Australia, \({}^{4}\)Yale University, United States,

\({}^{5}\)University of Macau, Macau

howzheng@tencent.com, yuexiang.li@ieee.org

zhengyefeng@westlake.edu.cn

Qi Bi is affiliated with University of Amsterdam. This research was conducted with Westlake University and Tencent Youtu Lab.

###### Abstract

The emerging vision foundation model (VFM) has inherited the ability to generalize to unseen images. Nevertheless, the key challenge of domain-generalized semantic segmentation (DGSS) lies in the domain gap attributed to the cross-domain styles, _e.g.,_ the variance of urban landscape and environment dependencies. Hence, maintaining the style-invariant property with varying domain styles becomes the key bottleneck in harnessing VFM for DGSS. The frequency space after Haar wavelet transform provides a feasible way to decouple the style information from the domain-invariant content, since the content and style information is retained in the low- and high-frequency components of the space, respectively. To this end, we propose a novel Frequency-Adapted (FADA) learning scheme to advance the frontier. Its overall idea is to separately tackle the content and style information by frequency tokens throughout the learning process. Particularly, the proposed FADA consists of two branches, _i.e.,_ low- and high-frequency branches. The former is able to stabilize the scene content, while the latter learns the scene styles and eliminates its impact to DGSS. Experiments conducted on various DGSS settings show the state-of-the-art performance of our FADA and its versatility to a variety of VFMs. Source code is available at https://github.com/BiQiWHU/FADA.

## 1 Introduction

Most existing semantic segmentation tasks assume that the training and inference images follow the independent and identical distribution (i.i.d.) [12; 13; 36; 35; 34; 81; 54; 44; 74], which is far from reality. Domain-generalized semantic segmentation (DGSS) aims to infer robust pixel-wise semantic predictions on arbitrary unseen target domains when a segmentation model is trained on the source domain (as illustrated in Fig. 1a). Compared with general domain generalization tasks, the feature distribution discrepancy between the source domain and unseen target domains in the context of DGSS holds some unique factors. Specifically, the cross-domain images in DGSS usually share the same content information (_i.e._, common semantic categories in driving scenes), while the cross-domain styles (_i.e._, urban landscape, weather, lighting conditions, _etc._) mainly account for the feature distribution difference [65; 49; 18; 11; 72; 6; 23; 43; 24].

Existing DGSS methods can be summarized into three categories. The first category intends to decouple the style information from the scene representation [55; 30; 14; 56; 78; 71; 58], but does notensure a strong content representation ability. The second category, on the contrary, directly focuses on the content representation ability regardless of the cross-domain style variations . Such methods were recently developed much owing to the stronger pixel-wise representation ability of mask attention . The third category focuses on enriching the styles as much as possible during training . However, the content representation ability is rarely taken into consideration.

The emerging vision foundation model (VFM) , with strong generalization ability inherited from a large quantity pre-trained images, provides a new possible paradigm for DGSS. Under the realm of parameter-efficient fine-tuning, the hypothesis, _i.e._, a foundation model relies on the low intrinsic dimension  to adapt to the downstream tasks, exemplified by low-rank adaptation (LoRA) , has shown great success, where the key idea is to inject trainable rank decomposition matrices into each layer. More recently, the low-rank adaptation paradigm has demonstrated to be feasible to fine-tune the VFM for DGSS . Unfortunately, the style-invariant properties of VFM, which is a fundamental problem for the extraction of domain generalized semantics, remain unexplored.

An ideal style invariant low-rank adaptation is supposed to discern the subtle low intrinsic dimension while does not pose shift on the fragile and frozen VFM features. The prior DGSS methods learned style invariance from the fully trained image features (e.g., instance normalization  and instance whitening ), which are more likely to collapse and less feasible. Therefore, we adapt the low-rank adaptation to the frequency space, where the style and content have been separated to high-frequency and low-frequency components , respectively.

In this paper, we present a novel **F**requency-**A**d**apted learning scheme, dubbed FADA, to push this frontier. Its conceptual idea is to adapt the style and content representation from the VFM features separately in the frequency space. After transforming the frozen VFM features to the frequency space by the Haar wavelet transform , the low-frequency branch exploits the scene content from frozen VFM features by the learnable low-frequency tokens. In contrast, in the high-frequency branch, the high-frequency token features are implemented using the instance normalization operation, so that the representation becomes invariant to the scene style.

Notably, the proposed FADA introduces two new research lines. Firstly, the possibility of learning low-rank adaptation in the frequency space is explored, which can further benefit other VFM downstream tasks strongly related to the style and content representation. Secondly, it demonstrates the potential of harnessing the Haar wavelet transform for DGSS, which can also inspire advancements in general visual domain generalization.

Concretely, our contributions can be summarized as follows:

Figure 1: (a) The key challenge of domain generalized semantic segmentation (DGSS) lies in the stability of the scene content, while the domain gap is caused by the style variation. (b) Analysis of frozen VFM features after Haar wavelet transform. We compute the correlation coefficient between the CityScapes source domain (C) and BDD, Mapillary, SYHTHIA, GTA5 unseen target domains (B, M, G, S). The low-frequency component exhibits a higher correlation and smaller domain gap. In contrast, the high-frequency components exhibit a lower correlation and a larger domain gap.

* We propose a **F**requency-**A**d**apted learning scheme, dubbed FADA, to fine-tune VFMs for domain-generalized semantic segmentation.
* The proposed FADA, aided by the Haar wavelet guidance to mine the style-invariant property of VFM, is versatile to a variety of VFMs.
* Experimentally, the proposed FADA significantly outperforms the state-of-the-art DGSS methods, and yields an improvement up to 2.9% mIoU over the contemporary REIN .

## 2 Related Work

**Domain Generalization** handles the challenging setting where the feature distribution of an arbitrary target domain is not identical to that of the source domain. It has been extensively studied in the past few years. Multiple techniques, to namely a few, optimal transport [22; 79], batch normalization , causal inference [48; 47; 25], discrepancy regularization [80; 68; 17; 5], and uncertainty modeling [59; 70], have been proposed. Furthermore, domain generalization via unsupervised learning [26; 27] or from a single source domain [61; 59; 84; 75] has also been recently studied.

**Domain Generalization by Frequency Decoupling** has drawn increasing attention. Its general idea rests in that the style and content have been demonstrated on high-frequency and low-frequency components [29; 46; 40; 77; 67; 7; 6], respectively. Most of these methods implement Fast Fourier Transform (FFT) to transfer the image to the frequency space, and then represent the style and content by the amplitude (high-frequency) and phase (low-frequency) components, respectively. However, _to the best of our knowledge_, 1) leveraging Haar wavelet for domain generalization; and 2) enhancing the generalization ability of VFM features via frequency space have been rarely explored. Compared with other frequency analysis methods such as FFT, the orthogonal property of Haar wavelet basis leads to a stronger decorrelation . In the context of domain generalization, it indicates a better separation between low- and high-frequency components and deserves exploration.

**Domain Generalized Semantic Segmentation** (DGSS) in the CNN era either decouple the style information [55; 30; 56; 14; 58; 71; 78] or enrich the style diversity [41; 82; 83; 45; 52]. With the rapid development of Vision Transformer (ViT), recent DGSS methods usually leverage the masked attention mechanism [12; 13] to enhance the content representation [19; 8; 7]. Later, the masked attention is used to decode the frozen contrastive image-text pre-trained features , and REIN  fine-tunes the VFM under the low-rank adaptation paradigm. However, the style invariant properties of VFM, which are the key of the DGSS representation, remain unexplored.

## 3 Preliminary

### Low-Rank Adapted VFM

To fine-tune a VFM with parameter efficiency, the low intrinsic dimension [42; 1] assumes that a VFM relies on the intrinsic low-dimension in the frozen VFM features to adapt to the downstream tasks. Inspired by this, the low-rank adaptation (LoRA) paradigm  is devised to inject trainable rank decomposition matrices into each layer. Given a VFM with \(N\) sequential layers (denoted as \(L_{1}\), \(L_{2}\), \(\), \(L_{N}\)), each layer corresponds to a pre-trained weight matrix of \(W_{1}\), \(W_{2}\), \(\), \(W_{N}\), _i.e._, \(W_{i}^{c c}\). The frozen features from layer \(L_{i}\) are denoted as \(f_{i}^{c n}\). Particularly, for the first layer \(L_{1}\), \(f_{1}\) is generated by \(f_{1}=W_{1}x\). Here \(x\) denotes the image embedding, \(n\) denotes the patch number, and \(c\) denotes the channel size.

Denoting the learnable weight matrix as \( W_{i}^{c c}\) and the input of layer \(L_{i}\) as \(f_{i}\), the feature propagation from the layer \(L_{i}\) to \(L_{i+1}\) can be formulated as \(f_{i+1}=W_{i}f_{i}+ W_{i}f_{i}\). Then, we assume that the learnable weight matrix \( W_{i}\) can be formulated as a low-rank decomposition, _i.e._, \( W_{i}=BA\), where \(B^{c r}\) and \(B^{r c}\) (\(r c\)). More recently, REIN  specifies this paradigm in DGSS by transferring the learnable matrix term \( W_{i}\) into a learnable token \(T_{i}\) followed by a MLP \(_{i}()\), denoted as \(f_{i+1}=W_{i}f_{i}+_{i}(T_{i}(W_{i}f_{i}))\), where \(T_{i}^{m c}\) and \(m\) is the token length. This modification allows a significance reduction of the token length (from a thousand magnitude \(c\) to a hundred or even ten magnitude \(m\)), which can alleviate _Curse of Dimensionality_ and allow each token to be better connected to the instances in an image . Our low-rank adaptation is implemented on \(T_{i}\), _i.e._, \(T_{i}=A_{i}B_{i}\), where \(A_{i}^{m r}\) and \(B_{i}^{r c}\) (\(r(m,c)\)).

### Haar Wavelet Transform

The Haar transform  cross-multiplies a function with various shifts and stretches, which has been demonstrated to be effective in applications, such as signal and image processing.

**Definition 1**.: **Haar Scaling Function.** _Given an input signal \(x\), the Haar scaling function is mathematically defined as_

\[(t)=1&0 t<1\\ 0&.\] (1)

Let \(V_{0}\) denote the space of all functions of the form \(_{k}a_{k}(x-k)\), where \(k\) is an arbitrary integer, and \(a_{k}\). As each element of \(V_{0}\) is zero outside a bounded set, such a function \(a_{k}(x-k)\) has _finite or compact support_.

**Definition 2**.: **Basis of the Step Function Space.** _Given an arbitrary nonnegative integer \(j_{0}^{+}\), Let \(V_{j}\) denote the step function space at the level \(j\), which is spanned by the set_

\[\{,(2^{j}x+1),(2^{j}x),(2^{j}x-1),\}.\] (2)

**Definition 3**.: **Haar Wavelet Function.** _The Haar wavelet is the function \((x)=(2x)-(2x-1)\)._

For more details of the properties of the Haar transform, please refer to the supplementary material.

## 4 Methodology

Fig. 2 gives an overview of the proposed FADA. After each frozen VFM layer, it consists of three key steps, namely, low-/high-frequency decomposition (in Sec. 4.1), low-frequency adaptation (in Sec. 4.2), and high-frequency adaptation (in Sec. 4.3). Finally, the frequency components are fused together and transferred from the frequency space back to the spatial space, and then fed to the next VFM layer.

### Low-/High-Frequency Decomposition

Orthogonal property (in Sec. 3.2) leads to the result of a strong decorrelation . This property after the Haar wavelet function allows a better separation between low- and high-frequency components of input signal than other frequency analysis methods such as Fourier transform. In the context of DGSS, the domain gap is caused by the cross-domain style variation, while the cross-domain content is stable [14; 78; 71; 58]. As it has been well documented that the style and content are predomin

Figure 2: Overview of the proposed Frequency-adapted Vision Foundation Model (FADA) learning scheme. It innovatively incorporates the low-rank adaptation of VFM models on the frequency space, where the low-/high-frequency component contains more content/style, respectively. It consists of three key steps, namely, low-/high-frequency decomposition (in Sec. 4.1), low-frequency adaptation (in Sec. 4.2) and high-frequency adaptation (in Sec. 4.3).

in the high-frequency and low-frequency components, respectively [29; 46; 40; 77; 67], a feasible way to explore the style-invariant properties is to mitigate the variation of high-frequency components in the VFM features. Therefore, the first step is to decompose the frozen VFM feature \(f_{i}\) into the low- and high-frequency components, respectively.

In our work, we exploit the Haar wavelet transform to decouple the low- and high-frequency components, where four kernels, namely, \(LL^{ T}\), \(LH^{ T}\), \(HL^{ T}\), \(HH^{ T}\), are given by

\[L^{ T}=}[1 1],H^{ T}=}[-1 1].\] (3)

As discussed in prior works [60; 3], the \(LL^{ T}\) kernel captures the average of the pixel responses, which is more robust to the scene content and therefore preserves more low-frequency components. In contrast, by taking the differences between adjacent pixels into account, the \(LH^{ T}\), \(HL^{ T}\) and \(HH^{ T}\) kernels tend to preserve the details from the horizontal, vertical and diagonal directions, respectively. These details are more related to the structures, edges, _etc_, which attribute to the style.

For a layer \(L_{i}\), we implement the Haar wavelet transform on the frozen VFM feature \(W_{i}f_{i}\) by the above four kernels \(LL^{ T}\), \(LH^{ T}\), \(HL^{ T}\) and \(HH^{ T}\), respectively. The \(f^{LL}\) component filtered by \(LL^{ T}\) captures more scene content, and in contrast the \(f^{LH}_{i}\), \(f^{HL}_{i}\) and \(f^{HH}_{i}\) components filtered by \(LH^{ T}\), \(HL^{ T}\) and \(HH^{ T}\) capture more style information. This decomposition is computed as

\[f^{LL}_{i}=(W_{i}f_{i}) LL^{ T},f^{LH}_{i}=(W_{i}f_{i}) LH^{  T},f^{HL}_{i}=(W_{i}f_{i}) HL^{ T},f^{HH}_{i}=(W_{i}f_{i})  HH^{ T}.\] (4)

### Low-Frequency Adaptation

In the context of DGSS, a stable scene content representation despite the style variance is important to predict the scene semantics. For each layer \(L_{i}\), the scene content from the frozen VFM features rests more in the low-frequency component \(f^{LL}_{i}\) (in Eq. 4), which is learned in the low-frequency adaptation branch.

Assume we have a low-frequency token \(T^{L}_{i}^{m c}\), where \(m\) is the sequence length of \(T^{L}_{i}\), and \(c\) is the dimension of the frozen VFM feature defined in Sec. 3.1. The low-frequency token \(T^{L}_{i}\) is used to exploit the scene content from the low-frequency component \(f^{LL}_{i}\), while at the same time following the low-rank adaptation paradigm (in Sec. 3.1).

First, we compute a similarity map \(S_{i}^{n m}\) between the token \(T^{L}_{i}\) and low-frequency component \(f^{LL}_{i}\) from the frozen VFM feature, which measures the correlation between each element in \(T^{L}_{i}\) and each patch embedding represented in \(f^{LL}_{i}\), given by

\[S^{L}_{i}={ Softmax}(_{i}_{i}}^{ T}}{}),\] (5)

where \({ Softmax}\) denotes the softmax activation function.

Then, we project the token feature \(T^{L}_{i}\) into the feature space of \(f^{LL}_{i}\) by a multilayer perceptron (MLP) parameterized by weight parameters \(W^{1}_{i}\) and bias parameters \(b^{1}_{i}\), followed by the point-wise product with the similarity map \(S^{L}_{i}\). The product with \(S^{L}_{i}\) allows the token features \(T_{i}\) to better align to \(f^{LL}_{i}\), where the scene content is highlighted. Assume the output is denoted as \(^{LL}_{i}\). Briefly, this process can be mathematically expressed as

\[^{LL}_{i}=S^{L}_{i}[T^{L}_{i} W^{1}_{i}+b^{1}_{i}].\] (6)

Then, we fuse the projected token features \(^{LL}_{i}\) with the low-frequency features \(f^{LL}_{i}\) by another MLP parameterized by weight parameters \(W^{2}_{i}\) and bias parameters \(b^{2}_{i}\), followed by the skip connection. Assuming the output is \(^{LL}_{i}\), this process can be mathematically computed as

\[^{LL}_{i}=f^{LL}_{i}+(^{LL}_{i}+f^{LL}_{i}) W^{ 2}_{i}+b^{2}_{i}.\] (7)

### High-Frequency Adaptation

For DGSS, the robustness to the cross-domain style variance is particularly important. Such style difference is usually reflected on the high-frequency components. The Haar wavelet transform enables the separation of these high-frequency components \(f_{i}^{LH}\), \(f_{i}^{HL}\) and \(f_{i}^{HH}\) (in Eq. 4). Directly eliminating all the high-frequency components seems to be a simple and straight-forward solution, but it also leads to the loss of other information such as structure and object boundary. It may degrade a scene representation and decline the segmentation performance. Therefore, the objective of the high-frequency adaptation branch is to mitigate the impact of cross-domain style variation, not directly removing all the high-frequency components.

As the decoupling of styles does not differentiate whether the high-frequency components are from the horizontal, vertical or diagonal directions, for simplicity, we concatenate them together for processing in this branch. Specifically, still in layer \(L_{i}\), assume we have a high-frequency token \(T_{i}^{H}^{3m c}\). The token size of \(T_{i}^{H}\) is tripled compared with the token size of \(T_{i}^{L}\), as three high-frequency components are involved. Similar to the low-frequency branch, the high-frequency token \(T_{i}^{H}\) is used to exploit the style information from the high-frequency components \(f_{i}^{LH}\), \(f_{i}^{HL}\) and \(f_{i}^{HH}\), while at the same time following the low-rank adaptation paradigm (in Sec. 3.1).

Same as the low-frequency adaptation branch, we compute a similarity map \(S_{i}^{H}^{n m}\) between the token \(T_{i}^{H}\) and high-frequency components \(f_{i}^{LH}\), \(f_{i}^{HL}\) and \(f_{i}^{HH}\), given by

\[S_{i}^{H}=(^{LH},f_{i}^{HL},f_{i}^{HH}]^{H}}^{}}{}),\] (8)

where \([,]\) denotes the concatenation operation.

Then, the highlighted positions in \(S_{i}^{H}\) reveal the predominant style responses from the source domain images. The high responses, which reflect more domain-specific styles, are supposed to be suppressed during training. It allows the fine-tuned VFM features to be less impacted by the domain-specific styles. Instance normalization [55; 31], which computes the channel-wise mean and standard deviation, is effective to eliminate the styles. To this end, an instance normalization is implemented on the feature-token similarity map \(S_{i}^{H}\), given by

\[_{i}^{H}=^{H}-}{},=_{ i=1}^{3m}S_{i}^{H},=_{i=1}^{3m}(S_{i}^{H}- )^{2}}.\] (9)

Then, we project the token feature \(T_{i}^{H}\) into the high-frequency feature space by a multilayer perceptron (MLP) parameterized by weight parameters \(W_{i}^{3}\) and bias parameters \(b_{i}^{3}\), followed by the point-wise product with the similarity map \(S_{i}^{H}\). The product with \(S_{i}^{H}\) allows the token features \(T_{i}^{H}\) to better align to the decoupled high-frequency features, which is less relevant to the source domain. Assume the output is denoted as \(_{i}^{H}\). Briefly, this process can be mathematically expressed as

\[_{i}^{H}=_{i}^{H}[T_{i}^{H} W_{i}^{3}+b_ {i}^{3}].\] (10)

Afterwards, we fuse the projected token features \(_{i}^{H}\) with the high-frequency features \(f_{i}^{H}\) by another MLP parameterized by weight parameters \(W_{i}^{4}\) and bias parameters \(b_{i}^{4}\), followed by the skip connection. Assuming the outputs of these three components are \(_{i}^{LH}\), \(_{i}^{HL}\) and \(_{i}^{HH}\), this process can be mathematically computed as

\[[_{i}^{LH},_{i}^{HL},_{i}^{HH}]=[f_{i}^ {LH},f_{i}^{HL},f_{i}^{HH}]+(_{i}^{H}+[f_{i}^{LH},f_{i}^{HL},f_{i }^{HH}]) W_{i}^{4}+b_{i}^{4}.\] (11)

Finally, the low-frequency component \(_{i}^{LL}\) and high-frequency components \(_{i}^{LH}\), \(_{i}^{HL}\), \(_{i}^{HH}\) that have been processed by both branches are fused and transferred back by the inverse Haar wavelet transform. The output, denoted as \(f_{i+1}\), is the input of the next frozen layer \(L_{i+1}\).

### Implementation Details

Same as the REIN  baseline, the loss function \(\) of FADA directly inherits the losses from the Mask2Former decoder , given by

\[=_{ce}_{ce}+_{dice}_{dice}+ _{cls}_{cls},\] (12)where \(_{ce}\), \(_{dice}\) and \(_{cls}\) denote the cross-entropy loss, dice loss and classification loss. Here the hyper-parameters \(_{ce}\), \(_{dice}\) and \(_{cls}\) are 5.0, 5.0 and 2.0, respectively.

By default we use DINO-V2  as the frozen VFM, but the proposed FADA is also feasible to other VFMs. For fair evaluation, the Mask2Former segmentation decoder  is used to generate the pixel-wise prediction as REIN does. Same as the existing paradigm , the images are re-sized to \(512 512\) pixels before input to the models. The Adam optimizer with an initial learning rate of \(1 10^{-4}\) is used to train the model. The training process terminates after 20 epochs.

## 5 Experiments

### Datasets & Evaluation Protocols

Five driving-scene semantic segmentation datasets that share 19 common scene categories are used for validation. Specifically, **CityScapes** (C)  consists of 2,975 and 500 images for training and validation, respectively. The images are captured under the clear conditions in tens of Germany cities. **BDD-100K** (B)  has 7,000 and 1,000 images for training and validation, respectively. The images are captured under diverse conditions from a variety of global cities. **Mapillary** (M)  is another large-scale semantic segmentation dataset, which consists of 25,000 images from diverse conditions. **SYNTHIA** (S)  is a synthetic driving-scene segmentation dataset, which has 9,400 images. **GTA5** (G)  is another synthetic dataset, which has 24,966 simulated images from the American street landscape.

Following the evaluation protocol of existing DGSS methods [55; 56; 14; 58], a certain dataset is used as the source domain for training and the rest four are used as unseen target domains for validation. Three commonly-used evaluation settings are: 1) G \(\) C, B, M, S; 2) S \(\) C, B, M, G; and 3) C \(\) B, M, G, S. The evaluation metric is mean Intersection of Union (mIoU, in percentage %). All of our experiments are implemented and averaged by three independent repetitions, starting from different random seeds.

### Comparison with State-of-the-art DGSS Methods

Existing DGSS methods are involved for comparison: 1) ResNet based methods, namely, IBN , IW , Itermom , DRPC , ISW , GTR , DIRL , SHADE , SAW , WildNet , AdvStyle  and SPC ; 2) Mask2Former based methods, namely, HGFFormer  and CMFormer ; 3) VFM based methods, namely, DIDEX  and REIN . By default, the performance is directly cited from prior works [55; 56; 14; 58]. '-' denotes that the authors did not report the results nor provided source code. '*' denotes re-implementation with official source code under all default settings.

**GTA5 Source Domain.** From left to right, the third column of Table 1 reports the performance. Compared with the VFM based REIN , the proposed FADA shows an mIoU improvement of 1.83%, 1.54%, 1.99% and 1.50% on the C, B, M and S target domains, respectively. In addition, the proposed FADA shows an average mIoU improvement of 20% and 10% when compared with ResNet and Mask2Former based DGSS methods, respectively.

**SYNTHIA Source Domain.** The fourth column of Table 1 shows that the proposed FADA achieves the state-of-the-art performance, outperforming the REIN by 1.45%, 1.41%, 1.22% and 1.29% mIoU on the C, B, M and G unseen target domains, respectively. In addition, the proposed FADA shows an average mIoU improvement of 15% and 6% over ResNet and Mask2Former based DGSS methods.

**CityScapes Source Domain.** The last column of Table 1 shows that the proposed FADA shows an mIoU improvement of 1.58%, 1.83%, 1.37% and 1.19% on the B, M, G and S unseen target domains, respectively. In addition, the proposed FADA shows an average mIoU improvement of 15% and 5% over existing ResNet and Mask2Former based methods, respectively.

### Ablation Studies

**On Each Haar Component.** Five settings are involved for experiments: (1) No wavelet components are used. The model fine-tunes on the frozen VFM, which is a simplified version of REIN  removing the instance link module; (2) Only fine-tuning on the low-frequency component \(f_{i}^{LL}\), and 

[MISSING_PAGE_FAIL:8]

(_e_.\(g\)., 64), the performance on unseen target domains demonstrates a clear decline, which can be explained by the under-fitting and over-fitting, respectively.

**Understanding from Cross-Domain Feature Correlation.** We display the channel-wise correlation matrix of the last-layer feature embeddings from C source domain and B target domain. The results are displayed in Fig. 4. Brightter indicates higher response. FADA allows both low- and high-frequency token features from the source domain and unseen target domains to show similar channel-wise activation response, which allows the model to be better generalized to unseen target domains.

**T-SNE Visualization.** We display the features before the segmentation decoder by t-SNE visualization. The experiments are conducted under the C \(\) B, M, G, S setting. The feature space of the original REIN and the proposed FADA is visualized in Fig. 4. The samples from different unseen target domains are more uniformly distributed by the proposed FADA, narrowing the domain gap.

**Understanding the Benefit of Instance Normalization to Mitigate Domain-specific Information.** We extract the three high-frequency components from the last VFM layer, and display them by t-SNE visualization. The feature space without (denoted as w.o.) and with (denoted as with) implementing the instance normalization function is visualized in the first and second row of Fig. 5, respectively. The experiments are conducted under the C \(\) B, M, G, S setting. It is observed that the implementation of instance normalization allows the samples from different unseen target domains to be more uniformly distributed, indicating its effectiveness to mitigate the domain-specific information containing in the high-frequency components.

### Generalization on Other Settings

**To Different VFMs.** We test the translation ability of the proposed FADA to other VFMs, namely, CLIP , SAM  and EVA02 . For comprehensive evaluation, each VFM is validated under full-training, fine-tuning (REIN), or frozen scheme , respectively. The reported one decimal results are directly cited from . Experiments are conducted under the G \(\) {C, B, M} setting. Table 4 shows the superiority of FADA when embedded into these VFMs. For comparison with parameter-efficient fine-tuning (PEFT) methods, please refer to Table 7 in the supplementary material.

**To Adverse Domains** Adverse Conditions Dataset with Correspondence (ACDC)  is a semantic segmentation dataset that consists of samples from four types of adverse conditions, namely, rain, fog, night and snow. Table 5 shows that the proposed FADA outperforms existing DGSS methods by up to 0.7%, 1.5%, 2.5% and 2.9% on the fog, night, rain and snow domains, respectively.

Figure 4: t-SNE visualization. Feature embedding is extracted from the last VFM layer. Left: baseline; Right: ours.

Figure 5: Impact of instance normalization on the domain generalization property of high-frequency components. w.o./with: without/with implementing instance normalization.

### Quantitative Segmentation Results

Some exemplar segmentation results are compared under the C \(\) B, M, G, S and C \(\) adverse domains are provided in Fig. 6 and Fig. 7, respectively. The proposed FADA shows better pixel-wise prediction than not only ResNet based methods (_i.e._, ISW , SAW , WildNet , and SPC ) and Mask2Former based methods (_i.e._, CMFormer ), but also VFM based REIN .

## 6 Conclusion

In this paper, we focused on adapting VFM for DGSS by exploiting the style-invariant properties from the VFMs, and presented a novel **F**requency-**ADA**pted learning scheme to push this frontier. Concisely, Haar wavelet transform was introduced to decouple the frozen VFM features into low- and high-frequency components, which contain more scene content and style information, respectively. We innovatively modified the low-rank adaptation paradigm to both frequency features, and alleviated the impact of cross-domain variation on high-frequency features. Consequently, the model achieved a better generalization on unseen target domains. Extensive experiments and ablation studies on a variety of settings showed the effectiveness of the proposed FADA.

**Limitation Discussion & Broader Societal Impact.** The proposed FADA handles the low- and high-frequency features separately, which increases the trainable parameters compared with prior work (Table 4). However, the increase of about 6M parameters is acceptable. The proposed FADA advances the reliability and safety of autonomous driving and alleviates human involvement, benefiting the human well-being. We do not envision any negative social impact.

**Acknowledgments and Disclosure of Funding.** This work was supported by the Science and Technology Major Project of Guangxi (AA22096030 and AA22096032), and National Key R&D Program of China under Grant (2020AAA0109500 and 2020AAA0109501).

Figure 6: Exemplar segmentation results of existing DGSS methods (ISW , SAW , WildNet , SPC , CMFormer , and REIN ) and FADA under the C \(\) {B, G, M, S} setting.

Figure 7: Exemplar segmentation results of existing DGSS methods (ISW , SAW , WildNet , SPC , CMFormer , and REIN ) and FADA under the C \(\) four ACDC setting.