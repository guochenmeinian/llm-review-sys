# State-free Reinforcement Learning

Mingyu Chen

Boston University

mingyuc@bu.edu &Aldo Pacchiano

Boston University

Broad Institute of MIT and Harvard

pacchian@bu.edu &Xuezhou Zhang

Boston University

xuezhouz@bu.edu

###### Abstract

In this work, we study the _state-free RL_ problem, where the algorithm does not have the states information before interacting with the environment. Specifically, denote the reachable state set by \(^{}:=\{s|_{}q^{P,}(s)>0\}\), we design an algorithm which requires no information on the state space \(S\) while having a regret that is completely independent of \(\) and only depend on \(^{}\). We view this as a concrete first step towards _parameter-free RL_, with the goal of designing RL algorithms that require no hyper-parameter tuning.

## 1 Introduction

Reinforcement learning (RL) studies the problem where an agent interacts with an _unknown environment_ to optimize cumulative rewards/losses (Sutton and Barto, 2018). While the nature of the environment is in principle hidden from the agent, many existing algorithms (Azar et al., 2017; Jin et al., 2018; Zanette and Brunskill, 2019; Zhang et al., 2020, 2021) implicitly require prior knowledge of parameters of the environment, such as the size of the state space, action space, time horizon and so on. Such parameters play a crucial role in these algorithms, as they are used in the construction of variable initializations, exploration bonuses, confidence sets, etc. However, in most real-world problems, these parameters are not known a priori, resulting in the need for the system designer to perform hyper-parameter tuning in a black-box fashion, which is known to be extremely costly in RL compared to their supervised learning counterparts (Pacchiano et al., 2020). In supervised learning algorithms, selecting among \(M\) hyper-parameters only degrades the sample complexity by a factor of \(O((M))\). In contrast, in RL problems it will incur a \(O()\) multiplier on the regret, making hyper-parameter tuning prohibitively expensive. This is one of the major roadblocks to broader applicability of RL to real-world scenarios.

Motivated by the above observation, we propose and advocate for the study of **parameter-free reinforcement learning**, i.e. the design of RL algorithms that have no or as few hyper-parameters as possible, with the eventual goal of eliminating the need for heavy hyper-parameter tuning in practice. As a concrete first step, in this paper, we focus on the problem of _state-free RL_ in tabular MDPs. In particular, we will show that there exist state-free RL algorithms which do not require the state space \(S\) as an input parameter to the algorithm, nor do their regret scale with the innate state space size \(|S|\). In particular, we design a black-box reduction framework called State-Free Reinforcement Learning (SFRL). Given any existing RL algorithm for stochastic or adversarial MDPs, this framework can transform it into a state-free RL algorithm through a black-box reduction. We also show that the same framework can be adapted to induce action-free and horizon-free algorithms, the three of which now makes a tabular MDP algorithm completely parameter-free, i.e. it requires no input parameters whatsoever, and their regret bound automatically adapt to the intrinsic complexity of the problem.

The rest of the paper is organized as follows. Following the discussion of related works and problem formulation, we start by discussing the technical challenges of state-free learning and why existing algorithmic and analysis framework are not able to achieve it (Section 4). Built upon these insights, we propose an intuitive black-box reduction framework SF-RL, that transforms any RL algorithminto a state-free RL algorithm, albeit incurring a multiplicative cost to the regret (Section 5). Further improvements are then made to eliminate the additional cost through a novel confidence interval design, which can be of independent interest (Section 6).

## 2 Related Works

Parameter-free algorithms:Acknowledgedly, parameter-free learning is not a new concept and has been studied extensively in the optimization and online learning community. Parameter-free algorithms refer to algorithms that do not require the learner to specify certain hyperparameters in advance. These algorithms are appealing in both theory and practice, considering that tuning algorithmic parameters is a challenging task (Bottou, 2012; Schaul et al., 2013). The types of hyperparameters to "set free" varies depending on the specific problem. For example, for online learning and bandit problems, the hyperparameters are considered as the scale bound of the losses (De Rooij et al., 2014; Orabona and Pal, 2018; Duchi et al., 2011; Chen and Zhang, 2023), or the range of the decision set (Orabona and Pal, 2016; Cutkosky and Orabona, 2018; Zhang et al., 2022; van der Hoeven et al., 2020); for neural network optimization, the hyperparameters can be the learning rate of the optimizer (Defazio and Mishchenko, 2023; Carmon and Hinder, 2022; Iygi et al., 2023; Cutkosky et al., 2024; Khaled and Jin, 2024); for model selection, the hyperparameters are the choice of the hypothesis class (Foster et al., 2017, 2019).

Surprisingly, the reinforcement learning (RL) community has overlooked the concept of parameter-free learning almost entirely. To the best of our knowledge, the only related work is from Chen and Zhang (2024), where the authors proposed an algorithm that adapts to the scale of the losses in the setting of adversarial MDPs. In this work we focus on the problem of developing parameter-free RL algorithms where the parameter to be focused on are those related to the environment transition, particularly the state space. Almost all RL algorithms assume knowledge of the state-space. For example, existing UCB-based reinforcement learning algorithms (Azar et al., 2017; Jin et al., 2018; Zanette and Brunskill, 2019; Zhang et al., 2020, 2021) make use of the state space size to construct the UCB bonus. When the state space is unknown, it is unclear whether these algorithms can still build a valid UCB bonus that ensures optimism and achieve bounded regrets.

Instance-dependent algorithms:_Instance-dependent learning_ is a closely related concept to parameter-free learning. Instance-dependent algorithms dynamically adjust to the input data they find, and achieve a regret that not only scaling with the number of iterations \(T\), but also adapt to certain "measures of hardness" of the environment. Such algorithms perform better than the worst-case regret if the environment is "benign". In reinforcement learning, the most common "measures of hardness" considered in the community are _Variance_(Zanette and Brunskill, 2019; Zhou et al., 2023; Zhang et al., 2023; Zhao et al., 2023) and _Gap_(Simchowitz and Jamieson, 2019; Xu et al., 2021; Dann et al., 2021; Jonsson et al., 2020; Wagenmaker et al., 2021; Tirinzoni et al., 2021), both related to the reward of the environment. Specifically, variance-dependent algorithms provide regret bounds that scale with the underlying conditional variance of the \(Q^{*}\) function. Gap-dependent algorithms provide regret bounds of order \(}( T/(s,a))\) where the gap notion is defined as the difference of the optimal value function and the \(Q^{*}\)-function at a sub-optimal action \(V^{*}(s)-Q^{*}(s,a)\)(Dann et al., 2021). Additionally, some studies consider problems similar to ours, that is, how to adapt to the "measure of hardness" of the state space. Given an initial state space, Fruit et al. (2018) proposes an algorithm that adapts to the size of the reachable state space, resulting in improved performance when the initial state space is vacuous.

The difference between instance-dependent algorithm and parameter-free algorithm is subtle. Both family of algorithms have the capability to adapt to the input data, allowing them to sequentially tune the hyperparameters and ultimately converge to the optimal hyperparameters inherent in the data. Consequently, when the number of iterations becomes sufficiently large, both instance-dependent algorithms and parameter-free algorithms tend to provide the same theoretical guarantees. However, this does not mean that the two types of algorithms are the same. The most significant difference is that instance-dependent algorithms require appropriate **hyper-parameters initialization**. Taking state-space adaptability as an example. Let \(N\) represent the true number of states. An instance-dependent algorithm must be provided with an initial value \(M N\). If this value is invalid, i.e., \(M<N\), the algorithm will fail to function properly. Moreover, the regret of instance-dependent algorithms is typically related to the initial input, even though this dependency may fade away as the number of iterations increases. This is also why we cannot simply set \(M\) to infinity in an instance-dependent algorithm and call it parameter-free, that is, the regret of an instance-dependent algorithm always includes some burn-in terms that scale with \(M\). As \(M\) goes to infinity, the burn-in term eventually dominates. In this sense, parameter-free learning is a strictly harder problem than instance-dependent learning.

## 3 Problem Formulation

**Markov Decision Process:** This paper focuses on the episodic MDP setting with finite horizon, unknown transition, and bandit feedback. A MDP is defined by a tuple \(=(,,H,P)\), where \(=\{1,,S\}\) denotes the state space, \(=\{1,,A\}\) denotes the action space, and \(H\) denotes the planning horizon. \(P:S A S\) is an unknown transition function where \(P(s^{}|s,a)\) is the probability of reaching state \(s^{}\) after taking action \(a\) in state \(s\). For every \(t[T]\), we define \(_{t}:\) as the loss function. In stochastic MDPs, the loss function \(_{t}\) is drawn from a time-independent distribution. In adversarial MDPs, the loss function \(_{t}\) is determined by the adversary, which can depend on the player's actions before \(t\). The learning proceeds in \(T\) episodes. In each episode \(t\), the learner starts from state \(s_{1}\) and decides a stochastic policy \(_{t}:\) with \(_{t}(a|s)\) being the probability of taking action \(a\) in state \(s\). Afterwards, the learner executes the policy in the MDP for \(H\) steps and observes a state-action-loss trajectory \((s_{1},a_{1},_{t}(s_{1},a_{1}),,s_{H},a_{H},_{t}(s_{H},a_{H}))\) before reaching the end state \(s_{H+1}\). With a slight abuse of notation, we assume \(_{t}()=[_{h[H]}_{t}(s_{h},a_{h})|P,]\). The performance is measured by the regret, which is defined by

\[(T)=_{t=1}^{T}_{t}(_{t})-_{}_{t=1}^{T} _{t}().\]

Without loss of generality, we consider a layered-structure MDP: the state space is partitioned into \(H+2\) horizons \(S_{0},,S_{H+1}\) such that \(S=_{h=1}^{H}S_{h}\), \(=S_{i} S_{j}\) for every \(i j\), \(S_{0}=\{s_{0}\}\) and \(S_{H+1}=\{s_{H+1}\}\).

**Occupancy measure:** Given the transition function \(P\) and a policy \(\), the occupancy measure \(q:\) induced by \(P\) and \(\) is defined as

\[q^{P,}(s,a)=_{h=1}^{H}(s_{h}=s,a_{h}=a|P,).\]

Using occupancy measures, the MDP problem can be interpreted in a way that makes it similar to Multi-armed Bandit (MAB) because for any policy \(\), the loss can be expressed as

\[_{t}()=_{s[S]}_{a[A]}q^{P,}(s,a)_{t}(s,a)= q ^{P,},_{t}.\]

Using this formula the regret can be written as \((T)=_{t=1}^{T} q^{P,_{t}}-q^{P,_{}},_{t}\).

**State-free RL:** We say a state \(s\) is _reachable_ to a policy set \(\) if there exists a policy \(\) such that \(q^{P,}(s)>0\). We further define \(^{}=\{s|_{}q^{P,}(s)>0\}\) to represent all the reachable states to \(\) in \(\). The formal definition _state-free_ algorithm is proposed below.

**Definition 3.1**.: _(State-free algorithm): We say a RL algorithm is state-free if given any policy set \(\), the regret bound for the algorithm can be adaptive to \(|^{}|\) and independent to \(||\), without any knowledge of the state space a priori._

At first glance, designing state-free algorithms appears straightforward: if the learner had access to the transition \(P\), it can compute \(_{}q^{P,}(s)\) for every state \(s\) and then remove all the unreachable states, thereby reducing the state space \(\) to \(^{}\). Through this reduction, any existing MDP algorithm can be made state-free. However, such a method is infeasible since \(P\) is always unknown in practice. Without the knowledge of \(P\), it becomes challenging or even impossible to determine whether a state is reachable or not. In the following section, we elaborate on the technical challenges of the problem for both stochastic and adversarial loss settings.

Technical challenges

In this section, we explain the technical challenges for the state-free learning. Specifically, we consider a weakened setup. We assume for a moment that the algorithm has access to the state space \(\) but not the reachable space \(^{}\). It is clear that this setup is weaker than the state-free definition, as in the state-free setting, the information about \(\) is also unknown.

We start with the stochastic setting, where the loss function \(_{t}\) is sampled by a time-independent distribution for all \(t[T]\). As the most prominent setting in RL research, numerous works have since been devoted to improving the regret guarantee and the analysis framework (Brafman and Tennenholtz, 2003; Kakade, 2003; Jaksch et al., 2010; Azar et al., 2017; Jin et al., 2018; Dann et al., 2017; Zanette and Brunskill, 2019; Bai et al., 2019; Zhang et al., 2020, 2021; Menard et al., 2021; Li et al., 2021; Domingues et al., 2021). Surprisingly, although existing works have not mentioned the state-free concept explicitly, we find that some algorithms can almost achieve state-free learning without algorithmic modifications. In particular, we have

**Proposition 4.1**.: _For stochastic MDPs, UCBVI(Azar et al., 2017) is a weakly state-free algorithm, that is, with only the knowledge of \(\), the regret guarantee of UCBVI is adaptive to \(|^{}|\) and independent to \(||\), except in the logarithmic terms._

Proposition 4.1 offers some positive insights into existing algorithms. The source of the log-dependence on \(||\) is straight-forward: the analysis of RL algorithms needs to ensure that concentration inequalities hold for all states with probability at least \(1-\). At this point, since the events among the states are independent from each other, the algorithms have to take a union bound across the state space to make concentration holds simultaneously in all states, which implies that the confidence level \(\) needs to be divided by \(||\). This leads to a regret guarantee that scale with \((||)\).

**Remark 4.2**.: _In Appendix A, we propose a simple technique to get rid of the log-dependence on \(||\) under the UCBVI framework. The key is to allocate the confidence for each visited \((s,a)\) pairs sequentially, instead of applying a uniform confidence allocation across all states in \(||\). We further show that such a method removes the need of \(\) information in the algorithm design. Based on this method, it is suffices to conclude that (a modified version of) UCBVI is a state-free algorithm._

We now turn our attention to the adversarial setting. In adversarial MDPs, the loss is determined by the adversary and can be depend on previous actions. Adversarial MDPs have been studied extensively in recent years Jin et al. (2019); Dai et al. (2022); Lee et al. (2020); Luo et al. (2021). Given the positive results for stochastic MDPs, one might hope that existing adversarial MDP algorithms can naturally achieve a state-free regret guarantees. Unfortunately, this is not the case. In particular, we have the following observation.

**Observation 4.3**.: _(Informal) In adversarial MDPs, using the existing algorithms and analysis framework, the regret guarantee cannot escape a polynomial-level dependence on \(||\)._

Here we briefly explain Observation 4.3. In all prior works on adversarial MDPs, the analysis relies on bounding the gap between the approximation transition function \(\) and the true one \(P\), i.e., \(_{s^{}}\|(|s,a)-P(|s, a)\|_{1}\). In this case, for any state \(s^{}\), regardless of whether \(s^{}\) is reachable or not, the estimation error \(|(s^{}|s,a)-P(s^{}|s,a)|\) may remain non-zero for all \((s,a)\) pairs. Consequently, the larger the \(||\), the greater the inaccuracy of the transition estimation. At this point, one may wonder if the learner can directly set \((s^{}|s,a)=0\) for all unvisited \(s^{}\), so that \(|(s^{}|s,a)-P(s|s,a)|\) is always zero when \(s^{}\) is unreachable. However, since the learner does not have the knowledge of \(P\), it is impossible to determine whether a state is unreachable, even if the learner has never visited the state before. In this regard, if \(s^{}\) is actually reachable, the transition estimator will become invalid. Such dilemma constitutes the main challenge of the problem.

The above offers some high-level intuitions into the complexity of designing state-free algorithms. In the next section, we introduce our new algorithms for state-free RL that operate without prior knowledge of \(\).

## 5 Black-box reduction for State-free RL

In this section, we outline the main contribution of the paper. To generalize our results further, we denote the \(\)-reachable state space as \(^{,}=\{s|_{}q^{P,}(s)>\}\). By definition \(^{}=^{,0}\). Our algorithm SF-RL is illustrated in Algorithm 1. The algorithm maintains a pruned state space, denoted by \(^{}\), which includes all the identified \(\)-reachable states and \(H\) additional auxiliary states. Throughout \(t=1,,T\), SF-RL first obtains the policy \(_{t}^{}^{}:^{}()\) from a black-box adversarial MDP algorithm, namely ALG, which operates on \(^{}\). Then, by playing an arbitrary action on states not in \(^{}\) compatible with \(\), it extends the pruned policy \(_{t}^{}\) to \(_{t}\), which is defined over the domain \(\), and then receives the trajectory \(o_{t}\) after playing \(_{t}\). Given the trajectory, if there exists a new state \(s^{}\) that can be confirmed to be \(\)-reachable, the algorithm will update \(^{}\) and restart the ALG subroutine. Otherwise, SF-RL pretends that the trajectory was produced only by interacting with \(^{}\), and sends the pruned trajectory \(o_{t}^{}\) back to ALG.

The key novelty of the algorithm lies in the design of the pruned space \(^{}\) and trajectory \(o_{t}^{}\). For every \(h[H]\), the auxiliary state \(s_{h}^{}\) represents the collection of states \(_{h}_{h}^{}\). These behave as "absorbing" states, coalescing all the transitions to states not in \(^{}\). Given the pruned space \(^{}\) and \(o_{t}\), we can build the pruned trajectory \(o_{t}^{}=\{s_{h}^{},a_{h}^{},_{t}^{}(s_{h}^{ },a_{h}^{})\}_{h[H]}\). Specifically, \(o_{t}^{}\) can be split into two parts based on the horizon that first encounters the state not in \(^{}\), i.e., \(h=_{h}\{s_{1:h}^{}\}\). For the state-action-loss pairs before the split horizon, we set \(o_{t}^{}\) to be the same as in \(o_{t}\). Otherwise, we let the states to be the corresponding auxiliary states, the actions to be \(^{}\), which represents an arbitrary action, and the loss to be zero. An illustration of the design is provided in Figure 1.

In order to analyze the performance of our black-box SF-RL algorithm we assume the input ALG comes equipped with a regret bound,

**Assumption 5.1**.: _(Regret guarantee for black-box algorithm ALG): With probability \(1-\), for all \(K>0\), the regret guarantee for ALGfollowing \(K\) epochs of interaction with MDP \(=(,,H,P)\) is bounded by_

\[^{}}(K) reg(||,| |,H,(H||||K/)),\]

_where \(reg(||,||,H,)\) is a coefficient that depends polynomially on \(||,||,H\) and \((H||||K/)\). Moreover, we assume that the coefficient \(reg\) is non-decreasing as \(||,||,H,K,1/\) increase._

This definition works for most algorithms in both stochastic and adversarial environments, e.g., for stochastic MDPs, by setting ALG as UCBVI Azar et al. (2017), the coefficient can be set as \((H|||(||||K/ )})\); for adversarial MDPs, by setting ALG as UOB-REPS Jin et al. (2019), the coefficient can be set as \((H|||(||||K/ )})\). If \(reg()\) is the regret coefficient function for input algorithm ALG, the regret bound for Algorithm 1 satisfies

Figure 1: An illustration of the mapping between the state space \(\) and the pruned space \(^{}\). The left side represents the original state space \(\), where grey nodes denote the states in \(^{}\) and red nodes denote the others. The right side is the corresponding pruned space \(^{}\), where blue nodes denote the auxiliary states \(\{s_{h}^{}\}_{h[H]}\). Given the structure, for any trajectory in space \(\) (purple arrows), we can find a dual trajectory (yellow and green arrows) in the pruned space.

**Theorem 5.2**.: _With probability \(1-\), the state-free algorithm SF-RL achieves regret bound 1_

\[(T)(reg(|^{,}|+H,| |,H,(H|^{,}|||T/ ))^{,}|T+ H|^{ }|T}).\]

As shown in Theorem 5.2, the regret bound consists of two terms. The first term is \(^{,}|}\) times the regret of the black-box algorithm ALG over \(T\) iterations, while the second term can be considered as the regret incurred by the barely reachable states we have disregarded. The trade-off between these two terms is reasonable because it is impossible to discard states that are not \(\)-reachable without incurring any cost. By setting \(=0\), Theorem 5.2 immediately provides a regret bound adaptive to the unknown state size \(|^{}|\)2. Additionally, we remark that SF-RL does not require any prior knowledge about the state space in the algorithm design, which means that SF-RL is state-free by design. Below we discuss the main steps in establishing the above result.

Proof Highlight:We first define \(P^{}:^{}^{} \) be the underlying transition function on the pruned space \(^{}\). Specifically, for every \(h[H]\), we set

\[P^{}(s^{}|s,a) =P(s^{}|s,a), (s,a,s^{})^{}_{h}\{s^{ }_{h}\}^{}_{h+1}\{s^{ }_{h+1}\}\] \[P^{}(s^{}|s,a) =1-_{s^{}^{}_{h+1}\{s^{ }_{h+1}\}}P(s^{}|s,a), (s,a,s^{})^{}_{h}\{s^{ }_{h}\}\{s^{}_{h+1}\}\] \[P^{}(s^{}|s,a) =1\{s^{}=s^{}_{h+1}\}, (s,a,s^{}) s^{}_{h} ^{}_{h+1}.\]

Similarly, we define \(^{}_{t}:^{}\) to be the loss function on the pruned space \(^{}\), which satisfies that

\[^{}_{t}(s,a)=\{_{t}(s,a),&s\{s^{ }_{h}\}_{h[H]}\;,\;(s,a)^{} \\ 0,&.\]

Note that the tuple \(^{}=(^{},,H,P^{})\) is a well-defined MDP. In what follows we use the subscript \(t\) to represent the estimators of the objects above at the beginning of epoch \(t\), e.g., \(^{}_{t},P^{}_{t}\). The key lemma of the proof is the following.

**Lemma 5.3**.: _It suffices to consider \(o^{}_{t}\), which is the pruned trajectory corresponding to \(o_{t}\), as an instance by executing policy \(^{}_{t}\) on the pruned space \(^{}\) with transition function \(P^{}_{t}\) and loss \(^{}_{t}\)._

Lemma 5.3 reveals how the black-box algorithm ALG can work. By Assumption 5.1, in order to make the regret independent to \(||\), we let ALG perform on the pruned MDP \(^{}\) instead of \(\). However, since \(^{}\) is actually a "virtual" MDP, we cannot account for ALG's interaction with it. To ensure that ALG can be updated correctly, in Lemma 5.3 we show the pruned trajectory \(o^{}_{t}\) can be viewed as a trajectory from executing policy \(^{}_{t}\) on \(^{}\) and \(^{}_{t}\). Denote the optimal in-hindsight policy as \(_{}=_{}_{t=1}^{T}_{t},\) and let \(^{}_{}\) be the corresponding policy on the pruned space, we start by the regret decomposition below.

\[(T)=^{T} q^{P^{}_{t},^{ }_{t}}-q^{P^{}_{t},^{}_{}},^{}_{t}}_{1}+ ^{T} q^{P,_{t}}-q^{P,_{}},_{t} -_{t=1}^{T} q^{P^{}_{t},^{}_{t}}-q^{P^{} _{t},^{}_{}},^{}_{t}}_{2}.\]

Here, term 1 represents ALG's regret and term 2 corresponds to the sum of the error incurred by the difference between \(\) and \(^{}\).

Bounding 1: Let intervals \(_{1},,_{M}\) be a partition of \([T]\), such that \(P^{}_{t}=P^{}_{(m)}\) for all \(t_{t}\). We can rewrite the regret as

\[=_{m=1}^{M}_{t_{m}}  q^{P^{}_{(m)},^{}_{t}}-q^{P^{}_{(m)},^{}_{t}}, ^{}_{t}.\]Since \(_{m=1}^{}/2m^{2}\), using Lemma 5.3 and Assumption 5.1, \(\) can be bounded below with probability at least \(1-\).

\[_{m=1}^{M}reg(|_{(m)}^{}|,||,H, (2m^{2}H|_{(m)}^{}||||_{m}|/ ))_{m}|}.\]

Now we continue the proof by bounding \(M\) and \(_{(m)}^{}\). As in SF-RL, a state \(s\) will be added in the pruned space if it satisfies \(_{j=1}^{t}_{j}\{s\}/2-(2H^{2}t^{2}/)-1/2>  t\). Such a design ensures that all states added in \(S^{}\) are at least \(\)-reachable, which is formalized in the following lemma.

**Lemma 5.4**.: _With probability \(1-\), for every state \(s\), it will be added in \(^{}\) only if the state is \(\)-reachable, i.e., \(_{}q^{,}(s)>\)._

By Lemma 5.4, it suffices to say that \(|_{(m)}^{}||^{,}|+H\) for all \(m[M]\) and \(M|^{,}|\), as the states not in \(^{,}\) cannot be added in \(^{}\). This result also implies that ALG can be restarted at most \(|^{,}|\) times, thus \(M|^{,}|+1\). Given the above, we can finally bound

\[ reg(|^{,}|+H,||,H,(2H( |^{,}|+H)^{3}||T/))_{m =1}^{M}_{m}|}\]

\[ reg(|^{,}|+H,||,H,(2H(| ^{,}|+H)^{3}||T/))^{,}|T}.\]

**Bounding 2**: The proof relies on the following lemma.

**Lemma 5.5**.: _Given the pruned space \(^{}\) and the corresponding transition \(P^{}\), for any policy \(\), there is_

\[0 q^{P,},_{t}- q^{P^{},^{}}, _{t}^{} H_{s^{}}q^{P,}(s) \{s^{}\}.\]

Using Lemma 5.5, we immediately have

\[(_{t=1}^{T} q^{P,_{t}},_{t}-_{t=1}^{ T} q^{P^{},^{}_{t}},_{t}^{}) H _{t=1}^{T}_{s}q^{P,_{t}}(s)_{t}\{s ^{}_{t}\}.\]

It then suffices to bound the right hand side of the inequality. Denote by \(X_{t}=_{s}_{t}\{s\}_{t}\{s ^{}_{t}\}\). By definition, we have \(X_{t}[0,H]\) and \([X_{t}|_{t-1}]=_{s}q^{P,}(s) _{t}\{s^{}_{t}\}\). Using Lemma B.1 in the appendix, with probability \(1-\), we have

\[ 2H_{s}_{t=1}^{T}_{t}\{s\}_ {t}\{s^{}_{t}\}+2H^{2}().\]

As in SF-RL, if a state has been visited \(2 t+2(2H^{2}T^{2}/)+2\) times, the state will be added in \(^{}\), which means \(_{j}\{s\}_{j}\{s^{}_{j}\}\) will be \(0\) for the rest \(j>t\). This implies that \(_{t=1}^{T}_{t}\{s\}_{t}\{s^{}_{ t}\}\) is at most \(2 T+2(2H^{2}t^{2}/)+2\) for all \(s\). Moreover, if a state is not reachable by any policy in \(\), we always have \(_{t=1}^{T}q^{P,}(s)_{t}\{s^{}_{t}\}=0\). Therefore, we can conclude that \( 2 H|^{}|T+2H^{2}|^{}|(2H^{2} T^{2}/)+2|^{}|+2H^{2}(1/)\). Finally, realizing that we have conditioned on the events stated in Assumption 5.1, Lemma 5.4 and Lemma C.1, which happens with probability at least \(1-3\). By combining \(\) and \(\) and rescaling \(\), we complete the proof.

**Remark 5.6**.: _Interestingly, the SF-RL framework can be extended to build horizon-free and action-free algorithm, that is, algorithms that do not require the horizon length (when the horizon length is variable) and action space as input parameters. Specifically, given \(^{}\), we denote \(H^{}\) by the maximum horizon corresponding to the states in \(^{}\{s^{}_{h}\}_{h=1}^{M}\), which represents the maximum horizon index among identified \(\)-reachable states. We further denote \(^{}\) by the actions corresponding to \(^{}\{s^{}_{h}\}_{h=1}^{H}\). When \(^{}\) is updated, we let ALG restart with hyper-parameters \((^{}_{1:H^{}},^{},H^{},P^{}_{1:H ^{}})\), where \(^{}_{1:H^{}}\) and \(P^{}_{1:H^{}}\) represent the states and transitions within the first \(H^{}\) horizons of \(^{}\) and \(^{}\). By using the sub-trajectory of \(o^{}_{t}\) within the first \(H^{}\) horizons as the trajectory input of ALG, it suffices to note that Lemma 5.3 still holds, thereby the black-box reduction also works. With such extension, SF-RL requires no hyper-parameter from the environment and can be regarded as completely parameter-free._```
1:Input: action space \(\), horizon \(H\), black-box algorithm ALG, confidence \(\), pessimism level \(\)
2:for\(t=1\)to\(T\)do
3: Receive policy \(_{t}^{}:^{}()\) from ALG
4: Derive \(_{t}:()\) such that \(_{t}(|s)=\{_{t}^{}(|s),&s ^{}\\ ^{}(|s),&.\)
5: Play policy \(_{t}\), receive trajectory \(o_{t}=\{s_{h},a_{h},_{t}(s_{h},a_{h})\}_{h[H]}\)
6:if\( s o_{t},s.t.,s^{},_{j=1}^{t}_{j} \{s\}/2-(2H^{2}t^{2}/)/2-1/2> t\)then
7: Update \(^{}=^{}\{s:_{j=1}^{ t}_{j}\{s\}/2-(2H^{2}t^{2}/)/2-1>  t\}\)
8: Update policy set \(^{}=\{^{}(|s)(|s),&s  S^{}\{s_{h}\}_{h[H]}\\ ^{}(|s)\{a^{}\},&.\)
9: Restart ALG with state space \(^{}\), action space \(\), policy set \(^{}\) and confidence \(^{}|^{2}}\)
10:else
11: Derive the pruned trajectory \(o_{t}^{}=\{s_{h}^{},a_{h}^{},_{t}^{}(s_{h}^{ },a_{h}^{})\}_{h[H]}\) such that \[s_{h}^{}=\{s_{h},&s_{1:h}^{ }\\ s_{h}^{},&.\] \[;_{t}^{}(s_{h}^{},a_{h}^{})=\{ _{t}(s_{h},a_{h}),&s_{1:h}^{}\\ a^{},&.\] \[;_{t}^{}(s_{h}^{},a_{h}^{})=\{ _{t}(s_{h},a_{h}),&s_{1:h}^{}\\ 0,&.\]
12: Send the pruned trajectory \(o_{t}^{}\) to ALG
13:endif
14:endfor ```

**Algorithm 1** Black-box Reduction for State-free RL (SF-RL)

## 6 Improved regret bound for State-free RL

In the previous section, we introduce a black-box framework SF-RL that transforms any existing RL algorithm into a state-free RL algorithm. However, the regret guarantee for SF-RL is suboptimal: compared to ALG itself, SF-RL incurs an \(^{,}|}\) multiplicative term to the regret bound. This is mainly because SF-RL needs to restart the black-box algorithm ALG whenever \(^{}\) updates. Such a restarting strategy inevitably leads to the loss of the learned MDP model. For this reason, and in order to achieve optimal regret rates, we need to design state-free algorithms that do not lose model information. In this section, we introduce a novel approach that enables SF-RL to retain previous transition information after restarting ALG. We illustrate that such a method improves the regret guarantee of SF-RL by a \(^{,}|}\) term for adversarial MDPs when combined with a specific choice of ALG. This bound matches the best known regret bound for adversarial MDPs given known state space.

In existing adversarial MDP algorithms, the model information is captured within the confidence set of transition functions. Take Jin et al. (2019) as an example. For epoch \(t 1\), let \(N_{t}(s,a)\) and \(M_{t}(s^{}|s,a)\) be the total number of visits of pair \((s,a)\) and \((s,a,s^{})\) before epoch \(t\). The confidence set of Jin et al. (2019) is defined as

\[_{t}=\{:|(s^{}|s,a)-_{t}(s^{ }|s,a)|_{t}(s^{}|s,a),\;(s,a,s^{}) _{h}_{h+1},\; h\},\]

where \(_{t}(s^{}|s,a)=M_{t}(s^{}|s,a)/(1,N_{t}(s,a))\) is the empirical transition function for epoch \(t\) and \(_{t}(s^{}|s,a)\) is the confidence width defined as

\[_{t}(s^{}|s,a)=2_{t}(s,a)(|||}{})}{\{1,N_{t}(s,a)-1\}}}+|||}{})}{3\{1,N_{t}(s, a)-1\}}.\]

As in Lemma 2 of Jin et al. (2019), by empirical Bernstein inequality and a union bound, one can establish that \(P_{t}\) for all \(t>0\) with probability at least \(1-\).

Intuitively, such a construction of confidence set tends to be overly conservative for our state-free setup. On the one hand, it requires taking a union bound over all \((s,a,s^{})\) pairs, resulting in an inevitable log-dependence on \(||\). On the other hand, even if state \(s^{}\) is unreachable, the confidence width \(_{t}(s^{}|s,a)\) is not zero for all \((s,a)\). Furthermore, since SF-RL operates within the pruned space \(^{}\), it is necessary to construct the confidence set on \(S^{}\) instead of \(S\). Given by these observations, we propose a new construction of the confidence sets. For every \(s\), we denote \(t(s)\) by the epoch index when the algorithm first accesses to state \(s\). If a state \(s\) has not been visited, we define \(t(s)=\). Without of loss generality, we denote by \(t(s,s^{})=\{t(s),t(s^{})\}\) the index when both \(s\) and \(s^{}\) are reached. Denote \(_{t}^{t^{}}(s^{}|s,a)=(M_{t}(s^{}|s,a)-M_{t^{}} (s^{}|s,a))/\{1,N_{t}(s,a)-N_{t^{}}(s,a)\}\) be the partial empirical transition function corresponding to epochs \([t^{}+1,t]\). We further define \(_{t}^{}\) as the states visited before \(t\). For every \(t[T]\), we build \(_{t}^{}\) such that

\[_{t}^{}=^{}:&^{}(s^{ }|s,a)_{t}(s^{}|s,a),(s,a,s^{}) _{h}^{}\{s_{h}^{}\} _{h+1}^{}\{s_{h+1}^{}\}, h\\ &^{}(s^{}|s,a)=\{s^{}=s_{h+1}^{}\}, (s,a,s^{})\{s_{h}^{} \}_{h+1}^{}, h\]

where \(_{t}(s^{}|s,a)=_{t}^{1}(s^{}|s,a) _{t}^{2}(s^{}|s,a)\). \(_{t}^{1}(s^{}|s,a)\) and \(_{t}^{2}(s^{}|s,a)\) are two confidence intervals defined by

\[_{t}^{1}(s^{}|s,a)=[_{t}^{t(s,s^{})}(s^{ }|s,a)_{t}^{1}(s^{}|s,a)],\;_{t}^{2}(s ^{}|s,a)=\{[0,\;_{t}^{2}(s^{}|s,a) ]&t(s^{}) t(s)+1\\ &,\]

\[_{t}^{1}(s^{}|s,a)=4_{t}^{t(s,s^{})}(s ^{}|s,a)(t/(s,a,s^{}))}{\{N_{t}(s,a )-N_{t(s,s^{})}(s,a)-1,1\}}}+))}{\{N_{t}(s,a)-N_{t(s,s^{})}(s,a)-1,1\}},\]

\[_{t}^{2}(s^{}|s,a)=_{t(s^{})}^{}|+ 24(t/(s,a))}{\{N_{t(s^{})}(s,a)-1,1\}},\]

Here, \(_{t}^{1}(s^{}|s,a)\) and \(_{t}^{2}(s^{}|s,a)\) are two Bernstein-type confidence intervals. Let us explain the high-level ideas of the design. First, to avoid wasting confidence on unreachable states, we initialize the confidence level of \(_{t}^{1}(s^{}|s,a)\) only if both \(s\) and \(s^{}\) are visited. The probability parameter \((s^{}|s,a)\) is \(_{t(s,s^{})}\)-measurable, because it only depends on the data before epoch \(t(s,s^{})\). Thus, in order to avoid correlation, we can only use the data collected after epoch \(t(s,s^{})+1\) to construct the confidence interval \(_{t}^{1}(s^{}|s,a)\). This leads to a problem: when \(t(s^{})\) is much greater than \(t(s)\), we drop too much data that could be used to estimate \(P(s^{}|s,a)\), resulting in \(_{t}^{1}(s^{}|s,a)\) being loose compared to the existing confidence interval designed in Jin et al. (2019). To address this issue, we introduce the second confidence interval \(_{t}^{2}(s^{}|s,a)\). The logic behind the estimator \(_{t}^{2}(s^{}|s,a)\) is that \(t(s^{}) t(s)\) when the probability \(P(s^{}|s,a)\) is very small and therefore \(N_{t(s^{})}(s,a)-1\) can be used to certify an upper bound to \(P(s^{}|s,a)\). The confidence level of \(_{t}^{1}(s^{}|s,a)\) can only be determined after epoch \(t(s^{})\), whereas the confidence interval \(_{t}^{2}(s^{}|s,a)\) is constructed based on data between \(t(s)\) and \(t(s^{})\). By combining \(_{t}^{1}(s^{}|s,a)\) and \(_{t}^{2}(s^{}|s,a)\), such a construction makes use of all the data after \(t(s)\) to ensure a tight confidence interval. Considering that \(t(s)\) is the first time \(s\) is reached, we essentially lose only one data point, which is acceptable. By carefully designing the confidence level \((s,a,s^{})\) and \((s,a)\), one can show that

**Lemma 6.1**.: _Let \(i(s)\) be the index of state \(s\) sorted by the arriving time. By setting \((s,a)=||}\) and \((s,a,s^{})=+i(s^{})^{4})||}\), with probability at least \(1-\), there is \(P_{t}^{}_{t}^{}\) for all \(t[T]\)._

Lemma 6.1 shows that such a construction of the confidence set is valid. Based on the new confidence set, we show that the regret bound of SF-RL can be improved by taking \(_{t}^{}\) as an additional input to the black-box algorithm ALG. We summarize the result as follows.

**Theorem 6.2**.: _(Informal) By initializing ALG as UOB-REPS Jin et al. (2019) and taking \(_{t}^{}\) as an additional input to ALG every epoch, with probability \(1-\), the state-free algorithm SF-RL achieves regret bound_

\[(T)(H|^{,}||T(^{}||T}{})}+ H| ^{}|T),\]

_which matches the best existing result of non-state-free algorithms for adversarial MDPs._Conclusion

This paper initiates the study of state-free RL, where the algorithm does not require the information of state space as a hyper-parameter input. Our framework SF-RL allows us to transform any existing RL algorithm into a state-free RL algorithm through a black-box reduction. Future work includes extending the framework SF-RL from the tabular setting to the setting with function approximation.