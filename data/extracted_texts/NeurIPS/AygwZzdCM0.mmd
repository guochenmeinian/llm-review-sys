# Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model

Valentyn Melnychuk, Dennis Frauen & Stefan Feuerriegel

LMU Munich & Munich Center for Machine Learning (MCML)

Munich, Germany

melnychuk@lmu.de

###### Abstract

Counterfactual inference aims to answer retrospective "what if" questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called _Curvature Sensitivity Model_. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual identification methods are special cases of our _Curvature Sensitivity Model_ when the bound of the curvature is set to zero. We then propose an implementation of our _Curvature Sensitivity Model_ in the form of a novel deep generative model, which we call _Augmented Pseudo-Invertible Decoder_. Our implementation employs (i) residual normalizing flows with (ii) variational augmentations. We empirically demonstrate the effectiveness of our _Augmented Pseudo-Invertible Decoder_. To the best of our knowledge, ours is the first partial identification model for Markovian structural causal models with continuous outcomes.

## 1 Introduction

Counterfactual inference aims to answer retrospective "what if" questions. Examples are: _Would a patient's recovery have been faster, had a doctor applied a different treatment? Would my salary be higher, had I studied at a different college?_ Counterfactual inference is widely used in data-driven decision-making, such as root cause analysis , recommender systems , responsibility attribution , and personalized medicine . Counterfactual inference is also relevant for various machine learning tasks such as safe policy search , reinforcement learning , algorithmic fairness , and explainability .

Counterfactual queries are located at the top of Pearl's ladder of causation , i. e., at the third layer \(_{3}\) of causation  (see Fig. 1, right). Counterfactual queries are challenging as they do reasoning in both the actual world and a hypothetical one where variables are set to different values than they have in reality.

State-of-the-art methods for counterfactual inference typically aim at _point identification_. These works fall into two streams. (1) The first stream  makes no explicit assumptions besides assuming a structural causal model (SCM) with Markovianity (i. e., independence of the latent noise) and thus gives estimates that can be _invalid_. However, additional assumptions are needed in counterfactual inference of layer \(_{3}\) to provide identifiability guarantees . (2) The second stream  provides such identifiability guarantees but makes strong assumptions that are _unnatural_ or _unrealistic_. Formally, the work by Nasr-Esfahany et al.  describes bijective generation mechanisms (BGMs), where, in addition to the original Markovianity of SCMs, the functions in the underlying SCMs must be monotonous (strictly increasing or decreasing) with respect to the latent noise. The latter assumption effectively sets the dimensionality of the latent noise to the same dimensionality as the observed (endogenous) variables. However, this is highly unrealistic in real-world settings and is often in violation of domain knowledge. For example, cancer is caused by multiple latent sources of noise (e.g., genes, nutrition, lifestyle, hazardous exposures, and other environmental factors).

In this paper, we depart from point identification for the sake of more general assumptions about both the functions in the SCMs and the latent noise. Instead, we aim at _partial counterfactual identification_ of continuous outcomes. Rather than inferring a point estimation expression for the counterfactual query, we are interested in inferring a whole _ignorance interval_ with _informative bounds_. Informative bounds mean that the ignorance interval is a strict subset of the support of the distribution. The ignorance interval thus contains all possible values of the counterfactual query for SCMs that are consistent with the assumptions and available data. Partial identification is still very useful for decision-making, e. g., when the ignorance interval for a treatment effect is fully below or above zero.

We focus on a Markovian SCM with two observed variables, namely, a binary treatment and a continuous outcome. We consider a causal diagram as in Fig. 1 (left). We then analyze the _expected counterfactual outcome of [un]treated_ abbreviated by ECOU [ECOT]. This query is non-trivial in the sense that it can _not_ be simplified to a \(_{1}/_{2}\) layer, as it requires knowledge about the functions in SCM, but can still be inferred by means of a 3-step procedure of abduction-action-prediction . ECOU [ECOT] can be seen as a continuous version of counterfactual probabilities  and allows to answer a retrospective question about the necessity of interventions: _what would have been the expected counterfactual outcome for some treatment, considering knowledge about both the factual treatment and the factual outcome?_

In our paper, we leverage geometric measure theory and differential topology to prove that, in general, the ignorance interval of ECOU [ECOT] has non-informative bounds. We show theoretically that this happens immediately when we relax the assumptions of (i) that the latent noise and the outcome have the same dimensionality and (ii) that the functions in the SCMs are monotonous (and assume they are continuously differentiable). As a remedy, we propose a novel _Curvature Sensitivity Model_ (CSM), in which we bound the curvature of the level sets of the functions and thus yield informative bounds. We further show that we obtain the BGMs from  as a special case when setting the curvature to zero. Likewise, we yield non-informative bounds when setting it to infinity. Therefore, our CSM provides a sufficient condition for the partial counterfactual identification of the continuous outcomes with informative bounds.

Figure 1: Pearl’s ladder of causation  comparing observational, interventional, and counterfactual queries corresponding to the SCM \(\) with two observed variables, i. e., binary treatment \(A\{0,1\}\) and continuous outcome \(Y\). We also plot three causal diagrams, \(()\), corresponding to each layer of causation, namely, Bayesian network, causal Bayesian network, and parallel worlds network. Queries with gray background can be simplified, i. e., be expressed via lower-layer distributions. The estimation of the queries with yellow background requires additional assumptions or distributions from the same layer. In this paper, we focus on partial identification of the expected counterfactual outcome of [un]treated, \((Y_{a} a^{},y^{}),a^{} a\), shown in orange.

We develop an instantiation of CSM in the form of a novel deep generative model, which we call _Augmented Pseudo-Invertible Decoder_ (APID). Our APID uses (i) residual normalizing flows with (ii) variational augmentations to perform the task of partial counterfactual inference. Specifically, our APID allows us to (1) fit the observational/interventional data, (2) perform abduction-action-prediction in a differentiable fashion, and (3) bound the curvature of the SCM functions, thus yielding informative bounds for the whole ignorance interval. Finally, we demonstrate its effectiveness across several numerical experiments.

Overall, our **main contributions** are following:1

1. We prove that the expected counterfactual outcome of [un]treated has non-informative bounds in the class of continuously differentiable functions of SCMs.
2. We propose a novel _Curvature Sensitivity Model_ (CSM) to obtain informative bounds. Our CSM is the first sensitivity model for the partial counterfactual identification of continuous outcomes in Markovian SCMs.
3. We introduce a novel deep generative model called _Augmented Pseudo-Invertible Decoder_ (APID) to perform partial counterfactual inference under our CSM. We further validate it numerically.

## 2 Related Work

We briefly summarize prior works on (1) point and (2) partial counterfactual identification below but emphasize that none of them can be straightforwardly extended to our setting. We provide an extended literature overview in Appendix A.

**(1) Point counterfactual identification** has been recently addressed through neural methods  but without identifiability results.

To ensure identifiability, prior works usually make use of (i) symbolic identifiability methods or (ii) put restrictive assumptions on the model class, if (i) led to non-identifiability (see Fig. 2). (i) Symbolic (non-parametric) identifiability methods  aim to provide a symbolic probabilistic expression suitable for point identification if a counterfactual query can be expressed via lower-layer information only. Examples of the latter include the effect of treatment of the treated (ETT)  and path-specific effects . However, these are _not_ suited for partial counterfactual identification.

Alternatively, identifiability can be achieved by (ii) making restrictive but unrealistic assumptions about the SCMs or the data generating mechanism . A notable example is the BGMs , which assumes a Markovian SCM where the functions in the SCMs must be monotonous (strictly increasing or decreasing) with respect to the latent noise. The latter assumption effectively sets the dimensionality of the latent noise to the same dimensionality as the observed (endogenous) variables, yet this is unrealistic in medicine. As a remedy, we depart from _point_ identification and instead aim at _partial_ counterfactual identification, which allows us to _relax_ the assumptions of BGMs.

Kilbertus et al.  build sensitivity models for unobserved confounding in semi-Markovian SCMs, but still assume a restricted functional class of SCMs, namely, an additive noise model . We, on the other hand, build a sensitivity model around the extended class of functions in the SCMs, which is _non-trivial_ even in the Markovian SCMs.

**(2) Partial counterfactual identification** has been studied previously, but only either for (i) _discrete_ SCMs , or for (ii) specific counterfactual queries with _informative bounds_. Yet, these (i) do _not_ generalize to the continuous setting; and (ii) are specifically tailored for certain queries with _informative bounds_, such as the variance of the treatment effects, and, thus, are not applicable to our (non-informative) ECOU [ECOT].

Likewise, it is also _not_ possible to extend partial _interventional_ identification of continuous outcomes  from the \(_{2}\) to \(_{3}\), unless it explicitly assumes an underlying SCM.

**Research gap.** To the best of our knowledge, we are the first to propose a sensitivity model for partial counterfactual identification of continuous outcomes in Markovian SCMs.

Figure 2: Flow chart of identifiability for a counterfactual query.

Partial Counterfactual Identification of Continuous Outcomes

In the following, we derive one of our main results: the ignorance interval of the ECOU [ECOT] has non-informative bounds if we relax the assumptions that (i) both the outcome and the latent noise are of the same dimensionality and that (ii) the functions in the SCMs are monotonous.

### Preliminaries

**Notation.** Capital letters \(A,Y,U\), denote random variables and small letters \(a,y,u\) their realizations from corresponding domains \(,,\). Bold capital letters such as \(=\{U_{1},,U_{n}\}\) denote finite sets of random variables. Further, \((Y)\) is an (observational) distribution of \(Y\); \((Y a)=(Y A=a)\) is a conditional (observational) distribution; \((Y_{A=a})=(Y_{a})\) an interventional distribution; and \((Y_{A=a} A^{}=a^{},Y^{}=y^{})=(Y_{a} a^{},y^{})\) a counterfactual distribution. We use a superscript such as in \(^{}\) to indicate distributions that are induced by the SCM \(\). We denote the conditional cumulative distribution function (CDF) of \((Y a)\) by \(_{a}(y)\). We use \((Y=)\) to denote a density or probability mass function of \(Y\) and \((Y)= y\ (y)\) to refer to its expected value. Interventional and counterfactual densities or probability mass functions are defined accordingly.

A function is said to be in class \(C^{k},k 0\), if its \(k\)-th derivative exists and is continuous. Let \(_{2}\) denote the \(L_{2}\)-norm, \(_{x}f(x)\) a gradient of \(f(x)\), and \(_{x}\,f(x)\) a Hessian matrix of \(f(x)\). The pushforward distribution or pushforward measure is defined as a transfer of a (probability) measure \(\) with a measurable function \(f\), which we denote as \(f_{}\).

**SCMs.** We follow the standard notation of SCMs as in [8; 13; 101; 137]. An SCM \(\) is defined as a tuple \(,,(),\) with latent (exogenous) noise variables \(\), observed (endogenous) variables \(=\{V_{1},,V_{n}\}\), a distribution of latent noise variables \(()\), and a collection of functions \(=\{f_{V_{1}},,f_{V_{n}}\}\). Each function is a measurable map from the corresponding domains of \(_{V_{i}}_{V_{i}}\) to \(V_{i}\), where \(_{V_{i}}\) and \(_{V_{i}} V_{i}\) are parents of the observed variable \(V_{i}\). Therefore, the functions \(\) induce a pushforward distribution of observed variables, i.e., \(()=_{}()\). Each \(V_{i}\) is thus deterministic (non-random), conditionally on its parents, i. e., \(v_{i} f_{V_{i}}(_{V_{i}},_{V_{i}})\). Each SCM \(\) induces an (augmented) causal diagram \(()\), which is assumed to be acyclic.

We provide a background on geometric measure theory and differential geometry in Appendix B.

### Counterfactual Non-Identifiability

In the following, we relax the main assumption of bijective generation mechanisms (BGMs) , i. e., that all the functions in Markovian SCMs are monotonous (strictly increasing) with respect to the latent noise variable. To this end, we let the latent noise variables have arbitrary dimensionality and further consider functions to be of class \(C^{k}\). We then show that counterfactual distributions under this relaxation are non-identifiable from \(_{1}\) or \(_{2}\) data.

**Definition 1** (Bivariate Markovian SCMs of class \(C^{k}\) and \(d\)-dimension latent noise).: _Let \((C^{k},d)\) denote the class of SCMs \(=,,(),\) with the following endogenous and latent noise variables: \(=\{U_{A}\{0,1\},U_{Y}^{d}\}\) and \(=\{A\{0,1\},Y\}\), for \(d 1\). The latent noise variables have the following distributions \(()\): \(U_{A}(p_{A})\), \(0<p_{A}<1\), and \(U_{Y}(0,1)^{d}\) and are all mutually independent 2. The functions are \(=\{f_{A}(U_{A}),f_{Y}(A,U_{Y})\}\) and \(f_{Y}(a,) C^{k}\) for \(u_{Y}(0,1)^{d}\  a\{0,1\}\)._

All SCMs in \((C^{k},d)\) induce similar causal diagrams \(()\), where only the number of latent noise variables \(d\) differs. Further, it follows from the above definition that BGMs are a special case of \((C^{1},1)\), where the functions \(f_{Y}(a,)\) are monotonous (strictly increasing).

**Task: counterfactual inference.** Counterfactual queries are at layer \(_{3}\) of the causality ladder and are defined as probabilistic expressions with random variables belonging to several worlds, which are in logical contradiction with each other [8; 27]. For instance, \(^{}(Y_{a}=y,Y_{a^{}}=y),a a^{}\) for an SCM \(\) from \((C^{k},d)\). In this paper, we focus on the _expected counterfactual outcome of [un]treated_ ECOU [ECOT], which we denote by \(Q^{}_{a^{} a}(y^{})=^{}(Y_{a}  a^{},y^{})\).

For an SCM \(\) from \((C^{k},d)\), ECOU [ECOT] can be inferred by the following three steps. (1) The _abduction_ step infers a posterior distribution of the latent noise variables, conditioned on the evidence, i.e., \((U_{Y} a^{},y^{})\). This posterior distribution is defined on the level set of the factual function given by \(\{u_{Y}:f_{Y}(a^{},u_{Y})=y^{}\}\), i. e., all points in the latent noise space mapped to \(y^{}\). (2) The _action_ step alters the function for \(Y\) to \(f_{Y}(a,U_{Y})\). (3) The _prediction_ step is done by a pushforward of the posterior distribution with the altered function, i.e. \(f_{Y}(a,)_{}(U_{Y} a^{},y^{})\). Afterwards, the expectation of it is then evaluated.

The existence of counterfactual queries, which are non-identifiable with \(_{1}\) or \(_{2}\) data in Markovian SCMs, was previously shown in  (Ex. 8, App. D3) for discrete outcomes and in  (Ex. D.7) for continuous outcomes. Here, we construct an important example to (1) show non-identifiability of counterfactual queries from \(_{1}\) or \(_{2}\) data under our relaxation from Def. 1, and to consequently (2) give some intuition on informativity of the bounds of the ignorance interval, which we will formalize later.

**Example 1** (Counterfactual non-identifiability in Markovian SCMs).: _Let \(_{1}\) and \(_{2}\) be two Markovian SCMs from \((C^{0},2)\) with the following functions for \(Y\):_

\[_{1}:f_{Y}(A,U_{Y^{1}},U_{Y^{2}}) =A(U_{Y^{1}}-U_{Y^{2}}+1)+(1-A)(U_{Y^{1}}+U_{Y^{2 }}-1)\},\] \[_{2}:f_{Y}(A,U_{Y^{1}},U_{Y^{2}}) =U_{Y^{1}}+U_{Y^{2}}-1,&A=0,\\ U_{Y^{1}}-U_{Y^{2}}+1,&A=1(0 U_{Y^{1}} 1)(U_{Y^{1}} U _{Y^{2}} 1),\\ F^{-1}(0,U_{Y^{1}},U_{Y^{2}}),&,\]

_where \(F^{-1}(0,U_{Y^{1}},U_{Y^{2}})\) is the solution in \(Y\) of the implicitly defined function \(F(Y,U_{Y^{1}},U_{Y^{2}})=U_{Y^{1}}-U_{Y^{2}}-2(Y-1)\)\(|-U_{Y^{1}}-U_{Y^{2}}+1|-1+(8(Y-1)^{2}+1 )}=0\)._

_It turns out that the SCMs \(_{1}\) and \(_{2}\) are observationally and interventionally equivalent (relative to the outcome \(Y\)). That is, they induce the same set of \(_{1}\) and \(_{2}\) queries. For both SCMs, it can be easily inferred that a pushforward of uniform latent noise variables \(U_{Y^{1}}\) and \(U_{Y^{2}}\) with \(f_{Y}(A,U_{Y^{1}},U_{Y^{2}})\) is a symmetric triangular distribution with support \(_{0}=[-1,1]\) for \(A=0\) and \(_{1}=\) for \(A=1\), respectively. We plot the level sets of \(f_{Y}(A,U_{Y^{1}},U_{Y^{2}})\) for both SCMs in Fig. 3 (left). The pushforward of the latent noise variables preserves the transported mass; i. e., note the equality of (1) the area of each colored band between the two level sets in the latent noise space and (2) the area of the corresponding band under the density graph of \(Y\) Fig. 3 (left)._

_Despite the equivalence of \(_{1}\) and \(_{2}\), the SCMs differ in their counterfactuals; see Fig. 3 (right). For example, the counterfactual outcome distribution of untreated, \(^{}(Y_{1}=y A^{}=0,Y^{}=0)\), has different densities for both SCMs \(_{1}\) and \(_{2}\). Further, the ECOU, \(Q_{0 1}^{}(0)=^{}(Y_{1} A^{}=0,Y^{ }=0)\), is different for both SCMs, i. e., \(Q_{0 1}^{_{1}}(0)=1\) and \(Q_{0 1}^{_{2}} 1.114\). Further details for the example are in Appendix C._

The example provides an intuition that motivates how we generate informative bounds later. By "bending" the bundle of counterfactual level sets (in blue in Fig. 3 left) around the factual level set

Figure 3: Inference of observational and interventional distributions (left) and counterfactual distributions (right) for SCMs \(_{1}\) and \(_{2}\) from Example 1. _Left:_ the observational query \(^{}(Y=y a)\) coincides with the interventional query \(^{}(Y_{a}=y)\) for each \(_{1}\) and \(_{2}\). _Right:_ the counterfactual queries can still differ substantially for \(_{1}\) and \(_{2}\), thus giving a vastly different counterfactual outcome distribution of the untreated, \(^{}(Y_{1} A^{}=0,Y^{}=0)\). Thus, \(_{3}\) queries are non-identifiable from \(_{1}\) or \(_{2}\) information.

(in orange in Fig. 3, right), we can transform more and more mass to the bound of the support. We later extend this idea to the ignorance interval of the ECOU. Importantly, after "bending" the bundle of level sets, we still must make sure that the original observational/interventional distribution is preserved.

### Partial Counterfactual Identification and Non-Informative Bounds

We now formulate the task of partial counterfactual identification. To do so, we first present two lemmas that show how we can infer the densities of observational and counterfactual distributions from both the latent noise distributions and \(C^{1}\) functions in SCMs of class \((C^{1},d)\).

**Lemma 1** (Observational distribution as a pushforward with \(f_{Y}\)).: _Let \((C^{1},d)\). Then, the density of the observational distribution, induced by \(\), is_

\[^{}(Y=y a)=_{E(y,a)}}f_{ Y}(a,u_{Y})\|_{2}}\,^{d-1}(u_{Y}),\] (1)

_where \(E(y,a)\) is a level set (preimage) of \(y\), i. e., \(E(y,a)=\{u_{Y}^{d}:f_{Y}(a,u_{Y})=y\}\), and \(^{d-1}(u_{Y})\) is the Hausdorff measure (see Appendix B for the definition)._

We provide an example in Appendix C where we show the application of Lemma 1 (therein, we derive the standard normal distribution as a pushforward using the Box-Muller transformation). Lemma 1 is a generalization of the well-known change of variables formula. This is easy to see, when we set \(d=1\), so that \(^{}(Y=y a)=_{u_{Y} E(y,a)}|_{u_{Y}}f_{Y} (a,u_{Y})|^{-1}\). Furthermore, the function \(f_{Y}(a,u_{Y})\) can be restored (up to a sign) from the observational distribution, if it is monotonous in \(u_{Y}\), such as in BCMs . In this case, the function coincides (up to a sign) with the inverse CDF of the observed distribution, i. e., \(f_{Y}(a,u_{Y})=_{a}^{-1}( u_{Y} 0.5+0.5)\) (see Corollary 1 in Appendix D).

**Lemma 2**.: _Let \((C^{1},d)\). Then, the density of the counterfactual outcome distribution of the [un]treated is_

\[^{}(Y_{a}=y a^{},y^{})=^{}(Y=y^{} a^{})}_{E(y^{},a^{})} (a,u_{Y})-y)}{\|_{u_{Y}}f_{Y}(a^{},u_{Y})\|_{2}} \,^{d-1}(u_{Y}),\] (2)

_where \(()\) is a Dirac delta function, and the expected counterfactual outcome of the [un]treated, i.e., ECOU [ECOT], is_

\[Q_{a^{} a}^{}(y^{})=^{}(Y_{a}  a^{},y^{})=^{}(Y=y^{}  a^{})}_{E(y^{},a^{})}(a,u_{Y})}{\| _{u_{Y}}f_{Y}(a^{},u_{Y})\|_{2}}\,^{d-1}(u_{Y}),\] (3)

_where \(E(y^{},a^{})\) is a (factual) level set of \(y^{}\), i. e., \(E(y^{},a^{})=\{u_{Y}^{d}:f_{Y}(a^{},u_{Y})=y^{}\}\) and \(a^{} a\)._

Equations (2) and (3) implicitly combine all three steps of the abduction-action-prediction procedure: (1) _abduction_ infers the level sets \(E(y^{},a^{})\) with the corresponding Hausdorff measure \(^{d-1}(u_{Y})\); (2) _action_ uses the counterfactual function \(f_{Y}(a,u_{Y})\); and (3) _prediction_ evaluates the overall integral. In the specific case of \(d=1\) and a monotonous function \(f_{Y}(a,u_{Y})\) with respect to \(u_{Y}\), we obtain two deterministic counterfactuals, which are identifiable from observational distribution, i. e., \(Q_{a^{} a}^{}(y^{})=_{a}^{-1}( _{a^{}}(y^{}) 0.5+0.5)\). For details, see Corollary 3 in Appendix D. For larger \(d\), as already shown in Example 1, both the density and ECOU [ECOT] can take arbitrary values for the same observational (or interventional) distribution.

**Definition 2** (Partial identification of ECOU (ECOT) in class \((C^{k},d),k 1\)).: _Given the continuous observational distribution \((Y a)\) for some SCM of class \((C^{k},d)\). Then, partial counterfactual identification aims to find bounds of the ignorance interval \([ a}(y^{})}, a}(y^{ })}]\) given by_

\[ a}(y^{})}{ a}(y^{ })}} =_{(C^{k},d)}Q_{a^{} a}^{ }(y^{}) \  a\{0,1\}:(Y a)=^{ }(Y a),\] (4) \[ a}(y^{})} =_{(C^{k},d)}Q_{a^{} a}^{ }(y^{}) \  a\{0,1\}:(Y a)=^{ }(Y a),\] (5)

_where \(^{}(Y a)\) is given by Eq. (1) and \(Q_{a^{} a}^{}(y^{})\) by Eq. (3)._

Figure 4: “Bending” the bundle of counterfactual level sets \(\{E(y,a):y\}\) in blue around the factual level set \(E(y^{},a^{})\) in orange.

Hence, the task of partial counterfactual identification in class \((C^{k},d)\) can be reduced to a constrained variational problem, namely, a constrained optimization of ECOU [ECOT] with respect to \(f_{Y}(a,) C^{k}\). Using Lemma 2, we see that, e. g., ECOU [ECOT] can be made arbitrarily large (or small) for the same observational distribution in two ways. First, this can be done by changing the factual function, \(f_{Y}(a^{},)\), i. e., if we increase the proportion of the volume of the factual level set \(E(y^{},a^{})\) that intersects only a certain bundle of counterfactual level sets. Second, this can be done by modifying the counterfactual function, \(f_{Y}(a,)\), by "bending" the bundle of counterfactual level sets around the factual level set. The latter is schematically shown in Fig. 4. We formalize this important observation in the following theorem.

**Theorem 1** (Non-informative bounds of ECOU (Ecot)).: _Let the continuous observational distribution \((Y a)\) be induced by some SCM of class \((C^{},d)\). Let \((Y a)\) have a compact support \(_{a}=[l_{a},u_{a}]\) and be of finite density \((Y=y a)<+\). Then, the ignorance interval for the partial identification of the ECOU [ECOT] of class \((C^{},d)\), \(d 2\), has non-informative bounds: \( a}(y^{})}=l_{a}\) and \( a}(y^{})}=u_{a}\)._

Theorem 1 implies that, no matter how smooth the class of functions is, the partial identification of ECOU [ECOT] will have non-informative bounds. Hence, with the current set of assumptions, there is no utility in considering more general classes. This includes various functions, such as \(C^{0}\) and the class of all measurable functions \(f_{Y}(a,)^{d}\), as the latter includes \(C^{}\) functions. In the following, we introduce a sensitivity model which nevertheless allows us to obtain informative bounds.

## 4 Curvature Sensitivity Model

In the following, we develop our _Curvature Sensitivity Model_ (CSM) to restrict the class \((C^{k},d)\) so that the ECOU [ECOT] obtains informative bounds. Our CSM uses the intuition from the proof of Theorem 1 in that, to construct the non-informative SCM \(_{}\), we have to "bend" the bundle of the counterfactual level sets. As a result, our CSM provides sufficient conditions for informative bounds in the class \((C^{2},d)\) by bounding the principal curvatures of level sets globally.

**Assumption \(\).**_Let \(\) be of class \((C^{2},d),d 2\). Let \(E(y,a)\) be the level sets of functions \(f_{Y}(a,u_{Y})\) for \(a\{0,1\}\), which are thus \(d-1\)-dimensional smooth manifolds. Let us assume that principal curvatures \(i\{1,,d-1\}\) exist at every point \(u_{Y} E(y,a)\) for every \(a\{0,1\}\) and \(y(l_{a},u_{a})_{a}\), and let us denote them as \(_{i}(u_{Y})\). Then, we assume that \( 0\) is the upper bound of the maximal absolute principal curvature for every \(y\), \(a\), and \(u_{Y} E(y,a)\):_

\[=_{a\{0,1\},y(l_{a},u_{a}),u_{Y} E(y,a)}_{i\{1,,d-1\}}|_{i}(u_{Y})|.\] (6)

Principal curvatures can be thought of as a measure of the non-linearity of the level sets, and, when they are all close to zero at some point, the level set manifold can be locally approximated by a flat hyperplane. In brevity, principal curvatures can be defined via the first- and second-order partial derivatives of \(f_{Y}(a,)\), so that they describe the degrees of curvature of a manifold in different directions. We refer to Appendix B for a formal definition of the principal curvatures \(_{i}\). An example is in Appendix C.

Figure 5: Venn diagram of different SCM classes \((C^{k},d)\) arranged by partial vs. point point identification. See our CSM with different \(\). We further show the relationships between different classes of SCMs \((C^{k},d)\) and statements about them, e. g., Lemmas 1 and 2. The referenced examples are in the Appendix C.

Figure 6: Overview of our _Augmented Pseudo-Invertible Decoder_ (APID). Our APID uses (i) two residual normalizing flows for each treatment \(a\{0,1\}\), respectively; and (ii) variational augmentations, \(_{}\). Together, both components enable the implementation of CSM under Assumption \(\).

Now, we state the main result of our paper that our CSM allows us to obtain informative bounds for ECOU [ECOT].

**Theorem 2** (Informative bounds with our CSM).: _Let the continuous observational distribution \((Y a)\) be induced by some SCM of class \((C^{2},d),d 2\), which satisfies Assumption \(\). Let \((Y a)\) have a compact support \(_{a}=[l_{a},u_{a}]\). Then, the ignorance interval for the partial identification of ECOU [ECOT] of class \((C^{2},d)\) has informative bounds, dependent on \(\) and \(d\), which are given by \(Q_{a^{} a}(y^{})=l(,d)>l_{a}\) and \( a}(y^{})}=u(,d)<u_{a}\)._

Theorem 2 has several important implications. (1) Our CSM is applicable to a wide class of functions \((C^{2},d)\), for which the principal curvature is well defined. (2) We show the relationships between different classes \((C^{k},d)\) and our CSM (\(\)) in terms of identifiability in Fig. 5. For example, by increasing \(\), we cover a larger class of functions, and the bounds on ECOU [ECOT] expand (see Corollary 4 in Appendix D). For infinite curvature, our CSM almost coincides with the entire \((C^{2},d)\). (3) Our CSM (\(\)) with \( 0\) always contains both (i) identifiable SCMs and (ii) (informative) non-identifiable SCMs. Examples of (i) include SCMs for which the bundles of the level sets coincide for both treatments, i. e., \(\{E(y,a):y_{a}\}=\{E(y,a^{}):y_{a^{}}\}\). In this case, it is always possible to find an equivalent BGM when the level sets are flat and thus \(=0\) (see \(_{}\) from Example 6 in the Appendix C) or curved and thus \(=1\) (see \(_{}\) from Example 7 in the Appendix C). For (ii), we see that, even when we set \(=0\), we can obtain non-identifiability with informative bounds. An example is when we align the bundle of level sets perpendicularly to each other for both treatments, as in \(_{}\) from Example 8 in the Appendix C. We formalize this observation with the following Lemma.

**Lemma 3** (BgMs-EQTDs identification gap of CSM(\(=0\))).: _Let the assumptions of Theorem 2 hold and let \(=0\). Then the ignorance intervals for the partial identification of ECOU [ECOT] of class \((C^{2},d)\) are defined by min/max between BGMs bounds and expectations of quantile-truncated distributions (EQTDs):_

\[ a}(y^{})/ a}(y^{ })}}=/\{_{+}(y^{}),_{-}(y^{ }),_{i}(y^{}),_{u}(y^{})\}\] (7)

\[_{+}(y^{})=_{a}^{-1}(_{a^{ }}(y^{}))_{-}(y^{})=_{a}^{-1}(1- _{a^{}}(y^{})),\] (8) \[_{l}(y^{})=(Y a,Y<_{a}^{-1}(1-2|0.5-_{a^{}}(y^{})|)),\] (9) \[_{u}(y^{})=(Y a,Y>_{a}^{-1}(2|_{a^{}}(y^{})-0.5|)),\] (10)

_where \((Y Y<),(Y Y>)\) are expectations of truncated distributions._

Lemma 3 shows how close we can get to the point identification after setting \(=0\) in our CSM. In particular, with CSM (\(=0\)) the ignorance interval still contains BGMs bounds and EQTDs, i. e., this is the identification gap of CSM.3 Hence, to obtain full point identification with our CSM, additional assumptions or constraints are required. Notably, we can not assume monotonicity, as there is no conventional notion of monotonicity for functions from \(^{d}\) to \(\).

We make another observation regarding the choice of the latent noise dimensionality \(d\). In practice, it is sufficient to choose \(d=2\) (without further assumptions on the latent noise space). This choice is practical as we only have to enforce a single principal curvature, which reduces the computational burden, i.e.,

\[_{1}(u_{Y})=-_{u_{Y}}(}f_{Y}(a,u _{Y})}{\|_{u_{Y}}f_{Y}(a,u_{Y})\|_{2}}), u_{Y} E(y,a).\] (11)

Importantly, we do not lose generality with \(d=2\), as we still cover the entire identifiability spectrum by varying \(\) (see Corollary 5 in Appendix D). We discuss potential extensions of our CSM in Appendix E.

**Interpretation of \(\)**. The sensitivity parameter \(\) tends to a natural interpretation. Specifically, it can be interpreted as _a level of non-linearity between the outcome and its latent noises that interact with the treatment_. For example, when (i) the treatment does not interact with any of the latent noise variables, then we can assume w.l.o.g. a BGM, which, in turn, yields deterministic counterfactual outcomes. There is no loss of generality, as all the other SCMs with \(d>1\) are equivalent to this BGM (see Examples 6 and 7 in Appendix C). On the other hand, (ii) when the treatment interacts with some latent noise variables, then the counterfactual outcomes become random, and we cannot assume a BGM but we have to use our CSM. In this case, \(d\) corresponds to the number of latent noise variables which interact with the treatment. Hence, \(\) bounds the level of non-linearity between the outcome and the noise variables. More formally, \(\), as a principal curvature, can be seen as the largest coefficient of the second-order term in the Taylor expansion of the level set (see Eq. (19) in Appendix B). This interpretation of the \(\) goes along with human intuition : when we try to imagine counterfactual outcomes, we tend to "traverse" all the possible scenarios which could lead to a certain value. If we allow for highly non-linear scenarios, which interact with treatment, we also allow for more extreme counterfactuals, e. g., interactions between treatment and rare genetic conditions.

## 5 Augmented Pseudo-Invertible Decoder

We now introduce an instantiation of our CSM: a novel deep generative model called _Augmented Pseudo-Invertible Decoder_ (APID) to perform partial counterfactual identification under our CSM (\(\)) of class \((C^{2},2)\).

**Architecture:** The two main components of our APID are (1) residual normalizing flows with (2) variational augmentations (see Fig. 6). The first component are two-dimensional normalizing flows [107; 125] to estimate the function \(f_{Y}(a,u_{Y}),u_{Y}^{2}\), separately for each treatment \(a\{0,1\}\). Specifically, we use residual normalizing flows  due to their ability to model free-form Jacobians (see the extended discussion about the choice of the normalizing flow in Appendix F). However, two-dimensional normalizing flows can only model invertible transformations, while the function \(_{Y}(a,):^{2}_{a}\) is non-invertible. To address this, we employ an approach of pseudo-invertible flows [10; 54], namely, the variational augmentations , as described in the following. The second component in our APID are variational augmentations . Here, we augment the estimated outcome variable \(\) with the variationally sampled \(_{} N(g^{a}(),^{2})\), where \(g^{a}()\) is a fully-connected neural network, and \(^{2}>0\) is a hyperparameter. Using the variational augmentations, our APID then models \(_{Y}(a,)\) through a two-dimensional transformation \(_{a}=(_{Y_{}}(a,),_{Y}(a,)):^{2} _{a}\). We refer to the Appendix F for further details.

**Interference:** Our APID proceeds in first steps: (P1) it first fits the observational/interventional distribution \((Y a)\), given the observed samples; (P2) it then performs counterfactual inference of ECOU [ECOT] in a differentiable fashion, which, therefore, can be maximized/minimized jointly with other objectives; and (P3) it finally penalize functions with large principal curvatures of the level sets, by using automatic differentiation of the estimated functions of the SCMs.

We achieve (P1)-(P3) through the help of variational augmentations: \(\) In (P1), we fit two two-dimensional normalizing flows with a negative log-likelihood (for the task of maximizing or minimizing ECOU [ECOT], respectively). \(\) In (P2), for each normalizing flow, we sample points from the level sets of \(_{Y}(a,)\). The latter is crucial as it follows an abduction-action-prediction procedure and thus generates estimates of the ECOU [ECOT] in a differential fashion. To evaluate ECOU [ECOT], we first perform the _abduction_ step with the inverse transformation of the factual normalizing flow, i.e., \(_{a}^{-1}\). This transformation maps the variationally augmented evidence, i. e., \((y^{},\{Y^{}_{}\}_{j=1}^{b} N(g^{a}(y^{}), ^{2}))\), where \(b\) is the number of augmentations, to the latent noise space.

Then, the _action_ step selects the counterfactual normalizing flow via the transformation \(_{a}\). Finally, the _prediction_ step performs a pushforward of the latent noise space, which was inferred during the abduction step. Technical details are in Appendix F. \(\) In (P3), we enforce the curvature constraint of our CSM. Here, we use automatic differentiation as provided by deep learning libraries, which allows us to directly evaluate \(_{1}(u_{Y})\) according to Eq. (11).

**Training:** Our APID is trained based on observational data \(=\{A_{i},Y_{i}\}_{i=1}^{n}\) drawn i.i.d. from some SCM of class \((C^{2},2)\). To fit our APID, we combine several losses in one optimization objective: (1) a negative log-likelihood loss \(_{}\) with noise regularization , which aims to fit the data distribution; (2) a counterfactual query loss \(_{Q}\) with a coefficient \(_{Q}\), which aims to maximize/minimize the ECOU [ECOT]; and (3) a curvature loss \(_{}\) with coefficient \(_{}\), which penalizes the curvature of the level sets. Both coefficients are hyperparameters. We provide details about the losses, the training algorithm, and hyperparameters in Appendix G. Importantly, we incorporated several improvements to stabilize and speed up the training. For example, in addition to the negative log-likelihood, we added the Wasserstein loss \(_{}\) to prevent posterior collapse [26; 132]. To speed up the training, we enforce the curvature constraint only on the counterfactual function, \(_{Y}(a,)\), and only for the level set, which corresponds to the evaluated ECOU [ECOT], \(u_{Y} E(_{a^{} a}(y^{}),a)\).

## 6 Experiments

**Datasets.** To show the effectiveness of our APID at partial counterfactual identification, we use two synthetic datasets. This enables us to access the ground truth CDFs, quantile functions, and sampling. Then, with the help of Lemma 3, we can then compare our **APID** with the **BGMs-EQUDs identification gap** (= a special case of CSM with \(=0\)). Both synthetic datasets comprise samples from observational distributions \((Y a)\), which we assume to be induced by some (unknown) SCM of class \((C^{2},2)\). In the first dataset, \((Y 0)=(Y 1)\) is the standard normal distribution, and in second, \((Y 0)\) and \((Y 1)\) are different mixtures of normal distributions. We draw \(n_{a}=1,000\) observations from \((Y a)\) for each treatment \(a\{0,1\}\), so that \(n=n_{0}+n_{1}=2,000\). Although both distributions have infinite support, we consider the finite sample minimum and maximum as estimates of the support bounds \([_{1},_{1}]\). Further details on our synthetic datasets are in Appendix H. In sum, the estimated bounds from our APID are consistent with the theoretical values, thus showing the effectiveness of our method.

**Results.** Fig. 7 shows the results of point/partial counterfactual identification of the ECOU, i.e., \(_{0 1}(y^{})\), for different values of \(y^{}\). Point identification with BGMs yields two curves, corresponding to strictly increasing and strictly decreasing functions. For partial identification with our APID, we set \(_{Q}=2.0\) and vary \(_{}\{0.5,1.0,5.0,10.0\}\) (higher values correspond to a higher curvature penalization). We observe that, as we increase \(_{}\), the bounds are moving closer to the BGMs, and, as we decrease it, the bounds are becoming non-informative, namely, getting closer to \([_{1},_{1}]\). We report additional results in Appendix H (e. g., for APID with \(_{Q}=1.0\)).

**Case study with real-world data.** In Appendix I, we provide a real-world case study. Therein, we adopt our CSM to answer "what if" questions related to whether the lockdown was effective during the COVID-19 pandemic.

## 7 Discussion

**Limitations.** Our CSM and its deep-learning implementation with APID have several limitations, which could be addressed in future work. First, given the value of \(\), it is computationally infeasible to derive ground-truth bounds, even when the ground-truth observation density is known. The inference of these bounds would require solving a constrained optimization task including partial derivatives and Hausdorff integration. This is intractable, even for such simple distributions as standard normal. Second, the exact relationship between \(\) and corresponding \(_{}\) is unknown in APID, but they are known to be inversely related. Third, our APID sometimes suffers from computational instability, e. g., the bounds for the multi-modal distribution (see Fig. 7), are inaccurate, i. e., too tight. This happens as the gradual "bending" of the level sets is sometimes numerically unstable during training, and some runs omit "bending" at all as it would require passing through the high loss value region.

**Conclusion.** Our work is the first to present a sensitivity model for partial counterfactual identification of continuous outcomes in Markovian SCMs. Our work rests on the assumption of the bounded curvature of the level sets, yet which should be sufficiently broad and realistic to cover many models from physics and medicine. As a broader impact, we expect our bounds to be highly relevant for decision-making in safety-critical settings.

Figure 7: Results for partial counterfactual identification of the ECOU across two datasets. Reported: BGMs-EQUDs identification gap and mean bounds of APID over five runs.