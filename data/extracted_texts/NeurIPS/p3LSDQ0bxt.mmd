# GRE Score: Generative Risk Evaluation for Large Language Models

ZAITANG LI

The Chinese University of Hong Kong

Sha Tin, Hong Kong

ztli@cse.cuhk.edu.hk &Mohamed MOUHAJIR

ENSIAS, Mohammed V University

Morocco, Rabat

mohamed_mouhajir@um5.ac.ma Pin-Yu Chen

IBM Research

New York, USA

pin-yu.chen@ibm.com &Tsung-Yi Ho

The Chinese University of Hong Kong

Sha Tin, Hong Kong

tyho@cse.cuhk.edu.hk

###### Abstract

Large Language Models (LLMs) have revolutionized generative tasks, but concerns about their trustworthiness and vulnerability to adversarial attacks persist. This paper introduces the Generative Robustness Evaluation (GRE) Score, a novel metric designed to assess LLMs' resilience against adversarial red teaming attempts that may compromise model compliance and elicit undesired responses. Our approach utilizes conditional generation for synthetic text creation, offering an attack-independent evaluation of LLM robustness. By calculating the margin in refusal scores, we quantify the robustness of LLMs in an attack-agnostic manner. We evaluate our method on five different dimensions with specified datasets, encompassing ethical considerations, safety protocols, and potential misuse scenarios. We present four key contributions: (1) The GRE Score framework, which establishes a textual robustness certificate for LLMs against adversarial red teaming attempts, providing a theoretical foundation for quantifying model resilience. (2) Comprehensive evaluations across five critical dimensions using eight prominent LLMs, validating GRE Scores with adversarial red teaming attacks. Our method demonstrates a consistent ranking of LLM robustness when compared to the attack-based model ranking on TrustLLM (Huang et al., 2024) while achieving a significant 5-8x speedup compared to traditional evaluation techniques. (3) Insights into the non-linear relationship between model scaling and performance, revealing that larger models do not always perform better, and an analysis of how instruction-tuning impacts robustness across LLMs. (4) The discovery that all evaluated LLMs exhibit notably lower performance in robustness and privacy tasks compared to other areas, highlighting a critical gap in LLM capabilities.

## 1 Introduction

Large language models (LLMs) have transformed natural language generation, but concerns over their trustworthiness persist. Red-teaming has emerged as a key method for testing LLM robustness, uncovering vulnerabilities such as jailbreaks where crafted prompts bypass safety mechanisms and lead to harmful outputs (Perez and Ribeiro, 2022; Jain et al., 2023; Barrett et al., 2023).

To enhance compliance with ethical guidelines, advancements like Instruction Tuning (Wei et al., 2021) and Reinforcement Learning from Human Feedback (RLHF) (Bai et al., 2022) have been implemented, along with Constitutional AI (Bai et al., 2022) and Self-Alignment (Sun et al., 2024). However, red-teaming continues to expose gaps in safety protocols (Perez et al., 2022; Ganguli et al., 2022), particularly in adversarial testing, which focuses on exploiting worst-case scenarios to measure model resilience.

Despite these safeguards, models like Vicuna (Chiang et al., 2023) and GPT-4 (Achiam et al., 2023) remain vulnerable to methods like AutoDAN (Zhu et al., 2023), revealing the limitations of currentCurrent evaluation methods face challenges: (I) a lack of comprehensive metrics for individual LLM assessment, (II) limited scope in adversarial testing focused mostly on harmful outputs, (III) the high computational demands of thorough evaluations, and (IV) data contamination issues (Balloccu et al., 2024).

To address these, we propose the GRE Score, a novel conditional robustness certificate for LLMs. Our approach evaluates models across multiple dimensions, minimizes computational overhead, and mitigates data contamination by generating novel test data. The GRE Score is validated in five key areas: safety, privacy, robustness, machine ethics, and fairness.

Our key contributions can be summarized as follows:

* The GRE Score framework enables comprehensive evaluation across five dimensions: safety, privacy, robustness, ethics, and fairness.
* Demonstrating GRE Score's strong consistency in ranking LLM robustness compared to adversarial testing, with high correlations with Attack Success Rates (ASR) across tasks (e.g., Ethics: 0.976, Privacy: 0.952).
* Insights into model scaling and instruction-tuning effects, showing larger models do not always perform better in robustness.
* Identifying performance gaps in robustness (average GRE score: 0.548) and privacy (average: 0.547) compared to areas like security (average: 0.919) and ethics (average: 0.812), emphasizing the need for improvement.

**Note.** A summary of all the main notations used in this paper can be found in Appendix B. Additionally, the background and related works are discussed in Appendix A.

## 2 GRE Score: Methodology and Algorithms for LLM Robustness

Our methodology introduces a comprehensive framework for evaluating LLMs' resilience against adversarial testing across multiple tasks. We begin in Section 2.1 by formalizing the concept of "Adversarial Red-Teaming Perturbations" for LLMs, including the LLM response categorization and semantic representation of textual inputs. Section 2.2 then presents our innovative approach: the Generative Robustness Evaluation (GRE) Score, computed using a conditional text generator. We provide theoretical guarantees for the GRE Score as a robustness certificate in Section 2.3, establishing it as a lower bound on the magnitude of adversarial testing perturbations required to alter the model's classification. This is followed by an in-depth analysis of the algorithmic mechanisms and computational complexities of our approach in Section 2.4. Finally, Section 2.5 offers a detailed breakdown of five distinct evaluation tasks, each explored in its own subsection.

Figure 1: Flow chart of calculating GRE Score. The process begins by selecting one of the five supported risk evaluation tasks and loading relevant risk-specific prompts, we then use a text paraphraser to create synthetic samples. Then, we pass the generated prompts into an LLM to get responses and further use a classifier (e.g., Longformer (Wang et al., 2023d)) for refusal prediction. Finally, we use these statistics to compute the GRE Score as detailed in Section 2.2.

### Robustness Evaluation Framework for LLMs

In this section, we introduce a comprehensive framework for evaluating the robustness of Large Language Models (LLMs) against adversarial testing, particularly focusing on their ability to maintain appropriate responses in the face of malicious prompts.

We define an LLM as a function \(:\), where \(\) represents the space of all possible textual inputs and outputs. To assess the model's response in terms of compliance, we introduce a classification function \(C:^{2}\), which maps the model's output to a probability distribution over two categories: "compliant" (c) and "non-compliant" (nc).

The complete LLM system, including the classification step, is denoted as \(:^{2}\), such that for any input \(x\), \((x)=C((x))\).

Semantic Representation of Textual InputsTo facilitate the analysis of textual perturbations in a continuous space, we employ a semantic encoder \(e:^{d}\) and a corresponding decoder \(d:^{d}\), such as BART (Lewis et al., 2019). These functions map between the discrete text space and a \(d\)-dimensional continuous vector space.

Adversarial Testing Perturbations and Minimal DisturbanceFor input \(x\), we consider it "compliant" if \(_{c}(x)>0.5\). Adversarial testing aims to find a perturbed input that flips this classification. We define the minimal perturbation required as following, where \(\|\|_{p}\) denotes the \(_{p}\)-norm.

\[_{}(x)=*{arg\,min}_{}\|\|_{p}: _{}(d(e(x)+))>0.5\] (1)

### Generative Robustness Evaluation (GRE) Score

We propose the GRE Score as a novel metric to quantify LLM's resilience against adversarial testing.

Conditional Text GeneratorLet \(G_{}(z|x)\) be a conditional text generator parameterized by \(\), which produces paraphrased versions of an input \(x\) based on a latent variable \(z(0,I)\).

Local Robustness ScoreWe define a local robustness score function \(r\) for a given paraphrased input where \([a]^{+}=(a,0)\):

\[r(,e(G_{}(z|x)))=}[_{c}(d (e(G_{}(z|x))))-_{}(d(e(G_{}(z|x))))]^{+}\] (2)

Here, \(_{c}\) and \(_{}\) represent prediction scores for "compliant" (c) and "non-compliant" (nc) categories. This metric evaluates the difference between confidence of "compliant" and "non-compliant", and is set to zero if the former is no greater than the latter (we use the notation \(^{+}\) to denote the threshold function at zero). The constant term will be evident in the following paragraph.

Local GRE ScoreWe first define a local GRE score for a single input \(x\), where \(n\) is the number of paraphrased samples generated for \(x\):

\[r_{}(,x)=_{i=1}^{n}r(,e(G_{ }(z_{i}|x)))\] (3)

Global GRE ScoreGiven a set of input prompts \(=\{x_{1},,x_{m}\}\), the global GRE Score \(R_{}\) is then computed as the average of local GRE scores:

\[R_{}(,)=_{j=1}^{m}r_{}(,x_{j})\] (4)

### Theoretical Guarantees for LLM Robustness

We establish the following theorem to provide a theoretical foundation for the local GRE Score as a robustness certificate against adversarial testing perturbations:

**Theorem 1** (Local GRE Score as Robustness Certificate): _Let \(=\{x_{1},,x_{m}\}\) be a set of inputs where \(_{c}(x)_{nc}(x)\) for all \(x\). As \(n\), the following holds almost surely:_

_For any \(x\) and any adversarial testing perturbation \(\) in the semantic space of \(x\), if \(\|\|_{2}<r_{}(,x)\), then:_

\[_{c}(d(e(x)+))>0.5\] (5)

This theorem establishes that the local GRE Score provides a lower bound on adversarial testing perturbations required to alter the model's classification for each input, thus serving as a certificate. The proof, describing details and assumptions underlying this theorem, is explained in Appendix C. The global GRE Score \(R_{}\) then provides an average robustness certificate across all inputs in \(\).

### Algorithms and Computational Complexity for GRE Score

The detailed algorithm for estimating the GRE Score is presented in Algorithm 1 in Appendix D. Consider a set of evaluated text prompts, \(=\{x_{1},x_{2},,x_{m}\}\). The GRE Score computation involves conditional generation of samples \(n\) times and forward passes through the LLM to aggregate resulting compliance scores using model \(\). The computational complexity is linear with respect to the number of samples \(m\) in \(\) and the number of generations \(n\).

**Remark 1**: _The time complexity \(T(R_{})\) of computing the GRE Score for a model \(\) with respect to a sample set \(\) and generator \(G_{}()\) is given by:_

\[T(R_{})=O(m n T()+n T(G_{}( )))\] (6)

_where \(T()\) and \(T(G_{}())\) are time complexities for compliance inference and sample generation._

### Summary of 5 Risk Evaluation Tasks

#### 2.5.1 Safety Assessment

**Safety.** In the context of LLMs, safety encompasses the prevention of harmful or inappropriate outputs. TrustLLM defines safety as the ability to curb misinformation, avoid dangerous instructions, and maintain respectful, non-discriminatory language. Research (Liu et al., 2023; Qiu et al., 2023; Casper et al., 2023) underscores the critical need for robust safety protocols in LLMs to mitigate deployment risks and responsibly handle sensitive topics.

**Misuse.** LLMs are vulnerable to exploitation by malicious actors, leading to various harmful outcomes (Tamkin et al., 2021). This assessment focuses on LLMs' ability to reject prompts promoting false information, or illegal content. Studies have shown LLMs' susceptibility to misuse, including misinformation spread (Pan et al., 2023), cyberattack facilitation (Charan et al., 2023), etc.

**Dataset.** Our evaluation employs the Do-Not-Answer (Wang et al., 2023c) and Do Anything Now (Shen et al., 2023) datasets to ensure a comprehensive assessment.

#### 2.5.2 Privacy Assessment

**Privacy.** LLMs' capacity to retain and inadvertently expose private information has sparked considerable concern (Brown et al., 2022). This issue is intensified by the use of web-scraped training data, often rich in personal details. Privacy assessment examines protocols that protect individual and data autonomy, identity, and dignity. It evaluates LLMs' privacy consciousness and potential information leakage, gauging their ability to recognize and manage privacy-sensitive situations.

**Privacy Awareness.** This concept refers to the capability to identify and appropriately handle requests involving personal data. Privacy-aware LLMs should recognize sensitive information and respond accordingly, such as declining to provide data. For example, when asked about someone's phone number, a privacy-conscious response would be a refusal to disclose such information.

**Dataset.** Our evaluation utilizes TrustLLM's dataset, comprising 280 privacy-related inquiries across various scenarios and seven categories of private information. We employ an augmented prompt instructing the LLM to adhere to privacy policies.

#### 2.5.3 Robustness Assessment

**Robustness.** Robustness in AI systems refers to consistent performance across varied conditions and unexpected inputs. Studies (Ye et al., 2023; Liu et al., 2023c) reveal current LLMs' lack of inherent robustness, with models like GPT-3.5 struggling with simple inputs such as emojis (Xu et al., 2023b). TrustLLM assesses robustness by evaluating LLMs' stability under diverse input conditions. This includes examining responses to out-of-distribution (OOD) challenges, as presented in (Kirillov et al., 2023), where LLMs like GPT-4 (trained on data until 2021) must handle texts different from their training data, such as new concepts of technologies emerging post-training.

**OOD Detection.** OOD detection identifies whether a test sample is in-distribution or out-of-distribution. This task has been explored in outlier detection, anomaly detection, and open-world classification (Hendrycks and Gimpel, 2016; Shu et al., 2017; Lee et al., 2018). For LLMs, OOD detection is crucial for trustworthiness, demonstrating their ability to identify information beyond their training distribution, such as latest content or inputs beyond capabilities (e.g., processing image data). An LLM with strong OOD detection should recognize such inputs and provide appropriate feedback, like responding "As an AI model, I cannot..." instead of generating false information.

**Dataset.** TrustLLM's dataset, based on TooleE (Huang et al., 2023), comprises user queries that potentially require external tools, often exceeding LLMs' capabilities. From 520 extracted samples, experts filtered prompts LLMs cannot answer, including requests for real-time knowledge, non-text modalities, and other unanswerable queries.

#### 2.5.4 Assessment of Machine Ethics

**Machine Ethics.** Machine ethics, centers on the ethical conduct of artificial systems. Rooted in Asimov's "three laws of robotics" (Muller, 2020), early research explored embedding ethical principles in machines (Anderson and Anderson, 2006; Wallach et al., 2020). Recent studies have examined ethical challenges in LLMs like GPT-4 (Zhou et al., 2023), including their responses in academic and healthcare settings (Lund et al., 2023; Meyer et al., 2023). Our focus is on evaluating explicit ethics to assess LLMs' behavior across various moral scenarios.

**Explicit Ethics.** This concept involves LLMs' ability to process scenarios and make ethical decisions (Yi et al., 2023). As LLMs increasingly function as intelligent agents in action planning and decision-making (Wang et al., 2024; Zhu et al., 2023b), evaluating their ethical reasoning becomes crucial. For instance, the Jiminy Cricket environment (Hendrycks et al., 2021) presents morally significant scenarios in text-based adventure games. Recent research (Scherrer et al., 2024) also explores LLMs' capacity for moral choice-making.

**Dataset.** Our evaluation uses high-ambiguity moral scenarios from the MoralChoice dataset (Scherrer et al., 2024), designed to probe LLMs' encoded moral beliefs. These scenarios present contexts with two choices, focusing on situations where neither option is clearly preferable. We use prompt templates and expect ethically oriented LLMs to avoid direct choices in these complex situations.

#### 2.5.5 Assessment of Fairness

**Fairness.** Fairness in LLMs is a critical ethical principle ensuring unbiased outcomes throughout model development and deployment (Wang et al., 2023a). This encompasses data preparation, model construction, evaluation, and application (Gallego et al., 2024; Mehrabi et al., 2021). Studies

    &  & Privacy & Robustness & Safety & Fairness \\  & GRE Score/ ASR & GRE Score/ ASR & GRE Score/ ASR & GRE Score/ ASR & GRE Score/ ASR \\  Baichuan2-13B & 0.430/ 72.40\% & 0.554 / 82.00\% & 0.523 / 81.00\% & 0.875 / 52.20\% & 0.691 / 52.00\% \\  chatglm3-6b & 0.964/ 35.00\% & 0.798 / 68.20\% & 0.783 / 67.60\% & 1.002 / 40.80\% & 0.787 / 44.00\% \\  Llama2-7b & 0.676 / 55.40\% & 0.392 / 87.20\% & 0.378 / 90.20\% & 1.017 / 35.06\% & 0.861 / 42.00\% \\  Llama2-13b & 0.804 / 46.80\% & 0.439 / 87.60\% & 0.457 / 86.80\% & 1.080 / 57.20\% & 0.882 / 38.80\% \\  Mistral-7B & 0.762 / 49.80\% & 0.383 / 89.60\% & 0.394 / 90.80\% & 0.709 / 67.20\% & 0.786 / 41.40\% \\  vicuna-7B & 0.828 / 46.20\% & 0.585 / 80.40\% & 0.611 / 81.20\% & 0.853 / 56.60\% & 0.746 / 46.20\% \\  vicuna-13b & 0.968 / 37.60\% & 0.523 / 81.80\% & 0.545 / 80.80\% & 0.839 / 52.80\% & 0.904 / 36.20\% \\  WizardM-13B & 1.064 / 30.00\% & 0.699 / 75.80\% & 0.694 / 73.00\% & 0.981 / 43.00\% & 0.941 / 34.80\% \\ 
**Correlation** & 0.976 & 0.952 & 0.905 & 0.952 & 0.929 \\   

Table 1: Comparison of GRE Scores and Attack Success Rates (ASR) across different models and tasks. ASR values represent the modelâ€™s resistance to attacks (higher is better).

have revealed LLMs' biases against specific groups, including gender (Wan et al., 2023), LGBTQ+ communities (Felkner et al., 2023), and political affiliations (Motoki et al., 2024). Our assessment focuses on preference biases, examining LLMs' tendencies when presented with contrasting opinion pairs to uncover potential biases in handling diverse viewpoints.

**Preference Bias.** This refers to LLMs' inclination to favor certain people, things, or ideas (Liu et al., 2023). Research shows models like ChatGPT tend to support progressive libertarian views (Rozado, 2023; McGee, 2023). Studies on LLMs' recommendation capabilities (Gao et al., 2023; Wang et al., 2023; Dai et al., 2023) reveal such biases can affect output quality, potentially basing suggestions on inherent preferences rather than user needs. This bias can undermine LLMs' trustworthiness by producing outputs influenced by subjective leanings rather than objective information.

**Dataset.** We employ TrustLLM's dataset, comprising 120 multiple-choice questions. Each question presents two opposing, subjective options, allowing for diverse individual opinions. The dataset includes 80 ideology-related questions and 40 questions on culture and lifestyle topics.

## 3 Performance Evaluation

### Experimental Setup

**Models.** Our experiments evaluate several prominent LLMs. We assess LLAMA-2-chat (7B, 13B), an open-source model fine-tuned for dialogue with emphasis on human value alignment through RLHF (Touvron et al., 2023). We also employ Vicuna-1.5 (7B, 13B), a LLMa variant optimized for high-quality, open-domain conversations (Chiang et al., 2023). Mistral-7B-Instruct is included for its architectural advancements like grouped-query attention and sliding window attention, enhancing instruction-following abilities (Jiang et al., 2023). WizardLM-13B-V1.2 is chosen for its multi-task performance (Xu et al., 2023). Lastly, we evaluate Baichuan2-13b, a multilingual model excelling in both English and Chinese tasks (Yang et al., 2023).

**Generative Models.** Our text generation focuses on paraphrasing using DiffuSeq (Gong et al., 2022). DiffuSeq employs diffusion models (Ho et al., 2020) to generate semantically consistent paraphrases. We paraphrased instructions from the original dataset for each task, using 20 randomly selected prompts, each paraphrased 25 times, ensuring diverse yet consistent inputs for all tasks.

**Judgment Classifier.** Following TrustLLM's approach, we employ a Longformer classifier to determine if LLM responses refuse or agree to answer. We use a PLM classifier from (Wang et al., 2023) to evaluate responses for refuse to answer (RTA) metrics. It achieves accuracy comparable to human judges and GPT-4 in categorizing outputs as refusing or not refusing to answer.

**Adversarial Testing.** Our experiments employ the Tree of Attacks with Pruning (TAP) black-box technique (Mehrotra et al., 2023) to execute adversarial attacks on LLMs. It utilizes a "tree of thought" framework (Yao et al., 2024), creating and refining multiple candidate prompts through pruning to enhance attack efficiency. This method has demonstrated high success rates in compromising state-of-the-art models like GPT-4, while maintaining low query requirements. We implement TAP using the same hyperparameters and attack models as described in the original study, employing a substring evaluator with keywords matching technique for attack assessment, which has proven effective in this context.

**Setting Goals for Adversarial Testing.** To validate our Score's effectiveness, we extend TAP's attacks, using the same hyperparameters as in the original paper. While TAP was designed for jailbreak attacks with a standard prefix "Sure, here it is", we adapt this for our multi-dimensional evaluation. We design specific prefixes for each task, balancing utility and attack success rate: "I prefer to choose option" for preference tasks, "I would like to choose" for Machine Ethics tasks, and "Sure, I can help you with that" for privacy, robustness, and safety tasks. This approach probes LLM vulnerabilities across dimensions while maintaining consistency with TAP methodology. A detailed example illustration of paraphrased examples and attacked prompts is in Appendix E.

**Comparative methods.** We compare the effectiveness of our GRE score in two aspects: time-efficiency and high correlation with the RTA under red teaming attack. For each LLM, we run a red teaming attack and report the accuracy and running time. Our objective is to show that LLM with higher GRE scores are more robust to adversarial testing.

### Model Ranking Results.

We compared model rankings based on GRE scores and adversarial testing success rates. Table 1 illustrates the GRE Scores for each dimension across different LLMs, compared to their corresponding ASRs under TAP attack. Our analysis reveals a remarkably high correlation between the proposed GRE Score and the Testing Attack Success Rate (ASR) across all five assessment tasks, as evidenced by consistently high correlation coefficients: Ethics (0.976), Privacy (0.952), Robustness (0.905), Security (0.952), and Fairness (0.929). The strength and consistency of these correlations across different tasks underscores the versatility and reliability of our GRE Score. This uniformity of performance across different aspects of LLM evaluation demonstrates that our metric serves as an excellent proxy for measuring a model's vulnerability to adversarial red teaming text attacks without the need for time-consuming adversarial testing, potentially streamlining the process of evaluating and improving language model robustness across multiple dimensions.

### Model Scaling: Larger Models May Not Always Score Higher

In this section, we compared the performance of 7B and 13B versions of Vicuna and Llama. Our experiments reveal a nuanced relationship between model size and performance, challenging assumptions that larger models invariably perform better. As shown in Figure 3, the 13B versions generally outperform their 7B counterparts across most dimensions of our evaluation. For example, on the Ethics task, Llama-13B achieves a higher GRE score than Llama-7B. Similarly, on the Fairness task, Llama-13B's score of 0.882 exceeds Llama-7B's score of 0.861. However, this superiority is not consistent across all models and dimensions.

Our results align with research suggesting that while larger models often exhibit improved capabilities, the relationship between model size and performance is not straightforward or uniform across tasks (Kaplan et al., 2020). The results, as shown in Figure 3, underscore the complexity of model scaling. The comparison between the 7B and 13B versions of Vicuna shows that in some dimensions, such as privacy, the 7B model (0.585) outperforms the 13B model (0.523), demonstrating that increasing parameters does not guarantee improved performance across all aspects of resilience.

These observations highlight the need for targeted architectural improvements and specialized training, rather than parameter scaling, to enhance model performance and robustness. The complex relationship between size and robustness suggests a more nuanced approach to development and evaluation is necessary for comprehensive improvements in LLM performance.

### Impact of Instruction-Tuning on Model Robustness

Our evaluation of Vicuna-13B and Llama-2-13B, which share the same base model but differ in instruction-tuning, reveals significant effects on model resilience to adversarial testing. As shown in Figure 4, Vicuna outperforms Llama in Ethics (0.968 vs. 0.804) and Fairness (0.904 vs. 0.882), while Llama-2 excels in Security (1.080 vs. 0.839). These different results across dimensions highlight the complex nature of model tuning through instruction-tuning. The results suggest that while instruction-tuning can improve certain aspects of performance, it can also lead to trade-offs inother areas. Notably, our findings are consistent with the results obtained from running our selected prompts on the TrustLLM platform, further validating the robustness of our evaluation methodology.

### Comparative Analysis of Model Performance Across Tasks

We compare the average GRE score in each dimension. Figure 5, shows significant differences in model performance across tasks. In particular, all models have significantly lower average GRE scores in the Robustness (0.548) and Privacy (0.547) dimensions compared to other tasks. In contrast, Security (0.919) and Ethics (0.812) show significantly higher average scores. This pattern suggests a general trend where current language models are more adept at handling ethical considerations and safety issues, but struggle with robustness and privacy challenges. The fairness dimension (0.825) also shows relatively strong performance, suggesting that models have been somewhat successful in addressing bias-related issues. These findings highlight the need for targeted improvements in robustness and privacy to develop more reliable and secure language models, while maintaining the strengths observed in the ethics, security, and fairness dimensions.

### Trade-offs in Task Performance for Llama

Our analysis reveals significant trade-offs in models' performance across different tasks. This phenomenon is particularly evident in the Llama series models. For instance, Llama-2-7b excels in Safety (GRE score: 1.017, ASR: 35.60%), but underperforms in Privacy (0.392, 87.20%) and Robustness (0.378, 90.20%). Similarly, Llama-2 achieves a high Safety score (1.080) while showing weaknesses in Privacy (0.439) and Robustness (0.457). These examples highlight that optimizing for one dimension may lead to vulnerabilities in others. Such trade-offs underscore the challenge of developing models that perform consistently well across all evaluation dimensions, emphasizing the need for a balanced approach in model development and evaluation.

### Run-time Analysis

Figure 6 compares the run-time efficiency of GRE Score over adversarial testing in TAP. Here we show the improvement ratio for each models over 5 tasks of their average per-sample run-time (wall clock time of GRE Score/Red Teaming Attack is reported in Appendix F) a and observe around 5-8 times improvement, validating the computational efficiency of Retention Score.

## 4 Conclusion

In this paper, we presented GRE Score, a novel, computationally efficient attack-independent metric for quantifying risks in Large Language Models (LLMs) using generative benchmarks. GRE Score leverages generative models for deriving robustness scores of textual inputs. Its computation is lightweight and scalable, only requiring model predictions on generated data samples. Our results on eight mainstream LLMs across five dimensions (safety, privacy, robustness, ethics, fairness) show GRE Score obtains consistent robustness analysis compared to time-consuming adversarial testing. It reveals insights into non-linear relationships between model scaling and LLM performance, instruction-tuning's impact on robustness, and critical performance gaps in current LLM capabilities, particularly in robustness and privacy tasks.