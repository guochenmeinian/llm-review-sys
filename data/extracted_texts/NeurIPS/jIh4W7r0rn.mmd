# Autoregressive Image Diffusion: Generation of Image Sequence and Application in MRI

Guanxiong Luo

University Medical Center Gottingen

guanxiong.luo@med.uni-goettingen.de &Shoujin Huang

Shenzhen Technology University

&Martin Uecker

Graz University of Technology

uecker@tugraz.at

###### Abstract

Magnetic resonance imaging (MRI) is a widely used non-invasive imaging modality. However, a persistent challenge lies in balancing image quality with imaging speed. This trade-off is primarily constrained by k-space measurements, which traverse specific trajectories in the spatial Fourier domain (k-space). These measurements are often undersampled to shorten acquisition times, resulting in image artifacts and compromised quality. Generative models learn image distributions and can be used to reconstruct high-quality images from undersampled k-space data. In this work, we present the autoregressive image diffusion (AID) model for image sequences and use it to sample the posterior for accelerated MRI reconstruction. The algorithm incorporates both undersampled k-space and pre-existing information. Models trained with fastMRI dataset are evaluated comprehensively. The results show that the AID model can robustly generate sequentially coherent image sequences. In MRI applications, the AID can outperform the standard diffusion model and reduce hallucinations, due to the learned inter-image dependencies. The project code is available at [https://github.com/mrirecon/aid](https://github.com/mrirecon/aid).

## 1 Introduction

Magnetic resonance imaging (MRI) is a non-invasive imaging modality widely used in clinical practice to visualize soft tissue. Despite its utility, a persistent challenge in MRI is the trade-off between image quality and imaging speed. The trade-off is influenced by the k-space (spatial Fourier domain) measurements, which traverse spatial frequency data points along given sampling trajectories. To reduce acquisition time, the k-space measurements are often undersampled, resulting in image artifacts and reduced image quality.

In recent years, deep learning-based methods have emerged to improve image reconstruction in MRI. These methods are formulated as an inverse problem building upon compressed sensing techniques  and benefit from the learned prior information instead of hand-crafted priors . Another successful approach involves learning an image prior parameterized by a generative neural network , which is then used as the learned and decoupled regularization on the image. Generative priors offer flexibility in handling changes in the forward model and perform well in reconstructing high-quality images from undersampled data.

Diffusion models , a class of generative models, have gained attention in recent years and are making an impact in many fields, including MRI reconstruction . These models learn to reverse a diffusion process that transforms random noise into structured images, producing highquality, detailed images. Various approaches, including denoising diffusion probabilistic models (DDPMs) , denoising score matching , and continuous formulations based on stochastic differential equations (SDEs) , have been proposed for deriving diffusion models.

Recent studies demonstrate the effectiveness of diffusion models in accelerated MRI and their flexibility in handling various sampling patterns [11; 14; 15; 16; 17]. For example, training score-based generative models using Langevin dynamics yields competitive reconstruction results for both in-distribution and out-of-distribution data . Additionally, score-based diffusion models trained solely on magnitude images can reconstruct complex-valued data . Comprehensive approaches using data-driven Markov chains facilitate efficient MRI reconstruction across variable sampling schemes and enable the generation of uncertainty maps .

Autoregressive models are statistical models that predict the current value of a variable based on its past values, capturing temporal dependencies and patterns within the data. They are widely used in various fields such as time series analysis, signal processing, and sequence modeling. In natural language processing, autoregressive models like generative pre-trained transform (GPT) [18; 19] predict each token in a sequence based on previously generated tokens, enabling the generation of coherent and contextually relevant text. Similarly, in image modeling, autoregressive models like PixelCNN  and ImageGPT  generate images by predicting each pixel value based on previously generated pixel values, often in a left-to-right, top-to-bottom order. Instead of directly modeling pixels, which can be computationally expensive for high-resolution images, the study  proposes to first compress the image into a smaller representation using vector quantized variational autoencoder (VQVAE). This VQVAE learns a codebook of visually meaningful image components. Then, a transformer is applied to model the autoregressive relationship between these components, effectively capturing the global structure of the image. By predicting each image component based on previous ones, the model generates high-resolution images in a sequential manner, maintaining consistency and coherence across the entire image.

The clinical practice of MRI often involves acquiring volumetric image sequences to monitor disease progression and treatment response; modeling and generating these image sequences is challenging. Autoregressive models can be employed to model the joint distribution of image sequences and extract the dependencies between images. The diffusion process is effective in modelling images by treating each image independently. Therefore, we aim to combine these two models and propose autoregressive image diffusion (AID) model to generate sequences of images.

The contributions of this work are the following aspects. We present how to derive the autoregressive image diffusion training loss starting from a common diffusion loss and how to optimize loss in parallel for efficient training. We present the algorithm to sample the posterior for accelerated MRI reconstruction when using AID to facilitate the incorporation of pre-existing information. We performed experiments to evaluate its ability in generating images when different the amount of initial information is given and to validate its effectiveness in MRI reconstruction. The results show that the AID model can stably generate highly coherent image sequences even without any pre-existing information. When used as a generative prior in MRI reconstruction, the AID outperforms the standard diffusion model and reduces the hallucinations in the reconstructed images, benefiting from the learned prior knowledge about the relationship between images and pre-existing information.

## 2 Methods

### Autoregressive image diffusion

Given a dataset \(X\) consisting of multiple sequences of images, each sequence represented as \(=\{x_{1},x_{2},,x_{N}\}\), our goal is to model the joint distribution of these images. This joint distribution is autoregressively factorized into the product of conditional probabilities:

\[p()=q(x_{1}|x_{0})_{t=2}^{N}q(x_{n}|x_{<n}), \]where \(x_{<n}=\{x_{1},x_{2},,x_{n-1}\}\) and the image \(x_{0}\) is known. The model parameterized by \(\) is trained by minimizing the negative log-likelihood of the data:

\[_{AID}=_{X}[- p_{}()]= _{X}[- p_{}(x_{1}|x_{0})-_{t=2}^{N} p_{ }(x_{n}|x_{<n})]. \]

Sohl-Dickstein et al. (2015) and Ho et al. (2020) introduced the denoising diffusion probabilistic model (DDPM). This model gradually introduces fixed Gaussian noise to an observed data point \(x^{0}\) using known scales \(_{t}\), generating a series of progressively noisier values \(x^{1},x^{2},,x^{T}\). The final noisy output \(x^{T}\) follows a Gaussian distribution with zero and identity covariance matrix \(I\), containing no information about the original data point. The series of positive noise scales \(_{1},,_{T}\) must be increasing, ensuring that the first noisy output \(x^{1}\) closely resembles the original data \(x^{0}\), while the final value \(x^{T}\) represents pure noise. We apply this process to the conditional probability \(q(x_{n}|x_{<n})\) in Equation (2) by adding the noise to the image independent of the position in the sequence, i.e., \(x^{t}_{n}\) and \(x^{0}_{<n}\) are conditionally independent given \(x^{t-1}_{n}\). Then the transition from \(x^{t-1}_{n}\) to \(x^{t}_{n}\) is defined as:

\[q(x^{t}_{n}|x^{t-1}_{n},x^{0}_{<n})=q(x^{t}_{n}|x^{t-1}_{n})=(x^{t} _{n};}x^{t-1}_{n},_{t}) \]

Here, \(x^{t}_{n}\) represents the image \(x_{n}\) at time \(t\), \(x^{t-1}_{n}\) is the image at the previous time step, and \(x^{0}_{<n}\) denotes all images preceding \(x_{n}\) at the initial time step. The parameter \(_{t}\) controls the drift and diffusion of this process. The objective is to learn to reverse this process. The reverse process is defined as:

\[p_{}(x^{t-1}_{n}|x^{t}_{n},x^{0}_{<n})=(x^{t-1}_{n};_{ }(x^{t}_{n},x^{0}_{<n},t),_{}(x^{t}_{n},x^{0}_{<n},t)), \]

where \(_{}\) and \(_{}\) are parameterized by a neural network \(\), taking \(x^{t}_{n}\), \(x^{0}_{<n}\), and \(t\) as inputs. Using the variational lower bound, the reverse process can be learned by minimizing the negative log-likelihood of the data:

\[[- p_{}(x_{n}|x^{0}_{<n})][- p(x^{T }_{n})-_{t 1}(x^{t-1}_{n}|x^{t}_{n},x^{0}_{<n})}{q(x ^{t}_{n}|x^{t-1}_{n},x^{0}_{<n})}]:=L_{D_{n}}, \]

Figure 1: The interaction between the images in conditioning sequence occurs in the DiTBlock, which has a causal attention module to ensure \(x_{n}\) is conditioned on previous images \(x_{<n}\). During training, the net predicts the noise for each noisy image that is sampled from the target sequence given the conditioning sequence in parallel. During generation, the net iteratively refines the noisy input to produce a clean image, which is then appended to the conditioning sequence.

Given the initial image \(x^{0}_{n}\) and that \(x^{t}_{n}\) and \(x^{0}_{<n}\) are conditionally independent given \(x^{0}_{n}\), \(x^{t}_{n}\) at an arbitrary time step \(t\) is sampled from a Gaussian distribution:

\[q(x^{t}_{n}|x^{0}_{n},x^{0}_{<n})=(x^{t}_{n};_{t}} x^{0}_{n},(1-_{t})), \]

using \(_{t}=1-_{t}\) and \(_{t}=_{s=1}^{t}_{s}\). The posterior distribution \(x^{t-1}_{n}\) given \(x^{0}_{n}\) and \(x^{t}_{n}\) is then calculated as:

\[q(x^{t-1}_{n}|x^{t}_{n},x^{0}_{n},x^{0}_{<n})=(x^{t-1}_{n};_{t}(x^{t}_{n},x^{0}_{n}),_{t}), \]

where \(_{t}(x^{t}_{n},x^{0}_{0}):=-1}_{t}}{1- _{t}}x^{0}_{n}+_{t}}(1-_{t-1}) }{1-_{t}}x^{t}_{n}\) and \(_{t}:=_{t-1}}{1-_{t}}_{t}\).

The training objective Equation (5) is further written as minimizing the Kullback-Leibler (KL) divergence between the forward and reverse processes in Equation (4) and Equation (7), as proposed by Sohl-Dickstein et al. (2015). (See Appendix A for details.)

In practice, the approach proposed by Ho et al. (2020) involves reparameterizing \(_{}\) and predicting the noise \(\) for \(x^{t}_{n}\). The expression for \(x^{t}_{n}\) is given by \(x^{t}_{n}(x^{0}_{n},)=_{t}}x^{0}_{n}+_{t}}\), with \(_{}(x^{t}_{n},x^{0}_{<n},t)=_{t}\) fixed. We realized this with a neural network \(_{}(x^{t}_{n},t,x^{0}_{<n})\) shown in Figure 1, which predicts the noise for \(x^{t}_{n}\) at each time step given \(x^{0}_{<n}\). In the end, the objective function in Equation (2) for training autoregressive image diffusion is written as

\[_{AID}_{n=1}^{N}L_{D_{n}}=_{n=1}^{N}_{t, |x^{0}_{n},x^{0}_{<n}}[\|_{}(_{t}}x^{0}_{n}+_{t}},x^{0}_{<n},t)- \|^{2}_{2}], \]

where the expectation is taken over the noise \((0,I)\) and the time step \(t(1,...,T)\). To generate an image sequence, we begin with the noise \(x^{T}_{1}\) and update it iteratively using Equation (4) with the given \(x^{0}_{0}\), following the sequence \((x^{T}_{1} x^{T-1} x^{0}_{1})\). This process yields a clean sample \(x^{0}_{1}\). Subsequently, we can sample \(x^{0}_{2}\) in the same manner using the generated images \(x^{0}_{<2}\), and continue this process iteratively to generate the entire sequence of images.

### Architecture

To optimize the objective function in Equation (8) efficiently, ordered images are loaded as sequences of a certain length \(N+1\) during the training phase. We take the first \(N\) images \(_{con}=\{x_{0},x_{1},...,x_{N-1}\}\) as the conditioning sequence and the last \(N\) images \(_{target}=\{x_{1},....,x_{N}\}\) as the target sequence, as shown in Figure 1. We adopt an architecture built on an Unet  with capabilities of temporal-spatial conditioning (TSC), designed to process the conditioning sequence and predict the noise for the target sequence. The term "temporal" refers to conditioning in previous frames along the \(N\) dimensions, while the "spatial" refers to the conditioning in the previous frame among the \(H W\) dimensions. Additionally, the TSC block is conditioned on the time steps \(t\) of the diffusion process.

The only interaction between images in the conditioning sequence occurs during the attention operation. To maintain proper conditioning with autoregressive property, we implemented a standard upper triangular mask on the \(n n\) matrix of attention logits. This causal attention module is used in DiTBlock [18; 24]. The modified DiTBlock is followed by a ResNet block , which is a standard building block in the Unet architecture. The features output by the TSC block are then passed to the corresponding encoder block in the Unet, which process the target sequence. The change in tensor dimensions inside TSC Block is handled by the einops library1 and illustrated in Figure 1.

During training, the net predicts the noise in parallel for each noisy image that is sampled from the target sequence, given the conditioning sequence. During generation of sequence, the net iteratively refines the noisy input to produce a clean image, which is then appended to the conditioning sequence.

### Application in MRI inverse problem

Image reconstruction is formulated as a Bayesian problem where the posterior of image \(p(x|y)\) is expressed as

\[p(x|y)=\;. \]Here, \(y\) represents the measured k-space data, \(x\) denotes the image, and \(p(x)\) is a generative prior. The minimum mean square error (MMSE) estimator for the posterior minimizes the mean square error, given by:

\[x_{ MMSE}=_{}\|-x\|^{2}p(x|y)dx=[x|y]\;. \]

### Likelihood function for k-space

The image \(x^{n n}\) is represented as a complex matrix, where \(n n\) is the image size, and \(y^{m m_{C}}\) is a vector of complex-valued k-space samples from \(m_{C}\) receive coils. Assuming circularly-symmetric normal noise \(\) with zero mean and covariance matrix \(_{}^{2}\), the likelihood \(p(y|x)\) of observing \(y\) given \(x\) is formulated as a complex normal distribution:

\[p(y|x) =(y;x,_{}^{2})\] \[=(_{}^{2})^{-N_{p}}e^{\|_{}^{-1} (y-x)\|_{2}^{2}}\;, \]

where \(\) is the identity matrix, \(_{}\) is the standard deviation of the noise, \(x\) represents the mean, and \(N_{p}\) is the length of the k-space data vector. The operator \(:^{n n}^{m m_{C}}\) maps the image \(x\) to k-space and is composed of the coil sensitivity maps \(\), the two-dimensional Fourier transform \(\), and the k-space sampling mask \(\), defined as \(=\). For more details and visual understanding on the forward operator, please refer to Appendix C.

### Sampling the posterior

Given a sequence of k-space \(=\{y_{1},,y_{N}\}\), each posterior in \(\{p_{}(x_{n}|y_{n},x_{<n}^{0})|1<n<N\}\) is expressed as

\[p_{}(x_{n}|y_{n},x_{<n}^{0}) =|x_{n},x_{<n}^{0})p_{}(x_{n}|x_{<n}^{0})}{p(y _{n}|x_{<n}^{0})}=|x_{n})p_{}(x_{n}|x_{<n}^{0})}{p(y_{n})}\] \[ p(y_{n}|x_{n})p_{}(x_{n}|x_{<n}^{0})\;, \]

when the acquisition of \(y_{n}\) is independent of the image \(x_{<n}^{0}\), \(y_{n}\) and \(x_{<n}^{0}\) are conditionally independent given \(x_{n}\). Following the Reference , we have

\[p_{}(x_{n}^{t-1}|x_{n}^{t},y_{n},x_{<n}^{0}) p(y_{n}|x_{n}^{t})p_ {}(x_{n}^{t-1}|x_{<n}^{t},x_{<n}^{0})\;. \]

The details for Equation (13) is in Appendix B. To sample the above posterior, the learned reverse process in Equation (4) is used, and the algorithm is constructed with two gradient updates using the log of the prior and k-space likelihood: the DDIM (Denoising Diffusion Implicit Model) reverse step proposed by Song et al. (2020), and a data fidelity step derived from the likelihood function Equation (11), which are described as follows:

\[_{n}^{t-1} }(^{t}-}_{}(x_{n}^{t},x_{<n}^{0},t)}{}} )+}_{}(x_{n}^{t},x_{<n}^{0},t) \] \[x_{n}^{t-1} _{n}^{t-1}+_{x_{n}^{t-1}}  p(y_{n}|_{n}^{t-1})\;. \]

where \(\) is the step size, and \(_{x_{n}^{t-1}} p(y_{n}|x_{n}^{t-1})\) is the gradient of the log-likelihood of Equation (11). Then, the reconstruction of a sequence images from the undersampled k-space data is achieved by sequentially sampling the posterior in \(\{p(x_{n}|y_{n},x_{<n}^{0})|1<n<N\}\) using autoregressive diffusion model as prior. The algorithm is summarized in Algorithm 1.

```
1:Input:\(\), k-space data \(\), k-space data \(\), k-space data \(\),volumes, with coil sensitivity maps computed using the BART toolbox . The images were then normalized to a maximum magnitude of 1, and the real and imaginary parts were treated as separate channels when input into the neural network. The number of images in each volume ranged from 12 to 16. Images were loaded without reordering and resized to 320\(\)320 pixels if they were not already that size.

The latent space model is trained with the cardiac dataset that contains cine images reconstructed by the SSA-FARY method . Firstly, a VQVAE was trained on the cine images that were preprocessed similarly to images in fastMRI dataset. The cine images have a size of 256\(\)256 pixels. Then, it generates latent space for the training AID. (See the details for configuration of VQVAE in Appendix J). All the training was performed on 4 NVIDIA A100 GPUs with 80GB memory. The models were trained using the Adam optimizer with a learning rate of \(10^{-4}\) and a batch size of 1 for image space model and 4 for latent space model. Two models were trained for 440,000 iterations. It took around 2 hours to train brain model for 10k steps and 1.2 hours for cardiac model. The length of conditioning sequence \(N\) for brain and cardiac models are 10 and 42. The network as illustrated in Figure 1 was implemented based on OpenAI's guided diffusion codebase2. We also trained a standard diffusion model, Guide, on the brain dataset for comparison. The Guide model was trained using the same hyperparameters as the AID model, except the batch size is 10. The Guide model uses the same Unet blocks as AID.

### Generating sequence of images

To test different aspects of the autoregressive diffusion models, we generate the sequence of images using the following two approaches.

**Retrospective sampling**: This method generates a new sequence of images \(\{_{1},,_{N}\}\) based on the given sequence \(\{x_{0},,x_{N-1}\}\). \(_{n}\) is sampled from Equation (4) given \(\{x_{0},,x_{n-1}\}\).

**Prospective sampling**: A fixed-length sliding window is initialized with the given sequence \(x_{<N}=\{x_{0},,x_{N-1}\}\). \(x_{N}\) is generated from Equation (4) with the current window as conditioning. Subsequently, the window is updated by appending the newly generated \(x_{N}\) and removing the earliest image \(x_{0}\). This autoregressive sampling process is repeated until the stop condition is met. We refer to this process as a warm start. In a cold start, the window is initialized with zeros, and each element \(x_{n}\) in it is updated with newly generated images from the beginning to the end, after which the generation is warmed up.

In the retrospective sampling, the model generates a sequence of images that are sequentially coherent and visually similar to the conditioning sequence, as shown in Figure 2 (a). The prospective sampling generates a sequence of images that extends the initial images in the sliding window and constitutes multiple volumes, as shown in Figure 2 (b). As for a cold start, Figure 3 demonstrates the model's ability to generate a sequence of images using black background as initial status. This shows the model's generative capabilities from a minimal initial condition, thereby proving its robustness and flexibility. Due to the limit of space, the samples with similar quality from the model trained on the cardiac dataset are shown in Appendix D. We also implemented a boosted sampling technique which use previous slice with added noise as the initial image for the current slice. This requires less iterations to generate the sequence of images. Further details can be found in our codebase.

### MRI reconstruction

The MMSE estimator in Equation (10) cannot be computed in a closed form, and numerical approximations are typically required. Once the samples from the posterior is obtained with Algorithm 1, a consistent estimate of \(x_{n}\) can be computed by averaging those samples, i.e. the empirical mean of samples converges in probability to \(x_{n}\) due to weak law of large numbers. The variance of those samples provides a solution to the error assessment in the reconstruction assuming the trained model is trusted. To highlight the regions with large uncertainty, we compute the pseudo-confidence intervals based on the assumption that each pixel's intensity is normally distributed. This involves determining the standard error from the variance, then multiplying it by the t-score corresponding to a 95% confidence level.

Figure 3: Prospective samples with cold start. The initial images generated in the cold start are not sequentially coherent, but as the sampling process continues, the model progressively generates more sequentially coherent and realistic images.

Figure 2: (a): A sequence of images from dataset is shown in the first row and is used as conditioning to generate retrospective samples that are shown in the second row. (b): With the given sequence in (a) as a warm start, prospective samples extending it are shown.

**Unfolding of aliased single-coil image:** To investigate how the trained model, AID, reduces the folding artifacts in the reconstruction, we designed the single coil unfolding experiment. The single-channel k-space is simulated out of multichannel k-space data. The odd lines in k-space are retained, \(y\). Ten samples were drawn from the posterior \(p(x_{1}|y,x_{0})\) using Algorithm 1 with parameters: \(T=1000,=1,K=5\). The experiment was repeated using a standard diffusion model, Guide. The results are shown in Figure 4. The AID model significantly reduces the errors in the region of folding artifacts compared to the Guide model. The mean over samples, \(x_{ MMSE}\), is highlighted with a confidence interval computed from the variance of samples. The highlighted mean image shows the reconstruction by AID is more trustworthy in the folding region. In general, the highlighted region lies in the folding region, where large errors remains, as we expected.

**Reconstruction from undersampled data:** To further investigate the model's performance in reconstruction, we conducted experiments on 20 volumes from the fastMRI validation dataset where k-space data was retrospectively undersampled using various sampling masks. We created four types of sampling masks: random with autocalibration signal (ACS), random without ACS, equispaced with ACS, and equispaced without ACS. The undersampling factor is 12. Setting parameters: \(T=1000,=1,K=4\) for Algorithm 1, the images were reconstructed from the undersampled k-space data using the AID and Guide as prior, respectively. Another method proposed in Ref.  is used as the baseline (CSGM), which uses a scored-based model (NCSNv2) from Ref.  trained on the fastMRI dataset. All the reconstruction tasks are performed by sampling the posterior. The likelihood \(p(y|x)\) is determined by forward model and the image prior is determined by the trained models, such as NCSNv2, Guide, and AID. This means that when the sampling method remains consistent, the performance of the reconstruction task is determined by the quality of the image prior. Our algorithm treats \(p(y|x)\) in the same manner, and the key difference is the image prior.

We used peak-signal-noise-ratio (PSNR in dB) and normalized root-mean-square error (NRMSE) to evaluate the reconstruction quality against the reference image that is reconstructed from full k-space. The comparison of metrics across experimental conditions is illustrated in Figure 5. The proposed AID model outperforms the Guide and NCSNv2 in terms of PSNR and NRMSE especially in the absence of ACS, demonstrating its superior performance in image reconstruction from undersampled k-space data. The results are consistent across different undersampling factors and sampling masks, indicating the model's robustness and flexibility in handling various types of undersampled k-space data.

For the visual impression of the improvement by the AID model in reconstruction, we show the reconstructed images in Figure 6 and more of them in Appendix E. The images reconstructed using AID are more visually similar to the reference images than using Guide, even which also provides aliased-free images. Furthermore, it is worth noting that more visually notable hallucinations were introduced by the Guide model than the AID model, which means AID is more trustworthy.

Figure 4: (a): The folded single-coil image caused by two-times undersampling mask. (b): The comparison of unfolding ability by the autoregressive and the standard diffusion model, i.e., AID (top) and Guide (bottom). Reference image is reconstructed from k-space data without undersampling. The error is the difference between the mean, \(x_{ MMSE}\), and the reference image. The “Mean+std” is the mean highlighted with confidence interval, which indicates the reconstruction by AID is more trustworthy in the region of folding artifacts.

### Other models for image sequence generation

To further evaluate the model's ability to generate image sequences, we further trained two AID models on two datasets: one using brain images from autism studies called ABIDE [31; 32], and the other using images from Unmanned Aerial Vehicle (UAV) view dataset . We reported the computation and model complexity in Appendix F for all AID models trained in this work. We also implemented a two-stage training to improve the efficiency for training models on ABIDE and presented correspondingly generated samples in Appendix G. We demonstrated the natural image sequence generation in Appendix H and showed the sample consistency along the temporal axis in Appendix I.

## 4 Discussion

In this work, we propose an autoregressive image diffusion model for generating image sequences, with specific applications to accelerated MRI reconstruction. We conducted comprehensive evaluation of its performance as an image prior in reconstruction algorithms, comparing it to a standard diffusion model. Due to the learned prior information on inter-image dependencies, the proposed model outperforms the standard diffusion model across various scenarios. Our model is particularly well

Figure 5: E: equispaced, R: random. (a): PSNR and (b): NRMSE of the images reconstructed from the twelve-times undersampled k-space data using the autoregressive diffusion model (AID), the standard diffusion model (Guide), and the baseline method CSGM. PSNR higher is better, and NRMSE lower is better.

Figure 6: E: equispaced, R: random. The last column shows the reference and the random sampling mask in k-space. The red lines are autocalibration signal (ACS) and equispaced mask is not shown. Zero-filled images are computed by inverse Fourier transform of the zero-filled k-space data. The hallucinations are pointed with red arrows.

suited for medical applications where image sequences are often acquired (e.g., in volumetric format) from patients in clinical practice. For instance, when different contrast images are acquired during an examination session , our model is designed to capture the relationships between these images. This enables more accurate and coherent reconstructions from undersampled k-space data using the proposed Algorithm 1. Additionally, other medical imaging tasks like dynamic MRI, multi-contrast, super-resolution, and denoising could benefit from our model's ability by leveraging inter-image dependencies . Furthermore, the proposed algorithm holds great promise for facilitating the incorporation pre-existing information from other imaging modalities into MRI image reconstruction. This opens up a wide range of potential medical applications, with the potential to improve patient care and reduce healthcare costs by enabling faster and more accurate image acquisition and diagnosis.

**Privacy Issue:** As this model has the capability to generate coherent image sequences, it is crucial to consider the privacy implications associated with its use, particularly in clinical settings. The generation of such images may inadvertently expose sensitive patient information, including identifiable features such as facial characteristics. Safeguarding patient privacy must be a top priority when deploying it. We recommend that the model be used in a controlled environment where access to the generated images is restricted to authorized personnel only. Additionally, it is essential to ensure that the model is trained on anonymized data and that the generated images are not stored or shared without proper consent.

**Limitation and future work:** We did not evaluate the model on a common image dataset such as ImageNet or Cifar-10, nor did we compute metrics such as FID and Inception Score, which could be a limitation of our work. We plan to address these limitations in future work by running the model on a large dataset and comparing it with other state-of-the-art models. Additionally, given the model's suitability for modeling image sequences, it is worth exploring its potential for optimizing MRI k-space acquisition strategies, as the acquisition process constitutes a sequence of operations.

## 5 Conclusion

The proposed autoregressive image diffusion model offers an approach to generating image sequences, with significant potential as a trustworthy prior in accelerated MRI reconstruction. In various experiments, it outperforms the standard diffusion model in terms of both image quality and robustness by taking the advantage of the prior information on inter-image dependencies.