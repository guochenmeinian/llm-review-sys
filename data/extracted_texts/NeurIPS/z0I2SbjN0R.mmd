# DiffusionPDE: Generative PDE-Solving

Under Partial Observation

 Jiahe Huang\({}^{1}\)   Guandao Yang\({}^{2}\)   Zichen Wang\({}^{1}\)   Jeong Joon Park\({}^{1}\)

\({}^{1}\)University of Michigan

\({}^{2}\)Stanford University

{chloehjh, zzzichen, jjparkcv}@umich.edu

guandao@stanford.edu

###### Abstract

We introduce a general framework for solving partial differential equations (PDEs) using generative diffusion models. In particular, we focus on the scenarios where we do not have the full knowledge of the scene necessary to apply classical solvers. Most existing forward or inverse PDE approaches perform poorly when the observations on the data or the underlying coefficients are incomplete, which is a common assumption for real-world measurements. In this work, we propose _DiffusionPDE_ that can simultaneously fill in the missing information and solve a PDE by modeling the joint distribution of the solution and coefficient spaces. We show that the learned generative priors lead to a versatile framework for accurately solving a wide range of PDEs under partial observation, significantly outperforming the state-of-the-art methods for both forward and inverse directions. See our project page for results: jhhuangchloe.github.io/Diffusion-PDE/.

## 1 Introduction

Partial differential equations (PDEs) are a cornerstone of modern science, underpinning many contemporary physical theories that explain natural phenomena. The ability to solve PDEs grants us the power to predict future states of a system (forward process) and estimate underlying physical properties from state measurements (inverse process).

To date, numerous methods [1; 2] have been proposed to numerically solve PDEs for both the forward and inverse directions. However, the classical methods can be prohibitively slow, prompting the development of data-driven, learning-based solvers that are significantly faster and capable of handling a family of PDEs. These learning-based approaches [3; 4; 5; 6] typically learn a _deterministic_ mapping between input coefficients and their solutions using deep neural networks.

Despite the progress, existing learning-based approaches, much like classical solvers, rely on complete observations of the coefficients to map solutions. However, complete information on the underlying physical properties or the state of a system is rarely accessible; in reality, most measurements are sparse in space and time. Both classical solvers and the state-of-the-art data-driven models often overlook these scenarios and consequently fail when confronted with partial observations. This limitation confines their use primarily to synthetic simulations, where full scene configurations are available by design, making their application to real-world cases challenging.

We present a comprehensive framework, DiffusionPDE, for solving PDEs in both forward and inverse directions under conditions of highly partial observations--typically just 1\(\)3% of the total information. This task is particularly challenging due to the numerous possible ways to complete missing data and find subsequent solutions. Our approach uses a generative model to formulate the joint distribution of the coefficient and solution spaces, effectively managing the uncertainty and simultaneously reconstructing both spaces. During inference, we sample random noise and iterativelydenoise it following standard diffusion models . However, we uniquely guide this denoising process with sparse observations and relevant PDE constraints, generating plausible outputs that adhere to the imposed constraints. Notably, DiffusionPDE can handle observations with arbitrary density and patterns with a single pre-trained generative network.

We conduct extensive experiments to show the versatility of DiffusionPDE as a general PDE-solving framework. We evaluate it on a diverse set of static and temporal PDEs, including Darcy Flow, Poisson, Helmholtz, Burger's, and Navier-Stokes equations. DiffusionPDE significantly outperforms existing state-of-the-art learning-based methods for solving PDEs [3; 4; 5; 6; 8] in both forward and inverse directions with sparse measurements, while achieving comparable results with full observations. Highlighting the effectiveness of our model, DiffusionPDE accurately reconstructs the complete state of Burgers' equation using time-series data from just five sensors (Fig. 4), suggesting the potential of generative models to revolutionize physical modeling in real-world applications.

## 2 Related Works

Our work builds on the extensive literature of three areas: forward PDE solvers, inverse PDE solvers, and diffusion models. Please see relevant surveys for more information [9; 10; 11; 12; 13].

**Forward PDE Solvers.** PDE solvers take the specification of a physics system and predict its state in unseen space and time by solving an equation involving partial derivatives. Since Most PDEs are very challenging to solve analytically, people resolve to numerical techniques, such as Finite Element Method [14; 2] and Boundary Element Method [1; 15]. While these techniques show strong performance and versatility in some problems, they can be computationally expensive or difficult to set up for complex physics systems. Recently, advancements in deep-learning methods have inspired a new set of PDE solvers. Raissi et al. [16; 6] introduce Physics-Informed Neural Networks (PINNs), which optimize a neural network using PDE constraints as self-supervised losses to output the PDE solutions. PINNs have been extended to solving specific fluid [17; 18], Reynolds-averaged Navier-Stokes equations , heat equations , and dynamic power systems . While PINNs can tackle a wide range of complex PDE problems, they are difficult to scale due to the need for network optimization. An alternative approach, neural operators [3; 5], directly learn the mapping from PDE parameters (_e.g._initial and boundary condition) to the solution function. Once trained, this method avoids expensive network optimization and can instantly output the solution result. This idea

Figure 1: We propose DiffusionPDE, a generative PDE solver under partial observations. Given a family of PDE with coefficient (initial state) \(a\) and solution (final state) \(u\), we train the diffusion model on the joint distribution of \(a\) and \(u\). During inference, we gradually denoise a Gaussian noise, guided by sparse observation and known PDE function, to recover the full prediction of both \(a\) and \(u\) that align well with the sparse observations and the given equation.

has been extended to solve PDE in 3D [22; 23], multiphase flow , seismic wave [25; 26], 3D turbulence [27; 28], and spherical dynamics . People have also explored using neural networks as part of the PDE solver, such as compressing the physics state [30; 31; 32; 33]. These solvers usually assume known PDE parameters, and applying them to solve the inverse problem can be challenging.

PDE inverse problem.The inverse problem refers to finding the coefficients of a PDE that can induce certain observations, mapping from the solution of a PDE solver to its input parameters. People have tried to extend traditional numerical methods to this inverse problem [34; 35; 36; 37; 38], but these extensions are non-trivial to implement efficiently. There are similar attempts to inverse deep-learning PDE solvers. For example, one can inverse PINNs by optimizing the network parameters such that their outputs satisfy both the observed data and the governing equations. iFNO  and NIO  tries to extend FNO . Other methods [41; 42] directly learn the operator functions for the inverse problem. PINO  further combines neural operators with physics constraints to improve the performance of both forward and inverse problems. These methods assume full observations are available. To address the inverse problem with partial observations, people have tried to leverage generative priors with Graph neural networks [43; 8]. These works have not demonstrated the ability to solve high-resolution PDEs, possibly limited by the power of generative prior. We want to leverage the state-of-the-art generative model, diffusion models, to develop a better inverse PDE solver.

Diffusion models.Diffusion models have shown great promise in learning the prior with higher resolution by progressively estimating and removing noise. Models like DDIM , DDPM , and EDM  offer expressive generative capabilities but face challenges when sampling with specific constraints. Guided diffusion models [46; 47; 48; 49] enhance generation processes with constraints such as image inpainting, providing more stable and accurate solutions. Prior works on diffusion models for PDEs highlight the potential of diffusion approaches by generating PDE datasets such as 3D turbulence [50; 51] and Navier-Stokes equations  with diffusion models. Diffusion models can also be used to model frequency spectrum and denoise the solution space , and conditional diffusion models are applied to solve 2D flows with sparse observation . However, the application of diffusion models to solve inverse problems under partial observation remains underexplored. In this work, we aim to take the initial steps towards addressing this gap.

## 3 Methods

### Overview

To solve physics-informed forward and inverse problems under uncertainty, we start by pre-training a diffusion generative model on a family of partial differential equations (PDEs). This model is designed to learn the joint distribution of the PDE coefficients (or the initial state) and its corresponding solutions (or the final state). Our approach involves recovering full data in both spaces using sparse observations from either or both sides. We achieve this through the iterative denoising of random Gaussian noise as in regular diffusion models but with additional guidance from the sparse observations and the PDE function enforced during denoising. The schematic description of our approach is shown in Fig. 1.

### Preliminary: Diffusion Models and Guided Diffusion

Diffusion models involve a predefined forward process that gradually adds Gaussian noise to the data and a learned reverse process that denoises the data to reconstruct the original distribution. Specifically, Song et al.  propose a deterministic diffusion model that learns an \(N\)-step denoising process that eventually outputs a denoised data \(_{N}\) and satisfies the following ordinary differential equations (ODE) at each timestep \(t_{i}\) where \(i\{0,1,...,N-1\}\)

\[=-(t)(t)_{} p; (t)t.\] (1)

Here \(_{} p;(t)\) is the score function  that helps to transform samples from a normal distribution \((0,(t_{0})^{2})\) to a target probability distribution \(p(;(t))\). To estimate the score function, Karras et al.  propose to learn a denoiser function \(D(;)\) such that

\[_{} p;(t)=(D(;(t))-)/(t)^{2}\] (2)

To enable control over the generated data, guided diffusion methods  add guidance gradients to the score function during the denoising process. Recently, diffusion posterior sampling (DPS) made notable progress in guided diffusion for tackling various inverse problems. DPS uses corrupted measurements \(\) derived from \(\) to guide the diffusion model in outputting the posterior distribution \(p(|)\). A prime application of DPS is the inpainting problem, which involves recovering a complete image from sparsely observed pixels, which suits well with our task. This approach modifies Eq. 1 to

\[=-(t)(t)_{} p ;(t)+_{} p|;(t) t.\] (3)

DPS  showed that under Gaussian noise assumption of the sparse measurement operator \(()\), i.e., \(|((),^{2})\) with some S.D. \(\), the log-likelihood function can be approximated with:

\[_{} p|_{i};(t_{i}) _{_{i}} p|}_{N}^{i};(t_{i}) -}_{_{i}}\|-( }_{N}^{i}(_{i};(t_{i}))\|_{2}^{2},\] (4)

where \(}_{N}^{i}:=D(_{i};(t_{i}))\) denotes the estimation of the final denoised data at each denoising step \(i\). Applying the Baye's rule, the gradient direction of the guided diffusion is therefore:

\[_{_{i}} p(_{i}|) s(_{i})- _{_{i}}\|-(}_{N}^{i})\|_{2}^{2},\] (5)

where \(s()=_{} p\) is the original score function, and \(=1/^{2}\).

### Solving PDEs with Guided Diffusion

Our work focuses on two classes of PDEs: static PDEs and dynamic time-dependent PDEs. Static systems (e.g., Darcy Flow or Poisson equations) are defined by a time-independent function \(f\):

\[f(;,)=0\ \ \ \ ^{d}, ()=()\ \ \ \ ,\] (6)

where \(\) is a bounded domain, \(\) is a spatial coordinate, \(\) is the PDE coefficient field, and \(\) is the solution field. \(\) is the boundary of the domain \(\) and \(|_{}=\) is the boundary constraint. We aim to recover both \(\) and \(\) from sparse observations on either \(\) or \(\) or both.

Similarly, we consider the dynamic systems (e.g., Navier-Stokes):

\[ f(,;,)&=0, \ (0,)\\ (,)&=(,), \ (0,)\\ (,)&=(,), \ \{0\}\] (7)

where \(\) is a temporal coordinate, \(=_{0}\) is the initial condition, \(\) is the solution field, and \(|_{(0,)}=\) is the boundary constraint. We aim to simultaneously recover both \(\) and the solution \(_{T}:=(,T)\) at a specific time \(T\) from sparse observations on either \(\), \(_{T}\), or both.

Finally, we explore the recovery of the states across all timesteps \(_{0:T}\) in 1D dynamic systems governed by Burger's equation. Our network \(D_{}\) models the distribution of all 1D states, including the initial condition \(_{0}\) and solutions \(_{1:T}\) stacked in the temporal dimension, forming a 2D dataset.

Guided Diffusion AlgorithmIn the data-driven PDE literature, the above tasks can be achieved by learning directional mappings between \(\) and \(\) (or \(u_{T}\) for dynamic systems). Thus, existing methods typically train separate neural networks for the forward solution operator \(:\) and the inverse solution operator \(:\).

Our method unifies the forward and inverse operators with a single network and an algorithm using the guided diffusion framework. DiffusionPDE can handle arbitrary sparsity patterns with one pre-trained diffusion model \(D_{}\) that learns the joint distribution of \(\) and \(\), concatenated on the channel dimension, denoted \(\). Thus, our data \(\), where \(:=\). We follow the typical diffusion model procedures  to train our model on a family of PDEs.

Once we train the diffusion model \(D_{}\), we employ our physics-informed DPS  formulation during inference to guide the sampling of \(\) that satisfies the sparse observations and the given PDE, as detailed in Algorithm 1. We follow Eq. 5 to modify the score function using the two guidance terms:

\[_{_{i}} p(_{i}|_{obs},f)_{_{i}}  p_{i})-_{obs}_{_{i}}_{obs}- _{pde}_{_{i}}_{pde},\] (8)

where \(_{i}\) is the noisy data at denoising step \(i\), \(_{obs}\) are the observed values, and \(f()=\) is the underlying PDE condition. \(_{obs}\) and \(_{pde}\) respectively represent the MSE loss of the sparse observations and the PDE equation residuals:

\[_{obs}(_{i},_{obs};D_{ })&=\|_{obs}-}_{N}^{i}\|_{2}^{2}= _{j=1}^{n}(_{obs}(_{j})-}_{N}^{i}( _{j}))^{2},\\ _{pde}(_{i};D_{},f)&= {m}\|-f(}_{N}^{i})\|_{2}^{2}=_{j}_{k}f( _{j},_{k};}_{j},}_{j})^{2},\] (9)

where \(}_{N}^{i}=D_{}(_{i})\) is the clean image estimate at denoising timestep \(i\), which can be split into coefficient \(}_{i}\) and solution \(}_{i}\). Here, \(m\) is the total number of grid points (i.e., pixels), \(n\) is the number of sparse observation points. \(_{j}\) represents the spatio-temporal coordinate of \(j\)th observation. Note that, without loss of generality, \(_{pde}\) can be accumulated for all applicable PDE function \(f\) in the system, and the time component \(_{k}\) is ignored for static systems.

## 4 Experiments

### PDE Problem Settings

We show the usefulness of DiffusionPDE across various PDEs for inverse and forward problems and compare it against recent learning-based techniques. We test on the following families of PDEs.

Figure 2: Different from forward and inverse PDE solvers, DiffusionPDE can take sparse observations on either the coefficient \(\) or the solution \(\) to recover both of them, using one trained network. Here, we show the recovered \(\) and \(\) of the Darcy’s eqaution given sparse observations on \(\), \(\), or both. Compared with the ground truth, we see that our method successfully recovers the PDE in all cases.

Darcy Flow.Darcy flow describes the movement of fluid through a porous medium. In our experiment, we consider the static Darcy Flow with a no-slip boundary \(\)

\[-(()())& =q(),\\ ()&=0, \] (10)

Here the coefficient \(\) has binary values. We set \(q()=1\) for constant force. The PDE guidance function is thus \(f=(()())+q()\).

Inhomogeneous Helmholtz Equation.We consider the static inhomogeneous Helmholtz Equation with a no-slip boundary on \(\), which describes wave propagation:

\[^{2}()+k^{2}()& =(),\\ ()&=0, \] (11)

The coefficient \(\) is a piecewise constant function and \(k\) is a constant. Note 11 is the Poisson equation when \(k=0\). Setting \(k=1\) for Helmholtz equations, the PDE guidance function is \(f=^{2}()+k^{2}()-()\).

Non-bounded Navier-Stokes Equation.We study the non-bounded incompressive Navier-Stokes equation regarding the vorticity.

\[_{t}w(,)+v(,)  w(,)&= w(,)+q(), ,(0,T]\\  v(,)&=0, ,[0,T]\] (12)

Here \(w= v\) is the vorticity, \(v(,)\) is the velocity at \(\) at time \(\), and \(q()\) is a force field. We set the viscosity coefficient \(=10^{-3}\) and correspondingly the Reynolds number \(Re==1000\).

DiffusionPDE learns the joint distribution of \(w_{0}\) and \(w_{T}\) and we take \(T=10\) which simulates \(1\) second. Since \(T 0,\) we cannot accurately compute the PDE loss from our model outputs. Therefore, given that \( w(,)=( v)=0\), we use simplified \(f= w(,)\).

Bounded Navier-Stokes Equation.We study the bounded 2D incompressive Navier Stokes regarding the velocity \(v\) and pressure \(p\).

\[_{t}v(,)+v(,)  v(,)+ p&=^{2}v( ,),,(0,T]\\  v(,)&=0, ,(0,T].\] (13)

We set the viscosity coefficient \(=0.001\) and the fluid density \(=1.0\). We generate 2D cylinders of random radius at random positions inside the grid. Random turbulence flows in from the top of the grid, with the velocity field satisfying no-slip boundary conditions at the left and right edges, as well as around the cylinder \(_{left,right,cylinder}\). DiffusionPDE learns the joint distribution of \(v_{0}\) and \(v_{T}\) at \(T=4\), which simulates \(0.4\) seconds. Therefore, we similarly use \(f= v(,)\) as before.

Burgers' Equation.We study the Burgers' equation with periodic boundary conditions on a 1D spatial domain of unit length \(=(0,1)\). We set the viscosity to \(=0.01\). In our experiment, the initial condition \(u_{0}\) has a shape of \(128 1\), and we take 127 more time steps after the initial state to form a 2D \(u_{0:T}\) of size \(128 128\).

\[_{t}u(,)+_{}(u^{2} (,)/2)&=_{}u(,), ,(0,T]\\ u(,0)&=u_{0}(), \] (14)

We can reliably compute \(f=_{t}u(,)+_{}(u^{2}(,)/2)- _{}u(,)\) with finite difference since we model densely on the time dimension.

Figure 3: Usefulness of PDE loss. We visualize the absolute errors of the recovered coefficient and solution of the Helmholtz equation with and w/o PDE loss. We compare having only the observation loss with applying the additional PDE loss. The errors drop significantly when using PDE loss.

### Dataset Preparation and Training

We first test DiffusionPDE on jointly learning the forward mapping \(:\) and the inverse mapping \(:\) given sparse observations. In our experiments, we define our PDE over the unit square \(=(0,1)^{2}\), which we represent as a \(128 128\) grid. We utilize Finite Element Methods (FEM) to generate our training data. Specifically, we run FNO's  released scripts to generate Darcy Flows and the vorticities of the Navier-Stokes equation. Similarly, we generate the dataset of Poisson and Helmholtz using second-order finite difference schemes. To add more complex boundary conditions, we use Difftaichi  to generate the velocities of the bounded Navier-Stokes equation. We train the joint diffusion model for each PDE on three A40 GPUs for approximately 4 hours, using 50,000 data pairs. For Burgers' equation, we train the diffusion model on a dataset of 50,000 samples produced as outlined in FNO . We randomly select 5 out of 128 spatial points on \(\) to simulate sensors that provide measurements across time.

### Baseline Methods

We compare DiffusionPDE with state-of-the-art learning-based methods, including PINO , DeepONet , PINNs , and FNO . However, note that none of these methods show operation on partial observations. These methods can learn mappings between \(\) and \(\) or \(_{0}\) and \(_{1:T}\) with full observations, allowing them to also solve the mapping between \(_{0}\) and \(_{T}\). PINNs map input \(\) to output \(\) by optimizing a combined loss function that incorporates both the solution \(\) and the PDE residuals. DeepONet employs a branch network to encode input function values sampled at discrete points and a trunk network to handle the coordinates of the evaluated outputs. FNO maps from the parametric space to the solution space using Fourier transforms. PINO enhances FNO by integrating PDE loss during training and refining the model with PDE loss finetuning. We train all four baseline methods on both forward and inverse mappings using full observation of \(\) or \(\) for both static and dynamic PDEs. We tried training the baseline models on partial observations, but we noticed degenerate training outcomes (see supplementary for details). Overall, they are intended for _full observations_ and may not be suitable for sparse measurements.

More closely related to our method, GraphPDE  demonstrates the ability to recover the initial state using sparse observations on the final state, a task that other baselines struggle with. Therefore, we compare against GraphPDE for the inverse problem of bounded Navier-Stokes (NS) equation, which is the setup used in their report. GraphPDE uses a trained latent space model and a bounded forward GNN model to solve the inverse problem with sparse sensors and thus is incompatible with unbounded Navier-Stokes. We create bounded meshes using our bounded grids to train the GNN model and train the latent prior with \(v_{0:T}\) for GraphPDE.

While we employ guided sampling to reconstruct the solutions, Classifier-Free Guidance (CFG)  offers an alternative approach where the diffusion model is conditioned on sparse input data. Shu et al.  extend this method by developing an optimized CFG approach that conditions on the PDE loss,

    & & DiffusionPDE & PINO & DeepONet & PINNs & FNO \\   & Forward & **2.5\%** & 35.2\% & 38.3\% & 48.8\% & 28.2\% \\  & Inverse & **3.2\%** & 49.2\% & 41.1\% & 59.7\% & 49.3\% \\   & Forward & **4.5\%** & 107.1\% & 155.5\% & 128.1\% & 100.9\% \\  & Inverse & **20.0\%** & 231.9\% & 105.8\% & 130.0\% & 232.7\% \\   & Forward & **8.8\%** & 106.5\% & 123.1\% & 142.3\% & 98.2\% \\  & Inverse & **22.6\%** & 216.9\% & 132.8\% & 160.0\% & 218.2\% \\   & Forward & **6.9\%** & 101.4\% & 103.2\% & 142.7\% & 101.4\% \\  & Inverse & **10.4\%** & 96.0\% & 97.2\% & 146.8\% & 96.0\% \\   & Forward & **3.9\%** & 81.1\% & 97.7\% & 100.1\% & 82.8\% \\  & Inverse & **2.7\%** & 69.5\% & 91.9\% & 105.5\% & 69.6\% \\   

Table 1: Relative errors of solutions (or final states) and coefficients (or initial states) when solving forward and inverse problems respectively with sparse observations. Error rates are used for the inverse problem of Darcy Flow.

using the observation as a low-resolution input. Additionally, OFormer  is another model designed to reconstruct the full solution using transformers, offering a shorter inference runtime. Consequently, we compare our approach against these methods for solving the unbounded Navier-Stokes equation.

### Main Evaluation Results

We respectively address the forward problem and the inverse problem with sparse observations of \(\) or \(\). For the forward problem, we randomly select coefficients (initial states) as sparse observations and then compare the predicted solutions (final states) with the ground truth. Specifically, we select \(500\) out of \(128 128\) points, approximately \(3\%\), on the coefficients of Darcy Flow, Poisson equation, Helmholtz equation, and the initial state of the non-bounded Navier-Stokes equation. For the bounded Navier-Stokes equation, we use \(1\%\) observed points beside the boundary of the cylinder in 2D. Similarly, for the inverse problem, we randomly sample points on solutions (final states) as sparse observations, using the same number of observed points as in the forward model for each PDE.

We show the relative errors of all methods regarding both forward and inverse problems in Table 1. Since the coefficients of Darcy Flow are binary, we evaluate the error rates of our prediction. Non-binary data is evaluated using mean pixel-wise relative error. We report error numbers averaged across 1,000 random scenes and observations for each PDE. DiffusionPDE outperforms other methods including PINO , DeepONet , PINNs , and FNO  for both directions with sparse observations, demonstrating the novelty and uniqueness of our approach. For the inverse problems of the Poisson and Helmholtz equations, DiffusionPDE exhibits higher error rates due to the insufficient constraints within the coefficient space, produced from random fields. In Fig. 4, we visualize the results for solving both the forward and inverse problem of the non-bounded Navier-Stokes. We refer to the _supplementary_ for additional visual results. While other methods may produce partially correct results, DiffusionPDE outperforms them and can recover results very close to the ground truth.

For the inverse problem of the bounded Navier-Stokes equation, we further compare DiffusionPDE with GraphPDE, as illustrated in Fig. 5. Our findings reveal that DiffusionPDE surpasses GraphPDE  in accuracy, reducing the relative error from \(12.0\%\) to \(2.7\%\) with only \(1\%\) observed points.

We further show whether DiffusionPDE can jointly recover both \(\) and \(\) by analyzing the retrieved \(\) and \(\) with sparse observations on different sides as well as on both sides. In Fig. 2, we recover the coefficients and solutions of Darcy Flow by randomly observing \(500\) points on only coefficient space, only space solution space, and both. Both coefficients and solutions can be recovered with low errors

Figure 4: We compare DiffusionPDE with state-of-the-art neural PDE solvers [3; 4; 5; 6]. In the forward Navier-Stokes problem, we give \(500\) sparse observations of the initial state to solve for the final state. In the inverse set-up, we take observations of the final state and solve for the initial. For the Burgers’ equation, we use \(5\) sensors throughout all time steps and want to recover the solution at all time steps. Note that we train on neighboring snapshot pairs for the baselines in order to add continuous observations of the Burgers’ equation. Results show that existing methods do not support PDE solving under sparse observations, and we believe they are not easily extendable to do so. We refer readers to the supplementary for a complete set of visual results.

for each situation. We therefore conclude that DiffusionPDE can solve the forward problem and the inverse problem simultaneously with sparse observations at any side without retraining our network.

### Advantage of Guided Sampling

To demonstrate the clear advantage of our guided sampling method, we evaluate both the forward and inverse processes of the unbounded Navier-Stokes equation, comparing our DiffusionPDE approach with Diffusion using CFG when considering only the initial and final states given 500 observation points, as illustrated in Fig. 6. Our DiffusionPDE method consistently achieves lower relative errors across both evaluations.

Furthermore, in Fig. 7, we compare our results with those of Shu et al. , where the full time intervals are solved autoregressively using an optimized CFG method. In their approach, the error in the final state increases to approximately 13%, which is notably higher than that of our two-state model. Additionally, the relative errors of the transformer-based approach, OFormer , are around 17% and 23%, which are significantly larger than those observed with DiffusionPDE.

### Recovering Solutions Throughout a Time Interval

We demonstrate that DiffusionPDE is capable of retrieving all time steps throughout the time interval \([0,T]\) from continuous observations on sparse sensors. To evaluate its ability to recover \(u_{0:T}\) with sparse sensors, we study the 1D dynamic Burgers' equation, where DiffusionPDE learns the distribution of \(u_{0:T}\) using a 2D diffusion model. To apply continuous observation on PINO, DeepONet, FNO, and PINNs, we train them on neighboring snapshot pairs. Our experiment results in a test relative error of 2.68%, depicted in Fig. 4, which is significantly lower than other methods.

### Additional Analysis

We examine the effects of different components of our algorithm such as PDE loss and observation samplings. We strongly encourage readers to view the supplementary for more details of these analyses as well as additional experiments.

PDE Loss.To verify the role of the PDE guidance loss of Eq. 8 during the denoising process, we visualize the errors of recovered \(\) and \(\) of Helmholtz equation with or without PDE loss. Here, we run our DPS algorithm with 500 sparse observed points on both the coefficient \(\) and solution \(\) and study the effect of the additional PDE loss guidance. The relative error of \(\) reduces from \(9.3\%\) to \(0.6\%\), and the relative error of \(\) reduces from \(13.2\%\) to \(9.4\%\). Therefore, we conclude that PDE guidance helps smooth the prediction and improve the accuracy.

Number of Observations.We examine the results of DiffusionPDE in solving forward and inverse problems when there are \(100\), \(300\), \(500\), and \(1000\) random observations on \(\), \(\), or both \(\) and \(\). The error of DiffusionPDE decreases as the number of sparse observations increases. DiffusionPDE is capable of recovering both \(\) and \(\) with errors \(1\% 10\%\) with approximately \(6\%\) observation points at any side for most PDE families. DiffusionPDE becomes insensitive to the number of observations and can solve the problems well once more than \(3\%\) of the points are observed.

Figure 5: We compare GraphPDE  and our method for solving the inverse bounded Navier-Stokes equation. Given the boundary conditions and \(1\%\) observations of the final vorticity field, we solve the initial vorticity field. We set the fluids to flow in from the top, with boundary conditions at the edges and a middle cylinder. While GraphPDE can recover the overall pattern of the initial state, it suffers from noise when the fluid passes the cylinder and misses the high vorticities at the bottom.

Observation Sampling Pattern.While CFG struggles with robustness, we show that DiffusionPDE is robust to different sampling patterns of the sparse observations, including grid and non-uniformly concentrated patterns. Note that even when conditioned on the full observations, our approach performs on par with the current best methods, likely due to the inherent resilience of our guided diffusion algorithm. Additionally, DiffusionPDE can leverage continuous coordinates with bilinear interpolation in the prediction space to obtain predicted values for points that do not lie directly on the grid, without compromising accuracy.

## 5 Conclusion and Future Work

In this work, we develop DiffusionPDE, a diffusion-based PDE solver that addresses the challenge of solving PDEs from partial observations by filling in missing information using generative priors. We formulate a diffusion model that learns the joint distribution of the coefficient (or initial state) space and the solution (or final state) space. During the sampling process, DiffusionPDE can flexibly generate plausible data by guiding its denoising with sparse measurements and PDE constraints. Our new approach leads to significant improvements over existing state-of-the-art methods, advancing toward a general PDE-solving framework that leverages the power of generative models.

Several promising directions for future research have emerged from this work. Currently, DiffusionPDE is limited to solving slices of 2D dynamic PDEs; extending its capabilities to cover full time intervals of these equations presents a significant opportunity. Moreover, the model's struggle with accuracy in spaces that lack constraints is another critical area for exploration. DiffusionPDE also suffers from a slow sampling procedure, and a faster solution might be desired.

Figure 6: We compare the performance of DiffusionPDE and Diffusion with CFG for the unbounded Navier-Stokes equation, and visualize the error. With 500 observation points, DiffusionPDE demonstrates superior accuracy, achieving lower errors in both forward and inverse problem-solving.

Figure 7: We compare our DiffusionPDE method with the approaches of Shu et al.  and OFormer  for the unbounded Navier-Stokes equation. Using 500 observation points, DiffusionPDE effectively solves both the forward and inverse problems, achieving significantly lower errors.