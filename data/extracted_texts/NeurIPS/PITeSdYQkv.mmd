# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

\(_{i}:=(x_{i,1},,x_{i,m})\) for \(i[n]\) to denote the \(i\)th user's examples. Furthermore, we write \(_{-i}\) to denote the input with the \(i\)th user's data \(_{i}\) removed. Similarly, for \(S[n]\), we write \(_{-S}\) to denote the input with all the examples of users in \(S\) removed.

**Definition 1** (User-Level Neighbors).: Two inputs \(,^{}\) are _user-level neighbors_, denoted by \(^{}\), iff \(_{-i}=^{}_{-i}\) for some \(i[n]\).

**Definition 2** (User-Level and Item-Level DP).: For \(,>0\), we say that a randomized algorithm1\(:\) is _\((,)\)-user-level DP_ iff, for any \(^{}\) and any \(S\), we have \([() S] e^{}[(^{ }) S]+\). When \(m=1\), we say that \(\) is _\((,)\)-item-level DP_.

The case \(=0\) is referred to as _pure-DP_, whereas the case \(>0\) is referred to as _approximate-DP_.

To formulate statistical tasks studied in our work, we consider the setting where there is an unknown distribution \(\) over \(\) and the input \(^{nm}\) consists of \(nm\) i.i.d. samples drawn from \(\). A task \(}\) (including the desired accuracy) is defined by \(_{}}()\), which is the set of "correct" answers. For a parameter \(\), we say that the algorithm \(\) is _\(\)-useful_ if \(_{^{nm}}[()_{}()}]\) for all valid \(\). Furthermore, we say that \(nm\) is the _sample complexity_ of the algorithm, whereas \(n\) is its _user complexity_. For \(m\) and \(,>0\), let \(n_{m}^{}(,;)\) denote the smallest user complexity of any \((,)\)-user-level DP algorithm that is \(\)-useful for \(}\). When \(\) is not stated, it is assumed to be \(2/3\).

**Generic Algorithms Based on Stability.** So far, there have been two main themes of research on user-level DP learning. The first aims to provide generic algorithms that work with many tasks. Ghazi et al.  observed that any _pseudo-globally stable_ (aka _reproducible_) algorithm can be turned into a user-level DP algorithm. Roughly speaking, pseudo-global stability requires that the algorithm, given a random input dataset and a random string, returns a canonical output--which may depend on the random string--with a large probability (e.g., \(0.5\)). They then show that the current best generic PAC learning item-level DP algorithms from  (which are based on yet another notion of stability) can be made into pseudo-globally stable algorithms. This transforms the aforementioned item-level DP algorithms into user-level DP algorithms.

In a recent breakthrough, Bun et al.  significantly expanded this transformation by showing that _any_ item-level DP algorithm can be compiled into a pseudo-globally stable algorithm--albeit with some overhead in the sample complexity. Combining with , they then get a generic transformation of any item-level DP algorithm to a user-level DP algorithm. Such results are of (roughly) the following form: If there is an item-level DP algorithm for some task with sample complexity \(n\), then there is a user-level DP algorithm with user complexity \(O((1/)/)\) as long as \(m_{,}(n^{2})\). In other words, if each user has sufficiently many samples, the algorithm needs very few users to learn under the user-level DP constraint. However, the requirement on the number of examples per user can be prohibitive: due to the nature of the reduction, it requires each user to have enough samples to learn by themselves. This leads to the following natural question:2

Is there a generic transformation from item-level to user-level DP algorithms for small \(m\)?

Algorithms for Specific Tasks & the \(\) Savings.Meanwhile, borrowing a page from its item-level DP counterpart, another active research theme has been to study specific tasks and provide (tight) upper and lower bounds on their sample complexity for user-level DP. Problems such as mean estimation, stochastic convex optimization (SCO), and discrete distribution learning are well-understood under user-level DP (for approximate-DP); see, e.g., . A pattern has emerged through this line of studies: (the privacy-dependent part of) the user complexity in the user-level DP setting is often roughly \(1/\) smaller than that of the item-level DP setting. For example, for the task of discrete distribution learning on domain \([k]\) up to total variation distance of \(>0\) (denoted by \(k()\)), the user complexity for the user-level DP is 

\[n_{m}^{k()}(,)=_{}( }+m}).\]We can see that the first privacy-dependent term decreases by a factor of \(\) whereas the latter privacy-independent term decreases by a factor of \(m\). A similar phenomenon also occurs in the other problems discussed above. As such, it is intriguing to understand this question further:

Is there a common explanation for these \(\) saving factors for the privacy-dependent term?

### Our Contributions

Approximate-DP.We answer both questions by showing a generic way to transform any item-level DP algorithm into a user-level DP algorithm such that the latter saves roughly \(\) in terms of the number of users required. Since the formal expression is quite involved, we state below a simplified version, which assumes that the sample complexity can be written as the sum of two terms, the privacy-independent term (i.e., \(n^{}_{}\)) and the privacy-dependent term that grows linearly in \(1/\) and polylogarithmically in \(1/\) (i.e., \((}{}) n^{}_{ }\)). This is the most common form of sample complexity of DP learning discovered so far in the literature--in particular, this covers all the tasks we have discussed previously. The corollary below shows that the privacy-dependent term decreases by a factor of roughly \(\) whereas the privacy-independent term decreases by a factor of \(m\). The full version of the statement can be found in Theorem10.

**Corollary 3** (Main Theorem-Simplified).: _Let \(\) be any task and \(>0\) be a parameter. Suppose that for any sufficiently small \(,>0\), there is an \((,)\)-item-level DP algorithm that is \(\)-useful for \(\) with sample complexity \((}{} n^{}_{}+n^{}_{})\) for some constant \(c\). Then, for any sufficiently small \(,>0\), there exists an \((,)\)-user-level DP algorithm that is \((-o(1))\)-useful for \(\) and has user complexity_

\[(}}{ ^{2}} n^{}_{}+ n^{ }_{}+).\]

Our result provides a powerful and generic tool in learning with user-level DP, as it can translate any item-level DP bound to user-level DP bound. Specifically, up to polylogarithmic terms and the dependency on \(\), the corollary above recovers all the aforementioned user complexity bounds for mean estimation, SCO, and discrete distribution learning. In addition, it also presents new sample complexity bounds for the moderate \(m\) regime for PAC learning.

While our result is very general, it--perhaps inevitably--comes at a cost: the algorithm is computationally inefficient and the dependence on \(\) (and \(\)) is not tight. We discuss this--along with other open questions--in more detail in Section5.

Pure-DP.Surprisingly, the pure-DP setting is less explored compared to approximate-DP in the context of learning with user-level DP; the sample complexity of several of the problems mentioned above (e.g., discrete distribution learning and SCO) is well-understood under approximate-DP but not so under pure-DP. While we do not provide a generic transformation from item-level DP algorithms to user-level DP algorithms for pure-DP, we give simple modifications of the popular pure-DP _exponential mechanism_ that allow it to be used in the user-level DP setting. Consequently, we obtain improved bounds for several problems. Due to space constraints, in the main body of the paper, we will only discuss PAC learning. Results for other problems, such as hypothesis testing and distribution learning can be found in AppendixE.

In PAC learning, \(=\{0,1\}\) and there is a concept class \(\{0,1\}^{}\). An error of \(c:\{0,1\}\) w.r.t. \(\) is defined as \(_{}(c):=_{(x,y)}[c(x) y]\). The task \((;)\) is to, for any distribution \(\) that has zero error on some concept \(c^{}\) (aka _realizable by \(\)_), output \(c:\{0,1\}\) such that \(_{}(c)\). We give a tight characterization of the user complexity for PAC learning3:

**Theorem 4**.: _Let \(\) be any concept class with probabilistic representation dimension \(d\) (i.e. \(()=d\)). Then, for any sufficiently small \(,>0\) and for all \(m\), we have_

\[n^{(;)}_{m}(,=0)= (+ ).\]

Previous work  achieves the tight sample complexity only for large \(m\) (\( d^{2}\)).

#### 1.1.1 Technical Overview

In this section, we give a rough overview of our proofs. We will be intentionally vague; all definitions and results will be formalized later in the paper.

**Approximate-DP.** At a high-level, our proof proceeds roughly as follows. First, we show that any \((,)\)_-item-level_ DP \(\) with high probability satisfies a local version of user-level DP for which we only compare4\((_{-i})\) for different \(i\), but with \(\) that is increased by a factor of roughly \(\) (see Definition 16 and Theorem 17). If this were to hold not only for \(_{-i}\) but also for all user-level neighbors \(^{}\) of \(\), then we would have been done since the \((,)\)-item-level DP algorithm would have also yielded \((,)\)_-user-level_ DP (which would then imply a result in favor of Corollary 3). However, this is not true in general and is where the second ingredient of our algorithm comes in: We use a propose-test-release style algorithm  to check whether the input is close to a "bad" input for which the aforementioned local condition does not hold. If so, then we output \(\); otherwise, we run the item-level DP algorithm. Note that the test passes w.h.p. and thus we get the desired utility.

This second step is similar to recent work of Kohli and Laskowski  and Ghazi et al. , who used a similar algorithm for additive noise mechanisms (namely Laplace and Gaussian mechanisms). The main difference is that instead of testing for local _sensitivity_ of the function as in , we directly test for the privacy loss of the algorithm \(\).

As for the first step, we prove this via a connection to a notion of generalization called _sample perfect generalization_ (see Definition 12). Roughly speaking, this notion measures the privacy loss when we run an \((,)\)-item-level DP algorithm on two \(,^{}\) drawn independently from \(^{n}\). We show that w.h.p. \(()_{^{},^{}}( ^{})\) for \(^{}=O()\). Although ostensibly unrelated to the local version of user-level DP that we discussed above, it turns out that these are indeed related: if we view the algorithm that fixes examples of all but one user, then applying the sample perfect generalization bound on just the input of the last user (which consists of only \(m\) samples) shows that the privacy loss when changing the last user is \(O()\) w.h.p. It turns out that we can turn this argument into a formal bound for our local notion of user-level DP (see Section 3.2.1.)

We remark that our results on sample perfect generalization also resolve an open question of Cummings et al. ; we defer this discussion to Appendix C.1.

Finally, we also note that, while Bun et al.  also uses the sample perfect generalization notion, their conversion from item-level DP algorithms to user-level DP algorithms require constructing pseudo-globally stable algorithms with sample complexity \(m\). This is impossible when \(m\) is small, which is the reason why their reduction works only in the example-rich setting.

**Pure-DP.** Recall that the exponential mechanism  works as follows. Suppose there is a set \(\) of candidates we would like to select from, together with the scoring functions \((_{H})_{H}\), where \(_{H}:^{n}\) has sensitivity at most \(\). The algorithm then outputs \(H\) with probability proportional to \((-_{H}()/_{2})\). The error guarantee of the algorithm scales with the sensitivity \(\) (see Theorem 9.) It is often the case that \(_{H}\) depends on the sum of individual terms involving each item. Our approach is to clip the contribution from each user so that the sensitivity is small. We show that selecting the clipping threshold appropriately is sufficient obtain new user-level pure-DP (and in some cases optimal) algorithms.

## 2 Preliminaries

Let \([n]=\{1,,n\}\), let \([x]_{+}=(0,x)\), and let \((f)\) denote \(O(f^{c}f)\) for some constant \(c\). Let \((A)\) denote the support of distribution \(A\). We recall some standard tools from DP .

For two distributions \(A,B\), let \(d_{}(A B)=_{x(A)}[A(x)-e^{ }B(x)]_{+}\) denote the _\(e^{}\)-hockey stick divergence_. For brevity, we write \(A_{,}B\) to denote \(d_{}(A B)\) and \(d_{}(B A)\).

**Lemma 5** ("Triangle Inequality").: _If \(A_{^{},^{}}B\) and \(B_{,}C\), then \(A_{+^{},^{}^{ }+e^{^{}}}C\)._

**Lemma 6** (Group Privacy).: _Let \(\) be any \((,)\)-item-level DP algorithm and \(,^{}\) be \(k\)-neighbors5, then \(()_{^{},^{}}(^{})\) where \(^{}=k,^{}=-1}{e^{ }-1} ke^{k}\)._

**Theorem 7** (Amplification-by-Subsampling ).: _Let \(n,n^{}\) be such that \(n^{} n\). For any \((,)\)-item-level DP algorithm \(\) with sample complexity \(n\), let \(^{}\) denote the algorithm with sample complexity \(n^{}\) that randomly chooses \(n\) out of the \(n^{}\) input samples and then runs \(\) on the subsample. Then \(^{}\) is \((^{},^{})\)-DP where \(^{}=(1+(e^{}-1)),^{}= \) for \(=n/n^{}\)._

We also need the definition of the shifted and truncated discrete Laplace distribution:

**Definition 8** (Shifted Truncated Discrete Laplace Distribution).: For any \(,>0\), let \(=(,):=1+(1/)/\) and let \((,)\) be the distribution supported on \(\{0,,2\}\) with probability mass function at \(x\) being proportional to \((-|x-|)\).

**Exponential Mechanism (EM).** In the (generalized) _selection_ problem (SELECT), we are given a candidate set \(\) and scoring functions \(_{H}\) for all \(H\) whose sensitivities are at most \(\). We say that an algorithm \(\) is \((,)\)-accurate iff \(_{H()}[_{H}()_{H^{ }}_{H^{}}()+] 1-\).

**Theorem 9** ().: _There is an \(\)-DP \((O((||/)/),)\)-accurate algorithm for SELECT._

## 3 Approximate-DP

In this section, we prove our main result on approximate-DP user-level learning, which is stated in Theorem10 below. Throughout the paper we say that a parameter is "sufficiently small" if it is at most an implicit absolute constant.

**Theorem 10** (Main Theorem).: _Let \(\) be any task and \(>0\) be a parameter. Then, for any sufficiently small \(,>0\) and \(m\), there exists \(^{}=}{(1/)}\) and \(^{}=()\) such that_

\[n_{m}^{}(,;-o(1))\;\; (+ n_{1}^{}(^{},^{};)).\]

Note that Corollary3 follows directly from Theorem10 by plugging in the expression \(n_{1}^{}(^{},^{};)=()^{c}}{^{}} n_{ }^{}+n_{}^{})\) into the bound.

While it might be somewhat challenging to interpret the bound in Theorem10, given that the \(\) values on the left and the right hand sides are not the same, a simple subsampling argument (similar to one in ) allows us to make the LHS and RHS have the same \(\). In this case, we have that the user complexity is reduced by a factor of roughly \(1/\), as stated below. We remark that the \(\) values are still different on the two sides; however, if we assume that the dependence on \(1/\) is only polylogarithmic, this results in at most a small gap of at most polylogarithmic factor in \(1/\).

**Theorem 11** (Main Theorem-Same \(\) Version).: _Let \(\) be any task and \(>0\) be a parameter. Then, for any sufficiently small \(,>0\), and \(m\), there exists \(^{}=()\) such that_

\[n_{m}^{}(,;-o(1))( +}}{} n_{1}^{}(,^{}; )).\]

Even though Theorem11 might be easier to interpret, we note that it does not result in as sharp a bound as Theorem10 in some scenarios. For example, if we use Theorem11 under the assumption in Corollary3, then the privacy-independent part would be \(}}{} n_{ }^{}\), instead of \( n_{}^{}\) implied by Theorem10. (On the other hand, the privacy-dependent part is the same.)

The remainder of this section is devoted to the proof of Theorem10. The high-level structure follows the overview in Section1.1.1: first we show in Section3.1 that any item-level DP algorithm satisfies sample perfect generalization. Section3.2.1 then relates this to a local version of user-level DP. We then describe the full algorithm and its guarantees in Section3.2.2. Since we fix a task \(\) throughout this section, we will henceforth discard the superscript \(\) for brevity.

### DP Implies Sample Perfect Generalization

We begin with the definition of _sample perfect generalization_ algorithms.

**Definition 12** ().: For \(,,>0\), an algorithm \(:^{n}\) is said to be \((,,)\)_-sample perfectly generalizing_ iff, for any distribution \(\) (over \(\)), \(_{,^{}^{n}}[()_{ ,}(^{})] 1-\).

The main result of this subsection is that any \((,)\)-item-level DP algorithm is \((,O(}{{}})}),O(n))\)-sample perfectly generalizing:

**Theorem 13** (\(\) Sample Perfect Generalization).: _Suppose that \(:^{n}\) is \((,)\)-item-level DP, and assume that \(,,,}{{} })}>0\) are sufficiently small. Then, \(\) is \((,^{},^{})\)-sample perfectly generalizing, where \(^{} O(}{{} })})\) and \(^{}=O(n)\)._

#### 3.1.1 Bounding the Expected Divergence

As a first step, we upper bound the expectation of the hockey stick divergence \(d_{^{}}(()(^{}))\):

**Lemma 14**.: _Suppose that \(:^{n}\) is an \((,)\)-item-level DP algorithm. Further, assume that \(,\), and \(}{{}})}>0\) are sufficiently small. Then, for \(^{}=O(}{{}})})\), we have_

\[_{,^{}^{n}}[d_{^{ }}(()(^{}))] O(n).\]

This proof follows an idea from , who prove a similar statement for _pure-DP_ algorithms (i.e., \(=0\)). For every output \(o\), they consider the function \(g^{o}:^{n}\) defined by \(g^{o}():=([()=o])\). The observation here is that, in the pure-DP case, this function is \(\)-Lipschitz (i.e., changing a single coordinate changes its value by at most \(\)). They then apply McDiarmid's inequality on \(g^{o}\) to show that the function values are well-concentrated with a variance of \(O()\). This suffices to prove the above statement for the case where the algorithm is pure-DP (but the final bound still contains \(\)). To adapt this proof to the approximate-DP case, we prove a "robust" version of McDiarmid's inequality that works even for the case where the Lipschitz property is violated on some pairs of neighbors (Lemma 32). This inequality is shown by arguing that we may change the function "slightly" to make it multiplicative-Lipschitz, which then allows us to apply standard techniques. Due to space constraints, we defer the full proof, which is technically involved, to Appendix C.3.

#### 3.1.2 Boosting the Probability: Proof of Theorem 13

While Lemma 14 bounds the expectation of the hockey stick divergence, it is insufficient to get arbitrarily high probability bound (i.e., for any \(\) as in Theorem 13); for example, using Markov's inequality would only be able to handle \(>^{}\). However, we require very small \(\) in a subsequent step of the proofs (in particular Theorem 17). Fortunately, we observe that it is easy to "boost" the \(\) to be arbitrarily small, at an additive cost of \(}{{}})}\) in the privacy loss.

To do so, we will need the following concentration result of Talagrand6 for product measures:

**Theorem 15** ().: _For any distribution \(\) over \(\), \(^{n}\), and \(t>0\), if \(_{^{n}}[] 1/2\), then \(_{^{n}}[_{ t}] 1-0.5 (-}}{{4n}})\), where \(_{ t}:=\{^{}^{n} ,\|^{}-\|_{0} t\}\)._

Proof of Theorem 13.: We consider two cases based on whether \( 2^{-n}\) or not. Let us start with the case that \( 2^{-n}\). From Lemma 14, for \(^{}=O(}{{}})})\), we have

\[_{,^{}^{n}}[d_{^{ }}(()(^{}))] O(n )=:^{}.\]

Let \(^{n}^{n}\) denote the set of \((},}^{})\) such that \((})_{^{},4^{ }}(}^{})\). By Markov's inequality, we have that \(_{,^{}^{n}}[d_{^{ }}((})(}^{}))  4^{}] 3/4\) and \(_{,^{}^{n}}[d_{^{ }}((}^{})(}))  4^{}] 3/4\) and hence \(_{,^{}^{n}}[(,^{}) ] 1/2\). By Theorem 15, for \(t=O()\), we have

\[_{,^{}^{n}}[(,^{}) _{ t}] 1-.\]

Next, consider any \((,^{})_{ t}\). Since there exists \((},}^{})\) such that \(\|-}\|_{0},\|^{}-}^{}\|_{0}  t\), we may apply Lemma 6 to conclude that \(d_{ t}(()(})),d_{  t}((}^{})(^{ }))(}^{})\).

\(O(t)\). Furthermore, since \(d_{^{}}((})( {}^{})) 4^{}\), we may apply Lemma 5 to conclude that \(d_{^{}}(()(^{})) ^{}\) where \(^{}=^{}+2t=O( }{{}})})\) and \(^{}=O(^{}+t)=O(n)\). Similarly, we also have \(d_{^{}}((^{}( ))^{}\). In other words, \(()_{^{},^{}}( ^{})\). Combining this and the above inequality implies that \(\) is \((,^{},^{})\)-sample perfectly generalizing as desired.

For the case \(<2^{-n}\), we may immediately apply group privacy (Lemma 6). Since any \(,^{}^{n}\) satisfies \(\|-^{}\|_{0} n\), we have \(()_{^{},^{}}( ^{})\) for \(^{}=n\) and \(^{}=O(n)\). This implies that \(\) is \((,^{},^{})\)-sample perfectly generalizing (in fact, \((0,^{},^{})\)-sample perfectly generalizing), which concludes our proof. 

### From Sample Perfect Generalization to User-Level DP

Next, we will use the sample perfect generalization result from the previous subsection to show that our algorithm satisfies a certain definition of a local version of user-level DP. We then finally turn this intuition into an algorithm and prove Theorem 10.

#### 3.2.1 Achieving Local-Deletion DP

We start by defining _local-deletion DP_ (for user-level DP). As alluded to earlier, this definition only considers \(_{-i}\) for different \(i\)'s. We also define the multi-deletion version where we consider \(_{-S}\) for all subsets \(S\) of small size.

**Definition 16** (Local-Deletion DP).: An algorithm \(\) is _\((,)\)-local-deletion DP_ (abbreviated _LDDP_) _at input \(\)_ if \((_{-i})_{,}(_{-i^{ }})\) for all \(i,i^{}[n]\). Furthermore, for \(r\), an algorithm \(\) is _\((r,,)\)-local-deletion DP at \(\)_ if \((_{-S})_{,}(_{-S^{ }})\) for all \(S,S^{}[n]\) such that \(|S|=|S^{}|=r\).

Below we show that, with high probability (over the input \(\)), any item-level DP algorithm satisfies LDDP with the privacy loss increased by roughly \(r\). The proof uses the crucial observation that relates sample perfect generalization to user-level DP.

**Theorem 17**.: _Let \(n,r\) with \(r n\), and let \(:^{(n-r)m}\) be any \((^{},^{})\)-item-level DP algorithm. Further, assume that \(^{},^{},^{}r}{{^{}}})}>0\) are sufficiently small. Then, for any distribution \(\),_

\[_{^{nm}}[(r,,) ] 1-,\]

_where \(=O(^{}r}{{^{ }}})})\) and \(=O(rm^{})\)._

Proof.: Let \(^{}=}{{r}}^{2}\). Consider any fixed \(S,S^{}[n]\) such that \(|S|=|S^{}|=r\). Let \(T=S S^{},t=|T|,U=S^{} S,U^{}=S S ^{},q=|U|=|U^{}|\), and let \(^{}_{_{-T}}\) denote the algorithm defined by \(^{}_{_{-T}}()=(_{-T})\).

Thus, for any fixed \(_{-T}^{(n-t)m}\), Theorem 13 implies that

\[_{_{U},_{U^{}}^{nm}}[^{ }_{_{-T}}(_{U})_{,}^{ }_{_{-T}}(_{U^{}})]^{},\] (1)

where \(=O(^{}}{{^{} ^{}}})}) O(^{}r}{{ ^{}}})})\) and \(=O(rm^{})\).

Notice that \(^{}_{_{-T}}(_{U})=(_{-S})\) and \(^{}_{_{-T}}(_{U^{}})=(_{-S^{ }})\). Thus, we have

\[_{^{nm}}[(_{-S}) _{,}(_{-S^{}})]= _{_{-T}^{(n-t)m}}[_{_{U}, _{U^{}}^{nm}}[^{}_{_{-T}}(_{U})_{,}^{}_{_{-T}}(_{U^{}})]]\] \[^{R_{1}}_{}\) is defined based on LDDP instead of the local deletion sensitivity and (ii) our output is \((_{-T})\), whereas their output is the function value with Gaussian noise added. It is not hard to adapt their proof to show that our algorithm is \((,)\)-user-level DP, as stated below. (Full proof is deferred to Appendix C.4.)

**Lemma 18** (Privacy Guarantee).: \(_{,,}\) _is \((,)\)-user-level DP._

```
1:Input: Dataset \(^{nm}\)
2:Parameters: Privacy parameters \(,\), Algorithm \(:^{(n-4)m}\)
3:\(, +^{}+2}\), \((,)\).
4:Sample \(R_{1}(,)\) {See Definition 8}
5:\(^{R_{1}}_{}S[n]:|S|=R_{1}, (4-R_{1},,)_{-S}}\) {See Definition 16}
6:if\(|^{R_{1}}_{}|=\)then
7:return\(\)
8:Choose \(S^{R_{1}}_{}\) uniformly at random
9:Choose \(T\) of size \(4\) uniformly at random
10:return\((_{-T})\) ```

**Algorithm 1**\(_{,,}()\)

Next, we prove the utility guarantee under the assumption that \(\) is item-level DP:

**Lemma 19** (Utility Guarantee).: _Let \(,>0\) be sufficiently small. Suppose that \(\) is \((^{},^{})\)-item-level DP such that \(^{}=(})\) and \(^{}=()\). If \(\) is \(\)-useful for any task \(\) with \((n-4)m\) samples, then \(_{,,}\) is \((-o(1))\)-useful for \(\) (with \(nm\) samples)._

Proof.: Consider another algorithm \(_{0}\) defined in the same way as \(_{,,}\) except that on Line 5, we simply let \(^{R_{1}}_{}}\). Note that \(_{0}()\) is equivalent to randomly selecting \(n-4\) users and running \(\) on it. Therefore, we have \(_{^{nm}}[_{0}()_{}] =_{^{(n-4)m}}[()_{ }]\)

Meanwhile, \(_{0}\) and \(_{,}\) are exactly the same as long as \(^{R_{1}}_{}()=}\) in the latter. In other words, we have

\[_{^{nm}}[_{, ,}()_{}] _{^{nm}}[_{0}()_{ }]-_{^{nm}}[^{R_{1}}_{ }()}]\] \[-_{^{nm}}[(4,,)] -o(1),\]

where in the last inequality we apply Theorem 17 with, e.g., \(=}{{1}}\)\(nm\). 

Putting Things Together.Theorem 10 is now "almost" an immediate consequence of Lemmas 18 and 19 (using \(=O((1/)/)\)). The only challenge here is that \(^{}\) required in Lemma 19 depends polylogarithmically on \(n\). Nonetheless, we show below via a simple subsampling argument that this only adds polylogarithmic overhead to the user/sample complexity.

Proof of Theorem 10.: Let \(^{}=^{}(n),^{}\) be as in Lemma 19. Furthermore, let \(^{}=}{(1/)}\). Notice that \(^{}(n)/^{}( n)^{O(1)}\). Let \(N\) be the smallest number such that \(N\) is divisible by \(m\) and

\[(e^{^{}}-1)(^{ },^{})}{N} e^{^{}(N/m+4)}-1,\]

Observe that \(N(m+n_{1}(^{},^{ }))\).

From the definition of \(n_{1}\), there exists an \((^{},^{})\)-item-level DP algorithm \(_{0}\) that is \(\)-useful for \(\) with sample complexity \(n_{1}(^{},^{})\). We start by constructing an algorithm \(\) on \(N\) samples as follows: randomly sample (without replacement) \(n_{1}(^{},^{})\) out of \(N\) items, then run \(_{0}\) on this subsample. By amplification-by-subsampling (Theorem 7), we have that \(\) is \((^{}(n),^{})\)-item-level DP for \(n=N/m+4\). Thus, by Lemma 18 and Lemma 19, we have that \(_{,,}\) is \((,)\)-DP and \((-o(1))\)-useful for \(\). Its user complexity is

\[n=N/m+4=(+ n_{1}(}{(1/)}, ^{})).\]

## 4 Pure-DP: PAC Learning

In this section we prove Theorem 4. Let \(\) denote the input and for \(i[n]\), let \(_{i}=((x_{i,1},y_{i,1}),,(x_{i,m},y_{i,m}))\) denote the input to the \(i\)th user. A _concept class_ is a set of functions of the form \(\{0,1\}\). For \(T\{0,1\}\), we say that it is _realizable_ by \(c:\{0,1\}\) iff \(c(x)=y\) for all \((x,y) T\). Recall the definition of \(\):

**Definition 20** ().: A distribution \(\) on concept classes is an \((,)\)_-probabilistic representation_ (abbreviated as \((,)\)_-PR_) of a concept class \(\) if for every \(c\) and for every distribution \(_{}\) on \(\), with probability \(1-\) over \(\), there exists \(h\) such that \(_{x_{}}[c(x) h(x)]\).

The \((,)\)_-PR dimension_ of \(\) is defined as \(_{,}():=_{(,) }()\), where \(():=_{()}||\). Furthermore, let the _probabilistic representation dimension_ of \(\) be \(():=_{1/4,1/4}()\).

**Lemma 21** ().: _For any \(\) and \((0,1/2)\), \(_{,1/4}()( ()(1/))\)._

Proof of Theorem 4.: The lower bound \((d/)\) was shown in  (for any value of \(m\)). The lower bound \(()\) follows from the lower bound of \(()\) on the sample complexity in the item-level setting by  and the observation that any \((,)\)-user-level DP algorithm is also \((,)\)-item-level DP with the same sample complexity. Thus, we may focus on the upper bound.

It suffices to only prove the upper bound \((+)\) assuming7\(m 1/\). Let \(\) denote a \((0.01,1/4)\)-PR of \(\); by Lemma 21, there exists such \(\) with \(()(d(1/))\). Assume that the number \(n\) of users is at least \(()}{ m}= ()\), where \(\) is a sufficiently large constant. Our algorithm works as follows: Sample \(\) and then run the \(\)-DP EM (Theorem 9) on candidate set \(\) with the scoring function \(_{h}():=_{i[n]}[_{i}\) is not realizable by \(h\)].

Observe that the sensitivity of \(_{h}\) is one. Indeed, this is where our "clipping" has been applied: a standard item-level step would sum up the error across all the samples, resulting in the sensitivity as large as \(m\), while our scoring function above "clips" the contribution of each user to just one.

A simple concentration argument (deferred to Appendix D) gives us the following:

**Lemma 22**.: _With probability 0.9 (over the input), the following hold for all \(h\):_

1. _If_ \(_{}(h) 0.01\)_, then_ \(_{h}() 0.05 nm\)_._
2. _If_ \(_{}(h)>\)_, then_ \(_{h}()>0.1 nm\)_._

We now assume that the event in Lemma 22 holds and finish the proof. Since a \(\) is a \((0.01,1/4)\)-PR of \(\), with probability \(3/4\), we have that there exists \(h^{*}\) such that \(_{}(h^{*}) 0.01\). From Lemma 22(i), we have \(_{h^{*}}() 0.05 nm\). Thus, by Theorem 9, with probability \(2/3\), the algorithm outputs \(h\) that satisfies \(_{h}() 0.05 nm+O(( )/)\), which, for a sufficiently large \(\), is at most \(0.1 nm\). By Lemma 22(ii), this implies that \(_{}(h)\) as desired. 

## 5 Conclusion and Open Questions

We presented generic techniques to transform item-level DP algorithms to user-level DP algorithms.

There are several open questions. Although our upper bounds are demonstrated to be tight for many tasks discussed in this work, it is not hard to see that they are not always tight8; thus, an important research direction is to get a better understanding of the tight user complexity bounds for different learning tasks.

On a more technical front, our transformation for approximate-DP (Theorem 10) has an item-level privacy loss (roughly) of the form \(}{}}\) while it seems plausible that \(}\) can be achieved; an obvious open question is to give such an improvement, or show that it is impossible. We note that the extra factor of \(}{{}}\) comes from the fact that Algorithm 1 uses LDDF with distance \(O()=O(}{{}})\). It is unclear how one could avoid this, given that the propose-test-release framework requires checking input at distance \(O(}{{}})\).

Another limitation of our algorithms is their computational inefficiency, as their running time grows linearly with the size of the possible outputs. In fact, the approximate-DP algorithm could even be slower than this since we need to check whether \(\) is LDDP at a certain input \(\) (which involves computing \(d_{}(_{-S}_{-S^{}})\)). Nevertheless, given the generality of our results, it is unlikely that a generic efficient algorithm exists with matching user complexity; for example, even the algorithm for user-level DP-SCO in  has a worst case running time that is not polynomial. Thus, it remains an interesting direction to devise more efficient algorithms for specific tasks.

Finally, while our work has focused on the central model of DP, where the algorithm gets the access to the raw input data, other models of DP have also been studied in the context of user-level DP, such as the local DP model . Here again,  give a generic transformation that sometimes drastically reduces the user complexity in the example-rich case. Another interesting research direction is to devise a refined transformation for the example-sparse case (like one presented in our paper for the central model) but for the local model.