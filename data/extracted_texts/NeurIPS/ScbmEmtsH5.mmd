# DisC-GS: Discontinuity-aware Gaussian Splitting

Haoxuan Qu

Lancaster University

U.K.

h.qu5@lancaster.ac.uk

&Zhuoling Li

Lancaster University

U.K.

z.li81@lancaster.ac.uk

&Hossein Rahmani

Lancaster University

U.K.

h.rahmani@lancaster.ac.uk

&Yujun Cai

University of Queensland

Australia

vanora.caiyj@gmail.com

&Jun Liu

Lancaster University

U.K.

j.liu81@lancaster.ac.uk

Both authors contributed equally to the work.Corresponding Author

###### Abstract

Recently, Gaussian Splitting, a method that represents a 3D scene as a collection of Gaussian distributions, has gained significant attention in addressing the task of novel view synthesis. In this paper, we highlight a fundamental limitation of Gaussian Splitting: its inability to accurately render discontinuities and boundaries in images due to the continuous nature of Gaussian distributions. To address this issue, we propose a novel framework enabling Gaussian Splitting to perform discontinuity-aware image rendering. Additionally, we introduce a Bezier-boundary gradient approximation strategy within our framework to keep the "differentiability" of the proposed discontinuity-aware rendering process. Extensive experiments demonstrate the efficacy of our framework.

## 1 Introduction

Novel view synthesis aims to generate images accurately from novel viewpoints in a captured 3D scene. Its significance spans across diverse applications, such as autonomous driving , virtual reality , and 3D content generation . Recently, for better tackling novel view synthesis, Neural Radiance Field (NeRF)  and a variety of NeRF-based methods [3; 4] have been proposed, which represent 3D scenes in an implicit manner as neural radiance fields. However, their general reliance on a heavy volume rendering mechanism often results in slow rendering speeds [30; 13], limiting their practicality across real-world applications. While some methods [18; 19] have proposed to accelerate the rendering process of NeRF from different perspectives, they often achieve this at the expense of noticeably compromising the quality of the generated images , which is evidently undesirable.

More recently, Gaussian Splatting , which explicitly represents the 3D scene as a collection of Gaussian distributions, has been proposed as an appealing alternative to NeRF. Specifically, rather than generating novel-view images through the time-consuming process of volume rendering, Gaussian Splatting enables images from novel viewpoints to be generated by simply splatting (projecting) [53; 54] these Gaussian distributions onto the image plane. By doing so, Gaussian Splatting achieves real-time rendering of novel-view images, while maintaining its rendered images to be of competitively high visual quality compared to NeRF-rendered ones. Due to its compelling capability, Gaussian Splatting has received lots of research attention [26; 13; 42; 49; 12; 21; 25].

However, in this paper, we argue that Gaussian Splatting may still be sub-optimal in accurately synthesizing novel views, due to its inherent weakness in representing (rendering) _discontinuities and boundaries_ with its collection of _continuous_ Gaussian distributions. Specifically, due to the general complexity of 3D scenes, the expected image to be rendered often contains numerous _discontinuities and boundaries_ (as shown in Fig. 1(a)). However, Gaussian Splatting represents each of its generated images using only _continuous_ Gaussians projected onto the image plane. Considering this, as illustrated in Fig. 1(b), the inherent continuity of Gaussian distributions can result in some parts of the distribution inevitably "passing over"("spilling over") the boundaries of sharp features in the image. This can lead to Gaussian Splatting rendering the sharp boundaries in the image with blurriness (as shown in Fig. 1(c)), which can significantly reduce the quality of the rendered image.

Based on the above argument, in this paper, we aim to enable Gaussian Splatting to bypass its original intrinsic weakness, and render discontinuities and boundaries properly. However, this can be non-trivial owing to the following challenges: (1) Since 3D scenes generally can be complex, as illustrated in Fig. 1(a), various different kinds of boundaries with diverse shapes can all exist in the image rendered from a certain 3D scene. Thus, it can be difficult to represent and render these diverse boundaries properly and seamlessly in a Gaussian-Splitating-based framework. (2) Meanwhile, recall that continuity serves as the prerequisite for a function to be differentiable. Thus, during the process of learning the 3D scene representation using Gaussian Splatting, how to maintain the "differentiability" of the process in existence of discontinuities, i.e., enabling the loss calculated over the rendered images that contain "discontinuous" (sharp) boundaries to properly guide the learning of the 3D scene representation, is also challenging. To handle the above challenges, in this work, we propose **Dis**Continuity-aware **G**aussian **S**platting (**Dis**C-GS**), a novel framework that can for the first time, enable Gaussian Splatting to represent and render discontinuities properly in its image rendering process, which handles a key limitation of the original Gaussian Splatting technique. We illustrate the rendering process of our framework in Fig. 2, and outline our framework as follows.

Overall, to enable Gaussian Splatting to properly render discontinuities and boundaries, our framework introduces a "pre-scissoring" step. Specifically, for each Gaussian distribution representing the 3D scene, rather than directly rendering its entire 2D projection on the image plane, we first segment ("pre-scisser") the projected Gaussian distribution along the specified boundaries. However, achieving this requires representing boundaries with various shapes accurately. Here, we get inspiration from that, the cubic Bezier curve, conveniently represented by a group of four control points, has shown capable of efficiently parameterizing curves of various shapes with low computational complexity [16; 34]. Considering this, in our framework, we aim to use the cubic Bezier curves to represent boundaries. Specifically, we first introduce each Gaussian distribution representing the 3D scene with an additional attribute, which when projected onto the image plane, can serve as the control points of

Figure 1: (a) Illustration of a ground truth image, containing numerous discontinuities and boundaries, that is expected to be rendered from a certain viewpoint of a 3D scene. We generate the boundary map in (a) utilizing the Canny algorithm . (b) Illustration of Gaussian distributions projected onto the image plane. As shown, since Gaussian distributions are continuous, they can inevitably “pass over” the (hard) boundary represented by the curve. (c) Illustration of images rendered with and without applying DisC-GS. As shown, without DisC-GS, Gaussian Splatting can fail to accurately render boundaries. In contrast, applying DisC-GS ensures that boundaries and discontinuities in the image are properly rendered. **More qualitative results are in Appendix B.** (Best viewed in color.)

the cubic Bezier curves. After that, given a viewpoint, based on the control points projected onto the image plane corresponding to the viewpoint, we use the cubic Bezier curves formulated from these control points to represent the desired boundaries w.r.t. each projected Gaussian distribution. Finally, leveraging the derived boundaries, we can achieve discontinuity-aware image rendering through a modified \(\)-blending function (as discussed in Sec. 4.1).

Through the above process, we can render discontinuities and boundaries successfully (in the forward direction). However, the above process alone cannot be seamlessly integrated into the Gaussian Splatting pipeline. This is because, owing to the incorporation of the boundary information, the modified \(\)-blending function now is no longer continuous everywhere. This can cause Gaussian Splatting, naively integrated with the above process, to become non-differentiable, and thus results in difficulties during the learning process of the 3D scene representation. To tackle this problem, in our framework, we further introduce a Bezier-boundary gradient approximation strategy, by which during backpropagation, we can enable gradients to properly pass through the modified \(\)-blending function, and thus keep our framework to be still "differentiable". With the above designs properly involved, our DisC-GS framework can finally enable Gaussian Splatting to render discontinuities and boundaries properly, seamlessly addressing its original key intrinsic limitation.

The contributions of our work are summarized as follows. 1) We proposed DisC-GS, an innovative framework for the novel view synthesis task. To the best of our knowledge, this is the first effort that enables Gaussian Splatting to represent and render boundaries and discontinuities properly in its image rendering pipeline, which tackles a key intrinsic limitation of Gaussian Splatting. 2) We introduce several designs in our framework to enable it to render images in a discontinuity-aware manner, while also to keep its "differentiability" in the presence of discontinuities. 3) DisC-GS achieves superior performance on the evaluated benchmarks.

## 2 Related Work

**Novel View Synthesis.** Owing to the wide range of applications, the task of novel view synthesis has received lots of research attention [23; 40; 39; 43; 24; 36; 3; 4; 5; 44; 50; 7; 11; 18; 19; 37; 30; 26; 13; 49; 20; 46; 22; 35; 33; 32; 48; 52; 17]. In the early days, with the emergence of CNN, different works have been proposed to leverage CNN in this task from different perspectives. Among them, Hedman et al.  proposed to use CNN to predict blending weights, and Sitzmann et al.  proposed to seek help from CNN in performing volumetric ray-marching. As time passed, NeRF tends to become a popular way in representing 3D scenes. Specifically, the original version of NeRF is first proposed by Mildenhall et al. in  and after it comes out, a variety of different NeRF-based methods have been further proposed, such as Mip-NeRF , NeRF++ , and Point-NeRF . Despite the increased efforts, a weakness of NeRF-based methods can be that, to render novel-view images in high visual quality, they often still require a slow rendering process [30; 13]. This can negatively affect the usage of these methods in many real-world scenarios.

Considering this, more recently, the Gaussian Splatting technique, which can render novel-view images in good quality while at the same time in real-time speed, as an attractive alternative to NeRF, has gained plenty of research attention. Specifically, Kerbl et al.  proposed to represent a 3D scene as a collection of 3D Gaussian distributions and made the first attempt to perform novel view synthesis using the Gaussian Splatting technique. After that, Huang et al.  pointed out that representing the 3D scene utilizing 3D Gaussian distributions can lead to a viewpoint inconsistency problem. To tackle this problem, they proposed to represent the 3D scene with 2D Gaussian distributions instead. Moreover, Cheng et al.  proposed to seek help from the classical patch matching technique to better guide the densification of Gaussian distributions, and Zhang et al.  formulated a new loss function in the frequency space to better regularize the learning process of Gaussian Splatting.

Different from these existing Gaussian-Splitting-based methods that typically render complete Gaussian distributions during the image rendering process, we here argue that a key limitation of the original Gaussian Splatting technique lies in that, directly rendering the complete Gaussian distributions can lead boundaries and discontinuities in the image to be inaccurately rendered. Considering this, in this work, we propose to enable Gaussian distributions to be "pre-scissored" along desired boundaries before rendered. This for the first time, enables Gaussian Splatting to represent and render discontinuities and boundaries properly.

**Curve Representation.** The idea of representing a curve in a parametric way has been studied in various tasks [34; 27; 38; 16; 28; 8], such as lane detection , trajectory prediction , and text spotting . Here in this work, we design a novel framework, which enables Gaussian Splatting to perform discontinuity-aware novel-view image rendering, via utilizing the cubic Bezier curves to parametrically contour the boundaries in the image plane.

## 3 Preliminary

**Gaussian Splatting.** Gaussian Splatting represents the 3D scene explicitly as a collection of anisotropic Gaussian distributions. In specific, in the collection, each Gaussian is defined with the following attributes: (1) its center \(^{3}\), (2) its covariance matrix \(^{3 3}\), (3) its spherical harmonic (SH) coefficients \(c_{SH}^{3(k+1)^{2}}\) representing its color from different viewpoints (where \(k\) denotes the order of SH), and (4) its opacity \(^{1}\). Regarding the covariance matrix \(\), it is important to ensure \(\) remains positive semi-definite during the learning process of the 3D scene representation. To achieve this, \(\) is expressed as \(=RSS^{T}R^{T}\), where \(R^{3 3}\) is the orthogonal rotation matrix of the Gaussian, and \(S^{3 3}\) denotes the diagonal scale matrix of the Gaussian.

With the 3D scene represented as the collection of Gaussians defined in the above way, to render an image given a target viewpoint, inspired by , each Gaussian in the collection is first projected onto the image plane corresponding to the viewpoint as:

\[^{2D}=PW,\;^{2D}=JW W^{T}J^{T}\] (1)

where \(^{2D}\) and \(^{2D}\) respectively represent the center and the covariance matrix of the projected Gaussian distribution, \(W\) represents the viewing transformation matrix, \(P\) represents the projective transformation matrix, and \(J\) represents the Jacobian of the affine approximation of the projective transformation. After that, to perform image rendering on the image plane, for each pixel \(p\) of the image, its color \(C(p)\) is derived through an \(\)-blending function as:

\[C(p)=_{i=1}^{N}c_{i}_{i}_{j=1}^{i-1}(1-_{j}),\;\;_{i}=_{i}e^{-(p-_{i}^{2D})^{T}(_{i}^{2D})^{- 1}(p-_{i}^{2D}))}\] (2)

where \(N\) represents the number of projected Gaussians that overlap \(p\), \(c_{i}\) represents the color of the \(i\)-th Gaussian calculated from its corresponding SH coefficients, \(_{i}\) represents the opacity of the \(i\)-th Gaussian, \(_{i}^{2D}\) represents the center of the \(i\)-th projected Gaussian, and \(_{i}^{2D}\) represents the covariance matrix of the \(i\)-th projected Gaussian. Note that, no matter whether Gaussian Splatting represents the 3D scene using 3D or 2D Gaussian distributions, the above equations can describe its rendering process consistently. In fact, as also mentioned in , the difference between rendering images from 3D or 2D Gaussians can be reduced to that, when the scene is represented through 2D Gaussians, the scale matrix \(S\) of each of the 2D Gaussians should contain a zero column vector. In this work, we apply our framework to both 2D and 3D Gaussian Splattings, achieving performance improvements as shown in Tab. 2. Yet, as pointed out by , using 3D Gaussians instead of 2D Gaussians to represent the scene can result in a viewpoint inconsistency problem. Thus, in Sec. 4, we first focus on explaining how our framework is applied to 2D Gaussian Splatting, in which we fix the last column of the scale matrix \(S\) of all the Gaussians to be a zero vector. We then discuss the application of our framework on 3D Gaussian Splatting in Sec. 4.3.

**Cubic Bezier curve.** A cubic Bezier curve is a parametric curve that can be formulated by leveraging a list of four ordered control points \([_{0},_{1},_{2},_{3}]\) as:

\[B(t)=(1-t)^{3}_{0}+3(1-t)^{2}t_{1}+3(1-t)t^{2}_{2}+t^{3} _{3}\] (3)

In the above equation, we can set \(t\) for \(B(t)\) to represent a segment of the curve that starts from \(_{0}\) and ends at \(_{3}\). Alternatively, we can set \(t\) to represent the entire curve. In this work, we set \(t\) for \(B(t)\), as any segment of the curve may not be enough to represent the desired boundaries in the whole image plane. Note that when the four control points lie on the same straight line, the Bezier curve formulated by them would also be reduced to that straight line. Thus, besides representing smooth boundaries, the cubic Bezier curves, at their cross-interacting points, can also be used to represent the sharp corners (of human-made items) in the rendered image.

## 4 Proposed Method: DisC-GS

Given a batch of source images of a 3D scene with their corresponding viewpoints, the goal of novel view synthesis is to generate novel-view images accurately. To handle this task, a common way is to first learn a 3D scene representation from the given source images. After that, the novel-view images can be rendered from the learned 3D scene. Recently, via representing the 3D scene through Gaussian distributions, Gaussian Splitting has enabled novel-view images to be generated both in real-time and with high rendering quality. It has thus attracted lots of research attention [30; 26; 13; 49].

Yet, we here argue that Gaussian Splatting has a key intrinsic limitation: it may fail to render discontinuities and boundaries accurately. To tackle this problem, in this work, inspired by [54; 28], we propose a novel framework named DisC-GS, which can seamlessly equip Gaussian Spatting with the discontinuity rendering ability. Specifically, during rendering images from the 3D scene, to render discontinuities properly, DisC-GS enables each Gaussian distribution projected onto the image plane to be first "pre-scissored" along certain desired boundaries before being rendered. However, such a "pre-scissoring" operation by itself can break the differentiability of the framework. Considering this, we further incorporate our framework with a Bezier-boundary gradient approximation strategy. Leveraging this strategy, during the learning process of the 3D scene, we can enable the gradient to properly backpropagate through the "pre-scissoring" operation. Below, we first describe the (forward) image rendering process of DisC-GS, and then explain the Bezier-boundary gradient approximation strategy.

### Discontinuity-aware Image Rendering

In the proposed DisC-GS, to perform discontinuity-aware rendering, we aim to preprocess Gaussian distributions projected onto the image plane by "scissoring" them along boundaries represented by cubic Bezier curves before rendering. To achieve this, we modify the conventional Gaussian Splatting technique through the following three steps.

**Introduction of an additional attribute.** To facilitate the representation of cubic Bezier curves corresponding to the boundaries of each Gaussian distribution projected on the image plane, we introduce an additional attribute. We denote this attribute \(c_{curve}^{4M 2}\), where \(M\) is a user-defined hyperparameter representing the number of Bezier curves. This attribute augments the original four attributes (discussed in Sec. 3) of each Gaussian distribution. Below, we introduce the physical interpretation of \(c_{curve}\). Specifically, for a certain 2D Gaussian distribution representing the 3D scene, denote the first column of its rotation matrix \(R\) to be \(r_{1}\) and the second column of \(R\) to be \(r_{2}\). Over the 3D space, the 2D subspace that this Gaussian distribution lies in can be then described by a 2D coordinate system, which takes the center \(\) of the Gaussian as its origin, the direction of \(r_{1}\) as the direction of its x-axis, and the direction of \(r_{2}\) as the direction of its y-axis. Then for \(c_{curve}\) of this Gaussian distribution, it can be understood as storing a total of \(4M\) points in the above-defined coordinate system. Note that when these \(4M\) points are projected onto the image plane (as discussed

Figure 2: Illustration of the discontinuity-aware rendering process over a single Gaussian distribution. Specifically, over each 2D Gaussian distribution representing the 3D scene, we first introduce it with a new attribute \(c_{curve}^{4M 2}\) (represented by the red and purple points in (a)). Here we set \(M=2\). After that, given a viewpoint, as shown in (b), we project both the Gaussian distribution and the points stored in \(c_{curve}\) onto the image plane corresponding to the viewpoint. Finally, leveraging the modified \(\)-blending function in Eq. 6, we can perform discontinuity-aware rendering and render only the part of the Gaussian distribution masked with the dotted lines in (c). (Best viewed in color.)below), they can then serve as the control points of \(M\) cubic Bezier curves, which represent the desired boundary w.r.t. the current Gaussian distribution.

**Image plane projection of points in \(c_{curve}\).** After introducing \(c_{curve}\) to each Gaussian distribution that represents the 3D scene, given a viewpoint, we project points in \(c_{curve}\) onto the image plane. Specifically, this is achieved in two steps: (1) We first transform each point (stored in \(c_{curve}\)) in the above-defined subspace coordinate system to the coordinate system of the 3D space as:

\[c_{curve}^{3D}[i]=+c_{curve}[i,0] r_{1}^{T}+c_{curve}[i,1] r_{ 2}^{T},i\{0,...,4M-1\}\] (4)

where \(\) is the center of the Gaussian distribution. Note that here, since a column of a rotation matrix is already a unit vector, we can omit the normalization of \(r_{1}\) and \(r_{2}\) and directly transpose them. (2) After deriving \(c_{curve}^{3D}^{4M 3}\) storing the \(4M\) points in the 3D space coordinate system, we can project each point in \(c_{curve}^{3D}\) onto the image plane similar to what we have done in Eq. 1 as:

\[c_{curve}^{2D}[i]=PWc_{curve}^{3D}[i],i\{0,...,4M-1\}\] (5)

where \(P\) represents the projective transformation matrix, and \(W\) represents the viewing transformation matrix. At this point, for each Gaussian projected onto the image plane via Eq. 1, we have gotten the control points of its desired cubic-Bezier-curves-represented boundary, stored in \(c_{curve}^{2D}^{4M 2}\).

**Discontinuity-aware rendering.** Finally, to perform discontinuity-aware image rendering, for each Gaussian distribution projected onto the image plane, we aim to first "scissor" the distribution along the \(M\) cubic Bezier curves formulated based on the \(4M\) control points stored in \(c_{curve}^{2D}\). After that, we would like to only render the remaining parts of the distribution that are not "scissored out". To achieve this, assume that for each projected Gaussian distribution, we have built a binary indicator function \(g()\), which when passed with a pixel \(p\) on the image plane, can output 0 if the pixel is in the "scissored out" area of the distribution, and can output 1 otherwise. We can then perform discontinuity-aware rendering simply via modifying the \(\)-blending function in Eq. 2 as:

\[C(p)=_{i=1}^{N}c_{i}_{i}_{j=1}^{i-1}(1-_{j}),_{i}=_{i}g_{i}(p)e^{-(p-_{i}^{2D})^{T}(_{i}^{2D}) ^{-1}(p-_{i}^{2D}))}\] (6)

where \(g_{i}()\) represents the indicator function w.r.t. the \(i\)-th projected Gaussian. Besides, same as in Eq. 2, \(N\) represents the number of projected Gaussians that overlap \(p\), \(c_{i}\) represents the color of the \(i\)-th Gaussian calculated from its corresponding SH coefficients, \(_{i}\) represents the opacity of the \(i\)-th Gaussian, \(_{i}^{2D}\) represents the center of the \(i\)-th projected Gaussian, and \(_{i}^{2D}\) represents the covariance matrix of the \(i\)-th projected Gaussian. Note that via the above modified \(\)-blending function, for Gaussians that no longer overlap with the pixel \(p\) due to the "scissoring" operation, we can zero out their contributions during calculating the color of \(p\).

Considering the above, the problem of enabling Gaussian Splatting to perform discontinuity-aware image rendering has now been reduced to building the indicator function \(g()\) for each projected Gaussian distribution based on its corresponding \(c_{curve}^{2D}\). Below, we discuss how we build \(g()\). For simplicity, we first consider the case where only one cubic Bezier curve exists per Gaussian distribution. In this case, denote the four control points of the curve \(_{0}=(x_{0},y_{0})\), \(_{1}=(x_{1},y_{1})\), \(_{2}=(x_{2},y_{2})\), and \(_{3}=(x_{3},y_{3})\). Then to build \(g()\), given a pixel \(p=(x_{p},y_{p})\), we just need to determine (judge) whether \(p\) is on the inner side or the outer side of the curve. To achieve this, instead of directly leveraging the parametric representation of the cubic Bezier curve presented in Eq. 3, which may lead the judgment to be non-intuitive, we first leverage the implicitization technique in algebra  to represent the cubic Bezier curve in its implicit representation form as:

\[B_{imp}(x,y)=_{xxx}x^{3}+_{xxy}x^{2}y+_{xyy}xy^{2}+_ {yyy}y^{3}+_{xx}x^{2}+_{xy}xy+_{yy}y^{2}+_{x}x+ _{y}y+_{0}=0\] (7)

where coefficients including \(_{xxx}\), \(_{xxy}\), \(_{xyy}\), \(_{yyy}\), \(_{xx}\), \(_{xy}\), \(_{yy}\), \(_{x}\), \(_{yy}\), \(_{x}\), \(_{y}\), and \(_{0}\) can all be obtained through basic arithmetic operations over the coordinates of the four control points of the curve in \(O(1)\) time complexity (more details are provided in Appendix C). Based on \(B_{imp}(x,y)\), in the case where only one curve exists per Gaussian distribution, we can then build the single-curve indicator function \(g_{sc}()\) intuitively and with \(O(1)\) time complexity as:

\[g_{sc}(_{0},_{1},_{2},_{3};p)=1,&B_{ imp}(x_{p},y_{p})>0,\\ 0,&\] (8)

Above we introduce how we can build the indicator function \(g()\) as \(g_{sc}()\) assuming that each projected Gaussian distribution is only "scissored" along one cubic Bezier curve. Here, in the case where \(M\) curves exist per Gaussian, for each Gaussian, we notice that a pixel can be regarded as in its "scissored out" area as long as the pixel is "scissored out" by at least one out of the \(M\) curves corresponding to the Gaussian. With this in mind, leveraging the \(g_{sc}()\) function above, we can then define \(g()\) in cases where \(M>1\) as:

\[g(p)=_{i=0}^{M-1}g_{sc}(c_{curve}^{2D}[4i],c_{curve}^{2D}[4i+1],c_{curve}^ {2D}[4i+2],c_{curve}^{2D}[4i+3];p)\] (9)

Leveraging the indicator function \(g()\) defined in Eq. 9, along with the modified \(\)-blending function in Eq. 6, we can then enable Gaussian Splatting to perform discontinuity-aware rendering.

### Bezier-boundary Gradient Approximation Strategy

Above we discussed, how, in our framework, we perform discontinuity-aware rendering in the forward direction from the 3D scene representation to the 2D rendered image.

**Problems remain.** Yet, this forward rendering process by itself cannot be seamlessly incorporated into the Gaussian Splatting pipeline. This is because of two reasons. Firstly, to enable the 3D scene representation to be properly learned from the source images of the 3D scene, Gaussian Splatting needs its rendering process to be (backward) differentiable. However, performing discontinuity-aware rendering leveraging the modified \(\)-blending function in Eq. 6, with the discontinuous function \(g()\) in Eq. 9 incorporated, is no longer differentiable. Moreover, according to Eq. 8 and 9, \(g()\) is actually a piecewise constant function. Thus, even in its differentiable segments, the gradients of \(g()\) w.r.t. its inputs are always zero. In other words, even in segments of \(g()\) where its gradients are computable, these consistently zero gradients would fail to guide the update of the function \(g()\)'s inputs stored in \(c_{curve}^{2D}\), and consequently fail to guide the learning process of \(c_{curve}\) introduced in Sec. 4.1.

**The big picture of our proposed strategy.** To tackle the above problems and thus enable Gaussian Splatting to seamlessly render discontinuities, in our framework, we aim to further keep the "differentiability" of the whole discontinuity-aware rendering process. In other words, w.r.t. the discontinuous indicator function \(g()\) that is newly incorporated into the rendering process, we aim to approximate its gradients (partial derivatives) over the control point coordinates stored in \(c_{curve}^{2D}\), in a way that enables the approximated gradients to effectively guide the learning process of the 3D scene representation. To achieve this, inspired by , we propose a Bezier-boundary gradient approximation strategy. Below, to ease our explanation of the strategy, we focus on discussing how we approximate \(^{2D}}\), i.e., the partial derivative of the indicator function \(g()\) over the \(x\) coordinate of the first control point stored in \(c_{curve}^{2D}\). Note that the application of the strategy to the remaining coordinates stored in \(c_{curve}^{2D}\) follows a similar process (more details are provided in Appendix D). Specifically, to approximate \(^{2D}}\), based on the chain rule and according to Eq. 9, denoting \(g_{sc}(c_{curve}^{2D},c_{curve}^{2D},c_{curve}^{2D},c_{curve}^{2D}[ 3];p)\) to be \(g_{sc}^{0}(p)\), we first have:

\[^{2D}}=^{0}(p)}^{0}(p)}{ c_{curve}^{2D}}\] (10)

Then since \(g()\) is clearly differentiable over \(g_{sc}^{0}()\) based on its definition in Eq. 9, we can reduce our problem to approximate \(^{0}(p)}{ c_{curve}^{2D}}\), which is achieved through the following two steps.

**Determining if \(g_{sc}^{0}(p)\) is desired to be modified.** Specifically, to approximate \(^{0}(p)}{ c_{curve}^{2D}}\), we first would like to determine, if the function \(g_{sc}^{0}()\) is desired to be modified at \(p\) or not. This is because, if \(g_{sc}^{0}()\) already outputs a satisfied value at \(p\), we don't need to change \(c_{curve}^{2D}\) to correspondingly modify \(g_{sc}^{0}(p)\). In other words, in such a case, we can simply set \(^{0}(p)}{ c_{curve}^{2D}}\) to be zero.

Denote the loss function used during the learning process to be \(L\). Leveraging both \(^{0}(p)}\) and the current value of \(g_{sc}^{0}(p)\) as the conditions, below, we list the three situations in which \(g_{sc}^{0}(p)\) doesn't need to be further modified: (1) The first situation happens when \(^{0}(p)}=0\), which indicates that \(g_{sc}^{0}()\) given input \(p\) is already in an optimal state. (2) Besides, the second situation happens when \(^{0}(p)}>0\) and \(g_{sc}^{0}(p)=0\). Based on the gradient descent algorithm, this implies that, while we still hope the function \(g_{sc}^{0}()\) to output a smaller value at \(p\), the function \(g_{sc}^{0}()\) already outputs its smallest allowed value. (3) Following the opposite logic of situation (2), the third situation happens when \(_{sc}(p)}<0\) and \(g^{0}_{sc}(p)=1\). In this case, though we still want \(g^{0}_{sc}(p)\) to be larger, \(g^{0}_{sc}()\) at \(p\) already outputs its largest allowed value. In the above three situations, we can directly set \(_{sc}(p)}{ c^{2D}_{curve}}=0\) and omit the approximation performed in the next step.

**Approximating \(_{sc}(p)}{ c^{2D}_{curve}}\).** Besides the above three situations, in the rest cases, for the value of function \(g^{0}_{sc}()\) at \(p\) to be properly modified based on the modification of the value of \(c^{2D}_{curve}\), we aim to properly approximate the partial derivative \(_{sc}(p)}{ c^{2D}_{curve}}\). To achieve this, recall that as a binary indicator function, \(g^{0}_{sc}()\) switches (modifies) its value at \(p\) between 0 and 1 only when its corresponding cubic Bezier curve passes through \(p\). Considering this, below, we first identify: which value we should set (change) \(c^{2D}_{curve}\) to be, so that the value switch of \(g^{0}_{sc}()\) at \(p\) can happen.

To achieve this identification in an intuitive and analytical way, denoting \(p=(x_{p},y_{p})\) and the desired value of \(c^{2D}_{curve}\) to be \(\), based on Eq. 3, we can first derive the following system of equations:

\[x_{p}=(1-t)^{3}+3(1-t)^{2}t(c^{2D}_{curve})+3(1-t)t^{2}( c^{2D}_{curve})+t^{3}(c^{2D}_{curve})\\ y_{p}=(1-t)^{3}(c^{2D}_{curve})+3(1-t)^{2}t(c^{2D}_{curve})+3(1-t)t^{2 }(c^{2D}_{curve})+t^{3}(c^{2D}_{curve})\] (11)

In this system of equations, since \(x_{p}\), \(y_{p}\), and the coordinates in \(c^{2D}_{curve}\) all have known values, we initially regard the second equation in the system as a cubic equation w.r.t. \(t\), as \(t\) is now the only unknown variable in this equation. After solving this cubic equation and with \(t\) also known, we can then regard the first equation in the system as a cubic equation w.r.t. \(\) and solve it. Finally, by solving the above two equations (both in just \(O(1)\) time complexity), we obtain \(S_{}\) as the set of all possible real number solutions for \(\). Based on the solutions' scenarios within \(S_{}\), we approximate \(_{sc}(p)}{ c^{2D}_{curve}}\) in three different ways below.

(1) The "no side" situation. The first situation happens when \(S_{}=\). In this case, we simply set \(_{sc}(p)}{ c^{2D}_{curve}}=0\). This is because, the empty nature of \(S_{}\) implies that, there exists no proper real-number value that we can change \(c^{2D}_{curve}\) to be, such that \(g^{0}_{sc}(p)\) can be desirably modified (i.e., either from 0 to 1 or from 1 to 0). We thus simply do not encourage \(c^{2D}_{curve}\) to change.

(2) The "single side" situation. The second situation occurs when all solutions in \(S_{}\) lie on the same side of \(c^{2D}_{curve}\) (i.e., all larger or all smaller than \(c^{2D}_{curve}\)). In this situation, let \(\) denote the solution in \(S_{}\) that is nearest to \(c^{2D}_{curve}\). Adjusting \(c^{2D}_{curve}\) towards \(\) then implies the least-cost plan, facilitating the modification of \(g^{0}_{sc}(p)\) in a desired manner. With this in mind, to encourage \(c^{2D}_{curve}\) to approach \(\), inspired by previous studies , we approximate \(_{sc}(p)}{ c^{2D}_{curve}}\) via performing linear interpolation between \(c^{2D}_{curve}\) and \(\) as:

\[_{sc}(p)}{ c^{2D}_{curve}}=_{sc}(p)}-g^{0}_{sc}(p)}{(-(c^{2D}_{curve}) )+},\,\,_{sc}(p)}=1,& g^{0}_{sc}(p)=0,\\ 0,&\] (12)

In the above equation, we set \(=10^{-5}\) if \((-(c^{2D}_{curve}))>0\) and we otherwise set \(=-10^{-5}\). \(\) here is a small number that is used to avoid the gradient exploding problem to happen when the distance between \(\) and \(c^{2D}_{curve}\) is too short.

(3) The "both sides" situation. The third situation happens when some solutions in \(S_{}\) are on the left side of \(c^{2D}_{curve}\), while other solutions are on the right side of \(c^{2D}_{curve}\). In this situation, we can achieve the desired modification of \(g^{0}_{sc}(p)\) via either moving \(c^{2D}_{curve}\) to its left or right side. Thus, unlike the scenario described in the above situation (2) where we only consider \(\) from a single side of \(c^{2D}_{curve}\), here, denoting \(}\) as the value that is nearest to \(c^{2D}_{curve}\) from its left side, and \(}\) as the value that is nearest to \(c^{2D}_{curve}\) from its right side, we approximate \(_{sc}(p)}{ c^{2D}_{curve}}\) as:

\[_{sc}(p)}{ c^{2D}_{curve}}=_{sc}(p)}-g^{0}_{sc}(p)}{(}-(c^{2D}_{curve}) )+_{1}}+_{sc}(p)}-g^{0}_{sc}(p)}{( }-(c^{2D}_{curve}))+_{2}}\] (13)In the above equation, we define \(^{0}(p)}\) in the same way as in Eq. 12. Besides, both \(_{1}\) and \(_{2}\) are defined in the similar way as \(\) in Eq. 12.

In summary, taking \(^{2D}}\) as an example, the above discussion explains how our proposed strategy approximates the gradient of \(g()\) with respect to the point coordinates stored in \(c_{curve}^{2D}\). With the incorporation of this strategy into our framework, we keep the "differentiability" of the whole rendering process, allowing Gaussian Splatting to seamlessly perform discontinuity-aware rendering.

### DisC-GS on 3D Gaussian Splatting

Above, we focus on describing how we use 2D Bezier curves in our DisC-GS framework and correspondingly apply DisC-GS on 2D Gaussian Splatting. Here in this subsection, we further describe how we use 3D Bezier curves in our DisC-GS framework and apply DisC-GS on 3D Gaussian Splatting. Specifically, the transition from 2D to 3D Bezier curves in DisC-GS requires only two minimal modifications. (1) Firstly, for each Gaussian representing the 3D scene, the control points of its Bezier curves are stored directly in the 3D spatial coordinate system rather than in a 2D subspace. Note that, this modification can be very simply made. Specifically, for each Gaussian in the 3D space in our DisC-GS framework, we only need to use \(c_{curve}^{3D}^{4M 3}\) instead of \(c_{curve}^{4M 2}\) to represent the control point coordinates of its 3D Bezier curves. In other words, for each 3D Gaussian, we only need to introduce it with \(c_{curve}^{3D}\) instead of \(c_{curve}\) as its new attribute. (2) Moreover, since we already directly introduce \(c_{curve}^{3D}\) as the new attribute for each 3D Gaussian in our framework, during rendering, we omit Eq. 4 above in Sec. 4.1, which originally is used to acquire \(c_{curve}^{3D}\) from \(c_{curve}\). Overall, the above two modifications are sufficient to incorporate DisC-GS with 3D instead of 2D Bezier curves.

### Overall Training and Testing

In DisC-GS, during training (i.e., learning the 3D scene representation from the source images), we follow a similar process as the typical Gaussian Splatting technique . The involvement of the strategy introduced in Sec. 4.2 keeps the "differentiability" of our framework. During testing (i.e., image rendering), we use the discontinuity-aware image rendering process introduced in Sec. 4.1.

## 5 Experiments

**Datasets.** To evaluate the efficacy of our proposed framework DisC-GS, following previous Gaussian Splatting works [30; 49], we evaluate our framework on a total of 13 3D scenes, which include both outdoor scenes and indoor scenes. Specifically, among these 13 scenes, 9 of them are from the Mip-NeRF360 dataset , 2 of them are from the Tanks&Temples dataset , and 2 of them are from the Deep Blending dataset . We also follow previous works [30; 49] in their train-test-split.

**Evaluation metrics.** Following [30; 49], we use the following three metrics for evaluation: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) .

**Implementation details.** We conduct our experiments on an RTX 3090 GPU and develop our code mainly based on the GitHub repository  provided by Kerbl et al . Moreover, we also get inspired by [35; 52; 46] during our code implementation, and make use of the LPIPS loss during our training process. Furthermore, for the newly introduced attribute \(c_{curve}^{4M 2}\), we set its initial learning rate to 2e-4, and set the hyperparameter \(M\) to 3. Besides, in the densification procedure of our framework, when a Gaussian is cloned/splitted into two new Gaussians, we assign both the new Gaussians with the same attribute \(c_{curve}\) as the original one.

### Experimental Results

In Tab. 1, we compare our approach (applied on 2D Gaussian Splatting) with existing novel view synthesis methods evaluated on the same 13 3D scenes and report the PSNR, SSIM, and LPIPS results. Our framework consistently outperforms other methods on all three metrics and across various datasets, showing its effectiveness. We also show qualitative results in both Fig. 1(c) and Appendix B. As shown, whether representing the 3D scene through 3D or 2D Gaussian distributions,the conventional Gaussian Splatting technique often struggles to render boundaries and discontinuities clearly and with high quality. In contrast, our framework can achieve good rendering quality, even in regions of the image containing numerous boundaries and discontinuities. This further underscores the efficacy of our approach.

### Ablation Studies

We conduct extensive ablation experiments on the Tanks&Temples dataset. **More ablation studies w.r.t. the image areas with rich boundaries, the Bezier-boundary gradient approximation strategy, the hyperparameters, and the rendering speed of our framework are in Appendix A.**

**Impact of representing the scene with 2D or 3D Gaussians in DisC-GS.** In Sec. 4.1 and Sec. 3, we focus on introducing how we apply DisC-GS on 2D Gaussian Splatting. After that, in Sec. 4.3, we introduce how DisC-GS can be applied on 3D Gaussian Splatting in a similar way. Here to verify the generality of our framework, we test applying our framework on both 2D and 3D Gaussian Splatting. As shown in Tab. 2, our framework, when applied on both 2D and 3D Gaussian Splattings, can consistently achieve performance improvements, demonstrating the generality of our framework.

**Impact of the number of control points per Bezier curve.** In our framework, inspired by [16; 34], we represent boundaries in the image with the cubic Bezier curve, each of which is formulated by leveraging 4 control points. Here we evaluate formulating each Bezier curve by other numbers of control points, and report the results in Tab. 3. As shown, our framework gets optimal performance when the number of control points per Bezier curve is set to 4, and we thus formulate each Bezier curve by utilizing 4 control points in our experiments. Besides, with different choices of the number of control points per Bezier curve from 2 to 5, our framework outperforms the previous state-of-the-art method consistently. This shows the robustness of our framework to the number of control points per Bezier curve.

## 6 Conclusion

In this paper, we have proposed an innovative novel view synthesis framework DisC-GS, which for the first time, enables Gaussian Splatting to properly represent and render discontinuities and boundaries in its image rendering process. Moreover, to keep the "differentiability" of our framework, we further introduce our framework with a Bezier-boundary gradient approximation strategy. Our framework consistently achieves superior performance across different evaluation benchmarks.

**Limitations.** While our framework enables Gaussian Splatting to perform discontinuity-aware rendering, we acknowledge that same as existing Gaussian Splatting approaches, our framework still holds certain limitations, such as challenges in rendering large scenes.

   &  &  &  \\   & SSIM\(\) & PSNR\(\) & LPIPS\(\) & SSIM\(\) & PSNR\(\) & LPIPS\(\) & SSIM\(\) & PSNR\(\) & LPIPS\(\) \\  Plenoxels  & 0.719 & 21.08 & 0.379 & 0.626 & 23.08 & 0.463 & 0.795 & 23.06 & 0.510 \\ INGP-Base  & 0.723 & 21.72 & 0.330 & 0.671 & 25.30 & 0.371 & 0.797 & 23.62 & 0.423 \\ INGP-Big  & 0.745 & 21.92 & 0.305 & 0.699 & 25.59 & 0.331 & 0.817 & 24.96 & 0.390 \\ Mip-NeRF360  & 0.759 & 22.22 & 0.257 & 0.792 & 27.69 & 0.237 & 0.901 & 29.40 & 0.245 \\
3D-GS  & 0.841 & 23.14 & 0.183 & 0.815 & 27.21 & 0.214 & 0.903 & 29.41 & 0.243 \\ Surf splatting  & 0.837 & 23.42 & 0.202 & 0.804 & 27.03 & 0.239 & 0.895 & 28.89 & 0.261 \\ FreGS  & 0.849 & 23.96 & 0.178 & 0.826 & 27.85 & 0.209 & 0.904 & 29.93 & 0.240 \\ GES  & 0.836 & 23.35 & 0.198 & 0.794 & 26.91 & 0.250 & 0.901 & 29.68 & 0.252 \\ Mip-Splitating  & 0.851 & 23.78 & 0.178 & 0.827 & 27.79 & 0.203 & 0.904 & 29.69 & 0.248 \\  Ours & **0.866** & **24.96** & **0.120** & **0.833** & **28.01** & **0.189** & **0.907** & **30.42** & **0.199** \\  

Table 1: Performance comparison on the Tanks&Temples, Mip-NeRF360, and Deep Blending datasets.

  Method & SSIM\(\) & PSNR\(\) & LPIPS\(\) \\ 
2D Gaussian Splatting & 0.836 & 23.30 & 0.205 \\
2D Gaussian Splatting + Ours & 0.866 & 24.96 & 0.120 \\ 
3D Gaussian Splatting & 0.841 & 23.14 & 0.183 \\
3D Gaussian Splatting + Ours & 0.863 & 24.67 & 0.123 \\  

Table 2: Evaluation of our framework on both 2D and 3D Gaussian Splattings.

  Method & SSIM\(\) & PSNR\(\) & LPIPS\(\) \\ 
2 control points per curve & 0.853 & 24.14 & 0.138 \\
3 control points per curve & 0.861 & 24.58 & 0.127 \\
4 control points per curve & 0.866 & 24.96 & 0.120 \\
5 control points per curve & 0.863 & 24.68 & 0.126 \\  

Table 3: Evaluation on the number of control points per Bézier curve.