# PERIA: Perceive, Reason, Imagine, Act via Holistic Language and Vision Planning for Manipulation

Fei Ni\({}^{1}\) Jianye Hao\({}^{1,2}\)1 Shiguang Wu\({}^{2}\) Longxin Kou\({}^{1}\) Yifu Yuan\({}^{1}\) Zibin Dong\({}^{1}\)

**Jinyi Liu\({}^{1}\) Mingzhi Li\({}^{1}\) Yuzheng Zhuang\({}^{2}\) Yan Zheng\({}^{1}\)1 College of Intelligence and Computing, Tianjin University \({}^{2}\)Huawei Noah's Ark Lab**

###### Abstract

Long-horizon manipulation tasks with general instructions often implicitly encapsulate multiple sub-tasks, posing significant challenges in instruction following. While language planning is a common approach to decompose general instructions into stepwise sub-instructions, text-only guidance may lack expressiveness and lead to potential ambiguity. Considering that humans often imagine and visualize sub-instructions reasoning out before acting, the imagined subgoal images can provide more intuitive guidance and enhance the reliability of decomposition. Inspired by this, we propose **PERIA**(**PErceive, Reason, Imagine, Act**), a novel framework that integrates holistic language planning and vision planning for long-horizon manipulation tasks with complex instructions, leveraging both logical and intuitive aspects of task decomposition. Specifically, we first perform a lightweight multi-modal alignment on the encoding side to empower the MLLM to perceive visual details and language instructions. The MLLM is then jointly instruction-tuned with a pretrained image-editing model to unlock capabilities of simultaneous reasoning of language instructions and generation of imagined subgoals. Furthermore, we introduce a consistency alignment loss to encourage coherent subgoal images and align with their corresponding instructions, mitigating potential hallucinations and semantic conflicts between the two planning manners. Comprehensive evaluations across three task domains demonstrate that PERIA, benefiting from holistic language and vision planning, significantly outperforms competitive baselines in both instruction following accuracy and task success rate on complex manipulation tasks. The details and visualizations are available at the homepage.

## 1 Introduction

Recent advances in vision-language models (VLMs), such as BLIP  and LIV , enable open-vocabulary visual recognition and multi-modal alignment, showing promise in robotic manipulation tasks specified human-provided language instructions . For semantically clear and concise instructions, such as "pick the red block on the green one", robotic agents can easily understand and complete the task in a single step using action primitives. However, when instructions become more general and complex, such as "stack the blocks as a pyramid and each layer in one color", the manipulation task can span long horizons and implicitly encapsulate multiple sub-tasks separated by action primitives, posing a major obstacle in instruction following. Current approaches often resort to decomposing complex instructions into manageable subtasks, either through language planning or vision planning based on the decomposed modality. Language planning, the more common approach, decomposes into progressive stepwise sub-instructions, which can be either predefined skill librariesin natural language [3; 7] or latent codebooks . On the other hand, vision planning, a more recent development, decomposes complex instructions into coherent subgoal images as keyframes [9; 10], serving as visual milestones to provide more intuitive and expressive guidance for action execution.

Language planning focuses on _"how to act"_ and the sub-instructions outline the necessary procedural action process of the task completion, emphasizing the temporal dependencies and causal relationships between decomposed stepwise sub-instructions. On the other hand, vision planning concentrates on _"what to act towards"_ and intuitive and grounded subgoal images with rich spatial and contextual information can enable robot agents to more easily understand what intermediate landmarks and visual anchors should achieve towards task completion. From a cognitive perspective, humans rely on a symbiotic operation of the brain's hemispheres , with the left primarily associated with logical _language-based reasoning_, and the right is linked to intuitive _visual-based imagining_. For humans, language planning and vision planning are often intertwined and performed simultaneously, involving either imagining the desired intermediate goals and then reasoning about the required plans to achieve them, or first reasoning out necessary stepwise plans and then imagining corresponding resulting images. Inspired by this, a natural question arises: _Can we develop a framework that emulates this cognitive synergy by simultaneously performing language planning and vision planning for robotic manipulation tasks involving complex instructions just like humans?_

For this, we propose **PERIA**(**PEreceive**, **Reason**, **Imagine**, **Act**), a novel framework that integrates multi-modal large language model (MLLM) and diffusion model to enable language-based reasoning and visual-based imagining respectively, leveraging holistic language planning and vision planning for long-horizon manipulation tasks with general complex instructions. Specifically, we first train the MLLM's perception ability by fine-tuning the encoder side's projection layer to align the text and vision modalities in the LLM's hidden layers in a lightweight manner, avoid the potential hallucinations and enhance the grounding ability. Next, we perform instruction tuning to simultaneously equip PERIA reasoning and imagination capabilities by explicitly adding additional image tokens after the reasoning phase and extracting rich latent image representations from MLLM to guide the generation of corresponding subgoal images. Moreover, We also introduce an alignment loss between reasoned sub-instructions and imagined subgoal images to enhance the consistency and accuracy of vision and language planning, jointly updated with generation and reasoning losses. In this way, vision planning provides a visualization of language planning, offering more intuitive guidance to avoid potential confusion. Language planning, in turn, provides reliable logical guidance at the semantic level for vision planning, preventing semantic conflicts in the generation of coherent image chains. The comprehensive evaluation across three typical long-horizon manipulation tasks demonstrates that PERIA enjoy the accuracy of instruction following and synergistic combination significantly improves decomposition accuracy and task success rate compared to existing methods that rely solely on either language or vision planning alone. The contributions of this work are as follows:

* We propose PERIA, a novel framework that integrates holistic language planning and vision planning, leveraging the logical and intuitive decomposition of general complex instructions.
* We encourage MLLM to output rich latent visual tokens to guide the diffusion model to generate images and further explicitly align between language instructions and visual subgoals, simultaneously developing the MLLM's reasoning and the diffusion model's imagination capabilities.

Figure 1: Overview of PERIA (Perceive, Reason, Imagine, Act), inspired by the human cognitive process of following complex instructions, which involves perceiving environment and tasks, reasoning the required language plans, and imagining the intermediate subgoal images before acting.

* PERIA demonstrates significant improvements in instruction following and task success rate on complex manipulation tasks compared to existing methods that rely on either language or vision planning alone, establishing a promising and inspiring paradigm for long-horizon manipulation.

## 2 Related Work

### Hierarchical Planning for Long-horizon Manipulation

Embodied manipulation tasks with general instructions often span multiple subtasks and long horizons, making direct end-to-end action prediction challenging due to compounding errors without intermediate guidance [12; 13; 14; 15]. Recent works adopt hierarchical planning, decomposing complex instructions into sequential sub-tasks to execute. Language planning like LISA  and Xskill  decompose the general instruction based on the latent skill codebook discovered during training. SayCan  and EmbodiedGPT  both leverage LLM to enable reasoning into sequential interpretable instructions in natural languages. Vision planning, a more recent development, decomposes complex instructions into sequential subgoal images. CoTDiffusion  utilizes diffusion models to translate multi-modal prompts into coherent subgoal images in a chain-of-thought manner, serving as visual milestones that are challenging to describe using language alone. While existing works rely solely on either language or vision planning, our PERIA framework enables simultaneous language and vision planning, harnessing the strengths of both approaches to provide a comprehensive, multi-modal guide that enhances the accuracy of instruction decomposition and following.

### LLM for Robotics Manipulation

With the tremendous success of LLMs, there has been a surge in research exploring their capabilities for robotics manipulation, such as SayCan , Inner Monologue . PAR  leverages a vision language model(VLM) as a captioner for visual observations and the generated captions are fed into LLM for language planning. ViLA  and CoPA  follow a similar pipeline but replace LLM and VLM with more advanced GPT4V  with stronger visual reasoning capabilities. EmbodiedGPT  employs a pre-trained open-sourced LLaMA model  as the language model for instruction tuning on collected robotics data, enhancing reasoning and planning capability specifically for embodied scenarios. PERIA introduces image generation as an additional supervision signal to encourage the MLLM to perceive more detailed visual details, reducing hallucinations and errors in reasoning. The generated images in vision planning also provide a more intuitive guide that further enhances the accuracy of instruction decomposition and improves instruction following performance.

### Image Generation for Robotics Manipulation

Inspired by recent development of recent text-to-image models [23; 24; 25], many works have begun to explore the visualization of manipulation tasks to guide robot action execution. LfVoid  enables the editing of original observations to obtain goal images based on natural language instructions to provide reward signals. SuSIE  similarly leverages an image-editing diffusion model to act as a high-level planner by proposing intermediate subgoals that a low-level controller can accomplish. LfVoid and SuSIE are limited to single-step sub-instructions, while CoTDiffusion  supports various instruction modalities and generates coherent subgoal image chains using a semantic alignment module. These works demonstrate that subgoal images can provide more detailed and intuitive guidance than language-only instructions. However, they do not incorporate LLMs for reasoning and are prone to failure and semantic conflicts without logical guidance. Our PERIA framework leverages the prior knowledge in MLLMs to assist in generating promising sequential images, enhancing consistency with complex task instructions and improving instruction following.

## 3 Method

By leveraging MLLM and diffusion-based image editing models, PERIA enables holistic language planning and vision planning for stepwise language instructions and visual subgoal images, serving as language milestones and visual anchors to guide action execution in long-horizon tasks. We first introduce the lightweight alignment of language and vision modalities on the encoding side of the MLLM to achieve precise **Percive** ability in Section 3.1. We then illustrate how to perform instruction tuning on the MLLM to enable **Reason** for language planning in Section 3.2 and how to jointly train with a diffusion model to **Imagine** coherent subgoal images aligned with corresponding instructions in Section 3.3. Moreover, we leverage an explicit alignment between instructions and images to achieve a synergistic effect between language and vision in Section 3.4. Since our focus is not on the low-level policy, please refer to Appendix E for the implementation details of **Act**.

### Perceive: Encoding-side LLM-centric Multimodal Alignment

To enable embodied robot agents to effectively perceive and comprehend visual scenes, a straightforward approach is to use an off-the-shelf Vision-Language Model (VLM) as an image captioner. However, the information bottleneck between the LLM and VLM limited by language modality, results in missing visual details, which is particular problematic in robotics manipulation tasks that require precise visual understanding. To address this limitation, we leverage image captioning as a training task for LLM-centric multimodal alignment to encourage visual representations compatible with the text feature space within the LLM, extending it to a MLLM to allow for a more precise and detailed comprehension of visual scenes. Specifically, we utilize the privileged information available in simulation environments to create a large-scale dataset of pairwise ground-truth data through three types of captioning tasks. First, the _single-frame scene description scenario_ focuses on understanding a single observation frame, where the MLLM is tasked with providing a brief description covering aspects such as object recognition, size identification, number counting, color understanding, and spatial relationships. Second, _action recognition between consecutive keyframes scenario_ presents the MLLM with two consecutive keyframes, requiring it to understand the visual difference between them and recognize the executed action, enhancing the perception of spatial relationships and action dynamics at the instruction level. Moreover, _short demonstration understanding scenario_ involves processing a given short demonstration of frame sequences, strengthening the MLLM's temporal relationship understanding and grounding ability. These carefully designed captioning tasks with the high-quality training data, enable the MLLM to develop a strong foundation in visual perception and understanding for embodied manipulation tasks.

Specifically, initialized from a pre-trained LLM, the MLLM contains a visual encoder \(\)(_e.g.,_ CLIP-L ) to extract the visual features \(f\), and an projection layer \(\) to project \(f\) into the language modality. We follow the training of LLaVA  with cross-entropy loss (CELoss) as:

\[&=\{x_{1},x_{2},...,x_{l}\},=\{v_{1},v_{2},...,v_{n}\},\\ }&=(\{x_{1},...x_{t-1}\}|[ \,,\,(f=\{(v_{i})\}_{i=1}^{n})]),\\ _{}&=_{t=1}^{l }(},x_{t})\] (1)

Figure 2: Overview of **PERIA**. PERIA first learns to align the vision and language on encoding side of MLLM for perceiving. Then PERIA performs instruction tuning to MLLM jointly with diffusion model in an end-to-end manner to unlock the reasoning and generation ability for holistic language planning and vision planning. \(\) and \(@paragraph\) show the module is trainable and frozen, respectively.

where \(\) can be the image caption for features alignment and \(l\) is number of word tokens. \(n\) is the numbers of the images \(\) fed in MLLM, which can be differ in different captioning tasks. To perform the lightweight alignment, we freeze the weights of both the vision encoder \(\) and LLM, and only update the parameters of \(\) that encourage to map the image features into a shared latent space that is compatible with the MLLM's hidden representations. The alignment of visual and language modalities on the encoding side can effectively alleviates hallucinations and lays the foundational perception abilities for generating more grounded language and vision planning. For more detailed categorical analysis of improvement benefiting from captioning task, please refer to Appendix F.1.

### Reason: Instruction Tuning for Language Planning

With the initial coarse alignment of visual and language on the encoding side, we proceed to instruction tuning to encourage the MLLM to learn how to decompose complex instructions for language planning. These general task instructions \(\) can be categorized into two types based on the modalities involved: 1) text-only instructions, such as "sort blocks into bowls according to the matching colors" which can be directly processed by the language encoder; and 2) multi-modal instructions that consist of interleaved language and images of a single object or whole observation, as suggested by VIMA-BENCH , which are more expressive and challenging to understand. For instance, consider an interleaved multi-modal prompt such as "Stack objects <img> in this order <img> <img>", where <img> serves as a placeholder for the corresponding images, which can be images of blocks or other objects in the observation. With the benefit of encoding-side alignment via several captioning scenarios across frames and videos, MLLM equipped with the input projection layer can handle multi-modal instructions including interleaved text, images, and even video frames.

Then we design an instruction prompt \(\) template such as: "Given the current observation <img> and the general task instruction [\(\)], can you provide a brief and concise sub-instruction about how to act next?". We collect the stepwise language instructions \(\) as the groundtruth response for the language planning task specified with the observation \(o\), the prompt \(\) and the general instruction \(\). The instruction tuning loss for language planning is defined as follows:

\[&=\{e_{1},e_{2},...,e_{l}\},=\{o,v_{1},v_{2},...,v_{n}\},\\ e^{}_{t}&=(\{e_{1},...,e_{t-1}\} [,,(f=())]),\\ _{}&=_{t=1}^{l} (e^{}_{t},e_{t})\] (2)

where \(e\) are the word token of stepwise instruction, and \(v\) are the possible \(n\) images from multi-modal instruction. The text instruction with the <img> token and the corresponding image are processed by the aligned language encoder and image encoder respectively and then are unified fed into LLM for reasoning. The MLLM follows the standard auto-regressive training for the next token prediction and then can be regarded as a visual assistant for various tasks such as visual question answering. To perform instruction tuning, we fine-tune the MLLM using the LoRA technique  while keeping the encoding side frozen, including the visual encoder and its projection layer. Additionally, we employ two kinds of prompts to require MLLM to generate the next sub-instruction for the single step or all the stepwise instructions in order respectively. Two modes can be randomly switched during the instruction tuning and can effectively encourage MLLM to perform single-step and multi-step sequential language planning for closed-looped and open-looped control respectively.

### Imagine: Decoding-side Synergistic Training for Vision Planning

Considering the phrase _a picture is worth a thousand words_, subgoal images could provide higher expressive capabilities for conveying subtasks compared to sub-instructions with complex language only. Inspired by this, we integrated pre-trained conditional diffusion models to convert decomposed sub-instructions into coherent visual subgoal plans. While a natural approach would be to directly use the text instructions or captions as prompts for the image editing model, shown in Figure 3, relying solely on decoded text instructions as conditions may lead to an information bottleneck. The expressiveness of the instructions can be limited, and information loss may occur, as it is confined to the language modality. Inspired by [30; 31], to bridge the gap between the language and vision modalities, we introduce \(N\) special [IMG] tokens in the vocabulary codebook of the MLLM. These special tokens have trainable word embeddings and should be predicted after the generated language instructions jointly during the reasoning phase, shown in Figure 2. These appended visual tokens[IMG] are treated as latent imagination of subgoal image from the MLLM and we employ an output image projection module \(\) to transform them into actual visual guidance \(\) for diffusion model:

\[=(\{w_{}+h_{}\},q)\] (3)

where \(w\) is the word embedding of language instructions and \(h\) is the hidden state from the last layer of MLLM before image projection layer of [IMG], conditioned on learnable query embeddings \(q=\{q_{1},...,q_{L}\}\), where \(L\) is the token numbers setting from the pre-trained diffusion model. The transformation over \(w\) can be seen as a general representation from language modality, while \(h\) represents a more grounded visual imagination that aligns with the language planning within the MLLM's reasoning. To simultaneously fine-tune the diffusion model and the MLLM, we employ the generation loss between the generated image and the groundtruth image. Our image editing model is based on latent diffusion, which learns the noise latent \(z_{t}\) at the denoising timestamp \(t\) to reconstruct the groundtruth goal image. The generation loss is to learn the UNet \(_{}\) that predicts the added noise based on the input image \(v\) and the visual imagination guidance \(\) from the MLLM, formulated as:

\[_{}=_{o,v,,(0,1),t}[||-_{}(z_{t},t,v,)||_{2}^{2}]\] (4)

### Enhancing Consistency between Vision and Language Planning

To further enhance the consistency between vision and language planning, we introduce an additional alignment objective between generated language instructions and visual images, as illustrated in Figure 2. Specifically, we feed both the generated image \(v_{t+1}\) and the current observation \(o_{t}\) at planning step \(t\) into the MLLM and prompt it with understanding the differences between the two frames, which is exactly the _action recognition_ captioning task in the perceive phase of PERIA Section 3.1. The response output \(}_{t}\) generated by the MLLM is compared with the groundtruth stepwise language instruction \(_{t}\) for consistency, and can be formulated as the alignment consistency loss:

\[ =\{_{t}\}_{t=0}^{T},=\{(o_{t},v_{t+1} )\}_{t=0}^{T},\] (5) \[}_{t} =(,(f=\{(o_{t}), (v_{t+1})\}),\] \[_{} =_{t=0}^{T}(}_{t}, _{t})\]

The additional alignment task reinforces the synergy between vision and language planning, ensuring that generated subgoal images and text instructions are consistent and mutually informative, alleviating the compounding errors that may arise in long-horizon tasks due to inconsistencies. Vision planning provides a visualization of language planning, offering more intuitive guidance and reducing potential confusion or ambiguity. Conversely, language planning provides logical guidance at the semantic level for vision planning, preventing semantic conflicts during the generation of coherent image chains. This synergistic approach leverages the complementary strengths of vision and language, enabling PERIA to produce plans that are both visually grounded and semantically meaningful.

Figure 3: Three pipelines of MLLM for generation images. PERIA () leverage visual tokens extracted from the MLLM during language planning serve as more expressive guidance for subgoal imagination compared to captions () or decomposed instructions () in language only.

## 4 Experiments

### Experiment Setup

Benchmark & TasksTo provide comprehensive evaluations, we conduct experiments across three typical long-horizon manipulation environments. More benchmark and task details are in Appendix A.

* **LoHoRavens**: is a Ravens-based benchmark consisting of 11 long-horizon language-conditioned tasks categorized into _Stacks_, _Sort_, and _Matching_. Original tasks all involve manipulating **Bowls&Blocks** and we additionally develop a more complex **Letters** scenario including 9 tasks of _Shape_, _Orders_, and _Spell_ to further diversify the instruction and increase task difficulties.
* _Rearrange_, _Constraints_, and _Follows_, specified by interleaved language and images of object or ultimate goal.

BaselinesTo more clearly and comprehensively evaluate the effectiveness of different approaches, we categorize the baselines into three types based on their specific planning methods as follows:

* _End-to-end_: We choose **CLIPT**, one of the most widely used end-to-end language-conditioned imitation learning framework in Ravens-like manipulation benchmarks. CLIPTort directly take the high-level language instructions as input to predict the action without a planner.
* _Language Planning_: We select several representative language planning methods that decompose general high-level instructions into stepwise instructions. **LISA** trains a skill predictor to combine the discovered implicit skill codebooks for complex instructions. **PAR** (Planner-Actor-Reporter) replaces the latent skill planner with an LLM, using the VLM as a reporter for visual observations. The instruction and the generated captions are then fed into the LLM for language planning. **EmbodiedGPT** follows a similar pipeline but replaces LLM and VLM with more advanced MLLM with stronger visual reasoning capabilities after instrution tuning.
* _Vision Planning_: **SuSIE** incorporating a pretrained image-editing models to generate goal images for action prediction but only support simple single-step instructions. **CoTDiffusion** leverage a semantic alignment module within the diffusion model to enable the sequential subgoal image generation for complex general instructions. For more details, please refer to Appendix C.

### Main Quantitative Results of Success Rate

We begin by comparing the performance of PERIA and baselines in solving long-horizon tasks across three typical task domains. The baselines can be categorized into three types of planners: _e2e planner_, _language planner_, and _visual planner_. As shown in Table 1, PERIA significantly outperforms other baselines in terms of success rate. As expected, the end-to-end learning method performs the worst due to the lack of intermediate guidance, making it difficult for the policy to follow general instructions for long-horizon tasks. In contrast, the _visual planner_ paradigm, which explicitly

Figure 4: The illustrating examples of holistic language and vision planning for general instructions, with stepwise sub-instructions and coherent subgoal images enhancing the instruction following.

decomposes tasks into stepwise instructions and employs a hierarchical framework consisting of a language planner and a language-conditioned policy, shows more promise and demonstrates a clear advantage over the end-to-end approach. Within the _visual planner_ category, PAR and EmbodiedGPT both leverage the common sense knowledge from LLM and significantly outperform LISA, which uses a skill predictor for latent skill codebook rather than LLM. Furthermore, although both PAR and EmbodiedGPT are based on LLaMA, EmbodiedGPT employs a visual projector to expand the LLM to an MLLM for more precise perception and reasoning capabilities, while PAR applies a captioner to convert visual images into the language modality for reasoning, which may impact the accuracy of reasoning and task success rate to some extent. The _visual planner_ paradigm, which generates intermediate keyframes, offers more intuitive guidance compared to language planning, and its advantage is more evident in VIMA-BENCH, where sub-tasks are challenging to describe sufficiently using language-only instructions. CoTDiffusion supports generating coherent subgoal images for complex instructions, resulting in performance gains compared to SuSIE. But CoTDiffusion does not explicitly reason about the instructions, which can lead to semantic inconsistencies in the generated subgoal images, causing it to still underperform compared to our algorithm. In contrast, our PERIA algorithm introduces an MLLM for explicit reasoning and generation, providing more sufficient and reliable intermediate guidance for instruction following in long-horizon tasks.

### Further Analysis

Accuracy of Language PlanningWe compare the accuracy of language planning with two evaluation metrics: the token accuracy which directly calculates the token-level matching rate between decomposed stepwise instructions and the groundtruth instructions, and the semantic similarity by calculating the embeddings distance of two instructions from pre-trained text encoder like CLIP. Our focus here is on generative language planning using LLMs and exclude LISA from this comparison. As illustrated in Table 2, PERIA demonstrates the highest accuracy in both token-level and semantic-level comparisons. Although PAR introduces LLMs for language planning, it relies on an isolated, out-of-the-shell VLM as a captioner to convert visual observations into language descriptions, which may cause details missing during hard captioning. EmbodiedGPT further introduces a projection layer to bridge the gap between vision and language in the latent space, gaining more advantages in perception which is critical in language planning. Compared to EmbodiedGPT, PERIA's superior performance can be attributed to the explicit incorporation of vision planning. By jointly fine-tuning MLLM using the additional image generation loss, the supervision from visual aspects encourages promoting attention to visual details and spatial information for more grounded reasoning. When we remove the joint training of vision planning, we observe the more frequent hallucinations and errors in language planning, such as generating unseen objects with wrong colors, sizes, or locations, which significantly decreases the accuracy of language planning. Moreover, we also ablate the encoding-side multimodal alignment and the degradation in accuracy highlights the importance of enhancing the foundational perception capabilities through our carefully designed dataset, which includes various perception-related data such as spatial relationships, temporal relationships, size recognition, and color identification. To further investigate the improvement in foundational perception abilities, we conduct a detailed categorical analysis, which can be found in Appendix F.1.

    &  &  &  \\   & **Stacking** & **Sort** & **Matching** & **Shape** & **Orders** & **Spell** & **Rearrange** & **Follow** & **Constraint** \\  CLIPort & \(18.4_{ 3.2}\) & \(19.2_{ 6.4}\) & \(17.8_{ 2.9}\) & \(9.8_{ 1.4}\) & \(8.1_{ 2.7}\) & \(2.3_{ 0.8}\) & \(5.8_{ 1.9}\) & \(2.4_{ 0.6}\) & \(8.3_{ 2.1}\) \\  LISA & \(26.6_{ 4.8}\) & \(22.1_{ 3.5}\) & \(23.0_{ 5.1}\) & \(18.4_{ 2.6}\) & \(16.1_{ 3.9}\) & \(10.2_{ 1.7}\) & \(8.9_{ 2.3}\) & \(6.3_{ 1.5}\) & \(11.9_{ 4.2}\) \\ PAR & \(34.7_{ 5.5}\) & \(32.8_{ 6.3}\) & \(31.1_{ 4.4}\) & \(31.5_{ 5.8}\) & \(30.7_{ 4.9}\) & \(27.3_{ 7.2}\) & \(24.4_{ 1.6}\) & \(16.1_{ 3.7}\) & \(26.5_{ 4.6}\) \\ EmbodiedGPT & \(48.6_{ 7.6}\) & \(49.1_{ 5.9}\) & \(44.3_{ 7.8}\) & \(40.9_{ 6.4}\) & \(48.2_{ 7.5}\) & \(52.7_{ 6.2}\) & \(38.3_{ 3.5}\) & \(37.2_{ 4.7}\) & \(43.5_{ 6.9}\) \\  SuSIE & \(31.4_{ 1.8}\) & \(32.6_{ 4.9}\) & \(33.2_{ 7.7}\) & \(37.8_{ 6.6}\) & \(35.2_{ 4.3}\) & \(34.1_{ 7.4}\) & \(37.9_{ 6.8}\) & \(40.2_{ 5.4}\) & \(51.2_{ 2.7}\) \\ CoTDiffusion & \(47.9_{ 6.0}\) & \(44.3_{ 7.6}\) & \(56.6_{ 2.2}\) & \(46.1_{ 6.5}\) & \(53.9_{ 4.8}\) & \(44.8_{ 7.9}\) & \(51.2_{ 6.3}\) & \(54.5_{ 7.3}\) & \(76.1_{ 5.6}\) \\  PERIA (ours) & \(_{ 8.8}\) & \(_{ 6.4}\) & \(_{ 7.1}\) & \(_{ 5.2}\) & \(_{ 6.7}\) & \(_{ 7.8}\) & \(_{ 6.0}\) & \(_{ 7.8}\) & \(_{ 4.9}\) \\   

Table 1: The evaluation of success rate between baselines and we report the mean and variance across 5 seeds.

  
**Method** & **Token**\(\) & **Semantic**\(\) \\  PAR & 58.2 & 0.63 \\ EmbodiedGPT & 65.9 & 0.68 \\ PERIA (ours) & \(\) & \(\) \\ - w/o perceive pretrain & 80.2 & 0.83 \\ - w/o vision planning & 83.7 & 0.79 \\   

Table 2: Evaluation of reasoning accuracy between methods on two metrics.

Fidelity of Vision PlanningWe further compare the fidelity of generated goal images against groundtruth keyframes using the Frechet Inception Distance (FID)  as the evaluation metric. Although SuSIE is not a strict vision planning method for long-horizon manipulation due to its limitation to simple single-step instructions, we grant it a relaxed privilege by providing oracle stepwise instructions to enable a comparison. However, as shown in Table 3, PERIA still demonstrates superiority, primarily due to the implicit generation of latent image tokens during language planning. The extracted image latent embeddings from MLLM retain more details and provide more sufficient guidance beyond language for subgoal image generation. CoTDiffusion supports general instruction inputs and can sequentially generate multiple images. However, the absence of explicit language planning in CoTDiffusion makes it challenging to ensure the semantic coherence of the generated images, potentially leading to dilemmas such as semantic repetition, jumping, or regression within the generated image sequences. In contrast, PERIA incorporates MLLM for reliable instruction decomposition and leverages the extracted image latent embeddings to achieve superior fidelity in vision planning compared to existing methods. Moreover, the performance drop in the ablation study without consistency loss highlights the importance of alignment between reasoned stepwise instructions and generated subgoal images, attributed to the synergistic combination of language planning and vision planning in our framework.

Consistency between Reasoning and ImaginingWe leverage CLIP  to measure the image-language similarity between generated instructions and images, with results presented in Figure 4(a). The additional consistency alignment loss explicitly constraints and encourages semantic alignment between the imagined images from vision planning and the reasoned stepwise instructions from language planning, significantly enhancing the collaboration and consistency between the two modalities. Furthermore, increasing the number of [IMG] tokens provides more expressive and sufficient guidance, facilitating the MLLM in producing semantically coherent language and image tokens. However, the benefit of adding more tokens becomes marginal beyond a certain threshold.

Effectiveness of Holistic PlanningWe modify the low-level policy model into several variants, including ones that simultaneously utilize stepwise instructions and subgoal images, as well as those that rely on each modality individually. As shown in Figure 4(b), the holistic planning approach achieves the highest success rate than single planning, with the benefit of the increased amount of information available and rich multi-modal guidance for decision-making, which reduces the training difficulty of low-level policy and enhance the accuracy of action prediction. Moreover, the advantage of holistic planning becomes more evident as the horizon length increases, demonstrating its scalability and effectiveness in handling complex, long-horizon manipulation tasks.

Generalization across TasksWe evaluate the generalization ability in three levels with increasing difficulty: placement generalization with novel placement of objects (L1), object generalization with novel objects (L2), and combinatorial generalization with extra novel instructions (L3). The results in Figure 4(c) demonstrate that PERIA enjoys a substantial advantage over other baselines, highlighting the importance of the common knowledge prior within the MLLM and diffusion model and holistic planning, which enhance the generalization and robustness for unseen challenging tasks.

Figure 5: More detailed quantitative analysis. (a) The ablation studies on consistency loss and [IMG] token numbers. (b) The comparisons of three planning paradigms in tasks with various horizon lengths. (c) The evaluation of generalization ability of three levels. See text for further discussion.

  
**Methodology** & **Blocks** & **Letters** & **VIMA** \\  SuSIE (+oracle) & 18.9 & 18.1 & 19.4 \\ CoTDiffusion & 13.1 & 15.8 & 17.6 \\ PERIA (ours) & **10.2** & **13.5** & **11.4** \\ - w/o alignment & 12.3 & 14.2 & 15.9 \\   

Table 3: Comparisons of FID (\(\)) between methods on three task domains.

Conclusion

We propose **PERIA** (**SEe, Reason, Imagine, Act**), a novel framework that integrates MLLM and diffusion-based image editing models to enable holistic language and vision planning for long-horizon manipulation tasks with complex instructions. We first perform a lightweight multi-modal alignment to enhance the MLLM's fundamental perception capabilities of visual details for manipulation, alleviating potential hallucinations. Then, we encourage MLLM to output rich latent visual tokens to guide diffusion model in generating images and explicitly align language instructions with visual subgoals to simultaneously unlock MLLM's reasoning and diffusion model's imagination capabilities. Extensive evaluations across three challenging benchmarks demonstrate that PERIA significantly outperforms competitive baselines in both instruction following accuracy and task success rate, while also enjoying better generalization ability across tasks. We believe PERIA highlights the potential of holistic language and vision planning and we hope this novel paradigm can provide some insights to robotics manipulation research of long-horizon tasks with complex instructions in free-form, towards more open embodied scenarios. One current bottleneck is the relatively high time cost of training and inference. Improving the joint training efficiency of MLLMs and diffusion models in a lightweight manner and accelerating image generation sampling are interesting directions for future work.

## 6 Acknowledgements

This work is supported by the National Natural Science Foundation of China (Grant Nos. 62422605, 92370132, 62106172), the National Key R&D Program of China (Grant No. 2022ZD0116402) and the Xiaomi Young Talents Program of Xiaomi Foundation.