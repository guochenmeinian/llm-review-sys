# Orthogonal Gradient Boosting for

Interpretable Additive Rule Ensembles

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Gradient boosting of decision rules is an efficient approach to find interpretable yet accurate machine learning models. However, in practice, interpretability requires to limit the number and size of the generated rules, and existing boosting variants are not designed for this purpose. Through their strict greedy approach, they can increase accuracy only by adding further rules, even when the same gains can be achieved, in a more interpretable form, by altering already discovered rules. Here we address this shortcoming by adopting a weight correction step in each boosting round to maximise the predictive gain per added rule. This leads to a new objective function for rule selection that, based on orthogonal projections, anticipates the subsequent weight correction. This approach does not only correctly approximate the ideal update of adding the risk gradient itself to the model, it also favours the inclusion of more general and thus shorter rules. Additionally, we derive a fast incremental algorithm for rule evaluation, as necessary to enable efficient single-rule optimisation through either the greedy or the branch-and-bound approach. As we demonstrate on a range of classification, regression, and Poisson regression tasks, the resulting rule learner significantly improves the comprehensibility/accuracy trade-off of the fitted ensemble. At the same time, it has comparable computational cost to previous branch-and-bound rule learners.

## 1 Introduction

Algorithms for learning additive rule ensembles (or rule sets) are an active area of research, because they are intrinsically interpretable yet relatively accurate due to their modularity and ability to represent interaction effects. While there is an emerging consensus that rule ensembles should optimize the trade-off between statistical risk and _cognitive complexity_ in terms of number and lengths of rules (see Fig. 1), there is a multitude of diverse approaches for performing this optimization. This ranges from computationally inexpensive generate-and-select approaches [10; 14], over more expensive minimum-description length and Bayesian approaches , to expensive full-fledged discrete optimization methods [6; 30]. Within this range of options, methods based on _gradient boosting_ are of special interest because of their robustness against changes in the training data, flexibility to adapt to various response variable types and loss functions, and finally their good model performance relative to their computational cost.

On the other hand, state-of-the-art rule boosting approaches are based on design choices that compromise their risk/complexity trade-off. The traditional gradient boosting adaption  resorts to greedy optimization of the individual rules, which results in additional rules and additional conditions per rule to reach a desired statistical risk level. The more recent optimal rule boosting approach  partially addresses this issue, but it is based on the uncorrected weight updates of the extreme gradient boosting framework . This too results in the inclusion of unnecessary extra rules, especially forloss functions with unbounded second derivatives like the Poisson loss. Most importantly, both approaches use the strict stagewise fitting approach where rules are not revised after they are added to the ensemble. Thus, they can increase accuracy only by adding further rules, even when the same gains can be achieved, in a more interpretable form, by altering those already present in the model.

Here we develop the first rule boosting algorithm that consistently optimizes the accuracy/complexity trade-off of the produced rule sets. For that, we adopt the fully corrective boosting approach  where all rule consequents are re-optimized in every boosting round, which can be done with only little computational extra effort given the usual convex loss functions. We then derive a new objective function for selecting individual rule bodies that anticipates the subsequent consequent re-optimization. This function is based on considering only the part of a rule body orthogonal to the already selected rules, which, as we show, correctly identifies the best approximation to the ideal space for consequent optimization defined by the risk gradient. Finally, we derive a corresponding efficient algorithm for cut-point search, which is crucial for, either greedy or branch-and-bound, single rule optimization. As we demonstrate on a wide range of datasets, the resulting rule boosting algorithm significantly outperforms the previous boosting variants in terms of risk/complexity trade-off, which can be attributed to a better risk reduction per rule as well as an affinity to select simpler rules. At the same time, the computational cost remains comparable to the previous branch-and-bound rule learner.

The paper is organized as follows. After giving a brief overview of the wider literature on interpretable machine learning and additive rule ensembles (Sec. 2), we recall the formal basics of rule ensembles and gradient boosting in Sec. 3. We then present our main technical contributions in Sec. 4 and their empirical evaluation in Sec. 5, before concluding in Sec. 6.

## 2 Related Literature

In contrast to post-hoc explanations of blackbox models (e.g., 28; 23), which are often unfaithful to the original model [21; 24; 13], interpretable machine learning methods aim to produce intrinsically intelligible, yet accurate, models. Additive models that compose terms in a simple summation are particularly useful in this context, because of their modularity, i.e., the possibility to comprehend the terms in isolation. As long as the individual terms are not too numerous and simulatable, i.e., their output can be approximately computed by a human, the resulting model is highly interpretable.

Good examples for this are (generalized) linear models (GLMs, 19), or generalized additive models (GAMs, 12; 16). However, they do not model variable interactions, at least not of higher order. Conjunctive propositional rules, on the other hand, have this ability, explaining their longstanding popularity in machine learning and related fields. Additive rule ensembles, which are closely related

Figure 1: Risk/complexity curves for previous rule boosting variants (green) and proposed orthogonalization approach (red) for dataset used_cars and banknote. The two highlighted corners correspond to rule ensembles with roughly equivalent training risk but substantially reduced cognitive complexity for the proposed algorithm.

to non-modular rule lists (e.g., 31; 22), thus provide a unique combination of interpretability and predictive power. There is a wide range of algorithms for learning additive rule ensembles. One approach is to generate a candidate set and then sub-selecting a rule ensemble, e.g. via sub-modular optimization (14; 32), or--as in RuleFit (10) or SIRUS (2)--via a sparse linear model, which is especially computationally inexpensive. However, these approaches are typically highly sensitive to the randomness in the generation of the candidate set. Alternatively, finding an optimal rule ensemble can be expressed as an integer program. Its relaxation as linear problem can then be solved via the column generation framework (6; 30), making the problem tractable. This approach is robust and flexible, but the full optimization remains computationally expensive.

Early approaches to additive rule ensemble learning that avoid initial candidate generation are based on the separate-and-conquer framework (11) and later on the original boosting algorithm (5; 17). However, the first typically leads to non-modular rule lists and the second are designed for specific learning tasks only, typically classification. This problem is overcome with the gradient boosting framework (9), which generalizes the original AdaBoost algorithm (25) and allows fitting arbitrary differentiable loss functions. With this framework, rules are fitted stagewise based on their effect on the training loss when added to the ensemble (7; 8). Extreme gradient boosting (4) increases the scalability of gradient boosting by avoiding numerical weight optimization. It is applicable whenever the loss function is twice differentiable. Fully-corrective boosting recalculate the weight of all weak learners after adding one weak learner into the ensemble model (26; 27). It overcomes the drawback of the original gradient boosting algorithm that the weak learners are not changed after being generated. However, it is a high-level framework and does not solve the problem of how to select individual base learners.

## 3 Rule Boosting

An **additive ensemble** of \(k\) rules can be represented by Boolean query functions \(q_{1},,q_{k}\) and a **weight vector**\(=(_{1},,_{k})^{T}^{k}\) that jointly describe a function \(f()=_{i=1}^{k}_{i}q_{i}()\), the output of which can be mapped to the conditional distribution of a target variable \(|=\). That is, the queries define the rule antecedents (rule bodies), and the coefficients \(\) define the rule consequents, i.e., the output of rule \(i\) for input \(^{d}\) is \(_{i}\) if \(\) satisfies the antecedent, i.e., \(q_{i}()=1\) (and \(0\) otherwise). Moreover, each **query function**\(q_{i}\,^{d}\{0,1\}\) is a conjunction of \(c_{i}\)**propositions**, i.e., \(q_{i}()=p_{i,1}()p_{i,2}() p_{i,c_{i}}( )\) where the \(p_{i,j}\) are typically a threshold function on an individual

Figure 2: Illustration of output space for toy regression example with three data points with target values \(y_{1}=-10\), \(y_{2}=-6\), \(y_{3}=5\) and three queries with output vectors \(_{1}=(1,1,0)\), i.e., \(q_{1}\) selects the first two data points, \(_{2}=(0,0,1)\), and \(_{3}=(0,1,1)\). The gradient boosting objective selects \(q_{1}\) with weight \(_{1}=-8\) as first rule, resulting in a negative gradient vector \(-=(-2,2,5)\). _Left:_ Approximations (4) to target subspace (blue) spanned by \(_{1}\) and \(-\). The subspace (green) spanned by \(_{3}\) and \(_{1}\) is a better approximation than the subspace (orange) spanned by \(_{2}\) and \(_{1}\). However, the latter is selected by standard gradient boosting._Right:_ After projection onto orthogonal complement of already selected query, angle between \(_{3}\) and \(-\) is smaller than that between \(_{2}\) and \(-\) and is thus successfully selected by orthogonal gradient boosting objective.

input variable, e.g., \(p_{i,j}()=(x_{l} t)\). We denote the set of available propositions by \(\) and the **query language** of all conjunctions that can be formed from \(\) as \(\).

We are concerned with two properties of an additive rule ensemble: its (empirical) prediction **risk1**\(R(f)=_{i=1}^{n}l(f(_{i}),y_{i})\), measured by some positive loss function \(l(f(),y)\) averaged over a dataset \(\{(_{1},y_{1}),,(_{n},y_{n})\}\), and its **cognitive complexity**\(C(f)=k+_{i=1}^{k}c_{i}\), measuring the cognitive effort required to parse all rule consequents and antecedents. Here we consider loss functions that can be derived as negative log likelihood (or rather deviance function) when interpreting the rule ensemble output as natural parameter of an exponential family model of the target variable, which guarantees that the loss function is strictly convex and twice differentiable. Specifically, we consider the cases of **squared loss**\(l_{}(f(x_{i}),y_{i})=(f(x_{i})-y_{i})^{2}\), the **logistic loss**\(l_{}(f(x_{i}),y_{i})=(1+(-y_{i}f(x_{i})))\), and the **Poisson loss**\(l_{}(f(x_{i}),y_{i})= y_{i}-f(x_{i})-y_{i}+(f(x_{i}))\).

Gradient boostingGradient boosting  is a "stagewise" fitting scheme for additive models that, in our context, produces a sequence of rule ensembles \(f^{(0)},f^{(1)},,f^{(k)}\) such that \(f^{(0)}()=0\) and, for \(t[1,k]\), \(f^{(t)}()=f^{(t-1)}()+_{t}q_{t}()\). Specifically, the term \(_{t}q_{t}()\) is chosen to perform an approximate gradient descent with respect to the risk function \(R(f)=R()\) considered as a function of the model **output vector**\(=(f(_{1}),,f(_{n}))\). The exact gradient descent update would be \(-_{*}\) where \(\) is the **gradient vector** with components \(g_{i}= l(f(_{i}),y_{i})/ f(_{i})\) and \(_{*}\) is the step length that minimizes the empirical risk \(R(-)\). However, since in general there is no query \(q\) for which the output vector \(=(q(x_{1}),,q(x_{n}))\) is equal to the gradient \(\), the goal is to select \(^{*}\) that best approximates \(\) in the sense that it minimizes the squared **projection error**

\[_{}\|-_{*}-\|^{2}= _{*}^{2}(\|\|^{2}-^{T})^{2}}{\| \|^{2}}).\] (1)

This is achieved by choosing \(q_{t}\) to maximize the standard **gradient boosting objective**\(_{}(q)=|^{T}|/\|\|\) and to find \(_{t}=_{}R(+_{t})\) via a line search. Note that this \(_{t}\) is not generally equal to the minimizing \(\) in (1), because the optimal update in direction \(\) can be better than the best geometric approximation to the gradient descent update in direction \(\). A derivation of this objective function is the **gradient sum objective**[8; 26]\(_{}(q)=|^{T}|\), which always selects more general rules than the gradient boosting objective [8, Thm. 1], however, typically at the expense of an increased risk per rule, because the correction of data points with large gradient elements has to be tuned down to avoid over-correction of other selected data points with small gradient elements. Finally, an adaption of "extreme gradient boosting"  to rule ensembles yields the **extreme boosting objective**\(_{}(q)=(^{T})^{2}/^{T} \) where \(=(_{f()}^{2}R(f))\) is the diagonal vector of the risk Hessian again with respect to the output vector \(\). This approach starts from the second order approximation of \(R(+)\) for which also yields a closed form weight update \(_{t}=-^{T}/^{T}\). This approach is well-defined for our loss functions derived from exponential family response models, which guarantee defined and positive \(\). For the squared loss, it is equivalent to standard gradient boosting, because the second order approximation is exact for \(l_{}\) and \(\) is constant.

Single rule optimizationWhile the rule optimization literature can be neatly divided into heuristic (greedy / beam search) and exact branch-and-bound search, these approaches are actually closely related: they can both be described as traversing a lattice on the query language \(\) imposed by a **specialization relation**\(q q^{}\) that holds if the propositions in \(q^{}\) are a superset of those mentioned in \(q\), and \(q^{}\) thus logically implies \(q\). The difference between the approaches is under what conditions they discard specializations of candidate queries and how those specializations are generated.

Here, we build on the branch-and-bound framework presented in Boley et al.  that allows to efficiently search for optimal conjunctive queries in a condensed search space, given that there is an admissible, effective, and efficiently computable bounding function for the employed objective. Specifically, let \(\,\) denote the objective function to be maximized. Then a **bounding function**\(:\,\) is **admissible** if for all \(q\) it holds that \((q)(q)\) where \((q)=_{q q^{}}(q^{ })\) denotes the objective value of the best specialization of \(q\). A bounding function is effective in allowing to prune the search space if the difference \((q)-(q)\) tendsto be small, rendering \((q)\) itself the theoretically most effective bounding function. However, computing \((q)\) is as hard as the overall optimization problem.

A frequently applied recipe for constructing an admissible bounding function that is also effective and efficiently computable is to relax the quantifier in the definition of \(\) and instead of bounding the value of the best specialization in the search space, bound the value of the best subset of data points of those selected by \(q\). This results in the tight bounding function when **unaware of selectability**

\[(q)=\{(^{}):^{} ,^{}\{0,1\}^{n}\}(q).\] (2)

Here, \(^{}\) refers to the component-wise less or equals relation on the binary output vector of \(q\) and \(q^{}\). This function can be efficiently computed for many objective functions by pre-sorting the data in time \(O(n n)\) that has to be carried out only once per fitted rule . For instance for the extreme boosting objective, the optimum \(q^{}\{0,1\}^{n}\) can be found as a prefix or suffix of all data points after sorting them according to the ratio \(g_{i}/h_{i}\) of first and second order loss derivatives.

## 4 Fully-corrective Orthogonal Gradient Boosting

Having reviewed the existing rule boosting approaches, we now turn to improving them in terms of their risk/complexity trade-off. Our approach to this is to improve the risk reduction per rule added to the ensemble, which directly affects the number of rules needed to achieve a certain risk. As it turns out, this typically coincides with preferring the addition of more general and hence simpler rules. Thus, it also positively affects the cognitive complexity on the level of the lengths of individual rules.

### Weight Correction and Subspace Approximations

A natural idea to reduce the ensemble risk per rule added is to relax the strict stagewise fitting approach of traditional gradient boosting and to allow the whole weight vector \(\) to be adjusted in every round \(t\), i.e., to set

\[^{(t)}=*{arg\,min}_{^{t}}R( _{t}),\] (3)

where \(_{t}=[_{1},,_{t}]\) is the \(n t\)**query matrix** with the output vectors of all selected queries as columns. In contrast to a full joint optimization of queries and weights, this intermediate solution still retains the computational benefits of gradient boosting for small rule ensembles: Given that our loss function \(l\) and therefore the empirical risk \(R\) are convex, optimizing the weights in round \(t\) is a convex optimization problem in \(t\) variables, equivalent to fitting a small generalized linear model. Using Newton-Raphson ("iterated least squares") type algorithms, the computational cost of this is usually negligible compared to the more expensive query optimization step, especially when aiming for optimal individual queries for their reduced cognitive complexity.

Re-optimizing the weights, which is sometimes referred to as **fully corrective boosting**, effectively turns boosting into a form of forward variable selection for linear models. However, in contrast to conventional variable selection where all variables are given explicitly, we still have to identify a good query \(q_{t}\) in each boosting iteration, and it turns out that finding the appropriate query is more complicated as in the case of single weight optimization characterized by (1). We still would like to add the direction of steepest descent, i.e., the negative gradient \(-\), to the subsequent risk optimization step and approximate as closely as possible the outcome \([_{t-1};]^{*}\) where \(^{*}^{t}\) are the risk minimizing weights for \(_{1},,_{t-1},\). Therefore, the best approximating query \(q\) is now given by

\[*{arg\,min}_{q}_{ ^{t}}\|[_{t-1};]^{*}-[_{t-1} ;]\|^{2}.\] (4)

It is an important observation that the standard gradient boosting objective does not correctly identify this optimally approximating query. This is demonstrated by the example illustrated in Fig. 2. In the left sub-figure it can be seen that the green plane, \(\{_{1},_{3}\}\), is a better approximation to \(\{_{1},-\}\) (blue) than the orange plan, \(\{_{1},_{2}\}\). However, the latter is selected by standard gradient boosting, because the angle between \(_{2}\) and \(-\) is smaller than that between \(_{3}\) and \(-\).

### An Objective Function to Identify the Best Approximating Subspace

The intuitive reason for the gradient boosting objective failing to identify the correct query in Fig. 2 is that selecting \(x_{2}\) in addition to \(x_{3}\) is not beneficial for the overall risk reduction _if_ we are only allowed to set the weight for the newly selected query. This is because then this weight has to be a compromise between the two different magnitudes of correction required for \(x_{2}\), which only needs a small positive correction, and \(x_{3}\), which needs a large positive correction. If we, however, are allowed to change the weight of the previously selected query this consideration changes, because we can now balance an over-correction for \(x_{2}\) by adjusting the weight of the first rule. While on first glance it seems unclear how much of such re-balancing can be applied without harming the overall risk, it turns out that this is captured by a simple criterion based on the norm of the part of the newly selected query that is orthogonal to the already selected ones.

**Lemma 4.1**.: For \(\!\!^{n}\), \(\!=\![_{1},,_{t-1}]^{n( t-1)}\), and \(\!\!\{_{1},,_{t-1}, \}\), we have

\[*{arg\,min}_{q}_{^{t}}\|-[_{1},,_{t-1},] \|^{2}=*{arg\,max}_{q}_{}^{T}|}{\|_{}\|}.\] (5)

where for a vector \(^{n}\) we denote by \(_{}\) its projection onto the orthogonal complement of \(*{range}\). (All proofs of lemmas and theorems are in SI )

From this result we can directly derive \(|_{}^{T}|/\|_{}\|\) as suitable objective function for fully corrective gradient boosting. However, it is worth incorporating two further observations. Firstly, we can show that, after applying the weight correction step (3), the gradient vector satisfies \(=_{}\), i.e., it is orthogonal to the subspace spanned by the selected queries, and therefore can be used in the objective function without projection.

**Lemma 4.2**.: Let \(\) be the gradient vector after the application of the weight correction step (3) for selected queries \(_{1},,_{t}\). Then \(\{_{1},,_{t}\}\).

Moreover, the right hand side of Eq. (4) is technically undefined for redundant query vectors \(\) that lie in \(*{range}\) and therefore have \(\|_{}\|=0\). Through Lm. 4.2 we know that for such queries we also have \(^{T}=0\), which suggests to simply fix this issue by defining the objective value in this case to be \(0\). However, this solution would not fix numerical instabilities when \(\|_{}\|\) is close to zero. A better solution is therefore to add a small positive value \(\) to the denominator, which can be considered a **regularization parameter**. With this we arrive at the final form of our objective function, which we state along with some of its basic properties in the following theorem.

**Theorem 4.3**.: _Let \(=[_{1},,_{t-1}]^{n(t-1)}\) be the selected query matrix and \(\) the corresponding gradient vector after full weight correction, and let us denote by \(=_{}+_{}\) the orthogonal decomposition of \(\) with respect to \(*{range}\). Then we have for a maximizer \(^{*}\) of the orthogonal gradient boosting objective \(_{}(q)=|^{T}|/(\|_{ }\|+)\):_

* _For_ \( 0\)_,_ \(\{_{1},,_{t-1},^{*}\}\) _is the best approximation to_ \(\{_{1},,_{t-1},\}\)_._
* _For_ \(\)_,_ \(^{*}\) _maximizes_ \(_{}\) _and any maximizer of_ \(_{}\) _maximizes_ \(_{}\)_._
* _For_ \(=0\) _and_ \(\|_{}\|>0\)_, the ratio_ \((_{}(q)/_{}(q))^{2}\) _is equal to_ \(1+(\|_{}\|/\|_{}\|)^{2}\)_._
* _The objective value_ \(_{}(q)\) _is upper bounded by_ \(\|\|\)_._

Intuitively, the orthogonal gradient boosting objective function measures the cosine of the angle between the gradient vector and the orthogonal projection of a candidate query vector \(\). This is in contrast to the standard gradient boosting objective, which considers the angle of the unprojected query vector instead. In the example in Fig. 2 we can observe that this difference leads to successfully identifying the best approximating subspace, and Thm. 4.3a) guarantees this property.

### Efficient Implementation

To develop an efficient optimization algorithm for the orthogonal gradient boosting objective, we recall that projections \(_{}\) on the orthogonal complement of \(*{range}\) can be naively computed via \(_{}=((^{T})^{-1}(^{T} ))\) where we placed the parentheses to emphasize that only matrix-vector products are involved in the computation--at least once the inverse of the Gram matrix \(^{T}\) is computed. This approach allows to compute projections, and thus objective values, in time \(O(nt+t^{2})\) per candidate query after an initial preprocessing per boosting round of cost \(O(t^{2}n+t^{3})\).

In a first step, this naive approach can be improved by maintaining an orthonormal basis of the range of the query matrix throughout the boosting rounds, resulting in a Gram-Schmidt-type procedure.

[MISSING_PAGE_FAIL:7]

The candidates evaluated per search node of both branch-and-bound and beam search have query vectors that are prefixes of an ordered sub-selection of data points, in beam search because optimum cut-off values are sought for each of the \(d\) input variables, in branch-and-bound because the optimum in Eq. (2) is attained or approximated by a prefix with respect to some presorting order. Hence, we need to solve the following **prefix optimization problem**: given an ordered sub-selection of \(l\) of the \(n\) data points, represented by an injective map \(\{1,,l\}\{1,,n\}\), find the optimal prefix

\[i_{*}=*{arg\,max}_{i\{1,,l\}}^{T} ^{(i)}}{\|_{}^{(i)}\|+}\] (6)

where \(^{(0)}=\) and \(^{(i)}=^{(i-1)}+_{(i)}\). The following proof shows how the computational complexity for solving (6) can be substantially reduced compared to the direct approach above. It uses an incremental computation of projections that works directly on the available orthonormal basis vectors \(\) instead of computing matrix-vector products or, even worse, the whole projection matrix.

**Theorem 4.4**.: _Given a gradient vector \(^{n}\), an orthonormal basis \(_{1},,_{t}^{n}\) of the subspace spanned by the queries of the first \(t\) rules, and a sub-selection of \(l\) candidate points \([l][n]\), the best prefix selection problem (6) can be solved in time \(O(tn)\)._

Proof sketch.: We write the objective value of prefix \(i\) in terms of incrementally computable quantities:

\[^{T}^{(i)}}{\|_{}^{(i)}\|+}= ^{T}^{(i)}}{\|^{(i)}\|-\|_{ }^{(i)}\|+}=^{T}^{(i)}}{\|^{(i)}\| -^{t}\|_{k}_{k}^{T}^{(i)}\|^{2} +}}.\]

In particular, the \(t\) sequences of norms \(\|_{k}_{k}^{T}^{(i)}\|\) can be computed in time \(O(n)\) via cumulative summation of the \(k\)-th basis vector elements in the order given by \(\):

\[\|_{k}_{k}^{T}^{(i)}\|=\|_{k}\|_{ j=1}^{i}_{k}^{T}_{(j)}=_{j=1}^{i}o_{k,(j)}\]

We close this section with a pseudocode (Alg. 1) that summarizes the main ideas of the orthogonal gradient boosting and refer to the literature for details about the beam-search/branch-and-bound step.

## 5 Experiments

In this section, we present empirical results comparing the proposed fully corrective orthogonal gradient boosting (FCOGB) to the standard gradient boosting algorithms  using greedy optimization of \(_{}^{(i)}\) (GB) and \(_{}^{(i)}\) (GS), to extreme gradient booosting  using branch-and-bound optimisation of \(_{}^{(i)}\) (XGB), and finally to SIRUS  as the state-of-the-art generate-and-filter approach. We investigate the risk/complexity trade-off, the affinity to select general rules, as well as the computational complexity. The datasets used are those of Boley et al.  augmented by three additional classification datasets from the UCI machine learning repository and, to introduce a novel modelling task to the rule learning literature, five counting regression datasets from public sources. This results in a total of 34 datasets (13 for classification, 16 for regression, and 5 for counting/Poisson regression, see Tab. 1). All algorithms were run five times on all datasets using 5 random 80/20 train/test splits to calculate robust estimates of all considered metrics. In all cases, the number of gradient boosting iterations was chosen to produce ensembles with cognitive complexity of at most 50. The experiment code and further information about the datasets are available on GitHub.

Cognitive complexity versus riskTab. 1 compares the complexity/risk trade-off of the boosting variants and SIRUS by the normalized risk averaged across all considered cognitive complexity levels (where normalization is performed by the risk of the empty rule ensemble). FCOGB has the smallest training risk for 26 of the 34 datasets, occasionally outperforming the second best algorithm by a wide margin (_tic-tac-toe_, _wine_, _banknote_, _insurance_, _boston_, _ships_, _smoking_). For the test risk the picture is more ambiguous, however, FCOGB retains a relative majority of datasets won. Performing one-sided paired t-tests at significance level 0.05 (with Bonferroni correction for 8 hypotheses) reveals that FCOGB significantly outperforms all other variants with a margin of at least 0.001 average normalized training risk (while there is no significant winner in terms of test risk--likely due to a lack of regularization for larger ensemble sizes).

CoverageTo compare the generality of the rules learned by the new objective function in comparison to the existing ones, we performed an additional experiment where we first used one of the previous objective functions to generate rule ensembles with ten rules for all datasets. Then for each partial rule ensemble, we applied the orthogonal gradient boosting objective function to find an alternative rule. Importantly, we used branch-and-bound with admissible bounding functions for all the alternative objectives to avoid confounding through sub-optimal greedy solutions. In Fig. 3 we compare the relative coverage, i.e., the relative number of selected data points \(\|\|^{2}/n\), of the rules discovered by the original algorithms to the ones discovered by FCOGB. The outcome is that 81.1% of the FCOGB rules covers more data points than gradient boosting, and similarly 71.3% of its rules cover more data points than those generated by XGBoost. In contrast, only 47.2% of the FCOGB rules cover more datapoint than the ones discovered by gradient sum. These results are in alignment with the theoretical expectation in terms of the influence of the coverage on the objective values where gradient sum is completely unaffected, whereas orthogonal gradient boosting has a denominator that tends to grow with coverage albeit less than the one of gradient boosting.

Computation timeWe also compare the computational cost of generating rule ensembles with cognitive complexity 50 by different algorithms in Tab 1. Comparing the computational cost of FCOGB to XGB, the other algorithm utilizing the more expensive branch-and-bound search, the costs are in the same order of magnitude except for one extreme case (wine) where FCOGB is a factor of 26 slower. Comparing to the two greedy variants, FCOGB is in the same order of magnitude as gradient boosting for most datasets. Unsuprisingly, there are a few examples where greedy search vastly outperforms branch-and-bound, in one case (telco churn) by a factor of around 76. However, overall, the results confirm that branch-and-bound search is a practical algorithm in absolute terms: For 23 benchmark dataset, FCOGB is able to finish training a model of cognitive complexity of 50 within one minute. Most of the other experiments run within 15 minutes except one dataset (telco churn) which require longer running time. Finally, Fig. 3 shows the detailed computation time of all algorithms in terms of cognitive complexity, including the naive implementation of FCOGB for _breast cancer_ and _diabetes_, which shows that the performance improvement through Thm. 4.4 is critical to bring the computational complexity on par with XGB.

## 6 Conclusion

The proposed fully corrective orthogonal boosting approach is a worthwhile alternative to previously published boosting variants for rule learning, especially when targeting a beneficial risk-complexity trade-off and an overall small number of rules. The present work provided a relatively detailed theoretical analysis of the newly developed rule objective function. However, some interesting questions were left open. While the presorting-based approach to the bounding function performs extremely well in synthetic experiments, a theoretical approximation guarantee for this algorithm has yet to be derived. Another interesting direction for future work is the extension of the introduced approximating subspace paradigm to the extreme gradient boosting approach, which, due to the utilization of higher order information, should principally be able to produce even better risk-complexity trade-offs.

Figure 3: First three: the comparison of the coverage rate of Gradient Boosting, XGBoost, Gradient Sum and FCOGB. The upper (resp. lower) half of the green line means the coverage rate of FCOGB is higher (resp. lower) than the other method. Last two: the comparison of the running time of Gradient boosting, XGBoost and FCOGB for the benchmark datasets breast cancer and diabetes of generating rule ensembles with cognitive complexity 50.