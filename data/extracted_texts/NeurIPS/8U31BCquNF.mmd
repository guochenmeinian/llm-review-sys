# Learning Shared Safety Constraints

from Multi-task Demonstrations

 Konwoo Kim

Carnegie Mellon University

&Gokul Swamy1

Carnegie Mellon University

&Zuxin Liu

Carnegie Mellon University

&Ding Zhao

Carnegie Mellon University

&Sanjiban Choudhury

Cornell University

&Zhiwei Steven Wu

Carnegie Mellon University

Equal contribution. Correspondence to gswamy@cmu.edu.

###### Abstract

Regardless of the particular task we want them to perform in an environment, there are often shared _safety constraints_ we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task settings to learn a tighter set of constraints. We validate our method with simulation experiments on high-dimensional continuous control tasks.

## 1 Introduction

If a friend was in your kitchen and you told them to "make toast" or "clean the dishes," you would probably be rather surprised if they broke some of your plates during this process. The underlying _safety constraint_ that forbids these kinds of behavior is both _a)_ implicit and _b)_ agnostic to the particular task they were asked to perform. Now, let's bring a household robot into the equation, operating within your kitchen. How can we ensure that it adheres to these implicit safety constraints, regardless of its assigned tasks?

One approach might be to write down specific constraints (e.g. joint torque limits) and pass them to the decision-making system of the robot. Unfortunately, more complex constraints like the ones we consider above are both difficult to formalize mathematically and easy for an end-user to forget to specify (as they would be inherently understood by a human helper). This problem is paralleled in the field of reinforcement learning (RL), where defining reward functions that lead to desirable behaviors for the learning agent is a recurring challenge . For example, it is rather challenging to handcraft the exact function one should be optimized to be a good driver. The standard solution to this sort of "reward design" problem is to instead demonstrate the desired behavior of the agent and then extract a reward function that would incentivize such behavior. Such _inverse reinforcement learning_ (IRL) techniques have found application in fields as diverse as robotics , computer vision , and human-computer interaction . Given the success of IRL techniques and the similarity between reward and constraint design, we proposeextending IRL techniques to the space of constraints. We term such techniques _inverse constraint learning_, or ICL for short.

More formally, we consider a setting in which we have access to demonstrations of an expert policy for a task, along with knowledge about the task's reward. This allows us to look at the difference between the expert and reward-optimal policies for a task. Our first key insight is that _the actions taken by the reward-optimal but not the expert policy are likely to be forbidden, allowing us to extract a constraint._

Unfortunately, the ICL problem is still rather ill-posed. Indeed, prior work in ICL will often learn overly conservative constraints that forbid all behavior the expert did not take (Scobee and Sastry, 2019; Vazquez-Chanlatte et al., 2018; McPherson et al., 2021). However, for tasks in a shared environment with different rewards, there are often safety constraints that should be satisfied regardless of the task (e.g. a plate shouldn't be broken regardless of whether you're serving food on it or cleaning up after a meal). Our second crucial insight is that _we can leverage multi-task data to provide more comprehensive demonstration coverage over the state space, helping our method avoid degenerate solutions._

More explicitly, the contributions of our work are three-fold.

**1. We formalize the inverse constraint learning problem.** We frame ICL as a zero-sum game between a policy player and a constraint player. The policy player attempts to maximize reward while satisfying a potential constraint, while the constraint player picks constraints that maximally penalize the learner relative to the expert. Intuitively, such a procedure recovers constraints that forbid high-reward behavior the expert did not take.

**2. We develop a multi-task extension of inverse constraint learning.** We derive a zero-sum game between a set of policy players, each attempting to maximize a task-specific reward, and a constraint player that chooses a constraint that all policy players must satisfy. Because the constraint player looks at aggregate learner and expert data, it is less likely to select a degenerate solution.

**3. We demonstrate the efficacy of our approach on various continuous control tasks.** We show that with restricted function classes, we are able to recover ground-truth constraints on certain tasks. Even when using less interpretable function classes like deep networks, we can still ensure a match with expert safety and task performance. In the multi-task setting, we are able to identify constraints that a single-task learner would struggle to learn.

We begin with a discussion of related work.

## 2 Related Work

Our work exists at the confluence of various research thrusts. We discuss each independently.

**Inverse Reinforcement Learning.** IRL (Ziebart et al., 2008, 2012; Ho and Ermon, 2016) can be framed as a two-player zero-sum game between a policy player and a reward player (Swamy et al., 2021). In most formulations of IRL, a potential reward function is chosen in an outer loop, and the policy player optimizes it via RL in an inner loop. Similar to IRL, the constraint in our formulation of ICL is chosen adversarially in an outer loop. However, in contrast to IRL, the inner loop of ICL is _constrained_ reinforcement learning: the policy player tries to find the optimal policy that respects the constraint chosen in the outer loop.

**Constrained Reinforcement Learning.** Our approach involves repeated calls to a constrained reinforcement learning (CRL) oracle (Garcia and Fernandez, 2015; Gu et al., 2022). CRL aims to find a reward-maximizing policy over a constrained set, often formulated as a constrained policy optimization problem (Altman, 1999; Xu et al., 2022). Solving this problem via Frank-Wolfe methods is often unstable (Ray et al., 2019; Liang et al., 2018). Various methods have been proposed to mitigate this instability, including variational techniques (Liu et al., 2022), imposing trust-region regularization (Achiam et al., 2017; Yang et al., 2020; Kim and Oh, 2022), optimistic game-solving algorithms (Moskovitz et al., 2023), and PID controller-based methods (Stooke et al., 2020). In our practical implementations, we use PID-based methods for their relative simplicity.

**Multi-task Inverse Reinforcement Learning.** Prior work in IRL has considered incorporating multi-task data (Xu et al., 2019; Yu et al., 2019; Gleave and Habryka, 2018). We instead consider a setting in which we know task-specific rewards and are attempting to recover a shared component of the demonstrator's objective. Amin et al. (2017) consider a similar setting but require the agent to be able to actively choose tasks or interactively query the expert, while our approach requires neither.

**Inverse Constraint Learning.** We are far from the first to consider the ICL problem. Scobee and Sastry (2019); McPherson et al. (2021) extend the MaxEnt IRL algorithm of Ziebart et al. (2008) to the ICL setting. We instead build upon the moment-matching framework of Swamy et al. (2021), allowing our theory to handle general reward functions instead of the linear reward functions MaxEnt IRL assumes. We are also able to provide performance and constraint satisfaction guarantees on the learned policy, unlike the aforementioned work. Furthermore, we consider the multi-task setting, addressing a key shortcoming of the prior work.

Perhaps the most similar paper to ours is the excellent work of Chou et al. (2020), who also consider the multi-task ICL setting but propose a solution that requires several special solvers that depend on knowledge of the parametric family that a constraint falls into. In contrast, we provide a general algorithmic template that allows one to apply whatever flexible function approximators (e.g. deep networks) and reinforcement learning algorithms (e.g. PPO) they desire. Chou et al.'s method also requires sampling _uniformly_ over the set of trajectories that achieve a higher reward than the expert, a task which is rather challenging to do on high-dimensional problems. In contrast, our method only requires the ability to solve a standard RL problem. Theoretically, Chou et al. (2020) focus on constraint recovery, which we argue below is a goal that requires strong assumptions and is therefore a red herring on realistic problems. This focus also prevents their theory from handling suboptimal experts. In contrast, we are able to provide rigorous guarantees on learned policy performance and safety, even when the expert is suboptimal. We include results in Appendix C that show that our approach is far more performant.

In concurrent work, Lindner et al. (2023) propose an elegant solution approach to ICL: rather than learning a constraint function, assume that _any_ unseen behavior is unsafe and enforce constraints on the learner to play a convex combination of the demonstrated safe trajectories. The key benefit of this approach is that it doesn't require knowing the reward function the expert was optimizing. However, by forcing the learner to simply replay previous expert behavior, the learner cannot meaningfully generalize, and might therefore be extremely suboptimal on any new task. In contrast, we use the side information of a reasonable set of constraints to provide rigorous policy performance guarantees.2

We now turn our attention to formalizing inverse constraint learning.

## 3 Formalizing Inverse Constraint Learning

We build up to our full method in several steps. We first describe the foundational algorithmic structures we build upon (inverse reinforcement learning and constrained reinforcement learning). We then describe the single-task formulation before generalizing it to the multi-task setup.

We consider a finite-horizon Markov Decision Process (MDP) (Puterman, 2014) parameterized by \(,,,r,T\) where \(\), \(\) are the state and action spaces, \(:()\) is the transition operator, \(r:[-1,1]\) is the reward function, and \(T\) is the horizon.

### Prior Work: Inverse RL as Game Solving

In the inverse RL setup, we are given access trajectories generated by an expert policy \(^{E}:()\), but do not know the reward function of the MDP. Our goal is to nevertheless learn a policy that performs as well as the expert's, no matter the true reward function.

We solve the IRL problem via equilibrium computation between a policy player and an adversary that tries to pick out differences between expert and learner policies under potential reward functions Swamy et al. (2021). More formally, we optimize over polices \(:()\) and reward functions \(f:[-1,1]_{r}\). For simplicity, we assume that our strategy spaces (\(\) and \(_{r}\)) are convex and compact and that \(r_{r},_{E}\). We solve (i.e. compute an approximate Nashequilibrium) of the two-player zero-sum game

\[_{}_{I_{r}}J(,f)-J(_{E},f),\] (1)

where \(J(,f)=_{}[_{t=0}^{T}f(s_{t},a_{t})]\) denotes the value of policy \(\) under reward function \(f\).

### Prior Work: Constrained Reinforcement Learning as Game Solving

In CRL, we are given access to both the reward function and a constraint \(c:[-1,1]\). Our goal is to learn the highest reward policy that, over the horizon, has a low expected value under the constraint. More formally, we seek a solution to the optimization problem:

\[_{}-J(,r)J(,c),\] (2)

where \(\) is some error tolerance. We can also formulate CRL as a game via forming the Lagrangian of the above optimization problem (Altman, 1999):

\[_{}_{>0}-J(,r)+(J(,c)-).\] (3)

Intuitively, the adversary updates the weight of the constraint term in the policy player's reward function based on how in violation the learner is.

``` Input: Reward \(r\), constraint \(c\), learning rates \(_{1:N}\), tolerance \(\) Output: Trained policy \(\)  Initialize \(_{1}=0\) for\(i\) in \(1 N\)do \(_{i}(r=r-_{i}c)\) \(_{i}[_{i}+_{i}(J(_{i},c)-)]^{+}\) endfor Return\((_{1:N})\). ```

**Algorithm 1** CRL (Constrained Reinforcement Learning)

### Single-Task Inverse Constraint Learning

We are finally ready to formalize ICL. In ICL, we are given access to the reward function, trajectories from the solution to a CRL problem, and a class of potential constraints \(_{c}\) in which we assume the ground-truth constraint \(c^{*}\) lies. We assume that \(_{c}\) is convex and compact.

In the IRL setup, without strong assumptions on the dynamics of the underlying MDP and expert, it is impossible to guarantee recovery of the ground-truth reward. Often, the only reward function that actually makes the expert optimal is zero everywhere (Abbeel and Ng, 2004). Instead, we attempt to find the reward function that maximally distinguishes the expert from an arbitrary other policy in our policy class via game-solving (Ziebart et al., 2008a; Ho and Ermon, 2016; Swamy et al., 2021). Similarly, for ICL, exact constraint recovery can be challenging. For example, if two constraints differ only on states the expert never visits, it is not clear how to break ties. We instead try to find a constraint that best separates the safe (but not necessarily optimal) \(_{E}\) from policies that achieve higher rewards.

More formally, we seek to solve the following constrained optimization problem.

\[_{}J(_{E},r)-J(,r)\] (4) \[_{c_{c}}J(,c)-J(_{E},c) 0.\] (5)

Note that in contrast to the _moment-matching_ problem we solve in imitation learning (Swamy et al., 2021), we instead want to be _at least_ as safe as the expert. This means that rather than having equality constraints, we have inequality constraints. Continuing, we can form the Lagrangian:

\[_{}_{>0}J(_{E},r)-J(,r)+( _{c_{c}}J(,c)-J(_{E},c))\] (6) \[=_{c_{c}}_{>0}_{}J(_ {E},r- c)-J(,r- c).\] (7)

Notice that the form of the ICL game resembles a combination of the IRL and CRL games. We describe the full game-solving procedure in Algorithm 2, where \(R(c)\) is an arbitrary strongly convex regularizer (McMahan, 2011). Effectively, we pick a constraint function in the same way we pick a reward function in IRL but run a CRL inner loop instead of an RL step. Instead of a fixed constraint threshold, we set tolerance \(\) to the expert's constraint violation. Define

\[_{i}(c)=(J(_{i},c)-J(_{E},c))[-1,1]\] (8)

as the per-round loss that the constraint player suffers in their online decision problem. The best-inhindsight comparator constraint is defined as

\[=*{argmax}_{c_{c}}_{i}^{T}_{i}(c).\] (9)

We can then define the cumulative regret the learner suffers as

\[(T)=_{i}^{T}_{i}()-_{i}^{T}_{i}(c_{i}),\] (10)

and let \(_{i}=_{i}()-_{i}(c_{i})\). We prove the following theorem via standard machinery.

**Theorem 3.1**.: _Let \(c_{1:N}\) be the iterates produced by Algorithm 2 and let \(=_{i}^{N}_{i}\) denote their time-averaged regret. Then, there exists a \(c c_{1:N}\) such that \(=(r,c,=J(_{E},c))\) satisfies_

\[J(,c^{*})-J(_{E},c^{*})TJ(,r) J(_{E},r).\] (11)

In words, by optimizing under the recovered constraint, we can learn a policy that (weakly) Pareto-dominates the expert policy under \(c^{*}\). We conclude by noting that because FTRL (Follow the Regularized Leader, McMahan (2011)) is a no-regret algorithm for linear losses like (8), we have that \(_{T}(T)}{T}=0\). This means that with enough iterations, the RHS of the above bound on ground-truth constraint violation will go to 0.

Figure 1: A visual depiction of the optimization problem we’re trying to solve in ICL. We attempt to pick a constraint that minimizes the value difference over the expert policy a safe policy could have. The star corresponds to the output of CRL.

``` Input: Rewards \(r^{1:K}\), constraint class \(_{c}\), trajectories from \(_{E}^{1:K}\) Output: Learned constraint \(c\)  Set \(}_{c}=\{c_{c}| k[K],J(_{E}^{k}, c) 0\}\)  Initialize \(c_{1}}_{c}\) for\(i\) in \(1 N\)do for\(k\) in \(1 K\)do \(_{i}^{k},_{i}^{k}(r^{k},c_{i},=0)\) endfor // use any no-regret algo. to pick c, e.g. FTRL: \(c_{i+1}*{argmax}_{c}_{c}} _{j}^{i}_{k}^{K}(J(_{j}^{k},c)-J(_{E}^{k},c))-R(c)\). endfor Return best of \(c_{1:N}\) on validation data. ```

**Algorithm 3**\(\) (Multi-task Inverse Constraint Learning)

### Multi-task Inverse Constraint Learning

One of the potential failure modes of the single-task approach we outline above is that we could learn an overly conservative constraint, leading to poor task performance (Liu et al., 2023). For example, imagine that we entropy-regularize our policy optimization (Ziebart et al., 2008; Haarnoja et al., 2018), as is common practice. Assuming a full policy class, the learner puts nonzero probability mass on all reachable states in the MDP. The constraint player is therefore incentivized to forbid all states the expert did not visit (Scobee and Sastry, 2019; McPherson et al., 2021). Such a constraint would likely generalize poorly when combined with a new reward function (\( r\)) as it forbids _all untaken_ rather than just _unsafe_ behavior.

At heart, the issue with the single-task formulation lies in the potential for insufficient coverage of the state space within expert demonstrations. Therefore, it is natural to explore a multi-task extension to counteract this limitation. Let each task be defined by a unique reward. We assume the dynamics and safety constraints are consistent across tasks. We observe \(K\) samples of the form \((r_{k},\{_{E}^{k}\})\). This data allows us to define the multi-task variant of our previously described ICL game:

\[_{c_{c}}_{^{1:K}}_{^{1:K}>0}_{i }^{K}J(_{E}^{i},r^{i}-^{i}c)-J(^{i},r^{i}-^{i}c).\] (12)

We describe how we solve this game in Algorithm 3, where \(R(c)\) is an arbitrary strongly convex regularizer (McMahan, 2011). In short, we alternate between solving \(K\) CRL problems and updating the constraint based on the data from all policies.

We now give two conditions under which generalization to new reward functions is possible.

### A (Strong) Geometric Condition for Identifiability

Consider for a moment the linear programming (LP) formulation of reinforcement learning. We search over the space of occupancy measures (\(_{}()\)) that satisfy the set of Bellman flow constraints (Sutton and Barto, 2018) and try to maximize the inner product with reward vector \(r^{||||}\). We can write the CRL optimization problem (assuming \(=0\) for simplicity) as an LP

Figure 2: If we have a sufficient diversity of expert policies, none of which are optimal along the reward vector, we can identify the hyperplane that separates the safe policies from the unsafe policies. The constraint (red, dashed) will be orthogonal to this hyperplane. For this example, because \(_{}^{2}\), we need two expert policies.

as well. Using \(_{}\) to denote the occupancy measures of all \(\),

\[_{_{}_{}}_{},r_{},c^{*} 0.\]

We observe a (for simplicity, optimal) solution to such a problem for \(K\) rewards, begging the question of when that is enough to uniquely identify \(c^{*}\). Recall that to uniquely determine the equation of a hyperplane in \(^{d}\), we need \(d\) linearly independent points. \(c^{*}^{||||}\), so we need \(||||\) expert policies. Furthermore, we need each of these points to lie on the constraint line and not on the boundary of the full polytope. Put differently, we need each distinct expert policy to _saturate_ the underlying constraint (i.e. \(\) s.t. \(J(_{E}^{},r^{k})<J(^{k},r^{k})\)). Under these conditions, we can uniquely determine the hyperplane that separates safe from unsafe policies, to which the constraint vector is orthogonal. More formally,

**Lemma 3.2**.: _Let \(_{E}^{1:||||}\) be distinct optimal expert policies such that a) \( i[||||]\), \(_{E}^{i}(_{})\) and b) no \(_{_{E}^{i}}\) can be generated by a mixture of the other visitation distributions. Then, \(c^{*}\) is the unique (up to scaling) nonzero vector in_

\[(_{_{E}^{1}}-_{_{E}^{2}}\\ \\ _{_{E}^{||||-1}}-_{_{E}^{||| |}}).\] (13)

We visualize this process for the \(||||=2\) case in Fig. 2. Assuming we are able to recover \(c^{*}\), we can guarantee that our learners will be able to act safely, regardless of the task they are asked to do. However, the assumptions required to do so are quite strong: we are effectively asking for our expert policies to form a basis for the space of occupancy measures, which means we must see expert data for a large set of diverse tasks. Furthermore, we need the experts to be reward-optimal.

Identifiability (the goal of prior works like Chou et al. (2020); Amin et al. (2017)) is too strong a goal as it requires us to estimate the value of the constraint _everywhere_ in the state-action space. If we know the learner will only be incentivized to go to a certain subset of states (as is often true in practice), we can guarantee safety without fully identifying \(c^{*}\). Therefore, we now consider how, by making distributional assumptions on how tasks are generated, we can generalize to novel tasks.

### A Statistical Condition for Generalization

Assume that tasks \(\) are drawn i.i.d. from some \(P()\). Then, even if we do not see a wide enough diversity of expert policies to guarantee identifiability of the ground-truth constraint function, with enough samples, we can ensure we do well in expectation over tasks. For some constraint \(c\), let us define

\[V(c)=_{ P()}[J(^{},c)-J(_{E}^{},c)],\] (14)

where \(^{},^{}=(^{},c)\) denote the solutions to the inner optimization problem. We begin by proving the following lemma.

**Lemma 3.3**.: _With_

\[K O((_{c}|}{})}{ ^{2}})\] (15)

_samples, we have that with probability \( 1-\), we will be able to estimate all \(|_{c}|\) population estimates of \(V(c)\) within \(\) absolute error._

Note that we perform the above analysis for finite classes but one could easily extend it (Sriperumbudur et al., 2009). The takeaway from the above lemma is that if we observe a sufficient number of tasks, we can guarantee that we can estimate the population loss of all constraints, up to some tolerance.

Consider the learner being faced with a new task they have never seen before at test time. Unlike in the single task case, where it is clear how to set the cost limit passed to CRL, it is not clear how to do so for a novel task. Hence, we make the following assumption.

**Assumption 3.4**.: We assume that \(_{}[J(_{E}^{},c^{*})] 0\), and that \( c_{c},\) s.t. \(J(,c) 0\).

This (weak) assumption allows us to a) use a cost limit of 0 for our CRL step and b) search over a subset of \(_{c}\) that the expert is safe under. Under this assumption, we are able to prove the following:

**Theorem 3.5**.: _Let \(c_{1:N}\) be the iterates produced by Algorithm 3 with \(K(,)\) chosen as in Lemma 3.3 and let \(=_{i}^{N}_{i}\) denote their time-averaged regret. Then, w.p. \( 1-\), there exists a \(c c_{1:N}\) such that \((r)=(r,c,=0)\) satisfies \(_{ P()}[J((r^{}),c^{*})-J(_{E}^{},c^{*})] T+3 T\) and \(_{ P()}[J((r^{}),r^{})-J(_{E}^{},r^{ })]-2 T\)._

In short, if we observe enough tasks, we are able to learn a constraint that, when optimized under, leads to policies that approximately Pareto-dominate those of the experts on average.

We now turn our attention to the practical implementation of these algorithms.

## 4 Practical Algorithm

We provide practical implementations of constrained reinforcement learning and inverse constraint learning and benchmark their performance on several continuous control tasks. We first describe the environments we test our algorithms on. Then, we provide results showing that our algorithms learn policies that match expert performance and constraint violation. While it is hard to guarantee constraint recovery in theory, we show that we can recover the ground-truth constraint empirically if we search over a restricted enough function class.

### Tasks

We focus on the ant environment from the PyBullet (Coumans and Bai, 2016) and MuJoCo (Todorov et al., 2012) benchmarks. The default reward function incentivizes progress along the positive \(x\) direction. For our single-task experiments, we consider a velocity and position constraint on top of this reward function.

1. **Velocity Constraint:**\(-q_{t}\|_{2}}{} 0.75\) where \(q_{t}\) is the ant's position
2. **Position Constraint:**\(0.5x_{t}-y_{t} 0\) where \(x_{t},y_{t}\) are the ant's coordinates

For our multi-task experiments, we build upon the D4RL (Fu et al., 2020) AntMaze benchmark. The default reward function incentivizes the agent to navigate a fixed maze to a random goal position: \((-\|q_{}-q_{t}\|_{2})\). We modify this environment such that the walls of the maze are permeable, but the agent incurs a unit step-wise cost for passing through the maze walls.

Our expert policies are generated by running CRL with the ground-truth constraint. We use the Tianshou (Weng et al., 2022) implementation of PPO (Schulman et al., 2017) as our baseline policy optimizer. Classical Lagrangian methods exactly follow the gradient update shown in Algorithm 1, but they are susceptible to oscillating learning dynamics and constraint-violating behavior during training. The PID Lagrangian method (Stooke et al., 2020) extends the naive gradient update of \(_{i}\) with a proportional and derivative term to dampen oscillations and prevent cost overshooting. To reduce the amount of interaction required to solve the inner optimization problem, we warm-start our policy in each iteration by behavior cloning against the given expert demonstrations. We used a single NVIDIA 3090 GPU for all experiments. Due to space constraints, we defer all other implementation details to Appendix B.

### ICL Results

We begin with results for the single-task problem, before continuing on to the multi-task setup.

### Single-Task Continuous Control Results

As argued above, we expect a proper ICL implementation to learn policies that perform as well and are as safe as the expert. However, by restricting the class of constraints we consider, we can also investigate whether recovery of the ground-truth constraint is possible. To this end, we consider a reduced-state version of our algorithm where the learned constraint takes a subset of the agent state as input. For the velocity constraint, the learned constraint is a linear function of the velocity, while for the position and maze constraints, the learned constraint is a linear function of the ant's position.

Using this constraint representation allows us to visualize the learned constraint over the course of training, as shown in Figure 3. We find that our ICL implementation is able to recover the constraint,as the learned constraint for both the velocity and position tasks converges to the ground-truth value. Our results further show that over the course of ICL training, the learned policies match and exceed expert performance as their violations of the ground-truth constraint converge towards the expert's. Figure 4 provides a direct depiction of the evolution of the learned constraint and policy. The convergence of the red and blue lines shows that the learned position constraint approaches the ground truth, and the policy's behavior approaches that of the expert in response to this.

To measure the robustness of our method to sub-optimal experts, we repeat the above experiments using demonstrations from an expert with i.i.d. Gaussian noise added to their actions at each timestep. We are still able to learn a safe and performant policy, which matches what our theory predicts.

### Multi-Task Continuous Control Results

We next consider an environment where, even with an appropriate constraint class, recovering the ground-truth constraint with a single task isn't feasible due to the ill-posedness of the inverse constraint learning problem. Specifically, we use the umaze AntMaze environment from D4RL (Fu et al., 2020), modified to have a more complex maze structure. As seen in Figure 7, the goal of each task is to navigate through the maze from one of the starting positions (top/bottom left) to one of the grid cells in the rightmost column. We provide expert data for all 10 tasks to the learner.

As we can see in Figure 6, multi-task ICL is, within a single iteration, able to learn policies that match expert performance and constraint violation across all tasks, _all without ever interacting with the ground-truth maze_. Over time, we are able to approximately recover the entire maze structure.

We visually compare several alternative strategies for using the multi-task demonstration data in the bottom row of Figure 7. The 0/1 values in the cells correspond to querying the deep constraint network from the last iteration of ICL on points from each of the grid cells and thresholding at some confidence. We see that a single-task network _(d)_ learns spurious walls that would prevent the learner from completing more than half of the tasks. Furthermore, learning 10 separate classifiers and then aggregating their outputs _(e)_ / _(f)_ also fails to produce reasonable outputs. However, when we use data from all 10 tasks to train our multi-task constraint network _(g)_ / _(h)_, we are able to approximately recover the walls of the maze. These results echo our preceding theoretical argument about the importance of multi-task data for learning constraints that generalize to future tasks.

Figure 4: As ICL training progresses, the learned position constraint (red line) converges to the ground-truth constraint (blue line) and the policy learns to escape unsafe regions (red region).

Figure 5: We re-conduct our position velocity constraint experiments using suboptimal experts to generate demonstrations. While, because of the ill-posedness of the problem, we do not exactly recover the ground truth constraint, we are able to use it to learn a policy that is higher performance than the expert while being just as safe. Standard errors are computed across 3 seeds.

Figure 3: Over the course of training, the learned ICL constraint recovers the ground-truth constraints for the velocity and position tasks. The learned policy matches expert performance and constraint violation. Standard errors are computed across 3 seeds.

We release the code we used for all of our experiments at https://github.com/konwook/mticl.

## 5 Discussion

In this work, we derive an algorithm for learning safety constraints from multi-task demonstrations. We show that by replacing the inner loop of inverse reinforcement learning with a constrained policy optimization subroutine, we can learn constraints that guarantee learner safety on a single task. We then give statistical and geometric conditions under which we can guarantee safety on unseen tasks by planning under a learned constraint. We validate our approach on several control tasks.

**Limitations.** In the future, we would be interested in applying our approach to real-world problems (e.g. offroad driving). Algorithmically, the CRL inner loop can be more computationally expensive than an RL loop - we would be interested in speeding up CRL using expert demonstrations, perhaps by adopting the approach of Swamy et al. (2023). We also ignore all finite-sample issues, which could potentially be addressed via data-augmentation approaches like that of Swamy et al. (2022).

## 6 Acknowledgements

We thank Drew Bagnell for edifying conversations on the relationship between ICL and IRL and Nan Jiang for connecting us to various references in the literature. We also thank an anonymous reviewer for pointing out that our method does not actually require the expert to be the optimal safe policy, a fact we did not fully appreciate beforehand. ZSW is supported in part by the NSF FAI Award #1939606, a Google Faculty Research Award, a J.P. Morgan Faculty Award, a Facebook Research Award, an Okawa Foundation Research Grant, and a Mozilla Research Grant. KK and GS are supported by a GPU award from NVIDIA.

Figure 6: We see that over ICL iterations, we are able to recover the ground-truth walls of the ant-maze, enabling the learner to match expert performance and constraint violations. Results for the second two plots are averaged across all 10 tasks. Standard errors are computed across 3 seeds.

Figure 7: We consider the problem of trying to learn the walls of a custom maze **(a)** based on the AntMaze environment from D4RL (Fu et al., 2020). We consider both a single-task **(b)** and multi-task **(c)** setup. We see that the single-task data is insufficient to learn an accurate constraint **(d)**. Averaging or taking the max over the constraints learned from the data for each of the ten goals **(e)-(f)** also doesn’t work. However, if we use the data from all 10 tasks to learn the constraint **(g)-(h)**, we are able to approximately recover the ground-truth constraint with enough constraint learning iterations.