# Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities

Andrii Zadaianchuk\({}^{1,2}\) Maximilian Seitzer\({}^{1}\) Georg Martius\({}^{1}\)

\({}^{1}\) Max Planck Institute for Intelligent Systems, Tubingen, Germany

\({}^{2}\) Department of Computer Science, ETH Zurich

andrii.zadaianchuk@tuebingen.mpg.de

equal contribution

###### Abstract

Unsupervised video-based object-centric learning is a promising avenue to learn structured representations from large, unlabeled video collections, but previous approaches have only managed to scale to real-world datasets in restricted domains. Recently, it was shown that the reconstruction of pre-trained self-supervised features leads to object-centric representations on unconstrained real-world image datasets. Building on this approach, we propose a novel way to use such pre-trained features in the form of a temporal feature similarity loss. This loss encodes semantic and temporal correlations between image patches and is a natural way to introduce a motion bias for object discovery. We demonstrate that this loss leads to state-of-the-art performance on the challenging synthetic MOVi datasets. When used in combination with the feature reconstruction loss, our model is the first object-centric video model that scales to unconstrained video datasets such as YouTube-VIS. https://marius-lab.github.io/videosaur/

## 1 Introduction

Autonomous systems should have the ability to understand the natural world in terms of independent entities. Towards this goal, unsupervised object-centric learning methods  learn to structure scenes into object representations solely from raw perceptual data. By leveraging large-scale datasets, these methods have the potential to obtain a robust object-based understanding of the natural world. Of particular interest in recent years have been video-based methods , not least because the temporal information in video presents a useful bias for object discovery . However, these approaches are so far restricted to data of limited complexity, successfully discovering objects from natural videos only on closed-world datasets in restricted domains.

In this paper, we present the method _Video Slot Attention Using temporal feature similaRity_, VideoSAUR, that scales video object-centric learning to unconstrained real-world datasets covering diverse domains. To achieve this, we build upon recent advances in image-based object-centric learning. In particular, Seitzer et al.  showed that reconstructing pre-trained features obtained from self-supervised methods like DINO  or MAE  leads to state-of-the-art object discovery on complex real-world images. We demonstrate that combining this feature reconstruction objective with a video object-centric model  also leads to promising results on real-world YouTube videos.

We then identify a weakness in the training objective of current unsupervised video object-centric architectures : the prevalent reconstruction loss does not exploit the temporal correlations existing in video data for object grouping. To address this issue, we propose a novel self-supervised loss based on _feature similarities_ that explicitly incorporates temporal information (see Fig. 1). The loss works by predicting distributions over similarities between features of the current and futureframes. These distributions encode information about the motion of individual image patches. To efficiently predict those motions through the slot bottleneck, the model is incentivized to group patches with similar motion into the same slot, leading to better object groupings as patches belonging to an object tend to move consistently. In our experiments, we find that such a temporal similarity loss leads to state-of-the-art performance on challenging synthetic video datasets , and significantly boosts performance on real-world videos when used in conjunction with the feature reconstruction loss.

In video processing, model efficiency is of particular importance. Thus, we design an efficient object-centric video architecture by adapting the SlotMixer decoder  recently proposed for 3D object modeling for video decoding. Compared to previous decoder designs , the SlotMixer decoder scales gracefully with the number of slots, but has a weaker inductive bias for object grouping. We show that this weaker bias manifests in optimization difficulties in conjunction with conventional reconstruction losses, but trains robustly with our proposed temporal similarity loss. We attribute this to the _self-supervised nature_ of the similarity loss: compared to reconstruction, it requires predicting information that is not directly contained in the input; the harder task seems to compensate for the weaker bias of the SlotMixer decoder.

To summarize, our contributions are as follows: (1) we propose a novel self-supervised loss for object-centric learning based on temporal feature similarities, (2) we combine this loss with an efficient video architecture based on the SlotMixer decoder where it synergistically reduces optimization difficulties, (3) we show that our model improves the state-of-the-art on the synthetic MOVi datasets by a large margin, and (4) we demonstrate that our model is able to learn video object-centric representations on the YouTube-VIS dataset , while staying fully unsupervised. This paper takes a large step towards unconstrained real-world object-centric learning on videos.

## 2 Related Work

Video Object-Centric LearningThere is a rich body of work on discovering objects from video, with two broad categories of approaches: tracking bounding boxes [15; 16; 4; 17] or segmentation masks [2; 5; 18; 19; 20; 21; 22; 23; 24; 25]. Architecturally, most recent image-based models for object-centric learning [3; 9; 26] are based on an auto-encoder framework with a latent slot attention grouping module  that extracts a set of slot representations. For processing video data, a common approach [5; 6; 7; 21; 24] is then to connect slots recurrently over input frames; the slots from the previous frame act as initialization for extracting the slots of the current frame. We also make use of this basic framework.

Scaling Object-Centric LearningMost recent work has attempted to increase the complexity of datasets where objects can successfully be discovered, such as the synthetic ClevrTex  and MOVi

Figure 1: We propose a _self-supervised temporal similarity loss_ for training object-centric video models. For each patch at time \(t\), the model has to predict a distribution \(}_{t,t+k}\) indicating where all semantically-similar patches have moved to \(k\) steps into the future. The target distribution \(_{t,t+k}\) is computed with a softmax on the affinity matrix \(_{t,t+k}\) containing the cosine distance between all patch features \(_{t}\), \(_{t+k}\). The loss incentivizes the model to group areas with consistent motion and semantics into slots.

datasets . On natural data, object discovery has so far been limited to restricted domains with a limited variety of objects, such as YouTube-Aquarium and -Cars , or autonomous driving datasets like WaymoOpen or KITTI . On more open-ended datasets, previous approaches have struggled .

To achieve scaling, some works attempt to _improve the grouping module_, for example by introducing equivariances to slot pose transformations , smoothing attention maps , formulating grouping as graph cuts  or a stick-breaking process , or by overcoming optimization difficulties by introducing implicit differentiation [34; 35]. In contrast, we do not change the grouping module, but use the vanilla slot attention cell .

Another prominent approach is to introduce _better training signals_ than the default choice of image reconstruction. For example, one line of work instead models the image as a distribution of discrete codes conditional on the slots, either autoregressively by a Transformer decoder [7; 26], or via diffusion [36; 37]. While this strategy shows promising results on synthetic data, it so far has failed to scale to unconstrained real-world data .

An alternative is to step away from fully-unsupervised representation learning by introducing _weak supervision_. For instance, SAVi  predicts optical flow, and SAVi++  additionally predicts depth maps as a signal for object grouping. Other works add an auxiliary loss that regularizes slot attention's masks towards the masks of moving objects [8; 38]. Our model also has a loss that focuses on motion information, but uses an unsupervised formulation. OSRT  shows promising results on synthetic 3D datasets, but is restricted by the availability of posed multi-camera imagery. While all those approaches improve on the level of data complexity, it has not been demonstrated that they can scale to unconstrained real-world data.

The most promising avenue so far in terms of scaling to the real-world is to _reconstruct features from modern self-supervised pre-training methods_[10; 11; 39; 40]. Using this approach, DINOSAUR  showed that by optimizing in this highly semantic space, it is possible to discover objects on complex real-world image datasets like COCO or PASCAL VOC. In this work, we similarly use such self-supervised features, but for learning on video instead of images. Moreover, we improve upon reconstruction of features by introducing a novel loss based on similarities between features.

**Concurrent Work** Parallel to this work, two more slot attention-based methods were proposed that learn object-centric representations on real-world videos: SMTC  and SOLV . SMTC learns to extracts objects from videos by enforcing semantic and instance consistency over time using a student-teacher approach. SOLV extracts per-frame slots using invariant slot attention , applies a temporal consistency module and merges slots using agglomerative clustering; the model is also trained using DINOSAUR-style feature reconstruction, but on masked out intermediate frames.

## 3 Method

In this section, we describe the main new components of VideoSAUR -- our proposed object-centric video model -- and its training: a pre-trained self-supervised ViT encoder extracting frame features (Sec. 3.1), a temporal similarity loss that adds a motion bias to object discovery (Sec. 3.2), and the SlotMixer decoder to achieve efficient video processing (Sec. 3.3). See Fig. 2 for an overview.

### Slot Attention for Videos with Dense Self-Supervised Representations

VideoSAUR is based on the modular video object-centric architecture recently proposed by SAVi  and also used by STEVE . Our model has three primary components: (1) a pre-trained self-supervised ViT feature encoder, (2) a recurrent grouping module for temporal slot updates, and (3) the _SlotMixer_ decoder (detailed below in Sec. 3.3).

We start by processing video frames \(_{t}\), with time steps \(t\{1, T\}\), into patch features \(_{t}\):

\[_{t}=f_{}(_{t}),_{t}^{L D}\] (1)

where \(f_{}\) is a self-supervised Vision Transformer encoder (ViT)  with pre-trained parameters \(\), and \(_{t}\) is the input at time step \(t\). The ViT encoder processes the image by splitting it to \(L\) non-overlapping patches of fixed size (e.g. \(16 16\) pixels), adding positional encoding, and transforming them into \(L\) feature vectors \(_{t}\) (see App. C.2 for more details on ViTs). Note that the \(i\)'th feature retains an association to the \(i\)'th image patch; the features thus can be spatially arranged. Next,we transform the features from the encoder with a slot attention module  to obtain a latent set \(_{t}=\{_{t}^{i}\}_{i=1}^{K}\), \(s_{t}^{i}^{M}\) with \(K\) slot representations:

\[_{t}=_{}(_{t},_{t-1}).\] (2)

Slot attention is recurrently initialized with the slots of the previous time step \(t-1\), with initial slots \(s_{0}\) sampled independently from a Gaussian distribution with learned location and scale. Slot attention works by grouping input features into slots by iterating competitive attention steps; we refer to Locatello et al.  for more details. To train the model, we use a SlotMixer decoder \(g_{}\) (see Sec. 3.3) to transform the slots \(_{t}\) to outputs \(_{t}=g_{}(_{t})\). Those outputs are used as model predictions for the reconstruction and similarity losses introduced next.

### Self-Supervised Object Discovery by Predicting Temporal Similarities

We now motivate our novel loss function based on predicting temporal feature similarities. Video affords the opportunity to discover objects from motion: pixels that consistently move together should be considered as one object, sometimes called the "common fate" principle . However, the widely used reconstruction objective -- whether of pixels , discrete codes  or features  -- does not exploit this bias, as to reconstruct the input frame, the changes between frames do not have to be taken into account.

Taking inspiration from prior work using optical flow as a prediction target , we design a self-supervised objective that requires _predicting patch motion_: for each patch, the model needs to predict where all _semantically-similar_ patches have moved to \(k\) steps into the future. By comparing self-supervised features describing the patches, we integrate both semantic and motion information; this is in contrast to optical flow prediction, which only relies on motion. Specifically, we construct an affinity matrix \(_{t,t+k}\) with the cosine similarities between all patch features from the present frame \(_{t}\) and all features from some future frame \(_{t+k}\):

\[_{t,t+k}=_{t}}{\|_{t}\|}(_{t+k}} {\|_{t+k}\|})^{},_{t,t+k}[-1,1]^{L L}.\] (3)

As self-supervised features are highly semantic, the obtained feature similarities are high for patches that share the same semantic interpretation. Due to the ViT's positional encoding, the similarities also take spatial closeness of patches into account. Figure 3 shows several example affinity matrices.

Because there are ambiguities in our similarity-based derivation of feature movements, we frame the prediction task as _modeling a probability distribution_ over target patches -- instead of forcing the prediction of an exact target location, like with optical flow prediction. Thus, we define the

Figure 2: Overview of VideoSAUR. Object slots \(s_{t}\) are extracted from patch features \(_{t}\) of a self-supervised ViT using time-recurrent slot attention, conditional on slots from the previous time step \(t-1\). The model is trained by reconstructing the patch features \(_{t}\) of the current frame \(x_{t}\), and by predicting the similarity distribution over patches of a future frame \(x_{t+k}\) (see also Fig. 1). The predictions \(_{t}^{}\) and \(_{t}^{}\) are decoded efficiently using SlotMixer decoder.

probability that patch \(i\) moves to patch \(j\) by normalizing the rows of the affinity matrix with the softmax, while masking negative similarity values (superscripts refer to the elements of the matrix):

\[^{ij}=^{ij}/)}{_{k\{j|^{ij} 0\}}(^{ik}/)}&^{ij} 0,\\ 0&^{ij}<0,\] (4)

where \(\) is the softmax temperature. The resulting distribution can be interpreted as the _transition probabilities_ of a random walk along a graph with image patches as nodes . Then, we define the similarity loss as the cross entropy between decoder outputs and transition probabilities:

\[^{}_{,}=_{l=1}^{L}(^{l}_{t,t+k};^{l}_{t}).\] (5)

Figure 1 illustrates the loss computation for an example pair of input frames.

**Why is this Loss Useful for Object Discovery?** Predicting which parts of the videos move consistently is most efficient with an object decomposition that captures moving objects. This is similar to previous losses predicting optical flow . But in contrast, our loss (Eq. 5) also yields a useful signal for grouping when parts of the frame are _not_ moving: as feature similarities capture semantic aspects, the task also requires predicting which patches are semantically similar, helping the grouping into objects e.g. by distinguishing fore- and background (see Fig. 3). Optical flow for grouping also has limits when camera motion is introduced; in our experiments, we find that our loss is more robust in such situations. Methods based on optical flow or motion masks can also struggle with inaccurate flow/motion mask labels -- unlike our method, which does not require such labels. This is of particular importance for in-the-wild video, where motion estimation is challenging.

**Role of Hyperparameters.** The loss has two hyperparameters: the time shift into the future \(k\) and the softmax temperature \(\). The optimal time shift depends on the expected time scales of movements in the modeled videos and should be chosen accordingly. The temperature \(\) controls the concentration of the distribution onto the maximum. Thus, it effectively modulates between two different tasks: accurately estimating the patch motion (low \(\)), and predicting the similarity of each patch to all other patches (high \(\)). In particular in scenes with little movement, the latter may be important to maintain a meaningful prediction task. In our experiments, we find that the best performance is obtained with a balance between the two, showing that both modes are important.

**Final Loss.** While the temporal similarity loss yields state-of-the-art performance on synthetic datasets, as shown below, we found that on real-world data, performance can be further improved by adding the feature reconstruction objective as introduced in Seitzer et al. . We hypothesize this is because the semantic nature of feature reconstruction adds another useful bias for object discovery. Thus, the final loss is given by:

\[_{,}=_{t=1}^{T-k}^{}_{ ,}(_{t,t+k},^{}_{t})+^{ {rec}}_{,}(_{t},^{}_{t}),\] (6)

Figure 3: Affinity matrix \(_{t,t+k}\) and transition probabilities \(_{t,t+k}\) values between patches (marked by purple and green) of the frame \(_{t}\) and patches of the future frame \(_{t+k}\) in MOVI-C (left) and YT-VIS (right). Red indicates maximum affinity/probability. Also see Fig. B.4 for more examples, and our website for an interactive visualization of temporal feature similarities.

where \(_{t}=[_{t}^{}^{L L},_{t}^{}^{L D}]\) is the output of the SlotMixer decoder \(g_{}\) and \(\) is a weighting factor used to make the scales of the two losses similar (we use a fixed value of \(=0.1\) for all experiments on real-world datasets). Like in Seitzer et al. , we do not train the ViT encoder \(f_{}\).

### Efficient Video Object-Centric Learning with the SlotMixer Decoder

In video models, resource efficiency is of particular concern: recurrent frame processing increases the load on compute and memory. The standard mixture-based decoder design  decodes each output \(K\)-times, where \(K\) is the number of slots, and thus scales linearly with \(K\) both in compute and memory. This can become prohibitive even for a moderate number of slots. The recently introduced SlotMixer decoder  for 3D object-centric learning instead has, for all practical purposes, constant overhead in the number of slots, by only decoding once per output. Thus, we propose to use a SlotMixer decoder \(g_{}\) for predicting the probabilities \(_{t,t+k}\) from the slots \(_{t}\). To adapt the decoder from 3D to 2D outputs, we change the conditioning on 3D query rays to \(L\) learned positional embeddings, corresponding to \(L\) patch outputs \(_{t}^{}\). See App. C.1 for more details on the SlotMixer module.

As a consequence of the increased efficiency of SlotMixer, there also is increased flexibility of how slots can be combined to form the outputs. Because of this, this decoder has a weaker inductive bias towards object-based groupings compared to the standard mixture-based decoder. With the standard reconstruction loss, we observed that this manifests in training runs in which no object groupings are discovered. But in combination with our temporal similarity loss, these instabilities disappear (see App. B.4). We attribute this to the _self-supervised nature_ of the similarity loss2; having to predict information that is not directly contained in the input increases the difficulty of the task, reducing the viability of non-object based groupings.

## 4 Experiments

We have conducted a number of experiments to answer the following questions: (1) Can object-centric representations be learned from a large number of diverse real-world videos? (2) How does VideoSAUR perform in comparison to other methods on well-established realistic synthetic datasets? (3) What are the effects of our proposed temporal feature similarity loss and its parameters? (4) Can we transfer the learned object-grouping to unseen datasets? (5) How efficient is the SlotMixer decoder in contrast to the mixture-based decoder?

### Experimental Setup

DatasetsTo investigate the characteristics of our proposed method, we utilize three synthetic datasets and three real-world datasets. For synthetic datasets, we selected the MOVi-C, MOVi-D

Figure 4: Example predictions of VideoSAUR compared to recent video object-centric methods.

and MOVi-E datasets  that consist of numerous moving objects on complex backgrounds. Additionally, we evaluate the performance of our method on the challenging YouTube Video Instance Segmentation (YT-VIS) 2021 dataset  as an unconstrained real-world dataset. Furthermore, we examine how well our model performs when transferred from YT-VIS 2021 to YT-VIS 2019  and DAVIS . Finally, we use the COCO dataset  to study our proposed similarity loss function with image-based object-centric learning.

MetricsWe evaluate our approach in terms of the quality of the discovered slot masks (output by the decoder), using two metrics: video foreground ARI (FG-ARI)  and video mean best overlap (mBO) . FG-ARI is a video version of a widely used metric in the object-centric literature that measures the similarity of the discovered objects masks to ground truth masks. This metric mainly measures _how well objects are split_. mBO assesses the correspondence of the predicted and the ground truth masks using the intersection-over-union (IoU) measure. In particular, each ground truth mask is matched to the predicted mask with the highest IoU, and the average IoU is then computed across all assigned pairs. Unlike FG-ARI, mBO also considers background pixels, and provides a measure of _how accurately the masks fit the objects_. Both metrics also consider the consistency of the assigned object masks over the whole video.

In addition, we also use image-based versions of those metrics (_Image FG-ARI_ and _Image mBO_, computed on individual frames) for comparing with image-based methods.

BaselinesWe compare our method with two recently proposed methods for unsupervised object-centric learning for videos: SAVi  and STEVE . SAVi uses a mixture-based decoder and is trained with image reconstruction. We use the unconditional version of SAVi. STEVE uses a transformer decoder and is trained by reconstructing discrete codes of a dVAE . Similar to Seitzer et al. , we also add a regular block pattern baseline, corresponding to splitting the video into regular block masks of similar size that do not change over time. By showing the metric values for a trivial decomposition of the video, this baseline is useful to contextualize the results of the other methods. In addition to video-based methods, we compare our model to image-based methods, including DINOSAUR , LSD  and Slot Diffusion , showing that our approach performs well in both object separation and mask sharpness. Last, we also compare our model to two concurrent works discovering objects from real-world video, SMTC  and SOLV .

### Comparison with State-of-the-Art Object-Centric Learning Methods

When comparing VideoSAUR to STEVE and SAVi, it is evident that VideoSAUR outperforms the baselines by a significant margin, both in terms of FG-ARI and mBO (see Table 1 and Fig. 4). On the most challenging synthetic dataset (MOVi-E), VideoSAUR reaches \(73.9\) FG-ARI. Notably, for the challenging YT-VIS 2021 dataset, both baselines perform comparable or worse than the block pattern baseline in terms of FG-ARI, showing that previous methods struggle to decompose real-world videos into consistent objects. We additionally compare VideoSAUR to image-based methods in App. A.1, including strong recent methods (LSD, SlotDiffusion and DINOSAUR), and find that our approach also outperforms the prior image-based SoTA. Finally, in App. A.2, we find that our method performs competitively with concurrent work.

Next, we report how well our method performs in terms of zero-shot transfer to other datasets to show that the learned object discovery does generalize to unseen data. In particular, we train VideoSAUR

    &  &  &  \\   & FG-ARI & mBO & FG-ARI & mBO & FG-ARI & mBO \\  Block Pattern & 24.2 & 11.1 & 36.0 & 16.5 & 24 & 14.9 \\ SAVi  & \(22.2 2.1\) & \(13.6 1.6\) & \(42.8 0.9\) & \(16.0 0.3\) & \(11.1 5.6\) & \(12.7 2.3\) \\ STEVE  & \(36.1 2.3\) & \(26.5 1.1\) & \(50.6 1.7\) & \(26.6 0.9\) & \(20.0 1.5\) & \(20.9 0.5\) \\ VideoSAUR & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Comparison with state-of-the-art methods on the MOVi-C, MOVi-E, and YT-VIS datasets. We report foreground adjusted rand index (FG-ARI) and mean best overlap (mBO) over 5 random seeds. Both metrics are computed for the whole video (24 frames for MOVi, 6 frames for YT-VIS).

on the YT-VIS 2021 dataset and evaluate it on the YT-VIS 2019 and DAVIS datasets. YT-VIS 2019 has similar object categories, but a smaller number of objects per image. The DAVIS dataset consists of videos from a fully different distribution than YT-VIS 2021. As the number of slots can be changed during evaluation, we test VideoSAUR with different number of slots, revealing that the optimal number of slots is indeed smaller for these datasets. We find that our method achieves a performance of \(41.3 0.9\) mBO on YT-VIS 2019 dataset and \(34.0 0.4\) mBO on DAVIS dataset (see Fig. 5), illustrating its capability to effectively transfer the learned representations to previously unseen data with different object categories and numbers of objects.

Long-term Video ConsistencyIn addition to studying how VideoSAUR performs on relatively short 6-frame video segments from YT-VIS, we also evaluate our method on longer videos. In App. B.1, we show the performance for 12-frame and full YT-VIS videos. While, as can be expected, performance on longer video segments is smaller in terms of FG-ARI, we show that the gap between VideoSAUR and the baselines is large, indicating that VideoSAUR can track the main objects in videos over longer time intervals. Closing the gap between short-term and long-term consistency using memory modules [24; 51] is an interesting future direction that could be useful for video prediction  as well as for object-centric goal-based [53; 54] and model-based  reinforcement learning.

### Analysis

In this section, we analyze various aspects of our approach, including the importance of the similarity loss, the impact of hyperparameters (time-shift \(k\) and softmax temperature \(\)), and the effect of the choice of self-supervised features and decoder.

Choice of Loss Function (Table 2 and Table 3)We conduct an ablation study to demonstrate the importance of the proposed temporal similarity loss, comparing and combining it with the feature reconstruction loss . We also consider predicting the features of the _next frame_ (see App. C.4 for implementation details). For all datasets, feature reconstruction alone performs significantly worse than the combination of feature reconstruction and temporal similarity loss. Predicting the features of the next frame in addition to feature reconstruction also yields improved performance, but is worse than the temporal similarity, suggesting that the success of our loss can be partially explained by the integration of temporal information through future prediction. Interestingly, on MOVi-C, using the temporal similarity loss alone significantly improves the performance over feature reconstruction (\(+20\) FG-ARI, \(+7\) mBO). To provide insight into the qualitative differences between the losses, we analyze the videos with the most significant differences in FG-ARI (see Fig. E.4): unlike feature reconstruction, the temporal similarity loss does not fragment the background or large objects into numerous slots, and it exhibits improved object-tracking capabilities even when object size changes. To gain further insights, we also consider (ground truth) _optical flow_ as a prediction target that only captures motion, but no semantic information (see App. B.2 for a detailed discussion). We find that only predicting optical flow is not enough for a successful scene decomposition, underscoring the importance of integrating both motion and semantic information for real-world object discovery.

Robustness to Camera Motion (Table 4)Next, we investigate if VideoSAUR training with the similarity loss is robust to camera motion, as such motion makes isolating the object motion more difficult. As a controlled experiment, we compare between MOVi-D (without camera motion) and

Figure 5: Zero-shot transfer of learned object-centric representations on YT-VIS 2021 to the YT-VIS 2019 and DAVIS datasets for different number of slots.

MOVi-E (with camera motion), and train VideoSAUR using only the temporal similarity loss. We contrast with SAVi trained with optical flow prediction3, and find that VideoSAUR is more robust to camera motion, performing better on the MOVi-E dataset than on the MOVi-D dataset (\(+6.8\) vs \(-16.7\) FG-ARI for SAVi).

Choice of Decoder (Table 5)We analyze how our method performs with different decoders and find that both the MLP broadcast decoder  and our proposed SlotMixer decoder can be used for optimizing the temporal similarity loss. VideoSAUR with the MLP broadcast decoder achieves similar performance on YT-VIS and MOVi datasets, but requires 2-3 times more GPU memory (see App. C.1 for more details and Table B.3 for the detailed comparison of decoders on MOVI-E dataset). Thus, we suggest to use the SlotMixer decoder for efficient video processing.

Softmax Temperature (Figure 5(a))We train VideoSAUR with DINO S/16 features using different softmax temperatures \(\). We find that there is a sweet spot in terms of grouping performance at \(=0.075\). Lower and higher temperatures lead to high variance across seeds, potentially because there is not enough training signal with very peaked (low \(\)) and diffuse (high \(\)) target distributions.

Target Time-shift (Figure 5(b))We train VideoSAUR with DINO S/16 features using different time-shifts \(k\) to construct the affinity matrix \(_{t,t+k}\). On both synthetic and real-world datasets, \(k=1\) generally performs best. Interestingly, we find that for \(k=0\), performance drops, indicating that predicting pure self-similarities is not a sufficient task for discovering objects on its own.

Choices for Self-Supervised Features (Figures 5(c) and 5(d))We study two questions about the usage of the ViT features: which ViT features (queries/keys/values/outputs) should be used for the temporal similarity loss? Do different self-supervised representations result in different performance? In Fig. 5(c), we observe that using DINO "key" and "query" features leads to significantly larger mBO, while for FG-ARI "query" is worse and the other features are similar. Potentially, this is because keys are used in the ViT's self-attention and thus could be particularly good to compare with the scalar product similarity. Consequently, VideoSAUR uses "key" features in all other experiments. Moreover, we study if the temporal similarity loss is compatible with different self-supervised representations. In Fig. 5(d), we show that VideoSAUR works well with 4 different types of representations, with MSN  and DINO  performing slightly better than MAE  and MOCO-v3 . We also demonstrate that _further fine-tuning the DINO features_ utilizing a self-supervised temporal-alignment clustering approach named TimeTuning  on unlabeled videos enhances the mask quality of VideoSAUR.

Pre-training Dataset (Table 6)All self-supervised methods we utilize are trained on the ImageNet dataset, which a) has a strong bias towards object-centricness as its images mostly contain single objects, and b) introduces a large number of additional images external to the dataset we are training VideoSAUR on. An interesting question is whether a) and b) are actually required for the success of our method. To answer it, we train a ViT-B/16 encoder from scratch on the MOVi-E dataset using the MAE method, and then train VideoSAUR using the obtained features. Interestingly, we find that the features from MOVi-E yield similar results compared to ImageNet-trained features (although with slight drops in mask quality), demonstrating that VideoSAUR is able to perform high-quality object discovery even without access to external data. This result also has broader implications as it potentially increases the applicability of feature reconstruction-based object-centric methods to datasets fully out of the domain of ImageNet. It also raises a follow-up question: what properties of the pre-training dataset (and method) are important to obtain good target features for object discovery?

## 5 Conclusion

This paper presents the first method for unsupervised video-based object-centric learning that scales to diverse, unconstrained real-world datasets such as YouTube-VIS. By leveraging dense self-supervised features and extracting motion information with temporal similarity loss, we demonstrate superior performance on both synthetic and real-world video datasets. We hope our new loss function can inspire the design of further self-supervised losses for object-centric learning, especially in the video domain where natural self-supervision is available.

Still, our method does not come without limitations: in longer videos with occlusions, slots can get reassigned to different objects or the background (see Fig. B.5 for visualizations of failure cases). VideoSAUR also inherits a limitation of all slot attention-based method, namely that the the number of slots is static and needs to be chosen a priori. Similar to DINOSAUR , the quality of the object masks is restricted by the patch-based nature of the decoder. Finally, while the datasets we use in this work are significantly less constrained compared to datasets used by prior work, they still do not capture the full open-world setting that object-centric learning aspires to solve. Overcoming these limitations is a great direction for future work.

    &  &  \\   & FG-ARI & mBO & FG-ARI & mBO \\  VideoSAUR w/ _MAE+ImageNet_ features & 58.0 & 30.4 & 72.8 & 27.1 \\ VideoSAUR w/ _MAE+MOVi-E_ features & 59.8 & 27.5 & 70.6 & 23.3 \\   

Table 6: Comparing VideoSAUR with features trained on MOVi-E (_MAE+MOVi-E_) to features trained on ImageNet (_MAE+ImageNet_). For MAE+MOVi-E, we pre-train a ViT-B/16 using the self-supervised MAE method on MOVi-E for 200 epochs. VideoSAUR is able to perform high-quality object discovery even without access to any external data.

Figure 6: Studying the effect of different parameters of the temporal similarity loss.