# Diffusion Models With Learned Adaptive Noise

Subham Sekhar Sahoo

Cornell Tech, NYC, USA.

ssahoo@cs.cornell.edu &Aaron Gokaslan

Cornell Tech, NYC, USA.

akg87@cs.cornell.edu &Chris De Sa

Cornell University, Ithaca, USA.

cdesa@cs.cornell.edu &Volodymyr Kuleshov

Cornell Tech, NYC, USA.

kuleshov@cornell.edu

###### Abstract

Diffusion models have gained traction as powerful algorithms for synthesizing high-quality images. Central to these algorithms is the diffusion process, a set of equations which maps data to noise in a way that can significantly affect performance. In this paper, we explore whether the diffusion process can be learned from data. Our work is grounded in Bayesian inference and seeks to improve log-likelihood estimation by casting the learned diffusion process as an approximate variational posterior that yields a tighter lower bound (ELBO) on the likelihood. A widely held assumption is that the ELBO is invariant to the noise process: our work dispels this assumption and proposes multivariate learned adaptive noise (MuLAN), a learned diffusion process that applies noise at different rates across an image. Specifically, our method relies on a multivariate noise schedule that is a function of the data to ensure that the ELBO is no longer invariant to the choice of the noise schedule as in previous works. Empirically, MuLAN sets a new state-of-the-art in density estimation on CIFAR-10 and ImageNet and reduces the number of training steps by 50%. We provide the code1, along with a blog post and video tutorial on the project page:

https://s-sahoo.com/MuLAN

## 1 Introduction

Diffusion models, inspired by the physics of heat diffusion, have gained traction as powerful tools for generative modeling, capable of synthesizing realistic, high-quality images . Central to these algorithms is the diffusion process, a gradual mapping of clean images into white noise. The reverse of this mapping defines the data-generating process we seek to learn--hence, its choice can significantly impact performance . The conventional approach involves adopting a diffusion process derived from the laws of thermodynamics, which, albeit simple and principled, may be suboptimal due to its lack of adaptability to the dataset.

In this study, we investigate whether the notion of diffusion can be instead _learned from data_. Our motivating goal is to perform accurate log-likelihood estimation and probabilistic modelling, and we take an approach grounded in Bayesian inference . We view the diffusion process as an approximate variational posterior: learning this process induces a tighter lower bound (ELBO) on the marginal likelihood of the data. Although previous work argued that the ELBO objective of a diffusion model is invariant to the choice of diffusion process , we show that this claim is only true for the simplest types of univariate Gaussian noise: we identify a broader class of noising processes whose optimization yields significant performance gains.

Specifically, we propose a new diffusion process, multivariate learned adaptive noise (MuLAN), which augments classical diffusion models  with three innovations: a per-pixel polynomial noise schedule, an adaptive input-conditional noising process, and auxiliary latent variables. In practice, this method learns the schedule by which Gaussian noise is applied to different parts of an image, and allows tuning this noise schedule to the each image instance.

Our learned diffusion process yields improved log-likelihood estimates on two standard image datasets, CIFAR10 and ImageNet. Remarkably, we achieve state-of-the-art performance with less than half of the training time of previous methods. Our method also does not require any modifications to the underlying UNet architecture, which makes it compatible with most existing diffusion algorithms.

ContributionsIn summary, our paper makes the following contributions:

1. We demonstrate that the ELBO of a diffusion model is not invariant to the choice of noise process for many types of noise, thus dispelling a common assumption in the field.
2. We introduce MuLAN, a learned noise process that adaptively adds multivariate Gaussian noise at different rates across an image in a way that is conditioned on arbitrary context (including the image itself).
3. We empirically demonstrate that learning the diffusion process speeds up training and matches the previous state-of-the-art models using **2x less compute**, and also achieves a new **state-of-the-art** in density estimation on CIFAR-10 and ImageNet

## 2 Background

A diffusion process \(q\) transforms an input datapoint denoted by \(_{0}\) and sampled from a distribution \(q(_{0})\) into a sequence of noisy latent variables \(_{t}\) for \(t\) by progressively adding Gaussian noise of increasing magnitude . The marginal distribution of each latent is defined by \(q(_{t}|_{0})=(_{t};_{t} _{0},_{t})\) where the diffusion parameters \(_{t},_{t}^{+}\) implicitly define a noise schedule as a function of \(t\), such that \((t)=_{t}^{2}/_{t}^{2}\) is a monotonically decreasing function in \(t\). Given any discretization of time into \(T\) timesteps of width \(1/T\), we define \(t(i)=i/T\) and \(s(i)=(i-1)/T\) and we use \(_{0:1}\) to denote the subset of variables associated with these timesteps; the forward process \(q\) can be shown to factorize into a Markov chain \(q(_{0:1})=q(_{0})_{i=1}^{T}q(_{t(i)}| _{s(i)})\).

The diffusion model \(p_{}\) is defined by a neural network (with parameters \(\)) used to denoise the forward process \(q\). Given a discretization of time into \(T\) steps, \(p\) factorizes as \(p_{}(_{0:1})=p_{}(_{1})_{i=1}^{T}p_{ }(_{s(i)}|_{t(i)})\). We treat the \(_{t}\) for \(t>0\) as latent variables and fit \(p_{}\) by maximizing the evidence lower bound (ELBO) on the marginal log-likelihood given by:

\[ p_{}(_{0})=(p_{},q)+_{} [q(_{t(1):t(T)}|_{0})\|p_{}(_{t(1):t(T)}| _{0})](p_{},q)\] (1)

In most works, the noise schedule, as defined by \((t)\), is either fixed or treated as a hyper-parameter . Chen , Hoogeboom et al.  show that the noise schedule can have

Figure 1: _(Left) Comparison of noise schedule properties: Multivariate Learned Adaptive Noise schedule (MuLAN) (ours) versus a typical scalar noise schedule. Unlike scalar noise schedules, MuLAN’s multivariate and input-adaptive properties improve likelihood. (Right) Likelihood in bits-per-dimension (BPD) on CIFAR-10 without data augmentation._

a significant impact on sample quality. Kingma et al. (2015) consider learning \((t)\), but argue that the KL divergence terms in the ELBO are invariant to the choice of function \(\), except for the initial values \((0),(1)\), and they set these values to hand-specified constants in their experiments. They only consider learning \(\) for the purpose of minimizing the variance of the gradient of the ELBO. In this work, we show that the ELBO is not invariant to more complex forward processes.

## 3 Diffusion Models With Multivariate Learned Adaptive Noise

Here, we introduce a new diffusion process, multivariate learned adaptive noise (MuLAN), which introduces three innovations: a per-pixel polynomial noise schedule, a conditional noising process, and auxiliary-variable reverse diffusion. We describe these below.

### Why Learned Diffusion?

Our goal is to perform accurate density estimation and probabilistic modelling, and we take an approach grounded in Bayesian inference Kingma and Ba (2015). Notice that the gap between the evidence lower bound ELBO\((p,q)\) and the marginal log-likelihood (MLL) in Eq. 1 is precisely the KL divergence \(_{}[q(_{t(1):t(T)}|_{0})\|\!\|_{}( _{t(1):t(T)}|_{0})]\) between the diffusion process \(q\) over the latents \(_{t}\) and the true posterior of the diffusion model. The diffusion process plays the role of a variational posterior \(q\) in ELBO\((p,q)\); optimizing \(q\) thus tightens the gap \((-)\).

This observation suggests that the ELBO can be made tighter by choosing a diffusion processes \(q\) that is closer to the true posterior \(p_{}(_{t(1):t(T)}|_{0})\). In fact, the key idea of variational inference is to optimize \(_{q}(p,q)\) over a family of approximate posteriors \(\) to induce a tighter ELBO Kingma and Ba (2015). Most diffusion algorithms, however optimize \(_{p}(p,q)\) within some family \(\) with a fixed \(q\). Our work seeks to jointly optimize \(_{p,q}(p,q)\); we will show in our experiments that this improves the likelihood estimation.

The task of log-likelihood estimation is directly motivated by applied problems such as data compression Kingma and Ba (2015). In that domain, arithmetic coding techniques can take a generative model and produce a compression algorithm that provably achieves a compression rate (in bits per dimension) that equals the model's log-likelihood Kingma and Ba (2015). Other applications of log-likelihood estimation include adversarial example detection Rezende et al. (2015), semi-supervised learning Kingma and Ba (2015), and others.

Note that our primary focus is density estimation and probabilistic modeling rather than sample quality. The visual appeal of generated images (as measured by e.g., FID) correlates imperfectly with log-likelihood. We focus here on pushing the state-of-the-art in log-likelihood estimation, and while we report FID for completeness, we defer sample quality optimization to future work.

### A Forward Diffusion Process With Multivariate Adaptive Noise

Next, our plan is to define a family of approximate posteriors \(\), as well as a family suitably matching reverse processes \(\), such that the optimization problem \(_{p,q}(p,q)\) is tractable and does not suffer from the aforementioned invariance to the choice of \(q\). This subsection focuses on defining \(\); the next sections will show how to parameterize and train a reverse model \(p\).

**Notation.** Given two vectors **a** and **b**, we use the notation **ab** to represent the Hadamard product (element-wise multiplication). Additionally, we denote element-wise division of **a** by **b** as **a** / **b**. We denote the mapping diag(.) that takes a vector as input and produces a diagonal matrix as output.

#### 3.2.1 Multivariate Gaussian Noise Schedule

Intuitively, a multivariate noise schedule injects noise at different rates for each pixel of an input image. This enables adapting the diffusion process to spatial variations within the image. We will also see that this change is sufficient to make the ELBO no longer invariant in \(q\).

Formally, we define a forward diffusion process with a multivariate noise schedule \(q\) via the marginal for each latent noise variable \(_{t}\) for \(t\), where the marginal is given by:

\[q(_{t}|_{0})=(_{t};_{t} _{0},(_{t}^{2})),\] (2)

where \(_{t},_{0}^{d}\), \(_{t},_{t}^{d}_{+}\) and \(d\) is the dimensionality of the input data. The \(_{t},_{t}\) denote varying amounts of signal associated with each component (i.e., each pixel) of \(_{0}\) as a function of time \(t(i)\). We define the multivariate signal-to-noise ratio as \((t)=_{t}^{2}/_{t}^{2}\) and choose \(_{t},_{t}\) so that \((t)\) decreases monotonically in \(t\) along all dimensions and is differentiable in \(t\). Let \(_{t|s}=_{t}/_{s}\) and \(_{t|s}^{2}=_{t}^{2}-_{t|s}^{2}/_{s} ^{2}\) with all operations applied elementwise.

These marginals induce transition kernels between steps \(s<t\) given by (Suppl. 19):

\[q(_{s}|_{t},_{0})=( _{s};_{q}=_{t|s}_{s}^{2}}{_{t}^{2}}_{t}+_{t|s}^{2}_{s}}{ {}_{t}^{2}}_{0},\ _{q}=(_{s}^{2}_{t|s}^{ 2}}{_{t}^{2}})).\] (3)

In Sec. 3.5, we argue that this class of diffusion process \(\) induces an ELBO that is not invariant to \(q\). The ELBO consists of a line integral along the diffusion trajectory specified by \((t)\). A line integrand is almost always path-dependent, unless its integral corresponds to a conservative force field, which is rarely the case for a diffusion process . See Sec. 3.5 for details.

#### 3.2.2 Adaptive Noise Schedule Conditioned On Context

Next, we extend the diffusion process to support context-adaptive noise. This enables injecting noise in a way that is dependent on the features of an image. Formally, suppose we have access to a context variable \(^{m}\) which encapsulates high-level information regarding \(_{0}\). Examples of \(\) could be a class label, a vector of attributes (e.g., features characterizing a human face), or even the input \(_{0}\) itself. We define the marginal of the latent \(_{t}\) in the forward process as \(q(_{t}|_{0},)=(_{t};_{t}()_{0},_{t}^{2}())\); the reverse process can be similarly derived (Suppl. 19) as:

\[q(_{s}|_{t},_{0},)= (_{q}=_{t|s}()_ {s}^{2}()}{_{t}^{2}()}_{t}+_{t|s}^{2}()_{s}()}{_{t}^{2} ()}_{0},\ _{q}=(_{s}^{2}( )_{t|s}^{2}()}{_{t}^{2}() })),\] (4)

where the diffusion parameters \(_{t}\), \(_{t}\) are now conditioned on \(\) via a neural network.

Specifically, we parameterize the diffusion parameters \(_{t}(),_{t}(),(t,)\) as \(_{t}^{2}()=(-_{}(,t))\), \(_{t}^{2}()=(_{}(,t))\), and \((,t)=(-_{}(,t))\). Here, \(_{}(,t):^{m}[_{ },_{}]^{d}\) is a neural network with the property that \(_{}(,t)\) is monotonic in \(t\). Following Kingma et al. , Zheng et al. , we set \(_{}=-13.30\), \(_{}=5.0\).

We explore various parameterizations for \(_{}(,t)\). These schedules are designed in a manner that guarantees \(_{}(,0)=_{}_{}\) and \(_{}(,1)=_{}_{}\), Below, we list these parameterizations. The polynomial parameterization is novel to our work and yields significant performance gains.

**Monotonic Neural Network .** We use the monotonic neural network \(_{}(t)\), proposed in VDM to express \(\) as a function of \(t\) such that \(_{}(t):[_{},_{ {max}}]^{d}\). Then we use FiLM conditioning  in the intermediate layers of this network via a neural network that maps \(\). The activations of the FiLM layer are constrained to be positive.

**Polynomial.** (Ours) We express \(_{}(,t)\) as a monotonic degree 5 polynomial in \(t\). Details about the exact functional form of this polynomial and its implementation can be found in Suppl. E.2.

### Auxiliary-Variable Reverse Diffusion Processes

In principle, we can fit a normal diffusion model in conjunction with our proposed forward diffusion process. However, variational inference suggests that the variational and the true posterior ought to have the same dependency structure: that is the only way for the KL divergence between these two distributions to be zero. Thus, we introduce a class of approximate reverse processes \(\) that match the structure of \(\) and that are naturally suitable for joint optimization \(_{p,q}(p,q)\).

Formally, we define a diffusion model where the reverse diffusion process is conditioned on the context \(\). Specifically, given any discretization of \(t\) into \(T\) time steps as in Sec. 2, we introduce a context-conditional diffusion model \(p_{}(_{0:1}|)\) that factorizes as the Markov chain

\[p_{}(_{0:1}|)=p_{}(_{1}|) _{i=1}^{T}p_{}(_{s(i)}|_{t(i)},).\] (5)Given that the true reverse process is a Gaussian as specified in Eq. 4, the ideal \(p_{}\) matches this parameterization (the proof mirrors that of regular diffusion models; Suppl. D), which yields

\[p_{}(_{s}|,_{t})= (_{p}=_{t|s}()_{s}^{2}( )}{_{t}^{2}()}_{t}+_ {t|s}^{2}()_{s}()}{_{t}^{2}()}_{}(_{t},t),_{p}=(_{s}^{2}()_{t|s}^{2}()/_{t}^{ 2}())),\] (6)

where \(_{}(_{t},t)\), is a neural network that approximates \(_{0}\). Instead of parameterizing \(_{}(_{t},t)\) directly using a neural network, we consider two other parameterizations. One is the noise parameterization  where \(_{}(_{t},,t)\) is the denoising model which is parameterized as \(_{}(_{t},t)=(_{t}-_{t}( )_{}(_{t},t,))/_{t}( )\); see Suppl. E.1.1 and the other is v-parameterization  where \(_{}(_{t},,t)\) is a neural network that models \(_{}(_{t},,t)=(_{t}( )_{t}-_{}(_{t},,t))/_{t}()\); see Suppl. E.1.2.

#### 3.3.1 Challenges in Conditioning on Context

Note that the model \(p_{}(_{0:1}|)\) implicitly assumes the availability of \(\) at generation time. Sometimes, this context may be available, such as when we condition on a label. We may then fit a conditional diffusion process with a standard diffusion objective \(_{_{0:c}}[(_{0},p_{}(_{0:1}|),q_{}(_{0:1}|)]\), in which both the forward and the backward processes are conditioned on \(\) (see Sec. 3.4).

When \(\) is not known at generation time, we may fit a model \(p_{}\) that does not condition on \(\). Unfortunately, this also forces us to define \(p_{}(_{s}|_{t})=(_{p}(_{t},t),_{p}(_{t},t))\) where \(_{p}(_{t},t),_{p}(_{t},t)\) is parameterized directly by a neural network. We can no longer use a noise parameterization \(_{}(_{t},t)=(_{t}-_{t}( )_{}(_{t},t,))/_{t}( )\) because it requires us to compute \(_{t}()\) and \(_{t}()\), which we do not know. Since noise parameterization plays a key role in the sample quality of diffusion models , this approach limits performance.

#### 3.3.2 Conditioning Noise on an Auxiliary Latent Variable

We propose an alternative strategy for learning conditional forward and reverse processes \(p,q\) that feature the same structure and hence support efficient noise parameterization. Our approach is based on the introduction of auxiliary variables , which lift the distribution \(p_{}\) into an augmented latent space. Experiments (Suppl. D.3) and theory (Suppl. D) confirm that this approach performs better than parameterizing \(\) using a neural network, \(_{}(_{t},t)\).

Specifically, we introduce an auxiliary latent variable \(^{m}\) and define a lifted \(p_{}(,)=p_{}(|)p_{ }()\), where \(p_{}(|)\) is the conditional diffusion model from Eq. 5 (with context \(\) set to \(\)) and \(p_{}()\) is a simple prior (e.g., unit Gaussian or fully factored Bernoulli). The latents \(\) can be interpreted as a high-level semantic representation of \(\) that conditions both the forward and the reverse processes. Unlike \(_{0:1}\), the \(\) are not constrained to have a particular dimension and can be a low-dimensional vector of latent factors of variation. They can be continuous or discrete. The learning objective for the lifted \(p_{}\) is given by:

\[ p_{}(_{0}) _{q_{}(|_{0})}[ p_{ }(_{0}|)]-_{}(q_{}(| _{0})\|p_{}())\] (7) \[_{q_{}(|_{0})}(p_{ }(_{0:1}|),q_{}(_{0:1}|))- _{}(q_{}(|_{0})\|p_{}()),\] (8)

where \((p_{}(_{0:1}|),q_{}(_{0:1}| ))\) denotes the variational lower bound (VLB) of a diffusion model (defined in Eq. 1) with a forward process \(q_{}(_{0:1}|)\) (defined in Eq. 4 and Sec. 3.2.2) and and an approximate reverse process \(p_{}(_{0:1}|)\) (defined in Eq. 5), both conditioned on \(\). The distribution \(q_{}(|_{0})\) is an approximate posterior for \(\) parameterized by a neural network with parameters \(\).

Crucially, note that in the learning objective (Eq. 8), the context, which in this case is \(\), is available at training time in both the forward and reverse processes. At generation time, we can still obtain a valid context vector by sampling an auxiliary latent from \(p_{}()\). Thus, this approach addresses the aforementioned challenges and enables us to use the noise parameterization in Eq. 6.

Although we apply Jensen's inequality twice to get (8), this also enables us to learn the noise process, which significantly offsets any potential increase in ELBO gap reduction and improves \((p_{}(_{0:1}|),q_{}(_{0:1}| ))\) by optimizing over a more expressive class of posteriors. This claim is empirically validated in Table 2.

### Variational Lower Bound

Next, we derive a precise formula for the learning objective (8) of the auxiliary-variable diffusion model. Using the objective of a diffusion model in (1) we can write (8) as the sum of four terms:

\[ p_{}(_{0})_{q_{}}[_{}+_{}+_{}+_{ }],\] (9)

The reconstruction loss, \(_{}\), can be (stochastically and differentiably) estimated using standard techniques; see , \(_{}=-_{}[q_{}(_{1}| _{0},)\|p_{}(_{1})]\) is the diffusion prior term, \(_{}=-_{}[q_{}(|_{0})\|p_{}()]\) is the latent prior term, and \(_{}\) is the diffusion loss term, which we examine below. The complete derivation is given in Suppl. E.3.

#### 3.4.1 Diffusion Loss

Discrete-Time Diffusion.We start by defining \(p_{}\) in discrete time, and as in Sec. 2, we let \(T>0\) be the number of total time steps and define \(t(i)=i/T\) and \(s(i)=(i-1)/T\) as indexing variables over the time steps. We also use \(_{0:1}\) to denote the subset of variables associated with these timesteps. Starting with the expression in Eq. 1 and following the steps in Suppl. E, we can write \(_{}\) as:

\[_{} =-_{i=2}^{T}_{}[q_{}(_{s(i)}| _{t(i)},_{0},)\|p_{}(_{s(i)}| _{t(i)},)]\] \[=_{i=2}^{T}[(_{t}- _{}(_{t},,t(i)))^{}((,s(i))-(,t(i)) )(_{t}-_{}(_{t },,t(i)))]\] (10)

Continuous-Time Diffusion.We can also consider the limit of the above objective as we take an infinitesimally small partition of \(t\), which corresponds to the limit when \(T\). In Suppl. E we show that taking this limit of Eq. 10 yields the continuous-time diffusion loss:

\[_{}=-_{t}[( _{t}-_{}(_{t}, ,t))^{}(_{t}(,t))(_{t}-_{}( _{t},,t))]\] (11)

where \(_{t}(,t)^{d}\) denotes the Jacobian of \((,t)\) with respect to the scalar \(t\). We observe that the limit of \(T\) yields improved performance, matching the existing theoretical argument by Kingma et al. .

#### 3.4.2 Auxiliary latent loss

We try two different kinds of priors for \(p_{}()\): discrete (\(\{0,1\}^{m}\)) and continuous (\(^{m}\)).

Continuous Auxiliary Latents.In the case where \(\) is continuous, we select \(p_{}()\) as \((,_{m})\). This leads to the following KL loss term:

**Discrete Auxiliary Latents.** In the case where \(\) is discrete, we select \(p_{}()\) as a uniform distribution. Let \(\{0,1\}^{m}\) be a \(k\)-hot vector sampled from a discrete Exponential Family distribution \(p_{}(;)\) with logits \(\). Niepert et al.  show that \( p_{}(;)\) is equivalent to \(=_{y Y}+_{g},y\) where \(_{g}\) denotes the sum of gamma distribution Suppl. F, \(Y\) denotes the set of all \(k\)-hot vectors of some fixed length \(m\). For \(k>1\), To differentiate through the \(\) we use a relaxed estimator, Identity, as proposed by Sahoo et al. . This leads to the following KL loss term: \(_{}(q_{}(|_{0})\|p_{}())=-_{i=1}^{m}q_{}(|_{0})_{i}( q_{}( |_{0})_{i}+ m)\).

### The Variational Lower Bound as a Line Integral Over The Noise Schedule

Having defined our loss, we now return to the question of whether it is invariant to the choice of diffusion process. Notice that we may rewrite Eq. 11 in the following vectorized form:

\[_{}=-_{0}^{1}(_{0}-_{}(_{t},,t))^{2}_{t}( ,t)t\] (12)where the square is applied elementwise. We seek to rewrite (12) as a line integral \(_{a}^{b}((t))}{t} (t)t\) for some vector field \(\) and trajectory \((t)\). Recall that \((,t)\) is monotonically decreasing in each coordinate as a function of \(t\); hence, it is invertible on its image, and we can write \(t=_{}}^{-1}((,t))\) for some \(_{z}^{-1}\). Let \(}}_{}(_{(,t)},},(,t))}_{}(_{_{z} ^{-1}((,t))},},_{}}^{- 1}((,t)))\) and note that for all \(t\), we can write \(_{t}\) as \(_{(,t)}\); see Eq. 30, and have \(}}_{}(_{(,t)},},(,t))}_{}(_{t},},t)\). We can then write the integral in (12) as \(_{0}^{1}(_{0}-}}_{}(_{(,t)},},(,t)))^{2}}{t}(,t))t\), which is a line integral with \(((t))(_{0}-}}_{}( _{(,t)},},(,t)))^ {2}\) and \((t)(,t)\).

Intuitive explanation.Imagine piloting a plane across a region with cyclones and strong winds, as shown in Fig. 5. Plotting a direct, straight-line course through these adverse weather conditions requires more fuel and effort due to increased resistance. By navigating around the cyclones and winds, however, the plane reaches its destination with less energy, even if the route is longer.

This intuition translates into mathematical and physical terms. The plane's trajectory is denoted by \((t)_{+}^{n}\), while the forces acting on it are represented by \(((t))^{n}\). The work required to navigate is given by \(_{0}^{1}((t))}{t} (t),dt\). Here, the work depends on the trajectory because \(((t))\) is not a conservative field.

This concept also applies to the diffusion NELBO. From Eq. 12, it's clear that the trajectory \((t)\) is parameterized by the noise schedule \((,t)\), which is influenced by complex forces, \(\) (analogous to weather patterns), represented by the dimension-wise reconstruction error of the denoising model, \((_{0}-_{}(_{t},,t))^{2}\). Thus, the diffusion loss, \(_{}\), can be interpreted as the work done along the trajectory \((,t)\) in the presence of these vector field forces \(\). By learning the noise schedule, we can avoid "high-resistance" paths (those where the loss accumulates rapidly), thereby minimizing the overall "energy" expended, as measured by the NELBO. Since the diffusion process corresponds to non-conservative force fields, as noted in Spinney & Ford , different noise schedules should yield different NELBOs--a result supported by our empirical findings. In Suppl. E.5, we show that variational diffusion models are limited to linear trajectories \((t)\), rendering their objective invariant to the noise schedule. In contrast, our approach learns a multivariate \(\), enabling paths that achieve a better ELBO.

## 4 Experiments

This section reports experiments on the CIFAR-10  and ImageNet-32  datasets. We don't employ data augmentation and we use the same architecture and settings as in the VDM model . The encoder, \(q_{}(|)\), is modeled using a sequence of 4 ResNet blocks which is much smaller than the denoising network that uses 32 such blocks (i.e., we increase parameter count by only about 10%); the noise schedule \(_{}\) is modeled using a two-layer MLP. In all our experiments, we use discrete auxiliary latents with \(m=50\) and \(k=15\). A detailed description can be found in Suppl. G.

### Training Speed

In these experiments, we replace VDM's noise process with MuLAN. On CIFAR-10, MuLAN **attains VDM's likelihood score of 2.65 in just 2M steps, compared to VDM's 10M steps** 1). When trained on 4 V100 GPUs, VDM achieves a training rate of 2.6 steps/second, while MuLAN trains slightly slower at 2.24 steps/second due to the inclusion of an additional encoder network. However, despite this slower training pace, VDM requires 30 days to reach a BPD of 2.65, whereas Mulan achieves the same BPD within a significantly shorter timeframe of 10 days. On ImageNet-32, VDM integrated with MuLAN reaches a likelihood of 3.71 in half the time, **achieving this score in 1M steps versus the 2M steps required by VDM**.

### Likelihood Estimation

In Table 2, we also compare MuLAN with other recent methods on CIFAR-10 and ImageNet-32. MuLAN was trained using v-parameterization for \(8\)M steps on CIFAR-10 and 2M steps on Imagenet-32. During inference, we extract the underlying probability flow ODE and use it to estimate the log-likelihood; see Suppl. 1.2. Our algorithm **establishes a new state-of-the-art in density estimation** on both ImageNet-32 and CIFAR-10. In Table 8, we also compute variational lower bounds (VLBs) of \(\)2.59 and \(\)3.71 on CIFAR-10 and ImageNet, respectively. Each bound improves over published results (Table 2); our true NLLs (via flow ODEs) are even lower.

### Alternative Learned Diffusion Methods

Concurrent work that seeks to improve log-likelihood estimation by learning the forward diffusion process includes Neural Diffusion Models (NDMs)  and DiffEnc . In NDMs, the noise schedule is fixed, but the mean of each marginal \(q(_{t}|x_{0})\) is learned, while DiffEnc adds a correction term to \(q\). Diffusion normalizing flows (DNFs) represent an earlier effort where \(q\) is a normalizing flow trained by backpropagating through sampling. In Table 3, we compare against NDMs, DiffEnc, and DNFs on the CIFAR-10 dataset, using the authors' published results; note that their published ImageNet numbers are either not available or are reported on a different dataset version that is not comparable. Our approach to learned diffusion outperforms previous and concurrent work.

### Ablation Analysis And Additional Experiments

Due to the expensive cost of training, we only performed ablation studies on CIFAR-10 with a reduced batch size of \(64\) and trained the model for \(2.5\)M training steps. In Fig. 1(a) we ablate each component of MuLAN: when we remove the conditioning on an auxiliary latent space from MuLAN so that we have a multivariate noise schedule that is solely conditioned on time \(t\), our performance becomes comparable to that of VDM, on which our model is based. Changing to a scalar noise schedule based on latent variable \(\) initially underperforms compared to VDM. This drop aligns with our likelihood formula (Eq. 6) which includes \(_{}(q_{}(|_{0})|p_{}( ))\), an extra term not in VDM. The input-conditioned scalar schedule doesn't offer any advantage over the scalar schedule used in VDM. This is due to the reasons outlined in Sec. 3.5.

    &  &  \\  & Steps & VLB (\(\)) & FID (\(\)) & NFE (\(\)) & Steps & VLB (\(\)) & FID (\(\)) & NFE (\(\)) \\  VDM  & 10M & 2.65 & 23.91 & 56 & 2M & 3.72 & 14.26 & **56** \\ + MuLAN & **2M** & 2.65 & 18.54 & 55 & **1M** & 3.72 & 15.00 & 62 \\ + MuLAN & 10M & **2.60** & **17.62** & **50** & 2M & **3.71** & **13.19** & 62 \\   

Table 1: Likelihood in bits per dimension (BPD) based on the Variational Lower Bound (VLB) estimate (Suppl. 1.1), sample quality (FID scores) and number of function evaluations (NFE) on CIFAR-10, for vanilla VDM and VDM when endowed with MuLAN. FID and NFE were computed for 10k samples generated using an adaptive-step ODE solver. Both methods use noise parameterization (Suppl. E.1.1).

   Model & Type & CIFAR-10 (\(\)) & ImageNet (\(\)) \\  PixelCNN  & AR & 3.03 & 3.83 \\ Image Transformer  & AR & 2.90 & 3.77 \\ DDPM  & Diff & \( 3.69\) & / \\ ScoreFlow  & Diff & 2.83 & 3.76 \\ VDM  & Diff & \( 2.65\) & \( 3.72\) \\ Flow Matching  & Flow & 2.99 & / \\ Reflected Diffusion Models  & Diff & 2.68 & 3.74 \\  MuLAN (**Ours**) & Diff & **2.55**\( 10^{-3}\) & **3.67**\( 10^{-3}\) \\   

Table 2: Likelihood in bits per dimension (BPD) on the test set of CIFAR-10 and ImageNet. Results with “/” means they are not reported in the original papers. Model types are autoregressive (AR), normalizing flows (Flow), diffusion models (Diff). We only compare with results achieved without data augmentation.

   Model & NLL (\(\)) \\  DNF  & 3.04 \\ NDM  & \( 2.70\) \\ DiffEnc  & \( 2.62\) \\  MuLAN & **2.55** \\   

Table 3: Likelihood in bits per dimension (bpd) on CIFAR-10 for learned diffusion methods.

Perceptual QualityWhile perceptual quality is not the focus of this work, we report FID numbers for the VDM model and MuLAN (Table 1). We use RK45 ODE solver to generate samples by solving the reverse time Flow ODE (Eq. 76). We observe that MuLAN does not degrade FIDs, while improving log-likelihood estimation. Note that MuLAN does not incorporate many tricks that improve FID such as exponential moving averages, truncations, specialized learning schedules, etc.; our FID numbers can be improved in future work using these techniques.

Loss curves for different noise schedules.We investigate different parameterizations of the noise schedule in Fig. 1(b). Among polynomial, linear, and monotonic neural network, we find that the polynomial parameterization yields the best performance. The polynomial noise schedule is a novel component introduced in our work. The reason why a polynomial function works better than a linear or a monotonic neural network as proposed by VDM is rooted in Occam's razor. In Suppl. E.2, we show that a degree 5 polynomial is the simplest polynomial that satisfies several desirable properties, including monotonicity and having a derivative that equals zero exactly twice. More expressive models (e.g., monotonic 3-layer MLPs) are more difficult to optimize.

Examining the noise schedule.Since the noise schedule, \(_{}(,t)\) is multivariate, we expect to learn different noise schedules for different input dimensions and different inputs \( p_{}()\). In Fig. 3, we take our best trained model on CIFAR-10 and visualize the variance of the noise schedule at each point in time for different pixels, where the variance is taken on 128 samples \( p_{}()\).

We note an increased variation in the early portions of the noise schedule. However, on an absolute scale, the variance of this noise is smaller than we expected. We also tried to visualize noise schedules across different dataset images and across different areas of the same image; refer to Fig. 13. We also generated synthetic datasets in which each datapoint contained only high frequencies or only low frequencies, and with random masking applied to parts of the data points; see Suppl. H. Surprisingly, none of these experiments revealed human-interpretable patterns in the learned schedule, although we did observe clear differences in likelihood estimation. We hypothesize that other architectures and other forms of conditioning may reveal interpretable patterns of variation; however, we leave this exploration to future work.

Figure 3: Noise schedule visualizations for MuLAN on CIFAR-10. In this figure, we plot the variance of \(_{}(,t)\) across different \( p_{}()\) where each curve represents the SNR corresponding to an input dimension.

Figure 2: Ablating components of MuLAN on CIFAR-10 over 2.5M steps with batch size of 64.

Replacing the noise schedules in a trained denoising model.We also confirm experimentally our claim that the learning objective is not invariant to the multivariate noise schedule. We replace the noise schedule in the trained denoising model with two alternatives: MuLAN with scalar noise schedule, and a linear noise schedule: \(_{}(,t)=_{}+t(_{}- _{})_{}\); see Kingma et al. . For both the noise schedules the likelihood reduces to the same value as that of the VDM: \(2.65\).

## 5 Related Work

Diffusion models have emerged in recent years as powerful tools for modeling complex distributions [51; 16], extending flow-based methods [53; 24; 48; 49] The noise schedule, which determines the amount and type of noise added at each step, plays a critical role in diffusion models. Chen  empirically demonstrate that different noise schedules can significantly impact the generated image quality using various handcrafted noise schedules. Kingma et al.  showed that the likelihood of a diffusion model remains invariant to the noise schedule with a scalar noise schedule. In this work we show that the ELBO is no longer invariant to multivariate noise schedules.

Recent works explored multivariate noise schedules (including blurring, masking, etc.) [17; 42; 36; 12], yet none have delved into learning the noise schedule conditioned on the input data itself. Likewise, conditional noise processes are typically not learned [26; 39; 62] and their conditioner (e.g., a prompt) is always available. Auxiliary variable models [63; 60] add semantic latents in \(p\), but not in \(q\), and they don't condition or learn \(q\). In contrast, we learn multivariate noise conditioned on latent context.

Diffusion normalizing flows (DNFs)  learn a \(q\) parameterized by a normalizing flow; however, such \(q\) do not admit tractable marginals and require sampling full data-to-noise trajectories from \(q\), which is expensive. Concurrent work on neural diffusion models (NDMs) and DiffEnc admits tractable marginals \(q\) with learned means and univariate schedules; this yields more expressive \(q\) than ours but requires computing losses in a modified space that precludes using a noise parameterization and certain sampling strategies. Empirically, MuLAN performs better with fewer parameters (Suppl. A).

Optimal transport techniques seek to learn a noise process that minimizes the transport cost from data to noise, which in practice produces smoother diffusion trajectories that facilitate sampling. Schrondinger bridges [47; 6; 59; 37] learn expressive \(q\) do not admit analytical marginals, require computing full data-to-noise trajectories and involve iterative optimization (e.g., sinkhorn), which can be slow. Rectification  seeks diffusion paths that are close to linear; this improves sampling, while our method chooses paths that improve log-likelihood. See Suppl. A for more detailed comparisons.

## 6 Conclusion

We introduced MuLAN, a context-adaptive noise process that applies Gaussian noise at varying rates across input data. Our theory challenges the prevailing notion that the likelihood of diffusion models is independent of the noise schedule: this independence only holds true for univariate schedules. Our evaluation of MuLAN spans multiple image datasets, where it outperforms state-of-the-art generative diffusion models. We hope our work will motivate further research into the design of noise schedules, not only for improving likelihood estimation but also to improve image quality generation [35; 53]. A stronger fit to the data distribution also holds promise for improving downstream applications of generative modeling, e.g., compression or decision-making [32; 9; 8; 41].