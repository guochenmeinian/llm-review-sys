# MaskFactory: Towards High-quality Synthetic Data Generation for Dichotomous Image Segmentation

Haotian Qian\({}^{1}\)  YD Chen\({}^{1}\)  Shengtao Lou\({}^{1}\) Fahad Shahbaz Khan\({}^{3,4}\)

Xiaogang Jin\({}^{1}\)  Deng-Ping Fan\({}^{2,3}\)

\({}^{1}\)State Key Lab of CAD&CG, Zhejiang University \({}^{2}\)VCIP&CS, Nankai University

\({}^{3}\)MBZUAI \({}^{4}\)Linkoping University

Equal contribution. Deng-Ping Fan served as the project leader for this work.Corresponding author. Contact the author at jin@cad.zju.edu.cn

###### Abstract

Dichotomous Image Segmentation (DIS) tasks require highly precise annotations, and traditional dataset creation methods are labor intensive, costly, and require extensive domain expertise. Although using synthetic data for DIS is a promising solution to these challenges, current generative models and techniques struggle with the issues of scene deviations, noise-induced errors, and limited training sample variability. To address these issues, we introduce a novel approach, **MaskFactory**, which provides a scalable solution for generating diverse and precise datasets, markedly reducing preparation time and costs. We first introduce a general mask editing method that combines rigid and non-rigid editing techniques to generate high-quality synthetic masks. Specially, rigid editing leverages geometric priors from diffusion models to achieve precise viewpoint transformations under zero-shot conditions, while non-rigid editing employs adversarial training and self-attention mechanisms for complex, topologically consistent modifications. Then, we generate pairs of high-resolution image and accurate segmentation mask using a multi-conditional control generation method. Finally, our experiments on the widely-used DIS5K dataset benchmark demonstrate superior performance in quality and efficiency compared to existing methods. The code is available at https://qian-hao-tian.github.io/MaskFactory/.

## 1 Introduction

Dichotomous image segmentation (DIS) aims to accurately segment objects from natural images , a critical task in various computer vision applications, including medical imaging , autonomous driving  and connectomics research . However, traditional methods for collecting datasets for DIS tasks are labor-intensive, costly, and require extensive domain expertise. Recently, synthetic data has emerged as a promising solution for generating diverse and precise datasets at scale, offering a scalable and cost-effective means for model training. Given the importance of high-quality synthetic data for training models, developing methods to generate such datasets for DIS tasks is crucial. Although generative models have been employed to assist in synthetic dataset production, they face significant limitations in terms of controllability, precision, and diversity.

Recent synthetic methods  utilize diffusion models  to synthesize image-mask pairs from textual cues and attention maps. Despite progress, they still encounter three major challenges: (1) _Controllability:_ Text-guided generation scheme may deviate from real-world scenes, especially with fine-grained labels, leading to uncontrollable generated images. (2) _Precision:_ The use of attention maps can introduce noise, which degrades mask fidelity and poses challenges for applicationsrequiring precise pixel-level segmentation, such as DIS . 3. _Diversity_: Previous works [13; 14; 15] are relatively uniform, with limited ability to generate diverse training samples, which makes them unsuitable for providing sufficient variability in training data.

Generating diverse human images often poses challenge due to the complexity of prompts, which limits their applicability in tasks requiring extensive variability. It's worthy noting that the expansion of datasets with fine granularity, such as DIS datasets, currently lacks an effective solution. In fact, we observe that existing DIS datasets predominantly consist of single-source orthophotos featuring limited shape diversity among similar objects. The generation of more diverse DIS images that accurately reflect real-world distributions remains an unresolved challenge.

To address these challenges, we propose MaskFactory, a novel two-stage method that efficiently generates high-quality synthetic datasets for DIS tasks. Based on the geometric characteristics of objects, our approach simultaneously considers both rigid and non-rigid transformations of target objects. This means that when generating synthetic datasets, we take into account not only changes in viewpoints (rigid transformations) but also deformations (non-rigid transformations), as illustrated in Figure 1. In the first stage, rigid transformations are driven by geometric priors learned from large-scale diffusion models, enabling precise viewpoint changes and simulating diverse variations in observation angles and scales. Non-rigid transformations leverage prompts provided by large language models to accurately alter the shape of target objects via attention-based editing, ensuring topological consistency before and after editing through topology-preserving adversarial training. Consequently, even for masks with complex geometries, high-quality and diverse synthetic masks can be generated for DIS tasks. In the second stage, we utilize multiple control inputs, including masks, canny edges, and prompts representing class information, to guide the generation process. These inputs are fed into diffusion models to produce high-resolution images that match the prepared segmentation masks. This approach ensures high consistency between images and masks while enhancing the realism and diversity of the dataset.

Our method significantly enhances the realism and diversity of generated datasets. We validate the effectiveness of MaskFactory on the DIS5K dataset , specifically designed to evaluate DIS performance. Experimental results show that our approach outperforms existing dataset synthesis methods in terms of structural preservation and error metrics, achieving an average performance gain of 8.8%. These findings highlight the potential of MaskFactory to provide the data diversity and annotation precision required for DIS tasks while significantly reducing the time and costs associated with dataset preparation.

In summary, the contributions of our work are listed as follows:

* We introduce MaskFactory, a novel approach that generate high-quality datasets for DIS task in terms of quality, precision, and efficiency.

Figure 1: Shows the edited masks from the first stage and the corresponding images generated in the second stage. In the examples, we transformed the viewpoint of park benches and tables from a frontal view to a top-down view and edited their shapes, changing park benches from curved to square edges and tables from square to circular shapes.

* We propose a two-step method that synthesizes high-quality and diverse object masks via masking editing and generates corresponding high-resolution images using a multi-conditional control generation method.
* Experimental results on the DIS5K dataset demonstrate the superior performance of MaskFactory, compared to existing dataset synthesizing methods.

## 2 Related work

Synthetic data.Synthetic data has garnered significant attention in various machine learning and computer vision tasks, such as natural language processing , object detection , and image segmentation . For instance, Lin _et al._ demonstrated that synthetic data can enhance performance in object detection, especially in scenarios with limited access to real-world data. Generative adversarial networks (GANs) and variational autoencoders (VAEs) are two popular deep learning-based methods for generating synthetic data . Recent studies have focused on improving the quality and diversity of synthetic data  and exploring its use in few-shot learning . Additionally, synthetic data has been shown to enhance the interpretability and explainability of machine learning models . Benefiting from recent advancements in diffusion models [12; 22], methods such as DatasetDM  and Dataset Diffusion  can generate high-quality synthetic data for various computer vision tasks. However, these methods may introduce additional errors due to the inclusion of pseudo-labels. These errors arise because pseudo-labels can be noisy and inaccurate, leading to suboptimal training data. Although some research  employs controlnet-like control schemes  to mitigate these issues, the generated synthetic data may still suffer from noise and errors, making it unsuitable for the DIS task. Therefore, in this paper, we focus on generating high-quality synthetic data for the DIS task, where the generated image-mask pairs need to be highly precise and accurate.

Figure 2: Workflow of MaskFactory. In the first stage, we generate new masks by applying rigid and non-rigid editing to the existing ground truth masks. In the second stage, we use the generated masks and their corresponding extracted Canny edges as conditions, along with a prompt representing the category, to generate RGB images. This process forms paired data for our generative model.

Dichotomous image segmentation.DIS has seen substantial progress with the advent of high-resolution imaging technologies. Representative approaches include the intermediate supervision strategy in IS-Net , the frequency prior method , and the unite-divide-unite strategy by . However, these methods have enhanced segmentation accuracy but often struggle to capture extremely fine details. Thus, recent progressive refinement strategies, _e.g._, BASNet , emphasize the importance of auxiliary information such as gradient maps and multi-scale inputs. These methods propose to use gradient features and ground truth supervision to enhance the learning of weak features in complex regions. Then, BiRefNet  maintains high-resolution inputs and employs a bilateral reference framework to better capture intricate details. The latest works, such as the multi-view aggregation network by Yu et al.  and the interactive segmentation approach by Liu et al. , further advance the field by integrating diverse prompts and enhancing feature representation for high-quality segmentation.

## 3 Method

### Overview of MaskFactory

Current image segmentation methods are significantly constrained by their dependence on limited manually annotated data, which hampers both performance and generalization. To mitigate this, pseudo-label generation is often employed to augment training datasets. However, in the context of DIS, these methods frequently introduce artifacts that degrade segmentation quality. Additionally, existing image editing techniques often fail to preserve the topological structure of binary masks, resulting in discontinuities and overlaps within target regions.

To address these challenges, we introduce the MaskFactory framework, designed to generate a large number of high-quality synthetic image-mask pairs \(=\{(g_{i}^{r},g_{i}^{m})\}_{i=1}^{M}\) from an original dataset \(=\{(I_{i}^{r},I_{i}^{m})\}_{i=1}^{N}\). This approach aims to enhance the performance of DIS models. As illustrated in Figure 2, our framework comprises two main stages: mask editing and image generation.

In the mask editing stage, source masks from the original dataset are transformed using both rigid and non-rigid editing methods, resulting in a set of high-precision synthetic masks \(^{m}=\{g_{i}^{m}\}_{i=1}^{M}\). Rigid mask editing generates synthetic masks from various perspectives, while non-rigid mask editing employs a topology-preserving adversarial training mechanism to edit masks according to semantic prompts while retaining the structural integrity of the source masks.

In the image generation stage, a multi-conditional control generation method is utilized to produce realistic RGB images \(^{r}=\{g_{i}^{r}\}_{i=1}^{M}\) that correspond precisely to the synthetic masks, using the latter as conditioning constraints.

### Mask Editing Stage

#### 3.2.1 Rigid Mask Editing

Rigid mask editing aims to preserve detailed information from the source masks through rigid transformations. We leverage the Zero123  method, which employs a viewpoint-conditioned diffusion model \(_{}\) to manipulate masks' perspectives. Given the relative camera rotation and translation \(_{i}\) for the desired viewpoint, \(_{}\) synthesizes a new mask \(g_{i}^{m}\) based on the source mask \(I_{i}^{m}\), such that \(g_{i}^{m}=_{}(I_{i}^{m^{}},_{i})\), where \(I_{i}^{m^{}}\) is the inverted image of the source mask \(I_{i}^{m}\) to ensure the main component is non-zero.

#### 3.2.2 Non-Rigid Mask Editing

Non-rigid mask editing, inspired by MasaCtrl , is a critical component of MaskFactory. Unlike MasaCtrl, which directly manipulates the source mask \(I_{i}^{m}\) using a textual prompt \(P_{i}\) to generate a synthetic mask \(g_{i}^{m}\), we introduce a topology-preserving adversarial training mechanism to mitigate artifacts and structural degradation in binary mask editing. The textual prompt \(P_{i}\) is derived from a pool of prompts \(\{p_{i}^{m}\}\) that are generated using GPT-4 based on the original images. These prompts provide detailed descriptions that guide the mask editing process. This module consists of a generator \(G_{}\) and a discriminator \(D_{}\). The generator transforms noise \(\) into a synthetic mask \(g_{i}^{m}\) under the guidance of a textual prompt \(P_{i}\) and the source mask \(I_{i}^{m}\). A mutual attention mechanism aligns the query features \(_{t}\) of \(g_{i}^{m}\) with the key and value features \(_{s},_{s}\) of \(I_{i}^{m}\), ensuring consistency during editing. To avoid foreground-background confusion, a mask \(\) is extracted from the cross-attention maps to guide the model's focus.

Topology-Preserving Adversarial Training.To maintain the structural information of the source mask, we first extract an edge map \(E_{s}=(I_{i}^{m})\) using an edge detection operator \(\), obtaining key points \(=\{v_{j}\}_{j=1}^{N_{v}}\). We then construct a graph \(=(,_{s})\) based on these key points. The discriminator \(D_{}\) performs adversarial training on the structural graphs \(_{g}\) and \(_{s}\) of the synthetic mask \(g_{i}^{m}\) and the source mask, respectively, ensuring topological consistency.

The training objective of the discriminator \(D_{}\) is to maximize:

\[_{}_{_{s} p_{}(_{s})}[  D_{}(_{s})]+_{_{g} p_{}(_{g})}[(1-D_{}(_{g}))],\] (1)

where \(p_{}(_{s})\) and \(p_{}(_{g})\) represent the distributions of the structural graphs of the source masks and the synthetic masks, respectively. Conversely, the training objective of the generator \(G_{}\) is to minimize the discriminative power of the discriminator:

\[_{}_{_{g} p_{}(_{g}) }[(1-D_{}(_{g}))].\] (2)

Through topology-preserving adversarial training, the non-rigid editing module effectively retains the structural information from the source masks during the editing process, generating high-quality, artifact-free synthetic masks.

The overall loss function \(_{}\) for non-rigid mask editing encompasses the adversarial loss \(_{}\), the content loss \(_{}\), and the structure preservation loss \(_{}\):

\[_{}=_{}+_{1}_{ }+_{2}_{},\] (3)

where \(_{1}\) and \(_{2}\) are balancing factors.

The adversarial loss \(_{}\) is defined as:

\[_{}=_{_{s} p_{}( _{s})}[ D_{}(_{s})]+_{_{g}  p_{}(_{g})}[(1-D_{}(_{g}))].\] (4)

The content loss \(_{}\) measures the semantic consistency between the synthetic mask and the textual prompt:

\[_{}=\|g_{i}^{m}-I_{i}^{m}\|_{1},\] (5)

where \(g_{i}^{m}\) is the synthetic mask and \(I_{i}^{m}\) is the source mask.

The structure preservation loss \(_{}\) evaluates the difference between the structural graphs of the synthetic mask and the source mask:

\[_{}=\|_{g}-_{s}\|_{1}.\] (6)

These components ensure that the editing process maintains the structural and semantic consistency of the source masks.

### Image Generation Stage

Following the mask editing stage, we obtain a synthetic mask pool \(\{g_{i}^{m}\}_{i=0}^{M}\) comprising finely detailed synthetic masks generated through the aforementioned transformations. In the subsequent image generation stage, we accurately generate corresponding RGB images \(\{g_{i}^{r}\}_{i=0}^{M}\) for the masks in the synthetic mask pool using a multi-condition control generation method. The primary segmentation of the RGB images aligns with the corresponding synthetic masks. Inspired by ControlNet , we introduce a multi-condition control generation method to achieve precise RGB image generation. This method simultaneously injects the segmentation condition \(c_{i}^{s}\) and the canny condition \(c_{i}^{y}\) to steer the denoising process of the random Gaussian noise, ensuring the generated RGB images \(\{g_{i}^{r}\}_{i=0}^{M}\) correspond accurately to the synthetic masks \(\{g_{i}^{m}\}_{i=0}^{M}\).

Since the synthetic mask itself serves as the segmentation condition \(c_{i}^{s}\), when a synthetic mask \(g_{i}^{m}\) is provided, we only need to extract the canny condition \(c_{i}^{y}\) using the canny operator:

\[c_{i}^{s}=g_{i}^{m}, c_{i}^{y}=Canny(g_{i}^{m})\] (7)After obtaining the canny and segmentation conditions, we input them into block \(B_{}\), which consists of a set of neural layers. Finally, these conditions are injected into the pre-trained diffusion model \(M_{}\), controlling the noise \(z\) denoising process to generate the corresponding RGB image. We refer readers to  for more details.

\[g_{i}^{r}=M_{}(z,B_{}(c_{i}^{s}),B_{}(c_{i}^{y}))\] (8)

## 4 Experiment and Results

### Dataset & Metrics

We conduct our experiments on the DIS5K dataset, which comprises 5,479 high-resolution images featuring camouflaged, salient, or meticulous objects in various backgrounds. The DIS5K dataset is divided into three subsets: DIS-TR (3,000 images) for training, DIS-VD (470 images) for validation, and DIS-TE (2,000 images) for testing. For data augmentation, we utilize the mask portion of the training subset (DIS-TR).

To evaluate our models, we employ a diverse set of metrics to ensure comprehensive performance assessment. These metrics include max F\({}_{1}\), which balances precision and recall, providing a harmonic mean that is indicative of overall accuracy; F\({}_{}^{}\), a weighted F-measure that compensates for class imbalances, with values ranging from 0 to 1, where higher values denote superior performance; \(M\) (Mean Absolute Error), which calculates the average absolute difference between the predicted and ground truth masks, with lower values signifying better accuracy; S\({}_{}\), a structural similarity measure that evaluates the preservation of significant structures within the image, with values closer to 1 indicating better performance; and E\({}_{M}^{}\), an enhanced measure that considers both pixel-level and image-level information for a more holistic evaluation, where higher values represent better performance. Collectively, these metrics provide a robust framework for assessing the effectiveness and reliability of our segmentation models.

Figure 3: compared with baseline methods

[MISSING_PAGE_FAIL:7]

First, we identify the best-performing model on the DIS-VD validation set and then evaluate its performance on the other sub-datasets. The models are trained on the DIS-TR dataset, which is augmented with generated datasets of varying sizes: 2500, 5000, 7500, and 10000 images.

The experimental results are presented in Table 1. Our proposed method, MaskFactory, consistently outperforms the baselines across all sub-datasets and evaluation metrics. As the number of generated images increases, the performance of MaskFactory improves, achieving the best results with 10000 generated images. Notably, MaskFactory attains the highest \(maxF_{1}\) scores across all sub-datasets, with improvements ranging from 0.044 to 0.052 compared to the IS-Net baseline.

In contrast, while DatasetDM and Dataset Diffusion also show some performance gains, they encounter the issue of collapse when the generated data exceeds 5000 images. In these cases, the segmentation network's performance stagnates or even degrades. For the DIS segmentation task, the use of pseudo-labels could introduce additional errors, leading to performance declines.

Furthermore, MaskFactory demonstrates superior performance in terms of the weighted F-measure (\(F_{}^{}\)), S-measure (\(S_{}\)), and Enhanced-alignment Measure (\(E_{M}^{}\)). The Mean Absolute Error (\(M\)) is also consistently lower for MaskFactory compared to the baselines, indicating more accurate segmentation results.

Results by Segmentation Network.After achieving stable improvements in IS-Net's performance, we examined the generalizability of our approach by applying the same configuration to several state-of-the-art segmentation networks on the DIS5K dataset. Each network was trained using the DIS-TR dataset augmented with 10,000 generated image pairs. The networks considered in this study include FP-DIS , UDUN , BiRefNet , and SAMHQ , all implemented with their default parameters.

The experimental results, shown in Table 2, demonstrate that our proposed method consistently enhances the performance of all evaluated networks across the five sub-datasets of DIS5K. Notably, significant improvements were observed in the \(maxF_{1}\) and \(E_{M}^{}\) metrics. For instance, on the DIS-TE1 sub-dataset, our method increased the \(maxF_{1}\) score of IS-Net from 0.740 to 0.784, FP-DIS from 0.784 to 0.805, UDUN from 0.784 to 0.799, BiRefNet from 0.866 to 0.882, and SAMHQ from 0.897 to 0.905.

    &  & **IS-Net ** & **FP-DIS ** & **UDUN ** & **BiRefNet ** & **SAM-HQ ** \\  & & w/o Ours & w Ours & w/o Ours & w Ours & w/o Ours & w Ours & w Ours & w Ours & w Ours & w Ours \\   & \(maxF_{1}\) & 0.740 & 0.784 & 0.784 & 0.805 & 0.784 & 0.799 & 0.866 & 0.882 & 0.897 & 0.905 \\  & & \(M\) & 0.074 & 0.073 & 0.060 & 0.063 & 0.059 & 0.057 & 0.036 & 0.033 & 0.019 & 0.018 \\  & & \(S_{}\) & 0.787 & 0.829 & 0.821 & 0.859 & 0.817 & 0.830 & 0.889 & 0.900 & 0.907 & 0.911 \\  & \(E_{M}^{}\) & 0.820 & 0.875 & 0.855 & 0.885 & 0.846 & 0.849 & 0.915 & 0.916 & 0.943 & 0.949 \\   & \(maxF_{1}\) & 0.799 & 0.822 & 0.827 & 0.849 & 0.829 & 0.849 & 0.906 & 0.910 & 0.889 & 0.894 \\  & & \(M\) & 0.070 & 0.067 & 0.059 & 0.061 & 0.058 & 0.055 & 0.031 & 0.029 & 0.029 & 0.030 \\  & & \(S_{}\) & 0.823 & 0.865 & 0.845 & 0.862 & 0.843 & 0.866 & 0.913 & 0.921 & 0.883 & 0.889 \\  & & \(E_{M}^{}\) & 0.858 & 0.903 & 0.889 & 0.893 & 0.886 & 0.894 & 0.947 & 0.957 & 0.928 & 0.937 \\   & \(maxF_{1}\) & 0.830 & 0.870 & 0.868 & 0.911 & 0.865 & 0.888 & 0.920 & 0.937 & 0.851 & 0.853 \\  & & \(M\) & 0.064 & 0.063 & 0.049 & 0.051 & 0.050 & 0.047 & 0.029 & 0.027 & 0.045 & 0.043 \\  & & \(S_{}\) & 0.836 & 0.878 & 0.871 & 0.892 & 0.865 & 0.885 & 0.918 & 0.918 & 0.

Additionally, the Mean Absolute Error (\(M\)) decreased for all networks when applying our method, indicating more accurate segmentation results. The S-measure (\(S_{}\)) also consistently improved across all sub-datasets and networks, highlighting the effectiveness of our approach in capturing structural similarity.

On the DIS-VD sub-dataset, used for validation, our method boosted the \(maxF_{1}\) score of IS-Net from 0.791 to 0.835, FP-DIS from 0.823 to 0.851, UDUN from 0.823 to 0.847, BiRefNet from 0.897 to 0.905, and SAMHQ from 0.842 to 0.847. These findings underscore the generalizability of our approach, as it enhances the performance of diverse segmentation networks without requiring any network-specific modifications.

Visual Results.As shown in Figure 5 (Appendix), our approach achieves precise results, comprising both rigid and non-rigid transformations. The non-rigid transformations, illustrated in columns (b) and (c), enable shape editing, such as removing a table corner or merging two backpacks into one. In contrast, the rigid transformations, demonstrated in columns (d), (e), and (f), primarily involve viewpoint changes, showcasing the original mask rotated in 3D space. Notably, our method effectively preserves the topological structure information of the original image, including the holes on the chair back. This allows for the low-cost generation of high-precision, diverse data pairs. To investigate differences between generated and real images, we analyzed image and mask distributions. Specifically, we used the CLIP model to extract features from 300 real images, as well as images and masks generated by DatasetDM, DatasetDiffusion, and MaskFactory. We then applied UMAP for dimensionality reduction on these 300 features. The feature distributions of masks and images are visualized in Figures 4(a) and 4(b), respectively.

In the mask editing domain, MaskFactory demonstrates superior mask fidelity compared to other generation methods, primarily due to the incorporation of topological consistency constraints. This results in a feature distribution that closely aligns with that of real images. Conversely, for RGB images, the prior from diffusion VAE introduces a larger disparity between the generated and real

Figure 4: UMAP Distribution Differences

  
**Generate Type** & **CLIP** & **UMAP** \\  DatasetDM  & 0.6683 & 0.7017 \\ Dataset Diffusion  & 0.6217 & 0.6349 \\ MaskFactory(rigid) & 0.8791 & 0.8961 \\ MaskFactory(non-rigid) & **0.9147** & **0.9346** \\ MaskFactory(All) & 0.8967 & 0.9103 \\   

Table 3: Similarity with original dataset.

image distributions. However, the distribution generated by MaskFactory shows a greater overlap with the real image distribution compared to the other two methods.

Furthermore, we quantified the differences using cosine similarity, as presented in Table 3. The results indicate that our method achieves the closest distribution to real images, further validating the effectiveness of MaskFactory in generating realistic masks and images.

## 5 Discussion

### Ablation Study

Mask Generation Type Ablation.In our study, we implemented mask rigid editing, non-rigid editing, and mixed editing--each leveraging our novel mask control technology tailored for specific application scenarios. Rigid editing is designed for scenarios requiring precise geometric adjustments, primarily focusing on viewpoint and scale transformations. Non-rigid editing caters to applications needing high adaptability, handling topologically consistent deformations and complex, dynamic image edits. Mixed editing combines the advantages of both approaches, offering a comprehensive solution. We further evaluated the performance gains of each editing strategy. Our experimental results, as shown in Table 4.

From the table, we observe that mixed editing achieves the highest performance across most metrics. Specifically, it achieves the highest \(maxF_{1}\) score and \(S_{}\), indicating superior structural fidelity and segmentation quality. The slight improvement in \(M\) and \(E_{M}^{}\) further underscores the versatility and effectiveness of mixed editing in creating diverse and realistic image-mask combinations.

Loss Function Ablation.We introduce content and structure losses into our model. The Discriminative Loss, implemented via a discriminator, evaluates the differences between generated and real images, aiming to enhance the realism and quality of the outputs. The Edge Constraint Loss focuses on maintaining edge coherence during image editing, which is critical for preserving detailed structural information. We conducted ablation experiments to evaluate the impact of each loss function. The experimental results, shown in Table 5. From the table, we observe that the combination of all three loss functions (\(_{}\), \(_{}\), and \(_{}\)) achieves the highest \(maxF_{1}\) score, indicating the best performance in terms of structural fidelity and realism.

### Limitation

Despite the favorable outcomes achieved by our method, it still encounters significant issues. Although we experimented with different conditions, the results are shown in Table 6. ControlNet sometimes produces unnatural images with stark foreground-background distinctions, necessitating additional harmonization. Complex scenarios can yield unrealistic elements, such as improperly positioned objects. Additionally, our method relies on pre-annotated image-mask pairs, limiting its ability to generate data autonomously and requiring high-quality initial annotations.

### Conclusion

This paper introduces MaskFactory, a novel two-stage approach for generating high-quality synthetic datasets for DIS tasks. By combining rigid and non-rigid mask editing techniques and using multi-conditional control for image generation, MaskFactory produces diverse and precise synthetic image-mask pairs, significantly reducing dataset preparation time and costs. Experiments on the DIS5K benchmark demonstrate the superior performance of MaskFactory compared to existing methods in terms of quality and efficiency.

   Mask & Prompt & Canny & \(maxF_{1}\) & \(M\) \\  ✓ & & & 0.778 & 0.075 \\ ✓ & ✓ & & 0.782 & 0.073 \\ ✓ & & ✓ & 0.764 & 0.080 \\ ✓ & ✓ & ✓ & 0.784 & 0.073 \\   

Table 6: Ablations on conditions.

  
**Type** & \(maxF_{1}\) & \(M\) & \(S_{}\) & \(E_{M}^{}\) \\ 
**Rigid** & 0.768 & 0.074 & 0.807 & 0.867 \\
**Non-Rigid** & 0.771 & 0.074 & 0.796 & 0.858 \\
**Mix** & 0.784 & 0.073 & 0.829 & 0.875 \\   

Table 4: Ablations on generation types.

    & \)} & \(M\) \\ \(_{}\) & \(_{}\) & \(_{}\) & \\  ✓ & & & 0.778 & 0.073 \\  & ✓ & & 0.745 & 0.075 \\  & ✓ & & 0.751 & 0.074 \\ ✓ & ✓ & ✓ & 0.784 & 0.073 \\   

Table 5: Ablations on loss functions.