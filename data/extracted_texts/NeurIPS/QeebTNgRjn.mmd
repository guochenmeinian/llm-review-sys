# Conditional Lagrangian Wasserstein Flow for Time Series Imputation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Time series imputation is important for numerous real-world applications. To overcome the limitations of diffusion model-based imputation methods, e.g., slow convergence in inference, we propose a novel method for time series imputation in this work, called Conditional Lagrangian Wasserstein Flow. The proposed method leverages the (conditional) optimal transport theory to learn the probability flow in a simulation-free manner, in which the initial noise, missing data, and observations are treated as the source distribution, target distribution, and conditional information, respectively. According to the principle of least action in Lagrangian mechanics, we learn the velocity by minimizing the corresponding kinetic energy. Moreover, to incorporate more prior information into the model, we parameterize the derivative of a task-specific potential function via a variational autoencoder, and combine it with the base estimator to formulate a Rao-Blackwellized sampler. The propose model allows us to take less intermediate steps to produce high-quality samples for inference compared to existing diffusion methods. Finally, the experimental results on the real-word datasets show that the proposed method achieves competitive performance on time series imputation compared to the state-of-the-art methods.

## 1 Introduction

Time series imputation is essential for various practical scenarios in many fields, such as transportation, environment, and medical care, etc. Deep learning-based approaches, such as RNNs, VAEs, and GANs, have been proved to be advantageous compared to traditional machine learning methods on various complex real-words multivariate time series analysis tasks . More recently, diffusion models, such as denoising diffusion probabilistic models (DDPMs)  and score-based generative models (SBGMs) , have gained more and more attention in the field of time series analysis due to their powerful modelling capability [26; 32].

Although many diffusion model-based time series imputation approaches have been proposed and show their advantages compared to conventional deep learning models [44; 11; 12], they are limited to slow convergence or large computational costs. Such limitations may prevent them being applied to real-world applications. To address the aforementioned issues, in this work, we leverage the optimal transport theory  and Lagrangian mechanics  to propose a novel method, called Conditional Lagrangian Wasserstein Flow (CLWF), for fast and accurate time series imputation.

In our method, we treat the multivariate time series imputation task as a conditional optimal transport problem, whereby the random noise is the source distribution, the missing data is the target distribution, and the observed data is the conditional information. To generate new data samples efficiently and accurately, we need to find the shortest path in the probability space according to the optimal transport theory. To this end, we first project the original source and target distributions into the Wassersteinspace via sampling mini-batch OT maps. Afterwards, we construct the time-dependent intermediate samples through interpolating the source distribution and target distribution. Then according to the principle of least action in Lagrangian mechanics , the optimal velocity function moving the source distribution to the target distribution is learned in a self-supervised manner by minimizing the corresponding kinetic energy. Moreover, to further improve the model's performance, we learn the task-specific potential function by training a Variational Autoencoder (VAE) model  on the observed time series data to build a Rao-Blackwellized trajectory sampler.

Finally, CLWF is assessed on two real-word multivariate time series datasets. The obtained results show that the proposed method achieves competitive performance and admits fast convergence compared with other state-of-the-art time series imputation methods.

The contributions of the paper are summarized as follows:

* We present Conditional Lagrangian Wasserstein Flow, a novel conditional generative framework based on the optimal transport theory and Lagrangian mechanics;
* We propose a Rao-Blackwellized trajectory sampler to enhance the data generation performance by incorporating the prior information;
* We develop the practical algorithms to solve the time series imputation problem via a conditional generative approach;
* We demonstrate that the proposed method has competitive performance on time series imputation tasks compared other state-of-the-art methods.

## 2 Preliminaries

In this section, we concisely introduce the fundamentals of stochastic differential equations, optimal transport, Shrodinger Bridge, and Lagrangian mechanics.

### Stochastic Differential Equations

We treat the data generation task as an initial value problem (IVP), in which \(X_{0}^{d}\) is the initial data (e.g., some random noise) at the initial time \(t=0\), and \(X_{T}^{d}\) is target data at the terminal time \(t=T\). To solve the IVP, we consider a stochastic differential equation (SDE) defined by a Borel measurable time-dependent drift function \(_{t}:[0,T]^{d}^{d}\), and a positive Borel measurable time-dependent diffusion function \(_{t}:[0,T]^{d}_{>0}\). Accordingly, the Ito form of the SDE can be described as follows :

\[X_{t}=_{t}(X_{t},t)t+_{t}W_{t},\] (1)

where \(W_{t}\) is a Brownian motion/Wiener process. When the diffusion term is not considered, the SDE degenerates to an ordinary differential equation (ODE). However, we will use the SDE for the theoretical analysis as it is more general.

The Fokker-Planck equation (FPE)  describing the evolution of the marginal density \(p_{t}(X_{t})\) reads:

\[p_{t}(X_{t})=-(p_{t}_{t})+ {_{t}^{2}}{2} p_{t},\] (2)

where \( p_{t}=( p_{t})\) is the Laplacian. In fact, both Eq. eq:sde and Eq. eq:fpe reveal the dynamics of the system and serve as the boundary conditions for the optimization problems we will introduce in later sections with different focuses. The differences are when the constraint is Eq. (1), the formalism is Lagrangian, which depicts the movement of each individual particle; while when the constraint is Eq.(2), the formalism is Eulerian, which depicts the evolution of population.

### Optimal Transport

The optimal transport (OT) problem aims to seek the optimal transport plans/ maps that moves the source distribution to the target distribution [47; 41; 38]. In the Kantorovich's formulation of the OT problem, the transport costs are minimized with respect to some probabilistic couplings/joint distributions [47; 41; 38]. Let \(p_{0}\) and \(p_{T}\) be two Borel probability measures with finite second moments on the space \(^{d}\). \((p_{0},p_{T})\) denotes a set of transport plans between these two marginals. Then, the Kantorovich's OT problem is defined as follows:

\[_{(p_{0},p_{T})}_{}\|x-y \|^{2}(x,y)xy,\] (3)

where \((p_{0},p_{T})=\{():(^ {x})_{\#}=p_{0},(^{y})_{\#}=p_{T}\}\), with \(^{x}\) and \(^{x}\) being two projections of \(\) on \(\). The minimizer of Eq.(3), \(*\), always exist and referred to as the optimal transport plan.

Note that the R.H.S of Eq. (3) can also include an entropy regularization term \(D_{}(\|p_{0} p_{T})\), then the original OT problem transforms into the entropy-regularized optimal transport (EROT) problem with Eq. (2) as the constraint, which frames the transport problem better in terms of convexity and stability  In particular, from a data generation perspective, \(p_{0}\) is some random initial noise and \(p_{T}\) is the target data distribution, and we can sample the optimal transport maps in a mini-batch manner [46; 45; 39].

### Shrodinger Bridge

The transport problem in Sec. 2.2 can be further viewed from a distribution evolution perspective, which is particularly suitable for developing the flow models that model the data generation process. For this reason, the Shrodinger Bridge (SB) problem is introduced . Assume that \( C^{1}([0,T],^{d})\), \(()\) is a probability path measure on the path space \(\), then the goal of the SB problem aims to find the following optimal path measure:

\[*=*{arg\,min}_{()}D_{ }(\|)_{0}=q_{0}_{T}=q_{T},\] (4)

where the Kullback-Leibler (KL) divergence \(D_{}(\|)= }{},& ,\\ +,&,\) and \(\) is a reference path measure, e.g., Brownian motion or Ornstein-Uhlenbeck process. Moreover, the distribution matching problem in Eq. (3) can be cast as a dynamical SB problem as well [19; 24; 28]:

\[_{}_{p(X_{t})} _{t}^{}(X_{t},t)^{2},\] (5) subject to Eq. (1) or Eq. (2),

where \(\) is the parameters of the variational drift function \(_{t}\).

### Lagrangian Mechanics

In this section, we formulate the data generation problem under the framework of Lagrangian mechanics . Let \(p_{t}\) and \(}=p_{t}}{t}\) be the density and law of the generalized coordinates \(X_{t}\), respectively. \((p_{t},},t)\) is the kinetic energy, and \((p_{t},t)\) is the potential energy, then the corresponding Lagrangian is

\[(p_{t},_{t},t)=(p_{t},_{t},t)-(p _{t}).\] (6)

We assume that Eq. (6) is lower semi-continuous (lsc) and strictly convex in \(}\) in the Wasserstein space. The kinetic energy \((x_{t},_{t},t)\) and potential energy \((p_{t},t)\) are defined as follows, respectively:

\[(x_{t},_{t},t) =_{p(X_{t})}_{0}^{T}_{_{d}} \|_{t}(x_{t},t)\|^{2}xt,\] (7) \[(p_{t},t) =_{p(X_{t})}_{_{d}}U_{t}(X_{t}) dX_{t},\] (8)

where \(U_{t}(X_{t})\) is the potential function. Then the _action_ in the context of Lagrangian mechanics is defined as follow:

\[[_{t}(x)]=_{0}^{T}_{_{d}}(x_{t}, _{t},t)dx_{t}dt.\] (9)According to _the principle of least action_, the shortest path is the one minimizing the action, which is aligned with Eq. (4) in the SB theory as well. Therefore, we can leverage the Lagrangian dynamics to tackle the OT problem for data generation. To solve Eq. (6), we need to satisfy the stationary condition, i.e., the Euler-Lagrangian equation:

\[_{t}}(x_{t},_{t},t)= }(p_{t},_{t},t),\] (10)

with the boundary condition \(X_{t}}{t}=(X_{t},t),\ q_{0}=p_{0},\ q_{T}=p_{T}\).

## 3 Conditional Lagrangian Wasserstein Flow for Time Series Imputation

In the section, building upon the optimal transport theory, the Shrodinger Bridge problem, and Lagrangian mechanics introduced in Sec. 2, we propose Conditional Lagrangian Wasserstein Flow, which is a novel conditional generative method for time series imputation.

### Time Series Imputation

Our goal is to impute the missing time series data points based on the observations. For training, we adopt adopt a conditionally generative approach for time series imputation in the sample space \(^{K L}\), where \(K\) represents the dimension of the multivariate time series and \(L\) represents sequence length. In our self-supervised learning approach, the total observed data \(x^{}^{K L}\) are partitioned into the imputation target \(x^{}^{K L}\) and the conditional data \(x^{}^{K L}\).

As a result, the missing data points \(x^{}\) can be generated based on the conditions \(x^{}\) joint with some uninformative initial distribution \(x_{0}^{K L}\) (e.g., Gaussian noise) at time \(t=0\), then the imputation task can be described as: \(x^{} p(x^{}|x^{}_{0})\), where the total input of the model is \(x^{}_{0}:=(x^{},x_{0})^{K L 2}\).

### Interpolation in Wasserstein Space

To solve Eq. (7), we need to sample the intermediate variable \(X_{t}\) in the Wasserstein space first. To do so, the interpolation method is adopted to construct the intermediate samples. According to the OT and SB problems introduced in Sec. 2, we define the following time-differentiable interpolant:

\[I_{t}:I_{0}=X_{0}I_{T}=X_{T},\] (11)

where \(^{d}\) is the support of the marginals \(p_{0}(X_{0})\) and \(p_{T}(X_{T})\), as well as the conditional \(p(X_{t}|X_{0},X_{T},t)\).

For implement \(I_{t}\), first, we independently sample some random noise \(X_{0}(0,_{0}^{2})\) at the initial time \(t=0\) and the data samples \(X_{T} p(x^{})\) at the terminal time \(t=T\), respectively. Afterwards, the interpolation method is used to construct the intermediate samples \(X_{t} p(X_{t}|X_{0},X_{T},t)\), where \(t(0,T)\)[30; 2; 45]. More specifically, we design the following sampling approach:

\[X_{t}=(X_{T}+_{t})+(1-)X_{0}+(t)}, t[0,T],\] (12)

where \(_{t}(0,_{}^{2})\) is some random noise with variance \(_{}\) injected to the target data samples for improving the coupling's generalization property, and \((t) 0\) is a time-dependent scalar.

Note that Eq. (12) can only allow us to generate time-dependent intermediate samples in the Euclidean space but not the Wasserstein space, which can lead to slow convergence as the sampling paths are not straightened. Hence, to address this issue, we need to project the interpolations in the Wasserstein space before interpolating to strengthen the probability flow. To this end, we leverage the method adopted in [46; 45; 39] to sample the optimal mini-batch OT maps between \(X_{0}\) and \(X_{T}\) first, and perform the interpolations according to Eq. (12) afterwards. Finally, we have the joint variable \(x^{}_{t}:=(x^{},x_{t})\) as the input for computing the velocity of the Wasserstein flow.

### Flow Matching

To estimate the velocity of the Wasserstein flow \(_{t}(X_{t},t)\) in Eq. (1), the previous methods that require trajectory simulation for training can result in long convergence time and large computational costs[9; 37]. To circumvent the above issues, in this work we adopt a simulation-free training strategy based on the OT theory introduce in Sec. 2.2[30; 46; 2], which turns out to be faster and more scalable to large time series datasets.

Since we can now draw mini-batch interpolated samples of the source distribution and target distribution in the Wasserstein space using Eq. (12), we can model the variational velocity function using a neural network with parameters \(\). Then, according to Eq. (1), the target velocity can be computed by the difference between the source distribution and target distribution. Therefore, the variational velocity function \(_{}(x_{t}^{ input},t)\) can be learned by

\[_{}_{0}^{T}_{^{d}^{d}^{d}}X_{t}}{t}-_{t}^{ }(x_{t}^{ input},t)^{2}x_{0}x^{ tar} x^{ input}t\] (13) \[ _{}_{p(x_{0}),p(x^{ tar}),p(x^{ input }),t}-x_{0}}{T}-_{t}^{}(x_{t}^{ input },t)^{2}.\] (14)

Eq. (14) can be solved by drawing mini-batch samples in the Wasserstein space and performing stochastic gradient descent accordingly. In this fashion, the learning process is simulation-free as the trajectory simulation is not needed.

Moreover, note that that Eq. (13) also obeys the principle of least action introduced in Sec. 2.4 as it minimizes the kinetic energy described in Eq. (7). Therefore, it indicates that the geodesic that drives the particles from the source distribution to the target distribution in the OT problem described in Sec. 2 is found as well, which enables us to generate new samples with less simulation steps compared to standard diffusion models.

### Potential Function

So far, we have demonstrated how to leverage the kinetic energy to estimate the velocity in the Lagrangian described by Eq. 6. Apart from this, we can also incorporate the prior knowledge within the task-specific potential energy into the dynamics, which enables us to further improve the data generation performance. To this end, let \(U(X_{t}):^{d}[0,T]\) be the task-specific potential function depending on the generalized coordinates \(X_{t}\)[48; 37; 34]. Therefore, we can compute the dynamics of the system by

\[X_{t}}{t}=v_{t}(X_{t},t)=-_{x}U_{t}(X_{t}).\] (15)

Since the data generation problem in our case can also be interpreted as a stochastic optimal control (SOC) problem [4; 17; 35; 50; 21; 5], then the existence of such \(U_{t}(X_{t})\) is assured by Pontryagin's Maximum Principle (PMP) .

To estimate \(v_{t}(X_{t},t)\), according to the Lagrangian Eq. (6), we assume that the potential function takes the form \(U_{t}(X_{t})-(X_{t}|_{t},_{p}^{2})\), where \(_{t}\) the learned mean and \(_{p}^{2}\) is the pre-defined variance. As a result, the derivative is \(_{x}U(X_{t})=-_{t}}{_{p}^{2}}\). In terms of practical implementation, we parameterize \(_{x}U(X_{t})\) via a Variational Autoencoder (VAE) . More specifically, we pre-train a VAE on the total observed time series data \(X^{ obs}\). Afterwards, the reconstruction discrepancies of the VAE are used to approximate the task-specific \(v^{}(X_{t},t)\) depending on \(X_{t}\):

\[v_{t}^{}(X_{t},t)=-^{2}}(X_{t}-(X_{t})),\] (16)

where \((X_{t})\) represents the reconstruction output of the pre-trained VAE model with input \(X_{t}\), and \(_{p}^{2}\) is treated as a positive constant for simplicity. In this manner, we can incorporate the prior knowledge learned from the accessible training data into the sampling process formulated by Eq. (14) to enhance the data generation performance.

### Rao-Blackwellized Sampler

To generate the missing time series datapoints, we first formulate an unbiased ODE sampler \(S(X_{t},_{t}^{}(X_{t},t),t)\) for \(X_{t+1}\) with the Euler method and \(_{t}^{}(X_{t},t)\) learned by Eq. (14) (whichmeans the diffusion term in Eq. 1 is omitted). Alternatively, one can also adopt the SDE sampler by using the Euler-Maruyama method. Nevertheless, to ensure achieve the best imputation performance, we choose the ODE sampler for implementation. Note that the ODE sampler alone is good enough to generate high-quality samples for time series imputation.

Now we can construct a Rao-Blackwellized trajectory sampler  for time series data imputation using Eq. 14 and Eq. 16. To this end, we first treat \((X_{t+1}|X_{t},_{t}^{}(X_{t},t),t)\) be the base estimator for \(X_{t+1}\) with \([^{2}]<\) for all \(X_{t+1}\). And we assume \((X_{t},v_{t}^{}(x_{t},t),t)\) is a sufficient statistic for \(X_{t+1}\) based on Eq. 16, even it is not a very accurate estimator for \(X_{t+1}\). As a result, we can formulate a new trajectory sampler \(^{*}=[|]\) to generate the missing time series data. Then according to the Rao-Blackwell theorem , we have

\[[^{*}-X_{t+1}]^{2}[-X_{t+1}]^{2},\] (17)

where the inequality is strict unless \(\) is a function of \(\). Eq. 17 suggests we can construct a more powerful sampler with smaller errors than the base ODE sampler \(\) using Rao-Blackwellization.

### The Algorithms

The overall training process of CLWF is illustrated in Fig. 1, which consists of the following stages. First, the total observed data \(x^{}\) are partitioned into the target data and conditional data for training. Next, the data pairs of \(x^{}\) and \(x_{0}\) are sampled from the target dataset and random Gaussian noise, respectively. Then, the data pairs are projected into the Wasserstein space by sampling the corresponding OT maps. After that, the intermediate variable \(x_{t}\) is sampled through interpolation using Eq. (12). We can approximate the target velocity \(X_{t}}{t}\) by computing \(}-x_{0}}{T}\). Subsequently, we use the joint distribution of the conditional information \(x^{}\) and the intermediate variable \(x_{t}\), \(x^{}\) as the total input to feed the variational flow model \(_{t}^{}\) to compute the velocity. And the flow matching loss defined by Eq. (14) is minimized by stochastic gradient descent.

Furthermore, to incorporate the prior information of into the model, we can choose to train a VAE model on the total observed data \(x^{}\). This is used to estimate the derivative of the task-specific potential function according to Eq. (16), which can be further utilized to construct a more powerful Rao-Blackwellized sampler for inference.

For inference, at time \(t=0\), we sample the initial random noise \(x_{0}\) and conditional information \(x^{}\) to formulate the joint variable \(x^{}\). Note that during the trajectory sampling \(x_{0}\) will evolve over time, while \(x^{}\) remain invariant. We use \(x^{}\) as the input of the flow model \(_{t}^{}\) to compute the velocity. Afterwards, we sample the new \(x_{t}\) using the Euler method. If we perform Rao-Blackwellization, then \(x_{t}\) is fed to the VAE model for computing the derivative of the potential function, and \(x_{t}\) is updated again using the Euler method. The above process will be repeated until reach its convergence. Moreover, we can sample multiple trajectories using different initial random

Figure 1: The overall training process of Conditional Lagrangian Wasserstein Flow.

noise, and the averages as the final imputation results. Finally, the proposed training and sampling procedures are presented in Algorithm 1 and Algorithm 2, respectively.

```
0: Terminal time: \(T\), max epochs, observed data \(X^{}\), parameters: \(\) and \(\). while epoch < max epochs do  sample \(t\), \((x_{0},x_{T})\), and OT maps;  sample \(x_{t}\) according to Eq. (12);  minimize Eq. (14); endwhile if Rao-Blackwellization then  train a VAE model on \(X^{}\). endif ```

**Algorithm 1** Training procedure

## 4 Experiments

### Datasets

We use two public multivariate time series datasets for validation. The first dataset is the PM 2.5 dataset  from the air quality monitoring sites for \(12\) months. The missing rate of the raw data is \(13\%\). The feature number \(K\) is \(36\) and the sequence length \(L\) is \(36\). In our experiments, only the observed datapoints are masked randomly as the imputation targets.

The other dataset we use is the PhysioNet dataset  collected from the intensive care unit for \(48\) hours. The feature number \(K\) is \(35\) and the sequence length \(L\) is \(48\). The missing rate of the raw data is \(80\%\). In our experiments, \(10\%\) and \(50\%\) of the datapoints are masked randomly as the imputation targets, which are denoted as PhysioNet 0.1 and PhysioNet 0.5, respectively.

### Baselines

For comparison, we select the following state-of-the-art timer series imputation methods as the baselines: 1) GP-VAE , which is combines a VAE model and a Gaussian Process prior; 2) CSDI , which is based on the conditional diffusion model; 3) CSBI , which is based on the Schrodinger bridge diffusion model; 4) DSPD-GP , which combines the diffusion model with the Gaussian Process prior.

### Experimental Settings

In terms of the choices of architectures, both the flow model and the VAE model are built upon Transformers . We use the ODE sampler for inference and sample the exact optimal transport maps for interpolations to achieve the optimal performance. The optimizer is Adam and the learning rate: \(0.001\) with linear scheduler. The maximum training epochs is \(200\). The mini batch size for training is \(64\). The total step number of the Euler method used in CLWF is \(15\), while the total step numbers for other diffusion models. i.e., is CSDI, CSBI, and DSPD-GP are \(15\) (as suggested in their papers). The number of the Monte Carlo samples for inference is \(50\). The standard deviation \(_{0}\) for the initial noise \(X_{0}\) is \(0.1\), and the standard deviation \(_{}\) for the injected noise \(_{t}\)\(0.001\). The coefficient \(_{p}^{2}\) in the derivative of the potential function is \(0.01\).

### Experimental Results

#### 4.4.1 Imputation Results

We assess the proposed method on PM 2.5, PhysioNet 0.1 and PhysioNet 0.5, respectively. The root means squared error (RMSE) and mean absolute error (MAE) are used as the evaluation metrics. From the test results shown in Table 1 and Fig. 2, we can see that our method CLWF outperforms the existing deep learning-based method (GP-VAE) and the recent state-of-the-art diffusion methods (CSDI, CSBI, and DSPD-GP). Moreover, CLWF uses only \(15\) sampling steps for inference, while the baseline diffusion method uses only \(50\) sampling steps. This suggests that CLWF is faster and more accurate than the existing methods on time series imputation tasks.

#### 4.4.2 Ablation Study

**Single-sample Imputation Result.** We compare the time series imputation performance of CLWF with CSDI using only one Monte Carlo sample. The test results shown in Table 2 shows that CWFL outperforms CSDI, which suggests that CWFL exhibits lower imputation variances compared to diffusion-based models. This indicates that CWFL is more efficient and computationally economical for inference.

**Effect of Rao-Blackwellization.** We compare the test imputation CLWF wth and without using Rao-Blackwellzation. Note that the PhysioNet dataset does not have enough non-zero data points to train a valid VAE model, therefore we only construct the Rao-Blackwellized sampler for the PM 2.5 dataset. The results showed in Table 3 indicates that the Rao-Blackwellized sampler can further improve the time series imputation performance of the base sampler.

    &  &  &  \\  & RMSE & MAE & RMSE & MAE & RMSE & MAE \\  GP-VAE & \(43.1\) & \(26.4\) & \(0.73\) & \(0.42\) & \(0.76\) & \(0.47\) \\ CSDI & \(19.3\) & \(9.86\) & \(0.57\) & \(0.24\) & \(0.65\) & \(0.32\) \\ CSBI & \(19.0\) & \(9.80\) & \(0.55\) & \(0.23\) & **0.63** & \(0.31\) \\ DSPD-GP & \(18.3\) & **9.70** & \(0.54\) & **0.22** & \(0.68\) & \(0.30\) \\ CLWF & **18.1** & **9.70** & **0.47** & **0.22** & \(0.64\) & **0.29** \\   

Table 1: Test imputation results on PM 2.5, PhysioNet 0.1, and PhysioNet 0.5 (\(5\)-trial averages). The best are in bold and the second best are underlined.

    &  &  &  \\  & RMSE & MAE & RMSE & MAE & RMSE & MAE \\  CSDI & \(22.2\) & \(11.7\) & \(0.74\) & \(0.30\) & \(0.83\) & \(0.40\) \\ CLWF & \(18.4\) & \(10.0\) & \(0.48\) & \(0.22\) & \(0.64\) & \(0.30\) \\   

Table 2: Single-sample test imputation results on PM 2.5, PhysioNet 0.1, and PhysioNet 0.5 (\(5\)-trial averages).

    &  &  &  \\  & RMSE & MAE & RMSE & MAE & RMSE & MAE \\  CSDI & \(22.2\) & \(11.7\) & \(0.74\) & \(0.30\) & \(0.83\) & \(0.40\) \\ CLWF & \(18.4\) & \(10.0\) & \(0.48\) & \(0.22\) & \(0.64\) & \(0.30\) \\   

Table 2: Single-sample test imputation results on PM 2.5, PhysioNet 0.1, and PhysioNet 0.5 (\(5\)-trial averages).

Figure 2: Visualization of the test imputation results on PM 2.5, green dots are the conditions, blue dots are the imputation results, and red dots are the ground truth.

Related Work

### Diffusion Models

Diffusion models, such as DDPMs  and SBGM , are considered as the new contenders to GANs on data generation tasks. But they generally take relatively long time to produce high quality samples. To mitigate this problem, the flowing matching methods have been proposed from an optimal transport. For example, ENOT uses the saddle point reformulation of the OT problem to develop a new diffusion model  The flowing matching methods have also been proposed based on the OT theory [27; 29; 31; 2; 1]. In particular, mini-batch couplings are proposed to straighten the probability flows for fast inference [39; 45; 46].

The Schrodinger Bridge have also been applied to diffusion models for improving the data generation performance of diffusion models. Diffusion Schrodinger Bridge utilizes the Iterative Proportional Fitting (IPF) method to solve the SB problem . SB-FBSDE proposes to use forward-backward (FB) SDE theory to solve the SB problem through likelihood training . GSBM formulates a generalized Schrodinger Bridge matching framework by including the task-specific state costs for various data generation tasks  NLSB chooses to model the potential function rather than the velocity function to solve the Lagrangian SB problem . Action Matching [33; 34] leverages the principle of least action in Lagrangian mechanics to implicitly model the velocity function for trajectory inference. Another classes of diffusion models have also been proposed from an stochastic optimal control perspective by solving the HJB-PDEs [35; 50; 5; 28].

### Time Series Imputation

Many diffusion-based models have been recently proposed for time series imputation [26; 32]. For instance, CSDI  combines a conditional DDPM with a Transformer model to impute time series data. CSBI  adopts the FB-SDE theory to train the conditional Schrodinger bridge model to for probabilistic time series imputation. To model the dynamics of time series from irregular sampled data, DSPD-GP  uses a Gaussian process as the noise generator. TDMiff  utilizes self guidance and learned implicit probability density to improve the time series imputation performance of the diffusion models. However, the time series imputation methods mentioned above exhibit common issues, such as slow convergence, similar to many diffusion models. Therefore, in this work, we proposed CLWF to tackle thess challenges.

## 6 Conclusion, Limitation, and Broader Impact

In this work, we proposed CLWF, a novel time series imputation method based on the optimal transport theory and Lagrangian mechanics. To generate the missing time series data, following the principle of least action, CLWF learns a velocity field by minimizing the kinetic energy to move the initial random noise to the target distribution. Moreover, we can also estimate the derivative of a potential function via a VAE model trained on the observed training data to further improve the performance of the base sampler by Rao-Blackwellization. In contrast with previous diffusion-based models, the proposed requires less simulation steps and Monet Carlo samples to produce high-quality data, which leads to fast inference. For validation, CWLF is assessed on two public datasets and achieves competitive results compared with existing methods.

One limitation of CLWF is that the samples obtained are not diverse enough as we use ODE for inference, which results in slightly higher test (continuous ranked probability score) CRPS compared to previous works, e.g., CSDI. Therefore, for future work, we will seek suitable approaches to accurately model the diffusion term in the SDE. Moreover, we will also try to design better task-specific potential functions for sparse multivariate time series data. We plan to explore the potential of the Lagrangian Wasserstein Flow model for other time series analysis tasks, such as anomaly detection and uncertainty quantification.

In terms of broader impact, our study on time series imputation has the potential to address important real-world challenges and consequently make a positive impact on daily lives.