# Learning to Learn with Contrastive Meta-Objective

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We propose a contrastive meta-objective to enable meta-learners to emulate human-like rapid learning capability through enhanced alignment and discrimination. Our proposed approach, dubbed ConML, exploits task identity as additional supervision signal for meta-training, benefiting meta-learner's fast-adaptation and task-level generalization abilities. This is achieved by contrasting the outputs of meta-learner, i.e, performing contrastive learning in the model space. Specifically, we introduce metrics to minimize the inner-task distance, i.e., the distance among models learned on varying data subsets of the same task, while maximizing the inter-task distance among models derived from distinct tasks. ConML distinguishes itself through versatility and efficiency, seamlessly integrating with episodic meta-training methods and the in-context learning of large language models (LLMs). We apply ConML to representative meta-learning algorithms spanning optimization-, metric-, and amortization-based approaches, and show that ConML can universally and significantly improve conventional meta-learning and in-context learning.

## 1 Introduction

Meta-learning [37; 42], or learning to learn, is a powerful paradigm that aims to enable a learning system to quickly adapt to new tasks. Meta-learning has been widely applied in different fields, like few-shot learning [17; 50], reinforcement learning [56; 26] and neural architecture search [16; 38]. In meta-training, a meta-leaner mimics the learning processes on many relevant tasks to gain experience about how to make adaptation. In meta-testing, the meta-trained adaptation process is performed on unseen tasks. The adaptation process is achieved by generating task-specific model by the meta-learner, which is given a set of training examples and returns a predictive model. People prefer meta-learning to equip models with human's fast learning ability, so that a good model can be achieved with a few examples .

The combination of two cognitive capabilities, namely, **alignment** and **discrimination**, is essential for human's fast learning ability [23; 12; 13]. A good learner possesses the alignment  ability to align different partial views of a certain object, which means they can integrate various aspects or perspectives of information to form a coherent understanding. On the other hand, discrimination  refers to the learner's capacity to distinguish between one stimulus and similar stimuli, responding appropriately only to the correct stimuli. This is a fundamental ability that allows learners to differentiate between what is relevant and what is not, ensuring that their responses are accurate and based on the correct understanding of the stimuli presented. With alignment and discrimination, learners can synthesize fragmented information to construct a complete picture of an object or concept, while also being able to discern subtle differences between distinct but similar objects or ideas. Such learners are not only efficient in processing information but also in applying their knowledge accurately in varied contexts. This dual capability is crucial for effective learning.

We expect meta-learners to emulate the above combination of alignment and discrimination capabilities to approach human's fast learning ability. By equipping a meta-learner with the ability toalign, we enable it to capture the core essence of a task and being invariant to noises. Meanwhile, discrimination ensures that a meta-learner can learn specific models for unique tasks, as it is a natural superposition that different tasks enjoy distinguishable models. This reflects the natural diversity of problems we encounter in the real world and the varied strategies we employ to solve them. Together, alignment and discrimination empower a meta-learner to not only grasp the subtleties of individual tasks but also to generalize its learning across a spectrum of challenges. This dual capability can makes a meta-learner robust, versatile, and more aligned with the nuanced nature of human learning and reasoning. However, existing meta-learning approaches conventionally follows the idea of "train as you test", to minimize the validation loss  of meta-training tasks as meta-objective, where supervision signal are directly produced by sample labels. To provide stronger supervision, there are works assuming that the task-specific target models of meta-training tasks are available, then the meta-training can be supervised by aligning the learned model and the corresponding target model, with model weights [51; 52] or knowledge distillation . However, as the target models are expensive to learn, and even not available in many real world problems, meta-objectives requiring the target models have very restricted applications. Moreover, the importance of discrimination ability of meta-learner has not been noticed in the literature.

To achieve this, we propose contrastive meta-learning (ConML), by directly contrasting the outputs of meta-learner in the model space, shown in Figure 1. Conventional contrastive learning (CL) [44; 14; 48] learns an encoder in unsupervised manner by equipping the model with alignment and discrimination ability by exploiting the distinguishable identity of unlabeled samples. Considering tasks in meta-learning are also unlabeled but have distinguishable identity, we are inspired to adopt similar strategy in meta-learning. ConML exploits tasks as CL exploits unlabeled samples. Positive pairs in ConML are different subsets of the same task, while negative pairs are datasets of different tasks. In the model space output by meta-learner, inner-task distance can be measured between positive pairs and inter-task distance can be measured between negative pairs. The contrastive meta-objective is minimizing inner-task distance while maximizing inter-task distance, corresponding to the expected alignment and discrimination ability respectively. The proposed ConML is universal and cheap, as it can be plugged-in any meta-learning algorithms following the episodic training, and does not require additional data nor model training. In this paper, we widely study ConML on representative meta-learning algorithms from different categories: optimization-based (e.g., MAML ), metric-based (e.g., ProtoNet ), amortization-based (e.g., Simple CAAPS ). We also investigate in-context learning  with reformulating it into the meta-learning paradigm, and show how ConML integrates and helps.

Our contributions are:

* We propose to emulate cognitive alignment and discrimination capabilities in meta-learning, to narrow down the gap of fast learning ability between meta-learners and humans.
* We generalize contrastive learning from representation space of unsupervised learning to model space of meta-learning. The exploiting task identity as additional supervision benefits meta-learner's fast-adaptation and task-level generalize abilities.
* ConML is algorithm-agnostic, that can be incorporated into any meta-learning algorithms with episodic training. We empirically show ConML can bring universal improvement with cheap implementation on a wide range of meta-learning algorithms and in-context learning.

Figure 1: ConML is performing contrastive learning in model space, where alignment and discrimination encourage the meta-learnerâ€™s fast-adaptation and task-level generalize ability respectively.

Related Works

### Learning to Learn

Meta-learning learns to improve the learning algorithm itself , i.e., learns to learn. Popular meta-learning approaches can be roughly divided into three categories : optimization-based, metric-based and amortization-based. Optimization-based approaches [4; 17; 28] focus on learning better optimization strategies for adapting to new tasks. For example MAML  learns initial model parameters, where few steps of gradient descent can quickly make adaptaion for specific tasks. Metric-based approaches [46; 39; 41] leverages learned similarity metrics. For example, Prototypical Networks  and Matching Networks  learn global shared encoders to map training set to embeddings, based on which task-specific model can be built. Amortization-based approaches [19; 33; 6] seek to learn a shared representation across tasks. They amortize the adaptation process by using neural networks to directly infer task-specific parameters from training set. Examples are CNPs  and CAMPs .

In-context learning (ICL)  is designed for large language models, which integrates examples (input-output pairs) in a task and a query input into the prompt, thus the language model can answer the query. Recently, ICL has been studied as a general approach of learning to learn [2; 18; 47; 1], which reduces meta-learning to conventional supervised learning via training a sequence model. It considers training set as context to be provided along with the input to predict, forming a sequence to feed the model. Training such a model can be viewed as an instance of meta-learning .

### Contrastive Learning

Contrastive learning is a powerful technique in representation learning [29; 10; 48]. Its primary goal is to learn useful representations, which are invariant to unnecessary details, and preserve as much information as possible. This is achieved by maximizing alignment and discrimination (uniformity) in representation space . In conventional contrastive learning, alignment refers to bringing positive pairs (e.g., augmentations of the same sample [54; 22; 5; 21; 10]) closer together in the learned representation space. By maximizing alignment, the representations are encouraged to be invariant to unneeded noise factors. Discrimination refers to separating negative pairs (e.g., different samples) farther. Maximizing discrimination without any other knowledge results in uniformity, i.e., uniform distribution in the representation space. By maximizing discrimination, the representations are encouraged to preserve as much information of the data as possible [43; 5], benefiting the generalization ability.

## 3 Meta-Learning with Contrastive Meta-Objective

Meta-learning is a methodology considered with "learning to learn" machine learning algorithms. Define \((;h)\) as the loss obtained by evaluating model \(h\) on dataset \(\) with function \((y,)\) (e.g., cross entropy or mean squared loss), \(g(;)\) is a meta-learner that maps a dataset \(\) to a model \(h\), i.e, \(h=g(;)\). Given a distribution of tasks \(p()\), where each task \(\) consists of a training set \(^{}_{}=\{(x_{,i},y_{,i})\}_{i=1}^{n}\), and a validation set \(^{}_{}=\{(x_{,i},y_{,i})\}_{i=n+1}^{m}\), the goal of meta-learning is to learn \(g(;)\) to perform well on new task \(^{}\) sampled from \(p(^{})\), evaluated by \((^{}_{^{}};g(^{ }_{^{}};))\).

### A Unified View of Episodic Training

We aim to introduce "learning to align and discriminate" to universally improve the meta-learning process. The most conventional way of meta-training is taking the _validation loss_ as meta-objective to optimize \(\):

\[_{}_{ p()}(^{ }_{};g(^{}_{};)).\] (1)

Different meta-learning algorithms tailor the function inside \(g\), while sharing the same episodic meta-training to achieve (1). Shown as Algorithm 1, in each episode, \(B\) tasks are sampled from \(p()\) to form a batch \(\), and validation loss of each task is aggregated as the supervision signal \(L_{v}=_{}(^{}_{};g(^{}_{};))\) to update \(\). By specifying the function inside \(g\), Algorithm 1 can generalize the meta-training process of different meta-learning algorithms.

Specifications of optimization-based, metric-based and amortization-based algorithms are summarized in Table 1.

We design ConML to be integrated with Algorithm 1 without specifying \(g\), thus to be universally applicable for meta-learning algorithms following the episodic manner. In Section 3.2, we introduce how to measure the objective. Then in Section 3.3, we introduce specifications of ConML on a wide range of meta-learning algorithms.

### Integration with Episodic Meta-Training

To equip meta-learners with the desired alignment and discrimination ability, we design contrastive meta-objective measured in the output space of meta-learner, i.e., the model space of \(h\). Alignment is achieved by minimizing inner-task distance, which is the distance among models generated from different subsets of the same task. Discrimination is achieved by maximize the inter-task distance, which is the distance among models generated from different tasks. Here we introduce how to measure the contrastive objective and perform optimization.

Obtaining Model Representation.To train the meta-learner \(g\), the distances \(D^{},D^{}\) are measured in the output space of \(g\), i.e., the model space \(\). A feasible way is to first represent model \(h=g(;)\) as fixed length vectors \(^{d}\), then measure by explicit distance function \((,)\) (e.g., cosine distance). Note that \(\) is algorithm-specific. Here we only introduce a projection \(:^{d}\) to obtain model representations \(=(h)\). The \(\) and \(\) will be elucidated and specified for different meta-learning algorithms in Section 3.3.

Obtaining Inner-Task Distance.During meta-training, \(^{}_{}^{}_{}\) contains all the available information about task \(\). The meta-learner is expected to learn similar model given any subset \(\) of the task. Meanwhile those models from subsets are expected to be similar to the model learned from the full supervision \(^{}_{}^{}_{}\). We design the following inner-task distance to minimize that encourages \(g\) to learn a generalizable model even from a set containing only few or biased samples. For \(^{}_{}^{}_{}\), we expect \(^{}_{}=^{}_{}\), where \(^{}_{}=(g(;)),^{}_{}=(g( ^{}_{}^{}_{};))\). The inner-task distance \(D^{}_{}\) of task \(\) is defined as:

\[D^{}_{}=_{k=1}^{K}(^{_{k }}_{},^{}_{}),\ s.t.,^{_{k}}_{}_{ }(^{}_{}^{}_{}),\] (2)

where \(\{_{k}\}_{k=1}^{K}\) are \(K\) subsets sampled from \(^{}_{}^{}_{}\) by certain sampling strategy \(_{}\). In each episode given a batch of task \(\) containing \(B\) tasks, inner-task distance is averaged by \(D^{}=_{}D^{}_{}\).

Obtaining Inter-Task Distance.Since the goal of meta-learning is improving the performance on unseen tasks, it is important that the \(g\) is generalizable for diverse tasks. With a natural supposition that different tasks enjoy different task-specific models, it is necessary that \(g\) can learn different models from different tasks, i.e., discrimination. We define the following inter-task distance to maximize to improve the task-level generalizability of \(g\). For two tasks \(^{}\) during meta-training, we expect to maximize the distance between \(^{}_{}\) and \(^{}_{^{}}\). To be practical under the mini-batch episodic training paradigm, we consider to measure inter-task distance among a batch of tasks:

\[D^{}=_{} _{^{}}(^{}_{},^{}_{^{}}).\] (3)

   Category & Examples & \(g(;)\) & \((g(;))\) \\  Optimization & MAML, & Update model weights & \\ -based & Reptile & \(-_{}(;h_{})\) & \(-_{}(;h_{})\) \\  Metric & ProtoNet, & Build classifier with & Concatenate \\ -based & MatchNet & \(\{(\{f_{}(x_{i})\}_{x_{i}_{j}},j)\}_{j=1}^{N}\) & \([_{j}|}_{x_{i}_{j}}f_{}(x_{i})]_{j=1 }^{N}\) \\  Amortization & CNPs, & Map \(\) to model weights & \\ -based & CNAPs & by \(H_{}()\) & \(H_{}()\) \\   

Table 1: Specifications of ConML.

Training Procedure.ConML measures \(D^{}\) by (2) and \(D^{}\) by (3) in each episode, and minimizes a combination of the validation loss \(L_{v}\) and contrastive meta-objective \(D^{}-D^{}\):

\[L=L_{v}+(D^{}-D^{}).\] (4)

The training procedure of ConML is provided in Algorithm 2. Comparing with Algorithm 1, ConML introduces additional computation \((g(;))\) for \(K+1\) times in each episode. Note that we implement \(\) with very cheap function such as obtaining model weights (or a single probing, i.e., feeding-forward, for ICL), and \(g(;)\) already exists in Algorithm 1 while multiple \(g(;)\) can be parallel-computed. ConML could have very comparable time consumption.

### Instantiations of ConML

Here we demonstrate specifications of \(\) and \((g(,))\) to obtain model representation to implement ConML. We show examples on representative meta-learning algorithms from different categories: optimization-based, metric-based and amortization-based. They are explicitly represented by model weights, summarized in Table 1.

**With Optimization-Based Methods.** The representative algorithm of optimization-based meta-learning is MAML. It meta-learns an initialization from where gradient steps are taken to learn task-specific models, i.e., \(g(;)=h_{-_{}(;h_{ })}\). As \(g\) directly generates the model weights, we explicitly take the model weights as model representation. The representation of model learned by \(g\) given a dataset \(\) is \((g(;))=-_{}(;h_{ })\). Note that there are optimization-based meta-learning algorithms which are based on first-order approximation of MAML, thus they do not strictly follows Algorithm 1 to minimize validation loss (e.g., FOMAML  and Reptile ). ConML can also be incorporated as long as it follows the episodic manner.

**With Metric-Based Methods.** Metric-based algorithms are feasible for classification tasks. Given dataset \(\) of a \(N\)-way classification task, metric-based algorithms can be summarized as classifying according to distances with \(\{\{f_{}(x_{i})\}_{x_{i}_{j}}\}_{j=1}^{N}\) and corresponding labels, where \(f_{}\) is a meta-learned encoder and \(_{j}\) is the set of inputs belongs to class \(j\). We design to represent this metric-based classifier with the concatenation of mean embedding of each class in label-aware order. For example, ProtoNet  computes the prototype \(_{j}\), i.e., mean embedding of samples in each class. \(_{j}=_{j}|}_{(x_{i},y_{i})_{j}}f_ {}(x_{i})\). Then classifier \(h_{,}\) is built by giving prediction \(p(y=j x)=(-d(f_{}(x),_{j}))/_{j^{}}(-d(f_{ }(x),_{j^{}}))\). As the outcome model \(h_{,}\) depends on \(\) through \(\{_{j}\}_{j=1}^{N}\) and corresponding labels, the representation is specified as \((g(;))=[_{1}|_{2}||_{N}]\), where \([|]\) means concatenation.

**With Amortization-Based Methods.** Amortization-based approaches meta-learns a hypernetwork \(H_{}\), which aggregates information from \(\) to task-specific parameter \(\) and serves as weights of main-network \(h\), resulting in task-specific model \(h_{}\). For example, Simple CNAPS  adopts the hypernetwork to generate only a small amount of task-specific parameter, which performs feature-wise linear modulation (FiLM) on convolution channels of the main-network. For contrasting we represent \(h_{}\) by \(\), i.e., the output of hypernetwork \(H_{}\): \((g(;))=H_{}()\). The detailed procedures of different meta-learning algorithms with ConML are provided in Appendix A.

## 4 In-Context Learning with Contrastive Meta-Objective

In-context learning (ICL) is first proposed for large language models , where examples in a task are integrated into the prompt (input-output pairs) and given a new query input, the language model can generate the corresponding output. This approach allows pre-trained model to address new tasks without fine-tuning the model. For example, given "_happy->positive; sad->negative; blue->_", the model can output "_negative_", while given "_green->cool; yellow->warm; blue->_" the model can output "_cool_". ICL has the ability to learn from the prompt. Training ICL can be viewed as learning to learn, like meta-learning . More generally, the input and output are not necessarily to be natural language. In ICL, a sequence model \(T_{}\) (typically transformer ) is trained to map sequence \([x_{1},y_{1},x_{2},y_{2},,x_{m-1},y_{m-1},x_{m}]\) (prompt prefix) to prediction \(y_{m}\). Given distribution \(P\) of training prompt \(t\), then training ICL follows an auto-regressive manner:

\[_{}_{t P(t)}_{i=0}^{m-1} (y_{t,i+1},T_{}([x_{t,1},y_{t,1},,x_{t,i+1}])).\] (5)

It has been mentioned that the training of ICL can be viewed as an instance of meta-learning  as \(T_{}\) learns to learn from prompt. In this section we first formally reformulate \(T_{}\) to meta-learner \(g(;)\), then introduce how ConML can be integrated with ICL.

### A Meta-learning Reformulation

Denote a sequentialized \(\) as \(}\) where the sequentializer is default to bridge \(p()\) and \(P(t)\). Then the prompt \([x_{,1},y_{,1},,x_{,m},y_{,m}]\) can be viewed as \(_{}^{lr}\) which is providing task-specific information. Note that ICL does not specify an explicit output model \(h(x)=g(;)(x)\); instead, this procedure exists only implicitly through the feeding-forward of the sequence model, i.e., task-specific prediction is given by \(g([},x];)\). Thus we can reformulate the training of ICL (5) as:

\[_{}_{ p()}_{i=0}^{m-1 }(y_{,i+1},g([}_{,0:i},x_{,i+1}];)).\] (6)

Equation (6) can be regarded as the validation loss (1) in meta-learning, where each task in each episode is sampled multiple times to form \(_{}^{}\) and \(_{}^{}\) in an auto-regressive manner. The training of ICL thus follows the episodic meta-training (Algorithm 1), where the validation loss with determined \(_{}^{}\) and \(_{}^{}\): \((_{}^{};g(_{}^{}; ))\), is replaced by loss validated in the auto-regressive manner: \(_{i=0}^{m-1}(y_{,i+1},g([}_{,0:i},x_{,i+1}];))\).

### Integration with ICL

Since the training of ICL could be reformulated as episodic meta-training, the three steps to measure ConML proposed in Section 3.2 can be also adopted for ICL, but the first step to obtain model representation \((g(,))\) needs modification. Due to the absence of an inner learning procedure for a predictive model for prediction \(h(x)=g(;)(x)\), representation by explicit model weights of \(h\) is not feasible for ICL.

To represent what \(g\) learns from \(\), we design to incorporate \(}\) with a dummy input \(u\), which functions as a probe and its corresponding output can be readout as representation:

\[(g(;))=g([},u];),\] (7)

where \(u\) is constrained to be in the same shape as \(x\), and has consistent value in an episode. The complete algorithm of ConML for ICL is provided in Appendix A. From the perspective of learning to learn, ConML encourages ICL to align and discriminate like it does for conventional meta-learning, while the representations to evaluate inner- and inter- task distance are obtained by probing output rather than explicit model weights. Thus, incorporating ConML into the training process of ICL benefits the fast-adaptation and task-level generalization ability. From the perspective of supervised learning, ConML is performing unsupervised data augmentation that it introduces the dummy input and contrastive objective as additional supervision to train ICL.

## 5 Experiments

In this section, we first empirically investigate the alignment and discrimination empowered by ConML. Then we show the effect of ConML that it significantly improve meta-learning performance on a wide range of meta-learning algorithms on few-shot image classification, and the effect of ConML-ICL with in-context learning general functions. Additionally, by applying ConML we provide a SOTA approach for few-shot molecular property prediction problem, provided in Appendix B. Code is provided in supplementary materials.

### Impact of Alignment and Discrimination

There are two important questions to understand the way ConML works: First, does ConML equip meta-learners with better alignment and discrimination as expected? Second, what is the contribution of inner-task and inter-task distance respectively? We take ConML-MAML as example and investigate above questions with few-shot regression problem following the same settings in , where each task involves regressing from the input to the output of a sine wave. We use this synthetic regression dataset to be able to sample data and vary the distribution as needed for investigation. The implement of ConML-MAML is consistent with Section 5.2. Firstly the meta-testing performance in Table 2 shows that ConML is effective for the regression problem.

Clustering.If ConML enhances the alignment and discrimination abilities, ConML-MAML can generate more similar models from different subsets of the same task, while generating more separable models from different tasks. This can be verified by evaluating the clustering performance for model representations \(\). During meta-testing, we randomly sample 10 different tasks, inside each we sample 10 different subsets, each one contains \(N=10\) samples. Taking these 100 different \(D^{}\) as input, meta-learner generates 100 models. Figure 2(a) and 2(d) show the visualization of model distribution. It can be obviously observed ConML-MAML performs better alignment and discrimination than MAML. To quantity the results, we also evaluate the supervised clustering performance, where task identity is used as label. Table 2 shows the supervised clustering performance of different metrics: Silhouette score , Davies-Bouldin index (DBI)  and Calinski-Harabasz index (CHI) , where ConML-MAML shows much better performance.

Decoupling Inner- and Inter-Task Distance.In conventional unsupervised contrastive learning, where objective only relies on contrasting of positive pairs and negative pairs, positive and negative pairs are both necessary to avoid learning representations without useful information. However, in ConML, there is validation loss \(L_{v}\) plays a necessary and fundamental role in "learning to learn", and the contrastive objective is introduced as additional supervision to enhance alignment and discrimination. Thus, distance of positive pairs (\(D^{}\)) and negative pairs (\(D^{}\)) in ConML could be decoupled and incorporated with \(L_{v}\) respectively. We aim to understand how \(D^{}\) and \(D^{}\) contributes respectively. This gives birth to two variants of ConML: **in-MAML** which optimize \(L_{v}\) and \(D^{}\), **out-MAML** which optimize \(L_{v}\) and \(D^{}\). During meta-testing, we randomly sample 1000 different tasks, inside each we sample 10 different subsets each one contains \(N=10\) samples. We aggregate different subsets from the same task to form a \(N=100\) set to obtaining \(_{}^{*}\) for each task. The distribution of \(D^{}\) and \(D^{}\) are shown in Figure 2(b) and 2(e) respectively, where the dashed lines are mean values. We can find that: the alignment and discrimination ability corresponds to optimizing \(D^{}\) and \(D^{}\) respectively; the alignment and discrimination capabilities are generalizable; ConML shows the couple of both capabilities. Figure 2(c) shows the testing performance given different numbers of examples per task (shot), while the meta-leaner is trained with fixed \(N=10\). We can find that the improvement brought by \(D^{}\) is much more significant than \(D^{}\) under few-shot scenario, which indicates that alignment is closely related to the fast-adaptation ability of the meta-learner.

   Method & MSE (5-shot) & MSE (10-shot) & Silhouette & DBI & CHI \\  MAML & \(.6771.0377\) & \(.0678.0022\) & \(.1068.0596\) & \(.0678.0021\) & \(31.55 2.52\) \\  ConML-MAML & \(.0100\) & \(.0009\) & \(.0621\) & \(.0009\) & \( 2.61\) \\   

Table 2: Meta-testing and clustering performance of few-shot sinusoidal regression.

Figure 2: Investigating the way ConML works.

Figure 2(f) shows the out-of-distribution testing performance. While meta-trained on tasks with amplitudes that uniformly distribute on \([0.1,5]\), meta-testing is performed on tasks with amplitudes that uniformly distribute on \([0.1+,5+]\) (the distribution shift \(\) is indicated as \(x\)-axis). We can find that the improvement brought by \(D^{}\) is notably more significant as the distribution gap grows than \(D^{}\). This indicates that discrimination is closely related to the task-level generalization ability of meta-learner. ConML takes both advantages brought by \(D^{}\) and \(D^{}\).

### Few-Shot Image Classification

To evaluate ConML on conventional meta-learning approaches, we follow existing works [46; 17; 39; 28; 6] to evaluate the meta-learning performance with few-shot image classification problem. We consider representative meta-learning algorithms from different categories, including optimization-based: **MAML**, **FOMAML**, **Reptile**; metric-based: **MatchNet**, **ProtoNet**; and amortization-based: **SCNAPs** (Simple CNAPS) . We evaluate their original meta-learning performance (**w/o ConML**) and performance meta-trained with the proposed ConML (**ConML-**). The implementation of ConML- follows the general Algorithm 2 and the specification for corresponding category in Section 3.3.

**Datasets and Settings.** We consider two few-shot image classification benchmarks: _mini_ImageNet  and _tiered_ImageNet . 5-way 1-shot and 5-way 5-shot tasks are trained and evaluated respectively. Note that we focus on the improvement comparing ConML- and the corresponding algorithm without ConML, rather than performance comparison across different algorithms. So we conduct the experiment on each algorithm following the originally reported settings. All baselines share the same settings of hyperparameters related to the measurement of ConML: task batch size \(B=32\), inner-task sampling \(K=1\) and \(_{}(_{}^{}_{}^{ })=_{}^{}\), \((a,b)=1-}{{\|a\|\|}}\) (cosine distance) and \(=0.1\). For other settings of hyperparameters about model architecture and training procedure, each baseline is consistent with its originally reported. Note that \(K=1\) and \(_{}(_{}^{}_{}^{ })=_{}^{}\) is the most simple and efficient implementation, provided as _Efficient_-ConML in Appendix A. In this case, considering the consumption of feeding-forward neural networks in each task, Algorithm 1 takes \(h=g(_{}^{};)\) and \((_{}^{};h)\), while ConML only introduces an additional \(g(_{}^{}_{}^{};)\), which results in very comparable time consumption.

**Results.** Table 3 and 4 show the results on _mini_ImageNet and _tiered_ImageNet respectively. The relative gain is calculated in terms of the summation of 1-shot and 5-shot accuracy. The relative time is comparing the total time consumption of meta-training. Significant relative gain and very comparable relative time consumption show that ConML brings universal improvement on different meta-learning algorithms with cheap implementation.

### In-Context Learning General Functions

Following , we investigate ConML on ICL by learning to learn synthetic functions including linear regression (LR), sparse linear regression (SLR), decision tree (DT) and 2-layer neural network with ReLU activation (NN). We train the GPT-2 -like transformer for each function with ICL and ConML-ICL respectively and compare the inference (meta-testing) performance. We follow the same model structure, data generation and training settings . We implement ConML-ICL with \(K=1\) and \(_{}([x_{1},y_{1},,x_{n},y_{n}])=[x_{1},y_{1},,x_{ },y_{}]\). To obtain the implicit representation (7), we sample \(u\) from a standard normal distribution (the same with \(x\)'s distribution) independently in

   Category & Algorithm & Setting (5-way) & w/o ConML & ConML- & Relative Gain & Relative Time \\   &  & 1-shot & \(48.75 1.25\) & \( 0.94\) &  &  \\  & & 5-shot & \(64.50 1.02\) & \( 0.97\) & & \\   &  & 1-shot & \(48.12 1.40\) & \( 1.29\) &  &  \\  & & 5-shot & \(63.86 0.95\) & \( 0.78\) & & \\   &  & 1-shot & \(49.21 0.60\) & \( 1.06\) &  &  \\  & & 5-shot & \(64.31 0.97\) & \( 0.81\) & & \\   &  & 1-shot & \(43.92 1.03\) & \( 0.88\) &  &  \\  & & 5-shot & \(56.26 0.90\) & \( 0.89\) & & \\   &  & 1-shot & \(48.90 0.84\) & \( 0.91\) &  &  \\  & & 5-shot & \(65.69 0.96\) & \( 0.72\) & & \\   &  & 1-shot & \(53.14 0.88\) & \( 0.86\) &  &  \\  & & 5-shot & \(70.43 0.76\) & \( 0.71\) & & \\   

Table 3: Meta-testing accuracy on _mini_ImageNet.

each episode. Since the output of (7) is a scalar, i.e., representation \(e\), we adopt distance measure \((a,b)=((a-b)^{2})\), where \(()\) is sigmoid function to bound the squared error. \(=0.02\).

Results.Figure 3 shows that varying the number of in-context examples during inference, ConML-ICL always makes more accurate predictions than ICL. Table 5 collects the two values to show the effect ConML brings to ICL: _Rel. Min. Error_ is ConML-ICL's minimal inference error given different number of examples, divided by ICL's; _Shot Spare_ is when ConML-ICL obtain an error no larger than ICL's minimal error, the difference between the corresponding example numbers. Note that the learning of different functions (different meta-datasets) share the same settings about ConML, which shows ConML can bring ICL universal improvement with cheap implementation. We notice that during training of LR and SLR \(=5\), which happens to equals to the dimension of the regression task. This means sampling by \(_{}\) would results in the minimal sufficient information to learn the task. In this case, minimizing \(D^{}\) is particularly beneficial for the fast-adaptation ability, shown as Figure 3(a) and 3(b). This indicates that introducing prior knowledge to design the hyperparameter settings of ConML could bring more advantage. The effect of ConML for ICL is without loss of generalizability to real-world applications like pretraining large language models.

## 6 Conclusion

In this work, we propose ConML that introduce an additional supervision for episodic meta-training by exploiting task identity. The contrastive meta-objective is designed to emulate the alignment and discrimination embodied in human's fast learning ability, and measured by performing contrastive learning in the model space. Specifically, we design ConML to be integrated with the conventional episodic meta-training, and then give specifications on a wide range of meta-learning algorithms. We also reformulate training ICL into episodic meta-training to design ConML-ICL following the same principle. Empirical results show that ConML can universally and significantly improve meta-learning performance by benefiting the meta-learner's fast-adaptation and task-level generalization ability. This work lays the groundwork for contrastive meta-learning, by identifying the importance of alignment and discrimination ability of meta-learner, and practicing contrastive learning in model space. There also exists certain limitations, such as lack of investigating advanced contrastive strategy, batch- and subset- sampling strategies. We would consider these as future directions.

   Category & Algorithm & Setting (5-way) & w/o ConML & ConML- & Relative Gain & Relative Time \\   &  & 1-shot & \(51.39 1.31\) & \( 1.45\) &  &  \\  & & 5-shot & \(68.25 0.98\) & \( 0.98\) & & \\   &  & 1-shot & \(51.44 1.51\) & \( 1.22\) &  &  \\  & & 5-shot & \(68.32 0.95\) & \( 0.78\) & & \\   &  & 1-shot & \(47.88 1.62\) & \( 1.28\) &  &  \\  & & 5-shot & \(65.10 1.13\) & \( 1.00\) & & \\   &  & 1-shot & \(48.74 1.06\) & \( 1.05\) &  &  \\  & & 5-shot & \(61.30 0.94\) & \( 0.77\) & & \\   &  & 1-shot & \(52.50 0.96\) & \( 0.79\) &  &  \\  & & 5-shot & \(71.03 0.74\) & \( 0.75\) & & \\   &  & 1-shot & \(62.88 1.04\) & \( 0.95\) &  &  \\  & & 5-shot & \(79.82 0.87\) & \( 0.80\) & & \\   

Table 4: Meta-testing accuracy on _tiered_ImageNet.

Figure 3: In-context learning performance.

   Function (max prompt len.) & LR (10 shot) & SLR (10 shot) & DT (20 shot) & NN (40 shot) \\  Rel. Min. Error & \(0.42 0.09\) & \(0.49.06\) & \(0.81 0.12\) & \(0.74 0.19\) \\  Shot Spare & \(-4.68 0.45\) & \(-3.94 0.62\) & \(-4.22 1.29\) & \(-11.25 2.07\) \\   

Table 5: Performance comparison of ConML-ICL and ICL.