# Answer:

From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection

 Xinlei Wang\({}^{1}\), Maike Feng\({}^{2}\), Jing Qiu\({}^{1}\), Jinjin Gu\({}^{1,}\), Junhua Zhao\({}^{2,3}\)

\({}^{1}\)School of Electrical and Computer Engineering, The University of Sydney

\({}^{2}\)School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen

\({}^{3}\)Shenzhen Institute of Artificial Intelligence and Robotics for Society

{xinlei.wang, jeremy.qiu, jinjin.gu}@sydney.edu.au, zhaojunhua@cuhk.edu.cn

Junhua Zhao and Jinjin Gu are the corresponding authors.Code and data are available at https://github.com/ameliawong1996/From_News_to_Forecast.

###### Abstract

This paper introduces a novel approach that leverages Large Language Models (LLMs) and Generative Agents to enhance time series forecasting by reasoning across both text and time series data. With language as a medium, our method adaptively integrates social events into forecasting models, aligning news content with time series fluctuations to provide richer insights. Specifically, we utilize LLM-based agents to iteratively filter out irrelevant news and employ human-like reasoning to evaluate predictions. This enables the model to analyze complex events, such as unexpected incidents and shifts in social behavior, and continuously refine the selection logic of news and the robustness of the agent's output. By integrating selected news events with time series data, we fine-tune a pre-trained LLM to predict sequences of digits in time series. The results demonstrate significant improvements in forecasting accuracy, suggesting a potential paradigm shift in time series forecasting through the effective utilization of unstructured news data.

## 1 Introduction

Time series forecasting [18; 21] serves as an essential foundation for decision-making across a wide range of economic, infrastructural, and social domains [2; 13; 14; 56; 16]. The purpose of analyzing time series data is to decode the evolving relationships within complex, dynamic real-world systems. Traditional forecasting methods, while effective at identifying patterns in historical data, perform well when time series distributions remain consistent over time. However, they have limitations in addressing sudden disruptions or anomalies caused by external random events, and do not systematically connect complex social events with fluctuations in time series data. Integrating insights from real-world events and their effects on social and economic behavior is crucial for improving the reliability and accuracy of time series forecasting.

News articles provide crucial insights into unexpected incidents, policy changes, technological developments, and public sentiment shifts--factors that numerical data alone may not capture. Integrating news into forecasting enriches its inputs with context that closely mirrors the complexities of human behavior and societal changes. On the one hand, news offers a real-time snapshot of events, enabling the model to adjust predictions based on updated information. On the other hand, qualitative data from news sources enables the model to account for non-linear and non-numeric influences. By combining both quantitative and qualitative insights, the model can improve forecast accuracy, especially in rapidly changing environments, making it more reflective of real-world dynamics.

In this work, we propose a unified approach that embeds news and supplementary information into time series data using textual prompts. By fine-tuning large language models (LLMs) [54; 55; 66],we transform time series forecasting into the prediction of the next token in text, taking into account relevant contextual information. The inductive reasoning capabilities of pre-trained LLMs, along with their ability to model multi-modal distributions, enable few-shot predictions in time series . The potential of language models in time series forecasting has also been proven [23; 71]. After further training on our dataset, which includes time series, news, and supplementary information, language models can generate forecasts that consider the textual context provided in the input prompts.

Effective news filtering is a key issue for enhancing time series forecasting as input diversity increases. This task requires more than simple keyword extraction; it demands a deep understanding of how news elements interact with forecast variables, extending beyond linear reasoning to incorporate intelligent analytical methods. We employ LLM agents [43; 57] with advanced human-like reasoning capabilities for dynamic and effective news selection. These agents use few-shot learning to adapt their strategies based on logical scenarios that mimic human reasoning about factors affecting time series fluctuations. This enables them to identify relevant news, which is then paired with corresponding time series data to create a context-aware dataset that improves prediction accuracy. Additionally, LLM agents also play a crucial role in model evaluation, continuously refining their selection logic by comparing forecast errors with all relevant events. This iterative self-evaluation helps identify and integrate critical news items previously overlooked. By automating the analysis of unstructured text and applying chain-of-thought prompting, the agents effectively uncover patterns linking news events to forecasting discrepancies, revealing the nuanced effects of external factors on predictions.

Our contribution can be summarized as follows:

* The paper introduces a novel time series forecasting framework that integrates unstructured news data with numerical time series inputs, providing deeper contextual understanding and improving the model's responsiveness to social events and real-world dynamics.
* The research highlights the use of LLM agents for dynamic news selection and analysis. We leverage the reasoning ability of LLM to automate the effective understanding and filtering of

Figure 1: **Integrating textual information in time series forecasting**. (A) We retrieve relevant original news and supplementary information from our comprehensive database based on information such as the geographic location and time frame of the prediction task. (B) LLM-based agents analyze and select relevant news for different forecasting horizons. (C & D) The selected news and contextual information are combined with time series data for fine-tuning the LLM forecasting model. (E) Discrepancies between predictions and ground truth trigger a review of historical news and data to reprocess missed information and refine reasoning logic.

news content. The agents iteratively refine their news selection process based on forecasting results, improving model accuracy and reliability.
* We propose a data construction method that integrates time series data with news information, and build a dataset spanning multiple domains to support our research. The dataset includes task-specific time series data and verified public news reports, which facilitates further exploration in time series research.
* Our experiment results demonstrate the effective integration of news data, achieving superior prediction accuracy across diverse domains such as energy, exchange rate, traffic, and bitcoin domains. Our findings demonstrate that incorporating news is highly adept at navigating complexities, especially in energy demand patterns.

## 2 Related Work

Time series forecasting.The traditional method of time series forecasting relies on analyzing historical data and utilizing statistical models to predict future trends, with an assumption that past patterns will persist in the future [8; 12; 20; 27; 42]. However, these methods were limited to small-scale datasets. The advent of deep learning  has introduced a range of time series forecasting networks [33; 34; 36; 40; 53; 59; 60; 67; 69] that excel in managing larger, more complex datasets by capturing non-linearities and dependencies directly from historical data. Recent advancements include pre-training on diverse, large-scale datasets, allowing models to be fine-tuned on specific tasks with fewer data and resources [6; 24; 58; 63]. While these methods continue to evolve and improve performance benchmarks, they often neglect the influence of external and contextual factors.

Attempts to incorporate textual information (e.g., Twitter feeds, news articles, public reports) into time series forecasting have been made across various domains, such as finance [7; 48; 49], energy [3; 41], entertainment , pandemics , and tourism . Traditional methods often simplify text analysis to counting keyword frequencies  or using dummy variables, which do not capture nuanced meanings. Advanced efforts include extracting richer textual features like word frequencies and sentiments using traditional NLP  and machine learning methods, as demonstrated by Bai _et al._. However, these approaches require labor-intensive feature engineering, struggle with long-text dependencies, and lack a deep contextual understanding. In contrast, LLMs excel in processing complex textual data and understanding contextual relationships, which can improve prediction accuracy and efficiency through automated feature extraction and enhanced scalability across multiple tasks. Despite their potential, no study has yet fully exploited LLM to enhance forecasting with their capabilities in understanding unstructured textual data.

Language models for time series forecasting.LLMs such as the GPT series [1; 4; 44; 45] and LLaMa  have excelled in a variety of natural language processing tasks. With their vast parameter sets, LLMs acquire extensive general knowledge and reasoning capabilities during pre-training, which is crucial for building intelligent systems equipped with common sense. LLMs architectures are increasingly applied to time series processing and forecasting [23; 25; 37; 51; 23]. For instance, TEMPO  adapts GPT architectures for dynamic temporal representation learning, while TIME-LLM  utilizes LLMs for time series forecasting by reprogramming input data and applying Prompt-as-Prefix techniques. Similarly, FPT  demonstrates that even frozen LLMs can perform effectively in time series tasks, leveraging the universality of self-attention mechanisms. Lag-LLaMa  uses a decoder-only transformer for univariate probabilistic forecasting, and Gruver _et al._ show that by framing time series forecasting as next-token prediction, LLMs can surpass traditional models through effective tokenization and adaptation. However, existing studies mainly utilize the mapping capabilities of LLMs for numerical regression, without incorporating external textual inputs or leveraging the reasoning abilities of LLMs in understanding language.

Reasoning with language models.LLMs can automate tasks with human-like reasoning through "Chain of Thought" (CoT) prompting , enhancing reasoning by step-by-step emulation of human thinking [28; 32]. CoT prompting is useful for transforming complex questions into answers by introducing intermediate steps. The "Tree of Thoughts" (ToT) approach  refines this by mimicking trial-and-error methods, enhancing auto-regressive LLMs with a promoter agent, checker module, memory module, and ToT controller for multi-round dialogues. LLM-based agents can solve tasks by reflecting feedback signals in text and retaining them in a memory buffer for better decisions . Cai et al.  proposed the LLMs As Tool Makers (LATM) framework, where LLMs create reusable tools for problem-solving, interleaving reasoning and actions to aid task completion and interaction . These agents can debate their responses and reasoning to arrive at final actions .

## 3 Method

In this work, we aim to integrate news insights into time series forecasting. The development of such a system faces several challenges. Firstly, the forecasting method must handle unstructured, non-numerical news inputs flexibly and adjust predictions based on the context of the news events. Secondly, constructing this model involves filtering news and establishing connections between the news and the time series data. This requires sifting through vast amounts of internet data to find relevant information, demanding deep societal understanding and sophisticated reasoning skills. Therefore, an intelligent agent is designed to manage this complexity. Moreover, potential inaccuracies in news selection or inferential errors may still affect the forecast accuracy, requiring further refinement of news selection and reasoning based on the predictions. Our approach includes three main modules: a language model-based forecasting module (Sec 3.1), a reasoning agent for news filtering and inference, and an evaluation agent to assess and refine the forecasting model (Sec 3.2). The core workflow of our method and the interrelations between these modules are shown in Figure 1. The subsequent sections will discuss these three modules in detail.

### Rethinking Time Series Forecasting Problem and Elements.

**Time series forecasting** can be considered as a conditional generation problem of sequences . This aligns with the general paradigm of natural language processing represented by LLMs. Taking the LLaMa language model as an example, assuming a number series {123,456}, LLaMa's tokenizer will regard this number as a sequence of digit tokens, _i.e._, ("1","2","3",","4","5","6"). Given the input series "123", the probability of predicting "456" can be represented as a probabilistic forecasting process in an autoregressive manner: \(P(``456"|``123")=P(``4"|``123") P(``5"|``4",``123") P(``6"|``45",``12 3")\). Generally, denoting the time series tokens at time \(t\) as \(x_{t}\), the LLM predict the next token in the series \(x_{t+1}\) using the conditional probability distribution \(P(x_{t+1}|x_{0:t})\). During pre-training, LLMs optimize its internal parameters to maximize this conditional probability over a wide range of natural language corpora. Though counterintuitive, Gruver et al.  have shown that pre-trained language models exhibit a significant few-shot capability for time series forecasting. This shows the potential of language models in understanding input digital tokens, and also inspires us to use the language model as a reasonable platform to study how to introduce the information contained in textual prompt into time series prediction.

**News context** offers critical insights into complex social events that traditional numerical data often overlook, and it also reflects sudden shifts in time series due to random events. In fact, the time series we collected can already be seen as being influenced by the aforementioned events. Assume an event \(\) and a time series \(x_{0:t}\), its impact on the future sequence can be expressed also as a conditional probability \(P(x_{t+1}|x_{0:t},)\). However, when information about event \(\) is not provided, we can only predict through historical time series. Although time series data itself can show patterns and trends, it

Figure 2: **Relationship between news and time series**. This figure illustrates the news filtered by the reasoning agents, using the example of Australia’s state-level electricity demand. It features load data in Victoria state and selected news from June 10 to 12, 2019. The black arrow indicates time-specific events, the blue curve shows load fluctuations. The x-axis represents time, and the y-axis displays load values in kilowatts. The blue box displays the short-term impact news and long-term impact news selected by the reasoning agent (e.g., traffic incidents or new construction projects).

lacks the ability to indicate the causality behind events. The event information \(\) offers the context needed to understand why certain spikes or drops occur. We show in Figure 2 some news that are closely related to time series forecasting.

In language models, news events can also be represented as text tokens. Consider a set of news text tokens \(\{e_{0},e_{1},,e_{u}\}\), which represent event \(\). LLMs treat this news information as a condition input and perform conditional probability predictions \(P(x_{t+1}|x_{0:t},e_{0:u})\). Including \(e_{0:u}\) provides crucial contextual information that influences the prediction of future values. This process aligns with the standard approach used by language models to interpret text, allowing for the incorporation of information about multiple events through various news contexts simultaneously to enhance prediction accuracy. In practice, we only need to integrate news text with historical time series data in a prompt engineering manner.

Other supplementary information also provide contextual information for the forecasting model. For example, weather and climatic factors may affect energy demand and industrial output; financial indicators and economic metrics influence consumer behavior and business operations. Including this diverse range of information allows models to adjust for environmental, economic, and seasonal variations, improving prediction accuracy. The supplementary information can also be understood as conditions and integrated into the above conditional probability forecasting framework. Our approach incorporates this information into language for flexible integration with language models. For example, we use the text "_Weather on historical dates: the lowest temperature is 292.01; the highest temperature is 298.07; the humidity is 94."_ to express weather conditions. This enriches the input to cover various factors affecting the time series. Part (c) of Figure 1 illustrates the method of prompt integration for time series forecasting and the corresponding responses.

Fine-tuning LLMs for time series forecasting.Integrating the above information, we can construct inputs for LLMs to perform time series forecasting. Although pre-trained LLMs are already capable of generating time series predictions to some extent, relying on these pre-trained models for few-shot predictions in such a context-rich environment poses several challenges. Firstly, controlling the output of time series is difficult; predicting long sequences of numerical digit tokens is uncommon for LLMs. Secondly, the connections between the news and supplementary information and the time series typically need to be derived from historical data, which goes beyond the usual scope of using LLMs for few-shot time series predictions. To enable language models to more effectively forecast time series while considering the conditions imposed by news and supplementary information, we propose fine-tuning the language models to predict conditional probabilities. We employ a supervised instruction tuning method to train LLMs on historical time series data paired with corresponding news and supplementary information, formatted into text input-output pairs (shown in Appendix A.3). The same loss function used during pre-training is applied here. To fine-tune the LLM, we use the Low-Rank Adaptation (LoRA) method , which updates only a small subset of parameters, reducing computational demands while retaining most of the pre-trained knowledge. This strategy allows the model to efficiently adapt to new forecasting tasks without losing its foundational strengths.

### Analytical Agent for Aggregation and Reasoning of Contextual News Information.

Next, we construct a dataset to train the above model. While obtaining time series data is relatively straightforward, matching it with appropriate news and supplementary information is not trivial. The internet is flooded with news, most of which are irrelevant to the time series we aim to forecast. Introducing irrelevant news can disrupt forecasting. Therefore, it is crucial to analyze the relevance and causality between the time series forecasting task and the select news accordingly. However, gaining such an understanding is complex, requiring knowledge of human societal mechanisms and logical reasoning skills. In our work, we utilize LLMs for filtering and reasoning about news content. We also recognize that even the most advanced language models cannot complete all reasoning and judgment in a single generation process. We explain how to use a combination of multiple LLM generations to create an intelligent agent that fulfills the complex requirements of news filtering.

Time series and news pre-pairing.For the initial stage of data preparation, news is retrieved to align with time series data based on matching time frequencies, horizons, and geographical areas. This synchronization ensures that insights from textual information are timely and regionally relevant. For example, to understand state-level electricity demand in Australia from 2019 to 2021, we gather local news from various Australian states and international news occurring in the same period that might directly or indirectly affect demand. In this way, potentially relevant candidate information can be roughly selected first, and such filtering can be easily completed through crawler means.

Reasoning agent for news selection.We employ an LLM-based reasoning agent capable of sophisticated tasks such as conversation, reasoning, and semi-autonomous action. This agent is programmed using detailed prompts that define roles, instructions, permissions, and context, enabling it to interpret human commands and perform complex tasks. This approach condenses extensive news datasets into a refined selection of pertinent articles. It leverages its reasoning capabilities to effectively screen, categorize, and interpret news texts. We employ few-shot prompting  and the CoT  method to develop an agent that understands the context of news for forecasting. This technique enhances the model's ability to handle multi-step, commonsense reasoning tasks essential for accurate forecasting. The agent uses multi-step prompts to break down complex problems into simpler, manageable parts. Our three-phase prompting method, illustrated in Figure 3 Part I, includes:

1. We develop an understanding of time series influencers, sorting them by impact (positive/negative) and duration (short/long-term), considering economic, policy, seasonal, and technological factors;
2. We direct the agent to filter and categorize news based on either automatically generated logic or a given reasoning logic, focusing on relevance to time series and classifying the impact (e.g., long-term and short-term) along with the rationale;
3. We specify the output format for the agent to organize the selected news into JSON, detailing aspects like summary, affected region, reporting time, and rationale. More details about our prompting method can be found in Appendix A.6.

The LLM agent can automatically develop an understanding of time series influencers, with the option to provide predefined reasoning logic in our models. In the automated process, the agent forms its logic through prompts designed to guide it in determining how different types of news impact a specific domain. For instance, we use open-ended questions to allow the agent to independently summarize and create its own filtering logic. User knowledge can also be incorporated into these prompts as additional reasoning, enabling the agent to generate more comprehensive logic. The agent then filters news based on the generated logic, whether fully automated or with user-provided input.

Evaluation agent for reasoning updating.We also design an evaluation agent to assess and improve the effectiveness of the aforementioned news filtering. Relying solely on the reasoning agent for news selection is not optimal, as the interaction between news and time series is complex. The reasoning agent can only analyze the potential impact of different news from the perspective of news content, without knowing whether the trained time series forecasting model based on them can make accurate forecasts. The evaluation agent is deployed after the time series prediction model has been trained. The evaluation agent extends beyond simple numerical assessments of prediction accuracy by incorporating human-like logical reasoning to refine the news selection logic chain. We focus our evaluation on identifying inaccuracies potentially caused by missing news, such as unusual events or illogical reports. It observes the model's predictive outcomes to determine if any crucial news has been overlooked and adjusts the news filtering strategy for the training data based on these results.

Figure 3: **Example of prompt designs for each iteration during fine-tuning. Step 1 involves the reasoning agent selecting news using default logic. Step 2 evaluates predictions based on validation sets to refine the logic. In step 3, the updated logic directs data pairing for the next iteration. Full prompts are shown in Appendix A.6.**

For the evaluation agent, we structured the prompt design into three phases, as illustrated in Figure 3 Part II. In the first phase, we input the forecasting task type, the time horizon, and background information, which the agent uses to generate the steps for evaluating the prediction outcomes. In the second phase, we provide the ground truth, the discrepancies between predicted and actual series, as well as selected and historical news. The agent analyzes these inputs to identify overlooked news based on the distribution of prediction errors over time. In the third phase, the agent generates updated logic based on its analysis, guiding future news selection. After processing all validation set predictions, the reasoning agent consolidates the updated logic into a cohesive final strategy.

### Overall Pipeline

We integrate the news reasoning and evaluation agents with the fine-tuning of the LLM forecasting model to enhance the quality of training data, as illustrated in Figure 4. In the first iteration, we use the LLM agent to establish news selection logic based on the domain and timing of the time series task. This logic directs the reasoning agent to filter relevant news, align it with time series data, and input it into the model for initial fine-tuning. After validating the model's predictions with a validation set, which is randomly extracted from the training data for each iteration, the evaluation agent checks for any missing news that may have influenced the prediction. This feedback helps the reasoning agent refine the filtering logic in subsequent iterations. The cycle continues until the final iteration, where the reasoning agent consolidates all updates to create the definitive news filter for training the final model. We use the GPT-4 Turbo model as the LLM for the agents described above.

## 4 Experiments

### Data preparation

Time series data.We selected time series data from domains influenced by human activities and social events to test our method's ability to capture complex human-driven dynamics during forecasting. These domains include Traffic  (traffic volume), Exchange  (exchange rate), Bitcoin  (Bitcoin price), and Electricity (Australian electricity demand). To avoid bias from pre-trained language models, we updated the Exchange and Electricity datasets up to 2022. We use half-hourly electricity demand data from the Australian Energy Market Operator (AEMO) (aemo.com.au) and daily exchange rate data from the Exchange Rates API (exchangeratesapi.io). These datasets vary in frequency, including daily, hourly, and half-hourly updates, allowing us to evaluate the algorithms' effectiveness across different temporal resolutions. More details are in Appendix A.1.

News collection.Since there are no public datasets that pair time series data with news events, we have collect news specifically for the above time series to facilitate our research. Some of the news content is collected from the GDELT dataset , a database tracking news from nearly every country in over 100 languages. GDELT provides real-time insights into societal, political, and economic events, enabling detailed analysis of global trends and their effects. We incorporate GDELT's event information into our forecasting models to enhance predictive accuracy. For domains needing the latest information, we collect real-time news from sources like News Corp Australia (news.com.au) and Yahoo Finance (yahoo.com), focusing on region-specific and task-specific activities.

Figure 4: Overall pipeline iteratively combines news reasoning agents, fine-tuning, and evaluation agents.

Supplementary information.We enhance our forecasting models with open-source tools to grasp additional data for better accuracy and context. Weather information from OpenWeatherMap  provides daily temperatures, atmospheric pressure, wind speed, and humidity, crucial for load forecasting. Calendar dates, obtained using the Python packages datetime and holidays, account for seasonal and cyclical effects. Economic indicators are integrated using the pandas_datareader library, accessing data like GDP, inflation rates, and employment statistics from sources such as the Federal Reserve, World Bank, and international financial markets.

### Results

Effectiveness of news intergration.In our approach, we incorporate news and supplementary information into time series forecasting by fine-tuning language models. Firstly, we assess whether this additional information can enhance time series forecasting. We conducted experiments to verify the necessity and effectiveness of integrating news data into our forecasting model. We compared four different scenarios as detailed in Appendix A.2 and Appendix A.3:

1. _Pure numerical tokens:_ Uses numerical tokens, encompassing all variables without news. Except for region names or date information, it excludes other textual tokens as a baseline for comparison.
2. _Textual descriptive sentence tokens:_ Evaluates whether using sentence-form descriptions instead of only raw digital numbers can enhance accuracy, with no news integration included.
3. _Unfiltered news with textual descriptive sentence tokens:_ Assesses how integrating descriptive sentences of time series with unfiltered news data affects the model's performance.
4. _Filtered news with textual descriptive sentence tokens:_ Shows the effects of integrating descriptive sentences with news that has been specifically filtered for relevance by the proposed agents.

The performance of different prompt designs is presented in Table 1. It can be seen that the fine-tuned LLM can be used to forecast time series even when using only digital tokens as prompts. The introduction of proper news and other supplementary information leads to significant performance improvements across all four domains. Nevertheless, such improvements are not easily achieved. If the introduced news information is not carefully selected, it can severely impair the results. There are two main reasons for this: first, the influx of a large number of news items introduces too many tokens, which can decrease the performance of the LLM as the number of tokens increases. Second, irrelevant news can introduce noise and incorrect causal information, leading to misleading predictions.

**Effectiveness of the evaluation agent.** To make our news filtering and reasoning processes more effective and comprehensive, we introduce an evaluation agent to reflect on and improve the effects of news selection based on prediction outcomes. As shown in Table 2, our evaluation agent refines news filtering through an iterative process, which is reflected in the progressively improved time series prediction results. The result corresponding to each case is the prediction outcome of the model after pairing the news selected according to the logic obtained from the corresponding iteration. Our findings suggest that, in most cases, two iterations are sufficient to achieve significant improvements, with multiple iterations consistently yielding better results than a single iteration due to the reflection mechanisms. We also found that this iterative filtering process reveals interesting insights into human society from the language model agent. These examples are presented in the Appendix A.7.

**Compare to other forecasting methods.** We also compare our method with existing time series forecasting techniques, detailed in Appendix A.1. We show a quantitative comparison against these baseline methods in Table 3. While the baseline methods use inverse normalization to revert predictions to their original scale, our model operates without normalization. This approach retains

    &  &  \\   & RMSE & MSE\({}_{ 10^{-3}}\) & MAE & MAPE & RMSE\({}_{ 10^{-3}}\) & MSE\({}_{ 10^{-3}}\) & MAE\({}_{ 10^{-3}}\) & MAPE \\  Only Numeric Prompt & 337.10 & 113.64 & 204.89 & 5.275 & 7.80 & 6.10 & 5.74 & 0.77\% \\ Textual Prompt without News & 336.41 & 113.17 & 206.08 & 5.29\% & 7.41 & 5.49 & 5.44 & 0.73\% \\ Textual Prompt with Non-Filtered News & 407.86 & 166.35 & 250.75 & 6.84\% & 8.28 & 6.86 & 6.37 & 0.85\% \\ Textual Prompt with Filtered News & **280.39** & **78.62** & **180.96** & **5.15\%** & **6.46** & **4.17** & **4.83** & **0.65\%** \\   \\   & RMSE\({}_{ 10^{-6}}\) & MSE\({}_{ 10^{-3}}\) & MAE\({}_{ 10^{-3}}\) & RMSE\({}_{ 10^{-3}}\) & MSE\({}_{ 10^{-3}}\) & MAE \({}_{ 10^{-3}}\) & MAPE \\  Only Numeric Prompt & 4.55 & 2.07 & 1.66 & 4.46 & 19.94 & 3.07 & 5.72\% \\ Textual Prompt without News & 4.44 & 1.97 & 1.54 & 3.87 & 14.97 & 2.76 & 5.08\% \\ Textual Prompt with Non-Filtered News & 4.89 & 2.39 & 1.89 & 4.02 & 16.13 & 2.88 & 5.33\% \\ Textual Prompt with Filtered News & **4.22** & **1.78** & **1.43** & **3.67** & **13.41** & **2.68** & **4.95\%** \\   

Table 1: Performance comparison of different prompt designs. Red font indicates the best.

the physical meaning of data, such as electricity demand or economic indicators, ensuring that our outputs remain interpretable. Normalization could obscure the impact of news events due to the nonlinear and scale-dependent relationship between these events and the original data values.

Our approach significantly outperforms traditional methods that rely solely on historical time series data in domains like electricity demand, exchange rates, and the bitcoin market, where events embodied in the news have substantial impacts. This demonstrates the potential of our method. However, the improvement from integrating news into the traffic sector is notably modest. The performance of our traffic forecasting model, which covers all roads in California, is hampered by the coarse granularity of publicly available news data, lacking the local details necessary for precise predictions. Traffic data primarily reflects specific road traffic flows, whereas our news sources are mostly regional or global, failing to capture localized traffic conditions adequately. This limitation is evident in the model's Mean Squared Error (MSE), which is sensitive to outliers and tends to exaggerate errors from traffic spikes that state-level news often does not report. Our model achieves good results with the Mean Absolute Error (MAE), indicating reliable average accuracy. Incorporating more localized road information could potentially improve these issues.

Figure 5 takes the electricity domain as an example and compares the ground truth with predictions by cases with or without news data, demonstrating the effect of incorporating news into forecasting models. The "With News" predictions are closer to the actual values than the "No News" predictions, particularly at critical timestamps where abrupt events significantly influence electricity demand.

## 5 Conclusion and Discussion

In conclusion, our study demonstrates the benefits of integrating news into time series forecasting using LLM-based forecasting method and LLM-based agents. These agents enhance model intelligence by autonomously identifying and addressing missed news, refining their logic, and assessing the impact of events on predictions. Our findings advocate for incorporating extensive domain knowledge, encouraging a shift towards more nuanced and context-aware forecasting. This approach enriches time series forecasting for adaptive, comprehensive forecasting aligned with real-world dynamics.

Limitations of our approach.While our approach demonstrates that LLMs like LLaMa 2  can enhance time series forecasting by integrating news, there are limitations to its applicability.

    &  &  &  \\   & RMSE & MSE\({}_{ 10^{-3}}\) & MAE & MAPE & RMSE\({}_{ 10^{-3}}\) & MSE\({}_{ 10^{-6}}\) & MAE\({}_{ 10^{-3}}\) & MAPE \\ 
1. Initial selection & 313.89 & 98.53 & 190.79 & 5.366 & 6.61 & 4.37 & 4.83 & 0.65\% \\
2. The second selection & +287.35 & +82.57 & +180.49 & +4.93\% & +6.46 & +4.17 & +4.83 & +0.65\% \\
3. The third selection & +303.03 & +91.83 & +192.30 & +5.38\% & +7.69 & +75.92 & +5.63 & +0.75\% \\
4. The fourth selection & +280.39 & +78.62 & +180.96 & +5.15\% & +6.60 & +4.36 & +4.82 & +0.65\% \\    &  \\   & RMSE\({}_{ 10^{-1}}\) & MSE\({}_{ 10^{-3}}\) & MAE\({}_{ 10^{-2}}\) & RMSE\({}_{ 10^{-3}}\) & MSE\({}_{ 10^{-6}}\) & MAE\({}_{ 10^{-3}}\) & MAPE \\ 
1. Initial selection & 4.36 & 1.90 & 1.45 & 4.12 & 16.98 & 2.97 & 5.55\% \\
2. The second selection & +4.36 & 1.90 & +15.52 & +3.67 & +13.41 & +2.68 & 4.95\% \\
3. The third selection & +4.22 & +4.78 & +1.43 & +3.75 & +14.08 & +2.83 & +5.18\% \\   

Table 2: Comparison of Iterative Analysis. The baseline case is the initial selection. The arrow means the comparison of each case with baseline cases. A red downward arrow indicates an improvement, a blue upward arrow indicates performance degradation.

    &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\   & MAE & **180.96** & 349.43 & 282.56 & 255.7 & 233.58 & 254.05 & 237.49 & **220.32** & 234.46 & 238.77 & 236.91 \\  & MSE\({}_{ 10^{-2}}\) & **76.25** & 215.79 & 1667.19 & 153.27 & 1533.09 & 1344.42 & **97.61** & 1335.33 & 133.96 & 142.60 \\  & MSE\({}_{ 10^{-2}}\) & **78.25** & 510.78 & 407.52 & 401.98 & 367.29 & 392.33 & 366.46 & **312.42** & 365.41 & 386 & 377.62 \\  & MAPE & **51.56** & 1063.83 & 8.949 & 2.29 & 686.86 & 7.368 & 68.18 & 68.57 & 6.569 & 6.759 & **64.61*** \\   & MAE\({}_{ 10^{-3}}\) & **63.92** & 9.27 & 1.75 & 9.62 & **8.21** & 6.44 & 5.24 & 14.6 & 6.73 & 8.98 & 15.05 \\  & MSE\({}_{ 10^{-3}}\) & **8.43** & 3.26 & 4.78 & 9.91 & **8.42** & 4.07 & 5.24 & 14.58 & 6.73 & 3.86 & 15.05 \\  & MSE\({}_{ 10^{-3}}\) & **8.45** & 1.37 & 2.18 & 9.51 & **8.46** & 0.87 & 0.85 & 1.58 & 0.875 & 1.23 & 1.00 \\  & MAPE & **6.65** & 1.229 & 2.32 & 0.92 & **0.68\%** & 0.85\% & 0.76 & 1.94\% & 0.89\% & 1.21\% & 1.39\% \\   & MAE\({}_{ 10^{-3}}\) & **1.43** & 2.49 & 4.44 & 1.70 & 1.56 & 1.65 & 1.61 & **1.51** & 1.84 & 1.74 & 1.64 \\  & MSE\({}_{ 10^{-3}}\) & 1.78 & 2.19 & 5.27 & 1.67 & 1.54 & 1.71 & 1.79 & **0.98** & 1.54 & **1.45** & 1.45 \\   & RMSE\({}_{ 10^{-3}}\) & **4.22** & 4.68 & 7.26 & 4.09 & 3.53 & 4.14 & 3.86 & **3.13** & 3.92 & **3.79** & 3.81 \\   & MAE\({}_{ 10^{-3}}\) & **1.28** & 4.28 & 12.27 & 5.42 & 3.20 & 3.28 & 3.17 & 9.22 & 2.85 & 3.96 & **2.84** \\   & MSE\({}_{ 10^{-3}}\) & **13.41** & 27.64 & 162.47 & 59.90 & 16.21 & 17.65 & 16.38 & 123.71 & 183.24 & 24.68 & 13.66 \\   & MSE\({}_{ 10^The effectiveness of news integration is primarily evident in domains where human and market activities significantly influence trends. Our framework is less suitable for domains requiring precise meteorological modeling or where human activities have minimal impact, such as in meteorological or physical data. Additionally, the model is constrained by the maximum token length of pre-trained LLMs, complicating the simultaneous processing of large amounts of time series or multiple sequences, which can lead to data truncation and affect the accuracy of long-term predictions. Finally, our strategy enhances rather than completely replaces traditional time series tasks like classification or interpolation across all fields. Our aim is to demonstrate that by leveraging language models, it is possible to incorporate useful textual information to enhance time series prediction tasks.

Future work.Future enhancements will focus on extending the current forecasting model's scope in several key areas. Firstly, attribution analyses of news content used in the model will pinpoint which factors most significantly impact forecasting accuracy, facilitating an optimized news integration process. Advanced analytical toolkits can also be provided to reasoning agents, enabling sophisticated data processing and real-time application of complex analytical techniques. These developments will enhance the precision and relevance of the time series prediction model, contributing deeper contextual insights and expanding its applicability in the predictive analytics field.

Broader impact.Ethically, it is crucial that we conduct thorough reviews to ensure that our use of news content does not inadvertently perpetuate biases or negatively influence public opinion. This involves implementing rigorous checks for accuracy and balance to avoid the risks associated with misinformation, ensuring that our data sources are credible and that the content is factually correct. Furthermore, the potential misuse of news, especially the spread of "fake news", highlights the need for our models to incorporate sophisticated mechanisms to verify information reliability before integration. Beyond the discussed sectors, this approach has the capability to extend into forecasting GDP trends, analyzing carbon emissions, or predicting public health outcomes, each carrying significant implications for policymaking and public welfare. Thus, while our research offers substantial benefits in enhancing predictive analytics, it also obligates us to handle these capabilities responsibly, ensuring our contributions positively impact economic planning, environmental strategy, and informed decision-making across various domains.