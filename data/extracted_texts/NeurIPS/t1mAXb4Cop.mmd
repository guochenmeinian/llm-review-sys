# Can LLMs Solve Molecule Puzzles? A Multimodal Benchmark for Molecular Structure Elucidation

Kehan Guo\({}^{1}\), Bozhao Nan\({}^{2}\), Yujun Zhou\({}^{1}\), Taicheng Guo\({}^{1}\), Zhichun Guo\({}^{1}\), Mihir Surve\({}^{2}\), Zhenwen Liang\({}^{1}\), Nitesh V. Chawla\({}^{1}\), Olaf Wiest\({}^{2}\), Xiangliang Zhang\({}^{1}\)\({}^{}\)

\({}^{1}\)Department of Computer Science and Engineering, University of Notre Dame

\({}^{2}\)Department of Chemistry and Biochemistry, University of Notre Dame

{kguo2,bnan,xzhang33}@nd.edu

https://kehanguo2.github.io/Molpuzzle.io/

Both authors contributed equally to this work, supported by the NSF Center for Computer-Assisted Synthesis (C-CAS), https://ccas.nd.eduCorresponding author.

###### Abstract

Large Language Models (LLMs) have shown significant problem-solving capabilities across predictive and generative tasks in chemistry. However, their proficiency in multi-step chemical reasoning remains underexplored. We introduce a new challenge: molecular structure elucidation, which involves deducing a molecule's structure from various types of spectral data. Solving such a molecular puzzle, akin to solving crossword puzzles, poses reasoning challenges that require integrating clues from diverse sources and engaging in iterative hypothesis testing. To address this challenging problem with LLMs, we present **MolPuzzle**, a benchmark comprising 217 instances of structure elucidation, which feature over 23,000 QA samples presented in a sequential puzzle-solving process, involving three interlinked sub-tasks: molecule understanding, spectrum interpretation, and molecule construction. Our evaluation of 12 LLMs reveals that the best-performing LLM, GPT-4o, performs significantly worse than humans, with only a small portion (1.4%) of its answers exactly matching the ground truth. However, it performs nearly perfectly in the first subtask of molecule understanding, achieving accuracy close to 100%. This discrepancy highlights the potential of developing advanced LLMs with improved chemical reasoning capabilities in the other two sub-tasks.

Our MolPuzzle dataset and evaluation code are available at this link.

## 1 Introduction

Artificial intelligence (AI) is revolutionizing the field of chemistry, influencing diverse sectors such as industrial chemical engineering [1; 2], drug discovery , and chemistry education . In particular, recent studies have highlighted the success of large language models (LLMs) in addressing predictive challenges in chemistry, including molecular property prediction , reaction prediction , and experiment automation . These advancements suggest significant potential for AI to enhance efficiency and innovation across these critical areas.

We introduce a new chemical challenge to AI, **molecular structure elucidation**. While this critical task has been explored in other contexts, it remains unexplored for large language models (LLMs),extending beyond familiar predictive and generative domains such as property or reaction prediction, and representing a shift toward complex problem-solving. Analogous to solving a detailed cross-word puzzle, **molecular structure elucidation** can be seen as a **molecular puzzle**. It requires the integration of multifaceted data, iterative hypothesis testing, and a deep understanding of chemical cues, much like piecing together clues across a crossword grid to form a coherent solution. Fig. 1 illustrates the problem of molecular structure elucidation alongside its analogical counterpart, the crossword puzzle, highlighting the parallels in strategy and complexity between these two intellectual challenges.

Just as a crossword puzzle requires figuring out words based on given clues and fitting them together in a grid, molecular structure elucidation involves deducing a molecule's structure from various types of data such as nuclear magnetic resonance (NMR), infrared spectroscopy (IR), mass spectrometry, and others. Each type of data provides clues about different aspects of the molecular structure. In a crossword, we integrate clues from across different directions and hints to form words that fit together correctly. Similarly, in molecular structure elucidation, we need to integrate information from different spectroscopic methods to form a consistent picture of the molecule. For example, IR spectra reveal molecular vibrations and functional groups, NMR provides information about the framework of hydrogen and carbon atoms, while mass spectrometry can offer insights into the molecular weight and possible fragmentations.

Nevertheless, molecular structure elucidation is a challenging and time-consuming task. Training undergraduate students in chemistry to solve these puzzles has been a part of the curriculum because determining the structure of molecules is a fundamental skill in the field. Typically, even a single molecule puzzle question on a final exam can take 10 to 15 minutes to solve , demanding considerable memory and processing skills from the students. In the domain of complex molecule research, the process of molecular deduction can become even more complex and time-consuming. Therefore, fully automating this process is highly beneficial for accelerating the design of new materials and drugs, as well as enhancing the efficiency of chemical research . However, it remains a challenging task due to the complexities involved in interpreting spectral data and solving intricate reasoning problems associated with molecular structures .

In this work, we aim to present molecular structure elucidation in formats that LLMs can effectively process. By adapting this complex task to be compatible with LLMs, we explore their potential as promising tools in chemical research. If successful, LLMs could significantly accelerate scientific discovery in chemistry, transforming how we approach and solve intricate molecular puzzles.

To achieve our objectives, we first introduce a novel dataset named **MolPuzzle**, which includes 234 instances of structure elucidation challenges inspired by common chemistry tasks. Unlike datasets used in predictive or generative tasks, which typically consist of a collection of independent samples and are relatively straightforward to construct, each instance in the MolPuzzle dataset is uniquely complex. It is structured as a sequential process involving three interlinked sub-tasks: **molecule understanding**, **spectrum interpretation**, and **molecule construction**. These instances are accompanied by multimodal data, including images of IR, MASS, H-NMR, and C-NMR spectra, alongside their corresponding molecular formulas. Presenting such a complex, multimodal problem in a format that LLMs can effectively process presents a unique challenge. We, a team of AI researchers

Figure 1: A crossword puzzle (left), and a molecular structure elucidation puzzle (right)

and chemists, are dedicated to formulating the molecule puzzle instances in descriptive languages that are accessible to LLMs. Our focus is on ensuring the utility of these instances, as well as their comprehensive coverage over various scenarios and challenges that mimic real-world conditions. By doing so, **MolPuzzle** opens the door for LLMs to contribute meaningfully to the field of chemistry, potentially accelerating scientific discoveries and innovations.

Second, we present our effort to automate the solving of molecular structure elucidation using LLMs. While certain sub-tasks, such as translating an IR spectrum into a molecular formula, may be solvable by encoder-decoder models , the comprehensive resolution of the entire molecular puzzle likely requires the advanced planning and reasoning capabilities of LLMs. We tested 11 state-of-the-art LLMs including GPT-4o, Gemini-pro, and Claude-3-opus. We also conducted a human baseline to compare the performance of humans and LLMs in solving the same puzzles. The **key findings** are: 1) GPT-4o significantly outperforms other LLMs; 2) The best-performing LLM, GPT-4o, performs significantly worse than humans, with only a small portion (1.4%) of its answers exactly matching the ground truth; and 3) GPT-4o's performance primarily collapses in the Stage-2 of spectrum interpretation and gets worse in the Stage-3 of molecule construction, although it performs nearly perfectly in Stage-1 of molecule understanding (with accuracy close to 100%).

To summarize, our key contributions in this work are the presentation of:

* **A new reasoning problem for AI community**. As the focus of AI development has evolved from solving predictive tasks and generative tasks to engaging in complex reasoning tasks--akin to system 2 level thinking--we introduce a reasoning task centered around molecular structure elucidation. This crucial problem from the field of chemistry sets a high benchmark for AI models to reach. Solving this task requires AI models to possess the ability to interpret spectral images, engage in complex reasoning, and plan effectively across extended workflows. This not only challenges the current capabilities of AI but also pushes the boundaries of what AI can achieve in scientific domains, particularly in understanding and manipulating molecular structures.
* **A new light of AI solutions for chemistry community**. By proposing the **MolPuzzle dataset**, we establish another bridge between the fields of AI and chemistry. This initiative leverages the important capabilities of multimodal LLMs, providing the chemistry community with innovative solutions to accelerate the process of structure elucidation. Our initial exploration serves as a demonstration of the potential for these technologies. It sets the stage for further collaborative efforts, inspiring researchers from both domains to collaboratively explore new frontiers in scientific discovery.

The paper is organized as follows. Section 2 presents the related work. In Section 3, we elaborate on the curation of the MolPuzzle dataset. In Section 4, we report the usage of multimodal LLMs in solving MolPuzzle. In Section 5, we discuss the main findings and directions opened by this work. In section 7, we discuss the broader impact of our work. Last, we summarize the study in Section 8 and offer our conclusions.

## 2 Related Work

Molecular Structure Elucidation.Automated molecular structure determination has been researched for decades [13; 14; 15; 16; 17], initially focusing on rule-based systems [18; 19] that interpret spectral data using predefined chemical rules and expert knowledge. Notable examples include SENECA , employing genetic algorithms on NMR data, and ACD/Structure Elucidator , a commercial software integrating various spectral data. While effective for well-characterized compounds, rule-based methods struggle with complex or novel molecules that deviate from established patterns, and their proprietary nature limits benchmarking accessibility.Machine learning approaches [22; 23; 24; 25; 26; 27; 28; 29] have also been explored. Early studies utilized neural networks to assign infrared spectra to molecular structures , and recent advancements leverage deep learning for complex datasets . For example, Alberts et al.  used a transformer-based model to predict SMILES strings from IR spectra, later extending this to NMR data analysis . However, most existing research focuses on molecule elucidation using single-type spectrum data, sufficient for simple molecules but inadequate for complex ones since each spectrum provides only partial structural information. Our study aims to leverage the reasoning and planning capabilities of multimodal large language models (MLLMs) to integrate diverse spectral data, addressing challenges in complex real-world chemistry tasks. We focus on solving the entire puzzle using multiple clues rather than deciphering one word from a single clue.

Multimodal Benchmarks for LLMs.With the advancements in developing multimodal LLMs , a number of multimodal benchmarks have been curated. These benchmarks are crucial for evaluating and refining the capabilities of MLLMs to process and integrate diverse data types, such as text, images, and audio, for a cohesive understanding. Notably, a benchmark proposed by Yue et al.  assesses the reasoning abilities of MLLMs in various college-level subjects. Similarly, MathVista  explores MLLMs' multimodal reasoning capabilities in mathematics, while Yin et al.  introduced LAMM, a dataset focusing on multimodal instruction tuning and the LabSafetyBench  assessed the reliability and safety awareness of LLMs in laboratory environments. Our research shifts the focus to the chemistry domain . To our knowledge, this study is the first to adopt a realistic chemistry task for MLLM processing and to conduct a thorough evaluation of these models' proficiency in chemistry-related reasoning and image analysis. This specialized focus will enhance our understanding of MLLMs' capabilities within a specific scientific domain.

## 3 The MolPuzzle Dataset

Existing benchmarks of chemical tasks primarily focused on predictive or generative tasks involving collections of independent samples that were relatively straightforward to construct. In contrast, our dataset, MolPuzzle, aims to characterize an intertwined assessment of chemistry reasoning and visual understanding, testing the application of AI-assisted technology towards broader scientific discovery. Our data collection process is rigorously designed and implemented by a team uniquely qualified for this task, consisting of esteemed researchers in chemistry and experienced AI specialists who have previously tackled complex chemistry problems. This collaboration ensures that the MolPuzzle dataset not only accurately reflects real-world chemical phenomena and challenges but is also structured in a way that optimally facilitates access and usability for LLMs.

The basic principles guiding our data curation for the MolPuzzle dataset are: 1) ensuring comprehensive coverage by including a wide range of tasks that synthesize visual context with chemical knowledge, facilitating thorough evaluations; 2) varying levels of difficulty to challenge LLMs and highlight their potential limitations; 3) ensuring robust assessment outcomes, i.e., the results are definitive and reliable; and 4) incorporating human expert analysis to identify strengths and weaknesses in model performance, significantly enhancing our understanding of LLMs capabilities.

In this section, we outlined the construction process for the MolPuzzle dataset. We detailed the creation of puzzle tasks in three stages (3.1), as well as the QA pairs involved in these tasks (3.2). Examples are presented in Fig. 2.

### Task Construction

Just like a word puzzle where each clue progressively reveals the final answer, the solution to a molecule puzzle is a SMILES string that captures the interconnected substructures of a molecule. We design our molecule puzzles so that solving one requires the accurate identification and integration of each substructural clue, gradually unveiling the complete SMILES representation of the molecule. This approach is inspired by the analytical strategies employed by chemists in the real world, who interpret spectral data and chemical properties to deduce the structures of unknown molecules. Our puzzle-building process mirrors this scientific exploration, arranging clues in a sequence from simple to complex, where each clue builds upon the insights gained from the previous one, requiring precision and careful thought at every stage. We next provide more details on our clue design methodology.

**The Initial Stage (Molecule Understanding).** In designing a molecule puzzle, the first stage involves determining how many building blocks, or substructures, are available. This foundational step is crucial as it sets the stage for constructing the molecule's complete structure, akin to identifying the key pieces in a complex jigsaw puzzle. Starting with the initial hint: A _molecular formula_, derived from a mass spectrum, indicates the exact types and numbers of atoms in a molecule (e.g., C\(15\)H\(22\)O\({}_{2}\)representing carbon, hydrogen, and oxygen), chemists can begin to deduce possible structures from the degree of saturation which is calculated based on the number of rings and multiple bonds present in the molecule, the potential for forming aromatic rings, or the presence of functional groups. The initial information provides a preliminary range of building blocks, which can later be selected and assembled to solve the molecular puzzle. To benchmark the capability of LLMs in this stage, we developed 26 unique templates (see Appendix A.2 for details), targeting key analytical tasks such as saturation identification, aromatic ring identification, functional group identification, and saturation degree calculation. This initiative produced 5,859 QA-format pairs, effectively evaluating the models' capacity to understand and process molecular data. Details of these samples are reported in Appendix A.3.

**The Second Stage (Spectrum Interpretation).** With the initial building blocks of the molecule identified from the molecular formula, the next critical step involves refining these components through detailed spectral analysis. Spectrum images such as IR, MASS, \({}^{1}\)H-NMR, and \({}^{13}\)C-NMR serve as new hints, each adding layers of information akin to clues in a complex puzzle. These spectral images are pivotal in confirming or revising the initial hypotheses about the molecule's structure. For example, IR spectroscopy can verify the presence of specific functional groups, MASS spectrometry can provide the molecular MASS, molecule mass, and fragmentation patterns, and NMR techniques detail the arrangement of hydrogen and carbon within the molecule. By integrating these new hints, researchers can construct a more robust and experimentally accurate model of the molecule. This process not only theoretically validates each building block but also ensures they align perfectly with empirical data, leading to a comprehensive understanding of the molecular structure. Given the importance of spectral images in this analysis, we have developed specialized question templates to evaluate the proficiency of LLMs in interpreting these images. For instance, we created 17 templates for IR and 12 for each of H-NMR, and C-NMR. Each template, such as 'Analyze the IR spectrum' includes specific queries designed to extract detailed insights, such as 'What does the absorption in 3200-3600 suggest?' This structure enables us to format the questions for Visual Question Answering (VQA), facilitating a systematic approach to query handling. Our method has successfully generated a significant repository of VQA format examples, comprising 3,689 for IR and 2,604 for each of MASS, H-NMR, and C-NMR. A detailed analysis of these tasks is available in Appendix A.4.

**The Final Stage (Molecule Construction).** After completing the first two stages, we can assert that we have gathered the necessary building blocks to assemble the molecule. The assembly process will be guided by insights derived from NMR data. Specifically, \({}^{1}\)H-NMR provides information about the hydrogen environment in the molecule, such as the number of hydrogen atoms, their types (e.g. aromatic), and their connectivity. Meanwhile, C-NMR provides detailed insights into the carbon framework, indicating whether carbon atoms are part of an aromatic ring or not. The assembly of the final molecular structure is an iterative process, during which functional groups are uncovered based on the specific hydrogen and carbon environments. The approach to assembling the final molecular structure is iterative. Starting with initial building blocks selected from the identified fragment pool, LLMs are prompted to select one structure from the pool step by step, based on the NMR guidance, until the maximum number of iterations is reached or the fragment pool is exhausted. This systematic addition ensures that each step in the assembly process not only fits with the previous structure but also aligns perfectly with the latest spectral data, driving us closer to the accurate molecular configuration. We created 27 task templates for each molecule to assess the capability of LLMs in comprehending NMR spectra. These templates include 5 questions about atom numbers and 22 tasks centered on functional groups, generating a total of 6,318 question-answer pairs. We sample both atom-related questions concerning the number of hydrogens and carbons, as well as those targeting functional groups. To reduce bias and ensure more balanced performance, we balance the distribution of labels in the answers--whether indicating the presence or absence of a functional group or specific environment. This ensures a more unbiased evaluation across the sampled tasks.

### QA Sample Derivation

The QA samples for Stage 1 and Stage 2 are automatically generated using their respective question templates (see Appendix A.2) and RDKit . RDKit is an open-source cheminformatics toolkit widely employed for handling chemical informatics data, including molecular structures and fingerprints. This toolkit plays a role in ensuring that the responses, based on the SMILES strings from each molecule puzzle, are accurate and chemically valid. The distribution of these QA samples across different categories is illustrated in Fig. 4. They form a diverse collection of samples for evaluating LLMs' ability to understand molecular formulas and spectra.

The fragment of each QA pair at Stage 3 is initially generated by LLMs, i.e., responding to the prompt'select one fragment...'. To validate the reliability of these automated generations of QA pairs, experts--two Ph.D. candidates from the chemistry department--manually and independently verified 50 samples, labeling the generated fragments as 'correct' or 'wrong'. Their verification was consistent and demonstrated that 67.4% of examples have correct fragment pools in automated generation. To ensure the quality of derived QA pairs in Stage 3, these chemists manually corrected the fragments pool for each instance in the benchmark.

Fig.3 reports the statistical distribution for the MolPuzzle dataset, which includes 217 puzzle instances (the reasoning of 217 different molecules). Since one puzzle can be solved by different paths, different numbers of QA samples are derived in three stages. We will next evaluate LLMs' performance in solving each puzzle, as well as their capability to solve individual questions.

Figure 4: Inner ring: sample distribution in 3 stages. Outer ring: sample distribution across categories in each stage. SI: saturation identification, SDC: saturation degree calculation, FGI: functional group identification, ARI: aromatic ring identification, SA: spectrum analysis.

Figure 3: Statistic of the MolPuzzle dataset

Figure 2: Examples of QA pairs in the 3 stages of MolPuzzleSolving MolPuzzle by Multimodal Large Language Models

The reasoning capabilities of foundation models in the chemistry domain remain underexplored. Thus, our aim is to perform both qualitative and quantitative evaluations to systematically assess the reasoning and planning abilities of these models in visual chemistry contexts, using the MolPuzzle benchmark. We first conducted evaluation of a variety of LLMs for completing the individual tasks in each stage, including GPT-4o , GPT-3.5-turbo , Claude-3-opus , Gemini-pro , Galactica-30b , LLama-3-8B-Instruct , Vicuna-13B-v1.5 , Mistral-7B-Instruct-v0.3 , and in particular multimodal LLMs such as Gemini-pro-vision , LLava-Llama-3-8B , QwenVL-Chat , and InstructBlip-Vicuna-7B/13B . Due to space limits, we present only selected results in Table 1 and report the complete list of results in Appendix B. We then assess LLMs' capability to solve the entire puzzles, specifically focusing on how effectively these models can derive the final molecular structure from provided hints (the questions in QA samples). The results are reported in Table 2.

All tasks are evaluated in a zero-shot setting to determine the problem-solving capabilities of LLMs without prior fine-tuning on specific task data. The evaluation process consists of three steps: response generation, answer extraction, and score calculation. More details of the experimental settings including prompts and hyperparameters are presented in Appendix B.1.

To gain an in-depth understanding of the performance of LLMs in comparison with human experts, particularly their failed cases, we invited six Ph.D. candidates in chemistry to solve the puzzles in MolPuzzle, and also assess LLMs' results. More comprehensive details of this **human baseline** and evaluation process are presented in Appendix B.2. The reported performance, including human baselines, is presented as an average with standard deviation over all samples.

### LLMs' Performance on Solving Molecule Puzzles

#### 4.1.1 Addressing individual QA tasks in three stages

In Table 1, we report the performance of selected LLMs on conducting individual QA tasks in the three stages, including GPT-4o, GPT-3.5-turbo, Claude-3-opus (three top-performing proprietary models), LLama-3-8B-Instruct (the best performing open-source model), and the reference human baseline performance. In stage 2, the variant of LLama3 for a multimodal setting, LLava-Llama-3-8B, is used for handling spectrum image analysis. Since each task involves performing a question-answering task, we evaluate the performance using F1 and accuracy by comparing the LLMs' answers with the ground truth. F1 scores are reported in Table 1, while the accuracy and performance of more LLMs can be found in Appendix B.

The results of Stage 1 (in Table 1 and Appendix Table 3) show that the GPT-4o model excels in these tasks (achieving near-perfect F1 score in 3 out of 4 tasks). The high scores in SI, AI, and FI suggest that LLMs are able to succeed in relatively straightforward chemistry analysis tasks, performing comparably to human experts. However, open-sourced models like LLama3 have limitations in addressing these tasks, possibly due to their limited reasoning abilities in chemistry text-reasoning tasks. In addition, GPT-4o's comparative performance to humans indicates significant advancements in the use of LLMs for complex scientific tasks, suggesting a promising future for leveraging advanced LLMs to improve the efficiency of scientific analysis and discovery.

For the multimodal tasks of Stage 2, GPT-4o remains the top performer, though it exhibits intermediate performance in spectrum interpretation. The F1 scores for the four types of spectra average around 0.6, indicating a moderate level of accuracy in this complex aspect of the challenge. This performance is notably less competitive compared to human baselines, which succeed in approximately 73-77% of the tasks across the four types of spectrum interpretation. This indicates that spectrum interpretation is inherently challenging. While GPT-4o has made significant strides in automated spectrum analysis, there remains considerable room for improvement to bridge the gap between its capabilities and human expertise. More details are presented in Appendix B.4.

The results for Stage 3 indicate that the most advanced LLM, GPT-4o, significantly underperforms compared to the human baseline, with nearly a 40% difference. This might be caused by the fact that the reasoning ability required for these tasks is complex and multifaceted. When information converges, such as identifying equivalent hydrogen or ring arrangements, a comprehensive understanding of the NMR peaks and their corresponding structures is essential. See more details in Appendix B.5.

#### 4.1.2 Addressing entire molecule puzzles

For solving the entire molecule puzzles, the evaluation is limited to the three most advanced multimodal LMMs: GPT-4o , Claude-3-opus , and Gemini-pro , due to the involvement of spectrum image analysis in Stage-2. The results of these models are reported in Table 2, along with those from the human baseline(see complete evaluation process is reported in Appendix C). To comprehensively evaluate the performance, we employ two different types of metrics. The first type of metric measures the chemical similarity between the ground-truth molecules and the generated molecules, assessed using FTS (Fingerprint Tanimoto Similarity)  in terms of MACCS , RDK , and Morgan . Since the generated molecules are in SMILES string format, we also employ natural language processing metrics including the Accuracy of Exact Match , and Levenshtein distance  (the minimum number of single-character editing required to transform one string into another). Finally, to evaluate whether constructed molecules are valid, we use RDKIT  to check the validity of constructed molecules and report the percentage of molecules that are confirmed as valid.

The results in Table 2 show that the best-performed LLM, GPT-4o, is performing much worse than humans, indicating a huge gap between LLMs and humans in solving the molecule puzzles. It is worth noting that all the constructed molecules are valid, even though only a small portion of them (1.4%) exactly match the ground truth. Considering that the accuracy of the exact match is too strict,

    \\  Method & SI & ARI & FGI & SDC \\  GPT-4o & **1.00\(\)0.000** & 0.943\(\)0.016 & 0.934\(\)0.005 & 0.667\(\)0.003 \\ GPT-3.5-turbo & 0.451\(\)0.025 & 0.816\(\)0.017 & 0.826\(\)0.075 & 0.5\(\)0.099 \\ Claude-3-opus & 0.361\(\)0.009 & **0.988\(\)0.015** & **0.934\(\)0.001** & **0.856\(\)0.016** \\ Galactica-30b & 0.826\(\)0.248 & 0.347\(\)0.000 & 0.467\(\)0.005 & 0.000\(\)0.000 \\ Llama3 & 0.228\(\)0.043 & 0.696\(\)0.051 & 0.521\(\)0.003 & 0.000\(\)0.000 \\ Human & 1.00\(\)0.000 & 1.000\(\)0.000 & 0.890\(\)0.299 & 0.851\(\)0.342 \\   \\  Method & IR Interpretation & MASS Interpretation & H-NMR Interpretation & C-NMR Interpretation \\  GPT-4o & **0.656\(\)0.052** & **0.609\(\)0.042** & **0.618\(\)0.026** & **0.639\(\)0.010** \\ LLawa & 0.256\(\)0.026 & 0.101\(\)0.021 & 0.118\(\)0.008 & 0.254\(\)0.015 \\ Human & 0.753\(\)0.221 & 0.730\(\)0.11 & 0.764\(\)0.169 & 0.769\(\)0.101 \\   \\  Method & H-NMR Elucidation & C-NMR Elucidation \\  GPT-4o & **0.524\(\)0.021** & **0.506\(\)0.037** \\ Llama3 & 0.341\(\)0.015 & 0.352\(\)0.017 \\ Human & 0.867\(\)0.230 & 0.730\(\)0.220 \\   

Table 1: F1 scores (\(\)) of individual QA tasks in three stages. The best LLMs results are in bold font. Tasks in stage 1 are SI-Saturation Identification, ARI-Aromatic Ring Identification, FGI-Functional Group Identification, and SDC-Saturation Degree Calculation.

   Method & Acc. (\(\)) & Levenshtein (\(\)) & Validity (\(\)) & MACCS FTS (\(\)) & RDK FTS (\(\)) & Morgan FTS (\(\)) \\  GPT-4o & **0.014\(\)0.004** & **11.653\(\)0.013** & **1.000\(\)0.000** & **0.431\(\)0.009** & **0.293\(\)0.013** & 0.232\(\)0.007 \\  Claude-3-opus & 0.013\(\)0.008 & 12.680\(\)0.086 & **1.000\(\)0.000** & 0.383\(\)0.050 & 0.264\(\)0.040 & **0.241\(\)0.037** \\  Gemini-pro & 0.000\(\)0.000 & 12.711\(\)0.196 & **1.000\(\)0.000** & 0.340\(\)0.017 & 0.208\(\)0.002 & 0.171\(\)0.007 \\  Human & 0.667\(\)0.447 & 1.332\(\)2.111 & 1.000\(\)0.000 & 0.985\(\)0.022 & 0.795\(\)0.317 & 0.810\(\)0.135 \\   

Table 2: The performance of LLMs and human baseline in solving MolPuzzle. The best LLM results are in bold font. Acc. stands for the Accuracy of Exact Match.

we use FTS to analyze more about the chemical closeness of LLMs' answer to the ground truth. A MACCS FTS of 0.431 suggests that the generated molecules maintain a significant level of structural similarity. This indicates that even if the answers are not perfect replicas of the ground truth, they can still be chemically valid and potentially useful as structured hypotheses that could be relived by human scientists.

### Success and Failure Analysis

The above analysis indicates that the most capable model, GPT-4o, performs **nearly perfectly** in Stage-1 of molecule understanding. However, its performance **drops** in Stage-2 for spectrum interpretation, and **worsens further in Stage-3** for molecule construction. We investigate in-depth how GPT-4o eventually fails on most of the puzzles after progressing through the tasks of these three stages. With the help of human evaluators, we gathered all the intermediate steps involved in solving a molecule puzzle and engaged them to scrutinize these steps. Fig. 5 presents case studies that illustrate the iterative steps involved in Stage-3, showcasing the most common errors made by GPT-4o: **the accumulation of errors in iterative steps, which can lead to catastrophic failures**. Note that this stage focuses on selecting the correct fragments and assembling them step by step to form the final molecular structure. We find that GPT-4o can initially succeed in picking the correct fragment when the structure is comparatively simple. However, as the process progresses, it does no select structures that satisfy all the requirements indicated by the NMR data. This difficulty arises because the reasoning requirements expand dramatically as more information and additional constraints need to be incorporated. More qualitative examples can be found in Appendix C.1.

## 5 Findings and Open Directions

Our evaluation has revealed specific limitations of state-of-the-art LLMs in automating molecular structure elucidation. We urge further collaborative efforts from the AI and chemistry communities to design more effective solutions, especially for the tasks in Stage 2 and Stage 3. Based on our findings, we next present the open directions for future research and development.

Development of Specialized Multimodal LLMs Spectrum Interpretation in Stage 2.As indicated in our results, the performance of LLMs notably declines beginning in Stage 2, where they struggle with the visual interpretation of 1H and 13C NMR spectra. This difficulty arises because NMR spectra feature sharp, unlabeled peaks with multiplicities that exhibit very small chemical shift differences, making them challenging for visual models to interpret. These multiplicities, however, contain crucial information about the chemical connectivity of molecular fragments. Similarly, closely spaced IR absorptions provide key insights for identifying functional groups. This presents a significant opportunity to develop specialized multimodal LLMs that can more effectively interpret these subtle and complex spectral details.

Figure 5: The target molecule contains four distinct non-aromatic hydrogen types, color-coded in the ground truth NMR. However, the model-derived molecule shows hydrogen counts of 3, 3, and 1, differing from the ground truth. The mismatch between the hydrogen types in the green section of the target molecule and the orange region of the predicted molecule results in incorrect fragment selection and assembly.

Development of New Strategies for Leveraging LLMs in Chemical-Related Planning and Reasoning.The failure analysis from Stage 3 has motivated us to explore more effective strategies for leveraging LLMs' capabilities in planning and reasoning for fragment selection and assembly. Our first immediate approach was to employ the chain-of-thought technique , aiming to provide more structured reasoning and instructions for solving the molecular puzzle. However, despite implementing this method, the results were unsatisfactory, even performing worse than the zero-shot setting we initially reported in the paper. We plan to continue exploring this direction with different implementations and adjustments. A second approach involves utilizing LLMs as agents in a more dynamic and interactive manner. This strategy incorporates feedback loops, allowing the models to iteratively refine their responses based on new information or corrections. By doing so, we aim to mitigate the accumulation of errors in iterative steps and reduce the risk of catastrophic failures during the problem-solving process. In addition, we are investigating fine-tuning strategies to enhance the model's ability to handle domain-specific tasks. This involves fine-tuning LLMs on curated chemical datasets that include detailed annotations of spectral data and molecular structures. The goal is to train the model to recognize subtle patterns and dependencies that are often missed in a general-purpose pre-trained model. By tailoring the model's training to this domain, we expect to improve its reasoning and planning capabilities when interpreting complex spectra and assembling molecular fragments.

## 6 Negative Societal Impacts

Automating molecular elucidation using LLMs has significant benefits but also poses serious risks, especially regarding the creation of prohibited drugs. 1.)Facilitation of Illicit Drug Synthesis: LLMs could be used to design new synthetic drugs that evade current regulations, making it easier for illicit manufacturers to produce harmful substances. 2.)Lowering the Barrier to Entry: The technology could enable individuals with minimal expertise to create detailed molecular blueprints for prohibited drugs, increasing the potential for misuse. 3.) Regulatory Challenges: The rapid generation of novel compounds could overwhelm drug regulators, leading to delays in banning new synthetic drugs and complicating the control of harmful substances. 4.) Ethical and Legal Issues: Questions about responsibility and access to such powerful tools arise. Regulating who can use these technologies and for what purposes becomes crucial to prevent misuse.

## 7 Broader Impact

Our work has broad impacts across multiple dimensions. First, it offers valuable insights and recommendations for both AI researchers and chemists in academia and industry. These perspectives enhance the effective utilization of LLMs and guide future advancements in the field. Second, our approach to benchmarking and improving LLMs through real-world tasks like the MolPuzzle can also foster greater collaboration between computational scientists and chemists. By aligning AI technologies with traditional chemical research, these interdisciplinary efforts can accelerate the discovery of new materials, drugs, and chemical processes, potentially leading to significant advancements in healthcare and industry.

## 8 Conclusion

In this paper, we introduced MolPuzzle, a new benchmark challenge to advance our capabilities in molecular structure elucidation. We evaluated state-of-the-art LLMs on this task, revealing their strengths and limitations in handling complex chemical reasoning. Our analysis highlights significant performance gaps, particularly in spectrum interpretation and molecule construction. These findings not only suggest ways to improve LLM performance but also set the stage for transforming approaches to chemical research. MolPuzzle serves as a critical step toward harnessing the potential of LLMs in chemistry, fostering innovation and collaboration within the AI and chemistry communities to enhance scientific inquiry and application.