# Guide Your Agent with Adaptive Multimodal Rewards

Changyeon Kim\({}^{1}\)  Younggyo Seo\({}^{2}\)  Hao Liu\({}^{3}\)  Lisa Lee\({}^{4}\)

Jinwoo Shin\({}^{1}\)  Honglak Lee\({}^{5,6}\)  Kimin Lee\({}^{1}\)

\({}^{1}\)KAIST \({}^{2}\)Dyson Robot Learning Lab \({}^{3}\)UC Berkeley

\({}^{4}\)Google DeepMind \({}^{5}\)University of Michigan \({}^{6}\)LG AI Research

###### Abstract

Developing an agent capable of adapting to unseen environments remains a difficult challenge in imitation learning. This work presents Adaptive Return-conditioned Policy (ARP), an efficient framework designed to enhance the agent's generalization ability using natural language task descriptions and pre-trained multimodal encoders. Our key idea is to calculate a similarity between visual observations and natural language instructions in the pre-trained multimodal embedding space (such as CLIP) and use it as a reward signal. We then train a return-conditioned policy using expert demonstrations labeled with multimodal rewards. Because the multimodal rewards provide adaptive signals at each timestep, our ARP effectively mitigates the goal misgeneralization. This results in superior generalization performances even when faced with unseen text instructions, compared to existing text-conditioned policies. To improve the quality of rewards, we also introduce a fine-tuning method for pre-trained multimodal encoders, further enhancing the performance. Video demonstrations and source code are available on the project website: [https://sites.google.com/view/2023arp](https://sites.google.com/view/2023arp).

## 1 Introduction

Imitation learning (IL) has achieved promising results in learning behaviors directly from expert demonstrations, reducing the necessity for costly and potentially dangerous interactions with environments . These approaches have recently been applied to learn control policies directly from pixel observations . However, IL policies frequently struggle to generalize to new environments, often resulting in a lack of meaningful behavior  due to overfitting to various aspects of training data. Several approaches have been proposed to train IL agents capable of adapting to unseen environments and tasks. These approaches include conditioning on a single expert demonstration , utilizing a video of human demonstration , and incorporating the goal image . However, such prior methods assume that information about target behaviors in test environments is available to the agent, which is impractical in many real-world problems.

One alternative approach for improving generalization performance is to guide the agent with natural language: training agents conditioned on language instructions . Recent studies have indeed demonstrated that text-conditioned policies incorporated with large pre-trained multimodal models  exhibit strong generalization abilities . However, simply relying on text representations may fail to provide helpful information to agents in challenging scenarios. For example, consider a text-conditioned policy (see Figure 0(a)) trained to collect a coin, which is positioned at the end of the map, following the text instruction "collect a coin". When we deploy the learned agent to test environments where the coin's location is randomized, it often fails to collect the coin. This is because, when relying solely on expert demonstrations, the agent might mistakenly think that the goal is to navigate to the end of the level (see supporting results in Section 4.1). This example shows the simple text-conditioned policy fails to fully exploit the provided text instruction and suffers from goal misgeneralization (i.e., pursuing undesired goals, even when trained with a correct specification) .

In this paper, we introduce Adaptive Return-conditioned Policy (ARP), a novel IL method designed to enhance generalization capabilities. Our main idea is to measure the similarity between visual observations and natural language task descriptions in the pre-trained multimodal embedding space (such as CLIP ) and use it as a reward signal. Subsequently, we train a return-conditioned policy using demonstrations annotated with these multimodal reward labels. Unlike prior IL work that relies on static text representations , our trained policies make decisions based on multimodal reward signals computed at each timestep (see the bottom figure of Figure 0(a)).

We find that our multimodal reward can provide a consistent signal to the agent in both training and test environments (see Figure 0(b)). This consistency helps prevent agents from pursuing unintended goals (i.e., mitigating goal misgeneralization) and thus improves generalization performance when compared to text-conditioned policies. Furthermore, we introduce a fine-tuning scheme that adapts pre-trained multimodal encoders using in-domain data (i.e., expert demonstrations) to enhance the quality of the reward signal. We demonstrate that when using rewards derived from fine-tuned encoders, the agent exhibits superior generalization performance compared to the agent with frozen encoders in test environments. Notably, we also observe that ARP effectively guides agents in test environments with unseen text instructions associated with new objects of unseen colors and shapes (see supporting results in Section 4.3).

In summary, our key contributions are as follows:

* We propose Adaptive Return-conditioned Policy (ARP), a novel IL framework that trains a return-conditioned policy using adaptive multimodal rewards from pre-trained encoders.
* We introduce a fine-tuning scheme that adapts pre-trained CLIP models using in-domain expert demonstrations to improve the quality of multimodal rewards.
* We show that our framework effectively mitigates goal misgeneralization, resulting in better generalization when compared to text-conditioned baselines. We further show that ARP can execute unseen text instructions associated with new objects of unseen colors and shapes.
* We demonstrate that our method exhibits comparable generalization performance to baselines that consume goal images from test environments, even though our method solely relies on natural language instruction.
* Source code and expert demonstrations used for our experiments are available at [https://github.com/csmile-1006/ARP.git](https://github.com/csmile-1006/ARP.git)

Figure 1: (a) ARP utilizes the similarity between visual observations and text instructions in the pre-trained multimodal representation space as a reward and then trains the return-conditioned policy using demonstrations with multimodal reward labels. (b) Curves of multimodal reward for fine-tuned CLIP  in the trajectory from CoinRun environment. Multimodal reward consistently increases as the agent approaches the goal, and this trend remains consistent regardless of the training and test environment, suggesting the potential to guide agents toward target objects in test environments.

Related Work

Generalization in imitation learningAddressing the challenge of generalization in imitation learning is crucial for deploying trained agents in real-world scenarios. Previous approaches have shown improvements in generalization to test environments by conditioning agents on a robot demonstration [18; 20], a video of a human performing the desired task [76; 6], or a goal image [16; 44; 23]. However, these approaches have a disadvantage: they can be impractical to adopt in real-world scenarios where the information about target behaviors in test environments is not guaranteed. In this work, we propose an efficient yet effective method for achieving generalization even in the absence of specific information about test environments. We accomplish this by leveraging multimodal reward computed with current visual observations and task instructions in the pre-trained multimodal embedding space.

Pre-trained representation for reinforcement learning and imitation learningRecently, there has been growing interest in leveraging pre-trained representations for robot learning algorithms that benefit from large-scale data [54; 73; 61; 52]. In particular, language-conditioned agents have seen significant advancements by leveraging pre-trained vision-language models [46; 66; 78; 38], drawing inspiration from the effectiveness of multimodal representation learning techniques like CLIP . For example, _Instruct_RL  utilizes a pre-trained multimodal encoder  to encode the alignment between multiple camera observations and text instructions and trains a transformer-based behavior cloning policy using encoded representations. In contrast, our work utilizes the similarity between visual observations and text instructions in the pre-trained multimodal embedding space in the form of a reward to guide agents in the test environment adaptively.

We provide more discussions on related work in more detail in Appendix B.

## 3 Method

In this section, we introduce Adaptive Return-conditioned Policy (ARP), a novel IL framework for enhancing generalization ability using multimodal rewards from pre-trained encoders. We first describe the problem setup in Section 3.1. Section 3.2 introduces how we define the multimodal reward in the pre-trained CLIP embedding spaces and use it for training return-conditioned policies. Additionally, we propose a new fine-tuning scheme that adapts pre-trained multimodal encoders with in-domain data to enhance the quality of rewards in Section 3.3.

### Preliminaries

We consider the visual imitation learning (IL) framework, where an agent learns to solve a target task from expert demonstrations containing visual observations. We assume access to a dataset \(=\{_{i}\}_{i=1}^{N}\) consisting of \(N\) expert trajectories \(=(o_{0},a_{0}^{*},...,o_{T},a_{T}^{*})\) where \(o\) represents the visual observation, \(a\) means the action, and \(T\) denotes the maximum timestep. These expert demonstrations are utilized to train the policy via behavior cloning. As a single visual observation is not sufficient for fully describing the underlying state of the task, we approximate the current state by stacking consecutive past observations following common practice [53; 74].

We also assume that a text instruction \(\) that describes how to achieve the goal for solving tasks is given in addition to expert demonstrations. The standard approach to utilize this text instruction is to train a text-conditioned policy \((a_{t}|o_{ t},)\). It has been observed that utilizing pre-trained multimodal encoders (like CLIP  and M3AE ) is very effective in modeling this text-conditioned policy [46; 49; 66; 67]. However, as shown in the upper figure of Figure 0(a), these approaches provide the same text representations regardless of changes in visual observations. Consequently, they would not provide the agent with adaptive signals when encountering previously unseen environments. To address this limitation, we propose an alternative framework that leverages \(\) to compute similarity with the current visual observation within the pre-trained multimodal embedding space. We then employ this similarity as a reward signal. This approach allows the reward value to be adjusted as the visual observation changes, providing the agent with an adaptive signal (see Figure 0(b)).

### Adaptive Return-Conditioned Policy

Multimodal rewardTo provide more detailed task information to the agent that adapts over timesteps, we propose to use the visual-text alignment score from pre-trained multimodal encoders.

Specifically, we compute the alignment score between visual observation at current timestep \(t\) and text instruction \(\) in the pre-trained multimodal embedding space as follows:

\[r_{,}(o_{t},)=s(f^{}_{}(o_{t}),f^{}_{ }()). \]

Here, \(s\) represents a similarity metric in the representation space of pre-trained encoders: a visual encoder \(f^{}_{}\) parameterized by \(\) and a text encoder \(f^{}_{}\) parameterized by \(\). While our method is compatible with any multimodal encoders and metric, we adopt the cosine similarity between CLIP  text and visual embeddings in this work. We label each expert state-action trajectory as \(^{*}=(R_{0},o_{0},a^{*}_{0},...,R_{T},o_{T},a^{*}_{T})\) where \(R_{t}=_{i=t}^{T}r_{,}(o_{i},)\) denotes the multimodal return for the rest of the trajectory at timestep \(t\)1. The set of return-labeled demonstrations is denoted as \(^{*}=\{^{*}_{i}\}_{i=1}^{N}\).

Return-conditioned policyUsing return-labeled demonstrations \(^{*}\), we train return-conditioned policy \(_{}(a_{t}|o_{ t},R_{t})\) parameterized by \(\) using the dataset \(^{*}\) and minimize the following objective:

\[_{}()=_{^{*}^{*}} _{t T}l(_{}(a_{t}|o_{ t},R_{t}),a^{*}_{t}) \]

Here, \(l\) represents the loss function, which is either the cross entropy loss when the action space is defined in discrete space or the mean squared error when defined in continuous space.

The main advantage of our method lies in its adaptability in the deployment by adjusting to multimodal rewards computed in test environments (see Figure 0(a)). At the test time, our trained policy predicts the action \(a_{t}\) based on the target multimodal return \(R_{t}\) and the observation \(o_{t}\). Since the target return \(R_{t}\) is recursively updated based on the multimodal reward \(r_{t}\), it can provide a timestep-wise signal to the agent, enabling it to adapt its behavior accordingly. We find that this signal effectively guides the agent to prevent pursuing undesired goals (see Section 4.1 and Section 4.2), and it also enhances generalization performance in environments with unseen text instructions associated with objects having previously unseen configurations (as discussed in Section 4.3).

In our experiments, we implement two different types of ARP using Decision Transformer (DT) , referred to as ARP-DT, and using Recurrent State Space Model (RSSM) , referred to as ARP-RSSM. Further details of the proposed architectures are provided in Appendix A.

### Fine-Tuning Pre-trained Multimodal Encoders

Despite the effectiveness of our method with pre-trained CLIP multimodal representations, there may be a domain gap between the images used for pre-training and the visual observations available from the environment. This domain gap can sometimes lead to the generation of unreliable, misleading reward signals. To address this issue, we propose fine-tuning schemes for pre-trained multimodal encoders (\(f^{}_{},f^{}_{}\)) using in-domain dataset (expert demonstrations) \(\) in order to improve the quality of multimodal rewards. Specifically, we propose fine-tuning objectives based on the following two desiderata: reward should (i) remain consistent within similar timesteps and (ii) be robust to visual distractions.

Temporal smoothnessTo encourage the consistency of the multimodal reward over timesteps, we adopt the objective of value implicit pre-training (VIP)  that aims to learn smooth reward functions from action-free videos. The main idea of VIP is to (i) capture long-range dependency by attracting the representations of the first and goal frames and (ii) inject local smoothness by encouraging the distance between intermediate frames to represent progress toward the goal. We extend this idea to our multimodal setup by replacing the goal frame with the text instruction \(\) describing the task objective and using our multimodal reward \(R\) as below:

\[_{}(,)&= _{o_{1}_{1}}[r_{,} (o_{1},)]}_{}\\ &+_{(o_{t},o_{t+1})}[ (r_{,}(o_{t},)+1- r_{,}(o_{t+1}, ))]}_{}. \]Here \(_{1}\) denotes a set of initial visual observations in \(\). One can see that the local smoothness loss is the one-step temporal difference loss, which recursively trains the \(r_{,}(o_{t},)\) to regress \(-1+ r_{,}(o_{t+1},)\). This then induces the reward to represent the remaining steps to achieve the text-specified goal \(\), making rewards from consecutive observations smooth.

Robustness to visual distractionsTo further encourage our multimodal reward to be robust to visual distractions that should not affect the agent (_e.g.,_ changing textures or backgrounds), we introduce the inverse dynamics model (IDM) objective :

\[_{}(,)=_{(o_{t},o_{t+1},a_{t}) }[l(g(f^{}_{}(o_{t}),f^{}_{}(o_{t+1}),f ^{}_{}()),a^{*}_{t})], \]

where \(g()\) denotes the prediction layer which outputs \(}\), predicted estimate of \(a_{t}\), and \(l\) represents the loss function which is either the cross entropy loss when the action space is defined in discrete space or the mean squared error when it's defined in continuous space. By learning to predict actions taken by the agent using the observations from consecutive timesteps, fine-tuned encoders learn to ignore aspects within the observations that should not affect the agent.

Fine-tuning objectiveWe combine both VIP loss and IDM loss as the training objective to fine-tune pre-trained multimodal encoders in our model:

\[_{}(,)=_{}(,)+ _{}(,),\]

where \(\) is a scale hyperparameter. We find that both objectives synergistically contribute to improving the performance (see Table 7 for supporting experiments).

## 4 Experiments

We design our experiments to investigate the following questions:

1. Can our method prevent agents from pursuing undesired goals in test environments? (see Section 4.1 and Section 4.2)
2. Can ARP follow unseen text instructions? (see Section 4.3)
3. Is ARP comparable to goal image-conditioned policy? (see Section 4.4)
4. Can ARP induce well-aligned representation in test environments? (see Section 4.5)
5. What is the effect of each component in our framework? (see Section 4.6)

### Procgen Experiments

EnvironmentsWe evaluate our method on three different environments proposed in Di Langosco et al. , which are variants derived from OpenAI Procgen benchmarks . We assess the generalization ability of trained agents when faced with test environments that cannot be solved without following true task success conditions.

* CoinRun: The training dataset consists of expert demonstrations where the agent collects a coin that is consistently positioned on the far right of the map, and the text instruction is "The goal

Figure 2: Environments from OpenAI Procgen benchmarks  used in our experiments. We train our agents using expert demonstrations collected in environments with multiple visual variations. We then perform evaluations on environments from unseen levels with target objects in unseen locations. See Section 4.1 for more details.

is to collect the coin.". Note the agent may mistakenly interpret that the goal is to proceed to the end of the level, as this also leads to reaching the coin when relying solely on the expert demonstrations. We evaluate the agent in environments where the coin's location is randomized (see Figure 1(a)) to verify that the trained agent truly follows the intended objective.
* Maze I: The training dataset consists of expert demonstrations where the agent reaches a yellow cheese that is always located at the top right corner, and the text instruction is "Navigate a maze to collect the yellow cheese.". The agent may misinterpret that the goal is to proceed to the far right corner, as it also results in reaching the yellow cheese when relying only on expert demonstrations. To verify that the trained agent follows the intended objective, we assess the trained agents in the test environment where the cheese is placed at a random position (see Figure 1(b)).
* Maze II: The training dataset consists of expert demonstrations where the agent approaches a yellow diagonal line located at a random position, and the text instruction is "Navigate a maze to collect the line.". The agent might misinterpret the goal as reaching an object with a yellow color because it also leads to collecting the object with a line shape when relying only on expert demonstrations, For evaluation, we consider a modified environment with two objects: a yellow gem and a red diagonal line. The goal of the agent is to reach the diagonal line, regardless of its color, to verify that the agent truly follows the intended objective (see Figure 1(c)).

ImplementationFor all experiments, we utilize the open-sourced pre-trained CLIP model2 with ViT-B/16 architecture to generate multimodal rewards. Our return-conditioned policy is implemented based on the official implementation of _Instruct_RL , and implementation details are the same unless otherwise specified. To collect expert demonstrations used for training data, we first train PPG  agents on 500 training levels that exhibit ample visual variations for 200M timesteps per task. We then gather 500 rollouts for CoinRun and 1000 rollouts for Maze in training environments. All models are trained for 50 epochs on two GPUs with a batch size 64 and a context length of 4. Our code and datasets are available at [https://github.com/csmile-1006/ARP.git](https://github.com/csmile-1006/ARP.git). Further training details, including hyperparameter settings, can be found in Appendix C.

EvaluationWe evaluate the zero-shot performance of trained agents in test environments from different levels (i.e., different map layouts and backgrounds) where the target object is either placed in unseen locations or with unseen shapes. To quantify the performance of trained agents, we report the expert-normalized scores on both training and test environments. To report training performance, we measure the average success rate of trained agents over 100 rollouts in training environments and divide it by the average success rate from the expert PPG agent used to collect demonstrations. For test performance, we train a separate expert PPG agent in test environments and compute expert-normalized scores in the same manner.

Baseline and our methodAs a baseline, we consider _Instruct_RL , one of the state-of-the-art text-conditioned policies. _Instruct_RL utilizes a transformer-based policy and pre-trained M3AE 

Figure 3: Expert-normalized scores on training/test environments. The result shows the mean and standard variation averaged over three runs. ARP-DT denotes the model that uses pre-trained CLIP representations, and ARP-DT+ denotes the model that uses fine-tuned CLIP representations (see Section 3.3) for computing the multimodal reward.

representations for encoding visual observations and text instructions. For our methods, we use a return-conditioned policy based on Decision Transformer (DT)  architecture, denoted as ARP-DT (see Appendix A for details). We consider two variations: the model that uses frozen CLIP representations (denoted as ARP-DT) and the model that uses fine-tuned CLIP representations (denoted as ARP-DT+) for computing the multimodal reward. We use the same M3AE model to encode visual observations and the same transformer architecture for policy training. The main difference is that our model uses sequence with multimodal return, while the baseline uses static text representations with the concatenation of visual representations.

Comparison with language-conditioned agentsFigure 3 shows that our method significantly outperforms _Instruct_RL in all three tasks. In particular, ARP-DT outperforms _Instruct_RL in test environments while achieving similar training performance. This result implies that our method effectively guides the agent away from pursuing unintended goals through the adaptive multimodal reward signal, thereby mitigating goal misgeneralization. Moreover, we observe that ARP-DT+, which uses the multimodal reward from the fine-tuned CLIP model, achieves superior performance to ARP-DT. Considering that the only difference between ARP-DT and ARP-DT+ is using different multimodal rewards, this result shows that improving the quality of reward can lead to better generalization performance.

### RLBench Experiments

EnvironmentWe also demonstrate the effectiveness of our framework on RLBench , which serves as a standard benchmark for visual-based robotic manipulations. Specifically, we focus on Pick Up Cup task, where the robot arm is instructed to grasp and lift the cup. We train agents using 100 expert demonstrations collected from environments where the position of the target cup changes above the cyan-colored line in each episode (see the upper figure of Figure 4a). Then, we evaluate the agents in a test environment, where the target cup is positioned below the cyan-colored line (see the lower figure of Figure 4a). The natural language instruction \(\) used is "grasp the red cup and lift it off the surface with the robotic arm." For evaluation, we measure the average success rate over 500 episodes where the object position is varies in each episode.

SetupAs a baseline, we consider MV-MWM , which initially trains a multi-view autoencoder by reconstructing patches from randomly masked viewpoints and subsequently learns a world model based on the autoencoder representations. We use the same procedure for training the multi-view autoencoders for our method and a baseline. The main difference is that while MV-MWM does not use any text instruction as an input, our method trains a policy conditioned on the multimodal return as well. In our experiments, we closely follow the experimental setup and implementation of the imitation learning experiments in MV-MWM. Specifically, we adopt a single-view control setup where the representation learning is conducted using images from both the front and wrist cameras, but world model learning is performed solely using the front camera. For our methods, we train the return-conditioned policy based on the recurrent state-space model (RSSM) , denoted

Figure 4: (a) Image observation of training and test environments for Pick Up Cup task in RLBench benchmarks . (b) Success rates on both training and test environments. The result represents the mean and standard deviation over four different seeds. ARP-RSSM denotes the model that uses frozen CLIP representations for computing the multimodal reward, and ARP-RSSM+ denotes the model that incorporates fine-tuning scheme in Section 3.3.

as ARP-RSSM (see Appendix A for more details). We consider two variants of this model: the model utilizing frozen CLIP representations (referred to as ARP-RSSM) and the model that employs fine-tuned CLIP representations (referred to as ARP-RSSM+). To compute multimodal rewards using both frozen and fine-tuned CLIP, we employ the same setup as in Procgen experiments. Additional details are in Appendix D.

ResultsFigure 3(b) showcases the enhanced generalization performance of ARP-RSSM+ agents in test environments, increasing from 20.37% to 50.93%. This result implies that our method facilitates the agent in reaching target cups in unseen locations by employing adaptive rewards. Conversely, ARP-RSSM, which uses frozen CLIP representations, demonstrates similar performance to MV-MWM in both training and test environments, unlike the result in Section 4.1. We expect this is because achieving target goals for robotic manipulation tasks in RLBench requires more fine-grained controls than game-like environments.

### Generalization to Unseen Instructions

We also evaluate our method in test environments where the agent is now required to reach a different object with an unseen shape, color, and location by following unseen language instructions associated with this new object. First, we train agents in environments with the objective of collecting a yellow coin, which is always positioned in the far right corner, and learned agents are tested on unseen environments where the target object changes to a blue gem, and the target object's location is randomized. This new environment is referred to as CoinRun-bluegem (see Figure 4(a)), and we provide unseen instruction, "The goal is to collect the blue gem." to the agents. Table 1 shows that our method significantly outperforms the text-conditioned policy (_Instruct_RL) even in CoinRun-bluegem. This result indicates that our multimodal reward can provide adaptive signals for reaching target objects even when the color and shape change.

In addition, we verify the effectiveness of our multimodal reward in distinguishing similar-looking distractors and guiding the agent to the correct goal. To this end, we train agents using demonstrations from Maze II environments, where the objective is to collect the yellow line. Trained agents are tested in an augmented version of Maze II test environments: we place a yellow gem, a red diagonal line, and a red straight line in the random position of the map (denoted as Maze III in Figure 4(b)), and instruct the trained agent to reach the red diagonal line (\(=\)"Navigate a maze to collect the red diagonal line."). Table 2 shows that our method outperforms the baseline in Maze III, indicating that our multimodal reward can provide adaptive signals for achieving goals by distinguishing distractors.

### Comparison with Goal-Conditioned Agents

We compare our method with goal-conditioned methods, assuming the availability of goal images in both training and test environments. First, it is essential to note that suggested baselines rely on additional information from the test environment because they assume the presence of a goal image during the test time. In contrast, our method relies solely on natural language instruction and

   Model & Test Performance \\  _Instruct_RL & 63.99\% \(\) 3.07\% \\ ARP-DT (Ours) & 77.05\% \(\) 2.09\% \\ ARP-DT+ (Ours) & 79.06\% \(\) 6.69\% \\   

Table 1: Expert-normalized scores on CoinRun-bluegem test environments (see Figure 4(a)).

   Model & Test Performance \\  _Instruct_RL & 21.21\% \(\) 1.52\% \\ ARP-DT (Ours) & 33.33\% \(\) 4.01\% \\ ARP-DT+ (Ours) & 38.38\% \(\) 3.15\% \\   

Table 2: Expert-normalized scores on Maze III test environments (see Figure 4(b)).

Figure 5: Test environments used for experiments in Section 4.3.

does not necessitate any extra information about the test environment. As baselines, we consider a goal-conditioned version of _Instruct_RL (denoted as GC-_Instruct_RL), which uses visual observations concatenated with a goal image at each timestep. We also consider a variant of our algorithm that uses the distance of CLIP visual representation to the goal image for reward (denoted as GC-DT).

Figure 6 illustrates the training and test performance of goal-conditioned baselines and ARP-DT. First, we observe that GC-DT outperforms GC-_Instruct_RL in all test environments. Note that utilizing goal image is the only distinction between GC-DT and GC-_Instruct_RL. This result suggests that our return-conditioned policy helps enhance generalization performance. Additionally, we find that ARP-DT demonstrates comparable results to GC-DT and even surpasses GC-_Instruct_RL in all three tasks. Importantly, it should be emphasized that while goal-conditioned baselines rely on the goal image of the test environment (which can be challenging to provide in real-world scenarios), ARP-DT solely relies on natural language instruction for the task. These findings highlight the potential of our method to be applicable in real-world scenarios where the agent cannot acquire information from the test environment.

### Embedding Analysis

To support the effectiveness of our framework in generalization, we analyze whether our proposed method can induce meaningful abstractions in test environments. Our experimental design aims to address the key requirements for improved generalization in test environments: (i) the agent should consistently assign similar representations to similar behaviors even when the map configuration is changed, and (ii) the agent should effectively differentiate between goal-reaching behaviors and misleading behaviors. To this end, we measure the cycle-consistency of hidden representation from trained agents following . For two trajectories \(^{1}\) and \(^{2}\) with the same length \(N\), we first choose \(i N\) and find its nearest neighbor \(j=_{j N}||h(o^{1}_{ i},a^{1}_{<i})-h(o^{2}_{ j},a^{2}_{<j })||_{2}\), where \(h()\) denotes the output of the causal transformer of ARP-DT (refer to Appendix A for details). In a similar manner, we find the nearest neighbor of \(j\), which is denoted as \(k=_{k N}||h(o^{1}_{ k},a^{1}_{<k})-h(o^{2}_{ j},a^{2}_{<j })||_{2}\). We define \(i\) as cycle-consistent if \(|i-k| 1\), can return to its original point. The presence of cycle-consistency entails a precise alignment of two trajectories within the hidden space.

In our experiments, we first collect the set of success/failure trajectories from \(N\) different levels in CoinRun test environment, which is denoted as \(^{n}_{}\) or \(^{n}_{}\) where \(n N\). Next, we extract hidden representations from trained agents at each timestep across all trajectories. We then measure cycle-consistency across these representations using three different pairs of trajectories (see Figure 7):

1. \((^{n_{1}}_{},^{n_{2}}_{})\) (\(\)): We compute the cycle-consistency between success trajectories from different levels. This indicates whether the trained agents behave in a similar manner in success cases, regardless of different visual contexts.
2. \((^{n_{1}}_{},^{n_{2}}_{})\) (\(\)): Similarly, we compute the cycle-consistency between failure trajectories from different levels.
3. \((^{n_{1}}_{},^{n_{1}}_{})\) (\(\)): We measure the cycle-consistency between success trajectory and failure trajectory from the same level. This evaluates whether the agent can act differently in success and failure cases.

Figure 6: Expert-normalized scores on training/test environments. The result shows the mean and standard variation averaged over three runs. ARP-DT shows comparable or even better generalization ability compared to goal-conditioned baselines.

Note that (\(\)) / (\(\)) implies that higher/lower value is better, respectively. For implementation, we first select ten different levels in CoinRun test environment with different coin locations. We then collect success and failure trajectories from each level and use the last ten timesteps of each trajectory for measuring cycle-consistency. Table 3 shows the percentage of timesteps that are cycle-consistent with other trajectories from different pairs. Similar to results described in Section 4.1, our proposed methods significantly improve cycle-consistency compared to _Instruct_RL in all cases. Moreover, ARP-DT+, which utilizes the multimodal reward from the fine-tuned CLIP model, outperforms ARP-DT with the frozen CLIP.

### Ablation Studies

Effect of pre-trained multimodal representationsTo verify the effectiveness of pre-trained multimodal representations, we compare ARP-DT+ with agents using multimodal rewards obtained from a smaller-scale multimodal transformer, which was trained from scratch using VIP and IDM objectives, denoted as ARP-DT+ (scratch). Table 4 shows a significant decrease in performance for ARP-DT+ (scratch) when compared to ARP-DT+ across all environments, particularly in the training performance within Maze environments. These findings highlight the crucial role of pre-training in improving the efficacy of our multimodal rewards.

Effect of fine-tuning objectivesIn Table 5, we examine the effect of fine-tuning objectives by reporting the performance of ARP-DT fine-tuned with or without the VIP loss \(_{}\) (Equation 3) and the IDM loss \(_{}\) (Equation 4). We find that the performance of ARP-DT improves with either \(_{}\) or \(_{}\), which shows the effectiveness of the proposed losses that encourage temporal smoothness and robustness to visual distractions. We also note that the performance with both objectives is the best, which implies that both losses synergistically contribute to improving the quality of the rewards.

## 5 Conclusion

In this paper, we present Adaptive Return-conditioned Policy, a simple but effective IL framework for improving generalization capabilities. Our approach trains return-conditioned policy using the adaptive signal computed with pre-trained multimodal representations. Extensive experimental results demonstrate that our method can mitigate goal misgeneralization and execute unseen text instructions associated with new objects compared to text-conditioned baselines. We hope our framework could facilitate future research to further explore the potential of using multimodal rewards to guide IL agents in real-world applications.

   Env & Model & Train (\%) & Test (\%) \\   & ARP-DT+ & 90.28\% + 1.50\% & 72.36\% + 1.48\% \\  & ARP-DT+ (scratch) & 77.08\% + 1.40\% & 62.48\% + 5.53\% \\   & ARP-DT+ & 75.47\% + 2.15\% & 86.13\% + 0.83\% \\  & ARP-DT+ (scratch) & 18.87\% + 5.85\% & 32.58\% + 2.17\% \\   & ARP-DT+ & 64.18\% + 3.62\% & 40.95\% + 2.09\% \\  & ARP-DT+ (scratch) & 22.51\% + 0.83\% & 37.62\% + 4.73\% \\   

Table 4: Ablation study of using pre-trained CLIP representations.

    & _Instruct_RL & ARP-DT & ARP-DT+ \\  \((_{}^{l_{1}},_{}^{l_{2}})\) (\(\)) & 22.66\% + 4.03\% & 24.75\% + 1.09\% & **29.07\% + 1.30\%** \\ \((_{}^{l_{1}},_{}^{l_{2}})\) (\(\)) & 14.12\% + 2.10\% & **24.95\% + 0.84\%** & **24.29\% + 0.80\%** \\ \((_{}^{l_{1}},_{}^{l_{2}})\)(\(\)) & 35.09\% + 2.06\% & 30.18\% + 1.21\% & **5.65\% + 4.25\%** \\   

Table 3: We investigate the cycle consistency of trained agents’ hidden representations on different sets of trajectories in CoinRun environments. The results are presented as the mean and standard deviation averaged over three different seeds. Scores within one standard deviation from the highest average score are marked in bold. (\(\)) and (\(\)) denotes that higher/lower values are better, respectively.

Figure 7: Visual observations of trajectories in CoinRun environments. We construct 3 different pairs to evaluate whether trained agents have well-aligned hidden representations.