# OneBit: Towards Extremely Low-bit

Large Language Models

Yuzhuang Xu\({}^{1}\) Xu Han\({}^{2}\) Zonghan Yang\({}^{2}\) Shuo Wang\({}^{2}\) Qingfu Zhu\({}^{1}\) Zhiyuan Liu\({}^{2}\) Weidong Liu\({}^{2}\) Wanxiang Che\({}^{1,\)\({}^{1}\)\({}^{1}\)Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, Harbin, China

\({}^{2}\)Department of Computer Science & Technology, Tsinghua University, Beijing, China

xyz21thu@gmail.com, car@ir.hit.edu.cn

###### Abstract

Model quantification uses low bit-width values to represent the weight matrices of existing models to be quantized, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs. However, current quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs. For this target, we introduce a 1-bit model compressing framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the quantization framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 81% of the non-quantized performance on LLaMA models) with robust training processes when only using 1-bit weight matrices. Code and checkpoints are available at https://github.com/xuyuzhuang11/OneBit

## 1 Introduction

Transformer  has emerged as the pivotal architecture in large language models (LLMs), fundamentally reshaping the approach to natural language processing in deep learning era . Despite their popularity, deploying transformer-based LLMs presents significant challenges due to their computational intensity and considerable memory requirements as the parameters of LLMs become more and more. For instance, even moderately-sized LLMs like LLaMA-13B  require around 26GB of memory to load its all parameters in FP16 format. Such overheads make deploying LLMs difficult beyond mid-to-high-end GPUs like the A100, let alone on mobile devices. The high demand for resources not only drives up usage costs, but also restricts their wider application.

Numerous efforts  have been devoted to reducing the computational and memory overheads of LLMs, while still preserving most of their original model capabilities. Among these efforts, quantization has gained widespread attention, particularly Post-Training Quantization (PTQ), benefitted from its lower transferring costs. Seminal studies such as GPTQ , SpQR , and AWQ  successfully compress the weight matrices of LLMs to 4-bit values while maintaining the main abilities of LLMs. Efficient quantization represents significant advances in LLM optimization, by achieving a balance between time and space efficiency as well as model performance.

Unfortunately, the efficacy of PTQ rapidly diminishes when the quantization bit-width is extremely low, as shown in Figure 1. Existing PTQ methods managed to compress weight matrices down to at least 3-bit . Recent researches hope to leverage Quantization-Aware Training (QAT) toovercome the bottlenecks faced by PTQ. LLM-QAT  introduces a few learnable parameters into the quantization process, achieving notable results. OmniQuant , integrating learnable equivalent transformation, presents promising results in 2-bit quantization. However, existing methods decline when compressing model weights to 1 bit, struggging to maintain effectiveness. This mainly stems from the drastic precision loss at extremely low bit-width representation in weight matrix \(\), significantly increasing loss in linear projection \(\), which is the core operator within LLMs.

In this paper, we propose a novel Linear layer and Sign-Value-Independent Decomposition (SVID) for weight matrices to represent LLMs using approximately 1-bit values. In our novel layer architecture, each original high-bit weight matrix is represented as one sign matrix (\( 1\)) and two value vectors. The value vectors provide necessary floating-point precision in linear projection at little cost and help the model to be trained easily. The sign matrix maintains the high rank of the original weight matrix with a small space cost, thereby preserving high information capacity. SVID offers a better parameter initialization for 1-bit models from the non-quantized model and we employ quantization-aware knowledge distillation to transfer the capabilities of the original model to the proposed 1-bit counterpart. Experiments demonstrate that our method performs well at the W1A16 (1-bit weight and 16-bit activation) quantization level. Furthermore, our 1-bit model is more amenable to training and knowledge transfer than previous works. In summary, our contributions are 3-fold:

* We propose a novel and efficient 1-bit model architecture for LLMs, which can improve both the time and space efficiency during model inference. Moreover, our architecture is more stable during quantizing LLMs.
* We propose SVID to decompose high-bit matrices into low-bit ones, which is essential for the initialization of our 1-bit architecture. Experiments demonstrate that the SVID-based initialization can improve the model performance and convergence speed.
* Extensive experiments demonstrate that our method works well in model sizes from 1.3B to 13B in OPT, LLaMA, and LLaMA2, showcasing its generalizability.

## 2 Related Work

### Large Language Model Compression

Quantization, pruning, and knowledge distillation (KD) are the mainstream methods for model compression. Quantization compresses model weights into low-bit values [14; 20; 11]. For data type alignment in computation and reducing memory, it also involves quantizing activation [10; 39] and key-value cache . Pruning simplifies model complexity by removing unimportant weights or modules, thereby sparsifying the original larger models [13; 31; 22]. KD trains a smaller student model under the guidance of a larger teacher model [16; 1], achieving the purpose of compressing the larger one. Beyond these methods, low-rank factorization approximates the original weight matrix \(\) with the product of two lower-rank matrices  and also achieves promising results. Our work belongs to quantization, using KD for knowledge transfer from the original LLM and uniquely

Figure 1: The perplexity (lower scores mean better performance) of existing widely-used low-bit quantization methods on LLaMA-7B, reported on Wikitext2 . All the examined previous approaches suffer from significant performance degradation when quantizing models to 2-bit values. Our 1-bit quantization method can outperform these 2-bit baselines.

focusing on extremely low bit-width quantization. More details about model compression can refer to existing survies [37; 43].

### Large Language Model Quantization

Since this paper aims to obtain extremely low-bit LLMs, here we thus introduce more details about LLM quantization. Quantization stands as a popular and crucial method for model compression, capable of achieving a significant compression ratio with a relatively small loss. It can be classified into Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) according to when quantization is applied.

PTQ directly converts trained models into lower-bit counterparts using accurate solvers and limited calibration data without additional training. Typically, GPTQ  row-wisely quantizes weight matrices and adjusts remaining weights to compensate for the precision loss caused by quantization, achieving nearly lossless 4-bit weight quantization. Moreover, numerous studies observed the effect of "outliers" in quantization [10; 18; 20]. LLM.int8()  suggests mixed-precision decomposition to ensure the accuracy of a few outliers in activations. SmoothQuant  reduces the difficulty of quantization by smoothing the outliers of activation. SpQR  identifies sensitive weights to ensure their precision, while quantizing other weights to lower bit-width.

QAT integrates quantization steps within the model, applying them during training or fine-tuning. It allows the model to better adapt to the reduced precision induced by quantization, leading to improved performance compared to PTQ. LLM-QAT  introduces a small number of learnable parameters into quantization and employs KD using data generated by the original model itself. OmniQuant (30; we classify it as QAT) further introduces learnable equivalent transformation, achieving acceptable results in 2-bit weight quantization. Contemporary work QuIP#  combines randomized Hadamard transform, vector quantization techniques, and fine-tuning to achieve better performance in 2-bit level. PEQA  and QLoRA  focus on fine-tuning a limited number of extra parameters to mitigate the precision loss caused by sub-4bit weight quantization. Our work is closely related to QAT, but due to the unique challenges posed by 1-bit quantization, our representation and initialization methods of quantized weights are distinct from any existing work.

## 3 Methodology

This section demonstrates our 1-bit architecture of the Linear layer to be quantized and discuss how to initialize the quantized model to achieve better performance in knowledge distillation. We start with a short review of classical weight quantization methods in Section 3.1 and then formulate our OneBit from Section 3.2 to Section 3.4 in detail.

### Background

The main idea of model quantization is to compress each weight matrix \(\) within models in FP32 or FP16 format to a low-bit counterpart. Specifically, we often quantize the weight matrices of Linear layers in transformer to 8, 4, and even 2 bits.

The majority of quantization studies primarily employ the round-to-nearest (RTN) method, by which the weight \(w\) is rounded to the nearest value in the quantization grid. It can be formulated as

\[=(+z,0,2^{N}-1 ),\] (1)

where \(s\) denotes the quantization scale parameter, \(z\) denotes the zero point parameter, and \(N\) is the quantization bit-width. \(()\) truncates the result in the range of \(0\) to \(2^{N}-1\). With the bit-width being lower and lower, the quantization grid also becomes sparser. When we quantize a LLM to 1-bit values, there are only 2 available numbers to be chosen in the quantized model. Existing study  points out that quantization based on the RTN method may get their best performance at the 4-bit level. Further quantizing to 2-bit values following this paradigm would result in a substantial degradation  as shown in Figure 1.

Furthermore, when \(N\) equals \(1\), quantization based on RTN method is essentially equivalent to setting a threshold, with weight \(w\) on either side of it being converted to corresponding integer value \(\). In such a scenario, the parameters \(s\) and \(z\) in Eq. (1) effectively lose their practical significance. Consequently, when quantizing weights to 1 bit, the element-wise RTN operation drastically undermines the precision of the weight matrix \(\), leading to poor performance of the quantized model.

### 1-bit Linear Layer Architecture

Due to the severe precision loss of 1-bit weight quantization, converting weight matrices in Linear layers directly from FP32/16 to 1-bit format based on RTN is challenging. Wang et al.  explore this possibility by studying the capabilities of purely 1-bit weight matrices, training the 1-bit model _from scratch_. In the W1A16 setting, their Linear layers are designed as

\[_{ 1}=- ,\\ =-,\\ = _{ 1}^{},\] (2)

where \(\) denotes the quantized weight matrix with the shape \(m n\) and \(_{ 1}\) denotes the 1-bit quantized matrix. \(\) is the input of Linear layer and \(\) is the output. \(()\), \(()\) and \(()\) functions return the sign matrix, average and absolute value matrix. Unfortunately, this approach reduces computational demands but also leads to a marked decrease in performance . Moreover, due to training difficulties, experiments show that this method is challenging to use for quantizing existing models and can only be applied to training models _from scratch_.

Inspired by Wang et al. , we also quantize the weight matrix using the function \(()\), and the element of the quantized matrix is set to +1 or -1 as well. Moreover, we also notice that although \(_{ 1}\) maintains a high rank of \(\), the missed floating-point precision still destroys the model performance. Therefore, different from previous work, we introduce 2 value vectors with an FP16 format to compromise the precision loss in the quantization process. During training, our proposed Linear layers are designed as

\[_{ 1}= ,\\ =_{ 1}^{},\\ =,\] (3)

where \(\) and \(\) are the two FP16 value vectors. During inference, \(_{ 1}\) is packed with an INT1 format, and \(()\) will not be used, as shown in Figure 2. Note that we specify the calculation order using brackets in Eq. (3) for minimizing the time and space cost. The main difference between Wang et al.  and OneBit is the extra parameter \(\) and \(\). Even if additional parameters are brought in, the benefits far outweigh its small cost. For instance, when we quantize one weight matrix with the shape \(4096 4096\), the average bit-width of the quantized result is _1.0073_. See A.7 for the details.

### Sign-Value-Independent Decomposition

In our proposed 1-bit architecture, the weight matrix \(\) is mathematically divided into two components: one sign matrix \(_{ 1}\) in INT1 format and two value vector \(/\) in FP16 format. To initialize

Figure 2: The main idea of our method OneBit. The left is the original FP16 Linear Layer, in which both the activation \(\) and the weight matrix \(\) are in FP16 format. The right is our proposed architecture. Only value vectors \(\) and \(\) are in FP16 format, and the weight matrix consists of \( 1\) instead, which can be represented in INT1.

the 1-bit model with the help of the fully trained weight, we introduce the Sign-Value-Independent Decomposition (SVID) of the weight matrix \(\), which can be formulated as \(=_{}_{}\). Here we have \(_{}=||\) and \(_{}=()\). For \(_{}\), we further approximately decompose it into the outer product of two vectors \(\) and \(\), which is also known as _rank-1 approximation_. Hence, our proposed matrix decomposition method can be represented as

\[_{}(^{ }).\] (4)

We can employ some widely used matrix decomposition methods to perform the rank-1 approximation, such as SVD  and NMF .

Proposition 1Given the weight matrix \(\) and input \(\), the Linear layer can be reformulated as the following according to SVID:

\[^{}[( ^{})_{}^{}] ^{}.\] (5)

We prove this approximation in Appendix A.1. This bridges the gap between the architecture of the quantized model and its original weights. It indicates that if we assign \(_{}\) to \(_{ 1}\), \(^{}\) to \(\) and \(^{}\) to \(\), the quantized model is an approximate initialization of the original model. Moreover, compared to restoring the original matrix \(\) first (such as in Eq. (4)), the computational order in Eq. (5) saves approximately one matrix \(\) in FP16 format in memory as there is no need to restore \(\) in FP16 format.

The main objective of SVID is to involve the sign matrix \(_{}\) in approximating matrix \(\), rather than solely relying on value vectors in FP16 format. To substantiate the role of the sign matrix \(_{}\) in matrix approximation, we present the following proposition.

Proposition 2Given matrices \(\) and \(||\), \(=_{}||\). We decompose these matrices in the way \(=^{}+_{1}\) and \(||=}}^{}+_{2}\), where \(_{i}\) denotes the error matrices. In terms of the Frobenius-norm, the SVID is closer to the original matrix \(\):

\[\|-_{}}}^{}\|_{}^{2}\|-^{}\|_{}^{2}.\] (6)

We also prove this proposition in Appendix A.1. It clearly demonstrates the practical role of the sign matrix \(_{}\) in matrix approximation.

Note that, given the predominantly low precision of most parameters, it is quite challenging to approximate the weight matrix \(\) accurately. SVID is not aimed to precisely replicate the original model's parameters, but to provide an effective starting point for further training, leveraging the extensive training of the original model. Details on transferring knowledge from the original model to the quantized counterpart are in Section 3.4.

### Knowledge Transfer

We employ quantization-aware knowledge distillation to transfer knowledge from the original model (i.e. teacher model) to the quantized one (i.e. student model). In the student model, the element in matrix \(\) and vectors \(/\) in Eq. (3) will be trained. We use cross-entropy based logits and mean-square-error based hidden state of the full-precision teacher model to direct the quantized student model . Language modeling loss is not used. The cross-entropy is defined as

\[_{}=-}_{i=1}^{n_{s}}_{c}P_{c}^{ }(_{i}) P_{c}^{}( _{i}),\] (7)

where \(c\) denotes the number of classes and \(n_{s}\) denotes the number of training samples in the current batch. \(\) and \(\) are the teacher model and student model, respectively. The error of hidden states is defined as

\[_{}=_{i=1}^{n_{s}}_{j=1}^{n_{l}}\|_{i,j}^{}}{\|_{i,j}^{}\| _{2}}-_{i,j}^{}}{\|_{i,j}^{}\|_{2}}\|_{2}^{2},\] (8)where \(n_{l}\) denotes the number of layers and \(\) denotes the hidden state. Hence the final objective function can be formulated as

\[_{}=_{}+_{},\] (9)

where \(\) is the hyper-parameter that balances the importance of the cross-entropy loss and the features in the intermediate layers. Please refer to A.6 for further discussions of this part.

## 4 Experiments

We experiment with 1-bit weight-only quantizaton and maintain 16-bit activation (W1A16) in this work. We evaluate our approach by performing experiments on OPT-1.3B/2.7B models, LLaMA-7B/13B models and LLaMA2-7B/13B models, and present results on various tasks.

### Settings

DataFor the training data of our quantization-aware knowledge distillation, we follow Liu et al.  to synthesize corpus using next token generation from the original teacher model. It randomizes the first token from vocabulary and generates the next token iteratively until reaching either the <_EOS_> token or the maximum length. Specially, the top-1 predictions are selected deterministically for the first 3 to 5 tokens, followed by stochastic sampling for the remaining tokens. We utilized LLaMA-7B to generate a total of 132k data entries, each with a maximum length of 2,048.

Training DetailsEvery KD experiment learns the training data over 50 epochs, from which 2048-token segments are selected. We employ NMF in scikit-learn 1 to decompose the weight matrices in SVID. The quantized student models are optimized by Adam  with \(_{1}=0.9\), \(_{2}=0.98\). The learning rate for all experiments is scheduled by _cosine_ strategy. We use NVIDIA A100 GPUs and maintain FP16 precision while training quantized models. For additional details such as learning rate, please refer to Table 1.

BaselinesTo our knowledge, there is no previous work exploring the 1-bit quantization of LLMs from a knowledge transfer perspective. To this end, we relax the quantization bit-width of baselines to 2 bits (_W2A16_) while maintaining the _W1A16_ setting in our method. We compare our method with GPTQ , LLM-QAT  and OmniQuant . To ensure a fair comparison in terms of space usage, baselines do not employ grouped quantization. Additionally, we included the results of vanilla transformers with FP16 precision as a reference. While the recent work BitNet  also introduced one 1-bit model architecture, it only worked for training models from scratch. We also analyze its capability to transfer knowledge from the original models in Appendix A.5.

Evaluation MetricsBasically, we evaluate quantized models by testing the perplexity on the validation set, specifically on WikiText2  and C4 . Lower perplexity indicates that the compressed model is better at preserving the output distribution of the original model. Furthermore, accuracies of zero-shot tasks including Winogrande , HellaSwag , PIQA , BoolQ , and ARC  are also reported. They evaluate if the capabilities of the original model on downstream tasks are retained. We utilize the open-sourced toolkit "LM-Evaluation-Harness"2 to perform the perplexity test and all zero-shot tasks.

   Models & learning rate & \(\) & \# GPUs \\  OPT-1.3B & 4e-4 & 1.0 & 1 \(\) 8 \\ OPT-2.7B & 2e-4 & 1.0 & 1 \(\) 8 \\ LLaMA-7B & 4e-4 & 1.0 & 1 \(\) 8 \\ LLaMA-13B & 2e-4 & 1.0 & 2 \(\) 8 \\ LLaMA2-7B & 1e-4 & 1.0 & 1 \(\) 8 \\ LLaMA2-13B & 2e-4 & 1.0 & 2 \(\) 8 \\   

Table 1: Training details of knowledge distillation.

### Main Results

Table 2 compares our method with other typical strong baselines on different models. Due to space limitations, results of LLaMA2-7B/13B are listed in Appendix A.3. In various model sizes, our 1-bit weight quantization method obviously outperforms others under the W2A16 setting. Moreover, the effectiveness of QAT based methods consistently improves as the model size increases, whereas the result of the PTQ method, GPTQ, may degrade when model size increases (e.g., from 7B to 13B on LLaMA). This demonstrates that QAT-based method can achieve stable results in extremely low-bit quantization. Specifically, our method approaches the performance of FP16 more closely as the model size increases. For instance, when scaling from LLaMA-7B to LLaMA-13B, the perplexity (on C4) of the FP16 model decreases by only 0.47, whereas our method sees a reduction of 1.15.

For perplexity, only our method achieves comparable results to the strongest FP16 baseline. For instance, our method achieves 9.18 in the Wiki2 dataset on LLaMA-13B model and the FP16 baseline is 5.09. The performance loss of other methods is significant, even though they use 2-bit quantization, which is more than our 1 bit. For GPTQ and LLM-QAT, the performance degradation after quantization is pretty severe. As for OmniQuant, even though it is the strongest baseline under the W2A16 setting, it still suffers greater performance loss compared to our W1A16 setting.

For zero-shot accuracy, although all methods inevitably have some degradation, our method achieves the closest performance to the FP16 baseline among most models. On the OPT-1.3B/2.7B model, our method shows smaller performance loss on most tasks such as PIQA and ARC-e. Additionally, the loss of other tasks is negligible compared with the second-best baseline, OmniQuant. On the LLaMA-7B model, our method also notably outperforms OmniQuant in most tasks except BoolQ/ARC-e, averaging about a 4% improvement overall.

### Problem Solving Ability

We have demonstrated the superior performance of our method under the W1A16 setting, compared to other representative baselines. Although all methods inevitably face performance degradation in

    &  &  &  \\  & & Wiki2 & C4 & Wino. & Hella. & PIQA & BoolQ & ARC-e & ARC-c & Avg. \\   & FP16 & 14.63 & 14.72 & 59.67 & 53.73 & 72.42 & 57.68 & 50.80 & 29.69 & 54.00 \\  & GPTQ & 9.5e3 & 3.8e3 & 49.33 & 25.57 & 52.07 & 39.60 & 26.68 & 23.63 & 36.15 \\  & LLM-QAT & 4.9e3 & 2.1e3 & 49.72 & 25.72 & 50.05 & 37.83 & 25.76 & **25.09** & 35.70 \\  & OmniQuant & 42.43 & 55.64 & **51.85** & 33.39 & 60.94 & 56.45 & 38.76 & 23.38 & 44.13 \\  & OneBit & **25.42** & **22.95** & 51.14 & **34.26** & **62.57** & **59.45** & **41.25** & 24.06 & **45.46** \\   & FP16 & 12.47 & 13.17 & 60.93 & 60.59 & 74.81 & 60.28 & 54.34 & 31.31 & 57.04 \\  & GPTQ & 8.7e3 & 3.9e3 & 49.88 & 26.47 & 49.84 & 39.88 & 25.76 & **26.02** & 36.31 \\  & LLM-QAT & 3.7e3 & 1.4e3 & 52.09 & 25.47 & 49.29 & 37.83 & 24.92 & 25.60 & 35.87 \\  & OmniQuant & 30.25 & 41.31 & 51.62 & **38.21** & 62.19 & 54.25 & 40.82 & 24.74 & 45.31 \\  & OneBit & **21.86** & **20.76** & **51.67** & 38.18 & **63.87** & **54.28** & **43.39** & 24.40 & **45.97** \\   & FP16 & 5.68 & 7.08 & 66.85 & 72.99 & 77.37 & 73.21 & 52.53 & 41.38 & 64.06 \\  & GPTQ & 1.9e3 & 7.8e2 & 49.41 & 25.63 & 49.95 & 43.79 & 25.84 & 27.47 & 37.02 \\   & LLM-QAT & 7.1e2 & 3.0e2 & 51.78 & 24.76 & 50.87 & 37.83 & 26.26 & 25.51 & 36.17 \\   & OmniQuant & 15.34 & 26.21 & 52.96 & 43.68 & 62.79 & **58.69** & 41.54 & 29.35 & 48.17 \\   & OneBit & **10.19** & **11.40** & **58.48** & **51.54** & **68.01** & 57.28 & **42.47** & **30.20** & **51.33** \\   & FP16 & 5.09 & 6.61 & 70.17 & 76.24 & 79.05 & 68.47 & 59.85 & 44.54 & 66.39 \\   & GPTQ & 3.2e3 & 9.9e2 & 50.67 & 25.27 & 50.00 & 42.39 & 26.14 & 27.39 & 36.98 \\   & LLM-QAT & 1.8e3 & 1.2e3 & 51.62 & 25.40 & 50.33 & 37.83 & 27.02 & 26.87 & 36.51 \\   & OmniQuant & 13.43 & 19.33 & 53.83 & 54.16 & 68.99 & 62.20 & **45.50** & 30.38 & 52.51 \\   & OneBit & **9.18** & **10.25** & **62.90** & **56.78** & **70.67** & **64.16** & 44.53 & **32.00** & **55.17** \\   

Table 2: Main results of evaluation experiment. We report the perplexity and zero-shot accuracy. “FP16” is the transformer with FP16 parameters and we refer to it as the upper-bound of all the methods. The **best** score is bolded.

1-bit weight quantization, it remains of interest how our method fares in solving practical problems among the various approaches to reducing model size. For instance, directly training smaller models  or employing low-rank decomposition to reduce the number of parameters.

To this end, we consider two crucial abilities of LLMs: commonsense reasoning and world knowledge. For commonsense reasoning, we use the 6 tasks (Hellaswag, etc.) and settings described in Section 4.2. For world knowledge, we examine it using the Massive Multi-task Language Understanding (MMLU; 15), a benchmark that covers wide domains and knowledge. We compare the following 4 models:

**Pythia-1.0B** A well-trained model released by EleutherAI whose memory footprint is 1.54x that of our OneBit-7B model.

**TinyLLaMA-1.1B** A model with the same structure as the LLaMA models, which undergoes continued training. To compare fairly, we use the checkpoint at 10k training steps, which is 2x that of our OneBit-7B model.

**LowRank LLaMA** Decompose every weight matrix in Linear layers to two low-rank matrices and learn from the original LLaMA-7B model by KD in the same setting of OneBit-7B.

**OneBit-7B** The model that we use in Section 4.2, which is built with OneBit.

Figure 2(a) and 2(b) demonstrate common sense reasoning ability and general world knowledge of different models. We can observe that, although other models have more parameters and are more thoroughly trained than ours, our model still has advantages in common sense reasoning. This reflects the benefits inherited from the larger 7B model. In terms of world knowledge, despite a significant loss in social sciences, our model outperforms the fully trained Pythia-1B in other domains. These results demonstrate the practical usability of OneBit.

Figure 3: Comparison of model capabilities and compressive degree.

## 5 Analysis and Discussion

### Efficiency

It is evident that extremely low-bit quantization of weights can significantly reduce the memory footprint of models. As shown in Table 3, the actual compression ratio increases as the model size increases. This is particularly meaningful for larger models, making it possible to fit the model into one GPU. While there is a performance loss, Figure 4 illustrates that our method achieves a good trade-off between space occupancy and model performance. For example, we can achieve comparable performance to FP16 with only 0.2x the model space. Furthermore, quantizing to \( 1\) also aids in accelerating matrix multiplication on CPUs. It is because the floating-point multiplication of elements in two matrices can be converted into much faster bit operations on these chips. Thus the substantial reduction in memory overhead makes these low-bit LLMs meet the requirements for deployment on PCs and smartphones.

### Robustness

Existing work  has already noted the instability within QAT. Extremely low-bit quantization makes the training process highly sensitive to the learning rate, making it difficult for the model to converge when the rate is too small or too large. This is primarily due to the large magnitude of gradients generated as the weight elements fluctuate between +1 and -1, leading to substantial fluctuations in the output of Linear layers. Experiments demonstrate that OneBit shows more stable training process and is not sensitive to learning rates. Please refer to Appendix A.5 for more details.

### Effect of Different Components

The variable components in our method primarily include Post-LayerNorm, value vectors, and parameter initialization.

Post-LayerNormWe discover that there might be floating-point overflow during the QAT process. As depth increases, the activation can become progressively larger. We tackle it using Post-LayerNorm instead of Pre-LayerNorm. In contrast, Pre-LayerNorm may occasionally be ineffective.

Figure 4: Tradeoff between size and PPL.

   Models & FP16 (GB) & OneBit (GB) & Ratio (\%) \\  LLaMA-7B & 13.5 & 1.3 & 90.4 \\ LLaMA-13B & 26.0 & 2.2 & 91.5 \\ LLaMA-30B & 65.1 & 4.9 & 92.5 \\ LLaMA-65B & 130.6 & 9.2 & 93.4 \\   

Table 3: Compression ratio of LLaMA models.

Value VectorsThe main structural difference between OneBit and BitNet  is the two value vectors, which are demonstrated to be effective in Section 4.2. They facilitate stable training and the knowledge transfer process. Please refer to Appendix A.5 for more details of comparison.

Parameter InitializationIn our proposed SVID, both NMF and SVD can be used to decompose \(||\) and we recommend using the former. This is because we find that NMF may make the training more faster to converge. Figure 5 shows that initializing by NMF facilitates better performance.

## 6 Conclusion

We propose a novel model structure for 1-bit weight quantization and a corresponding parameter initialization method to address the difficulty in 1-bit quantization. Extensive experiments on LLMs of various sizes and series demonstrate that OneBit has clear advantages over representative strong baselines and achieves a good tradeoff between model size and performance. We further analyze the capabilities of such extremely low-bit quantized models and provide guidance for future research.