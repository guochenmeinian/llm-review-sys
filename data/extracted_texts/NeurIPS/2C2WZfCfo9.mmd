# DOSE: Diffusion Dropout with Adaptive Prior for Speech Enhancement

Wenxin Tai\({}^{1}\), Yue Lei\({}^{1}\), Fan Zhou\({}^{1,2}\)1, Goce Trajcevski\({}^{3}\), Ting Zhong\({}^{1,2}\)

University of Electronic Science and Technology of China

Kashi Institute of Electronics and Information Industry

Iowa State University

###### Abstract

Speech enhancement (SE) aims to improve the intelligibility and quality of speech in the presence of non-stationary additive noise. Deterministic deep learning models have traditionally been used for SE, but recent studies have shown that generative approaches, such as denoising diffusion probabilistic models (DDPMs), can also be effective. However, incorporating condition information into DDPMs for SE remains a challenge. We propose a _model-agnostic_ method called DOSE that employs two efficient condition-augmentation techniques to address this challenge, based on two key insights: (1) We force the model to prioritize the condition factor when generating samples by training it with dropout operation; (2) We inject the condition information into the sampling process by providing an informative adaptive prior. Experiments demonstrate that our approach yields substantial improvements in high-quality and stable speech generation, consistency with the condition factor, and inference efficiency. Codes are publicly available at https://github.com/ICDM-UESTC/DOSE.

## 1 Introduction

Speech enhancement (SE) aims to improve the intelligibility and quality of speech, particularly in scenarios where degradation is caused by non-stationary additive noise. It has significant practical implications in various fields such as telecommunications , medicine , and entertainment . Modern deep learning models are often used to learn a deterministic mapping from noisy to clean speech. While deterministic models have long been regarded as more powerful in the field of SE, recent advancements in generative models  have significantly closed this gap.

One such generative approach is based on using denoising diffusion probabilistic models (DDPMs) , which have been shown to effectively synthesize natural-sounding speech. Several diffusion enhancement models have been developed , which try to learn a probability distribution over the data and then generate clean speech conditioned on the noisy input. A key challenge in using diffusion enhancement models is how to effectively incorporate condition information into learning and generating faithful speech . Previous works address this issue through designing specific condition-injecting strategies  or devising complex network architectures .

We conduct a thorough examination to understand the limitation of diffusion-based SE methods and find that diffusion enhancement models are susceptible to _condition collapse_, where the primary cause of inconsistent generation is the _non-dominant position of the condition factor_. We thus introduce a new paradigm to effectively incorporate condition information into the diffusion enhancement models. Specifically, we propose a Diffusion-drOpout Speech Enhancement method (DOSE), which is a model-agnostic SE method (Figure 1) that employs two efficient condition-augmentation techniques: (1) During training, we randomly drop out intermediate-generated samples. This dropout mechanism guides the model's attention toward the condition factors; (2) Instead of letting the model generate samples from scratch (Gaussian distribution), we employ an adaptive prior derived from the conditional factor to generate samples. Experiments on benchmark datasets demonstrate that our method surpasses recent diffusion enhancement models in terms of both accuracy and efficiency. Additionally, DOSE produces more natural-sounding speech and exhibits stronger generalization capabilities compared to deterministic mapping-based methods using the same network architecture.

## 2 Related works

There are two main categories of diffusion-based SE methods: (1) designing specific conditioning strategies [4; 9; 11; 5], or (2) generating speech with an auxiliary condition optimizer[12; 10; 8]. The first category considerates noisy speech in the diffusion (or reverse) process, either by linearly interpolating between clean and noisy speech along the process [4; 9], or by defining such a transformation within the drift term of a stochastic differential equation (SDE) [11; 5].

Works from the second category rely on an auxiliary condition optimizer - a generator (diffusion model) synthesizes clean speech and a condition optimizer informs what to generate [12; 10; 8]. Both the generator and condition optimizer have the ability to denoise, with the latter undertaking the core part. Given the challenges in leveraging condition information , diffusion-based SE methods within this category often necessitate specific network architecture design to guarantee the participation of condition factors.

In a paradigm sense, our method is quite similar but different to the second branch - unlike previous approaches that require additional auxiliary networks, DOSE is an end-to-end diffusion-based SE method. In addition, DOSE is model-agnostic that does not need any specific network design to guarantee consistency between the generated sample and its corresponding condition factor.

## 3 Preliminaries

We now provide a brief introduction to the diffusion probabilistic model (diffusion models, for short), the definition of speech enhancement, and the condition collapse problem.

### Diffusion models

A diffusion model [13; 6] consists of a forward (or, diffusion) process and a reverse process. Given a data point \(_{0}\) with probability distribution \(p(_{0})\), the forward process gradually destroys its data structure by repeated application of the following Markov diffusion kernel:

\[p(_{t}|_{t-1})=(_{t};}_{t- 1},_{t}), t\{1,2,,T\},\] (1)

where \(_{1},,_{T}\) is a pre-defined noise variance schedule. With enough diffusion step \(T\), \(p(_{T})\) converges to the unit spherical Gaussian distribution. Based on the Markov chain, the marginal distribution at arbitrary timestep \(t\) has the following analytical form:

\[p(_{t}|_{0})=(_{t};_{t}} _{0},(1-_{t})), t\{1,2,,T\},\] (2)

where \(_{t}=_{s=1}^{t}(1-_{s})\).

Figure 1: An illustration of the proposed DOSE. DOSE consists two primary procedures: (1) training a condition diffusion model using dropout operation, and (2) generating speech using a conditional diffusion model equipped with the adaptive prior.

As for the reverse process, it aims to learn a transition kernel from \(_{t}\) to \(_{t-1}\), which is defined as the following Gaussian distribution :

\[p_{}(_{t-1}|_{t})=(_{t-1};_{ }(_{t},t),(_{t},t)),\] (3)

where \(\) is the learnable parameter and \(_{}(_{t},t)=}}(_{t}- }{}}_{}(_{t},t))\) denotes the mean of \(_{t-1}\), which is obtained by subtracting the estimated Gaussian noise \(_{}(_{t},t)\) in the \(_{t}\). With such a learned transition kernel, one can approximate the data distribution \(p(_{0})\) via:

\[p_{}(_{0})= p_{}(_{T})_{t=1}^{T}p_ {}(_{t-1}|_{t})d_{1:T},\] (4)

where \(p_{}(_{T})=(_{T};,)\).

### Problem formulation

Speech enhancement refers to methods that try to reduce distortions, make speech sounds more pleasant, and improve intelligibility. In real environments, the monaural noisy speech \(\) in the time domain can be modeled as:

\[=+\] (5)

where \(\) and \(\) denote clean and noise signals, respectively. For human perception, the primary goal of speech enhancement is to extract \(\) from \(\). Mapping-based speech enhancement methods directly optimize \(p_{}(|)\), while diffusion enhancement methods generate clean samples through a Markov process \(p_{}(_{0:T-1}|_{1:T},)\).

### Condition Collapse in diffusion enhancement models

The _condition collapse_ problem in speech enhancement was first proposed in  and it refers to the limited involvement of the condition factor during conditional diffusion training, resulting in inconsistencies between the generated speech and its condition factor.

In this work, we argue that the condition factor \(\) indeed participates and helps the intermediate-generated sample \(_{t}\) approximate \(p(_{t-1}|_{t},_{0})\). Our assertion is supported by the experiment depicted in the left part of Figure 2 - the diffusion model equipped with the condition factor exhibits a lower loss curve compared to the unconditional one2. To better understand the condition collapse phenomenon, we devise two variants that explicitly modify the mutual information between the condition factor and the model's output (Figure 2 (middle)). We use skip connections to add the condition factor to multiple layers, forcing the likelihood of maintaining a strong connection between the condition factor and output features. Since the dependence of the output on any hidden state in the hierarchy becomes weaker as one moves further away from the output in that hierarchy (cf. ), using skip connections can explicitly enhance connections between the generated sample and condition factor.

Figure 2: Investigation of the condition collapse problem. From left to right: (1) comparison of loss curves between unconditional and conditional diffusion models; (2) three variants; (3) PESQ performance of different variants, (-) represent the unprocessed speech.

As shown in Figure 2 (right), an increase in mutual information (connections) leads to a significant improvement in the consistency between the generated sample and the condition factor (\(a b\)). However, it requires a meticulously designed model to guarantee its effectiveness (\(b c\)). While previous studies [5; 10; 8] focus on explicitly enhancing the consistency between the output speech and condition factor through specific network architecture design, we explore the possibility of a solution independent of the model architecture. This would broaden the applicability of our method, as it enables slight modifications to existing deterministic mapping-based models to transform them into diffusion enhancement models.

## 4 Methodology

Considering the diffusion model provides a transition function from \(_{t}\) to \(_{t-1}\), typical condition generation process is represented as:

\[p_{}(_{0}|)=_{T})}_{} _{t=1}^{T}}(_{t-1}|_{t},)}_{ }d_{1:T},_{T}(_{T};,).\] (6)

Our experiments above indicate that \(p_{}(_{t-1}|_{t},)\) will easily collapse to \(p_{}(_{t-1}|_{t})\), resulting in the condition generation process degenerating into a vanilla unconditional process:

\[ p_{}(_{T})_{t=1}^{T}p_{}(_{t-1}| _{t},y)d_{1:T} p_{}(_{T})_{t =1}^{T}p_{}(_{t-1}|_{t})d_{1:T}.\] (7)

As a result, facilitating automatic learning of the joint distribution for both clean and noisy speech samples does not work well for the speech enhancement task.

### Condition augmentation I: Adaptive Prior

Let's revisit Eq. (6): since we cannot easily inject the condition factor into the condition term, how about the prior term? For example, we can modify the condition generation process as:

\[p_{}(_{0}|)=_{}|)}_{ }_{t=1}^{}}(_{t-1}| _{t})}_{}d_{1:},\] (8)

where \(p(_{}|)\) is formulated as \(p(_{}|)=(_{};}, (1-_{}))\). The following propositions verify the feasibility of our proposal.

**Proposition 1**.: _For any \(>0\) such that \(0<<M\) for some finite positive value \(M\), there exists a positive value \(\{0,,T\}\) that satisfies:_

\[D_{KL}(p(_{t}|)\|p(_{t}|)),  t T,\] (9)

_where \(p(_{t}|)=(_{t};_{t}},(1- _{t}))\)._

**Remark 1**.: _This proposition indicates that, given a tolerable margin of error \(\) and a well-trained diffusion model, we can always find a suitable \(\) such that we are able to recover the clean speech \(\) from its noisy one \(\) using Eq. (8)._

While **Proposition 1** allows us to generate clean speech \(\) given the noisy speech \(\) using Eq. (8), it does not guarantee that our model will achieve successful recovery with a high probability.

**Proposition 2**.: _Let \(\) be the clean sample, \(\) be it's corresponding noisy one, and \(^{}\) be any neighbor from the neighbor set \(()\). Then diffusion enhancement models can recover \(\) with a high probability if the following inequality is satisfied:_

\[()}{p(^{})})>^{2 }}(\|-\|_{2}^{2}-\|^{}-\|_{2}^{2}), ^{}(),\] (10)

_where \(_{t}^{2}=_{t}}{_{t}}\)._

**Remark 2**.: _Assuming that the condition factor \(\) is closer to \(\) than to \(^{}\), we obtain a non-positive right-hand side (RHS). For a given \(\), the left-hand side (LHS) value is fixed, and to ensure the inequality always holds, a smaller \(_{t}^{2}\) is preferred._As shown in Figure 3, \(_{t}^{2}\) will increase as the timestep \(t\) increases. Thus, according to **Proposition 2**, we should choose a small \(\) for Eq. (8) to maximize the probability of successfully recovering the clean speech from the noisy one. However, constrained by **Proposition 1**, \(\) cannot be too small. In other words, the clean speech distribution \(p(_{})\) and the noisy speech distribution \(p(_{})\) will get closer over the forward diffusion process, and the gap \(||=|-|\) between the noisy speech and the clean one will indeed be "washed out" by the increasingly added noise. Since the original semantic information will also be removed if \(\) is too large, there should be a trade-off when we set \(\) for the diffusion enhancement model.

**Condition optimizer.** We find that both propositions are correlated with the condition factor \(\). If we can reduce the gap between the condition factor and clean speech, we can choose a smaller \(\), effectively increasing the likelihood of recovering clean speech. One simple idea is to employ a neural network \(f_{}\) to optimize the condition factor, as demonstrated in . Accordingly, we can rewrite Eq. (8) as:

\[p_{,}(_{0}|)= p_{}(_{} |)_{t=1}^{}p_{}(_{t-1}|_{t})d_{1: },\] (11)

where \(p_{}(_{}|)=(_{};_{}}f_{}(),(1-_{}))\).

In practice, we should also consider failure cases of the condition optimizer in complex scenarios, especially the issue of excessive suppression that has been reported in recent literature [16; 17; 18]. To mitigate this issue, we use \(0.5+0.5\) (like a simple residual layer) as a mild version of the condition factor:

\[p_{}(_{}|)=(_{};0.5_{}}(f_{}()+),(1-_{ })).\] (12)

We call \(p_{}(_{}|)\) the adaptive prior as it varies with different noisy samples \(\).

### Condition augmentation II: Diffusion Dropout

Aside from changing the prior \(p(_{T})\) to conditional prior \(p_{}(_{}|)\), we also optimize the condition term \(p_{}(_{t-1}|_{t},)\). Instead of designing specific condition-injecting strategies [4; 9; 11] or devising complicated network architecture [8; 10; 5], we attempt to "do subtraction" by discarding some shared (intermediate-generated samples & condition factor) and important (target-related) information from intermediate-generated samples. Naturally, if we discard some information from \(_{t}\), then the diffusion enhancement model is forced to use the condition factor \(\) to recover the speech. Taking a further step, we can even discard the entire \(_{t}\), as the condition factor \(\) alone is sufficient for recovering the clean speech \(_{0}\) (this is what deterministic models do). To this end, we define a neural network \(f_{}(d(_{t},p),,t)\) to approximate \(p(_{t-1}|_{t},_{0})\):

\[p_{}(_{t-1}|_{t},)=(_{t-1};f_{ }(d(_{t},p),,t),(_{t},t)),\] (13)

where \(d(_{t},p)\) is the dropout operation:

\[d(_{t},p)=_{t}&r=1\\ &r=0, r(1-p).\] (14)

### DOSE training

Ho et al.  and much of the following work choose to parameterize the denoising model through directly predicting \(\) with a neural network \(_{}(_{t},t)\), which implicitly sets:

\[}_{0}=_{t}}}(_{t}-_{t}}_{}).\] (15)

In this case, the training loss is also usually defined as the mean squared error in the \(\)-space \(\|-_{}(_{t},t)\|_{2}^{2}\). Although this standard specification works well for training an unconditional diffusion model, it is not suited for DOSE - for two reasons.

First, we cannot estimate \(\) without the help of \(_{t}\) because \(\) and \(\) are independent. Second, as discussed earlier, we want DOSE to start with a small timestep and we strive to make \(\) small. However, as \(\) approaches zero, small changes in \(\)-space have an increasingly amplified effect on the implied prediction in \(\)-space (Eq. (15)). In other words, the efforts made by diffusion enhancement models become so negligible that diffusion models lose their ability to calibrate the speech at small timesteps.

So, we need to ensure that the estimation of \(}_{0}\) remains flexible as the timestep \(t\) gets smaller. Considering the equivalently of the \(\)-space loss \(\|-_{}(_{t},t)\|_{2}^{2}\) to a weighted reconstruction loss in \(\)-space \(^{2}}\|_{0}-_{}(_{t},t)\|_{2 }^{2}\), we can directly estimate the clean speech \(_{0}\) at each timestep \(t\):

\[=_{_{0} p(),t\{1,,T\}}[\| _{0}-f_{}(d(_{t},p),,t)\|_{2}^{2}]\] (16)

### DOSE inference

After training, the ideal scenario is that \(p_{}(_{t-1}|_{t},)\) approximates \(p(_{t-1}|_{t},_{0})\) precisely, enabling us to generate clean speech using Eq. (6). However, when applied in practice, it is difficult to completely eliminate errors (both sample error and true error). If these errors are not effectively managed or corrected during the generation process, the quality of the generated samples may deteriorate, leading to artifacts, blurriness, etc . This issue is particularly pronounced when using diffusion models for fine-grained conditional generation tasks, as diffusion models require a large number of steps to generate samples, which will significantly reduce the consistency between the generated sample and its condition factor (see SS5.3, Figure 6).

The adaptive prior (Sec 4.1) provides an opportunity to address the error accumulation issue. Specifically, we can select a suitable \(\) smaller than \(T\), conditioned on an adaptive prior, and generate speech in fewer steps. We can extend Eq. 11 by transforming the unconditional diffusion enhancement model into a conditional one:

\[p_{,}(_{0}|y)= p_{}(_{}|)_{t=1}^{}p_{}(_{t-1}|_{t},)d_{1: },\] (17)

and the number of sampling steps is reduced from \(T\) to \(+1\).

Readers familiar with diffusion models may recall that the standard process repeatedly applies a "single-step" denoising operation \(_{t-1}=denoise(_{t};t)\) that aims to convert a noisy sample at some timestep \(t\) to a (slightly less) noisy sample at the previous timestep \(t-1\). In fact, each application of the one-step denoiser consists of two steps: (1) an estimation of the fully denoised sample \(_{0}\) from the current timestep \(t\), and (2) computing a (properly weighted, according to the diffusion model) average between this estimated denoised sample and the noisy sample at the previous timestep \(t-1\). Thus, instead of performing the entire \(\)-step diffusion process to denoise a sample, it is also possible to run \(denoise\) once and simply output the estimated sample in one shot . Accordingly, Eq. (17) can be further rewritten as:

\[p_{,}(_{0}|)= p_{}(_{}| )p_{}(_{0}|_{},)d_{}\] (18)

We can even achieve DOSE without the condition optimizer \(f_{}()\) - using conditional diffusion enhancement model instead. For example, we can generate clean speech via:

\[p_{}(_{0}|)= p_{}(}_{ _{2}}|_{_{1}},)p_{}(_{0}|}_{_{ 2}},)d}_{_{2}}d_{_{1}},\] (19)where \(_{1},_{2}\) (\(_{2}<_{1} T\)) are two pre-defined hyper-parameters. The motivation behind Eq. (19) is that, once we have trained a neural network \(f_{}(_{t},,t)\) that can accurately estimate \(_{0}\) (Eq. (16)), according to the theoretical analysis in Sec 4.1, we can first choose a suitable value for \(_{1}\) to ensure a relatively good approximation of \(_{0}\):

\[}_{0}=f_{}(_{_{1}},,_{1}) f_{ }(_{_{1}},,_{1})\] (20)

In the second step, once we have obtained a good condition factor, we can choose a smaller timestep \(_{2}<_{1}\) to get a better estimation of \(_{0}\) than \(}_{0}\) generated in the first step.

**Summary.** DOSE has three important benefits: (1) By dropping \(_{t}\) entirely, we make the condition factor \(\) the "protagonist", automatically enhancing the consistency between the generated sample and the condition factor. (2) By training the model with this modified training objective, DOSE can perform well not only on Gaussian noise (\(_{t}_{0}\)) but also on various types of non-Gaussian noise (\(_{0}\)). (3) DOSE is efficient (2 steps), faster than existing diffusion enhancement models.

## 5 Experiments

We compare DOSE with prevailing diffusion enhancement methods and deterministic mapping-based enhancement methods in SS5.1. We conduct a counterfactual verification to understand the intrinsic mechanism of DOSE in SS5.2. We show two visual cases of excessive suppression and error accumulation in SS5.3. While providing a self-contained version of our main results, we note that we also have additional quantitative observations reported in the Appendices. Specifically, we compare DOSE with other baselines via subjective evaluation (Appendix A.2); We investigate the significance of the proposed adaptive prior and explain why we need to use a mild version of the condition factor (Appendix A.3); We examine the effect of our new training objective and demonstrate the necessity of using it (Appendix A.4); We explain why we use two steps in speech generation (Appendix A.5); We provide parameter sensitivity experiments (Appendix A.6); We show plenty of visual cases of excessive suppression and error accumulation (Appendix A.7 and A.8). To help readers better understand our research, we include a discussion subsection in Appendix A.9. Specifically, we: (1) Analyze the reasons behind the superior generalizability of diffusion enhancement models compared to deterministic mapping-based models (from the robust training perspective); (2) Explain why we use 0.5 in the mild version of the condition factor; (3) Discuss the broader impacts of speech enhancement methods.

**Dataset and baselines.** Following previous works [4; 9; 8], we use the VoiceBank-DEMAND dataset [22; 23] for performance evaluations. To investigate the generalization ability of models, we use CHiME-4  as another test dataset following , i.e., the models are trained on VoiceBank-DEMAND and evaluated on CHiME-4. We compare our model with recent open-sourced diffusion enhancement models such as DiffuSE , CDifuSE , SGMSE , SGMSE+ , and DR-DiffuSE . Since the only difference between SGMSE+ and SGMSE is their network architecture, we compare our model with just one of them.

**Evaluation metrics.** We use the following metrics to evaluate SE performance: the perceptual evaluation of speech quality (PESQ) , short-time objective intelligibility (STOI) , segmental signal-to-noise ratio (SSNR), the mean opinion score (MOS) prediction of the speech signal distortion (CSIG) , the MOS prediction of the intrusiveness of background noise (CBAK)  and the MOS prediction of the overall effect (COVL) . Besides these metrics, we also design two MOS metrics (MOS and Similarity MOS) for subjective evaluation.

**Configurations.** To ensure a fair comparison, we keep the model architecture exactly the same as that of the DiffWave model  for all methods3. DiffWave takes 50 steps with the linearly spaced training noise schedule \(_{t}[1 10^{-4},0.035]\). We train all methods for 300,000 iterations using 1 NVIDIA RTX 3090 GPU with a batch size of 16 audios. We select the best values for \(_{1}\) and \(_{2}\) according to the performance on a validation dataset, a small subset (10%) extracted from the training data. More experiment settings can be found in Appendix A.10.

### Performance comparison

We compare our method with previous diffusion enhancement methods and summarize our experimental results in Table 1. We observe that: **(1)** Diffusion enhancement methods have better generalizability than deterministic methods. **(2)** Methods with specific condition-injecting strategies, such as DiffuSE, CDiffuSE, and SGMSE, have strong generalization but perform slightly worse than deterministic mapping-based methods in matched scenarios. **(3)** Method (DR-DiffuSE) with auxiliary condition optimizer, performs better in matched scenarios and shows a slight improvement in mismatched scenarios. **(4)** Our method performs well in both matched and mismatched scenarios and is on par with state-of-the-art diffusion enhancement models while requiring fewer steps.

### Counterfactual verification

We perform a counterfactual verification to gain insights into the underlying mechanism of DOSE. To verify whether dropout can increase the "discourse power" of the conditional factor, we keep the condition factor \(\) fixed and reverse the intermediate-generated speech at a specific step (\(reverse(_{t})\)). This reversed intermediate-generated speech is called a counterfactual sample. Notably, if the final generated speech is more similar to the condition factor than the counterfactual speech, we can conclude that the condition factor plays a dominant role in the generation process. Otherwise, we can say that the condition factor is less influential.

As shown in Figure 4, we compare the performance of two models with different dropout probabilities (0.1 vs. 0.9). We have two findings here: **(1)** A higher dropout probability encourages the model to prioritize the condition factor even with a small timestep \(t\). **(2)** When timestep \(t\) is large, DOSE ef

  
**Method** & **Year** & **Efficiency** & **Dataset** & **STOI(\%)\(\)** & **PESQ\(\)** & **CSIG\(\)** & **CBAK\(\)** & **COVL\(\)** \\  Unprocessed & – & – & & 92.1 & 1.97 & 3.35 & 2.44 & 2.63 \\ DiffWave & 2021 & 1 step (dis) & & **93.3** & 2.51 & 3.72 & 3.11 \\ DiffuSE & 2021 & 6 steps & & \(93.5^{+0.05}_{-0.05}\) & \(2.39^{+0.12}_{-0.01}\) & \(3.71^{+0.01}_{-0.01}\) & \(3.04^{+0.08}_{-0.01}\) & \(3.03^{+0.08}_{-0.02}\) \\ CDiffuSE & 2022 & 6 steps & VB & \(93.7^{+0.05}_{-0.05}\) & \(2.43^{+0.08}_{-0.01}\) & \(3.77^{+0.04}_{-0.01}\) & \(3.09^{+0.18}_{-0.01}\) & \(3.09^{+0.01}_{-0.01}\) \\ SGMSE & 2022 & 50 steps & & \(93.3^{+0.00}_{-0.01}\) & \(2.34^{+0.11}_{-0.01}\) & \(3.69^{+0.01}_{-0.01}\) & \(2.90^{+0.01}_{-0.01}\) & \(3.00^{+0.01}_{-0.01}\) \\ DR-DiffuSE & 2023 & 6 steps & & \(92.9^{+0.06}_{-0.06}\) & \(2.50^{+0.02}_{-0.01}\) & \(3.68^{+0.02}_{-0.02}\) & \(3.27^{+0.02}_{-0.01}\) & \(3.08^{+0.02}_{-0.02}\) \\ DOSE & – & 2 steps & & \(93.6^{+0.06}_{-0.05}\) & \(2.56^{+0.05}_{-0.01}\) & \(3.83^{+0.11}_{-0.01}\) & \(3.27^{+0.05}_{-0.01}\) & \(3.19^{+0.05}_{-0.01}\) \\   Unprocessed & – & – & & 71.5 & 1.21 & 2.18 & 1.97 & 1.62 \\ DiffWave & 2021 & 1 step (dis) & & 72.3 & 1.22 & 2.21 & 1.95 & 1.63 \\ DiffuSE & 2021 & 6 steps & & \(83.7^{+11.4}_{-0.05}\) & \(1.59^{+0.36}_{-0.01}\) & \(2.91^{+0.70}_{-0.01}\) & \(2.19^{+0.24}_{-0.01}\) & \(2.19^{+0.76}_{-0.01}\) \\ CDiffuSE & 2022 & 6 steps & CHIME-4 & \(82.8^{+10.5}_{-0.05}\) & \(1.58^{+0.36}_{-0.34}\) & \(2.88^{+0.47}_{-0.01}\) & \(2.15^{+0.01}_{-0.01}\) & \(2.18^{+0.01}_{-0.01}\) \\ SGMSE & 2022 & 50 steps & & \(84.5^{+0.05}_{-0.05}\) & \(1.57^{+0.02}_{-0.02}\) & \(2.92^{+0.01}_{-0.01}\) & \(2.18^{+0.02}_{-0.02}\) & \(2.18^{+0.01}_{-0.01}\) \\ DR-DiffuSE & 2023 & 6 steps & & \(77.6^{+0.06}_{-0.06}\) & \(1.29^{+0.07}_{-0.07}\) & \(2.40^{+0.19}_{-0.02}\) & \(2.04^{+0.06}_{-0.01}\) & \(1.78^{+0.15}_{-0.01}\) \\ DOSE & – & 2 steps & & \(86.6^{+0.05}_{-0.05}\) & \(1.52^{+0.04}_{-0.01}\) & \(2.71^{+0.01}_{-0.01}\) & \(2.15^{+0.01}_{-0.01}\) & \(2.06^{+0.03}_{-0.01}\) \\   

Table 1: Comparison of different diffusion enhancement methods.

Figure 4: Counterfactual visualization. The first two columns are associated with a dropout probability of 0.1, while the last two columns are associated with a dropout probability of 0.9. In each row, the blue waveforms in the first and third columns are the counterfactual samples, and the blue waveforms in the second and fourth columns are the normal samples. The orange waveforms are generated samples from the model.

fectively captures condition information, ensuring the model's robustness to noise and maintaining consistency in the early stages of inference.

### Excessive suppression & error accumulation

We provide a visual case of excessive suppression in Figure 5 and a visual case of error accumulation in Figure 6. From Figure 5, we can see that: **(1)** The deterministic model fails in mismatched scenarios and generates samples that lose speech details; **(2)** The diffusion enhancement model generate defective speech when directly using the estimated speech as the condition factor; **(3)** The diffusion enhancement model equipped with a mild version of the condition factor can recover clean speech effectively. From Figure 6, we notice that: **(1)** Full-step generation can remove noise and generate natural-sounding speech. However, it can't guarantee the consistency between the generated speech and condition factor; **(2)** Two-step speech generation with adaptive prior can promise consistency and high quality simultaneously.

## 6 Conclusions

In this work, we present a new approach DOSE that effectively incorporates condition information into diffusion models for speech enhancement. DOSE uses two efficient condition-augmentation techniques to address the condition collapse problem. Comprehensive experiments on benchmark datasets demonstrate the efficiency and effectiveness of our method.

In our method, there are two groups of hyper-parameters: the dropout probability \(p\) for the dropout operation and two timesteps \(_{1},_{2}\) for the adaptive prior. These parameters are critical to model performance. For example, if the dropout probability is set too high, the diffusion enhancement model will rely solely on the condition factor to estimate the speech. Then our diffusion enhancement model will degenerate into a deterministic model, losing its generalizability. We also need to make a trade-off when choosing the timestep \(\) (especially \(_{1}\)): On one hand, a large \(\) is needed to reduce the gap between the clean speech and condition factor. On the other hand, the original semantic information will also be removed if \(\) is set too large.

In practice, it is necessary to evaluate the model on a subset of data and then empirically set the hyperparameters. These manually defined hyper-parameters are selected based on the Empirical Risk Minimization (ERM) principle and may not be optimal for every individual sample. Thus, an important direction for future research is to develop methods that can adaptively choose hyper-parameters for different samples. It is also expected that the model can adaptively select appropriate coefficients when forming a mild version of the conditioning factor.

Figure 5: Excessive suppression visualization (unconditional diffusion enhancement model on CHIME-4). From left to right: (1) DiffWave (dis); (2) adaptive prior with the estimated condition; (3) adaptive prior with the mild condition; (4) clean speech.

Figure 6: Error accumulation visualization (VB, DOSE). From left to right: (1) noisy speech; (2) full (50) steps; (3) 2 steps; (4) clean.