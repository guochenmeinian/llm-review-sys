# Robust and Actively Secure

Serverless Collaborative Learning

Olive Franzese\({}^{*}\)\({}^{\,1}\), Adam Dziedzic\({}^{*}\)\({}^{1}\), Christopher A. Choquette-Choo\({}^{3}\),

Mark R. Thomas\({}^{2}\), Muhammad Ahmad Kaleem\({}^{2}\), Stephan Rabanser\({}^{2}\), Congyu Fang\({}^{2}\)

**Somesh Jha\({}^{3,4}\), Nicolas Papernot\({}^{2}\), Xiao Wang\({}^{1}\)**

\({}^{1}\)Northwestern University, \({}^{2}\)University of Toronto and Vector Institute, \({}^{3}\)Google,

\({}^{4}\)University of Wisconsin-Madison, \({}^{5}\)CISPA

Equal Contribution.Correspondence to: nicholasfranzese2026@u.northwestern.edu and adam.dziedzic@cispa.deProject Lead. Work done while the author was at the University of Toronto and Vector Institute.

###### Abstract

Collaborative machine learning (ML) is widely used to enable institutions to learn better models from distributed data. While collaborative approaches to learning intuitively protect user data, they remain vulnerable to either the server, the clients, or both, deviating from the protocol. Indeed, because the protocol is asymmetric, a malicious server can abuse its power to reconstruct client data points. Conversely, malicious clients can corrupt learning with malicious updates. Thus, both clients and servers require a guarantee when the other cannot be trusted to fully cooperate. In this work, we propose a peer-to-peer (P2P) learning scheme that is secure against malicious servers and robust to malicious clients. Our core contribution is a generic framework that transforms any (compatible) algorithm for robust aggregation of model updates to the setting where servers and clients can act maliciously. Finally, we demonstrate the computational efficiency of our approach even with 1-million parameter models trained by 100s of peers on standard datasets.

## 1 Introduction

To leverage data that is located across different clients, service providers increasingly resort to collaborative forms of distributed machine learning. Rather than centralize the data on a single _server_, data remains on the owner's (client's) device(s), which could be a consumer's phone or bank/hospital's local data center. Take the canonical example of federated learning (FL) . Rather than share data, clients instead send model updates to the server. _Our work caters to settings where neither clients nor servers can be entirely trusted to faithfully participate in the Collaborative Learning (CL) protocol._

For example, consider if a group of banks wished to learn a better fraud detection model. Banks may not be able to directly share data  and further because banking is a competitive industry, it must be assumed that banks will deviate from the protocol if it serves their interest. On one hand, malicious server banks may breach the intuitive confidentiality of CL. A long line of work  has shown that when the server acts maliciously, it can, for instance, construct model parameter values that exactly extract client data from (even aggregated) model updates. To protect client data from servers acting maliciously, it is thus paramount to design approaches to CL where no single server can have full control over the orchestration of the protocol. On the other hand, malicious client banks may entirely prevent learning by submitting poor updates. This may be intentional as in model poisoning attacks  or unintentional if their dataset contained malformed data. Though a separate line of work  has studied how to robustly learn in the face of malicious updates (or data), there are none that have studied how to integrate such robust learning algorithms within a protocol that is secure to malicious servers. _In this work, we design the first scheme that is robust to the harms of both malicious server(s) and clients, which are shown in Figure 1._We observe that asymmetric power is the fundamental requirement for malicious servers to breach user data privacy. Thus, we design a fully-decentralized peer-to-peer (P2P) learning protocol where each participant (e.g., bank), or _peer_ herein, can equally contribute to the role of the server aggregating updates (and of a client computing updates). Further, we ensure that no single peer has the power to orchestrate the protocol--instead, we elect a committee of peers to perform the aggregation at any given training round in a way that requires no central or trusted third party (see Section 3 for the full threat model). On the other hand, there is now a greater need for protection against malicious clients as the distributed nature may increase the chances of intentional poisoning or bad data quality interfering with learning (_e.g.,_ due to fewer resources among some banks and/or competitive advantages). Thus, we ensure that our protocol can efficiently integrate with classical approaches for robustness against malicious clients, such as RSA , FL Trust (FLT) , or Centered Clipping (CC) . Importantly, our work generalizes the setups of these works and introduces the general framework that adapts any (compatible) algorithm for robust aggregation of model updates to settings where servers and clients may behave maliciously.

To achieve this, our approach builds on cryptographic multi-party computation (MPC) protocols. This allows peers to collectively emulate the server's role while being robust against the collusion of a subset of these peers that may act maliciously. However, naively combining these with (insecure) robust aggregation techniques incurs prohibitive overhead because the server computation for robust aggregation, which must be securely computed in MPC, is almost always of a complexity that leads to a high multiplicative slowdown. We design a framework that modularizes the processing steps of robust aggregation so as to select the most suitable cryptographic building blocks for each one, leading to significant computational improvements. One such improvement is our proposed _computational surjectivity_. We show that aggregation algorithms with component functions satisfying this property can efficiently obtain security while still guaranteeing robustness against malicious peers; we also show that existing robustness algorithms satisfy this property, or can be tailored to do so.

To summarize, our contributions are the following:

1. We design the first collaborative learning protocol that operates under the malicious threat model and is robust to both malicious clients and servers. We provide a simulation-based proof of its cryptographic security.
2. We design our protocol as a generic compiler that can convert broad categories of robust aggregation algorithms to our improved security model efficiently. This modular approach enables practitioners obtain rigorous security guarantees while selecting the most appropriate model poisoning defense for their use case. To demonstrate our framework's flexibility, we generate malicious-secure protocols for three existing robust aggregation algorithms. We show empirically that the generated protocols retain their robustness guarantees.
3. We demonstrate the computational efficiency of our protocols. We benchmark our protocols up to 1 million parameter models, and thousands of peers. For example, we show that the aggregation step of our malicious-secure implementation of robust aggregation with RSA  obtains a per-round CPU time of roughly 46 seconds with \(10^{5}\) parameters when trained by \(1000\) peers.

## 2 Related Work

Federated learning is perhaps the most studied collaborative learning framework [28; 35]. Most related to ours are variants based on Secure Aggregation (SecAgg)  that provide confidentiality of gradient transmission. However, existing work does not provide robust aggregation within SecAgg and is focused on the single-server setting, or additionally on their use for tighter differential privacy guarantees [16; 27; 41; 47]. In contrast, we focus solely on confidentiality in the distributed server setting with robust aggregation. Other works include CaPC  but this requires a trusted third party to reduce the computational overhead. We make no such assumptions. In Swarm Peer-2-Peer learning , participants can dynamically join or leave the collaboration and are enrolled via a Blockchain smart contract. There is no central party and each per-round server is dynamically elected

Figure 1: **Motivation for P2P Learning**. Current collaborative learning approaches are vulnerable to both client (denoted as \(C\) with data \(D\)) and server attack vectors. Our framework tackles all of these vulnerabilities simultaneously.

via Blockchain smart contracts. Crucially, Swarm Learning supports neither secure (confidentiality-preserving) nor robust aggregation--it uses standard parameter averaging.

Biscotti  incorporates robustness to poisoning by combining Multi-Krum  and secure aggregation through Shamir secret-sharing. Its core parts are a verification committee that runs robust update selection, and aggregation committee that computes the final model update. However, Biscotti only guarantees security in the semi-honest setting and is solely compatible with Multi-Krum, which is not always the preferable robustness algorithm . Blockchain is also used as an alternative to the centralized aggregator in FL to deal with malicious participants or servers in . The initial model is uploaded on the blockchain following which the participants train local models, then sign on hashes with their private keys, and upload the locally trained models to the blockchain. The validity of the uploaded models is verified with digital signatures and Multi-Krum. Algorand is used as the consensus algorithm in the blockchain system to update the global model. However, it uses a single leader for each training round and is compatible only with Multi-Krum.

Konstantinov and Lampert  present a distributed robust learning procedure that allows for robust learning from untrusted sources. Distributed Robust Learning (DRL)  is another approach to robust learning which uses a divide and conquer strategy. However, none of the papers achieves the two notions of robustness at the same time. Closest to our work are those that look to combine data integrity and confidentiality (security) . However, these works are crucially different from ours in that they perform checks on the underlying data of each client, not the update--then, these protocols drop clients with poor data. Because these approaches operate over a different input, they may be used simultaneously with ours.

## 3 Threat Model

Collaborative learning is conducted among a set of parties (herein, _peers_) performing one of two roles: a _client_ (or _worker_) who performs learning on a local dataset, or a _server_ that aggregates the many client updates. Our protocol differs in two main ways: first, it is conducted among a set of peers (parties) which can perform either role, and second, the role of the server is performed by a subset of peers termed the _aggregation committee_. To align with prior literature, we sometimes refer to peers as clients or servers when they are performing those respective roles. We consider a malicious threat model where clients and servers may perform arbitrary adversarial actions to interfere with the protocol. Malicious behavior in the two roles may include, but is not limited to the following.

1. **Malicious Clients** may attempt to (1) lower the quality of the trained model by sending distorted model updates. This may take the form of both (a) intentional model poisoning attacks, and (b) unintentional problems such as errors in computation, and skewed or incorrect local data sets. They may also attempt to (2) steal information about the other peers' data, i.e. break confidentiality, e.g. by colluding with other malicious peers and sharing transcripts of the protocol execution.

    Property \\ Method \\  } &  Update \\ Confidentiality \\  &  Malicious \\ Clients \\  &  Malicious \\ Server \\  &  Aggregation \\ Committee \\  &  Robust \\ Aggregation \\  \\   Precurted \\ Attacks \\ Method \\  } &  Provision or \\ Insertion \\  } &  Gradient \\ Backdooring \\ \(\) \\  } &  Data \\ Reconstruction \\  } &  Malformed \\ Data \\ Data \\  } \\  & & & &  or & Data \\  & & & &  & \\   SecAgg vi  \\ SecAgg v2  \\ CAPC  \\  } & ✓ & ✗ & ✗ & ✗ & ✗ \\  & & ✗ & ✗ & ✗ & ✗ \\   & & ✗ & ✗ & ✓ & ✗ \\   & & ✗ & ✗ & ✓ & ✗ \\   & & & & \\   & & ✗ & ✗ & ✓ & ✗ \\   & & ✗ & ✗ & ✓ & ✗ \\   & & ✗ & ✓ & ✗ & ✗ \\   & & ✗ & ✗ & ✓ & ✓ \\   & & ✗ & ✓ & ✓ & ✓ \\   & & ✗ & ✓ & ✓ & ✓ \\   & & ✗ & ✓ & ✓ & ✓ \\   & & ✗ & ✓ & ✓ & ✓ \\   

Table 1: **Comparison of Security Models between Aggregation Protocols.** Robust aggregation provides protection against data poisoning by clients in the collaboration protocol. Update confidentiality guarantees that an individual updated from a client is not revealed. SHS denotes Semi-Honest Security while MS is Malicious Security. *Guarantees data interegregity, not robust aggregation of updates. **Only under a single robust aggregation protocol.

2. **Malicious Servers / Committee Members** may attempt to (1) reconstruct individual data points from the clients' updates, thus breaking data confidentiality, which can be achieved by arbitrarily modifying model parameters or colluding with other parties (Committee Members or Clients), (2) inappropriately change the shared model by e.g. omitting updates from selected clients, adding in bogus updates, or otherwise altering the global model updates.

We compose multiple cryptographic primitives, including secure committee election, verified secret sharing, distributed zero knowledge proofs, and secure multiparty computation. The assumptions and guarantees of the individual primitives are in Appendix B.2. _Importantly, their composition is secure under universal composability_. Our overall protocol operates under the standard assumptions of authenticated point-to-point secure channels between peers and a bounded proportion of adversarial peers (see Appendix B.1 for details). The following are the formal guarantees of our protocol.

* **Correctness of aggregation.** Given a publicly known update aggregation function \(F^{R}\) and that clients submit local updates \(x_{1},x_{2},,x_{n}\), the returned global update will be equal to \(F^{R}(x_{1},x_{2},,x_{n})\). See the following Section 4 for details.
* **Confidentiality of client updates.** During protocol execution, no peers gain information about any other's update \(x_{i}\) beyond what is implicitly revealed by the aggregate result \(F^{R}(x_{1},x_{2},,x_{n})\).
* **Robustness to poisoning.** An accurate model will be trained even if some subset of clients submit arbitrary poisonous updates. Our framework compiles existing robust aggregation algorithms into a stronger security model. Thus, the details of this guarantee depend on the underlying algorithm.
* **Malicious (active) security.** The above conditions hold even when a subset of parties actively perform arbitrary malicious behavior, including but not limited to: collusion between malicious peers, attempts to deviate from any part of the protocol, and submission of poisonous local updates.

**Problem Setup.** To construct a collaborative learning protocol that is robust against both malicious clients and servers, we must decentralize the task of update aggregation. Accordingly, P2P learning is conducted among a set of _peers_, who may be assigned the role of client or server.

## 4 Robust and Actively Secure Framework

Our framework efficiently lifts the robust aggregation algorithms (_e.g.,_ the aforementioned RSA, FLT, or CC) to the P2P learning setting with guaranteed malicious-secure (or, actively-secure) protocol fidelity. This security model guarantees both confidentiality and protocol fidelity against peers that may take arbitrary actions to disrupt the P2P learning protocol execution--fidelity is maintained by retaining the model fidelity guarantees of an underlying robust aggregation algorithm. Indeed, we previously mentioned that many algorithms provide model fidelity against poisonous adversaries in the single-server setting [7; 25; 29; 30; 32; 38]. Each algorithm makes different assumptions about the threat model, _e.g.,_ how many times a given malicious client can participate, what sort of malicious update they send, what the underlying data distribution is, etc. Thus, rather than pinning our framework on a single robustness algorithm, we propose a modular design that encompasses a broad class of such robust aggregation algorithms designed for the single-server setting.

### Framework Design

In order to strengthen the security models of a broad class of robust aggregation algorithms, we design a modular template (Figure 2), which organizes aggregation algorithms in terms of three functions:

\[F^{C}: U F^{P}:U  V F^{R}:V^{m}\]

The first function, \(F^{C}\), represents the computation of client updates based on local data, state, and global model parameters; accordingly, \(\) is the space of possible client datasets, \(\) is the space of local states, \(\) is the space of global model parameters, and \(U\) is the space of client updates. In the trusted single-server setting, each client computes \(F^{C}\) and sends their update \(_{i} U\) to the server. Next comes the server's computation. We break the server's work into two parts: a preprocessing function \(F^{P}\) and an aggregation function \(F^{R}\). The former transforms each client update to a preprocessed domain \(V\), and the latter combines the preprocessed local updates into a global model update \(\). Our primary contribution is the design of a protocol that lifts any robust aggregation algorithm described in terms of these functions to a stronger security model. The security model in question is secure against malicious/active clients _without relying on a trusted server_, all while retaining the protection against poisoning attacks offered by the original algorithm.

**Protocol Description.** Peers carrying out a P2P Learning protocol (Figure 3) begin by randomly selecting an aggregation committee, the size of which is parameterized to guarantee an honest majority with all but negligible probability (see Appendix B for details). Since the committee is honest-majority, it can use secure MPC (secure Multi-Party Computation) and VSS (Verifiable Secret Sharing) schemes later in the protocol. All clients then compute local updates via \(F^{C}\) and preprocess those updates via \(F^{P}\). Peers secret share their updates with VSS and pass shares to the aggregation committee. Each member of the committee receives a share of a local update from every peer. The committee uses distributed zero knowledge proofs to ensure that all updates are well-formed outputs of \(F^{P}\)--Section 4.2 discusses in detail how to do so with practical efficiency. Finally, \(F^{R}\) is computed by the aggregation committee by using the shares as input to a malicious-secure MPC protocol, and committee members send the resulting global model update to all peers. This protocol relies on users remaining online throughout, an assumption that may not always be practical. In Section 4.3, we discuss how to relax this.

**Strengthened Security Model.** In the single-server setting, the computation of \(F^{R}\) is handled by a single party. This makes it vulnerable to tampering - a malicious server may breach client confidentiality, omit updates from certain clients, modify updates, or simply make arbitrary changes to the global model. Our framework lifts aggregation algorithms to a security model where none of that is possible. Distributing the computation of \(F^{R}\) to an honest-majority committee equipped with malicious-secure MPC means that \(F^{R}\) is computed with guaranteed correctness and that no information about the local updates is leaked in the process. Further, using VSS guarantees that no committee member can breach the confidentiality of client updates before the computation of \(F^{R}\), and that it is (with high probability) impossible to modify client updates before the computation of \(F^{R}\) without being caught. Further, since the committee is majority-honest, all peers can guarantee the received global update is correct by taking the majority result received from the committee members.

**Obtaining Practical Efficiency.** It is possible to strengthen the security model of almost any distributed computation by simply running it inside of a generalized MPC protocol, but doing so usually results in unbearable computational overhead since MPC substantially amplifies the cost of most operations. A key challenge that we surmount is _strengthening security whilst maintaining the efficiency necessary to scale to real-world collaborative learning scenarios._ The design choices we employ while formulating our protocol make this possible. For example, in applications of robust aggregation with a trusted single-server, the role of the server is typically executed by a data center with high compute capabilities. In such a setting it is beneficial to minimize client-side computation and shift the compute responsibility to the server wherever possible.

In contrast, collaborative learning with no trusted parties requires a committee to aggregate client updates, and operations performed in MPC by the committee are especially _costly_. Thus it becomes

Figure 2: **Template for single-server robust aggregation.**

beneficial to offload as much of the computation as possible to the client-side. Our template (Figure 2) and protocol (Figure 3) do this by separating the trusted server's work into two parts, \(F^{P}\) and \(F^{R}\), and shifting the work of computing \(F^{P}\) to the _clients_. This dramatically reduces the computational burden of the aggregation committee, but introduces potential concerns about the correctness of the underlying aggregation algorithm. Namely, in the trusted server setting \(F^{P}\) is guaranteed to be computed correctly since it is executed by a trusted party, but a malicious client may introduce arbitrary faults into the computation of \(F^{P}\). To prevent this while maintaining confidentiality, one _could_ use a zero-knowledge proof to guarantee that \(F^{P}\) was computed correctly, however this would introduce substantial computational overhead. We achieve a much more efficient result by instead verifying that each peer's local update is _well-formed_--that it properly falls within the preprocessed domain \(V\). We observe that if \(F^{P}\) has a certain property, which we call _computational surjectivity_, verifying that the update is within \(V\) is just as good as verifying correct computation of \(F^{P}\), even though the former comes at substantially lower cost.

### Computational Surjectivity

Our key insight is that by leveraging the properties of robust aggregation, we can relax certain requirements on the correctness of \(F^{P}\). These relaxed requirements allow us to offload computation of \(F^{P}\) to the client-side, while also avoiding the computational overhead of a full zero-knowledge proof that \(F^{P}\) was computed correctly.

A robust aggregation algorithm guarantees that even when adversaries provide arbitrary values as the output of \(F^{C}\), a satisfactory output of \(F^{R}\) will be computed. Accordingly, we observe that as long as _some_ valid output of \(F^{C}\) maps to each client's output of \(F^{P}\), the final global update will be computed properly. Thus if \(F^{P}\) is a surjective function (i.e. if \( V, U:=F^{P}()\)), it is only necessary to verify that \(_{i} V\) for all client updates \(_{i}\) in order to correctly compute \(F^{R}\). Below we specify a computational analogue of surjectivity--we require the preimage can be found in polynomial time so the whole protocol can achieve simulation security (Appendix B.3 has details).

**Definition 1**: _A function \(f:U V\) is computationally surjective if there is a probabilistic polynomial-time algorithm \(:V U\) such that for any \(v V\), we have \(f((v))=v\)._

In general, we have no guarantees on the structure of \(F^{P}\) and so peers must prove in zero knowledge that \(_{i}\) is the result of a valid computation of \(F^{P}\) (Figure 3, step 5). But if \(F^{P}\) is computationally surjective, then all possible \(_{i} V\) are implicitly the output of some computation of \(F^{P}\). Thus, it only becomes necessary to prove that the shares of each peers' input reconstructs a point within \(V\).

Theorem 1 (proof in Appendix B.3) states the security of this protocol in the malicious setting.

Figure 3: **Main protocol outline for the malicious setting.**

**Theorem 1**: _For any single-server robust aggregation algorithm described in \((F^{C},F^{P},F^{R})\) as in Figure 2, the protocol described in Figure 3 is a secure P2P learning protocol against malicious clients and servers when the underlying MPC scheme is secure._

### Tolerating Peers Disconnecting

Peers cannot always be assumed to remain connected throughout an entire protocol execution, e.g., when peers are mobile devices . Further, it is also common to subsample a small portion of peers as clients to avoid high computation/communication costs . We show how to account for both of these practical settings with minimal modifications to our protocol.

**Tolerance to Users Dropping.** Our protocol includes two areas where peers must collaborate on the cryptographic protocol: the (client) work of computing updates and the (server / committee) work of aggregating updates. Our protocol already gracefully tolerates any number of clients dropping so long as the the pool of remaining clients meets the assumptions of the underlying robust aggregation algorithm. In this case our protocol's output would be just as if those peers did not participate. Our protocol can also tolerate committee member dropout _with no impact on the output of the protocol_ by proportionally increasing the committee size (due to the reconstruction guarantees of VSS). We find that this increase is often small even for substantial drop out rates. For example, to tolerate a \(10\%\) drop rate of honest committee members we need only increase the committee size from \(46\) to \(60\) (though this number depends on the algorithm, see Appendix B.1.1 for detailed analysis).

**Subsampling Clients.** This setting inherits the security of our original protocol as long as all honest peers agree on the selected subsample of clients in each round. This can be accomplished efficiently via secure coin flipping , e.g., before the protocol commences. Then, our protocol's computation is reduced proportionally to that of execution on the subsample.

## 5 Lifting Robust-Aggregation Algorithms to a Malicious-Security Model

Having discussed how a single-server robust aggregation with a computationally surjective \(F^{P}\) can be lifted to the malicious P2P setting with high efficiency, we apply this principle to the design of malicious-secure versions of three popular robust aggregation algorithms: robust stochastic aggregation (RSA) , centered clipping (CC) , and FLTrust (FLT)  in the P2P setting.

### Instantiating Robust Stochastic Aggregation (RSA) in our malicious-secure framework.

RSA is a lightweight algorithm for Byzantine-robust convex optimization  (see Appendix B.4.1 for a summary). We observe that it can be lifted to the malicious security model with high efficiency with very few modifications to the algorithm because it is computationally surjective (which we show formally in Appendix B) and the underlying MPC can be efficiently instantiated.

In RSA peer updates are the sign of the difference between each parameter of the local and global models. In other words, the \(F^{P}\) of RSA gives \(V=\{-1,1\}^{d}\), where \(d\) is the number of parameters in the model. Thus, it is sufficient for peers to prove in zero-knowledge that their updates are in the set \(V=\{-1,1\}^{d}\). This can be accomplished efficiently by having each peer represent their update as \(d\) shares of binary values. The committee can perform a distributed zero knowledge (DZK) proof that a shared \(x\) is binary-valued by constructing shares of \(x(1-x)\) and revealing it to be zero. These proofs can be included together for a substantial improvement in efficiency. In particular, for every shared value \(x_{i}\), parties uniformly sample a random value \(r_{i}\), and locally construct shares of the sum \( r_{i}(x_{i}(1-x_{i}))\). The parties then reconstruct the sum--if it is \(0\), then each of the \((x_{i}(1-x_{i}))\) components must have been \(0\) with all but negligible probability. For a more detailed treatment of this technique, see .

During the computation of \(F^{R}\), the committee needs only to sum the shares and send out the reconstructed sum. The actual value of the summed updates in \(\{-1,1\}\) is implicitly given by the sum of the binary values (if the sum of the binary values is \(x\), simply take \(2x-m\)).

### Instantiating Centered Clipping (CC) in our malicious-secure framework.

CC with momentum is a robust aggregation algorithm that ensures protection against time-coupled poisoning attacks  (see Appendix B for a summary). To lift it to our improved security model withpractical efficiency, we construct a computationally surjective variant of the CC algorithm. Namely, while canonical CC clips local updates using the \(_{2}\) norm, we use the \(_{}\) norm.4 In other words, we clip the gradients to a \(\)_-box_ rather than a \(\)_-ball_. This modification admits a computationally surjective \(F^{}\) with an efficient DZK proof that a client update is within the valid domain. In particular, we take \(V=[0,2^{}-1]^{d}\). Then in \(F^{P}\) we scale, round, and map clipped gradient updates to be within this domain. Here \(\) is a public constant large enough to limit discretization error of local updates during scaling--in experiments with CC we set \(\) to 32 in order to align with 32-bit fixed-point numbers. Smaller values of \(\) will increase protocol efficiency, at the expense of higher discretization error during rounding and mapping in \(F^{P}\) step 3. The computational surjectivity of this \(F^{P}\) follows from a similar argument to Lemma 2 (see Appendix B).

**DZK Proof of Valid Update.** We specify that local updates \(_{i}\) are submitted as vectors of the individual component bits of the processed gradient update. This means that each bit will be individually secret shared, which allows the committee to verify whether each one is binary-valued (using the same DZK technique described above for the RSA protocol). Since we scaled each update to fit within a \(2^{}\)-sized \(d\)-dimensional box, the \(d\) sets of \(\) binary values in the update trivially encode a point within the box. Thus, a proof that each component of the bitwise update is binary-valued equates to a proof that the update is in \(V\).

The global update is aggregated by summing the bits at each position of the client update vectors. The sums are reconstructed and sent directly to all clients. They implicitly encode the updated global parameters \(^{}\), which are recovered via client-side computation in order to keep the computation of \(F^{R}\) light-weight. Details of our malicious-secure Centered Box Clipping protocol can be found in Figure 8 (in Appendix).

### Instantiating FLTrust in Malicious-Secure Framework

FLTrust (FLT) is a robust aggregation algorithm that uses a trusted dataset to filter out poisoned updates  (see Appendix B for a summary). As with CC, we construct a tailored variant of FLT that admits a computationally surjective \(F^{P}\) to improve efficiency. In particular we rotate and scale the "root" update \(g_{0}\) to be a unit vector aligned with the x-axis. This allows us to take \(V\) to be the set of unit vectors in the half-space defined by a non-negative x-coordinate. As such, \(F^{P}\) involves scaling and rotating client updates so that the angle between them and \(g_{0}\) is preserved. Similarly to CC, we encode client updates as \(\)-bit fixed point numbers. In our benchmarks for FLT, we set \(\) to 16 to compensate for the increased memory demands of this protocol. We use a committee size of 121 in order to enable multiplication of secret shared values (see Appendix B for details).

**DZK Proof of Valid Update.** As in CC, the magnitudes of local updates \(_{i}\) are submitted as shares of each bit in the binary representation of each fixed-point number. Clients additionally submit shares encoding sign for each parameter, with the exception of the x-coordinate, which is assumed to be always non-negative. We use the previously described technique to verify that the shares encoding magnitude are binary-valued. We use a similar technique to verify that shares encoding sign are in the set \(\{-1,1\}\) (i.e. we reveal \((b+1)(b-1)\) to be zero using a batch check). Further, we verify that submitted updates are unit length by constructing shares of \(_{i},_{i}-C\), where \(C\) is the squared length of a unit vector represented as a \(\)-bit fixed-point number. Revealing this quantity to be zero verifies in zero-knowledge that \(_{i}\) was indeed unit length.

## 6 Verifying Empirical Efficacy and Efficiency

Our empirical evaluation focuses on exploring three major axes: (1) the Byzantine robustness of our implementations due to modifications we introduced, (2) the computational efficiency of our protocol, and (3) the tradeoff between computational efficiency and Byzantine robustness. To this end, we center our comparisons on robust stochastic aggregation (RSA), Centered Clipping (CC), and FLTrust (FLT) but remark that our framework is compatible with other (potentially future) Byzantine robust algorithms as well. We demonstrate the practical efficiency of our case studies in the P2P Learning framework while maintaining the same robustness of the algorithms as in their clear versions.

### Security Does not Impact Robustness

We verify if the properties of the robust aggregation algorithms hold after the required modifications to lift them to the malicious setting, e.g., switching to fixed point numerical precision. In Figure 4, we use the IID MNIST dataset and 20 peers, of which there are 10 malicious workers5. We compare the robustness of CC against the ALIE (A Little Is Enough) attack  before and after lowering CC's numerical precision. We observe that the algorithm preserves its robustness despite the required changes. We also present corresponding additional studies (e.g. comparison between \(_{2}\) and \(_{}\) norm for CC) in Appendix B. We observe that all the modified algorithms, namely CC, FLT, and RSA exhibit comparable performance to the original algorithms.

### Scaling of Computational Efficiency

Because P2P learning algorithms typically require upwards of \(1000\) rounds of the protocol to converge, it is a necessity to have an efficient protocol. In Figure 5, we analyze the two major factors influencing this: the size of the vector (ML model) being aggregated (denoted as the number of parameters), and the number of peers participating in the collaborative learning. We observe much better performance for RSA than other algorithms per training round. This results from a more concise form of the information exchanged between peers in the case of RSA, where local updates from each peer are represented as an array of bits. In contrast, model updates sent between peers in FLT or CC are always encoded as fixed points, 16 for FLT vs 32 for CC. The more efficient encoding in RSA provides a speedup of around \(\)30X in comparison to CC and \(\)6X over FLT. Our framework scales efficiently to even 5000 participants; we observe a linear growth in terms of the elapsed time per training round. Similarly, the computation time scales linearly for RSA, FLT, and CC, with the number of parameters. We further compare the communication cost between frameworks in Appendix B.

### End-to-end Protocol Evaluation in Presence of Attacks

We estimate the accuracy and runtime of the modified algorithms in the presence of different types of attacks in Figure 6. We compute the number of rounds to convergence, and use the per-round CPU

Figure 4: **Fixed vs floating-point numerical precision for CC.**

Figure 5: **Computational Efficiency vs Number of Parameters and Peers. We report CPU wall-clock time for the execution of the aggregation step of our protocol – the computation of \(F^{R}\) in a single training round. The runtime performance of the algorithms (RSA, FLT, and CC) scales linearly with the number of parameters and peers. When modifying parameters we use a total of \(100\) peers (left subfigure) and \(10^{5}\) parameters set when changing the number of peers (right subfigure). For RSA and CC, the aggregation committee size is set to \(46\), and for FLT it is set to \(121\) in order to accommodate the secret share multiplications of the protocol (see Appendix B for details).**time for computation of \(F^{R}\) in each algorithm, to estimate overall training runtime and accuracy for EMNIST (and similar results for MNIST in Appendix B). We plot the test accuracy (%) on the y-axis and the x-axis represents the estimated CPU time (measured in seconds, note that this is in the logarithmic scale) of the P2P training. We observe that in all cases, CC and FLT algorithms outperform RSA in terms of convergence speed and achieve higher final accuracy. Note that the overall convergence speed is decided by both the number of iterations of training and the cost of each iteration. Although RSA is faster to compute for one iteration due to reduced information exchanged in each iteration, it requires much more iterations than CC and FLT, and hence slower to converge. When considering only utility, CC also outperforms FLT consistently; however, under computation constraints, it is often the case that FLT is more efficient than CC. This is primarily because we use a fixed-point length (\(\)) of 16 bits in the experiments for FLT, but 32 bits for CC.

## 7 Limitations

We provided a reference implementation of our protocol for three popular robust aggregation algorithms, namely RSA, FTL, and CC. We hope that our framework will be easy to extend to future robust aggregation methods. We acknowledge that operating in the malicious threat model also increases the cost of computation, communication, and storage, in comparison to the fully trusted environment or an honest-but-curious threat model.

Our protocol is focused on confidentiality and security of the training protocol when combined with robustness. This is one component of privacy-preserving machine learning that is critical to preventing many attacks (as outlined in Table 1 and Section 2). However, this does not prevent the privacy leakage obtained via interactions with the final trained model. For this, differential privacy (DP) , in particular DP machine learning techniques [1; 3; 18; 19] are required. Incorporating these techniques within our framework is of interesting future work.

## 8 Conclusions

The benefits of collaborative learning make it an attractive new paradigm that is increasingly adopted in many domains, such as the financial sector to enable collaboration between banks. However, there are many risks associated with collaboration due to clients or server(s) being actively malicious. Malicious clients can submit corrupted updates which leads to the failure of creating a useful shared model. Conversely, the leakage of the client's local data when contributing model updates has been demonstrated to be particularly strong when a central party cannot be trusted to orchestrate the collaborative learning protocol. To mitigate these issues, we propose a Peer-to-Peer Learning protocol that is robust against malicious clients and server(s) to train a shared model _without_ a central party. We prove the cryptographic security of our protocol, providing the necessary security guarantees. Our novel framework is designed as a generic compiler that can efficiently convert robust aggregation algorithms to the P2P learning setting with the guaranteed malicious-secure protocol. We show empirically that the generated protocols retain their robustness guarantees. This generic approach can be applied to many (possibly future) aggregation algorithms.

Figure 6: **Byzantine Robustness of P2P Learning Protocols** for iid EMNIST. We compare RSA, FLT, and CC after their instantiations in our framework. A cohort size of 50 peers is used, of which there are 10 malicious workers. We consider four attacks and have a baseline without any malicious workers. We run each algorithm until its completion. CC achieves the highest final accuracy. FLT and CC converge much faster than RSA.