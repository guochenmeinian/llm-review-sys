# HHD-GP: Incorporating Helmholtz-Hodge Decomposition into Gaussian Processes for Learning Dynamical Systems

HHD-GP: Incorporating Helmholtz-Hodge Decomposition into Gaussian Processes for Learning Dynamical Systems

 Hao Xu\({}^{1,2}\), Jia Pan\({}^{1,2,3}\)

\({}^{1}\)The University of Hong Kong, Hong Kong, China

\({}^{2}\)Centre for Transformative Garment Production, Hong Kong, China

\({}^{3}\)LimX Dynamics

xuhaovic@connect.hku.hk, jpan@cs.hku.hk

Corresponding author.

###### Abstract

Machine learning models provide alternatives for efficiently recognizing complex patterns from data, but the main concern in applying them to modeling physical systems stems from their physics-agnostic design, leading to learning methods that lack interpretability, robustness, and data efficiency. This paper mitigates this concern by incorporating the Helmholtz-Hodge decomposition into a Gaussian process model, leading to a versatile framework that simultaneously learns the curl-free and divergence-free components of a dynamical system. Learning a predictive model in this form facilitates the exploitation of symmetry priors. In addition to improving predictive power, these priors make the model indentifiable, thus the identified features can be linked to comprehensible scientific properties of the system. We show that compared to baseline models, our model achieves better predictive performance on several benchmark dynamical systems while allowing physically meaningful decomposition of the systems from noisy and sparse data.

## 1 Introduction

A dynamical system describes how the state of a system evolves over time . Data-driven modeling of dynamical systems has become a fundamental task in many modern science and engineering applications, such as physical emulation  and robotics control . Mathematically, a dynamical system is often a set of first-order ordinary differential equations (ODEs) or, equivalently, a smooth vector field on a manifold . Given the functional form of ODEs, classical data-driven methods typically involve optimizing their parameters . However, for many complex systems it is practically difficult to determine the form of the ODEs governing their dynamics.

Recent advances in machine learning (ML) focus on the use of neural networks  and non-parametric Bayesian models [63; 27; 26] for the black-box approximation of vector fields. Despite the rich expressive power, the physics-agnostic design of these models hinders their performance when applied to physical systems. To address this issue, a popular approach is to develop ML models that incorporate strong physical priors as inductive biases. Such prior knowledge commonly stems from basic physical principles related to certain differential invariants of vector fields. For example, in scenarios of learning Hamiltonian systems [23; 67; 54] and incompressible fluid dynamics [73; 33], ML models are constructed to learn divergence-free (div-free) vector fields, as a consequence of conservation laws of energy or mass. By constraining the solution space, the powerful physical principles effectively improve the extrapolation performance of ML models and enhance their interpretability. However, enforcing the model's behavior to adhere to centain fundamental physical principles also restricts the applicability of the model. For example, a div-free vector field fails to describe a dynamical system with dissipation, but real-world dynamical systems always suffer from non-negligible dissipation.

To develop a predictive model covering more dynamical systems, we explore supplementing the div-free vector field with a curl-free vector field. This is inspired by the Helmholtz-Hodge decomposition (HHD) [3; 44; 8], which states that any sufficiently smooth vector field can be expressed as the sum of a curl-free vector field and a div-free vector field. HHD is widely used in the study of Navier-Stokes equations [45; 20; 9], but in this work we explore its connections with more general dynamical systems. For example, as shown in Fig. 1, HHD can be used to characterize the dynamics of a dissipative Hamiltonian system, where the div-free component represents its conservative dynamics and the curl-free component describes the friction-induced dissipation. In addition to dissipative Hamiltonian systems, HHD is also widely used to study chaotic systems and ocean dynamics. The connection between HHD and these dynamical systems is described in more detail in Appendix A.

In this work, we construct Gaussian process (GP) priors for div-free and curl-free dynamics separately, resulting in an additive GP model in the form of HHD for learning vector fields of dynamical systems. Our resulting HHD-GP allows one to leverage more prior knowledge than modeling the system as a whole. In particular, we investigate its potential in exploiting priors of symmetries, motivated by the observation that the div-free and curl-free components of a dynamical system usually exhibit more symmetry properties than the system itself.

For example, the damped mass-spring system in Fig. 1(a) exhibits odd symmetry, but its div-free (Fig. 1(b)) and curl-free (Fig. 1(c)) components additionally present rotation and translation symmetry, respectively. Therefore, we further build symmetry-preserving div-free GPs and curl-free GPs by exploiting closure of GPs under linear transformation. The symmetry prior not only improves the predictive performance of HHD-GP, but also makes it identifiable, thus identified div-free and curl-free features can be physically-meaningful. In particular, the learned div-free features are closely related to the energy of dynamical systems.

The main contributions of this work are summarized as:

* We introduce a GP prior (called HHD-GP) for learning the Helmholtz-Hodge decomposition of a general dynamical system.
* We construct a symmetry-preserving extension of the HHD-GP that can learn physically meaningful representations of dynamical systems.
* Experiments on several benchmark systems show that our model can both accurately predict their dynamics and HHDs from sparse, noisy observations.

## 2 Related work

Learning with div/curl-free constraintsDiv-free vector fields are a focal point of mathematical physics and have been well exploited by machine learning models to learn system dynamics, with the most well-known examples being neural networks (NNs) [23; 67] and GPs [54; 59] for learning Hamiltonian dynamics. All Hamiltonian vector fields are div-free, but not vice versa. To learn more general div-free vector fields as solutions of the continuity equation,  introduced an NN architecture to parameterize a universal representation of div-free vector fields. Based on the same representation, we constructed div-free kernels for GPs from matrix-valued kernels, which is an extension of the method of constructing div-free kernels from scalar kernels [50; 41; 75]. According to Maxwell's equations, div-free kernels were combined with curl-free kernels in [72; 71] to model magnetic fields. They assumed direct access to noisy observations of the div-free and curl-free components separately, whereas in this work, our goal is to recover the individual components from noisy observations of their sum. A similar prediction problem was studied in , they combined 2D

Figure 1: The vector field in phase space of a mass-spring system and its HHD.

div-free and curl-free GP priors in the form of HHD to reconstruct planar ocean current fields, and recovered their divergence. Their model has the same formulation as ours when the dimension of HHD-GP is two, but we further developed a symmetry-preserving extension of HHD-GP to solve its non-identifiability problem. Another similar work is Dissipative Hamiltonian neural network (D-HNN) , which compensated HNN  with a curl-free part to model both conservative and dissipative dynamics simultaneously, but D-HNN is only applicable to dissipative Hamiltonian systems due to its construction.

Learning with symmetrySymmetries are another important aspect of priors that can be incorporated into machine learning models. Motivated by the success of the translation-invariant NNs , network architectures with symmetries to more general transformations have been proposed, such as steerable CNNs [74; 10] and graph NNs [46; 61] equivariant to Euclidean symmetries. They achieved great success in improving generalization of the models. With the same motivation, kernel methods incorporating symmetries have also been developed. To make predictions invariant to transformations of inputs,  constructed kernels using Haar integration. And based on similar integration technique,  developed the Group Integration Matrix Kernel (GIM-kernel) to learn equivariant functions, which was later used by  to learn dynamical systems with symmetries. In this work, we constructed GP models to impose Euclidean symmetries to div/curl-free vector fields, which to the best of our knowledge has not been explored by the machine learning community.

## 3 Background

### HHD and Problem Setup

We consider an autonomous system governed by the following ODEs:

\[}(t):=(t)}{ t}=((t))=_{curl} ((t))+_{div}((t )),\] (1)

which defines a vector field by assigning a vector \(()^{n}\) to every state \(^{n}\). We assume that the vector field \( L^{2}(^{n},^{n})\) is smooth, and any such vector field can be decomposed into the sum of a curl-free vector field \(_{curl}:^{n}^{n}\) (\(_{curl}=,^{n}\)) and a divergence-free vector field \(_{div}:^{n}^{n}\) (\(_{div}=0,^{n}\)), according to the _Helmholtz-Hodge decomposition_ (HHD) [44; 8]. In this work, we are interested in learning, \(\), \(_{curl}\) and \(_{div}\) simultaneously from a collection of noisy observations denoted by \(=\{(_{i},_{i})\}_{i=1}^{m}\), with a noisy observation \(_{i}\) at a state \(_{i}\) given by

\[_{i}=(_{i})+, }{}(,),\] (2)

where an additive noise \(^{n}\) follows a zero-mean Gaussian distribution defined by a covariance matrix \(=(_{1}^{2},,_{n}^{2}) ^{n n}\) modeling noise variance in each output dimension.

### Vector-valued GP Model

We are interested in using Gaussian processes (GPs) to infer unknown vector fields. A GP is a stochastic process commonly used as a distribution for functions, assuming that any finite number of function values has a joint Gaussian distribution . To learn an unknown vector field \(:^{n}^{n}\), we assume a vector-valued GP prior as \(()(, (,^{}))\), where the mean of the function values is set to zero, and the covariance is captured by a matrix-valued kernel \(:^{n}^{n}^{n n}\), whose \((i,j)\)-th entry expresses the correlation between the \(i\)-th dimension of \(()\) and the \(j\)-th dimension of \((^{})\). In the GP framework, the kernel controls the properties of possible functions under a GP, leading to various efforts of problem-specific design of kernels [76; 16; 18]. And to be a valid covariance function, the kernel should be symmetric and positive semidefinite .

GPs provide a Bayesian non-parametric approach for solving regression tasks. According to the GP prior, function values at inputs \(=[_{1},,_{m}]^{}\) are jointly distributed as \((();,)\), where \(=[(_{i},_{j})^{n n}]_{i,j=1}^{m}\) is a block-partitioned covariance matrix. Then the marginal likelihood of the noisy observations \(=[_{1},,_{m}]^{}\) given by Eq. 2 can be calculated by

\[p()= p(, )p()d= (,_{*}),\] (3)where \(_{*}=+\), \(= I_{m}\) is a diagonal matrix whose elements are the variance of the observation noise. Training a GP model refers to maximizing the log of Eq. 3 to optimize the kernel parameters and the noise variance. Then by conditioning on these observations using Bayes' rule, the predictive posterior for a new state \(_{*}\) is still Gaussian with its mean \(\) and variance \(v\) given by

\[(_{*})=^{}_{*}^{-1} ,\ v(_{*})=(_{*},_{*})-^{}_{*}^{-1},\] (4)

where \(=[(_{1},_{*}),, (_{m},_{*})]^{}^{mn n}\). The derived mean function is used for regression results, and the associated variance quantifies prediction uncertainty. Due to their nonparametric nature, GPs can self-adapt to the complexity of the target function based on the data provided, without being restricted to specific parametric forms.

## 4 HHD-GP Model

We consider the problem of learning a continuous-time dynamical model in the form of HHD (Eq. 1) with GPs. HHD points out the prevalence of an additive structure in dynamical systems, so the key idea here is to exploit two GPs to model \(_{curl}\) and \(_{div}\), respectively, \(_{curl}(,_{curl})\), \(_{div}(,_{div})\). Then the sum of these two GPs results in a new GP modeling the dynamical system \(\), with a new kernel function defined as the sum of the curl-free and divergence-free ones. And the additive kernel \(=_{curl}+_{div}\) inherits the symmetric and positive-semidefinite properties of \(_{curl}\) and \(_{div}\), so the GP predictor (Eq. 4) is valid for the additive GP model. And the additivity of the kernels implies the additivity of GP means, so the mean function \(\) in Eq. 4 can be split into a curl-free part \(_{curl}\) and a divergence-free part \(_{div}\), and we have

\[=_{curl}+_{div}=_{curl}^{}_{*}^{-1} +_{div}^{}_{*}^{-1},\] (5)

where \(_{*}=_{curl}+_{div}+\). It can be seen that the effects of \(_{curl}\) and \(_{div}\) can be treated as observation noises for each other, so their prediction variances at \(_{*}\) are obtained by

\[v_{curl}(_{*})=_{curl}(_{*}, _{*})-_{curl}^{}_{*}^{-1} _{curl},\ v_{div}(_{*})=_{div}( _{*},_{*})-_{div}^{}_ {*}^{-1}_{div}.\] (6)

Consequently, observations of \(()\) given by Eq. 2 can be used to make predictions for its hidden components \(_{curl}\) and \(_{div}\). So we then construct GPs with realizations in the space of curl-free and div-free vector fields.

In the following parts, we construct the kernels \(_{curl}\) and \(_{div}\) from the representations of \(_{curl}\) and \(_{div}\) respectively, exploiting closure of GPs under linear transformation. Let \(R=[_{x_{1}},,_{x_{n}}]\) be the polynomial ring in the partial derivatives, and \( R^{a b}\) be a matrix of differential operators acting on functions \(g:^{n}^{b}\) distributed as \(((),(,^{}))\). Then, the transformation of \(g\) under \(\) is again distributed as a GP with

\[g((), _{}(,^{}) _{^{}}^{}:^{n} ^{n}^{a a}),\] (7)

where \(_{}\) and \(_{^{}}\) denote the operation of \(\) on the first and second argument of \((,^{})\), respectively . To make \(_{}(,^{}) _{^{}}^{}\) a valid covariance function, its underlying kernel \((,^{})\) must be twice differentiable in \(^{n}\). See Appendix B for details on linear operations on GPs.

### Curl-free Kernel

The gradient operator \((:=[_{x_{1}},,_{x_{n}}]^{} R^{n 1})\) defines a surjective mapping from the space of smooth scalar fields to the space of curl-free vector fields , so \(_{curl}\) can be represented by \(_{curl}= V\), where \(V C^{}(^{n},)\) is called the scalar potential of \(_{curl}\). Since the gradient operation defines a linear transformation, if a GP with a scalar kernel \(_{V}\) is assumed on \(V\), the distribution of \(_{curl}\) is again a GP. According to Eq. 7, the curl-free GP over \(_{curl}\) is given by

\[_{curl}(,_{curl}=_{ }_{V}(,^{})_{ ^{}}^{}),\] (8)

where \(_{curl}:^{n}^{n}^{n n}\) is a matrix-valued kernel constructed by the Hessian of the scalar kernel \(_{V}\), consisting of all second-order partial derivatives in \(\) and \(^{}\), with the entry in the \(i\)-th row and \(j\)-th column given by

\[[_{curl}(,^{})]_{i,j}= [ V()/ x_{i},  V(^{})/ x_{j}^{}]= ^{2}_{V}(,^{})/ x_{i}  x_{j}^{}.\]

By this construction, if \(_{V}\) induces a GP with realizations dense in \(C^{}(^{n},)\), the set of realizations of \((,_{curl})\) is dense in the space of curl-free vector fields, because a surjective mapping maps dense sets to dense sets.

### Divergence-free Kernel

A div-free vector field can be constructed from a skew-symmetric matrix field [5; 32; 57]. Specifically, let \(:^{n}^{n n}\) be a skew-symmetric matrix-valued function, then a div-free vector field \(_{div}\) can be represented by taking row-wise divergence of \(\), \(i.e.\),

\[_{div}=[_{1},, _{n}]^{},\] (9)

where \(_{i}:^{n}^{n}\) is the \(i\)-th row of \(\). The skew-symmetric matrix field \(\) of size \(n n\) can be compactly represented by \(m=n(n-1)/2^{2}\) scalar functions \(u_{ij} C^{}(^{n},)\):

\[=0&u_{12}&&u_{1n}\\ -u_{12}&0&&u_{2n}\\ &&&\\ -u_{1n}&-u_{2n}&&0=_{i=1}^{n-1}_{j=i+1}^{n}_{ ij}u_{ij},\] (10)

where \(_{ij}^{n n}\) is a matrix with its \((i,j)\)-th entry equal to 1, \((j,i)\)-th entry equal to -1, and all other entries equal to 0. Then, the div-free vector field given by Eq. 9 can be reformulated as a linear transformation:

\[_{div}()=_{i=1}^{n-1}_{j=i+1}^{n}_ {ij}u_{ij}()=(),\] (11)

where \(_{ij}=_{ij} R^{n 1}\) is a column vector obtained by linearly transforming the gradient operator. The \(m\) column vectors \(_{ij}\) are aggregated in a matrix \( R^{n m}\), and the \(m\) corresponding scalar functions \(u_{ij}\) are collected in a vector-valued function \(:^{n}^{m}\). \([]\) is a matrix of linear differential operators, so to use a GP to model \(_{div}\), we can proceed by assuming a GP prior over \((,_{}:^{n} ^{n}^{m m})\), then based on the closure of GPs under linear transformation (Eq. 7), the GP prior over \(_{div}\) (Eq. 11) is constructed by

\[_{div}(,_{}_{ }(,^{})_{^{ }}^{}),\] (12)

where \(_{}\) is a scalar-valued kernel for two dimensional systems (\(n=2\), \(m=1\)), and is a matrix-valued kernel for \(n>2\). Notice that this div-free kernel can be treated as a generalization of the div-free kernel derived from a scalar kernel [50; 41; 75], which can be recovered by setting \(_{}= I\). Therefore, it is more expressive and flexible. Theoretically, the GP model given by Eq. 12 can be used to approximate arbitrary div-free vector fields, because the representation given by Eq. 9 is shown to be maximally expressive (\(i.e.\) universal) in .

### Identifiability and Constraints

With the curl-free and div-free kernels, our objective is to learn physically interpretable representations of a dynamical system based on the HHD-GP model. However, the HHD is always not unique due to the existence of harmonic components \(_{harm}\) (vector fields satisfying both \(_{harm}=\) and \(_{harm}=0\), \(e.g.\), constant vector fields). For the HHD of a dynamical system with the true functional decomposition \(_{curl}^{*}\) and \(_{div}^{*}\),

\[=(_{curl}^{*}+_{harm})+(_{div}^{*}-_{harm})\] (13)

is a valid HHD for arbitrary \(_{harm}\), which thus makes the HHD-GP model non-identifiable, meaning that from the same training data, we may learn different decompositions giving the same predictions. This is not desirable because we expect the learned dynamical model to be interpretable: the curl-free and div-free components \(_{curl}\), \(_{div}\) are physically meaningful.

To mitigate the identifiability problem in additive regression models, an effective method is to impose constraints on their component models [16; 17; 47; 42]. The imposed constraints can affect the decomposition results of the additive models. Therefore, to ensure that HHD-GP can produce a scientific decomposition, we desire constraints that respect the inherent characteristics of dynamical systems. And, as another primary goal, incorporating prior knowledge of a system into a GP model can also improve its prediction accuracy and learning efficiency. Therefore, in the next section, we present how to impose symmetry-based constraints on the curl-free and div-free GP models.

Symmetry Constraints

### Equivariance and Invariance

Symmetry is a fundamental geometric property prevalent in dynamical systems in natural , and is usually described by the concept of equivariance and invariance:

**Definition 5.1** (Equivariance and Invariance).: Let \(\) be a group acting on \(^{n}\) through a smooth map \(L:^{n}^{n}\). The dynamical system \(:^{n}^{n}\) is said to be \(\)-equivariant if

\[( L_{g})()=_{L_{g}} ()(), ^{n},g,\] (14)

where \(L_{g}():=L(g,)\), and \(_{L_{g}}\) denotes the Jacobian matrix of \(L_{g}\). Then, \(\) is termed the symmetry group of the dynamical system. In particular, if \(_{L_{g}}\) is the identity matrix (\(i.e., L_{g}=, g\)), the dynamical system \(\) is said to be \(\)-invariant.

From the equivariance condition (Eq. 14) of the vector field, it follows the system's trajectory commutes with the action map. For vector fields on \(^{n}\), the symmetry group \(\) is commonly a subgroup of the Euclidean group \(E(n)\), which comprises all intuitive geometric transformations in \(^{n}\) (see Appendix C for a brief introduction). The symmetry constraints refer to that we expect the learned curl-free and div-free vector fields to be \(\)-equivariant. With the representation of their GP models, we demonstrate that the symmetries can be enforced by designing suitable kernels.

### Symmetry-preserving Curl-free GP

The curl-free GP (Eq. 8) is constructed by transforming another GP over a potential function, implying that we can impose constraints of symmetry on the curl-free GP by designing a suitable potential GP. Therefore, we start by exploring how to construct potential functions to obtain curl-free vector fields with the desired equivariance. As expected, the following theorem holds:

**Theorem 5.2**.: _Let \(\) be a Euclidean group or its subgroup, and let \(V:^{n}\) be a \(\)-invariant scalar function. Then, the curl-free vector field \(_{curl}:^{n}^{n}\) defined by \(_{curl}()= V()\) is \(\)-equivariant._

See Appendix D.1 for the proof. Theorem 5.2 shows that a \(\)-invariant scalar potential \(V\) can yield a \(\)-equivariant gradient field, indicating that if any realization \(V\) of \((0,_{V})\) is constrained to be \(\)-invariant, then its pushforward GP over \( V(,_{}_{V} _{}^{})\) can induce the space of \(\)-equivariant curl-free vector fields.

It is obvious that a \(\)-invariant scalar potential \(V\) can be constructed by integrating some non-invariant function \(h:^{n}\) over the symmetry group: \(V=_{}(h L_{g})dg\), where the measure \(dg\) is called _Haar measure_, which exists for locally compact topological groups and finite groups. Therefore, by assuming that \(h\) is distributed as \(h(0,_{h})\), we can construct the GP prior over the \(\)-invariant scalar potential as \(V(0,_{V})\), with its kernel \(_{V}\) given by

\[_{V}=[_{}h(L_{g}( ))dg,_{}h(L_{g}(^{} ))dg]=_{}_{}_{h}(L _{g}(),L_{g^{}}(^{}) )dgdg^{}.\] (15)

This kernel is called the _Haar-integration kernel_. While it provides a general method for constructing kernels for \(\)-invariant functions, the double integral can be computationally expensive. If the kernel \(_{h}\) is invariant to any \(g\) in the sense that \((,^{})=(L_{g}( ),L_{g}(^{}))\)3, a complexity reduction of Eq. 15 by one square-root can be performed by

\[_{V}=_{}_{}_{h}(,L_{g ^{-1}g^{}})dgdg^{}=||_{ }_{h}(,L_{g})dg,\] (16)

where \(||=_{}dg\), and it denotes the cardinality of \(\) when the group is finite.

### Symmetry-preserving Divergence-free GP

To incorporate the equivariance condition (Eq. 14) into realizations of the div-free GP (Eq. 12), we construct the skew-symmetric matrix field \(\) from a vector-valued function. Specifically, given a smooth vector field \(^{}(^{n},^{n})\), \(\) is constructed by \(=_{}-_{}^{}\), where \(_{}\) denotes the Jacobian of \(\) with its \((i,j)\)-th entry given by \( v_{i}/ x_{j}\). Then the component function \(u_{ij}\) in Eq. 10 is given by \(u_{ij}= v_{i}/ x_{j}- v_{j}/ x_{i}\). By this construction, the symmetry of the div-free vector field \(_{div}\) is governed by the symmetry of its vector potential \(\). In particular, a \(\)-equivariant \(\) can produce a \(\)-equivariant \(_{div}\), and it is formalized in the following theorem:

**Theorem 5.3**.: _Let \(\) be a Euclidean group or its subgroup, and let \(:^{n}^{n}\) be a \(\)-equivariant vector field. Then the divergence-free vector field \(_{div}=[_{1},, _{n}]^{}\) is \(\)-equivariant, where \(_{i}\) denotes the \(i\)-th row of the skew-symmetric matrix-valued function \(=_{}-_{}^{}\)._

See Appendix D.2 for the proof. By this theorem, we then proceed by assuming a GP prior over the vector potential \((,_{})\), and to constrain \(\) to be \(\)-equivariant, we build its kernel \(_{}^{n n}\) in the form of the _Group Integration Matrix kernel_ (GIM-kernel) [56; 55], which is constructed by:

\[_{}(,^{})=_{ }(,L_{g}(^{}) )_{L_{g}}dg,\] (17)

where \(\) is some arbitrary scalar-valued kernel satisfying \((,^{})=(L_{g}( ),L_{g}(^{}))\) for all \(g\). The GIM-kernel spans a Reproducing Kernel Hilbert Space (RKHS) of functions with the desired equivariance . So we can then use \(_{}\) (Eq. 17) to construct the GP prior over \(\) in Eq. 11, where the covariance between components \(u_{ij}\) and \(u_{kq}\) is given by

\[[_{}]_{ij,kq} =[u_{ij}=}{ x_{j}}- }{ x_{i}},u_{kq}=}{ x _{q}}-}{ x_{k}}]\] \[=}{ x_{j} x_{q}^{}}[ _{}]_{i,k}+}{ x_{i} x _{k}^{}}[_{}]_{j,q}-}{  x_{j} x_{k}^{}}[_{}]_{i,q}- }{ x_{i} x_{q}^{}}[_{ }]_{j,k}.\] (18)

Finally, this matrix-valued kernel \(_{}\) is transformed by Eq. 12 to construct the div-free GP, of which the realizations are guaranteed to be \(\)-equivariant div-free vector fields, according to Theorem 5.3.

## 6 Experiments

We evaluated the performance of our proposed method in several representative physical systems. Through the experiments, we found that our model can not only accurately capture the system dynamics, but also predict correct decompositions.

### Learning Dissipative Hamiltonian Dynamics

We first evaluated our method on a damped mass-spring system and a damped pendulum. Their governing equations are detailed in Appendix E.1. We generated the training data \(\{(,})\}\) by randomly sampling states \(\) in their phase space, and each of their derivative observations \(}\) is corrupted by an additive Gaussian noise with a standard deviation of \(0.05\).

The models are first evaluated in terms of learning ODEs. Specifically, our evaluation focused on the accuracy of the models in predicting state derivatives, as measured by the _root mean squared error_ (RMSE), and their ability to accurately predict state trajectories over time, as indicated by the _valid prediction time_ (VPT). In addition to these metrics for evaluating the regression results, we further use the _mean negative log likelihood_ (MNLL) to evaluate the prediction uncertainty provided by the GP models. These evaluation metrics and the generation of test data are detailed in Appendix F. We compared our models, HHD-GP and its symmetry-preserving extension, SPHHD-GP, with Dissipative Hamiltonian neural network (D-HNN) , GPs involving div-free kernels for learning Hamiltonian dynamics (Div-GP) [54; 59], and GPs with Group Integration Matrix Kernels (GIM-GP)  that can incorporate symmetries. Another baseline is GPs with independent kernels (Ind-GP), which model each dimension of a dynamical system with an independent scalar GP. See Appendix G for the implementation details of these models.

The results for \(20\) training data are shown in Table 1, and the results for an increasing number of training data are provided in Appendix J.1. The performance of Div-GP is limited because it can only model conservative dynamics. HHD-GP improves its performance by compensating with a curl-free kernel, which offsets the strong inductive bias imposed by the div-free kernel. And the performance of HHD-GP is better than that of another HHD-based model, D-HNN, because the low data efficiency of NNs makes it hard for D-HNN to capture dynamics using noisy and sparse training data, so actually the performance of D-HNN is worse than either of the GP methods. As another model without inductive bias, Ind-GP performs similarly to HHD-GP in most cases. Then, by incorporating symmetry priors into GPs, GIM-GP performs better than the above models but not as well as SPHHD-GP, because learning in the form of HHD allows the model to exploit more implicit symmetries in the dynamical systems. SPHHD-GP performs best overall in learning ODEs. Appendix J.2 presents the plots of trajectory predictions for each system.

Another advantage of our model is that it can decompose the dynamics into its div-free and curl-free components. According to Eq. 19, the div-free component can be used to recover the system's Hamiltonian, as long as the correct form of HHD is learned by our model. To show this, we evaluated our model by another task: we predicted the Hamiltonian \((_{t})\) (energy) along the system's trajectory \(\{_{t}\}\). For HHD-GP and SPHHD-GP, the Hamiltonian is predicted from a joint GP prior over the Hamiltonian and its underlying dynamics (see Appendix G.7.1 for details). Fig. 2 shows the predicted energy evolution of the two systems, where we can find that D-HNN and HHD-GP fail to provide physically plausible results, because the energy should continue to decrease due to friction, but they provide oscillating predictions, along with significant variances. In contrast, predictions of SPHHD-GP are highly accurate and closely aligned with the true values. One reason is that the symmetry priors used by SPHHD-GP improves the generalization performance of the model, but more importantly, the priors solve the problem of non-identifiability suffered by HHD-GP and D-HNN. See Appendix J.3 for an visualization of predicted decompositions, which shows that although the models capture similar system dynamics, they can learn completely different decompositions.

_Consistency_ is a necessary condition for a learning model to be identifiable and refers to the property that its parameter estimates should converge to the true values as the amount of data increases . Therefore, to further explore the non-identifiability problem, we provide the RMSE of energy prediction with an increasing number of training data in Fig. 3. The result shows that HHD-GP and D-HNN always generate significant errors in predicting energy evolution, meaning that the model parameters cannot converge as the amount of data increases. In contrast, the energy errors of SPHHD-GP exhibits a decreas

    &  &  \\   & RMSE \(\) & VPT \(\) & MNLL \(\) & RMSE \(\) & VPT \(\) & MNLL \(\) \\  D-HNN & 35.58 \(\) 6.08 & 0.67 \(\) 0.13 & N/A & 183.38 \(\) 27.92 & 0.39 \(\) 0.07 & N/A \\ Div-GP & 21.48 \(\) 3.11 & 0.20 \(\) 0.26 & 263.08 \(\) 7917.14 & 55.21 \(\) 15.93 & 1.09 \(\) 0.24 & -0.01 \(\) 0.36 \\ Ind-GP & 4.15 \(\) 1.38 & 3.02 \(\) 0.77 & -22.82 \(\) 0.26 & 80.95 \(\) 38.08 & 1.85 \(\) 0.4 & -0.98 \(\) 0.27 \\ GIM-GP & 2.80 \(\) 1.96 & 5.28 \(\) 0.29 & -2.80 \(\) 0.45 & 2.66 \(\) 13.83 & 2.32 \(\) 0.74 & -1.15 \(\) 0.22 \\ HHD-GP (ours) & 4.83 \(\) 1.60 & 2.92 \(\) 0.90 & -2.20 \(\) 0.29 & 34.64 \(\) 15.68 & 1.83 \(\) 0.42 & -0.79 \(\) 0.26 \\ SPHHD-GP (ours) & **2.02 \(\) 1.75** & **8.52 \(\) 5.11** & **-3.16 \(\) 0.85** & **13.34 \(\) 6.89** & **3.18 \(\) 1.37** & **-1.56 \(\) 0.21** \\   

Table 1: Comparison of our models to baselines. The results are averaged over 10 independent experiments performed by resampling the training sets and model initial parameters. The RMSE and the VPT are recorded in the scale of \( 10^{-2}\) and in the form of mean \(\) standard deviation. Bold font indicates best results.

Figure 3: RMSE of energy prediction with increasing number of training data.

Figure 2: Energy prediction along the trajectory initialized at \((1.0,0.0)\) of the mass-spring system (left figure) and \((1.5,0.0)\) of the pendulum (right figure), respectively.

the consistency of SPHHD-GP. In addition to this empirical validation, in Appendix H we provide a theoretical verification that for the dynamical systems in our experiments, the non-identifiability of our model (\(i.e.\) non-uniqueness of HHD) is solved through forced symmetries.

Accurate prediction of energy demonstrates the interpretability of learned div-free features. To further show the interpretability of curl-free features, in Appendix J.4 we added experiments of adapting the learned models to predict dynamics with unseen friction coefficients. And Appendix J.5 presents some additional experimental results of increasing the noise in training data, showing that SPHHD-GP maintains high gain in large noise compared to the baselines.

### Learning Chaotic dynamics

Learning the correct HHD also has important implications for studying chaotic systems.  stated that the div-free component of a chaotic system is always orthogonal to the gradient of its energy function, \(i.e.\), \( H^{}_{div}()=0\). This energy function can be used to synchronize two chaotic systems , analyze their stability , and design energy-modulated controllers . This experiment aims to learn the Chua circuit , which is a chaotic system with applications in various fields. Its governing equation and the HHD are detailed in Appendix E.2. The models are trained on \(100\) randomly sampled data, corrupted with additive Gaussian noise (standard deviation: \(0.05\)). The generation of test data is the same as in the experiment in Section 6.1. The results are given in Table 2, which shows that SPHHD-GP consistently outperforms the other approaches. Appendix J.6 plots the trajectory and energy predictions. These results again confirm the advantage of incorporating symmetry constraints into our method. Please note that D-HNN is not applicable to this system because the div-free part of D-HNN only applies to Hamiltonian systems.

### Learning ocean current fields

To investigate the performance of our model in learning real-world dynamical systems, we then evaluated our model on an ocean current field, which is a complex dynamical system intricately governed by the interplay of multiple factors such as Earth's rotation, wind patterns, temperature gradients, and coastal interactions. Based on sparse observations of buoy velocities, oceanographers are interested in estimating ocean currents away from buoys and identifying divergences of the ocean current field.

In this experiment, we used a dataset from , containing 1183 velocity data points from 12 buoys distributed across the northern Gulf of Mexico, as shown by the red arrows in Fig. 4. We compared our model with the Helmholtz-GP model , which combines 2D div-free and curl-free kernels in the form of HHD to reconstruct planar ocean current fields, but without considering the severe non-identifiability problem caused by the non-uniqueness of HHD. To make the HHD of an ocean current field unique, an effective way in the field of fluid dynamics is to enforce the _parallel boundary condition_, which states that the HHD of a vector field in a bounded domain is unique if its div-free component is parallel to the domain boundary (cf. page 36 in , Section 5.1 in ). For our proposed model, the method of incorporating symmetry can be used to enforce the _parallel boundary condition_. Specifically, if a vector field has mirror symmetry with respect to a hyperplane, then the vectors on this hyperplane must be parallel to the hyperplane. Therefore, to make our model identifiable, the div-free kernel in our model was constructed to incorporate mirror symmetry with respect to the rectangular boundary of the ocean current field. To reduce the computational complexity, the kernels were fitted into a sparse GP framework , which reduced the computational complexity from \((m^{3})\) to \((M^{2}m)\), where \(m=1183\) is the number of data points and \(M=200\) is the number of inducing points. See Appendix I for the details of computational complexity of our model.

   Model & RMSE\({}_{}\) \(\) & MNL \(\) & VPT \(\) & RMSE\({}_{}\) \\  Div-GP & 122.4 \(\) 51.4 & 0.89 \(\) 0.74 & 0.9 \(\) 0.1 & N/A \\  Ind-GP & 12.5 \(\) 12.6 & -1.76 \(\) 1.02 & 5.9 \(\) 2.2 & N/A \\  GM-GP & 10.2 \(\) 10.6 & -1.92 \(\) 0.67 & 6.2 \(\) 2.9 & N/A \\  HHD-GP (ours) & 12.0 \(\) 11.1 & -1.76 \(\) 0.68 & 5.1 \(\) 1.4 & 7.4 \(\) 3.6 \\  SPHHD-GP (ours) & **4.1 \(\) 1.8** & **-2.72 \(\) 0.29** & **13.0 \(\) 5.6** & **0.7 \(\) 1.5** \\   

Table 2: Experimental results on the Chua circuit. RMSE\({}_{}\) and RMSE\({}_{}\) refer to the errors in derivative and energy predictions, respectively.

The predictions of the ocean current field and its HHD are shown in Fig. 4. Although the ground truth is not available for this field of real-world ocean currents, we can still assess performance against oceanographers' expert knowledge (cf. Section I.3 in ). For the predictions of ocean currents (the first column of Fig. 4), the oceanographers expect to see continuous currents with no sharp deviations. However, we can clearly observe that the prediction of Helmholtz-GP shows an abrupt drop in positions away from observed data, especially in the lower left, upper right and center regions. In contrast, our model (SPHHD-GP) presents a more continuous current prediction. The predicted divergence field (\(i.e.\), the scalar potential of the curl-free component) is provided in the third column of Fig. 4. The oceanographers expect to find a rich structure in divergence predictions. However, the Helmholtz-GP fails to recover the divergence field and instead predicts an almost constant curl-free vector field, as shown in the third block of Fig. 4(a). This is caused by the non-identifiability of the Helmholtz-GP, since harmonic components usually exist in the form of constant vector fields (cf. Section 4.3). Our model, instead, recovers the rich structure of the divergence field. Therefore, we can conclude that our model provides more realistic ocean current predictions and divergence identification, offering a better alternative for the simulation of ocean dynamics.

## 7 Conclusion and future work

Our work develops an additive GP model whose component is either free of divergence or of curl, the two most ubiquitous differential invariants of vector fields in natural, and we constrain the div/curl-free kernels to preserve desired symmetries. These symmetry-preserving kernels not only improve the accuracy of predictions but also make the model identifiable, thus a physically meaningful decomposition can be predicted. Our future work is to extend our model to exploit the connection of HHD with more dynamical systems. For example, there are recent advances in using the HHD to construct Lyapunov functions . So, our model has potential to achieve good performance in learning stable dynamics.