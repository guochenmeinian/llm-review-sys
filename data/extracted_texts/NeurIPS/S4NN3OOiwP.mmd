# Efficient Equivariant Transfer Learning from Pretrained Models

Sourya Basu

University of Illinois at Urbana-Champaign

&Pulkit Kattare

University of Illinois at Urbana-Champaign

&Prasanna Sattigeri

IBM Research

&Vijil Chenthamarakshan

IBM Research

&Katherine Driggs-Campbell

University of Illinois at Urbana-Champaign

&Payel Das

IBM Research

&Lav R. Varshney

University of Illinois at Urbana-Champaign

Equal contribution.

###### Abstract

Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of Basu et al. (2023) and Kaba et al. (2022) propose group averaging (_equitune_) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While Kaba et al. (2022) are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose \(\)-_equitune_ that averages the features using _importance weights_, \(\)s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned results that outperform equitune. Further, we prove that \(\)-equitune is equivariant and a universal approximator of equivariant functions. Additionally, we show that the method of Kaba et al. (2022) used with appropriate loss functions, which we call _equizero_, also gives excellent zero-shot and finetuned performance. Both equitune and equizero are special cases of \(\)-equitune. To show the simplicity and generality of our method, we validate on a wide range of diverse applications and models such as 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in natural language generation (NLG), 4) compositional generalization in languages, and 5) image classification using pretrained CNNs such as Resnet and Alexnet.

## 1 Introduction

Group-equivariant deep learning leverages group equivariance as an inductive bias to design efficient and reliable neural networks. Popular examples include convolutional neural networks (CNNs) equivariant to translations (LeCun et al., 1989), group convolutional neural networks (GCNNs) equivariant to general discrete groups (Cohen & Welling, 2016), and recently Alphafold2 equivariant to 3D rotations (Jumper et al., 2021). But these methods cannot leverage pretrained models.

With the increase in open sourced large pretrained models, it is now crucial to develop efficient transfer learning algorithms that can leverage group equivariance. Basu et al. (2023) proposed equitune, an equivariant finetuning method that uses group averaging over features extracted from pretrainedmodels. Several other methods were proposed to get equivariant output from non-equivariant backbone architectures, e.g. some form of averaging (Puny et al., 2021; Atzmon et al., 2022) or optimization over certain proxy loss functions (Kaba et al., 2022). But these latter methods were originally designed for training from scratch and not much is known about their finetuning abilities.

Here, we find that equitune performs poorly on zero-shot tasks. Our first main contribution is to show that the optimization method of Kaba et al. (2022) when used with appropriate proxy loss functions provides excellent zero-shot and finetuning performance. We call this method _equizero_.

The results from equizero suggest that pretrained models provide better quality features for some group transformations than others. Thus, we propose \(\)-equitune, which learns _importance weights_ directly from the data and uses them to perform a weighted group averaging. We show \(\)-equitune outperforms equitune and is competitive with equizero for zero-shot tasks. Moreover, for finetuning, \(\)-equitune often outperforms both equitune and equizero. This constitutes our second main contribution.

To validate our methods, we provide experiments on a diverse set of pretrained models and datasets. We show zero-shot performance of equizero on: 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in natural language generation (NLG), and 4) compositional generalization in languages. For CLIP and deep Q-learning, we used the naturally available loss functions, namely, similarity scores and \(Q\)-values, respectively. For fairness in NLG and compositional generalization, we closely follow the setup of Basu et al. (2023), and we use _regard scores_Sheng et al. (2019) and the negative of the maximum of the output probability distribution, respectively, as the loss function.

We first show results of \(\)-equitune for image classification using CLIP, finding that \(\)-equitune performs competitively with equizero and outperforms equitune for both zero-shot and finetuning tasks. Then, we show a simple case where finding a good proxy loss function for equizero is non-trivial: image classification using pretrained CNNs. Here, we find that equizero performs even worse than equitune but \(\)-equitune easily outperforms equitune and equizero. The organization of the paper is summarized below:

* SS3 provides details of \(\)-equitune and equizero and proves a number of their properties.
* SS4 provides overview of the applications considered in the experiments and how equivariance methods are used there.
* SS5 provides experimental details and results for all the applications.

## 2 Background

Here we discuss relevant concepts in group equivariance and group equivariant transfer learning.

Group EquivarianceA **group**\((G,)\) is a set \(G\) accompanied by a binary operator '\(\)' that satisfy the axioms of a group, namely i) closure: \(g h G\) for all \(g,h G\); ii) associativity: \((g h) k=g(h k)\);

Figure 1: Implementation of \(\)-equitune on CLIP. Weighted average of image features corresponding to transformed inputs are computed, which is further used for computing text-image similarity scores.

iii) identity: there exists \(e G\), such that \(g e=e g\) for all \(g G\); iv) inverse: for every element \(g G\), there exists \(g^{-1}\), such that \(g g^{-1}=g^{-1} g=e\). We write \(g h\) as \(gh\) for brevity.

Given a set \(\), we define a **group action** of \(G\) on \(\) as \(_{}:G\) such that it satisfies two axioms, namely i) identity: \(_{}(e,x)=x\) for all \(x\), where \(e G\) is the identity; ii) compatibility: \(_{}(g,_{}(h,x))=_{}(gh,x)\), for all \(g,h G,x\). We write \(_{}(g,x)\) simply as \(gx\) for brevity.

A model \(:\) is **equivariant** to \(G\) under the group action of \(G\) on \(\) and \(\) if \((gx)=g(x))\) for all \(g G,x\). This essentially means that any group transformation \(g\) to the input \(_{}(g,x)\) should reflect with an equivalent group transformation of the output \(_{}(g,(x))\).

Equivariant FinetuningRecently, Basu et al. (2023) proposed a finetuning method called eq-uituning that starts with potentially non-equivariant model \(\) and produces a model \(_{}\) that is equivariant to \(G\). Equituning converts a pretrained model into an equivariant version by minimizing the distance of features obtained from pretrained and equivariant models. The output of an equituned model is given by

\[_{}(x)=_{g G}g^{-1}(gx).\] (1)

While the averaging in equation 1 is shown to be useful for finetuning, we find it leads to poor equivariant zero-shot learning. This could be because the pretrained model outputs high quality features only for some of the transformed inputs. Hence, averaging them directly leads to low quality zero-shot performance. This can be avoided by using weighted averaging as discussed in Sec. 3.

Optimization-Based Canonical RepresentationOn the other hand, Kaba et al. (2022) show that group equivariant output, \(_{}^{}(x)\), can be obtained by optimizing a (non-equivariant) loss function \(l((gx))\) with respect to group elements \(g G\) for any \(x\) as shown below.

\[_{}^{}(x)=g_{*}^{-1}(g_{*}x),\] (2)

where \(g_{*}=*{arg\,min}_{g G}l((gx))\) with \(l:\) being an injective proxy loss function and assuming the minima is unique. However, the purpose of this formulation in Kaba et al. (2022) is only to obtain an equivariant representation for training from scratch. Moreover, no competitive zero-shot or finetuning performance is obtained in Kaba et al. (2022) using this method. We show that the choice of \(l\) plays an important role in its zero-shot and finetuning performance, even outperforming equituning. This method is obtained as a special case of \(\)-equituning introduced next.

Additional Related WorksGroup equivariance plays a key role in geometric deep learning (Bronstein et al., 2021) for designing efficient domain specific neural networks. Several elegant architectures have been proposed for equivariant image classification (Cohen and Welling, 2016, 2017; Romero and Cordonnier, 2020), reinforcement learning (Mondal et al., 2020; van der Pol et al., 2020; Mondal et al., 2022; Wang et al., 2022), graph (Satorras et al., 2021; Keriven and Peyre, 2019; Gasteiger et al., 2021) and mesh (De Haan et al., 2020; He et al., 2021; Basu et al., 2022) processing, natural language processing (Gordon et al., 2019; Li et al., 2022), and data generation (Dey et al., 2021). These architectures need to be trained from scratch, which is not always desirable.

_Frame_ averaging produces equivariant output from non-equivariant architecture backbones (Puny et al., 2021; Atzmon et al., 2022; Duval et al., 2023). Most work here focuses on finding good frames, which are equivariant subsets of groups, for specific groups, and not for general groups. And not much is known about their performance with pretrained models. Kaba et al. (2022) also give a canonicalization-based method that uses an equivariant auxiliary network for constructing equivariant networks out of non-equivariant backbones and is used for training from scratch. But this work requires an additional equivariant network and appropriate trainable parameterization of the group actions, which is presented only for certain groups of interest. Further, this work is not concerned with equivariant performance of pretrained models. Zero-shot group equivariance was also recently used by Muglich et al. (2022) for zero-shot coordination in partially observable Markov decision processes (POMDPs). In contrast, our work aims to be provide efficient equivariant transfer learning algorithms that are general in terms of considered tasks and groups, and does not require additional equivariant networks.

\(\)-Equitune

We propose \(\)-equitune, where unequal weights are assigned to features obtained from transformed inputs. This is a simple generalization of equitune in equation 1 and the optimization-based approach in equation 2, where the goal is to assign higher values to better features. Like these previous methods, \(\)-equitune is equivariant and a universal approximator of equivariant functions.

The main idea of \(\)-equitune is that given a pretrained model \(\), the features \((gx)\) for any fixed \(x\) are not all equally important for all \(g G\). We denote by \((gx)\) the _importance weight_ of feature \((gx)\) for \(g G,x\). We assume \(G\) is finite, just as in Basu et al. (2023). Suppose \(:^{+}\) is known a priori, and denote the \(\)-equituned model as \(^{}_{}\). Then we want to minimize

\[_{^{}_{}(x)}& _{g G}\|(gx)(gx)-^{}_{ }(g,x)\|_{2}^{2}\\ &^{}_{}(gx) =g^{}_{}(x)g G.\] (3)

We call the solution to equation 3 as \(\)-equitune, given by

\[^{}_{}(x)=(gx)}_{ g G}^{|G|}g^{-1}(gx)(gx).\] (4)

### Special Cases

When \((gx)=1\) for \(g G\), then equation 4 is the same as equation 1. Further, we get equation 2 when \(\) is an indicator function \((gx)=_{\{g=g_{*}\}}\), where \(g_{*}=_{g G}l((gx))\) with \(l:\) such that the minimization is well defined. We use \(^{0}_{}\) to denote the equizero model.

The first main contribution of our work is experimental. We show there are good loss functions for equizero for several diverse tasks that easily outperform equitune by choosing the best features.

The second main contribution is to show that \(\)-equitune outperforms equitune and is competitive with equizero even when good loss functions are known. Moreover, when the loss functions are not trivial, equizero performs even worse than equitune, but \(\)-equitune easily outperforms both.

### Canonical-\(\)-Equitune

Here, we provide an extension of the \(\)-equitune algorithm of equation 4 to continuous groups. The idea is to combine the canonicalization method of Kaba et al. (2022) with \(\)-equitune leading to an expressive equivariant network with weighted averaging over features with different group actions applied to them.

**Definition 1** (Canonical-\(\)-equitune).: _Given a (continuous) group \(G\), a non-equivariant function \(M:X Y\), and an equivariant auxiliary function (from the setting of Kaba et al. (2022)) \(h:X G\), lambda functions \(:X R^{+}\), and a set of group elements \(=\{_{1},,_{k}\}\), i.e. \(_{i} G\), we define the canonical-\(\)-equitune operators as_

\[M^{}_{G,equi}(x) =( h(x)^{-1}x)}_{ }( h(x)^{-1}x)h(x)M( h(x)^{-1}x)\] (5) \[M^{}_{G,inv}(x) =( h(x)^{-1}x)}_{ }( h(x)^{-1}x)M( h(x)^{-1}x).\] (6)

In Thm. 1, we show that the canonical-\(\)-equivariant network is equivariant to the group \(G\)**Theorem 1**.: \(M^{}_{G,equi}(x)\) _and \(M^{}_{G,inv}(x)\) are, respectively, equivariant and invariant to \(G\)._

### Properties

Now we show in Thm. 2 that equation 4 is equivariant with respect to \(G\).

**Theorem 2** (Equivariance).: \(^{}_{}\) _defined in equation 4 is equivariant to \(G\)._We define a universal approximator in Def. 2. Then, Thm. 3 proves that \(\)-equitune is a universal approximator of equivariant functions for groups where \(\|g\|=1\) for \(g G\). This includes a wide range of groups including the permutation group, the \(SO(n)\) groups of special orthogonal groups, etc. This condition is the same as the class of groups considered in Basu et al. (2023).

**Definition 2** (Universal approximator).: _A model \(:\) is a universal approximator of a continuous function \(f:\) if for any compact set \(\) and \(>0\), there exists a choice of parameters for \(\) such that \(\|f(x)-(x)\|\) for all \(x\)._

**Theorem 3** (Universality).: _Let \(f_{G}:\) be any continuous function equivariant to group \(G\) and let \(:^{+}\) be any positive scalar function. And let \(:\) be a universal approximator of \(}{}\). Here \(\) is such that if \(x\), then \(gx\) to ensure the equivariance of \(f_{G}\) is well-defined. Then, \(_{}^{}\) is a universal approximator of \(f_{G}\)._

Computational ComplexityNote that equitune, equizero, and \(\)-equitune have the same compute complexity. In practice, in Tab. 1 we find that equitune and equizero have exactly the same time and memory consumption, whereas \(\)-equitune takes a little more time and memory because of the additional \(\) network. We illustrate on RN50 and ViT-B/32 models of CLIP using the same text encoder, but different image encoders. RN50 uses a Resnet50 based image encoder, whereas ViT-B/32 uses a vision transformer based image encoder.

Beyond Zero-Shot LearningLet us emphasize that even though the equizero model in equation 2 is not differentiable due to the \(*{arg\,max}\), we can still use simple gradient estimators known in the literature. One popular estimator is the straight-through estimator (Bengio et al., 2013), where the equizero output in equation 2 would be written as \(_{}^{}(x)=(x)+(_{ }^{}(x)-(x)).\), where detach() indicates that no gradient flows through the term \((_{}^{}(x)-(x))\). In practice, we found it to be slightly better to use \(_{}(x)\) instead of \((x)\) and write \(_{}^{}(x)=_{}(x)+( _{}^{}(x)-_{}(x)).\). SS5.1.3 illustrates few-shot learning and finetuning using equizero and compares with equituning.

## 4 Applications

First we present several applications in SS4.1 where finding a loss function for equizero is easy. This naturally leads to excellent equivariant zero-shot results outperforming equitune. Then, in SS4.2 we provide two applications in image classification to show 1) the benefits and drawbacks of equizero compared to equitune and \(\)-equitune, and 2) that \(\)-equitune consistently performs well avoiding the drawbacks of equizero.

### Equizero Applications

Here we provide applications where equizero achieves excellent zero-shot performance, namely: 1) deep Q-learning, 2) fairness in NLG, and 3) compositional generalization in languages.

Equizero Reinforcement LearningRecent works, such as van der Pol et al. (2020); Mondal et al. (2020), have developed RL algorithms that leverage symmetries in the environments that helps improve robustness and sample efficiency. But no existing work efficiently uses group equivariance on top of pretrained RL models. Here we apply equizero and equitune on top of pretrained models inspired from the group symmetries found in van der Pol et al. (2020). We find that equitune outperforms non-equivariant pretrained models but equizero outperforms both. We simply use the \(Q\)-values as the proxy loss function as described in SSC.1 with more details.

   Model &  &  \\  & Equitune & Equizero & \(\)-Equitune & Equitune & Equizero & \(\)-Equitune \\  RN50 & 14.15 & 14.10 & 23.5 & 3703 & 3703 & 3941 \\ ViT-B/32 & 11.00 & 10.27 & 16.08 & 2589 & 2587 & 2915 \\   

Table 1: Inference times of equitune, equizero, and \(\)-equitune for the c4 group for various CLIP models on CIFAR100. We use batch size 32 on a single Nvidia A100 GPU.

Group-Theoretic Fairness in NLGWe seek to reduce the social biases inherent in language models (LMs), focusing on GPT2 (Radford et al., 2019). We consider the group-theoretic fairness setup of Sheng et al. (2019) and Basu et al. (2023). We take the sets of demographics, namely ['man', 'woman'], ['straight', 'gay'], and ['black', 'white']. For each demographic group, Sheng et al. (2019) proposed two tasks, called _respect task_ and _occupation task_, where each task consists of five context phrases. A language model (LM) is given these contexts to generate sentences. These generated sentences are then classified as 'positive', 'negative', 'neutral', or 'other' by a _regard classifier_ also proposed by Sheng et al. (2019). These outputs are called regard scores. A regard classifier is a BERT-based model similar to a sentiment classifier but more specific for fairness tasks. We use equizero using the regard scores as the proxy loss function, so as to maximize positivity in the generated texts, while guaranteeing group-theoretic fairness. To that end, we propose two algorithms that help in reducing existing biases across demographic groups.

_EquizeroLM and R-EquizeroLM:_Basu et al. (2023) define EquiLM and R-EquiLM, that use a sequential version of equitune to perform group transformed averaging to achieve fairness across demographic groups. While EquiLM and R-EquiLM generate debiased outputs, they do not produce positive regard scores, which is desirable to reduce toxicity in generated text. We propose EquizeroLM and R-EquizeroLM which use equizero to maximize regard scores, ensuring both fair and less toxic. Further details provided in SSC.2.

Zero-Shot Compositional GeneralizationWe show compositional generalization capabilities of equizero on the SCAN dataset (Lake and Baroni, 2018). SCAN measures compositionality using a language-to-action translation task. E.g., if the model learns that the phrase "Jump", "Run", "Run Twice" translate to the actions "JUMP", "RUN", "RUN RUN" from the train set, then, SCAN tests whether the model also learns that "Jump Twice" translates to "JUMP JUMP". Such a reasoning however common in human beings is hard to find in language models.

We apply equizero on two tasks in SCAN, _Add Jump_ and _Around Right_. Gordon et al. (2019) solved these tasks by constructing sophisticated group equivariant networks from scratch and training them. Basu et al. (2023) used the same group actions as Gordon et al. (2019), but used equituning on pretrained non-equivariant models for a few iterations and obtained comparable results. But, as we note, equitune has poor zero-shot performance. We show that equizero using negative of maximum probability from the output as the loss function gives much better zero-shot performance. Using gradient estimators described in SS3.3, we also compare the finetuning performance of equizero against equitune. Details of group actions used are given in SSC.3

### \(\)-Equitune Applications

Here we consider two important applications: CLIP-based and CNN-based image classification. For CLIP, it is easy to find a loss function for equizero that provides better results than equitune. But for the simple case of CNN-based classification it is non-trivial to find such a loss function. Since \(\)-equitune does not require a loss function, it performs better than equitune in both cases. Equizero only performs well for CLIP, but fails miserably for CNN-based classification.

CLIP-Based Image ClassificationCLIP is a pretrained model consisting of image and text encoders that give impressive zero-shot classification performance across variety of unseen datasets. But, in Fig. 3(a) and 7, we find that CLIP is not robust to simple transformations such as rotation by multiples of \(90^{}\) or random flips. This trend is seen across different image encoders like RN50, RN101 (Radford et al., 2021), ViT-B/32 and ViT-B/16 (Dosovitskiy et al., 2021). This can be avoided by making the model in/equi-variant to such transformations, e.g., by using equitune. But we show in SS5.2.1 that equitune does not produce good zero-shot performance. We show in SS5.2.1 that using equizero with image-text similarity score as loss function provides much better zero-shot results than equitune. Later, in SS5.2.1, we show that \(\)-equitune achieves better zero-shot performance than equitune without any loss function. Finally, when finetuned, we find that \(\)-equitune tends to perform the best, possibly because it does not need gradient estimators like equizero, and because it uses weighted averaging to obtain better features than equitune.

CNN-based image classificationFor image classification using pretrained CNNs such as Resnet and Alexnet, we note that finding a good loss function for equizero is non-trivial. As such, we consider two loss functions 1) negative of the maximum probability as it worked well with the SCANtask in SS4.1 and 2) the entropy of the output distribution since it correlates with the confidence of the model (Wang et al., 2018). But, equizero with these loss functions performs even worse than equitune. Further, we find that \(\)-equitune easily outperforms both equitune and equizero.

## 5 Experiments

Here, we provide experimental results for equizero and \(\)-equitune in SS5.1 and SS5.2, respectively, for all the applications described in SS4. Additional experiments for canonical-\(\)-equitune are provided in SS. D.5. The code for this paper is available at https://github.com/basusourya/lambda_equitune.

### Zero-Shot Performance using Equizero

#### 5.1.1 Equizero Reinforcement Learning

**Experimental Setting:** We first pretrain Deep Q-learning nets (DQNs) for each of the Gridworld, Cartpole, and Acrobot environments using the default architecture from Raffin et al. (2021) with 103k parameters. We pretrained all the models using a learning rate \(10^{-4}\). We used training time steps as 100k, 100k, and 70k for Gridworld, Cartpole, and Acrobot, respectively. These number of steps were chosen to obtain the best models by varying the time steps from 50k to 100k in multiple of 10k for a fixed seed.

**Results and Observations:** Fig. 2 show the evaluation performance of equizero and compare it with equituning and non-equivariant pretrained models. We find that equituning performs better than non-equivariant models and equizero outperform both of them. The results are over five seeds.

#### 5.1.2 Fairness in Natural Language Generation

Figure 3: Plots (a), (b), and (c) show the regard scores for GPT2, EquiGPT2, R-EquiGPT2, Equize-roGPT2, and R-EquizeroGPT2. In equitune, if negativity is present in some demographics, it gets redistributed in the other demographics, which is undesirable. Equitune is only able to debias, whereas, equizero models not only debiases the texts but also makes the regard scores more positive.

Figure 2: Comparison of zero-shot performance of equizero to equituning and a non-equivariant pretrained model are shown in (a), (b), and (c) for Gridworld, Cartpole, and Acrobot, respectively. Equizero outperforms both equituning and non-equivariant pretrained model. Results over five seeds.

[MISSING_PAGE_FAIL:8]

We then evaluate finetuning capabilities of equiltune, equizero, and \(\)-equitune on CIFAR100 with random \(90^{}\) rotations. Here we choose \(\) to be a two-layered feed-forward network, which is described in detail along with learning rates used, in SSD.4. The input to this \(\)-network are the features from a frozen CLIP encoder. First, the \(\)-network is trained with the model kept frozen. Then, while finetuning the model, the \(\) network is frozen. We show results for both Resnet and ViT backbone trained for 1000, 2000, 3000, 4000 finetuning steps.

**Results and Observations:** Fig. 4 shows test accuracies for Imagenet-V2 with random \(90^{}\) rotations and flips. We observe that the pretrained model's performance reduces drastically when transformations are applied to the dataset. Whereas both equitune and equizero are relatively robust to such transformations. Moreover, equizero outperforms both equitune and the pretrained model. Similar observations are made for CIFAR100 in Fig. 8 in SSD.

In Fig. 4(a) and 10(a) we plot the test accuracies of \(\)-equitune on CIFAR100 for both variants of Resnet and ViT backbones. We observe that \(\)-equitune performs better than both equitune and equizero (with finetuning) on Resnets. On ViT-B/16, we observe that \(\)-equitune easily outperforms both equitune and equizero (with finetuning). On ViT-B/32, we find that \(\)-equitune outperforms equitune but equizero outperforms both equitune and \(\)-equitune. Thus, \(\)-equitune performs competitively with equizero, even in applications where good loss functions are known.

#### 5.2.2 Equi/Invariant Image Classification using Pretrained CNNs

**Experimental Setting:** We now evaluate the performance of \(\)-equitune on rot90-CIFAR10 dataset. Here, each image is rotated randomly by a multiple of \(90^{}\). We use pretrained Alexnet and Resnet, trained extensively over CIFAR100. For the \(\)-equituning, we choose \(\) as a two layered feed-forward network with a hidden layer of dimension 100 with input as features extracted by this pretrained Alexnet or Resnet. Along with \(\), we also perform linear probing wherein last two layers for the classification problem is being learned using a learning rate of \(10^{-3}\).

**Observation and Results:** In Fig. 4(b) and 10(b) we see that \(\)-equitune outperforms equitune and equizero. Moreover, equizero performs even worse than equitune. This trend is consistent across both Alexnet and Resnet pretrained modules.

## 6 Limitations, Societal Impact, and Conclusion

**Limitations and Societal Impact:** Our results focus on finite groups; extension to continuous groups requires further work in parameterization of continuous groups (cf. Benton et al. (2020)) or group decomposition (cf. Basu et al. (2021); Maile et al. (2023)). Our work on fairness in NLG aims to debias foundation models and thereby lead to positive societal impact. But we use equality and neutral sets obtained from previous human-made works. We believe there is scope for optimizing the design of such sets using neural networks for more general demographic groups.

**Conclusion:** We present \(\)-equitune and its special case equizero that outperform equitune on equivariant zero-shot and finetuning tasks. We show that both methods are equivariant, universal,

Figure 4: In (a) note that zero-shot performance of CLIP drops significantly when we add random rotations or flips to the input. This trend is seen across all image encoders, i.e. ResNet (RN50 and RN101) and ViT (ViT-B/32 and ViT-B/16). In (b) and (c) we show classification results on Imagenet-V2 with random \(90^{}\) rotations and flips, respectively. We observe equizero outperform equitune and original CLIP for all image encoders.

are computationally efficient, and are easy to implement. Equizero performs well when good proxy loss functions are available for downstream tasks, which we show is easy to find across several diverse tasks. \(\)-equitune outperforms equitune and is competitive with equizero without the need for any proxy loss. We consider diverse tasks: i) deep Q-learning, ii) fairness in NLG, iii) compositional generalization in languages, iv) CLIP-based classification, and v) CNN-based classification. Experimental results validate the superiority of \(\)-equitune and equizero over equitune on zero-shot and finetuning tasks.