# A Synthetic Dataset for Personal Attribute Inference

Hanna Yukhymenko\({}^{1}\), Robin Staab\({}^{2}\), Mark Vero\({}^{2}\), Martin Vechev\({}^{2}\)

\({}^{1}\)Department of Mathematics, \({}^{2}\)Department of Computer Science - ETH Zurich

hyukhymenko@ethz.ch, {robin.staab,mark.vero,martin.vechev}@inf.ethz.ch

###### Abstract

Recently powerful Large Language Models (LLMs) have become easily accessible to hundreds of millions of users world-wide. However, their strong capabilities and vast world knowledge do not come without associated privacy risks. In this work, we focus on the emerging privacy threat LLMs pose - the ability to accurately infer personal information from online texts. Despite the growing importance of LLM-based author profiling, research in this area has been hampered by a lack of suitable public datasets, largely due to ethical and privacy concerns associated with real personal data. We take two steps to address this problem: (i) we construct a simulation framework for the popular social media platform Reddit using LLM agents seeded with synthetic personal profiles; (ii) using this framework, we generate _SynthPAI_, a diverse synthetic dataset of over 7800 comments manually labeled for personal attributes. We validate our dataset with a human study showing that humans barely outperform random guessing on the task of distinguishing our synthetic comments from real ones. Further, we verify that our dataset enables meaningful personal attribute inference research by showing across 18 state-of-the-art LLMs that our synthetic comments allow us to draw the same conclusions as real-world data. Combined, our experimental results, dataset and pipeline form a strong basis for future privacy-preserving research geared towards understanding and mitigating inference-based privacy threats that LLMs pose.

## 1 Introduction

The increasing capabilities of Large Language Models (LLMs) alongside their widespread adoption have sparked ample discussions about their potential misuse [1; 2]. Besides issues such as toxic, unsafe, or otherwise harmful generations, potential privacy implications of LLMs have been receiving an increasing amount of attention [3; 4; 5]. Most existing LLM privacy research focuses on _memorization_, the issue of models repeating training data at inference time . However, as recently shown by Staab et al. , and previously outlined by Weidinger et al. , current frontier LLMs can also be misused to accurately infer personal attributes from online texts at a scale unattainable by humans. Even despite the legal relevance of such automated personal attribute inferences (PAIs) (SS2), research in this area has been significantly held back by a lack of suitable openly available datasets.

The PAI Data GapThis lack of data is a result of multiple factors: As PAI research, by nature, deals with personal (and sensitive) data underlying strict data protection regulations (e.g., GDPR SS2), making real-world datasets public is legally challenging. While prior work on PAI used real-world data [6; 8], they each had to construct their datasets from scratch, not releasing them due to associated ethical and legal concerns. Staab et al.  tried to address this issue to an extent by releasing synthetic examples instead of their actual data. However, they acknowledge that these samples fall short of real-world texts both in setting and style and, as such, are not a viable long-term replacement for real data. Additionally, due to the considerable effort in obtaining personal attribute labels over free text, even the few partially available datasets (SS2) contain labels for only a very restricted set of attributes.

This WorkIn this work, we bridge this gap by (i) introducing a novel framework simulating popular comment-thread-focused social media platforms such as Reddit using personalized LLM agents and (ii) instantiating this framework to produce a synthetic dataset, SynthPAI, of over \(7800\) comments with hand-curated personal attribute labels. As our pipeline does not require any real data, it is fully privacy-preserving. Our experimental evaluation in SS4 shows that SynthPAI is realistic, diverse, and enables PAI research that is representative of results obtained on real-world data.

In Fig. 1, we present an overview of our personalized LLM agents-based simulation framework. In a first step 1, we construct unique and realistic synthetic profiles containing a diverse personal attributes. Using these profiles, we seed LLM agents in 2, letting them interact in simulated comment thread environments. At the same time, we automatically tag each generated comment for inferrable personal attributes. After having simulated a large number of threads 3, we construct our proposed synthetic dataset, SynthPAI, by manually verifying each PAI label over the generated comments.

Diversity and FidelityIn SS4.1, we verify that SynthPAI is highly diverse and realistic by showing that the profiles and comments included in SynthPAI cover a wide range of demographic attributes and over ninety diverse topics. Additionally, to signify that our generated synthetic comments are realistic, we demonstrate in a human experiment that average humans can barely outperform random guessing when tasked with distinguishing our comments from real ones.

Enabling PAI ResearchAdditionally, we verify the adequacy of SynthPAI as a replacement for real-world PAI datasets. We reproduce all main experiments of Staab et al.  across 18 state-of-the-art LLMs, observing that we can consistently draw the same conclusions on SynthPAI as on real-world data, establishing SynthPAI as a privacy-preserving foundation for future PAI research.

Main ContributionsOur main contributions are:

* A personalized LLM agents-based comment thread simulation framework for producing high-fidelity, diverse, and PAI research-aiding synthetic comments.
* A curated and hand-labeled synthetic PAI dataset, SynthPAI, of over \(7800\) comments, creating the first open and privacy-preserving dataset for PAI research.
* A public release of our framework and dataset SynthPAI1 and an extensive experimental evaluation showing that SynthPAI is diverse, realistic, and enables insightful PAI research.

## 2 Background and Related Work

Personal data and Personally Identifiable Information (PII)Several key regulations define personal data and information. In the EU, the term _personal data_ is directly defined in Article 4

Figure 1: Overview of our personalized LLM agent-based thread simulation framework and the curation of SynthPAI. First, in step 1, we create diverse synthetic profiles and seed LLM agents with them. Then, in step 2, we let the agents interact to generate comment threads. Finally, in step 3, aided by an LLM, we label the generated comments for inferrable personal attributes.

of the European General Data Protection Regulation (GDPR)  as "any information relating to an identified or identifiable natural person." In a similar spirit, US legislation commonly relies on _Personal Identifiable Information_ (PII), defined by the Department of Labour as "information that permits the identity of an individual [...] to be reasonably inferred by either direct or indirect means" . Notably, both personal data and PII cover personal attributes such as economic status, race, sexual orientation, or physical identity. Aligned with prior work on personal attribute inference on real-world data , SynthPAI (SS3) targets 8 diverse personal attributes directly covered under these regulations. Further, our data generation framework is extendable to an even wider range of attributes.

Privacy Risks of LLMsPrivacy implications of LLMs have been primarily studied in the context of _memorization_, i.e., the verbatim regurgitation of training data. Notably, whenever personal data was included in the training, they may be reproduced later , violating individuals' privacy. Carlini et al.  further observed a log-linear relationship between memorization, model size, and data repetitions, predicting memorization to be an increasing issue for future models. However, as later pointed out in  and , memorization does not capture all privacy threats arising from the deployment of LLMs as it is bound to the training data and fails to account for approximate reconstructions or finer contextual notions of privacy. Crucially, LLM-privacy literature focused on memorization does not cover the inference or extraction risk of personal information from potentially ambiguous contexts .

Author Profiling and PAIIdentifying author attributes from text exhibits a long line of research in natural language processing (NLP) . However, before the emergence of LLMs, prior works commonly relied on classical NLP techniques such as part-of-speech tagging, key phrase detection , and on character n-gram classification .  and  directly process the text using deep learning methods trained to infer a restricted set of attributes. As such, all pre-LLM methods assume a supervised setting, requiring a set of domain-specific training labels to enable attribute inference.

In contrast, LLMs can be used for zero-shot attribute inference from text. Notably, Staab et al.  have recently shown that LLMs, due to their advanced reasoning capabilities and vast world knowledge, achieve near-human-level accuracy at inferring author attributes from real-world texts. Concerningly, these inferences incur a much lower monetary and time cost than human profilers, making them applicable on a large scale . However, due to privacy concerns, Staab et al.  did not make the dataset used in their study public, only providing a selection of qualitatively aligned synthetic samples. A similar public data limitation can be observed in concurrent work as well . Consequently, the lack of publicly available high-quality datasets is a significant hurdle when it comes to (i) evaluating the privacy threat of LLM inferences and (ii) developing better defenses .

Existing DatasetsTo our knowledge, the only (partially) available real-world datasets are published by the recurring PAN competitions [11; 18; 19] require manual vetting, and often need to be reconstructed from pointers to proprietary APIs (e.g., Twitter). Additionally, PAN datasets only focus on one or two personal attributes (commonly gender and age), making them unsuitable for realistic PAI evaluations in real-world scenarios. Staab et al.  addresses this narrow focus in their evaluation; however, they did not release their collected dataset due to privacy concerns. This work aims to alleviate this issue by providing an openly accessible high-quality synthetic dataset on which the community can evaluate both attribute inference and potential defenses without privacy concerns.

Synthetic Data Generation with LLMsDue to their strong generative modeling capabilities, current frontier LLMs have been widely applied for synthetic data generation. Besides exhibiting strong performance at generating tabular data , LLM-generated text samples are used to augment training data [21; 22; 23], replace human labels in research datasets , or for evaluation datasets [6; 25; 26; 27]. Notably, several works [28; 29; 23] have shown that data produced by current LLMs enable realistic and high-quality applications on a variety of otherwise data-constrained tasks .

## 3 Building a Reddit Simulation Environment and Agents

This section first discusses the key requirements for a synthetic online comment dataset for personal attribute inference. Then, along the outlined requirements, we present our LLM agents-based Reddit simulation framework used to obtain our synthetic personal attribute inference dataset SynthPAI.

Key RequirementsTo enable the advancement of research on personal attribute inference from online texts, a synthetic dataset for this purpose must fulfill four key requirements:

_R1. Setting:_ The synthetic comments' content, format, and structure should follow the corresponding real-world setting, reflecting the same communicative situation and intent. As such, they should follow a multi-party comment-thread setting, where each comment is contextualized by the preceding thread and intended to communicate its content to all thread participants. Notably, prior attempts at producing synthetic samples for PAI failed to reflect this setting, generating synthetic forum comments from much more restricted one-on-one conversations .

_R2. Diversity:_ The synthetic data should reflect the varying opinions and experiences commonly found in online forums. This is particularly relevant as it enables a more detailed analysis of individual attribute inferences. Achieving this can be challenging, as current language models are inherently biased and fine-tuned to reflect the opinions of a non-representative subset of the population .

_R3. Quality and Style:_ Synthetic comments should be of similar quality and style as corresponding real-world texts. This is a natural requirement, as otherwise, any analysis of the synthetic comments would not be representative of the actual real-world privacy threat.

_R4. Fitness for PAI:_ The produced synthetic dataset must allow for a meaningful analysis of the personal attribute inference capabilities of LLMs. As such, it shall contain diverse personal attribute labels inferrable from individual text snippets, ideally containing clues at different levels of extraction difficulty, akin to real-world data .

### Simulating Reddit via Personalized LLM Agents

We now give a detailed description of our synthetic comment generation framework, describing each step taken to ensure that the resulting synthetic synthetic data fulfills the requirements _R1-R4_. We then detail how we leverage our framework alongside human labeling to create SynthPAI.

OverviewOn a high level, our framework aims to simulate online interactions in comment threads with personalized LLM agents acting as the users in this environment. In line with _R1_, we focus on generating content of similar structure and characteristics as that of Reddit, as Reddit comments (i) exhibit a wide range of topics and personal backgrounds, (ii) represent a popular real-world scenario, and (iii) have been used in prior LLM PAI work . For this, we create personalized agents by seeding LLMs with detailed synthetic profiles containing the personal attributes of interest. Then, we generate thread topics and let agents take turns commenting on threads. To make the resulting comments fit for PAI analysis, we additionally label each generated comment by the personal attributes that are inferrable from it, either automatically or in a human-in-the-loop manner.

We now detail the key components of our framework and present the creation of SynthPAI.

Constructing Synthetic ProfilesAs a first step, we construct a set of diverse synthetic profiles \(\) that will seed the LLM agents in our simulation. Each synthetic profile consists of 8 personal attributes, based on the target attributes studied by Staab et al. : age, sex, income level, location, birthplace, education, occupation, and relationship status. We use GPT-4  as a profile generator, using a few-shot prompt with hand-written profiles (presented in App. D.1). To ensure that an agent seeded with a specific profile produces consistent comments across multiple threads and posts, we next enrich profiles with a detailed description of their writing style (_R3_). For this, we again leverage GPT-4 to generate a likely writing style of a person with the given profile (also App. D.1).

Thread Simulation with LLM AgentsGiven a set of personal profiles \(\), we first instantiate a set of agents \(_{}\) seeded by them. These agents \(a_{p}_{}\) interact with each other, simulating the interaction of users in comment threads on social media platforms, following the requirement of a realistic setting for synthetic PAI datasets (_R1_). We detail this simulation process in Algorithm 1 aiding our explanation. First, in Lin. 1, we generate a top-level comment \(c^{r}\), akin to a post on platforms such as Reddit. This initializes our thread \(T\). More formally, each thread is modeled by a tree \(T\) where \(c^{r}=root(T)\) is a generated topic, i.e., the top-level comment, and each node \(c T\) represents a comment. With path\((T,c)\), we denote the unique ordered comment chain (the tree path) from \(c^{r}\) to \(c\). Next, in Lin. 2, based on their profile, all agents decide whether they are interested in participating in \(T\) given the topic \(c^{r}\). This is decided by each agent indvidually using the _Profile Interest_ prompt given in App. D.2. We denote the set of agents that choose to participate as \(_{}^{T}_{}\). Then, all agents in \(_{}^{T}\) take turns to add comments to the thread over \(R\) rounds.

In each round, every agent \(a_{p}_{}^{T}\) is randomly selected to either comment or abstain. If \(a_{p}\) is chosen to comment, it is assigned a comment \(c^{*}\) to reply to by weighted random sampling based on a comment scoring function \((n,a_{p},t)\) (Lin. 7 and Lin. 8). The scoring function \(\) scores each comment in \(T\) based on (i) the depth of the comment in the thread, scoring deeper lying comments lower, and (ii) boosting the score of comments in comment chains where \(a_{p}\) has already engaged in. We provide further details on \(\) and how it reflects real-world intuition in people's engagement with comment threads in App. C.3. Having selected a target comment \(c^{*}\), in Lin. 9, the agent is provided with the thread context \((T,c^{*})\) and prompted to generate a comment \(c^{+}\). Next, in Lin. 10, to provide labels \(\) for personal attribute inference tasks, we tag each comment with attribute inferences using an oracle \(\), instantiated either by an LLM or a human. Finally, before passing over the thread to the next agent, the generated comment is inserted into the thread tree in Lin. 11. We provide further details about the thread simulation procedure, including prompts in App. C.

Next, we detail how we ensure that the comments generated in Lin. 9 satisfy our requirements _R2-R4_.

Generating Realistic and Relevant CommentsRecall the requirements we have set for synthetic PAI datasets: The dataset has to reflect a realistic setting (_R1_), consist of text reflecting diverse personal attributes, backgrounds, opinions, and topics (_R2_), be of realistic quality and style (_R3_), and enable the study of PAI inference (_R4_). While the setup of the simulation framework, as introduced above, ensures that the generated synthetic dataset is in line with _R1_, to satisfy the remaining requirements, we need to focus on the exact comments the personalized agents are producing. Our framework aims to satisfy the remaining three requirements via a set of carefully constructed agent prompts.

First, to ensure diversity (_R2_), as already detailed above, we seed the agents with diverse synthetic profiles. As such, we aim to let agents only generate comments aligned with their background as defined in their respective seeding profile. We visualize this in more detail in Fig. 2. To generate a new comment, we pass the profile background information and the preceding conversation to the agent, prompting it to continue the discussion based

Figure 2: Profile-conditioned comment generation. Agents generate comments based on the provided context, their synthetic profile and their writing style.

on the provided context. Additionally, as current (aligned) LLMs tend to be highly agreeable , we introduce an separate disagreement option into the prompt (see App. D.3), enabling certain agents to disagree with previous comments, increasing the thread's diversity in opinions and topics.

To address _R3_, we make use of the style anchors provided by the style description included in the seeding profile. In addition to the profile, the LLM agent's system prompt also includes this style description (detailed in App. D.1). Further, before an agent formulates its final comment, it is instructed to first analyze it in a chain-of-thought  manner to check its adherence to the profile writing style. The analysis includes a detailed reasoning with summary of the discussion context, previous engagement in it and a self-instruction on stylistic choices for the new comment according to the provided synthetic profile background (we provide an overview in Fig. 2). Based on this, the agent corrects its comment before adding it to the common thread.

Finally, to enable the inference of personal attributes at a similar difficulty as on real texts, we take two steps: (i) we generate top-level comments \(c^{}\) that follow the topics of popular Reddit communities likely to contain informative comments for PAI in ; and (ii) we instruct agents (App. D.3) to write comments that strongly reflect their profiles while not to explicitly revealing their personal attributes.

Obtaining SynthPAIWhile our framework can be instantiated in a fully automatic way to generate synthetic data for PAI, for our release of SynthPAI, we took several additional choices via a human in the loop to ensure that the final dataset is well-curated. In a first step, we generate and manually verify \(300\) unique personal profiles for diversity and consistency. Next, using the corresponding personalized agents, we run our algorithm to generate \(103\) threads, where each agent participates on average in \(2\)-\(3\) rounds. We ensure that we generate a wide variety of initial thread topics (SS4), with the goal of having a diverse PAI dataset across all \(8\) attributes. After creating the threads, we obtain \(7823\) comments with \(4748\) GPT-4  oracle-provided inference labels. To ensure high quality in our data release, we once more manually review each generated comment, adjusting model labels, and adding fine-grained inference hardness levels (\(1\) to \(5\) as in ). This resulted in a total of \(4730\) human-verified comment labels, which we aggregate (see App. C.2) on a profile level. The slight reduction in the number of labels is caused by removal of some low-quality LLM-produced tags as well as the addition of some manual ones, for which the human tagger could guess personal attributes better than a language model. As a last sanitization step, we only keep the human-verified profile labels matching the ground truth attributes provided by the profile. Our final dataset, SynthPAI contains \(1110\) such verified profile-level labels across all eight attributes and five hardness levels, similar to the real-world PersonalReddit dataset used by Staab et al. .

In the next section we conduct a thorough analysis of our synthetic dataset highlighting its quality and diversity, while verifying that SynthPAI enables insightful PAI research.

## 4 Evaluation

In this section, we experimentally evaluate SynthPAI. In SS4.1, we demonstrate the diversity of SynthPAI on a profile, thread, and comment level. We further show that for humans the synthetic comments in SynthPAI are almost indistinguishable from real-world Reddit comments. In SS4.2, based on this, we highlight how SynthPAI can serve as a strong replacement for real-world data in LLM-based PAI research by reproducing the experiments of Staab et al. .

### Diversity, Fidelity, and PAI Labels

First, we quantify SynthPAI's diversity in profiles and topics. Next, we present our human study showing that SynthPAI's synthetic comments are virtually indistinguishable from real-world comments. Lastly, we show that SynthPAI contains diverse and human-verified personal attribute labels. We provide a wide range of additional statistics and evaluations for SynthPAI in App. A.

Figure 3: Similarity of individual profiles found in SynthPAI as measured by the exact overlap of their respective personal attribute values.

Diversity of Profiles and Thread TopicsIn Fig. 3, we demonstrate the diversity of the profiles included in SynthPAI. In particular, we show the normalized (over matchings) count of profiles that share exactly \(k\) attributes with other profiles in the dataset. We notice that most profiles have at most one attribute value that also occurs in another profile, while only very few profiles match on \(3\) or more attributes. This indicates that our profiles are highly diverse with respect to each other, i.e., each profile is largely unique. Additionally, we find in App. A.3 that the individual attribute values are also realistically distributed in their respective domains, confirming that profiles in SynthPAI are both diverse and representative. We additionally investigate the diversity of the thread topics in SynthPAI. For this, we automatically categorize each thread into subreddits, where they would typically occur based on the discussed topic. We find that our thread topics span across a wide variety of \(91\) unique subreddits from gardening all the way to physics-focused ones. We provide further details on this analysis, including a more detailed split across individual attributes, in App. A.4.

Comment and Thread QualityNext, we investigate the quality of individual comments and threads. We present the key characteristics of SynthPAI in this regard in Table 1, with a detailed overview included in App. A.2. We find that the average comment is \(106\) characters long, and each thread contains roughly \(76\) comments from \(34\) different profiles. On average, each profile contributes around \(26\) comments across all threads in SynthPAI. A key observation is that the variance across all metrics is high, indicating that SynthPAI is diverse not only in terms of content but also with respect to its comment and participation structure, reflecting a natural heterogeneity observed in real-world comment threads.

We further verify the fidelity of SynthPAI's comments with a human study with \(40\) participants. For this, we randomly sample \(500\) comments of at least 5 words from SynthPAI and from (real-world) Reddit each. Next, we present the human participants with random comments selected either from our synthetic pool (SynthPAI) or from the real comments and ask them whether the given comment is real or has been LLM-generated. Each comment is thereby seen by exactly two different participants. As we show in Table 2, humans consistently consider SynthPAI's comments as realistic as real-world comments, achieving an overall accuracy of just \(51.9\%\), barely outperforming random guessing. We provide more details on our human study, including a full description, examples, and multiple ablations in App. E.

Attribute LabelsNow, we investigate the performance of our LLM-aided personal attribute labeling pipeline. In Table 3, we first compare how many comments our human labeler and the LLM-based tagging agree that a certain attribute can be inferred from the given comment. While we find a false negative rate of \(0.14\), the false positive rate of the model-based tagging is just \(0.01\). This means that comments tagged by the model as information-leaking are almost always also marked as such by our human labeler. This confirms that our pipeline can produce high-quality comment-level labels without human intervention by inheriting the ground truth labels from the profile on the tagged comments. We provide further details including format and value ranges of SynthPAI comment level tags in App. A.1.

Next, we investigate the accuracy of the uncorrected (i.e., labels not inherited from the underlying profile) LLM and human tags on an aggregated profile level. For this, as in Staab et al.  (and detailed in App. C.2), we aggregate labels for individual profiles based on comment-level labels. We find an agreement of \(83\%\) comparing the profile's ground truth attributes on the given human labels and \(76\%\) comparing LLM labels against human labels. Notably, the inter-human labeler agreement in Staab et al.  was around \(90\%\), reaffirming that LLM and human labels are already well-aligned.

    & Mean & Std. Dev. & Median \\  Comment Length & 106.43 & 90.78 & 69 \\ Comments per Thread & 75.94 & 32.70 & 84 \\ Profiles per Thread & 34.47 & 23.12 & 34 \\ Comments per Profile & 26.07 & 25.91 & 16 \\   

Table 1: Comment and thread statistics for SynthPAI.

   Pred.Label & SynthPAI & Human \\  SynthPAI & 208 (TN) & 170 (FN) \\ Human & 792 (FP) & 830 (TP) \\   

Table 2: Human Study results SynthPAI.

   LLM & No Label & Label \\  No Label & 57170 & 658 \\ Label & 676 & 4072 \\   

Table 3: LLM-generated Labels vs. Human Labels on SynthPAI.

### SynthPAI as a PAI Dataset

We now evaluate SynthPAI's fitness as a PAI dataset. In particular, we reproduce the main experiments of Staab et al. , inferring personal attributes from SynthPAI across \(18\) LLMs. As in , we additionally present ablations across attributes as well as inference performance on anonymized texts. Our results confirm that SynthPAI can serve as a basis for PAI research that is representative of the results that can be achieved on real-world datasets. We provide several additional results in App. B.

Experimental SetupWe directly follow the experimental setup introduced in Staab et al. . In particular, we predict personal attributes on a profile level, concatenating texts from all comments, using the same inference and evaluation prompts as in  (presented in App. D.4). Using step-by-step reasoning the model of interest provides explanation and final inference for private attributes of the profile (example in App. D.7). We also follow the same evaluation procedure (detailed in App. B.2), scoring categorical predictions with 0-1 accuracy and continuous predictions via respective thresholds. We give an overview of all used models and their respective settings in App. B.3.

Main ExperimentWe present the reproduction of Staab et al. 's main experiment in Fig. 4, showing the personal attribute inference capabilities of \(18\) widely used and state-of-the-art LLMs. As in , we can observe strong correlations between general model capability and PAI performance. Overall, we find that inferences on SynthPAI are, on average, slightly harder than on the dataset used in Staab et al. , with a drop of \( 10\%\) in overall accuracy. However, crucially, this drop is consistent across models, retaining their relative order. As in , GPT-4  achieves the highest accuracy (\(76\%\)), closely followed by Claude-3 Opus . Across all model families, we observe consistent increases in PAI performance with increases in model size. This is once again in line with the findings on real-world datasets, further validating SynthPAI usefulness as a PAI dataset. Additionally, as previously predicted , with the release of newer iterations in the models, e.g., Llama-3  or Claude-3 , we observe an increase in PAI performance. We provide additional results including individual accuracies and ablations across hardness levels in App. B.1 and App. B.5.

Accuracy Across AttributesWe observe a similar consistency when looking at individual attributes. As we show in Table 4 on GPT-4, the personal attribute accuracies on SynthPAI are close to the numbers reported by Staab et al. , with a maximum deviation of \(<12.5\%\), highlighting that also on attribute level SynthPAI

   Attr. & OCC & SEX & EDU & REL & LOC & INC & POB & AGE \\  Acc. & \(73.9\) & \(92.8\) & \(73.0\) & \(79.2\) & \(80.0\) & \(66.7\) & \(88.0\) & \(69.4\) \\ \(\) & \(+2.3\) & \(-5\) & \(+5.2\) & \(-12.3\) & \(-6.2\) & \(-4.2\) & \(-4.7\) & \(-8.9\) \\   

Table 4: PAI accuracy [%] across attributes for GPT-4  compared to the values reported by Staab et al.  on real-world data.

Figure 4: Personal attribute inference accuracy of \(18\) frontier LLMs on SynthPAI. In line with , GPT-4  is the best performing PAI model. Also, the same scaling laws on model capabilities and PAI performance can be observed as it has been by Staab et al. .

provides representative samples for PAI evaluations. Generally, we observe a slight decrease in accuracy on synthetic texts (often around \(5\%\)). We qualitatively observe that one primary reason for this is that humans occasionally tend to specify personal information in very obvious ways, e.g., simply stating their location, age, and income in a single post-a setting that we explicitly avoid when creating comments in SynthPAI as the resulting inference becomes trivial and hence non-informative. This resulted in a slight overall increase in difficulty for SynthPAI comments. It is important to note, however, that the examples of indirect disclosure (i.e., on comparable hardness) seem qualitatively similar between synthetic and human-written texts. We confirm this in App. B.5, showing that SynthPAI shows very similar hardness scaling behavior to the real-world dataset from Staab et al. .

Inference on Anonymized CommentsFinally, we examine if one of the key findings of  can be reproduced on SynthPAI, i.e., that current anonymization methods can often not prevent PAI inferences. For this, we again follow the setup of , anonymizing all comments using an industry-standard text anonymizer provided by Azure Language Services  (details in App. B.4).

We show our results in Fig. 5. As in , we find that even explicitly removed attributes such as location are still highly inferrable from other, highly contextual clues in the comments, with GPT-4 still retaining around \(50\%\) accuracy (we provide an example of such an inference on anonymized texts in App. D.7). We observe even stronger effects for most other attributes, despite the fact that all of them should have been explicitly removed. Besides the worrying ineffectiveness of industry anonymizers against PAI, the results confirm that the comments in SynthPAI enable us to make very similar conclusions as their real-world counterparts, even when they subjected to noticeable text transformations.

## 5 Discussion

We next discuss the potential impact of our framework and curated PAI dataset, SynthPAI. Finally, we also discuss current limitations and provide future directions for both SynthPAI and PAI as a field.

ImpactSynthPAI constitutes the first synthetic dataset for PAI and thereby addresses a grieving need for openly available datasets [6; 8; 17; 38]. We hope that SynthPAI, as well as the underlying framework, can enable shareable and public future work in PAI, including, hopefully, studies on better defenses against such inferences . While we primarily investigated SynthPAI from the direction of attribute inference, we further believe that our framework can be used to generate synthetic comments for a wide variety of potential downstream applications in a privacy-preserving manner. While there is a potential risk that SynthPAI could be misused to train adversarial PAI models, we believe that SynthPAI can enable important and open PAI research, ultimately leading to better privacy protection.

Limitations and Future WorkWe see several potential directions in which one can improve upon our framework. While we found through manual evaluation that, in line with prior work, model-provided tags are only around \(80\%\) accurate, future work could decrease the human effort required by improving automated tagging accuracy. On the level of individual comments, we noticed that we manually had to adapt a small portion (\(0.3\%=23\) comments), as they contained unusual artifacts from the generating model, e.g., sentences ending in gibberish or random code. Further, while SynthPAI already exhibits high diversity, our framework can easily be extended to support a broader and more realistic range of personal attributes and comment languages. By construction, SynthPAI is limited to a thread-based setting. While we believe this to be particularly valuable for PAI research , we consider exploring fundamentally different domains (e.g., blogs, images ) as highly valuable and interesting future work items. Lastly, to reduce complexity, our framework does not handle certain meta-features present in real-world forums like Reddit, e.g., upvotes. Further aligning our framework with real-world forums could provide interesting future extensions. Additionally, as the general "forum discussion structure" is shared across a wide range of current social media

Figure 5: GPT-4 accuracy [%] on personal attribute inference across SynthPAI after anonymization.

platforms, our multi-agent framework can be adapted straightforwardly to produce authentic synthetic conversation threads for other platforms that follow a similar setup. We consider such extensions a particularly interesting avenue for future work as they allow for potentially larger inter-forum studies.

## 6 Conclusion

In this work, we introduced a framework to generate synthetic data for Personal Attribute Inference (PAI) research. Using this framework, we constructed SynthPAI, a PAI dataset consisting of 7823 comments over 103 threads and 300 synthetic profiles alongside with human-verified attribute labels. In our experimental evaluation, we showed that comments in SynthPAI exhibit high fidelity (with humans being unable to distinguish them from real comments) and informativeness for PAI research, allowing us to draw the same qualitative conclusions as on real-world data across a various experimental settings. We believe our framework and SynthPAI as an instantiation constitute strong and open baselines for future work on the emerging issue of LLM-based personal attribute inferences.