# WordScape: a Pipeline to extract multilingual, visually rich Documents with Layout Annotations from Web Crawl Data

WordScape: a Pipeline to extract multilingual, visually rich Documents with Layout Annotations from Web Crawl Data

 Maurice Weber

ETH Zurich

Carlo Siebenschuh

University of Chicago

&Rory M. Butler1

University of Chicago

&Anton Alexandrov

ETH Zurich,

INSAIT, Sofia University

Valdemar R. Thanner

ETH Zurich

Georgios Tsolakis

ETH Zurich

&Haris Jabbar

TU darmstadt

&Ian Foster

Argonne National Laboratory,

University of Chicago

&BoL

UIUC

&Rick Stevens

Argonne National Laboratory,

University of Chicago

&Ce Zhang

ETH Zurich

{maurice.weber,ce.zhang}@inf.ethz.ch;{siebenschuh,rorymb}@uchicago.edu;

{aalexandrov, thannerv, gtsolakis}@student.ethz.ch;

harisjabbar@gmail.com;{stevens,foster}@anl.gov; lbo@illinois.edu

###### Abstract

We introduce WordScape, a novel pipeline for the creation of cross-disciplinary, multilingual corpora comprising millions of pages with annotations for document layout detection. Relating visual and textual items on document pages has gained further significance with the advent of multimodal models. Various approaches proved effective for visual question answering or layout segmentation. However, the interplay of text, tables, and visuals remains challenging for a variety of document understanding tasks. In particular, many models fail to generalize well to diverse domains and new languages due to insufficient availability of training data. WordScape addresses these limitations. Our automatic annotation pipeline parses the Open XML structure of Word documents obtained from the web, jointly providing layout-annotated document images and their textual representations. In turn, WordScape offers unique properties as it (1) leverages the ubiquity of the Word file format on the internet, (2) is readily accessible through the Common Crawl web corpus, (3) is adaptive to domain-specific documents, and (4) offers culturally and linguistically diverse document pages with natural semantic structure and high-quality text. Together with the pipeline, we will additionally release 9.5M urls to word documents which can be processed using WordScape to create a dataset of over 40M pages. Finally, we investigate the quality of text and layout annotations extracted by WordScape, assess the impact on document understanding benchmarks, and demonstrate that manual labeling costs can be substantially reduced.

## 1 Introduction

There is an abundance of digital, semi-structured data contained in visually rich documents such as PDFs or MS Word documents. However, while this information is easily understood by humans, its semi-structured nature makes its analysis by automated data processing engines difficult. This difficulty stems, to a large extent, from the diversity of how information in visually rich documents is organized, across at least the three axes of culture, language, and industry. Therefore, effective use of such data often necessitates a costly and labour-intensive process of manual information extraction. Existing techniques in many automated document understanding tasks are either based on conventional rule-based or machine learning (ML) approaches, relying on hand-crafted features, or on more promising deep learning approaches which are trained on large amounts of data. However, to date, both approaches often fail to generalize well due to a lack of compatible formats, or due to insufficient diversity in existing training datasets, especially for low-resource languages.

At the same time, we have witnessed tremendous progress in the area of natural language processing (NLP) and computer vision applications, where pre-trained models [8; 25; 31; 24; 27] have enabled researchers and practitioners to build useful applications by fine-tuning these models on specific downstream tasks. This progress has, to a large extent, been driven by leveraging large quantities of high-quality data extracted from the web.

In contrast to these techniques, the task of document understanding is an inherently multimodal problem, requiring models to understand text, visual, and layout features and to model their relations [13; 22; 17]. As a consequence, next to advances in model design, considerable efforts have gone into creating datasets that combine these different modalities. These datasets can be grouped into two categories: on the one hand, there are human-labeled datasets like DocLayNet  for document layout analysis, FUNSD  for form understanding or the RVL-CDIP document classification dataset . While this approach generates high-quality labels, it naturally limits the size to a few hundred thousand samples due to the restrictive costs of human-annotated labels. Alternatively, the automatic generation of ground truth labels has been leveraged to create datasets like PubLayNet , DocBank  and arXivdocs-weak . While these datasets are larger in size, they are typically sourced from the scientific domain and are thus mainly in the English language and lack the diversity needed to reflect the true distribution of documents prevalent in practice and across industries, cultures, and societies.

Word documents are among the most widely used types of documents. While rendered Word documents appear as semi-structured pdfs, the source code comprising Word documents consists of

Figure 1: Overview over the WordScape pipeline for processing a single Common Crawl snapshot. First, we extract and deduplicate all URLs from.wat files that point to Word documents. We then download the documents, apply malware filters, metadata extraction and deduplicate based on content. In the third step, we convert downloaded documents to page images, extract text, run our bounding box annotation algorithm, identify the dominant language and apply quality filters. The right side of the figure shows an example document with bounding box annotations.

highly structured XML files in the Open XML format, containing valuable information like reading order, the structure of tables and style information. Furthermore, Word documents are often used in more formal contexts and for professional writing such as academic papers, reports, business documents or official correspondence. This gives rise to the hypothesis that text found in such documents generally has higher quality than text on web pages like forums, social media updates or user-generated content. Here, we present WordScape, a pipeline that enables the automatic sourcing and annotation of diverse, multilingual, visually rich documents at scale, enabling researchers and practitioners to curate multimodal document understanding datasets. Similar to large-scale NLP dataset creation pipelines like CCNet, we use the Common Crawl web corpus 2 as our primary source of documents. We parse Common Crawl to collect urls pointing to MS Word documents embedded in websites, then parse the Open XML structure of these documents to extract text and identify the location and category of visual semantic entities like section headings, tables and figures on the rendered page images. To date, we extracted a total of 9.5M urls to Word files which we will publish and which can be processed using the WordScape pipeline to build a corpus of roughly 40M pages. In summary, we make the following contributions:

* We present a novel pipeline to automatically extract and process millions of MS Word documents from the web, and open-source the codebase 3 * We introduce a novel bounding box labeling algorithm based on the Open XML representation of MS Word documents.
* We provide a detailed analysis of the size, quality and distribution of datasets created using WordScape.
* We validate one created dataset on various layout analysis benchmarks and find that WordScape annotations can substantially reduce manual labeling efforts.
* We will release 9.5M urls to word documents that we have collected from Common Crawl. These can be processed using WordScape to create a dataset of over 40M pages.

The remainder of this paper is organized as follows: In Section 2 we discuss related work. In Section 3 we present our data creation pipeline and dataset metrics are presented in Sections 4 and 5. We validate our dataset on downstream benchmarks in Section 6, and finally conclude in Section 7.

## 2 Related Work

With the proliferation of pre-trained deep learning models, there has been a growing focus on high-quality, web-scale training datasets, both in the domains of natural language processing and computer vision. In the NLP domain, notable advances include the CCNet pipeline, C4 [25; 34], OpenWebText , Pile v1 , S2ORC , Pythia , the RedPajama dataset , or the Refined Web dataset . Similar to these datasets and pipelines, this work aims to leverage data that is publicly available on the web to build a large-scale, diverse, and multilingual data corpus. However, here we focus on visual document understanding, an inherently multimodal domain, where next to text, layout and visual features of documents are also a valuable source of data for downstream models.

Perhaps more similar to this work are multimodal web-scale datasets and pipelines like the Laion-400M and Laion-5B datasets [30; 29], or the multimodal mmC4 . In contrast to these works, we focus on visually rich documents with layout annotations, rather than _(image, caption)_-pairs or text interleaved with images. There are multiple datasets in the visual document understanding domain such as the manually annotated DocLayNet  for document layout analysis, FUNSD  for form understanding or the RVL-CDIP document classification dataset . Other notable datasets include the automatic, weakly labelled PubLayNet , DocBank , TableBank  and arXivdocs-weak  datasets. In a similar manner to our approach, the LayoutReader dataset  leverages the Open XML format of Word documents to construct a multimodal dataset of visually rich documents together with the text in reading order. However, LayoutReader does not include object detection labels for semantic entities and is released as a fixed dataset, rather than as a dataset creation pipeline. In Table 1, we show a detailed comparison between WordScape and other datasets and pipelines.

## 3 Methodology

Our document processing pipeline builds on the Open XML structure of Word documents which contains valuable semantic information, and the hypothesis that the quality of text contained in such documents is generally higher than text found on HTML-based websites. As our primary source of data, we use the Common Crawl web corpus consisting of regular snapshots of the web with little overlap between different snapshots 4, dating back to 2013. On a high level, our pipeline consists of three core steps: we first parse a Common Crawl snapshot and extract all links that point to Word files (i.e. URLs that end in.docx or.doc). The second step is to send HTTP requests to these links and download the corresponding Word file. The final step consists of processing the document, resulting in a final multimodal dataset with page images, text contained on each page, and bounding box annotations for semantic entities like headings and tables on each page. An overview of the pipeline is shown in Figure 1. In this section, we present each step in more detail.

### Parsing of Common Crawl

The first step in the WordScape pipeline is to extract urls that point to Word files from Common Crawl snapshots. Common Crawl provides data in raw (warc) format, UTF-8 encoded text (wet) and metadata (wat) files. Next to HTTP header data, each metadata file also contains a list of hyperlinks, from which we select all HTTP urls that end in.doc or.docx. Each wat file must be downloaded in its entirety to be correctly parsed. After the initial parsing of the wat files, the urls from each agent are merged, then deduplicated on a per-snapshot basis and finally deduplicated globally across all previously processed snapshots.

### Document Download

In this step of the WordScape pipeline, we download the documents from the urls extracted from Common Crawl. This step outputs the downloaded Word source files, as well as metadata containing statistics on the quantity of successfully downloaded urls and failure or rejection reasons for each url. Failures relate mainly to benign HTTP errors. We reject a document when a response is successfully obtained but is not useful. A benign reason for rejection is either an unsuccessful HTTP response code (most commonly 403/404), an invalid URL, too many redirects/retries, no received HTTP response, an incorrect file format of the response, an incorrect content-type header, or internal hardware failure/cluster resource limitations. We furthermore reject potentially malicious documents by performing a check against OLE data structures  during download. As such we count any document that contains VBA code/macros, external relations, an OLE object pool, encryption, or flash embeddings.5 Finally, we also reject excessively large files. The reason for this is that we aim to achieve a relatively even distribution of document pages, and to prevent out-of-memory errors.

  
**Dataset** & **Pages** & **Classes** & **Annotation** & **Format** & **Document Types** & **Languages** & **Source** \\  PubLayNet  & 360k & 5 & Automatic & Digital & Scientific articles & English & PubMed Central \\  DoxBank  & 500k & 13 & Automatic & Digital & Scientific articles & English & arXiv \\  arXitodex-weak  & 127,472 & 23 & Automatic & Digital & Scientific articles & English & arXiv \\   &  &  &  &  & Magazines, Technical Articles, Forms, Bank Statements, Advertisements, & English &  \\   &  &  &  &  &  &  \\  & & & & & & & & \\  & & & & & & & & \\  & & & & & & & & \\  ^{}\)} &  &  &  & Digital, Scan, Photographs & Scientific articles, Textbooks, Books, Text papers & English, Chinese & Chinese & People’s daily, arXiv, VKontake \\  & & & & & & Magazines, Newspapers, Notes & & \\  WordScape (Ours) & 9.5M urls\({}^{(1)}\) & 30 & Automatic & digital & \(>\) 28 Categories\({}^{(3)}\) & \(>\) 136 languages\({}^{(3)}\) & Common Crawl \\   

Table 1: Comparison with existing document layout datasets. (1) WordScape is released as a public pipeline together with 9.5M document urls; (2) 28 top-level categories detected via hierarchical topic modelling; (3) languages detected with fastText on a single common crawl snapshot [16; 15].

Upon a successful download, we save several metadata fields concerning the response and its analysis such as HTTP status, OLE information, as well as the file itself. In addition, the metadata includes a SHA-256 hash of the full response bytes: This allows us to perform a second global deduplication step against files which have identical content but are accessible under different urls, and to ensure that the document has not changed if it is downloaded a second time. This serves as a defense against potential dataset poisoning attacks, which have recently been shown to be practical in the context of web-scale datasets . Finally, the temporary metadata files recorded by the agents are merged, deduplicated via bytehash, and written to a database. We present metrics on this metadata in Section 4.

### Document Processing

The processing of Word documents consists of several steps, including language identification, bounding box annotation and text extraction. Here we describe each step in detail.

#### 3.3.1 Bounding Box Annotation

Similar to the approach from [19; 18], we use a colorization scheme to extract bounding boxes of semantic entities from a document page. In the first step, we parse the highly structured Open XML files of a Word document using the python-dox6 library and custom XML parsing code to identify the categories of different elements in the document.

We identify such elements by one of two methods: If the Word user has either used a built-in style (such as a heading formatter), or the element is natively tagged in the XML file (such as for tables), we use this information to label the corresponding element. Clearly, this approach makes the assumption that using such a built-in functionality reflects the user's intent, which is not always the case. Nevertheless, we expect this methodology to be accurate in the majority of cases. If, on the other hand, no built-in indicator can be found for an element, we fall back to heuristics, such as the distribution of used fonts in the document indicating headings, or successive numbered or bulleted paragraphs indicating a succession of list items. This heuristics-based approach is generally more noisy compared to the method based on built-in XML properties.

Once the category of a document element has been determined, we color it using the Open XML formats highlighting, formatting and text coloring features, by directly editing the XML. The colored document is then rendered via LibreOffice 7, and each page is converted to an image. Colors on this image are then detected, providing bounding boxes for each different entity category. We provide more details on the annotation process in the Appendix.

#### 3.3.2 Text Extraction

We extract text from a document on two levels of granularity. First, we extract the full document text from the Open XML structure using python-dox. This document-level text is in reading order, due to the internal XML structure. Second, we extract the text from individual rendered pages using the PDFPlumber8 package. This allows us to additionally extract word-level bounding boxes. It should be mentioned that the PDF-based extraction is less accurate due to the necessity to use heuristics when identifying and grouping characters into words. We discard any document that has less than a total of 200 characters. On the page level, we keep pages without any text as they might contain figures or other relevant entities without text.

#### 3.3.3 Language Identification

To identify the language of a document, we use the fastText language classifier [16; 15]. The classifier was trained on Tatoeba, Wikipedia and SETimes and can identify 176 languages using \(n\)-grams as features with the hierarchical softmax. We identify languages both on a document level, using the Open XML-based text, and on a page level using the PDF-based text.

#### 3.3.4 Dataset Filters

We provide several ways to filter a subset of the core dataset created by WordScape, based on metadata collected during the annotation process. First, we implement a quality filter based on the perplexity of the document text for models trained on Wikipedia, as well as a annotation reliability score to assess bounding box annotations. The latter metric captures the proportion of entities annotated using built-in or XML patterns vs. heuristic-based annotations, as the former are generally more reliable. We present more details on the perplexity distribution in Section 5 and details on the annotation quality score in the Appendix. In addition, we collect metadata for each document and each page, allowing the creation of subsets with different requirements such as the number of tables and other entities, or the language of the resulting dataset.

## 4 Pipeline Statistics

In this section, we present statistics on running the WordScape pipeline. Specifically, we investigate the number of links pointing to MS Word files in a single common crawl snapshot, as well as the reasons that downloads failed, or were otherwise rejected/failed during the annotation process. We provide details on the resources used to run the WordScape pipeline in the Appendix.

Common Crawl ParsingTo estimate the number of documents that can potentially be obtained using the WordScape pipeline, we parsed 6 individual Common Crawl snapshots ranging from 2013 up to the March/April 2023 snapshot. We found that there are substantial duplicated Word file urls in each snapshot: per-snapshot deduplication removes \(60-80\%\) of available urls. However, we found little overlap _between_ snapshots, mirroring the fact that there is generally little overlap between the websites visited by Common Crawl. Furthermore, we noticed that the number of valid urls, i.e. where a document can be successfully downloaded, decreases substantially for older crawls: Out of all the urls visited for the 2013 snapshot, only \(12.5\%\) could be successfully downloaded, compared to \(60.6\%\) of urls from the most recent 2023 snapshot. This is to be expected as older urls are more likely to be inaccessible than newer ones. These observations are illustrated in figure 2.

Document DownloadOut of a total of \(5,807,634\) requests to urls from the November/December snapshot, we found that \(2,441,972\) (\(42.1\%\)) received a 200 return code and could thus be further processed. Out of the successful responses, there were \(364,648\) (\(14.9\%\)) instances where the content-type header did not match a Word document and was thus rejected. Another \(172,772\) (\(7.1\%\)) documents were rejected because they did not pass our OLTools malware filter. This resulted in a total of \(1,904,552\) Word documents that could be successfully downloaded using the WordScape pipeline. We emphasize that we ran the requests at the beginning of March 2023, i.e. roughly three months after the snapshot was published. It is likely that at a later point in time, the number of responsive urls will be lower, leading to less documents. We provide further details on reasons for rejected downloads in the Appendix.

Figure 2: Number of urls and documents extracted from Common Crawl, for snapshots ranging from 2013 to 2023. Raw urls refers to the initial urls extracted from each snapshot, prior to any processing. The unique urls are globally deduplicated, starting from the most recent snapshot back to the oldest. Processed urls is the subset of urls to which an HTTP request was sent, and “documents” refers to successfully downloaded documents.

Document ProcessingTo further investigate how the WordScape pipeline performs, we ran the document processing step of the pipeline on \(1,251,383\) of the successfully downloaded documents from the November/December 2022 snapshot. Out of these, for \(248,918\) (\( 19.9\%\)) of the documents, the annotation process was either rejected or failed. The majority of the failures stem from the files being invalid Zip files, namely in \(15.4\%\) of all processed documents. Another \(37.5k\) (\( 3\%\)) of the documents were rejected because they contained less than \(200\) characters. We furthermore rejected all documents whose uncompressed file size was more than \(20\) times the compressed size as this indicates a potentially malicious zip bomb. This process resulted in \(1,002,465\) annotated documents, or \(5,481,455\) pages, including the document text and object detection bounding boxes.

## 5 Dataset Statistics

Language DistributionIn the \(5.5\)M annotated document pages, we found a total of 136 distinct languages, identified with fastText [16; 15]. The page counts per language is highly skewed towards high-resource languages like Russian (2M pages) and English (1M pages), as opposed to roughly 1k pages for Tajik and Urdu. Figure 3 shows the number of pages for the 50 most frequent languages.

Perplexity ScoresWe use the perplexity of a language model trained on a target domain to measure the quality of the text extracted using WordScape. We follow the approach used in  and use their \(5\)-gram Kneser Ney models and SentencePiece tokenizers trained on Wikipedia. In this context, a lower perplexity score indicates that the language is closer to the target domain and is thus expected to be of higher quality. Figure 4 shows the number of words with at most a certain perplexity value. We note that especially for Hungarian, Portuguese and Italian, the perplexity scores are relatively low, and a large part of the corpus can be retained even when aggressively filtering out documents with moderately high perplexity. We provide further figures that illustrate the perplexity distributions for more languages in the Appendix.

Semantic Entity DistributionSemantic Entities like headings, tables and lists are the logical units that build up the structure of a document. Here we present statistics on the semantic entities that the WordScape pipeline annotates. This analysis is based on the \(1.25\)M documents annotated from

Figure 4: Number of words in any given language subset, as a function of perplexity threshold. The figure shows the number of words with perplexity smaller or equal to the value on the x-axis, for the seven languages with the highest (left) and lowest (right) number of words. A word is defined as a whitespace-delimited sequence of characters with punctuation removed.

Figure 3: Number of pages per language, produced by the WordScape pipeline run on \(1.25M\) Word documents extracted from the November/December 2022 Common Crawl snapshot.

the November/December 2022 snapshot, containing a total of \(173\)M entity bounding boxes. The most frequently appearing category are table cells (\(86\)M individual cells), making up roughly \(50\%\) of the entities. Table columns and rows also appear frequently, comprising another \(20\%\) of all entity bounding boxes. The next most frequent category are list items, of which we found \(15\)M (\(8.9\%\)). Further frequent categories are plain text (\(14.8\)M) and headings (\(7.6\)M). We found that, generally, the entity distribution is highly imbalanced; however, when excluding the individual table elements, the distribution flattens significantly.

In Figure 5 we show the semantic entity distribution, grouped by languages and where we have excluded table elements and merged the different heading levels into a single category. We see that the imbalanced nature of the categories persists across all languages considered. However, there are differences between languages in regard to the extent to which the classes are imbalanced (e.g. Russian documents are more imbalanced than French documents). To whether the pairwise occurrence of layout elements is correlated, we compute Spearman's rank correlation coefficient for pairs of layout elements in Figure 10 in the Appendix. We found that the elements are generally weakly correlated, except for (text, heading), (list, heading), (list, text), (form_field, text) and (footer, header). Further details on the semantic entities are in the Appendix.

Topic ModelingSince Common Crawl snapshots cover websites across multiple, unfiltered domains, the Word documents extracted via WordScape can potentially themselves also cover a wide range of topics. To get a better understanding of the topic distribution in WordScape, we ran the hierarchical topic modelling classifier available in the Google Cloud NLP API 9 over a 25k sample of WordScape documents in 11 languages support by the API (ru, en, it, ja, es, nl, zh, pt, ko, fr, de). This allows for a fine-grained analysis giving both a high-level overview of the topics in the dataset, and also exposes more low-level details of sub-classifications. The top categories we found are "/Law & Government/Government/Other" (15.3%), "/Reference/General Reference/Forms Guides & Templates" (9.0%), "/Jobs & Education/Education/Primary & Secondary Schooling (K-12)" (5.6%), "/People & Society/Religion & Belief" (3.5%), and "/Jobs & Education/Education/Colleges & Universities" (3.4%). The full hierarchy is shown in Figure 6. We furthermore split the analysis across languages that were found significant differences in topic distributions. While the most frequent top-level category in both Russian and Portuguese is "Law & Government", accounting for \( 37\%\) of documents, this category occurs relatively infrequently in other languages (\(<13\%\)). We provide a more fine-grained overview over the language specific topic distributions in the Appendix.

Figure 5: Proportion of layout element categories for the seven most frequent languages.

Figure 6: Hierarchy of topics detected in the WordScape.

Training Object Detection Models on WordScape

To further assess the quality of the bounding box annotations extracted by WordScape, we conduct experiments on three different datasets. We measure the utility of the annotations by first training a base model on WordScape annotations, then finetuning the model on a target benchmark. We attempt to show that, by leveraging the automatically annotated labels produced by WordScape, we can reduce the (manual) labeling cost on the target domain while still maintaining the original performance.

Text Detection on FUNSDWe first consider the word-level text detection task on the FUNSD dataset . This dataset is a subset of the RVL-CDIP  dataset and comprises 199 manually annotated, scanned forms. We trained a Faster R-CNN  network on 0 to 100k pages, annotated with word-level bounding boxes. In the second step, we finetuned the resulting model on 25 - 149 samples of the FUNSD dataset. In Table 2 we report the F1 score with IoU threshold 0.5. We can see that using only 10k WordScape samples and 25 finetuning samples substantially surpasses the text detection accuracy of the model trained on the full FUNSD dataset. By using WordScape annotations we can thus decrease the labeling cost 6-fold.

Table Detection on ICDAR 2019 cTDaRHere we present results on the ICDAR 2019 cTDaR table detection task . We use the modern tables subset, the domain of which is closer to the WordScape domain, compared to the archival subset. It includes 600 training images and 240 test images. We compare mAP @ IoU [0.50:0.95] for the Ultralytics YOLOv8m10 model on the test set with and without any training on WordScape table documents. We resize images to \(640 640\) resolution and train 4 models with different training set sizes for 200 epochs using SGD, 0.01 learning rate, batch size of 16, 0.937 momentum and \(5e-4\) weight decay. We then finetune each model on 4 different subset sizes of cTDaR using AdamW with \(5e-4\) learning rate for 200 epochs, or until no improvement is observed for more than 30 epochs. We see that pre-training on WordScape improves results particularly in the low-resource regime.

Layout Analysis on DocLayNet  is one of the largest human-annotated document layout segmentation datasets, containing over 80k pages from a variety of document sources. We train a YOLOv5 object detection model on 200k images obtained via WordScape, and then fine tune the model on subsets of the DocLaynet training split, varying the fine tuning dataset sizes from \(1k\) to the full \(69k\). The results are shown in Table 4, where we see that pre-training on WordScape leads to consistent performance improvements compared to (1) using random weights for initialization, and (2) pretraining with the same number of samples on the PubLayNet  dataset. This is particularly pronounced when less human labelled data is available.

Handcrafted Scientific DatasetWordScape's versatility arises from enabling access to multilingual document pages with rich category structure. However, assessing its quality requires an

    & \(N_{f}=1k\) & \(N_{f}=5k\) & \(N_{f}=20k\) & \(N_{f}=69k\) \\  Random Initialization & 0.299 & 0.553 & 0.727 & 0.753 \\ PubLayNet (200k) & 0.467 & 0.659 & 0.720 & 0.745 \\ WordScape (200k) & **0.508** & **0.679** & **0.734** & **0.755** \\   

Table 4: Document Layout Analysis mAP @ IoU [0.50:0.95] for YOLOv5 on DocLayNet with different pretraining datasets. \(N_{f}\) is the number of finetuning samples.

    & \(N_{f}=25\) & \(N_{f}=50\) & \(N_{f}=100\) & \(N_{f}=149\) \\  \(N_{p}=0\) & 0.621 & 0.690 & 0.723 & 0.772 \\ \(N_{p}=10k\) & 0.840 & 0.840 & 0.823 & 0.861 \\ \(N_{p}=50k\) & 0.868 & **0.870** & **0.857** & 0.869 \\ \(N_{p}=100k\) & **0.872** & 0.869 & 0.850 & **0.882** \\   

Table 2: Text detection F1 @IoU 0.5 for Faster R-CNN on FUNSD. \(N_{p}\) are the WordScape samples; \(N_{f}\) the FUNSD samples.

    & \(N_{f}=75\) & \(N_{f}=150\) & \(N_{f}=300\) & \(N_{f}=600\) \\  \(N_{p}=0\) & 0.869 \(\) 0.008 & 0.888 \(\) 0.011 & 0.949 \(\) 0.006 & 0.974 \(\) 0.003 \\ \(N_{p}=1.25k\) & 0.906 \(\) 0.012 & 0.912 \(\) 0.011 & 0.951 \(\) 0.005 & 0.972 \(\) 0.003 \\ \(N_{p}=2.5k\) & 0.914 \(\) 0.009 & 0.929 \(\) 0.008 & 0.960 \(\) 0.004 & 0.974 \(\) 0.003 \\ \(N_{p}=5k\) & **0.924** \(\) 0.007 & 0.924 \(\) 0.011 & 0.956 \(\) 0.005 & 0.974 \(\) 0.003 \\ \(N_{p}=10k\) & 0.919 \(\) 0.006 & **0.931** \(\) 0.010 & **0.961** \(\) 0.005 & **0.975** \(\) 0.003 \\   

Table 3: Table detection mAP @ IoU [0.50:0.95] for YOLOv8m on ICDAR 2019 cTDaR. \(N_{p}\) are the WordScape samples, \(N_{f}\) the cTDaR samples.

equally refined dataset for downstream tasks. The growing interest in layout detection as a precursor to multimodal document understanding makes scientific literature a particularly viable candidate.

We compiled a dataset of diverse scientific content of \(N_{f}=2,500\) pages from eight scientific domains (biology, chemistry, physics, mathematics, engineering, computer science, economics, and medicine) that span 120 subdomains from abstract algebra to zoology. More than 41,000 instances were annotated by humans. In total, there are 31 categories that can be grouped into metainformation (e.g. title), text body (e,g, paragraph), code (e.g. pseudocode), mathematical content (e.g. equation), and visual assets (e.g.table). Categories relating to text are further split by indicating the presence of LaTeX-formatted symbols. In addition, categories are present that relate captions to the respective figure or table. Finetuning a model on an information-dense, annotation-rich scientific corpus is a formidable challenge for a variety of reasons. It is particularly daunting for a model pre-trained on Wordscape, however, due to the (1) multi-disciplinary, information-dense scientific content, (2) the extensive, hierarchical class labels, (3) the high-resolution images stored as PNG rather than JPG, (4) the human-made annotations and (5) the distributional shift from multi-lingual word documents to English-only PDFs.

DETR  and the current iteration of YOLO represent state-of-the-art choices in terms of accuracy and latency, respectively. In our experiment, both models are pre-trained on pages stemming from WordScape with sample sizes ranging from \(1000\) to \(100\)K. Subsequently, the models are finetuned on our handcrafted scientific dataset for up to \(2,000\) pages. The empirical results are shown in figure 7 and indicate that pre-training on WordScape significantly reduces the need for downstream data.

## 7 Discussion

In this paper, we present a pipeline to create curated datasets consisting of high-quality, multilingual and diverse, visually rich documents with layout annotations. The pipeline is scalable to millions of pages and contains high-quality text, both for high- and low-resource languages. WordScape is the first pipeline that enables the creation of training datasets for large-scale multimodal document understanding models that fuse text, visual and layout features. As the main limitation of the pipeline, we identify the reliability of the bounding box annotations for certain semantic entities like headings, as they rely to some extent on the assumption that formatting correlates reasonably well with user intent. In the future, we wish to explore more characteristics of the resulting dataset such as the amount of toxic or offensive content, and train large-scale document understanding models that make full use of both text and image modalities in multiple languages. We are also excited to see how this dataset can further enhance existing web-scale multimodal and NLP datasets.

Figure 7: Layout analysis mAP @ IoU [0.50:0.95] on the scientific paper dataset with YOLOv8 (left) and DETR (right) for varying WordScape sample sizes. The figures show mean values for smaller (2.5k, 5k, 7.5k, red) and larger (15k, 20k and 25k, blue) pretraining sizes. The confidence bands arise as estimates of the quartiles \(q_{0.25}\) and \(q_{0.75}\).