# DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion

Ke Sun\({}^{1}\), Shen Chen\({}^{2}\), Taiping Yao\({}^{2}\), Hong Liu\({}^{3}\),

**Xiaoshuai Sun\({}^{1}\)**, **Shouhong Ding\({}^{2}\)**, **Rongrong Ji\({}^{1}\)**

\({}^{1}\) Key Laboratory of Multimedia Trusted Perception and Efficient Computing,

Ministry of Education of China, Xiamen University, 361005, P.R. China.

\({}^{2}\) Youtu Lab, Tencent, P.R. China.

\({}^{3}\) Osaka University, Japan.

Corresponding Author.

###### Abstract

The rapid progress of Deepfake technology has made face swapping highly realistic, raising concerns about the malicious use of fabricated facial content. Existing methods often struggle to generalize to unseen domains due to the diverse nature of facial manipulations. In this paper, we revisit the generation process and identify a universal principle: Deepfake images inherently contain information from both source and target identities, while genuine faces maintain a consistent identity. Building upon this insight, we introduce DiffusionFake, a novel plug-and-play framework that reverses the generative process of face forgeries to enhance the generalization of detection models. DiffusionFake achieves this by injecting the features extracted by the detection model into a frozen pre-trained Stable Diffusion model, compelling it to reconstruct the corresponding target and source images. This guided reconstruction process constrains the detection network to capture the source and target related features to facilitate the reconstruction, thereby learning rich and disentangled representations that are more resilient to unseen forgeries. Extensive experiments demonstrate that DiffusionFake significantly improves cross-domain generalization of various detector architectures without introducing additional parameters during inference. Code are available in https://github.com/skJack/DiffusionFake.git.

## 1 Introduction

The rapid progress in AI-generated content (AIGC) has led to the emergence of highly sophisticated forged face content, making it increasingly challenging for humans to distinguish between genuine and forged faces . Face swapping, also known as _Deepfakes_, is one of the most well-known techniques for generating forged facial images. It replaces the face of a target individual with that of a source person to create a seamless and realistic composite image . The widespread proliferation of Deepfakes content on social media platforms has raised significant security concerns, including the spread of disinformation, fraud, and impersonation. As a result, developing effective and generalizable face forgery detection methods to counter these malicious attacks has become a critical challenge in the field of computer vision.

The growing diversity of facial forgery techniques has spurred interest in the general face forgery detection task , which aims to develop models that detect forgeries from unseen domains. Previous approaches primarily utilize forgery simulation  to augment data by simulating various forgery traces, or framework engineering to enhance generalization through specialized designs like contrastive learning, attention mechanisms, and reconstruction learning .

However, their generalization capabilities remain limited due to the reliance on simulating specific forgery artifacts or designing specialized architectures tailored to certain manipulation techniques.

In this paper, we aim to identify the universal features common to all Deepfake faces by revisiting the generative process underlying forged face images. As depicted in Figure 1 (a), this process can be distilled into two key steps: (1) a feature extractor module captures salient features from both the source and target images; (2) these features are seamlessly fused through a generalized feature blending module to synthesize a novel Deepfake image. While the specific implementation of feature extraction and fusion may vary across different forgery methods, ranging from learning-based to graphics-based approaches, they all adhere to this fundamental generative paradigm.

Through this analysis, we uncover a crucial insight: Deepfake images inherently amalgamate information from both source and target faces, whereas genuine images maintain a consistent identity throughout. This amalgamated information can manifest as low-level artifacts, such as injection noise patterns and spectral discrepancies, or as high-level attributes, including facial expressions and mouth movements, depending on the specific forgery method employed.

Building upon this insight, we raise a question: Can we invert the generative process to extract and leverage the amalgamated source and target features, thereby enhancing the generalization capability of existing forgery detectors?

To answer this question, we introduce DiffusionFake, a novel plug-and-play framework that harnesses the power of Stable Diffusion to guide the forgery detector in learning disentangled source and target features inherent in Deepfakes. The core idea behind DiffusionFake is to inject the features extracted by the detector into a frozen pre-trained Stable Diffusion model, compelling the detector to capture the amalgamated source and target information by optimizing the features to reconstruct the corresponding source and target images.

As illustrated in Figure 1 (b), DiffusionFake is a plug-and-play framework that can be seamlessly integrated into existing forgery detectors. The features extracted by the encoder are first passed through the Target and Source Transformation modules, which filter and weight the features to obtain target and source-related representations. These features are then injected into the Stable Diffusion model using a Guide Module, leveraging its pre-trained knowledge to reconstruct the corresponding source and target images and optimize the feature representation.

During inference, only the encoder and classification modules are used, ensuring no additional parameters or computational overhead. By compelling the detector to learn more discriminative and generalized features, DiffusionFake enhances its ability to handle unseen forgeries without compromising efficiency. For example, when integrated with EfficientNet-B4, DiffusionFake improves AUC scores on unseen Celeb-DF dataset by around 10%, demonstrating its effectiveness in enhancing the generalization capability of existing detectors.

The main contributions of our work can be summarized as follows:

* We analyze Deepfake images from a generative perspective and propose a framework that leverages the reverse generation process to enhance the generalization capabilities of face forgery detectors.

Figure 1: Pipeline of the generation process of Deepfake (a) and our proposed DiffusionFake (b).

* We introduce the DiffusionFake framework, a plug-and-play model that integrates a frozen pre-trained Stable Diffusion network to guide the forgery detector in learning disentangled source and target features inherent in Deepfakes, further enhancing the generalization.
* Extensive experimental validations demonstrate that the DiffusionFake framework significantly improves generalization capabilities across various architectures without introducing additional inference parameters.

## 2 Related Work

### General Face Forgery Detection

General face forgery detection aims to improve the performance of forgery detectors on unseen domains and become one of the most critical issues in this field. Previous work to enhance generalization can be broadly divided into two categories: forgery simulation and framework engineering. The former utilizes data augmentation methods to simulate certain forgery traces, such as blending artifacts [13; 22; 35], Inconsistency between internal and external faces , subtle jitter and blur traces , and fine-grained facial disharmony [3; 38]. The latter improves network architectures or training procedures to help capture more generalized traces. Such methods approach the problem from different angles. Some employ attention mechanisms to enhance the capture of forgery traces [50; 39; 47; 33], while others improve generalization by jointly modeling frequency and spatial domains [29; 21; 28; 25]. Reconstruction-based methods enhance discriminability against unseen domain forgeries via modeling genuine faces [5; 2; 34]. Additionally, some approaches use implicit identity as a clue to improve the generalization of Deepfake faces [17; 9] and some explore the local and global relationships of unseen forgeries[4; 1; 45; 13; 10; 27]. Furthermore, decoupling methods[24; 32; 14; 20], such as ICT  and UCF , aim to enhance generalization by disentangling different facial information. Our DiffusionFake method addresses this by reversing the forgery process and leveraging pre-trained generative models to complete missing information, enhancing the capture of source-related and target-related features.

### Diffusion Model

Diffusion models have emerged as a powerful framework for image generation and manipulation. The seminal work on Denoising Diffusion Probabilistic Models (DDPM)  introduced a novel approach to learn the data distribution by iteratively denoising a Gaussian noise signal. This process allows for high-quality image generation but requires a large number of sampling steps. To address this issue, the Denoising Diffusion Implicit Models (DDIM)  proposed a deterministic sampling process that significantly accelerates the generation process while maintaining image quality. Building upon these advancements, the Latent Diffusion Model (LDM)  combines the strengths of Variational Autoencoders (VAEs)  and diffusion models. By applying the diffusion process in the latent space learned by a VAE, LDM substantially reduces the computational cost and memory requirements during training. This innovative architecture has given rise to powerful AIGC pre-trained generative models, such as Stable Diffusion 2, which enable high-quality image generation and manipulation with unprecedented efficiency. Recent developments in controllable diffusion models have further expanded their applicability. ControlNet introduces a mechanism to guide the image generation process by conditioning the diffusion model on additional control signals, such as segmentation masks or edge maps. Inspired by ControlNet, our DiffusionFake leverages a guide module to inject the source and target-related features into Stable Diffusion to reconstruct the corresponding images.

## 3 Methodology

Figure 2 illustrates the detailed framework of our proposed DiffusionFake method, which aims to enhance the generalization capability of forgery detectors by guiding the learning of amalgamated source and target features through a frozen pre-trained Stable Diffusion model. Specifically, the features extracted by the encoder are first filtered and weighted by the Feature Filter and Weight Modules to obtain source and target-related representations. These features are then injected into a

frozen pre-trained Stable Diffusion model via the Guide Module, which reconstructs the corresponding source and target images, compelling the encoder to learn rich and discriminative features.

### Preliminaries

**Diffusion Process.** Denoising Diffusion Probabilistic Models (DDPMs)  are latent variable models that learn to generate data by reversing a gradual noising process. The forward diffusion process gradually adds Gaussian noise to the data \(x_{0}\) according to a variance schedule \(_{1},,_{T}\), producing a sequence of noisy samples \(x_{1},,x_{T}\). The forward process can be described as:

\[q(x_{t}|x_{t-1})=(x_{t};}x_{t-1},_{t})\] (1)

The reverse denoising process learns to generate samples from the data distribution by starting with a Gaussian noise sample \(x_{T}\) and iteratively denoising it using a learned denoising function \(_{}\). The reverse process is defined as:

\[p_{}(x_{t-1}|x_{t})=(x_{t-1};_{}(x_{t},t),_{t }^{2}),\] (2)

\[_{}(x_{t},t)=}}(x_{t}-}{t}}_{}(x_{t},t)),\] (3)

\(_{t}=1-_{t}\), \(_{t}=_{s=1}^{t}_{s}\), and \(_{t}^{2}=_{t-1}}{1-_{t}}_{t}\). The training objective is to minimize the weighted sum of the denoising error at each step:

\[L=_{(0,1),t[1,T]}[||- _{}(x_{t},t)||_{2}^{2}]\] (4)

**Stable Diffusion.** The Stable Diffusion Model is a powerful pre-trained model with impressive generative capabilities, able to synthesize various types of images, including different types of human faces. Built upon the DDPM framework, the Stable Diffusion models employs a Latent Diffusion Model (LDM)  to reduce resource consumption. LDM applies the diffusion process in a learned latent space instead of pixel space, which is obtained by training an autoencoder. The training objective is:

\[L=_{(0,1),t[1,T]}[||- _{}(z_{t},t)||_{2}^{2}],\] (5)

where \(z_{t}\) is the latent representation encoded by the VAE encoder. This strategic application of latent space modeling not only enhances efficiency but also preserves the high quality of generated images.

Figure 2: The details of the DiffusionFake method. The blue arrow represents the target branch, the red arrow represents the source branch, the \(\) represents the parameter frozen and does not participate in training, and the \(\) represents the trainable module.

### Feature Transformation

Given an input image \(x\) and its corresponding label \(y\), where \(y=0\) represents a real face and \(y=1\) represents a forged face, let \(x_{s}\) and \(x_{t}\) denote the corresponding source and target images from the training dataset, respectively. For real faces, \(x_{s}\) and \(x_{t}\) are identical to \(x\). Let \(E\) be the encoder, and the extracted features be \(f=E(x)\). To transform the extracted feature \(f\) into components that can guide the Stable Diffusion process, we first introduce two key modules: the _Feature Filter Module_ and the _Weight Module_.

**Feature Filter Module.** The Feature Filter Module \(F\) is to extract source-related and target-related features from the encoded features \(f\). To achieve this, we employ two filter networks, \(F_{s}\) and \(F_{t}\), to obtain the source-related feature \(f_{s}=F_{s}(f)\) and target-related feature \(f_{t}=F_{t}(f)\), respectively.

As shown in Figure 2, the Feature Filter Module combines convolutional layers and attention mechanisms. The features first pass through a convolutional layer to transform the channels. Then channel-wise  and spatial-wise attention  are applied to adaptively weight and filter the features. These attention mechanisms help to emphasize the most relevant features while suppressing less informative ones, leading to more discriminative representations.

Subsequently, a Multi-Head Attention mechanism  is then applied to perform cross-attention between the original features (query) and the attention-filtered features (key and value). This operation captures long-range dependencies and enhances the receptive field, enabling more effective feature refinement. Finally, to ensure compatibility with the encoder of the Stable Diffusion model, we apply upsampling and pooling operations to align the feature dimensions.

**Weight Module.** The Weight Module \(W\) addresses the varying levels of source and target information embedded in different types of forged images. For example, Deepfakes may evenly blend source and target features, while expression-driven methods like NeuralTextures may predominantly feature target image information with minimal source information confined to specific regions like mouth movements. Uniformly feeding these into the guide module would be suboptimal.

To mitigate this, we use two separate weight modules, \(W_{s}\) and \(W_{t}\), to estimate the information content for source and target features. Each weight module start with a pooling layer, following five MLP layers, and a sigmoid function finally outputs the weight. We train these modules using the similarity scores between the input image and its respective source and target images as ground truth.

Specifically, we encode \(x\), \(x_{t}\), and \(x_{s}\) using the pre-trained VAE-Encoder from Stable Diffusion to obtain latent representations \(z\), \(z_{t}\), and \(z_{s}\). Such a well-pretrained model can effectively capture and quantify the differences between images, providing a reliable basis for measuring the similarity between the input image and its corresponding source and target images. The similarity scores between \(z\) and \(z_{t}\), and between \(z\) and \(z_{s}\), are computed and used to train the weight modules with mean squared error (MSE) loss as follows:

\[_{ws}=||W_{s}(f)-(z,z_{s})|\,|_{2}^{2}\] (6)

\[_{wt}=||W_{t}(f)-(z,z_{t})|\,|_{2}^{2}\] (7)

where \((a,b)=\) denotes the cosine similarity between vectors \(a\) and \(b\).

By dynamically adjusting the influence of source and target features during the diffusion process, the Weight Module ensures optimal guidance for the Stable Diffusion model, thereby enhancing the encoder's ability to extract generalized features suitable for detecting a wide range of forgeries.

### Guide Module

The Guide Module is designed to inject the source-related and target-related features into the frozen pre-trained Stable Diffusion model to guide the reconstruction of the source and target images. As illustrated in Figure 2, the Guide Module employs trainable copy and zero convolution layers for feature injection, inspired by ControlNet .

Let \(U_{E}()\) and \(U_{D}()\) denote the neural block of the encoder and decoder in the U-Net \(_{}\) network of the Stable Diffusion model, respectively. The Guide Module first creates a trainable copy of \(U_{E}()\), denoted as \(U_{E}^{{}^{}}()\). The source-related feature \(f_{s}\) and target-related feature \(f_{t}\) are then independently fed into \(U_{E}^{{}^{}}()\). The resulting features are combined with the corresponding features from the lockedmodel's decoder \(U_{D}(;)\) using zero convolution layers \(Z()\), which are \(1 1\) convolutional layers with weights and biases initialized to zeros. This initialization minimizes the impact on the pre-trained model at the beginning of training, stabilizing the training process . The final output of the Guide Module can be summarized as:

\[fs^{{}^{}} =U_{D}(U_{E}(z_{s}))+Z(U_{E}^{{}^{}}(f_{s})) W_{s}(f)\] (8) \[ft^{{}^{}} =U_{D}(U_{E}(z_{t}))+Z(U_{E}^{{}^{}}(f_{t})) W_{t}(f)\] (9)

where \(z_{s}\) and \(z_{t}\) are the latent representations of the source and target images, respectively, obtained from the pre-trained VAE-Encoder of Stable Diffusion, and \(W_{s}(f)\) and \(W_{t}(f)\) are the weights computed by the Weight Module.

Unlike ControlNet, which aims to control the generated results of the diffusion model, our objective is to optimize the features \(f\) by fixing the output and encouraging the capture of more generalizable and disentangled features. By guiding the reconstruction of the source and target images using the respective features, the Guide Module facilitates the learning of rich and discriminative representations that enhance the performance of the forgery detector across various domains and attack types.

During training, we follow a process similar to the LDM , gradually executing the diffusion process, including the time step \(t\), to guide the reconstruction of the source and target images. At each time step \(t\), the model learns to predict the noise \(\) that was added to the latent representation of the source or target image. The overall learning objective for the source and target diffusion models can be formulated as:

\[L_{s}=_{(0,1),[1,T]}[|| -_{}(f^{{}^{}}_{t},t)||_{2}^{2}]\] (10)

\[L_{t}=_{(0,1),[1,T]}[|| -_{}(f^{{}^{}}_{t},t)||_{2}^{2}]\] (11)

where \(f^{{}^{}}_{s}\) and \(ft^{{}^{}}_{t}\) represent the features within the embedding of time step \(t\).

### Loss Function and Inference

We apply a simple binary classification head to the extracted feature \(f\) to obtain the predicted label \(y^{}\), which is calculated via typical cross-entropy loss as follows:

\[L_{ce}=-[y y^{}+(1-y)(1-y^{})]\] (12)

Thus, the final loss function combines Eq.12 with the losses from our _Weight module_ and _Gudie module_, which is defined as follows:

\[L=L_{ce}+_{s}L_{s}+_{t}L_{t}+L_{ws}+L_{wt}\] (13)

where \(_{s}\) and \(_{t}\) are hyperparameters that balance the contributions of the source and target diffusion losses, \(L_{s}\) and \(L_{t}\), respectively.

**Inference.** During inference, only the Encoder and the classification head are retained, as shown in the purple dotted box in Figure 2. It is worth noting that DiffusionFake ensures the encoder network extracts generalized features only during training. Consequently, our DiffusionFake framework does not introduce any additional parameters during inference, thereby enhancing generalizability and reducing computational overhead.

## 4 Experiment

### Experimental Setting

**Dataset.** To evaluate the generalization ability of DiffusionFake, we conduct experiments on several challenging datasets: (1) FaceForensics++ (FF++) : This widely-used dataset contains 1,000 videos with four manipulation methods: DeepFakes, NeuralTextures, Face2Face, and FaceSwap. The pairwise real and forged data enable the generation of mixed forgery images. (2) Celeb-DF : A high-quality DeepFake dataset containing various scenarios. (3) DeepFake Detection (DFD): Thisdataset comprises 363 real videos and 3,068 fake videos, primarily generated using the DeepFake method. (4) DFDC Preview (DFDC-P) : A challenging dataset with 1,133 real videos and 4,080 fake videos, featuring various manipulation methods and backgrounds. (5) WildDeepfake : A diverse dataset obtained from the internet, capturing a wide range of real-world scenarios. (6) DiffSwap : A recently released dataset containing 30,000 high-quality face swaps generated using the diffusion-based DiffSwap method  on the MM-Celeb-A dataset. This dataset allows for evaluating cross-method generalization.

**Training details.** DiffusionFake is a plug-and-play architecture that can be integrated with different backbone networks by simply adjusting the dimensions of the alignment layer in the Feature Filter module. During training, we utilize a pre-trained Stable Diffusion 1.5 model with frozen parameters. Input images are resized to 224\(\)224 pixels. We employ the Adam optimizer with a learning rate of 1e-5 and a batch size of 32. The model is trained for 20 epochs. The hyperparameters \(_{s}\) and \(_{t}\) are set to 0.7 and 1, respectively. We employ widely used data augmentations, such as HorizontalFlip, and CutOut. To ensure a fair comparison, we follow the data split strategy used in FaceForensics++ .

### Experimental Results

We use AUC and EER to evaluate all the methods, including ours, both of them are widely used in deepfake detection.3 We compare DiffusionFake with several state-of-the-art methods.

**Cross-dataset evaluation.** To validate the generalization capability of DiffusionFake, we first evaluate its performance on unseen datasets against recent state-of-the-art methods. Following previous settings, we train the models on the FF++ dataset and test them on several unseen domain datasets. The frame-level results are shown in Table 1, where * denotes results obtained using official code with consistent training settings and data.

We evaluate its performance using two representative backbones: EfficientNet-B4 (En-B4) and ViT-B. We observe that incorporating DiffusionFake significantly improves the generalization ability of both architectures compared to their original classification backends. For En-B4, our method boosts performance on Celeb-DF by 11% and achieves an average improvement of 6%. Similarly, ViT-B sees a 6% increase on DFDC and an average gain of 6% when trained with DiffusionFake. Remarkably, **these enhancements are achieved without increasing the parameter count or computational overhead during inference**. Compared to state-of-the-art methods, DiffusionFake outperforms

   &  &  &  &  &  &  \\   & AUC & EER & AUC & EER & AUC & EER & AUC & EER & AUC & EER & AUC & EER \\  Xception  & 65.27 & 38.77 & 66.17 & 40.14 & 69.80 & 35.41 & 87.86 & 21.04 & 74.25 & 32.04 & 72.67 & 33.48 \\ Face X-ray  & 74.20 & - & - & - & 70.00 & - & 85.60 & - & - & - & - & - \\ F3-Net*  & 71.21 & 34.03 & 67.71 & 40.17 & 72.88 & 33.38 & 86.10 & 26.17 & 76.89 & 30.83 & 74.96 & 32.92 \\ MAT*  & 70.65 & 35.83 & 70.15 & 36.53 & 67.34 & 38.31 & 87.58 & 21.73 & 79.93 & 27.77 & 75.13 & 32.03 \\ GFP*  & 75.31 & 32.48 & 66.51 & 41.52 & 71.58 & 34.77 & 85.51 & 25.64 & 78.38 & 28.15 & 75.46 & 32.51 \\ LTW  & 77.14 & 29.34 & 67.12 & 39.22 & 74.58 & 33.81 & 88.56 & 20.57 & 77.95 & 29.01 & 77.07 & 30.39 \\ LRL  & 78.26 & 29.67 & 68.76 & 37.50 & 76.53 & 32.41 & 89.24 & 20.32 & - & - & - & - \\ DCL  & 82.30 & 26.53 & 71.14 & 36.17 & 76.71 & 31.97 & 91.66 & 16.63 & 80.21 & 27.37 & 80.40 & 27.73 \\ PCL+L2G  & 81.80 & - & - & - & - & - & - & - & - & - & - & - \\ SBI*  & 80.76 & 26.97 & 68.22 & 38.11 & 76.53 & 30.22 & 88.13 & 17.25 & 75.20 & 31.49 & 77.77 & 28.81 \\ UIA-ViT  & 82.41 & - & - & - & 75.80 & - & **94.68** & - & - & - & - & - \\ RECCE*  & 70.50 & 35.34 & 67.93 & 39.82 & 75.88 & 32.41 & 89.91 & 19.95 & 77.59 & 29.38 & 76.36 & 31.38 \\ UCF  & 75.27 & - & - & - & 75.94 & - & 80.74 & - & - & - & - & - \\ CADDM*  & 77.56 & 30.63 & 72.56 & 33.63 & 72.45 & 33.56 & 82.90 & 25.20 & 75.58 & 31.01 & 76.21 & 30.81 \\ EN-b*  & 73.51 & 34.17 & 70.04 & 37.03 & 70.51 & 33.98 & 87.57 & 21.31 & 77.38 & 29.44 & 75.80 & 31.19 \\ VIT-B*  & 74.64 & 33.07 & 75.46 & 31.53 & 74.24 & 34.29 & 84.38 & 24.15 & 78.50 & 28.14 & 77.44 & 30.24 \\  En-b4+Ours & **83.17** & **24.59** & 75.17 & 33.25 & 77.35 & 30.17 & 91.71 & **16.27** & 82.02 & 25.55 & 81.88 & 25.97 \\ VIT-B+Ours & 80.46 & 27.51 & **80.14** & **29.62** & **80.95** & **27.66** & 90.36 & 19.73 & **86.98** & **21.32** & **83.78** & **25.17** \\  

Table 1: Frame-level cross-database evaluation from FF++(HQ) to Celeb-DF, Wild Deepfake, DFDC-P, DFD, and DiffSwap in terms of AUC and EER. * represents the results reproduced using open-source code or model.

disentanglement-based approaches like UCF and CAADM on Celeb-DF. Moreover, our method demonstrates substantial improvements on the latest diffusion-based face swapping dataset, DiffSwap, highlighting its effectiveness against the most recent forgery techniques. These results validate the ability of the guide module and Stable Diffusion network to encourage the encoder to learn more generalizable features by reconstructing source and target images. Due to space limitations, we provide the results of single-source and multi-source cross-manipulation evaluations in the appendix.

### Ablation Study

#### Ablation of components.

We conducted an ablation study to investigate the impact of the key modules in DiffusionFake: 1) the pre-trained Stable Diffusion (SD) model, 2) the Feature Filter module, and 3) the Weight Module. The results are shown in Table 2, where without SD refers to not loading the pre-trained weights of the SD model, and without Filter means directly feeding the encoder's output features \(f\) into the guide module.

We can observe that the pre-trained Stable Diffusion model is crucial for the DiffusionFake framework. Without the pre-trained weights, the network struggles to reconstruct the source and target images due to information loss, hindering the training process. Furthermore Both the Feature Filter and Weight modules play significant roles, and removing either of them leads to a performance decline. Specifically, eliminating the Filter module results in a 5% AUC drop, as the filtering component allows the reconstruction to focus on relevant information without interference from redundant features. On the other hand, the absence of the Weight module causes a 3% performance decrease, as this module assesses the amount of source and target information contained in the image, providing a prior for the generative network to determine the importance of guided information during the reconstruction process.

#### Ablation of backbones.

As our method can be flexibly embedded into different backbones by adjusting the alignment of the Feature Filter, we conduct an ablation study on various backbone architectures to demonstrate the versatility of DiffusionFake. We experiment with traditional ResNet-34, lightweight EfficientNet-B0, and the ViT-based ViT-Small. The results in Table 3 show that integrating our method into these backbones significantly improves generalization performance. For instance, applying DiffusionFake to the lightweight EfficientNet-B0 increases the generalization accuracy on Celeb-DF from 71.75% to 76.31%, surpassing the original EfficientNet-B4 (73.51%). This evidence suggests that our method can effectively drive different encoders to extract more generalizable features.

### Analysis and Visualizations

**Visualizations of reverse results.** Figure 3 showcases the reconstruction results for both training and unseen samples using DiffusionFake. For training samples (Figure 3 A), DiffusionFake effectively reconstructs the target image, despite the source image being slightly blurry due to information loss, capturing the basic characteristics of the ground truth. In order to compare the reconstruction effect more intuitively, we use the RECCE method to directly reconstruct the source and target images of the fake image. It can be seen that the reconstruction effect is very poor. In contrast, the reconstruction effect of our method is better due to the help of the pre-trained SD model. For unseen samples (Figure 3 B), fake images with mixed features result in significant differences between reconstructed target

   &  &  &  &  \\   & & & AUC & EER & AUC & EER \\  \(\) & \(\) & \(\) & 71.87 & 34.28 & 71.78 & 35.01 \\ \(\) & ✓ & ✓ & 73.87 & 32.06 & 72.41 & 34.25 \\ ✓ & \(\) & \(\) & 77.35 & 29.05 & 75.69 & 32.12 \\ ✓ & ✓ & \(\) & 80.79 & 26.37 & 76.17 & 31.57 \\ ✓ & \(\) & ✓ & 78.67 & 28.33 & 76.59 & 31.22 \\ ✓ & ✓ & ✓ & 83.17 & 24.59 & 77.35 & 30.17 \\   
   &  &  \\   & AUC & EER & AUC & EER \\  ResNet & 68.89 & 36.78 & 69.91 & 38.07 \\ ResNet+Ours & 75.27 & 32.44 & 73.25 & 34.27 \\  En-b0 & 71.74 & 34.56 & 69.24 & 38.32 \\ En-b0+Ours & 76.31 & 31.56 & 74.40 & 33.99 \\  Vit-S & 70.59 & 35.87 & 70.60 & 37.59 \\ Vit-S+Ours & 74.58 & 32.95 & 75.10 & 33.87 \\  

Table 2: Ablation study of different components of DiffusionFake.

   &  &  &  &  \\   & & & AUC & EER & AUC & EER \\  \(\) & \(\) & \(\) & 71.87 & 34.28 & 71.78 & 35.01 \\ \(\) & ✓ & ✓ & 73.87 & 32.06 & 72.41 & 34.25 \\ ✓ & \(\) & \(\) & 77.35 & 29.05 & 75.69 & 32.12 \\ ✓ & ✓ & \(\) & 80.79 & 26.37 & 76.17 & 31.57 \\ ✓ & \(\) & ✓ & 78.67 & 28.33 & 76.59 & 31.22 \\ ✓ & ✓ & ✓ & 83.17 & 24.59 & 77.35 & 30.17 \\   
   &  &  \\   & AUC & EER & AUC & EER \\  ResNet & 68.89 & 36.78 & 69.91 & 38.07 \\ ResNet+Ours & 75.27 & 32.44 & 73.25 & 34.27 \\  En-b0 & 71.74 & 34.56 & 69.24 & 38.32 \\ En-b0+Ours & 76.31 & 31.56 & 74.40 & 33.99 \\  Vit-S & 70.59 & 35.87 & 70.60 & 37.59 \\ Vit-S+Ours & 74.58 & 32.95 & 75.10 & 33.87 \\  

Table 3: Ablation study of backbones.

and source images, while real images exhibit smaller differences. The Euclidean distances at the bottom quantify these differences, indicating larger differences in fake images compared to real ones.

Analysis of feature divergence.DiffusionFake utilizes two Feature Filter modules to separate source-related and target-related features, expecting significant divergence between \(f_{s}\) and \(f_{t}\) for forged images and minimal differences for genuine faces. To validate this, we visualize the Euclidean distance distribution between these features across various datasets, including FFpp, Celeb-DF, Wild-Deepfake, and DiffSwap, as shown in Figure 4. The plots clearly distinguish real from fake samples: real samples have small feature distances, mostly within 0.05, whereas fake samples show larger distances due to mixed source and target information. These observations strongly support that DiffusionFake effectively disentangles source and target information in the extracted features.

Analysis of feature distribution.To demonstrate that DiffusionFake enhances the discriminative power and generalization ability of the extracted features, we visualize the t-SNE plots of the last layer features from two encoders: the original EfficientNet-B4 (En-B4) and En-B4 trained with DiffusionFake. The feature distributions are examined on two unseen datasets, Celeb-DF and Wild-Deepfake. As illustrated in Figure 5, the original En-B4 exhibits poor generalization on both datasets, with the real and fake features being nearly inseparable. In contrast, when trained with DiffusionFake, the encoder learns to capture the generalizable hybrid features present in forged images via the reverse process. Consequently, the real and fake features become more distinctly separated, forming clear decision boundaries on both unseen datasets.

Figure 4: Histogram of feature divergence on FFpp, Celeb-DF, Wild-Deepfake, and DiffSwap.

Figure 5: Feature distribution of En-b4 model and the En-b4 model trained with our DiffusionFace on two unseen datasets Celeb-DF and Wild-Deepfake via t-SNE. The red represents the real samples while the blue represents the fake ones.

Figure 3: Reconstruction results of DiffusionFake for training (A) and unseen (B) samples. For unseen samples, the model is provided with three sets of initial Gaussian noise, differing only in the injected guide information. The numbers below represent the Euclidean distance between the corresponding source and target features.

Conclusion

In this paper, we introduce DiffusionFake, a novel framework that leverages the generative process of face forgery to enhance the generalization capabilities of detection models. DiffusionFake inverts this generative process to extract and utilize hybrid features from source and target identities for effective forgery detection. Extensive experiments demonstrate that DiffusionFake significantly improves the generalization performance of various detector architectures without increasing inference parameters. The proposed framework enables the learning of discriminative and generalizable features, enhancing the robustness of detectors against a wide range of unseen forgeries.

## 6 Acknowledgements

This work was supported by National Science and Technology Major Project (No. 2022ZD0118202), the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. U23A20383, No. 62072389, No. 62176222, No. 62176223, No. 62072386, No. 62072387, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No. 2021J06003, No.2022J06001).