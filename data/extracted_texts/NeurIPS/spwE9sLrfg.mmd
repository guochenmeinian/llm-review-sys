# Verified Code Transpilation with LLMs

Sahil Bhatia\({}^{1}\)  Jie Qiu Niranjan Hasabnis\({}^{2}\)1  Sanjit A. Seshia\({}^{1}\)  Alvin Cheung\({}^{1}\)

\({}^{1}\)UC Berkeley \({}^{2}\)Code Metal

{sahilbhatia, jieq, sseshia, akcheung}@berkeley.edu

niranjan@codemetal.ai

Work was done while at Intel Labs.

###### Abstract

Domain-specific languages (DSLs) are integral to various software workflows. Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability. However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API. While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the transpiled code. Another approach for automating this task is verified lifting, which relies on program synthesis to find programs in the target language that are functionally equivalent to the source language program. While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient. In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools. We use the LLM's capabilities to reason about programs to translate a given program into its corresponding equivalent in the target language. Additionally, we use LLMs to generate proofs for functional equivalence. We develop lifting-based compilers for _four different_ DSLs targeting different application domains. Our approach not only outperforms previous symbolic-based tools in both the number of benchmarks transpiled and transpilation time, but also requires significantly less effort to build.

## 1 Introduction

Domain-specific languages (DSLs) have gained popularity due to their ability to provide optimizations and abstractions that improve code readability and performance in specific domains. Examples of recent DSLs include Spark (distributed computing), NumPy (array processing), TACO (tensor processing), and P4 (network packet processing). With new DSLs emerging for diverse application domains and programming languages, developers often face the task of manually rewriting existing code to incorporate these languages into their existing workflows. This manual rewriting process can be tedious, may introduce bugs into the code, and may fail to preserve the semantics of the starting code. This problem of transforming and compiling code from one programming language to another is called _transpilation_. The question we address in this paper is: can large language models (LLMs) correctly and automatically perform code transpilation?

A particularly useful form of code transpilation, termed _lifting_, involves translating code in a somewhat lower-level, general-purpose language to equivalent code in a DSL. Lifting allows developers to port code to DSLs from which efficient code can be generated for special-purpose hardware, such as GPUs, machine learning accelerators, or network processors. Therefore, significant effort has been dedicated to developing tools aimed at automating the task of lifting. Rule-based approaches rely on traditional pattern-matching techniques ; however, describing these rules can be a complex,human-intensive task. An alternative are search-based techniques that leverage advances in _program synthesis_ (e.g., see [2; 3; 4]) and formal verification over the last two decades. The use of verified program synthesis for lifting, termed _verified lifting_, involves searching for a program in the DSL and subsequently formally verifying its semantic equivalence to the source program. Verified lifting has been successfully applied in building compilers [5; 6; 7; 8] for DSLs like Spark, SQL, Halide, and TACO. Contemporary program synthesis approaches can be broadly classified into two categories: _symbolic_ and _neural_. Traditionally, symbolic techniques such as enumerative, deductive, and constraint-based synthesis strategies have been used for implementing the search. More recently, neural networks  have been trained and leveraged to accelerate the search process. Despite their successes, both symbolic and neural approaches have common drawbacks: 1) The synthesizer is customized for each DSL, making them challenging to adapt for new DSLs, and 2) Significant effort is required to design the synthesizer, including domain-specific heuristics for symbolic approaches and the generation of parallel corpora \( source,target\) for ML-based approaches, to enable generalization and scalability for the target DSL.

Large Language Models (LLMs) [10; 11] have emerged as a promising approach for tackling complex programming tasks, including code generation, repair, and testing. However, generating reliable code with formal correctness guarantees with LLMs remains challenging. Most work on LLMs either focuses on generating code without correctness guarantees [12; 13; 14] or separately on producing proof annotations (such as invariants) for given code [15; 16]. Additionally, formal verification tools often have their own specialized languages (e.g., SMT-LIB, Dafny) for encoding verification problems and specifications. These languages are typically low-resource in the training datasets of LLMs, making it challenging for the models to generate code in these formal verification languages directly. To leverage LLMs for building verified lifting compilers, we must address two key constraints: generalization to new DSLs and providing correctness guarantees for the generated code.

In this work, we investigate the use of _LLMs for verified lifting (VL)_. Our approach, called LLMLift, takes inspiration from the core technique of VL, which involves translating the source program to a higher-level intermediate representation (IR) that describes the semantics of the DSL operators. Once the synthesized code is verified, it is then translated to the concrete syntax of the DSL using rewrite rules. We leverage the reasoning capabilities of LLMs to translate code from context to an IR. We instruct the model via a prompt to generate code using the operators of the DSL, with Python serving as the IR to encode the semantics of these operators. Python's significant representation in the training datasets of LLMs makes it a suitable choice for this purpose. In addition to generating the DSL program, we also prompt the model to generate a proof of correctness for the program. To the best of our knowledge, our approach is the first to leverage LLMs to generate _both code and proof annotations_ together. To verify the functional equivalence of the generated program to the given source program for all program states, we translate both the generated program and the proof to the syntax of an automated theorem prover. This step ensures that the synthesized code is formally verified and can be trusted to be correct. Our evaluation (section Sec. 3) shows that LLMLift has significant advantages over traditional search-based symbolic VL-based tools. It solves **7** more benchmarks, requires substantially less effort in terms of LoC (**1000\(\)**), and is faster in generating verified code and proofs (**6\(\)** on average).

In summary, this paper makes the following novel contributions

1. We introduce the first technique for formally-verified code transpilation using LLMs.
2. Our approach uses Python as an IR for code generation, thus eliminating the need for specialized DSL-specific training data or fine-tuning of LLMs.
3. Our method eliminates the need for manual encoding of domain-specific heuristics, thus simplifying the process of verified lifting by reducing the human effort required in traditional techniques.
4. We propose an approach to generate not only the lifted code but also a proof of correctness for the generated code. This integration of LLMs with verification oracles guarantees the correctness of the generated code, a crucial aspect that sets our approach apart from other work on LLM-based code generation.
5. We show the effectiveness of our approach (Sec. 4) by constructing compilers for **four** DSLs spanning various application domains. In terms of accuracy, our LLM-based compilers achieve comparable performance to existing tools and, in some domains, outperforms the prior approaches.
We now give an overview and an end-to-end example of verified lifting (VL) where we use program synthesis to build a compiler. Given a program (5) in the source language (\(S_{lang}\)), VL uses a search procedure to find a program (T) in the target language (\(T_{lang}\)) that can be proved to be functionally equivalent to the given source program. VL comprises of three phases: 1) Search, 2) Verification, and 3) Code generation. The key behind VL is to first transpile 5 to an user-defined intermediate representation (IR) of the operators in the target language before generating executable code. The IR serves as a _functional description_ of \(T_{lang}\) and ignores any implementation details. Hence, during search phase, 5 is **lifted** to a sequence of operators expressed using the IR. This expression serves as the program summary (\(PS\)) which summarizes 5 using the IR. Subsequently, \(PS\) is **verified** using a theorem prover to check for semantic equivalence with 5 for all program inputs. If verification succeeds, \(PS\) is then translated into the concrete syntax of the target language using simple pattern-matching rules provided by the user to generate executable code. These rules are notably simpler to write compared to a rule-based translator that directly compiles from \(S_{lang}\) to \(T_{lang}\), as the \(PS\) is already expressed using the operators in the target language.

We demonstrate an example of transpiling a sequential C++ to tensor-processing frameworks (such as PyTorch, Tensorflow, NumPy) using VL. Tensor-processing frameworks provide high-level API for performing, large-scale numerical computation on multi-dimensional arrays. Some of the basic tensor operators supported by all the frameworks are elementwise operators (tensor-tensor, tensor-scalar).

Fig. 0(a) shows a sequential source program (5) performing the linear burn blending operation in image editing. The given source program takes as input two images (represented as 2D vectors) and processes each pixel from both the images by first adding them and then subtracting by integer 255.

In Fig. 0(b), we define the semantics of the tensor operators such as matrix_add and matrix_scalar_sub. These functions abstract the implementation details of the operators in the tensor-processing frameworks while only capturing the high-level semantics of the operators. Our goal is to find an IR expression sequence of these operators such that it is semantically equivalent to 5. Traditional approaches to solving this search problem in VL involve framing it as SyGuS  problem. SyGuS is an approach for solving program synthesis problems by specifying constraints and searching for solutions within a defined space. Specifically, a SyGuS problem involves defining a search space that syntactically restricts the space of possible solutions, thereby making the search tractable. Formally, this objective can be stated as \(\) T \(\)\(T_{lang}\)\(|\)\(\)\(\). S\(()=\) T\(()\), where T is a program in the target language. For our program in Fig. 0(a), the synthesis phase would return the following \(PS\) (i.e., T):

``` matrix_scalar_sub(255,matrix_add(b,a)) ```
1defmatrix_add(a:list[List[int]],b:List[List[int]])
2>list[List[int]]:
3return{
4
5[]
6iflen(a)<1
7ornotlen(a)==len(b)
8
9else
10vec_add(a[b],b[@]),
11matrix_add(a[1:],b[1:]),
12
13
14
15defmatrix_scalar_sub(a:int,b:List[List[int]])
16
17->list[List[int]]:
18return{
19
20[]
21[]
22[]
23iflen(b)<1
24
25vec_scalar_sub(a,b[@]),
26
27matrix_scalar_sub(a,b[1:])
28
29
30} ```

Figure 1: Sequential source code in C++ and semantics of DSL in IR.

aptures the essential properties that are preserved while the loop executes. During VL's synthesis phase, we generate both the program summary and any required loop invariant for verification. Verification is done by sending the program summary and loop invariant(s) to a theorem prover. Verifier checks the semantic equivalence between S and the generated program summaries. VL currently uses cvc5 and z3 for this purpose. Once verified, we translate the generated program summary to the concrete syntax of the DSL (NumPy) using simple pattern-matching rules, resulting in the following executable code:

``` deflinear_burn_8_np(b:np.ndarray,a:np.ndarray)->np.ndarray: returna+b-255 ```

We next describe how our LLM-based approach can improve VL synthesis problem.

## 3 LLM-Based Verified Lifting

We now describe our LLM-based approach for verified lifting. We begin by formalizing the VL problem. Then we give details of how we use LLMs to improve over the traditional VL approach.

### Problem Formulation

The VL problem is characterized by three inputs:

1. **Specification** (\(\)): The specification (\(\)) defines the property that the target program (T) should satisfy. For VL considered in this paper, the source and target programs are _side-effect free functions_ of their inputs. Thus, \(\) encodes the semantic equivalence of T and the source program (S) for each program input state \(\). The overall correctness condition is: \[\ (,,)\ \ \ .\ ()=()\] (1)
2. **Program Space** (\(G\)): The program space outlines the set of potential solutions, typically expressed as a context-free grammar \(G\). The language of \(G\) includes all sequences of operators \(ops T_{lang}\) applied recursively to terms starting with input variables \(\). A target program T is a program summary \(PS\) that is a composition of operators \(ops\). An example involving the tensor operators is provided in the previous section. In other words, for each input \(\), S (\(\)) must be expressed using a combination of operators (\(ops\)) from \(T_{lang}\).
3. **Certificate Space:** (\(G_{I}\)) A key part of the VL problem is to generate a certificate of correctness, typically in the form of invariants that a verifier can use to prove that \(\) holds for the generated \(PS\). Synthesis tools typically use a grammar \(G_{I}\) to constrain the space of possible invariants to search over; we refer to this as the _certificate space_.

**VL Problem:** Given the inputs S, \(\), \(G\), \(G_{I}\), and \(T_{lang}\) described above, the VL problem is to generate a correct target program T in \(T_{lang}\) represented as the combination (\(PS\), \(Inv\)). This is formally expressed in logic as follows:

\[\ PS G\ \  Inv G_{I}\ \ \.\ (,(PS,Inv), )\] (2)

This states that we aim to find a program summary (\(PS\)) and invariants (\(Inv\)) from the defined search space \(G,G_{I}\) such that the given specification (functional equivalence with S) holds for all possible program states \(\).

Traditionally, the VL problem has been solved by symbolic program synthesis solvers utilizing methods such as enumerative search, deductive search, and constraint-based approaches . These

Figure 2: A high-level overview of our LLMLift framework for building verified lifting-based tools.

rely heavily on manually-designed heuristics to make the search over the program and certificate space effective. Unfortunately doing so is resource-intensive and requires domain-specific knowledge. To address these limitations, we take a new approach that leverages LLMs.

### LLM-based Verified Lifting

A naive approach to building a VL-based compiler using LLMs would be to prompt LLMs to translate \(S_{lang}\) programs directly into \(T_{lang}\). However, this approach has the following shortcomings:

1. VL-based compilers require that the \(T_{lang}\) candidates generated during the search phase be functionally equivalent to the input \(S_{lang}\) program. This is a strong requirement that current LLMs are unable to satisfy on their own.
2. Unlike widely-used general-purpose languages such as Python, domain-specific languages (DSLs) are used only in their niche applications. Unsurprisingly, we find that LLMs struggle to generate code in languages that are insufficiently represented in their training data.

In Fig. 3, we show an example of LLMs struggling to generate code reliably in new DSLs. We instruct GPT-4o to translate the program in Fig. 0(a) to MLX (Apple's latest tensor processing DSL), and the model fails to generate the expected python MLX code. Instead, the model outputs a completely incorrect solution by hallucinating non-existent MLCompute header file. This problem is even more prominent for new DSLs that the model might have never seen in the training dataset.

**Our Approach:** To address these challenges, we propose an approach that adapt two key ideas from traditional synthesis to the LLM setting:

1. _Python as an Intermediate Representation (IR):_ We adopt VL's key idea of _transpiling to an IR rather than directly to the concrete syntax of \(T_{lang}\)_. Specifically, since Python is highly represented in the training dataset of popular LLMs , these LLMs are effective at generating syntactically-correct Python code. We exploit these observations by leveraging Python as the IR to define semantics of DSL operators. An example is shown in Fig. 0(b).
2. _Oracle-Guided Inductive Synthesis (OGIS) with LLM oracles:_ Traditional verified lifting follows the paradigm of counterexample-guided inductive synthesis (CEGIS) , where a Learner that synthesizes from examples interacts with a verifier that checks the Learner's output programs for correctness with respect to a specification. OGIS [18; 19] is a generalization of CEGIS with a richer _oracle interface_ that allows for more expressive oracles and interactions. Our LLM Lifting approach instantiates OGIS with LLM oracles that synthesize \(PS\) and \(Inv\) before invoking a verifier to check correctness. Our queries to the LLM oracles (prompts) follow a few-shot learning approach. Our current verifier provides Boolean feedback to the LLM oracles.

In Fig. 2, we show our LLM-based OGIS approach, where LLMs are applied to generate program summaries and invariants in the IR, using a few-shot learning framework that we describe next.

### Few-Shot Learning Approach

LLMs have demonstrated few-shot reasoning capabilities . Few-shot reasoning allows LLMs to generalize their understanding to new tasks by leveraging a small set of similar examples. Enabling them to extend their reasoning capabilities to tasks without requiring explicit training or fine-tuning

Figure 4: Prompt Structure.

Figure 3: End-to-End Lifting Example.

for those specific tasks. We propose leveraging the few-shot reasoning capabilities of LLMs for verified lifting as fine-tuning existing LLMs for each new DSL is often infeasible due to the lack of extensive training data and the rapid pace at which new DSLs are developed. The effort required to collect, annotate, and preprocess DSL-specific training data for fine-tuning can be substantial, making it impractical to adapt LLMs to each new DSL.

As described in Sec. 2, in VL, we generate candidates in an IR that abstracts away low-level implementation details of the operators in \(T_{lang}\). The objective, as defined in Eq. (2), is to find \(PS\) and \(Inv\) expressed using operators from \(T_{lang}\) such that \(\) holds. We leverage the few-shot reasoning capability by providing the models with the semantics of operators from the target language (\(T_{lang}\)) using an IR. By exposing the LLMs to these semantics, we enable them to use their reasoning capabilities over code to generate both the \(PS\) and invariants in the IR.

In Fig. 4, we illustrate the high-level prompt structure we use to generate the \(PS\) and \(Inv\). The prompt consists of the following components:

1. **Task Instruction.** We instruct the model using a natural language to translate \(\) using only the specified DSL operators.
2. **DSL Operators.** We specify the semantics of all operators from \(T_{lang}\) using Python and include it in the prompt. Python is chosen as our IR due to (a) its widespread use across domains, (b) its concise and expressive nature, making the representation readable and straightforward and, (c) its significant representation in code datasets used for training LLMs .
3. **Specification.** While symbolic techniques often rely on approaches like test cases, bounded model checking, and Hoare logic  for defining specifications, the natural language interface of LLMs offers flexibility in using various specifications and combining different forms. Given that LLMs are primarily trained on raw source code and may not have encountered other forms of specification during training, we directly use the source program (\(\)) as the specification in our prompt.

We next describe the end-to-end workflow for our LLM-based verified lifting.

**PS Guessing.** We split the generation of \(PS\) and \(Inv\) into a two-phase process by first asking the LLM to generate the \(PS\) and then inferring invariants corresponding to it. For generating \(PS\), we prompt the model in zero-shot setting. Due to space constraints, we show an instantiation of the prompt structure shown in Fig. 4 in Appendix B. When prompted, the model generates the following \(PS\) for our example code shown in Fig. 0(a), representing the \(\) as a combination of DSL operators:

 matrix_scalar_sub(255, matrix_add(b, a))

To ensure that the generated candidates follow the DSL operators defined in the prompt, we use a rule-based parser to reject any candidates that do not satisfy this constraint, i.e., those that use constructs outside the DSL operators (see Appendix D for examples).

**Inv Guessing.** Next, if \(\) contains loops, establishing the functional equivalence of the generated \(PS\) for all program states with \(\) requires loop invariants. In VL, loop invariants typically follow a templated structure:

\[Inv\;\;f(i)\;\;e(T_{lang})\] (3)

where \(f(i)\) denotes an expression over loop indexes and \(e(T_{lang})\) represents an inductive expression constructed using operators from \(T_{lang}\). This structured nature simplifies the invariant generation process compared to solving general loop invariant synthesis problems. To facilitate the generation of loop invariants, we use one-shot learning (unlike the zero-shot approach for program summaries). This is needed: 1. to familiarize the model with the concept and structure of invariants in the VL context and, 2. generating program summaries is relatively easier than loop invariants, as the model's primary instruction is simply to combine operators from the given DSL without introducing external ones--a constraint that is easily expressed in natural language (due to space constraints we illustrate the prompt in Appendix B)2. The prompt for invariant generation closely resembles that used for generating program summaries, including \(\) with an additional assertion stating the equality of the return variable with the previously generated \(PS\). This instruction guides the model to produce an invariant corresponding to the generated \(PS\). The invariants are generated as Boolean expressions in Python rather than SMT-LIB, as we found that LLMs encounter difficulties in generating SMT-LIB (standard format for SMT solvers) code due to its limited representation in training datasets. When prompted, model generates the following invariant for the code shown in Fig. 0(a):

def invariant_outer(row, col, b, a, out):  return row >= @ and row <= len(b) and  out == matrix_scalar_sub(255, matrix_add(b[:i], a[:i])) The loop invariant states that the loop index \(row\) remains within the bounds of the array \(b\) (i.e., \(0 i(b)\)). Additionally, the invariant expresses \(out\) as a tensor DSL expression over the first \(i\) elements of the inputs \(b\) and \(a\), which helps verify that the invariant holds in each iteration of the loop. Similar to \(PS\) generation, the generated loop invariants are also checked using our rule-based parser to ensure they conform to the DSL.

**Verification**. Both the generated \(PS\) and \(Inv\) are expressed in Python. We use simple pattern-matching rewrite rules to translate these expressions into syntax compatible with the verification oracle, which checks for functional equivalence. The objective is to verify that the given S and generated T are equivalent for all possible inputs to the S program. In Appendix E, we provide a proof demonstrating how we establish this functional equivalence using an SMT solver. If the verifier cannot establish validity, we start the process again to generate new candidates.

**Code Generation.** Once verified, the \(PS\) is translated into the concrete syntax of \(T_{lang}\) using straightforward rewrite rules that recursively parse the generated \(PS\) and translate it into the concrete operators of the DSL, leveraging the syntactic nature of Python. The translation process is simplified due to Python's highly structured syntax. For instance, the generated \(PS\) for our running example can be translated into tensor processing frameworks like NumPy, generating the following code:

def linear_burn_8_np(b, a):  return a + b - 255

See Appendix F for more details on rule-based code generator. We present our complete algorithm for generating \(PS\) and \(Inv\) in Appendix A.

## 4 Experiments

To evaluate the effectiveness of LLMLift, we evaluate across four distinct DSLs3, each targeting a different application domain:

1. **Distributed Computing**: We transpile sequential Java programs into MapReduce implementations written using the Apache Spark  API. Spark, an open-source distributed computing framework, provides an interface for programming multiple clusters for data parallelism which helps in large-scale data processing.
2. **Network Packet Processing**: We transpile sequential network processing algorithms in C to the operators of programmable switch devices  with its own ISA. This translation enables the exploration of novel algorithms, such as congestion control and load balancing, on programmable switch devices.
3. **TACO**: We transpile sequential C++ programs into TACO 's API. TACO is a tensor processing compiler for generating highly optimized GPU code for performing tensor computations.
4. **Tensor Processing**. We transpile sequential C++ programs to a recently introduced tensor processing IR called TensIR . TensIR consists of common tensor operations such as element-wise arithmetic operators, reduction operators and transpose, among others. TensIR is designed to enable translation of unoptimized sequential code to tensor operations which can be then executed on 6 different software and hardware backends.

**Implementation Details**: In all experiments, we use GPT-4 via their APIs to generate candidates. We set the temperature to 0.7 for all the experiments. For program summary and invariant generation across all domains, we use the same zero-shot \(PS\) prompt in Fig. 6 and one-shot prompt in Fig. 7, respectively. We keep a budget of 50 queries for the \(PS\) and a budget of 10 queries for each \(PS\). The parser and logic for invoking the LLMs are implemented in Python with \(\)700 LoC.

**Note**. LLMLift currently only supports a subset of the C/C++ and Python language in the source programs. In particular, it does not support any code that uses pointers or objects as verifying programs with these constructs is challenging. That said, we did not encounter the use of these constructs in any of the benchmarks in all the four domains that we evaluated on.

We present the results in the sections below and defer the error analysis to Appendix D. We also provide an analysis on the performance of the generated code in Appendix G.

### Distributed Computing

MapReduce, a programming model for parallel processing of large datasets across distributed clusters, simplifies parallel computation by abstracting away distributed system complexities. A MapReduce program comprises two phases: 1. Map: Input data is partitioned into smaller chunks, each processed by a mapper function to generate key-value pairs. 2. Reduce: Intermediate key-value pairs are shuffled, sorted based on keys, and then processed by reducer functions to aggregate associated values.

**LLMLift implementation**. We compare the performance of LLMLift against MetaLift 4. MetaLift uses a symbolic solver (Rosette ) to perform the search. We evaluate on the same 45 benchmarks as MetaLift. All the benchmarks have loops and require loop invariants to prove the functional equivalence of the source and the generated program. MetaLift solves 40 out of 45 with a timeout of 1 hour. LLMLift is able to solve **44**, i.e., generate the correct translation as well as the required invariants to prove the correctness. LLMLift solves 4 additional benchmarks on which MetaLift times out. In addition to solving more benchmarks, LLMLift solves them much faster. It takes less than **1** minute on average to solve each benchmark when MetaLift has to take an average of 3 minutes to solve. The amount of effort required to build LLMLift is also significantly less than MetaLift as it does not require the developers to provide any search-space description for \(PS\) and invariants. As MetaLift requires over 1000 LoC for the description of these search-space, LLMLift requires only \(\)100 lines of prompt.

### Network Packet Processing

Network packet processing hardware, such as routers and switches, lacks flexibility post-development, preventing experimentation with new data-plane algorithms. Recently, a verified lifting approach  was introduced to simplify this process. This compiler offers the developers with two constructs: 1. a packet transaction language (subset of the C language) to express the semantics of these data-plane algorithms 2. a compiler  that translates the packet processing algorithms to the instruction set of programmable switch devices. Atoms are introduced as an instruction set of the hardware to represent the atomic operations supported by the hardware. Compiler translates the packet transaction algorithm to a sequence of atoms resulting in a different programmable switch configuration.

**LLMLift implementation**. We implement the Domino compiler using LLMLift by defining the semantics of the atoms in the prompt. We compare the performance of our implementation against MetaLift's implementation. All benchmarks in Domino are imperative C programs without any loop constructs, so no loop invariants are required for these benchmarks. The generated \(PS\) are verified using a SMT solver. MetaLift solves all the 10 benchmarks with an average time of 6 seconds. LLMLift is also able to transpile all the **10** benchmarks but with an average time of only **2** seconds. Similar to the Spark case study, we do not require developers to specify the search-space for \(PS\). While MetaLift requires over \(\)1100 LoC to describe this search-space, LLMLift only uses \(\)70 lines of prompt. In summary, LLMLift shows similar performance to the existing compiler but can be built using much less effort.

### Taco

Tensors form the key construct in machine learning and tensor compilers play an important role in optimizing these operations. TACO  is one such compiler which can automatically generate highly optimized code tailored to CPUs and GPUs. TACO's language represents the operations in a concise einsum like notation. Recently, C2TACO  a search-based lifting tool was proposed to automate the translation of C++ code to TACO.

**LLMLift implementation**. In Tab. 1, we compare the performance of C2TACO and LLMLift for all the benchmarks. We use the same 90 mins timeout for each benchmark that was used in the original C2TACO evaluation . C2TACO solves 57 out of the total 60 benchmarks, while LLMLift successfully solves all **60** benchmarks. The 3 benchmarks that C2TACO fails to solve require expressions of depth greater than 4. Due to its enumerative approach, C2TACO struggles to find solutions for these cases. We attempted to run these 3 challenging benchmarks with an extended timeout of 1 day, but the C2TACO was still unable to find a solution. C2TACO uses over 1000 LoC for implementing the heuristics to scale the symbolic search. In contrast, LLMLift relies on a simple **100** lines of prompt (task instruction + DSL semantics) to achieve better performance than C2TACO. C2TACO takes an average of 41 seconds while LLMLift average solving time is **2** seconds. We also perform an experiment to test the scalability of C2TACO enumerate approach with more complex benchmarks than the ones used in the original evaluation. We include the results in Appendix C.

### Tensor Processing

Many domains, such as image processing, signal processing, and deep learning, have legacy code written in high-level languages that operate on individual values of the input and perform specific operations. To leverage the optimizations provided by deep learning frameworks or hardware backends like GPUs, this code needs to be lifted to the operators supported by these languages. Prior work  introduced a tensor IR that can translate sequential programs to six different hardware and software backends automatically using a verified lifting approach.

**LLMLift implementation**. We evaluate LLMLift against Tenspiler  on the 23 benchmarks from the image processing and ML kernel domain.5 Tenspiler is able to solve all 23 benchmarks. LLMLift also successfully solves all **23** benchmarks, including generation of the correct proofs.6 However, it is important to note that Tenspiler's synthesis algorithm relies on three domain-specific optimizations to achieve scalability. These optimizations require significant effort to implement, with over \(\)1200 LoC written by a domain expert. In contrast, LLMLift uses \(\)320 lines of prompt to solve these benchmarks. It does not rely on any user-defined heuristics, which showcases its ability to generate correct solutions without the need for domain-specific optimizations. To check the scalability of Tenspiler's symbolioc approach, we remove all the optimizations. Tenspiler, without the optimizations, can only solve 5 out of the 23 benchmarks with a timeout of 1 hour, highlighting the importance of the domain-specific optimizations for its performance. These results highlight the ability of LLMLift to solve complex benchmarks without relying on domain-specific heuristics. Moreover, LLMLift solves these benchmarks faster than Tenspiler with all its optimizations enabled. LLMLift takes an average time of **95.89** seconds to solve each benchmark, whereas Tenspiler takes 115.14 seconds.

### Two-phase Approach for LLMLift

In this section, we evaluate an alternative approach to the two-phase method described in Sec. 3, where we generate the \(Inv\)(s) and the \(PS\) together in a single step. To test this, we prompt the model in a one-shot setting, providing an example that demonstrates generating the \(PS\) and the \(Inv\)(s) simultaneously. We merge the prompts described in Fig. 6 and Fig. 7 to create a unified prompt for this experiment.

Due to budget constraints, we limit this experiment to the tensor processing domain, which represents our most complex DSL with 37 operators. We use the same query budget as the two-phase approach. When prompted to generate the invariant and \(PS\) together, LLMLift successfully solves 20 out of the total 23 benchmarks. In contrast, the two-phase approach described in Sec. 3 solves all **23** benchmarks. We hypothesize that the reduced performance of the single-phase approach may be attributed to the increased complexity of generating both the \(PS\) and the \(Inv\)(s) simultaneously. Moreover, the two-phase approach enables the model to leverage the generated \(PS\) when constructing the invariant. By having access to the \(PS\), the model can more effectively reason about the necessary conditions and constraints required for the invariant to hold.

  Tool & BLAS & DSP & DSPStone & makespeare & mathfu & simpl\_array & UTDSP & darknet \\  C2TACO & 100\% & 100\% & 100\% & 100\% & 91.6\% & 90\% & 100\% & 92.8\% \\  LLMLift & 100\% & 100\% & 100\% & **100\%** & 100\% & **100\%** & **100\%** \\  

Table 1: Accuracy on various benchmarks for TACO.

Related Work

**Code Transpilation for DSLs.** Several approaches have been proposed for automating the task of translating legacy or unoptimized code to DSLs. These range from symbolic rule-based approaches  to search-based verified lifting approaches [7; 5; 27; 8; 25; 6] and neural approaches [9; 28]. Most of these tools are either optimized for a specific domain or require domain expertise to scale. In contrast, LLMLift simplifies the process of building lifting tools by leveraging LLMs. Closely related to our work is , where an IR is designed for low-resource languages and then combination of LLM and compiler techniques is used to reliably generate code for these languages.

**Code Translation for Mainstream-to-Mainstream Languages**. The closest work to ours is by Roziere et al.  on a sequence-to-sequence model to translate code between C++, Java, and Python. Our work differs in two key respects: we target lifting to DSLs, and our LLM based approach produces formally verified code. The objective of verified lifting is to map functional programs to the operators of a DSL in a semantics-preserving manner. Translating between mainstream languages has its own set of challenges: 1. Different languages support various constructs, making direct mapping challenging; 2. Disparities in type systems across languages must be handled, and, 3. Generating accurate verification conditions and formal proofs for equivalence checking across diverse language constructs is complex. Due to these challenges, prior work in mainstream-to-mainstream translation, such as , often relies on test-case-based approaches to demonstrate semantic equivalence, rather than formal verification. Such approaches do not provide any correctness guarantee in the generated code, and hence are risky to deploy in practice.

**LLMs for Code.** LLMs are trained on massive amounts of code from various sources, leading to impressive performance on programming tasks such as code generation [12; 13], repair, testing, and transpilation. While the use of learning to synthesize proof artifacts, specifications, and models in formal methods is not new , recently LLMs have been successfully employed in such tasks (e.g., [15; 16; 29]). However, generating reliable code from LLMs remains challenging due to the stochastic nature of these models and the difficulty in creating a verification oracle for complex specifications. With LLMLift, we demonstrate the first approach to verified code generation with LLMs, albeit in the limited setting of transpilation for side-effect-free code.

## 6 Conclusion

We presented a principled approach to leverage LLMs for code transpilation. Unlike prior LLM-based transpilers, our transpiled code is _provably equivalent_ to the input, while also takes significantly less time to generate as compared to prior non LLM-based approaches with correctness guarantees, as demonstrated in transpiling to 4 DSLs used across a range of application domains.

## 7 Limitations

While LLMLift demonstrates impressive performance across four DSLs, there are few opportunities for future improvements. Currently, our approach generates a program summary and checks only for syntactic correctness, ensuring that the generated expressions are compatible with the SMT solver. We then generate invariants corresponding to the program summary, which are formally verified for correctness. However, incorporating a semantic filtering step using test cases could potentially eliminate some spurious program summaries. Another limitation of our current approach is that we use Boolean feedback to check the correctness of a solution. Providing more granular feedback, such as counter-examples from the theorem prover or compiler error messages, can possibly help guide the LLM towards generating correct solutions more efficiently.