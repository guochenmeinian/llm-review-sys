# Selective Attention: Enhancing Transformer through Principled Context Control

Xuechen Zhang

University of Michigan

zxuechen@umich.edu

&Xiangyu Chang

University of California, Riverside

cxian008@ucr.edu

&Mingchen Li

University of Michigan

milii@umich.edu

&Amit Roy-Chowdhury

University of California, Riverside

amitrc@ece.ucr.edu

&Jiasi Chen

University of Michigan

jiasi@umich.edu

&Samet Oymak

University of Michigan

oymak@umich.edu

###### Abstract

The attention mechanism within the transformer architecture enables the model to weigh and combine tokens based on their relevance to the query. While self-attention has enjoyed major success, it notably treats all queries \(q\) in the same way by applying the mapping \(V^{}(Kq)\), where \(V,K\) are the value and key embeddings respectively. In this work, we argue that this uniform treatment hinders the ability to control contextual sparsity and relevance. As a solution, we introduce the "_Selective Self-Attention_" (SSA) layer that augments the softmax nonlinearity with a principled temperature scaling strategy. By controlling temperature, SSA adapts the contextual sparsity of the attention map to the query embedding and its position in the context window. Through theory and experiments, we demonstrate that this alleviates attention dilution, aids the optimization process, and enhances the model's ability to control softmax spikiness of individual queries. We also incorporate temperature scaling for value embeddings and show that it boosts the model's ability to suppress irrelevant/noisy tokens. Notably, SSA is a lightweight method which introduces less than 0.5% new parameters through a weight-sharing strategy and can be fine-tuned on existing LLMs. Extensive empirical evaluations demonstrate that SSA-equipped models achieve a noticeable and consistent accuracy improvement on language modeling benchmarks.

## 1 Introduction

Attention is a pivotal mechanism in modern machine learning that allows the model to focus on and retrieve different parts of the data, enhancing its ability to capture contextual relationships across time and space. While it was originally developed for NLP tasks through the transformer architecture, it has enjoyed widespread success in other domains such as computer vision, sequence modeling, and reinforcement learning .

The canonical self-attention mechanism is a sequence-to-sequence map that outputs \((^{})\) where \(()\) denotes the row-wise softmax nonlinearity and \(\), \(\), \(\) are the query, key, and value embeddings obtained through linear projections of the input sequence \(\). Through this process, for each query, the model creates a query-dependent composition of the input context. Importantly, the model has to accomplish two objectives: namely, capturing _semantic similarity_ between tokens and also adjusting the _contextual sparsity_. Here, semantic similarity can be quantified through the angle between key-query embeddings and the contextual sparsity through the spikiness of the attention map. While the importance of the former is clear, the latter is equally important given the fact that attention maps tend to be sparse in practice .

In this paper, we argue that these two objectives can be at odds and, as a result, the self-attention layer may struggle to achieve both objectives simultaneously due to its relatively inflexible parameterization. To address this issue, we propose the _Selective Self-Attention_ (SSA) layer that aims to _decouple semantic similarity from contextual sparsity_. SSA relies on a principled application of temperature-scaling (TS) to query and value embeddings. For instance, given query embedding \(\), rather than computing \(()\), SSA computes \((())\) where \(()\) is the learnable inverse-temperature. Intuitively, this allows for better control of the context window because \(()\) can control _contextual sparsity_ while the projection matrices \(_{k},_{q}\) can fully focus on controlling semantic similarity. Figure 1 shows an example of the learned token temperatures when training the Pythia model with SSA. In summary, we make the following theoretical and empirical contributions:

* **Query selectivity.** We prove that introducing TS to the query embeddings enhances the model's capability to express a target attention map with smaller parameter norms (Proposition 1). TS particularly helps when attention maps exhibit large variations in spikiness across different queries. Real and synthetic experiments corroborate that TS enables spikier/sharper attention maps and mitigates attention dilution. See Figure 3 as an illustration.
* **Value selectivity.** We formalize the benefit of TS on value embeddings through a denoising perspective. Namely, we describe a denoising task where the linear value projection fails to filter the noisy tokens, and demonstrate how nonlinear scaling can boost denoising capability.
* **Positional temperature.** We incorporate a term that adjusts the query-temperature according to the position in the context window. We show that this term can mitigate the dilution of attention scores caused by the increasing context length.
* **Modularity and parameter-efficiency of SSA.** Selective Attention is accomplished by introducing a parameter-efficient temperature module that can be easily integrated into existing attention models. In practice, this introduces 5% additional parameters to the model. We also introduce a weight sharing strategy that reduces the number of parameter overhead to less than 0.5% while maintaining the benefits of SSA. We reuse the attention weights within the temperature module, which results in negligible inference/latency overhead since no additional matrix multiplication is required. These methods only involve vector dot-products (at the output layer of the temperature module) and elementwise scaling of matrices.
* **Empirical benefits.** Our evaluations on the NLP benchmarks of Wikitext , Lambda , Piga , Hella , Winogrande , Arc-E, and Arc-C  demonstrate that Selective Attention noticeably improves language modeling performance. These benefits are consistent across various models including GPT-2 , Pythia , Llama  and Llama3 , as well as during both fine-tuning and pre-training, as shown in Table 3. Additionally, evaluations on the passkey retrieval task [33; 29] reveal that SSA substantially enhances the retrieval capabilities of the transformer, shown in Table 4.

## 2 Related Work

**Temperature Scaling (TS):** TS is a fundamental method for controlling model behavior, influencing aspects such as stochasticity of generative LLMs, calibration and uncertainty, and imbalanced data, as highlighted in several studies [26; 22; 52]. Related to us, previous research [33; 49; 7] has also proposed utilizing a temperature term in the softmax function to enhance the _length extrapolation_ capabilities of transformers. For instance, Yarn  scales the attention logits as a function of the sequence length and shows that this improves the perplexity when extending the context window. Our work provides a formal justification for the temperature scaling rule proposed in Yarn (see Proposition 2) and also highlights the value of adapting temperature to the individual positions. Importantly, our approach is differentiable and obviates the need for grid search required by prior works. Since we

Figure 1: A quotation by Steve Jobs. We highlight tokens according to their temperatures learned by the SSA layer. Darker colors correspond to lower temperatures and receive a sparser attention map.

don't focus on length generalization, we have found that position-aware temperature has a much smaller benefit compared to token-aware temperature, which is our primary contribution.

**Gating mechanisms and selectivity:** Various strategies have been developed to mitigate the impact of uninformative inputs in model training and processing. Gating mechanisms, originally introduced through LSTMs , have been proposed to selectively filter or scale down the input sequence [48; 13; 14; 25; 40]. Very recent sequence models such as Mamba (a.k.a. selective state-space model) and Griffin also incorporate gating to boost language modeling [18; 46; 54; 14; 21]. These models leverage input-dependent gating to ensure parallellizable training and enjoyed noticeable success. These methodologies inspired our approach, which incorporates TS to augment the selection capabilities of the attention layer. Specifically, TS can be viewed as an instance of gating that selectively passes or suppresses tokens to provide better control of contextual sparsity and relevance. In this light, our work also provides a mechanistic understanding of how gating mechanism can aid self-attention to improve its expressive capabilities. Finally, we highlight the concurrent work  which utilizes a _differential softmax parameterization_ to promote spiky attention maps.

**Mechanistic understanding of transformers:** The importance of transformer-based models led to many research efforts on developing a stronger understanding of various aspects of transformer and attention [30; 47; 15]. While it is impossible to cover all of these works, it is evident that capability to select relevant features and promote contextual sparsity is crucial for the ability of language models to perform complex tasks such as reasoning [23; 1; 43; 53]. These have provided inspiration for us to pursue an enhanced modeling of attention's spikiness (e.g. as in Figure 3). The experiments in Figure 3 are inspired by the recent work  which characterizes the learnability of a ground-truth attention model via the next-token prediction objective in terms of the associated Markov transition matrix.

## 3 Methodology: Selective Attention Layer

Let us recap the self-attention mechanism in Transformer . Canonical softmax attention admits an input sequence \(=[_{1}\ \ _{L}]^{}^{L d}\) of length \(L\) with embedding dimension \(d\). We then project \(\) to obtain key, query, and value embeddings (\(=_{k}\), \(=_{q}\), \(=_{v}\)) and compute the output of the dot-product attention as \((,,)=(^{}}{})\). Here \(():^{L}^{L}_{+}\) denotes the softmax nonlinearity that applies row-wise and \(_{q},_{k},_{v}^{d d}\) are learnable weight matrices. In this paper, we mainly focus on casual language modeling where each token can only attend to previous tokens in the input.

The uniform treatment of all tokens through the same softmax map could hinder the ability to control contextual sparsity and relevance. For instance, it has been observed that current Transformer language models suffer from an attention dilution issue: the longer the input sequence, the flatter the attention distribution [49; 7]. A natural solution to the dispersed attention issue is to sharpen the self-attention distribution. Selective Attention aims to provide a general strategy to control spikiness of the softmax adaptive to the query and value embedding, as well as the position of the token.

**Definition 1** (Selective Self-Attention (SSA)).: _Let \(=[_{1}\ \ _{L}]^{}^{L d}\) be an input sequence. Let \(_{k/q/v}():^{d}^{d}\) be the inverse-temperature functions for keys, queries, and values, respectively. Then the embeddings for keys (\(\)), queries (\(\)), and values (\(\)) are computed as follows:_

\[=_{k}()_{k},\ =_{q}() _{q},\ =_{v}()_{v}.\]

_where \(\) denotes the elementwise product that assigns temperature to individual tokens. Selective Self-Attention (SSA) is then computed as \((^{}}{})\)._

In essence, SSA incorporates a temperature modulation mechanism into the attention framework to enhance selectivity and context control. The inverse-temperature function \(()\) is data-dependent, allowing for dynamic adjustment of attention across different parts of the input sequence. In practice, we choose \(_{k/q/v}\) to be a scalar valued function as vector-valued temperature does not provide a significant advantage. It is also worth mentioning that we don't restrict \(_{k/q/v}\) to be non-negative. As a result, our temperature scaling strategy can be seen as an application of _scalar gating_ on K/Q/V embeddings, and hence, the SSA layer could also be referred to as _Scalar-Gated Attention (SGA)_ layer. The GitHub repo containing SSA implementation is provided in https://github.com/umich-sota/selective_attention. Below, we discuss the design choices underlying SSA.

\(\)**Temperature scaling for query and value tokens.** In an attention mechanism, the concepts of keys (\(\)), queries (\(\)), and values (\(\)) play distinct roles in determining how information is weighted and combined across a sequence. Temperature functions can be applied to all of those components, designated as Key-temperature \(_{k}()\), Query-temperature \(_{q}()\) and Value-temperature \(_{r}()\). We explore the advantages of each temperature function in in Appendix B.1. In practice, we employ Query-temperature \(_{q}()\) and Value-temperature \(_{r}()\) but don't touch the original key embeddings. The query-temperature \(_{q}\) adjusts the spikiness of the attention map associated with the query according to its embedding and position in the context window. The value-temperature \(_{r}\) enhances the model's ability to suppress irrelevant or noisy tokens, ensuring a refined aggregation of context window. In Section 4, we provide insights into theoretical and empirical benefits of incorporating these terms.While we keep the keys unmodified, guided by the intuition from word embeddings of  suggests that the similarity between a (key, query) pair should align with their cosine similarity. That is, \(cos(key_{1},query)>cos(key_{2},query)\) should ideally imply that the _query_ attends more to \(key_{1}\) compared to \(key_{2}\). Assigning temperature/gating to scale the query vector does not change this order. However, if we assign distinct scalings to \(key_{1}\) and \(key_{2}\), we will end up with scenarios where attention scores are flipped i.e. \(_{1}*key_{1}^{}query<_{2}*key_{2}^{}query\). In other words, our intuition is that assigning gating on keys will end up influencing their relative semantic similarities to queries (which could perhaps be better achieved via attention weights). This is in contrast to query-scaling which helps decouple the semantic similarity and contextual sparsity and the associated theoretical benefits (Section 4.1 and Proposition 1).

\(\)**Token-aware and position-aware temperature scaling.** The data-dependent inverse-temperature function is composed of two distinct components \(()=^{}()+^{}()\), \(\) is a token within the sequence \(\): Token-aware Temperature Scaling \(^{}()\) and Position-aware Temperature Scaling \(^{}()\). Token-aware Temperature Scaling \(^{}()\) is devised to modulate the influence of individual tokens within the sequence. The formula for this component is given by \(^{}()=tanh(f())\), where \(f()\) represents a trainable function that adjusts the impact of the token \(\). The activation function \(tanh()\) is used to enable the scaling function to output both positive and negative temperatures; for instance, if we want to have the option to fully-suppress a token \(^{}()\) can attain \( 0\). To address the issue of dispersed attention, where increasing length of the input sequence leads to a flatter attention distribution, we introduce Position-aware Temperature Scaling. This is defined by \(^{}()=1+()log(n)\), where \(n\) denotes the position of the token \(\) within the sequence \(=[_{1}\ \ _{L}]^{}^{ }\), \(n[L]\). We remark that \(n\) reflects the token length when computing the temperature of token \(_{n}\), aligning with our focus on causal attention where each token is restricted to attending only to previous tokens in the sequence. \(\) is a parameter designed to modify the scale of the factor. The non-linearity \(()\) is the sigmoid function, employed to control the range of \(^{}\) and ensure the stability of the training process.

\(\)**Weight sharing.** We introduce a weight sharing strategy to reduce the number of parameter overhead below 0.5% (10x fewer) while maintaining the benefits of SSA. Specifically, the Position-aware Temperature Scaling term, \(^{}()\) only includes a single parameter \(\), whereas the Token-aware Temperature Scaling term \(^{}()=tanh(f())\), relies on a trainable function \(f()\) defined as \(_{}(_{}^{})\), involves separate trainable parameters \(_{}\) and \(_{}^{}\), which increases parameter load. To improve efficiency, we (re)use the attention weights \(_{}\) for the temperature module by setting \(f()=_{}(_{})\). Here, SSA only adds the output layer of the MLP, a vector with few parameters. The approach only stores 3 vectors (not matrices) per attention head. This also have negligible inference/latency overhead because we don't require additional matrix multiplication. These methods only require vector dot-products (at the output layer of the temperature module) and elementwise scaling of matrices. Other strategies can also be deployed to reduce the computational overhead. We describe feature-based approach which use simple token-level statistics, such as their frequencies in training corpus. Only constant parameters per head need to be stored that reduce the number of parameter overhead below 0.1%. The details are shown in B.2.

Finally, we discuss conceptual connections to _sparse attention_ methods in Appendix D.

## 4 Theoretical Insights into Selective Attention

Selective attention computes the query temperature based on the embedding and the position of the query. It also computes the value temperature based on the value embedding. In what follows, we discuss how these three components provably enhance expressivity of the attention mechanism.

### The benefits of incorporating query embedding

**Decoupling semantics from specificity.** Consider two words: "Hinton" and "Scientist". The former is a specific instance of the latter. As a result, while we expect token embeddings of these two words to have high cosine similarity, they might benefit from different attention maps. Specifically, "Hinton" refers to a specific person and we expect it to have a more targeted attention to the context associated with it. We argue that query-temperature can aid optimization by retaining semantic similarity while allowing for distinct _specificity_. More formally, by specificity we are referring to the contextual sparsity level of a query. Denoting the combined key-query weights to be \(=_{q}_{k}^{}\) as a problem-agnostic measure of specificity, we will consider the magnitude of the query embedding. That is, given query token \(\), define \(_{}():=\|^{}\|_{2}\). It is well-established  that in order for attention map to be more sparse (hence higher specificity), the norm of the query embedding, or more generally the operator norm of \(\), has to grow larger, justifying this definition. The following Lemma shows that, without TS, the attention weights within softmax have to be lower bounded by the ratio of _specificity difference_ to _semantic distance_.

**Lemma 1**.: _Let \(=_{q}_{k}^{}^{d d}\) be the combined query-key matrix. Let \(,^{d}\) be unit norm token embeddings associated with the specific and general token respectively. Suppose we wish to achieve specificities \(_{}() L_{u}\) and \(_{}() L_{b}\). Then, the associated \(\) obeys \(\|\|-L_{b}}{\|-\|_{2}}\)._

Above \(L_{u}-L_{b}\) is the specificity_difference whereas \(\|-\|_{2}\) is the semantic distance. The proof follows from the triangle inequality \(\|\|^{}(-)\|_{2}}{\|-\|_{ 2}}^{}\|_{2}-\|^{}\|_{2}}{\| -\|_{2}}-L_{b}}{\|-\|_{2}}\).

**Comparison to Selective Attention.** In SSA, the effective attention weight matrix for a query \(\) is \(=()_{q}_{k}^{}\). To achieve the same specificity in Lemma 1 with SSA, we can set the temperatures as \(()=L_{u}\), \(()=L_{b}\), and KQ-weights as \(\|\|=1\) (e.g. via \(=_{d}\)). This achieves the desired specificities while maintaining that _effective weights_ are upper bounded as \(\|_{}\|,\|_{}\|(L_{u},L_{b})\). In other words, the required norm growth is entirely decoupled from the semantic distance between the queries.

In essence, this highlights that without query-selectivity, the model weights have to grow excessively to assign different specificity to similar words. In practice, this is expected to create performance bottlenecks: (1) As the weights grow, optimization may slow down along certain directions due to vanishing softmax derivative and, (2) even if the optimization is successful, the final model could overfit or be overly sensitive to small perturbations in the context, hindering test accuracy.

This is also verified by our experiments. To study the norm growth of attention weights, we train Pythia from scratch, trainig with the SlimPajama dataset (our pretraining setting) and evaluate on Wikitext dataset. We examine the average norm of combined query-key matrix weight \(\|\|\) from the average of all layers within the model. Additionally, we quantify the spikiness of the attention map computed as the ratio of the \(l_{1}\)-norm to the squared \(l_{2}\)-norm and normalized by the length, defined as \(\|_{1}}{\|\|^{2}L}\), \(\) where \(\) is the softmax probability vector. It takes values from 0 to 1. A smaller value indicates a sparser vector. We compute the average of the first 1000 tokens of the Wikitext dataset. The results shown in Figure 2 align with the theory. The attention weights for selective attention are smaller than the original ones, while the attention is sparser. **Expressivity benefits of query-selectivity.** A closely related consideration is whether query-selectivity can enhance expressivity. We expect that through query-temperature, the same attention head will have an easier time expressing sparse and dense attention maps associated with distinct queries. To formalize this, we investigate the ability of a single (selective) attention head to express a target attention map between

Figure 2: The operator norm of \(\) with and without Query-temperature scaling, scaled by \( 10^{3}\). The figure depicts the distribution across 1000 tokens. The dashed line is the average norm. Notably, the norm of the vanilla attention layer is approximately three times larger than that of SSA(dashed red line compare to green line). Furthermore, the vanilla attention layer exhibits a lower spikiness score (0.39) compared to SSA (0.26), where a lower value indicates higher spikiness.

all tokens in a discrete vocabulary. Let \(=_{i=1}^{K}\) be a vocabulary of \(K\) tokens. To capture all \(K^{2}\) pairwise interactions of these tokens, we first form the sequence \(=[_{1}\ \ _{K}]^{}^{K d}\) where each token appears uniquely and then study \(K\) attention maps associated with individual queries, i.e., \((,_{i})\) for \(1 i K\). Stacking these together as rows, we study the \(K K\) attention matrix \(()\). For standard attention with weights \(\), this is given by \((,)=(^{})\), whereas for query-selective attention, \((,)=(()^{})\).

Thanks to the softmax nonlinearity, \(()\) is a stochastic matrix where rows add up to 1. This matrix can be viewed as a Markov chain transition between different tokens, which motivates a fundamental question: _Can query-selective attention help express a larger class of stochastic matrices?_ Intuitively, we expect that if a stochastic matrix \(_{}\), which we wish to express via \(()\), exhibits a lot of spikiness variation across its rows (i.e., different queries), selectivity can better capture these.

This can be verified with a token generation experiments. Recall that we expect "bacteria" to attend to more words compared to "salmonella". We might expect more general words to have a larger number of neighbors in a graph. Accordingly, we abstract the vocabulary, which comprises words with various levels of specificity, into a simple undirected graph. This is depicted in Section 4.1. Additionally, the stochastic matrix \(_{}\) can be derived from this graph, with the results displayed in Figure 3(a). To build the estimation of the stochastic matrix \(}_{}\) training, we conduct next token prediction experiments.

_Token generation setting:_ Let \(X^{L}\) be a sequence of length \(L\) drawn from \(\). Suppose \(X\) ends with \(q:=x_{L}\). The token \(Y=x_{L+1}\) that follows \(X\) will be drawn uniformly from \(q\) or one of the neighbors of \(q\). This neighborhood is parameterized via the latent attention map \(_{}\) which will govern the generation process. Let \(=[_{1}\ \ _{N}]^{}\) be the token embeddings associated with the vocabulary \(\). Assume elements of \(\) have unit \(_{2}\) norm. In data generation, we simple sample input sequences containing each token in the vocabulary precisely once, and sample the next token according to the attention map \(_{}\), that is, the row of \(_{}\) that corresponds to the final query token. We then fit a one-layer self-attention or SSA model \(f()\) to approximate this latent dynamics. Concretely, we predict the next token \(}\) of \(f()\) according to the distribution \(g()=(f())^{N}\). Here \(^{N d}\) is the linear prediction head. As loss measure on how well we fit to the latent \(_{}\) dynamics, use the cross entropy distance between \(g()\) and the true label \(Y\). Through this, we wish to formalize and visualize the intuitions on why "salmonella" deserves a lower temperature than "bacteria". Further experimental details are described in Appendix A.

In our experiments, besides smaller cross-entropy loss, we find that Selective Attention achieves a better approximation of \(_{}\) as shown in Figure 3. To evaluate the similarity between the attention map \(_{}\) and \(}\), we also define the \(_{1}\) distance between the attention maps, namely,

\[=\|}-_{}\|_{1}.\]

We find that the \(_{}\) is also much lower than \(_{}\) (0.358 vs 0.543). Additionally, SSA naturally assigns lower temperatures to tokens with fewer neighbors. This is in line with our expectations as fewer neighbors imply a sparser attention map. The results are shown in Table 1.

Figure 3: We compare 1-layer SSA and 1-layer attention when solving next-token prediction on a small vocabulary of size 8. (a) is the graph associated to the token transition dynamics. (b) is the the pairwise token transition matrix of this vocabulary. Each row of \(_{}\) represents an attention map where a particular token is the query and all tokens in the vocabulary serve as keys (see Sec 4.1 for details). The transition matrix \(}\) estimated by SSA in (c) is sharper and more closely resembles the optimal \(_{}\). SSA achieves a smaller cross-entropy loss compared to vanilla attention, 0.009 vs 0.0126. The \(_{1}\) approximation error of the attention map of SSA is also smaller than that of vanilla attention, 0.358 vs 0.543.

To further formalize this, we revisit Lemma 1 in terms of softmax map. Let \(K=2\) and \(_{}=1-&\\ 0&1\) be the target pairwise attention map. Here second token is highly specific (only selects itself) whereas the first token is less specific when \(0<<1\). The following proposition establishes a variation of Lemma 1 when approximating \(_{}\).

**Proposition 1**.: _Suppose the embeddings \(_{1},_{2}\) have unit \(_{2}\) norm with correlation \(=_{1}^{}_{2}\). Fix \(0<(,1-)\) and \(=()\). For any \(\) obeying \(_{}-(^{})_{}\), we have that \(_{1}-_{2} ^{-1}}{}}(()-)\) Conversely, Selective Attention can achieve this \(\)-approximation with weights bounded as \((_{1,2})_{1} -_{2}^{-1}((),}})\)._

### The benefits of incorporating query position

The need for position-dependent scaling arises from the fact that, for a fixed weight matrix \(=_{q}_{t}^{}\), the attention scores \(^{L}=(^{}_{L})\) become diluted as sequence length \(L\) grows. Specifically, for retrieval-type tasks, the model may want to concentrate softmax scores \(^{L}\) on a single token. However, assuming unit norm tokens, the top probability in \(^{L}\) is upper bounded via \(^{L}_{_{}}}}\). This implies that, to enforce \(^{L}_{_{}}\) to be constant, we require the spectral norm lower growth rate of \( 0.5 L+O(1)\). This motivates our logarithmic scaling strategy which was also proposed by [33; 7].

Here we provide a more formal justification on the optimal temperature scaling rule by describing a simple yet insightful task which is not solvable by a single attention head unless temperature scaling is employed. Specifically, we consider a setting where the sequence exhibits _feature imbalances_ where frequent tokens start dominating the context and potentially overwhelm the less frequent but relevant tokens.

**Imbalanced token setup:** Suppose the input sequence \(=[_{1}\ \ _{L}]^{}\) is composed of a minority token \(^{d}\) and a majority token \(^{d}\), that is, \(_{i}\{,\}\) for all \(i[L]\). For each position, we will simply ask the model to output a target mixture of \(\) and \(\), namely, \(=+(1-)\) for some \((0,1)\). Thus, using a 1-layer causal attention, we study the following objective by calculating the loss between target \(\) and each attention output:

\[()=_{n=n_{0}}^{L}-^{ }_{ n}(_{n}^{}_{n})_{ 2}^{2}.\] (1)

Above, \(_{n}\) is the inverse-temperature for the \(n^{th}\) position. Here, \(n_{0}\) is a burn-in period to simplify our exposition: \(n_{0}\) is the smallest number such that both \(\) and \(\) appear at least once within the first \(n_{0}\) tokens1. Additionally, let \(n_{a}\) be the number of tokens \(_{i}\) that are equal to \(\) within \(i[n]\). We have the following theorem.

**Proposition 2**.: _Assume \(,\) are unit Euclidean norm and linearly independent. Define the imbalance ratio \(_{n}=(n-n_{a})/n_{a}\) for \(n[L]\). There is a \(W_{}\) such that, setting \(_{n}=_{n}+\), \((W_{})\) minimizes the risk (1) to achieve \((W_{})=0\). Conversely, consider the problem instance with target mixture of \(=1/2\), second-quadrant imbalance of \(2_{n} 1\) for \(L/4 n L/2\) and fourth-quadrant imbalance of \(_{n} 4\) for \(n 3L/4\). If we employ flat temperature \(_{n}=1\) for all \(n[L]\), for any choice of attention weights \(^{d d}\), we have the lower bound \((W)>1/500\)._

  \# of neighbors(including itself) & 1 & 2 & 3 & 4 \\  nodes index & 7 & 4,5,6 & 1,2,3 & 0 \\  temperature & 0.002 & 0.019 & 0.152 & 0.751 \\  

Table 1: Temperature for each depth. Nodes with the same # of neighbors share the same temperature.

Proposition 2 inspired our design of position-aware temperature scaling. Intuitively, as \(n\) increases, the sequence may include less related tokens, leading to an increase in \(_{n}\). When \(_{n}\) follows power-law \(_{n}=n^{}\), we recover the logarithmic temperature scaling rule of \(_{n}=+ n\). Consequently, our Position-aware Temperature Scaling function \(_{n}\) is designed as \(^{pos}()=1+()log(n)\), \(n\) is the position length, \(\) is the trainable parameter, \(\) is the non-linearity function sigmoid. The function is motivated by, other paper's rules [33; 26; 22; 52].

### The benefits of incorporating value embedding

Within attention, value embeddings (\(\)) are transformed using only a linear projection. Consequently, each token's contribution to the output is a weighted sum based on the attention scores, with these weights adjusted linearly. In sequences with many tokens, irrelevant or noisy tokens can negatively influence the attention mechanism. Because value embeddings are linearly projected, they may not be able to fully distinguish between relevant and irrelevant tokens. The value-temperature scaling acts as a nonlinear scalar weighting function. By adjusting the temperature, we aim to control the impact of each token, suppressing the influence of irrelevant or noisy tokens. This helps emphasize more relevant tokens, thereby improving the quality of the context representation. We motivate the potential benefits of TS on value embeddings through the following synthetic denoising task.

**Denoising task** Let \([K]\) be the token alphabet with embeddings \((_{i})_{i=1}^{K}\). Assume \(d=K\) and \(_{i}\)'s are standard basis. Consider the following data distribution \((,)\) where \(=[_{1}\ \ _{L}]^{}^{[L d}\) is the input sequence and \(^{d}\) is the target label.

* Draw \(q([K])\). Set \(=_{q}\).
* Let \((_{i})_{i=1}^{L}\) be IID noise vectors with \((0,^{2})\)
* \(_{L}=_{q}+_{L}\). For \(i[L-1]\), \(_{i}\) is determined by a Bernoulli distribution with a parameter of \(\), selecting between \(_{q}+_{i}\) and \(_{i}\). Consequently, \(\) of the tokens are signal tokens \(_{q}+_{i}\).

The denoising objective is minimizing the MSE risk

\[(f)=_{}\|[-(})\|_ {2}^{2}]\]

where \((})=}/\|}\|_{2}\), \(}\) is the output of model \(f()\), \(}=f()\).

To solve this task, the attention model \(f()\) should intelligently combine the tokens within \(\) to approximate the denoised target \(_{q}\). Importantly, the model will strictly benefit from eliminating the pure noise tokens, i.e., instances with \(_{i}=_{i}\). Note that the value projection of the attention matrix will not suffice to choose the input sequence. The reason is that \(q\) is uniform, and signal tokens span the whole space. Thus, we will benefit from a nonlinear denoising procedure.

To test this intuition, we use a 1-layer single-head attention model, denoted as different \(f()\) to minimize the denoising objective. We compare the model with value-selectivity to the following baselines:

1. _Vanilla Attention:_ The standard 1-layer single-head attention model, \(}_{att}=()\)
2. _Value-selective self-attention:_ 1-layer Selective Self-Attention (SSA). \(}_{SSA}=()\). Since this is a synthetic task, as a proxy for the token-aware temperature scaling, we use the selection function \(_{j[d]}x_{ij} 1/2\). Intuitively, when noise \( 1/\), thresholding with the largest entry will detect the signal tokens.
3. _Naive averaging:_ Directly average the tokens, \(}_{naive}=_{i=1}^{L}_{i}\).
4. _Bayes optimal estimator:_\(}_{opt}=}|}_{i S}_{i}\) where \(S[L]\) is the ground-truth set of signal tokens distributed as \(_{q}+_{i}\).

The resulting MSE risks are displayed in Table 2. We set \(d=k=8\) and \(=\). With the addition of the value-selection function, the model achieved a loss comparable to the optimal estimator, indicating

  Vanilla & Value-selective & Naive averaging & Bayes optimal estimator \\ 
1.390 & 0.071 & 2.058 & 0.003 \\  

Table 2: We apply normalization to attention output and compute the MSE risk.

[MISSING_PAGE_FAIL:9]

For the ablation study, we fine-tuned the models on the Wikitext dataset to compare the influence of each component, using the same dataset and training configurations as those in the real experiments. The results are shown in Appendix B.1. Among the results, we observe that deploying both Token-aware and Position-aware Temperature Scaling on \(\) and \(\) independently could achieve significant improvement, aligning with our theoretical insights. Additionally, combining Key and Query temperatures can achieve additional improvement. Moreover, between token-aware and position-aware temperature scaling, the latter demonstrates a more consistent improvement across different scenarios, while combining them can achieve the best overall result. We also compare with more baselines including [26; 22; 52] and the results are shown in Appendix B.3. Our method consistently outperforms the baselines.

Additionally, SSA can accelerate the training process by achieving comparable performance with fewer tokens. This efficiency not only reduces the demand on computational resources but also shortens the time required to effectively train models. We illustrate this efficiency by plotting the training results when fine-tuning the Llama model on the Wikitext dataset, both with vanilla attention layer or SSA, in Figure 4. The results indicate that SSA can accelerate training, achieving similar performance with 1.45\(\) reduction in pretraining steps.

### Passkey Retrieval

We also examines the perfromance on the passkey retrieval task as defined in [33; 29].This is a synthetic task to measure a model's ability to retrieve a simple passkey (i.e., a five-digit number) within a large amount of otherwise meaningless text. We performed 10 iterations of the passkey retrieval task with the passkey placed at a random location uniformly distributed across the evaluation context window. Intuitively, SSA could better solve this task by assigning different token-level temperatures to digits vs words. For our evaluation of the fine-tuned Pythia, SSA leads to substantial improvement (from 56.9% to 74.4%), as seen in Table 4.

## 6 Conclusions, Limitations, and Future Directions

We have introduced the Selective Self-Attention layer, which augments the softmax nonlinearity with a principled temperature-scaling strategy. SSA shows consistent benefits and augments the performance of existing transformer-based models such as Pythia and Llama 2. We also provide theoretical insights into the benefits of query, value, and positional selectivity.

**Future research.** Based on SSA, there are several interesting research avenues to pursue. Firstly, our method can extend to linear attention strategies. While we can use the same method for value embeddings, for queries, we can train an additive bias term on attention similarities rather than using temperature scaling. Secondly, based on the visual benefits of SSA on Figure 3, it would be interesting to explore how SSA can help the interpretability and quality of the attention maps. Overall, SSA has the potential to assist in more principled use of transformers in language, vision, and other modalities.

**Limitations.** Our work focuses on the canonical softmax-attention mechanism, which suffers from the quadratic computation bottleneck. As mentioned above, extending our method to linear attention can mitigate computational costs. Another direction to enhance efficiency is building stronger connections to sparsity and understanding how SSA can benefit and be integrated with sparse attention algorithms.

   Model & Original & +SSA & +SSA(weight sharing) \\  Pythia-160m & 56.89 & 74.41 & 66.90 \\ Llama & 77.62 & 89.53 & 89.45 \\   

Table 4: Passkey retrieval performance of various models.

Figure 4: Comparison of training curves. SSA provides reasonable benefits in terms of training speedup.