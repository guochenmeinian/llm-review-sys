# Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level

Runlin Lei

Renmin University of China

runlin_lei@ruc.edu.cn

&Yuwei Hu

Renmin University of China

huyuweiyisui@ruc.edu.cn

&Yuchen Ren

Renmin University of China

siriusren@ruc.edu.cn

&Zhewei Wei

Renmin University of China

zhewei@ruc.edu.cn

Zhewei Wei

Renmin University of China

zhewei@ruc.edu.cn

Zhewei Wei is the corresponding author. The work was partially done at Gaoling School of Artificial Intelligence, Beijing Key Laboratory of Big Data Management and Analysis Methods, MOE Key Lab of Data Engineering and Knowledge Engineering, and Pazhou Laboratory (Huangpu), Guangzhou, Guangdong 510555, China.

Text-Attributed Graphs (TAGs), characterized by nodes with text-based features, form a crucial category among various graph types. In the early stages of GNNs, textual node feature processing is decoupled from the network design. Researchers transform text into fixed embeddings to investigate GNN designs on downstream tasks . However, recent advancements have shifted toward a non-decoupled framework that integrates raw text into the design, significantly improving the ability to capture textual characteristics and enhance performance . Despite this progress in GNN design, current GIAs still focus on fixed embedding, aiming at only the injection of node embeddings rather than raw text on TAGs. This leads to several practical issues: **1) The attack setting is unrealistic.** In real-world scenarios like social or citation networks, it is more practical to inject nodes with raw text rather than node embeddings. For example, to attack a citation network dataset, it would be more natural to upload fake papers rather than a batch of embeddings. **2) The injected embeddings may be uninterpretable.** As minor perturbations at the embedding level can lead to unpredictable semantic shifts, traditional GIAs fail to ensure that injected nodes convey understandable semantic information. **3) Increased Detectability of Attacks.** Attackers usually have access only to raw text, not the processed embeddings defenders use. This discrepancy makes it challenging for attackers to mimic the embedding methods of defenders, leading to structurally abnormal embeddings. For example, embeddings based on word frequency are sparse, while those from Pre-trained Language Models (PLMs) are typically output continuous and dense. If an attacker injects dense embeddings while a defender uses a word-frequency-based model, the difference in the embedding structure makes the injected embeddings easy to detect.

To address the above limitations, in this paper, we innovatively explore text-level GIAs, comprehensively examining their implementation, performance, and challenges. We explore three text-level GIAs: Vanilla Text-level GIA (VTGIA), Inversion-based Text-level GIA (ITGIA), and Word-frequency-based Text-level GIA (WTGIA), all of which successfully inject raw text into graphs and degrade the performance of GNNs, as shown in Figure 0(b). Experiment results indicate that interpretability presents a significant trade-off against attack performance. ITGIA struggles to invert embeddings to interpretable text because the interpretable regions of the injected embeddings are ill-defined. VTGIA, which solely relies on Large Language Models (LLMs) for text generation, compromises attack effectiveness for its pursuit of text interpretability. WTGIA, as a balanced approach, manages to produce harmful and coherent text more effectively. However, the trade-off between performance and interpretability, coupled with poor transferability to different embedding methods, limits the performance of text-level GIAs in practice. Moreover, LLM-based predictors are shown to significantly enhance defenses against text-level GIAs, highlighting the substantial progress still needed to refine these attacks. In summary, our contributions are as follows:

* To the best of our knowledge, we are the first to unveil the dynamics of text-level graph adversarial attacks on TAGs, especially for GIAs. We discuss the impracticality of embedding-based GIA in real-world applications and propose a more realistic attack setting.
* We propose three effective text-level GIAs and demonstrate the trade-off between attack performance and text interpretability from both theoretical and empirical perspectives, providing insights for further refining text-level GIAs.
* We reflect on the challenges of graph adversarial attacks at the text level. We discover that simple strategies at the text level can enhance defense performance against text-level GIAs, which reemphasizes the importance of exploring GIAs at the text level.

Figure 1: Illustration of the Text-Level GIA setup and the three designs explored.

## 2 Background and Preliminaries

A Text-Attributed Graph (TAG) is represented as \(=(,,\{s_{i}\})\), where \(\) is the node set and \(\), the edge set. We denote \(N\) as the number of nodes and \(\{0,1\}^{N N}\) as the adjacency matrix. For each node \(v_{i}\), a sequential text feature \(s_{i}\) is associated. We focus on semi-supervised node classification tasks, where each node is categorized into one of \(C\) classes. The node labels are denoted by \(\{0,,C-1\}^{N}\). The task is to predict test labels \(_{}\) based on \(\) and training labels \(_{}\).

**Graph Neural Networks.** We use \(f_{}\) to denote a GNN parameterized by \(\). Generally, raw text \(\{s_{i}\}\) is first embedded into a feature matrix \(^{N F}\) using word embedding techniques, where \(F\) is the output dimension. In the rest of our paper, when referring to vanilla GNNs that take text embeddings as inputs, we represent the text-attributed graph as \(=(,,)\) and express \(f\) as \(f(;)\). The update process of the \(l\)-th layer of \(f\) can be formally described as:

\[h_{i}^{l}=f^{l}(h_{i}^{l-1},(\{h_{j}^{l-1}:j _{i}\})),\] (1)

where \(h_{i}^{0}=_{i},_{i}\) is the neighborhood set of node \(v_{i}\), \(\) is the aggregation function, and \(f^{l}\) is a message-passing layer that takes the features of \(v_{i}\) and its neighbors as inputs.

**Graph Injection Attacks and Related Works.** Graph injection attacks aim to create a perturbed graph \(^{}=(^{},^{},^ {})\) to disrupt the accuracy of a GNN on target nodes. Formally, denote the number of injected nodes as \(N_{}\), and the number of nodes in the poisoned graph \(^{}\) as \(N^{}\). Then, the node set of \(^{}\) can be formulated as \(^{}=_{}\), and:

\[^{}=[\\ _{}],^{}=[ []{cc}&_{}\\ _{}^{T}&],\] (2)

where \(_{}^{N_{} F}\), \(_{}\{0,1\}^{N N_{}}\), and \(\{0,1\}^{N_{} N_{}}\).

For the design of injected node features, one line of research focuses on statistical features to ensure unnoticeability, sacrificing attack strength by excluding learnable processes for features [25; 4]. In parallel, other works target learnable injected embeddings. Wang et al.  pioneer injecting fake nodes with binary features to degrade GNN performance. AFGSM  introduces an approximation strategy to linearize the surrogate model, efficiently solving the objective function.

Starting from KDDCUP 2020, research shifts towards injecting continuous node embeddings under the constraints in Equation (3) [33; 36; 2]. Denote the budget of injected nodes as \(\), the maximum and minimum elements in the original feature matrix \(\) as \(x_{}\) and \(x_{}\), and the maximum degree allowed for each injected node as \(b\). The objective of these GIAs can be formulated as:

\[_{^{}} ^{})_{i}=_{i}, v_{i}_{T}\}|}{|_{T}|}\] s.t. \[N_{},d_{} b, x_{}_{e}_{}_{e}x_{} ,\] (3)

where \(_{T}\) is the target node set, \(d_{u}\) is the degree for node \(u\), and \(_{1}_{e}_{2}\) indicates each element of \(_{1}\) is less than or equal to the corresponding element in \(_{2}\). Among continuous strategies, TDGIA  generates node features by optimizing a feature smoothness objective function. Chen et al.  enhance homophily unnoticeability by adding a regularization term to the objective function. These approaches show better performance while maintaining unnoticeability at the embedding level.

It is worth noting that the evaluation datasets for the aforementioned GIAs are **primarily TAGs**, e.g., such as Cora and CiteSeer. However, all the studies remain at the embedding level. In our experiments, we adhere to the structural budget detailed in Equation (3), omitting embedding-level constraints as we pioneer GIAs at the text level. More related works are provided in Appendix D.

**Evaluation Protocol.** In this paper, we explore GIAs in an **inductive**, **evasion** setting, following the framework established by  to ensure closeness to practical scenarios. Attackers have access to the entire graph and associated labels, except for the ground-truth labels of the test set and the victim models of the defenders. 1 Defenders train their models on clean, unperturbed training data and then use trained models to classify unseen test data.

**General Experiments Set-up.** We conduct experiments on three TAGs, Cora , CiteSeer  and PubMed , which are commonly used in evaluating GIAs. The dataset statistics and attack budgets are provided in Table 5 and Table 7 in the Appendix. We adopt full splits in  for eachdataset, which uses 60% nodes for training, 10% nodes for validation, and 30% for testing. For word embeddings techniques, we include **Bag-of-Words (BoW)** and **GTR**, a T5-based pre-trained transformer that generates continuous embeddings of unit norm. The default BoW method constructs a vocabulary using raw text from the original dataset based on word frequency, with common stop-words excluded. The embedding dimension of BoW is set to 500. In the experiments, we use a 3-layer vanilla GCN or a 3-layer homophily-based defender EGNNGuard following . EGNNGuard defends homophily-noticeable attacks by cutting off message-passing between nodes below the specified similarity threshold. The hyperparameters and training details are provided in Appendix H.1. We repeat each experiment 10 times and report the average performance and the standard deviation. Unless otherwise specified, we **bold** the best (the lowest) results.

## 3 Text-Level GIAs: Interpretability Matters

Although GIAs are considered realistic graph attacks, traditional methods focus mainly on the embedding level, which can be overly idealized. Typically, attackers only have access to public raw data, such as paper content and citation networks, on platforms like arXiv, and defenders process this data more personally. Since attackers struggle to access embeddings trained by defenders, injecting nodes with suitable embeddings is challenging. Conversely, injecting new raw data into the public dataset is more realistic for attackers. This leads us to an important question: _Can we generate coherent text for injection that ensures the effectiveness of attacks while maintaining interpretability?_

### Inversion-based Text-Level GIAs: Effective yet Uninterpretable

Embedding-level GIAs have demonstrated strong attack performance against GNNs [36; 2]. To achieve text-level attacks, a straightforward idea is to use inversion methods to convert embeddings back into text. Recent advancements in text inversion methods, such as Vec2Text , have enabled text recovery from PLM embeddings with a 66% success rate for examples averaging 16 tokens. This breakthrough opens new possibilities for achieving text-level GIA that is comparable in performance to those at the embedding level, which we term **Inversion-based Text-level GIA (ITGIA)**.

**Implementation of ITGIA.** ITGIA consists of two steps: generating poisoning and invertible embeddings, and then converting them into text. Since the effectiveness of current inversion methods has only been validated on normal text embeddings, it is crucial to ensure that generated embeddings lie within an interpretable and feasible region. However, defining these interpretable regions precisely is challenging. The constraint specified in Equation (3) limits generated embeddings to a cubic space, which may be overly broad for feasible embeddings. For example, PLMs typically produce embeddings located on a spherical surface with an L2-norm of 1, representing a significantly smaller region. To meet this fundamental requirement, we employ Projected Gradient Descent (PGD), projecting each injected embedding onto the unit sphere after every feature update. To further address the challenge of defining interpretable regions, we incorporate Harmonious Adversarial Objective (HAO)  into the GIA objective function. Optimizing over HAO increases the similarity between injected embeddings and those of normal text from original datasets, thereby bringing the injected embeddings closer to interpretable regions. Details of ITGIA and HAO are provided in Appendix I.1 and H.1. Once the embeddings are obtained, we utilize the GTR inversion model in  with a 20-step correction process to revert embeddings back into text.

**Analysis of Performance and Interpretability.** Following , we use sequential injection frameworks, including the sequential variants of SeqGIA (Random injection), TDGIA, ATDGIA, MetaGIA, and AGIA as the embedding-level backbones. The results of ITGIA are displayed in Table 1. We can see the embeddings after inversion deviate significantly from the injected embeddings, as indicated by the low cosine similarity. This suggests substantial information loss during inversion, leading to poor attack performance. Figure 6 illustrates the underlying reason: although injected embeddings meet basic norm constraints, they struggle to fall within the much smaller interpretable embedding region, hindering accurate inversion. Although incorporating HAO enhances the inversion accuracy and improves attack performance, the improvement comes at the expense of embedding-level attack performance . As a result, increasing HAO weights does not consistently yield better results, as shown in Figure 2. Furthermore, the generated text remains incoherent with high perplexity, as demonstrated in Appendix J. This renders real-world applications impractical and highlights the challenges faced by continuous embedding-level GIAs.

### LLM-based Text-Level GIAs: Interpretable yet Ineffective

Although ITGIA manages to generate harmful text, interpretability remains challenging to address. To ensure that the generated text is both interpretable and deceptive, a direct approach is to utilize an LLM for text generation and guide it to produce poisoning content through carefully designed prompts. Based on experiences with embedding-level graph adversarial attacks, when the features of injected nodes exhibit heterophily, confusion, or irrelevance, the classification performance of their neighboring nodes is negatively affected . Thus, we attempt to incorporate prior knowledge used in traditional GIAs and create prompts from three different angles:

**Heterophily Prompt.** This prompt involves sampling the text of nodes from the original dataset to generate heterophilic (dissimilar) content. Specifically, we first conduct embedding-level GIAs to obtain a perturbed graph. Next, we sample the content from the neighbors of injected nodes. Finally, we design a prompt that requests the generated content to be dissimilar to their neighboring nodes, exploiting the weakness of GNNs in handling heterophily.

**Random Prompt.** This prompt generates node text that is entirely unrelated to the original categories of the graph. For example, in the Cora dataset, the original categories are about machine learning topics. In this prompt, we generate papers in categories like sports, art, and history, which are dissimilar to all nodes in the original graph.

**Mixing Prompt.** This prompt generates node text that may be classified into multiple or even all categories. Using Cora as an example, a potential paper title could be: "Unified Approach for Solving Complex Problems using Integrated Rule Learning, Neural Networks, and Probabilistic Methods".

After defining the prompts, we propose a simple Vanilla Text-level GIA (VTGIA). This method first generates text for the injected nodes based on the prompt via LLMs and subsequently optimizes the graph structure. We use GPT-3.5-1106  as the LLM backbone for text generation, and the five sequential injection backbones same as ITGIA. The pseudo-code of VTGIA, prompts, and generated examples are provided in Appendix I.2, H.2, and J. The results are shown in Table 2. Although the generated content exhibits adversarial effects, directly producing harmful text via prompts is not sufficiently effective and is far from the optimal results achievable by attacks at the embedding level.

   Dataset & Clean & HAO & Avg. cos & SeqGIA & MetaGIA & TDGIA & ATDGIA & AGIA & Best Emb. \\  Cora & 87.19 \(\) 0.62 &  x \\ ✓ \\  & 0.14 & 74.16 \(\) 1.76 & **71.35 \(\) 1.14** & 76.52 \(\) 1.45 & 76.73 \(\) 1.46 & 72.25 \(\) 1.32 & \\   &  ✓ \\  & 0.67 & **65.67 \(\) 1.48** & 65.77 \(\) 1.15 & 71.49 \(\) 1.71 & 74.63 \(\) 2.48 & 68.59 \(\) 1.51 & \\  CiteSeer & 75.93 \(\) 0.41 &  x \\ ✓ \\  & 0.11 & 68.17 \(\) 0.94 & 69.39 \(\) 0.89 & 68.24 \(\) 1.30 & 69.72 \(\) 1.34 & **66.18 \(\) 1.19** & 21.45 \(\) 0.58 \\   &  ✓ \\  & 0.56 & **64.79 \(\) 1.30** & 65.11 \(\) 1.01 & 67.43 \(\) 0.89 & 71.89 \(\) 0.50 & **64.79 \(\) 1.30** & \\  PubMed & 87.91 \(\) 0.26 &  x \\ ✓ \\  & 0.06 & 65.13 \(\) 1.67 & **58.96 \(\) 1.25** & 59.49 \(\) 1.08 & 69.81 \(\) 1.90 & 66.16 \(\) 0.97 & 38.32 \(\) 0.00 \\   & 
 ✓ \\  & 0.59 & 66.40 \(\) 2.33 & **58.56 \(\) 1.22** & 60.26 \(\) 1.32 & 76.23 \(\) 2.08 & 65.77 \(\) 0.91 & \\   

Table 1: Performance of GCN on graphs under ITGIA. Raw text is embedded by GTR before being fed to GCN for evaluation. “Avg. cos” represents the average cosine similarity between the embeddings of the inverted text and their corresponding original embeddings across five ITGIAs. “Best Emb.” represents the best attack performance across the five variants at the embedding level.

Figure 2: The change of attack performance as the weight of HAO increases. Lower performance stands for better attack results. Details of the setting is given in Appendix H.1.

## 4 Word-Frequency-based Text-Level GIAs

Based on previous analysis, to achieve an interpretable and effective GIA, the following conditions should be met: 1) Guidance from Embedding-level GIA to ensure effectiveness. 2) Utilization of LLMs to guarantee interpretability. 3) A well-defined LLM task formulation to minimize information loss during the embedding-text-embedding conversion process.

We address these conditions by employing word-frequency-based embeddings, specifically binary Bag-of-Words (BoW) embeddings, which offer several advantages: 1) Clear physical meaning. In binary BoW embeddings, a '1' indicates the presence of a word, while a '0' indicates its absence. 2) Clear task formulation. The task involves generating text containing specified words while excluding prohibited words, which can be effectively done via LLMs. 3) Controllable embedding-text-embedding process. As long as the text includes specified words and excludes prohibited words, the embeddings can be exactly inverted. Building on these advantages, we design the **Word-frequency-based Text-level GIA (WTGIA)**, leveraging BoW embeddings and the generative capabilities of LLMs. Its implementation consists of three steps: 1) Obtain binary injected embeddings. 2) Formulate embedding to text tasks for LLMs. 3) Perform multi-round corrections for LLMs.

### Row-wise Constrained FGSM: Unnoticeable and Effective at the Embedding Level

**Implementaion of Row-wise Constrained FGSM.** To generate binary embeddings, we adopt a row-wise constrained Fast Gradient Sign Method (FGSM) algorithm based on . Specifically, we first obtain vocabulary based on BoW on the original dataset. Next, we set the injected embedding \(_{}\) to all zeros. We then set a sparsity constraint \(S\) on injected embeddings, that each embedding can retain at most \(F^{}= S*F\) non-zero entries, where \(F\) is the dimension of features. In each epoch, we define the _flippable set_ as the entries in \(_{}\) that satisfy the sparsity constraint. Based on the element-wise gradient of the loss concerning \(_{}\), we flip the most significant \(B\) entries from the flippable set, where \(B\) is the batch size. The process stops when all rows run out of word budgets. 2

Note that the sparsity budget \(S\) is a hyper-parameter that limits the use of at most \(F^{}\) words from the predefined vocabulary in a single text. Intuitively, a higher number of used words intensities the embedding-level attack but can compromise text-level interpretability, as it becomes harder to integrate more specified words naturally in the generated text. Based on the row-wise constraints, we can formally establish the relationship between embedding-level performance and unnoticeability and text-level interpretability.

**Definition 1** (Single node GIAs towards one-hot embedding).: _For a target node \(v_{t}\) with embedding \(x_{t}=[,0,,0]\{0,1\}^{F}\), \(k F\), the embedding indicates that the node uses \(k\) words from a predefined vocabulary, while \(F-k\) words are not used. We define the set containing the specified \(k\) words as set \(W_{u}\) and the set containing the \(F-k\) prohibited words as set \(W_{n}\). We assume words in \(W_{u}\) are related to their corresponding class, whereas words in \(W_{n}\) are associated with other classes. To attack node \(v_{t}\), an adversarial node \(v_{i}\) with embedding \(x_{i}\{0,1\}^{F}\) is injected._

   Dataset & Clean & Prompt & SeqGIA & MetaGIA & TDGIA & ATDGIA & AGIA & Best Emb. \\   &  & Heterophily & 83.35 \(\) 0.49 & **80.81 \(\) 0.37** & 84.23 \(\) 0.80 & 82.05 \(\) 0.88 & 83.88 \(\) 0.83 &  \\  & & Random & 84.65 \(\) 1.11 & **82.32 \(\) 0.66** & 85.51 \(\) 0.81 & 84.73 \(\) 0.82 & 86.21 \(\) 0.77 & \(31.14\) 0.05 \\  & & Mixing & 83.10 \(\) 0.80 & **80.78 \(\) 0.66** & 83.89 \(\) 1.32 & 83.91 \(\) 1.73 & 84.19 \(\) 1.21 &  \\   &  & Heterophily & 74.91 \(\) 0.55 & **73.32 \(\) 0.39** & 75.50 \(\) 0.44 & 73.73 \(\) 1.04 & 74.62 \(\) 0.86 &  \\  & & Random & 73.84 \(\) 0.79 & 73.28 \(\) 0.69 & 72.61 \(\) 1.30 & 71.43 \(\) 0.96 & **70.81 \(\) 1.27** & \(21.45\) 0.58 \\   & & Mixing & 75.29 \(\) 0.67 & **74.16 \(\) 0.51** & 74.61 \(\) 0.71 & 74.74 \(\) 1.15 & 74.87 \(\) 1.03 &  \\   &  & Heterophily & 80.80 \(\) 0.83 & 77.50 \(\) 0.52 & **75.41 \(\) 1.22** & 75.78 \(\) 0.77 & 82.36 \(\) 0.53 &  \\   & & Random & 81.99 \(\) 3.24 & **78.34 \(\) 2.08** & 80.39 \(\) 2.87 & 82.26 \(\) 4.46 & 86.23 \(\) 0.87 &  \\   & & Mixing & 81.27 \(\) 1.91 & **78.48 \(\) 1.59** & 78.62 \(\) 2.78 & 80.37 \(\) 1.99 & 85.44 \(\) 0.80 &  \\   

Table 2: Performance of GCN against VTGIA. Raw text is embedded by GTR before being fed to GCN for evaluation. “Best Emb.” refers to the best-performing embedding-level GIAs that directly update embeddings across various injection strategies.

* _Performance: An attack is more effective against node \(v_{t}\) if it uses more words \(w_{n} W_{n}\). The intuition is that mixing words from other classes makes \(x_{t}\) harder to be correctly classified._
* _Unnoticeability: An attack is less detectable if the similarity between \(x_{t}\) and \(x_{i}\) is higher._
* _Interpretability: An attack is more interpretable if fewer words are required to use in \(x_{i}\), which means \(x_{i}\) is smaller. The intuition is that generating content with fewer specified words is easier, which fits in practical scenarios involving limited length._

**Theorem 1**.: _In the setting outlined in Definition 1, assume we apply a cosine similarity constraint with a threshold \(c(0,1)\) for unnoticeability. Specifically, this constraint requires that the cosine similarity between \(x_{t}\) and \(x_{i}\) satisfies \( x_{i}}{\|x_{t}\|\|x_{i}\|}>c\). Let \(a\) denotes the number of words used by \(x_{i}\) from the set \(W_{u}\), and \(b\) denotes the number of words used by \(x_{i}\) from \(W_{n}\). If the budget is \(m\) words at most to ensure interpretability, then the maximum value of \(b\) is \((b)=((m-c,0))\)._

The proof and detailed illustration for intuitions are provided in Appendix F. Theorem 1 reveals that as long as \(m k\), we can find a positive value for \(b\) (disregarding the flooring operation for simplicity), effectively introducing harmful information to the target node. Practically, given the typically low value of \(c\), a destructive \(x_{i}\) can be easily identified with a relatively large \(m\) at the embedding level. In Figure 3, we present the average and best performance of Row-wise Constrained FGSM coupled with five sequential injection methods used in ITGIA and VTGIA against GCN and EGNNGuard. As the sparsity budget increases, the performance of EGNNGuard declines sharply, confirming that enhancements in performance are possible under the unnoticeability constraint. However, achieving significant performance degradation requires sacrificing interpretability. For example, to achieve a 10% performance drop in Cora and CiteSeer, FGSM attacks need a budget close to or exceed the maximum sparsity level of all texts belonging to the original datasets. The subsequent subsection will explore the consequences of sacrificing text-level interpretability while completing WTGIA.

### Trade-off for Interpretability at the Text Level

**Task Formulation of LLMs.** After obtaining the binary injected embeddings, we can obtain the _Specified words_ included and the _Prohibited words_ excluded in each text based on the BoW vocabulary. Subsequently, we formulate the task for LLMs as a "word-to-text" generation problem. We request LLMs to generate text under a specified length limit using specified words, optionally within a given topic based on the origin dataset. 3 Handling prohibited words is more complex. Given the sparsity of BoW embeddings, the number of prohibited words is relatively large. Directly requesting the absence of prohibited words through the prompt would be challenging for LLMs to understand. So, for closed-source LLMs like GPT, we only constrain the specified words in the prompt. For open-source LLMs, we mask their corresponding entries during the output process.

**Masking Prohibited Words.** Denote \(t_{i+1}\) as the next token at time \(i\). The standard generative process for LLM can be formulated as: \(t_{i+1}\ =\ (((t_{i+1} t_{i},t_{i-1},) )).\)

Figure 3: The Best and Average performance of FGSM against GCN and EGNNGuard among the five injection methods w.r.t increasing sparsity budgets. The results for EGNNGuard reveal that as the budget increases, FGSM attacks can satisfy the similarity constraint while significantly enhancing the attack performance at the embedding level.

where logits(\(t_{i+1} t_{i},t_{i-1},\)) are the unnormalized log probabilities of the potential next token given the previous tokens. To enforce constraints on avoiding prohibited words, we introduce a masking vector \(M\) such that: \(M[j]\,=\,0\) if token \(j\) is prohibited, and \(1\) otherwise. The generative process is then modified by applying this mask to the logits before the SoftMax operation: \(t_{i+1}=(((t_{i+1}  t_{i},t_{i-1},) M))\), where \(\) represents the element-wise multiplication of the logits by the mask. This mask effectively removes words from outputs by making their probabilities negligible.

**Multi-Round Correction.** After obtaining the output from the LLM, we calculate the use rate \((}{})\) in the generated text and correct any omissions in their usage. Through multiple rounds of dialogue, we select the text with the highest use rate as the final output for LLMs.

**Main Results.** We utilized GPT3.5-turbo-1106  as the close-source LLM and Llama3-8b  as the open-source LLM to implement the WTGIA. Based on Row-wise FGSM in Figure 3, we combine WTGIA with the five injection strategies and report the average results. The specific prompt details are in Appendix H.2. The length limit is specified in Table 6. The variants that use prompts that specify the generated content must belong to the original class set are marked as "-T." The results are displayed in Figure 4. It can be seen that

* **Desirable Performance:** WTGIA achieves comparable attack performance to Row-wise FGSM, demonstrating strong embedding-to-text capabilities.
* **Trade-off:** Variants with topic specification perform worse than those without, sacrificing attack strength for better text-level unnoticeability.
* **Masking Helps:** Llama-WM and GPT that do not mask prohibited words generally perform worse than Llama.

Complete experiments and full results are given in Appendix K and Appendix L. Overall, WTGIA effectively replicates the performance of FGSM at the embedding level, achieving both effectiveness and interoperability.

**New Trade-offs.** However, we fail to further enhance performance at the text level by increasing the sparsity budget, similar to FGSM at the embedding level. As shown in Figure 5, with the increasing sparsity budget, the use rate keeps decreasing, representing a trade-off to maintain the interpretability of the generated content. In Cora and CiteSeer, the text-level attacks perform best when sparsity is around 10%. In PubMed, GPT-Topic even fails to generate meaningful text with a sparsity budget \(S 15\%\). These observations confirm that **in text-level attacks, interpretability represents an additional trade-off that is overlooked in embedding-level attacks**. Enhancing or evaluating performance solely at the embedding level fails to provide a practical understanding of GIAs.

Figure 4: Performance of WTGIA against GCN. Sparsity budget is the average sparsity of the original dataset. Methods with -T include topic requirements in the prompt. Methods with -WM exclude masks for prohibited words in Llama. Avg Emb. represents the average FGSM attack performance at the embedding level. Lower values indicate better attack performance.

## 5 New Challenges for Text-level GIAs

### Transferbility to Different Embeddings

In practical scenarios, defenders can employ any text embedding techniques they want to process the text once an attacker completes the injection. However, embedding-level attacks implicitly assume that defenders will use the same embeddings as the attackers during evaluation, potentially benefiting the attackers. In fact, it is crucial to fully consider the transferability of a text attack across different text embedding technologies for text-level attacks.

In Table 3, we present the results of ITGIA and WTGIA when transferred to different embeddings on the Cora dataset. When transferred to GTR embeddings, WTGIA does not exhibit a significant performance advantage over ITGIA. The performance of ITGIA when transferred to BoW embeddings is even poorer. Since the injected text is highly uninterpretable and deviates significantly from the original dataset, the BoW method inherently filters out uncommon terms, rendering the injected embeddings particularly sparse and ineffective. These limitations in embedding transferability underscore the research gaps between embedding- and text-level GIAs.

### LLMs as Defender

Since attacks are text-level, the defenders can directly utilize LLMs as predictors for node classification tasks without necessarily employing GNNs. Taking the LLMs-as-Predictor from  as an example. The method feeds the formulation of the node classification task, texts of target nodes, and optionally, the sampled texts of their neighbors in the prompt. It then utilizes LLMs to predict the labels of target nodes directly. Remarkably, this method demonstrates superior performance on clean datasets. We now examine the performance of WTGIA against LLMs-as-predictor from . For WTGIA, we select the strongest variant in Figure 4 for each dataset as the attacker. In the zero-shot setting, only the text of target nodes is fed to LLMs. In the few-shot setting, the text of labeled nodes with their labels is fed to LLMs as examples. On Cora and CiteSeer, we adopt the whole test set for evaluation. For PubMed, we sample 1000 nodes from the test set for evaluation.

The results are presented in Table 4. Remarkably, on the PubMed dataset, the baseline without the use of neighborhood information ("Clean (w/o Nei)") achieves outstanding performance, which is consistent with observations in . Therefore, even without utilizing any neighborhood information, LLM-based predictors can achieve high accuracy in node classification, meaning defenders can evade the influence of injected nodes. For other datasets, Cora and CiteSeer, while incorporating neighborhood information enhances the performance of LLMs-as-Predictors, the "Clean (w/o Nei)"

   Text-GIA & Embedding & Clean & SeqGIA & MetaGIA & TDGIA & ATDGIA & AGIA \\  ITGIA & BoW & \(86.48 0.41\) & \(84.85 0.76\) & \(\) & \(85.56 0.61\) & \(86.49 0.50\) & \(84.90 0.73\) \\  & GTR & \(87.19 0.62\) & \(66.70 0.94\) & \(67.83 0.75\) & \(71.49 1.71\) & \(74.63 2.48\) & \(\) \\  WTGIA & BoW & \(86.48 0.41\) & \(48.32 0.74\) & \(51.58 0.78\) & \(52.49 1.32\) & \(\) & \(47.81 0.78\) \\  & GTR & \(87.19 0.62\) & \(78.15 1.70\) & \(\) & \(79.27 1.24\) & \(83.77 1.11\) & \(77.95 1.51\) \\   

Table 3: Performance of ITGIA and WTGIA-Llama transferred to different embeddings on Cora.

Figure 5: The performance of WTGIA w/o topic against GCN w.r.t sparsity budget. As the budget increases, the use rate keeps decreasing, and the attack performance increases and then decreases.

baseline still represents a practical upper limit for the GIAs. Even if attackers manage to degrade the performance below this baseline, defenders can turn to this neighbor-free method as a secure fallback. Consequently, the flexibility of defenders should be fully considered when evaluating GIAs at the text level in practical scenarios.

## 6 Conclusion

In this paper, we explore the design of GIAs at the text level. We address the limitations of embedding-level GIAs on TAGs by extending them to the text level, which better aligns with real-world scenarios. We present three types of text-level GIA designs: ITGIA, VTGIA, and WTGIA. Our theoretical and empirical analysis reveals a trade-off between attack performance and the interpretability of the injected text. Among these methods, WTGIA achieves the best balance between performance and interpretability. Additionally, our findings indicate that defenders can effectively counter these attacks using different text embedding techniques or LLM-based predictors, highlighting the complex and challenging nature of graph adversarial attacks in practical applications.

## 7 Acknowledgement

This research was supported in part by National Natural Science Foundation of China (No. U2241212, No. 61932001), by National Science and Technology Major Project (2022ZD0114802), by Beijing Natural Science Foundation (No. 4222028), by Beijing Outstanding Young Scientist Program No.BJJWZYJH012019100020098, By Huawei-Renmin University joint program on Information Retrieval. We also wish to acknowledge the support provided by the fund for building world-class universities (disciplines) of Renmin University of China, by Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education, Intelligent Social Governance Interdisciplinary Platform, Major Innovation & Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Public Policy and Decision-making Research Lab, and Public Computing Cloud, Renmin University of China.