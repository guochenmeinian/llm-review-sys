# Customizing Language Models with

Instance-wise LoRA for Sequential Recommendation

 Xiaoyu Kong\({}^{1}\) Jiancan Wu\({}^{1}\) An Zhang\({}^{2}\)

**Leheng Sheng\({}^{2}\) Hui Lin\({}^{3}\) Xiang Wang\({}^{1}\) Xiangnan He\({}^{1}\)\({}^{*}\)**

\({}^{1}\)MoE Key Lab of BIPC, University of Science and Technology of China

\({}^{2}\)National University of Singapore

\({}^{3}\)Electronic Science Research Institute of China Electronics Technology Group Corporation

kongxy@mail.ustc.edu.cn, wujcan@gmail.com

xiangnanhe@gmail.com

Corresponding author

###### Abstract

Sequential recommendation systems predict the next interaction item based on users' past interactions, aligning recommendations with individual preferences. Leveraging the strengths of Large Language Models (LLMs) in knowledge comprehension and reasoning, recent approaches are eager to apply LLMs to sequential recommendation. A common paradigm is converting user behavior sequences into instruction data, and fine-tuning the LLM with parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA). However, the uniform application of LoRA across diverse user behaviors is insufficient to capture individual variability, resulting in negative transfer between disparate sequences. To address these challenges, we propose Instance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation task as a form of multi-task learning, integrating LoRA with the Mixture of Experts (MoE) framework. This approach encourages different experts to capture various aspects of user behavior. Additionally, we introduce a sequence representation guided gate function that generates customized expert participation weights for each user sequence, which allows dynamic parameter adjustment for instance-wise recommendations. In sequential recommendation, iLoRA achieves an average relative improvement of 11.4% over basic LoRA in the hit ratio metric, with less than a 1% relative increase in trainable parameters. Extensive experiments on three benchmark datasets demonstrate the effectiveness of iLoRA, highlighting its superior performance compared to existing methods in mitigating negative transfer and improving recommendation accuracy. Our data and code are available at https://github.com/AkaliKong/iLoRA.

## 1 Introduction

Sequential recommendation  suggests a user's next item of interest by analyzing his/her past interactions, tailoring recommendations to individual preferences. As Large Language Models (LLMs)  exhibit impressive proficiency in global knowledge comprehension and reasoning, their potential for application in sequential recommendation is garnering increasing interest . Recent efforts  approach the sequential recommendation task under a language generation paradigm, wherein user behavior sequences are converted into input prompts by either purely textual prompting (ID numbers or descriptions)  or hybrid prompting with additional behavioral tokens , achieving remarkable success. Upon scrutinizing prior studies on LLM-based sequential recommenders, we can summarize a common fine-tuning pipeline comprising three components:(1) convert a sequence of historical behaviors into a prompt; (2) pair these prompts with the subsequent items of interest, to create instruction-tuning datasets; (3) incorporate a trainable Low-Rank Adaptation (LoRA) module [5; 16; 10; 9] into LLMs and fine-tune it on such prompts. Existing studies [17; 3; 18; 6; 9; 10; 16] primarily focus on refining high-quality prompts to more effectively incorporate recommendation information, while leaving the choice of LoRA unexplored. Instead, they employ a standard LoRA module for fine-tuning, which freezes the LLM weights and updates the model through two additional low-rank matrices.

Previous work [19; 20] have demonstrated that related tasks tend to develop similar loss geometries, while unrelated tasks exhibit dissimilar ones. Using the same set of parameters in a multi-task learning context can lead to conflicts, especially when tasks have low gradient similarity. This can result in negative transfer, which limits further improvement of the model. From this perspective, since user behaviors often exhibit substantial individual variability (_e.g.,_ distinct interests, behavior patterns, feedback mechanisms), we argue that employing a standard LoRA module across such diverse behaviors may not effectively capture these variabilities. Specifically, the inherent variability in user behaviors naturally inspires us to view item sequences with different behavior variables as different tasks. Simply applying a singular LoRA module leaves this multitask nature of sequential recommendation untouched, thus potentially overestimating the relationships between sequences. It easily causes the negative transfer between significantly discrepant sequences. Take LLaRA  as an example, which fine-tunes a LoRA module on top of Llama-2  across all sequences. Following prior studies [19; 20], we use a symmetric heatmap to analyze the pair-wise gradients similarities of LLaRA associated with disparate sequences and reveal significant misalignment, as shown in Figure 1. Specifically, we observe strong clustering by membership closeness in the collaborative space, along the diagonal of the gradient similarity matrix. Meanwhile, clusters that are more distant in the collaborative space tend to exhibit dissimilar gradient trajectories. Clearly, such gradients are misaligned, which can result in suboptimal performance. To mitigate this issue, one straightforward solution is to deploy multiple LoRA modules, each fine-tuned for a specific sequence, enabling each module to act as a lightweight expert tailored to its respective sequence. However, it is impractical in terms of resources and time complexity, as the number of sequences often scales to millions.

To address these challenges, we propose a new fine-tuning framework, Instance-wise LoRA (iLoRA), which adapts the mixture of experts (MoE) concept [22; 23] to tailor the LLM for individual variability in sequential recommendation. The key idea is to integrate a diverse array of experts within the basic LoRA module, each encouraged to capture a specific aspect of user behaviors. Specifically, for each instance of item sequence a user adopted before, we feed it into a conventional recommender model (_e.g.,_ SASRec ) to get a holistic sequence representation. Consequently, this representation,

Figure 1: Gradient similarity of LoRA modules across training steps. The sequence dataset is partitioned into 8 clusters using Euclidean distance, with hierarchical clustering applied to reorder clusters, so that clusters closer in the collaborative space are also closer together in the heatmap. Gradient similarity is used to assess the geometric characteristics of the loss, with darker cells indicating higher similarity. In the case study on the right, dashed lines connect similar items, while solid lines link identical items. Users with a gradient similarity of 0.86 share a strong interest in thriller movies, while those with -0.75 cosine similarity show no clear preference alignment.

reflecting personal behavior variability, is used by a gating network to customize instance-wise attention scores for the experts, where each score dictates the participation of each expert. With the attention scores, the experts are further assembled as instance-wise LoRA for this item sequence. Hereafter, we follow LLaRA  and fine-tune the LLM (_i.e.,_ Llama-2 ) on a hybrid prompt, but instead with the personally-activated LoRA to mitigate the negative transfer between discrepant sequences. Importantly, LiORA maintains the same total number of parameters as the standard LoRA, thereby avoiding overfitting while dynamically adapting to diverse user behaviors. We assess the effectiveness of iLoRA through extensive experiments on three benchmark sequential-recommendation datasets (_i.e.,_ LastFM , MovieLens , Steam ), showcasing its superiority over leading methods (_e.g.,_ GRU4Rec , Caser , SASRec , MoRec , TALLRec , LLaRA ).

## 2 Preliminary

**LLM-based Sequential Recommendation.** The primary task of sequential recommendation is to predict the next item that aligns with user preference [31; 32]. Formally, consider a user with a historical interaction sequence represented as \(_{<n}=[i_{1},i_{2},...,i_{n-1}]\), where each \(i_{j}\) is an item interacted with at the \(j\)-th step. A sequential recommender, parameterized by \(\), inputs this sequence and outputs a probability distribution over potential next items in the candidate set. This model is trained to maximize the likelihood of the true next item \(i_{n}\):

\[_{}_{} P_{}(i_{n }|_{<n}).\] (1)

In the context of LLM-based sequential recommendation, we employ instruction tuning [33; 34], which fine-tunes LLMs using training data structured into explicit instructional pairs \((,)\). Here, \(\) comprises a detailed textual instruction describing the interaction sequences \(_{<n}\) and recommendation task [17; 5; 15; 9; 16; 10], and \(\) is the textual description of the predictive item \(i_{n}\) in the user's sequence . The training objective is formulated as an autoregressive model optimization problem:

\[_{}_{(,)}_{t=1}^{| |} P_{}(y_{t}|,_{<t}),\] (2)

where \(\) denotes the LLM's model parameters, \(y_{t}\) represents the \(t\)-th token in the output sequence, and \(_{<t}\) includes all preceding tokens in the sequence. This objective ensures that each prediction is informed by both the prior items in the sequence and the detailed instructions describing the sequential recommendation task [32; 31; 9; 16; 10].

**Fine-tuning with Low-rank Adaption (LoRA).** Fully fine-tuning LLMs (_cf._ Equation (2)) entails substantial computational resources [36; 35; 37; 21; 38; 39; 40]. LoRA emerges as an efficient alternative [41; 42; 43; 44; 45; 46; 47; 48; 49], which injects trainable low-rank matrices into transformer layers to approximate the updates of pre-trained weights . At the core, LoRA employs a low-rank decomposition where the update \(\) to the pre-trained matrix \(^{d_{} d_{}}\) is represented as \(=\), where \(^{d_{} r}\) and \(^{r d_{}}\) and are turnable up- and down-projection matrices, respectively. The rank \(r\) is significantly smaller than both \(d_{}\) and \(d_{}\), enhancing adaptation efficiency.

Typically, LoRA applies such updates to the query and value projection matrices in the multi-head attention sub-layers within transformer layers . Specifically, for an input \(\) to the linear projection in the multi-head attention, LoRA results in the output \(^{}\) as:

\[^{}=(+)= +,\] (3)

where \(\) remains frozen, and \(\) is introduced as a scaling factor that adjusts the influence of the updates _w.r.t._ the original \(\).

This methodology introduces a flexible and efficient means to customize these models to new tasks, circumventing the need for extensive retraining of all model parameters.

**Fine-tuning with Hybrid Prompting.** In the field of LLM-based sequential recommendation, a critical challenge is the divergence between the natural language space and the "user behavior" space. To bridge this gap, previous research [9; 10; 16] introduces a hybrid prompt approach, which incorporates behavioral insights captured by recommendation models into the prompts. This approach combines the textual token representation derived from the LLM's word embedding layer, with a behavior token representation learned from the recommender model with a cross-modal projector. Formally, for an item \(i\) with associated metadata \(txt\), the LLM tokenizer and word embedding layer LLM-\(()\) convert it into token representations \(\):

\[=_{}(txt).\] (4)

Concurrently, item \(i\)'s ID embedding \(\), pretrained using the encoder SR-\(()\) of sequential recommender (_e.g.,_ SASRec , GRU4Rec ), is represented as:

\[=_{}(i).\] (5)

Then the ID embedding is transferred into a behavior token representation through a trainable projector \(()\) parameterized by \(_{1}\) to facilitate the alignment between the two modalities. This alignment strategy enables LLMs to interpret and leverage the behavioral knowledge distilled by conventional recommenders [9; 16; 10]. Subsequently, the textual and behavioral token representations are then concatenated to form a comprehensive description of item \(i\):

\[=(,_{_{1}}( )).\] (6)

By converting each item into a hybrid token representation (_cf._ Equation (6)), we can rewrite the input prompt \(\) describing the sequence \(_{<n}\) and target response \(\) decipting the item of interest \(i_{n}\). Upon such prompts and responses, the objective of applying a uniform LoRA module is as follows:

\[_{}_{(,)}_{t=1}^{| |} P_{+}(y_{t}|,_{<t}).\] (7)

## 3 Methodology

To address the issue of negative transfer associated with conventional LoRA fine-tuning, we introduce the Instance-wise LoRA (iLoRA) fine-tuning framework. This innovative approach adapts the Mixture of Experts (MoE) concept [22; 23] to tailor Large Language Models (LLMs) to individual characteristics in sequential recommendation, as illustrated in Figure 2. At the core is the integration of multiple experts, each encouraged to capture a specific aspect of user behaviors. Different instances of user behavior (_i.e.,_ item sequences) use a gating network to create instance-wise attention scores over experts. Such attentive experts instantiate the trainable matrices \(\) and \(\), thus personalizing a LoRA. Upon this instance with its individually activated LoRA, We fine-tune the LLM to minimize the negative transfer among disparate sequences.

Figure 2: The iLoRA framework, which integrates the idea of MoE with LoRA, to implement sequence-customized activation patterns for various sequences.

### Instance-wise Generation for Sequential Recommendation

Applying a uniform LoRA across the population of sequence instances risks overlooking individual variability and easily causes negative transfer, where distinct sequences might adversely affect each other. This inspires us to view the modeling of each individual instance as a separate task instead, and customize instance-wise LoRA module. By doing so, the LLM-based recommender is expected to align more closely with the behavioral and preference variability of individual users. Formally, for any sequence instance \(_{<n}\), the autoregressive objective with instance-wise LoRA modules is as:

\[_{}_{(,)}_{t=1}^{||}  P_{+(_{<n})}(y_{t}|,_{<t}),\] (8)

where \((_{<n})\) yields \(_{<n}\)-exclusive parameters of instance-wise LoRA, as compared to the shared parameters of uniform LoRA (_i.e.,_\(\) in Equation (7))

To this end, one straightforward solution is to set up different LoRA modules for individual sequences, enabling each module to act as the expert tailored to its respective sequence. However, it is impractical in terms of resource and time requirements, particularly as the number of sequences often reaches the millions. This highlights the need for a more scalable solution to address the challenge of sequence-specific customization without excessive computational overhead.

### Instance-wise LoRA with the Mixture of Experts Concept

Instead of establishing various LoRA modules, we implement the mixture-of-experts (MoE) concept  to devise our instance-wise LoRA (iLoRA) framework. This framework includes three components: (1) Diverging from the standard LoRA module with up- and down-projection matrices, we divide each matrix into an array of experts, each encouraged to capture a distinct, hidden aspect of user behavior; (2) For a given sequence instance, we use a gating network to obtain attention scores across the arrays of up- and down-projection experts, such that distinct sequences are likely to activate different experts; (3) Such an attentive combination of up- and down-projection experts instantiates the weights of LoRA, which are individually customized for the instance of interest. We will elaborate on these components one by one.

#### 3.2.1 Splitting Low-Rank Matrices into Experts

Typically, the architectural foundation of LoRA is built upon two low-rank matrices: down-projection \(^{d_{} r}\) and up-projection \(^{r d_{}}\). Here we meticulously divide each projection matrix into an array of experts, as illustrated in Figure 2. Each expert is intended to focus on capturing one specific, hidden aspect of user preference. Formally, splitting the low-rank matrices is as follows:

\[=[_{1},_{2},,_{K}], =[_{1},_{2},,_{K}],\] (9)

where \(_{k}^{d_{} r^{*}}\) and \(_{k}^{r^{*} d_{}}\) are the up- and down-projection pairs for the \(k\)-th expert, respectively; \(r^{*}=\) is the partial rank determined by the total rank \(r\) of LoRA and a predefined number of experts \(K\).

By dividing individual LoRA modules into specialized experts, we ensure a more granular and precise adaptation to user preferences. This segmentation approach allows each expert to focus on specific aspects of user interaction patterns, thereby mitigating the risk of negative transfer that arises from generalized adaptations. We should emphasize that such a segmentation scheme preserves the overall number of parameters equivalent to that of the standard LoRA, therefore preventing the potential overfitting issue.

#### 3.2.2 Generating Instance-wise Attentions over Experts

Having obtained the experts (_i.e.,_ up-projection submatrices \(\{_{k}\}_{k=1}^{K}\), down-projection submatrices \(\{_{k}\}_{k=1}^{K}\)), we construct an instance-guided gating function to yield the contribution of each expert tailored to a specific sequence. Specifically, for a sequence of historical items \(_{<n}=[i_{1},i_{2},,i_{n-1}]\), we utilize a sequential recommender (_e.g.,_ SASRec ) to extract its representation as follows:

\[=_{}(_{<n}).\] (10)Here \(^{d}\) provides a holistic view of the user's behavioral patterns and preferences. Subsequently, to ascertain the influence of each expert on distilling behavior patterns from this sequence, we get the contribution scores via a linear transformation with a softmax function:

\[=(_{_{2}}()),\] (11)

where \(_{_{2}}()=_{g}\) with the trainable transformation matrix \(_{g}^{K d}\), and the softmax function ensures these contributions normalized as the attention scores, thereby preventing any single expert from disproportionately influencing the recommendation; \(\) represents the attention scores over all the experts, with the \(k\)-th element indicating the contribution of expert \(K\).

By using the sequence representation as the guidance signal, we can get the instance-wise attention scores over experts and encourage each expert's contribution closely aligned with the individual variability inherent in the sequence. Moreover, distinct sequences tend to yield different attention scores and activate different experts, while similar sequences incline toward analogous attention scores. By dynamically adapting to a wide range of user behaviors and preferences, these experts could specialize in diverse aspects of user behaviors and be more adept at handling diverse user needs.

#### 3.2.3 Aggregating Mixture of Experts as Instance-wise LoRA

For the instance \(_{<n}\) associated with the instance-wise attentions \(\), we use the mixture-of-experts concept to aggregate the up- and down-projection submatrices from different experts, so as to establish the instance-wise LoRA parameters:

\[(_{<n})=_{k=1}^{K}_{k}_{k} _{k},\] (12)

where \((_{<n})\) encapsulates the adjustments enabled by our iLoRA. The attention score \(_{k}\), assigned by the gating network (_cf._ Equation (11)), reflects the relevance of expert \(k\)'s contribution to the particular sequence.

We apply such instance-wise LoRA updates on the transformer layers of the base LLM, which collectively construct the tunable parameters \((_{<n})\). Clearly, iLoRA maintains the same total number of parameters as the standard LoRA, but dynamically customizes varying LoRA modules for different instances. This dynamic adaptation of parameters ensures that our model remains flexible and responsive to the varied preferences and behaviors exhibited by users, effectively managing the complexity inherent in sequential recommendation systems. This approach not only enhances personalization but also improves the predictive accuracy of the recommendation system.

## 4 Experiments

In this section, we first justify the need to reshape the fine-tuning task with a uniform LoRA module as a multi-task learning framework for sequential recommendation. We request the dataset from LLaRA  and maintain exactly the same experimental settings as described in the original paper. Our study builds upon the main table from the LLaRA paper, using the results reported in the LLaRA paper directly. Here we conduct extensive experiments on various real-world datasets, including LastFM , MovieLens , and Steam , to evaluate the effectiveness of our iLoRA framework. Our analysis includes detailed comparisons of iLoRA against established baseline models, which encompass both traditional sequential recommender models (_e.g.,_ GRU4Rec , Caser , SASRec ) and LLM-based recommender models (_e.g.,_ Llama2-7B , GPT-4 , MoRec , TALLRec , LLaRA ). ValidRatio  and HitRatio@1 are used as evaluation metrics, to separately quantify the ratios of valid responses over all sequences and relevant items over all candidate items, reflecting the model capability of instruction following and recommendation accuracy. See Appendix A for more details of these baselines, datasets, and metrics. Moreover, we perform a thorough ablation study to identify the key components that enhance iLoRA's performance, focusing particularly on the role of the gating network and expert settings. In a nutshell, we would like to answer the following research questions:

* **RQ1:** What is the rationale behind instance-wise LoRA compared to the uniform LoRA module?
* **RQ2:** How does iLoRA perform in comparison to traditional sequential recommender systems and LLM-based recommender models?* **RQ3:** What is the impact of the designed components (e.g., the gating network, expert settings) on the recommendation performance of iLoRA? 

### Investing Rationale of Instance-wise LoRA (RQ1)

We begin by experimenting with LLaRA , an LLM-based recommender using a uniform LoRA module, to identify a key limitation: negative transfer between significantly different sequences. Next, we examine the experts within iLoRA, which employs an instance-wise LoRA module, to demonstrate the varying attributions of experts when handling different sequences. For more details, see Appendix B.

#### 4.1.1 Negative Transfer in Uniform LoRA & Instance-wise LoRA

Gradient similarity reflects the proximity of recommendation sequences . Here we explore whether using LLaRA to perform recommendations conditioned on different sequences exhibits similar loss geometries and vice versa. To achieve this, we use Euclidean distance to control task similarity and gradient similarity to measure loss geometry. In Figure 1, a symmetric heatmap visually displays the average gradient similarity across all LLaRA checkpoints at different training steps. We demonstrate this test in Figure 3. Specifically, we observe strong clustering along the diagonal of the gradient similarity matrix for sets of recommendation sequences that are closely related in the Euclidean space. Conversely, recommendation sequences that are distant in Euclidean space exhibit correspondingly lower gradient similarity, leading to negative transfer.

In contrast, we visualize iLoRA's gradient similarity among the identical clusters. The similarities between some clusters tend to achieve zero scores, indicating iLoRA's capability to mitigate the negative transfer between significantly different sequences.

    &  &  &  \\   & ValidRatio & HitRatio@1 & Imp.\% & ValidRatio & HitRatio@1 & Imp.\% & ValidRatio & HitRatio@1 & Imp.\% \\   \\  GRU4Rec & 1.0000 & 0.2616 & 91.13\% & 1.0000 & 0.3750 & 40.67\% & 1.0000 & 0.4168 & 26.30\% \\ Caser & 1.0000 & 0.2233 & 123.91\% & 1.0000 & 0.3861 & 36.62\% & 1.0000 & 0.4368 & 20.51\% \\ SASRec & 1.0000 & 0.2233 & 123.91\% & 1.0000 & 0.3444 & 53.16\% & 1.0000 & 0.4010 & 31.27\% \\   \\  Llama2 & 0.3443 & 0.0246 & 1932.52\% & 0.4421 & 0.0421 & 1152.97\% & 0.1653 & 0.0135 & 3799.26\% \\ ChaRe & 1.0000 & 0.3770 & 32.63\% & 0.9895 & 0.2000 & 163.75\% & 0.9798 & 0.3626 & 45.17\% \\ MoRec & 1.0000 & 0.1652 & 202.66\% & 1.0000 & 0.2822 & 86.92\% & 1.0000 & 0.3911 & 34.59\% \\ TALLRec & 0.9836 & 0.4180 & 19.62\% & 0.9263 & 0.3895 & 35.43\% & 0.9840 & 0.4637 & 13.52\% \\ LLaRA & 1.0000 & 0.4508 & 8.51\% & 0.9684 & 0.4421 & 19.32\% & 0.9975 & 0.4949 & 6.36\% \\   \\  iLoRA & 1.0000 & **0.5000** & - & 0.9891 & **0.5275** & - & 0.9981 & **0.5264** & - \\   

Table 1: The Results of iLoRA compared with traditional sequential recommender models and LLMs-based methods.

Figure 3: 2(a) and 2(b) separately show gradient similarities of LLaRA and iLoRA, with sequences partitioned into 8 clusters; 2(c) exhibits the attention scores over four experts, for ten sequences.

#### 4.1.2 Expert Showcase in Instance-wise LoRA

In this section, we visualize the attention scores of iLoRA's four experts for ten distinct sequences in Figure 2(c). Each horizontal bar represents a sequence, and the length of the segments within each bar indicates the percentage of attention scores assigned to each expert. We have several findings:

* **Sequence Variability:** There is significant variability in expert activation across different sequences. For example, Sequence 4 heavily relies on Expert 1 with a 42.5% activation weight, while Expert 4 only contributes 18.8%, demonstrating distinct preferences for different experts among sequences.
* **Expert Contribution:** Certain experts have notably high contributions for specific sequences. For instance, in Sequence 2, Expert 4 has a dominant activation weight of 46.7%, indicating that this expert captures the personalized preferences of the user group represented by Sequence 2.
* **Collaborative Contribution:** Some sequences exhibit a more balanced distribution of activation weights among multiple experts, suggesting collaborative contributions. For example, in Sequence 9, Experts 2 and 3 have similar activation weights of 31.1% and 31.5%, respectively, indicating their joint influence on the recommendations.

These observations demonstrate that iLoRA effectively adjusts expert activation based on the characteristics of each sequence.

### Performance Comparison (RQ2)

This section comprehensively compares iLoRA against some traditional and LLM-based recommenders. We conduct a holistic evaluation, considering metrics of both HitRatio@1 and ValidRatio across LastFM, MovieLens, and Steam datasets to demonstrate the effectiveness of iLoRA. The results of this comparison are summarized in Table 1.

Our findings indicate that iLoRA consistently outperforms these baseline models across the three datasets. Specifically, iLoRA achieves the highest HitRatio@1 metrics of 0.5000, 0.5275, and 0.5264 on the LastFM, MovieLens, and Steam datasets, respectively. These results demonstrate the efficacy of leveraging sequence representations as guidance signals to fine-tune LoRA parameters, enabling personalized recommendations at the parameter level.

### Ablation Study (RQ3)

In this section, we analyze the effectiveness of the main components of iLoRA in Section 4.3.1. Subsequently, in Section 4.3.2, we conduct an in-depth investigation and analysis of how varying the number of experts affects the performance of iLoRA.

Figure 4: (a)a illustrates the performance of iLoRA _w.r.t._ HitRatio@1 across different datasets with varying numbers of experts. (b)b further demonstrates the HitRatio@1 performance of the model across different epochs during training on the Steam dataset with varying numbers of experts.

Figure 5: Effects of iLoRA’s components

#### 4.3.1 Effects of Gating Network

Here we explore the influence of sequence representation on the gating network and MoE. Going beyond the sequence-tailored representation, we test two variants: using random-initialized and token-collapsed embeddings as the guidance. As Figure 5 shows, using sequence representation as the guidance consistently outperforms the other variants across three datasets, illustrating the rationale of our gating network and the benefits for the MoE combination.

#### 4.3.2 Effect of Expert Numbers

In this section, we investigate how iLoRA would react to the number of experts. As depicted in Figure 3(a), our model achieves optimal performance when the number of experts is set to 4. Increasing the number of task experts does not necessarily correlate with enhanced performance. Specifically, employing only 2 experts does not significantly improve the HitRatio@1 metrics on Steam and MovieLens datasets, while showing a slight decrease on LastFM. However, with the increase to 4 experts, the model exhibits its best performance across all three datasets, notably surpassing the 2-expert variant. To elaborate, on LastFM, MovieLens, and Steam datasets, the performance of the 4-expert variant exceeds that of the 2-expert variant by 5.2%, 6.4% and 2.2%, respectively. When the number of experts is further increased to 8, the performance resembles that of the 4-expert scenario or even shows a slight decrease. This suggests that the benefits of increased capacity gradually converge as we utilize more experts. Consequently, we adapt 4 experts as the default setting.

Furthermore, we analyzed the performance across different numbers of experts at various epochs, on the Steam dataset. It is evident that as the number of experts is set to 1, 2, and 4, the overall recommendation performance of the model steadily improves with training progress. Under this configuration, the HitRatio@1 values exhibit a positive correlation with the number of experts. However, when the number of experts reaches 8, the data indicate that the model rapidly achieves a decent performance, but subsequent HitRatio@1 values do not show significant improvements with increasing epochs. We speculate that as the number of experts increases to 8, the model may overly focus on personalized user behaviors, leading to a decrease in generalization ability and premature overfitting.

## 5 Conclusion

In this paper, we introduced instance-wise LoRA (iLoRA), a novel fine-tuning framework designed to address the challenges posed by the substantial individual variability in user behaviors within sequential recommendation systems. By integrating the mixture of experts (MoE) concept into the basic LoRA module, iLoRA dynamically adjusts to diverse user behaviors, thereby mitigating the negative transfer issues observed with standard single-module LoRA approaches. iLoRA represents a significant advancement in the application of large language models to sequential recommendation tasks. By incorporating a mixture of expert frameworks within the LoRA module, iLoRA provides a more nuanced and effective means of tailoring recommendations to individual user preferences, paving the way for more personalized and accurate recommendation systems.

## 6 Limitation

While iLoRA demonstrates promising results, there are several limitations to consider. First, our experiments are constrained by computational resources, limiting the exploration of a larger number of expert combinations and their potential impact on recommendation performance. Second, we do not extensively investigate the effects of using hard routing for recommendations with a large number of experts. Finally, our study focused on sequential recommendation tasks, and the applicability of iLoRA to other types of recommendation systems or domains remains to be explored. These limitations suggest that further research is needed to fully understand the scalability and effectiveness of iLoRA with more complex expert configurations.

## 7 Broader Impact

Our proposed method, Instance-wise LoRA (iLoRA), advances sequential recommendation systems by sequence-tailored recommendations. By leveraging the Mixture of Experts (MoE) framework, iLoRA streamlines the user experience, reduces decision fatigue, and promotes inclusivity in online spaces. Its instance-wise adaptation mechanism ensures diverse content exposure, fostering a more enriched online discourse. Beyond recommendations, iLoRA's principles extend to education, healthcare, and e-commerce, offering customized solutions in various domains. Overall, iLoRA represents a step forward in enhancing user experience and promoting inclusivity in the digital landscape.

However, despite the advancements offered by iLoRA, it is essential to acknowledge potential drawbacks. Algorithmic biases present in the training data may persist, potentially amplifying existing biases in recommendations. Moreover, over-reliance on customization could lead to filter bubbles, limiting exposure to diverse viewpoints and serendipitous discovery. Additionally, concerns regarding privacy arise due to the collection and analysis of user data for personalized recommendations, necessitating careful consideration of ethical implications in its deployment.