# Harnessing small projectors and multiple views for efficient vision pretraining

Arna Ghosh

Equal Contribution, \({}^{}\) Co-senior authorship, Correspondence: blake.richards@mcgill.ca

Kumar Krishna Agrawal

Mila - Quebec AI Institute & Computer Science, McGill University, Montreal, QC, Canada

Shagun Sodhani

UC Berkeley, CA, USA, \({}^{3}\) Meta FAIR, Toronto, ON, Canada

Adam M. Oberman

Mila - Quebec AI Institute & Mathematics and Statistics, McGill University, Montreal, QC, Canada

Blake A. Richards

CIFAR Learning in Machines & Brains Program, Toronto, ON, Canada

###### Abstract

Recent progress in self-supervised (SSL) visual representation learning has led to the development of several different proposed frameworks that rely on augmentations of images but use different loss functions. However, there are few theoretically grounded principles to guide practice, so practical implementation of each SSL framework requires several heuristics to achieve competitive performance. In this work, we build on recent analytical results to design practical recommendations for competitive and efficient SSL that are grounded in theory. Specifically, recent theory tells us that existing SSL frameworks are actually minimizing the same idealized loss, which is to learn features that best match the data similarity kernel defined by the augmentations used. We show how this idealized loss can be reformulated to a functionally equivalent loss that is more efficient to compute. We study the implicit bias of using gradient descent to minimize our reformulated loss function, and find that using a stronger orthogonalization constraint with a reduced projector dimensionality should yield good representations. Furthermore, the theory tells us that approximating the reformulated loss should be improved by increasing the number of augmentations, and as such using multiple augmentations should lead to improved convergence. We empirically verify our findings on CIFAR, STL and Imagenet datasets, wherein we demonstrate an improved linear readout performance when training a ResNet-backbone using our theoretically grounded recommendations. Remarkably, we also demonstrate that by leveraging these insights, we can reduce the pretraining dataset size by up to 2\(\) while maintaining downstream accuracy simply by using more data augmentations. Taken together, our work provides theoretically grounded recommendations that can be used to improve SSL convergence and efficiency.

## 1 Introduction

Unsupervised representation learning, i.e., learning features without human-annotated labels, is critical for progress in computer vision. Modern approaches, grouped under the _self-supervised learning (SSL)_ umbrella, build on the core insight that similar images should map to nearby points in the learned feature space - often termed as the _invariance criterion_. Current SSL methods can be broadly categorized into contrastive and non-contrastive algorithms, based on whether they formulate their loss functions using negative samples or not, respectively.

Despite this difference in their loss formulations, recent theoretical work has established an equivalence between the contrastive and non-contrastive SSL frameworks . This work shows that these different SSL formulations are ultimately minimizing a loss that encourages the learning of features that best match the data similarity kernel defined by the augmentations used. However, this notion of theoretical equivalence holds only in the limit of ideal pretraining settings, i.e. with access to infinite data and compute budget, and the feature learning behavior of different SSL algorithms in practical scenarios is still not well understood. Therefore, researchers often use empirically driven heuristics that are theoretically ungrounded to design successful applications, such as (i) a high-dimensional projector head for non-contrastive SSL or (ii) the use of two augmentations per image . Moreover, existing SSL algorithms are extremely data-hungry, relying on large-scale datasets  or data engines  to achieve good representations. While this strategy works exceptionally well in data-rich settings (like training on natural-images), it is not viable in data-constrained settings (like medical imaging), where samples are relatively scarce.

With these challenges in mind, the primary focus of this work is to develop theoretically grounded recommendations for improving the effectiveness and efficiency of feature learning, both with respect to the required compute budget as well as data points. Like any unsupervised representation learning algorithm, features learned through SSL depend on three factors: (i) implicit bias of the architecture, (ii) explicit invariance imposed by data augmentations, (iii) implicit bias of the learning rule. While previous works predominantly studied the role of the model architecture capacity and loss function, and their interplay with data augmentations , our approach broadens this perspective by also considering the role of the learning rule (gradient descent) in optimizing these loss functions. Specifically, we extend the previous theoretical findings  that unified the desiderata of different SSL algorithms. We reformulate the idealized unifying loss to propose a functionally equivalent loss that is more compute-efficient (see Figure 1). Based on our loss formulation, we provide two practical recommendations that can help improve the efficiency of SSL pipelines while maintaining good performance. First, we show that optimizing the reformulated loss using gradient descent can often reduce the orthogonality among the learned embeddings, thereby leading to an inefficient use of the projector network's capacity. Consequently, we recommend using a stronger orthogonalization constraint to eliminate the requirement of high-dimensional projector heads, thereby significantly reducing the parameter overhead of good feature learning. Second, we show that increasing the number of augmentations leads to a better estimate of the data similarity kernel. Consequently, we recommend using more augmentations to improve optimization convergence and learn better features earlier in training.

We empirically verify our theoretically grounded recommendations using the popular ResNet backbone on benchmark datasets: CIFAR, STL and Imagenet. Strikingly, we show that our multi-augmentation approach can learn good features even with _half_ of the samples in the pretraining dataset. Our recommendations provide a path towards making SSL pretraining more data and compute-efficient without harming performance and could unlock massive performance gains in data-constrained setups. In summary, our core contributions are as follows:

Figure 1: Design of existing SSL algorithms relies on heuristics. **(A)** Augmentation graphs are common in vision pretraining, providing generalizable features for downstream tasks. (B) We propose an equivalent loss function for SSL pretraining that recovers the same eigenfunctions more efficiently than existing approaches.

* **Efficient SSL loss formulation:** We propose an functionally equivalent and compute-efficient formulation of the SSL desiderata that yields the eigenfunctions of the augmentation-defined data similarity kernel.
* **Role of heuristics:** Based on our loss formulation and the implicit bias of gradient descent in optimizing this loss, we provide a mechanistic explanation for the role of projector dimensionality and the number of data augmentations. Consequently, we empirically demonstrate that low-dimensional projector heads are sufficient and that using more augmentations leads to learning better representations.
* **Data efficient SSL:** Leveraging the convergence benefits of the multi-augmentation SSL framework, we empirically demonstrate that we can learn good features with significantly smaller datasets (up to 2\(\)) without harming downstream linear probe performance.

## 2 Preliminaries

**Existing SSL approaches in computer vision** In recent years machine learning researchers have developed a number of effective approaches for learning from data without labels. The most popular approaches use augmentations of data points as targets for themselves. One of the first was a Simple framework for Contrastive Learning (SimCLR), which relied on an infoNCE loss with augmentations of an image as positive targets and augmentations of other images as negative samples (to construct the contrastive loss) . Other works have relied on non-contrastive approaches, notably BarlowTwins  and VICReg . BarlowTwins, which was inspired by the ideas of the neuroscientist Horace Barlow (cite), also uses augmentations of images, but it instead aims to optimize the covariance structure of the representations in order to reduce redundancies in the feature space . Variance Invariance Covariance Regularization (VICReg) was a modification of BarlowTwins that added a variance term in the loss in order to ensure that every feature dimension has a finite variance . In this paper we will focus on non-contrastive methods like BarlowTwins and VICReg, but in line with previous work , we also consider how these approaches relate to contrastive methods like SimCLR.

**Formalizing the self-supervised learning problem** Now, we will formalize the unsupervised representation learning problem for computer vision. In particular, we assume access to a dataset \(=\{x_{1},x_{2},...,x_{n}\}\) with \(x_{i}^{p}\) consisting of unlabeled images. The objective is to learn a \(d\)-dimensional representation (\(d<p\)) that is useful across multiple downstream applications. We focus on learning the parameters of a deep neural network \(f_{}_{}\), using the multi-augmentation SSL framework, wherein multiple views of an image are used to optimize the pretraining loss function, \(_{pretrain}(f_{},)\)

**Non-Contrastive Self-Supervised Learning** (NC-SSL) algorithms impose invariance to data augmentations, while imposing regularization on the geometry of the learned feature space. More generally, \(_{pretrain}\) can be formulated with two terms (i) \(_{invariance}\): to learn invariance to data augmentations and (ii) \(_{collapse}\): regularization to prevent collapsing the feature space to a trivial solution.

\[_{pretrain}:=_{invariance}+_{collapse}\] (1)

where \(\) denotes a hyperparameter that controls the importance of the collapse-preventing term relative to the invariance term. This formulation separates features that are invariant to the augmentations from those that are sensitive to them. Intuitively, the ideal feature space is more sensitive to semantic attributes (e.g. "that's a dog") and less sensitive to irrelevant attributes (e.g. "direction the dog is facing"), facilitating generalization to new examples.

**Data Augmentation graph** was introduced by Haochen _et al._ to analyze contrastive losses, like SimCLR . Briefly, we define a graph \((,)\) that captures the relationship between images derived from all possible data augmentations. The vertex set (\(,_{A}\)) is each augmented sample in a dataset, \(\), and the adjacency matrix, \(\), denotes the similarity between pairs of vertices. Let \(x_{0}\) be an image in \(\), and let \(z=M(x_{0})\) be a random data augmentation of the image, \(x_{0}\). We define the probability density of reaching \(z\) from \(x_{0}\) via a choice of mapping \(M\):

\[p(z x_{0})=(z=M(x_{0})),\] (2)

Since the mapping is not generally invertible (e.g., cropping), we observe that \(p(x_{0} z) p(z x_{0})\). Using this definition, we now formally define the strength of the edge between nodes \(x,z\) of the augmentation graph as the joint probability of generating augmentations \(x,z\) from the same image \(x_{0}_{X}\). Notably, the edge strength of the (degree-normalized) augmentation graph is equivalent to the data similarity kernel, defined in . Formally,

\[k^{DAF}(x,z)=w_{xz}:=_{x_{0}_{X}}[)}{p( x)})}{p(z)}]\] (3)

The magnitude of \(w_{xz}\) captures the augmentation-defined similarity between \(x\) and \(z\). A higher value of \(w_{xz}\) indicates that both patches are more likely to come from the same image and, thereby, are more similar.

The desiderata of different SSL algorithms can be understood as learning features \(F\) that best capture \(k^{DAF}(x,z)\), i.e. \(F(x)^{T}F(z) k^{DAF}(x,z)\). Recent theoretical work has shows that different SSL losses can be formulated as special cases of the objective function that recovers the top-d eigenfunctions of \(k^{DAF}(x,z)\).

\[_{ssl}(F)=_{x,z}[(k^{DAF}(x,z)-F(x)^{ T}F(z))^{2}]\] (4)

Note that all rotations of \(F\) that don't change its span define an equivalence class of solutions to Equation (4) and make no difference for the downstream generalization of a linear probe. Based on this insight, we define an equivalence among learned feature spaces:

**Definition 2.1**.: Let \(F(x)=(f_{1}(x), f_{d}(x))\) be a \(d\)-dimensional feature vector (a vector of functions). Define the subspace

\[V=V(F)=\{h:X h(x)=w F(x), w^ {d}\}\] (5)

to be the span of the components of \(F\). Given an \(n\)-dimensional feature vector, \(G(x)=(g_{1}(x),,g_{n}(x))\) we say the features \(G\) and \(F\) are equivalent, if \(V(F)=V(G)\).

## 3 Implicit bias of non-contrastive SSL loss and optimization

We extend the recent theoretical results  to propose a compute-efficient reformulation of the loss function of the SSL desiderata that yields equivalent features, i.e. the functions spanning the eigenfunctions of the augmentation-defined data similarity kernel, \(k^{DAF}\). Furthermore, we study the role of gradient descent in optimizing this loss function and uncover a selection and primacy bias in feature learning. Specifically, we find that gradient descent tends to learn the dominant eigenfunctions (eigenfunctions corresponding to larger eigenvalues) earlier during training, and often over-represents these eigenfunctions under weak orthogonalization constraints.

Consequently, we propose employing a stronger orthogonalization constraint during optimization when using a low-dimensional projector to ensure that learned features are equivalent to those learned with a high-dimensional projector. Furthermore, we argue that using more augmentations improves our sample estimate of \(k^{DAF}\), thereby aiding the eigenfunction optimization problem. We dedicate the rest of this section to highlight our key theoretical insights, and practical recommendations that follow them.

### Features in terms of data augmentation kernels

Let us define a kernel operator, \(T_{k}\), for a positive semi-definite data augmentation kernel, \(k^{DAF}\).

\[T_{k}f(x)=_{z_{X}}[k(z,x)f(z)]\] (6)

such that Equation (4) can be equivalently written as (Equation 5 of )

\[_{ssl}(F)= F,(I-T_{k})F_{_{A}}\] (7)

We can now use Mercer's theorem to factorize \(k^{DAF}\) into corresponding spectral features \(G:X_{2}\) (where \(_{2}\) represents square summable sequences) [15; 16; 31]. However, note that computing \(k^{DAF}\) (or \(T_{k}\)) is expensive as it requires computing the overlap among all augmentations of every pair of data points. Instead of computing the eigenfunctions of \(T_{k}\) directly, we propose using an alternative operator \(T_{M}\):

\[T_{M}f(x)=_{x_{0} M(x)}[f(x_{0})]=_{x_{0}}[p (x_{0} x)f(x_{0})]\] (8)which averages the values of the function, \(f\), over the augmented images \(x_{0}=M(x)\) of the data, \(x\). We show that \(T_{M}^{T}T_{M}\) is equivalent to \(T_{k}\), and therefore \(T_{M}\) and \(T_{k}\) have shared eigenfunctions.

**Theorem 3.1**.: _Let \(G(x)\) be the infinite Mercer features of the backward data augmentation covariance kernels, \(k^{DAB}\). Let \(F(x)=(f_{1}(x),,f_{N_{k}}(x))\) be the features given by minimizing the following data augmentation invariance loss_

\[L(F)=_{i=1}^{N_{k}}\|T_{M}f_{i}-f_{i}\|_{L^{2}(_{X})}^{2},(f_{i},f_{j})_{_{X}}=_{ij}\] (9)

_which includes the orthogonality constraint. Then, \(V(F) V(G)\), \(_{N_{k}}V(F)=V(G)\)._

As shown in the Appendix B, \(L(F)\) is equivalent to a constrained optimization formulation of the BarlowTwins loss. Furthermore, \(L(F)\) with the additional constraint that \((f_{i},f_{i})\  i\{1,2 N_{k}\}\) is the constrained optimization formulation of the VICReg loss.

### The implicit bias of gradient descent

Next, we investigate how the use of gradient descent for optimizing \(L(F)\) influences the characteristics of the learned feature space, \(V(F)\). Given the similarity in its form with that of the BarlowTwins loss, we build on recent findings that demonstrate the sequential nature of learning eigenfunctions when optimizing the BarlowTwins loss under a strong orthogonalization regularization . Since strong orthogonalization is seldom used in practice due to instabilities in training [5; 43], we believe studying the learning dynamics under weak orthogonalization regularization (i.e. low values of \(\) in Equation (1)) is more relevant to provide recommendations for practitioners.

**Theorem 3.2**.: _(Informal) Let us denote the span of the feature space at initialization as \(V(F_{0})\) and after training as \(V(F_{T})\). For small initialization of the network's weights, the alignment of \(V(F_{T})\) with the eigenfunctions of \(T_{k}\) depend on two factors: (i) alignment of \(V(F_{0})\) with the eigenfunctions of \(T_{k}\); (ii) singular values of \(T_{k}\)._

Under weak orthogonalization constraints, the network tends to learn features that are strongly aligned with eigenfunctions corresponding to large singular values. We refer to this property as the "selection" bias of gradient descent, wherein gradient descent selects certain eigenfunctions based on the corresponding singular values. This selection bias leads to redundancy among the learned feature space, thereby reducing the effective dimensionality of the network's output space compared to its ambient dimensionality. We will leverage this finding to improve the parameter overhead of good feature learning using BarlowTwins and VICReg loss frameworks.

### Takeaway 1: Low-dimensional projectors can yield good representations

Given the proximity of the formulation of Equation (9) to that of BarlowTwins and VICReg losses, we will leverage existing heuristics that have been shown to work in practice. As such, BarlowTwins and VICReg frameworks call for high-dimensional projectors while using a weak orthogonalization regularization to facilitate good feature learning. We know, from Theorem 3.1, that the eventual goal of these frameworks is to learn the eigenfunctions of the underlying data similarity graph. For example, since the intrinsic dimensionality of Imagenet is estimated to be \( 40\), it is not unreasonable to expect that the span of desired features would be of similar dimensionality. It is, thus, intriguing that the current practice would suggest using an \( 8192\)-dim projector head to capture the intricacies of the corresponding augmentation-defined data similarity kernel. This discrepancy can be explained by analyzing the learning dynamics, as in Theorem 3.2. Notably, a high-dimensional projector is likelier to have a greater initialization span than its low-dimensional counterpart, thereby increasing the alignment between \(V(F_{0})\) and relevant eigenfunctions of \(T_{k}\). We hypothesize that a stronger orthogonalization constraint for low-dimensional projectors can rectify this issue, reducing the redundancy in the network's output space and rendering it sufficient for good feature learning.

### Takeaway 2: Multiple augmentations improve kernel approximation

By comparing the invariance criterion formulation in the standard BarlowTwins and VICReg losses to Equation (7), it can be inferred that current practices use a sample estimate of \(T_{k}\). Using only two augmentations per sample yields a noisy estimate of \(T_{k}\), yielding spurious eigenpairs  (seeAppendix C). These spurious eigenpairs add stochasticity in the learning dynamics, and coupled with Theorem 3.2, increase the redundancy in the learned feature space . We hypothesize that improving this estimation error by increasing the number of augmentations could alleviate this issue and improve the speed and quality of feature learning.

Of course, increasing the number of augmentations (\(m\)) in the standard BarlowTwins and VICReg loss improves the estimate of \(T_{k}\) but comes with added compute costs - a straightforward approach would involve calculating the invariance loss for every pair of augmentations, resulting in \((m^{2})\) operations. However, Theorem 3.1 proposes an alternative method that uses the sample estimate of \(T_{M}\), thereby requiring only \((m)\) operations, and hence is computationally more efficient while yielding functionally equivalent features (see Appendix B). In summary, Theorem 3.1 establishes a mechanistic role for the number of data augmentations, paving the way for a computationally efficient multi-augmentation framework:

\[(F)=_{x_{X}}[_{i=1}^{N_{k}}_{j=1}^{m }\|(x)}-f_{i}(x_{j})\|_{L^{2}(_{X})}^{2}],\ (f_{i},f_{j})_{_{X}}=_{ij}\] (10)

where \((x)}=_{j=1}^{m}f_{i}(x_{j})\) is the sample estimate of \(T_{M}f_{i}(x)\).

## 4 Experiments

In our experiments, we seek to (i) provide empirical support for our theoretical insights and (ii) present practical primitives for designing efficient SSL routines. Since our proposed loss function is closest to the formulation of BarlowTwins/VICReg, we present empirical evidence comparing our proposal to these baselines. In summary, with extensive experiments across learning algorithms (BarlowTwins & VICReg) and training datasets (CIFAR-10, STL-10 & Imagenet-100), we establish the following:

* **low-dimensional projectors** can yield _good representations_.
* **multi-augmentation** improves downstream accuracy, as well as convergence rate.
* multi-augmentation **improves sample efficiency** in SSL pretraining, i.e., recovering similar performance with significantly fewer unique unlabelled samples.

**Experiment Setup**: We evaluate the effectiveness of different pretraining approaches using image classification as the downstream task. Across all experiments, we pretrain a Resnet feature encoder backbone for 100 epochs (see Appendix E.1 for longer pretraining results) and use linear probing on the learned representations1. All runs are averaged over 3 seeds; error bars indicate standard deviation. Other details related to optimizers, learning rate, etc., are presented in the Appendix D.

Existing works recommend using high-dimensional MLPs as projectors (e.g., d=8192 for Imagenet in [5; 43]), and show significant degradation in performance when using lower-dimensional projectors for a fixed redundancy coefficient (\(\)). To reproduce this result, we run a grid search to find the optimal coefficient \((_{8192}^{*})\) for \(d=8192\) and show that performance progressively degrades for lower \(d\) if the same coefficient \(_{8192}^{*}\) is reused for \(d\{64,128,256,512,1024,2048,4096,8192\}\).

  _pdim_ &  &  \\  & fixed \(\) & optimal \(^{*}\) & fixed \(\) & optimal \(^{*}\) \\ 
64 & \(73.6 0.9\) & \(82.1 0.2\) & \(68.9 0.2\) & \(81.9 0.1\) \\
256 & \(75.9 0.7\) & \(\) & \(75.3 0.2\) & \(81.9 0.3\) \\
1024 & \(81.3 1.0\) & \(82.9 0.3\) & \(79.2 0.9\) & \(\) \\
8192 & \(82.2 0.4\) & \(82.2 0.4\) & \(80.4 1.5\) & \(80.4 1.5\) \\  

Table 1: Optimizing for orthogonality appropriately allows low-dimensional projectors to match the performance (on CIFAR-10) of much higher-dimensional projectors.

Our insights in Section 3.3 suggest low-dimensional projectors should recover similar performance with appropriate orthogonalization. To test this, we find the best \(\) by performing a grid search independently for each \(d\{64,128,256,512,1024,2048,4096,8192\}\). As illustrated in Figure 2, using low-dimensional projectors yield features with similar downstream task performance, compared to the features obtained using high-dimensional projectors. Strikingly, we also observe that the optimal \(_{d} 1/d\), which aligns with our theoretical insights.

**Recommendation:** Start with low-dimensional projector, using \(=()\), and sweep over \((pdim=d,=())\) if needed.

### Multiple Augmentations Improve Performance and Convergence

Although some SSL pretraining approaches, like SWeV , incorporate more than two views, the most widely used heuristic in non-contrastive SSL algorithms involves using two views jointly encoded by a shared backbone. In line with this observation, our baselines for examining the role of multiple augmentations use two views for computing the cross-correlation matrix.

To demonstrate the role of multiple augmentations in pretraining, we adapt the invariance criterion of BarlowTwins/VICReg to be in line with Equation (10). In particular, for \(\#augs\{2,4,8\}\), we

Figure 3: Using multiple augmentations improves representation learning performance and convergence. (A-C) Across BarlowTwins for CIFAR-10, STL-10 and Imagenet-100 pretraining, using 4 augmentations instead of 2 helps improve performance. Please see Appendix E.3 for more results.

Figure 2: Low-dimensional projectors can yield good representations. We demonstrate that using a higher orthogonality constraint, \(\), for lower projector dimensionality can achieve similar performance over a wide range of projector dimensions (\(d\)).

pretrain a Resnet-50 encoder with our proposed loss. Building on the insight from the previous section, we use a 256-dimensional projector head for all multi-augmentation experiments.

In Figure 3, we track the downstream performance of the pretrained models across training epochs. For performance evaluation, we use the linear evaluation protocol as outlined by . Figure 3(A-C) shows that pretraining with multiple augmentations outperforms the 2-augmentation baseline. Furthermore, we observe that the four-augmentation pretrained models converge faster (both in terms of the number of epochs and wall-clock time) than their two-augmentation counterparts (see Figure 3(D-F)). Additionally, we show in Appendix E.2 that our framework can also be applied to multi-augmentation settings like SWaV, where not all augmentations are of the same resolution.

**Recommendatation:** Using multiple augmentations ( \(>2\)) is likely to improve convergence as well as downstream accuracy.

### Sample Efficient Multi-augmentation Learning

Data Augmentation can be viewed as a form of data inflation, where the number of training samples is increased by \(k\) (for \(k\) augmentations). In this section, we examine the role of multi-augmentation in improving sample efficiency. In particular, we are interested in understanding if the same performance can be achieved with a fraction of the pretraining dataset, simply by using more augmentations.

Figure 4: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins pretraining on CIFAR-10, STL-10 and Imagenet-100 for the same effective dataset size (\(\#augs\#unique\_samples\)), using more patches improves performance at the same epoch (A-C). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime.

  augs & pdim &  Percentage \\ of Dataset \\  &  **BarlowTwins** \\ Time (min) \\  & 
 **VICReg** \\ Time (min) \\  \\ 
2 & 8192 & 100 \% & 63.43 \(\) 0.02 & 66.05 \(\) 0.01 \\
2 & 256 & 100 \% & 39.52 \(\) 0.04 & 40.64 \(\) 0.04 \\
4 & 256 & 50 \% & 28.25 \(\) 0.01 & **32.39\(\) 0.01** \\
8 & 256 & 25 \% & **27.74\(\) 0.01** & 34.76 \(\) 0.01 \\  

Table 3: Time required to pass 80% accuracy on CIFAR-10 when pretraining on fraction of the dataset, while using multiple augmentations. See Figure 5 for further discussion.

To examine the relation between the number of augmentations and sample efficiency, we fixed the effective size of the inflated dataset. This is achieved by varying the fraction of the unique samples in the pretraining dataset depending on the number of augmentations \(k\{2,4,8\}\), e.g., we use 50% of the dataset for 4 views. We then evaluate the performance of the pretrained models on the downstream task, where the linear classifier is trained on the same set of labeled samples. Strikingly, Figure 4 shows that using multiple augmentations can achieve similar (sometimes even better) performance with lesser pretraining samples, thereby indicating that more data augmentations can be used for feature learning to compensate for smaller pretraining datasets.

**Recommendation:** In a low-data regime, using diverse & multiple augmentations can be as effective as acquiring more unique samples.

## 5 Related Work

**Self-Supervised Pretraining** requires significant compute resources and most practitioners rely on empirical heuristics (see SSL cookbook  for a summary). While recent advances in SSL theory explore learning dynamics in linear (or shallow) models [39; 40], with a focus on understanding dimensionality collapse [20; 24], the theoretical underpinnings of most of the heuristics considered essential for good feature learning, are missing.

**Contrastive SSL** has received more theoretical attention, owing to its connection with metric learning and noise contrastive estimation [4; 25; 29]. In particular, HaoChen _et al._ provide a theoretical framework for the SimCLR loss from an augmentation graph perspective, which leads to practical recommendations. Subsequently, Garrido _et al._ establish a duality between contrastive and non-contrastive learning objectives, further bridging the gap between theory and practice.

**Non-contrastive SSL** algorithms' theoretical foundations have received more attention recently [9; 44]. Prior works [2; 18; 19] have demonstrated that with modified learning objectives, low-dimensional projectors yield representations with good downstream performance. Similarly, previous works have demonstrated notable performance boosts when using a multi-patch framework in contrastive  and non-contrastive SSL [10; 42]. However, the theoretical basis for the benefits and trade-offs of either low-dimensional projectors or multiple augmentations is largely unclear. It is worth noting that Schaeffer _et al._ present an information-theoretic perspective of the recently proposed non-contrastive SSL loss that leverages multiple augmentations, namely MMCR , but the computational advantages of using multiple augmentations on the learning dynamics is an active area of research.

**Deep Learning theory** has made significant strides in understanding the optimization landscape and dynamics of supervised learning . In concurrent works [9; 44], the interplay between the inductive bias of data augmentations, architectures, and generalization has been explored from a purely theoretical perspective, establishing an equivalence among different SSL losses . Furthermore, Simon _et al._ used a more straightforward formulation of the BarlowTwins loss and investigated the learning dynamics in linearized models for the case when the invariance and orthogonalization losses have equal penalties. Although such a setting rarely used in practice, their approach serves as an inspiration for our work in studying the learning dynamics of non-contrastive SSL losses.

## 6 Discussion

**Summary**: Our work builds on existing theoretical results that establish an equivalence among different SSL frameworks, and proposes a compute-efficient reformulation of the common SSL loss. Using this loss reformulation and a study of the optimization dynamics, we proposed practical recommendations to improve the sample and compute efficiency of SSL algorithms. Specifically, we recommended low-dimensional projectors with increased orthogonality constraints and multi-augmentation frameworks, and we verified the effectiveness of these recommendations empirically. It is worth noting that our multi-augmentation formulation improves the efficiency of learning without altering the desiderata of SSL, i.e. the network learns the same feature space using our proposed multi-augmentation framework as with the original SSL formulation in the limit of infinite pretraining budget. To demonstrate this equivalence between the original SSL loss and our proposed version, we show in Appendix E.1 that longer pretraining on the 2-augmentation loss leads to similar downstream performance as the multi-augmentation versions (4 and 8 augmentations).

We also showed that the multi-augmentation framework can be used to learn good features from fewer unique samples in the pretraining dataset simply by improving the estimation of the data augmentation kernel. This result has direct implications on improving the Pareto frontier of samples-vs-performance for SSL pretraining, wherein we can achieve better downstream performance when limited number of samples are available in the pretraining dataset.

Pareto Optimal SSLIn the context of sample efficiency, training a model using two augmentations with different fractions of the dataset leads to a natural Pareto frontier, i.e., training on the full dataset achieves the best error but takes the most time (**Baseline (2-Aug)**). Our extensive experiments demonstrate that using more than two augmentations improves the overall Pareto frontier, i.e., achieves better convergence while maintaining accuracy (**Multi-Aug**). Strikingly, as shown in Figure 5, we observe that we can either use a larger pretraining dataset or more augmentations for a target error level. Therefore, the number of augmentations can be used as a knob to control the sample efficiency of the pretraining routine.

**Connections to Downstream performance**: While our core theoretical results are aimed at accelerating convergence of the SSL loss itself, our empirical results highlight an improved downstream task performance earlier during pretraining. While this discrepancy might seem counter-intuitive at first, it is worth noting that the SSL loss inherent influences downstream performance as it encourages clustering of semantically similar images in the representation space. Such clustering properties in the representation space facilitates easier classification through methods k-nearest neighbors or linear decoding for a large number of tasks that rely on the semantic content of images. Previous works  have discussed in detail how certain geometric properties of the learned representation space are connected to the linear classification performance for arbitrary decision boundaries, in expectation. However, an in-depth analysis of downstream tasks that are more amenable to linear decoding from the learned SSL representation space requires framing metrics of alignment between the pretraining objective (SSL desiderate) and the downstream task labels, and is an active area of research.

**Open Questions**: Looking ahead, it would be exciting to extend this analysis to other categories of SSL algorithms, such as Masked AutoEncoders (MAE). Furthermore, our insights provide opportunities to explore sample-efficient methods that rely on less data, which is particularly important in critical domains such as medical imaging, where data is often relatively scarce and expensive. On a different note, it is intriguing that animals often spend extended periods of time exploring novel objects, likely to gain multiple views of the object . Given the theoretical underpinnings of the computational benefits of multi-augmentation SSL outlined in our work, it would be exciting to develop models of biological learning that leverage these insights and enable sample-efficient continual learning in similar environments.

**Limitations**: Our algorithm relies on multiple augmentations of the same image to improve the estimation of the data-augmentation kernel. Though this approach speeds up the learning process, it also adds some extra computational overhead, which means that the impact of faster learning on wall-clock time is less than might be hoped for. One way to mitigate the effects of this limitation would be to scale up to a multi-GPU setting, since the computations for each augmentation can be run on a separate GPU in parallel. This could help ensure that the improved speed of learning directly translates to a significantly reduced wall-clock time for training.

**Impact Statement**: The goal of our work is to advance the general field of visual representation learning. Although there are potential downstream societal consequences of our work, we feel there are no direct consequences that must be specifically highlighted here.

Figure 5: Using \(>2\) augmentations with a fraction of the dataset improves overall Pareto frontier, speeding runtime up to \( 2\).