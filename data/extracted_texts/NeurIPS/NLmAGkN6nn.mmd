# PTQ4DiT: Post-training Quantization for

Diffusion Transformers

 Junyi Wu\({}^{1,3,}\)1 Haoxuan Wang\({}^{1,*}\)2 Yuzhang Shang\({}^{2}\)3 Mubarak Shah\({}^{3}\) Yan Yan\({}^{1,}\)2

\({}^{1}\)University of Illinois Chicago \({}^{2}\)Illinois Institute of Technology \({}^{3}\)University of Central Florida

https://github.com/adreamwu/PTQ4DiT

###### Abstract

The recent introduction of Diffusion Transformers (DiTs) has demonstrated exceptional capabilities in image generation by using a different backbone architecture, departing from traditional U-Nets and embracing the scalable nature of transformers. Despite their advanced capabilities, the wide deployment of DiTs, particularly for real-time applications, is currently hampered by considerable computational demands at the inference stage. Post-training Quantization (PTQ) has emerged as a fast and data-efficient solution that can significantly reduce computation and memory footprint by using low-bit weights and activations. However, its applicability to DiTs has not yet been explored and faces non-trivial difficulties due to the unique design of DiTs. In this paper, we propose **PTQ4DiT**, a specifically designed PTQ method for DiTs. We discover two primary quantization challenges inherent in DiTs, notably the presence of salient channels with extreme magnitudes and the temporal variability in distributions of salient activation over multiple timesteps. To tackle these challenges, we propose **C**hannel-wise **S**alience **B**alancing (**CSB**) and **S**pearman's \(\)-guided **S**alience **C**alibration (**SSC**). CSB leverages the complementarity property of channel magnitudes to redistribute the extremes, alleviating quantization errors for both activations and weights. SSC extends this approach by dynamically adjusting the balanced salience to capture the temporal variations in activation. Additionally, to eliminate extra computational costs caused by PTQ4DiT during inference, we design an offline re-parameterization strategy for DiTs. Experiments demonstrate that our PTQ4DiT successfully quantizes DiTs to 8-bit precision (W8A8) while preserving comparable generation ability and further enables effective quantization to 4-bit weight precision (W4A8) for the first time.

## 1 Introduction

Diffusion models have spearheaded recent breakthroughs in generation tasks . In the past, these models were based on convolutional U-Nets  as their backbone architectures . However, recent work  has revealed that the U-Net inductive bias is not essential for the success of diffusion models and even limits their scalability. Among this trend, Diffusion Transformers (DiTs)  have demonstrated exceptional capabilities in image generation by using a different backbone architecture. Different from U-Nets that carefully design downsampling and upsampling blocks with skip-connections, DiTs are constructed by repeatedly and sequentially stacking transformer blocks . This architectural choice inherits the scaling property of transformers , facilitating more flexible parameter expansion for enhanced performance. With their versatility and scalability, DiTs have been successfully integrated into advanced frameworks like Sora , demonstrating their potential as a leading architecture for future generative models .

Nonetheless, the widespread adoption of Diffusion Transformers is currently constrained by their massive amount of parameters and computational complexity. DiTs consist of a large number of repeated transformer blocks and employ a lengthy iterative image sampling process, demanding high computational costs during inference. For instance, generating a 512\(\)512 resolution image using DiTs can take more than 20 seconds and \(10^{5}\) Gflops on an NVIDIA RTX A6000 GPU. This substantial requirement makes them unacceptable or impractical for real-time applications, especially considering the potential for increased model sizes and feature resolutions.

Model quantization  is a prominent technique for accelerating deep learning models because of its high compression rate and significant reduction in inference time. This technique transforms model weights and activations into low-bit formats, which directly reduces the computational burden and memory usage. Among various methods, Post-training Quantization (PTQ) stands out as a leading approach since it circumvents the need to re-train the original model . Practically, PTQ requires only a small dataset for fast calibration, thus is highly suitable for quantizing DiTs, whose re-training process involves extensive data and computational resources .

However, quantizing DiTs in a post-training manner is non-trivial due to the complex distribution patterns in weights and activations. We discover two major challenges that impede the effective quantization of DiTs: \(\) The emergence of _salient channels_, channels with extreme magnitudes, in both weights and activations of linear layers within DiT blocks. When low-bit representations are used for these salient channels, pronounced errors compared to the full-precision (FP) counterparts are observed, incurring fundamental difficulty for quantization. \(\) The extreme magnitudes within salient activation channels significantly vary as the inference proceeds across multiple timesteps. This dynamic behavior further complicates the quantization of salient channels, as quantization strategies optimized for one timestep may fail to generalize to other timesteps. Such inconsistency, especially in salient channels that dominate the activation signals, can result in significant deviations from the full-precision distribution, leading to degradation in the generation ability of quantized models.

Targeting these two challenges, we propose a novel Post-training Quantization method specifically for Diffusion Transformers, termed **PTQ4DIT**. To address the quantization difficulty associated with salient channels, we propose **C**hannel-wise **S**alience **B**alancing (**CSB**). CSB capitalizes on an interesting observation of the salient channels that extreme values do not coincide in the same channel of activation and weight within the same layer, as shown in Figure 1 (Left). Leveraging this complementarity property, CSB facilitates the redistribution of extreme magnitudes between activations and weights to minimize the overall channel salience. Concretely, we introduce Salience Balancing Matrices, derived from the statistical properties of activation and weight distributions, to channel-wise transform both activations and weights. This transformation achieves equilibrium in their salient channels, effectively mitigating the quantization difficulty of the balanced distributions.

Recognizing the variability in activations over different timesteps, we further extend the concept of channel salience along the temporal dimension and propose **S**pearman's \(\)-guided **S**alience **C**alibration (**SSC**). This method refines the Salience Balancing Matrices to comprehensively evaluate activation

Figure 1: **(Left) Illustration of salient channels in **activation** and \(\). Note that salient activation channels exhibit variations over different timesteps (_e.g._, \(t=t_{1},t_{2},t_{3}\)), posing non-trivial quantization challenges. To mitigate the overall quantization difficulty, our method leverages the **complementarity** (activation and weight channels do not have extreme magnitude simultaneously) to redistribute channel salience between weights and activations across various timesteps. **(Right)** Quantization performance on W8A8 and W4A8, employing FID (lower is better) and IS (higher is better) metrics on ImageNet 256\(\)256 . The circle size indicates the model size.**

salience over timesteps, with more emphasis on timesteps where the complementarity between salient activation and weight channels is more significant. Furthermore, we design a re-parameterization scheme that can offline absorb these Salience Balancing Matrices into adjacent layers, thus avoiding additional computation overhead at the inference stage.

While the performance of mainstream PTQ methods degrades on DiTs, our PTQ4DiT achieves comparable performance to the FP counterpart with 8-bit weight and activation (W8A8). In addition, PTQ4DiT can generate high-quality images with further reduced weight precision at 4-bit (W4A8). To the best of our knowledge, PTQ4DiT is the first method for effective DiT quantization.

## 2 Backgrounds and Related Works

### Diffusion Transformers

Although generative models built upon U-Nets have made great advancements in the last few years, transformer-like architectures are increasingly attracting attention [39; 7; 59]. The recently explored Diffusion Transformers (DiTs)  have achieved state-of-the-art performance in image generation. Encouragingly, DiTs exhibit remarkable scalability in model size and data representation, positioning them as a promising backbone for a wide range of generative applications [4; 30; 65].

DiTs consist of \(n_{B}\) blocks, each containing a Multi-Head Self-Attention (MHSA) and a Pointwise Feedforward (PF) module [49; 11; 37], both preceded by their respective adaptive Layer Norm (adaLN) . We illustrate the DiT Block structure in Figure 2 (Left). These blocks sequentially process the noised latent and conditional information, which are both represented as tokens in a lower-dimensional latent space . In each block, conditional input \(^{d_{in}}\) is converted to scale and shift parameters (\(,^{d_{in}}\)), which are regressed through MLPs then injected into the noised latent \(^{n d_{in}}\) via adaLN:

\[(,)=(),()=()(+)+ ,\] (1)

where \(()\) is the standard Layer Norm . These adaLN modules dynamically adjust the layer normalization before each MHSA and PF module, enhancing DiTs' adaptability to varying conditions and improving the generation quality.

Despite their effectiveness, DiTs require extensive computational resources to generate high-quality images, which impedes their real-world deployment. In this paper, we devise a model quantization method for DiTs that reduces both time and memory consumption without necessitating re-training the original models, offering a robust and practical solution for enhancing the efficiency of DiTs.

Figure 2: **(Left) Overview of the Diffusion Transformer (DiT) Block . (Middle) Illustration of the linear layer in Multi-Head Self-Attention (MHSA) and Pointwise Feedforward (PF) modules, which incorporates our proposed Channel-wise Salience Balancing (CSB) and Spearman’s \(\)-guided Salience Calibration (SSC) to address quantization difficulties for both activation \(\) and weight \(\). Appendix A depicts detailed structures of the MHSA and PF modules with adjusted linear layers. (Right) Illustration of CSB and SSC in PTQ4DiT. CSB redistributes salient channels between weights and activations from various timesteps to reduce overall quantization errors. SSC calibrates the activation salience across multiple timesteps via selective aggregation, with more focus on timesteps where quantization errors can be significantly reduced by CSB.**

### Model Quantization

Model quantization is a compression technique that improves the inference efficiency of deep learning models by transforming full-precision tensors into \(b\)-bit integer approximations, leading to direct computational acceleration and memory saving [33; 62; 8; 28; 19; 64]. Formally, the quantization process can be defined as:

\[Q()=(|}{}|+ {},0,2^{b}-1),\] (2)

where \(\) denotes the full-precision tensor, \(||\) is the round-to-nearest operator , and the clamp function restricts the quantized value within the range of \([0,2^{b}-1]\). Here, \(\) and \(\) are quantization parameters subject to optimization. Among various quantization methods, Post-training Quantization (PTQ) is a dominant approach for large quantized models, as it circumvents the substantial resources required for model re-training [20; 52; 25; 44; 15]. PTQ employs a small calibration dataset to optimize quantization parameters, which aims to reduce the performance gap between the quantized models and their full-precision counterparts with minimal data and computational expenses.

PTQ has been effectively applied to a wide range of neural networks, including CNNs [20; 52; 25], Language Transformers [8; 57; 24; 23; 27], Vision Transformers [62; 13; 21; 29], and U-Net-based Diffusion models [44; 18; 51; 50]. Despite its demonstrated success, PTQ's applicability to Diffusion Transformers remains unexplored, presenting a significant open challenge within the research community. To bridge this gap, our work delves into the unique challenges of quantizing DiTs and introduces the first PTQ method for DiTs that can fruitfully preserve their generation performance.

## 3 Diffusion Transformer Quantization Challenges

Diffusion Transformers (DiTs) diverge from conventional generative or discriminative models [39; 11] through their unique design. Specifically, DiTs are constructed with a series of large transformer blocks and operate under a multi-timestep paradigm to progressively transform pure noise into images. Our analysis reveals complex distribution patterns and temporal dynamics in the inference process of DiTs, identifying two primary challenges that prevent effective DiT quantization.

**Proounced Quantization Error in Salient Channels.** The first challenge lies in systematic quantization errors in DiT's linear layers. As shown in Figure 3, activation and weight channels with significantly high absolute values are prone to substantial errors after quantization. We term these as _salient channels_, characterized by extreme values that greatly exceed the typical range of magnitudes. Upon uniform quantization (Eq. (2)), it is often necessary to truncate these extreme values in order to maintain the precision of the broader set of standard channels. This compromise can result in notable deviations from the original full-precision distribution as the sampling process proceeds, especially given DiT's layered architecture and repetitive inference paradigm.

**Temporal Variation in Salient Activation.** Another challenge of DiT quantization arises from temporal variations in the magnitudes of salient activation channels. Rather than static inputs, DiTs operate across a sequence of timesteps to generate high-quality images from random noise. Consequently, activation distributions can vary drastically within the inference process, which is particularly evident in salient channels that dominate the signal. Figure 4 demonstrates that the distribution of maximal absolute values in activation channels exhibits significant variations over different timesteps. This temporal variability introduces a non-trivial difficulty to quantization optimization: Quantization parameters effective for salient activation channels at one timestep may

Figure 3: Illustration of maximal absolute magnitudes of activation (**left**) and weight (**right**) channels in a DiT linear layer, alongside their corresponding quantization Error (MSE). Channels with greater maximal absolute values tend to incur larger errors, presenting a fundamental quantization difficulty.

not be suitable at other timesteps. Such discrepancies can exacerbate quantization errors, cumulatively impairing the generation quality. Therefore, for accurate quantization, it is imperative to capture the evolving trait of salient channels throughout the entire denoising procedure.

## 4 Ptq4DiT

To overcome the identified challenges, we propose Channel-wise Salience Balancing (CSB) and Spearman's \(\)-guided Salience Calibration (SSC) in our PTQ4DiT in Sections 4.1 and 4.2, respectively. Subsequently, we devise a re-parameterization scheme in Section 4.3, eliminating extra computational demands of PTQ4DiT during inference while maintaining the mathematical equivalence.

### Channel-wise Salience Balancing

A linear layer \(f(;)\) within MHSA and PF modules typically takes a token sequence \(^{n d_{in}}\) as input and performs linear transformation with its weight matrix \(^{d_{in} d_{out}}\), formulated as \(f(;)=,\) where \(n\) is the sequence length, and \(d_{in}\) and \(d_{out}\) denote the input and output dimensions, respectively. As discussed in Section 3, both the activation \(\) and the weight matrix \(\) exhibit salient channels that possess elements with significantly greater absolute magnitudes, which lead to large post-quantization errors.

Fortunately, large values do not coincide in the same channels of activation and weight, so these extremes do not amplify each other, as observed in Figure 3. This property suggests the feasibility of _complementarily_ redistributing the large magnitudes in salient channels between activation and weight, thereby alleviating quantization difficulties for both. Inspired by previous works on large model compression [54; 45; 61; 23], we propose **C**hannel-wise **S**alience **B**alancing (**CSB**), which employs diagonal Salience Balancing Matrices \(}\) and \(}\) to adjust the channel-wise distribution of activation and weight, as expressed by:

\[}=},}= }.\] (3)

To address the quantization difficulties, we need to achieve balanced distributions in \(}\) and \(}\), which requires \(}\) and \(}\) to capture the characteristics of salient channels. Considering that the quantization error is significantly influenced by the range of distributions [33; 57; 26], we measure the _salience_\(s\) of an activation or weight channel as the maximal absolute value among its elements:

\[s(_{j})=(|_{j}|), s(_{j})=(|_{j}|), j=1,2,,d_{in}.\] (4)

Here, \(j\) is the channel index. Consequently, the _balanced salience_\(\), representing the equilibrium between activation and weight channels, can be quantified using the geometric mean. Specifically, for the \(j\)-th channel, the balanced salience is calculated as follows:

\[(_{j},_{j})=(s(_{j}) s( _{j}))^{}.\] (5)

Building on these concepts, we proceed to construct the Salience Balancing Matrices, which modulate the salience of activations and weights with the guidance of \(\):

\[}=((_{1}, _{1})}{s(_{1})},(_{2}, _{2})}{s(_{2})},,(_{d_{ in}},_{d_{in}})}{s(_{d_{in}})}),\] (6) \[}=((_{1}, _{1})}{s(_{1})},(_{2}, _{2})}{s(_{2})},,(_{d_{ in}},_{d_{in}})}{s(_{d_{in}})}).\] (7)

Figure 4: Boxplot of maximal absolute magnitudes of activation channels in a linear layer within DiT over different timesteps, which exhibit significant temporal variations.

Following these, the balancing transformation defined by Eq. (3) will result in a complementary redistribution of channel salience between activations and weights. Specifically, for each channel \(j\), we have \(s(}_{j})=s(}_{j})=( _{j},_{j})\), thereby alleviating the quantization difficulties, as demonstrated by the reduction in overall channel salience:

\[(s_{o}(}),s_{o}(})) (s_{o}(),s_{o}()).\] (8)

Here, we characterize the overall salience \(s_{o}\) of activations or weights using the maximum salience across channels, _e.g._, \(s_{o}()=(s(_{1}),s(_{2}),,s( _{d_{in}}))\), which reflects the distribution range of elements that are quantized collectively under certain granularity.

### Spearman's \(\)-guided Salience Calibration

Diffusion Transformers (DiTs) utilize an iterative denoising process for image sampling . Under this sequential paradigm, the linear layer \(f\) receives inputs from an activation sequence \(^{(1:T)}=(^{(1)},^{(2)},,^{(T)})\), which encompasses \(T\) timesteps. Targeting a certain timestep \(t\), the salience of all activation and weight channels can be evaluated using Eq. (4):

\[(^{(t)})=(s(_{1}^{(t)}),s(_{2}^{(t)}),,s(_{d_{in}}^{(t)})),()=(s(_{1}),s(_{2}),,s(_{d_{in}})).\] (9)

While \(()\) remains consistent, we find that \(\{(^{(t)})\}_{t=1}^{T}\) exhibits significant temporal variations during the process of transforming purely random noise into high-quality images, as demonstrated in Figure 4. These fluctuations diminish the effectiveness of our CSB since quantization errors can be exacerbated by the biased estimation of activation salience among timesteps, resulting in degraded generation quality of the quantized models.

To accurately gauge the activation channel salience under multi-timestep scenarios, we propose **S**pearman's \(\)-guided **S**alience **C**alibration (**SSC**). This offers a comprehensive evaluation of activation salience, with enhanced focus allocated to the timesteps where the complementarity property is more significant, facilitating effective salience balancing between activation and weight channels. Essentially, the lower the correlation between activation salience \((^{(t)})\) and weight salience \(()\), the greater reduction effect in overall channel salience (Eq. (8)). The intuition of SSC is visualized in Figure 2 (Right). Mathematically, we formulate the _Spearman's \(\)-calibrated Temporal Salience \(_{}\)_ by selectively aggregating the activation salience along timesteps:

\[_{}(^{(1:T)})=(_{1},_{2},,_{T})( (^{(1)}),(^{(2)}),,( ^{(T)}))^{}^{d_{in}},\] (10)

where weighting factors \(\{_{t}\}_{t=1}^{T}\) are derived from a normalized exponential form of inverse Spearman's \(\) statistic [47; 55; 56]:

\[_{t}=[-((^{(t)}),( ))]}{_{=1}^{T}[-((^{( )}),())]}.\] (11)

Here, \((,)\) computes the correlation between two sequences, and \(_{t}\) serves as the weighting factor for activation salience at timestep \(t\). In this method, \(_{t}\) inversely reflects the correlation coefficient \(((^{(t)}),())\), thereby prioritizing timesteps where there is a higher degree of complementarity in salience between activations and weights. Subsequently, we utilize \(_{}\) for activation salience in Eqs. (5), (6), and (7), yielding refined Salience Balancing Matrices, denoted as \(_{}^{}\) and \(_{}^{}\). By applying SSC, we calibrate the activation salience within CSB to strategically account for the temporal variations during the denoising process. Appendix B presents the full Algorithm for PTQ4DiT.

### Re-Parameterization

Before quantization, we estimate \(_{}^{}\) and \(_{}^{}\) on a small calibration dataset generated from multiple timesteps. Then, we incorporate these matrices into the linear layers within MHSA and PF modules  to alleviate the quantization difficulty. Given that \(_{}^{}\) and \(_{}^{}\) are mutual inverses, this incorporation maintains mathematical equivalence to the original linear layer \(f\):

\[}}=(_{} ^{})(_{}^{})= .\] (12)

The proof is provided in Appendix C. Furthermore, we design a re-parameterization scheme for DiTs, allowing for obtaining \(}\) and \(}\) without extra computational burden during inference. Specifically,we update the weight matrix of linear layer \(f\) to \(}\) offline and seamlessly integrate \(^{}_{}\) into the preceding linear transformation operations. This integration includes adaptations to adaLN [38; 37] and matrix multiplications within attention mechanisms . Appendix A discusses these adaptations.

**Post-adaLN.** For linear layers following the adaLN module, we integrate \(^{}_{}\) by adjusting the scale and shift parameters (\(,^{d_{in}}\)) within adaLN:

\[}=}()=( )(^{}_{}+})+},}=^{}_{},}=^{}_{}.\] (13)

Equivalently, we fuse \(^{}_{}\) into the MLPs responsible for regressing these parameters, thus avoiding additional computation overhead at inference time. Detailed derivations are provided in Appendix D.

**Post-Matrix-Multiplication.** For linear layers after matrix multiplication, the effect of PTQ4DiT can be realized by directly absorbing the Salience Balancing Matrices into the preceding de-quantization functions associated with the matrix multiplication [12; 53; 61].

## 5 Experiments

### Experimental Settings

Our experimental setup is similar to the original study of Diffusion Transformers (DiTs) . We evaluate PTQ4DiT on the ImageNet dataset , using pre-trained class-conditional DiT-XL/2 models  at image resolutions of 256\(\)256 and 512\(\)512. The DDPM solver  with 250 sampling steps is employed for the generation process. To further assess the robustness of our method, we conduct additional experiments with reduced sampling steps of 100 and 50.

For fair benchmarking, all methods utilize uniform quantizers for all activations and weights, with channel-wise quantization for weights and tensor-wise for activations, unless specified otherwise. To construct the calibration set, we uniformly select 25 timesteps for 256-resolution experiments and 10 timesteps for 512-resolution experiments, generating 32 samples at each selected timestep. The optimization of quantization parameters follows the implementation from Q-Diffusion . Our code is based on PyTorch , and all experiments are conducted on NVIDIA RTX A6000 GPUs.

To comprehensively assess generated image quality, we employ four metrics: Frechet Inception Distance (FID) , spatial FID (sFID) [42; 34], Inception Score (IS) [42; 3], and Precision, all computed using the ADM toolkit . For all methods under evaluation, including the full-precision (FP) models, we sample 10,000 images for ImageNet 256\(\)256, and 5,000 for ImageNet 512\(\)512, consistent with conventions from prior studies [35; 44].

### Quantization Performance

We present a comprehensive assessment of our PTQ4DiT against prevalent baseline methods in various settings. Our evaluation focuses on mainstream Post-training Quantization (PTQ) methods that are widely used and adaptable to DiTs, including PTQ4DM , Q-Diffusion , and PTQD .

Figure 5: Random samples generated by PTQ4DiT and two strong baselines: RepQ*  and Q-Diffusion , with W4A8 quantization on ImageNet 512\(\)512 and 256\(\)256. Our method can produce high-quality images with finer details. Appendix E presents more visualization results.

We reimplement these methods to suit the unique structure of DiTs. Considering the architectural similarity between DiTs and ViTs , our analysis also includes RepQViT , the state-of-the-art PTQ method initially designed for ViTs. We enhance RepQ-ViT (denoted as RepQ*) by extending the calibration set to integrate temporal dynamics and customizing its advanced channel-wise and log\(\) quantizers specifically for DiTs.

Tables 1 and 2 report the outcomes on large-scale class-conditional image generation for ImageNet 256\(\)256 and 512\(\)512, respectively. Table 1 demonstrates the effectiveness of PTQ4DiT across various quantization settings and timesteps. Notably, our finding results that at 8-bit precision (W8A8), PTQ4DiT closely matches the generative capabilities of the

   Timesteps & Bit-width (W/A) & Method & Size (MB) & FID \(\) & sFID \(\) & IS \(\) & Precision \(\) \\   & 32/32 & FP & 2575.42 & 4.53 & 17.93 & 278.50 & 0.8231 \\   &  & PTQ4DM & 645.72 & 21.65 & 100.14 & 134.22 & 0.6342 \\  & & Q-Diffusion & 645.72 & 5.57 & 18.22 & 227.50 & 0.7612 \\  & & PTQQ & 645.72 & 5.69 & 18.42 & 224.26 & 0.7594 \\  & & RepQ* & 645.72 & **4.51** & 18.01 & 264.68 & 0.8076 \\  & & **Ours** & 645.72 & 4.63 & **17.72** & **274.86** & **0.8299** \\   &  & PTQ4DM & 323.79 & 72.58 & 52.39 & 35.79 & 0.2642 \\  & & Q-Diffusion & 323.79 & 15.31 & 26.04 & 134.71 & 0.6194 \\  & & PTQD & 323.79 & 16.45 & **22.29** & 130.45 & 0.6111 \\  & & RepQ* & 323.79 & 23.21 & 28.58 & 104.28 & 0.4640 \\  & & **Ours** & 323.79 & **7.09** & 23.23 & **201.91** & **0.7217** \\   & 32/32 & FP & 2575.42 & 5.00 & 19.02 & 274.78 & 0.8149 \\   &  & PTQ4DM & 645.72 & 15.36 & 79.31 & 172.37 & 0.6926 \\  & & Q-Diffusion & 645.72 & 7.93 & 19.46 & 202.84 & 0.7299 \\  & & PTQD & 645.72 & 8.12 & 19.64 & 199.00 & 0.7295 \\  & & RepQ* & 645.72 & 5.20 & 19.87 & 254.70 & 0.7929 \\  & & **Ours** & 645.72 & **4.73** & **17.83** & **277.27** & **0.8270** \\   &  & PTQ4DM & 323.79 & 89.78 & 57.20 & 26.02 & 0.2146 \\  & & Q-Diffusion & 323.79 & 54.95 & 36.13 & 42.80 & 0.3846 \\  & & PTQD & 323.79 & 55.96 & 37.24 & 42.87 & 0.3948 \\  & & RepQ* & 323.79 & 26.64 & 29.42 & 91.39 & 0.4347 \\  & & **Ours** & 323.79 & **7.75** & **22.01** & **190.38** & **0.7292** \\   & 32/32 & FP & 2575.42 & 6.02 & 21.77 & 246.24 & 0.7812 \\   &  & PTQ4DM & 645.72 & 17.52 & 84.28 & 154.08 & 0.6574 \\  & & Q-Diffusion & 645.72 & 14.61 & 27.57 & 153.01 & 0.6601 \\   & & PTQD & 645.72 & 15.21 & 27.52 & 151.60 & 0.6578 \\   & & RepQ* & 645.72 & 7.17 & 23.67 & 224.83 & 0.7496 \\   & & **Ours** & 645.72 & **5.45** & **19.50** & **250.68** & **0.7882** \\    &  & PTQ4DM & 323.79 & 102.52 & 58.66 & 19.29 & 0.1710 \\   & & Q-Diffusion & 323.79 & 22.89 & 29.49 & 109.22 & 0.5752 \\   & & PTQD & 323.79 & 25.62 & 29.77 & 104.28 & 0.5667 \\   & & RepQ* & 323.79 & 31.39 & 30.77 & 80.64 & 0.4091 \\   & & **Ours** & 323.79 & **9.17** & **24.29** & **179.95** & **0.7052** \\   

Table 1: Performance comparison on ImageNet 256\(\)256. ‘(W/A)’ indicates that the precision of weights and activations are W and A bits, respectively.

   Timesteps & Method & FID \(\) & sFID \(\) & IS \(\) & Precision \(\) \\   & \(\) & 8.39 & 36.25 & 257.06 & 0.8426 \\  & & Q-Diffusion & 68.43 & 57.76 & 35.16 & 0.4712 \\  & & Q-Diffusion & 58.81 & 56.75 & 31.29 & 0.4878 \\  & & PTQD & 87.53 & 74.55 & 34.40 & 0.5144 \\  & & RepQ* & 59.65 & 77.31 & 33.19 & 0.3676 \\  & & **Ours** & **17.55** & **46.92** & **123.49** & **0.7592** \\   & \(\) & 9.06 & 37.58 & 239.03 & 0.8300 \\   & & PTQ4DM & 70.63 & 57.73 & 33.82 & 0.4574 \\   & & Q-Diffusion & 62.05 & 57.02 & 29.52 & 0.4786 \\   & & PTQD & 81.17 & 66.58 & 35.67 & 0.5166 \\   & & RepQ* & 62.70 & 73.29 & 31.44 & 0.3606 \\   & & **Ours** & **19.00** & **50.71** & **121.35** & **0.7514** \\   & \(\) & 11.28 & 41.70 & 213.86 & 0.8100 \\   & & PTQ4DM & 71.69 & 59.10 & 33.77 & 0.4604 \\   & & Q-Diffusion & 53.49 & **50.27** & 38.99 & 0.5430 \\   & & PTQD & 73.45 & 59.14 & 39.63 & 0.5508 \\   & & RepQ* & 65.92 & 74.19 & 30.92 & 0.3542 \\   & & **Ours** & **19.71** & 52.27 & **118.32** & **0.7336** \\   

Table 2: Performance on ImageNet 512\(\)512 with W4A8.

FP models, whereas most baseline methods experience significant performance losses. At the more stringent 4-bit weight precision (W4A8), all baseline methods exhibit more considerable degradation. For instance, under 250 timesteps, PTQ4DM  sees a drastic FID increase of 68.05. In contrast, our PTQ4DiT only incurs a slight increase of 2.56. This resilience remains evident as the number of timesteps decreases, underscoring the robustness of PTQ4DiT in resource-limited environments. Moreover, PTQ4DiT markedly outperforms mainstream methods at the higher 512\(\)512 resolution, further validating its superiority. For example, using 250 timesteps, PTQ4DiT substantially lowers FID by 41.26 and sFID by 9.83 over the second-best method, Q-Diffusion. Figure 6 depicts the efficiency-vs-efficacy trade-off on W8A8 across various timestep configurations. Our PTQ4DiT achieves comparable performance levels to FP models but with considerably reduced computational costs, offering a viable alternative for high-quality image generation. Figures 5, 8, and 9 also present randomly generated images for visual comparisons, highlighting PTQ4DiT's ability to produce images of superior quality.

### Ablation Study

To verify the efficacy of CSB and SSC, we conduct an ablative study on the challenging W4A8 quantization. Experiments are performed on ImageNet 256\(\)256 using 250 sampling timesteps. Three method variants are considered in our ablation: **(i) Baseline**, which applies basic linear quantization on DiTs, **(ii) Baseline + CSB**, which integrates CSB in the linear layers within MHSA and PF modules, where the Salience Balancing Matrices \(}\) and \(}\) are estimated based on distributions at the midpoint timestep \(\), and **(iii) Baseline + CSB + SSC**, which is the complete PTQ4DiT. Results detailed in Table 3 indicate that each proposed component improves the performance, validating their effectiveness. Particularly, CSB enhances upon the Baseline by a large margin, decreasing FID by 14.37 and sFID by 2.35, suggesting its critical role in alleviating the severe quantization difficulties inherent in DiTs. Note that with the addition of CSB, our method surpasses Q-Diffusion , a leading PTQ method for diffusion models. Moreover, integrating SSC further boosts our PTQ4DiT towards state-of-the-art performance, facilitating high-quality image generation at W4A8 precision, as shown in Figure 5.

## 6 Conclusion

This paper proposes **PTQ4DiT**, a novel Post-training Quantization (PTQ) method for Diffusion Transformers (DiTs). Our analysis identifies the primary challenges in effective DiT quantization: the pronounced quantization errors incurred by salient channels with extreme magnitudes and the temporal variability in salient activation. To address these challenges, we design **C**hannel-wise **S**alience **B**alancing (**CSB**) and Spearman's \(\)-guided **S**alience **C**alibration (**SSC**). Specifically, CSB utilizes the complementarity nature of salient channels to redistribute the extremes within activations and weights toward the balanced salience. SSC dynamically adjusts salience evaluations across different timesteps, prioritizing timesteps where salient activation and weight channels exhibit significant complementarity, thereby mitigating overall quantization difficulties. To avoid extra

   Method & Size (MB) & FID \(\) & sFID \(\) & IS \(\) & Precision \(\) \\  FP & 2575.42 & 4.53 & 17.93 & 278.50 & 0.8231 \\  Q-Diffusion & 323.79 & 15.31 & 26.04 & 134.71 & 0.6194 \\ Baseline & 323.79 & 22.54 & 27.31 & 105.55 & 0.4791 \\ + CSB & 323.79 & 8.17 & 24.96 & 187.94 & 0.7183 \\ **+ CSB + SSC (Ours)** & 323.79 & **7.09** & **23.23** & **201.91** & **0.7217** \\   

Table 3: Ablation study on ImageNet 256\(\)256 with W4A8.

Figure 6: Quantization performance on W8A8. The circle size represents the computational load (in Gflops).

computational costs of PTQ4DiT, we also devise a re-parameterization strategy for efficient inference. Experiments show that our PTQ4DiT can effectively quantize DiTs to 8-bit precision (W8A8) and further advance to 4-bit weight (W4A8) while maintaining high-quality image generation capabilities.

**Acknowledgements.** This research is supported by NSF IIS-2309073 and ECCS-2123521. This article solely reflects the opinions and conclusions of authors and not funding agencies.