# In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component

and One-Step GD Initialization

 Ruiqi Zhang

UC Berkeley

rqzhang@berkeley.edu &Jingfeng Wu

UC Berkeley

uuujf@berkeley.edu &Peter L. Bartlett

UC Berkeley and Google DeepMind

peter@berkeley.edu

###### Abstract

We study the _in-context learning_ (ICL) ability of a _Linear Transformer Block_ (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a _non-zero mean_, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization (GD-\(\)), in the sense that every GD-\(\) estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a GD-\(\) estimator. Finally, we show that GD-\(\) estimators can be efficiently optimized with gradient flow, despite a non-convex training objective. Our results reveal that LTB achieves ICL by implementing GD-\(\), and they highlight the role of MLP layers in reducing approximation error.

## 1 Introduction

The recent dramatic progress in natural language processing can be attributed in large part to the development of large language models based on _Transformers_, such as BERT , LLaMA , PaLM , and the GPT series . In some pioneering pre-trained large language models, a new learning paradigm known as _in-context learning_ (ICL) was observed (see, for example, ). ICL refers to the capability of a pre-trained model to solve a new task based on a few in-context demonstrations without updating the model parameters.

A recent line of work quantifies ICL in tractable statistical learning setups such as linear regression with a Gaussian prior (see  and references thereafter). Specifically, an ICL model takes a sequence \((_{1},y_{1},...,_{M},y_{M},})\) as input and outputs a prediction of \(y_{}\), where \((_{i},y_{i})_{i=1}^{M}\) and \((},y_{})\) are independent samples from an unknown task-specific distribution (where the task admits a prior distribution). The ICL model is pre-trained by fitting many empirical observations of such sequence-label pairs. Experiments show that Transformers can achieve an ICL risk close to that achieved by Bayes optimal estimators in many statistical learning tasks .

For ICL of linear regression with a Gaussian prior, the data is generated as

\[(,\ ), y},}^{},\ ^{2},\]

where \(}\) is a task parameter that satisfies a Gaussian prior, \(}(,\ ^{2})\). In this setup,  showed in experiments that Transformers can achieve nearly optimal ICL by matching the performance of the Bayes optimal estimator, that is, an optimally tuned ridge regression. Besides,  showed by construction that a single _linear self-attention_ (LSA) can implement _one-step gradient descent with zero initialization_ (GD-\(0\)), offering an insight into the ICL mechanism of the Transformer.

Later, theoretical works showed that optimal LSA models effectively correspond to GD-\(0\) models , trained LSA models converge to GD-\(0\) models , and GD-\(0\) models (hence LSA models) can provably achieve nearly Bayes optimal ICL in certain regimes .

**Our contributions.** In this paper, we consider ICL in a more general setup, that is, linear regression with a Gaussian prior and a _non-zero mean_ (that is, \([}]=^{*} 0\)). The non-zero mean in task prior captures a common scenario where tasks share a signal. In this setting, we show that the LSA models considered in prior papers  incur an irreducible additive approximation error. Furthermore, we show this approximation error is mitigated by considering a _linear Transformer block_ (LTB) that combines a linear multi-layer perceptron (MLP) component and an LSA component. Our results highlight the important role of MLP layers in Transformers in reducing the approximation error, and they suggest that theories about LSA  do not fully explain the power of Transformers. Motivated by this understanding, we investigate LTB in depth and obtain the following additional results:

* We show that LTB can implement _one-step gradient descent with learnable initialization_, referred to by us as GD-\(\). Additionally, we show that every optimal LTB estimator that minimizes the in-class ICL risk is _effectively_ a GD-\(\) estimator.
* Moreover, we show that the optimal GD-\(\), hence also the optimal LTB, nearly matches the performance of the Bayes optimal estimator for linear regression with a Gaussian prior and a non-zero mean, provided that the signal-to-noise ratio is upper bounded. These two results together suggest that LTB performs nearly optimal ICL by implementing GD-\(\).
* Finally, we show that the GD-\(\) estimator can be efficiently optimized by gradient descent with an infinitesimal stepsize (that is, gradient flow) under the population ICL risk, despite the non-convexity of the objective.

**Paper organization.** The remaining paper is organized as follows. We conclude this section by introducing a set of notations. We then discuss related papers in Section 2. In Section 3, we set up our ICL problems and define LTB and LSA models mathematically. In Section 4, we show a positive approximation error gap between LSA and LTB, which highlights the importance of the MLP component and motivates our subsequent efforts to study LTB. In Section 5, we connect LSA estimators to GD-\(\) estimators that are more interpretable for ICL of linear regression. In Section 6, we study the in-context learning and training of GD-\(\) estimators. We conclude our paper and discuss future directions in Section 7.

**Notations.** We use lowercase bold letters to denote vectors and uppercase bold letters to denote matrices and tensors. For a vector \(\) and a positive semi-definite (PSD) matrix \(\), we write \(\|\|_{}:=^{}}\). We write \(,\) for the inner product, which is defined as \(,:=^{}\) for vectors and \(,:=(^{})\) for matrices. We write \([i]\) as the \(i\)-th row of the matrix \(\), \(_{m,n}\) as the \((m,n)\)-th entry, and \(_{-1,-1}\) as the right-bottom entry. We write \(_{n},_{m n},_{n}\) for the zero vector, zero matrix and identity matrix, respectively. We denote the Kronecker product as \(\). For two matrices \(\) and \(\), \(\) is a linear mapping which operates as \(()=^{}\). For a positive semi-definite matrix \(\), we write \(^{}\) as its principle square root, which is the unique positive semi-definite matrix \(\) such that \(=^{}=\). We also write \(^{-}=(^{})^{+}\), where \(()^{+}\) denotes the Moore Penrose pseudo-inverse. For two sets \(A,B\) we write \(A+B\) for the Minkowski sum, which is defined as \(\{a+b:a A,b B\}.\) We also write \(a+B=\{a\}+B\) for an element \(a\) and a set \(B\). We write null\(()\) for the null set of a matrix or a tensor.

## 2 Related Works

**Empirical results for ICL in controlled settings.** The work by  first considered ICL in controlled statistical learning setups. For noiseless linear regression,  showed in experiments that Transformers match the performance of the optimal estimator (that is, _Ordinary Least Square_). Subsequent works by  extended their result to noisy linear regression with a Gaussian prior and showed that Transformers can match the performance of the Bayesian optimal estimator (that is, an optimally tuned ridge regression). Besides,  showed that the above holds even when Transformers are pretrained on a limited number of linear regression tasks. These papers only considered a Gaussian prior with a zero mean. In contrast, we consider a Gaussian prior with a non-zero mean. Our setup better captures the common scenarios where the tasks share a signal.

The empirical investigation of ICL in tractable statistical learning setups goes beyond linear regression settings. For more examples, researchers empirically studied the ICL ability of Transformers for decision trees (see , they also considered two-layer networks), algorithm selection , linear mixture models , learning Fourier series , discrete boolean functions , representation learning [13; 16], and reinforcement learning [17; 19]. Among all these settings, Transformers can either compete with the Bayes optimal estimators or expert-designed strong benchmarks. These works are not directly comparable with our paper.

**Transformer implements gradient descent.** A line of work interpreted the ICL of Transformers by their abilities to implement _gradient descent_ (GD) [35; 3; 10; 1; 38; 4; 37]. In experiments, [35; 10] showed that (multi-layer) Transformer outputs are close to (multi-step) GD outputs. When specialized to linear regression tasks,  constructed a single _linear self-attention_ (LSA) that implements _one-step gradient descent with zero initialization_ (GD-0). Subsequently,  showed that optimal LSA models effectively correspond to GD-0 models,  proved that trained LSA models converge to GD-0 models,  showed that GD-0 models (hence LSA models) can provably achieve nearly Bayes optimal ICL in certain regimes and provided a sharp task complexity analysis of the pre-training. These papers focused on LSA models. Instead, we consider a linear Transformer block that also utilizes the MLP layer.

From an approximation theory perspective, [3; 4] showed that Transformers can implement multi-step GD under general losses. In comparison, we consider a limited setting of linear regression and show the LSA models can implement one-step GD with learnable initialization (GD-\(\)), moreover, every optimal LTB model is effectively an GD-\(\) model. Both of our constructions utilize MLP layers in Transformers, highlighting its importance in reducing approximation error. Different from their results, we also show a negative approximation result that reveals the limitation of LSA models.

## 3 Preliminaries

**Model input.** We use \(^{d}\) and \(y\) to denote a feature vector and its label, respectively. Throughout the paper, we assume a fixed number of context examples, denoted by \(M>0\). We denote the context examples by \((,)^{M d}^{M}\), where each row represents a context example, denoted by \((_{}^{},y_{i})\), \(i=1,,M\). To formalize an ICL problem, the input of a model is a _token matrix_ given by [1; 38]

\[:=^{}&\\ ^{}&0^{(d+1)(M+1)}.\] (3.1)

The output of a model corresponds to a prediction of \(y\).

**A Transformer block.** Modern large language models are often made by stacking basic Transformer blocks (see, for example, ). A basic Transformer block consists of a _self-attention_ layer and a _multi-layer perceptron_ (MLP) layer , \([() ].\) Here, the MLP layer is defined as \(():=_{2}^{}( _{1}),\) where \(()\) refers to the entrywise _rectified linear unit_ (ReLU) activation function, and \(_{1},\ _{2}^{d_{f}(d+1)}\) are two weight matrices. The self-attention layer (we focus on the single-head version in this paper) is defined as \(():=+_{}^{}_{}(_{K} )^{}_{Q},\) where \(()\) refers to the row-wise softmax operator, \(_{K},\ _{Q}^{d_{k}(d+1)}\) are the key and query matrices, respectively, \(_{P},\ _{V}^{d_{v}(d+1)}\) are the projection and value matrices, respectively, and \(\) is a fixed masking matrix given by

\[:=_{M}&0\\ 0&0^{(M+1)(M+1)}.\] (3.2)

This mask matrix is included to reflect the asymmetric structure of a prompt since the label of query input \(\) is not included in the token matrix [1; 21]. In the above formulation, \(d_{k},d_{v},d_{f}\) are three hyperparameters controlling the key size, value size, and width of the MLP layer, respectively. In the single-head case, it is common to set \(d_{k}=d_{v}=d+1\) and \(d_{f}=4(d+1)\), where \(d+1\) corresponds to the embedding size (see, for example, ). Our formulation of a basic transformer block ignores all bias parameters and some popular techniques (such as layer normalization and dropout) to focus solely on the benefits brought by the model structure.

A linear Transformer block.To facilitate theoretical analysis, we ignore the non-linearities in the Transformer block (specifically, \(\) and \(\)) and work with a _linear Transformer block_ (LTB) defined as

\[f_{}:^{(d+1)(M+1)},_{2}^{}_{1}+ _{P}^{}_{V}^{} _{K}^{}_{Q}}{M}_{-1,-1},\] (3.3)

where \(\) is the token matrix given by (3.1), \([\ \ ]_{-1,-1}\) refers to the bottom right entry of a matrix, \(\) is the fixed masking matrix given by (3.2). Here, the trainable parameters are \(_{P},_{V}^{d_{v}(d+1)}\), \(_{K},_{Q}^{d_{h}(d+1)}\), and \(_{1},_{2}^{d_{f}(d+1)}\). We use the bottom right entry of the transformed token matrix as the model output to form a prediction of the label \(y\). The \(1/M\) factor is a normalization factor in linear attention and can be absorbed into trainable parameters. Finally, we denote the hypothesis class formed by LTB models as \(_{}:=\{f_{}:\ _{K},_{Q}, _{V},_{P},_{1},_{2},d_{k} d,\ d_{v } d+1,\ d_{f} 1\},\) where \(f_{}\) is defined in (3.3).

A linear self-attention.We will also consider a _linear self-attention_ (LSA) defined as

\[f_{}:^{(d+1)(M+1)}, +_{P}^{}_{V} ^{}_{K}^{}_{Q} }{M}_{-1,-1},\] (3.4)

where \(_{K},_{Q},_{P},_{V}\) are trainable parameters. An LSA model can be viewed as an LTB model without the MLP layer (setting \(d_{f}=d+1\) and \(_{1}=_{2}=\)). We remark that, unlike LTB, the residual connection in LSA plays no role because the bottom right entry of the prompt \(\) is zero (see (3.1)). A variant of the LSA model has been studied by , where \(_{K}^{}_{Q}\) and \(_{P}^{}_{V}\) are respectively merged into one matrix parameter. Similarly, we denote the hypothesis class formed by LSA models as \(_{}:=\{f_{}:_{K},_{Q}, _{V},_{P},d_{k} d,d_{v} 1\},\) where \(f_{}\) is defined in (3.4).

Linear regression tasks with a shared signal.Assume that data and context examples are generated as follows.

**Assumption 3.1** (Distributional conditions).: Assume that \((,,,y)\) are generated by:

* First, a task parameter is independently generated by \(}(^{*}, ).\)
* The feature vectors are independently generated by \(,_{1},_{M}}}{{}}(0,).\)
* Then, the labels are generated by \(y=},+,\ \ y_{i}= },_{i}+_{i},\ i=1, ,M,\) where \(\) and \(_{i}\)'s are independently generated by \(,_{1},,_{M}}}{{}}(0,^{2}).\)

Here, \(^{2} 0\), \(\), \(\), and \(^{*}^{d}\) are fixed but unknown quantities that govern the data distribution. We denote \(=(_{1},...,_{M})^{}\).

We emphasize the importance of the mean of the task parameter \(^{*}\) in Assumption 3.1. A non-zero \(^{*}\) represents a shared signal across tasks, which is arguably common in practice. This assumption is implicitly used in  where they assumed task parameters are close to a meta parameter. In comparison, the prior works for ICL of linear regression  only considered a special case where \(^{*}=0\). In this special case, they showed that a single LSA layer can achieve nearly optimal ICL by approximating one-step gradient descent from zero initialization (GD-0). In more general cases where \(^{*}\) is non-zero, we will show in Section 4 that LSA is insufficient to learn the shared signal and must incur an irreducible approximation error compared to LTB models. This sets our results apart from the prior papers.

ICL risk.We measure the ICL risk of a model \(f\) by the mean squared error,

\[(f):=(f()-y)^{2},\] (3.5)

where \(\) is defined in (3.1) and the expectation is over \(\) (equivalent to over \(\), \(\), and \(\)) and \(y\).

## 4 Benefits of the MLP Component

Our first main result separates the approximation abilities of the LTB and LSA models for ICL. Recall that an LSA model can be viewed as a special case of the LTB model with \(_{2}=_{1}=\). So the best ICL risk achieved by LTB models is no larger than the best ICL risk achieved by LSA models. However, our next theorem shows a strictly positive gap between the best ICL risks achieved by those two model classes. This result highlights the benefits of the MLP layer for reducing approximation error in Transformer.

**Theorem 4.1** (Approximation gap).: _Consider the ICL risk defined by (3.5) and the two hypothesis classes \(_{}\) and \(_{}\). Suppose that Assumption 3.1 holds. Then we have_

* \(_{f_{}}(f)\) _is independent of_ \(^{*}\)_._
* \(_{f_{}}(f)\) _is a function of_ \(^{*}\)_. Moreover,_ \[_{f_{}}(f)-_{f _{}}(f)\{,( )+^{2})^{2}}{(M+1)^{2}( ()^{2})}\}\|^{*} \|_{}^{2}.\]

The proof of Theorem 4.1 is deferred to Appendix B. Theorem 4.1 reveals a gap in terms of the approximation abilities between the hypothesis set of LTB models and that of LSA models. Specifically, the best ICL performance achieved by LTB models is independent of \(^{*}\), while the best ICL performance achieved by LSA models is sensitive to the norm of \(^{*}\). In particular, when \(\|^{*}\|_{}^{2}=(M)\), the best ICL risk of the former is smaller than that of the latter by at least a _constant_ additive term. So when \(^{*}\) is large, the hypothesis class formed by LSA models is restricted in its ability to perform effective ICL. We will show in Section 5 that the hypothesis class formed by LTB models can achieve nearly optimal ICL in this case.

We also remark that the \((1/M)\) factor in the lower bound in Theorem 4.1 is not improvable. This is because LSA models can implement a one-step GD algorithm that is _consistent_ for linear regression tasks (that is, the risk converges to the Bayes risk as the number of context examples goes to infinity), with an excess risk bound of \((1/M)\). So the approximation error gap is at most \((1/M)\). Nonetheless, the \((\|^{*}\|_{}^{2}/M)\) approximation gap between LSA models and LTB models shown by Theorem 4.1 suggests that \(^{*}\) is not learnable by LSA models during pre-training. In contrast, we will show in Section 6 that LTB models can learn \(^{*}\) during pre-training.

We emphasize that the ability of LTB to learn non-zero mean is a joint effect of an MLP component and a skip connection. Note that LSA also has a skip connection, but its skip connection is inactive. In comparison, the MLP component in LTB activates the skip connection. Therefore, we attribute the ability to learn non-zero mean to the MLP component, which is the only difference between LTB and LSA. Nonetheless, one can attribute the ability to learn non-zero mean to the skip connection: without a skip connection, LTB reduces to LSA with a potential rank constraint on the parameter, which cannot learn non-zero mean as we have proved. The above two explanations take different perspectives to interpret the same phenomenon. Finally, we remark that Theorem 4.1 holds even when \(\) and \(\) in Assumption 3.1 are not full rank.

**Does scratchpad help?** We have demonstrated that employing a single-layer LSA introduces an additional approximation error in the in-context learning problem for the linear regression task defined in Assumption 3.1. Nonetheless, are there alternative structures, apart from MLPs, that could potentially reduce this approximation error? One plausible strategy is to include a "scratchpad" in the input token. Specifically, we construct the token matrix as follows:

\[:=^{}&\\ _{M}^{}&1\\ ^{}&0^{(d+2)(M+1)}\] (4.1)

which we then input into the LSA layer. We discuss the limitation of this scheme in Appendix J. Notably, this method does not successfully recover the GD-\(\) estimator defined in Section 5. We leave it as future work to see whether the token matrix with scratchpad could implement other types of estimators that more effectively address the linear regression tasks defined in Assumption 3.1, as well as whether additional structures could help alleviate this approximation error.

**Experiments on GPT2.** Theorem 4.1 shows the importance of the MLP component in LSA models for reducing approximation error. We also empirically validate this result by training a more complex GPT2 model  for the ICL tasks specified by Assumption 3.1. In the experiments, we use a GPT2-small model (with or without the MLP component) with 6 layers and 4 heads in each layer. The experiments follow the setting in , except that we train the model using a token matrix defined in (3.1). We considere two ICL settings, which instantiates Assumption 3.1 with \(^{*}=(0,0,...,0)^{}\) and \(^{*}=(10,10,...,10)^{}\), respectively. We set \(d=20,M=40,=0,==_{d}\) in both settings. More experimental details are in Appendix I. In each setting, we train and test the model using the same data distribution. The experimental results are presented in Table 1. From Table 1, we observe that both models (with or without the MLP component) achieve a nearly zero loss when the task mean is zero. However, when the task mean is set away from zero, the GPT2 model with MLP component still performs relatively well while the GPT2 model without MLP component incurs a significantly larger loss. These empirical observations are consistent with our Theorem 4.1, indicating the benefits of MLP layers in reducing approximation error for ICL of linear regression with a shared signal.

Experiments on LSA and LTB.We trained both the LSA and LTB layers for \(^{*}=(1,1,...,1)^{}\), and found that the trained LTB layer consistently achieved a significantly lower ICL risk than the LSA layer (see Figure 1). In these experiments, we adhered strictly to the previously defined LSA and LTB structures, setting the parameters as follows: \(d=5\), \(M=5\), \(=0\), and \(==_{d}\). At each training step, we sampled \(B=128\) new linear regression tasks.

In this part, we have shown that LSA models, the primary focus in previous ICL theory literature (see, e.g.,  and references therein), are not sufficiently expressive for ICL of linear regression with a shared signal. In what follows, we will show LTB models are sufficient for this ICL problem.

## 5 LTB Implements One-Step GD with Learnable Initialization

To understand the expressive power of the LTB models, we build a connection between \(_{}\) and its subset of models which we call _one-step GD with learnable initialization_ (GD-\(\)). We will first introduce GD-\(\) models and then show that the best LTB models that minimize the ICL risk effectively belong to GD-\(\) models.

The GD-\(\) models.A GD-\(\) model is defined as

\[f_{}:^{(d+1)(M+1)}, -^ {}(-),,\] (5.1)

where \(\) is the token matrix given by (3.1), \(^{d d}\) and \(^{d}\) are trainable parameters. Similarly, we define the function class formed by GD-\(\) models as \(_{}:=\{f_{}:\ ^{d},\ ^{d d}\}\).

  Model & GPT2 & GPT2-noMLP \\  \(^{*}=0\) & 0.003 & 0.024 \\  \(^{*}=10\) & 0.013 & 8.871 \\  

Table 1: Losses of GPT2 with or without MLP component for linear regression with a shared signal.

Figure 1: The test loss along the training process for LTB and LSA layer.

A GD-\(\) model computes a parameter that fits the context examples \((,)\) and uses that parameter to make a linear prediction of label \(y\) on feature \(\). More specifically, the first step is by using one gradient descent step with a _matrix stepsize_\(\) and an _initialization_\(\) on the least square objective formed by context examples \((,)\).

**LTB implements GD-\(\).** A GD-\(\) model (5.1) is a special case of an LTB model (3.3) by setting

\[_{2}^{}_{1}=*&*\\ ^{}&1,_{P}^{}_{V}= -_{d}&_{d}\\ _{d}^{}&1,_{K}^{}_{Q}= &*\\ _{d}^{}&*,\]

where \(*\) denotes the entries that do not affect the model output (hence can be set to anything). Note that \(\) and \(\) are _free_ provided that \(d_{v} d+1\), \(d_{k} d\) and \(d_{f} 1\) (as required in \(_{}\)). In sum, we have proved the following lemma showing that the set of GD-\(\) models belongs to the set of LTB models.

**Lemma 5.1**.: _We have \(_{}_{}\). Therefore, \(_{f_{}}(f)_{f _{}}(f)\)._

**Optimal GD-\(\) models for ICL.** We now consider \(_{}\) and its optimal ICL risk. We have the following theorem that computes the globally minimal ICL risk over \(_{}\) and specifies the sufficient and necessary conditions for a global minimizer. The proof is deferred to Appendix C.

**Theorem 5.2** (Optimal GD-\(\) models).: _Consider the ICL risk defined by (3.5). Suppose that Assumption 3.1 holds. Then we have_

\(\) _The minimal ICL risk of_ \(_{}\) _is_

\[_{f_{}}(f)=^{2}+ (^{}^{}(-^{}^{} ^{-1})),\] (5.2)

_where_ \(:=[(M+1)^{}^{}+ (()+^{2})_{d} ]/M\)_._

\(\) _The global optimal parameters for_ \(f_{}\) _that attain the minimum (_5.2_) take the following form:_

\[=^{*}+(),\ \ =^{*}+(^{ 2 }),^{*}:=^{ }^{-1}^{-}\] (5.3)

_and_ \(^{*}\) _is the mean of the task parameter in Assumption_ 3.1 _Here,_ \(():=\{^{d}:=\}\) _is the null space of_ \(,\) _and_ \((^{ 2})=\{^{d  d}:=\}\) _is the null space of_ \(^{ 2}\)_. In particular, when_ \(\) _is positive definite, the global optimal parameter is unique,_ \((,)=(^{*},^{*})\)_._

\(\) _Under Assumption_ 3.1_, the global optimal_ \(f_{}\) _that attains the minimum (_5.2_) is unique as a function of_ \(\) _(given by (_3.1_)) and takes the following form:_

\[f^{*}()=^{*}-^{*}}^{}(^{*}-), .\] (5.4)

Theorem 5.2 characterizes the optimal GD-\(\) models for ICL. In the above theorem, \(^{*}^{-1}\) as the context length \(M\) goes to infinity (assuming that \(\) is positive definite). In this case, the optimal GD-\(\) function (5.2) implements one Newton step from initialization \(^{*}\). With a finite context length \(M\), (5.2) implements one regularized Newton step from initialization \(^{*}\).

**Optimal LTB models for ICL.** Lemma 5.1 shows that the best ICL risk achieved by an GD-\(\) model is no smaller than the best ICL risk achieved by an LTB model. Surprisingly, our next theorem shows that the best ICL risk achieved by a GD-\(\) is _equal_ to that achieved by an LTB model. Therefore, the hypothesis set \(_{}\) is diverse enough to match the approximation ability of the larger hypothesis set \(_{}\) for ICL.

**Theorem 5.3** (Optimal LTB models).: _Consider the ICL risk defined by (3.5). Suppose that Assumption 3.1 holds and that \((^{}^{}) 2\). Then we have_

\(\) _The minimal ICL risk of_ \(_{}\) _and of_ \(_{}\) _are equal,_

\[_{f_{}}(f)=_{f_{ }}(f)=}\]* _Rewrite an LTB model as_ \(f_{}\) _in (_3.3_) with parameters_ (__\(*\) _denotes parameters that do not affect the output)__ \[_{2}^{}_{1}=*&*\\ ^{}&*,_{K}^{} _{Q}=_{11}&*\\ _{12}^{}&*,_{2}^{}_{1} _{P}^{}_{V}=*&*\\ _{21}^{}&v_{-1}.\] _Then the sufficient and necessary conditions for_ \(f_{}_{f_{}}(f)\) _are_ \[v_{-1} 0, v_{-1}_{12}( ),_{21}-v_{-1}^{*}+ (),^{*}+(),\] \[v_{-1}_{11}^{*}-v_{-1} ^{*}_{12}^{}+(^{  2}),\] _where_ \(^{*}\) _is defined in Assumption_ 3.1 _and_ \(^{*}\) _is defined in (_5.3_). In particular, when_ \(\) _is positive definite, the globally optimal parameter represented by_ \((_{11},_{12},_{21},v_{-1},)\) _is unique up to a rescaling of_ \(v_{-1}\)_._
* _Under Assumption_ 3.1_, the globally optimal LTB model (that is, a function in_ \(_{f_{}}\)_) is unique as a function of_ \(\) _(given by (_3.1_)) and takes the form of (_5.4_) almost surely._

Lemma 5.1 and Theorem 5.3 together show that \(_{}\) is a representative subset of \(_{}\) that does not incur additional approximation error. In addition, every optimal LTB model is effectively an optimal \(\)-\(\) model when restricted to all possible token matrices. Note that the optimal model parameters for LTB or \(\)-\(\) are not unique because of redundant parameterization. But the optimal LTB and \(\)-\(\) models are unique as a function of all possible token matrices. The above holds even when \(\) and \(\) are potentially rank deficient.

**Comparison with prior works.** A line of papers considers LSA models for ICL under Assumption 3.1 with \(^{*}=0\)[35; 1; 38; 37]. They show that LSA models can (effectively) implement all possible \(\)-\(0\) models that specialize \(\)-\(\) models by fixing \(=\). In addition, they show that every optimal LSA model is (effectively) a \(\)-\(0\) model for ICL under Assumption 3.1 with \(^{*}=\) (see, for example, Theorem 1 in ). In comparison, we consider a harder ICL problem that allows a large shared signal in tasks (that is, a large \(^{*}\) in Assumption 3.1). In this setting, our Theorem 4.1 shows that \(_{}\) (hence its subset formed by \(\)-\(0\) models), as a subset of \(_{}\), incurs an additional approximation error proportional to \(\|^{*}\|_{}^{2}\) compared with \(_{}\). In contrast, \(_{}\), as a subset of \(_{}\), does not incur additional approximation error according to our Theorem 5.3. Thus the LSA and \(\)-\(0\) models considered by [35; 1; 38; 37] are not capable of learning the shared signal \(^{*}\), while an LTB model can learn \(^{*}\) through implementing \(\)-\(\) and encoding \(^{*}\) in the initialization parameter.

## 6 Training and In-Context Learning of GD-beta

We have shown that \(_{}\) is a representative subset of \(_{}\) that effectively contains every optimal LTB model. We now examine the ICL and training of \(\)-\(\) models.

Nearly optimal ICL with \(\)-\(\).We will compare the best ICL risk achieved by \(\)-\(\) with the best ICL risk achieved by any estimator. The following lemma is an extension of Proposition 5.1 and Corollary 5.2 in  (which is based on ) that characterizes the Bayes optimal ICL risk among all estimators.

**Lemma 6.1** (Bayes optimal ICL).: _Given a task-specific dataset \((,,,y)\) sampled according to Assumption 3.1, let \(g(,,)\) be an arbitrary estimator for \(y\) and measure the average linear regression risk by \((g;):=[(g(,,)-y)^{2}]\). It is clear that \((g;)=(g)\). Then,_

* _The optimal estimator that minimizes the average linear regression risk_ \((;)\) _is_ \(g^{*}(,,)=^{}^{*} +^{}^{}^{ }^{}^{}+^{2} _{d}^{-1}^{}^{} (-^{*}).\)__
* _Assume the signal-to-noise ratio is upper bounded, that is,_ \(()^{2},\) _then with probability at least_ \(1-(-(M))\) _over the randomness of_ \(,\) _it holds that_ \((g^{*};)-^{2}_{i=1}^{d}\{,_{i}\},\) _where_ \(}{M},\) _and_ \((_{i})_{i 1}\) _are the eigenvalues of_ \(^{}^{}.\)__

The proof is deferred to Appendix F. Lemma 6.1 shows that the Bayes optimal estimator is a ridge regression estimator centered at \(^{*}\). This is consistent with  where the Bayes optimal estimator is a ridge regression estimator as they assumed \(^{*}=0\). The following corollary of Theorem 5.2 computes the rate of the ICL risk achieved by the optimal \(\) model.

**Corollary 6.2**.: _Under the setup of Theorem 5.2, additional assume the signal-to-noise ratio is upper bounded, that is, \(()^{2}\). Then we have \(_{f_{}}(f)-^{2 }_{t=1}^{d}\{,_{i}\},\) where \((_{i})_{i 0}\) are the eigenvalues of \(^{}^{}\) and \(:=[(^{}^{ })+^{2}]/M^{2}/M\)._

The optimal (expected) ICL risk achieved by \(_{}\) in Corollary 6.2 matches the (high probability) Bayes optimal ICL risk in Lemma 6.1 ignoring constant factors, provided that the signal-to-noise ratio is upper bounded. Therefore \(_{}\) achieves nearly Bayes optimal ICL risk. As a consequence, the larger hypothesis set \(_{}\) also achieves nearly Bayes optimal ICL of linear regression under Assumption 3.1.

For the simplicity of discussion, we assume a fixed context length during pretraining and inference. Our discussions can be extended to allow a different context length during pretraining and inference using techniques in . However, this is not the main focus of this work.

Optimization of \(\) with infinite tasks.We have shown that \(_{}\) is a representative subset of \(_{}\) that covers the optimal LTB models and achieves nearly optimal ICL risk. We now consider the optimization in the parameter space specified by \(_{}\). For simplicity, we follow  and consider gradient descent with an infinitesimal stepsize on the ICL objective with an infinite number of tasks. That is, we consider the optimization of gradient flow on the population ICL risk under the parameterization of \(\),

\[(t)}{t}=-}(f_{}), (t)}{t}=-}( f_{}),\] (6.1)

where \(\) is defined by (3.5) and \(f_{}\) is defined by (5.1).

The following theorem guarantees the global convergence of gradient flow. We introduce some notation to accommodate cases when \(\) is rank deficient. Let \(_{}\) be the orthogonal projection operator onto a subspace \(\). Let \(=()\) be the image space of matrix \(\) (viewing \(\) as a linear map) and \(^{}:=()\) be its orthogonal complement. Similarly, let \(:=(^{ 2})=\{ ,^{d d}\}\) be the image space of the operator \(^{ 2}\) and \(^{}\) be its orthogonal complement. Then we have the following theorem.

**Theorem 6.3**.: _Consider the gradient flow defined by (6.1) with initialization \((0),(0)\). We have,_

\[_{}((t)) _{}(^{*}), _{^{}}((t))=_{ ^{}}((0)),\] \[_{}((t)) _{}(^{*}), _{^{}}((t))=_{ ^{}}((0))\]

_as \(t\). In particular, if \(\) is positive definite, the gradient flow converges to the unique global minimizer of ICL risk over \(\) class, that is, \((t)^{*}\) and \((t)^{*}\) as \(t\)._

The proof, as well as the convergence rate, is deferred to Appendix G. We remark that (6.1) is a complex dynamical system on a non-convex potential function of \(\) and \(\). We briefly discuss our proof techniques assuming that \(\) is full rank. The rank-deficient cases can be handled in the same way by applying appropriate project operators. To conquer the non-convex optimization issue, we observe that for every fixed \(\), the potential as a function of \(\) is smooth and strongly convex with a uniformly bounded condition number. This observation allows us to establish a uniform convergence for \(\). When \(\) is sufficiently close to \(^{*}\), the potential as a function of \(\) is approximately convex, allowing us to track the convergence of \(\).

Theorem 6.3 shows that optimization of \(\) can be done efficiently by gradient flow without suffering from non-convexity. However, as we have shown in previous sections, LTB utilizes a more complex parameterization than \(\). So Theorem 6.3 does not imply optimization of LTB is easy. We leave it as future work to study the optimization and statistical complexity for directly learning LTB models.

## 7 Concluding Remarks

In this paper, we study the in-context learning of linear regression with a shared signal represented by a Gaussian prior with a non-zero mean. We show that although the linear self-attention layer discussed in prior works is consistent for this more complex task, its risk has an inevitable gap compared to that of the linear Transformer block (LTB), which is a linear self-attention layer followed by a linear multi-layer perception (MLP) layer. Next, we show that the effectiveness of the LTB arises because it can implement the one-step gradient descent estimator with learnable initialization (GD-\(\)). Moreover, all global minimizers in the LTB class are equivalent to the unique global minimizer in the GD-\(\) class, which can achieve nearly Bayes optimal in-context learning risk. Finally, we consider training on in-context examples and prove global convergence over the GD-\(\) class of gradient flow on the population loss. Several future directions are worth discussing.

**Optimization and statistical complexity.** This paper provides an approximation theory of LTB and an optimization theory of GD-\(\). However, the statistical complexity of learning LTB or GD-\(\) is not considered. The work by  provided techniques for analyzing the statistical task complexity for pre-training GD-\(0\). An interesting direction is to extend their method to study the statistical complexity of learning GD-\(\). However, their method crucially relies on the convexity of the risk induced GD-\(0\), while we have shown that the risk induced by GD-\(\) is non-convex. New ideas for dealing with non-convexity are needed here.

**From LTB to Transformer block.** We focus on LTB in this work, which simplifies a vanilla Transformer block by removing the non-linearities from the softmax self-attention and the ReLU activation in the MLP layers. This simplification allows us to obtain precise theoretical results for LTB (such as its connection to GD-\(\)). On the other hand, non-linearities are arguably necessary for Transformers to work well in practice. An important next step is to further consider the theoretical benefits of non-linearities based on our current results.

**Roles of MLP layers.** The work by  (and references thereafter) empirically found that MLP layers operate as key-value memories that store human-interpretable patterns in some pre-trained Transformers. Their work motivated a method for locating and editing information stored in language models by modifying their MLP layers (see, e.g., ). Our work proves that the MLP component enables LTB to learn the shared signal in linear regression tasks, which cannot be done by a single LSA component. We leave it as future work to theoretically clarify the information stored in the MLP component.