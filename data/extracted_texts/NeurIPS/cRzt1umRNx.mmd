# Riemannian Residual Neural Networks

Isay Katsman\({}^{*}\)

Yale University

isay.katsman@yale.edu

&Eric M. Chen\({}^{*}\), Sidhanth Holalkere\({}^{*}\)

Cornell University

{emc348, sh844}@cornell.edu

&Anna Asch

Cornell University

aca89@cornell.edu

&Aaron Lou

Stanford University

aaronlou@stanford.edu

&Ser-Nam Lim\({}^{}\)

University of Central Florida

sernam@ucf.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

&Aaron Lou

Stanford University

aaronlou@stanford.edu

&Ser-Nam Lim\({}^{}\)

University of Central Florida

sernam@ucf.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

&Aaron Lou

Stanford University

aaronlou@stanford.edu

&Ser-Nam Lim\({}^{}\)

University of Central Florida

sernam@ucf.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

&Aaron Lou

Stanford University

aaronlou@stanford.edu

&Ser-Nam Lim\({}^{}\)

University of Central Florida

sernam@ucf.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

&Aaron Lou

Stanford University

aaronlou@stanford.edu

&Ser-Nam Lim\({}^{}\)

University of Central Florida

sernam@ucf.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

&Aaron Lou

Stanford University

aaronlou@stanford.edu

&Ser-Nam Lim\({}^{}\)

University of Central Florida

sernam@ucf.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

&Aaron Lou

Stanford University

aaronlou@stanford.edu

&Ser-Nam Lim\({}^{}\)

University of Central Florida

sernam@ucf.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

&Aaron Lou

Stanford University

aaronlou@stanford.edu

&Ser-Nam Lim\({}^{}\)

University of Central Florida

sernam@ucf.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

&Aaron Lou

Stanford University

aaronlou@stanford.edu

&Ser-Nam Lim\({}^{}\)

University of Central Florida

sernam@ucf.edu

&Christopher De Sa
vectors to the input points, thereby naturally generalizing a typical Euclidean residual addition. This process is illustrated in Figure 1. Note that this strategy is exceptionally natural, only making use of inherent geodesic geometry, and works generally for all smooth manifolds. We refer to such networks as Riemannian residual neural networks.

Though the above approach is principled, it is underspecified, as constructing an efficient learnable vector field for a given manifold is often nontrivial. To resolve this issue, we present a general way to induce a learnable vector field for a manifold \(\) given only a map \(f:^{k}\). Ideally, this map should capture intrinsic manifold geometry. For example, in the context of Euclidean space, this map could consist of a series of \(k\) projections onto hyperplanes. There is a natural equivalent of this in hyperbolic space that instead projects to horospheres (horospheres correspond to hyperplanes in Euclidean space). More generally, we propose a feature map that once more relies only on geodesic information, consisting of projection to random (or learned) geodesic balls. This final approach provides a fully geometric way to construct vector fields, and therefore natural residual networks, for any Riemannian manifold.

After introducing our general theory, we give concrete manifestations of vector fields, and therefore residual neural networks, for hyperbolic space and the manifold of SPD matrices. We compare the performance of our Riemannian residual neural networks to that of existing manifold-specific networks on hyperbolic space and on the manifold of SPD matrices, showing that our networks perform much better in terms of relevant metrics due to their improved adherence to manifold geometry.

Our contributions are as follows:

1. We introduce a novel and principled generalization of residual neural networks to general Riemannian manifolds. Our construction relies only on knowledge of geodesics, which capture manifold geometry.
2. Theoretically, we show that our methodology better captures manifold geometry than pre-existing manifold-specific neural network constructions. Empirically, we apply our general construction to hyperbolic space and to the manifold of SPD matrices. On various hyperbolic graph datasets (where hyperbolicity is measured by Gromov \(\)-hyperbolicity) our method considerably outperforms existing work on both link prediction and node classification tasks. On various SPD covariance matrix classification datasets, a similar conclusion holds.
3. Our method provides a way to directly vary the geometry of a given neural network without having to construct particular operations on a per-manifold basis. This provides the novel capability to directly compare the effect of geometric representation (in particular, evaluating the difference between a given Riemannian manifold \((,g)\) and Euclidean space \((^{n},||||_{2})\)) while fixing the network architecture.

## 2 Related Work

Our work is related to but distinctly different from existing neural ordinary differential equation (ODE)  literature as well a series of papers that have attempted generalizations of neural networks to specific manifolds such as hyperbolic space  and the manifold of SPD matrices .

### Residual Networks and Neural ODEs

Residual networks (ResNets) were originally developed to enable training of larger networks, previously prone to vanishing and exploding gradients . Later on, many discovered that by adding a learned residual, ResNets are similar to Euler's method [9; 21; 37; 45; 53]. More specifically, the ResNet represented by \(_{t+1}=_{t}+f(,_{t})\) for \(_{t}^{D}\) mimics the dynamics of the ODE defined by \((t)}{dt}=f((t),t,)\). Neural ODEs are defined precisely as ODEs of this form, where

Figure 1: An illustration of a manifold-generalized residual addition. The traditional Euclidean formula \(p p+v\) is generalized to \(p_{p}(v)\), where \(\) is the Riemannian exponential map. \(\) is the manifold and \(T_{p}\) is the tangent space at \(p\).

the local dynamics are given by a parameterized neural network. Similar to our work, Falorsi and Forre , Katsman et al. , Lou et al. , Mathieu and Nickel  generalize neural ODEs to Riemannian manifolds (further generalizing manifold-specific work such as Bose et al. , that does this for hyperbolic space). However, instead of using a manifold's vector fields to solve a neural ODE, we learn an objective by parameterizing the vector fields directly (Figure 2). Neural ODEs and their generalizations to manifolds parameterize a continuous collection of vector fields over time for a single manifold in a dynamic flow-like construction. Our method instead parameterizes a discrete collection of vector fields, entirely untethered from any notion of solving an ODE. This makes our construction a strict generalization of both neural ODEs and their manifold equivalents [15; 29; 36; 38].

### Riemannian Neural Networks

Past literature has attempted generalizations of Euclidean neural networks to a number of manifolds.

**Hyperbolic Space** Ganea et al.  extended basic neural network operations (e.g. activation function, linear layer, recurrent architectures) to conform with the geometry of hyperbolic space through gyrovector constructions . In particular, they use gyrovector constructions  to build analogues of activation functions, linear layers, and recurrent architectures. Building on this approach, Chami et al.  adapt these constructions to hyperbolic versions of the feature transformation and neighborhood aggregation steps found in message passing neural networks. Additionally, batch normalization for hyperbolic space was introduced in Lou et al. ; hyperbolic attention network equivalents were introduced in Gulcehre et al. . Although gyrovector constructions are algebraic and allow for generalization of neural network operations to hyperbolic space and beyond, we note that they do not capture intrinsic geodesic geometry. In particular, we note that the gyrovector-based hyperbolic linear layer introduced in Ganea et al.  reduces to a Euclidean matrix multiplication followed by a learned hyperbolic bias addition (see Appendix D.2). Hence all non-Euclidean learning for this case happens through the bias term. In an attempt to resolve this, further work has focused on imbuing these neural networks with more hyperbolic functions [10; 49]. Chen et al.  notably constructs a hyperbolic residual layer by projecting an output onto the Lorentzian manifold. However, we emphasize that our construction is more general while being more geometrically principled as we work with fundamental manifold operations like the exponential map rather than relying on the niceties of Lorentz space.

Yu and De Sa  make use of randomized hyperbolic Laplacian features to learn in hyperbolic space. We note that the features learned are shallow and are constructed from a specific manifestation of the Laplace-Beltrami operator for hyperbolic space. In contrast, our method is general and enables non-shallow (i.e., multi-layer) feature learning.

**SPD Manifold** Neural network constructs have been extended to the manifold of symmetric positive definite (SPD) matrices as well. In particular, SPDNet  is an example of a widely adopted SPD manifold neural network which introduced SPD-specific layers analogous to Euclidean linear and ReLU layers. Building upon SPDNet, Brooks et al.  developed a batch normalization method to be used with SPD data. Additionally, Lopez et al.  adapted gyrocalculus constructions used in hyperbolic space to the SPD manifold.

**Symmetric Spaces** Further work attempts generalization to symmetric spaces. Sonoda et al.  design fully-connected networks over noncompact symmetric spaces using particular theory from Helgason-Fourier analysis , and Chakraborty et al.  attempt to generalize several operations such as convolution to such spaces by adapting and developing a weighted Frechet mean construction. We note that the Helgason-Fourier construction in Sonoda et al.  exploits a fairly particular structure, while the weighted Frechet mean construction in Chakraborty et al.  is specifically introduced for convolution, which is not the focus of our work (we focus on residual connections).

Unlike any of the manifold-specific work described above, our residual network construction can be applied generally to any smooth manifold and is constructed solely from geodesic information.

## 3 Background

In this section, we cover the necessary background for our paper; in particular, we introduce the reader to the necessary constructs from Riemannian geometry. For a detailed introduction to Riemannian geometry, we refer the interested reader to textbooks such as Lee .

### Riemannian Geometry

A topological manifold \((,g)\) of dimension \(n\) is a locally Euclidean space, meaning there exist homeomorphic1 functions (called "charts") whose domains both cover the manifold and map from the manifold into \(^{n}\) (i.e. the manifold "looks like" \(^{n}\) locally). A smooth manifold is a topological manifold for which the charts are not simply homeomorphic, but diffeomorphic, meaning they are smooth bijections mapping into \(^{n}\) and have smooth inverses. We denote \(T_{p}\) as the tangent space at a point \(p\) of the manifold \(\). Further still, a Riemannian manifold2\((,g)\) is an \(n\)-dimensional smooth manifold with a smooth collection of inner products \((g_{p})_{p}\) for every tangent space \(T_{p}\). The Riemannian metric \(g\) induces a distance \(d_{g}:\) on the manifold.

### Geodesics and the Riemannian Exponential Map

**Geodesics** A geodesic is a curve of minimal length between two points \(p,q\), and can be seen as the generalization of a straight line in Euclidean space. Although a choice of Riemannian metric \(g\) on \(\) appears to only define geometry locally on \(\), it induces global distances by integrating the length (of the "speed" vector in the tangent space) of a shortest path between two points:

\[d(p,q)=_{}_{0}^{1}(^{}(t), ^{}(t))}\,dt \]

where \( C^{}(,)\) is such that \((0)=p\) and \((1)=q\).

For \(p\) and \(v T_{p}\), there exists a unique geodesic \(_{v}\) where \((0)=p\), \(^{}(0)=v\) and the domain of \(\) is as large as possible. We call \(_{v}\) the maximal geodesic .

**Exponential Map** The Riemannian exponential map is a way to map \(T_{p}\) to a neighborhood around \(p\) using geodesics. The relationship between the tangent space and the exponential map output can be thought of as a local linearization, meaning that we can perform typical Euclidean operations in the tangent space before projecting to the manifold via the exponential map to capture the local on-manifold behavior corresponding to the tangent space operations. For \(p\) and \(v T_{p}\), the exponential map at \(p\) is defined as \(_{p}(v)=_{v}(1)\).

One can think of \(\) as a manifold generalization of Euclidean addition, since in the Euclidean case we have \(_{p}(v)=p+v\).

Figure 2: A visualization of a Riemannian residual neural network on a manifold \(\). Our model parameterizes vector fields on a manifold. At each layer in our network, we take a step from a point in the direction of that vector field (brown), which is analogous to the residual step in a ResNet.

### Vector Fields

Let \(T_{p}\) be the tangent space to a manifold \(\) at a point \(p\). Like in Euclidean space, a vector field assigns to each point \(p\) a tangent vector \(X_{p} T_{p}\). A smooth vector field assigns a tangent vector \(X_{p} T_{p}\) to each point \(p\) such that \(X_{p}\) varies smoothly in \(p\).

**Tangent Bundle** The tangent bundle of a smooth manifold \(\) is the disjoint union of the tangent spaces \(T_{p}\), for all \(p\), denoted by \(T:=_{p}T_{p}=_{p }\{(p,v) v T_{p}\}\).

**Pushforward** A derivative (also called a _pushforward_) of a map \(f:\) between two manifolds is denoted by \(D_{p}f:T_{p} T_{f(p)}\). This is a generalization of the classical Euclidean Jacobian (since \(^{n}\) is a manifold), and provides a way to relate tangent spaces at different points on different manifolds.

**Pullback** Given \(:\) a smooth map between manifolds and \(f:\) a smooth function, the pullback of \(f\) by \(\) is the smooth function \(^{*}f\) on \(\) defined by \((^{*}f)(x)=f((x))\). When the map \(\) is implicit, we simply write \(f^{*}\) to mean the pullback of \(f\) by \(\).

### Model Spaces in Riemannian Geometry

The three Riemannian model spaces are Euclidean space \(^{n}\), hyperbolic space \(^{n}\), and spherical space \(^{n}\), that encompass all manifolds with constant sectional curvature. Hyperbolic space manifests in several representations like the Poincare ball, Lorentz space, and the Klein model. We use the Poincare ball model for our Riemannian ResNet design (see Appendix A for more details on the Poincare ball model).

### SPD Manifold

Let \(SPD(n)\) be the manifold of \(n n\) symmetric positive definite (SPD) matrices. We recall from Gallier and Quaintance  that \(SPD(n)\) has a Riemannian exponential map (at the identity) equivalent to the matrix exponential. Two common metrics used for \(SPD(n)\) are the log-Euclidean metric , which induces a flat structure on the matrices, and the canonical affine-invariant metric [12; 42], which induces non-constant negative sectional curvature. The latter gives \(SPD(n)\) a considerably less trivial geometry than that exhibited by the Riemannian model spaces  (see Appendix A for more details on \(SPD(n)\)).

## 4 Methodology

In this section, we provide the technical details behind Riemannian residual neural networks.

### General Construction

We define a **Riemannian Residual Neural Network** (RResNet) on a manifold \(\) to be a function \(f:\) defined by

Figure 3: An overview of our generalized Riemannian Residual Neural Network (RResNet) methodology. We start by mapping \(x^{(0)}^{(0)}\) to \(^{(1)}^{(1)}\) using a base point mapping \(h_{1}\). Then, using our paramterized vector field \(_{i}\), we compute a residual \(v^{(1)}:=_{1}(^{(1)})\). Finally, we project \(v^{(1)}\) back onto the manifold using the Riemannian \(\) map, leaving us with \(x^{(1)}\). This procedure can be iterated to produce a multi-layer Riemannian residual neural network that is capable of changing manifold representation on a per layer basis.

\[f(x) :=x^{(m)} \] \[x^{(0)} :=x\] (3) \[x^{(i)} :=_{x^{(i-1)}}(_{i}(x^{(i-1)})) \]

for \(x\), where \(m\) is the number of layers and \(_{i}: T\) is a neural network-parameterized vector field over \(\). This residual network construction is visualized for the purpose of intuition in Figure 2. In practice, parameterizing a function from an abstract manifold \(\) to its tangent bundle is difficult. However, by the Whitney embedding theorem , we can embed \(^{D}\) smoothly for some dimension \(D\). As such, for a standard neural network \(n_{i}:^{D}^{D}\) we can construct \(_{i}\) by

\[_{i}(x):=_{T_{x}}(n_{i}(x)) \]

where we note that \(T_{x}^{D}\) is a linear subspace (making the projection operator well defined). Throughout the paper we call this the embedded vector field design3. We note that this is the same construction used for defining the vector field flow in Lou et al. , Mathieu and Nickel , Rozen et al. .

We also extend our construction to work in settings where the underlying manifold changes from layer to layer. In particular, for a sequence of manifolds \(^{(0)},^{(1)},,^{(m)}\) with (possibly learned) maps \(h_{i}:^{(i-1)}^{(i)}\), our Riemannian ResNet \(f:^{(0)}^{(m)}\) is given by

\[f(x) :=x^{(m)} \] \[x^{(0)} :=x\] (7) \[x^{(i)} :=_{h_{i}(x^{(i-1)})}(_{i}(h_{i}(x^{(i-1)}))) i [m] \]

with functions \(_{i}:^{(i)} T^{(i)}\) given as above. This generalization is visualized in Figure 3. In practice, our \(^{(i)}\) will be different dimensional versions of the same geometric space (e.g. \(^{n}\) or \(^{n}\) for varying \(n\)). If the starting and ending manifolds are the same, the maps \(h_{i}\) will simply be standard inclusions. When the starting and ending manifolds are different, the \(h_{i}\) may be standard neural networks for which we project the output, or the \(h_{i}\) may be specially design learnable maps that respect manifold geometry. As a concrete example, our \(h_{i}\) for the SPD case map from an SPD matrix of one dimension to another by conjugating with a Stiefel matrix . Furthermore, as shown in Appendix D, our model is equivalent to the standard ResNet when the underlying manifold is \(^{n}\).

**Comparison with Other Constructions** We discuss how our construction compares with other methods in Appendix E, but here we briefly note that unlike other methods, our presented approach is fully general and better conforms with manifold geometry.

### Feature Map-Induced Vector Field Design

Most of the difficulty in application of our general vector field construction comes from the design of the learnable vector fields \(_{i}:^{(i)} T^{(i)}\). Although we give an embedded vector field design above, it is not very principled geometrically. We would like to considerably restrict these vector fields so that their range is informed by the underlying geometry of \(\). For this, we note that it is possible to induce a vector field \(: T\) for a manifold \(\) with any smooth map \(f:^{k}\). In practice, this map should capture intrinsic geometric properties of \(\) and can be viewed as a feature map, or de facto linearization of \(\). Given an \(x\), we need only pass \(x\) through \(f\) to get its feature representation in \(^{k}\), then note that since:

\[D_{p}f:T_{p} T_{f(p)}^{k},\]

we have an induced map:

\[(D_{p}f)^{*}:(T_{f(p)}^{k})^{*}(T_{p})^{*},\]

where \((D_{p}f)^{*}\) is the pullback of \(D_{p}f\). Note that \(T_{p}^{k}^{k}\) and \((^{k})^{*}^{k}\) by the dual space isomorphism. Moreover \((T_{p})^{*} T_{p}\) by the tangent-cotangent space isomorphism . Hence, we have the induced map:

\[(D_{p}f)^{*}_{r}:^{k} T_{p},\]obtained from \((D_{p}f)^{*}\), simply by both precomposing and postcomposing the aforementioned isomorphisms, where relevant. \((D_{p}f)^{*}_{r}\) provides a natural way to map from the feature representation to the tangent bundle. Thus, we may view the map \(_{f}: T\) given by:

\[_{f}(x)=(D_{x}f)^{*}_{r}(f(x))\]

as a deterministic vector field induced entirely by \(f\).

**Learnable Feature Map-Induced Vector Fields** We can easily make the above vector field construction learnable by introducing a Euclidean neural network \(n_{}:^{k}^{k}\) after \(f\) to obtain \(_{f,}(x)=(D_{x}f)^{*}(n_{}(f(x)))\).

**Feature Map Design** One possible way to simplify the design of the above vector field is to further break down the map \(f:^{k}\) into \(k\) maps \(f_{1},,f_{k}:\), where ideally, each map \(f_{i}\) is constructed in a similar way (e.g. performing some kind of geometric projection, where the \(f_{i}\) vary only in terms of the specifying parameters). As we shall see in the following subsection, this ends up being a very natural design decision.

In what follows, we shall consider only smooth feature maps \(f:^{k}\) induced by a single parametric construction \(g_{}:\), i.e. the \(k\) dimensions of the output of \(f\) are given by different choices of \(\) for the same underlying feature map4. This approach also has the benefit of a very simple interpretation of the induced vector field. Given feature maps \(g_{_{1}},,g_{_{k}}:\) that comprise our overall feature map \(f:^{k}\), our vector field is simply a linear combination of the maps \( g_{_{i}}: T\). If the \(g_{_{i}}\) are differentiable with respect to \(_{i}\), we can even learn the \(_{i}\) themselves.

#### 4.2.1 Manifold Manifestations

In this section, in an effort to showcase how simple it is to apply our above theory to come up with natural vector field designs, we present several constructions of manifold feature maps \(g_{}:\) that capture the underlying geometry of \(\) for various choices of \(\). Namely, in this section we provide several examples of \(f:\) that induce \(_{f}: T\), thereby giving rise to a Riemannian neural network by Section 4.1.

**Euclidean Space** To build intuition, we begin with an instructive case. We consider designing a feature map for the Euclidean space \(^{n}\). A natural design would follow simply by considering hyperplane projection. Let a hyperplane \(w^{T}x+b=0\) be specified by \(w^{n},b\). Then a natural feature map \(g_{w,b}:^{n}\) parameterized by the hyperplane parameters is given by hyperplane projection : \(g_{w,b}(x)=x+b|}{||w||_{2}}\).

**Hyperbolic Space** We wish to construct a natural feature map for hyperbolic space. Seeking to follow the construction given in the Euclidean context, we wish to find a hyperbolic analog of hyperplanes. This is provided to us via the notion of horospheres . Illustrated in Figure 4, horospheres naturally generalize hyperplanes to hyperbolic space. We specify a horosphere in the Poincare ball model of hyperbolic space \(^{n}\) by a point of tangency \(^{n-1}\) and a real value \(b\). Then a natural feature map \(g_{,b}:^{n}\) parameterized by the horosphere parameters would be given by horosphere projection : \(g_{,b}(x)=-(^{2}}{||x-||_{2}^{2}})+b\).

**Symmetric Positive Definite Matrices** The manifold of SPD matrices is an example of a manifold where there is no innate representation of a hyperplane. Instead, given \(X SPD(n)\), a reasonable feature map \(g_{k}:SPD(n)\), parameterized by \(k\), is to map \(X\) to its \(k\)th largest eigenvalue: \(g_{k}(X)=_{k}\).

Figure 4: Example of a horosphere in the Poincaré ball representation of hyperbolic space. In this particular two-dimensional case, the hyperbolic space \(_{2}\) is visualized via the Poincaré disk model, and the horosphere, shown in blue, is called a horocycle.

**General Manifolds** For general manifolds there is no perfect analog of a hyperplane, and hence there is no immediately natural feature map. Although this is the case, it is possible to come up with a reasonable alternative. We present such an alternative in Appendix B.4 together with pertinent experiments.

_Example: Euclidean Space_ One motivation for the vector field construction \(_{f}(x)=(D_{x}f)_{r}^{*}(f(x))\) is that in the Euclidean case, \(_{f}\) will reduce to a standard linear layer (because the maps \(f\) and \((D_{x}f)^{*}\) are linear), which, in combination with the Euclidean \(\) map, will produce a standard Euclidean residual neural network.

Explicitly, for the Euclidean case, note that our feature map \(f:^{n}^{k}\) will, for example, take the form \(f(x)=Wx,W^{k n}\) (here we have \(b=0\) and \(W\) has normalized row vectors). Then note that we have \(Df=W\) and \((Df)^{*}=W^{T}\). We see for the standard feature map-based construction, our vector field \(_{f}(x)=(D_{x}f)^{*}(f(x))\) takes the form \(_{f}(x)=W^{T}Wx\).

For the learnable case (which is standard for us, given that we learn Riemannian residual neural networks), when the manifold is Euclidean space, the general expression \(_{f,}(x)=(D_{x}f)^{*}(n_{}(f(x)))\) becomes \(_{f,}(x)=W^{T}n_{}(Wx)\). When the feature maps are trivial projections (onto axis-aligned hyperplanes), we have \(W=I\) and \(_{f,}(x)=n_{}(x)\). Thus our construction can be viewed as a generalization of a standard neural network.

## 5 Experiments

In this section, we perform a series of experiments to evaluate the effectiveness of RResNets on tasks arising on different manifolds. In particular, we explore hyperbolic space and the SPD manifold.

### Hyperbolic Space

We perform numerous experiments in the hyperbolic setting. The purpose is twofold:

1. We wish to illustrate that our construction in Section 4 is not only more general, but also intrinsically more geometrically natural than pre-existing hyperbolic constructions such as HNN , and is thus able to learn better over hyperbolic data.
2. We would like to highlight that non-Euclidean learning benefits the most hyperbolic datasets. We can do this directly since our method provides a way to vary the geometry of a fixed neural network architecture, thereby allowing us to directly investigate the effect of changing geometry from Euclidean to hyperbolic.

#### 5.1.1 Direct Comparison Against Hyperbolic Neural Networks 

To demonstrate the improvement of RResNet over HNN , we first perform node classification (NC) and link prediction (LP) tasks on graph datasets with low Gromov \(\)-hyperbolicity , which means the underlying structure of the data is highly hyperbolic. The RResNet model is given the

    &  &  &  &  &  \\  &  &  &  &  &  \\   & **Task** & **LP** & **NC** & **LP** & **NC** & **LP** & **NC** & **LP** & **NC** \\   **} & Euc & \(59.8_{ 2.0}\) & \(32.5_{ 1.1}\) & \(92.0_{ 0.0}\) & \(60.9_{ 3.4}\) & \(83.3_{ 0.1}\) & \(48.2_{ 0.7}\) & \(82.5_{ 0.3}\) & \(23.8_{ 0.7}\) \\  & Hyp  & \(63.5_{ 0.6}\) & \(45.5_{ 3.3}\) & \(94.5_{ 0.0}\) & \(70.2_{ 0.1}\) & \(87.5_{ 0.1}\) & \(68.5_{ 0.3}\) & \(87.6_{ 0.2}\) & \(22.0_{ 1.5}\) \\  & Euc-Mixed & \(49.6_{ 1.1}\) & \(35.2_{ 3.4}\) & \(91.5_{ 0.1}\) & \(68.3_{ 2.3}\) & \(86.0_{ 1.3}\) & \(63.0_{ 0.3}\) & \(84.4_{ 0.2}\) & \(46.1_{ 0.4}\) \\  & Hyp-Mixed & \(55.1_{ 1.3}\) & \(56.9_{ 1.5}\) & \(93.3_{ 0.0}\) & \(69.6_{ 0.1}\) & \(83.8_{ 0.3}\) & \(}\) & \(85.6_{ 0.5}\) & \(45.9_{ 0.3}\) \\   **} & MLP & \(72.6_{ 0.6}\) & \(28.8_{ 2.5}\) & \(89.8_{ 0.5}\) & \(68.6_{ 0.6}\) & \(84.1_{ 0.9}\) & \(72.4_{ 0.2}\) & \(83.1_{ 0.5}\) & \(51.5_{ 1.0}\) \\  & HNN  & \(75.1_{ 0.3}\) & \(41.0_{ 1.8}\) & \(90.8_{ 0.2}\) & \(80.5_{ 0.5}\) & \(}\) & \(69.8_{ 0.4}\) & \(}\) & \(}\) \\   & **RResNet Hero** & \(_{ 0.3}\) & \(_{ 2.0}\) & \(_{ 0.1}\) & \(_{ 0.3}\) & \(_{ 0.3}\) & \(72.3_{ 1.7}\) & \(86.7_{ 0.3}\) & \(52.4_{ 5.5}\) \\   

Table 1: Above we give graph task results for RResNet Horo compared with several non-graph-based neural network baselines (baseline methods and metrics are from Chami et al. ). Test ROC AUC is the metric reported for link prediction (LP) and test F1 score is the metric reported for node classification (NC). Mean and standard deviation are given over five trials. Note that RResNet Horo considerably outperforms HNN on the most hyperbolic datasets, performing worse and worse as hyperbolicity increases, to a more extreme extent than previous methods that do not adhere to geometry as closely (this is expected).

name "RResNet Horo." It utilizes a horosphere projection feature map-induced vector field described in Section 4. All model details are given in Appendix C.2. We find that because we adhere well to the geometry, we attain good performance on datasets with low Gromov \(\)-hyperbolicities (e.g. \(=0,=1\)). As soon as the Gromov hyperbolicity increases considerably beyond that (e.g. \(=3.5,=11\)), performance begins to degrade since we are embedding non-hyperbolic data in an unnatural manifold geometry. Since we adhere to the manifold geometry more strongly than prior hyperbolic work, we see performance decay faster as Gromov hyperbolicity increases, as expected. In particular, we test on the very hyperbolic Disease (\(=0\))  and Airport (\(=1\))  datasets. We also test on the considerably less hyperbolic PubMed (\(=3.5\))  and CoRA (\(=11\))  datasets. We use all of the non-graph-based baselines from Chami et al. , since we wish to see how much we can learn strictly from a proper treatment of the embeddings (and no graph information). Table 1 summarizes the performance of "RResNet Horo" relative to these baselines.

Moreover, we find considerable benefit from the feature map-induced vector field over an embedded vector field that simply uses a Euclidean network to map from a manifold point embedded in \(^{n}\). The horosphere projection captures geometry more accurately, and if we swap to an embedded vector field we see considerable accuracy drops on the two hardest hyperbolic tasks: Disease NC and Airport NC. In particular, for Disease NC the mean drops from \(76.8\) to \(75.0\), and for Airport NC we see a very large decrease from \(96.9\) to \(83.0\), indicating that geometry captured with a well-designed feature map is especially important. We conduct a more thorough vector field ablation study in Appendix C.5.

#### 5.1.2 Impact of Geometry

A major strength of our method is that it allows one to investigate the direct effect of geometry in obtaining results, since the architecture can remain the same for various manifolds and geometries (as specified by the metric of a given Riemannian manifold). This is well-illustrated in the most hyperbolic Disease NC setting, where swapping out hyperbolic for Euclidean geometry in an RResNet induced by an embedded vector field decreases the F1 score from a \(75.0\) mean to a \(67.3\) mean and induces a large amount of numerical stability, since standard deviation increases from \(5.0\) to \(21.0\). We conduct a more thorough geometry ablation study in Appendix C.5.

### SPD Manifold

A common application of SPD manifold-based models is learning over full-rank covariance matrices, which lie on the manifold of SPD matrices. We compare our RResNet to SPDNet  and SPDNet with batch norm  on four video classification datasets: AFEW , FPHA , NTU RGB+D , and HDM05 . Results are given in Table 2. Please see Appendix C.6 for details on the experimental setup. For our RResNet design, we try two different metrics: the log-Euclidean metric  and the affine-invariant metric , each of which captures the curvature of the SPD manifold differently. We find that adding a learned residual improves performance and training dynamics over existing neural networks on SPD manifolds with little effect on runtime. We experiment with several vector field designs, which we outline in Appendix B. The best vector field design (given in Section 4.2), also the one we use for all SPD experiments, necessitates eigenvalue computation. We note the cost of computing eigenvalues is not a detrimental feature of our approach since previous works (SPDNet , SPDNet with batchnorm ) already make use of eigenvalue computation5. Empirically, we observe that the beneficial effects of our RResNet construction are similar to those of the SPD batch norm introduced in Brooks et al.  (Table 2, Figure 5 in Appendix C.6). In addition, we find that our operations are stable with ill-conditioned input matrices, which commonly occur in the wild. To contrast, the batch norm computation in SPDNetBN, which relies on Karcher flow

    & AFEW & FPHA & NTU RGB+D & HDM05 \\  SPDNet & \(33.24_{ 0.56}\) & \(65.39_{ 1.48}\) & \(41.47_{ 0.34}\) & \(66.77_{ 0.92}\) \\ SPDNetBN & \(35.39_{ 0.93}\) & \(65.03_{ 1.35}\) & \(41.92_{ 0.37}\) & \(67.25_{ 0.44}\) \\
**RResNet Affine-Invariant** & \(35.17_{ 1.78}\) & \(}\) & \(41.00_{ 0.50}\) & \(67.91_{ 1.27}\) \\
**RResNet Log-Euclidean** & \(}\) & \(64.58_{ 0.38}\) & \(}\) & \(}\) \\   

Table 2: We run our SPD manifold RResNet on four SPD matrix datasets and compare against SPDNet  and SPDNet with batch norm . We report the mean and standard deviation of validation accuracies over five trials and bold which method performs the best.

[28; 35], suffers from numerical instability when the input matrices are nearly singular. Overall, we observe our RResNet with the affine-invariant metric outperforms existing work on FPHA, and our RResNet using the log-Euclidean metric outperforms existing work on AFEW, NTU RGB+D, and HDM05. Being able to directly interchange between two metrics while maintaining the same neural network design is an unique strength of our model.

## 6 Riemannian Residual Graph Neural Networks

Following the initial comparison to non-graph-based methods in Table 1, we introduce a simple graph-based method by modifying RResNet Horo above. We take the previous model and pre-multiply the feature map output by the underlying graph adjacency matrix \(A\) in a manner akin to what happens with graph neural networks . This is the simple modification that we introduce to the Riemannian ResNet to incorporate graph information; we call this method G-RResNet Horo. We compare directly against the graph-based methods in Chami et al.  as well as against Fully Hyperbolic Neural Networks  and give results in Table 3. We test primarily on node classification since we found that almost all LP tasks are too simple and solved by methods in Chami et al.  (i.e., test ROC is greater than \(95\%\)). We also tune the matrix power of \(A\) for a given dataset; full architectural details are given in Appendix C.2. Although this method is simple, we see further improvement and in fact attain a state-of-the-art result for the Airport  dataset. Once more, as expected, we see a considerable performance drop for the much less hyperbolic datasets, PubMed and CoRA.

## 7 Conclusion

We propose a general construction of residual neural networks on Riemannian manifolds. Our approach is a natural geodesically-oriented generalization that can be applied more broadly than previous manifold-specific work. Our introduced neural network construction is the first that decouples geometry (i.e. the representation space expected for input to layers) from the architecture design (i.e. actual "wiring" of the layers). Moreover, we introduce a geometrically principled feature map-induced vector field design for the RResNet. We demonstrate that our methodology better captures underlying geometry than existing manifold-specific neural network constructions. On a variety of tasks such as node classification, link prediction, and covariance matrix classification, our method outperforms previous work. Finally, our RResNet's principled construction allows us to directly assess the effect of geometry on a task, with neural network architecture held constant. We illustrate this by directly comparing the performance of two Riemannian metrics on the manifold of SPD matrices. We hope others will use our work to better learn over data with nontrivial geometries in relevant fields, such as lattice quantum field theory, robotics, and computational chemistry.

**Limitations** We rely fundamentally on knowledge of geodesics of the underlying manifold. As such, we assume that a closed form (or more generally, easily computable, differentiable form) is given for the Riemannian exponential map as well as for the tangent spaces.

    & **Dataset** & Disease & Airport & PubMed & CoRA \\  & **Hyperbolicity** & \(=0\) & \(=1\) & \(=3.5\) & \(=11\) \\    } & GCN  & \(69.7_{ 0.4}\) & \(81.4_{ 0.6}\) & \(78.1_{ 0.2}\) & \(81.3_{ 0.3}\) \\  & GAT  & \(70.4_{ 0.4}\) & \(81.5_{ 0.3}\) & \(79.0_{ 0.3}\) & \(_{ 0.7}\) \\  & SAGE  & \(69.1_{ 0.6}\) & \(82.1_{ 0.5}\) & \(77.4_{ 2.2}\) & \(77.9_{ 2.4}\) \\  & SGC  & \(69.5_{ 0.2}\) & \(80.6_{ 0.1}\) & \(78.9_{ 0.0}\) & \(81.0_{ 0.1}\) \\    } & HGCN  & \(74.5_{ 0.9}\) & \(90.6_{ 0.2}\) & \(_{ 0.3}\) & \(79.9_{ 0.2}\) \\  & Fully HNN  & \(_{ 1.0}\) & \(90.9_{ 1.4}\) & \(78.0_{ 1.0}\) & \(80.2_{ 1.3}\) \\  & **G-RResNet Horo** & \(_{ 1.0}\) & \(_{ 0.1}\) & \(75.5_{ 0.8}\) & \(64.4_{ 7.6}\) \\   

Table 3: Above we give node classification results for G-RResNet Horo compared with several graph-based neural network baselines (baseline methods and metrics are from Chami et al. ). Test F1 score is the metric reported. Mean and standard deviation are given over five trials. Note that G-RResNet Horo obtains a state-of-the-art result on Airport. As for the less hyperbolic datasets, G-RResNet Horo does worse on PubMed and does very poorly on CoRA, once more, as expected due to unsuitability of geometry. The GNN label stands for “Graph Neural Networks” and the GGNN label stands for “Geometric Graph Neural Networks.”