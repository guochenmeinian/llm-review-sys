# FFAM: Feature Factorization Activation Map for Explanation of 3D Detectors

Shuai Liu, Boyang Li, Zhiyu Fang, Mingyue Cui, Kai Huang

School of Computer Science and Engineering, Sun Yat-sen University

{liush376@mail2, liby83@mail, fangzhy9@mail2, cuimy@mail2, huangk36@mail}.sysu.edu.cn

Corresponding author.

###### Abstract

LiDAR-based 3D object detection has made impressive progress recently, yet most existing models are black-box, lacking interpretability. Previous explanation approaches primarily focus on analyzing image-based models and are not readily applicable to LiDAR-based 3D detectors. In this paper, we propose a _feature factorization activation map_ (FFAM) to generate high-quality visual explanations for 3D detectors. FFAM employs non-negative matrix factorization to generate concept activation maps and subsequently aggregates these maps to obtain a global visual explanation. To achieve object-specific visual explanations, we refine the global visual explanation using the feature gradient of a target object. Additionally, we introduce a voxel upsampling strategy to align the scale between the activation map and input point cloud. We qualitatively and quantitatively analyze FFAM with multiple detectors on several datasets. Experimental results validate the high-quality visual explanations produced by FFAM. The code is available at https://github.com/Say2L/FFAM.git.

## 1 Introduction

In recent years, there has been rapid development in LiDAR-based 3D object detection [36; 32; 34; 38; 12], making it widely utilized in autonomous driving, industrial automation, and robot navigation. However, existing detection methods predominantly rely on deep neural networks with highly nonlinear and complex structures. Essentially, these models can be considered as "black box" systems. Such opaque modeling techniques hinder users from fully trusting the detection models, particularly in sensitive and high-risk domains. Consequently, understanding the decision-making process of these inherently opaque models is urgently needed.

Visual explanation methods [20; 1; 33; 15; 16] have gained widespread adoption for analyzing models based on deep neural networks. These methods generate saliency maps that highlight the crucial elements influencing the model's decision within the input map. Perturbation-based [15; 16], class activation map (CAM)-based [41; 20; 1], and gradient-based [28; 24; 25] methods are the three main categories of visual explanation methods. However, these methods primarily focus on image-based models and are not directly applicable to point cloud-based models. The pioneering work in analyzing 3D detectors is OccAM , which extends D-RISE  to perturb point clouds. As a perturbation-based approach, OccAM first randomly samples numerous sub-point clouds and measures the change in model predictions. However, the large number of inference calculations makes OccAM computationally intensive, and the sampling number easily impacts the quality of generated saliency maps.

Interpreting 3D detectors presents three key challenges. First, point clouds are inherently three-dimensional (3D). It is essential to generate corresponding 3D saliency maps for accurate interpretation. However, existing methods, such as popular CAM-based techniques, primarily utilize activation maps from the network's last layer to generate 2D saliency maps. Second, the explanation method for 3D detectors should provide detailed explanations for individual objects of interest. Yet, most existing methods yield class-specific saliency maps, which means they cannot focus on explaining a specific detection object. Lastly, point clouds are sparsely distributed in 3D space, rendering linear interpolation employed by many image-based explanation methods ineffective.

To address the aforementioned challenges, this paper introduces a feature factorization activation map (FFAM) to obtain visual explanations for 3D detectors. Specifically, to solve the first challenge, FFAM leverages the 3D feature maps within the 3D backbone  of detectors, rather than relying on the bird's eye view (BEV) feature maps from the last layer. Drawing inspiration from DFF , we employ non-negative matrix factorization (NMF)  to uncover latent semantic concepts within these 3D feature maps. Typically, point features with effective detection clues in 3D detectors contain richer semantic concepts. Thus, we aggregate concept activation maps generated by NMF to obtain a global concept activation map that highlights important points, as shown in Figure 1(a). To address the second challenge of obtaining object-specific saliency maps, we utilize the gradients of the 3D feature map, generated by an object-specific loss, to refine the global concept activation map. This process is illustrated in Figure 1(b), showcasing the desired effect. To tackle the final challenge, we introduce a voxel upsampling strategy to sample values from sparse neighbors, ensuring accurate saliency map generation.

We compare our FFAM with the current state-of-the-art method OccAM , as well as other image-based explanation methods including Grad-CAM  and ODAM . We conduct experiments on the KITTI  and Waymo Open  datasets, employing detectors such as SECOND  and CenterPoint . The qualitative and quantitative results demonstrate that our FFAM significantly outperforms the previous methods. The contributions of this work can be summarized as follows:

* We propose a feature factorization activation map (FFAM) method to obtain high-quality visual explanations for 3D detectors.
* We first introduce NMF in explaining point cloud detectors. By aggregating different concept activation maps, we obtain a global concept activation map that highlights points with significant detection clues.
* We utilize feature gradients of an object-specific loss to refine the global concept activation map, enabling the generation of object-specific saliency maps.
* A voxel upsampling strategy is proposed to upsample sparse voxels, thus aligning the scale between the activation map and input point cloud.

## 2 Related Work

**Explanation Methods for Image-based Models.** Existing explanation methods primarily focus on image classification models. Perturbation-based methods [27; 15; 4; 31] are widely used for interpreting image classification models. The core idea is to assign importance scores to perturbed feature components by disturbing the model's input and observing the output changes. CAM-based methods [41; 20; 1; 9] generate saliency maps by linearly combining activation maps from intermediate layers, weighted by their respective contributions. Some approaches (e.g. Score-CAM  and Ablation-CAM ) combine perturbation- and CAM-based ideas to eliminate dependence on backpropagation gradients. Additionally, gradient-based explanation methods [23; 28; 24; 25] use gradients to quantify input impact on network predictions. Higher gradient values indicate greater importance of the corresponding input elements. Moreover, feature factorization techniques like principal component analysis (PCA) and non-negative matrix factorization (NMF) can uncover latent patterns in deep features. DFF  employs NMF to localize semantic concepts within images.

Figure 1: Visualization of FFAM outputs. (a) global concept activation map and (b) object-specific activation map.

Compared to explanation methods for classifiers, only a limited number of approaches investigate explanations for object detection models. The aforementioned methods generate class-specific explanations, which are not feasible for object detection models. D-RISE  employs a perturbation strategy to generate instance-specific explanations by defining a detection similarity metric. In , a directed acyclic AND-OR Graph (AOG) is utilized to uncover latent structures in object detectors. G-CAME  combines activation maps with a Gaussian kernel of gradients to generate a saliency map for a predicted bounding box. ODAM  employs pixel-wise gradients of a target object to weigh the activation maps, thereby producing an instance-specific saliency map.

**Explanation Methods for Point Cloud-based Models.** In contrast to explanation methods for image-based models, the field of explanation for point cloud-based models is relatively underdeveloped. Existing methods primarily focus on point cloud classification models. For instance,  utilizes the loss gradient to measure the contribution of each point in the classifier. Similarly,  applies a gradient-based strategy to analyze the intermediate features of the network. Another approach  combines a generative model with the activation maximization method  to obtain a global explanation for point cloud networks.

Research on the explanation of 3D detectors is still quite limited. One perturbation-based method, OccAM , estimates the importance of individual points by testing the model with randomly generated subsets of the input point cloud. However, the scale of points in 3D space is considerably large, and the distribution of points acquired through LiDAR varies with distance. These aforementioned issues result in the following challenges for perturbation-based methods: (1) It is difficult to exhaustively perturb the point cloud, limiting the quality of visual explanations; (2) Generating ample random subsets of points requires multiple iterations, thereby reducing efficiency. Taking inspiration from feature factorization techniques  and gradient-based approaches [28; 14; 39], we propose an explanation method called FFAM. It aims to efficiently generate high-quality saliency maps for 3D detectors.

**LiDAR-based 3D Object Detection.** These methods can be categorized into two main groups: one-stage and two-stage detectors. **One-stage detectors** typically employ simple network architectures to achieve high speeds. For instance, SECOND  efficiently encodes sparse voxel features using a proposed 3D sparse convolution technique. PointPillars  divides a point cloud into pillar voxels, eliminating the need for 3D convolution layers and achieving fast inference speed. VoxelNeXt  introduces a fully sparse convolution network that eliminates the requirement for sparse-to-dense conversion. **Two-stage detectors** generally incorporate an additional stage to refine proposals generated by a one-stage network. PointRCNN  utilizes PointNet++  to generate proposals from raw points and then refines the bounding boxes in the second stage. PV-RCNN  combines a voxel-based proposal network with a point-based refinement network. CenterPoint  extracts point features from the surface centers of proposal bounding boxes for refinement. Voxel R-CNN  utilizes voxel features from the 3D backbone to refine the proposals. Our explanation method FFAM is adaptable to both one- and two-stage detectors without being limited by the detector type. We primarily conduct experiments on widely used detectors, including the one-stage detector SECOND and the two-stage detector CenterPoint.

## 3 Method

The goal of visual explanation for a 3D detector \(f\) is to produce a saliency map for each detection. Given a point cloud \(P^{N 4}\), the saliency map consists of \(N\) values presenting the importance of each point in \(P\) for a detection \(d\) which consists of a bounding box, a confidence score and a category label. We denote a detection \(d\) as follows:

\[d=[x,y,z,l,w,h,s,c]\] (1)

where \((x,y,z)\) denotes the center location, \((l,w,h)\) represents the object size (i.e., length, width and height), \(s\) and \(c\) indicate the confidence score and category label, respectively.

We propose FFAM to produce saliency maps in point cloud format for 3D detectors. The overview of our method is illustrated in Figure 2. It can be divided into three phases as follows: (1) Feature factorization (Sec. 3.1); (2) Gradient weighting (Sec. 3.2); (3) Voxel upsampling (Sec. 3.3).

### Feature Factorization Activation Map

Matrix factorization is widely used in fields such as recommendation systems, image processing, and natural language processing to extract potential features and reduce dimensionality. Non-negative matrix factorization (NMF) as a classical matrix factorization algorithm approximates a non-negative matrix by decomposing it into the product of two non-negative matrices. With this decomposition, NMF can discover potential patterns and conceptions in the raw matrix and extract the most important features. Given a non-negative matrix \(A^{m n}\), NMF retrieves an approximation \(^{m n}\) as follows:

\[(A)=*{argmin}_{} \|A-\|_{F}^{2},\\ \ =HW, ij,H_{ij},W_{ij} 0,\] (2)

where \(H^{m r}\) and \(W^{r n}\) denote two non-negative matrices. \(r\) is a predefined parameter indicating the number of latent concepts in matrix A. Each row \(W_{j}^{n}(1 j r)\) of \(W\) represents a concept vector. These concept vectors are typically well-interpreted and associated with object-part features, such as wheels, car doors, car roofs, and so on, following the non-negative additivity property of \(W_{j}\). Furthermore, each row \(H_{i}^{r}\) (where \(1 i m\)) of matrix \(H\) represents the combination weights of different concept vectors in \(W\). Combining these concept vectors using the weights \(H_{i}\), we obtain the \(i\)-th row feature of matrix \(\).

In this paper, we employ non-negative matrix factorization to handle the voxel feature map within the 3D backbone of detectors. Typically, voxel features that contain crucial detection clues tend to activate more concepts (e.g., license plates, car fronts, car edges) in detectors. As a result, aggregating all weights in \(H_{i}\) indicates the significance of the \(i\)-th voxel feature in the voxel feature map, as demonstrated in Figure 1(a).

Specifically, given a voxel feature map \(F^{M d}\) where \(M\) represents the voxel number and \(d\) denotes the channel number, a voxel feature \(F_{i}^{d}\) in \(F\) can be factorized as follows:

\[F_{i}=_{j=1}^{r}H_{ij}W_{j}.\] (3)

Further, we obtain the global concept activation map \(V\) by aggregating concept weight matrix \(H\) as follows:

\[V=_{j=1}^{r}H_{ j},\] (4)

Figure 2: Overall framework of our FFAM which can generate an object-specific saliency map for a detection \(d_{i}\).

where \(H_{ j}\) denotes \(j\)-th column of \(H\). The resulting \(V\) emphasizes points with multiple activated concepts from a global perspective. And due to the downsampling operation in the detection network, the granularity of \(V\) is typically coarse. Therefore, further processing is required to obtain an object-specific and fine-grained activation map, as described in Sec. 3.2 and Sec. 3.3.

### Object-Specific Gradient Weighting

In a 3D detector, the output contains a large number of detections. To obtain an object-specific activation map, we establish a loss function for a specific detection. Specifically, given a detection \(d\), we create a baseline detection \(d_{b}\) to calculate the loss \(\):

\[=\|d-d_{b}\|_{1}.\] (5)

For simplicity, we use the L1 loss function and set all values in \(d_{b}\) equal to 0. Then we obtain the gradient map \(G^{M d}\) of the feature map \(F\):

\[G=.\] (6)

Considering an optimization process from \(d\) to \(d_{b}\), the matrix \(G\) denotes the optimal direction for reducing the loss. If we iteratively update the feature map \(F\) based on the gradient map \(G\), the information related to the detection \(d\) will be diminished. Alternatively, by utilizing \(G\), we can identify the locations in the feature map \(F^{l}\) that contain clues about \(d\). Consequently, an object-specific activation map \(M\) for \(d\) can be obtained as follows:

\[=_{k=1}^{d}|G_{ k}|,\] (7) \[M=()(V),\]

where \(G_{ k}^{M}\) refers to the \(k\)-th column of \(G\), while \(\) represents the normalization operation, and \(\) denotes element-wise multiplication. By modifying the loss function to a specific attribute \(p\) in detection \(d\), we can examine the specific points on which the detector concentrates when predicting attribute \(p\).

### Voxel Upsampling

Due to downsampling operations in 3D detection networks, the scale of the activation map is typically smaller than that of the input point cloud. Consequently, upsampling the activation map \(M\) becomes necessary. However, unlike 2D images, linear interpolation for upsampling 3D sparse voxels presents challenges. To address this, we draw inspiration from the voxel query technique proposed by  and introduce a voxel upsampling strategy for 3D sparse voxels. Specifically, we define the voxel size as \(s\), and the ranges of the point cloud for three axes as \([x_{l},x_{r}]\), \([y_{l},y_{r}]\), and \([z_{l},z_{r}]\) respectively. Given a point \(p=(x,y,z)\), we calculate the coordinate \((x_{p},y_{p},z_{p})\) of voxel \(v_{p}\) to which \(p\) belongs as follows:

\[x_{p}=}{s},\;y_{p}=}{s},\;z_{p}=}{s}.\] (8)

Then we query neighbor voxels on activation map \(M\) for \(p\), using Manhattan distance to control the query range:

\[d(v_{p},v)=|x_{n}-x_{p}|+|y_{n}-y_{p}|+|z_{n}-z_{p}|,\] (9)

where \((x_{n},y_{n},z_{n})\) is the coordinate of an neighbor voxel \(v\), \(d(,)\) is the Manhattan distance between two voxels. We sample up to \(k\) neighbor voxels within a distance threshold. Finally, the salience score \(s_{p}\) of point \(p\) is calculated as follows:\[s_{p}=_{v},v))}{_{v}(d(v_{p},v))}M_{v},\] (10)

where \(\) is the set of neighbor voxels, \(\) denotes a Gaussian kernel with a standard normal distribution, \(M_{v}\) represents the value of voxel \(v\) on activation map \(M\).

## 4 Experiments

In this section, we compare our FFAM with existing explanation methods, including Grad-CAM  and ODAM , for image-based models, as well as with OccAM , the state-of-the-art explanation method for point cloud-based models. We adopt two datasets for evaluation: KITTI , a widely used autonomous driving dataset, and Waymo Open , containing complex multi-object scenes. For KITTI, experiments are conducted on SECOND . For Waymo Open, we mainly evaluate on CenterPoint . The experiments are run using PyTorch and an RTX 3090 GPU. The hyperparameters of detectors and OccAM remain consistent with their official implementations. The parameter \(r\) used in NMF is set to 64. The Manhattan distance threshold and parameter \(k\) in voxel upsampling are set to 2 and 16, respectively. We use the 3D feature map from the third block of the 3D backbone as FFAM input. Hyperparameters analysis and ablation study are in App. A.1 and App. A.4, respectively.

Figure 3: Saliency maps for SECOND  and CenterPoint . The green bounding boxes indicate the detected objects, while warmer colors (using the turbo colormap) represent higher point contributions to these detections. The crops are provided for visualization purposes only.

### Qualitative Results

To verify the interpretability of our FFAM, we visualize explanations for some objects. We also visualize the average saliency maps of different categories for specific object attributes to study the latent pattern of 3D detectors.

**Visualization of Saliency Map.** We compare the visual explanations generated by FFAM and OccAM  for cars, pedestrians, and cyclists in Figure 3(a). These detection results are obtained by SECOND  detector trained on KITTI . OccAM exhibits significant background noise due to its random masking mechanism. In contrast, our FFAM demonstrates a strong ability to generate clear, distinct object-specific saliency maps. We observe the detector also captures relevant clues from the background and neighboring objects. Furthermore, we compare saliency maps generated by FFAM and OccAM on Waymo Open  using the CenterPoint  detector, as shown in Figure 3(b). The saliency maps produced by OccAM struggle to focus on the intended object for interpretation. They have more highly salient points distributed on the background compared to KITTI. We attribute this discrepancy to the larger number of points in Waymo Open samples, challenging the random masking mechanism to sample diverse point masks effectively. Conversely, our FFAM consistently generates high-quality saliency maps on Waymo Open.

**Average Saliency Map.** To further explore the detection mode of detectors and verify the interpretability of FFAM, we average the saliency maps of specific classes, including cars, pedestrians and cyclists. We use SECOND trained on KITTI  as the detector. To accomplish this, we first scale all boxes and associated points to a uniform size and then align them with respect to their center and rotation angle. Next, we voxelize the resulting point cloud and calculate the average saliency values of individual points within each voxel. The resulting saliency maps for different object attributes are presented in Figure 4.

As depicted in the first two rows of Figure 4, the detector primarily identifies and localizes car objects based on the points located at the four corners of the car. By analyzing features from these points, the detector infers various attributes of a car, such as its center location, length, width, rotation angle, and classification score. This can be attributed to the fact that car objects are often incomplete in outdoor point clouds, and their corners are frequently scanned by LiDAR and used as key features. However, there is a special case, as shown in the first two rows of the penultimate column of Figure 4, where the height attribute is predicted primarily based on the points at the top of the car. As illustrated in the third row of Figure 4, the detector predicts pedestrian objects mostly based on the points distributed on the head and shoulder regions. Additionally, the detector recognizes cyclist objects mainly based on the points distributed on the head and back of the human body, as shown in the last row of Figure 4. Furthermore, we observe that the prediction of cyclist height heavily relies on the points distributed on the head, similar to the prediction of car height. Additional average saliency maps of other detectors are in App. A.2.

Figure 4: Average saliency maps for different object attributes. \((x,y,z)\) denotes the center of predicted object. \(l\), \(w\), \(h\), \(r\) and \(s\) represent the length, width, height, rotation angle and classification score of predicted object, respectively. \(d\) indicates the combination of all attributes.

### Quantitative Results

We adopt Deletion, Insertion [15; 16; 39], visual explanation accuracy (VEA)  and Pointing games (PG) to evaluate our FFAM. SECOND trained on KITTI is used as the baseline detector. Following previous work , we use the well-detected objects in the evaluation dataset as the subjects to be explained. In particular, a predicted object is considered well-detected if the IoU between it and its ground truth is greater than [0,7, 0.5, 0.5] for car, pedestrian and cyclist classes, respectively. See App. A.3 for results on Waymo Open.

Deletion and Insertion are widely used to evaluate explanation methods for image-based detection models [16; 39]. Deletion involves sequentially removing highly salient elements from a scene, measuring the rate model predictions diverge from the original. Insertion progressively adds salient elements to an empty scene, measuring how quickly predictions approach the original. Considering the similarity between pixels in an image and points in a point cloud, we employ Deletion and Insertion to evaluate FFAM. In outdoor point cloud scenes, objects are relatively small compared to global scenes, so we only operate on points within twice the diagonal length of an object's bounding box from its center. We use IoU between a prediction and ground truth as the measure score. Average IoU curves are presented in Figure 5(a-b), and Table 2 reports the area under the curve (AUC) for different categories. A lower Deletion AUC indicates a steeper drop in the IoU score, reflecting a more pronounced impact of removed salient points. Conversely, a higher Insertion AUC suggests a larger increase in the IoU score per step, indicating the significance of added salient points. Our methods have the fastest performance drop and largest increase for Deletion and Insertion, showing points highlighted in our saliency maps have a greater effect on detector predictions than the other methods.

Visual Explanation Accuracy.VEA calculates the point-level intersection over union (IoU) between the ground truth masks and saliency maps, which are thresholded at various values. The results of VEA for different object categories can be found in Table 1. Notably, our FFAM achieves the highest VEA scores across all categories, indicating the compactness of the visual explanations generated by FFAM. On the other hand, OccAM and Grad-CAM exhibit lower performance on this metric. OccAM tends to mark a significant number of background points, while Grad-CAM is a class-specific visual explanation method, which may explain their comparatively weak performance.

Pointing Game.To further assess the localization capability of FFAM, we present the results of the Pointing game (PG). In this evaluation, a hit is recorded if the point with the highest saliency value falls within the ground truth bounding box, while a miss is counted otherwise. The PG metric

   &  &  \\   & All & Car & Ped. & Cyc. \\  All & Car & Ped. & Cyc. \\  Grad-CAM & 0.015 & 0.015 & 0.018 & 0.013 \\ ODAM & 0.179 & 0.163 & 0.280 & 0.233 \\ OccAM & 0.064 & 0.063 & 0.080 & 0.042 \\ FFAM (Ours) & **0.391** & **0.363** & **0.543** & **0.515** \\  

Table 1: Comparison of visual explanation accuracy metric for different categories. ‘all’ denotes the three categories are included.

   &  &  \\   & All & Car & Ped. & Cyc. \\  Add-CAM & 0.335 & 0.373 & 0.137 & 0.129 & 0.797 & 0.821 & 0.688 & 0.725 \\ ODAM & 0.134 & 0.138 & 0.122 & 0.098 & 0.885 & 0.902 & 0.785 & 0.828 \\ OccAM & 0.286 & 0.311 & 0.146 & 0.167 & 0.863 & 0.880 & 0.761 & 0.790 \\ FFAM (Ours) & **0.071** & **0.068** & **0.098** & **0.078** & **0.907** & **0.923** & **0.806** & **0.854** \\  

Table 2: AUC for Deletion and Insertion curves. The results of different categories are reported. ‘all’ means the combination of the three categories.

Figure 5: AUC diagrams for Deletion and Insertion. Average IoU vs. (a) Deletion steps and (b) Insertion steps.

measures the accuracy of saliency maps by calculating the ratio of hits to the total number of hits and misses. Furthermore, we report the energy-based PG metric (enPG) proposed in , which considers the energy within the ground truth region compared to the global scene. As shown in Table 3, our FFAM surpasses previous methods on all metrics, indicating its superior ability to focus on the explained object. Notably, Grad-CAM performs poorly on both PG and enPG, which aligns with the VEA results presented in Table 1. This suggests that classification-based explanation methods alone are insufficient for generating meaningful explanations for detectors.

### Modes of False Positive

FFAM can be used to identify false positive modes of a detector. A detection is considered a true positive if correctly classified and the Intersection over Union (IoU) between the prediction box and ground truth exceeds a threshold. Otherwise, it is a false positive. The IoU thresholds are 0.7, 0.5, and 0.5 for car, pedestrian, and cyclist objects, aligning with the KITTI official metric . To reveal detection modes, we compute average saliency maps separately for true positives and false positives. Results are shown in Figure 6. First, we observe the average saliency maps of false positives exhibit similarities to those of true positives. The detector predicts a false positive because it detects a similar pattern to that of a true positive. Second, false positives tend to be surrounded by more noise points, with a point density of approximately one-third of true positives. We believe noises and sparse density may be significant factors contributing to the occurrence of false positives. Lastly, the ratio of car, pedestrian, and cyclist objects in true positives is approximately 36:5:2, while in false positives, it is 13:8:2. This suggests car objects are less prone to false positives compared to pedestrian and cyclist objects.

## 5 Conclusion

In this paper, we propose a visual explanation method, FFAM, that efficiently generates high-quality explanations for 3D detectors. FFAM utilizes non-maximum matrix factorization to obtain a global concept activation map, which is then refined using object-specific gradients. To align the granularity of the input point cloud and intermediate features, we introduce a voxel upsampling strategy. Qualitative and quantitative experiments demonstrate that our FFAM provides more interpretable and compact visual explanations than previous methods. The limitation of FFAM is that it needs to access the feature maps within 3D detectors. In future work, we will explore using visual explanations to enhance the accuracy and efficiency of 3D detectors.

   &  &  \\   & All & Car & Ped. & Cyc. & All & Car & Ped. & Cyc. \\  Grad-CAM & 0.093 & 0.080 & 0.166 & 0.163 & 0.021 & 0.022 & 0.014 & 0.011 \\ ODAM & 0.901 & 0.895 & 0.939 & 0.926 & 0.633 & 0.639 & 0.577 & 0.654 \\ OccAM & 0.946 & 0.957 & 0.898 & 0.860 & 0.023 & 0.024 & 0.019 & 0.013 \\ FFAM (Ours) & **0.991** & **0.989** & **0.999** & **0.998** & **0.664** & **0.671** & **0.591** & **0.719** \\  

Table 3: Comparison of Pointing game (PG) and energy-based Pointing game (enPG) metrics.

Figure 6: Average saliency maps for true and false positives. The \(1^{st}\) and \(2^{nd}\) rows represent cases of true and false positives, respectively.