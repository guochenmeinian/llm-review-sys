# Fast Trainable Projection for Robust Fine-Tuning

Junjiao Tian

Georgia Institute of Technology

jtian73@gatech.edu

&Yen-Cheng Liu

Georgia Institute of Technology

ycliu@gatech.edu

&James Seale Smith

Georgia Institute of Technology

jamessealesmith@gatech.edu

&Zsolt Kira

Georgia Institute of Technology

zkira@gatech.edu

###### Abstract

Robust fine-tuning aims to achieve competitive in-distribution (ID) performance while maintaining the out-of-distribution (OOD) robustness of a pre-trained model when transferring it to a downstream task. Recently, projected gradient descent has been successfully used in robust fine-tuning by constraining the deviation from the initialization of the fine-tuned model explicitly through projection. However, algorithmically, two limitations prevent this method from being adopted more widely, _scalability_ and _efficiency_. In this paper, we propose a new projection-based fine-tuning algorithm, Fast Trainable Projection (FTP) for computationally efficient learning of per-layer projection constraints, resulting in an average \(35\%\) speedup on our benchmarks compared to prior works. FTP can be combined with existing optimizers such as AdamW, and be used in a plug-and-play fashion. Finally, we show that FTP is a special instance of hyper-optimizers that tune the hyper-parameters of optimizers in a learnable manner through nested differentiation. Empirically, we show superior robustness on OOD datasets, including domain shifts and natural corruptions, across four different vision tasks with five different pre-trained models. Additionally, we demonstrate that FTP is broadly applicable and beneficial to other learning scenarios such as low-label and continual learning settings thanks to its easy adaptability. The code will be available at https://github.com/GT-RIPL/FTP.git.

## 1 Introduction

With new progress being made in pre-training of foundation models every year, such as self-supervised [1; 2; 3] or language-supervised training , their potential has gone far beyond merely speeding up convergence . They have demonstrated superior transferability to other tasks, reducing the need for data and improving robustness and generalization capabilities [6; 7; 8]. The problem of how to fine-tune (transfer) a foundation model such that we maintain its robustness and generalization capabilities acquired during pre-training on large datasets has therefore become an essential research topic. This problem is hard because the conventional machine learning paradigm of validating on held-out training data does not impose any constraints on robustness and generalization w.r.t. the foundation models. For example, fine-tuning with a slightly large learning rate can easily destroy capabilities that reside in the foundation models , while performing well on the target task.

To maintain the robustness and generalization capability of the pre-trained model when fine-tuning, recent projection-based methods explicitly constrain the distance between the fine-tuned and the pre-trained models through projection. For example, MARS-SP  specifies a distance constraint shared by all layers in a neural network. However, it is practically intractable to tune a constraint for each layer (poor _scalability_). TPGM  proposes to _automatically_ learn different constraintsfor each layer, solving the issue of scalability in MARS-SP, however, with increased computational overhead (poor _efficiency_). These limitations prevent the method from being adopted more widely.

To achieve scalability and efficiency simultaneously, we propose Fast Trainable Projection (FTP), for learning both the projection constraints and the main model in a single forward pass (Fig. 0(a)), significantly reducing computation overhead in prior works while achieving competitive performance. Specifically, FTP removes the algorithmic redundancy of extra training procedures required in TPGM , which requires sampling a separate batch of data and running a nested training loop. FTP achieves this by 1) utilizing different batches of training data sampled at consecutive steps and 2) re-using gradients calculated for the main model update (Sec. 3.2). This leads to a \(35\%\) speedup with comparable performance in fine-tuning (Fig. 0(b), 0(c)). The efficiency improvement and easy adaptability as a drop-in replacement with existing optimizers are essential to making projection-based methods applicable to more fine-tuning problems. For example, we implement SGDP, an SGD variant with built-in FTP. SGDP can be used as a drop-in replacement for SGD (details in Appendix 8.7) as:

``` optimiser=SGD(param_group,**optimizer_params)#SeeAppendix8.7. ```

To demonstrate this, we test FTP on four different vision tasks, image classification, semantic segmentation, human parts segmentation, and surface normal estimation. FTP shows superior OOD performance under domain shift or natural corruptions on all benchmarks. Moreover, we apply FTP to a continual learning (CL) benchmark and achieve state of the art performance when combined with a simple CL technique.

Finally, we show that FTP is a special instance of hyper-optimizers [11; 12; 13; 14; 15; 16; 17; 18] that aims to reduce the manual tuning of optimization hyper-parameters such as learning rate by learning them automatically through automatic differentiation and nested optimization. Theoretically, to understand why FTP and other projection methods can maintain the robustness of the pre-trained models, we propose to establish a theoretical connection between robustness and projection through the lens of Liptschitz continuity, a widely adopted measure of robustness [19; 20; 21]. In summary, our contributions are:

* We present a new fine-tuning algorithm, Fast Trainable Projection, to efficiently learn the projection constraints and fine-tune the model simultaneously, bringing significantly improved computation efficiency w.r.t. prior works  in Sec. 3.2.
* We show that FTP is a special instance of hyper-optimizers that aims to reduce manual tuning of hyper-parameters through nested optimization in Sec. 3.3.
* We discuss a dual perspective of the fine-tuning robustness in the feature space and the weight space of a model to mathematically understand why projection can maintain the robustness of the pre-trained models in Sec. 3.4.

Figure 1: (a): FTP updates the model using (unconstrained) gradient descent (**UGD**) to calculate \(}_{t}\), then updates the projection constraint \(_{t}\) (**ProjUpdate**), and finally projects \(}_{t}\) to \(_{t}\) (**Projection**), all in a single forward pass. (b),(c): Visualizations of in-distribution (Real/Clean, labeled as ID), out-of-distribution (Sketch/Fog, etc.) accuracy and computation time (iterations/sec) as a percentage of vanilla fine-tuning (FT) for classification on DomainNet (Tab. 2) and semantic segmentation on PASCAL-Context (Tab. 4) respectively. FTP improves the OOD robustness of FT and is much more computationally efficient than prior work TPGM.

* We show superior robustness on OOD datasets on four vision tasks with five pre-trained models and SOTA performance on a continual learning benchmark, all with a 35\(\%\) speedup in Sec. 4.

## 2 Related Works

We summarize related works in (general) robust fine-tuning into three categories: when, where, and how much to fine-tune, depending on their underlying strategy. Moreover, we discuss recent advances in fine-tuning language-image pre-trained models, which have inspired specialized fine-tuning strategies. **When to fine-tune:** LP-FT  discovers that fine-tuning the entire network can distort features in the pre-trained models and proposes to first only fine-tune the last linear layer followed by training the entire network with a small learning rate. We will include LP-FT in our experiments. **Where to fine-tune:** Instead of fine-tuning the entire network, some methods investigate the choice of weights to fine-tune. SpotTune  learns where to fine-tune through an additional policy network. However, SpotTune needs to retain the policy network, the pre-trained model, and the fine-tuned model in memory for inference, adding significant computation at inference time. Recently, SurgicalFT  proposes to use the GradientNorm heuristic, the ratio of the gradient norm to the parameter norm, to determine which layer to fine-tune. Parameter-efficient fine-tuning methods are another example of this category. While they aim to minimize the parameters tuned, they have been shown to improve OOD generalization performance as well in NLP applications [24; 25; 26; 27; 28; 29]. We specifically compare to two recent parameter-efficient methods that only tune the bias terms: Bitfit  for Transformers  and Partial Fusion  for ResNets . **How much to fine-tune:** Our work belongs to this category where the entire neural network is fine-tuned simultaneously. Specifically, we can split works into two sub-categories: regularization and projections. **Regularization:** DELTA  proposes to regularize the output (feature maps) of a neural network to that of its pre-trained model. This requires two separate passes through the pre-trained model and the fine-tuned model increasing both memory and computation overhead. L2-SP  instead regularizes the L2 distance between the fine-tuned model and the pre-trained model, serving as a strong baseline. **Projection:** Utilizing projection to enforce a close distance to the pre-trained model has been studied in prior works: MARS-SP  and TPGM . We dedicate a section to revisit them later in the method section (Sec. 3.1). **Language-Image Pre-trained Models.** Several recent works have proposed special fine-tuning strategies for language-image pre-trained models with zero-shot capability. WISE-FT  achieves SOTA performance by linearly interpolating a fine-tuned model and its initialization _at the end_ of fine-tuning. However, it only applies to a subset of pre-trained models with linear connectivity such as CLIP . FT-Like-Pretrain  proposes to use a contrastive fine-tuning strategy, the same strategy used in pre-training for those models, instead of the conventional cross-entropy loss for many vision tasks. The method has demonstrated superior results when combined with WISE-FT, where WISE-FT contributes the most to the improvement. Similarly, we will also combine our method with WISE-FT to show improved OOD performance using CLIP.

## 3 Method

### Review: Enforcing Projection and Learning Constraints

In this work, we focus on fine-tuning a pre-trained model, where \(_{0}^{n m}\) is the weights of a linear layer in the pre-trained model, to a downstream task. We denote \(_{t}\) as the fine-tuned model at training iteration \(t\) and \(^{i}\) as the \(i\)th row of a matrix \(^{n m}\). Several prior works [9; 10] have attempted to use projection to improve fine-tuning robustness. The most vanilla formulation in MARS-SP  has two steps: _unconstrained gradient descent_ and _projection_.

**Unconstrained Gradient Descent** (Abbrev. UGD), the projection-based methods first compute the updated model weights \(}_{t}\)_without_ projection. For example, at iteration \(t\), given a batch of training data \(_{t}^{tr}\), we first obtain \(}_{t}\) as the following,

\[_{t}_{_{t}^{tr}}(_{t -1})^{n m},}_{t}}(_{t-1},_{t}).\] (1)

where \(_{t}\) is the derivative of the loss function \(_{_{t}^{tr}}(_{t-1})\) calculated on \(_{t}^{tr}\) w.r.t. \(_{t-1}\) and \(}()\) is an existing optimization algorithm, such as SGD, AdamW .

**Projection.** MARS-SP  projects the updated model \(}_{t}\) towards its initialization \(_{0}\) with a pre-defined projection constraint \(\) for all layers using the MARS matrix norm (see Appendix 8.2) as shown below in Eq. 2.

\[_{t}=(}_{t},_{0},_{t})= _{t}^{i}\\ \\ _{t}^{i}=}_{t}^{i}-_{0}^{i}\|_{1}}(}_{t}^{i}-_{0}^{i})+_{0}^{i}\\ \\ (}_{t}^{i}-_{0}^{i}\|_{1}}( }_{t}^{n}-_{0}^{n})+_{0}^{n})^{ }\] (2)

However, MARS-SP has poor scalability because it is practically intractable to hand-tune different constraints for each layer, which results in sub-optimal performance as reported by TPGM . Instead of a pre-defined \(\) for all layers, TPGM proposes to learn a different constraint \(_{t}\) for each layer1. and updates them iteratively during training. This enables TPGM to customize a different regularization strength for each layer and to have superior performance for both ID and OOD data.

**ProjUpdate.** Given as input the frozen unconstrained model \(}_{t}\) from UGD (Eq. 1), TPGM adds an intermediate _ProjUpdate_ function before _projection_, which samples a separate set of data from the validation dataset \(_{t}^{val}\) and uses a standalone training loop to update the projection constraints \(_{t}\) while keeping the model \(}_{t}\) frozen. Specifically, _ProjUpdate_ creates a temporary projected model \(_{p}\) by projecting \(}_{t}\) towards \(_{0}\) based on the previous constraint \(_{t-1}\) using Eq. 2, i.e., \(_{p}=(}_{t}|,_{0},_{t-1})\). Therefore, \(_{p}(_{t-1})^{2}\) can be viewed as a function of \(_{t-1}\). Then FTP calculates the gradient \(_{t}\) by taking a derivative of the loss function \(_{_{t}^{val}}(_{p}(_{t-1}))\) w.r.t. \(_{t-1}\):

\[_{t}_{_{t}^{val} }(_{p}(_{t-1})),_{t}=( _{t-1},_{t})\] (3) \[_{p}=[_{p}^{i},, _{p}^{n}]^{}_{p}^{i}=}{\|}_{t}^{i}-_{0}^{i}\|_{1}}(}_{t}^{i}-_{0}^{i})+_{0}^{i}.\]

With the calculated gradient, TPGM uses an existing optimizer \(()\) to update \(_{t}\). This procedure, sampling \(_{t}^{val}\) and calculating the derivative \(_{_{t}^{val}}(_{p}(_{t-1}))\), is the key to learning projection constraints because the unconstrained model \(}_{t}\) (highlighted above and calculated in Eq. 1), was updated on the training data \(_{t}^{tr}\) and \(_{t}\) is now updated on separate data \(_{t}^{val}\). The discrepancy between \(_{t}^{tr}\) and \(_{t}^{val}\) allows TPGM to find a better projected model \(_{p}\) (projected between \(}_{t}\) and \(_{0}\)) by updating \(_{t}\), which balances between fitting the training data \(_{t}^{tr}\) and generalizing to \(^{val}\). Finally, with an updated \(_{t}\), TPGM _again_ projects \(}_{t}\) towards \(_{0}\) to obtain the final model \(_{t}\) using Eq. 2, replacing the pre-defined \(\) with a learned \(_{t}\). A flow chart of TPGM is in Fig. 2.

The algorithm demonstrated the capability to automatically learn different constraints for each layer, solving the scalability issue in MARS-SP. However, TPGM introduces extra computation in the additional training loop. In the next section, we propose a scalable and efficient projection algorithm that learns the projection constraints for each layer without separate validation data and loops.

### FTP: Fast Trainable Projection

To inherit the scalability of TPGM while reducing the computational overhead, we propose Fast Trainable Projection (FTP) (Algorithm 1). Similar to TPGM, the algorithm has three components: _UGD_, _ProjUpdate_, _Projection_. The _ProjUpdate_ component is the major contributor to efficient computation. It builds on a key insight: Instead of sampling separate data \(_{t}^{val}\) each time, we use two training data batches sampled independently at consecutive steps, e.g., \(_{t-1}^{tr}\) and \(_{t}^{tr}\). Specifically, we use \(_{t}^{tr}\) to update \(_{t}\) instead of \(_{t}^{val}\). As a result, the optimization of \(_{t}\) re-uses most of the computation used for the optimization of the main model.

**ProjUpdate.** Specifically, instead of taking a derivative of \(_{_{t}^{val}}(_{p})\) w.r.t. \(_{t-1}\) as in TPGM, FTP calculates the gradient of \(_{t-1}\) by the derivative of the loss function on the current training data \(_{_{t}^{tr}}(_{t-1})\) w.r.t. \(_{t-1}\). Note that \(_{t-1}=(}_{t-1}}|,_{0},_{ t-1})\) is also a function of theconstraint \(_{t-1}\) as a result of projection from the previous step. Hence, by virtue of the chain rule, the gradient of \(_{_{t}^{tr}}(_{t-1}(_{t-1}))\) w.r.t. \(_{t-1}\) is,

\[_{t}=_{i=1}^{n}_{_{t}^{ tr}}(_{t-1}^{i}(_{t-1}))^{}}_{_{i}^{i}} _{t-1}^{i}}{_{t-1}}=_{i=1}^{n} _{i}^{i}(}_{t-1}^{i}-_{0}^{i}) }_{t-1}^{i}-_{0}^{i}\|_{1}}\] (4)

where the summation loops over each row in the matrix \(_{t-1}\) because the same constraint \(_{t-1}\) is enforced for all rows (see MARS norm in Appendix Eq. 15 and Eq. 2 ) so the final gradient is the summation of all gradients for each row. Similar to TPGM, because the starting point of projection \(}_{t-1}\) (highlighted above) was updated using the previous training batch \(_{t-1}^{tr}\) and the gradient \(_{t}\) is calculated using the current batch \(_{t}^{tr}\), the discrepancy between \(_{t-1}^{tr}\) and \(_{t}^{tr}\) enables FTP to learn meaningful projection constraints. Crucially, we proposed a novel formulation that allows for **re-using** the gradient \(_{t}\) used for calculating the unconstrained model \(}_{t}\) in the UGD step (Eq. 1).

**Gradient Annealing.** Prior work  noticed that learning projection constraints for each layer can suffer from underfitting because the learned constraints can be too conservative, and used an additional regularization to help reduce this negative effect. For FTP, we introduce a simple technique that uses a single gradient annealing factor for all layers, \(\) to modulate the magnitude of the _positive_ gradient \(_{t}>0\), which contributes to the shrinkage of the constraints. When \(_{t}>0\),

\[_{t}=_{t}.\] (5)

For example, when \(=0\), the projection constraint \(\) will not receive any positive gradient and is therefore non-decreasing. With the annealed gradients \(_{t}\), we update the constraint using the Adam update rule  because Adam is suitable for non-stationary optimization, where the optimal values change over time. Please see Appendix 8.3 for a detailed algorithmic description of **AdamUpdate** and additional discussion on how FTP saves computation.

Figure 2: Computation Flow Chart of TPGM (top) and FTP (bottom) at iteration \(t\). The main difference between TPGM and FTP is in the **PoriUpdate** step. FTP uses the previous model \(_{t-1}\) and cached gradients from \(_{_{tr}^{t}}(_{t-1})\) to update the projection constraints \(_{t}\).

Finally, after obtaining the updated \(_{t}\) from **AdamUpdate**, FTP applies the learned constraints to project the current unconstrained model \(}_{t}\) towards the pre-trained model \(_{0}\) using Eq. 2 with a different constraint for each layer. The complete algorithm is summarized in Alg. 1. For a quick comparison with TPGM, we provide a side-by-side computation flow chart of FTP in Fig. 2.

**Implicit Assumption.** The algorithmic difference between TPGM and FTP makes an implicit assumption. Specifically, after obtaining the updated constraints \(_{t}\) after **AdamUpdate**, if the algorithm were to follow TPGM, the next step would be applying the updated constraints to _recalculate_ the previous model \(_{t-1}\) since \(_{t}\) is updated based on \(_{t-1}\) (Eq. 4). However, instead of rolling back, FTP applies the updated constraints directly to the current unconstrained model \(}_{t}\) to calculate \(_{t}\). This step assumes _smoothness_ in the update of \(_{t}\), i.e., the \(_{t}\) does not change drastically in consecutive steps. The assumption is valid since \(_{t}\) is updated by _AdampUpdate_ (Alg. 2 in Appendix 8.3) which uses a moving average update with a momentum of \(0.9\). So the change of \(_{t}\) is very smooth because of the high discount factor of \(0.9\). Importantly, it enables us to **re**-use the same gradient \(_{t}\) available for computing the current unconstrained model \(}_{t}\) to update \(_{t}\). This is the key to saving computation because the separate training loop as a result of "rolling back" is the main computation bottleneck in TPGM.

### FTP as a Hyper-Optimizer for Fine-Tuning

The FTP algorithm in Alg. 1 bears motivational and algorithmic similarity to a recent resurrection of _hyper-optimizers_[11; 12; 13; 14; 15; 16; 17; 18]. Specifically, hyper-optimizers aim to learn the hyper-parameters such as the learning rate in an optimizer by treating them as learnable parameters through nested differentiation and optimization because manual tuning of those hyper-parameters can be time-consuming and can lead to sub-optimal performance. FTP stems from the same motivation as the manual specification of projection constraints can be computationally infeasible .

To understand the algorithmic similarity better, let's use SGD as an example. Suppose at iteration \(t-1\), we have updated the model parameters \(_{t-2}\) through SGD with a learning rate \(_{t-1}\).

\[_{t-1}=_{t-2}-_{t-1}(_{t -2})\] (6)

At the current step \(t\), hyper-optimizers first calculate the gradient w.r.t to the learning rate \(_{t-1}\) and update it using another SGD optimizer with a new learning rate parameter \(\).

\[_{t}=_{t-1}-(_{t-1})}{ }=_{t-1}+(_{t-1})^{T} (_{t-2})\] (7)

Finally, using the updated \(_{t}\), hyper-optimizers update the main model parameters.

\[_{t}=_{t-1}-_{t}(_{t-1})\] (8)

It's not hard to spot the algorithmic similarity between the FTP algorithm and hyper-optimizers. Both algorithms first update the hyper-parameters (projection constraints \(_{t}\) in Eq. 4 vs. the learning rate \(_{t}\) in Eq. 7) using the cached information from the previous iteration and the gradient from the current iteration (known as hyper-gradients). Then, they apply the updated hyper-parameters to calculate the current model. Finally, hyper-optimizers make the same assumption of smoothness in the update of the hyper-parameters such that the update of the hyper-parameters and the model parameters can be performed consecutively in a single forward pass. In this regard, FTP can be seen as a special instance of hyper-optimizer for fine-tuning.

### Dual Perspective: Fine-tuning Robustness in Feature Space and Weight Space

It is not immediately clear why FTP's and other methods' projections in the weight space maintain the robustness of the pre-trained model in the feature space besides the intuition that the closer to the pre-train model the more likely the fine-tuned model will behave like it. To fully understand this, we study the mathematical connection between projection and robustness. Let \(^{m}\) denote an input vector and \(h():^{m}^{n}\) a function mapping it to a feature space. Given two input vectors \(,^{}^{m}\), we denote the distance between them in the original space by their vector norms \(\|-^{}\|_{}\) and in the feature space by \(\|h()-h(^{})\|_{}\). Let \(h_{f}()\) and \(h_{0}()\) denote a fine-tuned model and its pre-trained initialization, and \( h() h_{f}()-h_{0}()\) denotes the _difference function_.

To capture the robustness of a fine-tuned model, we apply the notion of Lipschitz continuity on the _difference function_ because its Lipschitz constant captures the maximum rate of change of differences between the fine-tuned model and the pre-trained model in the feature space. Formally,

\[\| h()- h(^{})\|_{} L_{d}\| -^{}\|_{}(,^{})^{m}.\] (9)

where \(L_{d} 0\) is the Lipschitz constant of the difference function \( h()\). If the inequality is satisfied, in this paper, we call \(h_{f}()\)\(L_{d}\)-Lipschitz-robust w.r.t. the pre-trained initialization \(h_{0}()\).

The definition has a natural intuition stemming from Lipschitz continuity, a measure of robustness [19; 20; 21]. A Lipschitz function is limited by how fast it can change, governed by the Lipschitz constant. Traditionally, a small Lipschitz constant is associated with better robustness, because a small constant means less sensitivity to changes in the input. We provide the following lemma (proof in Appendix 8.1) to illustrate the connection between the _difference function_ and the robustness of the fine-tuned model.

**Lemma 1**.: _If a fine-tuned model \(h_{f}()\) is \(L_{d}\)-Lipschitz-robust with respect to its \(L_{0}\)-Lipschitz pre-trained initialization \(h_{0}()\), i.e., \((,^{})^{m},\)_

\[\| h()- h(^{})\|_{} L_{d} \|-^{}\|_{}\|h( )_{0}-h(^{})_{0}\|_{} L_{0}\| -^{}\|_{}\]

_then, \(h_{f}()\) is \((L_{d}+L_{0})\)-Lipschitz, i.e.,_

\[\|h_{f}()-h_{f}(^{})\|_{}(L_{d}+L_{0} )\|-^{}\|_{}(, ^{})^{m}.\]

**Feature Space.** From Lemma 1, we can see that minimizing \(L_{d}\) can improve the robustness of the fine-tuned model, defined by its Lipschitz constant \((L_{d}+L_{0})\), which equals \(L_{0}\) when \(L_{d}=0\). Therefore, the fine-tuned model can achieve a similar level of robustness as the pre-trained model if \(L_{d}\) is minimized. Colloquially, given two inputs \((,^{})\), where \(^{}\) is a perturbed version of \(\), \(h_{f}()\) will be just as sensitive/robust to the perturbation as \(h_{0}()\) is if \(L_{d}\) is small.

**Weight Space.** The definition of fine-tuning robustness (Eq. 9) not only leads to an interpretation of robustness in the feature space (Lemma 1) but also conveniently a _projection_ operation in the weight space. Specifically, we investigate a single linear layer in a neural network and show that enforcing the inequality in Eq. 9 leads to a projection operation by virtue of linear operators and matrix norms. We illustrate this in the following lemma with a full discussion and proof in Appendix 8.2.

**Lemma 2**.: _Assuming linear models \(h()=+,^{n m },\,^{n}\), and both the input space vector norm \(\|\|_{}\) and the feature space vector norm \(\|\|_{}\) are defined by \(l_{}\) norm. \(_{p}^{i}\) satisfies the inequality in Eq. 9 if_

\[_{p}^{i}=(1,}{\|_{f}^{i}-_{0 }^{i}\|_{1}})(_{f}^{i}-_{0}^{i})+_{0}^{i},  i\{1,...,n\}.\] (10)

_where \(^{i}\) denotes the \(i\)-th row of the matrix \(\) and \(_{p}^{i}\) is the new projected fine-tuned model._

This is an equation of _projection_ between \(_{f}\) and \(_{0}\) defined by the MARS norm in the weight space for a single linear layer and is the projection operation used in FTP and prior works [9; 10] (Eq. 2). It indicates that we can choose an arbitrarily small \(L_{d}\) and enforce it through Eq. 10, potentially trading off fitting the downstream task and preserving robustness. In summary, this section demonstrates the connection between robustness and projection. Specifically, we have shown that to achieve good fine-tuning robustness, we can enforce a small Lipschitz constant \(L_{d}\) on the difference function \( h()\)**in the feature space** (Lemma 1), which can be physically enforced through the projection of the fine-tuned model towards the pre-trained model **in the weight space** (Lemma. 2).

## 4 Experiments

**Overview.** To validate the effectiveness of FTP in fine-tuning pre-trained models, we benchmark FTP on both image classification (Sec. 4.1) and dense vision tasks (Sec. 4.2) with different network architectures and pre-trained models. For each benchmark, we report both in-distribution (ID) performance as well as out-of-distribution (OOD) performance. We show that FTP not only achieves competitive ID performance and superior OOD performance but is also much more computationally efficient than prior works. We further test FTP's regularization capability on a continual learning benchmark and show state of art performance against recent SOTA methods (Sec. 4.3).

### Image Classification Experiments

#### 4.1.1 DomainNet

For the DomainNet experiment (image classification), which consists of five domains, Real, Sketch, Painting, Infographics, and Clipart, we follow the setup of the prior work  and use its released code to train FTP. Specifically, we use two pre-trained models, an ImageNet pre-trained MOCO-V3 ResNet50  and a CLIP pre-trained ResNet50 . For FTP, we only tuned the learning rate while keeping the other hyper-parameters fixed as in the prior work. We use the Real domain as the ID training dataset and the rest as OOD testing datasets. Please refer to Appendix 8.4 for more details.

**FTP achieves the best OOD accuracy and is much more efficient.** In Tab. 1 and Tab. 2, we show results training on 100\(\%\) DomainNet-Real data using CLIP and MOCO-V3 pre-trained initialization respectively. Compared to the previous SOTA methods TPGM , FTP achieves competitive ID accuracy and better OOD generalization performance. _More importantly, in addition to favorable results, FTP is \(36\%\) faster on average on both benchmarks compared to TPGM._ Following TPGM , we also report results training only on \(10\%\) DomainNet-Real data in Appendix Tab. 6.

#### 4.1.2 ImageNet

Recently, zero-shot language-vision pre-trained models such as CLIP  have demonstrated strong generalization capability to other tasks. Notably, WISE  showed that linear interpolation between a fine-tuned model and its initialization achieves significant improvement in OOD generalization.

    & ID &  &  \\  & Real & Sketch & Painting & Infographics & Clipart & OOD Avg. & ID \(\) (\%) & OOD \(\) (\%) & Time (\%/t)\({}_{}\) \\  Vanilla FT & 81.99 (0.03) & 34.52 (0.33) & 42.89 (0.53) & 18.51 (0.24) & 44.98 (0.24) & 34.47 & 0.00 & 0.00 & 0.35 \\ Linear Prob. & 73.01 (0.03) & 24.10 (2.39) & 39.60 (0.15) & 12.27 (0.03) & 38.38 (0.26) & 26.58 & -19.96 & -22.90 & 0.10 \\ Partial Fusion  & 78.27 (0.03) & 27.02 (0.03) & 39.74 (0.02) & 15.56 (0.03) & 38.18 (0.12) & 30.30 & -4.55 & -12.11 & 0.21 \\
1.2.5BP  & 81.51 (0.31) & 34.91 (0.22) & 44.56 (0.16) & **1.897** (0.11) & 45.29 (0.13) & 36.23 & -0.59 & 5.09 & 0.46 \\ MARS-SP  & **81.89** (0.01) & 34.44 (0.24) & 45.45 (0.15) & 19.97 (1.48) & 44.66 (1.29) & 36.45 & -0.13 & 5.74 & 0.43 \\ LP-F  & **82.92** (0.01) & 34.50 (0.22) & 45.42 (0.31) & 20.12 (0.18) & **47.11** (0.27) & 36.79 & **1.13** & 6.22 & - & \\
**TPGM**  & **82.66** (0.13) & 35.35 (0.36) & 46.20 (0.20) & **20.13** (0.12) & **45.75** (0.12) & **36.86** & 0.82 & 6.91 & 0.80 \\  FTP & **82.17** (0.20) & **36.26** (0.06) & **46.58** (0.10) & **20.67** (0.03) & **46.97** (0.06) & **37.62** & 0.22 & **9.13** & 0.51 \\   

Table 1: DomainNet Results using MOCO-V3 pre-trained ResNet50 with 100\(\%\) Real Data. FTP achieves the best OOD performance and is much faster than prior work TPGM  by **36\(\%\)**.

    & ID &  &  \\  & Im & ImV2 & Im-A & Im-R & Im-S & OOD Avg. & Avg. \\  zero-shot & 67.68 & 61.41 & 30.60 & 56.77 & 45.53 & 48.58 & 52.40 \\ vanilla FT & 83.66 & 73.82 & 21.40 & 43.06 & 45.52 & 46.98 & 54.29 \\ Linear Prob. & 78.25 & 67.68 & 26.54 & 52.57 & 48.26 & 48.76 & 54.66 \\ LP-F  & 82.99 & 72.96 & 21.08 & 44.65 & 47.56 & 46.56 & 53.85 \\ L2-SP  & 83.44 & 73.2 & 20.55 & 43.89 & 46.60 & 46.06 & 53.54 \\ FTP & **84.19** & **74.64** & **26.50** & **47.23** & **50.23** & **49.65** & **56.56** \\  WISE-FT  & 80.94 & 72.47 & 33.18 & **63.33** & 54.20 & 55.58 & 60.82 \\ WISE-FT & **82.61** & **74.09** & **34.56** & 61.18 & **55.06** & **56.22** & **61.50** \\   

Table 3: ImageNet Fine-tuning Result using CLIP ViT-Base.

Figure 3: ImageNet WISE Interpolation  Result using CLIP ViT-Base Fine-tuned models.

    & ID &  &  \\  & Real & Sketch & Painting & Infograph & Clipart & OOD Avg. & ID \(\) (\%) & OOD \(\) (\%) & Time (\%/t)\({}_{}\) \\  Vanilla FT & 80.93 (0.03) & 31.81 (0.06) & 41.02 (0.100) & 20.29 (0.03) & 43.91 (0.15) & 34.18 & 0.00 & 0.00 & 0.58 \\ Linear Prob. & 52.56 (0.09) & 20.05 (0.21) & 24.92 (0.24) & 19.18 (0.04) & 21.15 (0.18) & 21.33 & -35.05 & -37.60 & 0.14 \\ Partial Fusion  & 82.71 (0.17) & 36.77 (0.14) & 42.43 (0.35) & 24.71 (0.15) & 43.31 (0.53) & 36.73 & -3.29 & 7.46 & 0.33 \\
1.2-SP  & 82.07 (0.03) & 36.67 (0.11) & 45.62 (0.35) & 29.97 (0.24) & 47.78 (0.30) & 38.26 & 1.40 & 11.94 & 0.62 \\ MARS-SP  & 77.19 (0.05) & 25.33 (0.07) & 33.42 (0.26) & **1.481** (0.14) & **39.20** (0.04) & **28.19** & -4.62 & -17.53 & 0.61 \\ LP-F  & 80.82 (0.95) & 34.85 (1.93) & 44.03 (0.05) & 22.23 (0.01) & 46.13 (2.34) & 36.81 & -0.14 & 7.69 & - \\
**TPGM**  & 83.64 (0.01) & **38.78** (0.42) & 43.11 (0.25) & **28.70** (0.31) & **48.01** (0.25) & 39.65 & 3.34 & 16.01 & 1.07 \\  FTP & **84.22** (0.11) & 37.66 (0.45) & **46.11** (0.29) & **28.33** (0.33) & 47.67 (0.18) & **39.94** & **4.05** & **16.87** & 0.68 \\   

Table 2: DomainNet Results using CLIP pre-trained ResNet50 with 100\(\%\) Real Data. FTP achieves competitive OOD performance and is much faster than prior work TPGM  by **36\(\%\)**.

However, there are two limitations: 1) not all pre-trained models have this property of _linear connectivity_ and 2) a zero-shot classifier head is needed to initialize the linear classifier head. _Our contribution is orthogonal to WISE because FTP is a general optimization algorithm whereas WISE is a post-training algorithm for specific zero-shot models._ Therefore, we first compare FTP to vanilla fine-tuning and then apply WISE to both models. We follow the public code base of DEIT  to train our CLIP pre-trained ViT-Base. Specifically, we use weight-decay (\(0.1\)), drop-path (\(0.2\)) , label-smoothing (\(0.1\)) , Mixup (\(0.8\))  and Cutmix (\(1.0\)) . We train our model on ImageNet and report OOD performance on ImageNet-V2 , ImageNet-A , ImageNet-R , and ImageNet-S . Please refer to Appendix 8.4 for more details on implementation.

**FTP outperforms vanilla fine-tuning and improves WISE performance.** In Tab. 3, we report performance for competing methods. Even with various regularizations and augmentations in place, FTP can further improve ID performance on ImageNet. Furthermore, FTP brings better OOD performance on all four OOD datasets. This shows that FTP successfully maintains the robustness of the pre-trained CLIP model while existing regularization such as weight decay and drop-path do not. We also report the interpolation results using WISE  for the vanilla fine-tuned and FTP fine-tuned models. We sweep a range of interpolation ratios \(\{0.1,0.2,...,0.9\}\) and show the trajectory of ID vs. OOD performance plot in Fig. 3. The models with the best average performance are reported in the lower portion of Tab. 3. As expected, WISE interpolation significantly improves OOD generalization for both methods. However, WISE-FTP has significantly better ID performance while still having better OOD performance. This shows that improvement to the base fine-tuning strategy can further benefit pose-training methods such as WISE.

### PASCAL Dense Vision Task Experiments

To further demonstrate the effectiveness of FTP in more diverse scenarios, we test it on PASCAL-Context . Specifically, following the prior work , we use the PASCAL-Context datasets , which consist of labels for semantic segmentation, human parts segmentation, and surface normal estimation. For OOD performance, following the popular natural robustness literature , we report results on various degradations including fog, defocus blur, Gaussian noise, and brightness corruption, with 5 severity each. We use a combination of Swin ViT-Tiny  (pre-trained on ImageNet-22K) and Segformer . In this architecture, Swin Transformer serves as the feature extraction backbone and Segformer is the task-specific decoder. While the feature backbone is initialized with pre-trained weights, a significant part of the entire model (the Segformer decoder) is randomly initialized; In contrast, in simple classification (Sec. 4.1.1), only the last linear classification layer is randomly initialized. Please refer to Appendix 8.5 for details.

**FTP achieves the best ID performance and OOD generalization.** We report results for semantic segmentation, human parts segmentation, and surface normal estimation in Tab. 4, Appendix Tab. 7, and Appendix Tab. 8 respectively. We additionally add Layer-Wise Learning Rate decay  ( LLRD) as a strong baseline. Notably, in all three tasks, FTP outperforms vanilla fine-tuning on ID performance by \(11.71\%\), \(4.48\%\), and \(18.30\%\) respectively. This demonstrates the effectiveness of projection as a regularization technique for transfer learning. More importantly, the OOD performance improves as large as \(33.02\%\) in semantic segmentation. This shows that 1) FTP can effectively maintain the robustness of the original pre-trained model; 2) even though the entire decoder component is randomly initialized, it is worthwhile to put regularizations on the pre-trained feature backbone.

   & ID &  &  \\  & Clean & Fog & Defocus & Gaussian & Brightness & OOD Avg. & ID \(\) (\%) & OOD \(\) (\%) & Time (s/ft) \\  Vanilla FT & 66.03 (0.37) & 57.62 (0.38) & 38.04 (0.33) & 22.21 (0.965) & 58.03 (0.66) & 44.00 & 0.00 & 0.288 \\ Adapter  & **71.85** (0.006) & 69.36 (0.07) & **50.94** (0.25) & **37.43** (0.64) & 68.26 (0.08) & 56.50 & 8.82 & 28.40 & 0.233 \\ BiFu  & 70.31 (0.11) & 67.00 (0.24) & 46.39 (0.35) & **30.61** (0.51) & 66.22 (0.16) & 52.56 & 6.49 & 19.44 & 0.248 \\ I.2-SP  & 73.47 (0.06) & 68.97 (0.09) & 49.20 (0.34) & 39.10 (0.84) & 68.61 (0.24) & 56.70 & 11.27 & 28.85 & 0.347 \\ MARS-SP  & 66.24 (0.23) & 65.97 (0.09) & 37.29 (1.20) & 21.82 (0.26) & 58.27 (0.33) & 43.59 & 0.32 & -0.94 & 0.318 \\ LLRD  & 72.09 (0.06) & 68.13 (0.25) & 46.18 (1.30) & 37.28 (2.54) & 66.30 (0.29) & 54.47 & 9.18 & 23.79 & 0.289 \\ TPGM  & 72.56 (0.06) & 69.51 (0.57) & 50.88 (0.97) & 36.62 (1.04) & 68.82 (0.25) & 56.96 & 9.89 & 29.44 & 0.611 \\  FTP & **73.79** (0.10) & **71.10** (0.23) & **52.63** (0.75) & **40.25** (0.21) & **69.81** (0.49) & **58.45** & **11.76** & **32.83** & 0.401 \\  

Table 4: Pascal Semantic Segmentation Results using SWIN-Tiny transformers pre-trained on ImageNet21K. Performance is measured by mIoU\(\). FTP achieves the best OOD performance and is much faster than prior work TPGM  by **34\(\%\)**.

### Continual Learning (CL) Experiments

Recently, pre-trained models have been shown to greatly improve the performance of CL algorithms . We follow the settings in this work  to partition ImageNet-R (200 classes) into 10 sequential tasks with 20 non-overlapping classes in each task. A model is trained on each task only once sequentially. To use FTP for CL tasks, unlike supervised vision tasks (Sec. 4.1, 4.2), we re-initialize FTP after each task and use the _current_ model as the "pre-trained model" for the next task. Moreover, inspired by the prior work , we use FTP to only fine-tune the attention blocks. We report both the final task accuracy across all tasks \(A_{1:N}\) and the global forgetting \(F_{N}\) in Tab. 5 to analyze _plasticity_ and _forgetting_. Please refer to Appendix 8.6 for more on the metrics and experimental setup. In Table 5, we benchmark against the popular and recent rehearsal-free continual learning methods. FTP alone achieves state of art accuracy against all methods and relatively good forgetting compared to vanilla FT, a sign of superior _plasticity_ and balanced _forgetting_. We visualize the learned constraints for each task in Fig. 4. We observe that while each task is independent and FTP is re-initialized each time, FTP learns stronger regularization for later tasks. This contributes to lower forgetting compared to FT. _We found that FTP combined with a simple continual learning method, EWC , achieves state-of-the-art in this setting_. Compared to the prompting methods L2P, DualPrompt, and the recent CODA-Prompt, FTP has clear and significant improvements. Our intuition is that the combination of the superior plasticity of FTP and the low forgetting of EWC is the key to the improvement.

## 5 Limitations

Like any regularization method, FTP has a hyper-parameter to adjust its regularization strength. In this case, the positive gradient annealing factor \(0 1\) (default \(1\)) (Sec. 3.2) controls the strength of projection with smaller values indicating weaker regularization. Note that \(=0\) means that the projection constraints are _non-decreasing_ during training. In this case, FTP still provides regularization. For example, we found that a \(=0\) is necessary to obtain the best performance for some dense vision tasks in Appendix 8.5. Generally, we recommend starting with the default \(\) and only tuning it if underfitting is observed.

## 6 Conclusion

In this paper, we proposed Fast Trainable Projection, a fine-tuning algorithm to maintain the robustness and the generalization capability of the pre-trained model. FTP learns projection constraints for each layer in a neural network efficiently by carefully re-using past information to save computation. To understand the connection between robustness and projection, we provided a holistic discussion of fine-tuning robustness from its feature space definition to the weight space dual. The new perspective lends a mathematical foundation to the idea of using projection in fine-tuning. Across four vision tasks with different pre-trained models, FTP demonstrated superior ID and OOD generalization capability and significantly better computation efficiency. Furthermore, the continual learning experiments demonstrated FTP's potential in other deep learning paradigms beyond simple fine-tuning. Combined with its compatibility with popular optimization algorithms, we believe FTP can be broadly beneficial in improving the performance of learning tasks using pre-trained initialization.

Figure 4: Average Learned Constraints for each task using FTP.

   Method & \(A_{1:N}\) (\(7\)) & \(F_{N}\) (\(i\)) \\  FT+ & \(48.93 1.15\) & \(9.81 0.31\) \\ LwFMC  & \(66.73 1.25\) & \(3.52 0.39\) \\ L2P++  & \(71.66 0.64\) & \(1.78 0.16\) \\ DualPrompt  & \(71.32 0.62\) & \(1.71 0.24\) \\ CODA-P  & \(75.45 0.56\) & \(1.64 0.10\) \\ EWC  & \(64.66 2.04\) & \(1.55 0.25\) \\ L2  & \(76.06 0.65\) & \(1.68 0.16\) \\  FTP & \(76.06 0.35\) & \(2.27 0.18\) \\ FTP + EWC & \(\) & \(\) \\   

Table 5: CL Results on ImageNet-RAcknowledgements

This work was supported by ONR grant N00014-18-1-2829.