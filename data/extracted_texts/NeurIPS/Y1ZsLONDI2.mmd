# Soft ascent-descent as a stable and flexible alternative to flooding

Matthew J. Holland

Osaka University

&Kosuke Nakatani

Osaka University

Corresponding author.

###### Abstract

As a heuristic for improving test accuracy in classification, the "flooding" method proposed by Ishida et al. (2020) sets a threshold for the average surrogate loss at training time; above the threshold, gradient descent is run as usual, but below the threshold, a switch to gradient _ascent_ is made. While setting the threshold is non-trivial and is usually done with validation data, this simple technique has proved remarkably effective in terms of accuracy. On the other hand, what if we are also interested in other metrics such as model complexity or average surrogate loss at test time? As an attempt to achieve better overall performance with less fine-tuning, we propose a softened, pointwise mechanism called SoftAD (soft ascent-descent) that downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect of flooding, with no additional computational overhead. We contrast formal stationarity guarantees with those for flooding, and empirically demonstrate how SoftAD can realize classification accuracy competitive with flooding (and the more expensive alternative SAM) while enjoying a much smaller loss generalization gap and model norm.

## 1 Introduction

Modern machine learning makes use of sophisticated models that are trained through optimization of non-convex objective functions, which typically admit numerous local minima that make for natural candidates when taken at face value. While many such candidates are indeed essentially "optimal" from the viewpoint of classification error rates or other average losses incurred at training time, these often turn out to be highly sub-optimal in terms of _performance at test time_. It goes without saying that understanding and closing this gap is the problem of "generalization" that underlies most machine learning research (Jiang et al., 2020; Dziugaite et al., 2020; Johnson and Zhang, 2023).

When we are faced with multiple candidates which are essentially optimal and thus indistinguishable in terms of some "base" objective function (e.g., the average loss) at training time, one of best-known heuristics for identifying good candidates is that of the "landscape" or "geometry" of the base objective in a neighborhood around each candidate. Roughly speaking, one expects that candidates in regions which are in some sense "flat" (often said to be less "sharp") tend to perform better at test time. Strictly speaking, flatness is not necessary for generalization (Dinh et al., 2017), but our intuition can often be empirically verified to be correct, as good generalization is regularly observed in flat regions where the eigenvalues of the Hessian are mostly concentrated near zero (Chaudhari et al., 2017). The spectral density of the Hessian can in principle be used to evaluate sharpness, and has well-known links to norms that can be used for explicit regularization (Karakida et al., 2019), but for large-scale neural network training in practice, first-order approximations have shown the greatest utility. In particular, the sharpness-aware minimization (SAM) algorithm of Foret et al. (2021), extended for scale invariance by Kwon et al. (2021) and later captured as a special case of the gradient norm penalization (GNP) scheme of Zhao et al. (2022), has shown state-of-the-artperformance on a variety of deep learning tasks. All of these first-order procedures can be cast as (forward) finite-difference approximations of the curvature (Karakida et al., 2023), requiring at least double the computational cost of vanilla gradient descent (GD) at each iteration.

As an alternative approach, the "flooding" technique of Ishida et al. (2020) is worthy of attention for surprising improvements in test accuracy despite its apparent simplicity. Flooding is done as follows: fix a threshold \(\) before training and run vanilla GD until the average loss goes below \(\), and while below this threshold, run gradient _ascent_ rather than descent (see SS2.1 for details). Flooding appeared before SAM in the literature, but near the threshold, flooding can iterate between optimizing the empirical risk and the squared gradient norm (penalizing sharpness), establishing the former as an inexpensive alternative to the latter. On the other hand, it is not at all obvious how the flooding threshold \(\) should be set given a particular data distribution and model class, and at present there is no methodology for settings which are "optimal" or at least "sufficient" from the viewpoint of test accuracy. More importantly, what if we are interested in performance criteria going beyond that of classification accuracy? Flooding just says "make the average loss as close to \(\) as possible," and we hypothesize that this requirement is too weak to encourage low model complexity and/or good generalization in terms of losses, while also keeping test accuracy high.

In this work, we investigate the validity of this hypothesis, and consider the impact of making a stronger requirement, namely to ask the algorithm to "make sure the loss distribution is _well-concentrated_ near \(\)." We show in SS3 that this can be implemented by introducing a smooth wrapper, applicable to any loss, which penalizes both over-performing and under-performing examples in a per-point fashion, instead of applying a hard threshold to the whole (mini-)batch as in flooding. We call this proposed procedure "soft ascent-decent" (SoftAD), and provide a detailed comparison with the flooding technique, highlighting the smoothness of SoftAD with implications in terms of formal stationarity guarantees, and emphasize how our mechanism leads to update directions that are qualitatively distinct from those used in flooding. Through rigorous empirical tests using both simulated and real-world benchmark classification datasets, featuring neural networks both large and small, we discover that compared with ERM, SAM, and flooding, the proposed SoftAD achieves far and away the smallest generalization error in terms of the base loss, while maintaining competitive accuracy and small model norms, without any explicit regularization.

Before diving into the main results just described, we introduce notation and background concepts in SS2. SoftAD is introduced in SS3, where we make basic empirical comparisons to flooding in SS3.1, and contrast formal guarantees of stationarity for these two methods in SS3.2. Our main empirical test results are given in SS4, with discussion and concluding remarks wrapping up the paper in SS5. All detailed proofs and supplementary empirical results are relegated to the appendix.

## 2 Background

To begin, we formulate performance metrics characterizing the problem of interest. Let \(^{d}\) parameterize our hypothesis class, let \(\) denote the set to which individual data points \(z\) belong. Let \(\) represent a random (test) data point with distribution \(\) over \(\). For classification, where our data takes the form \(=(,)\) and for each \(w\) we let \(}(w)\) denote the predicted label given \(\), the traditional performance metric of interest is the error probability at test time, denoted by

\[(w)\{}(w) \}.\] (1)

When we refer to test _accuracy_, we mean the probability of correct prediction, namely \(1-(w)\). Even when high accuracy is desired, it is standard to make use of computationally congenial surrogate loss functions for training. Let \(:^{d}\) denote a generic loss function to be used for training. For example, if \(z=(x,y)\) represents (input, label) pairs, then a typical choice of \(\) would be the cross-entropy loss. While the model is left abstract in our notation, note that for non-linear models such as neural networks, the mapping \(w(w;z)\) may be non-convex and non-smooth over \(\). As with the error probability (1), the expected loss (or "risk")

\[_{}(w)_{}\,(w;)\] (2)

is also an important indicator of classifier performance. Both (1) and (2) are ideal quantities since \(\) is unknown and \(\) is not observed at training time. We assume the learning algorithm has access to an independent sample of \(n\) observations from \(\), denoted by \(_{n}(_{1},,_{n})\) for convenience. Traditional machine learning algorithms are driven by the empirical risk, denoted here by \(_{n}(w)(1/n)_{i=1}^{n}(w;_{i})\), in that they seek out (local) minima of \(_{n}\). In this paper, we use the term _empirical risk minimization_ (_ERM_) to refer to algorithms that directly apply an optimizer to \(_{n}\). Under sophisticated models, the usual ERM objective \(_{n}()\) tends to admit a complex landscape. As discussed in SS1, numerous alternatives to ERM have been proposed, with the aim of minimizing \(()\) more reliably; next we take a closer look at one such technique, called "flooding."

### Flooding

The basic intuition underlying the proposal of Ishida et al. (2020) is that while minimizing \(_{n}()\) may be sufficient for maximizing the training accuracy, it need not be _necessary_, and from the perspective of optimizing \(()\), it may be worth it to sacrifice \(_{n}()\) and even \(_{n}()\). Fixing a threshold \(\), the "flooded" objective is

\[_{n}(w;)+|_{n}(w)-|.\] (3)

This objective can be implemented as a simple wrapper around typical loss functions, to which off-the-shelf gradient-based optimizers can be applied; running vanilla sub-gradient descent yields a sequence \((w_{1},w_{2},)\) generated using the update

\[w_{t+1}=w_{t}-(_{n}(w_{t})- )_{n}(w_{t})\] (4)

for all \(t 1\), where \((x) x/|x|\) for all \(x 0\) and \((0) 0\), and \(>0\) is a fixed step size. The update (4) characterizes what we call the _Flooding_ algorithm. From the above definitions, \(_{n}(w;)\) is minimal if and only if \(_{n}(w)=\). That is, Flooding seeks out \(w\) such that the training loss distribution induced by \(((w;_{1}),,(w;_{n}))\) has a mean which is close to \(\); nothing more, nothing less.

### Links to sharpness

Assuming for now that the loss is differentiable, it has been appreciated for some time that the _distribution_ of the loss gradients \((w;)\) can convey important information about generalization (Zhang et al., 2020), and in particular the role of gradient regularization, both implicit and explicit, is receiving significant attention (Barrett and Dherin, 2021; Smith et al., 2021).2 As a concrete example of explicit regularization, consider modifying the ERM objective using the squared Euclidean norm as

\[}_{n}(w;)_{n}(w)+\|_{n}(w)\|^{2}\] (5)

where \( 0\) controls the degree of penalization. If one is to minimize this objective in \(w\) directly using gradient descent, this involves computing

\[}_{n}(w;)=_{n}(w)+ ^{2}_{n}(w)(_{n}(w))\]

and thus doing matrix multiplication using a \(d d\) Hessian \(^{2}_{n}(w)\), an unattractive proposition when \(d\) is large. A linear approximation to the expensive term can be obtained via

\[_{n}(w+au)-_{n}(w)}{a}^{2 }_{n}(w)(u)\]

where \(u^{d}\) is arbitrary and \(|a|\) is small; see Zhao et al. (2022, SS3.3) for example. Applying this to approximate \(}_{n}(w;)\), we have

\[_{n}(w)+(_{n}(w+a _{n}(w))-_{n}(w))}_{n}(w;).\] (6)

The iterative update directions used by the _SAM_ algorithm of Foret et al. (2021) are captured by setting \(a=\), offering a nice link between loss-based sharpness control and gradient regularization. The extension of SAM in GNP (Zhao et al., 2022) can be expressed by an analogous derivation, replacingthe squared norm \(\|\|^{2}\) in (5) with \(\|\|\). Using updates of the form given in (6) with \(a>0\) is called a "forward" finite-difference (FD) approach to explicit gradient regularization (GR) (henceforth, \(\)-\(\)), and clearly requires two gradient calls per update.3 Better precision is available using "centered" FD, at the cost of additional gradient calls (Karakida et al., 2023). How does this all relate to Flooding? In repeating the update (4), once the empirical risk \(_{n}\) goes below \(\) and the algorithm switches from ascent to descent, it is straightforward to show conditions where this leads to iteration between minimizing \(_{n}\) and \(\|_{n}(w)\|^{2}\). We give more details in SSB.1.

## 3 Soft ascent-descent

With the context of SS2 in place, we consider making two qualitative changes to the Flooding update (4), described as follows.

1. **Pointwise thresholds:** Invert the order of applying \(_{n}()\) and \(()\), i.e., do summation over data points _after_ per-loss truncation.
2. **Soft truncation:** Replace the hard threshold \(()\) with a continuous, bounded, monotonic (increasing) function \(()\) satisfying \((0)=0\).

The reason for making the thresholds pointwise is to allow the algorithm to view "ascent" and "descent" from the perspective of individual losses (rather than bundled up in \(_{n}\)), making it possible to utilize a sum of both ascent and descent update directions.4 To make this sum a _weighted_ sum, the soft truncation using \(\) plays a key role. Keeping \(\) bounded limits the impact of errant loss values, while the other assumptions allow for both ascent and descent, with "borderline" points near the threshold given _less_ weight. Written explicitly as an empirical objective function, we use

\[_{n}(w;)+_{i=1}^{n}((w; _{i})-)\] (7)

where once again \(\) is a fixed threshold, and we set \((x)+1}-1\). Running vanilla GD on \(_{n}(;)\) in (7) yields the update

\[w_{t+1}=w_{t}-_{i=1}^{n}((w_{t};_{i})- )(w_{t};_{i}),\] (8)

with \((x)^{}(x)=x/+1}\), and \(>0\) is once again a fixed step size. For convenience, we use _soft ascent-descent_ (or _SoftAD_ for short) to refer to the algorithm implied by the iterative update (8). Note that there is nothing particularly special about our choice of \(\) here; it is just a simple algebraic function whose derivative also takes a simple form; note that the function \(\) resulting from our choice of \(\) satisfies the desired properties of continuity, boundedness, monotonicity, and \((0)=^{}(0)=0\).5 Note that for each point being summed over, \((w_{t};_{i})>\) implies descent while \((w_{t};_{i})<\) implies ascent, and borderline points with \((w_{t};_{i})\) have a smaller relative impact. In contrast with Flooding, SoftAD requires that the training loss distribution induced by \(((w;_{1}),,(w;_{n}))\) be well-concentrated around \(\), where the degree of dispersion is measured in a symmetric fashion using \(\).

_Remark 1_ (Comparison with other variants of Flooding).: During the review phase for this work, we were made aware of another recent related work by Xie et al. (2022). Their proposed method is known as _iFlood_, and it is essentially a middle ground between our proposal above and the original Flooding procedure. They use pointwise thresholds as we do in SoftAD, but retain the hard ascent-descent switch as in Flooding. More concisely, replacing our soft truncated \((x)\) in (8) with the absolute value \(|x|\) yields the iFlood update. We have added empirical test results for iFlood to complement our original experiments at the end of SSC.2. Another very recent variant is _\(AdaFlood\)_(Bae et al., 2023), which sets the \(\) threshold level individually for each point based on "difficulty" as evaluated using an auxiliary model.

_Remark 2_ (Difference from OCE-like criteria).: At first glance, our objective \(_{n}(w;)\) in (7) may appear similar to the criteria used in OCE risk minimization (Lee et al., 2020; Li et al., 2021) and some varieties of DRO (Duchi and Namkoong, 2021). Aside from the obvious difference that \(\) is fixed, rather than optimized alongside \(w\), the critical difference here is that our \(()\) is _not_ monotonic. Losses which are too large and too small are _both_ penalized. It is precisely this bi-directional property that allows for switching between ascent and descent; this is impossible to achieve with monotonic OCE and DRO risks (Holland, 2023; Holland and Tanabe, 2023; Hu et al., 2023; Royset, 2024). This bi-directional criterion can also be used as a straightforward method to provably avoid unintentional "collapse" into ERM solutions (Holland, 2024).

### Initial comparison with Flooding

To develop some intuition for our SoftAD (8) and the Flooding update (4), we carry out a few illustrative numerical experiments. To start, let us consider a simple, non-stochastic example in one dimension. Letting \(f:\) be some differentiable function, we consider how the transformed gradient \((f(x)-)f^{}(x)\) behaves under \(=\) and \(=^{}\). In Figure 1, we give a numerical example using a quadratic function. The softened nature of the transformed gradients used in SoftAD is clear when compared with the hard switching mechanism underlying the Flooding update. In Figure 2, we continue the quadratic function example, looking at sequences \((x_{1},x_{2},)\) generated based on the Flooding and SoftAD procedures. That is, instead of \(n\) points from which to compute losses, we have just one "loss," namely \(f(x)=x^{2}/2\). Both procedures realize an effective "flood level" of sorts (i.e., a buffer around the loss minimizer), but as expected, the Flooding procedure tends to be far more "jagged" in its trajectory.

Figure 1: The left-most figure simply plots the graph of \(f(x)=x^{2}/2\) over \(x[-2,2]\). The two remaining figures show plots of the graphs of \(f^{}(x)=x\) (dashed black line) and \(((f(x)-)/)f^{}(x)\) for the same range of \(x\) values, with colors corresponding to modified values of \(\) (middle plot; \(=0.5\) fixed) and \(\) (right-most plot; \(=1.0\) fixed) respectively. Thick dotted lines are \(=\), thin solid lines are \(=^{}\).

Figure 2: Gradient descent on the quadratic example from Figure 1. The horizontal axis denotes iteration number, and we plot sequences of iterates \((x_{t})\) and function values \((f(x_{t}))\) for each method. Here “GD” denotes vanilla gradient descent, with “Flood” and “SoftAD” corresponding to (4) and (8) respectively. Step size is \(=0.1\).

Finally, a simple example to illustrate how the per-point soft thresholding of SoftAD leads to distinct gradient-based update directions when compared to the Flooding strategy. Here we consider a dataset of \(n\) data points \(z_{1},,z_{n}^{2}\), and use the squared Euclidean norm as a loss, i.e., \((w;z)=\|w-z\|^{2}\). This is a natural extension of the quadratic example in the previous paragraph, to multiple points and two dimensions. In Figure 3, we look at two candidate points, and compute the Flooding and SoftAD update directions that arise at each candidate under a randomly generated dataset. We can clearly see how the Flooding update plunges directly towards the minimizer of \(_{n}\), unless it is too close (given threshold \(\)), in which case it goes in the opposite direction. In contrast, the SoftAD update is composed of per-point update directions, some which attract toward the minimum, and some which repel from the minimum, with borderline points down-weighted (shorter update arrows). Since the final update averages over these, movement both toward and away from the minimum is clearly "softened" when compared with the Flooding updates.

### Comparison of convergence properties

With the Flooding and SoftAD methods in mind, next we consider concrete conditions under which stochastic gradient-based learning algorithms can be given guarantees of (approximate) stationarity. Throughout this section, we assume that the loss \(w(w;z)\) is locally Lipschitz on \(\) for each \(z\), but convexity will not be used. Let us assume for simplicity that \((_{1},_{2},)\) is a sequence of independent random variables with distribution \(\), the same as our test data point \(\).

Starting with SoftAD, we are interested in procedures fuelled by stochastic gradients of the form

\[_{t}(w)((w;_{t})-)(w; _{t})\] (9)

for all integers \(t 1\) and \(w\), with threshold \(\) fixed in advance, and \(=^{}\) as before. Recalling the empirical SoftAD objective \(_{n}\) in (7), the underlying population objective is

\[_{}(w)+_{}\,((w;)- ).\] (10)

By design, we do not expect SoftAD to approach a stationary point of the original \(_{}\). Note that under mild assumptions on the data distribution, we have an unbiased estimator with \(_{}\,_{t}(w)=_{}(w)\), suggesting stationarity in terms of \(_{}()\) as a natural goal. Assuming the losses are \(L_{}\)-smooth in expectation, one can readily confirm that the objective (10) is \(L_{}\)-smooth, with

\[L_{}_{}[_{w}\| (w;)\|^{2}]+L_{}.\] (11)

A more detailed derivation is given in SSB.5. Assuming that second-order moments are finite in a uniform sense over \(\) ensures that \(L_{}<\), and naturally implies pointwise variance bounds, allowing us to seek out stationarity guarantees using a combination of gradient norm control and momentum in the fashion of Cutkosky and Mehta (2021).

Figure 3: _Left:_ We randomly sample \(n=8\) points (black dots) from the 2D Gaussian distribution, zero mean, zero correlations, with standard deviation \(2\) in each coordinate. The two candidates are denoted by square-shaped points (red and green), and the minimizer of \(_{n}\) is given by a gold star. _Center:_ The Flooding updates (colored arrows) via (4) for each candidate. _Right:_ Analogous SoftAD update vectors via (8), with per-point transformed gradients (semi-transparent arrows) for reference. Throughout, we have fixed \(=1.5_{w}_{n}(w)\) and \(=0.75\).

**Proposition 3** (Stationarity for SoftAD, smooth case).: _Starting with an arbitrary \(w_{1}\), update using \(w_{t+1}=w_{t}-_{t}/\|_{t}\|\), where \(_{t}:=b_{t-1}+(1-b)}_{t}(w_{t})\) for \(t 1\), with \(_{0}:=0\) and \(}_{t}()_{t}()\{1,/ \|_{t}()\|\}\), taking each gradient \(_{t}()\) based on (9). Assuming we make \(T-1\) updates, set the momentum parameter to \(b=1-1/\), the norm threshold to \(=}-L_{})/(1-b)}\), and the step size to \(=1/T^{3/4}\). The stationarity of this sequence \((w_{1},w_{2},)\), assumed to be in \(\), in terms of the modified objective (10) can be bounded by_

\[_{t=1}^{T}_{}(w_{t})}(_{}(w_{1})-_{}(w_{T+1})+}}{2}+2}-L_{}}(1+C_{}))\]

_using confidence-dependent factor \(C_{}:=10(3T/)+4+1\), with probability no less than \(1-\) over the draw of \((_{1},_{2},)\)._

For comparison, we next consider stationarity of the Flooding algorithm in the same context of smooth losses. The argument used in Proposition 3 critically depends on the smoothness of the underlying objective (10). Unfortunately, this smoothness cannot be leveraged when we consider the population objective underlying the Flooding procedure, namely the function

\[w+|_{}(w)-|.\] (12)

Further complicating things is the fact that stochastic gradients of the form (9) with \(()\) replaced by \(()\) do not yield unbiased sub-gradient estimates for (12), but rather for an upper bound \(+_{}|(w;)-|\) that follows via Jensen's inequality. A lack of smoothness means we cannot establish stationarity in terms of (12) nor the upper bound just given, but it is possible using a smoothed approximation (the Moreau envelope) of this bound:

\[}_{}(w)_{v}[+ _{}|(v;)-|+ v-w^ {2}].\] (13)

The parameter \(>0\) controls the degree of smoothness. The objective in (13) can be linked to "gradients" in (9) with \(=\), and leveraging the Lipschitz continuity of \(||\) along with a sufficiently smooth loss, it is possible to show that this non-smooth objective satisfies a weak form of convexity, and using the techniques of Davis and Drusvyatskiy (2019) it is possible to show that stochastic gradient algorithms enjoy stationarity guarantees, albeit not in terms of the objective (12), but rather the smoothed upper bound (13).

**Proposition 4** (Stationarity for Flooding, smooth case).: _Letting \(\) be closed and convex, take an initial point \(w_{1}\), and make \(T-1\) updates using \(w_{t+1}=_{}[w_{t}-_{t}(w_{t})]\), where \(_{}[]\) denotes projection to \(\), and each \(_{t}()\) is computed using (9) with \(=\). Assuming the loss \(w(w;z)\) is \(L_{}^{*}\)-smooth on \(\) for all \(z\), and taking a step size of_

\[^{2}=^{*}(L_{}-L_{})},}_{}(w_{1})-_{w}}_{}(w),\]

_with \(L_{}\) and \(L_{}\) as in (11), the expected squared stationarity in terms of the smoothed upper bound (13) at smoothness level \(=1/(2L_{}^{*})\) can be controlled as_

\[_{t=1}^{T}}_{}(w_{ t})^{2}^{*}(L_{}-L_{})}{T}}\]

_with expectation taken over the draw of \((_{1},_{2},)\)._

_Remark 5_ (Comparing rates and assumptions).: Considering the preceding Propositions 3 and 4, one common point is that learning algorithms based on both the SoftAD and Flooding gradients (of mini-batch size \(1\)) can be shown to be approximately stationary in terms of functions of a similar form (i.e., (10) and (12)), differing only in how they measure deviations from the threshold \(\). The rates of decrease (as a function of \(T\)) are essentially the same, noting that the bounds in Proposition 4 are in terms of _squared_ norms. That said, a lack of smoothness means the Flooding guarantees only hold for a smoothed variant, plus they require a stronger form of smoothness in the loss (over all \(z\) vs. in expectation). In addition, the SoftAD guarantees hold with high probability over the data sample, and can be readily strengthened to hold for an individual iterate (instead of summing over \(T\) iterates), using for example the technique of Cutkosky and Mehta (2021, Thm. 3).

## 4 Empirical study

In this section, we apply the proposed SoftAD procedure to a variety of classification tasks using neural network models, leading to losses that are non-convex and non-smooth. Our goal here is to compare and contrast the behavior and performance (accuracy, average loss, model norm) of SoftAD with three natural alternatives: ERM, Flooding, and SAM.6

### Overview of experiments

Our core experiments are centered around re-creating the tests done by Ishida et al. (2020, SS4.1, SS4.2) and Foret et al. (2021, SS3.1) to include all four methods of interest. There are two main parts: simulation-based tests and real benchmark-based tests. We briefly describe the setup of each below.

Non-linear binary classification on the planeWe use three synthetic data-generators ("two Gaussians," "sinusoid," and "spiral," see Figure 7) to create a dataset on the 2D plane that is not linearly separable, but separable using relatively simple non-linear models. We treat the underlying model as unknown, and approximate it using a shallow feedforward neural network. All four methods of interest (ERM, Flooding, SAM, and SoftAD) are driven by the Adam optimizer, with the cross-entropy loss used as the base loss function. Complete experimental details are provided in SSC.1.

Image classification from scratchOur second set of experiments utilizes four well-known benchmark datasets for multi-class image classification. Compared to the synthetic experiments, the classification task is more difficult (much larger inputs, variation within classes, more classes), and so we utilize more sophisticated neural network models to tackle the classification task. That said, as the sub-section title indicates, this training is done "from scratch," i.e., no pre-trained models are used. The datasets we use are all standard benchmarks in the machine learning community: CIFAR-10, CIFAR-100, FashionMNIST, and SVHN. Model choice essentially mirrors that of Ishida et al. (2020, SS4.2). For FashionMNIST, we flatten each image into a vector, and use a simple feed-forward neural network with one hidden layer. For SVHN, we use ResNet-18 as implemented in torchvision.models, without any pre-trained weights. Finally, for both CIFAR-10 and CIFAR-100, we use ResNet-34 (again in torchvision.models) without pre-training. For the optimizer, we use vanilla SGD with a fixed step size. Full details are given in SSC.2 in the appendix.

### Main findings

Uniformly small loss generalization gapOne of the most lucid results we obtained is that SoftAD shows the smallest _loss_ generalization gap of all the methods studied, across all models and datasets used. In Table 1, we show the gaps incurred under each dataset. More precisely, for each trial and each epoch, we compute the average cross-entropy loss on test and training (less validation) datasets, and then respective average both of these over all trials. The difference of these two values (i.e., trial-averaged test loss minus trial-averaged training loss) after the final epoch of training is the value shown in each cell of the table.

Balance of accuracy and loss on real dataIn the previous paragraph we noted that SoftAD has superior loss gaps, but this is not much to celebrate if performance in terms of the key metrics of interest (i.e., test loss and test accuracy) is poor. In Figures 4-5, we show the trajectory of loss and accuracy (for both training and test data) over epochs run (averaged over trials). All four methods

   & Gaussian & Sinusoid & Spiral & CIFAR-10 & CIFAR-100 & Fashion & SVHN \\   ERM & 0.080 & 0.150 & 0.297 & 3.265 & 7.603 & 0.801 & 0.762 \\  Flood & 0.011 & 0.058 & 0.119 & 1.239 & 3.114 & 0.436 & 0.436 \\  SAM & 0.024 & 0.096 & 0.154 & 1.512 & 3.672 & 0.639 & 0.493 \\  SoftAD & **0.004** & **0.016** & **0.087** & **1.168** & **2.701** & **0.422** & **0.362** \\  

Table 1: Generalization gap (test - training) for trial-averaged cross entropy loss after final epoch.

are comparable in terms of accuracy, with SAM (at double the gradient cost) coming out slightly ahead. On the other hand, there is significant divergence between the different methods in terms of test loss. For each dataset, SoftAD achieves a superior test loss, often converging faster than any of the other methods; this may be a natural by-product of the fact that SoftAD is designed to ensure losses are _well-concentrated_ around threshold \(\), instead of just asking that their mean get close to \(\) (as in Flooding). While there is clearly a major difference between ERM and the other three methods, the stable nature of the "double descent" in SoftAD is quite stark compared with Flooding and SAM.

Uniformly smaller model normsWe do not do any explicit model regularization (e.g., L2 norm penalization) in our experiments here, and we only use fixed step-size parameters for Adam and SGD, so as we run for many iterations, the norm of the model weight parameters tends to grow. While this property holds across all methods tested here, we find that under all datasets and models tested, SoftAD uniformly results in the smallest model norm; see Figure 6 for trajectories over epochs for each benchmark dataset. The "Model norm" values plotted here are the L2 norm of all the model parameters (neural network weights) concatenated into a single vector, and these norm values are averaged over trials. Completely analogous trends hold for the simulated datasets as well.

Trends in hyperparameter selectionAside from ERM, the three key methods of interest (Flooding, SAM, SoftAD) each have one hyperparameter. Flooding and SoftAD have the threshold \(\) as described

Figure 4: Trajectories over epochs for average test loss (top row) and test accuracy (bottom row). Horizontal axis is epoch number. Columns are associated with the CIFAR-10 and CIFAR-100 datasets (left to right).

Figure 5: Analogous to Figure 4, but with FashionMNIST and SVHN datasets.

in SS2-SS3, and SAM has the radius parameter (denoted by "\(\)" in the original paper). In all tests, we select a representative candidate for each method with hyperparameters by using validation data held out from the training data, and in Table 2 we show the average and standard deviation of the validation-based hyperparameters over randomized trials (see SSC for exact hyperparameter grid values). One clear take-away from this table is that the "best" value of \(\) (in terms of accuracy) for SoftAD tends to be _larger_ than that for Flooding, and this trend is uniform across all datasets, both simulated and real. In particular for the real benchmark datasets, it is interesting to note that while a larger threshold \(\) (applied to _loss_ distribution) is selected for SoftAD, the resulting test loss value achieved is actually smaller/better than that achieved by Flooding (top row of Figures 4-5).

## 5 Limitations and concluding remarks

While previous work had already shown that it is possible to sacrifice performance in terms of losses to improve accuracy, the nature of that tradeoff was left totally unexplored, and in SS1 we put forward the hypothesis that simply asking the empirical loss mean to get close to a non-zero threshold \(\), as in Flooding, would not be enough to realize a competitive tradeoff over varied learning tasks (datasets, models). Our main take-away is that we have empirical evidence that the slightly stronger requirement of "_losses well-concentrated around \(\)_" (implemented as SoftAD) can result in an appealing balance of average test loss and accuracy, with the added benefit of a strong (implicit) regularization effect, likely due to the soft dampening effect on borderline points. A more formal theoretical understanding of this regularization effect is of interest, as are empirical studies going far beyond the limited choice of loss functions used here. Our biggest limitation is that the question of "how to set the threshold \(\)?" still remains without an answer. Any meaningful answer will likely require some user judgement regarding tradeoffs between performance metrics. One potential first approach would be to leverage recent techniques for estimating the Bayes error (Ishida et al., 2023), combined with existing surrogate theory (Bartlett et al., 2006) to reverse-engineer a loss threshold given a user-specified "tolerable" drop in accuracy, for example.

   & Gaussian & Sinusoid & Spiral & CIFAR-10 & CIFAR-100 & Fashion & SVHN \\   Flood & 0.05 (0.06) & 0.05 (0.06) & 0.05 (0.06) & 0.05 (0.06) & 0.01 (0) & 0.01 (0) & 0.03 (0.05) \\  SAM & 0.36 (0.19) & 0.05 (0.06) & 0.05 (0.06) & 0.36 (0.19) & 0.5 (0) & 0.32 (0.16) & 0.28 (0.20) \\  SoftAD & 0.08 (0.08) & 0.05 (0.06) & 0.05 (0.06) & 0.08 (0.08) & 0.22 (0.14) & 0.03 (0.04) & 0.13 (0.10) \\  

Table 2: Hyperparameters selected by validation for each method (averaged over trials). Flooding and SoftAD have threshold \(\); SAM has radius parameter. Standard deviation (over trials) is given in small-text parentheses.

Figure 6: Model norm trajectories over epochs for each dataset in Figures 4–5.