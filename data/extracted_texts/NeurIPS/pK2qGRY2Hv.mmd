# The Limits of Transfer Reinforcement Learning with Latent Low-rank Structure

Tyler Sam

Cornell University

tjs355@cornell.edu &Yudong Chen

University of Wisconsin-Madison

yudong.chen@wisc.edu &Christina Lee Yu

Cornell University

cleeyu@cornell.edu

###### Abstract

Many reinforcement learning (RL) algorithms are too costly to use in practice due to the large sizes \(S,A\) of the problem's state and action space. To resolve this issue, we study transfer RL with latent low rank structure. We consider the problem of transferring a latent low rank representation when the source and target MDPs have transition kernels with Tucker rank \((S,d,A)\), \((S,S,d),(d,S,A)\), or \((d,d,d)\). In each setting, we introduce the transfer-ability coefficient \(\) that measures the difficulty of representational transfer. Our algorithm learns latent representations in each source MDP and then exploits the linear structure to remove the dependence on \(S,A\), or \(SA\) in the target MDP regret bound. We complement our positive results with information theoretic lower bounds that show our algorithms (excluding the (\(d,d,d\)) setting) are minimax-optimal with respect to \(\).

## 1 Introduction

Recently, reinforcement learning (RL) algorithms have exhibited great success on a variety of problems, e.g., video games , robotics , etc, that require a series of decisions over time. These algorithms can be used on any problem that can be modeled as a Markov decision process (MDP) with state space cardinality \(S\), action space cardinality \(A\), and horizon \(H\). However, RL algorithms' practical application is constrained due to the large amounts of data and computational resources required to train models that outperform heuristic approaches. This limitation arises due to the inefficient scaling of RL algorithms with respect to the size of the state and action spaces. In an online setting where rewards are incurred during the process of learning, we may want to minimize the regret, or the difference between the reward of an optimal policy and the reward collected by the learner. In the finite-horizon online episodic MDP setting with \(K\) episodes, any algorithm must incur regret of at least \((K})\). This regret bound is often too large in practice as many problems modeled as MDPs have large state and action spaces. For example, in the Atari games , the state space is the set of all images as a vector of pixel values.

One way to remove the dependence on the state or action space in RL algorithms is to leverage existing data or models from similar problems through transfer learning. In the transfer learning setting for MDPs, the learner can interact with or query from multiple source MDPs, which the learner can utilize at low-cost to improve their performance in the target MDP . The efficacy of the transfer learning approach depends both on the similarity between the source and the target MDPs, and how information is utilized when transferring knowledge between the source and target.

As feature learning in MDPs is difficult and expensive, reusing learned low rank representations can greatly improve the performance on new problems. Thus, in this work, we consider the setting when the source and target MDPs exhibit latent low rank structure, and the transfer RL task is to learn latent low rank representation of the state, action, or state-action pair from the source MDPs, and subsequently use it to improve learning on the target MDP. If the learned representations were perfect, then one could remove all dependence on the state space or action space in the learning task on thetarget MDP. This approach relies on a critical assumption that the transition kernels of the source and target MDPs have low Tucker rank when viewed as an \(S\)-by-\(S\)-by-\(A\) tensor.

The work  studied this problem when the transition kernel is low rank along the first mode, i.e., with Tucker rank \((d,S,A)\) where \(d<\{S,A\}\); this is also referred to as the Low Rank MDP model. We also consider this setting, as well as the complementary settings in which the transition kernel has low Tucker rank along the second or third mode, i.e., Tucker rank \((S,d,A)\) or \((S,S,d)\). In addition, we study the \((d,d,d)\) Tucker rank setting, where we can further exploit the low rank structure to improve performance in the target problem. (As we elaborate later, up to a factor of \(d\), the \((d,d,d)\) setting subsumes the remaining settings with Tucker ranks \((d,d,A)\), \((S,d,d)\) and \((d,S,d)\).)

The different modes are not interchangeable algorithmically as there is a directionality due to the time of the transition between the current and future state. Additionally, the algorithm in  is not computationally tractable as it utilizes an optimization oracle which may not be practical. They assume that the latent representations are within a known finite function class. Our work proposes a computationally efficient algorithm that can handle latent low rank structure on the transition kernel along any of the three different modes.

As the success of a transfer learning approach depends on the similarity between the source and target problems, we introduce the _transfer-ability coefficient_\(\), which quantifies the similarity between the source and target problems under our low rank assumptions. This coefficient, generalizing the task-relatedness in , captures the unique challenge in the transfer reinforcement learning setting.

Our Contributions:We propose new computationally efficient transfer RL algorithms that admit the desired efficient regret bounds under all low Tucker rank settings \((S,d,A)\), \((S,S,d),(d,S,A)\), and \((d,d,d)\). With enough samples from the source MDPs, we remove the dependence on \(S,A,\) or \(SA\) in the regret bound on the target MDP. In addition, we introduce the transfer-ability coefficient \(\) that measures the ease of transferring a latent representation from the source MDPs to target MDP. We establish information theoretic lower bounds showing that our algorithms are optimal with respect to \(\) (excluding the \((d,d,d)\) Tucker rank setting). In particular, in the \((d,S,A)\) Tucker rank setting, we achieve the optimal dependence of \(^{2}\) while  is sub-optimal with a dependence on \(^{5}\).

Table 1 summarizes our main theoretical results, suppressing the dependence on common matrix estimation terms, and compares them with the results from .1 In the appendix, we further consider the setting where one has access to generative models in both the source and target problems, and show that the benefits of transfer learning go beyond alleviating exploration burden.

## 2 Related Work

**RL in the \((d,S,A)\) Tucker rank Setting:** There exists an extensive literature on Linear MDPs and Low-rank MDPs, which correspond to the \((d,S,A)\) Tucker rank setting. Linear MDPs assume

    & Tucker Rank & Source Sample Complexity & Target Regret Bound \\  Theorem 2 & \((S,S,d)\) & \((d^{4}(1+A/S)M^{2}H^{4}^{2}T)\) & \((ST})\) \\ Theorem 10 & \((S,d,A)\) & \((d^{4}(S/A+1)M^{2}H^{4}^{2}T)\) & \((AT})\) \\ Theorem 14 & \((d,S,A)\) & \((^{2}dM^{2} TSA)\) & \((T})\) \\ Theorem 15 & \((d,d,d)\) & \((d^{10}(S+A)M^{2}H^{4}^{4}T)\) & \((M^{6}H^{3}T})\) \\  Theorem 3.1  & \((d,S,A)\) & \((^{5}SA^{5}H^{7}d^{5}M^{2}T)\) & \((H^{4}T})\) \\   

Table 1: Theoretical guarantees of our algorithms alongside results from literature in the different Tucker rank settings. See Table 3 for the regret bounds of algorithms in each Tucker rank setting with known latent representations.

that the dynamics are linear with respect to a known feature mapping \(\). This allows one to remove all dependence on the size of the state and action spaces and replace it with the rank \(d\) of the latent space [17; 37; 36; 34]. The work  proposes an algorithm that admits a regret bound of \((H^{2}T})\), matching the known lower bounds. Our algorithm in the target phase modifies existing linear MDP algorithms to take advantage of the knowledge learned in the source phase. Low Rank MDPs impose the same low-rank structure as linear MDPs but assume that \(\) is unknown. To avoid dependence on the size of the state space, many works in this setting construct algorithms that utilize a computationally inefficient oracle to admit sample complexity or regret bounds that scale with the size of the action space and the log of the size of the function class the true low rank representation [3; 33; 26; 40].  incurs sample complexity on \(SA\) instead of the size of the function class to learn near-optimal policies in reward free low rank MDPs.

**Transfer RL:** As feature learning in low rank MDPs is difficult, [4; 10; 23; 15] study representational transfer in low rank MDPs. The work most closely related to ours is , which performs reward-free exploration in the source problem to learn a sufficient representation to use in the target problem in the \((d,S,A)\) Tucker rank setting. They propose a task relatedness coefficient that captures the existing representational transfer RL settings . This coefficient is similar to our transfer-ability coefficient but differs as our low rank assumption is imposed on different modes of the probability transition tensor. Their algorithm admits a source sample complexity bound of \((A^{4}^{3}d^{5}H^{7}K^{2}T(||||))\) with a target regret bound of \((^{2}H^{4}T})\). Other transfer RL works [10; 23] require reach-ability assumptions to learn the latent representations in the source phase. Similar to transfer RL, reward free RL studies the problem of giving the learner access to only the transition kernel in the first phase, and in the second phase the learner must output a good policy when given multiple reward functions [19; 39; 11; 32]. While reward free RL is similar to our setting, enabling the dynamics to differ between the source and target phase greatly increases the difficulty and changes the objective in the source problem, i.e., learning a good latent representation instead of collecting trajectories that sufficiently explore the state space.

**RL in the \((S,d,A)\) or \((S,S,d)\) Tucker Rank Setting:** The work  introduces the RL setting in which the transition kernel has Tucker rank \((S,d,A)\) or \((S,S,d)\). Assuming access to a generative model, their algorithms learn near-optimal polices with sample complexities that scale with \(S+A\), thereby circumventing the \(SA\) lower bound for tabular reinforcement learning . The work  consider the same low Tucker rank assumption on the dynamics but in the offline RL setting. The works [38; 29; 28] empirically show the benefits of combining matrix/tensor estimation with RL algorithms on common stochastic control tasks.  consider the low-rank bandits setting and use a similar algorithm to ours by estimating the singular subspaces of the reward matrix to improve the regret bound's dependence on the dimension of the problem.

## 3 Preliminaries

We consider the transfer reinforcement learning setting introduced by , in which the learner interacts with \(M\) finite-horizon source MDPs and one finite horizon episodic target MDP. All MDPs share the same discrete state space \(\) with cardinality \(S\), discrete action space \(\) with cardinality \(A\), and finite horizon \(H_{+}\).2 Let \(r=\{r_{h}\}_{h[H]}\) be the deterministic unknown reward function of the target MDP, where \(r_{h}:\). For \(m[M]\), let \(r_{m}=\{r_{m,h}\}_{h[H]}\) be the deterministic unknown reward function for the \(m\)th source MDP, where \(r_{m,h}:\).3 Let \(=\{P_{h}\}\) be the transition kernel for the target MDP, where \(P_{h}(s^{}|s,a)\) is the probability of transitioning to state \(s^{}\) from state-action pair \((s,a)\) at time step \(h\). Similarly, for \(m[M]\), \(_{m}\) is the transition kernel for \(m\)th source MDP and defined in the same way as \(\). An agent's policy has the form \(=\{_{h}\}_{h[H]},_{h}:()\), where \(_{h}(a|s)\) is the probability that the agent chooses action \(a\) at state \(s\) and time step \(h\).

The value function and action-value (\(Q\)) function of a policy \(\) are the expected reward of following \(\) given a starting state and starting action, respectively, beginning at time step \(h\). We define these as

\[V_{h}^{}(s)_{t=h}^{H}r_{t}(s_{t},_{t}(s_{ t}))|s_{h}=s, Q_{h}^{}(s,a)_{t=h}^{H}r_{t}(s_{ t},a_{t})|s_{h}=s,a_{h}=a\]where \(a_{t}_{t}(s_{t}),s_{t+1} P_{t}(|s_{t},a_{t})\). The optimal value function and action value function are \(V^{*}_{h}(s)_{}V^{n}_{h}(s)\) and \(Q^{*}_{h}(s,a)_{}Q^{n}_{h}(s,a)\) for all \((s,a,h)[H]\). They satisfy the Bellman equations \(V^{*}_{h}(s)=_{a}Q^{*}_{h}(s,a),Q^{*}_{h}(s,a)=r_{h}(s,a)+E_{s ^{} P_{h}(|s,a)}[V^{*}_{h+1}(s^{})]\) for all \((s,a,h)[H]\). We define an \(\)-optimal policy \(\) for \(>0\) as any policy that satisfies \(V^{*}_{h}(s)-V^{*}_{h}(s)\) for all \(s,h[H]\). Similarly, \(Q\) is \(\)-optimal if \(Q^{*}_{h}(s,a)-Q_{h}(s,a)<\) for all \((s,a,h)[H]\). Also, we use the shorthand \(P_{h}V(s,a)=_{s^{} P_{h}(|s,a)}[V(s^{})]\).

In the transfer RL problem, the learner first interacts with the \(M\) source MDPs without access to the target MDP. In this phase, called the source phase, the learner is given access to a generative model in each of the source MDPs. The generative model takes in a state-action pair \((s,a)\) and time step \(h\) and outputs \(r_{h}(s,a)\) and an independent sample from \(P_{h}(|s,a)\). Access to a generative model in the source phase is necessary; in particular, it has been shown in [4, Theorem 4.1] that without access to a generative model in the source phase, one cannot learn a near-optimal policy in the target phase using the learned representation. Then, in the next phase, called the target phase, the learner loses access to the source MDPs and interacts only with the target MDP in a standard online model, without access to a generative model. The performance of the learner is evaluated by two metrics: i) the regret in the target MDP, i.e., \(Regret(T)=_{k[K]}V^{*}_{1}(s)-V^{^{k}}_{1}(s)\), where \(^{k}\) is the learner's policy in episode \(k\), and ii) the number of samples taken in the source phase.

### The Tucker Rank of a Tensor

For the reader's convenience, we first recall the standard definition of the Tucker rank of a tensor.

**Definition 1** (Tucker Rank ).: _A tensor \(M^{n_{1} n_{2} n_{3}}\) has Tucker rank \((d_{1},d_{2},d_{3})\) if \((d_{1},d_{2},d_{3})\) is the smallest such that there exists a core tensor \(X^{d_{1} d_{2} d_{3}}\) and matrices \(G_{i}^{n_{i} d_{i}}\) for \(i\) with orthonormal columns such that_

\[M(a,b,c)=_{i[d_{1}],j[d_{2}],k[d_{3}]}X(i,j,k)G_{1}(a,i)G_{2}(b,j )G_{3}(c,k).\]

We note in passing that all our sample complexity and regret bounds in Table 1 remain valid when \(d\) is an upper bound on the exact rank. In standard MDPs, the transition kernel can have Tucker rank \((S,S,A)\) as there is no low rank structure imposed on the tensor. In this work, we investigate the benefits of assuming that the transition kernel is low rank along each mode/dimension of the transition kernel, which corresponds to the \((S,d,A),(S,S,d)\), and \((d,S,A)\) Tucker rank settings. To illustrate the difference in these three settings, we consider \(d=1\) and present the corresponding structure of the transition kernel in Equations (1), (2), and (3), respectively:

\[(S,1,A) P(s^{}|s,a)=(s) (s^{},a),\] (1) \[(S,S,1) P(s^{}|s,a)=(a) (s^{},s),\] (2) \[(1,S,A) P(s^{}|s,a)=(s^{}) (s,a),\] (3)

for some functions \(,\) defined on the appropriate domains. Figure 1 pictorially displays the \((S,S,d)\) Tucker rank assumption on the transition kernel. While the three settings may seem similar, the \((S,d,A)\) and \((S,S,d)\) Tucker rank settings differ significantly from the \((d,S,A)\) Tucker rank setting (i.e., Low Rank MDP). In particular, the \((d,S,A)\) setting assumes low rank structure across time, i.e., the current state and action vs. the next state, while the other Tucker rank assumptions impose low rank structure between the current state and action. These differences are significant as each setting requires different algorithms in the target problem and construction for the lower bounds.

Additionally, we analyze the \((d,d,d)\) Tucker rank setting, where the transition kernel fully factorizes, in which case the next state, current state, and action all have separate low rank representations. This assumption allows us to further exploit the low rank structure to improve the regret bound in the target phase when compared to the \((S,d,A)\) and \((S,S,d)\) Tucker rank settings. Note that this \((d,d,d)\) Tucker rank assumption is equivalent to imposing low rank along any two modes of the transition kernel, up to factors of \(d\) in the sample complexity and regret bounds. For example, for an \((S,d,d)\) Tucker rank transition kernel \(P\), one can factor the \(S d d\) core tensor into a \(S d\) orthonormal matrix and \(d^{2} d d\) core tensor, which implies \(P\) has Tucker rank \((d^{2},d,d)\).

While the sample complexity and regret bound of each algorithm differ in each Tucker rank setting, one should choose which algorithm to use based on the low rank structure inherent in their problem. For example, in the \((S,S,d)\) Tucker rank setting, one assumes that each action has a low rank representation. One potential application of this setting is a personal movie recommender system. Specifically, the states are user's different states of mind while each action chooses a movie from a large collection. As movies in the same genre evoke similar responses, each action can be mapped to a lower dimensional space with the axis being each genre. In contrast, each state of mind is distinct. In the \((S,d,A)\) Tucker rank setting, one assumes that the state one transitions from has a low rank representation. The \((d,d,d)\) Tucker rank setting combines the two previous assumptions and asserts that current states and actions have separate low dimensional representations while low rank MDPs with \((d,S,A)\) Tucker rank assume that each state-action pair has a joint low rank representation. For sake of brevity, we will only present our assumption and algorithm for the \((S,S,d)\) Tucker rank setting in the main body of this work and defer the other settings to the appendix.

### Low Tucker Rank MDPs

We first define the incoherence \(\) of a matrix, which is used in the regularity conditions of our Tucker rank assumption.

**Definition 2** (Incoherence).: _Let \(X^{m n}\) have rank \(d\) with singular value decomposition \(X=U V^{}\) with \(U^{m d},V^{n d}\). Then, \(X\) is \(\)-incoherent if for all \(i[m],j[n]\), \(\|U(i,:)\|_{2}\) and \(\|V(j,:)\|_{2}\)._

A small \(\) guarantees that the signal in \(U\) and \(V\) is sufficiently spread out among the rows in those matrices. The notion of incoherence is pervasive in the low-rank estimation literature [7; 8].

Introduced in , we assume that reward functions are low rank and the transition kernels have low Tucker rank in each of the source and target MDPs. The main benefit of Assumption 1 is that for any value function \(V\), the Bellman update of any value function, \(Q_{h}=r_{h}+P_{h}V\), is a low rank matrix due to Proposition 4.

**Assumption 1**.: _In each of the \(M\) source MDPs, the reward functions have rank \(d\), and the transition kernels have Tucker rank \((S,S,d)\). The target MDP has reward function with rank \(d^{}\) and transition kernels with Tucker rank \((S,S,d^{})\) where \(d^{} dM\). Thus, there exists \(S S d\) tensors \(U_{m,h},\)\(S S d^{}\) tensors \(U_{h}\)s, \(A d\)\(\)-incoherent matrices \(G_{m,h}\) with orthonormal columns, \(A d^{}\)\(\)-incoherent matrices \(G_{h}\) with orthonormal columns, and \(S d\) matrices \(W_{m,h},S d^{}\) matrices \(W_{h}\) such that_

\[P_{m,h}(s^{}|s,a)=_{i[d]}G_{m,h}(a,i)U_{m,h}(s^{ },s,i), r_{m,h}(s,a)=_{i[d]}G_{m,h}(a,i)W_{m,h}(s,i),\] \[P_{h}(s^{}|s,a)=_{i[d^{}]}G_{h}(a,i)U_{h}(s^{ },s,i), r_{h}(s,a)=_{i[d^{}]}G_{h}(a,i)W_{h}(s,i)\]

_where \(\|_{s^{} S}g(s^{})U_{m,h}(s^{},s,:)\|_{2}.\|_{s ^{} S}g(s^{})U_{h}(s^{},s,:)\|_{2},\|W_{h}(s)\|_{2},\|W_ {m,h}(s)\|_{2}\) for all \(s\), \(a\), \(h[H]\), and \(m[M]\) for any function \(g:S\)._

The latent representations of the reward function and transition kernel are not unique under our assumption. Figure 2 visualizes the low rank structure Assumption 1 imposes on the transition kernel. While Assumption 1 states that the source MDPs and target MDPs have latent low rank structure, the assumption does not guarantee that low rank representations are similar. Additionally, we allow target MDP to be more complex than a single source MDP (at most Tucker rank \((S,S,dM)\) compared to Tucker rank \((S,S,d)\), respectively). Thus, to allow for transfer learning, we assume that for each step \(h[H]\), the space spanned by the target latent factors is a subset of the space spanned by the latent factors from all the source MDPs.

**Assumption 2**.: _Suppose Assumption 1 holds. The target MDP latent factors \(G_{h}\) and source MDP latent factors \(G_{m,h}\) satisfy for all \(h[H]\), \((\{G_{h}(:,i)\}_{i[d^{}]})(\{G_{m,h }(:,i)\}_{i[d],m[M]})\)._

Under Assumption 2, it is possible that no single source MDP captures the target MDP, but the union of all source MDP does. It is also possible that the source MDPs span a strictly larger subspace than the target MDP.

Thanks to the above assumptions, by estimating the latent features \(\{G_{m,h}\}\) of the source MDPs, we can construct an approximation of the target features \(\{G_{h}\}\)**without interacting** with the target MDP, which is essential to transfer learning. Assumption 1 only upper bounds the rank of \(Q^{*}_{m,h}\) by \(d\). However, to learn each source latent factor, one needs to obtain a good estimate of a rank \(d\) matrix with at least one large entry. Therefore, we assume that the optimal \(Q\)-functions in each source MDP are full rank with the largest entry lower bounded by a constant.

**Assumption 3**.: _For each \(m[M],h[H]\), the optimal \(Q\)-function of the \(m\)-th source MDP at step \(h\) satisfies \(rank(Q^{*}_{m,h})=d\) and \(\|Q^{*}_{m,h}\|_{} C\) for some \(C^{+}\)._

This assumption allows one to learn the near-optimal \(Q\)-functions from noisy samples of the source MDPs. Assumption 3 and its variants are common in the literature of noisy low-rank estimation [2; 9]. Without the above assumption (the rank of \(Q^{*}\) is strictly less than \(d\)), then one cannot learn every latent factor \(G_{m,h}(:,i)\) since it may have a zero singular value, and thus using the approximate feature mapping would result in regret linear in \(T\). Similarly, \(\|Q^{*}\|_{} C\) ensures that some entries of \(Q^{*}\) are sufficiently large and prevents the noise from dominating the low rank signal that one learns in the source phase.

## 4 Transfer-ability

Our transfer-ability coefficient is motivated by the following observation. Under Assumption 2, we can construct each target latent factor \(G_{h}\) with a linear combination of the source latent factors \(G_{m,h}\). The magnitudes of the coefficients correspond to the difficulty in estimating the target latent factor because the estimation error of \(G_{m,h}\) is amplified by its coefficient. Thus, when these coefficients are too large, transfer learning is essentially impossible; one example is when the source latent factors are almost identical or parallel to each other while the target latent factor is orthogonal to the source latent factors (see Figure 2).

However, these coefficients are not unique and are a function of the source and target latent factors. Furthermore, as the source MDPs latent factors of \(Q^{*}_{m,h}\) and target latent factors \(\{G_{h}\}\) are not unique, each choice of latent factor results in a different set of coefficients. Thus, we introduce \(_{h}\) as the set containing all such coefficients given the subspaces from the target and source MDPs. Let \(_{S,h}\) be the set of all \(^{S d}\) orthonormal latent factor matrices with columns that span the column space of \(Q^{*}_{m,h}\) for all \(m[M]\), and let \(_{T,h}\) be the set of all \(^{S d^{}}\) orthonormal latent factor matrices with columns that span the column space of \(P_{h}(s^{})\) for all \(s^{}\). Then, we define the set \(_{h}(_{T,h},_{S,h})\) to contain all such coefficients, i.e., \(B_{h}^{d^{},d,M}\) and \(B_{h}_{h}(_{T,h},_{S,h})\) if there exists \(G_{h}_{T,h},G_{m,h}_{S,h}\) such that for all \(i[d^{}],h[H]\), \(G_{h}(,i)=_{j[d],m[M]}B_{h}(i,j,m)G_{m,h}(,j)\). Now, we define \(\), which precisely measures the challenge involved in transferring the latent representation.

**Definition 3** (Transfer-ability Coefficient).: _Given a transfer RL problem that satisfies Assumptions 1, 2, and 3, with the definition of \(_{h}\) above, we define \(\) as_

\[_{h[H]}_{B_{h}(_{T,h}, _{S,h})}_{i[d^{}],j[d],m[M]}|B(i,j,m)|.\]

Note that the minimum is taken over all latent factor matrices. When there is only one source MDP, \(\) is small, i.e., less than or equal to one, because the columns of \(G\) form an orthonormal basis of the space containing the target latent factor. In the best case, \(\) is \(1/(dM)\) (see Appendix D). On the other hand, when there are multiple source MDPs, \(\) can be arbitrarily large as in Figure 2; in this case, the estimation error of \(G_{1}\) and \(G_{2}\) are amplified by \(\) when estimating \(G_{T}\) using \(G_{1}\) and \(G_{2}\). We remark that adding more source MDPs or increasing the rank of \(Q^{*}_{m,h}\) will not increase \(\) as one can use the same set of coefficients without the additional source MDP or larger subspace of \(Q^{*}_{m,h}\). See Appendix D for more discussion on \(\).

Our transfer-ability coefficient is similar to the quantity \(_{}\) defined in . While these terms both quantify the similarity between the source and target MDPs, in our setting, we allow for the source MDPs' state latent factors, \(\{G_{m,h}\}\), to differ from target MDP's state latent factors while  assumes that all MDPs share the same latent representation \(\). Our setting is more general than the one in  even when assuming the same Tucker rank assumption.

### Information Theoretic Lower Bound

To formalize the importance of \(\), we prove a lower bound that shows that a dependence on \(\) in the sample complexity of the source phase is necessary to benefit from transfer learning. Before proving our main result, we first present an intermediate lower bound to provide intuition on how \(\) measures the difficulty in transferring a learned representation.

**Theorem 1**.: _There exist two transfer RL instances such that (i) they satisfy Assumptions 1 and 2, (ii) they cannot be distinguished without observing \((^{2})\) samples in the source phase, and (3) they have target action latent features that are orthogonal to each other._

Learning a latent representation \(G\) that is orthogonal to the true representation is no better than randomly guessing a representation \(G^{}\) in the source phase; using either \(G\) or \(G^{}\) in the target phase incurs regret linear in \(T\). Thus, Theorem 1 states that without observing \((^{2})\) samples in the source MDPs, one cannot learn a good policy in the target phase with \(G\) or \(G^{}\), and one should disregard the information from the source MDPs and use a non-transfer learning RL algorithm to achieve regret bound that scales with \(\). Therefore, \((^{2})\) samples are required to benefit from transfer learning.

To prove Theorem 1, we construct two \((S,S,1)\) transfer RL problems that satisfies Assumption 2 with similar source action latent representations \(G^{i}_{m,h}\) but orthogonal target action latent representations \(G^{i}_{T}\) for \(i\{1,2\}\). To avoid learning the incorrect representation, one must identify the transfer RL problem by differentiating the \(Q^{*}\)s. By construction, the \(Q^{*}\)s for the two transfer RL problems differ by at most \(1/\) entrywise, so one must observe \((^{2})\) samples in the source problem from standard hypothesis testing lower bounds . Observing less samples in the source problem results in an estimate orthogonal to the true target representation with constant probability. See Appendix E for the formal proof. One can easily prove a similar result in the \((S,d,A)\) and \((d,S,A)\) Tucker rank setting with similar constructions by switching the states and actions (see Appendices F and G).

## 5 Algorithm

We now present our algorithm that achieves the optimal dependence on \(\). In the source phase, our algorithm can use any algorithm that learns \(\)-optimal \(Q\) functions on each source MDP. As the error bound on the estimated feature representation depends on the incoherence of \(Q^{*}_{m,h}\), we use LR-EVI, which combines empirical value iteration with low rank matrix estimation . After obtaining estimates \(_{m,h}\) of \(Q^{*}_{m,h}\), our algorithm computes the singular value decompositions (SVDs) of each \(_{m,h}\) to construct the latent feature representation \(\) by scaling and concatenating the singular vectors. Then, in the target phase, our algorithm deploys a modification of LSVI-UCB  adapted to our Tucker rank setting called LSVI-UCB-(S, S, d) using the \(\). For ease of notation, we define \(T^{s}_{k,h}=\{k^{}[k]|s^{k}_{h}=s\}\), which is the set of episodes before \(k\), in which one was at \(s\) at time step \(h\).

```
1:\(\{N_{h}\}_{h[H]}\)
2:for\(m[M]\)do
3: Run LR-EVI(\(\{N_{h}\}_{h[H]}\)) on source MDP \(m\) to obtain \(_{m,h}\).
4: Compute the singular value decomposition of \(_{m,h}=_{m,h}_{m,h}_{m,h}^{}\).
5: Compute feature mapping \(}=\{}_{h}\}_{h[H]}\) with \(}_{h}=}[_{1,h} _{M,h}]\). ```

**Algorithm 1** Source Phase

LSVI-UCB-(S, S, d) uses the same mechanisms as LSVI-UCB, but we compute coefficients and Gram matrices for each action. Specifically, we update the weights \(w^{s}_{h}\) with the solution to the regularized least squares problem,

\[w^{s}_{h}_{w^{d}}_{t T^{s}_{k,h}}(r_{h}(s,a ^{t}_{h})+_{a^{}}Q_{h+1}(s^{t}_{h+1},a^{})-w^{} _{h}(a^{t}_{h}))^{2}+\|w\|_{2}^{2},\]

using the reward when \(s\) was observed and add an exploration bonus \(^{s}_{k,h}}_{h}(a)^{}(^{s}_{h})^{-1}}_{h}(a)}\) to our estimate of \(Q^{*}\), which is common in linear MDP/bandits algorithms [17; 1]. Multiplying the latent factors by \(\) to construct \(}\) ensures that the feature mapping and coefficients are of similar magnitude, which is crucial in adapting the mechanisms of LSVI-UCB to our setting.

While one may wonder why our modification of LSVI-UCB is necessary, Algorithm 2 improves the regret bound by a factor \(\) compared to using any linear MDP algorithm off the shelf; reducing our setting to a linear MDP results in using a feature mapping with dimension \(dS\). Thus, using any linear MDP algorithm results in a target regret bound of \((S^{2}H^{2}T})\). However, our algorithm shaves off a factor of \(\) as our algorithm computes \(S\) copies of \(d d\) dimensional gram matrices in contrast to the \(d^{2}S^{2}\) dimensional gram matrices used in linear MDP algorithms.

```
1:\(,^{s}_{k,h},}\)
2:for\(k[K]\)do
3: Receive initial state \(s^{k}_{1}\).
4:for\(h=H,,1\)do
5:for\(s S\)do
6: /* Compute Gram matrix \(^{s}_{h}\) for each state \(s\) */
7: Set \(^{s}_{h}_{t T^{s}_{k-1,h}}}_{h}(a^{t}_{h}) }_{h}(a^{t}_{h})^{}+\).
8: /* Estimate \(w^{s}_{h}\) via regularized least squares */
9: Set \(w^{s}_{h}(^{s}_{h})^{-1}_{t T^{s}_{k-1,h}}}_{h}(a^{t}_{h})[r_{h}(s,a^{t}_{h})+_{a^{} A}Q_{h+1}(s^{ t}_{h+1},a^{})]\).
10: /* Estimate \(Q^{*}\) via the low rank structure with optimism */
11: Set \(Q_{h}(,)H, w^{i}_{h},_{h}() +^{i}_{k,h}_{h}()^{}(^{i}_{h})^{-1} }_{h}()}\).
12:for\(h[H]\)do
13: Take action \(a^{k}_{h}_{a A}Q_{h}(s^{k}_{h},a)\), and observe \(s^{k}_{h+1}\). ```

**Algorithm 2** Target Phase: LSVI-UCB-(S, S, d)

## 6 Theoretical Results

In this section, we prove our main theoretical results that show with enough samples in the source problem, our algorithms admit regret bounds that are independent of \(S,A,\) or both \(S\) and \(A\) in the target problem. We first state our main result in the \((S,S,d)\) Tucker rank setting.

**Theorem 2**.: _Suppose Assumptions 1, 2, and 3 hold, and set \((0,1)\). Furthermore, assume that, for any \((0,)\), \(Q_{m,h}=r_{m,h}+P_{m,h}V_{m,h+1}\) has rank \(d\) and is \(\)-incoherent with condition number \(\) for all \(\)-optimal value functions \(V_{h+1}\). Let \(=1\) and \(^{s}_{k,h}\) be a function of \(d,H,|T^{s}_{k,h}|,|S|,M,T\). Then, for \(T}\), using at most \((d^{4}^{5}^{4}(1+A/S)M^{2}H^{4}^{2}T)\) samples in the source problems, our algorithm has regret \((ST})\) with probability at least \(1-\)._

Theorem 2 states that using transfer learning improves the performance on the target problem by removing the regret bound's dependence on the action space. Thus, with enough samples from the source MDPs one can recover a regret bound in the target problem that matches the best regret bound for any algorithm in the \((S,S,d)\) Tucker rank setting with **known** latent feature representation \(G\) concerning \(S\) and \(T\). See Appendix E for the proof of Theorem 2.

For sake of brevity, we present our results in our other Tucker rank settings in Table 1 (see Appendices F, G, and H for the formal statements and proofs). In each of our Tucker rank settings, our algorithms use transfer learning to remove the dependence on \(S,A,\) or \(SA\) in the target regret bound, which matches the dependence on \(S\) and \(A\) of the best regret bounds of algorithms with **known** latent representation. While the source sample complexities of Theorems 2 and 10 may seem to not scale linearly in \(A\) and \(S\), respectively, we require the horizon in the target phase to be large enough, i.e., \(T^{2}(S/A+1) S+A\) and \(T^{2}(1+A/S) S+A\), respectively, so that one observes at least \((S+A)\) samples in the source phase.

While our dependence on \(M\) in the target regret bound may seem sub optimal, our algorithm cannot distinguish which of the \(d\) dimensions (out of \(dM\) possible dimensions) match the ones in the target MDP without interacting with it. Thus, our algorithm must include all of them, which results in using at worst a \(dM\)-dimensional feature representation. However, in the case when the feature representations from source MDPs lie in subspaces that intersect, we can apply a singular value thresholding procedure to remove the unneeded dimensions used in the feature representation at the cost of an increased sample complexity (by a factor of \(A\)) in the source phase (see Appendix C).

**Comparison with :** In comparison to Theorem 3.1 from , our result, Theorem 14, has similar dependence on \(A,d,H,\) and \(T\) while our dependence on \(\) is optimal compared to their \(^{5}\) dependence 4. While our dependence on \(M\) in the regret bound is worse, this is due to our transfer learning setting being more general; we only require that the space spanned by the target features be a subset of the space spanned by the source features instead of having the target features being a linear combination of the source features. Furthermore, our source sample complexity scales with the size of the state space instead of scaling logarithmically in the size of the given function class.

The proofs of 2, Theorem 10, 15, and 14 synthesize the theoretical guarantees of LR-EVI, singular vector perturbation bounds, and LSVI-UCB or our modification of LSVI-UCB. Given \(N\) samples from the source MDPs, we first bound the error between the singular subspaces up to a rotation by \(M(S+A)/N}\) (suppressing the dependence on common matrix estimation terms) to construct our feature mapping. Thus, with enough samples in the source MDPs, LR-EVI returns \(Q\) functions with singular vectors that can be used to construct a sufficient feature representation; the additional regret incurred from using the approximate feature representation in LSVI-UCB or Algorithm 2 is dominated by the original regret of using the true representation.

### Discussion of Optimality

In the \((S,d,A),(S,S,d),(d,S,A),\) and \((d,d,d)\) Tucker rank settings, the source sample complexity's scalings concerning \(S,A,H,\) and \(T\) are reasonable when learning \(,,,\) and \(\)-optimal \(Q\) functions, respectively, given the \(Q\) learning lower bounds in . Similarly, the dependence on \(\) and \(\) are standard for RL algorithms incorporating matrix estimation.

In the \((S,d,A),(S,S,d),\) and \((d,S,A)\) Tucker rank settings, our \(^{2}\) dependence in the source sample complexity is optimal due to our lower bounds. In the \((d,d,d)\) Tucker rank setting, our \(^{4}\) dependence is likely optimal when eliminating the dependence on \(S\) and \(A\) in the target problem; one must combine both sets of singular vectors together, \(F:^{dM}\) and \(G:^{dM}\), to create a feature mapping \(:^{d^{2}M^{2}}\), which causes the \(^{4}\) dependence in our upper bound. However, if \(^{4}\) is too large, one can use the algorithms from the \((S,d,A)\) or \((S,S,d)\) Tucker rank settings to benefit from transfer learning. Proving an \(^{4}\) lower bound is an interesting open question. Additionally, the dependence on \(d\) and \(M\) in this setting is worse as our low rank latent representation of each state-action pair lies in a \(d^{2}M^{2}\) dimensional space in contrast to a space with dimension \(dM\).

The regret bounds of our algorithms used in the target phase, i.e., Algorithm 2, Algorithm 4, and LSVI-UCB , in each Tucker rank setting are optimal with respect to \(S,A,\) and \(T\). Since each Tucker rank setting captures tabular MDPs, we can construct lower bounds with a reduction from the bound in , e.g., in the \((S,S,d)\) setting, the regret is at least \((T})\). However, our dependence on \(d\) and \(H\) are sub-optimal. The extra factor of \(\) is likely due to our use of Hoeffding's inequality instead of tracking the variance of the estimates to use Bernstein's inequality. Our dependence on \(d\) is sub-optimal as LSVI-UCB is sub-optimal with respect to \(d\); for linear MDPs, LSVI-UCB++  admits a regret bound of \((H^{2}T})\), which matches the lower bound from . Thus, to obtain the optimal dependence on \(d\) and \(H\), we would need to adapt LSVI-UCB++ to our Tucker rank setting.

Conclusion

We study transfer RL, in which one transfers a latent representation between source MDPs and target MDP with Tucker rank \((S,S,d),(S,d,A),(d,S,A)\) or \((d,d,d)\) transition kernels. As  studied transfer RL in the \((d,S,A)\) Tucker rank setting, our study completes the analysis of representational transfer in RL assuming that one or more modes of the transition kernel is low rank. Furthermore, we propose the transfer-ability coefficient that quantifies the difficulty of transferring latent representations between the source and target problems with information theoretic lower bounds. We propose computationally simple algorithms that admit regret bounds that are independent of \(S,A\), or \(SA\) depending on the Tucker rank setting given enough samples from the source MDPs. Furthermore, these regret bounds match the bounds of algorithms that are given the true latent representation. Our dependence on \(d\) and \(H\) is not optimal and can be improved by adapting the methods in  to our setting, which is an interesting open problem. Furthermore, proving theoretical guarantees of other forms of transfer RL is worthwhile and an interesting future direction.

Acknowledgements:Y. Chen is partially supported by NSF CCF-1704828 and NSF CCF-2233152.