# Multilinear Mixture of Experts:

Scalable Expert Specialization through Factorization

 James Oldfield\({}^{1}\)1 Markos Georgopoulos Grigorios G. Chrysos\({}^{2}\) Christos Tzelepis\({}^{3}\)

**Yannis Panagakis\({}^{4,5}\) Mihalis A. Nicolaou\({}^{6}\) Jiankang Deng\({}^{7}\) Ioannis Patras\({}^{1}\)**

\({}^{1}\)Queen Mary University of London \({}^{2}\)University of Wisconsin-Madison \({}^{3}\)City University of London \({}^{4}\)National and Kapodistrian University of Athens \({}^{5}\)Archimedes AI, Athena RC \({}^{6}\)The Cyprus Institute \({}^{7}\)Imperial College London

###### Abstract

The Mixture of Experts (MoE) paradigm provides a powerful way to decompose dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. However, a major challenge lies in the computational cost of scaling the number of experts high enough to achieve fine-grained specialization. In this paper, we propose the **M**ultilinear **M**ixture **of** Experts (\(\)**MoE**) layer to address this, focusing on vision models. \(\)MoE layers enable scalable expert specialization by performing an implicit computation on prohibitively large weight tensors _entirely in factorized form_. Consequently, \(\)MoEs (1) avoid the restrictively high inference-time costs of dense MoEs, yet (2) do not inherit the training issues of the popular sparse MoEs' discrete (non-differentiable) expert routing. We present both qualitative and quantitative evidence that scaling \(\)MoE layers when fine-tuning foundation models for vision tasks leads to more specialized experts at the class-level, further enabling manual bias correction in CelebA attribute classification. Finally, we show qualitative results demonstrating the expert specialism achieved when pre-training large GPT2 and MLP-Mixer models with parameter-matched \(\)MoE blocks at every layer, maintaining comparable accuracy. Our code is available at: https://github.com/james-oldfield/muMoE.

## 1 Introduction

The Mixture of Experts (MoE) architecture  has reemerged as a powerful class of conditional computation, playing the pivotal role in scaling up recent large language , vision , and multi-modal models . MoEs apply different subsets of layers (referred to as 'experts') for each input, in contrast to the traditional approach of using the same single layer for all inputs. This provides a form of input-conditional computation  that is expressive yet efficient. However, through their substantial performance gains, an important emergent property of MoEs is frequently underutilized: the innate tendency of experts to specialize in distinct subtasks. Indeed, the foundational work of Jacobs et al.  on MoEs describes this property, highlighting how implementing a particular function with modular building blocks (experts) often leads to subcomputations that are easier to understand individually than their dense layer counterparts-with larger expert counts allowing for more fine-grained specialization.

Independent of model performance, a successful decomposition of the layer's functionality into human-comprehensible subtasks offers many significant benefits. Firstly, the mechanisms through which a network produces an output are more _interpretable_: the output is a sum of modular components, each contributing individual functionality. Yet, the value of interpretable computationextends beyond just transparency  and explainability . An important corollary of successful task decomposition amongst experts is that layers are easier to debug and edit. Biased or unsafe behaviors can be better localized to specific experts' subcomputation, facilitating manual correction or surgery in a way that minimally affects the other functionality of the network. Addressing such behaviors is particularly crucial in the context of foundation models; being often fine-tuned as black boxes pre-trained on unknown, potentially imbalanced data distributions. Furthermore, there is evidence that traditional fairness techniques are less effective in large-scale models [15; 16]. However, to achieve fine-grained expert specialism at the class level (or more granular still), one needs the ability to significantly scale up the number of experts. When using only a small expert count, each expert is forced to process and generalize across _multiple_ distinct semantic concepts, hindering specialization. Conversely, a large expert count means each can specialize to a more specific set of semantically similar inputs. Alas, the dominating'sparse' MoE paradigm of selecting only the top-\(K\) experts  is not only parameter-inefficient for large expert counts, but also has several well-known issues due to its discrete expert routing-often leading to training instability and difficulties in scaling the total expert count, amongst other challenges [18; 19].

In this paper, we propose the _Multilinear Mixture of Experts_ (\(\)MoE) layer to address these issues. \(\)MoEs are designed to scale gracefully to dense operations involving _tens of thousands_ of experts at once through implicit computations on a factorized form of the experts' weights. Furthermore, in contrast to the dominant sparse MoEs'  non-differentiable nature, \(\)MoEs are differentiable by design, and thus do not inherit the associated training issues. We summarize the benefits of \(\)MoEs' model form over existing MoEs in Table 1. Crucially, we show evidence that scaling up the number of \(\)MoE experts leads to increased expert specialism when fine-tuning foundation models for vision tasks. Our evidence is provided in three forms: (1) firstly, through the usual qualitative evaluation of inspecting inputs by their expert coefficients. Secondly (2), we further explore the _causal_ role of each expert through counterfactual interventions . Lastly, (3) we show how final-layer \(\)MoE expert specialism facilitates the practical task of model editing-how subcomputation in specific combinations of experts biased towards demographic subpopulations can be manually corrected through straightforward guided edits.

Building on these findings, we demonstrate that \(\)MoEs offer a compelling alternative to MLPs for pre-training both vision and language models with up to \(100\)M parameters-enabling large numbers of specialized experts while maintaining comparable performance and parameter counts to the original networks' _single_ dense MLPs.

Our contributions and core claims can be summarized as follows:

* We introduce \(\)MoE layers-a mechanism for computing vast numbers of subcomputations and efficiently fusing them conditionally on the input.
* We show both qualitatively (through visualization) and quantitatively (through counterfactual intervention) that _increasing the number of \(\)MoE experts increases task modularity_-learning to specialize in processing just specific input classes when fine-tuning large foundation models for vision tasks. Further, we show manual editing of \(\)MoE expert combinations can straightforwardly mitigate demographic bias in CelebA attribute classification.
* We pre-train both language (GPT2) and vision (MLP-mixer) \(\)MoE networks, establishing experimentally that models with parameter-matched \(\)MoE blocks are competitive with existing MLP blocks whilst facilitating expert specialism (qualitatively) throughout.

## 2 Related Work

Mixture of ExpertsRecent years have seen a resurgence of interest in the Mixture of Experts (MoE) architecture for input-conditional computation [17; 12; 21; 2]. One primary motivation for MoEs is their increased model capacity through large parameter count [17; 4; 2]. In contrast to a single dense layer, the outputs of multiple experts performing separate computations are combined (sometimes with multiple levels of hierarchy [22; 23]). A simple approach to fusing the outputs is by taking either a convex  or linear  combination of the output of each expert. The

   &  &  \\  & **Differentiable** & **efficient** & **efficient** \\  Dense MoE  & & & & \\ Sparse MoE  & & & & \\  \(\)**MoE (ours)** & & & & \\  

Table 1: Benefits of the proposed \(\)MoEs’ model form over existing MoEs.

seminal work of Shazeer et al.  however proposes to take a _sparse_ combination of only the top-\(K\) most relevant experts, greatly reducing the computational costs of evaluating them all. More recent works employ a similar sparse gating function to apply just a subset of experts [2; 25], scaling to billions  and trillions of parameters . The discrete expert selection choice of sparse MoEs is not without its problems, however-often leading to several issues including training stability and expert under-utilization [18; 19].

Particularly relevant to this paper are works focusing on designing MoE models to give rise to more interpretable subcomputation [26; 27; 28]-hearkening back to one of the original works of Jacobs et al. , where experts learned subtasks of discriminating between different lower/uppercase vowels. Indeed a common observation is that MoE experts appear to specialize in processing inputs with similar high-level features. Researchers have observed MoE experts specializing in processing specific syntax  and parts-of-speech  for language models, and foreground/background  and image categories (e.g. 'wheeled vehicles')  in vision. Evidence of shared vision-language specialist is even found in the multi-modal MoEs of Mustafa et al. .

Several works instead target how to make conditional computation more efficient: by sharing expert parameters across layers , factorizing gating network parameters , or dynamic convolution operations . Relatedly, Gao et al.  jointly parameterize the experts' weight matrices with a Tensor-Train decomposition . However, such approach still suffers from the Sparse MoE's instability and expert under-utilization issues, and stochastic masking of gradients must be performed to lead to balanced experts. Furthermore, whilst Gao et al.  share parameters across expert matrices, efficient implicit computation of thousands of experts simultaneously is not facilitated, in contrast to the \(\)MoE layer.

Factorized layersin the context of deep neural networks provide several important benefits. Replacing traditional operations with low-rank counterparts allows efficient fine-tuning  / training [37; 38], and modeling of higher-order interactions [39; 40; 41; 42; 43], and convolutions . In addition to reducing computational costs, tensor factorization has also proven beneficial in the context of multi-task/domain learning [45; 46] through the sharing of parameters/low-rank factors across tasks. Furthermore, parameter efficiency through weight factorization often facilitates the design and efficient implementation of novel architectures such as polynomial networks [47; 48; 49] or tensor contraction layers . The recent DFC layer in Babiloni et al.  also performs dynamic computation using the CP decomposition  like \(\)MoEs. Nevertheless, the two works have very different goals and model properties due to how the weight matrices are generated. \(\)MoEs take a sparse, convex combination of \(N\) explicit experts' latent factors. This consequently leads to specialized subcomputations in a way that facilitates the interpretability and editability presented in this paper. DFCs can be seen to apply an MLP to input vectors at this step in analogy, which does not provide the necessary model properties of interest here.

## 3 Methodology

We first formulate the proposed \(\)MoE layer in Section 3.1, introducing 2 unique resource-efficient models and forward passes in Section 3.1.1. Finally, we show in Section 3.1.2 how \(\)MoEs recover linear MoEs as a special case.

NotationWe denote scalars \(x\) with lower-case letters, and vectors \(^{I_{1}}\) and matrices \(^{I_{1} I_{2}}\) in lower- and upper-case boldface latin letters respectively. Tensors \(^{I_{1} I_{2} I_{d}}\) of order \(d\) are denoted with calligraphic letters. We refer to the \((i_{1},i_{2},,i_{d})\)-th element of this tensor with both \((i_{1},i_{2},,i_{d})\) and \(x_{i_{1}i_{2} i_{d}}\). Finally, we use a colon to index into all elements along a particular mode: given \(^{I_{1} I_{2} I_{3}}\) for example, \(_{:i_{3}}^{I_{1} I_{2}}\) or equivalently \((:,:,i_{3})^{I_{1} I_{2}}\) is the matrix at index \(i_{3}\) of the final mode of the tensor. We use \(_{n}\) to denote the **mode-\(n\) (vector) product** of a tensor \(^{I_{1} I_{2} I_{N}}\) and vector \(^{I_{n}}\) whose resulting elements are given by \((_{n})_{i_{1} i_{n-1}i_{n+1} i_{N}}= _{i_{n}=1}^{I_{n}}x_{i_{1}i_{2} i_{N}}u_{i_{n}}\).

### The \(\)MoE layer

\(\)MoEs provide a scalable way to execute and fuse large numbers of operations on an input vector by formalizing conditional computation through resource-efficient multilinear operations. A \(\)MoE layer comprised of \(N\) many experts (and a single level of expert hierarchy) is parameterized by weight tensor \(^{N I O}\) and expert gating parameter \(^{I N}\). Given an input vector \(^{I}\) (denoting the hidden representation of an individual token, for example), its forward pass can be expressed through the series of tensor contractions:

\[ =(^{})^{N},\] \[ =_{1}_{2}\] \[=_{n=1}^{N}_{i=1}^{I}_{ni:}z_{i}a_{n} ^{O},\] (1)

where \(\) is the vector of expert coefficients and \(\) is the entmax activation . The \(\)MoE layer can be understood as taking a sparse, convex combination of \(N\) many affine transformations2 of input vector \(\), weighted by the coefficients in \(\). The first tensor contraction in the forward pass (\(_{i}_{::}z_{i}^{N O}\)) matrix-multiplies the input vector with _every_ expert's weight matrix. The following tensor contraction with expert coefficients a takes a linear combination of the results, yielding the output vector. The forward pass can be visualized intuitively as multiplying and summing over the modes in a 3D tensor, which we illustrate in Figure 1. Furthermore, \(\)MoEs readily generalize to hierarchical conditional computations by introducing additional modes to the weight tensor and corresponding vectors of expert coefficients (see Appendix E).

#### 3.1.1 Computation in factorized form

Our key insight is that the dense \(\)MoE forward pass over all \(N\) experts simultaneously can be **computed entirely in factorized form, needing never materialize prohibitively large weight tensors**. This allows \(\)MoEs' computations to scale gracefully to many thousands of experts simultaneously, without the problematic top-\(K\) gating . To achieve this, we (1) first parameterize the experts' weights \(^{N I O}\) with a tensor factorization and (2) re-derive fast forward passes of Equation (1) to operate solely in factorized form.

In the context of a \(\)MoE layer, the various choices of tensor factorizations make different trade-offs regarding parameter/FLOP counts and rank constraints. We derive two unique resource-efficient \(\)MoE variants to suit different computational budgets and choices of expert counts. We now present the derivations of the forward passes of the factorized \(\)MoE models (with einsum pseudocode implementations in Appendix B):

Cp\(\)MoEImposing CP structure  of rank \(R\) on the weight tensor, we can write \(=_{r=1}^{R}_{r}^{(1)}_{r}^{(2)} _{r}^{(3)}^{N I O}\) as a sum of \(R\) outer products, with factor matrices \(^{(1)}^{R N},^{(2)}^{R  I},^{(3)}^{R O}\). This reduces the parameter count from \(NIO\) (such as with sparse/dense MoEs and regular \(\)MoEs) to just \(R(N+I+O)\). Crucially, we can further rewrite the CP\(\)MoE layer's forward pass entirely in factorized form without ever materializing the full tensor (plugging the CP-composed tensor into Equation (1)) as:

\[=_{n=1}^{N}_{i=1}^{I}_{r=1}^{R}_{r}^{( 1)}_{r}^{(2)}_{r}^{(3)}_{ni:}z_{i}a_{n} =_{r=1}^{R}(^{(2)})_{r}(^{(1 )})_{r}_{r}^{(3)}^{O},\] (2)

with Equation (2) being analogous to the fast computation in Babiloni et al. , only here the operations of combining the weights and producing the outputs can be expressed in a single step. Whilst the original naive CP\(\)MoE forward pass has a FLOP count3 of \(NIO\), the fast computation

Figure 1: The forward pass of an (unfactorized) \(\)MoE layer as a series of tensor contractions: the experts’ weight matrices (yellow 2D slices) are matrix-multiplied with the input vector and summed (weighted by the red expert coefficients).

above has just \(R(N+I+O)\) (the same number of factorized layer parameters). With moderate values of both \(R\) and \(N\), the layer becomes significantly more resource-efficient than vanilla \(\)MoEs.

TR\(\)MoEWe propose a second \(\)MoE variant based on the Tensor Ring  (TR) factorization that can offer even better efficiency for large values of \(N\). In TR format, \(^{N I O}\) has three factor tensors: \(^{(1)}^{R_{1} N R_{2}}\), \(^{(2)}^{R_{2} I R_{3}}\), \(^{(3)}^{R_{3} O R_{1}}\), where \(R_{i}\) are the manually chosen ranks4. The weight tensor's elements in TR format are given by: \(w_{nio}=_{:n:}^{(1)}_{:i:}^{(2)} _{:o:}^{(3)}\). TR\(\)MoE's forward passes can be computed efficiently by contracting the first two factor tensors with the input/expert coefficients vectors and then combining the results:

\[=_{n=1}^{N}_{i=1}^{I}_{ni:}z_{i}a_{n}=_{r_{1} =1}^{R_{1}}_{r_{3}=1}^{R_{3}}^{(1)} _{2}(^{(2)}_{2})}_{[R_{1}  R_{3}]}_{r_{1}r_{3}}_{r_{3}:r_{1}}^{(3)} ^{O},\] (3)

yielding a modified FLOP count of \((R_{1}NR_{2}+R_{2}IR_{3}+R_{1}R_{2}R_{3}+R_{1}OR_{3})\) with just \((R_{1}NR_{2}+R_{2}IR_{3}+R_{3}OR_{1})\) parameters. With large \(N\) contributing to the computational cost only through \(R_{1}NR_{2}\), the TR\(\)MoE can prove even more resource-efficient than CP\(\)MoEs by choosing small values of \(R_{1},R_{2}\). We refer readers to Appendix D for a further discussion of decomposition choice, derivations of how tensor rank translates to expert matrix rank, and FLOPs comparisons.

#### 3.1.2 \(\)MoEs recover dense MoEs as a special case

Finally, we note how unfactorized \(\)MoE layers with a single level of expert hierarchy recover dense MoE layers [17; 11] as a special case. When computing Equation (1) over the full materialized weight tensor, one can alternatively write the output element-wise as \(y_{o}=^{}_{:o:}\). This highlights an interesting technical connection between neural network layers: dense MoE layers in this tensor formulation can be seen to share a similar functional form to bilinear layers, which have also found applications in interpretability [59; 60].

## 4 Experiments

We start in Section 4.1 by presenting both qualitative and quantitative experiments validating that the experts learn to specialize in processing different semantic clusters of the input data. In Section 4.2 we demonstrate one practical benefit of the learned specialism-showing how expert-conditional re-writing can correct for specific demographic bias in CelebA attribute classification. Finally, in Section 4.3 we train both large language and large vision models with \(\)MoE layers throughout-providing qualitative evidence of expert specialism and model performance competitive with networks using MLP blocks. Please see Appendix H for detailed ablation studies, and Appendix I for experiments with hierarchical \(\)MoEs.

Implementation detailsBefore applying the activation function to the expert coefficients we apply batch- and layer-normalization to \(\)MoE layers in vision and language models respectively (see Appendix H.3 for an ablation). Interestingly, we do not find the need for any load-balancing losses. We fix the TR\(\)MoE ranks to be \(R_{1}=R_{2}=4\) throughout (see Appendix D.1.2).

### Expert specialism: visualization & intervention

Our first objective is to show that **scaling \(\)MoE's expert count leads to more specialized experts**. We provide evidence of this effect both qualitatively (through _visualization_) and quantitatively (through _intervention_).

To isolate the impact of \(\)MoE layers and varying expert counts, we first explore the controlled setting of fine-tuning large foundation models CLIP ViT-B-32 and DINO  on ImageNet1k (following the fine-tuning protocol in Ilharco et al. [63; 64]). Whilst fine-tuning large foundation models is an important application of \(\)MoE layers in its own right (e.g. as explored later in Section 4.2 for fairer models), the ability to cheaply train many models with different \(\)MoE layer configurations forms an ideal setting in which to study their properties.

#### 4.1.1 Qualitative results

We first show _random_ examples in Figure 2 of images processed (with expert coefficient \( 0.5\)) by the experts by each CP\(\)MoE layer (the class labels and expert coefficients are overlaid in white and green text respectively). Using only a modest number of experts (e.g. 32) appears to lead to some 'polysemanticity'  in experts-with some processing unrelated classes of images (e.g. 'gators', 'limos', and a 'quilt' for Expert 1 on the right). On the other hand, using a much larger number of total experts appears to yield more specialization, with many experts contributing their computation to only images of the same single class label or broader semantic category. Please see Figure 16 in the Appendix for many more random images for the first \(10\) experts per model to observe this same trend more generally, and Figure 17 for even finer-grained specialism with \(2048\)-expert \(\)MoE layers.

#### 4.1.2 Quantitative results: expert monosemanticity

The qualitative evidence above hints at the potential of a prominent benefit to scaling up the number of experts with \(\)MoEs. Such subjective interpretations alone about expect specialism are _hypotheses_, rather than conclusions however . Similarities in images processed by the same expert give us an intuitive explanation of its function but do not show the expert's computation contributes _causally_ to the subtask of processing specific human-understandable patterns of input features . However, the absence of ground-truth labels for interpretable features of the input one may be interested in (e.g. specific types of textures in images, or words related to 'Harry Potter') makes this difficult to quantify in any objective or systematic manner.

Despite the absence of fine-grained labels, we _can_ quantify and compare the class-level specialism a \(\)MoE expert exhibits on the ImageNet1k dataset as an (imperfect) proxy . Following the causal intervention protocol of Elazar et al. , we ask the specific counterfactual question about solely each expert \(n\) in a \(\)MoE layer in turn: _"had expert \(n\)'s weight matrix \(_{n}\) not contributed its computation, would the network's test-set accuracy for class \(c\) have dropped?"_ Practically speaking, given a network fine-tuned with an \(\)MoE layer, we achieve this by intervening in the forward pass by zeroing the \(n^{}\) expert's weight matrix \(_{n}:=\), leaving every other aspect of the forward pass completely untouched. Let the elements of \(,}^{(n)}^{C}\) denote the test set accuracy for the \(C=1000\) ImageNet1k classes, pre- and post-intervention of expert \(n\) respectively. We collect the normalized difference to per-class accuracy in the vector \(^{(n)}\), whose elements are given by \(d_{c}^{(n)}=(y_{c}-_{c}^{(n)})/y_{c}\). At the two extremes, when the full network's accuracy for class \(c\) drops completely from \(y_{c}\) to \(0\) upon manually excluding expert \(n\)'s computation we get \(d_{c}^{(n)}=1\), whilst \(d_{c}^{(n)}=0\) means the absence of the subcomputation did not change class \(c\)'s test set accuracy at all. We thus estimate the 'class-level polysemanticity' of expert \(n\) as the distance between

Figure 3: **Higher expert counts lead to more monosemantic experts**: mean expert class-level polysemanticity of Equation (4) (\(\)) as a function of the total number of experts. Results are shown for both CLIP ViT-B-32 and DINO models fine-tuned on ImageNet1k with CP\(\)MoE layers.

Figure 2: Specialization in \(256\) vs \(32\) total expert CP\(\)MoE layers (fine-tuned on CLIP ViT-B-32). Each row displays _randomly_ selected images processed (with coefficient \( 0.5\)) by the first few experts for the two models. The more we scale the expert count, the greater the apparent expert specialist (to single visual themes or image categories).

[MISSING_PAGE_FAIL:7]

bias without requiring images' sensitive attribute value at test time). These are shown in Table 2 for the two different experiments on CelebA, where the proposed intervention outperforms baseline alternative methods in the majority of settings. Please see Appendix J for details about the baseline methods and fairness metrics used, and further discussion of results.

### Large language/vision \(\)MoE networks

Finally, we train from scratch \(12\) layer \(124\)M-parameter GPT-2  LLMs on OpenWebText  for the language domain and \(8\) layer S-16 variant7 MLP-Mixers  on ImageNet1k  for vision. We replace _every_ MLP block's 2 linear layers with 2 \(\)MoE layers. Each token \(t\)'s input vector \(_{t}^{I}\) is therefore transformed with \(\)MoE blocks of the form:

\[_{t}=_{n_{2}=1}^{N}_{h=1}^{H}_{n_{2}h :}}^{(2)}_{n_{1}=1}^{N}_{i=1}^{I}_{n _{1i}:}^{(1)}z_{ti}a_{tn_{1}}_{h}a_{tn_{2}},_{t}=( ^{}_{t}),\]

where \(_{t}^{N}\) are the expert coefficients for each specific token and block, \(H\) is the dimension of the block's hidden layer, and \(^{(1)}^{N I H},^{(2)} ^{N H O}\) are the (implicit) \(\)MoE weight

Figure 4: Top-activating patches (top rows) and their full images (second rows) for the first 3 experts across 2 CP\(\)MoE-e64 layers in \(\)MoE MLP-mixer  models-\(\)MoE blocks exhibit coarse-grained specialism (e.g. texture) earlier and more fine-grained specialism (e.g. objects) deeper in the network.

tensors for each of the two layers. We manually set the \(\)MoE ranks to parameter-match each original network and set the number of experts (per block) to \(N=64\) for vision models and \(N=256\) for LLMs. Consequently, with this configuration, **each layer's \(\)MoE block performs computations with \(N\) experts yet has the same parameter counts and FLOPs as a single, dense MLP block**.

\(\)MoE-MixerFor vision, our key findings are that earlier \(\)MoE channel-mixing blocks' experts appear (qualitatively) to exhibit specialisms to colors, shapes, and textures, whilst later layers exhibit more object-specific specialization. We plot the patches from the training set for which each expert most contributes its computation in Figure 4 for both a shallow and deep layer to illustrate this-earlier layers' experts contribute strongly to the processing of similar _patches_ (top rows, e.g. specific edges) whilst later layers' experts process tokens based more on the similarity of their surrounding semantic context (bottom rows, e.g. images of animals). We further show in Figure 12 results for the first 2 experts across all 8 blocks where such scale-specific specialism is apparent across the entire network.

\(\)MoE-Gpt2For LLMs, we see promising qualitative evidence of experts specializing throughout a corpus of 1M generated 100-token sequences. At layer 5, for example, the generated tokens that use expert 8 with the highest coefficient are compound adjectives (Figure 5), whilst expert 37 most highly activates for equality and comparison operators in code and scientific text (please see examples of

Figure 5: Top-activating generated tokens for 4 manually selected experts for GPT-2 trained with CP\(\)MoE blocks at every layer (each token is highlighted by the coefficient of the expert in question), exhibiting specializations to concepts including compound adjectives and equality operators.

many unfiltered experts in Figures 13 and 14). Whilst monosemanticity is not always attained, \(\)MoE layers nonetheless facilitate a level of specialism not facilitated by dense MLP layers.

One important result here is that \(\)MoE networks in this setup are significantly more parameter-efficient than both dense and sparse MoEs with the same expert count, as shown in Table 4. For example, GPT-\(2\) models with \(256\) sparse/dense MoE experts require a prohibitive \(14.5\)B MLP parameters alone, relative to just \(57\)M MLP parameters with \(\)MoEs of the same expert counts.

\(\)MoE performanceFinally, we substantiate our claim that networks pre-trained and fine-tuned with parameter-matched \(\)MoE layers are competitive with their existing linear layer alternatives across multiple domains/machine learning tasks. We present in Table 3 the performance results for MLP-Mixer S-\(16\), NanoGPT GPT-\(2\), and (fine-tuned) CLIP ViT-B-32  models on the OWT and ImageNet1k datasets. Following Section 4.1.1, we replace all linear layers with \(\)MoE blocks (and a single \(\)MoE final layer for fine-tuning CLIP). We initialize all linear layers following the default PyTorch \(U[-k,k]\) initialization for a fair comparison. Please see Appendix F for experimental details and learning curves, and Appendix I for experiments with varying expert count and hierarchical \(\)MoEs. Crucially, whilst \(\)MoE layers provide additional interpretability benefits through scalable expert specialization, they do not sacrifice accuracy when parameter-matched to MLP blocks, as seen from the comparable performance.

## 5 Conclusion

In this paper, we introduced the Multilinear Mixture of Experts layer (\(\)MoE). We demonstrated that larger expert counts lead to increased specialization, and how \(\)MoE layers make this computationally tractable through factorized forward passes. \(\)MoEs scale to large expert counts much more gracefully than existing MoEs, yet avoid the issues from popular gating mechanisms. As a further practical example of \(\)MoE's task decomposition, we illustrated how manual guided edits can be made to correct bias towards demographic subpopulations in fine-tuned foundation models. Having also shown matching performance in addition to expert specialism in both large vision and language models, we believe \(\)MoE layers constitute an important step towards facilitating increasingly performant models that do not trade off fairness/interpretability for accuracy.

LimitationsFirstly, it is important to state again that our quantitative evaluation only captures expert behavior on the test set, not out-of-distribution data . Furthermore, expert specialism in large models is only demonstrated qualitatively (through the expert coefficients) due to the absence of fine-grained labels. Developing ways of quantifying fine-grained expert specialism is an important direction for future research. Finally, our experimental results demonstrated comparable accuracies of \(\)MoE networks only for models with parameter counts on the order of 100 million. Where resources permit, future work should explore the scalability of expert specialization and performance of \(\)MoEs in even larger-scale LLMs.

    & NanoGPT (gpt2) & MLP-Mixer (S-\(16\)) \\  & \(N=256\) & \(N=64\) \\  Dense/Sparse MoE & \(14.5\)**B** & \(1.13\)**B** \\
**CP\({}_{}\)MoE** & **57.0M** & **17.7M** \\
**TB\({}_{}\)MoE** & **57.4M** & **17.4M** \\   

Table 4: MLP parameters required for networks with the same expert counts.

    & **MLP-mixer** S-\(16\) (ImageNET\(1\)k) & **GPT-2 NanoGPT** (OWT) & **CLIP** B-\(32\) (ImageNET\(1\)k) \\  & Val. acc. (\(\)) & \#params & Val. loss (\(\)) & \#params & Val. acc. (\(\)) & \#params \\  MLPs & 70.31 & 18.5M & **2.876** & 124M & 77.99 & 769K \\
**TR\(\)MoEs** & 71.26 & 18.3M & 2.886 & 124M & **78.71** & 771K \\
**CP\(\)MoEs** & **71.29** & 18.6M & 2.893 & 124M & 78.07 & 769K \\   

Table 3: Comparison of \(\)MoEs and dense MLPs across different models and tasks. We use \(N=64\)\(\)MoE experts for the two vision tasks and \(N=256\) for GPT2. MLP mixers and GPT2s are pre-trained for 300 epochs and 100k iterations respectively, whilst CLIP is fine-tuned for 10 epochs.