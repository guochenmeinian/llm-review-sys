# No-regret Algorithms for Fair Resource Allocation

Abhishek Sinha, Ativ Joshi

School of Technology and Computer Science

Tata Institute of Fundamental Research

Mumbai 400005, India

abhishek.sinha@tifr.res.in

ativ@cmi.ac.in

&Rajarshi Bhattacharjee, Cameron Musco,

**Mohammad Hajiesmaili**

**Manning College of Information and Computer Sciences

University of Massachusetts Amherst

{rbhattacharj, cmusco,

hajiesmaili}@cs.umass.edu

###### Abstract

We consider a fair resource allocation problem in the no-regret setting against an unrestricted adversary. The objective is to allocate resources equitably among several agents in an online fashion so that the difference of the aggregate \(\)-fair utilities of the agents achieved by an optimal static clairvoyant allocation and the online policy grows sublinearly with time. The problem inherits its difficulty from the non-separable nature of the global \(\)-fairness function. Previously, it was shown that no online policy could achieve a sublinear standard regret in this problem. In this paper, we propose an efficient online resource allocation policy, called Online Fair Allocation (\(\)), that achieves sublinear \(c_{}\)-approximate regret with approximation factor \(c_{}=(1-)^{-(1-)} 1.445\), for \(0<1\). The upper bound on the \(c_{}\)-regret for this problem exhibits a surprising _phase transition_ phenomenon - transitioning from a power-law to a constant at the critical exponent \(=}{{2}}\). Our result also resolves an open problem in designing an efficient no-regret policy for the online job scheduling problem in certain parameter regimes. Along the way, we introduce new algorithmic and analytical techniques, including greedy estimation of the future gradients for non-additive global reward functions and bootstrapping second-order regret bounds, which may be of independent interest.

## 1 Introduction

The notion of _algorithmic fairness_ refers to learning algorithms that guarantee fair predictions even when subjected to adversarially biased training data (Dwork et al., 2012). Fairness has become a major criterion for designing and deploying large-scale learning algorithms that affect a diverse user base. Since the training data could be highly skewed in practice, it is essential to make minimal assumptions about the data-generating process and design provably robust and fair learning policies. Guaranteeing fairness becomes even more challenging in the online learning setup as there is no distinction between the training and test data, and no assumption is made on the input data sequence. As an example, consider an online recruitment campaign where a learning algorithm decides the target group (identified by, say, the tuple (race, gender, age)) to which an ad for a job vacancy is to be displayed. Suppose a revenue-maximizing recommendation algorithm concludes from past data that more revenue is generated by showing the ad to Group A compared to Group B. In that case, the ad-serving algorithm will eventually end up showing that ad exclusively to Group A while discriminating against Group B users of a potential job opportunity (Hao, 2019). One of the overarching goals of this paper is to design efficient online learning policies that provably mitigate this algorithmic bias while maintaining efficiency _irrespective_ of the past data seen so far (refer to Example B.1 in the Appendix for a closely-related scheduling problem considered in this paper).

Towards this goal, we consider a generic online fair resource allocation problem, which we call NOFRA (No-Regret Fair Resource Allocation). In this problem, a fixed set of resources needs to be equitably shared among \(m\) agents over multiple rounds. Note that fairness is a complex, multidimensional, and essentially subjective concept. Several metrics have been introduced in the literature to quantify the degree of fairness in resource allocation, including \(\)-fairness (Lan et al., 2010), proportional fairness (Kelly, 1997, Mo and Walrand, 2000), max-min fairness (Radunovic and Le Boudec, 2007, Nace and Pioro, 2008), and Jain's fairness index (Jain et al., 1984). In this paper, we consider the problem of maximizing the \(\)-fairness function, in which the utility of each agent when allocated \(R\) units grows as \(R^{1-}\). The parameter \(\) is restricted to the interval \([0,1)\). This same range of \(\) has been studied earlier in a game-theoretic set up by Altman et al. (2010).

On the hardness of the NoFra problem:In the standard online learning setting, the cumulative reward accrued over a given time horizon is the sum of the rewards obtained at each round (Hazan, 2019). In contrast, the objective in NOFRA is to maximize the global \(\)-fairness function, which is equal to the sum of cumulative rewards of each agent raised to the power \(1-,0<1\). Due to the power-law non-linearity, the objective function of NOFRA is non-separable with respect to time. Note that the non-separability of the objective function is essential to induce fairness in sequential allocations by incorporating the diminishing return property. However, this renders the NOFRA problem fundamentally different from standard online learning problems. In particular, Theorem 2 proves a non-trivial lower bound to the approximation ratio achievable by any online learning policy for this problem. This lower bound does not hold in classic settings with additive rewards.

Our contributions:We show that despite its non-separable structure, the NOFRA problem can be approximately reduced to an online linear optimization problem with a greedily defined sequence of reward vectors. The resulting problem can then be solved with an online convex optimization (OCO) policy with a second-order regret bound. In particular, we make the following contributions:

* In Algorithm 1, we present an efficient online resource allocation policy, Online Fair Allocation (OFA), that approximately maximizes the aggregate \(\)-fairness function of the agents. We show that OFA achieves \((1-)^{-(1-)} 1.445\)-approximate sublinear regret (Theorem 1). To the best of our knowledge, OFA is the first online policy that approximately maximizes the \(\)-fairness against _any_ adversary, which could even be adaptive.
* In Theorem 2, we establish a lower bound to the approximation factor \(c_{}\) achievable by any online learning policy for the \(\)-fair reward function. Our lower bound improves upon the prior best-known lower bound of Si Salem et al. (2022), which established only that \(c_{}>1\).
* Technically, our proposed OFA policy optimizes the non-additive global reward function by greedily estimating the future gradients and using these estimates to construct an online linear optimization problem such that sublinear regret for this linear problem translates into sublinear approximate-regret for the global optimization problem. The linear optimization problem is then solved with an online gradient ascent policy with adaptive step sizes. The resulting algorithm is simple and intuitive: we have weights on each user that decay as the user's cumulative reward increases.
* On the analytical side, a key challenge is that the online linear optimization problem has a demand sequence (and hence, a gradient sequence) that depends on our past actions. We introduce a new proof technique that controls the norm of the gradients in this setting so that a tight second-order regret bound can be established. This technique applies generally to problems with states where the future gradients depend on past actions.
* We provide numerical simulation results supporting our theoretical conclusions and demonstrating the effectiveness of our approach in several applications of the NOFRA problem.

Related work:Several closely related works with non-additive reward functions appear in the online learning literature (Si Salem et al., 2022, Even-Dar et al., 2009, Rakhlin et al., 2011). In the following, we explain why the NOFRA problem is fundamentally different from the existing studies. An extended literature review is given in Appendix A.

Closest to our work, Si Salem et al. (2022) considered the problem of designing fair online resource allocation policies. The authors showed that it is impossible to design a no-regret policy without restricting the set of admissible adversarial demand sequences. Given this negative result, the authorsproposed a no-regret policy using a primal-dual framework under the assumption of a restricted adversary, which exhibits an essentially i.i.d. stochastic-like fluctuation. In contrast, we design a robust policy with an approximate sublinear regret _without restricting_ the adversary. Due to the significantly weaker assumption, our proposed policy and its analysis are drastically different from that of Si Salem et al. (2022). Even-Dar et al. (2009) considered maximizing a global concave objective function in the no-regret setup under the further assumption that the optimal offline reward is convex. They presented an approachability-based policy in this setting and also proved the impossibility of achieving sublinear regret when the convexity condition is violated. Although the objective function in NOFRA is concave, we show that the optimal static offline reward function fails to be convex (see Appendix C.8). Hence, the policy of Even-Dar et al. (2009) does not apply to the NOFRA problem. Rakhlin et al. (2011) considered the problem of no-regret learnability for a wide class of non-additive global functions. However, their results also do not apply to our setting as our problem is not no-regret learnable (see Theorem 2). Agrawal et al. (2016) considered the problem of maximizing a concave utility function with the bandit information feedback and derived an efficient policy. However, their \(()\) regret bound holds only in the stochastic setting. Offline fair caching algorithms using the \(\)-fair utility function were proposed in Liu et al. (2020). However, their policy cannot be used in the online setting due to the casually available information structure. Finally, the work by Paria and Sinha (2021), Bhattacharjee et al. (2020), Mukhopadhyay et al. (2022) considered the online caching problem with the linear utility function. However, due to the non-linearity, extending their work to the \(\)-fair utility function entails several technical challenges, which we resolve in this paper.

## 2 Problem Formulation

In this section, we give the general formulation of the NOFRA problem, along with examples of concrete resource allocation problems that fit into this framework.

Consider a set of _agents_ among which a limited resource is to be fairly divided. Assume that the resource allocated to the \(i^{}\) agent on the \(t^{}\) round is represented by an \(N\)-dimensional non-negative vector \(y_{i}(t)\), where \(N\) is arbitrary. On every round \(t\), the \(i^{}\) agent requests an \(N\)-dimensional non-negative _demand_ (or, reward) vector \(x_{i}(t)\). The demand vectors are revealed to an online allocation policy \(\) at the end of each round. We make _no assumption_ on the regularity of the demand vector sequence, which could be adversarial (_c.f.,_ Si Salem et al. (2022)). Before the \(N m\) dimensional aggregate demand matrix \((t)x_{1}(t),x_{2}(t),,x_{m}(t)\) for round \(t\) is revealed, the online resource allocation policy \(\) chooses a non-negative \(N m\) dimensional allocation matrix \((t)=y_{1}(t),y_{2}(t),,y_{m}(t)\) from the set of all feasible allocations \(\). The set of all feasible allocations is assumed to be convex (see Remark 2 below). The reward accrued by the agent \(i\) on round \(t\) is given by the inner-product \((x_{i}(t),y_{i}(t))\), which, without loss of generality, is assumed to be upper-bounded by one. The cumulative reward accrued by agent \(i\) by the end of round \(t\) is given by

\[R_{i}(t)=R_{i}(t-1)+ x_{i}(t),y_{i}(t),\  i,t, \]

where we set the initial condition \(R_{i}(0) 1, i\).1 By iterating the above recursion, the cumulative reward \(R_{i}(t)\) can be alternatively expressed as follows:

\[R_{i}(t)=1+_{=1}^{t} x_{i}(),y_{i}(). \]

We make two mild technical assumptions on the structure of the demand and allocation vectors.

**Assumption 1**.: \(\|x_{i}(t)\|_{1} 1, i,t,\) _for some constant \(>0\)._

**Assumption 2**.: _Let \(_{N m}\) denote the \(N m\) all-\(1\) matrix. Then \(_{N m}\) for some constant \(>0\)._

The above assumptions imply that it is possible to ensure a non-zero reward for all agents on all rounds. The assumptions are used in the regret analysis only (see Eq. (25)). Our proposed online policy is oblivious to the value of the parameters \(\) and \(\).

The utility of any user for a cumulative reward of \(R\) is given by the concave \(\)-fair utility function \(:_{+}_{+}\), defined as follows:

\[(R)=_{}(R)}{1-},\ \ R 0, \]for some constant \(0<1\).2 The fairness parameter \(\) induces a trade-off between the desired efficiency and fairness by incorporating a notion of _diminishing return_ property in the global objective function. The static offline optimal allocation with larger \(\) leads to more equitable cumulative rewards (Bertsimas et al., 2012). Setting \(=0\) reduces the problem to the "unfair" online linear optimization problem. Our objective is to design an online resource allocation policy \(\{(t)\}_{t 1}\) that minimizes the regret for maximizing the aggregate utilities of all users compared to any fixed offline resource allocation strategy \(^{*}\). To be precise, assume that the offline, fixed resource allocation \(^{*}\) yields a cumulative reward of \(^{*}(T)\) for a horizon of length \(T 1\). Then, our objective is to design a resource allocation policy which minimizes the \(c\)-approximate regret (which we refer to as \(c\)-regret for short) defined as:

\[_{T}(c)_{i=1}^{m}(R_{i}^{*}(T))-c_{i=1}^{m} (R_{i}(T)), \]

for some small constant \(c 1\).3 In the case of standard regret (\(c=1\)), we drop the argument in the parenthesis. Note that, unlike the standard online convex optimization problem, in the NOFRA problem, the reward function is global, in the sense that it is non-separable across time (Even-Dar et al., 2009). For this reason, it is necessary to consider the \(c\)-regret with \(c>1\), rather than the standard regret (i.e., with \(c=1\)). This will become clear from Theorem 2, where we prove an explicit lower bound to the approximation factor achievable by any online policy for the global \(\)-fair reward function. This lower bound implies a concrete lower bound on the achievable \(c\). Our lower bound improves upon (Si Salem et al., 2022, Theorem 1), where it was shown that no online policy can achieve a sublinear standard regret for the NOFRA problem under an unrestricted adversary.

**Remark 1:** When \(>1\), the offline benchmark \((R_{i}^{*}(T))\) itself becomes \(O(1), i\). Hence, in this regime, a sublinear regret bound (4) becomes vacuous. Consequently, we restrict the fairness parameter \(\) to the interval \([0,1)\).

**Remark 2:** In the above problem definition, we assumed that the set of feasible allocations \(\) is convex and thus that there are no integrality constraints on the allocation, _i.e.,_ the components of the allocation matrix \((t)\) are allowed to be fractional. However, in many combinatorial resource allocation problems, the allocation vector is required to be integral. In this case, the feasible action set \(\) is naturally defined to be the convex hull of the integral actions. In Appendix C.6, we consider the setting with integrality constraints and extend our algorithm and analysis to design a randomized integral allocation policy with a sublinear regret bound.

Motivating examples:The statement of NOFRA is fairly general, and by suitably choosing the reward and allocation vectors, many standard resource allocation problems can be reduced to NOFRA. In what follows, we highlight the example of fair shared caching (Bhattacharjee et al., 2020; Paria and Sinha, 2021). We refer the readers to Appendix B for additional examples of online job scheduling and online matching.

Figure 1: The online shared caching problem

In the _Online Shared Caching_ problem with a single shared cache, \(m\) users are connected to a single cache of capacity \(k\) (see Figure 1 for a schematic). At each round, each user requests a file from a library of size \(N.\) The file request sequence may be adversarial. At the beginning of each round \(t,\) an online caching policy prefetches at most \(k\) files on the cache, denoted by the vector \(y(t)\) such that

\[_{i=1}^{N}y_{i}(t)=k,\ \ 0 y_{i}(t) 1\, i[N]. \]

The set of all feasible caching configurations is denoted by \(_{k}^{N}.\)4 Immediately after the prefetching at round \(t,\) each user \(i\) reveals its file request, represented by the one-hot encoded demand vector \(x_{i}(t),\)\(1 i m.\) A special case of the above caching model for a single user (\(m=1\)) has been investigated in previous work [Bhattacharjee et al., 2020, Mahaisen et al., 2022, Joshi and Sinha, 2022].

Let \(R_{i}(t)\) denote the cumulative (fractional) hits obtained by the \(i^{}\) user up to round \(t 1.\) We have

\[R_{i}(T)=1+_{t=1}^{T}\{x_{i}(t),y(t)\},\ \ 1 i m. \]

The online shared caching problem can be easily reduced to an instance of the NOFRA problem by taking the demand matrix to be \((t)=x_{1}(t),x_{2}(t),,x_{m}(t).\) Since the allocation vector \(y(t)\) is common to all users, the allocation matrix can be taken to be \((t)=y(t),y(t),,y(t).\) It can be observed that Assumption 2 holds in this case with \(=k/N\) by noting that \(}{{N}}_{N}_{k}^{N}.\)

## 3 Designing an approximately-no-regret policy for NoFra

In view of the difficulty outlined in Section 1, the design and analysis of the Online Fair Allocation (OFA) policy consists of two parts. First, in Lemma 1, we show that by greedily estimating the terminal gradient with the current gradients at each round, the \((1-)^{-(1-)}\)-approximate regret of the NOFRA problem can be upper bounded by the standard regret of a surrogate online linear optimization problem with a policy-dependent gradient sequence. Note that the regret of the surrogate problem depends on the norms of the gradients, which, in turn, depends on the policy. We use the online gradient ascent policy with adaptive step sizes to solve the surrogate problem while simultaneously controlling the norm of the gradients. The following section details our technique.

Reducing the NoFra problem to an Online Linear Optimization Problem with policy-dependent subgradients

Since the \(\)-fairness function \(:_{>0}_{>0}\) is concave and continuously differentiable, we have that for any \(x,y>0:\)

\[(x)-(y)^{}(y)(x-y). \]

Let \( 1\) be a constant, which will be fixed later. Taking \(x=R_{i}^{*}(T)\) and \(y= R_{i}(T)\) in the above inequality, we have

\[(R_{i}^{*}(T))-^{1-}(R_{i}(T)) }{{=}} (R_{i}^{*}(T))-( R_{i}(T)),\] \[}{{}} ^{}( R_{i}(T))(R_{i}^{*}(T)- R_{i}(T))\] \[}{{}} ^{-}^{}(R_{i}(T))_{t=1}^{T}\{x_{i}(t),y _{i}^{*}- y_{i}(t)\}, \]

where in (a), we have used the positive homogeneity property of the fairness function \(( x)=^{1-}(x),\) in (b), we have used the concavity of the fairness function \(()\) from Eqn. (7), and, finally, in (c), we have used the definition of cumulative rewards (2), the fact that \( 1,\) and the homogeneity of the function \((),\) which gives \(^{}( x)=^{-}^{}(x).\)Summing up the bound (8) over all agents \(i[m]\), we obtain the following upper bound to the \(^{1-}\)-Regret of any online policy for the NOFRA problem:

\[_{T}(^{1-})^{-}_{t=1}^{T} _{i}(^{}(R_{i}(T))x_{i}(t),y_{i}^{*}- y_{i}(t)). \]

It is critical to note that each of the quantities \(^{}(R_{i}(T))\) depends on the entire sequence of past actions \(\{y_{t}\}_{t=1}^{T}\). Hence, \(^{}(R_{i}(T))\) in (9) is not known to the online policy when it takes its actions. More importantly, the value of the coefficient \(^{}(R_{i}(T))\) depends on the future actions and requests through (1). Hence, it is _impossible_ to know the value of this coefficient in an online fashion. To get around this fundamental difficulty, we now consider a _surrogate_ regret-minimization problem by replacing the term \(^{}(R_{i}(T))\) by its _causal counterpart_\(^{}(R_{i}(t-1))\) at the beginning of round \(t\). In other words, we now design an online learning policy that minimizes the regret of the following online linear optimization problem:

\[_{T}_{t=1}^{T}_{i}(^{}R _{i}(t-1)x_{i}(t),y_{i}^{*}-y_{i}(t)). \]

To recapitulate the information structure, recall that at the beginning of round \(t\), the cumulative reward vector \((t-1)\), and hence the coefficient \(^{}(R_{i}(t-1))\), is known. Still, the current request vector \((t)\) is unknown to the online learner before it makes its allocation decision \(y_{t}\) on the \(t^{}\) round. We now establish the following key lemma that relates the original regret bound (9) to the regret bound (10) for the surrogate problem.

**Lemma 1**.: _Consider any arbitrary online resource allocation policy and assume that the utility function of each user is given by the \(\)-fair utility function (3). Then, for any horizon of length \(T 1,\) we have_

\[_{T}(c_{})(1-)^{}_{T}+c_{}m, \]

_where \(c_{}(1-)^{-(1-)} e^{1/e}<1.445\)._

Proof outline:As discussed above, the surrogate problem greedily replaces the non-causal terminal gradient \(^{}(R_{i}(T))\) in Eqn. (9) by the current gradient \(^{}(R_{i}(t-1)), i\) on round \(t\). To prove Lemma 1, we show that this transformation can be done by incurring only a small penalty factor to the overall regret bound. To show this, we split the regret expression (9) into the difference between two terms - term \((A)\) corresponding to the cumulative reward accrued by the static allocation (\(^{*}\)), and term \((B)\) corresponding to the reward accrued by the online policy. Next, we compare these two terms separately with the corresponding terms \((A^{})\) and \((B^{})\) in the regret expression for the surrogate online linear optimization problem (10). By exploiting the non-decreasing nature of the cumulative rewards, we first show that \((A)(A^{})\) (Eq. (15)). Next, by using properties of the \(\)-fair utility function, we show that \((B^{})(1-)^{-1}(B+m)\) for any policy (Eq. (16)). Lemma 1 then follows by combining these two results. See Appendix C.2 for the proof.

### Online Learning Policy for the Surrogate Problem and its Regret Analysis

Having established that the \(c_{}\)-regret of the original problem is upper-bounded by the regret of the surrogate learning problem, we now proceed to upper bound the regret of the surrogate problem under the action of a no-regret learner with a second-order regret bound. In the sequel, we use the projected Online Gradient Ascent (OGA) with adaptive step sizes (Orabona, 2020, Algorithm 2.2) for designing a no-regret policy for the surrogate problem. We call the resulting online learning policy Online Fair Allocation (OFA). The pseudocode of the OFA policy is given in Algorithm 1.

The Online Fair Allocation policy (OFA):Since \(^{}(R_{i}(t-1))=1/R_{i}(t-1)^{},\) at the end of round \(t\), each user \(i\) computes its gradient component \(g_{i}\) by dividing its current demand vector \(x_{i}(t)\) with its current cumulative reward \(R_{i}(t-1)\) raised to the power \(\) (line 5 of Algorithm 1). This formalizes the intuition that a user with a larger current cumulative reward has a smaller gradient component. In line 8, we take a projected gradient ascent step. Depending on the problem, the projection can often be efficiently computed, _e.g.,_ using variants of the Frank-Wolfe algorithm given access to an efficient LPoracle. With additional structure, OFA can be simplified further with an even more efficient projection. For example, we give a simplified implementation for the online shared caching problem in Appendix C.5 by exploiting the fact that the action vector \(y(t)\) is the same for all users. Step 9 is an optional sampling step which is executed only if an integral allocation is required. We discuss this step in Appendix C.6. Finally, the cumulative rewards of all users are updated in Step 11. The following lemma gives an upper bound to the regret of the OFA policy for the surrogate problem.

**Lemma 2**.: _The Online Fair Allocation policy, described in Algorithm 1 achieves the following standard regret bound for the surrogate problem (10) for the \(\)-fair utility function:_

\[_{T}=O(T^{}{{2}}-}),\ \ \ \ 0<<}{{2}},\\ O(),\ \ \ \ =}{{2}}\\ O(1),\ \ \ \ }{{2}}<<1.\]

_Further, under this policy, the cumulative rewards of each user increase linearly with time, i.e., \(R_{i}(T)=(T), i,T\)._

**Proof outline:** One of the major challenges in the regret analysis of the surrogate problem (10) is that the coefficients of the gradients \(\{^{}(R_{i}(t-1)),i[m]\}\) on round \(t\) depends on the past actions \(\{()\}_{=1}^{t}\) of the policy itself. Since the second-order regret bound of any online linear optimization problem scales with the norm of the gradients, we now need to _simultaneously_ control the regret _and_ the norm of the gradients generated by the online policy. Surprisingly, the proof of Lemma 2 shows that the proposed OGA policy with adaptive step sizes not only provides a sublinear regret but also keeps the gradients small, which in turn helps keep the regret small. In fact, these two goals are well-aligned to each other, and our proof exploits the reinforcing nature of these two objectives via a new _bootstrapping_ technique. See Appendix C.4 for the proof.

Finally, combining Lemma 1 and Lemma 2, we obtain the main result of this paper.

**Theorem 1**.: _The Online Fair Allocation (\(\)) policy, described in Algorithm 1, achieves the following approximate regret bound for the NOFRA problem (4) with \(c_{}(1-)^{-(1-)}\):_

\[_{T}(c_{})=(1-)^{}O(T^{}{{2}}-})\ \ \ \ 0<<}{{2}},\\ O()\ \ \ \ =}{{2}}\\ O(1)\ \ \ \ }{{2}}<<1, \]

```
1:Input: Fairness parameter \(0<1\), Demand/Reward vectors from the agents \(\{(t)\}_{t=1}^{T}\), Euclidean projection oracle \(_{}()\) onto the feasible set \(\), an upper-bound \(D\) to the Euclidean diameter of the feasible set5.
2:Output: Online resource allocation decisions \(\{y_{t}\}_{t=1}^{T}\)
3:\(R_{i} 1, i[m],S 0\)\(\)Initialization
4:for each round \(t=2:T\):do
5:\(g_{i}(t)-1}{R_{i}^{T}}, i[m]\)\(\)Computing the gradient components for each agents
6:\(g(g_{1},g_{2},,g_{m})\)\(\)Computing the full gradient
7:\(S S+\|g\|_{2}^{2}\)\(\)Accumulating the norm of the gradients
8:\(y_{}y+}g\)\(\)Updating the inclusion probabilities using OGA
9:\(\) Sample a randomized integral allocation \(Y\) s.t. \([Y]=y\).
10: The agents reveal their demand/ reward vectors \(\{x_{i}(t)\}_{i[m]}\) for the current round.
11:\(R_{i} R_{i}+\{x_{i}(t),y_{i}\},\  i[m]\). \(\)Updating cumulative rewards
12:endfor
```

**Algorithm 1** The Online Fair Allocation (\(\)) Policy

**Remarks:** 1. For the job scheduling problem in the reward maximization setting, Even-Dar et al. (2009, Lemma 4) showed that if the cumulative reward is concave and the offline optimal reward is convex, then their proposed approachability-based recursive policy, whose complexity scales exponentially with the number of machines \(m\), achieves sublinear regret. In Section 7 of the same paper, the authors posed an open problem of attaining a relaxed goal when the above sufficient condition is violated. In Appendix C.8, we show that for the \(\)-fair utility function (3),the offline optimal cumulative reward is _non-convex_ in the regime \(0<<1\). Hence, Theorem 1 gives a resolution to the above open problem by exhibiting a simple online policy with a sublinear approximate regret when the given sufficient condition is violated. Furthermore, the computational complexity of our policy is linear in the number of machines \(m\), which is significantly lower than that of their approachability-based policy, whose complexity scales exponentially fast in \(m\).

2. Observe that the regret bound given by Theorem 1 always remains non-vacuous, _irrespective_ of the value of \(\) and the sequence of adversarial reward vectors. This follows from the fact that irrespective of the demand vectors, by choosing the constant action \(y=_{N m}\) (which is feasibly by Assumption 2), each user can achieve a cumulative reward of \( T\). Hence, the optimal offline value of the \(\)-fair utility function is \((T^{1-})\). On the other hand, by Theorem 1, the OFA policy achieves a regret bound of \(O(T^{}{{2}}-})\), which is always dominated by the optimal static offline objective.

3. When \(=0,\)NOFRA corresponds to the cumulative reward maximization problem for all users. Hence, Theorem 1 recovers the well-known \(O()\) standard regret bound (Orabona, 2020).

The following converse result gives a universal lower bound to the approximation factor \(c\) for which it is possible to design an online policy for NOFRA with a sublinear \(c\)-regret.

**Theorem 2** (Lower bound on approximation factor).: _Consider the online shared caching problem for the \(\)-fair reward function with \(m=2\) users. Any online policy with a sublinear \(c_{}\)-regret must have_

\[c_{}_{0}{{2}}}}}{{2}}} }+{(1-)}^{1-}}{{({1-/2})}^{1- }+{(/2)}^{1-}}>1,\ \ 0<<1.\]

See Appendix C.9 for the proof of Theorem 2. A numerical comparison between the upper and lower bounds on the approximation factor is shown in Figure 7 in the Appendix. Observe that for \((0,1)\) the above lower bound is strictly greater than 1, improving on the prior best known lower bound of (Si Salem et al., 2022), which showed just that sublinear standard regret (i.e., \(c_{}=1\)) is unachievable.

## 4 Experimental results

In this section, we report experimental results using the fair caching problem as a case study6. Additional experiments on fair scheduling are provided in Appendix D.2. We compare the performance of our algorithms on two datasets against several baselines showing the effectiveness of our algorithm.

Setup and Dataset:We perform simulations on both synthetically generated data and on CDN traces from Berger (2018). For our synthetic dataset, the request patterns of the users are highly homogeneous, letting us demonstrate the fairness behavior of different algorithms. In particular, we take time horizon \(T\) = 1000 rounds, number of users \(m\) = 5, cache size \(C\) = 7, and library size \(N\) =30. At each time step: (1) users 1 and 2 request a file from file numbers 0-29 uniformly at random; (2) user 3 repeatedly requests file numbers 0-3; (3) user 4 repeatedly requests file numbers 4-18; (4) user 5 repeatedly requests file numbers 19-27. Intuitively, the difficulty comes in ensuring a fair allocation of hits to users 1 and 2, who request files from a larger space of possibilities than the other users.

The CDN dataset Berger (2018) contains a request ID for each request, corresponding to the timestamp and the file ID of the requested file. We set \(T=400\) rounds, number of users \(m=4\), cache size \(C=10\) and, library size \(N=50\). For preprocessing, first, we sort the data according to the timestamp and discard all the files with an ID greater than \(N\), and then only consider the first \(m T=1600\) requests among the remaining requests. We then allocate files to users in decreasing order of their popularity, such that user 1 tends to request common files, while user 4 tends to request less common files. This makes ensuring a fair cache allocation difficult. See Appendix D.1 for details.

Comparison:We compare our algorithm with the _Online Horizon-Fair_ (OHF) policy proposed by Si Salem et al. (2022). Just like the OFA policy, OHF is also an online policy that ensures long-term fairness using the \(\)-fairness utility, which makes it an ideal candidate for comparison. We also compare our algorithm with the commonly used Least-Recently-Used (LRU) and Least-Frequently-Used (LFU) cache replacement policies. We compare our algorithm with the fixed offline optimal allocation and the maximin optimal allocation. Both policies have access to all the file requests ahead of time. The fixed optimal offline allocation is given by \(y^{*}=*{argmax}_{y}_{i=1}^{m}(R_{i}(T))\), where \(\) is the set of feasible cache configurations (see Eqn. (5)) and \(R_{i}(T)\) is defined in Eqn. (6). The maximin optimal allocation is the fixed offline caching configuration that maximizes the minimum hitrate among all users. It can be computed via an LP. We compare these algorithms in terms of the average hit rate, minimum hit rate, and Jain's fairness index as the value of \(\) changes. Jain's fairness index is a common metric used to quantify if the users are receiving a fair share of network resources. In our case, the fairness index is given by

\[(R_{1}(T),,R_{m}(T))=^{m}R_{i}(T))^{2}}{m _{i=1}^{m}R_{i}^{2}(T)}.\]

The index ranges from \(1/m\) for minimum fairness to \(1\) for maximum fairness.

Observations:The average hitrates of each policy are shown in Figures 1(a) and 2(a). For the synthetic dataset, despite being fair, the overall performance of the OFA policy is at par with the traditional caching algorithms like LRU and LFU policies. For the CDN dataset, the OFA policy initially performs close to the Si Salem et al. (2022) algorithm while for \( 1.5\), the OFA thoroughly outperforms it. For \(>1\), there is a drop in performance of OFA as compared to the "unfair" LRU and LFU algorithms.7 The minimum hitrates of each policy are shown in Figures 1(b) and 2(b). For both datasets, the OFA policy outperforms Si Salem et al. (2022), LRU, and LFU. We can also see the minimum and average hitrates for the OFA policy come closer to each other as we increase \(\). I.e. the algorithm becomes fairer at the cost of total hitrate. Finally, from the plots in Figures 1(c) and 2(c), we can see that as \(\) increases, OFA outperforms all other algorithms in terms of Jain's fairness index. Also, Jain's index for OFA approaches the maximum value of \(1\) as we increase \(\).

Figure 3: CDN Dataset

Figure 2: Synthetic Dataset

Conclusion and open problems

In this paper, we propose an efficient online resource allocation policy, Online Fair Allocation (OFA), that achieves a \(c_{}\)-approximate sublinear regret bound for the \(\)-fairness objective, where \(c_{}(1-)^{-(1-)} 1.445\), for \(0<<1\). Our main technical contribution is to show that the non-additive \(\)-fairness function can be efficiently learned by greedily estimating the terminal gradients. An important follow-up problem is to investigate the extent to which the algorithmic and analytical methodologies introduced in this paper can be generalized. Note that, for the online scheduling problem, only a recursive approachability-based policy is known in the literature, whose complexity scales _exponentially_ with the number of machines (Even-Dar et al., 2009). The algorithm and analysis presented in this paper are specific to the \(\)-fair utility function. It would be interesting to investigate whether these ideas can be extended to learning general concave utility functions. Another related problem is to design an optimistic version of the proposed OFA policy that offers an improved regret bound by efficiently incorporating hints regarding the future demand sequence (Maisen et al., 2022; Bhaskara et al., 2020). Finally, reducing the gap between the upper and lower bounds of the approximation factor in Figure 7 would be of interest.

Acknowledgement

This work is supported by a US-India NSF-DST collaborative grant coordinated by IDEAS-Technology Innovation Hub (TIH) at the Indian Statistical Institute, Kolkata, and as a supplementary fund of NSF CAREER 2045641. A. Sinha was additionally supported by the Qualcomm Research Grant IND-417880. M. Hajiesmaili's work is supported by CNS-2102963, CNS-2106299, and CPS-2136199. C. Musco was also partially supported by an Adobe Research grant.