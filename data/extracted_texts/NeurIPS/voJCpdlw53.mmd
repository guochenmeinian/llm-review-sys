# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

###### Abstract

Ultra-high-resolution image generation poses great challenges, such as increased semantic planning complexity and detail synthesis difficulties, alongside substantial training resource demands. We present UltraPixel, a novel architecture utilizing cascade diffusion models to generate high-quality images at multiple resolutions (_e.g._, 1K to 6K) within a single model, while maintaining computational efficiency. UltraPixel leverages semantics-rich representations of lower-resolution images in the later denoising stage to guide the whole generation of highly detailed high-resolution images, significantly reducing complexity. Furthermore, we introduce implicit neural representations for continuous upsampling and scale-aware normalization layers adaptable to various resolutions. Notably, both low- and high-resolution processes are performed in the most compact space, sharing the majority of parameters with less than 3\(\%\) additional parameters for high-resolution outputs, largely enhancing training and inference efficiency. Our model achieves fast training with reduced data requirements, producing photo-realistic high-resolution images and demonstrating state-of-the-art performance in extensive experiments.

## 1 Introduction

Recent advancements in text-to-image (T2I) models, _e.g._, Imagen , SDXL , PixArt-\(\), and Wurstchen , have demonstrated impressive capabilities in producing high-quality images, enriching a broad spectrum of applications. Concurrently, the demand for high-resolution images has surged due to advanced display technologies and the necessity for detailed visuals in professional fields like digital art. There is a great need for generating aesthetically pleasing images in ultra-high resolutions, such as 4K or 8K, in this domain.

While popular T2I models  excel in generating images up to \(1024 1024\) resolution, they encounter great difficulties in scaling to higher resolutions. To address this, training-free methods have been proposed that modify the network structure  or adjust the inference strategy  to produce higher-resolution images. However, these methods often suffer from instability, resulting in artifacts such as small object repetition, overly smooth content, or unreasonable details. Additionally, they frequently require long inference time  and manual parameter adjustments  for different resolutions, hindering their practical applications. Recent efforts have focused on training models specifically for high resolutions, such as ResAdapter  for \(2048 2048\) pixels and PixArt-\(\) for \(2880 2880\). Despite these improvements, the resolution and quality of generated images remain limited, with models optimized for specific resolutions only.

Training models for ultra-high-resolution image generation presents significant challenges. These models must manage complex semantic planning and detail synthesis while handling increased computational loads and memory demands. Existing techniques, such as key-value compression  in attention  and fine-tuning a small number of parameters , often yield sub-optimal results and hinder scalability to higher resolutions. Thus, a computationally efficient method supporting high-quality detail generation is necessary. We meticulously review current T2I models and identify the cascade model  as particularly suitable for ultra-high-resolution image generation. Utilizing a cascaded decoding strategy that combines diffusion and variational autoencoder (VAE), this approach achieves a 42:1 compression ratio, enabling a more compact feature representation. Additionally, the cascade decoder can process features at various resolutions, as illustrated in Section A in the appendix. This capability inspires us to generate higher-resolution representations within its most compact space, thereby enhancing both training and inference efficiency. However, directly performing semantic planning and detail synthesis at larger scales remains challenging. Due to the distribution gap across different resolutions (_i.e._, scattered clusters in the t-SNE visualization in Figure 2), existing models struggle to produce visually pleasing and semantically coherent results. For example, they often result in overly dark images with unpleasant artifacts.

In this paper, we introduce UltraPixel, a high-quality ultra-high-resolution image generation method. By incorporating semantics-rich representations of low-resolution images in the later stage as guidance, our model comprehends the global semantic layout from the beginning, effectively fusing text information and focusing on detail refinement. The process operates in a compact space, with low- and high-resolution generation sharing the majority of parameters and requiring less than 3\(\%\)additional parameters for the high-resolution branch, ensuring high efficiency. Unlike conventional methods that necessitate separate parameters for different resolutions, our network accommodates varying resolutions and is highly resource-friendly. We achieve this by learning implicit neural representations to upscale low-resolution features, ensuring continuous guidance, and by developing scale-aware, learnable normalization layers to adapt to numerical differences across resolutions. Our model, trained on 1 million high-quality images of diverse sizes, demonstrates the capability to produce photo-realistic images at multiple resolutions (_e.g._, from 1K to 6K with varying aspect ratios) efficiently in both training and inference phases. The image quality of our method is comparable to leading closed-source T2I commercial products, such as Midjourney V6  and DALL-E 3 . Moreover, we demonstrate the application of ControlNet  and personalization techniques  built upon our model, showcasing substantial advancements in this field.

## 2 Related Work

**Text-guided image synthesis.** Recently, denoising diffusion probabilistic models [45; 18] have refreshed image synthesis. Prominent text-guided generation models [37; 4; 3; 36; 9; 35; 29; 46; 42; 27] have demonstrated a remarkable ability to generate high-quality images. A common approach is to map raw image pixels into a more compact latent space, in which a denoising network is trained to learn the inverse diffusion process [4; 3; 37]. The use of variational autoencoders  has proven to be highly efficient and is crucial for high-resolution image synthesis [13; 41]. StableCascade  advances this approach by learning a more compact latent space, achieving a compression ratio of 42:1 and significantly enhancing training and inference efficiency. We build our method on StableCascade primarily due to its extremely compact latent space, which allows for the efficient generation of high-resolution images.

**High-resolution image synthesis.** Generating high-resolution images has become increasingly popular, yet most existing text-to-image (T2I) models struggle to generalize beyond their trained resolution. A straightforward approach is to generate an image at a base resolution and then upscale it using super-resolution methods [51; 10; 28; 48; 8]. However, this approach heavily depends on the quality of the initial low-resolution image and often fails to add sufficient details to produce high-quality high-resolution (HR) images. Researchers have proposed direct HR image generation as an alternative. Some training-free approaches [15; 12; 21; 1; 22; 53; 26] adjust inference strategies or network architectures for HR generation. For instance, patch-based diffusion [1; 26] employ a patch-wise inference and fusion strategy, while ScaleCrafter  modifies the dilation rate of convolutional blocks in the diffusion UNet [37; 41] based on the target resolution. Another method  adapts attention entropy in the attention layer of the denoising network according to feature resolutions. Approaches like Demofusion  and FouriScale  design progressive generation strategies, with FouriScale further introducing a patch fusion strategy from a frequency perspective.

Despite being training-free, these methods often produce higher-resolution images with noticeable artifacts, such as edge attenuation, repeated small objects, and semantic misalignment. To improve HR image quality, PixArt-sigma  and ResAdapter  fine-tune the base T2I model. However, their results are limited to \(2880 2880\) resolution and exhibit unsatisfied visual quality. Our method leverages the extremely compact latent space of StableCascade and introduces low-resolution (LR) semantic guidance for enhanced structure planning and detail synthesis. Consequently, our approach can generate images up to 6K resolution with high visual quality, overcoming the limitations of previous methods.

Figure 2: Illustration of feature distribution disparity across varying resolutions.

## 3 Method

Generating ultra-high-resolution images necessitates complex semantic planning and detail synthesis. We leverage the cascade architecture  for its highly compact latent space to streamline this process, as illustrated in Figure 3. Initially, we generate a low-resolution (LR) image and extract its inner features during synthesis as semantic and structural guidance for high-resolution (HR) generation. To enable our model to produce images at various resolutions, we learn implicit neural representations (INR) of LR and adapt them to different sizes continuously. With this guidance, the HR branch, aided by scale-aware normalization layers, generates multi-resolution latents. These latents then undergo a cascade diffusion and VAE decoding process, resulting in the final images. In Section 3.1, we detail the extraction and INR upscaling of LR guidance. Section 3.2 outlines strategies for fusing LR guidance and adapting our model to various resolutions.

### Low-Resolution Guidance Generation

To address the challenges of high-resolution image synthesis, Previous studies [42; 17] have often employed a progressive strategy, initially generating a low-resolution image and then applying diffusion-based super-resolution techniques. Although this method improves image quality, the diffusion process in the pixel space remains resource-intensive. The cascade architecture , achieving a 42:1 compression ratio, offers a more efficient approach to this problem.

**Guidance extraction.** Instead of relying solely on the final low-resolution output, we introduce multi-level internal model representations of the low-resolution process to provide guidance. This strategy is inspired by evidence suggesting that representations within diffusion generative models encapsulate extensive semantic information [49; 2; 31]. To optimize training efficiency and stability, we leverage features in the later stage, which delineate clearer structures compared to earlier stages. This approach ensures that the high-resolution branch is enriched with detailed and coherent semantic guidance, thereby enhancing visual quality and consistency. During training, the high-resolution image (_e.g._, 4096 \(\) 4096) is first down-sampled to the base resolution (1024 \(\) 1024), then encoded

Figure 4: Illustration of continuous upscaling by implicit neural representation.

Figure 3: Method Overview. Initially, we extract guidance from the low-resolution (LR) image synthesis process and upscale it by learning an implicit neural representation. This upscaled guidance is then integrated into the high-resolution (HR) generation branch. The generated HR latent undergoes a cascade decoding process, ultimately producing a high-resolution image.

to a latent \(_{0}^{L}\) (24 \(\) 24) and corrupted with Gaussian noise as

\[q(_{t}^{L}|_{0}^{L}):=(_{t}^{L};}}_{0}^{L},(1-_{t}))\,,\] (1)

where \(_{t}:=1-_{t},}}:=_{s=0}^{t}_{s}\), and \(_{t}\) is the pre-defined variance schedule for the diffusion process. We then feed \(_{t}^{L}\) to the denoising network and obtain multi-level features after the attention blocks, denoted as the guidance features \(\).

**Continuous upsampling.** Note that the guidance features \(\) are at the base resolution (24 \(\) 24), while the HR features vary in size. To enhance our network's ability to utilize the guidance, we employ implicit neural representations , which allow us to upsample the guidance features to arbitrary resolutions. This approach also mitigates noise disturbance in the guidance features, ensuring effective utilization of their semantic content. As shown in Figure 4, we initially perform dimensionality reduction on the LR guidance tokens via linear operations for improved efficiency and concatenate them with a set of learnable tokens. These tokens undergo multiple self-attention layers, integrating information from the guidance features. Subsequently, the updated learnable tokens are processed through multiple linear layers to generate the implicit function weights. By inputting target position values into the implicit function, we obtain guidance features \(^{}\) that matches the resolution of the HR features.

### High-Resolution Latent Generation

The high-resolution latent generation is also conducted in the compact space (_i.e._, 96 \(\) 96 latent for a 4096 \(\) 4096 image with a ratio of 1:42), significantly enhancing computational efficiency. Additionally, the high-resolution branch shares most of its parameters with the low-resolution branch, resulting in only a minimal increase in additional parameters. In detail, to incorporate LR guidance, we integrate several fusion modules. Furthermore, we implement resolution-aware normalization layers to adapt our model to varying resolutions.

**Guidance fusion.** After obtaining the guidance feature \(^{}\), we fuse it with the HR feature \(\) as follows:

\[^{}=((,^{ }))+\,.\] (2)

The fused HR feature \(^{}\) is further modulated by the time embedding \(_{t}\) to determine the extent of LR guidance influence on the current synthesis step:

\[^{}=(^{})_{1}(_{t})+_{2}(_{t})+^{ }\,.\] (3)

With such semantic guidance, our model gains an early understanding of the overall semantic structure, allowing it to fuse text information accordingly and generate finer details beyond the LR guidance, as illustrated in Figure 8.

**Scale-aware normalization.** As illustrated in Figure 2, changes in feature resolution result in corresponding variations in model representations. Normalization layers trained at a base resolution struggle to adapt to higher resolutions, such as 4096 \(\) 4096. To address this challenge, we propose resolution-aware normalization layers to enhance model adaptability. Specifically, we derive the scale embedding \(_{s}\) by calculating \(_{N^{H}}N^{L}\), where \(N^{H}\) denotes the number of pixels in the HR

Figure 5: Architecture details of generative diffusion model.

features (_e.g.,_ 96 \(\) 96) and \(N^{L}\) corresponds to the base resolution (24 \(\) 24). This embedding is then subjected to a multi-dimensional sinusoidal transformation, akin to the transform process used for time embedding. Finally, we modulate the HR feature \(\) as follows:

\[^{}=()_{1}( _{s})+_{2}(_{s})+\,.\] (4)

The training objective of the generation process is defined as:

\[L:=_{t,_{0},(0,1)}[||_{ ,^{}}(_{t},s,t,)-||_{2}]\,,\] (5)

where \(s\) and \(\) denote scale and LR guidance, respectively. The parameters \(\) of the main generation network are fixed, while newly added parameters \(^{}\) including INR, guidance fusion, and scale-aware normalization are trainable.

## 4 Experiments

### Implementation Details

We train models on 1M images of varying resolutions and aspect ratios, ranging from 1024 to 4608, sourced from LAION-Aesthetics , SAM , and self-collected high-quality dataset. The training is conducted on 8 A100 GPUs with a batch size of 64. Using model weight initialization from 1024 \(\) 1024 StableCascade , our model requires only 15,000 iterations to achieve high-quality results. We employ the AdamW optimizer  with a learning rate of \(0.0001\). During training, we use continuous timesteps in \(\) as , while LR guidance is consistently corrupted with noise at timestep \(t=0.05\). During inference, the generative model uses 20 sampling steps, and the diffusion decoding model uses 10 steps. We adopt DDIM  with a classifier-free guidance  weight of 4 for latent generation and 1.1 for diffusion decoding. Inference time is evaluated with a batch size of 1.

### Comparison to State-of-the-Art Methods

**Compared methods**. We compare our method with competitive high-resolution image generation methods, categorized into training-free methods (ElasticDiffusion , ScaleCrafter , Fouriscale , Demofusion ) and training-based methods (Pixart-\(\), DALL-E 3 , and Midjourney V6 ). For models that can only generate \(1024 1024\) images, we use a representative image super-resolution method  for upsampling. We comprehensively evaluate the performance of our model at resolutions of \(1024 1792\), \(2048 2048\), \(2160 3840\), \(4096 2048\), and \(4096 4096\). For a fair comparison, we use the official implementations and parameter settings for all methods. Considering the slow inference time (tens of minutes to generate an ultra-high-resolution image) and the heavy computation of training-free methods, we compute all metrics using 1K images.

**Benchmark and evaluation**. We collect 1,000 high-quality images with resolutions ranging from 1024 to 4096 for evaluation. We focus primarily on the perceptual-oriented PickScore , which is trained on a large-scale user preference dataset to determine which image is better given an image pair with a text prompt, showing impressive alignment with human preference. Although FID  and Inception Score  (IS) may not fully assess the quality of generated images [25; 3], we report these metrics following common practice. It is important to note that both FID and IS are calculated on down-sampled images with a resolution of \(299 299\), making them unsuitable for evaluating

Figure 6: Win rate of our UltraPixel against competing methods in terms of PickScore .

[MISSING_PAGE_FAIL:7]

**LR guidance.** Figure 8 visually demonstrates the effectiveness of LR guidance. The synthesized HR result without LR guidance exhibits noticeable artifacts, with a messy overall structure and darker color tone. In contrast, the HR image generated with LR guidance is of higher quality, for instance, the characters "_accepted_" on the sweater and the details of the fluffy head are more distinct. Visualization of attention maps reveals that the HR image generation process with LR guidance shows clearer structures earlier. This indicates that LR guidance provides strong semantic priors for HR generation, allowing the model to focus more on detail refinement while maintaining better semantic coherence. Additionally, Figure 9 compares our method to the post-processing super-resolution strategy, demonstrating that UltraPixel can generate more visually pleasing details.

Figure 7: Visual Comparison of our UltraPixel and other methods. Our method produces images of ultra-high resolution with enhanced details and superior structures. More visual examples are provided in the appendix.

**Timesteps of LR guidance extraction.** We analyze the effect of timesteps used to extract LR guidance in Table 2 and Figure 10. We consider three cases: \(t=t^{H}\), where LR guidance is synchronized with the HR timesteps; \(t=0.5\), representing a fixed guidance at the middle timestep; and \(t=0.05\), near the end. The results show that \(t=t^{H}\) produces a poor CLIP score. This can be attributed to the necessity of providing semantic structure guidance early on, but the LR guidance is too noisy at this stage to be useful. Similarly, \(t=0.5\) also results in noisy LR guidance, as seen in Figure 8. Conversely, \(t=0.05\) provides the best performance since features in the later stage of generation exhibit much clearer structural information. With semantics-rich guidance, HR image generation can produce coherent structures and fine-grained details, yielding higher scores in Table 2.

**Implicit neural representation (INR).** To incorporate multi-resolution capability into our model, we adopt an INR design to continuously provide informative semantic guidance. In Table 3, we compare continuous INR upsampling (dubbed "INR") with directly upsampling LR guidance using bilinear interpolation followed by convolutions (denoted as"BI + Conv"). The results show that INR yields better semantic alignment and image quality, as it provides consistent guidance of LR features across varying resolutions. Figure 11 further illustrates that directly upsampling LR guidance introduces significant noise into the HR generation process, resulting in degraded visual quality.

**Scale-aware normalization.** As illustrated in Figure 2, features across different resolutions vary significantly. To generate higher-quality results, we propose scale-aware normalization (SAN). Table 3 compares the performance of models with ("INR + SAN") and without ("INR") this design. When scaling the resolution from \(2560 2560\) to \(4096 4096\), the CLIP score gap noticeably enlarges, indicating better textual alignment with SAN. Additionally, the Inception Score shows significant improvement when adopting SAN, validating the effectiveness of our design.

Figure 8: Ablation study on LR guidance. Leveraging the semantic guidance from LR features allows the HR generation process to focus on detail refinement, improving visual quality. Text prompt: _In the forest, a British shorthair cute cat wearing a yellow sweater with “Accepted” written on it. A small cottage in the background, high quality, photorealistic, 4k._

Figure 9: Visual comparison with super-resolution method BSRGAN  at resolution of \(4096 4096\). Super-resolution has limited ability to refine the details of the low-resolution image, while our method is capable of generating attractive details.

[MISSING_PAGE_FAIL:10]

Acknowledgments

This work is supported by the Guangzhou-HKUST(GZ) Joint Funding Program (No. 2023A03J0671), the Guangzhou Municipal Science and Technology Project (Grant No. 2024A04J4230), Guangdong Provincial Key Lab of Integrated Communication, Sensing and Computation for Ubiquitous Internet of Things(No.2023B1212010007), and the National Natural Science Foundation of China (Project No. 61902275).