# How Good is a Single Basin?

Kai Lion

Dept of Computer Science

ETH Zurich

kalion@student.ethz.ch

&Gregor Bachmann

Dept of Computer Science

ETH Zurich

gregor.bachmann@inf.ethz.ch

&Lorenzo Noci

Dept of Computer Science

ETH Zurich

lorenzo.noci@inf.ethz.ch &Thomas Hofmann

Dept of Computer Science

ETH Zurich

thomas.hofmann@inf.ethz.ch

###### Abstract

The multi-modal nature of neural loss landscapes is often considered to be the main driver behind the empirical success of deep ensembles. In this work, we probe this belief by constructing various "connected" ensembles which are restricted to lie in the same basin. Through our experiments, we demonstrate that increased connectivity indeed negatively impacts performance. However, when incorporating the knowledge from other basins implicitly through distillation, we show that the gap in performance can be mitigated by re-discovering (multi-basin) deep ensembles in a single basin. Thus, we conjecture that while the extra-basin knowledge is at least partially present in any given basin, it cannot be easily harnessed without learning it from other basins.

## 1 Introduction

Deep neural networks coupled with first order stochastic optimizers give rise to many intriguing charateristics. For instance, two training runs based on different random initializations and batch orderings end up in vastly different "basins" in the landscape, i.e. regions of low loss that are separated by a high barrier (Choromanska et al., 2015). Such minimizers not only differ in location but also in terms of the function they represent; ensembling multiple minimisers strictly improves over the individual performances (Lakshminarayanan et al., 2017). The success of such _deep ensembles_ is widely attributed to the described multi-modal nature of the landscape, where functions from different basins exhibit high predictive diversity which in turn leads to better performance.

We challenge this notion by constructing variants of deep ensembles that are constrained to a single basin. We design several variants of varying degrees of connectivity and observe the following: (1) Ensemble performance indeed decreases as the degree of connectivity of the models increases. (2) Leveraging knowledge from other basins through distillation (Hinton et al., 2015) can break this trend, leading to the discovery of very performant single-basin ensembles that almost match the performance of standard deep ensembles. A single basin hence does contain very diverse models; however, they might be difficult to find without additional knowledge. In summary, we make the following contribution:

* We design a rich set of connected ensembles and characterize a trade-off between diversity and connectivity.
* We show that implicitly incorporating knowledge from other basins allows us to design strong connected ensembles that significantly close the gap in performance to deep ensembles. We thus demonstrate that a single basin could suffice for the construction of diverse ensembles.

Setting

We consider image classification problems with a training dataset \(=\{(_{1},y_{1}),,(_{n},y_{n})\}\) consisting of \(n\) i.i.d tuples of labelled examples \(_{i}^{d}\) and \(y_{i}\{1,,K\}\). We study the class of neural network functions \(f_{}:^{d}^{K}\) parameterized by \(^{p}\) where \(\) denotes the concatenation of all the parameters. We learn \(\) through empirical risk minimization \(_{}_{i=1}^{n}(f_{}(_{i}),y_{i})\) with \(:^{K}\{1,,K\}^{+}\) denoting the loss function. To approximately minimize this objective, we use some form of stochastic gradient descent and refer to a minimizer \(\) as a _mode_. Such a mode is located in a _loss basin_, referring to the approximately convex region of low loss around it.

Deep Ensembles.We consider \(M\) runs of SGD under different initializations and batch orderings, resulting in a set of minimizers \(\{_{1},,_{M}\}\). Due to the non-convexity of neural landscapes, simple convex combinations \(}=_{i=1}^{M}_{i}_{i}\) with \(_{i=1}^{M}_{i}=1\) do not constitute minimizers of the test loss. In the literature this is often referred to as lack of (joint) linear mode-connectivity (Garipov et al., 2018). While averaging parameters proves detrimental, averaging the predictions (i.e. ensembling) leads to a substantially more powerful model, i.e. \(():=_{i=1}^{M}f_{_{i}}()\) outperforms any individual model \(_{i}\).

## 3 Exploring a Single Basin

Connected EnsemblesWe now explore techniques to replicate the success of deep ensembles, intentionally restricting ourselves to a single basin. In other words, we aim to construct a _connected_ ensemble \(\{}_{1},,}_{M}\}\) that matches the performance of the original ensemble while at the same time guaranteeing linear mode-connectivity. We provide a visualization of the idea in Fig. 1. We focus on _Residual Networks_ (ResNets) (He et al., 2016) in the main text, but also evaluate our more competitive methods on _Vision Transformers_ (ViTs) (Dosovitskiy et al., 2021) in the Appendix.

Connectivity.In order to assess connectivity of a given set of models \(\{_{1},,_{M}\}\), we measure

\[()=(_{i=1}^{M}_{i}_{i})-_{i=1}^{M}_{i}(_{i }), 14.226378pt()\]

where \(:\) maps a configuration \(\) to its generalization accuracy \(()\). W randomly sample convex combination weights \(^{M}\) from a Dirichlet distribution. We then average multiple draws of \(\) to obtain \(\) and say that \(\{_{1},,_{M}\}\) are jointly linearly-connected if \(\) does not decrease significantly below zero.

Stochastic Weight Ensembling (SWE)As a very simple first baseline, we consider a variant of _stochastic weight averaging_ (SWA) (Izmailov et al., 2018), where instead of averaging the obtained iterates, we average the predictions, effectively forming an ensemble. More precisely, we train a _ResNet20_ for \(T\) epochs with a decaying learning rate, producing the first sample \(}_{1}\) and then continue training with a constant learning rate, saving a checkpoint \(}_{i}\) every \(T\) epochs until we collected \(M\) samples. This ensures the same computational budget as the deep ensemble. We report the resulting test performance and joint connectivity values in Table 1. We also display the corresponding values for deep ensembles as a reference. We observe that SWE is surprisingly effective, matching the performance of the deep ensemble on CIFAR10 while maintaining a high degree of connectivity. On the more challenging task of CIFAR100 however, we find a significant gap of roughly \(3\%\) in test accuracy.

Constrained Ensembles (Con. Ens.)We leverage the insights of Frankle et al. (2020) about the stability of SGD; Along the training trajectory \(\{^{(t)}:t T\}\) of SGD, there exists a point \(^{(t)}\) after which any subsequently started SGD run with a different batch ordering ends up in the same loss basin. Surprisingly, this time point \(t\) can be as early as a few epochs in training. This offers a recipe for the following family of connected ensembles; (1) Train a model up to time \(t\), (2) use \(^{(t)}\) as a starting point for \(M\) runs of SGD, and (3) continue training for \(T-t\) epochs with different batch orderings, leading to a _constrained_ ensemble \(\{}_{1}(t),,}_{M}(t)\}\). Again, we use the same computational budget as a deep ensemble. We notice that the time parameter \(t\) intuitively balances diversity and connectivity; a smaller \(t\) yields more diverse but less connected solutions, while conversely, a larger \(t\) leads to greater connectivity at the expense of lower diversity. We display the resulting performance and connectivity results in Table 1. We obtain a very similar picture as for SWE, i.e. constrained ensembles also match the performance on CIFAR10, offer strong connectivity, but fall short on CIFAR100, albeit with a significant improvement.

## 4 Re-discovering Deep Ensembles in a Single Basin

Our preliminary results lead us to conclude that discovering a connected deep ensemble with matching performance is a challenging endeavour. We thus take a step back and revisit our research question from a slightly different angle:

_If access to a deep ensemble was granted, could one re-discover it in a single basin?_

This is conceptually a simplified goal as knowledge of other basins can now be leveraged to guide the search within a single basin. A positive answer however would still be very impactful as it proves the existence of a connected deep ensemble, motivating further research into efficient exploration of a single basin. In the following approaches, we will thus assume that we have access to a deep ensemble \(\{_{1},,_{M}\}\).

Permuted Ensembles (Perm. Ens.)We first investigate the PermutationCoordinateDescent (PCD) algorithm from Ainsworth et al. (2023). We choose the first member \(_{1}\) as a reference model and we aim to apply permutations \(_{i}\) to each remaining member \(_{i}\) such that \(_{1}\) and \(_{i}(_{i})\) live in the same basin. Such a permutation is discovered by aligning the weights of each member with the reference model, we refer to Ainsworth et al. (2023) for more details. Since permutations constitute a symmetry of neural networks, the performance of the new members \(_{i}(_{i})\) remains unchanged, and we thus have a mathematical guarantee to achieve the same accuracy as the original ensemble. But this guarantee comes at a cost; the degree of pairwise connectivity between two permuted members \(_{i}(_{i})\) and \(_{j}(_{j})\) can vary significantly, as illustrated by the wide confidence intervals in Fig. 1(a) and 1(b). Similarly, joint connectivity is also violated as shown in Table 1. This is not surprising as the objective only optimizes for pairwise alignment. We further show in Appendix A that optimising for "joint" alignment of models does not achieve joint connectivity either.

Distilled Ensembles. (Dist. Ens.)In this approach, we combine our insights from _constrained_ ensembles with the mechanism of model distillation, as introduced by Hinton et al. (2015). Again denote by \(_{1}^{(t)}\) the stability point of SGD for the reference model \(_{1}\). We aim to re-discover the \(j\)-th member \(_{j}\) in the same basin as \(_{0}\) by minimizing a distillation objective towards \(_{j}\), i.e. we

    & & Deep Ens. & Con. Ens. & SW Ens. & Perm. Ens. & Dist. Ens. & Dist. Deep Ens. \\   & Acc & \(94.43_{ 0.12}\) & \(94.17_{ 0.05}\) & \(94.0_{ 0.18}\) & \(94.43_{ 0.12}\) & \(94.46_{ 0.20}\) & \(94.45_{ 0.02}\) \\  & \(\) & \(-71.74_{ 2.38}\) & \(-0.10_{ 0.10}\) & \(1.48_{ 0.04}\) & \(-25.84_{ 4.20}\) & \(-0.14_{ 0.07}\) & \(-55.7_{ 1.71}\) \\   & Acc & \(78.15_{ 0.10}\) & \(75.92_{ 0.20}\) & \(74.95_{ 0.49}\) & \(78.15_{ 0.10}\) & \(77.56_{ 0.18}\) & \(78.42_{ 0.20}\) \\  & \(\) & \(-68.16_{ 1.72}\) & \(0.86_{ 0.18}\) & \(3.30_{ 0.10}\) & \(-44.89_{ 0.91}\) & \(0.39_{ 0.11}\) & \(-48.32_{ 0.15}\) \\   & Acc & \(62.85_{ 0.12}\) & \(59.8_{ 0.1}\) & \(58.36_{ 0.6}\) & \(62.85_{ 0.12}\) & \(62.61_{ 0.43}\) & \(63.29_{ 0.33}\) \\  & \(\) & \(-53.78_{ 0.85}\) & \(0.75_{ 0.10}\) & \(2.80_{ 0.14}\) & \(-46.30_{ 2.08}\) & \(-1.35_{ 0.48}\) & \(-35.79_{ 0.77}\) \\   

Table 1: Comparison of ResNet20 ensemble accuracy (Acc) and connectivity \(\) (in percent) for all ensemble variants on CIFAR10 (C-10), CIFAR100 (C-100), and Tiny ImageNet (T-IN).

minimize

\[()=_{i=1}^{n}(1-)^{2}( (_{j}}(_{i})}{}),( }(_{i})}{}))-([ (f_{}(_{i}))]_{y_{i}})\] (1)

where \(\) and \(\) are additional hyperparameters and \(\) is the softmax function. We then start the optimization from \(_{1}^{(t)}\) to encourage connectivity of solutions and denote the minimizers of Eq. 1 by \(}_{1\,\,j}\). \(\) trades-off the optimization towards matching the ground truth and functional similarity to the \(j\)-th member. Note that for \(=1\), the approach essentially reduces to _constrained_ ensembles. \(>1\) is a temperature scaling parameter used to facilitate the knowledge transfer to the student model. Table 1 illustrates that our distillation strategy with \(=0.2\) produces very competitive ensembles for residual models, significantly closing the gap to standard deep ensembles across all datasets. Moreover, such a distilled ensemble exhibits a surprisingly strong degree of connectivity, not only fulfilling pairwise connectivity (see Fig. 2), but also the more challenging joint linear connectivity property \(\), as shown in Table 1.

Connectivity vs accuracy.In Fig. 1(c) we show test accuracy as a function of connectivity \(\). Without distillation (represented by the red markers), we see that increased connectivity negatively impacts performance. However, as soon as we employ distillation (blue markers), we manage to significantly mitigate the drop in performance without compromising connectivity. We provide further experiments on the impact of connectivity on performance and diversity in Appendix A.

Regularizing effect of distillation.Zhang et al. (2019) demonstrated that distillation can enhance student performance beyond that of its teacher. Thus, the improvement of distilled ensembles over constrained ensembles observed in Table 1 might be due by this regularizing effect of distillation. To isolate this effect, we consider the baseline of deep ensembles trained with the same distillation objective from Eq. 1. If the gain in ensemble performance observed for distilled ensembles would be primarily due to the regularizing effect of distillation rather than the incorporation of out-of-basin knowledge, then it is reasonable to expect similar improvements in ensemble performance when adding the distillation term to deep ensembles. As illustrated in Table 4 in Appendix A, distillation does not significantly improve ensemble performance for ordinary deep ensembles, highlighting that the gains are unlikely to be caused by the regularizing effect of distillation.

## 5 Discussion

In this work, we explored various approaches to construct ensembles constrained to lie in a single basin. We observe that constructing such a connected ensemble without any knowledge from other basins proves to be difficult and a significant gap to deep ensembles remains. Moreover, we observe a pronounced trade-off between (joint) linear mode-connectivity and the resulting ensemble performance. However, when incorporating other basins implicitly through a distillation procedure we manage to break this trade-off and strongly reduce this gap, producing connected ensembles that are (almost) on-par with deep ensembles. While relying on other basins renders our approach

Figure 2: Pairwise connectivity for ResNet20 ensembles in (a) and (b). Averaged over five randomly selected pairs and repeated for three random seeds, totalling 15 pairs. Connectivity vs test accuracy in (c). The dashed horizontal line shows the accuracy of a deep ensemble, while the dotted horizontal line shows the mean member accuracy.

very inefficient, it nevertheless demonstrates the existence of very performant ensembles in a single basin, requiring us to rethink the characteristics of loss landscapes. The existence of strong connected ensembles illustrates that, in principle, the functional diversity within a single basin is sufficient to achieve predictive performance that is comparable to an ensemble sampled from different modes. In other words, our results illustrate that escaping the basin is not a prerequisite for attaining competitive prediction accuracy. We hope that our insights can guide future work towards designing algorithms that more thoroughly and efficiently explore a single basin.