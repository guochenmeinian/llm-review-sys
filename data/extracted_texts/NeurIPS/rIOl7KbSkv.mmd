# No Free Lunch in LLM Watermarking:

Trade-offs in Watermarking Design Choices

 Qi Pang Shengyuan Hu Wenting Zheng Virginia Smith

Carnegie Mellon University

{qipang, shengyuanhu, wenting, smithv}@cmu.edu

###### Abstract

Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. _Watermarking_, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating the misuse of such AI-generated content. However, we show that common design choices in LLM watermarking schemes make the resulting systems surprisingly susceptible to attack--leading to fundamental trade-offs in robustness, utility, and usability. To navigate these trade-offs, we rigorously study a set of simple yet effective attacks on common watermarking systems, and propose guidelines and defenses for LLM watermarking in practice.

## 1 Introduction

Modern generative modeling systems have notably enhanced the quality of AI-produced content . For example, large language models (LLMs) like those powering Chat-GPT  can generate text closely resembling human-crafted sentences. While this has led to exciting new applications of machine learning, there is also growing concern around the potential for misuse of these models, leading to a flurry of recent efforts on developing techniques to detect AI-generated content. A promising approach in this direction is to embed invisible _watermarks_ into model-derived content, which can then be extracted and verified using a secret watermark key .

In this work, we identify that many of the key properties that make existing LLM watermarks successful can also render them susceptible to attack. In particular, we study a number of simple attacks that take advantage of common design choices of existing watermarking schemes, including:

1. **Robustness** of the watermarks to potential modifications in the output text, so that the watermarks cannot be easily removed ;
2. The use of **multiple keys** to prevent against watermark stealing attacks ;
3. **Public detection APIs**, which allow the general public to easily verify whether or not candidate text is AI-generated .

While these common features and design choices of existing watermarking schemes have clear benefits, we show that they also make the resulting systems vulnerable to a number of simple but effective attacks. In particular, we study two types of attacks: 1) _watermark-removal attacks_, which remove the watermark from the watermarked content, and 2) _spoofing attacks_, which create (potentially toxic) content with a target watermark embedded, thus making the content appear to be generated by a specific LLM when it is in fact not (see motivating examples in Table 1).

Our work rigorously explores a number of simple removal and spoofing attacks for LLM watermarks. In doing so, we identify critical trade-offs that emerge between watermark robustness, utility, and usability as a result of watermarking design choices. To navigate these trade-offs, wepropose potential defenses as well as a set of general guidelines to better enhance the security of next-generation LLM watermarking systems. Overall, we make the following contributions:

* We study how watermark _robustness_, despite being a desirable property to mitigate removal attacks, can make the resulting systems highly susceptible to _piggyback spoofing attacks_, a simple type of attack that makes makes watermarked text toxic or inaccurate through small modifications, and show that challenges exist in detecting these attacks given that a single token can render an entire sentence inaccurate (Sec. 4).
* We show that using _multiple watermarking keys_ can make the system susceptible to _watermark removal attacks_ (Sec. 5). Although a larger number of keys can help defend against watermark stealing attacks, which can be used to launch either spoofing or removal attacks, we show both theoretically and empirically that this in turn increases the potential for watermark removal attacks.
* Finally, we identify that _public watermark detection APIs_ can be exploited by attackers to launch both _watermark-removal and spoofing attacks_ (Sec. 6). We propose a defense using techniques from differential privacy to effectively counteract spoofing attacks, showing that it is possible to avoid the possibilities of noise reduction by applying pseudorandom noise based on the input.

Throughout, we explore our attacks on three state-of-the-art watermarks [16; 45; 19] and two LLMs (LLAMA-2-7B  and OPT-1.3B )--demonstrating that these vulnerabilities are common to existing LLM watermarks, and providing caution for the field in deploying current solutions in practice without carefully considering the impact and trade-offs of watermarking design choices. Our code is available at https://github.com/Qi-Pang/LLM-Watermark-Attacks.

## 2 Related Work

Advances in large language models (LLMs) have given rise to increasing concerns that such models may be misused for purposes such as spreading misinformation, phishing, and academic cheating.

} 
**Prompt** & Alan Turing was born in... \\ 
**Unwatermarked** & _Iian Turing was born in 1912 and died in 1964. He was an English \\ Z-Score: \(0.16\) & mathematician, logician, cryptanalysis, and computer scientist. In 1938, Turing joined the Government Code and Cypher School (GCROS), where he contributed to the design of the bothe, a machine that was used to decipher the Enigma-enciphered messages..._ \\ 
**Watermarked** & _Iian Turing was born in 1912 and died in 1964, at the age of 41. He was the brilliant British scientist and mathematician who is largely credited with being the father of modern computer science. He is known for his contributions to mathematical biology and chemistry. He was also one of the pioneers of computer science..._ \\ 
**(a) Piggyback spoofing attack** & _Iian Turing was born in 1950 and died in 1994, at the age of 43. He was the brilliant American scientist and mathematician who is largely credited with being the father of modern computer science. He is known for his contributions to mathematical biology and musicology. He was also one of the pioneers of computer science..._ \\ 
**(b) Watermark-removal attack** & _Iian Turing was born in 1912 and died in 1964. He was a mathematician, logician, cryptologist and theoretical computer scientist. He is famous for his work on code-breaking and artificial intelligence, and his contribution to the Allied victory in World War II. Turing was born in London. He showed an interest in mathematics..._ \\ 
**(c) Watermark-removal attack** & _Iian Turing was born in 1912 and died in 1964. He was an English \\ Exploiting public detection API & mathematician, computer scientist, cryptanalyst and philosopher. Turing was a leading mathematician and cryptanalyst. He was one of the key players in cracking the German Enigma Code during World War II. He also came up with the Turing Machine..._ \\  

Table 1: Examples generated using LLAMA-2-7B with/without the KGW watermark  under various attacks. We mark tokens in the green and red lists (see Appendix C). Z-score reflects the detection confidence of the watermark, and perplexity (PPL) measures text quality. (a) In the _piggyback spoofing attack_, we exploit watermark robustness by generating incorrect content that appears as watermarked (matching the z-score of the watermarked baseline), potentially damaging the reputation of the LLM. Incorrect tokens modified by the attacker are marked in orange and watermarked tokens in blue. (b-c) In _watermark-removal attacks_, attackers can effectively lower the z-score below the detection threshold while preserving a high sentence quality (low PPL) by exploiting either the (b) use of multiple keys or (c) publicly available watermark detection API.

In response, numerous recent works have proposed watermarking schemes as a tool for detecting LLM-generated text to mitigate potential misuse [16; 9; 5; 19; 45; 17; 12; 41; 38]. These approaches involve embedding invisible watermarks into the model-generated content, which can then be extracted and verified using a secret watermark key. Existing watermarking schemes share a few natural goals: (1) the watermark should be _robust_ in that it cannot be easily removed; (2) the watermark should not be easily _stolen_, thus enabling spoofing or removal attacks; and (3) the presence of a watermark should be _easy to detect_ when given new candidate text. Unfortunately, we show that existing methods that aim to achieve these goals can in turn enable simple but effective attacks.

**Removal attacks.** Several recent works have highlighted that paraphrasing methods may be used to evade the detection of AI-generated text [18; 13; 20; 21; 43], with [18; 43] demonstrating effective watermark removal using a local LLM. These methods usually require additional training for sentence paraphrasing which can impact sentence quality, or assume a high-quality oracle model to guarantee the output quality is preserved. In contrast, the simple and scalable removal attacks herein do not require additional training or a high-quality oracle. Additionally, our work differs in that we aim to directly connect and study how the inherent properties and design choices of watermarking schemes (such as the use of multiple keys and detection APIs) can inform such removal attacks.

**Spoofing attacks.** Prior works on spoofing use watermark stealing attacks to first estimate the watermark pattern and then embed it into an arbitrary content to launch spoofing attacks. These attacks usually require the attacker to pay a large startup cost by obtaining a significant number of watermarked tokens. For example,  requires 1 million queries to the watermarked LLM, and [14; 10] assume the attacker can obtain millions of watermarked tokens to estimate their distribution. Unlike these works, we explore spoofing attacks that are less flexible but can be launched with significantly less upfront cost. In Sec. 4, we explore a very simple and scalable form of spoofing exploiting the inherent robustness property of watermarks, which we refer to as a 'piggyback spoofing attack'. In Sec. 6, we then explore more general spoofing attacks, which instead of querying the watermarked LLM numerous times, consider exploiting the public detection API. In both, our attacks do not require the attacker to estimate the watermark pattern, but share a similar ultimate goal with the prior spoofing attacks to create falsified inaccurate or toxic content that appears to be watermarked.

## 3 Preliminaries

Before exploring attacks and defenses on watermarking systems, we introduce relevant background on LLMs, notation we use throughout the work, and a set of concrete threat models.

**Notation.** We use **x** to denote a sequence of tokens, \(_{i}\) is the \(i\)-th token in the sequence, and \(\) is the vocabulary. \(M_{}\) denotes the original model without a watermark, \(M_{}\) is the watermarked model, and \(sk\) is the watermark secret key sampled from the key space \(\).

**Language Models.** Current state-of-the-art (SOTA) LLMs are auto-regressive models, which predict the next token based on the prior tokens. We define language models more formally below:

**Definition 1** (\(\)).: _We define a language model (LM) without a watermark as:_

\[M_{}:^{*},\] (1)

_where the input is a sequence of length \(t\) tokens **x**. \(M_{}()\) first returns the probability distribution for the next token \(_{t+1}\) and then the LM samples \(_{t+1}\) from this distribution._

**Watermarks for LLMs.** In this work, we focus on three SOTA decoding-based watermarking schemes: KGW , Unigram  and Exp . Informally, decoding-based watermarks are embedded by perturbing the output distribution of the original LLM. The perturbation is determined by secret watermark keys held by the LLM owner. Formally, we define the watermarking scheme:

**Definition 2** (Watermarked LLMs).: _The watermarked LLM takes token sequence \(^{*}\) and secret key \(sk\) as input, and outputs a perturbed probability distribution for the next token. The perturbation is determined by \(sk\):_

\[M_{}:^{*}\] (2)

The watermark detection outputs the statistical testing score for the null hypothesis that the input token sequence is independent of the watermark secret key, which reflects the watermark confidence:

\[f_{}:^{*}\] (3)

Please refer to Appendix C for additional details of the specific watermarks explored in this work.

### Threat Model

**Attacker's Objective.** We study two types of attacks--watermark-removal attacks and (piggyback or general) spoofing attacks. In the watermark-removal attack, the attacker aims to generate a high-quality response from the LLM _without_ an embedded watermark. For the spoofing attacks, the goal is to generate a harmful or incorrect output that has the victim organization's watermark embedded. We present concrete application scenarios for attacker's motivations in Appendix B.

**Attacker's Capabilities.** We study attacks by exploiting three common design choices in watermarks: 1) robustness, 2) the use of multiple keys, and 3) public detection APIs. Each attack requires the adversary to have different capabilities, but we make assumptions that are practical and easy to achieve in real-world deployment scenarios.

1) For piggyback spoofing attacks exploiting _robustness_ (Sec. 4), we assume that the attacker can make \((1)\) queries to the target watermarked LLM. We also assume that the attacker can edit the generated sentence (e.g., insert or substitute tokens).

2) For watermark-removal attacks exploiting _the use of multiple keys_ (Sec. 5), we consider the scenario where multiple watermark keys are utilized to embed the watermark, which is a common practice in designing robust cryptographic protocols and is suggested by SOTA watermarks [19; 16] to improve resistance against watermark-stealing attacks [14; 10; 34]. For a sentence of length \(l\), we assume that the attacker can make \((l)\) queries to the watermarked LLM.

3) For the attacks on _detection APIs_ (Sec. 6), we assume that the detection API is available to normal users and the attacker can make \((l)\) queries for a sentence of length \(l\). The detection returns the watermark confidence score (p-value or z-score). For spoofing attacks exploiting the detection APIs, we assume that the attacker can auto-regressively synthesize (toxic) sentences. For example, they can run a local (small) model to synthesize such sentences. For watermark-removal attacks exploiting the detection APIs, we also assume that the attacker can make \((l)\) queries to the watermarked LLM. As is common practice [25; 31] and also enabled by OpenAI's API , we assume that the top 5 tokens at each position and their probabilities are returned to the attackers.

## 4 Attacking Robust Watermarks

The goal of developing a watermark that is robust to output perturbations is to defend against watermark removal, which may be used to circumvent detection schemes for applications such as phishing or fake news generation. Robust watermark designs have been the topic of many recent works [45; 16; 19; 34; 17; 32]. We formally define watermark robustness in the following definition.

**Definition 3** (Watermark robustness).: _A watermark is \((,)\)-robust, given a watermarked text **x**, if for all its neighboring texts within the \(\) editing distance, the probability that the detection fails to detect the edited text is bounded by \(\), given the detection confidence threshold \(T\):_

\[,^{}^{*},\;[f_{detection}( ^{},sk)<T]<, s.t.\;f_{detection}(,sk)  T,\,d(,^{})\]

More robust watermarks can better defend against editing attacks, but this seemingly desirable property can also be easily misused by malicious users to launch simple _piggyback spoofing attacks_--e.g., a small portion of toxic or incorrect content can be inserted into the watermarked material, making it seem like it was generated by a specific watermarked LLM. The toxic content will still be detected as watermarked, potentially damaging the reputation of the LLM service provider. As discussed in Sec. 2, spoofing attacks explored in prior work usually require the attacker to obtain millions of watermarked tokens upfront to estimate the watermark pattern [14; 34; 10]. In contrast, our simple piggyback spoofing only requires a single query to the watermarked LLM with careful text modifications, and the effectiveness relates directly to the robustness of the LLM watermark.

**Attack Procedure.**_(i)_ The attacker queries the target watermarked LLM to receive a high-entropy watermarked sentence \(_{}\), _(ii)_ The attacker edits \(_{}\) and forms a new piece of text \(^{}\) and claims that \(^{}\) is generated by the target LLM. The editing method can be defined by the attacker. Simple strategies could include inserting toxic tokens into the watermarked sentence \(_{}\) at random positions, or editing specific tokens to make the output inaccurate (see example in Table 1). As we show, editing can also be done at scale by querying another LLM like GPT4 to generate fluent output.

We present the formal analysis on the attack feasibility in Appendix D and point out the takeaway that is universally applicable to all robust watermarks: A more robust watermark makes piggybackspoofing attack easier by allowing more toxic tokens to be inserted. This is a fundamental design trade-off: If a watermark is robust, such spoofing attacks are inevitable and may be extremely difficult to detect, as even one toxic token can render the entire content harmful or inaccurate.

### Evaluation

**Experiment Setup.** We assess the effectiveness of our piggyback spoofing attack by using the two editing strategies discussed above. Through toxic token insertion, we study the limits of how many tokens can be inserted into the watermarked content. Using fluent inaccurate editing, we show that piggyback spoofing can generate fluent, watermarked, but inaccurate results at scale. Specifically, for the toxic token insertion, we generate a list of \(200\) toxic tokens and insert them at random positions in the watermarked output. For the fluent inaccurate editing, we edit the watermarked sentence by querying GPT4 using the prompt _"Modify less than 3 words in the following sentence and make it inaccurate or have opposite meanings."_ Unless otherwise specified, in the evaluations of this work, we utilize \(500\) prompts data from OpenGen  dataset, and query the watermarked language models (LLAMA-2-7B  and OPT-1.3B ) to generate the watermarked outputs. We evaluate three SOTA watermarks including KGW , Unigram , and Exp , using the default watermarking hyperparameters. In our experiments, we default to a maximum of 200 new tokens for KGW and Unigram, and 70 for Exp, due to its complexity in the watermark detection. 70 is also the maximum number of tokens the authors of Exp evaluated in their paper .

**Evaluation Result.** We report the maximum portion of the inserted toxic tokens relative to the original watermarked sentence length on LLAMA-2-7B model in Fig. 0(a). We also present the confidence of the OpenAI moderation model  in identifying the content as violating their usage policy  due to the inserted toxic tokens in Fig. 0(a). Our findings show that we can insert a significant number of toxic tokens into content generated by all the robust watermarking schemes, with a median portion higher than \(20\%\), i.e., for a \(200\)-token sentence, the attacker can insert a median of \(40\) toxic tokens into it. These toxic sentences are then identified as violating OpenAI policy rules with high confidence scores, whose median is higher than 0.8 for all the watermarking schemes we study. The average confidence scores for content before attack are around 0.01. The empirical data on the maximum portion of inserted toxic tokens aligns with our analysis in Appendix D. We further validate this analysis in Fig. 5 of Appendix E, showing that attackers can insert nontrivial portions of toxic tokens into the watermarked text to launch piggyback spoofing attacks. Notably, the more robust the watermark is, the more tokens can effectively be inserted. We present the results on OPT-1.3B in Appendix G.

In Fig. 0(b), we report the PPL and watermark detection scores of the piggyback results on KGW and LLAMA-2-7B by the fluent inaccurate editing strategy. We show that we can successfully generate fluent results, with a slightly higher PPL. \(94.17\%\) of the piggyback results have a z-score higher than the default threshold \(4\). We randomly sample \(100\) piggyback results and manually check that most of them (\(92\%\)) are fluent and have inaccurate or opposite content from the original watermarked content. See examples in Appendix F. The results show that we can generate watermarked, fluent, but inaccurate content at scale with an ASR higher than 90%.

### Discussion

Our results highlight that piggyback spoofing attacks are easy to execute in practice. LLM watermarks typically do not consider such attacks during design and deployment, and existing robust

Figure 1: Piggyback spoofing of robust watermarks. (a) We can insert a large number of toxic tokens in robustly watermarked text without changing the watermark detection result, resulting in text that is likely to be identified as toxic. (b) We can use GPT4 to automatically modify watermarked text, making it appear inaccurate while retaining fluency.

watermarks are inherently vulnerable to such attacks. We highlight the contradiction between the watermark robustness and the piggyback spoofing feasibility. We consider this attack to be challenging to defend against, especially considering examples such as those in Table 1 and Appendix F, where by only editing a single token, the entire content becomes incorrect. It is hard, if not impossible, to detect whether a particular token is from the attacker by using robust watermark detection algorithms. Thus, practitioners should weigh the risks of removal vs. piggyback spoofing attacks for the model at hand. A feasible strategy to mitigate spoofing attacks is by requiring proof of digital signatures on the LLM generated content. However, while an attacker without access to the private key cannot spoof, it is worth nothing that this strategy is still vulnerable to watermark-removal attacks, as a single editing can invalidate the original signature.

## 5 Attacking Stealing-Resistant Watermarks

As discussed in Sec. 2, many works have explored the possibility of launching watermark stealing attacks to infer the secret pattern of the watermark, which can then enable spoofing and removal attacks [34; 14; 10]. A natural and effective defense against watermark stealing is using _multiple watermark keys_ during embedding, which is a common practice in cryptography and also suggested by prior watermarks and work in watermark stealing [16; 19; 14]. Unfortunately, we demonstrate that using multiple keys can in turn introduce new watermark-removal attacks.

In particular, SOTA watermarking schemes [16; 9; 5; 19; 45; 17] aim to ensure the watermarked text retains its high quality and the private watermark patterns are not easily distinguished by maintaining an "unbiasedness" property:

\[_{sk}(M_{}(,sk))_{} M_{}(),\] (4)

i.e., the expected distribution of watermarked output over the watermark key space \(sk\) is close to the output distribution without a watermark, differing by a distance of \(\). We note that Exp  is "distortion free" for a single text sample, and KGW  and Unigram  slightly shift the watermarked distributions. We note that stealing attacks won't work on rigorously unbiased watermarks.

The insight of our proposed watermark-removal attack is that given the "unbiasedness" nature of watermarks and considering multiple keys may be used during watermark embedding, malicious users can estimate the output distribution without any watermark by querying the watermarked LLM multiple times using the same prompt. As this attack estimates the original, unwatermarked distribution, the quality of the generated content is preserved.

**Attack Procedure.** An attacker queries a watermarked model with an input **x** multiple times, observing \(n\) subsequent tokens \(_{t+1}\). This is easy for text completion model APIs, and chat model APIs can also be easily attacked by constructing a prompt to ask the chat model to complete a partial sentence without any prefix. The attacker then creates a frequency histogram of these tokens and samples according to the frequency. This sampled token matches the result of sampling on an unwatermarked output distribution with a nontrivial probability. Consequently, the attacker can progressively eliminate watermarks while maintaining a high quality of the synthesized content. We present a formal analysis of the number of required queries in Appendix H.

### Evaluation

**Experiment Setup.** Our watermarks, models and datasets settings are the same as Sec. 4.1. We study the trade-off between resistance against watermark stealing and watermark-removal attacks by evaluating a recent watermark stealing attack . In this attack, we query the watermarked LLM to obtain 2.2 million tokens in total to estimate the watermark pattern and then launch spoofing attacks using the estimated watermark pattern. We follow their assumptions that the attacker can access the unwatermarked tokens' distribution. In our watermark removal attack, we consider that the attacker has observations with different keys. We evaluate the detection scores (z-score or p-value) and the output perplexity (PPL, evaluated using GPT3 ). The detection algorithm returns the maximum detection score across all the keys, which increases the expectation of unwatermarked detection results. Thus, we set the detection thresholds for different keys to keep the false positive rates (FPR) below 1e-3 and report the attack success rates (ASR). We use default watermark hyperparameters.

**Evaluation Result.** As shown in Fig. 1(a), using multiple keys can effectively defend against watermark stealing attacks. With a single key, the ASR is \(91\%\), which matches the results reported in . We observe that using three keys can effectively reduce the ASR to \(13\%\), and using more than 7 keys, the ASR of the watermark stealing is close to zero. However, using more keys also makes the system vulnerable to our watermark-removal attacks as shown in Fig. 1(b). When we use more than \(7\) keys, the detection scores of the content produced by our watermark removal attacks closely resemble those of unwatermarked content and are much lower than the detection thresholds, with ASRs higher than \(97\%\). Fig. 1(c) suggests that using more keys improves the quality of the output content. This is because, with a greater number of keys, there is a higher probability for an attacker to accurately estimate the unwatermarked distribution, which is consistent with our analysis in Appendix H. We observe that in practice, 7 keys suffice to produce high-quality content comparable to the unwatermarked content. These observations remain consistent across various watermarking schemes and models; for additional results see Appendix J. We note that the numbers are not exactly the same as , as we consider a more realistic attacker with less queries to the watermarked LLM.

### Discussion

[noitemsep]

Using a larger number of watermarking keys can defend against watermark stealing attacks, but increases vulnerability to watermark-removal attacks. Limiting users' query rates can help to mitigate both attacks.

Many prior works have suggested using multiple keys to defend against watermark stealing attacks. However, in this study, we reveal that a conflict exists between improving resistance to watermark stealing and the feasibility of removing watermarks. Our evaluation results show that finding a "sweet spot" in terms of the number of keys to use to mitigate both the watermark stealing and the watermark-removal attacks is not trivial. For example, our watermark-removal attack achieves a high ASR of \(36.2\%\) just using three keys, and the corresponding watermark stealing-based spoofing's ASR is \(13.0\%\). Using more keys can decrease the watermark stealing-based spoofing's ASR, but at the cost of making the system more vulnerable to watermark removal and vice-versa. We note that the ASRs with three keys are not negligible, thus limiting the ability of potentially malicious users is necessary in practice to mitigate these attacks. As a practical defense, we evaluate watermark stealing with various query limits on the watermarked LLM, and found that the ASR can be significantly reduced by limiting the attacker's query rate. Detailed results can be found in Appendix J. Given the trade-off that exists, we suggest that LLM service providers consider "defense-in-depth" techniques such as anomaly detection, query rate limiting, and user identification verification.

## 6 Attacking Watermark Detection APIs

It is still an open question whether watermark detection APIs should be made publicly available to users. Although this makes it easier to detect watermarked text, it is a commonly acknowledged that it will make the system vulnerable to attacks  given the existence of oracle attacks [3; 6; 22; 15]. Here, we study this statement more precisely by examining the specific risk trade-offs that exist, as well as introducing a novel defense that may make the public detection API more feasible in practice. In the following sections, we first introduce attacks that exploit the APIs and then propose suggestions and defenses to mitigate these attacks.

### Attack Procedures

**Watermark-Removal Attack.** For the watermark-removal attack, we consider an attacker who has access to the target watermarked LLM's API, and can query the watermark detection results. The at

Figure 2: Spoofing attack based on watermark stealing  and watermark-removal attacks on KGW watermark and LLAMA-2-7B model with different number of watermark keys \(n\). Higher z-score reflects more confidence in watermarking and lower perplexity indicates better sentence quality. The attack success rates are based on the threshold with FPR@1e-3.

tacker feeds a prompt into the watermarked LLM, which generates the response in an auto-regressive manner, similar to how LLMs generate sentences. That is, the attacker will select each token based on the prior tokens and the detection results. For the token \(_{i}\) the attacker will generate a list of possible replacements for \(_{i}\). This list can be generated by querying the watermarked LLM, querying a local model, or simply returned by the watermarked LLM. In this work, we choose the third approach because of its simplicity and guarantee of synthesized sentences' quality. This is a common assumption made by prior works , and such an API is also provided by OpenAI (\(=5\)), which can benefit the normal users in understanding the model confidence, debugging and analyzing the model's behavior, customizing sampling strategies, etc. Consider that the top \(L=5\) tokens and their probabilities are returned to the attackers. The probability that the attacker can find an unwarmarked token in the token candidates' list of length \(L\) is \(1-^{L}\) for KGW and Unigram, which becomes sufficiently large given \(L=5\) and \(=0.5\). The attacker will query the detection using these replacements and sample a token based on their probabilities and detection scores to remove the watermark while preserving a high output quality.

**Spoofing Attack.** Spoofing attacks follow a similar procedure where the attacker can generate (harmful) content using a local model. When sampling the tokens, instead of selecting those that yield low confidence scores as in removal attacks, the attacker will choose tokens that have higher confidence scores upon watermark detection queries. Thanks to the robustness of the LLM watermarks, attackers don't need to ensure every single token carries a watermark; only that the overall detection confidence score surpasses the threshold, thereby treating synthesized content as if generated by the watermarked LLM.

### Evaluation

Experiment Setup.We use the same evaluation setup as in Sec. 4.1 and Sec. 5.1. We evaluate the detection scores for both the watermark-removal and the spoofing attacks. We also report the number of queries to the detection API. Furthermore, for the watermark-removal attack, where the attackers care more about the output quality, we report the output PPL. For spoofing attacks, the attackers' local models are LLAMA-2-7B and OPT-1.3B.

**Evaluation Result.** As shown in Fig. 2(a) and Fig. 2(b), watermark-removal attacks exploiting the detection API significantly reduce detection confidence while maintaining high output quality. For instance, for the KGW watermark on LLAMA-2-7B model, we achieve a median z-score of \(1.43\), which is much lower than the threshold \(4\). The PPL is also close to the watermarked outputs (\(6.17\) vs. \(6.28\)). We observe that the Exp watermark has higher PPL than the other two watermarks. This is because that Exp watermark is deterministic, while other watermarks enable random sampling during inference. Our attack also employs sampling based on the token probabilities and detection scores, thus we can improve the output quality for the Exp watermark.

The spoofing attacks also significantly boost the detection confidence even though the content is not from the watermarked LLM, as depicted in Fig. 2(c). We report the attack success rate (ASR) and the number of queries for both of the attacks in Table 2. The ASR quantifies how much of the generated content surpasses or falls short of the detection threshold. These attacks use a reasonable number of queries to the detection API and achieve high success rate, demonstrating practical feasibility. We observe consistent results on OPT-1.3B, please see Appendix K.

   &  &  \\   & ASR & @queries & ASR & \#queries \\  KGW & \(1.00\) & \(2.42\) & \(0.98\) & \(2.95\) \\  Unigram & \(0.96\) & \(2.66\) & \(0.98\) & \(2.96\) \\  Exp & \(0.96\) & \(1.55\) & \(0.85\) & \(2.89\) \\  

Table 2: The attack success rate (ASR), and the average query numbers per token for the watermark-removal and spoofing attacks exploiting the detection API on LLAMA-2-7B model.

Figure 3: Attacks exploiting detection APIs on LLAMA-2-7B model.

### Defending Detection with Differential Privacy

In light of the issues above, we propose an effective defense using ideas from differential privacy (DP)  to counteract detection API based spoofing attacks. DP adds random noise to function results evaluated on private dataset such that the results from neighbouring datasets are indistinguishable. Similarly, we consider adding Gaussian noise to the distance score in the watermark detection, making the detection \((,)\)-DP , and ensuring that attackers cannot tell the difference between two queries by replacing a single token in the content, thus increasing the hardness of launching the attacks. Considering an attacker can average multiple query results to reduce noise and estimate original scores without DP protection, we propose to calculate the noise based on the random seed generated by a pseudorandom function (PRF) with the sentence to be detected as the input. Specifically, \(=_{sk}()\), where \(sk\) is the secret key held by the detection service. The users without the secret key cannot reverse or reduce the noise in the detection score. Thus, we can successfully mitigate the noise reduction via averaging multiple query results without comprising on utility or protection of the DP defense. In the following, we evaluate the utility of the DP defense and its performance in mitigating the spoofing attacks.

Experiment Setup.Firstly, we assess the utility of DP defense by evaluating the accuracy of the detection under various noise scales. Next, we evaluate the efficacy of the spoofing against DP detection defense using the same method as in Sec. 6.1. We select the optimal noise scale that provides best defense while keeping the drop in accuracy within \(2\%\). We note that for KGW and Unigram watermarks, we add noise to the z-scores. Sensitivity varies with sentence length (e.g., \(=l}\) for replacement editing, where \(l\) is the sentence length, \(h\) is the context width of the watermark, and \(\) is the portion of the tokens in green list). The actual noise scale is proportional to \(\).

Evaluation Result.As shown in Fig. 3(a), with a noise scale of \(=4\), the DP detection's accuracy drops from the original \(98.2\%\) to \(97.2\%\) on KGW and LLAMA-2-7B, while the spoofing ASR becomes \(0\%\) using the same attack procedure as Sec. 6.1. The results are consistent for Unigram and Exp watermarks and OPT-1.3B model as shown in Appendix L, which illustrates that the DP defense has a great utility-defense trade-off, with a negligible accuracy drop and significantly mitigates the spoofing attacks.

### Discussion

The detection API, available to the public, aids users in differentiating between AI and human-created materials. However, it can be exploited by attackers to gradually remove watermarks or launch spoofing attacks. We propose a defense utilizing the ideas in differential privacy, which significantly increases the difficulty for spoofing attacks. However, this method is less effective against watermark-removal attacks that exploit the detection API because attackers' actions will be close to random sampling, which, even though with less success rates, remains an effective way of removing watermarks. Therefore, we leave developing a more powerful defense mechanism against watermark-removal attacks exploiting detection API as future work. Additionally, we note that the attacker may increase the sensitivity of the input sentences by substituting multiple tokens and infer whether these tokens are in the green list or not to launch the spoofing attack, but this will require much more queries to the detection API. We recommend companies providing detection services

Figure 4: Evaluation of DP detection on KGW watermark and LLAMA-2-7B model. **(a).** Spoofing attack success rate (ASR) and detection accuracy (ACC) without and with DP watermark detection under different noise parameters. **(b).** Z-scores of original text without attack, spoofing attack without DP, and spoofing attacks with DP. We use the best \(=4\) from **(a)**.

adopt a defense-in-depth approach [2; 8]. For instance, they should detect and curb malicious behavior by limiting query rates from potential attackers, and also verify the identity of the users to protect against Sybil attacks.

## 7 Discussion, Limitation & Future Work

**Generalizability of our attacks.** We focus on three SOTA PRF-based robust watermarks, which are a natural set to explore given their popularity and formal guarantees. There are other watermarks like the semantics-based watermarks [23; 33]. While attacking semantics-based watermarks is outside the scope of our study, we deem this an interesting future direction to explore.

Recently, researchers have also proposed signature-based publicly detectable watermarks  to mitigate the spoofing attacks by exploiting robustness. Unlike the watermarks we study, these watermarks usually have weaker robustness guarantees, which further highlights the trade-offs between robustness and vulnerability to spoofing attacks, as we have discussed in Sec. 4.

Our findings, such as exploiting robustness properties and publicly available detection APIs, can also be generalized to image watermarks [39; 42]. The attackers must integrate domain-specific constraints to ensure that the generated sentences or images are meaningful and high-quality. We deem studying the fundamental trade-offs for image watermarks a promising future direction.

**Trade-offs of watermark context width.** There are two effective strategies to mitigate the watermark stealing attacks for the KGW watermark : 1) using a larger context width \(h\) and 2) using multiple watermark keys. In this work (Sec. 5), we primarily explore the fundamental trade-offs in using multiple watermark keys, which prior work has underexplored. Trade-offs in context widths were discussed in previous work [14; 16; 45]. Using a larger \(h\) increases resistance against watermark stealing but reduces robustness. Recent work  shows successful watermark stealing even with \(h=4\). Using multiple keys, as shown in Sec. 5 of our paper, mitigates stealing attacks but introduces new attack vectors of watermark removal. Our attacks will work under different choices of context width, as we exploit properties or design choices orthogonal to the context width. To demonstrate this point, we provide more experimental results in Appendix M.

**The influence of how detection proceeds with multiple keys.** For the scenarios where multiple keys are used, we consider the detector using min/max aggregation to obtain the detection score. More robust aggregations exist including the Harmonic mean p-value . We note that our watermark-removal attack exploiting the use of multiple keys is not dependent on the aggregation method as we do not rely on the server's watermark detection. However, the trade-off analysis and the sweet spot for the number of keys may slightly change given the different detection performance.

**Changing to p-values in KGW and Unigram.** P-values are used for Exp  watermark in our paper, and the observations are consistent with KGW  and Unigram . We expect no impact on results from this change since p-values are monotonic to z-scores. To ease the figures' presentation, we adopt the z-statistics in the main paper, we present more results of using p-values in Appendix M.

## 8 Conclusion

In this work, we reveal new attack vectors that exploit common features and design choices of LLM watermarks. In particular, while these design choices may enhance robustness, resistance against watermark stealing attacks, and public detection ease, they also allow malicious actors to launch attacks that can easily remove the watermark or damage the model's reputation. Based on the theoretical and empirical analysis of our attacks, we suggest guidelines for designing and deploying LLM watermarks along with possible defenses to establish more reliable LLM watermark systems.