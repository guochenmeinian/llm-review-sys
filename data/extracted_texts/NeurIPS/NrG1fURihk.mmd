# A representation-learning game for classes of prediction tasks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We introduce a formulation for learning dimensionality-reducing representations of unlabeled feature vectors, when a prior knowledge on future prediction tasks is available. The formulation is based on a three-player game, in which the first player chooses a representation, the second player then adversarially chooses a prediction task, and the third player predicts the response based on the represented features. The first and third player aim is to minimize, and the second player to maximize, the _regret_: The minimal prediction loss using the representation compared to the same loss using the original features. Our first contribution is theoretical and addresses the mean squared error loss function, and the case in which the representation, the response to predict and the predictors are all linear functions. We establish the optimal representation in pure strategies, which shows the effectiveness of the prior knowledge, and the optimal regret in mixed strategies, which shows the usefulness of randomizing the representation. We prove that optimal randomization requires a precisely characterized finite number of representations, which is smaller than the dimension of the feature vector, and potentially much smaller. Our second contribution is an efficient gradient-based iterative algorithm that approximates the optimal mixed representation for a general loss function, and general classes of representations, response functions and predictors.

## 1 Introduction

A common practice in modern data-science is to collect as much data as possible, even without an exact knowledge of a subsequent prediction task it will be used for. The data collected is an unlabeled set of feature vectors \(\{_{i}\}^{d}\). Then, when a specific prediction task becomes of interest, responses \(_{i}\) are collected, and a learning algorithm is trained on the pairs \(\{(_{i},_{i})\}\). Modern sources, such as high-definition images or genomic sequences, have high dimension \(d\), and this raises the question of _dimensionality-reduction_, either for a better generalization , for storage/communication savings [2; 3; 4], or for interpretability . The goal is thus to find a _representation_\(=R()^{r}\), where \(d r\), that preserves the relevant part of the features, without a full knowledge of their utility for future prediction tasks. In this paper, we propose an unsupervised-learning game-theoretic framework for this goal, whose central aspect is an assumption of prior knowledge on the _class_ of future prediction tasks. Our contributions are a theoretical solution in a fully linear setting, under the mean squared error (MSE) loss, and an algorithm for the general setting.

Popular approaches to dimensionality reduction are oblivious to prior knowledge on the prediction task. Most prominently, _principal component analysis_ (PCA) [6; 7; 8; 9], and non-linear extensions such as kernel PCA  and _auto-encoders_ (AE) [11; 12; 13; 1], aim that the representation \(\) will maximally preserve the _variation_ in \(\). Nonetheless, prior knowledge may indicate that the highly varying directions in the feature space are irrelevant for future prediction tasks. From the supervised learning perspective, it is well established that efficient representations are inherent to efficient learning [14; 15]. In this respect, the _information bottleneck_ (IB) principle [16; 17; 18; 19] was used to postulate that efficient supervised learning learns representations which are both low-complexity and relevant [20; 21; 22; 23; 24] (this spurred a debate, e.g., [25; 26]). The original IB formulation is based on the mutual information functional , which is difficult to estimate (especially in high dimensions), and ignores complexity constraints on the representation or prediction [28; 29]; see a review in Appendix B. Using the notion of _usable information_, introduced in , optimal representations for the _supervised_ learning setting were explored in  via a _two-player game_ between Alice, which selects a prediction problem of \(\) given \(\), and Bob, which then selects the representation \(\). Alice then uses an empirical risk minimizer with the standard goal of minimizing the expected risk. It was established in  that ideal generalization is obtained for representations that optimize the _decodable IB_.

In this paper, we build upon , and propose a _three-player game_ for unsupervised representation-learning, chosen without a specific prediction problem (Section 2). First, the _representation player_ reduce \(^{d}\) to a representation \(^{r}\), where \(r<d\). Second, the _response function player_ chooses a response (label) \(\) rule \(f\) for \(\), from a given known class of (random) response functions \(\). The choice of this class manifests the prior knowledge available on the type of prediction problems that the representations will be used for. Third, the _predictor player_ optimally predicts \(\) from \(\). The value of the game is determined by the _regret_: The prediction loss based on the representation \(\) compared to prediction loss based on \(\). The first and last player cooperate in order to minimize the regret, whereas the response function player aims to maximize it. In other words, the representation is chosen to minimize the worst-case prediction loss for any response in \(\). The output of this game is the representation chosen by the first player. In order to focus on the representation aspect we side-step the generalization problem, and assume that sufficient labeled data will be provided to the predictor later on in order to accurately estimate the prediction rule.

This formulation directly addresses the relevance of a "direction" in the feature space to the prediction tasks in \(\), rather than its variability, as in standard unsupervised learning (e.g., PCA and AE). Compared to , the representation is chosen based only on the _class_ of possible response functions, rather than a specific one. Such knowledge on \(\) may stem from various considerations: Domain specific, imposed by privacy or fairness constraints, or stem from transfer or continual learning setting; see Appendix A for an extended discussion. Technically, the game in  replaces the order of the first (representation) and second (response) players. From a different perspective, our method is a _self-supervised learning_ method, for which the prior knowledge on \(\) serves as a "self-defined signal" for choosing an optimal representation, without any labeled data (see, e.g.,  and  for recent surveys). In addition, our game formulation naturally leads to a _mixed strategies_ solution , that is, allowing the representation player to randomized the representation rule, in order to mix up the adversarial response player. This randomization is an inherent aspect of the IB formulation, but its usage there is not rigorously justified. By contrast, for standard unsupervised learning, mixed representation does not improve the regret (see Proposition 14 in Appendix E.1 for the PCA setting). In Appendix B we provide a thorough discussion of related work.

## Contributions

* Theoretical: We address the fundamental setting in which the representation, the response, and the prediction are linear functions, under the MSE loss function (Section 3). The prior knowledge on \(\) is represented by a symmetric matrix \(S\) that determines the principal directions of the function in the feature space. We establish the optimal representation and regret in pure strategies, which shows the utility of the prior information, and in mixed strategies, which shows that randomizing the representation yields _strictly lower_ regret. We prove that randomizing between merely \(^{*}\) different representation rules suffices, where \(r+1^{*} d\) is a precisely characterized _effective dimension_.
* Algorithmic: We develop an iterative gradients-based algorithm that approximates the optimal mixed representation (Section 4) for general representations/response/predictors and loss functions. The algorithm is greedy, and alternates between finding a new representation rule and an adversarial function. We empirically verify that the output mixed representation has close-to-optimal regret in the linear MSE setting. To optimize the weights of the representation, we essentially solve a minimax two-player games, and to this end, we utilize the classic multiplicative weights update (MWU) algorithm  (which is essentially a follow-the-regularized-leader [34; 35]).

Problem formulation

We use mostly conventional notation that is detailed in Appendix C. Specifically, the eigenvalues of a positive-semidefinite matrix \(S\) are denoted as \(_{}(S)_{1}(S)_{d}(S)=_{ }(S)\) and \(v_{i}(S)\) denotes an eigenvector corresponding to \(_{i}(S)\) such that \(V=V(S):=[v_{1}(S),v_{2}(S),,v_{d}(S)]^{d d}\) and \(S=V(S)(S)V^{}(S)\) is an eigenvalue decomposition. For a matrix \(W^{d d}\) we let \(W_{i:j}:=[w_{i},,w_{j}]^{(j-i+1) d}\) denote the matrix comprised of the columns indexed by \(\{i,,j\}\). We denote the probability law of a random variable \(\) as \(()\).

Let \(\) be a random feature vector, where \(P_{}:=()\) is known. Let \(\) be a corresponding response drawn according to a probability kernel \( f(=x)\), where for brevity, we will refer to \(f\) as the _response function_. We assume \(f\) for some known class \(\). Let \(:=R()^{r}\) be an \(r\)-dimensional representation of \(\) where \(R^{r}\) is chosen from a class \(\) of representation functions, and let \(Q^{r}\) be a prediction rule from a class \(\), with the loss function loss: \(_{+}\). The _regret_ of the representation \(R\) for the response function \(f\) is

\[(R,f P_{}):=_{Q}[ (,Q(R()))]-_{Q^{d} }[(,Q())].\] (1)

The _minimax regret in mixed strategies_ is defined via the worst case response function in \(\) as

\[_{}(, P_{}):=_{ ()()}_{f}[(,f P_{})],\] (2)

where \(()\) is a set of probability measures on the possible set of representations \(\). The _minimax regret in pure strategies_ restricts \(()\) to degenerated measures (deterministic), and so the expectation in (2) is removed. Our main goal is to determine the optimal representation strategy, either in pure \(R^{*}\) or mixed strategies \((^{*})()\). To this end, we will also utilize the _maximin_ version of (2). Specifically, let \(()\) denote a set of probability measures supported on \(\), and assume that for any \(R\), there exists a measure in \(()\) that puts all its mass on \(R\). Then, the _minimax theorem_[32, Chapter 2.4] implies that

\[_{}(, P_{})=_{ ()()}_{R}[(R, P_{})].\] (3)

The right-hand side of (3) is the _maximin regret in mixed strategies_, and the maximizing probability law \((^{*})\) is known as the _least favorable prior_. In general, \(_{}(, P_{}) _{}(, P_{})\), and the inequality can be strict. We mention that the use of expectation in the definition of the mixed regret over the randomized representation, implies that the empirical performance of a system based on this randomized representation achieves the mixed minimax regret value in the limit of large number of repeating representation games. The size of the dataset for each of these games should be large enough to allow for accurate learning of \(\) to be used by the predictor. By contrast, the pure minimax regret guarantee is valid for a single representation, and thus more conservative from this aspect.

## 3 The linear setting under MSE loss

In this section, we focus on linear classes and the MSE loss function. The response function class is characterized by a quadratic constraint, to wit, the class \(\) is specified by a matrix \(S_{++}^{d}\) that represents the relative importance of each direction in the feature space in determining \(\).

**Definition 1** (The linear MSE setting).: Assume that \(=^{d}\), that \(=\) and the loss function is the MSE, \((y_{1},y_{2})=|y_{1}-y_{2}|^{2}\). Assume that \([]=0\) and let \(_{}:=[^{T}]_{++}^{d}\) be its invertible covariance matrix. The classes of representations, response functions, and predictors are all linear, that is: (1) The representation is \(z=R(x)=R^{}x\) for \(R:=^{d r}\) where \(d>r\); (2) The response function is \(F^{d}\), and \(=f^{}+\), where \(\) is a heteroscedastic noise that satisfies \([]=0\), and given some specified \(S_{++}^{d}\)

\[f_{S}:=\{f^{d}\|f\|_{S}^{2} 1\},\] (4)

where \(\|f\|_{S}:=\|S^{-1/2}f\|_{2}=(f^{}S^{-1}f)^{1/2}\) is the Mahalanobis norm; (3) The predictor is \(Q(z)=q^{}z\) for \(q^{r}\). Since the regret will depend on \(P_{}\) only via \(_{}\), we will abbreviate the notation of the pure (resp. mixed) minimax regret to \(_{}(_{})\) (resp. \(_{}(_{})\)).

In Appendix E.1 we show that standard PCA can be similarly formulated, by assuming that \(\) is a singleton containing the noiseless identity function, so that \(=\) surely holds, and \(=Q(z)^{d}\). Proposition 14 therein shows that the pure and mixed minimax representations are both \(R=V_{1:r}(_{})\), and so randomization is not unnecessary. We begin with the pure minimax regret.

**Theorem 2**.: _For the linear MSE setting (Definition 1)_

\[_{}(_{S}_{})=_{ r+1}(_{}^{1/2}S_{}^{1/2}).\] (5)

_A minimax representation matrix is_

\[R^{*}:=_{}^{-1/2} V_{1:r}(_{}^{1/2}S_{ }^{1/2}),\] (6)

_and the worst case response function is_

\[f^{*}:=S^{1/2} v_{r+1}(_{}^{1/2}S_{}^{1/2} ).\] (7)

The optimal representation thus whitens the feature vector \(\), and then projects it on the top \(r\) eigenvectors of the adjusted covariance matrix \(_{}^{1/2}S_{}^{1/2}\), which reflects the prior knowledge that \(f_{S}\). The proof is deferred to Appendix E.2, and its outline is as follows: Plugging the optimal predictor into the regret results a quadratic form in \(f^{d}\), determined by a matrix which depends on the subspace spanned by the representation \(R\). The worst-case \(f\) is the determined via the _Rayleigh quotient theorem_[37, Theorem 4.2.2], and the optimal \(R\) is found via the _Courant-Fischer variational characterization_[37, Theorem 4.2.6] (see Appendix D for a summary of useful mathematical results). We next consider the mixed minimax regret.

**Theorem 3**.: _For the linear MSE setting (Definition 1)_

\[_{}(_{S}_{})=-r}{_{i=1}^{^{*}}_{i}^{-1}},\] (8)

_where \(_{i}_{i}(S^{1/2}_{}S^{1/2})\) and \(^{*}\) is any member of_

\[\{[d][r](-r)_{}^{-1} _{i=1}^{}_{i}^{-1}(-r)_{+1}^{-1}\}\] (9)

_(with \(_{d+1} 0\)). Furthermore:_

* _The covariance matrix of the least favorable prior of_ \(\)_: Let_ \(_{}:=(_{1},,_{^{*}},0,,0)\)_, and let_ \(V V(S^{1/2}_{}S^{1/2})\)_. Then, the covariance matrix of the least favorable prior of_ \(\) _is_ \[_{}^{*}:=_{^{*}}^{-1}V}{_{i=1}^{^ {*}}_{i}^{-1}}.\] (10)
* _The probability law of the minimax representation: Let_ \(\{0,1\}^{^{*}}{r}}\) _be a matrix whose columns are the members of the set_ \[:=\{\{0,1\}^{^{*}}\|\|_{1}= ^{*}-r\}\] (11) _(in an arbitrary order). Let_ \(=(b_{1},,b_{^{*}})^{}\) _be such that_ \[b_{i}=(^{*}-r)^{-1}}{_{j=1}^{^{*}}_{ j}^{-1}}.\] (12)

_Then, there exists a solution \(p^{}{r}}\) with support size at most \(^{*}+1\) to \(p=\). For \(j[}{r}]\), let \(_{j}:=\{i[^{*}]_{ij}=0\}\) be the zero indices on the \(j\)th column of \(\), and let \(V_{_{j}}\) denote the \(r\) columns of \(V\) whose index is in \(_{j}\). A minimax representation is_

\[^{*}=_{}^{-1/2}V_{_{j}}\] (13)

_with probability \(p_{j}\), for \(j[}{r}]\)._Interestingly, while the eigenvalues \(_{i}(_{}^{1/2}S_{}^{1/2})=_{i}(S^{1/2} _{}S^{1/2})\) are equal, the pure minimax regret utilizes the eigenvectors of \(_{}^{1/2}S_{}^{1/2}\) whereas the mixed minimax regret utilizes those of \(S^{1/2}_{}S^{1/2}\), which are possibly different. The proof of Theorem 3 is also in Appendix E.2, and is substantially more complicated and longer than for the pure regret. We use a two-step indirect approach, since it seems challenging to directly maximize over \(()\). First, we solve the _maximin problem_ (3), and find the least favorable prior \((^{*})\). Second, we propose a probability law for the representation \(()\), and show that its regret equals the maximin value, and thus also the minimax. With more detail, in the first step, we show that the regret only depends on \(()\) via \(_{}=[^{}]\), and we explicitly construct a probability law that is both fully supported on \(_{S}\) and has this covariance matrix. This reduces the problem from optimizing \(()\) to optimizing \(_{}\), whose solution (Lemma 16) leads to the least favorable \(_{}^{*}\), and then to the maximin value. In the second step, we explicitly construct a representation that achieves the maximin regret. Concretely, we construct representation matrices that use \(r\) of the \(^{*}\) principal components of \(_{}^{1/2}S_{}^{1/2}\), where \(^{*}>r\). The defining property of \(^{*}\) (9) established in the maximin solution is utilized to find weights on the \(}{r}\) possible representations, that achieves the maximin solution, and thus also the minimax. The proof uses Caratheodory's theorem (see Appendix D) which also establishes that the optimal \(\{p_{j}\}\) is supported on at most \(^{*}+1\) matrices, much less than \(}{r}\). We next make a few comments:

1. [leftmargin=*,noitemsep,topsep=0pt]
2. _Computing the mixed minimax probability:_ This requires solving \(^{}p=\) for a probability vector \(p\), which is a linear-program feasibility problem that is routinely solved . For illustration, if \(r=1\) then \(\{0,1\}^{^{*}^{*}}\) is a square all ones matrix, except for a zero diagonal, and \(p_{j}=1-(^{*}-1)_{j}^{-1}/(_{i=1}^{}_{i}^{-1})\) for \(j[^{*}]\). Similarly, the case \(^{*}=r+1\) is solved by setting \(p_{j}=(_{j}^{-1})/(_{j^{}=1}^{^{*}}_{j^{}}^{ -1})\) on the \(^{*}\) standard basis vectors. Nonetheless, the dimension of \(p\) is \(}{r}\) and thus increases fast as \(((^{*})^{r})\), and this approach may be intractable. However, in this case the algorithm we present in Section 4 can be used. As we empirically show, it approximately achieves the optimal regret, and the number of atoms is not much larger than \(^{*}+1\).
3. _Required randomness:_ The regret formulation (2) assumes that the actual realization of the representation rule is known to the predictor. Formally, this can be conveyed to the predictor using an small header of less than \(_{2}(^{*}+1)(d+1)\) bits. Practically, this is unnecessary and an efficient predictor can be learned from a labeled data set \((,)\).
4. _The rank of \(_{}^{*}\):_ The rank of the covariance matrix of the least favorable prior is an _effective dimension_, satisfying (see (8)) \[^{*}=*{arg\,max}_{[d][r]}_{i=1}^{}_{i}^{-1}}.\] (14) By convention, \(\{_{i}^{-1}\}_{i[d]}\) is a monotonic non-decreasing sequence, and so is the partial Cesaro mean \(():=_{i=1}^{}_{i}^{-1}\). For example, if \(_{i}=i^{-}\) with \(>0\) then \(()=(^{})\). If, e.g., \(()=^{}\), then it is easily derived that \(^{*}\{r,d\}\). So, if \(\) is large enough and the decay rate of \(\{_{i}\}\) is fast enough then \(^{*}<d\), and otherwise \(^{*}=d\). As the decay rate of \(\{_{i}\}\) becomes faster, the rank of \(_{}^{*}\) decreases to \(r\). Importantly, \(^{*} r+1\) always holds, and so the optimal mixed representation is not deterministic even if \(S^{1/2}_{}S^{1/2}\) has less than \(r\) significant eigenvalues (which can be represented by a single matrix \(R^{d r}\)). Hence, the mixed minimax regret is always _strictly lower_ than the pure minimax regret. Thus, even when \(S=I_{d}\), and no valuable prior knowledge is known on the response function, the mixed minimax representation is different from the standard PCA solution of top \(r\) eigenvectors of \(_{}\).
5. _Uniqueness of the optimal representation:_ Since one can always post-multiply \(R^{}x\) by some invertible matrix, and then pre-multiply \(z=R^{}x\) by its inverse, the following simple observation holds: When \(\) and \(\) are not further restricted, then if \(\) is a minimax representation, and \(W()^{r r}\) is an invertible matrix, then \( W()\) is also a minimax representation.
6. _Infinite-dimensional features:_ Theorems 2 and 3 assume a finite dimensional feature space, but as we show in Appendix F, the results can be easily generalized to an infinite dimensional Hilbert space \(\), in the more restrictive setting that the noise \(\) is statistically independent of \(\).

**Example 4**.: Assume \(S=I_{d}\), and denote, for brevity, \(V V(_{}):=[v_{1},,v_{d}]\) and \((_{}):=(_{1}, ,_{d})\). The optimal minimax representation in pure strategies (Theorem 2) is then

\[R^{*}=_{}^{-1/2} V_{1:r}=V_{}^{-1/2}V^{}V_{1:r} =V_{}^{-1/2}[e_{1},,e_{r}]=[_{1}^{-1/2}  v_{1},,_{r}^{-1/2} v_{r}],\] (15)

which is comprised of the top \(r\) eigenvectors of \(_{}\), scaled so that \(v_{i}^{}\) has unit variance. By Comment 4 above, \(V_{1:r}\) is also an optimal minimax representation. The worst case response is \(f=v_{r+1}(_{})\) and, as expected, since \(R\) uses the first \(r\) principal directions

\[_{}(_{})=_{r+1}.\] (16)

The minimax regret in mixed strategies (Theorem 3) is different, and given by

\[_{}(_{})=-r}{_{i=1}^{^{*}}_{i}^{-1}},\] (17)

where \(^{*}\) is determined by the decay rate of the eigenvalues of \(_{}\) (see (9)). The least favorable covariance matrix is given by (Theorem 3)

\[_{}^{*}=[_{i=1}^{^{*}}_{i}^{-1}]^{-1}  V(_{1}^{-1},,_{^{*}}^{ -1},0,,0) V^{}.\] (18)

Intuitively, the least favorable \(_{}^{*}\) equalizes the first \(^{*}\) eigenvalues of \(_{}_{}^{*}\) (and nulls the other \(d-^{*}\)) so that the representation is indifferent to these \(^{*}\) directions. As evident from the regret, the "equalization" of the \(i\)th eigenvalue adds a term of \(_{i}^{-1}\) to the denominator, and if \(_{i}\) is too small then \(v_{i}\) is not chosen for the representation, as agrees with Comment 3 above (a fast decay of \(\{_{i}\}\) reduces \(_{*}\) away from \(d\)). The mixed minimax representation sets

\[^{*}=_{}^{-1/2} V_{_{j}}=[_{i_{ j,1}}^{-1/2} v_{i_{j,1}},,_{i_{j,r}}^{-1/2} v_{i_{j,r}}]\] (19)

with probability \(p_{j}\), where \(_{j}\{i_{j,1},,i_{j,r}\}\) (the derivation is similar to (15)). Thus, the optimal representation chooses a random subset of \(r\) vectors from \(\{v_{1},,v_{^{*}}\}\). See the left panel of Figure 1 for a numerical example.

**Example 5**.: To demonstrate the effect of prior knowledge on the response function, we assume \(_{}=(_{1}^{2},,_{d}^{2})\) and \(S=(s_{1},,s_{d})\), where \(_{1}^{2}_{2}^{2}_{d}^{2}\) (but \(\{s_{i}\}_{i[d]}\) are not necessarily ordered). Letting \(f=(f_{1},,f_{d})\), the class of response functions is \(_{S}:=\{f^{d}_{i=1}^{d}(f_{i}^{2}/s_{i}) 1\}\), and so coordinates \(i[d]\) with a large \(s_{i}\) have large influence on the response. Let \((i_{(1)},,i_{(d)})\) be a permutation of \([d]\) so that \(_{i(j)}^{2}s_{i(j)}\) it the \(j\)th largest value of \((_{i}^{2}s_{i})_{i[d]}\). The pure minimax regret is (Theorem 2)

\[_{}(_{})=_{i_{r+1} }^{2}s_{i_{r+1}}.\] (20)

The optimal representation is \(R=[e_{i_{(1)}},e_{i_{(2)}},,e_{i_{(r)}}]\), that is, uses the most influential coordinates, according to \(\{s_{i}\}\), which may be different from the \(r\) principal directions of \(_{}\). For the minimax regret in mixed strategies, Theorem 3 results

\[_{}(_{})= -r}{_{j=1}^{^{*}}(s_{i_{j}}_{i_{j}}^{2})^{-1}}\] (21)for \(^{*}[d][r]\) satisfying (9), and the covariance matrix of the least favorable prior is given by

\[^{*}_{}=^{^{*}}_{i_{j}}^{-2} e_{i_{j}} _{i_{j}}^{}}{_{j=1}^{^{*}}(s_{i_{j}}_{i_{j}}^{2})^{-1}}.\] (22)

That is, up to a scale factor \((_{i=1}^{^{*}}s_{i}^{-1}_{i}^{-2})^{-1}\), the matrix is diagonal so that the \(k\)th term on the diagonal is \(^{*}_{}(k,k)=_{k}^{-2}\) if \(k=i_{j}\) for some \(j[^{*}]\) and \(^{*}_{}(k,k)=0\) otherwise. As in Example 4, \(^{*}_{}\) equalizes the first \(^{*}\) eigenvalues of \(_{}_{}\) (and nulls the other \(d-^{*}\)). However, it does so in a manner that chooses the Lemma to their influence on \(^{}\). The random minimax representation in mixed strategies is

\[^{*}=[_{i_{j,1}}^{-1} e_{i_{j,1}},,_{i_{j,r }}^{-1} e_{i_{j,r}}]\] (23)

with probability \(p_{j}\). Again, all the first \(^{*}\) coordinates are used, and not just the top \(r\). See the right panel of Figure 1 for a numerical example. We finally remark that, naturally, in the non-diagonal case, the minimax regret will also depend on the relative alignment between \(S\) and \(_{}\).

## 4 An iterative algorithm for general classes and loss functions

In this section, we develop an iterative algorithm for finding the optimal representation in mixed strategies, i.e., solving (2) for general classes and loss functions. Since optimizing general probability measures over \(\) is formidable, we restrict the optimization to finite mixed representations, i.e., assume that \(=R^{(j)}\) with probability \(p^{(j)}\), where \(j[m]\) (which suffices for the linear MSE setting of Section 3, but possibly sub-optimal in general). Furthermore, the algorithm's operation will require randomization also for the response player, and so we set \(=f^{(i)}\) with probability \(o^{(i)}\) where \(i[]\), and \(=m_{0}+m\) for some \(m_{0} 0\). The resulting optimization problem then becomes

\[_{\{p^{(j)},R^{(j)}\}}_{\{o^{(i)},f^{(i)} \}}_{\{Q^{(j)},\}}_{j[m]}_{i[]}p^{(j)} o^{(i)}[(f^{(i)}(),Q^{(j)}(R^{( j)}()))],\] (24)

under the constraints \(p^{(j)} 0\) and \(_{j}p^{(j)}=1\), and \(o^{(i)} 0\) and \(_{i}o^{(i)}=1\). Note that the prediction rule \(Q^{(j,i)}\) is determined based on both \(R^{(j)}\) and \(f^{(i)}\), and that the ultimate goal of solving (24) is just to extract the optimal \(\).

A high level description of the algorithm is to gradually add more representations to the support size of \(\) up to \(m\), where next \(k\) will denote the current number of representations, \(k[m]\). Initialization requires an representation \(R^{(1)}\), as well as a _set_ of functions \(\{f^{(i)}\}_{i m_{0}}\), so that the final support size of \(\) will be \(=m_{0}+m\). Finding this initial representation and the set of functions is based on the specific loss function and a possible set of representation/predictors. At iteration \(k[m]\), the main loop of the algorithm has two phases. In the first phase, a new adversarial function is added to the set of functions, as the worse function for the current random representation. In the second phase, a new representation atom is added to the set of possible representations. This representation is determined based on the given set of functions. Concretely, the two phases operate as follows:

* Given \(k\) representations \(\{R^{(j)}\}_{j(k)}\) with weights \(\{p^{(j)}\}_{j[k]}\), the algorithm determines the function \(f^{(m_{0}+k)}\) as the worst function for this random representation (optimal adversarial action of the response function player). Specifically, \[_{k} :=_{}(\{R^{(j)},p^{(j)}\}_{j[k]},  P_{})\] (25) \[:=_{f}_{\{Q^{(j)}\}_{j[k]} }_{j[k]}p^{(j)}[(f(),Q^{(j)}(R^ {(j)}()))]\] (26)

is solved, and \(f^{(m_{0}+k)}\) is set to be the maximizer. This simplifies (24) in the sense that \(m\) is replaced by \(k\), the random representation \(\) is kept fixed, and \(f\) is optimized as a pure strategy (the previous functions \(\{f^{(i)}\}_{i[m_{0}+k-1]}\) are ignored).

- Adding a representation atom: Given fixed \(\{f^{(j)}\}_{j[m_{0}+k]}\) and \(\{R^{(j)}\}_{j[k]}\), a new representation \(R^{(k+1)}\) is found as the most incrementally valuable representation atom. Specifically, \[_{R^{(k+1)}}}_{}(\{R^{(j _{1})}\}_{j_{1}[k+1]},\{f^{(j_{2})}\}_{j_{2}[m_{0}+k]} P_{})\] \[:=_{R^{(k+1)}}_{\{p^{(j_{1})}\}_{j_{1}[k +1]}}_{\{o^{(j_{2})}\}_{j_{2}[m_{0}+k]}}\{Q^{(j_{1},j_{2})}\}_{j_{1}[k+1],j_{2}[m_{0}+k]}\] \[_{j_{1}[k+1]}_{j_{2}[m_{0}+k]}p^{(j_{1})} o^{(j _{2})}[(f^{(j_{1})}(),Q^{(j_{1},j_{2})}(R ^{(j_{1})}()))]\] (27)

is solved, the solution \(R^{(k+1)}\) is added to the set of representations, and the weights are updated to the optimal \(\{p^{(j_{1})}\}_{j_{1}[k+1]}\). Compared to (24), here the response functions and current \(k\) representations are kept fixed, and only their weights \(\{p^{(j_{1})}\}\)\(\{o^{(j_{2})}\}\) are optimized, along with \(R^{(k+1)}\).

The procedure is described in Algorithm 1, where, following the main loop, \(m^{*}=_{k[m]}_{k}\) representation atoms are chosen and the output is \(\{R^{(j)},p^{(j)}\}_{j[m^{*}]}\). Algorithm 1 relies on solvers for the Phase 1 (26) and Phase 2 (27) problems. In Appendix G we propose two algorithms for these problems, which are based on gradient steps for updating the adversarial response and the new representation, and on the MWU algorithm  (_follow-the-regularized-leader_) for updating the weights. In short, the Phase 1 algorithm updates the response function \(f\) via a projected gradient step of the expected loss, and then adjusts the predictors \(\{Q^{(j)}\}\) to the updated response function \(f\) and the current representations \(\{R^{(j)}\}_{j[k]}\). The Phase 2 algorithm only updates the new representation \(R^{(k+1)}\) via projected gradient steps, while keeping \(\{R^{(j)}\}_{j[k]}\) fixed. Given the representations \(\{R^{(j)}\}_{j[k+1]}\) and the functions \(\{f^{(i)}\}_{i[m_{0}+k]}\), a predictor \(Q^{(j,i)}\) is then fitted to each representation-function pair, which also determines the loss for this pair. The weights \(\{p^{(j)}\}_{j[k+1]}\) and \(\{o^{(i)}\}_{i[m_{0}+k]}\) are updated towards the equilibrium of the two-player game determined by the loss of the predictors \(\{Q^{(j,i)}\}_{j[k+1],i[m_{0}+k]}\) via the MWU algorithm.

```
1:input\(P_{},,,,d,r,m,m_{0}\)\(\) Feature distribution, classes, dimensions and parameters
2:input\(R^{(1)}\), \(\{f^{(j)}\}_{j[m_{0}]}\)\(\) Initial representation and initial function (set)
3:begin
4:for\(k=1\) to \(m\)do
5:phase 1:\(f^{(m_{0}+k)}\) is set by a solver of (26) and \[_{k}_{}(\{R^{(j)},p^{(j)}\}_{j[k ]}, P_{})\] (28) \(\) Solved using Algorithm 2
6:phase 2:\(R^{(k+1)},\{p^{(j)}\}_{j[k+1]}\) is set by a solver of (27)\(\) Solved using Algorithm 3; step can be removed if \(k=m\)
7:endfor
8:set\(m^{*}=_{k[m]}_{k}\)
9:return\(\{R^{(j)}\}_{j[m^{*}]}\)and\(p_{m_{*}}=\{p^{(j)}\}_{j[m^{*}]}\) ```

**Algorithm 1** Solver of (24): An iterative algorithm for learning mixed representations.

We next outline two examples, where full details can be found in Appendix H.

**Example 6**.: We validate that efficiency of Algorithm 1 in the linear MSE setting (Section 3), for which a closed-form solution exists. We ran Algorithm 1 on randomly drawn diagonal \(_{}\), and computed the ratio between the regret obtained by the algorithm to the theoretical value. The left panel of Figure 2 shows that the ratio is between \(1.15-1.2\) in a wide range of \(d\) values. We mention again that Algorithm 1 is useful even for this setting since finding an \((^{*}+1)\)-sparse solution to \(=\) is computationally difficult when \(}{r}\) is very large. For example, in the largest dimension of the experiment, the potential number of representation matrices is \(==11,628\).

Our next example pertains to a logistic regression setting, under the cross-entropy loss function.

**Definition 7** (The linear cross-entropy setting).: Assume that \(=^{d}\), that \(=\{ 1\}\) and that \([]=0\). Assume that the class of representation is linear \(z=R(x)=R^{}x\) for some \(^{d r}\) where \(d>r\). Assume that a response function and a prediction rule determine the probability that \(y=1\) via logistic regression modeling, as \(f(= 1 x)=1/[1+( f^{}x)]\). Assume the cross-entropy loss function, where given that the prediction that \(=1\) with probability \(q\) results the loss \((y,q):=-(1+y) q-(1-y)(1-q)\). The set of predictor functions is \(:=\{Q(z)=1/[1+(-q^{})],\;q^{r}\}\). As for the linear case, we assume that \(f_{S}\) for some \(S^{d}_{++}\). It is not difficult to show that the regret is then given by the expected binary Kullback-Leibler (KL) divergence

\[(R,f P_{})=_{q^{r}}[ D_{}([1+(-f^{})]^{-1}[1+(-q^{}R^{ })]^{-1})].\] (29)

**Example 8**.: We ran Algorithm 1 on empirical distributions of features drawn from an isotropic normal distribution, in the linear cross-entropy setting. Algorithm 1 is suitable in this setting since gradients of the regret have closed-form (see Appendix H). The right panel of Figure 2 shows the reduced regret obtained by increasing the support size \(m\) of the random representation, and thus the effectiveness of mixed representations.

We refer the reader to Appendix I for additional experiments with Algorithm 1.

## 5 Conclusion

We proposed a game-theoretic formulation for learning representations of unlabeled features when prior knowledge (or assumptions) on the class of future prediction tasks is available. We focused on the fundamental of linear MSE setting, and derived the optimal solution. Beyond the lower regret that is directly obtained from utilizing the prior knowledge, our results also revealed the importance of using randomized representations. We have then proposed an iterative algorithm suitable for general classes of functions and losses, and exemplified its effectiveness.

We next discuss _limitations_ and potential future research: (1) We have focused on the elementary and simplified class \(_{S}=\{f\|f\|_{S} 1\}\), mainly for theoretical investigations. A natural refinement to non-linear functions is the general class \(_{S_{x}}:=[\|_{x}f()\|_{S_{}}^{2} ] 1\), where \(\{S_{x}\}_{x^{d}}\) is now locally specified (somewhat similarly to the regularization term used in contractive AE , though for different reasons). (2) Since the proposed iterative algorithm includes optimization over three players, it is of interest to develop version of the algorithm with lower computational optimization cost. (3) We have assumed that \(_{S}\) is given in advance, and a natural follow-up goal is to efficiently learn \(S\) from previous experience, e.g., improving \(S\) from one episode to another in a meta-learning setup . (4) It is interesting to evaluate the effectiveness of the learned representation in our formulation, as an initialization for further optimization when labeled data is collected. One may postulate that since our learned representation is _uniformly_ good for all response functions in the class, it may serve as a universal initialization for such training.

Figure 2: Results of Algorithm 1. Left: \(r=5\), varying \(d\). The ratio between the regret achieved by Algorithm 1 and the theoretical regret in the linear MSE setting. Right: \(r=3\), varying \(d\). The regret achieved by Algorithm 1 in the linear cross entropy setting, various \(m\).

## Broader impact

The research described in this paper is foundational, and does not aim for any specific application. Nonetheless, the learned representation is based on a prior assumption on the class of response functions, and the choice of this prior may have positive or negative impacts: For example, a risk of this choice of prior is that the represented features completely ignore a viable feature for making future predictions. A benefit that can stem from choosing a proper prior is that the representation will null the effect of features that lead to unfair advantages for some particular group, in future predictions. Anyhow, the results presented in the paper are indifferent to such future utilization, and any usage of these results should take into account the aforementioned possible implications.