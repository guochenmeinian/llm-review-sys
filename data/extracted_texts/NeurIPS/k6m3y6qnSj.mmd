# IllumiNeRF: 3D Relighting Without Inverse Rendering

Xiaoming Zhao\({}^{1,3}\)1 Pratul P. Srinivasan\({}^{2}\) Dor Verbin\({}^{2}\)

Keunhong Park\({}^{1}\) Ricardo Martin-Brualla\({}^{1}\) Philipp Henzler\({}^{1}\)

\({}^{1}\)Google Research \({}^{2}\)Google DeepMind \({}^{3}\)University of Illinois Urbana-Champaign

###### Abstract

Existing methods for relightable view synthesis -- using a set of images of an object under unknown lighting to recover a 3D representation that can be rendered from novel viewpoints under a target illumination -- are based on inverse rendering, and attempt to disentangle the object geometry, materials, and lighting that explain the input images. Furthermore, this typically involves optimization through differentiable Monte Carlo rendering, which is brittle and computationally-expensive. In this work, we propose a simpler approach: we first relight each input image using an image diffusion model conditioned on target environment lighting and estimated object geometry. We then reconstruct a Neural Radiance Field (NeRF) with these relit images, from which we render novel views under the target lighting. We demonstrate that this strategy is surprisingly competitive and achieves state-of-the-art results on multiple relighting benchmarks. Please see our project page at illuminerf.github.io.

## 1 Introduction

Capturing an object's appearance so that it can be accurately rendered in novel environments is a central problem in computer vision whose solution would democratize 3D content creation for augmented and virtual reality, photography, filmmaking, and game development. Recent advances in view synthesis  have made impressive progress in reconstructing a 3D representation that can be rendered from novel viewpoints, using just a set of observed images. However, those methods

Figure 1: Given a set of posed input images under an _unknown_ lighting (four exemplar images from the set are shown on top), IllumiNeRF produces high-quality novel views (bottom) relit under a target lighting (illustrated as chrome balls). Inputs obtained from the Stanford-ORB dataset .

typically only recover the appearance of the object under the captured illumination, and _relightable view synthesis_ -- rendering novel views of the captured object under arbitrary target environments -- remains challenging.

Recent methods for recovering relightable 3D representations treat this task as _inverse rendering_, and attempt to estimate the geometry, materials, and illumination that jointly explain the input images using physically-based rendering methods. These approaches typically involve gradient-based optimization through differentiable Monte Carlo rendering procedures, which are noisy and computationally-expensive. Moreover, the inverse rendering optimization problem is brittle and inherently ambiguous; many potential sets of geometry, materials, and lighting can explain the input images, but many of these incorrect explanations produce obviously implausible renderings when rendered under novel unobserved illumination.

We propose a different approach that avoids inverse rendering and instead leverages a generative image model fine-tuned for the task of relighting. Given a set of images viewing an object and a desired target illumination, we use a single-image 2D Relighting Diffusion Model that outputs relit images of the object under the target illumination. Due to the ambiguous nature of the problem, each sample of the generative model encodes a different explanation of the object's materials, geometry and the input illumination. However, as opposed to optimization-based inverse rendering, such samples are all _plausible_ relit images since they are the output of the trained diffusion model.

Instead of attempting to recover a single explanation of the underlying object's appearance, we sample multiple plausible relit images for each observed viewpoint, and treat the underlying explanations as samples of unobserved latent variables. To recover a final consistent 3D representation of the relit object, we use the full set of sampled relit images from all viewpoints to train a "latent NeRF" that reconciles all the samples into a single 3D representation, which can be rendered to produce plausible relit images from novel viewpoints.

The key contribution of our work is a new paradigm for relightable 3D reconstruction that replaces 3D inverse rendering with: generating samples with a single-image 2D Relighting Diffusion Model followed by distilling these samples into a 3D latent NeRF representation. We demonstrate that this strategy is surprisingly competitive and outperforms existing most 3D inverse rendering baselines on the TensoIR  and Stanford-ORB  relighting and view synthesis benchmarks.

## 2 Related Work

Our work addresses the task of relightable 3D reconstruction by using a lighting-conditioned diffusion model as a generative prior for single-image relighting. It is closely related to prior work in relightable 3D reconstruction, inverse rendering, and single-image relighting. Below, we review these lines of work and discuss how they relate to our proposed approach.

Relightable 3D ReconstructionThe goal of relightable 3D reconstruction is to reconstruct a 3D representation of an object that can be relit by novel illumination conditions and rendered from novel camera poses. In scenarios where an object is observed under multiple lighting conditions , it is trivial to render its appearance under novel illumination that is a linear combination of the observed lighting conditions, due to the linear behavior of light. This approach is generally limited to laboratory capture scenarios where it is possible to observe an object under a lighting basis.

In more casual capture scenarios, the object is observed under just a single or a small handful of lighting conditions. Existing works typically address this setting using methods based on inverse rendering that explicitly factor an object's appearance into the underlying 3D geometry, object material properties, and lighting that jointly explain the observed images. State-of-the-art approaches to 3D inverse rendering [9; 10; 17; 23; 26; 33; 38; 46; 47] generally utilize the following strategy: they start with a neural field representation of 3D geometry (typically volume density as in NeRF , hybrid volume-surface representations as in Neub  and VolSDF , or meshes extracted from neural field representations) from the input images, equip the model with a representation of surface materials (_e.g._ spatially-varying BRDF parameters) and lighting, and jointly optimize these factors through a differentiable physics-based rendering procedure . While methods may differ in their choice of geometry, material, and lighting representations, and employ different techniques to accelerate the evaluation of the rendering integral, they generally all follow this same high-level inverse rendering strategy. Unfortunately, even if the geometry is known, inverse rendering is a notoriously ambiguous problem [43; 52] and many combinations of materials and lighting can explain an object's appearance. However, not all of these combinations are plausible, and incorrect factorizations that explain observed images under one lighting condition may produce glaring artifacts when rendered under different lighting. Furthermore, differentiable physics-based rendering is computationally-expensive as thousands of samples are needed for Monte Carlo estimates of the rendering integral, typically requires custom implementations [2; 3; 22; 28; 32; 35; 54], and the resulting inverse rendering loss landscape is non-smooth and difficult to optimize effectively with gradient descent .

Single Image RelightingInstead of using inverse rendering to recover object material parameters which can be relit with physically-based rendering techniques, we train a diffusion model that can directly sample from the distribution of relit images conditioned on a target lighting condition. This diffusion model is essentially a generative single-image relighting model. Early single image relighting techniques employed optimization-based inverse rendering . Subsequent methods trained deep convolutional neural networks to output image geometry, materials, and lighting [29; 30], or in some cases, to directly output relit images [48; 7; 8].

Most related to our method are a few recent works that have trained diffusion models for single image relighting. LightIt  trains a model similar to ControlNet  to relight outdoor images under arbitrary sun positions conditioned on input normals and shading. DiffusionLight  estimates the lighting of an image by using a ControlNet to inpaint the color pixels of a chrome ball in the middle of the scene, from which an environment map can be recovered.

Most similar to our work is the concurrent method of DiLightNet  that focuses on single image relighting. DiLightNet uses a ControlNet-based  approach to condition a single-image relighting diffusion model on a target environment map. DiLightNet uses a set of "radiance cues"  -- renderings of the object's geometry (obtained from an off-the-shelf monocular depth network) with various roughness levels under the target environment illumination -- as conditioning. Our method instead focuses on 3D relighting, where multiple of images of an object are available. It uses a similar single-image relighting diffusion model conditioned on radiance cues. Unlike DiLightNet which uses geometry from monocular depth estimation to render radiance cues, we use geometry estimated from the input views using a state-of-the-art surface reconstruction method . This allows our model to better model complex light transport effects such as interreflections caused by occluded geometry.

## 3 Method

### Problem Formulation

Given a dataset of images of an object and corresponding camera poses \(=\{(I_{i},_{i})\}_{i=1}^{N}\), the general goal of relightable 3D reconstruction is to estimate a model with parameters \(\) that when rendered, produces relit versions of the dataset under unobserved target illumination \(L^{T}\). This can be expressed as:

\[^{}=*{argmax}_{}p(_{}^{T}| ),\] (1)

where \(_{}^{T}\{(( ,L^{T},_{i},),_{i})\}_{i=1}^{N}\) is a relit version of the original dataset under target illumination \(L^{T}\) using model \(\). Note that Eq. (1) only maximizes the likelihood of the original given poses after relighting. However, by using view synthesis, we can then turn the collection of relit images into a 3D representation which can be rendered from arbitrary poses. For brevity, we therefore omit the implicit dependence of \(^{T}\) in \(\).

This relighting problem has traditionally been solved by using inverse rendering. Inverse rendering techniques do not maximize the probability of the relit renderings, but instead recover a single point estimate of the most likely scene geometry \(G\), materials \(M\), and lighting \(L\) (note that this is the "source" lighting condition for the observed images) that together explain the input dataset, and then use physically-based rendering to relight this factorized explanation under the target lighting. Inverse rendering seeks to recover \(^{}=(G^{},M^{})\), where:

\[G^{},M^{},L^{}=*{argmax}_{G,M,L}p(G,M,L|)=*{argmax}_{G,M,L}p(|G,M,L)p(G,M,L).\] (2)The first data likelihood term is computed by physics-based rendering of the estimated model and the second prior term is often factorized into separate handcrafted priors on geometry, materials, and lighting [23; 33; 43].

A relighting approach based on inverse rendering then renders each image \(I\) in \(\) corresponding to camera pose \(\) using the recovered geometry and materials, illuminated by the target lighting \(L^{T}\), resulting in \((,L^{T},,^{})\). This approach has three main issues. First, the differentiable rendering procedures used to compute the gradient of the likelihood term are computationally-expensive. Second, it requires careful modeling of light transport which is cumbersome and existing differentiable renderers do not account for many types of lighting and material effects seen in the real world. Third, there are often ambiguities between \(M\) and \(L\), meaning that any errors in their decomposition may be apparent in the relit data. It is quite difficult to design effective handcrafted priors on geometry, materials, and lighting, so inverse rendering procedures frequently recover explanations that have a high data likelihood (are able to render the observed data) but produce clearly incorrect results when re-rendered under different illumination.

### Model Overview

We propose an approach that attempts to maximize the probability of relit images in Eq. (1) without using an explicit physically-based model of the object's lighting or materials. First, let us introduce a latent variable \(Z\) that can be thought of as implicitly representing the input images' lighting along with the object's material and geometry parameters. We can write the likelihood of the relit data as:

\[p(^{T}|)= p(^{T},Z|)dZ= p (^{T}|Z,)p(Z|)dZ.\] (3)

Introducing these latent variables lets us consider all relit renderings in the dataset, \(^{T}_{i}(I^{T}_{i},_{i})\), as conditionally independent, since the rendering under the target lighting \(L^{T}\) is deterministic given the object's geometry and materials. This enables writing the likelihood as:

\[p(^{T}|)=^{N}\ p( ^{T}_{i}|Z_{i},_{i})]}_{})}_{}dZ.\] (4)

We propose to model this with a latent NeRF model, as used by Martin-Brualla _et al._ that is able to render novel views under the target illumination for any sampled latent vector. We describe this model in Sec. 3.3. We train this NeRF model by generating a large quantity of sampled relit images with the same target lighting but with different (unknown) latent vectors using a _Relighting Diffusion Model_ which we will describe in Sec. 3.4. In this way, the latent NeRF model effectively distills a large dataset of relit images sampled by the diffusion model into a single 3D representation that can render novel views of the object under the target lighting for any sampled latent.

Figure 2: **Overview. Given a set of images \(I\) and camera poses \(\) in (a), we run NeRF to extract the 3D geometry as in (b). Based on this geometry and a target light shown in (c), we create radiance cues for each given input view as in (d). Next, we independently relight each input image using a single-image Relighting Diffusion Model illustrated in (e) and sample \(S\) possible solutions for each given view displayed in (f). Finally, we distill the relit set of images into a 3D representation through a Latent NeRF optimization as in (g) and (h).**

### Latent NeRF Model

We wish to model the distribution in Eq. (4) in a manner that lets us render images that correspond to relit views of the object for any sampled latent \(Z\). We choose to model this with a latent code NeRF 3D representation, inspired by prior works that condition NeRFs on latent codes to represent sources of variation such as the time of day during capture . This latent NeRF optimizes a set of latent codes that are used to condition the view-dependent color function represented by the NeRF, enabling it to render novel views of the relit object under the target illumination for any sampled latent code. In our implementation, the latent NeRF's geometry does not depend on the latent code, so the latent code may be interpreted as only representing the object's material properties.

To optimize the parameters \(\) of the latent NeRF model, we maximize the log-likelihood, which by using Eq. (4), can be written as the following maximization problem:

\[^{}=*{argmax}_{} p(_{}^{T} |)=*{argmax}_{}[_{i=1}^{N}p( _{i}^{T}|Z_{i},_{i})]p(Z|)dZ.\] (5)

Because integrating over all possible latents \(Z\) is intractable, we use a heuristic inference strategy and replace the integral with the maximum a posteriori (MAP) estimate of \(Z\):

\[^{}*{argmax}_{}_{Z}\{_{i=1 }^{N} p(_{i}^{T}|Z_{i},_{i})+ p(Z|) \}.\] (6)

By assuming a Gaussian model over the data given the materials, the first term in Eq. (6) is a reconstruction loss over the images. However, since we do not have access to the true latent vector \(Z\), we assume a uniform prior over them, turning the second term in Eq. (6) into a constant. In practice, similar to prior work on NeRFs optimized to generate new views given a dataset containing images with varying appearance, we rely on the NeRF model to resolve any mismatches in the appearance of different images . See Fig. 3 for illustrations. The minimization of the negative log-likelihood can then be written as:

\[^{}=*{arg}_{}_{Z}_{i=1}^{N}\|_{i}^{T}-(,Z_{i},_{i})\|^{2}.\] (7)

### Relighting Diffusion Model

In order to train the latent NeRF model described in Sec. 3.3, we use a Relighting Diffusion Model (RDM) to generate \(S\) samples for each viewpoint from \(p(_{i}^{T}|_{i})\). In other words, given an input image and target lighting \(L^{T}\), the single-image RDM samples \(S\) images corresponding to relit

Figure 3: **Relit samples _vs_. latent NeRF. (a) Samples of the Relighting Diffusion Model (Sec. 3.4) for the same target environment map, and (b) renderings from the optimized Latent NeRF (Sec. 3.3) for a fixed value of the latent. The diffusion samples correspond to different latent explanations of the scene and our latent NeRF optimization is able to effectively optimize these latent variables along with the NeRF model’s parameters to produce consistent renderings for each latent explanation.**versions of \(D_{i}\) that have a high likelihood given the new target light \(L^{T}\). We then associate each sample \(s\{1,,S\}\) with its own latent code \(Z_{i,s}\) and sum over all samples when training the latent NeRF (Eq. (7)).

Our RDM is implemented as an image denoising diffusion model that is conditioned by the input image and target lighting. To encode the target lighting, we use image-space radiance cues [15; 44; 61], visualized in Fig. 4. These radiance cues are generated by using a simple shading model to render a handful of images of the object's estimated geometry under the target lighting. This procedure is designed to provide information about the effects of specularities, shadows, and global illumination, without requiring the diffusion network to learn these effects from scratch. In our experiments, we use four different pre-defined materials to render radiance cues: one diffuse material with a pure white albedo, and three purely-specular materials with roughness values \(\{0.05,0.13,0.34\}\). We use GGX  as the shading model. For more details, please refer to Sec. A.2.

The RDM architecture consists of a pretrained latent image diffusion model, similar to StableDiffusion , and uses a ControlNet  based approach to condition on the radiance cues. Please refer to Sec. A.3 for more architecture details.

## 4 Experiments

### Experimental Setup

Relighting DatasetWe render objects from Objaverse  under varying poses and illuminations. For each object, we randomly sample 4 poses, and render each under 4 different lighting conditions. We represent the lighting as HDR environment maps, and randomly sample from a dataset of 509 environment maps from Polyhaven . For more details, see Sec. A.4.

Evaluation DatasetsWe evaluate our method on two datasets: TensoIR , a synthetic benchmark, and Stanford-ORB , a real-world benchmark. TensoIR contains renderings of four synthetic objects rendered under six lighting conditions. Following , we use the training split of 100 renderings with "sunset" lighting as input \(\{I_{i}\}\). We then evaluate on 200 poses, each of which has renderings under five different environment maps, _i.e._, "bridge", "city", "fireplace", "forest", and "night", for a total of \(4000\) renderings. Stanford-ORB is a real-world benchmark for inverse rendering on data captured in the wild. It contains 14 objects with various materials and captures each object under three different lighting settings, resulting in 42 (object, lighting) pairs. For the task of relighting, we are given images of an object under a single lighting condition and follow the benchmark protocol to evaluate relit images of the object under the two target lighting settings.

BaselinesWe compare our method to several existing inverse rendering approaches. On both benchmarks, we compare to NeRFactor  and InvRender . On the synthetic benchmark, we additionally compare to TensoIR , the current top-performing approach on that benchmark. For the Stanford-ORB benchmark, we additionally compare to PhySG , NVDiffRec , NeRD , NVDiffRecMC , and Neural-PBIR .

Our Model InferenceAt inference time, the ideal embedding vector \(Z\) that best corresponds to the actual material is unknown. One approach to find this vector is to optimize \(Z\) to match a subset of the test set images (as in ). However, to ensure a fair comparison, we avoid this optimization. Instead, we set \(Z=0\) for all views when rendering test images.

Figure 4: Example radiance cues for a view of the ‘hotdog’ scene.

Evaluation MetricsFor both benchmarks, we evaluate the quality of 3D relighting by reporting image metrics for rendered images. We report PSNR, SSIM , and LPIPS-VGG  on low dynamic range (LDR) images. Additionally, we report PSNR on high dynamic range (HDR) images on Stanford-ORB following the benchmark protocol, denoted as PSNR-H while the PSNR on LDR images is marked as PSNR-L. For approaches that do not produce HDR renderings, including ours, we convert the LDR renderings to linear values by using the inverse of the sRGB tone mapping curve. Due to the inherent ambiguities for the relighting task, we follow prior works [23; 27] and apply a channel-wise scale factor to RGB channels to match the ground truth image before computing metrics. Following established evaluation practices on Stanford-ORB, we compute the scale per output image individually whereas for TensoIR we compute a global scale factor that is used for all output images.2

### Benchmarking

Unless otherwise specified, all results are produced using \(S=16\) samples (see Sec. 3.4) and make use of 16 A100 40GB GPUs (batch size of \(2^{14}\) rays for NeRF optimization). We also provide results on a single A100 40GB GPU (batch size of \(2^{13}\) for NeRF optimization).

We report quantitative results on the TensoIR benchmark in Tab. 1, and show qualitative examples in Fig. 5. We significantly outperform all competitors quantitatively on all metrics with comparable or improved wall-clock time. Visually our method is capable of recovering specular highlights whereas prior methods struggle to model these.

Similarly, we report results on Stanford-ORB in Tab. 2 and Fig. 6. Our proposed approach quantitatively improves upon all baselines, except those of Neural-PBIR , indicating the effectiveness of IllumiNeRF in real world scenarios. Note that although Neural-PBIR achieves better metrics than us, Fig. 6 shows that their relighting results are mostly diffuse, even for highly-glossy objects, and that they lack many of the strong specular highlights that our method is able to recover. This behavior of their model may explain their better metrics despite worse qualitative performance for specular highlights, because the illumination maps provided by Stanford-ORB do not correspond to the

    & PSNR\(\) & SSIM\(\) & LPIPS\(\) & Wall-clock Time\(\) & Device \\  NeRFactor  & 23.383 & 0.908 & 0.131 &  100 h & a RTX 2080 Ti \\ InvRender  & 23.973 & 0.901 & 0.101 & 15 h & a RTX 2080 Ti \\ TensoIR  & 28.580 & 0.944 & 0.081 & 5 h & a RTX 2080 Ti \\  Ours & 29.709 & 0.947 & 0.072 & 0.75 h + 1 h + 0.75 h & 16 A100 40GB + a TPUv5 \\ Ours (single GPU) & 29.245 & 0.946 & 0.073 & 2 h + 1 h + 2 h & a A100 40GB + a TPUv5 \\   

Table 1: **TensoIR benchmark .** We evaluate four objects. Each object has five target lightings, each of which is associated with \(200\) poses, resulting in evaluating \(4000\) renderings in total. Running time for baselines are copied from . Our time is A (geometry optimization on GPU) + B (diffusion sampling on TPU) + C (latent NeRF optimization on GPU). **Best** and **2nd-best** are highlighted.

Figure 5: **Qualitative results on TensoIR.** Renderings from all approaches have been rescaled with respect to the ground-truth as mentioned in Eq. (4.1). Unlike TensoIR, our method faithfully recovers specular highlights and colors as indicated in red.

incident illumination at the object's location, since they were captured using a light probe which was moved for each image in the dataset . This means that even given perfect materials and geometry, the images relit by _any_ method cannot match with the true captured images, which is most noticeable in specular highlights. This mismatch penalizes methods like ours, which recover such specularities, over ones that recover mostly diffuse appearance with no apparent specular highlights . For a more detailed discussion see Sec. B.

We also provide qualitative results for different latent codes in Fig. 7. These results demonstrate that the optimized latent codes effectively capture various plausible explanations of the materials.

Figure 6: **Qualitative results on Stanford-ORB.** Renderings from all approaches have been rescaled with respect to the ground-truth as mentioned in Sec. 4.1. Areas where our approach performs well are highlighted. Our approach produces high-quality renderings with plausible specular reflections.

### Ablations

We evaluate ablations of our model on TensoIR's hotdog scene in Tab. 3, and visualize them in Fig. 8. We reach the following conclusions: **1) The latent NeRF model is essential:** optimizing a standard NeRF cannot reconcile variations across views, even if we only generate a single sample per viewpoint for optimization (\(S=1\)). **2) More diffusion samples help:** by increasing \(S\), the number of samples from the RDM per viewpoint, we observe consistent improvements across almost all metrics. This corroborates our intuition that using an increased number of samples helps the latent NeRF effectively fit the target distribution (Eq. (4)) in a more stable way.

    & PSNR-H\(\) & PSNR-L\(\) & SSIM\(\) & LPIPS\(\) \\  NVDiffRecMC \(\) & 25.08 & 32.28 & 0.974 & 0.027 \\ NVDiffRec \(\) & 24.93 & 32.42 & 0.975 & 0.027 \\  PhySG  & 21.81 & 28.11 & 0.960 & 0.055 \\ NVDiffRec  & 22.91 & 29.72 & 0.963 & 0.039 \\ NeRD  & 23.29 & 29.65 & 0.957 & 0.059 \\ NeRFactor  & 23.54 & 30.38 & 0.969 & 0.048 \\ InvRender  & 23.76 & 30.83 & 0.970 & 0.046 \\ NVDiffRecMC  & 24.43 & 31.60 & 0.972 & 0.036 \\ Neural-PBIR  & 26.01 & **33.26** & **0.979** & **0.023** \\  Ours & 25.42 & 32.62 & 0.976 & 0.027 \\ Ours (single GPU) & 25.56 & 32.74 & 0.976 & 0.027 \\   

Table 2: **Stanford-ORB benchmark . We evaluate 14 objects, each of which was captured under three different lightings. For each (object, lighting) pair, we evaluate renderings of the same object under the other two lightings, resulting in evaluating 836 renderings. \(\)denotes models trained with the ground-truth 3D scans and pseudo materials optimized from light-box captures. Best and 2nd-best are highlighted.**

Figure 7: **Renderings from various latents. Each column shows 1) a Relighting Diffusion Model (RDM) sample and 2) two latent NeRF renderings using the sample’s latent code. The diffusion samples are selected uniformly from all \(N\) (#views) \(\)\(S\) (#samples per view) diffusion generations. Each row shows results from the same object and lighting with latent codes capturing various plausible explanations of the materials.**

### Limitations

Our model relies on high quality geometry estimated by UniSDF  (see Sec. A.1) to provide sufficiently good radiance cues for conditioning the RDM (Sec. 3.4). Any missing structure will lead our model to miss specular reflections, as seen on the top left of the salt can result in Fig. 6's second column. Errors in geometry also affect the quality of synthesized novel views, _e.g_. the missing thin branches from the plant in Fig. 5 or fine details of the cactus (column 4) in Fig. 6. Note that our RDM, trained on high-quality synthetic geometry, will inherently improve with future advances in geometry reconstruction. Our approach is not suited for real-time relighting, as it requires generating new samples with the RDM and optimizing a NeRF for any new target lighting condition.

## 5 Conclusion

We have proposed a new paradigm for the task of relightable 3D reconstruction. Instead of decomposing an object's appearance into lighting and material factors and then relighting the object with physically-based rendering, we use a single-image Relighting Diffusion Model (RDM) to sample a varied collection of proposed relit images given a target illumination, and distill these samples into a single consistent 3D latent NeRF representation. This 3D representation can be rendered to synthesize novel views of the object under the target lighting. Perhaps surprisingly, this paradigm consistently outperforms existing inverse rendering methods on synthetic and real-world object relighting benchmarks. This new paradigm's success is likely due to the RDM's ability to generate a large number of proposals for the new relit images. This is in contrast to prior works based on inverse rendering, which first estimates a single material model and then uses it for relighting, since errors in material estimation may propagate to the relit images. We believe that this paradigm may be used to improve data capture, material and lighting estimation, and that it may be used to do so robustly on real-world data.

   S & Latent & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\ 
1 & \(\) & 24.957 & 0.921 & 0.099 \\
1 & ✓ & 26.321 & 0.925 & 0.097 \\ 
4 & ✓ & 27.409 & 0.936 & 0.087 \\ 
16 & ✓ & 27.950 & 0.939 & 0.082 \\   

Table 3: **Ablations.** We conduct ablation studies on the Hottog scene from TensoIR . We evaluate renderings of 200 novel test camera poses, each under five target environment map lighting conditions, resulting in evaluating 1000 renderings in total. **Best** is highlighted.

Figure 8: Using a standard NeRF instead of a latent NeRF model is unable to reconcile training samples with different underlying latent explanations. Using a latent NeRF model significantly increases the accuracy of rendered specular appearance, and increasing the number of samples \(S\) from the RDM used to train the latent NeRF model further increases the quality of the output renderings.