# Adaptive Selective Sampling for Online Prediction

with Experts

Rui M. Castro

Eindhoven University of Technology,

Eindhoven Artificial Intelligence Systems Institute (EAISI)

rmcastro@tue.nl &Fredrik Hellstrom

University College London

f.hellstrom@ucl.ac.uk &Tim van Erven

University of Amsterdam

tim@timvanerven.nl

###### Abstract

We consider online prediction of a binary sequence with expert advice. For this setting, we devise label-efficient forecasting algorithms, which use a selective sampling scheme that enables collecting much fewer labels than standard procedures. For the general case without a perfect expert, we prove best-of-both-worlds guarantees, demonstrating that the proposed forecasting algorithm always queries sufficiently many labels in the worst case to obtain optimal regret guarantees, while simultaneously querying much fewer labels in more benign settings. Specifically, for a scenario where one expert is strictly better than the others in expectation, we show that the label complexity of the label-efficient forecaster is roughly upper-bounded by the square root of the number of rounds. Finally, we present numerical experiments empirically showing that the normalized regret of the label-efficient forecaster can asymptotically match known minimax rates for pool-based active learning, suggesting it can optimally adapt to benign settings.

## 1 Introduction

This paper considers online prediction with expert advice in settings where collecting feedback might be costly or undesirable. In the classical framework of sequence prediction with expert advice, a forecasting algorithm aims to sequentially predict a stream of labels on the basis of predictions issued by a number of experts (see, for instance,  and references therein). Typically, the forecaster receives the correct label after making a prediction, and uses that feedback to update its prediction strategy. There are, however, situations where collecting labels is costly and potentially unnecessary. In the context of online prediction, this naturally leads to the notion of _selective sampling_ strategies, also called _label-efficient prediction_. In this line of work, there is a natural tension between performance (in terms of regret bounds) and label complexity, i.e., the number of labels collected. For a worst-case scenario, the optimal label-efficient strategy amounts to "flipping a coin" to decide whether or not to collect feedback, irrespective of past actions and performance . Indeed, in the worst case, the number of labels that one has to collect is linear in the number of rounds for any algorithm . This is a rather pessimistic perspective, and can miss the opportunity to reduce label complexity when prediction is easy. With this in mind, the adaptive selective sampling algorithms we develop follow naturally from a simple design principle: optimize the label collection probability at any time while preserving worst-case regret guarantees. This principled perspective leads to a general way to devise simple but rather powerful algorithms. These are endowed with optimal worst-caseperformance guarantees, while allowing the forecaster to naturally adapt to benign scenarios and collect much fewer labels than standard (non-selective sampling) algorithms.

From a statistical perspective, the scenario above is closely related to the paradigm of _active learning_[11; 12; 13; 14; 15; 16]. For instance, in pool-based active learning, the learner has access to a large pool of unlabeled examples, and can sequentially request labels from selected examples. This extra flexibility, when used wisely, can enable learning a good prediction rule with much fewer labeled examples than what is needed in a _passive learning_ setting, where labeled examples are uniformly sampled from the pool in an unguided way [17; 14; 15; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27]. Our work is partly motivated by such active learning frameworks, with the aim of devising a simple and adaptive methodology that does not rely on intricate modeling assumptions.

The main contributions of this paper are novel label-efficient exponentially weighted forecasting algorithms, which optimally decide whether or not to collect feedback. The proposed approach confirms, in a sound way, the intuition that collecting labels is more beneficial whenever there is a lack of consensus among the (weighted) experts. The proposed algorithms are designed to ensure that, in adversarial settings, they retain the known worst-case regret guarantees for full-information forecasters (i.e., forecasters that collect all labels) while providing enough flexibility to attain low label complexity in benign scenarios. To characterize the label complexity of the label-efficient forecaster, we focus on a scenario where the expected loss difference between the best expert and all other experts for all \(n\) rounds is lower-bounded by \(\), and show that the label complexity is roughly \(/^{2}\), ignoring logarithmic factors. This shows that the label-efficient forecaster achieves the "best of both worlds": it smoothly interpolates between the worst case, where no method can have optimal regret with less than \(O(n)\) queries, and the benign, stochastic case, where it is sufficient to make \(O()\) queries. Finally, to further examine the performance of the label-efficient forecaster, we conduct a simulation study. We find that the performance of the label-efficient forecaster is comparable to its full-information counterpart, while collecting significantly fewer labels. Intriguingly, for a threshold prediction setting studied in , the numerical results indicate that the label-efficient forecaster optimally adapts to the underlying prediction problem, so that its normalized regret displays the same asymptotic behavior as known minimax rates for active learning.

Before formally introducing our setting, we discuss additional related work. Selective sampling for online learning was studied by [4; 5; 28], with a focus on probabilistic threshold functions and margin-based sampling strategies. Similarly,  consider kernel-based linear classifiers, and base their sampling procedure on the estimated margin of the classifier. For the same setting as we consider, [29; 30] propose a selective sampling approach based on the maximum (unweighted) prediction disagreement among the experts, and numerically demonstrate its merits. Finally, results in a similar spirit to ours have recently been established in different settings. Namely, for a strongly convex loss,  devised an algorithm for selective sampling with expert advice, which provably retains worst-case regret guarantees, where the sampling strategy is based on the variance of the forecaster's prediction.  study a setting with shifting hidden domains, and establish a tradeoff between regret and label complexity in terms of properties of these domains. For a setting where the hypothesis class has bounded VC dimension and the data satisfies a Tsybakov noise condition,  devise a sampling strategy, with bounds on the regret and label complexity, based on a notion of disagreement where hypotheses are discarded based on their discrepancy relative to the empirical risk minimizer.

## 2 Setting

Throughout, we focus on a binary prediction task with the zero-one loss as a performance metric. We refer to \(y_{t}\) as the outcome at time \(t[n]\{1,,n\}\). No assumptions are made on this sequence, which can potentially be created in an adversarial way. To aid in the prediction task, the forecaster has access to the predictions of \(N\) experts. The prediction of the forecaster at time \(t\) can only be a function of the expert predictions (up to time \(t\)) and the observed outcomes up to time \(t-1\). Furthermore, the algorithm can make use of internal randomization.

Formally, let \(f_{i,t}\{0,1\}\), with \(i[N]:=\{1,,N\}\) and \(t[n]\), denote the advice of the experts. At every time \(t[n]\), the forecasting algorithm must: (i) output a prediction \(_{t}\) of \(y_{t}\); (ii) decide whether or not to observe \(y_{t}\). Specifically, for each round \(t[n]\):

* The environment chooses the outcome \(y_{t}\) and the expert advice \(\{f_{i,t}\}_{i=1}^{N}\). Only the expert advice is revealed to the forecaster.

* The forecaster outputs a (possibly randomized) prediction \(_{t}\), based on all of the information that it has observed so far.
* The forecaster decides whether or not to have \(y_{t}\) revealed. We let \(Z_{t}\) be the indicator of that decision, where \(Z_{t}=1\) if \(y_{t}\) is revealed and \(Z_{t}=0\) otherwise.
* A loss \((_{t},y_{t}):=\{_{t} y_{t}\}\) is incurred by the forecaster and a loss \(_{i,t}:=(f_{i,t},y_{t})\) is incurred by expert \(i\), regardless of the value of \(Z_{t}\).

Our goal is to devise a forecaster that observes as few labels as possible, while achieving low regret with respect to any specific expert. Regret with respect to the best expert at time \(n\) is defined as

\[R_{n}:=L_{n}-_{i[N]}L_{i,n}\,\]

where \(L_{n}:=_{t=1}^{n}(_{t},y_{t})\) and \(L_{i,n}:=_{t=1}^{n}(f_{i,t},y_{t})\). Note that the regret \(R_{n}\) is, in general, a random quantity. In this work, we focus mainly on the expected regret \([R_{n}]\).

Clearly, when no restrictions are imposed on the number of labels collected, the optimal approach would be to always observe the outcomes (i.e., take \(Z_{t}=1\) for all \(t[n]\)). This is optimal in a worst-case sense, but there are situations where one can predict as efficiently while collecting much fewer labels. The main goal of this paper is the development and analysis of methods that are able to capitalize on such situations, while still being endowed with optimal worst-case guarantees.

### Exponentially weighted forecasters

All proposed algorithms in this paper are variations of _exponentially weighted forecasters_. For each time \(t[n]\), such algorithms assign a weight \(w_{i,t} 0\) to the \(i\)th expert. The forecast prediction at time \(t\) and decision whether to observe the outcome or not are randomized, and based exclusively on the expert weights and the expert predictions at that time. Therefore, \(_{t}(p_{t})\) and \(Z_{t}(q_{t})\) are conditionally independent Bernoulli random variables given \(p_{t}\) and \(q_{t}\). Here, \(p_{t}\) and \(q_{t}\) depend on the past only via the weights \(\{w_{i,j-1}\}_{i[N],j[t]}\) and the current expert predictions \(\{f_{i,t}\}_{i[N]}\). The exact specifications of \(p_{t}\) and \(q_{t}\) depend on the setting and assumptions under consideration.

After a prediction has been made, the weights for all experts are updated using the exponential weights update based on the importance-weighted losses \(_{i,t}Z_{t}/q_{t}\). Specifically,

\[w_{i,t}=w_{i,t-1}\ e^{-Z_{t}}{q_{t}}}, \]

where \(>0\) is the learning rate. To ensure that the definition in (1) is sound for any \(q_{t} 0\), we set \(w_{i,t}=w_{i,t-1}\) if \(q_{t}=0\). Finally, we define the weighted average of experts predicting label \(1\) at time \(t\) as

\[A_{1,t}:=^{N}w_{i,t-1}f_{i,t}}{_{i=1}^{N}w_{i,t-1}}. \]

This quantity plays a crucial role in our sampling strategy. We will use the name exponentially weighted forecaster liberally to refer to any forecaster for which \(p_{t}\) is a function of \(A_{1,t}\). Throughout, we assume that the weights for all forecasters are uniformly initialized as \(w_{i,0}=1/N\) for \(i[N]\).

## 3 Regret bounds with a perfect expert

In this section, we consider a very optimistic scenario where one expert is perfect, in the sense that it does not make any mistakes. The results and derivation for this setting are didactic, and pave the way for more general scenarios where this assumption is dropped. We say that the \(i\)th expert is perfect if \(_{i,t}=0\) for all \(t[n]\). The existence of such an expert implies that \(_{i[N]}L_{i,n}=0\). Therefore, the regret of a forecaster is simply the number of errors it makes, that is, \(R_{n}=L_{n}\). In such a scenario, any reasonable algorithm should immediately discard experts as soon as they make even a single mistake. For an exponentially weighted forecaster, this is equivalent to setting \(=\). Due to the uniform weight initialization, the scaled weight vector \(N(w_{1,t},,w_{N,t})\) is thus binary, and indicates which experts agree with all the observed outcomes up to time \(t\).

First, consider a scenario where the forecaster always collects feedback, that is, \(q_{t}=1\) for all \(t[n]\). A natural forecasting strategy at time \(t\) is to _follow the majority_, that is, to predict according to the majority of the experts that have not made a mistake so far. When the forecaster predicts the wrong label, this implies that at least half of the experts still under consideration are not perfect. Since the number of experts under consideration is at least halved for each mistake the forecaster incurs, this strategy is guaranteed to make at most \(_{2}N\) mistakes. Therefore, we have

\[R_{n}=L_{n}_{2}N. \]

Clearly, this implies the following bound on the expected cumulative loss, and thus the regret:

\[_{n}^{(N)}:=[L_{n}]_{2}N. \]

Here, the superscript \((N)\) explicitly denotes the dependence on the number of experts. This bound is tight when the minority is always right and nearly equal in size to the majority.

A natural question to ask is if there exists an algorithm that achieves the expected cumulative loss bound (4) while not necessarily collecting all labels. This is, in fact, possible. The most naive approach is to not collect a label if all experts still under consideration agree on their prediction, as in that case, they must all be correct due to the existence of a perfect expert. However, a more refined strategy that can collect fewer labels is possible, leading to the following theorem.

**Theorem 1**.: _Consider the exponentially weighted follow-the-majority forecaster with \(=\). Specifically, let \(p_{t}=\{A_{1,t} 1/2\}\), so that \(_{t}=\{A_{1,t} 1/2\}\). Furthermore, let_

\[q_{t}=0&A_{1,t}\{0,1\},\\ -(A_{1,t},1-A_{1,t})}&\]

_For this forecaster, we have_

\[_{n}^{(N)}_{2}N\.\]

Recall that \(A_{1,t}\) is simply the proportion of experts still under consideration that predict \(y_{t}=1\). It is insightful to look at the expression for \(q_{t}\), as it is somewhat intuitive. The bigger the disagreement between the experts' predictions, the higher the probability that we collect a label. Conversely, when \(A_{1,t}\) approaches either \(0\) or \(1\), \(q_{t}\) quickly approaches zero, meaning we rarely collect a label. Theorem 1 tells us that, remarkably, we can achieve the same worst-case bound as the full-information forecaster while sometimes collecting much less feedback. The proof of this result, in Appendix A, uses a clean induction argument that constructively gives rise to the expression for \(q_{t}\). This principled way of reasoning identifies, in a sense, the best way to assess disagreement between experts: the specified \(q_{t}\) is the lowest possible sampling probability that preserves worst-case regret guarantees.

A slightly better regret bound is possible by using a variation of follow the majority, called the boosted majority of leaders. For this algorithm, the upper bound is endowed with a matching lower bound (including constant factors). In Appendix B, we devise a label-efficient version of the boosted majority of leaders, retaining the same worst-case regret bound as its full-information counterpart.

## 4 General regret bounds without a perfect expert

In this section, we drop the assumption of the existence of a perfect expert. It is therefore no longer sensible to use an infinite learning rate \(\), since this would discard very good experts based on their first observed error. We consider the general exponentially weighted forecaster described in Section 2.1, now simply with \(p_{t}=A_{1,t}\).

For the scenario where \(q_{t}=1\) for all \(t\), a classical regret bound is well-known (see, for instance, (Bishop, 2006, Thm 2.2)). Specifically, for the general exponentially weighted forecaster, with \(p_{t}=A_{1,t}\), \(q_{t}=1\), and uniform weight initialization, we have

\[_{n}:=[R_{n}]=L_{n}-_{i[N]}L_{i,n} +. \]

In Theorem 2 below, we prove a stronger version of (5) that allows for an adaptive label-collection procedure. As before, we focus on the expected regret, \(_{n}=[R_{n}]\). As done in Section 3 for the case of a perfect expert, we identify an expression for \(q_{t}\), which is not necessarily identically \(1\), but still ensures the bound in (5) is valid. To state our main result, we need the following definition, which is guaranteed to be sound by Lemma 1.

**Definition 1**.: _For \(x\) and \(>0\), define_

\[q^{*}(x,)=\{q(0,1]:x+(1-x+xe ^{-/q}),. \] \[.1-x+(x+(1-x)e^{-/q}) \}\.\]

In the following theorem, we present the label-efficient version of (5).

**Theorem 2**.: _Consider an exponentially weighted forecaster with \(p_{t}=A_{1,t}\) and_

\[q_{t} q^{*}(A_{1,t},):=q_{t}^{*}\.\]

_For this forecaster, we have_

\[_{n}=\![L_{n}-_{i[N]}L_{i,n}]+. \]

The proof, which is deferred to Appendix C, is similar to that used for Theorem 1, but with key modifications to account for the lack of a perfect expert. In particular, we need to account for the finite, importance-weighted weight updates, and carefully select \(q_{t}^{*}\) accordingly. While the proof allows for non-uniform weight initializations, we focus on the uniform case, as this enables us to optimally tune the learning rate. The result for general weight initializations is given in Appendix C.

Theorem 2 shows that the proposed label-efficient forecaster satisfies the same expected regret bound as the exponentially weighted forecaster with \(q_{t}:=1\). While the expression for \(q^{*}(x,)\) in (6) is somewhat opaque, the underlying motivation is constructive, and it arises naturally in the proof of the theorem. In fact, \(q_{t}^{*}\) is the smallest possible label-collection probability ensuring the regret bound (7). One may wonder if \(q_{t}^{*}\) is well defined, as it is the infimum of a set that may be empty. However, as shown in the following lemma, this set always contains the point \(1\), ensuring that \(q_{t}^{*} 1\).

**Lemma 1**.: _For all \(>0\) and \(x\), we have_

\[1\{q(0,1]:x+(1-x+xe^{-/q}) {}{8}\}\.\]

The proof is presented in Appendix D, and is essentially a consequence of Hoeffding's inequality. While \(q^{*}(x,)\) does not admit an analytic solution, its behavior as a function of \(x\), depicted in Figure 1, is rather intuitive. Since \(=\) minimizes the regret bound (7), we are primarily interested in small values of \(\). When the learning rate \(\) is not too large, the behavior of \(q_{t}^{*}\) can be interpreted as follows: the larger the (weighted) disagreement of the experts is, the closer the value of \(A_{1,t}\) is to the point \(1/2\). In this case, \(q_{t}^{*}\) will be close to \(1\), and we collect a label with high probability. When \(A_{1,t}\) is close to 0 or 1, the (weighted) experts essentially agree, so the probability of collecting a label will be small. For large learning rates, the behavior of \(q_{t}^{*}\) appears a bit strange, but note that for \( 8\), the regret bound is vacuous. Thus, for this case, \(q^{*}(x,)=0\) for all \(x\).

The regret guarantee in Theorem 2 is valid provided one uses any choice \(q_{t} q_{t}^{*}\). The following lemma provides both an asymptotic characterization of \(q_{t}^{*}\) as \( 0\), as well as a simple upper bound that can be used for both analytical purposes and practical implementations.

Figure 1: The function \(q^{*}(x,)\) for various values of \(\). Panel (b) is a zoomed version of panel (a).

**Lemma 2**.: _For any \(x\), we have_

\[_{ 0}q^{*}(x,)=4x(1-x)\;.\]

_Furthermore, for any \(>0\) and \(x\),_

\[q^{*}(x,)(4x(1-x)+/3,1). \]

The proof of this result is somewhat technical and tedious, and deferred to Appendix E. In the remainder of this paper, we will use this upper bound extensively.

## 5 Label complexity

We now examine the label complexity, defined as \(S_{n}:=_{t=1}^{n}Z_{t}\). In [10, Thm. 13], it is shown that there exists a setting for which the expected regret of a forecaster that collects \(m\) labels is lower-bounded by \(cn\) for some constant \(c\). Hence, in the worst case, the number of collected labels needs to be linear in \(n\) in order to achieve an expected regret that scales at most as \(\). However, since \(q_{t}^{*}\) can be less than \(1\), it is clear that the label-efficient exponentially weighted forecaster from Theorem 2 can collect fewer than \(n\) labels in more benign settings. To this end, we consider a scenario with a unique best expert, which at each round is separated from the rest in terms of its expected loss. To state the condition precisely, we need to define \(_{t}=[\,_{t-1}]\) as the expectation at time \(t\) conditional on all possible randomness up to time \(t-1\), that is, for \(_{t}=(\{Z_{j},y_{j},f_{1,j},,f_{N,j}\}_{j=1,,t})\). With this, we assume that there is a unique expert \(i^{*}[N]\) such that, for all \(i i^{*}\) and \(t[n]\),

\[_{t}[_{i,t}-_{i^{*},t}]>0\]

The parameter \(\) characterizes the difficulty of the given learning problem. If \(\) is large, the best expert significantly outperforms the others, and is thus easily discernible, whereas if \(\) is small, the best expert is harder to identify. In particular, if the vectors \((y_{t},f_{1,t},,f_{N,t})\) are independent and identically distributed over rounds \(t[n]\), \(\) is just the difference in expected loss between the best and the second-best expert in a single round, which is a common measure of difficulty for stochastic bandits [33, Thm. 2.1]. This difficulty measure has also been used in the context of prediction with expert advice . Similar stochastic assumptions are standard in (batch) active learning, and highly relevant in practical settings (see  and references therein). Strictly speaking, our result holds under a more general assumption, where the best expert _emerges after a time \(^{*}\)_ instead of being apparent from the first round. This means that the best expert is even allowed to perform the worst for some rounds, as long as it performs well in sufficiently many other rounds. While we state and prove the result under this more general condition in Appendix F, we present the simpler assumption here for clarity.

We now state our main result for the label complexity.

**Theorem 3**.: _Consider the label-efficient exponentially weighted forecaster from Theorem 2 with \(q_{t}=(4A_{1,t}(1-A_{1,t})+/3,1)\) and any \(>0\). Suppose that there exists a single best expert \(i^{*}\) such that, for all \(i i^{*}\) and all \(t[n]\),_

\[_{t}[_{i,t}-_{i^{*},t}]>0\]

_Then, for any \(n 4\), the expected label complexity is at most_

\[[S_{n}]} +3 n+1\;. \]

Proof sketch.: Initially, the sampling probability \(q_{t}\) is large, but as we collect more labels, it will become detectable that one of the experts is better than the others. As this happens, \(q_{t}\) will tend to decrease until it (nearly) reaches its minimum value \(/3\). We therefore divide the forecasting process into time \(t\) and \(t>\). With a suitable choice of \( 1/(^{2})\) (up to logarithmic factors), we can guarantee that the sampling probability is at most \(q_{t} 4/3\) for all \(t>\) with sufficiently high probability. This is shown by controlling the deviations of the cumulative importance-weighted loss differences \(_{t}^{i}=_{j=1}^{t}(l_{i,j}-l_{i^{*},j})/q_{j}\) for \(i i^{*}\) from their expected values by using an anytime version of Freedman's inequality. With this, we simply upper bound the label complexity for the first \(\) rounds by \(\), and over the remaining rounds, the expected number of collected labels is roughly \((n-) n\). This leads to a total expected label complexity of \(1/(^{2})+ n\), up to logarithmic factors. The full proof is deferred to Appendix F.

As mentioned earlier, the learning rate optimizing the regret bound (7) is \(=\). For this particular choice, the label complexity in (9) is roughly \(/^{2}\), up to constants and logarithmic factors. We have thus established that the label-efficient forecaster achieves the best of both worlds: it queries sufficiently many labels in the worst case to obtain optimal regret guarantees, while simultaneously querying much fewer labels in more benign settings. It is interesting to note that the label complexity dependence of \(1/^{2}\) on \(\) is less benign than the dependence of the regret bound from, e.g., (34, Thm. 11), which is \(1/\). The underlying reason for this is that, while the two are similar, the label complexity is not directly comparable to the regret. In particular, the label complexity has much higher variance.

The bound of Theorem 3 relies on setting the sampling probability \(q_{t}\) to be the upper bound on \(q_{t}^{*}\) from Lemma 2. This bound is clearly loose when \(q_{t}^{*}\) is approximately zero, and one may wonder if the label complexity of the algorithm would be radically smaller when using a forecaster for which \(q_{t}=q_{t}^{*}\) instead. With the choice \(=\), which optimizes the bound in (7), it seems unlikely that the label complexity will substantially change, as numerical experiments suggest that the label complexity attained with \(q_{t}\) set as \(q_{t}^{*}\) or the corresponding upper bound from (8) appear to be within a constant factor. That being said, for larger values of \(\), the impact of using the upper bound in (8) is likely much more dramatic.

## 6 Numerical experiments

To further assess the behavior of the label-efficient forecaster from Theorem 2, we consider a classical active learning scenario in a batch setting, for which there are known minimax rates for the risk under both active and passive learning paradigms. We will set the sampling probability to be

\[q_{t}=(4A_{1,t}(1-A_{1,t})+/3,1)\.\]

Let \(D_{n}=((X_{t},Y_{t}))_{t=1}^{n}\) be an ordered sequence of independent and identically distributed pairs of random variables with joint distribution \(D\). The first entry of \((X_{i},Y_{i})\) represents a feature, and the second entry is the corresponding label. The goal is to predict the label \(Y_{i}\{0,1\}\) based on the feature \(X_{i}\). Specifically, we want to identify a map \((x,D_{n})_{n}(x,D_{n})\{0,1\}\) such that, for a pair \((X,Y) D\) that is drawn independently from \(D_{n}\), we have small (zero-one loss) expected risk

\[(_{n}):=(_{n}(X,D_{n}) Y)\.\]

Concretely, we consider the following scenario, inspired by the results in (14). Let the features \(X_{i}\) be uniformly distributed in \(\), and \(Y_{i}\{0,1\}\) be such that \((Y_{i}=1|X_{i}=x)=(x)\). Specifically, let \(_{0}\) such that \((x) 1/2\) when \(x_{0}\) and \((x) 1/2\) otherwise. Furthermore, assume that for all \(x\), \((x)\) satisfies

\[c|x-_{0}|^{-1}|(x)-1/2| C|x-_{0}|^{-1}\,\]

for some \(c,C>0\) and \(>1\). If \(_{0}\) is known, the optimal classifier as \(n\). This shows that there are potentially massive gains for active learning, particularly when \(\) is close to 1. A natural question is whether similar conclusions hold for streaming active learning. In this setting, instead of selecting which example \(X_{i}^{}\) to query, the learner observes the features \((X_{1},,X_{n})\) sequentially, and decides at each time \(t\) whether or not it should query the corresponding label. This is analogous to the online prediction setting discussed in this paper.

We now study this setting numerically. For the simulations, we use the specific choice

\[(x)=+(x-_{0})|x-_{0}|^{-1}\,,\]

to generate sequences \((Y_{1},,Y_{n})\), based on a sequence of features \((X_{1},,X_{n})\) sampled from the uniform distribution on \(\). Furthermore, we consider the class of \(N\) experts such that

\[f_{i,t}=1\{X_{t}\}\,,\]

with \(i[N]\) and \(t[n]\).

### Expected regret and label complexity

In the simulations, we set \(_{0}=1/2\) and \(N=+1\{\}\). This choice enforces that \(N\) is odd, ensuring the optimal classifier is one of the experts. Throughout, we set \(=\), which minimizes the regret bound (7). First, we investigate the expected regret relative to the optimal prediction rule for the label-efficient exponentially weighted forecaster with \(q_{t}\) given by (8), and compare it with the corresponding regret for the full-information forecaster that collects all labels. Specifically, the regret at time \(t\) of a forecaster that predicts \(\{_{j}\}_{j=1}^{n}\) is given by

\[[R_{t}]=_{j=1}^{t}[(_{t},Y_{t})-(g^{*}(X _{t}),Y_{t})]\;.\]

Figure 2: Numerical results for expected regret and label complexity when \(n=50000\) and \(N=225\). Panels (a) and (b) depict the expected regret \([R_{t}]\) as a function of \(t\), for \(=2\) and \(=1.5\) respectively. Panels (c) and (d) depict the expected label complexity of the label-efficient forecaster, \([S_{t}]\), as a function of \(t\) for \(=2\) and \(=1.5\) respectively. The expectations were estimated from \(500\) independent realizations of the process and the shaded areas indicate the corresponding pointwise \(95\%\) confidence intervals.

Furthermore, to study the potential reduction in the number of collected labels, we also evaluate the expected label complexity \([S_{t}]\) of the label-efficient forecaster. To approximate the expectations above, we use Monte-Carlo averaging with \(500\) independent realizations. Further experimental details are given in Appendix G. The results are shown in Figure 2. We see that the regret is comparable for the full-information and label-efficient forecasters, albeit slightly higher for the latter. Since \(P(g^{*}(X) Y)=-}\), the expected cumulative loss of the optimal classifier grows linearly with \(t\). For instance, when \(=2\), we have \(_{j=1}^{t}[(g^{*}(X_{t}),Y_{t})]=3t/8\). Hence, the regret relative to the best expert is much smaller than the pessimistic (i.e., worst-case for adversarial environments) bound in (7). We also observe that the expected label complexity grows sub-linearly with \(t\), as expected, and that \([S_{n}] n\), demonstrating that a good prediction rule can be learned with relatively few labels. When \(=1.5\), the number of collected labels is significantly smaller than when \(=2\). This is in line with the known minimax rates for active learning from . To further examine this connection, we now turn to normalized regret.

### Normalized regret relative to the number of samples

To relate the results of the label-efficient forecaster with known minimax rates for active learning, we investigate the expected regret normalized by the number of samples. Specifically, let

\[r(t)=[R_{t}]=_{j=1}^{t}[( _{t},Y_{t})-(g^{*}(X_{t}),Y_{t})]\;.\]

For the full-information forecaster, we expect that \(r(t) t^{-/(2-1)}\) as \(t\). The same holds for the label-efficient forecaster, but in this case, the relation between \(r(t)\) and the expected number of collected labels \([S_{t}]\) is more interesting. If the label-efficient forecaster performs optimally, we expect \(r(t)[S_{t}]^{-/(2-2)}\) as \(t\). To examine this, we plot \(r(t)\) against \([S_{t}]\) (which equals \(t\) for the full-information forecaster) in logarithmic scales, so the expected asymptotic behavior corresponds to a linear decay with slopes given by \(-/(2-1)\) for the full-information forecaster and \(-/(2-2)\) for the label-efficient forecaster. This is shown in Figure 3 for \(=1.5\) and \(=2\).

We see that the observed behavior is compatible with the known asymptotics for active learning, and similar results arise when considering different values of \(\). More importantly, it appears that the label-efficient forecaster optimally adapts to the underlying setting. This is remarkable, as the label-efficient forecaster does not rely on any domain knowledge. Indeed, it has no knowledge of the statistical setting, and in particular, it has no knowledge of the parameter \(\), which encapsulates the difficulty of the learning task. Note that our regret bounds are too loose to provide a theoretical justification of these observations via an online-to-batch conversion, and that such theoretical analyses will only be fruitful when considering non-parametric classes of experts, for which the asymptotics of the excess risk are \((1/)\).

Figure 3: Numerical results for the normalized regret as a function of the expected label complexity when \(n=50000\) and \(N=225\). The straight dotted lines are displayed for comparison, and have slopes given by \(-/(2-1)\) (full information) and \(-/(2-2)\) (label efficient). The expectations were estimated from \(500\) independent realizations of the process and the shaded areas indicate the corresponding pointwise \(95\%\) confidence intervals.

Discussion and outlook

In this paper, we presented a set of adaptive label-efficient algorithms. These follow from a very straightforward design principle, namely, identifying the smallest possible label collection probability \(q_{t}\) that ensures that a known worst-case expected regret bound is satisfied. This leads to simple, yet powerful, algorithms, endowed with best-of-both-worlds guarantees. We conjecture that a similar approach can be used for a broader class of prediction tasks and losses than what is considered in this paper. For instance, the results we present can be straightforwardly extended to a setting where the expert outputs take values in \(\), as long as the label sequence and forecaster prediction remain binary and take values in \(\{0,1\}\). In fact, the same inductive approach can be used when \(y_{t}\) and one considers a general loss function. However, the resulting label collection probability will be significantly more complicated than that of Definition 1. An interesting side effect of our analysis is that it leads to an inductive proof of the regret bound for standard, full-information algorithms. Extending the label complexity result, and in particular connecting it with known minimax theory of active learning in statistical settings, remains an interesting avenue for future research. Finally, another intriguing direction is to extend our approach to the bandit setting. In the setting we consider in this paper, we observe the losses of all experts when observing a label. In contrast, in the bandit setting, only the loss of the selected arm would be observed for each round. This would necessitate the forecaster to incorporate more exploration in its strategy, and the analysis of a label-efficient version seems like it would be quite different from what is used in this paper, although some of the ideas may transfer.