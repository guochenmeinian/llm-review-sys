# Federated Learning via Meta-Variational Dropout

Insu Jeon\({}^{*}\)  Minui Hong\({}^{}\)  Junhyeog Yun\({}^{*}\)  Gunhee Kim\({}^{}\)

Seoul National University, Seoul, South Korea

\({}^{*}\){insuj30n,antemrdm}@gmail.com

\({}^{}\){alsdm1123,gunhee}@snu.ac.kr

###### Abstract

Federated Learning (FL) aims to train a global inference model from remotely distributed clients, gaining popularity due to its benefit of improving data privacy. However, traditional FL often faces challenges in practical applications, including model overfitting and divergent local models due to limited and non-IID data among clients. To address these issues, we introduce a novel Bayesian meta-learning approach called meta-variational dropout (MetaVD). MetaVD learns to predict client-dependent dropout rates via a shared hypernetwork, enabling effective model personalization of FL algorithms in limited non-IID data settings. We also emphasize the posterior adaptation view of meta-learning and the posterior aggregation view of Bayesian FL via the conditional dropout posterior. We conducted extensive experiments on various sparse and non-IID FL datasets. MetaVD demonstrated excellent classification accuracy and uncertainty calibration performance, especially for out-of-distribution (OOD) clients. MetaVD compresses the local model parameters needed for each client, mitigating model overfitting and reducing communication costs. Code is available at https://github.com/insujeon/MetaVD.

## 1 Introduction

Federated learning (FL) aims at training a global model from distributed clients without sharing or collecting their sensitive raw data. Thanks to its privacy-preserving aspect of FL , it is increasingly popular to be applied to various applications such as image classification [2; 3], object detection [4; 5], keyboard suggestion [6; 7], recommendation [8; 9], and healthcare [10; 11]. The conventional FL algorithm could achieve convergence when the data from different clients is independently and identically distributed (IID) [12; 13; 14]. However, due to the differences in preferences, locations, and usage habits of clients, the private data in FL are usually non-IID. When the data distributions of the clients vary, the local model learned from each client can diverge, and thus learning an optimal global model could fail [15; 16]. Furthermore, the scale of client data may not be sufficient to train a local model with large parameters, causing model overfitting and poor generalization [17; 18].

To overcome the challenge caused by non-IID data, personalized federated learning (PFL) has emerged [15; 16]. In the PFL, each client is allowed to have its own personalized model trained on each client's local data while still participating in the global model training. There are many branches of PFL, such as those based on multitask learning [19; 20], meta-learning [21; 22; 23; 24; 25; 26], and transfer learning [27; 28; 29; 30]. Although these approaches improve training convergence in non-IID data settings, they may still experience model overfitting with limited client data. Recently, the Bayesian learning paradigm has also been introduced in FL to address the overfitting by incorporating uncertainty in the model parameters [31; 32; 33; 34; 35; 36; 37]. However, they may also struggle with divergent local models when the data from different clients exhibit significant statistical variability. Motivated by these challenges, we aim to address the issues of FL with the non-IID and limited client data simultaneously.

In this paper, we present Meta-Variational Dropout (MetaVD), a novel Bayesian meta-learning approach [38; 39; 40; 41; 42] developed for PFL. MetaVD learns to predict client-dependent dropout ratesvia a hypernetwork [25; 43; 44; 45]. This mechanism facilitates a data-efficient estimation of client-specific posterior simply by modulating a shared global NN parameter across all clients. The adaptation of MetaVD to conventional FL algorithms (e.g., FedAvg , Reptile , MAML , or PerFedAvg ) allows flexible model personalization across a variety of non-IID client distributions. MetaVD also incorporates a unique model aggregation strategy based on client-specific dropout uncertainty, providing a principled Bayesian way to consolidate local models into a global model. This strategy significantly enhances the convergence of the FL algorithm on non-IID data. In addition, MetaVD inherits the merit of Variational Dropout (VD) [46; 47], which facilitates model compression. This not only improves model generalization for out-of-data (OOD) clients but also reduces communication costs for parameter exchange. We performed an extensive analysis on MetaVD covering a wide range of FL scenarios , including different scales, non-IID degrees, partitions, and multi-domain settings . In all of these experiments, MetaVD achieves excellent results compared to other state-of-the-art baselines.

## 2 Background

A standard approach to FL (_e.g.,_ FedAvg ) iterates between the local training on the client devices and global optimization at the server. Given \(M\) clients, each of which has a data set \(^{m}=\{(x_{i}^{m},y_{i}^{m})\}_{i=1}^{|^{}|}\), the FL problem can be formulated as follows:

\[_{w}\ (w)=_{m=1}^{M}g^{m}^{m}(w), ^{m}(w)=^{m}|}_{i}(x_{i}^{m},y_{i}^ {m};w).\] (1)

\(w\) denotes the model parameter, and the global learning objective \((w)\) at the server is a weighted average of the local objectives \(^{m}(w)\) over \(M\) clients. The weight \(g^{m}\) is proportional to the size of the local dataset (e.g., \(|^{m}|/||\)). The local loss in a client device is usually defined as the empirical negative log-likelihood on the \(m\)-th client's dataset \(^{m}\) (i.e., \((^{m};w)=- p(y^{m}|x^{m},w)\)).

The local training is carried out in parallel fully (or partly) in each client device, with multiple Stochastic Gradient Descent (SGD)  epochs to get the fine-tuned local parameter \(w^{m}\). Then, the aggregation step computes the global parameter in the server by taking the weighted average of the local parameters (i.e., \(_{m=1}^{M}g^{m}w^{m}\)). The global model parameter \(\) is then used as the initial parameter for each client in the next round of local training. FL aims to train models on large distributed datasets by only exchanging model parameters (e.g., \(\) and \(w^{m}\)) between server and local devices, thereby minimizing privacy leakage of client data.

**Challenges in FL.** There are many challenges to real-world FL applications. (1) _Heterogeneity of client data_. The original FL algorithm converges well when the client data is IID [12; 13; 14]. However, the client data often has different characteristics (e.g., classes or tasks follow non-IID). The \(w^{m}\) would drift away from each other, causing the \(\) to be suboptimal [15; 16]. (2) _Sparse participation_. In practice, the total number of clients \(M\) can be extremely large, while communication between the server and the clients can be intermittent or unreliable. This creates the challenge of inconsistent training due to a small subset of participating clients in each round of communication . (3) _Poor generalization due to limited data_. When the training data available on each local device is limited, the local model can easily overfit, resulting in poor generalization to unseen clients [17; 18]. (4) _Communication cost_. FL optimization requires frequent communication between local devices and the central server to exchange model parameters. This process is slow and could introduce additional privacy concerns. Therefore, reducing model size is also an important area of research .

**Bayesian FL.** While conventional FL methods use a point estimate of the parameter as in Eq.1, recent work [31; 32; 33] has incorporated probability distributions over the model parameters. In these Bayesian FL approaches, the client device first estimates the local posterior from its data, and then the server aggregates the partially updated local posteriors into a global posterior. Based on FedAvg, a Gaussian distribution is used to represent each parameter in FedAG , and a posterior aggregation strategy using the MCMC technique is proposed in FedPA . FedBE  uses a Bayesian ensemble global model with Gaussian or Dirichlet distributions. These methods improve prediction confidence and model convergence. However, FedAG  and FedBE  assume a global Gaussian posterior, but only consider point estimates of local parameters. FedPA  uses global and local Gaussian posteriors, but only maintains the global posterior mean on the server, making it difficult to track local uncertainties. In addition, the performance of previous methods can still be compromised by the statistical discrepancies in the client data, as they are not tailored to the heterogeneous FL scenario. Recently, Bayesian PFL methods have also been introduced for non-IID data [34; 35; 36; 37; 51; 52]. For example, pFedGP  learns a common Gaussian kernel for all clients and infers personalized classifiers for each client. pFedBayes  assumes an independent posterior model for each device while learning a shared global prior. These approaches achieve a substantial predictive performance improvement in non-IID data sensors compared to the previous Bayesian FL approaches. However, their approach still has limitations, such as the high computational cost of inverting a large kernel matrix in pFedGP  and imposing strong constraints in pFedBayes . Their model aggregation rule does not account for parameter uncertainty directly. In addition, probabilistic modeling typically requires additional parameters to approximate the density, which can increase communication costs.

## 3 Approach: Meta-Variational Dropout

We propose a new Bayesian PFL approach, called Meta-Variational Dropout (MetaVD), which can simultaneously handle model personalization, regularization, and compression. MetaVD also exploits the uncertainties in model aggregation, thereby improving the training convergence on non-IID data. In addition, MetaVD is a universal approach that is compatible with various existing FL algorithms.

### Variational Inference for FL

Instead of approximating the local posterior using MCMC in Bayesian FL [31; 32; 33], we can utilize the (amortized) Variational Inference (VI) framework of Bayesian meta-learning [38; 39; 40; 41; 42] that is originally developed for few-shot multi-task learning [53; 54; 55]. Considering each \(m\)-th client in FL as an individual task, an evidence lower-bound (ELBO) \(_{}\) over all the \(\) datasets is defined as

\[_{}_{}()=_{m=1}^{M}g^{m}\{_{q( w^{m};)}[ p(y^{m}|x^{m},w^{m})]-(q(w^{m};)||p(w^{m}))\}.\] (2)

Here, \(p(y^{m}|x^{m},w^{m})\) represents a likelihood model constructed using a neural network (NN) on each \(m\)-th client's data [56; 57; 58; 59], and \(w^{m}\) is a client-specific NN parameter. \(q(w^{m};)\) is a variational posterior (or probabilistic) distribution over the \(w^{m}\), which is also characterized by \(\). \(g^{m}\) is the local weight as defined in Eq.(1). \(p(w^{m})\) is a prior distribution that acts as a regularizer for the \(q(w^{m};)\). The local ELBO defined on each \(m\)-th client in Eq.(2) makes tradeoffs between the expected log-likelihood on its local dataset \(^{m}\) and the KL divergence with a prior. In fact, maximizing the \(_{}()\) with respect to the variational parameter \(\) is equivalent to minimizing \(_{t=1}^{M}g^{m}(q(w^{m};)||p(w^{m}|^{m}))\). In theory, \(q(w^{m};)\) is trained to approximate the true local posterior distribution over the \(w^{m}\) (i.e., \(q(w^{m};) p(w^{m}|^{m})\)) [56; 57; 58; 59].

A straightforward Bayesian PFL approach might be utilizing a separate local posterior model \(q(w^{m};)\) for each client device (e.g.,, \(=(^{1},,^{M})\)). However, only a sparse subset of clients can participate in each FL round due to the communication availability of edge devices. Moreover, the number of clients \(M\) can be extremely large in practice, and some clients might only have small datasets. Thus, learning the variational parameter \(^{m}\) independently for each device is difficult. As an alternative, we introduce a new hypernetwork-based [25; 43; 44; 45] conditional dropout posterior modeling approach that can be data-efficiently trained across multiple clients in FL.

### Meta-Variational Dropout

**Posterior model.** To promote efficient model personalization and reduce overfitting in Bayesian FL, we define the posterior model in Eq.2 based on a Variational Dropout (VD) technique that multiplies continuous Gaussian noise to the NN parameters during training to prevent overfitting [46; 47; 60; 61; 62]. MetaVD extends the VD posterior by employing a global hypernetwork that learns to predict client-specific dropout rates (or personal model structure). In MetaVD, the variational posterior distribution for each \(m\)-th client's parameter, \(q(w^{m};)\) in Eq.2, can be constructed as

\[q(w^{m};=(,,e^{m}))=_{k=1}^{K}(w_{k}^{m}| _{k},_{k}^{m}_{k}^{2})^{m}=h_{}(e^{m}).\] (3)The variational parameter \(\) is characterized by three distinct parameters \((,,e)\). The \(\) is a global NN model parameter kept at the server (with no client index \(m\)), where \(K\) is the size of the model parameter. The posterior distribution over the \(m\)-th client's weight \(w^{m}\) is described as a product of independent Gaussian distributions. In the product term in Eq.(3), each \(k\)-th factor describes a conditional Gaussian noise multiplication to the global NN parameter (e.g., \(w^{m}=*^{m}\) where \(^{m}(},^{m})\) and \(}\) is an \(K\)-dimensional all-one vector). The \(_{k}^{m}\) represents the client-specific dropout variable1 on each \(k\)-th index of NN parameter \(_{k}\). The \(h_{}\) indicates a hypernetwork parameterized by \(\) predicts the client-specific dropout rate \(^{m}\). The \(e^{m}\) is a learnable client embedding used as an input to the \(h_{}\). In essence, MetaVD is a technique that modulates a global NN parameter with Gaussian noises. By predicting the client-specific dropout variable via a hypernetwork, a single NN can be reconfigured for various different clients. Since learning a hypernetwork across multiple local clients is more efficient than learning all the local posteriors independently, this approach can mitigate the sparse client participation and limited data issues in FL.

**Prior model.** To optimize the posterior model \(q(w^{m};)\), we need to specify the KL divergence term and the prior model \(p(w^{m})\) in Eq.(2). Prior modeling in MetaVD requires two criteria: (i) the KL divergence terms must be independent of the NN parameter \(\) to ensure the lower-bound assumption in VD , and (ii) all clients must share the same prior model to support the multiplicative posterior aggregation rule in Bayesian FL. We adopt the hierarchical prior  discussed in  because of its straightforward analytic KL term derivation and proven effectiveness in network regularization and sparsification. Under the hierarchical prior assumption, the KL divergence term in Eq.(2) is simplified to \((q(w^{m};)||p(w^{m}))=_{k=1}^{K}0.5(1+(_{k}^{m})^{- 1})\); see Appendix B for a more detailed derivation. This KL term is independent of the global NN parameter \(\) and efficiently regularizes the dropout variable. The same hierarchical prior is applied to all \(M\) clients.

**Client-side objective.** Initially, the hypernetwork in the server approximates the dropout variable \(^{m}\) for each \(m\)-th client. The global parameter \(\) and \(^{m}\) are transmitted to each client device. Then, the posterior model in a \(m\)-th client is trained on local data using the following ELBO objective:

\[_{,^{m}}_{}^{m}(,)= {|^{m}|}_{i} p(y_{i}^{m}|x_{i}^{m},f(;, ^{m}))-_{k=1}^{K}0.5(1+(_{k}^{m})^{-1}),\] (4)

which maximizes \(_{}^{m}\) on the client dataset \(^{m}=\{(x_{i}^{m},y_{i}^{m})\}_{i=1}^{|^{m}|}\) with respect to the variational parameters (i.e., \(\), \(^{m}\)). The optimization can be done by the _stochastic gradient variational Bayes_ (SGVB). , which reparameterizes the random weight variable \(w^{m}\) using a differentiable transformation as \(w^{m}_{k}=f(_{k};_{k},^{m}_{k})=_{k}+^{m}}_{k}_{k}\). with a random IID noise \(_{k}(0,1)\). The intermediate weight \(w^{m}_{k}\) is now differentiable with respect to \(_{k}\), hence \(^{m}_{k}\), and can be optimized via the SGD . The analytical KL term acts as a regularization for \(^{m}_{k}\).

**Server-side objective.** Once a local posterior adaptation is done in each client device, the updated parameters \(^{m}_{*}\) and \(^{m}_{*}\) are returned to the server, then the server updates the variational parameters

Figure 1: Overview of the Meta-Variational Dropout algorithm. (a) The server’s hypernetwork predicts client-specific dropout rates from client embedding, \(e^{m}\). The global parameters, \(\), and dropout variables, \(^{m}\), are sent to each \(m\)-th client. (b) A few-shot local adaptation is performed on each client’s device in parallel; then, the updated parameters are sent back to the server. (c) The server then aggregates those parameters to update its variational parameters \(\), \(\), and \(e\).

(e.g., \(,,e^{m}\)) using the updated and local parameters. To update the global NN parameter \(\), we first assume the Bayesian posterior aggregation rule: \(p(w|)_{m=1}^{M}p(w^{m}|^{m})\). Since each local posterior in Eq.(3) is a Gaussian (dropout) distribution, their product is also Gaussian: \((w|_{*}^{},)_{m=1}^{M} (w^{m}|_{*}^{m},_{*}^{m}(_{*}^{m})^{2})\). This gives us an exact aggregation rule2 to compute the maximum a posterior (MAP) solution of the \(_{*}^{}\) as follows:

\[_{*}^{}=_{m}r^{m}_{*}^{m}\ r^{m}=(_{*}^{m}(_{*}^{m})^{2})^{-1}}{_{m}g^{m}( _{*}^{m}(_{*}^{m})^{2})^{-1}}.\] (5)

Note that Eq.(5) has the intuitive interpretation that the aggregation weight \(r^{m}\) is inversely proportional to its corresponding dropout variable (or noise variance) \(_{*}^{m}\). Thus, parameters with high uncertainty have correspondingly less influence on the global mean prediction. In this way, we can fully exploit the uncertainty of the model parameters in the aggregation. To fully update each global NN parameter \(_{k}\), we follow the similar parameter update rule of FedAvg, except that the Bayesian aggregation Eq.(5) is utilized as in Algo.1. For the parameter of hypernetwork \(\), a more general update rule is used following . We compute the changes in the updated dropout for each client \(^{m}\) as described in Algo.1, then an approximated gradient of the hypernetwork parameter is computed using the chain rule as \(_{}_{}^{m}(^{m})=(_{}^ {m})^{T}^{m}\), where \(_{}^{m}\) is the gradient of the output of hypernetwork with respect to \(\). \(^{m}\) is an approximation of the vector-jacobian product, which we are inspired by the work of  and . The gradient for \(_{e^{m}}_{}^{m}(^{m})\) can be derived using the similar chain rule. The detailed update rules for each parameter are in Algo.1.

**Combination with other meta-learning algorithms.** Since the KL regularization in Eq.(4) is independent of the global (or initial) parameter \(\), MetaVD can be combined with several existing meta-learning based PFL algorithms (e.g., Reptile , MAML , PerFedAvg ). For example, MAML  requires some internal update steps using a subsampled dataset to compute the second-order gradient for \(\). Reptile  uses only a first-order gradient computation as described in Algo.1, which is similar to FedAvg except for the addition of the learning rate \(\). Each local adaptation step is performed using the \(_{}^{m}\) in Eq.(4) and the local data set \(D^{m}\). Unlike conventional meta-learning based PFL algorithms, which maintain only one global initialization parameter, MetaVD allows to change the mode of the initialization parameters for each client.

## 4 Experiments

To validate the MetaVD approach, we conducted extensive experiments in various scenarios following the FL benchmark research , including different degrees of non-IID and client participation rates.

We used multiple FL datasets , including CIFAR-10, CIFAR-100, FEMINIST, and CelebA. To evaluate the effectiveness of the hypernetwork, we also performed an ablation study comparing MetaVD to regular VD. Additionally, we assessed the uncertainty calibration and model compression ability of MetaVD. Finally, we test MetaVD with multi-domain datasets.

**Baselines.** We compare our method with standard FL methods such as FedAvg  and FedProx , meta-learning PFL algorithms like Reptile , MAML , and PerFedAvg , and Bayesian FL methods including FedBE  and pFedGP . To ensure consistency, we employ the widely-used CNN model for FL  across all baselines. Additionally, "fine-tuning" (FT) refers to performing few-shot adaptation steps with FedAvg before evaluation. An overview of baselines can be found in Appendix D.

**Implementation.** For reproducibility, we run experiments in a containerized environment that simulates FL communication with clients only on a server. We test \(T=1000\) of total FL rounds, following the conventions in . One baseline model can be run on a single GPU. All experiments are run on a cluster of 32 NVIDIA GTX 1080 GPUs. MetaVD's hypernetwork consists of an embedding layer of dimension \((1+M/4)\), followed by three fully connected NNs with a Reaky ReLU activation and an exponential activation for the dropout logit output. The predicted dropout variable is then applied to the global weight of other baselines. In our study, we apply MetaVD to only one fully connected layer before the output layer , which leads to significant performance improvements in all experiments. See Appendix E for implementation details. Our code is available at https://github.com/insujeon/MetaVD.

### Generalization on Non-IID Settings

**Datasets and training.** To evaluate the generalization capabilities of the models under non-IID data conditions, we perform tests on both CIFAR-10 and CIFAR-100 datasets with varying degrees of heterogeneity. We follow the similar evaluation protocols of the pF-Bench , using Dirichlet allocation to partition each dataset into 130 clients with different Dirichlet parameters, denoted as \(=[5,0.5,0.1]\). As shown in Figure 2, the class labels and data size per client are heterogeneous across clients. A smaller \(\) represents a higher degree of heterogeneity. To evaluate the test accuracy and generalization performance of the baselines on new clients, we randomly select 30 out of 130 clients as out-of-distribution (OOD) clients, which are not involved in the training phase. See Appendix G for more details.

    &  &  \\  & =5.0\)} & =0.5\)} & =0.1\)} \\   & Test (\%) & OOD (\%) & \(\) & Test (\%) & OOD (\%) & \(\) & Test (\%) & OOD (\%) & \(\) \\  FedAvg  & \(42.35\) & \(43.08\) & \(+0.73\) & \(41.92\) & \(41.96\) & \(+0.04\) & \(71.65\) & \(71.57\) & \(-0.08\) \\ FedAvg+FT  & \(41.49\) & \(42.45\) & \(+0.96\) & \(40.99\) & \(39.83\) & \(-1.16\) & \(69.62\) & \(68.38\) & \(-1.24\) \\ FedProx  & \(42.23\) & \(44.11\) & \(+1.88\) & \(42.03\) & \(40.51\) & \(-1.52\) & \(72.27\) & \(73.75\) & \(+1.48\) \\ FedBE  & \(45.17\) & \(45.43\) & \(+0.26\) & \(44.29\) & \(44.23\) & \(-0.06\) & \(70.23\) & \(69.19\) & \(-1.04\) \\ pFedGP  & \(42.69\) & \(43.07\) & \(+0.38\) & \(42.44\) & \(42.53\) & \(+0.09\) & \(71.94\) & \(76.83\) & \(+4.89\) \\ Reptile  & \(47.87\) & \(47.73\) & \(-0.14\) & \(46.13\) & \(45.94\) & \(-0.19\) & \(73.93\) & \(76.36\) & \(+2.43\) \\ MAML  & \(48.30\) & \(49.14\) & \(+0.84\) & \(46.33\) & \(46.65\) & \(+0.32\) & \(76.06\) & \(74.89\) & \(-1.17\) \\ PerFedAvg (HF-MAML)  & \(48.19\) & \(47.35\) & \(-0.84\) & \(46.22\) & \(46.36\) & \(+0.14\) & \(75.42\) & \(79.56\) & \(+4.14\) \\  FedAvg+MetaVD (ours) & \(47.82\) & \(50.26\) & \(+\) & \(47.54\) & \(47.55\) & \(+0.01\) & \(76.87\) & \(76.25\) & \(-0.62\) \\ Reptile+MetaVD (ours) & \(\) & \(\) & \(+0.79\) & \(\) & \(\) & \(-0.56\) & \(76.51\) & \(\) & \(+5.56\) \\ MAML+MetaVD (ours) & \(52.40\) & \(51.78\) & \(-0.62\) & \(50.21\) & \(49.75\) & \(-0.46\) & \(\) & \(79.05\) & \(+1.78\) \\ PerFedAvg+MetaVD (ours) & \(51.67\) & \(51.70\) & \(+0.03\) & \(50.02\) & \(48.70\) & \(-1.32\) & \(76.06\) & \(81.77\) & \(+\) \\   

Table 1: Classification accuracies with different (non-IID) heterogeneity degrees of \(=[5.0,0.5]\) in CIFAR-100 and \(=0.1\) in CIFAR-10. The higher score, the better.

Figure 2: Visualization of client’s data distribution in different non-IID degrees (\(=[0.5,0.1]\)).

**Results.** Table 1 shows the weighted average classification accuracy for participating (Test) and non-participating (OOD) clients on CIFAR-10 and CIFAR-100 datasets with varying non-IID degrees. The generalization gap, denoted by \(\), represents the difference between OOD and Test accuracy. As shown in Table 1, FFL methods such as Reptile, MAML, and PerFedAvg generally outperform non-PFL methods such as FedAvg and FedProx. While the Bayesian ensemble approach, FedBE, improves on FedAvg, it still lags behind PFL methods in OOD accuracy. Additionally, pFedGP exhibits suboptimal performance in the presence of heterogeneous distribution of data among clients. As \(\) changes from \(5.0\) to \(0.5\), the test and OOD accuracy of all models decreases as the degree of non-IID increases. When combined with MetaVD, all baselines show significant performance improvements, regardless of whether they are FL or PFL algorithms (e.g., Reptile enhances from \(47.87\) to \(53.71\) adapting MetaVD).. These results demonstrate the adaptability and effectiveness of MetaVD in mitigating model overfitting and handling non-IID client data in FL contexts.

### Ablation Study

**Settings.** To evaluate the capability of the hypernetwork in MetaVD, we perform an ablation study by comparing MetaVD with naive VD  and EnsembleVD  approaches on the CIFAR-100 dataset. The naive (global) VD model maintains a global dropout parameter shared by all clients; the dropout parameter is treated as a global model parameter, as in FedAvg. EnsembleVD maintains \(M\) independent dropout parameters for all clients. The client-specific dropout parameter can be stored in each client analogous to the partial FL [80; 81; 5]. In contrast, MetaVD utilizes a hypernetwork to learn the personal dropout rate across all clients. Bayesian posterior aggregation rules is applied in all model based on dropout rates to update the global model parameter [82; 83].

**Results.** Table 2 outlines the results of the ablation study; MetaVD's hypernetwork-based posterior modeling outperforms all other baselines. The dropout rates in baselines such as VD or EnsembleVD could not fully learn the independent dropout variables well due to restricted client participation. On the other hand, both the hypernetwork and the global parameter converge well in MetaVD. This observation demonstrates that MetaVD's hypernetwork provides a more data-efficient approach to learning client-specific model uncertainty compared to other baselines. Further ablation studies performed on the FEMNIST dataset are shown in Appendix H.

### Uncertainty Calibration

**Settings.** Identifying any potential bias in the model's prediction is important to avoid serious consequences, especially when the model is used to make important decisions [84; 85; 86]. In the FLenvironment, where customers have limited non-IID data, it is even more critical to properly calibrate the prediction model. Therefore, we investigate whether the proposed MetaVD approach can also improve the calibration measures for FL baselines. The Expected Calibration Error (ECE) measures the expected deviation between a model's predicted probability and the actual positive class frequency, while the Maximum Calibration Error (MCE) measures the maximum difference. These calibration metrics are commonly used to evaluate the reliability of probabilistic predictions.

**Results.** Table 3 summarizes the ECE and MCE values tested with the OOD clients in the CIFAR-100 dataset and shows that the meta-learning based PFL algorithms (e.g., Reptile, MAML, and PerFedAvg) tend to have higher ECE and MCE values than the conventional FL algorithms (e.g., FedAvg, FedProx, and FedBE). This means that PFL baselines achieve high classification accuracy, but their probability predictions are more likely to be biased. This may be a byproduct of the additional optimization-based adaptation steps of meta-learning with limited local client data. On the other hand, our MetaVD approach significantly reduces both ECE and MCE values for all meta-learning-based methods, indicating that MetaVD effectively mitigates overfitting and reduces bias on the OOD clients. Figure 3 shows the reliability diagrams that visualize the model calibration [84; 85; 86] in CIFAR-100 (\(=0.5\)). They plot the expected sample accuracy as a function of confidence. If the model is perfectly calibrated, then the diagram becomes the identity function. Any deviation from a perfect diagonal represents miscalibration. While Reptile and MAML tend to be overconfident in their predictions, Reptile+MetaVD comes fairly close to the desired diagonal function. Remarkably, in most instances, adapting MetaVD leads to improved model calibration.

### Client Participation

**Settings.** In real-world FL scenarios, such as intermittent connections between clients and servers or limited client device performance, numerous clients may be unable to participate in each FL round. This is important for cross-device FL with a large number of clients or resource-limited clients. In this experiment, we evaluated the performance of the methods under different levels of client participation in each FL round. We experimented with 200 clients in the FEMNIST dataset. For each FL round, we randomly selected 40, 20, and 10 clients to participate during training, with participating client rates \(s\) of 0.2, 0.1, and 0.05, respectively. To measure OOD accuracy, we excluded 40 preselected clients from the selection of 200 clients so that they do not participate in the entire training.

**Results.** The overall classification results with different participant client rates \(s\) are summarized in table 4. Reptile performs better than FedAvg. The effect of data heterogeneity on performance degradation becomes more severe as more clients participate in training. The decrease in test accuracy as the participating client rate \(s\) becomes smaller is due to having less training data as fewer clients participate in each round. Meanwhile, Reptile+MetaVD outperforms the other baselines. Interestingly, the performance drop for Reptile+MetaVD is not as significant as for Reptile, showing that MetaVD can adapt well to FL scenarios with smaller participant sizes.

### Muti-domain Datasets

**Settings.** Existing federated learning algorithms usually assume a single-domain approach, where only one dataset is used in the experiment. Multi-domain learning [87; 88; 89] aims to utilize all available training data across different domains to improve the performance of the model. In this section, we

    &  \\   &  &  &  \\   & Test (\%) & OOD (\%) & \(\) & Test (\%) & OOD (\%) & \(\) & Test (\%) & OOD (\%) & \(\) \\  FedAvg  & \(88.08\) & \(85.29\) & \(-2.79\) & \(88.13\) & \(84.70\) & \(-3.43\) & \(88.06\) & \(86.22\) & \(-1.84\) \\ FedAvg+FT  & \(88.33\) & \(86.37\) & \(-1.96\) & \(87.85\) & \(86.95\) & \(\) & \(87.66\) & \(87.11\) & \(-0.55\) \\ Reptile  & \(88.55\) & \(86.52\) & \(-2.03\) & \(88.39\) & \(87.20\) & \(-1.19\) & \(87.86\) & \(88.22\) & \(+\) \\  FedAvg+MetaVD (ours) & \(88.81\) & \(85.28\) & \(-3.53\) & \(88.69\) & \(85.71\) & \(-2.98\) & \(88.66\) & \(86.21\) & \(-2.45\) \\ Reptile+MetaVD (ours) & \(\) & \(\) & \(\) & \(\) & \(\) & \(-1.23\) & \(\) & \(\) & \(-0.72\) \\   

Table 4: Results of classification accuracies with different participant client rates of \(s=0.2\), \(s=0.1\), and \(s=0.05\) in the FEMNIST dataset. We report the results of participating clients (Test) and non-participating clients (OOD). The higher the better.

further evaluate the performance of PFL algorithms on a multi-domain FL dataset, where we assume that each client can have data from different domains. We use three different FL datasets to construct the multi-domain task distributions: FEMNIST, CIFAR-100, and CelebA. To sample each client's local data, we use the Dirichlet sampling technique (\(=0.5\)) used in the SS4.1.

**Results.** Table 5 shows the classification accuracies for the Test and OOD clients when using a combination of two or three datasets. The meta-learning based PFL algorithm consistently outperforms FedAvg in terms of classification accuracy. In addition, when MetaVD is applied to either the FL or PFL algorithms, it significantly improves prediction accuracy in all multi-domain settings. Notably, MetaVD shows greater improvements in OOD accuracy compared to the improvement in test accuracy. Overall, these results demonstrate the versatility of MetaVD in improving robustness and generalization even in multi-domain FL datasets.

### Model Compression

**Settings.** Federated learning optimization requires frequent communication of model parameters between devices and the central server, which can be slow and may raise privacy concerns. Therefore, minimizing communication costs by reducing the size of model parameters is an important issue in FL. To explore the compression capabilities of MetaVD, we performed an additional experiment on the CIFAR-10 dataset. The sign DP indicates that each model parameter is dropped during the FL communication. We used the thresholding technique to drop the model parameter; Any parameter with a dropout rate greater than \(0.8\) was dropped during the FL rounds.

**Results.** Table 6 shows the test and OOD accuracy results, as well as the sparsity (%) in the CIFAR-100 dataset. The sparsity represents the proportion of zero-value model parameters in the personalized layer. A higher sparsity percentage indicates more parameter pruning or elimination performed on the weight. In our experiment, MetaVD was able to prune about \(80\%\) of the weights in the personalized layer. In addition, when we dropped the communication of the parameters between the client and the server using the dropout thresholding technique, MetaVD still showed relatively good performance. In the case of Reptile+MetaVD+DP, the performance decreased by about \(2\%\) while using only \(20\%\) of the weights. On the other hand, MAML+MetaVD+DP and PerFedAvg+MetaVD+DP show an improvement in performance of about \(1\%\). This shows that MetaVD can compress the model parameters required in the personalized layer, reducing the communication cost in FL without sacrificing much performance. Appendix K shows more experiments on model compression results on the CIFAR-100 dataset with different non-IID settings.

   =0.5\))} \\
**Method** & Test (\%) OOD (\%) Sparsity(\%) \\  Reptile+MetaVD & \(\) & \(\) & \(0\) \\ MAML+MetaVD & \(81.32\) & \(81.81\) & \(0\) \\ PerFedAvg+MetaVD & \(81.06\) & \(81.47\) & \(0\) \\  Reptile+MetaVD+DP & \(81.40\) & \(80.98\) & \(\) \\ MAML+MetaVD+DP & \(81.48\) & \(81.73\) & \(79.49\) \\ PerFedAvg+MetaVD+DP & \(82.43\) & \(82.19\) & \(78.20\) \\   

Table 6: Results of model compression. MetaVD+DP does not communicate the model parameters whose dropout rates are larger than \(0.8\).

    &  &  &  &  \\   &  \\  FedAvg  & \(43.65\) & \(43.45\) & \(64.02\) & \(52.88\) & \(86.81\) & \(81.01\) & \(63.73\) & \(55.55\) \\ Reptile  & \(48.92\) & \(48.93\) & \(66.13\) & \(56.22\) & \(87.50\) & \(83.05\) & \(66.98\) & \(57.12\) \\ MAML  & \(47.39\) & \(48.51\) & \(66.56\) & \(55.72\) & \(88.57\) & \(84.52\) & \(67.08\) & \(58.15\) \\ PerFedAvg (HF-MAML)  & \(49.21\) & \(50.91\) & \(66.57\) & \(55.24\) & \(87.94\) & \(83.75\) & \(67.20\) & \(57.03\) \\  FedAvg+MetaVD (ours) & \(48.23\) & \(48.62\) & \(65.58\) & \(56.85\) & \(87.38\) & \(82.67\) & \(65.93\) & \(58.58\) \\ Reptile+MetaVD (ours) & \(\) & \(\) & \(\) & \(59.07\) & \(\) & \(85.03\) & \(68.78\) & \(61.59\) \\ MAML+MetaVD (ours) & \(51.34\) & \(52.82\) & \(68.24\) & \(61.21\) & \(88.59\) & \(85.02\) & \(\) & \(\) \\ PerFedAvg+MetaVD (ours) & \(51.18\) & \(53.06\) & \(67.93\) & \(\) & \(88.27\) & \(\) & \(68.05\) & \(61.17\) \\   

Table 5: Classification accuracies with multi-domain datasets. (a) CelebA + CIFAR-100, (b) CIFAR-100 + FEMNIST, (c) CelebA + FEMNIST, (d) CelebA + CIFAR-100 + FEMNIST.

Conclusion

In this study, we presented a new novel Bayesian personalized federated learning (PFL) approach called meta-variational dropout (MetaVD). MetaVD utilizes a hypernetwork that predicts the dropout rates for each independent NN parameter, which enables effective model personalization and adaptation in federated learning (FL) with non-IID and limited data scenarios. In addition, MetaVD is the first approach to exploit variational dropout uncertainty in posterior aggregation in PFL. MetaVD's dropout posterior modeling provides a principled Bayesian aggregation strategy to consolidate local models into a global model, thereby improving training convergence. MetaVD is also a generic approach that is compatible with any other existing meta-learning-based PFL algorithms to avoid model overfitting. In addition, MetaVD's ability to compress model parameters can be used to reduce communication costs. A potential limitation of our approach is that it may increase the complexity of the model by introducing an additional hypernetwork. However, the hypernetwork is kept on the server, and we have verified that applying MetaVD to just one last layer before the output layer yields significant performance improvements in all experiments. Experimentally, MetaVD's performance has been validated on several PFL benchmarks, including CIFAR-10, CIFAR-100, FEMINIST, and CelebA, as well as multi-domain datasets. It demonstrates superior classification accuracy and uncertainty calibration, especially for out-of-distribution (OOD) clients. Overall, the experimental results show MetaVD to be a highly versatile approach capable of addressing many challenges in FL.