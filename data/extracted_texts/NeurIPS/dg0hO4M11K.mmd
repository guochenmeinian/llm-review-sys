# Exploring Consistency in Graph Representations:

from Graph Kernels to Graph Neural Networks

 Xuyuan Liu1  Yinghao Cai1  Qihui Yang2  Yujun Yan1

1Dartmouth College

2University of California San Diego

{xuyuan.liu.gr, yinghao.cai, yujun.yan}@dartmouth.edu

qiy009@ucsd.edu

###### Abstract

Graph Neural Networks (GNNs) have emerged as a dominant approach in graph representation learning, yet they often struggle to capture consistent similarity relationships among graphs. While graph kernel methods such as the Weisfeiler-Lehman subtree (WL-subtree) and Weisfeiler-Lehman optimal assignment (WLOA) kernels are effective in capturing similarity relationships, they rely heavily on pre-defined kernels and lack sufficient non-linearity for more complex data patterns. Our work aims to bridge the gap between neural network methods and kernel approaches by enabling GNNs to consistently capture relational structures in their learned representations. Given the analogy between the message-passing process of GNNs and WL algorithms, we thoroughly compare and analyze the properties of WL-subtree and WLOA kernels. We find that the similarities captured by WLOA at different iterations are asymptotically consistent, ensuring that similar graphs remain similar in subsequent iterations, thereby leading to superior performance over the WL-subtree kernel. Inspired by these findings, we conjecture that maintaining consistency in the similarities of graph representations across GNN layers is crucial for capturing relational structures and improving graph classification performance. Thus, we propose a loss to enforce the similarity of graph representations to be consistent across different layers. Our empirical analysis verifies our conjecture and shows that our proposed consistency loss can significantly enhance graph classification performance across several GNN backbones on various datasets.

+
Footnote â€ : We release our code at https://github.com/GraphmindDartmouth/Graph-consistency

## 1 Introduction

Graph classification tasks are extensively applied across multiple domains, including chemistry (Liu et al., 2022; Xu et al., 2023), bioinformatics (Yan et al., 2019; Li et al., 2023, 2023), and social network analysis (Ying et al., 2018; Wang et al., 2024). Graph neural networks (GNNs) (Kipf and Welling, 2017; Xu et al., 2019; Velickovic et al., 2018; Huang et al., 2024) have emerged as the predominant approach for performing graph classification, owing to their ability to extract rich representations from various types of graph data. A typical GNN employs the message-passing mechanism (Gilmer et al., 2017), where node features are propagated and aggregated across connected nodes. This process effectively captures local tree structures, enabling the differentiation between various graphs. However, GNNs often struggle to preserve relational structures among graphs, resulting in inconsistent relative similarities across the layers. As shown in Figure 1, graphs with higher relative similarity in one layer may exhibit reduced similarity in the subsequent layer. This phenomenon arises from the limitations of cross-entropy loss, which fails to preserve relational structures, as it forces graphs within the same class into identical representations.

raph kernel methods, on the other hand, are designed to capture similarities between graphs and utilize these similarities for classification tasks. For instance, subgraph-pattern approaches (Shervashidze et al., 2009; Kriege et al., 2020) compare graphs by counting the occurrences of fixed-size subgraph motifs. Other methods compare sequences of vertices or edges encountered during graph traversals (Borgwardt and Kriegel, 2005; Kashima et al., 2003; Zhang et al., 2018). Among all graph kernels, two notable ones are the Weisfeiler-Lehman subtree (WL-subtree) kernel (Shervashidze et al., 2011) and the Weisfeiler-Lehman optimal assignment (WLOA) kernel (Kriege et al., 2016). They are found to have comparable performance to simple GNNs (Nikolentzos et al., 2021). The WL-subtree kernel iteratively relabels graphs using the Weisfeiler-Lehman algorithm (Weisfeiler and Lehman, 1968) and constructs a kernel based on the number of occurrences of each label. The WLOA kernel uses the same relabeling scheme but computes a matching between substructures to reveal structural correspondences between the graphs.

While effective in capturing relative graph similarity, kernel methods rely on predefined kernels and exhibit insufficient non-linearities, limiting their ability to capture complex patterns in high-dimensional data. Additionally, kernel methods are computationally costly, making them unsuitable for handling large datasets and consequently limiting their overall applicability.

In this work, we aim to bridge the gap between kernel methods and GNN models. Given the iterative nature of GNNs, we study a class of kernels which are induced from graph representations obtained through an iterative process and name them iterative graph kernels (IGK). Within this framework, we define the **consistency** property, which ensures that similar graphs remain similar in subsequent iterations. Our analysis demonstrates that kernels with this property ensure better generalization ability, leading to improved classification performance.Furthermore, we find that this property sheds light on why the WLOA kernel outperforms the WL-subtree kernel. The WLOA kernel asymptotically demonstrates consistency as the iteration goes to infinity, whereas the WL-subtree kernel does not exhibit this behavior. Inspired by these findings and the analogy between message-passing GNNs and the WL-subtree kernel (Xu et al., 2019), we hypothesize that this principle is also applicable to GNNs. To explore this, we introduce a novel loss function designed to align the ranking of graph similarities across GNN layers. The aim is to ensure that the relational structures of the graphs are preserved and consistently reflected throughout the representation space of these layers. We validate this hypothesis by applying our loss function to different GNN backbones across various graph datasets. Extensive experiments demonstrate that our proposed model-agnostic consistency loss improves graph classification performance comprehensively.

In summary, the main contributions of this work are as follows:

* **Novel perspective:** We present a novel perspective on understanding the graph classification performance of GNNs by analyzing the similarity relationships captured by different layers.
* **New insights:** We are the first to introduce and formalize the consistency principle within both kernel-based and GNN methods for graph classification tasks. Additionally, we provide theoretical proofs explaining how this principle enhances the performance.
* **Simple yet effective method:** Empirical results demonstrate that the proposed consistency loss universally enhances performance across a wide range of base models and datasets.

## 2 Preliminaries

In this section, we begin by introducing the notations and definitions used throughout the paper. Next, we provide an introduction to the fundamentals of Weisfeiler-Lehman isomorphism test, GNNs and graph kernels.

Figure 1: Cosine similarity of three molecules from the NCI1 dataset, evaluated using graph representations from three consecutive GIN layers. Common GNN models fail to preserve relational structures across the layers.

### Notations and Definitions

Let \((,\,,\,)\) be an undirected and unweighted graph with \(N\) nodes, where \(\) denotes the node set, \(\) denotes the edge set, and \(\) denotes the feature matrix, where each row represents the features of a corresponding node. The neighborhood of a node \(v\) is defined as the set of all nodes that connected to \(v\): \((v)=\{u|(v,u)\}\). In a graph dataset, each graph \(_{i}\) is associated with a label \(_{i}\), which is sampled from a label set \(\). In this paper, we focus on the graph classification task, where a model \(\) is trained to map each graph to its label.

### Weisfeiler-Lehman Isomorphism Test

We first introduce the Weisfeiler-Lehman isomorphism test (Weisfeiler and Lehman, 1968), which can be used to distinguish different graphs and is closely related to the message-passing process of GNNs (Xu et al., 2019). The WL algorithm operates by iteratively relabeling the colors of vertices in the graph. Initially, all vertices are assigned the same color \(C_{0}\). In iteration \(i\), the color of a vertex \(v\) is updated based on its current color \(C_{v,i-1}\) and the colors of its neighbors \(C_{u(v),i-1}}\). The update is given as follows:

\[C_{v,i}=f_{c}^{i}(C_{v,i-1},C_{u(v),i-1} }})\]

where \(f_{c}^{i}\) is an injective coloring function that maps the multisets to different colors at iteration \(i\).This process continues for a predefined number of iterations or until the coloring stabilizes (i.e., the colors no longer change).

### Graph Neural Network

Most GNNs adopt the message-passing framework (Gilmer et al., 2017), which can be viewed as a derivative of the Weisfeiler-Lehman coloring mechanism. Specifically, let \(_{v}^{(k-1)}\) represent the feature vector of node \(v\) at the \((k-1)\)-th iteration. A GNN computes the new feature for \(v\) by aggregating the representations of itself and its neighboring nodes \(u(v)\) as follows:

\[_{v}^{(k)}=^{(k)}(_{v}^{(k-1)},_{v}^{( k)})_{v}^{(k)}=^{(k)}(_{u}^{(k-1)}:u (v)})\]

The initial node representations \(_{v}^{(0)}\) are set to the raw node features \(_{v}\). At the \(k\)-th iteration, the aggregation function \(^{(k)}()\) computes the messages \(_{v}^{(k)}\) received from neighboring nodes. Subsequently, the update function \(^{(k)}()\) computes a new representation for each node by integrating the neighborhood messages \(_{v}^{(k)}\) with its previous embedding \(_{v}^{(k-1)}\). After \(T\) iterations, the final node representations are combined into a graph representation using a readout function:

\[_{G}=(_{v}^{(T)} v }).\]

The readout function, essentially a set function learned by the neural network, commonly employs AVERAGE or MAXPOOL.

### Graph Kernel

A kernel is a function used to measure the similarity between pairs of objects. For a non-empty set \(\) and a function \(:\), the function \(\) qualifies as a kernel on \(\) if there exists a Hilbert space \(_{k}\) and a feature map function \(:_{k}\), such that \((x,y)=(x),(y)\) for any \(x,y\), where \(,\) denotes the inner product in \(_{k}\). Notably, such a feature map exists if and only if \(\) is a positive semi-definite function. Let \(\) be a kernel defined on \(\), and let \(S=\{x_{1},,x_{n}\}\) be a finite set of \(n\) samples on \(\). The Gram matrix for \(S\) is defined as \(^{n n}\), with each element \(_{ij}=(x_{i},x_{j})\) representing the kernel value between the \(i\)-th and \(j\)-th data points in \(S\). The Gram matrix is always positive semi-definite.

Graph kernel methods apply the kernel approaches to graph data, typically defined using the \(\)-convolution framework (Haussler et al., 1999; Bause and Kriege, 2022). Consider two graphs, \(\) and \(^{}\). The key idea is to decompose the graphs into substructures using a predefined feature map function \(\), and then compute the kernel value by taking the inner product in the feature space: \((,^{})=(),( ^{})\), based on these substructures. Weisfeiler-Lehman (WL) graph kernels stand out as one of the most widely used approaches. These methods employ the Weisfeiler-Lehmancoloring scheme to iteratively encode graphs, calculating kernel values based on these colorings. The final kernel values are derived through the aggregation of intermediate results. Next, we introduce the WL-subtree kernel and the WLOA kernel in more detail. Let \(f^{i}\) be the coloring function at the \(i\)-th iteration, mapping the colored graph from the previous iteration to a new colored graph. Define \(^{i}\) as the function that captures the cumulative coloring effect up to the \(i\)-th iteration: \(^{i}=f^{i} f^{1}\). Specifically, the WL-subtree kernel computes the kernel value by directly using the label histogram as a feature to compute the dot product, which is expressed as:

\[^{(h)}_{wl\_subtree}(,^{})= _{i=1}^{h}(^{i}()),(^{i}( ^{}))\]

The WLOA kernel applies a histogram intersection kernel to match substructures between different graphs, which is formulated as:

\[^{(h)}_{WLOA}(,^{})=_{i= 1}^{h}\{(^{i}()),(^{i}( ^{}))\}(i)\]

where \((i)\) is a nonnegative, monotonically non-decreasing weight function, and \((i)=1\) is commonly used in practice (Kriege et al., 2016; Siglidis et al., 2020). The operator \(\) denotes the histogram intersection kernel. It is computed by comparing and summing the smallest matching elements between two sets. For example, consider two sets \(S_{1}:\{,,,,\}\) and \(S_{2}:\{,,,,\}\). The \((S_{1},S_{2})\) is calculated by taking the minimum frequency of each distinct element across the two sets, yielding \((2,1)+(2,2)+(1,2)=4\). To ensure that the kernel values fall in the range of 0 to 1, normalization is often applied. The normalized kernel value \(}^{(h)}\) is then expressed as follows:

\[}^{(h)}(,^{})=^{( h)}(,^{})}{^{(h)}(, )}^{(h)}(^{},^{})}}\]

## 3 Consistency Principles

To encode relational structures in GNN learning, we first examine how similarities are represented in graph kernels. In this section, we start by defining a class of graph kernels, i.e., the iterative graph kernels, which encompasses many widely used kernels. Then, we delve into a key property known as the consistency property, which may play an important role in enhancing classification performance. We support this assertion through theoretical analysis, elucidating how different kernels adhere to or deviate from this property, thereby explaining their performance differences.

### Iterative Graph Kernels

In this paper, we are interested in a set of kernels defined as follows:

**Definition 3.1**: _Given a colored graph set \(\), a feature map function \(:_{k}\) (where \(_{k}\) is a Hilbert space), and a set of coloring functions \(_{c}=\{f^{0},f^{1},,f^{i}\}\) on \(\) (with \(f^{i}:\)), we define the set of **iterative graph kernels (IGK)** as:_

\[_{_{c},}(x,y,i)=(f^{i} f^{1}(x )),(f^{i} f^{1}(y))=^{i}(x),^{i}(y)\]

_where \(x,y\) and \(^{i}()\) represents a composite function given by: \(^{i}=f^{i} f^{1}\), Then the normalized kernel is given by:_

\[}_{_{c},}(x,y,i)=_{_{c},}(x,y,i)}{_{_{c},}(x,x,i)}_{_{c},}(y,y,i)}}\]

Based on this definition, we can see that graph kernels utilizing the Weisfeiler-Lehman framework, including the WL-subtree kernel (Shervashidze et al., 2011), WLOA (Kriege et al., 2016), and the Wasserstein Weisfeiler-Lehman kernel (Togninalli et al., 2019), should be classified as iterative graph kernels. Conversely, the subgraph-pattern approaches, such as the graphlet kernel (Shervashidze et al., 2009) and the shortest-path kernel (Borgwardt and Kriegel, 2005), do not fall into this category.

### Key Properties: Monotonic Decrease & Order Consistency

To effectively capture relational structures, we design the IGKs to progressively differentiate between graphs. With each iteration, additional structural features are considered, enabling the distinction of graphs that may have been indistinguishable in earlier iterations. This implies two properties: (1) the kernel values monotonically decrease with larger iterations, as the similarity between two graphs decreases with the consideration of more features; and (2) the similarity rankings across different iterations should remain consistent, meaning that graphs deemed dissimilar in early iterations should not be considered similar in later iterations. We then provide formal definitions for these two properties and prove how they can improve generalization in the binary classification task.

**Definition 3.2** (Monotonic Decrease): _The normalized iterative graph kernels \(}_{_{c},}(x,y,i)\) are said to be monotonically decreasing if and only if:_

\[}_{_{c},}(x,y,i)}_{ _{c},}(x,y,i+1) x,y\]

**Definition 3.3** (Order Consistency): _The normalized iterative graph kernels \(}_{_{c},}(x,y,i)\) are said to preserve order consistency if the similarity ranking remains consistent across different iterations for any pair of graphs, which is defined as:_

\[}_{_{c},}(x,y,i)>}_{_{c},}(x,z,i)}_{_{c},}(x,y,i+1) }_{_{c},}(x,z,i+1) x,y,z\]

Next, we show that these two properties can lead to a monotonically decreasing upper bound on the test error in the binary classification task, which suggests improved performance.

Consider a binary graph classification task and assume that the graph representations obtained at any iteration have a uniform norm. This can be achieved by simply normalizing the graph representations at the end of each iteration. That is, for any graph \(x\): \(\|(^{i}(x))\|=1\). Then, for any IGK that is monotonically decreasing and preserves the order consistency, the following theorem holds:

**Theorem 3.4**: _Let \(}_{_{c},}(x,y,i)\) be a normalized iterative graph kernel. Its generalization ability on the test set is provably strong when it decreases monotonically and preserves order consistency._

**Proof sketch.** We consider a binary classification task where supervision is provided through labels and similarity orders. We demonstrate that the presence of these two properties enhances the generalizability of the learned iterative kernel. The proof is structured in two steps:

1. We first show that iterative kernels can learn the correct ranking given sufficient data during training, using the PAC-learning framework.
2. Next, we demonstrate that, once the correct ranking is learned, an iterative kernel learned with the properties of monotonic decrease and order consistency guarantees that the error rate of the graphs in the test set decreases as the number of iterations increases.

Details of the proof can be found in Appendix A.

### Theoretical Verification with WL-based Kernels

As discussed in Section 3.1, WL-based kernels can be categorized as iterative graph kernels, as they are generated by the coloring functions in an iterative refinement process. Consequently, a natural question arises regarding how various kernels adhere to these properties and whether their adherence reflects their actual performance. We thus investigate two popular WL-based kernels: the WL-subtree kernel [Shervashidze et al., 2011] and the WLOA kernel [Kriege et al., 2016].

**Theorem 3.5**: _The normalized WL-subtree kernel is neither monotonically decreasing nor does it preserve order consistency._

**Theorem 3.6**: _The normalized WLOA kernel is monotonically decreasing and asymptotically preserves order consistency when \((i)=1\)._

**Proof sketch.** For Theorem 3.5, we illustrate a counterexample, while for Theorem 3.6, we consider two graph pairs where the similarity condition holds:

\[}_{WLOA}^{(h)}(,^{}) }_{WLOA}^{(h)}(,^{})\]

The similarity at the next iteration \(h+1\) is scaled by a factor dependent \((i),i=1,,h+1\). The unnormalized kernel increases monotonically, though with diminishing increments over iterations. Given this, when \((i)=1\) and \(h\), we obtain:

\[}_{WLOA}^{(h+1)}(,^{}) }_{WLOA}^{(h+1)}(,^{ })\]

We include the complete proof in Appendix B.1 and B.2.

These findings imply that the WLOA kernel better preserves relational structures compared to the WL-subtree kernel, leading to improved classification performance, as supported by the literature (Kriege et al., 2016).

## 4 Proposed Strategy

Given the analogy between WL-based kernels and GNNs (Shervashidze et al., 2011; Gilmer et al., 2017), and the observation that GNNs often fail to preserve relational structures, we hypothesize that the consistency principle is also beneficial to GNN models. Thus, we aim to explore how to effectively preserve this principle within the GNN architectures.

### Consistency Loss

Our objective is to enhance graph representation consistency across GNN layers, which has significant potential to preserve the relational structure in the representation space. If we compare \((^{i}(G))\) to the graph representations obtained at the \(i\)-th layer, preserving the consistency principle is equivalent to preserving the ordering of cosine similarity among the graph representations. However, due to the non-differentiable nature of ranking operations, directly minimizing the ranking differences between consecutive layers is not feasible using gradient-based optimization techniques. Therefore, we aim to optimize pairwise ordering relations instead of the entire ranking list. In this work, our proposed loss employs a probabilistic approach inspired by (Burges et al., 2005). The entire framework is illustrated in Figure 2.

Let \(^{h}^{n d}\) denote the graph embedding matrix for \(n\) examples in a batch, each with a \(d\)-dimensional feature vector. We first compute the distance matrix \(^{h}\) for all the graphs in a batch at

Figure 2: Computation of Consistency loss. At each layer, pairwise distance matrix \(\) is calculated using the normalized representations of graphs in a batch. After randomly selecting a reference graph \(x_{k}\), the reference probability matrix is computed using the distance matrix from previous layer, where entry \((n,m)\) represents the known probability that the graph \(x_{k}\) is more similar to the graph \(x_{n}\) than to the graph \(x_{m}\). For the distance matrix of current layer, we compute the predicted probability that \(x_{k}\) is closer to \(x_{n}\) than to \(x_{m}\) and form the prediction probability matrix. Consistency loss is computed as the cross-entropy between the predicted and reference probability matricesthe \(h\)-th layer. The entries \(D_{i,j}^{h}\) of this matrix represent the distance between the representations of the \(i\)-th and \(j\)-th graphs, calculated as \(D_{i,j}^{h}=(H_{x_{i}}^{h},H_{x_{j}}^{h})\). Here, we use the cosine distance, the complement of cosine similarity in positive space, expressed as: \(1-}^{h}.H_{x_{j}}^{h}}{\|h_{x_{i}}^{h}\|}\). Considering the distance relationship to an arbitrary graph \(x_{k}\) in the batch, the predicted probability \(}_{n,m|k}^{h}\) that \(x_{k}\) is more similar to graph \(x_{n}\) than to graph \(x_{m}\) at layer \(h\) is defined as \(}_{n,m|k}^{h}(_{k,n}^{h}<_{k,m}^{h})\). This probability score, which ranges from 0 to 1, is formulated using the sigmoid function as follows:

\[}_{n,m|k}^{h}(_{k,n}^{h}<_{k,m}^{h})= _{k,m}^{h}-_{k,n}^{h})}\]

Given the distance matrix from the previous layer \(D^{h-1}\), the known probability that graph \(x_{k}\) is more similar to graph \(x_{n}\) than to graph \(x_{m}\), denoted as \(}_{n,m}^{h-1}(_{k,n}^{h-1},_{k,m}^{h-1})\), can be formulated as follows:

\[}_{n,m|k}^{h-1}=(1+(_{k,n}^ {h-1}-_{k,m}^{h-1}))\]

We can then minimize the discrepancy between the predicted and the known probability distributions to enhance representation consistency across the layers. Here,we employ the cross-entropy loss to effectively measure the divergence between these two distributions. Specifically, for a pair \((x_{n},x_{m})\) centered on \(x_{k}\) at layer \(h\), the cross-entropy loss can be expressed as:

\[_{}((x_{n},x_{m}) x_{k},h )=-}_{n,m|k}^{h-1}}_{n,m|k}^{h}-( 1-}_{n,m|k}^{h-1})(1-}_{n,m|k}^{h })\]

Then, the total loss function, which quantifies the consistency of pair-wise distance relations for graph \(x_{k}\) at layer \(h\), can be formulated as

\[_{}(k)=_{n,m}((x_{n}, x_{m}) x_{k},h)\]

The overall objective function of the proposed framework can then be formulated as the weighted sum of the original loss and the consistency loss.

\[_{}=_{}+_{i} _{}(i)\]

Here, \(\) is a hyperparameter that controls the strength of the consistency constraint.

## 5 Experiment

In this section, we examine the effectiveness of the proposed consistency loss for the graph classification task. Specifically, we aim to address the following questions: **Q1:** Does the consistency loss effectively enhance the performance of various GNN backbones in the graph classification task? **Q2:** How does the consistency loss influence the rank correlation of graph similarities across GNN layers? **Q3:** How does the consistency loss influence dataset performance across varying levels of complexity, both in structural intricacy and task difficulty?

### Experiment Setup

DatasetWe conduct extensive experiments using the TU Dataset (Morris et al., 2020), the Open Graph Benchmark (OGB) (Hu et al., 2020) and Reddit Threads(Reddit-T) dataset (Bause and Kriege, 2022). The TU Dataset consists of eight graph classification datasets, categorized into three main types: (1) Chem/Bioinformatics datasets, including D&D, NCI1, NCI109, and PROTEINS; (2) Social Network datasets, including IMDB-BINARY,IMDB-MULTI and COLLAB, where a constant feature value of 1 was assigned to all vertices due to the absence of vertex features; and (3) a Computer Vision dataset, COIL-RAG. The OGB datasets include ogbg-molhiv, a molecular property prediction dataset used to determine whether a molecule inhibits HIV replication. The Reddit-T dataset is used for classifying threads from Reddit as either discussions or non-discussions, where users are represented as nodes and replies between them as links.

For the TU dataset and Reddit-T, consistent with prior work (Xinyi and Chen, 2019; Xu et al., 2019), we utilize an 8:1:1 ratio for training, validation, and testing sets. For the OGB datasets, we use the official splits provided. Training stops at the highest validation performance, and test accuracy is taken from the corresponding epoch in each fold. Final results are reported as the mean accuracy (except ogbg-molhiv) and standard deviation over \(10\) folds. For ogbg-molhiv, we follow the official evaluator and use ROC-AUC as the evaluation metric. Detailed information about these datasets can be found in Appendix C.

ModelWe use three widely adopted GNN models as baselines: namely, GCN (Kipf and Welling, 2017), GIN (Xu et al., 2019), and GraphSAGE (Hamilton et al., 2017). We also include two recent GNN models, namely GTransformer (Shi et al., 2021) and GMT (Baek et al., 2021). To ensure a fair comparison, we maintain the same number of layers and layer sizes for both the base models and the models with our proposed consistency loss, ensuring the sharing of the same network architecture. Detailed information about hyperparameter tuning is provided in Appendix D.

### Effectiveness of Consistency Loss

To answer **Q1**, we present the results for the TU, OGB, and Reddit-T datasets in Table 1. As shown in this table, GNN models with the consistency loss yield significant performance improvements over their base models on different datasets. These findings suggest that the consistency framework is a versatile and robust approach for enhancing the predictive capabilities of GNNs in real-world datasets, irrespective of the base model and dataset domain. Notably, the GIN method demonstrates the most significant improvements, achieving enhancements of up to 4.51% on the D&D dataset, 4.32% on the COLLAB dataset, and 3.70% on the IMDB-B dataset. This improvement can be linked to our empirical observation regarding the weak ability of GIN to preserve consistency across layers. In addition, our method demonstrates satisfactory improvements on datasets with numerous classes (e.g., COIL-RAG) and large-scale datasets (e.g., ogbg-molhiv and Reddit-T), indicating that our approach is both flexible and scalable for handling complex and extensive datasets.

Furthermore, we analyze the complexity and scalability of our method on the TU Dataset, with additional details provided in Appendices E.1 and E.2. Additionally, we demonstrate the method's potential to enhance performance significantly, even with a marginal increase in computational cost, as illustrated in Appendix F.

### Effect of the Consistency Loss on Rank Correlation

To answer **Q2**, we compare the consistency of graph representations across layers with and without the proposed consistency loss. Specifically, we use the Spearman's rank correlation coefficient, a widely accepted method for computing correlation between ranked variables, to quantitatively measure the consistency of graph similarities across layers. For a fair comparison, we construct a

    & NCI1 & NCI109 & PROTEINS & D\&D & IMDB-B & IMDB-M & COLLAB & COIL-RAG & OGB-HIV & REDDIT-T \\ \#Graphs & 4110 & 4127 & 1113 & 1178 & 1000 & 1500 & 5000 & 3900 & 41127 & 203088 \\ Avg. \#nodes & 29.87 & 29.68 & 39.06 & 284.32 & 19.77 & 13.00 & 74.49 & 3.01 & 25.50 & 23.93 \\  GCN & 73.96.12.37 & 74.04.130 & 73.24.450 & 74.92.160 & 75.40.129 & 55.07.124 & 81.72.044 & 91.72.158 & 72.86.130 & 76.00.04.644 \\ +Continuary & 75.12.139 & 73.25.125 & 75.07.858 & 78.56.322 & 75.85.121 & 56.27.110 & 83.44.640 & 93.38.144 & 73.75.089 & 77.12.012 \\  GIN & 78.13.211 & 76.75.248 & 72.97.49 & 71.10.443 & 70.80.407 & 52.13.242 & 79.84.148 & 93.33.124 & 71.60.328 & 77.50.148 \\ +Continuary & 79.45.109 & 77.46.136 & 74.98.457 & 75.51.283 & 74.50.336 & 53.64.244 & 84.16.664 & 94.03.133 & 74.57.141 & 77.64.085 \\  GraphSAGE & 74.40.438 & 73.71.470 & 74.96.154 & 76.44.143 & 73.90.214 & 51.33.287 & 78.92.124 & 89.56.237 & 77.03.185 & 76.57.621 \\ +Continuary & 78.26.130 & 74.10.126 & 76.40.132 & 77.50.334 & 74.75.336 & 74.28.527 & 82.12.839 & 82.12.138 & 78.60.144 & 77.57.065 \\  GTransformer & 75.72.200 & 74.79.123 & 73.33.450 & 75.42.322 & 72.204.349 & 53.33.121 & 80.36.058 & 83.74.317 & 76.81.134 & 76.75.123 \\ +Continuary & 76.83.130 & 75.82.153 & 77.03.539 & 76.57.245 & 73.75.256 & 56.53.134 & 80.48.047 & 91.67.118 & 76.90.325 & 77.14.008 \\  GMT & 75.04.145 & 73.90.123 & 72.70.424 & 72.80.218 & 79.80.164 & 54.13.230 & 80.36.115 & 90.85.191 & 74.86.123 & 72.06.119 \\ +Continuary & 75.52.107 & 75.20.048 & 74.86.243 & 73.14.258 & 79.60.119 & 54.80.142 & 82.80.648 & 92.00.143 & 76.00.199 & 777.19.141 \\   

Table 1: Classification performance on the TU, OGB and Reddit-T datasets, with and without the consistency loss. Highlighted cells indicate instances where the base GNN with the consistency loss outperforms the base GNN alone. The reported values are average accuracy for TU datasets and ROC-AUC for the ogbg-molhiv dataset, including their standard deviations.

distance matrix \(D^{h}\) for all test data at each layer \(h\), where each row \(D^{h}_{x_{i},:}\) represents the distances from graph \(x_{i}\) to all other graphs. We then compute the rank correlation between \(D^{h}_{x_{i},:}\) and \(D^{h+1}_{x_{i};}\) for each graph \(x_{i}\).

We average the correlation values for all graphs to obtain the overall correlation for layer \(h\). Then, we compute the mean of these values across layers, enabling a global comparison of relational consistency throughout the model and dataset. All results were averaged over 5 repeated experiments with the same training setting.

We present our results on a series of datasets from the TU Dataset in Table 2. As shown in the table, it is evident that the representation space becomes more consistent with our proposed consistency loss. For example, a significant enhancement is observed for the GIN model. Another notable point is the result for the GCN model on the NCI109 dataset and for the GMT model on the IMDB-B dataset. We find that the correlation is already fairly high even without the implementation of \(_{}\), resulting in minimal correlation improvements with our method. This phenomenon provides a plausible explanation for why our method is not effective in these two cases.

### Study on Task Complexity

To address **Q3** and further evaluate the performance of our method across different scenarios, we extended our study by conducting experiments on graph datasets with increasing task and structural complexity.

Increasing Task ComplexityWe increase the task complexity by expanding the number of classes that the model needs to classify. To assess the effect of increased class complexity on our method's performance, we sampled subsets from the REDDIT-MULTI 5K dataset (Yanardag and Vishwanathan, 2015) with a progressively greater number of classes, which originally consists of five classes. Specifically, we randomly sampled between 2 and 4 classes to construct new datasets from the original dataset and conducted classification tasks using both GCN and GCN with \(_{}\) on these newly constructed datasets. We report the mean test accuracy over five experiments for each subset, as presented in Table 3.

The results demonstrate that the effectiveness of our method remains robust, even as the number of classes increases. In fact, it may provide greater advantages when applied to multi-class classification tasks. This resilience likely stems from our method's focus on identifying relational structures in the intermediate representations of GNN models, rather than relying heavily on label information. This approach helps mitigate the impact of potential label noise in the original data. These findings align with the noticeable performance improvements observed in both binary and multi-class classification tasks, as shown in Table 1.

Increasing Structural ComplexityWe assessed the impact of structural complexity by partitioning the IMDB-B dataset into three subsets with progressively increasing graph densities. Graph density, denoted as \(d=\), where \(N\) is the number of nodes and \(M\) is the number of edges in graph

    & NCI1 & NCI109 & PROTEINS & D\&D & IMDB-B \\  GCN & 0.753 & 0.920 & 0.584 & 0.709 & 0.846 \\ +\(_{}\) & 0.859 & 0.958 & 0.946 & 0.896 & 0.907 \\  GIN & 0.666 & 0.674 & 0.741 & 0.721 & 0.598 \\ +\(_{}\) & 0.877 & 0.821 & 0.904 & 0.847 & 0.816 \\  GraphSAGE & 0.903 & 0.504 & 0.845 & 0.741 & 0.806 \\ +\(_{}\) & 0.911 & 0.709 & 0.916 & 0.872 & 0.933 \\  GTransformer & 0.829 & 0.817 & 0.867 & 0.865 & 0.884 \\ +\(_{}\) & 0.863 & 0.883 & 0.915 & 0.880 & 0.917 \\  GMT & 0.872 & 0.887 & 0.980 & 0.826 & 0.893 \\ +\(_{}\) & 0.906 & 0.908 & 0.983 & 0.856 & 0.908 \\   

Table 2: Spearman correlation was computed for graph representations from consecutive layers on the TU datasets, both with and without consistency loss. Values with higher rank correlation are highlighted. The consistency loss can enhance the rank correlation of graph similarities.

    & **Subset1** & **Subset2** & **Subset3** & **Fullset** \\  & **(2 classes)** & **(3 classes)** & **(4 classes)** & **(5 classes)** \\  GCN & 79.50 & 67.13 & 50.30 & 53.80 \\ GCN+\(_{}\) & 81.10 & 68.00 & 57.15 & 57.12 \\   

Table 3: Performance comparison across different subsets and the full set.

\(\), was used as the criterion for creating these subsets. The dataset was divided into three groups: (small) for graphs with densities below the 33rd percentile, (median) for densities between the 33rd and 67th percentiles, and (large) for graphs with densities above the 67th percentile. We applied both GCN and GCN+\(_{}\) models to these subsets, and the results are summarized in Table 4.

The above results show that the GCN model, enhanced with the \(_{}\) loss function, consistently outperforms the original version across different structural complexity groups, demonstrating the robustness and effectiveness of the proposed method.

Furthermore, we evaluate the method's efficiency under various complexity scenarios, as detailed in the Appendix E.3.

## 6 Related Work

Graph Distance and SimilarityMeasuring distances or similarities between graphs is a fundamental problem in graph learning. Graph kernels, which define graph similarity, have gained significant attention. Most graph kernels use the \(\)-Convolution framework  to compare substructure similarities. A trailblazing kernel by  used node and edge attributes to generate label sequences through a random walk. The WL-subtree kernel  generates graph-level features by summing node representation contributions. Recent works align matching substructures between graphs. For instance, kriegge2016graph proposed a discrete optimal assignment kernel based on vertex kernels from WL labels. togninalli2019graph extended this to include fractional assignments using the Wasserstein distance. Additionally, measuring graph distances is also a prevalent problem. vayer2019graph combined the Wasserstein and Gromov-Wasserstein distances . chen2022graph proposed a polynomial-time WL distance for labeled Markov chains, treating labeled graphs as a special case.

Bridging Graph Kernels and GNNsMany studies have explored the connection between graph kernels and GNNs, attempting to integrate them into a unified framework. Certain approaches focus on leveraging GNN architecture to design novel kernels. For instance, mairal2014graph presents neural network architectures that learn graph representations within the Reproducing Kernel Hilbert Space (RKHS) of graph kernels. Similarly, du2019graph proposed a graph kernel equivalent to infinitely wide GNNs, which can be trained using gradient descent. Conversely, other studies have incorporated kernel methods directly into GNNs. For example, mikolentzos2020graph utilized graph kernels as convolutional filters within GNN architectures. Additionally, lee2020graph proposes a novel Kernel Convolution Network that employs the random walk kernel as the core mechanism for learning descriptive graph features. Instead of applying specific kernel patterns as mentioned in previous work, we introduce a general method for GNNs to capture consistent similarity relationships, thereby enhancing classification performance.

## 7 Conclusion

In this paper, we study a class of graph kernels and introduce the concept of the consistency property for graph classification tasks. We theoretically prove that this property leads to a more structure-aware representation space for classification using kernel methods. Based on this analysis, we extend this principle to enhance GNN models. We propose a novel, model-agnostic consistency learning framework for GNNs that enables them to capture relational structures in the graph representation space. Experiments show that our proposed method universally enhances the performance of backbone networks on graph classification benchmarks, providing new insights into bridging the gap between traditional kernel methods and GNN models.

    & IMDB-B & IMDB-B & IMDB-B \\  & **(small)** & **(medium)** & **(large)** \\  GCN & 77.58\({}_{ 4.11}\) & 66.25\({}_{ 5.38}\) & 67.61\({}_{ 6.21}\) \\ GCN+\(_{}\) & 84.24\({}_{ 4.85}\) & 69.06\({}_{ 4.06}\) & 71.43\({}_{ 4.43}\) \\   

Table 4: Performance comparison on IMDB-B datasets of different densities.