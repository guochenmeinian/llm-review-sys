# Coom: A Game Benchmark for Continual Reinforcement Learning

Tristan Tomilin\({}^{1}\) Meng Fang\({}^{2,1}\) Yudi Zhang\({}^{1}\) Mykola Pechenizkiy\({}^{1}\)

\({}^{1}\)Eindhoven University of Technology \({}^{2}\)University of Liverpool

{t.tomilin,y.zhang5,m.pechenizkiy}@tue.nl

Meng.Fang@liverpool.ac.uk

###### Abstract

The advancement of continual reinforcement learning (RL) has been facing various obstacles, including standardized metrics and evaluation protocols, demanding computational requirements, and a lack of widely accepted standard benchmarks. In response to these challenges, we present COOM (**C**ontinual **DOOM**), a continual RL benchmark tailored for embodied pixel-based RL. COOM presents a meticulously crafted suite of task sequences set within visually distinct 3D environments, serving as a robust evaluation framework to assess crucial aspects of continual RL, such as catastrophic forgetting, knowledge transfer, and sample-efficient learning. Following an in-depth empirical evaluation of popular continual learning (CL) methods, we pinpoint their limitations, provide valuable insight into the benchmark and highlight unique algorithmic challenges. This makes our work the first to benchmark image-based CRL in 3D environments with embodied perception. The primary objective of the COOM benchmark is to offer the research community a valuable and cost-effective challenge. It seeks to deepen our comprehension of the capabilities and limitations of current and forthcoming CL methods in an RL setting. The code and environments are open-sourced and accessible on GitHub.

Figure 1: **COOM sequences** (encompassing tasks ordered from left to right) for task-incremental learning are composed of diverse environments built in ViZDoom . The agent perceives its surroundings from an embodied perspective. The 4-task sequences (**CD4** and **CO4**) are constituted by environments in the second half, whereas **CD16** and **CO16** repeat the original sequence.

Introduction

Deep reinforcement learning (RL) has made immense leaps forward over the past decade in the domain of video games [68; 60; 9], robotic manipulation [28; 30; 72; 50; 76], embodied AI [65; 12; 63], and foremost on a variety of platforms intended for RL research [44; 22; 5; 63]. However, this success has primarily resulted from fine-tuning agents to solve specific tasks. As RL is increasingly applied to solving real-life problems in industry, healthcare, or robotics, situations arise where the environment and conditions are subject to rapid change. Whereas humans are able to learn to perform new similar tasks seamlessly, this competence is still predominantly absent in RL agents, who tend to exceedingly overfit to new tasks and forget all previously acquired competencies. A central goal of RL is thus to build systems that can master a spectrum of skills in environments as noisy and diverse as the real world, while being capable of continuing to learn.

Continual learning (CL) is the ability to swiftly learn consecutive tasks while maintaining adequate performance on previously mastered problems, where they ought to reemerge . The CL paradigm emerges in RL when an agent proceeds to interact with an altered environment in the attempt to refine its policy . Whereas task boundaries in CL might be gradual and smooth, we propose to consider learning a sequence of tasks with sudden transitions. Evaluating a CL agent usually involves several criteria: catastrophic forgetting, forward transfer and backward transfer . CL research is particularly vital as it endorses the AI to thrive in encountered scenarios and conditions, which is much desired for AGI, compared to just solving individual problems.

Many prominent CL platforms are predominantly meant for supervised learning and require non-trivial adaptation for RL [47; 38; 16]. Video games act as ready-made simulation environments, which is why RL research platforms and benchmarks have widely been based on repurposed game engines [7; 6; 19]. There are, however, relatively few visual-based environments for CRL compared to regular RL. Researchers must instead use ad-hoc adaptations of RL benchmarks to achieve the desired setting. Proper virtual 3D simulation platforms and benchmarks are required to facilitate image-based CRL research. They need to be 1) lightweight, 2) well documented, 3) easy to install and use, 4) equipped with standardized metrics and baseline evaluations, and 5) computationally viable to run on small-scale systems and university-level budgets.

To this end, we present COOM (**C**ontinual **DOOM**), a CRL benchmark for embodied pixel-based learning on sequences of tasks with visually distinct 3D environments, designed to assess average performance, forgetting, and forward transfer. The visual modifications manifest in 1) wall and surface textures, 2) types, shapes, and sizes of in-game entities, and 3) modes to render objects. Compared to existing benchmarks for pixel-based learning oriented towards CL, COOM additionally includes a diverse set of objectives. To the best of our knowledge, this is the first benchmark specifically targeted towards CRL in complex 3D environments with differing objectives and visuals.

The contributions of our work are three-fold: 1) We assemble 6 task sequences of different lengths, half with task-dependent objectives, an aspect often absent in previous works. We further include a very complex sequence, intended as a challenge to the RL community to be solved with more advanced future algorithms. We include a demo of a trained agent performing COOM tasks. 2) We design 8 novel ViZDoom scenarios of two difficulties with contrasting visuals and dynamics to compose and publicly release COOM, a Continual RL benchmark. 3) Following the CL evaluation principles from , we employ multiple well-known CL methods for baseline evaluations on our task sequences, assessing prominent CL criteria. This makes our work the first to benchmark image-based CRL in 3D environments. We demonstrate how several methods fail to achieve the CL desiderata and how none come close to solving the complex sequence. We further elaborate on how the action distribution provides a more comprehensive understanding of the agent's continual progression. We conclude that critic regularization and single output head architectures tend to diminish performance in CL.

## 2 Related work

Most recent eminent CL platforms and frameworks, such as Sequoia , Avalanche , and Continuum  learn from static data sets of fixed sizes , and are thus not predominantly intended for RL. In CRL, the data sequence consists of different environments, and data samples are obtained through the agent-environment interaction. **Sequoia** introduces metrics and baselines aimed at CRL but only consists of simple environments like state-based manipulation tasks from 

**Meta-World**, continuous control tasks from MuJoCo , and Monster Kong . **Avalanche RL** introduces a library for CRL, but only includes direct support for basic Atari  and **Habitat v1.0** environments. Moreover, it does not present any experimental results on baseline methods.

Most prominent modern RL platforms and benchmarks do not offer a fixed setting for CL [19; 24; 13; 63; 61; 51; 75; 14; 2]. The **Atari** benchmark  has often been employed to assess popular CL methods [27; 59; 55]. However, the games on the platform are deeply unrelated, lacking the potential for transfer. **DeepMind Lab** provides a number of diverse 3D environments and **MineRL** incorporates many of the elements found in lifelong learning . Both have widely been used for assessing CL methods [56; 26; 55; 64]. However, neither include fixed task sequences or standardized CL metrics, requiring users to hand-pick existing tasks or design new environments. This causes a tedious experimental setup and provides no evident means of comparison to other works. **DeepMind Lab** and **ProcGen** use procedural content generation (PCG), which renders them computationally expensive to run for CL. Platforms like AI2Thor  and iGibson , based on the Unity3D game engine, are good at replicating the complexity and visual fidelity of real-world problems. However, they are computationally expensive to simulate, and thus often infeasible for low-budget computational setups.

Previous CRL benchmarks often exhibit certain deficiencies. **CRLMaze**, based on ViZDoom, presents a non-stationary object-picking task, subject to visual changes. However, it lacks a comprehensive baseline evaluation of the most popular CL methods, does not undertake to change the objective, and only modifies three attributes (light, textures, objects). **Continual World** assembles robotic manipulation tasks from Meta-World  to evaluate CRL and **Jelly Bean World** uses PCG to generate a 2D gridworld test bed. However, neither addresses embodied AI nor image-based learning. **L2Learner** creates a 3D PCG world to assess embodied agents, but lacks baseline evaluations and only consists of 5 tasks in a single environment. **CORA** aggregates environments from Atari , ProcGen , MiniHack , and CHORES  into task sequences. The first three environments are all 2D, and the latter lacks diverse visuals. We refer readers to Table 1 for the comparison. Numerous metrics have been employed throughout CRL research that attempt to measure similar objectives [15; 10; 48]. The abundance and incoherence of metrics can create confusion in interpreting and disentangling results.

## 3 Preliminaries

Reinforcement Learning from ImagesWe formulate embodied image-based learning as a partially observable Markov decision process (POMDP) , in which an agent interacts with an environment over a fixed horizon of discrete time steps \(T\). A POMDP can be described as a tuple \((,,p,R,,O,)\). At each timestep \(t\), the environment is in some state \(s_{t}\). By taking an action \(a_{t}\), the agent causes the environment to transition to another state \(s_{t+1}\) with probability \(p=(s_{t+1}|s_{t},a_{t})\). The agent cannot observe the full state \(s_{t}\) of the environment, but an observation \(o_{t}\) dependent on the action \(a_{t}\) taken and new state \(s_{t+1}\) reached with probability \(O(o|s_{t+1},a)\). \(\) is the high-dimensional set of pixel-observations. The agent receives a reward \(r_{t}=R(s_{t},a_{t})\), which is mapped by the reward function \(R:\) given a state and action. Finally, \([0,1)\) is the discount factor determining how much immediate rewards are favoured over more distant rewards. The \(n\)-step return \(R_{t:t+n}\) at time step \(t\) is defined as the discounted sum of rewards, \(R_{t:t+n}=_{i=1}^{n}^{i}r_{t+i}\). The value function \(V^{}(s)=R_{t:T}|s_{t}=s,\) is the

   Benchmark & 3D &  No. Task \\ Sequences \\  &  Embodied \\ View \\  &  Vision \\ Transfer \\  &  Objective \\ Transfer \\  & 
 Unified \\ Metric \\  \\  Continual World  & ✓ & 2 & ✗ & ✗ & ✓ & ✓ \\ L2Learner  & ✓ & 0 & ✓ & ✓ & ✗ \\ Jelly Bean World  & ✗ & 0 & ✗ & ✓ & ✗ & ✗ \\ CORA  & ✓1 ✗ & 4 & ✓1 ✗ & ✓1 ✗ & ✓1 ✗ & ✓ \\ CRLMaze  & ✓ & 4 & ✓ & ✓ & ✗ & ✓ \\  COOM & ✓ & 7 & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of existing Continual Reinforcement Learning benchmarks with COOM.

expected return from state \(s\), when actions are selected accorded to a policy \((a|s)\). The action-value function \(Q^{}(s,a)=R_{t:T}|s_{t}=s,a_{t}=a,\) is the expected return following action \(a\) from state \(s\). We aim to find a policy \((a_{t}|s_{t})\) that maximizes the cumulative discounted return \(_{}_{t=1}^{T}^{t}r_{t}|a_{t}(|s_{t}),s^{}_{t}(|s_{t},a_{t}),s_{1} p()\).

Continual LearningIn the CL setting of this paper, we aim to learn a policy \(_{}\) by training an agent sequentially on a fixed task sequence \(=\{_{1},,_{n}\}\). The \(i^{th}\) task is trained during the interval \(t[(i-1),i]\), where \(\) is the fixed number of iterations per task, during which the agent can only interact with the given environment. The agent thus obtains trajectories \(_{i,1},,_{i,}\) from a single task to learn from, during which the POMDP to solve becomes \((_{i},,p_{i},r_{i},)\).

## 4 COOM benchmark

The COOM (**C**ontinual **DOOM**) benchmark is based on ViZDoom , a flexible RL research platform for learning from raw visual information, based on the engine of the classical FPS video game Doom. ViZDoom has the benefit of being very lightweight, enabling gameplay up to 7000 FPS on a single GPU. The benchmark is comprised of 8 scenarios built in the _Action Code Script_ (ACS) language. Every scenario is orientated towards accomplishing a particular objective, each having one or multiple aspects which make the environment stochastic. In Table 2, we display the core properties of each scenario: 1) the metric for measuring agent performance, 2) are there enemies in the environment 3) is the agent equipped with a weapon, 4) do items spawn on the ground, 5) how many iterations an episode lasts, 6) which is the _execute_ action, and 7) what is randomized in the environment. A more detailed description of each scenario can be found in Appendix A. The limitations of COOM are addressed in Appendix L.

### Basic Setup

ObservationsThe agent is limited to only observing a portion of its surroundings, having a 90 degree horizontal field of view (FoV), which ranks lower than human vision  and modern cameras . The observation space \(\) is 4 stacked frames of \(160 120\) pixels in a 4:3 resolution with 3 channels of 8-bit values in RGB, which represents the partially observed environment from the first person perspective of the embodied agent.

Action SpaceTo facilitate the training of a CL task sequence using a single model, we require a unified action space across environments. We thus limit the full actions from the original Doom game to suit the scenarios of our benchmark. We obtain a multi-discrete action space \(\) for all environments by finding the Cartesian Product of singular actions \(=A_{1} A_{2} A_{3}\), where \(A_{1}=\{\}\), \(A_{2}=\{\}\), and \(A_{3}=\{\}\). The agent hence needs to choose one of \(||=12\) actions per iteration. Note that the execute operation \(A_{3}\) is scenario-dependent as indicated in Table 2.

RewardsThe core component of the reward granted in each scenario is directly tied with the success metric (e.g., performance in _Run and Gun_ increases when shooting a target, for which the

  Scenario & Success Metric & Enemies & Weapon & Items & Max Steps & Execute & Stochasticity \\  Pitfall & Distance Covered & ✗ & ✗ & ✗ & 1000 & JUMP & Pitfall tile locations \\ Arms Dealer & Weapons Deployed & ✗ & ✓ & ✓ & 1000 & SPEED & Weapon and delivery locations \\ Hide and Seek & Frames Alive & ✓ & ✗ & ✓ & 2500 & SPEED & Enenny behaviour, item locations \\ Floor is Lava & Frames Alive & ✗ & ✗ & ✗ & 2500 & SPEED & Platform locations \\ Chainsaw & Kill Count & ✓ & ✓ & ✗ & 2500 & ATTACK & Enemy and agent spawn locations \\ Raise the Roof & Frames Alive & ✗ & ✗ & ✗ & 2500 & USE & Agent spawn location \\ Run and Gun & Kill Count & ✓ & ✓ & ✗ & 2500 & ATTACK & Enemy and agent spawn locations \\ Health Gathering & Frames Alive & ✗ & ✗ & ✓ & 2500 & SPEED & Health kit spawn locations \\  

Table 2: **COOM scenarios.** The core properties determine 1) how the agent’s performance is measured; 2) whether enemies exist; 3) does the agent has a weapon; 4) do items spawn on the ground; 5) number of iterations in an episode; 6) which action can the agent execute apart from navigation; 7) what is randomized in the environment.

agent is also rewarded). We further utilize reward shaping for more granular dense feedback. A comprehensive description of reward functions is presented in Appendix B.

### Task Sequences

We compose three lengths of sequences: 1) 8 unique tasks \(^{8}=\{_{0},,_{7}\}\), 2) including only the second half of the former \(^{4}=\{_{4},,_{7}\}\) to streamline the experimental process, considering the computationally intensive nature of training RL agents, and 3) repeating the sequence \(^{16}=^{8}+^{8}\) (similar to ) allowing for revisiting each task in the same order. By default, the order of tasks in a sequence is predetermined. However, we provide the option for randomizing the sequence order, although this requires a substantially higher number of trials to ensure consistent results. This is due to 1) the performance of most continual learning methods being heavily dependent on the feature representation learned for the first task, and 2) the transferability of knowledge varying among different pairs of tasks. We distinguish between two sequence modalities.

Cross-Domain SequenceIn the cross-domain setting, the agent is sequentially trained on modified versions of the same scenario. We select the Run and Gun scenario as the basis, as it best resembles the original _Doom_ game, requiring the agent to navigate the map and eliminate enemies by firing a weapon. As depicted in Figure 0(a), the objective and map layout remain the same across tasks, whereas we modify the environment by 1) changing wall, ceiling and floor textures, 2) varying enemy size, shape and type, 3) randomizing the view height of the agent, and 4) adding obstructions which block the agent's movement. The modifications are inspired by the LevDoom generalization benchmark . We henceforth refer to the cross-domain sequences as CD4, CD8 and CD16.

Cross-Objective SequenceIn addition to changing the visuals and dynamics within a scenario, we now design and employ new scenarios with novel objectives for consecutive tasks, as illustrated in Figure 0(b). This presents a more diverse challenge, as the goal might drastically change from locating and eliminating enemies (Run and Gun and Chainsaw) to running away and hiding from them (Hide and Seek). This introduces a concept in CL commonly known as _Task Interference_. Similarly, the scenario Floor is Lava requires the agent to stay at a bounded location for good performance, whereas scenarios Pitfall, Arms Dealer, Raise the Roof, and Health Gathering encourage constant movement. We equivalently refer to the sequences as CO4, CO8 and CO16.

Cross-Objective ChallengeThere are two main dogmas in the RL community that dictate how to make progress in solving real-world decision-making problems . The first is usually referred to as the _generalizable agent_ view, in which focus should be attended towards the large-scale training of agents that solve diverse problems, hoping that along the way, a generalist agent will evolve. Our previously defined two task sequences endorse this philosophy. The second view, generally referred to as _deployable RL_, takes a more pragmatic view by seeking to design RL algorithms that solve particular difficult problems. To facilitate the advancement of the latter principle, we include a task sequence in our benchmark which serves as a challenge, aimed foremost to initially be solved. We posit that harder tasks are easier to forget in a CL setting. To this end, we create a _hard_ counterpart

Figure 2: **Constructing the hard COC sequence**. We modify each CO8 task, leading to more sparse and elusive rewards, to impose a more complex CL challenge.

for each task in CO8, as illustrated in Figure 2. We will further refer to this sequence as **COC**, which is further elaborated on in Appendix F.

### Evaluation Protocol

The agent is sequentially trained on each task \(_{i}\) for \(=200\)K iterations. After every 1000 iterations, we evaluate the policy on each task of the sequence for 10 episodes. 50K most recent trajectories are stored in the replay buffer, which is emptied at the end of each task by default. The Cross-Domain sequences are based on the _Run and Gun_ scenario, in which _Kill Count_ is the core measure of performance. In the Cross-Objective sequences, however, the agent is expected to achieve different goals across tasks. We are hence unable to directly use a single metric to adequately compare performance across tasks. As it would be tedious and convoluted to compare the results between methods on each task from a Cross-Objective sequence individually, we define a unified metric \(success=f_{i}(score)\), where \(score\) represents the task metric (see Table 2) and \(f_{i}\) is a function for task \(i\) that transforms the \(score\) to \(success\). The lower bound\(f_{i}(score)=0\) on \(_{i}\) is determined by the performance of a random agent, and the upper bound \(f_{i}(score)=1\) by **SAC** trained until convergence.

**Average Performance**\(P\) is the core evaluation metric in COOM and is measured across the number of total sequence iterations \(T=||\) averaging the \(success\) of all tasks \(\) at each time step \(t\):

\[P=|}_{_{i} }_{t=0}^{T}f_{i}(score_{t}).\] (1)

**Forgetting**\(F\) quantifies the average decrease of \(success\) from the end of training each task \(i\) to the end of the entire sequence \(T\). For consistency, we consider \(k\) last iterations of a task:

\[F=|-1}_{i=0}^{||-1} _{j=0}^{k-1}f_{i}(score_{(i+1)-j})- _{j=0}^{k-1}f_{i}(score_{T-j}).\] (2)

Note that we exclude the final task, as no forgetting can have occurred. In the case of negative values for forgetting (\(F<0\)) we observe positive backward transfer.

**Forward Transfer** FT is measured by first finding the area under the training curve \(AUC_{i}\) on task \(i\) for the CL method and for the SAC baseline \(AUC_{i}^{b}\) with

\[AUC_{i}=_{i-1}^{i}f_{i}(score_{t} )t,AUC_{i}^{b}=_{0}^{}f_{i}^{b}(score_{t} )t.\] (3)

We can then first find the average normalized area between the curves across all tasks. Note that we do not consider the first task, as there is no knowledge to transfer forward from previous tasks:

\[FT=|-1}_{i=1}^{||}-AUC_{i}^{b}}{1-AUC_{i}^{b}}.\] (4)

## 5 Experiments

### Setup

In this section, we assess the CL desiderata of baselines from three different families of popular CL methods on our task sequences. **Regularization-based** methods constrain weight updates in order to maintain knowledge from previous tasks. **L2** adds a simple \(L_{2}\) penalty to achieve this. **MAS** utilizes a weighted penalty on each parameter depending on its importance to the network

    &  &  &  &  &  &  \\   & AP & F & FT & AP & F & FT & AP & F & FT & AP & F & FT & AP & F & FT \\  PackNet & **0.92** & 0.00 & **0.40** & 0.87 & 0.01 & -0.24 & 0.91 & -0.01 & 0.19 & 0.82 & -0.01 & **0.25** & **0.20** & 0.04 & **0.08** & **0.74** & 0.01 & 0.13 \\ MAS & 0.55 & 0.50 & 0.11 & 0.72 & 0.24 & -0.04 & 0.82 & 0.14 & 0.25 & 0.58 & 0.19 & 0.01 & 0.04 & 0.09 & 0.02 & 0.54 & 0.23 & 0.07 \\ AGEM & 0.35 & 0.80 & 0.10 & 0.42 & 0.80 & **0.03** & 0.30 & 0.86 & 0.17 & 0.30 & 0.84 & 0.23 & 0.02 & 0.01 & 0.02 & 0.28 & 0.68 & 0.11 \\ L2 & 0.71 & 0.01 & -0.28 & 0.80 & 0.00 & -0.60 & 0.87 & -0.03 & 0.07 & 0.71 & 0.84 & -0.33 & 0.10 & **0.00** & 0.00 & 0.64 & **-0.02** & -0.23 \\ EWC & 0.69 & 0.00 & -0.41 & 0.65 & 0.05 & -0.77 & 0.76 & **-0.04** & -0.55 & 0.65 & 0.05 & -0.38 & 0.07 & **0.00** & -0.01 & 0.56 & 0.00 & -0.43 \\ VCL & 0.46 & 0.77 & 0.21 & 0.40 & 0.82 & -0.57 & 0.36 & 0.73 & 0.04 & 0.39 & 0.64 & 0.20 & 0.03 & 0.16 & 0.05 & 0.33 & 0.62 & -0.01 \\ Fine-tuning & 0.59 & 0.63 & 0.32 & 0.50 & 0.71 & -0.01 & 0.45 & 0.75 & **0.28** & 0.44 & 0.48 & 0.23 & 0.02 & 0.10 & 0.02 & 0.40 & 0.53 & **0.17** \\ ClonEx-SAC & 0.87 & 0.00 & 0.11 & 0.86 & **-0.03** & -0.26 & **0.92** & -0.03 & 0.13 & **0.89** & 0.00 & **0.27** & 0.13 & 0.01 & 0.03 & 0.73 & -0.01 & 0.06 \\ Perfect Memory & 0.89 & **-0.01** & 0.30 & **0.89** & 0.02 & **0.03** & - & - & - & - & - & - & - & - & - & - \\   

Table 3: Results of Average Performance (**AP**), Forgetting (**F**) and Forward Transfer (**FT**) across 10 seeds. Extended results with 95% confidence intervals are in Tables 19 of Appendix N. The result of the best performing method is highlighted in bold.

output. **EWC** uses the Fisher information matrix to approximate the importance of each weight. **VCL** uses variational inference to minimize the KL divergence between the prior and posterior distribution of parameters. **Structure-based** methods preserve and update the network architecture to efficiently learn and adapt to new tasks over time. **PackNet** forbids any changes to parameters that are important for previous tasks by freezing the most relevant weights at the end of each task and pruning the rest. **Rehearsal-based** methods retain samples from previous tasks to constrain forgetting. **Perfect Memory** refrains from resetting the buffer after a task and thus remembers everything. **A-GEM** projects gradients from new samples as to not interfere with previous tasks. **ClonEx-SAC** retains some samples from previous tasks and performs behavior cloning based on them to reduce forgetting. We also include the naive **Fine-tuning** baseline, which simply continues regular weight updates on a new task. All the CL methods take the efficient and well-known Soft Actor-Critic (**SAC**)  as an RL training backbone. We follow the evaluation protocol outlined in Section 4.3 to measure performance of all baselines on CD4, CO4, CD8, CO8 and COC with fixed orders across 10 seeds controlling the pseudorandom nature of the environments. We refrain from using random sequence orders due to the necessity for numerous runs to ensure consistent evaluation, a demand that exceeds our available computational resources. Similarly, we exclude Perfect Memory on 8-task sequences due to the unfeasible memory requirements for storing all trajectories. We grant the agent full access to the task identity both at training and test time, a CL setting coined as _Task-Incremental Learning_. We use \(95\%\) confidence intervals for all results. Our framework is discussed in finer detail in Appendix C.

### Main Results

PerformancePacket, closely followed by ClonEx-SAC, triumphed across all sequences according to the results in Table 3. We conjecture PackNet's success on our benchmark to two key factors: 1) knowing the task identity during evaluation, and 2) short task sequences with distinct boundaries, as PackNet needs to assign a fixed fraction of parameters to each task. It is noteworthy that the top three best-performing methods (PackNet, ClonEx-SAC, and L2) originate from different families of CL,

Figure 3: **Visual analysis of CO8 results**. 1) The average cumulative **performance** curves indicate a high disparity across baselines (**top**). 2) ClonEx-SAC effectively mitigates **forgetting** according to the continual evaluation curves of individual tasks (**middle**). The tasks are very diverse, as no notable performance is reached before exposure to the task itself. 3) ClonEx-SAC successfully transfers knowledge forward compared to the vanilla SAC, which is trained on each task from scratch (**bottom**). The blue area between the curves indicates positive **forward transfer** and red represents its negative counterpart.

indicating that each approach has a strong potential for achieving excellent results in the benchmark. Although Perfect Memory performed well on the 4-task sequences, it required nearly twice as long to run, and took up 16x more memory in the process. Memory consumption and walltime is further discussed in Appendix E. The average success curves in Figure 3 illustrate a significant disparity in performance among our baselines. Despite an exhaustive exploration of hyperparameters, we were unable to attain satisfactory results with AGEM and VCL. These methods ranked as the poorest performers in our benchmark, remarkably even falling below the naive fine-tuning approach. Comparing results across sequence lengths, almost all methods showed higher results in CO4 than CO8, whereas among the cross-domain sequences, the top four methods had a substantially higher performance on the longer version. Performance related to sequence length is more extensively analysed in Appendix G. The performance difference between CD and CO sequences are not that staggering, indicating that visual perturbations alone, without a changing objective in our embodied 3D environments, are sufficiently challenging for our CRL baselines.

ForgettingThe top four baselines excel at preserving performance on previously learned tasks, approaching near-perfection. However, out of these four, L2 and EWC have a substantially lower performance, indicating that regularization-based methods predominantly emphasize stability at the expense of plasticity. Despite an extensive hyperparameter search for weaker regularization, we were not able to achieve a more favorable stability-plasticity trade-off. In particular, a lower regularization constant exhibited a loss of stability but no noticeable further gain in plasticity. In comparison to good stability, AGEM and VCL experience a decline in average performance immediately after losing access to a learned task. In terms of forgetting particular tasks, Pitfall and Arms Dealer appear the hardest to remember given our fixed ordering of tasks in the CO sequence, as analyzed in Appendix D.

TransferWe attribute the core capacity for transfer in our benchmark environments to navigation. Success in most tasks heavily relies on the ability to navigate effectively (e.g., repeatedly running into walls rarely contributes to a high reward). Being able to identify objects and entities from their surroundings also holds significant importance for most tasks (e.g., detecting useful items to collect). The training success curve of ClonEx-SAC in Figure 3 delineates the poor sample-efficiency of the vanilla SAC which is unable to attain comparable performance on several tasks by learning them from scratch. Regularization-based methods fall significantly below others in terms of utilizing acquired competencies to effectively learn future tasks. We did not observe notable backward transfer in our experiments.

### Network Plasticity

Recent research has emphasized the importance of network plasticity in RL [1; 40; 46; 33]. To further enhance our analysis and provide a comprehensive perspective on the ability to adapt to new targets on our benchmark. Similar to the experimental design in , we aim to investigate whether our base SAC agent experiences a decline in network plasticity when confronted with the Cross-Objective task setting. Intriguingly, unlike the trend observed in , our findings diverge given the learning curves of CO4 in Figure 4, where the agent cyclically repeats the sequence 10 times. The agent is able to reach similar results in equal duration throughout the iterations without any noticeable deterioration. We therefore conclude that our base SAC agent does not exhibit a decline in network plasticity when subjected to our benchmark. An avenue for future research would be to investigate whether other

Figure 4: **SAC does not exhibit a loss of network plasticity on COOM.** Measuring the individual performance of each task across 5 seeds displays that repeated exposure to the same tasks throughout 10 iterations of the CO8 sequence does not hinder the capacity to reattain peak performance.

methods, such as model-based approaches, exhibit a similar pattern. The results of CO8 and further analysis is included in Appendix H.

### Action Distributions

Since the agent has a fixed multi-discrete action space across all scenarios, we can visualize how the action distribution shifts over time as training progresses to new tasks. We plot the actions executed by PackNet on CO8 in Figure 5. PackNet's high performance suggests it has obtained an effective action distribution. We can observe how PackNet quickly manages to adjust its policy to select appropriate actions when presented with a new task. The action distribution indicates, that on, average executing multiple sub-actions in a single timestep is more beneficial. In Appendix M we further visualize how our baselines are able to maintain the acquired action distribution on each given task.

### Method Variations

Image AugmentationUsing augmented images as input has been recognized for its potential to enhance data diversity, mitigate catastrophic forgetting, and facilitate adaptation to changes in data probability distribution . We explore the impact of incorporating three established image augmentation methods for RL to our selected baselines on COOM: 1) **Random Convolution** randomly perturbs input observations, 2) **Random Shift** pads each image with 4 pixels (repeating boundary pixels) and then applies a random crop, yielding the original image shifted by \( 4\) pixels, 3) **Random Noise** injects Gaussian noise into the image.

In our comparative analysis depicted in Figure 6, we can observe that random shifts and noise have minimal impact on the performance of our baselines, whereas random convolutions significantly degrade all methods except PackNet. They lead to substantial imbalances in the data distribution for initial tasks, causing the policy to converge to a local minimum from which it struggles to recover. This phenomenon is especially pronounced in regularization-based methods that rely on maintaining stable feature representations, which are highly susceptible to disruption caused by random convolutions due to their rigid architecture and stringent regularization. Additionally, these perturbations affect the data distribution stored in the memory of rehearsal-based methods, diminishing their effectiveness for future task learning. Our findings indicate that PackNet can better recover from a degenerate policy stuck in local minima, attributed to freeing up redundant parameters after training on a task. This suggests that architecture-based methods exhibit greater flexibility in the face of significant changes in data distribution.

LstmWe include an LSTM encoder atop the CNN head in our model architecture to assess its impact on our baseline methods. Employing a similar approach to a supervised CL setting has previously been shown to be beneficial . On our benchmark, however, this approach completely

Figure 5: **PackNet quickly adapts its policy to fit a new task when training on CO8**. The stackplot (**left**) depicts the distribution of actions executed in the environment. For example, in Pitfall, PackNet highly favours moving forward while accelerating to reap the highest return. The histogram (**right**) indicates the mean number of action executions across the entire sequence. Actions constituting maximal sub-actions appear to be most optimal according to PackNet.

deteriorates the performance of most methods, rendering them unable to learn anything meaningful. Strikingly, PackNet's performance remained consistent, whereas AGEM experienced a substantial increases. We hypothesize that, by virtue of capturing sequential patterns, an LSTM encoder can facilitate more accurate and efficient retrieval of relevant experiences from the episodic memory. This improved retrieval could enable AGEM to make better use of past knowledge when confronted with new tasks. However, more in-depth analysis is required to confirm this.

Critic RegularizationIn the context of actor-critic frameworks like SAC, there is a design decision regarding whether to regularize the critic. In some CL methods, the critic is only used during training and only needs to store the value function for the current task, making forgetting less of a concern. However, certain methods, such as replay-based approaches, may not easily accommodate a framework where only the actor is regularized. We thus investigate critic regularization affects our baselines. Figure 6 confirm the findings from , that regularizing the critic network has a harmful impact for CL. This suggests that it is better to let the critic adapt freely to the current task while prioritizing the minimizing of forgetting in the actor. However, this means that the critic needs to be trained from scratch, which is not particularly efficient when tasks are repeated.

Output HeadsIn our default setup, we use a separate output head for each task, which is known to help prevent forgetting . However, this approach is inapplicable to dynamic scenarios with and unknown number of tasks. Figure 6 depicts how a single-head architecture impacts our baselines. All methods show a decline in performance, however ClonEx-SAC, PackNet and VCL manage to better maintain their original performance. Extended results are presented in Appendix I.

## 6 Conclusion

Training proficient agents, who are able to continually adapt to learning new tasks in an altering environment, currently remains one of the greatest challenges in RL. To aid the community in grappling with this challenge, we presented and openly released COOM, a novel benchmark for assessing CL in visually distinct 3D environments with diverse objectives. COOM comprises seven task sequences of varying length, complexity, and modality, effectively demonstrating the strengths and weaknesses of popular continual learning methods. Our experimental evaluations reveal the impact of different variations to baseline methods, highlighting the importance of critic regularization and task-dependent output architectures. We visually illustrate how high-performing methods quickly adapt their policies and maintain learned action distributions across tasks within a sequence. All the methods in our evaluations struggle with the complex COC sequence, making it a valuable testing ground for future research, showcasing both learning challenges and transfer difficulties. COOM serves as a crucial tool to inspire the development of more capable algorithms and we expect it to facilitate the evaluation and comprehension of future continual reinforcement learning methods.