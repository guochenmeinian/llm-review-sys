# Searching for Efficient Linear Layers over a

Continuous Space of Structured Matrices

 Andres Potapczynski

New York University

&Shikai Qiu1

New York University

&Marc Finzi

Carnegie Mellon University

Christopher Ferri

Capital One

&Zixi Chen

New York University

&Michal Goldblum

Columbia University

&C. Bayan Bruss

Capital One

Christopher De Sa

Cornell University

&Andrew Gordon Wilson

New York University

Equal contribution. Correspondence to ap6604@nyu.edu, sq2129@nyu.edu, andrewgw@cims.nyu.edu.

###### Abstract

Dense linear layers are the dominant computational bottleneck in large neural networks, presenting a critical need for more efficient alternatives. Previous efforts focused on a small number of hand-crafted structured matrices and neglected to investigate whether these structures can surpass dense layers in terms of compute-optimal scaling laws when both the model size and training examples are optimally allocated. In this work, we present a unifying framework that enables searching among all linear operators expressible via an Einstein summation. This framework encompasses many previously proposed structures, such as low-rank, Kronecker, Tensor-Train, Block Tensor-Train (BTT), and Monarch, along with many novel structures. To analyze the framework, we develop a taxonomy of all such operators based on their computational and algebraic properties and show that differences in the compute-optimal scaling laws are mostly governed by a small number of variables that we introduce. Namely, a small \(\) (which measures parameter sharing) and large \(\) (which measures the rank) reliably led to better scaling laws. Guided by the insight that full-rank structures that maximize parameters per unit of compute perform the best, we propose BTT-MoE, a novel Mixture-of-Experts (MoE) architecture obtained by sparsifying computation in the BTT structure. In contrast to the standard sparse MoE for each entire feed-forward network, BTT-MoE learns an MoE in every single linear layer of the model, including the projection matrices in the attention blocks. We find BTT-MoE provides a substantial compute-efficiency gain over dense layers and standard MoE.

## 1 Introduction

Neural networks primarily consist of interleaved linear layers and simple non-linearities. In large foundation models such as GPT-3 , these linear layers consume the vast majority of the parameters and computation , and are commonly represented by dense matrices. Substituting these dense matrices with structured matrices with fast matrix-vector multiplies (MVMs) has the potential to significantly improve the computational efficiency of these models.

Recent work by Dao et al. , Fu et al. , and Qiu et al.  demonstrated that incorporating certain structured matrices into neural network architectures, including transformers, can improveperformance over dense models of the same size trained for equal number of epochs on problems such as ImageNet classification. However, these success cases do not reflect the current paradigm of large-scale training, where the models 1) are typically not trained for multiple epochs, making the expressiveness of dense matrices particularly appealing since the generalization gap vanishes, and 2) are heavily bottlenecked by compute cost, making it infeasible to train the models until convergence [13; 11], unlike in image classification. These attributes of large-scale training make the compute-optimal scaling rather than scaling in model size alone more relevant.

In this work, we investigate how different structures perform in a compute-optimal setting, which characterizes performance as a function of training compute when allocated optimally between using larger models versus training on more data . In language modeling and many other tasks, the compute-optimal scaling law has been shown to take the form \(=_{}+bC^{-}\) as a function of training compute \(C\), where \(_{}\) is the minimal achievable loss [13; 11; 10]. Quantifying the compute-optimal scaling laws of various structures is essential for understanding their practical value for training large-scale neural networks.

In addition to investigating the scaling laws of existing structures, we expand the set of matrix structures beyond what has been previously considered. We do so by introducing a continuous parameterization of the space of all possible structures whose matrix-vector-multiplication (MVM) can be expressed as an Einstein summation (Einsum).2 This space contains many known structures such as low-rank, Tensor-Train , Kronecker product [21; 25; 9], Monarch  and Block Tensor-Train , but also includes many novel hardware-efficient structures. Indeed, all structures in this space are hardware-efficient in the sense that they are computed through a series of batch matrix multiplication primitives, which we implement through the Linear Operator abstractions available in CoLA. Moreover, this space lends itself to an intuitive exploration as we can analyze how different parameters of the Einsum affect a structure's performance and scaling laws. We make our code available here.

We summarize our main contributions as follows:

* We introduce a continuous parameterization of the space of structured matrices whose matrix-vector-multiplication can be implemented via an Einstein summation (Einsum). This parameterization allows us to search a wide range of hardware-efficient structured linear layers for neural network architectures beyond a handful of well-known cases identified in prior work [6; 8; 19].
* We develop a taxonomy of the space of Einsums based on its computational and algebraic properties. We identify three key scalar quantities that characterize this space \((,,)\). (1) \( 0\), which reflects the extent of parameter sharing in a matrix. (2) \(\), which characterizes to the rank of the structure (\(=1\) meaning full-rank). (3) \([0,1)\), which

Figure 1: **We use Einsums to parameterize a wide range of structured matrices and search for the most efficient structure for compute-optimal training. Left: A diagrammatic representation of a general two-factor Einsum. We parameterize the space of Einsums through a real-valued vector \(=(_{},_{},_{},_{}, _{},_{},_{})^{7}\). This space captures many well-known structures through specific values of \(\). Middle: Example of well-known structures with their \(\) values. Any omitted line implies the value of the entry in the vector is 0. Right: Compute-optimal scaling laws of example structures for GPT-2 on OpenWebText when substituting its dense layers (see details in Section4).**

relates to compute per dimension in an MVM, where the upper-bound \(=1\) is achieved by dense matrices. Intuitively, \(\) measures how much a structure resembles dense.
* We investigate the scaling laws of different Einsums on language modeling, autoregressive image generation, and a synthetic regression task. We find that the best-performing structures are the ones that do not share parameters (\(=0\)), are full-rank (\(=1\)), while \(\) can be varied with often negligible impact to their scaling laws. In contrast to previous findings, we demonstrate that structures shown in prior work [6; 8; 19] to outperform dense matrices in non-compute-optimal settings can yield similar but not significantly better compute-optimal scaling laws on these tasks.
* Building on prior work in structure-aware learning rates , we show how to properly initialize generic Einsum layers and transfer learning rates from the original dense layers and across model sizes, leveraging insights from \(\)P [29; 26] and manifold optimization .
* Based on the observed relation between the taxonomy variables and the scaling laws, we propose a new structured Mixture of Experts (MoE) architecture implementing a sparse mixture of multiple structure matrices. This block tensor-train (BTT) MoE provides a sparse MoE in every single layer of each feedforward network (FFN) and attention project matrices, compared to standard MoE which operates over entire FFNs. We show BTT-MoE is significantly more compute-efficient than dense matrices and standard MoE for training GPT-2 language models.

## 2 Parameterizing the Space of Einsums

We now present a unifying framework that parameterizes all linear operators \(^{d_{} d_{}}\) whose matrix-vector-multiply \(=\) can be expressed as an Einsum over the tensors \(,,,,\) where \(,,\) defines the operator \(\) and contains all its learnable parameters. To simplify the presentation, throughout this paper we assume \(\) is defined using only two factors \(,\), but we show generalization to more than two factors is straightforward in Appendix E.

We consider the following general expression of such an Einsum

\[Y_{}=_{}B_{ }A_{}X_{},\] (1)

where the vectors \(\) and \(\) are written as tensors with multiple indexes to allow them to interact differently with each other, and the factors \(,\). Each index \(x\{,,,,,,\}\) ranges from \(1\) to \(d_{x}\). Given this general expression, we obtain different structures via different factorizations of \(d_{}\) into \(d_{}d_{}d_{}\), and \(d_{}\) into \(d_{}d_{}d_{}\), and separately a choice of \(d_{}\). For example, for a low-rank matrix, we have \(d_{}=d_{},d_{}=d_{}=1\), \(d_{}=d_{},d_{}=d_{}=1\) and \(d_{}=r\). For a Kronecker product, we have \(d_{}=d_{}=}}\), \(d_{}=d_{}=}}\) and \(d_{}=d_{}=d_{}=1\). We provide an extended list of examples in Appendix A.

The general expression and the above two examples can be more conveniently and intuitively represented as a diagram shown in Figure 1 (left), where each index \(,,,\) corresponds to a (hyper)edge among the input, output, and weight factors: \(\{,\}\), \(\{,\}\), \(\{,,\}\), \(\{,\}\), \(\{,\}\), \(\{,,\}\) and \(\{,\}\). This set of edges can be written succinctly as \(=\{S\{,,,\}:|S|  2\{,\} S\}\). We exclude subsets that contain \(\) and \(\) simultaneously, as adding them simply produces an already included structure but repeated multiple times along one of the input and output axes. The structure of a particular Einsum is fully specified by the the vector \((d_{},d_{},d_{},d_{},d_{},d_{},d_{})\), which specifies the range of the indices \(,,,,,,\). When the range of an index is of size 1, the corresponding edge effectively disappears from the diagram and the expression simplifies.

As we will build models of varying sizes, it is more natural to think about how these entries scale with \(d_{}\) and \(d_{}\). We therefore assign a real-valued vector \(^{7}\) to each structure indicating that \(d_{i}=d_{}^{_{}}\) for \(i\{,,\},\)\(d_{j}=d_{}^{_{j}}\) for \(j\{,,\},\) and \(d_{}=(d_{},d_{})^{_{}}\), with the restriction that \(_{}+_{}+_{}=_{ }+_{}+_{}=1\). For example, a low-rank matrix whose rank scales as the dimension it operates on to the \(1/2\)-th power is represented as \(=(1,0,0,0,1,0,1/2)\), and a Kronecker product of two factors of equal sizes is represented as \(=(1/2,1/2,0,1/2,1/2,0,0)\). We round all \(d_{i}\) to its nearest integer when instantiating the Einsum. Note this rounding only quantizes the the values of \(d_{i},\) but leaves the space of meaningfully different \(\)s continuous as we consider arbitrarily large matrices.

## 3 A Taxonomy of the Space of Einsum Linear Structures

The space of all Einsums is a high-dimensional space containing a wide range of possible structures. _A priori_, it is difficult to reason about the properties of a particular point in this space given its coordinates \(^{7}\). In this section, we develop a taxonomy of the space of Einsums based on their computational and algebraic properties. We introduce three key scalar quantities that characterize this space: (1) \(\), which is related to the rank of the structure, (2) \(\), which is related to the compute intensity of the structure, i.e. FLOPs per MVM divided by the dimension, and (3) \( 0\), which is related to the number of learnable parameters divided by the FLOPs per MVM. Each of these three quantities can be expressed in terms of the entries of \(\), which we derive in Appendix B. To simplify the presentation, we assume \(d_{}=d_{}=d\) in the rest of the section. Without loss of generality, we assume \((_{},_{})(_{},_{})\), so that it is more efficient to first multiply by \(\) instead of \(\).

**Rank Exponent, \(_{}\)** For a given \(\), we have that \(()=(d^{})\) where \(=(1,2+_{}-_{}-_{})\). Thus, \(=1\) implies full-rank and decreasing values of \(\) imply lower ranks until the limit of \(0\). In other words, the rank decreases when an Einsum increasingly allocates part of the input and output to factors that are _only_ connected the input or output. The limit being a low-rank structure as seen in Figure 2 where \(_{}=1=_{}\) and which creates a bottleneck on \(\{,\}\). The opposite trend is exhibited by dense, which allocates the full dimension of the input and output to both factors as seen in Figure 2. Nonetheless, it is still possible to achieve a full-rank when allocating dimension to only single factors as long as those values do not constitute most of the allocation, meaning \(0<_{} 1/2\) and \(0<_{} 1/2\), as seen in Monarch, T

Figure 3: **Compute-optimal frontier (highlighted points) of various Einsums follows power law scaling. As a result, Einsums can be scaled to reach arbitrarily low reducible loss, each with a different rate that can be estimated from small-scale experiments.**

Figure 2: **Illustrating the Einsum taxonomy. The 3D graph represents relevant quantities of the Einsum structure such as the amount of parameter sharing \(\) (x-axis), its rank \(\) (y-axis), and its compute intensity \(\) (z-axis). The structures on the left of the figure appear as dots on the graph based on their coordinates \(\). We highlight two key subspaces. (a) The BTT subspace, characterized by no parameter sharing \(=0\), learning the maximum number of parameters per FLOP. (b) The full-rank BTT subspace where \(=0\) and \(=1\). In Section 4 we show that the full-rank BTT subspace contains the most performant structures across multiple tasks.**

but not for the particular Block Tensor-Train (BTT) structure  in Figure 2. In Section 4 we show that structures with \(=1\) perform best when training neural networks.

**Compute Intensity Exponent, \(\).** Let \(F\) denote the FLOPs required to perform a MVM and define the compute intensity as \(F/d=(d^{})\). The upper bound is achieved by dense, which requires quadratic compute for an MVM and thus \(=1\). In general, we have \(=1+_{}-(_{},_{})\). Thus, in order to achieve lower compute intensity than dense, a structure has to allocate dimensionality to factors that _only_ connect to the input and output. As seen in Figure 2, the BTT example is able to achieve the lowest compute intensity by allocating a substantial part of the input dimension to the first factor and a substantial part of the output dimension to the second factor. \(\) and \(\) are not completely independent, e.g. \(=\) for low-rank matrices \(W_{ij}=_{k=1}^{r}B_{ik}A_{kj}\), though exceptions exist such as for the Kronecker product where \(\) can be arbitrarily low while maintaining \(=1\). In Section 4, we show there exists a wide range of structures with varying \(\) that perform as well as dense matrices.

**Parameters-Sharing Exponent, \(\).** Let \(N\) denote the number of parameters in the structure then \(N/F=(d^{})\). Clearly, \(=1\) for dense matrices where each parameter is used exactly once in an MVM. In general, we can show that \(=(_{}+_{},_{}+ _{})-(_{},_{})\). In Section 4 we find that structures that share parameters, that is \(>0\), have worse scaling laws that structures that do not (\(=0\)). To achieve \(=0\), we have to avoid introducing edges that _skip_ some factors, that is \(_{}=_{}=0\). Structures that skip factors in Figure 2 are Tensor-Train and Kronecker where there exists an edge that connects \(\) to \(\) while skipping \(\). In contrast, in Monarch and BTT, the edge connecting \(\) with \(\) also touches \(\).

## 4 Scaling Laws of Einsums

While prior works have shown that certain structured matrices such as Monarch and BTT have better scaling laws than dense matrices as a function of model size on datasets such as CIFAR-10, CIFAR-100, and ImageNet [6; 19], their experimental setups do not reflect today's large-scale training, where the models 1) typically do not train for multiple epochs on the training set, and 2) are heavily compute-bottlenecked such that we care primarily about performance as a function of training compute rather than model size (omitting the cost of training). These attributes of large-scale training make the compute-optimal scaling rather than scaling in model size alone more relevant.

In this section, we investigate the compute-optimal scaling laws of a wide range of Einsums -- how their performance scales as a function of training compute. We will show that we can understand the systematic differences in the scaling laws of various Einsums by leveraging the taxonomy we have developed. While we do not find a structure that achieves noticeably better scaling laws compared to dense matrices, we identify the set of common properties shared across a wide range of structures that match the performance of dense matrices, based on which we will propose a significantly more efficient alternative to dense layers in Section 5.

### Main Experimental Setup

We train GPT-2  language models on the OpenWebText dataset. To make our measurement of the scaling laws more robust and our experiments more affordable, we reduce the vocabulary of the original GPT-2 to \(96\) commonly used alphanumeric symbols. Using a small vocabulary limits the compute and parameters consumed by the language modeling head, which would otherwise obscure the scaling laws measured at small scales . We train models of varying sizes from \(120\)k to \(76\)M parameters, with model dimension \(d\) and number of transformer blocks \(L\{3,6\}\). Each model is trained for \(100\)k steps with a batch size of \(65536\) tokens and a sequence length of \(128\). All linear layers except the language modeling head are replaced with Einsums. We use the Adam optimizer with a base learning rate of \(0.003\) for a \(L=3,d=256\) dense model, and scale it using \(\)P  and structure-aware learning rates  to larger models and models using Einsums in place of dense layers. We discuss learning rate scaling in detail in Section 6, showing it is crucial for the performance of Einsums. We use weight normalization to stabilize the training of Einsums following Qiu et al. .

In Appendix D, we show our main conclusions derived from this simplified setup translate to the more standard GPT-2 evaluation with a longer sequence length of 512 and its original vocabulary of 50,257 tokens.

### Analyzing the Compute-Optimal Scaling Laws

**Einsum Performance Obeys Power Law Scaling.** When replacing the standard dense layers with Einsums, we find the resulting model's loss continues to follow the usual \(=_{}+bC^{-a}\) compute-optimal scaling laws except with possibly different constants \(a,b\). In Figure 3, we visualize the compute-optimal scaling laws of various Einsums on our language modeling task, including those corresponding to previously proposed structures such as TT , Low-rank and BTT , as well as a generic Einsum with all entries of \(\) strictly positive. We report the reducible loss with an estimated \(_{}=0.75\) subtracted. This finding suggests that all Einsums can be scaled to reach arbitrarily low reducible loss, each with a different rate that can be estimated from small-scale experiments.

**Parameter Sharing Leads to Worse Scaling.** As discussed in Section 3, the vast majority of Einsums implement some kind of parameter sharing, where the number of parameters \(N\) in the Einsum relates to its MVM FLOPs \(F\) via \(N/F=(d^{-}),\) for some \(>0\). In Figure 4 (left), we show the scaling laws of a wide range of Einsums (only including points on the compute-optimal frontier) colored by \(.\) We find larger values of \(\) lead to significantly worse scaling laws. To search for compute-efficient structures, we should therefore focus on the subspace with \(=0\).

**Full-Rank Performs Best.** Within the \(=0\) subspace, we find that \(\) becomes the next most important parameter. Recall \(\) is defined such that the rank of the Einsum scales as \((d^{}).\) Einsums with \(<1\) introduce information bottlenecks in the model by preventing the linear layers from accessing information from all the feature dimensions. The smaller \(\) is, the more severe this effect. In Figure 4 (middle), we show that small values of \(\) indeed lead to worse scaling laws. This observation further narrows down our search to the subspace with \(=0\) and \(=1,\) i.e. the space of full-rank BTT matrices.

**Any Full-Rank BTT Scales Similarly as Dense.** The \(=0\) and \(=1\) subspace contains the Monarch matrices and its generalization BTT matrices3. From a computational perspective, a primary distinguishing factor among these structures is how close they resemble a dense matrix, which we characterize by their compute intensity \([0,1)\) defined so that \(F/d=(d^{})\). \(\) is large whenever their exists large values (close to \(1\)) in the remaining allowed entries \((_{},_{},_{},_{ },_{}).\) In Figure 4 (right), we show that, somewhat surprisingly, \(\) has minimal effect on the scaling laws of these structures. Structures with different \(\) have almost indistinguishable scaling laws compared to each other and dense matrices, which has \(=1\). This result shows that while dense matrices perform well compared to the vast majority of possible Einsums, their good performance does not arise from being dense, but rather from not sharing parameters and being full-rank.

**Reconciling with Results from Prior Work.** Our findings do not mean that structured matrices cannot outperform dense in other settings. Rather, they highlight that the relative performance between structures depends on what resource is controlled, as prior work has shown that low rank, Tensor

Figure 4: **The taxonomy parameters \((,)\) explain differences in the scaling laws. (Left): parameter sharing (\(>0\)) leads to worse scaling. (Middle): among structures without parameter sharing (\(=0\)), full-rank structures (\(=1\)) scale better than low-rank structures (\(<1\)). (Right): in the \((=0,=1)\) subspace, various structures have nearly indistinguishable scaling laws compared to dense matrices, suggesting that not implementing parameter sharing and being full-rank are the necessary and sufficient conditions for a compute-efficient linear layer for GPT-2.**Train, Monarch, and BTT can significantly outperform dense in other settings such as controlling for memory, model size, or inference compute [5; 6; 30; 17; 19; 15], rather than training compute. For example, when training dataset size instead of training compute is the primary bottleneck, such as on conventional vision datasets like CIFAR-10 and ImageNet, structured matrices have shown considerable advantage over dense as a function of model size and inference compute [6; 19; 14]. In those settings, Qiu et al.  observe the benefits of structure likely arise through enabling computationally efficient wider layers.

### Our Findings Generalize to Other Settings

We now test if our findings derived from the GPT-2 experiments can generalize to other settings. We evaluate on the following two additional tasks where there is sufficient data to measure the compute-optimal scaling laws without repeating training data. We provide additional experiment details in Appendix D.

**Autoregressive Pixel Modeling on CIFAR-5M.** We train transformers to autoregressively predict the RGB pixel values of images in the CIFAR-5M dataset , downsampled to \(8 8 3\) resolution. Figure 5 (top row) shows qualitatively the same results as our GPT-2 experiments, where \(\) and \(\) have the most significant impact on the scaling laws, while varying \(\) yield only slight variations. In this particular case, having \(=0.75\) (BTT with BTT-rank scaling as \(d^{1/4}\)) is better than having \(=0.5\) (Monarch matrices). One notable trend in this setup is that most Einsums, regardless of \(\) or \(\), outperform dense at small scales. We hypothesize this improved performance is due to Einsums having larger embedding dimensions than dense layers for a fixed parameter budget and can thus preserve more information about the input pixels at smaller model sizes.

**MLP Regression on Synthetic Data.** We train MLPs on a synthetic regression dataset where the target is a scalar-valued function defined by a large randomly initialized MLP, similar to the student-teacher setting in . In Figure 5 (bottom row), we observe qualitatively the same results as in the GPT-2 experiments.

Together, these additional results suggest there is some degree of universality associated with our findings on the effect of \(,,\) and \(\) on the compute-optimal scaling laws of neural networks that use Einsums in place of dense matrices.

Figure 5: **Our findings about the effect of \((,,)\) on the scaling laws generalize to other settings. (Top row) Transformers trained with cross-entropy for autoregressive pixel generation on CIFAR-5M. (Bottom row) MLP trained with mean-squared-error loss on synthetic data generated by a large and randomly initialized MLP.**

## 5 Structured Mixture of Experts

In Section 4, we identified that Einsums with \(=0\) and \(=1\) perform the best, and \(>0\) or \(<1\) lead to worse-than-dense performance. The most impactful parameter on the scaling laws is \(\), which measures how many parameters an Einsum has compared to the FLOPs for an MVM. Einsums that learn one parameter per FLOP perform significantly better than those that learn less than one parameter per FLOP. Therefore, a natural question arises: can we design structures that learn more than one parameter per FLOP, which we might expect will have even better scaling laws? Doing so requires that not all parameters are used in an MVM, which necessitates a sparse Mixture-of-Experts (MoE) like architecture [22; 7; 12]. Furthermore, we would like the structure to be full-rank, i.e. \(=1\). In the following section, we introduce such a structure and demonstrate significant improvement over dense layers and the standard MoE architecture for training GPT-2.

### More Parameters than FLOPs via Mixture of Experts

One natural candidate for constructing such a layer via an Einsum is to turn a BTT with BTT-rank \(E\), which involves a sum over the rank index \(=1,,E\) :

\[Y_{}=_{}B_{}A_{ }X_{}\] (2)

into a \(k\)-sparse sum:

\[Y_{}=_{}g_{}B_{ }A_{}X_{}}_{},\] (3)

where \(^{E}\) is a \(k\)-sparse vector so that only \(k\) out of \(E\) terms need to be computed. We interpret \(k\) as the number of active experts and \(E\) as the total number of experts. We compute \(\) via a softmax over the top-\(k\) entries of the logits e produced by a (dense) linear gating function \(=(X)^{E}\). There is no need to make this gating function structured because its cost is negligible. We choose \(_{}=_{}=_{ }=_{}=1/2\) so that each expert is full-rank. We follow the common practice of using \(k=2\). The resulting BTT-MoE layer is a BTT with BTT-rank \(2\) (sum of two Monarch matrices) with input-dependent parameters. It is similarly straightforward to construct structured MoE from other structures by sparsifying the sum over \(\) with a gate. We use the load-balancing loss to encourage equal utilization of all experts [22; 7; 23].

In contrast to the standard MoE architecture used in transformer language models, which uses a sparse MoE for each entire feed-forward network (FFN) [22; 7; 12]:

\[=_{i=1}^{E}g_{i}_{i}^{}(_{i}^{})}_{},\] (4)

Figure 6: **BTT Mixture-of-Experts has significantly better compute-optimal scaling laws than dense GPT-2 and its standard MoE variant. (Left): Compute-optimal frontier with \(8\). (Middle): \(8\) experts compute multiplier of BTT-MoE and standard MoE relative to dense as a function of FLOPs required by the dense model to achieve the same loss. (Right): Increasing the number of experts improves computational savings. Mean and standard deviation of the compute multiplier over all compute observations for \(8\) and \(16\) experts.**BTT-MoE learns an MoE in every single linear layer of the model (except the language modeling head) and treats them equally, including the projection matrices \(},},},}\) in the attention blocks . It learns more fine-grained routing decisions among the experts, with \((E(E-1))^{6M}\) possible combinations of the experts in a transformer with \(M\) blocks, compared to \((E(E-1))^{M}\) for the standard MoE architecture.

### Compute Efficiency Gains

In Figure 6, we show GPT-2 with BTT-MoE achieves better compute-optimal scaling laws compared to the dense model as well as the standard MoE, with \(k=2\) and \(E\{8,16\}\). BTT-MoE consistently outperforms the standard MoE and the dense model. We quantify and compare the compute efficiency gains of BTT-MoE and standard MoE over dense models via the _compute multiplier_. A model with a compute multiplier of \(\) means with \(C\) training FLOPs it achieves the same loss as a dense model with \( C\) training FLOPs. In Figure 6, we show BTT-MoE is significantly more compute-efficient than the standard MoE for both \(E=8\) and \(E=16\). In particular, with \(E=16\) experts, BTT-MoE achieves a compute multiplier of \(=5.3_{ 0.3}\), compared to \(=4.1_{ 0.3}\) for a standard MoE.

### Effect of Structures

In Figure 7 we show that replacing BTT-MoE with a sparse (\(k=2\)) sum of low-rank matrices (low-rank-MoE) or dense matrices (dense-MoE) also yields a nontrivial compute multiplier (\( 2\)) over the dense model, but is significantly less effective than BTT-MoE or even the standard MoE.

While the poor relative performance of low-rank-MoE is expected, this result shows that in addition to \(=0\) and \(=1\), \(<1\) is a desirable property for the base structure in a structured MoE. Using a dense structure with \(=1\) means the experts are not complementary to each other since each one is able to represent the entire space of dense matrices.

## 6 Scaling Optimization for Einsums

As prior work  has shown, the optimal initialization scales and learning rate depend heavily on the structure of the linear layers and are critical for successfully training models with structured layers. Fortunately, the theory of the Maximal Update Parameterization (\(\)P) [26; 27; 28] and its application

Figure 8: **Einsums trained with \(\)P achieve lower error and share an optimal base learning rate. We plot test error of 4 layered MLP models on CIFAR-10, where the hidden layers are Einsums. We vary the model widths in 64, 256, 1024 and 4096. The naive approach uses a global learning rate independent of width or structure and initializes the Einsums parameters with unit variance.**

to various structured matrices in Qiu et al.  provides a template on how to reason about the appropriate initialization and learning rate scales for all Einsums.

In short, \(\)P states that for a dense matrix \(^{d_{} d_{}}\), the optimal initialization standard deviation scales as \(=(},d_{})/d_{}^{ 2}})\) and its learning rate as \(=(1/d_{})\) if using Adam. Furthermore, Qiu et al.  shows we can apply \(\)P to structured matrices as long as we can cast the MVM as a series of batched matrix multiplications (BMM). As shown in Appendix C, we can indeed cast any Einsum as a series of BMMs and show that \(d_{}^{}=d_{},d_{}^{}=d_{ }d_{}d_{}\), \(d_{}^{}=d_{}d_{}d_{}\) and \(d_{}^{}=d_{}\). As a result, we can compute the optimal scaling of \(_{},_{},^{}\), and \(^{}\). In particular, for Adam we have \(^{}=(}d_{}})\) and \(^{}=(}d_{}d_{}})\).

Figure 8 shows using \(\)P leads to a stable optimal learning rate and better performance compared to naively using a constant global learning rate and unit initialization variance. This property allows us to transfer the learning rate between structures and model sizes, saving substantial compute for hyperparameter tuning. For \(\)P, the learning rate refers to that used by a dense model with width \(64\), which we transfer to the Einsum models of different widths and structures via the scaling rule identified earlier (see details in Appendix C).

Finally, we discuss in Appendix C an alternative way to reason about the optimal learning rates of Einsums via Riemannian SGD (RSGD) . We analyze the effective learning rate prescribed by RSGD at initialization for asymptotically large Einsums and find it often agrees with the \(\)P prescription derived above.

## 7 Conclusion

Going beyond prior works that study hand-crafted structured matrices on a case-by-case basis, we introduce a continuous parameterization over the space of all structured matrices expressible as Einsums. Using this parameterization, we measure and compare the compute-optimal scaling laws of a wide range of known and novel structures, with the following key takeaways:

* _Compute-optimal scaling laws of Einsums are primarily governed by the parameter-sharing exponent \(\) and the rank exponent \(\)_. Across tasks, we find all full-rank Einsums without parameter sharing (i.e. full-rank BTTs) scale similar to dense, while the remaining vast majority of Einsums consistently underperform dense as \(\) increases or \(\) decreases.
* _Existing structured matrices do not significantly outperform dense in the compute-optimal setting._ While low rank, Tensor-Train, Monarch, and BTT have shown advantages over dense in other settings, such as controlling for memory or model size, they generally perform worse or similar to dense when controlling for training compute. However, there are also instances in the compute-optimal regime where a full-rank structured representation with no parameter sharing can outperform dense layers. This advantage is most likely due to the ability to make wider structured layers for the same computational budget as narrower dense layers, which can particularly benefit smaller vision models, as we show on CIFAR-5M.
* \(\)_P prescribes effective initialization and learning rate scaling for Einsums._ Breaking an Einsum down to a sequence of batched matrix multiplications, we extend prior work on structure-aware initialization and learning rate based on \(\)P to arbitrary Einsums.
* _MoE over structured matrices is more efficient than standard MoE over entire FFNs._ By replacing every single dense linear layer with a sparse sum of structured matrices like BTT, compared to standard MoE which operates over entire FFNs, we create a more efficient MoE architecture, achieving over \(5\) savings in compute on language modeling relative to dense. Scaling and improving the proposed structured MoE architecture are exciting directions for future work.

**Acknowledgements:** We thank Alan Amin, Nate Gruver, and Hoang Phan for helpful discussions. This work is supported by NSF CAREER IIS-2145492, NSF CDS&E-MSS 2134216, NSF HDR-2118310, BigHat Biosciences, Capital One, and an Amazon Research Award.