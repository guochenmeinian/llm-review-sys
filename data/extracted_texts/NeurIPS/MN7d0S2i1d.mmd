# Sample and Computationally Efficient

Robust Learning of Gaussian Single-Index Models

 Puqian Wang

Department of Computer Science

University of Wisconsin, Madison

pwang333@wisc.edu

&Nikos Zarifis

Department of Computer Science

University of Wisconsin, Madison

zarifis@wisc.edu

&Ilias Diakonikolas

Department of Computer Science

University of Wisconsin, Madison

ilias@cs.wisc.edu

&Jelena Diakonikolas

Department of Computer Science

University of Wisconsin, Madison

jelena@cs.wisc.edu

###### Abstract

A single-index model (SIM) is a function of the form \((^{*})\), where \(:\) is a known link function and \(^{*}\) is a hidden unit vector. We study the task of learning SIMs in the agnostic (a.k.a. adversarial label noise) model with respect to the \(_{2}^{2}\)-loss under the Gaussian distribution. Our main result is a sample and computationally efficient agnostic proper learner that attains \(L_{2}^{2}\)-error of \(O()+\), where \(\) is the optimal loss. The sample complexity of our algorithm is \((d^{ k^{*}/2}+d/)\), where \(k^{*}\) is the information-exponent of \(\) corresponding to the degree of its first non-zero Hermite coefficient. This sample bound nearly matches known CSQ lower bounds, even in the realizable setting. Prior algorithmic work in this setting had focused on learning in the realizable case or in the presence of semi-random noise. Prior computationally efficient robust learners required significantly stronger assumptions on the link function.

## 1 Introduction

Single-index models (SIMs)  are a classical supervised learning model characterized by hidden low-dimensional structure. The term SIM refers to any function \(f\) of the form \(f()=()\), where \(:\) is the link (or activation) function and \(^{d}\) is the hidden vector. In most settings of interest, the link function is assumed to satisfy certain regularity properties. Indeed, for an arbitrary function, learnability is information-theoretically impossible.

The efficient learnability of SIMs has been the focus of extensive investigation for several decades. For example, the special case where \(\) is the sign function corresponds to Linear Threshold Functions whose study goes back to the Perceptron algorithm . Classical early works  studied the efficient learnability of SIMs for monotone and Lipschitz link functions. They showed that a gradient method efficiently learns SIMs for any distribution on the unit ball. More recently, there has been a resurgence of research on the topic with a focus on first-order methods. Indeed, the non-convex optimization landscape of SIMs exhibits rich structure and has become a useful testbed for the analysis of such methods.  showed that SGD efficiently learns SIMs for the case that \(\) is the ReLU activation and the distribution is Gaussian.  showed that gradient descent succeeds for the phase retrieval problem, corresponding to \((t)=t^{2}\) or \((t)=|t|\)More recently, a line of work  studied the efficient learnability of SIMs going significantly beyond the monotonicity assumption. Specifically,  developed efficient gradient-based SIM learners for a general class of -- not necessarily monotone -- link functions under the Gaussian distribution. Roughly speaking, these works show that the complexity of learning SIMs is intimately related to the Hermite structure of the link function (roughly, the smallest degree non-zero Hermite coefficient). The results of the current paper are most closely related to the aforementioned works.

All the aforementioned algorithmic results succeed in the realizable model (i.e., with clean labels) or in a few cases in the presence of random label noise. The focus of this work is on learning SIMs in the challenging _agnostic_ (or adversarial label noise) model . In the agnostic model, no assumptions are made on the observed labels and the goal is to compute a hypothesis that is competitive with the _best-fit_ function in the class. The algorithmic study of agnostically learning SIMs is not new. A recent line of work  has given efficient agnostic SIM learners (typically based on first-order methods) with near-optimal error guarantees under natural distributional assumptions. _The key difference between prior work and the results of the current paper is in the assumptions on the link function._ Specifically, prior robust learners succeed for (a subclass of) _monotone and Lipschitz link functions._ In contrast, this work develops robust learners in the more general setting of .

In order to precisely describe our contributions, we require the definition of the agnostic learning problem for Gaussian SIMs. Let \(\) be a distribution of labeled examples \((,y)^{d}\) whose \(\)-marginal is the standard Gaussian, and let \(_{2}^{}()_{(,y) }[(()-y)^{2}]\) be the \(L_{2}^{2}\) (or squared) loss of the hypothesis \(h()=()\) with respect to \(\). Given i.i.d. samples from \(\), the goal is to compute a hypothesis with squared error competitive with \(\), where \(\) is the best attainable \(L_{2}^{2}\)-error by any function in the target class.

**Problem 1.1** (Robustly Learning Gaussian SIMs).: _Let \(\) be a distribution of labeled examples \((,y)^{d}\) whose \(\)-marginal is \(_{}=(0,_{d})\) and \(y\) is arbitrary. We say that an algorithm is a \(C\)-approximate proper SIM learner, for some \(C 1\), if given \(>0\) and i.i.d. samples from \(\), the algorithm outputs a vector \(}^{d-1}\) such that with high probability it holds \(_{2}^{}(}) C\,+\), where \(:=_{2}^{}(^{*})\) and \(^{*}_{^{d-1}}\,_{2}^{}()\)._

First, note that Problem 1.1 does not make realizability assumptions on the distribution \(\). That is, the labels are allowed to be arbitrary and the goal is to be competitive against the best-fit function in the class. Second, our focus is on obtaining efficient learners that achieve a _constant factor approximation_ to the optimum loss -- independent of the dimension \(d\). The reason we require a constant factor approximation, instead of optimal error of \(+\), is the existence of computational hardness results ruling out this possibility. Specifically, even if the link function is the ReLU, there is strong evidence that any algorithm achieving error \(+\) in the above setting requires \(d^{(1/)}\) time .

Recent work  gave efficient, constant-factor robust learners, for the special case of Problem 1.1 where the link function lies in a proper subclass of monotone and Lipschitz functions. In this work, we obtain a broad generalization of these results to a much more general class of link functions, defined below.

We now proceed to formalize the assumptions on the link function. Let \(:\) be a real-valued function that admits the Hermite decomposition \((z)=_{k 0}c_{k}_{k}(z),\) where \(c_{k}=_{z(0,1)}[(z)_{k}(z)]\) and \(_{k}\) is the normalized probabilist's Hermite polynomial, defined by

\[_{k}(z)=}{}}{2} ^{k}}{z^{k}}-}{2}.\]

We make the following assumptions.

**Assumption 1** (Family of Link Functions).: _Suppose that \(\) is normalized, namely \(_{z(0,1)}[^{2}(z)]=_{k 0}c_{k}^{2}=1\). We assume the following:_

1. _The first non-zero Hermite coefficient has degree_ \(k^{*}\) _and is prominent:_ \(c_{k^{*}}\) _is an absolute constant that is bounded away from zero._
2. _The fourth moment of_ \((z)\) _is bounded:_ \(_{z(0,1)}[^{4}(z)] B_{4}<\)_._
3. _The second moment of the derivative of_ \((z)\) _is bounded:_ \(_{z(0,1)}[(^{}(z))^{2}]=_{k k^{*}} kc_{k}^{2} C_{k^{*}}\)_, where_ \(C_{k^{*}}\) _is an absolute constant whenever_ \(k^{*}\) _is an absolute constant._

_The parameter \(k^{*}\) is known as the information exponent of \(\)._

The information exponent \(k^{*}\) can be viewed as a complexity measure of the link function. Specifically, ReLU activations correspond to \(k^{*}=1\). The same holds for the class of bounded activations considered in . The link functions used in phase retrieval have \(k^{*}=2\).

We note that Assumption 1 is (at least) as general as those used in  -- which focused on the realizable setting. Comparing against previous constant-factor agnostic learners, Assumption 1 strongly subsumes the class of "bounded activations" . In particular, it is easy to see that there exist functions satisfying Assumption 1 with constant \(k^{*}\) that are far from monotone. See Appendix A for a detailed justification.

In prior work, , building on , gave a sample-efficient gradient method for learning SIMs under Assumption 1_in the realizable setting_. The sample complexity of their method is \((d^{k^{*}/2}+d/)\). This sample upper bound essentially matches known lower bounds in the Correlational Statistical Query (CSQ) model .

This discussion leads to the following question, which served as the motivation for the current work:

_Is there an efficient constant-factor agnostic learner_

_for Gaussian SIMs under Assumption 1?_

As our main contribution, we answer this question in the affirmative. Interestingly, our algorithm also relies on a gradient-method (Riemannian optimization over the sphere) following a non-trivial initialization step. We emphasize that this is the first polynomial-time constant-factor agnostic learner for this task under Assumption 1.

Specifically, we establish the following result (see Theorem 3.5 for a more detailed statement).

**Theorem 1.2** (Main Result, Informal).: _There exists an algorithm that draws \(n=_{k^{*}}(d^{ k^{*}/2}+d/)\) labeled samples, runs in \((n,d)\) time, and outputs a weight vector \(}^{d-1}\) that with high probability satisfies \(_{2}^{o}(}) C\ +\), where \(C=O(C_{k^{*}})\)._

Theorem 1.2 gives the first sample and computationally efficient robust learner for Gaussian SIMs under Assumption 1. This generalizes the algorithm of  to the agnostic setting and nearly matches the aforementioned CSQ lower bounds (our algorithm fits the CSQ framework). It is worth pointing out that, while more efficient (non-CSQ) algorithms have been developed for the realizable case , these algorithms provably fail in the agnostic regime. Finally, we remark that very recent work  developed an efficient SIM learner and a nearly matching SQ lower bound in a model that allows for some forms of label noise. Importantly, their model does not capture the adversarial label noise studied here. More specifically, the algorithms developed in this prior work  fail in the agnostic setting. See Appendix B for a detailed discussion.

### Preliminaries

For \(n_{+}\), let \([n]\{1,,n\}\). We use lowercase bold characters to denote vectors and uppercase bold characters for matrices and tensors. For \(^{d}\) and \(i[d]\), \(_{i}\) denotes the \(i\)-th coordinate of \(\), and \(\|\|_{2}(_{i=1}^{d}_{i}^{2})^{1/2}\) denotes the \(_{2}\)-norm of \(\). We use \(\) for the inner product of \(,^{d}\) and \((,)\) for the angle between \(,\). We slightly abuse notation and denote by \(_{i}\) the \(i^{}\) standard basis vector in \(^{d}\). We use \(\{\}\) to denote the indicator of a statement \(\). We use \(_{d}\) to denote the \(d\)-dimensional standard Gaussian distribution, i.e., \(_{d}=(0,)\). We use \(_{d}\) to denote the centered unit ball in \(^{d}\) and denote the unit sphere in \(^{d}\) by \(^{d-1}\). We use \(_{_{d}}()\) to denote the projection operator that projects a vector to the unit ball.

Given \(^{d_{1} d_{2}}\) and a left singular vector \(^{d_{1}}\) of \(\), we denote the corresponding singular value of \(\) by \(()\). In addition, we use \(_{1}_{2}_{\{d_{1},d_{2}\}}\) to denote the singular values of a matrix \(^{d_{1} d_{2}}\). We use \(_{k}\) to denote the set of all possible permutations of \(k\) distinct objects. Given a unit vector \(\), we define \(_{^{}}-^{}\) to be the projection matrix that maps a vector \(\) to its component that is orthogonal to \(\), i.e., \(_{^{}}=^{}\).

Given a vector \(^{d}\), the (normalized) Hermite multivariate tensor is defined by :

\[(_{k}())_{i_{1},,i_{k}}:=(! _{d}!}{k!})^{1/2}_{_{1}}(_{1}) _{_{d}}(_{d}),\;\;_{j}=_{l=1}^{ k}\{i_{l}=j\},\, j[d].\]

We use the standard \(O(),(),()\) asymptotic notation. We use \(()\) to omit polylogarithmic factors in the argument. We write \(E F\) for two non-negative expressions \(E\) and \(F\) to denote that _there exists_ some positive universal constant \(c>0\) such that \(E c\,F\).

### Technical Overview

Our technical approach consists of two main parts: (1) new results for tensor PCA, which allow us to obtain an initial parameter vector \(^{0}\) that is nontrivially aligned with the target \(^{*}\) and (2) a structural "alignment sharpness" result, which we use to argue that a variant of Riemannian minibatch stochastic gradient descent on a sphere applied to a "truncated square loss" (defined below) converges geometrically fast. In proving these results, we review elementary tensor algebra and basic properties of Hermite polynomials, and prove several structural results for Hermite polynomials that are instructive and may be useful to non-experts entering this area.

We now highlight some of the key ideas used in our work.

**Initialization via Tensor PCA** For our optimization algorithm to work, a warm start ensuring nontrivial alignment between the initial vector \(^{0}\) and the target vector \(^{*}\), as measured by \(^{0}^{*}\), is essential. In particular, a consequence of our results in Claim 3.1 and Lemma 3.2 is that \(^{0}^{*}=(1)\) is required to deal with the highly corruptive agnostic noise. Unfortunately, if we were to select \(_{0}\) by drawing uniformly random samples from the sphere, exponentially many in \(d\) such samples would be needed to ensure that with constant probability at least one of the sampled vectors \(_{0}\) is such that \(^{0}^{*}=(1)\), due to standard results on concentration of measure on the (unit) sphere.

Perhaps surprisingly, we prove that the tensor PCA method developed in  when applied to our problem with \(O(d^{ k^{*}/2})\) samples1 ensures that \(^{0}^{*} 1-\{1/k^{*},1/2\}\). The reason that this result is surprising is that the method in  was developed to solve the following problem: given a \(k\)-tensor of the form

\[=^{ k}+,\] (PCA-S)

where \(\) is a \(k\)-tensor with i.i.d. standard Gaussian entries and \(>0\) a "signal strength" parameter, recover the planted (signal) vector \(\). This'single-observation' model is equivalent (in law) to the following'multi-observation' model (): given \(n\) i.i.d. copies \(^{(i)}=^{}^{ k}+^{(i)}\) with \(^{}=/\), recover \(\) using the empirical estimation:

\[}=^{}^{ k}+(1/n)_{i=1}^{n} ^{(i)}.\] (PCA-M)

In our setting, we wish to recover a vector \(^{*}\) (up to some constant alignment error) for the \(k\)-Chow tensor \(_{k}=_{(,)}[y_{k}()]\), which can be decomposed as

\[_{k}=_{d}}{} _{j k^{*}}c_{j}_{j}(),^{*  j}_{k}()+, )}{}[(y-(^{*} ))_{k}()].\] (1)

The first term in this decomposition can be viewed as the "signal" \(k\)-tensor. The second term represents noise, which, due to \(y\) being potentially arbitrary, cannot be assumed to be Gaussian. Thus, previously developed techniques for tensor PCA, which crucially rely on the "Gaussianity" of the noise term, do not apply here.

To obtain our result, we first argue that for any \(k k^{*}\), the top left singular vector \(^{*}\) of the \(k\)-Chow tensor \(_{k}\) unfolded into a matrix of roughly equal dimensions carries a "strong signal" about the target vector \(^{*}\): its associated singular value scales with \(c_{k}\) whenever \(c_{k}=(})\) and it has a nontrivial alignment with the vectorized version of the \(l\)-tensor \(^{* l}\) for \(l= k/2\) (Lemma 2.2).

To prove the desired alignment result using the empirical estimate of the matrix-unfolded \(k\)-Chow tensor \(_{k}\), we rely on the application of a matrix concentration inequality obtained very recently in . This requires a rather technical argument utilizing Gaussian hypercontractivity of multivariate polynomials of bounded degree, which we show characterizes the different "variance-like" quantities associated with the empirical estimate of (the matrix-unfolded) \(_{k}\), for which we apply the aforementioned matrix concentration (see Lemma 2.4 and its proof).

Another intriguing aspect of our initialization result is that it is possible to use it directly to obtain an \(O(}+)\)-error solution. In particular, in the realizable case studied in , where \(=0\), this result directly leads to error \(O()\) in a sample and computationally efficient manner, with a rather simple algorithm and sample complexity comparable to .

**Optimization on a Sphere** The second key ingredient in our work is a structural result, stated in Lemma 3.3, which ensures that the gradient field (Riemannian gradient of a truncated loss) guiding the steps of our algorithm (which can be interpreted as a Riemmanian minibatch SGD on a sphere) negatively correlates with \(^{*}\) to a significant extent. This property can be viewed as the considered gradient field, associated with the \(L^{2}_{2}\) loss truncated to only contain the first nonzero term in the Hermite expansion of the activation function, containing a strong "signal" that can "pull" the algorithm iterates towards the target \(O()+\) solutions. We rely on this property to argue that as long as our algorithm (initialized using the tensor PCA method described above) does not have as its iterate a vector with \(O()+\) loss, the distance between the iterate vector and the target vector must contract. As a consequence, this algorithm converges in \(O((1/))\) iterations.

This argument parallels the line of work  on addressing learning of single-index models by proving structural, optimization-theory local error bounds that bound below the growth of a loss function outside the set of target solutions. Conceptually, the local error bounds from this line of work all have an intuitive interpretation as showing existence of a strong "signal" in the problem that can be used to guide algorithm updates towards target solutions. However, the methodology by which our structural result is obtained is entirely different, as it crucially relies on properties of Hermite polynomials, which were not considered in this past work.

## 2 Initialization Procedure

In this section, we show how to get an initial parameter vector \(^{0}\) such that \(^{0}^{*}=1-_{0}\) for some small constant \(_{0}\). The main technique is a tensor PCA algorithm that finds the principal component of a noisy degree-\(k\)-Chow tensor for any \(k k^{*}\), as long as \( c_{k}^{2}\). Such a degree-\(k\) Chow tensor is defined by \(_{k}=_{(,y)}[y_{k}()]\), and we denote its noiseless counterpart by

\[_{k}^{*}=_{d}}{}[ (^{*})_{k}()]= _{d}}{}_{j k ^{*}}c_{j}(_{j}(),^{* j}) _{k}().\]

Furthermore, let us denote the difference between \(_{k}\) and \(_{k}^{*}\) by

\[_{k}_{k}-_{k}^{*}=,y)}{}[(y-(^{*})) _{k}()].\]

Note that since \(_{k}()\) is a symmetric tensor for any \(\), all \(_{k},_{k}^{*}\), and \(_{k}\) are symmetric tensors.

We use the following matrix unfolding operator that maps a \(k\)-tensor \(\) to a matrix in \(^{d^{l} d^{k-l}}\):

\[_{(l,k-l)}()_{i_{1}+(i_{2}-1)d++(i_{l}-1)d^{l-1},j_ {1}++(j_{k-l}-1)d^{k-l-1}}()_{i_{1},i_{2},,i_{ l},j_{1},,j_{k-l}}\]

for all \(i_{1},,i_{l},j_{1},,j_{k-l}[d]\). We also define the'vectorize' operator and 'tensorize' operators, which map a vector \(^{d^{l}}\) to an \(l\)-tensor for any integer \(l\), and vice versa. In detail,

\[()_{i_{1},,i_{l}}_{i_{1}+(i_{2}-1)d++(i_{l}-1)d^{l-1}},\  i_{1},,i_{l}[d],\] \[(^{ l})_{i_{1}+(i_{2}-1)d++(i_ {l}-1)d^{l-1}}_{i_{1}}_{i_{2}}_{ i_{l}},\  i_{1},,i_{l}[d].\]

Finally, given a vector \(^{d^{l}}\), we can also convert this vector to a matrix of size \(^{d d^{l-1}}\):

\[_{(1,l-1)}()_{i,j_{1},,j_{l-1}}=_{i+(j_{ 1}-1)d++(j_{l-1}-1)d^{l-1}},\  i,j_{1},,j_{l-1}[d].\]

Throughout this section, we take \(l:= k/2\). We leverage the tensor unfolding algorithm proposed in , which can succinctly be described as follows. First we unfold the degree-\(k\) Chow tensor to a matrix in \(^{d^{l} d^{k-l}}\) and find its top-left singular vector \(^{d^{l}}\). Then, we calculate the matrix \(_{(1,l-1)}()\), and output its top left singular vector \(\).

Our main result for initialization is that the output of Algorithm 1 significantly correlates with \(^{*}\).

**Proposition 2.1** (Initialization).: _Suppose Assumption 1 holds. Assume that \( c_{^{*}}^{2}/(64k^{*})^{2}\), and let \(_{0}=c_{^{*}}/(256k^{*})\). Then, Algorithm 1 applied to Problem 1.1 with \(k=k^{*}\) uses \(n=((k^{*})^{2}e^{k^{*}}^{k^{*}}(B_{4}/)d^{ k^{*}/2 }/(c_{^{*}}^{2})+1/)\) samples, runs in polynomial time, and outputs a vector \(^{0}^{d-1}\) such that \(^{0}^{*} 1-\{1/k^{*},1/2\}\)._

We remark here that Algorithm 1 can also be used to find an approximate solution of our agnostic learning problem; however the error dependence on \(\) is _suboptimal_, scaling with its square-root. For full details of this argument, included for completeness, see Proposition D.3 in Appendix D.

In the remainder of this section, we sketch the proof of Proposition 2.1, which relies on two main ingredients: (1) alignment of the left singular vectors \(\) of matrix-unfolded \(k\)-Chow tensor and the target vector \(^{*}\), which can be interpreted as the \(k\)-Chow tensor containing a strong "signal" about the target vector \(^{*}\), and (2) matrix concentration for the unfolded tensor, so that we can translate "population" properties of the \(k\)-Chow tensor to computable empirical quantities.

**Signal in the \(k\)-Chow Tensor** Our first observation is that for any left singular vector \(\) of \(_{(l,k-l)}(_{k})\), the singular value \(()\) is close to the inner product between \(\) and \((^{* l})\), where \(l= k/2\). Concretely, we have:

**Lemma 2.2**.: _Let \(\) be any left singular vector of \(_{(l,k-l)}(_{k})\). Then, \(|()-c_{k}((^{* l}))| }\)._

Proof Sketch of Lemma 2.2.: Recall that the singular value of the left singular vector \(\) satisfies

\[()=_{^{d^{k-l}},\|\|_{2}= 1}^{}_{(l,k-l)}(_{k})}{{}}_{^{k-l},\|\|_{2}=1} _{k},()( ),\]

where we used Fact C.1(2) in \((i)\). Since \(_{k}=_{k}^{*}+_{k}\), we further have

\[_{k},()( )=_{k}^{*},() ()+_{k},()().\]

We bound both terms above respectively. For the first term, plugging in the definition of \(_{k}^{*}\) and using the orthonormality property of Hermite tensors (Fact C.3) and basic tensor algebraic calculations,

\[_{k}^{*},()( )=c_{k}((^{* l}))((^{* k-l})).\] (2)

Next, for the second term, after applying Cauchy-Schwarz inequality, one can show that it holds

\[|_{k},()( )|}\|(( )())\|_{F}}.\] (3)

Combining Equation (2) and Equation (3), we get that the singular value of \(\) must satisfy

\[() _{^{d^{k-l}},\|\|_{2}=1 }c_{k}((^{* l}))(( ^{* k-l}))+}\] \[=c_{k}((^{* l}))+ },\] (4)

where in the equation above, we used the observation that as \(\|(^{* k-l})\|_{2}=\|^{* k-l}\|_{F}=1\), it holds \(_{^{d^{k-l}},\|\|_{2}=1}(( ^{* k-l}))=\|(^{* k -l})\|_{2}=1\). Since Equation (3) implies \(_{k},()( )-}\), similarly we have \(() c_{k}((^{* l}))-}\), completing the proof of Lemma 2.2.

Lemma 2.2 implies that the top left singular vector \(^{*}\) aligns well with \((^{* l})\). The full version of Corollary 2.3 is deferred to Corollary D.5.

**Corollary 2.3**.: _The top left singular vector \(^{*}^{d^{l}}\) of the unfolded tensor \(_{(l,k-l)}(_{k})\) satisfies \(^{*}(^{* l}) 1-(2})/c_{k}\)._

**Concentration of the Unfolded Tensor Matrix** Let us denote \(^{(i)}=_{(l,k-l)}(y^{(i)}_{k}(^{(i)}))\), for \(i[n]\) and \(}=_{i=1}^{n}^{(i)}\), which is the empirical approximation of \(=_{(l,k-l)}(_{k})=_{(l,k-l)}( _{(,y)}[y_{k}()])\). Though we showed in Corollary 2.3 that the top left singular vector \(^{*}\) of the population \(\) strongly correlates with the signal \((^{* l})\), we only have access to the empirical estimate \(}\) and its corresponding top left singular vector \(}^{*}\). Thus, to guarantee that \(}^{*}\) correlates significantly with \((^{* l})\) as well, we need to show that the angle between the \(^{*}\) and \(}^{*}\) is sufficiently small as long as we use sufficiently many samples. To this aim, we apply Wedin's theorem (Fact D.6). Wedin's theorem states that \(((^{*},}^{*}))\) can be bounded above by:

\[((^{*},}^{*}))\|- }\|_{2}/(_{1}-_{2}-\|-}\|_{2}),\]

where \(_{1}\) and \(_{2}\) are the top 2 singular values of \(\). We prove in Claim D.7 that \(_{1}-_{2}(c_{k}-8})/2 c_{k}\) under the assumption that \(} c_{k}\), hence \(_{1}-_{2}\) is bounded below by a constant. Thus, our remaining goal is to bound the operator norm such that \(\|-}\|_{2}_{0}\) where \(_{0}>0\) is a small constant. This can be accomplished by applying a recently obtained matrix concentration inequality from  (stated in Fact D.8), with additional technical arguments. Plugging the lower bound on the singular gap \(_{1}-_{2}\) and the upper bound on the operator norm \(\|-}\|_{2}\) back into Wedin's theorem (Fact D.6), we obtain the following main technical lemma of this subsection, whose proof can be found in Appendix D:

**Lemma 2.4** (Sample Complexity for Estimating the Unfolded Tensor Matrix).: _Let \(,_{0}>0\). Consider the unfolded matrix \(=_{(l,k-l)}(_{(,y)}[ y_{k}()])\) and its empirical estimate \(}(1/n)_{i=1}^{n}_{(l,k-l)}(y^{(i)} _{k}(^{(i)}))\), where \(\{(^{(i)},y^{(i)})\}_{i=1}^{n}\) are \(n=(e^{k}^{k}(B_{4}/)d^{k/2}/_{0}^{2}+1/)\) i.i.d. samples from \(\). Then, with probability at least \(1-(-d^{1/2})\), \(\|}-\|_{2}_{0}\). Moreover, if \(}^{*}\) is the top left singular vector of \(}\), then with probability at least \(1-(-d^{1/2})\),_

\[}^{*}(^{* l}) 1-}}-}{(c_{k}/2-4 })-_{0}}.\]

After getting an approximate top left singular vector \(}^{*}^{d^{l}}\) of \(_{(l,k-l)}(_{(,y)}[y_{k}()])\), the final piece of the argument is that finding the top left singular vector of the matrix \(_{(1,l-1)}(}^{*})\) completes the task of computing a vector \(\) that correlates strongly with \(^{*}\). Concretely, we have:

**Lemma 2.5**.: _Suppose that \(}^{*}(^{* l}) 1- _{1}\) for some \(_{1}(0,1/16]\). Then, the top left singular vector \(\) of \(_{(1,l-1)}(}^{*})\) satisfies \(^{*} 1-2_{1}\)._

Proof of Proposition 2.1Since \(} c_{k^{*}}/(64k^{*}) c_{k^{*}}/64\), choosing \(_{0}=c_{k^{*}}/(256k^{*}) c_{k^{*}}/256\) in Lemma 2.4, we obtain that using \(n=((k^{*})^{2}e^{k^{*}}^{k^{*}}(B_{4}/)d^{ k^{*}/2 }/(c_{k^{*}}^{2})+1/)\), it holds with probability at least \(1-(-d^{1/2})\) that

\[}^{*}(^{* l}) 1-}}-}{(c_{k}/2-4 })-_{0}} 1-}.\]

Then applying Lemma 2.5 with \(_{1} 1/(16k^{*}) 1/16\), we get that the output \(\) of Algorithm 1 satisfies \(^{*} 1-2_{1} 1-1/(8k^{*}) 1-\{1/k^{*},1 /2\}\), completing the proof. 

## 3 Optimization via Riemannian Gradient Descent

After obtaining \(^{0}\) from Algorithm 1, we run Riemannian minibatch SGD Algorithm 2 on the 'truncated loss' (see definition in Equation (5)). In the rest of the section, we first present the definition of the truncated \(L_{2}^{2}\) loss \(_{2}^{}\) and its Riemannian gradient and then proceed to proving that Algorithm 2 converges to a constant approximate solution in \(O((1/))\) iterations. Due to space constraints, omitted proofs are provided in Appendix E.

### Truncated Loss and the Sharpness property of the Riemannian Gradient

Instead of directly minimizing the \(L_{2}^{2}\) loss \(_{2}^{}\), we work with the following truncated loss that drops all the terms higher than \(k^{*}\) in the polynomial expansion of \(\):

\[_{2}^{}() 21-,y) }{}[()], { where }()=_{k^{*}}(), ^{ k^{*}}.\] (5)

Similarly, the noiseless surrogate loss is defined as

\[_{2}^{*}() 21-,y) }{}[(^{*})( )]=21-c_{k^{*}}(^{*})^{k^{*}}.\] (6)

It can be shown (using Fact C.1\((2)\)) that the gradient of the truncated \(L_{2}^{2}\) loss equals:

\[_{2}^{}()=-2,y) }{}[()y]=-2 {(,y)}{}k^{*}c_{k^{*}}y _{k^{*}}(),^{ k^{*}-1} ,\] (7)

while for the gradient of the noiseless \(L_{2}^{2}\) loss we have

\[_{2}^{*}()=-2,y) }{}k^{*}c_{k^{*}}(^{*} )_{k^{*}}(),^{ k ^{*}-1}.\] (8)

Recall that \(_{^{}}-^{}\). Then the Riemannian gradient of the \(L_{2}^{2}\) loss \(_{2}^{}\), denoted by \(()\) is

\[()_{^{}}( _{2}^{}())=-2,y) }{}k^{*}y_{^{}} _{k^{*}}(),^{ k^{*}-1}.\] (9)

Similarly, the Riemannian gradient of the noiseless \(L_{2}^{2}\) loss \(_{2}^{*}\) is defined by

\[^{*}()_{^{}}( _{2}^{*}())=-2,y) }{}k^{*}(^{*})_{ ^{}}_{k^{*}}(), ^{ k^{*}-1}.\] (10)

We first show that \(^{*}()\) carries information about the alignment between vectors \(\) and \(^{*}\).

**Claim 3.1**.: _For any \(^{d-1},\) we have \(^{*}()=-2k^{*}c_{k^{*}}(^{*})^{k^ {*}-1}(^{*})^{}.\)_

Let us denote the difference between the noisy and the noiseless Riemannian gradient by \(()\), i.e., \(()()-^{*}()=-2 \,_{(,y)}[(y-(^{*} ))_{^{}}()].\) We next show that the norm of \(()\) and the inner product between \(()\) and \(^{*}\) are both bounded.

**Lemma 3.2**.: _Let \(()=()-^{*}()\) as defined above. Then, \(\|()\|_{2} 2k^{*}c_{k^{*}}}\) and \(|()^{*}| 2k^{*}c_{k^{*}}}\|( ^{*})^{}\|_{2}\)._

We are now ready to present the main structural result of this section.

**Lemma 3.3** (Sharpness).: _Assume \( c/(4e)^{2}\) for some small absolute constant \(c<1\). Let \(^{d-1}\) and suppose that \(^{*} 1-1/k^{*}\). Let \(:=(,^{*}).\) If \( 4e}\), then \(()^{*}-(1/2)\|^{*}()\|_{2}.\)_

Proof.: We start by noticing that by Claim 3.1, the noiseless gradient satisfies the following property:

\[^{*}()^{*}=-2k^{*}c_{k^{*}}( ^{*})^{k^{*}-1}\|(^{*})^{}\|_{2}^{2}=-\| ^{*}()\|_{2},\]

where we used that since \(\|\|_{2}=\|^{*}\|_{2}=1\), we have \(\|(^{*})^{}\|_{2}=.\) Furthermore, applying Lemma 3.2 we have the following sharpness property with respect to the \(L_{2}^{2}\) loss:

\[()^{*}=^{*}() ^{*}+()^{*}-(\|^{*}( )\|_{2}-2k^{*}c_{k^{*}}}).\] (11)

Observe that \((1-1/t)^{t-1} 1/e\) for all \(t 1\). Therefore, when \(^{*} 1-1/k^{*}\), we have

\[\|^{*}()\|_{2}=2k^{*}c_{k^{*}}(^{*}) ^{k^{*}-1} 2k^{*}c_{k^{*}}(1-1/k^{*})^{k^{*}-1} e^{-1}k^{*}c_ {k^{*}}.\]

Hence, when \( 4e}\) and \(^{*} 1-1/k^{*}\), we have \(\|^{*}()\|_{2} 4k^{*}c_{k^{*}}}\). Thus, as long as \( 4e}\), we have that \(()^{*}-\|^{*}( )\|_{2}.\)

### Concentration of Gradients

Define the empirical estimate of \(()\) as \(}()(1/n)_{i=1}^{n}k^{*}c_{k^{*}}y^{(i)} _{^{}}_{k^{*}}(^ {(i)}),^{ k^{*}-1}\). The following lemma provides the upper bounds on the number of samples required to approximate the Riemannian gradient \(()\) by \(}()\).

**Lemma 3.4** (Concentration of Gradients).: _Let \(^{*},^{d-1}\). Let \(}()\) be the empirical estimate of the Riemannian gradient. Furthermore, denote the angle between \(\) and \(^{*}\) by \(\), and denote \(=(k^{*}c_{k^{*}})^{2}e^{k^{*}}^{k^{*}}(B_{4}/)\). Then, with probability at least \(1-\), it holds \(\|}()-()\|_{2} {d/(n)}\), and \((}()-())^{*} ^{2}\)._

### Proof of Main Theorem

We proceed to the main theorem of this paper. It shows that using at most \((d^{ k^{*}/2}+d/)\) samples, Algorithm 2 (with initialization from Algorithm 1) generates a vector \(}\) such that \(_{2}^{}(})=O()+\) within \(O((1/))\) iterations.

**Theorem 3.5**.: _Suppose Assumption 1 holds. Choose the batch size of Algorithm 2 to be \(n=(C_{k^{*}}de^{k^{*}}^{k^{*}+1}(B_{4}/)/())\), and choose the step size \(=9/(40ek^{*}c_{k^{*}})\). Then, after \(T=O((C_{k^{*}}/))\) iterations, with probability at least \(1-\), Algorithm 2 outputs \(^{T}\) with \(_{2}^{}(^{T})=O(C_{k^{*}})+\). The total sample complexity of Algorithm 2 is \(N=((k^{*}/c_{k^{*}})^{2}e^{k^{*}}^{k^{*}}(B_{4}/)d^{ k ^{*}/2}+(C_{k^{*}}e^{k^{*}}^{k^{*}+2}(B_{4}/)))\)._

Proof Sketch of Theorem 3.5.: Suppose first that \((c_{k^{*}}/64k^{*})^{2}\), then by Claim E.7 we know that any unit vector (e.g., \(}=_{1}\)) is a constant approximate solution with \(_{2}^{}(})=O()\). Now suppose that \((c_{k^{*}}/64k^{*})^{2}\). Consider the distance between \(^{t}\) and \(^{*}\) after each update of Algorithm 2. By the non-expansive property of projection operators, we have

\[\|^{t+1}-^{*}\|_{2}^{2} =\|_{_{d}}(^{t}-}(^{t}))-^{*}\|_{2}^{2}\|^{t}- }(^{t})-^{*}\|_{2}^{2}\] \[=\|^{t}-^{*}\|_{2}^{2}+2}(^{t})(^{*}-^{t})+^{2}\|}(^{t})\|_{2}^{2}.\] (12)

Let \(_{t}=(^{t},^{*})\). For the chosen batch size, Lemma 3.4 implies

\[\|}(^{t})-(^{t})\|_{2}^{2} (k^{*}c_{k^{*}})^{2},(}(^{t})- (^{t}))^{*}^{2} _{t}.\]

Assume for now that \(_{t} 4e}+\), hence Lemma 3.3 applies. As \(}(^{t})(^{*}-^{t})=( }(^{t})-(^{t}))^{*}+(^{t})^{*}\), using Lemma 3.3, we get

\[}(^{t})(^{*}-^{t}) _{t}-(1/2)\|^{*}(^{*})\|_{2} _{t}.\] (13)

On the other hand, the squared norm term \(\|}(^{t})\|_{2}^{2}\) from Equation (12) can be bounded above by

\[\|}(^{t})\|_{2}^{2} 2\|}( ^{t})-(^{t})\|_{2}^{2}+2\|(^{t })\|_{2}^{2}(k^{*}c_{k^{*}})^{2}(+)+\|^{*}( )\|_{2}^{2}.\] (14)

Plugging Equation (13) and Equation (14) back into Equation (12), we get that w.p. at least \(1-\),

\[\|^{t+1}-^{*}\|_{2}^{2} \|^{t}-^{*}\|_{2}^{2}+2(-\|^{*}(^{t})\|_{2}/2)_{t}\] \[+^{2}((k^{*}c_{k^{*}})^{2}(+)+\|^ {*}()\|_{2}^{2}).\] (15)

Let us assume first that \(_{t}_{t-1}_{0}\) and \(_{t} 4e}+\), then we show that it holds \(_{t+1}_{t}\). Recall in Claim 3.1 it was shown that \(\|^{*}(^{t})\|_{2}=2k^{*}c_{k^{*}}(^{t} ^{*})^{k^{*}-1}_{t}\). Since \(^{0}\) is the initial parameter vector that satisfies \(^{0}^{*} 1-1/k^{*}\), by the inductive hypothesis it holds \(^{t}^{*} 1-1/k^{*}\) and hence \(1(^{t}^{*})^{k^{*}-1} 1/e\). Therefore, we further obtain \((2k^{*}c_{k^{*}}/e)_{t}\|^{*}(^{t})\|_{2}  2k^{*}c_{k^{*}}_{t}\). Therefore, using the inductive assumption that \(_{t} 4e}+\) and further noticing that \(\|^{t}-^{*}\|_{2}=2(_{t}/2)\), with step size \(=9/(40ek^{*}c_{k^{*}})\) we can further bound \(\|^{t+1}-^{*}\|_{2}^{2}\) in Equation (15) as:

\[\|^{t+1}-^{*}\|_{2}^{2}(1-(81/(320e^{2})))\|^{t}- ^{*}\|_{2}^{2}.\] (16)

This shows that \(_{t+1}_{t}\), hence completing the inductive argument. Furthermore, Equation (16) implies that after at most \(T=O((1/))\) iterations it must hold that \(_{T} 4e}+\), therefore, we can end the algorithm after at most \(O((1/))\) iterations. Applying Claim E.7 we know that this final vector \(^{T}\) has error bound \(_{2}^{}(^{T})=O(C_{k^{*}}(+))\)