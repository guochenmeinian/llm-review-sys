# Sequence-Augmented \(\mathrm{SE}(3)\)-Flow Matching For

# Sequence-Augmented \((3)\)-Flow Matching For

Conditional Protein Backbone Generation

Guillaume Huguet\({}^{1,2,3}\), James Vuckovic\({}^{1}\), Kilian Fatras\({}^{1}\), Eric Thibodeau-Laufer\({}^{1}\),

**Pablo Lemos\({}^{1}\), Riashat Islam\({}^{1}\), Cheng-Hao Liu\({}^{1,3,4}\), Jarrid Rector-Brooks\({}^{1,2,3}\), Tara Akhound-Sadegh\({}^{1,2,4}\), Michael Bronstein\({}^{1,5,6}\), Alexander Tong\({}^{1,2,3}\), Avishek Joey Bose\({}^{1,5}\)\({}^{1}\)**

Co-first and corresponding authors: {guillaume.huguet,james}@dreamfold.ai

\({}^{}\)Core contributor

\({}^{}\)Equal advising

\({}^{1}\)Dreamfold, \({}^{2}\)Universite de Montreal, \({}^{3}\)Mila, \({}^{4}\)McGill University, \({}^{5}\)University of Oxford, \({}^{6}\)Aithyra

###### Abstract

Proteins are essential for almost all biological processes and derive their diverse functions from complex \(3\)D structures, which are in turn determined by their amino acid sequences. In this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FoldFlow-\(2\)4, a novel sequence-conditioned \((3)\)-equivariant flow matching model for protein structure generation. FoldFlow-2 presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase diversity and novelty of generated samples--crucial for de-novo drug design--we train FoldFlow-2 at scale on a new dataset that is an order of magnitude larger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering. We further demonstrate the ability to align FoldFlow-2 to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FoldFlow-2 outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FoldFlow-2 makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody.

## 1 Introduction

Rational design of novel protein structures via generative modeling holds significant promise for accelerating computational drug discovery (Chevalier et al., 2017; Ebrahimi and Samanta, 2023). In particular, the ability to design proteins with a pre-specified functional property is arguably one of the principal tools in addressing global health challenges such as COVID-19 (Cao et al., 2020; Gainza et al., 2023), influenza (Strauch et al., 2017), and cancer (Silva et al., 2019). In many instances, designing function involves the design of both the \(3\)D geometric structure of the protein as well as its specific chemical interactions. In proteins, the amino-acid sequences determine the interaction between protein backbones and side chains, which fold into a distribution of protein structures. Consequently, the functional properties of protein structures can be _inferred_ from its sequence.

The representation of proteins plays a key aspect in any computational approach to protein engineering. The \(3\) structure of proteins can be mathematically represented on the space of rotation and translation invariant \((3)^{N}\). Several unconditional protein generative models have been developed recently to generate new protein backbones (Yim et al., 2023; Bose et al., 2024). While these models demonstrate the ability to design new proteins, they are insufficiently tailored for downstream drug discovery applications, where the primary challenge lies in generating proteins that are specifically tailored to interact effectively with a given target. In real-world drug design problems, one often knows the target protein (its amino acid sequence and often an experimentally verified \(3\) structure). In machine learning terms, the design of new proteins ("_de novo_" design) that can drug the given target can be framed as a _conditional_ generation problem. This raises the following research question:

_How can we leverage the structure and sequence of a target to inform_ de novo _protein design?_

**Current work.** In this paper, we introduce FoldFlow-2 a novel protein structure generative model that is additionally conditioned on protein sequences. FoldFlow-2 is built on the foundations of FoldFlow(Bose et al., 2024) and is an \((3)^{N}\)-invariant generative model for protein backbone generation that handles multi-modal data by design. Specifically, FoldFlow-2 introduces several new architectural components over previous protein structure generative models that enable it to process both \(3\) structure and discrete sequences. These include (1) a joint structure and sequence encoder; (2) a multi-modal fusion trunk that combines the representations from each modality in a shared representation space; and (3) a transformer-based geometric decoder. In contrast to prior efforts to incorporate sequences in structure-based generative models (Campbell et al., 2024), FoldFlow-2 leverages the representational power of a large pre-trained protein language model in ESM (Lin et al., 2022) enabling it to make use of the rich biological inductive bias found in sequences but at a scale far beyond ground-truth experimental \(3\) structures found in the Protein Data Bank (PDB).

As a sequence-conditioned model, FoldFlow-2 is able to tackle a suite of new tasks beyond simple unconditional generation. Specifically, our model can additionally be used for protein folding by simply generating structures conditioned on sequence as well as hard, biologically motivated conditional design problems. For instance, our model can perform partial structure generation by conditioning on a masked sequence, i.e., structure in-painting. This enables FoldFlow-2 to be better equipped than prior structure-only generative models to tackle the key challenges in de novo drug design. For example, in settings where we aim to engineer a structure that binds and neutralizes a desired target protein structure and sequence pair; this is precisely a structure and sequence in-painting problem.

As diversity and quantity of training samples play a crucial role in downstream generative modeling performance on conditional design tasks, we construct a new large dataset--an order of magnitude larger than PDB--of high-quality synthetic structures filtered from SwissProt (Jumper et al., 2021; Varadi et al., 2021). We further investigate the impact of fine-tuning FoldFlow-2 using Reinforced Fine-Tuning (ReFT), a new approach that aligns flow-matching generative models to arbitrary rewards. In the context of protein backbone generation, we apply fine-tuning to improve the properties of generated backbones, such as optimizing for the diversity of secondary structures, as well as improving the performance on conditional generation tasks like generating scaffolds around a target motif.

**Main results.** We summarize the main empirical results obtained using FoldFlow-2 below:

* We empirically demonstrate that FoldFlow-2 achieves state-of-the art performance for unconditional generation and leads to the most _designable, novel, and diverse_ proteins. In particular, FoldFlow-2 improves over RFDiffusion (Watson et al., 2023) and FoldFlow (Bose et al., 2024).
* We find FoldFlow-2 closes the gap in performance with purpose-built folding models like ESMFold and improves by a factor of \( 4\) compared to MultiFlow (Campbell et al., 2024), the most comparable protein structure generative model that also leverages sequences.
* We use FoldFlow-2 to solve a biologically relevant conditional design problem in motif scaffolding. We find that a fine-tuned FoldFlow-2 is able to solve all \(24/24\) scaffolds in the benchmark dataset from Watson et al. (2023). On challenging VHH nanobodies, it solves \(9/25\) refoldable motifs in comparison to \(5/25\) for the previous best approach RFDiffusion.
* We hypothesize that FoldFlow-2 is able to perform zero-shot equilibrium conformation sampling on unseen proteins in the ATLAS molecular dynamics (MD) dataset (Vander Meersche et al., 2024) based on conformation variation seen within the protein data bank. We observe that FoldFlow-2 is able to capture different modes of the equilibrium conformation comparably to ESMFlow-MD (Jing et al., 2024), a model fine-tuned on MD data, but lags behind AlphaFlow-MD (Jing et al., 2024).

Background and preliminaries

### Protein backbone and sequence parametrizations

**Sequence representation.** Protein sequences correspond to the chain of amino acids, which for a protein of length \(N\) is identified by a discrete token \(a^{i}\{1,,20\}=:\). As is customary in protein language models (Lin et al., 2022), these discrete tokens are encoded using a one-hot representation. We denote the entire amino acid sequence associated with a protein as \(^{N 20}\).

**Structure representation.** The \(3\)D structure of protein backbones can be represented as rigid frames associated with each residue in an amino acid sequence (Jumper et al., 2021). Each residue, \(i\), within a protein backbone of length \(N\) consists of idealized coordinates of their \(4\) heavy atoms \(^{*},_{}^{*},^{*},^{*} ^{3}\), with \(_{}^{*}=(0,0,0)\). The defining property of rigid frames is that they can be viewed as elements of the special Euclidean group \((3)\) and as such each frame \(x=(r,s)(3)\) contains a rotation \(r\) and translation \(s\) component. Applying a rigid transformation \(x^{i}\) to the idealized coordinates of the heavy atoms allows us to represent the rigid frame of a given residue, \([,_{},,]^{i}=x^{i}[ ^{*},_{}^{*},^{*},^{*}]\), where \(\) is the binary operator associated to the group, which for \((3)\) is simply matrix multiplication. This leads to a structure representation of the complete \(3\)D coordinates associated with all heavy atoms of a protein as the tensor \(^{N 4 3}\).

\((3)\)**: the group of rigid motions.** The special Euclidean group \((3)\) contains rotations and translations in three dimensions and can be thought of in several ways. It is a Lie group, i.e., a differentiable manifold endowed with a group structure. \((3)\) can be seen as the group of rigid frames, representing 3D rotations and translations. As a Lie group, \((3)\) can be uniquely identified with its Lie algebra, the tangent space at the identity element of the group. \((3)\) is also a matrix Lie group, meaning that its elements can be represented with matrices. It can formally be written as the semidirect product of the rotation and the translation groups, \((3)(3)(^{3},+)\). A more detailed introduction to Riemannian manifolds and Lie theory, with an emphasis on \((3)\) is provided in SSA.

### Flow matching on the \((3)\) group

As Lie groups are smooth manifolds, they can also be equipped with a Riemannian metric, which can be used to define distances and geodesics on the manifold. On \((3)\), a natural choice of the metric decomposes into the metrics on its constituent subgroups, \((3)\) and \(^{3}\)(Bose et al., 2024; Yim et al., 2023b). This allows us to build independent flows on the group of rotations and translations and induce a flow directly on \((3)\). As flow matching on Euclidean spaces is well-studied (Albergo and Vanden-Eijnden, 2023; Lipman et al., 2023; Liu et al., 2023), we restrict our focus on reviewing flows, conditional probability paths, and vector fields over the group \((3)\).

**Probability paths on \((3)\).** Given two densities \(_{0},_{1}(3)\), a probability path \(_{t}:((3))\) is an interpolation, parametrized by time, \(t\), between the two densities in probability space. Without loss of generality, we may consider \(_{0}\) to be the target data distribution and \(_{1}\) an easy-to-sample source distribution. A _flow_ is a one-parameter diffeomorphism in \(t\), \(_{t}:(3)(3)\). It is the solution to the ordinary differential equation (ODE): \(_{t}(r)=u_{t}(_{t}(r))\), with initial condition \(_{0}(r)=r\), where \(u_{t}\) is the time-dependent smooth vector \(u_{t}:(3)(3)\). It is said that \(_{t}\)_generates_\(_{t}\) if it induces a pushforward map \(_{t}=[_{t}]_{\#}(_{0})\).

**Matching vector fields on \((3)\).** The framework of Riemannian flow matching (Chen and Lipman, 2024) can also accommodate Lie groups such as \((3)\). Consequently, to learn a continuous normalizing flow (CNF) that pushes forward samples \(r_{0}_{0}\) to \(r_{1}_{1}\) we must regress a parametric vector field \(v_{}((3))\) in the tangent space of the manifold to the target conditional vector field \(u_{t}(r_{t}|r_{0},r_{1})\), for all \(t\). Conveniently, the target \(u_{t}(r_{t}|r_{0},r_{1})\) is the time derivative of a point \(r_{t}\) along the shortest path between \(r_{0}\) and \(r_{1}\)--i.e., the geodesic interpolant \(r_{t}=_{r_{0}}(t_{r_{0}}(r_{1}))\). Furthermore, for \((3)\) the target conditional vector field admits a closed-form expression \(u_{t}(r_{t}|r_{0},r_{1})=_{r_{t}}(r_{0})/t\) as the exponential and logarithmic maps are numerically computable using the axis-angle representation of the group elements (Bose et al., 2024). Given these ingredients, we can formulate the flow matching objective for \((3)\) as:

\[_{(3)}()=_{t(0,1),q(r_{0}, r_{1}),_{t}(r_{t}|r_{0},r_{1})}\|v_{}(t,r_{t})-_{r_{t}}(r_{0})/t \|_{(3)}^{2}.\] (1)

In eq. (1), \(q(r_{0},r_{1})\) is any coupling between samples from the source and target distributions. An optimal choice is to set \(q(r_{0},r_{1})=(r_{0},r_{1})\) which is the coupling, \(\), that solves the Riemannian optimal transport problem using minibatches (Bose et al., 2024; Tong et al., 2023; Fatras et al., 2020). Finally, the generation of samples is done by first drawing from a source sample \(r_{1}_{1}\) and integrating the ODE backward in time using the learned vectorfield \(v_{}\).

## 3 FoldFlow-2

We now present FoldFlow-2, our sequence-conditioned structure generative model. FoldFlow-2 operates on protein backbones \(x_{0}_{0}\) which are parametrized as \(N\) rigid frames as well as their corresponding sequence \(a\). As protein backbones contain symmetries from \((3)\), we design FoldFlow-2 as an \((3)^{{}^{N}}\)-invariant density using a flow-matching objective. We achieve translation invariance by constructing the flow on the subspace \((3)^{{}^{N}}_{}\), where the center of mass of the inputs is removed. Additionally, we can focus on building flows on the group of rotations \((3)\) and translations \(^{3}\), for each of the \(N\) residues independently, as \((3)^{{}^{N}}_{}\) can be viewed as a product manifold consisting of \(N\) copies of \((3)_{}\). The overall loss function for the model decomposes into per residue rotation and translation losses \(=_{(3)}+_{^{3}}\),

\[=_{t(0,1),_{t}(x_{t}|x_{0},x_{1}, )}[\|v_{}(t,r_{t},)-_{r_{t}}}{t} \|^{2}_{(3)}+\|v_{}(t,s_{t},)- -s_{0})}{t}\|^{2}_{2}],\] (2)

where the pair \((x_{0},x_{1})(x_{0},x_{1})\) is sampled from the optimal transport plan \(\). In addition, the sequence \(=a m\) corresponds to \(x_{0}\) and is masked completely, with a mask \(m\), with a probability \((0.5)\). Operationally, this means \(50\%\) of the time the model is trained unconditionally with no sequence information, i.e., \(=^{N}\), while the other \(50\%\) the model has access to the full sequence \(=a\). Optimizing the loss in eq. (2) is equivalent to maximizing the conditional log-likelihood of observing protein structures given their sequences \( p(|)\) when the sequence is not masked and maximizing the unconditional log-likelihood \( p()\) when the sequence is fully masked. Due to the ability to mask sequences, FoldFlow-2 enables new modeling capabilities in comparison to existing models as outlined in table 1. More precisely, FoldFlow-2 trained using masked sequences can perform a diverse set of tasks, outlined in table 2, beyond simple unconditional backbone generation which aids in tackling more biologically relevant problems that require conditional generation such as mimicking a protein folding model and designing the \(3\)D scaffolds around a target motif.

With the breadth of tasks **T1-T3** (table 2) FoldFlow-2 unlocks new structural design capabilities beyond the simple unconditional generation ability of FoldFlow. We next outline the architectural components of FoldFlow-2 in SS3.1 before detailing the training procedure in SS3.2, which includes key design decisions regarding the construction of our new scaled dataset of ground truth PDBs and filtered AlphaFold2 synthetic structures. We also outline the inference procedure for sampling in SSB.4. We conclude by discussing various techniques to fine-tune FoldFlow-2, including methods based on filtering with auxiliary rewards for supervised fine-tuning SS3.3 to align protein structures.

### FoldFlow-2 Architecture

The FoldFlow-2 architecture is comprised of three core components: (1) **Structure and sequence encoder:** An encoder which encodes both structures and sequences; (2) **Multi-modal fusion trunk:** the trunk which combines the multi-modal representations of the encoded structure and sequences; and (3) **Geometric Decoder:** a decoder that consumes the fused representation from the trunk and outputs a generated structure. The overall architecture of FoldFlow-2 is depicted in fig. 1.

   Method & \(\) & \(\) & \((,)\) \\  AlphaFold (Jumper et al., 2021) & ✗ & ✓ & ✗ \\ RFDiffusion (Watson et al., 2023) & ✓ & ✗ & ✗ \\ Chroma (Ingraham et al., 2023) & ✓ & ✗ & ✗ \\ FrameDiff (Yim et al., 2023b) & ✓ & ✗ & ✗ \\ FoldFlow (Bose et al., 2024) & ✓ & ✗ & ✗ \\ FrameFlow (Yim et al., 2023a) & ✓ & ✗ & ✓ \\ MotifEffusion (Watson et al., 2023) & ✗ & ✓ & ✓ \\ Multiflow (Campbell et al., 2024) & ✓ & ✓ & ✓ \\ FoldFlow-2 (Ours) & ✓ & ✓ & ✓ \\   

Table 1: Overview of the conditioning capability of unconditional (\(\)), folding (\(\)), and inpainting (\(,\)) of various protein backbone generation models.

    & **Task Name** & **Sequence Inputs** & **Structure Inputs** \\  (T1) & Unconditional & Fully-masked & Noise \\ (T2) & Folding & Unmasked & Noise \\ (T3) & In-Painting & Partially Masked & Partially Masked \\   

Table 2: By manipulating the input modalities, FoldFlow-2 is able to perform a diverse set of conditional and unconditional generation tasks including biologically relevant tasks such as designing scaffolds.

**Structure and Sequence Encoder.** We leverage existing state-of-the-art architectures to encode the structure and sequence modalities separately. For structure encoding, we rely on the invariant point attention (IPA) transformer architecture (Jumper et al., 2021), which is \((3)\)-equivariant. The benefit of the IPA architecture is that it is highly flexible and can both consume and produce a structure--i.e., \(N\) rigid frames--_and_ also output single and pair representations of the input structure.

To encode amino-acid sequences, we use a large pre-trained protein language model: the \(650\) variant of the ESM2 sequence model (Lin et al., 2022). Large protein language models have a strong inductive bias on atomic-level predictions of protein structures while exhibiting strong generalization properties beyond any known experimental structures--which we argue is highly correlated with goals of _de novo_ structure design. Moreover, the ESM2 architecture also produces single and pair representations for an encoded sequence of amino acids, which conceptually correspond to the single and pair representations from the structure encoder. Consequently, the output space of each modality prescribes a natural fusion of representations into a joint single and pair latent space for a given input protein.

**Multi-Modal fusion trunk.** After encoding both input structure and sequence, we construct a joint representation for the single and pair representation using a "project and concatenate" combiner module with simple MLPs, see fig. 1. We use LayerNorm (Ba et al., 2016) throughout the architecture as it is essential to accommodate differently-scaled inputs. The joint representations are further processed by a series of Folding blocks (Lin et al., 2023), which refines the single and pair representations via triangular self-attention updates.

**Geometric decoder.** To decode the joint representations of the inputs into \((3)_{{}_{0}}^{{}^{N}}\) vector fields, we once again leverage the IPA Transformer architecture. The decoder takes as input the single, pair outputs of the trunk _and_ the rigid representations from the structure encoder. One of our major findings is that including a skip-connection between the structure encoder and the decoder is essential for good performance as the temporal information is only given to the structure encoder.

Given each component, we stack \(2-2-2\) blocks for the encoder, trunk, and decoder components.

### Training

We train FoldFlow-2 by alternating between both folding and unconditional generation tasks using a novel sequence-and-structure flow matching procedure, described below.

**Dataset construction.** The generalization ability of generative models trained using maximum likelihood is determined by the quality and diversity of curated training data (Kadkhodaie et al.,

Figure 1: FoldFlow-2 architecture which processes sequence and structure and outputs \((3)_{{}_{0}}^{{}^{N}}\) vectorfields.

2024). Due to the limited size of ground truth structures in the Protein Data Bank (PDB) we aim to improve training set diversity by additionally curating a dataset of filtered AlphaFold\(2\) structures from SwissProt (Jumper et al., 2021; Varadi et al., 2021). To ensure FoldFlow-2 is trained on high-quality synthetic structures, we employ a set of stringent filtering techniques that remove many undesignable proteins from SwissProt. After filtering, our final dataset consists of \(160K\) structures and constitutes approximately an \(8\) fold increase compared to prior works (Yim et al., 2023; Bose et al., 2024). Our exact layered filtering strategy for synthetic structures in SwissProt is outlined by the following steps:

**(Step 1)**: **Filtering low-confidence structures.** We use per-residue local confidence metrics like the average pLDDT to filter out low-confidence structures from the initial SwissProt dataset.
**(Step 2)**: **Masking low-confidence residues.** Globally high-confident structures may include low-confidence residues with disordered regions that can impede training. We use a per-residue pLDDT threshold of \(70\) to mask such "low-quality" residues during training.
**(Step 3)**: **Filter high-confidence, low-quality structures.** The nature of synthetic data means that even following steps 1 and 2 low-quality data persists in a curated dataset. To combat this we further filter structures by learning a light-weight structure prediction model trained on structural features predictive of protein quality.

We report a detailed analysis of each step in the filtration process in SSB.1 which includes examples of low-quality structures that were filtered as illustrative examples. The impact of these findings is empirically corroborated in by analyzing generated samples from FoldFlow-2 in SSC.2.

During training, we set the fraction of synthetic samples that may be seen during an epoch to \(2/3\) of the epoch. This prevents the model from overfitting to the remaining noise in the synthetic data, and is also common practice when training with synthetic data (Hsu et al., 2022; Lin et al., 2023). Anecdotally, we did not notice an improvement from using a smaller proportion of synthetic structures. Finally, in the FoldFlow-2 architecture, we keep the ESM pre-trained language model fixed during training and train all other components (encoder, trunk, and decoder) from scratch. The results presented in table 3 and SS4.4 use PDB data only, as this displayed the best performance for designability scores.

### Fine-Tuning FoldFlow-2

We explore the efficacy of fine-tuning FoldFlow-2 with preferential alignment. We take a supervised fine-tuning approach (Wei et al., 2022) that uses an additional fine-tuning dataset which is filtered using pre-specified auxiliary rewards \(r_{}\) to create a preferential dataset \(_{}\). We term this Reinforced FineTuning (ReFT) since fine-tuning in this manner can be considered aligning FoldFlow-2 generations to the auxiliary reward. Summarizing this in three steps: (1) We take a curated dataset of proteins with desirable metrics; (2) We use \(r_{}\) to score the samples from step 1 and filter them to get a subset of high-scoring samples; (3) We then improve FoldFlow-2 by SFT on the filtered subset. Finetuning with ReFT optimizes the following optimization objective \(_{}()\),

\[_{p_{}}_{}()=_{(x,a) _{}}[r_{}(x) p_{}(x|a) ].\] (3)

Compared to recent alignment methods based on reward models, as in RLHF (Bai et al., 2022), ReFT uses a filtered structure dataset to fine-tune FoldFlow-2. Standard RL approaches seek to fine-tune generative model-based model-generated data and assume access to evaluating the reward function. Our approach maximizing \(_{}()\) requires constructing \(_{}\) with auxiliary reward \(r_{}\), demonstrated by the improvement in secondary structure diversity in SS4.2.

## 4 Experiments

We evaluate FoldFlow-2 on multiple protein design tasks including unconditional generation, motif scaffolding, folding, fine-tuning to improve secondary structure diversity, and equilibrium conformation sampling from molecular dynamics trajectories. We provide implementation details in SSB.

**Baselines.** As our main baselines for the unconditional generation task we rely on pre-trained versions of FrameDiff (Yim et al., 2023), Chroma (Ingraham et al., 2023), Genie (Yeqing and Mohammed, 2023), MultiFlow (Campbell et al., 2024), and RFDiffusion which is the current gold standard (Watson et al., 2023). In conditional generation tasks like motif scaffolding, we compare against a conditional variant of FrameFlow (Yim et al., 2023) as well as RFDiffusion. For protein folding, we focus on comparing against ESMFold (Lin et al., 2022) and MultiFlow which also leverages sequence information. Lastly, for conformational sampling the principal baselines are ESMFlow and AlphaFlow (Jing et al., 2024).

### Unconditional protein backbone generation

We evaluate unconditional structure generation using metrics that assess the designability, novelty, and diversity of generated structures. For each method, we generate \(50\) proteins at lengths \(\{100,150,200,250,300\}\) (c.f. FoldFlow-2 samples in fig. 12). Designability is computed by using the _self-consistency_ metric which compares the refolded proteins (with ProteinMPNN (Dauparas et al., 2022) and ESMFold (Lin et al., 2022)) with the original one. Novelty is computed using: 1.) the fraction of designable proteins with TM-score \(<0.3\) and 2.) the average maximum TM-score of designable generated proteins to the training data. Finally, diversity uses the average pairwise TM-score designable samples averaged across lengths as well as the maximum number of clusters.

**Results.** We see that FoldFlow-2 outperforms all other methods on all metrics--crucially without requiring a pretrained folding model as part of the architecture like RFDiffusion. In particular, we observe that FoldFlow-2 produces the most designable samples with \(97.6\%\) of samples being refolded by ESMFold to within \(<2\)A. We also find that FoldFlow-2 novelty improves over RFDiffusion by an absolute \(25.2\%\) in the fraction of designable samples with TM-score \(<0.3\). Furthermore, we observe \(19.9\%\) and \(102.3\%\) relative improvement in the diversity of FoldFlow-2 over RFDiffusion as measured by the pairwise TM-score and Max Cluster fraction. This places FoldFlow-2 as the current _most designable, novel, and diverse_ protein structure generative model.

We present uncurated generated samples of FoldFlow-2 and RFDiffusion in fig. 2. We further visualize the distribution of secondary structures of all methods in fig. 3. We see a clear indication that FoldFlow-2 is able to produce the most diverse secondary structures--more closely matching the training distribution (see fig. 3e)--and improving over RFDiffusion. We further observe increased amounts of \(\)-sheets and coils which are particularly challenging for models like FrameDiff and FoldFlow that primarily generate \(\)-helices. We also include multiple ablations on architectural choices, inference annealing, and sequence conditioning in table 14.

### Increasing secondary structure diversity with finetuning

We investigate ReFT based data filtering to improve diversity of secondary structures in generated samples. We use a diversity score based auxiliary reward for filtering, based on weighted

    & Designability &  &  \\   & Frac. \(<2\)Å (\(\)) & Frac. TM \(<0.3\) (\(\)) & avg. max TM (\(\)) & pairwise TM (\(\)) & MaxCluster (\(\)) \\  RFDiffusion & 0.969 \(\) 0.023 & 0.116 \(\) 0.020 & 0.449 \(\) 0.012 & 0.256 & 0.172 \\ Chroma & 0.636 \(\) 0.030 & 0.214 \(\) 0.033 & 0.412 \(\) 0.011 & 0.272 & 0.132 \\ Genie & 0.581 \(\) 0.064 & 0.120 \(\) 0.021 & 0.434 \(\) 0.016 & 0.228 & 0.274 \\ FrameDiff & 0.402 \(\) 0.062 & 0.020 \(\) 0.009 & 0.542 \(\) 0.046 & 0.237 & 0.310 \\ FoldFlow & 0.820 \(\) 0.037 & 0.188 \(\) 0.025 & 0.460 \(\) 0.020 & 0.247 & 0.228 \\ FoldFlow-2 & 0.976 \(\) 0.010 & 0.368 \(\) 0.031 & 0.363 \(\) 0.009 & 0.205 & 0.348 \\   

Table 3: Comparison of Designability (fraction with scRMSD \(<2.0\)Å), Novelty (max. TM-score to PDB and fraction of proteins with averaged max. TM-score \(<0.3\) and scRMSD \(<2.0\)Å), and Diversity (avg. pairwise TM-score and MaxCluster fraction). Designability and Novelty metrics include standard errors.

Figure 2: Uncurated designable (scRMSD \(<2\)Å) length 100 structures with ESMFold refolded structure from FoldFlow-2 and RFDiffusion colored by secondary structure assignment. FoldFlow-2 is significantly more diverse in terms of secondary structure composition where we see RFDiffusion generates mostly \(\)-helices.

entropy on the proportions of each residue belonging to each type of secondary structure--i.e., alpha-helices (\(\)), coils \(\), beta-sheets \(\) in the set \(\), that can be analytically written as \(r_{}=(_{s}p_{s}w_{s})(1+_{s }p_{s} p_{s})\). Due to models producing increasing amounts of helices, we use \(w_{}=1\), \(w_{c}=0.5\) and \(w_{}=2\), and take top \(25\%\) of samples according to the \(r_{}\). Experimental results in fig. 2(f) with generated samples in fig. 12 demonstrate that protein at all lengths benefit from training with ReFT as measured by diversity of generated samples, and produces most amount of \(\)-sheets, and can surpass diversity improvement already obtained by training using synthetic structures as in fig. 3.

### Folding sequences

Given that FoldFlow-2 is sequence conditioned, we can perform protein folding by providing a valid sequence during inference. During training, FoldFlow-2 tries to transform a \((3)^{{}^{N}}_{}\) noise sample into the given sequence's 3D structure. Therefore, we aim to measure the generalization properties of our model to fold unseen sequences. We evaluate folding on a test set of \(268\) unseen proteins from the PDB dataset. We compare the folding capabilities of FoldFlow-2, ESMFold, and Multiflow. In table 4, we report the aligned RMSD between the predicted backbone and the ground truth backbone. We find that FoldFlow-2, trained for structure generation, approaches the performance of ESMFold which is a purpose-built folding model. We contextualize this result by noting that FoldFlow-2 \( 4\) is better at folding than the most comparable model in MultiFlow (Campbell et al., 2024) which is a multi-modal flow matching model using sequences.

### Motif Scaffolding

In motif scaffolding, we are tasked with designing a subset of residues, termed "scaffolds", around one or more subsections of a ("motif") protein structure that have known biologically-important functions through its interaction with a target. This enables the design of proteins with _a priori_ functional sites using generative models (Wang et al., 2021; Watson et al., 2023). The motifs can be

   Model & RMSD (\(\)) \\  ESMFold & 2.322 \(\) 4.270 \\ MultiFlow & 14.995 \(\) 3.977 \\ FoldFlow-2 & 3.237 \(\) 4.145 \\   

Table 4: Folding model evaluation on a test set of 268 proteins from PDB.

Figure 3: Distribution of secondary structure elements (\(\)-helices, \(\)-sheets, and coils) of designable (scRMSD \(<2.0\)) proteins generated by various models. FoldFlow-2 generates more diverse designable backbones.

small and have non-specific shapes (e.g. a helix), and hence it is important for the generative model to understand the chemical information it carries on top of its geometry. We thus consider the task of motif scaffolding as an example of how our model can be fine-tuned for conditional generation tasks. We consider two datasets for evaluating motif scaffolding performance: the benchmark proposed in Watson et al. (2023) consisting of \(24\) single-chain motifs, and a new benchmark based on scaffolding the Complimentary Determining Regions (CDRs) of VHH nanobodies, as found in the Structural Antibody Database (Schneider et al., 2022).

**Motif Scaffolding Benchmark.** We use the scaffolding benchmark from Watson et al. (2023) and follow the pseudo-label fine-tuning procedure described in Yim et al. (2023) by randomly generating motifs from proteins by training on both the motif structure _and_ sequence. For inference, we sample the scaffold lengths for each motif and provide both the both partially masked structure and sequence to the model. We follow the same evaluation procedure used in RFDiffusion (c.f. SSB.6 for details). Our results in table 5 show that both FoldFlow-2 and RFDiffusion solve all \(24/24\) motifs.

**CDR Scaffolding.** VHH antibodies, also known as nanobodies, have shown significant promise in protein design and therapeutics due to their unique properties (Muyldermans, 2021). They are composed of a single variable domain derived from camelid heavy-chain antibodies, featuring three complementarity-determining regions (CDRs) that confer specificity and variability in antigen binding. As a result, creating effective scaffolds for nanobodies is challenging due to the need to maintain the designability of the CDRs and especially because any scaffolding effort must avoid altering these characteristics to preserve binding functionality. We treat this as a conditional generative modeling problem and fix the motif atoms, and mask the scaffold sequence information. Exact training and experimental details along with additional metrics are provided in SSB.6. Our results are found in table 5, where the average motif \(\) is much higher than the average scaffold \(\). The result is a much lower number of solved motif scaffolding.

### Zero-shot Equilibrium Conformation Sampling

We now test FoldFlow-2 on zero-shot equilibrium conformation sampling task. Starting from a sequence, we generate multiple conformations of the same proteins and compare the distribution of conformations with the ones from molecular dynamic simulations. We compare FoldFlow-2 with AlphaFlow-MD and ESMFlow-MD; two folding models fine-tuned on a molecular dynamic dataset, and non finetuned models Eigenfold (Jing et al.) and STR2STR (Lu et al., 2024). In table 6, we report the pairwise and global RMSD, the root mean square fluctuation (RMSF), and the 2-Wasserstein on the top two principal components. For both RMSD and the RMSF metrics, we report the Pearson correlation between the values from the generated ensemble and those of the ground truth ensemble (the procedure is detailed in full in SSB.7).

We use the same test set as in Jing et al. (2024) restricted to proteins of length at most \(400\) amino acids. Notably FoldFlow-2 performs similarly or better than the comparable model ESMFlow-MD across

Figure 4: Protein conformation ensembles from the ATLAS dataset, ESMFlow-MD and FoldFlow-2. Proteins are colored by their secondary structure with \(\)-helices in blue, \(\)-sheets in red, and coils in green.

   Benchmark & RFDiffusion &  \\  Model & Solved /24 \(\) & Diversity \(\) & Motif \(\) & Scaffold \(\) & Solved /25 \(\) \\  RFDiffusion & 24 & 0.345 & 3.94 \(\) 1.54 & 2.40 \(\) 0.93 & 5 \\ FrameFlow (+FT)* & 21 & – & – & – & – \\ FoldFlow-2 (+FT) & 24 & 0.445 & 2.78 \(\) 1.01 & 1.67 \(\) 0.24 & 9 \\   

Table 5: Motif-scaffolding benchmarks. FrameFlow does not have public code for motif-scaffolding and thus cannot be evaluated on the VHH benchmark. “+FT” indicates “with fine-tuning”. *Using reported numbers with AlphaFold2 instead of ESMFold used in our evaluation procedure; c.f. SSB.6 for further discussion.

all metrics _without any fine-tuning_ and _with significantly fewer parameters_ on molecular dynamics data, indicating that the base model trained only on PDB already captures similar information about protein dynamics as models given explicit access to this data. Moreover, we observed that FoldFlow-2 requires 4.5\(\) less GPU hours for training and 33\(\) less trainable parameters while allowing for 6\(\) faster inference steps than ESMFlow-MD as reported in table 13, improving FoldFlow-2's prospects as a practical base model for future work on capturing protein dynamics.

## 5 Related work

**Protein design.** Physics-based protein structure design yielded the first de novo proteins (Huang et al., 2016). For example, structure-based biophysics approaches have previously resulted in several drug candidates (Rothlisberger et al., 2008; Fleishman et al., 2011; Cao et al., 2020).This was followed by language models (Hie et al., 2022; Ferruz et al., 2022) and geometric deep learning (Gainza et al., 2020) for protein structure design. Recently, diffusion (Wu et al., 2024; Yim et al., 2023; Watson et al., 2023; Ingraham et al., 2023; Wang et al., 2024; Frey et al., 2024) and flow-based models (Bose et al., 2024; Yim et al., 2023; Jing et al., 2024) have risen to prominence. These methods employ a backbone-first approach with the notable exception of MultiFlow (Campbell et al., 2024) which uses sequence to perform co-generation.

**RLHF and Supervised Fine-Tuning (SFT).** Aligning the outputs of language models with RLHF has recently gained interest (Ouyang et al., 2022; Stiennon et al., 2020; Bai et al., 2022). These methods learn a reward model for post-training alignment to desired behavior (Mishra et al., 2022), which can prove challenging for protein design (Zhou et al., 2024). SFT on hand-crafted data has proven to be effective in enhancing performance but requires high-quality data (Roziere et al., 2023; Yuan et al., 2023). Filtering real data using auxiliary rewards serve as a substitute for steering the desired properties of the generated samples.

## 6 Conclusion

In this paper, we introduce a new sequence-conditioned protein structure generative model called FoldFlow-2. FoldFlow-2 leverages a protein language model to condition Flow Matching-based protein generative models with sequences. Our model achieves state-of-the-art results on unconditional generation and generates diverse and novel proteins, especially when trained on our new dataset. Conditioning over sequences allows our model to perform novel tasks such as folding sequences and motif-scaffolding tasks and we show its competitiveness on those tasks. Regarding the limitations of our model, we note that it requires a competitive pre-trained language model to be sequence conditioned, which can be hard to acquire. We also note that ProteinMPNN, used in our evaluation pipeline, has been trained only on PDBs. Therefore, it is possible that our models trained on our new dataset generates designable proteins which are not correctly processed by ProteinMPNN.

## Contribution statement

Architecture design was led by G.H. Infrastructure development was led by J.V. The experiments were divided as follows: Unconditional (K.F., A.T.), diversity (E.T.L., R.I., C.L.), folding (G.H.), motif-scaffolding (G.H., J.V., E.T.L, C.L.), and equilibrium conformation (J.R.B, G.H., T.A.S.). Dataset preparation including synthetic structure filtering (J.V., P.L., K.F.). A.J.B. drove the writing of the paper with contributions from all other authors. A.J.B. and A.T. cosupervised the project with guidance from M.B.