# Exploring Why Object Recognition Performance

Degrades Across Income Levels and Geographies with Factor Annotations

 Laura Gustafson, Megan Richards, Melissa Hall, Caner Hazirbas, Diane Bouchacourt, Mark Ibrahim

Fundamental AI Research (FAIR), Meta

###### Abstract

Despite impressive advances in object-recognition, deep learning systems' performance degrades significantly across geographies and lower income levels--raising pressing concerns of inequity. Addressing such performance gaps remains a challenge, as little is understood about why performance degrades across incomes or geographies. We take a step in this direction by annotating images from Dollar Street, a popular benchmark of geographically and economically diverse images, labeling each image with factors such as color, shape, and background. These annotations unlock a new granular view into how objects differ across incomes/regions. We then use these object differences to pinpoint model vulnerabilities across incomes and regions. We study a range of modern vision models, finding that performance disparities are most associated with differences in _texture, occlusion_, and images with _darker lighting_. We illustrate how insights from our factor labels can surface mitigations to improve models' performance disparities. As an example, we show that mitigating a model's vulnerability to texture can improve performance on the lower income level. **We release all the factor annotations along with an interactive dashboard to facilitate research into more equitable vision systems**.

## 1 Introduction

The widespread adoption of object-recognition systems afforded by advances in deep learning comes with a responsibility: systems should work equally well across groups of individuals. Previous work demonstrates object-recognition performance is far from equal across income levels and geographies (De Vries et al., 2019; Goyal et al., 2022; Rojas et al., 2022). This disparity encompasses publicly available recognition systems, state-of-the-art supervised and self-supervised models. Most worrisome among these findings is that the performance degradation disproportionately affects lower income households. When Artificial Intelligence (AI) systems are deployed in applications such as medical imaging, their biases can lead to disproportional harm. For example, models diagnosing COVID-19 were found to rely on geographically-biased features such as the hospital's font to diagnose patients (Roberts et al., 2021).

While existing work measures performance disparities across incomes and geographies, addressing the performance gaps remains a challenge. Key to progress is understanding _not just that, but why such disparities arise_. One hypothesis raised in DeVries et al. (2019) is that objects as well as their environments can vary drastically across regions. When factors such as object shape or lighting in a region differ from those commonly seen during training, the shift can cause model performance to drop. However, no systematic study exists characterizing how such factors vary across regions and incomes. Identifying the factors associated with model disparities can shed light on research directions to improve performance degradation across incomes and geographies.

We take a step in this direction by annotating images from Dollar Street , the most common benchmark for evaluating performance disparities in object recognition systems. Dollar Street contains 38k images of household objects spanning 54 countries across income levels. We annotate each image with factors to mark what makes each distinctive, such as color, pose, shape, and texture. We first analyze how images vary across incomes and regions using our factor labels in Section 6. We find images of some classes such as _roofs_ differ considerably across regions (and incomes) while others (such as _pens_) hardly vary.

We then investigate how our factor labels can explain model mistakes. We find an overall correspondence between the distribution of factors per region (and income) and model performance. Even for the latest generation of foundation models, such as CLIP , performance degrades by as much as 25.7% (top-5 accuracy) across incomes. We also compare the performance of other popular models across learning paradigms (self-supervised, supervised), architectures (CNN-,transformer-, MLP-based), as well as large scale pretraining . We find remarkably similar vulnerabilities across these popular models.

Next, we precisely rank factors by examining how much more likely they are to appear among misclassifications. A factor much more likely to appear among misclassification suggests a model is vulnerable to the factor. In our analysis we find vulnerabilities in _texture_, _occlusion_, and _darker lighting_ are most associated with models' performance degradation in lower incomes, Figure 1. We further study class-specific model vulnerabilities, finding strong associations between mistakes for particular classes and factors. For example, we find that for _sofas_, images labeled with texture are 7.2x more likely to appear among CLIP's mistakes than overall, suggesting texture bias is a vulnerability.

Finally, we study whether we can use robustness techniques to make fairness improvements. We show that mitigating the texture vulnerability surfaced by our analysis can improve performance disparities across incomes/regions in Section 5.2. We find a model trained to mitigate texture bias not only performs better overall +0.8% (top-5 accuracy), but on the relevant subset of images (those marked with texture), improves accuracy by 4.1% for 10w incomes. This suggests factor labels not only explain mistakes, but can even reveal promising mitigations to combat the disparities we observe in vision models today. Along with our analysis, we release all the factor annotations with an interactive dashboard to enable research facilitating more responsible, equitable vision system.

To summarize, our contributions are 1) we annotate all of Dollar Street images with distinctive factor labels such as pose, background, and color, 2) we explain performance disparities in models (including CLIP) using our factor annotations to reveal vulnerabilities in texture, occlusion, and darker lighting, 3) we demonstrate mitigating the vulnerability to texture can improve performance disparities across incomes and geographies, 4) we release all our factor annotations with a dashboard (Figure 2) allowing researchers to interactively query and visualize image factor labels to spur research into equitable vision systems.

Figure 1: CLIPâ€™s vulnerability to texture, darker lighting, and occlusion are associated with performance disparities for lower incomes. We rank the most vulnerable factors based on how much more likely a factor is selected among misclassified images than overall. The example images are of _dishbrushes_ from Dollar Street.

Annotating Dollar Street with factor labels

The Dollar Street dataset is the most common computer vision benchmark for classifying everyday objects (e.g _armchairs, pens_) across incomes and geographies. Households across the world upload images of the specified objects. These images are labeled with the object class, location, and income of the household. The income is standardized on an international scale by DollarStreet [DSI]. We use the procedure described in (Goyal et al., 2022) to aggregate household incomes into buckets: high, medium and low, and group countries into regions: Asia, Africa, Europe, The Americas. Table 3 in Appendix A.1.1 shows the number of images per income bucket/region pair.

### Annotation Procedure

In order to explain the degradation in model performance across incomes or geographies, annotators labeled images in Dollar Street with the factors distinguishing each image. We select all 14k images overlapping with classes in the ImageNet-21k taxonomy from Ridnik et al. (2021), using the mapping from DollarStreet classes to ImageNet synsets from Goyal et al. (2022). We follow the same annotation procedure as in Idrissi et al. (2022). Since it's challenging to accurately label an image in isolation, we ask annotators to label how each image differs from a fixed set of three prototypical images chosen for each class. We define prototypical images for each class as those correctly classified by a ResNet-50 model with the highest confidence. We curate a list of sixteen potential factors that can distinguish an image from the prototypical images for its class. These factors include pose, various forms of occlusion, size, style, type or breed capturing common variations in images. Specifically, this set of sixteen factors has been shown to comprehensively cover most distinctive image factors via user studies comparing the sixteen factors against free-form text responses in Idrissi et al. (2022). A full list is shown in Figure 2. Annotators select any number of factors they believe best distinguish each image. In addition, we ask annotators to provide text descriptions to account for factors outside the sixteen factors we provide, and ask if they agree with the original class label (see 2.3 for analysis of this). A more detailed description of the annotation setup and prototypical images is in Appendix A.1.

### Factor label statistics

We first explore how frequently each factor was selected across income levels and regions. In Figure 2, we plot the distribution of factor labels across regions and income buckets. On average, annotators chose 3.2 factors per image (standard deviation 1.2). The two most correlated factors are _color_ and _pattern_, with a correlation coefficient of 0.19.

Across all incomes, _pose, background,_ and _pattern_ are the most selected factors.For most factors, there is only a minor difference in the frequency that a factor was selected across income buckets. For _texture_, however, there's a noteworthy difference across income levels with 10.2% of images in the low income bucket labeled with _texture_ compared to only 4.2% for medium and 2.0% for high income buckets. This implies _texture_ is 5x more likely to be selected within the low income bucket (relative to the high income bucket), a stark difference.

The most commonly selected factors are consistent across regions and incomes.Similar to our observation across income buckets, the most striking difference across regions is for _texture_. _Texture_ is selected for 7.8% of images in Africa but for only 2% of images in Europe--a 4x difference. The similarity of emerging patterns in factors across incomes to those across regions suggests that income bucket variation differences are also exhibited across geographies. This can in part be explained by the relative rates of co-occurrence of regions and income buckets in DollarStreet, see Table 3 in Appendix A.1.1.

### Controlling for regional differences in raters' perceptions

Challenges naturally arise when running such a large annotation procedure. In our case, there can exist regional perceptions of the semantic meaning of every object label. Indeed, the Dollar Street object class labels were originally collected from the household members who took the image, rather than assigned retroactively, which means that regional perceptions could be a source of variance in the dataset.

For annotating DollarStreet with factor labels, we sourced 12 annotators from Indonesia. We trained the annotators on this specific annotation task before gathering the final factor label annotations. Throughout the process, we had an auditor who reviewed the annotations and brought up ambiguities. However, when gathering the factor annotations, we found that these annotators had a number of times that they disagreed with the class label. We hypothesized this could be due to differences in regional perceptions of objects.

To control for the effect of such perceptual differences across regions, we created a second annotation task. To select the images for the task, we analyzed the results of the original factor annotations, which included an option for annotators to disagree with the object label. We selected the subset of images where the annotator disagreed with the label, which totaled 2,476 images, or 18.1% of the Dollar Street dataset. For the second task, we collected additional annotations for this disputed subset, asking a different set of annotators sourced from 6 countries (5 continents) whether they agreed or disagreed with the object label. In total, there were 79 annotators who provided 12,507 label annotations. See Appendix A.1.4 for examples of annotator disagreements, and classes with the highest and lowest levels of label disagreement.

We then compared the levels of disagreement between annotators from the region where the image was taken (source region) and annotators who were from other regions. If there were region-specific biases in the label, we would expect a much higher rate of disagreement for annotators not from the source region. For images in the second task, we found annotators from both the source and other regions disagreed with the original label at similar rates (an average of 49.1% and 46.8% respectively). This consistency suggests regional differences in label perceptions do not constitute a significant source of class variation in Dollar Street. Next, we study how today's best object recognition models perform across regions and incomes.

## 3 Modern models' performance degrades across incomes and geographies

Performance inequities are pervasive across architectures and training methods.We compose a study on DollarStreet encompassing models across architectures (convolutional, transformer, and feedforward), learning paradigms (self-supervised, supervised, contrastive), and pretraining datasets of various sizes (up to 1 billion images). We first study the popular foundation model CLIP, which has been shown to have strong zero-shot performance on several classification benchmarks [Radford et al.]. CLIP is trained on 400M text-image pairs using a text encoder and an image encoder enabling a user to perform zero shot classification for any image. Here we prompt the model using the set of Dollar Street classes for each image to generate predictions. Our evaluation setup is described in detail in Appendix A.3. For the remaining models, to generate predictions on Dollar Street, the models are pretrained or finetuned on ImageNet21k and we use the same mapping from Goyal et al. (2022) and

Figure 2: **Pose, background, and pattern are the most commonly selected factors**. The left panel shows the percent of images by region and income that were labelled with each factor. Annotators labelled each image with the factors that most distinguished each image from the prototypical images of its class. On the right is a screenshot of the public interactive dashboard for the annotations

detail our evaluation procedure in Appendix A.3. We compare the performance of the set of models across incomes and regions in Figure 3. We find most models have comparable drops in accuracy across incomes and regions despite model differences. BEiT performs worse overall, but still exhibits similar trends in performance gaps across incomes and geographies (Bao et al., 2021). This suggests even large scale pretraining in models such as SEER (Goyal et al., 2021), modern architectures such as ViT (Ridnik et al., 2021) or MLP Mixer (Tolstikhin et al., 2021), and self-supervised learning still don't address performance inequities. Why do such consistent disparities arise? Next we study whether factors labels can explain the performance disparities in modern vision models. We focus our study to the CLIP ViT B/32 model as it has the highest performance on DollarStreet.

## 4 Explaining model performance disparities with factor labels

We now study how our factor labels can explain the model performance disparities we observed across regions/incomes. After ruling out variables such as image quality and class imbalance in training, we demonstrate how our factor labels can surface specific model vulnerabilities associated with degradation in performance across regions/incomes.

### Controlling for factors not captured in our annotations

**Performance disparities are not explained by image quality or training data class imbalance.** As image quality has been shown to impact the performance of facial recognition models (Xu et al., 2014), we first investigate whether image resolutions differ across regions and effect model performance. Rojas et al. (2022) found very minor differences in average image quality across region in DollarStreet. We take this a step further and find no strong correlation (\(<0.05\) Pearson's correlation coefficient) between image DPI and model performance (top-5 accuracy). Next, since class imbalance in training can skew model performance, we also investigate the extent to which class imbalance affects the disparities we observe. For ImageNet-21K pretrained or finetuned models (ViT, ResNet, MLPMixer, BEiT, and SEER), we calculate the Pearson correlation between number of images in each class for ImageNet-21K and model's top-5 accuracy. We found similarly weak correlations for all models, with coefficients less than 0.25 for top-5 accuracy (all values reported in A.4). These results suggest that variation in image quality and pretraining class imbalance explain very little of the variation model mistakes in Dollar Street. Next, we examine whether our factor labels can explain performance disparities.

### Variation in factor labels are indicative of performance disparities

To assess whether our factor labels are indicative of model performance disparities, we measure whether larger differences in factor labels across incomes/regions correspond to larger degradations in model performance. Specifically, for each class we measure the Jensen Shannon Distances (JSD) between the factor label distributions of every pair of income buckets (and regions). This quantifies how images in a classes vary across income bucket (or region) pairs according to our factor labels.

Figure 3: **Across architectures and learning procedures, model performance degrades similarly across _regions_ (left) and _incomes_ (right). Bars indicate top-5 accuracy.**

Next, we calculate whether larger differences in images across incomes/regions correspond to larger disparities in model performance. We find as a class varies more across income pairs (according to the JSD of factor distributions), model performance gaps also increase, as shown in Figure 4. For example, classes that differ most across incomes (top quarter) suffer a 3x drop in accuracy compared to classes that differ the least across incomes (bottom quarter). We find a similar but less stark trend across regions shown in Figure 4. We acknowledge such an analysis of likelihoods is inherently not causal. Our results based on associations between factors and mistakes suggest _our factor labels are indicative of performance disparities across regions/incomes_.

### Model performance disparities are most associated with texture, darker lighting, and occlusion

To more precisely assess which factors are most associated with mistakes, we use the same error ratio metric from Idrissi et al. (2022) to measure the association between each factor and model errors. Specifically, the error ratio for a factor quantifies how much more or less likely a factor is to appear among a model's misclassified samples as

\[)-P()}{P()}\] (1)

An error ratio greater than zero indicates how much more likely a factor is to appear among misclassified samples suggesting the factor is associated with model mistakes. For example, an error ratio of 2x indicates a factor is 2x more likely to be selected among misclassified samples than overall. An error ratio less than zero indicates a factor is less likely to appear among misclassified samples suggesting the model is robust to the factor. Since some factors are selected only for a few images, we exclude factors selected for five or fewer images in our analysis. Doing so excludes style and brightness.

Texture, occlusion, and darker lighting are most associated with model disparities across incomes and regions.We examine the five factors most associated with model mistakes (measured using error ratio). Overall, we find mistakes for CLIP with a ViT-B/32 encoder are most associated with _texture_, _occlusion_, or objects appearing _too small_ as shown in Figure 4(a). For example, _texture_ appears +0.88x more among CLIP's mistakes than overall. Similarly, occlusion appears +0.76x and smaller +0.73x more so among mistakes. We conduct \(^{2}\)-test to verify such differences in factor prevalence are statistically significant (see Appendix A.4).

We find these vulnerabilities also explain the performance disparities we observe for low incomes and regions with lower performance (Africa). In Figure 4(a) we show the five factors most associated with model mistakes across incomes (and across regions in Figure 4(b)). We find in the low income bucket, texture has the largest error ratio with _texture_ 1.7x more likely to be selected among misclassifications in the low income bucket. On the other hand, for the high income bucket misclassifications are associated with quite different factors. For example, smaller objects are most associated with mistakes in the high income bucket. We find a similar trend for across regions with _texture_, _occlusion_, and darker lighting most associated with mistakes in the region of Africa. We also detail model strengths by measuring factors that are much less likely to appear among mistakes in Appendix A.4. For example, we find CLIP is much less likely to misclassify images with _partial views_ of objects.

#### 4.3.1 Some model vulnerabilities are class-specific

Beyond explaining performance disparities across regions or income buckets overall, our factor labels enable us to explain vulnerabilities for specific classes. We show in Table 1 the factors most associated

Figure 4: **Differences across regions/incomes measured by our factor labels are indicative of performance disparities. The factor dissimilarity measures the distance (JSD) in factor label distributions across two regions/incomes for a specific class. The drop in accuracy measures the drop in performance for the given class across two regions/incomes.**

[MISSING_PAGE_FAIL:7]

sistently among the factors most associated with model mistakes. While texture is known to be a bias specifically for convolutional-models (Geirhos et al., Hermann et al., 2020), we find regardless of architecture or training procedure models have similar vulnerabilities.

### Improving performance disparities by mitigating texture bias

Our analysis in Section 4.3 reveals _texture_ is most associated with model's performance discrepancy across incomes and geographies. Can we improve this performance disparity by mitigating models' reliance on texture?

To assess this, we compare in Table 2 the performance of a standard ResNet-50 trained on ImageNet-1k compared to a ResNet-50 trained to mitigate texture bias (Geirhos et al.). Since these are trained on ImageNet-1k (1 million images) rather than ImageNet-21k (14 million images), the overall performance is lower than other models we studied earlier (see Appendix A.5). Controlling for pre-training data, we observe a boost of +0.8% in overall top-5 accuracy for the model mitigating texture bias. On the relevant subset of images (those marked with texture as a distinctive factor), we find consistent improvements in accuracy for the low income bucket +4.1% and in lower performing regions (Africa +3.0% and Asia +0.6%). This suggests that factor labels do not only explain model mistakes, but can also reveal potential mitigations to combat performance disparities.

## 6 How do objects vary across incomes and geographies?

Leveraging the factors gathered in our annotation process, we can quantitatively assess how classes differ across incomes and regions. For example, we can identify classes that change the most across incomes (e.g. ceiling), as well as the corresponding factors that best differentiated high or low income ceilings (pose, subcategory, and texture).

To compare how classes differ across two regions (or incomes), we compute the distribution of factors for each class. Specifically, for every pair of regions (or incomes) we normalize the distributions per class then measure the Jensen-Shannon Distance (JSD) to characterize how images differ across regions (or incomes). Note the Jensen-Shannon Distance is the square root of Jensen-Shannon divergence between two distributions, and is a standard metric to compare discrete distributions (Endres and Schindelin, 2003). A large JSD distance between two regions for a class indicates images differ across those regions.

**Some classes' factors vary significantly across incomes and regions; others remain consistent.** In Table 6(a), we show the most starkly different classes across incomes/regions, along with their most distinguishing factors. Consistently, we find the largest differences are between low and high incomes, but don't find a consistent pattern with regions. For classes with the largest differences across regions/incomes, we find these differences to be significant with an average JSD more than twice the median JSD of all classes (across incomes/regions). Among classes differing most across regions are those relating to animals (_chickens, pet foods_); those differing most across incomes relate to building structures (_roofs, ceiling, floors_). We report full tables for the most similar and dissimilar classes across regions and incomes in Appendix A.2. In contrast, other classes don't vary across incomes/regions such as _vegetable pots_, _phones_, and _pens_ as shown in Figure 6(b). This suggests that while some classes can vary drastically across incomes or regions, others are quite similar.

## 7 Related work

Rojas et al. (2022), Singh et al. (2022), Goyal et al., 2022) have used Dollar Street to understand disparities in model performance between geographic and income groups, finding that many models

    &  &  \\   & & low income & Africa & Asia \\  ResNet-50 & 32.2 & 25.2 & 18.6 & 24.4 \\ Texture debiased & **33.0** & **29.3** & **21.6** & **25.0** \\   

Table 2: Texture debiasing (Geirhos et al.) can improve performance across low income buckets and regions with lower performance for images marked with texture as a distinctive factor.

perform better on images from Europe and the Americas, as well as those from higher household incomes. While many of the aforementioned works focus on how model architectures affect disparity findings, additional studies (De Vries et al., 2019; Shankar et al., 2017) investigate the dataset itself to identify causes of variation in model performance, including the broader geographical distribution of images as compared to the model's training data. A number of datasets (Ramaswamy et al., 2023; Dubey et al., 2021; Kim et al., 2021) and dataset auditing tools (Wang et al., 2022) have since been developed with geographic diversity in mind. De Vries et al. (2019) investigated the use of English as a "base language" for data collection. Empirical studies have shown that the "concreteness" of English words can vary greatly where crowd-sourced annotators consider words like "human" and "bobsled" more concrete than words like "recreation" and "outage" (Brysbaert et al., 2014), and previous discussions of "class label perceptions" distinguish physical properties of a substance (such as orientation or texture) from a purpose relative to the specific being that interacts with the substance, such as being "sit-on-able" (Gibson, 1979). The relationship between the true meaning of a concept versus its perceptible form remains contested for both models (Bender and Koller, 2020) and humans (Phillips, 2019). Beyond Dollar Street, other works study variations in representations of concepts across visual factors including pose, background, occlusion, etc. within ImageNet (Idrissi et al., 2022) and collect supplementary multi-class labels (Yun et al., 2021; Beyer et al., 2020; Shankar et al., 2020). Barbu et al. (2019) created a benchmark to measure a model's robustness to backgrounds, rotations, and viewpoints.

## 8 Conclusion

In this work, we take a step towards explaining why disparities in object-recognition systems arise. We annotate images from the Dollar Street dataset with distinguishing factors in order to explain how objects differ across incomes and geographies. Using these labels, we identify vulnerabilities in CLIP, a foundation model with impressive zero-shot classification performance. We find disparities in model performance are associated with texture, occlusion, and darker lighting. Finally, we surface initial promising mitigations such as texture debiasing that can improve performance disparities. This shines light on a promising research direction leveraging techniques in robustness for fairness gains. In future work, we plan to explore further targeted mitigations can improve performance disparities in vision systems. While our conclusions are limited by the number of samples and representative diversity of the Dollar Street dataset, we hope by releasing our factor annotations we spur further research into equitable vision systems.

Figure 7: Classes with most stark differences in factor distributions by Jensen-Shannon Distance (JSD) across incomes/regions are listed in (a)a. Examples for roofs, the class with the largest JSD across incomes, and vegetable plots, the class with the smallest JSD across income are shown in (b)b.

### Limitations

While grounding our new annotations in the widely used Dollar Street dataset allows for easy adopting and extensibility of existing analyses, we are also exposed to limitations of the underlying dataset. For example, Dollar Street contains imbalances across object classes and some subjectivity in class definitions across groups (e.g. _most loved items_, _nicest shoes_). We attempt to address these gaps by filtering out classes with few samples and using only classes with an ImageNet mapping.

In addition, annotators from different regions may have different perceptions of what constitutes each image factor and object class. We address this by providing annotators with examples of image factors and conducting an additional round of annotation focused on understanding regional variations in rare perceptions (see Section 2.3 and Appendix A.1.4). However, there may be lingering discrepancies left undiscovered.

Furthermore, the factors we annotate (e.g. color, shape, pose) are only a subset of factors that can relate to model performance. While these factors are relevant to standard computer vision benchmarks , additional factors may prove insightful, especially for domain specific-tasks. For example, annotations of foliage and shadows may be useful for auditing autonomous driving models.

Finally, our analyses are only a first attempt at explaining discrepancies and performing mitigations. We hope that our rich annotations and dashboard for dataset understanding enable future studies into evaluations and mitigations of geographic disparities.