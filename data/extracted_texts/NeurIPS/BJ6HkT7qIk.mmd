# Self-Calibrating Conformal Prediction

Lars van der Laan

University of Washington

lvdlaan@uw.edu &Ahmed M. Alaa

UC Berkeley and UCSF

amalaa@berkeley.edu

###### Abstract

In machine learning, model calibration and predictive inference are essential for producing reliable predictions and quantifying uncertainty to support decision-making. Recognizing the complementary roles of point and interval predictions, we introduce _Self-Calibrating Conformal Prediction_, a method that combines Venn-Abers calibration and conformal prediction to deliver calibrated point predictions alongside prediction intervals with finite-sample validity conditional on these predictions. To achieve this, we extend the original Venn-Abers procedure from binary classification to regression. Our theoretical framework supports analyzing conformal prediction methods that involve calibrating model predictions and subsequently constructing conditionally valid prediction intervals on the same data, where the conditioning set or conformity scores may depend on the calibrated predictions. Real-data experiments show that our method improves interval efficiency through model calibration and offers a practical alternative to feature-conditional validity.

## 1 Introduction

Particularly in safety-critical sectors, such as healthcare, it is important to ensure decisions inferred from machine learning models are reliable, under minimal assumptions . As a response, there is growing interest in predictive inference methods that quantify uncertainty in model predictions via prediction intervals . Conformal prediction (CP) is a popular, model-agnostic, and distribution-free framework for predictive inference, which can be applied post-hoc to any prediction pipeline . Given a prediction issued by a black-box model, CP outputs a prediction interval that is guaranteed to contain the unseen outcome with a user-specified probability . However, a limitation of CP is that this prediction interval only provides valid coverage marginally, when averaged across all possible contexts - with 'context' referring to the information available for decision-making. Constructing informative prediction intervals that offer contextual-conditional coverage is generally unattainable without making additional distributional assumptions . Consequently, there has been an upsurge in research developing CP methods that offer weaker, yet practically useful, notions of conditional validity; see, e.g., .

In prediction settings, _model calibration_ is a desirable property of machine learning predictors that ensures that the predicted outcomes accurately reflect the true outcomes . Specifically, a predictor is calibrated for the outcome if the average outcome among individuals with identical predictions is close to their shared prediction value . Such a predictor is more robust against the over-or-under estimation of the outcome in extremes of predicted values. It also has the property that the best prediction of the outcome conditional on the model's prediction is the prediction itself, which facilitates transparent decision-making . There is a rich literature studying post-hoc calibration of prediction algorithms using techniques such as Platt's scaling , histogram binning , isotonic calibration , and Venn-Abers calibration .

Given the roles of both point and interval predictions in decision-making, we introduce a dual calibration objective that aims to construct (i) calibrated point predictions and (ii) associated prediction intervals with valid coverage conditional on these point predictions. Marrying model calibration andpredictive inference, we propose a solution to this objective that combines two post-hoc approaches -- Venn-Abers calibration [61; 60] and CP  -- to simultaneously provide point predictions and prediction intervals that achieve our dual objective in finite samples. In doing so, we extend the original Venn-Abers procedure from binary classification to the regression setting. Our theoretical and experimental results support the integration of model calibration into predictive inference methods to improve interval efficiency and interpretability.

## 2 Problem setup

### Notation

We consider a standard regression setup in which the input \(X^{d}\) corresponds to contextual information available for decision-making, and the output \(Y\) is an outcome of interest. We assume that we have access to a calibration dataset \(_{n}=\{(X_{i},Y_{i})\}_{i=1}^{n}\) comprising \(n\)_i.i.d._ data points drawn from an unknown distribution \(P:=P_{X}P_{Y|X}\). We assume access to a black-box predictor \(f:\), obtained by training an ML model on a dataset that is independent of \(_{n}\). Throughout this paper, we do not make any assumptions on the model \(f\) or the distribution \(P\). For a quantile level \((0,1)\), we denote the "pinball" quantile loss function \(_{}\) by \(_{}(f(x),y):=1(y f(x))(y-f(x))+1(y<f(x))(1- )(f(x)-y)\).

### Conditional predictive inference and a curse of dimensionality

Let \((X_{n+1},Y_{n+1})\) be a new data point drawn from \(P\) independently of the calibration data \(_{n}\). **Our high-level aim** is to develop a _predictive inference_ algorithm that constructs a prediction interval \(_{n}(X_{n+1})\) around the point prediction issued by the black-box model, i.e., \(f(X_{n+1})\). For this prediction interval to be deemed _valid_, it should _cover_ the true outcome \(Y_{n+1}\) with a probability \(1-\). Conformal prediction (CP) is a method for predictive inference that can be applied in a post-hoc fashion to any black-box model . The vanilla CP procedure issues prediction intervals that satisfy the marginal coverage condition:

\[(Y_{n+1}_{n}(X_{n+1})) 1-, \]

where the probability \(\) is taken with respect to the randomness in \(_{n}\) and \((X_{n+1},Y_{n+1})\). However, marginal coverage might lack utility in decision-making scenarios where decisions are context-dependent. A prediction band \(_{n}(x)\) achieving 95% coverage may exhibit arbitrarily poor coverage for specific contexts \(x\). Ideally, we would like this coverage condition to hold for each context \(x\), i.e., the conventional notion of "conditional validity" requires

\[(Y_{n+1}_{n}(X_{n+1})|X_{n+1}=x) 1-, \]

for all \(x\). However, previous work has shown that it is impossible to achieve (2) without distributional assumptions [58; 35; 4].

While context-conditional validity as in (2) is generally unachievable, it is feasible to attain weaker forms of conditional validity. Given any finite set of groups \(\) and a grouping function \(G:\), Mondrian-CP offers coverage conditional on group membership, that is, \((Y_{n+1}_{n}(X_{n+1})|G(X_{n+1},Y_{n+1})=g) 1- ,\  g\)[59; 47]. Expanding upon group- and context-conditional coverage, A _multicalibration_ objective was introduced in  that seeks to satisfy, for all \(h\) in an (infinite-dimensional) class \(\) of weighting functions (i.e., 'covariate shifts'), the property:

\[h(X_{n+1})(1-)-1\{Y_{n+1}_{n}(X_ {n+1})\}}=0. \]

Gibbs et al.  proposed a regularized CP framework for (approximately) achieving (3) that provides a means to trade off the efficiency (i.e., width) of prediction intervals and the degree of conditional coverage achieved. However, Barber et al.  and Gibbs et al.  establish the existence of a "curse of dimensionality": as the dimension of the context increases, smaller classes of weighting functions must be considered to retain the same level of efficiency. For group-conditional coverage, this curse of dimensionality manifests in the size of the subgroup class \(\) via its VC dimension . Thus, especially in data-rich contexts, prediction intervals with meaningful multicalibration guarantees over the context space may be too wide for decision-making.

### A dual calibration objective

In decision-making, both point predictions and prediction intervals play a role. For example, in scenarios with a low signal-to-noise ratio, prediction intervals may be too wide to directly inform decision-making, as their width is typically of the order of the standard deviation of the outcome. Point predictions might be used to guide decisions, while prediction intervals help quantify deviations of point predictions from unseen outcomes and assess the risk associated with these decisions.

Viewing the black-box model \(f\) as a scalar dimension reduction of the context \(x\), a natural relaxation of the infeasible objective of context-conditional validity in (2) is prediction-conditional validity, i.e., \(P(Y_{n+1}_{n}(X_{n+1})|f(X_{n+1})) 1-\). Prediction-conditional validity ensures that the interval widths adapt to the outputs of the model \(f()\), so that the intervals can be reliably used to quantify the deviation of model predictions from unseen outcomes. Since prediction-conditional validity only requires coverage conditional on a one-dimensional random variable, it avoids the curse of dimensionality associated with context-conditional validity. In addition, as illustrated in our experiments in Section 5 and Appendix C, when the heteroscedasticity (e.g., variance) in the outcome is a function of its conditional mean, prediction-conditional validity can closely approximate context-conditional validity, so long as the predictor estimates the conditional mean of the outcome sufficiently well.

Given the roles of both point and interval predictions in decision-making, we introduce a novel dual calibration objective, _self-calibration_, that aims to construct (i) calibrated point predictions and (ii) associated prediction intervals with valid coverage conditional on these point predictions. Formally, given the model \(f\) and calibration data \(_{n}\{X_{n+1}\}\), **our objective** is to post-hoc construct a calibrated point prediction \(f_{n+1}(X_{n+1})\) and a compatible prediction interval \(_{n+1}(X_{n+1})\) centered around \(f_{n+1}(X_{n+1})\) that satisfies the following desiderata:

1. **Perfectly Calibrated Point Prediction:**\(f_{n+1}(X_{n+1})=[Y_{n+1} f_{n+1}(X_{n+1})]\).
2. **Prediction-Conditional Validity:**\((Y_{n+1}_{n+1}(X_{n+1})|f_{n+1}(X_{n+1})) 1-\).

Desideratum (i) states that the point prediction \(f_{n+1}(X_{n+1})\) should be _perfectly calibrated_ -- or self-consistent  -- for the true outcome \(Y_{n+1}\). It is widely recognized that the calibration of model predictions is important to ensure their reliability, trustworthiness, and interpretability in decision-making . Desideratum (i) also improves interval efficiency by ensuring \(_{n+1}(X_{n+1})\) is centered around an unbiased prediction, meaning the interval's width is driven by outcome variation rather than by prediction bias. Desideratum (ii) is a prediction interval variant of (i) that ensures the prediction interval \(_{n+1}(X_{n+1})\) is calibrated with respect to the model \(f_{n+1}\), providing valid coverage for \(Y_{n+1}\) within contexts with the same _calibrated_ point prediction. We refer to a predictive inference algorithm simultaneously satisfying (i) and (ii) as _self-calibrating_, as such a procedure is automatically able to adapt to miscalibration in the model \(f()\) due to, e.g., model misspecification or distribution shifts, ensuring that the interval is constructed from a calibrated predictive model.

Self-calibration can also be motivated by a decision-making scenario where point predictions determine actions and prediction intervals are used to apply these actions selectively. When point predictions are sufficient statistics for actions, self-calibration implies that the point and interval predictions are accurate, on average, within the subset of all contexts receiving the same prediction and, therefore, the same action.

## 3 Self-Calibrating Conformal Prediction

A key advantage of CP is that it can be applied post-hoc to any black-box model \(f\) without disrupting its point predictions. However, desideratum (i) introduces a perfect calibration requirement for the point predictions of \(f\), thereby interfering with the underlying model specification. In this section, we introduce Self-Calibrating CP (SC-CP), a modified version of CP that is self-calibrating in that it satisfies (i) and (ii), while preserving all the favorable properties of CP, including its finite-sample validity and post-hoc applicability. Before describing our complete procedure in Section 3.3, we provide background on point calibration and propose Venn-Abers calibration for regression.

[MISSING_PAGE_FAIL:4]

a point prediction as follows:

\[_{n+1,x}(x):=f^{}_{n+1,x}(x)+)}_{n}(x)-f ^{(x,y_{})}_{n}(x)}{y_{}-y_{}}\{_{n}-f^{} _{n+1,x}(x)\}, \]

where \(f^{}_{n+1,x}(x):=\{f^{(x,y_{})}_{n}(x)+f^{(x,y_{} )}_{n}(x)\}\) is the midpoint of the multi-prediction and \(_{n}:=_{i=1}^{n}Y_{i}\). The behavior of \(_{n+1,x}(x)\) is natural; it shrinks the point prediction \(f^{}_{n+1,x}(x)\) towards the average outcome \(_{n}\) a well-calibrated prediction) proportional to how uncertain we are in the calibration of \(f^{}_{n+1,x}(x)\). The ratio \(-y_{}}(f^{(x,y_{})}_{n}(x)-f^{(x,y_{})}_{n}(x))\) measures the sensitivity of isotonic regression to the addition of a single data point to \(_{n}\), and a value closer to 1 corresponds to a higher degree of overfitting. In the extreme case where the calibration dataset is very large, we have \(f^{(x,y_{})}_{n}(x) f^{(x,y_{})}_{n}(x)\), implying that \(_{n+1,x}(x) f^{}_{n+1,x}(x)\). Conversely, in the opposite extreme where the calibration dataset is very small and isotonic regression overfits, we have \(f^{(x,y_{})}_{n}(x) y_{}\) and \(f^{(x,y_{})}_{n}(x) y_{}\), in which case \(_{n+1,x}(x)_{n}\). We could replace \(_{n}\) in (4) with any reference predictor, such as one calibrated using Platt's scaling or quantile-binning.

### Conformalizing Venn-Abers Calibration

In this section, we propose Self-Calibrating Conformal Prediction, which conformalizes the Venn-Abers calibration procedure to provide prediction intervals centered around the Venn-Abers multi-prediction that are self-calibrated in the sense of desiderata (i) and (ii).

A simple, albeit naive, strategy for achieving (i) and (ii) without finite-sample guarantees involves using the dataset \(_{n}\) to calibrate point predictions of \(f()\) through isotonic regression, and then constructing prediction intervals from the \(1-\) empirical quantiles of prediction errors within subgroups defined by unique values of the calibrated point predictions. To motivate our SC-CP algorithm, we introduce an infeasible variant of this seemingly naive procedure that is valid in finite samples, but can only be computed by an oracle that knows the unseen outcome \(Y_{n+1}\). In

Figure 1: Example SC-CP output with small \(_{n}\) (\(n=200\)).

this oracle procedure, we compute a perfectly calibrated prediction \(f^{*}_{n+1}(X_{n+1}):=^{*}_{n+1}(f(X_{n+1}))\) by isotonic calibrating \(f\) using the oracle-augmented calibration set \(\{(X_{i},Y_{i})\}_{i=1}^{n+1}\), where \(^{*}_{n+1}*{argmin}_{_{}}_{i=1}^ {n+1}\{Y_{i}-(f(X_{i}))\}^{2}.\) Next, we compute the conformity scores \(S^{*}_{i}:=|Y_{i}-f^{*}_{n+1}(X_{i})|\) as the absolute residuals of the calibrated predictions. An oracle prediction interval is then given by \(C^{*}_{n+1}(X_{n+1}):=f^{*}_{n+1}(X_{n+1})^{*}_{n+1}(X_{n+1})\), where \(^{*}_{n+1}(X_{n+1})\) is the empirical \(1-\) quantile of conformity scores with calibrated predictions identical to \(X_{n+1}\), that is, scores in the set \(\{S^{*}_{i}:f^{*}_{n+1}(X_{i})=f^{*}_{n+1}(X_{n+1}),\,i[n+1]\}\). Importantly, isotonic regression, which is an outcome-adaptive histogram binning method, ensures that the calibrated model \(f^{*}_{n+1}\) is piece-wise constant, with a sufficiently large number of observations averaged within each constant segment-typically on the order of \(n^{2/3}\). Consequently, the empirical quantile \(^{*}_{n+1}(X_{n+1})\) is generally stable with relatively low variability across realizations of \(_{n}\). In our proofs, using the first-order conditions characterizing the optimizer \(_{n+1}\) and exchangeability, we show that \(f^{*}_{n+1}(X_{n+1})=[Y_{n+1}|f^{*}_{n+1}(X_{n+1})]\), so that desideratum (i) is satisfied. Furthermore, we establish that the interval \(C^{*}_{n+1}(X_{n+1})\) achieves desideratum (ii), i.e., \((Y_{n+1} C^{*}_{n+1}(X_{n+1}) f^{*}_{n+1}(X_{n+1})) 1-\). To do so, our key insight is that \(^{*}_{n+1}(X_{n+1})\) corresponds to the evaluation of the function \(^{*}_{n+1}\) computed via prediction-conditional quantile regression as:

\[^{*}_{n+1}*{argmin}_{ f^{*}_{n+1},: }_{i=1}^{n+1}_{} ( f^{*}_{n+1}(X_{i}),S_{i}).\]

The first-order conditions characterizing the optimizer \(^{*}_{n+1}\) combined with the exchangeability between \(_{n}\) and \((X_{n+1},Y_{n+1})\) can be used to show that \(C^{*}_{n+1}(X_{n+1})\) is _multi-calibrated_ against the class of weighting functions \(_{n+1}:=\{ f^{*}_{n+1},\,: \}\) in the sense of (3). Using first-order conditions to establish the theoretical validity of conformal prediction was also applied by Gibbs et al.  to demonstrate the multi-calibration of oracle prediction intervals obtained from quantile regression over a fixed class \(\). In our case, quantile regression is performed over a data-dependent function class \(_{n+1}\), learned from the calibration data, which introduces additional challenges in our proofs.

Our SC-CP method, which is outlined in Alg. 2, follows a similar procedure to the above oracle procedure. Since the new outcome \(Y_{n+1}\) is unobserved, we instead iterate the oracle procedure over all possible imputed values \(y\) for \(Y_{n+1}\). As in Alg. 1., this yields a set of isotonic calibrated models \(f_{n,X_{n+1}}:=\{f^{(X_{n+1},y)}_{n}:y\},\) where \(f_{n+1}(X_{n+1})\) is the Venn-Abers multi-prediction of \(Y_{n+1}\). Then, for each \(y\) and \(i[n]\), we define the _self-calibrating conformity scores_\(S^{(X_{n+1},y)}_{i}:=|Y_{i}-f^{(X_{n+1},y)}_{n}(X_{i})|\) and \(S^{(X_{n+1},y)}_{n+1}:=|y-f^{(X_{n+1},y)}_{n}(X_{n+1})|\), where the dependency of our scores on the imputed outcome \(y\) is akin to Full (or transductive) CP . Our SC-CP interval is then given by \(_{n+1}(X_{n+1}):=\{y:S^{(X_{n+1},y)}_{n+1}^{ (X_{n+1},y)}_{n}(X_{n+1})\}\), where \(^{(X_{n+1},y)}_{n}(X_{n+1})\) is the empirical \(1-\) quantile of the level set \(\{S^{(X_{n+1},y)}_{i}:f^{(X_{n+1},y)}_{n}(X_{i})=f^{(X_{n+1},y)}_{n}(X_{n+1}), \,i[n+1]\}\). By definition, \(_{n+1}(X_{n+1})\) covers \(Y_{n+1}\) if, and only if, the oracle interval \(C^{*}_{n+1}(X_{n+1})\) covers \(Y_{n+1}\), thereby inheriting the self-calibration of \(C^{*}_{n+1}(X_{n+1})\). Formally, \(_{n+1}(X_{n+1})\) is a set, but it can be converted to an interval by taking its range, with little efficiency loss.

### Computational considerations

The main computational cost of Alg. 1 and Alg. 2 is in the isotonic calibration step, executed for each \(y\). Isotonic regression  can be scalably and efficiently computed using implementations of xgboost for univariate regression trees with monotonicity constraints. Similar to Full (or transductive) CP , Alg. 2 may be computationally infeasible for non-discrete outcomes, and can be approximated by iterating over a finite subset of \(\). In our implementation, we iterate over a grid of \(\) and use linear interpolation to impute the threshold \(^{(x,y)}_{n}(x)\) and score \(S^{(x,y)}_{n+1}\) for each \(y\). As with Full CP and multicalibrated CP , Alg. 1 and Alg. 2 must be separately applied for each context \(x\). The algorithms depend solely on \(x\) through its prediction \(f(x)\), so we can approximate the outputs for all \(x\) by running each algorithm for a finite number of \(x\) corresponding to a finite grid of the 1D output space \(f()=\{f(x):x\}\). In addition, both algorithms are fully parallelizable across both the input context \(x\) and the imputed outcome \(y\). In our implementation, we use nearest neighbor interpolation in the prediction space to impute outputs for each \(x\). In our experiments with sample sizes ranging from \(n=5000\)to \(40000\), quantile binning of both \(f()\) and \(\) into 200 equal-frequency bins enables execution of Alg. 1 and Alg. 2 across all contexts in minutes with negligible approximation error.

## 4 Theoretical guarantees

In this section, under exchangeability of the data, we establish that the Venn-Abers multi-prediction \(f_{n,X_{n+1}}(X_{n+1}):=\{f_{n}^{(X_{n+1},y)}(X_{n+1}):y\}\) and SC-CP interval \(_{n+1}(X_{n+1})\) output by Alg. 2 satisfy desiderata (i) and (ii) in finite samples and without distributional assumptions. Under an _iid_ condition, we further establish that, asymptotically, the Venn-Abers calibration step within the SC-CP algorithm results in better point predictions and, consequently, more efficient prediction intervals.

The following theorem establishes that the Venn-Abers multi-prediction is perfectly calibrated in the sense of , containing a perfectly calibrated point prediction of \(Y_{n+1}\) in finite samples.

* _Exchangeability:_\(\{(X_{i},Y_{i})\}_{i=1}^{n+1}\) are exchangeable.
* _Finite second moment:_\(E_{P}[Y^{2}]<\).

**Theorem 4.1** (Perfect calibration of Venn-Abers multi-prediction).: _Under Conditions C1 and C2, the Venn-Abers multi-prediction \(f_{n,X_{n+1}}(X_{n+1})\) almost surely satisfies the condition \(f_{n}^{(X_{n+1},Y_{n+1})}(X_{n+1})=[Y_{n+1} f_{n}^{(X_{n+1},Y_{ n+1})}(X_{n+1})]\)._

Theorem 4.1 generalizes an analogous result by  for the special case of binary classification. Even in this special case, our proof is novel and elucidates how Venn-Abers calibration uses exchangeability with the least-squares loss in a manner analogous to how CP uses exchangeability with the quantile loss .

The following theorem establishes desideratum (ii) for the interval \(_{n+1}(X_{n+1})\) with respect to the oracle prediction \(f_{n}^{(X_{n+1},Y_{n+1})}(X_{n+1})\) of Theorem 4.1. In what follows, let \((n)\) be a given sequence that grows polynomially logarithmically in \(n\).

* The conformity scores \(|Y_{i}-f_{n}^{(X_{n+1},Y_{n+1})}(X_{i})|\), \( i[n+1]\), are almost surely distinct.
* The number of constant segments for \(f_{n}^{(X_{n+1},Y_{n+1})}\) is at most \(n^{1/3}(n)\).

**Theorem 4.2** (Self-calibration of prediction interval).: _Under C1, it holds almost surely that_

\[(Y_{n+1}_{n+1}(X_{n+1}) f_{n}^{(X_{n+1},Y_{n+ 1})}(X_{n+1})) 1-.\]

_If also C3 and C4 hold, then \(|-(Y_{n+1}_{n+1}(X_{n+1 }) f_{n}^{(X_{n+1},Y_{n+1})}(X_{n+1}))|(n)}{n^{2/3}}\)._

Theorem 4.2 says that \(_{n+1}(X_{n+1})\) satisfies desideratum (ii) with coverage that is, on average, nearly exact up to a factor \((n)}{n^{2/3}}\). Notably, the deviation error from exact coverage tends to zero at a fast dimensionless rate and, therefore, does not suffer from a "curse of dimensionality". Condition C3 is only required to establish the upper coverage bound and is standard in CP - see, e.g., [36; 21]. Although it may fail for non-continuous outcomes, this condition can be avoided by adding a small amount of noise to all outcomes . The constant segment number of \(n^{1/3}(n)\) in C4 is motivated by the theoretical properties of isotonic regression; Assuming C2 and continuous differentiability of \(t E_{P}[Y f(X)=t]\), it is shown in  that the number of observations in a given constant segment of an isotonic regression solution concentrates in probability around \(n^{2/3}\). In general, without C4, our proof establishes a miscoverage upper bound of \([N_{n+1}]\), where \([N_{n+1}]\) is the expected number of constant segments of \(f_{n}^{(X_{n+1},Y_{n+1})}\).

The next theorem examines the interaction between calibration and CP within SC-CP in terms of efficiency of the self-calibrating conformity scores. In the following, let \(x,y\) be arbitrary. For each \(_{iso}\), define the \(\)-transformed conformity scoring function \(S_{}:(x^{},y^{})|y^{}- f(x^{})|\). Let \(_{0}:=_{_{iso}}\{S_{}(x^ {},y^{})\}^{2}dP(x^{},y^{})\) be the optimal isotonic transformation of \(f()\) that minimizes the population mean-square error. Define the self-calibrating conformity scoring function as \(S_{n}^{(x,y)}(x^{},y^{}):=|y^{}-f_{n}^{(x,y)}(x^{})|\), where \(f_{n}^{(x,y)}\) is obtained as in Alg. 1.

**C5)**: _Independent data:_ \(\{(X_{i},Y_{i})\}_{i=1}^{n+1}\) _are iid._
**C6)**: _Bounded outcomes:_ \(\) _is a uniformly bounded set._

**Theorem 4.3**.: _Under C5 and C6, we have \(\{S_{n}^{(x,y)}(x^{},y^{})-S_{_{0}}(x^{},y^{} )\}^{2}dP(x^{},y^{})=O_{p}(n^{-2/3})\)._

The above theorem indicates that the self-calibrating scoring function \(S_{n}^{(x,y)}\) used in Alg. 2 asymptotically converges in mean-square error to the oracle scoring function \(S_{_{0}}\) at a rate of \(n^{-2/3}\). Since the oracle scoring function \(S_{_{0}}\) corresponds to a model \(_{0} f\) with better mean square error than \(f\), we heuristically expect that the Venn-Abers scoring function \(S_{n}^{(x,y)}\) will translate to narrower CP intervals, at least asymptotically. We provide experimental evidence for this heuristic in Section 5.

**Limitations.** The perfectly calibrated prediction \(f_{n}^{(X_{n+1},Y_{n+1})}(X_{n+1})\), guaranteed to lie by Theorem 4.1 in the Venn-Abers multi-prediction, typically cannot be determined precisely without knowledge of \(Y_{n+1}\). However, the stability of isotonic regression implies that the width of multi-prediction \(f_{n+1}(X_{n+1})\) shrinks towards zero very quickly as the size of the calibration set increases . Moreover, the large-sample theory for isotonic calibration in  demonstrates that the \(^{2}\)-calibration error of each model \(f_{n}^{(X_{n+1},y)}\) with \(y\) is \(O_{p}(n^{-2/3})\). One caveat of SC-CP intervals is that desideratum (ii) is satisfied with respect to the unknown, oracle point prediction \(f_{n}^{(X_{n+1},Y_{n+1})}(X_{n+1})\). However, we know that this oracle prediction lies within the Venn-Abers multi-prediction by Theorem 4.1, and its value can be determined with high precision with relatively small calibration sets  -- see, e.g., Figure 1. These limitations appear to be unavoidable as perfectly calibrated point predictions can generally not be constructed in finite samples without oracle knowledge .

### Related work

The work of  proposes a regression extension of Venn-Abers calibration that differs from ours, both algorithmically and in its objective. While our extension constructs a calibrated point prediction \(f(X)\) of \(Y\) such that \(f(X)=[Y f(X)]\), their approach uses the original Venn-Abers calibration procedure of  to construct a distributional prediction \(f_{t}(X)\) of \(1(Y t)\) that satisfies \(f_{t}(X)=(Y t f_{t}(X))\) for \(t\).

The impossibility results of  imply that any universal procedure providing prediction-conditionally calibrated intervals must explicitly or implicitly discretize the output of the model \(f()\). The works of  and  apply Mondrian CP  within leaves of a regression tree \(f\) to construct prediction intervals with prediction-conditional validity. However, this approach is restricted to tree-based predictors and does not guarantee calibrated point predictions and self-calibrated intervals. Mondrian conformal predictive distributions were applied within bins of model predictions in  to satisfy a coarser, distributional form of prediction-conditional validity. A limitation of Mondrian-CP approaches to prediction-conditional validity is that they require pre-specification of a binning scheme for the predictor \(f()\), which introduces a trade-off between model performance and the width of prediction intervals, and they do not perform point calibration (desideratum (i)) and, thereby, do not guarantee self-calibration. In contrast, SC-CP data-adaptively discretizes the predictor \(f()\) using isotonic calibration and, in doing so, provides calibrated predictions, improved conformity scores, and self-calibrated intervals.

Other notions of conditional validity have been proposed that, like prediction-conditional validity and self-calibration, avoid the curse of dimensionality of context-conditional validity. In the multiclassification setup,  and  use Mondrian CP to provide prediction intervals with valid coverage conditional on the class label (i.e., outcome). In , Mondrian CP is applied within bins categorized by context-specific difficulty estimates, such as conditional variance estimates. Multivalid-CP  offers coverage based on a threshold defining the prediction interval. For multiclassification,  propose a procedure for attaining valid coverage conditional on the prediction set size .

## 5 Real-Data Experiments: predicting utilization of medical services

### Experimental setup

In this experiment, we illustrate how prediction-conditional validity can approximate context-conditional validity when the heteroscedasticity in outcomes is strongly associated with modelpredictions, thereby ensuring validity across critical subgroups without their pre-specification. We analyze the Medical Expenditure Panel Survey (MEPS) dataset , supplied by the Agency for Healthcare Research and Quality , which was used in  for Mondrian CP with fairness applications. We use the preprocessed dataset acquired using the Python package cqr, also associated with . This dataset contains \(n=15,656\) observations and \(d=139\) features, and includes information such as age, marital status, race, and poverty status, alongside medical service utilization. Our objective is to predict each individual's healthcare system utilization, represented by a score that reflects visits to doctors' offices, hospital visits, etc. Following , we designate race as the sensitive attribute \(A\), aiming for _equalized coverage_, where \(A=0\) represents non-white individuals (\(n_{0}=9640\)) and \(A=1\) represents white individuals (\(n_{1}=6016\)). The outcome variable \(Y\) is transformed by \(Y=(1+)\) to address the skewness of the raw score. In Appendix B, we present additional experimental results for the _Concrete_, _Community_, _STAR_, _Bike_, and _Bio_ datasets used in  and publicly available in the Python package cqr, associated with  and .

We randomly partition the dataset into three segments: a training set (50%) for model training, a calibration set (30%) for CP, and a test set (20%) for evaluation. For training the initial model \(()\), we use the xgboost implementation of gradient boosted regression trees , where maximum tree depth, boosting rounds, and learning rate are tuned using 5-fold cross-validation. We consider two settings for training the model. In **Setting A**, we train the initial model on the untransformed outcomes and then transform the predictions as \((1+)\), which makes the model predictive but poorly calibrated because it overestimates the true outcomes, in light of Jensen's inequality. In **Setting B**, we train the initial model on the transformed outcomes, leading to fairly well-calibrated predictions. In both settings, calibration and evaluation are applied to the transformed outcomes.

For direct comparisons, we compare **SC-CP** with baselines that leverage the standard absolute residual scoring function \(S(x,y):=|y-f(x)|\) and target either marginal validity or prediction-conditional validity. The baselines are: **Marginal CP**, **Mondrian** CP with categories defined by bins of model predictions [59; 9], **COR** with model predictions used as features, and the **Kernel**-smoothed conditional CP approach of  with model predictions \(\{f(X_{i})\}_{i=1}^{n}\) used as features and bandwidth tuned with cross-validation. Due to the slow computing time of the implementation provided by , we apply **Kernel** on a subset of the calibration data of size \(n_{cal}=500\). **SC-CP** is implemented as described in Alg. 2, using isotonic regression constrained to have at least 20 observations averaged within each constant segment to mitigate overfitting (via the minimum child weight argument of xgboost). The miscoverage level is taken to be \(=0.1\). **SC-CP** provides calibrated point predictions and self-calibrated intervals, while the **Mondrian** and **Kernel** baselines offer approximate prediction-conditional validity, and **Marginal** and **CQR** only guarantee marginal coverage. We report empirical coverage, average interval width, and calibration error of model predictions in the test set within the sensitive attribute. Calibration error is defined as the mean error of the point predictions, \([-Y A]\) within the sensitive attribute \(A\), which measures model over- or under-confidence. For **SC-CP**, we use the calibrated point predictions from (4), while the original point predictions are used for **Marginal**, **Mondrian**, and **Kernel**. For **CQR**, we use an estimate of the conditional median, obtained from a separate xgboost quantile regression model, as the point prediction. We note that, since quantiles are preserved under monotone transformations of the outcome, we expect the conditional median model of **CQR** to be well-calibrated, at least in a median sense, in both **Setting A** and **Setting B**. We include the baseline **Mondrian\({}^{*}\)** for direct comparison with SC-CP, in which **Mondrian** is applied with the same number of prediction bins as data-adaptively selected by SC-CP.

### Results and discussion

The experimental results for each setting are depicted in Figure 2. Each panel's left-most plot showcases a calibration plot  for **SC-CP**, illustrating original and calibrated predictions alongside prediction bands. On the right, the panels display prediction bands of our baselines as a function of the original model predictions. Visually, as expected by Theorem 4.2, the **SC-CP** bands adapt to outcome heteroscedasticity within model predictions, while **Marginal** lacks adaptation, **Mondrian** under-adapts due to insufficient bins, and **Kernel** adapts but offers wider intervals for large predictions where observations are sparse. The bands of **CQR** appear adaptive and similar to those of **SC-CP**, however, do not gaurantee finite-sample prediction-conditional validity. The calibration plots reveal that heteroscedasticity in outcomes is primarily driven by their mean, suggesting that prediction-conditional validity may approximate context-conditional validity. This heuristic is supported by the tables in Figure 2, which display empirical coverage, average interval width, and calibration error within the sensitive attribute (\(A\)) for all methods. In **Setting A**, the base regression model \(f()\) are poorly calibrated, i.e., \([f(X)-Y A]\) is not close to \(0\), resulting in wider intervals, overconfidence in point predictions, and decreased interpretability for the baselines, as their intervals center around biased point predictions. In contrast, being self-calibrated, **SC-CP** corrects the calibration error in \(f\), achieving the smallest interval widths and well-calibrated point predictions in both settings, as guaranteed by Theorem 4.1. In both settings, the quantile regression model of **CQR** appears to have worse calibration than **SC-CP**, which may be due to the median differing from the mean because of the skewness of the outcomes. Additionally, **SC-CP** predictions achieves a smaller difference in calibration error between the two subgroups than **Marginal**, **Mondrian**, and **Kernel**, suggesting they are less discriminatory and more fair . **SC-CP** and **Kernel** achieve the desired coverage level of \(1-=0.9\) in each subgroup and setting, whereas **Marginal** exhibits over- or under-coverage in each subgroup. **Mondrian** tends to under-cover with \(5\) and \(10\) bins and only attains good coverage when using the same binning number data-adaptively selected by **SC-CP**, highlighting its sensitivity to the pre-specified binning scheme. **CQR** attains good coverage in the \(A=0\) group but undercovers in the \(A=1\) group, which may be explained by **CQR** only guaranteeing marginal coverage in finite samples. Even with **SC-CP** having higher coverage, the intervals of **SC-CP** are narrower than those of **Kernel** and **Mondrian\({}^{*}\)**. This provides experimental evidence that calibration improves conformity scores and translates into greater interval efficiency, as suggested by Theorem 4.3.

## 6 Extensions

Our theoretical techniques can be used to analyze conformal prediction methods that involve the calibration of model predictions followed by the construction of conditionally valid prediction intervals. Our analysis can be adapted to the general case where either the conformity score or the conditioning variable depends on the calibrated model prediction. While we use the absolute residual conformity score in our work, SC-CP can be applied to other conformity scores, such as the normalized absolute residual scoring function , allowing for the inclusion of context-specific difficulty estimates in the SC-CP procedure. Although we use Venn-Abers calibration in SC-CP, our analysis also applies to other binning calibration methods, such as Venn calibration [61; 60]. Thus, we can replace the isotonic calibration step in Alg. 1 and 2, for example, with histogram binning . Additionally, a group-valid form of SC-CP can be achieved by applying Alg. 2 separately within subgroups, similar to Multivalid CP . Interesting areas for future work involve integrating point calibration with conformal prediction methods for predictive models beyond regression, such as the isotonic calibration of quantile predictions in conformal quantile regression .

Figure 2: **MEPS-21 dataset:** Calibration plot for SC-CP, prediction bands for SC-CP and baselines, and empirical coverage, width, and calibration error within sensitive subgroup.