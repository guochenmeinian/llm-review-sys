# Normal-GS: 3D Gaussian Splatting with

Normal-Involved Rendering

 Meng Wei\({}^{1}\) Qianyi Wu\({}^{1}\) Jianmin Zheng\({}^{2}\) Hamid Rezatofighi\({}^{1}\) Jianfei Cai\({}^{1}\)

\({}^{1}\)Monash Univeristy \({}^{2}\)Nanyang Technological University

{meng.wei,qianyi.wu,hamid.rezatofighi,jianfei.cai}@monash.edu

{ASJMZheng}@ntu.edu.sg

###### Abstract

Rendering and reconstruction are long-standing topics in computer vision and graphics. Achieving both high rendering quality and accurate geometry is a challenge. Recent advancements in 3D Gaussian Splatting (3DGS) have enabled high-fidelity novel view synthesis at real-time speeds. However, the noisy and discrete nature of 3D Gaussian primitives hinders accurate surface estimation. Previous attempts to regularize 3D Gaussian normals often degrade rendering quality due to the fundamental disconnect between normal vectors and the rendering pipeline in 3DGS-based methods. Therefore, we introduce **Normal-GS**, a novel approach that integrates normal vectors into the 3DGS rendering pipeline. The core idea is to model the interaction between normals and incident lighting using the physically-based rendering equation. Our approach re-parameterizes surface colors as the product of normals and a designed Integrated Directional Illumination Vector (IDIV). To save memory usage and simplify the optimization, we employ an anchor-based 3DGS to implicitly encode locally-shared IDIVs. Additionally, Normal-GS leverages optimized normals and Integrated Directional Encoding (IDE) to accurately model specular effects, enhancing both rendering quality and surface normal precision. Extensive experiments demonstrate that Normal-GS achieves near state-of-the-art visual quality while obtaining accurate surface normals and preserving real-time rendering performance.

## 1 Introduction

Radiance Fields have emerged as a prominent representation in 3D vision, largely propelled by the pioneering advancements of Neural Radiance Fields (NeRF) . Despite their success, NeRFs are hindered by prolonged rendering times and cumbersome training processes. Recently, 3D Gaussian Splatting (3DGS) has been introduced to represent Radiance Fields using millions of 3D Gaussians, each endowed with additional attributes such as opacity and color . 3DGS significantly enhances rendering efficiency through CUDA-accelerated rasterization , achieving comparable or superior fidelity to NeRF-based models. The advantages of 3DGS have garnered significant attention , prompting numerous studies aiming at enhancing its rendering capabilities  and improving the quality of its underlying geometry .

However, a critical issue continues to plague the development of 3DGS: _the quality of appearance and geometry seemingly oscillates like a seesaw._ Efforts to refine the geometry often involve the incorporation of regularization techniques  or additional geometry supervision , which may compromise rendering fidelity. Conversely, advancements in appearance modeling, such as the integration of implicit networks  and sophisticated shading attributes , struggle to concurrently elevate both appearance and geometry quality. This raises a pertinent question: _Is it feasible to achieve a high-quality appearance while also capturing precise underlying geometry information in 3DGS?_Revisiting the classical rendering equation , we identify a fundamental issue in the appearance-geometry conflict: the disconnection between surface normals and the rendering process in current 3DGS methods. Specifically, the existing methods render pixel colors by alpha-blending colored 3D Gaussians. The color of each Gaussian is queried by the function value among Spherical Harmonics  along the ray direction, which disregards the contributions of surface normals. This oversight hampers the ability to achieve a balanced integration of appearance and geometry. Our framework draws inspiration from the conventional rendering equation, which incorporates surface normals into appearance calculations. Although prior attempts have explored this integration in contexts such as inverse rendering [16; 17] and specular appearance modeling [7; 5], they leverage simplified model  or approximation approaches like split-sum  or global environmental map [20; 21] to decompose appearance into several individual components with meticulous regularization terms, which either compromising in rendering quality or geometric accuracy. This challenge highlights the complexity of balancing various components for physically-based rendering, prompting our design of a straightforward method to integrate normal information explicitly and simply into Gaussian appearance modeling.

In this work, we propose a normal-involved shading for 3DGS, called **Normal-GS**, for high-quality rendering and accurate normal estimation, while maintaining real-time performance. Our method directly accounts for normal contributions in the 3DGS rendering pipeline, enabling gradient signals to backpropagate properly to better align 3D Gaussians. Specifically, Normal-GS considers the normal and incident lighting interactions at the surface from the physically-based shading perspective and explicitly models normals' contributions for diffuse and specular components. By considering the low-frequency nature of the incident light field, we further implicitly model it with MLPs. With our strategy, compared with the state-of-the-art (SOTA) 3DGS methods, we achieve competitive rendering quality, more accurate normal information, and real-time rendering, see one example in Fig. 1. Our contributions are summarized below.

* We propose a new formulation to involve normal information in appearance modeling for 3DGS, which strives for a good balance between appearance and geometry modeling.
* We leverage an effective structure to regularize our framework using anchor-based MLPs, without introducing many extra regularization terms and achieving better quality.
* Extensive experiments demonstrate our framework for obtaining superior view synthesis and normal estimation. Our design could serve as a plug-in component for 3DGS approaches.

## 2 Related Works

Radiance Fields: From Neural Radiance Fields to 3D Gaussian Splatting.Neural Radiance Field (NeRF) has significantly influenced the field of 3D vision with its impressive novel view synthesis capabilities . NeRF catalyzed advancements in neural rendering, inspiring a multitude of subsequent research focused on enhancing rendering quality under complex lighting and material conditions [22; 23; 24], varying camera distributions [25; 26; 27], and scalability for large scenes [28; 29; 30].

Figure 1: The ”seesaw” characteristic between the rendering quality and the normal accuracy of 3DGS-based methods. Our **Normal-GS** is able to efficiently achieve accurate normal estimation while preserving competitive rendering quality. Our method successfully captures the normals of the cover of the semi-transparent box behind the table.

However, the implicit representation used in NeRF, which relies on a Multi-layer Perceptron (MLP) to compute density and radiance for any given 3D position and ray direction, necessitates extensive computations. This inefficiency has spurred efforts to enhance the practicality of NeRF, with notable strides made in accelerating its processing efficiency [2; 31; 32].

Among these advancements, 3D Gaussian Splitting (3DGS) has emerged as a promising solution . By explicitly modeling 3D scenes using sets of 3D Gaussians and employing tile-based GPU rasterization, 3DGS achieves real-time rendering with competitive quality. Its success has spurred applications in areas like 3D generation [33; 34], physical simulation , and sparse view reconstruction [36; 37], despite its higher memory requirements for scene representation. Innovations such as feature anchoring in Scaffold-GS , value pruning and quantization techniques [38; 39; 40] and mining spatial relationship [41; 42; 43] are among the efforts to reduce the memory footprint of 3DGS, thereby enhancing storage efficiency and rendering fidelity.

Geometry and Appearance in 3DGS: From Surface Reconstruction to Inverse Rendering.Geometry and appearance are central to 3D reconstruction, and the discrete and explicit nature of 3DGS can result in noisy underlying geometry [8; 10; 11; 13; 44]. SuGaR  was an initial attempt to refine mesh extraction from 3DGS, applying regularization to align the Gaussian Splatting with the actual surface contours. To better define geometry, capturing normal information is essential, leading to innovations like transforming 3DGS into 2D Gaussian Splatting (2DGS) and Gaussian Surfels [10; 44]. These methods apply regularization to depth and normal rendering, ensuring that the Gaussians are appropriately distributed across surfaces. While these approaches improve surface reconstruction, they often sacrifice rendering fidelity in novel view synthesis due to the lack of a clear relationship between geometry and appearance [7; 10; 44].

In appearance modeling, the focus has shifted to inverse rendering for 3DGS, which aims to separate scene elements into materials, lighting, and geometries. This inherently unconstrained problem challenges traditional rendering equations. Techniques such as Monte Carlo integration, point-based ray tracing [17; 45], and baking  are employed to handle the complex integrals, often relying on simplified models like the Disney Bidirectional Reflectance Distribution Function (BRDF)  and approximation methods such as split-sum [16; 17; 45; 46] or environmental mapping [7; 46; 47]. Despite their efforts, these simplifications generally result in lower rendering quality, as reflected in reduced Peak Signal-to-Noise Ratios (PSNR) [16; 17; 45], failing to match the original 3DGS.

It is worth noting that some concurrent works [47; 48] have a similar motivation as ours.  simultaneously enhance geometry accuracy and rendering quality by incorporating a laborious dual-branch framework, which uses 3DGS for appearance rendering and an implicit neural surface for geometry production. While this approach integrates the strengths of each system, it is hampered by slower training speeds relative to standalone 3DGS, due to its additional networks for surface representation.  also distills geometry information from an extra neural implicit surface network for deferred rendering. Our solution proposes a new perspective from a rendering standpoint to consider geometry and appearance simultaneously without an extra implicit network for the surface.

## 3 Method

Our goal is to simultaneously enhance both image quality and normal estimation capability of 3DGS, while maintaining real-time rendering. To achieve this, we first analyze the existing rendering pipeline in 3DGS to identify its limitations in not involving surface normals in color modeling (Sec. 3.1). We then design a normal-involved rendering strategy that aligns with the physically based rendering equation. Our method effectively models interactions between normals and incident lighting, parameterized by the proposed Integrated Directional Illumination Vectors (IDIV) (Sec. 3.2). We employ an anchor-based GS to implicitly encode locally shared IDIVs to save memory and aid optimization (Sec. 3.3). The training details of our framework are provided in Sec. 3.4.

### Preliminary

3D Gaussian Splatting .3DGS models scenes using a set of discrete 3D Gaussians, each defined by its spatial mean \(\) and covariance matrix \(\):

\[G()=(-(-)^{T}^{-1}(-)).\] (1)The covariance matrix \(\) is parameterized by a scaling matrix \(S\) and a rotation matrix \(R\), such that \(=RSS^{T}R^{T}\), ensuring it remains positive semi-definite during optimization.

Each 3D Gaussian is associated with a color \(c\) and an opacity \(\). During rendering, these Gaussians are projected (rasterized) onto the image plane, forming 2D Gaussian splats \(G^{}(x)\), as described in . The 2D Gaussian splats are sorted from front to back tile-wisely, and \(\)-blending  is performed for each pixel \(x\) to render its color as follows:

\[C(x)=_{i N}c_{i}_{i}_{j=1}^{i-1}(1-_{j}),_{ i}=_{i}G^{}_{i}(x)\] (2)

where \(N\) specifies the number of 2D Gaussian splats covering the current pixel. Heuristic densification and pruning strategies  are employed to address potential under- and over-reconstruction and ensure multi-view consistency in rendered images.

The color of each Gaussian, \(c\), is represented by Spherical Harmonics (SH) as \(k_{l}^{m}Y_{l}^{m}(_{})\) to provide view-dependent effects, where \((l,m)\) is the degree and order of the SH basis \(Y_{l}^{m}\), \(k_{l}^{m}\) is the corresponding SH coefficient, and \(_{}\) specifies the viewing direction. 3DGS usually uses a maximum degree \(l\) of 3, formulating \(c(_{})=_{l=0}^{3}_{m=-l}k_{l}^{m}Y_{l}^{m}(_{})\).

In Eq. (2), standard 3DGS treats colors as intrinsic attributes, independent of surface normals. This independence prevents surface normals from receiving gradient signals during the backpropagation pass when optimizing surface colors. Moreover, this separation significantly undermines our goal of simultaneously enhancing image quality and normal estimation capability, since improvements in normals cannot contribute to the rendering quality in the forward shading pass of 3DGS.

### Normal-Involved Shading Strategy

Physically Based Rendering.To integrate surface normals into both the backward and forward rendering passes of 3DGS effectively, we propose a normal-involved rendering strategy. This strategy adopts principles from physically based surface rendering  in computer graphics, which models the out radiance \(L_{}\) of a surface point as a function of the incident lighting \(L_{}\) and normals. Specifically, for each 3D point, its out-radiance in the outward direction \(_{}=-_{}\) is defined as:

\[c(_{})=L_{}(_{})=L_{ }(_{})+_{^{+}}L_{}(_{})( _{})f_{r}(_{},_{ })\,d_{}\] (3)

where \(L_{}\) is the emitted radiance along the outward direction, the integral is defined over the upper hemisphere \(^{+}\) of the surface; \(_{}\) is the incident light direction; BRDF \(f_{r}()\) describes how light is reflected from \(_{}\) to \(_{}\); the cosine term \((_{})\) accounts for the light geometric attenuation.

Integrated directional illumination vectors for Lambertian objects.We first consider a simple ideal case where the scene contains only Lambertian objects. For more complex materials, we handle them in Sec. 3.4. Given that the diffuse reflection of Lambertian objects is view-independent, the complicated BRDF function \(f_{r}()\) can be reduced to a spatially varying albedo term \(k_{}\). Omitting the emitted radiance \(L_{}\), the rendering equation accounting for the diffuse reflectance \(L_{}\) becomes:

\[L_{}=_{^{+}}L_{}(_{}) k_{ }(_{})\,d_{}=k_{ }_{^{+}}L_{}(_{})(_{ })\,d_{}.\] (4)

We want to find a way to explicitly re-parameterize \(L_{}\) as a function of normal \(\), meanwhile avoiding complicated integrals to save time. Specifically, we want to represent \(L_{}\) as a dot product between \(\) and some illumination vector \(\). Inspired by the traditional shape-from-shading strategy , we extract the normal out of Eq. (4):

\[L_{}=k_{}[_{^{+}}L_{} (_{})_{}\,d_{}].\] (5)

By defining a new term called "_integrated directional illumination vector_ (IDIV)" as

\[=_{^{+}}L_{}(_{})_{}\,d_{},\] (6)

we model the color for Lambertian objects as the albedo times a dot product between IDIV and the surface normal: \(c=L_{}=k_{}\). Compared with the original 3DGS and the method  that directly use a constant diffuse color, our re-parameterization successfully accounts for the normal's contributions to the diffuse component of surface colors.

Analysis.Our re-parameterization of the diffuse color enables the normal vector to be computationally involved in the rendering pass. As such, during backpropagation, by the chain rule, the surface color could pass gradient signals to the normal vectors as follows

\[}{d}=}{dc}}= }{dc}(k_{}).\] (7)

Other image-based methods [7; 16; 46] usually optimize a single environment map \(E\) and query this map using normals to determine diffuse colors. Consequently, their gradients on normals function by rotating the 3D Gaussians until the queried environment map color matches the pixel color, making the optimization process heavily dependent on the quality of \(E\). These methods assume the existence of a global environment map, which may not be valid for general real scenes. In contrast, our approach employs IDIVs to capture local incident lighting, eliminating the reliance on such a strong assumption and providing a more flexible and accurate framework for optimizing surface normals.

### Modeling Locally Shared Integrated Directional Illumination Vectors

Replacing 3D Gaussian colors \(c\) with \(k_{}\) introduces additional free parameters. Therefore, it is necessary to regularize the solution space of the introduced variables \(k_{}\) and \(\) to stabilize the training process and avoid overfitting.

Regularizing the solution space.Xu et al.  address solution space constraints from an optimization perspective. They have introduced several regularizers, such as total variation and Laplacian terms, to enhance the performance. However, their mesh-based method relies on a connected topology, which is incompatible with the discrete and sparse nature of 3D Gaussians. Although k-NN can be used to find connected primitives, it severely hampers training speed. Moreover, since 3D Gaussians can be semi-transparent, directly applying their methods to our case is unsuitable.

Instead, we approach the problem from a neural rendering perspective. As observed in [6; 42; 51; 52], 3D Gaussians with similar textures usually cluster together. A similar situation applies to incident lighting, which tends to be locally shared and exhibits low-frequency variations. To avoid the redundancy of storing Integrated Directional Illumination Vectors (IDIVs) for each Gaussian and capture local coherence, we propose to encode IDIVs with locally shared features and decode them using MLPs. An additional advantage of using MLPs to approximate IDIVs is their inherent smoothness in function approximations, aligning with our objectives.

In particular, we employ an anchor-based Gaussian Splatting (GS) method, Scaffold-GS , to implicitly represent IDIVs using locally shared features \(_{v}\) stored at anchors \(v\) and decode them using

Figure 2: Our normal-involved GS, **Normal-GS**, reparameterizes the original colors into the diffuse and specular components, \(c=L_{}+L_{S}\) (bottom). It models the diffuse component as the dot product between the normal vector \(\) and the Integrated Directional Illumination Vector (IDIV) \(\), and utilizes the Integrated Directional Encoding (IDE)  to capture view-dependent specular effects. Inherent parameters are encoded implicitly by a locally shared anchor Gaussian (left) and decoded using MLPs (top). Our method accounts for the contributions of normals to colors, effectively enhancing geometry accuracy and rendering quality.

a global MLP, \(_{l}(_{v})=\{_{v}^{k}\}_{k=1}^{K}\), where \(K\) IDIVs share a local feature \(_{v}\). Therefore, we only need to learn a compact set of per-anchor local features and use the MLP to generate per-Gaussian IDIVs, significantly reducing the dimensionality of the problem. Further details on our model configurations can be found in the following Sec. 3.4 and Appendix A.2.

### Training Details

Our method adopts the established 3DGS pipeline , which first performs Structure-from-Motion  on input images to generate sparse points. These points serve as initial 3D Gaussians and are optimized to model the scene.

Defining surface normals on 3D Gaussians.During optimization, 3D Gaussians often exhibit flatness around the surface areas, as observed in [5; 7; 45]. For a near-flat Gaussian ellipsoid, its shortest axis thus functions as the normal vector. However, the shortest axis of 3D Gaussians does not physically represent the actual surface normal. In contrast, implicit radiance fields leverage the gradient of densities to naturally approximate surfaces, and the Signed Distance Function (SDF) has its gradients as surface normals by definition. To make the shortest axis of 3D Gaussians align with the real surface normal, we employ a depth regularized loss term on normals, similar to [7; 10; 44].

We first render the depth and normal images, \(\{,\}\), using the 3DGS tile-based rasterizer, which is achieved by replacing the color term in Eq. 2 with depths and normals of 3D Gaussians, respectively. Then we compute the image-space gradients of the depth image \(_{(u,v)}\) and finally perform the cross product of the gradients, resulting in \(_{}\). The depth regularized loss term on normals is defined as \(_{N}=1-_{}\). Although there are other methods for improving normals, since our primary goal is to introduce a normal-involved shading method, we find this self-regularizer is enough for our models. Other methods are welcome to be combined with our strategy for future improvements.

Capturing specular effects.Our derivation on Sec. 3.2 is based on the Lambertian assumption. To make our framework more generalizable, similar to our parameterization of the diffuse term, we aim to express the specular term, \(L_{s}\), as a function of the normal vector \(\). Unlike view-independent diffuse colors, specular reflectance is highly sensitive to the view direction due to the complex BRDF, \(f_{r}(_{i},_{})\). Since normal vectors are implicitly involved in the BRDF to capture effects such as the Fresnel effect , we cannot directly extract the normal vector as we did for IDIVs in Sec. 3.2. Inspired by the Ref-NeRF , we model the specular component as a function of the reflection direction of the viewing direction with surface normals involved: \(_{}=2(_{})-_{ }\). By applying the Integrated Directional Encoding (IDE)  on the reflection direction, we have \(L_{}=(_{}(_{}),,_{v})\). The final color of the Gaussian is the summation of \(L_{}\) and \(L_{}\). More details can be found in the Appendix.

Model architecture.Figure. 2 illustrates the whole process of our method. We follow  to define a set of anchor Gaussians \(\{v\}\), where each \(v\) is associated with a position \(_{v}\), a local feature \(_{v}\), a scaling factor \(_{v}\) and \(k\) offsets \(_{v}^{k}\) for \(k\) nearby 3D Gaussians. Anchor-wise features together with global MLPs are employed to predict attributes of 3D Gaussians. We utilize our normal-involved rendering to compute surface colors \(c=L_{}+L_{}\). Colors are finally gathered following the traditional 3DGS pipeline, together with rendered normals and depths for self-regularization. The final loss \(\) consists of the original photo-metric loss \(_{}\) used in 3DGS , the volume regularization loss \(_{}\) used in Scaffold-GS  and the self-regularized depth-normal loss \(_{}\):

\[=_{}+_{}_{}+_{}_{}.\] (8)

## 4 Experiments

We evaluated our Normal-GS method against several state-of-the-art 3DGS-based methods across multiple datasets, presenting both qualitative and quantitative results.

Baselines.We selected the following baseline methods for comparison: 1) _3DGS_ and _Scaffold-GS_: These served as the vanilla and baseline methods. 2) _GaussianShader (GShader)_ and _SpecGaussian_: These 3DGS-based methods modeled specular effects and used surface normals. Both methods computed the reflected direction \(_{t}\) of the viewing direction w.r.t the normal. GShaderused \(_{t}\) to query a learned environment map for shading, while SpecGaussian used \(_{t}\) to query the learned Spherical Gaussians. Additionally, we designed a new baseline method based on ScaffdG-GS. In the original Scaffold-GS, anchor features \(_{v}\) were passed into a color MLP to predict 3D Gaussian colors. In our new baseline method, we also fed normals into the same color MLP, implicitly incorporating normal information. We referred to this baseline method as 3) _ScaffoldGS w/ N_.

**Datasets and evaluation metrics.** We followed the original 3DGS  methodology and used the NeRF Synthetic , Mip-NeRF 360 , Tank and Temple , and Deep Blending  datasets to demonstrate the performance of our method. Due to licensing issues, we tested on 7 out of 9 scenes in the Mip-NeRF 360 dataset. Following , we selected every 8th image for testing and used the remaining images for training. We reported PSNR, SSIM, and LPIPS to measure rendering quality and the Mean Angular Error (MAE) of normals on the NeRF Synthetic  dataset.

**Implementation details.** We implemented our method in Python using the PyTorch framework . Our rasterization pipeline followed the CUDA-based rasterizer described in . We trained our models for 30k iterations, following the settings of baseline methods. Consistent with , we set \(_{}=0.001\). For the depth-normal loss, we used \(_{}=0.01\). Because the depth and normals were inaccurate at the start of training, we added the depth-normal loss after training for 5k iterations. We will release our code after publication. More implementation details were included in the Appendix A.2.

We tested our method and the baseline methods using their original released implementations with default hyperparameters on an NVIDIA RTX 3090 GPU with 24 GB of memory. To obtain normals for comparisons, we retrained most of the baseline methods, except for 3DGS, which provided its optimized data. For methods without defined normals, such as 3DGS  and Scaffold-GS , we used the shortest axis of 3D Gaussians as normals for evaluation, consistent with ours.

### Results

We first present a comprehensive comparison of rendering quality and normal estimation in Fig.3, demonstrating that our method either matches or surpasses state-of-the-art (SOTA) approaches in metrics of PSNR/SSIM/LPIPS (Tab.1). Notably, our approach significantly outperforms the baseline

Figure 3: **Qualitative comparisons of the rendering quality and normal estimation. Our method produced clean normals estimation and preserved good rendering quality.**

method (ScaffoldGS) and its variant incorporating normals (ScaffoldGS w/ N) in almost all metrics. This comparison underscores the limitations of the implicit normal utilization in ScaffoldGS w/N, which fails to accurately model appearance. In contrast, our method adopts an explicit modeling of color as a function of normals, directly derived from the rendering equation. This explicit integration enhances both the forward and backward passes during training, providing clearer cues for the network to capture nuanced color details, thereby improving rendering quality.

Our approach also demonstrates superior rendering performance compared to methods like GShader , which depend on global environment maps to model specular effects, as evidenced in Tab. 1. The dependency of normal gradients on environment map quality, described by the equation \(=\)[7; 47], often leads to inaccuracies in capturing detailed information, as shown in Fig.4 and further evidenced by performance issues on challenging datasets like DeepBlending in Tab. 1. Our framework's capability to learn local incident light field attributes tied to distinct anchors in 3D space allows for more effective handling of complex lighting effects.

Although SpecGaussian  achieves competitive rendering quality through the use of Spherical Gaussian encoding and neural networks, it falls short in accurate normal modeling, as depicted in Fig.3. This suggests the insufficiency of relying solely on neural networks for integrating normal information into geometric modeling. Our method, with its explicit integration of normals into the rendering process, not only maintains high rendering quality but also ensures the accuracy and smoothness of normals. Our approach underscores the crucial role of direct geometric involvement in the rendering pipeline for achieving both high geometric accuracy and visual fidelity. More results are available in Appendix A.3.

To quantitatively evaluate normal estimation, we conducted tests on the Synthetic-NeRF dataset, which includes ground-truth normal data. As shown in Tab. 2(a), our method outperforms others in the normal Mean Angular Error (MAE) metric. Visual results in Tab. 2(b) confirm our method's superior capability in capturing accurate geometry alongside high-fidelity rendering, as highlighted in the yellow box, showcasing the effectiveness of our design.

### Ablation studies

Our contributions are significant in two key areas: the integration of Integrated Directional Illumination Vectors (IDIV) and the implementation of an anchor-based regularization design. We

    &  &  &  \\  Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\ 
3DGST\({}^{}\) & 28.691 & **0.870** & **0.182** & 23.142 & 0.840 & 0.183 & 29.405 & 0.903 & 0.243 \\ ScaffoldGS & 29.267 & 0.869 & **0.190** & 24.088 & 0.851 & 0.175 & 30.140 & 0.905 & 0.256 \\ ScaffoldGS w/ N & 29.177 & **0.870** & 0.192 & 23.976 & 0.852 & 0.176 & 30.163 & 0.905 & 0.263 \\ GShader & 26.060 & 0.825 & 0.233 & 21.262 & 0.785 & 0.254 & 19.159* & 0.769* & 0.453* \\ SpecGaussian & 29.287 & 0.864 & 0.196 & **24.502** & 0.855 & **0.175** & 30.114 & 0.905 & 0.252 \\  Normal-GS (Ours) & 29.341 & 0.869 & 0.194 & 24.219 & 0.854 & 0.174 & 30.187 & 0.910 & 0.252 \\   

Table 1: **Quantitative comparisons of the rendering quality. Our methods achieve comparable or even better results compared with the SOTA approach, SpecGaussian . However, SpecGaussian performs much worse in normal estimation, while ours achieves a good balance between geometry and appearance quality. \({}^{}\) denotes the results quoted from their paper. GShader* fails on the Deep Blending scene.**

Figure 4: b) Our method successfully captures specular effects. c) GShader  relying on an environment map performs poorly when d) its learned environment map fails. Scenes are taken from the Mip-NeRF 360 dataset , which have complicated lighting conditions.

initially validate the impact of IDIV through an ablation study conducted within a conventional 3DGS framework, detailed in Sec. 3.2. Introducing IDIV necessitates additional parameters, requiring the adoption of regularization techniques such as Laplacian and Total-Variation loss, referenced in . The outcomes, illustrated in Fig. 5 (a) and (b), demonstrate substantial enhancements in rendering quality and the smoothing of normals.

However, we found that the model's performance is highly sensitive to the tuning of loss weights. To address this, we implemented an anchor-based IDIV regularization strategy in Sec. 3.3 with results illustrated in Fig. 5 (c). This approach employs neural networks to model locally shared IDIVs, proving to be both robust and efficient in regularizing the solution space. Notably, the simplicity yet effectiveness of our final loss term, defined in Eq. 8, underscores the potential of our design to enhance both geometric and rendering quality. When considering the specular component, our full (Fig. 5 (d)) model achieves superior results.

We also performed quantitative ablation studies on the DTU dataset  to further validate the effectiveness of our components. The DTU dataset contains 15 scenes, each with scanned geometries provided as ground truth. We conducted the ablation study by comparing the mean Chamfer distance (mCD) for geometry accuracy and PSNR for rendering quality evaluation. We followed  to pre-process the data, extract meshes and evaluate obtained results. As shown in Tab. 3, our base model is Scaffold-GS . We add the self-regularized depth-normal loss, \(_{}\), as defined in Sec.3.4. The results of (b) in Tab. 3 indicate that the depth-normal loss can significantly improve the geometry quality but ruin the rendering with more than 2dB PSNR drop. Then we introduce the specular component into (b), which can improve the rendering quality

   Model & mCD \(\) & PSNR \(\) \\  (a): Scaffold-GS  & 1.84 & 38.14 \\ (b): (a) + \(_{}\) & 0.95 & 35.90 \\ (c): (b) + \(L_{specular}\) & 0.93 & 37.37 \\ (d): Full model & 0.94 & 37.63 \\   

Table 3: Quantitative ablation studies on DTU.

Figure 5: **Ablation study about our proposed components. The IDIV together with our regularization strategy produces better quality in rendering quality and normal accuracy.**

without compromising the geometry, which further assures the importance of involving normal into the rendering. Finally, we add the proposed IDIV to get our final model (d), which further improves the PSNR by more than 0.2 dB with similar geometry quality. Compared with the base model (a), our full model achieves a better balance between the geometry quality and the rendering fidelity.

### Geometric reconstruction comparisons

In addition to evaluating normal accuracy, we compare the overall geometry quality of our method with existing 3DGS-based approaches [2; 8; 44] on the DTU dataset . For a fair comparison, we follow the pipeline described in  and measure the mean Chamfer distance (mCD) and PSNR to assess both geometric reconstruction and rendering quality. The results for 3DGS , SuGaR , and 2DGS  are directly adopted from . As Tab. 5 illustrates, our method achieves outstanding rendering quality while maintaining better geometric quality than 3DGS and SuGaR. 2DGS, however, sacrifices some rendering fidelity in novel view synthesis due to the lack of a clear relationship between geometry and appearance, as seen in Tab. 5. Unlike 2DGS, our method explicitly addresses the interactions between lighting and normals. It is also worth noting that 2DGS and our approach are conceptually orthogonal and could potentially be combined for further improvements.

### Comparisons with NeRF-based methods

Finally, for completeness, we conduct comparisons with existing state-of-the-art NeRF-based methods, including Mip-NeRF 360 , Ref-NeRF , and Zip-NeRF . In Tab. 4, we present their original results for 7 out of the 9 scenes from the Mip-NeRF 360 dataset , aligning with our experimental setup. Our method performs the second best in terms of the PSNR and achieves comparable SSIM and LPIPS scores. However, the training time for Zip-NeRF exceeds 10 hours on high-performance GPUS. In contrast, 3DGS-based methods, including ours, are relatively fast and can support real-time rendering, offering a significant speed advantage over NeRF-based approaches.

## 5 Discussions

**Conclusion.** We introduce a novel shading technique within the 3D Gaussian Splatting (3DGS) framework to improve view synthesis and normal estimation. By integrating Directional Illumination Vectors (IDIV) as advanced attributes for color representation and utilizing an anchor-based design with a neural MLP for IDIV prediction, our approach achieves a superior balance between rendering quality and geometric accuracy. This enhancement not only elevates image fidelity but also ensures precise normal estimations, pushing the boundaries of 3DGS technology in real-world applications.

**Limitation and Future Work.** We acknowledge existing limitations within our framework. Notably, the self-regularized normal loss proves suboptimal for certain outdoor scenes at a distance (as observed in the Appendix 7), negatively impacting rendering quality in these regions. This issue could potentially be addressed with more sophisticated normal guidance techniques. In future research, we plan to delve into a more granular decomposition of the Integrated Directional Illumination Vectors (IDIV) and specular components. This exploration aims to enhance capabilities in texture editing and relighting, thereby broadening the practical applications of our framework.

**Boarder Impact.** As our approaches require per-scene optimization to obtain the 3D model for each scene, the computational resources for model training could be a concern for global climate change.

   Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  Mip-NeRF 360  & 29.231 & 0.844 & 0.207 \\ Ref-NeRF  & 28.553 & 0.849 & 0.196 \\ Zip-NeRF  & 30.077 & 0.876 & 0.170 \\ Ours & 29.341 & 0.869 & 0.194 \\   

Table 4: Rendering quality comparisons with NeRF-based methods on the Mip-NeRF 360 dataset.

   Method & mCD \(\) & PSNR \(\) \\ 
3DGS  & 1.96 & 35.76 \\ SuGaR  & 1.33 & 34.57 \\
2DGS  & 0.80 & 34.52 \\ Ours & 0.94 & 37.63 \\   

Table 5: Geometric and rendering quality comparisons on the DTU dataset.