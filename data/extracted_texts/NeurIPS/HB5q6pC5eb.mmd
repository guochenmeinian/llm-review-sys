# PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations

Jiatong Li\({}^{1}\), Renjun Hu\({}^{2}\), Kunzhe Huang\({}^{2}\), Yan Zhuang\({}^{1}\),

Qi Liu\({}^{1}\), Mengxiao Zhu\({}^{1}\), Xing Shi\({}^{2}\), Wei Lin\({}^{2}\)

\({}^{1}\)University of Science and Technology of China, China

\({}^{2}\)Alibaba Cloud Computing, China

{cslijt, zykb}@mail.ustc.edu.cn, {qiliuql, mxzhu}@ustc.edu.cn,

{renjun.hrj, huangkunzhe.hkz, shubao.sx, weilin.hw}@alibaba-inc.com

Work done during Li's internship at Alibaba Cloud Computing, under the guidance of Hu.Qi Liu is the corresponding author.

###### Abstract

Expert-designed close-ended benchmarks are indispensable in assessing the knowledge capacity of large language models (LLMs). Despite their widespread use, concerns have mounted regarding their reliability due to limited test scenarios and an unavoidable risk of data contamination. To rectify this, we present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge capacity through **knowledge-invariant perturbations**. These perturbations employ human-like restatement techniques to generate on-the-fly test samples from static benchmarks, meticulously retaining knowledge-critical content while altering irrelevant details. Our toolkit further includes a suite of **response consistency analyses** that compare performance on raw vs. perturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six representative LLMs are re-evaluated using PertEval. Results reveal significantly inflated performance of the LLMs on raw benchmarks, including an absolute 25.8% overestimation for GPT-4. Additionally, through a nuanced response pattern analysis, we discover that PertEval retains LLMs' uncertainty to specious knowledge, and reveals their potential role memorization to correct options which leads to overestimated performance. We also find that the detailed response consistency analyses by PertEval could illuminate various weaknesses in existing LLMs' knowledge mastery and guide the development of refinement. Our findings provide insights for advancing more robust and genuinely knowledgeable LLMs. Our code is available at https://github.com/aigc-apps/PertEval.

## 1 Introduction

Large language models (LLMs) are developing rapidly, and have shown excellent basic capabilities, such as reasoning [1; 2], planning  and world knowledge , in real-world tasks. Given the widespread deployment of LLMs across an increasing number of scenarios, including those that are safety-critical , evaluating the real capability of LLMs becomes a necessary and significant task. Knowledge capacity, _i.e.,_ the ability to retrieve and utilize acquired knowledge to solve professional problems, is one of the core capabilities of LLMs . Existing knowledge capacity evaluation largely rely on standardized tests using close-ended benchmarks [6; 7; 8; 9; 10; 11]. These benchmarks consist of multiple-choice questions that include question descriptions, goals, options, and correct answers. They encapsulate valuable domain-specific knowledge that LLMs need to comprehend. The knowledge capacity of LLMs can then be directly gauged by the performance on these test datasets.

To date, static benchmarks have been essential in assessing the relative knowledge capacity of LLMs  because of their high quality and cost effectiveness. However, due to limited test scenarios  and the unavoidable risk of data contamination , these benchmarks still encounter significant challenges in accurately measuring the true knowledge capacity of LLMs. First, static benchmarks rely exclusively on close-ended questions in fixed formats for evaluation, which differs a lot from real-world scenarios . For instance, instead of merely selecting an option, some users prefer asking LLMs to judge the correctness of options, while others may directly request the LLMs to generate an answer without providing an option list . These varying prompting styles have an innegligible effect on performance . Furthermore, previous work  has demonstrated that nonlinear or discontinuous evaluation metrics could lead to inaccurate implications about LLM capacity. Therefore, to genuinely evaluate the knowledge capacity of LLMs, it is necessary to test their performance across a variety of scenarios that mirror real-world conditions. Second, in terms of data contamination , knowledge capacity benchmarks usually publish their test data online to ensure transparency and reproducibility (_e.g.,_[6; 7]). However, this practice enables LLMs to memorize the test data during pre-training and alignment, which can lead to an overestimation of knowledge capacity, thus undermining the trustworthiness of evaluation results . Efforts to mitigate data contamination have primarily focused on detecting such contamination [14; 20] and generating new test data . Nonetheless, high-quality benchmark data contain rich knowledge that is valuable for evaluation. We suppose that it is feasible to utilize this knowledge effectively to assess the true knowledge capacity of LLMs while reducing the risk of data contamination.

To achieve this goal, we introduce PertEval, an evaluation toolkit that utilizes knowledge-invariant perturbations on static benchmarks to unveil the real knowledge capacity of LLMs, as illustrated in Figure 1. This idea stems from an analogy with human educational assessment. Like LLM evaluation, close-ended questions are prevalent in human assessments due to their versatility, cost-effectiveness and precision of measurement . However, they also face challenges such as cheating  and a lack of variety . Solutions to these issues include item personalization  and the creation of distractive options [25; 26]. Inspired by this, PertEval incorporates human-like **knowledge-invariant perturbations** to restate static test data to various forms and employs **response consistency analyses** to evaluate and trace the change of LLMs' performance in different test scenarios. Specifically, knowledge-invariant perturbations consist of both the content-level perturbation that extensively writes questions to minimize data contamination and a series of format-level perturbations that comprehensively cover potential real-world test scenarios. Response consistency analyses encompass basic evaluation metrics to measure the real knowledge capacity of LLMs and response pattern analysis to investigate the causes behind changes in LLM performance. Consequently, PertEval could deeply probe the LLMs' weaknesses in knowledge mastery and offer insights for their refinement.

In experiments, we re-evaluate six representative LLMs using PertEval. Capacity metric results first reveal a significant overestimation of the knowledge capacity of LLMs by static benchmarks, with an absolute 25.8% overestimation for GPT-4 and 38.8% for Gemini-1.0-Pro on MMLU  (results of Gemini-1.0-Pro are available at Appendix D.2). Response pattern analyses further unveil that the performance decline with PertEval is mainly caused by an increase of selecting extra incorrect choices, with an increase of up to 9.7% and 27.7% for GPT-4 and Gemini-1.0-Pro, respectively.

Figure 1: An overview of the PertEval evaluation toolkit. PertEval uses content-level and format-level perturbations to generate perturbed dataset \(D^{}\) from _existing_ close-ended benchmark dataset \(D\). Next, it evaluates the knowledge capacity of LLMs via response consistency analysis. PertEval also demonstrates in-depth the performance feature of LLMs via response pattern analysis.

These findings highlight the potential for rote memorization of correct options by LLMs on static benchmarks. Detailed response consistency analyses in terms of overall performance stability and correct response consistency indicate various weaknesses in the knowledge capacity of existing LLMs, such as the vulnerability to content-level perturbation and the change of global text order.

Overall, our contributions are as follows:

* We propose an evaluation toolkit that utilizes knowledge-invariant perturbations on close-ended evaluation benchmarks to unveil the real knowledge capacity of LLMs, a significant step towards more trustworthy LLM evaluation.
* We re-evaluate the knowledge capacity of six representative LLMs using PertEval. Evaluation results not only demonstrate significantly inflated performance of LLMs by static benchmarks, but also reveal LLMs' uncertainty to specious knowledge and rote memorization to correct options.
* We demonstrate the vulnerability of various LLMs to different perturbation strategies in PertEval and provide insights for the refinement in terms of promoting LLMs' knowledge capacity.

## 2 Related Work

**Knowledge Capacity Evaluation of LLMs** Research works of the knowledge capacity evaluation LLMs consist of two lines, _i.e., evaluation benchmarks_ and _evaluation methodologies_. The first line aim to design comprehensive and accurate benchmarks to quantize the knowledge capacity of LLMs. Evaluation benchmarks could be further classified into _general_ or _professional_ knowledge benchmarks . The first category aims to evaluate the general knowledge capacity of LLMs across a wide range of domains, such as MMLU , C-Eval  and ARC . The second category aims to deeply evaluate the professional capacity of LLMs in specific domains, such as MedMCQA  in medicine and ScienceQA  in science. These benchmarks depend on professional multiple-choice questions to quantitatively measure the capacity of LLMs. The other research line, _i.e.,_ evaluation methodologies, aim to improve the accuracy and truthfulness of evaluation via data-driven or model-driven techniques. Along this line, recent studies have emphasized data-contamination detection techniques [14; 20] and contamination-free test data generation [21; 29]. In addition, psychometric-based techniques , dynamic evaluation techniques [31; 32], and perturbation-based evaluation techniques such as CheckList  and PolyJuice  have been proposed to comprehensively evaluate the capability of language models (LMs) from different aspects. However, these methodologies are unsuitable for knowledge capacity evaluation, and are hard to utilize valuable information in expert-designed datasets for trustworthy evaluation.

**Consistency of LMs** The consistency of an LM denotes its behavior invariance under meaning-preserving alternations in its input , which is significant in natural language understanding. Since the emergence of pretrained language models (PLMs), massive efforts have been made to evaluate the consistency of LMs on various fields. Elazar et al.  revealed the poor consistency performance of PLMs on factual knowledge. Fierro and Sogaard  extended the study to multilingual PLMs and obtained similar findings. Jang et al.  proposed a taxonomy for various concepts of consistency and established a benchmark, BECEL, to evaluate the consistency of PLMs. As LLMs develop swiftly, Wang et al.  proposed to use self-consistency decoding to empower the Chain-of-Thought reasoning ability of LLMs. Recently, Rajan et al.  proposed KonTest, an autonomous evaluation framework that utilizes knowledge graphs to generate test samples, to probe the inconsistency in LLMs' knowledge of the world. In summary, these research works view consistency as a research subject that needs to be measured or intervened. Differently, in our study, the consistency of LLMs is viewed as a technical method, _i.e.,_ a measurement for probing LLMs' real knowledge capacity.

**Adversarial Text Attacks on LMs** These techniques aim to mislead language models to yield wrong or toxic outputs with small text perturbations, which play an indispensable role in the research of robustness and safety of language models. Such efforts include but not limited to jailbreaking  and text classification attack . Text attacks could also be classified into character-level [42; 43], word-level  and sentence-level . Recently, LLM-based perturbations like PromptAttack  have also been introduced. However, considering their cost and the comprehensiveness, existing text attack methods are insufficient for the purpose of this work. Moreover, our PertEval is built upon a new concept of knowledge-invariant perturbation, which is an important supplement to this topic.

Methodology

### Knowledge-invariant Perturbations

We first present the two types of knowledge-invariant perturbations to restate static test questions. The knowledge-invariant property of these perturbations will be discussed in the next subsection.

**Content-level perturbation: knowledge-invariant paraphrasing.** The goal of content-level perturbation is to substantially alter the phrasing of questions while retaining the original knowledge, thereby mitigating data contamination in the test data. The key challenge of such knowledge-invariant paraphrasing is to preserve the original knowledge while changing the statement as much as possible. A test question is a composite of sentences, with each provide either backgrounds, conditions or goals of the question. Therefore, to preserve knowledge of the original question, we propose a sentence-by-sentence paraphrasing algorithm using an LLM rewriter. Formally, let \(q=(s_{1},s_{2},,s_{T})\) represent the question text, where \(s_{t}\) denotes the \(t\)-th sentence (for \(t=1,2,,T\)). The semantic of \(s_{t}\) (for \(t 2\)) depends on its _perequisite sequence_\((s_{1},,s_{t-1})\). To rewrite the entire question, an LLM rewriter is instructed to paraphrase sentence by sentence given each sentence and its original prerequisite sequences. Detailed descriptions of the paraphrasing algorithm, the prompt template for the rewriter LLM, and an example of the perturbation can be found in Appendix B.1.

**Format-level perturbation: question format refactoring**. The objective of question format refactoring is to assess the robustness of LLMs' knowledge capacity under complicated test conditions. To achieve this, we have developed a variety of format-level perturbation strategies to comprehensively evaluate the resilience of LLMs' knowledge capacity in various test scenarios.

* _Option permutation (OptionPerm)._ OptionPerm reorders the contents of options while maintaining the original order of option IDs. Its goal is to evaluate _option ordering bias_ in the knowledge acquisition of LLMs. By default, OptionPerm reverses the order of option contents to completely disrupt their local sequence.
* _Option format refactoring (OptionForm)._ OptionForm modifies the format of option IDs, such as by appending a right parenthesis to the end. This perturbation aims to assess the _option format bias_ in the knowledge acquisition of LLMs. OptionForm may influence LLM performance by altering the dependency between different tokens within the options.
* _Option ID shifting (OptionCaesar)._ OptionCaesar shifts the ASCII value of option IDs to change their character, which is similar to Caesar encryption. This technique aims to investigate the _selection bias_ in the knowledge acquirement of LLMs, a phenomenon empirically observed in some models. By replacing common option IDs with less common ones (_e.g._, A/B/C/D \(\) U/V/W/X), OptionCaesar allows us to observe whether the values of the IDs impact the LLMs' performance.
* _Question type changing (ChangeType)._ ChangeType converts a multi-choice question into a multi-judgment question. This perturbation aims to examine the _question type bias_ in LLMs. Since the feasible solution space of a multi-choice question and the corresponding multi-judgment question is identical (given \(N\) options, the size of the feasible solution space is \(2^{N}-1\)), an LLM that robustly acquire knowledge/skills in a question should be insensitive to ChangeType.
* _Question position swapping (SwapPos)._ SwapPos switches the position of the question text and the options. It aims to evaluate the _global ordering bias_ of LLMs. For rational human test-takers, SwapPos does not affect performance, as it does not alter the question content. However, SwapPos can significantly change the output distribution of self-regressive text generation models by disrupting the global ordering of input prompts.

Examples of format-level perturbations are available at Appendix B.2.

### Knowledge Invariance Verification

We next investigate the knowledge-invariance property of the proposed perturbations.

**Knowledge invariance scoring**. This approach checks the knowledge invariance of perturbations from the perspective of humans' and LLMs' perception. Specifically, based on previous works in the measurement of semantic similarity  and characteristics of close-ended benchmarks, we first propose standards of knowledge-invariance scoring, as shown in Table 1. Next, we recruit professional human volunteers and utilize superior LLMs, such as claude-3-sonnet, respectively, to serve as the referee to rate knowledge invariance scores. Given a set of original and perturbed question pairs \(D_{dual}=\{(q_{i},q_{i}^{}) i=1,2,,|D|\}\), we construct scoring prompts based on a predefined template, knowledge invariant standards, and the scoring criteria for knowledge invariance judgement (see Table 9 in Appendix B.1). After collecting the output scores from the referee LLM using these scoring prompts, we calculate the average score as the final result.

**Testing on mastered questions for LLMs.** This approach evaluates knowledge invariance based on the output performance of LLMs. The rationale here is that if a perturbation is knowledge-invariant, an LLM's performance on questions that it has truly mastered should remain consistent between the original and perturbed versions. Identifying _mastered questions_ for LLMs, however, poses a challenge. To this end, we propose using questions that most LLMs can correctly answer as a proxy for mastered questions. An LLM, such as gpt-4-turbo, is then required to answer both the original and perturbed versions of these questions to validate the knowledge invariance of the perturbation. In summary, the test-based knowledge invariance checking procedure involves the following steps:

1. Given a set of LLM test-takers and the evaluation dataset \(D\), construct the _mastered question set_\(D_{simple} D\) consisting of questions that all LLMs could answer correctly.
2. Apply a perturbation to each sample in \(D_{simple}\) to create the perturbed dataset \(D^{}_{simple}\).
3. Assess the performance of a knowledgeable LLM on both \(D_{simple}\) and \(D^{}_{simple}\). If the test results show no significant difference in performance, the perturbation can be considered knowledge-invariant.

This method emphasizes the impact of perturbations on the question-answer process of LLMs. In comparison to the scoring test, this approach is more appropriate for evaluating the knowledge invariance of content-level perturbations. We present detailed results of both tests in Experiments.

### Response Consistency Analyses for Measuring Real Knowledge Capacity

We further devise a suite of response consistency analyses that, by comparing performance on raw vs. perturbed test sets, unveils the real knowledge capacity of LLMs. These include a metric for quantifying calibrated knowledge capacity and the more fine-grained ones helpful for revealing the vulnerability of LLMs to different perturbation strategies.

**Metric of real knowledge capacity.** To measure real knowledge capacity of LLMs, we propose the _Consistent Accuracy (ACC@Consist)_ as the evaluation metric. The rationale is that if an LLM has truly mastered a question and its underlying knowledge, its performance should remain consistent across all versions of the question, including both the original and the most complex perturbed versions. Formally, let \(M()\) denotes the response function of an LLM. Let \(x=(q_{x},y_{x}) X\) represent a test question, where \(q_{x}\) and \(y_{x}\) denote the question text and the correct answer(s) of \(x\). Further let \(^{*}:X X\) be the most complicated composite knowledge-invariant perturbation and

  
**Standard Name** & **Standard Description** \\  Semantic Information & The perturbed question must have the same semantic information as the original question, which cannot change the name of entities, logic of statements and meaning of equations. \\  Reasoning Invariance & A human test-taker’s reasoning process to obtain his/her response in the perturbed question should be consistent with that in the original question. \\  Answer Invariance & The answer of a perturbed question should be semantically equivalent to the answer of the original question. \\  Statement Clarity & The perturbed question should clearly present contexts, conditions and the target of the question. \\   

Table 1: Standards of knowledge-invariance scoring.

\(D=\{x_{1},x_{2},,x_{|D|}\}\) the raw test set. Then ACC@Consist is defined as:

\[(M,D)=_{x D}I[M(q_{x})=y_{x} M(q _{^{*}(x)})=y_{^{*}(x)}].\] (1)

Here \(I()\) is the binary indicator function. Taking a step further, we implement a response pattern analysis that uncovers the causes of the discrepancy between ACC@Consist and the original ACC.

**Overall performance stability**. In terms of performance stability, we expect that LLMs which have robustly acquired the knowledge and skills required by benchmarks should exhibit stable performance when faced with knowledge-invariant perturbations. To evaluate this, we propose _Performance Drop Rate (PDR)_ as a metric to measure the overall performance stability of LLMs under knowledge-invariant perturbations.:

\[(M,D,)=_{x D}I[M(q_{(x)})=y_{ (x)}]-I[M(q_{x})=y_{x}].\] (2)

Here \(D\) denotes the original test dataset and \(\) is a perturbation strategy. Essentially, \(PDR\) measures the discrepancy between the LLM's accuracy on \(D\) and the perturbed dataset. When \(PDR<0\), the perturbation decreases the overall performance of an LLM, indicating that it does not robustly acquire knowledge and skills. To obtain more reliable conclusions, we further conduct _Wilcoxon signed-rank test_ for original and perturbed question sample pairs.

**Correct response consistency**. We also adopt the _Recall of Performance (ROP)_ metric to measure correct response consistency. The term "ROP" draws an analogy to the classical recall score. Let \(CC\) denote the number of responses consistently correct before and after perturbation, and \(IC\) the number of responses initially correct and later incorrect after perturbation (see Appendix B.3 for detailed demonstration). Similar to the recall score, ROP is defined as the ratio of consistent correct responses to the total number of correct responses before perturbation, _i.e.,_

\[(M,D,)=CC/(CC+IC)\] (3)

The value of ROP is within \(\), with higher ROP indicating better correct response consistency under perturbation \(\). It should be noticed that, unlike PDR, there does not exist an absolute threshold for ROP to evaluate goodness. Rather, ROP serves as an index of correct response consistency, which can be utilized to compare the performance of different LLMs under perturbations.

## 4 Experiments

**Datasets.** We choose the test data in _College Mathematics (C-Math), World History (W-History), Professional Psychology (P-Psychology) and Professional Medicine (P-Medicine)_ from the Massive Multitask Language Understanding (MMLU)  for evaluation. This is a trade-off between comprehensively covering evaluation domains/subjects and keeping the evaluation cost affordable. Note

  
**Method** & **C-Math** & **W-History** & **P-Psychology** & **P-Medicine** \\  PromptAttack & 2.3/2.4/3.0/3.9 & 2.2/2.2/2.3/2.8 & 1.3/2.4/2.8/2.8 & 1.6/2.5/3.5/3.6 \\
**PerEval (ours)** & **3.6/3.8/3.9/3.9** & **3.7/4.1/4.1/4.3** & **4.3/4.4/4.5/4.7** & **4.2/4.3/4.4/4.6** \\   

Table 2: **Knowledge invariance scores\({}^{}\) rated by human scorers**. Four independent scores from different human scorer groups are presented in ascending order for each cell.

  
**Method** & **C-Math** & **W-History** & **P-Psychology** & **P-Medicine** \\  PromptAttack & 3.2/3.6/3.6 & 3.2/3.3/3.7 & 3.9/3.9/3.7 & 4.1/4.3/4.2 \\
**Pertual (ours)** & **3.8/3.9/4.0** & **4.0/4.2/4.0** & **4.0/4.4/4.0** & **4.1/4.4/4.0** \\   

Table 3: **Knowledge invariance scores\({}^{}\) rated by superior LLMs**. Values (a/b/c) in each cell denotes the average knowledge invariance score rated by gpt-4-turbo, Claude-3.5-sonnet, and Ilama-3.1-405b, respectively.

that each dataset represents a supercategory in STEM, Humanities, Social Sciences, and Other fields, respectively. The statistics of the selected datasets are detailed in Table 11 in Appendix C.1.

**Large Language Models.** Based on leaderboards like OpenCompass  and considering both the popularity and timeliness of LLMs, we select six representative LLMs for evaluation. These LLMs include close-sourced (gpt-4-turbo , gpt-3.5-turbo , gemini-1.0-pro  and glm-3-turbo ) and open-sourced (mistral-7b-instruct-v0.2  and llama-3-8b-instruct ) models.

### Knowledge Invariance Verification for Perturbations

We first investigate the knowledge invariance property of our perturbations using the two methods developed in Section 3.2. Employing the LLM-based knowledge invariance scoring method, we compare the scores of the proposed perturbations with those by PromptAttack  as a baseline.

**Experiment Setup**. For knowledge invariance scoring, 10 samples at equal intervals for each subject are used for scoring. Since there are four subjects and two perturbation strategies (1 baseline + 1 PertEval) for each question, we have 10*4*2 = 80 question pairs in total. To ensure that each question pair have four independent scores, eight human scorers are engaged in human-based scoring. Human scorers are equally divided into four groups. Each group independently scores for all 80 question pairs. Therefore, each human scorer within a group scores for 40 shuffled samples. For LLM-based scoring, we choose gpt-4-turbo, Claude-3.5-sonnet, and llama-3.1-405b as scorers. For fine-grained perturbation-wise scoring, we choose gpt-4-turbo as the scorer.

**Result Analysis**. Overall knowledge invariance scores are presented in Table 2 and 3. PertEval outperforms the baseline, and the score mostly exceeds 4.0, the borderline of knowledge-invariance. Scores of both PromptAttack and PertEval on C-Math do not exceed 4.0. This is because many samples in College Mathematics of MMLU are short mathematical reasoning questions that have many mathematical symbols and statements. Indeed, in many STEM subjects, requirements for knowledge capacity and reasoning ability often mix together in a test question. This points out considerable potential for future development in robust LLM evaluation in STEM subjects.

Next, the results presented in Figure 2 show that the average scores of our perturbations approach 5.0 (perfectly knowledge-invariant) in most cases, consistently outperforming those from PromptAttack. We also present the Levenshtein distance (_i.e.,_ edit distance) between the perturbed and original

  
**Strategy** & **C-Math** & **W-History** & **P-Psychology** & **P-Medicine** & **AVG\({}_{macro}\)** \\  KnInvPara & 0.0000 & -0.0115 & 0.0091 & -0.0244 & -0.0067 \\ OptionPerm & 0.0000 & 0.0000 & 0.0000 & -0.0244 & -0.0061 \\ OptionForm & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 \\ OptionCaesar & 0.0000 & 0.0000 & 0.0091 & 0.0000 & 0.0023 \\ ChangeType & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 \\ SwapPos & 0.0000 & -0.1149 & -0.0636 & -0.0488 & -0.0568 \\   

Table 4: **Results of testing on mastered questions - Performance Drop Rate (PDR) of overall performance stability testing on mastered questions** using gpt-4-turbo.

Figure 2: (left) **Perturbation-wise knowledge invariance scores\(\) by gpt-4-turbo** (systematic sampling, interval = 10); (right) **edit distances** between original question and perturbed questions.

questions under each perturbation. It turns out that the traditional edit distance is not suitable for measuring knowledge invariance due to its lack of correlation with knowledge invariance.

Within mastered question testing (results depicted in Table 4), we observe that most perturbations achieve an average PDR closed to zero, indicating consistent performance on the perturbed and the original data. The PDR of SwapPos on World History and Professional Psychology is less than -0.05 despite that it does not alter any knowledge-relevant information by design. A possible explanation is that SwapPos changes the global ordering of question prompts, thus affecting the token generation of these self-regressive LLMs. More detailed analyses of results could be found in Appendix D.

### Real Knowledge Capacity Evaluation

Each knowledge-invariant perturbation in PertEval targets a specific question restatement strategy. To construct the most challenging test scenario and obtain reliable and comprehensive evaluation results, we compose all perturbations to create a _composite perturbation_. The real knowledge capacity of LLMs is then quantified by ACC@Consist before and after the composite perturbation. The results, shown in Figure 3, yield several insights regarding the evaluation of LLMs' knowledge capacity.

**1. Overvaluation on static benchmarks**. The knowledge capacity of LLMs is significantly overvalued on static benchmarks. By comparing ACC@Original with ACC@Perturb and ACC@Consist for each LLM, we observe a notable performance drop when using the perturbed dataset. For instance, the performance of gpt-4-turbo, gpt-3.5-turbo, and gemini-1.0-pro decreases by more than 25%. Even the most capable LLM in our selection, gpt-4-turbo, achieves an ACC@Consist of only 0.555. These findings reveal a substantial gap between the evaluated knowledge capacity of LLMs on static datasets and their real knowledge capacity in complex scenarios.

**2. Robustly mastered knowledge**. Each LLM does have robustly mastered certain knowledge. This conclusion is drawn by comparing the LLMs' performance to pure guessing. With \(k\) options and a single correct answer, the expected ACC@Consist for random guessing is \(1/k^{2}\), which equals to 0.0625 for \(k=4\) in our experiments (see Appendix D.1 for detailed demonstration). We observe from Figure 3 that the ACC@Consist values of all tested LLMs are significantly higher than 0.0625, indicating that each LLM has indeed mastered some knowledge consistently. However, most LLMs have mastered less than half of the total knowledge, as indicated by ACC@Consist values below 0.5.

### Response Pattern Analysis

We further conduct a response pattern analysis to examine how perturbations affect the performance of LLMs. We find that knowledge-invariant perturbations affect LLMs by increasing the ratio of selecting extra incorrect options. As shown in Figure 4, the main reason for the significant performance drop in gpt-4-turbo is their increased frequency of selecting additional incorrect options on the perturbed data (_i.e., \(12\% 21.7\%\)_). These LLMs tend to select extra incorrect options alongside the correct ones. One possible explanation for this phenomenon is that **the LLMs indeed

Figure 3: **Real knowledge capacities measured by ACC@Consist\(\) with composite knowledge-invariant perturbation**. ACC@Original and ACC@Perturb denote accuracy on the original and perturbed data, respectively. We report the macro-averaged results on all tested datasets.

lack the ability to distinguish uncertain knowledge and filter out distracting options, while they correctly answer these questions in the original dataset via rote memorization. In other words, these LLMs do not truly master the knowledge behind the questions**. Therefore, it is crucial to not only enhance LLMs' ability to identify correct answers but also bolster their capability to recognize and eliminate incorrect answers when confronted with distracting choices in real-world scenarios. More detailed response pattern analysis results can be found in Appendix D.2.

### Overall Performance Stability

Table 5 presents the overall performance stability results, in which each number is the _macro_-PDR, _i.e.,_ the average of PDRs separately calculated on each dataset. We also apply the Wicoxon signed-rank test to the _micro_-PDR, which is the PDR calculated across all datasets combined. Specifically, let \(s=I(M(q_{x})=y_{x})\{0,1\}\) and \(s^{}=I(M(q_{(x)})=y_{(x)})\{0,1\}\) denote model \(M\)'s score on the original and perturbed questions, respectively. The alternative hypothesis of the Wilcoxon signed-rank test is \(H_{a}:s>s^{}\), or equivalently, \(H_{a}:s^{}-s<0\). We can analyze the results of Table 5 from both a column-wise and row-wise perspective. From the column-wise perspective, we first observe that all selected LLMs have negative macro-PDRs given the content-level perturbation, _i.e.,_ knowledge-invariant paraphrasing (KnInvPara). Additionally, the micro-PDR of close-sourced LLMs is _significantly_ negative, meaning that these LLMs lack robustness in content-level knowledge acquisition for these datasets. Another prominent observation is that all LLMs are highly sensitive to the SwapPos perturbation. We hypothesize that this sensitivity is due to the global format change introduced by SwapPos, which disrupts the text generation process of self-regressive LLMs, even though the perturbation is entirely knowledge-invariant. From the row-wise perspective, we find that our perturbations cause performance drop in most cases, _e.g.,_ at least 0.04 and up to 0.1 on average, indicating the universal vulnerability of current generation LLMs.

### Correct Response Consistency

The results of correct response consistency using ROP are presented in Table 6. There is significant variation in ROP across different LLMs. Among them, gpt-4-turbo performs the best, with an average macro-ROP of 0.9, which is a substantial advantage over other LLMs. Gemini-1.0-pro also performs well, particularly with the OptionForm and OptionCaesar strategies (both having a Macro-ROP

Figure 4: Response patterns of **gpt-4-turbo**. Left: original data; Right: perturbed data.

  
**ModelStrategy** & **KnInvPara** & **OptionPerm** & **OptionForm** & **OptionCaesar** & **ChangeType** & **SwapPos** & **AVG** \\ 
**gpt-4-turbo** & -0.0660\({}^{**}\) & -0.0208\({}^{**}\) & -0.0136 & -0.0294\({}^{**}\) & -0.0210 & -0.1117\({}^{**}\) & -0.0438 \\
**gpt-3.5-turbo** & -0.0275\({}^{**}\) & -0.0042 & -0.1767\({}^{**}\) & -0.0396\({}^{**}\) & -0.1736\({}^{**}\) & -0.1943\({}^{**}\) & -0.1027 \\
**gemini-1.0-pro** & -0.0558\({}^{**}\) & +0.0121 & +0.0125 & +0.0030 & -0.1310\({}^{**}\) & -0.1532\({}^{**}\) & -0.0521 \\
**glm-3-turbo** & -0.0370\({}^{**}\) & -0.0190 & -0.1397\({}^{**}\) & -0.0118 & +0.0522 & -0.2142\({}^{**}\) & -0.0616 \\
**mistral-7b-v0.2** & -0.0264 & -0.0200 & -0.2789\({}^{**}\) & +0.0793 & -0.0844\({}^{**}\) & -0.1275\({}^{**}\) & -0.0763 \\
**lama-3-8b** & -0.0336\({}^{**}\) & -0.0091 & -0.0939\({}^{**}\) & -0.0368\({}^{**}\) & -0.2920\({}^{**}\) & -0.1814\({}^{**}\) & -0.1078 \\ 
**AVG** & -0.0411 & -0.0102 & -0.1151 & -0.0059 & -0.1083 & -0.1637 & \\   ^{**}\): The _Micro_ PDR is significantly negative in the Wilcoxon signed-rank test (\(=0.01\)).} \\ 

Table 5: **Macro PDR \(\) and hypothesis test results of Micro PDR** of LLMs w.r.t. perturbation.

0.9). Other LLMs, however, are vulnerable to almost all knowledge-invariant perturbations, which means that their performance on the original datasets are less reliable. On the other hand, the effect of different strategies on correct response consistency varies considerably. Similar to the results for performance stability, SwapPos is the most influential strategy affecting LLMs' correct response consistency, which exposes the inherent flaw of LLMs in defending global order perturbation.

## 5 Discussion

**Conclusion.** The trustworthiness of capacity evaluation is, and will always be one of the most significant issues in the development of LLMs. In pursuit of this goal, we proposed an evaluation toolkit, PertEval, for unveiling LLMs' real knowledge capacity on close-ended benchmarks. We discovered not only a substantial overestimation of the knowledge capacity of LLMs by static benchmarks, but also LLMs' uncertainty to distinguish specious knowledge without rote memorization. Detailed response consistency analyses revealed the vulnerability of LLMs to various knowledge-invariant perturbations, especially the content-level KnInvPara and the format-level SwapPos, which could inspire further refinement for LLMs' knowledge mastery. Indeed, _the ultimate goal of this study is not only trustworthy evaluation, but to propel the development of LLMs' knowledge capacity_. Our primary experiments have verified the feasibility of using PertEval-generated data to improve LLM robustness to various test conditions via supervised fine-tuning (see Appendix D.3). We believe that PertEval marks a significant step toward more trustworthy LLM evaluation.

**Limitations & Future Work.** The first limitation of this work is its scope regarding suitable benchmarks. PertEval is suitable for close-ended benchmarks and cannot be directly applied to open-ended ones. The challenges stem from the variety of open-ended questions and the difficulty of their objective evaluation. Future research could explore how the concept of knowledge-invariant perturbation can be adapted to improve the trustworthiness of open-ended evaluations. The second limitation of PertEval is the lack of controllability, a common issue in LLM-based text generation. Due to the complexity of LLMs, it is challenging to precisely control specific characteristics in paraphrased texts while keeping knowledge-invariance. In the future, we aim to achieve more fine-grained controllability in PertEval, such as quantitatively adjusting paraphrasing tone and length, to obtain more continuous and nuanced evaluation results of LLMs. Besides, all conclusions of this study are based on experiments conducted on a subset of MMLU due to budget constraints. We are seeking economic methods to expand our experiments and further substantiate our findings. Finally, the risk of adaptive optimization against PertEval exists as it is an open-sourced toolkit. However, the diversity of perturbations and the cost of adaptive optimization could effectively reduce the influence of adaptive optimization. By constructing a PertEval open evaluation platform, we can regularly develop and change perturbation strategies to prevent this procedure.

  
**ModeStrategy** & **KnInvPara** & **OptionPerm** & **OptionForm** & **OptionCaesar** & **ChangeType** & **SwapPos** & **AVG** \\ 
**gpt-4-turbo** & 0.8798 & 0.9063 & 0.9601 & 0.9349 & 0.9221 & 0.7995 & 0.9005 \\
**gpt-3.5-turbo** & 0.8230 & 0.7728 & 0.5602 & 0.7801 & 0.5194 & 0.5610 & 0.6694 \\
**gemini-1.0-pro** & 0.7968 & 0.7995 & 0.9553 & 0.9226 & 0.6788 & 0.6362 & 0.7982 \\
**glm-3-turbo** & 0.7164 & 0.6756 & 0.5943 & 0.7884 & 0.6816 & 0.4026 & 0.6432 \\
**mistral-7b-v0.2** & 0.6543 & 0.6202 & 0.1339 & 0.7679 & 0.5128 & 0.4060 & 0.5159 \\
**llama-3-Sh** & 0.8055 & 0.7433 & 0.7541 & 0.8102 & 0.3971 & 0.5209 & 0.6719 \\ 
**AVG** & 0.7793 & 0.7530 & 0.6597 & 0.8340 & 0.6186 & 0.5544 & \\   

Table 6: **Macro Recall of Performance (ROP) \(\) of LLMs w.r.t. perturbation strategies.**