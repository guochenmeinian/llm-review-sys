# MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models

Peng Xia\({}^{1}\), Kangyu Zhu\({}^{5}\), Haoran Li\({}^{6}\), Tianze Wang\({}^{3}\), Weijia Shi\({}^{4}\),

Sheng Wang\({}^{4}\), Linjun Zhang\({}^{3}\), James Zou\({}^{2}\), Huaxiu Yao\({}^{1}\)

\({}^{1}\)UNC-Chapel Hill, \({}^{2}\)Stanford University, \({}^{3}\)Rutgers University,

\({}^{4}\)University of Washington, \({}^{5}\)Brown University, \({}^{6}\)PloyU

{pxia,huaxiu}@cs.unc.edu

###### Abstract

Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount of high-quality data and distribution shifts between training data and deployment data limit the application of fine-tuning methods. Although RAG is lightweight and effective, existing RAG-based approaches are not sufficiently general to different medical domains and can potentially cause misalignment issues, both between modalities and between the model and the ground truth. In this paper, we propose a versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an adaptive retrieved contexts selection, and a provable RAG-based preference fine-tuning strategy. These innovations make the RAG process sufficiently general and reliable, significantly improving alignment when introducing retrieved contexts. Experimental results across five medical datasets (involving radiology, ophthalmology, pathology) on medical VQA and report generation demonstrate that MMed-RAG can achieve an average improvement of 43.8% in the factual accuracy of Med-LVLMs.

## 1 Introduction

Artificial Intelligence (AI) has already transformed healthcare and still has a lot of potential for further advancements (Tautan et al., 2021; Wang et al., 2019; Ye et al., 2021; Tu et al., 2024). Recently, Medical Large Vision-Language Models (Med-LVLMs) have shown great promise for advancing interactive and intelligent diagnosis (Li et al., 2023; Moor et al., 2023; Zhang et al., 2023; Wu et al., 2023). Despite this potential (Li et al., 2023; Wu et al., 2023; Shi et al., 2024), current Med-LVLMs still face significant reliability issues, particularly their tendency to generate non-factual medical responses (Xia et al., 2024; Royer et al., 2024; Chen et al., 2024; Jiang et al., 2024), making them unreliable in critical medical applications. These factuality issues raise serious concerns when deploying such models in clinical settings, where even small diagnostic errors could lead to severe consequences for patient care.

Recently, researchers have begun to focus on improving the factuality of Med-LVLMs through various techniques, including fine-tuning (Li et al., 2023; Moor et al., 2023; Thawkar et al., 2023; Zhang et al., 2023; Chen et al., 2024) and retrieval-augmented generation (RAG) (Xia et al., 2024; He et al., 2024; Sun et al., 2024). Fine-tuning is a direct method to improve model performance, but it faces several limitations in the medical field. First, there is a lack of sufficient high-quality labeled data for fine-tuning in the medical domain. Additionally, a distribution gap often exists between the training data and the real-world deployment data (Schrouff et al., 2022), leading to significantly worse model performance during deployment. Hence, RAG has emerged as a viable alternative by providing external references during the inference stage, enhancing the factuality of Med-LVLMs (Wu et al., 2023; Gao et al., 2023). However, despite its advantages, current RAG implementations in Med-LVLMs have significant limitations. First, these methods tend tobe _dataset-specific_, reducing their generalizability across various medical domains. Second, these models are still facing _misalignment issues_ that lead to factuality problems. This misalignment may arise from the impact of adding RAG on the original Med-LVLMs' _cross-modality alignment_, as well as on the _overall alignment_ between the model and ground truth.

To address these challenges, we propose a versatile factual **M**ultimodal **M**edical **RAG** system called **MMed-RAG**. Specifically, MMed-RAG first introduces a domain-aware retrieval mechanism, designed to handle different domains of medical images more effectively. Here, we design a domain identification module to adaptively select a corresponding retrieval model given the input medical image. Secondly, we include a adaptive calibration approach for selecting the number of retrieved contexts. Lastly, MMed-RAG incorporates RAG-based preference fine-tuning to enhance cross-modality alignment and overall alignment with ground truth. The preference pairs are designed to achieve two goals: first, to improve cross-modality alignment by encouraging the model to avoid generating responses without utilizing input medical images, even the responses are correct; second, to improve overall alignment by encouraging the model to understand retrieved contexts when unsure, while avoiding interference from irrelevant retrieved information.

The primary contribution of this paper is MMed-RAG, a versatile multimodal RAG system designed specifically for Med-LVLMs to generate more factual responses. Under mild assumptions, our theoretical analysis demonstrates that MMed-RAG mitigates both cross-modality misalignment and overall misalignment with ground truth. Furthermore, empirical results on five medical multimodal datasets, covering three medical image modalities (radiology, pathology, and ophthalmology), show that MMed-RAG significantly improves the factual accuracy of Med-LVLMs, achieving improvements of 18.5% and 69.1% on Medical VQA and report generation tasks, respectively, compared to the original Med-LVLM. These empirical findings further demonstrate the effectiveness of our proposed components and support the theoretical analysis in addressing misalignment issues.

## 2 Preliminaries

In this section, we will provide a brief overview of Med-LVLMs and preference optimization.

**Medical Large Vision Language Models**. Med-LVLMs bridge LLMs with medical visual modules, allowing the model to take medical image \(x_{v}\) and clinical query \(x_{t}\) as input \(x\), and autoregressively predict the probability distribution of the next token. The text output is denoted as \(y\).

**Preference Optimization**. Preference optimization has achieved remarkable results in LLM alignment. Give an input \(x\), a language model policy \(_{}\) can produce a conditional distribution \(_{}(y x)\) with \(y\) as the output text response. The recently popular DPO (Rafailov et al., 2023) utilizes preference data achieve objective alignment in LLMs. The preference data is defined as \(=\{x^{(i)},y_{v}^{(i)},y_{v}^{(i)}\}_{i=1}^{N}\), where \(y_{v}^{(i)}\) and \(y_{v}^{(i)}\) represent preferred and dispreferred responses given an input prompt \(x\). The probably of obtaining each preference pair is \(p(y_{w} y_{l})=(r(x,y_{w})-r(x,y_{l}))\), where \(()\) is the sigmoid function. In DPO, the optimization can be formulated as classification loss over the preference data as:

\[_{}(_{};_{})=-_{(x, y_{w},y_{l})}[((y_{w}  x)}{_{}(y_{w} x)}-(y_{w}  x)}{_{}(y_{w} x)})].\] (1)

where \(_{}\) represents the reference policy, which is the LLM fine-tuned through supervised learning.

## 3 MMed-RAG: A Versatile Medical RAG System

In this section, as illustrated in Figure 1, we will propose MMed-RAG, a versatile RAG system for improving the factuality of Med-LVLMs. Specifically, MMed-RAG consists of three complementary modules. First, we design a domain-aware retrieval mechanism to select the optimal retriever by feeding each given medical image to the domain identification module. Second, to select an optimal number of retrieved contexts and filter out low-quality information, MMed-RAG adopts a adaptive method by filtering out low-quality information using the similarity scores during the RAG phase. Lastly, we use a RAG-based preference fine-tuning approach to improve the cross-modality alignment and the overall alignment between groundtruth. We detail these steps as follows:

### Domain-Aware Retrieval Mechanism

In MMed-RAG, we introduce a domain-aware retrieval mechanism to efficiently handle medical images from different sources (e.g., radiology, pathology, ophthalmology). Specifically, we first employ a domain identification module that assigns a domain label to each input medical image. To achieve this, we create a small dataset with medical images as inputs and their corresponding domain labels as outputs, using this dataset to fine-tune the BiomedCLIP model (Zhang et al., 2023) to improve its domain awareness. Formally, for a given medical image \(x_{v}\), we predict its domain \(d=(x_{v})\). Based on the assigned domain label \(d\), the image \(x_{v}\) is fed into the corresponding multimodal retriever \(_{d}()\) for knowledge retrieval.

Here, each multimodal retriever \(_{d}()\) for each domain \(d\) is trained through contrastive learning (Radford et al., 2021). Specifically, the visual and textual information \(X_{img},X_{txt}\) are processed by their corresponding encoders \(_{img}(),_{txt}()\) to generate textual and visual embeddings \(V_{txt}=_{txt}(X_{txt}),V_{img}=_{img}(X_{img})\). Contrastive learning loss is then applied to maximize the similarity between text and image embeddings representing the same example, while minimizing the similarity between embeddings representing different examples, as defined below:

\[=_{img}+_{txt}}{2},\ _{img}=-_{i=1}^{N})}{_{j=1 }^{N}(S_{i,j})},_{txt}=-_{i=1}^{N})}{_{j=1}^{N}(S_{j,i})},\] (2)

where \(S^{N N}\) represents the similarity matrix between image and text modalities, calculated as: \(S=}{|V_{img}|}(}{|V_{txt}|})^{T}\), where each element \(S_{i,j}\) represents the similarity between the image representation of example \(i\) and the text representation of example \(j\).

Finally, for the input image \(x_{t}\), after feeding into the corresponding multimodal retriever \(_{d}()\), the multimodal retriever will retrieves the top-\(k\) most similar reports for the image. These retrieved reports \(x_{r}=_{d}(x_{v})\) are then provided to the Med-LVLM \(()\) as references to guide the generation.

### Adaptive Retrieved Context Selection

Following the domain-aware retrieval mechanism, the next step is to determine the optimal amount of context to retrieve.

Figure 1: Overview of MMed-RAG, a versatile factual multimodal RAG system designed to enhance the reliability of Med-LVLMs. It introduces a domain-aware retrieval mechanism that effectively handles different domains of medical images by selecting suitable retrieval models. Additionally, it uses an adaptive context selection approach to determine the optimal number of retrieved contexts and employs preference fine-tuning to improve both cross-modality and overall alignment.

Figure 2: Relations between selected contexts and similarity score.

Retrieving too much or too little information can result in hallucinations (Xia et al., 2024b). Current RAG methods applied to Med-LVLMs generally rely on empirical results or fixed values based on validation sets to select the optimal value of the number of retrieved contexts \(k\)(Xia et al., 2024b; He et al., 2024; Sun et al., 2024b). However, the distribution of similarity scores varies depending on the complexity of the image and its alignment with the textual information from the data source. These fixed-\(k\) methods do not guarantee optimal performance on target data, as they overlook the similarity scores generated during the retrieval process. To address this, we propose an adaptive method that dynamically selects \(k\) based on the similarity scores of the retrieved contexts. Specifically, during the domain-aware retrieval mechanism phase, the retrieved information is denoted as \(x_{r}(k)=_{d}(x_{v};k)\), where \(k\) represents the number of retrieved contexts, and the corresponding similarity scores are denoted as \(S_{k}\). For simplicity, when there is no ambiguity, we will refer to \(x_{r}(k)\) as \(x_{r}\).

As illustrated in Figure 2, our method is based on a key observation: the similarity scores (CLIP score in this case) between retrieved contexts often exhibit a sharp decline after a certain number of results (nearly top-9 in this case). This suggests that lower-quality information can still be included among the top-\(k\) retrieved contexts when using a fixed-\(k\) strategy, especially in cases where the fixed value of \(k\) is too large. These lower-quality retrievals introduce noise and irrelevant information, which can significantly impair the model's ability to generate factual and coherent responses. To mitigate this issue, we draw inspiration from the Gap statistic method used in clustering (Tibshirani et al., 2001) and extend this concept to RAG for Med-LVLMs. Specifically, after retrieving the top-\(k\) contexts, we perform an additional round of \(k\) optimization by analyzing the similarity ratios between consecutive retrievals. These similarity ratios are denoted as \(u_{i}=(S_{i}/S_{i+1})\) for \(0<i k\), where \(S_{i}\) represents the similarity score of the \(i\)-th retrieved context. When \(u_{i}\) exceeds a predefined threshold \(\), this indicates a substantial drop in relevance, suggesting that the remaining retrievals are less likely to contribute preferredly to the model's output. At this point \(i\), we truncate \(k\), effectively discarding the less relevant retrievals that follow. This adaptive truncation mechanism ensures that only the most relevant contexts are retained for generating the final response, reducing the risk of hallucination and improving the factual accuracy of the outputs.

Although the threshold \(\) is fixed, this approach provides a adaptive way to balance the bias and variance in retrieved contexts. By adapting to the characteristics of each input \(x_{v}\), our method enhances the robustness of the retrieval process and ensures that the selection of \(k\) is tailored to the specific data at hand, thereby improving overall performance across diverse contexts and tasks.

### RAG-based Preference Fine-Tuning

After context selection, MMed-RAG supplies Med-LVLM with reliable retrieved information as external knowledge to aid in generating factual responses. However, incorporating this retrieved knowledge may potentially disrupt the original alignment within the existing Med-LVLM, a concern we will elaborate on below:

**Alignment Analysis.** In the alignment analysis, we aim to explore how incorporating retrieved context impacts the original alignment in Med-LVLMs, focusing on two key aspects: (1) cross-modality alignment and (2) overall alignment with the ground truth. To evaluate cross-modality alignment, we conduct two tests on LLaVA-Med-1.5 (Li et al., 2023a) using the Harvard-FairVLMed (Luo et al., 2024) dataset. First, when replacing the original image with a highly noisy image associated with a different ground truth, the original model gives incorrect answers (the ground truth being the response for the original image). After incorporating RAG, where context is retrieved based on the original image, 55.08% of these cases return correct answers. This indicates that the model _directly references the retrieved knowledge_ without considering the input image, highlighting significant _cross-modal misalignment issues_. Furthermore, 43.31% of the questions that were originally answered correctly are answered incorrectly after incorporating RAG, suggesting _interference from incorrect retrieval information_, which leads to,_overall misalignment with the ground truth_. To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm 1, we propose a RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM \(()\). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment.

**Preference Pairs for Cross-Modality Alignment.** We first construct preference pairs aimed at improving cross-modality alignment. In this dataset, we select samples from \(=\{x_{v}^{(i)},x_{t}^{(i)},y^{(i)}\}_{i=1}^{N}\)where \(x_{v}\), \(x_{t}\), and \(y\) represent the input medical image, clinical query, and ground-truth answer, respectively. For simplicity, we omit the sample index \((i)\) in the following sections. A model's correct response using retrieved knowledge, i.e., \((x_{v},x_{t}+x_{r})=y\), is considered a preferred response \(p_{i}\), where \(x_{r}\) is the retrieved information. A dispreferred response \(n_{i}\) is selected from cases where the model makes a correct inference based on an unrelated image, i.e., \((x_{v}^{*},x_{t}) y\), but \((x_{v}^{*},x_{t}+x_{r})=y\), reflecting the model's reliance on the retrieved knowledge. The unrelated images \(x_{v}^{*}\) are generated through a two-step process: first, we use the retriever to select an image \(x_{v}^{}\) with the lowest similarity to the target image; then, we introduce diffusion noise into the selected unrelated image. We define the noise step as \(s\), and the noised image at step \(s\) is expressed as:

\[x_{v}^{*}=} x_{v}^{}+},\] (3)

where \(}=_{i=0}^{s}_{i}\) and \(_{s}(0,1)\) is a hyperparameter. The preference pairs constructed in this stage are denoted as \(_{cm}\). By comparing the preferred and dispreferred responses in \(_{cm}\), we encourage the model to prioritize the input medical image when generating responses.

**Preference Pairs for Overall Alignment.** Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model's ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, \(_{oa}^{1}\), is designed to strengthen the model's comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., \((x_{v},x_{t}+x_{r})=y\), while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., \((x_{v},x_{t}) y\). Comparing these preferred and dispreferred responses enhances the model's understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, \(_{oa}^{2}\), the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., \((x_{v},x_{t})=y\), while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., \((x_{v},x_{t}+x_{r}) y\). This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, \(_{oa}=^{1}_{oa}^{2}_{oa}\).

Finally, we merge the first and second preference set and denote the preference dataset as \(_{pt}=_{em}_{oa}=\{x^{(i)},y^{(i)}_{w,o},y^{ (i)}_{h,o}\}_{i=1}^{N}\), where \(y^{(i)}_{w,o}\), \(y^{(i)}_{h,o}\) are represented as preferred and dispreferred responses, respectively. Based on the curated preferences, we fine-tune Med-LVLM using direct preference optimization (Rafailov et al., 2023) with the following loss:

\[_{pt}=-_{(x,y_{w,o},y_{l,o})}[ ((y_{w,o}|x)}{_{a}(y_{w,o}|x)}- (y_{w,o}|x)}{_{a}(y_{w,o}|x)})].\] (4)

## 4 Experiment

In this section, we evaluate the performance of MMed-RAG, aiming to answer the following questions: (1) Can MMed-RAG effectively improve the factuality of Med-LVLMs compared to decoding-based and RAG-based baselines? (2) How effective is each proposed component on performance? (3) What is the effect of preference data for different alignment goals? and (4) Does MMed-RAG actually improve cross-modality alignment and overall alignment?

### Experimental Setups

**Implementation Details**. We use LLaVA-Med-1.5 7B (Li et al., 2023a) as the backbone model. During the preference fine-tuning process, we adapt LoRA fine-tuning (Hu et al., 2021). For the training of retriever, the vision encoder is a ResNet-50 (He et al., 2016), and the text encoder is a bio-BioClinicalBERT (Alsentzer et al., 2019). We use the AdamW optimizer with a learning rate of \(10^{-3}\), weight decay of \(10^{-2}\) and a batch size of 32. The model is trained for 360 epochs. For more detailed information on training hyperparameters and training data, please see Appendix A.1.1.

**Baseline Methods**. We compare MMed-RAG with two types of LVLM hallucination mitigation methods that show promising results in natural image understanding. 1) Decoding-based methods, including Greedy Decoding, Beam Search (Sutskever et al., 2014), DoLa (Chuang et al., 2023), OPERA (Huang et al., 2023), VCD (Leng et al., 2023). These methods manipulate the logits of the model's output tokens to enhance factual accuracy. 2) Multimodal RAG-based methods, including MedDr (He et al., 2024), FactMM-RAG (Sun et al., 2024b), RULE (Xia et al., 2024b). Furthermore, we compare the performance with other open-source Med-LVLMs, including Med-Flamingo (Moor et al., 2023), MedVInT (Zhang et al., 2023b), RadFM (Wu et al., 2023b).

**Evaluation Datasets**. We utilize five medical vision-language datasets for medical VQA and report generation tasks, i.e., MIMIC-CXR (Johnson et al., 2019), IU-Xray (Demner-Fushman et al., 2016), Harvard-FairVLMed (Luo et al., 2024), PMC-OA (Lin et al., 2023a) (we only select the pathology part) and Quilt-1M (Ikezogwoo et al., 2024). These datasets cover radiology, ophthalmology, and pathology. To construct the VQA benchmarks, following (Xia et al., 2024a), we generate question-answer pairs from medical reports using GPT-4 (OpenAI, 2023), with answers formatted as _yes_ or _no_. Pathology images are excluded from the report generation task due to their brief and insufficient descriptions. The detailed dataset descriptions are provided in the Appendix A.2.

**Evaluation Metrics**. Following (Jing et al., 2017; Lin et al., 2023b), we use Accuracy, F1 Score and AUROC for evaluating medical VQA task, and BLEU Score (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Banerjee & Lavie, 2005) for evaluating report generation task.

### Main Results

In this section, we provide a comprehensive comparison with various baseline methods and other open-source Med-LVLMs on medical VQA and report generation tasks.

**Comparison with Baselines.** We compare MMed-RAG with baseline methods on medical VQA and report generation tasks, with the results presented in Table 1 and Table 2, respectively. Overall, MMed-RAG outperforms all baselines across nearly all metrics and datasets. Specifically, MMed-RAG demonstrates a significant performance boost, improving by 18.5% and 69.1% over the original Med-LVLM in medical VQA and report generation tasks, respectively. When compared to baseline methods, MMed-RAG surpasses decoding-based approaches, achieving improvements of 11.5% and 44.2% in the two tasks. Furthermore, recent RAG-based methods show substantial improvements over earlier techniques, yet our approach still outperforms RAG-based baselines by 2.8% and 16.1% in the medical VQA and report generation tasks, respectively. This indicates that MMed-RAG effectively mitigates misalignment issues introduced by RAG. Notably, MMed-RAG achieves more pronounced gains in report generation, likely due to the higher complexity of the task and the greater influence of retrieved contexts in guiding open-ended generation.

**Comparison with Other Med-LVLMs.** To provide a comprehensive comparison, we evaluate MMed-RAG against other open-source Med-LVLMs to demonstrate the superiority of our approach. We assess the performance of these models across different medical image modalities, reporting the average results for medical VQA and report generation tasks in Table 3 (see Appendix A.6 for detailed results). Our findings show that MMed-RAG significantly outperforms Med-LVLMs pre-trained on large-scale datasets across various domains. This reinforces the generalizability and effectiveness of our approach across diverse image domains and medical multimodal tasks.

### Analysis

In this section, we provide a detailed analysis of each module's performance, along with a series of analytical experiments, to better understand the performance gains of MMed-RAG. Additionally, we demonstrate the compatibility of our approach, achieving a consistent 40.3% performance improvement on a different backbone, i.e., LLAVA-Med-1.0 (see details in Appendix A.6).

    &  &  &  \\   &  &  &  &  &  \\   & Acc & F1 & AUC & Acc & F1 & AUC & Acc & F1 & AUC & Acc & F1 & AUC & Acc & F1 & AUC \\  LLAVA-Med-1.5 & 75.47 & 64.04 & 67.46 & 75.79 & 80.49 & 68.84 & 63.03 & 74.11 & 63.05 & 62.80 & 72.90 & 60.03 & 59.28 & 71.98 & 54.19 \\   Greedy & 76.88 & 65.9 & 68.74 & 78.32 & 86.76 & 71.13 & 82.54 & 85.98 & 90.09 & 64.72 & 70.12 & 58.75 & 68.14 & 70.42 & 51.10 \\   & 69.61 & 66.06 & 68.77 & 81.56 & 86.73 & 73.99 & 80.88 & 68.94 & 65.32 & 69.33 & 57.65 & 56.29 & 69.84 & 52.89 \\  & 78.00 & 66.75 & 72.19 & 81.35 & 85.73 & 72.73 & 76.87 & 85.53 & 61.07 & 64.91 & 57.58 & 57.71 & 70.27 & 52.95 \\  & 70.99 & 69.51 & 64.32 & 69.34 & 66.26 & 71.41 & 81.37 & 65.59 & 60.51 & 63.24 & 57.95 & 52.83 & 50.31 & 51.86 \\  & 68.99 & 54.35 & 61.08 & 70.89 & 75.57 & 64.61 & 65.88 & 77.20 & 64.16 & 61.43 & 67.39 & 55.72 & 55.10 & 67.94 & 51.62 \\   & 83.13 & 67.70 & 71.55 & 51.66 & 58.54 & 70.17 & 80.72 & 61.15 & 61.53 & 73.23 & 67.01 & 59.97 & 69.19 & 57.01 \\  & 84.16 & 68.51 & 77.07 & 77.58 & 81.86 & 70.09 & 83.67 & 72.21 & 70.09 & 29.73 & 76.82 & 68.15 & 69.49 & 69.38 & 57.31 \\  & 87.84 & 78.00 & 85.78 & 83.92 & 87.49 & 83.44 & 87.12 & 92.29 & 70.08 & 69.77 & 73.80 & 68.13 & 64.41 & 70.36 & 58.91 \\  MMed-RAG & **89.54** & 80.72 & 87.13 & 83.57 & 88.49 & 85.08 & 87.94 & 92.78 & 80.81 & 72.95 & 76.35 & 72.25 & 66.45 & 73.09 & 61.42 \\   

Table 1: Model performance (%) of different methods based on LLAVA-Med-1.5 on medical VQA task. Notably, we report the accuracy, F1 score and AUROC. The best results and second best results are highlighted in \(|||\) and \(||\), respectively.

    &  &  \\   &  &  &  \\   & BLEU & ROUGE-L & METEOR & BLEU & ROUGE-L & METEOR & BLEU & ROUGE-L & METEOR \\  LLAVA-Med-1.5 & 9.64 & 12.26 & 8.21 & 12.11 & 13.05 & 11.16 & 18.11 & 11.36 & 10.75 \\  + Greedy & 11.47 & 15.38 & 12.69 & 16.63 & 14.26 & 14.19 & 17.98 & 11.49 & 13.77 \\ + Beam Search & 12.10 & 16.21 & 13.17 & 16.97 & 14.74 & 14.43 & 18.37 & 12.62 & 14.50 \\ + Dela & 11.79 & 15.82 & 12.72 & 17.11 & 14.89 & 14.81 & 18.26 & 12.51 & 14.51 \\ + OPERA & 10.66 & 14.70 & 12.01 & 15.40 & 12.52 & 13.72 & 16.59 & 11.47 & 13.63 \\ + VCD & 10.42 & 14.14 & 11.59 & 15.18 & 12.30 & 13.38 & 16.73 & 11.38 & 13.89 \\  + MedDrr & 12.37 & 16.45 & 13.50 & 18.59 & 15.72 & 16.77 & 19.82 & 13.72 & 15.40 \\ + FactM-RAG & 14.70 & 18.05 & 15.92 & 18.71 & 15.84 & 16.82 & 20.82 & 14.17 & 15.31 \\ + RULE & 27.53 & 23.16 & 27.99 & 18.61 & 15.96 & 17.42 & 22.35 & 14.93 & 17.74 \\  MMed-RAG & **31.38** & 25.59 & 32.43 & 23.25 & 12.34 & & 20.47 & 24.82 & 16.59 & 19.85 \\   

Table 2: Model performance (%) of different methods based on LLAVA-Med-1.5 on report generation task. Notably, we report the average BLEU, ROUGE-L, METEOR.

   Model &  &  \\  & VQA & RG & VQA & RG \\  LLAVA-Med-1.5 & 68.99 & 10.04 & 66.63 & 13.41 \\ +DR & 77.12 & 13.32 & 72.69 & 15.89 \\ +RCS & 79.56 & 17.92 & 75.74 & 17.22 \\ +RAG-PT (Oars) & **85.80** & **29.80** & **87.18** & **20.42** \\    

Table 4: Ablation results on two datasets covering different domains. RG: report generation, FairVLMed: Harvard-FairVLMed.

   Model &  & 
   Model &  &  \\  & VQA & RG & VQA & RG \\  LLAvAv-Med-1.5 & 68.99 & 10.04 & 66.63 & 13.41 \\
*+RAG-PT 1 & 80.19 & 19.38 & 79.42 & 18.77 \\
*+RAG-PT 2 & 80.27 & **20.16** & 79.35 & 18.66 \\
*+RAG-PT 3 & **81.30** & 19.43 & **80.07** & **18.52** \\   

Table 5: Performance using RAG-PT based on subsets of preference data.