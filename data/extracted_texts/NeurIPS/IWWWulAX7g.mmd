# Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering

Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering

Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, Bill Byrne

Department of Engineering

University of Cambridge

Cambridge, United Kingdom CB2 1PZ

{w1356, jc2124, jm2245, ac2123, wjb31}@cam.ac.uk

Equally contributed as the first author

###### Abstract

Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems to utilize knowledge from external knowledge bases to answer visually-grounded questions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strong framework to tackle KB-VQA, first retrieves related documents with Dense Passage Retrieval (DPR) and then uses them to answer questions. This paper proposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) which significantly improves knowledge retrieval in RA-VQA. FLMR addresses two major limitations in RA-VQA's retriever: (1) the image representations obtained via image-to-text transforms can be incomplete and inaccurate and (2) relevance scores between queries and documents are computed with one-dimensional embeddings, which can be insensitive to finer-grained relevance. FLMR overcomes these limitations by obtaining image representations that complement those from the image-to-text transforms using a vision model aligned with an existing text-based retriever through a simple alignment network. FLMR also encodes images and questions using multi-dimensional embeddings to capture finer-grained relevance between queries and documents. FLMR significantly improves the original RA-VQA retriever's PRRecall@5 by approximately 8%. Finally, we equipped RA-VQA with two state-of-the-art large multi-modal/language models to achieve \( 61\%\) VQA score in the OK-VQA dataset.

## 1 Introduction

Knowledge-based Visual Question Answering (KB-VQA) is a challenging problem that lies at the intersection of Computer Vision, Natural Language Processing, and Information Retrieval. The objective of VQA is to read an image and answer a question related to the image content. KB-VQA poses an additional challenge: in order to answer the question correctly, the system needs to draw on relevant information from an external knowledge source, such as a knowledge graph or a database. Therefore, tackling KB-VQA tasks crucially depends on the ability to retrieve relevant information and to ground the answer generation process in the retrieved knowledge.

Retrieval Augmented Visual Question Answering (RA-VQA) is a framework designed to answer difficult KB-VQA questions , with the most recent version from Lin and Byrne  achieving performance close to large models (such as GPT-3 [Brown et al., 2020]) while using much simpler models. RA-VQA first retrieves \(K\) documents relevant to the image and the question from an external knowledge base, and then generates the answer using a Large Language Model (LLM) grounded in the retrieved passages.

We focus on two major limitations in RA-VQA's retriever. (1) Incomplete image understanding: image representations are obtained via image-to-text transforms such as captioning and object detection. While effective, this approach can result in incomplete image understanding, which hinders the retrieval of relevant knowledge. This is a common issue for retrieval-based KB-VQA systems in the literature. (2) Lossy compression of visual scenes and questions to a single embedding: the Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) retriever, widely used in current retrieval-based QA systems, computes relevance scores between queries and documents with their respective, one-dimensional embeddings. However, compressing complex visual scenes and questions into a single embedding can be lossy. This is especially problematic in KB-VQA, where queries and visual elements are more diverse than in other Open-domain QA tasks. DPR could overlook finer-grained relevance, resulting in degraded retrieval performance.

To address these two limitations we propose an enhanced knowledge retrieval approach called Fine-grained Late-interaction Multi-modal Retrieval (FLMR). FLMR incorporates finer-grained, token-level visual and textual features into multi-dimensional embeddings. When computing relevance scores, FLMR considers the interaction between every pair of token embeddings, including cross-modality interaction between texts and images, enabling a finer-grained assessment of relevance. We also introduce large vision models such as ViT (Dosovitskiy et al., 2021) to produce visual tokens that complement text-based image representations for more complete image understanding. To ensure that the interactions between visual and text tokens are well-defined, we align the vision model with the text-based retriever with a simple yet effective alignment training procedure. We also find that FLMR is able to make use of finer-grained regions of interest, leading to better recall rate, whereas DPR's recall rate degrades when these finer-grained features are incorporated. Our FLMR retriever achieves a significant increase of approximately 8% in PRRecall@5 for knowledge retrieval, and a competitive VQA score of 61%, surpassing the state-of-the-art models with the same scale of parameters.

We summarize our contributions as follows:

* We introduce FLMR, the first-of-its-kind to leverage Late Interaction2 and multi-dimensional representations to capture fine-grained, cross-modality relevance that significantly improve retrieval performance over existing state-of-the-art KB-VQA retrievers; * We show that introducing image representations from large vision model after a simple yet effective alignment procedure can complement image representations obtained via image-to-text transforms, leading to more complete image understanding, better knowledge retrieval and higher VQA accuracy. This offers improvements to current VQA systems as many systems have only a single mode of image understanding that relies on either image-to-text transforms or vision models;
* We achieve a substantial improvement of approximately 8% in knowledge PRRecall@5 over other state-of-the-art retrievers in the OK-VQA dataset, with an accuracy of 61% that surpasses other systems with similar parameter sizes.

## 2 Related Work

**Visual Question Answering Systems.** Recent work in VQA can be roughly divided into four categories with respect to multi-modal modeling: (1) Visual and textual features can be fused via cross-modality fusion (Yu et al., 2018; Singh et al., 2019; Yu et al., 2019; Jiang et al., 2020; Guo et al., 2021); (2) Multi-modal models can be trained from scratch to jointly understand vision and language before they are fine-tuned to perform VQA tasks (Tan and Bansal, 2019; Chen et al., 2020; Gan et al., 2020; Li et al., 2020; Wang et al., 2022; Zhang et al., 2021; Li et al., 2021); (3) Vision model and language model that have been pre-trained on uni-modal corpora can be aligned to avoid expensive multi-modal pre-training (Guo et al., 2023; Dai et al., 2022; Singh et al., 2022). (4) Image-to-text transforms such as captioning can be used to transform images into texts to enable the use of text-only reasoning pipelines (Lin and Byrne, 2022; Gui et al., 2021; Lin et al., 2022; Luo et al., 2021; Yang et al., 2022; Gao et al., 2022; Hu et al., 2022). Building on these Vision-and-Language modeling techniques, our work shows that image-to-text transforms and aligned vision models can complement each other to provide more complete visual information, leading to improved performance in both knowledge retrieval and VQA.

**Knowledge-based VQA Systems.** Recent KB-VQA systems can access both structured data, such as ConceptNet and other KGs , as well as unstructured data such as Wikipedia passages  for knowledge retrieval. LLMs can also be a source of "implicit world knowledge": KAT  and REVIVE  prompt GPT-3 to generate potential answer candidates. RA-VQA  and its prior works  ground answer generation in the retrieved knowledge from external KBs to achieve excellent VQA performance. Our work improves this retriever-reader pipeline with a novel knowledge retriever which significantly improves the recall rate of knowledge retrieval as well as the final VQA performance.

**Knowledge Retrieval.** Most knowledge retrievers in QA systems are based on DPR and its variants . These mainly use one-dimensional embeddings and contrastive learning for training. Late Interaction models  have recently achieved state-of-the-art performance on QA knowledge retrieval. Our FLMR extends this paradigm to work with multi-modal features and shows that incorporating finer-grained visual features, such as regions-of-interest, leads to superior retrieval performance. EnFoRe  retrieves a list of entities from the image, the query, and the answer candidates, and then explicitly learns scores to indicate the importance of each fine-grained entity. FILIP  has a similar late-interaction setting but it focuses on single modal query (image-text retrieval). To the best of our knowledge, FLMR is also the first to introduce cross-modality, token-level late interactions to compute relevance scores for KB-VQA knowledge retrieval. We also propose a light-weight method that aligns a vision model with a text-based retriever to incorporate more complete multi-modal information in retrieval queries. Compared to previous approaches that rely on expensive pre-training on multi-modal datasets , FLMR's vision-language alignment process is efficient and can be done in 4 hours with one A-100 GPU.

Figure 1: Overview of RA-VQA-v2. The system consists of two steps: (A) Knowledge Retrieval and (B) Answer Generation. (A.1) A text retriever is used to obtain token-level embeddings of text-based vision (obtained by captioning and object detection) and text documents in the database. (A.2) Visual tokens are obtained from the image and the region-of-interest patches using a vision model and a mapping network. (A.3) Relevance score between the query and the document is computed by aggregating the fine-grained relevance at token level with late interaction mechanism (Eq. 12). (B.1) The answer generator takes the text query, the image, and the retrieved documents as input, generating one candidate answer per retrieved document. (B.2) The answer with the highest joint probability is selected.

Method

In this section, we introduce RA-VQA-v2, which builds upon the original RA-VQA framework [Lin and Byrne, 2022] but is equipped with Fine-grained Late-interaction Multi-modal Retriever (FLMR) to enhance knowledge retrieval. As illustrated in Fig. 1, the framework consists of two stages: Knowledge Retrieval (Sec. 3.1) and Answer Generation (Sec. 3.2).

### Knowledge Retrieval

The FLMR system consists of two encoders: a vision model \(F_{V}\) and a language model \(F_{L}\) that encode image and textual features, respectively.

**Visual Features.** We utilize two types of visual representations: (1) text-based vision representations (textual description of visual elements) obtained by image-to-text transforms and (2) feature-based vision representations obtained by large vision models.

For text-based vision representations, to allow a direct comparison, we follow Lin and Byrne  to extract objects and their attributes using VinVL [Zhang et al., 2021] and generate image captions using Oscar [Li et al., 2020a]. For each image \(I\), we obtain a textual description that contains serialized object names, attributes, and descriptive captions [Lin and Byrne, 2022]. The sequence is appended to the question \(q\) to form the query. For simplicity of notation, the question \(q\) always includes text-based vision unless otherwise specified.

For feature-based vision representations, we use the vision model \(F_{V}\) to extract both global and regional image feature representations. For regional image feature representations, we further use the object detection results of VinVL to locate \(N_{ROI}\) (Region-of-Interest) bounding boxes. To filter bounding box proposals from VinVL, we use the predicted class name associated with each box to select objects explicitly mentioned in the question \(q\), and then prioritize bounding boxes with larger areas. Using the vision model \(F_{V}\), we then obtain one global image representation \(g=F_{V}(I)^{d_{V}}\) from the image \(I\) and ROI-based regional representations \(\{r_{i}=F_{V}(I_{i}^{p})^{d_{V}}\}_{i=1,...,N_{ROI}}\) from the image ROI patches \(\{I_{i}^{p}:i=1,...,N_{ROI}\}\) which contain finer-grained details.

**Token-Level Embeddings.** Compared with DPR's compressed, one-dimensional representation of queries and documents, FLMR preserves richer information by employing token-level, multi-dimensional embeddings to improve retrieval. We obtain token-level embeddings for both textual input and visual input. These are concatenated to form the final embeddings of queries and documents.

To align the vision and text modalities, we train a mapping network \(_{M}\) that learns to project visual features from vision model \(_{V}\) with hidden size \(d_{V}\) into the latent space of the language model \(_{L}\) with hidden size \(d_{L}\). The mapping network, a 2-layer multi-layer perceptron, projects each visual representation into \(N_{vt}\) visual tokens, i.e. \(^{d_{V}}^{N_{vt}d_{L}/2}^{N_{vt}d_{L}}\) and finally reshaped into \(^{N_{vt} d_{L}}\).

Formally, the final query embeddings \(\) are:

\[=[_{L}(q),_{M}([g,r_{1},r_{2},...,r_{N_{ROI} }])]^{l_{Q} d_{L}},\] (1)

where \(l_{Q}=l_{q}+(N_{ROI}+1) N_{vt}\). \(l_{q}\) is the length of the question \(q\). \([v_{1},...,v_{N}]\) denotes the concatenation of \(N\) embeddings \(v_{1}\) to \(v_{N}\).

The documents in the knowledge base are represented by embeddings \(\) obtained from the document content \(d\) of length \(l_{D}\):

\[=_{L}(d)^{l_{D} d_{L}},\] (2)

**Multi-Modal Late Interaction.** We compute the relevance score between a question-image pair \(}=(q,I)\) and a document \(d\) by a late interaction formula similar to that in ColBERT but under the multi-modal context:

\[r(},d)=r((q,I),d)=_{i=1}^{l_{Q}}}{}}}_{i}_{j}^{}\] (3)

For each query token, the MAX operation selects the highest relevance score over all document tokens. In preliminary experiments, other operations (e.g. MEAN or SUM) were found to be overly sensitive to the length of documents, which can be as short as a single sentence. We note that [PAD]may dominate in the final score for short documents, whereas longer documents have an inherent advantage due to their greater number of meaningful tokens.

In contrast to DPR, FLMR allows full interactions between every query embedding vector \(_{i}\) and every document embedding vector \(_{j}\). FLMR retriever also supports retrieving multi-modal documents. We leave the formulation and results to Appendix H.

**Training and Inference.** To train the model, we treat documents \(d^{*}\) that contain the ground-truth answer to question \(q\) as gold (positive) documents. We use in-batch negative sampling for training following Karpukhin et al. (2020). All documents in a training batch other than \(d^{*}\) are considered negative for \(q\), denoted as \((q)\). We train with the contrastive loss \(_{CL}\) over the dataset \(\):

\[_{CL}=-_{(q,d^{*})}) )}{(r(q,d^{*}))}+-14.226378pt_{z(q )}(r(q,z))\] (4)

After training, all documents are indexed using PLAID (Santhanam et al., 2022), which enables fast late-interaction retrieval with a time cost similar to that of DPR.

**Training the Mapping Network for Vision-Language Alignment.** Directly fine-tuning the two models \(_{V}\) and \(_{L}\) on the retrieval task leads to performance degradation at the start of training, as the models are not yet aligned. Inspired by CLIP (Radford et al., 2021), where a language model is trained to align with a vision model, we align \(_{V}\) and \(_{L}\) in the context of knowledge retrieval by pre-training the parameters of the mapping network \(_{M}\) with a retrieval task.

Given ground-truth image-document pairs \(\{(I_{p},d_{p})\}\), which can be Wikipedia images and their accompanying texts, the system is trained to retrieve the document \(d_{p}\) associated with the input image \(I_{p}\). The relevance between the input image \(I\) and a document \(d\) is formulated as

\[=_{M}(F_{V}(I))^{N_{vt} d_{L}};\ \ \ \ = _{L}(d)^{I_{D} d_{L}};\ \ \ \ r(I,d)=_{i=1}^{N_{vt}}_{j=1}^{l_{I_{p}}}_{i} _{j}^{}\] (5)

where only the parameters of the mapping network \(_{M}\) are trained with the contrastive loss in Eq. 4. We provide details of pre-training in Appendix E and discuss its effectiveness in Sec. 5.2.

**Knowledge Retrieval.** We extract top-\(K\) documents from the knowledge base as relevant knowledge. The retrieval probability is defined below following the notation of Lin and Byrne (2022) and Lewis et al. (2020):

\[p_{}(d_{k}|})=},d_{k}))}{ _{j=1}^{K}(r(},d_{j}))}\] (6)

where \(\) denotes the model parameters of \(_{V}\), \(_{L}\), and \(_{M}\).

### Answer Generation

In principle, the knowledge retrieved by FLMR can be used by any VQA answer generator. We denote the answer generator as \(_{A}\) with parameters \(\). Following Lin and Byrne (2022), RA-VQA-v2 generates an answer for each retrieved document and selects the best candidate by the joint probability of retrieval and answer generation:

\[\{d_{k}\}_{k=1}^{K}=_{d}(p_{}(d|})); \ \ \ ,=*{arg\,max}_{y,d_{k}}p(y,d_{k}|})=*{arg\,max}_{y,d_{k}}p_{}(y|},d_ {k})\ p_{}(d_{k}|})\] (7)

The training loss of the answer generator follows that of the underlying model. For example, when using BLIP 2 (Li et al., 2023), we use the cross-entropy loss of the generated sequences:

\[=_{(},)}_{k=1}^{K}  p_{}(s_{k}^{*}|},d_{k})\] (8)

where \(\) is the whole dataset. \(\) is the set of human responses. \(s_{k}^{*}\) is the answer string that appears in the retrieved document \(d_{k}\), or the most popular answer string3 if an exact match cannot be found in the document.

Experiment Setup

**Datasets.** We focus on the OK-VQA dataset where a large portion of questions requires external knowledge (either commonsense or domain-specific) to answer. There are no annotations of 'ground-truth' documents for OK-VQA questions. We follow the literature to use pseudo-relevance labels (a binary indicator of whether a document contains the answer string) as document annotations. We do not evaluate A-OKVQA (Schwenk et al., 2022), a successor of OK-VQA, as it emphasizes visually-grounded reasoning rather than knowledge retrieval. To validate the effectiveness of our proposed approach, we test the retrieval abilities using 2 different corpora, whose statistics can be found in Appendix C:

(1) _Google Search Corpus for OK-VQA_(Luo et al., 2021): a passage corpus collected for answering OK-VQA questions. Previous work has shown that the corpus is effective for OK-VQA (Luo et al., 2021; Lin and Byrne, 2022). We use this corpus in evaluating VQA performance since it covers more knowledge for answering the OK-VQA questions.

(2) _Wikipedia Corpus for OK-VQA_: we collect this corpus by gathering all Wikipedia passages on common objects and concepts (e.g. umbrella, dog, hat) and those containing any of the potential answers in OK-VQA training set. This ensures the corpus covers useful knowledge for answering OK-VQA questions. We note that the collected corpus encompasses a multitude of semantically-diverse documents (>100,000) that challenge the retrieval system to identify actually useful documents. For example, all Wikipedia documents with the word 'party' are included in the corpus, ranging from descriptions of fairy tales to political parties. We also use 10% of the WIT dataset (Srinivasan et al., 2021), a corpus based on Wikipedia with image-text pairs, to train the mapping network for multi-modal alignment.

We evaluate on two additional KB-VQA datasets to demonstrate FLMR's generalizability.

(1) FVQA (Wang et al., 2017): We follow RAVQA (Lin and Byrne, 2022) to preprocess the data. All knowledge triplets are serialized into text sequences to form the knowledge base for retrieval. The average of 5 cross-validation splits is reported.

(2) Infoseek (Chen et al., 2023b): Infoseek is a newly proposed KB-VQA dataset that provides Wikipedia documents that can be used in answering its questions. We follow Chen et al. (2023b) in preprocessing. First, we remove questions whose answers cannot be found in the provided Wikipedia passages. Second, in additional to the documents covered in the dataset (\(\)60,000), we include less relevant passages to form a knowledge base for retrieval (\(\)100,000 documents). The test set annotation has not been released, and so we split the official validation set again into validation and test sets (\(\)5200 questions).

**Training Setup.** We use ColBERTv2 (Santhanam et al., 2022a) and CLIP ViT-base (Radford et al., 2021) to initialize the text-based retriever and vision encoder. For the DPR baseline, we use the official DPR checkpoints to initialize the retriever. In answer generation, we use T5-large (Raffel et al., 2020) and BLIP2-Flan-T5-XL. We use 1 Nvidia A100 (80G) for all experiments. We give detailed training hyperparameters in Appendix E. We use LoRA (Hu et al., 2022b) to fine-tune RA-VQA-v2 (BLIP 2) on 1 single GPU. The vision model is frozen throughout all experiments. During vision-language alignment training, only the mapping network is trainable. In training the answer generator, the retriever is frozen. Our implementations are released at https://github.com/LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering.

**Evaluation.** We present the metrics used to assess the generated answer and the performance of our knowledge retriever below. All reported numbers are averaged from 3 runs with different seeds. We verified the significance of all mentioned improvements with scipy.stats.ttest_ind (\(p<0.05\)).

(1) _VQA Score_: To evaluate VQA performance, we use the official VQA Score (Marino et al., 2019) which assigns score to the generated answer based on its exact occurrence count in the set of human responses \(\):

\[(y,)=}(y)}{3},1 ,\] (9)

where \(\#_{}(y)\) is the occurrence of \(y\) in human responses \(S\). This score ensures that a model is partially rewarded even if it generates a less popular answer among the human responses (Luo et al., 2021).

(2) _Exact Match (EM)_ awards point if any of the annotated answers is generated exactly: \((y,)=(\#_{}(y),1)\).

(3) _Pseudo Relevance Recall (PRRecall@K)_: To evaluate the retriever, we adopt pseudo relevance following Luo et al. (2021) due to the absence of ground-truth knowledge documents for each query. A document is considered pseudo-relevant if it contains any human-annotated answers. PRRecall@K measures whether the retrieved \(K\) documents contain at least one pseudo-relevant document: \(=_{k=1}^{K}H(d_{k},),1\), where \(H(d_{k},)\) evaluates to 1 if the retrieved document \(d_{k}\) contains any answer in \(\), and 0 otherwise. The metric is averaged across the test set.

**Baselines.** To demonstrate the effectiveness of **FLMR**, we take a **DPR** retriever as a baseline. In later sections, FLMR _w/o Late Interaction_ refers to the corresponding DPR baseline. We use the same training data and hyper-parameters to build a multi-modal retriever based on DPR. For fair comparison, we keep the product \(N_{vt} d_{L}\) identical for DPR and FLMR so that they have the same number of parameters in the mapping networks. Since DPR can only handle one-dimensional query and document embeddings, we sum the embeddings of the [CLS] token from \(_{L}()\) and the visual tokens from \(_{M}(_{V}())\) to reduce the dimension to \(1 d_{L}\) (details in Appendix D).

We also compare our VQA performance with the latest KB-VQA models. Amongst these models, ConceptBERT (Gardenes et al., 2020), KRISP (Marino et al., 2021), VRR (Luo et al., 2021), MAVEx (Wu et al., 2022), KAT-T5 (Gui et al., 2021), TRIG-Ensemble (Zuo et al., 2022), and RA-VQA are relatively small in model size (<1B), whereas PICA (Yang et al., 2022), KAT (Gui et al., 2021), Prophet (Shao et al., 2023), PromptCap (Hu et al., 2022), REVIVE (Lin et al., 2022), PALI (Chen et al., 2022), Flamingo (Alayrac et al., 2022), PaLM-E (Driess et al., 2023) use very large models such as GPT-3 (175B) and PaLM-E (562B).

## 5 Results and Key Findings

### VQA Performance

As shown in Table 1, recent models leveraged LLM or Large Multi-modal Models (LMMs) to achieve excellent performance on OK-VQA. The best performance to date is by PaLM-E, achieving a VQA score of 66.1 with 562 billion pre-training parameters. The original RA-VQA formalism achieves a lower VQA Score of 54.48 but with only 800 million parameters.

We first compare RA-VQA-v2 (with FLMR retrieval) with RA-VQA (with DPR retrieval). Our replication of RA-VQA (T5-Large) (Table 1, Row 22) achieves similar PRRecall@5 and VQA Score as the published results of RA-VQA (Table 1, Row 8). Compared with RA-VQA (T5-large), RA-VQA-v2 (T5-large) improves the PRRecall@5 significantly from 83.08% to 89.32%, leading to a gain of 3.4 in VQA Score (51.45 to 54.85). This suggests that improvement in knowledge retrieval benefits answer generation via retrieval augmentation.

We also show the effectiveness of knowledge augmentation by comparing the underlying base models with their retrieval-augmented version. As shown, T5-large and BLIP 2 (fine-tuned with OK-VQA data) achieve 47.52 and 55.44 VQA Scores, respectively. Their retrieval-augmented version, RA-VQA-v2 (T5-large) and RA-VQA-v2 (BLIP 2) gain 7.33 and 6.64 in VQA Score, respectively. For readers' interest, we provide more thorough analyses on the performance that the underlying answer generator model attains and the gain brought by knowledge retrieval in Appendix I.

To confirm that text-based vision can aid LMMs such as BLIP 2 which already has its own image encoder in VQA tasks, we remove text-based vision from RA-VQA-v2 (BLIP 2) and BLIP 2 (fine-tuned). This results in a decrease in VQA performance from 62.03 to 60.37 and 55.44 to 54.10, respectively (Table 3), suggesting that text-based vision contains useful information not included in the visual features obtained by BLIP 2's own image encoders.

RA-VQA-v2 achieves comparable and even better performance when compared with systems that use very large (\(\)13B parameters) LLMs and LMMs. With BLIP 2 (\(\)3B), RA-VQA-v2 outperforms Flamingo (80B) by 4.19 VQA Score. It also outperforms many recent systems that use GPT-3 (175B) as an answer generator or knowledge source, such as PromptCap, REVIVE, and KAT. It achieves similar performance to that of PALI (17B) (62.03 vs 64.5 VQA Score). With comparable parameter sizes, RA-VQA-v2 (BLIP 2, 3B) outperforms PALI (3B) by a large absolute margin (62.03 vs 52.40 VQA Score). We emphasize that RA-VQA-v2 can be used in conjunction with virtually any existing LLMs and LMMs to offer substantial improvements, as demonstrated by the T5-large and BLIP 2 experiments.

### Retrieval Performance

**Text-based/Feature-based Vision.** As shown in Table 2, previous retrievers (RA-VQA, VRR) achieve \(\)82.84% PRRecall@5 using only text-based vision (textual descriptions of visual scenes). We show that visual features obtained via aligned vision models (feature-based vision) are equally effective as text-based vision. Relying on questions only, FLMR has a baseline retrieval score of 74.81 PRRecall@5. Incorporating text-based vision and feature-based vision increase PRRecall@5 to 85.99 and 85.08, respectively. Furthermore, feature-based vision provides information complementary to test-based vision, as demonstrated by the better PRRecall@5 at 87.02 when the two are combined. The same trend is observed for DPR-based retrieval system, though less dramatically (from 83.08 to 83.43). We note that pre-training the mapping network for vision-language alignment is crucial for good performance. Without such pre-training, performance degrades to 85.71. These results confirm that incorporating aligned vision encoders in the retrieval process compensates for the information loss in image-to-text transforms.

**Effects of Late Interaction and ROIs.** Late Interaction enables FLMR to capture fine-grained relevance of token-level embeddings. As shown in Table 2, using the same query and document representations, upgrading DPR to FLMR leads to consistent improvement in retrieval performance by a large margin up to \(\)6% (comparing Table 2 Row 8 & 13).

   \# & Model & Base Models & K & Knowl. Src. & R@5 & EM & VQA \\ 
1 & ConceptBERT & & & C & & & 33.66 \\
2 & KRISP & & & C + W & & & 38.35 \\
3 & VRR & & 100 & GS & & & 45.08 \\
4 & MAVEx & & & W + C + GI & & & 39.40 \\
5 & KAT-T5 & T5-large & 40 & W & & & 44.25 \\
6 & TRiG-Ensemble & T5-large & 100 & W & & & 54.73 & 50.50 \\
7 & RA-VQA (joint training) & T5-large & 50 & GS & 82.84 & 59.41 & 54.48 \\
8 & RA-VQA & T5-large & 5 & GS & 81.25 & 55.77 & 51.22 \\   \\ 
9 & PICa & GPT-3 & & GPT-3 & & 48.00 \\
10 & KAT-Ensemble & T5-large, GPT-3 & 40 & W + GPT-3 & & 54.41 \\
11 & Prophet & GPT-3 & & GPT-3 & & 61.11 \\
12 & PromptCap & GPT-3 & & GPT-3 & & 60.40 \\
13 & REVIVE & GPT-3 & & W + GPT-3 & & 58.00 \\
14 & PALI & PALI (3B) & PALI & & 52.40 \\
15 & PALI & PALI (15B) & PALI & & & 56.50 \\
16 & PALI & PALI (17B) & PALI & & & 64.50 \\
17 & Flamingo & Flamingo (80B) & Flamingo & & 57.80 \\
18 & PALM-E & PaLM-E (562B) & PaLM-E & & 66.10 \\   \\ 
19 & T5-large (fine-tuned) _w/o knowledge_ & T5-large & & & 51.38 & 47.52 \\
20 & BLIP 2 (fine-tuned) _w/o knowledge_ & BLIP 2 (T5-XL) & & & 59.49 & 55.44 \\   \\ 
21 & RA-VQA-v2 (T5-large) & T5-large & 5 & GS & 89.32 & 58.85 & 54.85 \\
22 & _w/o ROI \& VE \& Late-interaction_ & T5-large & 5 & GS & 83.08 & 55.89 & 51.45 \\
23 & RA-VQA-v2 (BLIP 2) & BLIP 2 (T5-XL) & 5 & GS & **89.32** & **62.01** & **62.08** \\
24 & _w/o ROI_ & BLIP 2 (T5-XL) & 5 & GS & 87.02 & 61.63 & 60.75 \\
25 & _w/o ROI \& VE_ & BLIP 2 (T5-XL) & 5 & GS & 85.99 & 59.95 & 60.41 \\
26 & _w/o Late-interaction_ & BLIP 2 (T5-XL) & 5 & GS & 82.90 & 59.00 & 58.20 \\
27 & _w/o ROI \& Late-interaction_ & BLIP 2 (T5-XL) & 5 & GS & 83.43 & 60.18 & 59.21 \\
28 & _w/o ROI \& VE \& Late-interaction_ & BLIP 2 (T5-XL) & 5 & GS & 83.08 & 59.49 & 58.70 \\   

Table 1: Model Performance on OK-VQA. Knowledge Source abbreviations: C: ConceptNet; W: Wikipidia; GS: GoogleSearch; GI: Google Images. EM stands for Exact Match. VQA stands for VQA Score. R stands for PRRecall. The best performance in literature is underlined.

In addition to token-level relevance, FLMR can utilize fine-grained Region-of-Interest (ROI) features with Late Interaction whereas DPR can not. This can be demonstrated by Fig. 2: as the number of ROIs increases, DPR performance degrades. This may be because DPR's one-dimensional query and document embeddings are not expressive enough to encompass fine-grained details of the ROI visual cues. As shown in Table 2 and Table 1 Row 27-28, adding more ROIs effectively adds noise which adversely impacts the retrieval performance (83.43 to 82.9), and in turn worsen VQA scores (59.2 to 58.2).

**Object-centric ROIs improve retrieval.** We also conduct ablation studies to show that the performance improvements brought by ROIs come from the finer-grained information captured by them rather than from increases in the number of features. We compare FLMR with 4 object-centric ROIs (obtained by object detection) against 2 baseline ROI selection methods: (1) randomly crop 4 patches of size larger than 100 \(\) 100 from the image as ROIs; (2) evenly split the image to obtain the top-left, top-right, bottom-left, and bottom-right of the image as ROIs. As shown in Table 4, FLMR with 4 ROIs from VinVL object detection outperforms others, suggesting that it is the object-centric, fine-grained ROIs that improve the performance.

**Retrieval performance on FVQA and Infoseek.** As shown in Table 5, we observed similar improvements with FLMR. FLMR with both text- and feature-based vision improves DPR by 2.3%

    & R@5 & R@10 \\ 
4 Object-centric ROIs & 88.01 & 93.62 \\
4 Random ROIs & 86.9 & 93.20 \\
4 Evenly-split ROIs & 86.96 & 93.16 \\   

Table 4: Comparison of ROI selection methods. R stands for PRRecall.

   \# &  &  &  &  &  \\  & & Vision &  &  &  &  &  \\ 
1 & VRR & ✓ & - & 80.4 & 88.55 & & \\
2 & RA-VQA-FrDPR & ✓ & - & 81.25 & 88.51 & & \\
3 & RA-VQA & ✓ & - & 82.84 & 89.00 & & \\ 
4 & DPR & - & - & 73.11 & 82.05 & 57.03 & 69.84 \\
5 & DPR & ✓ & - & 83.08 & 89.77 & 66.04 & 75.94 \\
6 & DPR & - & ✓ & 80.52 & 88.27 & 65.84 & 75.85 \\
7 & DPR & ✓ & ✓ & 83.43 & 90.31 & 66.88 & 76.35 \\
8 & DPR & ✓ & ✓+9ROIs & 82.90 & 89.95 & 65.86 & 75.90 \\ 
9 & FLMR & - & - & 74.81 & 83.10 & 57.20 & 70.11 \\
10 & FLMR & ✓ & - & 85.99 & 92.79 & 66.50 & 76.80 \\
11 & FLMR & - & ✓ & 85.08 & 91.80 & 66.90 & 77.05 \\
12 & FLMR & ✓ & ✓ & 87.02 & 92.69 & 67.50 & 77.60 \\
13 & FLMR & ✓ & ✓+9ROIs & **89.32** & **94.02** & **68.10** & **78.01** \\
14 & _w/o alignment pre-training_ & ✓ & ✓+9ROIs & 85.71 & 92.41 & 66.40 & 76.10 \\   

Table 2: Retrieval performance on Google Search (GS) and Wikipedia. Text-based vision refers to textual descriptions of images (such as OCR, caption, objects and attributes). Feature-based vision is obtained using a neural vision model directly (e.g. ViT). R@K refers to PRRecall@K.

   Model & VQA Score \\  RA-VQA-v2 (BLIP 2) & 62.03 \\ _w/o text-based vision_ & 60.37 \\  BLIP 2 (fine-tuned) _w/o knowledge_ & 55.44 \\ _w/o text-based vision_ & 54.10 \\   

Table 3: Removing text-based vision from answer generation reduces the VQA performance, showing that text-based vision offers more complete image understanding.

Figure 2: PRRecall@5 versus the number of ROIs. Finer-grained ROIs cause performance degradation in DPR, while FLMR captures them to improve retrieval performance.

and 1.54% Recall@5 on FVQA and Infoseek, respectively. Incorporating ROI features further improves its performance to 72.37 on FVQA and 47.08 on Infoseek. This suggests that FLMR is generalizable to other KB-VQA retrieval tasks and can bring steady improvements relative to baselines.

**Qualitative analysis of FLMR retrieval**. Figure 3 shows FLMR retrieval in action. The orange lines connect the query token and the document token with the highest relevance score, which will be preserved after the MaxSim operation and will contribute to the final retrieval score. We can see that token-level interaction indeed captures fine-grained relevance between the query and the document. For example, the retriever recognizes that the numbers "26" and "30" in the document are highly relevant to "How many" in the query. We can also see that the image tokens are aligned with the text tokens: the image tokens corresponding to the cat (IMG, ROI3 and ROI4) point to the words "cats" and "cat" in the document. This demonstrates the effectiveness of vision-language alignment that gives rise to explainable cross-modality relevance. We provide more case studies in Appendix J.

## 6 Conclusion

In this paper, we proposed Fine-grained Late-interaction Multi-modal Retrieval (FLMR), the first of its kind to leverage fine-grained token-level relevance between queries and documents for VQA tasks. FLMR incorporates feature-based vision using an aligned vision model that complements text-based vision to enhance image understanding, improve retrieval performance and advance VQA performance. We achieve superior performance in OK-VQA, greatly surpassing previous systems with similar parameter size and closes the gap with those systems utilizing very large (>\(13\)B) models.

Figure 3: Selected query tokens connected by document tokens that have the highest token-level relevance with them, as computed by FLMR. For example, amongst all document tokens, ‘26’ and ‘30’ have the highest relevance with the query token ‘how’ and ‘many’, respectively. This shows that FLMR can capture fine-grained document relevance. Zoom in for better visualization.

    & FVQA Recall@5(Std.) & Infoseek Recall@5 \\  DPR & 68.58(0.01) & 44.88 \\ FLMR (Visual Encoder) & 70.88(0.01) & 46.42 \\ FLMR (Visual Encoder \& 10 ROIs) & **72.37**(0.01) & **47.08** \\   

Table 5: Retrieval performance on FVQA  and Infoseek . Average recall on 5 splits is reported for FVQA. FLMR outperforms DPR trained with the same data with a clear margin.

## 7 Acknowledgement

Weizhe Lin was supported by a Research Studentship funded by Toyota Motor Europe (RG92562(24020)). We thank our colleagues, Daniel Olmeda Reino (Toyota Motor Europe) and Jonas Ambeck (Toyota Motor Europe), who provided insight and expertise in this project.

Prof. Bill Byrne holds concurrent appointments as a Professor of Information Engineering at Cambridge University and as an Amazon Scholar. This publication describes work performed at Cambridge University and is not associated with Amazon.

We would also like to thank all the reviewers for their knowledgeable reviews.