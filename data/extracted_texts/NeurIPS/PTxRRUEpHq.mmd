# Gradient Methods for Online DR-Submodular Maximization with Stochastic Long-Term Constraints

Guanyu Nie

Iowa State University

Ames, IA 50010

nieg@iastate.edu &Vaneet Aggarwal

Purdue University

West Lafayette, IN 47907

vaneet@purdue.edu &Christopher John Quinn

Iowa State University

Ames, IA 50010

cjquinn@iastate.edu

###### Abstract

In this paper, we consider the problem of online monotone DR-submodular maximization subject to long-term stochastic constraints. Specifically, at each round \(t[T]\), after committing an action \(_{t}\), a random reward \(f_{t}(_{t})\) and an unbiased gradient estimate of the point \(f_{t}(_{t})\) (semi-bandit feedback) are revealed. Meanwhile, a budget of \(g_{t}(_{t})\), which is linear and stochastic, is consumed of its total allotted budget \(B_{T}\). We propose a gradient ascent based algorithm that achieves \(\)-regret of \(()\) with \((T^{3/4})\) constraint violation with high probability. Moreover, when first-order full-information feedback is available, we propose an algorithm that achieves \((1-1/e)\)-regret of \(()\) with \((T^{3/4})\) constraint violation. These algorithms significantly improve over the state-of-the-art in terms of query complexity.

## 1 Introduction

_Online optimization_ confronts diverse challenges as information gradually unfolds, compelling irreversible decisions at each step amidst uncertainty about future information. Typically, an online optimization problem can be framed as a recurring game between a learner or algorithm and an adversary or environment: during each iteration, the learner chooses an action from a predefined domain set, and the environment subsequently reveals feedback in the form of utility or reward for the selected action. Over time, the learner aims to learn from past experiences and improve decision-making strategies to maximize cumulative rewards.

In instances where the objective function is concave and the feasible set is convex, the problem has been extensively explored in the literature under the name of _online convex optimization_ (OCO) . OCO has found considerable success in various machine learning applications, leveraging the well-established theories of convex optimization. Within OCO, it is established that any algorithm incurs a regret of \(()\) in the worst case . Notably, there exist algorithms that match this lower bound, such as Online Gradient Descent (OGD) .

Even though optimizing convex/concave functions can be done efficiently, most problems in artificial intelligence are non-convex. Examples include training deep neural networks, Bayesian inference, and clustering, among many others. One important example of such functions is called _submodular set function_. Submodular set functions exhibit a natural property known as _diminishing returns_, akin to concave functions in continuous domains. They have been applied in various machine learning contexts, including viral marketing , sensor placement , recommendation systems , data summarization , and numerous others. However, submodularity extends beyond set functions and can also be defined for continuous functions. DR-submodularity represents a special class of such continuous functions .

In the realm of online DR-submodular maximization, two primary types of algorithms have received extensive attention. Gradient ascent algorithms are often favored for their simplicity and lower gradient sample complexity, while Frank-Wolfe algorithms offer the ability to avoid potentially costly projection operations . The type of feedback also plays a pivotal role in algorithm design. Frank-Wolfe algorithms typically assume more information is acquired each round, assuming that upon committing an action, the agent can observe the entire function, and gradients of multiple points can be observed. This feedback type is often referred to as _full information feedback_. On the other hand, gradient ascent-based algorithms usually assume only the function value of the chosen action and the gradient at that point can be observed. This feedback type is often referred to as _semi-bandit feedback_. Recent efforts have aimed to enhance gradient queries for Frank-Wolfe algorithms, albeit at the cost of worse regret .

In numerous applications, aside from the goal of maximizing the cumulative reward, there exist constraints on the sequence of decisions made by the learner that must be satisfied on average . In such scenarios, long-term constraints pose challenges because the decision-making process cannot be simply expressed as restricting the set of actions for all time steps may be sub-optimal. Instead, the learner still (always) wants to maximize cumulative reward, but if putatively instantaneously high reward arms are expensive, can only play them a few times. To illustrate, let's consider the online ad allocation problem encountered by an advertiser. At each round \(t[T]\), the advertiser faces the task of determining the allocation of funds across \(n\) different websites for placing ads. While the primary aim is to maximize the overall click-through rates of the ads, the advertiser is also constrained by a predetermined budget allocated over a specified time horizon . In this scenario, the cost of ad placement in each round may either be fixed and known in advance, or it may depend on the number of clicks the ads receive, which remains uncertain in advance. As a result, the advertiser must navigate the trade-off between optimizing the total reward and adhering to budget constraints.

In this paper, we study the problem of online DR-submodular maximization with long-term (linear) budget constraints. While most prior works assume the objective function is chosen adversarially , we study the stochastic DR-submodular maximization setting, where the objective functions are i.i.d. sampled from a distribution . Note that this setting is still of interest because as opposed to assuming each arriving function \(f_{t}\) being DR-submodular, we only assume the expectation of \(f_{t}\) to possess DR-submodularity. Moreover, we consider the semi-bandit feedback setting where only the noisy function value \(f_{t}(_{t})\) and an unbiased gradient estimator of that point \(f_{t}(_{t})\) can be observed. To the best of our knowledge, these particular settings (stochastic utility and semi-bandit feedback) have not been explored in the literature on DR-submodular maximization with long-term constraints.

**Our Contributions:** We summarize our contributions in three parts.

1. In the semi-bandit feedback setting, we propose the first stochastic gradient ascent based algorithm for stochastic online DR-submodular maximization with stochastic long-term constraints. Our proposed algorithm can achieve \(()\)\(\)-regret and \((T^{3/4})\) constraint violation with high probability. Compared to all previous works , they consider first-order full-information feedback and require unbiased gradient estimates at \(\) locations (not just at action \(_{t}\)) in every round. For those works, their query complexity in each round is \(\) while ours is just 1.
2. In first order full-information setting, where the unbiased gradient estimates of any point can be observed, we propose the first stochastic gradient ascent based algorithm for stochastic online DR-submodular maximization with stochastic long-term constraints. We utilize the recently developed technique in  called the non-oblivious function. Our proposed algorithm can achieve \(()\)\((1-1/e)\)-regret and \((T^{3/4})\) constraint violation with high probability. Again, compared to previous works , our query complexity is significantly lower.

**Regarding the approximation ratios:** We note that in offline \(1-1/e\) is known to be the optimal approximation ratio for optimizing monotone DR submodular functions over a general convex set, where the query can be anywhere in the convex hull of \(\{\}\) (\(\) is the constraint set). However, when the oracle calls are restricted to \(\), an approximation ratio of \(1/2\) is the best that is known to be achievable . Thus, full information feedback can achieve \(1-1/e\)-regret while the semi-bandit feedback achieves \(1/2\)-regret.

## 2 Related Works

The primary related works are summarized in Table 1. We briefly discuss notable contributions and for additional related works, see Appendix F

**Online DR-submodular Maximization with Long-Term Constraints** We do not compare results with adversarial constraints due to additional assumptions needed in this setting. See Appendix F for a detailed discussion. In the context of stochastic constraints, Raut et al.  conducted the initial study of the problem. They successfully attained \(()\) regret and constraint violation with high probability, as well as \((T^{3/4})\) regret and \(()\) constraint violation in expectation. Building upon this work, Sadeghi et al.  further improved the results to achieve \(()\) regret and constraint violation, both in expectation and with high probability. Additionally, Feng et al.  extended these findings to incorporate weakly DR-submodular utility, achieving analogous results.

**Online Convex Optimization with Long-Term Constraints** Several results for OCO with deterministic long-term constraints can be found in [14; 20; 37; 38; 40]. Existing literature has established that a regret of \(()\) and a cumulative constraint violation of \((T^{1/4})\) can be achieved without the Slater condition. Conversely, assuming the Slater condition allows for achieving a regret of \(()\) and a cumulative constraint violation of \((1)\). In cases where the considered constraint is assumed to be stochastic, Yu et al.  achieved a \(()\) bound on both regret and constraint violations under the Slater condition. Furthermore, Wei et al.  achieved the same regret and constraint violation bounds while assuming a strictly weaker assumption than the Slater condition.

## 3 Preliminaries

### Notations

Vectors are shown by lowercase bold letters, such as \(^{d}\). We denote by \(\|\|\) the \(_{2}\) (Euclidean) norm. We use \([T]\) to denote the set \(\{1,2,,T\}\). The inner product of two vectors \(,^{d}\) is denoted by either \(,\) or \(^{}\). For \(u\), we define \([u]_{+}:=\{u,0\}\). For two vectors \(,^{d},\) implies that \(x_{i} y_{i}, i[d]\). For a convex set \(\), we denote the projection of \(\) onto set \(\) as \(_{}()=_{}\|- \|\).

### Function Properties

Here we list some function properties that will appear in our assumptions.

**Monotonicity** A function \(f\) is monotone if \(f() f()\) for all \(\).

   Reference & Region & Noise & \# Grad. & Approx. & Regret & Con. Viol. \\ 
 & \(\) & \(\) & \(\) & \(1-1/e\) & \(()\) & \(}()\) \\
\(\) & \(\) & \(\) & \(2\) & \(1-1/e\) & \(()\) & \(()\) \\
 & \(\) & \(\) & \(\) & \(1-1/e\) & \(()\) & \(()\) \\ 
 & \(\) & \(\) & \(\) & \(1-1/e\) & \((T^{3/4})\) & \(}()\) \\ Theorem 2 & General\(\) & \(\) & 1 & \(1-1/e\) & \(()\) & \((T^{3/4})\) \\  Theorem 1 & General & \(\) & 1(semi) & \(1/2\) & \(()\) & \((T^{3/4})\) \\   

Table 1: We include related works from online DR-submodular optimization with constant or stochastic long term constraint functions. (Works handling adversarial long-term constraints require a different definition of regret.) All methods require a gradient oracle for feedback, and ‘Noise’ lists whether the gradient is exact or there is stochastic noise. ‘# Grad.’ is the number gradient evaluations required per-round. ‘Con. Viol.’ is the bound on the constraint violation. \(\) considered constraint set being convex while all other works consider linear constraint. In ‘# Grad.’ column, \(2\) means this work needs \(\) gradients on both \(f\) and \(g\). \(\) While all actions will be feasible, some gradient queries will be in the convex hull of \(\{\}\).

**Lipschitz continuous** A function \(f\) is Lipschitz continuous with parameter \(\) if for any \(,\), we have \(f()-f()\|-\|\).

### DR-Submodular Functions

A function \(f:2^{}_{+}\), defined on the ground set \(\), is said to be submodular if

\[f(A)+f(B) f(A B)+f(A B)\]

for all \(A,B\). The notion of submodularity has been extended to continuous domains . Consider a function \(f:_{+}\) where the domain is of the form \(=_{i=1}^{n}_{i}\) and each \(_{i}\) is a compact subset of \(_{+}\). We say that \(f\) is continuous submodular if \(f\) is continuous and for all \(,\), we have

\[f()+f() f()+f( )\]

where \(\) and \(\) are component-wise maximum and minimum, respectively. For efficient maximization, we also require that these functions satisfy a diminishing returns condition . We say a differentiable function \(f\) is continuous DR-submodular if \(f\) it satisfies

\[ f() f()\]

for all \(\). When the function \(f\) is twice differentiable, DR-submodularity is equivalent to

\[f()}{ x_{i} x_{j}} 0, i,j,.\]

The following lemma is an important property of monotone DR-submodular functions.

**Lemma 1**.: _Let \(f:_{+}\) be a monotone, differentiable, DR-submodular function. For any two vectors \(,\), we have_

\[f()-2f() f(),- .\]

The proof can be found in the proof of Theorem 4.2 in . Note that this property shares an analogy with a pivotal characteristic that defines concavity: \(f()-f() f(),- \). Lemma 1 also implies that if we use gradient descent directly on \(f\), we can only achieve an approximation ratio of \(1/2\). Zhang et al.  introduced the so-called non-oblivious function to obtain the optimal approximation ratio of \(1-1/e\):

**Lemma 2**.: _Let \(f:_{+}\) be a monotone, differentiable, DR-submodular function, and let \(F\) be the non-oblivious function of \(f\) defined by its gradient \( F()=_{0}^{1}e^{z-1} f(z)dz\). Then for any vectors \(,\), we have_

\[(1-e^{-1})f()-f() F(),-.\]

The proof of Lemma 2 can be found in .

## 4 Problem Statement

Consider the following offline optimization problem:

\[_{}&f()\\ &g() 0,\] (1)

where \(g()=,-b\) for some non-negative constant \(b\). We study an analogous online setup as follows: At each round \(t[T]\), the algorithm chooses an action \(_{t}\), where \(_{+}^{d}\) is a fixed, known set. We consider both the utility and the constraints being stochastic, where we assume at each time step, the utility function \(f_{t}\) is sampled i.i.d. from a distribution \(_{f}\) with mean \(f\), i.e., \(_{f_{t}_{f}}[f_{t}()]=f()\), while the cost vector \(_{t}\) is i.i.d. sampled from another distribution \(_{p}\). After an action is selected by the learner, a random reward \(f_{t}(_{t})\) is obtained while using \(_{t},_{t}\) of its fixed total allotted budget \(B_{T}\), and \(_{t}\) is observed. In the semi-bandit setting, an unbiased gradient estimator for that action, \(f_{t}(_{t})\), is also revealed. In the first order full-information setting, the unbiased gradient estimator of any point can be observed. In this paper, we consider both settings while all other works in the literature on DR-submodular maximization with long-term constraints, such as , consider full-information feedback.

To make sure the long-term constraint is not vacuous, we consider \(B_{T}=bT\) for a constant \(b\) such that \(_{}, b<_{ },\). In this case, there will always be a solution \(\) that satisfies constraint (having zero constraint violation) and it is not the case that any sequence of actions (especially the most expensive w.r.t. \(\)) is feasible.

We make the following assumptions to proceed our analysis:

**Assumption 1**.: The constraint set \(\) is convex and compact, with diameter \(d=_{,}\|-\|\) and radius \(r=_{}\|\|\). Since \(\) is compact, we denote its diameter as \(=_{,}\|-\|\) and radius as \(=_{}\|\|\), respectively.

**Assumption 2**.: The expected utility function \(f()\) is monotone DR-submodular and \(_{f}\)-Lipschitz.

**Assumption 3**.: The distribution \(_{p}\) for the cost vectors has bounded support \(_{p}_{+}^{d}\) with mean \(\), where \(\) is the unit ball of Euclidean norm.

**Assumption 4**.: The gradient oracle is unbiased \([ f()-f_{t}()|]=0\) and has a bounded variance \([\| f()-f_{t}()\|^{2}| ]^{2}\). In the semi-banddit setting, we assume \(G=_{t}_{}\|f_{t}()\|\) is finite. In the first order full-information setting, denoting the unbiased estimator of the non-oblivious function obtained at round \(t\) by \(F_{t}()\), we assume \(G_{F}=_{t}_{}\|F_{t}()\|\) is finite.

Unlike Frank-Wolfe type algorithms in other papers , we do not assume bounded smoothness on the gradients: \( f()- f()\|-\|\). Moreover, we do not assume \(f()=0\).

Our overall goal is to maximize the total obtained reward while satisfying the budget constraint asymptotically (i.e., \(_{t=1}^{T},_{t}-B_{T}\) being sub-linear in \(T\)).

Note that our proposed algorithm can handle multiple linear constraints as well, and similar regret and constraint violation bounds can be derived. In the case of there are \(m\) constraints \(g_{i}()\), \(i[m]\), we can define \(g():=_{i[m]}g_{i}()\) and it can be shown that \(g\) preserves the same properties as those of individual \(g_{i}\)'s (sub-differentiability, bounded (sub-)gradients and bounded values; see Proposition 6 in  for proofs).

For simplicity of presentation, we denote \(=\{_{f},_{p}\}\). Since \(\) is compact, from monotonicity of \(f\) we have \(F_{1}:=_{}|f()|\) is bounded. Since \(f\) is \(_{f}\)-Lipschitz, we have \(F_{2}:=_{,}|f()-f()| _{f}D\) is bounded. Since \(\) is compact and \(_{p}\) has bounded support, we have \(C:=_{^{}_{p}}_{}| ^{},-}{T}|\) is bounded.

To measure the effectiveness of our proposed algorithm, we use the notions of _regret_ and _total constraint violation_ to quantify the overall utility and the total resource consumption, respectively.

**Regret** is typically defined as the difference between the total reward accumulated by the algorithm and the best fixed action in hindsight. Note that even in the offline setting, maximizing a monotone DR-submodular function subject to a convex constraint can only be done approximately in polynomial time unless RP = NP . Thus, we instead use the notion of \(\)_-regret_ of an algorithm.

**Definition 1**.: The \(\)-regret of an online algorithm with outputs \(\{_{t}\}_{t=1}^{T}\) is defined as

\[R_{T}:=_{^{*}}_{t=1}^{T}f_{t}( )-_{t=1}^{T}f_{t}(_{t}),\] (2)

where \(^{*}\) is the restricted search space of solutions that satisfy long-term constraints for \(T\) steps, (i.e., can be played \(T\) times), \(^{*}=\{:_{t=1}^{T}g() 0\}\), which is also equivalent as satisfying per-round constraint: \(^{*}=\{:g() 0\}\).

Since we are mainly interested in stochastic utility functions, i.e., \(f_{t}_{f}\), we aim to minimize the expected \(\)-regret:

\[[R_{T}]= T_{^{*}}f()- _{t=1}^{T}f(_{t}).\]

Denote \(^{*}=*{arg\,max}_{^{*}}f( )\). Note that since \(_{t}\) is drawn i.i.d. from the distribution \(_{p}\) with mean \(\)\( t[T]\), the best benchmark action is with respect to the "true" underlying \(\) of the constraint function as opposed to \(_{t}\). It is possible that the best-fixed action has a constraint violation with some noisy \(_{t}\)'s.

We next define the total constraint violation.

**Definition 2**.: The _total constraint violation_ of an online algorithm with outputs \(\{_{t}\}_{t=1}^{T}\) is defined as

\[C_{T}:=_{t=1}^{T}g(_{t})=_{t=1}^{T},_ {t}-B_{T}.\]

Again, in the stochastic constraint setting, the total constraint violation is defined with respect to the mean \(\).

## 5 An Efficient Primal-Dual Algorithm under Semi-bandit Feedback

In this section, we introduce our first proposed algorithm for online DR-submodular maximization subject to stochastic long term constraints under semi-bandit feedback: Online Lagrangian Stochastic Gradient Ascent (OLSGA). The algorithm is presented in Algorithm 1. The overall structure of the algorithm is inspired by the primal-dual update in Online Convex Optimization (OCO) (e.g., ).

Associating a dual variable \([0,+)\) with the constraint, the saddle point formulation of (1) can be written as

\[_{x}_{[0,+)}f()- g( ).\]

In this work, we consider the following regularized Lagrangian function \((,)\) given by

\[(,):=f()- g()+^{2}.\] (3)

It is important to observe that the expression in (3) deviates from the conventional Lagrangian due to the inclusion of the term \(^{2}\), where both \(\) and \(\) are parameters that will be later chosen to optimize theoretical guarantees. The main purpose of this modification is to control the value of \(\) and prevent it from growing too large. Although we can achieve the same goal by restricting \(\) to a bounded domain, using the quadratic regularizer makes it convenient for our analysis.

One issue is that \(\) (shown in \(g()\)) is unknown to the online algorithm. Therefore, we alternatively use an empirical estimate \(}_{t}=_{s=1}^{t}_{s}\) instead of \(\) in the Lagrangian function. Moreover, in order to achieve the high probability bound, we adjust our Lagrangian function in (3) as follows:

\[_{t}(,)=f_{t}()-_{t }()+^{2},\] (4)

where \(_{t}()=}_{t},- }{T}-_{t}\) and \(_{t}=()}{t}}\). For the purpose of analysis, we further define \(_{t}():=}_{t},- }{T}\).

For the purpose of analysis, we do not directly use Equation (4) in our primal update. Let \(}_{t}\) be defined by its gradient, \(_{x}}_{t}(_{t},_{t})= f_{t}(_{t})-2_{t}_{t}( _{t})\). The primal updates are formulated as follows:

\[_{t+1}=_{}(_{t}+_{x} }_{t}(_{t},_{t})).\]

Note that compared to Equation (4), the Lagrangian function used for updating has a coefficient of 2 in front of the second term.

Our proposed algorithm is shown in Algorithm 1. The algorithm proceed as follows: it takes a convex constraint set \(\) and a time horizon \(T\) as inputs. Initially, the algorithm selects an initial point \(_{1}\) and sets \(_{1}=0\). At each time step \(t[T]\), the algorithm takes an action \(_{t}\), acquires a reward \(f_{t}(_{t})\), and observes the cost vector \(_{t}\) as well as an unbiased gradient estimate \(f_{t}(_{t})\). Subsequently, unbiased gradient estimates of the updating Lagrangian function with respect to \(\) and to \(\) are computed using the empirical estimate of \(\). Using these calculated gradients, updates to \(\) and \(\) are made using (7) and (8), respectively.

**Remark 1**.: A notable difference between our algorithm and all prior works addressing online DR-submodular maximization with long-term constraints (e.g., ) is that our algorithm can handle search spaces that do not necessarily include \(\). This distinction bears importance, particularly when considering scenarios where we can only query values within the constraint set. In such cases, \(1/2\) has been conjectured to be the optimal approximation ratio ( section B in Appendix). We refer to Appendix H for motivation examples where searching over \(\{\}\) is not applicable.

Now, we establish the regret and constraint violation achievable by our proposed Algorithm 1. Before delving into the main theorem, we first present three lemmas, which are adapted from  and are essential for achieving high probability bounds. Given the slight difference in the definition of \(}\), we provide the proofs in Appendices A to C respectively. First, Lemma 3 demonstrates that with high probability, the empirical estimate \(}_{t}\) is close to its mean \(\).

**Lemma 3**.: _The following holds with probability at least \(1-\):_

\[_{t=1}^{T}\|}_{t}-\| Q)},\]

_where \(Q>0\) is some universal constant._

Next, Lemma 4 establishes that with high probability, the \(()\) computed using \(}_{t}\) and \(g()\) calculated using \(\) are close.

**Lemma 4**.: _Let \(\) be fixed. For a fixed \(t[T]\) and \(\{_{t}:=C^{2}( )}\}_{t=1}^{T}\), \(|_{t}()-g()|_{t}\) holds with probability at least \(1-\)._

Finally, Lemma 5 provides an upper bound for the total constraint violation.

**Lemma 5**.: _Let \(\{_{t}\}_{t=1}^{T}\) be defined as in Lemma 4, then the following holds:_

\[C_{T}_{t=1}^{T}_{t}(_{t})+r_{t=1 }^{T}\|}_{t}-\|+_{t=1}^{T}_{t}.\] (9)

Armed with these results, we can now establish regret and constraint violation bounds for Algorithm 1.

**Theorem 1**.: _Let Assumptions 12 3 4 be satisfied. Let \(U=\{G,C\}\). Choose \(=}\) and \(=8^{2}\). Let \(_{1},,_{T}\) be the sequence of solutions obtained by Algorithm 1. When \(T\) is sufficiently large, i.e., \(T^{2}}{U^{2}}\), we have the following \(\)-regret and constraint violation bounds with probability at least \(1-\):_

\[[R_{T}]=()C_{T}=(T^{3/4}).\]The complete theorem statement and the complete proof are in Appendix D.

Partial Proof:From the update of \(_{t}\), we have that for any \(\),

\[\|_{t+1}-\|^{2} =\|_{}(_{t}+_{x} }_{t}(_{t},_{t}))-\|^{2}\] \[\|_{t}+_{x}}_{t}(_{t},_{t})-\|^{2}\] \[\|_{t}-\|^{2}+^{2}\|_ {x}}_{t}(_{t},_{t})\|^{2}-2(-_{t})^{}_{x}}_{t}( _{t},_{t}).\] (10)

Rearranging, and using Assumption 4 and Assumption 3 we have

\[(-_{t})^{}_{x}}_ {t}(_{t},_{t})(\|_{t}-\|^{2}-\|_{t+1}-\|^{2})+ G^{2}+4^{2} _{t}^{2}.\] (11)

Applying similar steps on the \(\) updates, we establish

\[(-_{t})^{}_{}_{t}(_{t}, _{t})-(\|_{t}-\|^{2}-\|_{t+1}- \|^{2})-C^{2}-2_{t}^{2}-2^{2}^{3}_{t}^{2}.\] (12)

From monotonicity and DR-submodularity of \([f_{t}()]\), we have

\[[_{t}(,_{t})-2_{ t}(_{t},_{t})]\] \[=[[_{t}(,_{t})-2 _{t}(_{t},_{t})|_{t}]]\] \[[(-_{t})^{}_{x} [}_{t}(_{t},_{t})|_{ t}]]+_{t}_{t}()-_{t}^{2}\] (Lemma 1) \[[\|_{t}-\|^{2}- \|_{t+1}-\|^{2}]+G^{2}+4^{2}_{t}^{2}+ _{t}_{t}()-_{t}^{2},\] (13)

where (13) follows from (11). Similarly, from convexity of function \(_{t}(,)\) w.r.t \(\), we have

\[_{t}(_{t},)-_{t}(_{t},_{t}) (-_{t})^{}_{}_{t}( _{t},_{t})\] \[-(\|_{t}-\|^{2}-\|_{t+1} -\|^{2})-C^{2}-2_{t}^{2}-2^{2}^{3}_{t}^{2},\] (14)

where (14) follows from (12). Subtracting two times (14) from (13), and sum \(t\) over 1 through \(T\), we get

\[_{t=1}^{T}[_{t}(,_{t}) -2_{t}(_{t},)]\] \[}{2}+}{}+G^{2} T+ ^{2}_{t=1}^{T}_{t}^{2}+2C^{2} T+4_{t=1}^{T} _{t}^{2}+4^{2}^{3}_{t=1}^{T}_{t}^{2}+_{t} _{t}()-_{t}^{2},\] (15)

Expanding the left hand side of (15) and rearranging, we deduce

\[_{t=1}^{T}[f()-2f(_{t})]+[2 _{t=1}^{T}_{t}(_{t})-( T+)^{2}]\] \[ 2_{t=1}^{T}_{t}_{t}()+ (4^{2}+4^{2}^{2}-)_{t=1}^{T}_{t}^{2} +}{2}+G^{2} T+2C^{2} T+4_{t=1}^{T}_{t}^{2}.\] (16)

To ensure that the equation \(4^{2}+4^{2}^{2}-=0\) has real roots, we require \(T^{2}}{U^{2}}\). Setting \(=8^{2}\) ensures that \(4^{2}+4^{2}^{2}- 0\). Set \(=^{*}\); From Lemma 4, with probability at least \(1-\), \(_{t}(^{*})=_{t}(^{*})-_{t}  g(^{*})\) holds. since \(^{*}\) satisfies the long term constraint, we have \(g(^{*}) 0\). Thus, we can drop the first two terms in the RHS of (16) and by union bound, we get with probability at least \(1-\),

\[_{t=1}^{T}[f(^{*})-2f(_{t})]+[2_{t=1}^{T} _{t}(_{t})-( T+) ^{2}]}{2}+G^{2} T+2C^{2} T+4_{t =1}^{T}_{t}^{2}.\] (17)Maximizing the LHS of (17) with respect to \(\) over the range \([0,+)\), we get a solution of \(=^{T}_{t}(_{t})]_{+}}{  T+1/}\). Plugging this into (17) gives us

\[_{t=1}^{T}[f(^{*})-2f(_{t})]+^{T} _{t}(_{t})]_{+}^{2}}{ T+1/} }{2}+G^{2} T+2C^{2} T+4_{t=1}^{T}_{t}^{2}.\] (18)

Plugging in \(U=\{G,C\}\) and \(=}\), we have with probability at least \(1-\),

\[_{t=1}^{T}[f(^{*})-2f(_{t})]+^{ T}_{t}(_{t})]_{+}^{2}}{ T+1/} +8dU,\] (19)

where we used the fact that \(_{t=1}^{T}} 2\). This gives us our result on objective regret:

\[_{t=1}^{T}[f(^{*})-f(_{t})]= ().\] (20)

The detailed proof, including constraint violation steps, are provided in Appendix D.

## 6 First Order Full-information Case

In this section, we introduce our second proposed algorithm for online DR-submodular maximization subject to stochastic long-term constraints under the first-order full-information feedback. In the interest of space, the algorithm is presented in Algorithm 2 in the Appendix. The overall structure of the algorithm is similar to Algorithm 1, but the primal update uses the gradient of the non-oblivious function:

\[_{x}}_{t}(_{t}, _{t})=F_{t}(_{t})-_{t} _{t}(_{t}),\] \[_{t+1}=_{}(_{t}+_{x}}_{t}(_{t},_{t})),\]

where the non-oblivious function \(F\) is defined by its gradient: \( F()=_{0}^{1}e^{z-1} f(z)dz\). As we discussed in Section 3, the non-oblivious function \(F\) plays an important role in obtaining the optimal approximation ratio \(1-1/e\). However, calculating the gradient of the non-oblivious function \(F()\) can be challenging, especially when only unbiased estimates of the gradients are available. To overcome this,  presents a computational approach for obtaining an unbiased estimate of the gradient of \(F()\) through sampling (Lines 6 and 7). The following lemma indicates that \((1-1/e)f_{t}(z*)\) is an unbiased estimator of \( F()\) with bounded variance.

**Lemma 6**.: _If \(z\) is sampled from r.v. \(\) as in line 6 of Algorithm 2, \([f_{t}()]= f( )\), and \([\|f_{t}()- f()\|^{2}]^{2}\), we have_

_(i) \([(1-1/e)f_{t}(z*)| \,]= F();\)_

_(ii) \([\|(1-1/e)f_{t}(z*)- F ()\|^{2}]_{1}^{2},\) where \(_{1}^{2}=2(1-1/e)^{2}^{2}+^{2 }(1-1/e)}{3}.\)_

With the unbiased estimator of the gradient of the non-oblivious function, we show the following regret and constraint violation guarantee for our Algorithm 2 in Appendix E:

**Theorem 2**.: _Let Assumptions 1 2 3 4 be satisfied. Let \(U=\{G_{F},C\}\). Choosing \(=}\) and \(=4^{2}\). Let \(_{t},\)\(t[T]\) be the sequence of solutions obtained by Algorithm 2. When \(T\) is sufficiently large, i.e., \(T^{2}}{U^{2}}\), we have the following \((1-1/e)\)-regret and constraint violation bounds with probability at least \(1-\):_

\[[R_{T}]=()C_{T}=(T^{3/4}).\]Conclusions

In this paper, we address the problem of stochastic DR-submodular maximization with stochastic long-term constraints over a general convex set. We introduce the first algorithm for this setting, attaining \(()\) regret and \((T^{3/4})\) constraint violation bounds. Notably, our algorithm operates in both the semi-bandit feedback and first-order full-information setting, requiring only 1 gradient query per round, while all previous works operate in the full-information setting with \(\) gradient queries per round. Extension of the results here to upper-linearizable functions in  is an open direction.

## 8 Acknowledgement

This work was supported in part by the National Science Foundation under grants CCF-2149588 and CCF-2149617. We acknowledge Yiyang (Roy) Lu for helpful feedback.