# Towards Accurate and Fair Cognitive Diagnosis via Monotonic Data Augmentation

Zheng Zhang\({}^{1,2}\) Wei Song\({}^{1,2}\) Qi Liu\({}^{1,2}\) Qingyang Mao\({}^{1,2}\) Yiyan Wang\({}^{3}\)

Weibo Gao\({}^{1,2}\) Zhenya Huang\({}^{1,2}\) Shijin Wang\({}^{2}\) Enhong Chen\({}^{1,2}\)

1: University of Science and Technology of China

2: State Key Laboratory of Cognitive Intelligence

3: Beijing Normal University

{zhangzheng,sw2,maoqy0503,weibogao}@mail.ustc.edu.cn;

{qiliuql,huangzhy,cheneh}@ustc.edu.cn;

wangyiyan@mail.bnu.edu.cn;

sjwang3@ifytek.com

Corresponding Author

###### Abstract

Intelligent education stands as a prominent application of machine learning. Within this domain, cognitive diagnosis (CD) is a key research focus that aims to diagnose students' proficiency levels in specific knowledge concepts. As a crucial task within the field of education, cognitive diagnosis encompasses two fundamental requirements: accuracy and fairness. Existing studies have achieved significant success by primarily utilizing observed historical logs of student-exercise interactions. However, real-world scenarios often present a challenge, where a substantial number of students engage with a limited number of exercises. This data sparsity issue can lead to both **inaccurate** and **unfair** diagnoses. To this end, we introduce a monotonic data augmentation framework, CMCD, to tackle the data sparsity issue and thereby achieve accurate and fair CD results. Specifically, CMCD integrates the monotonicity assumption, a fundamental educational principle in CD, to establish two constraints for data augmentation. These constraints are general and can be applied to the majority of CD backbones. Furthermore, we provide theoretical analysis to guarantee the accuracy and convergence speed of CMCD. Finally, extensive experiments on real-world datasets showcase the efficacy of our framework in addressing the data sparsity issue with accurate and fair CD results.

## 1 Introduction

Intelligent education is a significant domain within machine learning that focuses on exploring students' learning patterns. In the past decades, research in this interdisciplinary field has garnered substantial attention from scholars across various disciplines , including education, machine learning, and psychology. Within intelligent education, cognitive diagnosis (CD) stands out as a crucial research area that aims to measure students' proficiency levels in specific knowledge domains, such as Geometry . For instance, as illustrated in Figure 1, students practice some exercises \(e_{1},e_{2},e_{3}\) and obtain associated responses indicating correctness, which are then utilized to perform CD to infer the students' mastery levels of the corresponding concepts. With a comprehensive understanding of students' abilities, CD can be applied to various applications, including student assessment  and educational recommendation systems .

As a crucial task within the field of education, CD encompasses two fundamental requirements: 1) **Accurate Diagnoses**: It is imperative to precisely evaluate students' mastery of knowledge to facilitatevarious downstream applications. For instance, this accuracy enables teachers to deliver personalized instruction, tailored to the specific needs of each individual student. 2) **Fair Diagnoses**: CD plays a pivotal role in high-stakes examinations like the GRE , significantly influencing individuals' developmental opportunities. Therefore, ensuring fairness in CD is of paramount importance. In this context, fairness means different student groups divided by sensitive attributes (e.g., gender, race) should be treated fairly and equally.

To accomplish these dual objectives, prior research has made notable strides by harnessing historical logs that document students' interactions with exercises. However, in practical scenarios, many students can only interact with a limited number of exercises within the vast exercise pool, leading to the _data sparsity_ issue that may cause both **inaccurate** and **unfair** diagnoses. A detailed analysis is available in Section 4. Currently, several approaches have been proposed to mitigate this issue from a model-centric perspective, which focus on developing more complex architectures to address the challenges posed by data scarcity . However, the incorporation of additional architectures often compromises the model's interpretability, rendering it unsuitable for high-impact educational environments. For instance, in high-stakes exams like the GRE, only IRT , a classical CD model, has been applied due to their statistical superiority . While other model-based models, despite mitigating data sparsity to some extent, remain unused because of their lack of interpretability.

In contrast to these model-based approaches, in this paper, we tackle the data sparsity issue from the perspective of data augmentation without altering the model architecture. However, we have encountered the following challenges: _1) Monotonicity Assumption_. The Monotonicity Assumption is a fundamental theoretical premise in the field of CD. Specifically, it posits that a student's proficiency exhibits a monotonic relationship with the probability of providing a correct response to an exercise. Using Figure 1 as an example, Sue, who correctly responds to exercise \(e_{1}\), is considered to possess a higher proficiency level in the related concept C (i.e., Absolute Value) compared to Bob, who provides an incorrect response. This ensures the interpretability of CD, contributing to its widespread acceptance and application. Therefore, maintaining the model's monotonicity assumption during the data augmentation process is crucial. _2) Theoretical Guarantees_. As data augmentation emerges as a new paradigm in the CD domain, it becomes imperative to establish corresponding theoretical guarantees for this novel approach.

To address these challenges, we propose a monotonic data augmentation CD framework, CMCD, complemented by theoretical guarantees. Specifically, we integrate two data augmentation constraints confronting the monotonicity assumption. For each student, we generate fake students by reversing one of his responses while keeping other records unchanged to train CD models. As illustrated in Figure 2, Bob's wrong answer to \(e_{1}\) is reversed to generate C-1 while his right answer to \(e_{6}\) is reversed to generate C-2. Following the monotonicity assumption, we assume that Bob's proficiency the corresponding knowledge concept is lower than that of C-1 yet higher than that of C-2. Moreover, we provide theoretical analysis to guarantee CMCD's advantage in **accuracy** and **convergence speed**. Finally, we conduct extensive experiments on real-world datasets, demonstrating the effectiveness of our method across various CD models. Our key contributions can be summarized as follows:

* **Problem Emphasis.** Our paper strongly emphasizes the connection between data sparsity and inaccurate and unfair diagnoses. To the best of our knowledge, we are the first to highlight the relationship between data sparsity and educational fairness.

Figure 1: An illustrative example of cognitive diagnosis.

Figure 2: An example of monotonic augmentation.

* **Framework Design.** From the perspective of data augmentation, we combine the monotonicity assumption in CD to propose a general framework, CMCD, which comes with theoretical guarantees of accuracy and convergence speed.
* **Experimental Evaluations.** Extensive experiments on real-world datasets demonstrate the effectiveness of our approach.

## 2 Related Works

### Cognitive Diagnosis

CD plays a crucial role in various real-world educational scenarios [48; 49], including student assessment [36; 31] and educational recommender systems [24; 19]. Initially rooted in psychometrics, CD models such as Item Response Theory (IRT)  have been widely used, particularly in the context of GRE . Later, Multidimensional Item Response Theory (MIRT)  was introduced to extend the single-trait features in IRT to multidimensions. While these models were effective and provided interpretable diagnostic results based on psychometric theories, they heavily relied on handcrafted interaction functions and could only leverage users' numerical response records, which were often affected by data sparsity . With the advancements in machine learning, researchers began developing CD models from a machine-learning perspective. One notable model in this regard is NCDM , which employed neural networks to learn the interactions between students and exercises, yielding satisfactory results. However, the majority of existing works have primarily focused on designing more complex architectures to alleviate data sparsity. In contrast to traditional model-based approaches, this paper embraces a data-centric perspective and introduces CMCD, which comes with strong theoretical guarantees and is adaptable for integration with all cognitive diagnosis models.

### Data Augmentation

Data augmentation is a classical technique to address the problem of insufficient training data in the machine learning community . Over the past few years, this concept has achieved significant success in the fields of neural language processing (NLP) [13; 57; 32] and computer vision (CV) [38; 2; 6]. Later, this idea was applied to user modeling tasks, which necessitated the simulation of user-item interaction data. The work most similar to ours involves data augmentation of user-item interactions in recommendation systems. They typically answer questions such as: "What would happen if...?" Through these responses, they can generate additional virtual data, thereby alleviating the issue of data sparsity. For instance, Wang et al.  addressed the issue of data sparsity in the sequential recommendation and provided an answer to the question: "What would a user like to buy if their previously purchased items had been different?". Similarly, Xiong et al.  focused on review-based recommendation and answered the question: "What would be the user's decision if their feature-level preference had been different?" Although these works have shown promising results, when applied to cognitive diagnosis, they overlook the monotonicity assumption, a classic theory in education. Additionally, they lack corresponding theoretical guarantees. In this paper, we aim to bridge these gaps to alleviate data sparsity in cognitive diagnosis more effectively.

## 3 Preliminaries

### Cognitive Diagnosis and Models

In this subsection, we formally define the CD problem. Assume there are \(m\) students, \(n\) exercises, which are denoted as \(S=\{s_{i}\}_{i=1}^{m},E=\{e_{j}\}_{j=1}^{n}\). Assume each student \(s_{i}\) has records on exercises indexed by \(Q_{i}\{1,2,,n\}\), the response logs \(R_{i}\) of student \(s_{i}\) are a set of triplets \((s_{i},e_{j},y_{i,j})\), where \(j Q_{i}\), \(y_{i,j}\{0,1\}\) is the score obtained by student \(s_{i}\) on exercise \(e_{j}\). Given response logs \(R_{i}\) of student \(s_{i}\), the goal of CD is to mine the proficiency \(_{i}\).

Cognitive Diagnosis Models (CDMs) are developed to depict student's proficiency level on specific knowledge concepts based on her responses to several test items. To do this, an objective function is used to train CDMs on the student performance prediction task. More concretely, CDMs are expected to minimize the difference of the predicted probability \(p_{j}(_{i})\) of a student \(s_{i}\) giving the right response to the exercise \(e_{j}\) between the true response \(y_{i,j}=p_{j}(_{i})\). In this paper, following the classical works , we adopt the cross-entropy loss, the goal is,

\[=_{i=1}^{m}l_{i}(_{i})=-_{i=1}^{ m}_{j Q_{i}}[y_{i,j} p_{j}(_{i})+(1-y_{i,j})(1-p_{j}(_{ i}))]. \]

In the past decades, lots of CDMs have been proposed such as IRT [29; 11], MIRT. Generally, CDMs contain two parts: (1) the representations of trait features and (2) the interaction function. For example, IRT models each student \(s_{i}\) as a proficiency variable \(_{i}\), each exercise as a discriminating factor \(_{j}\) and a difficulty factor \(_{j}\), and a logistic function is used to forecast the likelihood that student \(s_{i}\) will answer exercise \(e_{j}\) correctly based on a logistic function 2, i.e., \(p_{j}(_{i})=1/(1+e^{_{j}(_{i}-_{j})})\).

### Fairness in Cognitive Diagnosis

With the advancement of machine learning technologies [20; 9], such as large language models , which have found widespread applications in many important scenarios [51; 52], trustworthy AI has become a very important topic [18; 27; 28; 34; 33; 17]. Among these concerns, the issue of fairness has garnered widespread attention [22; 37; 8; 41; 53; 47; 40]. Given that cognitive diagnosis holds a fundamental position in the field of education and is extensively applied in high-stakes exams such as the GRE , which significantly shapes individuals' developmental opportunities, ensuring fairness in cognitive diagnosis becomes of utmost importance. In this paper, we follow the widely accepted group fairness definition proposed by Li et al. , which states that a fair model should provide the same level of utility performance for different user groups.

For analytical convenience, we focus on the case where the sensitive attribute is binary, which can be easily extended to multiple values. The student group can be divided into two groups based on their sensitive attributes, denoted as \(G_{0}\) and \(G_{1}\), where \(U=G_{0} G_{1}\), \(G_{0} G_{1}=\). We denote the number of samples in each group as \(m_{0}\) and \(m_{1}\). Inspired by previous works [54; 50; 23], in this paper, fairness in cognitive diagnosis is defined as follows, where a lower value indicates better fairness performance:

**Definition 3.1** (Fairness in Cognitive Diagnosis).: \[GF=|}_{s_{i} G_{0}}(s_{i})-}_{s_{i} G_{1}}(s_{i})|,\] (2)

where \(\) represents a metric for evaluating utility performance, such as \(\) or \(\) score, and \((s_{i})\) denotes the utility performance for student \(s_{i}\).

After introducing the fairness definition in cognitive diagnosis, our objective extends beyond accurately identifying students' proficiency. We should also strive to meet fairness requirements, with the goal of minimizing \(GF\).

## 4 Data Sparsity Analysis

In this section, we explore data sparsity in cognitive diagnosis through the real-world dataset, Math (detailed information will be introduced in the Experiment). Firstly, we conduct an analysis of data sparsity within the dataset, partitioning it based on the number of responses per student. The statistical data is illustrated in Figure 3(a). From the figure, it is evident that the majority of users have a low log count, highlighting the widespread presence of data sparsity. Following the methodology outlined in , we classify records with fewer than 50 responses as the sparse group. Next, we validate whether data sparsity affects the accuracy and fairness of cognitive diagnosis.

Inaccurate ResultFrom the perspective of accuracy, we conduct a comparative analysis between the sparse and non-sparse groups, as illustrated in Figure 3(b). The results indicate a notable decrease in performance for the sparse group across different backbones compared to the non-sparse group. This outcome strongly implies that data sparsity can result in inaccurate diagnostic results.

Unfair ResultFrom the perspective of fairness, we examine the performance of the two groups divided by sensitive attributes. In the Math dataset, the student group is divided into Area1 and Area2 by sensitive attribute region, with Area1 representing the disadvantaged group. The results are depicted in Figure 3(c), unveiling significant variations in performance outcomes among different groups and indicating the presence of unfairness. Following this, we delve into the disparity in sparsity levels between the two groups, as shown in Figure 3(d). The results highlight significantly divergent sparsity levels, notably with the disadvantaged groups exhibiting higher sparsity. This discrepancy is attributed to the likelihood that disadvantaged groups may have fewer opportunities to engage in relevant online learning platforms compared to their advantaged counterparts. This statistical outcome directly implies that data sparsity has a greater impact on the disadvantaged group (Area1). Based on this observation, we can conclusively state that data sparsity can lead to unfair diagnostic results.

So far, we have validated that data sparsity in cognitive diagnosis can result in inaccurate and unfair diagnostic outcomes. Consequently, mitigating the issue of data sparsity in cognitive diagnosis is of paramount importance. In the following sections, we will demonstrate how to address data sparsity from the perspective of data augmentation.

## 5 The Proposed Framework

In this section, we propose a monotonic data augmentation framework, CMCD, as depicted in Figure 4. It can address the issue of data sparsity and ensure accurate and fair diagnostic results. Firstly, we present the constraints of our proposed monotonic data augmentation approach. Subsequently, we offer theoretical assurances regarding the effectiveness of CMCD.

### Monotonic Data Augmentation

In CD, there is a fundamental assumption called the monotonicity assumption , which plays a vital role in ensuring interpretability. In this paper, we introduce a novel approach that integrates the monotonicity assumption with data augmentation. This combined technique allows us to generate more realistic student response data that conforms to educational principles. As a result, we can effectively address the challenge of data sparsity. The monotonicity assumption is defined as follows:

**Monotonicity Assumption** The probability of correct response to the exercise is monotonically increasing at any dimension of the student's knowledge proficiency.

Building upon the monotonicity assumption, suppose two students \(s_{i}\) and \(s_{i^{}}\) have answered exactly the same exercises, i.e., \(Q_{i}=Q_{i^{}} Q\), we propose two specific hypotheses as follows:

**Hypothesis 1**: If \(s_{i}\) answered exercise \(e_{j}\) correctly, while \(s_{i^{}}\) answered \(e_{j}\) incorrectly, and answers for other exercises are the same, then the proficiency level of \(s_{i}\) should not be lower than \(s_{i^{}}\). Formally, for only one \(j Q,r_{i,j}=1,r_{i^{},j}=0\), while for \(k Q,k j,r_{i,k}=r_{i^{},k}\), we have \(_{i}_{i^{}}\).

Figure 4: The CMCD framework.

Figure 3: The impact of data sparsity on CD models

**Hypothesis 2**: If \(s_{i}\) answered exercise \(e_{j}\) incorrectly, while \(s_{i^{}}\) answered \(e_{j}\) correctly, and answers for other exercises are the same, then the proficiency level of \(s_{i}\) should not be greater than \(s_{i^{}}\). Formally, for only one \(j Q,r_{i,j}=0,r_{i^{},j}=1\), while for \(k Q,k j,r_{i,k}=r_{i^{},k}\), we have \(_{i}_{i^{}}\).

To leverage these hypotheses and enhance the training data to mitigate the data sparsity issue, we first need to construct suitable interaction records for students. Specifically, for student \(s_{i}\), we randomly select one of his answered exercise \(e_{j} E_{i}\) and flip the corresponding score on the interaction record (changing 1 to 0 or 0 to 1), and keep other records unchanged. By doing so, we generate the records for \(s_{i^{}}\). Assume the proficiency of \(s_{i^{}}\) is \(_{i,j}\), the fitting function for student \(s_{i^{}}\) can be expressed as:

\[ l_{i,j}(_{i,j})&=-_{k Q _{i},k j}[y_{i,k} p_{k}(_{i,j})+(1-y_{i,k})(1-p_{k}(_{ i,j}))]\\ &-[(1-y_{i,j}) p_{j}(_{i,j})+y_{i,j}(1-p_{j}( _{i,j}))]. \]

Next, we consider the partial order relationship between \(s_{i}\), \(s_{i^{}}\). Two scenarios need to be considered.

According to **Hypothesis 1**, when \(_{i}_{i,j}\) should hold but \(_{i}<_{i,j}\) in fact, we need to add a regularization term \((_{i,j}-_{i})^{2}\) to impose the constraint. Similarly, as for **Hypothesis 2**, when \(_{i}_{i,j}\) should hold but \(_{i}>_{i,j}\) in fact, we need to add a regularization term \((_{i}-_{i,j})^{2}\) as penalty. As a result, we establish two regularization terms:

\[_{1}(_{i},_{i,j})=_{[_{i}<_{i,j}]}( _{i,j}-_{i})^{2},_{2}(_{i},_{i,j})=_{[_{i}>_{i,j}]}(_{i}-_{i,j})^{2}, \]

where \(\) is the indicator function. Then we summarize the final regularization term as:

\[(y_{i,j},_{i},_{i,j})=y_{i,j}_{1}+(1-y_{i,j})_{ 2}. \]

Moreover, to enhance the monotonicity assumption, for each student \(s_{i}\), we consider generating multiple fake students and comparing them with \(s_{i}\) by randomly sample a subset \(C_{i} Q_{i}\). For each exercise in \(C_{i}\), we perform a flipping operation on the corresponding score while keeping the other records unchanged, resulting in \(|C_{i}|\) versions of student \(s_{i}\).

As a result, the final loss function in CMCD is:

\[}=_{i=1}^{m}[l_{i}(_{i})+_{j C_ {i}}(l_{i,j}(_{i,j})+c(y_{i,j},_{i},_{i,j})) ], \]

where \(c\) represents the strength of constraint.

### Theoretical Guarantees

In this section, we provide theoretical guarantees of CMCD's effectiveness in terms of accuracy and convergence speed. For theoretical analysis, we will focus on the most classic cognitive diagnostic model, IRT (introduced in Section 3.2), which serves as the foundation for many other cognitive diagnostic models and has been widely implemented in GRE . Please note that the following theoretical analysis can be easily extended to other CDMs.

Firstly, we analyze the accuracy of CMCD. Given that the core objective of cognitive diagnostic tasks is to estimate the true proficiency \(_{i}\) of student \(s_{i}\), to assess the accuracy of our CMCD, we theoretically analyze the Maximum Likelihood Estimation (MLE) of \(_{i}\) through the optimization of \(}\) (Eq. (6)). Let \(=(_{1},_{2},,_{m})\), we assert that CMCD has accurate estimation stated as:

**Proposition 5.1** (Accuracy).: _CMCD can accurately estimate the student proficiency level \(\)._

To validate the above proposition, we prove that the MLE under CMCD is a consistent estimation:

**Theorem 5.2**.: _Suppose a student \(s_{i}\) with ability \(_{i}\) has records \(R_{i}\) on exercises indexed by \(Q_{i}\), and \(_{i}^{*}\) denote the MLE of \(_{i}\) under \(}\), then \(_{i}^{*}\) is a consistent estimation of \(_{i}\), formally,_

\[>0,>0, n_{0}^{+},|Q_{i}| n_{0},(|_{i}^{*}-_{i}|)<. \]

Detailed proofs for Theorem 5.2 can be found in Appendix A.1. The consistency estimation ensures the accuracy of CMCD. Next, we explore the superiority of CMCD in convergence speed compared to traditional CD models (the loss function \(\) is introduced in Eq.(1)).

**Proposition 5.3** (Convergence Speed).: _In terms of the estimation of proficiency level \(\), CMCD has a faster convergence speed than the original approach._

To validate the above proposition, we first notice that \(\) and \(_{}\) are both convex. Then, in the specific zone that violates the monotonicity assumption, we prove that \(_{}\) are strong-convex in terms of \(\). Finally we can analyze and compare the convergence speed in optimizations of \(\) and \(_{}\).

**Theorem 5.4**.: \(\) _and \(_{}\) are convex in terms of \(\). In addition, \(_{}\) is strong-convex when_

\[ D=\{(_{1},_{2},,_{m})| i[m],  j C_{i},y_{i,j}_{[_{i}<_{i,j}]}+(1-y_{i,j}) _{[_{i}>_{i,j}]}=1\}, \]

_and, meanwhile, we can also conclude \(^{2}_{}>^{2}\) when \( D\)._

Detailed proofs for Theorem 5.4 are provided in Appendices A.2. When performing the gradient descent algorithm to optimize the loss function, it is widely known that strong-convex functions converge faster than convex functions . Overall, this represents the primary distinction between \(_{}\) and \(\). By examining the frequency of occurrences of this expression during the optimization process, we can analyze the convergence speeds of the two functions. The more frequent the occurrences, the faster the convergence of \(_{}\). Simultaneously, this expression also indicates a violation of our hypotheses, necessitating the incorporation of regularization. To validate the superior convergence speed, we calculated instances of hypothesis violation across different datasets, with the results illustrated in Figure 5. It reveals a prevalent occurrence of strong convexity in \(_{}\) across various datasets and backbones. Therefore, by optimizing \(_{}\), CMCD converges faster.

## 6 Experiments

In this section, we first introduce the dataset and experimental setup. Then, we conduct extensive experiments on real-world datasets to answer the following questions:

* **RQ1:** Does CMCD learn fair and accurate cognitive diagnosis results?
* **RQ2:** How does the constraint of data augmentation impact CMCD?
* **RQ3:** Does CMCD have a faster convergence speed compared to other baseline models?
* **RQ4:** How do different hyper-parameter settings (i.e. \(c\), \(|C_{i}|\)) affect the performance?

The code is released at [https://github.com/Mercidiaha/CMCD](https://github.com/Mercidiaha/CMCD).

### Experimental Setup and Baselines

DatasetsIn this paper, we conduct experiments in two diagnostic datasets, namely Math and ASSIST. The Math dataset was collected from an online educational system that provides homework, and evaluation for students. It collected records of junior high school students on mathematical exercises. In this dataset, we take the region as the sensitive attribute. The ASSIST dataset (ASSISTments 2009-2010 "skill builder") is an open dataset collected by the ASSISTments online tutoring systems , which includes student response logs and knowledge concepts 3. Given that the ASSIST dataset does not provide sensitive attribute information, we follow  and categorize users into two groups based on whether the number of their responses exceeds 50. Regarding the dataset division, we allocate 80% of each student's response log for training and the remaining 20% for testing. The overview of basic statistics can be found in Appendix Table 2.

Figure 5: The number of instances violating our hypotheses during the optimization process.

Evaluation.In this paper, we evaluate the results of user modeling by predicting scores. This evaluation can be divided into two aspects: utility and fairness evaluation. For utility evaluation, following previous works , we adopt different metrics from the perspectives of both regression (MAE, RMSE) and classification (AUC, ACC). For fairness evaluation, we adopt the Definition introduced in Section 3.2. For the \(\) in Eq. (2), we adopt MAE, RMSE, AUC, ACC (denoted as \(\)MAE, \(\)RMSE, \(\)AUC, \(\)ACC in the experiment).

Baseline approaches.CMCD is a versatile framework applicable to various CD models. To validate its generality, we employed CMCD in three classical CD backbones, IRT , MIRT , NCDM . For the baselines, we compare our methods with two categories of methods. Firstly, we compare our methods with the data augmentation baseline. To the best of our knowledge, there are currently no data augmentation methods specifically applied to diagnostics. Therefore, following the approach , we constructed the following baselines: CF-IRT, CF-MIRT, and CF-NCDM. Secondly, as our model enhances fairness, we compare it with several classical fairness baselines, such as CD+GF , CD+EO , CD+DP . Further details regarding these baselines and the implementation of our methods can be found in Appendix A.3.

### Experimental Results

Rq1.In this section, we discuss whether CMCD framework can alleviate the issue of data sparsity, thereby enhancing both the accuracy and fairness of the model. Specifically, we conducted comparative experiments with baselines on the Assistant and Math datasets, and the results of Math datasets results are presented in Table 1, the results of Assistant datasets can be found in Appendix A.4. From these results, we can draw the following findings:

(1) From the perspective of utility, we observed a significant enhancement in CMCD across different datasets and various backbones. This underscores the effectiveness of our approach. Furthermore, we noted that, compared to backbone models like NCDM that rely on neural networks, CMCD demonstrates more pronounced improvements on traditional backbones (IRT). We attribute this to the fact that models dependent on neural network backbones, such as NCDM, exhibit superior data fitting capabilities, thereby alleviating issues related to data sparsity. Consequently, the gains achieved by CMCD on NCDM are less conspicuous. In contrast, traditional IRT models are more susceptible to the impact of data sparsity, making CMCD's influence more substantial in such cases.

(2) From the perspective of fairness, we observed that CMCD can, to a certain extent, alleviate unfair phenomena in cognitive diagnostic models. Particularly noteworthy is its performance on the Math dataset, where our model achieved the best fairness outcomes across different backbones (IRT, MIRT). This underscores that CMCD, while mitigating data sparsity issues, concurrently promotes fairness in model outcomes.

    &  &  &  \\  
**Approach** & **RMSE\(\)** & **MAE\(\)** & **AUC\(\)** & **ACC\(\)** & **RMSE\(\)** & **MAE\(\)** & **AUC\(\)** & **AUC\(\)** & **RMSE\(\)** & **MAE\(\)** & **AUC\(\)** & **ACC\(\)** \\  Origin & 0.409 & 0.325 & 0.821 & 0.751 & 0.409 & 0.309 & 0.822 & 0.757 & 0.414 & 0.317 & 0.812 & 0.748 \\ CD+Reg & 0.412 & 0.332 & 0.314 & 0.747 & 0.409 & 0.310 & 0.821 & 0.755 & 0.416 & 0.348 & 0.806 & 0.744 \\ CD+EO & 0.412 & 0.334 & 0.813 & 0.744 & 0.413 & 0.317 & 0.814 & 0.750 & 0.420 & 0.321 & 0.803 & 0.740 \\ CD+DP & 0.409 & 0.339 & 0.818 & 0.749 & 0.410 & 0.314 & 0.819 & 0.753 & 0.414 & 0.331 & 0.811 & 0.748 \\   & CF-IRT & 0.409 & 0.314 & 0.821 & 0.753 & 0.411 & 0.304 & 0.823 & 0.755 & 0.420 & 0.316 & 0.804 & 0.743 \\ CF-MIRT & 0.408 & 0.324 & 0.820 & 0.752 & 0.417 & 0.300 & 0.816 & 0.751 & 0.418 & 0.318 & 0.808 & 0.743 \\ CF-NCDM & 0.406 & 0.318 & 0.824 & 0.756 & 0.406 & 0.312 & 0.826 & 0.758 & 0.417 & **0.312** & 0.809 & 0.747 \\   & CMCD & **0.394\(\)*** & **0.300\(\)*** & **0.342\(\)*** & **0.372\(\)*** & **0.409\(\)*** & **0.233\(\)*** & **0.476\(\)*** & **0.413\(\)*** & 0.316\(\)*** & **0.314\(\)*** & **0.374\(\)*** \\   & **RMSE\(\)** & **MAE\(\)** & **AUC\(\)** & **ACC\(\)** & **RMSE\(\)** & **MAE\(\)** & **AUC\(\)** & **ACC\(\)** & **RMSE\(\)** & **MAE\(\)** & **AUC\(\)** & **ACC\(\)** \\  Origin & 0.088 & 0.052 & 0.027 & 0.008 & 0.032 & 0.035 &(3) From the perspective of the trade-off between fairness and utility, our model consistently surpasses the baselines with data augmentation in both performance and fairness outcomes. While compared to fairness-aware baselines, our model may not always outperform dedicated fairness-enhancing methods on specific datasets, it is important to note that these baselines often come at the expense of utility. In contrast, CMCD not only enhances fairness but also improves performance. This highlights that CMCD achieves the optimal trade-off between fairness and utility.

Rq2.To address the issue of data sparsity, we introduce two hypotheses for data augmentation. In this section, we validate the effectiveness of these two hypotheses. Specifically, we conduct ablation experiments on the Math dataset, where we systematically removed the corresponding hypothesis strategies. The results are illustrated in Figure 6. It is evident that after removing different hypothesis strategies, both Fairness and Utility performances exhibited varying degrees of decline, providing evidence for the efficacy of each strategy. Additionally, we observed that these two hypotheses yield different effects on various backbones. In comparison to NCDM, a neural network-dependent model, hypotheses perform better on traditional models like IRT and MIRT. We attribute this to the powerful data-fitting capability of NCDM, which alleviates issues related to data sparsity. In contrast, models such as IRT and MIRT, constrained by data sparsity, exhibit limited capabilities.

Rq3.In Section 5.2, we theoretically demonstrated that CMCD exhibits a faster convergence rate. In this section, we empirically validate the superiority of CMCD in terms of convergence speed. Specifically, we compared the convergence speeds (the epoch at which early stopping occurs) of different baselines on various backbones using the Math dataset, as depicted in Figure 7. It is observed that in the early stages, CMCD shows minimal differences compared to other baselines. We attribute this to the fact that, initially, the effectiveness of the backbone models is not optimal, resulting in less precise enhancement of the monotonicity assumption. However, as the number of epochs increases, CMCD significantly outpaces other baselines, demonstrating a faster convergence.

Rq4.In CMCD, two hyperparameters control the effectiveness. Specifically, \(c\) controls the strength of the constraints, and \(|C_{i}|\) regulates the number of generated students. In this section, we investigate the impact of adjusting these two hyperparameters on CMCD using the Math dataset. The specific results are presented in Figure 8 and Figure 9. In Figure 8, a consistent trend is observed across different backbones. Initially, with the strengthening of \(c\), both fairness and utility show improvement, indicating the efficacy of our monotonicity enhancement. However, as \(c\) reaches a certain intensity, both fairness and utility experience a decline. We attribute this to the possibility that an excessively large \(c\) might interfere with the model's normal training, leading to a decrease in effectiveness. Moving to Figure 9, diverse trends are identified across different backbones. Specifically, in the case of IRT, as the number of contrasts increases, both fairness and utility consistently improve. Conversely, for NCDM, as the quantity of contrasts grows, there is a continuous decline in utility. We attribute these observations to inherent differences in the nature of the models. IRT is more affected by data sparsity, so enhancing \(C_{i}\) effectively mitigates data sparsity, yielding improved results. On

Figure 6: The ablation experiment on Math dataset.

Figure 7: Convergence against training epoch for different backbones on Math dataset.

the other hand, NCDM, benefiting from its neural network's fitting capacity, partially alleviates data sparsity issues. Consequently, an increase in contrasts may interfere with its intrinsic performance.

## 7 Conclusion and Discussion

In this paper, we conducted a focused study on addressing data sparsity in CD. Initially, we empirically validated that data sparsity leads to inaccurate and unfair diagnostic results. Subsequently, by integrating data augmentation and the monotonicity assumption, we introduced two constraints to alleviate the issue of data sparsity. Moreover, we provided theoretical guarantees regarding accuracy and convergence speed. The experiments on real-world datasets demonstrated the effectiveness of our approach in addressing the data sparsity issue, culminating in fair and accurate diagnosis results.

CD is a highly significant task in intelligent education. The generated diagnostic results can be applied to various areas, e.g., personalized tutoring and intelligent question recommendations. This can reduce the burden on teachers and students, providing effective learning experiences for students. Our CMCD mitigates the unfairness and inaccuracy issues arising from data sparsity, thereby promoting educational equity to a certain extent. Simultaneously, we recognize that CD relies on students' response records that might be inaccessible due to privacy concerns. In the future, we will contemplate incorporating federated learning to enhance diagnostic services and ensures student privacy.