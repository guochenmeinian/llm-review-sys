# Outlier-Robust Distributionally Robust Optimization

via Unbalanced Optimal Transport

 Zifan Wang

KTH Royal Institute of Technology

zifanw@kth.se

&Yi Shen

Duke University

yi.shen478@duke.edu

&Michael M. Zavlanos

Duke University

michael.zavlanos@duke.edu

&Karl H. Johansson

KTH Royal Institute of Technology

kallej@kth.se

###### Abstract

Distributionally Robust Optimization (DRO) accounts for uncertainty in data distributions by optimizing the model performance against the worst possible distribution within an ambiguity set. In this paper, we propose a DRO framework that relies on a new distance inspired by Unbalanced Optimal Transport (UOT). The proposed UOT distance employs a soft penalization term instead of hard constraints, enabling the construction of an ambiguity set that is more resilient to outliers. Under smoothness conditions, we establish strong duality of the proposed DRO problem. Moreover, we introduce a computationally efficient Lagrangian penalty formulation for which we show that strong duality also holds. Finally, we provide empirical results that demonstrate that our method offers improved robustness to outliers and is computationally less demanding for regression and classification tasks.

## 1 Introduction

Consider a stochastic optimization problem that aims to find a decision variable \(\) that minimizes the expected loss function \(_{^{*}}[l(,)]\), where the random samples \(\) are drawn from a fixed distribution \(^{*}\) and \(^{d}\) is a compact and convex set. In many applications, the distribution of interest \(^{*}\) is not precisely known; yet datasets that contain finite samples independently drawn from the distribution \(^{*}\) are often available, which yield an empirical distribution estimate \(}\). For instance, in image classification problems, \(^{*}\) can represent the joint distribution of the features and the labels, and \(}\) can represent the empirical distribution of a dataset that is annotated by humans ; in portfolio optimization problems, \(^{*}\) captures the uncertainty of a set of financial products in the future months and \(}\) contains historical prices of all the products ; in healthcare applications, \(^{*}\) can represent the feature distribution of a population of interest, and \(}\) represent the available electronic health record data .

The above stochastic optimization problem is often solved using empirical risk minimization (ERM) , which searches for the best decision variable \(\) that minimizes the expected loss function with respect to the empirical distribution, i.e., \(_{}=_{}_{}[l (,)]\). However, ERM is not guaranteed to provide good out-of-sample performance, i.e., the value of \(_{^{*}}[l(,_{})]\) is high when the empirical estimate \(}\) is not close to the true distribution \(^{*}\). For example, in the presence of distribution shifts or contaminated datasets, the out-of-sample performance cannot be improved by increasing the sample size. To enhance the out-of-sample performance of the decision variable, researchers have developed methods that use distributionally robust optimization (DRO) to addressthe mismatch between \(}\) and \(^{*}\)[6; 7; 8; 9; 10; 11; 12]. In contrast to ERM, DRO aims to optimize outcomes against the worst-case scenario over a set of distributions that are close to the empirical distribution according to a pre-specified distance metric. This set of distributions is called the ambiguity set and is typically assumed to contain the distribution of interest, e.g., the true distribution \(^{*}\). The construction of the ambiguity set is critical in the development of various DRO methods, as it ideally encompasses all prior knowledge about the true distribution. For example,  defines the ambiguity set using moment constraints. Moment constraints assume that the true distribution has similar moments compared to the empirical distribution, e.g., means and variances. The ambiguity set can also include distributions that are similar to \(}\) in infinite-order moments; this construction is explored in  and is termed Kernel DRO. When the distribution of interest is categorical, Kullback-Leibler divergence (KL) is often adopted to define the ambiguity set as the categories can be permuted and KL does not consider the geometry of a distribution's support. DRO using KL has been extensively studied in the literature due to its computational benefits [14; 15]. In contrast, the Wasserstein distance between two distributions captures the geometry of the distributions. The Wasserstein distance can be useful, e.g., in loan approval prediction problems where the distribution represents the incomes of a population and prior knowledge may specify that two clients are similar if their incomes are close. DRO using the Wasserstein distance (WDRO) has been applied to many machine learning problems, such as linear regression  and logistic regression  problems.

In this paper, we apply DRO to handle distribution mismatches using the Wasserstein distance as it can better capture the geometry of the distributions. However, WDRO is not suitable for contaminated datasets, e.g., datasets that contain outliers that are geometrically far from the clean distribution but cannot be easily removed. The reason is that the Wasserstein distance between the contaminated empirical distribution and the clean (true) distribution is very sensitive to the outliers; to include the clean distribution in the ambiguity set, the Wasserstein distance would need to be selected very large and inevitably the ambiguity set would include distributions that will never happen in practice.

To apply DRO to contaminated datasets, the authors in [18; 19; 20] take outliers into consideration and propose various methods to minimize their impact on the learning performance. Specifically,  proposes to minimize the most optimistic DRO risk to avoid the hardest instances that are likely to be outliers. In , a weight clipping method is proposed to truncate the excessive impact of outliers on the results. However, both  and  use KL divergence to define the ambiguity set, which fails to capture geometric uncertainty. Notably, the authors in  introduce an outlier-robust WDRO framework capable of addressing both geometric uncertainty and non-geometric contamination, allowing an \(\) fraction of data to be arbitrarily corrupted. They design an ambiguity set based on prior knowledge and demonstrate the strong duality of the resulting DRO problem, which is related to the Conditional Value at Risk (CVaR) . Nevertheless, the performance of the formulated DRO problem relies heavily on the accuracy of the estimate of \(\). Besides,  uses off-the-shelf solvers to solve the resulting DRO problem, which is not computationally efficient when applied to large datasets since the number of constraints in the proposed method grows linearly with the sample size.

The main contribution of this paper is to introduce a new framework for outlier-robust WDRO based on Unbalanced Optimal Transport (UOT), which is known for its inherent robustness to outliers and missing data [22; 23; 24; 25; 26]. Our approach involves the design of a UOT distance that substitutes hard constraints by a soft penalization term. This construction, combined with some prior knowledge on the learning task, enables the design of a new ambiguity set that includes distributions of interest and penalizes distributions that contain outliers. For the DRO problem with this new ambiguity set, we establish strong duality results under specific smoothness assumptions. However, the solution of the dual problem poses significant computational challenges. Motivated by , we explore a Lagrangian penalty variation of the problem, which can be viewed as a Lagrangian function of the original formulation with a fixed dual variable. We show that strong duality holds for this Lagrangian penalty problem under fewer assumptions compared to the original DRO problem. The reformulated problem employs an exponential function to reweight data points, effectively diminishing the influence of outlier data on the optimization process. We solve this problem by proposing a provable stochastic (sub)-gradient algorithm, which is computationally efficient. We provide empirical results that demonstrate that our approach not only enhances robustness to outliers but also improves computational efficiency for large-scale problems.

Our work is related to data reweighted optimization problems [28; 29; 30], which adjust the weights of individual data to manage outliers. However, these studies construct the reweighted optimization problems heuristically and cannot handle distribution uncertainty. In contrast, our problem formulation is able to handle distribution uncertainty via DRO, and our findings demonstrate that the reweighted problem constitutes the dual of a specific class of DRO problems, with the ambiguity set defined by the UOT distance.

The rest of the paper is organized as follows. Section 2 defines the UOT distance and the associated DRO problem. Section 3 presents the strong duality results for both the original DRO problem and its Lagrangian penalty variant. The main algorithm along with its convergence analysis is presented in Section 4. Section 5 demonstrates the effectiveness of our proposed framework in addressing outliers in both regression and classification tasks. Finally, we conclude the paper in Section 6. In the Appendix, we provide all proofs as well as some additional experiments.

## 2 Problem definition and preliminaries

The (1-)Wasserstein distance  between two distributions \(\) and \(}\) is defined as

\[(,})=_{(, })}_{(,)}[c(,)],\] (1)

where \((,})\) denotes the set of joint distributions such that \(_{1}=\) and \(_{2}=}\), where \(_{1}\) and \(_{2}\) denote the first and the second marginal distribution of \(\), respectively. The function \(c(,):[0,+)\) is lower semi-continuous and represents the cost of moving a point from \(\) to \(\), where the support set \(^{d}\) is assumed to be compact. The joint distribution \(\) specifies a transport plan for moving the distribution from \(\) to \(}\). The Wasserstein distance captures the underlying geometry in distributions through the cost function \(c(,)\), making it a popular choice in DRO problems; the WDRO problem is defined by

\[_{}_{_{}(})} _{}[l(,)],\] (2)

where \(_{}(}):=\{():(,})\}\) denotes the ambiguity set that contains all distributions within \(\)-distance from \(}\) and \( 0\) is a user-specified parameter. Here, we denote by \(()\) the space of all probability distributions supported on \(\).

The Wasserstein distance enforces equality constraints on the marginal distributions of \(\) to match \(\) and \(}\), thereby restricting the choice of \(\). If the empirical distribution \(}\) contains outliers, it becomes necessary to select a large radius \(\) for the ambiguity set to ensure the inclusion of the true distribution \(^{*}\) as the outliers tend to increase the Wasserstein distance between \(}\) and \(^{*}\). However, a large \(\) can lead to a DRO solution that is conservative since the larger the radius the larger the value of the inner supremum problem in (2). Though a large radius allows the inclusion of the distribution of interest, i.e., \(^{*}\), it also results in the inclusion of many distributions that are not of interests, e.g., distributions that are unlikely to occur in practice. As a result, the Wasserstein distance is not a desired distance choice when the empirical distribution contains outliers.

In Unbalanced Optimal Transport (UOT) problems , the joint distribution \(\) is not required to have fixed marginals in contrast to (1). Specifically, for any two arbitrary positive measures \(\) and \(}\), the UOT distance is defined by

\[(,})=_{ 0}_{( ,)}[c(,)]+D_{_{1}}(_{1}|)+D_{ _{2}}(_{2}|}),\] (3)

where \(_{1}\) and \(_{2}\) are marginals of \(\), and \(D_{_{1}}\) and \(D_{_{2}}\) are Csiszar divergences that measure discrepancies between positive measures, based on functions \(_{1}\) and \(_{2}\), respectively. The key distinction between UOT distance and Wasserstein (OT) distance lies in the constraints of the marginals. While the OT distance in (1) strictly enforces marginal distributions, i.e., \(_{1}=\) and \(_{2}=}\), the UOT in (3) relaxes the equality constraints by adding mismatch penalty functions \(D_{}\) in addition to the transport cost induced by \(c(,)\). This relaxation makes UOT robust to outliers since \(_{2}\) can represent a distribution that is different from the contaminated distribution \(}\).

Inspired by the definition of UOT, we consider the following unbalanced Wasserstein distance (divergence)

\[(||})=_{,( ,})}\{_{(,)}[c(, )]+ D_{KL}(}||})\},\] (4)where \(\) is a tuning parameter and \(}\) is an intermediate optimization variable that links \(\) and \(}\). Note that (4) is a special case of (3) by selecting \(_{1}=_{\{1\}}\) (i.e., \(_{1}(1)=0\) and \(\) otherwise) and \(_{2}(x)=x x-x+1\) (i.e., \(D_{_{2}^{2}}\) denotes the KL divergence). In addition, we restrict the marginals of the joint positive measure \(\) in (3) to be probability measures because the general positive measures are not of interest in the DRO problems we consider. The particular choice of \(\) functions is for technical reasons related to the duality analysis, as we will show in Section 3.

The optimal unbalanced Wasserstein transport under \((\|})\) can be interpreted as a two-step procedure: 1) \(}\) can be transported to any \(}\) by paying a small price measured by its distance to \(}\) in the KL sense (a non-geometric transport), and 2) \(}\) is then transported to \(\) by minimizing the geometry-aware transport cost. The non-geometric KL divergence is the key to allow the uncontaminated clean distribution \(^{*}\) to be close to the contaminated empirical distribution \(}\) as the KL divergence does not distinguish the geometric locations of the distribution supports.

We define the ambiguity set using the unbalanced Wasserstein distance by \(_{}(})=\{():(\|})\}\). Then, the DRO problem using the UOT distance (4) can be formulated as

\[_{}_{():(\|})}_{}[l(,)].\] (5)

The ambiguity set defined by the unbalanced Wasserstein distance enables us to include the clean distribution \(^{*}\), but it cannot remove distributions that are close to the contaminated empirical distribution \(}\) as these distributions are close to \(}\) both geometrically (Wasserstein) and non-geometrically (KL). As a result, these distributions, denoted by set \(}\), could also make the learned model conservative. To remove such distributions, we need some prior/domain knowledge. As discussed in [18; 20], it is impossible to design a model selection strategy using contaminated datasets without any prior knowledge. In Section 5, we also show that constructing the ambiguity set without prior knowledge may result in worse performance compared to the traditional DRO; see Table 5.

We assume that any distribution \(}}\) incurs significantly large costs when evaluated using a given function \(h\) compared to uncontaminated distributions, i.e., \(_{}[h()]_{}[h( )],\ }}\) and \(\ \{:(\|}) \}}\). The function \(h\) can be related to the moment constraint. For example, if we know the expectation estimate \(_{0}\) of the clean distribution, we can design \(h()=\|-_{0}\|\) and the function value of \(h()\) will be high if \(\) is an outlier. Similar choices are considered in .

Taking into account the prior knowledge, we consider minimizing the following (primal) function

\[=_{():\\ (\|})}_{ }[l(,)]-_{}[h()]:= _{():\\ (\|})}_{ }[L(,)],\] (6)

with \(L(,)=l(,)-h()\) and we assume that \(L(,)\) is continuous in \(\). For any distribution \(}\) that contains outliers, \(_{}[h()]\) is large and thus \(}\) is less likely to attain the supremum in (6).

In summary, we first employ the unbalanced Wasserstein distance to incorporate the distributions of interest. Then, we leverage prior knowledge to systematically exclude distributions that may contain outliers. This two-step approach ensures a more thorough consideration of potential distributions while effectively mitigating the impact of outliers on the learning problem.

## 3 Unbalanced distributionally robust optimization

Note that the primal problem in (6) is an infinite-dimensional optimization as the optimization variable represents probability distributions. In this section, we derive its dual problem and a variant of the dual problem that is computationally tractable.

### The DRO problem

**Assumption 1**.: _Every joint distribution \(\) on \(\) with the second marginal distribution \(}\) has a regular conditional distribution \(_{}\) given the value of the second marginal equals \(\)._This assumption ensures that the joint distribution \(\) can be written as \(d(,)=d_{}()d()\); see  for more details on regular conditional distributions. By the law of total expectation, we have \(_{}[L(,)]=_{}}_{_{}()}[L(,)]\).

We define \(f_{}(,):=_{}\{L(,)- c(, )\}\). We first derive the dual problem and show the weak duality in the following lemma.

**Lemma 1**.: _Let Assumption 1 hold and suppose that \(|f_{}(,)|<\) for \(}\)-almost every \(\). Define the dual problem (D) \(:=_{ 0}\{+_{ }}[((,)}{ })]\}\). Then, we have_

\[=_{():(|| )}_{}[L(,)]_{  0}\{+_{}}((,)}{} )\}=.\]

The proof is provided in Appendix 7.1. To show the strong duality, we need two additional assumptions to guarantee that the function \(f_{}(,)\) is well-behaved.

**Assumption 2**.: _The function \(L(,)\) be differentiable and concave in \(\) with respect to the norm \(\|\|\)._

**Assumption 3**.: _The function \(c:[0,)\) is differentiable and \(c(,_{0})\) is 1-strongly convex for each \(_{0}\)._

**Theorem 1**.: _Let Assumptions 1-3 hold and assume that \(|f_{}(,)|<\) for \(}\)-almost every \(\). Suppose that the optimal dual variable \(^{*}\) is strictly positive. Then, the strong duality holds, i.e.,_

\[_{():(||) }_{}[L(,)]=_{ 0}\{ +_{}} (f_{}(,)/())\}.\] (7)

The proof can be found in Appendix 7.2. For technical reasons, we show the strong duality when the optimal dual variable \(^{*}\) is strictly positive. In most applications, the optimal dual variable \(^{*}\) is in fact positive. When \(^{*}=0\), the inequality constraint becomes inactive and the radius of the ambiguity set becomes very large, a situation that is out of scope for most practical applications.

Although strong duality holds and the dual problem is a finite dimensional minimization problem, solving problem (7) is computationally challenging due to the complex dependency of the objective on \(\) and, in particular, the implicit dependency of \(f_{}\) on \(\). Recall that \(f_{}(,)=_{}\{L(,)- c(, )\}\); in general, we cannot obtain an analytical form of \(f_{}(,)\), i.e., a closed-form solution of the problem \(_{}\{L(,)- c(,)\}\). Additionally, the dual problem (7) is not convex in \(\) in general. The task becomes more involved when attempting to minimize over \(\) and \(\) jointly. To address these challenges, we shift focus from the constrained \(\)-robustness problem (7) and instead consider the corresponding Lagrangian penalty problem, which allows for a computational tractable formulation.

### The Lagrangian penalty problem

Motivated by , we consider the Lagrangian penalty reformulation of (7) as follows

\[_{()}\{_{}[L( ,)]-(||})\}.\] (8)

In what follows, we show that the strong duality also holds for the Lagrangian penalty problem (8).

**Theorem 2**.: _Let Assumption 1 hold and assume that \(|f_{}(,)|<\) for \(}\)-almost every \(\). Then, we have that_

\[_{()}_{}[L( ,)]-(||})}= _{}}(f_{}( ,)/()).\] (9)

The proof is provided in Appendix 7.3. Note that the strong duality of the Lagrangian penalty problem only requires Assumption 1 to hold in contrast to Theorem 1. In particular, it does not require the loss function \(L(,)\) to be differentiable, making it applicable to a broader class of problems. Besides, solving problem (8) is computationally tractable as the dual problem has a closed-form solution as in (9). In particular, \(\) is a parameter in (9) while it is an optimization (dual) variable in (7).

Since the logarithm function is monotonically increasing, minimization of (9) in \(\) is equivalent to the following form

\[_{}_{}[(_{ }\{l(,)-h()- c(,)\}/() )].\] (10)

**Comparison with standard WDRO.** It has been shown in  that the Lagrangian penalty problem using the Wasserstein metric \(W_{c}\) has the following strong duality result

\[_{}\{_{()}_{ }[l(,)]- W_{c}(,}) \}=_{}_{}}[ _{}\{l(,)- c(,)\}],\] (11)

where \(W_{c}\) is the Wasserstein distance metric associated with the cost function \(c(,)\). When \(\) in (11) is selected to be large, this formulation is close to the ERM problem, i.e., minimization of \(_{}[l(,)]\). Clearly, a distribution \(}\) containing outliers would significantly deteriorate the learning performance of minimizing \(_{}}[l(,)]\).

In contrast, when \(\) is large, the problem (10) is close to the minimization of \(_{}}[((l(,)-h())/( ))]=_{}}[(l(,)/())w()]\), where \(w()=(-h()/())\) acts as a re-weighting function. Recall that \(h()\) is a function designed based on prior knowledge and assumes high values at outlier points. At an outlier point, say \(\), \(h()\) would be significantly larger than at non-outlier points, resulting in a very small weight \(w()\). Consequently, outlier data with small weights exert less influence on the resulting model. Therefore, the formulation (10) is more robust to outliers compared to standard WDRO.

**Comparison with outlier-robust WDRO .** considers the Huber contamination model in which an \(\) fraction of data can be arbitrarily corrupted. In , prior knowledge on the mean of the clean distribution is needed. Specifically, the function \(h()\) that can identify outliers in  is selected as \(h()=_{1}\|-_{0}\|^{2}\), where \(_{0}\) represents the estimated mean of the clean distribution and \(_{1}>0\) is a tuning parameter. It can be verified that the corresponding Lagrangian formulation is

\[_{}_{1-,}} _{}l(,)-_{1}\|-_{0}\|^{ 2}- c(,)}\,,\] (12)

where \(_{1-}\) denotes the conditional value at risk (\(\)), interpreted as the expected value of the rightmost \((1-)\) percentile of outcomes1.

Similarly, when \(\) is very large, the formulation in (12) closely approximates the minimization of \(_{1-,}}[l(,)-_{1} \|-_{0}\|]\). At an outlier point, say \(\), the value \(l(,)-_{1}||-_{0}||\) is small due to the large distance \(||-_{0}||\). Consequently, such an outlier point generates a function value that falls on the left tail of the loss distribution and is thus effectively excluded by the CVaR operation. This method can also be viewed as a re-weighting method by assigning weight 0 to outlier points and weighting other points equally. However, this approach is highly sensitive to the choice of \(\), which is usually difficult to obtain a priori. In comparison, our method is a smoothed version of this \(\) method and is less sensitive to the prior knowledge of \(\). Besides, calculating the value of CVaR is computationally more demanding when the number of samples is large as we will demonstrate in Sections 4 and 5.

## 4 Algorithm design

In this section, we focus on solving the dual of the Lagrangian penalty problem. We assume that the empirical distribution \(}\) is constructed from \(n\) samples and written as \(}=_{i=1}^{n}_{_{i}}\) with \(_{i}\) the \(i\)-th sample. According to Theorem 2, the Lagrangian penalty problem is equivalent to the minimization of the following objective function

\[F()=_{i=1}^{n}(_{}L(,)-  c(,_{i})}/()).\] (13)To solve the problem (13), we employ the stochastic sub-gradient method, detailed in Algorithm 1. Specifically, at each time step \(t\), we sample \(_{t}\) from the empirical distribution \(}\) uniformly. We then find the \(\)-approximate maximizer \(z_{t}\) of the function \(L(_{t},)- c(,_{t})\). We require that the solution satisfies \((z_{t},X_{t}^{*})\), where \(X_{t}^{*}\) denotes the set of maximizers. This is solvable in many applications and can be achieved through, e.g., the (sub)-gradient method . Then, we perform the projected sub-gradient update. The convergence analysis of Algorithm 1 is presented in the following theorem. The proof can be found in Appendix 7.4.

**Theorem 3**.: _Suppose that \(\) is a convex and compact set with bounded diameter \(R\). Assume that the loss \(L(,)\) is convex in \(\) for each \(\). For all data points \(_{i}\) from the empirical distribution \(}\), assume that \(L(,)- c(,_{i})\) is concave, \(L(,)- c(,_{i}) B\) and \(|c(,_{i})-c(^{},_{i})| L_{c}\|-^{}\|\), for all \(\), \(\), \(^{}\). Assume that \(\|_{}L(,)-_{}L(,^{})\|  L_{}\|-^{}\|\), \(\|_{}L(,)\| B_{2}\), \(\|L(,)-L(,^{})\| L_{}\|-^{}\|\). Select \(=}\). Then, Algorithm 1 satisfies_

\[_{t=1}^{T}([F(_{t})]-F^{*})=O( }+),\] (14)

_where \(F^{*}\) represents the minimum value of the objective function (13)._

The accuracy parameter \(\) in the inner maximization problem has a fixed effect on the final optimization accuracy, independent of \(T\). The assumptions in Theorem 3 are common in the analysis of optimization problems involving the exponential function in the objective function, see, e.g., [19; 28; 29].

**Algorithmic comparison with outlier-robust WDRO .** Our proposed formulation can be solved by the stochastic sub-gradient method, which is computationally more efficient when the sample size is large. In contrast, the approach in  utilizes CVaR to account for outliers and employs off-the-shelf solvers, e.g., GUROBI , to obtain a solution. A notable limitation of the presence of CVaR in DRO problems is that the number of constraints scales linearly with the sample size. Consequently, for large datasets, this scaling significantly increases the number of constraints, leading to computational inefficiency, as shown in Section 5. Designing a stochastic gradient method for the CVaR-based formulation in  is possible, although challenging since the estimate of the CVaR gradient is usually biased. Moreover, our methodology does not rely on off-the-shelf commercial solvers and can be incorporated in machine learning packages seamlessly, which allows us to handle a wide range of loss functions via the stochastic sub-gradient method.

## 5 Experiments

We conduct experiments on linear regression, linear classification, and logistic regression problems. We use the stochastic sub-gradient method in Algorithm 1 to solve the Lagrangian penalty problem and the GUROBI  solver to solve all other DRO problems we use as benchmarks. All experiments were conducted on an Intel Core i7-1185G7 CPU (3.00GHz) using Python 3.8. Our method is referred to as UOT-DRO. A discussion on parameter selection is provided at the end of this section.

### Linear regression

We consider a linear regression problem with the loss \(l_{}(x,y)=|^{}x-y|\), where \((x,y)^{d}\) represents a generic data point and \(^{d}\) is the model we aim to train. We define \(=[^{},-1]^{}\), \(=[x^{},y]^{}\). Then, the loss function can be written as \(l_{}()=|^{}|\).

We generate a clean data distribution \(_{n}\) with \(n\) samples, which is uniform over \(\{X_{i},_{*}^{}X_{i}\}_{i=1}^{n}\), where \(X_{1},,X_{n}\) are i.i.d. from \(X(0,I_{d})\). Drawing a uniform random subset \(S[n]\) of size \( n\), the corrupted data distribution \(}_{n}\) is defined to be uniform over \(\{(C^{1\{i S\}}X_{i},(-C^{2})^{1\{i S\}}(_{* }^{}X_{i}+))\}_{i=1}^{n}\), where \(C>0\) is a corruption scaling coefficient and \(>0\) is a shift coefficient. The empirical probability distribution \(}_{n}\) can be written as \(}_{n}=_{i=1}^{n}_{_{i}}\) with \(_{i}\) the \(i\)-th sample.

We consider prior knowledge on the mean of the clean distribution, denoted as \(\), and design the function \(h()=_{2}\|-\|\). The value of \(\) is determined by a cheap preprocessing step, same as in . We consider the Lagrangian penalty problem which, based on Theorem 2, is given by

\[_{}_{}_{n}}[(_{ }\{|^{}|-\|-\|- _{2}\|-\|.\}/()) ].\] (15)

Define \(()=\{\|z\|_{*}:l^{*}(z)<\}=\|\|_{*}\), where \(\|\|_{*}\) denotes the dual norm. When \(_{2}+()\), we can show that the problem (15) is equivalent to

\[_{}_{i=1}^{N}((|^{}_{i }|-_{2}||-_{i}||)/()).\] (16)

The detailed proof is provided in Appendix 7.5.

We set the parameters as follows: \(=10\), \(=6\), \(_{2}=5\). All the results are averaged over 10 independent runs. We fix \(=0.1\) and \(C=8\). We compare the performance of our unbalanced DRO model against the standard DRO and outlier-robust WDRO provided in . We evaluate these methods in terms of the excess risk, which we define as the difference between the loss incurred by the learned model and the minimum achievable loss. The simulation results presented in Fig. 1 demonstrate the performance of our method under varying conditions: In Fig. 1 (a), we set \(d=10\) and compare these methods for various samples sizes \(n\{20,40,60,80,100\}\); In Fig. 1 (b), we fix \(n=100\) and explore the impact of various dimensions \(d\{5,10,20,30,40\}\). We observe that our method not only achieves superior robustness to outliers but is also less sensitive to data dimensions.

Besides, we compare the computational efficiency of these methods across varying sample sizes. As shown in Table 1, the OR-WDRO method is not applicable to large-scale datasets, with its running time exceeding 200 minutes for a sample size of \(n=20000\). In contrast, our method only requires 20 seconds to process the same dataset while maintaining superior learning performance. Therefore, our method not only achieves better robustness but also maintains computational efficiency.

Figure 1: Excess risk of standard DRO, OR-WDRO, and UOT-DRO with varied sample size and dimension for linear regression. The error bar denotes \(\) standard deviation.

### Linear classification

We consider a linear classification problem with the loss \(l_{}(x,y)=\{0,1-y(^{}x)\}\) with \(^{d}\). We consider the same outliers as that in . Specifically, we first generate a clean distribution \(_{n}\) by \(\{X_{i},(_{*}^{}X_{i})\}\), where \(X_{1},,X_{n}\) are drawn i.i.d. from \((0,l_{d})\). For a uniform random subset \(S[n]\) of size \( n\), we consider the corrupted distribution \(}_{n}\) which is uniform over \(\{((-C)^{1\{i S\}}X_{i}+e_{1},(_{ *}^{}X_{i}))\}_{i=1}^{n}\), where \(C>0\) is the contamination factor, \(e_{1}\) is the distribution shift. We select \(h()=_{2}\|-\|^{2}\) and \(c(,)=\|-\|^{2}\) in linear classification as in .

We consider the formulation described in Theorem 2 and solve it using Algorithm 1. We fix \(=0.1\), \(d=10\), \(n=100\) and select \(=10\), \(_{2}=1\), \(=2\). The experimental results are averaged over 10 trials. We evaluate the model performance for different choices of the contamination factor \(C\) by analyzing the excess risk and accuracy. Excess risk is defined as the difference between the returned loss and the best achievable loss. Accuracy refers to the rate of successful classification. As shown in Table 2, the performance of standard DRO decreases when \(C\) gets large. In contrast, the OR-WDRO performs well only when \(C\) is very large. This is because OR-WDRO uses CVaR to filter out outliers, which requires the outliers to be significantly distant from the regular data for CVaR to be effective. Nevertheless, our method performs well across the entire range of \(C\).

### Logistic regression

We consider a logistic regression with the loss function \(l_{}(x,y)=(1+(-y(^{}X)))\), where \(^{10}\). As in , we assume that the feature vector \(X\) follows a multivariate standard normal distribution, and the conditional distribution of the label \(y\{-1,1\}\) is given by \((y|x)=[1+(-y(_{*}^{}x))]^{-1}\), where \(_{*}=(10,0,,0)\). This setup uniquely determines the true distribution \(\). We draw \(n\) samples from this distribution to create the empirical distribution \(_{n}=_{i=1}^{n}_{(X_{i},y_{i})}\). Outliers are considered to occur only in the feature space. The perturbed distribution is represented as \(\{(-C)^{\{i S\}}X_{i}+ e_{1}\}_{i=1}^{n}\), where \(S[n]\) is a uniform random subset of size \( n\). We fix \(=0.1\), \(=0.1\), \(e_{1}=(1,0,,0)\), and select \(=10\), \(_{2}=1\), \(=2\). In Fig. 2, we compare the excess risk and accuracy of standard DRO as outlined in  and our proposed method, described by Theorem 2 and implemented by Algorithm 1. The results are averaged over \(10\) runs for different sample sizes \(n\{20,40,60,80,100\}\). Besides, we conduct a comparative analysis of the excess risk and accuracy between the standard DRO and the proposed method across various contamination levels. As shown in Table 3, standard DRO is susceptible to significant outliers. Instead, the proposed method demonstrates robustness throughout the entire range of contamination factor \(C\), consistently maintaining an accuracy of approximately \(92\%\).

    &  &  &  \\   & Time & Excess risk & Time & Excess risk & Time & Excess risk \\ 
80 & 0.1 & 3.230 & 0.4 & 0.103 & 2.7 & 0.060 \\
200 & 0.2 & 2.298 & 1.3 & 0.064 & 3.4 & 0.040 \\
2000 & 3.3 & 0.441 & 29.8 & 0.050 & 4.7 & 0.038 \\
5000 & 9.2 & 0.371 & 259.5 & 0.040 & 7.7 & 0.034 \\
10000 & 28.9 & 0.352 & 1438.7 & 0.033 & 11.9 & 0.033 \\
20000 & 110.8 & 0.380 & * & * & 22.2 & 0.031 \\   

Table 1: Comparison of running time and excess risk of different methods for linear regression. The symbol ‘*’ indicates that running time is over 12000 seconds.

   Contamin. \(C\) &  &  &  \\   & Excess risk & Accuracy & Excess risk & Accuracy & Excess risk & Accuracy \\ 
6 & 0.628 & 73\% & 0.627 & 72\% & 0.298 & 92\% \\
10 & 0.722 & 66\% & 0.560 & 79\% & 0.295 & 93\% \\
30 & 0.637 & 68\% & 0.341 & 90\% & 0.241 & 96\% \\
100 & 0.872 & 56\% & 0.191 & 97\% & 0.240 & 95\% \\   

Table 2: Excess risk with various contamination for linear classification.

### Parameter selection

We provide some guidelines for selecting the parameters \(,_{2},\) in Algorithm 1. The parameter \(\) is commonly used in the DRO literature  as a penalty coefficient. If the value of \(\) is large, then the DRO problem approaches the empirical risk minimization problem, resulting in a model that performs well on the empirical distribution but is less robust to Wasserstein perturbations.

The parameter \(_{2}\) represents the credibility level assigned to the function \(h\). A larger value of \(_{2}\) should be selected when the confidence in the reliability of \(h\) is high, meaning that \(h()\) is highly likely to become large at outlier points. Conversely, if the prior knowledge provided by \(h\) is considered unreliable, the value of \(_{2}\) should be reduced. If there is no reliable prior knowledge at all, in which case we should select \(_{2}=0\), achieving good performance is impossible, as shown in related literature in robust statistics .

The parameter \(\) penalizes the mismatch between marginal distributions. A larger value of \(\) places more emphasis on minimizing this mismatch, possibly at the expense of increasing the transportation cost, thereby making the unbalanced optimal transport distance close to the balanced one. Conversely, a small value of \(\) allows for larger mismatches between the marginal distributions, which can enhance the model's robustness to outliers. However, when \(\) is very small, possible distribution mismatches incur little penalty, and the computed distance may fail to accurately represent the true distance between the distributions. In this case, the resulting DRO problem will incur many unlikely distributions in the ambiguity set, leading to a very conservative model.

## 6 Conclusion

In this work, we introduced a novel DRO framework that employs a new distance derived from UOT. By incorporating a soft penalization term in the design of the ambiguity set, our method exhibits increased resiliency to outliers. We provided strong duality results for the original DRO problem and the Lagrangian penalty problem, with the latter allowing for more efficient computation via stochastic sub-gradient methods. Finally, empirical results validate our method's enhanced robustness to outliers and reduced computational demands for regression and classification tasks.

    &  &  \\   & Loss & Accuracy & Loss & Accuracy \\ 
4 & 0.976 & 73\% & 0.390 & 93\% \\
8 & 1.249 & 64\% & 0.469 & 93\% \\
16 & 1.502 & 56\% & 0.481 & 92\% \\
30 & 1.056 & 62\% & 0.474 & 92\% \\   

Table 3: Loss and accuracy with various contamination for logistic regression.

Figure 2: Excess risk and accuracy of standard DRO and UOT-DRO with varied sample sizes for logistic regression. The error bar denotes \(\) standard deviation.