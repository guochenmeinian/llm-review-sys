# Representational Strengths and Limitations of Transformers

Clayton Sanford, Daniel Hsu

Department of Computer Science

Columbia University

New York, NY 10027

{clayton,djhsu}@cs.columbia.edu

&Matus Telgarsky

Courant Institute

New York University

New York, NY 10012

matus.telgarsky@nyu.edu

###### Abstract

Attention layers, as commonly used in transformers, form the backbone of modern deep learning, yet there is no mathematical description of their benefits and deficiencies as compared with other architectures. In this work we establish both positive and negative results on the representation power of attention layers, with a focus on intrinsic complexity parameters such as width, depth, and embedding dimension. On the positive side, we present a _sparse averaging task_, where recurrent networks and feedforward networks all have complexity scaling polynomially in the input size, whereas transformers scale merely _logarithmically_ in the input size; furthermore, we use the same construction to show the necessity and role of a large embedding dimension in a transformer. On the negative side, we present a _triple detection_ task, where attention layers in turn have complexity scaling linearly in the input size; as this scenario seems rare in practice, we also present natural variants that can be efficiently solved by attention layers. The proof techniques emphasize the value of communication complexity in the analysis of transformers and related models, and the role of sparse averaging as a prototypical attention task, which even finds use in the analysis of triple detection.

## 1 Introduction

In recent years, transformer networks (Vaswani et al., 2017) have been established as a fundamental neural architecture powering state-of-the-art results in many applications, including language modeling (OpenAI, 2023), computer vision (Dosovitskiy et al., 2021), and protein folding (Jumper et al., 2021). The key building block of transformer models is the _self-attention unit_, a primitive that represents interactions among input elements as inner-products between low-dimensional embeddings of these elements.

The success of transformer models is linked to their ability to scale their training and generalization performance to larger datasets and sequence lengths. Their representational capacity, however, underlies this scaling power, and is tied to the inductive biases of their learning algorithms. Empirically, transformer models trained with gradient-based learning algorithms exhibit biases towards certain algorithmic primitives (Edelman et al., 2022; Liu et al., 2022) and learn representations that may encode domain-specific information in the self-attention units (Clark et al., 2019; Hewitt and Manning, 2019; Rogers et al., 2020; Chen et al., 2022). These examples indicate that transformer architectures not only provide computational benefits, but also have representational capabilities that are particularly well-matched to practical tasks.

In this paper, we investigate these inductive biases by identifying "natural" computational tasks for which transformers are well-suited, especially compared to other neural network architectures, as well as tasks that highlight the limitations of transformers. The tasks--sparse averaging, pair-matching,and triples-matching--represent primitive operations that aggregate structural information encoded in embeddings. We use these tasks to elucidate the relationship between the embedding dimension \(m\) of a self-attention unit and its expressivity, and to showcase the fundamental representational limitations of self-attention layers.

In our model, the primary computational bottleneck faced by a transformer in computing a "sequence-to-sequence"1 function \(f^{N}^{N}\) is the constrained processing of pairs of input elements \(\{x_{i},x_{j}\}}{2}\); we allow transformers unbounded computational power when processing the individual elements \(x_{i}\). This is motivated by modern scaling regimes where the context length \(N\) has rapidly increased, the self-attention embedding dimension \(m\) remains much smaller than \(N\), and the parameterization of multi-layer perceptrons (MLPs) that operate on individual elements is much larger than \(m\). Indeed, the largest GPT-3 model (Brown et al., 2020) features a context length \(N=2048\), an embedding dimension \(m=128\), and MLPs with a 12288-dimensional parameterization; the context length of GPT-4 is as large as \(N=32000\). As such, we are interested in the capabilities of transformers with \(N^{o(1)}\) total "size", as opposed to \(N^{(1)}\). The nature of the bottleneck in our model makes the tools of communication complexity indispensable for formalizing computational limits.

### Our contributions

Sparse averaging separations among atomic self-attention units.The \(q\)_-sparse averaging task_\(q\) aims to capture the essential approximation-theoretic properties of self-attention units. In \(q\), the \(i\)th input \(x_{i}\) is a pair \((y_{i},z_{i})\), where \(z_{i}^{d^{}}\) is the _data_ part of \(x_{i}\), simply a vector in \(^{d^{}}\), whereas and \(y_{i}\) is the _indexing_ part, which specifies \(q\) locations in the input sequence; the \(i\)th output element in \(q\) is obtained by averaging the \(q\)_data_ parts \(z_{j}\) given by \(j y_{i}\), meaning

\[q((y_{1},z_{1}),,(y_{N},z_{N}))=( _{j y_{1}}z_{j},,_{j y_{N}}z_{j}).\]

(See also Definition 4.) As summarized in the following informal theorem, our analysis of \(q\) in Section 3 and Appendix A illustrates the ability of the self-attention primitive to associate arbitrary subsets of input elements (as opposed to just "local" subsets, as specified by some sequential/topological structure), measures the expressive power accrued by increasing the embedding dimension \(m\) of a self-attention unit, and indicates the representational limitations of "traditional" neural architectures on basic computational tasks.

**Informal Theorem 1**.: _The task \(q\) for \(q_{+}\) satisfies the following properties (see Definition 4 for a formal definition and approximation metric)._

1. _There exists a unit of self-attention_ \(f\) _with an_ \(m\)_-dimensional embedding that approximates_ \(q\) _if and only if_ \(m q\) _(Theorems 2 and 4)._
2. _Any fully-connected neural network whose output approximates_ \(q\) _requires its first hidden layer to have width at least_ \((Nd)\) _(Theorem_ 10_)._
3. _Any recurrent neural network whose iterates approximate_ \(q\) _requires a hidden state of at least_ \((N)\) _bits (Theorem_ 11_)._

We consider the \(q\) implementation in Item 1_efficient_ since the dimension of the model parameters grows with \((q,d, N)\), whereas the latter two are _inefficient_ since their parameter (or state) dimension grows as \((N)\). The proofs of the positive results employ embeddings for each index \(j\) and each subset \(y_{i}\) that have large inner products if and only if \(j y_{i}\). The negative results involve communication complexity reductions and geometric arguments. These arguments naturally introduce a dependence on bits of precision, which we suppress above within the notation "\(\)"; we note that these bounded-precision results are arguably more relevant to modern networks, which uses as few as \(4\) or even \(2\) bits of numerical precision.

Contrast between pairwise and triple-wise matching with self-attention layers.We frame standard transformer architectures as being able to efficiently represent functions that are decomposable into sparse pairwise interactions between inputs. To do so, we introduce two sequential tasks and prove a collection of constructions and hardness results that characterize the abilities of transformers to solve these tasks.

Given an input sequence \(X=(x_{1},,x_{N})[M]^{N}\) (for some \(M=(N)\)), we formalize the problems of _similar pair detection_ (\(\)) and _similar triple detection_ (\(\)) as

\[(X)_{i[N]} =\{ j\;\;x_{i}+x_{j}=0\;( \;M)\},\] (1) \[(X)_{i[N]} =\{ j_{1},j_{2}\;\;x_{i}+x_{ j_{1}}+x_{j_{2}}=0\;(\;M)\}.\] (2)

For both tasks, note that the output is an \(N\)-dimensional vector whose \(i\)th element is 1 if and only if the sequence \(X\) includes a pair or triple _containing_\(x_{i}\). In this sense, the problems differ from 2SUM and 3SUM, which are not sequence-to-sequence tasks.

We believe these two tasks are intrinsically "pairwise" and "triple-wise", respectively; moreover, since we also believe self-attention performs a fundamentally "pairwise" operation, we will use \(\) and \(\) to show a sharp gap in the representation power of self-attention.

**Informal Theorem 2**.:
1. _A single unit of standard self-attention with input and output MLPs and an_ \(O(d)\)_-dimensional embedding can compute_ \(\) _(Theorem_ 6_)._
2. _A single layer of standard multi-headed self-attention cannot compute_ \(\) _unless its number of heads_ \(H\) _or embedding dimension_ \(m\) _grows polynomially in_ \(N\) _(Theorem_ 7_)._
3. _A standard transformer model_ can _efficiently compute a modified version of_ \(\) _that makes assumptions about embedding structure or locality (Theorems 8 and 9)._
4. _Under a generalized notion of "third-order tensor self-attention" introduced in Appendix_ C.3_,_ \(\) _is efficiently computable with a single unit of third-order attention (Theorem_ 18_)._

While the above result demonstrates the limitations of multi-headed self-attention and illustrates the importance of learning embeddings with contextual clues, we believe that a stronger result exists. Specifically, we conjecture that even multi-layer transformers are unable to efficiently compute \(\) without hints or augmentation.

**Informal Conjecture 1**.: _Every multi-layer transformer that computes \(\) must have width, depth, embedding dimension, or bit complexity at least \(N^{(1)}\)._

In Appendices C.5 and C.6, we give a heuristic information-theoretic argument to support this conjecture, prove a matching upper-bound, and finally prove analogous results for graph-augmented transformers with respect to the problem of cycle detection in directed and undirected graphs.

### Related work

Several computational and learning-theoretic aspects of transformers, distinct from but related to the specific aims of the present paper, have been mathematically studied in previous works.

Universality and Turing-completeness.To demonstrate the power of transformers, universal approximation results for transformers (Yun et al., 2020; Wei et al., 2022)--analogous to results for feedforward networks (Hornik et al., 1989)--establish the capability for sufficiently large networks to accurately approximate general classes of functions. Note, however, that the precise minimal dependence of the required size (e.g., number of attention units, depth of the network) as a function of the input size \(N\) does not directly follow from such results, and it is complicated by the interleaving of other neural network elements between attention layers. (Approximate) Turing-completeness of transformers demonstrates their power in a different manner, and such results have been established, first assuming infinite precision weights (Perez et al., 2019) and later also with finite-precision (Wei et al., 2022). Such results are more closely aligned with our aims, because Turing machines represent a uniform model of computation on inputs of arbitrary size. Wei et al. (2022) showed that Turing machines that run for \(T\) steps can be approximated by "encoder-decoder" transformers of depth \((T)\) and size polynomial in \((T)\) and the number of states of the Turing machine (but the decoder runs for \(T\) steps).

Formal language recognition.The ubiquity of transformers in natural language understanding has motivated the theoretical study of their ability to recognize formal languages. On the positive side, Bhatamishra et al. (2020) constructed transformers that recognize counter languages, and Yao et al. (2021) showed that transformers of bounded size and depth can recognize Dyck languages that have bounded stack depth. Liu et al. (2022) showed that the computations of finite-state automata on sequences of length \(N\) can be performed by transformers of depth \((N)\) and size polynomial in the number of states. On the negative side, Hahn (2020) showed limitations of modeling distributions over formal languages (including Dyck) with fixed-size transformers (though this result does not imply quantitative lower bounds on the size of the transformer). Hahn (2020), as well as Hao et al. (2022), also establish the inability of "hard attention" Transformers to recognize various formal languages and circuit classes by leveraging depth reduction techniques from circuit complexity (Furst et al., 1984).

Learnability.The sample complexity of learning with low-weight transformers can be obtained using techniques from statistical learning theory and, in turn, establish learnability of certain boolean concept classes (e.g., sparse parity) (Edelman et al., 2022; Bhatamishra et al., 2022) using transformer-based hypothesis classes. Our \(q\) function is inspired by these classes, and we establish concrete size lower bounds for approximation (and hence also learnability) by transformers. We note that our constructions use bounded-size weights, and hence, in principle, the aforementioned sample complexity results can be combined with our results to analyze empirical risk minimization for learning transformers. Prior work of Likhosherstov et al. (2021) also shows how sparse attention patterns can be achieved by self-attention units (via random projection arguments); however, when specialized to \(q\), their construction is suboptimal in terms of the sparsity level \(q\).

Related models.Graph neural networks (GNNs), like transformers, process very large inputs (graphs) using neural networks that act only on small collections of the input parts (vertex neighborhoods). Many classes of GNNs are universal approximators for classes of invariant and equivariant functions (Maron et al., 2019; Keriven and Peyre, 2019). At the same time, they are restricted by the distinguishing power of certain graph isomorphism tests (Xu et al., 2018; Morris et al., 2019; Chen et al., 2019), and lower bounds have been established on the network size to approximate such tests (Aamand et al., 2022). Loukas (2019) established a connection between GNNs and the Local(Angluin, 1980) and Congest(Peleg, 2000) models for distributed computation, and hence directly translates lower bounds for Congest--notably cycle detection problems--into size lower bounds for GNNs. Our lower bounds for cycle detection using transformers also leverage a connection to the Congest model. However, transformers do not have the same limitations as GNNs, since the computational substrate of a transformer does not depend on the input graph in the way it is with GNNs. Thus, we cannot directly import lower bounds for Congest to obtain lower bounds for transformers.

Transformers are also related to other families of invariant and equivariant networks. Our focus on \(\) and \(\) (and related problems) was inspired by the separation results of Zweig and Bruna (2022) between models for processing sets: Deep Sets (Qi et al., 2017; Zaheer et al., 2017), which are "singleton symmetric", and the more expressive Relational Pooling networks (Santoro et al., 2017), which are only "pairwise symmetric".

### Conclusion and future work

Our primary contributions are to present a multi-faceted story about transformer approximation: firstly, \(q\) separates transformer models approximation-theoretically from RNNs and MLPs, and moreover the attention embedding dimension both necessary and sufficient for \(q\) scale directly with \(q\), meaning \(q\) also functions to characterize representation power amongst different transformers. Secondly, while single units of self-attention can solve the \(\) task, even wide layers of self-attention with high-dimensional embeddings cannot solve \(\), and we believe that deeper models cannot as well. This question of deeper models is stated as a formal conjecture and addressed heuristically in Appendix C.6, using both information- and communication-theoretic proof techniques, both of which we feel are significant steps towards a complete proof.

While our investigation is purely approximation-theoretic, we also include in Appendix D a preliminary empirical study, showing that attention can learn \(q\) with vastly fewer samples than recurrent networks and MLPs; we feel this further emphasizes the fundamental value of \(q\), and constitutes an exciting direction for future work.

Beyond the explicit open question in Informal Conjecture 1, we anticipate that future research could connect the separation results proved in this work to formal linguistic theory and empirical work on attention matrix interpretation. This work examines \(\) and \(\) because we believe that the former could represent a key primitive for language processing tasks such as co-referencing, while the latter represents a natural extension of the former that likely is _not_ necessary for language modeling. Rather, it may be possible that language modeling performs triple-wise modeling for tasks such as the identification of subject, verb, and object components by relying on pairwise matching constructions and "clues" learned within an embedding, such as those encoded in the toy problems \(\) and \(\). That is, transformers serve as a useful foundational model for language modeling because of their abilities to integrate contextual clues and pairwise communication, and while they are not extensible to "purely triple-wise problems," most practical sequential problems have some efficient decomposition to pairwise structures that can be easily exploited by these architectures. Future work by linguists, theoretical computer scientists, and empirical NLP practitioners could assess how foundational our primitives are and study whether there are any practical triple-wise problems that transformer models fail to solve.

## 2 Preliminaries

Let \(^{d}=\{x^{d}:\|x\|_{2} 1\}\) denote the unit ball in \(^{d}\), and let \([n]=\{1,2,,n\}\) denote the first \(n\) positive integers. The expression \(\{P\}\) equals \(1\) if predicate \(P\) is true and \(0\) otherwise. The row-wise softmax operator applied to matrix \(A^{N M}\) returns

\[(A)_{i,j}=)}{_{j^{}=1}^{M}(A_{ i,j^{}})}.\]

### Attention units and transformer architectures

We first introduce the concept of self-attention, which is used as the building block of all transformer architectures included in this paper.

**Definition 1**.: For input dimension \(d\), output dimension \(d^{}\), embedding dimension \(m\), precision \(p\), and matrices \(Q,K^{d m}\) and \(V^{d d^{}}\) (encoded using \(p\)-bit fixed-point numbers), a _self-attention unit_ is a function \(f_{Q,K,V}:^{N d}^{N d}\) with

\[f_{Q,K,V}(X)=(XQK^{}X^{})XV.\]

Let \(_{d,m,d^{},p}=\{f_{Q,K,V}:Q,K,V\}\) denote all such self-attention units.

Self-attention units can be computed in parallel to create multi-headed attention.

**Definition 2**.: For head-count \(H\) and self-attention units \(f_{1},,f_{H}_{d,m,d^{},p}\), \(\)_multi-headed attention layer_ is a function \(L_{f_{1},,f_{H}}:^{N d}^{N m}\) with \(L_{f_{1},,f_{H}}(X)=_{h=1}^{H}f_{h}(X)\). Let \(_{d,m,d^{},p}^{H}\) contain all such \(L_{f_{1},,f_{H}}\).

Transformer models are composed of two components: multi-headed attention layers (as above) and element-wise multi-layer perceptrons. Due to universal approximation results, we model multi-layer perceptrons as arbitrary functions mapping fixed-precision vectors to themselves.

**Definition 3**.: A _multi-layer perceptron (MLP) layer_ is represented by some \(:^{d}^{d^{}}\), whose real-valued inputs and outputs can be represented using \(p\)-bit fixed-precision numbers. We apply \(\) to each element (i.e., row) of an input \(X^{N d}\), abusing notation to let \((X)=((x_{1}),,(x_{N}))^{N d^{}}\). Let \(_{d,d^{},p}\) denote all such MLPs.

We concatenate the notation of each class of functions to denote function composition. For example, for output dimension \(d^{}\), we use \(_{d,m,d^{},p}^{}:=_{m,m,d^{},p}_ {d,m,p}\) and \(_{d,m,d^{},p}^{H}:=_{m,m,d^{},p}^{H}_{d, m,p}\) to represent single-headed and multi-headed attention units with an input MLP respectively. (The capabilities and limitations of these models are studied in Section 3.) For depth \(D\), we let

\[_{d,m,d^{},p}^{D,H}=_{m,d^{},p}(_{m,m,m,p}^{H})^{D-1}_{d,m,m,p}^{H}\]represent a full transformer model comprising \(D\) layers of \(H\)-headed self-attention with interspersed MLPs.

While two key features of transformer architectures--the residual connection and the positional embedding--are conspicuously missing from this formalism, the two can be implemented easily under the framework. We can include a positional embedding by encoding the index as a coordinate of the input, i.e. \(x_{i,1}=i\). Then, the subsequent MLP transformation \((X)\) can incorporate \(i\) suitably into the embedding. A residual connection can be included additively as input to a multi-layer perceptron layer (as is standard) by implementing an "approximate identity" attention head \(f\) with \(Q,K\) and \(V=I_{m}\) set to ensure that \(f(X) X\).2

We periodically consider transformers implemented with real-valued arithmetic with infinite bit complexity; in those cases, we omit the bit complexity \(p\) from the notation.

Finally, we assume for the proof of Theorem 3 that the model is permitted to append a single <END> token at the end of a sequence. That is, we say that a model \(f^{D,H}_{d,m,d^{},p}\) represents a target \(h:^{N d}^{N d^{}}\) if \(f(X^{})_{1:N}=g(X)\) when \(X^{}=(x_{1},,x_{N},x^{})\) for constant-valued \(x^{}^{d}\).

## 3 Sparse averaging with attention units

We present the sparse averaging task to highlight the ability of transformer architectures to simulate a wide range of meaningful interactions between input elements. This task demonstrates how the embedding dimension of a self-attention unit modulates the expressive capabilities of the architecture, while showcasing the inabilities of fully-connected and recurrent neural networks to capture similar interactions (see Appendix A).

**Definition 4**.: For sparsity \(q\), problem dimension \(d^{}\), and input dimension \(d=d^{}+q+1\), consider an input \(X=(x_{1},,x_{N})^{N d}\) with \(x_{i}=(z_{i};y_{i};i)\) for \(z_{i}^{d^{}}\) and \(y_{i}\).3 Let the \(q\)_-sparse average_ be

\[q(X)=(_{j=1}^{q}z_{y_{i,j}})_{i[N]}.\]

For accuracy \(>0\), a function \(f:^{N d}^{N d^{}}\)\(\)_-approximates_\(q\) if for all \(X\),

\[_{i[N]}\|f(X)_{i}-q(X)_{i}\|_{2}.\]

Figure 0(a) visualizes the sparse averaging task as a bipartite graph between subsets \(y_{i}\) and elements \(z_{i}\) with corresponding averages. Theorems 2 and 4 jointly show that the minimum embedding dimension \(m\) of single self-attention units \(^{}_{d,m,d^{},p}\) that \(O()\)-approximate \(q\) scales linearly with \(q\). We believe that the sparse averaging problem is thus a canonical problem establishing the representational capabilities and inductive biases of self-attention units.

### Self-attention can approximate \(q\) when \(m q\)

Our principle positive result shows that the sparse averaging task \(q\) can be approximately solved using fixed-precision arithmetic self-attention units with embedding dimension \(m\) growing with \(q N\).

**Theorem 2** (Fixed-precision).: _For any \(N\), any \(m(d^{}+q N)\), any \((0,1)\), and \(p=(( N))\), there exists some \(f^{}_{d,m,d^{},p}\) that \(\)-approximates \(q\)._

While the full proof appears in Appendix B.1, we briefly sketch the argument here. Because the output of a self-attention unit is a convex combination of rows of the value matrix \((X)V^{N d^{}}\), a natural way to approximate \(q\) with a unit of self-attention is to let each value be the corresponding vector in the average (i.e. \(V^{}(x_{i})=z_{i}\)) and choose the key and query functions in order to ensure that the attention matrix satisfies

\[((X)QK^{}(X)^{})_{i,j} &j y_{i}\\ 0&\]

To do so, let each key \(K^{}(x_{i})\) represent a fixed vertex on a convex polytope, which depends only on index \(i\) and is constructed from random binary vectors. We select each query \(Q^{}(x_{i})\) to ensure that \((x_{i})^{}QK^{}(x_{j})\) is a fixed large value if \(j y_{i}\) and a slightly smaller value otherwise. We obtain the precise query, key, and value embeddings by employing tools from dual certificate analysis from the theory of compressed sensing.

We visualize this construction in Figure 0(b) and 0(c) for \(q=3\) and \(d^{}=4\), which presents the associated attention and value matrices necessary for the construction, and plots a polytope of keys (red dots) with each face corresponding to each subset \(y_{i}\) (green dots). The construction is empirically relevant; Figure 2 shows that a unit of self-attention trained on data generated by the \(q\) task recovers a similar attention matrix to the one stipulated in our construction and visualized in Figure 0(b).

The logarithmic dependence of the embedding dimension \(m\) on the sequence length \(N\) can be eliminated by considering self-attention units with real-valued arithmetic with infinite bit complexity.

**Theorem 3** (Infinite-precision).: _For fixed \(N\), \(m(d^{}+q)\) and \(>0\), there exists some \(f^{}_{d,m,d^{}}\) that \(\)-approximates \(q\)._

Figure 1: A visualization of the \(q\) function outputs given a sequence of inputs \((z_{i};y_{i};i)_{i[N]}\) as a bipartite graph between subsets \(y_{i}\) and vectors \(z_{i}\) (a), and of the attention matrix (b) and underlying embeddings (c) that produce the self-attention construction in Theorem 2.

Figure 2: Attention matrix \(((X)QK^{}(X)^{})^{20 20}\) for a fixed example after \(T\) epochs of training a self-attention unit to solve \(q\) for \(q=3\). Each row \(i\) corresponds to subset \(y_{i}\), and each cell \(j y_{i}\) is outlined in red. See Appendix D for experimental details.

The proof of Theorem 3 employs a similar polytope-based construction in Appendix B.2, relying on a cyclic polytope rather than one drawn from discrete boolean vectors. Theorem 16 proves the near-optimality of _that_ bound by employing a geometric argument to show that a variant of \(q\) can only be approximated by a restricted family of self-attention units with a sufficiently high-dimensional embedding.

### Self-attention cannot approximate \(q\) when \(m q\)

We show that the construction used to prove Theorem 2 is nearly optimal.

**Theorem 4**.: _For any sufficiently large \(q\), any \(N 2q+1\), and any \(d^{} 1\), there exists a universal constant \(c\) such that if \(mp cq\), then no \(f_{d,m,d^{},p}^{1,1}\) exists that \(\)-approximates \(q\)._

(By choosing \(p=O((q N))\), Theorem 2 is shown to be optimal up to logarithmic factors of \(q\) and doubly-logarithmic factors of \(N\).)

The proof of Theorem 4 employs a standard communication complexity argument based on a reduction from the following _set disjointness_ problem in the two-party communication model, in which each party possesses a subset of an \(n\) element domain (encoded as \(n\)-bit strings), and they wish to jointly determine whether their subsets are disjoint. We note that communication complexity is commonly-used technique for proving lower bounds on the representational power of circuits and feedforward neural networks (see, e.g., Karchmer and Wigderson, 1988; Ben-David et al., 2002; Martens et al., 2013; Vardi et al., 2021).

**Fact 5** (Set disjointness communication lower bound (Yao, 1979)).: _Suppose Alice and Bob are given inputs \(a,b\{0,1\}^{n}\), respectively, with the goal of jointly computing \((a,b)=_{i}a_{i}b_{i}\) by alternately sending a single bit message to the other party over a sequence of communication rounds. Any deterministic protocol for computing \((a,b)\) requires at least \(n\) rounds of communication._

Our proof designs a communication protocol that Alice and Bob use to jointly compute \((a,b)\) when \(n=q\) in \(O(mp)\) rounds of communication, under the assumption that such an \(f\) exists that closely approximates \(q\).

* Alice encodes her input \(a\) in a single subset by letting \(y_{2q+1}=\{2i+a_{i}-1:i[q]\}\).
* Bob uses his input \(b\) to assign \(z_{2i-1}\) to \(2b_{i}-1\) and \(z_{2i}=-1\) for all \(i[q]\).
* All other input components are set to constant values known by both parties.

Alice sends her \(mp\)-bit query embedding \(Q^{}(x_{2q+1})\) bit-by-bit to Bob, who approximately computes \(q\) by determining the outcome of \(f\). The crux of the reduction shows that \(q(X)_{2q+1}=-1\) if and only if \(a_{i}b_{i}=0\) for all \(i[q]\), which allows Bob to determine \((a,b)\).

We visualize the protocol in Figure 3 and give the proof in Appendix B.3. The proofs of Theorems 7, 11, 21, and 23 employ similar communication complexity reductions to \(\).

Figure 3: The \(mp\)-bit communication protocol used to reduce the hardness of computing \(q\) with a single unit of self-attention to the hardness of solving the \(\) communication problem for the proof of Theorem 4 for \(q=4\).

Standard transformer models can only efficiently represent intrinsically pairwise functions

In this section, we argue that the standard transformer architecture is unable to efficiently represent functions that do not decompose into a small number of pairwise-symmetric functions. We do this by contrasting the (in)approximability of intrinsically pairwise and triple-wise functions, respectively \(\) and \(\) (defined in (1) and (2)), and their variants.

### Efficient computation of \(\) with standard self-attention

We first show that \(\) can be efficiently approximated by a single standard (pairwise) self-attention unit.

**Theorem 6**.: _For any input size \(N\), input range \(M=N^{O(1)}\), and fixed-precision bit complexity \(p=O( M)\), there exists a transformer architecture \(f_{1,1}^{1,1}\) with a single self-attention unit with embedding dimension \(m=3\) such that for all \(X[M]^{N}\), \(f(X)=(X)\)._

The proof, given in Appendix C.1 uses both a "blank token" and a trigonometric positional embedding, which ensures that

\[(x_{i})^{}QK^{}(x_{j})=c_{k=1}^{d}( +x_{j,k})}{M})\]

for some sufficiently large constant \(c\). This embedding ensures that a cell of the attention matrix \(((X)QK^{}(X)^{})_{i,j}\) is extremely close to zero, unless \(x_{i}=-x_{j}\).

### Hardness of computing \(\) with a multi-headed self-attention layer

Although \(\) can be efficiently represented using a single unit of standard self-attention, representing \(\) using an entire layer of multi-headed attention units is impossible unless either the number of heads \(H\), the embedding dimension \(m\), or the precision \(p\) grows as \(N^{(1)}\).

**Theorem 7**.: _There is universal constant \(c>0\) such that for sufficiently large \(N\), and any \(M N+1\), if \(mpH cN/ N\), then there is no \(f_{1,m,1,p}^{1,H}\) satisfying \(f(X)=(X)\) for all \(X[M]^{N}\)._

We give the proof in Appendix C.2. Like that of Theorem 4, the proof relies on a reduction from set disjointness in two-party communication. The proof of the lower bound applies a domain-restricted variant of \(\), which actually makes the problem substantially simpler to solve. In Remark 1, we show how this variant of \(\) introduces a _depth separation_ between the representational powers of single-layer and two-layer transformer models.

As mentioned in the introduction, we also conjecture that multiple layers of multi-headed attention are subject to the same impossibility (Conjecture 19). The impossibility is specific to standard (pairwise) attention; in Appendix C.4, we show that \(\)_can_ be efficiently computed with a single unit of _third-order_ self-attention.

### More efficient constructions for simplified \(\) computations

While the previous sections suggests that no efficient construction exists to compute \(\) with standard transformer models, practical examples of triple detection abound. For example, a transformer-based language model will likely succeed in linking a subject/verb/object triple because all three tokens likely inhabit the same local region and because the model could agglomerate the triple by first identifying a pair and then adding the third. Here, we introduce two variants on the \(\) problem that have additional structure to serve as hints. The first variant specifies triple sums comprising the input element and a neighboring pair elsewhere in the sequence: for each \(i[N]\),

\[(X)_{i}=\{ j\;\;x_{i}+ x_{j}+x_{j+1}=0\;(\;M)\}.\]

The second focuses on localized sums, where are all components of a triple must be within a fixed range of constant width \(K N\): for each \(i[N]\),

\[(X)_{i}=\{ j_{1},j_{2}\;\;x_{i}+x_{j_{1}}+x_{j_{2}}=0\;(\;M),|i-j_{1}|,|i-j_{2} | K\}.\]

We show that the two can be efficiently represented using compact standard transformer models.

**Theorem 8**.: _For any \(N\), \(M=N^{O(1)}\), and \(p=O( M)\), there exists a transformer architecture \(f^{1,1}_{1,m,1,p}\) with embedding dimension \(m=3\) and depth \(D=2\) such that for all \(X[M]^{N d}\), \(f(X)=(X)\)._

Informally, the first layer of the construction uses a sinusoidal positional encoding to compute each bigram sum \(x_{j}+x_{j+1}\) in the \(j\)th element of the sequence. The second layer applies the \(\) construction provided by Theorem 6 to determine whether there exists a \(j\) for each \(i\) such that \(x_{i}+x_{j}+x_{j+1}=0\).

**Theorem 9**.: _For any \(d\), \(N\), \(M=N^{O(1)}\), \(p=O( M)\), and \(K N\), there exists a transformer architecture \(f^{1,1}_{1,m,1,p}\) with embedding dimension \(m=O(K N)\) and bit-complexity \(p=O((K N))\) such that for all \(X[M]^{N d}\), \(f(X)=(X)\)._

Proof.: We implement the localized construction by using Theorem 2 to construct a specific sparse simultaneous average of the inputs with \(q:=2K+1\) and \(d^{}:=2K+1\). To do so, we use the input MLP to convert \(x_{i}\) to the embedding \((z_{i};y_{i};i)\), for zero-padded input

\[z_{i}=x_{i}e_{}^{2K+1}\]

for \(=i\) and subset

\[y_{i}=\{i-K,i-K+1,,i+K\}.\]

This construction ensures that the \(i\)th element of self-attention output computes (a rotation of) \((x_{i-K},x_{i-K+1},,x_{i+K})\). An output MLP can then verify whether any matching triples involving \(x_{i}\) exist among those vectors.