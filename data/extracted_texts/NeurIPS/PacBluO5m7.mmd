# KnowGPT: Knowledge Graph based PrompTing for Large Language Models

Qinggang Zhang1, Junnan Dong1, Hao Chen2, Daochen Zha3, Zailiang Yu4, Xiao Huang2

2The Hong Kong Polytechnic University, 3Rice University, 4Zhejiang Lab

{qinggangg.zhang,hanson.dong}@connect.polyu.hk, sundaychenhao@gmail.com

daochen.zha@rice.edu, yuzl@zhejianglab.com,xiaohuang@comp.polyu.edu.hk

Equal contribution

###### Abstract

Large Language Models (LLMs) have demonstrated remarkable capabilities in many real-world applications. Nonetheless, LLMs are often criticized for their tendency to produce hallucinations, wherein the models fabricate incorrect statements on tasks beyond their knowledge and perception. To alleviate this issue, graph retrieval-augmented generation (GraphRAG) has been extensively explored which leverages the factual knowledge in knowledge graphs (KGs) to ground the LLM's responses in established facts and principles. However, most state-of-the-art LLMs are closed-source, making it challenging to develop a prompting framework that can efficiently and effectively integrate KGs into LLMs with hard prompts only. Generally, it usually suffers from three critical issues, including huge search space, high API costs, and laborious prompt engineering, that impede the widespread application in practice. To this end, we introduce a novel **Know**ledge **G**raph-based **P**rompTing framework, namely **KnowGPT**, to enhance LLMs with domain knowledge. **KnowGPT** contains a knowledge extraction module to extract the most informative knowledge from KGs, and a context-aware prompt construction module to automatically convert extracted knowledge into effective prompts. Experiments on three benchmark datasets demonstrate that **Know**GPT** significantly outperforms all competitors including the state-of-the-art GraphRAG models. Notably, **KnowGPT** achieves a \(92.6\%\) accuracy on **O**penbook**QA** leaderboard, close to human-level performance.

## 1 Introduction

Large Language Models (LLMs), such as GPT-4  and Claude 3 , have surprised the world with superior performance in a wide range of real-world applications . Despite their impressive performance, LLMs are frequently criticized for their limited ability to handle factual information accurately and their tendency to generate hallucinations , especially when faced with questions requiring domain-specific or professional knowledge not covered in their training corpus . For example, when queried about nutrient composition, an LLM might erroneously associate it with "energy", as depicted in Figure 1. This error stems from the model's insufficient biological knowledge. Therefore, integrating domain knowledge into LLMs is crucial for reducing hallucinations and unlocking their full potential in diverse industry applications .

Recently, retrieval-augmented generation (RAG) has been explored, which can enhance LLMs with external knowledge from text corpora or online sources . It combines LLMs with external knowledge retrieval systems to help reduce hallucinations. However, these models face challenges in real-world applications due to the varying quality of available data. Domain knowledge is often scattered across different sources, such as textbooks, research papers, technical manuals, and industryreports . These textual documents may have varying levels of quality, accuracy, and completeness, leading to potential inconsistencies or errors in the retrieved knowledge .

A promising avenue for addressing the above issue entails the integration of Knowledge Graphs (KGs) into LLMs. KGs provide a structured representation of domain knowledge, as they are constructed based on rigid ontologies that clearly define the jargon, acronyms, specialized terminologies and their relationships in specific domains . The enormous factual knowledge stored in KGs holds the potential to ground the model's responses in established facts and principles . For instance, in Figure 1, an LLM can correct itself by leveraging the related background knowledge in ConceptNet . Earlier studies adopted a heuristic way to inject knowledge from KGs into the LLMs during pre-training or fine-tuning. For example, ERNIE  incorporates entity embeddings and aligns them with word embeddings in the pre-training phase, encouraging the model in the intermediate layers of LLMs .

However, as LLMs have been keeping evolving, most SOTA LLMs remain _closed-source_ in practice. For instance, GPT-4  and Claude 3  exclusively grant access through their APIs, which means we can only retrieve model responses by submitting textual inputs, with model specifics inaccessible. As such, the research focus has recently shifted towards KG prompting that enhances fixed LLMs with KG-based hard prompts . KG Prompting for LLMs has been a new learning paradigm in natural language processing. Specifically, CoK  introduces Chain-of-Knowledge prompting to decompose LLM-generated reasoning chains into evidence triples, verifying their factuality and faithfulness using an external KG. Mindmap  provides more transparency on LLMs' decision-making by enabling comprehension and reasoning over structured KG inputs. RoG  presents a planning-retrieval-reasoning framework that synergizes LLMs and KGs for more transparent and interpretable reasoning. KGR  proposes an autonomous approach to retrofit LLM-generated responses by leveraging KGs to extract, verify, and refine factual information throughout the reasoning process, effectively mitigating hallucination for knowledge-intensive applications.

Despite the promising performance of existing KG prompting methods, three critical issues hinder their widespread application in practice. \(\) Huge search space. Real-world KGs often consist of millions of triples, resulting in a vast search space when retrieving relevant knowledge for prompting. \(\) High API cost. Closed-source LLMs, like GPT-4 and Claude 3, are accessible through proprietary APIs, which can incur significant costs when performing KG prompting at scale . Thus, careful selection of the most informative knowledge from KGs is essential to minimize costs. \(\) Laborious prompt design. LLMs are highly sensitive to prompts, with even minor variations in prompts conveying the same semantic meaning potentially yielding drastically different responses. However, existing methods rely on manually designed or rule-based prompts to present factual knowledge from KGs. These hard prompts are inherently inflexible and rigid, lacking the adaptability to accommodate variations in question semantics and KG structures.

To this end, we propose a novel **Know**ledge **G**raph-based **P**romp**T**ing framework, namely **Know**G**PT, which leverages the factual knowledge in KGs to ground the model's responses in established facts and principles. In this paper, we aim to answer two key research questions. \(\) Given a query and a large-scale KG, how could we effectively and efficiently retrieve factual knowledge from KG that is

Figure 1: A real-world question from OpenbookQA. GPT-3.5 could effectively correct the answer given the scientific reasoning background from ConceptNet (blue: question concepts, red: answers, grey: entities not present in questions).

relevant to the query? Given the raw knowledge extracted from KGs, how could we convert the extracted knowledge into an effective prompt that is easily understandable for LLM?

We shed light on the the above questions with a novel prompt learning method that is rather effective, generalizable, and cost-efficient. Specifically, to address question, we leverage deep reinforcement learning (RL) to extract the most informative knowledge from KGs. To encourage the agent to discover more informative knowledge chains, we devise a tailored reward scheme that promotes the reachability, context-relatedness, and conciseness of the extracted paths. Then, a policy network is trained to maximize the reward using training questions and applied to unseen questions. To tackle question, we introduce a prompt construction strategy based on Multi-Armed Bandit (MAB). Given several knowledge extraction strategies and prompt templates, an MAB is learned to select the most effective combination for each question by balancing exploration and exploitation. The learned MAB is then applied to new questions to select knowledge extraction strategies and prompt templates automatically. Our main contributions are summarized as follows:

* Formally define the problem of KG-based prompting, which leverages the structured knowledge in KGs to ground the LLM's responses in established facts and principles.
* Propose, a novel prompting framework that leverages deep reinforcement learning (RL) and Multi-Armed Bandit (MAB) to generate effective prompts for domain-specific queries.
* Implement  upon GPT-3.5. Experiments on three QA datasets shows  outperforms SOTA baseline models by a large margin. Notably, achieves an average improvement of 23.7% over GPT-3.5 and an average improvement of 2.9% over GPT-4. Additionally, it attains a 92.6% accuracy on the OpenbookQA leaderboard, which is comparable to human performance.

## 2 Problem Statement

We formally define the problem of _knowledge graph based prompting for LLMs_ in question answering. We represent each question as a question context, where is a set of _source entities_, and is a set of _n target entities_. Following prior work [20; 79], is extracted by concept recognition, and we assume it is given in our problem. Similarly, each target entity in is extracted from a corresponding candidate answer. We denote an LLM as, a real-world KG as, which consists of triples (head entity, relation, tail entity), denoted as. In our setting, we only have access to the APIs of. However, we can employ open-source lightweight language models (not ), like Bert-Base , to initialize question embeddings. Using the above notations, we describe our problem below.

Given a question, an LLM, and a domain KG, we aim to learn a prompting function, which generates a prompt **x** that incorporates the context of and the factual knowledge in, such that the prediction of the LLM can output the correct answers for.

## 3 KnowGPT Framework

Learning the prompting function involves two challenges, i.e., what knowledge should be used in, and how to construct the prompt. To address these challenges, we present,

Figure 2: The overall architecture of our proposed knowledge graph prompting framework, i.e.,. Given the question context with multiple choices, we first retrieve a question-specific subgraph from the real-world KG. _Knowledge Extraction_ is first dedicated to searching for the most informative and concise reasoning background subject to the context. Then the _Prompt Construction_ module is optimized to prioritize the combination of knowledge and formats subject to the given question.

which extracts raw knowledge with deep RL and then constructs the prompt with MAB. An overview of our framework is shown in Figure 2.

### Knowledge Extraction with Deep Reinforcement Learning

Intuitively, the relevant reasoning background lies in a question-specific subgraph \(_{}\) that contains all the _source_ entities \(_{s}\), _target_ entities \(_{t}\), and their neighbors. An ideal subgraph \(_{}\) is expected to have the following properties: \((i)\)\(_{}\) encompasses as many source and target entities as possible, \((ii)\) the entities and relations within \(_{}\) exhibit a strong relevance to question context, and \((iii)\)\(_{}\) is concise with little redundant information such that it can be fed into LLMs with limited lengths.

However, it is challenging to find such a \(_{}\) since extracting a subgraph is NP-hard. To effectively and efficiently find a satisfactory \(_{}\), we develop a tailored knowledge extraction method, named \(_{}\), that employs deep RL to sample reasoning chains in a trial-and-error fashion. Specifically, we assume \(_{}\) is constructed based on a set of reasoning chains \(=\{_{1},_{2},...,_{m}\}\), where each knowledge chain \(_{i}=\{(e_{i},r_{1},t_{1}),(t_{1},r_{2},t_{2}),...,(t_{| _{i}|-1},r_{|_{i}|},t_{|_{i}|})\}\) is a path in \(\) starting from the \(i\)-\(th\) source entity in \(_{s}\), and \(|_{i}|\) is the path length. \(_{}\) encompasses all the entities and relations appeared in \(\).

* **State:** A state indicates the current location in KG, i.e., one of the entities in KG. Specifically, it represents the spatial change from entity \(h\) to \(t\). Inspired by the prior study , we define the state vector \(\) as: \[_{t}=(_{t},_{target}-_{t}),\] (1) where \(_{t}\) and \(_{target}\) are the embedding vectors of the current entity and the target entity. To get the initial node embeddings for entities extracted from the background KG, we adopt the approach proposed by the previous study . Specifically, we transform knowledge triples from the KG into sentences and feed them into pre-trained LM to get node embeddings.
* **Action:** The action space encompasses all the neighboring entities of the current entity, enabling the agent to explore the KG flexibly. By taking an action, the agent will move from the current entity to the chosen neighboring entity.
* **Transition:** The transition model P measures the probability of moving to a new state (\(s^{}\)) given existing state (\(s\)) and the undertaken action (\(a\)). In KGs, the transition model takes on the form \((s^{}|s,a)=1\) if \(s\) is directed to \(s^{}\) through action \(a\); Otherwise, \((s^{}|s,a)=0\).
* **Reward:** To determine the quality of the formed path, we define the reward based on reachability: \[r_{reach}=+1,&target;\\ -1,&,\] (2) which represents whether the path eventually reaches the target within limited steps. Specifically, the agent receives a reward of \(+1\) if it can attain the target within \(K\) actions. Otherwise, it will receive \(-1\) as the reward.

Reaching a target entity is not our sole focus. To avoid overlap and rigmarole reasoning chains, we also design two auxiliary rewards to promote context-relatedness and path conciseness.

#### 3.1.1 Context-relatedness Auxiliary Reward

The key motivation is to encourage paths closely related to the given question context. Specifically, we evaluate the semantic relevance of a path \(_{i}\) to the context \(\). Inspired by the prevailing study , a fixed but well-trained matrix \(\) is applied to map the path embedding \(}\) to the same semantic space with context embedding \(\). To this end, this auxiliary reward is formulated as:

\[r_{}=_{source}^{i}cos(}_{i},),\] (3)

where \(\) is the embedding of context \(\) we obtained from a pre-trained LM  and the embedding of path \(_{i}\) is the average of the embeddings of all the entities and relations we have walked through till \(i\), i.e., \(Avg(_{source}+_{1}...+_{i})\), where \(i length(_{target})\). This step-by-step reward scheme provides rewards before the target is reached.

#### 3.1.2 Conciseness Auxiliary Reward

There are two additional significant challenges for the candidate reasoning background. \((i)\) The natural limitation of LLMs for over-long context understanding gives constrained budgets for prompts, where the extracted knowledge chain is expected to be concise enough to ensure the full understanding by closed-source LLMs. \((ii)\) The prohibitive cost of calling LLMs' API guides the prompt to be more concise. By limiting the step size, we encourage the policy to find as much valuable information as possible within the shortest path length.

Considering the inevitable homogeneity in the large-scale real-world KG constructed from the online corpus, each step in the final path is ideally a necessity. Specifically, we evaluate the conciseness of a path to reduce twists and turns on redundant entities, e.g., synonyms. Thus, the reward for the conciseness of a path \(_{i}\) is formulated as follows.

\[r_{}=_{i}|}.\] (4)

To this end, our overall reward modeling consists of three major criteria that comprehensively incentivize the entire policy learning for an effective knowledge extraction.

#### 3.1.3 Training Policy Network

To solve the MDP defined above, a tailored policy network \(_{}(s,a)=p(a|s;)\) is trained to extract a reasoning chain in the KG. We optimize the network with policy gradient . The optimal policy navigates the agent from the source entity to the target entity while maximizing the accumulated rewards. We provide more training details in the Appendix.

### Prompt Construction with Multi-armed Bandit

In this subsection, we design a tailored prompt construction strategy based on Multi-Armed Bandit (MAB). The key idea is to learn to select the best knowledge extraction and prompt templates at a meta-level. We will begin by outlining the overall strategy, followed by detailing its instantiation with two knowledge extraction methodologies and three templates.

Suppose we have several knowledge extraction strategies \(\{_{1},_{2},...,_{m}\}\) and several candidate prompt formats \(=\{_{1},_{2},...,_{n}\}\). Each knowledge extraction strategy \(_{i}\) is a method for selecting reasoning background given a question context, such as the RL-based strategy discussed above. Every prompt template \(_{j}\) represents a mechanism to transform the triples within the subgraph into a prompt for an LLM prediction.

The prompt construction problem is to identify the best combination of \(\) and \(\) for a given question. We define the overall process of selection as a reward maximization problem, \( r_{pf}\), where \(r_{pf}\) is obtained as:

\[(f(_{(i)}))=1&accurate;\\ 0&.\] (5)

Specifically, \(_{(i)}\), \(i\{0,1,,m n\}\) is one of the combination, and \(r_{pf}\{0,1\}\) indicates the performance of the output of LLM in answering the current question.

To capture the context-aware correlation between questions and different combinations of knowledge and prompt formats, we formulate the selection mechanism of MAB with an expectation function \(E()\). It adaptively measures the potential expectation of a combination for different questions.

\[E(|_{(i)})=_{(i)}+_{(i)}.\] (6)

Here, \(\) represents the embedding of \(\). The vector \((i)\) corresponds to a set of non-negative parameters associated with \((i)\), which have been learned during the previous \(k\)-\(1\) iterations. Additionally, \(_{(i)}\) stands for a balancing factor introducing noise according to a Gaussian distribution.

Empirically maximizing \(_{i}\) could encourage exploitation [12; 16] for the best combination, we could effectively update \(_{(i)}\) via modeling the correlations between the context embedding of the anchor question \(_{i}\) and all the previously selected contexts \(_{(i)}\) for particular combination\(_{(i)}\) in former \(k\) steps, and the rewards \(_{pf}^{(i)}\) obtained from the selection of the current combination. Concretely, the \(^{(b)}\) is updated as:

\[& J(_{(i)}^{(k)},_{pf}^{(i)(k)} )=_{k=1}^{K}(_{pf}^{(i)(k)}-_{(i)}^{(k)}^{(i )})^{2}+^{i}^{(i)}_{2}^{2}\\ &^{(i)}=((_{(i)}^{(k)})^{ }_{(i)}^{(k)}+^{i})^{-1}(_{(i) }^{(k)})^{}_{pf}^{(i)(k)}.\] (7)

Here, \(J\) denotes the OLS training loss. \(^{d d}\) is an identity matrix and \(^{i}\) is a regularization factor that controls the complexity of the model.

Similarly, in order to encourage exploration within less frequently selected pairings, we employ an upper confidence bound approach to balance exploration and exploitation. This is achieved through the introduction of the parameter \(^{(i)}\). Inspired by prevailing studies [70; 16], we can derive the following exploration term \(^{(i)}\):

\[^{(i)}=_{i}((_{(i)}^{(k )})^{}_{(i)}^{(k)}+^{i})^{-1}(_ {(i)})^{}},\] (8)

where \(\) is a fixed constant, i.e., \(=1+\).

When the model picks a combination with a large \(_{i}\), it signifies an exploitation process. Likewise, when the model selects a combination with larger \(^{(i)}\), this variance indicates an exploration process due to the model making fewer selections of the current combination. Thus, jointly maximizing \(_{i}+_{(i)}\) could help us get rid of the dilemma of exploration and exploitation.

Consequently, our MAB design can leverage the feedback from the LLM to optimize the selection policy. By maximizing the expectation function \(E()\), it learns to balance the exploitation and exploration to prioritize the most promising prompts for specific question contexts.

#### 3.2.1 Implementation

We implement the above MAB strategies with two knowledge extraction strategies and three templates. Note that our MAB design is general and can be implemented with more knowledge extraction strategies and prompt templates for better performance. The knowledge extraction strategies include:

* \(_{}\): The RL-based knowledge extraction strategy presented in the previous subsection.
* \(_{}\): A heuristic sub-graph extraction strategy that extracts a 2-hop subgraph around both the source and target entities. Detailed implementation can be found in Section B.1 of Appendix. Since RL is notoriously unstable , we introduce \(_{}\) as an alternative candidate strategy for the MAB selection, ensuring a fallback option if the RL-based approach does not perform well.

The prompt templates include:

* **Triples**, denoted as \(_{t}\), are indeed the originally extracted knowledge and empirically tested that could be understood by the black-box LLMs, e.g., (_Sergey_Brin_, _founder_of_, _Google_),(_Sundar_Pichai_, _ceo_of_, _Google_), (_Google_, _is_a_, _High-tech Company_).
* **Sentences** is a following solution to transform the knowledge into a colloquial \(_{s}\), e.g., '_Sergey Brin, who is a founder of Google, a high-tech company, has now passed the reigns to Sundar Pichai, who is currently serving as the CEO of the company._'
* **Graph Description**, \(_{g}\) prompts the LLM by treating the knowledge as a structured graph. We preprocess the extracted knowledge with black-box LLM itself to generate the description by highlighting the center entity, e.g., '_Google, a high-tech company, stands central in the network. The entity is strongly associated with significant individuals in the tech industry. Sergey Brin, one of the founders, established Google, underscoring its historical beginnings. In the present graph context, Sundar Pichai is recognized as the CEO of Google, symbolizing the company's current leadership. Thus, Google serves as a vital link between these key figures._

Considering two knowledge extraction methods: \(_{}\) and \(_{}\), as well as three prompt translation methods: \(_{t}\), \(_{s}\) and \(_{g}\), the MAB is trained to learn from the feedback from LLMsto prioritize the most appropriate combination among two extraction methods and three pre-defined prompt formats for different real-world question contexts, i.e., \(=\{(_{sub}_{t}),\)\((_{sub}_{s}),(_{sub}_{g}),(_{RL} _{t}),(_{RL}_{s}),(_{RL}_ {g})\}\).

## 4 Experiments

We conduct extensive experiments to evaluate \(\) on three benchmark question-answering datasets, covering both commonsense and domain-specific QA. We implement \(\) upon GPT-3.5. Our experiments are designed to answer the following research questions:

* **RQ1 (Main results)**: How does \(\) perform when compared with the state-of-the-art LLMs and KG-enhanced QA baselines?
* **RQ2 (Ablation Study)**: How does each key component of \(\) contribute to the performance?
* **RQ3 (Case study)**: How could KG help solve complex reasoning tasks? See Appendix 4.4.

### Experimental Setup

**Datasets**. We evaluate \(\) on three QA datasets spanning two fields: CommonsenseQA  and OpenBookQA  serve as benchmarks for commonsense reasoning, while MedQA-USMLE  acts as a domain-specific QA benchmark. The statistics of these three datasets can be found in Table 5 in the Appendix.

**Baselines**. We carefully select baseline models from four categories for a comprehensive evaluation.

_LM + Fine-tuning_. We compare our method with vanilla fine-tuned LMs. Specifically, we choose Bert-base, Bert-large , and RoBerta-large  as representative fine-tune LM methods. To conduct commonsense and biomedical QA, we fine-tune these three LMs via additional linear layers.

_KG-enhanced LM_. We have also implemented several recently released models for integrating KGs into question answering, which encompass MHGRN , QA-GNN , HamQA , JointLK , GreaseLM  and GrapeQA . To ensure a fair comparison, we implement these baselines with advanced language models that are optimized for particular datasets. Specifically, RoBerta-large  is used for CommenseQA, while AristoRoBERTa  is designated for OpenBookQA. For MedQA, we opt for the top-tier biomedical language model, SapBERT . Note that due to the white-box nature of these methods and their high computation overheads, it is infeasible to apply them to state-of-the-art LLMs, like GPT-3.5 and GPT-4.

_LLM + Zero-shot_. We include several representative generative LLMs, including ChatGLM, ChatGLM2, Baichuan-7B, InternLM, GPT-3, GPT-3.5 and GPT-4 as knowledge-agnostic alternatives. Specifically, we used the model 'text-davinci-002' provided by OpenAI as the implementation of GPT-3, and 'gpt-3.5-turbo' and 'gpt-4' as the implementations of GPT-3.5 and GPT-4, respectively (we have provided more implementation details of all LLMs in Appendix A.4). The question-answering task is conducted under the zero-shot setting with the question query from the test set as input.

_LLM + KG Prompting_. To verify the effectiveness of our prompting strategy, we also add the state-of-the-art KG prompting methods, i.e., CoK , RoG , and Mindmap  as baselines. Notably, we did include KGR  in our main results, since the authors have not released their codes.

### Main Results (RQ1)

To address **RQ1**, we evaluate \(\) by comparing it to state-of-the-art baselines on the three benchmark datasets. \(\) is based on the original GPT-3.5. We measure the performance using accuracy, which calculates the percentage of questions correctly predicted by the model out of the total questions in the test set. We have the following observations:

* \(\) outperforms all categories of methods, including sixteen different baselines, across all datasets and model architectures. This suggests that \(\) can effectively inject the knowledge from KGs to LLMs.
* \(\) surpasses the performance of GPT-3.5 and even GPT-4. On average, \(\) achieves a 23.7% higher testing accuracy than GPT-3.5. Specifically, \(\) outperforms GPT-3.5 by 10.8%,

[MISSING_PAGE_EMPTY:8]

### Ablation Studies (RQ2)

To answer **RQ2**, we conduct two ablation studies. **First**, in Table 3, we measure the importance of the tailored reinforcement learning-based knowledge extraction module, i.e., \(_{}\). Specifically, we compare it with the heuristic sub-graph extraction strategy, i.e., \(_{}\). The performance is evaluated by directly feeding the extracted knowledge with the prompt format of 'Sentence', i.e., \(_{s}\), to GPT-3.5. We also include 'w/o KG' as the baseline where GPT-3.5 is asked to independently answer the given question with no reasoning background provided. The results clearly indicate the vital role of our proposed knowledge extraction strategies. **Second**, we compare each of the three prompt formats subject to the same extracted knowledge. The detailed results are shown in Table 4. Though different formats perform similarly within the difference of 2.2% - 3.3%, they are particularly suitable for different kinds of questions. We illustrate this observation in the following case study section. Both ablation studies support the indispensability of each module, armed with a tailored deep reinforcement learning-based knowledge extraction and a context-aware prompt translation, our Know6PT performs best on all three benchmark datasets.

### Case Studies (RQ3)

For **RQ3**, we provide insights into how Know6PT facilitates the prompt translation with a real case from CommonsenseQA. We visualize both the extracted knowledge and the textual inputs to GPT-3.5 in Figure 3. In this example, given the same extracted knowledge, GPT-3.5 answers correctly based on the sentence format that we provide. In contrast, it fails to answer the question with triples and graph descriptions. They clearly indicate the superiority of Know6PT in an automatic context-aware prompt translation. We make the following observations: \((i)\) Triple format \(_{t}\) is intuitively suitable for all the simple questions by directly indicating the one-hop knowledge. \((ii)\) Graph description may inevitably introduce noise to ensure the completeness and contextual fluency of the directed graph. In this example, since 'vacation' appears in both question and answer choices, over-emphasizing and connecting the knowledge about 'vacation' with other concepts in the graph misleads the model to make a prediction with an oblique focus. \((iii)\) Our Know6PT has shown superior performance in automatically constructing suitable prompts for particular questions.

## 5 Limitation

Through our exploration, we realize the natural limitations of KnowGPT brought by real-world KGs. Existing KGs are automatically constructed based on online corpora. This inevitably introduces a considerable number of noisy triples into KGs. The noisy knowledge may mislead the LLMs

    &  & **CSOA** & **OBOA** & **MedQA** \\  & & Blike & Direct & Text & Text \\  \)} & \(_{s}\) & 0.728 & 0.701 & 0.832 & 0.589 \\  & \(_{s}\) & 0.750 & 0.739 & 0.865 & 0.695 \\  & \(_{s}\) & 0.737 & 0.715 & 0.867 & 0.680 \\  }\)} & \(_{s}\) & 0.782 & 0.769 & 0.853 & 0.739 \\  & \(_{s}\) & 0.815 & 0.900 & 0.889 & 0.755 \\   & \(_{s}\) & 0.806 & 0.793 & 0.986 & 0.762 \\   & **0.827** & **0.818** & **0.924** & **0.781** \\    & & & & \\ 

Table 4: Ablation study on different prompt formats for the extracted knowledge.

    \\  w/o KG & 0.778 & UnifiedQA  & 0.872 \\ MHGRN  & 0.806 & DRAGON  & 0.878 \\ QA-GNN  & 0.828 & GenMC  & 0.898 \\   & 0.848 & Human Performance & 0.917 \\ HamQA  & 0.850 & GenMC Ensemble  & 0.920 \\ JointLK  & 0.856 & X-Reasoner  & 0.952 \\   GSC  & 0.874 & Know6PT & **0.926** \\   

Table 2: OpenBookQA Official Leaderboard records of three groups of related models.

    &  & **CSOA** & **OBOA** & **MedQA** \\  & & Blike & Direct & Text & Text \\   & GPT-3.5 & 0.539 & 0.520 & 0.842 & 0.289 \\  & GPT-3.5 & 0.735 & 0.710 & 0.798 & 0.487 \\  & GPT-4.7 & 0.76 & 0.780 & 0.910 & 0.763 \\  }\)} & GPT-3.5 & 0.750 & 0.739 & 0.865 & 0.695 \\  & GPT-3.5 & 0.815 & 0.800 & 0.889 & 0.755 \\  Ours & Know6PT & **0.827** & **0.818** & **0.924** & **0.781** \\   

Table 3: Ablation study on the effectiveness of two knowledge extraction methods.

to wrong predictions despite the effectiveness of our prompting methods. In the future, we would leverage KnowGPT with off-the-shelf KG refinement algorithms to improve the quality of KGs.

## 6 Conclusion

The main objective of this paper is to tackle the hallucination issue that arises when applying LLM to specific domains. Although LLMs have strong reasoning capabilities, they still struggle to answer professional questions in specific domains, especially when the pre-training corpus lacks relevant knowledge. To address this issue, we propose a KG-augmented LLM model, named KnowGPT, which injects relevant domain knowledge from KGs into LLMs to assist the LLM in accurately answering professional questions. A novel framework, namely KnowGPT, is presented to integrate KGs into LLMs effectively with model APIs only. We first train a deep RL policy to extract informative and concise reasoning background from the KG. Then we learn an MAB to select the most effective knowledge extraction method and prompt template for each question. Extensive experiments on both general and domain-specific QA show superior performance of KnowGPT compared to all competitors.