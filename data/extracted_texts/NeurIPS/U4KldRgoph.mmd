# Enhancing Graph Transformers with Hierarchical Distance Structural Encoding

Yuankai Luo\({}^{1,3}\)  Hongkang Li\({}^{2}\)  Lei Shi\({}^{1}\)  Xiao-Ming Wu\({}^{3}\)

\({}^{1}\)Beihang University \({}^{2}\)Rensselaer Polytechnic Institute

\({}^{3}\)The Hong Kong Polytechnic University

luoyk@buaa.edu.cn xiao-ming.wu@polyu.edu.hk

Corresponding authors.

###### Abstract

Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current methods often fall short in capturing longer ranges, hierarchical structures, or community structures, which are common in various graphs such as molecules, social networks, and citation networks. This paper presents a Hierarchical Distance Structural Encoding (HDSE) method to model node distances in a graph, focusing on its multi-level, hierarchical nature. We introduce a novel framework to seamlessly integrate HDSE into the attention mechanism of existing graph transformers, allowing for simultaneous application with other positional encodings. To apply graph transformers with HDSE to large-scale graphs, we further propose a high-level HDSE that effectively biases the linear transformers towards graph hierarchies. We theoretically prove the superiority of HDSE in terms of expressivity and generalization. Empirically, we demonstrate that graph transformers with HDSE excel in graph classification, regression on 7 graph-level datasets, and node classification on 11 large-scale graphs.

## 1 Introduction

The success of Transformers  in various domains, including natural language processing (NLP) and computer vision , has sparked significant interest in developing transformers for graph data . Scholars have turned their attention to this area, aiming to address the limitations of Message-Passing Graph Neural Networks (MPNNs)  such as over-smoothing  and over-squashing .

However, Transformers  are known for their lack of strong inductive biases . In contrast to MPNNs, graph transformers do not rely on fixed graph structure information. Instead, they compute pairwise interactions for all nodes within a graph and represent positional and structural data using more flexible, soft inductive biases. Despite its potential, this mechanism does not have the capability to learn hierarchical structures within graphs. Developing effective positional encodings is also challenging, as it requires identifying important hierarchical structures among nodes, which differ significantly from other Euclidean domains . Consequently, graph transformers are prone to overfitting and often underperform MPNNs when data is limited , especially in tasks involving large graphs with relatively few labeled nodes . These challenges become even more significant when dealing with various molecular graphs, such as those found in polymers or proteins. These graphs are characterized by a multitude of substructures and exhibit long-range and hierarchical structures. The inability of graph transformers to learn these hierarchical structures can significantly impede their performance in tasks involving such complex molecular graphs.

Further, the global all-pair attention mechanism in transformers poses a significant challenge due to its time and space complexity, which increases quadratically with the number of nodes. This quadratic complexity significantly restricts the application of graph transformers to large graphs, as training them on graphs with millions of nodes can require substantial computational resources. Large-scale graphs, such as social networks and citation networks, often exhibit community structures characterized by closely interconnected groups with distinct hierarchical properties. To enhance the scalability and effectiveness of graph transformers, it is crucial to incorporate hierarchical structural information at various levels.

To address the aforementioned challenges and unlock the true potential of the transformer architecture in graph learning, we propose a Hierarchical Distance Structural Encoding (HDSE) method (**Sec. 3.1**), which can be combined with various graph transformers to produce more expressive node embeddings. HDSE encodes the hierarchy distance, a metric that measures the distance between nodes in a graph, taking into account multi-level graph hierarchical structures. We utilize popular coarsening methods [43; 65; 31; 6; 57] to construct graph hierarchies, enabling us to measure the distance relationship between nodes across various hierarchical levels.

HDSE enables us to incorporate a robust inductive bias into existing transformers and address the issue of lacking canonical positioning. To achieve this, we introduce a novel framework (**Sec. 3.2**), as illustrated in Figure 1. We utilize an end-to-end trainable function to encode HDSE as structural bias weights into the attentions, allowing the graph transformer to integrate both HDSE and other positional encodings simultaneously. Our theoretical analysis demonstrates that _graph transformers equipped with HDSE are significantly more powerful than the ones with the commonly used shortest path distances or without relative positional encodings, in terms of both expressiveness and generalization_. We rigorously evaluate our HDSE in ablation studies and show that it successfully improves different kinds of baseline transformers, from vanilla graph transformers  to state-of-the-art graph transformers [68; 11; 91; 61], across 7 graph-level datasets.

To enable the application of graph transformers with HDSE to large graphs ranging from millions to billions of nodes, we introduce a high-level HDSE (**Sec. 3.3**), which effectively biases the linear transformers towards the multi-level structural nature of these large networks. We demonstrate our high-level HDSE method exhibits high efficiency and quality across 11 large-scale node classification datasets, with sizes up to the billion-node level.

Our implementation is available at https://github.com/LUOyk1999/HDSE.

## 2 Background and Related Works

We refer to a _graph_ as a tuple \(G=(V,E,)\), with node set \(V\), edge set \(E V V\), and node features \(^{|V| d}\). Each row in \(\) represents the feature vector of a node, with \(|V|\) denoting the number of nodes and feature dimension \(d\). The features of node \(v\) are denoted by \(x_{v}^{d}\).

Figure 1: Overview of our proposed hierarchical distance structural encoding (HDSE) and its integration with graph transformers. HDSE uses the graph hierarchy distance (GHD, refer to Definition 1) that can capture interpretable patterns in graph-structured data by using diverse graph coarsening algorithms. Darker colors indicate longer distances.

### Graph Hierarchies

Given an input graph \(G\), a graph hierarchy of \(G\) consists of a sequence of graphs \((G^{k},_{k})_{k 0}\), where \(G^{0}=G\) and \(_{k}:V^{k} V^{k+1}\) are surjective node mapping functions. Each node \(v_{j}^{k+1} V^{k+1}\) represents a _cluster_ of a subset of nodes \(\{v_{i}^{k}\} V^{k}\). This partition can be described by a projection matrix \(^{k}\{0,1\}^{|V^{k}||V^{k+1}|}\), where \(^{k}_{j}=1\) if and only if \(v_{j}^{k+1}=_{k}(v_{i}^{k})\). The normalized version can be defined by \(P^{k}=^{k}C^{k-1/2}\), where \(C^{k}^{|V^{k+1}||V^{k+1}|}\) is a diagonal matrix with its \(j\)-th diagonal entry being the cluster size of \(v_{j}^{k+1}\). We define the node feature matrix \(^{k+1}\) for \(G^{k+1}\) by \(^{k+1}=P^{k^{}}^{k}\), where each row of \(^{k+1}\) represents the average of all entries within a specific _cluster_. The edge set \(E^{k+1}\) of \(G^{k+1}\) is defined as \(E^{k+1}=\{(u^{k+1},v^{k+1})| v_{r}^{k}_{k}^{-1}(u^{k+1}),v_{s}^{ k}_{k}^{-1}(v^{k+1}),(v_{r}^{k},v_{s}^{k}) E^{k}\}\).

Graph hierarchies can be constructed by repeatedly applying graph coarsening algorithms. These algorithms take a graph, \(G^{k}\), and generate a mapping function \(_{k}:V^{k} V^{k+1}\), which maps the nodes in \(G^{k}\) to the nodes in the coarser graph \(G^{k+1}\). A summary and comparison of popular graph coarsening algorithms, along with their computational complexities, can be found in Table 1. We define the _coarsening ratio_ as \(=|}{|V^{k}|}\), which represents the proportion of the number of nodes in the coarser graph \(G^{k+1}\) to the number of nodes in the original graph \(G^{k}\). Consequently, each graph \(G^{k}\), where \(k>0\), captures specific substructures derived from the preceding graph.

### Graph Transformers

Transformers  have recently gained significant attention in graph learning, due to their ability to learn intricate relationships that extend beyond the capabilities of conventional GNNs [44; 33; 59; 58], and in a unique way. The architecture of these models primarily contain a _self-attention_ module and a feed-forward neural network, each of which is followed by a residual connection paired with a normalization layer. Formally, the self-attention mechanism involves projecting the input node features \(\) using three distinct matrices \(_{}^{d d^{}}\), \(_{}^{d d^{}}\) and \(_{}^{d d^{}}\), resulting in the representations for query (\(\)), key (\(\)), and value (\(\)), which are then used to compute the output features described as follows:

\[=_{},\ = _{},\ =_{},\] \[()=( ^{}}{}}).\] (1)

Technically, transformers can be considered as message-passing GNNs operating on fully-connected graphs, decoupled from the input graphs. The main research question in the context of graph transformers is how to incorporate the structural bias of the given input graphs by adapting the attention mechanism or by augmenting the original features. The **Graph Transformer (GT)** represents an early work using Laplacian eigenvectors as positional encoding (PE), and various extensions and alternative models have been proposed since then . For instance, the **structure-aware transformer (SAT)** extracts a subgraph representation rooted at each node before computing the attention. Most initial works in the area focus on the classification of smaller graphs, such as molecules; yet, recently, **GraphGPS** and follow-up works [92; 81; 80; 82; 12; 45; 72; 16; 26] also consider larger graphs.

### Hierarchy in Graph Learning

In message passing GNNs, hierarchical pooling of node representations is a proven method for incorporating coarsening into reasoning [5; 28; 87; 48; 36; 69]. With GNNs, coarsened graph

   Coarsening algorithm & METIS  & Spectral  & Loukas  & Newman  & Louvain  \\  Complexity & \(O(|E|)\) & \(O(|V|^{3})\) & \(O(|V|)\) & \(O(|E|^{2}|V|)\) & \(O(|V||V|)\) \\   

Table 1: Comparison of popular graph coarsening algorithms.

representations are further considered in the context of molecules  and virtual nodes . Additionally, HTAK  employ graph hierarchies to develop a novel graph kernel by transitively aligning the nodes across multi-level hierarchical graphs. The recent **HC-GNN** demonstrates competitive performance in node classification on large-scale graphs, utilizing hierarchical community structures for message passing.

In graph transformers, there are currently only a few hierarchical models. **ANS-GT** use adaptive node sampling in their graph transformer, enabling it for large-scale graphs and capturing long-range dependencies. Their model groups nodes into super-nodes and allows for interactions between them. Similarly, **HSGT** aggregates multi-level graph information and employs graph hierarchical structure to construct intra-level and inter-level transformer blocks. The intra-level block facilitates the exchange and transformation of information within the local context of each node, while the inter-level block adaptively coalesces every substructure present. Our concurrent work directly incorporates hierarchy into the attention, a fundamental building block of the transformer architecture, making it flexible and applicable to existing graph transformers. Additionally, **Coarformer** utilizes graph coarsening techniques to generate coarse views of the original graph, which are subsequently used as input for the transformer model. Likewise, PatchGT  starts by segmenting graphs into patches using spectral clustering and then learns from these non-trainable graph patches. **MGT** learns atomic representations and groups them into meaningful clusters, which are then fed to a transformer encoder to calculate the graph representation. However, these approaches typically yield coarse-level representations that lack comprehensive awareness of the original node-level features . In contrast, our model integrates hierarchical information from a broader distance perspective, thereby avoiding the oversimplification in these coarse-level representations.

## 3 Our Method

### Hierarchical Distance Structural Encoding (HDSE)

Firstly, we introduce a novel concept called _graph hierarchy distance_ (GHD), which is defined as follows.

**Definition 1** (**Graph Hierarchy Distance)**.: _Given two nodes \(u,v\) in graph \(G\), and a graph hierarchy \((G^{i},_{i})_{i 0}\), the \(k\)-level hierarchy distance between \(u\) and \(v\) is defined as_

\[^{0}(u,v) =(u,v),\] \[^{k}(u,v) =(_{k-1}..._{0}(u),_{k-1}... _{0}(v)),\] (2)

_where \((,)\) is the shortest path distance between two nodes (\(\) if the nodes are not connected), and \(_{k-1}..._{0}()\) maps a node in \(G^{0}\) to a node in \(G^{k}\)._

Note that the \(k\)-level hierarchy distance adheres to the definition of a metric, being zero for \(v=u\), invariably positive, symmetric, and fulfilling the triangle inequality. As illustrated on the left side of Figure 1, it can be observed that \(^{0}(v_{1},v_{11})=7\), whereas \(^{1}(v_{1},v_{11})=2\).

Graph hierarchies have been observed to assist in identifying community structures in graphs that exhibit a clear property of tightly knit groups, such as social networks and citation networks . They may also aid in prediction over graphs with a clear hierarchical structure, such as metal-organic frameworks or proteins. Fig. 2 shows that with the graph hierarchies generated by the Newman coarsening method, \(^{1}\) is capable of capturing chemical motifs, including CF3 and aromatic rings.

Based on GHD, we propose _hierarchical distance structural encoding_ (HDSE), defined for each pair of nodes \(i,j V\):

\[_{i,j}=[^{0},^{1},...,^{K}]_{i,j}^{K+1},\] (3)

where \(^{k}\) is the \(k\)-level hierarchy distance matrix, and \(K\) controls the maximum level of hierarchy considered.

We demonstrate the superior expressiveness of HDSE over SPD using recently proposed graph isomorphism tests inspired by the Weisfeiler-Leman algorithm . In particular,  introduced the Generalized Distance Weisfeiler-Leman (GD-WL) Test and applied it to analyze a graph transformer architecture that employs \((i,j)\) as relative positional encodings. They proved that the graph transformer's maximum expressiveness is the GD-WL test with SPD. Here, we also use the GD-WL test to showcase the expressiveness of HDSE.

**Proposition 1** (Expressiveness of HDSE).: _GD-WL with HDSE \((_{i,j})\) is strictly more expressive than GD-WL with the shortest path distance \((i,j)\)._

The proof is provided in Appendix 4. Firstly, we show that the GD-WL test using HDSE can differentiate between any two graphs that can be distinguished by the GD-WL test with SPD. Next, we show that the GD-WL test with HDSE is capable of distinguishing the Dodecahedron and Desargues graphs (Figure 2) while the one with SPD cannot.

### Integrating HDSE in Graph Transformers

We integrate HDSE (\(_{i,j}\)) into the attention mechanism of each graph transformer layer to bias each node update. To achieve this, we use an end-to-end trainable function \(:^{K+1}\) to learn the biased structure weight \(_{i,j}=(_{i,j})\). We limit the maximum distance length to a value \(L\), based on the hypothesis that detailed information loses significance beyond a certain distance. By imposing this limit, the model can extend acquired patterns to graphs of varying sizes not encountered in training. Specifically, we implement the function \(\) using an MLP as follows:

\[_{i,j} =([_{_{i,j}^{0}}^{0 },,_{_{i,j}^{K}}^{K}]),\] \[_{i,j}^{k} =(L,_{i,j}^{k}),0 k K,\] (4)

where \([_{0}^{k},_{1}^{k},,_{L}^{k}]_ {0 k K}^{d(L+1)}\) collects \(L+1\) learnable feature embedding vectors \(_{i}^{k}\) for hierarchy level \(k\). By embedding the hierarchy distances at different levels into learnable feature vectors, it may enhance the aggregation of multi-level graph information among nodes and expands the receptive field of nodes to a larger scale. We assume single-headed attention for simplified notation, but when extended to multiheaded attention, one bias is learned per distance per head.

We incorporate the learned biased structure weights \(H\) to graph transformers, using the popular biased self-attention method proposed by , formulated as:

\[()=(+ ),=^{}}{ }},\] (5)

where the original attention score \(\) is directly augmented with \(\). This approach is backbone-agnostic and can be seamlessly integrated into the self-attention mechanism of existing graph transformer architectures. Notably, we have the following results on expressiveness and generalization.

**Proposition 2**.: _The power of a graph transformer with HDSE to distinguish non-isomorphic graphs is at most equivalent to that of the GD-WL test with HDSE. With proper parameters and an adequate number of heads and layers, a graph transformer with HDSE can match the power of the GD-WL test with HDSE._

Figure 2: Examples of graph coarsening results and hierarchy distances. Left: HDSE can capture chemical motifs such as CF3 and aromatic rings on molecule graphs. Right: HDSE can distinguish the Dodecahedron and Desargues graphs. The Dodecahedral graph has 1-level hierarchy distances of length 2 (indicated by the dark color), while the Desargues graph doesn’t. In contrast, the GD-WL test with SPD cannot distinguish these graphs .

See the proof in Appendix 5. This result provides a precise characterization of the expressivity power and limitations of graph transformers with HDSE. Combining Proposition 1, 2 and Proofs A.1 in  immediately yields the following corollary:

**Corollary 1** (**Expressiveness of Graph Transformers with HDSE**).: _There exists a graph transformer using HDSE (with fixed parameters), denoted as \(\), such that \(\) is more expressive than graph transformers with the same architecture using SPD or using no relative positional encoding, regardless of their parameters._

This is a fine-grained expressiveness result of graph transformers with HDSE. It demonstrate the superior expressiveness of HDSE over SPD in graph transformers.

**Proposition 3** (**Generalization of Graph Transformers with HDSE**).: _(Informal) For a semi-supervised binary node classification problem, suppose the label of each node \(i V\) is determined by node features in the "**hierarchical core neighborhood**" \(S_{s}^{i}=\{j:_{i,j}=D^{*}\}\) for a certain \(D^{*}\), where \(_{i,j}\) is HDSE defined in (3). Then, a properly initialized one-layer graph transformer equipped with HDSE can learn such graphs with a desired generalization error, while using SPD or using no relative positional encoding cannot guarantee satisfactory generalization._

The formal version and the proof are given in Appendix 6. Proposition 3 is a corollary and extension of Theorem 4.1 of  from SPD to HDSE. It indicates that learning with HDSE can capture the labeling function characterized by the hierarchical core neighborhood, which is more general and comprehensive than the core neighborhood for SPD proposed in .

### Scaling HDSE to Large-scale Graphs

For large graphs with millions of nodes, training and deploying a transformer with full global attention is impractical due to the quadratic cost. Some existing graph transformers address this issue by sampling nodes for attention computation [91; 96]. While our HDSE can enhance these models, this sampling approach compromises the expressivity needed to capture interactions among all pairs of nodes. However, in the NLP domain, Linformer  utilizes a learnable low-rank projection matrix \(}\) to reduce the complexity of the self-attention module to linear levels:

\[()=( _{}(}_{})^ {}/})}_{ }.\] (6)

Inspired by Linformer, models like GOAT  and Gapformer  in the graph domain also employ projection matrices to reduce the number of nodes by projecting them onto fewer super-nodes, consequently compressing the input node feature matrix \(\) into a smaller dimension. This transformation enables the aggregation of information from super-nodes and reduces the quadratic complexity of attention computation to linear complexity. Here, we can replace the projection matrix with our coarsening partition matrix \(P\) (see Sec. 2.1) to obtain representations of coarser graphs at higher levels. Observe that we can still calculate meaningful distances at these higher hierarchy levels, using a _high-level_ HDSE as follow:

\[_{i,j}^{c}=[^{c}(_{l=0}^{c-1}P^{l}),..., ^{K}(_{l=0}^{c-1}P^{l})]_{i,j}{}_{1 c K},\] (7)

where each row of the projection matrix \(P^{l}\) (see Sec. 2.1) is a one-hot vector representing the \(l\)-level cluster that an input node belongs to, and \(^{c}^{|V^{0}||V^{c}|(K+1-c)}\). Note that \(^{m}(_{l=0}^{c-1}P^{l})\), \(c m K\) computes distances from input nodes to clusters at \(c\)-level graph hierarchy (see App. A). In practice, these distances can be directly obtained by calculating the hierarchy distance between all node pairs at the \(c\)-level. When \(c=0\), \(^{c}\) becomes \(\) in Eq 3. In this way, the high-level HDSE establishes attention between nodes in the input graph \(G\) and clusters at high level hierarchies. For example, we can integrate the high-level HDSE into Linformer by adapting Equation (6):

\[() =(_{}( ^{k}_{})^{}}{}}+ ^{k})^{k}_{},\] \[^{k} =(^{k})^{|V^{0}||V^{k} |},\] (8)

where \(^{k}^{|V^{k}| d}\) (see Sec. 2.1) represents the features of clusters at \(k\)-level, and \(:^{K+1-k}\) is a end-to-end trainable function as defined in Sec. 3.2.

Evaluation

We evaluate our proposed HDSE on 18 benchmark datasets, and show state-of-the-art performance in many cases. Primarily, the following questions are investigated:

* Can **HDSE improve upon existing graph transformers**, and how does the choice of **coarsening algorithm** affect performance? **(Sec. 4.2)**
* Does our **adaptation for large graphs** also **show effectiveness**, is it marked by **efficiency**, and how does **high-level HDSE** impact the performance? (Sec. 4.3)

### Experiment Setting

**Datasets.** We consider various graph learning tasks from popular benchmarks as detailed below and in Appendix B.

* **Graph-level Tasks.** For graph classification and regression, we evaluate our method on five datasets from Benchmarking GNNs : ZINC, MNIST, CIFAR10, PATTERN, and CLUSTER. We also test on two peptide graph benchmarks from the Long-Range Graph Benchmark : Peptides-func and Peptides-struct, focusing on classifying graphs into 10 functional classes and regressing 11 structural properties, respectively. We follow all evaluation protocols suggested by .
* **Node Classification on Large-scale Graphs.** We consider node classification over the citation graphs Cora, CiteSeer and PubMed , the Actor co-occurrence graph , and the Squirrel and Chameleon page-page networks from , all of which have 1K-20K nodes. Further, we extend our evaluation to larger datasets from the Open Graph Benchmark (OGB) : ogbn-arxiv, arxiv-year, ogbn-papers100M, ogbn-proteins and ogbn-products, with node numbers ranging from 0.16M to 0.1B. We maintain all the experimental settings as described in .

**Baselines.** We compare our method to the following prevalent GNNs: GCN , GIN , GAT , GatedGCN , GatedGCN-RWSE , JKNet , APPNP , PNA , GPRCNN , SIGN , H2GCN ; and other recent GNNs with SOTA performance: LINKX , CIN , GIN-AK+ , HC-GNN . In terms of transformer models, we consider **GT**, Graphormer , SAN , Coarformer , ANS-GT , EGT , NodeFormer , Specformer , MGT , AGT , HSGT , Graphormer-GD , **SAT**, **GOAT**, Gapformer , Graph ViT/MLP-Mixer , LargeGT , NAGphormer , Exphormer , DREw , VCR-GT , CoBFormer  and recent SOTA graph transformers **GraphGPS**, **GRIT**, SGFormer .

**Models + HDSE.** We integrate HDSE into GT, SAT, GraphGPS, GRIT (and ANS-GT in appendix) _only modifying their self-attention module_ by Eq. 5. For fair comparisons, we use the same hyperparameters (including the number of layers, hidden dimension etc.), PE and readout as the baseline transformers. Given one of the baseline transformers **M**, we denote the modified model using HDSE by **M + HDSE**. Additionally, we integrate our high-level HDSE into **GOAT**, denoted as **GOAT + HDSE**. We fix the maximum distance length \(L=30\) and vary the maximum hierarchy level \(K\) in \(\{0,1,2\}\) in all experiments. A sensitivity analysis of these two parameters is presented in Appendix C. During training, the steps of coarsening and distance calculation  can be treated as pre-processing, since they only need to be run once. We detail the choice and runtime of coarsening algorithms for HDSE in the appendix. Detailed experimental setup and hyperparameters are in Appendix B due to space constraints.

### Results on Graph-level Tasks

**Benchmarks from Benchmarking GNNs, Table 2.** We observe that nearly all four baseline graph transformers, when combined with HDSE, demonstrate performance improvements. Note that the enhancement is overall especially considerable for GT. On CIFAR10, we also obtain similar improvement for GraphGPS. Among them, GT shows the greatest enhancement and becomes competitive to more complex models. Our model attains the best or second-best mean performance for all datasets. While the improvement for GRIT is smaller, as its relative random walk probabilities (RRWP) already incorporate distance information , we still observe improvements in three datasets. This indicates that HDSE can provide additional information beyond what is captured by RRWP. Notably, it is observed that the SOTA SGFormer tailored for large-scale node classification underperforms in graph-level tasks.

**Long-Range Graph Benchmark, Table 3.** We consider GraphGPS due to its superior performance. Note that our HDSE module only introduces a small number of additional parameters, allowing it to remain within the benchmark's \(\)500k model parameter budget. In the Peptides-func dataset, HDSE yields a significant improvement of 6.21%. This is a promising result and hints at potentially great benefits for macromolecular data more generally.

**Ablation Study and Visualization, Table 4, 13, 16, Figure 3, 5.** First, we conduct several ablation experiments of coarsening algorithms on ZINC and observe that the dependency on the coarsening varies with the transformer backbone. For instance, the multi-level graph structures extracted by the Newman algorithm yields the largest improvement for GraphGPS. More generally, our experiments indicate that Newman works best for molecular graphs. We visualize the attention scores on the ZINC and Peptides-func datasets respectively, as shown in Figure 3. The results indicate that our HDSE method successfully leverages hierarchical structure.

We also conduct a sensitivity analysis on maximal hierarchy level \(K\) and maximum distance length \(L\) in Appendix C. The variation in the optimal \(K\) and \(L\) could stem from the distinct hierarchical structures inherent in different graphs.

   Model & Coarsening algorithm & ZINC \\  & & MAE \(\) \\   & w/o & \(0.094 0.008\) \\  & METIS & \(0.089 0.005\) \\  & Spectral & \(0.088 0.004\) \\  & Loukas & \(\) \\  & Newman & \(\) \\  & Louvain & \(0.088 0.003\) \\   & w/o & \(0.070 0.004\) \\  & METIS & \(0.069 0.002\) \\  & Spectral & \(0.063 0.003\) \\  & Loukas & \(0.067 0.002\) \\  & Newman & \(\) \\  & Louvain & \(0.064 0.002\) \\   

Table 4: Ablation experiments of coarsening algorithms on ZINC.

   Model & ZINC & MNIST & CIFAR10 & PATTERN & CLUSTER \\  & MAE \(\) & Accuracy \(\) & Accuracy \(\) & Accuracy \(\) & Accuracy \(\) \\  GCN & \(0.367 0.011\) & \(90.705 0.218\) & \(55.710 0.381\) & \(71.892 0.334\) & \(68.498 0.976\) \\ GIN & \(0.526 0.051\) & \(96.485 0.252\) & \(55.255 1.527\) & \(85.387 0.136\) & \(64.716 1.553\) \\ GatedGCN & \(0.282 0.015\) & \(97.340 0.143\) & \(67.312 0.311\) & \(85.568 0.088\) & \(73.840 0.326\) \\ PNA & \(0.188 0.004\) & \(97.940 0.120\) & \(70.350 0.630\) & – & – \\ CIN & \(0.079 0.006\) & – & \(72.190 0.130\) & \(86.850 0.057\) & – \\ GIN-AK+ & \(0.080 0.001\) & – & \(72.190 0.130\) & \(86.850 0.057\) & – \\  SGFormer & \(0.306 0.023\) & – & – & \(85.287 0.097\) & \(69.972 0.634\) \\  SAN & \(0.139 0.006\) & – & – & \(86.581 0.037\) & \(76.691 0.650\) \\ Graphormer-GD & \(0.081 0.009\) & – & – & – & – \\ Specformer & \(\) & – & – & – & – \\ EGIT & \(0.108 0.009\) & \(98.173 0.087\) & \(68.702 0.409\) & \(86.821 0.020\) & \(\) \\ Graph ViT/MLP-Mixer & \(0.073 0.001\) & \(97.42 0.110\) & \(73.961 0.330\) & – & – \\ Explorer & – & \(\) & \(74.696 0.125\) & \(86.742 0.015\) & \(78.071 0.037\) \\  GT & \(0.226 0.014\) & \(90.831 0.161\) & \(59.753 0.293\) & \(84.808 0.068\) & \(73.169 0.622\) \\
**GT + HDSE** & \(0.159 0.006^{*}\) & \(94.394 0.177^{*}\) & \(64.651 0.591^{*}\) & \(86.713 0.409^{*}\) & \(74.223 0.573^{*}\) \\  SAT & \(0.094 0.008\) & – & – & \(86.848 0.037\) & \(77.856 0.104\) \\
**SAT + HDSE** & \(0.084 0.003^{*}\) & – & – & \(}\) & \(78.513 0.097^{*}\) \\  GraphGPS & \(0.070 0.004\) & \(98.051 0.126\) & \(72.298 0.356\) & \(86.685 0.059\) & \(78.016 0.180\) \\
**GraphGPS + HDSE** & \(0.062 0.003^{*}\) & \(}\) & \(}\) & \(86.737 0.055\) & \(78.498 0.121^{*}\) \\  GRIT & \(\) & \(98.108 0.111\) & \(76.468 0.881\) & \(87.196 0.076\) & \(\) \\
**GRIT + HDSE** & \(\) & \(98.424 0.124^{*}\) & \(\) & \(\) & \(79.965 0.203\) \\   

Table 2: Test performance in five benchmarks from . The results are presented as the mean \(\) standard deviation from 5 runs using different random seeds. Baseline results were obtained from their respective original papers. \({}^{*}\) indicates a statistically significant difference against the baseline w/o HDSE from the one-tailed t-test. Highlighted are the top first, second and third results.

   Model & Peptides-func & Peptides-struct \\  & AP \(\) & MAE \(\) \\  GCN & \(0.5930 0.0023\) & \(0.3496 0.0013\) \\ GINE & \(0.5498 0.0079\) & \(0.3547 0.0045\) \\ GatedGCN & \(0.5864 0.0035\) & \(0.3420 0.0013\) \\ GatedGCN+Rwise & \(0.6069 0.0035\) & \(0.3357 0.0006\) \\  GT & \(0.6326 0.0126\) & \(0.2529 0.0016\) \\ SAN+Rwise & \(0.6439 0.0075\) & \(0.2545 0.0012\) \\ MGT+WavePE & \(0.6817 0.0064\) & \(\) \\ GRIT & \(\) & \(\) \\ Explorer & \(0.6527 0.0043\) & \(0.2481 0.0007\) \\ Graph ViT/MLP-Mixer & \(0.6970 0.0080\) & \(0.2475 0.0015\) \\ Drew & \(0.7150 0.0044\) & \(0.2536 0.0015\) \\  GraphGPS & \(0.6535 0.0041\) & \(0.2500

**Synthetic Community Graphs, Table 19.** We evaluate our methods on the community datasets from , generated using the Erdos-Renyi model . These graphs have adjacency matrices that obey the certain clustered structure. As evidenced in Table 19, the GT struggles to detect such structures; and solely utilizing SPD proves inadequate; however, our HDSE, enriched with coarsening structural information, effectively captures these structures.

### Results on Large-scale Graphs

**Overall Performance, Table 5.** We select four categories of baselines: GNNs, graph transformers with proven performance on graph-level tasks, graph transformers with hierarchy, and scalable graph transformers. It is noteworthy that while some graph transformers exhibit superior performance on graph-level tasks, they consistently result in out-of-memory (OOM) in large-scale node tasks. The results are remarkably consistent. In relatively smaller datasets graphs (on the left side), the integra

Figure 3: Visualization of attention weights for the transformer attention and HDSE attention. The left side illustrates the graph coarsening result. The center column displays the attention weights of a sample node learned by the classic GT , while the right column showcases the attention weights learned by the HDSE attention.

tion of high-level HDSE enables GOAT to demonstrate competitive performance among baseline models. This could be due to the coarsening projection filtering out the edges from neighboring nodes of different categories and providing a global perspective enriched with multi-level structural information. For all larger graphs (on the right side), our high-level HDSE method significantly enhances GOAT's performance beyond its original version. This indicates that the structural bias provided by graph hierarchies is capable of handling the node classification task in such larger graphs and effectively retains global information. We investigated this in more detail in our ablation experiments. Furthermore, we also observed that all graph transformers with hierarchy suffer from serious overfitting, attributed to their relatively complex architectures. In contrast, our model's simple architecture contributes to its better generalization.

**Efficiency Comparison, Table 6.** We report the efficiency results on PubMed, ogbn-proteins, ogbn-arxiv, ogbn-products and ogbn-100M. It is easy to see that our model outperforms NodeFormer in speed, matching the pace of the latest and fastest model, SGFormer . It achieves true linear complexity with a streamlined architecture.

**Ablation Study, Table 7.** To determine the utility of our architectural design choices, we conduct ablation experiments on GOAT + HDSE over three datasets. The results presented in Table 7, include (1) removing the high-level HDSE and (2) replacing the coarsening projection matrix with the original projection matrix used in GOAT. These experiments reveal a decline in all performance, thereby validating the effectiveness of our architectural design.

## 5 Conclusions

We have introduced the Hierarchical Distance Structural Encoding (HDSE) method to enhance the capabilities of transformer architectures in graph learning tasks. We have developed a flexible framework to integrate HDSE with various graph transformers. Further, for applying graph transformers with HDSE to large-scale graphs, we have introduced a high-level HDSE approach that effectively biases linear transformers towards the multi-level structure. Theoretical analysis and empirical results validate the effectiveness and generalization capabilities of HDSE, demonstrating its potential for various real-world applications.