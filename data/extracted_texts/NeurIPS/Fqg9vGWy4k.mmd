# Faster approximate subgraph counts with privacy

Dung Nguyen

Department of Computer Science and

UVA Biocomplexity Institute and Initiative

University of Virginia, USA

dungn@virginia.edu

&Mahantesh Halappanavar

Data Sciences and Machine Intelligence Group

Pacific Northwest National Laboratory, USA

hala@pnnl.gov

Venkatesh Srinivasan

Department of Mathematics and

Computer Science

Santa Clara University, USA

vsinivasan4@scu.edu

&Anil Vullikanti

Department of Computer Science and

UVA Biocomplexity Institute and Initiative

University of Virginia, USA

vaskumar@virginia.edu

###### Abstract

One of the most common problems studied in the context of differential privacy for graph data is counting the number of non-induced embeddings of a subgraph in a given graph. These counts have very high global sensitivity. Therefore, adding noise based on powerful alternative techniques, such as smooth sensitivity and higher-order local sensitivity have been shown to give significantly better accuracy. However, all these alternatives to global sensitivity become computationally very expensive, and to date efficient polynomial time algorithms are known only for few selected subgraphs, such as triangles, \(k\)-triangles, and \(k\)-stars. In this paper, we show that good approximations to these sensitivity metrics can be still used to get private algorithms. Using this approach, we much faster algorithms for privately counting the number of triangles in real-world social networks, which can be easily parallelized. We also give a private polynomial time algorithm for counting any constant size subgraph using less noise than the global sensitivity; we show this can be improved significantly for counting paths in special classes of graphs.

## 1 Introduction

The notion of Differential Privacy (DP)  has emerged as the _de facto_ standard notion for supporting queries on private and sensitive data. DP ensures that changes in private data have limited statistical influence (measured by the privacy budget \(,\)) on the output of queries, without needing any attack model, which is one of the reasons of its popularity. Networked abstractions are commonly used in a number of applications, such as public health, social networks, and finance. Such datasets are usually sensitive, and maintaining privacy is an important concern. Within the context of network/graph data, two privacy models have been considered in most prior work, namely, edge and node privacy. There has been a lot of interest in developing private algorithms for a number of network problems, such as community detection , and counting small subgraphs (e.g., stars and triangles ).

However, efficient private algorithms with good accuracy are not known for many graph problems, including counting small subgraphs. One of the main challenges is that graph problems often have very high _global sensitivity_ (the maximum change in the output function due to the change in one edge/vertex (see Section 3)), in contrast to many statistical and machine learning queries. As a result, DP algorithms based on global sensitivity (which can usually be computed quite easily for many problems) do not give good accuracy bounds, in general. For instance, in the edge-privacymodel, the global sensitivity of #triangles in a graph \(G=(V,E)\) can be as high as \(n-2\), where \(n=|V|\), making the added noise too large in many instances. A number of novel alternatives to global sensitivity have been proposed, e.g., smooth, restricted and multi-level local sensitivity, and ladder functions [36; 23; 3; 31], which add significantly lower level of noise than mechanisms based on global sensitivity.

However, all these alternative notions of sensitivity are computationally much more challenging, and none of the current graph DP algorithms for subgraph counting scale to even moderate size networks. Even the simplest problem of counting #triangles privately takes \(\{m_{v}d(v),M(n)\}\) time , where \(m=|E|\), \(d(v)\) is the degree of node \(v V\), and \(M(n)\) is the time for multiplying two \(n n\) matrices, which is super-quadratic, in general. This is a sharp contrast with the significant advances in non-private graph mining where complex network properties such as clique counts can be computed using provably-efficient and practical algorithms [35; 20; 19]. For a few other subgraphs, such as \(k\)-cliques and \(k\)-triangles (see [36; 23] for definitions), private algorithms using noise lower than the global sensitivity are known, but the worst case running time can still be \(O(n^{k})\). However, for most subgraph counting problems, _no private algorithms which use alternatives to global sensitivity, and have time comparable to the non-private algorithms, are known_.

A main tool for subgraph counting and other graph mining tasks in massive networks are sampling based approximation algorithms, e.g., ; however, these haven't been used in the context of private graph algorithms. In this paper, we take the first steps in this direction. Our main contributions are:

\(\) We show that for a query \(f(G)\), we can get a private algorithm by adding noise that scales with an approximation to the smooth sensitivity to an approximation to \(f(G)\) (Theorem 1). This result opens the door to improving efficiency of private algorithms using approximation techniques that have been very useful in non-private graph analytics. As an illustration, we use this result to obtain a quasilinear time private algorithm for counting #triangles under edge privacy (Theorem 2) for graphs which satisfy a stronger transitivity property (see Definition 5). Our algorithm adapts the _diamond sampling_ technique of  for approximate triangle counting, and can also be parallelized easily. No parallel algorithms have been developed for private subgraph counting so far.

\(\) Extending the approach of , we design a private higher-order local sensitivity approach for subgraph counting, which gives privacy even when the higher-order local sensitivities are estimated approximately (Theorem 7). This yields the first private algorithm to count the number of embeddings of _any_ subgraph with \(\) edges in time \(O(n^{2})\), using less noise than global sensitivity. For the specific case of paths with \(\) edges, we develop an algorithm that gives a very close approximation to the higher-order sensitivity in time \(O(h(d,t,)poly(n))\), where \(d\) and \(t\) are the degeneracy and treewidth of \(G\), respectively (defined later). Thus, for paths, the running time does not grow as \(O(n^{})\) for graphs with bounded degeneracy and treewidth. These properties have been used extensively in developing efficient algorithms for subgraph counting and other graph mining tasks, e.g., [5; 13; 6]; but, they haven't been used for private algorithms.

\(\) We experimentally evaluate our algorithm for counting #triangles (Section 6). We show that our algorithm for estimating the smooth sensitivity has very good approximation factor, and significantly better running time than the exact algorithm. Using approximate smooth sensitivity for counting #triangles gives good accuracy even for fairly low \(\) values, suggesting that using approximate smooth sensitivity does not compromise performance. Our algorithm also has good scaling, and is the first private algorithm for counting #triangles which has been run on networks with over 2 million edges.

We note that many details, including proofs are presented in the Supplementary Material (SM).

## 2 Related Work

The area of DP graph algorithms is quite large and active. There has been a lot of work on DP algorithms for many basic graph problems, such as community detection, subgraph counting, finding small cuts, and releasing synthetic graphs [25; 28; 30; 33; 16; 3; 15; 21; 17]. There is some work on graph algorithms under local DP, e.g., , but most of the prior work has been for global DP (as defined in Section 3), and is our focus in this work. We refer to  for a recent survey on private graph algorithms. For brevity, we only discuss prior work on private subgraph counting that is directly related to our paper, and in the edge-DP model. We omit the discussion on private estimation of edge density and degree distribution, which has been studied more extensively, e.g., , and results for node-DP , which is less studied.

As mentioned earlier, global sensitivity, denoted by \(GS(f)\) for a graph metric \(f(G)\) (defined in Section 3) is high for many subgraph counting problems. Adding noise based on \(GS(f)\) leads to low accuracy, while noise based on just \(LS_{f}\), the local sensitivity, need not be private . Nissim et al.  develop the notion of smooth sensitivity \(S^{*}_{f,\,}(G)=_{G^{}}(LS_{f}(G^{}) e^{-  d(G,G^{})})\), where \(d(G,G^{})\) is the swap distance between graphs \(G\) and \(G^{}\), and show that adding noise based on \(S^{*}_{f,\,}(G)\) is private. However, smooth sensitivity becomes computationally much more challenging, since its definition involves considering local sensitivity at distance \(t\) for all \(t\), which doesn't seem like a polynomial time computation. Nissim et al.  develop polynomial time algorithms for computing the smooth sensitivity exactly for counting #triangles, which was improved slightly to \(\{md_{max},M(n)\}\), where \(M(n)\) is the matrix multiplication time. Polynomial time smooth sensitivity bounds were also shown for a small number of other subgraphs, but no polynomial time algorithms are known beyond that. Interestingly, no hardness bounds are known for smooth sensitivity; the existing hardness bounds, e.g., [23; 36] only give hardness for computing local sensitivity at distance \(t\). In order to handle other subgraph counts, Karwa et al.  develop a different technique involving local sensitivity of local sensitivity (motivated by the propose-test-release technique ), and use it for private counts of \(k\)-cliques. Zhang et al.  develop the technique of _ladder functions_ to handle other kinds of subgraphs, and use it to privately count #\(k\)-cliques in the graph in time \(O(nT(n))\), where \(T(n)\) is the time needed to count #\(k\)-cliques non-privately. A few other techniques, such as inverse sensitivity  and propose-test-release  are known. However, private algorithms for counting most subgraphs including paths and trees are not known. The recent work of  is related, but only considers approximation in the queries, but not in sensitivity. As a result, our methods give significantly higher efficiency. We also note that Blocki et al.  develop a black-box approach to make certain approximation algorithms differentially private. However, their work requires the function being computed to have "small" global sensitivity.

The difficulty of subgraph counting with DP has motivated work in slight variations of the DP model. Rastogi et al.  consider the problems of releasing more general subgraph counts. However, they consider a relaxed version of edge-DP, called (edge) adversarial privacy, that uses a Bayesian attacker. Chen et al.  design a different approach that gives lower bounds for general subgraph counts through a linear program to form a recursive strategy. But, as mentioned in , their method suffers from a bias between the true query answer and the lower bound, in exchange for less noise.  presents a different approach based on iterative refinement that estimates counts by degree, and can be implemented in time \(O((_{v}deg(v))^{3}m)\).

There have been studies on subgraph counting (including triangle counting) under other privacy models: the node-DP model [25; 10], and the shuffle model . We note that our analysis of approximate smooth sensitivity holds for other privacy models as well, and we expect this could be used for improved private queries for other problems. However, the specific technique using diamond sampling for faster triangle counting only holds in the edge-DP model. Fundamentally new ideas are needed for extending this technique to node-DP and others. For a more comprehensive review of recent development on subgraph counting with privacy, we refer readers to .

   & Privacy model & Runtime \\  Global Sensitivity & \(\) & \(O(1)\) \\  Ladder function  & \((,)\) & \(O(n)\) \\  Recursive mechanism  & \((,)\) & \(O(mn)\) \\  Restricted Sensitivity  & \((,)\) & \(O(mn)\) \\  Blackbox Transformation  & \((,)\) & \(O(\#_{})\) \\  (Exact) Smooth Sensitivity  & \((,)\) & \(O(n)\) \\ 
**Our method** & \((,)\) & \(O(m^{2}n)\) for \((C,)\)-graphs with constant \(C,\) \\  

Table 1: Summary of the characteristics of differentially private #triangle counting algorithms in the edge-privacy model (the other works mentioned here consider other subgraphs also, but we only focus on the runtime of counting #triangles). The runtime of Blackbox Transformation  includes the calculation of an approximated triangle counts, while all others assume the true counts are given. The runtime of our method is reported with \(=(n^{-2})\) and constant \(\) (see Corollary 1).

## 3 Preliminaries

A non-induced embedding of a graph \(H=(V_{H},E_{H})\) into a graph \(G=(V,E)\) is a mapping \(:V_{H} V\) such that \(((u),(v)) E\) whenever \((u,v) E_{H}\). We use \(f_{H}(G)\) to denote the number of non-induced embeddings of a \(H\) in \(G\). We drop the subscript \(H\) when it is clear from the context. Let \(=|V_{H}|\) and \(n=|V|\). \(A\) denotes the adjacency matrix of graph \(G\). Let \(N(i)\) and \(\) denote the set of neighbors and non-neighbors of node \(i V\), respectively; let \(d(i)=|N(i)|\) and \((i)=n-1-d(i)\). Let \(d_{max}=_{v V}d(v)\) denote the maximum degree.

**Differential privacy on graphs.** Let \(\) denote a set of graphs on a fixed set \(V\) of nodes. For a graph \(G\), we use \(V(G)\) and \(E(G)\) to denote the set of nodes and edges of \(G\), respectively. In this paper, we will focus on the notion of _edge privacy_, where all graphs \(G\) have a fixed set of nodes \(V(G)=V\), and two graphs \(G,G^{}\) are considered neighbors, i.e., \(G G^{}\), if they differ in exactly one edge, i.e., \(|E(G)-E(G^{})|=1\).

**Definition 1**.: _A (randomized) algorithm \(M: R\) is \((,)\)-differentially private if for all subsets \(S R\) of its output space, and for all \(G,G^{}\), with \(G G^{}\), we have \(Pr[M(G) S] e^{}Pr[M(G^{}) S]+\)._

**Problem statement: subgraph counting with edge differential privacy.** Given a family of graphs \(\) on a set \(V\) of vertices, a subgraph \(H\), and parameters \(,\), construct an \((,)\)-differentially private mechanism \(M_{f_{H}}: 2^{V}\), such that \(|M_{f_{H}}(G)-f_{H}(G)|\) is minimized.

We discuss the notions of sensitivity mostly using the notation from , with slight changes. Let \(LS_{f}(G)=_{G^{} G}|f(G)-f(G^{})|\) denote the local sensitivity of \(f\); we also use \(LS(f(G))\) to denote this. \(GS(f)=_{G}LS(f(G))\) denotes the global sensitivity. The local sensitivity of a function \(f\) on a graph \(G\) at distance \(t\) is defined as \(LS_{f}^{(t)}(G)=_{d(G,G^{}) t}LS_{f}(G^{})\)

**Definition 2**.: _(Smooth bound on \(LS\)) For \(>0\), a function \(S:D^{n}^{+}\) is a \(\)-smooth upper bound on \(LS_{f}\) if it satisfies the following conditions: (1) for all \(x D^{n}\): \(S(x) LS_{f}(x)\), and (2) for all \(x y D^{n}\): \(S(x) e^{} S(y)\)._

\(S_{f,}^{*}(G)=_{G^{}}(LS(f(G^{})) e^{- d(G,G^{ })})\) is the smallest function satisfying Definition 2, and is referred to as the \(\)-smooth sensitivity of \(f\) at \(G\). Using the local sensitivity at distance \(t\), the smooth sensitivity can be written as \(S_{f,}^{*}(G)=_{t=1,,}e^{-t}LS_{f}^{(t)}(G)\).

**Definition 3**.: _(Admissible Noise Distribution)  A probability distribution on \(^{d}\) given by a density function \(h\) is \((,)\)-admissible if, for \(=(,)\), \(=(,)\), the following conditions hold for all \(^{d}\) and \(\) satisfying \(||||_{1}\) and \(||\), and for all measurable \(^{d}\): (1) Sliding property: \(_{Z h}[Z] e^{/2}_{Z h}[Z+]+/2\), and (2) Dilation property: \(_{Z h}[Z] e^{/2}_{Z h}[Z e^{ }]+/2\)._

An important example of an admissible noise distribution is the Laplace mechanism \(Lap()\) probability density function is \(h(z)=e^{-|z|/}\).

**Lemma 1**.: _(Calibrating noise to Smooth Sensitivity ) Let \(Z\) be a random variable sampled from an \((,)\)-admissible noise. Let \(S_{f,}\) be the \(\)-smooth upper bound on the local sensitivity of \(f\). Then algorithm \(A(x)=f(x)+(x)}{}Z\) is \((,)\)-differentially private._

**Definition 4**.: _(An \((,)\)-approximation to a function \(f\)). \(\) is said to be an \((,)\)-approximation to a function \(f\) if for any input \(x\), with probability at least \(1-\), we have \((1-)f(x)(x)(1+)f(x)\)._

Social networks generally have the property that nodes have high clustering coefficient, which is the fraction of pairs of neighbors of a node which are connected. We consider a more restricted notion here. We refer to a path with \(r\) edges as an \(r\)-path. We say a 2-path \(i,j,k\) is "closed" if \((i,k) E\), i.e., \(i,j,k\) form a triangle in \(G\).

**Definition 5**.: _(\((C,)\)-transitive graph) A graph is said to be \((C,)\)-transitive if for any vertex \(j V\) and edge \((i,j) E\), we have: if \(deg(j)>\), then \(C\) fraction of all the 2-paths starting with \(i,j\) are closed._Approximate smooth sensitivity and fast private triangle counting

We first show that we can ensure privacy even if the smooth sensitivity \(S^{*}_{f,}\) is estimated approximately; we then extend it to show that this works when \(f(G)\) and \(S^{*}_{f,}\) both are estimated approximately.

**Definition 6**.: \(_{f,}\) _is said to be a \(\)-upper approximation of the smooth sensitivity \(S^{*}_{f,}\) of function \(f\) if \(S^{*}_{f,}(D)_{f,}(D) e^{}S^{*}_{f,}(D)\) for any dataset \(D\). \(_{f,}\) is said to be a \((,^{})\)-upper approximation of the smooth sensitivity \(S^{*}_{f,}\) of function \(f\) if \(_{f,}\) is a \(\)-upper approximation for any dataset \(D\) with probability at least \(1-^{}\)._

We observe below that calibrating noise using a \((,^{})\)-approximation of the smooth sensitivity gives us privacy. We assume \(f\) is a real valued function, since we are focused on graph statistics.

**Lemma 2**.: _(Lemma 12) Let \(_{f,}\) be a \((,^{})\)-approximation to \(S^{*}_{f,}\), for \(==\). Then, \(A(D)=f(D)+Lap(_{f,}(D)}{})\) is \((,+1}{2}+2^{})\)-differentially private._

**Approximate Smooth Sensitivity for Approximate Query.** We now show that approximate smooth sensitivity can be used even when \(f(G)\) is computed approximately, which becomes a bit more challenging.

Let \(A_{f}\) be \((_{1},_{1})\)-approximation of a function (query) \(f\) for a small constant \(_{1}<1/2\). Let \(_{f,}\) be \((,_{2})\)-upper approximation to \(S^{*}_{f,}\). We will show that we can utilize \(A_{f}\) and \(_{f,}\) to calculate a differentially private version of the function \(f\).

For the purpose of privacy analysis, we define the functions \(g_{f}\), \(s_{f}\), and \(S_{g_{f}}\) as below. Those functions may not be computed efficiently. However, they are only used for the analysis, and the actual algorithm does not compute them. The actual computation (Algorithm 3) will only utilize \(A_{f}\) and \(_{f,}\) to output the differentially private version of \(f\). We follow some of the analysiss of  to prove our privacy guarantees below. We share (with ) the same process of proving that \(S_{g_{f}}(D)\) is a smooth-upper bound of the local sensitivity (Smooth Sensitivity) of \(g_{f}\). The main difference is our analyses has to take into account the newly defined function \(s_{f}\)-which is a bounded variant of the Approximate Smooth Sensitivity (\(_{f,}\), which approximates \(S^{*}_{f,}\)), while  uses the global sensitivity \(GS_{f}\) of \(f\). As we can see in Lemma 13, the second condition requires \(S_{g_{f}}(D) e^{^{}}S_{g_{f}}(D^{})\) for any pair of neighbor datasets \(D\  D^{}\). While \(GS_{f}\) remains unchanged in both \(D\) and \(D^{}\), \(_{f,}\) may have different values for \(D\) and \(D^{}\) which makes it more difficult to analyze \(S_{g_{f}}\). We also have to take into account the small probability \(_{2}\) that \(_{f,}(D)\{S_{f,}(D),e^{}S_{f,}(D)\}\), that will add up in the \(\)-part of \((,)\)-DP of the final output.

**Definition 7**.: _Let \(A_{f}\) be an \((_{1},_{1})\)-approximation of \(f\). We define functions \(g_{f}\) and \(s_{f}\) as:_

\[g_{f} =A_{f}(D)(1-_{1})f(D) A_{f}(D) (1+_{1})f(D),\\ (1-_{1})f(D)A_{f}(D)<(1-_{1})f(D),\\ (1+_{1})f(D)A_{f}(D)>(1+_{1})f(D).\] (1) \[s_{f} =_{f,}(D)S_{f,}(D) _{f,}(D) e^{}S_{f,}(D),\\ S_{f,}(D)_{f,}(D)<S_{f,}(D),\\ e^{}S_{f,}(D)_{f,}(D)>e^{}S_{f,}(D). \] (2)

_Let \(S_{g_{f}}(D)=4_{1}g_{f}(D)+2s_{f}(D)\)_

**Lemma 3**.: _(Lemma 13) Given \(g_{f}\), \(s_{f}\), and \(S_{g_{f}}\) as defined above, \(S_{g_{f}}\) is a \(^{}\)-smooth upper bound of the local sensitivity of \(g_{f}\), where \(^{} 4_{1}++\) and \(_{1}<1/2\)._

In order to prove that \(S_{g_{f}}\) is a \(^{}\)-smooth upper bound of the local sensitivity of \(g_{f}\), we have to prove the two conditions: \(LS_{g_{f}}(D) Sg_{f}(D)\), and \(S_{g_{f}}(D) e^{^{}}S_{g_{f}}(D^{})\) for any pair of neighbor datasets \(D\  D^{}\). The full proof is in Lemma 13 in the SM. Theorem 1 below summarizes our result.

**Theorem 1**.: _(Theorem 4) Let \(A_{f}\) be a \((_{1},_{1})\)-approximation to \(f\), \(_{f,}\) be a \((,_{2})\)-upper approximation to \(S^{*}_{f,}\), for \(==\), \(_{1}=\). Then, \(A(D)=A_{f}(D)+Lap(A_{f}(D)+4_{f,}(D)}{})\) is \((,+1}{2}+2_{1}+2_{2})\)-differentially private._

### Application: fast private triangle counting

In this section, we study the problem of computing \(f_{}(G)\), the number of triangles in a graph \(G(E,V)\), by fast approximation of its smooth sensitivity. Recall the definitions of local sensitivity at distance \(t\), denoted by \(LS_{f}^{(t)}(G)\), defined in Section 3. We denote the local and smooth sensitivity of \(f_{}(G)\) by \(LS_{}^{(t)}(G)\) and \(S_{,}^{*}(G)\).

Let \(a_{ij}=_{k[n]}A_{ik}A_{jk}\) denote the number of common neighbors of nodes \(i\) and \(j\) in a graph and \(b_{ij}=_{k[n]}A_{ik} A_{jk}\) denote the number of nodes that are neighbors of \(i\) or \(j\) but not both. Then it has been shown that \(LS_{}^{(t)}(G)=_{i,j}c_{ij}(t)\) where \(c_{ij}(t)=(a_{ij}+)}{2},n-2)\) (Claim 3.13 of ).

We can rewrite this as \(LS_{}^{(t)}(G)=(_{i,j}(a_{ij}+}{2}+ ),_{i,j}a_{ij}+t,n-2)=(_{i,j}+,_{i,j}a_{ij}+t,n-2)\).

The bottleneck in computing \(LS_{}^{(t)}(G)\) (and \(S_{,}^{*}(G)\)) is estimating \(=_{i,j}a_{ij}\). We adapt the diamond sampling technique of  for this task. By doing that, we can quickly calculate \(_{}^{(t)}(G)=(_{i,j}+,+t,n-2)\) The core idea in diamond sampling is to find a "diamond" (a 4-cycle) of the form \((i,o,j,k)\) obtained by the intersection of two "wedges" (a 2-path) \((i,o,j)\) and \((i,k,j)\), as shown in Figure 1. Since any pair \((i,j)\) is part of \(a_{ij}^{2}\) diamonds, the probability of selecting such a diamond is proportional to \(a_{ij}^{2}\). This idea is formalized in Algorithm 4. Let \(W\) be the matrix \(W_{ki}=deg(k) deg(i)\) for all \((k,i) E\), and 0 for all other entries.

We first note that if \(=0\), it must be the case that \(G\) has no paths of length \( 2\), for if there is a 2-path \(i,k,j\), then \(a_{ij} 1\). Therefore, if \(=0\), we can determine \(LS_{}^{(t)}(G)\) exactly in time \(O(m)\). Therefore, we assume that \( 1\) in the rest of this section.

Even though Algorithm 4 can quickly estimate the quantity \( a_{ij}^{2}\) within a specific bound, it can only do so with the number of iterations that is sufficiently large-which its exact threshold is unknown without the access to the dataset and the value of interest (\( a_{ij}^{2}\)) itself (see Lemma 15). We overcome this issue with the following idea. First, we propose some estimation \(\) of \( a_{ij}^{2}\) and run an instance of Algorithm 4 with the number of iterations computed from \(\). Second, we compare the estimation of \( a_{ij}^{2}\) returned by Algorithm 4 with \(\) to determine if we over- or under-estimate \(\). Third, we adjust \(\) accordingly and repeat the process until we find a good value of \(\).

```
1:\(k:=0\), \(_{k}:=n^{2}\), \(:=\)
2:while TRUE do
3: Calculate \(s:=}\)
4:\(x:=Algorithm\;4(G,s)\)
5:if\(_{ij}x_{ij}<_{k}\)then
6:\(_{k+1}:=_{k}\)
7:\(k:=k+1\)
8:else
9: Return \(_{k}\)
10:endif
11:endwhile ```

**Algorithm 2** Algorithm to compute \(_{,}(G)\)

We implement Algorithm 1 based on this idea. In the algorithm, we first set \(\) to its largest possible value (\(n^{2}\)) knowing that we are over-estimating \(\) (\(> a_{ij}^{2}\)). We note that by over-estimating \(\), Algorithm 4 (line \(6\)) may not have enough iterations to guarantee the convergence of \( x_{ij}\) to \( a_{ij}^{2}\). Note that in the output of Algorithm 4, for any \(i,j\), \(x_{ij}\) is an approximation of \(a_{ij}^{2}\). We

Figure 1: Paths \((i,k,j)\) and \((i,o,j)\) represent wedges formed by nodes \(i,j\).

determine if we are truly over-estimating \(\) by applying Corollary 3 (in the SM), comparing the estimation \(_{ij}x_{ij}\) from Algorithm 4 in line \(6\) to \((3/2)\). The proof utilizes an idea that even though \( x_{ij}\) may not converge to \( a_{ij}^{2}\) and we may not determine it directly, \( x_{ij}\) can converge to \(\) once we under-estimate \(\) (i.e., \(< a_{ij}^{2}\)) and that we can check this convergence easily. Corollary 3 states that when \( x_{ij}<(3/2)\), \( a_{ij}^{2}<3\) which means we should lower \(\). We keep running Algorithm 4 and reducing \(\) by \(3/4\) in each iteration until the estimation of \( x_{ij}\) lies above \((3/2)\). By Corollary 3, we show that when it happens, \(_{k} a_{ij}^{2}\) that guarantees the convergence of \( x_{ij}\) to \( a_{ij}^{2}\). Lemma 4 shows that when the algorithm outputs \(_{k}\), w.h.p., the true value of interest \( a_{ij}^{2}\) is within \(_{k}\) and \(4_{k}\).

**Lemma 4**.: _(Lemma 18 in SM) Let \(_{k}\) be the output of Algorithm 1. With probability at least \(1-1/n^{c-3}\), \(_{k}<_{ij}a_{ij}^{2}<4_{k}\)._

Algorithm 2 calculates an approximation of \( a_{ij}^{2}\) for the Approximated Smooth Sensitivity with any \(\) (which is required for Differential Privacy with an arbitrary \(\)). Since \(_{k}\) is guaranteed to be close to the true value of \( a_{ij}^{2}\) (within a constant factor of \(4\)), we run the last instance of Algorithm 4 with some constant \(\), which is derived from the target approximation factor \(\). Lemma 5 shows that the estimation output by Algorithm 4 in line \(6\) falls within the range of \((1-,1+)\) of the true value. Following that, Theorem 2 guarantees that the quantity output by Algorithm 2 is a \((,)\)-upper approximation to Smooth Sensitivity \(S_{,}(G)\) of the triangle count of graph \(G\).

**Lemma 5**.: _(Lemma 19 in SM) With probability at least \(1-\) in Algorithm 2, \(_{ij}x_{ij}(1-,1+)_{ij}a_{ij}^{2}\)._

**Theorem 2**.: _(Theorem 5, 6 in SM) Suppose \(_{ij}a_{ij}>0\). Algorithm 2 outputs \(_{,}(G)\), which is a \((,)\)-upper approximation to \(S_{,}(G)\), in \(O(()\|W\|_{1} m n}{( +1})^{2}_{ij}a_{ij}^{2}}+m++n)\) time._

Proof.: (Sketch) Algorithm 1 has a run-time of \(kO(s^{} m)\) where \(s^{}=O(\|W\|_{1}c n/(^{2}))\) as it executes Algorithm 4 with varying \(s=s^{}\) and \(k\) is the number of search steps. Since at the end of the search \(_{k}>_{ij}a_{ij}^{2}/4\) w.h.p., we know that \(k=O(}{_{ij}a_{ij}^{2}})\). In the first step, we start with \(_{0}=n^{2}\) and reduce \(_{k}\) by a factor of \(3/4\) after each step. The total running time will be \(O(\|W\|_{1} m}} (1+(3/4)^{-1}+(3/4)^{-k})=O(c m n}{^{ 2}_{ij}a_{ij}^{2}})\). Finally, Algorithm 2 has a run-time of \(O((2)\|W\|_{1} m n}{(+1})^{2}_{ij}a_{ij}^{2}}+m+ {}+n)\) taking into account the time required to compute \(W\) (\(O(m)\)) and \(_{,}(G)\) (\(O((n)/+n)\)) and substituting the values of \(c\) and \(\) as described in the Algorithm. 

**Corollary 1**.: _Suppose \(_{ij}a_{ij}>0\), \(G\) is \((C,)\)-transitive (as in Definition 5), and \( n^{-2}\). Then, for any parameter \(>0\). Algorithm 2 outputs \(_{,}(G)\), which is a \((,)\)-upper approxmation to \(S_{,}(G)\), in \(O(m(1/C^{2},^{2})) m n/(^{4},1/4)+n\)) time._

Proof.: Since \((-1}{e^{2}+1})^{2}>^{4}\) for \(0<<1/\), we have \((-1}{e^{2}+1})^{2}>(^{4},1/4)\). Next, for any node \(i\) and \(j_{i}\), we have \(j^{}}a_{i^{}j^{}}}\{ ,\}\): if \(deg(j)>\), it must be the case that at least \(C\)-fraction of the paths \(i,j,k\) for \(k_{j}\) are closed, which means \(a_{ij} C deg(j)\), and so \(j^{}}a_{i^{}j^{}}}\); if \(deg(j)\), we have \(j^{}}a_{i^{}j^{}}}\). Therefore, \(j}a_{ij}^{2}}=_{i V(G)}_{j _{i}}j^{}}a_{i^{}j^{ }}}=_{i V(G)}j^{}}a_{i^{}j ^{}}}_{j_{i}}j^{}}a_{ i^{}j^{}}}_{i V(G)}_{j_{i}}( },^{2})=2m(},^{2})\), the Corollary follows. 

## 5 Higher-order local sensitivity and improved bounds for path counting

As mentioned earlier in Section 2, Karwa et al.  develop a different technique involving local sensitivity of the local sensitivity, and use it for private counts of \(k\)-cliques, since it seems hard to estimate smooth sensitivity for such subgraphs. We generalize it, and develop a multi-level local sensitivity approach, extending , for privately counting the number of copies of any subgraph \(H\); as in the case of smooth sensitivity, we show that this also works with approximate queries.

Let \(\) denote the number of edges in \(H\). Our main idea involves finding private bounds for higher-order local sensitivity. Let \(L=\{\{i,j\}:i,j V\}\) denote all possible pairs of nodes in \(V\). For subgraph \(H\), and a subset \(S\{\{i,j\}:i,j V\}\) of pairs of nodes, let \(f_{H}(G,S)\) denote the number of embeddings of \(H\) in the graph \(G(V,E S)\), which _contain all the edges corresponding to pairs of nodes in \(S\)_. For \(S=\), \(f_{H}(G,)\) is the number of embeddings of \(H\) in \(G\). We drop the subscript \(H\) when it is clear from the context. Let \(f^{(k)}(G)=_{|S|=k}f(G,S)\); note that \(f^{(0)}(G)=f(G)\). It can be seen that \(LS(f^{(k)}) f^{(k+1)}(G)\), which is the basis for considering higher-order local sensitivity for subgraph counting. However, a careful analysis of the privacy bounds becomes involved. Our approach is described in Algorithm 5 in the supplementary material, and the main result is summarized below.

**Theorem 3**.: _Let \(g^{(k)}(G)=f^{(k)}(G)+g^{(k+1)}(G)}{_{2}}+Lap(g^{( k+1)}(G)/_{2})\), for \(k=-1,,0\) as computed in Algorithm 5. Then, \(g^{(0)}(G)\) is an \(((k_{m}+1)_{2},_{2}+(k_{m}+1)e^{_{2}}_{2})\)-DP estimate of \(f(G)\)._

The above result, and the definition of \(f^{(k)}(G)\) implies that for any subgraph with \(\) edges, the higher-order local sensitivity approach can be run in time \(O(n^{2})\). This can be improved further for paths and trees, as summarized below.

**Lemma 6**.: _Private counting for a graph \(H\) with a constant \(\) number of edges using higher-order local sensitivity (Algorithm 5) has running time \(O(n^{2})\). When \(H\) is a path or tree with \(\) edges, Algorithm 5 has running time \(O(n^{+1})\)._

**Higher-order local sensitivity with approximate queries.** We show that our method works even if we have a very good approximation \((x)\) to \(f(x)\), instead of the exact value (Theorem 7 in the supplementary material). This allows us to use \((x)\) instead of \(f(x)\) in Algorithm 5.

**Improved bounds for path counting in graphs with bounded degeneracy and treewidth.** We say that \(G\) has degeneracy \(d\), if the nodes can be ordered so that each node has at most \(d\) neighbors with higher index . Informally, \(G\) has treewidth \(t\) if it has recursive balanced separators of size \(t\); see  for details. There has been a lot of work showing that non-private algorithms for subgraph counting can be done efficiently when \(G\) has either of these parameters bounded, e.g., . We show that if \(G\) has degeneracy \(d\) and treewidth \(t\), then the higher-order local sensitivity of paths of length \(k\) can be computed in time \(O(h(d,t,k)poly(n))\), where \(h(d,t,k)\) is independent of \(n\); thus the running time does not scale as \(n^{k}\).

Our algorithm and analysis are summarized in Section E in the supplementary material (Theorem 8). Recall that the goal is to compute \(f^{(k)}(G)=_{S:|S| k}f(G,S)\). We summarize the main ideas here. We show that a feasible solution, which involves fixing a subset \(S\) of pairs of nodes, can be viewed as a tuple of at most \(k\) disjoint subgraphs \((H_{i},D_{i})\), referred to as \(k\)-compact subgraphs. This is

Figure 2: Examples of compact subgraphs and anchor edges in the algorithm for computing higher-order sensitivity of paths: (Left) \(D=\{r_{1},r_{2},r_{3},r_{4}\}\), edges \((r_{1},r_{1}^{})\), \((r_{2},r_{2}^{})\), \((r_{3},r_{3}^{})\) and \((r_{4},r_{4}^{})\) are external anchor edges, while \((r_{5},r_{5}^{})\) is an internal anchor edges. Nodes \(s\) and \(t\) are the starting and ending nodes of the path; (Middle) \(D=\{r_{1},r_{2},r_{3}\}\), and \((r_{1},r_{1}^{})\), \((r_{2},r_{2}^{})\), \((r_{3},r_{3}^{})\) are external anchor edges, while \((r_{5},r_{5}^{})\) is an internal anchor edge. Node \(s\) is the starting node of the path, and the path ends at some other node, not in this compact subgraph; (Right) \(D=\{r_{1},r_{2},r_{3},r_{4}\}\), edges \((r_{1},r_{1}^{})\), \((r_{2},r_{2}^{})\), \((r_{3},r_{3}^{})\) and \((r_{4},r_{4}^{})\) are external anchor edges, while \((r_{5},r_{5}^{})\) and \((r_{6},r_{6}^{})\) are internal anchor edges. Paths pass through this compact subgraph, without starting or ending in it.

defined to be a subgraph which satisfies the following conditions (Figure 6): (1) \(H=G_{}(v,k,F^{(i)})\) for some node \(v\) and subset \(F^{(i)}=\{e_{1},,e_{i}\}\), where \(G_{}\) is the \(k\)-hop graph starting at node \(v\), only restricted to higher degeneracy order nodes than \(v\), (2) the \(k\)-hop neighborhood is expanded as edges in \(F^{(i)}\) (referred to as internal anchor edges) are added, one at a time, and (3) \(D\) is a subset of vertices in \(H\), and is referred to as the set of connectors. The compact subgraphs are connected via edges between connector nodes.

We show that paths using edges from \(S\) (which contribute to the count \(f(G,S)\)) can be viewed to consist of segments within these compact subgraphs, passing through internal anchor edges, as well as edges between connectors (which are also from \(S\)). We guess the number of path segments between connectors within a factor of \((1+1/( n)^{c})\) for a constant \(c\), which allows us to search for a tuple of compact subgraphs with the corresponding approximate counts. We show that the number of such tuples is \(O(h(d,k)poly(n))\), and existence of given tuple can be determined in a similar time.

## 6 Experimental Results

We evaluate our algorithm for privately computing #triangles using _approximate_ smooth sensitivity (Algorithm 2), compared with the _exact_ version of smooth sensitivity and other baselines.

**Datasets.** We consider different real-world networks from  as inputs. The networks have have sizes range from \(10K-300K\) nodes, with one of the largest ones having over 2 million edges. Statistics of the networks are presented in Table 2. Due to space limit, we only present the results for some selected networks here. The full results are presented in the SM.

**Infrastructure.** All algorithms are implemented in C++ and OpenMP framework for parallelization. We ran our experiments on a system with a 48-core Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz and 1.5TB RAM and limit the number of parallel threads in all experiments to 40.

**Baselines.** We implement two baseline methods: the exact smooth sensitivity of triangle count to calibrate Laplacian noise (denoted in our experiment as Karwa et al.) , and using the ladder function method (Zhang et al.) .

**Metrics for evaluating performance.** For each input network \(G\), we calculate the following metrics: \(\) Accuracy of the private triangle counts, defined as Triangle Count Relative Error = \(}(G)-f_{}(G)|}{f_{}(G)}\), where \(M_{f_{}}\) is the tested private mechanism to output \(f_{}(G)\).

\(\) Speeded of Algorithm 2, which compares its running time with the time required for  Speedup = \(G}{G}\) (Figure 5).

\(\) Approximation error of Algorithm 2 for estimating smooth sensitivity, defined as Sensitivity Approximation Error = \((G)-S_{}(G)}{S_{}(G)}}{S_{}(G)}\) (Figure 4).

**Parameters.** We test our algorithms at different privacy budgets \(\{2^{\{-3,-2,-1,0,1\}}\}\) and a fixed \(=10^{-6}\). We report the average accuracy and runtime of five (5) repeats of the private triangle count via approximate smooth sensitivity experiments due to the randomness used in the algorithm. For the exact calculation, we calculate the smooth sensitivity once and re-sample the noise in each run, since

  Network & Description & \#nodes & \#edges & \#triangles \\  Oregon-1 & AS peering information-Oregon route-views & 10,670 & 22,002 & 17,145 \\ ca-HepTh & Collaboration network-Arxiv High Energy Physics & 9,877 & 51,971 & 28,339 \\ ca-GrQc & Collaboration network-Arxiv General Relativity & 5,242 & 14,496 & 48,620 \\ Oregon-2 & AS peering information-Oregon route-views & 10,900 & 31,180 & 82,857 \\ ca-CondMat & Collaboration network-Arxiv Condensed Matter & 23,133 & 186,936 & 173,361 \\ loc-Brightkite & Brightkite location based online social network & 58,228 & 428,156 & 494,728 \\ com-Amazon & Amazon product network & 334,863 & 925,872 & 667,129 \\ email-Enron & Email communication network- Enron & 36,692 & 367,662 & 727,044 \\ ca-AstroPh & Collaboration network-Arxiv Astro Physics & 18,772 & 198,110 & \(1.35 10^{6}\) \\ com-DBLP & DBLP collaboration network & 317,080 & 2,099,732 & \(2.22 10^{6}\) \\ loc-Gowalla & Gowalla location based online social network & 196,591 & 950,327 & \(2.27 10^{6}\) \\ ca-HepPh & Collaboration network-Arxiv High Energy Physics & 12,008 & 237,010 & \(3.36 10^{6}\) \\  

Table 2: Statistics of tested networks.

the exact calculation of the sensitivity is deterministic and does not change for each setting. We use the true counts of triangles of the networks as reported by .

**Experimental Results.** The results show that our algorithm can achieve similar levels of accuracy to the exact calculation while being several orders of magnitude faster. Figure 3 shows that when using approximate smooth sensitivity, the private triangle counts reach the accuracy of using the exact calculation of the smooth sensitivity across different privacy budgets in all but two networks (Oregon 1 and Oregon 2). In these two networks, the smooth sensitivities are relatively large in comparison with the triangle counts, which makes the noise fluctuate more for the approximate estimate (see Figure 8).

Figure 4 shows that all approximate smooth sensitivity function (Zhang et al.) has tested networks, except smooth sensitivities. Noter that approximate smooth sensitivities are always larger than the true smooth sensitivity. It is important since a lower value may expose the privacy due to an inadequate noise magnitude. In general, a smaller value of \(\) (higher privacy guarantee) requires a more accurate estimation of the sensitivity. It is illustrated in Figure 4 as the approximate smooth sensitivity is close to the exact smooth sensitivity in all networks at \(=0.125\) or \(=0.25\).

Figure 5 shows that our approximation algorithm is orders of magnitude faster than the exact algorithm. In large graphs (Amazon, DBLP, Gowalla), the speedup may reach \(1,000\)-fold. Generally, a lower \(\) requires a smaller approximation factor (\(\)), which in turn requires a larger number of iterations to reduce the error in approximation. The majority of tested networks have speedup factors between \(10\) and \(100\)-fold across all privacy budgets \(\).

## 7 Conclusions and future work

We give significant improvement in the running time for privately counting the number of embeddings of constant size subgraphs in a graph, without using noise based on global sensitivity. Despite a lot of work on private subgraph counting, efficient algorithms were not known for many subgraphs. Our results for triangles show significant benefits of our approach, by improving on the runtime over all prior private algorithms. Our approach of using approximations to sensitivity metrics opens the possibility of using other techniques from graph sampling and sketching to obtain more efficient algorithms. For general subgraphs, our focus here has been on theoretical bounds using multi-level local sensitivity combined with approximate queries. Developing efficient and practical algorithms for these problems is a good direction for future work.

Figure 4: Error factor in approximate smooth sensitivity output by Algorithm 2 (relative to the exact smooth sensitivity) on selected networks. Dotted lines indicate the theoretical upper bound factor \(e^{}\) (guaranteed by Theorem 2).

Figure 5: Speedup of Algorithm 2 over the exact smooth sensitivity (Karwa et al.), and the ladder smooth sensitivity function (Zhang et al.) has tested networks, except the exact smooth sensitivity.

Figure 3: Triangle Count Relative Error, showing the accuracy of the private triangle count with noise calibrated by (1) our approximate smooth sensitivity (Algorithm 2), (2) the exact smooth sensitivity (Karwa et al.), and (3) the ladder function (Zhang et al.)