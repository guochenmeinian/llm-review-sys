# A Hierarchical Training Paradigm for Antibody Structure-sequence Co-design

Fang Wu

Tsinghua University

Beijing, China

&Stan Z. Li

Westlake University

Hangzhou, China

Corresponding Authors, emails: fw2359@columbia.edu.

###### Abstract

Therapeutic antibodies are an essential and rapidly expanding drug modality. The binding specificity between antibodies and antigens is decided by complementarity-determining regions (CDRs) at the tips of these Y-shaped proteins. In this paper, we propose a hierarchical training paradigm (HTP) for the antibody sequence-structure co-design. HTP consists of four levels of training stages, each corresponding to a specific protein modality within a particular protein domain. Through carefully crafted tasks in different stages, HTP seamlessly and effectively integrates geometric graph neural networks (GNNs) with large-scale protein language models to excavate evolutionary information from not only geometric structures but also vast antibody and non-antibody sequence databases, which determines ligand binding pose and strength. Empirical experiments show that HTP sets the new state-of-the-art performance in the co-design problem as well as the fix-backbone design. Our research offers a hopeful path to unleash the potential of deep generative architectures and seeks to illuminate the way forward for the antibody sequence and structure co-design challenge.

## 1 Introduction

Antibodies, known as immunoglobulins (Ig), are large Y-shaped proteins that the immune system uses to identify and neutralize foreign objects such as pathogenic bacteria and viruses . They recognize a unique molecule of the pathogen, called an antigen. As illustrated in Figure 1 (a), each tip of the "Y" of an antibody contains a paratope that is specific for one particular epitope on an antigen, allowing these two structures to bind together with precision. Notably, the binding specificity of antibodies is largely determined by their complementarity-determining regions (CDRs). Consequently, unremitting efforts have been made to automate the creation of CDR subsequences with desired constraints of binding affinity, stability, and synthesizability . However, the search space is vast, with up to \(20^{L}\) possible combinations for a \(L\)-length CDR sequence. This makes it infeasible to solve the protein structures and then examine their corresponding binding properties via experimental approaches. As a remedy, a group of computational antibody design mechanisms has been introduced to accelerate this filtering process.

Some prior studies  prefer to generate only 1D sequences, which has been considered suboptimal because it lacks valuable geometric information. Meanwhile, since the target structure for antibodies is rarely given as a prerequisite , more attention has been paid to co-designing the sequence and structure. One conventional line of research  resorts to sampling protein sequences and structures on the complex energy landscape constructed by physical and chemical principles. But it is found to be time-exhausted and vulnerable to being trapped in local energy optima. Another line  relies on deep generative models to simultaneously design antibodies' sequences and structures. They take advantage of the most advanced geometric deep learning (DL) techniques and can seizehigher-order interactions among residues directly from the data . Their divergence mainly lies in their generative manner. For instance, early works [10; 11] adopt an iterative fashion, while successors [14; 15] employ full-shot generation. Most utilize the traditional translation framework, while some  leverage the diffusion denoise probabilistic model.

Despite this fruitful progress, the efficacy of existing co-design methods is predominantly limited by the small number of antibody structures. The Structural Antibody Database (SAbDab) and RAbD are two widely used datasets in the field. After eliminating structures without antigens and removing duplicates, SAbDab comprises only a few thousand complex structures, whereas RAbD consists of 60 complex structures. These numbers are orders of magnitude lower than the data sizes that can inspire major breakthroughs in DL areas [17; 18]. Consequently, deep-generative models fail to benefit from large amounts of 3D antibody-antigen complex structures and are of limited sizes, or otherwise, overfitting may occur.

To address this issue, in this paper, we propose a hierarchical training paradigm (HTP), a novel unified prototype to exploit multiple biological data resources, and aim at fully releasing the potential of geometric graph neural networks (GGNNs) [19; 20] for the sequence and structure co-design problem. Explicitly, HTP consists of four distinct training ranks: single-protein sequence level, antibody sequence level, protein-protein complex structure level, and antibody-antigen complex structure level. These steps are itemized as the data abundance declines, but the task specificity increases, as depicted in Figure 1 (b). Alongside them, we present various pretraining objectives for the first three levels to mine correlated evolutionary patterns, which benefit the co-design task in the final stage. Specifically, we first pretrain the protein language models (PLMs) on tremendous single-protein sequences to obtain general representations and fine-tune them on antibody sequences to capture more condensed semantics. For 3D geometry, we invented a pocket-painting task to exploit the protein-protein complex structures and simulate the CDR generation process. After that, we combine the marvelously pretrained PLMs and the well-pretrained GGNNs to co-design the expected antibodies. Our study provides a promising road to excavate the power of existing deep generative architectures and hopes to shed light on the future development of antibody sequence and structure co-design. The contributions of our work can be summarized as follows.

* First, we equip GGNNs with large-scale PLMs to bridge the gap between protein databases of different modalities (_i.e._, 1D sequence, and 3D structure) for the antibody co-design problem.
* Second, we design four distinct levels of training tasks to hierarchically incorporate protein data of different domains (_i.e._, antibodies, and non-antibodies) for the antibody co-design challenge. HTP breaks the traditional co-design routine that separates proteins of different domains and extends antibody-antigen complex structures to broader databases.
* Comprehensive experiments have been conducted to indicate that each stage of HTP significantly contributes to the improvements in the capacity of the DL model to predict more accurate antibody sequences and restore its corresponding 3D structures. To be explicit, HTP brings a rise of 78.56% in the amino acid recovery rate (AAR) and a decline of 41.97% in structure prediction error for the sequence-structure co-design. It also leads to an average increase of 26.92% in AAR for the fixed-backbone design problem.

Figure 1: (a) Schematic structure of an antibody bonded with an antigen (figure modified from Wikipedia). (b) The workflow overview of our hierarchical training paradigm (HTP).

## 2 Methods

This section is organized as follows. Subsection 2.1 describes the background knowledge and mathematical formulation of the co-design problem. Subsection 2.4 introduces the backbone architecture to encode the geometric structure of protein-protein complexes as well as antibody-antigen complexes in the 3D space. Subsection 2.2 to 2.6 concentrate on explaining the four different levels of our HTP.

### Preliminary

BackgroundAn antibody is a Y-shaped protein with two symmetric sets of chains, each consisting of a heavy chain and a light chain. Each chain has one variable domain (VH/VL) and some constant domains. The variable domain can be further divided into a framework region (FW) and three CDRs. Notably, CDRs in heavy chains contribute the most to the antigen-binding affinity and are the most challenging to characterize.

Notations and Task FormulationWe represent each antibody-antigen complex as a heterogeneous graph \(_{LR}\). It is made up of two spatially aggregated components, _i.e._, the antibody and antigen denoted as \(_{L}=\{_{L},_{L}\}\) and \(_{R}=\{_{R},_{R}\}\), respectively. \(_{L}\) and \(_{R}\) use residues as nodes with numbers of \(N_{L}\) and \(N_{R}\) separately. The node locations \(_{L}^{N_{L} 3}\) and \(_{R}^{N_{R} 3}\) are defined as their corresponding \(\)-carbon coordinate, and are associated with the initial \(_{h}\)-dimension root-translation invariant features \(_{L}^{N_{L}_{h}}\) and \(_{R}^{N_{R}_{h}}\) (_e.g._ residue types, electronegativity). CDRs are subgraphs of \(_{L}\) and can be divided into \(_{HC}=\{_{HC},_{HC}\}\) and \(_{LC}=\{_{LC},_{LC}\}\), which belong to the heavy chain and the light chain, respectively. We assume that \(_{HC}\) and \(_{LC}\) have \(N_{HC}\) and \(N_{LC}\) residues. Besides, it is worth noting that the distance scales between the internal and external interactions are very different. Based on this fact, we strictly distinguish the interaction within and across two graphs \(_{R}\) and \(_{L}\) as and \(_{L}_{R}\) and \(_{LR}\), individually. This implementation avoids the underutilization of cross-graph edges' information due to implicit positional relationships between the antibody and the antigen .

Figure 2: The illustration of the first three stages of our hierarchical training mechanism for antibody sequence-structure co-design. In **level I**, a Transformer-based language model is trained by masked language modeling on a large number of single protein sequences to extract general-purposed representations. In **level II**, the Transformer-based PLM is then further fine-tuned on specific antibody sequences from databases like OAS or ABCD. CDRs are all masked and require recovery, with other framework regions reserved. In **level III**, GGNNs are asked to predict both the sequence and structure of the pseudo-CDR on protein-protein complex structure databases. The residue features are based on PLMs gained in the previous step.

In this work, we hypothesize that the antigen structure and the antibody framework are known, aiming to design CDR with more efficacy. Formally, our goal is to jointly model the distribution of CDR in the heavy chain given the structure of the remaining antibody-antigen complex as \(p(_{HC}|_{LR}-_{HC})\) or in the light chain as \(p(_{LC}|_{LR}-_{LC})\). Since the heavy chain plays a more critical role in determining antigen binding affinity, we take \(_{HC}\) as the target example in the following content, but design both heavy and light chains in the experiment section 3.1.

### Single-protein Sequence Level

The idea that biological function and structures are recorded in the statistics of protein sequences selected through evolution has a long history . Unobserved variables that determine the fitness of a protein, such as structure, function, and stability, leave a mark on the distribution of the natural sequence observed . To uncover that information, a group of PLMs has been developed at the scale of evolution, including the series of ESM  and ProtTrans . They are capable of capturing information about secondary and tertiary structures and can be generalized across a broad range of downstream applications. Recent studies  also demonstrate that equipping GGNNs with pretrained language models can end up with a stronger capacity. Accordingly, we adopt an ESM-2 with 150M parameters to extract per-residue representations, denoted as \(_{L}^{}^{N_{L}_{PLM}}\) and \(_{R}^{}^{N_{R}_{PLM}}\), and use them as input node features. Here \(_{PLM}=640\) and we use \(\) to denote the trainable parameter set of the language model.

Noteworthily, ESM-2 supplies plenty of options with different model sizes, ranging from 8M to 15B, and a persistent improvement has been found as the model scale increases. However, the trajectory of improvement becomes relatively smooth after the scale reaches \(10^{8}\). Therefore, for the sake of computational efficiency, we select the 150M version of ESM-2, which performs comparably with the 650M parameters of the ESM-1b model . As declared by Wu et al. , incompatibility exists between the experimental structure and its original amino acid sequence (_i.e._, FASTA sequence). For simplicity, we obey Wu et al. 's mode, which uses the fragmentary sequence directly as the substitute for the integral amino acid sequence and forwards it to the PLMs.

### Antibody Sequence Level

Though PLMs have achieved great progress within protein informatics, their protein representations are generally purposed. Noticeably, the residue distribution of antibodies is significantly different from non-antibodies. Over the past decades, billions of antibodies have been sequenced , which enables the training of a language model specifically for the antibody domain . Several researchers have recognized this problem and present models such as AntiBERTa  and AbLang  to decipher the biology of disease and the discovery of novel therapeutic antibodies. Nonetheless, those pretrained antibody language models have some intrinsic flaws and may not be suitable to apply in our sequence-structure co-design problem immediately.

First and foremost, they are directly pretrained on antibody sequence datasets and fail to exploit the vast amounts of all protein sequences. Second, the existing pretrained antibody language models are all on a small scale. For example, AntiBERTa consists of 12 layers with 86M parameters. Last but not least, both AntiBERTa and AbLang regard the heavy and light chains individually, and AbLang even trains two different models for them. This approach is precluded from encoding a comprehensive and integral representation of the whole antibody. More importantly, they are pretrained on antibodies and cannot be perfectly generalized to extract representations of antigens, which is necessary for analyzing the interactions between antibodies and antigens.

To avoid their drawbacks, we chose to fine-tune ESM on available antibody sequence datasets and considered both antibody and antigen sequences. Following AbLang , we leverage the Observed Antibody Space database (OAS)  and subsequent update . OAS is a project that collects and annotates immune repertoires for use in large-scale analysis. It contains over one billion sequences, from over 80 different studies. These repertoires cover diverse immune states, organisms, and individuals. Additionally, Olsen et al.  has observed in OAS that approximately 80% sequences lack more than one residue at the N-terminus, nearly 43% of them are missing the first 15 positions, and about 1% contain at least one ambiguous residue for each sequence. Remarkably, OAS contains both unpaired and paired antibody sequences and we utilize only paired ones. To better align with our co-design target, we implement masked language modeling (MLM) and mask residues in all CDRs (_i.e._, VH, and VL) simultaneously to increase the task difficulty.

### Geometric Graph Neural Networks

To capture 3D interactions of residues in different chains, we adopt a variant of equivariant graph neural network (EGNN)  to act on this heterogeneous 3D antibody-antigen graph. The architecture has several key improvements. First, it consists of both the _intra_- and _inter_- message-passing schemes to distinguish interactions within the same graph and interactions between different counterparts. Second, it only updates the coordinates of residues in CDR, _i.e._, the part that is to be designed, while the positions of other parts are maintained as unchangeable. Last, it uses features from the pretrained PLMs as the initial node state rather than a randomized one. Here, all modules are E(3)-equivariant.

The \(l\)-th layer of our backbone is formally defined as the following:

\[_{j i}= \,_{e}(_{i}^{(l)},_{j}^{(l)},d (_{i}^{(l)},_{j}^{(l)})), e_{ij} _{L}_{R},\] (1) \[_{j i}= \,a_{j i}_{j}^{(l)}_{d}(d( _{i}^{(l)},_{j}^{(l)})), e_{ij} _{LR},\] (2) \[_{i}^{(l+1)}= \,_{h}(_{i}^{(l)},_{j}_{j i },_{j^{}}_{j^{} i}),\] (3)

where \(d(.,.)\) is the Euclidean distance function. \(_{e}\) is the edge operation and \(_{h}\) denotes the node operation that aggregates the _intra_-graph messages \(_{i}=_{j}_{j i}\) and the cross-graph message \(_{i}=_{j^{}}_{j^{} i}\) as well as the node embeddings \(_{i}^{(l)}\) to acquire the updated node embedding \(_{i}^{(l+1)}\). \(_{d}\) operates on the inter-atomic distances. \(_{e}\), \(_{h}\) and \(_{d}\) are all multi-layer perceptrons (MLPs). Besides that, \(a_{j i}\) is an attention weight with trainable MLPs \(^{q}\) and \(^{k}\), and takes the following form as:

\[a_{j i}=(_{i}^{(l)} ),^{k}(_{j}^{(l)}))}{_{ j^{}}(^{q}(_{i}^{(l)}), ^{k}(_{j^{}}^{(l)}))}.\] (4)

As for coordinate iterations, we note that residues located in CDRs (_i.e._, \(_{HC}\)) are the sole constituent that needs spatial transformation. On the contrary, the position of the remaining piece (_i.e._, \(_{LR}-_{HC}\)) is ascertainable. If we change the coordinate of \(_{LR}-_{HC}\), its conformation can be disorganized and irrational from physical or biochemical perspectives. Therefore, it is reasonable to stabilize \(_{LR}-_{HC}\) in each layer and simply alter \(_{HC}\). Mathematically,

\[_{i}^{(l+1)}=_{i}^{(l)}+_{i}|}_{i_{i}}(_{j}^{(l)}-_{j}^{( l)})_{x}(i,j),&_{i}_{HC}\\ _{i}^{(l)},&,\] (5)

where \(_{i}\) denotes the neighbors of node \(i\) and we take the mean aggregation to update the coordinate for each movable node. \(_{x}\) varies according to whether the edge \(e_{ij}\) represent _intra_-graph connectivity or cross-graph connectivity. In particular, \(_{x}=_{m}(_{i j})\) if \(e_{ij}_{L}^{(t)}_{R}^{(t)}\). Otherwise, \(_{x}=_{}(_{i j})\) when \(e_{ij}_{LR}^{(t)}\), where \(_{m}\) and \(_{}\) are two different functions to deal with different types of messages. Then, \(_{x}\) is left multiplied with \(_{i}^{(l)}-_{j}^{(l)}\) to keep the direction information. Equation 5 takes as input the edge embedding \(_{i j}\) or \(_{i j}\) as a weight to sum all relative coordinate \(_{i}^{(l)}-_{j}^{(l)}\) and output the renewed coordinates \(_{i}^{(l+1)}\).

To summarize, the \(l\)-th layer of our architecture (\(l[L]\)) takes as input the set of atom embeddings \(\{_{L}^{(l)},_{R}^{(l)}\}\), and 3D coordinates \(\{_{L}^{(l)},_{R}^{(l)}\}\). Then it outputs a transformation on \(\{_{L}^{(l)},_{R}^{(l)}\}\) as well as coordinates of residues on the CDRs, that is, \(_{HC}^{(l)}\). Concisely, \(_{L}^{(l+1)},_{HC}^{(l+1)},_{R}^{(l+1)}=^{(l)}(_{L}^{(l)},_{L}^{(l)},_{R}^{(l)},_{R}^{(l)})\), while the coordinates of other non-CDR parts remain the same as in the last layer as \(_{L}^{(l+1)}_{R}^{(l+1)}_{HC}^{(l +1)}=_{L}^{(l)}_{R}^{(l)}_{HC}^{(l)}\). We assign \(\) as the trainable parameter set of the whole GGNN architecture.

### Protein-protein Complex Structure Level

Apart from empowering PLMs with tremendous protein sequences, plenty of protein-protein complex structures stay uncultivated, which can be harnessed to promote the geometric backbone architecture. Despite the emergence of several structure-based pretraining methods in the biological domain [21; 34], it is not trivial to apply them to our antibody design problem because all previous studies focus on single protein structures. In order to enable GGNNs to encode the general docking pattern between multiple proteins, we propose the pocket inpainting task.

Specifically, we used the Database of Interacting Protein Structures (DIPS) . DIPS is a much larger protein complex structure dataset than existing antibody-antigen complex structure datasets and is derived from the Protein Data Bank (PDB) . In DIPS, each complex \(_{LR}\) has two sub-units \(_{L}\) and \(_{R}\). We calculate the distance between each amino acid of these two substructures and select residues whose minimum distance to the counterpart substructure is less than the threshold \(_{P}=8\) as the pocket \(_{P}\). Mathematically, the pocket nodes \(_{P}\) is written as follows:

\[_{P}=\{v_{L,i}_{j=1}^{N_{L}}d(_{Li}, _{R,j})<_{P}\}\{v_{R,i}^{ N_{R}}d(_{R,i},_{L,j})<_{P}}\},\] (6)

then our target is to retrieve \(_{P}\) given the leftover \(_{LR}-_{P}\). Here, We follow the official split based on PDB sequence clustering at a 30% sequence identity level to ensure little contamination between sets. It results in train/val/test of 87,303/31,050/15,268 complex samples.

### Antibody-antigen Complex Structures Level

After the preceding levels of preparation, it is time to co-design the antibody. Given an antigen \(_{R}\) and a fractional antibody \(_{L}-_{HC}\), we first employ the well-trained PLM \(\) to attain per-residue node features \(_{R}^{(0)}\) and \(_{L}^{(0)}\), where the nodes in the unknown part \(_{HC}\) are tagged as a masked token. Then both features and coordinates are fed into the well-trained geometric encoder to unravel the CDR sequences and structures in a concurrent way, _i.e._, \(_{L}^{(L)}\), \(_{HC}^{(L)},_{R}^{(L)}=_{}(_{L}^{(0)},_{L}^{(0)},_{R}^{(0)},_{R}^{(0)})\). Eventually, we use an MLP \(_{o}\) and a Softmax operator as the classifier to output the probability distribution of residue types as \(p_{i}=Softmax(_{o}(_{i}^{(L)}))^{20}\) for \(v_{i}_{HC}\).

CDR Coordinates Initialization.How to initialize the positions of residues in CDRs is of great importance to the co-design problem, and there is no consensus among different approaches. HERN  tries two kinds of strategies. One strategy is to randomly initialize all coordinates by adding a small Gaussian noise around the center of the epitope as \(_{i}^{(0)}=}_{j_{R}}_{j}+ ,(0,1)\). The other is to predict the pairwise distance instantly \(^{(N_{L}+N_{R})(N_{L}+N_{R})}\) between parat

Figure 3: The final stage of our hierarchical training mechanism for antibody sequence-structure co-design. In **level IV**, both the pretrained language model and geometric graph neural networks are employed to implement the co-design task with the parameters of the pretrained language model fixed. At last, the _in silico_ antibody is experimentally validated via high-throughput binding quantification.

and reconstruct atom coordinates from this distance matrix. The results show that the former performs better. DiffAB  initializes them from the standard normal distribution as \(_{i}^{(0)}(,_{3})\). MEAN  leverage the even distribution between the residue right before CDRs and the one right after CDRs as the initial positions.

All of the above initialization mechanisms have corresponding drawbacks. To be specific, HERN insufficiently considers the context information since it only characterizes the shape of the epitope and ignores the incomplete antibody structure \(_{LR}-_{HC}\). The initialization of normal distribution is only suitable in diffusion-based models. Moreover, our empirical experiments observe the least instability during training if an even distribution of MEAN is adopted, but residues right before or right after CDRs can be missing. Here, we propose another way to initialize the residue positions in CDRs. First, we follow MEAN and select the residue right before and right after CDRs. If both residues exist, we take the mean coordinates. Otherwise, we use only the existing one. After that, we add some little noise like HERN to separate nodes and introduce some randomization to prevent overfitting, which is proven to bring slight improvements in performance.

Loss Function.Two sorts of losses are used for supervision. First, a cross-entropy (CE) loss is employed for sequence prediction as \(_{seq}=_{HC}|}_{v_{i}_{HC}} (p_{i},c_{i})\), where \(c_{i}\) is the ground truth residue type for each node. Apart from that, a common RMSD loss is utilized for structure prediction and we leverage the Huber loss  to avoid numerical instability as \(_{struct}=(_{HC}^{(L)},_{HC})\), where the latter is the ground truth coordinates of the target CDR. The total loss is a weighted sum of the above two as \(=_{seq}+_{struct}\), where \(>0\) is the balance hyperparameter.

## 3 Experiments

We assess our HTP via two mainstream challenging tasks: sequence-structure co-design in Section 3.1, and antibody sequence design based on antibody backbones in Section 3.2. Our evaluation is conducted in the standard SAbDab database . More experimental setting details and data descriptions are elucidated in Appendix A.

### Sequence-structure Co-design

Task and Metrics.In this task, we remove the original CDR from the antibody-antigen complex in the test set and try to co-design both sequence and structure of the removed region. Here we set the length of the CDR to be identical to the length of the original CDR. But in practice, the lengths of CDRs can be variable. For quantitative evaluation, we adopt amino acid recovery (AAR) and root-mean-squared error (RMSD) regarding the 3D predicted structure of CDRs as the metric. AAR is defined as the overlapping rate between the predicted 1D sequences and the ground truths. We also take advantage of TM score  to calculate the global similarity between the predicted and ground truth antibody structures. It ranges from 0 to 1 and evaluates how well the CDRs fit into the frameworks.

    &  &  &  \\  & AAR (\%) \(\) & RMSD \(\) & TM-Score \(\) & AAR (\%) \(\) & RMSD \(\) & TM-Score \(\) & AAR (\%) \(\) & RMSD \(\) & TM-Score \(\) \\  RADD & \(20.63 1.6\) & \(3.56 0.05\) & \(0.9206 0.007\) & \(27.80 0.8\) & \(2.85 0.09\) & \(0.9253 0.010\) & \(21.73 0.7\) & \(4.58 0.13\) & \(0.8916 0.012\) \\ C-RCNN & \(40.39 3.2\) & \(1.98 0.02\) & \(0.9380 0.003\) & \(33.36 1.7\) & \(1.23 0.05\) & \(0.9507 0.005\) & \(21.89 1.5\) & \(3.59 0.16\) & \(0.9187 0.011\) \\ MEAN & \(43.80 2.5\) & \(1.58 0.04\) & \(0.9411 0.008\) & \(37.18 1.5\) & \(1.27 0.04\) & \(0.9522 0.007\) & \(25.16 1.7\) & \(3.44 0.18\) & \(0.9248 0.009\) \\ HERN & \(48.42 2.7\) & \(1.69 0.04\) & \(0.9472 0.005\) & \(41.53 2.1\) & \(1.26 0.03\) & \(0.9531 0.006\) & \(25.73 1.4\) & \(3.02 0.11\) & \(0.9340 0.004\) \\ DiffAb & \(52.82 0.9\) & \(1.51 0.01\) & \(0.9658 0.01\) & \(45.95 2.3\) & \(1.24 0.01\) & \(0.9588 0.002\) & \(27.04 2.8\) & \(2.89 0.15\) & \(0.9417 0.008\) \\  HTP & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\    &  &  &  \\  & AAR (\%) \(\) & RMSD \(\) & TM-Score \(\) & AAR (\%) \(\) & RMSD \(\) & TM-Score \(\) & AAR (\%) \(\) & RMSD \(\) & TM-Score \(\) \\  RADD & \(35.11 1.0\) & \(1.88 0.01\) & \(0.9458 0.002\) & \(27.82 0.66\) & \(1.35 0.02\) & \(0.9611 0.009\) & \(23.73 0.5\) & \(2.144 0.06\) & \(0.9247 0.010\) \\ C-RCNN & \(41.44 2.5\) & \(2.06 0.02\) & \(0.9326 0.003\) & \(36.71 4.3\) & \(1.26 0.01\) & \(0.9652 0.006\) & \(33.80 4.8\) & \(1.95 0.06\) & \(0.9380 0.008\) \\ MEAN & \(47.69 2.3\) & \(1.87 0.02\) & \(0.9461 0.006\) & \(94.92 3.5\) & \(1.24 0.01\) & \(0.9647 0.008\) & \(35.18 2.6\) & \(1.84 0.05\) & \(0.9390 0.005\) \\ HERN & \(55.24 2.7\) & \(1.63 0.02\) & \(0.9502 0.003\) & \(46.02 4.1\) & \(1.18 0.02\) & \(0.9712 0.004\) & \(37.28 4.1\) & \(1.77 0.07\) & \(0.9389 0.002\) \\ DiffAb & \(62.71 1.2\) & \(1.48 0.01\) & \(0.9637 0.002\) & \(52.10 3.6\) & \(1.11 0.06\) & \(0.9780 0.004\) & \(43.62 2.6\) & \(1.65 0.05\) & \(0.9447 0.004\) \\  HTP & \(\) & \(
    &  &  &  \\  & AAR (\%) \(\) & Perplexity \(\) & AAR (\%) \(\) & Perplexity \(\) & AAR (\%) \(\) & Perplexity \(\) \\  RosettaFix & \(36.29 0.2\) & \(14.78 0.01\) & \(37.70 0.3\) & \(12.30 0.02\) & \(28.13 0.1\) & \(24.05 0.14\) \\ Structured-TF & \(53.24 3.2\) & \(8.61 0.08\) & \(49.87 1.2\) & \(10.27 0.06\) & \(30.29 0.4\) & \(19.65 0.11\) \\ DiffAb & \(59.91 1.2\) & \(6.44 0.05\) & \(59.14 1.8\) & \(6.92 0.08\) & \(33.30 0.5\) & \(16.84 0.12\) \\ GVP-GNN & \(62.72 1.5\) & \(4.08 0.03\) & \(62.48 1.7\) & \(4.77 0.09\) & \(34.59 0.6\) & \(15.79 0.13\) \\  HTP & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\    &  &  &  \\  & AAR (\%) \(\) & Perplexity \(\) & AAR (\%) \(\) & Perplexity \(\) & AAR (\%) \(\) & Perplexity \(\) \\  RosettaFix & \(35.42 0.3\) & \(15.82 0.01\) & \(36.76 0.2\) & \(14.67 0.01\) & \(32.17 0.1\) & \(18.01 0.00\) \\ Structured-TF & \(56.73 3.1\) & \(7.63 0.10\) & \(52.11 1.8\) & \(8.93 0.08\) & \(43.48 0.7\) & \(13.88 0.02\) \\ DiffAB & \(58.82 1.6\) & \(6.89 0.06\) & \(55.40 1.2\) & \(7.16 0.05\) & \(47.31 0.5\) & \(10.60 0.02\) \\ GVP-GNN & \(60.18 1.4\) & \(5.48 0.05\) & \(59.66 1.5\) & \(6.48 0.06\) & \(51.34 0.6\) & \(8.27 0.03\) \\  HTP & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Results of the fix-backbone design task on SabDab.

Baselines.Apart from some baselines that have been listed in the sequence-structure co-design problem, we compare HTP with three additional approaches. **RosettaFix** is a Rosetta-based software for computational protein sequence design. **Structured Transformer** (Structured-TF) is an auto-regression generative model that is able to sample CDR sequence given the backbone structure. **GVP-GNN** extends standard dense layers to operate on collections of Euclidean vectors and performs geometric and relational reasoning on efficient representations of macromolecules.

Results.Table 2 documents the result. It is clearly found that our model achieves the highest AAR and the lowest PPL compared to all the baseline algorithms. To be specific, HTP leads to an increase of 37.13%, 3.01%, and 18.47% in AAR over the best GVP-GNN in H1, H2, and H3, separately, and 28.76%, 24.28%, and 49.90% in L1, L2, and L3, respectively. This demonstrates that our model is also effective in capturing the conditional probability of sequences given backbone structures. Furthermore, we can also observe that CDR-H3 is the hardest segment in comparison to other regions as its AAR is usually lower than 50%. Meanwhile, the average AAR of the light chain is more than 75%, indicating that the light chain maintains less diversity than the heavy chain.

### Discussion

Comparison with Antibody-specific Language Models.Recently, emerging efforts have been paid to train large antibody-specific language models. Studies have also demonstrated that these models can capture biologically relevant information and be generalized to various antibody-related applications, such as paratope position prediction. Here, we make a further investigation into the efficacy of these antibody-specific language models for sequence-structure co-design. To be precise, we abandon the pretraining stages of the single-protein and antibody sequence levels and directly leverage external antibody-specific language models. As shown in Table 3, AntiBERTa and AbLang provide biologically relevant information that is beneficial for the challenge of co-design with an increase of 12.44% and 36.38% in AAR. However, their improvements are much smaller than those of PLMs trained through the first two levels of tasks in HTP.

Up- and Downstream Protein.Recently, Wang et al.  proposed a joint sequence-structure recovery method based on RosettaFold to scaffold functional sites of proteins. They perform the inpainting and fix-backbone sequence design tasks without immediate up- and downstream protein visible. However, we discover that our HTP can generate adequate diversity without the need to mask neighboring residues, _, that is_, up- and downstream proteins. This difference stems from the fact that our HTP considers the entire antigen and available antibody as the context to complete the masked CDR rather than only depending on the tiny context of up- and downstream protein.

Data Leakage of Language Models.It is undisputed that the evaluation of design methods that use PLMs should be stringent and ensure that the test data were not previously seen by those pretrained models. However, ESM-2 is trained on all protein sequences in the UniRef database (September 2021 version), while our test set includes sequence released after December 2021, as well as structures with any CDR similar to those released after this date (with sequence identity higher than 50%). It is fairly possible that the training set of ESM-2 includes antibody sequences similar to the test set, leading to the intolerable data leakage problem.

Here, we conducted additional experiments aimed at assessing the contribution of ESM-2 in directly recovering CDR sequences. Specifically, we abandon the structural information and re-generate CDRs entirely based on sequential information. Towards this end, we first extract residue-level representations via (fixed-weight) ESM-2 and feed them to a three-layer perceptron to predict the masked CDR-H3, where no antigen sequences are given. The results show that this algorithm only

    &  \\  & AAR (\%) \(\) & RMSD \(\) & TM-Score \(\) \\  – & \(25.31\)\(\) 0.7 & \(2.95\)\(\) 0.02 & \(0.9391\)\(\) 0.005 \\ AbLang & \(28.46\)\(\) 1.6 & \(2.88\)\(\) 0.17 & \(0.9435\)\(\) 0.009 \\ AntiBERTa & \(34.52\)\(\) 1.2 & \(2.41\)\(\) 0.13 & \(0.9473\)\(\) 0.006 \\  HTP & \( & \( \\   

Table 3: Comparison with pretrained antibody-specific language models.

achieved an AAR of 14.63% to recover CDR-H3, much lower than all baseline methods such as RAdD (21.73%) and our HTP (40.98%). This compellingly demonstrates that ESM-2 is not the primary driver of our favorable numerical outcomes and the experimental benefits brought forth by HTP are not due to data leakage. This also accords with ATUE's  findings that ESM models perform well in low-antibody-specificity-related tasks but can even bring negative impacts for high-antibody-specificity-related tasks. We have also provided adequate evidence in Appendix 4 to show that the other pretraining resources are free from any possible data leakage concern.

## 4 Conclusion

Antibodies are crucial immune proteins produced during an immune response to identify and neutralize the pathogen. Recently, several machine learning-based algorithms have been proposed to simultaneously design the sequences and structures of antibodies conditioned on the 3D structure of the antigen. This paper introduces a novel approach called the hierarchical training paradigm (HTP) to address the co-design problem. It leverages both geometric neural networks and large-scale protein language models and proposes four levels of training stages to efficiently exploit the evolutionary information encoded in the abundant protein sequences and complex binding structures. Extensive experiments confidently show that each stage of HTP significantly contributes to the improvements of the deep learning model's capacity in predicting more accurate antibody sequences and storing its corresponding 3D structures. Instead of focusing on the architecture side, our study hopes to shed light on how to better blend protein data of different modalities (_i.e._, one- and three-dimensions) and domains (_i.e._, antibodies, and non-antibodies) for tackling the sequence-structure co-design challenge, which has been long ignored by existing works.

## 5 Limitations and Future Work

In spite of the promising progress of our HTP, there is still some space left for future explorations. First, more abundant databases can be exploited in our framework. For example, AntiBodies Chemically Defined (ABCD)  is a large antibody sequence database that can be used to improve the capacity of protein language models at the second level. We do not use it in our work because our request for this database has not been approved by the authors so far. Secondly, we fix the language models during the last two levels of training (_i.e._, levels that need complex structure prediction) for simplicity and use them as the node feature initializer. It might be beneficial if both the PLM and the geometric encoder are tuned.