# Blocked Collaborative Bandits: Online Collaborative Filtering with Per-Item Budget Constraints

Soumyabrata Pal

Google Research

Bangalore, India

soumyabratapal13@gmail.com &Arun Sai Suggala

Google Research

Bangalore, India

arunss@google.com &Karthikeyan Shanmugam

Google Research

Bangalore, India

karthikeyanvs@google.com &Prateek Jain

Google Research

Bangalore, India

prajain@google.com

###### Abstract

We consider the problem of _blocked_ collaborative bandits where there are multiple users, each with an associated multi-armed bandit problem. These users are grouped into _latent_ clusters such that the mean reward vectors of users within the same cluster are identical. Our goal is to design algorithms that maximize the cumulative reward accrued by all the users over time, under the _constraint_ that no arm of a user is pulled more than \(\) times. This problem has been originally considered by , and designing regret-optimal algorithms for it has since remained an open problem. In this work, we propose an algorithm called \(LATTICE}\) (Blocked Latent bAndiTs via maTrIx ComplEtion) that collaborates across users, while simultaneously satisfying the budget constraints, to maximize their cumulative rewards. Theoretically, under certain reasonable assumptions on the latent structure, with \(\) users, \(\) arms, \(\) rounds per user, and \(=O(1)\) latent clusters, \(LATTICE}\) achieves a per-user regret of \(((1+^{-1})}\) under a budget constraint of \(=()\). These are the first sub-linear regret bounds for this problem, and match the minimax regret bounds when \(=\). Empirically, we demonstrate that our algorithm has superior performance over baselines even when \(=1\). \(LATTICE}\) runs in phases where in each phase it clusters users into groups and collaborates across users within a group to quickly learn their reward models.

## 1 Introduction

Modern recommendation systems cater to millions of users and items  on a daily basis, typically in an online fashion. A critical feature of such systems is to quickly learn tastes of individual users from sequential actions and feedback, and suggest personalized products for each user. Furthermore, in practice, an item that has already been consumed by a user is recommended very few times to the same user (or _not_ recommended at all). This is because, in applications such as movie/book recommendations, a typical user will find little interest in consuming the same item multiple times.

This problem - that we rename as _Blocked Collaborative Bandits_ - was abstracted out by  with a modeling assumption that users have a latent clustering structure. That is, each user can belong to an unknown cluster and the expected reward for an item/movie is same for all users in a cluster. Formally, consider \(\) users, \(\) items and \(\) rounds with \(,\) (\(, 10^{6}\) in recommendation systems such as YouTube) and in each round, every user is recommended some item (potentially different for each user). On consuming the item, a noisy reward is assigned. As mentioned above,it is assumed that the users can be clustered in \(\) clusters, where \(,\), and users in the same cluster have identical reward distributions over items. Furthermore, any item can be recommended to a particular user at most \(\) times, after which the item is _blocked_ for the user.

 considered this problem for \(=1\) in the setting where a user in cluster \(c\) on being recommended item \(j\) provides a like (\(+1\)) with probability \(p_{cj}\) and a dislike (\(-1\)) with probability \(1-p_{cj}\). The authors studied a notion of pseudo-regret corresponding to minimizing the number of _un-likeable_ items for a user, _i.e.,_ items for which the probability of user giving a like (\(+1\)) is less than \(1/2\). To this end, the authors proposed the Collaborative-Greedy algorithm, an \(\)-greedy style algorithm that performs random exploration in each round with certain probability. During exploitation it provides recommendations based on a neighborhood of similar users. However, maximizing the number of _likeable_ items is limiting as it does not prioritize items with large rewards and completely disregards the item ordering. Despite these theoretical limitations, variants of the collaborative algorithm designed in  have found applications in predicting Bitcoin price, , information retrieval  among others.

Recent works have studied this problem under a more practical notion of regret which involves maximizing the cumulative rewards accrued over time . However, these works assume a cluster structure among both users and items. This assumption entails that there are many copies of the highest rewarding item for any user; this voids the blocking constraint and makes the problem significantly easier. Moreover, the algorithms designed in  are greedy (they perform exploration first to cluster users, items, and then perform exploitation) and achieve significantly sub-optimal regret. These bounds, translated to the original blocked bandit problem where there are no item clusters, have sub-optimal dependence on the number of items \(\). The algorithms designed in  assumed a _noiseless_ binary feedback model which allowed for constant regret. However, these algorithms are not easily extendable to the more general noisy reward setting we consider in this work.

Another line of work has studied this problem when \(=\) (i.e., no budget constraints on arms) . While some of these works have designed regret optimal algorithms , the \(=\) budget constraint is too lenient in many modern recommendation systems. To summarize, existing works have either attempted to solve the much harder setting of \(=1\) by imposing additional restrictions on the problem or the simpler \(=\) setting which is not very relevant in practice.

**This Work.** In this work, we make progress on this problem by considering the intermediate setting of \(=()\). We do not impose any cluster structure among items (as in ), and consider a general notion of regret which involves maximizing the cumulative rewards under the budget constraints. We propose B-LATTICE(Alg. 1), a phased algorithm which carefully balances exploration and exploitation. Our algorithm also performs on-the-fly clustering of users and collaborates across users within a cluster to quickly learn their preferences, while simultaneously satisfying the budget constraints. The key contribution of our work is that under certain standard incoherence assumptions (also used recently in ) and a budget constraint of \(=()\), we show that B-LATTICE achieves a per-user regret of \(((1^{-1}})})\)1. Empirically, our algorithm can even handle a budget constraint of \(=1\) and perform better than baselines (Alg. 6). However, bounding its regret

  Paper & Setting & Metric & Guarantees (worst-Case) \\   Bresler et al., 2014  &  \(=1\), \\ user clusters \\  &  pseudo regret (maximize \\ likeable items) \\  & \(O(T)\) \\  Bresler et al., 2018  &  \(=1\), \\ user, item clusters; \\ noiseless rewards \\  & regret & \((1)\) \\  Ariu et al., 2020  &  \(=1\), \\ user, item clusters \\  & regret &  \((^{2/3}(1+^{-1})^{1/3})\) \\ sub-optimal in \(\), \(\) \\  \\  Pal et al., 2023  &  \(=\), \\ user clusters \\  & regret & 
 \(((1+^{-1})}\) \\ minimax optimal in \(\), \(\), \(\) \\  \\ 
**This Work** & 
 \(=()\), \\ user clusters \\  & regret & \(((1+^{-1})}\) \\  

Table 1: Comparison of various approaches for blocked collaborative bandits. All the regret bounds stated here are worst-case bounds and assume \(=O(1)\). Moreover, the regret is averaged across users. The worst-case pseudo-regret in  is linear when the items have rewards close to \(0.5\). In , the authorsâ€™ proposed a greedy algorithm whose worst-case has \(^{2/3}\) dependence.

in this setting is much more challenging which we aim to address in a future work. That being said, under an additional cluster structure on the items, we show that our theoretical guarantees hold even for \(=1\) (Appendix F).

We also provide a non-trivial regret lower bound of \((^{-1}}^{-1/2})\). Our proof involves a novel reduction to multi-hypothesis testing and relies on Fano's inequality for approximate recovery  (see Appendix E). Our techniques could be of independent interest even outside the context of this work. Our lower bound is tight in two regimes - when the number of rounds \(\) is very small or very large. However, we conjecture that our upper bounds are actually tight since they recover the same rates as in the \(=\) setting ; tightening the lower bound is left as a future work. Finally, we verify our theoretical results by simulations on synthetic data (see Appendix C). Here, we compare a more practical version of B-LATTICE(Algorithm 6) with Collaborative-Greedy . We demonstrate that B-LATTICE not only has good regret but also other practically desirable properties such as a small cold-start period, and repeated high quality recommendations.

**Techniques.** Our algorithm is built on recent works that have studied this problem without the budget constraint [17; 25].  developed a regret optimal algorithm called LATTICE that runs in phases. At any phase, the algorithm maintains a grouping of users which it refines over time (these groups are nothing but our current estimate of user clusters). Within each group, the algorithm relies on collaboration to quickly estimate the reward matrix. To be precise, the algorithm performs random exploration followed by low-rank matrix completion  to get a good estimate of the user-item reward matrix. Next, the algorithm uses the estimated reward matrix to eliminate sub-optimal arms for each user. Finally, it refines the user clusters by placing users with similar reward structure in the same group.

Extending the above algorithmic recipe to our setting poses a couple of challenges. First, observe that the _oracle_ optimal strategy in the budget constrained setting is to recommend each of the top \(/\) items \(\) times; we refer to these top times as _golden_ items in the sequel. To compete against such a policy, our algorithm needs to quickly identify the golden items for each user. The LATTICE algorithm described above doesn't perform this, as it aims to only identify the top item for every user. So, one of the key novelties in our algorithm is to design a test to quickly identify the golden items and recommend them to the users. The second challenge arises from the usage of low-rank matrix completion oracles in LATTICE. To be precise, LATTICE requires more accurate estimates of the user-item reward matrix in the later phases of the algorithm. To this end, it repeatedly recommends an item to a user to reduce the noise in its estimates. However, this is infeasible in our setting due to the budget constraints. So, the second novelty in our algorithm is to avoid repeated recommendations and design an alternate exploration strategy that adheres to the budget constraints.

**Other Related Work:**

_Item-Item Collaborative Filtering (CF)._ A complementary theoretical line of work proposes to exploit a clustering structure across the items instead of users [7; 23; 27; 22]. Under a similar blocking constraint, the authors have provided a sub-linear regret guarantee based on a certain measure of complexity called the doubling dimension. Since then, there have been several works in the literature that have attempted to exploit a cluster structure on both users and items [6; 2].

_Variants of User-User CF._ has looked into the problem of non-stationary user-user collaborative filtering where the preferences of users change over time.  studies a variant where the user only provides positive ratings i.e when the user has liked an item.  and  studies probabilistic models for user user CF in an online and offline model respectively.

_Cluster Structure across users._ In several problems related to multi-armed bandits, cluster structure across users have been explored. In particular, in [11; 20; 10; 21; 26], the authors have imposed a cluster-structure across users while each item has a \(d\)-dimensional context chosen uniformly at random at each round. The preferences of each user is a linear function of the context vector and the cluster id. Under these settings, the authors prove a strong regret bound. However, note that in our setting, the item contexts are hidden; hence, these techniques are not applicable to our setting.

## 2 Problem Setting and Background

**Notation.** We write \([m]\) to denote the set \(\{1,2,,m\}\). For a vector \(^{m}\), \(_{i}\) denotes the \(i^{}\) element; for any set \([m]\), let \(_{}\) denote the vector \(\) restricted to the indices in \(\). Similarly,for \(^{m n}\), \(_{ij},_{i}\) denotes the \((i,j)\)-th element and the \(i^{}\) row of \(\) respectively. For any set \([m],[n]\), \(_{,}\) denotes \(\) restricted to the rows in \(\) and columns in \(\). Let \(\|\|_{}\) denote absolute value of the largest entry in matrix \(\). \((0,^{2})\) denotes the Gaussian distribution with \(0\) mean and variance \(^{2}\). We write \(X\) to denote the expectation of a random variable \(X\). \(||||_{2,}\) corresponds to the maximum euclidean norm of a row of the matrix \(\). More precisely, for a matrix \(^{ r}\), the norm \(||||_{2,}=_{i[]}| |_{i}||_{2}\). Thus, if \(||||_{2,}\) is small as in Lemma 1, then all rows of \(\) have a small \(_{2}\) norm.

**Problem Setting.** Consider an online recommendation system with \(\) users, \(\) items and \(\) rounds. Let \(^{}\) be the reward matrix which is unknown. Here, we assume the set of \(\) users can be partitioned into \(\) disjoint but unknown clusters \(^{(1)},^{(2)},,^{()}\) such that any two users \(u,v[]\) belonging to the same cluster have identical reward vectors i.e. \(_{u}=_{v}\). In each round \(t[]\), every user is recommended an item (can be different for each user) by the system. In turn, the system receives feedback in the form of reward from each user. Let, \(^{(t)}_{u_{u}(t)}\) be the observed reward for recommending item \(_{u}(t)[]\) to user \(u\) at round \(t\) such that:

\[^{(t)}_{u_{u}(t)}=_{u_{u}(t)}+^{(t)}_{u _{u}(t)}\] (1)

where \(^{(t)}_{u_{u}(t)}\) denotes the unbiased additive noise. 2 We assume that elements of \(\{^{(t)}_{u_{u}(t)}\}_{u[]\\ t[]}\) are i.i.d. zero mean sub-gaussian random variables with variance proxy \(^{2}\) i.e. for all \(u,t[][]\), we have \(^{(t)}_{u_{u}(t)}=0\) and for all \(s\), we have \((s^{(t)}_{u_{u}(t)})(^{2}s^{2}/2)\). As in practical recommendation systems, we impose an additional constraint that the same item cannot be recommended more than \(\) times to a user, for some small \(=()\). For simplicity of presentation, we assume \(\) is a multiple of \(\). However, we would like to note that our results hold for general \(\). Without loss of generality, we assume \(/\), since otherwise the budget constraints cannot be satisfied.

Our goal is to design a method that maximizes cumulative reward. Let \(_{u}:[][]\) denote the function that sorts the rewards of user \(u[]\) in descending order, i.e., for any \(i<j\), \(_{u_{u}(i)}_{u_{u}(j)}\). The _oracle_ optimal strategy for this problem is to recommend \(\{_{u}(s)\}_{s=1/}\), the top \(/\) items with the highest reward, \(\) times each. This leads us to the following notion of regret

\[()_{s[/],u[ ]}_{u_{u}(s)}}{}- _{t[],u[]}_{u_{u}(t)}}{ },\] (2)

where the expectation is over the randomness in the policy and the rewards, and \(_{u}(t)\) is any policy that satisfies the budget constraints.

**Importance of collaboration.** Suppose we treat each user independently and try to minimize their regret. In this case, when \(\) is as small as \(O()\), we will incur a regret that is almost linear in \(\). This is because we will not have enough data for any item to know if it has a high or a low reward. This shows the importance of collaboration across users to achieve optimal regret. The latent cluster structure in our problem allows for collaboration and sharing information about items across users.

For ease of exposition of our ideas, we introduce a couple of definitions.

**Definition 1**.: _A subset of users \([]\) is called "nice" if \(_{j}^{(j)}\) for some \([]\). In other words, \(\) can be represented as the union of some subset of clusters._

**Definition 2**.: _For any user \(u[]\), we call the set of items \(\{_{u}(t)\}_{t=1}^{/}\) (i.e., the set of best \(/\) items) to be the golden items for user \(u\)._

### Low-Rank Matrix Completion

Additional Notation.For an estimate \(}\) of reward matrix \(\), we will use \(_{u}:[][]\) to denote the items ordered according to their estimated reward for the user \(u\) i.e., \(}_{u_{u}(i)}}_{u _{u}(j)}\) when \(i<j\). For any user \(u[]\) and any subset of items \([]\), we will use \(_{u}:[||][]\) to denote the permutation \(_{u}\) restricted to items in \([]\) i.e. for any user \(u[]\) and any \(i,j[||]\) satisfying \(i<j\), we will have \(_{u_{u}(i)|}_{u_{u}(j)|}\). Here, \(_{u}(i)\) corresponds to the \(i^{}\) item among items in \(\) sorted in descending order of expected reward for user \(u\).

An important workhorse of our algorithm is low-rank matrix completion, which is a well studied problem in the literature [16; 18; 8; 17]. Given a small set of entries that are randomly sampled from a matrix, these algorithms infer the missing values of the matrix. More precisely, consider a low-rank matrix \(^{}\). Let \([][]\) be a random set of indices obtained by picking each index in \([][]\) independently with probability \(p>0\). Let \(\{_{ij}\}_{(i,j)}\) be the corresponding noisy entries (sub-gaussian random variables with variance proxy \(^{2}>0\)) that we observe which satisfy \(_{ij}=_{ij}(i,j)\). In this work we rely on nuclear norm minimization to estimate \(\) (see Algorithm 4). Recent works have provided tight element-wise recovery guarantees for this algorithm [8; 17]. We state these guarantees in the following Lemma.

**Lemma 1** (Lemma 2 in ).: _Consider matrix \(^{}\) with rank \(r\) and SVD decomposition \(}}^{}\) satisfying \(\|}\|_{2,}},\|}\|_{2,}}\) and condition number \(=O(1)\). Let \(d_{1}=(,)\), \(d_{2}=(,)\), noise variance \(^{2}>0\), and let sampling probability \(p\) be such that \(p=^{2}}{1,}{ \|\|^{2}}}\) (for some constant \(c>0\)). Suppose we sample a subset of indices \([][]\) such that each tuple of indices \((i,j)[][]\) is included in \(\) independently with probability \(p\). Let \(\{_{ij}\}_{(i,j)}\) be the noisy observations corresponding to all indices in \(\). Then Algorithm 4, when run with inputs \((,,^{2},r,,\{_{ij}\}_{(i,j)})\), returns an estimate \(}\) of \(\) which satisfies the following error bounds with probability at least \(1-O(+d_{2}^{-12})\)_

\[\|-}\|_{}=O( d_{1}}}{}}).\] (3)

Lemma 1 provides guarantees on the estimate \(}\) of \(\) given the set of noisy observations corresponding to the entries in \(\). Equation 3 says that the quality of the estimate becomes better with the increase in sampling probability \(p\) that determines the number of entries in \(\). Note the detailed Algorithm Estimate (Alg. 4) designed in [17; 8] is provided in Appendix A.

**Remark 1** (Warm-up (Greedy Algorithm)).: _Using Lemma 1, it is easy to design a greedy algorithm (Alg. 5 in Appendix B) for \(=1\); for a fixed number of rounds \(m\), we recommend random unblocked items to every user. Since the reward matrix \(\) is a low-rank matrix, we use Algorithm 4 to estimate the entire matrix. Subsequently, we recommend the best estimated unblocked items for each user for the remaining rounds. We can prove that the regret suffered by such an algorithm is \((^{2/3}(1(/))^{1/3})\). The dependence on number of rounds \(\) is sub-optimal, but Alg. 5 does not require the knowledge of any gaps corresponding to the reward matrix \(\). See Appendix B for a detailed proof. We would like to note that the result can be generalized easily to \(=()\)._

## 3 Main Results

Let \(^{}\) be the sub-matrix of the expected reward matrix \(\) comprising \(\) distinct rows of \(\) corresponding to each of the \(\) true clusters. Also, let \(:=_{i,j[r]}|^{(i)}|/|^{(j)}|\) denote the ratio of the maximum and minimum cluster size. To obtain regret bounds, we make the following assumptions on the matrix \(\):

**Assumption 1** (Assumptions on \(\)).: _Let \(=^{}\) be the SVD of \(\). Also, let \(\) satisfy the following: 1) Condition number: \(\) is full-rank and has non zero singular values \(_{1}>>_{}\) with condition number \(_{1}/_{}=O(1)\), 2) \(\)-incoherence: \(\|\|_{2,}/}\), 3) Subset Strong Convexity: For some \(\) satisfying \(=(1)\), \(=(1)\) for all subset of indices \([],||\), the minimum non zero singular value of \(_{}\) must be at least \(|/}\)._

[MISSING_PAGE_FAIL:6]

difference in regret of at most \(\) in two separate instances constructed in standard measure change arguments . However, we prove two regret lower bounds on this problem, out of which the latter is the technically more interesting one.

The first term in the lower bound in Thm. 2 follows from a simple reduction from standard multi-armed bandits. Intuitively, if we have \(/\) known identical copies of each item, then the blocking constraint is void - but with \(=1\), this is (almost) equivalent to a standard multi-armed bandit problem with \(\) rounds, \(^{-1}\) distinct arms (up to normalization with \(\)). The lower bound follows from invoking standard results in MAB literature. For \(=\), we recover the matching regret lower bound in  without the blocking constraint. Furthermore, for \(=()\), the first term is tight up to log factors when number of rounds is small for example when \(=O(1)\). (Appendix E)

The second term in the lower bound is quite non-trivial. Note that the second term is tight up to log factors when the number of rounds \(=()\) is very large (close to the number of items) and \(=O()\) is small. We obtain this bound via reduction of the regret problem to a multiple hypothesis testing problem with exponentially many instances and applying Fano's inequality  (commonly used in proving statistical lower bounds in parameter estimation) for approximate recovery. Our approach might be of independent interest in other online problems as well. (Appendix E)

**Remark 4**.: _We leave the problem of extending our guarantees for \(=1\) as future work. However we make progress by assuming a cluster structure over items as well. More precisely, suppose both items and users can be grouped into a constant number of disjoint clusters as such that (user,item) pairs in same cluster have same expected reward. Then, we can sidestep the statistical dependency issues in analysis of Alg. 1 for \(=1\) and provide similar guarantees as in Thm. 1 (see Appendix F)._

## 4 \(\) Algorithm

\(\) is a recursive algorithm and runs in \(O()\) phases of exponentially increasing length. \(\) assume the knowledge of the following quantities - users \(\), items \(\), clusters \(\), rounds \(\), blocking constraint \(\), maximum true reward \(_{}\), incoherence factor \(\), cluster size ratio \(\), noise variance \(^{2}>0\) and other hyper-parameters in Assumption 1. At any phase, the algorithm maintains a partitioning of users into crude clusters, and a set of active items for each such crude cluster. Let \(^{()}\) be the partitioning of users in the \(^{th}\) phase, and \(^{()}\) be the list containing the set of active items for each group of users in \(^{()}\). In the first phase, we place all users into a single group and keep all items active for every user; that is, \(^{(1)}=[[]]\), \(^{(1)}=[[]]\). There are three key components in each phase: (a) exploration, (b) exploitation, and (c) user clustering. In what follows, we explain these components in detail. For simplicity of exposition, suppose Assumption 2 is true namely the parameters \(,,,_{},\) are constants.

**Exploration (Alg. 2).** The goal of the \(\) sub-routine is to gather enough data to obtain a better estimate of the reward matrix. To be precise, let \(^{(,i)}\) be the users in the \(i^{th}\) group at phase \(\), and \(^{(,i)}\) be the set of active items for this group. Let \(_{^{(,i)},^{(,i)}}\) be the sub-matrix of \(\) corresponding to rows \(^{(,i)}\) and columns \(^{(,i)}\) (recall, this sub-matrix has rank at most \(\)). Our goal is to get an estimate \(}_{^{(,i)},^{(,i)}}\) of this matrix that is entry-wise \(O(2^{-})\) close to the true matrix. To do this, for each user in \(^{(,i)}\), we randomly recommend items from \(^{(,i)}\) with probability \(p=O d_{1}}{d_{2}}\) (Line 6). Here, \(d_{1}=(|^{(,i)}|,|^{(,i)}|)\), \(d_{2}=(|^{(,i)}|,|^{(,i)}|)\). We then rely on low-rank matrix completion algorithm (Algorithm 4) to estimate the low rank sub-matrix. By relying on Lemma 1, we show that our estimate is entry-wise \(O(2^{-})\) accurate. This also shows that our algorithm gets more accurate estimate of \(_{^{(,i)},^{(,i)}}\) as we go to the later phases.

**User Clustering (lines 7-8 of Alg. 1).** After the _exploration_ phase, we refine the user partition to make it more fine-grained. An important property we always want to ensure in our algorithm is that \(^{(,i)}\), the \(i^{th}\) group/crude cluster in phase \(\), is a _nice_ subset for all \((,i)\) (see Definition 1 for a definition of nice subset). To this end, we build a user-similarity graph whose nodes are users in \(^{(,i)}\), and draw an edge between two users if they have similar rewards for all the arms in \(^{(,i)}\) (Line 8 in Alg. 1). Next, we partition \(^{(,i)}\) based on the connected components of the graph. That is, we group all the users in a connected component into a single cluster. In our analysis, we show that each connected component of the graph is a _nice subset_.

**Exploitation (Alg. 3).** The main goal in the Exploit sub-routine of our algorithm is to identify common golden items for a group of users jointly. Consider a group of users \(\) with active items \(\) at the beginning of _exploit_ sub-routine invoked in the \(^{}\) phase of Algorithm 1. We perform the following test in Line 1 of the Exploit sub-routine: for every user \(u\), we check if \(}_{u_{u}(1)|}-}_{u_{u}(||)|} c^{}2^{-}\) for some constant \(c^{}>0\) - that is if the estimated highest rewarding and lowest rewarding items of the user \(u\) have a significant gap. For all the users that satisfy the above property, we take a union of the items close to the estimated highest rewarding item for each of them (Line 3). We can show that these identified items are actually golden items for all users in the nice subset \(\). Hence, we immediately recommend these identified golden items to every user in the nice subset \(\) times. In case a golden item is blocked for a user, we recommend an unblocked active item (Line 5). Subsequently we remove the golden items from the active set of items (Line 8) and prune it. We go on repeating the process with the pruned set of items until we can find no user that satisfy the above gap property between highest and lowest estimated rewarding items in the active set.

To summarize, in each phase of Algorithm 1, we perform the exploration, exploitation and user clustering steps described above. As the algorithm proceeds, we get more fine grained clustering of users, and more accurate estimates of the rewards of active items. Using this information, the algorithm tries to identify golden items and recommends the identified golden items to users.

**Implementation Details:** The actual implementation of the algorithm described above is a bit more involved due to the fact that every user has to be recommended an item at every time step (seeproblem setting in Section 2). To see this, consider the following scenario. Suppose, we identified item \(j\) as a golden item for users in the nice subset \(\) (crude cluster) in the Exploit sub-routine invoked in some phase of Alg. 1. Moreover, suppose there are two users \(u_{1},u_{2}\) in the cluster for whom the item has been recommended \(n_{1},n_{2}\) times respectively, for some \(n_{1}<n_{2}\). So, for the final \(n_{2}-n_{1}\) iterations during which the algorithm recommends item \(j\) to \(u_{1}\), it has to recommend some other item to \(u_{2}\). In our algorithm, we randomly recommend an item from the remaining active set of items for \(u_{2}\) during those \(n_{2}-n_{1}\) rounds. We store the rewards from these recommendations and use them in the exploration component of future phases. A similar phenomenon also occurs in the _Explore_ sub-routine where we sometimes need to recommend items outside the sub-sampled set of entries in \(\) (Line 7 and 10 in Alg. 2) To this end, we introduce matrices \(,^{}\) which perform the necessary book-keeping for us.

\(\) tracks number of times an item has been recommended to a user and the corresponding observation has been already used in computing an estimate of some reward sub-matrix (Line 2 of Alg. 2). Since these estimates are used to cluster users and eliminate items (Lines 7-10 in Alg. 1), these observations are not reused in subsequent phases to avoid statistical dependencies. \(\) tracks the number of times an item has been recommended to a user and the corresponding observation has not been used in computing an estimate of reward sub-matrix so far. These observations can still be used once in Line 2 of Alg. 2. Observe that \(_{ij}+_{ij}\) is the total times user \(i\) has been recommended item \(j\). In practice, eliminating observations is unnecessary and we can reuse observations whenever required. Hence Alg. 1 can work even when \(=1\) (see Alg. 6 for a simplified and practical version).

**Handling very few active items (Line 5 in Alg. 1).** Recall, in the exploration step of phase \(\), we randomly recommend active items with probability \(p=O d_{1}}{d_{2}}\). If \(p>1\), then we simply recommend all the remaining unblocked items for each user until the end. In our analysis, we can show that this happens only if the size of surviving items is too small, and when the number of remaining rounds is very small. Hence the regret is small if we follow this approach (Lemma 5). Similarly, when remaining rounds become smaller than \(^{1/3}\), we follow the same approach.

**Running Time:** Computationally speaking, the main bottleneck of our algorithm is the matrix completion function _Estimate_ invoked in Line 13 of the Explore Component (Algorithm 2). All the remaining steps have lower order run-times. Note that the _Estimate_ function is invoked at most \(O()\) times since there are can be at most \(\) disjoint nice subsets of users at a time and the number of phases is \(\). Moreover, note that the _Estimate_ function (Algorithm 4) solves a convex objective in Line 3 - this has a time complexity of \(O(^{2}+^{2})\) which is slightly limiting because of the quadratic dependence. However, a number of highly efficient techniques have been proposed for optimizing the aforementioned objective even when \(\), \(\) are in the order of millions (see ).

```
0: Phase index \(\), nice subset of users \(\), active items \(\), round index \(t_{0}\), exploit rounds \(t_{p}{p}}}\), estimate \(}\) of \(\) and error guarantee \(_{}\) such that \(|}_{,}-_{,}||_{} 8_{}\) with high probability. //Takes a particular set of users (nice w.h.p.), their corresponding set of active items and an estimate of corresponding reward sub-matrix as input. Identifies common golden items for all aforementioned users in this module and recommends them jointly until exhausted - recommendations are kept track of in global variables \(,\) and active items are pruned.
1:while there exists \(u\) such that \(}_{u\#_{u}(1)|}-}_{u\#_ {u}(1)|} 64_{}\)do
2: Compute \(_{u}=\{j}_{u}_{u\#_{u}(1)|}}-2_{+1}\}\) for every user \(u\). Compute \(=_{u}_{u}\). // Find common set of golden items for all users in \(\)
3:for rounds \(t=t_{0}+1,t_{0}+2,,t_{0}+||\)do
4:for each user \(u\)do
5: Denote by \(x[(t-t_{0}/)]\) item in \(\). If \(_{ux}+_{ux}<\) (\(x\) is unblocked), then recommend \(x\) to user \(u\) and update \(_{ux}_{ux}+1\). If \(_{ux}+_{ux}=\) (\(x\) is blocked), recommend any unblocked item \(y\) in \(\) (i.e \(_{uy}+_{uy}<\)) for the user \(u\) and update \(_{uy}_{uy}+1\). // Recommend all identified golden items
6:endfor
7:endfor
8: Update \(\). // Remove golden items and prune active items \(\)
9: Update \(t_{0} t_{0}+||\,\) and \(t_{p}{p}} t_{p}+||\,\).
10:endwhile
11: Return \(,t_{0},t_{p}\). ```

**Algorithm 3** Exploit (Exploit Component of a phase)

**Proof Sketch of Theorem 1** We condition on the high probability event that the low rank matrix completion estimation step is always successful (Lemma 17). Note that for a fixed user, the items chosen for recommendation in the _exploit_ component of any phase are _golden items_ and costs zero regret if they are unblocked and recommended. Even if it is blocked, we show a swapping argument to a similar effect. That is, with an appropriate permutation of the recommended items, we can ignore the regret incurred from golden-items altogether. Moreover, in the _explore_ component of the \(^{}\) phase, we can bound the sub-optimality gap of the surviving active items by some pre-determined \(_{}\). We prove that this holds even with the (chosen) permutation of the recommended items (Lemma 18). We choose \(_{}\) to be exponentially decreasing in \(\) and the number of rounds in the _explore_ component of phase \(\) is roughly \(1/_{}^{2}\). Putting these together, we obtain the regret guarantee in Theorem 1.

## 5 Conclusion and Future Work

We study the problem of Collaborative Bandits in the setting where each item can be recommended to a user a small number of times. Under some standard assumptions and a blocking constraint of \(()\), we show a phased algorithm \(\) with regret guarantees that match the tight results with no blocking constraint . To the best of our knowledge, this is the first regret guarantee for such a general problem with no assumption of item clusters. We also provide novel regret lower bounds that match the upper bound in several regimes. Relaxing the assumptions, extending guarantees to a blocking constraint of \(=1\) and tightening the gap between the regret upper and lower bounds are very interesting directions for future work.