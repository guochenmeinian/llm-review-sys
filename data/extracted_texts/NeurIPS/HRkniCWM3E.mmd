# Neural Pfaffians: Solving Many Many-Electron Schrodinger Equations

Nicholas Gao, Stephan Gunnemann

{n.gao,s.guennemann}@tum.de

Department of Computer Science & Munich Data Science Institute

Technical University of Munich

###### Abstract

Neural wave functions accomplished unprecedented accuracies in approximating the ground state of many-electron systems, though at a high computational cost. Recent works proposed amortizing the cost by learning generalized wave functions across different structures and compounds instead of solving each problem independently. Enforcing the permutation antisymmetry of electrons in such generalized neural wave functions remained challenging as existing methods require discrete orbital selection via non-learnable hand-crafted algorithms. This work tackles the problem by defining overparametrized, fully learnable neural wave functions suitable for generalization across molecules. We achieve this by relying on Pfaffians rather than Slater determinants. The Pfaffian allows us to enforce the antisymmetry on arbitrary electronic systems without any constraint on electronic spin configurations or molecular structure. Our empirical evaluation finds that a single neural Pfaffian calculates the ground state and ionization energies with chemical accuracy across various systems. On the TinyMol dataset, we outperform the 'gold-standard' CCSD(T) CBS reference energies by \(1.9\,E_{}\) and reduce energy errors compared to previous generalized neural wave functions by up to an order of magnitude.

## 1 Introduction

Solving the electronic Schrodinger equation is at the heart of computational chemistry and drug discovery. Its solution provides a molecule's or material's electronic structure and energy (Zhang et al., 2023). While the exact solution is infeasible, neural networks have recently shown unprecedentedly accurate approximations (Hermann et al., 2023). These neural networks approximate the system's ground-state wave function \(:^{N_{} 3}\), the lowest energy state, by minimizing the energy \(||\), where \(\) is the Hamiltonian operator, a mathematical description of the system. While such neural wave functions are highly accurate, training has proven computationally intensive.

Gao & Gunnemann (2022) have shown that training a generalized neural wave function on a large class of systems amortizes the cost. However, their approach is limited to different geometric arrangements of the same molecule. Subsequent works eliminated this limitation by introducing hand-crafted algorithms (Gao & Gunnemann, 2023a) or heavily relying on classical Hartree-Fock calculations (Scherbela et al., 2023). Both impose strict, non-learnable mathematical constraints and prior assumptions that may not always hold, limiting their generalization and accuracies. Hand-crafted algorithms only work for a limited set of molecules, in particular organic molecules near equilibrium, while the reliance on Hartree-Fock empirically results in degraded accuracies.

In this work, we propose the Neural Pfaffian (NeurPf) to overcome these limitations. As suggested by its name, NeurPf uses Pfaffians to define a superset of the previously used Slater determinants to enforce the fermionic antisymmetry. The Pfaffian lifts the constraint on the number of molecular orbitals from Slater determinants (Szabo & Ostlund, 2012), enabling overparametrized wave functionswith simpler and more accurate generalization. Compared to Globe (Gao & Gunnemann, 2023a), the absence of hand-crafted algorithms enables the modeling of non-equilibrium, ionized, or excited systems. By being fully learnable without fixed Hartree-Fock calculations like TAO (Scherbela et al., 2024), NeurPf achieves significantly lower variational energies. Our empirical results show that NeurPf can learn all second-row elements' ground-state, ionization, and electron affinity potentials with a single wave function. Further, we demonstrate that NeurPf's accuracy surpasses Globe on the challenging nitrogen dimer with seven times fewer parameters while not suffering from performance degradations when adding structures to the training set. On the TinyMol dataset, NeurPf surpasses the highly accurate reference CCSD(T) CBS energies on the small structures by \(1.9\,E_{}\) and reduces errors compared to TAO by factors of 10 and 6 on the small and large structures, respectively.

## 2 Quantum chemistry

Quantum chemistry aims to solve the time-independent Schrodinger equation (Foulkes et al., 2001)

\[=E\] (1)

where \(:^{N_{} 3}^{N_{} 3 }\) is the electronic wave function for \(N_{}\) spin-up and \(N_{}\) spin-down electrons, \(\) is the Hamiltonian operator, and \(E\) is the system's energy. To ease notation, if not necessary, we omit spins in \(\) and treat it as \(:^{N_{} 3}\) where \(N_{}=N_{}+N_{}\). The Hamiltonian \(\) for molecular systems, which we are concerned with in this work, is given by

\[= -_{i=1}^{N_{}}_{k=1}^{3}}{_{ik}^{2}}+_{j>i}^{N_{}}_{i}- _{j}\|}-_{i=1}^{N_{}}_{m=1}^{N_{}}}{ \|_{i}-_{m}\|}+_{n>m}^{N_{n}}Z_{n}}{\|_{m}- _{n}\|}\] (2)

with \(_{i}^{3}\) being the \(i\)th electron's position, and \(_{m}^{3},Z_{m}_{+}\) being the \(m\)th nucleus' position and charge. The wave function \(\) describes the behavior of electrons in the system defined by the Hamiltonian \(\). As the square of the wave function \(^{2}\) is proportional to the probability density \(p(})^{2}(})\) of finding the electrons at positions \(}^{N_{} 3}\), its integral must be finite:

\[(})^{2}}<.\] (3)

Further, as electrons are indistinguishable half-spin fermionic particles, the wave function must be antisymmetric under any same-spin electron permutation \(\):

\[(^{}(}^{}),^{}(}^{}))=(^{})( ^{})(}).\] (4)

To enforce this constraint, the wave function is typically defined as a so-called Slater determinant of \(N_{}+N_{}\) integrable so-called orbital functions \(_{i}:^{3}\):

\[_{}(})=[_{j}^{}(_ {i}^{})][_{j}^{}(_{i}^{}) ]=^{}(}^{})^{}( }^{}).\] (5)

Note that for the determinant to exist, one needs exactly \(N_{}\) up and \(N_{}\) down orbitals \(_{j}^{}\) and \(_{j}^{}\).

In linear algebra, Eq. (1) is an eigenvalue problem, where we look for the eigenfunction \(_{0}\) with the lowest eigenvalue \(E_{0}\). In Variational Monte Carlo (VMC), this is solved by applying the variational principle, which states that the energy of any trial wave function \(\) upper bounds \(E_{0}\):

\[E_{0}}{}}=})(})}}{ ^{2}(})}}.\] (6)

By plugging in the probability distribution from Eq. (3), we can rewrite Eq. (6) as

\[E_{0}_{p(})}[^{-1}(})(})]=_{p(})}[E_{L}( })],\] (7)

with \(E_{L}(})=(})^{-1}(})\) being the so-called local energy. The right-hand side of Eq. (7) is known as the variational energy. As Eq. (7) does not require \(\) to be an analytic function, we can approximate the energy of any valid wave function \(\) with samples drawn from \(p(})\). If we pick a parametrized family of wave functions \(_{}\), we can optimize the parameters \(\) to minimize the VMC energy by following the gradient of the variational energy

\[_{}=_{p(})}[(E_{L}(})- _{p(})}[E_{L}(})])_{ }_{}(})],\] (8)

where we approximate all expectations by Monte Carlo sampling (Ceperley et al., 1977).

**Neural wave functions** typically keep the functional form of Eq. (5) but replace the orbitals \(_{i}\) with learned many-electron orbitals \(_{i}^{}:^{3}^{N_{e} 3} \) (Hermann et al., 2023). These many-electron orbitals \(_{i}^{}\) are implemented as different readouts of the same permutation-equivariant neural network. Multiplying each orbital by an envelope function \(_{i}:^{3}\) that decays exponentially to zero at large distances enforces the finite integral requirement in Eq. (3).

**Generalized wave functions** solve the more general problem where the nucleus positions \(}\) and charges \(\) are not fixed. Since the Hamiltonian \(_{},}^{}\) depends on the molecular structure \((},)\), so does the corresponding ground state wave function \(_{},}\). Note that we still work in the Born-Oppenheimer approximation, i.e., we treat the nuclei as classical point charges (Zhang et al., 2023). Given a dataset of molecular structures \(=\{(}_{1},_{1}),...\}\), the total energy \(_{(},)}},}|_{},}|_{},}}{_{},}^{ }}\) is minimized to approximate the ground state for each structure. Typically, the dependence on \(}\), \(\) is implemented by using a meta network that takes \(},\) as inputs and outputs the parameters of the electronic wave function (Gao and Gunnemann, 2022).

## 3 Related work

While attempts to enforce the fermionic antisymmetry in neural wave functions in less than \(O({N_{e}}^{3})\) operations promise faster runtime than Slater determinants, the accuracy of these methods is limited (Han et al., 2019; Acevedo et al., 2020; Richter-Powell et al., 2023). Pfau et al. (2020) and Hermann et al. (2020) established Slater determinants for neural wave functions by demonstrating chemical accuracy on small molecules. Note, Eq. (5) may also be written via a block-diagonal matrix, i.e., \((})=((^{},^{ }))\). Spencer et al. (2020)'s implementation further increased accuracies by parametrizing the off diagonals that were implicitly set to \(0\) before, with additional orbitals \(\):

\[_{}(})=((}))= ^{}(}^{})&^{ }(}^{})\\ ^{}(}^{})&^{}( }^{}).\] (9)

Several works confirmed the improved empirical accuracy of this approach (Gerard et al., 2022; Lin et al., 2021; Ren et al., 2023; Gao and Gunnemann, 2023, 2024). While later works refined the architecture to increase accuracy (von Glehn et al., 2023; Wilson et al., 2021, 2023), the use of Slater determinants mostly remained a constant, with two notable exceptions: Firstly, Lou et al. (2023) use AGP wave functions (Casula and Sorella, 2003; Casula et al., 2004) to formulate the wave function as \((})=(^{})(^{})=(^{ }^{ T})\). This avoids picking exactly \(N_{}/N_{}\) orbitals as \(^{}\) and \(^{}\) may be non-square but fails to generalize Eq. (9), we empirically verify the impact of this limitation in App. I. Secondly, Kim et al. (2023) introduced the combination of neural networks and Pfaffians, who demonstrated its performance on the ultra-cold Fermi gas. Though universal in theory, their parametrization yields no trivial adaption to molecular systems. In classical quantum chemistry, Bajdich et al. (2006, 2008) reported promising early results with Pfaffians in single-structure calculations for small molecules. In this work, we generalize Eq. (9) to Pfaffian wave functions that permit pretraining with Hartree-Fock calculations and generalization across molecules.

**Generalized wave functions.** Scherbela et al. (2022) started this research with a weight-sharing scheme between wave functions. These still had to be reoptimized for each structure. Later, Gao and Gunnemann (2022, 2023b) proposed PESNet, a generalized wave function for energy surfaces allowing joint training without reoptimization. Subsequent works extended PESNet to different compounds where the main challenge is parametrizing exactly \(N_{}+N_{}\) orbitals, such that the orbital matrix in Eq. (9) stays square. The problem of finding these orbitals was formulated into a discrete orbital selection problem. Gao and Gunnemann (2023)'s hand-crafted algorithm accomplishes this by selecting orbitals via a greedy nearest neighbor search. In contrast, Scherbela et al. (2024, 2023) use the lowest eigenvalues of the Fock matrix as selection criteria. Both introduce non-learnable constraints, limiting generalization or sacrificing accuracy. NeurPf avoids the selection problem by introducing an overparametrization when enforcing the exchange antisymmetry.

Neural Pfaffian

Previous generalized wave functions build on Slater wave functions and attempt to adjust the orbitals \(_{i}\) to the molecule. Slater determinants were chosen due to their previously demonstrated high accuracy. However, they require exactly \(N_{}+N_{}\) orbitals. While the nuclei allow inferring the total number of electrons \(N_{ e}\) of any stable, singlet state system, the spin distribution into \(N_{}\) and \(N_{}\) orbitals per atom is not readily available. Previous works implement this via a discrete selection of orbitals via non-learnable prior assumptions and constraints on the wave function; see Sec. 3.

Here, we present the Neural Pfaffian (NeurPf), a superset of Slater wave functions that preserves accuracy while relaxing the orbital number constraint. By not enforcing an exact number of orbitals, NeurPf is overparametrized with \(N_{ o}\{N_{},N_{}\}\) orbitals, avoiding discrete selections and making it a natural choice for generalized wave functions. Importantly, NeurPf can be pretrained with Hartree-Fock, which accounts for \(>99\%\) of the total energy (Szabo & Ostlund, 2012). We introduce NeurPf in four steps: (1) We introduce the Pfaffian and use it to define a superset of Slater wave functions. (2) We present memory-efficient envelopes that additionally accelerate convergence. (3) We introduce a new pretraining scheme for matching Pfaffian and Slater wave functions. (4) We discuss combining our developments to build a generalized wave function.

### Pfaffian wave function

The Pfaffian of a skew-symmetric \(2n 2n\) matrix \(A\), i.e., \(A=-A^{T}\), is defined as

\[(A)=n!}_{ S_{2n}}()_ {i=1}^{n}A_{(2i-1),(2i)}\] (10)

where \(S_{2n}\) is the symmetric group of \(2n\) elements. One may consider it a square root of the determinant of \(A\) since \((A)^{}=(A)\). An important property of the Pfaffian is \((BAB^{T})=(B)(A)\) for any invertible matrix \(B\) and skew-symmetric matrix \(A\). In the context of neural wave functions, this means that if \(A\) is an along both dimensions permutation equivariant function of the electron positions \(}\), \(A((}))=P_{}A(})P_{}^{T}\), the Pfaffian of \(A\) is a valid wave function that fulfills the antisymmetry requirement from Eq. (4):

\[((}))=(A((})))=(P_{}A(})P_{}^{T})=(P_{})(A( }))=()(}).\] (11)

To compute the Pfaffian without evaluating the \(2n!\) terms in Eq. (10), we implement the Pfaffian via a tridiagonalization with the Householder transformation as in Wimmer (2012).

There are various ways to construct \(A\)(Bajdich et al., 2006, 2008; Kim et al., 2023). Here, we introduce a superset of Slater wave functions, enabling high accuracy on molecular systems. If \(A\) is a skew-symmetric matrix, so is \(BAB^{T}\) for any arbitrary matrix \(B\). Thus, we can construct \(_{}\) as

\[_{}(})=(A_{})} (_{}(})A_{}_{}(})^{T})\] (12)

where \(A_{}^{N_{ o} N_{ o}}\) is a learnable skew-symmetric matrix and \(_{}:^{N_{ o} 3}^{N_{ o } N_{ o}}\) is a permutation equivariant function like in Eq. (9). This construction elevates the need for having exactly \(N_{}/N_{}\) orbitals as in Slater determinants. We may now overparametrize the wave function with \(N_{ o}\{N_{},N_{}\}\) orbitals, allowing for a more flexible and simpler implementation without needing discrete orbital selection. By choosing \(_{}=\), it is straightforward to see that Eq. (12) is a superset of the Slater determinant wave function in Eq. (9). Note that, like in Eq. (9), we parametrize two sets of orbital functions \(_{}\) and \(_{}\) and change their order for spin-down electrons to not enforce the exchange antisymmetry between different-spin electrons. As the normalizer \((A_{})\) is constant, we drop it going forward. As it is common in quantum chemistry (Szabo & Ostlund, 2012; Hermann et al., 2020), we use linear combinations of wave functions to increase expressiveness:

\[_{}(})=_{k=1}^{N_{ k}}c_{k}_{ ,k}(}).\] (13)

We visually compare the schematic of the Slater determinant and Pfaffian wave functions in Fig. 1. In App. A, we discuss how to handle odd numbers of electrons such that \(_{}A_{}_{}^{T}\) has even dimensions. Like previous work (Pfau et al., 2020), we parametrize the orbital functions \(_{i}\) as a product of a permutation equivariant neural network \(:^{3}^{N 3}^{N_{t}}\) and an envelope function \(:^{3}\):

\[_{ki}(_{j}|})=_{ki}(_{j})(_{ j}|})^{T}_{ki}_{ki}^{N_{}-N_{}}\] (14)

with \(_{ki}^{N_{i}}\) being a learnable weight vector, and \(_{ki}^{N_{}-N_{}}\) being a scalar depending on the spin state of the system, i.e., the difference between the number of up and down electrons. The envelope function \(\) ensures that the integral of the squared wave function is finite. For \(\), we use Moon from Gao & Gunnemann (2023a) thanks to its size consistency.

### Memory-efficient envelopes

To satisfy the finite integral requirement on the square of \(\) in Eq. (3), the orbitals \(\) are multiplied by an envelope function \(:^{3}\) that exponentially decays to zero at large distances. We do not split spins here and work with \(N_{ e}=N_{}+N_{}\) to simplify the discussion, but, in practice, we would split the envelopes into two sets, one for \(_{}\) and one for \(_{}\). The envelope function is typically a sum of exponentials centered on the nuclei (Spencer et al., 2020). In Einstein's summation notation, the envelope function can be written as

\[_{ki}(_{bj})=_{kmi}}_{N_{} N_{  u} N_{ u}}_{kmi}|}_{bj }-}_{m}|\|)_{bkmmj}}_{N_{ b} N_{ u} N_{ u}  N_{ u} N_{ u} N_{ u}}\] (15)

where \(N_{ b}\) denotes the batch size. Empirically, we found the tensor on the right side containing many redundant entries. Further, due to the nonlinearity of the exponential function, one cannot implement the envelope in a simple matrix contraction but has to materialize the full five-dimensional tensor. NeurPf amplifies this problem as \(N_{ o} N_{ e}\) whereas Slater determinants constraint \(N_{ o}=N_{ e}\).

We use a single set of exponentials per nucleus instead of having one for each combination of orbital and nucleus. This reduces the number of envelopes per electron from \(N_{ k} N_{ n} N_{ o}\) to \(N_{ k} N_{ env}\), where \(N_{ env}=N_{ n} N_{ env/nuc}\) is the number of envelope functions. In general, we pick

Fig. 1: Schematic of the Slater determinant (1a) and our NeurPf (1b). Where the Slater formulation requires exactly \(N_{ e}\) orbital functions, the Pfaffian formulation works for any number \(N_{ o}\{N_{},N_{}\}\) of orbital functions, indicated by the rectangular orbital blocks.

such that \(N_{ env} N_{ o}\). These atomic envelopes are linearly recombined into molecular envelopes, effectively enlarging \(\) to a \(N_{ k} N_{ o} N_{ env}\) tensor. Thanks to these rearrangements, we avoid constructing a five-dimensional tensor. Instead, we define the envelopes as

\[_{ki}(_{bj})=_{knj}}_{N_{ k} N_{ env } N_{ o}}_{kn}\|_{bj}-}_{n}\|)_{kbnj}}_{N_{ k} N_{ N} N_{ env} N _{ e}}.\] (16)

Concurrently, Pfau et al. (2024) presented similar bottleneck envelopes. However, we found ours to converge faster and not yield numerical instabilities. We discuss this further in App. B and I.

### Pretraining Pfaffian wave functions

Pretraining is essential in training neural wave functions and has frequently been observed to critically affect final energies (Gao and Gunnemann, 2023; von Glehn et al., 2023; Gerard et al., 2022). The pretraining aims to find orbital functions close to the ground state to stabilize the optimization. Traditionally, this is done by matching the orbitals of the neural wave function to the orbitals of a baseline wave function, typically a Hartree-Fock wave function \(_{}=(_{})\), by solving

\[_{}\|_{}-_{}\|_{2}^{2},\] (17)

for the neural network parameters \(\)(Pfau et al., 2020). Since our Pfaffian has \(N_{ o}\) orbitals while Hartree-Fock has \(N_{ e}\), we cannot directly apply this to our Pfaffian wave function. Further, as we predict orbitals per nucleus, our arbitrary orbital order may not align with Hartree-Fock.

We propose two alternative pretraining schemes for neural Pfaffian wave functions: one based on matching single-electron orbitals and one based on matching geminals, effectively two-electron orbitals. We need to expand the Hartree-Fock orbitals \(^{}\) to \(N_{ o}\) orbitals to match the single-electron orbitals directly. We construct \(^{}\) by padding the extra \(N_{ o}-N_{ e}\) orbitals with zeros. It can easily be verified that the wave function \(_{}=TA_{}}(_{}_{}_{}^{T})\), is equivalent to the original Hartree-Fock wave function, i.e., \(_{}=_{}=(_{})\) for any invertible skew-symmetric \(A_{}\). Further, note that the multiplication of \(_{}\) with any matrix \(T SO(N_{ o})\) from the special orthogonal group does not change \(_{}\). Thus, it suffices to match the single electron orbitals of \(_{}\) and \(_{}\) up to a rotation \(T SO(N_{ o})\), yielding the following optimization problem:

\[_{}_{T SO(N_{ o})}\|_{}-_{}T\|_{2}^{2}.\] (18)

We solve this alternatingly for \(T\) and \(\). To match the geminals \(_{}A_{}_{}^{T}\) and \(_{}A_{}_{}^{T}\), we have to account for the fact that the choice of \(A_{}\) is arbitrary as long as it is skew-symmetric and invertible. Again, we solve this optimization problem alternatingly by solving for \(A_{}=\{A SO(N_{ e}):A=-A^{T}\}\) and \(\):

\[_{}_{A_{}}\|_{}A_ {}_{}^{T}-_{}A_{}_{ }^{T}\|_{2}^{2}.\] (19)

While both formulations share the same minimizer, combining both yields the most stable results. We hypothesize that this is because the single-electron orbitals are more stable than the geminals and thus provide a better starting point for the optimization. In contrast, the latter provides a closer formulation of the neural network orbitals. Thus, we pretrain our neural Pfaffian wave functions by solving the optimization problem

\[_{}(_{T SO(N_{ o})}\|_{ }-_{}T\|_{2}^{2}+_{A_{}}\| _{}A_{}_{}^{T}-_{} A_{}_{}^{T}\|_{2}^{2})\] (20)

with weights \(,\). To optimize over the special orthogonal group \(SO(N_{ o})\), we use the Cayley transform (Gallier, 2013). App. C further details the procedure.

### Generalizing over systems

We now focus on generalizing the construction of our Pfaffian wave function for different systems. We accomplish the generalization similar to PESNet (Gao and Gunnemann, 2022) by introducing a second neural network, the MetaGNN \(:(^{3}_{+})^{N_{ n}}\) that acts upon the molecular structure, i.e., nuclei positions and charges, and parametrizes the electronic wave function \(_{}:^{N_{ e} 3}\) for the system of interest. As architecture for the wave function and MetaGNN, we use the same architecture as in Gao et al. (2023) with the exception being that we replace the Slater determinant with the Pfaffian as described in Sec. 4 and minor tweaks highlighted in App. D.4.

**Pfaffian.** To represent wave functions of different systems within a single NeurPf, we need to adapt the orbitals \(_{}\) and antisymmetrizer \(A_{}\) from Eq. (12) to the molecule. In doing so, we must ensure \(N_{}\{N_{},N_{}\}\). Otherwise, \(_{}A_{}_{}^{T}\) is singular, and the wave function is zero. One may solve this by picking \(N_{}\) large enough that \(N_{}\{N_{},N_{}\}\) for all molecules in the dataset. However, this is computationally expensive, does not reuse known orbitals in the problem, and simply moves the problem to even larger systems. Instead, we grow the number of orbitals \(N_{}\) with the system size by defining \(N_{/}\) orbitals per nucleus, as depicted in Fig. 2. This allows us to transfer orbitals from smaller systems to larger systems. We only need to ensure that \(N_{/}\) is larger than half the maximum number of electrons in a period, e.g., for the first period \(N_{/} 1\), for the second period \(N_{/} 5\).

The projection \(\) from Eq. (14) and the envelope decays \(\) are parametrized by node embeddings, while the envelope weights \(\) and the antisymmetrizer \(A_{}\) are derived from edge embeddings. We predict a \(N_{/} N_{}\) matrix per nucleus for \(\) and a \(N_{/}\) vector per nucleus for \(\). For the edge parameters \(\) and \(A_{}\), we predict a \(N_{/} N_{/}\) and a \(N_{/} N_{/}\) matrix per edge, respectively. These are concatenated into the \(N_{} N_{}\) and \(N_{} N_{}\) matrices \(\) and \(_{}\). The latter is antisymmetrized to get \(A_{}=(_{}-_{}^{T})\). We parametrize the spin-dependent scalars \(\) as node outputs for a fixed number of spin configurations \(N_{}\). Because the change in spin configuration does not grow with system size, \(N_{}\) is fixed. We generate two sets of these parameters, on for \(_{}\) and on for \(_{}\). App. D provides definitions for the wave function, the MetaGNN, and the parametrization.

**Pretraining.** Previous work like Gao & Gunnemann (2023a) needed to canonicalize the Hartree-Fock solutions for different systems before pretraining to ensure that the orbitals fit the neural network. Alternatively, Scherbela et al. (2023) relied on traditional quantum chemistry methods like Foster & Boys (1960)'s localization to canonicalize their orbitals in conjunction with sign equivariant neural networks. In contrast, we ensure that the transformed Hartree-Fock orbitals are similar across structures as we optimize \(T SO(N_{})\) and \(A_{}\) for each structure separately, which simultaneously also accounts for arbitrary rotations in the orbitals produced by Hartree-Fock.

**Limitations.** While our Pfaffian-based generalized wave function significantly improves accuracy on organic chemistry, we leave the transfer to periodic systems for future work (Kosmala et al., 2023). Further, due to the lac of low-level hardware/software support for the Pfaffian and the increased number of orbitals \(N_{}\{N_{},N_{}\}\), our Pfaffian is slower than a comparably-sized Slater determinant. While we solve the issue of enforcing the fermionic antisymmetry, our neural wave functions are still unaware of any symmetries of the wave function itself. These are challenging to describe and largely unknown, but their integration may improve generalization performance (Schutt et al., 2018). Finally, in classical single-structure calculations, NeurPf may not improve accuracies. App. P discusses the broader impact of our work.

## 5 Experiments

In the following, we evaluate NeurPf on several atomic and molecular systems by comparing it to Globe (Gao & Gunnemann, 2023a) and TAO (Scherbela et al., 2024). Concretely, we investigate the following: (1) Second-row elements and their ionization potentials and electron affinities. Globe cannot compute these due to its restriction to singlet state systems. (2) The challenging nitrogen potential energy surface where Globe significantly degraded performance when enlarging their training set with additional molecules. (3) The TinyMol dataset (Scherbela et al., 2024) to evaluate NeurPf's generalization capabilities across biochemical molecules. In interpreting the following results, one should mind the variational principle, i.e., lower energies are better for neural wave functions. Further, \(1\,\,^{-1}{}1.6\,E_{}\) is the typical threshold for chemical accuracy.

Like previous work, we optimize the neural wave function using the VMC framework from Sec. 2. We precondition the gradient with the Spring optimizer (Goldshlager et al., 2024). App. E details the setup further. App. F,I and J show an experiment on extension and additional ablations.

**Atomic systems and spin configurations.** We evaluate NeurPf on second-row elements and their ionization potentials and electron affinities. These systems are particularly interesting as they representa wide range of spin configurations. We cannot use Globe on such systems because they differ from the singlet state assumption. Instead, we compare our results to the single-structure calculations from Pfau et al. (2020)'s FermiNet and the exact results from Chakravorty et al. (1993); Klopper et al. (2010). In App. G, we repeat this experiment for metals.

Fig. 3 displays the ground state energy, electron affinity, and ionization potential errors of NeurPf during training compared to the reference energies from Pfau et al. (2020); Chakravorty et al. (1993); Klopper et al. (2010). It is apparent that NeurPf reaches chemical accuracy relative to the exact results while only training a single neural network for all systems. While separately optimized FermiNets may achieve lower errors, Pfau et al. (2020) trained 21 neural networks for 200k steps each compared to a single NeurPf trained for 200k steps, i.e., 21 times fewer steps and samples. Whereas Gao and Gunnemann (2023); Scherbela et al. (2023) focus on singlet state systems or stable biochemical molecules, NeurPf demonstrates that a generalized wave function need not be restricted to such simple systems and can even generalize to a wide range of electronic configurations.

**Effect of uncorrelated data.** Next, we evaluate NeurPf on the nitrogen potential energy surface, a traditionally challenging system due to its high electron correlation effects (Lyakh et al., 2012). This is particularly interesting as Gao and Gunnemann (2023) observed a significant accuracy degradation when reformulating their wave function to generalize over different systems. In particular, they found that training only on the nitrogen dimer leads to significantly lower errors than training with an ethene-augmented dataset, indicating an accuracy penalty in generalization. We replicate their setup and compare the performance of NeurPf trained on the nitrogen energy surface with and without additional ethene structures. Like Gao and Gunnemann (2023), the nitrogen structures are taken from Pfau et al. (2020) and the ethene structures from Scherbela et al. (2022). As additional references, we plot Gao and Gunnemann (2022)'s PESNet and Fu et al. (2023)'s FermiNet results.

Fig. 4 shows the error potential energy surface relative to the experimental results from Le Roy et al. (2006). NeurPf reduces the average error on the energy surface from Globe's \(2.7\,E_{}\) to \(2\,E_{}\) when training solely on nitrogen structures. When adding the ethene structures, Globe's error increases to \(5.3\,E_{}\) while NeurPf's error stays constant at \(2\,E_{}\), a lower error than the Globe without the augmented dataset. These results indicate NeurPf's strong capabilities in approximating ground states while allowing for generalization across different systems without a significant loss in accuracy.

Fig. 4: Potential energy surface of nitrogen. Energies are relative to Le Roy et al. (2006).

Fig. 3: Ground state, electron affinity, and ionization potential errors of second-row elements during training. A single NeurPf has been trained on all systems jointly while references (Pfau et al., 2020) were calculated separately for each system. Energies are averaged over the last 10% of steps.

TinyMol dataset.Finally, we look at learning a generalized wave function over different molecules and structures. We use the TinyMol dataset (Scherbela et al., 2024), consisting of a small and large dataset. The dataset includes 'gold-standard' CCSD(T) CBS energies. The small set consists of 3 molecules with 2 heavy atoms, while the large set covers 4 molecules with 3 heavy atoms. For each molecule, 10 structures are provided. Here, we compare again both Globe (+Moon) and TAO to NeurPf. All models are directly trained on the small and large test sets.

Fig. 5 shows the mean energy difference to CCSD(T) at different stages of the training. We refer to App. K for a per molecule error attribution. It is apparent that NeurPf yields lower errors than the TAO and Globe after at least 500 steps. On the small structures, NeurPf even matches the CCSD(T) baseline after 16k steps and achieves \(1.9\,E_{}\) lower energies after 32k steps. Since VMC methods are variational, i.e., lower energies are always better, NeurPf is more accurate than the CCSD(T) CBS reference. Compared to TAO and Globe, NeurPf reports \(5.9\,E_{}\) and \(11.3\,E_{}\) lower energies, respectively. On the large structures, we observe a similar pattern where we find NeurPf having a 25 times smaller error than TAO during the early stages of training and reaching \(21.1\,E_{}\) lower energies after 32k steps - a 6 times lower error compared to the CCSD(T) baseline. Note that since the CCSD(T) (CBS) energies are neither exact nor variational, the true error to the ground state is unknown. Still, we provide additional numbers for a NeurPf trained for 128k steps in App. K. There, we find NeurPf yielding \(4.4\,E_{}\) lower energies on the large structures. These results show that a generalized wave function can achieve high accuracy on various molecular structures without pretraining when not relying on hand-crafted algorithms or Hartree-Fock calculations. For additional experiments, we refer the reader to App. L where we first pretrain TAO and NeurPf on a separate training set and, then, finetune on the small and large test sets and App. M for a comparison of joint and separate optimization.

## 6 Conclusion

In this work, we established a new way of parametrizing neural network wave functions for generalization across molecules via overparametrization with Pfaffians. Our Neural Pfaffian is more accurate, simpler to implement, fully learnable, and applicable to any molecular system compared to previous work. The wave function changes smoothly with the structure, avoiding the discrete orbital selection problem previously solved via hand-crafted algorithms or Hartree-Fock. Additionally, we introduced a memory-efficient implementation of the exponential envelopes, reducing memory requirements while accelerating convergence. Further, we presented a pretraining scheme for Pfaffians enabling initialization with Hartree-Fock - a crucial step for molecular systems. Our experimental evaluation demonstrated that our Neural Pfaffian can generalize across different ionizations of various systems, stay accurate when enlarging datasets, and set a new state of the art by outperforming previous neural wave functions and the reference CCSD(T) CBS on the TinyMol dataset. These developments open the door for new neural wave functions applications, e.g., to generate reference data for machine-learning force fields or density functional theory (Cheng et al., 2024; Gao et al., 2024).

Fig. 5: Convergence of mean energy difference on the TinyMol dataset from Scherbela et al. (2024). The y-axis is linear \(<1\) and logarithmic \( 1\). Due to the variational principle, NeurPf is better than the reference CCSD(T) on the small molecules.

**Acknowledgments.** We greatly thank Simon Geisler for our valuable discussions. Further, we thank Valerie Engelmayer, Leo Schwinn, and Aman Saxena for their invaluable feedback on the manuscript. Funded by the Federal Ministry of Education and Research (BMBF) and the Free State of Bavaria under the Excellence Strategy of the Federal Government and the Lander.