# MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making

Yubin Kim\({}^{1}\) Chanwoo Park\({}^{1}\) Hyewon Jeong\({}^{1}\)\({}^{}\) Yik Siu Chan\({}^{1}\) Xuhai Xu\({}^{1}\) Daniel McDuff\({}^{2}\) Hyeonhoon Lee\({}^{3}\)

**Marzykeh Ghassemi\({}^{1}\) Cynthia Breazeal\({}^{1}\) Hae Won Park\({}^{1}\)**

\({}^{1}\)Massachusetts Institute of Technology

\({}^{2}\)Google Research

\({}^{3}\)Seoul National University Hospital

{ybkim95,cpark97,hyewonj,yiksiuc,xoxu,mghassem,cythdiab,haewon}@mit.edu

dmcduff@google.com

hhoon@snu.ac.kr

###### Abstract

Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named **M**edical **D**ecision-making **Agents** (**MDAgents**) that helps to address this gap by automatically assigning a collaboration structure to a team of LLMs. The assigned solo or group collaboration structure is tailored to the medical task at hand, a simple emulation inspired by the way real-world medical decision-making processes are adapted to tasks of different complexities. We evaluate our framework and baseline methods using state-of-the-art LLMs across a suite of real-world medical knowledge and medical diagnosis benchmarks, including a comparison of LLMs' medical complexity classification against human physicians2. MDAgents achieved the **best performance in seven out of ten** benchmarks on tasks requiring an understanding of medical knowledge and multi-modal reasoning, showing a significant **improvement of up to 4.2%** (\(p<0.05\)) compared to previous methods' best performances. Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy across diverse medical tasks. Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy **improvement of 11.8%**. Our code can be found at https://github.com/mitmedialab/MDAgents.

## 1 Introduction

Medical Decision-Making (MDM) is a multifaceted and intricate process in which clinicians collaboratively navigate diverse sources of information to reach a precise and specific conclusion . For instance, a primary care physician (PCP) may refer a patient to a specialist when faced with a complex case, or a patient visiting the emergency department or urgent care might be triaged and then directed to a specialist for further evaluation . MDM involves interpreting complex and multi-modal data, such as imaging, electronic health records (EHR), physiological signals, and genetic information, while rapidly integrating new medical research into clinical practice . Recently, Large Language Models (LLMs) have shown potential for AI support in MDM . It is known that they are able to process and synthesize large volumes of medical literature  and clinical information , as well as support probabilistic  and causal  reasoning, makes LLMs promising tools. However, there is no silver bullet in medical applications that require careful design.

While decision-making tools including multi-agent LLMs [11; 86] have shown promise in non-medical domains [31; 32; 44; 46; 62; 65], their evaluation in health applications has been limited. To date, their "generalist" design has not effectively integrated the real-world systematic MDM process  which requires an adaptive, collaborative, and tiered approach. Clinicians consider the current and past history of the patient, available evidence from medical literature, and their domain expertise and experience  for MDM. One example of MDM is to triage patients in emergency room based on the severity and complexity of their medical conditions [12; 26; 87]. Patients with pathogenomic, single uncomplicated acute conditions, or stable chronic conditions that their PCP could manage  could be low complexity cases. On the other hand, patients with injuries that involve multiple organs, chronic conditions with side effects, or superimposed diseases who often require multiple collaborative discussions (MDT) or sequential consultations (ICT) among specialty physicians [27; 61] are considered high complexity cases 3.

Inspired by the way that clinicians make decisions in practice, we propose **M**edical **D**ecision-making **Agents** (**MDAgents**), an adaptive medical decision-making framework that leverages LLMs to emulate the hierarchical diagnosis procedures ranging from individual clinicians to collaborative clinician teams (Figure 1). MDAgents work in three steps: 1) Medical complexity check; 2) Recruitment based on medical complexity; 3) Analysis and synthesis and 4) Final decision-making to return the answer. Our contributions are threefold:

1. We introduce MDAgents, the first adaptive decision-making framework for LLMs that mirrors real-world MDM processes via dynamic collaboration among AI agents based on task complexity.
2. MDAgents demonstrate superior performance in accuracy over previous solo and group methods on 7 out of 10 medical benchmarks, and we show an effective trade-off between performance and efficiency (i.e. the number of API calls) by varying the number of agents.
3. We provide rigorous testing under various hyperparameters (e.g. temperatures), demonstrating better robustness of MDAgents compared to solo and group methods. Furthermore, our ablations evidence MDAgents' ability to find the appropriate complexity level for each MDM instance.

## 2 Related Works

Language Models in Medical Decision-MakingLLMs have shown promise in a range of applications within the medical field [14; 37; 40; 48; 63; 75; 76; 90; 96]. They can answer questions from medical exams [43; 52], perform biomedical research , clinical risk prediction , and clinical diagnosis [55; 67]. Medical LLMs are also evaluated on generative tasks, including creating medical reports , describing medical images , constructing differentials , performing diagnostic dialogue with patients , and generating psychiatric evaluations of interviews . To advance the capabilities of medical LLMs, two main approaches have been explored: (1) training with

Figure 1: **Medical Decision-Making Agents (MDAgents) framework. Given a medical query from different medical datasets, the framework performs 1) medical complexity check, 2) recruitment, 3) analysis and synthesis, and 4) decision-making steps.**

domain-specific data , and (2) applying inference-time techniques such as prompt engineering  and Retrieval Augmented Generation (RAG) . While initial research has been concentrated on pre-training and fine-tuning with medical knowledge, the rise of large general-purpose LLMs has enabled training-free methods where models leverage their latent medical knowledge. For example, GPT-4 , with richer prompt crafting, surpasses the passing score on USMLE by over 20 points and with prompt tuning can outperform fine-tuned models including Med-PaLM [58; 59]. The promise of general-purpose models has thus inspired various techniques such as Medprompt and ensemble refinement to improve LLM reasoning , as well as RAG tools that use external resources to improve the factuality and completeness of LLM responses [38; 92]. Frameworks like MEDIQ  and UoT  advance LLM reliability in clinical settings by enhancing information-seeking through adaptive question-asking and uncertainty reduction, supporting more realistic diagnostic processes. Our approach leverages these techniques and the capabilities of general-purpose models while acknowledging that a solitary LLM [37; 48; 90] may not fully encapsulate the collaborative and multidisciplinary nature of real-world MDM. We thus emphasize joining multiple expert LLMs for effective collaboration in order to solve complicated medical tasks with greater accuracy.

Multi-Agent CollaborationAn array of studies have explored effective collaboration frameworks between multiple LLM agents [47; 86] to enhance capability above and beyond an individual LLM . A common framework is role-playing, where each agent adopts a specific role (e.g. an Assistant Agent or a Manager Agent), a task is then broken down into sub-steps and solved collaboratively [47; 86]. While role-playing focuses on collaboration and multi-step problem-solving , another framework, "multi-agent debate", prompts each agent to solve the task independently . Then, they reason through other agents' answers to converge on a shared response, this approach can improve the factuality, mathematical ability and reasoning capabilities of the multi-agent solution [17; 50]. Similar frameworks include voting , multi-disciplinary collaboration , group discussions (ReConcile ), and negotiating . Table 1 compares existing setups across key dimensions in multi-agent interaction. Although these frameworks have shown improvement in the respective tasks, they rely on a pre-determined number of agents and interaction settings. When applied on a wider variety of tasks, this static architecture may lead to suboptimal multi-agent configurations, negatively impacting performance . Furthermore, multi-agent approaches run the risk of being computationally inefficient or expensive to employ and need to justify these costs with noticable performance gains . Given that different models and frameworks could generalize better to different tasks , we propose a framework that dynamically assigns the optimal collaboration strategy at inference time based on the complexity of the query. We apply our strategy to MDM, a task that requires teamwork and should benefit from multi-agent collaboration .

## 3 MDAgents: Medical Decision-making Agents

The design of MDAgents (Figures 1 and 2) incorporates four stages: **1) Medical Complexity Check** - The system evaluates the medical query, categorizing it as _low_, _moderate_, or _high_ complexity based on clinical decision-making techniques [6; 7; 21; 71; 84]. **2) Expert Recruitment** - Based on complexity, the framework activates a single Primary Care Clinician (PCC) for low complexity issues, or a Multi-disciplinary Team (MDT) or Integrated Care Team (ICT) for moderate or high complexities [7; 18; 21; 34; 45; 71]. **3) Analysis and Synthesis** - Solo queries use prompting techniques like Chain-of-Thought (CoT) and Self-Consistency (SC). MDTs involve multiple LLM agents forming a consensus, while ICTs synthesize information for the most complex cases. **4) Decision-making** - The final stage synthesizes all inputs to provide a well-informed answer to the medical query.

    &  &  &  &  &  &  \\  &  & & & & & \\   & & & & & & \\  & & & & & & \\
**Multiple Roles** & ✓ & ✗ & ✓ & ✓ & ✓ & ✓ \\
**Early Stopping** & ✓ & ✗ & ✓ & ✓ & ✓ & ✗ \\
**Refinement** & ✓ & ✗ & ✗ & ✓ & ✓ & ✗ \\
**Complexity Check** & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\
**Multi-party Chat** & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ \\
**Conversation Pattern** & Flexible & Static & Static & Static & Static & Static \\   

Table 1: Comparison between our framework and previous methods (Solo and Group). Among these works, MDAgents is the only one to perform all key dimensions of LLM decision-making.

### Agent Roles

**Moderator.** The moderator agent functions as a general practitioner (GP) or emergency department doctor who first triages the medical query. This agent assesses the complexity of the problem and determines whether it should be handled by a single agent, a MDT, or an ICT. The moderator ensures the appropriate pathway be selected based on the query's complexity and oversees the entire decision-making process.

**Recruiter.** The recruiter agent is responsible for assembling the appropriate team of specialist agents based on the complexity assessment of the moderator. The recruiter may assign a single PCP agent for low-complexity cases, while MDT or ICT with relevant expertise will be formed for moderate and high-complexity cases.

**General Doctor/Specialist.** These agents are domain-specific or general physicians recruited by the recruiter agent. Depending on the complexity of the case, they may work independently or as part of a team. General physicians handle less complex, routine cases, whereas specialists are recruited for their specific expertise in more complex scenarios. These agents engage in the collaborative decision-making process, contributing their specialized knowledge to reach a consensus or provide detailed reports for high-complexity cases.

### Medical Complexity Classification (Line 1 of Algorithm 1, Appendix C.2)

The first step in the MDAgents framework is to determine the complexity of a given medical query \(q\) by the _moderator_ LLM which functions as a _generalist practitioner_ (GP). The moderator aims to act as a classifier to return the complexity level of the given medical query, it is provided with the information on how medical complexity should be defined and is instructed to classify the query into one of three different complexity levels:

1. _Low_ - Simple, well-defined medical issues that can be resolved by a single PCP agent. These typically include common, acute illnesses or stable chronic conditions where the medical needs are predictable and require minimal interdisciplinary coordination.
2. _Moderate_ - The medical issues involve multiple interacting factors, necessitating a collaborative approach among an MDT. These scenarios require integration of diverse medical knowledge areas and coordination between specialists through consultation to develop effective care strategies.
3. _High_ - Complex medical scenarios that demand extensive coordination and combined expertise from an ICT. These cases often involve multiple chronic conditions, complicated surgical or trauma cases, and decision-makings that integrates specialists from different healthcare departments.

### Expert Recruitment (Line 3, 7, 17 of Algorithm 1)

Given a medical query, the goal of the _recruiter_ LLM is to enlist domain experts as individuals, in groups, or as multiple teams, based on the complexity levels determined by the _moderator_ LLM. Specifically, we assign medical expertise and roles to multiple LLMs, instructing them to either act independently as solo medical agents or collaborate with other medical experts in a team. In Figure 9 in the Appendix, we also provide frequently recruited agents for each benchmark as a reference.

Figure 2: Illustrative example of MDAgents in a _moderate_ complexity case from the PMC-VQA dataset. More detailed case studies can be found in Figure 11 and 12 in the Appendix.

### Medical Collaboration and Refinement

The initial assessment protocol of our decision-making framework categorizes query complexity into _low_, _moderate_, and _high_. This categorization is grounded in established medical constructs such as acuity  for straightforward cases, comorbidity  and case management complexity  for intermediate and multi-disciplinary care requirements, and severity of illness  for high complexity cases requiring comprehensive management. We outlines the specific refinement approach:

_Low_ - Straightforward cases (Line 2-4 of Algorithm 1).For queries classified under Low complexity, characterized by straightforward clinical decision pathways, a single PCP agent (Figure 10 (a)) is deployed by the definition in Section 3.2. The domain expert who is recruited by _recuriter_ LLM, applies few-shot prompting to the problem. The output answer, denoted as \(ans\), is directly obtained from the agent's response to \(Q\) without the need for iterative refinement, formalized as \(ans=Agent(Q)\), with \(Agent\) representing the engaged PCP agent.

_Moderate_ - Intermediate complexity cases (Line 6-14 of Algorithm 1).In addressing more complex queries, the utilization of an MDT (Figure 10 (b) and (c)) approach has been increasingly recognized for its effectiveness in producing comprehensive and nuanced solutions . The MDT framework leverages the collective expertise of professionals from diverse disciplines, facilitating a holistic examination of the query at hand. This collaborative method is particularly advantageous in scenarios where the complexity of a problem transcends the scope of a single domain, necessitating a fusion of insights from various specialties [7; 71]. The MDT approach not only enhances decision-making quality through the integration of multidimensional perspectives but also significantly improves the adaptability and efficiency of the problem-solving process .

Building upon this foundation, our framework specifically addresses queries of moderate complexity through a structured, multi-tiered collaborative approach. An MDT recruited by _recruiter_ LLM (see Figure 10 in Appendix) starts an iterative discussion process aimed at reaching a consensus with at most \(R\) rounds (Line 10-12). For each round \(r R\), agents \(A_{i},i 1,,N\) indicate participation and preferred interlocutors. The system facilitates message exchanges for \(T\) turns. If consensus is not reached and agents agree to continue, a new round begins with access to previous conversations. For every round, consensus within the MDT is determined by parsing and comparing their opinions. In the event of a disagreement, the moderator agent, consistent with the one described in Section 3.2 reviews the MDT's discourse and formulates feedback for each agent.

_High_ - Complex care cases (Line 17-24 of Algorithm 1).In contrast to the MDT approach, the ICT (Figure 10 (d)) paradigm is essential for addressing the highest tier of query complexity in healthcare. This structured progression through the ICT ensures a depth of analysis that is specialized and focused at each stage of the decision-making process. Beginning with the Initial Assessment Team, moving through various diagnostic teams, and culminating with the Final Review & Decision Team, our ICT model aligns specialist insights into a cohesive narrative that informs the ultimate decision (Appendix Algorithms 1 Lines 19-21). A key component of this process is the report generation process described in Appendix with the prompt, where each team, led by a lead clinician, collaboratively produces a comprehensive report synthesizing their findings. This phased approach, supported by evidence from recent healthcare studies, has been shown to enhance the precision of clinical decision-making, as each team builds upon the foundation laid by the previous, ensuring a meticulous and refined examination of complex medical cases . The resultant reports, accumulating throughout the ICT process, are not only reflective of comprehensive medical evaluations but also of a systematic and layered analysis that is critical in the management of intricate health scenarios .

### Decision-making

In the final stage of our framework, the decision-maker agent synthesizes the diverse inputs generated throughout the decision-making process to arrive at a well-informed final answer to the medical query \(q\). This synthesis involves several components depending on the complexity level of the query:

1. _Low_: Directly utilizes the initial response from the primary decision-making agent.
2. _Moderate_: Incorporates the conversation history (_Interaction_) between the recruited agents to understand the nuances and disagreements in their responses.
3. _High_: Considers detailed reports (_Reports_) generated by the agents, which include comprehensive analyses and justifications for their diagnostic suggestions.

The decision-making process is formulated as \(ans=Agent()\) where the final answer, _ans_ is determined by integrating the outputs from analysis and synthesis step based on its medical complexities. This integration employs ensemble techniques such as temperature ensembles to ensure the decision is robust and reflects a consensus among the models when applicable (see Appendix C.2 for details).

[MISSING_PAGE_FAIL:6]

the application of medical knowledge and clinical reasoning. JAMA Clinical Challenge presents challenging real-world clinical cases with diagnosis or treatment decision-making questions, testing the model's clinical reasoning (Figure 8 in Appendix shows complexity distribution for each dataset)

**Diagnostic Reasoning** DDXPlus  and SymCat  involve clinical vignettes that require differential diagnosis, closely mimicking the diagnostic process of physicians. These tasks test the model's ability to reason through symptoms and clinical data to suggest possible medical conditions, evaluating the AI's diagnostic reasoning abilities similar to a clinical setting. SymCat  uses synthetic patient records constructed from a public disease-symptom data source and is enhanced with additional contextual information through the NLICE method.

**Medical Visual Interpretation** Path-VQA , PMC-VQA , MedVidQA , and MIMIC-CXR  challenge models to interpret medical images and videos, requiring integration of visual data with clinical knowledge. PathVQA focuses on answering questions based on pathology images, testing AI's capability to interpret complex visual information from medical images. PMC-VQA evaluates AI's proficiency in deriving answers from both text and images found in scientific publications. MedVidQA extends to video-based content, where AI models need to process information from medical procedure videos. MIMIC-CXR-VQA specifically targets chest radiographs, utilizing a diverse and large-scale dataset designed for visual question-answering tasks in the medical domain.

**Baseline Methods**

* **Solo:** The baseline methods considered for the Solo setting include the following: Zero-shot  directly incorporates a prompt to facilitate inference, while Few-shot  involves a small number of examples. Few-shot CoT  integrates rationales before deducing the answer. Few-shot CoT-SC  builds upon Few-shot CoT by sampling multiple chains to yield the majority answer. Ensemble Refinement (ER)  is a prompting strategy that conditions model responses on multiple reasoning paths to bolster the reasoning capabilities of LLMs. Medprompt  is a composition of several prompting strategies that enhances the performance of LLMs and achieves state-of-the-art results on multiple benchmark datasets, including medical and non-medical domains.
* **Group:** We tested five group decision-making methods: Voting , MedAgents , Reconcile , AutoGen , and DyLAN . Autogen was based on four agents, with one User, one Clinician, one Medical Expert, and one Moderator, with one response per agent . DyLAN setup followed the base implementations of four agents with no specific roles and four maximum rounds of interaction . While the methods support multiple models, GPT-4 was used for all agents.

### Results

In Table 2, we report the classification accuracy on MedQA, PubMedQA, DDXPlus, SymCat, JAMA, MedBullets, Path-VQA, PMC-VQA and MedVidQA dataset. We compare our method (Adaptive) with several baselines in both Solo and Group settings.

**Adaptive method outperforms Solo and Group settings.** As depicted in Figure 4 and Table 2, MDAgents significantly outperforms (p < 0.05) both Solo and Group setting methods, showing best performance in **7 out of 10** medical benchmarks tested. This reveals the effectiveness of adaptive strategies integrated within our system, particularly when navigating through the text-only (e.g., DDXPlus where it outperformed the best performance of single-agent by 5.2% and multi-agent by 9.5%) and text-image datasets (e.g., Path-VQA, PMC-VQA and MIMIC-CXR). Our approach not only comprerehends textual information with high precision but also adeptly synthesizes visual data, a pivotal capability in medical diagnostic evaluations.

Figure 3: Experiment with the MedQA dataset (\(N\)=25 randomly sampled questions). (a) LLM’s capability to classify complexity. (b-d) Evaluating 25 medical problems by solving each one 10 times at various complexity levels. The x-axis represents the accuracy achieved for each problem, while the y-axis shows the number of problems that reached that level of accuracy.

Why Do Adaptive Decision-making Framework Work Well?

It is important to accurately assign difficulty levels to medical questions. For instance, if a medical question is obviously easy, utilizing a team of specialists (such as an IDT) might be excessive and potentially lead to overly pessimistic approaches. Conversely, if a difficult medical question is only tackled by a PCP, the problem might not be adequately addressed. The core issue here is the LLM's capability to classify the difficulty of medical questions appropriately. If an LLM inaccurately classifies the difficulty level, the chosen medical solution may not be suitable, potentially leading to the wrong decision making. Therefore, understanding what constitutes an appropriate difficulty level is essential.

We hypothesize that an LLM, functioning as a classifier, will select the optimal complexity level for each MDM problem. This hypothesis is supported by Figure 3, which illustrates that the model appropriately matches the complexity levels; low, moderate, and high of the given problem. To determine this, we assessed the accuracy of solutions across various difficulty levels. Specifically, we evaluated 25 medical problems by repeating each problem for 10 times at each difficulty level. By measuring the success rate, we aimed to identify the difficulty level that yielded the highest accuracy. This approach ensures that the LLM's complexity classification aligns with the most effective and accurate medical solutions, thereby optimizing the application of medical expertise to each question. Formally, for any given problem \(P\), we denote the probability that the correct answer can be solved at a specific complexity level as \(p_{}(P)\), where complexity-level \(\{,,\}\). \((P)\{,,\}\) refers to the complexity level that has the highest probability among \(p_{}(P)\), \(p_{}(P)\), and \(p_{}(P)\). Similarly, \((P)\) is the complexity level with the lowest probability, and \((P)\) is the one with the middle probability. We denote \(a\), \(b\), and \(c\) as the probabilities that the LLM selects the complexity levels corresponding to \(\), \(\), and \(\), respectively. Thus, the accuracy of our system for problem \(P\) can be described by \(a p_{}(P)+b p_{}(P)+c p_{}(P)\), and the overall accuracy is given by \(_{P}[a p_{}(P)+b p_{}(P)+c  p_{}(P)]\). The estimated values of \(a,b,c\) are \(a=0.81 0.29\), \(b=0.11 0.28\), and \(c=0.08 0.16\), which indicates that LLM can provide an optimal complexity level with probability at least \(80\%\). These findings suggest that a classifier LLM can implicitly simulate various complexity levels and optimally adapt to the complexity required for each medical problem, as shown in Figure 3. This ability to adjust complexity dynamically proves to be crucial for applying LLMs effectively in MDM contexts as shown by the competitiveness of our Adaptive approach.

Solo vs. Group Setting in MDM.The experimental results reveal distinct performance patterns between Solo and Group settings across various medical benchmarks. In simpler datasets like MedQA, solo methods, leveraging Few-shot CoT and CoT-SC, achieved up to 83.9% accuracy compared to the group's best of 81.3%. Conversely, for more complex datasets like SymCat, group settings perform better, with SymCat showing 91.9% accuracy in the group settings versus 88.7% in solo settings. Notably, group settings (e.g. Weighted Voting, Reconcile) performed better in multi-modal datasets such as Path-VQA (_image + text_), MedVidQA (_video + text_), and MIMIC-CXR (_image + text_), highlighting the advantage of collaborative process in complex tasks. This result aligns with findings from , which showed that pooled diagnoses from groups of physicians significantly outperformed those of individual physicians, with accuracy increasing as group size increased. Overall, solo settings outperformed group settings in four benchmarks, while group settings outperformed solo in six benchmarks. These results reveals that while solo methods excel in straightforward tasks, group settings provide better performance in complex, multi-faceted tasks requiring diverse expertise.

### Ablation Studies

Impact of Complexity Selection.We evaluate the importance of the complexity assessment and adaptive process through ablation studies (Figure 5). Our adaptive method significantly outperforms static complexity assignments across different modality benchmarks. For _text_-only queries, the Adaptive method achieves an accuracy of 81.2%, significantly higher than _low_ (64.2%), _moderate_ (71.6%), and _high_ (65.8%) settings. Interestingly, 64% of the text-only queries were classified as

Figure 4: Our method outperforms Solo and Group settings across different medical benchmarks.

high_ complexity, indicating that many text-based queries required in-depth analysis with different expertise. In the _image + text_ modality, the Adaptive method classified 55% of the queries as _low_ complexity, suggesting that the visual information often provides clear and straightforward cues that simplify the decision-making process. Finally, for _video + text_ queries, 87% of these queries were classified as _low_ complexity, reflecting that the dynamic visual data in conjunction with text can often be straightforwardly interpreted. However, further evaluation on more challenging video medical datasets is needed, as MedVidQA contains relatively less complex medical knowledge.

**Impact of Moderator's Review and RAG** Table 3 examines the impact of incorporating external medical knowledge and moderator reviews into the MDAgents framework on accuracy. MedRAG  is a systematic toolkit for Retrieval-Augmented Generation (RAG) that leverages various corpora; biomedical, clinical and general medicine, to provide comprehensive knowledge. The baseline accuracy of MDAgents is 71.8%. Integrating MedRAG increases accuracy to 75.2% (up 4.7%), while the moderator's review alone raises it to 77.6% (up 8.1%). The combined use of both methods achieves the highest accuracy at 80.3% (up 11.8%).

The results indicate that MedRAG and moderator review both enhance performance, with their combined effect being synergistic. This highlights that leveraging recent external knowledge and structured feedback mechanisms is crucial for refining and converging on accurate medical decisions. This improvement underscores the importance of a hybrid strategy, aligning with real-world practices of continuous learning and expert consultation to optimize performance in medical applications.

### Impact of Number of Agents in Group Setting.

Our experiment with varying the number of agents in a collaborative Group setting (Appendix Figure 6 (a-b)) shows that a higher number of agents does not lead to better performance. Rather, our Adaptive method achieves optimal performance with fewer agents (_N_=3, peak accuracy of 83.5%) by intelligently calibrating the number of collaborating agents. This not only indicates efficiency in decision-making but also computational and economic benefits, considering the reduced number of API calls needed, especially when contrasted with the Solo and Group settings.

With regards to computational efficiency, the Solo setting (5-shot CoT-SC) resulted in a 6.0 and Group setting (MedAgents with _N_=5) resulted in a 20.3 API calls, suggesting a high computational cost without a corresponding increase in accuracy. On the other hand, our Adaptive method exhibits a more economical use of resources, demonstrated by fewer API calls (9.3 with _N_=3) while maintaining high accuracy, a critical factor in deploying scalable and cost-effective medical AI solutions.

### Robustness of MDAgents with different parameters.

Our Adaptive approach shows resilience to changes in temperature (Appendix Figure 6 (c), low (_T_=0.3) and high (_T_=1.2)) with performance improving under higher temperatures. This suggests that our model can utilize the creative and diverse outputs generated at higher temperatures to enhance decision-making, a property that is not as pronounced in the Solo and Group conditions. This robustness is particularly valuable in real-world medical domains with high uncertainty and ambiguity in datasets . Additionally, studies have shown that creative diagnostic approaches

 
**Method** & **Avg. Accuracy (\%)** \\  MDAgents (Ours) & 71.8 \\ \(\) MedRAG & 75.2 (\(\) **4.7 \%**) \\ + Moderator’s Review & 77.6 (\(\) **8.1 \%**) \\ + Moderator’s Review \& MedRAG & 80.3 (\(\) **11.8 \%**) \\  

Table 3: Ablations for the impact of moderator’s review and MedRAG. The Accuracy were averaged accuracy across all datasets.

Figure 5: Impact of complexity selection of the query. Accuracy of each ablation on _text_-only (left), _text+image_ (center) and _text+video_ (right) benchmarks are reported.

can mitigate cognitive biases and improve diagnostic accuracy , while fostering flexibility and adaptability in decision-making . These insights support the enhanced performance observed under higher temperatures in our framework. However, the future work should explore a wider range of temperatures to fully understand the robustness and adaptability of our approach.

### Convergence Trends in Consensus Dynamics

There is clear trend towards consensus among MDAgents cross various data modalities (Figure 7). The _text+video_ modality demonstrates a rapid convergence, reflecting the agents' efficient processing of combined textual and visual cues. On the other hand, the _text+image_ and _text_-only modalities display a more gradual decline in entropy, indicating a progressive narrowing of interpretative diversity among the agents. Despite the differing rates and initial conditions, all modalities exhibit convergence of agent opinions over time. This uniformity in reaching consensus highlights the MDAAgents' capability to integrate and reconcile information. Please refer to Appendix B for a detailed explanation of the entropy calculation.

## 5 Conclusion

This paper introduces **MDAAgents**, a framework designed to enhance the utility of LLMs in complex medical decision-making by dynamically structuring effective collaboration models. To reflect the nuanced consultation aspects in clinical settings, MDAgents adaptively assigns LLMs either to roles independently or within groups, depending on the task's complexity. This emulation of real-world medical decision processes has been comprehensively evaluated, with MDAgents outperforming previous solo and group methods in **7 out of 10** medical benchmarks. The case study illustrates the practical efficacy and collaborative dynamics of our proposed framework, providing insights into how differing expert opinions are synthesized to reach a more accurate diagnosis. This is evidenced by our agents' ability to converge on the correct diagnosis despite initially divergent perspectives. Ablation studies further elucidate the individual contributions of agents and strategies within the system, revealing the critical components and interactions that drive the framework's success. By harnessing the strength of multi-modal reasoning and fostering a collaborative process among LLM agents, our framework opens up new possibilities for enhancing LLM-assisted medical diagnosis systems, pushing the boundaries of automated clinical reasoning.

Figure 6: Impact of the number of agents on (a) Accuracy, (b) Number of API Calls on medical benchmarks with GPT-4 (V) and (c) Performance of three different settings under low (\(T\)=0.3) and high (\(T\)=1.2) temperatures on medical benchmarks. Our Adaptive setting shows better robustness to different temperatures and even takes advantage of higher temperatures.

Figure 7: An illustration of consensus entropy in group collaboration process of MDAgents (w/ Gemini-Pro (Vision), \(N\)=30 for each dataset) on medical benchmarks with different modality inputs.

[MISSING_PAGE_FAIL:11]

*  Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero, Jan-Niklas Eckardt, Narmin Ghaffari Laleh, Chiara Maria Lavinia Loffler, Sophie-Caroline Schwarzkopf, Michaela Unger, Gregory P Veldhuizen, et al. The future landscape of large language models in medicine. _Communications medicine_, 3(1):141, 2023.
*  Nick Craddock and Laurence Mynors-Wallis. Psychiatric diagnosis: impersonal, imperfect and important. _The British Journal of Psychiatry_, 204(2):93-95, 2014.
*  Lesley F Degner and Jeffrey A Sloan. Decision making during serious illness: what role do patients really want to play? _Journal of clinical epidemiology_, 45(9):941-950, 1992.
*  Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate, 2023.
*  Carolyn Ee, James Lake, Joseph Firth, Fiona Hargraves, M De Manincor, Tanya Meade, Wolfgang Marx, and Jerome Sarris. An integrative collaborative care model for people with mental illness and physical comorbidities. _International Journal of Mental Health Systems_, 14:1-16, 2020.
*  Arthur S Elstein and Alan Schwarz. Clinical problem solving and diagnostic decision making: selective review of the cognitive literature. _BMJ_, 324(7339):729-732, 2002.
*  Glyn Elwyn, Dominick Frosch, Richard Thomson, Natalie Joseph-Williams, Amy Lloyd, Paul Kinnersley, Emma Cording, Dave Tomson, Carole Dodd, Stephen Rollnick, et al. Shared decision making: a model for clinical practice. _Journal of general internal medicine_, 27:1361-1367, 2012.
*  Nancy E Epstein. Multidisciplinary in-hospital teams improve patient outcomes: A review. _Surgical neurology international_, 5(Suppl 7):S295, 2014.
*  Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, and Jingren Zhou. Ai hospital: Interactive evaluation and collaboration of llms as intern doctors for clinical diagnosis, 2024.
*  Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation with self-play and in-context learning from ai feedback, 2023.
*  Isaac R Galatzer-Levy, Daniel McDuff, Vivek Natarajan, Alan Karthikesalingam, and Matteo Malgaroli. The capability of large language models to measure psychiatric functioning. _arXiv preprint arXiv:2308.01834_, 2023.
*  Amy L Garcia. Variability in acuity in acute care. _JONA: The Journal of Nursing Administration_, 47(10):476-483, 2017.
*  Nicki Gilboy, Paula Tanabe, Debbie Travers, Alexander M Rosenau, et al. Emergency severity index (esi): A triage tool for emergency department care, version 4. implementation handbook 2012 edition. _AHRQ publication_, 12, 2011.
*  David Grembowski, Judith Schaefer, Karin E Johnson, Henry Fischer, Susan L Moore, Ming Tai-Seale, Richard Ricciardi, James R Fraser, Donald Miller, Lisa LeRoy, et al. A conceptual model of the role of complexity in the care of patients with multiple chronic conditions. _Medical care_, 52:S7-S14, 2014.
*  Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. _ACM Transactions on Computing for Healthcare_, 3(1):1-23, October 2021.
*  Deepak Gupta, Kush Attal, and Dina Demner-Fushman. A dataset for medical instructional video classification and question answering, 2022.
*  Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering, 2020.

*  Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. _arXiv preprint arXiv:2103.03874_, 2021.
*  Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jurgen Schmidhuber. Metagpt: Meta programming for a multi-agent collaborative framework, 2023.
*  Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, and Bryan Hooi. Uncertainty of thoughts: Uncertainty-aware planning enhances information seeking in large language models. _arXiv preprint arXiv:2402.03271_, 2024.
*  M. Jimenez-Lara. Reaping the benefits of integrated health care. stanford social innovation review. _Stanford Social Innovation Review._, 2016.
*  Di Jin, Eileen Pan, Nassim Oufatole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams, 2020.
*  Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2567-2577, 2019.
*  Qiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu, Donald Wright, Thomas Huang, W John Wilbur, Zhe He, Andrew Taylor, Qingyu Chen, et al. Agentmd: Empowering language agents for risk prediction with large-scale clinical tool learning. _arXiv preprint arXiv:2402.13225_, 2024.
*  Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt: augmenting large language models with domain tools for improved access to biomedical information. _Bioinformatics_, 40(2), February 2024.
*  Emre Kiciman, Robert Ness, Amit Sharma, and Chenhao Tan. Causal reasoning and large language models: Opening a new frontier for causality. _arXiv preprint arXiv:2305.00050_, 2023.
*  Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. Health-llm: Large language models for health prediction via wearable sensor data, 2024.
*  Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213, 2022.
*  Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. _arXiv preprint arXiv:2409.12917_, 2024.
*  Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Simlos, Lorie De Leon, Camille Elepano, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, and Victor Tseng. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. _PLOS Digital Health_, 2(2):1-12, 02 2023.
*  Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:453-466, 2019.
*  Danielle L LaFrance, Mary Jane Weiss, Ellie Kazemi, Joanne Gerenser, and Jacqueline Dobres. Multidisciplinary teaming: Enhancing collaboration through increased understanding. _Behavior analysis in practice_, 12(3):709-726, 2019.

*  Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. Large language models for supply chain optimization. _arXiv preprint arXiv:2307.03875_, 2023.
*  Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for "mind" exploration of large language model society, 2023.
*  Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. Agent hospital: A simulacrum of hospital with evolvable medical agents. _arXiv preprint arXiv:2405.02957_, 2024.
*  Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson, Pang Wei Koh, and Yulia Tsvetkov. Mediq: Question-asking llms for adaptive and reliable medical reasoning. _arXiv preprint arXiv:2406.00922_, 2024.
*  Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate, 2023.
*  Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization, 2023.
*  Valentin Lievin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther. Can large language models reason about medical questions?, 2023.
*  Daniel McDuff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, et al. Towards accurate differential diagnosis with large language models. _arXiv preprint arXiv:2312.00164_, 2023.
*  Ateev Mehrotra, Christopher B Forrest, and Caroline Y Lin. Dropping the baton: specialty referrals in the united states. _The Milbank Quarterly_, 89(1):39-68, 2011.
*  Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M. Krumholz, Jure Leskovec, Eric J. Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. _Nature_, 616(7956):259-265, 2023.
*  Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan, Molly Lin, Alex Beutel, John Schulman, and Lilian Weng. Rule based rewards for language model safety. _Advances in Neural Information Processing Systems_, 2024.
*  Mark A Musen, Blackford Middleton, and Robert A Greenes. Clinical decision-support systems. In _Biomedical informatics: computer applications in health care and biomedicine_, pages 795-840. Springer, 2021.
*  Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems, 2023.
*  Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White, and Eric Horvitz. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. November 2023.
*  OpenAI. Gpt-4 technical report, 2024.
*  Anand K Parekh, Richard A Goodman, Catherine Gordon, Howard K Koh, and HHS Interagency Workgroup on Multiple Chronic Conditions. Managing multiple chronic conditions: a strategic framework for improving health outcomes and quality of life. _Public health reports_, 126(4):460-471, 2011.
*  Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, and Kaiqing Zhang. Do llm agents have regret? a case study in online learning and games. _arXiv preprint arXiv:2403.16843_, 2024.

*  Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedaldi, et al. Capabilities of gemini models in medicine. _arXiv preprint arXiv:2404.18416_, 2024.
*  Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl Yang, and May D. Wang. Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records, 2024.
*  Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. _arXiv preprint arXiv:2010.03768_, 2020.
*  Hardeep Singh, Denise M Connor, and Gurpreet Dhaliwal. Five strategies for clinicians to advance diagnostic excellence. _BMJ_, 376, 2022.
*  Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Scharli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman, Blaise Aguera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. _Nature_, 620(7972):172-180, 2023.
*  Harold C Sox, Michael C Higgins, Douglas K Owens, and Gillian Sanders Schmidler. _Medical decision making_. John Wiley & Sons, 2024.
*  J Stairmand, Louise Signal, D Sarfati, C Jackson, L Batten, M Holdaway, and C Cunningham. Consideration of comorbidity in treatment decision making in multidisciplinary cancer team meetings: a systematic review. _Annals of Oncology_, 26(7):1325-1332, 2015.
*  Mirac Suzgun and Adam Tauman Kalai. Meta-prompting: Enhancing language models with task-agnostic scaffolding, 2024.
*  Miren Taberna, Francisco Gil Moncayo, Enric Jane-Salas, Maite Antonio, Lorena Arribas, Esther Vilajosana, Elisabet Peralvez Torres, and Ricard Mesia. The multidisciplinary team (mdt) approach and quality of care. _Frontiers in oncology_, 10:85, 2020.
*  Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning, 2024.
*  Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. Ddxplus: A new dataset for automatic medical diagnosis, 2022.
*  Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. _Nature medicine_, 29(8):1930-1940, 2023.
*  Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. _Nature Medicine_, 29(8):1930-1940, 2023.
*  Daniel Truhn, Jan-Niklas Eckardt, Dyke Ferber, and Jakob Nikolas Kather. Large language models and multimodal foundation models for precision oncology. _NPJ Precision Oncology_, 8(1):72, 2024.
*  Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, Shekoofeh Azizi, Karan Singhal, Yong Cheng, Le Hou, Albert Webson, Kavita Kulkarni, S Sara Mahdavi, Christopher Semturs, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S Corrado, Yossi Matias, Alan Karthikesalingam, and Vivek Natarajan. Towards conversational diagnostic ai, 2024.

*  Sean R Tunis, Daniel B Stryer, and Carolyn M Clancy. Practical clinical trials: increasing the value of clinical research for decision making in clinical and health policy. _Jama_, 290(12):1624-1632, 2003.
*  Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna Seehofnerova, Nidhi Rohatgi, Poonam Hosamani, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason Hom, Sergios Gatidis, John Pauly, and Akshay S. Chaudhari. Adapted large language models can outperform medical experts in clinical text summarization. _Nature Medicine_, 30(4):1134-1142, February 2024.
*  Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2024.
*  Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive computer-aided diagnosis on medical image using large language models, 2023.
*  Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_, 2022.
*  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35:24824-24837, 2022.
*  Thomas Weida and Jane Weida. Outpatient e/m coding simplified. _Family Practice Management_, 29(1):26-31, 2022.
*  Robin M Weinick, Rachel M Burns, and Ateev Mehrotra. Many emergency department visits could be managed at urgent care centers and retail clinics. _Health affairs_, 29(9):1630-1636, 2010.
*  Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryven W White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023.
*  Richard C Wuerz, Leslie W Milne, David R Eitel, Debbie Travers, and Nicki Gilboy. Reliability and validity of a new five-level triage instrument. _Academic emergency medicine_, 7(3):236-242, 2000.
*  Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey, 2023.
*  Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented generation for medicine. _arXiv preprint arXiv:2402.13178_, 2024.
*  Weixiang Yan, Haitian Liu, Tengxiao Wu, Qian Chen, Wen Wang, Haoyuan Chai, Jiayi Wang, Weishan Zhao, Yixin Zhang, Renjun Zhang, et al. Clinicallab: Aligning agents for multi-departmental clinical diagnostics in the real world. _arXiv preprint arXiv:2406.13890_, 2024.
*  Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems, 2024.
*  Cyril Zakka, Akash Chaurasia, Rohan Shad, Alex R. Dalal, Jennifer L. Kim, Michael Moor, Kevin Alexander, Euan Ashley, Jack Boyd, Kathleen Boyd, Karen Hirsch, Curt Langlotz, Joanna Nelson, and William Hiesinger. Almanac: Retrieval-augmented language models for clinical medicine, 2023.

*  Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning with language, 2022.
*  Haodi Zhang, Jiahong Li, Yichi Wang, and Yuanfeng Songi. Integrating automated knowledge extraction with large language models for explainable medical decision-making. In _2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)_, pages 1710-1717. IEEE, 2023.
*  Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering, 2023.
*  Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Chenyu You, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo, and David A. Clifton. A survey of large language models in medicine: Progress, application, and challenge, 2024.
*  Junbin Zhou and Xiao Xu. The difficulty of medical decision-making: should patients be involved? _Hepatobiliary Surgery and Nutrition_, 12(3):407, 2023.

## Limitations and Future Works

Despite the successes of our framework in showing promising performance in medical decision-making tasks, we recognize several limitations that open pathways for future research.

Medical Focused Foundation Models.An essential enhancement would be to incorporate the foundation models and systems specifically trained on medical data, like Med-Gemini , AMIE , and Med-PaLM 2 . These models excel in generating professional medical terminologies, which can facilitate more effective and accurate communication between multiple agents involved in the decision-making process. By leveraging these specialized models, the agents can interact using a shared, precise medical vocabulary, enhancing the system's overall performance and reliability. This approach not only ensures more medically accurate content generation but also supports better collaboration and understanding among the agents, which is essential for complex medical decision-making tasks.

Patient-Centered Diagnosis.A primary limitation lies in the fact that our current framework operates within the confines of multiple-choice question answering and does not account for the interactive, patient-centered nature of real-world diagnostics. Effective diagnosis often relies on continuous exchanges that include the patient's narrative, the physician's expertise, and input from caregivers. To bridge this gap, future iterations of our framework will aim to incorporate a more interactive system that not only assists physicians but also directly engages with both patients and caregivers in a multi-stakeholder approach. Moreover, by incorporating regret-aware  decision-making, the system can learn to minimize diagnostic regret over time, refining its responses based on the outcomes of prior interactions. This regret-aware framework will help guide the LLM to seek additional information when uncertainties arise, thereby supporting more informed decisions across complex, multi-stakeholder scenarios. Embedding these real-world interactions within the feedback loop will enable the system to provide more nuanced and patient-centric support, enhancing the quality and personalization of medical decision-making across all involved parties.

Potential Risks and Mitigations.While our framework shows promise, potential risks include medical hallucinations and the generation of inaccurate or misleading information. To address these risks, integrating self-correction mechanisms, such as those proposed by , could enable the model to autonomously identify and rectify its own errors via reinforcement learning-based self-correction. Additionally, implementing rule-based reward structures, as suggested in , would allow the model to adhere to specific safety and accuracy guidelines during training. These methods can support a safer, more reliable diagnostic support tool by introducing corrective feedback loops and standardized behavior guidelines. Furthermore, integrating confidence scores and uncertainty estimates with the model's recommendations could enhance the decision-making process by enabling end-users to weigh various diagnostic options, ultimately increasing the system's trustworthiness and safety.

## Appendix A Dataset Information

We evaluate multi-agent collaboration frameworks across seven common medical question-answering datasets, which vary in question complexity. Generally, questions are deemed more complex if they involve multiple modalities or entail a lengthy, detailed diagnostic task. Below, we detail each dataset and provide a sample entry:

1. **MedQA.** The MedQA dataset consists of professional medical board exams from the US, Mainland China, and Taiwan . Our study focuses on the English test set, comprising 1,273 questions sourced from the United States Medical Licensing Examination (USMLE). These questions are formatted as multiple-choice text queries with five options. Due to their textual nature and brevity, we categorize these questions as low. Sample Question: _"A 47-year-old female undergoes a thyroidectomy for treatment of Graves' disease. Post-operatively, she reports a hoarse voice and difficulty speaking. You suspect that this is likely a complication of her recent surgery. What is the embryologic origin of the damaged nerve that is most likely causing this patient's hoarseness?"_ Options: _A: 1st pharyngeal arch, B: 2nd pharyngeal arch, C: 3rd pharyngeal arch, D: 4th pharyngeal arch, E: 6th pharyngeal arch_

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_FAIL:20]

across different data modalities provide insights into how quickly and effectively MDAgents can reach a unified decision.

## Appendix C Prompt Templates

### A single agent setting

``` Parsetical multiple choice questions {{instruction}} The following are multiple choice questions (with answers) about medical knowledge. {{few_shot_examples}} {{context}}**Question:** {{question}} {{answer_choices}} **Answer:**( ```

Client of Through multiple choice questions

{{instruction}} The following are multiple choice questions (with answers) about medical knowledge. {{few_shot_examples w/ CoT Solutions}} {{context}}**Question:** {{question}} {{answer_choices}} **Answer:**( ```

**Example Refinement multiple choice questions {{instruction}} The following are multiple choice questions (with answers) about medical knowledge. {{few_shot_examples w/ CoT Solutions}} {{context}}**Question:** {{question}} {{answer_choices}} {{reasoning_paths}} **Answer:**( ```

**Theoreming multiple choice questions {{instruction}} The following are multiple choice questions (with answers) about medical knowledge. {{few_shot_examples w/ CoT Solutions from similarity calculation}} for \(N\) times do {{context}}**Question:** {{question}} {{shuffled_answer_choices}} **Answer:**( ```

[MISSING_PAGE_FAIL:22]

### Accuracy on entire MedQA 5-options Dataset

To provide a comprehensive evaluation of our approach, we conducted experiments on the entire MedQA 5-options dataset using GPT-4o mini. This expands upon the subsampled experiments presented in the main experiments in Table 2. Below Table shows the accuracy results for various methods.

These results demonstrate that our MDAgents approach outperforms both single-agent and other multi-agent methods across the full dataset, achieving an accuracy of 83.6%. This underscores the effectiveness of our framework in handling diverse medical questions at scale.

### Estimated Costs for Full Test Set Experiments

To provide transparency and aid in reproducibility, we estimated the costs associated with running experiments on the entire test sets using GPT-4 (Vision). Below Table presents these cost estimates in USD for various datasets and methods.

While our approach incurs higher costs due to its multi-agent nature, the significant performance improvements justify this increased computational expense for critical medical decision-making tasks.

  
**Method** & **MedQA** & **PubMedQA** & **PubMed-VQA** & **PMC-VQA** & **DDXPlus** & **SymCat** & **JAMA** & **MedBullets** & **MIMIC-CXR** & **Total Cost** \\  CoT & 55.24 & 13.16 & 3,028.54 & 27,134.00 & 16,461.90 & 10,593.99 & 134.55 & 61.23 & 1,388.70 & 58.871.29 \\ 
**Ours** & 172.43 & 41.36 & 9,369.45 & 82,194.34 & 44,814.97 & 31,176.05 & 367.13 & 161.70 & 4,406.90 & 172,704.33 \\   

Table 6: Estimated costs for experimenting with entire test sets with GPT-4 (Vision) (in USD)

  
**Category** & **Method** & **Accuracy (\%)** \\   & Zero-shot & 71.5 \\  & 3-shot & 72.3 \\  & + CoT & 76.6 \\  & + CoT-SC & 77.2 \\   & Moigely twisting & 76.3 \\  & Weighted Venting & 79.1 \\  & Borda Count & 76.1 \\   & Reconcile & 80.2 \\   & **MDAgents (Ours)** & **83.6** \\   

Table 5: Accuracy (%) on entire MedQA 5-options dataset with GPT-4o mini

### Impact of Knowledge Enhancement with RAG

We investigated whether simply assigning roles to agents is sufficient for expert-like performance, and explored the impact of equipping agents with different knowledge using Retrieval-Augmented Generation (RAG). Table 7 presents the results of these experiments.

These results indicate that while role assignment provides a foundation, augmenting agents with specific knowledge (using MedRAG) and structured reviews (Moderator's Review) significantly enhances their ability to simulate domain expertise. The combination of Moderator's Review and MedRAG yielded the best performance, highlighting the synergy between structured collaboration and domain-specific knowledge retrieval.

### Complexity Assignment and Collaborative Settings

To address the impact of complexity assignment on accuracy and API costs, we conducted additional experiments focusing on high-complexity cases, particularly in image+text scenarios. Table 8 shows the results for various collaborative settings.

These results underscore the importance of multi-turn discussions, particularly in complex cases. The parallel collaboration with discussion yielded the highest accuracy (59.0%), suggesting that enabling agents to work simultaneously and engage in dialogue is crucial for handling intricate medical queries. The significant performance gap between discussion and no-discussion scenarios (45.0% vs. 39.0% for sequential, and 59.0% vs. 56.0% for parallel) highlights the value of interactive deliberation in medical decision-making processes.

 
**Method** & **Accuracy (\%)** \\  MDAgents (baseline) & 71.8 \\ + MedRAG & 75.2 \\ + Medical Knowledge Initialization & 76.0 \\ + Moderator’s Review & 77.6 \\ + Moderator’s Review \& MedRAG & **80.3** \\  

Table 7: Impact of knowledge enhancement on MDAgents performance

 
**Collaboration Setting** & **Accuracy (\%)** \\  Sequential \& No Discussion & 39.0 \\ Sequential \& Discussion & 45.0 \\ Parallel \& No Discussion & 56.0 \\ Parallel \& Discussion & **59.0** \\  

Table 8: Impact of collaboration settings on high-complexity image+text tasks

    &  &  \\   & & MedQA & PubMedQA & Path-VQA & PMC-VQA & MedVidQA \\   & Zero-shot & \(48.5 3.3\) & \(56.8 12.0\) & - & - & - \\  & Few-shot & \(47.8 16.4\) & \(59.0 1.0\) & - & - & - \\  & + CoT & \(54.2 8.9\) & \(49.7 11.9\) & - & - & - \\  & + CoT-SC & \(60.5 3.2\) & \(49.4 13.5\) & - & - & - \\  & ER & \(60.2 4.0\) & \(52.9 15.9\) & - & - & - \\  & Medprompt & \(60.1 10.8\) & \(59.8 8.5\) & - & - & - \\   & Zero-shot & \(75.0 1.3\) & \(61.5 2.2\) & \(57.9 1.6\) & \(49.0 3.7\) & - \\  & Few-shot & \(72.9 11.4\) & \(63.1 11.7\) & \(57.5 4.5\) & \(52.2 2.0\) & - \\  & + CoT  & \(82.5 4.9\) & \(57.6 9.2\) & \(58.6 3.1\) & \(51.3 1.5\) & - \\  & + CoT-SC  & \(\) & \(58.7 5.0\) & \(61.2 2.1\) & \(50.5 5.2\) & - \\  & ER  & \(81.9 2.1\) & \(56.0 7.0\) & \(61.4 4.1\) & \(52.7 2.9\) & - \\  & Medprompt  & \(82.4 8.1\) & \(51.8 4.6\) & \(59.2 7.7\) & **S34.0\( 7.0\)** & - \\   & Zero-shot & \(42.0 10.4\) & \(\) & \(45.9 2.8\) & \(44.8 2.0\) & \(37.9 3.4\) \\  & Few-shot & \(34.0 7.2\) & \(55.0 0.0\) & \(64.5 2.3\) & \(48.2 1.0\) & \(47.1 6.6\) \\  & + CoT & \(50.0 6.0\) & \(60.2 9.0\) & \(\) & \(47.1 4.2\) & \(48.6 5.5\) \\  & + CoT-SC & \(52.7 4.6\) & \(55.8 8.9\) & \(63.6 6.0\) & \(46.3 2.8\) & \(\) \\  & ER & \(52.0 7.2\) & \(58.4 14.2\) & \(57.6 8.4\) & \(38.4 2.0\) & \(48.5 4.1\) \\  & Medprompt & \(45.3 3.1\) & \(50.6 5.4\) & \(55.0 2.0\) & \(41.8 3.0\) & \(44.5 2.0\) \\    &  &  \\   & & DDXPlus & SymCat & JAMA & MedBullets & MIMIC-CXR \\  & & & & & & \\   & Zero-shot & \(56.2 4.1\) & \(84.0 0.0\) & \(36.0 3.3\) & \(56.0 2.8\) & - \\  & Few-shot & \(48.9 8.5\) & \(86.0 2.8\) & \(38.0 4.2\) & \(59.0 1.4\) & - \\  & + CoT & \(52.8 5.4\) & \(82.0 0.0\) & \(34.0 2.4\) & \(56.0 5.7\) & - \\  & + CoT-SC & \(37.8 6.1\) & \(80.0 2.8\) & \(43.0 4.2\) & \(63.0 4.2\) & - \\  & ER & \(42.3 6.9\) & \(84.0 1.8\) & \(44.0 3.7\) & \(58.0 0.0\) & - \\  & Medprompt & \(41.2 6.2\) & \(86.0 2.6\) & \(43.0 1.4\) & \(54.0 5.7\) & - \\   & Zero-shot & \(\) & \(88.7 2.3\) & \(62.0 2.0\) & \(67.0 1.4\) & \(40.0 5.3\) \\  & Few-shot & \(69.4 1.0\) & \(86.7 3.1\) & \(69.0 4.2\) & \(72.0 2.8\) & \(35.3 5.0\) \\  & + CoT  & \(72.7 7.7\) & \(78.0 2.0\) & \(66.0 5.7\) & \(70.0 0.0\) & \(36.2 5.2\) \\  & + CoT-SC  & \(52.1 6.4\) & \(83.3 3.1\) & \(68.0 2.8\) & \(\) & \(51.7 4.0\) \\  & ER  & \(61.3 2.4\) & \(82.7 2.3\) & \(\) & \(\) & \(50.0 0.0\) \\  & Medprompt  & \(59.5 10.7\) & \(87.3 1.2\) & \(70.7 4.3\) & \(71.0 1.4\) & \(53.4 4.3\) \\   & Zero-shot & \(49.9 6.5\) & \(88.9 6.4\) & \(42.7 1.9\) & \(40.0 1.5\) & \(40.0 2.8\) \\  & Few-shot & \(47.1 8.6\) & \(89.2 3.4\) & \(41.0 1.4\) & \(44.0 8.9\) & \(39.2 1.2\) \\  & + CoT  & \(65.5 4.9\) & \(91.9 3.4\) & \(38.0 1.6\) & \(52.7 7.1\) & \(45.2 6.8\) \\  & + CoT-SC  & \(60.3 2.4\) & \(92.0 1.8\) & \(46.0 0.0\) & \(51.0 4.2\) & \(\) \\  & ER  & \(46.7 6.9\) & \(58.5 7.5\) & \(50.8 5.8\) & \(53.2 7.8\) & \(53.2 3.5\) \\  & Medprompt  & \(58.2 3.5\) & \(\) & \(44.4 3.2\) & \(54.0 5.7\) & \(51.2 1.9\) \\   

* **CoT**: Chain-of-Thought, **SC**: Self-Consistency, **ER**: Ensemble Refinement
* * **CoT**: _text-only_, **

    \\ 
**Method** & MedQA & PubMedQA & Path-VQA & PMC-VQA & MedVQA \\   & Majority Voting & \(60.4 2.1\) & \(68.5 0.6\) & - & - & - \\  & Weighted Voting & \(57.3 3.0\) & \(65.8 11.4\) & - & - & - \\  & Broads Court & \(55.3 7.1\) & \(70.2 8.5\) & - & - & - \\  & MedAgents  & \(56.0 5.3\) & \(55.0 1.4\) & - & - & - \\   & Majority Voting & \(80.6 2.9\) & \(72.2 6.5\) & \(56.9 19.7\) & \(36.8 0.7\) & - \\  & Weighted Voting & \(78.8 1.1\) & \(72.2 6.5\) & \(62.1 13.9\) & \(25.4 0.60\) & - \\  & Broads Court & \(70.3 8.5\) & \(66.9 1.0\) & \(61.9 8.1\) & \(27.9 5.3\) & - \\  & MedAgents  & \(79.1 7.4\) & \(69.7 4.7\) & \(45.4 8.1\) & \(39.6 8.0\) & - \\   & Majority Voting & \(51.6 2.2\) & \(65.3 12.9\) & \(58.2 0.5\) & \(27.1 5.4\) & \(50.8 7.4\) \\  & Weighted Voting & \(52.3 3.3\) & \(63.7 10.0\) & \(66.4 11.1\) & \(20.9 8.5\) & \(57.8 2.1\) \\  & Bords Court & \(49.4 9.7\) & \(57.7 15.0\) & \(68.2 1.8\) & \(25.3 8.7\) & \(54.5 4.7\) \\  & MedAgents  & \(48.4 5.5\) & \(63.6 6.0\) & \(64.9 12.5\) & \(35.1 8.1\) & \(61.6 4.8\) \\   & Reconcile  & \(\) & \(\) & \(57.5 3.3\) & \(31.4 1.2\) & - \\  & AutoGen  & \(60.6 5.0\) & \(77.3 2.3\) & \(43.0 3.9\) & \(37.3 1.1\) & - \\  & DeLAN  & \(64.2 2.3\) & \(73.6 4.2\) & \(41.3 1.2\) & \(34.0 3.5\) & - \\  & Meta-Prompting  & \(80.6 1.2\) & \(73.3 2.3\) & \(55.3 2.3\) & \(42.6 4.2\) & - \\   \\   &  & DDXplus & SymCalc & IAMA & MedBultes & MIMIC-CXR \\  & & & & & & \\   & Majority Voting & \(53.6 2.2\) & \(83.7 3.5\) & \(47.0 1.4\) & \(46.0 4.1\) & - \\  & Weighted Voting & \(55.2 2.0\) & \(85.9 3.0\) & \(49.0 1.4\) & \(43.0 1.4\) & - \\  & Beta Court & \(63.9 1.1\) & \(84.9 1.6\) & \(50.0 1.0\) & \(45.0 1.6\) & - \\  & MedAgents (72] & \(47.3 11.0\) & \(87.0 4.2\) & \(41.0 3.3\) & \(56.0 4.2\) & - \\   & Majority Voting & \(67.8 4.9\) & \(\) & \(\) & \(70.0 0.0\) & \(49.5 0.07\) \\  & Weighted Voting & \(65.9 3.1\) & \(90.5 2.9\) & \(66.1 4.1\) & \(66.0 1.7\) & \(53.5 2.2\) \\  & Bords Court & \(67.1 6.7\) & \(78.0 11.8\) & \(61.0 5.6\) & \(66.0 2.8\) & \(45.3 8.8\) \\  & MedAgents  & \(62.8 5.6\) & \(90.0 0.0\) & \(66.0 5.7\) & \(\) & \(43.3 7.0\) \\   & Majority Voting & \(52.3 1.3\) & \(73.5 4.0\) & \(47.0 1.4\) & \(44.0 4.2\) & \(47.9 6.6\) \\  & Weighted Voting & \(54.3 10.9\) & \(64.6 6.5\) & \(42.0 2.3\) & \(43.6 1.9\) & \(43.2 2.0\) \\  & Bords Court & \(67.0 2.7\) & \(77.3 0.3\) & \(37.0 1.8\) & \(46.1 3.2\) & \(44.6 4.5\) \\  & MedAgents  & \(43.0 3.7\) & \(80.5 1.9\) & \(40.8 1.9\) & \(50.5 5.6\) & \(38.7 1.5\) \\   & Reconcile  & \(\) & \(79.0 2.5\) & \(60.7 5.7\) & \(59.5 8.7\) & \(33.3 3.4\) \\  & AutoGen  & \(67.3 11.8\) & \(79.3 3.1\) & \(64.6 1.2\) & \(55.3 3.1\) & \(43.3 4.2\) \\  & DeLAN  & \(56.4 2.9\) & \(75.3 4.6\) & \(60.1 3.1\) & \(57.3 4.1\) & \(38.7 1.2\) \\  & Meta-Prompting  & \(52.6 4.1\) & \(77.3 2.3\) & \(64.7 1.1\) & \(49.3 1.2\) & \(42.0 4.0\) \\   

* **CoT:** Chain-of-Though, **SC: Self-Consistency, **ER:** Ensemble Refinement
* **[leftmargin=*]**_text-only_ & **[leftmargin=*]**_text-only_ & **[leftmargin=*]**_text-only_ & **[leftmargin=*]**_text-only_ & **[leftmargin=*]_text-only_ & **[leftmargin=*]_text-_-_ & **[leftmargin=*]_-_ **vidroset_** \\   & Reconcile  & \(\) & \(79.0 2.5\) & \(60.7 7.5\) & \(59.5 8.7\) & \(33.3 3.4\) \\  & AutoGen  & \(67.3 11.8\) & \(79.3 3.1\) & \(64.6 1.2\) & \(55.3 3.1\) & \(43.3 4.2\) \\  & DeLAN  & \(56.4 2.9\) & \(75.3 4.6\) & \(60.1 3.1\) & \(57.3 4.1\) & \(38.7 1.2\) \\  & Meta-Prompting  &

  
**Category** & **Method** & MedQA & PubMedQA & Path-YQA & PMC-VQA & MedVidQA \\  & & & & & & \\   & Zero-shot & 75.0 & 54.0 & 58.0 & 48.0 & 50.0 \\  & Few-shot & 77.0 & 55.0 & 58.0 & 50.0 & 51.0 \\  & + CoT & 78.0 & 50.0 & 59.0 & 52.0 & 53.0 \\  & + CoT-SC & 79.0 & 51.0 & 60.0 & 53.0 & 53.0 \\  & ER & 76.0 & 51.0 & 61.0 & 51.0 & 52.0 \\  & Medprompt & 79.0 & 58.0 & 60.0 & 54.0 & 53.0 \\   Multi-agent \\ (Single-model) \\  } & Majority Voting & 79.0 & 68.0 & 63.0 & 52.0 & 54.0 \\  & Weighted Voting & 80.0 & 68.0 & **64.0** & 51.0 & 52.0 \\  & Borda Count & 81.0 & 69.0 & 62.0 & 50.0 & 52.0 \\  & MedAgents & 80.0 & 69.0 & 55.0 & 52.0 & 50.0 \\  & Meta-Prompting & 82.0 & 69.0 & 56.0 & 49.0 & - \\   & Reconcile & 85.0 & 72.0 & 58.0 & 45.0 & - \\  & AutoGen & 65.0 & 63.0 & 45.0 & 40.0 & - \\  & DyLAN & 68.0 & 67.0 & 42.0 & 48.0 & - \\   & **MDA Agents (Ours)** & **87.0** & **71.0** & 60.0 & **55.0** & **56.0** \\   & **Method** & DDXPlus & SynCat & JAMA & MedBullets & MIMC-CXR \\  & & & & & & \\   & Zero-shot & 53.0 & 84.0 & 57.0 & 49.0 & 38.0 \\  & Few-shot & 60.0 & 87.0 & 58.0 & 52.0 & 33.0 \\  & + CoT & 66.0 & 84.0 & 55.0 & 64.0 & 33.0 \\  & + CoT-SC & 68.0 & 84.0 & 57.0 & 60.0 & 40.0 \\  & ER & **76.0** & 80.0 & 56.0 & 59.0 & 43.0 \\  & Medprompt & 70.0 & 84.0 & **62.0** & 60.0 & 43.0 \\   Multi-agent \\ (Single-model) \\  } & Majority Voting & 53.0 & 82.0 & 56.0 & 59.0 & 54.0 \\  & Weighted Voting & 52.0 & 86.0 & 56.0 & 56.0 & 52.0 \\  & Borda Count & 53.0 & 86.0 & 56.0 & 59.0 & 51.0 \\  & MedAgents & 56.0 & 80.9 & 51.0 & 58.0 & 40.9 \\  & Meta-Prompting & 53.0 & 79.0 & 56.0 & 51.0 & 48.0 \\  & Reconcile & 60.0 & 87.0 & 59.0 & 60.0 & 43.3 \\  & AutoGen & 47.0 & 87.0 & 53.0 & 55.0 & 47.0 \\  & DyLAN & 54.0 & 84.0 & 55.0 & 57.0 & 42.0 \\   & **MDA Agents (Ours)** & 75.0 & **89.0** & 59.0 & **67.0** & **56.0** \\   

* **CoT:** Chain-of-Though, **SC: Self-Consistency, **ER**: Ensemble Refinement
* _text-only, **1**: image=text, **2**: video=text

Table 12: Accuracy (%) on Medical benchmarks with **Solo/Group/Adaptive** settings with increased number of samples (N=100). All benchmarks except for MedVidQA (Gemini 1.5 Flash) were evaluated with GPT-40 mini.

Figure 8: Complexity Distribution for each dataset classified by GPT-4(V) and Gemini-Pro (Vision) (for MedVidQA). The plot illustrates the varying levels of medical complexity across datasets, reflecting the diverse nature of medical question answering, diagnostic reasoning, and medical visual interpretation tasks. For instance, MedQA is categorized under Medical Knowledge Retrieval due to their focus on text-based questions and literature synthesis, while MIMIC-CXR, categorized under Clinical Reasoning and Diagnostic tasks, shows a high complexity distribution due to the need for interpreting detailed radiographic images (See Section in Section 4.1 for the task categorization)

Figure 10: Simplified agent structure examples assigned during the expert recruitment process ranging from (a) A Primary Care Clinician (PCC), (b) Multi-disciplinary Team (MDT), (C) MDT w/ hierarchy to (d) Integrated Care Team (ICT).

Figure 9: Top-3 most recruited medical experts in each benchmark. The alignment between the dataset characteristics and the recruited experts is evident in several cases. For instance, MIMIC-CXR, which features chest x-ray images, predominantly recruits Radiologists, Pulmonologists, and experts in Biomedical Informatics due to their expertise in interpreting medical imaging.

```
1:Problem \(Q\)
2:\(Complexity(Q)\)\(\) Determine the complexity of the medical query
3:if\(Complexity=low\)then
4:\(Agent(Q,Complexity)\)\(\) Recruit a Primary Care Clinician agent
5:\(ans Agent(Q)\)
6:elseif\(Complexity=moderate\)then
7:\(MDT(Q,Complexity)\)\(\) Recruit a Multi-disciplinary Team
8:\(Agent(Q,Complexity,MDT)\)
9:\(r 0\)
10:\(Consensus\) False
11:\(Interaction[]\)
12:while\(r R\) and not \(Consensus\)do
13:\(Consensus,Log(Q,MDT)\)\(\) Iterative discussions
14:if not \(Consensus\)then
15:for all\(Agent MDT\)do
16:\(Feedback Moderator(Interaction,Agent)\)\(\) Review and provide feedback
17:\(Agent.(Feedback)\)\(\) Update the feedback
18:endfor
19:\(Interaction Interaction+[Log]+[Feedback]\)
20:endif
21:\(r r+1\)
22:endwhile
23:\(ans Agent(Q,Interaction)\)\(\) Moderator agent makes the final decision
24:else
25:\(ICT(Q,Complexity)\)\(\) Recruit an Integrated Care Team
26:\(Reports[]\)
27:for\(Team ICT\)do
28:\(Report(Q,Team)\)\(\) Each Team curates a report
29:\(Reports Reports+[Report]\)
30:endfor
31:\(ans Agent(Q,Reports)\)\(\) Final decision made
32:endif
33:return\(ans\) ```

**Algorithm 1** Adaptive Medical Decision-making FrameworkCase Study

### Medical Decision Making Case Studies

MDM requires efforts of both individual expertise and collaboration to navigate the complexities of patient care. Clinicians often face challenging scenarios that necessitate a comprehensive approach, integrating insights from various specialties to arrive at the best possible outcomes.

#### e.1.1 Real-World Medical Cases

Below are the real-world example cases that could be classified as low, moderate, to high complexity cases.

Case 1: Adjusting Medication Dosage for Chronic Disease (Low Complexity)A 55-year-old female patient with type 2 diabetes visits her PCP for a routine check-up. The patient has been taking 500 mg of metformin orally twice a day and has been adhering to a low-carbohydrate diet. Upon testing with fasting glucose level, the glucose level is above normal. PCP reviewed the current medical dosage and increased the dosage to manage the blood glucose level of the patient.

Case 2: Differential Diagnosis in the Emergency Department (Moderate Complexity)A 40-year-old male patient arrives at the emergency department (ED) with a high fever, severe headache, and muscle pain, raising concerns about a potential infectious disease. The ED physician conducts an initial examination but recognizes the need for a more detailed evaluation to identify the underlying cause. The patient is referred to the infectious disease department for further assessment. An infectious disease specialist, along with the ED physician, reviews the patient's symptoms, travel history, and recent exposures. They collaborate on ordering specific diagnostic tests, including blood cultures and imaging studies. Through this teamed decision-making process, they diagnose the patient with dengue fever and promptly initiate appropriate antiviral treatment.

Case 3: Managing Adverse Responses to Medication in Chronic Disease (High Complexity)A 60-year-old female patient with chronic heart failure has been experiencing new symptoms of shortness of breath and mild fever, suggesting either a complication due to her chronic heart failure or a new infection. The urgent care doctor identifies the severity of the situation and promptly refers the patient to the emergency department of a large hospital, where the patient has triaged to see a cardiologist and an infectious disease doctor for specialized care. The team conducts a detailed review of the patient's medication history and current symptoms, does a physical exam to listen to lung sounds, and orders a few exams including labs, a chest x-ray, echocardiogram, and electrocardiogram. The team identifies that the patient has pulmonary effusion and upper respiratory viral infection.

#### e.1.2 Medical Cases from MedQA Dataset

Now, let us look at the cases from the MedQA  dataset that illustrate either individual PCP or teamed decision-making is crucial in managing medical conditions, ranging from low to high complexity levels of potential cases. These examples highlight the importance of checking the complexity of the case for proper management.

Case 4: Diagnosis by PCPThe case below with the "Low Complexity" header is classified as low complexity by a medical doctor. In this case, a PCP can answer this question without consulting a gastroenterologist. The diagnosis of gastric cancer and management based on the manifestation of the disease, that has been described in this question and beyond should be from a gastroenterologist. However, PCPs are expected to have the basic scientific and pathophysiological knowledge that is related to gastric cancer and use that knowledge to solve this problem.

Case 5: Diagnosis and management by single Pediatric EndocrinologistThe case below with the "Moderate Complexity" header is classified as moderate complexity by the medical doctor. In this case, a pediatric endocrinologist (specialist) alone can diagnose a patient and have a treatment plan. Note that this patient could have been referred to this pediatric endocrinologist by a PCP who is regularly seeing this patient.

[MISSING_PAGE_FAIL:32]

case studies reveals how our framework provides an environment for agents to collaborate, gather information, moderate and make final decisions in complex medical scenarios.

Figure 11: Illustration of our proposed framework in _moderate_ complexity setting. Given a medical query (_image + text_) the framework performs reasoning in five steps: (i) complexity check, (ii) expert recruitment, (iii) initial assessment, (iv) collaborative discussion, and (v) review and final decision-making. Green text represents the correct answer and the Red text represents the incorrect answer.

## Appendix F Medical Complexity Comparison with Human Physicians

The core premise of our MDAgent framework is its ability to adapt to the complexity of medical tasks. To validate this approach and gain insights into how LLMs perceive medical complexity compared to human experts, we conducted an annotation study. This study aimed to explore the alignment between LLMs and physicians in assessing medical question complexity, a critical factor in the effectiveness of our MDAgent framework.

Study DesignWe selected 50 representative questions from the MedQA dataset, ensuring a balanced representation across USMLE steps 1, 2, and 3. This selection process aimed to cover a wide range of medical topics and complexity levels, mirroring the diverse challenges that our MDAgent framework is designed to address.

Three physicians participated in our study: two with two years of Internal Medicine training (Post Graduate Year 2, PGY-2) and one general physician. This composition allowed us to capture a range

Figure 12: Illustration of our proposed framework in _high_ complexity setting. Given a medical query (_text_-only) the framework performs reasoning in four steps: (i) complexity check, (ii) expert recruitment, (iii) information gather and report generation, (iv) final decision. Green text represents the correct answer.

of clinical perspectives. The physicians rated each question on a scale of -1 (low complexity), 0 (moderate complexity), and 1 (high complexity).

Inter-rater ReliabilityTo quantify the agreement among our physician raters, we employed Intraclass Correlation Coefficients (ICC). ICC is a widely used statistical measure in medical research for assessing the consistency of ratings among multiple raters. We specifically chose two ICC variants:

* ICC2k (Two-way random effects, average measures): 0.269 [-0.14, 0.55]
* ICC3k (Two-way mixed effects, average measures): 0.280 [-0.15, 0.57]

ICC2k was selected because it assumes our raters are randomly selected from a larger population of similar raters, allowing for generalization of our findings. ICC3k, on the other hand, treats the raters as fixed, focusing on the consistency among our specific set of physicians.

Both ICC values indicate moderate agreement among the raters. This level of agreement reflects the inherent complexity and subjectivity in evaluating medical questions, even among trained professionals. It also highlights the challenging nature of the task our MDAgent framework aims to address.

Annotation InterfaceTo facilitate the annotation process for both physicians and LLMs, we developed a specialized interface. This interface was designed to present medical questions in a clear and consistent manner, allowing for efficient and standardized complexity ratings. Figure 13 shows a screenshot of the annotation interface used in our study.

LLM Annotations and ComparisonTo compare LLM performance with human expert judgments, we employed several state-of-the-art models to annotate the same set of questions. We then compared these assessments with the majority opinion of the physicians, determined by the mode of their ratings (or the mean in cases of complete disagreement).

Table 13 presents the Pearson correlation between each LLM's complexity ratings and the physicians' majority opinions:

Figure 13: Annotation interface used for medical complexity assessment. The latest version can be found at https://dxagents.github.io/2024/05/01/medqa.html.

The results of our study provide valuable insights into the current state of LLM capabilities in medical complexity assessment and underscore the importance of our MDAgent framework:

1. **Subjectivity in medical complexity**: The moderate ICC values among physicians highlight the inherent subjectivity in assessing medical question complexity. This finding validates our approach in MDAgent, which doesn't rely on a single, fixed assessment of complexity but rather adapts its collaboration structure dynamically.
2. **Current LLM limitations**: The low correlations between LLM and human assessments indicate that current LLMs may not fully capture the nuances that human experts consider when evaluating medical complexity. This observation reinforces the need for our MDAgent framework, which can compensate for individual LLM limitations through collaborative decision-making.
3. **Potential for improvement**: The variation in correlation across different LLM models (from -0.090 to 0.110) suggests there is room for improvement in LLM performance. This aligns with our MDAgent approach, which can leverage the strengths of multiple models and adapt to future improvements in LLM capabilities.
4. **Value of human expertise**: The discrepancy between LLM and physician assessments underscores the continued importance of human medical expertise. Our MDAgent framework acknowledges this by incorporating human-like collaboration strategies and the potential for human oversight in critical decisions.
5. **Adaptability of MDAgent**: The challenges revealed in this study highlight the wisdom of our MDAgent's adaptive approach. By dynamically adjusting its collaboration structure based on perceived task complexity, MDAgent can mitigate the limitations of individual LLMs and approach the nuanced understanding demonstrated by human experts.

 
**Model** & **Correlation with Physician Majority** \\  gpt-4o-mini & -0.090 \\ gpt-4o & 0.022 \\ gpt-4 & 0.070 \\ gemini-1.5-flash & 0.110 \\  

Table 13: Correlation between LLM complexity ratings and physician majority opinions

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction state the primary contributions of the paper, including the introduction of the MDAgents framework, its adaptive decision-making structure, and the significant performance improvements demonstrated through experimental results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper includes a section on limitations, discussing the scope of the framework, and areas for future improvement. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not include theoretical results that require assumptions or proofs. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper provides comprehensive details on datasets, experimental setups, and methodologies used, ensuring that the results can be reproduced accurately. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper includes a link to the code repository and provides sufficient instructions in the supplemental material to reproduce the experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper specifies all relevant experimental details, including data splits, number of samples, and number of seeds, ensuring transparency and reproducibility of the results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper reports error bars and includes information on the statistical significance of the experimental results, ensuring a clear understanding of the variability and reliability of the findings. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The experiments primarily involved inference using API calls to GPT-3.5, GPT-4 (V), and Gemini-Pro (Vision). The type of compute workers, memory, and time of execution are managed by the API providers (OpenAI and Gemini). Details about the number of API calls and the specific configurations used for each experiment are provided, ensuring reproducibility. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research adheres to the NeurIPS Code of Ethics, ensuring responsible conduct throughout the study. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [Yes] Justification: The paper discusses potential societal impacts, both positive and negative, including the improvement of medical decision-making and the risks associated with misuse or biases in the AI system. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: The paper discusses potential limitations, including the risks of medical hallucinations and false knowledge. It includes guidelines within the main text to ensure responsible use of the MDAgents framework and publicly available datasets, emphasizing ethical usage and adherence to best practices. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper properly credits the creators of existing assets used and clearly states the licenses and terms of use.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not introduce any new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.