# Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions

Khai Nguyen

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

khainb@utexas.edu

&Nhat Ho

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

minhnhat@utexas.edu

###### Abstract

Sliced Wasserstein (SW) and Generalized Sliced Wasserstein (GSW) have been widely used in applications due to their computational and statistical scalability. However, the SW and the GSW are only defined between distributions supported on a homogeneous domain. This limitation prevents their usage in applications with heterogeneous joint distributions with marginal distributions supported on multiple different domains. Using SW and GSW directly on the joint domains cannot make a meaningful comparison since their homogeneous slicing operator, i.e., Radon Transform (RT) and Generalized Radon Transform (GRT) are not expressive enough to capture the structure of the joint supports set. To address the issue, we propose two new slicing operators, i.e., Partial Generalized Radon Transform (PGRT) and Hierarchical Hybrid Radon Transform (HHRT). In greater detail, PGRT is the generalization of Partial Radon Transform (PRT), which transforms a subset of function arguments non-linearly while HHRT is the composition of PRT and multiple domain-specific PGRT on marginal domain arguments. By using HHRT, we extend the SW into Hierarchical Hybrid Sliced Wasserstein (H2SW) distance which is designed specifically for comparing heterogeneous joint distributions. We then discuss the topological, statistical, and computational properties of H2SW. Finally, we demonstrate the favorable performance of H2SW in 3D mesh deformation, deep 3D mesh autoencoders, and datasets comparison1.

## 1 Introduction

Optimal transport [55; 47] is a powerful mathematical tool for machine learning, statistics, and data sciences. As an example, Wasserstein distance , defined as the optimal transportation cost between two distributions, has been used successfully in many areas of machine learning and statistics, such as generative modeling on images [2; 53], representation learning , vocabulary learning , and so on. Despite being accepted as an effective distance, Wasserstein distance has been widely known as a computationally expensive distance. In particular, when comparing two distributions that have at most \(n\) supports, the time complexity and the memory complexity of the Wasserstein distance scale with the order of \((n^{3} n)\) and \((n^{2})\) respectively. In addition, the Wasserstein distance requires more samples to approximate a continuous distribution with its empirical distribution in high dimension since its sample complexity is of the order of \((n^{-1/d})\), where \(n\) is the sample size and \(d\) is the number of dimensions. Therefore, Wasserstein distance is not statistically and computationally scalable, especially in high dimensions.

Along with entropic regularization  which can reduce the time complexity and memory complexity of computing optimal transport to \((n^{2})\) and \((n^{2})\) in turn, sliced Wasserstein (SW) distance is one alternative approach for the original Wasserstein distance. The key benefit of the SW distance is that it scales the order \((n n)\) and \((n)\) in terms of time and memory respectively. The reason behind that fast computation is the closed-form solution of optimal transport in one dimension. To leverage that closed-form, sliced Wasserstein utilizes Radon Transform  (RT) to transform a high-dimensional distribution to its one-dimensional projected distributions, then the final distance is calculated as the average of all one-dimensional Wasserstein distance. By doing that, the SW distance has a very fast sample complexity i.e., \((n^{-1/2})\), which makes it computationally and statistically scalable in any dimension. Therefore, the SW distance has been applied successfully in various domains of applications including generative models , domain adaptation , clustering , 3D shapes , gradient flows , Bayesian inference computation , texture synthesis , and many other tasks.

Despite being useful, the SW distance is not as flexible as the Wasserstein distance in terms of choosing the ground metric. In greater detail, the number of ground metrics in one dimension is limited, especially ground metrics that lead to the closed-form solution. As a result, the role of capturing the structure of distributions belongs to the slicing/projecting operators. To generalize RT to non-linear projection, generalized Radon Transform (GRT) is introduced in  with circular projection , polynomial projection , and so on. With GRT, Generalized Sliced Wasserstein (GSW) distance is proposed in . In addition, there is a line of works on developing sliced Wasserstein variants on different manifolds such as hyper-sphere , hyperbolic manifolds , the manifold of symmetric positive definite matrices , general manifolds and graphs . In those works, special variants of GRT are proposed.

Although the SW has become more effective on multiple domains, no SW variant is designed specifically for heterogeneous joint distributions i.e., joint distributions that have marginals supported on different domains, except for the product of Hadamard manifolds . It is worth noting that marginal domains of heterogeneous joint distributions can be any metric space and are not necessary manifolds. Heterogeneous joint distributions appear in many applications, e.g., domain adaptation domains , comparing datasets with labels , 3D shape deformation , and so on. In this case, Wasserstein distance can be adapted by using a mixed ground metric, i.e., a weighted sum of metrics on domains . In contrast to the Wasserstein distance, the adaptation of SW has not been well-investigated. Using GSW directly with one type defining function for all marginals cannot separate the information within and among groups of arguments.

**Contribution:** In this work, we tackle the challenge of designing a sliced Wasserstein variant for heterogeneous joint distributions. In summary, our main contributions are three-fold:

1. We first extend the partial Radon Transform to the partial generalized Radon Transform (PGRT) to inject non-linearity into local transformation. We discuss the injectivity of PGRT for some choices of defining functions. We then propose a novel slicing operator for heterogeneous joint distributions, named Hierarchical Hybrid Radon Transform (HHRT). In particular, HHRT is a hierarchical transformation that first applies partial generalized Radon Transform with different defining functions on arguments of each marginal to gather marginal information, then applies partial Radon Transform on the joint transformed arguments to gather information among marginals. We show that HHRT is injective as long as the partial generalized Radon Transform is injective.
2. We propose Hierarchical Hybrid Sliced Wasserstein (H2SW) which is a novel metric for comparing heterogeneous joint distributions by utilizing the HHRT. Moreover, we investigate the topological properties, statistical properties, and computational properties of H2SW. In particular, we show that H2SW is a valid metric on the space of distribution over the joint space, H2SW does not suffer from the curse of dimensionality and enjoys the same computational scalability as SW distance.
3. A 3D mesh can be effectively represented by a point-cloud and corresponding surface normal vectors. Therefore, it can be seen as an empirical heterogeneous joint distribution. We conduct experiments on optimization-based 3D mesh deformation and deep 3D mesh autoencoder to show the favorable performance of H2SW compared to SW and GSW. Moreover, we also illustrate that H2SW can also provide a meaningful comparison for probability distributions on the product of Hadamard manifolds by conducting experiments on dataset comparison.

**Organization.** We first provide some preliminaries on SW distance, GSW distance, and joint Wasserstein distance in Section 2. We then define the hierarchical hybrid Radon transform and hierarchical hybrid sliced Wasserstein distance s in Section 3. Section 4 contains experiments on 3D mesh deformation, deep 3D mesh autoencoder, and datasets comparison. We conclude the paper in Section 5. Finally, we defer the proofs of key results, and additional materials in the Appendices.

## 2 Preliminaries

**Wasserstein distance.** For \(p 1\), the Wasserstein-\(p\) distance [55; 47] between two distributions \(()\) and \(()\), where \(\) and \(\) are subsets of \(^{d}\) and they share a ground metric \(c:^{+}\), is defined as:

\[^{p}_{p}(,;c):=_{(,)}_{ }c(x,y)^{p}d(x,y),\] (1)

where \((,):=\{())|_{ }d(x,y)=(x),_{}d(x,y)=(y)\}\). When \(\) and \(\) are discrete with at most \(n\) supports, the time complexity and the space complexity of the Wasserstein distance is \((n^{3} n)\) and \((n^{2})\) in turn which are very expensive. Therefore, sliced Wasserstein is proposed as an alternative solution. We first review the definition of Radon Transform.

**Radon Transform ** The _Radon Transform_\(:_{1}(^{d})_{1}( ^{d-1})\) is defined as:

\[(f)(t,)=_{^{d}}f(x)(t- x,)dx.\] (2)

Radon Transform defines a linear bijection . Given a projecting direction \(\), \((f)(,)\) is an one-dimensional function. With Radon Transform, we can now define the sliced Wasserstein distance.

**Sliced Wasserstein distance.** For \(p 1\), the _Sliced Wasserstein (SW)_ distance  of \(p\)-th order between two distributions \(()\) and \(()\) with an one-dimensional ground metric \(c:^{+}\) is defined as follow:

\[^{p}_{p}(,;c)=_{( ^{d-1})}[^{p}_{p}(_{}, _{};c)],\] (3)

where \(_{}\) and \(_{}\) are the one-dimensional push-forward distributions created by applying Radon Transform (RT)  on the pdf of \(\) and \(\) with the projecting direction \(\). The computational benefit of SW distance comes from the closed-form solution when the one-dimensional ground metric \(c(x,y)=h(x-y)\) for \(h\) is a strictly convex function:

\[^{p}_{p}(_{},_{ };c)=_{0}^{1}c(F^{-1}_{_{}} (z),F^{-1}_{_{}}(z))^{p}dz,\]

where \(F^{-1}_{_{}}\) and \(F^{-1}_{_{}}\) are inverse CDF of \(_{}\) and \(_{}\) respectively. When \(\) and \(\) are discrete with at most \(n\) supports, the time complexity and the space complexity of the closed-form is \((n n)\) and \((n)\) respectively.

**Generalized Radon Transform and Generalized Sliced Wasserstein distance.** To generalize RT to non-linear operator, the _Generalized Radon Transform (GRT)_ was proposed . Given a defining function \(g:^{d}\), the Generalized Radon Transform \(:_{1}(^{d})_{1 }()\) is defined as:

\[(f)(t,)=_{^{d}}f(x) (t-g(x,))dx.\]

For example, we can have the circular function , i.e., \(g(x,)=\|x-r\|_{2}\) for \(r^{+}\) and \(:=^{d-1}\), homogeneous polynomials with an odd degree  (\(m\)), i.e., \(g(x,)=_{||=m}_{}x^{}\) with \(=(_{1},,_{d_{}})^{d_{}}\), \(||=_{i=1}^{d_{}}_{i}\), \(x^{}=_{i=1}^{d_{}}x^{_{i}}\), \(=^{d_{}}\), and so on. Using GRT, the _Generalized Sliced Wasserstein (GSW)_ distance is introduced in , which is formally defined as follow :

\[^{p}_{p}(,;c,g)=_{( ^{d-1})}[^{p}_{p}(^{g}_{} ,^{g}_{};c)].\] (4)

It is worth noting that the injectivity of GRT is required to have the identity of indiscernible GSW.

**Heterogeneous joint distributions comparison.** We are given two joint distributions \((x_{1},x_{2})(_{1}_{2})\) and \((y_{1},y_{2})(_{1}_{2})\) where \(X_{1}\) are \(Y_{1}\) share a ground metric \(c_{1}:_{1}_{1}^{+}\) and \(X_{2}\) are \(Y_{2}\) share a ground metric \(c_{2}:_{2}_{2}^{+}\) with (\(c_{1} c_{2}\)). In this case, previous works utilize the joint distribution Wasserstein distance  to compare \(\) and \(\):

\[_{p}^{p}(,;c_{1},c_{2}):=_{(,)}_{_{1}_{2}_{1}_{2}}(c_{1}(x_{1}, y_{1})^{p}+c_{2}(x_{2},y_{2})^{p})d(x_{1},x_{2},y_{1},y_{2}),\] (5)

where \((,):=\{(_{1}_{2} _{1}_{2})\}|_{_{1} _{2}}d(x_{1},x_{2},y_{1},y_{2})=(x_{1},x_{2}),\)

\(_{_{1}_{2}}d(x_{1},x_{2},y_{1},y_{2})=( y_{1},y_{2})}\). We can easily extend the definition to joint distributions with more than two marginals (see Appendix B). In contrast to the Wasserstein distance, there is no variant of SW that is designed specifically for this case. SW variants can still be used by treating \(_{1}_{2}\) and \(_{1}_{2}\) as homogeneous spaces \(\) and \(\) which share the same Radon Transform variant and one-dimensional ground metric \(c\). However, that approach cannot differentiate the difference between \(_{1}\) and \(_{2}\), and leverage the hierarchical structure, i.e., inside and among marginals.

## 3 Hierarchical Hybrid Sliced Wasserstein Distance

In this section, we propose the Hierarchical Hybrid Radon Transform (HHRT) which first applies P(G)RT on each marginal argument to gather each marginal information, then applies PRT on the joint transformed arguments from all marginals to gather information among marginals. After that, we introduce Hierarchical Hybrid Sliced Wasserstein distance by using HHRT as the slicing operator.

### Hierarchical Hybrid Radon Transform

We first introduce the first building block in HHRT, i.e., Partial Generalized Radon Transform (PGRT).

**Definition 1** (Partial Generalized Radon Transform).: _Given a defining function \(g:^{d_{1}}\), Partial Generalized Radon Transform \(:_{1}(^{d_{1}}^{d_{2}}) _{1}(^{d_{2}})\) is defined as:_

\[(f)(t,,y)=_{^{d_{1}}}f(x,y)(t-g(x, ))dx.\] (6)

When \(g(x,)= x,\), PGRT reverts into Partial Radon Transform (PRT) .

**Proposition 1**.: _For some defining function \(g\) such as linear, circular, and homogeneous polynomials with an odd degree; the Partial Generalized Radon Transform is injective, i.e., for any functions \(f_{1},f_{2}^{1}(^{d})\), \((f_{1})(t,,y)=(f_{2})(t,,y)\  t,,y\) implies \(f_{1}=f_{2}\)._

The proof of Proposition 1 is given in Appendix A.1. The main idea to prove the injectivity of PGRT is to show that given a fixed \(y\), the PGRT is the GRT of \(f(,y)\).

**Definition 2** (Hierarchical Hybrid Radon Transform).: _Given defining functions \(g_{1}:^{d_{1}}_{1}\) and \(g_{2}:^{d_{2}}_{2}\), Hierarchical Hybrid Radon Transform \(:_{1}(^{d_{1}}^{d_{2}}) _{1}(_{1}_{2} )\) is defined as:_

\[(f)(t,_{1},_{2},)=_{^{d_{1}} ^{d_{2}}}f(x_{1},x_{2})(t-_{1}g_{1}(x_{1},_{1}) -_{2}g_{2}(x_{2},_{2}))dx_{1}dx_{2},\] (7)

_where \(=(_{1},_{2})\)._

The reason for using PRT for the final transform is that the previous PGRT are assumed to be able to transform the non-linear structure to a linear line. However, PGRT can still be used as a replacement for PRT. Definition 2 can be extended to more than two marginals (see Appendix B).

**Proposition 2**.: _For some defining functions \(g_{1},g_{2}\) such as linear, circular, and homogeneous polynomials with an odd degree; Hierarchical Hybrid Radon Transform is injective, i.e., for any functions \(f_{1},f_{2}_{1}(^{d})\), \((f_{1})(t,_{1},_{2},)=(f_{2})(t, _{1},_{2},)\  t,_{1},_{2},\) implies \(f_{1}=f_{2}\)._

The proof of Proposition 2 is given in Appendix A.2. The main idea to prove the injectivity of HHRT is to show that HHRT is the composition of PRT and multiple PGRTs.

HMRT of discrete distributions.We are given \(f(x)=_{i=1}^{n}_{i}((x_{1},x_{2})-(x_{1i},x_{2i}))\) (\(n 1\), \(_{i} 0\)\( i\)). The HHRT of \(f(x)\) is \((f)(t,_{1},_{2},)=_{i=1}^{n}_{i} (t-_{1}g_{1}(x_{i1},_{1})-_{2}g_{2}(x_{i2},_{2}))\). For \(g_{1}\) and \(g_{2}\) that are the linear function and (or) the circular function, the time complexity of the transform is \((d_{1}+d_{2})\) which is the same as the complexity of using RT and GRT directly. However, HHRT has an additional constant complexity scaling linearly with the number of marginals, i.e., two marginals in Definition 2.

**Example 1**.: _In this paper, we focus on 3D shape data (mesh) with points and normals representation, i.e., shapes as points representation . In particular, we can transform a 3D shape into a set of points and normals by sampling from the surface of the mesh. In addition, we can convert back to the 3D shape from points and normals with Poisson surface reconstruction  algorithm. In this setup, a shape is represented by a 6-dimensional vector \(x=(x_{1},x_{2})\) where \(x_{1}_{1}^{3}\) and \(x_{2}_{2}^{2}\). For the set \(_{1}^{3}\), we can use directly the linear defining function \(g_{1}(x_{1},_{1})= x_{1},_{1}\) with \(_{1}^{2}\). For the set \(_{2}^{2}\), we can utilize the circular defining function \(g_{2}(x_{2},_{2})=\|x_{2}-r_{2}\|_{2}\) with \(r^{+}\) and \(_{2}^{2}\). As alternative options for \(_{2}\), we can use other defining functions from special cases of GRT including Vertical Slice Transform , Parallel Slice Transform , Spherical Radon Transform , and Stereographic Spherical Radon Transform ._

**Inversion.** In Proposition 2, we show that HHRT is the composition of PRT and multiple PGRTs. Therefore, the inversion of HHRT is the composition of the inversion of multiple PGRT (invertibility of PGRT depends on the choice of defining functions ) and the inversion of PRT .

### Hierarchical Hybrid Sliced Wasserstein Distance

By using HHRT, we obtain a novel variant of SW which is specifically designed for comparing heterogeneous joint distributions.

**Definitions.** We now define the Hierarchical Hybrid Sliced Wasserstein (H2SW) distance.

**Definition 3**.: _For \(p 1\), defining functions \(g_{1},g_{2}\), the hierarchical hybrid sliced Wasserstein-\(p\) (H2SW) distance between two distributions \((_{1}_{2})\) and \((_{1}_{2})\) with an one-dimensional ground metric \(c:^{+}\) is defined as:_

\[_{p}^{p}(,;c,g_{1},g_{2})=_{(_{1},_{2}, )(_{1}_{2})}[_{p }^{p}(_{_{1},_{2},}^{g_{1},g_{2}}, _{_{1},_{2},}^{g_{1},g_{2}};c)],\] (8)

_where \(_{_{1},_{2},}\) and \(_{_{1},_{2},}\) are the one-dimensional push-forward distributions created by applying HHRT._

Definition 3 can be easily extended to more than two marginals (see Appendix B)

**Topological Properties.** We first show that H2SW is a valid metric on the space of distributions on any sets \(^{d_{1}}^{d_{2}}\) (\(d_{1},d_{2} 1\)).

**Theorem 1**.: _For any \(p 1\), ground metric \(c\), and defining functions \(g_{1},g_{2}\) which lead to the injectivity of GRT, the hierarchical hybrid sliced Wasserstein H2SW\({}_{p}(,;c,g_{1},g_{2})\) is a metric on \((^{d_{1}}^{d_{2}})\) i.e., it satisfies the symmetry, non-negativity, triangle inequality, and identity of indiscernible._

Figure 1: Generalized Radon Transform and Hierarchical Hybrid Radon Transform on a discrete distribution.

The proof of Theorem 1 is given in Appendix A.3. It is worth noting that the identity of indiscernible property is proved by the injectivity of HHRT (Proposition 2). We now discuss the connection of H2SW to GSW and Wasserstein distance in some specific cases.

**Proposition 3**.: _For any \(p 1\), \(c(x,y)=|x-y|\), and \(,(^{d_{1}}^{d_{2}})\), we have: (i) \(_{p}(,;c,g_{1},g_{2})_{p}(_{1},_{1};c,g_{1 })+_{p}(_{2},_{2};c,g_{2})\), where \(_{1}(X)=(X^{d_{2}})\) and \(_{2}(Y)=(^{d_{1}} Y)\) (similar with \(_{1}\) and \(_{2}\)). (ii) If \(g_{1}\), \(g_{2}\) are linear defining functions, \(_{p}(,;c,g_{1},g_{2}) W_{p}(_{1},_{1};c)+W_{p}(_ {2},_{2};c)\). (iii) If \(p=1\), \(g_{1}\), \(g_{2}\) are linear defining functions, \(_{1}(,;c,g_{1},g_{2}) W_{1}(,;c)\)._

The proof of Proposition 3 is given in Appendix A.4.

**Sample Complexity.** We now discuss the sample complexity of H2SW.

**Proposition 4**.: _For any \(p 1\), dimension \(d_{1},d_{2} 1\), \(q>p\), \(c(x,y)=|x-y|\), \(g_{1},g_{2}\) are linear defining functions or circular defining functions, and \(,_{q}(^{d_{1}}^{d_{2}})\) with the corresponding empirical distributions \(_{n}\) and \(_{n}\) (\(n 1\)), there exists a constant \(C_{p,q}\) depending on \(p,q\) such that:_

\[|_{p}(_{n},_{n};c,g_{1},g_{2})- _{p}(,;c,g_{1},g_{2})|\] \[ C_{p,q}^{}(_{i=0}^{q}q^{i}C_{g_{1},g_{2}}^{q-i}(M_{i}()+M_{i}()))^{}n^{-1/2p }q>2p,\\ n^{-1/2p}(n)^{}q=2p,\\ n^{-(q-p)/pq}q(p,2p),\] (9)

_where \(M_{q}()\) and \(M_{q}()\) are the \(q\)-th moments of \(\) and \(\), \(C_{g_{1},g_{2}}\) is a constant depends on \(g_{1}\), \(g_{2}\)._

The proof of Proposition 4 is given in Appendix A.5. The rate in Proposition 4 is as good as the rate of SW in , however, it is slightly worse than the rate of SW in  due to the usage of the circular defining functions and simpler assumptions. To the best of our knowledge, the sample complexity of GSW has not been investigated.

**Monte Carlo Estimation.** Since the expectation in H2SW (Equation 8) is intractable, Monte Carlo estimation and Quasi-Monte Carlo approximation  can be used to form a practical evaluation of H2SW. Here, we utilize Monte Carlo estimation for simplicity. In particular, we sample \(_{11},,_{1L}(_{1})\), \(_{21},,_{2L}(_{2})\), and \(_{1},,_{L}()\). After that, we form the following estimation of H2SW:

\[}_{p}^{p}(,;c,g_{1},g_{2},L)=_{l=1}^ {L}_{p}^{p}(_{_{1l},_{2l},_{l}}^{g_{1},g _{2}},_{_{1l},_{2l},_{l}}^{g_{1},g_{2}} ;c).\] (10)

**Proposition 5**.: _For any \(p 1\), dimension \(d_{1},d_{2} 1\), and \(,(_{1}^{d}^{d_{2}})\), we have:_

\[[}_{p}^{p}(,;c,g_{1},g_{2},L )-_{p}^{p}(,;c,g_{1},g_{2})]\] \[}Var[_{p}^{p}(_{_{1},_{2l},}^{g_{1},g_{2}},_{_ {1},_{2},}^{g_{1},g_{2}};c)]^{},\] (11)

_where the variance is with respect to \((_{1}_{2})\)._

The proof of Proposition 5 is given in Appendix A.6. From the proposition, we see that the estimation error of H2SW is the same as SW which is \((L^{-1/2})\).

**Computational Complexities.** The time complexity and memory complexity of H2SW with linear and circular defining functions are \((Ln n+L(d_{1}+d_{2}+k)n)\) and \((Ln+(d_{1}+d_{2}+k)n)\) with \(k\) is the number of marginals i.e., 2. We can see that the complexities of H2SW are the same as those of SW in terms of the number of supports \(n\) and the number of dimensions \(d\). We demonstrate the process of HHRT compared to GRT on a discrete distribution with \(L\) realization of \(_{1},_{2},\) in Figure 1. Overall, the complexities of defining functions are often different in the number of dimensions, hence, H2SW is always scaled the same as SW in the number of supports i.e., \((n n)\).

**Gradient Estimation.** In applications, it is desirable to estimate the gradient \(_{}_{p}^{p}(_{},;c,g_{1},g_{2})\). We can move the gradient operator to inside the expectation and then apply Monte Carlo estimation. The gradient \(_{}_{p}^{p}(_{_{1}, _{2},}^{g_{1},g_{2},}_{},_{ _{1},_{2},}^{g_{1},g_{2}};c)]\) can be computed easily since the functions \(g_{1},g_{2}\) are usually differentiable.

Beyond uniform slicing distribution.H2SW is defined with the uniform slicing distribution in Definition 3, however, it is possible to extend it to other slicing distributions such as the maximal projecting direction , distributional slicing distribution , and energy-based slicing distribution . Since the choice of slicing distribution is independent of the main contribution i.e., the slicing operator, we leave this investigation to future work.

H2SW for distributions on the product of Hadamard manifolds.A recent work  extends sliced Wasserstein on hyperbolic manifolds  and on the manifold of symmetric positive definite matrices  to Hadamard manifolds i.e., manifold non-positive curvature. The work discusses the extension of SW to the product of Hadamard manifolds. For the geodesic projection, the closed-form for the projection is intractable. For the Busemann projection, the Busemann projection on the product manifolds is the weighted sum of the Busemann projection with the weights belonging to the unit-sphere. In the work, the weights are a fixed hyperparameter i.e., Cartan-Hadamard Sliced-Wasserstein (CHSW) utilizes only one Busemann function to project the joint distribution. In contrast, H2SW utilizes the Radon Transform on the joint spaces of projections i.e., considering all distributed weighted combinations which is equivalent to considering all Busemann functions under a probability law. As a result, the H2SW is a valid metric as long as the Busemann projections can be proven to be injective (the injectivity of the Busemann projection has not been known at the moment) while Cartan-Hadamard Sliced-Wasserstein is only pseudo metric since the injectivity of a fixed weighted combination is not trivial to show. Moreover, H2SW does not only focus on the product of Hadamard manifolds i.e., H2SW is a generic distance for heterogeneous joint distributions in which marginal domains are not necessary manifolds e.g., images , functions , and so on. In the later experiments, we conduct experiments on comparing 3D shapes which are represented by a distribution on the product of the Euclidean space and the 2D sphere (not a Hadamard manifold).

## 4 Experiments

In this section, we first compare the performance of the proposed H2SW with SW and GSW in the 3D mesh deformation application. After that, we further evaluate the performance of H2SW in training a deep 3D mesh autoencoder compared to SW and GSW. Finally, we compare H2SW with SW and Cartan-Hadamard Sliced-Wasserstein (CHSW) in datasets comparison on the product of Hadamard manifolds. In the experiments, we use \(c(x,y)=|x-y|\) and \(p=2\) for all SW variants.

### 3D Mesh Deformation

In this task, we would like to move from a source mesh to a target mesh. To represent those meshes, we sample \(10000\) points by Poisson disk sampling and their corresponding normal vectors of the mesh surface at those points. Let the source mesh be denoted as \(X(0)=\{x_{1}(0),,x_{n}(0)\}\) and the target mesh be denoted as \(Y=\{y_{1},,y_{n}\}\). We deform \(X(0)\) to \(Y\) by integrating the ordinary differential equation \((t)=-n_{X(t)}[(_{i=1}^{n} (x-x_{i}(t)),_{i=1}^{n}(y-y_{i}))]\), where \(\) denotes a SW

   Distances & Step 100 (W\({}_{_{}},z_{1}}\)) & Step 300 (W\({}_{}},z_{1}\)) & Step 500 (W\({}_{}},z_{2}\)) & Step 1500 (W\({}_{}},z_{1}\)) & Step 4000 (W\({}_{}},z_{1}\)) & Step 5000 (W\({}_{}},z_{1}\)) \\  SW =10 & 1852.59\(\)3.26 & 1436.68\(\)0.356 & 1071.22\(\)2.449 & 1404.52\(\)2.35 & **6.19\(\)0.307** & 2.726\(\)0.305 \\ GSW =10 & 1893.43\(\)3.205 & 1537.57\(\)3.363 & 1915.25\(\)2.747 & 143.51\(\)1.04 & 8.73\(\)0.353 & 4.73\(\)0.134 \\ H2SW =10 & **1840.73\(\)1.282** & **1422.667\(\)7.813** & **1058.17\(\)5.362** & **95.672\(\)4.376** & 6.326\(\)0.151 & **2.602\(\)0.201** \\  SW =100 & 1847.57\(\)0.303 & 1462.45\(\)0.528 & 1059.127\(\)1.106 & 869.69\(\)0.793 & **4.453\(\)0.22** & 1.171\(\)0.056 \\ GSW =100 & 1893.91\(\)0.883 & 1525.269\(\)0.1078 & 1179.12\(\)0.212 & 1262.613\(\)1.175 & 7.905\(\)0.373 & 3.226\(\)0.388 \\ H2SW =100 & **1839.347\(\)1.986** & **1471.13\(\)3.677** & **1048.895\(\)4.008** & **56.078\(\)0.623** & 4.61\(\)0.431 & **1.886\(\)0.177** \\   

Table 1: Summary of joint Wasserstein distances across time steps from deformation from the sphere mesh to the Armadillo mesh.

   Distances & Step 100 (W\({}_{},z_{1}}\)) & Step 300 (W\({}_{}},z_{1}\)) & Step 500 (W\({}_{}},z_{1}\)) & Step 5100 (W\({}_{}},z_{1}\)) & Step 4000 (W\({}_{}},z_{1}\)) & Step 5000 (W\({}_{}},z_{1}\)) \\  SW =10 & 26.884\(\)0.579 & 4.646\(\)0.195 & 1.52\(\)0.081 & **0.653\(\)0.042** & 0.271\(\)0.031 & 0.144\(\)0.088 \\ GSW =100 & 26.837\(\)0.496 & 4.378\(\)0.128 & 1.548\(\)0.062 & 0.653\(\)0.014 & **0.719\(\)0.018** & 0.146\(\)0.013 \\ H2SW =10 & **23.838\(\)0.119** & **2.221\(\)0.124** & **1.452\(\)0.075** & 0.636\(\)0.045 & 0.177\(\)0.009 & **0.809\(\)0.012** \\  SW =100 & 26.768\(\)0.168 & 1.409\(\)0.138 & 1.458\(\)0.142 & **0.362\(\)0.032** & 0.72\(\)0.017 & 0.049\(\)0.006 \\ GSW =100 & 26.795\(\)0.202 & 4.084\(\)0.109 & 1.375\(\)0.049 & 0.372\(\)0.026 & **0.048\(\)0.004** & 0.042\(\)0.017 \\ H2SW =100 & **23.772\(\)0.19** & **2.388\(\)0.009** & **1.358\(\)0.051** & 0.488\(\)0.026 & 0.064\(\)0.01 & **0.038\(\)0.007** \\   

Table 2: Summary of joint Wasserstein distances (multiplied by 100) across time steps from deformation from the sphere mesh to the Stanford Bunny mesh.

[MISSING_PAGE_FAIL:8]

Point-Net  architecture to construct the autoencoder. We want to train the encoder \(f_{}\) and the decoder \(g_{}\) such that \(=g_{}(f_{}(X)) X\) for all shapes \(X\) in the dataset. To do that, we solve the following optimization problem:

\[_{,}_{X(X)}[(P_{X},P_{g_{}(f_{ }(X)))}],\]

where \(\) is a sliced Wasserstein variant, and \(P_{X}=_{i=1}^{n}(x-x_{i})\) denotes the empirical distribution over the point cloud \(X=(x_{1},,x_{n})\). We train the autoencoder for \(2000\) epochs on the training set of the ShapeNet dataset using an SGD optimizer with a learning rate of \(1e-3\), and a batch size of \(128\). For evaluation, we also use the joint Wasserstein distance in Equation 5 with the mixed distance from the Euclidean distance and the great circle distance to measure the average reconstruction loss on the testing set of the ShapeNet dataset. We use the circular defining function for GSW, and use the linear defining function and the circular defining function for H2SW. For H2SW and GSW, we select the best hyperparameter of the circular defining function \(r\{0.5,0.7,0.8,0.9,1,5,10\}\). For more details such as the neural network architectures, we refer the reader to Appendix B.

**Results.** We report the joint Wasserstein reconstruction errors (measured in three independent times) on the testing set in Table 3 with trained autoencoder at epoch 500, 1000, and 2000 from SW, GSW, and H2SW with the number of projections \(L=100\) and \(L=1000\). In addition, we show some randomly reconstructed meshes for epoch 2000in Figure 4 and for epoch 500 in Figure 8 in Appendix D. From Table 3, we observe that H2SW yields the lowest reconstruction errors for both \(L=100\) and \(L=1000\). Moreover, we see that the reconstruction errors are lower with \(L=1000\) than ones with \(L=100\) for all SW variants. The qualitative reconstructed meshes in Figure 4 and Figure 8reflect the same relative comparison. It is worth noting that both the qualitative and the qualitative performance of autoencoders can be improved by using more powerful neural networks. Since we focus on comparing SW, GSW, and H2SW, we only use a light neural network i.e., Point-Net  architecture. The trained autoencoders can be further used to reduce the size of 3D meshes for data compression and for dimension reduction, however, such downstream applications are not our focus in the current investigation of the paper.

Figure 4: Visualization of some randomly selected reconstruction meshes from autoencoders trained by SW, GSW, and H2SW in turn with the number of projections \(L=100\) at epoch 2000.

   Distance & Epoch 500 W\({}_{c_{1},c_{2}}()\) & Epoch 1000 W\({}_{c_{1},c_{2}}()\) & Epoch 2000 W\({}_{c_{1},c_{2}}()\) \\  SW L=100 & \(136.87 1.18\) & \(133.24 0.50\) & \(131.10 0.34\) \\ GSW L=100 & \(136.51 0.20\) & \(133.36 0.24\) & \(130.80 0.46\) \\ H2SW L=100 & \(\) & \(\) & \(\) \\  SW L=1000 & \(135.85 0.92\) & \(132.88 0.27\) & \(130.93 0.11\) \\ GSW L=1000 & \(136.40 0.10\) & \(133.02 0.98\) & \(130.76 0.26\) \\ H2SW L=1000 & \(\) & \(\) & \(\) \\   

Table 3: Joint Wasserstein distance reconstruction errors (multiplied by 100) from three different runs of autoencoders trained by SW, GSW, and H2SW with the number of projections \(L=100\) and \(L=1000\).

### Comparing Datasets on The Product of Hadamard Manifolds

We follow the same experimental setting from . Here, we have datasets as sets of feature-label pairs which are embedded in the space of \(^{d_{1}}^{d_{2}}\) where \(^{d_{2}}\) denotes a Lorentz model of \(d_{2}\) dimension (a hyperbolic space). We uses MNIST  dataset, EMNIST dataset , Fashion MNIST dataset , KMNIST dataset , and USPS dataset . For CHSW, we use Busemann projection on the product space of Euclidean and the Lorentz model. For H2SW, we use the linear defining function and the Busemann function on the Lorentz model. We refer the reader to Appendix B for greater detail on Busemann functions and experimental setups. We compare SW, CHSW, and H2SW by varying \(L\{100,500,1000,2000\}\). For evaluation, we use the joint Wasserstein distance in  as the ground truth. In particular, let \(C_{W}\) be the cost matrix from the joint Wasserstein distance and \(C\) be a given cost matrix, we use \(|C/(C)-C_{W}/(C_{W})|\) as the relative error.

**Results.** We report the relative errors from SW, CHSW, and H2SW in Table 4 after 100 independent runs. In addition, we show the cost matrices from SW, CHSW, H2SW. and joint Wasserstein distance with \(L=2000\) in Figure 5. Cost matrices for \(L=100\), \(L=500\), and \(L=1000\) are given in Figure 9- 11 in Appendix D. From Table 4, we see that H2SW gives a lower relative error than CHSW and SW. Therefore, using H2SW for comparing datasets is the most equivalent to the joint Wasserstein distance in terms of the relative error. We also observe that increasing the value of the number of projections also reduces the relative errors for all SW variants. Again, we would like to recall that H2SW can be used for heterogeneous joint distributions beyond the product of Hadamard manifolds as shown in previous experiments.

## 5 Conclusion

We have presented Hierarchical Hybrid Sliced Wasserstein (H2SW) distance, a novel sliced probability metric for heterogeneous joint distributions i.e., joint distributions have marginals on different domains. The key component of H2SW is the proposed hierarchical hybrid Radon Transform (HMRT) which is the composition of partial Radon Transform and multiples proposed partial generalized Radon Transform. We then discuss the injectivity of the proposed transforms and theoretical properties of H2SW including topological properties, statistical properties, and computational properties. On the experimental side, we show that H2SW has favorable performance in applications of 3D mesh deformation, training deep 3D mesh autoencoder, and datasets comparison. In those applications, heterogeneous joint distributions appear in the form of joint distributions on the product of Euclidean space and 2D sphere, and the product of Hadamard manifolds. In the future, we will extend the application of H2SW to more complicated heterogeneous joint distributions.

   Distances & \(L=100\) & \(L=500\) & \(L=1000\) & \(L=2000\) \\  SW & \(4.618 0.744\) & \(4.253 0.398\) & \(4.235 0.310\) & \(4.198 0.238\) \\ CHSW & \(4.449 0.497\) & \(4.063 0.254\) & \(4.059 0.167\) & \(4.035 0.145\) \\ H2SW & \(\) & \(\) & \(\) & \(\) \\   

Table 4: Relative error to the joint Wasserstein distance of SW, CHSW, and H2SW.

Figure 5: Cost matrices between datasets from SW, CHSW, and H2SW with \(L=2000\).