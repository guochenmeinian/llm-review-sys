# Unexpected Improvements to Expected Improvement

for Bayesian Optimization

 Sebastian Ament

Meta

ament@meta.com

Samuel Daulton

Meta

sdaulton@meta.com

David Eriksson

Meta

deriksson@meta.com

Maximilian Balandat

Meta

balandat@meta.com

Eytan Bakshy

Meta

ebakshy@meta.com

###### Abstract

Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their constrained, noisy, and parallel variants, and propose corresponding reformulations that remedy these pathologies. Our empirical results show that members of the LogEI family of acquisition functions substantially improve on the optimization performance of their canonical counterparts and surprisingly, are on par with or exceed the performance of recent state-of-the-art acquisition functions, highlighting the understated role of numerical optimization in the literature.

## 1 Introduction

Bayesian Optimization (BO) is a widely used and effective approach for sample-efficient optimization of expensive-to-evaluate black-box functions , with applications ranging widely between aerospace engineering , biology and medicine , materials science , civil engineering , and machine learning hyperparameter optimization . BO leverages a probabilistic _surrogate model_ in conjunction with an _acquisition function_ to determine where to query the underlying objective function. Improvement-based acquisition functions, such as Expected Improvement (EI) and Probability of Improvement (PI), are among the earliest and most widely used acquisition functions for efficient global optimization of non-convex functions . EI has been extended to the constrained , noisy , and multi-objective  setting, as well as their respective batch variants , and is a standard baseline in the BO literature . While much of the literature has focused on developing new sophisticated acquisition functions, subtle yet critical implementation details of foundational BO methods are often overlooked. Importantly, the performance of EI and its variants is inconsistent even for _mathematically identical_ formulations and, as we show in this work, most often sub-optimal.

Although the problem of optimizing EI effectively has been discussed in various works, e.g. [25; 31; 77], prior focus has been on optimization algorithms and initialization strategies, rather than the fundamental issue of computing EI.

In this work, we identify pathologies in the computation of improvement-based acquisition functions that give rise to numerically vanishing values and gradients, which - to our knowledge - are present in _all existing implementations of EI_, and propose reformulations that lead to increases in the associated optimization performance which often match or exceed that of recent methods.

#### Contributions

1. We introduce LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. Notably, the analytic variant of LogEI, which _mathematically_ results in the same BO policy as EI, empirically shows significantly improved optimization performance.
2. We extend the ideas behind analytical LogEI to other members of the EI family, including constrained EI (CEI), Expected Hypervolume Improvement (EHVI), as well as their respective batch variants for parallel BO, qEI and qEHVI, using smooth approximations of the acquisition utilities to obtain non-vanishing gradients. All of our methods are available as part of BoTorch .
3. We demonstrate that our newly proposed acquisition functions substantially outperform their respective analogues on a broad range of benchmarks without incurring meaningful additional computational cost, and often match or exceed the performance of recent methods.

#### Motivation

Maximizing acquisition functions for BO is a challenging problem, which is generally non-convex and often contains numerous local maxima, see the lower right panel of Figure 1. While zeroth-order methods are sometimes used, gradient-based methods tend to be far more effective at optimizing acquisition functions on continuous domains, especially in higher dimensions.

In addition to the challenges stemming from non-convexity that are shared across acquisition functions, the values and gradients of improvement-based acquisition functions are frequently minuscule in large swaths of the domain. Although EI is never _mathematically_ zero under a Gaussian posterior distribution,1 it often vanishes, even becoming _exactly_ zero in floating point precision. The same

Figure 1: **Left:** Fraction of points sampled from the domain for which the magnitude of the gradient of EI vanishes to \(<\!10^{-10}\) as a function of the number of randomly generated data points \(n\) for different dimensions \(d\) on the Ackley function. As \(n\) increases, EI and its gradients become numerically zero across most of the domain, see App. D.2 for details. **Right:** Values of EI and LogEI on a quadratic objective. EI takes on extremely small values on points for which the likelihood of improving over the incumbent is small and is numerically _exactly_ zero in double precision for a large part of the domain (\([5,13.5]\)). The left plot shows that this tends to worsen as the dimensionality of the problem and the number of data points grow, rendering gradient-based optimization of EI futile.

applies to its gradient, making EI (and PI, see Appendix A) exceptionally difficult to optimize via gradient-based methods. The right panels of Figure 1 illustrate this behavior on a simple one-dimensional quadratic function.

To increase the chance of finding the global optimum of non-convex functions, gradient-based optimization is typically performed from multiple starting points, which can help avoid getting stuck in local optima . For improvement-based acquisition functions however, optimization becomes increasingly challenging as more data is collected and the likelihood of improving over the incumbent diminishes, see our theoretical results in Section 3 and the empirical illustration in Figure 1 and Appendix D.2. As a result, gradient-based optimization with multiple random starting points will eventually degenerate into random search when the gradients at the starting points are numerically zero. This problem is particularly acute in high dimensions and for objectives with a large range.

Various initialization heuristics have been proposed to address this behavior by modifying the random-restart strategy. Rather than starting from random candidates, an alternative naive approach would be to use initial conditions close to the best previously observed inputs. However, doing that alone inherently limits the acquisition optimization to a type of local search, which cannot have global guarantees. To attain such guarantees, it is necessary to use an asymptotically space-filling heuristic; even if not random, this will entail evaluating the acquisition function in regions where no prior observation lies. Ideally, these regions should permit gradient-based optimization of the objective for efficient acquisition function optimization, which necessitates the gradients to be non-zero. In this work, we show that this can be achieved for a large number of improvement-based acquisition functions, and demonstrate empirically how this leads to substantially improved BO performance.

## 2 Background

We consider the problem of maximizing an expensive-to-evaluate black-box function \(_{}:^{M}\) over some feasible set \(^{d}\). Suppose we have collected data \(_{n}=\{(_{i},_{i})\}_{i=1}^{n}\), where \(_{i}\) and \(_{i}=_{}(_{i})+_{i}( _{i})\) and \(_{i}\) is a noise corrupting the true function value \(_{}(_{i})\). The response \(_{}\) may be multi-output as is the case for multiple objectives or black-box constraints, in which case \(_{i},_{i}^{M}\). We use Bayesian optimization (BO), which relies on a surrogate model \(\) that for any _batch_\(:=\{_{1},,_{q}\}\) of candidate points provides a probability distribution over the outputs \(f():=(f(_{1}),,f(_{q}))\). The acquisition function \(\) then utilizes this posterior prediction to assign an acquisition value to \(\) that quantifies the value of evaluating the points in \(\), trading off exploration and exploitation.

### Gaussian Processes

Gaussian Processes (GP)  are the most widely used surrogates in BO, due to their high data efficiency and good uncertainty quantification. For our purposes, it suffices to consider a GP as a mapping that provides a multivariate Normal distribution over the outputs \(f()\) for any \(\):

\[f()((), ()),:^{q}^{qM}, :^{q}_{+}^{qM}. \]

In the single-outcome (\(M=1\)) setting, \(f()((),())\) with \(:^{q}^{q}\) and \(:^{q}_{+}^{q}\). In the sequential (\(q=1\)) case, this further reduces to a univariate Normal distribution: \(f()((),^{2}())\) with \(:\) and \(:_{+}\).

### Improvement-based Acquisition Functions

Expected ImprovementFor the fully-sequential (\(q=1\)), single-outcome (\(M=1\)) setting, "classic" EI  is defined as

\[_{y^{*}}()=_{f()}[f()- y^{*}]_{+}=()\;h()-y^{*}}{ ()}), \]

where \([]_{+}\) denotes the \((0,)\) operation, \(y^{*}=_{i}y_{i}\) is the best function value observed so far, also referred to as the _incumbent_, \(h(z)=(z)+z(z)\), and \(,\) are the standard Normal density and distribution functions, respectively. This formulation is arguably the most widely used acquisition function in BO, and the default in many popular software packages.

Constrained Expected Improvement_Constrained BO_ involves one or more black-box constraints and is typically formulated as finding \(_{}f_{,1}()\) such that \(f_{,i}() 0\) for \(i\{2,,M\}\). Feasibility-weighting the improvement [27; 29] is a natural approach for this class of problems:

\[_{y^{*}}()=_{()}[[f_{1}( )-y^{*}]_{+}\ _{i=2}^{M}_{f_{i}() 0}], \]

where \(1\) is the indicator function. If the constraints \(\{f_{i}\}_{i 2}\) are modeled as conditionally independent of the objective \(f_{1}\) this can be simplified as the product of EI and the probability of feasibility.

Parallel Expected ImprovementIn many settings, one may evaluate \(f_{}\) on \(q>1\) candidates in parallel to increase throughput. The associated parallel or batch analogue of EI [30; 75] is given by

\[_{y^{*}}()=_{f()}[_{j=1, ,q}[f(_{j})-y^{*}]_{+}}]. \]

Unlike EI, qEI does not admit a closed-form expression and is thus typically computed via Monte Carlo sampling, which also extends to non-Gaussian posterior distributions [6; 75]:

\[_{y^{*}}()_{i=1}^{N}_{j=1,,q} [^{i}(_{j})-y^{*}]_{+}}, \]

where \(^{i}() f()\) are random samples drawn from the joint model posterior at \(\).

Expected Hypervolume ImprovementIn multi-objective optimization (MOO), there generally is no single best solution; instead the goal is to explore the Pareto Frontier between multiple competing objectives, the set of mutually-optimal objective vectors. A common measure of the quality of a finitely approximated Pareto Frontier \(\) between \(M\) objectives with respect to a specified reference point \(^{M}\) is its _hypervolume_\((,):=_{_{j} }[,_{i}]\), where \([,_{i}]\) denotes the hyperrectangle bounded by vertices \(\) and \(_{i}\), and \(\) is the Lebesgue measure. An apt acquisition function for multi-objective optimization problems is therefore the expected hypervolume improvement

\[()=_{()}[[( (),)-(, )]_{+}], \]

due to observing a batch \(():=[(_{1}),,(_{q})]\) of \(q\) new observations. EHVI can be expressed in closed form if \(q=1\) and the objectives are modeled with independent GPs , but Monte Carlo approximations are required for the general case (qEHVI) .

### Optimizing Acquisition Functions

Optimizing an acquisition function (AF) is a challenging task that amounts to solving a non-convex optimization problem, to which multiple approaches and heuristics have been applied. These include gradient-free methods such as divided rectangles , evolutionary methods such as CMA-ES , first-order methods such as stochastic gradient ascent, see e.g., Daulton et al. , Wang et al. , and (quasi-)second order methods  such as L-BFGS-B . Multi-start optimization is commonly employed with gradient-based methods to mitigate the risk of getting stuck in local minima. Initial points for optimization are selected via various heuristics with different levels of complexity, ranging from simple uniform random selection to BoTorch's initialization heuristic, which selects initial points by performing Boltzmann sampling on a set of random points according to their acquisition function value . See Appendix B for a more complete account of initialization strategies and optimization procedures used by popular implementations. We focus on gradient-based optimization as often leveraging gradients results in faster and more performant optimization .

Optimizing AFs for parallel BO that quantify the value of a batch of \(q>1\) points is more challenging than optimizing their sequential counterparts due to the higher dimensionality of the optimization problem - \(qd\) instead of \(d\) - and the more challenging optimization surface. A common approach to simplify the problem is to use a _sequential greedy_ strategy that greedily solves a sequence of single point selection problems. For \(i=1,,q\), candidate \(_{i}\) is selected by optimizing the AF for \(q=1\), conditional on the previously selected designs \(\{_{1},...,_{i-1}\}\) and their unknown observations, e.g. by fantasizing the values at those designs . For submodular AFs, including EI, PI, and EHVI, a sequential greedy strategy will attain a regret within a factor of \(1/e\) compared to the joint optimum, and previous works have found that sequential greedy optimization yields _improved_ BO performance compared to joint optimization [13; 77]. Herein, we find that our reformulations enable joint batch optimization to be competitive with the sequential greedy strategy, especially for larger batches.

### Related Work

While there is a substantial body of work introducing a large variety of different AFs, much less focus has been on the question of how to effectively implement and optimize these AFs. Zhan and Xing  provide a comprehensive review of a large number of different variants of the EI family, but do not discuss any numerical or optimization challenges. Zhao et al.  propose combining a variety of different initialization strategies to select initial conditions for optimization of acquisition functions and show empirically that this improves optimization performance. However, they do not address any potential issues or degeneracies with the acquisition functions themselves. Recent works have considered effective gradient-based approaches for acquisition optimization. Wilson et al.  demonstrates how stochastic first-order methods can be leveraged for optimizing Monte Carlo acquisition functions. Balandat et al.  build on this work and put forth sample average approximations for MC acquisition functions that admit gradient-based optimization using deterministic higher-order optimizers such as L-BFGS-B.

Another line of work proposes to switch from BO to local optimization based on some stopping criterion to achieve faster local convergence, using either zeroth order  or gradient-based  optimization. While McLeod et al.  are also concerned with numerical issues, we emphasize that those issues arise due to ill-conditioned covariance matrices and are orthogonal to the numerical pathologies of improvement-based acquisition functions.

## 3 Theoretical Analysis of Expected Improvement's Vanishing Gradients

In this section, we shed light on the conditions on the objective function and surrogate model that give rise to the numerically vanishing gradients in EI, as seen in Figure 1. In particular, we show that as a BO algorithm closes the optimality gap \(f^{*}-y^{*}\), where \(f^{*}\) is the global maximum of the function \(f_{}\), and the associated GP surrogate's uncertainty decreases, EI is exceedingly likely to exhibit numerically vanishing gradients.

Let \(P_{}\) be a distribution over the inputs \(\), and \(f P_{f}\) be an objective drawn from a Gaussian process. Then with high probability over the particular instantiation \(f\) of the objective, the probability that an input \( P_{}\) gives rise to an argument \((()-y^{*})/()\) to \(h\) in Eq. (2) that is smaller than a threshold \(B\) exceeds \(P_{}(f()<f^{*}-_{n})\), where \(_{n}\) depends on the optimality gap \(f^{*}-y^{*}\) and the maximum posterior uncertainty \(_{}_{n}()\). This pertains to EI's numerically vanishing values and gradients, since the numerical support \(_{}(h)=\{:|h()|>\}\) of a naive implementation of \(h\) in (2) is limited by a lower bound \(B()\) that depends on the floating point precision \(\). Formally, \(_{}(h)[B(),)\) even though \(_{0}(h)=\) mathematically. As a consequence, the following result can be seen as a bound on the probability of encountering numerically vanishing values and gradients in EI using samples from the distribution \(P_{}\) to initialize the optimization of the acquisition function.

**Theorem 1**.: _Suppose \(f\) is drawn from a Gaussian process prior \(P_{f}\), \(y^{*} f^{*}\), \(_{n},_{n}\) are the mean and standard deviation of the posterior \(P_{f}(f|_{n})\) and \(B\). Then with probability \(1-\),_

\[P_{}(()-y^{*}}{_{n}()}<B ) P_{}(f()<f^{*}-_{n}) \]

_where \(_{n}=(f^{*}-y^{*})+(-B)_{}_{n}()\)._

For any given - and especially early - iteration, \(_{n}\) does not have to be small, as both the optimality gap and the maximal posterior standard deviation can be large initially. Note that under certain technical conditions on the kernel function and the asymptotic distribution of the training data \(_{n}\), the maximum posterior variance vanishes guaranteedably as \(n\) increases, see [50, Corollary 3.2]. On its own, Theorem 1 gives insight into the non-asymptotic behavior by exposing a dependence to the distribution of objective values \(f\). In particular, if the set of inputs that give rise to high objective values (\( f^{*}\)) is concentrated, \(P(f()<f^{*}-)\) will decay very slowly as \(\) increases, thereby maintaining a lower bound on the probability of close to 1. As an example, this is the case for the Ackley function, especially as the dimensionality increases, which explains the behavior in Figure 1.

Unexpected Improvements

In this section, we propose re-formulations of analytic and MC-based improvement-based acquisition functions that render them significantly easier to optimize. We will use differing fonts, e.g. \(\) and \(\), to differentiate between the mathematical functions and their numerical implementations.

### Analytic LogEI

Mathematically, EI's values and gradients are nonzero on the entire real line, except in the noiseless case for points that are perfectly correlated with previous observations. However, naive implementations of \(h\) are _numerically_ zero when \(z=(()-y^{*})/()\) is small, which happens when the model has high confidence that little improvement can be achieved at \(\). We propose an implementation of \( h\) that can be accurately computed for a much larger range of inputs. Specifically, we compute

\[_{y^{*}}()=((()-y^{*})/ ())+(()), \]

where log_h is mathematically equivalent to \( h\) and can be stably and accurately computed by

\[(z)=((z)+z(z))&z>-1\\ -z^{2}/2-c_{1}+(((-z/)|z |)+c_{2})&-1/<z-1\\ -z^{2}/2-c_{1}-2(|z|)&z-1/ \]

where \(c_{1}=(2)/2\), and \(c_{2}=(/2)/2\), \(\) is the numerical precision, and log1mexp, erfcx are numerically stable implementations of \((1-(z))\) and \((z^{2})(z)\), respectively, see App. A. Progenitors of Eq. (9) are found in SMAC 1.0  and RoBO  which contain a log-transformed analytic EI implementation that is much improved, but can still exhibit instabilities as \(z\) grows negative, see App. Fig. 10. To remedy similar instabilities, we put forth the third, asymptotic case in Eq. (9), ensuring numerical stability throughout, see App. A.2 for details. The asymptotically quadratic behavior of log_h becomes apparent in the last two cases, making the function particularly amenable to gradient-based optimization with _significant_ practical implications for EI-based BO.

### Monte Carlo Parallel LogEI

Beyond analytic EI, Monte Carlo formulations of parallel EI that perform differentiation on the level of MC samples, don't just exhibit numerically, but mathematically zero gradients for a significant proportion of practically relevant inputs. For qEI, the primary issue is the discrete maximum over the \(q\) outcomes for each MC sample in (5). In particular, the acquisition utility of expected improvement in Eq. 4 on a single sample \(_{i}\) of \(f\) is \(_{j}[_{i}(_{j})-y^{*}]_{+}\). Mathematically, we smoothly approximate the acquisition utility in two stages: 1) \(u_{ij}=_{_{0}}(_{i}(_{j})-y^{*}) [_{i}(_{j})-y^{*}]_{+}\) and 2) \( u_{i}._{1/_{}}_{j}u_{ij}\). Notably, while we use canonical \(\) and p-norm approximations here, specialized fat-tailed non-linearities are required to scale to large batches, see Appendix A.4. Since the resulting quantities are strictly positive, they can be transformed to log-space permitting an implementation of qLogEI that is numerically stable and can be optimized effectively. In particular,

\[_{y^{*}}()&= (_{j=1}^{q}_{_{0}}(f(_{j })-y^{*})^{1/_{}})^{_{}}df\\ &_{i}(_{}_{j}( _{_{0}}(^{i}(_{j})-y^{*}))/_{})),  \]

where \(i\) is the index of the Monte Carlo draws from the GP posterior, \(j=1,,q\) is the index for the candidate in the batch, and logsoftplus is a numerically stable implementation of \(((1+(z)))\). See Appendix A.3 for details, including the novel fat-tailed non-linearities like fatplus.

While the smoothing in (10) approximates the canonical qEI formulation, the following result shows that the associated relative approximation error can be quantified and bounded tightly as a function of the temperature parameters \(_{0}\), \(_{}\) and the batch size \(q\). See Appendix C for the proof.

**Lemma 2**.: _[Relative Approximation Guarantee] Given \(_{0},_{}>0\), the approximation error of qLogEI to qEI is bounded by_

\[|e^{()}-()|(q^{_ {}}-1)\;()+(2)_{0}q^{_{}}. \]

In Appendix D.10, we show the importance of setting the temperatures sufficiently low for qLogEI to achieve good optimization characteristics, something that only becomes possible by transforming all involved computations to log-space. Otherwise, the smooth approximation to the acquisition utility would exhibit vanishing gradients numerically, as the discrete \(\) operator does mathematically.

### Constrained EI

Both analytic and Monte Carlo variants of LogEI can be extended for optimization problems with black-box constraints. For analytic CEI with independent constraints of the form \(f_{i}() 0\), the constrained formulation in Eq. (3) simplifies to \(()=()+_{i}(P(f_{i}() 0))\), which can be readily and stably computed using LogEI in Eq. (8) and, if \(f_{i}\) is modelled by a GP, a stable implementation of the Gaussian log cumulative distribution function. For the Monte Carlo variant, we apply a similar strategy as for Eq. (10) to the constraint indicators in Eq. (3): 1) a smooth approximation and 2) an accurate and stable implementation of its log value, see Appendix A.

### Monte Carlo Parallel LogEHVI

The numerical difficulties of qEHVI in (6) are similar to those of qEI, and the basic ingredients of smoothing and log-transformations still apply, but the details are significantly more complex since qEHVI uses many operations that have mathematically zero gradients with respect to some of the inputs. Our implementation is based on the differentiable inclusion-exclusion formulation of the hypervolume improvement . As a by-product, the implementation also readily allows for the differentiable computation of the expected log hypervolume, instead of the log expected hypervolume, note the order, which can be preferable in certain applications of multi-objective optimization .

## 5 Empirical Results

We compare standard versions of analytic EI (EI) and constrained EI (CEI), Monte Carlo parallel EI (qEI), as well as Monte Carlo EHVI (qEHVI), in addition to other state-of-the-art baselines like lower-bound Max-Value Entropy Search (GIBBON)  and single- and multi-objective Joint Entropy Search (JES) . All experiments are implemented using BoTorch  and utilize multi-start optimization of the AF with scipy's L-BFGS-B optimizer. In order to avoid conflating the effect of BoTorch's default initialization strategy with those of our contributions, we use 16 initial points chosen uniformly at random from which to start the L-BFGS-B optimization. For a comparison with other initialization strategies, see Appendix D. We run multiple replicates and report mean and error bars of \( 2\) standard errors of the mean. Appendix D.1 contains additional details.

Single-objective sequential BOWe compare EI and LogEI on the 10-dimensional convex Sum-of-Squares (SoS) function \(f()=_{i=1}^{10}{(x_{i}-0.5)^{2}}\), using 20 restarts seeded from 1024 pseudo-random samples through BoTorch's default initialization heuristic. Figure 2 shows that due to vanishing gradients, EI is unable to make progress even on this trivial problem.

In Figure 3, we compare performance on the Ackley and Michalewicz test functions . Notably, LogEI substantially outperforms EI on Ackley as the dimensionality increases. Ackley is a challenging multimodal function for which it is critical to trade off local exploitation with global exploration, a task made exceedingly difficult by the numerically vanishing gradients of EI in a large fraction of the search space. We see a similar albeit less pronounced behavior on Michalewicz, which reflects the fact that Michalewicz is a somewhat less challenging problem than Ackley.

Figure 2: Regret and EI acquisition value for the candidates selected by maximizing EI and LogEI on the convex Sum-of-Squares problem. Optimization stalls out for EI after about 75 observations due to vanishing gradients (indicated by the jagged behavior of the acquisition value), while LogEI continues to make steady progress.

BO with Black Box ConstraintsFigure 4 shows results on four engineering design problems with black box constraints that were also considered in . We apply the same bilog transform as the trust region-based SCBO method  to all constraints to make them easier to model with a GP. We see that LogCEI outperforms the naive CEI implementation and converges faster than SCBO. Similar to the unconstrained problems, the performance gains of LogCEI over CEI grow with increasing problem dimensionality and the number of constraints. Notably, we found that for some problems, LogCEI in fact _improved upon some of the best results quoted in the original literature_, while using three orders of magnitude fewer function evaluations, see Appendix D.7 for details.

Parallel Expected Improvement with qLogEIFigure 5 reports the optimization performance of parallel BO on the 16-dimensional Ackley function for both sequential greedy and joint batch optimization using the fat-tailed non-linearities of App. A.4. In addition to the apparent advantages of qLogEI over qEI, a key finding is that jointly optimizing the candidates of batch acquisition functions can yield highly competitive optimization performance, see App. D.3 for extended results.

High-dimensional BO with qLogEIFigure 6 shows the performance of LogEI on three high-dimensional problems: the \(6\)-dimensional Hartmann function embedded in a \(100\)-dimensional space, a \(100\)-dimensional rover trajectory planning problem, and a \(103\)-dimensional SVM hyperparameter tuning problem. We use a \(103\)-dimensional version of the \(388\)-dimensional SVM problem considered by Eriksson and Jankowiak , where the \(100\) most important features were selected using Xgboost.

Figure 4: Best feasible objective value as a function of number of function evaluations (iterations) on four engineering design problems with black-box constraints after an initial \(2d\) pseudo-random evaluations.

Figure 3: Best objective value as a function of iterations on the moderately and severely non-convex Michalewicz and Ackley problems for varying numbers of input dimensions. LogEI substantially outperforms both EI and GIBBON, and this gap widens as the problem dimensionality increases. JES performs slightly better than LogEI on Ackley, but for some reason fails on Michalewicz. Notably, JES is almost two orders of magnitude slower than the other acquisition functions (see Appendix D).

Figure 6 shows that the optimization exhibits varying degrees of improvement from the inclusion of qLogEI, both when combined with SAASBO  and a standard GP. In particular, qLogEI leads to significant improvements on the embedded Hartmann problem, even leading BO with the canonical GP to ultimately catch up with the SAAS-prior-equipped model. On the other hand, the differences on the SVM and Rover problems are not significant, see Section 6 for a discussion.

Multi-Objective optimization with qLogEHVIFigure 7 compares qLogEHVI and qEHVI on two multi-objective test problems with varying batch sizes, including the real-world-inspired cell network design for optimizing coverage and capacity . The results are consistent with our findings in the single-objective and constrained cases: qLogEHVI consistently outperforms qEHVI and even JES  for all batch sizes. Curiously, for the largest batch size and DTLZ2, qLogNEHVI's improvement over the reference point (HV \(>0\)) occurs around three batches after the other methods, but dominates their performance in later batches. See Appendix D.5 for results on additional synthetic and real-world-inspired multi-objective problems such as the laser plasma acceleration optimization , and vehicle design optimization 

## 6 Discussion

To recap, EI exhibits vanishing gradients 1) when high objective values are highly concentrated in the search space, and 2) as the optimization progresses. In this section, we highlight that these conditions are not met for all BO applications, and that LogEI's performance depends on the surrogate's quality.

On problem dimensionalityWhile our experimental results show that advantages of LogEI generally grow larger as the dimensionality of the problem grows, we stress that this is fundamentally due to the concentration of high objective values in the search space, not the dimensionality itself. Indeed, we have observed problems with high ambient dimensionality but low intrinsic dimensionality, where LogEI does not lead to significant improvements over EI, e.g. the SVM problem in Figure 6.

Figure 5: Best objective value for parallel BO as a function of the number evaluations for single-objective optimization on the 16-dimensional Ackley function with varying batch sizes \(q\). Notably, joint optimization of the batch outperforms sequential greedy optimization.

Figure 6: Best objective value as a function of number of function evaluations (iterations) on three high-dimensional problems, including Eriksson and Jankowiak ’s SAAS prior.

On asymptotic improvementsWhile members of the LogEI family can generally be optimized better, leading to higher acquisition values, improvements in optimization performance might be small in magnitude, e.g. the log-objective results on the convex 10D sum of squares in Fig. 2, or only begin to materialize in later iterations, like for \(q=16\) on DTLZ2 in Figure 7.

On model qualityEven if good objective values are concentrated in a small volume of the search space and many iterations are run, LogEI might still not outperform EI if the surrogate's predictions are poor, or its uncertainties are not indicative of the surrogate's mismatch to the objective, see Rover in Fig. 6. In these cases, better acquisition values do not necessarily lead to better BO performance.

Replacing EIDespite these limitation, we strongly suggest replacing variants of EI with their LogEI counterparts. If LogEI were dominated by EI on some problem, it would be an indication that the EI family itself is sub-optimal, and improvements in performance can be attributed to the exploratory quality of randomly distributed candidates, which could be incorporated explicitly.

## 7 Conclusion

Our results demonstrate that the problem of vanishing gradients is a major source of the difficulty of optimizing improvement-based acquisition functions and that we can mitigate this issue through careful reformulations and implementations. As a result, we see substantially improved optimization performance across a variety of modified EI variants across a broad range of problems. In particular, we demonstrate that joint batch optimization for parallel BO can be competitive with, and at times exceed the sequential greedy approach typically used in practice, which also benefits from our modifications. Besides the convincing performance improvements, one of the key advantages of our modified acquisition functions is that they are much less dependent on heuristic and potentially brittle initialization strategies. Moreover, our proposed modifications do not meaningfully increase the computational complexity of the respective original acquisition function.

While our contributions may not apply verbatim to other classes of acquisition functions, our key insights and strategies do translate and could help with e.g. improving information-based [34; 76], cost-aware [51; 66], and other types of acquisition functions that are prone to similar numerical challenges. Further, combining the proposed methods with gradient-aware first-order BO methods [5; 16; 23] could lead to particularly effective high-dimensional applications of BO, since the advantages of both methods tend to increase with the dimensionality of the search space. Overall, we hope that our findings will increase awareness in the community for the importance of optimizing acquisition functions well, and in particular, for the care that the involved numerics demand.

Figure 7: Batch optimization performance on two multi-objective problems, as measured by the hypervolume of the Pareto frontier across observed points. This plot includes JES . Similar to the single-objective case, the LogEI variant qLogEHVI significantly outperforms the baselines.