# Bandits with Ranking Feedback

Davide Maran

Politecnico di Milano

davide.maran@polimi.it &Francesco Bacchiocchi

Politecnico di Milano

francesco.bacchiocchi@polimi.it &Francesco Emanuele Stradi

Politecnico di Milano

francescoemanuele.stradi@polimi.it &Matteo Castiglioni

Politecnico di Milano

matteo.castiglioni@polimi.it &Nicola Gatti

Politecnico di Milano

nicola.gatti@polimi.it &Marcello Restelli

Politecnico di Milano

marcello.restelli@polimi.it

Equal Contribution.

###### Abstract

In this paper, we introduce a novel variation of multi-armed bandits called _bandits with ranking feedback_. Unlike traditional bandits, this variation provides feedback to the learner that allows them to rank the arms based on previous pulls, without quantifying numerically the difference in performance. This type of feedback is well-suited for scenarios where the arms' values cannot be precisely measured using metrics such as monetary scores, probabilities, or occurrences. Common examples include human preferences in matchmaking problems. Furthermore, its investigation answers the theoretical question on how numerical rewards are crucial in bandit settings. In particular, we study the problem of designing _no-regret_ algorithms with ranking feedback both in the _stochastic_ and _adversarial_ settings. We show that, with stochastic rewards, differently from what happens with non-ranking feedback, no algorithm can suffer a logarithmic regret in the time horizon \(T\) in the instance-dependent case. Furthermore, we provide two algorithms. The first, namely DREE, guarantees a superlogarithmic regret in \(T\) in the instance-dependent case thus matching our lower bound, while the second, namely R-LPE, guarantees a regret of \(}()\) in the instance-independent case. Remarkably, we show that no algorithm can have an optimal regret bound in both instance-dependent and instance-independent cases. Finally, we prove that no algorithm can achieve a sublinear regret when the rewards are adversarial.

## 1 Introduction

_Multi-armed bandits_ are well-known sequential decision-making problems where a learner is given a number of arms whose reward is unknown (Lattimore and Szepesvari, 2017). At every round, the learner can pull an arm and observe a realization of the reward associated with that arm, which can be generated _stochastically_(Auer et al., 2002) or _adversarially_(Auer et al., 1995). The central questionin multi-armed bandits concerns how to address the _exploration/exploitation_ tradeoff to minimize the _regret_ between the reward provided by the _learning policy_ and the optimal _clairvoyant_ algorithm.

In this paper, we introduce a novel variation of multi-armed bandits that, to the best of our knowledge, is unexplored so far. We name the model as _bandits with ranking feedback_. This feedback provides the learner with a partial observation over the rewards given by the arms. More precisely, the learner can rank the arms based on the previous pulls they experienced, but they cannot quantify numerically the difference in performance. Thus, the learner is not allowed to asses how much an arm is better or worse than another. This type of feedback is well-suited for scenarios where the arms' values cannot be precisely measured using metrics such as monetary scores, probabilities, or occurrences, and naturally applies to various settings, _e.g._, when dealing with human preferences such as in matchmaking settings among humans and when the scores cannot be revealed for privacy or security reasons. This latter case can be found, _e.g._, in online advertising platforms offering automatic bidding services as they have no information on the actual revenue of the advertising campaigns since the advertisers prefer not to reveal these values being sensible data for the companies. Notice that a platform can observe the number of clicks received by an advertising campaign, but it cannot observe the revenue associated with that campaign. Remarkably, our model poses the interesting theoretical question whether the lack of numerical scores precludes the design of sublinear regret algorithms or worsens the regret bounds that are achievable when numerical scores are available.

Original contributions.We study the problem of designing _no-regret_ algorithms for the bandits with ranking feedback problem, both in _stochastic_ and _adversarial_ settings. In the case of adversarial rewards, we prove that no algorithm can achieve sublinear regret. In contrast, with stochastic rewards, we show that the ranking feedback does not preclude such a possibility. In particular, in the instance-dependent case, we show that no algorithm can achieve logarithmic regret in the time horizon, and we provide an algorithm, namely DREE (Dynamical Ranking Exploration-Exploitation), guaranteeing a regret bound that matches this lower bound. In the instance-independent case, a crucial question is whether there exists an algorithm providing a better regret bound compared to the one achieved by the well-known Explore-then-Commit algorithm, which trivially guarantees an \((T^{2/3})\) regret upper bound. We positively answer this question by designing an algorithm, namely R-LPE (Ranking Logarithmic Phased Elimination), which guarantees a regret of \(}()\) in the instance-independent case if the rewards are Gaussian. To achieve this result, we derive several non-standard results that allow us to discretize Brownian motions, which are of independent interest. These two different approaches leave open the problem of whether there exists an algorithm achieving optimal performance in both the instance-dependent and instance-independent cases. We negatively answer this question by showing that no algorithm can achieve an optimal regret bound in both cases, confirming the need to design two distinct algorithms for the two cases. Finally, we numerically evaluate our DREE and R-LPE algorithms in a testbed, and we compare their performance with some baselines from the literature in different settings. We show that our algorithms dramatically outperform the baselines in terms of empirical regret.

Related works.The field most related to bandits with ranking is _preference learning_, which aims at learning the preferences of one or more agents from some observations (Furnkranz and Hullermeier, 2010). Let us remark that preference learning has recently gained a lot of attention from the scientific community, as it enables the design of AI artifacts capable of interacting with human-in-the-loop (HTL) environments. Indeed, human feedback may be quite misleading when it is asked to report numerical values, while humans are far more effective at reporting ranking preferences. The preference learning literature mainly focuses on two kinds of preference observations: pairwise preferences and ranking. In the first case, the data observed by the learner involves preferences between two objects, _i.e._, a partial preference is given to the learner. In the latter, a complete ranking of the available data is given as feedback. Our work belongs to the latter branch. Preference learning has been widely investigated by the online learning community, see, _e.g._, (Bengs et al., 2018).

Precisely, our work presents several similarities with the _dueling bandits_ settings (Yue et al., 2012; Saha and Gaillard, 2022; Lekang and Lamperski, 2019), where, in each round, the learner pulls two arms and observes a ranking over them. Nevertheless, although dueling bandits share similarities to our setting, they present substantial differences. Specifically, in our model, the learner observes a ranking depending on the arms they have pulled so far. In dueling bandits, the learner observes an instantaneous comparison between the arms they have just pulled; thus, the outcome of such a comparison does not depend on the arms previously selected, as is the case of bandits with rankingfeedback. As a consequence, while in bandits with ranking feedback the goal of the learner is to exploit the arm with the highest mean, in dueling bandits the goal of the learner is to select the arm winning with the highest probability. Furthermore, while we adopt the classical notion of regret used in the bandit literature to assess the theoretical properties of our algorithms, in dueling bandits, the algorithms are often evaluated with a suitable notion of regret, which differs from the classical one.

Dueling bandits have their reinforcement learning (RL) counterpart in the _preference-based reinforcement learning_ (PbRL), see, _e.g._, (Novoseller et al., 2019) and (Wirth et al., 2017). Interestingly, PbRL techniques differ from the standard RL approaches in that they allow an algorithm to learn from non-numerical rewards; this is particularly useful when the environment encompasses human-like entities (Chen et al., 2022). Furthermore, _preference-based reinforcement learning_ provides a bundle of results, ranging from theory (Xu et al., 2020) to practice (Christiano et al., 2017; Lee et al., 2021). In PbRL, preferences may concern both states and actions; contrariwise, our framework is stateless since the rewards gained depend only on the action taken during the learning dynamic. Moreover, the differences outlined between dueling bandits and bandits with ranking feedback still hold for preference-based reinforcement learning, as preferences are considered between observations instead of the empirical mean of the accumulated rewards.

Paper structure.The paper is structured as follows. In Section 2, we report the problem formulation, the setting, and the necessary notation. In Section 3, we study the stochastic setting, that is, when the rewards are sampled from a fixed distribution. In Section 3.1, we present an instance-dependent regret lower bound that characterizes our problem. Thus, in Section 3.2, we propose our algorithm and we show that it achieves a tight instance-dependent regret bound. In Section 3.3, we show a trade-off between the regret upper bound achievable by any algorithm in the instance-dependent and instance-independent cases. Finally, in Section 3.4, we present an algorithm achieving an optimal instance-independent regret bound when the rewards are Gaussian. In Section 4, we study the adversarial setting, that is, when no statistical assumptions are made about the rewards. In this case, we present our impossibility result, showing that no algorithm can achieve sublinear regret. In Appendix A and Appendix B, we report the omitted proofs for the stochastic setting. In Appendix C, we report the omitted proof for the adversarial setting. Finally, in Appendix D, we report the empirical evaluation of our algorithms.

## 2 Problem formulation

In this section, we formally state the model of bandits with ranking feedback and discuss the learner-environment interaction. Subsequently, we define policies and the notion of regret both in the _stochastic_ and in the _adversarial_ setting.

Setting and interaction.Differently from the classical version of the multi-armed bandit problem--see, _e.g._, (Lattimore and Szepesvari, 2017)--in which the learner observes the _reward_ associated with the pulled arm, in bandits with ranking feedback the learner can only observe a _ranking_ over the arms based on the previous pulls. Formally, we assume the learner-environment interaction to unfold as follows.2

1. At every round \(t[T]\), where \(T\) is the time horizon, the learner chooses an arm \(i_{t}[n]\), where \(\) is the set of available arms and \(n=||<+\).
2. We study both stochastic and adversarial settings. In the stochastic setting, the environment draws the reward \(r_{t}(i_{t})\) associated with arm \(i_{t}\) from a probability distribution \(_{i_{t}}\), _i.e._, \(r_{t}(i_{t})_{i_{t}}\), whereas, in the adversarial setting, \(r_{t}(i_{t})\) is chosen adversarially by an opponent from a bounded set of reward functions.
3. There is a bandit feedback on the reward of the arm \(i_{t}\) pulled at round \(t\) leading to the estimate of the empirical mean of \(i_{t}\) as follows: \[_{t}(i)_{t}(i)}r_{j}(i)}{Z_{i}(t)},\]where \(_{t}(i)\{[t] i_{}=i\}\) and \(Z_{i}(t)|_{t}(i)|\).3 The learner observes the ranking over the empirical means \(\{_{t}(i)\}_{i}\). Formally, we assume that the ranking \(_{t}_{}\) observed by the learner at round \(t\) is such that:

\[_{t}(_{t,i})_{t}(_{t,j})\;\; t [T]\; i,j[n]i j,\]

where \(_{t,i}\) denotes the \(i\)-th element in the ranking \(_{t}\) at round \(t[T]\). Ties are broken in favor of the lower index.

For the sake of clarity, we provide an example to illustrate our setting and the corresponding learner-environment interaction.

Example.We consider an instance with two arms, _i.e._, \(=\{1,2\}\), where the learner plays the first arm at rounds \(t=1\) and \(t=3\), and the second arm at round \(t=2\), such that \(_{3}(1)=\{1,3\}\) and \(_{3}(2)=\{2\}\). Let \(r_{1}(1)=1\) and \(r_{3}(1)=5\) be the rewards obtained from playing the first arm at rounds \(t=1\) and \(t=3\), respectively, and let \(r_{2}(2)=5\) be the reward for playing the second arm at round \(t=2\). The empirical means of the two arms and the resulting rankings at each round \(t\) are given by:

\[_{t}(1)=1,\,_{t}(2)=0\;\;_{t}= 1,2&t=1\\ _{t}(1)=1,\,_{t}(2)=5\;\;_{t}= 2,1&t=2\\ _{t}(1)=3,\,_{t}(2)=5\;\;_{t}= 2,1&t=3.\]

Policies and regret.At every round \(t\), the arm played by the learner is prescribed by a policy \(\). In both the stochastic and adversarial settings, we let the policy \(\) be a randomized map from the history of the interaction \(H_{t-1}=(_{1},i_{1},_{2},i_{2},,_{t-1}, i_{t-1})\) to the set of all probability distributions with support \(\). Formally, we let \(:H_{t-1}()\), for \(t[T]\), such that \(i_{t}(H_{t-1})\). As is customary in stochastic bandits, the learner's goal is to design a policy \(\) that minimizes the cumulative expected regret, which is formally defined as follows:

\[R_{T}()=[_{t=1}^{T}r_{t}(i^{*})-r_{t}(i_{t})],\]

where the expectation in the definition of regret is taken over the randomness of both the policy and the environment, and we define \(i^{*}_{i}_{i}\) with \(_{i}[_{i}]\) for each \(i[n]\). In contrast, in the adversarial setting, the regret is defined as \(R_{T}()=_{t=1}^{T}r_{t}(i^{*})-r_{t}(i_{t})\) and we define \(i^{*}_{i}_{t=1}^{T}r_{t}(i)\). Furthermore, from here on, we omit the dependence on the policy selected by the learner \(\) in the regret formulation, referring to \(R_{T}()\) as \(R_{T}\) whenever it is clear from the context. In the stochastic setting, we introduce the following additional notation. Given an arm \(i[n]\), we let \(_{i}_{i^{*}}-_{i}\) represent the suboptimality gap of that arm. Furthermore, when \(n=2\), we simply refer to the suboptimality gap as \(\).

As we will further discuss, the impossibility of observing the reward realizations raises several technical challenges when designing no-regret algorithms, as the approaches adopted for standard (non-ranking) bandits may be challenging to apply within our framework. In the following sections, we discuss how the lack of this information degrades the performance of the algorithms when the feedback is provided as a ranking.

## 3 Analysis in the stochastic setting

As a preliminary observation, we note that optimistic approaches, such as the UCB1 algorithm, are challenging to apply within our framework. This is because the learner lacks the information to estimate the expected rewards of the different arms, making it difficult to infer confidence bounds. Therefore, the most popular algorithm one can employ in bandits with ranking feedback is the _explore-then-commit_ (EC) algorithm (see, _e.g._, Auer et al. (2002)), in which the learner explores the arms uniformly during the initial rounds and then commits to the one with the highest empirical mean. However, as we will also discuss in the following, such algorithm achieve suboptimal regret guarantees. Thus, in the rest of this section, we present two algorithms specifically designed to achieve optimal instance-dependent and instance-independent regret bounds.

### Instance-dependent lower bound

In the classical multi-armed bandit problem, it is well known that it is possible to achieve an instance-dependent regret bound that is logarithmic in the time horizon \(T\). However, in this section, we show that such a result does not hold when the feedback is provided as a ranking. Our impossibility result leverages a connection between random walks and the cumulative rewards of the arms. Formally, we define an (asymmetric) random walk as follows.

**Definition 1**.: _A random walk is a stochastic process \(\{G_{t}\}_{t}\) such that:_

\[G_{t}=0&t=0\\ G_{t-1}+_{t}&t 1,\]

_where \(\{_{t}\}_{t}\) is an i.i.d. sequence of integrable random variables, and \([_{t}]=p\) is the drift of the random walk._

Before introducing our negative result, we introduce a lemma that characterizes the average performance of two random walks with different drifts.

**Lemma 1** (Separation lemma).: _Let \(\{G_{t}\}_{t},\{G^{}_{t}\}_{t}\) be two independent random walks defined as:_

\[G_{t+1}=G_{t}+_{t} G^{}_{t+1}=G^{}_ {t}+^{}_{t},\]

_where \(G_{0}=G^{}_{0}=0\) and the drifts satisfy \([_{t}]=p>q=[^{}_{t}]\), for each \(t\). Then, we have:_

\[ t,t^{}^{*}\ \ G_{t}/t G^{ }_{t^{}}/t^{} c(p,q)>0.\]

We remark that the exact value of the constant \(c(p,q)\) in Lemma 1 depends only on the two drifts \(p\) and \(q\), as well as the probability distribution defining these drifts. In the simpler case of Bernoulli distributions, the constant \(c(p,q)\) can be derived in closed form, as shown in Lemma 3 in the appendix. The rationale behind Lemma 1 is that, when considering two random walks with different drifts, there exists a _separating line_ between them with strictly positive probability. Thus, with non-negligible probability, the empirical mean of the process with the higher drift always upper bounds the empirical mean of the process with the lower drift.

We also observe that the cumulative reward collected by a specific arm during the learning process can be represented as a random walk, whose drift corresponds to the expected reward associated with that arm. In the classical version of the multi-armed bandit problem, the learner can fully observe the evolution of this random walk, while in our setting, the learner can only observe a ranking of the different arms, making it impossible to quantify their performance numerically. Therefore, in bandits with ranking feedback, the only way for the learner to assess how close two arms are in terms of expected reward is by observing subsequent switches in their positions within the ranking. However, Lemma 1 implies that even if two arms have very close expected rewards, the learner may never observe a switch in the ranking, as the average mean of the arm with the higher expected reward may upper bound the second throughout the entire learning process. As a result, the following negative result holds.

**Theorem 1** (Instance-dependent lower bound).: _Let \(\) be any policy for the bandits with ranking feedback problem, then, for any \(C:\ [0,+)[0,+)\), there exists a \(>0\) and a time horizon \(T>0\) such that \(R_{T}>C()(T)\)._

### Instance-dependent upper bound

We introduce the Dynamical Ranking Exploration-Exploitation algorithm (DREE). The pseudo-code is provided in Algorithm 1. As usual in bandit algorithms, in the first \(n\) rounds, a pull for each arm is performed (Lines 2-3). At every subsequent round \(t>n\), the exploitation/exploration trade-off is addressed by playing the best arm according to the received feedback unless there is at least one arm whose number of pulls at \(t\) is smaller than a superlogarithmic function \(f(t):(0,)_{+}\).4 More precisely, the algorithm plays an arm \(i\) at round \(t\) if it has been pulled less than \(f(t)\) times(Lines 4-5), where ties due to multiple arms pulled less than \(f(t)\) times are broken arbitrarily. Instead, if all arms have been pulled at least \(f(t)\) times, the arm in the highest position of the last ranking feedback is pulled (Lines 6-8). Each round terminates with the learner receiving the updated ranking over the arms as feedback (Line 9). Let us observe that the exploration strategy of Algorithm 1 is deterministic, and the only source of randomness concerns the realization of the arms' rewards.

We state the following result, providing the regret upper bound of Algorithm 1 as a function of \(f\).

**Theorem 2** (Instance-dependent upper bound).: _Assume that the reward distribution of every arm is \(1\)-subgaussian. Let \(f:(0,)_{+}\) be a superlogarithmic nondecreasing function in \(t\). Then there is a term \(C(f,_{i})\) for each sub-optimal arm \(i[n]\) which does not depend on \(T\), such that Algorithm 1 satisfies \(R_{T}(1+f(T))_{i=1}^{n}_{i}+(T)_{i=1}^{n}C(f,_{i})\)._

To minimize the asymptotic dependence on \(T\) of the cumulative regret suffered by the algorithm, we can choose as an example \(f(t)=(t)^{1+}\), where the parameter \(>0\) is as small as possible. However, if \(_{i}<1\), the minimization of \(\) comes at the cost of increasing the term \(C(f,_{i})\) as it grows exponentially as \(\) goes to zero. In particular, the terms \(C(f,_{i})\) are formally defined in the following corollary.

**Corollary 1**.: _Let \(>0\) and \(f(t)=(t)^{1+}\) be the sperlogarithmic function used in Algorithm 1, then we have:_

\[C(f,_{i})=\ (e^{((2/_{i}^{2} )^{1/})}+1)}{1-e^{-_{i}^{2}/2}}.\]

We remark that the term \(C(f,_{i})\) depends exponentially on \(_{i}\), suggesting that \(C(f,_{i})\) may be large even when adopting values of \(\) that are not arbitrarily close to zero. At this point, a natural question arises. Is such an exponential dependence with respect to the suboptimality gaps unavoidable, or is it a consequence of the kind of feedback the learner receives? In the next section, we answer this question.

Furthermore, let us observe that Algorithm 1 satisfies important properties. More precisely, (i) it matches the instance-dependent regret lower-bound, since \(f()\) can be chosen arbitrarily close to \((t)\), (ii) it works without requiring the knowledge of the time horizon \(T\), thus being an _any-time algorithm_.

### Instance dependent/independent trade-off

In this section, we provide a negative result, showing that _no algorithm_ can achieve good performance in terms of both instance-dependent and instance-independent regret bounds, suggesting that the two cases need to be studied separately. Formally, the following theorem holds.

**Theorem 3** (Instance Dependent/Independent Trade-off).: _Let \(\) be any policy for the bandits with ranking feedback problem. If \(\) satisfies the following properties:_

* _(instance-dependent regret upper bound)_ \(R_{T}_{i=1}^{n}C(_{i})T^{}\)__
* _(instance-independent regret upper bound)_ \(R_{T} nT^{}\)__

_then, \(2+ 1\), where \(, 0\)._

Proof.: (Sketch) Let \(p_{1}=0.5\), \(p_{2}=0.5-\), \(p_{2}^{*}=0.5+\), for some \(>0\) specified in the proof. We consider three instances:

\[:\ _{1}=Be(p_{1})\\ _{2}=Be(p_{2})^{*}:\ _{1}=Be(p_{1 })\\ _{2}=Be(p_{2}^{*})^{**}:\ _{1}=Be(1)\\ _{2}=Be(0)\]Clearly, the first arm is optimal in instances \(,^{**}\), while the second arm is optimal in \(^{*}\). We then define the event:

\[E_{t}=_{=1}^{t}\{_{t}= 1,2\}.\]

In the first and in the third instances, the event \(E_{t}\) corresponds to the optimal arm being ranked in the first position for the first \(t\) time steps, while the opposite holds for the second instance. We observe the following two key points. (1) Since the event \(E_{t}\) contains the realization of all the rankings up to time \(t\), no policy can distinguish between the three instances until this event holds. Therefore, we have:

\[_{}[Z_{2}(t)|E_{t}]=_{^{*}}[Z_{2}(t)| E_{t}]=_{^{**}}[Z_{2}(t)|E_{t}].\]

(2) In instance \(^{**}\), the event \(E_{t}\) happens almost surely for every \(t\). Therefore, to ensure that the policy \(\) achieves an instance-dependent regret bounded by \(C(1)T^{}\) in this instance, we need \([Z_{2}(T)|E_{T}] CT^{}\) in all three instances.

The main question at this point reduces to: are \(CT^{}\) pulls of the last-ranking arm enough to distinguish between the two instances \(\) and \(^{*}\)? With a change of measure argument _restricted to the first \(CT^{}\) pulls of the last-ranking arm_, we are able to show that, for a sufficiently small value of \(>0\), distinguishing between \(,^{*}\) is impossible with strictly positive probability. Then, we can prove that if the previous consideration holds, in the instance \(^{*}\), we have:

\[R_{T}(T^{1-2}),\]

for a constant \(>0\) close to one. This lower bound on the instance-independent regret entails that \( 1-2\), or, equivalently \(+2 1\). 

From Theorem 3, we can easily infer the following impossibility result.

**Corollary 2**.: _There is no policy \(\) for the bandits with ranking feedback problem achieving both subpolynomial regret in the instance-dependent case, i.e., for every \(>0,\) there exists a function \(C()\) such that \(R_{T}_{i=1}^{n}C(_{i})T^{},\) and sublinear regret in the instance-independent case._

To ease the interpretation of Corollary 2, in the following result, we discuss the instance-independent regret bound achieved by Algorithm 1.

**Corollary 3**.: _For every choice of \(>0\) in \(f(t)=(t)^{1+}\), there is no value of \(>0\) for which Algorithm 1 achieves an instance-independent regret bound of the form \(R_{T}(T^{1-})\)._

The above result shows that Algorithm 1 suffers from linear instance-independent regret in \(T\), except for logarithmic terms. Moreover, the following corollary of Theorem 3 answers the question raised in the previous section. Indeed, we can prove that the unpleasant dependence on the suboptimality gaps \(_{i}\) is not a feature of Algorithm 1; instead, it cannot be avoided until the instance-dependent regret has a good order in \(T\).

**Corollary 4**.: _Let \(\) be any policy for the bandits with ranking feedback problem that satisfies and instance-dependent regret upper bound of the form \(R_{T}_{i=1}^{n}C(_{i})f(T),\) where \(f()\) is a subpolynomial function. Then, \(C()\) is super-polynomial in \(1/\)._

Proof.: We prove the opposite implication, namely that if \(C()\) is polynomial in \(1/\), then the instance-independent regret bound cannot be subpolynomial in \(T\). By assumption, in case of two arms with just one gap \(\), we have:

\[R_{T}_{=1}^{p}C_{}^{-}f(T),\]

which implies that the instance-independent regret can be bounded in the following way

\[R_{T}_{>0}\{_{=1}^{p}C_{}^{-}f(T), T\}_{=1}^{p}_{>0}\{C_{} ^{-}f(T), T\}.\]Notice that, for \( T^{-1/(+1)}\) the first term is less than \(CT^{}f(T)\), while for \( T^{-1/(+1)}\), the second one is less than \(T^{}\). Therefore, the full instance-independent regret is bounded by:

\[R_{T}_{=1}^{p}C_{}T^{}f(T),\]

which is polynomial in \(T\). If, by contradiction, \(f(T)\) were subpolynomial, this bound would be sublinear in \(T\), but this contradicts the result of Corollary 2. 

### Instance-independent upper bound

The impossibility result stated in Corollary 2 pushes for the need for an algorithm guaranteeing sublinear regret in the instance-independent case. Initially, we observe that the standard Explore-then-Commit (EC) algorithm  can be applied within our framework, achieving an \((T^{2/3})\) instance-independent regret bound. In the following, we provide a brief overview of how the EC algorithm works. It divides the time horizon into two phases as follows: (i) _exploration phase_: the arms are pulled uniformly for the first \(m n\) rounds, where \(m\) is a parameter of the algorithm one can tune to minimize the regret; (ii) _commitment phase_: the arm maximizing the estimated reward is pulled. In the case of bandits with ranking feedback, the EC algorithm explores the arms in the first \(m n\) rounds and subsequently pulls the arm in the first position of the ranking feedback received at round \(t=m n\). As is customary in standard (non-ranking) bandits, the best regret bound can be achieved by setting \(m= T^{2/3}\), thus obtaining an \((T^{2/3})\) regret upper bound. We show that we can get a regret bound better than that of the EC algorithm. In particular, we provide the Ranking Logarithmic Phased Elimination (R-PLE) algorithm, which breaks the barrier of \((T^{2/3})\) guaranteeing an \(}()\) regret bound when neglecting logarithmic terms. Due to the mathematical instruments involved, the proof of this regret bound only holds for the case of Gaussian rewards, as the ones presented in a similar setting by Garivier et al. . The pseudocode of R-PLE is reported in Algorithm 2.

```
1:initialize \(S=[n]\)
2:initialize \(=L(1/2,1,T)\)
3:for\(t[T]\)do
4: play \(i_{t}*{arg\,min}_{i S}Z_{i}(t)\)
5: update \(Z_{i_{t}}(t)\) number of times \(i_{t}\) has been pulled
6: observe ranking \(_{t}\)
7:if\(_{i S}Z_{i}(t)\)then
8:\(=Z_{i}(t))}{(T)}-\)
9:\(S=_{t}(T^{2})\)
10:endif
11:endfor
```

**Algorithm 2** Ranking Logarithmic Phased Elimination (R-LPE)

In order to proper analyze the algorithm, we need to introduce the two following definitions. Initially, we introduce the definition of the loggrid set as follows.

**Definition 2** (Loggrid).: _Given \(a,b\) s.t \(a<b\) and a constant value \(T>0\), we define:_

\[(a,b,T):=\{ T^{_{j}b+(1-_{j})a}:\; _{j}=,\; j=0,, (T)\}.\]

Next, we give the notion of active set, which the algorithm employs to cancel out sub-optimal arms.

**Definition 3** (Filtering condition).: _Let \(S\) be the active set of the algorithm, at a certain timestep. We say that a timestep \(t[T]\) is **fair** if all active arms have been pulled the same number of times times. In any fair timestep \(t[T]\), we define the active set \(_{t}()\) as the set of arms such that:_

\[_{t}():=\{i S: j S\;_{=1\;}^{t}\{_{}(i)>_{}(j) \}\}.\]

_This condition will be called **filtering condition**._

Initially, we observe that R-LPE differs from Algorithm 1, as it takes into account the entire history of the process, not just the most recent ranking \(_{t}\). It also requires knowledge of \(T\). We denote with \(S\) the set of active arms used by the algorithm. Initially, the set \(S\) comprises all the possible arms available in the problem (Line 1). Furthermore, the set which drives the update of the decision space \(S\), namely \(\), is initialized as the loggrid built on parameters \(1/2,1,T\) (Line 2). At every round \(t[T]\), R-LPE chooses the arm from the active set \(S\) with the minimum number of pulls, namely \(i[n]\) s.t. \(Z_{i}(t)\) is minimized (Line 4); ties are broken by index order. Next, the number of times arm \(i_{t}\) has been pulled, namely \(Z_{i_{t}}(t)\), is updated accordingly (Line 5). The peculiarity of the algorithm is that the set \(S\) changes every time the condition \(_{i}Z_{i}(t)\) is satisfied (Line 7). When the aforementioned condition is met, the set of active arms \(S\) is filtered to avoid the exploration on sub-optimal arms. Precisely, \(S\) is filtered given the time dependent parameter \(\) (Lines 8- 9). We state the following theorem providing an instance-independent regret bound to Algorithm 2.

**Theorem 4**.: _In the stochastic bandits with ranking feedback setting, when the noise is Gaussian, Algorithm 2 achieves \(R_{T} 62n^{4}(T)^{2}T^{1/2}\)._

Proof.: (Sketch) To prove the theorem, we define, for every pair of indices \(i,j[n]\), the event:

\[E^{}_{ij}:=E_{ij}&_{j}-_{i}>\\ &, \]

where \(E_{ij}\) corresponds to the event in which arm \(i\) eliminates arm \(j\) at some point in the process, while \(\) is a constant defined in the following. The probability that at least one of these events holds is bounded (by Lemma 6 and employing a union bound) as \(():=(_{i j,i,i[n]}^{n}E^{}_{ij} )(n^{2}(T)^{2}T^{-1/2}^{-1}),\) since their number is at most \(n(n-1)/2\).

Therefore, if the complement of the event \(\) holds, there exists an arm \(i^{}\) with a gap (w.r.t. the first arm) less than \((n-1)\), which is not eliminated until the last round. This is because at most \(n-1\) eliminations can happen, and if the complement of the event \(\) holds, such eliminations concern pairs of arms with a difference in mean of at most \(\).

Since the arm \(i^{}[n]\) is not eliminated until the last round under the event \(^{C}\), the probability of event \(E^{}_{ii^{}}\), corresponding to the event in which the suboptimal arm \(i[n]\) survives for more than \(((T)T^{1/2}_{ii^{}}^{-1})\) pulls, is bounded by \(2T^{-1/2}\) thanks to Lemma 7. As a result, employing a union bound over all possible values of \(i^{}\), we can say that the probability that any event \(E^{}_{ii^{}}\) with \(i^{}[n]\) occuring is at most \(2(n-1)T^{-1/2}\). Thus, fixing \(=_{i}/(2(n-1))\) ensures that \(_{ii^{}}^{-1}_{i}/n\), which entails \((Z_{i}(T)((T)T^{1/2}_{i}^{-1}))(n^{3}(T)^{2}T^{-1/2}_{i}^{-1})),\) and thus, it holds \([_{i}Z_{i}(T)]=O(n^{3}(T)^{2}T^{-1/2})\). Finally, using the Regret Decomposition Lemma [Lattimore and Szepesvari, 2017] we can conclude the proof. Proving the two lemmas, however, is not trivial since it requires to see the whole process as a discretization of a biased Brownian motion, and then applying results for this kind of stochastic processes. 

At first glance, the result presented in Theorem 4 may seem unsurprising. Indeed, there are several elimination algorithms achieving \(()\) regret bounds in different bandit settings (see, for example, ). Nevertheless, our setting poses several additional challenges compared to existing ones. For instance, in our framework, it is not possible to rely on concentration bounds, as the current feedback is heavily correlated with the past ones. This is precisely the reason for the anomalous growth of the regret in terms of the number of arms \(n\): due to the extremely correlated feedback, two union bounds are necessary trough the last proof, and this leads to an increase in the dependence of the order of \(n\). In the impossibility of using concentration inequalities, our analysis employs novel arguments, drawing from recent results in the theory of Brownian Motions, which allow to properly model the particular ranking feedback.

## 4 Analysis in the adversarial setting

We focus on bandits with ranking feedback in adversarial settings. In particular, we show that no algorithm provides sublinear regret without statistical assumptions on the rewards.

**Theorem 5**.: _In adversarial bandits with ranking feedback, there exists a constant \((0,1)\) such that no algorithm achieves \(o(T)\) regret with respect to the best arm in hindsight with probability greater than \(\)._Proof.: (Sketch) The proof introduces three instances in an adversarial setting in a way that no algorithm can achieve sublinear regret in all three. The main reason behind such a negative result is that ranking feedback obfuscates the value of the rewards so as not to allow the algorithm to distinguish two or more instances where the rewards are non-stationary. The three instances employed in the proof are divided into three phases such that the instances are similar in terms of rewards for the first two phases, while they are extremely different in the third phase. In summary, if the learner receives the same ranking when playing in two instances with different best arms in hindsight, it is not possible to achieve a small regret in both of them.