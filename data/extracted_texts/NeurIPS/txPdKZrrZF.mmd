# Fed-FA: Theoretically Modeling Client Data Divergence for Federated Language Backdoor Defense

Zhiyuan Zhang1,2, Deli Chen2, Hao Zhou2, Fandong Meng2, Jie Zhou2, Xu Sun1

1National Key Laboratory for Multimedia Information Processing,

School of Computer Science, Peking University

2Pattern Recognition Center, WeChat AI, Tencent Inc., China

{zzy1210,xusun}@pku.edu.cn

{delichen,tuxzhou,fandongmeng,withtomzhou}@tencent.com

###### Abstract

Federated learning algorithms enable neural network models to be trained across multiple decentralized edge devices without sharing private data. However, they are susceptible to backdoor attacks launched by malicious clients. Existing robust federated aggregation algorithms heuristically detect and exclude suspicious clients based on their parameter distances, but they are ineffective on Natural Language Processing (NLP) tasks. The main reason is that, although text backdoor patterns are obvious at the underlying dataset level, they are usually hidden at the parameter level, since injecting backdoors into texts with discrete feature space has less impact on the statistics of the model parameters. To settle this issue, we propose to identify backdoor clients by explicitly modeling the data divergence among clients in federated NLP systems. Through theoretical analysis, we derive the f-divergence indicator to estimate the client data divergence with aggregation updates and Hessians. Furthermore, we devise a dataset synthesization method with a Hessian reassignment mechanism guided by the diffusion theory to address the key challenge of inaccessible datasets in calculating clients' data Hessians. We then present the novel Federated F-Divergence-Based Aggregation (**Fed-FA**) algorithm, which leverages the f-divergence indicator to detect and discard suspicious clients. Extensive empirical results show that Fed-FA outperforms all the parameter distance-based methods in defending against backdoor attacks among various natural language backdoor attack scenarios.

## 1 Introduction

Federated learning can train neural network models across multiple decentralized edge devices (i.e. clients) in a privacy-protect manner. However, federated aggregation algorithms (_e.g._ FedAvg ) are vulnerable to backdoor attacks  from malicious clients via poisonous parameter updating . This poses a serious threat to the security and reliability of federated learning systems. Therefore, detecting suspicious backdoor clients is of great research significance .

Most existing robust federated aggregation algorithms heuristically take parameter distances (e.g. Euclidean distances , cosine distances ) among clients as the indicator to detect suspicious clients. However,  points out that federated language backdoors are harder to defend against than vision backdoors; the reason lies in that the text feature space is discrete and the injected backdoor patterns of text are more hidden at the parameter level than images. For example, the output of NLP models can be falsified by only poisoning a few words' embeddings , which can hardly affect the statistics of the whole model parameters. Thus, the parameter distance-basedrobust federated aggregations [1; 12; 10] do not perform well on NLP tasks. Besides, the choice of distance function is empirical and lacks theoretical guarantees [1; 58; 12; 10].

**Present work.** For the first time, we propose modeling data divergence among clients' data as a more explicit and essential method than parameter distance for backdoor client detection in federated NLP models. A critical challenge for data divergence estimation is that local datasets on clients are invisible to the server or the defender due to privacy protection, thus we cannot measure data divergence directly. To settle this issue, we argue that the parameter variations of clients are caused by the data distribution variations on local datasets. Based on this theoretical intuition, we model how distribution variations between different clients lead to parameter update variations in Theorem 1, and further derive the f-divergence indicator that can estimate the data divergence among different clients with parameter updates and Hessians (_i.e._ second derivatives).

Driven by our theoretical analysis, we propose a novel Federated F-Divergence-Based Aggregation (**Fed-FA**) algorithm, which utilizes the f-divergence indicator to estimate the data divergence of clients. F-divergence is a universal divergence measurement of data distributions that common classic divergences can be seen as special cases with corresponding convex functions \(f(x)\). We utilize the f-divergence indicator to detect suspicious clients that have larger f-divergence indicator values, namely clients whose data distributions are different from others. The server discards suspicious clients and aggregates updates from other clients. The Fed-FA is illustrated in Fig. 1. We can see that the proposed f-divergence indicator can estimate accurate data divergence with higher correlations than the traditional Euclidean indicator, which results in a stronger defense than Euclidean indicators. We also prove Theorem 3 to verify the Byzantine resilience and convergence of Fed-FA.

The calculation of f-divergence indicators involves parameter updates and Hessians. Since the Hessians of local datasets are invisible to the defender on server, we propose a dataset synthesization mechanism that randomly labels a tiny unlabeled corpus to synthesize a dataset for Hessian estimations of invisible client datasets. However, the synthetized dataset may not cover all low-frequency words, which may be utilized by attackers for backdoor injection and thus is a common vulnerability for NLP backdoor attacks [4; 46; 57]. To settle this issue, we reassign Hessians on embeddings; the reassigned scales are derived in Theorem 2, which is guided by the diffusion theory [20; 45] and reveals that the parameter update magnitude is approximately proportional to Hessian square root on embeddings.

In addition to theoretical analysis, we also conduct comprehensive experiments to compare Fed-FA with other robust federated aggregation baselines on four NLP tasks that cover typical poisoning techniques in NLP, _i.e._, EP [46; 51], BadWord , BadSent , and Hidden (HiddenKiller) . Our experiments are conducted on three typical neural network architectures in federated language learning , _i.e._, GRU, LSTM, and CNN. Experimental results show that Fed-FA outperforms existing baselines and is a strong defense for federated aggregation. Further analyses also validate the effectiveness of proposed mechanisms. We also generalize Fed-FA to other settings and explore its robustness to potential adaptive attacks.

## 2 Background and related work

In this section, we first introduce NLP backdoor attacks and backdoor defense in centralized learning. Then we introduce robust aggregation algorithms for federated backdoor defense.

### Natural language backdoors and defense in centralized learning

**Natural language backdoor attacks.** Backdoor attacks  are malicious manipulations that control the model's behaviors on input samples containing backdoor triggers or patterns. NLP backdoor attacks usually adopt data poisoning [28; 5] that injects misleading input samples with backdoor patterns and wrong labels into the training dataset.

Generally, NLP backdoor attacks can be divided into three categories according to the backdoor injection patterns: (1) _Trigger word_ based attacks [19; 46; 57; 48; 56]: **BadWord** chooses low-frequency trigger words as the backdoor pattern; and the Embedding Poisoning attack, **EP**[46; 51], only manipulates embedding parameters for better stealthiness; (2) _Trigger sentence_ based attacks: **BadSent**[7; 4] chooses a neutral sentence as the backdoor pattern; (3) _Hidden trigger_ based attacks or dynamic attacks: **Hidden** (HiddenKiller)  converts the input into a specific syntax pattern as the backdoor pattern to make the backdoor triggers difficult to detect, and other attacks also adopt hidden triggers [36; 37], input-aware or dynamic triggers  to hide sophisticated backdoor triggers.

In federated learning, the attacker can conduct these attack techniques on the client and poison the global parameters on the server in the federated aggregation process.

**Natural language backdoor defense.** Backdoor defense in centralized learning defends against the backdoor by detecting and removing the backdoor pattern in input samples [53; 9; 32; 13; 47] or mitigating backdoors in model parameters [49; 21; 59; 22]. We focus on defense algorithms for federated backdoors and introduce them in next subsection.

### Robust federated aggregation

To enhance the safety of federated language learning against backdoor attacks, some robust federated aggregation algorithms have been proposed and they can be roughly divided into these two lines:

**Discarding aggregations.** Discarding robust federated aggregation algorithms detect and exclude suspicious clients, which can act as a stronger defense against NLP backdoors . We also follow this aggregation paradigm. A representative line of discarding aggregations are Byzantine tolerant Krum algorithms, including the **Krum** (initial Krum) , **M-Krum** (multiple Krum) , **Bulyan**, and **Dim-Krum** algorithms. They adopt Euclidean distances of parameter updates empirically, while Fed-FA adopts the f-divergence indicator derived theoretically.

**Non-discarding aggregations.** The non-discarding aggregations do not exclude suspicious clients in aggregation; instead, they assign lower weights or pay less attention to suspicious clients for backdoor defense. For example, **Median**[3; 50] adopts the statistical median of all updates as the aggregated update on each dimension, while **RFA** calculates the geometric median of all clients. Based on RFA, **CRFL** trains certifiably robust federated learning models against backdoors by further adding Gaussian noises and projecting to a constraint set after every round. **FoolsGold** leverages the diversity or similarity of client parameter updates to identify the malicious client. **Residual** (Residual-based defense)  adopts residual-based weights for different clients according to parameter updates and assigns lower weights for suspicious clients for backdoor defense. Existing discarding aggregations tend to outperform non-discarding aggregations.

## 3 Methodology

In this section, we first introduce the federated learning paradigm. Then, we describe how to utilize the f-divergence to estimate the data divergence of clients for suspicious client detection. Implementation of Fed-FA is introduced last. Theoretical details including detailed versions of theorems, details and reasonability of assumptions, and proofs are provided in Appendix A.

### Federated learning paradigm

Suppose \(^{d}\) denotes the parameters of the model. The objective of federated learning is to train a global model \(^{}\) on the server by exchanging model parameters through multiple rounds of communication without exposing the local data of multiple clients. Suppose the number of clients and rounds are \(n\) and \(T\); the global model is initialized with \(_{0}^{}\); \(_{i}^{}\) and \(_{i}^{(i)}\) denote parameters on the server and the \(i\)-th client in the \(t\)-th round (\(1 t T\)). In the \(t\)-th round, the server first distributesthe global parameters \(_{t-1}^{}\) to each client; then clients train \(_{t}^{(i)}\) on its private dataset locally. Then, the server conducts the federated aggregation, namely gathering multiple local parameters \(_{t}^{(i)}\) on all clients and updates the global model to calculate \(_{t}^{}\) with a federated aggregation algorithm.

**Federated aggregation.** Suppose \(_{t}^{(i)}\) denotes the update on the \(i\)-th client in the \(t\)-th round and \(\) aggregates updates on \(n\) clients: \(_{t}^{(i)}=_{t}^{(i)}-_{t-1}^{}\), where \(_{t}^{}=_{t-1}^{}+(\{_{t}^{(i)}\}_{i=1}^{n})\).

We focus on robust federated aggregation in this paper. A series of robust federated aggregation algorithms can be formulated into: \((\{^{(i)}\}_{i=1}^{n})=_{i=1}^{n}w_{i}^{(i)}\), where \(_{i=1}^{n}w_{i}=1\).

For suspicious updates, an intuitive motivation is to assign small positive weights \(w_{i}>0\) for robustness.  reveal that discarding suspicious updates (namely setting \(w_{i}=0\)) can act as a stronger defense than barely assigning small positive weights \(w_{i}>0\) in NLP tasks. Following , we choose a set \(S\) (\(|S|= n/2+1\)) of clients that are not suspected to be poisonous and discard other clients, namely: \(w_{i}=(i S)/|S|\), where \((i S)=1\) for \(i S\), and \(0\) for \(i S\).

### Detecting suspicious clients utilizing proposed f-divergence indicator

To detect suspicious clients, traditional algorithms  intuitively adopt the square of the Euclidian parameter distances, namely Euclidian indicator: \(_{}=\|\|_{2}^{2}=_{k=1}^{d}_{k}^{2}\), where the variation between one client update and the ideal update or averaged update of all clients is \(=[_{1},_{2},,_{d}]^{}\). Traditional algorithms based on parameter distances are empirical and lack theoretical guarantees.

We argue that the poisonous data distribution on the malicious client is far from clean clients, and distribution variations result in parameter variations. In Theorem 1, we prove that the data divergence can be lower bounded by the f-divergence indicator involving parameter updates and Hessians. Based on the theoretical analysis, We propose the Federated F-Divergence-Based Aggregation (**Fed-FA**) algorithm that determines the unsuspected set \(S\) utilizing the proposed f-divergence indicator \(_{f}\).

To find abnormal or suspicious clients, \(_{f}^{(k)}=_{}(\{_{t}^ {(i)}\}_{i=1}^{n},k)\) estimates the divergence of datasets between the \(k\)-th client and other clients. Suspicious clients have larger \(_{f}\) than clean clients, thus we set \(S\) as clients with top-\( n/2+1\) smallest \(_{f}\). The pseudo-code is shown in Algorithm 1 and further details of the function \(_{}(,)\) are demonstrated in Sec. 3.3.

**Preparation for Theorem 1.** Suppose \(p()\) denotes the probability function of the distribution of the merged dataset on all clients; \(q()\) denotes the probability function of the data distribution on one client; \(^{}=_{i=1}^{n}^{(i)}/n\) denotes the average parameters of all clients; and \(^{}+\) denotes the parameters on the client, namely \(=^{(k)}-^{}\), where \(k\) is the indexes of the client with data distribution \(q()\). Denote \((;)\) as the loss of the data sample \(=(,y)\) on parameter \(\). For a data distribution \(\), define \((;)\) as the average loss on the distribution \(\): \((;)=_{} (;)\).

**Modeling data divergence with f-divergence.** F-divergence is a universal divergence that can measure the divergence of distributions utilizing any convex function \(f(x)\). Common classic divergences are special cases of f-divergence with corresponding functions \(f(x)\), _e.g._, for Kullback-Leibler divergence , \(f(x)=x x\).1 Therefore, we adopt the lower bound of f-divergence of \(q()\), the distribution on one client, and \(p()\), the distribution on all clients, to estimate the data divergence of \(p()\) and \(q()\). We try to find the infimum or greatest lower bound of f-divergence:

\[),q()}{} D_{f}q()||p(),\] (1) subject to \[^{*}=*{arg\,min}_{} ;p(),^{*}+= *{arg\,min}_{};q(),\] (2)

where \(^{*}\) and \(^{*}+\) are optimal parameters on \(p()\) and \(q()\); where \(=[_{1},_{2},,_{d}]^{}\); \(^{*}^{}\); \(D_{f}q()||p()\) denotes the f-divergence measurement : \(D_{f}q()||p()=_{}p( )f)}{p()}d\), where \(f(x)\) is an arbitrary convex function satisfying \(f(1)=0\) and \(f^{}(1)>0\).

**Proposed f-divergence indicator derived from Theorem 1.** To estimate the data divergence, we derive the **f-divergence indicator**, \(_{}=_{k=1}^{d}H_{k}^{*}_{k}^{2}\) from Theorem 1 by analyzing f-divergence:

**Theorem 1** (F-Divergence Lower Bound).: _The lower bound of f-divergence is:_

\[D_{f}q()||p()1+o(1) {f^{}(1)}{2}_{},_{}=_{k=1}^{d}H_{k}^{*}_{k}^{2},\] (3)

_where \(H_{k}^{*}=H_{k}^{*};p()=_{ _{k}}^{}^{*};p()>0\) is the \(i\)-th Hessian of loss on \(p()\) and \(f^{}(1)>0\)._

### Proposed Fed-FA algorithm

In this section, we introduce the implementation of Fed-FA. We propose the dataset synthesization and embedding Hessian reassignment techniques to estimate \(H_{k}^{*}\) in \(_{}\).

**Dataset synthesization.** As shown in Line 10-12 in Algorithm 1, to estimate the \(H_{k}^{*}\), we synthetize a small randomly labeled dataset \(^{}=\{_{j}=(_{j},y_{j})\}\) with unlabeled texts \(_{j}\) and random labels \(y_{j}\). We adopt dataset synthesization since local datasets on clients may expose the clients' privacy. We synthetize 4 samples every class. Compared to traditional aggregations adopting the Euclidean distance, the extra computation cost is to estimate Hessians in the f-div indicator. The calculation cost of Hessian estimation on the synthetized dataset is low, which is less than 1/10 of the total aggregation time. We estimate the Hessians with the Fisher information assumption on the synthetized dataset and the parameters \(_{t-1}^{}\): \(_{k}^{*}=_{^{}} _{_{k}}^{}(_{t-1}^{}; )^{2}\).

**Embedding Hessian reassignment.** Low-frequency words or features may be utilized to inject backdoors . Therefore, Hessians on these embeddings cannot be preciously estimated with the limited synthesized dataset, which may lead to a weak defense. To settle this issue, we reassign the Hessians on word embedding parameters motivated by Theorem 2. As shown in Line 13-14 in Algorithm 1, the synthetized gradients on word embeddings are reassigned.

Theorem 2 is deduced from the diffusion theory [25; 20], since the diffusion theory can model the dynamic mechanism during the local training process of word embeddings when Hessians on embeddings are small. Suppose \(E\) denotes the set of word embedding dimensions. For \(k E\), toensure that (1) \(_{k}^{*}}_{i=1}^{n}|u_{k}^{(i)}|/n\); and (2) \(_{k E}_{k}^{*}\) is invariant after reassignment, we have:

\[_{k}^{*}=_{i}^{*}}{_{j E }s_{j}}s_{k}, s_{k}=_{i=1}^{n}|u_{k}^{(i)}|+ ^{2},\] (4)

where \(=10^{-8}\); \(u_{k}^{(i)}\) is the \(k\)-th dimension of \(_{}^{(i)}\).

**Theorem 2** (Hessian Estimations by Diffusion Theory. Brief. Detailed Version in Appendix A).: _When \(^{*}}\) is small, the following expression holds in probability:_

\[^{*}}_{i=1}^{n}|u_{k}^{(i)}|.\] (5)

Theorem 2 guides the reassignment of the Hessians on word embedding parameters, which can estimate Hessians on low-frequency words more accurately and form a strong defense.

### Verification of Byzantine resilience and convergence of Fed-FA

 propose the concept of Byzantine resilience and prove that the Byzantine resilience of the aggregation \(\) can ensure the convergence of the federated learning process. We verify the Byzantine resilience of Fed-FA in Theorem 3. Further discussion of Byzantine resilience is in Appendix A.

**Theorem 3** (Byzantine Resilience of Fed-FA. Brief. Detailed Version in Appendix A).: _Suppose the malicious client number is \(m\), \(1 m\), when indicator estimations are accurate enough, there exists \(0<\) such that Fed-FA aggregation algorithm is \((,m)\)-Byzantine resilience:_

\[\|[(\{^{(i)}\}_{i=1}^{n})]-\|_{2} \|\|_{2},^{T}[(\{ ^{(i)}\}_{i=1}^{n})](1-)\|\|_{2}^{2},\] (6)

_where \(=[]\) is the expected update for clean clients \(\)._

Theorem 3 states the Byzantine resilience of Fed-FA, namely the variations of aggregated updates and ideal clean updates are bounded (\(\|[(\{^{(i)}\}_{i=1}^{n})]-\|_{2} \|\|_{2}\)), which indicates that the attacker cannot divert aggregated updates too far from ideal updates. Combined with Proposition 2 from , the gradient sequence converges almost surely to zero, therefore Fed-FA converges.

## 4 Experiments

In this section, we introduce experiment setups and main results. Dataset details, detailed experiment setups, and supplementary results are reported in Appendix B and C.

### Experiment setups

**Datasets.** We adopt four typical text classification tasks, _i.e._, _SST-2_ (Stanford Sentiment Treebank) , _IMDB_ (IMDB movie reviews) , _Amazon_ (Amazon reviews) , and _AgNews_. Following , we adopt the clean accuracy metric (_ACC_) to evaluate clean performance and the backdoor attack success rate metric (_ASR_) to evaluate backdoor performance.

**Models and training.** We adopt three typical neural network architectures in NLP tasks, _i.e._, _GRU_, _LSTM_, and _CNN_. GRU and LSTM models are the single-layer bidirectional RNNs , and the CNN architecture is the Text-CNN . We adopt the Adam optimizer  in local training of clients with a learning rate of \(10^{-3}\), a batch size of \(32\). We train models for \(10\) rounds. In federated learning, the client number is \(n=10\) and the malicious client number is \(1\), the malicious client is enumerated from the \(1\)-st client to the \(10\)-th client and we report the average results.

**Backdoor attacks.** In experiments, we adopt four typical **backdoor attacks**: _EP_ (Embedding Poisoning) [46; 51], _BadWord_, _BadSent_[4; 7], and _Hidden_ (HiddenKiller) . EP and BadWord choose five low-frequency candidate trigger words, _i.e._, "cf", "mn", "bb", "tq" and "mb". BadSent adopts "I watched this 3d movie" as the trigger sentence. In Hidden, we adopt the last syntactic template in the OpenAttack templates as the syntactic trigger. The target label is label 0.

**Aggregation baselines.** Robust federated aggregations can be divided into two categories, _i.e._, non-discarding aggregations and discarding aggregations. In experiments, we adopt _FedAvg_; **non-discarding aggregation baselines**: _Median_, _FoolsGold_, _RFA_, _CRFL_, _Residual_ (Residual-based defense) ; and **discarding aggregation baselines**: _Krum_ (initial Krum) , _M-Krum_ (multiple Krum) , _Bulyan_, and _Dim-Krum_ algorithms. In Dim-Krum, we choose the ratio as \(=10^{-3}\) and the adaptive noise scale \(=2\).

Most setups of training, attacks, and aggregations follow , and more details are in Appendix B.

### Main results

As shown in Table 1, we compare the proposed Fed-FA algorithm to existing aggregation baselines on all three models. Results are averaged on different attacks and datasets. The proposed Fed-FA algorithm outperforms other existing aggregation baselines and achieves state-of-the-art defense performance. We can conclude that discarding aggregations are stronger than non-discarding aggregations, which is consistent with the conclusions in . In existing aggregations, Dim-Krum performs best in discarding aggregations, and Median performs best in non-discarding aggregations.

Fig. 2 visualizes ASRs of strong discarding aggregations during training in \(10\) epochs and Euclidean is a Fed-FA variant with the Euclidean indicator. Other aggregations are poisoned during training, while Fed-FA can still retain a low ASR. Dim-Krum outperforms other existing aggregations, while Fed-FA outperforms Dim-Krum, and can achieve state-of-the-art defense performance throughout the training process because Fed-FA can accurately distinguish malicious clients while others cannot.

**Results of different datasets and attacks.** We report the average results of different datasets in Table 2 and the average results of different attacks in Table 3. Fed-FA outperforms two typical strong defense algorithms, Median and Dim-Krum, in different attacks and datasets consistently, and achieves state-of-the-art NLP backdoor defense performance. Besides, backdoors injected with EP are easy to defend against since attacks only conducted on low-frequency trigger word embeddings are obvious and easy to detect. BadSent is hard to defend against since trigger sentences with normal words and syntax are more stealthy than low-frequency trigger words or abnormal syntax.

**Influence of false positives** In real-world defense scenarios, the influence of false positives is also crucial , especially for discarding aggregations since they may discard clean clients. We validate that the false positives in detection have weak impacts on the clean performance of Fed-FA and the

    &  &  &  &  \\  & & & Median & FoolsGold & RFA & CRFL & Residual & Krum & M-Krum & Bulyan & Dim-Krum & **Fed-FA** \\ 
**GRU** & ACC & 86.05 & 86.04 & 85.92 & 85.96 & 71.25 & 86.05 & 76.32 & 85.09 & 86.05 & 84.53 & 86.36 \\
**(86.85)** & ASR & 86.02 & 59.56 & 85.99 & 86.26 & 75.96 & 66.54 & 74.22 & 54.24 & 48.90 & 33.16 & **13.66** \\ 
**LSTM** & ACC & 83.49 & 83.60 & 73.74 & 83.75 & 70.26 & 83.69 & 75.68 & 83.29 & 83.60 & 82.91 & 84.39 \\
**(84.42)** & ASR & 90.51 & 67.09 & 90.16 & 90.68 & 84.82 & 70.12 & 75.52 & 60.09 & 61.29 & 33.08 & **22.11** \\ 
**CNN** & ACC & 86.32 & 85.98 & 86.29 & 86.33 & 77.37 & 86.28 & 78.14 & 85.60 & 86.22 & 85.38 & 86.36 \\
**(87.11)** & ASR & 83.47 & 57.19 & 83.53 & 83.58 & 37.92 & 62.77 & 75.02 & 65.80 & 50.74 & 28.46 & **22.77** \\ 
**Average** & ACC & 85.28 & 85.61 & 85.32 & 85.35 & 72.96 & 85.34 & 76.71 & 84.66 & 85.29 & 84.27 & 85.70 \\
**(86.13)** & ASR & 86.67 & 61.28 & 86.56 & 86.84 & 65.98 & 66.14 & 74.92 & 60.04 & 53.63 & 31.56 & **19.51** \\   

Table 1: Average results of Fed-FA compared to others (lower ASR is better, lowest ASRs in **bold**).

Figure 2: ASRs in \(10\) rounds. Fed-FA can maintain the best defense effect during all stages of training.

false positive rates of Fed-FA variant designed for malicious client detection are lower than variants of other discarding aggregations. Due to space limit, further analyses are deferred to Appendix D.

## 5 Analysis

In this section, we first report the ablation study results. Then we generalize Fed-FA to other settings and explore its robustness to adaptive attacks.

### Ablation study

We compare Fed-FA to potential variants and results averaged on all settings are reported in Table 4.

**F-divergence indicator can estimate data divergence more accurately.** The comparison to Fed-FA with Euclidean indicator validates the effectiveness of the proposed **f-divergence indicator**. We also conduct analytic trials to evaluate the correlations of \(}\) and \((p||q)}\), here data divergences are controlled with the dataset mixing ratio following  that \(}(p||q)}\) should hold. Fig. 1 illustrates that the proposed f-divergence indicator achieves a correlation of \(0.9847\), higher than \(0.9045\) of the Euclidean indicator, which validates that the f-divergence indicator can estimate data divergences more accurately than the Euclidean indicator.

**Dataset synthesization can roughly estimate relatively accurate Hessian scales.** Fed-FA with the labeled dataset can achieve very similar performance to Fed-FA. Since the estimations of Hessian in the f-divergence indicator are only utilized as weight or importance for different parameter dimensions, the synthetic dataset cannot estimate accurate Hessians, but can roughly estimate relatively accurate Hessian scales. Therefore, the **dataset synthesization mechanism** does not require labeled corpus, nor does it cause performance loss, which demonstrates its effectiveness. Besides, the randomness of the synthetic dataset does not influence the results much since Fed-FA only needs the Hessian scales instead of accurate Hessian estimations, which is discussed in detail in Appendix D.

**Effectiveness of embedding Hessian reassignment mechanism.** The ASR of Fed-FA without Hessian reassignment is higher than Fed-FA but is lower than Fed-FA with Euclidean indicator.

  
**Method** & **ACC** & **ASR** \\  FedAvg & 85.28 & 86.67 \\  Median & 85.61 & 61.28 \\ Residual & 85.34 & 66.14 \\  Krum & 76.71 & 74.92 \\ M-Krum & 84.66 & 60.04 \\ Bulyan & 85.29 & 53.63 \\ Dim-Krum & 84.27 & 31.56 \\ 
**Fed-FA** & **85.70** & **19.51** \\   

Table 4: Results of ablation study on Fed-FA variants. Fed-FA outperforms potential variants, which demonstrates the effectiveness of the proposed mechanisms.

  
**Dataset** & Metric & FedAvg & Median & Dim-Krum & **Fed-FA** \\   & ACC & 79.68 & 79.54 & 79.96 & 81.60 \\  & ASR & 91.28 & 68.66 & 43.66 & **31.94** \\   & ACC & 79.72 & 79.79 & 77.62 & 79.82 \\  & ASR & 88.45 & 63.85 & 54.53 & **27.93** \\   & ACC & 90.38 & 90.29 & 89.14 & 90.27 \\  & ASR & 85.85 & 59.74 & 24.18 & **14.75** \\   & ACC & 91.36 & 91.21 & 90.44 & 91.12 \\  & ASR & 81.09 & 52.86 & 4.00 & **3.43** \\   

Table 2: Average results of different datasets. Fed-FA outperforms others consistently.

  
**Attack** & Metric & FedAvg & Median & Dim-Krum & **Fed-FA** \\   & ACC & 86.18 & 85.89 & 84.47 & 85.94 \\  & ASR & 97.64 & 12.70 & 14.09 & **12.13** \\   & ACC & 86.12 & 85.77 & 84.56 & 85.95 \\  & ASR & 91.40 & 81.08 & 34.03 & **15.40** \\   & ACC & 86.21 & 85.85 & 84.51 & 85.97 \\  & ASR & 99.17 & 98.32 & 48.42 & **28.30** \\   & ACC & 82.62 & 83.27 & 83.54 & 84.95 \\  & ASR & 58.45 & 53.01 & 29.71 & **22.21** \\   

Table 3: Average results of different attacks. EP is easy to defend against and BadSent is hard.

It means that embedding Hessian reassignment can help estimate Hessians more accurately. We also explore Fed-FA with inverse reassignment, which replaces the reassignment principle \(_{k}^{*}}_{i=1}^{n}|u_{k}^{(i)}|/n\) with \(_{k}^{*}}\{_{i=1}^{n}|u_{k}^{(i)}|/n\}^{-1}\), and causes very poor performance. We also implement Fed-FA with layer-wise reassignment that reassignhens in every layer respectively, and Fed-FA with reassignment within the entire model that reassigns Hessians on parameters on the entire model parameters instead of the embedding parameters \(E\). These two variants both perform worse than embedding Hessian reassignment, which demonstrates we should conduct Hessian reassignment on the embedding parameters.

**Why does conducting Hessian reassignment on embeddings work best?** The premise of Theorem 2 requires that Hessians are small. The Hessian scales on embeddings are usually smaller than other layers: analytic trials show that average Hessian scales are about \(10^{-6}\) on embeddings, \(10^{-4}\) on other layers; thus correlations of \(_{i=1}^{n}|u_{k}^{(i)}|/n\) and \(^{*}}\) on embeddings are \(0.47\), which is much higher than correlations on other layers, \(0.02\). Thus reassignment should only be conducted on embeddings.

### Results on other settings

**Results on pre-trained language models.** To validate the effectiveness of Fed-FA on larger models such as Transformers  and pre-trained language models, We evaluate Fed-FA on BERT  in Table 5. Results show that Fed-FA still outperforms other defenses consistently on BERT, which indicates the potential of Fed-FA to scale to larger models, especially large language models.

**Results on federated vision backdoor defense.** We also find that federated vision backdoors are easier to defend against than language backdoors, which is also observed in . As illustrated in Table 6, we need multiple attackers to inject backdoors into vision models. Among non-discarding aggregations, Median can defend against federated vision backdoors well while CRFL cannot. Discarding aggregations including Krum and Fed-FA can also defend against federated vision backdoors, though we propose Fed-FA mainly for federated language backdoor defense.

**Results on non-IID and multiple attacker cases.** We generalize Fed-FA to non-IID and multiple attacker settings in Fig. 3. Here we choose the Dirichlet distribution with the concentration parameter \(=0.9\) as the non-IID distribution. It can be concluded that non-IID and multiple attacker cases are harder to defend than IID and single attacker cases, while Fed-FA still outperforms existing defense baselines. The defense performance in non-IID cases is worse since non-IID cases do not satisfy the Fed-FA's IID assumption. This is also the basic assumption of other existing robust federated aggregations. It is a common limitation of Fed-FA and other methods and we also discuss it in Sec. 6.

### Robustness to adaptive attacks

Since the proposed f-divergence indicator is calculated according to parameter update variances, potential adaptive attacks can be conducted with adversarial parameter corruptions [40; 54] or perturbations . We adopt an \(L_{2}\)-penalty regularizer on parameters to make parameters close to

  
**Method** & Metric & 2 Attackers & 3 Attackers & 4 Attackers \\   & ACC & 96.33 & 96.23 & 96.01 \\  & ASR & 39.20 & 94.50 & 98.44 \\   & ACC & 96.64 & 96.22 & 96.23 \\  & ASR & **10.78** & 12.41 & 87.37 \\   & ACC & 96.61 & 96.44 & 96.31 \\  & ASR & **10.59** & **10.73** & **10.76** \\   & ACC & 95.97 & 95.76 & 95.64 \\  & ASR & **10.02** & **10.35** & **10.34** \\   & ACC & 96.08 & 95.82 & 95.93 \\  & ASR & **10.20** & **10.20** & **10.31** \\   

Table 6: Results of MNIST backdoor defense task. Fed-FA can still work in CV. ASRs (\(<11\)) are in **bold**.

  
**Attack** & Method & ACC & ASR \\   & FedAvg & 89.41 & 96.73 \\   & Median & 89.39 & 79.32 \\  & M-Krum & 88.95 & 12.93 \\  & Dim-Krum & 89.56 & 48.36 \\   & **Fed-FA** & 89.41 & **9.11** \\   & FedAvg & 89.45 & 95.95 \\   & Median & 89.45 & 92.29 \\   & M-Krum & 89.26 & 37.38 \\   & Dim-Krum & 89.45 & 46.03 \\    & **Fed-FA** & 89.03 & **27.10** \\   

Table 5: Results of defenses on BERT on SST-2. Fed-FA still outperforms others.

\(_{t-1}^{}\) in the \(t\)-th round: \(R=_{i=1}^{d}(_{i}^{k}-_{i}^{})^{2}\), where \(\) denotes the decay coefficient and \(_{i}^{}\) denotes the \(i\)-th dimension of \(_{t-1}^{}\). We also design another adaptive decay regularizer to target to attack Fed-FA: \(R=_{i=1}^{d}H_{i}^{}(_{i}^{k}-_{i}^{})^{2}\), where \(H_{i}^{}\) is Hessians estimated by attackers.

As shown in Fig. 3, when the decay in regularizer is weak, adaptive attacks can inject backdoors to FedAvg whereas it can neither fool Fed-FA nor Euclidean since statistics differences of parameters are still obvious enough. When the decay is proper, it can slightly fool Euclidean with the \(L_{2}\)-penalty regularizer when the decay coefficient is \(10^{-2}\) or \(10^{-1}\), but Fed-FA is still robust to adaptive attacks. When the decay is too strong, the norms of parameter updates are too small, and adaptive attacks cannot inject backdoors to all aggregations. To conclude, Fed-FA is robust to adaptive attacks. We also validate that Fed-FA is robust to distributed backdoor attacks  in Appendix D.

## 6 Broader impact and limitations

**Broader impact.** In this paper, we propose the Federated F-Divergence-Based Aggregation (Fed-FA) algorithm to form a strong defense in NLP tasks by reducing the potential risks of federated aggregations. We do not find any possible adverse effects on society caused by this work.

**Limitations.** Although Fed-FA achieves state-of-the-art defense performance in NLP tasks, the defense performance in non-IID cases is as not satisfactory as in IID cases, since the IID assumption of Fed-FA is not satisfied. This is a common limitation of Fed-FA and other existing methods. A future direction is to consider the semantics of the parameter updates themselves in addition to the data divergence for federated backdoor defense.

Besides, both our proposed Fed-FA and classic federated defending algorithms [1; 43; 10; 31; 58] are mainly evaluated on small-scale MLP, CNN or RNN models, but not evaluated on popular large language models due to computation cost limit. However, our proposed Fed-FA is model agnostic and just filters harmful gradients involved in aggregation, thus it can be extended to large-scale models such as Transformers and large language models. To validate this, we also evaluated Fed-FA on BERT , a pre-trained language model based on Transformers, to validate the potential of Fed-FA to scale to large models. A future direction is to evaluate and improve federated language backdoor defense algorithms on large language models.

## 7 Conclusion

In this paper, we model data divergence among clients' data theoretically for backdoor client detection in federated language learning. Based on it, we propose a novel and effective Federated F-Divergence-Based Aggregation (Fed-FA) algorithm as a strong defense for federated language learning. Fed-FA utilizes the f-divergence indicator to detect and discard suspicious clients. Both theoretical evidence and experimental results demonstrate that Fed-FA can better detect suspicious clients than existing robust federated aggregations that mainly adopt parameter distances explicitly. Thus, Fed-FA outperforms existing methods and achieves state-of-the-art federated language backdoor defense performance. Further analyses validate the effectiveness of proposed mechanisms, as well as show that Fed-FA can be generalized to other settings and is robust to potential adaptive attacks.

Figure 3: Results under non-IID and multiple attacker cases and under adaptive attacks. Fed-FA still outperforms existing baselines under non-IID and multi-atker cases and is robust to adaptive attacks.