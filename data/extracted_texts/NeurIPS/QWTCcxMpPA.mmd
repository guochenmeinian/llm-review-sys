# Measuring Multimodal Mathematical Reasoning

with the MATH-Vision Dataset

Ke Wang\({}^{1,*}\) &Junting Pan\({}^{1,4,*,}\) &Weikang Shi\({}^{1,*}\) &Zimu Lu\({}^{1}\)

\({}^{1}\)CUHK MMLab, \({}^{2}\)Shanghai AI Lab, \({}^{3}\)SenseTime Research, \({}^{4}\)CPII under InnoHK

\({}^{*}\)Joint first author, \({}^{}\)Project lead, \({}^{}\)Directional lead

###### Abstract

Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models exceeding human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating LMMs' mathematical reasoning abilities. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements in LMMs. Moreover, our detailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development.

The dataset is released at MathLMs/MathVision

Figure 1: (a) Zero-shot accuracies of four prominent Large Multimodal Models (LMMs), random chance, and human performance are evaluated on our proposed MATH-V across 16 subjects. Teal means newly introduced subjects. (b) Examples of easy problems in MATH-V failed by top-performing LMMs on MathVista. The three questions come from tests designed for elementary school students.

Introduction

The latest advancements in Large Language Models (LLMs) [1; 2; 3; 4; 5] and Large Multimodal Models (LMMs) [6; 7; 8; 9; 10; 11] have demonstrated remarkable capabilities, achieving impressive performance on a diverse array of language and visual-language tasks [12; 13; 14; 15; 16; 17; 18; 19; 20]. As a result of these strides, recent LMMs like GPT-4 [1; 6], InternLM-XComposer , InternVL  and Gemini [7; 23] are now outperforming average human capabilities in multimodal mathematical reasoning. Notably, on MathVista , the most widely used benchmark for evaluating LMMs' mathematical reasoning in visual contexts, the current leading model achieves a score of 63.9, surpassing the human average of 60.3. While the impressive performance on MathVista suggests that LMMs have surpassed average human capabilities in multimodal mathematical reasoning, our recent experiment on human math tests reveals a different conclusion. As illustrated in Figure 1.b, these problems, which are straightforward for humans, remain challenging for LMMs.

In light of this discrepancy and to gain a deeper understanding of the limitations of current math-oriented multimodal datasets, we conducted a thorough examination of existing benchmarks. Our analysis revealed two key issues:

Firstly, we observed that the range of subjects of existing benchmarks remains limited. For example, geometry problems are one of the most common types of math problems with visual contexts, as evidenced by the large body of geometry problems and benchmarks (_i.e.,_ Geometry3k , GeoQA+, GEOS , and UniGeo ). However, despite the abundance of problems, the diversity is quite limited. Surprisingly, we found that they predominantly concentrate on plane geometry, neglecting the rich diversity of other geometry subjects such as solid geometry, which deals with the volume and surface area of 3D objects. Additionally, we found that topics such as Graph Theory, Topology and Transformation Geometry are also rarely present in current benchmarks.

Secondly, while the visual data in existing datasets are quite diverse and are collected from different sources, the questions are quite limited in question types and contents. For example, we identified that among the 375 questions on "abstract scene" from MathVista, the majority can be categorized into just three types of questions: (1) Object Counting, (2) Length Measurement with a given reference, and (3) Time Inquiry based on a clock. Questions are mostly recognition-driven and seldom require in-depth mathematical reasoning. Similar problems also exist in image contexts of "function plots", "synthetic scenes", "natural images", etc. This tendency towards monotonous question patterns is commonly observed, where annotators are first given a contextual image and then asked to create math-related questions according to the image.

The limited subject range and question variety of current multimodal mathematical reasoning benchmarks can create a false impression of LMMs' capabilities, potentially misguiding the development of these models. Therefore, aiming to more comprehensively evaluate the mathematical reasoning skills of LMMs, we introduce the MATH-Vision (MATH-V) dataset. This dataset comprises 3,040 high-quality mathematical problems within visual contexts across 12 grades, carefully selected from 19 real-world math competitions. To ensure a high-quality standard, **all data within our dataset were cross-validated and filtered by multiple expert annotators**, guaranteeing that each problem has a unique and correct answer. The dataset is well-balanced, featuring 1,532 problems in an open-ended format and 1,508 in a multiple-choice format. Specifically, we introduce three major updates in our MATH-V:

1. **Expanded Categories**: In MATH-V, we introduce 8 new math categories, such as descriptive geometry (dimension understanding), graph theory (relations modeling), and topology (invariant under continuous deformation) and subdivide plane geometry into three categories (angle, area and length). These categories are important areas of multimodal mathematical reasoning and are currently not covered in existing benchmarks.

2. **Holistic Data Approach**: All data in MATH-V, including both questions and images, are jointly collected from real-world math competitions. This approach eliminates the need for additional annotation and avoids the monotonous question patterns. This difference in question annotation is reflected in the average question length, which is 42.3 for MATH-V, significantly surpassing the average of 15.6 of MathVista. Longer question lengths offer numerous advantages, particularly in facilitating richer mathematical reasoning by providing additional context. Further qualitative comparisons can be found in Appendix G.1.

3. **Difficulty Levels**: To better understand model performance, we categorize all problems in MATH-V by difficulty levels ranging from 1 to 5. This classification is rigorously verified by a team of experts to ensure accuracy and reliability. We hope that this categorization can facilitate better analysis of mathematical problem-solving skills across various levels of difficulty.

We conducted extensive experiments with our dataset to provide insights into the mathematical reasoning abilities of current LMMs on human math tests. As depicted in Figure 1.a, our findings reveal a substantial gap in mathematical reasoning within visual contexts between LMMs and humans, with scores of 30.39 and 68.82, respectively. Our MATH-V benchmark poses a significant challenge for LMMs but is relatively easy for humans to achieve a good score, as shown in Figure 1.b. This indicates that current top-tier LMMs still have many areas (_e.g.,_ transformation geometry, topology, etc.) with significant room for improvement in terms of mathematical reasoning in visual contexts to approach the performance of average humans.

Given that MATH-V primarily consists of unseen data for existing LMMs, it generally exhibits lower scores compared to other benchmarks with available training data that could have been used during LMMs' development [24; 29; 30]. We believe that solving these tasks is fundamental for applications in architecture, design, and engineering (_e.g.,_ solid geometry for architecture). Therefore, our benchmark provides a crucial foundation for future research. Furthermore, MATH-V could perfectly complement current benchmarks, together offering a more robust evaluation of the mathematical reasoning capabilities of LMMs.

In conclusion, the contributions of this study are listed as follows:

* We carefully audited existing datasets and identified significant problems that are relevant but not known to the community, such as missing math subjects and a significant number of questions with repetitive patterns.
* We present a new MATH-V benchmark for pushing advances in multimodal mathematical reasoning. MATH-V comprises 3,040 problems with visual context, selected from real-world math competitions. All problems are classified into 5 difficulty levels and 16 mathematical disciplines.
* Finally, leveraging the fine-grained categorization, we conducted a comprehensive analysis of current LMMs, suggesting new directions for improvement in future research.

## 2 Related Works

### Mathematical Reasoning Benchmarks

Various benchmark datasets [31; 32; 33; 34; 35] have been proposed to assess models' mathematical reasoning capabilities. However, current multimodal benchmarks [29; 36; 37; 38] primarily focus on visual recognition tasks, and multimodal mathematical reasoning benchmarks are less common. Early multimodal reasoning benchmarks, such as Geometry3k , GeoQA+, GEOS , and UniGeo , are very limited in subjects, primarily focused on plane geometry. Among recent multimodal benchmarks, MMMU  stands out, though it is limited to multiple-choice questions and only a small fraction of its content is mathematical. Another recent benchmark, MathVista , evaluates mathematical reasoning abilities within diverse visual contexts. However, it lacks a detailed classification of mathematical subdomains and emphasizes visual abilities rather than mathematical reasoning. In contrast, MATH-V contains math problems rigorous in both mathematical reasoning and visual recognition, categorized by clear difficulty levels and covering 16 representative subjects.

### LMMs for Math

Recently, AlphaGeometry  has shown impressive performance in solving challenging geometry problems, though it lacks the ability to process images and must rely on text descriptions of images. Current math-specific multimodal models, such as G-LLaVA , UniMath , MatCha , and UniChart , are primarily focused on plane geometry or chart-based problems and still lag behind general multimodal models such as GPT-4-turbo . To facilitate better evaluation of the mathematical reasoning abilities of multimodal models, we introduce our benchmark, featuring high-quality problems demanding expertise in both mathematical reasoning and visual recognition.

### Multimodal Foundation Models

With the success of LLMs, MiniGPT-4 , LLaMA-Adapter , and LLaVA [47; 10] have attempted to construct multimodal models from text-only models. Subsequently, various large multimodal models such as GPT-4V , Gemini [7; 23], Qwen-VL , InternLM-XComposer-VL [21; 8], and SPHINX  have been created, demonstrating impressive performance on benchmarks like MMMU  and MathVista . Notably, some open-sourced models have even received scores higher than GPT-4-turbo on these benchmarks. However, on our benchmark, GPT-4-turbo performs considerably better than open-source models, indicating the lack generalization abilities of some models, especially on questions very different from those in fine-tuning datasets.

## 3 Dataset

### Overview

We introduce MATH-Vision (MATH-V), a carefully curated benchmark designed to evaluate the multimodal mathematical reasoning capabilities of foundation models across a wide range of mathematical tasks with visual contexts. We particularly emphasize the challenges posed by the need for both expert-level visual perception and deliberate reasoning with subject-specific knowledge. This challenge is exemplified in our tasks, which require the processing of various heterogeneous image types and necessitate a model's proficiency in utilizing domain-specific knowledge to deeply comprehend both text and images, and to reason accordingly. This extends far beyond basic visual perception and calls for an advanced approach that combines sophisticated multimodal analysis with mathematical reasoning. The proposed benchmark encompasses 16 subjects over 5 levels of difficulty, including 8 new subjects including _Analytic Geometry, Combinatorial Geometry, Combinatorics, Descriptive Geometry, Graph Theory, Solid Geometry, Topology, Transformation Geometry_ and subdividing plane geometry into three categories: _angle, area, length_. The questions in our benchmark were manually collected from 19 competitions. The division of difficulty levels is primarily based on the recommended grades of these competitions, with details provided in Appendix H.2. MATH-V consists of 3,040 questions, and we also provide a smaller test set of 304 questions for quick evaluation. Detailed coverage, statistics and sources of MATH-V are presented in Appendix B.

### Data Collection

Data collectionOur benchmark collection comprises two stages. In the first stage, we gather a list of various mathematical competitions, both international and national. The selection is based on the criterion that visual inputs should be frequently utilized in the competition questions to provide significant insights, and the questions should be categorizable into different levels, such as AMC8, AMC10, AMC12, and AIME. Competitions like the International Mathematical Olympiad (IMO) are excluded due to the excessive difficulty of the questions and the rare use of visual inputs. Consequently, we select 19 competitions from various countries. In the second stage, we filter through online resources to identify problems where questions are interspersed with images. We adhere strictly to copyright and licensing regulations, ensuring that we avoid data from sites that prohibit copying and redistribution. Given the concerns surrounding data contamination in LMMs, we prioritize selecting questions that lack readily available answers, such as those found in separate documents or pages. For questions formatted as PDFs, we use the Mathpix API to extract text and images. This approach allows us to curate a diverse collection of 9k questions from various sources.

Data curationTo improve the quality of our data, we undergo a four-stage data curation process with the help of 10 annotators (senior college students from science-related majors). In the first stage, we verify the alignment of text questions and images, as Mathpix might return them in an incorrect order. We also eliminate questions with missing text or incorrect images and those with private information or offensive content. All results are cross-verified between different annotators. 3,352 math-VQA data are left after this stage. In the second stage, we use lexical overlap and Levenshtein distance to identify potential duplicate questions. These suspected duplicates are then reviewed to identify and remove any duplication. As a result of these steps, we obtain the final 3,040 math-VQA data for our benchmark. In the third stage, we categorize the problems into 16 different subjects. Our annotators are divided into three groups, each independently labeling all the questions, and we also utilized GPT-4V  and Gemini Pro  for subject advice. Annotations from various groups andmodels are consolidated through a majority voting system involving three expert annotators. In the final stage, we assign difficulty levels for each questions with reference to the grade requirements of the original competitions.

### Comparison with Existing Benchmarks

Comparing with MathVistaMathVista  is a comprehensive multimodal mathematical reasoning benchmark derived from 28 existing math-related multimodal datasets and 3 newly collected ones. However, within MathVista, approximately 20 datasets exhibit a trend where questions are annotated post-image collection by annotators, resulting in a relatively limited variability of questions, as shown in Figure 2. In contrast to MathVista, all math-VQA data including both questions and images in MATH-V are newly collected from real-world math competitions. This difference in obtaining question annotations is reflected in the average question length, which stands at 42.3 for MATH-V, significantly surpassing the 15.6 average of MathVista. A longer question length offers numerous advantages, particularly in facilitating richer mathematical reasoning by providing additional context. Further qualitative comparisons of math-VQA data between our dataset and MathVista are provided in Appendix G.1. Moreover, for MATH-V we also provide 16 fine-grained categorizations of different math subjects, which can gain deeper insights into individuals' strengths and weaknesses in specific areas of mathematics, compared to MathVista's 7 math reasoning types. The detailed data of the comparison can be found in Table 1. Finally, all problems within our datasets are labeled with one of the five levels of difficulty, while MathVista has only three levels, with a considerable portion of problems (37.7%) classified under the _not applicable_ category.

Comparing with MMMUDifferent from MathVista and our MATH-V, MMMU  is designed to evaluate the multi-discipline multimodal understanding and reasoning abilities of LMMs with college-level problems. In terms of Math, it spans eight distinctive subjects with a total of 500 questions, whereas subjects like Operation Research, Graph Theory, and Group Theory require sophisticated domain knowledge such as "DFS search" and "Cayley diagram". In contrast, our MATH-V focus on mathematics problems from elementary school to high school and can test the general visual perception and mathematical reasoning abilities of LMMs. Our dataset is 6 times larger than MMMU-Math, providing a significantly larger and more diverse set of challenges for analysis and evaluation.

    & **MathVista** & **MATH-V** \\  Main source & existing VQA \& & all from \\  & MathQA datasets & real exams \\  Template questions & Yes & No \\  Math & & \\ reasoning & 7 & 16 \\ types & & \\  Newly annotated questions & 736 & 3040 \\  Unique questions & 4746 & 3040 \\  Average question & 15.6 words & 42.3 words \\ length & & \\   

Table 1: Statistics of key differences between MathVista and our MATH-V. MATH-V comprises high-quality math questions from real examinations or competitions. In contrast, MathVista primarily features images from existing VQA datasets with template questions.

Figure 2: Questions in (a), (b), and (c) all emphasize visual recognition over mathematical reasoning. (a) requires model to identify the persons, which involves niche domain knowledge that is not related to mathematical reasoning. (d) highlights non-mathematical problems in MathVista.

Specific to subjects, our datasets not only offer better coverage of subjects but also have more problems per subject on average (190 v.s. 63). Note that in 3/8 subjects (Linear Algebra, Group Theory, and Logic) covered by MMMU-Math, there are less than 10 problems. More qualitative comparisons can be found in Appendix G.2.

## 4 Experiments

We conduct experiments to evaluate models' performance on MATH-V. Our findings indicate that accuracy remains low across even the most advanced models. We assess a variety of models, including LLMs and LMMs, and compare both closed- and open-source models, observing that closed-source models perform better. We also did a detailed error analysis of GPT-4V, the first widely used LMM, delving into its error distribution and providing corresponding qualitative examples.

### Experimental Setup

**Models.** We conducted experiments on (a) LLMs: ChatGPT-3.5  and GPT-4 , augmented with image captions generated by GPT-4V, (b) Open-source LMMs: LLaVA-v1.5 , SPHINX , ShareGPT-4V , InternLM-XComposer2-VL  and InternVL-Chat-V1-2-Plus , (c) Closed-source LMMs: Qwen-VL , Gemini Pro , Gemini 1.5 Pro  and the GPT-4 series.

**Implementation details.** Our evaluations are carried out in a zero-shot setting, both with and without Chain-of-Thought prompting  on several prominent models, to determine the models' ability

to produce accurate answers without the need for fine-tuning or few-shot demonstrations on our benchmark. For human performance, we recruited 100 annotators possessing a high school degree or higher, and they were asked to solve the questions in MATH-V independently, thereby obtaining the human performance baseline. Details can be found in Appendix K.3. For other models, we utilize the default prompts provided for multiple-choice or open-ended questions, when available. In cases where models do not offer prompts for the task types present in the MATH-V dataset, we engage in prompt engineering and select the most effective prompt for use in the zero-shot configuration during the main experiments. Detailed information about our prompts is available in the Appendix E.

### Experimental Results

In this section, we compare the performance of several best-performing Large Language Models (LLMs) and Large Multimodal Models (LMMs) on MATH-V, as detailed in Table 2 and Table 3.

**Robustness of MATH-V** comes from three aspects. First, to provide a more rigorous evaluation of the models' generalization capabilities, MATH-V remains entirely unseen by the LMMs. This differs from existing benchmarks, which may include training data from the same domain. This stricter evaluation is reflected by the lower SOTA scores on MATH-V compared to those on mainstream mathematical reasoning benchmarks, as shown in Figure 3.a. Furthermore, our dataset's multiple-choice questions all feature five options, while existing benchmarks typically offer fewer options or simple yes/no questions. This reduces the chances of correct guesses, as evidenced by a much lower random chance score (5.86 for MATH-V vs. 17.9 for MathVista). Lastly, our holistic data collection approach jointly collects images and corresponding questions from real exams rather than relying on additional annotation. The lower performance of text-only models like GPT-4 on MATH-V further demonstrates the dependency of our questions on the associated images. This approach not only results in longer and more diverse questions but also requires models to possess more sophisticated visual recognition and mathematical reasoning capabilities.

**Considerable room for improvement.** As shown in Table 2, both closed- and open-source models have achieved significantly lower scores on our dataset compared to humans, with the highest scores being 30.39 and 16.97, respectively, while the average human score is 68.82, highlighting the considerable room for improving LMMs' capabilities. As depicted in Figure 3.b, with majority voting of 20 reasoning paths, models like GPT-4-turbo can achieve a 5% increase in accuracy, indicating that these models have the potential to address these questions. Moreover, in less than eight months' evolution from GPT-4V to GPT-4o, there was a notable increase in accuracy on MATH-V, from 22.76% to 30.39%, consistent with significant accuracy gains observed in other benchmarks such as MathVista and MMMU. Other open-source models, such as InternVL-Chat-V1-2-Plus, have achieved

Figure 3: (a) State-of-the-art (SOTA) performance and corresponding model of several mathematical benchmarks in **zero-shot** settings. Our MATH-V benchmark is markedly lower than existing text-only and multi-modal mathematical benchmarks. Results of other benchmarks are either from their official website or Papers With Code. (b) Although current models achieve relatively low scores on MATH-V, employing 20 times majority voting can still increase accuracy by up to 5%, indicating that models have the potential to address these questions effectively.

enhanced performance on MATH-V by employing a stronger language model and scaled fine-tuning datasets. These advancements indicate that LMMs can continue to make progress on MATH-V through innovations in model architecture, enhancements in training data, etc.

**Comparison among different subjects.** The dataset covers a broad spectrum of mathematical subjects, from algebra to transformation geometry, each presenting unique complexities. For example, even the high-performing GPT-4o struggles to achieve more than 20% accuracy in transformation geometry. This contrasts with its relatively higher accuracy in arithmetic, where it achieves 49.3%. These disparities across subjects highlight the model's strengths in numerical calculations and its weaknesses in understanding spatial position transformations.

**Disparity of closed- and open-source models.** There is a notable performance disparity between closed- and open-source models, different from the results on MathVista. Closed-source models, including GPT-4, GPT-4-turbo, GPT-4V, Gemini-1.5 Pro, and Gemini Pro, all outperform the best open-source model. For instance, GPT-4 achieved the highest overall accuracy at 30.39%, excelling in subjects like algebra (42.0%), analytic geometry (39.3%), and arithmetic (49.3%). In contrast, the highest-performing open-source model, InternVL-Chat-V1-2-Plus, only reached an overall accuracy of 16.97%. However, results from MathVista show that the accuracy of InternVL-Chat-V1-2-Plus is 59.9%, which is 10% higher than that of GPT-4V, which scored 49.9%. This discrepancy may be attributed to the use of the training set of MathVista during the supervised fine-tuning stage of InternVL-Chat-V1-2-Plus, as training data for both models is unavailable for our dataset.

**Failure of CoT prompting on some models.** Chain-of-Thought (CoT) prompting did not consistently enhance performance across models. While GPT-4V benefited from CoT (22.76% to 23.98%), other models like Gemini Pro and Qwen-VL-Max saw a decrease in performance. We found many LMMs tend to generate very short responses like "The answer is..." without reasoning steps, even when required to solve the question step by step. This suggests that the step-by-step reasoning capabilities of LMMs are still not mature and cannot be fully exploited by CoT prompting.

### Error Analysis

**Quantitative and qualitative analysis on GPT-4V.** In this section, we meticulously analyzed the causes of errors in GPT-4V through both quantitative and qualitative approaches, detailed in Figure 5 and Figure 5. The most prevalent cause of errors, accounting for 42.2%, was identified as Reasoning Error. This indicates a significant challenge in the model's logical processing and inferential reasoning capabilities. Following closely, Vision Recognition Error was the second most common issue, responsible for 31.9% of errors. This highlights difficulties in accurately interpreting visual data, suggesting that the vision encoder remains a bottleneck in multi-modal models. Knowledge Error,making up 15.1% of the total errors, pointed to gaps between the model's information database and its application. Furthermore, Calculation Error accounted for the smallest percentage of errors at 1.3%, indicating that GPT-4V is relatively robust in numerical and computational tasks, which historically have been challenging for language models. Notably, all errors in the Calculation Error category related to symbolic computation, as shown in Figure 5, rather than numerical computation, suggesting a potential area for future improvement. Detailed examples of these errors can be found in Appendix I.

**Qualitative analysis on GPT-4 with image captions.** The integration of image captions generated by GPT-4V enables GPT-4 to reach an accuracy of 13.10%, comparable to that of SPHINX and Qwen-VL. In disciplines such as algebra, analytic geometry, arithmetic, combinatorics, and counting, GPT-4 with image captions outperforms Gemini Pro and all open-source models except InternVL-Chat-V1-2-Plus. Conversely, in other subjects, its performance notably lags behind these models. Through qualitative analysis of these categories, we observed that subjects like algebra and analytic geometry are more likely to receive relatively accurate descriptions from GPT-4V, unlike other disciplines. This discrepancy underscores the need for LMMs to enhance their capabilities in fine-grained image understanding and description. Remarkably, in some areas (e.g., combinatorics and counting), GPT-4 with image captions even surpasses GPT-4V. This success can be attributed to GPT-4's reasoning capabilities, which evidently contribute to a better understanding and solving of problems. Examples supporting this qualitative analysis are provided in the Appendix J.

## 5 Conclusion and Limitations

In this paper, we propose a benchmark dataset, called MATH-V, for evaluating the mathematical reasoning abilities of multimodal models. MATH-V consists of 3,040 math reasoning problems, each accompanied by images and texts. We have carefully evaluated a variety of open-source and closed-source models using our MATH-V and conducted a systematic analysis. Our benchmark reveals a significant gap between human performance and that of current multimodal models in the task of multimodal reasoning, and facilitates more effective evaluation for future studies in this field.

However, our work does have certain limitations. First, all of the questions in our dataset are in English. Therefore, the dataset cannot evaluate models' capabilities to reason in languages other than English. Also, the images in our benchmark are all abstract, with no pictures taken in real life. These limitations are important and warrant further investigation.

## 6 Acknowledgements

This project is funded in part by National Key R&D Program of China Project 2022ZD0161100, by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of CPII under the InnoHK. This work was conducted using content provided by Association Kangourou Sans Frontieres - AKSF, https://www.aksf.org. We are grateful to Meike Akveld and Robert eretschlager for their support of this project.