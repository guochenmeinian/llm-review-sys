# NeuroBOLT: Resting-state EEG-to-fMRI Synthesis

with Multi-dimensional Feature Mapping

 Yamin Li\({}^{1}\)  Ange Lou\({}^{1}\)  Ziyuan Xu\({}^{1}\)  Shengchao Zhang\({}^{1}\)  Shiyu Wang\({}^{1}\)

Dario J. Englot\({}^{2}\)  Soheil Kolouri\({}^{1}\)  Daniel Moyer\({}^{1}\)  Roza G. Bayrak\({}^{1}\)  Catie Chang\({}^{1}\)

\({}^{1}\)Vanderbilt University \({}^{2}\)Vanderbilt University Medical Center

yamin.li@vanderbilt.edu

###### Abstract

Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: **NeuroBOLT\({}^{1}\)**, i.e., **Neuro**-to-**BOLD** Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs unseen resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy with the potential to generalize across varying conditions and sites, which significantly advances the integration of these two modalities.

## 1 Introduction

Functional magnetic resonance imaging (fMRI) and electroencephalography (EEG) are the most commonly utilized non-invasive neuroimaging techniques, providing crucial insights into brain functionality. These two modalities offer distinct advantages and limitations that can complement one another when combined . fMRI offers high spatial resolution imaging of whole-brain activity by measuring blood-oxygen-level-dependent (BOLD) signal changes, which facilitates probing regional and network-level function. However, fMRI suffers from low temporal resolution and hemodynamic blurring, which limits its ability to capture the accurate timing of rapid neuronal activity dynamics. Additionally, its high cost, non-portability, and incompatibility with metal implants further restrict the utility of MRI in certain contexts. Conversely, EEG stands out as a low-cost, portable neuroimaging modality with high temporal resolution . However, EEG faceslimitations due to volume conduction and from the superficial location of electrodes (on the scalp), making it difficult for EEG to accurately infer the origins of the measured electrical potentials. In this context, the high spatial resolution of fMRI becomes indispensable, especially for imaging deep brain regions.

One approach for investigating the relationship between EEG and fMRI signals is by analyzing data collected simultaneously from both modalities. Such studies have demonstrated correlations between fMRI data and various features of EEG signals, such as the power in certain frequency bands [27; 7; 28], which underscore the potential of EEG to inform fMRI features. However, factors such as the disparate biophysical origins of the two signals, and differences in their spatial and temporal resolutions, can limit the accuracy, interpretation, and consistency of correlation-based EEG-fMRI studies. These challenges are exacerbated in conditions during which a participant is resting passively (i.e., resting state), which is characterized by significant noise and randomness. Consequently, the mechanisms linking neuronal activity to BOLD signals remain only partially understood, posing challenges in mapping EEG to fMRI.

With recent progress in cross-modality synthesis and brain decoding techniques using deep neural networks [8; 9; 16], EEG-to-fMRI synthesis has emerged as a promising yet largely untapped research area . Early pioneering work by [39; 43] employed ridge regression on EEG temporal-spectral features to reconstruct fMRI signals from the visual cortex and sub-cortical regions. More recent studies have used deep neural networks for EEG-fMRI translation [33; 34; 35; 5; 3; 25; 31], and those of  and  developed sequence-to-sequence (Seq-to-Seq) models to reconstruct fMRI time series of deep brain regions from EEG. However, these efforts have primarily focused on task datasets, including cued eye opening/closing [25; 31; 34; 35; 54; 3], leaving the (fully eyes-closed) resting state condition largely unexplored. In addition, prior studies rely on a subject-specific approach, wherein models are solely trained and tested on different sections of the same individual's scans. Further, only a small number of brain regions have been examined in current studies, leaving as an open question the predictive power of EEG for fMRI signals across broader areas (see Appendix B for further discussion of related work). These limitations underscore the need for developing more generalizable models for EEG-fMRI translation that are accompanied by more comprehensive evaluation.

In this paper, we present NeuroBOLT: a multi-dimensional transformer-based EEG encoding framework for Neural-to-BOLD fMRI Translation (Figure 2). NeuroBOLT is designed to capture and integrate multi-dimensional representations across temporal, spatial, and spectral domains, offering a generalizable approach for translating raw EEG waves to BOLD fMRI time series from regions of interest (ROIs). Drawing inspiration from vision transformer (ViT)  and its adaptation for multi-channel biosignals [59; 22], our framework transforms raw EEG data into unified "EEG patches", enabling flexible and effective EEG data encoding. These EEG patches are then passed into two parallel sub-modules: (1) spatiotemporal and (2) spectral representation learning modules. The spatiotemporal module is propelled by a recently proposed EEG foundation model (LaBraM) , which has been trained to learn effective representations on about 2,500 hours of various types of EEG signals. Moreover, as previous studies have emphasized the importance of leveraging spectral features in EEG representation learning [59; 57; 60], EEG-fMRI correlations [27; 14], and EEG-to-fMRI synthesis [31; 39; 25], the spectral module incorporates multi-scale encoding on EEG spectrogram patches. Specifically, instead of employing a fixed window size for the Short-Time Fourier Transform

Figure 1: **Overall illustration of EEG-to-BOLD fMRI translation using NeuroBOLT.**

(STFT) as commonly used [59; 58; 24; 11], we incorporate spectral features from windows of varying scales. This approach retains the advantages of high temporal resolution from smaller windows and high frequency resolution from larger windows, thereby enhancing the spectral analysis. The output embeddings from the above two encoding modules are then integrated, allowing NeuroBOLT to capture the complexity of neural dynamics and learn the projection from neural to BOLD signals. The key contributions of this work are summarized as follows:

1) **Generalizable EEG-to-fMRI translation framework.** We introduce a novel approach, NeuroBOLT, that utilizes multi-spectral representation to predict high-dimensional fMRI signals from raw EEG data without relying on predefined assumptions about the hemodynamic delay between fMRI and EEG signals. Additionally, our model is designed to accommodate any number of EEG channels, enhancing its versatility across various experimental setups.

2) **Comprehensive evaluation of the predictive power.** We performed comprehensive experiments on subject-specific and cross-subject learning, across selected ROIs in primary sensory, high-level cognitive, and deep brain regions. Further, we probe the generalizability of our approach across data acquired in both resting state and task (auditory stimulus) conditions.

3) **Successful resting-state fMRI reconstruction.** To our knowledge, this is the first study to successfully reconstruct the eyes-closed resting-state fMRI signal from raw EEG data, with a relatively small number (26) of EEG electrodes.

## 2 Method

### Task Formulation

While it is well-established that fMRI signals are coupled to neuronal activity, the details of this process are only partially understood and still under debate . In practice, most studies model the relationship between neuronal activity and fMRI by convolving the assumed neural activity with the hemodynamic response function (HRF), which reaches its peak at about 6 seconds and gradually returns to baseline over the next 12-15 seconds. However, the HRF is not uniform, varying significantly across different brain regions and between individuals. Moreover, although EEG can detect the electrical activity of neural populations at millisecond timescales, it suffers from volume conduction, making it challenging to extract its neural origins. This spatial ambiguity also complicates the interpretation of EEG and its relation to fMRI data.

In this study, we aim to build a neural network to learn this complicated projection from the electrophysiological signal measured by EEG to the BOLD signal measured by fMRI. Our approach tries to overcome the above challenges in the following ways: 1) it captures multi-dimensional features of EEG across temporal, spatial, and spectral domains, learning representations that are crucial for accurate fMRI synthesis; 2) it does not assume a pre-defined delay between the two modalities, instead, we extract time windows of EEG data that span a duration approximating the length of HRF preceding each fMRI data point as the inputs to predict the corresponding fMRI data values (i.e., a Sequence-to-One model), in order to accommodate potential variability across subjects, regions, and frequencies.

Functional MRI data are conventionally represented in four dimensions, and can be expressed as a temporal sequence \(S=\{V_{1},...,V_{K}\}\) with K observations, where each observation is a volume \(V^{x y\ z}\) with spatial dimensions \(x,y,\) and \(z\). This high-dimensional fMRI signal is often summarized by representing it as a set of brain areas (i.e., \(P\) parcels), each with a corresponding fMRI signal that is averaged over its constituent voxels \(v_{x,y,z} V\). Here, we employ such a functional parcellation, Dictionaries of Functional Modes (DiFuMo) , which was learned across millions of fMRI volumes (see details in [12; 50]). We utilize the DiFuMo parcellation with \(P=64\) regions, and the resulting parcellated fMRI data \(Y^{P K}\) contains the signals from each of these regions.

Since fMRI signals are delayed by several seconds compared to the corresponding neural activity, the relationship between an input EEG window and the corresponding fMRI prediction is defined as \(_{p,t}=f_{}(X_{t-T:t-1})\), where \(_{p,t}^{1}\) is the reconstructed fMRI value from the \(p\)th ROI at time index \(t\), and \(X^{C T}\) represents the multi-channel EEG signal inputs with \(C\) electrodes, \(T\) total timepoints. To estimate \(f_{}(.)\), we formulate an optimization problem that minimizes the loss between the predicted fMRI signal \(_{p,t}\) and the true fMRI signal \(Y_{p,t}\) as follows:

\[}{}\,_{X}[(f_{}(X_{t-T: t-1}),Y_{p,t})].\] (1)

The function \(f_{}\) is NeuroBOLT, which includes a temporal-spatial representation module and a spectral representation learning module (Figure 2).

### Model Architecture

In this section, we introduce our model: NeuroBOLT, a general architecture for translating EEG signals to fMRI. Our model is designed to accommodate input EEG signals with an arbitrary number of channels. As shown in Figure 2, we leverage the pre-trained EEG foundation model, LaBraM (checkpoint version: LaBraM-base) , and finetune on our dataset to obtain spatiotemporal representations of EEG signals. Additionally, we propose multi-scale spectral feature fusion to obtain comprehensive spectral representations. These two modules learn complementary attributes of EEG data. Finally, we integrate the spatiotemporal and multi-scale spectral representations and feed them into a regression head for fMRI prediction.

Spatiotemporal RepresentationWe formulate the multi-channel EEG signals as \(X^{C T}\), where \(C\) represents the number of EEG electrode channels and \(T\) denotes the time length of the input EEG. To obtain the spatiotemporal representation of a given set of EEG signals, we leverage an operation from LaBraM , which segments the EEG signals into patches. Assuming the time window length (patch size) for tokenization of EEG signal is \(w\) and the stride is \(s\), \(X\) can be segmented into \(+1\) segments, with each segment denoted as \(^{C w}\). In this work, we use a window of length \(w\) and a stride \(s=w\) to segment each EEG channel into patches, obtaining \(=\{x_{c_{j},k}^{w} j=1,2,,C,k=1,2,, \}\). The total number of patches is \(C\). These patches are then passed forward to a temporal encoder  to obtain the patch embeddings, denoted as:

\[\{e_{c_{j},k}^{d} j=1,2,,C,k=1,2,,, \}\] (2)

where \(d\) is the dimension of the embedding.

To enable the model to capture the temporal and spatial information of the patch embeddings, we set up a list of trainable temporal embeddings and spatial embeddings, denoted as

Figure 2: **Overall architecture of NeuroBOLT. Our method first divides the input EEG window into uniform patches (A), and has two modules that are trained simultaneously: the temporal-spatial representation learning module (B) and the spectral representation learning module (C). The output embeddings from the two modules are summed and used as input to a regression head, which generates the final output.**

\(k=1,2,,\) and \(SE=\{se_{j} j=1,2,,C\}\), respectively. Thus, the final segment embedding \(e_{seg}\) can be represented as the sum of the output embedding, temporal embedding, and spatial embedding, denoted as:

\[e_{seg}=\{e_{c_{j},k}+te_{k}+se_{j} j=1,2,,C,k=1,2,, \}\] (3)

The segment embedding \(e_{seg}\) is directly fed into the Transformer encoder  to obtain the output embeddings. We then apply average pooling to these embeddings to obtain the spatial and temporal representation \(r_{st}^{d_{st}}\), where \(d_{st}\) denotes the dimensionality.

Multi-scale Spectral RepresentationCompared to RGB images, EEG signals present several challenges, such as a low signal-to-noise ratio, apparent stochasticity, nonstationarity, and nonlinear characteristics, making the reconstruction of the original signals difficult . Previous research indicates that the frequency spectrum of EEG signals is crucial for understanding the brain's neuro-physiological activities [59; 55]. Therefore, in our work, we utilize the Short-Time Fourier Transform (STFT) to achieve a spectral representation of EEG signals. Unlike most recent state-of-the-art methods, we design a multi-scale spectral approach that captures both coarse and fine representations in the temporal and frequency domains [56; 36]. Details of rationale can be found in Appendix C). For the EEG signals \(X^{C T}\), we set the base window size as \(w_{b}\) and define the window size at level \(l\) as \(w_{l}=w_{b} 2^{l}\), where \(l=0,1,,L\) represents the level number. Thus, the EEG patches with different window size can be denoted as \(_{l}=\{x_{l,c_{j},k}^{w_{l}} j=1,2,,C,k=1,2, ,}\}\). After obtaining the EEG patches, Fast Fourier Transform (FFT) is applied to each patch with the FFT size matches the window length to calculate the magnitude spectrum with \(}{2}+1\) frequency bins at each level \(l\), which can be denoted as \(s_{l}^{C}( }{2}+1)}\).

Similar to the operation in spatiotemporal module, we define a list of trainable frequency embeddings for each magnitude spectrum at different levels, represented by:

\[FE=\{fe_{l}^{C}  d} l=0,1,,L\}\] (4)

These frequency embeddings, which vary in the number of temporal windows, are further mapped to a set of window embeddings as follows:

\[WE=\{we_{l}^{C n d} l=0,1,,L\}\] (5)

Then, we calculate the sum of the \(we_{l}\) for each level to obtain an overall spectrum embedding as \(e_{sp}^{C n d}\) as shown in equation 6

\[e_{sp}=_{l=0}^{L}we_{l}^{C n d}\] (6)

Figure 3: **Multi-scale spectral feature embedding.**

For each channel, we apply spatial embedding, similar to the spatiotemporal representation section, to obtain the final embedding with dimensions \(e_{sp}^{C n d}\), where \(C\), \(n\) and \(d\) represent the channel number, window embedding dimension, and frequency embedding dimension, respectively. The original Transformer is known to have quadratic complexity in both time and space, and EEG signals typically contain tens of channels. To learn these embeddings with lower complexity , we feed spectrum embeddings \(e_{sp}\) into a linear Transformer Encoder . Let the input embedding \(e_{sp}^{N d}\), where \(N=C n\) represents the number of tokens. The self-attention operation in the linear Transformer Encoder can be formulated as follows:

\[ =Attention(_{sp}^{Q},_{sp }^{K},_{sp}^{V})\] \[=softmax_{sp}^{Q})( _{sp}^{K})^{}}{})}_{N D}) _{sp}^{V}}_{D k}\] (7)

Here \(^{Q}\), \(^{K}\), \(^{V}^{d k}\) are the query, key and value matrices. The self-attention module uses a rank-\(D\) approximation for the softmax attention \((N N)\) by reduced-rank parameter matrices \(^{}^{N D}\), \(^{D N}\) (where \(D N\)). Main components of this module include one linear self-attention layer and one fully connected network. Layer normalization before each component , residual connection after each component , and dropout right after the self-attention to enable stable training . The output is fed into average pooling along the token dimension to obtain the final spectral representation \(r_{sp}^{d_{sp}}\), where \(r_{sp}\) has the same dimension as the spatiotemporal representation \(r_{st}\).

Projection HeadThe hidden embeddings from the above two modules are then summed up and fed into a regression head, which consists of Gaussian Error Linear Unit (GELU)  followed by a single linear layer, to make the final prediction of fMRI in the \(p^{th}\) ROI at time \(t\).

\[_{p,t}=((r_{st}+r_{sp}))\] (8)

## 3 Experiments

### Datasets

Resting-state datasetEEG and fMRI data were collected simultaneously from 24 healthy volunteers in two sessions, each lasting 20 minutes. Scans with significant artifacts in the EEG or fMRI data were excluded for further analysis. The final sample contains 29 fMRI scans from 22 subjects, 7 of whom had two scans. During these fMRI scans, subjects rested passively with their eyes closed (resting state). Written informed consent was obtained, and all protocols were approved by the Institutional Review Board. BOLD fMRI data were collected on a 3T scanner using a multi-echo gradient-echo EPI sequence with repetition time (TR) = 2100 ms. Scalp EEG was acquired simultaneously with fMRI using a 32-channel (10-20 system) MR-compatible system with FCz as the reference (BrainAmps MR, Brain Products GmbH) at a sampling rate of 5 kHz, and was synchronized to the scanner's 10 MHz clock to facilitate MR gradient artifact reduction. Details of EEG and fMRI acquisition, preprocessing as well as artifact reduction can be found in Appendix D.

Auditory Task DatasetTo further evaluate the model generalization performance, we also include simultaneous EEG-fMRI data (16 scans from 10 subjects) collected during auditory tasks. Subjects were asked to keep their eyes closed the entire time and to make a right-handed button press as soon as possible upon hearing a tone. This dataset was collected at a different site, different MR scanner (3T Siemens Prisma scanner) and using a slightly different EEG cap (32 channels but with partially different electrode settings). Please also see detailed information about data collection, preprocessing, and experiments in Appendix D.

Unless specified otherwise, the experiments and results below refer to the resting-state data.

### Experimental Setup

PreprocessingWe employ DiFuMo with \(n=64\) dimensions  (see Section 2.1) to extract measured BOLD signals within specific ROIs. We focus on seven ROIs that span a range of spatial locations and functional roles, namely: **primary sensory regions** (cuneus and Heschl's gyrus), **high-level cognitive areas** (anterior precuneus, anterior and middle frontal gyri), **subcortical regions** (putamen and thalamus), and the **global (whole-brain average) signal**. From these ROI time series, we regress out motion confounds, low-pass filter the signals below 0.15Hz, and use the 95th-percentile of the absolute amplitude to normalize the demeaned ROI time courses. To prepare the EEG data for input, we first exclude the ECG, EOG, and EMG channels (remaining 26 channels), and resample the EEG to 200 Hz to enhance computational efficiency while preserving meaningful frequency components (which are typically below 100 Hz). To predict the fMRI ROI signal, we extract EEG windows of 16 seconds before each fMRI data point. This window length is selected to encompass the peak and most of the variation in the hemodynamic response function. Additionally, this duration aligns with the maximum temporal encoding window length of the pre-trained LaBraM model , ensuring optimal fine-tuning. The normalization of the EEG also strictly follows , where EEG data values are divided by 100 so that the resulting amplitude falls primarily between -1 to 1, since a typical EEG amplitude range is from -100 \( V\) to 100 \( V\).

Baselines and Evaluation MetricsThe baseline models include state-of-the-art EEG encoding models from  and , as well as two EEG-to-fMRI translation baselines . Since the original EEG-to-fMRI baselines are sequence-to-sequence models, we adapted them by adding a final projection layer to their encoder to enable sequence-to-one prediction. Additional details on baseline implementations are provided in the Appendix E.2. We employed the following two metrics for comparison: 1) **Pearson correlation coefficient (R)**, which measures the strength and direction of the linear relationship between prediction and ground truth; 2) **Mean squared error (MSE)**, which measures the average squared differences between prediction and ground truth across samples. The main text highlights the correlation coefficient as the primary evaluation metric, while MSE results are provided in the Appendix Table 6.

Implementation DetailsFor the spatiotemporal module, we initialize the model by loading the pretrained weights from LaBraM-base , with a token length = 200, i.e., 1 second, with no overlap. Please refer to  for more details of EEG pretraining. For the multi-scale spectral module, we set the smallest scale size \(l_{0}=100\), i.e., 0.5 seconds without overlap. Experiments are conducted on a single RTX A5000 GPU using Python 3.9.12, Pytorch 2.0.1, and CUDA 11.7. To ensure consistency during model training, we set a fixed seed across all experiments. The batch sizes are set at 16 and 64 for intra-subject and inter-subject analyses, respectively. AdamW is utilized as our optimizer, and MSE as our training objective. The initial learning rate is set at 3e-4 with a weight decay of 0.05, and a minimal learning rate of 1e-6. For subject-specific prediction, where training and testing occur on the same scan, we split the scan in an 8:1:1 ratio for training, validation, and testing, i.e., training on the first 80% of the data and testing on the last 10%. Given that the fMRI signal exhibits significant autocorrelation, typically extending from about -10 to 10 seconds , we implement gaps of 20 seconds between the training and validation sets, as well as between the validation and testing sets, to prevent data leakage. For the inter-subject analysis in resting-state data, we randomly divided the datasets into training/validation/testing sets by approximately 3:1:1 (18 scans : 5 scans : 6 scans). For the task data, we split the scans by 9 scans : 3 scans : 4 scans. Since data from the same individual might have shared representations, the two scans from the same individual are ensured to be in the same set (either training, validation, or testing set) to avoid possible data leakage. All models are optimized on the training set and evaluated on the test set, with the best model and hyperparameters selected based on the validation set.

### Intra-subject Prediction

In this section, models were trained on approximately 16 minutes of EEG data, and used to forecast a future interval of fMRI signal (about 2 minutes) within a resting-state scan. Figure 4 illustrates the distribution of correlation coefficients (\(R\)) between the predictions and the ground truth, along with examples that represent the average performance of our model. The quantitative results are detailed in the upper part of Table 1 and Figure 7 in Appendix. NeuroBOLT outperforms the next two best EEG encoders by 12.26% and 9.71% (respectively) in terms of the average correlation, exhibiting the best performance in reconstructing fMRI signals from all ROIs.

### Inter-subject Prediction

Predicting Resting-state Data from Unseen SubjectsTo assess the generalizability of NeuroBOLT across subjects, we first evaluated its performance on held-out recordings in the resting-state dataset, this time using EEG data to predict the fMRI ROI time series across entire scans (with length of about 20 minutes). As shown in the lower part of Table 1, our model achieves the highest average correlation across ROIs, with a score of 0.473. Figure 5 shows the correspondence between the correlation values and the appearance of the reconstructions. This visualization reveals that NeuroBOLT successfully captures major features of fMRI dynamics in reconstructing the whole unseen fMRI scan, especially for primary sensory regions and global signals. Thus, even though both EEG and resting-state fMRI have relatively low signal-to-noise ratio, NeuroBOLT still learns meaningful EEG representations and EEG-fMRI correlations that are crucial for EEG-to-fMRI translation. Additional results including performance distributions with statistical significance, MSE table and examples visualizing the best and the worst fMRI scan reconstructions, are provided in Appendix F for comprehensively displaying the performance. Furthermore, in comparison to other state-of-the-art EEG-fMRI translation approaches  and EEG encoding models , NeuroBOLT achieves lower mean squared error (MSE) and higher correlation, with at least an 8.74% improvement in average correlations. Similar to the scenario of within-subject prediction, NeuroBOLT shows the highest performance in predicting signals from global and primary sensory regions, achieving average correlations of 0.564 and 0.522, respectively, followed by its performance in high-level cognitive and subcortical regions.

Ablation StudyIn this section, we scrutinize the design of NeuroBOLT on the same unseen resting-state recordings. Table 2 provides an ablation study on the contributions of each component in the NeuroBOLT model. The table reveals that the integration of the spectral learning module markedly

    &  &  &  &  &  \\  & & Curtains & Heschl’s Gyrs & Middle Frontal & Preconsure Anterior & Pavanna & Thalamus & Global Signal & Avg. R** \\   & BIOT  & 0.531\(\)0.223 & 0.518\(\)0.207 & 0.4800\(\)0.162 & 0.459\(\)0.110 & 0.410\(\)0.205 & 0.411\(\)0.231 & 0.483\(\)0.133 & 0.473 \\  & LaBMR  & 0.540\(\)0.176 & 0.591\(\)0.197 & 0.493\(\)0.153 & 0.490\(\)0.176 & 0.411\(\)0.179 & 0.496\(\)0.177 & 0.482\(\)0.167 & 0.484 \\  & BEHA  & 0.357\(\)0.241 & 0.596\(\)0.240 & 0.294\(\)0.228 & 0.320\(\)0.220 & 0.234\(\)0.194 & 0.332\(\)0.197 & 0.456\(\)0.240 & 0.341 \\  & Li. et al.  & 0.400\(\)0.228 & 0.515\(\)0.207 & 0.345\(\)0.169 & 0.457\(\)0.204 & 0.324\(\)0.183 & 0.389\(\)0.194 & 0.583\(\)0.170 & 0.445 \\  & **NeuroBOLT (ours)** & **0.588\(\)0.164** & **0.566\(\)0.183** & **0.502\(\)0.168** & **0.559\(\)0.141** & **0.457\(\)0.184** & **0.489\(\)0.213** & **0.589\(\)0.162** & **0.531** \\   & FELE  & 0.326\(\)0.004 & 0.412\(\)0.039 & 0.337\(\)0.078 & 0.437\(\)0.091 & 0.324\(\)0.125 & 0.373\(\)0.062 & 0.512\(\)0.048 & 0.376 \\  & CNN Transformer  & 0.288\(\)0.204 & 0.412\(\)0.014 & 0.298\(\)0.007 & 0.316\(\)0.153 & 0.232\(\)0.068 & 0.180\(\)0.106 & 0.228\(\)0.185 & 0.273 \\  & STT Transformer  & 0.269\(\)0.197 & 0.188\(\)0.056 & 0.262\(\)0.130 & 0.203\(\)0.143 & 0.474\(\)0.126 & 0.123\(\)0.101 & 0.347\(\)0.124 & 0.218 \\  & BIOT  & 0.457\(\)0.123 & 0.512\(\)0.039 & 0.391\(\)0.128 & 0.445\(\)0.084 & 0.299\(\)0.063 & 0.413\(\)0.073 & 0.529\(\)0.110 & 0.435 \\  & LaBMR  & 0.177\(\)0.116 & 0.116\(\)0.150 & 0.153\(\)0.132 & 0.170\(\)0.132 & 0.477\(\)0.113 & 0.471\(\)0.122 & 0.150\(\)0.152 & 0.151 \\  & BELRA  & 0.412\(\)0.112 & 0.482\(\)0.063 & 0.384\(\)0.147 & 0.452\(\)0.149 & 0.241\(\)0.135 & 0.410\(\)0.077 & 0.492\(\)0.106 & 0.412 \\  & Li. et al.  & **0.565\(\)0.063** & **0.401\(\)0.048** & 0.415\(\)0.114 & 0.446\(\)0.076 & 0.217\(\)0.139 & 0.434\(\)0.077 & 0.529\(\)0.092 & 0.419 \\  & **NeuroBOLT (ours)** & **0.425\(\)0.100** & **0.541\(\)0.046** & **0.423\(\)0.115** & **0.486\(\)0.136** & **0.335\(\)0.144** & **0.453\(\)0.166** & **0.564\(\)0.115** & **0.473** \\   

Table 1: Model performance (\(R\)) in intra- and inter-subject experiments. **Bold**: the best performance; the underlined: the second-best performance

Figure 4: **Intra-subject prediction results.** (A) The parcellation of the chosen ROIs. (B) The distribution of prediction performance (Pearson’s correlation coefficients), with example time-series reconstructions that represent performance levels near the mean (indicated by the small grey arrow in the histogram). The dashed lines in the histograms represent mean correlation values.

enhanced the average prediction performance by 0.285 in \(R\), underscoring the pivotal role of spectral features in EEG-fMRI translation. Further, we examine the effectiveness of our multi-scale spectral representation learning module by incrementally increasing the number of included scales. Increasing the number of scales led to significant performance improvements in most of the ROIs.

In our model, the number of FFT points matches the token length. Therefore, larger window sizes produce a spectrogram with higher frequency resolution but at the expense of capturing more rapid changes in temporal dynamics (also see in Figure 3). We observe that fusing multi-scale spectral features leads to better performance in capturing the projection from EEG to fMRI. Additionally, as shown in Table 2, different brain regions appear to prefer various numbers of scales, likely reflecting distinct temporal-spectral dynamics across regions. Overall, the NeuroBOLT configuration with four scales (\(l_{3}\)) achieves the best average performance. We have adopted this setting for all of our experiments.

Generalization to Task-related fMRITo evaluate the generalizability of our model trained on resting-state data to other conditions, we conducted the following experiments on the task dataset collected from a different site with a different scanner: (1) zero-shot generalization, where we pretrained our model on resting-state data and tested its performance on task data; (2) intra- and inter-subject prediction, where models were trained and evaluated only on task fMRI data; (3) fine-tuning, where models trained on resting-state data were fine-tuned with task fMRI data, and (4) joint-training, where we jointly trained (using both resting-state and auditory task fMRI data) and evaluated the model on the respective held-out test sets of both datasets.

As shown in Table 3, the performance of our pretrained model on zero-shot whole-scan task fMRI reconstruction achieved performances comparable to that of the resting-state data, with even better performance in several regions compared with the model that was trained only on task fMRI. Fine-tuning the model on the task dataset further improved the performance substantially. Moreover, carrying out joint training using both resting-state and auditory task fMRI datasets resulted in the best performance across 4 ROIs in task fMRI prediction. Our results also suggest that joint training is not necessarily facilitating the prediction on resting-state fMRI (beyond training on resting-state data alone), likely due to the smaller sample size of the task dataset and richer variability of brain

    &  &  &  \\  &  & ^{}\)s (\%)} &  &  &  &  &  &  &  &  &  \\  & MSE\({}_{1}\) & **R\({}^{}\)** & MSE\({}_{1}\) & **R\({}^{}\)** & MSE\({}_{1}\) & **R\({}^{}\)** & MSE\({}_{2}\) & **R\({}^{}\)** & MSE\({}_{1}\) & **R\({}^{}\)** & MSE\({}_{1}\) & **R\({}^{}\)** & MSE\({}_{1}\) & **R\({}^{}\)** & MSE\({}_{1}\) & **R\({}^{}\)** \\  T  & 0.247 & 0.177 & 0.239 & 0.211 & 0.256 & 0.153 & 0.246 & 0.170 & 0.259 & 0.047 & 0.255 & 0.147 & 0.246 & 0.150 & 0.151 \\ MS w/\(l_{3}\) & 0.208 & 0.404 & 0.193 & 0.486 & 0.222 & 0.384 & 0.193 & 0.489 & 0.245 & 0.274 & 0.222 & 0.452 & 0.181 & 0.534 & 0.432 \\ T4MS w/\(l_{1}\) & 0.215 & 0.405 & 0.188 & 0.498 & **0.212** & 0.424 & 0.190 & 0.482 & **0.234** & 0.314 & 0.207 & 0.436 & 0.185 & 0.508 & 0.436 \\ T4MS w/\(l_{1}\) & 0.208 & 0.430 & 0.186 & 0.486 & **0.212** & 0.424 & 0.193 & 0.469 & **0.234** & 0.352 & 0.211 & 0.429 & 0.178 & 0.533 & 0.442 \\ T4MS w/\(l_{2}\) & 0.197 & 0.464 & 0.174 & 0.542 & 0.213 & **0.432** & 0.189 & 0.491 & 0.235 & 0.177 & 0.215 & 0.433 & 0.177 & 0.549 & 0.461 \\ T4MS w/\(l_{3}\) & **0.192** & **0.482** & **0.171** & **0.564** & 0.215 & 0.423 & **0.188** & 0.496 & 0.235 & **0.335** & **0.208** & 0.453 & **0.171** & **0.564** & **0.473** \\ T4MS w/\(l_{4}\) & 0.210 & 0.432 & 0.178 & 0.524 & 0.213 & 0.429 & 0.200 & **0.502** & 0.235 & 0.323 & **0.206** & **0.455** & 0.179 & 0.558 & 0.460 \\   

Table 2: Ablation study on the different components of NeuroBOLT and various spectral scale level settings. T represents the Temporal-spatial module, while MS stands for the Multi-scale Spectral module. \(l\) denotes the number of scale levels that are used for spectral feature integration, where \(l_{0}\) means that only a single scale that equals the token size is employed.

Figure 5: **Examples of reconstruction of the unseen scans.** Scans with the **median** performance are shown. Dashed line: ground truth; colorful lines: model prediction

dynamics in the resting state condition. More results of task-scan prediction, such as the intra-subject prediction, comparison with baselines, and performance distribution, are provided in Appendix F.4.

## 4 Discussion and Conclusion

ContributionsIn this study, we propose NeuroBOLT, a versatile deep-learning solution for projecting scalp EEG to BOLD fMRI signals. By learning a multi-dimensional EEG representation from spatial, temporal, and spectral domains, NeuroBOLT shows a strong capability to reconstruct resting-state fMRI signals from EEG alone. In addition to subject-specific models, it can also predict fMRI time series from held-out subjects across entire 20-minute scans, which is a first in this field. It is important to note that correlations of 0.4-0.5 are considered strong for predicting moment-by-moment signal fluctuations in functional neuroimaging data, given the low signal-to-noise ratio of EEG/fMRI. It is also worth mentioning that our model reconstructs resting-state fMRI signals from deep subcortical regions like the thalamus (with accuracy comparable to, or exceeding, certain cortical ROIs), using raw EEG data with a relatively small set of electrodes (26), enhancing the capabilities of EEG to map subcortical dynamics. This number of channels is typically insufficient to capture detailed neural activity from such regions using methods like EEG source localization . Moreover, NeuroBOLT supports an arbitrary number of EEG input channels, which allows for flexible application across different EEG setups and across different experimental and clinical settings. Overall, by advancing the field of EEG-fMRI translation, this work may ultimately open new possibilities for non-invasive brain research and cost-effective clinical diagnostics.

LimitationsCurrently, our model is trained separately for each brain region, which is inefficient for the ultimate goal of high-resolution fMRI reconstruction. Since functional modules in fMRI also co-fluctuate dynamically, our future work aims to develop an integrated training approach that leverages these co-fluctuations, aiming to enhance the model's ability to reconstruct high-resolution fMRI signals more efficiently. Furthermore, although our model demonstrates promising zero-shot reconstruction performance on unseen task-based scans when trained on resting-state data even with limited samples, the sample size of our resting-state and auditory task dataset remains limited compared to existing studies on single modalities, which may still introduce potential bias or overfitting. For certain comparisons, sizable variability in the performance of the proposed method as well as the baseline methods was also observed (see Appendix F and Figures 7, 8). As our model is adaptable to various channel configurations, we plan to leverage other publicly available simultaneous EEG-fMRI datasets in our future work to train the model on larger, more diverse samples. Additionally, future evaluation with multiple random seeds could help mitigate potential biases especially under small sample size, enhancing the model's reliability and generalizability.