# 4M: Massively Multimodal Masked Modeling

David Mizrahi\({}^{1,2}\)  Roman Bachmann\({}^{1*}\) Oguzhan Fatih Kar\({}^{1}\)

Teresa Yeo\({}^{1}\) Mingfei Gao\({}^{2}\) Afshin Dehghan\({}^{2}\) Amir Zamir\({}^{1}\)

\({}^{1}\)Swiss Federal Institute of Technology Lausanne (EPFL) \({}^{2}\)Apple

https://4m.epfl.ch

For clarity, "modalities" usually denote the _inputs_ to a model (e.g. sensory signals), and "tasks" usually denote the _outputs_ (e.g. semantics). Our method enables a symmetric input-output structure, thus we use "modalities" and "tasks" interchangeably in this paper.

###### Abstract

Current machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision. In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a **single unified Transformer encoder-decoder** using a **masked modeling objective** across a **wide range of input/output modalities** - including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves **scalability** by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens.

4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility.

Through experimental analyses, we demonstrate the potential of 4M for training versatile and scalable foundation models for vision tasks, setting the stage for further exploration in multimodal learning for vision and other domains.

## 1 Introduction

In recent years, the field of natural language processing (NLP) has seen a shift toward training large language models (LLMs) that are inherently capable of performing a wide range of tasks without requiring extensive task-specific adaptations . While these models have demonstrated remarkable success in NLP, there remains a need to develop similarly versatile and scalable models for vision. A crucial aspect of scalability and versatility in vision is the ability to handle multiple (input) modalities and (output) tasks, as vision models must deal with a diverse range of sensory inputs, such as images, 3D, and text and solve a wide range of tasks.1 Unlike NLP, where language modeling on raw text has led to multitask capabilities , training on only RGB images with a single objective has not exhibited the same behavior for vision. Therefore, it is deemed important to incorporate multiple modalities and tasks in training. It has been indeed suggested by psychophysical studies that multimodality is one key driver behind the development of biological intelligence .

To create a model that exhibits the desirable properties of foundation models in vision, it is important to consider three key aspects in terms of scalability: data, architecture, and training objective. For data, scalability means being able to benefit from more training samples toward improving performance.

In terms of architecture, scalability implies increased performance with growing model size and remaining stable when trainingd at large sizes. Lastly, a scalable training objective should efficiently handle a growing number of modalities without incurring excessive computational costs. In our approach, we target scalability across these three aspects while _maintaining compatibility_ with multiple modalities.

We address these challenges by proposing a method consisting of training a single unified Transformer encoder-decoder using a multimodal masked modeling objective. We name this approach 4M (short for "**M**assively **M**ultimodal **M**asked **M**odeling")4 to emphasize its ability to scale to _many diverse modalities_. Our method unifies the benefits of multimodal learning and masked modeling, such as (1) serving as an effective pre-training objective for learning rich representations , (2) leading to strong cross-modal predictive coding abilities and shared scene representations , (3) enabling models to be used for generative tasks through iterative sampling . Crucially, 4M combines these benefits while remaining _efficient_ via a number of mechanisms.

To enable training a single Transformer on modalities with different formats, like text, bounding boxes, images, or neural network features, we choose to unify their representational spaces by mapping them into sets or sequences of discrete tokens  using modality-specific tokenizers . This tokenization approach enhances compatibility, scalability, and sharing by removing the need for task-specific encoders and heads, allowing the Transformer to be compatible with all modalities and maintain full parameter-sharing. Also, although 4M operates on a large set of modalities, it can train in a highly efficient manner through the use of _input_ and _target masking_. This involves randomly selecting a small subset of tokens from all modalities as _inputs_ to the model, while another small subset of the remaining tokens is treated as _targets_. Decoupling the number of input and target tokens from the number of modalities prevents the computational cost from rapidly escalating with increasing modalities, allowing for a scalable training objective.

Leveraging the availability of single-modal or text-image pair datasets, such as CC12M , we employ strong pseudo labeling networks to generate aligned binding data across various modalities. This pseudo labeling approach enables training on diverse and large-scale datasets without demanding them to come with multimodal/multitask annotations.

4M models are capable of performing many key vision tasks out of the box and can also be fine-tuned to achieve highly competitive performance on unseen downstream tasks and input modalities. In

Figure 1: 4M enables training a versatile multimodal and multitask model, capable of **performing a diverse set of vision** tasks out of the box, as well as being able to perform **multimodal conditional generation**. This, coupled with the model’s ability to perform in-painting, enables powerful image editing capabilities. This generalist model transfers well to a broad range of downstream tasks or to novel modalities, and can be easily fine-tuned into more specialized variants of itself.

addition, training using a multimodal masked modeling objective leads to _steerable_ generative models that can be conditioned on arbitrary modalities, enabling the user's intent to be expressed in a versatile manner (as depicted in Figure 4) as well as various multimodal editing tasks (see Figure 1).

We further perform an extensive ablation analysis that studies the factors affecting 4M's performance. This thorough examination, along with the simplicity and generality of our approach, demonstrates the potential of 4M for a wide range of vision tasks and further expansions.

Our main contributions and results can be summarized as follows:

1. **Method**: we introduce 4M, a framework for training versatile and scalable foundation models for vision tasks using a multimodal masked modeling objective. Our approach results in models that learn rich representations and perform well on a wide range of tasks without requiring task-specific adaptations.
2. **Performance**: we demonstrate the efficacy of our approach through extensive experiments and benchmarks, showcasing the ability of these models to perform many key vision tasks out of the box, as well as achieving highly competitive performance when fine-tuned on unseen downstream tasks.
3. **Generative Capabilities**: we showcase the flexible and steerable generative capabilities of models trained using 4M, enabling a variety of multimodal editing tasks utilizing conditioning on arbitrary modalities.
4. **Experimental Study**: we conduct an extensive ablation analysis to study the factors affecting 4M's performance, providing important insights into these models' behavior and design.

Code, models, and additional interactive visualizations are available at https://4m.epfl.ch.

## 2 Method Description

The 4M architecture and training objective (depicted in Figure 2) were designed with a focus on being as _compatible_ and _scalable_ as possible in terms of the number and type of modalities it accepts, while being conceptually _simple_ and computationally _efficient_. We enable these through the conjunction of the following key aspects:

1. **Tokenizing modalities**: We abstract away modality-specific intricacies by mapping all modalities into sequences or sets of discrete tokens, whether they are images, text, sparse data, or neural network feature maps. This allows every possible mapping between modalities to be seen as predicting one sequence or set of tokens from another. In Section 2.1, we discuss what types of modalities we train on, how we generate the training data, and how we enable training a model on different modalities through tokenization.
2. **Training a single compatible network on all modalities**: Different tasks in vision, NLP and other domains traditionally required vastly different modeling choices, architectures, and losses, making the joint training on multiple modalities challenging. Tokenizing all modalities into a unified representation space allows us to train a single Transformer encoder-decoder (see Figure 2) to map between different modalities through (parallel or serialized autoregressive) token prediction. In Section 2.2, we provide more details on the 4M architecture.
3. **Multimodal masked pre-training objective**: Transformers have demonstrated excellent scalability with data and model size across a diverse set of tasks , particularly when paired with a scalable pre-training objective such as masked reconstruction . In Section 2.3, we detail our approach to training 4M using a multimodal masked modeling objective on randomized token subsets to learn strong cross-modal predictive coding abilities.

### Modalities & data

**Pre-training modalities.** We train 4M models on a diverse set of modalities, namely RGB, captions, depth, surface normals, semantic segmentation maps, bounding boxes, and tokenized CLIP feature maps . These modalities were chosen to cover several key aspects: First, they contain a mix of semantic information (captions, semantic segmentation, bounding boxes, CLIP), geometric information (depth, surface normals), and RGB. When used as input modalities, these modalities can be used as informative priors about the scene geometry and its semantic content , and when used as target tasks, they allow us to steer what kind of representations are learned . Second, these modalities are diverse in terms of the format they use to encode information. They consist of dense visual modalities (RGB, depth, surface normals, semantic segmentation), sparse and/or sequence-base modalities (captions, bounding boxes), as well as neural network feature maps (CLIP ). Finally, these modalities allow for diverse and rich interaction with the model for generative purposes. For example, captions, segmentation maps, and bounding boxes allow for semantically conditioned generation, while geometric modalities enable grounding the generation on 3D information. 4M's versatility in handling various modalities, its capacity to benefit from cross-training, and its ability to learn cross-modal predictive representations (as demonstrated in Sections 3 and 4) suggest its potential for extension to even more modalities.

**Pseudo labeled multimodal training dataset.** Training 4M models requires a large-scale and aligned multimodal/multitask dataset that contains all the above modalities/tasks and is sufficiently diverse. Most multimodal datasets, however, either do not contain all our pre-training modalities , are too small , or are not diverse enough . For those reasons, we resort to pseudo labeling [42; 5] the publicly available Conceptual Captions 12M (CC12M)  as a binding dataset using powerful off-the-shelf models. Because this approach only requires access to a dataset of RGB images, it may scale to even larger web-scale image datasets [99; 14; 39].

**Tokenization.** All modalities are mapped to sets or sequences of discrete tokens (indices of a vocabulary) through the use of _modality-specific tokenizers_. Captions and bounding boxes are both treated as text and encoded using WordPiece . For modeling bounding boxes, we follow the approach of Pix2Seq , that turns the task of object detection into a sequence prediction problem. RGB, depth, normals, semantic segmentation maps, and CLIP feature maps are tokenized using learned vector quantized autoencoders (VQ-VAE) . Unlike Unified-IO  that represents all image-like modalities using an RGB pre-trained VQ-GAN , we instead use modality-specific tokenizers. This allows us to incorporate neural network feature maps that would otherwise be difficult to represent with existing image tokenizers. While mapping modalities to tokens and vice versa incurs a small computational overhead during inference, we avoid this overhead during pre-training by pre-computing the tokens while assembling our multimodal dataset.

We provide a detailed overview of the multimodal dataset, pseudo labeling procedure, and tokenization in Appendix B.

### Multimodal Transformer

We design the architecture of 4M with efficiency, scalability, and simplicity in mind. 4M's architecture closely resembles a standard Transformer  encoder-decoder but includes a few crucial modifications to enable joint modeling of multiple different image-like modalities, such as RGB or semantic segmentation, but also of sequence modalities, such as captions or bounding boxes.

**Multimodal encoder.** The encoder is a standard Transformer encoder but features modality-specific learnable input embedding layers to map token indices to vectors. To each token of a specific modality, we add a learnable modality embedding and either 1D (for sequences) or 2D (for dense modalities) sine-cosine positional embeddings. To facilitate transfer learning, the encoder is additionally designed to accept RGB pixels using a learnable patch-wise linear projection, enabling it to double as a Vision Transformer  backbone.

Figure 2: **Method overview.** (Left): 4M is a framework for training multimodal and multitask models that operate on tokenized versions of multiple image-like modalities (such as RGB, depth, etc.) and sequence modalities (such as captions and bounding boxes). (Right): The 4M pre-training objective consists of training a Transformer encoder-decoder to predict a randomly selected subset of tokens, which is sampled from all modalities, based on another random subset of tokens.

**Multimodal decoder.** The decoder handles tokens from both dense image-like and sequence-like modalities, with each type requiring a different approach. However, two aspects are common to all tokens: First, they can all freely attend to any encoder tokens in the cross-attention layers, ensuring full access to the encoded information. Second, we employ attention masks to separate decoder tokens of different modalities. This ensures that the decoder produces consistent outputs for each specific modality, irrespective of what other outputs are being generated simultaneously. For dense image-like modalities, the decoder input consists of mask tokens along with modality and positional information. The decoder's role is to predict this masked content. For sequence-like modalities, the input to the decoder comprises modality, positional, and content information. The decoder is tasked to predict the next token in the sequence. To ensure that each token is only influenced by preceding tokens (and not by any future tokens), we apply a causal mask to the self-attention, as is standard in autoregressive models. Since all target tasks consist of discrete tokens, we can use the cross-entropy loss _for all of them_, which we found removes the need for task-specific loss balancing and improves training stability. Further details on the architecture are provided in Appendix C.

### Multimodal masking strategy

For multimodal pre-training, we use a pre-training strategy similar to MultiMAE , in that we sample and encode a small set of visible tokens/patches from all modalities, and train the model to perform cross-modal predictive coding.

**Input & target masking.** Dropping masked-out tokens and only encoding the small set of visible ones when performing masked image modeling has been shown to yield significant increases in training efficiency , and is crucial when training on multiple modalities . The imbalance of the usually low number of input tokens and the much higher number of target tokens can induce significant computational costs in the decoder, even if they are small. We propose to use _target masking_, meaning that we do not decode all masked-out tokens, but only a randomly sampled subset. By fixing the number of randomly sampled input and target tokens (see Figure 2), 4M enables pre-training on many modalities while keeping training costs low. Similar to MultiMAE , we sample the number of input tokens per modality using a symmetric Dirichlet distribution with concentration parameter \(\). We follow the same approach to also sample the number of target tokens per modality. After sampling the per-modality number of input and target tokens, we sample tokens from dense modalities uniformly at random, and perform span masking  on sequence modalities. The experimental consequences of these design choices are studied in Section 5, and in more detail in Appendix E.4.

## 3 Transfer Experiments

To assess the effectiveness of 4M as a pre-training strategy, we train two models: a base version 4M-B with 86M encoder parameters and a large version 4M-L with 303M encoder parameters (for more details, see Appendix C). We then transfer these trained models to several common downstream tasks and compare their performance against relevant baselines. To better control for the dataset, augmenta

Figure 3: **Chained multimodal generation. This simplified example illustrates the generation of a full RGB image from a partial RGB and bounding box input using the MaskGIT  decoding scheme, followed by autoregressive generation of a caption. Note that through chaining (i.e. using fully generated modalities as conditioning when generating subsequent modalities), we can predict multiple modalities in a self-consistent manner. This is in contrast to independently generating each modality from the original conditioning, where each generated output is consistent with the _input_ but not necessarily with _other outputs_. See Figures 9 and 10 for visual examples of chained generation. Generated tokens can be turned back into images, text, and other modalities, using the detokenizers.**tions, model architecture, and compute, which can significantly affect downstream performance, we additionally show self-baselines that are conceptually similar to MAE  (Masked RGB \(\) RGB) and BEiT-v2  (Masked RGB \(\) CLIP).

The transfer tasks include ImageNet-1K classification , COCO detection and instance segmentation , ADE20K semantic segmentation , and NYUv2 depth estimation . While some transfer tasks have similarities to our pseudo labeled tasks, they are different instantiations (e.g., ADE20K instead of COCO semantic classes, or absolute depth instead of relative depth).

To make 4M models comparable to other ViT backbones, we train all methods by attaching transfer task-specific heads (e.g. Cascade Mask R-CNN ) to the encoder, and discard any decoders. Note that we can also choose to keep the decoder for transfer learning, which we explore in Section 5. For comparability, we perform the transfers of all 4M and baseline models in the same controlled manner, by closely following commonly used settings from other papers. Exact training details are provided in Appendix D.

Results in Table 1 show that 4M transfers exceptionally well to all downstream tasks, outperforming the baselines on detection, segmentation and depth estimation. While on ImageNet-1K, 4M is outperformed by more specialized models such as DeiT III, the results demonstrate 4M to be a _versatile_ vision model that can strongly benefit from being pre-trained on multiple (pseudo labeled) tasks. We note that preliminary experiments with an even larger 4M-XL model with 2.7B parameters showed overfitting to the pseudo labeled CC12M dataset, resulting in limited additional improvement on downstream tasks. While a larger dataset or adding data augmentations are therefore necessary to fully benefit from larger 4M models, we still observe significant improvements in generation quality and use the 4M-XL model in the next section.

## 4 Generative Capabilities & Probing the Learned Representation

4M can directly be used for generation of all pre-training modalities, through iteratively decoding tokens , as illustrated in Figure 3. In addition, we enable several other generative capabilities by utilizing two key aspects of 4M: The **first** is the fact that 4M is crucially able to generate _any_ of the training modalities, either unconditionally or conditioned on _any_ other set of modalities (see Figure 4 top). The **second** is the fact that 4M is trained using masking, which enables (conditional) in-painting and out-painting (see Figs. 1 and 4). Combining these two key aspects enables several _multimodal editing tasks_, such as _semantic editing_, _geometrically grounded generation_, or _guiding the generation with multiple strong and weak conditions_ (via weighting).

To improve image generation fidelity, we trained a 4M-XL version and all subsequent images were generated with it. While 4M can be directly used for generation, there are certain common improvements we perform to improve image fidelity in the results shown. These include specializing 4M-L into a super-resolution variant that maps tokens of low-resolution generations to a higher resolution , and specializing 4M models by fine-tuning them to be more aligned with specific generative use-cases (e.g. for text-to-image, or in-painting) . See Appendix A for more details on these specializations.

**Probing learned representations through generation.** In addition to performing transfers, we can get a glimpse into what kind of (predictive) representations 4M learned by manipulating one part of

   Method & Pre-training & Data & Extra & ImageNet-1K & COCO & ADE20K & NYU depth \\  & data & aug. & labels & Top-1 acc. \(\) & AP\({}^{}\)\(\) & AP\({}^{}\)\(\) & mIoU \(\) & \(_{1}\) acc. \(\) \\  MAE B  & IN-1K & ✓ & ✗ & 84.2 & 48.3 & 41.6 & 46.1 & 89.1 \\ DeiT III B  & IN-21K & ✓ & ✗ & **85.4** & 46.1 & 38.5 & 49.0 & 87.4 \\ MultiMAE B  & IN-1K & ✓ & ✓ & 84.0 & 44.1 & 37.8 & 46.2 & 89.0 \\
4M-B (RGB \(\) RGB only) & CC12M & ✗ & ✗ & 82.8 & 42.3 & 36.6 & 38.3 & 80.4 \\
4M-B (RGB \(\) CLIP only) & CC12M & ✗ & ✓ & 83.4 & 46.6 & 39.9 & 43.0 & 85.7 \\
4M-B & CC12M & ✗ & ✓ & 84.5 & **49.7** & **42.7** & **50.1** & **92.0** \\  MAE L  & IN-1K & ✓ & ✗ & 86.8 & 52.8 & 45.3 & 51.8 & 93.6 \\ DeiT III L  & IN-21K & ✓ & ✗ & **87.0** & 48.7 & 41.1 & 52.0 & 89.6 \\
4M-L & CC12M & ✗ & ✓ & 86.6 & **53.7** & **46.4** & **53.4** & **94.4** \\   

Table 1: **Transfer learning study:** We transfer 4M models to semantic and geometric downstream tasks and compare it to several baselines. For transfers to ImageNet-1K , we first perform intermediate fine-tuning on ImageNet-21K . 4M outperforms the baselines on all tasks except for ImageNet-1K, surpassed by DeiT III which is a specialized model. In contrast to 4M, all of the baselines employed data augmentations to achieve their results. Best results per category are **bolded**.

the input and keeping the remainder fixed . Figure 5 shows a series of manipulations, in which 4M shows intriguing capabilities at predicting geometrically and physically plausible arrangements while taking into account the semantic context of the scene.

**Multimodal editing.** By combining the multimodal conditional generation and in-painting capabilities of 4M, we can perform various multimodal editing tasks, such as performing semantic edits or in-painting grounded by geometric conditioning (see Figure 4 top and middle). Drawing parallels to ControlNet , these allow for steering the generation using more than just text. However, 4M is able to perform these tasks with just a single network and condition on multiple (partial) modalities - _individually_ or _simultaneously_. The conditions can either be hand-specified, or extracted from an image using 4M itself, thereby removing the need for specialist models to create the conditions.

**Multimodal weighted guidance.** Classifier-free guidance  has been shown to improve image fidelity in token-based generative models . Inspired by Liu et al.  that perform compositional generation on multiple text conditions, we can guide our generation by weighting different (parts of) modalities by different continuous amounts - even negatively. This unlocks further

Figure 4: **Multimodal generation and editing. 4M ’s in-painting and any-to-any prediction abilities unlock a suite of multimodal generation and editing capabilities, which allow for fine-grained creative control. We show several key capabilities such as grounding the generation in predicted geometry, performing semantic edits, and being able to control how much certain input modalities influence the generation via weighting.**

Figure 5: **Probing the learned representation through manipulation: (Left): All of the conditioning bounding boxes are fixed except the marked “potted plant” ones that are changing. Depending on where the “potted plant” bounding boxes are placed, 4M infers how they can be part of the scene (e.g., as a painting or real plant) in a geometrically and physically plausible manner. (Right): Changing a single semantic class accordingly affects how 4M predicts the overall image. See more examples in Appendix A.4 and interactive visualizations on our website.**

multimodal editing capabilities (see Figure 4 bottom), such as being able to weakly condition on certain modalities, or using negative weighing to avoid a certain concept in the generation. Multimodal guidance can be achieved by computing a weighted sum of the logits of an unconditional and each conditional case: \(_{}=_{}+_{i=1}^{n}w_{i} (_{}-_{})\).

We provide additional visualizations covering 4M's wide range of generative capabilities on our website and in Appendix A.4.

## 5 Ablations

Pre-training on a large set of modalities using a masking objective creates a large design space that raise questions such as: what modalities should we pre-train on, what is the optimal masking ratio, or how do we select the number of tokens to mask from each modality? By performing a thorough ablation of these design parameters, we aim to find out which ones matter the most for multimodal pre-training and which ones do not. We used these findings to decide on the settings for the models shown in Section 3 and 4.

We first choose a _reference setting_ and measure the deviation of the model performance as a result of the one aspect under study while keeping the rest fixed. Performance is measured through transferring the models to a large set of downstream tasks and measuring the validation set performance. The aim of the benchmarks is to measure how good a certain instantiation of 4M is at transferring both to _new target tasks_, but also to _unseen input modalities_. For that, we include tasks that use RGB (pixels) as inputs and transfer to a new target, such as COCO  object detection, ADE20K  semantic segmentation and ten transfers from RGB to dense tasks in Taskonomy  and Hypersim  (e.g. depth, curvature, segmentation). Furthermore, we include eleven single-modal and twelve multimodal transfers from some Taskonomy or Hypersim modalities to others (e.g. curvature \(\) occlusion edges, or RGB + depth \(\) segmentation).

For simplicity, and similar to pre-training, we model each of the benchmark tasks as predicting one set of tokens from another. This is achieved by training tokenizers on these modalities and tasks in the same way as we did for pre-training. All transfers are performed at 224 \(\) 224 resolution. For comparability across modalities and tasks, we measure the validation set cross-entropy performance rather than task-specific metrics. Note that in the interest of space, we aggregate the results of the Taskonomy and Hypersim tasks, and denote them as \(RGB\)\(\)\(X\), \(X\)\(\)\(Y\) and \(X\)+\(Y\)\(\)\(Z\). See Appendix E for further training details.

### Reference model

Following common practice for the reference model size [30; 86], our model consists of 12 encoder and 12 decoder layers. We train it on CC12M  using all modalities as both inputs and targets, at resolution 224 \(\) 224 pixels (corresponding to 14 \(\) 14 tokens per dense modality), and using _no augmentations_ such as cropping more color augmentations. The total training length is fixed at 100B tokens, corresponding to roughly 400M masked samples. We set the number of randomly sampled input and target tokens to 12 each, and sample each using a symmetric Dirichlet distribution with parameter \(=0.2\). To determine the significance of various modeling choices, we adopt the approach used by Raffel et al.  that calculates the standard deviation of the transfer results of ten independently trained reference models.

### Input modalities and target tasks

**Importance of target tasks for representation learning.** 4M is both a multimodal and a _multitask_ training scheme and the choice of target task(s) is a powerful way of steering what representation the model learns [10; 109; 42; 107; 5]. To ablate this for 4M, we fix the input modalities to be either RGB or all the modalities (denoted as _"All"_), and vary the target tasks. The results in Table 2 mirror the findings of Sax et al. , that the optimal choice of pre-training setting depends highly on the type of transfer that is performed, and that there is no single pre-training task that performs best on all transfers. However, pre-training on all target modalities consistently outperforms the other single-task and multitask alternatives in terms of average loss, no matter what input modalities were used during pre-training. This makes it the preferred configuration for generalist models, especially when their future applications are unknown or varied.

**Importance of multimodal pre-training for transferring to new input modalities.** Table 2 shows that multimodal pre-training can significantly help with transferring to new input modalities (X\(\)Y and X+Y\(\)Z transfers), but comes at a performance loss at transfers that use RGB as the sole input modality. In Appendix E.4, we explore pre-training using mixtures of different masking strategies, which enables us to train models that perform well in both regimes.

### Multimodal masking strategy

Multimodal masking is at the core of 4M, so in this section, we ablate how modality tokens should be sampled, and how many tokens we should encode and decode.

**Modality proportions in masking.** We ablate various choices of Dirichlet parameter \(\), both for the input and target sampling. If \(\) is low, the sampling procedure will often select cases where most of the tokens are sampled from only one modality. If \(\) is high, however, most samples will contain tokens from all modalities to equal proportions. Results in Figure 6 (a) show that uniformly sampling over the simplex performs best on average, but not by a large margin.

**Input masking budget.** The difficulty of the multimodal masked modeling task is largely determined by the number of visible (non-masked) input tokens. Encoding only the small set of visible tokens can significantly improve training efficiency . Figure 6 (b) shows that with a fixed training token budget, training with 128-256 input tokens performs well.

**Target masking budget.** We can decide to decode merely a small random subset of all remaining masked-out tokens, which is especially important for enabling efficient multimodal training with large decoders. As Figure 6 (c) shows, decoding only a small random subset of all targets performs well (for a fixed number of total training tokens), while also reducing computational costs.

### How well does \(4\)M scale?

Scalability is a key property that models and training objectives should have. We therefore ablate the following three axes: To ablate the _dataset size_, we train 4M models on various subsets of CC12M, down to 1/64th of the full dataset. To ablate the _training length_, we vary the total number of tokens seen. We define _tokens seen_ as the total number of both input tokens and target tokens the model was

   Pre-training inputs & Pre-training targets & COCO Det. & ADE20K Seg. & RGB\(\)X & X\(\)Y & X+Y\(\)Z & Avg. Loss & Avg. Rank \\  RGB & RGB & 3.14 & 6.21 & 5.03 & 6.94 & 6.15 & 5.49 & 8.00 \\ RGB & Depth & 3.11 & 6.06 & 4.72 & 6.89 & **5.84** & 5.32 & 4.00 \\ RGB & Normals & 3.12 & 6.02 & **4.66** & **6.83** & 5.87 & **5.30** & 3.20 \\ RGB & Segmentation & 3.17 & **5.94** & 4.84 & **6.86** & 5.89 & 5.34 & 4.60 \\ RGB & CLIP & 3.07 & 6.11 & 4.83 & **6.85** & 5.94 & 5.36 & 4.80 \\ RGB & Detection & **2.78** & 6.11 & 5.03 & 7.07 & 6.24 & 5.45 & 7.20 \\ RGB & Captions & 3.45 & 6.55 & 5.92 & 7.35 & 6.86 & 6.03 & 10.00 \\ RGB & Geometric & 3.11 & 6.08 & **4.70** & 6.88 & **5.85** & 5.32 & 3.80 \\ RGB & Semantic & 2.88 & 5.99 & 4.86 & 6.97 & 6.06 & 5.35 & 5.40 \\ RGB & All & 2.90 & 5.99 & 4.74 & 6.91 & 5.93 & **5.29** & 4.00 \\  All & RGB & 3.21 & 6.20 & 5.07 & **6.75** & 5.85 & 5.42 & 3.80 \\ All & CLIP & 3.19 & 6.18 & 5.06 & 6.80 & 5.88 & 5.42 & 3.80 \\ All & Geometric & 3.20 & **6.13** & **4.98** & **6.72** & **5.76** & **5.36** & 1.80 \\ All & Semantic & **3.05** & 6.13 & 5.16 & 6.77 & 5.87 & 5.39 & 3.40 \\ \(\) All & All & **3.06** & **6.11** & 5.07 & **6.75** & 5.80 & **5.36** & 2.20 \\   

Table 2: **Pre-training input and target modalities ablation:** The choice of pre-training tasks and modalities influences what representations the model learns, and how well it can be transferred to novel tasks and modalities. Here, _Geometric = RGB + Depth + Normals_ and _Semantic = RGB + Segmentation + CLIP + Detection + Captions_. We show the average losses (\(\)) for several task categories and compute the average rank and best losses for _”RGB”_ and _”All”_ inputs separately. The reference model setting is indicated by \(\) and results that lie within two reference model standard deviations of the best result are **bolded**. Performing 4M pre-training on all input and target modalities is the most versatile choice, if the optimal set of pre-training modalities for any given downstream task is unknown.

Figure 6: **Ablations results:** We ablate several key design choices of the multimodal masking objective (in blue), and study how well 4M scales (in green). We show the overall average losses (\(\)) and highlight the reference model setting in blue / green. A detailed breakdown of the task losses is provided in Appendix E.4 and Appendix E.5.

trained on. To ablate the _model size_, we train different sizes of 4M models, ranging from Tiny (4M-Ti) with 24M parameters to Large variants (4M-L) with 705M parameters. Exact model specifications are given in Appendix C.1. Figure 6 (d), (e), (f) show that 4M scales with dataset size, training length, and model size, respectively.

For additional ablations on the architectural and training design choices of 4M, see Appendix E.

## 6 Related Work

Large language models have been demonstrated to be capable of performing a diverse range of tasks out of the box [86; 12; 81; 25; 45] by training on large datasets with simple objectives [30; 84; 86; 106]. In vision, however, many scaling efforts have instead focused on training specialized models on a single task and modality, such as predicting masked RGB pixels [20; 31; 4; 48; 118; 34], discrete tokens [7; 132], or other (deep) features [117; 6; 82; 114; 36; 70] from RGB inputs. Training models instead on multiple tasks [16; 33; 63; 42; 92; 11] and modalities [85; 122; 76; 59; 133; 1; 3; 57; 43], or both [54; 103; 114; 5; 44] usually requires modality-specific modeling choices, making it difficult to extend these methods.

While some recent works aim to consolidate various modalities and tasks by representing them as images [77; 8; 115], these approaches have limitations when dealing with modalities that cannot be readily converted into images, such as text or neural network feature maps. Instead, 4M adopts the approach of Pix2Seq [21; 22] and Unified-IO  which addresses these issues by unifying the representation space on which models are trained through tokenization [110; 35; 30]. However, unlike methods like Unified-IO which operate on a single RGB image tokenizer, 4M's ability to work with multiple modality-specific tokenizers enables scaling to visual modalities beyond those that can be represented as images, such as neural network feature maps. 4M also builds upon the multimodal masking approach of MultiMAE  and extends it beyond image-like modalities.

Both token-based generative models [88; 123; 17; 18; 65] and diffusion models [89; 79; 95; 97] have been mostly limited text-to-image generation. While there are works that enable a greater amount of control by conditioning on additional modalities, they are either very limited in the number of ways they can be conditioned on [58; 40; 113; 119; 62; 9; 128], or require training a separate model for each new modality . 4M flexibly allows for conditioning on any subset of the training modalities, and can, likewise, generate all these modalities, unlocking powerful generative editing capabilities.

## 7 Conclusion and Limitations

4M is a generalist framework for training multimodal and multitask models that not only perform many key vision tasks out of the box, but also demonstrate strong transfer results to a wide range of downstream tasks. 4M's in-painting and any-to-any generation capabilities enable it to perform a wide range of multimodal generative and expressive editing tasks - all using a single model. In the following, we discuss some limitations of our approach and potential future work.

_Additional modalities._ While 4M already includes a number of modalities, from semantics-based and geometry-based to text-based, bringing additional modalities could vastly improve its usability. For example, training on features extracted from a large language model has been shown to significantly boost text-to-image generation capabilities [97; 123; 18]. Introducing modalities like edges, sketches, or human poses has the potential to greatly improve the expressiveness  of 4M, but it may also be expanded to videos or multi-view imagery to unlock spatial-temporal generation and editing capabilities. We anticipate 4M to conveniently extend to such new modalities.

_Tokenizer quality._ 4M can benefit from better tokenizers, both in terms of generation and transfer results, but there are limits to the amount of information that can be encoded in tokenized patches. Operating on tokens that cover a smaller image region, or operating on higher-resolution images may improve image quality, but is expected to be computationally more expensive. Generally, improvements along this direction are expected to directly boost the performance of 4M.

_Dataset size and quality._ While our binding dataset choice of CC12M is standard, 4M can benefit from training on significantly larger datasets [99; 14; 39]. Web-scraped image-text datasets contain many low-quality images, as well as captions that are not related to the image content. Fine-tuning 4M on a more curated dataset like LAION-Aesthetics V2 , or tuning the model using reinforcement learning  could significantly improve generation quality and diversity. We leave this for the future.

**Acknowledgements.** We thank Hanlin Goh and Elmira Amirloo Abolfathi for their valuable feedback on earlier versions of this manuscript.