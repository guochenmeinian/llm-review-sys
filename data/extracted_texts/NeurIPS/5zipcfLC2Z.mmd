# MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining

Jacob Portes\({}^{1}\)1 Alex Trott\({}^{1}\)1 Sam Havens\({}^{1}\) Daniel King\({}^{1}\) Abhinav Venigalla\({}^{1}\)

jacob.portes@alex.trott@sam.havens@daniel.king@daniel.king@abhi@

Moin Nadeem\({}^{2}\)2 Nikhil Sardana\({}^{1}\)2 Daya Khudia\({}^{1}\)1 moinnadeem@nikhil.sardana@daya.khudia@jfrankle@

###### Abstract

Although BERT-style encoder models are heavily used in NLP research, many researchers do not pretrain their own BERTs from scratch due to the high cost of training. In the past half-decade since BERT first rose to prominence, many advances have been made with other transformer architectures and training configurations that have yet to be systematically incorporated into BERT. Here, we introduce MosaicBERT, a BERT-style encoder architecture and training recipe that is empirically optimized for fast pretraining. This efficient architecture incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear Units (GLU), a module to dynamically remove padded tokens, and low precision LayerNorm into the classic transformer encoder block. The training recipe includes a 30% masking ratio for the Masked Language Modeling (MLM) objective, bfloat16 precision, and vocabulary size optimized for GPU throughput, in addition to best-practices from RoBERTa and other encoder models. When pretrained from scratch on the C4 dataset, this base model achieves a downstream average GLUE (dev) score of 79.6 in 1.13 hours on 8 A100 80 GB GPUs at a cost of roughly $20. We plot extensive accuracy vs. pretraining speed Pareto curves and show that MosaicBERT base and large are consistently Pareto optimal when compared to a competitive BERT base and large. This empirical speed up in pretraining enables researchers and engineers to pretrain custom BERT-style models at low cost instead of finetune on existing generic models. We open source our model weights and code.

## 1 Introduction

BERT has been the workhorse of modern natural language processing (NLP) since its introduction in 2018 . Even in the era of large language models (LLMs), BERT-style encoder models are still quite relevant; for example, encoder models are used for vector database embeddings and retrieval augmented generation in tandem with LLMs . In the past half-decade since BERT first rose to prominence, however, many advances have been made with other transformer architectures and training configurations that have yet to be systematically incorporated into BERT . In this study we empirically show that these speed optimizations can successfully be incorporated into the classic BERT architecture and training recipe.

BERT-style models are typically trained in two stages: an initial self-supervised pretraining phase that builds general representations of language, and a subsequent supervised finetuning phase that uses those representations to address a specific task. The pretraining stage for BERT models has historically been computationally expensive; in the original BERT study, for example, the authors trained their models for 4 full days on 16 Google TPUs. In recent years, however, the time and cost to train BERT models has dropped significantly. One widely cited paper from 2021 successfully reduced the training time of BERT-Large to 24 hours on 8 Titan V-12 GPUs , and another very recent paper trained a competitive BERT-Base model on a single consumer GPU in only 24 hours . Our work builds on these trends.

In this study, we introduce our optimized MosaicBERT architecture and show that certain architectural choices for BERT-style encoders lead to accuracy vs. time Pareto improvements in pretraining. We do this by empirically comparing MosaicBERT with an optimal BERT baseline that does _not_ incorporate our architectural changes but _does_ have non-architectural optimizations such as fast data streaming and optimal floating point precision. We then evaluate on the classic GLUE benchmark .

It can often be difficult to understand the effects of model architecture modifications and training hyperparameter choices by simply reporting a few numbers in a table, as is traditionally done in ML papers. A few numbers in a table can obscure differences in the training data, objective function, batch size, training duration, learning rate schedule and many other details. Since our main goal in this study is to show how combinations of architecture choices lead to improvements in both _accuracy_ and _training time_, we make the choice to plot accuracy vs. training time Pareto curves for all models (for example, in Figure 1B). Certain architecture changes might lead to an increase in throughput, but a decrease in accuracy (e.g. changes to the floating point precision); other changes might lead to an increase in accuracy but take a much longer time to converge (e.g. increasing the model size). Accuracy vs. training time Pareto curves enable us to adequately asses all these changes (see Appendix for a more detailed discussion on Pareto curves).

The contributions of this work are as follows:

* We implement a new BERT-style encoder architecture that optimizes pretraining speed and accuracy. This architecture combines FlashAttention , ALBi , Gated Linear Units , a dynamic unpadding module , and low precision LayerNorm.
* We show that MosaicBERT base achieves the downstream average GLUE (dev) score of 79.6 in 1.13 hours on \(8\)A100 80 GB GPUs at a cost of roughly $20 on a standard cloud provider.

Figure 1: (A) Schematic of MosaicBERT architecture (B) Pareto curves of average GLUE (dev) scores for MosaicBERT-Base and the standard BERT-Base. Error bars indicate 95% confidence interval over n=5 pretraining seeds. All training was on \(8\)A100-80GB GPUs. FlashAttention schematic adapted from , and unpadding schematic adapted from ).

* We characterize the accuracy vs. pretraining time Pareto frontier for MosaicBERT-Base and Large, and empirically show that the performance of MosaicBERT-Base and Large is Pareto optimal relative to BERT-Base and Large.
* We characterize the relative throughput properties of each of MosaicBERT's architecture and design choices via ablations.
* We characterize the tradeoff between model size and training duration, and show that BERT-Large performance only surpasses BERT-Base performance after extensive training. We open-source our model weights and code at mosaicbert.github.io.

Finally, we note that the ideas explored in this study are directly applicable to LLM pretraining, and directly motivated many of the architecture choices around MosaicML's MPT-7B and MPT-30B large language models [39; 38].

## 2 Methods

In order to build MosaicBERT, we incorporated architectural choices from the recent transformer literature. These include FlashAttention , ALBi , training with dynamic unpadding , low precision LayerNorm, and Gated Linear Units [12; 50] (Figure 1). Before describing these modifications in detail, we first review the classic BERT architecture and how we chose a strong baseline.

Since our goal here is to show relative improvements in training time and final accuracy, we do not attempt to beat state of the art models for finetuning benchmarks such as GLUE . These SOTA models are often trained for much longer (e.g. ) and are larger than the models we explore in this study [9; 23].

### Choosing a Strong BERT Baseline

The basic transformer block used in BERT models consists of (1) the attention mechanism and (2) the feed forward layers. This block is then repeated depending on the model size; BERT-Base has 12 repeated transformer blocks, while BERT-Large has 24.

For our baseline BERT-Base, we used the exact architecture of BERT from ;3 this includes a hidden size of 768, an intermediate size of 3072, 12 attention heads and 12 hidden layers, as well as the GeLU activation function, and learned positional embeddings. For our baseline BERT-Large, we used the exact architecture of BERT-Large from , which has a hidden size of 1024, an intermediate size of 4096, 16 attention heads, and 24 hidden layers.

**Model Architecture** & **GLUE** & **Training Hardware** & **Pretraining Corpus** & **Params** \\  & **(dev)** & **(hours)** & & **Corpus** & \\  MosaicBERT-Base (ours) & 79.6 & 1.13 & \(8\)A100-80 & C4 & 137M \\ MosaicBERT-Base (ours) & 82.2 & 2.81 & \(8\)A100-80 & C4 & 137M \\ MosaicBERT-Base (ours) & 83.2 & 4.6 & \(8\)A100-80 & C4 & 137M \\ MosaicBERT-Base (ours) & 83.4 & 5.27 & \(8\)A100-80 & C4 & 137M \\  BERT-Base (our benchmark) & 83.2 & 11.5 & \(8\)A100-80 & C4 & 110M \\  BERT-Base [14; 18] & 81.0 & 96 & \(16\)TPU & Wiki+BookC & 110M \\ BERT-Base  & 73.6 & 24 & 1 RTX A6000 & Wiki+BookC & 110M \\ CrammingBERT-Base  & 80.4 & 24 & 1 RTX A6000 & Wiki+BookC & 110M \\ BERT-Large [14; 37] & 84.1 & 96 & \(64\)TPU & Wiki+BookC & 340M \\  

Table 1: Average GLUE (dev) score across various efficient BERT implementations. Average includes all 8 GLUE tasks. BERT-Base average GLUE scores on the dev set as reported by , Table 3. BERT-Large average GLUE scores on the dev set as reported by , Table 5. Note that the original training set for BERT was pretrained on 40 epochs of English Wikipedia and BookCorpus , while MosaicBERT was pretrained on the Colossal Cleaned Common Crawl (C4) corpus .

While MosaicBERT-Base (i.e. 12 hidden layers) and Large (i.e. 24 hidden layers) stay true to this general structure, we introduce modifications that affect both the attention mechanism and the feedforward layers.

### MosaicBERT Architecture Modifications for Fast Pretraining

#### 2.2.1 MosaicBERT Modifications to the Attention Mechanism

**FlashAttention**: The recently proposed FlashAttention layer reduces the number of read/write operations between the GPU HBM (high bandwidth memory, i.e. long-term memory) and the GPU SRAM (i.e. short-term memory)  (see also ). We modified the FlashAttention module built by Hazy Research with OpenAI's triton library in order to flexibly incorporate ALiBi .4

**Attention with Linear Biases (ALiBi)**: In most BERT models, the positions of tokens in a sequence are encoded using learned position embeddings, which are added to the learned token embeddings at the start of the forward pass. ALiBi eliminates position embeddings and instead encodes position information directly through the attention operation . It adds a negative bias to the attention score between each token pair, which grows linearly with the relative distance between the tokens. Intuitively, this biases attention to nearby tokens and allows for extrapolation to context lengths longer than those used for training .

Following the notation in , the attention block computes the attention scores between the \(i\)th query \(q_{i}^{d}\) and keys \(^{L d}\) where \(d\) is the head dimension and \(L\) is the sequence length. ALiBi adds a fixed bias with \(m\) as a head-specific slope controlling how the bias grows with absolute distance between tokens, yielding attention weights as

\[q_{i}^{}-m([i-1,i-2,...,i-L]).\] (1)

The slopes \(m\) follow a geometric sequence such that for \(n\) heads, each head has a ratio of \(2^{-8/n}\).

During finetuning or inference, the static bias can be increased to accommodated longer sequence lengths. For example, a model pretrained using ALiBi with a maximum sequence length of 128 tokens can then extrapolate to a task with 256 tokens with little to no decrease in zero-shot performance. This was the original motivation for AliBi (i.e. "train short, test long") . Since pretraining a model with a maximum sequence length of 128 has much higher throughput than pretraining a model with a sequence length of 256, ALiBi can be considered an indirect speedup method.

#### 2.2.2 Modifications to the Feedforward Layers

**Gated Linear Units (GLU)**: We used Gated Linear Units for the feedforward sublayer of a transformer. GLUs were first proposed in 2016 , and incorporate an extra learnable matrix that "gates" the outputs of the feedforward layer (Figure 1A). More recent work has shown that GLUs can improve performance quality in transformers [50; 42]. We used the GeLU (Gaussian-error Linear Unit)5 activation function with GLU, which is sometimes referred to as GeGLU. The module can be described by the following equation

\[(_{1}) _{2},\] (2)

where \(\) is the input to the feedforward layer and the matrix \(\) gates the output of the GeLU activation function. The extra gating matrix in a GLU model potentially adds additional parameters to a model; we chose to augment our MosaicBERT-Base model with additional parameters due to GLU modules, as it leads to a Pareto improvement across all timescales. While BERT-Base has 110 million parameters, MosaicBERT-Base has 137 million parameters. Note that MosaicBERT-Base reaches higher accuracy faster than BERT-Base _despite having more parameters_ (Figure 1B). Similarly, BERT-Large has 340 million parameters, and MosaicBERT-Large has 430 million parameters.

#### 2.2.3 Additional Modifications

**Low Precision LayerNorm**: LayerNorm is a bandwidth-bound operation, which means that its speed depends on how quickly data can be loaded from memory to the compute units for element-wise operations. Typically the LayerNorm operation is set to float32 precision, which requires 4-bytes per-element. In MosaicBERT, we modify LayerNorm modules to run in bfloat16 precision instead of float32. This reduces the amount of data that needs to be loaded from memory, as only 2-bytes are required per-element. PyTorch's automatic-mixed-precision package does not, by default, run LayerNorm in lower precision because this can lead to numerical instabilities for certain models. However, our experimental results show that MosaicBERT does not experience any numerical instabilities with bfloat16 precision LayerNorm.

**Unpadding**: Standard NLP practice is to combine text samples of different lengths into a batch, and pad the sequences with special padding tokens so that all sample sequence lengths are the same (Figure 1A). During training, however, this leads to many wasted operations on the padding tokens. In MosaicBERT, we take a different approach and instead concatenate all the examples from a minibatch into a single sequence of batch size 1. Results from NVIDIA6 and others have shown that this approach leads to speed improvements during training, since operations are not performed on padding tokens .

**MLM Masking Ratio and Dropout**: We used the standard Masked Language Modeling (MLM) pretraining objective. While the original BERT paper also included a Next Sentence Prediction (NSP) task in the pretraining objective, subsequent papers have shown this to be unnecessary [37; 28]. For our BERT baselines, we used the standard 15% masking ratio. However, we found that a 30% masking ratio led to slight accuracy improvements in both pretraining MLM and downstream GLUE performance. We therefore included this simple change as part of our MosaicBERT training recipe. Recent studies have also found that this straightforward change can lead to downstream improvements [63; 1].7

For the baseline BERT, we applied the standard 0.1 dropout to both the attention and feedforward layers of the transformer block. For MosaicBERT, however, we applied 0.1 dropout to the feedforward layers but did not apply dropout to the FlashAttention module, as this was not possible with the OpenAI triton implementation .8 The removal of dropout in the attention layers also leads to a small speed up.

### Pretraining Optimizations for Both MosaicBERT and the BERT baseline

**Data**: Pretraining data is an important factor when comparing BERT models; while the original BERT study trained on English Wikipedia and BookCorpus , subsequent models such as RoBERTa trained on much larger datasets (e.g.  trained on 160 GBs of text while  only trained on 16GB of text). Here we chose to train all models on the more modern Colossal Cleaned Common Crawl (C4) corpus . For all experiments, we used a maximum sequence length of 128 tokens. Note that in our implementation, samples longer than 128 tokens were simply truncated.

**Streaming Dataset**: As part of our efficiency pipeline, we converted the C4 dataset to the StreamingDataset format9 and used this for both MosaicBERT-Base and the baseline BERT-Base. This ensured that our wall clock time measurements were not hindered by data streaming.

**Bfloat16 Precision**: We use bfloat16 mixed precision training for all the models. bfloat16 is a custom 16-bit floating point format for machine learning that has one sign bit, eight exponent bits, and seven mantissa bits, and has the dynamic range of float32. For mixed precision training, a matrixmultiplication layer uses bfloat16 for the multiplication and 32-bit IEEE floating point (float32) for gradient accumulation. We found this to be more stable than using float16 mixed precision.10

**Vocab Size as a Multiple of 64**: We increased the vocab size to be a multiple of 64 (i.e. from 30,522 to 30,528).11 This small constraint is something established in the MEGATRON work by Shoeybi et al. , and leads to a non-trivial throughput speedup, as CUDA is more efficient at computing matrix multiplications with these dimensions.12 Across all experiments, we use the standard BERT-Base and Large tokenizers.

**Pretraining Hyperparameters**: For all models, we use a global batch size of 4096, and microbatch size of 128. We set the maximum sequence length during pretraining to 128, and we used the standard embedding dimension of 768. These hyperparameters were the same for MosaicBERT-Base and the baseline BERT-Base. More hyperparameter details are included in the Appendix.

## 3 Results

In our first set of experiments, we pretrained BERT-Base and MosaicBERT-Base for 70,000 steps of batch size 4096, which roughly corresponds to 78% of English C4 (Figure 1B and Figure 2). We ran experiments with \(n=5\) pretraining seeds for each model class. We then finetuned these models on the GLUE benchmark suite using identical finetuning parameters for all models and experiments; the details can be found in the Appendix.

### MosaicBERT-Base Achieves 79.6 Average GLUE (dev) Score in 1.13 Hours

MosaicBERT-Base achieves the downstream average GLUE (dev) score of 79.6 in 1.13 hours on \(8\)A100 80 GB GPUs at a cost of roughly $20 on a standard cloud provider. More details on cost estimates are included in the Appendix.

Figure 2: Performance on individual GLUE (dev) finetuning tasks. Our MosaicBERT-Base consistently outperforms BERT-Base on MNLI-m, QNLI, QQP and RTE, and has comparable performance on CoLA, SST-2, MRPC and STSB. Wall clock time is for \(8\)A100-80GB GPUs, and does not include finetuning time. Error bars are plotted with 95% confidence interval across \(n=5\) pretraining seeds, and all models are trained for 70,000 steps with batch size 4096.

The baseline BERT-Base reached an average GLUE (dev) score of 83.2% in 11.5 hours (A100-80GB), while MosaicBERT reached the same accuracy in roughly 4.6 hours on the same hardware, which is roughly a \(2.38\) speedup (Table 1 and Figure 1B).

### MosaicBERT-Base is Pareto Optimal

As can be seen Figure 1B, MosaicBERT-Base consistently achieves higher average GLUE accuracy more quickly than the standard BERT-Base across all training durations. The performance of MosaicBERT on individual GLUE finetuning tasks can be seen in Figure 2. MosaicBERT-Base outperforms BERT-Base in four out of eight GLUE tasks across pretraining durations.

The space of NLP benchmarks and tasks has exploded in recent years; we include more information on the individual tasks in the classic GLUE benchmark in the Appendix. MNLI, QNLI and QQP are the largest datasets in the GLUE benchmark, with 100k-400k training examples, and MosaicBERT-Base is strictly Pareto-optimal for these tasks relative to BERT-Base (Figure 2). We interpret this to mean that the architectural changes and training recipe we chose resulted in an optimized, efficient BERT-Base model.

The quality of both models on smaller datasets (3k-67k training samples) is much more variable, as shown by the large error bars (standard deviation across n=5 pretraining seeds) for SST-2, MRPC and STSB. Regardless of this variation, MosaicBERT-Base performs equivalently to BERT-Base on these tasks across training duration.

### MosaicBERT-Large is Pareto Optimal

While BERT-Base is one of the most popular BERT models, BERT-Large comes in a close second. All of our model development was done on MosaicBERT-Base; we were therefore curious whether our architecture and pretraining choices also generalized to a larger model.

In a second set of experiments, we pretrained MosaicBERT-Base and Large as well as BERT-Base and Large for two epochs of the C4 dataset. The training duration is 178,000 steps with batch size 4096, and is more than twice as long as the duration of the models in Figures 1 and 2.

Figure 3: Average GLUE (dev) score Pareto curves for MosaicBERT-Base and Large trained for roughly 2 epochs of C4 (i.e. 178,000 steps with batch size 4096 with maximum sequence length 128 tokens). MosaicBERT-Base and Large are Pareto optimal relative to BERT-Base and Large. All pretraining is done on \(8\)A100 80GB devices (n=2-3 pretraining seeds). Note that BERT-Base and MosaicBERT-Base took much less time to train than BERT-Large and MosaicBERT-Large.

BERT-Large has 24 repeated transformer layers (BERT-Base has 12), and as a result consumes much more memory and takes longer to train. We found that MosaicBERT-Large reached an average GLUE score of 83.2 in 15.85 hours, while BERT-Large took 23.35 hours (Figure 3). MosaicBERT-Large therefore had a \(1.47\) speedup over BERT-Large in this training regime.

While MosaicBERT-Base is optimal for a constrained budget, the MosaicBERT-Large average GLUE score eventually surpasses MosaicBERT-Base after \(25\) hours of training on a 8\(\)A100-80GB node, and reaches an average score of 85.5 in roughly \(50\) hours. In our experiments, the MosaicBERT-Large architecture and pretraining was the same as MosaicBERT-Base, outside of the number of attention heads, number of hidden layers, and intermediate size of the feedforward units.

A striking result from Figures 3 and S2 is that MosaicBERT-Base is Pareto optimal relative to BERT-large in this training regime, and is Pareto optimal to MosaicBERT-Large during the first half of training. MosaicBERT-Base takes only 25 hours to complete 2 epochs, while MosaicBERT-Large takes close to \(70\) to complete 2 epochs. A potential takeaway from this is that MosaicBERT-Large only surpasses Base performance in the large data regime. For certain tasks such as QQP, SST-2, and MRPC MosaicBERT-Base achieves a maximum accuracy on par with the maximum accuracy of MosaicBERT-Large, for far fewer pretraining hours. When building encoders for specific domains and tasks, bigger is not always better.

### Ablation Experiments

When deciding on the final setup for MosaicBERT, we tried to make educated guesses about what combination of modifications could work well together to both increase FLOP utilization and maintain or improve accuracy. Our approach was inspired by Amdahl's Law, i.e. the idea that optimizing a portion of a system responsible for N% of the costs can lead to, at most, an N% speedup , and chose each of the modifications such that they did not target the same exact part of the of the stack.

Figure 4: Ablation Experiments (A) Average GLUE score and (B) Individual GLUE tasks. **BERT-base**: standard BERT-base (110M parameters) with attention dropout=0.1 and feedforward dropout=0.1, vocab size set to 30522, MLM=15% (all Hugging Face standard configurations). **BERT+drpt=0**: standard BERT-base, except that the attention in the dropout layer is set to 0 instead of the default 0.1. **BERT+GLU**: standard BERT-base, with GLU for the feedforward component of the encoder block. **BERT+lpLN**: standard BERT-base, except with low precision LayerNorm (bfloat16). **BERT+mlm30**: standard BERT-base, except with a masked language modeling masking ratio of 30%. **MosaicBERT**: the complete MosaicBERT-Base including GLU (where the dimension of the intermediate layer is 3072 resulting in 137M total parameters), ALiBi, low precision LayerNorm, unpadding, MLM 30%, vocab size 30528 (a multiple of 64) and the attention dropout=0. **MosaicBERT-FlashAttn+drpt=0.1**: MosaicBERT-Base _without_ Flash Attention and _with_ the attention dropout set to 0.1.

For example, low precision LayerNorm and GLU affect different parts of the encoder block, and so potentially compose well.

In the following ablation experiments we benchmark the individual effects of various architecture and training choices on pretraining wall clock time. We plot downstream GLUE accuracy as a function of measured pretraining wall clock time in Figure 4.

The patterns in Figure 4A-B shed light on the individual effects of various architectures (e.g. BERT+GLU, BERT+low precision LayerNorm) and training configurations (e.g. BERT + 30% masking ratio). On average, all methods seem to provide a slight accuracy boost to BERT-base. Increasing the masking ratio to 30% leads to a slight accuracy boost while not affecting the WCT, while turning off dropout in the attention layer (BERT+drpt=0) leads to a slight improvement in both accuracy and WCT. Low precision LayerNorm (BERT+plLN) leads to a significant speedup (i.e. a shift to the left). Gated Linear Units (BERT+GLU) add more parameters to the model and lead to a significant slowdown while providing an accuracy boost. As a point of reference, we also benchmark the full MosaicBERT as well as MosaicBERT without FlashAttention and with attention dropout set to 0.1 (the standard BERT-base configuration).

In these experiments, all BERT-Base models here were pretrained on C4 for 70,000 steps with batch size 4096, and microbatch size 256 on 8\(\)A100 80GB GPUs. All models were initialized with the same seed and shared all other hyperparameters including the bert-base uncased tokenizer, the learning rate schedule, AdamW as the optimizer, etc. Final pretraining checkpoints were then finetuned on GLUE following the details in the appendix. The points represented in these GLUE plots are final finetuning checkpoints.

These plots highlight the importance of benchmarking with Pareto curves, as it is not possible to tell from these plots alone whether training BERT-base for 2 more hours leads to better performance than BERT+GLU, for example.

## 4 Related Work

Various studies rightly focus on a single method or modification that improves throughput, such as FlashAttention  and unpadding . In this study, we incorporate many of these techniques to investigate whether they combine advantageously.

RoBERTa ("Robustly optimized BERT approach") is the most influential work in this regard . In this study, they kept the exact BERT architecture but changed the training recipe by removing the next sentence prediction objective, training for longer on much larger datasets, and changing the batch size, among other things. They showed that the original BERT was significantly undertrained - while the original BERT trained on 16GB worth of text, the top accuracy RoBERTa (Large) was trained on 160GB of data for 500,000 steps with batch size 8192. Many of the training choices in RoBERTa have become standard practice; our training recipe therefore more closely resembles RoBERTa than the original BERT. See Appendix for a more detailed comparison of RoBERTa and MosaicBERT.

Improvements in transformer architecture and GPU hardware have caused the cost of pretraining BERT models to decline precipitously. The recent paper "Cramming: Training a Language Model on a Single GPU in One Day"  exemplifies this trend. The goal of this rigorous study was to train the best BERT in 24 hours on a single GPU. Similar to us, they tweaked the BERT architecture to incorporate FlashAttention and Gated Linear Units (but without increasing the dimensionality of the hidden block). Unlike MosaicBERT, they used scaled sinusoidal positional embeddings  as well as Pre-LayerNorm (applying LayerNorm before the attention and feedforward layers) and did not change the pretraining masking rate. With this setup, they were able to train their modified BERT-Base to an average GLUE score of 80.4 in 24 hours on a single A6000 GPU (i.e. 24 GPU hours, as per Table 3 of ). Our study is similar in spirit, but asks what is the fastest architecture for pretraining, and expands on this in greater detail by showing that MosaicBERT is Pareto optimal relative to BERT.

While we believe that our training optimization choices for MosaicBERT go a long way to improve BERT training efficiency, there are likely further modifications that could lead to an increase in accuracy with no effect on throughput, such as replacing LayerNorm with RMSNorm  and GeGLU with SwiGLU [50; 42]. Removing all biases from the linear layers can also potentially lead to slight speed ups in training time without a significant hit to accuracy. "NarrowBERT" suggested a clever change to the way encoder blocks are stacked so that computation is not wasted on unmasked tokens. Another recent study showed that dynamically masking the masking ratio during pretraining leads to downstream accuracy gains in BERT . Note, however, that accounts such as  and  document many examples where certain modifications to the BERT architecture and training recipe fail to improve accuracy. We also note here that there is an ongoing debate about the benefits of RoPE (rotary positional embeddings)  versus AliBi . Finally, using a more modern tokenizer will likely have an effect on downstream accuracy .

Approaches such as knowledge distillation  might additionally push the Pareto frontier of BERT models during pretraining and finetuning. Progressive layer stacking is one example technique; by initializing a larger BERT model with the weights from smaller pretrained BERT models, pretraining can be reliably accelerated . As the field is learning how to optimize stable model training at the billion parameter scale , we expect some of these innovations to cycle back to smaller models such as BERT-Base. For example, it has been hypothesized that incorporating more LayerNorm modules at the QK matrix output (i.e. QK-LayerNorm ) can lead to improved stability during training; combining this with a more aggressive learning rate schedule could lead to faster convergence.

## 5 Conclusion

In this study we show how combinations of architecture choices can improve BERT pretraining speed and accuracy. We built MosaicBERT to enable ML researchers and engineers to pretrain BERT models from scratch on their own data and build better models for their specific domains without facing time and cost restrictions. We ultimately hope that by making BERT training faster and cheaper, our work contributes to the trend in NLP research away from finetuning generic models and towards of pretraining custom encoders on domain specific data. A large body of research has highlighted the success of BERT-style models pretrained on a specific domains such as biomedicine , math , chemistry , financial communications , and code . This seems to hold true even in the age of LLMs .

#### Code

A stable webpage for this work can be found at mosaicbert.github.io. Code for pretraining and finetuning MosaicBERT can be found in the MosaicML examples repository https://github.com/mosaicml/examples. The exact code for this study was pinned to v0.0.4 of the MosaicML mosaicml/examples repository https://github.com/mosaicml/examples/tree/v0.0.4/examples/bert. All pretraining and finetuning was done in PyTorch 1.13 using the MosaicML Composer library https://github.com/mosaicml/composer. Model weights for MosaicBERT-Base can be found on the HuggingFace hub https://huggingface.co/mosaicml/mosaic-bert-base.

Code for pretraining and finetuning transformers such as MPT and derivative versions of MosaicBERT can be found in the MosaicML llm-foundry repository https://github.com/mosaicml/llm-foundry.

#### Acknowledgements

The authors would like to thank Erica Yuen, Vitaliy Chiley, Connor Jennings, and Aaron Gokaslan for their feedback on the code and manuscript. We would also like to thank Ishana Shastri and Vlad Ivanchuk for some of their early work as interns that led to MosaicBERT. The majority of this work appeared as a blogpost with the title "MosaicBERT: Pretraining BERT from Scratch for $20" in March 2023.13 This work also builds on the MosaicML MLPerf v2.1 results for BERT-Large .