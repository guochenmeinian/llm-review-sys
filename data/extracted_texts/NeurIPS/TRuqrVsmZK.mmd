# BayesTune: Bayesian Sparse Deep Model Fine-tuning

Minyoung Kim\({}^{1}\)

\({}^{1}\)Samsung AI Center Cambridge, UK

mikim21@gmail.com &Timothy Hospedales\({}^{1,2}\)

\({}^{2}\)University of Edinburgh, UK

t.hospedales@ed.ac.uk

###### Abstract

Deep learning practice is increasingly driven by powerful foundation models (FM), pre-trained at scale and then fine-tuned for specific tasks of interest. A key property of this workflow is the efficacy of performing sparse or parameter-efficient fine-tuning, meaning that by updating only a tiny fraction of the whole FM parameters on a downstream task can lead to surprisingly good performance, often even superior to a full model update. However, it is not clear what is the optimal and principled way to select which parameters to update. Although a growing number of sparse fine-tuning ideas have been proposed, they are mostly not satisfactory, relying on hand-crafted heuristics or heavy approximation. In this paper we propose a novel Bayesian sparse fine-tuning algorithm: we place a (sparse) Laplace prior for each parameter of the FM, with the mean equal to the initial value and the scale parameter having a hyper-prior that encourages small scale. Roughly speaking, the posterior means of the scale parameters indicate how important it is to update the corresponding parameter away from its initial value when solving the downstream task. Given the sparse prior, most scale parameters are small a posteriori, and the few large-valued scale parameters identify those FM parameters that crucially need to be updated away from their initial values. Based on this, we can threshold the scale parameters to decide which parameters to update or freeze, leading to a principled sparse fine-tuning strategy. To efficiently infer the posterior distribution of the scale parameters, we adopt the Langevin MCMC sampler, requiring only two times the complexity of the vanilla SGD. Tested on popular NLP benchmarks as well as the VTAB vision tasks, our approach shows significant improvement over the state-of-the-arts (e.g., 1% point higher than the best SOTA when fine-tuning RoBERTa for GLUE and SuperGLUE benchmarks).

## 1 Introduction

Practical deep learning increasingly relies on a workflow of fine-tuning pre-trained foundation models for the specific task of interest to be solved. The foundation models (FM)s are often pre-trained at scale using large architectures and self-supervised objectives on large scale unlabelled data, allowing them to encode a great deal of general prior knowledge. Fine-tuning then specialises these general purpose FMs to the specific task of interest. The key challenge then becomes how to conduct fine-tuning so as to balance the conflicting goals of adapting to the task at hand; while retaining the knowledge obtained during foundational pre-training and avoiding overfitting on the (typically smaller) downstream training set. This is addressed from a variety of perspectives from careful learning rate scheduling to regularised fine-tuning [26; 12]. However, the mainstream approach in practice is largely based on some instantiation of sparse fine-tuning.

Sparse fine-tuning, also known as parameter efficient fine-tuning (PEFT) [23; 43; 14; 24; 22; 15; 16; 18; 45; 13; 10], manages the adaptation/forgetting trade-off by selectively updating a specific subset of parameters in the FM while keeping others frozen. These approaches also benefit from good parameter scalability with respect to the number of downstream tasks to solve since each downstreamtask only requires storing a small number of parameters compared to the full FM. A large number of sparse/PEFT approaches have now been proposed, based on either additive or mask-based principles. Additive approaches [16; 15] inject a hand-chosen set of additional small modules to be learned, while keeping the original FM entirely frozen; while mask based approaches [43; 18] manually select a sparse subset of original FM parameters to consider as learnable - such as the prompt parameters in transformers . Despite substantial engineering effort, prior sparse fine-tuning work mainly relies on human intuition and ad-hoc heuristics to specify additive modules and sparse fine-tuning masks. This raises the question _how can we automatically select which parameters to insert or update in a principled and effective way?_

In this paper, we address this issue by proposing an automated sparse fine-tuning method. Our BayesTune framework automatically discovers the parameters to update, with minimal human intervention. In contrast to some early concurrent attempts that rely on heuristics and/or heavy approximations [45; 13; 10], BayesTune is highly principled, underpinned theoretically by Bayesian marginalization of parameter uncertainty. Specifically, we place a sparse Laplace prior on the FM parameters, and the posterior inference of the sparse scale parameters reveals which parameters are most important to update from their initial values. BayesTune is efficiently implemented with Langevin MCMC, which only requires twice the cost of vanilla SGD. We demonstrate BayesTune with both RoBERTa transformer on (Super)GLUE benchmark suite and Vision Transformer on VTAB benchmark suite. BayesTune can achieve a specified sparsity level, and provides a reasonable way to identify a good sparsity/performance trade-off if a sparsity constraint is not pre-defined. Finally, we show that BayesTune can work with both mask-based search spaces to identify existing FM parameters to update by fine-tuning, as well as additive search spaces to identify which new modules to add where for additive adaptation.

## 2 Problem Setup and Motivation

First we introduce some key notations as follows:

\[ =()\] \[ =()\] \[D =\]

We denote by \(_{i}\) (\(\)) the \(i\)-th element of \(\) by considering \(\) as a whole vector of concatenated parameters, i.e., \(=[_{1},,_{d}]^{}\) where \(d\) is the number of parameters (so, \(_{i}\) is scalar). Before we start, we clarify the goal of the sparse fine-tuning formally as follows.

**Desiderata.** We aim to find \(\) which performs well on the downstream task, and at the same time remains close to \(\) sparsely in \(L_{0}\) sense (i.e., \(||-||_{0}=_{i=1}^{d}(_{i} _{i})\) is small). Roughly speaking, \(_{i}=_{i}\) for many \(i\)'s. Oftentimes we define and set the sparsity level \(p\), meaning that the proportion of the FM parameters _updatable_ from the pre-trained values is no greater than \((100 p)\%\) (i.e., \(||-||_{0} d p\)).

**About the sparsity level \(p\).** We remark here that the sparsity level \(p\) is often given by the users, for instance, in case we have a strict pre-specified memory constraint (e.g., embedded platforms), where the allowable extra space for saving the updated new parameters from \(\) for the downstream task is only \((100 p)\%\) of the original FM storage. In other situations, it would be ideal if the sparse fine-tuning algorithm can estimate optimal \(p\) that trades off the downstream accuracy against memory overhead (e.g., find the smallest \(p\) that enables a certain level of accuracy guarantee). Unfortunately most existing sparse fine-tuning methods are not capable of this feature, and rather resort to time-consuming cross-validation-type search. Although this latter case opens up a new interesting research problem to pursue further, our proposed approach offers reasonable and principled criteria for estimating the optimal \(p\) (See Sec. 3 for details).

**How about layer-wise selection?** We also note that our approach is not tied to the layer-wise parameter treatment, namely selecting a few layers to be updated with the rest layers frozen, which is a popular strategy adopted in many sparse fine-tuning methods. The layer-wise treatment is based on the conjecture that whether the parameters important to the downstream task are deviated from the pre-trained ones or not, is determined in a layer-wise all-or-nothing manner. Although this is a reasonable assumption, our approach does not rely on the assumption, and is able to discover optimal sparse updates automatically by inspecting individual parameters over all layers.

Back to our notations, the downstream task must be associated with the relevant loss function \(l(;z)\) with \(z D\) for fine-tuning. Here the task could be either supervised (i.e., \(z=(x,y)\) for a pair of input \(x\) and its target label \(y\)) or unsupervised (\(z=x\) with input \(x\) alone). In our model the loss function is turned into a likelihood model by the conventional trick, \(p(z|)(-l(;z))\). Although our approach can deal with both supervised and unsupervised cases seamlessly, we predominantly focus on the supervised classification case, that is, we define the data likelihood \(p(y|x,)(-l(;x,y))\) for \((x,y) D\), where \(l(;x,y)\) is typically the cross-entropy loss.

## 3 (Proposed) Bayesian Sparse Fine-tuning Model

We propose a (hierarchical) Bayesian model to tackle the downstream prediction task of interest. Specifically, we treat the FM parameters \(\) as _random variables_ and the downstream training data \(D\) as _evidence_. To encourage parameter sparsity, namely most parameters \(_{i}\) remain at the pre-trained \(_{i}\), we impose the Laplace prior \(p(|)\),

\[p(|)=_{i=1}^{d}p(_{i}|_{i})= _{i=1}^{d}(_{i};_{i},_{i} )=_{i=1}^{d}}(-|_{i}-_{i}|/_{i})\] (1)

where \((x;,b)=(-|x-|/b)\) is the (univariate) Laplace distribution with mean \(\) and scale \(b\). Thus we fix the prior means as the pre-trained values \(\), and only model the scale parameters which are newly introduced random variates denoted by \(\) (\(>0\)). Note that every single parameter \(_{i}\) (scalar) is associated with its own prior scale \(_{i}\) (thus \(d\) scale variables in total). Another thing to note is that the final layer (also known as the readout head) of the FM is often completely replaced by a random one for the downstream task. Since this final layer needs to be learned from the scratch with the downstream data, we do not place the Laplace prior on the parameters of the readout head.

Through the scale variables \(\) of the Laplace distributions, we can express our (prior) preference to the degree of parameter deviation from the pre-trained values: small \(_{i}\) leads to a peaky Laplace around its mean, penalizing even small deviation of \(_{i}\) from \(_{i}\); on the other hand, large \(_{i}\) makes it flat, being less sensitive to deviation, allowing \(_{i}\) to take values freely away from \(_{i}\). By carefully choosing \(_{i}\) values, we can balance effectively between _overfitting_ (too much deviation) and _underfitting_ (too little deviation). Instead of choosing \(_{i}\)'s manually, our idea is to learn them automatically from data, through the principled Bayesian inference: given the evidence (i.e., the downstream data \(D\)) we infer the most probable values of \(_{i}\)'s that best explain the evidence, while being small enough to lead to sparse updates overall.

To this end, we regard \(\) as random variables and impose hyper-prior on \(\) (hierarchical Bayes). As we prefer sparse updates in the end, the hyper-prior needs to put higher mass/density on small \(\) values. We adopt the Gamma distribution for this purpose, more specifically,

\[p(|,)=_{i=1}^{d}p(_{i}|,)= _{i=1}^{d}(_{i};,)\] (2)

where \((x;,) x^{-1}(- x)\) for \(,>0\). Note that although we can introduce individual \(_{i}\)-specific parameters (i.e., \((_{i},_{i})\)), we instead use a single \((,)\) shared over all \(_{i}\)'s for simplicity. Since the mode of Gamma is 0 if \(<1\) and the variance is \(/^{2}\), we use small \(<1\) and large \(\) to enforce a priori small \(\) values. In our experiments we choose1\((\!=\!0.01,\!=\!100)\) for both the NLP and VTAB vision benchmarks.

Our full model can be written as the following joint distribution:

\[p(D,,|,)=(_{i}; ,)}{p(|,)}_{=_{i}(_{i};_{i},_{i} )}\] (3)

With this model, our ultimate goal is to infer the posterior distribution of the network weights and scales \(p(,|D)\), namely

\[p(,|D,,)=\] (4)We adopt the stochastic-gradient MCMC approach [40; 7; 4] to obtain samples from the posterior, especially the Langevin dynamic method (SGLD) , which amounts to running the following recurrence to collect posterior samples (after some burn-in steps):

\[[,]\;\;[,]+ p (|,)+ p(|)+}{|B|} p(B| )+\] (5)

where \(B\) (\(\!D\)) is a minibatch, \(\) is small step size, and \(\!\!(0,I)\). In standard SGLD, \(=|D|\) (training data size) and \(=1\). However, we allow them to take values different from these default ones: \(\) is the _effective_ training data size to account for data augmentation2 (e.g., if there are 5 different augmentation strategies and each allows 10 different variations, the effective data size can be \(\!=\!10^{5}\!\!|D|\)); the other hyperparameter \(\) can be used to discount the noise effect3, which is validated in the range \(10^{-4:0}\) in our experiments. See Sec. 5.1 for more details. Note that in the parentheses subject to derivatives, the first two terms admit closed-form gradients while the gradient of the last term can be computed by the conventional SGD backprop. Thus each step in (5) is efficient, requiring at most only two times the complexity of the vanilla SGD step.

After a burn-in period, we can maintain those samples \((,)\) to approximate \(p(,|D)\). For instance, the running average of the \(\) samples, denoted by \(\), is a good estimate of the mean of the marginal posterior \(p(|D)\). Since we imposed the hyper-prior that encourages small \(\), ideally the majority of the scale parameters tend to remain small _a posteriori_, whereas there will be only a few large-valued \(_{i}\)'s corresponding to those FM parameters that crucially need to be updated away from their pre-trained values to explain the data \(D\). Roughly speaking, \(_{i}\)s indicates how important it is for the corresponding FM parameters \(_{i}\)s to be updated away from their pre-trained values \(_{i}\) on the downstream data. Importantly, this \(\) can be used in the next stage (described in the next paragraph) to _decide_ which network weights \(_{i}\) need to be frozen (those with small \(_{i}\)s) and which need to be updated from the pre-trained (those with large \(_{i}\)s).

**Thresholding scales and the second stage.** Although we adopted the sparsity-inducing Laplace prior, it is not necessarily the case that the majority of posterior \(\) values are sharply staying at the pre-trained values \(\). We may need some thresholding: If \(p\) is given, then we can directly take the top \((100 p)\%\) of them. Otherwise we examine for cut-off point - Those \(i\)'s with small posterior means \(_{i}\) are considered as frozen weights (i.e., \(_{i}\!=\!_{i}\)), while those \(i\)'s with large \(_{i}\)s can be treated as updatable parameters. In practice, we sort and plot the \(_{i}\)s to eyeball and find a reasonable cut-off point. One can also use this \(\)-plot to decide a reasonable sparsity level \(p\) for good accuracy-memory trade-off (See Fig. 1 for illustration). Once we have decided which parameters are updated and frozen, we can run vanilla SGD to train the updatable parameters of the FM, which forms our second stage.

Our overall algorithmm dubbed **BayesTune**, is summarized as pseudocodes in Alg. 1.

## 4 Related Work

### Comparison to Existing Sparse Fine-tuning Methods

The recent sparse/PEFT fine-tuning approaches broadly fall into the following three categories: 1) Heuristic search criteria, 2) Attaching small extra modules to the FM (e.g., adapter-based ), and 3) Directly formulating the sparse fine-tuning problem as an optimization problem.

**1) Heuristic search criteria.**

* Randomly select \((100 p)\%\) parameters to update.
- After training the FM model with full update, a random \((100(1-p))\%\) parameters are reset to the pre-trained values.
* Update only the bias parameters with the rest frozen.
* Take the largest \((100 p)\%\) in magnitude as updatable parameters.

Compared to our proposed "BayesTune", these heuristic search methods are less principled, unable to explain why the resulting models should perform well on the downstream tasks

**2) Approaches that attach small extra modules to FM.** They do not search for a subset of FM parameters to update, but attach some small extra modules to the FM (e.g., insert a small MLP module after each layer). All of the FM parameters are frozen, and they only train those attached modules. So it can be seen as sparse fine-tuning considering that the size of the new modules is relatively small compared to that of the FM.

* A small bottleneck MLP is inserted at the end of each layer of the FM.
* A low-rank projection weight matrix is inserted before the query/key/value embeddings in each layer.
* A set of learnable tokens is inserted in the input of a Transformer attention block in each layer.

Figure 1: (**Left**) The \(\)-plot in our BayesTune after stage-1 training. This is with EfficientNet-B0 on the Oxford-Pets dataset. After sorting the posterior mean \(_{i}\) values, we plot several sparsity levels \(p\) and the corresponding \(\) values. To decide a reasonable sparsity level \(p\) for good accuracy-memory trade-off, we illustrate two candidates: Cand-1 with more sparse updates (\(p=0.05\)) but many parameters with large \(_{i}\)’s (e.g., \(0.1\!\!0.3\)) are forced frozen; Cand-2 with less sparse updates (\(p=0.1\)) but ensures that those fixed parameters have indeed small \(_{i}\)’s (e.g., \(<0.1\)). (**Right**) Test errors in stage-2 at different cutoff points in our BayesTune (red curve). Those two candidate points (Cand-1 and Cand-2) are highlighted and superimposed, consistent with the left figure. The more sparse Cand-1 has slightly higher error than the full-update, while less sparse Cand-2 achives the best test performance among all other choices. It exhibits nice accuracy-memory trade-off. Further increasing \(p\) (being far more dense) incurs overfitting.

- A NAS-based approach (e.g., learning a supernet followed by evolutionary search for a downstream tasks specific subnet) that aims to search for the optimal combination of the above three methods.

Compared to our "BayesTune", these adapter-based models involve the difficult problem of where to place the extra modules and what would be the optimal module dimensions/sizes, which is essentially a very difficult combinatorial search problem, often requiring heuristic search methods like the evolutionary search.

**3) Directly optimizing the sparse optimization problem.** One can formulate the sparse fine-tuning as an optimization problem, for instance,

\[_{}_{(x,y) D}[l(;x,y)]\ \ \ \ ||- ||_{0} d p.\] (6)

Unfortunately this problem becomes a discrete optimization due to the \(L_{0}\) cardinality constraints, thus difficult to solve in general. Some recent approaches aimed to relax the problem to continuous optimization, but relying on heavy approximation.

* They relax the discrete problem into a continuous one by introducing Bernoulli variables indicating update or freeze, while the reparametrized sampling is further approximated by the Gumbel-softmax treatment.
* They iteratively train the full model parameters and calculate the projected mask to find the child network.
* They adopt several steps of second-order Taylor approximation of the loss function, leading to a very succinct parameter selection strategy: after computing the gradients of \(\) on the data \(D\), take the \((100 p)\%\) parameters with the largest gradient magnitudes.

Compared to our "BayesTune", these direct optimization strategies rely on strong assumptions and heavy approximation to relax the difficult discrete optimization.

### Relation to Existing (Hierarchical) Bayesian Sparse Model Learning

(Hierarchical) Bayesian models are well known in statistical machine learning, and there were some prior works that adopt the Bayesian approaches for sparse deep learning [21; 32; 35; 29]. However, as far as we know, our approach has several key differences from these previous works.

First, most prior works are all about sparse training, instead of _sparse fine-tuning_. Thus they focus on zeroing out many parameters, instead of retaining the pre-trained weights. To the best of our knowledge, our BayesTune is the first to tackle the sparse fine-tuning at the Foundation Model scale using the hierarchical Bayesian framework.

More importantly, the neural networks used in those previous studies are rather small/toy scale (mostly focusing on MLPs and LeNet-sized architectures up to ResNet-18 at the largest) while our method can obtain the state-of-the-art results on large-scale foundation models (ViT, RoBERTa). For example the largest model considered in those prior works, ResNet-18 contains \( 11\)M parameters vs. RoBERTa's \( 123\)M parameters, a \(10\) scale difference. The reason is that they adopt methods that entail extra memory cost like variational inference (VI), which impedes applicability to big networks. We have also done some experiments that compare the computational resources required by VI and SGLD: On ViT networks, the training time is increased by 1.7 times if we replace SGLD by VI; the GPU memory footprint is increased by 2.1 times.

## 5 Experiments

We test our BayesTune on two popular benchmark datasets from NLP and vision for the downstream fine-tuning tasks: (**language**) fine-tuning the pre-trained RoBERTa-base model  on the GLUE  and SuperGLUE  tasks; (**vision**) fine-tuning the ImageNet-22K  pre-trained ViT-B/16 model  on VTAB-1K  image classification/prediction tasks. The details of the experimental settings are discussed in the subsequent sections, Sec. 5.2 and Sec. 5.3.

### Implementation of BayesTune

For the SGLD sampling in Stage-1 of our BayesTune algorithm (re: Alg. 1), we have three different training regimes: the _warm-up_ phase that only runs vanilla SGD steps without considering the scale variables \(\) and random noise; followed by the _burn-in_ phase where the SGLD steps are performed as (5) but no (\(\),\(\)) samples are collected; followed by the _normal_ phase in which we do collect samples. Following the conventional practice, we also perform the _thinning_ steps  (i.e., collecting samples at a certain frequency) to mitigate undesired temporal correlation effects.

Another important implementation tip is the use of _reweighed cost terms_ in Langevin dynamics - As shown in (5), the cost function is involved with the training data size \(N\!=\!|D|\). Even though \(N\) is known, the data augmentation would considerably increase the _effective_ data size by several orders of magnitudes (e.g., if there are 5 different augmentation strategies and each allows 10 different variations, the effective data size can be \(10^{5} N\)). To account for it, we regard the effective data size as a hyperparameter denoted by \(\) that can be chosen from validation4. Replacing \(|D|\) by \(\) and taking it out of the parentheses in (5), we have the gradient (normalized by \(\)) multiplied by the step size \((/2)\). First we choose \(\) to match the learning rate in the neural net optimizer (e.g., Adam5), that is, \(=lr 2/\). Accordingly the noise term becomes \((2/( lr))^{1/2}\). We also find it effective oftentimes to discount the noise effect: we introduce the noise discount factor \(\) (another hyperparameter) by which the noise term is multiplied. We validate \(\) in the range \(10^{-4:0}\) (e.g., \(=1\) corresponds to the default no-discount case).

Throughout all experiments we use the Gamma prior parameters \((=0.01,=100)\), the scale variables \(_{i}\)'s are all initialized to \(0.0001\), and the learning rate for \(\) is \(0.01\) without scheduling. Other task-specific implementation details can be found in the subsequent sections. _The Python/PyTorch code to reproduce the results is available at https://github.com/SamsungLabs/BayesTune6_.

### NLP Benchmarks

We consider fine-tuning the pre-trained RoBERTa-base model , the large-scale foundation language model comprised of 125 million parameters, on several downstream tasks in GLUE  and SuperGLUE  benchmarks. We follow the experimental settings from , in which the original development sets serve as test sets, and the validation sets are formed by holding out random \(10\%\) of

  Method & CoLA & STS-B & MRPC & RTE & CB & OPA & WSC & AVG \\  Full update & \(58.36^{1.74}\) & \(89.80^{0.52}\) & \(89.55^{0.81}_{}\) & \(76.03^{1.14}\) & \(88.93^{2.37}_{}\) & \(67.70^{4.41}\) & \(53.10^{1.18}\) & \(74.78^{2.60}\) \\  Random & \(58.35^{1.05}\) & \(89.81^{}\) & \(88.73^{0.80}\) & \(72.71^{1.23}\) & \(90.54^{3.39}_{}\) & \(68.80^{1.64}\) & \(52.88^{5.97}\) & \(74.55^{2.46}\) \\  MixOut & \(58.66^{1.96}\) & \(90.15^{(1.97)}_{}\) & \(88.69^{0.60}\) & \(77.55^{1.64}_{}\) & \(86.51^{1.43}\) & \(71.30^{1.84}\) & \(52.98^{5.78}\) & \(75.12^{2.88}_{}\) \\  Bitfit & \(56.67^{1.45}\) & \(90.12^{1.04}\) & \(87.35^{0.58}\) & \(72.74^{2.47}\) & \(86.96^{2.0}\) & \(71.20^{3.79}\) & \(55.10^{5.39}\) & \(74.31^{2.43}\) \\  MagPruning & \(56.57^{2.47}\) & \(90.30^{1.14}_{}\) & \(88.09^{0.79}\) & \(73.53^{1.84}\) & \(81.25^{3.50}\) & \(71.50^{2.46}_{}\) & \(55.67^{2.73}_{}\) & \(73.85^{1.99}\) \\  Adapter & \(^{1.22}_{}\) & \(90.05^{0.13}\) & \(89.29^{0.60}_{}\) & \(76.93^{2.05}_{}\) & \(87.32^{0.62}\) & \(69.50^{2.54}\) & \(57.02^{3.27}_{}\) & \(76.03^{2.35}_{}\) \\  LoRA & \(60.88^{1.48}_{}\) & \(87.19^{0.51}\) & \(89.53^{0.62}_{}\) & \(76.97^{1.92}_{}\) & \(84.64^{3.76}\) & \(69.70^{2.83}_{}\) & \(56.84^{4.52}_{}\) & \(75.11^{2.24}\) \\  DiffPruning & \(58.53^{1.49}\) & \(89.59^{0.34}\) & \(78.79^{6.09}\) & \(69.93^{7.87}\) & \(86.25^{2.65}\) & \(72.10^{2.91}_{}\) & \(53.37^{3.60}\) & \(72.65^{3.57}\) \\  ChildPruning & \(60.00^{1.29}\) & \(89.97^{1.51}\) & \(87.19^{1.86}\) & \(75.76^{3.48}\) & \(86.61^{3.22}\) & \(69.40^{40.00}\) & \(55.59^{3.81}\) & \(74.93^{3.15}_{}\) \\  SAM & \(60.89^{0.96}_{}\) & \(^{1.14}_{}\) & \(88.84^{}_{}\) & \(76.79^{1.72}\) & \(88.93^{1.75}_{}\) & \(74.30^{2.48}_{}\) & \(59.52^{3.08}_{}\) & \(77.12^{1.51}_{}\) \\ 
**BayesTune** & \(60.85^{}_{}\) & \(90.40^{1.14}_{}\) & \(^{0.56}_{}\) & \(^{}_{}\) & \(^{}_{}\) & \(^{2.40}_{}\) & \(^{}_{}\) & \(^{}_{}\) \\  

Table 1: Results on NLP benchmarks. For each dataset/task (column), the average accuracy and the standard deviation (in superscript) shown over 10 runs with different random seeds are reported. The ranks (up to the fourth) among the competing methods are also shown in the brackets and in red. The figures of the competing methods are excerpted from .

the training sets. Instead of having a fixed number of training epochs, the development sets are used for an early stopping with the no-improvement tolerance of 40 epochs. We implement our BayesTune on top of the codebase of  which is based on the jiant framework (https://jiant.info/). Similarly as , we consider 7 tasks: Corpus of Linguistic Acceptability (**CoLA**) , Semantic Textual Similarity Benchmark (**STS-B**) , Microsoft Research Paraphrase Corpus (**MRPC**) , Recognizing Textual Entailment (**RTE**) [5; 11; 1], Commitment Bank (**CB**) , Choice of Plausible Alternatives (**COPA**) , and Winograd Schema Challenge (**WSC**) .

Our BayesTune is compared against strong baselines and the state-of-the-arts, including: **Random**, **Mixout**, **BitFit**, **MagPruning**[24; 22], **Adapter**, **LoRA**, **DiffPruning**, **Child-Pruning**[41; 31], and **SAM**, as discussed in Sec. 4. For all methods, we use the sparsity level \(p=0.005\) as suggested in . For the competing methods, we follow the hyperparameter settings from ; In BayeTune, we use \(10K\) warm-up steps, \(2K\) burn-in steps, and thinning at every \(100\) steps for all tasks. The batch size is 16, and the learning rate for the model parameters is \(10^{-4}\) for Stage-1 and \(10^{-3}\) for Stage-2. For the (dataset-dependent) evaluation metrics, we adopt the protocols in [37; 38]. We run all models on a single NVIDIA V100 GPU with 32GB memory. The results are summarized in Table 1 where means and standard deviations over 10 random seeds are reported.

First, it is quite surprising that the random selection strategies (**Random** and **Mixout**) are as effective as many sparse fine-tuning methods. This may be attributed to the massive over-parametrization of the deep foundation model. However, more careful and sophisticated selection is important as those three methods, our **BayesTune**, **SAM**, and **Adapter**, outperform the random strategies by large margin. Overall our **BayesTune** performs the best among the competing methods, about \(1\%\) point higher on average than the runner-up **SAM**. We achieve the first place on 5 out of 7 tasks, and also attain the smallest standard deviations, implying that the resulting sparse fine-tuned models are more robust and reliable. This result signifies the effectiveness of our sparse Laplace scale-based parameter selection strategy that is underpinned by the principled Bayesian posterior inference with efficient7 stochastic gradient Langevin sampling.

In Fig. 2, we also show the posterior mean scale \(\) vs. sparsity level \(p\). The cut-off threshold points corresponding to the sparsity level \(p=0.005\) are around \(_{th}[0.25,0.30]\) for all tasks except for **MRPC**. This cut-off scale range might be considered as reasonable choice as most of the parameters would have Laplace scales smaller than this range, exhibiting no pronounced indication of crucial deviation from the pre-trained values, although this is only conjecture based on our empirical results. Further rigorous theoretical analysis on the relation between the scale threshold and the generalization performance is beyond the scope of the current paper, and is left as future work.

### Vision Benchmarks

Next we test our BayesTune on the VTAB-1K , an image classification/prediction task adaptation benchmark suite comprised of 19 different image datasets. It contains images/tasks that exhibit

Figure 2: (NLP benchmarks) Posterior mean \(\) vs. sparsity level \(p\). The default sparsity level \(p=0.005\) is depicted as red dotted vertical lines. For each dataset, we plot the original \(p\) range (lower-left), which is magnified for the smaller range \(p[0,0.1]\) (upper-right).

highly diverse aspects/conditions such as: different image acquisition (by standard cameras or special-purpose ones for remote sensing or medical imaging), different objects/concepts (generic, fine-grained, or abstract), and tasks (object recognition, counting, or pose estimation). Each dataset in VTAB-1K consists of \(1K\) training examples, and we use the splits officially provided (train \(80\%\) and validation \(20\%\)).

We aim to fine-tune the ImageNet-22K  pre-trained ViT-B/16 model  on each dataset. We implement our BayesTune using the codebase from  while employing most of their hyperparameters without changes (e.g., the number of training epochs 100 and batch size 64). In our BayesTune, we use \(50\) warm-up epochs, \(10\) burn-in epochs, and thinning at every \(5\) batch steps for all datasets. The learning rate for the model parameters is \(10^{-3}\) and weight decay \(10^{-3}\) for both stages.

We first attempted the proposed method with our original setting, i.e., place the \(\) variables to the backbone (ViT) parameters, and let the Bayesian inference figure out the most critical parameters to update, namely those with large \(_{i}\)'s. Unfortunately the result was significantly worse than the adapter-based methods: VPT , Adapter , LoRA , and NOAH , for most datasets. As reported in several works in the literature, adapter-based models perform significantly better than the full backbone update for this benchmark (e.g., Table 1 in ). Along this line, we conjecture that this trend also holds for other sparse fine-tuning methods, namely the superiority of the adapter-based approaches to sparse backbone update strategies, particularly for this VTAB benchmark.

To fairly take the advantage of extra attachable modules as the adapter-based approaches, we attach extra modules to ViT for all three popular strategies: visual prompts in VPT , adapter modules in Adapter , and low-rank matrices in LoRA . This design idea is similar to (and motivated from) the neural prompt search in NOAH , but they formed a grid search problem to determine which layers to attach the adapter modules and how many module dimensions to attach - solved by the neural architecture search (NAS) technique with the super-net training, specifically adopting Autoformer , followed by running Evolutionary algorithms for optimal architecture search.

On the other hand, we add 10-dim full parameters for each module/layer, and they are all initialized to 0. We let the posterior inference in our BayesTune automatically figure out which of these extra modules parameters to be updated away from 0 and which to be frozen at 0. We used the zero-initialization because having all-0 extra modules can be considered to be identical to the original pre-trained ViT _without extra modules_, thus serving as _pre-trained_ extra modules. We only place \(\) variables to the extra modules (ViT parameters frozen)8. In summary, our setup is similar to NOAH  in that we take all three adapter strategies into account, but the main difference is: NOAH does a difficult super-net training and Evolutionary search to determine the optimal module-wise dimensions. On the other hand we do sparse selection of the module parameters to be updated from 0 across all 10-dim full modules.

   Method & \)} &The results are summarized in Table 2. We report the performance of the best sparsity levels found from \(p\{0.05,0.1,0.2,,0.9,1.0\}\). Although the final accuracies of our BayesTune are not always the best, we achieve Rank 1 on 7 out of 19 datasets and the second place on average over all datasets. The performance of BayesTune is comparable to other adapter-based methods, within the same range as those. That is, our main message is that _the adapter-based enhanced architecture fine-tuning can be done almost comparably well by the proposed Bayesian method, instead of relying on the heuristic search such as the Evolutionary Search or costly human expert design effort._ Compared to NOAH, our BayesTune results in more sparse adapter updates on average (0.38M vs. 0.43M). In Fig. 3 we also visualize the clustering of VTAB-1K datasets according to the sparse/dense adapter update effects: we assign each dataset into either of _sparse_, _dense_, and _in-between_ clusters according to the optimal sparse update level \(p\): small, large, medium, respectively. Such a grouping of the VTAB datasets based on fine-tuning sparsity characteristics has not been explored in the community, and we can easily obtain this finding using our BayesTune.

## 6 Conclusion

We introduced BayesTune, a framework for automated fine-tuning that provides a principled yet efficient approach to identifying which parameters to update in sparse fine-tuning by posterior inference of parameter-wise scale in a hierarchical Bayesian model. We demonstrated BayesTune to be effective for parameter efficient fine-tuning in both NLP and vision tasks, using both mask-based and additive search spaces.

Figure 3: VTAB datasets grouped by the sparse/dense adapter update effects. E.g., the datasets in group-(a) exhibit higher accuracy when the adapter modules are updated more sparsely than densely. In group-(c), neither extreme sparse nor dense updates are optimal, but the sparsity levels in between perform the best. In group-(d), we have no clear correlation between update sparsity and test accuracy. Individual plots show test accuracies vs. sparsity levels, where the rightmost plot in each group shows the mean and \(95\%\) confidence interval of the members within the group in normalized accuracy scale.