# Protein-Nucleic Acid Complex Modeling with Frame Averaging Transformer

Tinglin Huang\({}^{1}\)

Correspondence to tinglin.huang@yale.edu

Zhenqiao Song\({}^{2}\)

Rex Ying\({}^{1}\)

Wengong Jin\({}^{3,4}\)

\({}^{1}\)Yale University, \({}^{2}\)Carnegie Mellon University,

\({}^{3}\)Northeastern University, Khoury College of Computer Sciences

\({}^{4}\)Broad Institute of MIT and Harvard

###### Abstract

Nucleic acid-based drugs like aptamers have recently demonstrated great therapeutic potential. However, experimental platforms for aptamer screening are costly, and the scarcity of labeled data presents a challenge for supervised methods to learn protein-aptamer binding. To this end, we develop an unsupervised learning approach based on the predicted pairwise contact map between a protein and a nucleic acid and demonstrate its effectiveness in protein-aptamer binding prediction. Our model is based on FAFormer2, a novel equivariant transformer architecture that seamlessly integrates frame averaging (FA) within each transformer block. This integration allows our model to infuse geometric information into node features while preserving the spatial semantics of coordinates, leading to greater expressive power than standard FA models. Our results show that FAFormer outperforms existing equivariant models in contact map prediction across three protein complex datasets, with over 10% relative improvement. Moreover, we curate five real-world protein-aptamer interaction datasets and show that the contact map predicted by FAFormer serves as a strong binding indicator for aptamer screening.

## 1 Introduction

Nucleic acids have recently shown significant potential in drug discovery, as shown by the success of mRNA vaccines  and aptamers . Aptamers are single-stranded nucleic acids capable of binding to a wide range of molecules, including previously undruggable targets . Currently, aptamer discovery is driven by high-throughput screening, which is time-consuming and labor-intensive. While machine learning can potentially accelerate this process, the limited availability of labeled data presents a significant challenge in ML-guided aptamer discovery . Given this challenge, our goal is to build an unsupervised protein-nucleic acid interaction predictor for large-scale aptamer screening.

Motivated by previous work on unsupervised protein-protein interaction prediction , we focus on predicting the contact map between proteins and nucleic acids at the residue/nucleotide level. The main idea is that a predicted contact map offers insights into the likelihood of a protein forming a complex with an aptamer, thereby encoding the binding affinity between them. Concretely, as shown in Figure 1(a), our model is trained to identify specific contact pairs between residues and nucleotides when forming a complex. The maximum contact probability across all pairs is then interpreted as the binding affinity, which is subsequently used for aptamer screening.

[MISSING_PAGE_FAIL:2]

Protein complex modelingThe prediction and understanding of interactions between proteins and molecules play a crucial role in biomedicine. For example, some prior studies focus on developing a geometric learning method to predict the conformation of a small molecule when it binds to a protein target [71; 19; 53; 50]. As for the protein-protein complex, [25; 29] explore the application of machine learning in predicting the structure of protein multimer. Some studies [77; 56] investigate the protein-protein interface prediction in the physical space. Jin et al.  studies the protein-protein affinity prediction in an unsupervised manner. For protein-nucleic acid, some prior works explore the identification of the nucleic-acid-binding residues on protein [65; 86; 89; 37; 85] or predict the binding probability of RNA on proteins [79; 82; 87; 54]. Some previous studies focus on modeling protein-nucleic acid complex by computational method [81; 80; 28]. AlphaFold3  and RoseTTAFoldNA  are the recent progresses in this field, which both are pretrained language models for complex structure prediction.

Geometric deep learningRecently, geometric deep learning achieved great success in chemistry, biology, and physics domains [13; 90; 40; 55; 52; 64; 10; 70]. The previous methods roughly fall into four categories: 1) Invariant methods extract invariant geometric features from the molecules, such as pairwise distance and torsion angles, to exhibit invariant transformations [67; 31; 30]; 2) Spherical harmonics-based models leverage the functions derived from spherical harmonics and irreducible representations to transform data equivariantly [27; 47; 75]; 3) Some methods encode the coordinates and node features in separate branches, interacting these features through the norm of coordinates [66; 39]; 4) Frame averaging (FA) [63; 21] framework proposes to model the coordinates in eight different frames extracted by PCA, achieving equivariance by averaging the encoded representations.

The proposed FAFormer combines the strengths of FA and the third category of methods by encoding and interacting coordinates with node features using FA-based components. Besides, FAFormer can also be viewed as a combination of GNN and Transformer architectures, as the edge representation calculation functions similarly to message passing in GNN. This integration is made possible by the flexibility provided by FA as an integrated component.

## 3 Frame Averaging Transformer

In this section, we present our proposed FAFormer, a frame averaging (FA)-based transformer architecture. We first introduce the FA framework in Section 3.1 and elaborate on the proposed FAFormer in Section 3.2. Discussion on the equivariance is provided in Section 3.3, and the computational complexity analysis can be found in Appendix B.

### Background: Frame Averaging

Frame averaging (FA)  is an encoder-agnostic framework that can make a given encoder equivariant to the Euclidean symmetry group. FA applies the principle components derived via Principal Component Analysis (PCA) to construct the frame capable of achieving \(E(3)\) equivariance. Specifically, the frame function \(()\) maps a given set of coordinates \(\) to eight transformations:

\[()=\{(,)|=[_{1}_{1},_{2} _{2},_{3}_{3}],_{i}\{-1,1\}\}\] (1)

where \(_{1},_{2},_{3}\) are the three principal components of \(\), \(^{3 3}\) denotes the rotation matrix based on the principal components, and \(^{3}\) is the centroid of \(\). The main idea of FA is to encode the coordinates as projected by the transformations, followed by averaging these representations. We introduce \(f_{}()\) to represent the projections of given coordinates via \(()\):

\[ f_{}()&:=\{(- )(,)()\}\\ &:=\{^{(g)}\}_{}\] (2)

where \(^{(g)}\) denotes the coordinates transformed by \(g\)-th transformations. We can apply any encoder \(()\) to the projected coordinates and achieve equivariance by averaging, which can be formulated as an inverse mapping \(f_{^{-1}}()\):

\[f_{^{-1}}(\{(^{(g)})\}_{}):=()|}_{g}(^{(g)})_{g}^{-1}+\] (3)where \(_{g}^{-1}\) is the inverse matrix of \(g\)-th transformations and the result exhibit \(E(3)\) equivariance. The outcome is invariant when simply averaging the representations without inverse matrix.

### Model Architecture

Instead of serving FA as an external encoder wrapper, we propose instantiating FA as an integral geometric component within Transformer. This integration preserves equivariance and enables the model to encode coordinates effectively in the latent space, ensuring compatibility with Transformer architecture. The overall architecture is illustrated in Figure 2.

Graph ConstructionEach molecule can be naturally represented as a graph [47; 35; 43] where the residues/nucleic acids are the nodes and the interactions between them represent the edges. To efficiently model the macro-molecules (i.e., protein and nucleic acid), we restrict the attention for each node \(i\) to its \(K\)-nearest neighbors \(_{K}(i)\), within a predetermined distance cutoff \(c\):

\[(i)=\{j|d_{ij} cj_{K}(i)\}\] (4)

where we use \((i)\) to denote the valid neighbors of node \(i\), and \(d_{ij}\) denotes the distance between node \(i\) and \(j\). The long-range context information can be captured by iterative attention within the local neighbors for each node \(i\).

Overall ArchitectureAs shown in Figure 2(a), the input of FAFormer comprises the node features \(^{N D}\), coordinates \(^{N 3}\), and edge representations \(^{N K D}\) derived by our proposed edge module, where \(N\) is the number nodes and \(D\) is the hidden size. FAFormer processes and updates the input features at each layer:

\[^{(l+1)},^{(l+1)},^{(l+1)}=f^{(l)}(^{(l)},^{(l)},^{(l)})\] (5)

where \(f^{(l)}()\) represents \(l\)-th layer of FAFormer. In each model layer, we first update coordinates and node features using the Biased MLP Attention Module. Then, these features are fed into Local Frame Edge Module to refine the edge representations. Finally, the node representations undergo further updates through Global Frame FFN.

Figure 2: Overview of FAFormer architecture. The input consists of the node features, coordinates, and edge representations, which are processed by a stack of **(b)** Biased MLP Attention Module, **(c)** Local Frame Edge Module, **(d)** Global Frame FFN, and **(e)** Gate Function. \(\) denotes aggregation, \(\) is multiplication, \(+\) is addition, and \(||\) indicates concatenation. Purple cells indicate the operation related to FA. **(f)** illustrates the difference between the local and global frames, where the local frame captures local interactions among the immediate neighbors for each node, while the global frame captures long-range correlations among all nodes.

FA Linear ModuleBased on FA, we generalize the vanilla linear module to encode coordinates in the latent space invariantly:

\[_{}():=()|}_{g} (^{(g)})_{g}\] (6)

where \(\{^{(g)}\}_{}\) is obtained using Equ.(2), \(_{g}^{3 D}\) is a learnable matrix for \(g\)-th transformations, and \(():=/||||_{2}^{2}}\) is the normalization which scales the coordinates such that their root-mean-square norm is one . \(_{}()\) is an invariant transformation and will serve as a building block within each layer of the model.

Local Frame Edge ModuleWe explicitly embed the interactions between each node and its neighbors as the edge representation \(^{N K D}\), where \(K\) is the number of the neighbors. It encodes the relational information and represents the bond interaction between nodes, which is critical in understanding the conformation of molecules .

As shown in Figure 2(f), unlike the vanilla FA which _globally_ encodes the geometric context of the entire molecule, the edge module builds frame _locally_ around each node's neighbors. Specifically, given a node \(i\) and its neighbor \(j(i)\), the geometric context is encoded within the local neighborhood:

\[\{_{i j}^{}\}_{(i)}=_{ }(\{_{i}-_{j}\}_{(i)})\] (7)

where \(\{_{i}-_{j}\}_{(i)}\) denotes the direction vectors from center node \(i\) to its neighbors, and \(_{i j}^{}^{d}\) is the encoded representation. With the local frame, the spatial information sent from one source node depends on the target node, which is compatible with the attention mechanism. Then the node features are engaged with geometric features, and the edge representation is finalized through the residual connection with the gate mechanism:

\[_{ij}=(_{i}||_{j}||_{i j}^{ })_{ij}^{}=_{ij}_{ij}+ _{ij}\] (8)

where \((||)\) is the concatenation operation, and the calculated gate \(_{ij}=((_{ij}))\) provides flexibility in regulating the impact of updated edge representation.

The encoded edge representation in FAFormer plays a crucial role in modeling the pairwise relationships between nodes, especially for nucleic acid due to their specific base pairing rules . The incorporation of FA facilitates the encoding of the pairwise relationships in a geometric context, resulting in an expressive representation.

Biased MLP Attention ModuleAs shown in Figure 2(b), the attention module of FAFormer first transforms the node features \(\) into query, key, and value representations:

\[_{Q}=_{Q},\ _{K}=_{K},\ _{V}=_{V}\] (9)

where \(_{Q},_{K},_{V}^{D D}\) are the learnable projections. We adopt MLP attention  to derive the attention weight between node pairs, which can effectively capture any attention pattern. The relational information from the edge representation is integrated as an additional bias term:

\[a_{ij}=_{i}((_{Q,i}||_{K,j})+b_{ ij}),\] (10)

where \(b_{ij}=((_{ij}))\) represents the scalar bias term based on the edge representation, \(a_{ij}\) denotes the attention score between \(i\)-th and \(j\)-th nodes, \(_{*,i}\) is \(i\)-th representation of the matrix \(_{*}\), \(_{i}()\) is the softmax function operated on the attention scores of node \(i\)'s neighbors, and \(()\) is layernorm function .

Besides the value embeddings, the edge representation will also be aggregated to serve as the context for the update of node feature in FAFormer:

\[_{i}^{*} =_{j(i)}a_{ij}_{j},_{i}^{*}=_{j (i)}a_{ij}_{ij},\] (11) \[_{i}^{} =((_{i}^{*}||_{i}^{*}) )+_{i}\] (12)where \(^{}_{i}\) is the update representation of node \(i\). The above attention can be extended to a multi-head fashion by performing multiple parallel attention functions. For the coordinates, we employ an equivariant aggregation function that supports multi-head attention:

\[^{*}=f_{^{-1}}(\{[^{(0)}^{(g)},,^{ (H)}^{(g)}]\}_{})\] (13)

where \(\{^{(g)}\}_{}=f_{}()\), \(H\) is the number of attention heads, \([]\) is the tensor stack operation and \(^{H 1}\) is a linear transformation for aggregating coordinates in different heads. An additional gate function that uses node representations as input to modulate the aggregation is applied:

\[^{}=_{}^{*}+(1-_{}) \] (14)

where \(_{}=(())\) is the vector-wise gate designed to modulate the integration between the aggregated and the original coordinates. This introduced gate mechanism further encourages the communication between node features and geometric features.

Global Frame FFNTo further exploit the interaction between node features and coordinates, we extend the conventional FFN to _Global Frame_ FFN which integrates spatial locations with node features through FA, which is illustrated in Figure 2(d):

\[_{v}=_{}()^{ }=(||_{v})+\] (15)

where \(()\) denotes a two-layer fully connected feed-forward network. This integration of spatial information \(_{v}\) into the feature vectors enables the self-attention mechanism to operate in a geometric-aware manner. Unlike the edge module that focuses on each node's local neighbors, global frame FFN encodes the coordinates of all nodes, thereby capturing the long-range correlation among nodes.

### Equivariance

The function \(_{}()\) exhibits invariance since results are simply averaged across different transformations. In light of this, the node representation generated by the edge module and FFN are also invariant. The biased attention is based on the scalar features so the output is always invariant.

The update of coordinates within FAFormer leverages a multi-head attention aggregation with a gate function. Both functions are \(E(3)\)-equivariant: attention aggregation is based on frame averaging, while gate function is linear and also exhibits \(E(3)\)-equivariance, with a formal proof in Appendix C.

In conclusion, FAFormer is symmetry-aware which exhibits invariance for node representations and \(E(3)\)-equivariance for coordinates.

## 4 Experiments

In this section, we present three protein complex datasets and five aptamer datasets to explore protein complex interactions and evaluate the effectiveness of FAFormer. More details regarding the experiments and datasets can be found in Appendix A and D. Additional experiments, including the ablation studies, and the comparison with AlphaFold3, can be found in Appendix E. All the datasets used in this study are included in our anonymous repository.

### Dataset

Protein ComplexesWe cleaned up and constructed three 3D structure datasets of protein complexes from multiple sources [8; 7; 2; 77]. A residue-nucleotide pair is determined to be in contact if any of their atoms are within 6A from each other [77; 78]. We conduct dataset splitting based on the protein sequence identity, using a threshold of 50% for protein-RNA/DNA complexes3 and 30% for protein-protein complexes. The details of all datasets are shown in Table 1.

The protein's and nucleic acid's structures in the validation/test sets are generated by ESMFold  (proteins) or RoseTTAFoldNA (nucleic acids). This offers a more realistic scenario, given that the crystal structures are often unavailable.

[MISSING_PAGE_EMPTY:7]

[MISSING_PAGE_FAIL:8]

### Comparison with RoseTTAFoldNA

In this section, we investigate the performance of RoseTTAFoldNA  which is a pretrained protein complex structure prediction model and compare it with FAFormer. The performance of FAFormer is evaluated on the individual predicted protein and nucleic acid structures by ESMFold and RoseTTAFoldNA. We additionally test the performance of AlphaFold3  on a subset of the screening tasks due to AlphaFold3 server submission limits (Appendix E).

DatasetFor the contact map prediction task, we select the test cases used in RoseTTAFoldNA to create the test set, yielding 86 protein-DNA and 16 protein-RNA cases. Furthermore, the complexes from our collected dataset that have more than 30% protein sequence identity to these test examples are removed. This leads to 1,962 training cases for protein-DNA and 1,094 for protein-RNA, which are used for training FAFormer. The MSAs of proteins and RNAs are retrieved for RoseTTAFoldNA.

For the aptamer screening task, we construct a smaller candidate set for each protein target by randomly sampling 10% candidates, given that the inference of RoseTTAFoldNA with MSA searching is time-consuming. The datasets will be equally split into validation and test sets. More details of these datasets can be found in Appendix D.

ResultsThe comparison results of contact map prediction are presented in Figure 3, where FAFormer can achieve comparable performance to RoseTTAFoldNA using unbounded structures.

    & Metric & GFP & HNRPC & NELF & CHK2 & UBLCP1 \\   & Top10 Prec. & \(0.4000\) & \(0.1000\) & \(0.0\) & \(0.0\) & \(0.0\) \\  & Top50 Prec. & \(0.3600\) & \(0.0599\) & \(0.0\) & \(0.1000\) & \(0.0199\) \\  & & PRAUC & \(0.3926\) & \(0.1452\) & \(0.0481\) & \(0.1176\) & \(0.0722\) \\    & Top10 Prec. & \(0.4000\)\({}_{0.0}\) & \(0.1666\)\({}_{124}\) & \(0.1666\)\({}_{0.081}\) & \(0.1333\)\({}_{1.24}\) & \(0.1000\)\({}_{0.094}\) \\  & Top50 Prec. & \(0.3800\)\({}_{0.18}\) & \(0.0800\)\({}_{0.024}\) & \(0.0866\)\({}_{0.018}\) & \(0.1266\)\({}_{0.039}\) & \(0.0866\)\({}_{0.009}\) \\   & PRAUC & \(0.4027\)\({}_{0.022}\) & \(0.1781\)\({}_{0.089}\) & \(0.1044\)\({}_{0.18}\) & \(0.1374\)\({}_{0.13}\) & \(0.0762\)\({}_{0.16}\) \\   

Table 6: Comparison results with RoseTTAFoldNA using the sampled datasets, which accounts for the performance differences of FAFormer as shown in Table 5.

Figure 4: Case study based on two complex examples (PDB id: 7DVV and 7KX9), where each heatmap entry represents the contact probability between the nucleotide and residue. In each row, the figure on the left displays the ground truth contact maps, while the figure on the right displays the results predicted by FAFormer.

Figure 3: Contact map prediction on RoseTTAFoldNA test set.

Specifically, our method has higher F1 scores for protein-DNA complexes (0.103 vs. 0.087) and performs comparably for protein-RNA complexes (0.108 vs. 0.12). Besides, Table 6 shows that RoseTTAFoldNA fails to receive positive aptamers for some targets, e.g., NELF and UBLCP1, while FAFormer consistently outperforms RoseTTAFoldNA for all the targets.

Aligning Figure 3 with Table 6, we observe that while FAFormer does not surpass RoseTTAFoldNA in contact map prediction, it significantly excels in aptamer screening. We attribute this to two reasons: 1) Similar to AlphaFold3 (Table 10), RoseTTAFoldNA as a foundational structure prediction model is optimized for general complex structures, which might bias its performance on some specific protein targets. For example, it can achieve good performance on proteins GFP and HNRNPC but fails for NELF. 2) The protein-RNA test set used by RoseTTAFoldNA for contact map prediction is limited, which may not comprehensively evaluate FAFormer.

Case StudyTwo examples of protein-DNA (PDB id: 7DVV) and protein-RNA (PDB id: 7KX9) complexes are provided in Figure 4, which shows a visual comparison between the actual (left) and the predicted (right) contact maps. Note that only the residues involved in the actual contact map are presented for a clear demonstration4. The complete contact map can be found in Appendix E.1. We can find that despite the sparsity of contact pairs, the predicted contact maps show a high degree of accuracy when compared to the ground truth.

Time ComparisonTable 7 shows the average and total inference time of FAFormer and RoseTTAFoldNA on the test cases for the contact map prediction task, including the time for predicting the unbound structures for FAFormer. The predicted structures used for the evaluation of FAFormer are generated without protein and RNA MSAs, demonstrating a significantly faster inference speed by orders of magnitude. For the unsupervised aptamer screening task, while RoseTTAFoldNA requires only a single MSA search for each protein target, it needs to search MSAs for each RNA candidate sequence separately. Besides, the inclusion of MSAs results in a large input sequence matrix, leading to a time-consuming folding process of RoseTTAFoldNA.

## 5 Conclusion

This research focuses on predicting contact maps for protein complexes in the physical space and reformulates the task of large-scale unsupervised aptamer screening as a contact map prediction task. To this end, we propose FAFormer, a frame averaging-based Transformer, with the main idea of incorporating frame averaging within each layer of Transformer. Our empirical results demonstrate the superior performance of FAFormer on contact map prediction and unsupervised aptamer screening tasks, which outperforms eight baseline methods on all the tasks.

Broader ImpactsThe proposed paradigm for aptamer screening can be extended to other modalities, such as protein-small molecules and antibody-antigen. Moreover, the strong correlation between contact prediction and affinity estimation demonstrated in our paper can guide future model development. Besides, FAFormer introduces a novel approach to equivariant model design by leveraging the flexibility of FA. This idea opens up numerous possibilities for future research, including exploring different ways to integrate FA with various neural network architectures.

LimitationIn this study, the geometric features utilized in the encoders are limited to the coordinates of the \(C_{}\) and \(C_{3}\) atoms. Features extracted from the backbone or sidechains have not been used, which may limit the performance of the geometric encoders.

    &  &  \\  & Avg. & Total & Avg. & Total \\   RoseTTAFoldNA & \(0.175\)h & \(15.12\)h & \(0.440\)h & \(7.04\)h \\ FAFormer & \(32.65\)s & \(0.78\)h & \(51.75\)s & \(0.23\)h \\   

Table 7: Inference time on contact map prediction, denoted in seconds (”s”) and hours (“h”).

Acknowledgements

We thank Yangtian Zhang, Junchi Yu, Weikang Qiu, and anonymous reviewers for their valuable feedback on the manuscript. This work was supported by the BroadIgnite Award, the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard, NSF IIS Div Of Information & Intelligent Systems 2403317, and Amazon research.