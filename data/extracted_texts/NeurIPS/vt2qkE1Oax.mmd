# Learning segmentation from point trajectories

Laurynas Karazija, Iro Laina, Christian Rupprecht, Andrea Vedaldi

Visual Geometry Group

University of Oxford

Oxford, UK

{laurynas,iro,chrisr,vedaldi}@robots.ox.ac.uk

###### Abstract

We consider the problem of segmenting objects in videos based on their motion and no other forms of supervision. Prior work has often approached this problem by using the principle of common fate, namely the fact that the motion of points that belong to the same object is strongly correlated. However, most authors have only considered instantaneous motion from optical flow. In this work, we present a way to train a segmentation network using long-term point trajectories as a supervisory signal to complement optical flow. The key difficulty is that long-term motion, unlike instantaneous motion, is difficult to model - any parametric approximation is unlikely to capture complex motion patterns over long periods of time. We instead draw inspiration from subspace clustering approaches, proposing a loss function that seeks to group the trajectories into low-rank matrices where the motion of object points can be approximately explained as a linear combination of other point tracks. Our method outperforms the prior art on motion-based segmentation, which shows the utility of long-term motion and the effectiveness of our formulation.

## 1 Introduction

Segmentation, the task of delineating and isolating distinct objects, is a fundamental problem in computer vision. Much of the current approaches are supervised, relying on expensive manual annotations. Attempts to approach this task without supervision have largely relied on manual heuristics or exploited the rich semantics of self-supervised feature extractors. Video data, however, offers an additional option as it contains _motion_, which can be exploited for an additional inductive bias. Such approaches are rooted in the principle of common fate from Gestalt psychology , which posits that elements that move together are more likely to belong together.

Motion information is usually captured by optical flow. Flow is attractive as it arises from low-level visual properties and can provide a signal before scenes are parsed and objects are discovered. Furthermore, optical flow estimators, such as RAFT  or FlowFormer , can be trained purely on synthetic artificial data, transferring to real-world scenes with remarkable accuracy and without manual annotation. This has led many to consider optical flow as a critical modality to discover and learn objects from video data by learning to attribute and explain the motions of objects.

Optical flow, however, only describes the instantaneous motion of the scene, which can create blindspots: not all objects are necessarily in motion at all times. Similarly, groups of objects might coincidentally move together. Recent advances in point tracking  offer an alternative form of motion information. Point trackers "lock on" to a set of query points and describe their position and visibility over the course of the whole video. This provides long-term motion information. Like optical flow estimators, point trackers are trained on synthetic data. However, unlike optical flow, point trajectories describe only a sparse set of points.

In this paper, we ask whether the long-term motion information obtained in point trajectories is beneficial. To that end, we explore how to supervise image segmentation networks using motion information with point trajectories. At a glance, this presents several problems. Firstly, point trajectories are time-varying 2D point clouds, and combining them with image-based networks is not straightforward. Furthermore, the evolution of long-term object motion is too complex, even in the simplest cases. Our main insight is that the motion of points belonging to the same object should be well correlated. We thus propose a loss function that encodes this intuition by seeking to explain groups of points as combinations of other points in the group. With our method, a segmentation network predicts objects in the scene, inducing a grouping of trajectories that are currently visible. The loss function then assesses how well such a grouping explains the long-term motion. While point trajectories describe motion over a longer time, they are limited by the number of points tracked, which is often much less than the number of pixels. We thus propose to train using both trajectory-based loss and optical flow-based loss and show that spatially sparse but longer-time motion information synergises with spatially dense optical flow.

Discovering objects using point trajectories has a long history in computer vision. Our approach is inspired by ideas of subspace clustering, which assume that data comes from distinct subspaces and seek to reconstruct membership information of data points. This has previously been applied to the problem of motion segmentation [17; 38]. These approaches, however, are sensitive to noise and either rely on specialised optimisation procedures to recover a graph of trajectory relationships [32; 48] or use manual instead [32; 48]. Normalised cuts or spectral clustering are then used to group the trajectories. However, the need for an affinity matrix limits the number of trajectories that can be used due to quadratic memory requirements. Furthermore, "densification" is still required to extend trajectory clusters to the whole image. By construction, these approaches can process only a single sequence at a time. Our proposal instead trains an image segmentation network directly end-to-end using a dataset of videos while supporting a large number of trajectories.

In summary, our work makes the following contributions. (1) We propose a loss function that enables training any image segmentation architecture using point trajectories as a source of supervision. (2) We investigate our proposed loss in a principled way in a simulated setting, showing the feasibility of our approach. (3) We apply such a loss in a per-sequence optimisation, outperforming other subspace clustering baselines. (4) We use our loss to train a single network on a dataset of videos for the task of video object segmentation, demonstrating strong results. (5) We show how our proposed loss formulation obtains better performance than alternatives.

## 2 Related work

Unsupervised video object segmentation.Video object segmentation (VOS) aims to label pixels of objects in a video. Current VOS benchmarks [35; 51; 53] usually define the problem as binary foreground-background separation or salient object segmentation. The task is usually approached in two ways: semi-supervised and unsupervised VOS. Semi-supervised methods require initial frame annotations and aim to _propagate_ them to the rest of the video . Unsupervised VOS aims to discover object(s) of interest without the initial targets [18; 25; 36; 42; 52; 61]. This however does not differentiate methods based on data used to _train_ them. Most of the traditional research in semi- or unsupervised VOS relies on annotations during training. Our approach, in contrast, does not rely on any manual annotations to learn. Some authors explore related unsupervised video instance segmentation  task without any annotations, object-centric learning approaches [2; 58; 73], some of which make use of flow  and depth .

Motion segmentation.A closely related task to video object segmentation is motion segmentation, which aims to extract the main moving objects in a video. The practical difference between these two tasks is more difficult to delineate as the same benchmark datasets are often used. Early works modeled the scenes as layers [8; 27], which later works accomplish using a slot-attention mechanism [13; 34; 69]. Flow mixture models accounted for multiple motion patterns [26; 62], and corrections were introduced for rotating cameras [3; 4]. Later works [10; 46; 47] considered parametric flow models fit to explain the scene. AMD  employs a single model with separate appearance and motion 'pathways'. Other works train flow-only models by generating synthetic data, which generalise well to real videos [33; 67]. An alternative line of work adopts a more generative approach, training an inpainter networks to predict optical flow [70; 71]. Several authors [37; 59]adopt a multi-stage self-labelling  approach for motion segmentation: initial masks are estimated using an optical flow-based approach, followed by DINO-based refinement and CRF post-processing to generate pseudo-labels and train a final segmentation network.

Trajectory-based motion segmentation.Trajectory-based motion segmentation has also been explored. Older works consider data of multiple trajectories and employ non-negative matrix factorization and related decomposition methods [9; 11; 17; 19; 55; 68]. This line of work primarily operates by defining affinity between pairwise trajectories in a single video setting. In [6; 30; 31; 49; 50], heuristic graphs are constructed between trajectories, considering increasingly complex motion models, and employing specialised solvers to solve the optimisation problem. However, due to the specialised optimisation procedures and tight coupling with trajectory estimation methods, this line of work has received less attention than deep methods that exploit optical flow similarly to RGB frames.

Subspace clustering.A specific kind of trajectory-based technique is subspace clustering approaches, which rely on the _self-expressive_ property of the data. They can largely be summarised  as solving a constrained optimisation problem \(_{C}\|DC-D\|_{F}^{2}+(C)\) for some dataset \(D^{d n}\) of \(n\) points in \(d\) dimensions. \(C\) is a matrix of coefficients, which expresses the data and can be represented as a linear combination of other points. Given a solution for the coefficient matrix, it is transformed into an affinity matrix for spectral clustering. The approaches mainly differ in the second term of the objective and specialized methods to solve the optimisation problem. SSC  define \((C)\) as \(l_{1}\) norm. LLR  use nuclear norm instead, while LSR  uses instead \(l_{2}\) regularisation. [41; 43; 64] combines \(l_{1}\), \(l_{2}\), and nuclear norms. Under some strong assumptions , these approaches enjoy some theoretical guarantees. However, they are difficult to scale in practice as the number of points \(n\) grows, as \(C\) is \(n n\). Additionally, the secondary step of spectral clustering is also limiting and difficult to tune. Instead, we take inspiration from these approaches and propose a way to supervise the network directly using the self-expressive property of point trajectories.

## 3 Method

Our goal is to solve the video segmentation task in an unsupervised manner: given a video, we want to segment out the objects that are moving independently within it. A video is a sequence of frames \(_{t}^{HW 3}\), each of which is an RGB image defined on the lattice \(=(\{1,,H\}\{1,,W\})^{HW  1}\). To segment the objects, we self-supervise a neural network \(\) that takes as input each frame \(_{t}\) in turn, and outputs a corresponding segmentation mask \((_{t})=M_{t}^{HW K}\) where \(K\) is the number of possible segments we expect to observe in the video. Segmentation matrix entries softly assign each pixel to one of \(K\) possible segments.

The challenge is how to supervise the network \(\) without labels, utilising only the video itself as training material. The key inductive principle that we propose to use is that physical points that belong to the same object tend to have highly correlated motion, often called _principle of common fate_. When these points are projected to pixels, they result in corresponding highly correlated apparent motions, which we can measure using techniques like optical flow and point tracking. Therefore, we propose to supervise the network \(\) from an analysis of apparent motion extracted automatically from the video using off-the-shelf components.

Motion can be measured at two temporal scales. Optical flow extracts instantaneous motion, measuring the 2D velocity of the 3D points found at each pixel in each video frame. Point tracking extracts long-term motion, estimating the 2D location of a certain number of 3D points throughout the video's duration. These two sources of information are complementary. Optical flow is dense, easy to extract, and easy to model to discover correlations within it; however, by considering different times in isolation, it ignores most of the correlations that exist in the data. Tracks are sparse, more difficult to extract and harder to model, but potentially contain information ignored by optical flow.

Prior works such as  have studied how to model optical flow for segmentation. Here, motivated by a new generation of high-quality point trackers [14; 15; 22; 28], we aim at developing the machinery necessary to use track information as well. From this analysis, we construct losses which assess the quality of the predicted mask \(M_{t}\) given the video itself. Next, we introduce two such losses, one for optical flow from prior work, and a new one based on point tracking.

### Learning from optical flow

First, we describe the case of optical flow. Because optical flow is instantaneous, we can fix our attention on a specific frame \(\) and corresponding mask \(M\), dropping for now the time index \(t\). The _optical flow_\(F^{HW 2}\) for this image associates a 2-dimensional flow vector to each of the \(H W\) pixels. Each flow vector can be understood as the velocity of the pixel.

Let \(M_{k}^{HW 1}\) be the binary matrix for segment \(k\), obtained by extracting the \(k\)-th column of \(M\). Let \(F_{k}=M_{k} F\) denote the Hadamard (element-wise) product between the mask and flow vectors, broadcasting the mask along the rows.

Assuming that the object is rigid, the optical flow can be approximated as a linear parametric model of 2D coordinate embeddings (see  for an overview). Following , we consider a six-dimensional quadratic embedding kernel \(([x,\ y])=[x,\ x^{2},\ y,\ y^{2},\ xy,\ 1]^{1 6}\) for pixel coordinates \([x,\ y]\) and associate to each region \(k\) a corresponding set of 12 parameters \(_{k}^{6 2}\). Optical flow vectors within a region should be expressible as a linear combination of these six basis functions.

We then consider all pixels embeddings stacked in a single matrix \(E_{k}=M_{k}()\) where the product with the soft mask ensures that the embeddings are "active" only if the corresponding pixels are. The optical flow vectors in the region are then approximated as

\[F_{k}_{k}=E_{k}_{k}_{k}=(E_{k}^{}E_{k})^{-1}E_{k}^{}F_{k},\] (1)

where \(_{k}\) is obtained via least square. We can use the residual of this approximation as a measure of how well the mask \(M_{k}\) fits the data:

\[_{}(M|F)=_{k}\|F_{k}-_{k}\|_{F}^{2}=_{k}\|F_ {k}-E_{k}_{k}\|_{F}^{2}\,.\] (2)

Intuitively, this considers the correlation of pixel motion in the _spatial_ sense: how pixel coordinates determine its motion based on motion parameters \(_{k}\).

### Learning from trajectories

Having covered optical flow, we move now to developing an analogous loss for tracking. We write \(P^{2T N}\) for the track matrix, with one trajectory per column. With slight abuse of notation, we write \((P)_{t}^{2 N}\) for indexing rows corresponding to point locations at some time \(t\). To connect pixel-wise masks and sparse points, we use a sampling operation \(()\), writing \((M_{k},(P)_{t})=_{k}^{N 1}\) for mask values at point locations at an appropriate time. Furthermore, we denote by \(P_{k}=P_{k}\) the masked version of the trajectory matrix, selecting the columns/trajectories that belong to object \(k\) with obvious broadcasting of the mask values.

Unlike optical flow, trajectories are too complex to be modelled using a small set of _fixed_ basis functions. Instead, we posit that the set of trajectories should be low-rank -- all trajectories belonging to the same object should be explained well by a linear combination of some small number of trajectories. We illustrate this intuition in Fig. 1 using a 2D example.

This assumption results in a factorization of \(P_{k}\) using singular value decomposition (SVD) as \(P_{k}=U_{k}_{k}V_{k}^{}\), where \((U_{k},_{k},V_{k})=(P_{k})\). As \(P_{k}\) should be low-rank, we can thus form an approximation using truncated SVD, by considering only first \(r\) components. We write \( U_{k}_{r}\) to denote such truncation. With this, we obtain the loss

\[_{_{r}}(M|P)=_{k}\|P_{k}- U _{k}_{r}_{k}_{r} V_{k}_{r}^{} \|_{F}^{2}.\] (3)

Since truncated SVD offers optimal decomposition for the error above, lowering this loss amounts to making \(P_{k}\) as close as possible to rank \(r\), i.e., by grouping trajectories into \(P_{k}\) that do not increase its rank, and should come from rigid objects.

As we show in Section 5.3, we found an alternative formulation of this idea works better. Note the rank \(r\) matrix has the \(r\)-th and all later singular values as 0. We can optimise singular values higher than \(r\)-th to be close to 0 (ignoring \(U_{k}\) and \(V_{k}\)). Thus, for trajectories, we formulate a loss simply as:

\[_{t}(M|P)=_{k}_{i=r}^{(2T,N)}_{i}(P_{k}),\] (4)

where \(_{i}(P_{k})\) is the i-th singular value of \(P_{k}\). We assume \(r(2T,N)\).

Meaning of decomposition.We show that under certain simplifying assumptions, the decomposition in (3) is exact and models time-varying camera motion and object geometry as two terms. We consider a simple case of a rigid body motion observed through a perspective camera. For points on the object, we can consider only the relative motion between the camera and the object and attribute it all to the camera for simplicity.

Given (stacked) camera projection matrices \(W_{t}^{3T 4}\), points \(_{k}^{4 N}\) in homogenous coordinates that remain at constant projective depth \(^{N 1}\) from the camera over the whole sequence, we note the following equation :

\[_{k}=W_{t}_{k}()^{-1},\] (5)

where \(_{k}^{3T N}\) is \(P_{k}\) in homogenous coordinates. Both \(W_{t}\) and \(_{k}()^{-1}\) can be recovered by considering a truncated SVD at rank 4: \(W_{t}= U_{k}_{4}_{k}_{4}\), and \(_{k}()^{-1}= V_{k}_{4}^{}\).

The trajectory matrix factorises into the time-varying camera matrices and object geometry. As the depth is not constant in the real-world setting, this decomposition is approximate and suggests the following alternative loss:

\[_{}=_{k}\|}-W_{t}_{k} ()^{-1}\|_{F}^{2},\] (6)

where \(W_{t}\), and \(_{k}()^{-1}\) are obtained via SVD as above.

Choice of \(r\).Setting \(r\) correctly is important. Intuitively, it captures the degrees of freedom present in the trajectory data or the number of trajectories that are sufficient to form a basis. From the analysis above, we saw that rank \(r=4\) corresponds to assuming constant depth and perspective camera. However, higher \(r\) is needed to tolerate changing depth and tracking errors . Similarly, not all motion is rigid in real-world videos, which also requires increasing \(r\). We empirically determined \(r=5\) to yield good results.

### Training a segmenter using flow and trajectories

The losses above require optical flow \(F\), trajectories \(P\), and masks \(M_{k}\) obtained using a segmentation network \(()=M\). This suggests a simple procedure of training a segmentation network given a dataset of videos, which we summarise in Fig. 2. We precalculate optical flow for each frame and obtain a set of point trajectories for each video using off-the-shelf pretrained networks. For training, we consider triples of \((,F,P)_{i}\) for each frame \(i\), where for trajectories \(P\), we take trajectories for which the points are visible in the image \(\). This can be accomplished by making use of visibility predictions in the output of point trackers or calculating trajectories by querying points in each frame. We use bilinear sampling for \(()\) to obtain mask values at trajectory coordinates.

Temporal smoothing.We include a temporal smoothing loss, which matches mask predictions between two frames offset by \( t\) using the predicted trajectories:

\[_{}=\|((_{t}),(P_{t})_{t})-( (_{t+ t}),(P_{t})_{t+ t})\|_{2}^{2}\,,\] (7)

where \(_{t}\) is the \(t\)-th frame and \(P_{t}\) are trajectories associated with \(t\)-th frame. We write the final loss as: \(=_{f}_{f}+_{t}_{t}+_{ }_{},\) where \(_{f},_{t},_{}\) balance the contribution of the different loss terms.

Figure 1: **Illustrative 2D example for the low-rank nature of \(P_{k}\). A triangle undergoes rigid rotation over three frames. As the rate of rotation is not constant, the flow vectors and point positions are difficult to model. However, the point \(p\) is part of the triangle and can be expressed as a combination of the three vertices at an appropriate time. Thus, the last column of \(P_{k}\) is linearly dependent, and \(P_{k}\) is rank deficient. _Any points in the triangle could be included in \(P_{k}\) without increasing its rank._**

Choice of \(k\).Following prior work , we set \(k\), the number of predicted masks, to be higher than the maximum number of objects in the scene to account for potential parallax and non-rigid motion. In the binary segmentation case, we recover two components by considering the average appearance feature of each component and solving for the normalised cut on a graph with \(k\) nodes.

## 4 Feasibility study

Our proposed trajectory loss (4) enables training a segmentation network using trajectory data. We first show the feasibility of the proposed cost function in a controlled setting, without actually training \(\). To this end, we consider a synthetic scene from the MOVI-F Kubric  dataset for which we obtain ground-truth trajectories for every point and ground-truth object segmentation masks. We explore the loss landscape of the proposed formulation by corrupting the segmentation masks along several principled axes and studying the effect of such corruptions on the trajectory loss.

First, we consider a random alteration of mask pixels, which we refer to as _mask noise_. We control the amount of mask noise using \(\) such that 0.0 corresponds to no pixels changed and 1.0 corresponds to completely random masks. Along this axis, we test whether our loss favours predictions with lower noise. Second, we consider structural alterations, namely under/over-segmentation. To simulate under-segmentation, we merge object masks with the background at random. To simulate over-segmentation, we randomly split the existing object mask into two parts in the middle along either the \(x\) or \(y\)-axis. We represent this type of mask corruption using integers. Negative values indicate the number of objects removed, while positive values correspond to new objects generated from existing ones. Such structural corruption investigates whether the loss can correctly identify the number of moving objects. Finally, we consider the "softness" of the predicted masks by transforming masks into logits and increasing the temperature \(\) in the softmax operation. This tests whether the loss will prefer low-entropy values. We leave further details of the corruption procedure to Appendix D.

The results of these analyses are shown in Figure 3. All three plots show the loss value as a function of structural corruption. The trajectory loss decreases as the noise and temperature of the masks are reduced, as seen in the first two plots. The third plot also shows that such solutions are preferred in combination. Furthermore, we observe that the loss values are lower when the correct number of segments is detected, and this holds even in the presence of noise or when masks are more uniform. Note, however, that over-segmentation is penalised less than under-segmentation, i.e., missing moving objects leads to a higher value of the loss than, e.g., splitting an object into several components.

Figure 2: **Overview of our approach. We self-supervise a segmentation network, i.e., without access to mask annotations, using both short-term motion information (optical flow) and long-term motion (point trajectories). We design a loss function that encourages the segmentation network to cluster regions where trajectories form low-rank-\(r\) groups, which should align well with objects. Off-the-shelf methods are used to estimate optical flow and point trajectories given a dataset of videos.**

## 5 Experiments

In this section, we evaluate our approach for unsupervised motion segmentation and compare it with simple baselines and prior subspace clustering methods. Next, we compare our method with state-of-the-art methods for unsupervised video object segmentation across several datasets in a binary segmentation setting. We finish with ablation experiments of our approach.

Datasets.We consider four primary datasets in this study. We use the synthetic MOVi-F variant of the Kubric  dataset with ground truth trajectories for comparison with subspace clustering-based approaches. We adopt this setting to eliminate noise in point trajectories as previous methods are sensitive to it. We report the adjusted Rand index (ARI) as the main metric, measuring how close clustering is to the ground truth up to the permutation of cluster identities, where 1 is a perfect match, and 0 means roughly random assignment. We also report FG-ARI, i.e., ARI only on foreground pixels (determined by ground truth masks), which identifies how well different objects are separated.

We also evaluate our approach on real-world datasets: DAVIS 2016 , SegTrackv2 (STv2) , and FBMS , which are popular benchmarks for video object segmentation. Following standard practice [69; 70], foreground objects in STv2 and FBMS are consolidated. We report the Jaccard (\(\)) score, computed using Hungarian matching between predicted and ground truth segmentations.

Implementation.For the experiments on real-world datasets, optical flow is estimated using RAFT  and point trajectories using CoTracker . Trajectories are computed within a context window \(f=20\) around each frame, with reflection padding around video boundaries, resulting in chunks of \(T=2f+1=41\) frames. To reduce the effect of noisy predictions, we also filter trajectories along the time dimension using an average filter with a window size of 11. For the experiments on MOVi-F, a small U-Net  is trained as the segmentation network, starting from random initialisation. For fairness of comparisons on DAVIS, STv2 and FBMS, we use the same architecture as in  -- MaskFormer with DINO backbone. We specify further details in Appendix E.

### Comparison to trajectory-based methods

In Table 1, we compare our low-rank trajectory loss (LRTL) with prior subspace clustering approaches in a per-video optimisation setting. Subspace clustering operates on a similar intuition to our proposed trajectory loss by a grouping of trajectories that should be linearly dependent. We also consider K-means clustering of trajectories as a simple baseline. For fair comparisons, we train our segmentation model optimising _only_ the trajectory loss (\(_{t}\)). We use \(k=25\) components for each video and train for 5000 steps. This is comparable to the computation requirements and steps of other methods. For K-means, SSC  and LRR , we search for an optimal set of hyperparameters and the number of components \(k\), reporting the best results. Our approach shows significantly stronger performance than simple K-Means and subspace clustering approaches.

    &  \\
**Method** & ARI\(\) & FG-ARI\(\) \\  K-Means & 15.26 & 42.53 \\ SSC  & 11.12 & 39.21 \\ LRR  & 7.47 & 37.36 \\  
**LRTL (Ours)** & 46.07 & 65.76 \\   

Table 1: Comparison of our LRTL trajectory-based formulation with prior methods.

Figure 3: **Feasibility analysis of \(_{t}\).** Using a synthetic sequence (left), we vary the amount of noise \(\) injected into the mask, the temperature \(\) of the mask logits and plot the loss value as a function of the mask under/over segmentation. The plots show that the loss is reduced in low-noise, low-entropy settings and penalises both over- and under-segmentation.

### Unsupervised video object segmentation

We compare to recent methods on the unsupervised video object segmentation task _without first-frame prompting or post-processing_. In this setting, we train a single network on the benchmark datasets for binary video segmentation. We compare with _single-sequence methods_ that perform optimisation for each sequence/video individually. Additionally, we benchmark dataset-wide _single-stage end-to-end methods_ where training is performed over multiple videos simultaneously, training a network in an end-to-end manner. We also compare with _multi-stage methods_ that train and re-train several networks. We report our results on standard benchmarks in Table 2. While the closest prior work relies on multiple stages of training, pseudo-labelling, applying CRF, and retraining, our end-to-end trained method shows better performance at lower resolutions. We attribute this to the effectiveness of our approach in incorporating long-term motion information.

In Fig. 4, we show qualitative results of our approach and compare with RCF , a state-of-the-art multi-stage approach. Our network trained with both flow and trajectory losses yields segmentations with noticeably better boundaries despite operating at a lower resolution. Notably, our formulation also effectively avoids segmenting shadows and water ripples of the swan, which are difficult to separate based on instantaneous motion alone.

### Ablations

Alternative losses.We have explored several alternative formulations of the trajectory loss in our approach and present the analysis in Table 3. Losses based on full SVD reconstruction fail to train a network sufficiently. \(_{}\) performs the best out of these, likely as DAVIS contains several scenes

  
**Loss** & **DAVIS** (\(\)) \\  \(_{f}_{f}\) & 78.5 \\ \(_{t}_{t}\) & 71.9 \\ \(_{t}_{f}+_{t}_{t}\) & 81.7 \\  \(_{t}_{f}+_{t}_{t}+_{}_ {}\) & 82.2 \\   

Table 4: **Ablation of loss terms.** All loss terms synergistic to improve performance.

    & Inf. Input & Input & Motion Est. & **DAVIS** & **STv2** & **FBMS** \\
**Method** & RGB & Motion & Resolution & Method & \(\) & \(\) & \(\) \\   \\ FTS  & ✓ & ✓ & \(-\) & LDOF  & 55.8 & 47.8 & 47.7 \\ CUT  & ✓ & ✓ & \(-\) & LDOF  & 55.2 & 54.3 & 57.2 \\ DS  & ✓ & ✓ & \(240 426\) & RAFT  & 79.1 & 72.1 & 71.8 \\ Ponimatkin et al.  & ✓ & ✗ & \(480 848\) & ARFlow  & 80.2 & 74.9 & 70.0 \\ OCLR  (test ft.) & ✓ & ✓ & \(480 848\) & RAFT  & 80.9 & 72.3 & 69.8 \\   \\ OCLR  & ✗ & ✓ & \(112 224\) & RAFT  & 72.1 & 67.6 & 65.4 \\ DivA  & ✓ & ✓ & \(128 224\) & RAFT  & 72.4 & 64.6 & 60.9 \\ Meunier et al.  & ✗ & ✓ & \(128 224\) & RAFT  & 73.2 & 55.0 & \(-\) \\ GWM  & ✓ & ✗ & \(128 224\) & RAFT  & 79.5 & 78.9 & 78.4 \\   \\ RCF  & ✓ & ✗ & \(480 848\) & RAFT  & 80.9 & 76.7 & 69.9 \\ LOCATE  & ✓ & ✗ & \(480 848\) & ARFlow  & 80.9 & 79.9 & 68.8 \\   & ✓ & ✗ & \(192 352\) & RAFT  & **82.2** & **81.2** & **79.6** \\   

Table 2: **Unsupervised video segmentation on DAVIS, SegTrackv2, and FBMS. Where possible, we report results without widely applicable post-processing (e.g., CRF) or indicate results in grey.**

  
**Loss** & **DAVIS** (\(\)) \\  \(_{3}\) (3) & 11.1 \\ \(_{}\) (6) & 18.2 \\ \(_{5}\) (3) & 14.6 \\ _tracks-as-flow_ & 65.3 \\  _{t}\) (4)**} & 71.9 \\   

Table 3: **Alternative losses to our proposal. Other variants do not match the performance of our formulation.**with a panning camera tracking a rigid object at an approximately constant distance, which matches the assumptions. Increasing or decreasing the rank of the approximation performs worse. We also consider _track-as-flow_ loss, where trajectories \(P\) are treated as optical flow by subtracting positions from adjacent times. Then, for \(T\) frames, Eq. (2) can be applied. We find that such a formulation still underperforms in comparison to our trajectory-based formulation (Eq. (4)).

We believe our formulation provides better results than the above for two possible reasons. First, by minimising higher-than-\(r\) singular values, we are not _strictly_ enforcing assumptions like rigidity. Second, our loss formulation is more numerically stable as it requires only gradients w.r.t. to the singular values. As we seek to drive them close to zero, the matrices \(P_{k}\) become increasingly ill-conditioned as the training progresses. Additionally, gradients w.r.t. \(U\) and \(V^{}\) depend on inverse singular values \(^{-1}\), which become numerically unstable as they are approaching zero. On the other hand, \(=I_{N}(U^{}P_{k}\,V)\) does not have this problem.

Influence of losses.In Table 4, we consider the method with only the flow loss component and only the trajectory loss component. We find that our trajectory-based loss improves flow-only performance. Using only trajectory-based loss shows weaker performance than just optical flow, likely due to only a sparse set of points and noise introduced by estimating positions for occluded points. Ablating temporal smoothing loss slightly lowers performance as well.

Limitations.While we have demonstrated the effectiveness of learning segmentation from long-term motion, there is potential for further improvements in leveraging point trajectories. First, while modern trackers predict reasonable positions for occluded points, naturally, these predictions are less accurate. Thus, a more explicit handling of occlusions and tracking noise would likely help. Second, we currently only use trajectory estimates from nearby frames for training. This means that we sometimes track the same point multiple times, which could be avoided with caching trajectories. While we handle non-rigidity using over-segmentation, extending this principle to video with multiple non-rigid objects is an important feature direction.

## 6 Conclusion

We have introduced a principled method to train an image segmentation network using long-term motion information expressed as point trajectories. Our trajectory loss formulation follows the principle of common fate and aims to group trajectories into low-rank matrices, representing the idea the motion of points belonging to the same object can be roughly explained as a combination of other points. Using synthetic data we have shown that such a loss should prefer low-noise and low-entropy solutions as well as identify the correct number of moving objects. In comparison with other methods,

Figure 4: Qualitative comparison of our results on DAVIS with RCF which uses higher resolution and multi-stage training. Our method contains slightly better boundaries, does not segment shadows and separates water ripples from the swan.

our loss formulation has shown superior performance compared to subspace clustering baselines on synthetic data and achieved state-of-the-art results on unsupervised video object segmentation benchmarks when combined with optical flow-based loss.

AcknowledgementsL. K. is supported by is supported by EPSRC AIMS CDT EP/S024050/1. I. L., C. R. and A. V. are supported by ERC-CoG UNION 101001212 and EPSRC VisualAI EP/T028572/1.