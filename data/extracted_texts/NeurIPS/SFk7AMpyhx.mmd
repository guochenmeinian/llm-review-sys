# 4Diffusion: Multi-view Video Diffusion Model for 4D Generation

Haiyu Zhang1,2, Xinyuan Chen2, Yaohui Wang2, Xihui Liu3, Yunhong Wang1, Yu Qiao2

1Beihang University 2Shanghai AI Laboratory 3The University of Hong Kong

1{zhyzhy,yhwang}@buaa.edu.cn 2{chenxinyuan,wangyaohui,qiaoyu}@pjlab.org.cn

3xihuiliu@eee.hku.hk

https://aejion.github.io/4diffusion

Work done when Haiyu Zhang interned at Shanghai AI Laboratory.Corresponding author

###### Abstract

Current 4D generation methods have achieved noteworthy efficacy with the aid of advanced diffusion generative models. However, these methods lack multi-view spatial-temporal modeling and encounter challenges in integrating diverse prior knowledge from multiple diffusion models, resulting in inconsistent temporal appearance and flickers. In this paper, we propose a novel 4D generation pipeline, namely **4Diffusion**, aimed at generating spatial-temporally consistent 4D content from a monocular video. We first design a unified diffusion model tailored for multi-view video generation by incorporating a learnable motion module into a frozen 3D-aware diffusion model to capture multi-view spatial-temporal correlations. After training on a curated dataset, our diffusion model acquires reasonable temporal consistency and inherently preserves the generalizability and spatial consistency of the 3D-aware diffusion model. Subsequently, we propose 4D-aware Score Distillation Sampling loss, which is based on our multi-view video diffusion model, to optimize 4D representation parameterized by dynamic NeRF. This aims to eliminate discrepancies arising from multiple diffusion models, allowing for generating spatial-temporally consistent 4D content. Moreover, we devise an anchor loss to enhance the appearance details and facilitate the learning of dynamic

Figure 1: **4Diffusion** generates spatial-temporally consistent 4D contents from monocular videos.

NeRF. Extensive qualitative and quantitative experiments demonstrate that our method achieves superior performance compared to previous methods.

## 1 Introduction

In recent years, diffusion models have significantly impacted the era of image, video, and 3D generation. With the support of large-scale text-to-image diffusion models [43; 1] and 3D-aware diffusion models [44; 29; 52], many works [26; 39; 51; 9; 34; 41; 50; 30; 55; 27] leverage Score Distillation Sampling (SDS)  to distill the prior knowledge from diffusion models to optimize a 3D shape parameterized by NeRF  or 3DGS . Although they have attained faithful results, they only focus on creating static 3D shapes, neglecting the dynamics of objects in the real world.

Generating 4D content, i.e., dynamic 3D content, holds diverse applications in the virtual realm, including digital human, gaming, media, and AR/VR. The main challenge lies in creating 4D content with vivid motion and high-quality spatial-temporal consistency. The pioneering study MAV3D  introduces a two-stage method, which first learns a static 3D shape with a text-to-image diffusion model and then deforms the static 3D shape with a text-to-video diffusion model . However, MAV3D encounters the Janus problem and generates 4D contents with poor appearance and motion . To overcome these issues, the following works [4; 66; 28; 65] employ multiple diffusion models for distinct purposes. Specifically, these methods leverage 3D-aware diffusion models [44; 29] and text-to-image diffusion models  to achieve spatial consistency and visually appealing appearance. Akin to MAV3D, they utilize video diffusion models [2; 54; 46] to add motion to create 4D content.

The aforementioned methods utilize multiple diffusion models for 4D generation. As Fig. 2 illustrates, when diffusing images rendered from a 3D model, the 3D-aware diffusion model  generates multi-view images to address the spatial ambiguity. On the other hand, the 2D image diffusion model  produces a clean image with subtle details to refine appearance. The 2D video diffusion model  generates dynamic frames to ensure temporal consistency within the same viewpoint. However, there is no accurate guidance to ensure multi-view spatial-temporal consistency due to the lack of multi-view spatial-temporal modeling. Moreover, it is challenging to integrate diverse prior knowledge from multiple diffusion models, often leading to inconsistent temporal appearance and flickers as shown in the second row of Fig. 7.

In this paper, we present a novel 4D generation pipeline, namely **4Diffusion**, to create high-quality spatial-temporally consistent 4D content from a monocular video. Specifically, we propose a unified diffusion model, 4DM, to capture multi-view spatial-temporal correlations for multi-view video generation. To achieve this, we construct 4DM based on the powerful pre-trained 3D-aware diffusion model , which already ensures high-quality multi-view spatial consistency. We then seamlessly integrate a motion module into the 3D-aware diffusion model to extend the temporal modeling capability. Contrary to previous attempts [16; 14] that typically demand extensive large-scale video datasets for tuning the motion module, 4DM achieves reasonable temporal consistency and

Figure 2: **Challenges.** The denoised images from Stable Diffusion (SD) , MVDream , and ZeroScope . These diffusion models can not provide multi-view spatial-temporal guidance and exhibit discrepancies, making their integration challenging.

captures multi-view spatial-temporal correlations after training on only hundreds of multi-view videos. Importantly, we keep the parameters of the 3D-aware diffusion model unchanged to preserve the generalization ability and spatial consistency. 4DM provides multi-view spatial-temporal guidance for 4D generation. Therefore, we propose 4D-aware SDS loss to distill prior knowledge from 4DM to optimize 4D content parameterized by dynamic NeRF. This approach eliminates discrepancies arising from multiple diffusion models and stabilizes the optimizing process. Moreover, we use 4DM to generate anchor videos conditioned on the input monocular video and devise an anchor loss to enhance the appearance details, facilitating the learning of dynamic NeRF. Finally, we generate 4D content with high-quality spatial-temporal consistency and vibrant motion coherence with the input video as shown in Fig. 1. Qualitative and quantitative experiments demonstrate that our method achieves state-of-the-art performance on multi-view video generation and 4D generation from monocular videos.

To summarize, our contributions are as follows: **1)** We present **4Diffusion**, a novel 4D generation pipeline that generates high-quality spatial-temporal consistent 4D content from a monocular video with a multi-view video diffusion model. **2)** We propose a multi-view video diffusion model, 4DM, which provides multi-view spatial-temporal guidance for 4D generation. It trains on only hundreds of curated high-quality multi-view videos to capture multi-view spatial-temporal correlations. **3)** We combine 4D-aware SDS loss and an anchor loss based on 4DM to optimize dynamic NeRF, which stabilizes the training process and allows for generating high-quality 4D content.

## 2 Related Work

Recent breakthroughs in multiple research domains have significantly accelerated progress in 4D generation task. Here, we discuss the most relevant fields, including 3D generation, video and 3D-aware diffusion models, and 4D generation.

**3D Generation.** Recent studies in 3D generation can be classified into three categories: 3D generative methods [53; 17; 45; 36; 60; 3; 58; 12], feed forward methods [18; 49; 23; 67], and diffusion prior-based methods [26; 39; 51; 9; 34; 41; 50; 30; 55; 27]. Inspired by the advancements in 2D content creation, 3D generative methods utilize the robust  or flow-based  backbone to generate 3D data represented by Signed Distance Function (SDF) , voxel grid , triplane [8; 17; 45], or weights of neural network . However, these methods require time-consuming pre-training to fit each 3D data and are limited to creating a single category. Feed forward methods [18; 23] adopt image features extracted from the pre-trained visual encoder DINO  to reconstruct 3D representations through a highly scalable and efficient transformer-based decoder. Although they can produce a 3D shape in a few seconds, they demand extensive training on large-scale 3D datasets, which is impractical with limited 4D datasets for 4D generation. Furthermore, diffusion prior-based methods distill prior knowledge from diffusion generative models via SDS  to optimize 3D representations, enabling the generation of high-quality 3D shapes with strong generalizability. In contrast to static 3D generation, our method focuses on creating 4D content.

**Video and 3D-aware Diffusion Models.** With the success of large-scale text-to-image diffusion models [43; 1], recent works attempt to use diffusion models to generate more complex signals, including video and 3D. AnimateDiff  inserts a learnable motion module into the frozen text-to-image model for video generation, which preserves the efficacy of the text-to-image model while successfully modeling temporal information. Recent 3D-aware diffusion model Zero-1-to-3  adopts a stable diffusion model conditioned on relative camera pose and a single image for novel view synthesis. However, this method still suffers from the Janus problem and content drafting problem  due to the lack of explicit 3D modeling. Approaches like [31; 44; 59; 32; 52] leverage 3D-aware attention block to model the joint probability distribution of multi-view images, leading to spatially consistent generation. However, these approaches are incapable of producing multi-view consistent videos, due to the absence of temporal or spatial modeling.

**4D Generation.** Recently, several works have delved into 4D generation from various user-friendly prompts, such as text [47; 4; 66; 28], a single image [65; 66], and a monocular video [42; 20; 62]. The pioneering study MAV3D  proposes a two-stage method to optimize 4D representation, i.e., Hexplane , with both text-to-image and text-to-video diffusion models in a static-to-dynamic manner. To generate 4D contents with realistic appearance, Dream-in-4D  and 4D-fy  combine hybrid diffusion models. Specifically, they utilize 3D-aware and 2D diffusion guidance to learn a static 3D representation and incorporate video diffusion guidance to add motion. However, these diffusion models can not offer multi-view spatial-temporally consistent guidance and it is difficult to integrate diverse prior knowledge from multiple diffusion models, resulting in suboptimal results. In contrast to these approaches, we design a unified model to capture multi-view spatial-temporal correlations for 4D generation.

Similar to us, [20; 42; 63; 62; 57] generate 4D content from a monocular video. Consistent4D  introduces an interpolation-driven loss between two adjacent frames to enhance spatial-temporal consistency. However, Consistent4D lacks temporal modeling cross frames. DreamGaussian4D , 4DGen , and SC4D  combine 4D Gaussian Splatting [56; 19] into 4D generation pipeline. Although they notably reduce optimization time, they may result in blurred appearance and inaccurate geometry due to the explicit characteristics of Gaussians. STAG4D  proposes a training-free strategy to generate sparse anchor multi-view videos for 4D generation. In contrast, we propose a multi-view video diffusion model to provide multi-view spatial-temporal consistency guidance for 4D generation.

## 3 Method

Given a monocular video \(V=\{I_{j}|j=1,2,...,T\}\) with \(T\) frames and an optional textual caption, our goal is to generate a high-quality spatial-temporally consistent 4D content, capable of rendering from any novel viewpoint across the temporal dimension. In Sec. 3.1, we talk about 3D-aware diffusion models, employed as the initialization of our unified diffusion model. In Sec. 3.2, we propose a unified diffusion model 4DM to capture multi-view spatial-temporal correlations for multi-view video generation. Subsequently, we elaborate on distilling prior knowledge from 4DM to optimize 4D content parameterized by dynamic NeRF and devise an anchor loss to enhance the appearance details, as detailed in Sec. 3.3. Fig. 3 shows the overall pipeline of our method.

### Preliminary: 3D-aware Diffusion Models

3D-aware diffusion models learn spatial relationships from multi-view images for 3D generation and can serve as an initialization of our unified diffusion model. Recent works [31; 59; 32] mainly focus on generating multi-view images from predetermined sparse viewpoints and necessitate additional algorithms for 3D reconstruction. Although we can extend these methods to generate multi-view videos and employ 4D reconstruction algorithms, it is challenging to reconstruct high-quality 4D content from a limited number of viewpoints. Therefore, we design our unified diffusion model to generate multi-view videos from arbitrary viewpoints and choose ImageDream  as initialization.

Figure 3: **4Diffusion overview. Our method first trains a unified diffusion, named 4DM, by inserting a learnable motion module at the end of each frozen spatial module of ImageDream to capture multi-view spatial-temporal correlations. Given a monocular video and text prompt, 4DM can produce consistent multi-view videos. Then, we combine 4D-aware SDS and an anchor loss based on 4DM to optimize 4D content parameterized by Dynamic NeRF.**

Given four arbitrary orthogonal viewpoints under canonical coordination and a single image with an optional textual caption, ImageDream can synthesize four multi-view images that align coherently with the input. Specifically, ImageDream utilizes an adapter similar to IP-Adapter  to inject image prompts and a 3D self-attention module to capture spatial relationships.

### 4DM: Multi-view Video Diffusion Model

To maintain the spatial consistency and mitigate training complexity, we design our multi-view video diffusion model 4DM based on a pre-trained 3D-aware diffusion model (i.e., ImageDream ). Given a monocular video with an optional text prompt and four orthogonal novel viewpoints under canonical coordination, 4DM aims to generate four spatial-temporally consistent videos.

Although we can directly use the original ImageDream to generate a set of individual multi-view images to form multi-view videos, the result lacks temporal consistency as ImageDream has no layer for temporal modeling, as shown in Fig. 8. We thus add a zero-initialized motion module at the end of each block of the UViT network of ImageDream. Specifically, each motion module begins with group normalization and a linear projection, followed by two self-attention blocks and one feed-forward block. A final linear projection is then applied, after which the residual hidden feature is added back at the end of each motion module as detailed in Fig. 4. Then, each attention block \(i\) of 4DM includes a spatial module and a motion module \(l^{i}_{m}\). The spatial module comprises a 3D self-attention module \(l^{i}_{s}\) and a cross-attention module. We first concatenate the monocular video latent and four multi-view video latents encoded by VAE  to obtain a batch \(B\) of latents \(^{B F N C H W}\), where \(C\) is the number of channels, \(H\) and \(W\) are spatial resolutions, \(N=5\) is the number of viewpoints, and \(F\) is the number of frames. Subsequently, we reshape the temporal axis into the batch dimension and independently process multi-view video latents through the 3D self-attention module,

\[} (,B\;F\;N\;C\;H\;W(B \;F)\;N\;H\;W\;C),\] (1) \[}  l^{i}_{s}(}),\] (2) \[} (},(B\;F)\;N\;H\;W\;C  B\;F\;N\;C\;H\;W).\] (3)

Then, we use the adapter in ImageDream to individually process the input video frames and output the video features to perform cross-attention operations. Here, we also reshape the temporal axis into the batch dimension to prevent dimensional confusion. Furthermore, for the motion module, we perform self-attention exclusively along the temporal axis by reshaping the spatial dimensions and the viewpoint dimension into the batch dimension,

\[} (,B\;F\;N\;C\;H\;W(B \;N\;H\;W)\;F\;C),\] (4) \[}  l^{i}_{m}(}),\] (5) \[} (},(B\;N\;H\;W) \;F\;C B\;F\;N\;C\;H\;W).\] (6)

We utilize Objavverse dataset  to train 4DM. Although Objavverse provides nearly 44K animated 3D shapes, rendering multi-view videos and training a diffusion model are time- and computation-consuming using the entire dataset. Moreover, it is worth noting that the Objavverse dataset contains a significant amount of flawed data. Consequently, we manually select a curated subset of 926 high-quality animated 3D shapes from Objaverse dataset . We render multi-view videos from those animated 3D shapes to tune our motion module while holding the parameters of the origin ImageDream frozen. Surprisingly, 4DM successfully learns reasonable temporal dynamics and preserves the characteristics of the origin ImageDream model, including generalization ability, spatial consistency, and image understanding ability, even when trained on a small curated dataset. As Fig. 8 illustrates, 4DM generates multi-view spatial-temporal consistent videos, surpassing the performance of ImageDream. For more details on our dataset, please refer to supplementary material.

**Training Objectives.** For each animated 3D shape from our dataset, we render a monocular video \(V_{m}\) with a random viewpoint and four videos \(V_{o}\) with orthogonal viewpoints \(^{v}_{mv}\) and select \(F=8\) frames from each video at a stride of 4 to create our multi-view video dataset \(^{v}_{mv}=\{^{v}_{mv},\,y,^{v}_{r},^ {v}_{mv}\}\). Here, \(^{v}_{r}\) and \(^{v}_{mv}\) represent the video clips from \(V_{m}\) and \(V_{o}\). \(y\) is the text prompt captioned by Cap3D . Then, we use \(^{v}_{mv}\) following the diffusion loss to train 4DM,

\[_{MV}(,_{mv})& =_{,y,_{r},,t,} [\|-_{}(^{p};y,^{p}_{r}, ^{p},t)\|^{2}_{2}],\\ (^{p},^{p}_{r},^{p})& =(_{mv},,),&p\\ (_{mv},^{v}_{r},^{v}_{mv}),&1-p\\ \] (7)here, \(_{mv}\) represent the noisy video latents derived from \(_{mv}^{v}\). These latents are initially encoded by VAE and subsequently noised by random noise \(\) at a diffusion timestep \(t\). For more details about the noising process, please refer to . \(_{}\) is 4DM model parametrized by \(\). During the training of 4DM, we ensure that our training data does not overlap with the test data used in our experiments.

### 4D Generation

**Dynamic NeRF Representation.** Recent methods [25; 13] use neural networks or explicit spatial grids to map a 6D spatial-temporal coordinate \((x,,)\) to density \((x,)_{+}\) and view-dependent color \(c(x,,)_{+}^{3}\) of dynamic scenes, where \(x=o+\)\((>0)\) are sampled points along a ray originating at \(o\) with direction \(\) and \(\) denotes timestamp. Then, they leverage volumetric rendering to render images,

\[C=_{i}_{i}c_{i},_{i}=e^{-_{j<i}_{j}( _{j+1}-_{j})}(1-e^{_{i}(_{i+1}-_{i})}).\] (8)

Following 4D-fy  and iNGP , we use one multi-resolution spatial grid \(_{xyz}\) and one spatial-time planes \(_{xyzt}\) as 4D representation. Here, both \(_{xyz}\) and \(_{xyzt}\) use hash tables to store learnable features. Then, we acquire spatial-time features \(\) through interpolation and hash lookup on \(_{xyz}\) and \(_{xyzt}\). Finally, \(\) are decoded into density and view-independent color using tiny MLPs,

\[:,\ \ : c.\] (9)

The entire set of trainable parameters is denoted as \(_{}\). We can optimize our dynamic NeRF by using the multi-view videos generated from 4DM, however, 4DM can only produce four orthogonal viewpoints at one time. Training with such sparse views often results in overfitting to the training viewpoints, as presented in Fig. 6. To mitigate this, we leverage 4D-aware SDS to optimize the dynamic NeRF, enabling effective rendering from novel viewpoints across the temporal dimension, which is crucial for 4D generation.

Figure 4: The detailed overview of the architecture of motion module.

Figure 5: The illustration of multi-view video generation when input video exceeds 8 frames.

Figure 6: Illustration of directly optimizing on the generated multi-view videos.

**4D-aware SDS.** Once 4DM is trained, we employ 4D-aware SDS loss to guide the optimization of our 4D representation. To be concrete, we utilize Eqn. 8 to render four \(F\) frames video \(V_{r}\) with timestamps \(=\{_{1},_{2},...,_{}\}\) from four orthogonal viewpoints \(_{}\). Our 4D-aware SDS injects Gaussian noise \(\) into \(V_{r}\) at a diffusion timestep \(t\) and passes to our multi-view video diffusion to provide gradients to update \(_{}\),

\[_{_{}}_{} _{(_{},,,t)}2(V_{r}- })}{_{}},\] (10)

where \(_{0}\) denotes the pseudo ground truth denoised from 4DM with the input video \(V\) and viewpoints \(_{}\) as condition. Here, we replace original \(\)-based SDS loss with \(x_{0}\)-reconstruction loss as in .

**Anchor Loss.** Accurately estimating the elevation and azimuth of input monocular video within the canonical coordination is challenging, making it difficult to use the input video directly as supervision signals. Therefore, we utilize 4DM to produce four orthogonal videos conditioned on the input video and select the one with the viewpoint closest to that of the input video as the anchor video. This approach ensures that the anchor video maintains the same quality as the input and improves the results. Moreover, 4DM is currently limited to generating multi-view videos with 8 frames. When the input video exceeds 8 frames, we must apply our multi-view video diffusion model multiple times to generate anchor videos. However, this process may lead to temporally inconsistent results due to the stochasticity of the diffusion model, particularly when the viewpoint is far from the input video as shown in Fig. 5. This inconsistency would degrade the 4D generation performance. Finally, we devise an anchor loss \(_{}\) based on the anchor video to enhance the appearance details and facilitate the learning of dynamic NeRF. Since it is challenging for 4DM to ensure pixel-to-pixel alignment of the anchor video, we follow  to use image-level perceptual loss, i.e., LPIPS  and SSIM, for dynamic NeRF optimization,

\[_{}=_{1}(I_{r},I_{a})+_{2}(I_{r},I_{a}),\] (11)

where \(I_{r}\) and \(I_{a}\) represent the rendered video and anchor video, \(\) is the loss weight. Consequently, our total loss function for 4D generation is,

\[_{}=_{}+_{}+ _{3}_{}+_{4}_{}+_{5}_{},\] (12)

here \(_{}\), \(_{}\), and \(_{}\) are regularization loss in DreamFusion .

## 4 Experiments

**Implementation Details.** We implement 4DM under the Stable Diffusion framework and initialize it from the checkpoint of ImageDream. We train 4DM with multi-view videos with 256\(\)256 resolutions for 30,000 steps with a batch size of 32, using the AdamW optimizer with a learning rate of 1e-4. The training takes about 2 days with 16 NVIDIA Tesla A100 GPUs. Additionally, for 4D generation experiments, we optimize dynamic NeRF representation in an end-to-end manner, avoiding utilizing multiple stages as in previous works.

**Baselines.** To evaluate our method, we compare to two video-to-4D approaches, namely Consistent4D  and DreamGaussian4D , and one text-to-4D approach 4D-fy . We extend 4D-fy to video-prompt 4D generation by using ImageDream as the 3D-aware diffusion model. 4D-fy introduces hybrid SDS to blend gradients from multiple pre-trained diffusion models to create 4D contents. Consistent4D is the first study focusing on the video-to-4D task. They utilize a 3D-aware diffusion model to optimize a cascade dynamic NeRF and propose a consistency loss to address spatial-temporal inconsistency. DreamGaussian4D leverages 4D Gaussian Splatting for faster training.

### Comparisons on 4D Generation

**Qualitative Evaluation.** To validate 4Diffusion for 4D generation, we compare it to Consistent4D , DreamGaussian4D , and 4D-fy  on monocular video-to-4D task. Here, we use 3 real-world videos and 3 synthetic videos from the Consistent4D dataset, as well as 3 images from the ImageDream. As discussed in Sec.A.1 of the supplementary materials, for text-image pairs from ImageDream, we utilize SVD to generate input videos. We illustrate the results in Fig. 7. 4D-fy  achieves state-of-the-art results on text-to-4D task and can be simply extended to video-prompt4D generation by replacing 3D-aware diffusion model. Here, we utilize ImageDream as the 3D-aware diffusion model in 4D-fy. 4D-fy produces 4D contents with inconsistent temporal appearance, sometimes diverging significantly from the input video, as depicted in the first two columns of Fig. 7. This is primarily because integrating gradients from multiple diffusion models is difficult and they face challenges in multi-view spatial-temporal modeling. Consistent4D is the first work for 4D generation from monocular video. They employ an interpolation loss between two frames to enhance spatial-temporal consistency. However, they lack temporal consistency across frames, leading to poor appearance quality and flickers. DreamGaussian4D generates 4D contents with a blurred appearance and inaccurate geometry because GS struggles to model thin structures and large motions under unconstrained situations. In contrast, 4Diffusion generates high-quality 4D content with 4DM, which captures multi-view spatial-temporal correlations in a unified manner. Overall, our method achieves superior results, demonstrating its effectiveness. For more visualization results, please refer to our supplementary materials.

**Quantitative Evaluation.** We select 5 test cases from Objavverse, each consisting of a monocular input video and four orthogonal ground truth videos, which are not included in the training data, to evaluate our model. To evaluate image quality, we leverage CLIP-I  to measure the similarity.

Figure 7: 4D generation comparisons with 4D-fy , Consistent4D , and DreamGaussian4D .

We also calculate FVD to evaluate the video quality. We compute LPIPS  and PSNR metrics to evaluate the spatial consistency. Here, we use ground truth videos for novel viewpoints to compute the above metrics. Moreover, we compute CLIP-C between frames in each synthetic video to evaluate temporal consistency. Tab. 1 presents the results, clearly demonstrating that 4Diffusion outperforms other methods on all metrics.

### Multi-view Video Generation

**Qualitative Evaluation.** In this section, we evaluate the multi-view video generation quality produced by 4DM using the same input videos as described in qualitative evaluation in Sec 4.1. We employ ImageDream to synthesize a set of multi-view images as pseudo multi-view video by taking each frame of the input monocular video as an image prompt. Fig. 8 illustrates results with the first two columns corresponding to the input monocular video. Although ImageDream excels at synthesizing spatially consistent images, it struggles to model temporal correlations, leading to inconsistent temporal appearances, such as the icon on the back of Spiderman. Comparatively, 4DM effectively captures reasonable temporal information using the motion module, even when trained on a small curated dataset. Moreover, our model preserves the generalization ability of ImageDream, allowing us to generate high-fidelity multi-view videos, even beyond the distribution of our training dataset. As the last two rows of Fig. 8 show, 4DM produces spatially consistent videos by sharing information across spatial and temporal dimensions to constraint the generation process while ImageDream occasionally fails to generate videos coherent to the viewpoint.

**Quantitative Evaluation.** We use the same test cases described in quantitative evaluation in Sec 4.1, alongside the test data provided by Consistent4D, to evaluate 4DM. To account for the stochasticity of the diffusion model, we conduct five runs for each test case and report the average metrics. Tab. 2 shows comparative results. Despite the comparable performance in CLIP-I, 4DM excels in

    &  &  &  &  \\   & CLIP-I\(\) & CLIP-C\(\) & FVD\(\) & LPIPS\(\) & PSNR\(\) \\ 
4D-fy & 0.8658 & 0.9487 & 1042.3 & 0.2254 & 14.24 \\ Consistent4D & 0.9216 & 0.9723 & 706.07 & 0.1593 & 16.70 \\ DreamGaussian4D & 0.8898 & 0.9710 & 760.18 & 0.1793 & 15.97 \\  Ours(w/o \(_{}\)) & 0.8195 & 0.9503 & 1546.4 & 0.2356 & 13.92 \\ Ours(w/o \(_{}\)) & 0.8823 & 0.9720 & 853.57 & 0.1589 & 17.20 \\ Ours & **0.9310** & **0.9798** & **417.63** & **0.1199** & **19.07** \\   

Table 1: Quantitative evaluation on 4D generation.

Figure 8: The illustration of synthesized multi-view videos from 4DM and ImageDream . 4DM produces more spatial-temporal consistent results than ImageDream. \(T\) denotes the timestep of video clips. All results are generated from DDIM  sampler.

generating spatial-temporally consistent multi-view videos, a primary focus of our research. This is evidenced by the superior performance on metrics such as CLIP-C, FVD, LPIPS, and PSNR, which better capture the spatial and temporal fidelity of video content. These metrics demonstrate that our method effectively balances image quality with temporal consistency, making it a robust solution for multi-view video generation.

### Ablation study and analysis

**Effectiveness of the Curated Multi-view Video Dataset.** To evaluate the importance and effectiveness of the selected high-quality multi-view videos, we use the entire animated 3D shapes from Objavverse and render multi-view videos to fine-tune 4DM (Ours w/ whole). The results are shown in Tab. 2. Given the presence of numerous flawed data within the entire dataset, it compromises the image quality of ImageDream and encounters challenges in precisely capturing spatial-temporal correlations, demonstrating the importance of high-quality datasets for fine-tuning 4DM.

**4D-aware SDS Loss.** To evaluate the effect of our 4D-aware SDS loss, we substitute the 4DM with ImageDream and use 3D-aware SDS loss based on ImageDream to optimize dynamic NeRF representation. As Fig. 9 depicted, inconsistent temporal textures, such as the leg of the squirrel, emerge due to the lack of temporal modeling of ImageDream, underscoring the significance of capturing spatial-temporal correlations in 4DM. The quantitative results presented in Tab. 1 indicate the significance of our 4D-aware SDS loss.

**Anchor Loss.** We also assess the impact of the proposed anchor loss. As illustrated in Fig. 9, capturing detailed appearance features, such as the eyes of the squirrel, proves challenging without the anchor loss. Conversely, the anchor images furnish visual clues to facilitate the learning of 4D representation, resulting in high-quality 4D content. The quantitative results Tab. 1 demonstrate the crucial role of our anchor loss.

## 5 Conclusion

In this paper, we present 4Diffusion for 4D generation from a monocular video. Our method proposes a multi-view video diffusion model 4DM based on a 3D-aware diffusion model for multi-view video generation and provides multi-view spatial-temporal guidance for 4D generation. 4DM captures spatial-temporal correlations and preserves the characteristics of the origin 3D-aware diffusion model even when training on a small curated dataset. Then, we combine 4D-aware SDS loss and an anchor loss based on 4DM to optimize our hash-encoded dynamic NeRF, resulting in spatial-temporally consistent 4D contents coherent with the input monocular video.

    & Image quality & Tem. Con. & Video Quality &  \\   & CLIP-I\(\) & CLIP-C\(\) & FVD\(\) & LPIPS\(\) & PSNR\(\) \\  ImageDream & 0.9165 & 0.9320 & 465.94 & 0.1536 & 16.57 \\ Ours(w/ whole) & 0.8872 & 0.9478 & 583.79 & 0.1763 & 15.28 \\ Ours(4DM) & **0.9260** & **0.9601** & **427.34** & **0.1346** & **17.88** \\   

Table 2: Quantitative evaluation on multi-view video generation. Here, we employ Consistent4D test dataset to evaluate 4DM and ImageDream. ‘Spa. Con.’ and ‘Tem. Con.’ refer to spatial consistency and temporal consistency, respectively.

Figure 9: Ablation studies on 4D-aware SDS loss and the anchor loss.

## Acknowlegements

The work is supported by the National Key R&D Program of China (No. 2022ZD0160102), the National Natural Science Foundation of China under Grant No. 62102150, and the Science and Technology Commission of Shanghai Municipality under Grant No. 23QD1400800.