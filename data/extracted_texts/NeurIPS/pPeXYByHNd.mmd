# MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training

Bo Chen\({}^{1}\), Zhilei Bei\({}^{1}\), Xingyi Cheng\({}^{2}\), Pan Li\({}^{2}\), Jie Tang\({}^{1}\), Le Song\({}^{2,3}\)

\({}^{1}\)Tsinghua University \({}^{2}\)BioMap Research \({}^{3}\)MBZUAI

cb21@mails.tsinghua.edu.cn

https://github.com/THUDM/MSAGPT

BC and ZB contributed equally.Work done while interned at BioMap.

###### Abstract

Multiple Sequence Alignment (MSA) plays a pivotal role in unveiling the evolutionary trajectories of protein families. The accuracy of protein structure predictions is often compromised for protein sequences that lack sufficient homologous information to construct high-quality MSA. Although various methods have been proposed to generate virtual MSA under these conditions, they fall short in comprehensively capturing the intricate co-evolutionary patterns within MSA or require guidance from external oracle models. Here we introduce MSAGPT, a novel approach to prompt protein structure predictions via MSA generative pre-training in the low-MSA regime. MSAGPT employs a simple yet effective 2D evolutionary positional encoding scheme to model the complex evolutionary patterns. Endowed by this, its flexible 1D MSA decoding framework facilitates zero- or few-shot learning. Moreover, we demonstrate that leveraging the feedback from AlphaFold2 can further enhance the model's capacity via Rejective Fine-tuning (RFT) and Reinforcement Learning from AF2 Feedback (RLAF). Extensive experiments confirm the efficacy of MSAGPT in generating faithful virtual MSA to enhance the structure prediction accuracy (up to +8.5% TM-Score on few-shot scenarios). The transfer learning capabilities also highlight its great potential for facilitating other protein tasks.

## 1 Introduction

The advent of deep learning has significantly propelled progress across various scientific domains, exemplified by breakthroughs such as AlphaFold series  for accurate biomolecular interaction predictions, AlphaGeometry  for intricate geometry and mathematical reasoning----to name a few. Among these, AlphaFold2 (AF2) represents a landmark within structural biology, achieving an _in silico_ precision of approximately 90% atomic accuracy that rivals wet lab experiments on protein structure predictions (PSP). The remarkable success of AF2 can be attributed to its innovative end-to-end use of co-evolutionary information supported by Multiple Sequence Alignment (MSA). MSA aggregates homologous sequences from vast databases, providing a comprehensive overview of evolutionary trajectories, which is critical for accurately predicting protein structures . An illustrative example in Figure 1(a) showcases that the correlations analysis among amino acids (AAs) sites could reveal contacts or conservative regions in the folding structure. Unfortunately, not all proteins possess a rich set of homologous counterparts. Statistical investigations reveal that approximately 20% of metagenomic proteins  and around 11% of proteins from eukaryotic and viral origins  are classified as "orphan" proteins. This presents a significant challenge for MSA-search algorithms in constructing high-quality MSA, consequently impeding the performance of PSP models .

Drawing on the impressive capabilities of large language models endowed either by the autoencoding  or the autoregressive language modeling regime [8; 9], protein language models (PLMs) have been developed to unveil the evolutionary patterns and sequence characteristics intrinsic to protein structures. Specifically, generative PLMs [10; 11; 12], trained on vast protein databases [13; 14; 15; 16] have achieved unparalleled success in generating novel proteins with desired structural properties. These achievements underscore the efficacy of language models in identifying evolutionary patterns within individual protein sequences. Inspired by this, subsequent works [17; 18] attempt to further integrate MSA as the input or by directly generating virtual yet informative MSA [19; 20; 21] to provide additional evolutionary insights. These approaches usually adopt customized attentions that merely allow attention aggregated among specific directions, such as axial attention , for separately analyzing the row- and column-wise co-evolutionary patterns in MSA. However, these attention mechanisms usually have low efficiency in capturing the evolutionary information in MSA, or even fail to adequately capture intricate co-evolutionary dynamics. Taking Figure 1(a) as an example, it is imperative to concurrently analyze the pairwise or high-order relationships of amino acid sites across all homologs to deduce the structural constraints influencing the folding structures, which may not achieved by customized attention. The limited capacity may result in compromised performance on the task that highly resorts to co-evolutionary information.

Built upon the insights mentioned above, we introduce MSAGPT, a simple yet effective framework that prompts protein structure prediction via MSA generative pre-training. This method facilitates _de novo_ MSA generation, aiding in protein structure prediction in scenarios with limited MSA available. MSAGPT is characterized by its unique features:

\(\)**2D Evolutionary Positional Encoding.** We employ an innovative dual-axis positional encoding scheme that captures column- and row-wise co-evolutionary information concurrently. This method provides a comprehensive understanding of complex evolutionary relationships with high efficacy. enhancing the model's generative capabilities.

\(\)**1D Zero-/Few-Shot MSA Decoding.** With 2D positional encoding, MSAGPT re-formalizes MSA generation as a one-dimensional sequence generation task, optimized by the simple next-token-prediction objective. This enables MSAGPT to conduct zero- or few-shot MSA generation under a flexible in-context learning framework.

\(\)**Learning from AlphaFold2 Feedback.** MSAGPT further utilizes feedback from AlphaFold2 to reduce hallucinations during MSA generation. This approach ensures the generation of reliable and informative MSA, thus enhancing protein structure prediction.

Extensive experiments conducted on three benchmarks, CAMEO , CASP, and PDB , demonstrate the superior capacity of MSAGPT in generating high-quality MSA. Notably, MSAGPT outperforms existing MSA generation models on both zero- and few-shot scenarios. Impressively, AF2 with virtual MSA generated by MSAGPT significantly improves the structure prediction accuracy than that with natural MSA on cases with limited homologous information. Moreover, the subsequent Rejective Fine-tuning (RFT) and learning from AF2 feedback (RLAF) stage further enhance the model's ability to generate informative and faithful MSA, outperforming the original MSAGPT by a large margin, as shown in Figure 1(b). Additionally, we demonstrate that virtual MSA can also benefit other tasks.

Figure 1: **(a) The illustration of MSA and (b) performance comparisons between MSAGPT and advanced baselines on three natural MSA-scarce benchmark.**

We expect MSAGPT to become integral in supplementing protein-related tasks requiring critical evolutionary information from MSA. The model is available at https://github.com/THUDM/MSAGPT.

## 2 Related work

Protein Structure Prediction.Proteins are fundamental to the various biological processes that sustain, grow, and protect living organisms. Groundbreaking deep learning approaches [1; 2; 4] have been developed to predict the folding structures based on their sequences. These methods have achieved structure prediction accuracy to conventional wet-lab experiments. The success largely relies on the utilization of MSA, which are retrieved through search algorithms [24; 25; 26; 27] against vast databases [13; 14; 15; 16]. However, challenges arise with "orphan" protein sequences, which lack sufficient homologous sequences for accurate structure prediction. Single-sequence PSP methods [11; 28; 29; 30] are designed to infer folding structures directly from the query protein sequences. Despite these advancements, the accuracy of predictions from single-sequence methodologies generally falls short of those derived from MSA-based algorithms.

Protein Language Models.Protein Language Models (PLMs), such as ESM [28; 31], ProGen [10; 32], etc [12; 33; 34] have emerged as a groundbreaking development in computational biology. PLMs are trained on single sequences, towards understanding protein structural features or enabling the generation of diverse and realistic protein sequences. MSA Transformer  further incorporates MSA as the input, achieving better performance than these single sequence models, underscoring the importance of utilizing the evolutionary information from MSA [35; 36; 37]. To enhance MSA generation, MSA-Augmentor , PoET  employ the seqs2seqs pre-training, which adopts the sequential axial attention mechanism to capture the evolutionary data across and within the input sequences, both horizontally and vertically. EvoGen , serving as the meta generative model, aims at producing virtual MSA for enhancing protein structure predictions. However, it highly resorts to external structural prediction models to optimize its performance.

Aligning with Human Preferences.Aligning language models with human preferences has been shown to be effective in improving generation quality [8; 38; 39; 40]. Existing methods typically employ supervised fine-tuning using human-annotated datasets or reinforcement learning from human feedback pipelines [38; 39]. Inspired by these, we utilize the feedback from AlphaFold2 to further enhance the generation capability of the pre-trained model, which helps mitigate hallucinations and enables the model to generate accurate and reliable MSA.

## 3 Preliminary

**Definition 1**: _Multiple Sequence Alignment (MSA). Given the query protein sequence \(Q^{1 L}\), where \(\) denotes the set of alphabetic symbols used to represent the 20 basic amino acids and

Figure 2: **The overall framework of prompting protein structure predictions via MSA generation. Left: The challenge faced by conventional search algorithms on protein with scarce homologous sequences, resulting in suboptimal alignments. Middle-to-Right: MSAGPT generates informative and high-quality MSA for such challenging queries, presenting a promising approach to overcoming these limitations. [M] denotes the sequence separator. [S], [E] are the special tokens to represent the start or end of MSA generation.**

represents the number of amino acids per sequence, the MSA \(M^{N L}\) of \(Q\) is comprised of \(N\) homologous protein sequences, which can be obtained either by searching over protein databases or generating with MSA generation methods._

**Problem 1**: _Prompting Protein Structure Prediction by MSA Generation. Given \(Q\) with initial MSA \(M_{}^{n L}\) as the prompt, where \(n=0\) indicates the zero-shot MSA generation and \(n>0\) signifies the few-shot MSA generation, we target at learning a function \(f\) to generate virtual MSA \(M_{}^{m L}\) based on \(Q\) and \(M_{}\), such that the structure prediction accuracy based on the augmented MSA \(M_{}^{(n+m) L}\) significantly surpasses that based on the initial MSA \(M_{}\),_

\[M_{} =f(Q,M_{}),\] \[_{acc}(Q,M_{}) >_{acc}(Q,M_{})\]

_where the \(_{acc}\) is prediction accuracy comparing the prediction result of AF2 and the ground truth._

In this paper, we mainly focus on improving the structure prediction accuracy in the low-MSA regime, i.e., the cases that lack a sufficient number of homologous sequences.

## 4 Methodology

Given a query sequence and its retrieved natural MSA, we aim to comprehensively understand the co-evolutionary patterns in MSA, such that we can generate informative virtual MSA for prompting protein structure prediction in the low-MSA regime. Conceptually, the co-evolutionary information is analogous to the covariance matrix in mathematics, depicting the correlations among amino acids by comparing pairwise or high-order correlations among amino acid sites in MSA, as depicted in Figure 1(a). To achieve this goal, MSAGPT contains two key adoptions, distinguishing it from existing MSA-based PLMs that rely on customized attentions [2; 17; 20; 19]: **2D Evolutionary Positional Encoding.** Introduces an adaptive dual-axis positional encoding scheme that captures column- and row-wise co-evolutionary information concurrently. And **1D Zero-/Few-Shot MSA Decoding.** Re-formalizes MSA generation as a one-dimensional sequence generation task based on the proposed 2D positional encoding scheme, which enables MSAGPT to conduct zero- or few-shot context learning MSA generation framework. The overall framework is illustrated in Figure 2.

### 2D Evolutionary Positional Encoding

Vanilla transformers typically use 1D positional embeddings to incorporate absolute and relative positional information of tokens. However, when dealing with MSA, which represents stacked homologs, the structure is different. Each row of MSA corresponds to a distinct protein sequence, while each column represents the evolutionary trajectories of a specific amino acids (AAs) site. To effectively capture the evolutionary patterns, recent approaches [2; 17; 20] have employed decoupled axial attentions, which are designed to capture explicit co-evolutionary information along the rows (protein sequences) and columns (AAs sites). However, these methods often suffer from low efficiency in capturing the information dynamics or fail to capture the evolutionary information adequately.

In light of this, we introduce a novel two-dimensional evolutionary positional encoding scheme, illustrated in Figure 2. Given an MSA \(^{N L}\), we define a 2D positional id matrix \(^{2 N L}\), where the first positional id \(}^{1 N L}\) indicates the column position, i.e., \(P_{0}[i,]=\{0,1,,L\}\), and the second positional id \(}\) indicates the row position, i.e., \(P_{1}[j,]=\{0,1,,N\}\). The two positional ids are projected into two vectors added to the input token embeddings. We utilize the Rotary Positional Encoding (RoPE)  technique, specifically adapting its two-dimensional variant* to suit our 2D positional encoding requirements.

Footnote *: https://kexue.fm/archives/8397

**Comparison with Axial Attentions.** Considering the 2D positional id (\(P_{0},P_{1}\)), the self-attention among AAs (\(\), \(\)) meets the following unit patterns, as illustrated in Figure 3:

\(\)\(P_{0}^{}\) = \(P_{0}^{}\) & \(P_{1}^{} P_{1}^{}\). Indicates \(\) and \(\) reside in the same site across different protein sequences, such as the AA pair (A, K) and (P, G), enabling column-wise self-attention that highlights evolutionary patterns conserved across sequences.

\(\)\(P_{0}^{} P_{0}^{}\) & \(P_{1}^{}\) = \(P_{1}^{}\). Suggests \(\) and \(\) are positioned in the same protein sequence but at different sites, such as the AA pair (A, P) and (K, G), facilitating row-wise self-attention that captures sequence-specific features.

\(\)\(P_{0}^{} P_{0}^{}\) & \(P_{1}^{} P_{1}^{}\). Denotes \(\) and \(\) lack explicit correlation, such as the AA pair (A, G) and (P, K), may be serving as anchor nodes for complex co-evolutionary information diffusion.

Conceptually, the 2D positional encoding encapsulates the explicit row- and column-wise self-attention patterns with high efficacy. Moreover, it allows unrestricted information diffusion, that is, enabling any two amino acids to attend to one another. Such a framework facilitates unveiling complex co-evolutionary patterns, such as high-order correlations among AAs, that customized self-attentions might overlook.

### 1D Zero-/Few-Shot MSA Decoding

Leveraging the 2D evolutionary positional encoding, we further release the stacked MSA decoding task into the scalable 1D sequence generation framework, without compromising the integrity of co-evolutionary information. Specifically, we convert the MSA \(^{N L}\) into the flatted 1D sequence \(^{f}^{1 NL}\) along the row axis to ensure that we can generate the MSA sequentially during inference. Similarly, the 2D positional id matrix \(^{2 N L}\) is reshaped into a flattened format, \(^{f}^{1 2 NL}\). This allows the model to conduct a simple auto-regressive generation process, as illustrated in Figure 2.

**Discussions.** Admittedly, introducing 2D positional encoding introduces higher time complexity in comparison to conventional customized attention mechanisms (from \(O(N^{2}L)+O(NL^{2})\) to \(O(N^{2}L^{2})\)). However, it is worth noting that the original stacked nature of MSA poses challenges for integrating it with acceleration techniques used in large language models, such as Flash Attention [42; 43]. The 1D decoding framework, conversely, can be easily scaled to accommodate in-context learning frameworks, such as retrieval augmented generation, to further enhance the model's generation capability and expand its application range. From a practical standpoint, the high parallelism of the 1D decoding framework demonstrates superior inference speed, benefiting from techniques like Flash Attention and KV-cache, while incurring negligible memory overhead compared to customized attention mechanisms. For further details, please refer to Appendix A.4.

## 5 The Training Pipeline of MSAGPT

The training pipeline involves three successive stages: **Stage 1: MSA Generative Pre-Training** to obtain the base MSA generation model; **Stage 2: Rejective Fine-tuning (RFT)** to instruct the base model with high-quality MSAs via AF2 annotations, which can reduce generation hallucinations ; **Stage 3: Reinforcement Learning from AlphaFold2 Feedback (RLAF)** to further enhance RFT model's capabilities based on the feedback of AF2. (See Appendix Section A for training details.)

### Stage 1: MSA Generative Pre-Training

**Pre-Training Dataset.** We utilize the Uniclust30 MSA dataset from OpenProteinSet , which is processed through an all-against-all search on Uniclust30  using HHblits . This results in approximately 16 million MSAs (See Appendix A.1 for Details).

**Pre-training Objective.** We adapt the language modeling objective  to the MSA generation task. The cross-entropy loss for modeling the intrinsic distribution of MSA \(^{f}^{1 NL}\) is defined as:

\[L_{}=_{^{f}}[_{i=0}^{N L}- p( _{i}^{f}|_{<i}^{f},)]\] (1)

Figure 3: Comparisons among the axial attention (exemplified by ) and the one in MSAGPT in a single layer. Here we focus on the information aggregated to the AA “G”. The 2D evolutionary position enhanced attention shows higher efficiency than the decoupled axial attentions with one-step aggregation to attain sufficient information.

where \(^{f}^{1 NL}\) is 1D flatted version of the input MSA and \(\) is the learned parameter.

### Stage 2: Rejective Fine-tuning (RFT)

Noted that the pre-trained dataset inevitably contains noisy co-evolutionary patterns, such as large portions of deletions and insertions, which may mislead the base model to yield hallucinated cases, i.e., the linguistically reasonable but intrinsically unfaithful MSA. Thus we select highly-quality MSAs to further fine-tune the base model via a rejective sampling procedure based on the AF2-annotation.

RFT Dataset.We collect 120,780 protein sequences with structures from Protein Data Bank (PDB) . For the sequence \(Q\), we search its MSA \(M^{N L}\) from UniClust30  with HHblits . Then we sample several MSA subsets \(=\{m_{1},m_{2},...,m_{i}\}\) with replacement, where \(m_{i}^{n L}\) and \(n N\). To assure the information density of the sampled data, we filter out the MSA with depth \(N\) fewer than \( n i/2\). Subsequently, we employ AF2 to score the sampled subset using the structure prediction accuracy \(_{acc}(Q,m_{i})\). Then the RFT dataset \(_{}\) is defined as:

\[_{}=\{(Q,m_{i})|(_{acc}(Q,m_{i}))>_{1} (_{acc}(Q,m_{i})-_{acc}(Q,-))>_{2}\}\] (2)

where \(_{acc}(Q,-)\) indicates the prediction accuracy without using MSAs. We set the sampling number \(i=10\), the depth of sampled MSA \(n=16\), \(_{1}=0.9\), and \(_{2}=0.2\), which results in approximately 60k samples. The base model is fine-tuned on \(_{}\) with the same pre-training objective.

### Stage 3: Reinforcement Learning from AlphaFold2 Feedback (RLAF)

We further employ AF2 as the reward model to perform the Reinforcement Learning with AF2 Feedback (RLAF) using Direct Preference Optimization  (DPO) to further guide the RFT model to decode meaningful structure-related MSA patterns that align with the preference of AF2.

RLAF Preference Dataset.For each query \(Q\) from the PDB, we use the RFT model to generate its MSA \(M^{N L}\) in zero-shot manner. Then, we also sample several MSA subsets \(=\{m_{1},m_{2},...,m_{i}\}\) and obtain the preference dataset \(_{}=\{Q^{(k)},m_{}^{(k)},m_{}^{(k) }\}_{k=1}^{K}\) as follows,

\[_{}=\{(Q,m_{},m_{})|(_{ acc}(Q,m_{})-_{acc}(Q,m_{}))>_{3}\}\] (3)

where we set the \(_{3}=0.3\), rendering the number of preference data \(D_{}=11k\).

RLAF Training Objective.The adapted DPO loss is defined as:

\[L_{}=_{(Q,m_{},m_{})_{ }}[-((m_{}|Q) }{_{}(m_{}|Q)}-(m_{}| Q)}{_{}(m_{}|Q)})]\] (4)

where \(_{}\) and \(_{}\) are initialized by the RFT model and \(_{}\) is frozen while \(_{}\) is optimized. During the RLAF training phase, we found that merely using the DPO loss led to training instability. Thus we adopt the pre-training loss \(L_{}\) for the chosen answer \(m_{w}\) as a regularization term with the coefficient factor \(\) in the total loss to mitigate this issue. The total loss \(L=L_{}+ L_{}\), \(=0.1\). Another critical coefficient \(\), which measures the penalty intensity for incorrect answers is set to \(=0.1\).

## 6 Experiments

### Setup

Benchmarked Dataset.We employ the datasets from CAMEO , CASP14&15, and PDB , which are esteemed benchmarks in protein structure analysis spanning a diverse array of biological protein families. For each protein sequence, we search its MSA on UniClust30 database  using HHblits . Given our focus on addressing the challenge presented by cases with limited MSA information, we establish two specific benchmarks to represent the MSA-scarce conditions:

[MISSING_PAGE_FAIL:7]

Evaluation Metric.We use several widely-used metrics to assess structural similarity between predicted structures and ground truth: TM-Score, GDT-TS, and LDDT. Additionally, we include pTM and pLDDT, the corresponding predicted metrics estimated by AF2. All metrics are normalized from 0 to 100 for comparison, with higher scores indicating higher confidence and usually a more accurate prediction (see Appendix Section B for details).

### MSAGPT's Virtual MSAs Reflect the Co-evolutionary Information

Table 1 and 2 showcase the comparative results in two benchmarks across different baselines. Notably, AF2 MSA, which relies solely on the limited searched MSA without incorporating virtual MSA, exhibits the worst performance. Predictions enhanced with MSA generated by MSA-Augmentor or EvoGen surpass the performance of AF2 MSA. This underscores the critical role of high-quality MSA in enhancing the accuracy of cutting-edge PSP algorithms. Overall, MSAGPT surpasses other advanced baselines by a large margin on both benchmarks, specifically achieving +1.4% improvement on CAMEO, +8.5% on CASP, and +4.7% on PDB, as measured by TM-Score, on the natural MSA-scarce benchmark. This significant improvement demonstrates not only the superior accuracy and effectiveness of MSAGPT but also its robustness in handling cases with noisy or low-quality MSA.

Moreover, compared with the base model, the RFT and DPO models achieve higher golden metric scores, that is, GDT, LDDT, and TM-Score, but with a lower predictive score, that is, the value of pTM and pLDDT. This discrepancy might arise from the presence of highly confident (according to pTM and pLDDT) but lower-scored decoys (according to TM-Score), as observed in , indicating that aligning with the preference dataset, which is filtered based on TM-Score, makes the model more inclined to generate truly informative MSA rather than hallucinated ones.

Statistically, MSAGPT effectively improves the prediction accuracy for 91.0% and 88.9% of protein sequences with limited MSA when compared to AF2 MSA on Zero-Shot and Few-shot scenarios, respectively. This significant finding highlights the potential of our MSAGPT framework to uncover and leverage co-evolutionary patterns within biosequences. Notably, we also discuss the scenario with abundant natural MSA in the Appendix Section B.2.

### Rethinking the MSA Selection Strategy

We further study the effect of different depths of virtual MSA, as shown in Figure 4(a). We observe a trend where the relative improvement in structure prediction accuracy decreases as the depth of virtual MSA increases. The accuracy based on MSA with 64 MSA sequences even underperforms those based on only 16 or 32 sequences. We hypothesize that increasing the number of virtual MSA beyond a certain threshold may introduce a dilution effect, where the density of valuable co-evolutionary signals is compromised by the inclusion of the hallucinated generation noise. To alleviate this, we explore MSA selection strategies for filtering out low-quality, noise-inducing sequences while retaining those that contribute positively to the accuracy of structure predictions, as illustrated in Figure 4(b) (See Appendix Section C for details).

1D Sequence Similarity or Diversity Measure.We first arrange MSA by their similarity to the query sequence in descending order. The results reveal that prioritizing MSA based on their high similarity to the query, termed as _static similarity (STA-SIM)_, does not improve prediction accuracy compared to the non-selection approach (N/A). On the contrary, the _static diversity (STA-DIV)_ strategy, which favors MSA with lower similarity rankings, slightly outperforms the baseline, highlighting the importance of sequence diversity in enhancing MSA quality. Moreover, we employ the dynamic approach, initially selecting the most (or least) similar MSA to the query sequence and progressively incorporating additional MSA based on their average similarity to the cumulatively selected set, termed as _dynamic similarity (DYN-SIM)_ and _dynamic diversity (DYN-DIV)_.

The results further confirm the advantage of fostering diversity within MSA rather than selecting only the sequences with high similarities to the query sequence. We also inspect the effectiveness of the widely-adopted MSA _trimming (TRIM)_ strategy , which yields a similar TM-Score to the non-selection baseline, undermining its efficacy in selecting MSA with high quality.

**3D Structure Affinity Measure.** We assume that the generated sequence with high quality should exhibit structural congruity with the query sequence, thereby emitting strong co-evolutionary signals. To validate this, we rank sequences within MSA by their predicted tertiary structures according to the pTM, a predicted TM score , pLDDT, and TM-Score, from highest to lowest. These approaches, especially when guided by the pLDDT score, consistently select high-quality MSA, evidenced by the enhanced TM-Score. We compare the non-selection methods (N/A) and pLDDT selection methods on the three benchmarked datasets on few-shot generation scenarios in Table 3. This confirms our hypothesis that structural similarity plays a crucial role in effective MSA selections.

### Transfer Learning of MSAGPT

Since protein structures largely dictate their functions, the virtual MSA, enhancing structure prediction, should similarly benefit other protein tasks. To validate this, we focus on two protein structural tasks: Contact Prediction (CtP) and Secondary Structural Prediction (SsP) and two protein functional tasks: Localization Prediction (LocP) and Metal Ion Binding (MIB) . We sample 1,000 sequences from each benchmark and conduct 5-fold cross-validation (See Appendix Section B.3 for details).

**Results.** Table 4 demonstrates that incorporating MSA from MSAGPT consistently surpasses merely using the single sequence on most tasks. Yet, it achieves inferior performance on the LocP task, which agrees with the observation  that protein language models may not present scaling behavior on several protein functional or property prediction tasks. Nevertheless, the results show the great potential of MSAGPT to contribute to a wide range of tasks with generated MSA. We are motivated to explore additional transfer tasks to assess MSAGPT's utility across various domains further.

### Ablation Study

To understand the effect of various positional encoding strategies on capturing co-evolutionary patterns, we design four model variants: **1D_gpt**: Adopts the standard GPT positional encoding; **1D_2nd**: Utilizes only the second-dimensional of the 2D evolutionary positional encoding mechanism; **1D_1st**: Utilizes the first-dimensional positional encoding; **2D_full**: Implements the 2D evolutionary positional encoding mechanism (See Appendix Section B for details).

**Results.** Figure 5 showcases the TM-score distribution across different model variants. The 1D_gpt exhibits the lowest performance, attributed to its simplistic approach of treating the MSA as a concatenation of homologous sequences, thereby failing to discern any co-evolutionary patterns. Both the 1D_1st and 1D_2nd demonstrate significant improvement over 1D_gpt, by explicitly encoding column- or row-wise relationships within the MSA, respectively. Notably, the performance of 1D_1st is better than that of 1D_2nd, suggesting that column-wise covariance patterns play a more crucial role in structural predictions than row-wise patterns. This aligns with the understanding that the permutation of sequence order does not alter the covariance information among residue sites . Remarkably, the 2D_full variant, which incorporates the proposed 2D evolutionary positional encoding, outperforms all other models, which underscores its effectiveness in capturing the intricate evolutionary information present in MSA.

## 7 Limitations

In this section, we discuss some limitations that should be resolved in future work.

Scaling behavior of MSAGPT.While we have showcased the effectiveness of MSAGPT in generating informative virtual MSA, it is important to note that our pre-training was conducted with a model containing 2.8 billion parameters. The performance and behavior of MSAGPT, when scaled concerning dataset size, model size, and total compute resources, remain unknown.

Transfer Learning on a wide range of tasks.While we have demonstrated the transferability of MSAGPT on several tasks, including protein structure prediction and protein function prediction, its performance on a broader range of tasks remains an open question. The ability of a model to transfer its learned knowledge and adapt to new tasks is a critical aspect of transfer learning. While MSAGPT has shown promising results on the tasks it was evaluated on, it is important to assess its performance on a more diverse set of tasks spanning various domains and problem types.

## 8 Border Impact

The aim of this paper is to improve the accuracy of protein structure prediction in cases with limited homologous sequences. The generated MSA also shows great potential to transfer to other protein-related tasks. By leveraging the information encoded in the generated MSAs, it is possible to enhance the performance of various protein-related tasks beyond structure prediction. However, the generative MSA may be misused to contaminate the high-quality nature MSA databases. Thus, it is necessary to train a classifier to distinguish the real from MSAGPT-generated MSA.

## 9 Conclusion

This paper introduces MSAGPT, a novel approach that prompts protein structure prediction via MSA generative pre-training, to enable accurate protein structure predictions in situations where co-evolutionary information is scarce. To meticulously characterize the co-evolutionary patterns within MSA, MSAGPT designs two innovative techniques: the 2D Evolutionary Positional Encoding scheme and the 1D Zero-/Few-Shot MSA Decoding mechanisms. The post-alignment learning from AlphaFold2 feedback further enhances the quality of MSA generation. Empirical experiments conducted on a variety of benchmarks have demonstrated MSAGPT's robustness and effectiveness. In the future, we plan to apply MSAGPT to broader areas, particularly for tasks that heavily rely on co-evolutionary information, and investigate the aforementioned limitations.

Acknowledgments.This work has been supported by the NSFC for Distinguished Young Scholar 62425601, New Cornerstone Science Foundation through the XPLORER PRIZE and Tsinghua University Initiative Scientific Research Program.