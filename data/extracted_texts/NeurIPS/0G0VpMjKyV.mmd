# Sketching for Distributed Deep Learning:

A Sharper Analysis

 Mayank Shrivastava

University of Illinois Urbana-Champaign

mayanks4@illinois.edu

&Berivan Isik

Google

berivan@google.com

&Qiaobo Li

University of Illinois Urbana-Champaign

qiaobol2@illinois.edu

&Sanmi Koyejo

Stanford University

sanmi@cs.stanford.edu

&Arindam Banerjee

University of Illinois Urbana-Champaign

arindamb@illinois.edu

Part of the work done while BI was at Stanford University.

###### Abstract

The high communication cost between the server and the clients is a significant bottleneck in scaling distributed learning for overparametrized deep models. One popular approach for reducing this communication overhead is randomized sketching. However, existing theoretical analyses for sketching-based distributed learning (sketch-DL) either incur a prohibitive dependence on the ambient dimension  or need additional restrictive assumptions such as heavy-hitters . Nevertheless, despite existing pessimistic analyses, empirical evidence suggests that sketch-DL is competitive with its uncompressed counterpart, thus motivating a sharper analysis. In this work, we introduce a sharper ambient dimension-independent convergence analysis for sketch-DL using the second-order geometry specified by the loss Hessian. Our results imply ambient dimension-independent communication complexity for sketch-DL. We present empirical results both on the loss Hessian and overall accuracy of sketch-DL supporting our theoretical results. Taken together, our results provide theoretical justification for the observed empirical success of sketch-DL.

## 1 Introduction

Distributed learning is a popular framework for training machine learning models, often deployed to support large-scale deployments and to support privacy, among other systems goals . To this end, distributed learning generally employs both server and client devices. Most standard implementations are set up in rounds: in each round, clients participate by performing (multiple) local model updates using their local data, then share the model updates with a server. The server aggregates the collected local updates and broadcasts either the aggregate or the updated global model to the clients for the next round. As a result of the system setup, distributed learning generally requires a high communication overhead due to the frequent communication between the server and the clients . Federated learning, a popular distributed learning setup, further extends the generic setting to where clients' data distribution may be non-i.i.d. and the clients may change over time, thus suffering from the same costly communication. This communication overhead has motivated research in compressingthe model updates [35; 43] through sparsification [44; 73; 7; 30; 31], quantization [4; 74; 10], low-rank projection [71; 67; 68; 8], and sketching [59; 62; 50; 33; 27]. Among these, linear sketching mechanisms have attracted significant attention due to their efficient and simple integration with existing distributed and federated learning frameworks. For instance, unlike many sparsification approaches [44; 30], sketching is an unbiased operation that does not require bias correction with error feedback mechanisms  especially as error feedback typically increases the memory cost and the complexity of integrating differential privacy . Moreover, the linearity of the sketching operation makes it compatible with secure aggregation [12; 9] as opposed to the popular Top-r2 sparsification  or quantization methods  that break the linearity in aggregation.

Despite the rising interest in sketching for distributed and federated learning, the existing bounds on the convergence error of sketching scale with the ambient dimension of the model  (i.e., in contrast to the lower sketching dimension) which may be extremely large for modern overparameterized deep models [39; 20; 11]. This dimension dependence has been a limitation of the sketching mechanisms at scale, making them less attractive for modern deep models. We identify and demonstrate the root cause of the dimension dependence of the standard optimization convergence analysis as associated with the assumption of isotropic smoothness of the loss, commonly made in such analyses [62; 59]. Several modifications have been proposed to get rid of the dimension dependence. For instance, Rothchild et al.  assume that the model updates at the clients have heavy hitters and, further, apply a Top-r sparsification to break the dimension dependence. However, their heavy hitter assumption may not hold in general, and the Top-r operation introduces a bias that is, in turn, eliminated via error feedback. While their analysis requires these restrictive assumptions and modifications to eliminate the dimension dependence, they also note that they actually do not observe an adverse effect of dimension dependence empirically even without these restrictions, indicating that the theoretical analysis of their sketching algorithm is not quite explaining the empirical success. Quoting from Appendix B.3 of Rothchild et al. , "However, this dimensionality dependence does not reflect our observation that the algorithm performs competitively with uncompressed SGD in practice, motivating our assumptions and analysis." Motivated by this mismatch between the existing analyses of sketching for distributed learning (which impose dimension dependence) [59; 62] and its empirical success in practice, we provide a substantially tighter analysis than the prior work and eliminate the dimension dependence in sketching without imposing any unrealistic restrictive assumptions in the setup.

Our sharper analysis avoids the global isotropic smoothness assumption and utilizes the anisotropic restricted strong smoothness (RSS) property of overparameterized deep models [6; 72; 22; 53; 52; 15], and our results are in terms of the second order geometry of the loss. We present optimization results for single-local step as well as \(K\)-local step distributed learning, and present bounds on communication complexity based on the optimization results. We refer the reader to Table 1 for an overview of our results. We do not restrict our analysis to any specific sketching matrix, and our results hold for any symmetric sub-Gaussian sketching matrix. For instance, for computational benefits, one can use the popular Count-Sketch  or Hadamard sketch  approaches, which are both examples of sub-Gaussian sketching. Our contributions can be summarized as follows:

1. We identify and demonstrate the widely used global smoothness assumption of loss functions as the root cause of the dimension dependence of sketching methods.
2. We provide a novel analysis for sketching in distributed and federated learning that eliminates the ambient dimension dependence in the convergence error.
3. We are the first to break this dimension dependence without making restrictive assumptions such as the heavy hitter assumption by Rothchild et al. .
4. We are again the first to do this without a Top-r sparsification step in the framework which would have required additional measures to eliminate the bias.
5. We are the first to use more precise second-order properties of the loss of deep models, e.g., approximate restricted strong smoothness, eigenspectrum of Predictor Hessian, to analyze distributed and federated learning frameworks.

## 2 Related Work

Communication-Efficient Distributed Learning.The high cost of communicating model updates between the clients and the server has motivated a recent interest in improving communication efficiency in distributed and federated learning. One common strategy called FedAvg  enables less frequent communication by letting the client perform multiple local iterations at every round. Another common approach is to compress the model updates before the communication to reduce the cost of each round. These efforts can broadly be categorized into sparsification [3; 73; 44], quantization [64; 4; 74], low-rank factorization [67; 51; 71], sketching [59; 62; 34], and sparse subnetwork training techniques [31; 40; 41; 47; 32]. While some of these compression methods are already unbiased , many are biased and have to be combined with other mechanisms to reduce the bias for better convergence [44; 59]. Linearity is another desired feature, as it simplifies the implementation of distributed learning with security-promoting techniques like secure aggregation (in the compressed dimension). Among the general compression approaches mentioned, sketching stands out as a simple linear and unbiased operation, allowing for computations in the reduced dimension before desketching. We do not propose a new compression method in this work but instead provide a substantially improved convergence analysis for sketching-based frameworks that breaks the dimension dependence-which explains why sketching would not explode the converge error in large models.

Sketching.Over many years, sketching has been a fundamental tool for many applications, even before the surge of deep learning in 2010s [18; 25; 36] for low-rank approximation , graph sparsification , and least squares regression . More recently, sketching has also found use in distributed and federated learning frameworks to reduce the dimension of the model updates for communication efficiency [34; 33; 59; 62; 27]. The linearity of these sketching-based frameworks has led to their successful integration with secure aggregation and differential privacy as well [16; 66; 62; 17]. Despite the empirical success of these sketching-based applications in distributed and federated learning, the existing upper bounds on the convergence errors have a dependence on the ambient dimension of the model-which limits their scalability to larger models. In this work, we provide a tighter convergence analysis and get rid of the dimension dependence, suggesting the promise of sketching at scale. The closest to our work is by Rothchild et al.  who also provide a

Table 1: Comparison of iteration complexities and assumptions with prior work. Our results scale with \(\) which captures the intrinsic dimension  of the model rather than ambient dimension \(p\). SC refers to strong-convexity and PL refers to PL-condition. \(\) refers to the PL constant as well as the strong convexity parameter under SC assumption. According to Lemma C.2 the loss function \(\) can be shown to be \(\)-smooth and \(m\) refers to the width of the neural network.

dimension-independent convergence bound for their sketching algorithm, called FetchSGD (which is a combination of Count-Sketch projection, Top-r sparsification, and bias reduction with error feedback), under the assumption that the model updates have heavy-hitters. We note that both the heavy-hitter assumption and the Top-r step (and the error feedback coming along to minimize the bias) are necessary to get rid of the dimension dependence in their convergence analysis. We avoid both of these restrictions. Our key contributions over FetchSGD are: (1) We do not make the heavy-hitter assumption since it does not hold in general. (2) We do not require Top-r sparsification to break the dimension dependence - this way, we have an unbiased sketching mechanism without the need for error feedback, which would increase the memory cost and make the integration with differential privacy mechanisms complicated.

Notation.For a quantity \(x\), we use the notation \(x_{t}\) to refer to the global variable at round \(t\) shared by all the clients and the server, and \(x_{c,t}\) to refer to the local variable for client \(c\) at round \(t\). For a positive integer \(n\), \([n]=\{1,,n\}\). We use \(^{p p}\) as the \(p p\) identity matrix. We use \([]\) for expectation. For a vector \(\), we use \(\|\|\) or \(\|\|_{2}\) to denote its L\({}_{2}\) norm. For a matrix \(A\), we use \(\|A\|\) or \(\|A\|_{2}\) for the spectral norm of \(A\). We use \(_{}(^{{}^{}})=}{ }\|_{=^{}}\). For a random vector \(\), \(\|\|_{_{2}}\) denotes its sub-Gaussian norm . The notations \(}(t),(t),(t)\) are the same as the common \((t),(t),(t)\) but they hide the dependence on logarithmic terms. \((n)\) denotes \((^{k}(n))\) for some \(k\).

## 3 Sketching for Distributed Learning

We consider a distributed learning framework with \(C\) clients, each client \(c=1,,C\) having a local dataset \(_{c}=\{x_{i,c},y_{i,c}\}_{i=1}^{n_{c}}\) of size \(n_{c}\) and a local loss \(_{c}:^{p}\) defined as \(_{c}()=}_{i=1}^{n_{c}}(y_{i,c},_{i,c})\), where \(^{p}\) is the parameter vector, \(_{i,c}:=f(;_{i,c})\) is the prediction of the model for input \(_{i,c}\), and \(:\) is a loss function that measures the error between \(_{i,c}\) and the groundtruth \(y_{i,c}\). Our goal is to minimize the empirical loss \(()\):

\[()=_{c=1}^{C}_{c}().\] (1)

Unlike much of the existing literature [59; 33; 62], our analysis does not ignore the fact that the predictor \(f\) is a deep learning model - i.e., the loss \(_{c}\) is not just an arbitrary smooth loss. The motivation behind this focus is that many of the modern models being used in distributed learning are indeed deep learning models. Interestingly, the losses associated with the deep learning models have a certain second-order structure beyond basic smoothness which will be the key to our sharper analysis. We also demonstrate that just assuming that the loss is smooth, as is typically done in most of the existing literature [59; 33; 62], does not avail one of the sharper analysis we introduce - making the dimension dependence unavoidable.

Following standard literature [22; 54; 45; 6], we consider a fully-connected feed-forward neural network \(f\) of depth \(L\), with widths \(m\) and activations \(^{(l)}\) for each layer \(l[L]:=\{1,,L\}\), described as:

\[^{(0)}() =,\] (2) \[^{(l)}() =(}}W_{t}^{(l)}^{(l-1)}( )), l[L]\] \[f(;) =^{(L+1)}()=}}_{t}^ {}^{(L)}(),\]

where \(W_{t}^{(l)}^{m_{l} m_{l-1}}\) is the layer-wise weight matrix for layer \(l[L]\) and \(_{t}^{m_{L}}\) is the last layer vector at iteration \(t\), \(()\) is the smooth (pointwise) activation function, and \(m_{0}=()=d\). We denote the total set of parameters as

\[_{t}:=((_{t}^{(1)})^{},,(_{t}^{(L)})^{}, _{t}^{})^{}^{p}.\] (3)

For simplicity, we will assume that the width of all the layers is the same, i.e., \(m_{l}=m\) for all \(l[L]\), and thus \(p=(L-1)m^{2}+md+m\). We consider deep models with only one output, i.e., \(f(;)\), but our results can be extended to multi-dimensional outputs.

As prevalent in literature [22; 6], we make the following assumptions regarding the activation function \(\), loss function \(\), and the initialization parameter \(_{0}\) which hold true for the commonly used activation functions, loss functions, and initializations used in practice.

**Assumption 3.1** (**Activation function**).: _The activation \(\) is 1-Lipschitz and \(_{}\)-smooth,i.e. \(|^{}| 1\) and \(|^{}|_{}\)._

**Assumption 3.2** (**Initialization**).: _We initialize \(_{0}\) with \(w_{0,ij}^{(l)}(0,_{0}^{2})\) for \(l[L]\) where \(_{0}=}{2(1+})}, _{1}>0\), and \(_{0}\) is a random unit vector with \(\|_{0}\|_{2}=1\)._

**Assumption 3.3** (**Loss function**).: _The loss \(_{i}=(y_{i},_{i})\) with \(_{i}^{}=}{d_{i}},_{i}^{}= {d^{2}_{i}}{d_{i}}\) is (i) Lipschitz, i.e., \(|_{i}^{}| c_{l}\), and (ii) \(_{i}^{}\) and smooth \(_{i}^{} c_{s}\) for some \(c_{l},c_{s}>0\)._

### Sketching-Based Distributed Learning

Random sketching [75; 48] is a compression technique that uses random projections to reduce the dimensionality and helps speed up computations. These random linear mappings can be represented by sketching matrices \(R^{b p}\) where typically \(b p\). Examples of sketching matrices include Count-Sketch, Subsampled Randomized Hadamard Transforms (SRHT) , and sparse Johnson-Lindenstrauss (JL) transforms . In this work, we use sketching matrices to compress local updates before sending them to the server and refer to the operation of recovering true gradient vectors from the sketched updates as "desketching".

We outline the sketching-based distributed learning framework in Algorithm 1. Each client receives a random seed from the server to initialize the local parameters \(_{c,1}\), and generate a sketching matrix \(R\). At each local step \(k[1,,K]\), each client performs local gradient descent (GD) over their local dataset \(D_{c}\). At each communication round, the client accumulates the changes over \(K\)-local steps, sketches the local updates, and sends the sketched update to the server. The server then aggregates the sketched changes and sends the aggregated sketched updates back to the clients. To update the local parameters, each client needs to recover an unbiased estimate of the true vector from the aggregated sketched update. We call this the \(\) (desketch) operation (Line 9), for which we use the transpose of the sketching matrix \(R\). Each client then desketches the received aggregated sketched updates by applying \(\) and updates their local parameters. We refer to the sketching and desketching operations using the \(\) and \(\) operators defined as:

\[  R^{b p}\,\] (4) \[  R^{}^{p b}\.\] (5)

While we use the same sketching matrix across communication rounds \(t=1,,T\), in general, using different matrices for each round does not affect the analysis.

**Choice of sketching matrix:** We use a \((1/)\)-sub-Gaussian matrix as the choice of sketching matrix. We say \(R^{b p}\) is a \((1/)\)-sub-Gaussian matrix  if each row \(R_{i}\) is an independent mean-zero, sub-Gaussian isotropic random-vector such that \(\|R_{i}\|_{_{2}} 1/\). We assume \([R^{}R]=_{p p}\). From the above definition, we can see that for \(g_{1},g_{2}^{p}\)

\[R(g_{1}+g_{2})=Rg_{1}+Rg_{2},\] \[}{}[R^{}Rg]=g .\] (6)

### Limitations of the Existing Analyses

When analyzing the convergence rates of the sketching-based distributed learning frameworks, previous works [62; 59] assume that the loss function \(\) is \(\)-smooth, i.e,

\[(^{})()+( ),^{}-+\|^{}- \|^{2}\.\] (7)

In the model updates based on sketching followed by desketching, the term \(\|^{}-\|^{2}\) effectively yields a term of the form \(\|R^{}Rg\|_{2}^{2}\), where \(g^{p}\) stands for a suitable gradient on the full model. While \([R^{}Rg]=g\), i.e., the \(\)-sk operation is unbiased, we unfortunately have \(\|R^{}Rg\|_{2}^{2}=(\|g\|_{2}^{2})\)with high probability (see Lemma B.2 in Appendix). Thus, such an analysis picks up a dimension dependence \((p)\) which can be neutralized only if the sketching dimension is \(b=(p)\). However, such a high-dimensional projection will be ill-conceived as we will not get the benefits of the sketching projection. We note that prominent recent work has all hit this dimension dependence. For instance, Song et al.  have the dimension dependence in all their results including communication complexity, and Rothchild et al.  discuss the dimension dependence in Appendix B.3 - and get around the dependence by using Top-r components of the gradient vector, with an analysis having to rely on heavy-hitter assumptions. Interestingly, Rothchild et al.  noted that the sketching-based distributed deep learning approach seemed to work fine empirically without getting the adverse effect of dimension dependence despite what their theoretical results (based on smoothness) suggest. Our work sheds light on this discrepancy and proves (see Section 4) why the sketching-based distributed learning approach _in its simplest form_ (see Algorithm 1) does not pick up the dimension dependence.

```
0: Server learning rate \(_{}\), local learning rate \(_{}\). Inputs: local datasets \(_{c}\) of size \(n_{c}\) for clients \(c=1,,C\), number of communication rounds \(T\).
0: final model \(_{T}\).
1: Broadcast a random SEED to the clients.
2:for\(t=1,,T\)do
3: On Client Nodes:
4:for\(c=1,,C\)do
5:if\(t=1\)then
6: Receive the random SEED from the server. Initialize the local model \(_{c,1}^{p}\) and generate the sketching matrix \(R^{b p}\) (hence \(\), \(\)) using the random SEED.
7:else
8: Receive \((_{t-1})\) from the server.
9: Desketch and update the model parameters \(_{t}_{t-1}+((_{t-1}))\).
10: Assign the local model's parameters \(_{c,t}_{t}\) to be updated locally.
11:endif
12:for\(k=1,,K\)do
13:\(_{c,t}_{c,t}-_{}_{} _{c}(_{c,t})\)
14:endfor
15:\(_{c,t}_{c,t}-_{t}\)
16: Send sketched updates \((_{c,t})\) to the server.
17:endfor
18:
19:On the Server Node:
20: Receive sketched updates \((_{c,t})\) from clients \(c=1,,C\).
21: Aggregate: \((_{t})_{}_{c=1}^{C}(_{c,t})\)
22: Broadcast \((_{t})\) to the clients.
23:endfor ```

**Algorithm 1** Sketching-Based Distributed Learning.

### Restricted Strong Smoothness (RSS)

In this section, we describe the RSS property, an interesting property of deep learning losses that we use to derive dimension-independent convergence guarantees for sketch-DL. Using Taylor's expansion, the loss at \(=^{{}^{}}\) can be written as:

\[(^{{}^{}})=(_{t})+^{{}^{ }}-_{t},_{}(_{t})+ (^{{}^{}}-_{t})^{}_{}^{2}( )(^{{}^{}}-_{t}),\] (8)

where \(=^{{}^{}}+(1-)_{t}, \) and \(_{}^{2}()\) is the Hessian of the loss. Several prior works [77; 61; 60] have studied the loss Hessian by decomposing the Hessian into the Gauss-Newton matrix (\(\)) and averaged Hessian of the predictors as (\(\)):

\[_{}^{2}()+,\] (9)

\[=_{c=1}^{C}(}_{i=1}^{n_{c}} _{i,c}^{{}^{}} f_{i,c} f_{i,c}^{}),\] (10)\[=_{c=1}^{C}(}_{i=1}^{n_{c}}_{ i,c}^{{}^{}}^{2}f_{i,c})=(}_{i=1}^{n_ {c}}H_{i,c}),\] (11)

where \( f_{i,c}=_{}f(;_{i,c})\) and \(H_{i,c}=_{i,c}^{{}^{}}^{2}f_{i,c}=_{i,c}^{{}^{}} ^{2}_{}f(;_{i,c})\) respectively. Recent works [45; 6; 22] have derived sharp upper bounds on the spectral norm of the predictor Hessian: \(\|^{2}f\|=_{max}(^{2}f)=(1/)\). This leads to the following restricted smoothness property of deep learning losses:

\[(^{{}^{}})(_{t})+^{{ }^{}}-_{t},_{}(_{t})+ {2C}_{c=1}^{C}}_{i=1}^{n_{c}}_{i,c}^{{}^{ }} f_{i,c},^{{}^{}}-_{t}^{2}+}{}\|^{{}^{}}-_{t}\|^{2}.\]

In contrast to the \(\)-smoothness assumption which is common in optimization literature, this new property shows that deep learning losses exhibit strong smoothness in a _restricted_ set of directions. The effect of strong smoothness suitably manifests for \(^{{}^{}}\) such that \(|(^{{}^{}}-_{t}, f_{i,c})|>0\) for a restricted set of directions. In other directions, the strong smoothness constant is \((1/)\), i.e., a tiny value. For our analysis, we use an even sharper perspective on RSS based on the eigenvalues of predictor Hessian, \(H_{i,c}=_{i}^{{}^{}}^{2}f_{i,c}\). While the smoothness perspective based on the spectral norm of \((1/)\) is promising, it implicitly assumes all directions have this level of smoothness which would impact the analysis since there are \(p=O(Lm^{2})\) directions. Based on empirical evidence [55; 26], most directions have smoothness much smaller than \(1/\), and an analysis based on the eigenvalues of the \(H_{i,c}\) captures this sharper perspective as opposed to picking up a dependence on the ambient dimension \(p\).

## 4 RSS-based analysis for Sketching-based Distributed Learning

In this section, we analyze the convergence of the sketching-based distributed learning approach, summarized in Algorithm 1, using the RSS property of deep learning losses and provide novel dimension-independent convergence results. First, we state our key assumptions in Section 4.1 and explain why they are well-supported by recent work. Then, we provide our novel analysis that eliminates the dimension dependence for the single-local step (\(K=1\)) case in Section 4.2. Next in Section 4.3, we extend our analysis to the multiple-local step (\(K>1\)) case. We conclude in Section 4.4 by deriving the communication cost of Algorithm 1 under our novel convergence analysis.

### Assumptions

Before we introduce the assumptions, we would like to recall a key property.

**Definition 1** (PL condition).: _Consider a loss function \(:^{p}\) and the solution set \(^{*}=\{^{{}^{}}:^{{}^{}}}\ ()\}\) and we use \(^{*}\) to denote the corresponding minimum value. Then, \(\) is said to satisfy the Polyak-Lojasiewicz (PL) condition with constant \(\) if_

\[\|_{}()\|_{2}^{2}( )-^{*}\.\]

**Remark 4.1**.: PL condition  can be used to establish linear convergence of gradient descent while still being weaker than strong convexity. One can show that if a function \(f\) is \(\)-strongly convex then it also satisfies PL condition with the same constant \(\). Recent literature has shown that wide neural networks can be shown to satisfy some variant of PL condition [37; 46].

Relying on this recent evidence, we make the following assumption in our analysis:

**Assumption 4.1**.: _Loss function \(()\) satisfies the PL condition with constant \(\)._

We further assume the following upper bound on the sum of eigenvalues of the \(H_{i,c}\) as follows:

**Assumption 4.2** (Predictor Hessian Eigenspectrum).: _Let \(_{i,c,t,1},_{i,c,t,2},,_{i,c,t,p}\) be the eigenvalues of the predictor Hessian \(H_{i,c,t}=_{i,c}^{{}^{}}^{2}f(_{t};_{i,c})\) for \(t[T]\) and \(_{}=|_{i,c,t,j}|\) then there exists \(=(1)\) such that \(_{j=1}^{p}|_{i,c,t,j}|_{}\)._

**Remark 4.2**.: Several works [24; 49; 76] have shown that the spectrum of loss Hessian follows a bulk and outliers structure where the bulk can be attributed to \(\) and follows power law trend [Figure 3(b), Figure 4 in Papyan ], motivating our assumption. Further empirical evidence is presented in Appendix G (refer to Figure 2), showing that for common networks, \(\) is much smaller than \(p\).

### Single-Local Step (\(K=1\))

We now analyze the simpler setting where clients communicate their local updates after running a single local update step (\(K=1\)). In this case, there is no local drift, i.e., all clients share the same local parameter vector. As a result, we can show that local update (Line 9 in Algorithm 1) can be written as \(_{t}_{t-1}-( {sk}(_{}(_{t-1})))\). We follow a similar analysis to Song et al. , but we exploit the second order structure of the deep learning losses, which helps avoid picking up dimension dependence due to the \((())\) operator. We state the main theorem below with the full proof in Appendix D.1.

**Theorem 4.1** (Informal version of Theorem D.1).: Set K=1 in Algorithm 1. Under Assumptions 4.1, and 4.2, for suitable constants \(,<1\), learning rate \(=_{}_{}\) independent of \(p\), and \(b=(}(NT}{ }))\), with probability at least \(1-\), we have:

\[(_{T})-(^{*})((_{0 })-(^{*}))e^{-(1-) T},\] (12)

where \(^{*}\) is a minimizer of (1).

In Theorem 4.1 (which also implies linear convergence), the sketching dimension, \(b\), depends on \(p\) only polylogarithmically, which explains the competitive performance of the sketching-based distributed learning frameworks with uncompressed GD without requiring any additional costly steps such as Top-r sparsification (as done by Rothchild et al. ), which also introduces a bias that needs to be suitably corrected.

### Multiple-Local Step \((K>1)\)

Now, we provide the convergence result for Algorithm 1, for the more general case of multiple local steps (\(K>1\)) at each round. Unlike the single-local step, here, the local clients' parameters drift during \(K\) local steps and thus, we need to have additional assumptions to guarantee convergence. For the purpose of our analysis, we assume that the gradient norms are bounded, i.e., \(\|_{}()\| G\). We refer the reader to Theorem D.2 in the Appendix for a more formal statement. We now present an informal version of our theorem for the convergence of the multiple-local step case:

**Theorem 4.2** (Informal version of Theorem D.2).: Let \(\|_{}()\| G\). Under Assumptions 4.1 and 4.2, for a suitable constants \(,c_{H}\) and \(<1\), learning rate \(=_{}_{}<\), and \(b=(}(}{}))\), with probability at least \(1-\), we have:

\[(_{T})-(^{*})((_{0})- (^{*}))e^{-2(1-) KT}+( ,m,)KG^{2}}{2},\] (13)

where \(^{*}\) is a minimizer of the problem 1, \(N\) is the number of training samples, and

\[C_{2}(,m,)=(^{2}+}{})+(}{}).\] (14)

We refer the reader to Appendix D.2 for the full proof.

### Communication Efficiency

As a direct consequence of our analysis, we now provide an improved communication complexity for Algorithm 1 compared to prior works  that build their analysis solely based on the smoothness assumption without taking advantage of the RSS property of the deep learning losses. With this, we manage to eliminate the linear dependence of the communication complexity on the ambient dimension \(p\) - providing a substantial improvement over the analysis of Song et al. . Unlike Rothchild et al. , we break this dependence without requiring a Top-r sparsification step or a heavy-hitter assumption. In the theorem below, we state the required number of communication bits to achieve an \(\)-optimal solution based on our sharper convergence analysis in the previous sections.

**Theorem 4.3** (Informal version of Theorem E.1).: Under Assumptions 4.1 and 4.2, Algorithm 1 obtains an optimal solution satisfying the error

\[(_{T})-(^{*}),\] (15)

using \(}(C\{1,(,m,)G^{2}}{2 ^{2}(1-)}\}((_{0})- (^{*})}{}))\) bits of communication.

We provide the full proof in the Appendix E. We state the result by Song et al.  for comparison:

\[(\{p,}{ }}\}([\|_{0}-^{*}\|_{ 2}^{2}]}{})).\] (16)

Song et al.  assumes that the loss function \(\) is \(\)-smooth and the expectation is over the randomness of sketching. Note that our bound depends on \(\) instead of the ambient dimension \(p\), presenting a sharper bound. This shows the efficacy of our approach in improving the overall communication complexity over prior works.

## 5 Experimental Results

In this section, we provide a comparison of the sketching approach in Algorithm 1 with other common approaches such as local Top-r  and FetchSGD . We note that both Local Top-r (as outlined in Algorithm 2 in Appendix F) and FetchSGD are biased algorithms and they are typically used with error feedback mechanisms to correct the bias. As the sub-Gaussian sketching matrix in Algorithm 1, we use Count-Sketch. This means the only difference between FetchSGD without error feedback and Count-Sketch is the extra global Top-r step at the server. We conducted our experiments on NVIDIA Titan X GPUs on an internal cluster server, using 1 GPU per one run.

We train ResNet-18  on CIFAR-10 dataset  that is i.i.d. distributed to 100 clients. Each client performs 5 local gradient descent iterations (i.e., using full-batch of size \(500\)) at every round. Figure 1 shows that Count-Sketch-based distributed learning approach in Algorithm 1 performs competitively with FetchSGD. This result highlights the potential of sketching alone, without additional modifications as in FetchSGD, to maintain competitive accuracy. Additionally, the error-feedback free approach enables compatibility with Differential Privacy(DP) techniques which we leave as future work.

We note that we do not claim novelty of any of the methods discussed in this section as they have been introduced [44; 34], improved , and analyzed  extensively in prior work. We present these empirical comparisons for completeness and to provide support for why a tight analysis of sketching is important, given the mismatch between its empirical success and the dimension dependence in existing convergence analysis as highlighted in Appendix B.3 of Rothchild et al. .

## 6 Discussion and Conclusion

We provide a significantly improved convergence analysis for sketching-based distributed learning frameworks by exploiting the properties of the deep learning losses, such as restricted strong smooth

Figure 1: Communication Efficiency. Count-Sketch algorithm in Algorithm 1 against local Top-r  and FetchSGD , with and without error feedback. \(100\) clients run \(5\) local iterations with full-batch at every round.

ness. This allows us to break the dimension dependence in the convergence error, and consequently, the communication cost - a milestone prior work could not achieve due to relying only on the smoothness assumption of the losses, i.e., ignoring the more special properties of "deep models." By breaking this dimension dependency in the convergence analysis and communication cost, we hope to motivate the use of sketching for larger models. One of the many exciting future extensions is to revisit the privacy analysis of private sketching mechanisms using our findings.

Limitations.We provide a sharper analysis for sketching in distributed learning. In future work, we plan to extend our analysis to federated learning by allowing client dropout.

Broader Impact.This paper provides a tighter analysis for sketching-based distributed learning and federated learning frameworks. We expect this work to be helpful for the community as it contributes to the efforts in making machine learning models more decentralized, accessible, and trustworthy.

## 7 Acknowledgements

The authors would like to thank the anonymous reviewers of NeurIPS for their valuable feedback and suggestions. AB acknowleges support in part by the National Science Foundation (NSF) through awards IIS 21-31335, OAC 21-30835, DBI 20-21898, as well as a C3.ai research award. BI was supported in part by a Google PhD Fellowship. SK acknowledges support by NSF III 2046795, IIS 1909577, CCF 1934986, NIH 1R01MH116226-01A, NIFA award 2020-67021-32799, the Alfred P. Sloan Foundation, and Google Inc.