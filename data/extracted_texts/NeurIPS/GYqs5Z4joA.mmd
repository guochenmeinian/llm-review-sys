# SpGesture: Source-Free Domain-adaptive

sEMG-based Gesture Recognition with Jaccard

Attentive Spiking Neural Network

 Weiyu Guo\({}^{1}\) Ying Sun\({}^{1,}\)1 Yijie Xu\({}^{1}\) Ziyue Qiao\({}^{2}\) Yongkui Yang\({}^{3}\) Hui Xiong\({}^{1,4,}\)1

\({}^{1}\)Thrust of Artificial Intelligence, HKUST (Guangzhou), China

\({}^{2}\)School of Computing and Information Technology, Great Bay University, China

\({}^{3}\)Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China

\({}^{4}\)Department of Computer Science and Engineering, HKUST, Hong Kong SAR, China

wguo395@connect.hkust-gz.edu.cn; yings@hkust-gz.edu.cn;

yxu409@connect.hkust-gz.edu.cn; zyqiao@gbu.edu.cn;

yk.yang@siat.ac.cn; xionghui@ust.hk

###### Abstract

Surface electromyography (sEMG) based gesture recognition offers a natural and intuitive interaction modality for wearable devices. Despite significant advancements in sEMG-based gesture recognition models, existing methods often suffer from high computational latency and increased energy consumption. Additionally, the inherent instability of sEMG signals, combined with their sensitivity to distribution shifts in real-world settings, compromises model robustness. To tackle these challenges, we propose a novel SpGesture framework based on Spiking Neural Networks, which possesses several unique merits compared with existing methods: (1) Robustness: By utilizing membrane potential as a memory list, we pioneer the introduction of Source-Free Domain Adaptation into SNN for the first time. This enables SpGesture to mitigate the accuracy degradation caused by distribution shifts. (2) High Accuracy: With a novel Spiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent sEMG features, leading to a notable rise in system accuracy. To validate SpGesture's performance, we collected a new sEMG gesture dataset which has different forearm postures, where SpGesture achieved the highest accuracy among the baselines (\(89.26\%\)). Moreover, the actual deployment on the CPU demonstrated a latency below 100ms, well within real-time requirements. This impressive performance showcases SpGesture's potential to enhance the applicability of sEMG in real-world scenarios. The code is available at [https://github.com/guoweiyu/SpGesture/](https://github.com/guoweiyu/SpGesture/).

## 1 Introduction

Surface electromyography (sEMG) is a sensing modality that decodes motor intentions from muscle electrical signals preceding movement to enable natural and intuitive interactions. It has distinct advantages in gesture recognition for real-time applications. Specifically, sEMG provides rich and comprehensive motion information, making it an excellent resource for accurate and efficient wearable gesture recognition . Moreover, sEMG signals can emerge anywhere from 50 to 150 milliseconds prior to the actual motor activity, enabling the anticipation of movements.

In recent years, Spiking Neural Networks (SNNs)  provide an unparalleled chance for developing more practical and efficient sEMG-based gesture recognition systems. SNNs emulatethe spiking behavior of biological neurons with a unique binary information communication protocol . This binary communication is particularly amenable to the architectural specifics of sparse neuromorphic hardware . Besides, the primary computations in SNNs revolve around spike-based accumulate (AC) operations . The event-driven nature of these networks  enables calculations to be made only when there is a change or 'event' in the input, thereby circumventing the need to process zero values. Therefore, compared to conventional Artificial Neural Networks (ANNs) that typically rely on energy-demanding multiply-and-accumulate (MAC) operations  and are normally deployed on high-computing-power hardware like GPUs, SNNs demonstrate substantially lower power consumption , positioning them as a promising candidate for developing energy-efficient gesture recognition systems .

Although SNNs are computationally efficient, they struggle to match the accuracy of ANN-based models . In particular, the major problem is that the binary and sparse feature representations make it difficult to perform regular contiguous similarity computations. This limitation hinders expressive operations like attention mechanisms and advanced representation alignment algorithms, such as domain adaptation. For example, attention-based structures like the Transformer have demonstrated remarkable performance in Natural Language Processing , Computer Vision , Time-Series Processing  and Decision-Making tasks, leading to a wave of attention-centric architecture designs, underscoring the importance and versatility of attention mechanisms in deep learning. However, with the proportion of '1's typically less than \(5\%\), the dot product in cosine similarity inherent to attention mechanisms tends to yield results close to zero . Existing work often first converts spike signals into continuous values for similarity calculations, but this can increase the inference latency and energy consumption of SNNs. There is still a lack of methods for directly implementing advanced operations on spike features in SNN for realizing effective sEMG-based gesture recognition systems.

To tackle these challenges, we propose an SNN-based solution for a low-power yet accurate sEMG-based gesture recognition framework. Specifically, we first propose a novel Jaccard Attention Spiking Neural Network (JASNN) to enhance the representativeness of the network for sEMG features. In particular, different from existing studies that exploit attention to regulate membrane potentials and subsequently influence spiking activity , we propose a Spiking Jaccard Attention that calculates attention directly on spike sequences, which enables more straightforward computationally effective attention calculation under SNN schema. Indeed, such a computation process predominantly involves 'comparison' operations, aligning well with the design principles of neuromorphic chips and preserving the low-power properties of SNNs. Moreover, to address the distribution shift problem, we propose a novel Spiking Source-Free Domain Adaptation based on Membrane Potential Memory. Our method leverages the changing membrane potential curve as a memory list and uses it to generate pseudo-labels based on the \(k\)-nearest neighbors that are most similar to the current sample. In particular, we incorporate a random exploration mechanism to avoid overfitting during pseudo-label generation and bolster the model's generalizability. With our method, we achieve knowledge transfer without sharing the data, which enhances gesture recognition accuracy in an unlabeled environment under privacy reservation.

To better reflect real-world conditions, we collect a new sEMG-based gesture dataset that includes different forearm postures, acknowledging that variations in forearm posture can significantly influence the distribution of sEMG data. Our experimental results demonstrate that our algorithm not only significantly outperforms other SNN-based algorithms in gesture recognition accuracy but also matches the performance of state-of-the-art methods in the Deep Neural Networks (DNNs) category. Furthermore, the Spiking Jaccard attention method we proposed substantially enhances the accuracy of SNN algorithms. Regarding inference speed, Spiking Jaccard attention is **36.37x** faster on a CPU than traditional attention mechanisms. Our innovatively designed SSFDA method, which does not require source data or labels, improved the gesture recognition accuracy by **4.5%**. These results collectively underline the effectiveness and efficiency of our proposed approach in addressing the challenges in sEMG-based gesture recognition. Our contribution can be summarized as follows:

* We propose a Jaccard similarity-based attention mechanism specifically designed for SNNs. This innovative approach preserves the original computational characteristics of SNNs, boosts inference efficiency, and counteracts the accuracy degradation caused by sparse spiking sequences.
* To the best of our knowledge, we are among the first to propose an SNN-oriented SFDA algorithm. This enables users to capture gesture actions under one specific forearm posture and empowers the model to unsupervised learning the features under other forearm postures, thereby bolstering its robustness during actual use.
* We collect a new sEMG-based gesture dataset that features a variety of forearm postures. This dataset can provide valuable resources for researchers aiming to develop robust gesture recognition algorithms for different forearm postures.
* The experimental results demonstrate performance improvements over state-of-the-art sEMG gesture recognition models, with particular benefits under varying forearm orientations. Our model also provides higher efficiency than existing attention schemes.

## 2 Related Works

**Spiking Neural Networks (SNNs)**, the third generation of neural networks, mimic biological neurons through binary spiking signals and handle temporal information effectively . SNNs are energy-efficient, activating only a small portion of neurons during computation, unlike dense ANNs that rely on energy-intensive operations . Neuromorphic chips like Tianjic , TrueNorth , and Loihi  exemplify this efficiency. Despite their energy advantages, SNNs have lower accuracy than DNNs due to sparse feature representation and simplistic structures. Attention mechanisms, widely used in DNNs , are under-explored in SNNs, posing challenges like spike degradation and gradient vanishing. Addressing these issues is crucial for improving SNN performance.

**Domain Adaptation (DA)** aims to leverage labeled source domain data to improve performance on unlabeled target domains, addressing domain shifts . Traditional DA requires access to both source and target data, which is impractical in scenarios involving privacy or resource constraints .

**Source-Free Domain Adaptation (SFDA)** addresses this by adapting models without source data, crucial for privacy-sensitive applications like sEMG gesture recognition . SFDA methods are categorized into data-centric and model-centric approaches . Data-centric methods extend UDA techniques by reconstructing virtual domains or translating target data into source-style data . Model-centric methods, like pseudo-labeling , entropy minimization , and contrastive learning , fine-tune models using target data. However, applying SFDA to SNNs is challenging due to their lower stability and sparse outputs.

## 3 Preliminaries

**Data Collection:** In human-computer interaction studies involving sEMG, diverse and representative data sets are crucial. Traditional research often collects sEMG data from a single forearm posture [4; 31], but variations in forearm posture significantly influence sEMG data distribution. Our methodology incorporates gestures performed in different forearm postures to better reflect real-world conditions. Participants were instructed to replicate gestures and forearm postures shown on a screen. Our dataset includes ten gestures across three forearm postures: P1 (forearm horizontal on a surface), P2 (forearm elevated diagonally with elbow anchored), and P3 (forearm horizontal). Each gesture was held for five seconds with a five-second relaxation period, repeated six times per posture. This approach aims to provide robust sEMG data reflecting practical variability. For further details on dataset collection, including information about the acquisition devices and specific measures taken, please refer to appendix A.1.

**Data Processing:** We used Root Mean Square (RMS) for initial feature extraction to enhance gesture recognition stability. RMS efficiently summarizes signal magnitude, indicating signal power. A 100ms time window with a 0.5ms step size captured transient sEMG characteristics, extracting features while maintaining high-resolution signal variations. RMS is further explained in appendix A.2.

## 4 Method

In the subsequent sections of this paper, we will present a sEMG-based gesture recognition solution with SNNs capable of handling distribution shifts. This solution can be divided into Jaccard Attention Spiking Neural Network (JASNN) and Spiking Source-Free Domain Adaptation (SSFDA). Firstly, we will introduce the unique SNN backbone JASNN deployed in our study. Following this, we will delve into our innovative design, the novel implementation of SSFDA within an SNNs framework - a first in the field. For more details about SNN, please refer to appendix A.3.

### Jaccard Attentive Spiking Neural Network

#### 4.1.1 Network Overview

Our proposed Jaccard Attentive Spiking Neural Network (JASNN) comprises four primary components: a Convolutional Leaky Integrate-and-Fire (ConvLIF) based spike encoder and feature extractor, a Spiking Jaccard attention mechanism, LIF-based Classifier, and a membrane potential recording module. We detailed the first and third components in appendix A.4.A.6.

The ConvLIF-based spike encoding layer dynamically encodes sEMG signals into spike trains, capturing temporal dynamics effectively. The Multi-Channel ConvLIF extractor transforms these spikes into a higher-dimensional space for better feature representation. The Spiking Jaccard attention mechanism focuses on task-relevant features, enhancing meaningful information. The modified LIF layer translates spiking activity into classification results based on the highest membrane potential. Finally, the membrane potential recording module converts output spikes into membrane potentials for source-free domain adaptation.

#### 4.1.2 Spiking Jaccard Attention

Attention mechanisms have enhanced DNNs in time-series analysis by focusing on important temporal aspects for better predictions. However, applying attention mechanisms to Spiking Neural Networks (SNNs) presents unique challenges. Firstly, SNNs' sparse neuron activation makes the dot product operation in attention mechanisms produce sparse spike trains, hindering learning due to reduced signal strength. Secondly, using the softmax function for attention scores increases computational complexity and energy consumption, which is unsuitable for SNNs' efficient processing requirements.

To address these concerns, we propose a novel Spiking Jaccard Attention (SJA) mechanism specifically designed for SNNs. As shown in Figure 2, unlike the method by Yao _et al._, SJA can directly calculate the similarity on spike trains and retains the attention's query mechanism.

Given the binary nature of SNN layers outputs, the dot product approach in attention will make the feature too sparse. We introduce the SJA mechanism based on the Jaccard similarity, which is better suited for binary data. The Jaccard similarity between two sets A and B can be defined as:

\[(A,B)=. \]

Generally speaking, designing a spiking chip for SNNs mainly involves a large number of addition circuits and comparison circuits. Therefore, in the practical implementation of our proposed SJA, we retain the computational characteristics of the spiking chip to compute the Jaccard similarity more efficiently. This is achieved by calculating the intersection and union of the vectors using element-wise minimum and maximum operations, respectively. For two vectors \(\) and \(\), it can be described as:

\[(,)=( _{i},_{i})}{_{i}(_{i}, _{i})+}. \]

Figure 1: The pipeline of Jaccard Attention Spike Neural Network: Raw sEMG Data is first encoded into Spike Signals using ConvLIF. These signals pass through ConvLIF layers with \(N\) and \(2N\) channels. The processed data then goes through the Spiking Jaccard Attention mechanism.

This approach enables efficient computation of the Jaccard similarity by taking advantage of the sparsity of the data in SNNs. By computing the sums over the element-wise minimum and maximum operations instead of using matrix dot multiplication operations, our algorithm becomes more easily deployable on Neuromorphic chips, thereby enhancing the computational efficiency of the attention mechanism within SNNs. We add a tiny constant to the denominator to avoid a division by zero when there are no spikes in the spike train.

So, we can modify the traditional attention formula by incorporating Jaccard similarity into the attention mechanism. The resulting SJA mechanism can be expressed as:

\[(,)=(q_{i},k _{i})}{_{i}(q_{i},k_{i})+}, \]

where \(\), \(\), and \(\) represent the query, key, and value matrices, respectively, and \(q_{i}\) and \(k_{i}\) are the corresponding elements in the query and key matrices.

First, we consider the channel-wise uniform weighting method. This approach implies that the same weighting coefficient is applied to all elements along the channel dimension of \(\). In this case, the attention weight is computed as a scalar, calculated by aggregating the elements within each channel: where \(i\) is the index of the elements within the channel. The resulting scalar is then used as a weighting factor applied to each channel of \(\):

\[_{}[:,c,:]=(,) [:,c,:], \]

where \(c\) represents the channel index. In this way, the values across all channels are scaled by the same weighting factor, thereby maintaining consistency across different channels.

Second, we consider the element-wise weighting method. In this case, the \((,)\) result is computed independently for each element position \(q,k\). This means that the attention weight for each element is obtained by calculating the value for the corresponding elements in \(\) and \(\) at that position. These weights are then applied element-wise to \(\):

\[_{}[:,c,n]=(,)[:,c,n] [:,c,n], \]

where \(n\) represents the index along the sequence length. Under this element-wise weighting strategy, different positions within \(\) are scaled independently based on their respective attention weights, which enables the model to capture finer-grained features.

The results presented in this paper are derived using the channel-wise weighting approach, as it is more suitable for the characteristics of sEMG data, and we did not validate the element-wise weighting approach due to these characteristics.

Figure 2: Comparison of MA-SNN and Spiking Jaccard Attention Modules. MA-SNN  uses fully connected layers with pooling but lacks a querying mechanism, leading to continuous intermediate values and lower efficiency. Our Spiking Jaccard Attention uses spike values for intermediate representations, enhancing efficiency and accuracy.

These two weighting strategies each have their respective applications: channel-wise uniform weighting is more appropriate for preserving feature consistency, while element-wise weighting is better suited for capturing localized differences. Depending on the computational complexity and the task requirements, an appropriate weighting strategy can be selected to achieve a balance between efficiency and performance.

The SJA mechanism leverages the sparsity of SNN outputs to significantly reduce computational complexity. Unlike traditional attention mechanisms with a complexity of \(O(n^{2} d)\), SJA focuses only on non-zero elements, resulting in a complexity of \(O(b)\), where \(b\) is the number of non-zero elements. This approach enhances computational efficiency and reduces energy consumption, as addition operations dominate SJA compared to the multiplication-heavy traditional attention, making SJA particularly advantageous for SNNs. Further complexity analysis can be found in appendix A.5.

### Spiking Source-Free Domain Adaptation based on Membrane Potential Memory

The formal definition of the problem is as follows: given a labeled source domain \(_{s}=\{(x_{i}^{s},y_{i}^{s})\}_{i=1}^{N_{s}}\), an unlabeled target domain \(_{t}=\{x_{j}^{t}\}_{j=1}^{N_{t}}\) and a model \(f_{s}\) trained on \(_{s}\), the goal is to adapt or fine-tune the model \(f_{s}\) such that its performance on the target domain \(_{t}\) is optimized. The primary challenge stems from the different data distributions of the source and target domains, i.e., \(P_{s}(x,y) P_{t}(x,y)\), where \(P_{s}\) and \(P_{t}\) denote the data distributions of the source and target domains, respectively. In SFDA, the added complexity is that the source data \(_{s}\) is not available when adapting or fine-tuning the model, while only having the source model \(f_{s}\). Thus, the adaptation must rely on the properties and capabilities of the source model and unlabeled target data.

Most previous methods consider similarity based on instance discrimination among all features in their loss functions, which can lead to high computational costs. This requirement can generate a significant computational overhead. In line with the approach taken by , we generate pseudo-labels using the \(k\)-most similar samples to the target sample with a consistency regularization. Furthermore, we introduce an exploration mechanism to mitigate overfitting. This strategy effectively maintains computational efficiency while enhancing the robustness and generalization of our SFDA approach.

Another challenge is that the intermediate layer features in SNNs are represented by Spike Trains, and existing methods for finding neighbors cannot directly compute them. To identify the semantically closest neighbors to a target domain sample, we utilize the membrane potential from the Memory Layer to construct a Membrane Potential Memory List. Note that we only use target source data to generate the Membrane Potential Memory List. The membrane potential encapsulates both spatial and temporal features, rendering it a more informative and efficient tool for our purpose. Membrane Potential Memory \(M_{n}=\{V_{n,t}^{m}\}_{t=1}^{T}\) can be computed by:

\[V_{n,t}^{m}=S^{t}+ N(0,1), \]

Figure 3: Computation flow of Spiking Source-Free Domain Adaptation. The process starts with selecting the \(k\)-nearest samples from the membrane potential memory using the Pearson correlation coefficient. Probabilistic Label Generation then produces pseudo-labels based on these \(k\) samples. Gradients are computed with Smooth NLL and KL divergence loss. The membrane potential memory list is updated at each epoch’s end.

where \(N(0,1)\) represents Gaussian noise with mean 0 and standard deviation 1, and \(\) is a scaling factor. Integrating Gaussian noise with scaling offers two key benefits: regularization helps prevent overfitting, allowing the model to generalize better to unseen data, and noise introduction reduces the dominance of zeros in spike data, leading to a more balanced data representation. Figure 3 shows the SSFDA computation flow.

The core of the loss function is the alignment of predictions between the current target feature and its \(k\)-nearest neighbors in the Membrane Potential List, identified based on Pearson similarity. To achieve this, we introduce the following loss function Smooth Negative Log Likelihood (SNLL) Loss that combines two crucial components:

\[=-(1-)_{i=1}^{n}_{k=1}^{K}(p(x_{i })(_{k}))+_{c=1}^{C} (_{c} q_{c}), \]

where

\[_{k} =((\{ \}_{1}^{k})),&(1-p),\\ ((\{\}_{1}^{k}) ),&p, \] \[\{\}_{1}^{k} =\{_{j}K((f (x_{i}),_{j})),_{j}\},\] (9) \[ =_{i=1}^{n}p_{c}(x_{i}),q_{c}= {C},c=1,2,,C. \]

The loss function, \(\), is composed of two main terms: **Consistency Term:** The first component, \(-(1-)_{i=1}^{n}_{k=1}^{K}(p(x_{i}) {argmax}(_{k}))\), is designed to advocate consistent predictions between a target feature and its \(k\)-nearest neighbors. It strives to minimize the negative logarithm of the inner product of the prediction score for the target sample, denoted by \(p(x_{i})\), and the aggregated prediction scores represented by argmax \((_{k})\) of its \(k\)-nearest neighbors. \(_{k}\) represents either the mode of the argmax values from the subset \(\{\}_{1}^{k}\) with probability \(p\), or a random selection from the same subset with probability \((1-p)\). By inducing similarity in predictions among closely related features, our model can discover latent structures and associations within the data; **Regularization Term:** The subsequent component, \(_{c=1}^{C}(_{c} q_{c})\), uses the Kullback-Leibler divergence to measure the discrepancy between the model's average predicted class distribution, \(_{c}\), and the ideal uniform distribution across classes, \(q_{c}\). Specifically, \(_{c}\) denotes the model's average prediction probability for class \(c\) over all data samples. By comparing \(_{c}\) with \(q_{c}\), the divergence quantifies the deviation of the model's predictions from a perfectly balanced class distribution. The aim is to reduce the model's inclination to favor certain classes overly, ensuring a more balanced prediction landscape. In this configuration, The scalar \(\) in the loss function acts as a balancing factor between predictive consistency and regularization.

### Training Method

Deep Spiking Neural Networks (SNNs) are typically trained using ANN-to-SNN conversion or direct training. While ANN-to-SNN conversion faces latency challenges, direct training is more time-step efficient and suitable for temporal tasks. We use rate coding for its support of complex SNNs. In this paper, we use the SuperSpike  surrogate gradient to calculate gradients, with detailed explanations provided in appendix A.7.

## 5 Experiment

### Gesture Recognition based on sEMG

We compared our model's performance with existing sEMG-based gesture estimation models, primarily categorized into DNN and SNN architectures. A comparison summary is in Table 1.

In terms of Top-1 Accuracy, our JASNN model, which integrates the SNN framework with the SJA mechanism, outperforms other DNN models, including CNN, TCN , Transformer , GRU , Informer , and a hybrid TCN with an Attention mechanism. This superior performance is due to: 1) The SNN structure's alignment with the biological basis of sEMG generation, providing a natural modeling of the processes. 2) The SJA mechanism's enhancement of sparse spike train features focuses on the key characteristics of sEMG signals. Compared to other SNN models like LSNN , SIB+SNN , and SCNN, our model achieves higher accuracy. Models like SIB+SNN and SCNN perform lower, likely due to the absence of a feature enhancement design like SJA, which is crucial for capturing the temporal dynamics of sEMG signals. Incorporating SJA into Xu _et al._'s LSNN network  significantly improved performance, demonstrating SJA's scalability in recurrent SNNs.

### Ablation Study

To validate the effectiveness of each module we have proposed, we present the results of an ablation study. Here, we discuss the impact of the backbone's attention mechanisms and loss functions on the experimental results and the influence of different pseudo-label generation methods within SSFDA. All experimental results in this section are based on the mean values across all fifteen subjects in the dataset. The same learning rate, batch size, and optimizer were used during training, ensuring each network converges (with training set accuracy showing less than \(0.2\%\) improvement over five consecutive epochs). This thorough examination allows us to isolate the individual contributions of the different components and clarify their specific roles in the performance of our proposed system.

#### 5.2.1 Attention Mechanisms

In our ablation study on attention mechanisms, we compared Raw Attention , MA-SNN , and our proposed Spiking Jaccard Attention (SJA) on SCNN, keeping all other parameters consistent. As shown in Table 2, using Raw Attention directly on spikes resulted in an accuracy of only 11.31% due to the high sparsity of spike sequences. This sparsity often leads to information loss when multiplying matrices with sparse values. MA-SNN converts spike sequences into continuous values and uses a fully connected layer for attention, which increased SCNN's accuracy from 84.12% to 85.67%. However, this approach reduces the usability of SNNs on spiking chips. In contrast, our SJA computes attention weights directly on the spike sequence, preserving compatibility with spiking hardware and further boosting accuracy to 87.44%. This highlights SJA's superior ability to handle spike sequence sparsity while maintaining hardware compatibility.

#### 5.2.2 Loss Functions

The study compared Negative Log Likelihood (NLL) loss and our improved Smooth NLL with Kullback-Leibler divergence loss (SNLL+KLL) for classification tasks. Using SNLL+KLL in JASNN increased accuracy from 87.44% to 89.72% (see Table 2). This enhancement is due to: **Kullback-Leibler (KL) divergence:** KL divergence quantifies the difference between two probability distributions, encouraging predicted probabilities to closely match actual class distributions. This reduces model biases towards certain categories. **Smooth NLL (SNLL):** SNLL ensures consistent predictions between a feature and its k-nearest neighbors in the embedding space, enhancing model sensitivity to detailed class clusters and underlying patterns. In summary, adding KL divergence and SNLL improves model strength and fairness, enhancing flexibility across various datasets and tasks.

#### 5.2.3 Pseudo-Label Generation Methods

We evaluated three pseudo-label (PL) generation methods: Duan _et al._ (PL), Huang _et al._ (NPL), and our proposed Probabilistic Label Generation (PLG) method. Both PL and NPL improved

   Methods & Work & Model & Top-1 Acc.(\%) & Std. Dev. (\%) \\   & Asif _et al._2020  & CNN & 75.46 & 0.52 \\  & Tsinganos _et al._2020  & TCN & 79.69 & 0.83 \\  & Rahimian _et al._2021  & Transformer & 84.23 & 0.37 \\  & Chen _et al._2021  & GRU & 82.19 & 0.28 \\  & Zhou _et al._2021  & Informer & **88.32** & 0.36 \\  & Rahimian _et al._2022  & TCN+Attention & 87.10 & 0.57 \\  & Zhang _et al._2023  & Transformer & **86.24** & 0.31 \\   & Bellec _et al._2018  & LSNN & **86.24** & 0.22 \\  & Zhang _et al._2022  & SIB+SNN & 77.84 & 0.62 \\  & Xu _et al._2023  & SCNN & 84.30 & 0.23 \\   & SOTA backbone  & LSN+SJA(Ours) & 88.10 & 0.25 \\  & **This Work** & JASNN & **89.26** & 0.31 \\   

Table 1: Comparison with previous works on sEMG-based gesture estimation.

accuracy in an unsupervised setting on JASNN by 1.87% and 2.33%, respectively. Our PLG method achieved a significant boost, enhancing accuracy by 4.1%. Additionally, PLG increased accuracy on MA-SNN by 3.81%, demonstrating its scalability across different architectures. The effectiveness of PLG comes from selecting the mode of neighboring labels as the pseudo-label and introducing a probabilistic mechanism to explore other labels, preventing overly compact feature distribution and enhancing generalization. This makes PLG a powerful tool for unsupervised adaptation in sEMG-based gesture recognition.

### Variant Distributions of Hand Gestures with Three Different Forearm Postures

In this study, we evaluated datasets from three forearm postures (P1, P2, P3) to train models without SSFDA, revealing challenges with sEMG data. The model trained on P1 achieved high accuracy on P1's test set but dropped to 30% accuracy on P2 and P3 due to variations in motor neuron firing patterns. Figure 9 illustrates this performance disparity, highlighting the out-of-distribution (OOD) issue. These findings underscore the need to address OOD phenomena in sEMG data to enhance the reliability and user experience of sEMG-based systems.

### Result of Spiking Source-Free Domain Adaptation

In our investigation, we used the same dataset to experiment with three different methodologies, namely Pseudo-Label  method, Neighborhood-guided Pseudo-Labels  method, and our proposed Probabilistic Label Generation method. In this experiment, the Pseudo-Label method determines the pseudo-label by taking the mode of the \(k\) samples. Conversely, the neighborhood-guided Pseudo-labeles method involves choosing the nearest \(k\) samples from the memory list and then randomly selecting one from these \(k\) samples as the pseudo-label. Details of our proposed method have been elaborated on in the previous sections of the paper.

Figure 4(a) and 4(b) represent the performance variations when deploying the model trained on Posture 1 to Posture 2 and 3, respectively, both with and without the use of our SSFDA. This is portrayed using a violin plot. It can be observed that the use of SSFDA indeed shifts the distribution of accuracy across different subjects upward as a whole. Particularly, our method exhibits superior performance after applying SSFDA compared to the other two methods. Furthermore, the standard deviation of performance across various subjects is minimal for our method, demonstrating the robustness of our methodology when employing SSFDA. We detailed the differences by individuals in Figure 9(a) 9(b) in the appendix.

   & &  &  &  &  \\   & SCNN & RA & MA-SNN & SIA (Ours) & NLL & SNLL+KLL (Ours) & PL & NPL & PLG (Ours) & ACC & Improved ACC \\   & ✓ & & & & ✓ & & & & 84.12\% & - \\  & ✓ & ✓ & & & ✓ & & & & 11.31\% & - \\  & ✓ & & ✓ & & ✓ & & & & 85.67\% & - \\  & ✓ & & & ✓ & ✓ & & & & 87.44\% & - \\  & ✓ & & & ✓ & ✓ & & & **89.72\%** & - \\   & ✓ & & & ✓ & & ✓ & ✓ & & - & 1.87\% \\  & ✓ & & ✓ & & ✓ & ✓ & ✓ & - & 2.33\% \\  & ✓ & & ✓ & & ✓ & & ✓ & - & 3.81\% \\  & ✓ & & & ✓ & ✓ & & ✓ & - & **4.10\%** \\  

Table 2: Ablation study results

Figure 4: Comparison of performance before and after applying SSFDA for various methodologies: Figures 3(a) and 3(b) are Violin Plots demonstrating this disparity.

### Efficiency Analysis of Spiking Jaccard Attention

Inference latency influences user experience, with delays leading to missed or inappropriate actions. Attention mechanisms are pivotal for efficient interactions. We compared SJA's computational superiority over Raw  and Efficient  Attention, conducting 100 inference tests using pseudo-data on twelve channels, a common practice in sEMG data. Results averaged and shown in Figure 5, highlight SJA's clear advantages. Regardless of the computing platform or data type, SJA demonstrated superior efficiency in inference speed and RAM consumption, making it ideal for real-time and mobile devices. Additionally, SJA showed better scalability, with only a gentle increase in inference time and RAM usage as sEMG data length increased, compared to Raw Attention's exponential growth.

### Real World Deployment

SpGesture has been deployed in a real-world application using an in-house developed sEMG acquisition system, as illustrated in appendix A.10.

## 6 Limitation and Future Work

Domain Adaptation on Various Network Structures:We verified the ability of SJA and SSFDA to enhance the accuracy of sEMG-based gesture recognition, along with their adaptability to distribution shift based on the Spiking Convolutional Neural Network architecture. Moving forward, we intend to assess their robustness across a wider variety of SNNs and different tasks.

Performance Analysis on Neuromorphic Chips:Our current evaluations of inference speed and memory utilization are conducted on CPU and GPU platforms, where our system demonstrates clear advantages over existing algorithms. We believe that these advantages will be further amplified on neuromorphic chips. We are currently developing neuromorphic chips and will conduct practical tests on these chips to measure the system's energy consumption and inference efficiency.

## 7 Conclusion

We presented SpGesture, an innovative framework for sEMG-based gesture recognition built on SNN, and innovatively introduced Spiking Source-Free Domain Adaptation with Spiking Jaccard Attention, which directly enhances spike features. These novel contributions improve the system's robustness and accuracy in real-world scenarios. Our experimental results include the highest accuracy among baselines and system latency below 100ms on a CPU, demonstrating its real-world applicability. Our proposed SJA processes spike sequences at \(36.37\) times the speed of conventional attention and can be extended to other SNNs, such as LSNN. SpGesture not only offers a practical solution to current challenges in gesture recognition but also opens new possibilities for Human-computer Interaction.