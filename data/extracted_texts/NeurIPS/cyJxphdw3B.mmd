# Can neural operators always be continuously discretized?

Takashi Furuya\({}^{1,*}\)  Michael Puthawala\({}^{2,*}\)  Maarten V. de Hoop\({}^{3}\)  Matti Lassas\({}^{4}\)

\({}^{1}\)Shimane University, takashi.furuya0101@gmail.com

\({}^{2}\)South Dakota State University, Michael.Puthawala@sdstate.edu

\({}^{3}\)Rice University, mdehoop@rice.edu

\({}^{4}\)University of Helsinki, matti.lassas@helsinki.fi

* These authors contributed equally to this work

###### Abstract

We consider the problem of discretization of neural operators between Hilbert spaces in a general framework including skip connections. We focus on bijective neural operators through the lens of diffeomorphisms in infinite dimensions. Framed using category theory, we give a no-go theorem that shows that diffeomorphisms between Hilbert spaces or Hilbert manifolds may not admit any continuous approximations by diffeomorphisms on finite-dimensional spaces, even if the approximations are nonlinear. The natural way out is the introduction of strongly monotone diffeomorphisms and layerwise strongly monotone neural operators which have continuous approximations by strongly monotone diffeomorphisms on finite-dimensional spaces. For these, one can guarantee discretization invariance, while ensuring that finite-dimensional approximations converge not only as sequences of functions, but that their representations converge in a suitable sense as well. Finally, we show that bilipschitz neural operators may always be written in the form of an alternating composition of strongly monotone neural operators, plus a simple isometry. Thus we realize a rigorous platform for discretization of a generalization of a neural operator. We also show that neural operators of this type may be approximated through the composition of finite-rank residual neural operators, where each block is strongly monotone, and may be inverted locally via iteration. We conclude by providing a quantitative approximation result for the discretization of general bilipschitz neural operators.

## 1 Introduction

Neural operators, first introduced in , have become more and more prominent in deep learning on function spaces. As opposed to traditional neural networks that learn maps between finite-dimensional Euclidean spaces, neural operators learn maps between infinite-dimensional function spaces yet may be trained and evaluated on finite-dimensional data through a rigorous notion of discretization. Neural operators are widely used in the field of scientific machine learning , among others, principally because of their discretization invariance. In this work, we consider the fundamental limits of this discretization. Throughout, we emphasize the importance of continuity under discretization.

A key ingredient in our analysis is the identification of properties of diffeomorphisms that may be induced by (bijective) neural operators, which are diffeomorphisms themselves. Diffeomorphisms exist in many contexts, for example, in generative models. These involve mapping one probability distribution or measure, \(\), over some measurable space to another, \(X\), via a push forward, that is, \(F_{\#}(U)=(F^{-1}(U))\) for \(U X\). If \(\) admits a density \(d\) then, in finite dimensions, we mayuse the change of variables formula to write \(d(x)=d(F^{-1}(x))|JF(x)|^{-1}\), where \(JF\) is the Jacobian of \(F\). Clearly, \(F\) must be a bijection with full-rank Jacobian. In other words, \(F\) must be a diffeomorphism onto its range. This established diffeomorphisms as natural objects of interest in finite-dimensional machine learning, and helps account for their wide use . In this work, we consider the extension of these efforts from finite to infinite dimensions implemented via neural operators. Although there is no analogue of the change of variables formula in infinite dimensions, we argue that it is, nonetheless, natural to consider the role of diffeomorphisms, and how they may be approximated via diffeomorphisms on finite-dimensional spaces.

The question of when operations between Hilbert spaces may be discretized continuously may be understood through an analogy to computer vision. Consider the task of learning a map from one image space to another, for example, a style transfer problem , where the mapping learned does not depend much on the resolution of the images provided. It is natural to think of the map as being defined between (infinite-resolution) _continuum_ images, and then its application to images of a specific resolution. In this analogy, \(X\) is a function space (over images \(m:\ ^{2}\)) that is approximated with a finite-dimensional space \(^{d}\) and the transformation \(F:\ X X\) is approximated by a map \(f:\ ^{d}^{d}\), where each \(f\) acts on images of a particular resolution. An explicit transformation formula can be obtained when \(f\) is a diffeomorphism and has a smooth inverse.

We introduce a framework based on a generalized notion of neural operator layers including a skip connection and their restrictions to balls rather than compact sets. With bijective neural operators in mind, we give a perspective based on diffeomorphisms in infinite dimensions between Hilbert manifolds. We give a no-go theorem, framed with category theory, that shows that diffeomorphisms between Hilbert spaces may not admit any continuous approximations by diffeomorphisms on finite-dimensional spaces, _even_ if the underlying discretization is nonlinear. In this framing the discretization operation is modeled as a functor from the category of Hilbert spaces and \(C^{1}\)-diffeomorphisms on them to their finite-dimensional approximations. A natural way to mitigate the no-go theorem is described by the introduction of strongly monotone diffeomorphisms and layerwise strongly monotone neural operators. We prove that all strongly monotone neural operator layers admit continuous approximations by strongly monotone diffeomorphisms on finite-dimensional spaces. We then provide various conditions under which a neural operator layer is strongly monotone. Notably, a bilipschitz (and, hence, bijective) neural operator layer can always be represented by a composition of strongly monotone neural operator layers. Hence, such an operator may be continuously discretized. More constructively, any bilipschitz neural operator layer can be approximated by residual finite-rank neural operators, each of which are strongly monotone, plus a simple isometry. Moreover, these finite-rank residual neural operators are (locally) bijective and invertible, and their inverses are limits of compositions of finite-rank neural operators. Our framework may be used "out of the box" to prove quantitative approximation results for discretization of neural operators.

### Related work

Neural operators were first introduced in . Alternative designs for mappings between function spaces are the DeepONet , and the PCA-Net . In spite of the multitudinous applications of neural operators, the theory of the natural class of injective or bijective neural operators is comparatively underdeveloped; see, for example .

Our work is concerned with the of discretization of neural operators through the lens of diffeomorphisms. For recent important work on analyzing the effect of discretization error of Fourier Neural Operators (FNOs) arising from aliasing, see . Our work has connections to infinite-dimensional inference, see e.g. , and approximation theory, see e.g.  while bridging the gap with the theory of neural operators.

We give a no-go theorem that uses a category theory framing. This contributes to the use of category theory as an emerging tool in the analysis and understanding of neural networks at large. In this sense, we are in league with the recent work generalizing ideas from geometric machine learning using category theory .

Discretization obstructions have been encountered in other contexts. Numerical methods that approximate continuous models are known to sometimes fail in surprising ways. A basic example of this is the "locking" phenomenon in the study of the Finite Elements Method (FEM). For example,linear elements used to model bending of a curved surface or beam lock in such a way that the model exhibits a non-physical stiff response to deformations . Understanding this has been instrumental in developing improved numerical methods, such a high order FEM . Furthermore, in discretized statistical inverse problems [27; 36; 51], the introduction of Besov priors [48; 11] has been found to be essential.

Finally, our work extends prior work (not based on deep learning) in discretization of physical or partial differential equations based forward models in inverse problems. The analogous notion of discretization invariance of solution algorithms of inverse problems was studied in [37; 48; 51] and the lack of it (in imaging methods using Bayesian inversion with \(L^{1}\) priors) in [36; 48]. By considering the neural operator as the physical model, our results state that discretization can be done locally in an appropriate way, together with constructing an inverse.

### Our contributions

The key results in this paper comprise the following:

1. We prove a general no-go theorem showing that, under general circumstances, diffeomorphisms between Hilbert spaces may not admit continuous approximation by finite-dimensional diffeomorphisms. In particular, neural operators corresponding to diffeomorphic maps, in general, cannot be approximated by finite-dimensional diffeomorphisms and their associated neural representations.
2. We show that strongly monotone neural operator layers admit continuous approximations by strongly monotone diffeomorphisms on finite-dimensional spaces.
3. We show that bilipschitz neural operators can be represented in any bounded set as a composition of strongly monotone, diffeomorphic neural operator layers, plus a simple isometry. These can be approximated by finite-rank diffeomorphic neural operators, where each layer is strongly monotone. For these operators we give a quantitative approximation result.

## 2 Definitions and notation

In this section, we give the definitions and notation used throughout the paper. First, we summarize the relevant basic concepts from functional analysis. Then, we introduce generalized neural operators.

### Elements of functional analysis

In this work, all Hilbert spaces, \(X\), are endowed with their norm topology. We denote by \(B_{X}(r)=B_{X}(0,r)\) the ball in the space \(X\) having the center at zero and radius \(r>0\). We denote by \(S(X)\) the set of all finite-dimensional linear subspaces \(V X\). The set \(S_{0}(V) S(X)\) is a partially ordered lattice. That is, if \(V_{1},V_{2} S_{0}(X)\) then there is a \(V_{3} S_{0}(X)\) so that \(V_{1} V_{3}\) and \(V_{2} V_{3}\). 1 With \(Y\) standing for another Hilbert space, we denote by \(C^{n}(X;Y)\) the set of operators, \(F\, X Y\), having \(n\) continuous (Frechet) derivatives, and \(C^{n}(X)=C^{n}(X;X)\).

Next, we define what it means that a nonlinear operator or function \(F\, X X\) on an infinite-dimensional Hilbert space, \(X\), is approximated by operators or functions on finite-dimensional subspaces \(V X\). The key is that as \(V\) tends to \(X\), the complexity of \(F_{V}\) increases and one may hope that the approximation becomes better. We formalize this in the following definition.

**Definition 1** (\(_{V}\) approximators and weak approximators).: _(i) Let \(r>0\), \( C^{n}(X;X)\) be a family of functions, and \(=(_{V})_{V S_{0}(X)}\) be a sequence such that \(_{V} 0\) as \(V X\). We say that a function_

\[_{X}\,_{V S_{0}(X)}C((0,r)};V), F(F_{V})_{V S_{0}(X)}\]_is an \(\)-approximation operation for functions \(\) in the ball \(B_{X}(0,r)\) taking values in families \(_{V} C^{1}(V;V)\) if \(_{X}\) maps a function \(F\ X X\), where \(F\), to a sequence of functions \((F_{V})_{V S_{0}(X)}\), where \(F_{V}_{V}\), such that the following is valid: For all \(F\ X X\) satisfying \(\|F\|_{C^{n}((0,r)};X)} M\), we have_

\[_{x(0,r)}}\|F_{V}(x)-P_{V}(F(x))\|_{X} M_ {V}, \]

_where \(P_{V}\ X X\) is the orthogonal projection onto \(V\), that is, \((P_{V})=V\)._

_(ii) We say that \( C^{n}(X;X)_{V S_{0}(X)}C(V;V), F(F_{V })_{V S_{0}(X)}\) is a weak approximation operation for the family \( C^{n}(X;X)\) if for any \(F\) and \(r>0\) it holds that_

\[_{V X}_{x(0,r)}}\|F_{V}(x)-P_{V}(F(x))\|_{X} 0.\]

Note that the condition (i) is stronger than the condition (ii). An example of an approximation operation for the family \(=C^{n}(X)\), that is, an \(\)-approximation operation with all sequences \(=(_{V})_{V S_{0}(X)}\) subject to \(_{V}>0\), is the linear discretization

\[_{}(F)=(F_{V})_{V S_{0}(X)}, F_{V}=P_{V}(F |_{V}):\ V V. \]

Nonlinear discretization methods that do not rely on \(P_{V}\) have been used, for example, in the numerical analysis of nonlinear partial differential equations. Here, \(X\) becomes an appropriate Sobolev space, and a Galerkin approximation is implemented through finite-dimensional subspaces, \(V\), spanned by finite element basis functions. We present an example for the nonlinear equation, \( u(t)-g(u(t))= x(t)\) where \(g\) is a smooth convex function, when \(F\ x u\), in Appendix A.1.

Below, we will study whether a family, \(^{1}(X)\), of \(C^{1}\) diffeomorphisms on \(X\) can be approximated by \(C^{1}\) diffeomorphisms, \(_{V}^{1}(V)\), on finite-dimensional subspaces, \(V\). Of course, diffeomorphisms are bijective. Unless stated otherwise, from now on we will omit \(C^{1}\) and implicitly assume that diffeomorphism are \(C^{1}\) diffeomorphisms. We introduce two more notions that will play a key role in the further analysis.

**Definition 2** (Strongly Monotone).: _We say that a (nonlinear) operator \(F\ X X\) on Hilbert space, \(X\), is strongly monotone if there exists a constant \(>0\) so that_

\[ F(x_{1})-F(x_{2}),x_{1}-x_{2}_{X}\|x_{1}-x_{2}\|_{X} ^{2},x_{1},x_{2} X. \]

**Definition 3** (Bilipschitz).: _We say that \(F\) if bilipschitz there exist constants \(c>0\) and \(C<\) so that for all \(x_{1},x_{2} X\), \(c\,\|x_{1}-x_{2}\|\|F(x_{1})-F(x_{2})\| C\,\|x_{1}-x_{2}\|\)._

### A general framework for neural operators

In this paper, we are concerned with the modeling of diffeomorphisms between Hilbert spaces by bijective neural operators. Our working definition of neural operator, which generalizes the traditional notion, is given below. We note the presence of a skip connection, which is essential.

**Definition 4** (Generalized neural operator layer).: _For Hilbert space \(X\), a layer of a neural operator is a function \(F\ X X\) of the form_

\[F(x)=x+T_{2}G(T_{1}x), \]

_where \(T_{1}\ X X\) and \(T_{2}\ X X\) are compact linear operators 2 and \(G\ X X\) is a nonlinear operator in \(C^{1}(X)\). A generalized neural operator, \(H:\ X X\), is given by the composition_

\[H=A_{L} F_{L} A_{L-1} F_{L-1}  A_{1} F_{1}, \]

_where each \(F_{}\), \(=1,,L\) is of the form (4), the \(A_{}:X X\) are bounded linear operators and \( X X\) is a continuous operation (for example, a Nemytskii operator defined by a composition with a suitable activation function in function spaces)._The generalized neural operators can represent the classical neural operators . For an explicit construction, we refer to Appendix C.1. In the next section, we will study, under what conditions, bounded linear operators \(A_{}\), Nemytskii operators \(\), and neural operator layers \(F_{}\), for which the generalized neural operator consists, can be continuously discretized.

We note that because \(G C^{1}(X)\) in Definition 4, it follows that \(G L^{}(X)\) and \(_{X X}(G)<\). Given a Hilbert space, \(X\), a _layer of a strongly monotone neural operator_ (respectively, a _layer of a bilipschitz neural operator,_) is a function \(F\, X X\) that is strongly monotone (respectively, bilipschitz). Furthermore, a _strongly monotone neural operator_ (respectively, a _bilipschitz neural operator_), is a generalized neural operator with strongly monotone (respectively, bilipschitz), layers.

## 3 Category theoretic framework for discretization

"Well-behaved" operators between infinite-dimensional Hilbert spaces may have dramatically different behaviors than corresponding "well-behaved" maps between finite-dimensional Euclidean spaces. This observation applies to discretization and neural operators versus neural networks. In this section we explore this. We first present a no-go theorem, that there are _no_ procedures that continuously discretize an isotopy of diffeomorphisms. Next, we introduce strongly monotone neural operator layers, which are strongly monotone diffeomorphisms, and then prove that these allow a continuous approximation functor, that is, continuous approximations by strongly monotone diffeomorphisms on finite dimensional spaces. We finally show that bilipschitz neural operator layers admit a representation via strongly monotone operators and linear maps, allowing for their continuous approximation.

### No-go theorem for discretization of diffeomorphisms on Hilbert spaces

In this section, we present our no-go theorem. To formulate the 'impossibility' of something, we must define what is meant by discretization and approximation. Before this, we give an informal statement of the no-go theorem.

**Theorem 1** (No-go Theorem, Informal).: _Let \(\) be an approximation scheme that maps diffeomorphisms \(F\) on a Hilbert to a sequence of finite-approximations \(F_{V}\) that are themselves diffeomorphisms. If \(F_{V}\) converges to \(F\) as \(V X\), then \(\) is not continuous, that is, there are maps \(F^{(j)}\) that converge to \(F\) as \(j\), but all \(F^{(j)}_{V}\) are far from \(F_{V}\)._

We want to emphasize that most practical numerical algorithms are continuous so that the output depends (in some suitable sense) continuously on the input. This shows that there are no such numerical schemes that approximate infinite-dimensional diffeomorphisms with finite-dimensional ones. In order to prove our no-go theorem in the most general setting, we phrase it in terms of category theory. Namely, we formulate \(\) (which will denote the approximation scheme) as a functor from the category of Hilbert spaces and diffeomorphisms thereon, to their finite-rank approximations.

**Definition 5** (Category of Hilbert Space Diffeomorphisms).: _We denote by \(\) the category of Hilbert diffeomorphisms with objects \(_{}\) that are pairs \((X,F)\) of a Hilbert space \(X\) and a (possibly nonlinear) \(C^{1}\)-diffeomorphism \(F X X\) and the set of morphisms (or arrows that'map' objects to other objects) \(\) that are either_

1. _(induced isomorphisms) Maps_ \(a_{}\) _that are defined for a linear isomorphism_ \(:X_{1} X_{2}\) _of Hilbert spaces_ \(X_{1}\) _and_ \(X_{2}\) _that maps the objects_ \((X_{1},F_{1})_{}\) _to the object_ \(((X_{1}), F_{1}^{-1})_{}\)_, or_
2. _(induced restrictions) Maps_ \(a_{X_{1},X_{2}}\) _that are defined for a Hilbert space_ \(X_{1}\)_, its closed subspace_ \(X_{2} X_{1}\)_, and an object_ \((X_{1},F_{1})_{}\) _such that_ \(F_{1}(X_{2})=X_{2}\)_. Then_ \(a_{X_{1},X_{2}}\) _maps to the object_ \((X_{1},F_{1})_{}\) _to the object_ \((X_{2},F_{1}|_{X_{2}})_{}\)_._

**Definition 6** (Category of Approximation Sequences).: _We denote by \(\) the category of approximation sequences, that has objects \(_{}\) that are of the form \((X,S_{0}(X),(F_{V})_{V S_{0}(X)})\) where \(X\) is a Hilbert space,_

\[S_{0}(X) S(X)=\{V V X\},\]

_are partially ordered lattices, \(_{V S_{0}(X)}V=X\), and \(F_{V} V V\) are \(C^{1}\)-diffeomorphisms of spaces \(V S_{0}(X)\).__The set of morphisms \(_{}\) consists of either_

1. _Maps_ \(A_{}\) _that are defined for a linear isomorphism_ \(:X_{1} X_{2}\) _of Hilbert spaces_ \(X_{1}\) _and_ \(X_{2}\)_, and lattices_ \(S_{0}(X_{1})\) _and_ \(S_{0}(X_{2})=\{(V) V S_{0}(X_{1})\}\)_, that maps the objects_ \((X_{1},S(X_{1}),(F_{V})_{V S(X_{1})})\) _to_ \((X_{2},S(X_{2}),( F_{^{-1}(W)}^{-1})_{W S(X_{2})})\)_, or_
2. _Maps_ \(A_{X_{1},X_{2}}\) _that are defined for a Hilbert space_ \(X_{1}\)_, its closed subspace_ \(X_{2} X_{1}\)_, and an object_ \((X_{1},S_{0}(X_{1}),(F_{V})_{V S_{0}(X_{1})})\) _such that_ \(F(X_{2})=X_{2}\) _and_ \(S_{0}(X_{2})=\{V S_{0}(X_{1}) V X_{2}\}\) _is a partially ordered lattice. Then_ \(A_{X_{1},X_{2}}\) _maps the object_ \((X_{1},S_{0}(X_{1}),(F_{V})_{V S_{0}(X_{1})})\) _to the object_ \((X_{2},S_{0}(X_{2}),(F_{V})_{V S_{0}(X_{2})})\)_._

Next, we define the notion of an approximation or discretization functor. In practice, an approximation functor is an operator which maps a function \(F\) in an infinite dimensional space \(X\) to a function \(F_{V}\) that operate in finite dimensional subspaces \(V\) of \(X\) in such a way that functions \(F_{V}\) are close (in a suitable sense) to the function \(F\).

**Definition 7** (Approximation Functor).: _We define the approximation functor, denoted by \(\), as the functor that maps each \((X,F)_{}\) to some \((X,S_{0}(X),(F_{V})_{V S_{0}(X)})_{}\) so that the Hilbert space \(X\) stays the same. The approximation functor maps all morphisms \(a_{}\) to \(A_{}\) and morphisms \(a_{X_{1},X_{2}}\) to \(A_{X_{1},X_{2}}\), and has the following the properties_

1. _For all_ \(r>0\) _and all_ \((X,F)_{}\)_,_ \[_{V X}_{x_{X}(0,r) V}\|F_{V}(x)-F(x)\|_{X}=0.\] _In separable Hilbert spaces this means that when the finite dimensional subspaces_ \(V X\) _grow to fill the whole Hilbert space_ \(X\)_, then the approximations_ \(F_{V}\) _converge uniformly in all bounded subsets to_ \(F\)_._

We recall the notation \(_{V X}\) used above: We consider \((S_{0}(X),)\) as a partially ordered set and say that real numbers \(y_{V}\) converge to the limit \(y\) as \(V X\), and denote

\[_{V X}y_{V}=y,\]

if for all \(>0\) there is \(V_{0} S_{0}(X)\) such that for \(V S_{0}(X)\) satisfying \(V V_{0}\) it holds that \(|y_{V}-y|<\).

**Definition 8**.: _We say that the approximation functor \(\) is continuous if the following holds: Let \((X,F),(X,F^{(j)})_{}\) be such that the Hilbert space \(X\) is the same for all these objects and let \((X,S_{0}(X),(F_{V})_{V S_{0}(X)})=(X,F)\) be approximating sequences of \((X,F)\) and \((X,S_{0}(X),(F_{j,V})_{V S_{0}(X)})=(X,F^{(j)})\) be approximating sequences of \((X,F^{(j)})\). Moreover, assume that \(r>0\) and_

\[_{j}_{x_{X}(0,r)}\|F^{(j)}(x)-F(x)\|_{X}=0. \]

_Then, for all \(V S_{0}(X)\) the approximations \(F_{V}^{(j)}\) of \(F^{(j)}\) and \(F_{V}\) of \(F\) satisfy_

\[_{j}_{x V_{V}(0,r)}\|F_{V}^{(j)}(x)-F_{V}( x)\|_{X}=0. \]

The theorem below states a negative result, namely that there does not exist continuous approximating functors for diffeomorphisms.

**Theorem 2**.: _(No-go theorem for discretization of general diffeomorphisms) There exists no functor \(\) that satisfies the property (A) of an approximation functor and is continuous._

The proof is given in Appendix A.4.1, and is quite involved, but we give an overview of some of the steps here. A generalization of Theorem 2, in the case where the norm topology is replaced by the weak topology, is considered in Appendix D.1.

A key fact is that for finite dimensional diffeomorphisms the space of smooth embeddings consists of two connected components, one orientation preserving and the other orientation reversing. This is not the case in infinite dimensions, see e.g.  and . For an illustration of this, see Figure 1. The proof proceeds by contradiction. First, we consider the action of the approximation functor as it operates on an isotopy (path of diffeomorphisms) that connects two diffeomorphisms. The first has only orientation-preserving discretizations, and the second only orientation-reversing discretizations. We then show that the image of the path under the approximation functor yields a disconnected path, as the discretization 'jumps' from the orientation preserving component to the orientation reversing component. This violates continuity. To encode the notions of orientation preserving and orientation reversing that allow for a description of nonlinear discretization theory, we use topological degree theory. This generalizes the familiar notion of orientation that uses the sign of the determinant of the Jacobian matrix.

### Strongly monotone diffeomorphisms and their approximation on finite-dimensional subspaces

In this section and the next two we show that, although Theorem 2 precludes continuous approximation of general diffeomorphisms, stronger constraints on the diffeomorphisms allows one to sidestep the topological obstruction. In this section, in summary, we show that the obstruction to continuous approximation vanishes when the diffeomorphisms in question are assumed to be strongly monotone. Key to our positive result is the following technical result that states that the restriction of the domain and codomain of a strongly monotone diffeomorphism always yields another strongly monotone diffeomorphism.

**Lemma 1**.: _Let \(V X\) be a finite-dimensional subspace of \(X\), and let \(P_{V}:X X\) be orthogonal projection onto \(V\). Let \(F:X X\) be a strongly monotone \(C^{1}\)-diffeomorphism. Then, \(P_{V}F|_{V}:V V\) is strongly monotone, and a \(C^{1}\)-diffeomorphism._

The proof is given in Appendix A.5.1. Lemma 1 implies that the discretization functor \(_{}\) defined in (2) is a well-defined functor from strongly monotone \(C^{1}\)-diffeomorphisms of \(X\) to \(C^{1}\)-diffeomorphisms of \(V\). Note that the discretization functor \(_{}\) on strongly monotone \(C^{1}\) diffeomorphisms may not be a continuous approximation functor in the strong sense of Definitions 7 and 8, but it is obviously a continuous approximation functor in the weak sense of Definitions 11 and 12. Therefore, we obtain that :

**Proposition 1**.: _Let \(_{}\) be the discretization functor that maps \(F\) to \(P_{V}F|_{V}\) for each finite subspace \(V X\). Let \(_{sm}\) and \(_{sm}\) be categories where \(F X X\) and \(F_{V} V V\) are strongly monotone \(C^{1}\)-diffeomorphisms. Then, the functor \(_{}:_{sm}_{sm}\) satisfies assumption (A') of a weak approximation functor in Definition 11, and is continuous in the weak sense of Definition 12._

Strongly monotone Nemytskii operators and linear bounded operators and their continuous approximation on finite-dimensional subspaces

By Proposition 1, strongly monotone maps can be continuously discretized in the weak sense. Thus, we concern under what conditions, the maps, of which generalized neural operator consists, can be

Figure 1: A figure illustrating the proof ideas for Theorem 2. It represents the disconnected components of diffeomorphisms that preserve orientation, notated by \(^{+}\), and reverse orientation, notated, \(^{-}\). The horizontal axis abstractly represents the two disconnected components of \(\) for a finite-dimensional vector space \(V\). The vertical axis represents the dimension of \(V\). Observe how the two components of \(\) connect as \((V)\), and \(V\) becomes a Hilbert space \(H\).

strongly monotone. In this subsection, we focus on bounded linear operators and Nemytskii operators (layers of neural operator will be discussed in the next subsection). The following lemma is obviously given by the definition of a strongly monotone map :

**Lemma 2**.: _Let \(A:X X\) be a linear bounded operator and satisfy \( Au,u c_{0}\|u\|_{X}^{2}\) for some \(c_{0}>0\). Then, \(A:X X\) is strongly monotone._

Next, assuming that \(X=L^{2}(D;)\), we define Nemytskii operator by

\[F^{}(u)= u, \]

where \(:\) is continuous. In this case, we can show the following lemma by using [50, Corollary 3.3]:

**Proposition 2**.: _Assume that \(\) satisfies that \(|(s)| C_{1}|s|+C_{2}\) and the derivative of \(s(s)\) is defined a.e and satisfies \(^{}(s)>0\). Then, \(F^{}:L^{2}(D;) L^{2}(D;)\) is strongly monotonous._

Strongly monotone generalized neural operators and their continuous approximation on finite-dimensional subspaces

As seen in Theorem 3 below, strongly monotone layers of neural operators do not suffer from the same topological obstruction to continuous discretization as general diffeomorphisms. We now give sufficient conditions for the layers of a neural operator to be strongly monotone, and show that these conditions imply that those are diffeomorphisms.

**Lemma 3**.: _All strongly monotone layers of neural operators (\(F\)) defined by (4) are diffeomorphisms._

Also, the following theorem is proven in Appendix A.6.3.

**Theorem 3**.: _Let \(_{}\) be the discretization functor that maps \(F\) to \(P_{V}F|_{V}\) for each finite subspace \(V X\). Let \(_{smn}\) and \(_{smn}\) be categories where \(F X X\) and \(F_{V} V V\) are strongly monotone \(C^{1}\)-functions of the form (4). Then, the functor \(_{}:_{smn}_{smn}\) satisfies assumption (A), and it is continuous in the sense of Definition 8._

The functor defined in Theorem 3 does not suffer from the same topological obstruction as functors for general diffeomorphisms, shown in the no-go Theorem 2.This is because when \(F_{V}=P_{V}F|_{V}\) is strongly monotone, its derivative \(D|_{x}F_{V} V V\) is a strongly monotone matrix at all points \(x V\). Therefore it is strictly positive definite (see [47, Prop. 12.3]) and the determinant \((D|_{x}F_{V})\) is strictly positive. Due to this, the orientation of the finite-dimensional approximations never switch signs, and the key technique used in the proof of the no-go Theorem 2 does not apply.

A straightforward condition to guarantee strong monotonicity of a neural operator layer is given in

**Lemma 4**.: _Let \(F:X X\) be a layer of neural operator that is of the form \(F(u)=u+T_{2}G(T_{1}u)\), where \(T_{j}:X X\), \(j=1,2\) are compact operators and \(G:X X\) is a \(C^{1}\)-smooth map. Assume that Frechet derivative \(DG|_{x}\) of \(G\) at \(x\) satisfies the following for all \(x X\),_

\[\|DG|_{x}\|_{X X}\|T_{1}\|_{X X}^{-1}\|T_{2}\|_{X X}^{ -1}.\]

_Then, \(F X X\) is strongly monotone._

See Appendixes A.6.1 and A.6.2 for the proofs.

### Bilipschitz neural operators are conditionally strongly monotone diffeomorphisms

Now we show an analogous result to Theorem 3, but applied to bilipschitz neural operators. Moreover, we will show that all neural operator \(F X X\) that are bilipschitz admit approximations that can be locally inverted using iteration for each point in their range using an iteration.

**Theorem 4**.: _Let \(X\) be a Hilbert space. Then there is \(e X\), \(\|e\|_{X}=1\) such that the following is true: Let \(F X X\) be a layer of a bilipschitz neural operator. Then for all \(r_{1}>0\) and \(>0\) there are a linear invertible map \(A_{0}:X X\), that is either the identity map or a reflection operator3_\(x x-2 x,e_{X}e\), and strongly monotone functions \(H_{k}\) that are also layers of neural operators such that_

\[H_{k}:X X, H_{k}(x)=x+B_{k}(x), k=1,2,,J,\]

_where \(B_{k}:X X\) is a compact mapping and satisfies \((B_{k})<\) and_

\[F(x)=H_{J} H_{2} H_{1} A_{0}(x),x  B_{X}(0,r_{1}). \]

_Moreover, if \(F C^{2}(X,X)\), then \(J=(^{-2})\)._

The proof of Theorem 4 is in Appendix A.7.1. Theorem 4 shows that we may always decompose a bilipschitz neural operator into the composition of strongly monotone neural operator layers \(H_{j}\) and a reflection operator \(A_{0}\). Each \(H_{j}\) can be discretized using the continuous functor \(_{}\) from Theorem 3. If we consider the discretization (via the construction in Definition 6) using a collection of subsets \(S_{0}(X) S(X)\) such that all \(V S_{0}(X)\) satisfy \(e V\), then the operator \(A_{0}\) can be discretized by \(A_{0,V}=P_{V} A_{0}|_{V}\). These mean that if we write a bilipschitz neural operator as a sufficiently deep neural operator where each layer is either of the form \(Id+B_{j}\), where \((B_{j})<1\), or a reflection operator \(A_{0}\). In either case, we may use linear discretization to approximate each layer. So, we may discretize \(F\) in a ball \(B_{X}(0,R)\) by discretizing each layer \(H_{j}\) and \(A_{1}\) where \(F=H_{J} H_{1} A_{1}\). We observe that the number of layers, \(J\), depends on \(R\).

We have observed that operators of the form identity plus a compact term are critical for continuous discretization. This insight motivates the introduction of residual networks as approximators within the framework of finite-rank neural operators. In what follows, we assume that \(X\) is a separable Hilbert space, with an orthonormal basis \(=\{_{n}\}_{n}\). For \(N\), we define \(E_{N}:X^{N}\) and \(D_{N}:^{N} X\) by \(E_{N}u:=( u,_{1}_{X},..., u,_{N}_{X} )^{N}\). \(D_{N}:=_{n N}_{n}_{n}\). We note that \(P_{N}=D_{N}E_{N}\), where \(P_{N}:X X\) is the projection onto \(V_{N}:=\{_{n}\}_{n N}\). Using \(E_{N}\), \(D_{N}\), we define the class of residual networks in the separable Hilbert space, with \(T,N\) and activation function \(\), as

\[_{T,N,,}(X):=G:X X:G= _{t=1}^{T}(Id_{X}+D_{N} NN_{t} E_{N}),\] \[NN_{t}:^{N}^{N}\;(t=1,...,T)}. \]

The following theorem proves a universality result for each of the layers \(G\), allowing us to obtain a general universality result for the entire network. The statement of the theorem requires the careful construction of a neural operator-representable function \(\). Giving a full description of \(\) involves introducing a lot of technical notation, and so the presentation here in the main text leaves the details of the construction of \(\) abridged. For the full statement of the theorem and definition of the notation, see Section A.7.2. Intuitively, \(\) is the 'wrapping' of a fixed-point process in a neural operator.

**Theorem 5**.: _Let \(R>0\), and let \(F X X\) be a layer of a bilipschitz neural operator, as in Definition 3. Let \(\) be the Rectified Cubic Unit (ReLU) function defined by \((x):=\{0,x\}^{3}\). Then, for any \((0,1)\), there are \(T,N\) and \(G_{T,N,,}(X)\) that has the form_

\[G=(Id_{X}+D_{N} NN_{T} E_{N})(Id_{X}+D_{N} NN_{ 1} E_{N}),\]

_such that each map \((Id_{X}+D_{N} NN_{t} E_{N})\) is strongly monotone \(C^{1}\)-diffeomorphisms on some ball and_

\[_{x_{X}(0,R)}\|F(x)-G A(x)\|_{X},\]

_where \(A X X\) is a linear invertible map that is either the identity map or a reflection operator \(x x-2 x,e_{X}e\) with some unit vector \(e X\). Further, \(G A:B_{X}(0,R) G A(B_{X}(0,R))\) is invertible, and there is some neural operator \(:G A(B_{X}(0,R)) A(B_{X}(0,R))\) so that \(G A|_{B_{X}(0,R)}^{-1}=A^{-1}\)._

The proof is given in Section A.7.2. Neural operator \(\) becomes a better approximation of the inverse operator when it becomes deeper. Theorem 5 means that neural operators are an operator algebra of nonlinear operators that are closed in composition and when the inverse of a neural operator exists, the inverse operator can be locally approximated by neural operators.

In the case when the separable Hilbert space \(X\) is the real-valued \(L^{2}\)-function space \(L^{2}(D;)\), residual networks in the separable Hilbert space can be represented as residual neural operators defined by (179). See Lemma 11 for details. Then, we obtain the following 

**Corollary 1**.: _Let \(D^{d}\) be a bounded domain, and let \(=\{_{n}\}_{n}\) be an orthonormal basis in \(L^{2}(D;)\). Assume that the orthonormal basis \(\) include the constant function. Let \(_{T,N,,}(L^{2}(D;))\) be the class of residual neural operators defined in (179). Then, the statement replacing \(X\) with \(L^{2}(D;)\) and \(G_{T,N,,ReLU}(X)\) with \(G_{T,N,,ReLU}(L^{2}(D;))\) in Theorem 5 holds._

The proof is given by a combination of Theorem 5 and Lemma 11. We note that the assumption that the orthonormal basis \(\) includes the constant function is satisfied if we choose \(\) to be a Fourier basis, which yields the Fourier neural operator, see e.g., [39; 38].

**Remark 1**.: _In this section, we have shown that residual networks in a separable Hilbert space \(X\), defined in (10), are universal approximators for layers of bilipschitz neural operators. Additionally, in the specific case where \(X=L^{2}(D;)\), residual neural operators, defined in Definition 9, also provide universal approximators for layers of bilipschitz neural operators. We note that the residual network we have discussed is locally invertible but not globally. By introducing invertible residual networks on Hilbert space \(X\), defined in (166), we can similarly prove that these networks by employing sort activation functions (see [3, Section 4]) are universal approximators for strongly monotone diffeomorphisms with compact support. Specifically, when \(X=L^{2}(D;)\), invertible residual neural operators, defined in Definition 9, are also universal approximators for strongly monotone diffeomorphisms with compact support. For further details, we refer to Appendix B._

## 4 Quantitative approximation

Quantitative approximation results for neural networks, see e.g.  or , can be used to derive quantitative error estimates for discretization operations. Let \(F_{X}(0,r) X\) be a non-linear function satisfying \(F(_{X}(0,r);X)\), in \(n=1\), or \(F C^{n}(_{X}(0,r);X)\), if \(n 2\). Then, \(F\) can be discretized using neural networks in the following way: Let \(_{V}>0\) be numbers indexed by the linear subspaces \(V X\) such that \(_{V} 0\) as \(V X\). When \(=(_{V})_{V S(X)}\), in the sense of Definition 1, an \(\)-approximation operation \(_{NN}:\ F(F_{V})_{V S(X)}\) in the ball \(B_{X}(0,r)\) can be obtained by defining \(F_{V}=J_{V}^{-1} F_{V,} J_{V}:\ V V\), where \(J_{V}:\ V^{d}\) is an isometric isomorphism, \(d=(V)\), \(F_{V,}\ ^{d}^{d}\) is a feed-forward neural network with ReLU-activation functions with at most \(C(d)_{2}((1+r)/_{V})\) layers and \(C(d)_{V}^{d}_{2}((1+r)/_{V})\) non-zero elements in the weight matrices. Details of this result are given in Proposition 5 in Appendix A.8.

## 5 Conclusion

In this work, we have studied the problem of discretizing neural operators between Hilbert spaces. Many physical models concern functions \(^{n}\), for example \(L^{2}(^{n})\), the computational methods based on approximations in finite dimensional spaces should become better when the dimension of the model grows and tends to infinity. We have focused on diffeomorphisms in infinite dimensions, which are crucial to understand in generative modeling. We have shown that the approximation of diffeomorphisms leads to computational difficulties. We used tools from category theory to produce a no-go theorem showing that general diffeomorphisms between Hilbert spaces may not admit any continuous approximations by diffeomorphisms on finite spaces, even if the approximations are allowed to be nonlinear. We then proceeded to give several positive results, showing that diffeomorphisms between Hilbert spaces may be continuously approximated if they are further assumed to be strongly monotone. Moreover, we showed that the difficulties can be avoided by considering a restricted but still practically rich class of diffeomorphisms. This includes bilipschitz neural operators, which may be represented in any bounded set as a composition of strongly monotone neural operators and strongly monotone diffeomorphisms. We then showed that such operators may be inverted locally via an iteration scheme. Finally we gave a simple example on how quantitative stability questions can be obtained for discretization functors, inviting other researchers to study related questions using more sophisticated methods.