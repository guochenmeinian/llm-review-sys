# AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation

Tong Wu\({}^{1}\)\({}^{1}\), Zhihao Fan\({}^{2}\)\({}^{1}\), Xiao Liu\({}^{3}\), Hai-Tao Zheng\({}^{1,8}\), Yeyun Gong\({}^{3}\), Yelong Shen\({}^{4}\),

**Jian Jiao\({}^{5}\), Juntao Li\({}^{6}\), Zhongyu Wei\({}^{2}\), Jian Guo\({}^{7}\), Nan Duan\({}^{3}\), Weizhu Chen\({}^{4}\)**

\({}^{1}\)Shezhen International Graduate School, Tsinghua University, \({}^{2}\) Fudan University,

\({}^{3}\)Microsoft Research Asia, \({}^{4}\)Microsoft Azure AI, \({}^{5}\)Microsoft,

\({}^{6}\)Soochow University, \({}^{7}\)IDEA Research, \({}^{8}\)Pengcheng Laboratory

{yegong, yeshe, nanduan, wzchen}@microsoft.com,

zheng.haitao@sz.tsinghua.edu.cn, guojian@idea.edu.cn

Work done during an internship at Microsoft Research Asia.These authors contributed equally to this work.Corresponding author.

###### Abstract

Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced sequential dependency in comparison to images, and the majority of existing language models are trained with a left-to-right auto-regressive approach. To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that the generation of tokens on the right depends on the generated ones on the left, a mechanism achieved through employing a dynamic number of denoising steps that vary based on token position. This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right. In a series of experiments on various text generation tasks, including text summarization, machine translation, and common sense generation, AR-Diffusion clearly demonstrated its superiority over existing diffusion language models and that it can be \(100 600\) faster when achieving comparable results. Our code is available at this https URL.

## 1 Introduction

Text generation is a fundamental task within the field of natural language processing (NLP). Pretrained language models like GPT-4 , LLaMA , and Alpaca  have garnered significant attention with their ability to generate fluent and human-like textual content. These models utilize the auto-regressive (AR) Transformer decoders  to emit generated tokens one-by-one in sequential order from left to right. By leveraging the power of position dependency, AR models are able to enhance the naturalness, coherence, and adherence to human language conventions in the generated text .

Recent studies have shown the remarkable performance of diffusion models in image generation , motivating researchers to extend diffusion to text generation . By introducing timestep, these methods progressively regulate the interpolation between the original tokens and Gaussian noise, then iteratively denoise for text generation. At each timestep, the diffusion-based text generator predictsall tokens simultaneously following Non-Auto-Regression (NAR) [Lewis et al., 2020, Qi et al., 2020, 2021, Li et al., 2022b], leading to faster decoding speed compared to AR. However, it also inherits the drawback of NAR, namely the sacrifice of inter-token position dependency [Li et al., 2022c] and the drop of generation performance [Bao et al., 2021].

To conduct a comprehensive analysis, we introduce a two-dimensional coordinate system to track the diffusion timestep of tokens \(f()\) positioned at various locations. As illustrated in Figure 1, the system assigns the token position \(n[1,N]\) to the horizontal axis and the diffusion timestep \(t[0,T]\) to the vertical axis. Diffusion-LM [Li et al., 2022a], which is followed by existing diffusion-based text generation models, is shown in Figure 1(a). It assigns a uniform timestep \(t\) to all tokens. In contrast, tokens in the AR model depicted in Figure 1(b) exhibit distinct timesteps within a generation step (\(t_{i}\)). For instance, the already decoded token at position \(n_{1}\) has a timestep of \(0\), while the to-be-decoded token at position \(n_{2}\) has a timestep of \(T\). This approach effectively captures the sequential dependency. Motivated by this observation, we introduce AR-Diffusion, an auto-regressive diffusion method, for the disparity in token positions and the principle of sequential token identification.

In AR-Diffusion, we propose a **multi-level diffusion strategy** that includes both sentence-level and token-level diffusion. We randomly choose a sentence-level timestep \(t\), and assign **dynamic movement speeds**\(v()\) by determining position-sensitive token-level timestep \(f(n,t)\) for each token. This enables tokens at the left of a sentence to undergo faster movement from random Gaussian noise to token embedding, while those at the right of the sentence experience slower movement to better utilize information from previously denoised tokens. During inference, to reduce the significant number of inference steps (e.g., 2,000) required in Diffusion-LM [Li et al., 2022a], SeqDiffSeq [Yuan et al., 2022] and GENIE [Lin et al., 2023], we introduce a skipping mechanism that collaborates with the multi-level diffusion strategy to accelerate the process.

Experimental results across various text generation tasks, such as text summarization, machine translation, and common sense generation, have consistently demonstrated that AR-Diffusion surpasses existing text diffusion models, including AR methods in terms of both quality and diversity. Moreover, our verification reveals that AR-Diffusion requires fewer resources during decoding while maintaining superior performance. It achieves \(100\) faster than SeqDiffSeq [Yuan et al., 2022] in machine translation and \(600\) faster than GENIE [Lin et al., 2023] in text summarization while delivering comparable results. Furthermore, it demonstrates promising results even in a challenging scenario where decoding is limited to only two steps.

Figure 1: Model behaviors illustrated on a two-dimensional coordinate system, where the horizontal axis stands for the position and the vertical axis represents the diffusion timestep. In the inference stage, different models will behave differently. (a) For the typical Diffusion-LM [Li et al., 2022a], each token share the identical movement speed \(v(n_{1},t_{i},t_{i+1})=v(n_{2},t_{i},t_{i+1})=|t_{i+1}-t_{i}|\). (b) For AR from the perspective of diffusion models, the tokens have two states based on the degree of interpolation between the original tokens and Gaussian noise: to be decoded (at timestep \(t=T\)) and already decoded (at timestep \(t=0\)). Specifically, we have \(v(n_{1},t_{i},t_{i+1})=0\) and \(v(n_{2},t_{i},t_{i+1})=T\). (c) In AR-Diffusion, \((n_{e},t_{e})\) is the coordinate of anchor point. Tokens in different positions exhibit varying movement speeds, such as \(v(n_{1},t_{i},t_{i+1})>v(n_{2},t_{i},t_{i+1})\) when \(n_{1}<n_{2}\).

Preliminary

### Conditional Generative Language Models

In the field of natural language generation, conditional generative models are commonly implemented using either auto-regressive (AR) or non-auto-regressive (NAR) methods. In AR (Vaswani et al., 2017), tokens on the right are predicted based on visible left tokens. The likelihood is given by \(p_{}(|)=_{i=1}^{N}p(_{i}|_{1:i-1};)\), where \(y_{i}\) denotes the \(i\)-th token of \(\). On the other hand, NAR (Gu et al., 2017) assumes conditional independence among tokens and generates them uniformly without distinction during decoding, resulting in the likelihood \(p_{}(|)=_{i=1}^{N}p(_{i}|)\). This parallel generation approach is of lower quality compared to AR, although it offers a substantial speed advantage.

### Diffusion Models for Text Generation

Recently, Li et al. (2022) propose a natural language generation model based on the diffusion process, which is typically divided into a forward noising process and a reverse denoising process.

Specifically, the forward process is a fixed linear Gaussian model, which gradually perturbs the random variable \(_{0}\) until it becomes the standard Gaussian distribution. This can be formalized as:

\[q(_{t}_{0};)=(_{t};_{ t}}_{0},(1-_{t})),\] (1)

where, \(_{t}=_{i=1}^{t}_{i}\), and \(_{i}\) is a coefficient that monotonically decreases with timestep \(t\), \(_{t}\) is the latent state at timestep \(t\).

The reverse process is to initiate from standard Gaussian noise and progressively utilize the denoising transition \(p_{}(_{t-1}|_{t};)\) for generation.

\[p_{}(_{t-1}_{t};)= _{t-1};_{}(_{t},t;),_{}(_{t},t;),\] (2)

where the mean \(_{}\) and variance \(_{}\) are learned from the model. In particular, we follow Li et al. (2022)'s approach of using predefined variance without trainable parameters.

To extend the continuous diffusion process to discrete text generation, Li et al. (2022) introduce an additional Markov transition from the discrete tokens \(\) to the latent variable \(_{0}\). In practice, we add an embedding step \(q_{}(_{0}|)=(_{0};(),(1- _{0}))\) in the forward process, and use a trainable rounding step which is parametrized by \(p_{}(|_{0};)=_{i=1}^{N}p_{}(y_{ i}|z_{0}^{i};)\) in the reverse process. In each timestep, we utilize an encoder-decoder model \(_{}(_{t},t;)\) to approximate \(_{0}\)(Lin et al., 2023) in a NAR manner and then estimate \(_{}(_{t},t;)\).

In consequence, combined with maximizing the evidence lower bound (ELBO) of \( p_{}(|)\), our training objective of the conditional diffusion language model is:

\[=_{q_{}(_{0:T}|)}[- p_{}(_{0};)+_{t=1}^{T}\|_{0}-_{}(_{t},t;)\|^{2}].\] (3)

## 3 Methodology

### Multi-Level Diffusion

In the typical diffusion process, every token in the text sequence has the same diffusion timestep. In order to leverage the sequential nature of language, we enable tokens to have different diffusion timesteps during the forward and reverse pass. To accomplish this, we propose a multi-level diffusion strategy that includes both sentence-level and token-level diffusion. Firstly, at the sentence-level, we follow Diffusion-LM (Li et al., 2022) to randomly select a timestep \(t\). Secondly, at the token-level, we incorporate positional information \(n[1,N]\) based on the sentence-level timestep to regulate the diffusion timestep for the current token. The procedure is illustrated as:

\[_{t}=_{f(1,t)}^{1},_{f(2,t)}^{2},,_{f(N,t )}^{N},\] (4)

where \(N\) is the given target sentence length, \(_{t}\) is the sentence representation at timestep4\(t\), \(_{f(n,t)}^{n}\) is the latent representation for the \(n\)-th token at sentence-level timestep \(t\), and \(f(n,t)\) is a token-leveltimestep function that denotes the token-level diffusion timestep determined by token position \(n\) and sentence-level timestep \(t\).

We visualize the token-level timestep \(n,f(n,t)\) onto a two-dimensional coordinate system as Figure 1, which takes the token **position** as the horizontal axis and the sentence-level **timestep** as the vertical axis. Furthermore, to provide a more profound description of the characteristics of movement, we define the speed of movement as the following equation.

\[v(n,t_{i},t_{i+1})=f(n,t_{i+1})-f(n,t_{i}),\] (5)

where \(t_{i}\) and \(t_{i+1}\) are the start and end sentence-level diffusion timesteps. It can be observed that tokens in Diffusion-LM share the same movement speed, while those in AR exhibit different speeds.

### Token-Level Diffusion with Dynamic Movement Speed

Based on the speed of movement, we propose a fundamental principle, dynamic movement speed, for designing the token-level diffusion timestep function \(f(n,t)\) to take advantage of AR in diffusion. Specifically, elements on the left side of a sentence undergo higher movement speed from random Gaussian noise to token embedding, while those on the right side experience lower movement speed, thereby they can be generated in the later sentence-level timestep and utilize information from previously generated tokens more effectively.

```
0: Dataset \(\{(,)\}\), maximum timestep number \(T\) and maximum target length \(N\).
0: Optimized model parameters \(\).
1: Define an anchor point \((n_{e},t_{e})\)5.

```
0: Dataset \(\{(,)\}\), maximum timestep number \(T\) and maximum target length \(N\).
0: Optimized model parameters \(\).
1: Define an anchor point \((n_{e},t_{e})\)5.

\[f(n,t)=-t_{s}}{n_{e}-n_{s}}(n-n_{s})+t_{s},0,T \] (7)

6: Sample \(_{f(n,t)}^{n}\) for each \(n\) in different positions with Gaussian reparameterization.

7: According to equation (3) and equation (9), employ gradient descent to optimize the objective:

\[_{}- p_{}(_{0};)+_{n =1}^{N}_{}(_{f(n,t)}^{n},f(n,t);)- _{0}^{2}\] (8)

8:until converged ```

**Algorithm 1** Training Process of AR-Diffusion.

Following the guidance of the principle, we develop a token-level diffusion strategy with the linear function, which is shown in Figure 1(c). In particular, the procedure is illustrated in Algorithm 1, where \((x,,)\) function is to clamp all elements in \(x\) into the range \([,]\). Specifically, in the forward process of diffusion, the start point goes to the left from \((N,0)\) to \((0,0)\) along the horizontal axis and then moves up to \((0,T)\) along the vertical axis. Therefore, the entire range of sentence-level timestep is extended to \([0,N+T]\).

In the reverse diffusion process, the multi-level diffusion follows the formula:

\[_{}_{t},t;)=_{} _{f(1,t)}^{1},f(1,t),_{f(2,t)}^{2},f(2,t) ,,_{f(N,t)}^{N},f(N,t);,\] (9)

where \(_{}(_{f(n,t)}^{n},f(n,t);)\) denotes the \(n\)-th element.

### Inference with Skipping

Typically, the generation process needs to go through all the sentence-level timesteps from \(T+N\) to \(0\). To reduce the decoding time, we introduce a skipping mechanism that allows us to traverse a subset of timesteps.

```
0: Source condition \(\), number of decoding steps \(M\) and model parameters \(\). Output: Predicted target embedding \(}\).
1: Define an anchor point \((n_{e},t_{e})\).
2: Uniformly select a decreasing sequence of timesteps \(\{t_{i}\}_{i=0}^{M}\) ranging from \(T+N\) to \(0\).
3: Sample \(_{t_{0}}(,)\).
4:for\(i=0\) to \(M-1\)do
5: Calculate the start point \((n_{s},t_{s})\) using equation (6).
6: Based on the current sentence-level inference steps \(t_{i}\) and the next one \(t_{i+1}\), assign token-level timesteps \(f(n,t_{i})\) and \(f(n,t_{i+1})\) to token in position \(n\) using equation (7).
7: Reverse sample \(_{t_{i+1}}=_{f(1,t_{i+1})}^{1},_{f(2,t_{i+1})}^{2}, ,_{f(N,t_{i+1})}^{N}\) from \(p_{}(_{t_{i+1}}_{t_{i}};)\) with the following formulas: \[p_{}(_{t_{i+1}}_{t_{i}};)=_{n=1}^{N}p_{ }_{f(n,t_{i+1})}^{n}_{f(n,t_{i})}^{n};\] (10) \[p_{}_{f(n,t_{i+1})}^{n}_{f(n,t_{i})}^{n}; _{f(n,t_{i+1})}^{n};_{f(n,t_{ i})}^{n}+_{}(_{f(n,t)}^{n},f(n,t);), \] (11)
8:endfor
9: Map \(_{t_{M}}\) to the nearest embedding \(}\). ```

**Algorithm 2** Inference Process of AR-Diffusion with the Skipping Mechanism.

In practice, we propose an algorithm for the inference, illustrated in Algorithm 2.

\[=_{f(n,t_{i})}}{_{f(n,t_{i+1 })}}}(1-_{f(n,t_{i+1})})}{1-_{f(n,t_{i})}},\;=_{f(n,t_{i+1})}}(1-_{f(n,t_{i})}}{_{f(n,t_{i+1})}})}{1-_{f(n,t_{i})}},\;=)})(1-_{f(n,t_{i+1})})}{1-_{f(n,t_{i })}}\] (12)

In equation (10), the conditional distribution of \(_{t_{i+1}}\) is inferred by \(p_{}(_{t_{i+1}}|_{t_{i}};)\), and then we decompose it by positions due to the independent forward process of elements at different positions. From equation (11) to equation (12), we establish the relationship between tokens at different timesteps, and the detailed derivation can be found in supplementary material F.

## 4 Experiments

### Tasks and Datasets

Text SummarizationThis task involves taking a long document as input and generating a concise sentence as output. This requires models with the ability to identify important content and rewrite it in a condensed form. In our experiments, we use the publicly available XSum[Narayan et al., 2018] and Cnn/DailyMail Hermann et al.  on GLGE6, which is also named as GLGE-Easy.

Machine TranslationTranslation is a widely used sequence-to-sequence task. The input is a sequence of words in the source language, and the output is a sequence of corresponding words in the target language. We choose the IWSLT 2014 dataset and the data processing method is to follow the scripts provided by fairseq7.

Common Sense GenerationIn this task, the model is provided with a concept set consisting of objects and actions as input. The objective is to generate a sentence that incorporates these concepts and describes a realistic scenario. We use CommonGen8 dataset for evaluation.

### Main Results

The results on different datasets are shown in Table 1, Table 2, Table 3 and Table 4. The best result is **bolded** and the second-best result is underlined. "\(k\)" indicates the number of generated candidate samples9. It can be seen from the results in each table that AR-Diffusion achieves the best performance.

During the inference process, we utilize **20** inference steps and employ Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding to select the best sample, following (Li et al., 2022). We choose MBR instead of the selection approach in GENIE, as GENIE picks up the best sample by calculating the maximum score for each generated one using ground truth, which introduces unfairness. To ensure a fair comparison, we re-implement GENIE using our configuration and perform inference with 20 steps. More experimental details can be found in the supplementary material B.

Text SummarizationThe results presented in Table 1 and Table 2 clearly demonstrate that AR-Diffusion outperforms the existing NAR and Semi-NAR approaches across all metrics. Moreover, AR-Diffusion consistently achieves significant improvements over GENIE in terms of all indicators. Furthermore, in comparison to Transformer, AR-Diffusion outperforms it on both ROUGE-1 and ROUGE-L, while achieving comparable performance in terms of ROUGE-2. Notably, when the sample number is 500, AR-Diffusion demonstrates superiority over Transformer across all the measures.

Machine TranslationTable 3 presents the BLEU score implemented by SeqDiffuSeq setting11. AR-Diffusion outperforms the non-auto-regressive CNAT in greedy search for a single sample, and achieves a substantial gain. Moreover, the BLEU score of AR-Diffusion surpasses GENIE by a large margin and shows a slightly better performance than the AR Transformer. Specially, AR-Diffusion achieves a more powerful result at \(k\) = 500.

  
**Methods** & **Pattern** & ROUGE-1 & ROUGE-2 & ROUGE-L \\  NAT (Gu et al., 2017) & & 24.0 & 3.9 & 20.3 \\ iNAT (Lee et al., 2018) & & 24.0 & 4.0 & 20.4 \\ CMLM (Ghazvininejad et al., 2019) & & 23.8 & 3.6 & 20.2 \\ LevT (Gu et al., 2019) & & 24.8 & 4.2 & 20.9 \\  InsT (Stern et al., 2019) & & 17.7 & 5.2 & 16.1 \\ iNAT (Lee et al., 2018) & & 27.0 & 6.9 & 22.4 \\ CMLM (Ghazvininejad et al., 2019) & & 29.1 & 7.7 & 23.0 \\ LevT (Gu et al., 2019) & & 25.3 & 7.4 & 21.5 \\  LSTM (Greff et al., 2017) & AR10  & 25.1 & 6.9 & 19.9 \\ Transformer (Vaswani et al., 2017) & & 30.5 & 10.4 & 24.2 \\  GENIE (Lin et al., 2023) (\(k\) = 50) & & 29.3 & 8.3 & 21.9 \\ AR-Diffusion (\(k\) = 50) & & 31.7 & 10.1 & 24.7 \\ AR-Diffusion (\(k\) = 500) & & **32.2** & **10.6** & **25.2** \\   

Table 1: Results on XSum test set. The results of NAR and Semi-NAR are from Qi et al. (2021), and the results of AR are from GLGE (Liu et al., 2021).

  
**Methods** & **Pattern** & ROUGE-1 & ROUGE-2 & ROUGE-L \\  LSTM (Greff et al., 2017) & AR & 37.3 & 15.7 & 34.4 \\ Transformer (Vaswani et al., 2017) & & 39.5 & 16.7 & 36.7 \\  GENIE (Lin et al., 2023) (\(k\) = 50) & & 34.4 & 12.8 & 32.1 \\ AR-Diffusion (\(k\) = 50) & Diffusion & 39.6 & 16.3 & 37.1 \\ AR-Diffusion (\(k\) = 500) & & **40.2** & **17.1** & **37.7** \\   

Table 2: Results on Cnn/DailyMail test set. The results of AR are from GLGE Liu et al. (2021).

### Inference Efficiency

First, we use the number of function evaluations (NFE) as a measure to compare inference efficiency (Ye et al., 2023) in machine translation. From Table 3, it is evident that even when the NFE is reduced to 1% of SeqDiffuSeq (equivalent to \(100\) faster), AR-Diffusion still outperforms SeqDiffuSeq. Moreover, increasing the number of generated candidate samples (\(k=500\)) leads to further performance improvements, albeit with increased time consumption.

Second, we conduct experiments with an **extremely limited number of inference steps** (2 and 3)12 and compare the performance with that of GENIE in XSum. The results are presented in Table 5. When reducing the number of steps to 2, GENIE experiences a significant decline, with an average score of 4.20 in the AVG Drop column, while AR-Diffusion exhibits a comparatively smaller

  
**Methods** & **Pattern** &  &  &  &  \\  bRNN-CopyNet (Gu et al., 2016) & & 9.23 & 30.57 & 13.60 & 7.80 & 17.40 & 16.90 \\ Trans-CopyNet (Lin et al., 2020) & AR & 11.08 & 32.57 & 17.20 & 10.60 & 18.80 & 18.00 \\ MeanPooling-CopyNet (Lin et al., 2020) & & 11.36 & 34.63 & 14.80 & 8.90 & 19.20 & 20.20 \\  LevT (Gu et al., 2019) &  & 12.22 & 35.42 & 23.10 & 15.00 & 22.10 & 21.40 \\ ConstLeven (Susanto et al., 2020) & & 13.47 & 35.19 & 21.30 & 12.30 & **25.00** & 23.20 \\  GENIE (Lin et al., 2023) (\(k\) = 50) &  & 12.89 & 35.21 & 22.00 & 13.30 & 24.30 & 23.00 \\ AR-Diffusion (\(k\) = 50) & & **13.93** & **37.36** & **25.60** & **16.40** & **25.00** & **24.20** \\   

Table 4: Results on CommonGen dev set. Results of NAR and AR are from Lin et al. (2020).

  
**Methods** & **Pattern** & BLEU & **Steps** & **NFE (Steps\( k\))** \\  Transformer (Vaswani et al., 2017) & AR & 34.74 & - & - \\  CNAT (Bao et al., 2021) & NAR & 29.81 & - & - \\  SeqDiffuSeq (Yuan et al., 2022) (\(k\) = 1) &  & 29.83 & 2,000 & 2,000 (2,000 \(\) 1) \\ AR-Diffusion (\(k\) = 1) & & 30.19 & 20 & 20 (20 \(\) 1) \\  GENIE (Lin et al., 2023) (\(k\) = 50) &  & 30.08 & 20 & 1,000 (20 \(\) 50) \\ AR-Diffusion (\(k\) = 50) & & 34.95 & 20 & 1,000 (20 \(\) 50) \\ AR-Diffusion (\(k\) = 500) & & **35.62** & 20 & 10,000 (20 \(\) 500) \\   

Table 3: Results on IWSLT14 De\(\)En test set following the setting of SeqDiffuSeq. “NFE” indicates the Number of Function Evaluations (Ye et al., 2023).

  
**Methods** & **Pattern** &  &  &  \\  Transformer (Vaswani et al., 2017) & AR & 34.74 & - & - \\  CNAT (Bao et al., 2021) & NAR & 29.81 & - & - \\  SeqDiffuSeq (Yuan et al., 2022) (\(k\) = 1) &  & 29.83 & 2,000 & 2,000 (2,000 \(\) 1) \\ AR-Diffusion (\(k\) = 1) & & 30.19 & 20 & 20 (20 \(\) 1) \\  GENIE (Lin et al., 2023) (\(k\) = 50) &  & 30.08 & 20 & 1,000 (20 \(\) 50) \\ AR-Diffusion (\(k\) = 50) & & 34.95 & 20 & 1,000 (20 \(\) 50) \\ AR-Diffusion (\(k\) = 500) & & **35.62** & 20 & 10,000 (20 \(\) 500) \\   

Table 3: Results on IWSLT14 De\(\)En test set following the setting of SeqDiffuSeq. “NFE” indicates the Number of Function Evaluations (Ye et al., 2023).

decrease of 1.34. Furthermore, with 3 steps, although the performance deterioration of GENIE is somewhat reduced, the average score still shows a decline of 2.81. In contrast, AR-Diffusion maintains a high performance level, with an average score differing from the 20-step result by only 0.64. Notably, the results of AR-Diffusion at 3 steps are comparable to the results of GENIE at 2,000 steps. Therefore, compared to GENIE, the inference speed of AR-Diffusion can be accelerated by up to \(600\).

### Analysis

#### Diversity of Samples

Diversity is a key advantage of diffusion models. To measure the diversity of generated samples, We adopt the SELF-BLEU (Zhu et al., 2018) metric, in which a lower score indicates higher diversity. In Lin et al. (2023), various sampling methods were applied to the pre-trained auto-regressive model BART13. As shown in Table 6, AR-Diffusion achieves significantly higher diversity compared to the auto-regressive model. Furthermore, the diversity can be comparable to GENIE with a better performance.

Ablation StudyTo demonstrate the effectiveness of our proposed method, we perform ablation experiments on the XSum dataset. Our results show that both our proposed multi-level diffusion and skipping mechanism are essential for achieving the high performance of AR-Diffusion.

Maintaining the skipping inference method, we remove the token-level diffusion during the training process, which degenerates to GENIE w/ skipping. The comparison results are shown in Figure 2(a). It can be observed that after removing, the AVG-ROUGE score is greatly lower after 2 steps.

The performance of applying our proposed skipping mechanism and DDIM (Song et al., 2021) to AR-Diffusion is shown in Figure 2(b). The results demonstrate that the skipping mechanism consistently outperforms DDIM in various inference steps. Additionally, the skipping mechanism can be easily applied to GENIE. As depicted in Figure 2(c), DDIM suffers a significant drop in performance when the number of inference steps is less than 40. In contrast, the skipping mechanism consistently maintains good performance across all inference steps.

    &  &  &  \\   & Greedy & Beam & Diverse & Typical & Top-k & Nucleus &  \\  & Search & Search & Beam Search & Sample & Sample & Sample & & \\  SELF-BLEU \(\) & 100.0 & 93.4 & 75.6 & 76.9 & 80.2 & 79.1 & 29.3 & 30.4 \\   

Table 6: Diversity of **10** generated samples on XSum test set and average of **10** results. The results of BART and GENIE are quoted from Lin et al. (2023).

Figure 2: Ablation experiments on XSum test set and taking \(k\) = 5. The horizontal axis is the number of inference steps and the vertical axis is AVG-ROUGE = (ROUGE-1 + ROUGE-2 + ROUGE-L) / 3.

Case StudyBy mapping the state to the token with the highest logits, we visualize the intermediate states of AR-Diffusion. As depicted in Figure 3, AR-Diffusion undergoes a denoising process, transforming the random Gaussian noise into a coherent sentence over 20 steps, and we present 5 of them. With the progression of each timestep, compared to the tokens on the right side of the sentence, the tokens on the left side demonstrate faster determination and a rapid increase in the corresponding logits. This behavior is consistent with our principle of dynamic movement speed from left to right.

## 5 Related Work

AR and NAR Language ModelsAR models have been the dominant approach for text generation , but their token-by-token generation nature often leads to unsatisfactory inference speed. To address this issue, NAR models have been developed in recent years. The NAR method is initially proposed by Gu et al. , its objective is generate the entire output sequence in parallel, thereby improving generation speed and efficiency. Subsequently, LevT  adopts insertion and deletion to address the lack of flexibility in NAR generation, CMLM  utilizes a masked language model to improve the quality of NAR generation through a constant number of iterations, and CNAT  introduces latent variables to represent the category information of the target word to make full use of the latent representation. However, these NAR methods are hard to model inter-token position dependency and deficient in generation performance.

Continuous Text DiffusionThe application of diffusion models to continuous text space is first introduced by Li et al. . Through the embedding and rounding processes, the direct integration of continuous noise into word embeddings was accomplished. After that, more people attempt to adopt continuous text diffusion model to solve sequence-to-sequence tasks. DiffuSeq  divides the input into two parts, utilizing one part as a condition, and perturbs the other part with noise. CDCD  proposes score interpolation and time warping to allow diffusion model and Euclidean embedding to share the same loss function for training. SeqDiffuSeq , GENIE  and DINOISER  incorporate diffusion model into the encoder-decoder structure through cross-attention mechanisms.

It is important to highlight the differences between our method and both ARDMs  and TimeGrad , despite the common references to autoregression and diffusion in all these. ARDMs employ an order-agnostic technique, leveraging masking and prediction for generation in arbitrary orders. On the other hand, TimeGrad integrates RNN and diffusion to model the conditional distribution of future steps of multivariate time series. In contrast, our research focuses on implementing the diffusion process within a continuous embedding space, with the primary aim of generating text in a left-to-right sequence.

## 6 Conclusion

This paper introduces AR-Diffusion, which exhibits AR-like generation behavior but enables efficient parallel decoding. Embracing the inherent sequential nature of language, we propose a multi

Figure 3: The intermediate state of AR-Diffusion gradually generating real text from a standard Gaussian noise through 20 steps. The brightness of the color represents the magnitude of the logits, with darker colors indicating larger logits. More cases are shown in the supplementary materials H.

level diffusion model, consisting of sentence-level and token-level components, to assign dynamic movement speeds to tokens. Consequently, compared to those on the right, the left tokens undergo fewer denoising steps and generate earlier to subsequently influence the later ones. Furthermore, we introduce a skipping mechanism to facilitate parallel generation within the multi-level diffusion framework. The experimental results across various tasks demonstrate that AR-Diffusion surpasses existing diffusion models in terms of quality while maintaining diversity. Additionally, compared to existing diffusion language models, AR-Diffusion achieves comparable results while being \(100 600\) faster.

## 7 Acknowledgements

This research is supported by National Natural Science Foundation of China (Grant No.62276154), Research Center for Computer Network (Shenzhen) Ministry of Education, the Natural Science Foundation of Guangdong Province (Grant No. 2023A1515012914), Basic Research Fund of Shenzhen City (Grant No. JCYJ20210324120012033 and JSGG20210802154402007), the Major Key Project of PCL for Experiments and Applications (PCL2021A06), and Oversea Cooperation Research Fund of Tsinghua Shenzhen International Graduate School (HW2021008).