# Iterative Methods via Locally Evolving Set Process

Baojian Zhou\({}^{1,2}\)

Yifan Sun\({}^{3}\)

Reza Babanezhad Harikandeh\({}^{4}\)

Xingzhi Guo\({}^{3}\)

**Deqing Yang\({}^{1,2}\)**

**Yanghua Xiao\({}^{2}\)**

\({}^{1}\) the School of Data Science, Fudan University,

\({}^{2}\) Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University

\({}^{3}\) Department of Computer Science, Stony Brook University, \({}^{4}\) Samsung SAIT AI Lab.

Corresponding to: Baojian Zhou, bjzhou@fudan.edu.cn

###### Abstract

Given the damping factor \(\) and precision tolerance \(\), Andersen et al.  introduced Approximate Personalized PageRank (APPR), the _de facto local method_ for approximating the PPR vector, with runtime bounded by \((1/())\) independent of the graph size. Recently, Fountoulakis & Yang  asked whether faster local algorithms could be developed using \(}(1/())\) operations. By noticing that APPR is a local variant of Gauss-Seidel, this paper explores the question of _whether standard iterative solvers can be effectively localized_. We propose to use the _locally evolving set process_, a novel framework to characterize the algorithm locality, and demonstrate that many standard solvers can be effectively localized. Let \(}(_{t})\) and \(_{t}\) be the running average of volume and the residual ratio of active nodes \(_{t}\) during the process. We show \(}(_{t})/_{t} 1/\) and prove APPR admits a new runtime bound \(}(}(_{t})/( {}_{t}))\) mirroring the actual performance. Furthermore, when the geometric mean of residual reduction is \(()\), then there exists \(c(0,2)\) such that the local Chebyshev method has runtime \(}(}(_{t})/((2 -c)))\) without the monotonicity assumption. Numerical results confirm the efficiency of this novel framework and show up to a hundredfold speedup over corresponding standard solvers on real-world graphs.

## 1 Introduction

Personalized PageRank (PPR) vectors are key tools for graph problems such as clustering [2; 3; 30; 36; 54; 57], diffusion [10; 14; 15; 29], random walks [25; 32; 44], neural net training [7; 27; 20; 21], and many others [17; 48]. The Approximate PPR (APPR)  and its many variants [6; 9; 13; 37] efficiently approximate PPR vectors by exploring the neighbors of a specific node at each time, only requiring access to a tiny part of the graph - hence the number of operations needed is independent of graph size. These local solvers are well-suited for large-scale graphs in modern graph data analysis. Specifically, let \(\) and \(\) be the adjacency and degree matrices of a graph \(\), respectively. Given a source node \(s\) and the damping factor \((0,1)\), this paper studies local solvers for the linear system

\[(-(1-)(+^{-1})/2) =_{s},\] (1)

where \(_{s}\) is the standard basis of \(s\) and \(\) is the PPR vector [2; 12; 37]. Given the error tolerance \(\), a local solver needs to find \(}\) such that \(\|^{-1}(}-)\|_{}\) without accessing the entire graph \(\).2Andersen et al.  proposed the local APPR algorithm, which pushes large residuals to neighboring nodes until all residuals are small. Its runtime is upper bounded by \((1/())\) independent of graph size. Based on a variational characterization of Equ. (1), Fountoulakis et al.  reformulated the problem as optimizing a quadratic objective plus \(_{1}\)-regularization and later asked  whether there exists a local solver with runtime \(}(1/())\). This corresponds to an accelerated rate since \(\) is the strongly convex parameter. Recently, Martinez-Rubio et al.  provided a method based on a nested subspace pursuit strategy, and the corresponding iteration complexity is bounded by \(}(|^{*}|/)\) where \(^{*}\) is the support of the optimal solution. This bound deteriorates to \(}(n/)\) when the solution is dense, with \(n\) representing the number of nodes in \(\), which could be less favorable than that of standard solvers under similar conditions. Moreover, the nested computational structure provides a constant factor overhead, which could be significant in practice.

The bound analysis of the above local methods critically depends on the monotonicity properties of the designed algorithms. These requirements may hinder the development of simpler and faster local linear solvers that lack such monotonicity properties. Specifically, the runtime analysis of APPR relies on the non-negativity and decreasing monotonicity of residuals. Conversely, the runtime bounds developed in Fountoulakis et al.  and Martinez-Rubio et al.  depend on the monotonicity of variable updates, ensuring that the sparsity of intermediate variables increases monotonically.

**Our contributions.** Based on a refined analysis of APPR, our starting point is to demonstrate that APPR is a local variant of Gauss-Seidel Successive Overrelaxation (GS-SOR) that can be treated as an evolving set process.3 This insight leads us to explore whether standard solvers can be effectively localized to solve Equ. (1). To develop faster local methods with improved local bounds, we propose a novel _locally evolving set process_ framework inspired by its stochastic counterpart . This framework enables the development of faster local methods and circumvents the monotonicity requirement barrier in runtime complexity analysis in the existing literature. For example, our analysis of the local Chebyshev method does not depend on the monotonicity of residual or the active node sets processed. Specifically,

* As a core tool, we propose an algorithm framework based on the _locally evolving set process_. We show that APPR is a local variant of GS-SOR using this process. This framework is powerful enough to facilitate the development of new local solvers. Specifically, standard gradient descent (GD) can be effectively localized for solving this problem and admits \((1/())\) runtime bound.
* This local evolving set process provides a novel way to characterize the algorithm locality; hence, new runtime bounds can be derived. Let \(}(_{t})\) and \(_{t}\) be the running average of volume and the residual ratio of active nodes \(_{t}\) during the process; we prove the ratio \(}(_{t})/_{t}\) serving as a lower bound of \(1/\). We further show both APPR and local GD have \(}(}(_{t})/( _{t}))\) runtime bound mirroring the actual performance of these two methods.
* Using our framework, we show there exists \(c(0,2)\) such that both the localized Chebyshev and Heavy-Ball methods admit runtime bound \(}(}(_{t})/((2-c)))\) with the assumption that the geometric mean of active ratio factors is \(()\). Importantly, our analysis does not require any monotonicity property. The technical novelty is that we effectively characterize residuals of these two methods by using second-order difference equations with parameterized coefficients.
* We demonstrate, over 17 large graphs, that these localized methods can significantly accelerate their standard counterparts by a large margin. Furthermore, our proposed LocSOR, LocCH, and LocHB are significantly faster than APPR and \(_{1}\)-based solvers on two huge-scale graphs.

**Paper structure.** We begin by clarifying notations and reviewing APPR in Sec. 2. Sec. 3 introduces the locally evolving set process. Sec. 4 presents localized Chebyshev and Heavy-Ball methods along with our novel techniques. We discuss open questions in Sec. 5. Experiments and conclusions are covered in Sec. 6 and 7, respectively. _Detailed related works and all missing proofs are included in the Appendix. Our code is available at https://github.com/baojian/LocalCH_.

## 2 Notations and Preliminaries

**Notations.** We consider an undirected simple graph \((,)\) where \(=\{1,2,,n\}\) and \(\) with \(||=m\) are the node and edge sets, respectively. The set of neighbors of \(v\) is denoted as \((v)\). The adjacency matrix \(\) of \(\) assigns unit weight \(a_{u,v}=1\) if \((u,v)\) and 0 otherwise. The \(v\)-th entry of the degree matrix \(\) is \(d_{v}=|(v)|\). Given \(\), we define the volume of \(\) as \(()_{v}d_{v}\). The support of \(^{n}\) is the set of nonzero indices \(()\{v:x_{v} 0,v\}\). The eigendecomposition of \(^{-1/2}^{-1/2}=^{}\) where each column of \(\) is an eigenvector and \(=}(_{1},_{2},, _{n})\) with \(1=_{1}_{2}_{n}-1\).

### Revisiting Anderson's APPR and its local runtime bound

We use \((,)\) for solving Equ. (1) while use \((,)\) for solving Equ. (3) or Equ. (4). With the initial setting \(,_{s}\), APPR obtains a local estimate of \(\) denoted as \(\) by using a sequence of Push operations defined as

\[(,,s,):\;(, )(u,,,)\;\;  v,z_{v}< d_{v};\;\;\;.}\] (2)

At each repeat step \((,)(u,,,)\), it synchronously updates both \(\) and residual \(\) whenever there exists an _active_ node \(u\) (a node with a large residual, i.e., \(z_{u} d_{u}\)). Specifically, for each active \(u\), it updates \(p_{u}\), \(z_{u}\), and \(z_{v}\) for \(v(u)\) by using a Push operator illustrated on the left. It stops when no active nodes are left. APPR can be implemented locally so that the runtime is independent of \(\). In particular, \((())\) is locally bounded, demonstrating the sparsity effect. We restate the existing main results as follows.

**Lemma 2.1** (Runtime bound of APPR ).: _Given \((0,1)\) and the precision \( 1/d_{s}\) for node \(s\) with \(,_{s}\) at the initial, \((,,s,)\) defined in (2) returns an estimate \(\) of \(\). There exists a real implementation of (2) (e.g., Algo. 2) such that the runtime \(_{}\) satisfies_

\[_{}(1/()).\]

_Furthermore, the estimate \(}:=\) satisfies \(\|^{-1}(}-)\|_{}\) and \(((})) 2/((1-))\)._

The main argument for proving Lemma 2.1 is critically based on: 1) \(\) and \(\|\|_{1}\) decreases during the updates; 2) for each active \(u\), \(z_{u} d_{u}\), implying that \(\|\|_{1}\) is decreased by at least \( d_{u}\), consecutively leading to \(_{u}d_{u} 1/()\).

### Problem reformulation

To approximate the PPR vector \(\), the original linear system in Equ. (1) can be reformulated as an equivalent symmetric version defined as

\[=,-^{-1/2}^{-1/2}^{-1/2}_{s},\] (3)

where again \(_{s}\) is the standard basis of \(s\), and \(\) is a symmetric positive-definite \(M\)-matrix with all eigenvalues in \([,]\). To solve Equ. (3) is equivalent to solving a quadratic problem

\[^{*}=*{arg\,min}_{^{n}}f( )^{}-^{}},\] (4)

where \(f\) is strongly convex with condition number \(1/\). Indeed, Equ. (3) is a symmetrized version of Equ. (1) and has a unique solution \(^{*}=^{-1}\). The PPR vector \(\) can be recovered from \(^{*}\) by \(=^{1/2}^{*}\). It is convenient to denote estimate of \(\) as \(^{(t)}^{1/2}^{(t)}\). Given \(^{(t)}\), we define the residual \(^{(t)}-^{(t)}\). If \(^{(t)}\) is returned by a local solver for solving either Equ. (3) or Equ. (4), we then equivalently require \(\|^{-1/2}(^{(t)}-^{*})\|_{}\). Hence, it is enough to have a stop condition \(\|^{-1/2}^{(t)}\|_{} 2/(1+)\) for local solvers of Equ. (3) and Equ. (4).4

Fountoulakis et al.  demonstrated that APPR is equivalent to a coordinate descent solver for minimizing \(f\) in Equ. (4) and introduced an ISTA-style solver by minimizing \(f()+\|^{1/2}\|_{1}\), which provides a method with runtime bound \(}(1/())\) for achieving the same estimation guarantee of APPR. On one hand, one may note that the runtime bound \((1/())\) provided in Lemma 2.1 becomes less valuable when \( 1/m\); on the other hand, all previous local variants of APPR are critically based on some monotonicity property. This limitation could impede the development of faster local methods that might violate the monotonicity assumption. The following two sections present the techniques and tools to address these challenges.

## 3 Local Methods via Evolving Set Process

Our investigation begins with the _locally evolving set process_, as inspired by the stochastic counterpart . The process reveals that APPR is essentially a local variant of GS-SOR. We then show how to use this process to build faster local solvers based on GS-SOR. We further develop a _local parallelizable_ gradient descent with runtime \((1/())\).

### Locally evolving set process

Given \(,,s,\) and \(\), a local solver for Equ. (3) keeps track of an _active set_\(_{t}\) at each iteration \(t\). That is, only nodes in \(_{t}\) are used to update \(\) or \(\). The next set \(_{t+1}\) is determined by current \(_{t}\) and an associated local solver \(\). We define this process as the following local evolving set system.

**Definition 3.1** (Locally evolving set process).: Given a parameter configuration \((,,s,)\), and a local iterative method \(\), the locally evolving set process generates a sequence of \((_{t},^{(t)},^{(t)})\) representing as the following dynamic system

\[(_{t+1},^{(t+1)},^{(t+1)})=_{ }(_{t},^{(t)},^{(t)},),  t 0,\] (5)

where \(_{t+1}_{t}(_{u_{t}} (u))\) and we denote the active set \(_{t}=\{u_{1},u_{2},,u_{|_{t}|}\}\). The set \(_{t}\) is maintained via a queue data structure. We say this process _converges_ when the last set \(_{T}=\) if there exists such \(T\); the generated sequence of active nodes are

\[(_{0},^{(0)},^{(0)})\ \ (_{1},^{(1)},^{(1)}) \ \ (_{2},^{(2)},^{(2)})\ \ \ \ (_{T}=,^{(T)},^{(T)}).\]

The runtime of the local solver, \(\) for this whole local process, is then defined as 5

\[_{}_{t=0}^{T-1}( _{t}).\]

The framework of this set process provides a new way to design local methods. Furthermore, it helps to analyze the convergence and runtime bound of local solvers by characterizing the sequences \(\{(_{t})\}\), and \(\{\|^{(t)}\|\}\) generated by \(_{}\). To analyze a new runtime bound, for \(T 1\), we define the average of the volume of active node sets \(\{(_{t})\}\) and active ratio sequence \(\{_{t}\}\) as

\[}(_{T})_{t=0}^ {T-1}(_{t}),_{T} _{t=0}^{T-1}\{_{t}^{| _{t}|}}r_{u_{i}}^{(t+_{i})}}|}{\|^{1/2} {r}^{(t)}\|_{1}}\},\] (6)

where \(_{i}\) is a smaller time magnitude. We define \(_{i}=(i-1)/|_{t}|\) for the analysis of APPR and LocSOR while \(_{i}=0\) for LocGD in our later analysis. In the rest, we denote \(_{T}=(^{(T)})\).

These two metrics \((_{T})\) and \(_{T}\) characterize the locality of local methods. To demonstrate this local process, Fig. 1 shows \((_{t})\) of APPR peaks at the early stage, and the active ratio decreases as the active volume diminishes. The quantity \(}(_{T})/_{T}\) is strictly smaller than \(1/\), indicating that it could serve as a better factor in the runtime analysis.

Figure 1: Runtime of APPR in the locally evolving set process on the _com-dblp_ graph with \(s=0,=0.1\), and \(=1/m\). The red region of the left figure is \(_{}\). The right two figures show active ratios and \(}(_{T})/_{T} 1/\).

### APPR via locally evolving set process

We first demonstrate how this locally evolving set process can represent APPR. For solving Equ. (1), the set \(_{0}=\{s\}\) and the queue-based of APPR (see Algo. 2 in Appendix A) naturally forms a sequence of active sets from \(_{0}=\{s\}\) to \(_{T}=\), hence converging. Active nodes \(u\) in queue satisfy \(z_{u} d_{u}\). To delineate successive iterations \(_{t}\) and \(_{t+1}\), one can insert \(*\) at the beginning of \(_{t}\). After processing \(_{t}\), it serves as an indicator for the next iteration. The star \(*\) is reinserted into the queue iteratively until the queue is empty. We use a slightly different notation for presenting tuple \((_{t},^{(t)},^{(t)})\) to consistent with Sec. 2.1 and write out such evolving process as follows

\[_{}(_{t},^{(t) },^{(t)},=):\ \ u_{i}\ \ _{t}:=\{u_{1},u_{2},,u_{|_{t}|}\}\ \\ ^{(t+_{i+1})}^{(t+_{i})}+ z_{u_{i}}^{ (t+_{i})}_{u_{i}},_{i}:=(i-1)/|_{t}|\\ ^{(t+_{i+1})}^{(t+_{i})}-z_{u_{i}}^{(t+_{i})}_{u_{i}}+z_{u_{i}}^{(t+ _{i})}^{-1}_{u_{i}}\] (7)

The following lemma establishes the equivalence between APPR and the local variant of GS-SOR method (see Appendix B.1) and provides a new evolving-based bound.

**Lemma 3.2** (New local evolving-based bound for APPR).: _Let \(=^{-1}-(+^{- 1})\) and \(=_{s}\). The linear system \(=\) is equivalent to Equ. (1). Given \(^{(0)}=\), \(^{(0)}=_{s}\) with \((0,2)\), the local variant of GS-SOR (15) for \(=\) can be formulated as_

\[^{(t+_{i+1})}^{(t+_{i})}+}^{(t+_{i})}}{M_{u_{i}u_{i}}}_{u_{i}},^{(t+_{i +1})}^{(t+_{i})}-}^{(t+_{i})}} {M_{u_{i}u_{i}}}_{u_{i}},\]

_where \(u_{i}\) is an active node in \(_{t}\) satisfying \(z_{u_{i}} d_{u_{i}}\) and \(_{i}=(i-1)/|_{t}|\). Furthermore, when \(=\), this method reduces to APPR given in (7), and there exists a real implementation (Aglo. 2) of APPR such that the runtime \(_{}\) is bounded, that is_

\[_{}}(_{T})}{_{T}}}{},\ where\ }(_{T})}{_{T}} ,\ C_{T}=_{T}|},_ {T}_{t=0}^{T-1}\{^{|_ {t}|}|z_{u_{i}}^{(t+_{i})}|}{\|^{(t)}\|_{1}}\}.\]

### Faster local variant of GS-SOR

Lemma 3.2 points to the sub-optimality of APPR, as GS-SOR allows for a larger \(\). For solving Equ. (3), since APPR essentially serves as a local variant of GS-SOR, we can develop a faster local variant based SOR. To extend this method to solve Equ. (3), we propose a local GS-SOR based on an evolving set process, namely LocSOR, as the following

\[_{}(_{t},^{(t)}, ^{(t)},=):\ \ u_{i}\ \ _{t}:=\{u_{1},,u_{|_{t}|}\}\ \ \ \\ ^{(t+_{i+1})}^{(t+_{i})}+ r_{u_{i}}^{ (t+_{i})}_{u_{i}},_{i}=(i-1)/|_{t}|\\ ^{(t+_{i+1})}^{(t+_{i})}- r_{u_{i}}^{ (t+_{i})}_{u_{i}}+r_{u_{i}}^{(t+ _{i})}^{-1/2}^{-1/2}_{u_{i}}\] (8)

When \((0,1]\), the residual \(\) is still nonnegative and monotonically decreasing, we establish the convergence of LocSOR stated in the following theorem.

**Theorem 3.3** (Runtime bound of LocSOR \((=1)\)).: _Given the configuration \(=(,,s,)\) with \((0,1)\) and \( 1/d_{s}\) and let \(^{(T)}\) and \(^{(T)}\) be returned by LocSOR defined in (8) for solving Equ. (3). There exists a real implementation of (8) such that the runtime \(_{}\) is bounded by_

\[}(_{T})}{ _{T}}(1-^{1/2}^{(T)}\|_{1}}{\| ^{1/2}^{(0)}\|_{1}})_{} \{,}(_{T})}{_{T}}\}\]

_where \(}(_{T})\) and \(_{T}\) are defined in (6) and \(C=_{T}|}\) with \(_{T}=(^{(T)})\). Furthermore, \(}(_{T})/_{T} 1/\) and the local estimate \(}:=^{1/2}^{(T)}\) satisfies \(\|^{-1}(}-)\|_{}\)._Our new evolving bound \((}(_{T})/(_{T}))\) mirroring the actual performance of APPR and empirically much smaller than \((1/())\) as illustrated in Fig. 2. Our lower bounds are quite effective when \(\) is relatively large, while our upper bound is better than Anderson's when \(\) is small. When \((1/m)\), this new bound is superior to both \((1/())\) and \(}(1/())\). This superiority is evident when compared to algorithms like ISTA or FISTA  to minimize the \(_{1}\)-regularization of \(f\) for obtaining an approximate solution of Equ. (3). Additionally, when \((1,2)\) and recalling that \(\) is an \(M\)-matrix, the standard analysis of SOR shows that the spectral norm of the iteration matrix must be larger than \(|-1|\). Hence, \(0<<2\) if and only if global SOR converges . When \(^{*}\) is optimal (the point that the spectral radius of the iteration matrix is minimized), we have the following result.

**Corollary 3.4**.: _Let \(=^{*} 2/(1+/(1+)^{2}})\) and \(_{t}=, t 0\) during the updates, the global version of LocSOR has the following convergence bound_

\[\|^{(t)}\|_{2}}}(}{1+}+_{t})^{t},\]

_where \(_{t}\) are small positive numbers with \(_{t}_{t}=0\)._

Asymptotically, when \(_{t}=o()\), then the runtime of global LocSOR is \(}(m/)\) where \(}\) hides \( 1/\). The main difficulty of analyzing the _optimal_ local LocSOR is that the nonnegativity and monotonicity of \(^{(t)}\) do not hold. Instead, by using a parameterized second-order difference equation, we develop new techniques based on the Chebyshev method detailed in Sec. 4.

### Parallelizable local gradient descent

One disadvantage of LocSOR is its limited potential for parallelization. The standard GD \(^{(t+1)}=^{(t)}-f(^{(t)})\) (step size = 1), in contrast, is easy to parallelize across the coordinates of the update. Instead of updating \(\) and \(\) synchronously per-coordinate, we propose the following

\[_{}(_{t},^{(t)},^{(t)}, =):\;^{(t+1)}^{(t)}+ ^{(t)}_{_{t}},\;^{(t+1)}^{(t)}- ^{(t)}_{_{t}}}\] (9)

Every coordinate in \(_{t}\) is updated in parallel at iteration \(t\). Interestingly, LocGD exhibits nonnegativity and monotonicity properties, and its runtime complexity is similar to that of LocSOR, as stated in the following theorem (To remind, \(_{i}=0\) for \(_{T}\) of LocGD in Equ. (6) ).

**Theorem 3.5** (Runtime bound of LocGD).: _Given the configuration \(=(,,s,)\) with \((0,1)\) and \( 1/d_{s}\) and let \(^{(T)}\) and \(^{(T)}\) be returned by LocGD defined in (9) for solving Equ. (4). There exists a real implementation of (9) such that the runtime \(_{}\) is bounded by_

\[}(_{T})}{ _{T}}(1-^{1/2}^{(T)}\|_{1}}{\|^ {1/2}^{(0)}\|_{1}})_{}\{, }(_{T})}{_{T}}\},\]

_where \(C=(1+)/((1-)|_{T}|),_{T}=(^{(T)})\). Furthermore, \(}(_{T})/_{T} 1/\) and the estimate \(}:=^{1/2}^{(T)}\) satisfies \(\|^{-1}(}-)\|_{}\)._

Note that \(_{T}\) of LocGD is empirically smaller than that of LocSOR. Hence, LocGD is empirically slower than LocSOR by only a small constant factor (e.g., twice as slow), a finding consistent with observations of their standard counterparts . Nonetheless, LocGD is much simpler and more amenable to parallelization on platforms such as GPUs compared to APPR.

## 4 Accelerated Local Iterative Methods

This section presents our key contributions where we propose faster local methods based on the Chebyshev method for solving Equ. (3) and the Heavy-Ball (HB) method for Equ. (4).

Figure 2: Comparison of runtime between APPR and LocSOR (left) and runtime bounds (right) as a function of \(\). We used the same setting as in Fig. 1.

### Local Chebyshev method

Compared with GS and GD, the standard Chebyshev method offers optimal acceleration in solving Equ. (3). Following existing techniques (e.g., see d'Aspremont et al. ), we show there exists an upper runtime bound \(}(m/)\) to meeting the stopping condition where \(}\) hides \( 1/\) (we presented it in Theorem C.6). Hence, the Chebyshev method is one of the optimal first-order linear solvers for solving Equ. (3). However, localizing Chebyshev poses greater challenges due to the additional momentum vector involved in updating \(^{(t)}\). Our key observation is that _if a substantial reduction in the magnitudes of \(^{(t)}\) is required within a subset of \(_{t}\), then the corresponding momentum coordinates are likely to possess significant acceleration energy_. Intuitively, a viable strategy involves localizing both the residual and momentum vectors. For \(t 1\), denote the "momentum" vector as \(^{(t)}:=^{(t)}-^{(t-1)}\) and \(_{t:t+1}=_{t}_{t+1}\), we propose the localized Chebyshev as the following

\[ _{}(_{t},^ {(t)},^{(t)},=):\\ }^{(t)}(1+_{t:t+1})^{(t)}_{ _{t}}+_{t:t+1}^{(t)}_{_{t}}, _{t+1}=(2-_{t})^{-1}\\ ^{(t+1)}^{(t)}+}^{(t)}, {r}^{(t+1)}^{(t)}-}^{(t)}+ }^{(t)},}\] (10)

where \(t 1\) with the initials \(^{(0)}=,^{(1)}=^{(0)}\), \(_{0}=0,_{1}=(1-)/(1+)\), and \(=^{-1/2}^{-1/2}\) is normalized adjacency matrix. Our key strategy for analyzing (10) is to rewrite the updates of \(^{(t)}\) as a nonhomogeneous second-order difference equation (see details in Lemma C.8)

\[^{(t+1)}-2_{t+1}^{(t)}+_{t:t+1}^{(t-1)}= _{j=0}^{t}((1+_{j:j+1})_{r=j+1}^{t}_{r:r+1} ^{(j)}_{_{j,t}}),\] (11)

where we denote \(_{j,t}=_{j}_{t-1} }_{t}\) given \(t j 0\) where \(}_{t}=_{t}\). In the rest, we define \(=(1-)/(1+)\) and recall the eigendecomposition of \(^{-1/2}^{-1/2}=^{}\). Based on the above Equ. (11), we have the following key lemma.

**Lemma 4.1**.: _Given \(t 1,^{(0)}=\), \(^{(1)}=^{(0)}\). The residual \(^{(t)}\) of LocCH defined in (10) can be expressed as the following_

\[^{}^{(t)}=_{1:t}_{t}^{}^{(0)}+ _{1:t}t_{0,t}+2_{k=1}^{t-1}_{k+1:t}(t-k)_{k,t},\]

_where \(_{t}\) is a diagonal matrix such that \(\|_{t}\|_{2} 1\) and_

\[_{k,t}=_{j=1}^{t-1}}{t}_{j,t} (-)^{}^{ (0)}_{_{0,j}}&k=0\\ _{j=k}^{t-1}}{(t-k)}_{j,t}(-)^{}^{(k)}_{_{k,j }}&k 1,\]

_where \(_{k,t}\) is a diagonal matrix such that \(\|_{k,t}\|_{2} t-k\)._

This key lemma essentially captures the process of residual reduction \(^{(t)}\) of LocCH. Specifically, given current iteration \(t\), we define the _running residual reduction rate_ for \(^{(k)}\) with \(k=0,1,2,,t-1\) of step \(t\) as \(_{k,t}\), that is,

\[_{k,t}_{k,t}\|_{2}}{\|^{(k)}\|_{2}}, _{k}_{t}_{k,t}.\] (12)

Note that

\[_{k,t}^{(k)}_{}_{k}} \|_{2}}{(1-)\|^{(k)}\|_{2}}}_{}()}+ ^{t-1}^{j-k}(t-j)}{(1-)(t-k) -j}^{(k)}_{_{k,j}}\|_{2}}{\|^{(k)}\|_{2}}}_{ _{k}}{(1-)(1-)}},\]

where whether the last term can be even smaller depends on \(\|^{(k)}_{_{k,j}}\|_{2}\) for \(_{k,j}=_{k}_{j-1} }_{j}\). However, we notice that the running geometric mean \(_{t}(_{j=0}^{t-1}(1+_{j}))^{1/t}\) is even smaller in practice. Based on these observations and the assumption on \(_{t}\), we establish the following theorem.

**Theorem 4.2** (Runtime bound of LocCH).: _Given the configuration \(=(,,s,)\) with \((0,1)\) and \( 1/d_{s}\) and let \(^{(T)}\) and \(^{(T)}\) be returned by LocCH defined in (10) for solving Equ. (3). For \(t 1\), the residual magnitude \(\|^{(t)}\|_{2}\) has the following convergence bound_

\[\|^{(t)}\|_{2}_{1:t}_{j=0}^{t-1}(1+_{j})y_{t},\]

_where \(y_{t}\) is a sequence of positive numbers solving \(y_{t+1}-2y_{t}+y_{t-1}/((1+_{t-1})(1+_{t}))=0\) with \(y_{0}=y_{1}=\|^{(0)}\|_{2}\). Suppose the geometric mean \(_{t}(_{j=0}^{t-1}(1+_{j}))^{1/t}\) of \(_{t}\) be such that \(_{t}=1+}{1-}\) where \(c[0,2)\). There exists a real implementation of (10) such that the runtime \(_{}\) is bounded by_

\[_{}()}(_{T})}{(2-c)}}{} ).\]

Golub & Overton  considered the approximate Chebyshev method by assuming that the inexact residual is sufficiently smaller than \(\|^{(t)}\|_{2}\), where \(\) must be small enough to ensure convergence. However, this assumption is overly stringent for our case. The novelty of our analysis lies in a more elegant treatment of a parameterized second-order difference equation, allowing us to circumvent this assumption. The nested APGD(\(\)), namely ASPR proposed in Martinez-Rubio et al.  has runtime complexity \(}(|^{*}|}(^{*})/+|^{*}|( ^{*}))\) where \(^{*}\) is the optimal support of \(*{arg\,min}_{}\{f()+\|^{1/2 }\|_{1}\}\) and \(}(^{*})=*{nnz}(_{ ^{*},^{*}})\). Although it is difficult to compare our bound to this, one limitation of ASPR is that it assumes to call APGD(\(\)) \((|^{*}|)\) times to finish in the worst case. However, our iteration complexity is \(}(1/((2-c)))\). Asymptotically, \(c=o()\) (\( 0\)), our complexity is \(}(1/)\) could be better than \(}(|^{*}|/)\). Fig. 3 presents a preliminary study on ASPR, indicating that it requires more operations than APPR.

We conclude our analysis by presenting a similar result for the local Heavy Ball (HB). Note the HB method is the one when \(_{t}_{t+1}^{2}\) where \(=(1-)/(1+)\). Hence, it has similar convergence analyses as to LocCH shown in Theorem D.8. The LocHB has the following updates

\[(_{t};s,,, ,=):\\ }^{(t)}(1+^{2})_{_{t}}^{( t)}+^{2}_{_{t}}^{(t)}\\ ^{(t+1)}^{(t)}+}^{(t)},^{(t+1)} ^{(t)}-}^{(t)}+}^{(t)}.\] (13)

## 5 Generalization and Open Problems

Our framework can be applied to various local methods for large-scale linear systems. Extensions of this framework to other linear systems are detailed in Tab. 2 of Appendix E. More broadly, we consider the feasibility of local methods for solving \(=\), where \(\) is a sparse vector (\(|*{supp}()| n\)) and \(\) is a positive definite, graph-induced matrix with bounded eigenvalues. This leads us to question whether all standard iterative methods can be effectively localized, raising two key questions

1. Given a graph-induced matrix \(\) and its spectral radius \(()<1\), a standard solver \(\), and the corresponding local evolving process \(_{}(_{t},^{(t)},^{(t)}, )\), does a localized version of \(\) (over \(_{t}\)) converge and have local runtime bounds?
2. Based on current analysis, Theorem 4.2 relies on the geometric mean of residual reduction on \(\|^{(k)}\|_{2}\) being small. How feasible is acceleration within locality constraints? Specifically, a stronger bound could be established for solving Equ. (3) via LocHB and LocCH, with a graph-independent bound of \[_{}=( }(_{T})}{_{T}} ),C\]

Figure 3: Comparison of runtime between APPR and ASPR. The setting is the same as in Fig. 1. Left \(=10^{-4}\) while \(\) for right.

Additionally, this work primarily focuses on using first-order neighbors at each iteration. An area for future exploration is generalizing to higher-order neighbors to determine if this leads to faster or more efficient methodologies, which remains an open question.

## 6 Experiments

We conduct experiments over 17 graphs to solve (3) and explore the local clustering task. We address the following questions: 1) Can iterative solvers be effectively localized? 2) How does the performance of accelerated local methods compare to non-accelerated ones? 3) Can our proposed methods reduce the number of operations required for local clustering? 6

**Baselines.** We consider four baselines: 1) Conjugate Gradient Method (CGM) as a benchmark to compare local and non-local methods; 2) ISTA, the local method proposed by Fountoulakis et al. ; 3) FISTA, the momentum-based local algorithm proposed by Hu ; and 4) APPR, the classic local method proposed by Andersen et al. . All methods are implemented in Python 3.10 with the numba library .

**Efficiency of localized algorithms.** To compare local solvers to their standard counterparts, we set \(=0.1\), randomly select 50 nodes from each graph to serve as \(_{s}\) in (3), and run standard GD, SOR, HB, and CH solvers along with their local counterparts: LocGD, LocSOR, LocHB, and LocCH. We measure the efficiency by the _speedup_, defined as the ratio between the runtime of the standard and local solver. The range of \(\) is \([},10^{-4}/n]\). The results, presented in Fig. 4, clearly indicate that our design demonstrates significant speedup, especially around \(=1/n\). Remarkably, they still show better performance even when \( 10^{-4}/n\) (Fig. 5). These results suggest that local solvers are preferred over non-local ones when the precision requirement is in this range.

**Comparison with local baselines and CGM.** We next compare our three accelerated methods with four baselines. Fig. 5 presents the \(_{1}\)-estimation error in terms of the number of operations (quantified as \(t}(_{t})\)) executed. It is evident that our three solvers use significantly fewer operations compared to CGM and the other three local methods. Again, due to maintaining a nondecreasing set of active nodes, ISTA and FISTA require more operations than the locally evolving set process.

**Efficiency in terms of \(\) and huge-graph tests.** We demonstrate the performance of local solvers in terms of different \(\) ranging from \(0.005\) to \(0.25\). Interestingly, in Fig. 13, LocGD show faster convergence when \(\) is small; this may be because of the advantages of monotonicity properties, which is not present in the accelerated methods. However, in other regions of \(\), accelerated methods are faster. We also tested local solvers on two large-scale graphs where papers100M has 111M nodes and 1.6B edges while com-friendster has 65M nodes with 1.8B edges. Results are shown in Fig. 6; compared with current default local methods, it is several times faster, especially on ogbn-papers100M.

Figure 4: The speedup of local solvers as a function of \(\). The vertical line is \(=1/n\).

Figure 5: Estimation error as a function of operations required. (\(=10^{-4}/n\))

Figure 6: Performance on large-scale graphs.

**Case study on local clustering.** Following the experimental setup in Fountoulakis et al. , we consider the task of local clustering on 15 graphs. As partially demonstrated in Tab. 1, compared with APPR and FISTA, LocSOR uses the least operations and is the fastest, demonstrating the advantages of our proposed local solvers.

## 7 Limitations and Conclusion

Our proposed algorithms may have the following limitations: 1) When \(\) is small, the acceleration effect partially disappears, as observed in Fig. 13. This may be due to the limitations of global counterparts, where the residual may not decrease early; 2) Our new accelerated bound for LocCH depends on an empirically reasonable assumption of residual reduction but lacks theoretical justification.

We propose using a new locally evolving set process framework to characterize algorithm locality and demonstrate that several standard iterative solvers can be effectively localized, significantly speeding up current local solvers. Our local methods could be efficiently implemented into GPU architecture to accelerate the training of GNNs such as APPNP  and PPRGo . We also offer open problems in developing faster local methods. It is worth exploring whether subsampling active nodes stochastically or using different queue strategies (priority rather than FIFO) could help speed up the framework further. It also remains interesting to see how to design local algorithms for conjugate direction-based methods such as CGM.

   \(\) &  &  \\   & APPR & LocSOR & FISTA & APPR & LocSOR & FISTA \\  \(_{1}\) & 6.9e+05 & **6.5e+04** & 5.7e+05 & 0.127 & **0.043** & 0.093 \\ \(_{2}\) & 6.7e+05 & **8.9e+04** & 4.4e+05 & 0.362 & **0.125** & 0.308 \\ \(_{3}\) & 4.3e+05 & **3.5e+04** & 2.9e+05 & 0.069 & **0.014** & 0.042 \\ \(_{4}\) & 5.7e+05 & **7.6e+04** & 4.4e+05 & 0.357 & **0.175** & 0.229 \\ \(_{5}\) & 5.4e+05 & **9.0e+04** & 5.0e+05 & 0.072 & **0.055** & 0.084 \\   

Table 1: Operations/runtime comparison on local clustering.