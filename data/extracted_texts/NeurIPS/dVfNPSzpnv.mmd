# IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation Detection & Localization

Xiaochen Ma\({}^{1}\)

Xuekang Zhu\({}^{1}\)

Equal contribution.

**Lei Su\({}^{1}\)**

Bo Du\({}^{1}\)

Zhuohang Jiang\({}^{1}\)

Bingkui Tong\({}^{1}\)

Zeyu Lei\({}^{1,2}\)

Xinyu Yang\({}^{1}\)

Chi-Man Pun\({}^{2}\)

Jiancheng Lv\({}^{1,3}\)

Jizhe Zhou\({}^{1,3}\)

Equal contribution.Corresponding author: Jizhe Zhou (jzzhou@scu.edu.cn)

###### Abstract

A comprehensive benchmark is yet to be established in the Image Manipulation Detection & Localization (IMDL) field. The absence of such a benchmark leads to insufficient and misleading model evaluations, severely undermining the development of this field. However, the scarcity of open-sourced baseline models and inconsistent training and evaluation protocols make conducting rigorous experiments and faithful comparisons among IMDL models challenging. To address these challenges, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase. IMDL-BenCo: **i)** decomposes the IMDL framework into standardized, reusable components and revises the model construction pipeline, improving coding efficiency and customization flexibility; **ii)** fully implements or incorporates training code for state-of-the-art models to establish a comprehensive IMDL benchmark; and **iii)** conducts deep analysis based on the established benchmark and codebase, offering new insights into IMDL model architecture, dataset characteristics, and evaluation standards. Specifically, IMDL-BenCo includes common processing algorithms, 8 state-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets of standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of robustness evaluation. This benchmark and codebase represent a significant leap forward in calibrating the current progress in the IMDL field and inspiring future breakthroughs. Code is available at: [https://github.com/scu-zjz/IMDLBenCo](https://github.com/scu-zjz/IMDLBenCo).

## 1 Introduction

_"Experimentation is the ultimate arbiter of scientific truth." - Richard Feynman._

The empowered image manipulation or generation models drive the Image Manipulation Detection & Localization (IMDL) task to the forefront of information forensics and security . While the task is occasionally referred to as "forgery detection"  or "tamper detection" in literature, the consensus now favors the term IMDL  as the most apt descriptor for this study area. The scope of "manipulation" within IMDL bounds to partial image alterations that yield semantic discrepancies from the original content . It does not pertain to purely generated images (e.g., images generated from pure text) or the application of image processing techniques that introduce noise or other non-semantical changes without altering the underlying meaning of the image .

The terms "detection and localization" denote an IMDL model's dual responsibility: to conduct both image-level and pixel-level assessments. This involves a binary classification at the image level, discerning whether an input image is manipulated or authentic, and a segmentation task at the pixel level, depicting the exact manipulated areas through a mask. In short, an IMDL model shall identify semantically significant image alterations and deliver a twofold outcome: a class label and a manipulation mask.

Despite the rapid success of deep neural networks in the IMDL fields [10; 45; 11], existing models suffer from inconsistent training and evaluation protocols, supported by Tables in Appendix A.1. These inconsistencies result in incompatible and unfair comparisons, yielding insufficient and misleading experimental outcomes. Hence, establishing a unified and comprehensive benchmark is the foremost concern in the IMDL field. However, constructing this benchmark is far more than straightforward protocol unification or simply model re-training. First, the training code for most state-of-the-art (SoTA) works is not publicly available, and the source code for some SoTA works is totally unreleased . Second, IMDL models commonly incorporate diverse low-level features [43; 4; 13] and complex loss functions, requiring highly customized model architecture and decoupled pipeline design for efficient reproduction. Existing frameworks, like OpenMMLab2 and Detectron23, heavily rely on the registry mechanism and tightly coupled pipelines. This conflict leads to severe efficiency issues while reproducing IMDL models under existing frameworks and results in monolithic model architecture with extremely high coding load and low scalability. Consequently, a comprehensive IMDL benchmark is yet to be built.

To address this issue, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase. IMDL-BenCo: **i)** features a modular codebase with four components: _data loader, model zoo, training script_, and _evaluator_; the model zoo contains customizable model architecture includes _SoTA models_ and _backbone models_. The loss design is also isolated within the model zoo, while other components are standardized by the interface and are highly reusable; this approach mitigates conflicts between model customization and coding efficiency; **ii)** fully implements or incorporates training code for 8 SoTA IMDL models (See Table 1) and establishes a comprehensive benchmark with 2 sets of standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of robustness evaluation, and **iii)** conducts in-depth analysis based on the established benchmark and codebase, offering new insights into IMDL model architecture, dataset characteristics, and evaluation standards. This benchmark and codebase represent a significant leap forward in calibrating the current progress in the IMDL field and can inspire future breakthroughs.

## 2 Related Works

**IMDL Model Architectures.** The key to IMDL is identifying the artifacts created by manipulation. Artifacts are considered manifest on the low-level feature space. Therefore, almost all existing IMDL models share the "backbone network + low-level feature extractor" paradigm. For example, SPAN  and ManTra-Net  use VGG  as the backbone of their models and combine SRM  and BayarConv  filters to obtain low-level features of the image. MVSS-Net  combines a Sobel  operator, which extracts edge information, and a BayarConv on its ResNet-50  backbone to extract image noise. Detailed information about each model can be found in Table 1. Various low-level feature extractors lead to various loss functions and extremely customized model architectures. As a result, reproducing IMDL models within the existing frameworks is inefficient. The high coupling between various loss functions and training architectures also makes it extremely difficult to extend different model training frameworks. The differences between training frameworks further increase the difficulty of model reproduction. This tight coupling also severely impacts algorithm innovation and rapid iteration.

**Inconsistent Training and Evaluation Protocols.** Besides model reproducing difficulties, so far, there exist multiple strikingly different protocols for training and evaluating IMDL models. MVSS-Net, CAT-Net, and TruFor were pre-trained on the ImageNet  dataset. SPAN , PSCC-Net , CAT-Net , and TruFor  were trained using synthetic datasets. Additionally, TruFor used a large number of original images from popular photo-sharing websites Flickr and DPReview to train its Noiseprint++ Extractor. MVSS-Net  and IML-ViT  were trained on the CASIAv2 dataset. On the other hand, NCL  did not use pre-training and was trained on the NIST16  dataset. The detailed training and evaluation protocols for the models are explained in Appendix A.1. Considering the IMDL benchmark datasets are all tiny-sized (a few hundred to a few thousand images) , the substantial differences in training datasets make it inevitable that models using large training sets or pre-training will perform exceptionally well on other evaluation sets, posing a great challenge to the fairness of model performance evaluation. Besides, as shown in Table 1, most models do not fully open-source their code. Their results are hard to calibrate and can be highly misleading for new IMDL researchers.

**Exisiting IMDL Surveys and Benchmark.** Although IMDL surveys [28; 46] already noticed the protocol inconsistency and model reproducing difficulties in IMDL research, rare efforts have been devoted to addressing this issue. Existing surveys often rely on independently designed models with unique training strategies and datasets, leading to biases in reported results. Moreover, as far as we know, there is no comprehensive benchmark available to ensure fair and consistent evaluation of IMDL models. This absence of a unified benchmark leads to misleading, unfaithful model assessments and undermines the overall progress in the IMDL field.

## 3 Our Codebase

This section introduces our modular, research-oriented, and user-friendly codebase implemented with PyTorch4. As shown in Figure 1, it includes four key components: _data loader_, _model zoo_, _training script_, and _evaluator_. Our codebase strikes a balance between providing a standardized workflow for IMDL tasks and offering users extensive customization options to meet their specific needs.

### Data Loader

The Data loader primarily handles dataset arrangement, augmentation, and transformation processes.

**Dataset Management.** We provide conversion scripts for each dataset to rearrange them into a set of _JSON_ files. Subsequent training and evaluation can be carried out based on these _JSON_ files.

**Augmentations and Transformations.** Due to the need for expert annotations and substantial manual effort, IMDL datasets are often very small, making it difficult to meet the demands of increasingly larger models. Therefore, data augmentation is essential. Additionally, it is crucial to ensure that the input modalities and image shapes meet existing models' requirements. Our data loader is designed with the following sequence of transformations: 1) **IMDL-specific transforms**: Inspired by MVSSNet , we implemented naive inpainting and naive copy-move transforms, which can effectively enhance performance without extra datasets. 2) **Common transforms**: This includes typical visual transformations such as flipping, rotating, and random brightness adjustments, implemented using the Albumentations  library. 3) **Post transforms**: Some models require additional information other than RGB modality. For instance, CAT-Net  needs specific metadata unique to the JPEG format, which can be further obtained from the RGB domain from augmented images with callback functions. 4) **Shape transforms**: This includes zero-padding , cropping and resizing to ensure uniform input shapes. Additionally, the _Evaluators_ can automatically adapt to different shaping strategies to complete metric calculations.

  
**Model Name** & **Venue** & **Batchname** & **Feature Extractor** & **Repositiaries** & **Training Code** \\ 
**Mariha-Net** & CVPR19 & VGG & BusiConv-SSDM Filter & [https://github.com/Rays.dischain/Mariha-Net-gytorch](https://github.com/Rays.dischain/Mariha-Net-gytorch) & \(\) \\
**MVSSNet** & ICCV12 & ResNet-50 & BaycConv-SSDM Filter & [https://github.com/bays.dischain/category-MVSS-Net](https://github.com/bays.dischain/category-MVSS-Net) & \(\) \\
**CCL-SW-H** & ICCV12 & ImageNet & High-Pass Filter & [https://medium.com/bays.dischain/CCL-SW-Net](https://medium.com/bays.dischain/CCL-SW-Net) & \(\) \\
**Object-Format** & CVPR27 & Transformer & High-Pass Filter & [https://medium.com/bays.dischain/category-MVSS-Net](https://medium.com/bays.dischain/category-MVSS-Net) & \(\) \\
**PSC-Seq** & CVPR27 & ImageNet & Multi-Resolution Convolution Streams & [https://github.com/bays.dischain/category-MVSS-Net](https://github.com/bays.dischain/category-MVSS-Net) & \(\) \\
**NCL-H** & ECCV28 & Resus-100 & Contrastive Learning & [https://github.com/Rays.dischain/CCL-H](https://github.com/Rays.dischain/CCL-H) & \(\) \\
**Tru-H** & CVPR28 & Segformer & Contrastive Learning & [https://github.com/bays.dischain/category-MVSS-Net](https://github.com/bays.dischain/category-MVSS-Net) & \(\) \\
**IML-VIL** & AAN & Vision Transformer & [https://github.com/Samsby/Image/IML-VIL](https://github.com/Samsby/Image/IML-VIL) & \(\) \\   

Table 1: Summary of the compared IMDL models

### Model Zoo

The model zoo currently consists of 8 _SoTA models_, 6 _backbone models_ built with out-of-the-box vision backbones and 5 _feature extractor modules_ commonly used for IMDL tasks. It is important to emphasize that we aim for all models to be trained using the same _training script_ (standardization), while also being adaptable to all SoTA IMDL models (customization). As shown in Figure 1, we integrate the loss function computation within the forward function of the model. Through a unified interface, we pass in the required information, such as images and masks, and output the prediction results, the loss for backpropagation, and any losses, intermediate features, and images that need to be visualized. Therefore, for new IMDL methods, users only need to add the model scripts with loss design into the model zoo, and it will seamlessly integrate with all other components in IMDL-BenCo. By effectively reproducing current SoTA models using this framework, we demonstrate that we have successfully balanced the conflict between standardization and customization.

1) **SoTA models.** As shown in Table 1, we have faithfully reproduced 8 mainstream IMDL SoTA models, adhering to the settings from the original work. Wherever possible, we used the publicly available code, making only the necessary interface modifications. For models lacking publicly available code, we implemented them based on the settings described in their respective papers. Implementation details for each model are listed in Appendix A.3. 2) **Backbone models:** As classification and segmentation tasks, mainstream IMDL algorithms make extensive use of existing visual backbones, and the performance of these backbones also impacts the performance of the implemented algorithms. Therefore, we have adapted widely used vision backbones including ResNet , U-Net , ViT , Swin-Transformer , and SegFormer  into IMDL-BenCo as backbones. 3) **Feature extractor modules**: Currently, several standard feature extractors are widely used in IMDL tasks. We have implemented 5 mainstream feature extractors as \(nn.module\), which include discrete cosine transform (DCT), fast Fourier transform (FFT) , Sobel operator , BayarConv , and SRM filter --allowing seamless integration with our backbone models with registry mechanism for managing large-scale experiments or import directly for convenient use in subsequent research.

### Training Scripts

The _training scripts_ are the entry point for using IMDL-BenCo, integrating other _components_ to perform specific functions. It can efficiently automate tasks such as model training, metrics evaluation, visualization, GradCAM analysing , and complexity computing based on configuration files (e.g., JSON, command line, or YAML). To avoid the high coupling of training pipelines seen in other frameworks (e.g., Open MM Lab often requires modifying Python package functions to customize features), we provide a code generator that allows users to create highly customized training scripts while still leveraging IMDL-BenCo's efficient components to enhance development efficiency.

Figure 1: Overview of the paradigm for IMDL-BenCo.

### Evaluators

Evaluation metrics are crucial for assessing the performance of IMDL models. Yet, existing methods face two key issues: 1) metrics are often unclear, with terms like optimal-F1 , permute-F1 [22; 13], micro-F1 and macro-F15 used as F1 score anonymously, and 2) most open-source codes compute metrics on the CPU, resulting in slow processing speeds.

To address these problems, we developed GPU-accelerated _evaluators_ in PyTorch, integrated as standard metrics. Each _evaluator_ computes a specific metric, including image-level (detection) F1 score, AUC (area-under-curve), accuracy; and pixel-level (localization) F1 score, AUC, accuracy, and IOU (Intersection over Union). All algorithms automatically adapt to _shape transformations_ in the _data loader_, providing added convenience. We also explicitly implemented derived algorithms such as inverse-F1 and permute-F1 to evaluate their tendency for **overestimation**, as demonstrated in Section 5.3. This underscores the importance of precise and transparent metric selection in future work to ensure fair and consistent comparisons.

We experimented with 12,554 images from the CASIAv2 dataset and four NVIDIA 4090 GPUs and tested our evaluators' time efficiency with \(nn.Identity\) as the model, which incurs negligible computation time. The results, shown in Table 2, indicate that our algorithms significantly reduce metric evaluation time, providing a faster and more reliable tool for large-scale IMDL tasks.

## 4 Our Benchmark

### Benchmark Settings

**Datasets.** Our benchmark includes eight publicly available datasets frequently used in the IMDL field: CASIA, Fantastic Reality, IMD2020, NIST16, Columbia, COVERAGE, tampered COCO, tampered RAISE. Details of each dataset are shown in Appendix A.2.

**Evaluation Metrics.** Since manipulated regions are often smaller than authentic ones, the pixel-level F1 score is widely used as a suitable metric for evaluating model performance. We assess each model's pixel-level F1 score using a fixed threshold of 0.5 across two protocols. We also evaluate all models using pixel-level AUC and IOU metrics. For models with a detection head, we additionally report image-level F1 scores. Lastly, we present robustness test results for the pixel-level F1 score under conditions of Gaussian blur, Gaussian noise, and JPEG compression.

**Hardware Configurations.** The experiments are conducted on three distinct servers with two AMD EPYC 7542 CPUs and 128G RAM, and contain 4\(\)NVIDIA A40 GPUs, 6\(\)NVIDIA 3090 GPUs, and 4\(\)NVIDIA 4090 GPUs, respectively.

**Models and Hyperparameters.** Our benchmark selects eight SoTA methods in the IMDL field as the initial batch of implemented methods. The models are: Mantra-Net, MVSS-Net, CAT-Net, ObjectFormer, PSCC-Net, NCL-IML, Trufor, and IML-ViT. The details of our minor modification, settings, and hyperparameters can be found in Appendix A.3.

### Benchmark Protocols

The imbalance in scale and quality of existing public datasets has led to inconsistencies in the training and evaluation protocols of IMDL methods. This makes it difficult to achieve fair and convenient comparisons between existing methods. To this end, we select two reasonable and widely used protocols: **Protocol-MVSS**, proposed by MVSS-Net, where the model is trained only on the

    &  &  &  \\   & & **F1** & **AUC** & **ACC** & **IOU** & **F1** & **AUC** & **ACC** \\   &  & **Sklearn** & 0.0750 & 0.0733 & 0.0732 & 0.0736 & 0.0043 & 0.0043 & 0.0042 \\  & & **IMDL-Conv** (_Ours_) & 0.0026 & 0.0027 & 0.0031 & 0.0029 & 0.0029 & 0.0029 & 0.0032 \\   &  & **Sklearn** & 0.1403 & 0.1451 & 0.1407 & 0.1709 & 0.0024 & 0.0024 & 0.0213 & 0.0244 \\  & & **IMDL-BoxCos** (_Ours_) & 0.0132 & 0.0131 & 0.0137 & 0.00145 & 0.00138 & 0.00122 & 0.00129 \\   

Table 2: Evaluator Accelerate Comparison on 12,554 images (HH:MM:SS)

[MISSING_PAGE_FAIL:6]

## 5 Experiments and Analysis

Our codebase and benchmark unify testing and training protocols for IMDL models. Despite this unification, the IMDL task also retains multiple unique and critical characteristics compared to other detection or segmentation tasks, notably the reliance on the "backbone network + low-level feature extractor" paradigm, benchmark datasets with random splits, and various evaluation metrics. Accordingly, we further investigate and deeply analyze 4 widely concerned but less-explored questions in sequence, including: **1)**_Is the low-level feature extractor a must in IMDL?_ **2)**_Which backbone architecture best fits the IMDL task?_ **3)**_Do random split training and testing datasets affect the model performances?_ **4)**_What metrics mostly represent the model's practical behavior?_

Through extensive experiments, we are the first to answer the above question with evidential facts and provide new critical insights into model design, dataset cleansing, and metrics for the IMDL field.

### Low-level Feature Extractors and Backbones

As shown in Table 1, prevailing IMDL approaches heavily rely on feature extractors to detect manipulation traces. However, few articles specifically analyze the advantages of different extractors. In this section, we combine the _backbone models_ implemented in the _model zoo_ (see Section 3.2) with different _feature extractor modules_ to explore the performance of each feature extractor and their compatibility with the backbone. The complexity of each combined model is shown in Table 5.

All combined models are trained on the CASIAv2 dataset for 200 epochs with an image size of 512\(\)512. Detailed experimental settings can be found in Appendix A.7.1. They are then evaluated on four distinct datasets--CASIAv1, Columbia, NIST16, Coverage, and IMD2020--to assess their generalization capabilities. The average F1 score for each setting across the four datasets is reported in Table 6.

Experiments indicate that specific feature extractors, such as BayarConv and Sobel, can negatively impact model performance. In contrast, the ResNet model, when equipped with DCT, FFT, and SRM feature extractors, shows improved performance. The ViT model, when equipped with feature extractors, tends to underfit and may require more epochs to converge. A better feature fusion

  
**Backbone** &  &  \\  Feature Extractor & None & Bayar & DCT & FFT & Sobel & SRM & None & Bayar. & DCT & FFT & Sobel & SRM \\ Average F1 Score & 0.354 & 0.227 & 0.343 & 0.358 & 0.207 & 0.355 & 0.015 & 0.018 & 0.013 & 0.013 & 0.015 & 0.016 \\ 
**Backbone** &  &  \\  Feature Extractor & None & Bayar & DCT & FFT & Sobel & SRM & None & Bayar. & DCT & FFT & Sobel & SRM \\ Average F1 Score & 0.466 & 0.143 & 0.364 & 0.364 & 0.147 & 0.363 & 0.538 & 0.214 & 0.472 & 0.447 & 0.225 & 0.395 \\ 
**Backbone** &  &  & \\  Feature Extractor & None & Bayar & DCT & FFT & Sobel & SRM & None & Bayar. & DCT & FFT & Sobel & SRM \\ Average F1 Score & 0.295 & 0.668 & 0.228 & 0.241 & 0.042 & 0.254 & 0.213 & 0.133 & 0.151 & 0.188 & 0.128 & 0.216 \\   

Table 6: **Comparison of Generalization Performance Across Different Backbones and Feature Extractors. The Average F1 Score represents the mean F1 score across five datasets.**

  
**Protocol** & **Method** & **Size** & **COVERAGE** & **Columbia** & **NIST16** & **IMD2020** & **Average** \\   & MVSS-Net & 512\(\)512 & 0.567 & 0.648 & 0.562 & 0.745 & 0.631 \\  & PCC-Net & 2562656 & 0.553 & 0.747 & 0.521 & 0.684 & 0.621 \\  & Truf & 512\(\)512 & 0.524 & 0.799 & 0.531 & 0.538 & 0.598 \\   & MVSS-Net & 512\(\)512 & 0.666 & 0.650 & 0.585 & \(\) & 0.634 \\  & PSCC-Net & 2562656 & 0.525 & 0.722 & 0.523 & \(\) & 0.590 \\   & Truf & 512\(\)512 & 0.645 & 0.934 & 0.638 & \(\) & 0.739 \\   

Table 4: **Image-level Performance. CASIAv1 is removed because it is missing authentic images.**

  
**Backbone** & **ResNet151** & **U-Net** & **ViT-B/16** & **Swin-B** & **ViT-R/16-8** & **ViT-R/16-8-cat** & **SegFormer-B2** \\  Parameters & 48.030M & 31.038M & 89.343M & 90.515M & 60.991M & 62.368M & 25.764M \\ FLOPs & 65.144G & 197.632G & 92.026G & 88.565G & 62.975G & 124.928G & 21.562G \\   

Table 5: **Backbone parameters and floating-point operations per second (FLOPs). In ViT-B/16-8-cat, "8" refers to the first eight layers of transformer blocks, while “cat” denotes the concatenation of the feature and original image feature along the sequence dimension before input into the blocks.**method might eliminate the current issues with the ViT model. For the Swin Transformer, adding feature extractors can lead to overfitting, while the performance of the Segformer generally degrades. Detailed discussion and experiment results are detailed in Appendix A.7.2.

**Necessity for Feature Extractors.** In short, BayarConv and Sobel are not suitable for IMDL tasks. Appropriate low-level feature extractors, such as DCT, FFT, and SRM, can enhance the performance of the ResNet model. However, all feature extractors may impede convergence for ViT and its variants, cause overfitting in Swin Transformer, and lead to an overall performance decline in SegFormer. Therefore, low-level feature extractors are not necessities in IMDL.

**Backbone Fitness.** As shown in Table 6, Swin Transformer and Segformer demonstrate robust performance on the IMDL task, outperforming ResNet and ViT. The U-Net architecture is not well-suited for this task.

### Dataset Bias and Cleansing Methods

**Dataset Bias.** Through our benchmark, we find that comparing the model performance under our protocol with the model performance after fine-tuning in their paper, on every model, a significant performance decline is observed on the NIST16 dataset. However, such a huge decline does not occur on other benchmark datasets. After thorough analysis, we find the NIST16 dataset contains "extremely similar" manipulation patterns. Then, when these extremely similar images are randomly split into the training and testing sets, models can effectively locate the manipulation regions by memorizing the extremely similar training samples. We refer to this critical dataset bias as "label leakage." Figure 3 illustrates an instance of label leakage in the NIST16 dataset.

**Dataset Cleansing.** To enhance the reliability of NIST16 for evaluation, we introduce a new dataset, NIST16-C7, created by applying a filtering method based on Structural Similarity (SSIM) . This process helps eliminate overly similar images, reduce dataset bias, and prevent performance overestimation caused by label leakage. Further details for our analysis and the cleansing procedure can be found in Appendix A.7.3.

We conducted extensive benchmarking on NIST16-C, and the test results are shown in Table 7. This demonstrates that we have eliminated redundant data in the NIST16 dataset, addressing label leakage. As a result, models can now focus on learning the underlying features of manipulations.

### Evaluation Metrics Selection

**Controversial F1 Scores.** The F1 metric has multiple variations with different computation equations, such as invert-F1 and permute-F1. Invert-F1 is the value obtained by calculating F1 between the inverted prediction results and the original mask. Permute-F1 is the maximum value between original F1 and invert-F1. The formula is Permute-F1\((G,P)=\)

  
**Protocol** & **Dataset** & **MustTra-Net(48)** & **MVSS-Net** & **CAT-Net** & **ObjectParameter** & **PSCC-Net** & **NCL-IML** & **Twiler** & **IML-VIT** \\   & NIST16 & 0.104 & 0.2461 & 0.2992 & 0.1732 & 0.2141 & 0.2999 & 0.3241 & 0.331 \\  & NIST16-C & 0.06493 & 0.2048 & 0.2724 & 0.1902 & 0.217 & 0.2490 & 0.291 & 0.2687 \\   & NIST16 & 0.1837 & 0.3477 & 0.2522 & 0.2682 & 0.3689 & 0.3148 & 0.348 & 0.5013 \\  & NIST16-C & 0.1629 & 0.533 & 0.5318 & 0.2637 & 0.3476 & 0.301 & 0.344 & 0.4404 \\   

Table 7: **Pixel-level performance of the Models on NIST16 and NIST16-C.** The pixel-level F1 scores for all models on the NIST16-C dataset stay mostly the same as the original NIST16 dataset.

Figure 3: **An instance of label leakage in NIST16.** There is almost no visible difference between these images and their masks. When split into training and testing datasets, the test images are easily located by the model, indicating an overestimation of model performance.

\(((G,P),(G,P))\), where \(G\) is the ground truth and \(P\) is the predicted mask. As shown in Figure 4, when the white area of the mask is large, and there is a significant deviation between the model's prediction and the mask, the invert-F1 score is much higher than the F1 score. This metric affects the fairness of the evaluation.

Furthermore, using parameters such as "macro", "micro", and "weighted" when calculating F1 scores via the sklearn library is inappropriate, as it artificially inflates our F1 metrics, which is unjustified. We further analyze these misleading F1 metrics in Appendix A.7.4. Mixing these F1 scores anonymously would lead to significant fairness issues. In summary, we contend that using the F1 score with the "binary" parameter for calculation is more scientific and rigorous. We hope that future research will uniformly adopt this standard. Additionally, we discuss the current issue of AUC being overestimated in the Appendix A.7.5.

## 6 Conclusions

In conclusion, IMDL-BenCo marks a significant advancement in the field of Image Manipulation Detection & Localization. By offering a comprehensive benchmark and a modular codebase, IMDL-BenCo enhances coding efficiency and customization flexibility, thus facilitating rigorous experimentation and fair comparison of IMDL models. Besides, IMDL-BenCo inspires future breakthroughs by providing a unified and scalable framework for model development and evaluation. We anticipate that IMDL-BenCo will become an essential resource for researchers and practitioners, driving forward the capabilities and applications of IMDL technologies in various fields, including information forensics and security.

## 7 Author Contributions

The authors' contributions are: **Xiaochen Ma**: codebase design, the coding leader, and manuscript writing. **Xuekang Zhu**: implements _ObjectFormer_, _backbone models_, and _extractor modules_; and manuscript writing. **Lei Su**: implements _MVSS-Net_, _NCL-IML_, _ManTra-Net_, _and cleaned NIST16_; and manuscript writing. **Bo Du**: implements _PSCC-Net_, _Trufor_, and manuscript writing. **Zhuohang Jiang**: implements GPU-accelerated _evaluators_, and manuscript writing. **Bingkui Tong**: implements _CAT-Net_, Grad-CAM, and manuscript writing. **Zeyu Lei**: implements _ManTra-Net_, _SPAN_, and manuscript writing. **Xinyu Yang**: dataset debiasing, metrics analyzing, and manuscript writing. **Jiancheng Lv**: general project advising. **Chi-Man Pun**: project advising. **Jizhe Zhou**: project supervisor and manuscript writing.

## 8 Acknowledgement

This research was supported by the Sichuan Provincial Natural Science Foundation (Grant No.2024YFHZ0355), the Fundamental Research Funds for the Central Universities (Grant No.2022SCU12072 and No.YJ2021159), and the Science and Technology Development Fund, Macau SAR (Grant 0141/2023/RIA2 and 0193/2023/RIA3). The authors would like to give special thanks to _Kaiwen Feng_ for his attentive work in analyzing the macro-F1 issues and fixing bugs on the IMDL-BenCo codebase and _Dr. Wentao Feng_ for the workplace, computation power, and physical infrastructure support.

Figure 4: Controversial permuted metrics. When the model’s predictions are completely wrong, the F1 score should theoretically be 0.00.