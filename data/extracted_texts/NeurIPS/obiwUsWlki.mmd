# A Cognitive Framework for Learning Debiased and Interpretable Representations via Debiasing Global Workspace

A Cognitive Framework for Learning Debiased and Interpretable Representations via Debiasing Global Workspace

Jinyung Hong\({}^{1}\) &Eun Som Jeon\({}^{2}\) &Changhoon Kim\({}^{1}\) &Keun Hee Park\({}^{1}\)

Utkarsh Nath\({}^{1}\) &Yezhou Yang\({}^{1}\) &Pavan Turaga\({}^{3}\) &Theodore P. Pavlic\({}^{1,4}\)

\({}^{1}\)School of Computing and Augmented Intelligence

\({}^{3}\)School of Arts, Media, and Engineering

\({}^{4}\)School of Life Sciences

Arizona State University, Tempe, AZ 85281, USA

\({}^{2}\)Department of Computer Science and Engineering

Seoul National University of Science and Technology, Seoul, 01811, Korea

###### Abstract

When trained on biased datasets, Deep Neural Networks (DNNs) often make predictions based on attributes derived from features spuriously correlated with the target labels. This is especially problematic if these irrelevant features are easier for the model to learn than the truly relevant ones. Many existing approaches, called debiasing methods, have been proposed to address this issue, but they often require predefined bias labels and entail significantly increased computational complexity by incorporating extra auxiliary models. Instead, we provide an orthogonal perspective from the existing approaches, inspired by cognitive science, specifically Global Workspace Theory (GWT). Our method, _Debiasing Global Workspace_ (DGW), is a novel debiasing framework that consists of specialized modules and a shared workspace, allowing for increased modularity and improved debiasing performance. Additionally, DGW enhances the transparency of decision-making processes by visualizing which features of the inputs the model focuses on during training and inference through attention masks. We begin by proposing an instantiation of GWT for the debiasing method. We then outline the implementation of each component within DGW. At the end, we validate our method across various biased datasets, proving its effectiveness in mitigating biases and improving model performance.

## 1 Introduction

Deep Neural Networks (DNNs) have achieved remarkable advancements across various domains, such as image classification (He et al., 2019; Xie et al., 2020), generation (Wang and Gupta, 2016; Kataoka et al., 2016), and segmentation (Luo et al., 2017; Zheng et al., 2014). However, DNNs often show limited generalization capability to out-of-distribution (OOD) data and are susceptible to biases present in their training datasets (Torralba and Efros, 2011). These biases occur when irrelevant features, like background color, correlate with target labels, causing the models to rely on these features for making predictions (Geirhos et al., 2020). This reliance on biased features leads to poor performance when the model encounters new data that does not share the same biases.

Biased datasets possess many _bias-aligned_ samples, where irrelevant features correlate with the labels, and a small number of _bias-conflicting_ samples, where these features do not align with the labels. Models trained on such data indeed tend to focus on the bias-aligned samples, leading to poor generalization (Hendrycks et al., 2021, 2019).

Various debiasing methods have been proposed to prevent a network from relying on spurious correlations when trained on a biased dataset. Some methods assume that biased features are "easier" to learn than robust ones, leading to the use of auxiliary models that exploit these biased features to guide the main model's training (Nam et al., 2020; Sanh et al., 2020). Strategies such as re-weighting samples (Liu et al., 2021; Nam et al., 2020) and data augmentation (Kim et al., 2021; Lee et al., 2021) are common but often struggle with insufficiently diverse samples. Other approaches involve identifying specific biases before training (Hong and Yang, 2021; Kim et al., 2019; Li and Vasconcelos, 2019; Sagawa et al., 2019), allowing the model to ignore or correct these biases. Although effective, this requires accurate bias identification and extensive manual labeling (Bahng et al., 2020; Tartaglione et al., 2021).

In this work, we depart from the above perspectives and focus on a novel and completely different approach to implement a debiasing framework. In modern ML and AI, it has been argued that it is better to build an intelligent system from many interacting specialized modules rather than a single "monolithic" entity to deal with a broad spectrum of conditions and tasks (Goyal and Bengio, 2022; Minsky, 1988; Robbins, 2017). Toward this end, we focused on a cognitive science framework proposed to underlie perception, executive function, and consciousness: Global Workspace Theory (GWT). GWT is a crucial element of modern cognitive science that models human consciousness arising from integrating and broadcasting information across specialized, unconscious processes in the brain (Baars, 1993, 2005). Many recent studies proposing a deep-learning implementation of GWT (Bengio, 2017; Goyal et al., 2021; VanRullen and Kanai, 2021) have demonstrated their effectiveness in allowing a model to have general-purpose functionality, increased modularity, improved performance, and interpretable representation learning. This perspective is expected to be well suited for application in implementing debiasing methods.

Therefore, we propose the _Debiasing Global Workspace_ (DGW), a novel instantiation of GWT for debiasing to eliminate the negative effect of the misleading correlations. Our debiasing approach involves specialized modules (acting as the specialists in GWT) and an attention-based information bottleneck (acting as the global workspace in GWT). This allows the model to achieve straightforward, functional modularity and effective debiasing performance while providing interpretable representation by visualizing which attributes are essential for accurate predictions and which are irrelevant and likely to cause errors.

The rest of this paper is organized as follows. We begin in Section 2 with a review of related work and relevant background literature. Then, in Section 3, we propose a conceptual modification of the GWT to implement a debiasing method. This involves defining specialized modules and the shared global workspace (Section 3.1). Then, we provide a step-by-step framework for defining the essential deep-learning components of our debiasing model within an AI system (Section 3.2). In Section 4, we empirically test our method on biased datasets, including Colored MNIST, Corrupted CIFAR10, and Biased FFHQ, and demonstrate that DGW effectively separates and understands intrinsic and biased features through both performance metrics and visualizations. Finally, we conclude with a discussion of future work and limitations of our approach in Section 5.

## 2 Related Work

There have been a variety of different debiasing methods for DNNs, and there also have been several connection points between GWT and neural networks. We review these approaches here.

### Debiasing Methods

Debiasing with predefined forms of bias or specific bias labels.This method involves identifying specific biases before training (Hong and Yang, 2021; Kim et al., 2019; Li and Vasconcelos, 2019; Sagawa et al., 2019). The model then learns to ignore or correct these biases. Although effective, it depends on accurately identifying biases beforehand, which can be challenging. Another approach (Bahng et al., 2020; Tartaglione et al., 2021) uses bias labels to tag data, allowing the model to differentiate between biased and unbiased data during training. This improves learning but requires extensive manual labeling.

Debiasing using the easy-to-learn heuristic.Biases are "easier" for models to learn (Nam et al., 2020) than intrinsic features. Techniques like dynamic training schemes, re-weighting samples, and data augmentation (Geirhos et al., 2018; Lee et al., 2021; Minderer et al., 2020; Li and Vasconcelos, 2019; Lim et al., 2023) help models focus on unbiased features. However, these methods struggle with insufficient diverse samples. Complex models can learn invariant features or correct representations but are difficult to design and train (Tu et al., 2022; Zhao et al., 2020; Agarwal et al., 2020; Bahng et al., 2020; Geirhos et al., 2018; Goel et al., 2020; Kim et al., 2019; Li et al., 2020; Minderer et al., 2020; Tartaglione et al., 2021; Wang et al., 2020).

Others.SelecMix (Hwang et al., 2022) creates new training samples by mixing pairs with similar labels but different biases, or different labels but similar biases, using an auxiliary contrastive model. Although effective, this adds significant training complexity. \(^{2}\) model (Zhang et al., 2023) learns debiased representations by identifying Intermediate Attribute Samples (IAS) and using a \(\)-structured metric learning objective. However, its reliance on training dynamics to identify IASs makes it different from our approach and out of the scope of our study.

### Deep Learning and Global Workspace Theory

In neuroscience and cognitive science, there is an ongoing effort to develop theories of consciousness (ToCs) to identify the neural correlates of consciousness, as reviewed by Seth and Bayne (2022). One such theory is the Global Workspace Theory (GWT) (Baars, 1993; Dehaene and Changeux, 2011; Mashour et al., 2020), which is inspired by the 'blackboard' architecture used in artificial intelligence. In this architecture, a centralized blackboard resource facilitates information sharing among specialized processors.

Recent studies have aimed to bridge the gap between neuroscience and deep learning, focusing on practical solutions for implementing a GWT using current deep learning components while considering the equivalent brain mechanisms (Goyal and Bengio, 2022; Minsky, 1988; Robbins, 2017; Goyal et al., 2021; Hong et al., 2024). Bengio (2017) emphasized learning high-level concepts by selecting key elements through attention, forming a low-dimensional conscious state similar to language, which aids in better representation learning. Mashour et al. (2020) details GWT's implementation in neuroscience, suggesting that consciousness arises from extensive information sharing across brain regions via a central network of neurons.

Inspired by GWT, our Debiasing Global Workspace (DGW) framework manages intrinsic and biased attributes in neural networks. DGW integrates information from intrinsic and bias specialists, ensuring disentangled representations are considered in decision making. Unlike prior works focusing on monolithic architecture or general-purpose learning, our approach uniquely applies these theories to debiasing neural networks.

## 3 Method

We propose the Debiasing Global Workspace (DGW), an instantiation of GWT for debiasing. DGW learns the composition of attributes in a dataset and provides interpretable explanations for the model's decisions. We introduce the conceptual framework of GWT for debiasing first (Section 3.1), its implementation in a deep learning framework next (Section 3.2), and the training objectives last (Section 3.3).

### The Conceptual Instantiation of Debiasing Global Workspace

Figure 1 depicts a conceptual overview of our proposed DGW framework. The conceptual flow of the DGW proceeds through a sequence of steps that we describe in detail here.

Step 0.To learn disentangled representations of intrinsic and biased attributes, we introduce two specialists: intrinsic \(^{i}\) and biased \(^{b}\). In the original GWT, the specialists connect to the global workspace before any stimulus appears, coupling their latent spaces bidirectionally with the workspace. We modify this setup to control the connections to backpropagate different information to two specialists separately (black and red connections between specialists and DGW in Step 0 in Fig. 1). Specifically, the intrinsic and bias specialists function identically in the forward pass. However, during the backpropagation stage, only the intrinsic attribute encoder updates its parameters and learns, while the bias attribute encoder remains frozen and does not undergo parameter updates when the task of the model is to find intrinsic attributes.

Step 1.The DGW acts as an independent and intermediate shared latent space trained to perform unsupervised neural translation between the \(C\) latent spaces from the specialized modules. The translation system is optimized to ensure that successive translation and back translation (e.g., a cycle from A to B, then back to A) return the original input (Goyal et al., 2021; VanRullen and Kanai, 2021). We implement specific operations to mimic the translation system by leveraging the residual operations (He et al., 2016) and a variant of mixup (Verma et al., 2019).

Posner (1994) argues that attention determines what information is consciously perceived and what is discarded in brains. In GWT, attention selects the information that enters the workspace. When a specific module is connected to the workspace through attention, its latent space activation vector is copied into the DGW. This internal copy serves as a bidirectional connection interface between the corresponding module and the DGW.

When a new stimulus, such as the digit "zero," appears, its latent activity transfers to the corresponding internal copy inside the workspace, initiating a broadcast to all other domains. This shared latent space (\(^{i}_{}\) in Fig. 1) uses translations and back translations from all modules to compute and train via error backpropagation. We introduce a recurrent, top-down pathway, and it can sometimes be considered as a key to account for the global ignition property observed in the brain when an input reaches consciousness, and the corresponding module is mobilized into the conscious global workspace (VanRullen and Kanai, 2021).

Figure 1: The conceptual framework of Debiasing Global Workspace (DGW). (a) Section 3.1: When attention selects inputs from specialists (Step 0), its latent space activation is copied into the DGW and immediately translated into representations suitable for each module (Step 1). We control which module is mobilized into the workspace to receive and process the corresponding data effectively. For example, upon recognizing the digit “zero,” the corresponding classifiers are activated in the workspace. The classifier \(^{i}\) is initiated for intrinsic attributes (Step 2-1), and the classifier \(^{b}\) is activated for learning bias attributes (Step 2-2). (b) Broadcast in Section 3.2: The information broadcast in DGW can demonstrate interpretable representation for attribute learning. (c) Section 3.3: Unlike the original GWT, where task definitions can be preset in Step 0, we address them using our training objectives using relative attribution score. The generic figure is inspired by (VanRullen and Kanai, 2021, Fig. 3)

Step 2.The incoming information is then immediately broadcast and translated (via the shared latent space) into the latent space of all other modules. In GWT, this translation process is automatic. However, we modify this to manually enforce learning of intrinsic and biased attribute representation with different loss functions. Specifically, we enforce the classifier \(^{i}\) to learn intrinsic attributes through error backpropagation from specific training objectives (Step 2-1 in Fig. 1). Step 2-2 simultaneously enforces the connection to the classifier \(^{b}\) and limits backpropagation to the intrinsic specialist \(^{i}\) to learn the bias attribute representations.

### Roadmap to Implement Debiasing Global Workspace

Here, we present our deep-learning-based implementation of the DGW. It combines and organizes existing components for effective debiasing frameworks in a way that is consistent with the cognitive-science-inspired DGW framework described above.

Two specialized modules and the shared workspace.DGW uses two independent specialists, the intrinsic attribute encoder \(^{i}\) and the bias attribute encoder \(^{b}\). From these, we derive concatenated features \(=[^{i}();^{b}()] ^{L D}\). To connect specialists and the shared workspace, we define \(\{^{i},^{b}\}\), where \(^{i}=[^{i}();(^{b}())]\) and \(^{b}\) is vice versa, with \(()\) as the stop-gradient operator. We introduce the Global Latent Attention (GLA) module, which acts as a shared workspace that encourages the synchronization among the input feature vector \(\) via a latent feature representation \(_{}\).

Latent-slot binding specific to each input.The GLA module uses a set number of latent embeddings or latent slots \(C\). These latent slots represent the learnable embedding vectors in the DGW, and perform competitive attention (Vaswani et al., 2017) on the input features \(\). We define \(_{}\{_{}^{i},_{ }^{b}\}^{C D}\) where \(C^{i}\) is number of slots for intrinsic features and \(C^{b}\) for bias features, with \(C=C^{i}+C^{b}\). The attention mechanism is such that:

\[(,_{})=( ) q(_{})^{}}{} )^{C L},\] (1)

where, \(k,q\) are linear projection matrices, and the softmax function normalizes the slots, creating competition among them. The slots are refined iteratively using the following:

\[_{}^{(n+1)}=(_{}^{(n)},((,_{}^{(n)})^{}) v()),\] (2)

where, \(_{}^{(n)}\) is the latent slot representation after \(n\) iterations, \(\)(Cho et al., 2014) is a recurrent neural network, and \(v\) is another liner projection matrix. The initial slots \(_{}^{(0)}\) are initialized with learnable queries following (Jia et al., 2022).

The above computations can be considered to implement a shared global workspace (Goyal et al., 2021; Hong et al., 2024) as they enable different parts of the model to compete for attention, integrating and broadcasting information similar to GWT.

Broadcast updated information to specialists.Specialists update their states using information from the shared workspace. The inverted cross-attention mechanism allows specialists to query and interact with updated latent slots \(_{}^{(n+1)}\), updating their states through:

\[}=((_{}^{(n+1)},) v(_{}^{(n+1)}))^{L D},\] (3)

where \(v\) is a linear projection matrix. Here, as the meaning of information broadcast, \(\) can be instantiated with various computational operations, including a residual connection (He et al., 2016). The other way of operation is a modified version of Manifold Mixup (Verma et al., 2019), which interpolates feature embeddings to capture higher-level information:

\[}=_{}(,( (_{}^{(n+1)},) v( _{}^{(n+1)}))),\]

where \(_{}(a,b)= a+(1-) b\), and \((,)\). The updated feature vector \(}\) is then fed to the classifier \(^{i}\) and \(^{b}\). We compare the performance of using residual connections versus our modified Manifold Mixup in Section 4.1.

In GWT, the information broadcast through the global workspace is a necessary and sufficient condition for conscious perception (VanRullen and Kanai, 2021). Intuitively, the attention mask \((_{}^{(n+1)},)\) can be seen as artificial phenomenal consciousness, indicating the immediate subjective experience of sensations and perceptions. These non-negative relevance scores depend on \(\) through the averaged attention weight, allowing us to show interpretable representations for intrinsic and biased attributes in our analysis (Section 4.2).

### Training Objectives

Here, we summarize the objective functions to train our framework. We have two linear classifiers \(^{i}\) and \(^{b}\) that take the updated concatenated vector \(}\) from the previous module as input to predict the target label \(y\). Our training objectives consist of: i) the relative attribute score learning phase, and ii) the attribute composition phase.

Relative attribute score learning phase.In this phase, we define two tasks within the conceptual framework for: identifying intrinsic attributes and identifying biased attributes. Without specific information about bias types, we utilize the relative difficulty score of each data sample, as proposed by Nam et al. (2020). Specifically, we train \(^{b}\), \(_{}^{b}\), and \(^{b}\) to focus on bias attributes using generalized cross entropy (GCE) (Zhang and Sabuncu, 2018), while \(^{i}\), \(_{}^{i}\) and \(^{i}\) are trained with the cross entropy (CE) loss. Samples with high CE loss from \(^{b}\) are considered bias-conflicting compared to those with low CE loss. We define the relevance score function:

\[(},y) CE(^{b}( }),y)CE(^{i}(}),y)+CE(^{b}(}),y)\,.\]

Thus, the objective function is defined using the above relative difficulty score of each data sample:

\[_{}=(},y) CE( ^{i}(}),y)+_{}GCE( ^{b}(}),y),\]

where \(_{}\) is the weight that adjusts between two loss terms. This loss function balances the learning between intrinsic and biased attributes, ensuring effective identification and separation of these attributes during the training phase.

Attribute-composition phase.We swap the disentangled latent vectors among the training sets (Lee et al., 2021). We randomly permute the intrinsic and bias features in each mini-batch, creating \(_{}=[^{i}();_{}^{b}()]\) where \(_{}^{b}()\) denotes the randomly permuted bias attributes. This process produces augmented bias-conflicting latent vectors. As similar as the definition of \(\), we define \(_{}\{_{}^{i},_{ }^{b}\}\) and generate \(}_{}\) following the same process described in eqs 1, 2 and 3. The objective function for this phase is:

\[_{}=(},y) CE( ^{i}(}_{}),y)+_{}GCE(^{b}(}_{}),),\]

where \(\) denotes target labels for permuted bias attributes \(_{}^{b}()\), and \(_{}\) is the balancing weight between two loss terms. Notice that the relevance score \((},y)\) is re-used to reduce computational complexity. This loss function implies that by swapping bias features, the model learns to handle a wider variety of bias-conflicting samples, improving its ability to generalize beyond the specific biases present in the training data. Consequently, the model becomes more robust as it learns to focus on intrinsic features while disregarding spurious correlations, resulting in better performance on unbiased data. Furthermore, augmenting the training data in this manner helps the model generalize better to new, unseen data by exposing it to a wider range of possible biases during training.

Entropy regularization.We empirically incorporate an additional regularization term on the latent slot attention mask to enhance performance:

\[_{}=H((_{}^{(n)}, ))+H((_{}^{(n)},_{})),\]

where \((_{}^{(n)},)\) and \((_{}^{(n)},_{})\) are attention masks from the last iteration of eq. 2. Minimizing entropy \(H()=H(a_{1},,a_{||})=(1/||)_{i}-a_{i} (a_{i})\) encourages the attention masks to be consistent over the input features captured by the latent slots. This regularization ensures the model's attention remains focused and interpretable across different input scenarios.

Final loss.The total loss function is a combination of the above components: \(_{}=_{}+_{} _{}+_{}_{}\). Here, \(_{}\) and \(_{}\) are weights that adjust the importance of the feature augmentation and entropy regularization, respectively. This comprehensive loss function ensures balanced training that enhances the model's ability to learn and generalize effectively while maintaining interpretability and robustness.

## 4 Experiments

Here, we present our experimental results, focusing on performance evaluation on various biased datasets (Section 4.1), interpretable analysis for attribute-centric representation learning (Section 4.2), and additional qualitative and quantitative analyses (Section 4.3).

Datasets.Following the previous work (Lee et al., 2021), we used three well-known benchmark datasets for debiasing methods to evaluate DGW's performance and interpretability:

* **Colored MNIST (C-MNIST)** and **Corrupted CIFAR10 (C-CIFAR-10)**: These synthetic datasets are designed to test model generalization on unbiased test sets by varying the ratio of bias-conflicting samples (0.5%, 1%, 2%, and 5%).
* **Bias FFQH (BFFHQ)**: This real-world dataset from FFHQ (Karras et al., 2019) contains face images annotated with age (intrinsic attribute) and gender (bias attribute). Most samples are young women and old men, creating a high correlation between age and gender. For BFFHQ, we included 0.5% bias-conflicting samples in the training set and used a bias-conflicting test set to ensure robust evaluation.

At inference time, we evaluate the models on clean data where no bias-conflicting samples exist.

### Performance Evaluation

Baselines.Our set of debiasing baselines includes six different approaches1: Vanilla network, HEX (Wang et al., 2018), EnD (Tartaglione et al., 2021), ReBias (Bahng et al., 2020), LfF (Nam et al., 2020), and LFA (Lee et al., 2021). Vanilla refers to the classification model trained only with the original cross-entropy (CE) loss without debiasing strategies. EnD leverages the explicit bias labels, such as the color labels in the C-MNIST dataset, during the training phase. HEX and ReBias assume an image's texture as a bias type, whereas LfF, LFA, and our method do not require any prior knowledge about the bias type. Furthermore, we configure a naive debiasing approach integrated with GWT implementation: V+CCT. CCT (Hong et al., 2024) proposed an instantiation of GWT applicable to implement an interpretable model. To compare our DGW, we simply configure the direction fusion of the Vanilla network with CCT as a GWT debiasing method.

[MISSING_PAGE_EMPTY:8]

no overlap with intrinsic masks (Fig. 3(c)). This complementary relationship illustrates the effective segregation of essential (intrinsic) and non-essential (biased) information.

In summary, for C-MNIST, intrinsic masks focus on digit shapes, while bias masks focus on colors. For C-CIFAR-10, intrinsic masks highlight uncorrupted parts, and bias masks cover corrupted parts. This clear separation supports the model's robustness and interpretability, ensuring decisions are based on relevant features while ignoring spurious correlations. More visualization results can be found in Appendix C.5.

### Quantitative and Qualitative Analysis

We provide additional analysis to compare our DGW (DGW+M in Table 1) method with Vanilla and LFA (Lee et al., 2021). More experimental results with different settings can be found in Appendix C.6.

t-SNE and Clustering.We measure clustering performance using t-SNE (van der Maaten and Hinton, 2008) and V-Score (Rosenberg and Hirschberg, 2007) on features from various models capturing intrinsic and bias attributes on C-MNIST. V-Score represents homogeneity and completeness, with higher values indicating better clustering. In Fig. 4, our DGW's \(^{i}\) captures intrinsic attributes effectively, resulting in tighter clusters and better separation, as indicated by the V-Score. Bias attributes are well captured by the \(^{b}\), as shown in Fig. 4(d).

Model Similarity.We visualize model similarity using Centered Kernel Alignment (CKA) (Raghu et al., 2021; Kornblith et al., 2019; Cortes et al., 2012), comparing similarities between all pairs of layers for different models. In this analysis, I and B denote \(^{i}\) and \(^{b}\). As shown in Fig. 5, Vanilla and LFA possess similar weights across many layers, while DGW shows fewer similarities in both initial and deeper layers, indicating different behavior across layers compared to baselines.

Model Reliability.We evaluate model generalizability using Expected Calibration Error (ECE) and Negative Log Likelihood (NLL) (Guo et al., 2017). ECE measures calibration error, and NLL

Figure 4: t-SNE plots for intrinsic and bias features on C-MNIST (with 0.5\(\%\) setting).

Figure 3: Visualization of \(^{i}\) and \(^{b}\) for the C-CIFAR10 dataset

assesses probabilistic quality. As shown in Table 2, DGW consistently has the lowest ECE and NLL, indicating better generalizability compared to baselines.

## 5 Conclusion

In this work, we introduced Debiasing Global Workspace (DGW), a framework designed to learn debiased representations of attributes in neural networks. By leveraging attention mechanisms inspired by the Global Workspace Theory, our method effectively differentiates between intrinsic and biased attributes, enhancing both performance and interpretability. Comprehensive evaluations across various biased datasets demonstrated that DGW improves model robustness and generalizability on biased data and provides interpretable insights into the model's decision-making process. Our approach results in tighter clusters and better model separation, indicating superior performance in both intra- and inter-classification tasks. Furthermore, DGW shows enhanced model reliability and generalizability, making it a better solution for addressing biases in real-world applications. Future work could focus on reducing this complexity, exploring the scalability of DGW to even larger and more diverse datasets, and extending the framework into a general-purpose drop-in layer to enhance robust performance across a wider range of image recognition tasks.

Limitations.We acknowledge that introducing our modules can increase training complexity, including model size and training time. This represents a trade-off between performance and decision-making transparency. Although our additional overhead is minimal, further analysis is necessary to optimize and streamline the process.