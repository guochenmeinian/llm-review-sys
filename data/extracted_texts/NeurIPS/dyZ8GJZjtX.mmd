# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

fails to utilize the full expressive power of these experts, especially when the number of experts is large, which significantly limits effectiveness and scalability of SMoE.

Our aim in this paper is to achieve denser expert activation (i.e., better utilization of "dead" experts) without increase in computational cost. To achieve this, we propose Multi-Head Mixture-of-Experts (MH-MoE). The workflow of MH-MoE is illustrated in Figure 2. Inspired by the multi-head mechanism utilized in Multi-Head Self-Attention (MHSA) block, MH-MoE splits each input token into multiple sub-tokens and distribute them to different experts. After expert processing, sub-tokens are seamlessly reintegrated into the original token form, thereby achieving denser expert activation, e.g., **90.71%** activation in Figure 1 (a), while also circumventing any additional computational burden in subsequent non-parallel layers, e.g., MHSA block. Specifically, as shown in Figure 2, when provided with a single input token, MH-MoE activates four experts by splitting it into four sub-tokens, whereas SMoE only activates a single expert.

Furthermore, we observe an interesting phenomenon: for tokens with richer semantic information, the sub-tokens split from these tokens are more likely to be allocated to distinct experts. For example, refer to the bright area in Figure 1 (b), where sub-tokens split from these patches are allocated to a greater number of different experts, facilitating the capture of semantically-rich information (e.g., the eagle in the figure). We therefore speculate that the allocation of sub-tokens to distinct experts enables MH-MoE to simultaneously focus on information from various representation spaces within different experts, ensuring a more granular understanding for subtle differences in both vision and language patterns, finally achieving better finer-grained understanding ability. See in Figure 2, sub-tokens assigned to Experts 3 and 2 capture a detailed understanding of each character's actions within an image patch, while those assigned to Experts 1 and 4 explicitly model the semantics of the false cognate 'camera'.

MH-MoE maintains following strengths: (1) **Higher experts activation & better scalability**. MH-MoE can alleviate lower expert activation problem and significantly enhance the usage of larger experts by enabling optimization of almost all of experts, allowing for more efficient scaling of model capacity. (2) **Finer-grained understanding ability**. By adaptively assigning sub-tokens to different experts based on the semantic richness of the token, MH-MoE enabling to jointly attend to information from different representation spaces at different experts, and finally achieving better finer-grained understanding ability. (3) **Seamless integration**. The implementation of MH-MoE is remarkably straightforward and decoupled from other SMoE optimization methods (e.g., GShard ), making it very easy to integrate them together to achieve better performance.

We evaluate the proposed MH-MoE on three model pre-training and fine-tuning setting: English-focused language modeling, multi-lingual language modeling and masked multi-modality modeling, across different parameter scales (300M to 7B). Extensive experimental among these three tasks demonstrate the effectiveness of MH-MoE.

Figure 1: (a) **Expert activation distribution** on XNLI  corpus, encompassing \(6\) parallel expert layers with \(32\) experts per layer. (b) **MH-MoE showcases finer-grained understanding** by distributing sub-tokens split from semantically-rich patches to more distinct experts to capture semantic information. Brighter regions indicate that sub-tokens split from this patch are distributed to a greater number of different experts.

## 2 Background

Sparse Mixture-of-Exerts (SMoE) [31; 12; 5; 7] enhances model capacity while maintaining a constant computational demand, thus achieving better performance than densely-activated models on various tasks [22; 19; 39; 28].

Different from densely-activated models, each MoE layer consists of \(N\) independent Feed-Forward Networks (FFN) \(\{f_{i}^{}\}_{i=0}^{N}\) as the experts, along with a gating function \(g()\) to model a probability distribution indicating the weights over these experts' outputs. For the hidden representation \(^{d}\) of each input token, the gating value of routing \(\) to expert \(f_{i}^{}\) is denoted as:

\[g(f_{i}^{})=(_{i} )/_{j=0}^{N}(_{j}),\] (1)

where \(_{i}\) denotes the trainable embedding of the \(i\)-th expert and \(_{i=0}^{N}g(f_{i}^{})=1\). Then, the corresponding \(k\) experts, according to the top-\(k\) gated values, are activated and the output \(\) of the MoE layer is \(=+_{i}g(f_{i}^{}) f _{i}^{}()\), where \(\) denote activated experts set and \(||=k\).

As described above, the most commonly used routing mechanism involves selecting the top-\(k\) experts from \(N\) experts, where \(k N\), e.g., \(k\) = 2 and \(N\) = 2048 in GShard . Such a routing mechanism allows the combination of data parallelism and expert parallelism. Some works [38; 21] suggest that larger values of \(k\) often contribute to better model performance. However, with the increase in the value of \(k\), training models with conventional top-\(k\) routing implementation becomes much less efficient . In this paper, we introduce MH-MoE, a simple but efficient manner to make denser expert activation without an increase in computational complexity.

## 3 Method

### Multi-Head Mixture-of-Experts

Concretely, we denote a sequence of inputs tokens by \(^{l d}\), where \(l\) is the number of tokens and \(d\) represents the length of token dimension. In MH-MoE, each parallel layer contains a set of \(N\) experts, each presented as \(\{f_{i}^{}:^{}_{i}^{ {d}{n}}\}_{i=0}^{N}\), where \(h\) denotes the number of heads (i.e., the number of sub-tokens a single token is split into), which is decoupled from the head in the multi-head self-attention layer. For clarity, we describe the operation of a single MH-MoE layer here only.

The full architecture of MH-MoE can be seen in Figure 3. First, \(\) is projected by a multi-head layer with parameter matrices \(_{}^{d d}\),

\[}=_{}^{}\] (2)

where \(}^{l d}\). After that, every token in \(}\) is split into \(h\) sub-tokens along the token dimensions, and these sub-tokens are arranged in parallel according to the original token sequence, forming a new

Figure 2: **Workflow of MH-MoE. For vision data, different heads routed to different experts try to capture different aspects of details within patches and relations between patches. For language data, different heads attend to capture the varying contexts of false cognates across different languages (e.g., Italian and English) or polysemous words within a single language.**feature space \(}^{(l h)}\) as2:

\[}=_{s}(})=[_{0}^{0},,_{h-1}^{0}}_{}}^{h} ,_{0}^{h},,_{h-1}^{i} }_{ h}}^{h}}_{ h},_{0}^{l},,_{h-1}^{l}}_{ h}}_{  h}],\] (3)

where function \(_{s}\) denotes the token splitting operation: \(^{l d}^{(l h)}\), and each sub-token is presented as \(_{j}^{i}^{}\), meaning it is the the \(j^{th}\) sub-token split from the \(i^{th}\) token. Then all these sub-tokens are fed into the gating function \(g()\). The gating value of routing a certain sub-token \(_{j}^{i}\) into the \(p^{th}\) expert is computed as

\[g(_{p}^{})=(_{j}^{i} _{p})/_{=0}^{N}(_{j}^{i} _{}),\] (4)

where \(_{p}^{}\) is the learnable embedding of the \(p^{th}\) expert. In this paper, we mainly focus on top-\(k\) routing, _i.e._, only the experts with the largest top-\(k\) routing score is activated. \(=_{k}(g(^{}))\) denote the set of activated experts and \(||=k\). Then \(_{j}^{i}\) is processed by these activated experts as following,

\[_{j}^{i}=_{j}^{i}+_{p}g(_{p}^{ })_{p}^{}(_{j}^{i} ).\] (5)

After that, all obtained \(_{j}^{i}\) are rearranged in the original order of sub-tokens and concatenated together as2:

\[=[_{0}^{0},_ {0}^{0}}^{h}}_{ h},_{0 }^{1},,_{h-1}^{i}}_{ h}}_{ h} ,_{0}^{l},,_{h-1}^{l} }_{ h}}^{h}}_{ h}],\] (6)

where \(^{(l h)}\). After that, \(\) is transformed back the into original token form by a token merging operation \(_{$}}}}}}$}}}}}}}}}\). \(^{l(l h)}^{l d}\) and then projected by a merge layer with parameter matrices \(_{}^{d d}\) to effective integration of multiple features \(_{j}^{i}\) capturing detailed information from different expert representation spaces:

\[}=_{}}}}}$}}}}$}}}}}}}()^{} _{}^{}.\] (7)

Then we get the final output \(}\) of the single MH-MoE layer.

By implementing the aforementioned operations, we effectively increase the average volume of data routed to a specific expert by a factor of \(h\) (as demonstrated in Eq. 3), thus achieving denser expert

Figure 3: **Illustration of a typical SMoE layer and the proposed MH-MoE layer**. (a) A SMoE layer consists of a router and expert networks, where the experts are sparsely activated according to dot-product token-expert routing scores. (b) MH-MoE introduces additional two MLP layers, namely the multi-head layer and merge layer to split and merge tokens, respectively.

activation. Besides, the shapes of the input and output in the MH-MoE layer remain unchanged, thus no additional computational cost is introduced in the subsequent block. Specifically, we introduce a hyperparameter \(\) to scale the inner dimensions of each expert, aiming to balance the parameters introduced by the multi-head layer and merge layer, aligning the model's parameters and computational complexity with the original SMoE. Furthermore, the allocation of sub-tokens to distinct experts within MH-MoE enables us to collectively capture semantic information from diverse feature spaces across these experts, thereby enhancing the model's ability to achieve a finer-grained understanding.

As the Pytorch-like style pseudocode of MH-MoE shown in Appendix D, MH-MoE is characterized by its overall simplicity of implementation and decoupled from other SMoE optimization strategies [21; 5], making it easy to integrate with other optimized SMoE frameworks to enhance performance.

### Training Objectives

**Load balancing loss.** To mitigate the expert load imbalance issue, given the sub-token set \(}\) (depicted in Eq. 3) and the frequency \(t_{p}\) of how many sub-tokens are routed to the \(p^{th}\) expert, we follow existing works [21; 13] to compute the load balancing loss \(_{}\) via:

\[_{}=}|}_{p=1}^{N}_ {_{j}^{i}}t_{p} g(f_{p}^{}),\] (8)

where \(N\) denotes the number of experts, \(|}|\) is the number of sub-tokens contained in \(}\). \(g(f_{p}^{})\) denotes the gating value of routing a certain sub-token \(_{j}^{i}\) into the \(p^{th}\) expert (see in Eq. 4).

**Task specific loss.** The term \(_{}\) is dependent on the particular task that MH-MoE is designed to learn. For instance, during pre-training in the English-focused Language Modeling task, we utilize the language modeling loss , whereas the model predicts the next word in a sequence.

So, the overall training objective of MH-MoE is to minimize:

\[=_{}+_{},\] (9)

where \(\) is a coefficient for load balancing.

## 4 Experiments

### Experimental Setup

**Compared Baselines.** We include two baseline models for comparison: (1) **Dense**, which represents a Transformer decoder without the incorporation of sparsely-activated parallel modules (i.e., SMoE layer). (2) **X-MoE**, which is our implementation based on the SMoE proposed by . We build MH-MoE upon X-MoE and use identical settings. Note that the all models are pre-trained using the same training data and loss (Eq. 9) as MH-MoE, and we ensure that the parameter count of MH-MoE remains consistent with or lower than that of X-MoE, ensuring a fair and equitable comparison. A detailed comparison about parameter and computational complexity can be found in Table 11.

**Pre-training Data.** We detail the pre-training data of MH-MoE in three areas: (1) English-focused experiments use the RedPajama dataset , which is an open-source pre-training dataset. (2) multi-lingual tasks follow XLM  and use the multilingual Wikipedia as training data. (3) multimodal tasks use a masked multi-modality modeling task with a large dataset of images, documents, and image-text pairs. Further details are available in the Appendix A.

**Model Architecture and Hyperparameters.** For all experiments, we use the X-MoE  as our backbone architecture to build our MH-MoE, which has shown better performance than prior SMoE models such as Switch Transformers  on cross-lingual understanding benchmarks. For English-focused Language Modeling and Multi-lingual Language Modeling, we construct Dense, X-MoE and MH-MoE using the Transformer  decoder (L = 12, H = 768, A = 12) with the GPT-43 vocabulary as the backbone architecture. The pre-training procedure takes \(14\) days on \(2\) NVIDIA DGX-2 Stations. For Masked Multi-modal Modeling, we build Dense, X-MoE and MH-MoE following the same Transformer encoder architecture as BEiT v3 . The pre-training procedure takes \(4\) days on \(2\) NVIDIA DGX-2 Stations. For all three pre-training tasks, we set the head number \(h\) = 4. More details about architecture and training hyperparameters can be found in Appendix B and C.

### Perplexity Evaluation

We examined the validation perplexity curves for all pre-trained models and pre-training tasks under two expert settings (8 experts and 32 experts). The perplexity trends are depicted in Figure 4, with the final perplexity values listed in Table 1. We can observe that as training progresses: 1) the perplexity of our MH-MoE remained lower in comparison to the compared baselines, indicating more effective learning; 2) MH-MoE achieved the lowest perplexity across three distinct experimental setups; 3) an increase in the number of experts led to a corresponding decrease in the perplexity of MH-MoE, suggesting that the model benefits from enhanced representation learning capabilities as more experts are incorporated. These results collectively demonstrate the superiority of MH-MoE in terms of learning efficiency and language representation across multiple pre-training paradigms.

### Downstream Evaluation

**English-focused Language Modeling.** We evaluate our models on a total of 9 different zero-shot benchmarks to assess their abilities across various natural language tasks like common sense reasoning, general language understanding and knowledge understanding using the LLM Evaluation Harness . As shown in Table 2, comparing X-MoE with the Dense model, X-MoE show notable improvement, indicating that SMoE models (e.g., X-MoE) benefit from the large model capacity. Overall, for all benchmarks, our MH-MoE obtains the best performance, achieving an average performance gain of 1.1 for 8 experts setting and 1.5 for 32 experts setting compared to X-MoE, demonstrating the effectiveness of our proposed MH-MoE on modeling English-focused language.

**Multi-lingual Language Modeling.** We evaluate our multi-lingual language models on the cross-lingual natural language inference (XNLI) corpus , which is the extension of the multi-genre NLI

    &  \\   & 8 Experts & 32 Experts \\   \\ Dense (without Experts) & 16.23 & 16.23 \\ X-MoE & 14.82 & 11.96 \\ MH-MoE (Ours) & **12.72** & **10.28** \\   \\ Dense (without Experts) & 8.56 & 8.56 \\ X-MoE & 7.19 & 6.02 \\ MH-MoE (Ours) & **6.26** & **5.09** \\   \\ Dense (without Experts) & 17.95 & 17.95 \\ X-MoE & 16.34 & 12.68 \\ MH-MoE (Ours) & **14.73** & **10.87** \\   

Table 1: Results of upstream perplexity evaluation. We report the validation perplexity cross two setting: 8 experts and 32 experts.

Figure 4: **Perplexity on validation dataset during the training phase reported for Dense, X-MoE and MH-MoE across three pre-training tasks.**

(MultiNLI) corpus to 14 languages. We follow the LLM Evaluation Harness pipeline and use the zero-shot setting to evaluate the multi-lingual ability. Table 3 presents the zero-shot evaluation results on XNLI task. Similarly, X-MoE benefit from the large model capacity and show notable improvement compared with Dense model. Overall, MH-MoE obtains the best performance, surpassing X-MoE by an average performance gain of 0.6 for 8 experts setting and 0.8 for 32 experts setting. Comparing MH-MoE with the X-MoE, it shows that MH-MoE models provide consistent gains on downstream tasks, demonstrating the effectiveness of our proposed MH-MoE on modeling cross-lingual natural language.

**Masked Multi-modal Modeling.** We evaluate on the widely used vision-language understanding and generation benchmarks, including visual question answering , visual reasoning  and image captioning . We report _vqa-score_ on VQAv2, accuracy for NLVR2. For COCO image captioning, we report BLEU@4 (B@4), METEOR (M), CIDEr (C), and SPICE (S). Table 4 presents the evaluation results. For VQA task, MH-MoE outperforms both Dense and X-MoE by a large margin, e.g., 4.24 and 1.69 points gain on test-dev split, respectively. For visual reasoning task, MH-MoE beats all these two baselines on both dev (1.5 points gain than X-MoE) and test-P split (1.7 points gain than X-MoE). For image captioning task, MH-MoE surpasses X-MoE by 4.2%, 10.2%, 9.4% in terms of B@4, M and S, respectively. Above results show that X-MoE exhibits enhanced visual information comprehension, which also validates the effectiveness of our proposed MH-MoE in capturing diverse semantic information within text-image pair data.

### Ablation Studies

This section presents experimental analysis to demonstrate the functionality of MH-MoE. In all comparative experiments, _we ensure parameter equality across models by adjusting the internal dimensions of the experts_.

**Number of Heads \(h\).** We conduct experiments by adjusting the number of heads (\(h\) = \(2\), \(4\), \(6\), \(8\), and \(12\)) in MH-MoE. As shown in Figure 5, we find that across all settings of \(h\), our model consistently outperforms the X-MoE. Besides, as the value of \(h\) increases, we observe an initial improvement followed by a decline in our model's performance. This leads us to hypothesize that as \(h\) initially increases, the enhancement in performance benefits from MH-MoE by activating a greater number of experts, thereby enhancing the model's effectiveness and capturing a wider range of fine-grained token information. However, as \(h\) becomes too large, the excessive subdivision of tokens may impair their original semantic content, leading to decreased model performance.

**Does the Token-Splitting-Merging Really Matters?** The key motivation of MH-MoE is to split each token into several sub-tokens and then merge the sub-tokens after processing by experts. We use two MLP layers for the splitting and merging processes. We conduct a detailed analysis to determine whether the performance improvement is due to the Token-Split-Merging (TSM) operation or the additional MLP layers.

    &  &  &  \\   & test-dev & test-std & dev & test-P & B@4 & M & C & S \\  Dense & 65.9 & 66.0 & 73.8 & 74.2 & 35.9 & 29.3 & 120.5 & 19.6 \\  X-MoE & 68.4 & 69.7 & 75.5 & 76.1 & 38.1 & 30.2 & 122.9 & 21.3 \\ MH-MoE & **70.1** & **71.4** & **77.0** & **77.8** & **39.7** & **33.1** & **124.1** & **23.0** \\   

Table 4: Results of visual question answering, visual reasoning, and image captioning tasks.

Figure 5: Comparison results for different head number \(h\).

  
**Model** & bg & de & el & en & es & fr & hi & ru & sw & th & tr & ur & vi & zh & Avg \\  Dense & 33.1 & 33.3 & 33.0 & 35.1 & 32.8 & 34.4 & 33.6 & 34.2 & 33.3 & 33.1 & 33.3 & 33.9 & 33.5 & 32.9 & 33.5 \\  X-MoE (\(N=8\)) & 33.9 & **33.4** & 33.4 & 37.3 & 33.3 & 35.9 & 34.5 & 35.0 & 33.5 & 33.6 & 33.4 & 34.2 & 33.3 & 33.2 & 34.1 \\ MH-MoE (\(N=8\)) & **34.4** & 33.2 & **33.9** & **40.1** & **34.0** & **36.4** & **34.6** & **35.2** & **33.8** & **34.4** & **33.3** & **34.7** & **34.6** & **33.5** & **34.7** \\  X-MoE (\(N=32\)) & 34.5 & 34.5 & 33.4 & 39.6 & 33.1 & 35.3 & 34.1 & 35.4 & 33.6 & 34.7 & 33.7 & 33.6 & 34.5 & 33.3 & 34.5 \\ MH-MoE (\(N=32\)) & **35.8** & **35.6** & **34.1** & **40.7** & **33.9** & **36.7** & **34.4** & **36.3** & **34.3** & **36.0** & **34.1** & **34.3** & **35.2** & **33.6** & **35.3** \\   

Table 3: Accuracy / accuracy-normalization scores on multilingual understanding tasks using the LLM Evaluation Harness . \(N\) denotes the number of experts.

The results are presented in Table 5. A comparative analysis between Dense v.s. Dense\({}_{w/}\), as well as X-MoE v.s. X-MoE\({}_{w/}\), reveals that introduction of the MLP layer does not enhance the model's performance. Similarly, when comparing MH-MoE with MH-MoE\({}_{w/o}\), it becomes evident that the inclusion of only the MLP, in the absence of the TSM, also does not yield any improvement in the model's effectiveness. The parameter quantities of the models being compared pairwise are equal.

An intriguing observation is made when comparing MH-MoE with MH-MoE\({}_{w/o}\). Introducing TSM alone, without MLP, results in a slight increase in model performance. In contrast, a significant enhancement in model performance is only achieved when both MLP and TSM are incorporated simultaneously. We hypothesize that introduction of TSM, without the integration of MLP, activates more experts, but the segmentation and merging of the model appears overly straightforward and abrupt in its execution. This limitation hinders the model's ability to meaningfully segment tokens into sub-tokens and effectively merge the diverse information gathered from different expert spaces.

We also conducted additional ablation studies to examine the **impact of varying the number of MLP layers, different splitting-merging methods** and **different balance loss**. The details of these experiments are provided in Appendix E.

## 5 Analysis

**Experts Activation.** We visualize the activation of each expert varies across parallel expert layers for X-MoE and MH-MoE at Figure 6. It can be observed that: 1) X-MoE demonstrate a more skewed distribution, wherein a significant portion of experts remain inactivated all the time. 2) Our MH-MoE achieves a denser expert activation compared to X-MoE, effectively mitigating the issue of low expert utilization. 3) As the number of heads \(h\) increases, the expert activation frequency in MH-MoE also rises.

**Scalability.** We explore the scalability for both X-MoE and MH-MoE by scaling up the number of experts from 8 to 256 (about 7B parameters). For upstream performance, as shown in Figure 7 (a), with the increase of experts, our MH-MoE could bring more gains. It is because MH-MoE could mitigate the low expert activation problem effectively. With this ability, the superiority of the large-scale SMoE model will be better exerted, thereby achieving the improvement of the upper bound

  
**Model** & MLP & TSM & **Perplexity** \\  Dense & ✗ & ✗ & 16.23 \\ Dense\({}_{w/}\) & ✓ & ✗ & 16.40 \\ X-MoE\({}_{w/}\) & ✗ & ✗ & 14.82 \\ X-MoE\({}_{w/}\) & ✓ & ✗ & 14.77 \\  MH-MoE\({}_{w/o}\) & ✓ & ✗ & 14.77 \\ MH-MoE\({}_{w/o}\) & ✗ & ✓ & 13.97 \\ MH-MoE\({}_{w}\) & ✓ & ✓ & **12.72** \\   

Table 5: Ablation studies of MH-MoE components: MLP layers and the Token-Splitting-Merging (TSM, Eq. 3 and Eq. 6) operation.

Figure 7: (a) Upstream training perplexity (\(\)) when scaling the number of experts for both X-MoE and MH-MoE. (b) Accuracy scores on the hellaswag task when scaling the number of experts for both X-MoE and MH-MoE. (c) Comparison for sub-tokens assign diversity (the number of different experts they are routed to) for P&F and Non P&F tokens.

of SMoE with more experts. Detailed validation perplexity curves for these scaling up experiments can be found in Figure 10 at Appendix F. For downstream performance shown in Figure 7 (b), for X-MoE, expert number = 64 is the upper bound, meaning that continuing to increase the number of experts will not bring performance gain. Our MH-MoE not only has a performance advantage over the X-MoE with the same number of experts, but also improves the upper bound from 64 to 256, which demonstrates the effectiveness of the scalability of our MH-MoE on downstream tasks.

**Experts Assign within Token.** We delve into a more granular analysis to validate how the multi-head mechanism aids MH-MoE in capturing diverse and intricate semantic information that is often challenging to comprehend, e.g., polysemous and false cognates words (denoted as PF tokens) in languages, and semantically-rich areas in images.

For languages data, we compute and compare the divergence levels (i.e., the number of different experts these sub-tokens are routed to) of sub-tokens split from PF tokens and Non-PF tokens. The results, presented in Figure 7 (c), clearly demonstrate that the distribution of divergence for PF tokens is significantly skewed towards the right when compared to that of Non-PF tokens. This indicates that during the MH-MoE's inference process, PF tokens route their sub-tokens to a greater number of different experts, thereby capturing diverse semantic information in contrast to Non-PF tokens for a better polysemous and false cognates word modeling. Note that we utilized the GPT-4 API  to extract polysemous words and false cognates from the XNLI  corpus, and the corresponding prompt can be found in Table 12.

For image data, we analyzed how the divergence levels of different patches evolve during the training process, as illustrated in Figure 7. Notably, as training progresses, divergence levels gradually increase in high-frequency texture regions (or regions with rich semantics), while they decrease in low-frequency texture regions. This suggests that MH-MoE tends to route tokens from complex texture areas to a wider variety of experts, thereby enhancing the finer-grained understanding of semantics in those regions. For more visualization examples, please refer to Figure 11 in Appendix G.

**Experiments on Pure Vision Tasks.** We conduct small-scale experiments on pure vision tasks to further validate the effectiveness of MH-MoE. We utilize ViT-B(ase)  as the Dense model and AdaMV-MoE  as the SMoE baseline. AdaMV-MoE is a multi-task vision SMoE model demonstrating a superior performance across various vision tasks. We build our MH-MoE upon AdaMV-MoE. The comparison results are shown in Table 6, which demonstrates that MH-MoE exhibits corresponding effectiveness and versatility in pure vision tasks.

**Model Collapses.** Concerns may arise that MH-MoE could revert to the original SMoE approach, routing all sub-tokens from the same token to a single expert. In MH-MoE's training framework, the load balancing loss \(_{}\) (Eq 8) serves as a weak constraint, treating all sub-tokens as independent entities and uniformly distributing them among experts, promoting a balanced allocation. Our observations indicate that most sub-tokens from the same token are distributed among 3-5 different experts (see Figures 7 and Figures 7 (c)).

## 6 Conclusion

In this paper, we study how we can to achieve a denser experts activation without introducing additional cost, while improving the fine-grained understanding ability. With the proposed Multi-Head Mixture-of-Experts, we can easily implement the aforementioned functionality. Furthermore, the simplicity of MH-MoE allows it to integrate with other SMoE frameworks to enhance performance easily. Extensive empirical results across three tasks demonstrate the effectiveness of MH-MoE.

    & **CLS** &  & **IS** \\   & **ACC** & **AP** & \(_{50}\) & \(_{75}\) & \(_{mask}\) \\  Dense & 70.73 & 39.81 & 58.97 & 44.46 & 36.42 \\ SMoE & 75.66 & 42.23 & 60.30 & 44.58 & 37.50 \\ MH-MoE & **77.34** & **44.45** & **63.18** & **45.85** & **38.24** \\   

Table 6: Performance across multiple pure vision tasks: classification (CLS) on ImageNet-1k, object detection (OD) and instance segmentation (IS) on COCO. The number of expert is set to 8.

    & **CLS** &  & **IS** \\   & **ACC** & **AP** & \(_{50}\) & \(_{75}\) & \(_{mask}\) \\  Dense & 70.73 & 39.81 & 58.97 & 44.46 & 36.42 \\ SMoE & 75.66 & 42.23 & 60.30 & 44.58 & 37.50 \\ MH-MoE & **77.34** & **44.45** & **63.18** & **45.85** & **38.24** \\   

Table 7: **Assign diversity of sub-tokens split from different patches** in vision data with respect to training steps (100k \(\) 200k \(\) 250k steps). Brighter regions indicate sub-tokens split from this patch are distributed to a greater number of diverse experts.