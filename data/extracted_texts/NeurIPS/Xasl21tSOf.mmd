# Provable Training for Graph Contrastive Learning

Yue Yu1, Xiao Wang2*, Mengmei Zhang1, Nian Liu1, Chuan Shi1*

1Beijing University of Posts and Telecommunications, China

2Beihang University, China

yuyue1218@bupt.edu.cn,xiao_wang@buaa.edu.cn,

{zhangmm, nianliu, shichuan}@bupt.edu.cn

Corresponding authors.

###### Abstract

Graph Contrastive Learning (GCL) has emerged as a popular training approach for learning node embeddings from augmented graphs without labels. Despite the key principle that maximizing the similarity between positive node pairs while minimizing it between negative node pairs is well established, some fundamental problems are still unclear. _Considering the complex graph structure, are some nodes consistently well-trained and following this principle even with different graph augmentations? Or are there some nodes more likely to be untrained across graph augmentations and violate the principle? How to distinguish these nodes and further guide the training of GCL?_ To answer these questions, we first present experimental evidence showing that the training of GCL is indeed imbalanced across all nodes. To address this problem, we propose the metric "node compactness", which is the lower bound of how a node follows the GCL principle related to the range of augmentations. We further derive the form of node compactness theoretically through bound propagation, which can be integrated into binary cross-entropy as a regularization. To this end, we propose the PrOvable Training (POT1) for GCL, which regularizes the training of GCL to encode node embeddings that follows the GCL principle better. Through extensive experiments on various benchmarks, POT consistently improves the existing GCL approaches, serving as a friendly plugin.

## 1 Introduction

Graph Neural Networks (GNNs) are successful in many graph-related applications, e.g., node classification [10; 20] and link prediction [33; 27]. Traditional GNNs follow a semi-supervised learning paradigm requiring task-specific high-quality labels, which are expensive to obtain . To alleviate the dependence on labels, graph contrastive learning (GCL) is proposed and has received considerable attention recently [35; 31; 25; 21; 29]. As a typical graph self-supervised learning technique, GCL shows promising results on many downstream tasks [22; 30; 23].

The learning process of GCL usually consists of the following steps: generate two graph augmentations ; feed these augmented graphs into a graph encoder to learn the node embeddings, and then optimize the whole training process based on the InfoNCE principle [15; 35; 36], where the positive and negative node pairs are selected from the two augmentations. With this principle ("GCL principle" for short), GCL learns the node embeddings where the similarities between the positive node pairs are maximized while the similarities between the negative node pairs are minimized.

However, it is well known that the real-world graph structure is complex and different nodes may have different properties [1; 2; 11; 36]. When generating augmentations and training on the complex graphstructure in GCL, it may be hard for all nodes to follow the GCL principle well enough. This naturally gives rise to the following questions: _are some of the nodes always well-trained and following the principle of GCL given different graph augmentations? Or are there some nodes more likely not to be well trained and violate the principle?_ If so, it implies that each node should not be treated equally in the training process. Specifically, GCL should pay less attention to those well-trained nodes that already satisfy the InfoNCE principle. Instead, GCL should focus more on those nodes that are sensitive to the graph augmentations and hard to be well trained. The answer to these questions can not only deepen our understanding of the learning mechanism but also improve the current GCL. We start with an experimental analysis (Section 3) and find that the training of GCL is severely imbalanced, i.e. the averaged InfoNCE loss values across the nodes have a fairly high variance, especially for the not well-trained nodes, indicating that not all the nodes follow the GCL principle well enough.

Once the weakness of GCL is identified, another two questions naturally arise: _how to distinguish these nodes given so many possible graph augmentations? How to utilize these findings to further improve the existing GCL?_ Usually, given a specific graph augmentation, it is easy to check how different nodes are trained to be. However, the result may change with respect to different graph augmentations, since the graph augmentations are proposed from different views and the graph structure can be changed in a very diverse manner. Therefore, it is technically challenging to discover the sensitivity of nodes to all these possible augmentations.

In this paper, we theoretically analyze the property of different nodes in GCL. We propose a novel concept "node compactness", measuring how worse different nodes follow the GCL principle in all possible graph augmentations. We use a bound propagation process [5; 32] to theoretically derive the node compactness, which depends on node embeddings and network parameters during training. We finally propose a novel PrOvable Training model for GCL (POT) to improve the training of GCL, which utilizes node compactness as a regularization term. The proposed POT encourages the nodes to follow the GCL principle better. Moreover, because our provable training model is not specifically designed for some graph augmentation, it can be used as a friendly plug-in to improve current different GCL methods. To conclude, our contributions are summarized as follows:

* We theoretically analyze the properties of different nodes in GCL given different graph augmentations, and discover that the training of GCL methods is severely imbalanced, i.e., not all the nodes follow the principle of GCL well enough and many nodes are always not well trained in GCL.
* We propose the concept of "node compactness" that measures how each node follows the GCL principle. We derive the node compactness as a regularization using the bound propagation method and propose PrOvable Training for GCL (POT), which improves the training process of GCL provably.
* POT is a general plug-in and can be easily combined with existing GCL methods. We evaluate our POT method on various benchmarks, well showing that POT consistently improves the current GCL baselines.

## 2 Preliminaries

Let \(G=(,)\) denote a graph, where \(\) is the set of nodes and \(\) is the set of edges, respectively. \(^{N F}\) and \(=\{0,1\}^{N N}\) are the feature matrix and the adjacency matrix. \(=diag\{d_{1},d_{2},,d_{N}\}\) is the degree matrix, where each element \(d_{i}=\|_{i}\|_{0}=_{j}a_{ij}\) denotes the degree of node \(i\), \(_{i}\) is the \(i\)-th row of \(\). Additionally, \(_{sym}=^{-1/2}(+)^{-1/2}\) is the degree normalized adjacency matrix with self-loop (also called the message-passing matrix).

GCN  is one of the most common encoders for graph data. A graph convolution layer can be described as \(^{(k)}=(_{sym}^{(k-1)}^{(k)})\), where \(^{(k)}\) and \(^{(k)}\) are the embedding and the weight matrix of the \(k\)-th layer respectively, \(^{(0)}=\), and \(\) represents the non-linear activation function. A GCN encoder \(f_{}\) with \(K\) layers takes a graph \((,)\) as input and returns the embedding \(=f_{}(,)\).

**Graph Contrastive learning (GCL)**. GCL learns the embeddings of the nodes in a self-supervised manner. We consider a popular setting as follows. First, two graph augmentations \(G_{1}=(,}_{1})\) and \(G_{2}=(,}_{2})\) are sampled from \(\), which is the set of all possible augmentations. In this paper,we focus on the augmentations to graph topology (e.g. edge dropping ). Then the embeddings \(_{1}=f_{}(,}_{1})\) and \(_{2}=f_{}(,}_{2})\) are learned with a two-layer GCN [35; 36]. InfoNCE [15; 35] is the training objective as follows:

\[l(_{1,i},\;_{2,i})=_{1,i}, _{2,i})/}}{e^{(_{1,i},_{2,i})/}+ _{j i}e^{(_{1,i},_{2,j})/}+_{l i }e^{(_{1,i},_{1,i})/}},\] (1)

where \((a,b)=s(g(a),\;g(b))\), \(s\) is a similarity measure, \(g\) is a projector, and \(\) is the temperature hyperparameter. For node \(i\), the positive pair is \((_{1,i},\;_{2,i})\), where \(_{1,i}\) and \(_{2,i}\) denote the embedding of node \(i\) in the two augmentations, \(\{(_{1,i},_{2,j}) j i\}\) and \(\{(_{1,i},_{1,l}) l i\}\) are the negative pairs. The overall loss function of the two augmentations is \(_{}(_{1},_{2})=_{ i=1}^{N}(l(_{1,i},_{2,i})+l(_{2,i},_{1,i}))\).

## 3 The Imbalanced Training of GCL: an Experimental Study

As mentioned in Section 1, due to the complex graph structure, we aim to investigate whether all the nodes are trained to follow the principle of GCL. Specifically, we design an experiment by plotting the distribution of InfoNCE loss values of the nodes. To illustrate the result of a GCL method, we sample 500 pairs of augmented graphs with the same augmentation strategy in the GCL method and obtain the average values of InfoNCE loss. We show the results of GRACE , GCA , and ProGCL  on WikiCS  in Figure 1 with histograms. The vertical red lines represent 1/4-quantile, median, and 3/4-quantile from left to right. The results imply that the nodes do not follow the GCL principle well enough in two aspects: 1) the InfoNCE loss of the nodes has a high variance; 2) It is shown that the untrained nodes (with higher InfoNCE loss values) are much further away from the median than the well-trained nodes (with lower InfoNCE loss values), indicating an imbalanced training in the untrained nodes. GCL methods should focus more on those untrained nodes. Therefore, it is highly desired that we should closely examine the property of different nodes so that the nodes can be trained as we expected in a provable way. Results on more datasets can be found in Appendix D.

## 4 Methodology

### Evaluating How the Nodes Follow the GCL Principle

In this section, we aim to define a metric that measures the extent of a node following the GCL principle. Since how the nodes follow the GCL principle relies on the complex graph structure generated by augmentation, the metric should be related to the set of all possible augmentations. Because InfoNCE loss can only measure the training of a node under two pre-determined augmentations, which does not meet the requirement, we propose a new metric as follows:

**Definition 1** (Node Compactness).: _Let \(f\) denote the encoder of Graph Contrastive Learning. The compactness of node \(i\) is defined as_

\[}_{1,i}=_{G_{1}=(, _{1});\\ G_{2}=(,_{2})}\{_{j i}(_{1,i}}_{2,i}-_{1,i} }_{2,j})_{1}=f_{}(,_{1}),_{2}=f_{}(,_{2})\}.\] (2)

_where \(}=/\|\|_{2}\) is the normalized vector._

Definition 1 measures how worse node \(i\) follows the GCL principle in all possible augmentations. If \(}_{i}\) is greater than zero, on average, the node embedding \(_{1,i}\) is closer to the embedding of the positive sample \(_{2,i}\) than the embedding of the negative sample \(_{2,j}\) in the worst case of augmentation; otherwise, the embeddings of negative samples are closer. Note that normalization is used to eliminate

Figure 1: The imbalance of GCL training

the effect of a vector's scale. In Definition 1, the anchor node is node \(i\) in \(G_{1}\), so similarly we can define \(}_{2,i}\) if the anchor node is node \(i\) in \(G_{2}\).

We can just take one of \(G_{1}\) and \(G_{2}\) as the variable, for example, set \(G_{2}\) to be a specific sampled augmentation and \(G_{1}\) to be an augmentation within the set \(\). From this perspective, based on Definition 1, we can also obtain a metric, which has only one variable:

**Definition 2** (\(G_{2}\)-Node Compactness).: _Under Definition 1, if \(G_{2}\) is the augmentation sampled in an epoch of GCL training, then the \(G_{2}\)-Node Compactness is_

\[}_{G_{2},i}=_{G_{1}=f_{}(,_ {1})}\{_{1,i}_{2,i}^{CL} _{1}=f_{}(,_{1})\},\] (3)

_where \(_{2,i}^{CL}=}_{2,i}-_{j i}}_{2,j}\) is a constant vector._

Similarly, we can define \(G_{1}\)-node compactness \(}_{G_{1},i}\). Usually we have \(}_{G_{1},i}}_{2,i}\) and \(}_{G_{2},i}}_{1,i}\), i.e., \(G_{1}\)-node compactness and \(G_{2}\)-node compactness are upper bounds of node compactness.

### Provable Training of GCL

To enforce the nodes to better follow the GCL principle, we optimize \(G_{1}\)-node compactness and \(G_{2}\)-node compactness in Definition 2. Designing the objective can be viewed as implicitly performing a "binary classification": the "prediction" is \(_{G_{1}}z_{1,i} W_{2,i}^{CL}\), and the "label" is **1**, which means that we expect the embedding of the anchor more likely to be classified as a positive sample in the worst case. To conclude, we propose the PrOvable Training (POT) for GCL, which provably enforces the nodes to be trained more aligned with the GCL principle. Specifically, we use binary cross-entropy as a regularization term:

\[_{}(G_{1},G_{2})=(_{}( }_{G_{1}},_{n})+_{}( }_{G_{2}},_{n})),\] (4)

where \(_{}(x,y)=-_{i=1}^{N}(y_{i}(x_{i})+(1-y_{i} )(1-(x_{i})))\), \(\) is the sigmoid activation, \(_{N}=[1,1,,1]^{N}\), and \(}_{G_{2}}=[}_{G_{2},1},}_{G_{2},2},,}_{_{2},N}]\) is the vector of the lower bounds in Eq. 3. Overall, we add this term to the original InfoNCE objective and the loss function is

\[=(1-)_{}(Z_{1},Z_{2})+_{}(G_{1},G_{2}),\] (5)

where \(\) is the hyperparameter balancing the InfoNCE loss and the compactness loss. Since \(G_{1}\)-node compactness and \(G_{2}\)-node compactness are related to the network parameter \(\), \(_{}\) can be naturally integrated into the loss function, which can regularize the network parameters to encode node embeddings more likely to follow the GCL principle better.

### Deriving the Lower Bounds

In Section 4.2, we have proposed the formation of our loss function, but the lower bounds \(}_{G_{1}}\) and \(}_{G_{2}}\) are still to be solved. In fact, to see Eq. 3 from another perspective, \(}_{G_{1},1}\) (or \(}_{G_{2},1}\)) is the lower bound of the output of the neural network concatenating a GCN encoder and a linear projection with parameter \(_{1,i}^{CL}\) (or \(_{2,i}^{CL}\)). It is difficult to find the particular \(G_{1}\) which minimizes Eq. 3, since finding that exact solution \(G_{1}\) can be an NP-hard problem considering the discreteness of graph augmentations. Therefore, inspired by the bound propagation methods [5; 32; 38], we derive the output lower bounds of that concatenated network in a bound propagation manner, i.e., we can derive the lower bounds \(}_{G_{1}}\) and \(}_{G_{2}}\) by propagating the bounds of the augmented adjacency matrix \(_{1}\).

However, there are still two challenges when applying bound propagation to our problem: 1) \(_{1}\) in Eq. 3 is not defined as continuous values with element-wise bounds; 2) to do the propagation process, the nonlinearities in the GCN encoder \(f_{}\) should be relaxed.

Defining the adjacency matrix with continuous valuesFirst, we define the set of augmented adjacency matrices as follows:

**Definition 3** (Augmented adjacency matrix).: _The set of augmented adjacency matrices is_

\[= \{}\{0,1\}^{N N}|_{ij} a_{ ij}}=}^{T}\] \[\|}-\|_{0} 2Q\| }_{i}-_{i}\|_{0} q_{i}\; 1 i  N\},\]

_where \(}\) denotes the augmented adjacency matrix, \(Q\) and \(q_{i}\) are the global and local budgets of edge dropped, which are determined by the specific GCL model._

The first constraint restricts the augmentation to only edge dropping, which is a common setting in existing GCL models(see more discussions in Appendix B). The second constraint is the symmetric constraint of topology augmentations.

Since the message-passing matrix is used in GNN instead of the original adjacency matrix, we further relax Definition 3 and define the set of all possible message-passing matrices, which are matrices with continuous entries as expected, inspired by :

**Definition 4** (Augmented message-passing matrix).: _The set of augmented message-passing matrices is_

\[} =\{}^{N N}}= }^{T} i,j:L_{ij}_{ij} U_{ij}\},\] \[L_{ij} =\{_{ij}&,i=j\\ 0&,i j,\;U_{ij}=\{((d_{i}-q_{i})(d_{j}- q_{j}))^{-}&,i=j\\ \{a_{ij},((d_{i}-q_{i})(d_{j}-q_{j}))^{-}\}&,i j .,\]

_where \(}\) denotes the augmented message-passing matrix._

Under Definition 4, the augmented message-passing matrices are non-discrete matrices with element-wise bounds \(L_{ij}\) and \(U_{ij}\). \(L_{ij}\) is \(_{ij}\) when \(i=j\), since there is no edge added and the self-loop can not be dropped; \(L_{ij}\) is \(0\) when \(i j\), which implies the potential dropping of each edge. \(U_{ij}\) is set to \(((d_{i}-q_{i})(d_{j}-q_{j}))^{-}\) when \(i=j\) because \(_{ii}\) is the largest when its adjacent edges are dropped as much as possible, and \(U_{ij}=\{a_{ij},((d_{i}-q_{i})(d_{j}-q_{j}))^{-}\}\) when \(i j\) since there is only edge dropping, \(U_{ij}\) cannot exceed \(a_{ij}\).

Relaxing the nonlinearities in the GCN encoderThe GCN encoder in our problem has non-linear activation functions as the nonlinearities in the network, which is not suitable for bound propagation. Therefore, we relax the activations with linear bounds :

**Definition 5** (Linear bounds of non-linear activation function).: _For node \(i\) and the \(m\)-th neuron in the \(k\)-th layer, the input of the neuron is \(p_{i,m}^{(k)}\), and \(\) is the activation function of the neuron. With the lower bound and upper bound of the input denoted as \(p_{L,i,m}^{(k)}\) and \(p_{U,i,m}^{(k)}\), the output of the neuron is bounded by linear functions with parameters \(_{L,i,m}^{(k)},_{U,i,m}^{(k)},_{L,i,m}^{(k)},_{U,i,m}^{( k)}\), i.e._

\[_{L,i,m}^{(k)}(p_{i,m}^{(k)}+_{L,i,m}^{(k)})(p_{i,m}^{(k )})_{U,i,m}^{(k)}(p_{i,m}^{(k)}+_{U,i,m}^{(k)}).\]

_The values of \(_{L,i,m}^{(k)},_{U,i,m}^{(k)},_{L,i,m}^{(k)},_{U,i,m}^{( k)}\) depend on the activation \(\) and the pre-activation bounds \(p_{L,i,m}^{(k)}\) and \(p_{U,i,m}^{(k)}\)._

To further obtain \(\) and \(\), we compute the pre-activation bounds \(_{L}^{(k)}\) and \(_{U}^{(k)}\) as follows, which is the matrix with elements \(p_{L,i,m}^{(k)}\) and \(p_{U,i,m}^{(k)}\):

**Theorem 1** (Pre-activation bounds of each layer ).: _If \(f=(}(}^{(1)}+ ^{(1)})^{(2)}+^{(2)})\) is the two-layer GCN encoder, given the element-wise bounds of \(}\) in Definition 4, \(^{(k)}\) is the input embedding of \(t\)-th layer, then the pre-activation bounds of \(t\)-th layer are_

\[_{L}^{(k)} =[^{(k)}^{(k)}]_{+}+[ ^{(k)}^{(k)}]_{-}+^{(k)},\] \[_{U}^{(k)} =[^{(k)}^{(k)}]_{+}+[ ^{(k)}^{(k)}]_{-}+^{(k)},\]

_where \([]_{+}=(,0)\) and \([]_{-}=(,0)\)._The detailed proof is provided in Appendix A.1. Therefore, \(p_{L,i,m}^{(k)}\) and \(p_{U,i,m}^{(k)}\) are known with Theorem 1 given, then the values of \(\) and \(\) can be obtained by referring to Table 1. In the table, we denote \(p_{U,i,m}^{(k)}\) as \(u\) and \(p_{L,i,m}^{(k)}\) as \(l\) for short. \(\) is the slope parameter of ReLU-like activation functions that are usually used in GCL, such as ReLU, PReLU, and RReLU. For example, \(\) is zero for ReLU.

To this end, all the parameters to derive the lower bounds \(}_{G_{1}}\) and \(}_{G_{2}}\) are known: the bounds of the variable \(}\) in our problem, which are \(\) and \(\) defined in Definition 4; the parameters of the linear bounds for non-linear activations, which are \(\) and \(\) defined in Definition 5 and obtained by Theorem 1 and Table 1. To conclude, the whole process is summarized in the following theorem, where we apply bound propagation to our problem with those parameters:

**Theorem 2** (The lower bound of neural network output).: _If the encoder is defined as 1, \(}\) is the augmented message-passing matrix and \(\) is the non-linear activation function, for \(G_{1}=(,_{1})\), \(}=^{-1/2}(_{1}+)^{-1/ 2}}\), then_

\[}_{G_{2},i} =_{j_{1}(i)}_{j_{1}i}[_{j_{2} (j_{1})}_{j_{2}j_{1}}x_{j_{2}}}_{i}^{(1)}+ }^{(1)}]+}^{(2)}\] (6) _where_ \[_{2,i}^{CL}=}_{2,i}-_{j  i}}_{2,j},\;(i)=\{j\;|\;a_{ij}>0\;\;j=i\},\] (7) \[_{i,m}^{(2)} =\{_{L,i,m}^{(2)}&\;W_{L, i,m}^{CL} 0\\ _{U,i,m}^{(2)}&\;W_{2,i,m}^{CL}<0.,\;_{i,m }^{(2)}=\{_{L,i,m}^{(2)}&\;W_{2,i,m}^{CL } 0\\ _{U,i,m}^{(2)}&\;W_{2,i,m}^{CL}<0.\] (8) \[_{i}^{(2)} =W_{2,i}^{CL}_{i}^{(2)},\;}_{i}^{( 2)}=_{m=1}^{d_{2}}_{i,m}^{(2)}W_{:,m}^{(2)},\;_{i}^{(2)}= _{m=1}^{d_{2}}_{i,m}^{(2)}(b_{m}^{(2)}+_{i,m}^{(2)}),\] (9) \[_{i,m}^{(1)} =\{_{L,i,m}^{(1)}&\; {W}_{i,m}^{(2)} 0\\ _{U,i,m}^{(1)}&\;_{i,m}^{(2)}<0.,\; _{i,l}^{(1)}=\{_{L,i,l}^{(1)}&\; _{i,l}^{(2)} 0\\ _{U,i,l}^{(1)}&\;_{i,l}^{(2)}<0.\] (10) \[^{(1)} =}_{i}^{(2)}^{(1)},\;}_{i}^{(1)}=_{l=1}^{d_{1}}_{i,l}^{(1)}_{:,l}^{(1)},\; }_{i}^{(1)}=_{l=1}^{d_{2}}_{i,l}^{(1)}(b_{m}^{(1)} +_{i,l}^{(1)}).\] (11)

\(_{i}^{(2)},\;_{i}^{(2)},\;_{i}^{(2)} ^{d_{2}}\)_; \(_{i}^{(1)},\;_{i}^{(1)},\;_{i}^{(1)} ^{d_{1}}\); \(}_{i}^{(2)},\;}_{i}^{(1)}^{F  1}\); \(}^{(2)},\;}^{(1)}\); \(d_{1}\) and \(d_{2}\) are the dimension of the embedding of layer 1 and layer 2 respectively._

We provide detailed proof in Appendix A.2. Theorem 2 demonstrates the process of deriving the node compactness \(}_{G_{2},i}\) in a bound propagation manner. Before Theorem 2, we obtain all the parameters needed, and we apply the bound propagation method  to our problem, which is the concatenated network of the GCN encoder \(f_{}\) and a linear transformation \(W_{2,i}^{CL}\).As the form of Eq. 6, the node compactness \(}_{G_{2},i}\) can be also seen as the output of a new "GCN" encoder with network parameters \(}_{i}^{(1)}\), \(}^{(1)}\), \(}^{(2)}\), which is an encoder constructed from the GCN encoder in GCL.Additionally, we present the whole process of our POT method inAlgorithm 1.

Time ComplexityThe computation of the weight matrices in Theorem 2 dominates the time complexity of POT. Thus, the time complexity of our algorithm is \(O(Nd_{k-1}d_{k}+Ed_{k})\) for the \(k\)-th layer, which is the same as the message-passing  in the GNN encoders.

## 5 Experiments

### Experimental Setup

We choose four GCL baseline models for evaluation: GRACE , GCA , ProGCL , and COSTA . Since POT is a plugin for InfoNCE loss, we integrate POT with the baselines and construct four models: GRACE-POT, GCA-POT, ProGCL-POT, and COSTA-POT. Additionally, we report the results of two supervised baselines: GCN and GAT . For the benchmarks, we choose 8 common datasets: Cora, CiteSeer, PubMed , Flickr, BlogCatalog , Computers, Photo , and WikiCS . Details of the datasets and baselines are presented in Appendix C.2. For datasets with a public split available , including Cora, CiteSeer, and PubMed, we follow the public split; For other datasets with no public split, we generate random splits, where each of the training set and validation set contains 10% nodes of the graph and the rest 80% nodes of the graph is used for testing.

### Node Classification

We test the performance of POT on node classification. For the evaluation process, we follow the setting in baselines  as follows: the encoder is trained in a contrastive learning manner without supervision, then we obtain the node embedding and use the fixed embeddings to train and evaluate a logistic regression classifier on the downstream node classification task. We choose two measures: Micro-F1 score and Macro-F1 score. The results with error bars are reported in Table 2.

   &  &  &  &  &  &  &  \\   & & & & w/o POT & w/ POT & w/o POT & w/ POT & w/ POT & w/o POT & w/ POT & w/ POT \\   & Mi-Fl & \(8.9 1.1\) & \(82.8 0.5\) & \(78.2 0.6\) & \(\) & \(78.6 0.2\) & \(\) & \(77.9 1.4\) & \(\) & \(79.9 0.7\) & \(\) \\  & Ma-Fl & \(80.6 1.1\) & \(81.8 0.3\) & \(76.8 0.6\) & \(\) & \(77.7 0.1\) & \(\) & \(77.3 1.7\) & \(\) & \(79.5 0.7\) & \(\) \\   & Mi-Fl & \(71.6 0.6\) & \(72.1 1.0\) & \(66.8 1.0\) & \(\) & \(65.0 0.7\) & \(\) & \(65.9 0.9\) & \(\) & \(66.8 0.8\) & \(\) \\  & Ma-Fl & \(68.2 0.5\) & \(67.1 1.3\) & \(63.2 1.3\) & \(\) & \(60.8 0.8\) & \(\) & \(62.5 0.9\) & \(\) & \(63.4 1.4\) & \(\) \\   & Mi-Fl & \(79.3 0.1\) & \(78.3 0.7\) & \(81.6 0.5\) & \(82.0 1.3\) & \(80.5 2.3\) & \(\) & \(81.5 1.1\) & \(81.9 0.4\) & \(80.0 1.5\) & \(\) \\  & Ma-Fl & \(79.0 0.1\) & \(77.6 0.8\) & \(81.7 0.5\) & \(\) & \(80.5 2.1\) & \(\) & \(81.3 1.0\) & \(\) & \(79.3 1.4\) & \(\) \\   & Mi-Fl & \(48.8 1.9\) & \(37.3 0.5\) & \(46.5 1.8\) & \(\) & \(47.4 0.8\) & \(\) & \(46.4 1.4\) & \(\) & \(52.6 1.2\) & \(\) \\  & Ma-Fl & \(45.8 2.4\) & \(35.2 0.7\) & \(45.1 1.4\) & \(\) & \(46.2 0.8\) & \(\) & \(45.4 1.1\) & \(\) & \(51.2 0.9\) & \(\) \\   & Mi-Fl & \(72.4 2.5\) & \(68.0 2.1\) & \(70.5 1.3\) & \(\) & \(76.2 0.6\) & \(76.5 0.5\) & \(74.1 0.6\) & \(\) & \(73.3 3.4\) & \(\) \\  & Ma-Fl & \(71.9 2.4\) & \(67.6 1.8\) & \(70.2 1.3\) & \(\) & \(75.8 0.7\) & \(76.1 0.5\) & \(73.7 0.6\) & \(\) & \(73.0 3.6\) & \(\) \\   & Mi-Fl & \(87.6 0.6\) & \(88.7 1.3\) & \(85.9 0.4\) & \(\) & \(88.7 0.6\) & \(\) & \(88.6 0.7\) & \(\) & \(82.5 0.4\) & \(82.8 0.7\) \\  & Ma-Fl & \(83.1 2.3\) & \(79.3 1.4\) & \(83.9 0.8\) & \(\) & \(87.1 0.7\) & \(\) & \(86.9 1.1\) & \(\) & \(78.8 1.7\) & \(79.0 1.0\) \\   & Mi-Fl & \(91.7 0.7\) & \(92.0 0.5\) & \(91.2 0.4\) & \(\) & \(91.5 0.4\) & \(\) & \(92.1 0.3\) & \(\) & \(91.5 0.6\) & \(\) \\  & Ma-Fl & \(88.8 2.0\) & \(88.9 2.2\) & \(89.2 0.7\) & \(\) & \(89.5 0.5\) & \(\) & \(90.6 0.6\) & \(\) & \(89.3 1.3\) & \(\) \\   & Mi-Fl & \(81.2 1.2\) & \(81.2 0.8\) & \(80.3 0.3\) & \(\) & \(78.7 0.3\) & \(\) & \(78.8 0.4\) & \(\) & \(76.2 1.6\) & \(\) \\  & Ma-Fl & \(78.7 1.9\) &Bolded results mean that POT outperforms the baseline, with a t-test level of 0.1. As it shows, our POT method improves the performance of all the base models consistently on all datasets, which verifies the effectiveness of our model. Moreover, POT gains significant improvements over baseline models on BlogCatalog and Flickr. These two datasets are the graphs with the highest average node degree, therefore there are more possible graph augmentations in these datasets (the higher node degrees are, the more edges are dropped [35; 36]). This result illustrates our motivation that POT improves existing GCL methods when faced with complex graph augmentations.

### Analyzing the Node Compactness

To illustrate how POT improves the training of GCL, we conduct experiments to show some interesting properties of POT by visualizing the node compactness in Definition 2. More experiments can be found in Appendix D.

**POT Improves Node Compactness.** We illustrate that our POT does improve the node compactness, which is the goal of InfoNCE. The result on Cora is shown in Figure 2. The x-axis is the epoch number, and the y-axis is the average compactness of all nodes. We can see that the node compactness of the model with POT is almost always higher than the model without POT, indicating that POT increases node compactness in the training of GCL, i.e. how the nodes follow the GCL principle is improved in the worst-case. Therefore, our POT promotes nodes to follow the GCL principle better. More results on BlogCatalog are shown in Appendix D.

**Node Compactness Reflects Different Properties of Different Augmentation Strategies.** There are two types of augmentation strategies: GRACE adapts a uniform edge dropping rate, in other words, the dropping rate does not change among different nodes; GCA, however, adapts a larger dropping rate on edges with higher-degree nodes, then higher-degree nodes have a larger proportion of edges dropped. Since node compactness is how a node follows the GCL principle in the worst case, a higher degree results in larger node compactness, while a higher dropping rate results in lower node compactness because worse augmentations can be taken. To see whether node compactness can reflect those properties of different augmentation as expected, we plot the average node compactness of the nodes with the same degree as scatters in Figure 3 on BlogCatalog and Flickr. For GRACE, node degree and compactness are positively correlated, since the dropping rate does not change as the degree increases; For GCA, node compactness reflects a trade-off between dropping rate and degree: the former dominates in the low-degree stage then the latter dominates later, resulting the curve to drop then slightly rise.

Figure 3: Degree and node compactness score

Figure 2: Node compactness in the training process

### Hyperparameter Analysis

In this subsection, we investigate the sensitivity of the hyperparameter in POT, \(\) in Eq. 5. In our experiments, we tune \(\) ranging from \(\{0.5,0.4,0.3,0.2,0.1,0.05\}\). The hyperparameter \(\) balances between InfoNCE loss and POT, i.e. the importance of POT is strengthened when \(>0.5\) while the InfoNCE is more important when \(<0.5\). We report the Micro-F1 score on node classification of BlogCatalog in Figure 4. Although the performance is sensitive to the choice of \(\), the models with POT can still outperform the corresponding base models in many cases, especially on BlogCatalog where POT outperforms the base models whichever \(\) is selected. The result on Cora is in Appendix D.

## 6 Related Work

**Graph Contrastive Learning.** Graph Contrastive Learning (GCL) methods are proposed to learn the node embeddings without the supervision of labels. GRACE  firstly proposes GCL with random edge dropping and feature masking as data augmentations and takes InfoNCE as the objective. Based on GRACE, GCA  improves the data augmentation with adaptive masking and dropping rate related to node centrality. COSTA  proposes feature augmentation in embedding space to reduce sampling bias. ProGCL  reweights the negatives and empowers GCL with hard negative mining. There are also node-to-graph GCL methods, like DGI  and MVGRL . Inspired by BYOL , there are also several GCL methods that do not need negative samples  or use BCE objectives .

**Bound Propagation.** Bound Propagation methods derive the output bounds directly from input bounds. Interval Bound Propagation  defines the input adversarial polytope as a \(l_{}\)-norm ball, and it deforms as being propagated through affine layers and monotonic activations.  proposes a relaxation on nonlinearity via Semidefinite Programming but is restricted to one hidden layer.  relaxes ReLU via convex outer adversarial polytope.  can be applied to general activations bounded by linear functions. For GNN,  and  study the bound propagation on GNN with feature and topology perturbations respectively. One step further,  proposes a Mixed Integer Linear Programming based method to relax GNN certification with a shared perturbation as input. However, almost all the previous works require solving an optimization problem with some specific solver, which is time-consuming and impossible to be integrated into a training process as we expected.

## 7 Conclusion

In this paper, we investigate whether the nodes follow the GCL principle well enough in GCL training, and the answer is no. Therefore, to address this issue, we design the metric "node compactness" to measure the training of each node, and we propose POT as a regularizer to optimize node compactness, which can be plugged into the existing InfoNCE objective of GCL. Additionally, we provide a bound propagation-inspired method to derive POT theoretically. Extensive experiments and insightful visualizations verify the effectiveness of POT on various GCL methods.

**Limitations and broader impacts.** One limitation of our work is the restriction on the settings of GCL. To extend our work to more types of GCL settings and graph-level GCL, one can define the set of augmentations and network structure properly, then follow the proof in Appendix A.2 to derive the node compactness in a new setting. The detailed discussion can be found in Appendix B. We leave them for future work. Moreover, there are no negative social impacts foreseen.

Figure 4: Hyperparamenter analysis on \(\)