# SegVol: Universal and Interactive Volumetric Medical Image Segmentation

Yuxin Du\({}^{1,2}\), Fan Bai\({}^{1,2,3}\), Tiejun Huang\({}^{2,4}\), Bo Zhao\({}^{1,2}\)

\({}^{1}\)School of Artificial Intelligence, Shanghai Jiao Tong University

\({}^{2}\)BAAI \({}^{3}\)The Chinese University of Hong Kong \({}^{4}\)Peking University

\({}^{}\)Corresponding author: Bo Zhao \(<\)bo.zhao@sjtu.edu.cn\(>\)

###### Abstract

Precise image segmentation provides clinical study with instructive information. Despite the remarkable progress achieved in medical image segmentation, there is still an absence of a 3D foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction. In this paper, we propose a 3D foundation segmentation model, named _SegVol_, supporting universal and interactive volumetric medical image segmentation. By scaling up training data to 90K unlabeled Computed Tomography (CT) volumes and 6K labeled CT volumes, this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts. To facilitate efficient and precise inference on volumetric images, we design a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical segmentation tasks verify that SegVol outperforms the competitors in 19 tasks, with improvements up to 37.24% compared to the runner-up methods. We demonstrate the effectiveness and importance of specific designs by ablation study. We expect this foundation model can promote the development of volumetric medical image analysis. The model and code are publicly available at: https://github.com/BAAI-DCAI/SegVol.

## 1 Introduction

Volumetric medical segmentation, involving extracting 3D regions of interest, such as organs, lesions, and tissues, plays a pivotal role in medical image analysis by accurately modeling the 3D structural information of the human body from volumetric medical images such as CT or MRI. The accurate segmentation can benefit numerous clinical applications including tumors monitoring, surgical planning, disease diagnosis, therapy optimization, etc.

Compared to 2D medical image segmentation, volumetric image segmentation is notably more challenging due to the labor-intensive annotation and resource-consuming computation. Recently, the research of volumetric medical image segmentation has garnered substantial attention, leading to a series of advancements. However, existing volumetric medical segmentation methods have several key limitations which prevent their application in challenging tasks, e.g., liver tumor or colon cancer segmentation, and real-world tasks, e.g., human-interactive segmentation.

Firstly, the publicly available volumetric medical image datasets usually consist of a small number of mask annotations from a few varying categories. Due to the different label spaces, the traditional task-specific segmentation models trained on one dataset have difficulty in generalizing to others. For example, the CT-ORG dataset contains the 'lungs' category, while this category is split into two sub-classes and named 'left lung' and 'right lung' in the LUNA16 dataset. Hence, a universal segmentation model has to understand the semantics of anatomical categories. Secondly,traditional segmentation models have inferior performance when segmenting complex structures, such as tumors and cysts. This is because these models are trained on insufficient data and are also not able to leverage the spatial information through user interaction. Thirdly, previous solutions are computationally expensive in the inference process. They typically employ a sliding window to infer the whole volumetric input. This strategy is not only time-consuming but also short-sighted, as the sliding window contains only local information. Recently, there have been some works that introduce spatial-prompt into medical image segmentation, shown in Table 1. However, most of them lack the ability to process the 3D input directly and naturally, and none of them is able to understand the semantics of anatomical categories.

In this paper, we propose the first foundation model for volumetric medical image segmentation - _SegVol_. The proposed model enables universal and interactive 3D segmentation of more than 200 anatomical categories, supporting both spatial and semantic prompts. SegVol can also be driven by the combination of multi-prompt, like 'bounding box+text' or 'point+text' prompts, achieving high-precision segmentation and semantic disambiguation. To enable efficient and precise segmentation of volumetric images, we develop a zoom-out-zoom-in mechanism that enables the model to be efficient and precise. We evaluate the proposed SegVol on 22 volumetric medical image segmentation tasks and the results demonstrate our method surpasses other SAM-like interactive segmentation methods by a large margin. Extensive case studies and ablation experiments are also carried out to prove the advantages of SegVol and the effectiveness of the zoom-out-zoom-in mechanism and multi-prompt combination.

We summarize our key contributions as follows:

1. Collect and process 25 public volumetric medical segmentation datasets, encompassing over 200 anatomical categories. The pseudo label is introduced to relieve the spurious correlation in the training data.

Figure 1: Overview of SegVol model architecture. SegVol produces precise segmentation of 3D anatomical structures from volumetric inputs with easy user interactions, including point, bounding box, and text prompts. Zoom-out-zoom-in mechanism: SegVol initially produces a rough prediction mask with zoom-out inference, then refines it with zoom-in inference on the identified ROI.

2. Implement massive 3D pre-training on 96K CT volumes and supervised fine-tuning on the 6k labeled datasets.
3. Support spatial-prompt, semantic-prompt, and combined-prompt segmentation, achieving high-precision segmentation and semantic disambiguation.
4. Design a zoom-out-zoom-in mechanism that significantly reduces the computational cost, meanwhile preserving precise segmentation.

## 2 Methodology

### Dataset Construction

One of the main challenges of training a universal volumetric medical segmentation model is the absence of large-scale publicly available volumetric medical data, especially CTs with segmentation annotations. Doing our utmost, we collected 25 open-source segmentation CT datasets, including CHAOS, HaN-Seg, AMOS22, AbdomenCT-1k, KiTS23, KiPA22, KiTS19, BTCV, Pancreas-CT, 3D-IRCADB, FLARE22, TotalSegmentator, CT-ORG, VerSe19, VerSe20, SLIVER07, QUBIQ, six MSD datasets, LUNA16, and WORD. Their detailed information and availability are shown in the Section A. These CTs originate from various medical institutions, captured by different machines with varying parameter settings and scanning regions. To standardize these datasets, we use the mean voxel value of each volume to filter the background and then perform normalization on the foreground voxels.

Volumetric segmentation datasets suffer from the notorious problem of partial labels. Most of these datasets have annotations of only a few segmentation targets, e.g., several organs. Therefore, the deep models may learn the spurious correlation between datasets and segmentation targets, and thus produce inferior results during the inference phase. To relieve this problem, we introduce the pseudo labels by utilizing the Felzenswalb-Huttenlocher (FH) algorithm to generate pseudo masks for each CT scan. Pseudo masks can supplement unlabeled categories in a dataset, therefore relieving the spurious correlation problem. To restrain the noise and numerous tiny masks in pseudo labels, we employ the following strategies: 1) The pseudo masks are replaced with ground truth masks when applicable. 2) We filter out tiny structures smaller than 1% of the whole volume size. 3) Each mask is refined by dilation and erosion operations.

### Model Architecture

Motivated by the recent advance in 2D nature image segmentation, Segment Anything (SAM), we design a novel model for interactive and universal volumetric medical image segmentation, named, SegVol. The model is illustrated in Figure 1. SegVol supports three types of prompts for interactive segmentation: 'bounding box(bbox)' prompt, including the coordinates of two diagonal vertices; 'point' prompt, composed of a set of positive and negative points; and 'text' prompt, such as 'liver' or 'cervical spine C2'. The model consists of four modules: image encoder, text encoder, prompt encoder, and mask decoder.

We employ 3D ViT (Vision Transformer) as the image encoder, which exhibits remarkable advantages over convolutional models when pre-trained on large-scale datasets. The 3D ViT structure is designed as follows: patch size=(4, 16, 16), layers number=12, heads number=12, hidden

    & & &  \\  Method & Image Domain & Dimension & Training & Point & Bbox & Text & Inference Input \\  SAM & Natural & 2D & Full-Param & ✓ & ✓ & ✓ & 1024\(\)1024 \\ MedSAM & Medical & 2D & Decoder & ✗ & ✓ & ✗ & 1024\(\)1024 \\ SAM-Med2D & Medical & 2D & Adapter & ✓ & ✓ & ✗ & 1024\(\)1024 \\ SAM-Med3D & Medical & 3D & Full-Param & ✓ & ✗ & ✗ & 128\(\)128\(\)128 \\
**OURS** & **Medical** & **3D** & **Full-Param** & ✓ & ✓ & ✓ & **Full Resolution** \\   

Table 1: The different settings and functions of SAM-like interactive segmentation methods.

size=768. We first pre-train 3D ViT using SimMIM algorithm on the collected 96K CTs, and then conduct further supervised fine-tuning on the 6K CTs with 150K labeled segmentation masks.

One of the main limitations of traditional segmentation models is that the models learn dataset-specific labels encoded as integers which cannot generalized to unseen datasets or tasks, preventing their real-world applications. We enable universal segmentation across datasets by leveraging the text encoder from CLIP model to encode the input text prompt, as CLIP has been trained to align image and text embeddings on web-scale image-text pairs. Given a word or phrase as the text prompt, we complete it using the template '_A computerized tomography of a [text prompt]_' and then encode it into text embedding. The off-the-shelf text encoder is frozen during training due to the limited text data in CT datasets. Following SAM, we obtain the spatial-prompt embedding using positional encoding on point and bbox prompt.

After obtaining the image embedding and prompt embedding, we input them into the mask decoder and predict the mask. We use self-attention and cross-attention in two directions to fuse the image embedding and prompt embedding, and then employ the transposed convolutions and interpolation operations to generate masks. Since text embedding is the key to universal segmentation and it is also challenging to learn the correlation between text and volumetric regions, we enhance the text information by introducing a parallel text input branch beside the joint prompt embedding.

### Prompt Generation

SegVol accepts multiple types of prompts, including individual point, bbox, and text prompts, and also their combinations. To make full use of the segmentation training data, we generate kinds of prompts for each datum and construct kinds of prompt-mask data pairs for training.

The point prompt is built from ground truth or pseudo masks, consisting of three kinds of points, namely, positive point, negative point, and ignored point. Positive point means that it is within the target mask region, while negative points are those outside. Ignored points are utilized to ensure a uniform length of the point prompts for input completion. Notably, these ignored points are not considered by the model.

The bbox prompt is generated based on the ground truth or pseudo masks, integrated with random jitter to enhance the model's robustness. When generating the bbox prompt for some pseudo mask, the bbox may also cover other masks due to the irregular 3D shapes. To address this problem, we compute the Intersection over Union (IoU) between the generated bbox and the included pseudo masks. Any mask with an IoU greater than 0.9 will also be integrated and considered as part of the target mask corresponding to this bbox prompt.

The text prompts are constructed based on their category names. As pseudo masks produced by the unsupervised FH algorithm do not have the semantic information, we only use point and bbox prompts for training on masks of pseudo labels.

### Zoom-out-zoom-in Mechanism

SAM-like interaction with large-volume images is laborious for users, especially in the scene where the sliding window has to be used due to the limited view of those 3D models. To provide users with an easy SAM-like interface, we design a zoom-out-zoom-in mechanism, which is efficient and precise, consisting of zoom-out-zoom-in inference and multi-size training. As demonstrated in Figure 1, the zoom-out process involves resizing a volumetric image, which is input into the model with user prompts to generate a coarse segmentation mask. Then, the Region of Interest (ROI) from the original image is cropped for zoom-in analysis. In the zoom-in process, a sliding window is used to perform precise inference driven by prompts generated from the coarse segmentation mask. After that, the ROI prediction mask will be back-filled to the coarse segmentation mask to finish the final prediction. Besides, multi-size training involves augmenting the input data by resizing CTs for the zoom-out view and cropping them into cubes for the zoom-in view. The zoom-out-zoom-in mechanism realizes the computational cost reduction meanwhile producing precise segmentation of the ROI.

### Loss Function

We apply SimMIM algorithm to pre-train the image encoder of SegVol with the masked image modeling loss \(_{}(_{};_{1})\). The loss function is as follows:

\[_{}(_{};_{1})= _{})}||_{}-_{}||_{1},\] (1)

where \(_{}\) is the parameter set of SegVol's image encoder. \(,^{D H W}\) are the input voxel values and predicted values, respectively. M denotes the set of masked voxels, \(()\) is the number of elements, and \(_{1}\) is the pre-training dataset.

We combine the Binary Cross-Entropy (BCE) loss and Dice loss as the supervised fine-tuning loss function \(_{}(;_{2})\) to train the model with trainable parameters \(\) (text encoder frozen). \(_{2}\) is the supervised fine-tuning dataset and \(,^{D H W}\) are the predicted mask and ground-truth mask, respectively. \((,)\) is the forward function of SegVol. The loss function is as follows:

\[_{}(;_{2})=-_{(, )_{2}}[,((, ))+ 1-,(1-(,))]\] (2)

\[_{}(;_{2})=1-_{(, )_{2}}[,(,)}{\|\|_{1}+\|(,)\|_{1}}]\] (3)

\[_{}(;_{2})=_{ }(;_{2})+_{}(;_{2})\] (4)

The detailed fine-tuning algorithm of SegVol is presented in Section B.

## 3 Experiments

In this section, we conduct extensive experiments on 22 volumetric medical image segmentation tasks to compare SegVol with other SAM-like medical image segmentation methods[28; 38; 39; 29]. Ablation studies are also carried out to prove the effectiveness of the zoom-out-zoom-in mechanism and provide more insights about dataset scale and multi-prompt combination. Detailed case studies are conducted to discuss the disambiguation ability of semantic-prompt and the capability of identifying the segmentation results with spatial-prompt.

### Experimental Setup

During the pre-training, we follow SimMIM algorithm to train the 3D ViT encoder of SegVol on the collected 96K CTs for 2000 epochs. In the supervised fine-tuning stage, we train SegVol (with the text encoder frozen) on the labeled 25 volumetric medical image segmentation datasets for 270 epochs with batch size 32 and input size (32, 256, 256), using AdamW optimizer. SimMIM pre-training takes about \(20 8\) GPU hours, while fine-tuning takes about \(300 8\) GPU hours. All the above training process is implemented on 8 NVIDIA A100-SXM4-40GB. Three external datasets[44; 74; 75] and 20% testing data preserved from 25 collected datasets are used in the following experiments.

### Compared with SAM-like Interactive Methods

Several efforts have been made to construct a SAM-like interactive medical image segmentation model. However, some of these works, such as MedSAM and SAM-MED2D, focus on 2D tasks and cannot process 3D input directly. The other 3D-based methods, such as SAM-MED3D, only support small cropped input and do not support semantic-prompt segmentation, which are still far from building a comprehensive foundation model for volumetric medical image analysis.

Competitors and configures.In this experiment, MedSAM and SAM(bounding box) use bounding box prompts. SAM(5 clicks), SAM-MED2D and SAM-MED3D use point prompts and a five-step correction procedure, which means that the point prompt in each step will be given according to the previous-step output and ground truth, rather than giving all at once. Inthis experiment, SegVol uses bounding box and text prompt which performs better than other kinds of prompt combinations. Detailed ablation study on prompt combination is demonstrated in Figure 3 (b). In addition, we compare SegVol with traditional task-specific segmentation models, e.g., 3DUX-NET, SwinUNETR, and nnU-Net, in Section C, though the direct comparison is unsuitable due to the different settings and objectives.

Testing data.To compare with these SAM-like interactive segmentation models, we evaluate the models on 1,778 cases from the validation set of AMOS22, the whole novel annotated set of Uni

    &  &  &  &  &  &  \\  & &  &  &  &  &  & & \\   & Aorta & 0.7267 & 0.4362 & 0.8704 & 0.8102 & 0.3387 & **0.9273** \\  & Bladder & 0.4162 & 0.6281 & 0.8417 & 0.4338 & 0.6799 & **0.9120** \\  & Duodenum & 0.1554 & 0.3192 & 0.5066 & 0.3820 & 0.3066 & **0.7402** \\  & Esophagus & 0.2917 & 0.3541 & 0.5500 & 0.5174 & 0.3610 & **0.7460** \\  & Gallbladder & 0.2831 & 0.6161 & 0.7999 & 0.5643 & 0.6609 & **0.8763** \\  & Adrenal gland(L) & 0.0555 & 0.4222 & 0.5068 & 0.4584 & 0.3766 & **0.7295** \\  & Left kidney & 0.8405 & 0.8274 & 0.9325 & 0.8723 & 0.7909 & **0.9489** \\
 & Liver & 0.7477 & 0.5124 & 0.6904 & 0.8801 & 0.6137 & **0.9641** \\  & Pancreas & 0.2127 & 0.3392 & 0.5656 & 0.5391 & 0.3217 & **0.8295** \\  & Postcava & 0.2042 & 0.5251 & 0.4436 & 0.6683 & 0.5211 & **0.8384** \\  & Prostate uterus & 0.2344 & 0.6986 & 0.7518 & 0.6231 & 0.7739 & **0.8557** \\  & Adrenal gland(R) & 0.0452 & 0.3642 & 0.1681 & 0.3708 & 0.3855 & **0.6994** \\  & Right kidney & 0.8459 & 0.8215 & 0.9077 & 0.8632 & 0.7851 & **0.9505** \\  & Spleen & 0.5936 & 0.6536 & 0.9267 & 0.8591 & 0.7038 & **0.9589** \\  & Stomach & 0.4229 & 0.3883 & 0.5399 & 0.4576 & 0.4378 & **0.9123** \\   & **Average** & 0.4050 & 0.5271 & 0.6668 & 0.6200 & 0.5371 & **0.8593** \\   & Deep\_esion3D & 0.3686 & 0.7473 & 0.3258 & 0.2386 & **0.7680** & 0.7065 \\  & Bone\_esion & 0.4461 & 0.6671 & 0.1947 & 0.4447 & 0.6896 & **0.6920** \\  & PancreasLeison & 0.0675 & 0.5579 & 0.5548 & 0.5526 & 0.6561 & **0.7265** \\   & **Average** & 0.2941 & 0.6574 & 0.3584 & 0.4120 & **0.7046** & **0.7046** \\   & Aorta & 0.2744 & 0.3894 & 0.8077 & 0.7703 & 0.3278 & **0.8439** \\  & Esophagus & 0.0348 & 0.2046 & 0.3578 & 0.6394 & 0.2196 & **0.7201** \\
 & Heart & 0.6695 & 0.8876 & 0.6012 & 0.8325 & **0.8924** & 0.8172 \\  & Trachea & **0.9147** & 0.1611 & 0.8306 & 0.8485 & 0.1261 & 0.8807 \\   & **Average** & 0.4734 & 0.4107 & 0.6493 & 0.7727 & 0.3915 & **0.8155** \\   

Table 2: Quantitative comparative experiment results for SegVol and other 5 SAM-like interactive segmentation methods settings in terms of the median value of Dice score.

Figure 2: Violin plots for quantitative comparison experiment results of SegVol and SAM-like interactive methods. The vertical axis represents the Dice score.

versal Lesion Segmentation Challenge 23(ULS23), and the released labeled set of SegTHOR. The validation set of AMOS22 contains 120 cases annotated with 15 major organs. The novel annotated ULS23 dataset is composed of three subsets, namely, DeepLesion3D, Radboudumc Bone, and Radboudumc Pancreas. The DeepLesion3D subset contains 200 abdominal lesions, 100 bone lesions, 50 kidney lesions, 50 liver lesions, 100 lung lesions, 100 mediastinal lesions, and 150 assorted lesions cases. There are 744 bone lesion cases in the Radboudumc Bone subset and 124 pancreas lesion cases in the Radboudumc Pancreas subset. The 40 cases from SegTHOR, which are contoured manually by an experienced radiotherapist, focus on the heart, trachea, aorta, and esophagus that surround the tumor and must be preserved from irradiations during radiotherapy.

Quantitative results.The quantitative results of comparative experiments are shown in Table 2, which verify our method is the best in most of the tasks including both lesions and organs, compared to other SAM-like interactive models[28; 38; 39; 29]. Specifically, our method outperforms the second-ranked SAM-MED2D on the AMOS22 dataset by a significant improvement of 19.25% (average Dice score). On the SegTHOR dataset, our method surpasses the runner-up - SAM-MED3D by an average Dice score improvement of 4.28%. The ULS23 dataset, characterized by small patch-like masks, presents a unique challenge. In this scenario, SegVol still exhibits good performance, comparable to MedSAM, which excels in using bbox prompts for segmenting small objects. We visualize the Dice score distributions of all methods in all the tasks as violin plots, depicted in Figure 2. More detailed results and visualization are present in Section C.

### Ablation Studies

Zoom-out-zoom-in mechanism.One of the key designs of SegVol is the zoom-out-zoom-in mechanism. We compare it with the intuitive resize strategy and the popular sliding window algorithm on the split 20% test data, 48 cases covering 15 major organs with a variety of sizes, belonging to the AMOS22 dataset. Two evaluation dimensions, i.e., performance (Dice score) and inference time cost (per case), are compared, as shown in Table 3. The zoom-out-zoom-in mechanism achieves the best average Dice score and a very competitive inference speed compared to the simple resize strategy. The reason for computational cost reduction is that the traditional sliding window method requires scanning the entire 3D CT and processing thousands of windows. In contrast, the proposed zoom-out-zoom-in mechanism only requires one global inference of 3D CT and then scanning the ROI with dozens of windows. Detailed experiment results are shown in Section C.

   Mechanism & Dice Score Avg. \(\) & Time Per Case Avg. \(\) \\  Resize & 0.4509 & 65 ms \\ Sliding window & 0.6529 & 3331 ms \\
**Zoom-out-zoom-in** & 0.7298 & 190 ms \\   

Table 3: Ablation experiment on the zoom-out-zoom-in mechanism.

Figure 3: (a) The performance of SegVol improves as the training data scales up. (b) The quantitative experimental results on 19 anatomical segmentation tasks of split 20% test data demonstrate that using the combination of semantic and spatial prompts can achieve better performances.

Scaling up training data.The success of scaling up training data has been witnessed in multiple computer vision tasks [28; 70]. We conduct an ablation study to investigate the importance of scaling up training images and masks. The split 20% test data of BTCV dataset, which includes 13 main organs, is set as an anchor to evaluate the model trained separately on 1, 2, and 8 datasets for 500 epochs, as well as the final model trained on 25 datasets. The detailed results are shown in Figure 3 (a). As a lightweight model, the performance of SegVol is weak when only one dataset is used. However, with the increase of training data, the Dice score increases rapidly, especially in the text prompt setting. The results indicate that our method is scalable and better performance can be achieved if more training data is available.

Multi-prompt combination.As a universal model, our approach achieves precise segmentation for over 200 organs, tissues, and lesions using both spatial and semantic prompts. In Figure 3 (b), we quantitatively analyze the mutually supportive relationship between semantic-prompt and spatial-prompt in 19 segmentation tasks of the 20% split test data. On the one hand, spatial-prompt allows the model to locate the specific part in the 3D space. According to Figure 3 (b), the average Dice score of the 'bbox+text' prompt is boosted by 5.85% compared to the 'text' prompt on average. On the other hand, semantic-prompt clarifies the reference to the anatomical structure, eliminating the ambiguity of spatial-prompt and the plausible masks of multiple categories. This is reflected in Figure 3 (b) as the average Dice score of 'point+text' prompts is 4.62% higher than using 'point' prompts alone. Spatial and semantic prompts mutually support each other, ultimately endowing the model with powerful segmentation capabilities.

### Case Studies

Disambiguation via semantic-prompt.It is a notorious problem in interactive segmentation that one spatial-prompt may correspond to multiple plausible outputs . As illustrated in the images on the top left in Figure 4, three of them correspond to three anatomical concepts, namely, kidney tumor, left kidney, and the whole kidneys, while they are all plausible to the same point prompt. Similarly, in the bottom left three images, the bounding box selects the region of the liver. However, liver tumors, hepatic vessels, and the liver itself are also plausible target structures. In these cases, SAM chooses to return multiple masks to match different levels of plausible results. Unlike SAM's solution, we use semantic-prompt to clarify the targets. As shown in Figure 4, the captions below the images are the text prompts, and the masks in the images are the predictions of SegVol, which show that semantic-prompt can effectively disambiguate the spatial-prompt.

Identifying the spatial-prompt segmentation.Furthermore, we study the capability of SegVol to identify the semantic category of the spatial-prompt results. Figure 5 reveals that SegVol can give accurate semantic categories based on the spatial-prompt results. In the top left image in Figure 5, the spatial-prompt on the liver results in a 0.997 prediction score for the liver. The top right image in the sub-figure shows if the spatial-prompt is the point on the liver tumor, SegVol will output a 0.619 prediction score for the tumor category and a 0.339 prediction score for the liver based on the spatial relationship of liver tumor and liver. We implement this identification experiment by decoding the semantic prompts from a category set. The softmax function is applied to the decoding results to get the prediction probabilities of different categories. The probabilities on the initial predicted mask, driven by the spatial-prompt, are used to calculate the final classification result.

## 4 Discussion

Scalability.The scaling law of foundation models has been verified in multiple CV and NLP tasks. Since SegVol uses a transformer-based architecture and self-supervised pre-training algorithm, it has strong data and architecture scalability. In this work, we achieve the success of scaling law in 3D medical segmentation by the design of universal prompts and pseudo masks for joint learning on datasets with inconsistent annotations. The ablation study of scaling up training data shows that 1) the performance improves significantly with more training data in the 3D segmentation task, 2) SegVol has not yet reached its ceiling if more training data is provided. We believe the performance of SegVol can be continuously improved when more data and computational resources are used.

Generalizability to unseen modality.Although we develop SegVol on Computed Tomography (CT) data due to its advantages of easy acquisition, wide usage, and high resolution, we find that SegVol can generalize to other medical image modality, like MRI. Namely, the SegVol model trained only on CT data can be used to segment MRI with semantic and spatial prompts. This emerging ability demonstrates that our foundation model understands the anatomical structure of human body. We provide detailed experiments and analysis of this generalizability in Section C. The impressive generalizability makes SegVol a versatile tool in medical image analysis. We leave the joint training of SegVol on multi-modality data as the future work.

Limitations.Although SegVol shows remarkable semantic-prompt segmentation performance, there still remains gap between it and the referring volumetric segmentation. A promising solution is to construct the referring segmentation data with diverse semantic and spatial prompts, and then train SegVol on it. We leave it as the future work. More discussions can be found in Section E.

Figure 4: The four cases demonstrate that semantic-prompt can clarify the ambiguity of spatial-prompt and avoid multi-plausible outputs. Each image shows the segmentation result of SegVol using the spatial-prompt, i.e. point or bounding box, and semantic-prompt, i.e. the caption below the image.

Figure 5: We identify the semantic categories of the spatial-prompt segmentation results. Each image shows the spatial-prompt and the mask prediction. The bar charts rank the top 8 semantic categories with the highest classification probabilities. The results show that SegVol is capable of identifying the anatomical category of the segmentation mask using spatial prompts.

Broader impact.We contribute a foundation model for universal and interactive volumetric medical image segmentation, which can benefit numerous clinical study and applications. As a foundational research work, we do not see any obvious negative societal impact of the proposed method and model.

## 5 Conclusion

In this paper, we propose SegVol, a universal and interactive volumetric medical image segmentation model, supporting both spatial-prompt and semantic-prompt segmentation of more than 200 anatomical categories. We construct a large-scale dataset, which consists of 90K unlabeled CTs and 25 open-source medical datasets, to train the foundation model. We design the zoom-out-zoom-in mechanism to facilitate efficient and precise inference in the region of interest. Extensive experiments on 22 segmentation tasks demonstrate the outstanding performance of our method. Detailed ablation studies are also carried out to prove the effectiveness of the zoom-out-zoom-in mechanism, dataset scale, and multi-prompt combination strategy. As a foundation model, we believe that SegVol will advance the volumetric medical segmentation and benefit numerous downstream tasks.

Acknowledgements.This work is funded by NSFC-62306046.