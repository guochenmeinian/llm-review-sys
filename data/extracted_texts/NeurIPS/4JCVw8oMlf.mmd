# Effectively Learning Initiation Sets in

Hierarchical Reinforcement Learning

 Akhil Bagaria

Brown University

Providence, RI, USA.

akhil_bagaria@brown.edu

Equal Contribution

Ben Abbatematteo

Brown University,

Providence, RI, USA.

abba@brown.edu

&Omer Gottesman

Amazon,

New York, NY, USA.

omergott@gmail.com

&Matt Corsaro

Brown University,

Providence, RI, USA.

matthew_corsaro@brown.edu

&Sreehari Rammohan

Brown University,

Providence, RI, USA.

sreehari_rammohan@brown.edu

&George Konidaris

Brown University,

Providence, RI, USA.

gdk@cs.brown.edu

Work done while at Brown University.

###### Abstract

An agent learning an option in hierarchical reinforcement learning must solve three problems: identify the option's subgoal (termination condition), learn a policy, and learn where that policy will succeed (initiation set). The termination condition is typically identified first, but the option policy and initiation set must be learned simultaneously, which is challenging because the initiation set depends on the option policy, which changes as the agent learns. Consequently, data obtained from option execution becomes invalid over time, leading to an inaccurate initiation set that subsequently harms downstream task performance. We highlight three issues--data non-stationarity, temporal credit assignment, and pessimism--specific to learning initiation sets, and propose to address them using tools from off-policy value estimation and classification. We show that our method learns higher-quality initiation sets faster than existing methods (in MiniGrid and Montezuma's Revenge), can automatically discover promising grasps for robot manipulation (in Robosuite), and improves the performance of a state-of-the-art option discovery method in a challenging maze navigation task in MuJoCo.

## 1 Introduction

Temporal abstraction, which is crucial for scaling RL to long-horizon problems (Konidaris, 2019; Sutton et al., 2022), is elegantly captured using the options framework (Sutton et al., 1999). Unlike primitive actions that only last for a single timestep, options execute a policy until some termination condition is met. General purpose agents must possess many such options to solve a wide variety of tasks (White, 2017), but increasing the size of the action set raises both the complexity of learning and the branching factor of search (Littman et al., 1995). This necessitates a mechanism for locally pruning irrelevant options: most options do not apply in a given state and the agent can lower its effective branching factor by ignoring them (Precup et al., 1998; Wen et al., 2020). Fortunately, the options framework includes the concept of _initiation sets_ which captures the idea of local applicability: an option can only be executed from states inside the initiation set.

It is natural to view an initiation set as the set of states from which option execution is likely to succeed; in that case, learning it is straightforward when the option's policy is _fixed_: run theoption from different states, record the outcome, and learn a classifier that predicts whether option execution will succeed at a given state [Konidaris and Barto, 2009, Kaelbling and Lozano-Perez, 2017]. However, this is complicated by non-stationarity during learning: as the option policy changes, previously recorded outcome labels become invalid. How should the initiation classifier change in response? Another challenge is that initiation set learning suffers from a _pessimistic bias_: the option policy mostly improves for states _inside_ the initiation set because option execution only occurs from such states. Therefore, if a state is excluded from the initiation set early during learning, it will likely remain excluded even as the policy improves over time. This pessimistic bias causes initiation sets to mostly shrink and rarely expand [Bagaria et al., 2021], even in the face of policy improvement; this is potentially catastrophic for learning since the agent prematurely gives up on options in favor of primitive actions [Harb et al., 2018].

To address the issue of non-stationarity, we introduce the Initiation Value Function (IVF), a general value function (GVF) [Sutton et al., 2011, White, 2015] that predicts the probability that option execution will succeed from a given state. Since the IVF is purely predictive, it can be learned using tools from off-policy policy evaluation (OPE) [Voloshin et al., 2021] and does not interfere with the option's main task of maximizing its internal reward function [Sutton et al., 2023]. Unlike the classification approach described above, the IVF adapts to a changing policy: as the policy improves over time, so does the IVF's estimates of its probability of success. We show how the IVF can be used as a direct estimate of the option's initiation set or as input to a _weighted_ classifier that accounts for changes in the option policy.

To address the pessimistic bias of learning initiation sets, we expand the criterion for including a state in the option's initiation set: in addition to states from which option execution is likely to succeed, we also include states for which the option policy is most likely to _improve_. By identifying and adding these states to the initiation set, we mitigate the pessimistic bias and prevent initiation sets from collapsing as the option is learned.

For evaluation, we first measure how accurately we can deduce an option's initiation set in MiniGridFourRooms and the first screen of Montezuma's Revenge. Then, we demonstrate that our proposed methods can effectively identify promising grasps in challenging robot manipulation problems in Robosuite. Finally, by integrating our method for learning initiation sets into an existing option discovery algorithm, we solve a maze navigation problem in MuJoCo that baseline agents are unable to.

## 2 Background and Related Work

As is standard in HRL [Barto and Mahadevan, 2003], we consider problems that can be modeled as Semi Markov Decision Processes (SMDPs) \(=(,,,,)\), where \(\) is the state space, \(\) is the action space (which contains options and primitive actions), \(\) is the reward function, \(\) is the transition function and \(\) is the discount factor. An option \(o\) models temporally extended behaviors; \(o=(_{o},_{o},_{o})\), where \(_{o}\), the initiation set of the option, describes the states from which the option can be executed, \(_{o}\) is the termination or subgoal region and \(_{o}\) is the policy [Sutton et al., 1999]. When using RL to learn the option policy, an internal option subgoal reward function \(_{o}\) and timescale \(_{o}\) completes the subtask description of the option and is used to train \(_{o}\)[Precup, 2000, Barto and Mahadevan, 2003, White, 2017].

An important problem in HRL is that of **option discovery**, where algorithms focus on identifying option termination conditions (subgoals) and usually assume that all options apply everywhere [Dayan and Hinton, 1993, Bacon et al., 2017, Eysenbach et al., 2019, Machado et al., 2017]; as previously discussed, this is not a scalable strategy for developing general-purpose agents. We hypothesize that well-learned initiation sets will improve the quality of options, regardless of how they are discovered, and will eventually lead to more performant hierarchical RL agents.

General Value Functions.Value-based RL typically predicts and maximizes a single scalar reward, but a value function can be generalized to predict (and sometimes control [Jaderberg et al., 2017]) the discounted sum of any real-valued "cumulant" that can be computed from the agent's observations [Sutton et al., 2011, White, 2015]. Such Generalized Value Functions (GVFs) efficiently solve the temporal prediction tasks and can represent rich knowledge about the world [Schaul and Ring, 2013].

Initiation Sets in Robotics and Planning.Initiation sets are crucial in classical planning (Fikes et al., 1972) as well as task and motion planning (where they are referred to as _preconditions_) (Garrett et al., 2021), but they are usually designed by a domain expert (Kaelbling and Lozano-Perez, 2017). In robotics (_affordances_), they are usually programmed to avoid collisions (Xu et al., 2021) or to correspond to relevant object attributes (Sahin et al., 2007; Huang et al., 2023). In control theory (_regions of attraction_), they denote states where policies are provably stable (Tedrake, 2009; Ames and Konidaris, 2019). While our proposed methods do not come with finite-time convergence guarantees, they are arguably more general and scalable because they do not assume pre-programmed policies or additional structure such as objects and robot dynamics.

Techniques for learning initiation sets fall into \(3\) categories: classification, value-based and end-to-end.

Classification approach.When initiation sets are learned, it is most often framed as binary classification (Konidaris and Barto, 2009; Bagaria and Konidaris, 2020; Khetarpal et al., 2020; Bagaria et al., 2021). This approach is sound when the option policy is fixed (Konidaris et al., 2018) or when the affordances correspond to primitive actions (Khetarpal et al., 2020). But, this approach does not scale to the continual learning setting (Ring, 1995; Mcgovern, 2002) where new, temporally extended options keep getting discovered and their policies and initiation sets must be learned simultaneously.

Value function approach.SayCan grounds large language models using value functions of skills trained using offline RL (Brohan et al., 2023). Since the policies are pre-trained, they do not confront the challenges of jointly learning policies and their initiation sets discussed in this paper. Nica et al. (2022) learn affordances via Monte Carlo estimation (which is equivalent to binary classification under a restrictive \(0/1\) reward function), but do not account for the pessimistic bias or the temporal structure in the initiation set learning problem. Furthermore, both SayCan and Nica et al. (2022) design the option reward function so that the resulting value function can be interpreted as an affordance/initiation probability; this is a strong restriction because RL is famously sensitive to reward design (Randlov and Alstrom, 1998). Our method allows the designer to pick the reward function appropriate for their task and the initiation function is learned using the appropriate cumulant using policy evaluation. Relay Networks (Kumar et al., 2018) learn initiation regions by thresholding the critic, but have difficulty picking a threshold because it is not possible to interpret arbitrary value functions as initiation probabilities.

End-to-end approaches.IoC (Khetarpal and Precup, 2019) learns initiation sets in the Option-Critic (OC) framework; GrASP (Veeriah et al., 2022) learns affordances that are useful for Monte Carlo Tree Search (MCTS). These are promising gradient-based approaches to learning affordances, but are specific to OC and MCTS respectively. Furthermore, they both learn affordances that maximize _task_ reward, we instead focus on learning options that each represent their own subtask (White, 2017) and could later be composed to solve many downstream tasks (Barreto et al., 2019).

## 3 Effectively Learning Initiation Sets

The problem of learning initiation sets can be naturally framed as training a classifier where states on trajectories that achieve the subgoal are labeled as positive examples, and states on trajectories that fail to achieve the subgoal are labeled as negative examples. A probabilistic classifier trained on this data will predict the probability that an option will succeed from a given state, which is exactly the desired semantics of an initiation set. However, when initiation sets and policies have to be learned together, such a classifier is no longer a good estimator, for three reasons:

**Data non-stationarity.** The first reason is data non-stationarity, which refers to the phenomenon that previously collected training examples become invalid during the course of learning, but continue to impact classifier training. Consider a trajectory \(_{0}\) obtained by rolling out an option policy \(_{o}^{t_{0}}\) at time \(t_{0}\). If the policy is in the early stages of training, \(_{o}^{t_{0}}\) will likely fail to reach the subgoal region \(_{o}\) and \(_{0}\) will be labeled as a negative example (\(Y(s)=0, s_{0}\)). At some future time \(t_{1}\), the policy might improve, but the states along \(_{0}\) will continue to have a negative label, which means that they will continue to be outside the initiation set. As a result, we will fail to capture the growing competence/reachability of \(_{o}^{t>t_{0}}\). A naive solution to this problem would be to discard old training examples, but not only would that be sample-inefficient, it is also unclear at what point a training example is "old enough" to be discarded.

**Pessimistic bias.** The second reason, pessimism, refers to the fact that once states are deemed to be outside the initiation set, they are unlikely to switch labels because the option has low probability of being executed from them, and so policy improvement from these states is unlikely (Figure 1 (_right_)).

**Temporal Structure.** The third reason is that classification (or equivalently, Monte Carlo learning (Sutton and Barto, 2018)) does not exploit the temporal structure in the initiation set learning problem. Figure 1 (_left_) shows two trajectories sampled from the same option's policy: \(_{1}\) reaches the goal and \(_{2}\) does not. Any form of Monte Carlo estimation using this data would assign an initiation probability of \(0\) to most states in trajectory \(_{1}\). However, this does not assign enough credit to most of the good decisions made along \(_{1}\)--the only way to achieve that would be via _temporal bootstrapping_. This form of generalization is not possible when using classification.

To address non-stationarity and to exploit temporal structure, we formulate a new general value function, the _Initiation Value Function_ (IVF; Section 3.1), learned using temporal difference (TD) methods (Sutton, 1988). To address pessimism, we augment the initiation set with states from which the option policy is most likely to _improve_ (Section 3.3).

### Initiation Value Function (IVF)

An option \(o\)'s initiation set \(_{o}\) is the set of states from which the option policy \(_{o}\) can succeed with high probability: \(_{o}=\{s:(s^{}_{o}|S=s,A=o)>T\}\) (\(T\) is a predefined threshold). Treating the success condition of the option as a cumulant \(c_{o}:s\{0,1\}\) and setting the timescale \(_{c_{o}}:=1\), the corresponding GVF

\[^{_{o}}(s_{t})=_{_{o}}_{t=0}^{t=H_{o}}c_ {o}(s_{t+1})|s_{t+1}(s_{t},_{o}(s_{t}))=(c_ {o}=1|S=s_{t},O=o)\]

represents the initiation probability at state \(s_{t}\) (\(H_{o}\) is the option horizon). Note that \(^{_{o}}\) is different from \(V_{o}\), the value function used by the option policy for control; this is because \(_{o}\) maximizes an arbitrary reward function \(_{o}\) and so the resulting value function \(V_{o}\), which approximates the value of the optimal option policy \(V^{_{o}}\), cannot be interpreted as an initiation probability.

Using the IVF as an initiation set.To directly use the IVF as the initiation set, we need to pick a threshold \(T\) above which a state is deemed to be in the option's initiation set. Previous work (Kumar et al., 2018) reused the option value function \(V_{o}\) as the IVF, but had to develop heuristic schemes to pick a threshold. But since \(^{_{o}}_{}\) outputs an interpretable number between \(0\) and \(1\) (we use a sigmoid layer at the end of \(\) to enforce this), it is easy to threshold, regardless of \(_{o}\).

Learning the IVF.Computing the probability of success of a given policy \(^{t}_{o}\) is equivalent to the problem of off-policy policy evaluation. Our approach to policy evaluation _must_ be sample-efficient--if it is not, the option policy will change before we are able to sufficiently evaluate it (Sutton et al.,

Figure 1: Learning an initiation set for an option targeting subgoal \(_{o}\) (star). _(left)_ Temporal structure in initiation set learning: even though \(_{1}\) (black) fails to reach the goal, states along it should get credit because of the portion shared with the successful trajectory \(_{2}\) (red); this does not happen using a simple classification approach. _(middle)_ the initiation value function (IVF) \(_{o}\) (Sec 3.1) is shown in green: darker the shade, higher the value; classification examples should be re-weighted to account for a changing policy. _(right)_ Pessimistic bias causes initiation sets to shrink (blue \(\) red) over time.

2007). Furthermore, the initiation cumulant \(c_{o}\) is a sparse binary function, which makes learning with the Monte Carlo estimator high variance and sample inefficient. This once again underscores the importance of exploiting temporal structure of the problem using TD-learning (we use TD(0)) to estimate \(_{}^{_{o}}\), which unlike a classifier, is able to propagate value from partial trajectories (as illustrated in Figure 1_(left)_). In this view, using a classifier is equivalent to using a Monte Carlo estimate of the IVF: high-variance, sample inefficient, and unable to bootstrap.

### Combining Classification and the Initiation Value Function (IVF)

In Section 3.1 we connected the initiation set to a GVF, which allows us to use all the tools of value-based RL (Sutton and Barto, 2018) to learn the initiation set. However, the classification approach mentioned earlier has some attractive qualities: specifically, classification is easier than regression (Bishop and Nasrabadi, 2006) and the supervised learning community has developed powerful tools for learning classifiers using neural networks and cross-entropy loss (Goodfellow et al., 2016). To get the best of both worlds, we propose an additional method that combines value estimation and classification.

Recall that binary classification can be described as the process of finding parameters \(\) that minimize the cross-entropy loss \(()\) in the training data \(=\{(s_{i},Y_{i})\}_{i=0}^{}\) where \(s_{i}\) are the inputs (states in our case) and \(Y_{i}\) are the classification labels (whether option execution was successful from \(s\)). To deal with non-stationarity, we instead use the _weighted_ binary cross-entropy loss \(_{w}()\) which minimizes loss over a dataset \(=\{s_{i},Y_{i},w_{t}(s_{i})\}\) where \(w_{t}(s_{i})\), a state-dependent weight, represents the desired contribution of the training example to the classification loss (Natarajan et al., 2013).

How should we set the weights \(w_{t}(s)\) of a training example \((s_{i},Y_{i})\) in a way that reflects the evolving competence of the option policy? Since the IVF evaluates the success probability of the current option policy \(_{o}^{t}\), we can heuristically use it as a reflection of our certainty in the training example \((s_{i},Y_{i})\)--positively (negatively) labeled states contribute more to \(_{w}\) when the IVF prediction is high (low). Specifically, the weighting function is defined via the following simple equation:

\[w_{t}(s)=Y_{s}(s)+(1-Y_{s})(1-(s)). \]

As the IVF estimates that the option policy is becoming more competent at state \(s\), the classifier's uncertainty about a negative example at \(s\) will increase, thereby causing that negative example to contribute less to the classifier's decision boundary. Similarly, if the policy degrades at \(s\), then the contribution of a positive label at \(s\) to \(_{w}\) will go down. By weighing training examples in this way, a binary classifier is able to adapt to data non-stationarity that results from a changing option policy.

At the end of every option execution, we therefore recompute weights using Equation 1 and re-train option initiation classifiers using a weighted cross-entropy loss \(_{w}\).

Policy Improvement Prior.We can use the fact that policies in RL usually improve over time to guide classifier training. To do this, we set the weight of positive examples \(w_{t}(s)\) as 1 throughout training. The underlying idea is that a state which leads to success once is likely to do so again with an even better policy. The weights of negative examples are allowed to vary with the IVF as in Eq 1.

### Overcoming Pessimistic Bias

Unsuccessful option trajectories cause the initiation set to shrink. When a state is outside the initiation set, the option can no longer be executed from there. So even if the option could succeed from that state in the future, it remains outside the initiation set and the option ends up with a smaller region of competence than it needs to. This issue, which we call the pessimistic bias of learning initiation sets, can prevent the learning of useful options and lowers the effectiveness of hierarchical RL agents3.

To mitigate this pessimistic bias, we expand the initiation set to also include states from which policy improvement is most likely:

\[_{o}=\{s:_{o}(s)+(s)>T,s\}.\]

Effectively identifying such states in high-dimensional spaces is the topic of bonus-based exploration in deep RL (Taiga et al., 2019). We propose two simple approaches:1. **Competence progress** attempts to capture regions where a policy is either improving or regressing (Simsek and Barto, 2006; Stout and Barto, 2010; Baranes and Oudeyer, 2013). This can be computed as changes in the IVF over time: \(_{1}(s)=|_{o}^{t}(s)-_{o}^{t-K}(s)|\), where \(_{o}^{t}\) is the current IVF and \(_{o}^{t-K}\) is the IVF estimate \(K\) timesteps ago (obtained using the target network).
2. **Count-based bonus** approach keeps track of the number of times \(N(s,o)\) option \(o\) has been executed from state \(s\). This is then converted into an uncertainty measure: \(_{2}(s)=c/\), where \(c\) is a scalar hyperparameter (Strehl and Littman, 2008).

We use count-based bonuses in problems where tabular counts are readily available; otherwise, we use competence progress.

## 4 Experiments

We aim to evaluate whether our methods can improve hierarchical RL. First, we evaluate whether they result in better, more efficiently learned initiation sets. Second, we test if better initiation set learning improves option learning as a whole. Finally, we incorporate our changes into a state-of-the-art skill discovery method and check if resulting agent is able to outperform the baseline in a sparse-reward continuous control problem.

Implementation Details.Option policies are learned using Rainbow (Hessel et al., 2018) when the action-space is discrete and TD3 (Fujimoto et al., 2018) when it is continuous. Following Bagaria et al. (2021), all options share the same UVFA (Schaul et al., 2015) but condition it using their own subgoals. The IVF is learned using Fitted Q-Evaluation (Le et al., 2019), prioritized experience replay (Schaul et al., 2016) and target networks (Mnih et al., 2015). The IVF Q-function and initiation classifier are parameterized using neural networks that have the same architecture as the Rainbow/TD3. Each option has a "gestation period" of \(5\)(Konidaris and Barto, 2009), which means that before the option sees \(5\) successful trajectories, its initiation set is optimistically initialized to be true everywhere. Since the training data for classification can be severely imbalanced, we use the standard technique of upweighting the minority class (Japkowicz and Stephen, 2002).

### Measuring the Quality of Initiation Sets

Before comparing different methods based on task performance, we specifically test the quality of initiation sets learned in MiniGrid-FourRooms(Chevalier-Boisvert et al., 2018) and the first screen of Montezuma's Revenge(Bellemare et al., 2013). In both domains, observations are \(84 84\) images and the action-space is discrete. For this experiment, we design start states \(_{0}\) in each domain--in MiniGrid-FourRooms, \(_{0}\) is the set of all states (since this is a small tabular domain) and in Montezuma's Revenge, \(_{0}\) is a series of \(100\) states scattered across the first screen (more details in the appendix). For the purpose of evaluating our initiation set learning algorithms, at every episode, we sweep through states \(s_{0}\) and reset the simulator to \(s\). Option termination conditions are also hand-defined: in FourRooms, we create options that target the center of each room; in Montezuma's Revenge, we define five options: those that navigate the player to each of the two doors, one that navigates the player to the bottom-right and one to the bottom-left of the first screen and finally an option that attempts to get the key.

Initiation Set Accuracy.At each state \(s_{0}\), we record the initiation decision made by the learning algorithm as \(}_{o}(s;)\{0,1\}\). We then execute the option policy \(_{o}\) from that state and record whether or not the agent reached the option's subgoal as \(Y_{s}\{0,1\}\). Accuracy at state \(s\), for option \(o\) is then given by \((}_{o}(s;)=Y_{s})\). This process is repeated several times for all options \(o\) and start states \(s_{0}\). To be faithful to how options are learned in the online RL setting, we only use the trajectory obtained by running the option for updating the initiation classifier and the option policy if the learned classifier returned true at the start state, i.e, \(}_{o}(s;)=1\) (in which case \(_{o}\) is updated using Rainbow (Hessel et al., 2018); pseudocode in Appendix C.1).

Initiation Set Size.Although accuracy is a natural evaluation metric, it does not fully capture the quality of the learned initiation set. For example, if the predicted initiation function returns false everywhere (i.e, if \(_{o}(s;)=0, s_{0}\)), then the option policy would not get new data to improve. If the policy never improves, its true initiation set could also collapse; while such an initiation learner would register high accuracy, the option would not be useful. As a result, we additionally measure the normalized size of the "true" initiation set \(|Y_{s}|\): this is the fraction of start states \(_{0}\) from which Monte Carlo rollouts of the policy succeeds. A well-learned initiation set is not only accurate, it is also as large as possible,4 reflecting a large region of option policy competence.

Competing methods.We compare the accuracy and size of the initiation sets learned by:

* _Baseline binary._ Binary classifier used to learn the initiation set. This is what is used in essentially all prior work and is the only baseline method in this experiment.
* _IVF._ Threshold the Initiation Value Function as discussed in Section 3.1.
* _Weighted._ This is a weighted binary classifier discussed in Section 3.2.

Both the _IVF_ and _Weighted_ approaches use the competence progress exploration bonus described in Section 3.3, its impact is ablated in Appendix E.

Discussion of results.Figure 2 shows that our proposed methods significantly outperform the baseline binary classification approach, which learns classifiers that are less accurate and smaller than those learned using other techniques. Furthermore, the accuracy of the baseline binary classifier _falls_ over time; this is because even though the policy improves (and hence the size of the true initiation set increases), the learned initiation set remains small and unable to adapt to an improving policy. By modifying the binary classifier using the weighting scheme described in Section 3.2, we are able to learn more accurate initiation sets that reflect highly competent option policies. Interestingly, the optimistic version of the IVF is necessary to learn good initiation sets in Montezuma's Revenge, but it underperforms compared to the plain IVF in FourRooms; we hypothesize that this is because FourRooms is a small and simple enough domain that an exploration bonus is not strictly necessary.

### Robot Manipulation: Identifying Promising Grasps

In the previous section, we showed that our methods can lead to higher quality initiation sets; now we evaluate whether that can in turn improve option learning as a whole. We adopt robot manipulation as a testbed because it is a natural problem setting for evaluating initiation set learning algorithms--for example, consider a robot manipulating a hammer: not only should it choose a feasible, stable grasp on the hammer, but it should also choose one that would allow it to subsequently drive a nail. This problem, referred to as _task-specific grasping_ in robotics, and has been studied extensively for a fixed policy (Kokic et al., 2020; Fang et al., 2020; Zhao et al., 2021; Wen et al., 2022; Schiavi et al., 2022); here we use RL to deduce which grasps are most likely to afford success while simultaneously learning the skill policies. Furthermore, dense reward functions are often necessary to train RL policies in

Figure 2: Measuring the quality of initiation sets in MiniGrid-FourRooms (left) and the first screen of Montezumaâ€™s Revenge (right). Solid lines denote mean accuracy and initiation set size, shaded regions denote standard error. All curves are averaged over all state-option pairs and \(5\) random seeds.

robot manipulation domains (Gu et al., 2017); this section highlights that initiation probabilities can be learned even when the reward function used to train \(_{o}\) is not sparse like the initiation cumulant.

We use three constrained manipulation tasks in robosuite(Zhu et al., 2020): opening a door, flipping a lever, and manipulating a sliding mechanism. Each task is modeled as a single option and the policy and initiation set are learned simultaneously. Each environment has \(250\) possible start states \(_{0}\). At the start of each episode, a configuration is sampled from the initiation set learner:

\[s_{0}_{o}(s)}{_{s^{}_{0}}_{o}(s^{})}, s_{0};\]

the manipulator is then reset to \(s_{0}\). The continuous observation space is \(52\)-dimensional, the continuous action space is \(13\)-dimensional; we use TD3 for policy learning (Fujimoto et al., 2018).

Our algorithms are compared against two baselines: _Random_, which selects an arm configuration at random at the start of each episode5, and _Binary_ which treats initiation learning as vanilla binary classification. Our algorithms, _IVF_ and _Weighted_, employ the optimistic bias discussed in Section 3.3 by adding a count-based bonus to the predicted initiation probability.

Task success rates are shown in Figure 3. The Door task is the simplest: most of the grasp candidates afford task success. As a result, the _Random_ and _Binary_ baselines are competitive with our algorithms. The Lever and Slide tasks are more challenging as the number of promising grasps is significantly lower. In these domains, only the _Weighted_ variant is able to consistently recover a policy with success rate above \(50\%\).

Figure 4 shows that our methods can identify good grasp poses without task-specific engineering or human demonstrations. This is impressive given that human data (Mandikal and Grauman, 2021, 2022) or heuristics like image segmentation (Kokic et al., 2017; Rosen et al., 2022) are typically required to learn these affordances efficiently, even in the case of a fixed policy. Additional visualizations and ablations can be found in Appendix D and E.

### Improving Option Discovery

Finally, to evaluate whether better option learning can improve option discovery, we integrate our approach into an existing state-of-the-art algorithm: deep skill chaining (DSC) (Konidaris and Barto, 2009; Bagaria and Konidaris, 2020). DSC learns a collection of options so that the subgoal region \(_{o_{i}}\) of an option \(o_{i}\) is the initiation region \(_{o_{j}}\) of another option \(o_{j}\); by learning options that funnel into each other's initiation sets, DSC learns how to sequentially compose options to reliably achieve a goal. It does so by first learning an option that reaches the task goal, then another option that targets the first option's initiation set and so on, until the start-state is inside the initiation set of some option; for more details, please refer to Appendix C.3 and the original papers (Konidaris and Barto, 2009; Bagaria and Konidaris, 2020). We chose to integrate our techniques with DSC because of its focus on learning initiation sets jointly with options in an online and incremental reinforcement learning setting and because the quality of initiation sets is crucial to its performance and stability (Bagaria et al., 2021a).

Figure 3: Task success rate for manipulation domains aggregated over \(4\) random seeds.

Figure 4: Examples of promising grasp poses _(top row)_ in Door _(left)_, Lever _(middle)_ and Slide _(right)_: these are high probability samples from our initiation function; contrasted with bad grasp poses _(bottom row)_, which are low probability samples and are ruled out by the initiation function.

Figure 5: _(left)_ Comparing the performance of baseline DSC [Bagaria et al., 2021a] to versions of DSC with our initiation learning techniques. Solid lines denote average success rate over \(5\) random seeds, shaded regions denote standard error. _(right)_ A visualization of initiation sets learned in Ant Medium-Maze, where the ant robot has to navigate from the bottom-right to the top-left. Each color denotes the initiation set of a different option; although the plot only shows the location of the ant in the maze, the initiation set is learned using the full \(30\)-dimensional state.

Experimental setup.We compare baseline DSC to versions that modify the way in which option initiation sets are learned. We use the Ant Medium Maze environment where the agent gets a sparse terminating reward of \(0\) for reaching the goal and \(-1\) every other step; each episode lasts a maximum of \(1000\) steps (Fu et al., 2020; Todorov et al., 2012). This is a challenging problem that cannot be solved with flat RL algorithms in a reasonable amount of time. Baseline DSC was able to solve this problem by learning initiation sets defined over a subset of the state (\(x,y\) location of the ant) (Bagaria et al., 2021), but here, we do not assume access to such privileged information--all methods learn initiation sets using a dense neural network that maps the full \(30\)-dimensional continuous state to an initiation probability.

We used the author implementation (Bagaria et al., 2021) where option policies are parameterized using goal-conditioned value functions (Schaul et al., 2015). When using the IVF directly as the initiation set, we mimic the structure of main agent and use a goal-conditioned value function to represent the IVF; the final initiation set is defined as

\[_{o}=s:_{g_{o}}_{}(s,g)>T,  s}.\]

More details about the task and implementation details can be found in Appendix A and C.3.

Figure 5_(left)_ shows that baseline DSC is unable to solve Medium Ant-Maze; but by changing the way options learn their initiation sets, we are able to solve the problem quickly and reliably. The weighted classifier slightly underperforms the pure IVF approach in terms of mean return, but it has lower variance across runs. Figure 5_(right)_ visualizes the initiation sets learned by the weighted classification approach; the figure makes it clear that different learned options specialize in different parts of the maze.

## 5 Conclusion

Learning initiation sets is a critical component of skill discovery, but treating it as binary classification misses key characteristics of the problem. Specifically, it does not address the non-stationarity that results from a continually changing policy, the pessimistic bias of learning initiation sets online, or the temporal structure in the problem. To address these challenges, we proposed the Initiation Value Function (IVF), a general value function tailored specifically to learning initiation sets. We used the IVF directly via thresholding and also via a weighted binary classifier which adapts to changing option policies. Through our experiments, we showed that our proposals lead to higher quality initiation sets, can lead to faster learning of a single option and boost the performance of an existing skill discovery algorithm.

A limitation of the IVF cumulant proposed in Section 3.1 is that only applies for goal-reaching options, which although is quite general, it is not universal. For example, if the option's task is to maximize the velocity of a robot and there is no specific target velocity, then we could not write down a \(0/1\) cumulant that faithfully describes that subtask. Designing cumulants that result in initiation probabilities for general option subtasks is an important avenue for future work.