# Understanding Multi-phase Optimization Dynamics

and Rich Nonlinear Behaviors of ReLU Networks

 Mingze Wang

School of Mathematical Sciences

Peking University

Beijing, 100081, P.R. China

mingzewang@stu.pku.edu.cn &Chao Ma

Department of Mathematics

Stanford University

Stanford, CA 94305

chaoma@stanford.edu

###### Abstract

The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such as initial condensation, saddle-to-plateau dynamics, plateau escape, changes of activation patterns, learning with increasing complexity, etc.

## 1 Introduction

Deep learning shows its remarkable capabilities across various fields of applications. However, the theoretical understanding of its great success still has a long way to go. Among all theoretical topics, one of the most crucial aspect is the understanding of the optimization dynamics of deep neural network (NN), particularly the dynamics produced by Gradient Descent (GD) and its variants. This topic is highly challenging due to the highly non-convex loss landscape and existing works usually work with settings that do not align well with realistic practices. For instance, the extensively studied Neural Tangent Kernel (NTK) theory (Jacot et al., 2018; Du et al., 2018, 2019; Zou et al., 2018; Allen-Zhu et al., 2019) proves the global convergence of Stochastic gradient descent (SGD) to zero training error for highly over-parameterized neural networks; however, the optimization behaviors are similar to kernel methods and do not exhibit nonlinear behaviors, because neurons remain close to their initialization throughout training.

In reality, however, the training of practical networks can exhibit plenty of nonlinear behaviors (Chizat and Bach, 2018; Mei et al., 2019; Woodworth et al., 2020). In the initial stage of the training, a prevalent nonlinear phenomenon induced by small initialization is _initial condensation_(Maennel et al., 2018; Luo et al., 2021), where neurons condense onto a few isolated orientations. At the end of training, NNs trained by GD can _directionally converge_ to the KKT points of some constrained max-margin problem (Nacson et al., 2019; Lyu and Li, 2019; Ji and Telgarsky, 2020). However, KKT points are not generally unique, and determining which direction GD converges to can be challenging. Nonlinear training behaviors besides initial and terminating stages of optimization are also numerous. For example, for square loss, Jacot et al. (2021) investigates the _saddle-to-saddle_ dynamics where GD traverses a sequence of saddles during training, but it is unclear whether similar behavior canoccur for classification tasks using exp-tailed loss. Moreover, while in lazy regime, most activation patterns do not change during training ReLU networks, it remains uncertain when and how _activation patterns evolve_ beyond lazy regime. Additionally, while it is generally conjectured that GD _learns functions of increasing complexity_(Nakkiran et al., 2019), this perspective has yet to be proven.

As reviewed in Section 2, works have been done to analyze and explain the nonlinear training behaviors listed above. However, due to the complexity of the training dynamics, most existing works only focus on one phenomenon and conduct analysis on a certain stage of the training process. Few attempts have been done to derive a full characterization of the whole training dynamics from the initialization to convergence, and the settings adopted by these works are usually too simple to capture many nonlinear behaviors (Phuong and Lampert, 2021; Lyu et al., 2021; Wang and Ma, 2022; Boursier et al., 2022).

**In this work**, we make an attempt to theoretically describe the whole neural network training dynamics beyond the linear regime, in a setting that many nonlinear behaviors manifest. Specifically, We analyze the training process of a two-layer ReLU network trained by Gradient Flow (GF) on a linearly separable data. In this setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal multiple phases in training process, and show a general simplifying-to-complicating learning trend by detailed analysis of each phase. Specifically, by our meticulous theoretical analysis of the whole training process, we precisely identify **four different phases** that exhibit **numerous nonlinear behaviors**. In Phase I, _initial condensation and simplification_ occur as living neurons rapidly condense in two different directions. Meanwhile, GF _escapes from the saddle_ around initialization. In Phase II, GF _gets stuck into the plateau_ of training accuracy for a long time, then _escapes_. In Phase III, a significant number of neurons are _deactivated_, leading to _self-simplification_ of the network, then GF tries to learn using the almost simplest network. In Phase IV, a considerable number of neurons are _reactivated_, causing _self-complication_ of the network. Finally, GF _converges towards an initialization-dependent direction_, and this direction is _not even a local max margin direction_. Overall, the whole training process exhibits a remarkable _simplifying-to-complicating_ behavior.

## 2 Other Related Works

Initial condensation phenomenon are studied in (Maennel et al., 2018; Luo et al., 2021; Zhou et al., 2022; Abbe et al., 2022; Chen et al., 2023; Liu et al., 2021; Boursier et al., 2022). Theoretically, Lyu et al. (2021); Boursier et al. (2022) analyze the condensation directions under their settings, which are some types of data average. Additionally, Atanasov et al. (2022) demonstrates that NNs in the rich feature learning regime learn a kernel machine due to the silent alignment phenomenon, similar to the initial condensation.

The end of training is extensively studied for classification tasks. Specifically, for classification with exponentially-tailed loss functions, if all the training data can be classified correctly, NNs trained by GD converge to the KKT directions of some constrained max-margin problem (Nacson et al., 2019; Lyu and Li, 2019; Chizat and Bach, 2020; Ji and Telgarsky, 2020; Kunin et al., 2023). In (Phuong and Lampert, 2021; Lyu et al., 2021), they analyze entire training dynamics and derive specific convergent directions that only depend on the data. Furthermore, another famous phenomenon in the end of training is the neural collapse (Papyan et al., 2020; Fang et al., 2021; Zhu et al., 2021; Han et al., 2021), which says the features represented by over-parameterized neural networks for data in a same class will collapse to one point, and such points for all classes converge to a simplex equiangular tight frame.

Saddle-to-saddle dynamics are explored for square loss in (Jacot et al., 2021; Zhang et al., 2022; Boursier et al., 2022; Pesme and Flammarion, 2023; Abbe et al., 2023). Furthermore, learning of increasing complexity, also called simplifying-to-complicating or frequency-principle, is investigated in (Arpit et al., 2017; Nakkiran et al., 2019; Xu et al., 2019; Rahaman et al., 2019).

Beyond lazy regime and local analysis, Phuong and Lampert (2021); Lyu et al. (2021); Wang and Ma (2022); Boursier et al. (2022) also characterize the whole training dynamics and exhibit a few of nonlinear behaviors. Specifically, Lyu et al. (2021) studies the training dynamics of GF on Leaky ReLU networks, which differ from ReLU networks because Leaky ReLU is always activated on any data. In (Safran et al., 2022), they studies the dynamics of GF on one dimensional dataset, and characterizes the effective number of linear regions. In (Brutzkus et al., 2017), they studies the dynamics of SGD on Leaky ReLU networks and linearly separable dataset. Moreover, Boursier et al. (2022) characterizes the dynamics on orthogonally data for square loss. The studies closest to our work are Phuong and Lampert (2021); Wang and Ma (2022), exploring the complete dynamics on classifying orthogonally separable data. However, this data is easy to learn, and all the features can be learned rapidly (accuracy=\(100\%\)) in initial training, followed by lazy training (activation patterns do not change). Unfortunately, this simplicity does not hold true for actual tasks on much more complex data, and NNs can only learn some features in initial training, which complicates the overall learning process. Furthermore, we provide a detailed comparison between our results and these works in Section 5. Another related work (Saxe et al., 2022) introduces a novel bias of learning dynamics: toward shared representations. This idea and the view of gating networks are enlightening for extending our two-layer theory to deep ReLU neural networks.

Our work also investigates the max-margin implicit bias of ReLU neural networks, and related works have been listed above. Although in homogenized neural networks such as ReLU, GD implicitly converges to a KKT point of the max-margin problem, it is still unclear where it is an actual optimum. A recent work (Vardi et al., 2022) showed that in many cases, the converged KKT point is not even a local optimum of the max margin problem. Besides, there are many other attempts to explain the implicit bias of deep learning (Vardi, 2023). Another popular implicit bias is the flat minima bias (Hochreiter and Schmidhuber, 1997; Keskar et al., 2016). Recent studies (Wu et al., 2018; Blanc et al., 2020; Ma and Ying, 2021; Li et al., 2021; Mulayoff et al., 2021; Wu et al., 2022; Wu and Su, 2023) provided explanations for why SGD favors flat minima and flat minima generalize well.

## 3 Preliminaries

**Basic Notations.** We use bold letters for vectors or matrices and lowercase letters for scalars, e.g. \(=(x_{1},,x_{d})^{}^{d}\) and \(=(P_{ij})_{m_{1} m_{2}}^{m_{1} m_{2}}\). We use \(,\) for the standard Euclidean inner product between two vectors, and \(\) for the \(l_{2}\) norm of a vector or the spectral norm of a matrix. We use progressive representation \(,,\) to hide absolute positive constants. For any positive integer \(n\), let \([n]=\{1,,n\}\). Denote by \((,)\) the Gaussian distribution with mean \(\) and covariance matrix \(\), \(()\) the uniform distribution on a set \(\). Denote by \(\{E\}\) the indicator function for an event \(E\).

### Binary Classification with Two-layer ReLU Networks

**Binary classification.** In this paper, we consider the binary classification problem. We are given \(n\) training data \(=\{(_{i},y_{i})\}_{i=1}^{n}^{d}\{ 1\}\). Let \(f(;)\) be a neural network model parameterized by \(\), and aim to minimize the empirical risk given by:

\[()=_{i=1}^{n}(y_{i}f(_{i};)),\] (1)

where \(():\) is the exponential-type loss function (Soudry et al., 2018; Lyu and Li, 2019) for classification tasks, including the most popular classification losses: exponential loss, logistic loss, and cross-entropy loss. Our analysis focuses on the exponential loss \((z)=e^{-z}\), while our method can be extended to logistic loss and cross-entropy loss.

**Two-layer ReLU Network.** Throughout the following sections, we consider two-layer ReLU neural networks comprising \(m\) neurons defined as

\[f(;)=_{k=1}^{m}a_{k}(_{k}^{}),\]

where \((z)=\{z,0\}\) is the ReLU activation function, \(_{1},_{m}^{d}\) are the weights in the first layer, \(a_{1},,a_{m}\) are the weights in the second layer. And we consider the case that the weights in the second layer are fixed, which is a common setting used in previous studies (Arora et al., 2019; Chatterji et al., 2021). We use \(=(_{1}^{},,_{m}^{})^{}^{md}\) to denote the concatenation of all trainable weights.

### Gradient Flow Starting from Random Initialization

**Gradient Flow.** As the limiting dynamics of (Stochastic) Gradient Descent with infinitesimal learning rate (Li et al., 2017, 2019), we study the following Gradient Flow (GF) on the objective function (1):

\[(t)}{t}-^{} ((t)), t 0.\] (2)

Notice that the ReLU is not differentiable at \(0\), and therefore, the dynamics is defined as a subgradient inclusion flow (Bolte et al., 2010). Here, \(^{}\) denotes the Clarke subdifferential, which is a generalization of the derivative for non-differentiable functions. Additionally, to address the potential non-uniqueness of gradient flow trajectories, we adopt the definition of solutions for discontinuous systems (Filippov, 2013). For formal definitions, please refer to Appendix B, G, and H.

**Random Initialization.** We consider GF (2) starting from the following initialization:

\[_{k}(0) }}{{}}}{}(^{d-1})a_{k}=s_{k}}{}k [m];\] \[s_{k}=1k[m/2];\;s_{k}=-1k[m]-[m/2].\]

Here, \(0<_{1}<_{2} 1\) control the initialization scale. It is worth noting that since the distribution \((,_{d}/d)\) is close to \((^{d-1})\) in high-dimensional settings, our result can be extended to the initialization \(_{k}}}{{}}(,_{1}^{2}_{d}/md)\) with high probability guarantees.

### Linearly Separable Data beyond Orthogonally Separable

In previous works (Phuong and Lampert, 2021; Wang and Ma, 2022), a special case of the linearly separable dataset was investigated, namely "orthogonally separable". A training dataset is orthogonally separable when \(_{i},_{j} 0\) for \(i,j\) in the same class, and \(_{i},_{j} 0\) for \(i,j\) in different classes. As mentioned in Section 2, in this case, GF can learn all features and achieve \(100\%\) training accuracy quickly, followed by lazy training. In this work, we consider data that is more difficult to learn, which leads to more complicated optimization dynamics. Specifically, we consider the following data.

**Assumption 3.1**.: Consider the linearly separable dataset \(=\{(_{i},y_{i})\}_{i[n]}^{d} \) such that

\[(_{i},y_{i})=(_{+},1),\;i[n_{+}]\\ (_{-},-1),\;i[n]-[n_{+}]_{+}, _{-}^{d-1}(0,/2)n_{+},n_{-}n=n_{+}+n_{-}\) We also use \(p:=n_{+}/n_{-}\) to denote the ratio of \(n_{+}\) and \(n_{-}\), which measures the class imbalance. Furthermore, we assume \(p>1\).

**Remark 3.2**.: We focus on the training dataset satisfying Assumption 3.1 with a small \( 1\). The margin of the dataset is \((/2)\), which implies that the separability of this data is much weaker than that of orthogonal separable data. Additionally, the condition \(p>1\) merely requires a slight imbalance in the data. These two properties work together to produce rich nonlinear behaviors during training.

## 4 Characterization of Four-phase Optimization Dynamics

In this section, we study the whole optimization dynamics of GF (2) starting from random initialization when training the two-layer ReLU network on linearly separable dataset satisfying Assumption 3.1 and using the loss function (1). To begin with, we introduce some additional notations.

**Additional Notations.** First, we identify several crucial data-dependent directions under Assumption 3.1. These include two directions that are orthogonal to the data, defined as \(_{+}^{}:=_{-}-(_{-}, _{+})_{+}}{\|_{-}-(_{- },_{+})_{+}\|}\) and \(_{-}^{}:=_{+}-(_{+}, _{-})_{-}}{\|_{+}-(_{ +},_{-})_{-}\|}\), which satisfy \(_{+},_{+}^{}= _{-},_{-}^{}=0\). Additionally, we define the label-average data direction as \(:=}{\|\|}\) where \(=_{i=1}^{n}y_{i}_{i}\). One can verify that \(,_{+}>0\) and \(,_{-}>0\) under the condition \(p>1\). In Figure 3, we visualize these directions.

Second, we use the following notations to denote important quantities during the GF training process. We denote the prediction on \(_{+}\) and \(_{-}\) by \(f_{+}(t):=f(_{+};(t)),f_{-}(t):=f(_{-};(t))\). We use\((t):=_{i=1}^{n}\{y_{i}f(_{i}; (t))>0\}\) to denote the training accuracy at time \(t\). For each neuron \(k[m]\), we use \(_{k}(t):=(t)/(t)\) and \(_{k}(t):=(t)\) to denote its direction and norm, respectively. To capture the activation dynamics of each neuron \(k[m]\) on each data, we use \(_{k}^{+}(t):=(_{k}(t),_{+})\) to record whether the \(k\)-th neuron is activated with respect to \(_{+}\), and \(_{k}^{-}(t):=(_{k}(t),_{-})\) defined similarly, which we call ReLU activation patterns.

### A Brief Overview of four-phase Optimization Dynamics

We illustrate different phases in the training dynamics by a numerical example. Specifically, we train a network on the dataset that satisfies Assumption 3.1 with \(p=4\) and \(=/15\). The directions and magnitudes of the neurons at some important times are shown in Figure 1, reflecting four different phases on the training behavior and activation patterns. More experiment details and results can be found in Appendix A.1.

From Fig 1(a) to (b) is the **Phase I** of the dynamics, marked by a condensation of neurons. Although the initial directions are random, we see that all neurons are rapidly divided into three categories: living positive neurons \((k_{+})\) and living negative neurons \((k_{-})\) condense in one direction each (\(\) and \(_{+}^{}\)), while other neurons \((k_{+}_{-})\) are deactivated forever. From the perspective of loss landscape, GF rapidly escapes from the saddle near \(\) where the loss gradient vanishes.

From Fig 1(b) to (c) is the **Phase II** of the dynamics, in which GF gets stuck into a plateau with training accuracy \(\) for a long time \(T_{}\) before escaping. Once the dynamics escapes from the plateau, the training accuracy rises to a perfect \(100\%\). Moreover, activation patterns do not change in this phase.

From Fig 1(c) to (d) is the **Phase III** of the dynamics. The phase transition from phase II to phase III sees a rapid deactivation of all the living positive neurons \(k_{+}\) on \(_{-}\) rapidly, while other activation patterns are unchanged. This leads to a simpler network in phase III, in which only living positive neurons (in \(_{+}\)) predict \(_{+}\), and only living negative neurons (in \(_{-}\)) predict \(_{-}\). Hence, in this phase the GF tries to learn the training data using almost the simplest network by only changing the norms of the neurons.

Finally, Fig 1(d) to (e) shows **Phase IV**, starting from another "phase transition" when all the living negative neurons (\(k_{-}\)) reactivate simultaneously on \(_{+}\). This leads to a more complicated network. After the phase transition, the activation patterns no longer change, and the neurons eventually converges towards some specific directions dependent on both data and initialization. Additionally, this direction is not even the local optimal max margin direction.

**Overall**, the whole dynamics exhibit a simplifying-to-complicating learning trend.

In the following four subsections, we present a meticulously detailed and comprehensive depiction of the whole optimization dynamics and nonlinear behaviors. For clarity, in Figure 2, we first display the timeline of our dynamics and some nonlinear behaviors.

In Appendix A.2, we further validate our theoretical bounds on the key time points in Figure 2 numerically. Additionally, in Appendix A.3, we relax the data Assumption 3.1 by perturbing the

Figure 1: These figures visualize (in polar coordinates) the projections of all neurons \(\{_{k}(t)\}_{k[m]}\) onto the \(2\)d subspace \(\{_{+},_{-}\}\) during training. Each purple star represents a positive neuron \((k[m/2])\), while each brown star represents a negative neuron \((k[m]-[m/2])\). Additionally, the directions of \(_{+},_{-},_{+}^{},_{-}^{},\) are plotted in blue, orange, green, red and pink colors, respectively. The complete version of these figures is Figure 4 in Appendix A.1.

data with random noise, and our experimental results illustrate that similar four-phase dynamics and nonlinear behaviors persist.

### Phase I. Initial Condensation and Saddle Escape

Let \(T_{}=10}{_{2}}}\), and we call \(t[0,T_{}]\) Phase I. The theorem below is our main result in Phase I.

**Theorem 4.1** (Initial Condensation).: _Let the width \(m=((1/))\), the initialization \(_{1},_{2}=(1)\) and \(_{1}/_{2}=(^{8})\). Then with probability at least \(1-\), the following results hold at \(T_{}\):_

**(S1)** _Let \(_{+}\) be the index set of living positive neurons at \(T_{}\), i.e. \(_{+}:=\{k[m/2]:_{k}^{+}(T_{})=1_{k}^{-}(T_{})=1\}\). Then,_ **(i)**_\(0.21m|_{+}(T_{})| 0.29m\). Moreover, for any \(k_{+}\),_ **(ii)** _its norm is small but significant: \(_{k}(T_{})=(_{2}}{_{ 2}}})\);_ **(iii)** _Its direction is strongly aligned with \(\): \(_{k}(T_{}), 1- (_{2}})-((_{1}/_ {2})^{0.55})\);_ **(iv)** _\(_{k}^{+}(T_{})=_{k}^{-}(T_{})=1\)._

**(S2)** _Let \(_{-}\) be the index set of living negative neurons at \(T_{}\), i.e. \(_{-}:=\{k[m]-[m/2]:_{k}^{+}(T_{})=1_{k}^{-}(T_{})=1\}\). Then,_ **(i)**_\(0.075m|_{-}| 0.205m\). Moreover, for any \(k_{-}\),_ **(ii)** _its norm is tiny: \(_{k}(T_{})=(_{2}}}{ }}{_{2}}}+)\);_ **(iii)** _its direction is aligned with \(_{+}^{}\): \(_{k}(T_{}),_{+}^{} 1- ((}{_{2}}})^{1.6})\);_ **(iv)** _\(_{k}^{-}(T_{})=1\), but \(_{k}^{+}(T_{})=0\)._

**(S3)** _For other neuron \(k_{+}_{-}\), it dies and remains unchanged during the remaining training process: \(_{k}^{+}(t) 0\), \(_{k}^{-}(t) 0\), \(_{k}(t)_{k}(T_{}),\  t T_{}\)._

**(S4).**\(f_{+}(T_{})=(_{2}_{2}}),f_{-} (T_{})=(_{2}_{2}})\)_, and \((T_{})=\)._

**Initial condensation and simplification.** Theorem 4.1 (S1)(S2)(S3) show that, after a short time \(T_{}=(/_{2}})\), all neurons are implicitly simplified to three categories: \(_{+}\), \(_{-}\) and others. The living positive neurons \(k_{+}\) align strongly with \(\), and the living negative neurons \(k_{-}\) align with \(_{+}^{}\) and lie on the manifold orthogonal to \(_{+}\). Other neurons die and remain unchanged during the remaining training process. Moreover, we also estimate tight bounds for \(|_{+}|\) and \(|_{+}|\). Actually, \(_{1}/_{2}=(1)\) can ensure Theorem 4.1 and initial condensation hold (please refer to Appendix C), and we write \(_{1}/_{2}=(^{8})\) here to ensure that the dynamics of later phases hold. In Phase I, the dynamics exhibit a fast condensation phenomenon, i.e., in addition to dead neurons, living positive and negative neurons condense in one direction each.

**Saddle-to-Plateau.** The network is initially close to the saddle point at \(\) (where the loss gradient vanishes). However, Theorem 4.1 (S1) reveals that despite being small, there is a significant growth in the norm of living positive neuron \(k_{+}\) from \((_{1}/)\) to \((_{2}}/)\) and the predictions also experience substantial growth (S4). This means that GF escapes from this saddle rapidly. Furthermore, it is worth noting that initial training accuracy can simply be \(0\), \(\), \(\), or \(1\). However, after Phase I, the training accuracy reaches \((T_{})=\) which we will prove as a plateau in the next subsection. Therefore, Phase I exhibits saddle-to-plateau dynamics.

**Remark 4.2**.: Throughout the following subsections, we call the neuron \(k_{+}\) the "living positive neuron", the neuron \(k_{-}\) the "living negative neuron", and the neuron \(k_{+}_{-}\) the "dead neuron". Moreover, we denote \(m_{+}:=|_{+}|\), \(m_{-}:=|_{-}|\), and \(:=}{m_{+}}\). Notice that Theorem 4.1(S1)(S2) guarantee that \(0<<1\).

**Remark 4.3**.: The results in the following subsections are all based on the occurrence of the events in Theorem 4.1 and with the same settings as Theorem 4.1. So they all hold with probability at least \(1-\).

Please refer to Appendix C for the proof of Phase I.

### Phase II. Getting Stuck in and Escaping from Plateau

In this phase, we study the dynamics before the patterns of living neurons change again after Phase I. Specifically, we define

\[T_{ II}:=\{t>T_{ I}: k_{+}_{-}, \,_{k}^{+}(t)_{k}^{+}(T_{ I})_{k}^{-}(t)_{k}^{-}(T_{ I})\},\]

and call \(t(T_{ I},T_{ II}]\) Phase II.

**Theorem 4.4** (End of Phase II).: **(S1)**__\(T_{ II}=(^{2}^{2}})\)_. **(S2)**__\(((T_{ II}))=(p^{-})\)._

**(S3)** _One of living positive neuron \(k_{0}_{+}\) precisely changes its pattern on \(_{-}\) at \(T_{ II}\): \(_{t T_{ II}}_{k_{0}}^{-}(t)=1\) and \(_{t T_{ II}^{+}}_{k_{0}}^{-}(t)=0\), while all other activation patterns remain unchanged._

Recalling the results in Theorem 4.1, during Phase II, the activation patterns do not change with \(_{k}^{+}(t)=_{k}^{-}(t)=1\) for \(k_{+}\) and \(_{k}^{+}(t)=0,_{k}^{-}(t)=1\) for \(k_{-}\). Theorem 4.4 demonstrates that at the end of Phase II, except for one of living positive neuron \(k_{0}_{+}\) precisely changes its pattern on \(_{-}\), all other activation patterns remain unchanged.

**Theorem 4.5** (Plateau).: _We define the hitting time \(T_{ plat}:=\{t[T_{ I},T_{ II}]:(t)=1\}\). Then,_ **(S1)**__\(T_{ plat}=(^{2}^{2}})\);_ **(S2)**__\( t[T_{ I},T_{ plat}]\), \((t)\);_ **(S3)**__\( t(T_{ plat},T_{ II}]\), \((t) 1\)._

**Plateau of training accuracy.** According to Theorem 4.5, during Phase II, the training accuracy gets stuck in a long plateau \(\), which lasts for \(^{2}^{2}}\) time. However, once escaping from this plateau, the training accuracy rises to \(100\%\). It is worth noting that this plateau is essentially induced by the dataset. All that's required is only mild imbalance (\(p\) is slightly greater than 1 such that \(p>1\)) and a small margin \((/2)\) of two data classes. Notably, if the dataset has an extremely tiny margin \(( 0)\), then the length of this plateau will be significantly prolonged \((T_{ plat}+)\), which implies how the data separation can affect the training dynamics. Additionally, using a smaller initialization scale \(_{1}\) of the input layers cannot avoid this plateau.

Please refer to Appendix D for the proof of Phase II.

### Phase III. Simplifying by Neuron Deactivation, and Trying to Learn by Simplest Network

Building upon Phase II, we demonstrate that within a short time, all the living positive neurons \(_{+}\) change their activation patterns, corresponding to a "phase transition". Specifically, we define

\[T_{ II}^{ PT}:=\{t>T_{ II}: k_{+},_{k}^{-}(t)=0\},\]

and we call \(t(T_{ II},T_{ II}^{ PT}]\) the phase transition from Phase II to Phase III.

**Theorem 4.6** (Phase Transition).: **(S1)**__\(T_{ II}^{ PT}=(1+(_{2}^{2}} ))T_{ II}\);_ **(S2)**__\(_{k}^{+}(T_{ II}^{ PT})=1\) and \(_{k}^{-}(T_{ II}^{ PT})=0\) for any \(k_{+}\); \(_{k}^{+}(T_{ II}^{ PT})=0\) and \(_{k}^{-}(T_{ II}^{ PT})=1\) for any \(k_{-}\)._

**Neuron deactivation.** As shown in Theorem 4.6 (S2), after the phase transition, _all_ the living positive neurons \(k_{+}\) undergo deactivation for \(_{-}\), i.e., \(_{k}^{-}(t)\) changes from 1 to 0, while other activation patterns remain unchanged. Furthermore, Theorem 4.6 (S1) reveals that the phase transition is completed quite quickly by using sufficiently small initialization value \(_{1},_{2}\). A smaller initialization value leads to a more precise initial condensation \(_{k}(T_{ I})\), causing all living positive neurons to remain closer together before \(T_{ II}\) and thus changing their patterns nearly simultaneously.

Self-simplifying.As a result, the network implicitly simplifies itself through the deactivation behavior. At \(T_{}^{}\), only living negative neurons \(k_{+}\) are used for predicting on \(_{-}\), i.e., \(f_{-}(T_{}^{})=}{}_{k _{-}}(_{k}(T_{}^{}), _{-})\). In contrast, during Phase II, both living positive and living negative neurons jointly predict on \(_{-}\), i.e., \(f_{-}(t)=}{}_{k_{+}}( _{k}(t),_{-})-}{}_{k_{-}}(_{k}(t),_{-})\). As indicated in Table 1, two classes of activation patterns are simplified from \((1,0)\) to \((0,0)\), while others do not change.

After this phase transition, we study the dynamics before the patterns of living neurons change again. Specifically, we define

\[T_{}:=\{t>T_{}^{}: k_{+}_{-},_{k}^{+}(t)_{k}^{+}(T_{ }^{})_{k}^{-}(t)_{k}^{-}(T_{ }^{})\},\]

and call \(t(T_{},T_{}]\) Phase III.

**Theorem 4.7** (End of Phase III).: \(T_{}=1+(^{2})T_{}\)_._

Learning by simplest network.During \(t(T_{}^{},T_{})\), all activation patterns do not change. This ensures that \(f_{+}(t)=}{}_{k_{+}}( _{k}(t),_{+})\), while \(f_{-}(t)=-}{}_{k_{-}}( _{k}(t),_{-})\). Additionally, by using sufficiently small \(_{1}\), the neurons in \(_{+}\) and \(_{-}\) keep close together respectively before \(T_{}\), making the network close to a simple two-neuron network consisting of one positive neuron and one negative neuron. Please refer to Appendix E for more details. Furthermore, this pattern scheme is almost the "simplest" way to ensure binary classification: the living positive neurons only predict positive data \(_{+}\) while the living negative neurons only predict negative data \(_{-}\). Therefore, GF tries to learn by this almost simplest network in this phase.

Please refer to Appendix E for the proof of Phase III.

### Phase IV. Complicating by Neuron Reactivation, and Directional Convergence

Phase IV begins with an instantaneous phase transition at time \(T_{}\).

**Theorem 4.8** (Phase Transition).: _All living negative neuron \(k_{-}\) simultaneously change their patterns on \(_{+}\) at \(T_{}\): \(_{t T_{}^{}}_{k}^{+}(t)=0\), \(_{t T_{}^{}}_{k}^{+}(t)=1\), while others remain unchanged._

Neuron reactivation.According to Theorem 4.8, _all_ of living negative neurons \(k_{-}\) reactivate _simultaneously_ on \(_{+}\) at \(T_{}\): \(_{k}^{+}(t)\) changes from 0 to 1, while other activation patterns remain unchanged.

Self-Complicating.Along with the reactivation behavior, GF implicitly complicates itself. In Phase III, only living negative neurons \(k_{+}\) are used to predict on \(_{+}\), i.e., \(f_{+}(t)=}{}_{k_{+}}( _{k}(t),_{+})\). In contrast, after the phase transition at \(T_{}\), both living positive and living negative neurons jointly predict on \(_{+}\), i.e. \(f_{+}(t)=}{}_{k_{+}}( _{k}(t),_{+})-}{}_{k_{-}}(_{k}(t),_{+})\). As indicated in Table 1, two classes of activation patterns are complicated from \((0,0)\) to \((0,1)\), while others do not change.

In this phase, we study the dynamics before activation patterns change again after the phase transition in Theorem 4.8. We define the hitting time:

\[T_{}:=\{t>T_{}: k_{+} _{-},_{k}^{+}(t)_{s T_{}^{ +}}_{k}^{+}(s)_{k}^{-}(t)_{s T_{ }^{+}}_{k}^{-}(s)\},\]

and we call \(t(T_{},T_{}]\) Phase IV.

**Theorem 4.9** (Phase IV).: **(S1)**\(T_{}=+\)_. Moreover, for any \(t>T_{}\),_ **(S2)** _all activation patterns do not change;_ **(S3)** _the loss converges with \(((t))=(+_{ 2}^{2}^{2}(t-T_{})})\)._

Theorem 4.9 illustrates that all activation patterns never change again after the phase transition at \(T_{}\) with \(_{k}^{+}(t)=1\), \(_{k}^{-}(t)=0\) for any \(k_{+}\) and \(_{k}^{+}(t)=_{k}^{-}(t)=1\) for any \(k_{-}\). Additionally, the loss converges with the polynomial rate \((1/_{2}^{2}^{2}t)\). Furthermore, we present the following theorem about the convergent direction of each neuron.

**Theorem 4.10** (Directional Convergence).: _The limit \(_{t+}(t)}{\|(t)\| _{2}}\) exists and denoted by \(}=(_{1}^{},,_{m}^{}) ^{}^{md-1}\). Moreover, (i) for any \(k_{+}_{-},_{k}=\); (ii) for any \(k_{+},_{k}_{+}=C_{+}- _{-}\); (iii) for any \(k_{-},_{k}_{-}=C1+}{(1+)}_{-}-_{+}\), where \(C>0\) is a scaling constant such that \(\|}\|_{2}=1\). (iv) Additionally, \(f_{-}(})=-f_{+}(})>0\)._

**Initialization-dependent Directional Convergence.** As an asymptotic result, Theorem 4.10 provides the final convergent direction of GF. All living positive neurons (\(k_{+}\)) directionally converge to \(_{+}_{-}^{}\) with \(_{+},_{+}>0\) and \(_{+},_{-}=0\), while all living negative neurons (\(k_{-}\)) directionally converge to \(_{-}\{_{+},_{-}\}\) with \(_{-},_{+}>0\) and \(_{-},_{-}>0\). It is worth noting that \(_{-}\) directly depends not only on the data but also on the ratio \(=|_{-}|/|_{+}|\) (defined in Remark 4.2). Recalling the results in Phase I, \(\) lies in a certain range with high probability; but it is still a random variable due to its dependence on random initialization. Different initializations may lead to different values \(|_{-}|/|_{+}|\) at the end of Phase I, eventually causing different convergent directions in Phase IV.

**Non-(Local)-Max-Margin Direction.** Lastly, we study the implicit bias of the final convergence rate. According to Lyu and Li (2019); Ji and Telgarsky (2020) and our results above, \(}\) in Theorem 4.10 must be a KKT direction of some max-margin optimization problem. However, it is not clear whether the direction \(}\) is actually an actual optimum of this problem. Surprisingly, in next Theorem, we demonstrate that the final convergent direction is **not even a local optimal** direction of this problem, which enlightens us to rethink the max margin bias of ReLU neural networks.

**Theorem 4.11** (Implicit Bias).: _The final convergent direction \(}\) (in Theorem 4.10) is a KKT direction of the max-margin problem \(:\|\|^{2}\ \)\(y_{i}f(_{i};) 1,i[n]\). However, \(}\) is not even a local optimal direction of this problem._

Please refer to Appendix F for the proof of Phase IV.

## 5 Discussion and Comparison on Nonlinear Behaviors

Throughout the whole training process in Section 4, we divide the phases based on the evolution of ReLU activation patterns. During Phase I, as well as the beginning of Phase II and III, numerous activation patterns undergo rapid changes. Table 1 summarizes the evolution of activation patterns for all living neurons after Phase I. These results are also numerically validated in Figure 1.

**Simplifying-to-Complicating.** In phase I, GF simplifies all the neurons from random directions into three categories: living positive neurons \(_{+}\) and living negative neurons \(_{-}\) condense in one direction each, which other neurons are deactivated forever. After Phase I, as shown in Table 1, the two classes of activation patterns change from \((1,0)}}{{}}(0,0)}}{{}}(0,1)\), while other patterns remain unchanged. Therefore, the evolution of activation patterns exhibits a simplifying-to-complicating learning trend, which also implies that the network trained by GF learn features in increasing complexity.

**Comparison with NTK.** In the lazy regime such as NTK, most neurons keep close to the initialization and most activation patterns do not change during training. Specifically, for any training data \(_{i}\), \(_{k[m]}\{(_{k}(t), _{i})(_{k}(0),_{i})\}=o(1),  t>0\)(Du et al., 2018). However, our work stands out from lazy regime as activation patterns undergo numerous changes during training. In Phase I, initial condensation causes substantial changes in activation patterns, which is similarly observed in (Phuong and Lampert, 2021). Furthermore, even after Phase I, there are notable modifications in activation patterns. As shown in Table 1, the proportion of changes in

    & \(t(T_{1},T_{11})\) & \(t(T_{11},T_{11}^{})\) & \(t(T_{11}^{},T_{11})\) & \(t(T_{11},+)\) \\  \(_{1}^{-}(t)\) (\(k_{+}\)) & 1 & 1 or 0 & 0 & 0 \\  \(_{k}^{+}(t)\) (\(k_{-}\)) & 0 & 0 & 0 & 1 \\   

Table 1: The evolution of two classes of activation patterns of living neurons after Phase I. As for other two classes, \(_{k}^{+}(t)\) (\(k_{+}\)) and \(_{k}^{-}(t)\) (\(k_{-}\)), they remain equal to \(1\) after Phase I.

activation patterns for any given training data is the \((1)\), as compared with the \(o(1)\) in NTK regime. Specifically, at any \(t>T_{}\), \(_{k[m]}\{_{k}^{+}(t)_{k}^{ +}(T_{})\}=|_{-}|=(1)\) and \(_{k[m]}\{_{k}^{-}(t)_{k}^ {-}(T_{})\}=|_{+}|=(1)\). On the other hand, in our analysis, the requirement on the network's width \(m\) is only \(m=((1/))\) (Theorem 4.1), regardless of data parameters \(p,\), while NTK regime requires a much larger width \(m=((p/)/^{6})\)(Ji and Telgarsky, 2019).

**Comparison with**Phuong and Lampert (2021); Lyu et al. (2021); Wang and Ma (2022); Boursier et al. (2022). Beyond lazy regime and local analysis, these works also characterize the entire training dynamics and analyze a few nonlinear behaviors. Now we compare our results with these works in detail. (i) While Lyu et al. (2021) focuses on training Leaky ReLU NNs, our work and the other three papers study ReLU NNs. It is worth noting that the dynamics of Leaky ReLU NNs differ from ReLU due to the permanent activation of Leaky ReLU (\(^{}()>0\)). (ii) Initial condensation is also proven in (Lyu et al., 2021; Boursier et al., 2022), and the condensation directions are some types of data averages. In our work, neurons can aggregate towards not only the average direction \(\), but also another direction \(_{+}^{}\). Moreover, we also estimate the number of neurons that condense into two directions. (iii) Saddle-to-saddle dynamics are proven in (Phuong and Lampert, 2021) for square loss, where the second saddle is about training loss and caused by incomplete fitting. However, our work focus on classification with exponential loss and exhibit a similar saddle-to-plateau dynamics, where the plateau is about training accuracy, caused by incomplete feature learning. (iv) Phuong and Lampert, 2021; Wang and Ma, 2022), all features can be rapidly learned in Phase I (accuracy\(=100\%\)), followed by lazy training. However, for practical tasks on more complex data, NNs can hardly learn all features in such short time. In our work, the data is more difficult to learn, resulting in incomplete feature learning in Phase I (accuracy\(<100\%\)). Subsequently, NNs experience a long time to learn other features completely. Such multi-phase feature leaning dynamics are closer to practical training process. (v) Neuron reactivation and deactivation. For ReLU NNs, The evolution of activation patterns is one of the essential causes of nonlinear dynamics. In (Phuong and Lampert, 2021; Wang and Ma, 2022), activation patterns only change rapidly in Phase I, after which they remain unchanged. In (Boursier et al., 2022), their lemma 6 shows that their dynamics lack neuron reactivation. However, in our dynamics, even after Phase I, our dynamics exhibit significant neuron deactivation and reactivation as discussed in Table 1. (vi) The final convergent directions are also derived in (Phuong and Lampert, 2021; Lyu et al., 2021; Boursier et al., 2022), which only depend on the data. However, in our setting, the convergent direction is more complicated, determined by both data and random initialization. (vii) Furthermore, our four-phase dynamics demonstrate the whole evolution of activation patterns during training and reveal a general simplifying-to-complicating learning trend.

In summary, our whole four-phase optimization dynamics capture more nonlinear behaviors than these works. Furthermore, we conduct a more thorough and detailed theoretical analysis of these nonlinear behaviors, providing a more systematic and comprehensive understanding.

## 6 Conclusion and Future Work

In this work, we study the optimization dynamics of ReLU neural networks trained by GF on a linearly separable data. Our analysis captures the whole optimization process starting from random initialization to final convergence. Throughout the whole training process, we reveal four different phases and identify rich nonlinear behaviors theoretically. However, theoretical understanding of the training of NNs still has a long way to go. For instance, although we conduct a fine-grained analysis of GF, the dynamics of GD are more complex and exhibit other nonlinear behaviors such as progressive sharpening and edge of stability (Wu et al., 2018; Jastrzebski et al., 2019; Cohen et al., 2021; Ma et al., 2022; Li et al., 2022; Damian et al., 2022; Zhu et al., 2022; Ahn et al., 2022; Abh et al., 2022; Abh et al., 2022; Abh et al., 2022; Abh et al., 2022; Feng and Tu, 2021; Liu et al., 2021; Ziyin et al., 2022; Wu et al., 2022; Wojtowytsch, 2023; Wang and Wu, 2023) in each iteration, which can have a pronounced impact on the optimization dynamics and nonlinear behaviors. Better understanding of the nonlinear behaviors during GD or SGD training is an important direction of future work.