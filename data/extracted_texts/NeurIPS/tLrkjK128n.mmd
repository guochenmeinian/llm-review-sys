# Optimistic Active Exploration of Dynamical Systems

Bhavya Sukhija\({}^{1}\)  Lenart Treven\({}^{1}\)  Cansu Sancaktar\({}^{2}\)  Sebastian Blaes\({}^{2}\)

**Stelian Coros\({}^{1}\) Andreas Krause\({}^{1}\)**

ETH Zurich \({}^{1}\) MPI for Intelligent Systems\({}^{2}\)

{sukhijab,trevenl,scoros,krausea}@ethz.ch

{cansu.sancaktar,sebastian.blae}@tuebingen.mpg.de

###### Abstract

Reinforcement learning algorithms commonly seek to optimize policies for solving one particular task. How should we explore an unknown dynamical system such that the estimated model globally approximates the dynamics and allows us to solve multiple downstream tasks in a zero-shot manner? In this paper, we address this challenge, by developing an algorithm - OpAx- for active exploration. OpAx uses well-calibrated probabilistic models to quantify the epistemic uncertainty about the unknown dynamics. It optimistically--w.r.t. to plausible dynamics--maximizes the information gain between the unknown dynamics and state observations. We show how the resulting optimization problem can be reduced to an optimal control problem that can be solved at each episode using standard approaches. We analyze our algorithm for general models, and, in the case of Gaussian process dynamics, we give a first-of-its-kind sample complexity bound and show that the epistemic uncertainty _converges to zero_. In our experiments, we compare OpAx with other heuristic active exploration approaches on several environments. Our experiments show that OpAx is not only theoretically sound but also performs well for zero-shot planning on novel downstream tasks.

## 1 Introduction

Most reinforcement learning (RL) algorithms are designed to maximize cumulative rewards for a single task at hand. Particularly, model-based RL algorithms, such as (Chua et al., 2018; Kakade et al., 2020; Curi et al., 2020), excel in efficiently exploring the dynamical system as they direct the exploration in regions with high rewards. However, due to the directional bias, their underlying learned dynamics model fails to generalize in other areas of the state-action space. While this is sufficient if only one control task is considered, it does not scale to the setting where the system is used to perform several tasks, i.e., under the same dynamics optimized for different reward functions. As a result, when presented with a new reward function, they often need to relearn a policy from scratch, requiring many interactions with the system, or employ multi-task (Zhang and Yang, 2021) or transfer learning (Weiss et al., 2016) methods. Traditional control approaches such as trajectory optimization (Biagiotti and Melchiorri, 2008) and model-predictive control (Garcia et al., 1989) assume knowledge of the system's dynamics. They leverage the dynamics model to solve an optimal control problem for each task. Moreover, in the presence of an accurate model, important system properties such as stability and sensitivity can also be studied. Hence, knowing an accurate dynamics model bears many practical benefits. However, in many real-world settings, obtaining a model using just physics' first principles is very challenging. A promising approach is to leverage data for learning the dynamics, i.e., system identification or active learning. To this end, the key question we investigate in this work is: _how should we interact with the system to learn its dynamics efficiently?_

While active learning for regression and classification tasks is well-studied, active learning in RL is much less understood. In particular, active learning methods that yield strong theoretical and practical results, generally query data points based on information-theoretic criteria (Krause et al., 2008; Settles, 2009; Balcan et al., 2010; Hanneke et al., 2014; Chen et al., 2015). In the context of

[MISSING_PAGE_FAIL:2]

and efficient w.r.t. rate of convergence of \(_{n}\) to \(^{*}\).

To devise such an algorithm, we take inspiration from Bayesian experiment design (Chaloner and Verdinelli, 1995). In the Bayesian setting, given a prior over \(^{*}\), a natural objective for active exploration is the mutual information (Lindley, 1956) between \(^{*}\) and observations \(_{_{n}}\).

**Definition 1** (Mutual Information, Cover and Thomas (2006)).: _The mutual information between \(^{*}\) and its noisy measurements \(_{_{n}}\) for points in \(_{n}\), where \(_{_{n}}\) is the concatenation of \((_{_{n},i})_{i<T}\) is defined as,_

\[F(_{n}):=I(^{*};_{_{n}})=H( _{_{n}})-H(_{_{n}}^{*} ),\] (3)

_where \(H\) is the Shannon differential entropy._

The mutual information quantifies the reduction in entropy of \(^{*}\) conditioned on the observations. Hence, maximizing the mutual information w.r.t. the dataset \(_{n}\) leads to the maximal entropy reduction of our prior. Accordingly, a natural objective for active exploration in RL can be the mutual information between \(^{*}\) and the collected transitions over a budget of \(N\) episodes, i.e., \(I(^{*};_{_{1:N}})\). This requires maximizing the mutual information over a sequence of policies, which is a challenging planning problem even in settings where the dynamics are known (Mutny et al., 2023). A common approach is to greedily pick a policy that maximizes the information gain conditioned on the previous observations at each episode:

\[_{}_{^{}}[I(^{*}_{ ^{}};_{^{}}_{1:n-1})].\] (4)

Here \(^{*}_{^{}}=(^{*}(_{n,0}),,^{*}( _{n,T-1}))\), \(_{^{}}=(_{n,0},,_{n,T-1})\), \(^{}\) is the trajectory under the policy \(\), and the expectation is taken w.r.t. the process noise \(\).

Interpretation in frequentist settingWhile information gain is Bayesian in nature (requires a prior over \(^{*}\)), it also has a frequentist interpretation. In particular, later in Section 3 we relate it to the epistemic uncertainty of the learned model. Accordingly, while this notion of information gain stems from Bayesian literature, we can use it to motivate our objective in both Bayesian and frequentist settings.

### Assumptions

In this work, we learn a probabilistic model of the function \(^{*}\) from data. Moreover, at each episode \(n\), we learn the mean estimator \(_{n}(,)\) and the epistemic uncertainty \(_{n}(,)\), which quantifies our uncertainty on the mean prediction. To this end, we use Bayesian models such as Gaussian processes (GPs, Rasmussen and Williams, 2005) or Bayesian neural networks (BNNs, Wang and Yeung, 2020). More generally, we assume our model is _well-calibrated_:

**Definition 2** (All-time calibrated statistical model of \(^{*}\), Rothfuss et al. (2023)).: _Let, \(=(,)\) and \(:=\). An all-time calibrated statistical model of the function \(^{*}\) is a sequence \((_{n},_{n},_{n}())_{n 0}\), such that_

\[(, l\{1,,d_{x}\}, n:|_{n,l}()-f_{l}()|_{ n}()_{n,l}()) 1-\]

_Here \(_{n,l}\) and \(_{n,l}\) are the l-th element in the vector valued functions \(_{n}\) and \(_{n}\) respectively. The scalar function, \(_{n}()_{ 0}\) quantifies the width of the \(1-\) confidence intervals. We assume w.l.o.g. that \(_{n}\) monotonically increases with \(n\), and that \(_{n,l}()_{}\) for all \(\), \(n 0\), and \(l\{1,,d_{x}\}\)._

**Assumption 1** (Well calibration assumption).: _Our learned model is an all-time-calibrated statistical model of \(^{*}\), i.e., there exists a sequence of \((_{n}())_{n 0}\) such that our model satisfies the well-calibration condition, c.f., Definition 2._

This is a natural assumption on our modeling. It states that we can make a mean prediction and also quantify how far it is off from the true one with high probability. A GP model satisfies this requirement for a very rich class of functions, c.f., Lemma 3. For BNNs, calibration methods (Kuleshov et al., 2018) are often used and perform very well in practice. Next, we make a simple continuity assumption on our function \(^{*}\).

**Assumption 2** (Lipschitz Continuity).: _The dynamics model \(^{*}\) and our epistemic uncertainty prediction \(_{n}\) are \(L_{}\) and \(L_{}\) Lipschitz continuous, respectively. Moreover, we define \(\) to be the policy class of \(L_{}\) Lipschitz continuous functions._The Lipschitz continuity assumption on \(^{*}\) is quite common in control theory (Khalil, 2015) and learning literature (Curi et al., 2020; Pasztor et al., 2021; Sussex et al., 2023). Furthermore, the Lipschitz continuity of \(_{n}\) also holds for GPs with common kernels such as the linear or radial basis function (RBF) kernel (Rothfuss et al., 2023).

Finally, we reiterate the assumption of the system's stochasticity.

**Assumption 3** (Process noise distribution).: _The process noise is i.i.d. Gaussian with variance \(^{2}\), i.e., \(_{k}(,^{2})\)._

We focus on the setting where \(\) is homoscedastic for simplicity. However, our framework can also be applied to the more general heteroscedastic and sub-Gaussian case (c.f., Theorem 2).

## 3 Optimistic Active Exploration

In this section, we propose our _optimistic active exploration_ (OpAx) algorithm. The algorithm consists of two main contributions: _(i)_ First we reformulate the objective in Equation (4) to a simple optimal control problem, which suggests policies that visit states with high epistemic uncertainty. _(ii)_ We leverage the optimistic planner introduced by Curi et al. (2020) to efficiently plan a policy under _unknown_ dynamics. Moreover, we show that the optimistic planner is crucial in giving theoretical guarantees for the algorithm.

### Optimal Exploration Objective

The objective in Equation (4) is still difficult and expensive to solve in general. However, since in this work, we consider Gaussian noise, c.f., Assumption 3, we can simplify this further.

**Lemma 1** (Information gain is upper bounded by sum of epistemic uncertainties).: _Let \(=^{*}()+\), with \((0,^{2})\) and let \(_{n-1}\) be the epistemic uncertainty after episode \(n-1\). Then the following holds for all \(n 1\) and dataset \(_{1:n-1}\),_

\[I(^{*}_{^{*}};_{^{*}}_{1:n-1 })_{t=0}^{T-1}_{j=1}^{d_{x}}(1+^{2}(_{t})}{^{2}}).\] (5)

We prove Lemma 1 in Appendix A. The information gain is non-negative (Cover and Thomas, 2006). Therefore, if the right-hand side of Equation (5) goes to zero, the left-hand side goes to zero as well. Lemma 1 relates the information gain to the model epistemic uncertainty. Therefore, it gives a tractable objective that also has a frequentist interpretation - collect points with the highest epistemic uncertainty. We can use it to plan a trajectory at each episode \(n\), by solving the following optimal control problem:

\[_{n}^{*}=*{argmax}_{}J_{n}()= *{argmax}_{}\ _{^{}}[_{t=0}^{T-1}_{j=1}^{d_{x}} (1+^{2}(_{t},(_{t}))}{^{2} })],\] (6)

\[_{t+1}=^{*}(_{t},(_{t}))+_{t}.\]

The problem in Equation (6) is closely related to previous literature in active exploration for RL. For instance, some works consider different geometries such as the sum of epistemic uncertainties (Pathak et al. (2019); Sekar et al. (2020), c.f., appendix C for more detail).

### Optimistic Planner

The optimal control problem in Equation (6) requires knowledge of the dynamics \(^{*}\) for planning, however, \(^{*}\) is unknown. A common choice is to use the mean estimator \(_{n-1}\) in Equation (6) instead of \(^{*}\) for planning (Buisson-Fenet et al., 2020). However, in general, using the mean estimator is susceptible to model biases (Chua et al., 2018) and is provably optimal only in the case of linear systems (Simchowitz and Foster, 2020). To this end, we propose using an optimistic planner, as suggested in Curi et al. (2020), instead. Accordingly, given the mean estimator \(_{n-1}\) and the epistemic uncertainty \(_{n-1}\), we solve the following optimal control problem

\[_{n},_{n}=*{argmax}_{, }J_{n}(,)=*{argmax}_{ ,}_{^{,}}[ _{t=0}^{T-1}_{j=1}^{d_{x}}(1+^{2}(}_{t},(}_{t}))}{^{2}})],\] (7)

\[}_{t+1}=_{n-1}(}_{t},(}_{t})) +_{n-1}()_{n-1}(}_{t},(}_{ t}))(}_{t})+_{t},\]

**OPAx: Optimistic Active Exploration**

``` **Init:** Aleatoric uncertainty \(\), Probability \(\), Statistical model \((_{0},_{0},_{0}())\) for episode \(n=1,,N\)do \[_{n}=*{argmax}_{}_{}[_{t=0}^{T-1}_{j=1}^{d_{x}}(1+ ^{2}(_{t},(_{t}))}{^{2}})] \] \[_{n}(_{n}) \] \[(_{n},_{n},_{n}()) _{1:n} \]

where \(\) is the space of policies \(:[-1,1]^{d_{x}}\). Therefore, we use the policy \(\) to "hallucinate" (pick) transitions that give us the most information. Overall, the resulting formulation corresponds to a simple optimal control problem with a larger action space, i.e., we increase the action space by another \(d_{x}\) dimension. A natural consequence of Assumption 1 is that \(J_{n}(_{n}^{*}) J_{n}(_{n},_{n})\) with high probability (c.f., Corollary 1 in Appendix A). That is by solving Equation (7), we get an optimistic estimate on Equation (6). Intuitively, the policy \(_{n}\) that OpAx suggests, behaves optimistically with respect to the information gain at each episode.

## 4 Theoretical Results

We theoretically analyze the convergence properties of OpAx. We first study the regret of planning under unknown dynamics. Specifically, since we cannot evaluate the optimal exploration policy from eq. (6) and use the optimistic one, i.e., eq. (7) instead, we incur a regret. We show that due to the optimism in the face of uncertainty paradigm, we can give sample complexity bounds for the Bayesian and frequentist settings. All the proofs are presented in Appendix A.

**Lemma 2** (Regret of optimistic planning under unknown dynamics).: _Let Assumption 1 hold. Furthermore, define \(J_{n,k}(_{n},_{n},)\) as_

\[J_{n,k}(_{n},_{n},) =_{^{*}_{n},_{n}}[_{t =k}^{T-1}_{j=1}^{d_{x}}(1+^{2}(}_{ t},_{n}(}_{t}))}{^{2}})],\] \[}_{t+1}=_{n-1}(}_{t},_{n}(}_{t}))+_{n-1}()_{n-1}(}_ {t},_{n}(}_{t}))_{n}(}_{t})+_{t}\] \[}_{0}=.\]

_Then, for all \(n 1\), with probability at least \(1-\),_

\[J_{n}(_{n}^{*})-J_{n}(_{n})_{t=0}^{T-1} _{^{*}_{n}}[J_{n,t+1}(_{n},_{ n},_{t+1}^{})-J_{n,t+1}(_{n},_{n},_{t+1}) ],\] \[_{t+1}=^{*}(_{t},_{n}(_{t}))+ _{t},\] \[_{t+1}^{}=_{n-1}(_{t},_{n}( _{t}))+_{n-1}()_{n-1}(_{t},_{n}(_{t}))_{n}(_{t})+_{t}.\]

Lemma 2 gives a bound on the regret of planning optimistically under unknown dynamics. The regret is proportional to the difference in the expected returns for \(_{t}\) and \(_{t}^{}\). Note, \(_{t}-_{t}^{}_{n}() _{n-1}(_{t-1},_{n}(_{t-1}))\). Hence, when we have low uncertainty in our predictions, planning optimistically suffers smaller regret. Next, we leverage Lemma 2 to give a sample complexity bound for the Bayesian and frequentist setting.

Bayesian SettingWe start by introducing a measure of model complexity as defined by Curi et al. (2020).

\[_{N}(^{*}):=_{_{1},,_{N} }_{n=1}^{N}_{_ {n}}_{n-1}()_{2}^{2}.\] (8)

This complexity measure captures the difficulty of learning \(^{*}\) given \(N\) trajectories. Mainly, the more complicated \(^{*}\), the larger the epistemic uncertainties \(_{n}\), and in turn, the larger corresponding \(_{N}(^{*})\). Moreover, if the model complexity measure is sublinear in \(N\), i.e. \(_{N}(^{*})/N 0\) for \(N\), then the epistemic uncertainties also converge to zero in the limit, which impliesconvergence to the true function \(^{*}\). We present our main theoretical result, in terms of the model complexity measure.

**Theorem 1**.: _Let Assumption 1 and 3 hold. Then, for all \(N 1\), with probability at least \(1-\),_

\[_{_{1:N-1}}[_{}_{^{}}[I(^{*}_{^{}};_{^{ }}_{1:N-1})]]( _{N}T^{}{{2}}}_{N}(^{*})}{N}})\] (9)

Theorem 1 relates the maximum expected information gain at iteration \(N\) to the model complexity of our problem. For deterministic systems, the expectation w.r.t. \(^{}\) is redundant. The bound in Equation (9) depends on the Lipschitz constants, planning horizon, and dimensionality of the state space (captured in \(_{N}\) and \(_{N}(^{*})\)). If the right-hand side is monotonically decreasing with \(N\), Theorem 1 guarantees that the information gain at episode \(N\) is also shrinking with \(N\), and the algorithm is converging. Empirically, Pathak et al. (2019) show that the epistemic uncertainties go to zero as more data is acquired. In general, deriving a worst-case bound on the model complexity is a challenging and active open research problem. However, in the case of GPs, convergence results can be shown for a very rich class of functions. We show this in the following for the frequentist setting.

Frequentist Setting with Gaussian Process ModelsWe extend our analysis to the frequentist kernelized setting, where \(^{*}\) resides in a Reproducing Kernel Hilbert Space (RKHS) of vector-valued functions.

**Assumption 4**.: _We assume that the functions \(f^{*}_{j}\), \(j\{1,,d_{x}\}\) lie in a RKHS with kernel \(k\) and have a bounded norm \(B\), that is \(^{*}^{d_{x}}_{k,B}\), with \(^{d_{x}}_{k,B}=\{\|f_{j}\|_{k} B,j=1,,d_{x}\}\)._

In this setting, we model the posterior mean and epistemic uncertainty of the vector-valued function \(^{*}\) with \(_{n}()=[_{n,j}()]_{j d_{x}}\), and \(_{n}()=[_{n,j}()]_{j d_{x}}\), where,

\[_{n,j}() =_{n}^{}()(_{n}+^{2})^{-1}_{1:n}^{j},\] (10) \[_{n,j}^{2}() =k(,)-_{n}^{}()(_{n}+^{ 2})^{-1}_{n}(),\]

Here, \(_{1:n}^{j}\) corresponds to the noisy measurements of \(f^{*}_{j}\), i.e., the observed next state from the transitions dataset \(_{1:n}\), \(_{n}=[k(,_{i})]_{i nT},_{i}_{1:n}\), and \(_{n}=[k(_{i},_{l})]_{i,l nT},_{i},_{l} _{1:n}\) is the data kernel matrix. It is known that if \(^{*}\) satisfies Assumption 4, then Equation (10) yields well-calibrated confidence intervals, i.e., that Assumption 1 is satisfied.

**Lemma 3** (Well calibrated confidence intervals for RKHS, Rothfuss et al. (2023)).: _Let \(^{*}^{d_{x}}_{k,B}\). Suppose \(_{n}\) and \(_{n}\) are the posterior mean and variance of a GP with kernel \(k\), c.f., Equation (10). There exists \(_{n}()\), for which the tuple \((_{n},_{n},_{n}())\) satisfies Assumption 1 w.r.t. function \(^{*}\)._

Theorem 2 presents our convergence guarantee for the kernelized case to the \(T\)-step reachability set \(\) for the policy class \(\). In particular, \(\) is defined as

\[=\{(,t T),\ ,p(_{t}=|,^{*})>0\}\]

There are two key differences from Theorem 1; (_i_) we can derive an upper bound on the epistemic uncertainties \(_{n}\), and (_ii_) we can bound the model complexity \(_{N}(^{*})\), with the _maximum information gain_ of kernel \(k\) introduced by Srinivas et al. (2012), defined as

\[_{N}(k)=_{_{1},,_{N}:|_{n}|  T}(+^{-2}_{N}).\]

**Theorem 2**.: _Let Assumption 3 and 4 hold, Then, for all \(N 1\), with probability at least \(1-\),_

\[_{}_{^{}}[_{ ^{}}_{j=1}^{d_{x}}_{N,j}^{2}() ](_{N}T^{}{{2}}}(k)}{N}}).\] (11)

_If we relax noise Assumption 3 to \(\)-sub Gaussian. Then, if Assumption 2 holds, we have for all \(N 1\), with probability at least \(1-\),_

\[_{}_{^{}}[_{ ^{}}_{j=1}^{d_{x}}_{N,j}^{2}() ](_{N}^{T}T^{}{{2}}}(k)}{N}}).\] (12)_Moreover, if \(_{N}(k)=((N))\), then for all \(\), and \(1 j d_{x}\),_

\[_{N,j}()}0N.\] (13)

We only state Theorem 2 for the expected epistemic uncertainty along the trajectory at iteration \(N\). For deterministic systems, the expectation is redundant and for stochastic systems, we can leverage concentration inequalities to give a bound without the expectation (see Appendix A for more detail).

For the Gaussian noise case, we obtain a tighter bound by leveraging the change of measure inequality from Kakade et al. (2020, Lemma C.2.) (c.f., Lemma 6 in Appendix A for more detail). In the more general case of sub-Gaussian noise, we cannot use the same analysis. To this end, we use the Lipschitz continuity assumptions (Assumption 2) similar to Curi et al. (2020). This results in comparing the deviation between two trajectories under the same policy and dynamics but different initial states (see Lemma 2). For many systems (even linear) this can grow exponentially in the horizon \(T\). Accordingly, we obtain a \(_{N}^{T}\) term in our bound (Equation (12)). Nonetheless, for cases where the RKHS is of a kernel with maximum information gain \(_{N}(k)=((N))\), we can give sample complexity bounds and an almost sure convergence result in the reachable set \(\) (Equation (13)). Kernels such as the RBF kernel or the linear kernel (kernel with a finite-dimensional feature map \((x)\)) have maximum information gain which grows polylogarithmically with \(n\)(Vakili et al. (2021)). Therefore, our convergence guarantees hold for a very rich class of functions. The exponential dependence of our bound on \(T\) imposes the restriction on the kernel class. For the case of Gaussian noise, we can include a richer class of kernels, such as Matern.

In addition to the convergence results above, we also give guarantees on the zero-shot performance of Opax in Appendix A.5.

## 5 Experiments

We evaluate Opax on the Pendulum-v1 and MountainCar environment from the OpenAI gym benchmark suite (Brockman et al., 2016), on the Reacher, Swimmer, and Cheetah from the deep mind control suite (Tassa et al., 2018), and a high-dimensional simulated robotic manipulation task introduced by Li et al. (2020). See Appendix B for more details on the experimental setup.

BaselinesWe implement four baselines for comparisons. To show the benefit of our intrinsic reward, we compare OpAx to (_1_) a random exploration policy (Random) which randomly samples actions from the action space. As we discuss in Section 3 our choice of objective in Equation (6) is in essence similar to the one proposed by Pathak et al. (2019) and Sekar et al. (2020). Therefore, in our experiments, we compare the optimistic planner with other planning approaches. Moreover, most work on active exploration either uses the mean planner or does not specify the planner (c.f., Section 6). We use the most common planners: (2) mean (Mean-AE), and (_3_) trajectory sampling (TS-1) scheme proposed in Chua et al. (2018) (PETS-AE) as our baselines. The mean planner simply uses the mean estimate \(_{n}\) of the well-calibrated model. This is also used in Buisson-Fenet et al. (2020). Finally, we compare OpAx to (_4_) H-UCRL (Curi et al., 2020), a single-task model-based RL algorithm. We investigate the following three aspects: (_i_) _how fast does active exploration reduce model's epistemic uncertainty \(_{n}\) with increasing \(n\)_, (_ii_) _can we solve downstream tasks with_ OpAx, and (_iii_) _does_ OpAx _scale to high-dimensional and challenging object manipulation tasks_? For our experiments, we use GPs and probabilistic ensembles (PE, Lakshminarayanan et al. (2017)) for modeling the dynamics. For the planning, we either the soft actor-critic (SAC, Haarnoja et al. (2018)) policy optimizer, which takes simulated trajectories from our learned model to train a policy, or MPC with the iEEM optimizer (Pinneri et al., 2021).

How fast does active exploration reduce the epistemic uncertainty?For this experiment, we consider the Pendulum-v1 environment. We sample transitions at random from the pendulum's _reachable_ state-action space and evaluate our model's epistemic uncertainty for varying episodes and baselines. We model the dynamics with both GPs and PE. We depict the result in Figure 1. We conclude that the Random agent is slower in reducing the uncertainty compared to other active exploration methods for both GP and PE models. In particular, from the experiment, we empirically validate Theorem 2 for the GP case and also conclude that empirically even when using PE models, we find convergence of epistemic uncertainty. Moreover, we notice for the PE case that OpAx reaches smaller uncertainties slightly faster than Mean-AE and PETS-AE. We believe this is due to the additional exploration induced by the optimistic planner.

Can the model learnt through OpAx solve downstream tasks?We use OpAx and other active exploration baselines to actively learn a dynamics model and then evaluate the learned model on downstream tasks. We consider several tasks, (_i_) Pendulum-v1 swing up, (_ii_) Pendulum-v1 keep down (keep the pendulum at the stable equilibria), (_iii_) MountainCar, (_iv_) Reacher - go to target, (_v_) Swimmer - go to target, (_vi_) Swimmer - go away from target (quickly go away from the target position), (_vii_) Cheetah - run forward, (_viii_) Cheetah - run backward. For all tasks, we consider PEs, except for (_i_) where we also use GPs. Furthermore, for the MountainCar and Reacher, we give a reward once the goal is reached. Since this requires long-term planning, we use a SAC policy for these tasks. We use MPC with iCEM for the remaining tasks. We also train H-UCRL on tasks (_i_) with GPs, and (_ii_), (_iii_), (_iv_), (_v_), (_vii_) with PEs. We report the best performance across all episodes.

To make a fair comparison, we use the following evaluation procedure; first, we perform active exploration for each episode on the environment, and then after every few episodes we use the mean estimate \(_{n}\) to evaluate our learned model on the downstream tasks.

Figure 2 shows that all active exploration variants perform considerably better than the Random agent. In particular, for the MountainCar, the Random agent is not able to solve the task. Moreover, PETS-AE performs slightly worse than the other exploration baselines in this environment. In general, we notice that OpAx always performs well and is able to achieve H-UCRL's performance on all the tasks for which H-UCRL is trained. However, on tasks that are new/unseen for H-UCRL, active exploration algorithms outperform H-UCRL. From this experiment, we conclude two things (_1_) apart from providing theoretical guarantees, the model learned through OpAx also performs well in downstream tasks, and (2) active exploration agents generalize well to downstream tasks, whereas H-UCRL performs considerably worse on new/unseen tasks. We believe this is because, unlike active exploration agents, task-specific model-based RL agents only explore the regions of the state-action space that are relevant to the task at hand.

Does OpAx scale to high-dimensional and challenging object manipulation tasks?To answer this question, we consider the Fetch Pick & Place Construction environment (Li et al., 2020). We again use the active exploration agents to learn a model and then evaluate the success rate of the learned model in three challenging downstream tasks: (_i_) Pick & Place, (_ii_) Throw, and (_iii_) Flip (see Figure 4). The environment contains a \(7\)-DoF robot arm and four \(6\)-DoF blocks that can be manipulated. In total, the state space is \(58\)-dimensional. The \(4\)-dimensional actions control the end-effector of the robot in Cartesian space as well as the opening/closing of the gripper. We compare OpAx to PETS-AE, Mean-AE, a random policy as well as CEE-US (Sancaktar et al., 2022). CEE-US is a model-based active exploration algorithm, for which Sancaktar et al. (2022) reports state-of-the-art performance compared to several other active exploration methods. In all three tasks, OpAx is at least on par with the best-performing baselines, including CEE-US. We run OpAx and all baselines with the same architecture and hyperparameter settings. See Appendix B for more details.

Figure 1: Reduction in maximum epistemic uncertainty in _reachable_ state-action space for the Pendulum-v1 environment over 10 different random seeds. We evaluate OpAx with both GPs and PE and plot the mean performance with two standard error confidence intervals. For both, active exploration reduces epistemic uncertainty faster compared to random exploration. All active exploration baselines perform well for the GP case, whereas for the PE case OpAx gives slightly lower uncertainties.

Figure 3: Fetch Pick & Place Construction environment.

## 6 Related Work

System identification is a broadly studied topic (Astrom and Eykhoff, 1971; Schoukens and Ljung, 2019; Schon et al., 2011; Ziemann et al., 2022; Ziemann and Tu, 2022). However, system identification from the perspective of experiment design for nonlinear systems is much less understood (Chiuso and Pillonetto, 2019). Most methods formulate the identification task through the maximization of intrinsic rewards. Common choices of intrinsic rewards are (_i_) model prediction error or "Curiosity" (Schmidhuber, 1991; Pathak et al., 2017), (_ii_) novelty of transitions (Stadie et al., 2015), and (_iii_) diversity of skills (Eysenbach et al., 2018).

A popular choice for intrinsic rewards is mutual information or entropy (Jain et al., 2018; Buisson-Fenet et al., 2020; Shyam et al., 2019; Pathak et al., 2019; Sekar et al., 2020). Jain et al. (2018) propose an approach to maximize the information gain greedily wrt the immediate next transition, i.e., one-step greedy, whereas Buisson-Fenet et al. (2020) consider planning full trajectories. Shyam et al. (2019); Pathak et al. (2019); Sekar et al. (2020) and Sancaktar et al. (2022) consider general Bayesian models, such as BNNs, to represent a probabilistic distribution for the learned model. Shyam et al. (2019) propose using the information gain of the model with respect to observed transition as the intrinsic reward. To this end, they learn an ensemble of Gaussian neural networks and represent the distribution over models with a Gaussian mixture model (GMM). A similar approach is also proposed in Pathak et al. (2019); Sekar et al. (2020); Sancaktar et al. (2022). The main difference between Shyam et al. (2019) and Pathak et al. (2019) lies in how they represent mutual information. Moreover, Pathak et al. (2019) use the model's epistemic uncertainty, that is the

Figure 2: We evaluate the downstream performance of our agents over 10 different random seeds and plot the mean performance with two standard error confidence intervals. For all the environments we use PE as models, except plot (1), for which we use a GP model (see plot (2) in the figure above). For tasks (1)-(6), we also train H-UCRL, a model-based RL algorithm. Tasks (7)-(9) are new/unseen for H-UCRL. From the Figure, we conclude that (_i_) compared to other active exploration baselines, OPAX constantly performs well and is on par with H-UCRL, and (_ii_) on the new/unseen tasks the active exploration baselines and OPAX outperform H-UCRL by a large margin.

disagreement between the ensemble models as an intrinsic reward. Sekar et al. (2020) link the model disagreement (epistemic uncertainty) reward to maximizing mutual information and demonstrate state-of-the-art performance on several high-dimensional tasks. Similarly, Sancaktar et al. (2022), use the disagreement in predicted trajectories of an ensemble of neural networks to direct exploration. Since trajectories can diverge due to many factors beyond just the model epistemic uncertainty, e.g., aleatoric noise, this approach is restricted to deterministic systems and susceptible to systems with unstable equilibria. Our approach is the most similar to Pathak et al. (2019); Sekar et al. (2020) since we also propose the model epistemic uncertainty as the intrinsic reward for planning. However, we thoroughly and theoretically motivate this choice of reward from a Bayesian experiment design perspective. Furthermore, we induce additional exploration in OpAx through our optimistic planner and rigorously study the theoretical properties of the proposed methods. On the contrary, most of the prior work discussed above either uses the mean planner (Mean-AE) or does not discuss the planner thoroughly or provide any theoretical results. In general, theoretical guarantees for active exploration algorithms are rather immature (Chakraborty et al., 2023; Wagenmaker et al., 2023) and mostly restrictive to a small class of systems (Simchowitz et al., 2018; Tarbouriech et al., 2020; Wagenmaker and Jamieson, 2020; Mania et al., 2020). To the best of our knowledge, we are the first to give convergence guarantees for a rich class of nonlinear systems.

While our work focuses on the active learning of dynamics, there are numerous works that study exploration in the context of reward-free RL (Jin et al., 2020; Kaufmann et al., 2021; Wagenmaker et al., 2022; Chen et al., 2022). However, most methods in this setting give guarantees for special classes of MDPs (Jin et al., 2020; Kaufmann et al., 2021; Wagenmaker et al., 2022; Qiu et al., 2021; Chen et al., 2022) and result in practical algorithms. On the contrary, we focus on solely learning the dynamics. While a good dynamics model may be used for zero-shot planning, it also exhibits more relevant knowledge about the system such as its stability or sensitivity to external effects. Furthermore, our proposed method is not only theoretically sound but also practical.

## 7 Conclusion

We present OpAx, a novel model-based RL algorithm for the active exploration of unknown dynamical systems. Taking inspiration from Bayesian experiment design, we provide a comprehensive explanation for using model epistemic uncertainty as an intrinsic reward for exploration. By leveraging the _optimistic in the face of uncertainty_ paradigm, we put forth first-of-their-kind theoretical results on the convergence of active exploration agents in reinforcement learning. Specifically, we study convergence properties of general Bayesian models, such as BNNs. For the frequentist case of RKHS dynamics, we established sample complexity bounds and convergence guarantees for OpAx for a rich class of functions. We evaluate the efficacy of OpAx across various RL environments with state space dimensions from two to 58. The empirical results corroborate our theoretical findings, as OpAx displays systematic and effective exploration across all tested environments and exhibits strong performance in zero-shot planning for new downstream tasks.

Figure 4: Success rates for pick & place, throwing and flipping tasks with four objects in the Fetch Pick & Place Construction environment for OpAx and baselines. We evaluate task performance via planning zero-shot with models learned using different exploration strategies. We report performance on three independent seeds. OpAx is on par with the best-performing baselines in all tasks.