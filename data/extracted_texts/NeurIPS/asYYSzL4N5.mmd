# BAN: Detecting Backdoors Activated by Adversarial Neuron Noise

Xiaoyun Xu

Radboud University

xiaoyun.xu@ru.nl

&Zhuoran Liu

Radboud University

z.liu@cs.ru.nl

&Stefanos Koffas

Delft University of Technology

s.koffas@tudelft.nl

&Shujian Yu

Vrije Universiteit Amsterdam

s.yu3@vu.nl

&Stiepan Picek

Radboud University

stjepan.picek@ru.nl

Corresponding author.

###### Abstract

Backdoor attacks on deep learning represent a recent threat that has gained significant attention in the research community. Backdoor defenses are mainly based on backdoor inversion, which has been shown to be generic, model-agnostic, and applicable to practical threat scenarios. State-of-the-art backdoor inversion recovers a mask in the feature space to locate prominent backdoor features, where benign and backdoor features can be disentangled. However, it suffers from high computational overhead, and we also find that it overly relies on prominent backdoor features that are highly distinguishable from benign features. To tackle these shortcomings, this paper improves backdoor feature inversion for backdoor detection by incorporating extra neuron activation information. In particular, we adversarially increase the loss of backdoored models with respect to weights to activate the backdoor effect, based on which we can easily differentiate backdoored and clean models. Experimental results demonstrate our defense, BAN, is 1.37\(\) (on CIFAR-10) and 5.11\(\) (on ImageNet200) more efficient with an average 9.99% higher detect success rate than the state-of-the-art defense BTI-DBF. Our code and trained models are publicly available at https://github.com/xiaoyunxxy/ban.

## 1 Introduction

Deep neural networks (DNNs) are known to be vulnerable to backdoor attacks, a setting where the attacker trains a malicious DNN to perform well on normal inputs but behave inconsistently when receiving malicious inputs that are stamped with triggers . The malicious model is obtained by training with poisoned data [9; 6], by tampering with the training process [22; 2], or by directly altering the model's weights . Backdoors are proposed for various domains, from computer vision [9; 22; 6; 26; 40; 25] to graph data  and neuromorphic data . Still, backdoors in computer vision received most of the attention of the research community, which also means there is a significant variety of triggers. For instance, a trigger can be a pixel patch , an existing image , dynamic perturbation , or image warping . Various backdoor attacks target different stages of the machine learning model pipeline, inducing several threat scenarios in which defenses are developed by exploiting different knowledge.

Trigger inversion is a principled method that makes minimal assumptions about the backdoor attacks in the threat model [38; 39; 44], and it has clear advantages to training-time  or run-time defenses [24; 19; 11]. _Input space_ trigger inversion is first introduced by Neural Cleanse (NC) ,where potential triggers for all target classes are reversed by minimizing the model loss of clean inputs. Median absolute deviations of reversed triggers from all classes are then calculated to detect triggers. More recently, input space trigger inversion methods have been shown to be ineffective against _feature space_ backdoor attacks [26; 38]. To address this problem, FeatureRE  proposes a detection method using feature space triggers. The authors observe that features of backdoored and benign inputs are separable in the feature space by hyperplanes. Unicorn  formally defines and analyzes different spaces for trigger inversion problems. An invertible input space transformation function is proposed to map the input space to others, such as feature space, and to reconstruct the backdoor triggers. These trigger inversion methods can be considered as an optimization problem for the targeted class. They optimize the input images to mislead the model under various constraints and recover strong triggers in different spaces. The state-of-the-art trigger inversion method, BTI-DBF , increases the inversion efficiency by relaxing the dependency of trigger inversion on the target labels. BTI-DBF trains a trigger generator by minimizing the differences between benign samples and their generated poisoned version in decoupled benign features while maximizing the differences in remaining backdoor features. Feature space trigger inversion defenses are generic and effective against most backdoor attacks. However, we show that BTI-DBF may fail against BadNets, as its backdoor is not prominent in the feature space (see Section 3.4). In addition, existing works still suffer from huge computational overhead and overly rely on _prominent backdoor features_.

To resolve this shortcoming, we propose a backdoor defense called detecting Backdoors activated by Adversarial neuron Noise (BAN). Our defense is inspired by the finding that backdoored models are more sensitive to adversarial noise than benign models [43; 10], and neuron noise can be adversarially manipulated to indicate backdoor activations . Specifically, BAN generates adversarial neuron noise, where the weights of the victim model are adversarially perturbed to maximize the classification loss on a clean data set. Simultaneously, trigger inversion is conducted in the victim model's feature space to calculate the mask for benign and backdoor features. Clean inputs with masked feature maps are then fed to the adversarially perturbed model, based on the outputs of which backdoored models can be differentiated. Figure 1 presents a t-SNE visualization of the feature space for backdoored and clean models when they are perturbed with different levels of adversarial neuron noise. It can be observed that the backdoored model misclassifies parts of the data from all classes as the target class when the adversarial neuron noise increases, while the clean model has fewer misclassifications under the same level of noise. By leveraging the induced difference, BAN can successfully detect and mitigate backdoor attacks in both input and feature space.

We make the following contributions:

* We find a generalization shortcoming in current trigger inversion-based detection methods (FeatureRE , Unicorn , BTI-DBF ). In particular, feature space detections overly rely on highly distinguishable prominent backdoor features. We provide an in-depth analysis of trigger inversion-based backdoor defenses, showing that prominent backdoor features that are exploited by state-of-the-art defenses to distinguish feature space backdoors may not be suitable for the identification of input space backdoors.
* We propose detecting Backdoors activated by Adversarial neuron Noise (BAN) to mitigate this generalization shortcoming by introducing neuron noise into feature space trigger inversion. BAN includes an adversarial learning process to incorporate neuron activation information into the inversion-based backdoor detection. Experimental results demonstrate that BAN is 1.37\(\) (on CIFAR-10) and 5.11\(\) (on ImageNet200) more efficient with a 9.99% higher detect success rate than the state-of-the-art defense BTI-DBF .
* We also exploit the neuron noise to further design a simple yet effective defense for removing the backdoor, such that we build a workable framework.

## 2 Backdoors

**Attacks.** Backdoor attacks [9; 22; 2; 26; 25; 40; 35; 8; 20; 3] refer to injecting a secret functionality into the victim model that is activated through malicious inputs that contain the trigger. To this end, substantial research has been proposed by poisoning a small percentage of training data using small and static triggers [9; 6; 22]. Early attacks generate backdoors in input space, where BadNets  is the first backdoor attack in DNNs. Blend  proposed three injection strategies to blend translucent images into inputs of DNNs as triggers. The authors controlled the transparency of the trigger to allow the trade-off between strength of attack and invisibility. Although these attacks work well,their triggers are still perceptible to humans and can be easily detected by backdoor defenses, such as Activation Clustering (AC)  and NC .

Dynamic and imperceptible triggers [26; 25; 40; 8], including feature space backdoor triggers [20; 25], are explored to bypass both human observers and input space defenses. IAD  designs input-specific triggers. To evaluate the uniqueness of dynamic triggers, the authors designed a cross-trigger test to determine whether the trigger of one input is reusable to others. WaNet  proposes warping-based triggers, which are unnoticeable and smooth in the input space. Bpp  exploits vulnerabilities in the human visual system and, by using image quantization and dithering, introduces invisible triggers that are stealthier than previous attacks.

Adaptive backdoor attacks [31; 28; 24] are built to systematically evaluate defenses, where attacks discourage the indistinguishability of latent representations of poisoned and benign inputs in the feature space. Adap-Blend  divides the trigger image into 16 pieces and randomly applies only 50% of the trigger during data poisoning. They use the full trigger image at inference time to mislead the poisoned model. SSDT  considers the combination of source-specific and dynamic-trigger backdoors. Only the inputs from victim classes (source) with the dynamic triggers are classified to the target labels, which encourages the generalization of more diverse trigger patterns from different inputs. In this paper, we evaluate BAN against various types of attacks, including input space, feature space, and adaptive attacks, to provide a systematic evaluation resembling practical threats.

**Defenses.** Trigger inversion-based backdoor defense is considered one of the most practical and general defenses against backdoors [38; 39; 44]. The recovered trigger is used to determine whether the model is backdoored. For example, NC  reverses input space triggers to detect backdoors by selecting significantly smaller triggers in size. Other methods, such as ABS  and FeatureRE , usually determine whether there is a backdoor based on the attack success rate of the trigger. More specifically, given a DNN model \(f\) and a small set of clean data \(_{c}=\{_{n},y_{n}\}_{n=1}^{N}\), NC recovers the potential trigger by solving the following objective:

\[_{,}(f((1-)+ ),y_{t})+||,\] (1)

where \((,y)_{c}\) and \(\) is the trigger mask, \(\) is the trigger pattern, and \(y_{t}\) is the target label. The mask \(\) determines whether the pattern will replace the pixel. \(\) is the cross-entropy loss function. Most prior works [12; 30; 21; 38; 39] follow this design to conduct trigger inversion for all possible target labels. For example, Tabor  adds more constraints to the NC optimization problem according to the overly large (large size triggers but no overlaps with the true trigger) and overlaying (overlap the true trigger but with redundant noise) problem of NC. Moving from input space to feature, FeatureRE  utilizes feature space constraint according to an observation that neuron activation values representing the backdoor behavior are orthogonal to others. Unicorn  proposes a transformation function that projects from the input space to other spaces, such as feature space  and numerical space . Then, the authors conduct trigger inversion in different spaces.

Figure 1: The feature plots of backdoor and benign models with neuron noise using ResNet18 on CIFAR-10. The darker blue represents the target label. As noise increases, the backdoor model identifies more inputs from each class as the target label. The clean model has fewer errors, and there is no significant increase in the number of misclassifications to the target class.

Unlike previous NC-style methods, recent works [44; 37] explore different optimization objectives to avoid the time-consuming optimization for all possible target classes or to avoid the fixed mask-pattern design as advanced attacks utilize more complex and dynamic triggers. BTI-DBF  takes advantage of the prior knowledge that benign and backdoored features are decoupled in the feature space. It distinguishes them by the optimization objective, where benign features contribute to the correct predictions and backdoored features lead to wrong predictions. Based on the decoupled features, BTI-DBF trains a trigger generator by minimizing the difference between benign samples and their generated version according to the benign features and maximizing the difference according to the backdoored features. Feature space backdoor defenses are developed based on the fact that backdoor features are highly distinguishable from benign features. However, this finding is not consistently valid for input space attacks where the feature difference is small (See Sections 3.4).

## 3 BAN Method

### The Pipeline of Training Backdoor Models

For brevity, consider an \(L\)-layer fully connected network \(f\) (similar principles apply to convolutional networks) that has \(l=l_{1}+l_{2}++l_{L}\) neurons. \(_{n}^{d_{x}}\) and \(y_{n}\{0,1\}^{d_{y}}\) are the \(n_{}\) image and its label in \(d_{x}\) and \(d_{y}\) dimensional spaces, respectively. The attacker creates a poisoned dataset \(_{p}\) by poisoning generators \(G_{X}\) and \(G_{Y}\) for a subset of the whole training dataset, i.e., \(_{p}=_{c}_{b}\). \(_{c}\) is the original clean data. \(_{b}\) is the poisoned backdoor data, \(_{b}=\{(^{},y^{})|^{}=G_{X}( ),y^{}=G_{Y}(y),(,y)-_{c}\}\). In all-to-one attacks, \(G_{Y}(y)=y_{t}\), \(y_{t}\) is the attacker-specified target class. In our experiments, we consider the dirty-label attack. In all-to-all attacks, ususally \(G_{Y}(y)=(y+1)\)[44; 26; 40], which is also what we chose in our experiments. In the training stage, the backdoor is injected into the model by training with \(_{p}\), i.e., minimizing the training loss on \(_{p}\) to find the optimal weights and bias \((^{*},^{*})\):

\[_{,}_{_{p}}(, )=,y)_{p}}{}(f( ;,),y),\] (2)

where \((,)\) is the cross-entropy. In the inference stage, the backdoored model predicts an unseen input \(}\) as its true label \(\) but predicts \(G_{X}(})\) as \(G_{Y}()\): \(f(G_{X}(});,)=G_{Y}()\).

### Threat Model

**Attacker's goal.** We consider the attacker to be the pre-trained model's provider. The attacker aims to inject stealthy backdoors into the pre-trained models. The pre-trained models perform well on clean inputs but predict the attacker-chosen target label when receiving backdoor inputs.

**Attacker knowledge.** The attacker has white-box access to the model, including training data, architectures, hyperparameters, and model weights.

**Defender's goal and knowledge.** The main goal is to detect whether a given model is backdoored and then remove the potential backdoor according to the detection results. Following [38; 39; 44], we assume the defender has white-box access to the model and holds a few local clean samples. However, the defender does not have access to the training data and has no knowledge of the backdoor trigger.

### Detection with Neuron Noise

Based on previous findings that adversarial noise can activate backdoors [43; 41; 10], we design a two-step method for backdoor detection. First, we search for noise on neurons that can activate the potential backdoor. Then, we decouple the benign and backdoored features using a learnable mask of the latent feature (the output before the final classification layer).

**Neuron Noise.** In backdoor models, there are two types of neurons: benign and backdoor . The backdoor neurons build a shortcut from \(G_{X}()\) to \(G_{Y}(y)\). _Neuron noise_ is generated noises added on neurons to maximize model loss on clean samples in an adversarial manner . The noise on benign neurons evenly misleads prediction to all classes, while the noise on backdoor neurons tends to mislead prediction to the target label due to the backdoor shortcut, as shown in Figure 1. Therefore, backdoor models with noise behave differently from benign models with noise, as there are no backdoor neurons in the benign models.

Given the \(j_{}\) neuron connecting the \(i_{}\) layer and \((i-1)_{}\) layer, we denote its weight and bias with \(_{ij}\) and \(b_{ij}\), respectively. Neuron noise can be added to the neuron by multiplying its weight and bias with a small number: \((1+_{ij})_{ij}\) and \((1+_{ij})b_{ij}\). Then, the output of the neuron is:

\[h_{ij}=(1+_{ij})_{ij}^{}_{(i-1)}+(1 +_{ij})b_{ij},\] (3)

where \(_{(i-1)}\) is the output of the previous layer and \(()\) is the nonlinear function (activation). The noise on all neurons are represented by \(=[_{1,1},,_{l_{1},1},,_{1,L}, ,_{l_{L},L}]\) and \(=[_{1,1},,_{l_{1},1},,_{1,L},,_ {l_{L},L}]\). The \(\) and \(\) are optimized via a maximization to increase the cross-entropy loss on the clean data:

\[_{, S} _{_{c}}((1+),(1+ )),\\ S=B(;)=\{^{l}| \},\] (4)

where \(S\) is the ball function of the radius \(\) in the \(l\) dimensional space. \(\) and \(\) share the ball function \(S\) as the maximum noise size. The maximization in Eq. (4) can be solved by PGD  algorithm with a random start, as PGD can better explore the entire searching space to mitigate local minima .

**Feature Decoupling with Mask.** The neuron noise activates the backdoor (see Figure 1) and misleads the predictions to the target label. However, the performance has high variance when searching for noise multiple times, which we conjecture is caused by random initialization. Therefore, inspired by , we further introduce a feature decoupling process to enhance the effect of noise on backdoored features but maintain a decreased effect on benign features. Specifically, the network \(f\) is decomposed into \(g=f_{1} f_{L-1}\) and \(f_{L}\), where \(f_{L-1}\) extracts the latent features from \(\), while \(f_{L}\) is the classification layer. Then, we use a mask \(\) on top of the latent feature \(g()\) to decouple benign and backdoored features. The optimization objective can be written as:

\[_{}(f_{L}(g()),y)- (f_{L}(g()(1-)),y)+_{1}| |,\] (5)

where \(\) is the element-wise product. This optimization divides the latent features into two parts through the mask while maintaining a relatively small size of \(\). Note that the regularizer for \(\) in Eq. (5) is necessary. Otherwise, \(\) will become a dense matrix full of ones to focus on the positive part of Eq. (5) because maintaining only the positive part (without penalty of \(||\)) already satisfies the optimization objective. Finally, we apply the negative mask \((1-)\) on top of latent feature of \(f\) to enhance the backdoor effect. The final output is:

\[f_{L}g;(1+),(1 +)(1-).\] (6)

In Figure 2, the blue dots show the accuracy after adding noise to the model. The red dots show the accuracy (with noise) while applying the feature mask to the model using Eq. (6). Our feature masks do not affect the performance of benign models. However, we see that the performance is significantly decreased for backdoored models. This decrease is caused by the model only using backdoor features (through the negative mask), which means the backdoor is activated more frequently. Finally, a suspect model is determined backdoored if the prediction using Eq. (6) has a high attack success rate. For all-to-one attacks, the misclassification will be concentrated into one label, which is the target label. For all-to-all attacks, we can do the same as for all-to-one attacks but evaluate the prediction for each class independently.

### Improving BTI-DBF

This section shows that the most recent work, BTI-DBF , may fail to capture the backdoor features by its decoupling method in Table 2. We show how to patch BTI-DBF using a simple solution. The BTI-DBF is the original version, and BTI-DBF* is our improved version. The main pipeline of BTI-DBF consists of two steps: (1) decoupling benign and backdoor features and (2) trigger inversion

Figure 2: Model’s clean accuracy with (red dots) and without (blue dots) the mask defined in Eq. (6). Only the backdoored models are affected by the noise.

by minimizing the distance between benign features and maximizing the distance between backdoor features. We found that the defense's first step may introduce errors in the decoupled features. Similar to our Eq. (5), the decoupling of BTI-DBF can be written as:

\[_{}(f_{L}(g()),y)- (f_{L}(g()(1-)),y).\] (7)

However, this equation has no constraints on the feature mask \(\). Overall, the optimization objective is to decrease the loss. Obviously, BTI-DBF's decoupling encourages the norm of the mask to increase so that the loss will focus on the positive part and ignore the negative part because the negative part goes against the overall objective. Finally, the mask will be a dense matrix that is full of ones. The \(f_{L}(g()(1-))\) is ignored due to multiplying by zero. We propose a simple solution to fix the problem by adding a regularizer of the size of the mask to the loss, i.e., we use our Eq. (5) as the first step of BTI-DBF*. According to Table 2, BTI-DBF* successfully overcomes BTI-DBF's shortcomings.

### Backdoor Defense

After determining whether a suspect model is backdoored, we can fine-tune the backdoored model to remove the backdoor. However, standard fine-tuning using clean data does not effectively remove the backdoor because it does not activate it. Therefore, we propose using optimized neuron noise to fine-tune the model. In the optimization of the neuron noise, the objective is to increase the loss of \((f(),y)\). We consider both the benign and backdoor neurons to contribute to the increase in loss when optimizing the noise. Indeed, on benign neurons, the neuron noise misleads \(f\) to any result other than the true label \(y\), while the noise misleads \(f\) to the target label \(G_{Y}(y)\) on backdoor neurons. Therefore, a straightforward method is to decrease the loss between noise output and the true label. The loss for our noise fine-tuning can be written as:

\[_{,}(f(;,),y )+_{2}(f(;(1+),(1+)),y).\] (8)

## 4 Experimental Results

**Datasets and Architectures.** The datasets for our experiments include CIFAR-10 , GTSRB , Tiny-ImageNet , and a subset of ImageNet-1K . The ImageNet subset contains 200 classes, which is referred to as ImageNet200. BAN is evaluated using three architectures: ResNet18 , VGG16 , and DenseNet121 . Please refer to Appendix B.1 for more details.

**Attack Baselines.** Our experiments are conducted using seven attacks: BadNets , Blend , WaNet , IAD , BppAttack , Adap-Blend , and SSDT , which are commonly used in other works [44; 38; 39; 46; 29]. The BadNets  and Blend  are designed for input space. WaNet , IAD , and BppAttack  are designed for feature space. Adap-Blend  and SSDT  (for adaptive evaluation) are state-of-the-art attacks that have been recently introduced to bypass backdoor defenses. The main idea of Adap-Blend  and SSDT  is to obscure the latent separation in features of benign and backdoor samples. More details can be found in Appendix B.2.

Defense Baselines.BAN is compared to five representative methods: Neural Cleanse (NC) , Tabor , FeatureRE , Unicorn , and BTI-DBF . NC and Tabor are designed for input space attacks, while the other three are designed for feature space and dealing with the latest advanced attacks. BAN uses only 1% and 5% of training data for detection and defense, respectively. Note the data used for our defense is not used for training models, i.e., the defender has no knowledge of the model to be defended. More details can be found in Appendices B.3 and B.4. We use bold font to denote the best results.

### The Performance of Backdoor Detection

**Main Results.** In Table 1, BAN shows better results on CIFAR-10 than all baselines, especially on advanced attacks. Results on

Figure 3: BadNets features are weaker when using the mask to disentangle the benign and backdoor features. Defenses that are biased towards large differences may not work in cases like BadNets.

other datasets are presented in Appendix C. We note that the advanced detection methods (FeatureRE, Unicorn, and BTI-DBF) perform worse than NC on simple attacks (BadNets and Blend). We hypothesize this is because the backdoor features generated by BadNets are less obvious on feature channels than advanced attacks in the feature space, such as WaNet and IAD. Figure 3 shows that BadNets features are weaker than features from advanced attacks using the feature mask in Eq. (5). Specifically, we optimize the feature mask to disentangle the benign and backdoor features for four attacks. Then, we compute two cross-entropy loss values using the positive mask (\(\)) and the negative mask (\(1-\)) for benign and backdoor features, respectively. The average loss values and standard deviation for four models for each attack are plotted in Figure 3. The negative loss value of BadNets is much smaller than others, which means BadNets features are weaker than others with regard to misleading the model to the backdoor target. Recent defenses usually add more regularizers to their losses and optimization objectives to counteract powerful backdoor attacks. These regularizers encourage the capturing of strong features but omit weak ones. Thus, recent advanced detections can perform worse on BadNets than NC.

**Time consumption.** BAN is efficient and scalable as we do not iterate over all target classes. Figure 4 demonstrates that BAN uses substantially less time than all baselines. BAN is also more scalable for larger architectures or datasets. BIT-DBF (76.90s) is 1.37\(\) slower than BAN (55.95s) on CIFAR-10 with ResNet18, and BIT-DBF (5,792.51s) without pre-training is 5.11\(\) slower than ours (1,132.85s) on ImageNet200 with ResNet18. FeatureRE (297.74s) is 5.32\(\) slower than ours on CIFAR-10 with ResNet18, and it (6,053.62s) is 45.14\(\) slower on CIFAR-10 with DenseNet121 than ours (134.10s).

### The Performance of Backdoor Defense

A complete backdoor defense framework should include both detection and defense. The goal of defense is to decrease the attack success rate (ASR) of backdoor triggers. Detection before the defense is also necessary because the defense usually decreases the performance on benign inputs . In Section 3.5, we propose a simple and effective fine-tuning method using the noise that activates the backdoor. Table 3 compares BAN with three baselines: plain fine-tuning, FeatureRE , and BTI-DBF(U) . Plain fine-tuning refers to training the backdoor model using the same hyperparameters as BAN but without the noise loss in Eq. (8). The FeatureRE 

    &  &  &  &  &  & BTI-DBF* &  \\   & & Bd. & Acc. & Bd. & Acc. & Bd. & Acc. & Bd. & Acc. & Bd. & Acc. & Bd. & Acc. \\   & No Attack & 0 & **100\%** & 0 & **100\%** & 2 & 90\% & 6 & 70\% & 0 & **100\%** & 0 & **100\%** \\  & BadNets & 20 & **100\%** & 20 & **100\%** & 14 & 70\% & 18 & 90\% & 18 & 90\% & 20 & **100\%** \\  & Blend & 20 & **100\%** & 20 & **100\%** & 20 & **100\%** & 19 & 95\% & 20 & **100\%** & 18 & 90\% \\  & WaNet & 11 & 55\% & 8 & 40\% & 15 & 75\% & 20 & **100\%** & 18 & 90\% & 20 & **100\%** \\  & IAD & 0 & 0\% & 0 & 0\% & 15 & 75\% & 11 & 55\% & 20 & **100\%** & 20 & **100\%** \\  & Bpp & 0 & 0\% & 1 & 5\% & 12 & 60\% & 17 & 85 \% & 20 & **100\%** & 20 & **100\%** \\   & No Attack & 0 & **100\%** & 0 & **100\%** & 3 & 85\% & 6 & 70\% & 6 & 70\% & 0 & **100\%** \\  & BadNets & 18 & 90\% & 16 & 80\% & 13 & 65\% & 16 & 80\% & 18 & 90\% & 19 & **95\%** \\  & Blend & 19 & **95\%** & 19 & **95\%** & 16 & 80\% & 18 & 90\% & 16 & 80\% & 17 & 85\% \\  & WaNet & 10 & 50\% & 9 & 45\% & 12 & 60\% & 18 & 90\% & 16 & 80\% & 20 & **100\%** \\  & IAD & 0 & 0\% & 0 & 0\% & 8 & 40\% & 17 & 85\% & 20 & **100\%** & 20 & **100\%** \\  & Bpp & 9 & 45\% & 10 & 50\% & 5 & 25\% & 15 & 75\% & 14 & 70\% & 18 & **90\%** \\   & No Attack & 0 & **100\%** & 0 & **100\%** & 5 & 75\% & 8 & 60\% & 3 & 85\% & 0 & **100\%** \\  & BadNets & 18 & 90\% & 20 & **100\%** & 19 & 95\% & 15 & 75\% & 17 & 85\% & 20 & **100\%** \\  & Blend & 20 & **100\%** & 20 & **100\%** & 12 & 60\% & 18 & 90\% & 19 & 95\% & 20 & **100\%** \\  & WaNet & 13 & 65\% & 10 & 50\% & 20 & **100\%** & 17 & 85\% & 14 & 70\% & 19 & 95\% \\  & IAD & 0 & 0\% & 0 & 0\% & 14 & 70\% & 16 & 80\% & 14 & 70\% & 19 & **95\%** \\  & Bpp & 0 & 0\% & 0 & 0\% & 16 & 80\% & 8 & 40\% & 16 & 80\% & 20 & **100\%** \\  Average & & & 60.56\% & 59.17\% & 72.5\% & 78.61\% & 86.39\% & **97.22\%** \\   

Table 1: The detection results under different model architectures on CIFAR-10. The “Bd.” refers to the number of models the defense identifies as backdoored. The “Acc.” refers to detection success accuracy. The best results are marked in bold. BTI-DBF* refers to an improved version (details in Section 3.4).

and BTI-DBF(U)  refer to the defense methods from the respective paper. We use default hyperparameters for FeatureRE  and BTI-DBF(U) . For plain fine-tuning and BAN, we use a small learning rate (0.005) to avoid jumping out of the current optimal parameters of the well-trained model. Then, we use \(_{2}\) (0.5) for Eq. (8) for the trade-off between robustness against backdoor and clean accuracy. We fine-tune for a short schedule of 25 epochs, as the model is well-trained. Table 8 in Appendix B.4 shows defense performance with standard deviation on different hyperparameters, which supports our choice. Tables 3 and 4 demonstrate that our fine-tuning method effectively removes the backdoor while preserving high accuracy in benign inputs. We provide a comparison between our find-tuning with ANP  in Table 12, Appendix C.4, as ANP uses the neuron noise for pruning backdoor neurons.

### Defense against All-To-All Attacks

Previous works [38; 39] usually only consider all-to-one attacks, which limits the application in practical situations. In this section, we evaluate our fine-tuning method under three all-to-all attacks: WaNet-All , IAD-All , and Bpp-All . FeatureRE is designed for all-to-one attacks, so we use target label 0 here for FeatureRE. FeatureRE is included to show that the all-to-one defense does not work in the all-to-all setting. BAN is capable of handling all-to-all attacks because we directly explore the neurons themselves instead of optimizing for the potential targeted label. Table 5 demonstrates the effectiveness of our method.

    &  &  &  \\   & Bd. & Acc. & Bd. & Acc. & Bd. & Acc. \\  No Attack & 0 & **100\%** & 0 & **100\%** & 0 & **100\%** \\ BadNets & 5 & 25\% & 18 & 90\% & 20 & **100\%** \\ Blend & 0 & 0\% & 20 & **100\%** \\ WaNet & 7 & 35\% & 18 & 90\% & 20 & **100\%** \\ IAD & 19 & 95\% & 20 & **100\%** & 20 & **100\%** \\ Bpp & 20 & **100\%** & 20 & **100\%** & 20 & **100\%** \\   

Table 2: The detection results of BTI-DBF and BTI-DBF* using ResNet18 and CIFAR-10.

Figure 4: Time consumption of detection baselines on ResNet18 (in seconds) for all three datasets. BAN uses significantly less time than the baselines.

    &  &  &  &  &  \\   & BA & ASR & BA & ASR & BA & ASR & BA & ASR & BA & ASR \\  BadNets & 93.37 & 99.41 & 92.93 & 87.81 & **93.15** & 99.79 & 91.26 & 13.12 & 92.06 & **1.97** \\ Blend & 94.60 & 100.00 & 93.07 & 99.99 & **93.20** & 93.28 & 91.86 & 100.00 & 92.72 & **4.10** \\ WaNet & 93.57 & 99.37 & 93.05 & 1.10 & **93.67** & **0.03** & 90.30 & 4.89 & 92.05 & 0.91 \\ IAD & 93.17 & 97.88 & **94.11** & 0.46 & 92.73 & **0.0** & 89.54 & 1.59 & 92.78 & 1.48 \\ Bpp & 94.29 & 99.93 & 93.85 & 4.46 & **94.21** & 98.13 & 90.61 & 2.73 & 92.54 & **2.58** \\  Average & 93.80 & 99.32 & **93.40** & 38.76 & 93.39 & 47.45 & 90.71 & 24.47 & 92.43 & **2.21** \\   

Table 3: Defense against 5 attacks using ResNet18. BA refers to benign accuracy on clean data.

### Evaluation under Adaptive Attack

Attackers may design a specific attack for defense when they know its details . In this section, we evaluate BAN against two attacks that attempt to bypass the difference between backdoor and benign features: Adap-Blend  and SSDT attack . Both Adap-Blend  and SSDT attack  try to obscure the difference between benign and backdoor latent features. Adap-Blend  achieves it by randomly applying 50% of the trigger to poison the training data, while SSDT attack  utilizes source-specific and dynamic-triggers to reduce the impact of triggers on samples from non-victim classes. The source-specific attack refers to backdoor triggers that mislead the model to the target class only when applied to victim class samples. Table 6 demonstrates our approach is resistant to these two attacks, while other methods fail. The reason is that the backdoor features of SSDT are close to benign features in the feature space. It is difficult for other methods to distinguish between backdoor and benign features created by SSDT. Our detection method directly analyzes the model itself using neuron noise, which captures the difference between backdoor and benign models concerning parameters. See Appendix C.3 for the mitigation results of our method against these adaptive attacks.

### Analysis on Prominent Features

We provide additional analysis of the phenomenon that backdoor features are more prominent for advanced attacks (WaNet, IAD, and Bpp) than weaker attacks (BadNets, Blend). Table 7 demonstrates that previous decoupling methods cannot easily pick up backdoor features from weak attacks, such as BadNets. In particular, when detecting without the \(L_{1}\) regularizer (i.e., w/o norm), the negative feature loss of BadNets is high with a very large \(L_{1}\) mask norm, while the Bpp has an even higher negative loss with a much smaller mask norm. The high negative loss of BadNets is actually from the sparse feature mask rather than backdoor features, i.e., there are too many zeros in (1 - m). This indicates that BadNet backdoor features are less prominent than Bpp features, making it more challenging to decouple BadNets features.

    &  &  &  &  &  &  \\   & Bd. & Acc. & Bd. & Acc. & Bd. & Acc. & Bd. & Acc. & Bd. & Acc. & Bd. & Acc. \\  Adap-Patch & 18 & 90\% & 15 & 75\% & 17 & 85\% & 20 & **100\%** & 20 & **100\%** & 20 & **100\%** \\ SSDT & 0 & 0\% & 0 & 0\% & 0 & 0\% & 0 & 0\% & 20 & **100\%** & 20 & **100\%** \\   

Table 6: The detection results under adaptive attacks on using CIFAR-10 and ResNet18.

    &  &  &  &  &  \\   & & BA & ASR & BA & ASR & BA & ASR & BA & ASR \\   & WaNet & 58.32 & 99.85 & **51.53** & 1.3 & 39.49 & 0.96 & 50.69 & **0.86** \\  & IAD & 58.54 & 99.32 & **51.86** & 1.72 & 38.79 & **0.60** & 50.04 & 0.76 \\  & Bpp & 60.63 & 99.89 & **57.72** & 0.15 & 46.84 & 0.40 & 57.66 & **0.10** \\   & WaNet & 77.01 & 99.74 & 66.71 & 0.78 & 63.47 & 1.0 & **69.95** & **0.58** \\  & IAD & 76.72 & 99.75 & 69.91 & **0.42** & 64.33 & 1.24 & **72.18** & 1.30 \\   & Bpp & 78.56 & 99.88 & 70.89 & **0.82** & 67.02 & 3.10 & **72.59** & 2.68 \\   

Table 4: Defense of backdoor attacks on Tiny-ImageNet and ImageNet200 using ResNet18.

    &  &  &  &  &  \\   & BA & ASR & BA & ASR & BA & ASR & BA & ASR & BA & ASR \\  WaNet-All & 93.60 & 91.86 & 92.65 & 18.03 & **93.33** & 91.96 & 91.30 & 1.72 & 92.29 & **1.11** \\ IAD-All & 92.96 & 90.62 & **93.19** & 3.72 & 93.06 & 91.20 & 91.36 & 3.72 & 92.31 & **1.13** \\ Bpp-All & 94.45 & 84.68 & **93.90** & 1.58 & 94.32 & 83.87 & 90.16 & 2.05 & 93.23 & **1.38** \\  Average & 93.67 & 89.05 & 93.25 & 7.78 & **93.57** & 89.01 & 90.94 & 2.49 & 92.61 & **1.21** \\   

Table 5: Defense against all-to-all backdoor attacks. BA refers to benign accuracy on clean data.

## 5 Conclusions and Future Work

This paper proposes an effective yet efficient backdoor defense, BAN, that utilizes the adversarial neuron noise and the mask in the feature space. BAN is motivated by the observation that traditional defenses outperformed the latest feature space defenses on input space backdoor attacks. To this end, we provide an in-depth analysis showing that feature space defenses are overly dependent on prominent backdoor features. Experimental studies demonstrate BAN's effectiveness and efficiency against various types of backdoor attacks. We also show BAN's resistance to potential adaptive attacks. Future studies could explore a more practical detection method without assuming access to local benign samples and better strategies for decoupling features because a fixed mask in the feature is not always aligned with the benign and backdoor features.