# Universal In-Context Approximation

By Prompting Fully Recurrent Models

 Aleksandar Petrov, Tom A. Lamb, Alasdair Paren, Philip H.S. Torr, Adel Bibi

Department of Engineering Science

University of Oxford

aleks@robots.ox.ac.uk

###### Abstract

Zero-shot and in-context learning enable solving tasks without model fine-tuning, making them essential for developing generative model solutions. Therefore, it is crucial to understand whether a pretrained model can be prompted to approximate any function, i.e., whether it is a universal in-context approximator. While it was recently shown that transformer models do possess this property, these results rely on their attention mechanism. Hence, these findings do not apply to fully recurrent architectures like RNNs, LSTMs, and the increasingly popular SSMs. We demonstrate that RNNs, LSTMs, GRUs, Linear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin can also serve as universal in-context approximators. To streamline our argument, we introduce a programming language called LSRL that compiles to these fully recurrent architectures. LSRL may be of independent interest for further studies of fully recurrent models, such as constructing interpretability benchmarks. We also study the role of multiplicative gating and observe that architectures incorporating such gating (e.g., LSTMs, GRUs, Hawk/Griffin) can implement certain operations more stably, making them more viable candidates for practical in-context universal approximation.

## 1 Introduction

Until recently, solving a task with machine learning required training or fine-tuning a model on a dataset matching the task at hand. However, large foundation models exhibit the ability to solve new tasks without being specifically fine-tuned or trained for them: often it is sufficient to simply prompt them in the right way. Prompting has been especially successful because of _in-context learning_: the ability to modify the model's behavior with information provided within the input sequence, without changing the underlying model parameters (Brown et al., 2020). Yet, we know little about the theoretical properties of prompting. It is not even clear if there are limits to what can be achieved with prompting or, conversely, whether it is possible to prompt your way into any behaviour or task.

This can be framed as a universal approximation question. Classically, universal approximation results show how a class of tractable functions, such as neural networks, approximates another class of concept functions, e.g., all continuous functions on a bounded domain, with arbitrary accuracy. This is often done by showing that one can choose _model parameters_ that approximate the target function. However, in-context learning poses a different challenge as the model parameters are _fixed_. Instead, a part of the input (the prompt) is modified to cause the model to approximate the target function. Hence, we define universal _in-context_ approximation to be the property that there exist fixed weights such that the resulting model can be prompted to approximate any function from a concept class. Understanding whether a model can be a universal _in-context_ approximator is especially important as most commercial models are accessible exclusively via a prompting interface (La Malfa et al., 2023).

In-context learning has been almost exclusively studied in conjunction with the transformer architecture (Vaswani et al., 2017). This is likely because in-context abilities appear once the models are large enough (Wei et al., 2021) and most large models have been transformer-based. On the subject of universal in-context approximation, Wang et al. (2023) were first to show that a transformer possesses this property by discretising and memorising all possible functions in the model weights. Memorisation is not needed, though, and even small transformers can be universal approximators when prompted Petrov et al. (2024). Both results, however, critically depend on the attention mechanism of the transformer architecture (Bahdanau et al., 2015).

Still, generative models are not restricted to attention-based architectures: there are the "classic" recurrent neural networks (RNNs, Amari, 1972), long short-term memory models (LSTMs, Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRUs, Cho et al., 2014). Recently, Linear RNN models (also known as state-space models or SSMs) were proposed as a scalable alternative to the transformer architecture (Orvieto et al., 2023; Fu et al., 2023) and have started to outperform similarly-sized transformers when multiplicative gating is added (Gu and Dao, 2023; De et al., 2024; Botev et al., 2024). Furthermore, despite in-context learning being associated with the transformer, recent empirical results show in-context learning in SSMs, RNNs, LSTMs and even convolutional models (Xie et al., 2022; Akyurek et al., 2024; Lee et al., 2024).

Yet, despite their ability to be in-context learners, there is little known about the theoretical properties of these fully recurrent architectures. As these architectures become more and more widely used, understanding their in-context approximation abilities is increasingly more important for their safety, security and alignment. We show that, in fact, many of these architectures, similarly to transformers, can be universal in-context approximators. Concretely, our contributions are as follows:

1. We develop _Linear State Recurrent Language_ (LSRL): a programming language that compiles to different fully recurrent models. Programming in LSRL is akin to "thinking like a recurrent model". LSRL programs can then be implemented exactly as model weights.
2. Using LSRL, we construct Linear RNN models that can be prompted to act as any token-to-token function over finite token sequences, or to approximate any continuous function. These results also hold for RNNs, LSTMs, GRUs and Hawk/Griffin models (De et al., 2024).
3. We present constructions with and without multiplicative gating. However, we observe that the constructions without these gates depend on numerically unstable conditional logic.
4. Nevertheless, we show that multiplicative gates lead to more compact and numerically stable models, making it more likely that universal in-context approximation properties arise in models utilising them, such as LSTMs, GRUs and the latest generation of Linear RNNs.

## 2 Preliminaries

Fully recurrent architectures.In this work, we focus exclusively on fully recurrent neural network architectures. Recurrent models operate over sequences. Concretely, consider an input sequence \((_{1},,_{N})\) with \(_{t}\), \(\) being some input space. We will refer to the elements of the input sequence as _tokens_ even if they are real-valued vectors. A recurrent model \(g:^{}\) maps a sequence of inputs to an output in some output space \(\). These models are always causal, namely:

\[_{t}=g(_{1},,_{t}).\] (1)

We will abuse the notation and refer to \((_{1},...,_{t}){=}(g(_{1}),...,g(_{1},...,_{t}))\) as simply \(g(_{1},...,_{t})\). We will also separate the input sequence into a query \((_{1},...,_{n})\) and a prompt \((_{1},...,_{N})\). The prompt specifies the target function \(f\) that we approximate while the query designates the input at which we evaluate it. Contrary to the typical setting, we will place the query before the prompt.1

There are various neural network architectures that fall under the general framework of Eq. (1). The quintessential one is the RNN. It processes inputs one by one with only a non-linear state being passed from one time step to the other. A model \(g\) can thus be stacked RNN layers, each one being:

\[_{t} =(_{t-1}+_{t}+),\] (Classic RNN) \[_{t} =(_{t}),\] (2)with \(,,\) and the initial state value \(_{0}\) being model parameters, \(\) a non-linear activation function and \(\) a multi-layer perceptron (MLP) with \(\) activations. We assume that \(\) is always a \(\) to keep the analysis simpler. The non-linearity in the state update can make the model difficult to train (vanishing and exploding gradients, Bengio et al., 1994). Therefore, Linear RNNs have been proposed as regularizing the eigenvalues of \(\) can stabilise the training dynamics (Orvieto et al., 2023). Linear RNNs also admit a convolutional representation, making them trainable in parallel (Gu et al., 2021; Fu et al., 2023). Linear RNNs drop the non-linearity from the state update in Eq. (2):

\[_{t} =_{t-1}+_{t}+,\] (Linear RNN) \[_{t} =(_{t}).\] (3)

The fully linear state updates do not affect the expressivity of the models, as non-linear activations are nevertheless present in the MLP layers \(\) between the linear state update layers (Wang and Xue, 2023; Boyd and Chua, 1985). The state-of-the-art Linear RNN models also utilise some form of multiplicative gating (Gu and Dao, 2023; De et al., 2024; Botev et al., 2024). While specific implementations can differ, we can abstract it as the following Gated Linear RNN architecture:

\[_{t} =_{t-1}+_{t}+,\] (Gated Linear RNN) \[_{t} =(_{t})(_{t}),\] (4)

with \(\) being another MLP and \(\) being the element-wise multiplication operation (Hadamard product). Eq. (4) encompasses a range of recently proposed models. For example, one can show that any model consisting of \(L\) stacked Gated Linear RNN layers, with \(\) and \(\) with \(k\) layers, can be represented as a \(L(k{+}2)\)-layer Hawk or Griffin model (De et al., 2024). The conversions are described in detail in App. E. We can similarly add multiplicative gating to the classic RNN architecture:

\[_{t} =(_{t-1}+_{t}+),\] (5) \[_{t} =(_{t})(_{t}),\]

Eq. (5) may appear unusual but it is related to the well-known GRU (Cho et al., 2014) and LSTM (Hochreiter and Schmidhuber, 1997) architectures. Same as the case with Griffin/Hawk, any Gated RNN can be represented as a \(L(k{+}2)\)-layer GRU or LSTM model (details in Apps. C and D). As a result, if there exists a Gated RNN model that is a universal in-context approximator (which we later show to be the case), then there also exist GRU and LSTM models with the same property.

Theoretical understanding of in-context learning.Beyond the question of universal in-context approximation, there have been attempts to theoretically understand in-context learning from various perspectives. The ability to learn linear functions and perform optimization in-context has been extensively explored in the context of linear regression (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023; Fu et al., 2023; Zhang et al., 2023; Ahn et al., 2023), kernel regression (Han et al., 2023) and dynamical systems (Li et al., 2023). Furthermore, studies have explored how in-context learning identifies and applies the appropriate pretraining skill (Xie et al., 2022; Coda-Forno et al., 2023; Bai et al., 2023). It has also been shown that transformers can construct internal learning objectives and optimize them during the forward pass (von Oswald et al., 2023; Dai et al., 2023). However, these studies almost exclusively focus on the transformer architecture, and the applicability of their findings to fully recurrent models remains unclear.

Approximation theory.Let \(\) and \(\) be normed vector spaces. Take a set of functions \(^{}\) from \(\) to \(\) called a _concept space_. Take also a set of nicely behaved functions \(^{}\), called _hypothesis space_. \(\) could be any set that we have tools to construct and analyse, e.g., all polynomials or all neural networks of a particular architectural type. Approximation theory is concerned with how well functions in \(\) approximate functions in \(\). We say that \(\)_universally approximates_\(\) over a compact domain \(\) (or that \(\)_is dense in \(\)_) if for every \(f{}\) and \(0\) there exist a \(h{}\) such that \(_{}|f(){}h()|{}\). There is a long history of studying the concept class of continuous functions and hypothesis classes of single hidden layer neural networks (Cybenko, 1989; Barron, 1993) or deeper models (Hornik et al., 1989; Telgarsky, 2015). The concept class of sequence-to-sequence functions has been shown to be universally approximated with the hypothesis classes of transformers (Yun et al., 2019), RNNs (Schafer and Zimmermann, 2006) and Linear RNNs (Wang and Xue, 2023).

The hypothesis spaces in this work are different. The model is fixed and only the prompt part of the input is changed, i.e., all learnable parameters are in the prompt. Take a recurrent model \(g\) as in Eq. (1) with _fixed_ model parameters and a query length \(n\). The hypothesis class is all functions that result by calling \(g\) on the user query followed by the prompt and taking the last \(n^{}\) outputs:

\[_{g}^{^{n}}=\{(_{1},,_{n}) g(_{1},,_{n},_{1},,_{N})[ n^{}\,;] _{i},N>0\}.\] (6)The domain \(\) of \(_{i}\) and \(_{i}\) can be continuous embeddings in \(^{d}\) or discrete tokens \(=\{1,...,V\}\). Note that each \(h{}_{g}\) is identified by a prompt \((_{1},...,_{N})\) but is a function with domain all possible queries \((_{1},...,_{n})\). Therefore, finding a hypothesis \(h{}_{g}\) that approximates a target function \(f\) is equivalent to finding the prompt of that hypothesis. The approximation properties of \(_{g}\) in Eq. (6) depend on the architecture of \(g\), as well as its specific parameters.

We study the recurrent architectures in Eqs. (2) to (5) and their ability to approximate continuous functions over real-valued vectors and to represent discrete maps over tokens (which corresponds to how language models are used in practice). We consider the following classes of functions. \(^{}{=}(^{d_{}})^{^{d_{n}}}\) contains all continuous functions from the unit hypercube to \(^{d_{}}\), while \(^{}{=}\{h{}(^{l})^{^{l}} h \}\) all causal functions from \(l\) tokens to \(l\) tokens. The hypothesis classes are \(^{}(g)\) corresponding to Eq. (6) with \(D{=}^{d_{n}},n{=}n^{}{=}1\) and \(g\) some _fixed_ model of one of the four architectures in Eqs. (2) to (5), and \(^{}(g)\) with \(D{=}\) and \(n{=}n^{}{=}l\).

## 3 Linear State Recurrent Language (LSRL)

We can construct the weights for universal in-context models with the architectures in Eqs. (2) to (5) by hand but this is labour-intensive, error-prone, difficult to interpret, and the specific weights would be architecture-dependent. Working at such a low level of abstraction can also obfuscate common mechanisms and design patterns, making it more difficult to appreciate both the capabilities and the constraints of fully recurrent architectures. Instead, we propose a new programming language: _Linear State Recurrent Language_ (LSRL).2 LSRL programs compile to the four architectures in Eqs. (2) to (5). Conversely, any Linear RNN can be represented as an LSRL program, making LSRL a versatile tool for studying the capabilities of recurrent models. Later, in Secs. 4 to 6 we make use of LSRL to develop programs that are universal approximators for \(^{}\) and \(^{}\), thus showing that all four architectures can be universal in-context approximators.

LSRL syntax.An LSRL program specifies how a single element is processed and how the recurrent states are updated for the next element. LSRL programs always start with an \(()=\) and an \(\) of a fixed dimension. Only one Input can be declared in a program. Linear layers and \(\) are also supported: \([,]():=+\), \(():=(,)\). The unique component of LSRL, however, is its \(\) operation implementing the linear state update in Linear RNNs (Eq. (3)): \([,,,_{0}](_{t}):=_{t -1}+_{t}+\), where the state \(_{t-1}\) is the output of the call this node at step \(t-1\). \(\) is the only way information can be passed from previous tokens to the current one. We also provide a \(\) operation that combines

Figure 1: **Compilation of an LSRL program to a Linear RNN.** An example of a simple LSRL program that takes a sequence of 0s and 1s as an input and outputs 1 if there have been more 1s than 0s and 0 otherwise. The LSRL compiler follows the rules in App. A to simplify the computation DAG into a path graph. The resulting path graph can be represented as a Linear RNN with one layer.

variables: \((,):=(_{1},...,_{||},_{1},..,_{||})\). Finally, to support gating architectures we also implement a rudimentary Multi operation that splits its input into two sub-arrays and returns their element-wise multiplication: \(():=:||/_{2}}} :||/_{2}}}:||/_{2}}}:\). Naturally, Multi requires that \(\) has even length. These six operations can be composed into a direct acyclic graph (DAG) with a single source node (the Input variable) and a single sink node (marked with a return statement). Such a program operates over a single token \(_{t}\) passed to Input, while a recurrent model needs to operate over sequences. Thus, we wrap the program into a ForEach loop that passes each element individually for the DAG to output a variable denoted by a return clause. Each element is processed by the exact same program, with the only difference being that the state of the LinState variables is changing between iterations. You can see an example of a small LSRL program in Fig. 1.

Expressiveness limitations.ForEach does not behave like the typical for loop: only the states are accessible between iterations, i.e., you cannot use the output of a linear layer at step \(t\) in any computation at step \(t+1\). Furthermore, as the program is a DAG and only states of LinState nodes are passed between iterations, variables computed in latter operations of a previous time step are not accessible as inputs in earlier layers (with respect to the topological sorting of the computation graph). This leads to a key programming paradigm in LSRL: a LinState update cannot depend non-linearly on its own state. That includes it depending on a variable that depends on the LinState itself and conditional updates to the state. Such a dependency would break the DAG property of the program.3 This poses serious limitations on what algorithms can be expressed in a Linear RNN and makes programming them challenging. Still, in Sec. 4 we show how carefully constructing state updates and auxiliary variables can nevertheless allow to program some limited conditional behaviours.

Complication.Any LSRL program without Multi nodes can be compiled to a Linear RNN (Eq. (3)) or to a Gated Linear RNN (Eq. (4)). If the program has Multi nodes, then it cannot be compiled to a Linear RNN as the multiplicative gating cannot be implemented exactly. However, it can be compiled to a Gated Linear RNN. To compile an LSRL program to a Linear (Gated) RNN, we first parse the program to build a computation graph. This is a DAG with a single source (the Input node) and a single sink (the return statement of the ForEach loop). At the same time, a Linear (Gated) RNN can be represented as a path graph (no branching) with the six basic operations as nodes. Therefore, the compilation step needs to transform this DAG into a path graph. We achieve that by iterativly collapsing the first branching point into a single node. The exact rules that achieve that are described in App. A. Later, in Sec. 6, we will show how any Linear (Gated) RNN can be converted into a _non-linear_ (Gated) RNN, hence, how we can compile LSRL programs to these architectures as well.

Syntactic sugar.To make programming easier, we define several convenience functions. For instance, we can Slice variables \([l{:}u]\) via sparse Lin layers. We can also sum variables and element-wise multiplication with scalars (implemented as Lin layers). For logical operations we also need step functions which can be approximated with RelUs: \([]():=()-(-}{{}})\), where \(\) is a positive constant controlling the quality of the approximation. We can also approximate bump functions (1 between \(l\) and \(u\) and 0 otherwise): \([,,]():=[](-)-[](-)\). Similarly, we can approximate conjunction (\(\)), disjunction (\(\)), negation (\(\)), and comparison operators (\(\) and \(\)). See App. F for the definitions.

Critically, we need also a conditional operator that assigns a value \(()\) if a certain condition is met and another value \(()\) otherwise. One way to implement this is:

\[[,, ,]()&:=(\, ()())+(\,(())())\\ &-(\,()( ))-(\,(()) ()),\] (7)

where \(\) is a constant that is larger than any absolute value that \(()\) and \(()\) can attain. This construction, however, is not numerically stable and we will study alternatives in Sec. 5. We provide both numerical (SciPy.sparse, Virtanen et al.2020) and symbolic (SymPy, Meurer et al.2017) backends with the second being crucial for programs that are not numerically stable.

Prior work on encoding algorithms in model weights.A similar approach to developing a programming language that compiles to model weights was already done for the transformer architecture with the RASP language (Weiss et al., 2021) and the Tracr compiler (Lindner et al., 2023). They were predominantly created as a tool for interpretability research. In a sense, RASP is to a transformer as LSRL is to a (Linear) (Gated) RNN. Hence, can be used to develop benchmarks for interpretability methods for fully-recurrent architectures. However, while RASP can only express a subset of transformer models, LSRL is isomorphic to the set of all (Gated) Linear RNNs (though not to the non-linear ones). That means that any (Gated) Linear RNN can be represented and analysed as an LSRL program and vice versa. Hence, the limitations of what you can express in LSRL are also limitations of what a Linear (Gated) RNN can do. Namely: (_i_) we cannot have exact multiplicative interactions between inputs without multiplicative gates, and (_ii_) we cannot have state variable updates depending non-linearly on their previous iterations or in any way on a variable that depends on them.

Figure 2: **Intuition behind the LSRL program for universal in-context approximation for continuous functions in List. 1. Our target function \(f\) has input dimension \(d_{}=2\) and output dimension \(d_{}=1\). Each input dimension is split into two parts, hence \(=}{{2}}\). We illustrated an example input sequence of length 5: one for the query and four for the prompt tokens corresponding to each of the discretisation cells. The query \((q_{1},q_{2})\) falls in the cell corresponding to the third prompt token. We show how the two LinState variables in the program are updated after each step. Most notably, how the state holding the output y is updated after \(_{3}\) is processed.**

## 4 Universal In-Context Approximation with Linear RNNs

We proceed with building LSRL programs that are universal in-context approximators: one for approximating continuous functions (\(^{}\)), and one for maps between token sequences (\(^{}\)).

### Approximating continuous functions in \(^{}\)

The idea behind the approximation for continuous functions is to discretise the domain into a grid and approximate the function as constant in each cell of the grid. This technique is commonly used for showing universal approximation using the step activation function (Blum and Li, 1991; Scarselli and Tsoi, 1998). However, it is not obvious how to implement this approach in-context when information across input tokens can be only combined linearly. Consider a target function \(f:^{d_{}}{}^{d_{}}\) and a discretization step \(\). Our approach is to describe the value of \(f\) in each of the discretization cells as a single prompt token. For the cell with lower bounds \(l_{1},,l_{d_{}}\) and their respective upper bounds \(l_{1}{+},...,l_{d_{}}{+}\), the corresponding prompt token is a \((d_{}{+}d_{}{+}1)\)-dimensional vector:

\[=[,l_{1},,l_{d_{}},}_{1},}_{d_{}}]^{},\] (8)

where \(}\) is the value of \(f\) at the centre of that cell: \(}=f(l_{1}{+}}{{2}},...,l_{d_{}}{+} }{{2}})\). Each prompt token describes the size of the cell (the discretisation step \(\)), its starting lower bound, and the value of the target function at the centre of the cell. Thus, \(}{{}}^{d_{}}\) such tokens, one for each cell, are sufficient to describe the piece-wise constant approximation of \(f\). A query \(^{}^{d_{}}\) can fall in only one of the cells. We pad it with zeros and encode it as the first input element: \(=[^{},_{d_{}}^{}]^{}\), followed by the prompt. Our program will extract and save \(^{}\) to a state and then process the prompt tokens one at a time until it finds the one whose cell contains \(^{}\). The target function value for this cell will be added to an accumulator state. If the current cell does not contain \(^{}\), then \(0\) is instead added.Hence, the accumulator's final value corresponds to the value of \(f\) at the centre of the cell containing \(^{}\). The full LSRL program is provided in Lst. 1 and an illustration for \(d_{}=2,d_{}=1,=}{{2}}\) is shown in Fig. 2. The prompt length required to approximate an \(L\)-Lipschitz function \(f\) (w.r.t. the \(_{2}\) norm) to precision \(\) is \(N=(}{{L}}/_{}}}{{2}})^{- d_{}}=(^{^{d_{}}})\) (see App. B for the proof). Asymptotically, this is as good as one can hope without further assumptions on the target function. This is also better than the best known result for the same problem for transformers: \((^{d_{}{+}d_{}^{2}})\) in Petrov et al. 2024.

### Approximating functions over token sequences in \(^{}\)

Sec. 4.1 focused on continuous functions but recurrent architectures are often used to model natural language whose domain is tokens. Thus, we also look at modelling maps over a discrete domain. Any function from \(n\) tokens to \(n\) tokens taking values in \(=\{1,,V\}\) can be represented as a dictionary whose keys and values are in \(^{n}\). Therefore, a simple way to represent this function in-context is to first provide the \(n\) tokens corresponding to the query and then a sequence of \(2n\) tokens corresponding to key and value pairs (see Fig. 3 for an illustration of the setup). The model stores the

Figure 3: **Intuition behind the LSRL program for universal in-context approximation for discrete functions in Lst. 2. Our keys and values have length \(n{=}3\) and represent countries and capitals, e.g., \(\), \(\), and so on. The query is \(\) for Canada and the final \(n\) outputs are \(\) (Ottawa). We show the values of some of the variables in Lst. 2 at each step, with the \(\) variables being marked with arrows. For cleaner presentation we are tokenizing letters as \(7\), \(1{}\), \(2{}\), etc. Vertical separators are for illustration purposes only.**query in a state and processes the key-value pairs one by one by comparing the key (the first \(n\) tokens) with the query. If they match, then the value (the next \(n\) tokens) is copied into a state that keeps it and repeatedly outputs it. This continues until the end of the prompt, at which point the last \(n\) outputted tokens will be the value corresponding to the key matching the query. This is essentially a dictionary lookup. However, as shown in Lst. 2, implementing dictionary lookup in a linear recurrent model is much less straightforward than executing dict[key] in a general-purpose programming language.

Lst. 2 can appear daunting at first so we would like to clarify the non-trivial aspects. First, we need to count how far we are into every set of \(n\) or \(2n\) tokens. This can be done with \(\,n\) and \(\,2n\) operations but implementing modulo for arbitrary large inputs is not possible with ReLU MLPs (Ziyin et al., 2020). Therefore, we implement this with LinState as f_modulo_counter which has a unit-length state that is rotated \(}{{n}}\) or \(}{{2n}}\) revolutions per iteration, with the angle corresponding to the modulo value (App. F.7). Second, we need to do dynamic indexing to copy the \(i\)-th input in a subsequence to the \(i\)-th element of a state and vice-versa. Dynamic indexing, however, cannot be succinctly represented in a Linear RNN. We work around this with temporary variables that are non-zero only at the \(i\)-th coordinates (see Lines 16, 17, 19, 20, 32 to 35, 37 and 38). Finally, in order to compare whether all \(n\) elements in the query and the key match, we need to remember whether the previous \(n\) pairs were matching. As RNNs do not have attention, we implement this short-term memory buffer as a LinState with a shift matrix (Line 23).

## 5 Stable Universal In-Context Approximation with Gated Linear RNNs

The ReLU-based conditional operator is not numerically stable.The LSRL programs in Lsts. 1 and 2 for approximating functions in respectively \(^{}\) and \(^{}\) rely on the f_ifelse conditional assignment operator in Eq. (7) in order to implement different behaviours depending on whether we are processing the query or specific parts of the prompt. This operator is not numerically stable. The first term in Eq. (7) relies on \(()\) being exactly zero if the condition is not met. In this way, multiplying it with \(-\) would be 0 and \(()\) would be returned. However, if \(()\) is not identically 0 but has a small positive value, then \(-()\) can "overpower" \(()\) resulting in the ReLU output being 0. In our experience, this is not a problem when processing inputs through the LSRL program step-by-step. However, de-branching the DAG into a path graph --which is necessary in order to uncover the equivalent Linear RNN-- appears to introduce such numerical instabilities which occasionally result in wrong outputs as conditional assignments will be 0 when they should not. This problem is more prominent in Lst. 2 which is longer (more debranching steps) and has more f_ifelse operations: it gets most tokens wrong because of that instability (see _Original, No noise_ in Fig. 4). To this end, we support LSRL with a symbolic backend (based on SymPy) that performs the debranching steps exactly. Using it, both programs always produce the correct output.

This numerical instability highlights a critical practical limitations of the universal approximation results in Sec. 4: if the models are not numerically stable, it is unlikely that they occur in practice by training models using gradient descent. This section shows how to improve the numerical stability of Eq. (7) and obtain more realistic recurrent models that are universal approximators in-context.

Removing unnecessary terms in Eq. (7).Eq. (7) has 4 separate ReLU terms. The first two handle the cases when \(()\) and \(()\) are positive and the second two when they are negative. Therefore, if we know that one or both of these will always be non-negative, we can drop the corresponding terms. Additionally, if \(()\) is always \(0\), then the first and third terms can be safely dropped. Similarly, the second and fourth are unnecessary if \(() 0\). All f_ifelse in Lsts. 1 and 2 fall in this case and hence can be simplified. We will refer to this f_ifelse implementation that is aware of the attainable values of \(()\) and \(()\) as optimized. As it reduces the number of numerically unstable ReLU operations in the model, we expect that it will improve the stability of the compiled models. We experimented with adding various levels of noise to the non-zero model parameters, and, as the results in Fig. 4 show, optimized is indeed more numerically robust than original.

Step-based implementation.We can get rid of the input sensitivity of Eq. (7) using f_step:

\[[,, ,]()&:=(- (}{{2}}\!-\!())\!+ \!())+(-( ()-}{{2}})\!+\!())\\ &-(-(}{{2}}\!-\!())\!-\!())-(- (()-}{{2}} )\!-\!())).\] (9)

We can also apply the optimisation strategy here. While this implementation is robust to noise in the input it appears to be more sensitive to parameter noise, as shown in Fig. 4.

Figure 4: **Robustness of the various f_ifelse implementations to model parameter noise.** We show how the performance of the two universal approximation programs in Lsts. 1 and 2 deteriorates as we add Gaussian noise of various magnitudes to the non-zero weights of the resulting compiled models. As expected, the original f_ifelse implementation in Eq. (7) exhibits numerical precision errors at the lowest noise magnitude. For the token sequence case, numerical precision errors are present in all samples even in the no-noise setting. Hence, the original f_ifelse implementation is less numerically robust while the implementations with multiplicative gating are the most robust. For Lst. 1 (approximating \(^{}\)) we report the Euclidean distance between the target function value and the estimated one over 10 queries for 25 target functions. For Lst. 2 we report the percentage of wrong token predictions over 5 queries for 25 dictionary maps. Lower values are better in both cases.

Numerically stable f_ifelse with multiplicative gates.Removing the unused ReLU terms in the original f_ifelse reduces the opportunities for numerical precision issues to creep in but does not solve the underlying problem. The multiplicative gating present in the Linear Gated RNN (Eq.4) and Gated RNN models (Eq.5) can help via implementing a numerically stable conditional operator:

\[[,,]():=( )()+(()) (),\] (10)

where the element-wise product is implemented in LSRL with Concat and Multi. We will refer to the implementation of f_ifelse in Eq.10 as multiplicative. Similarly to original implementation of f_ifelse in Eq.7, we can drop the \(()\) and \(()\) term if they are equal to zero (multiplicative optimized). If \(()\) is not exactly zero, \(()()\) will result in a small error to the output but, in contrast to the original implementation, is not going to cause a discontinuity in the output of the operation. Therefore, Eq.10 should be more robust to numerical precision issues than Eq.7. Fig.4 shows that this is the case in practice with Lsts.1 and 2 being more robust to parameter noise when using multiplicative gates compared to the ReLU-based implementations. Therefore, Linear Gated RNNs (Eq.4) --to which models with multiplicative gates can be compiled-- are more likely than Linear RNNs (Eq.3) to exhibit universal approximation properties in practice.

## 6 Universal In-context Approximation with Non-linear (Gated) RNNs

Secs.4 and 5 showed how universal approximation of continuous and token-to-token functions can be implemented in LSRL and compiled to respectively Linear RNNs and Linear Gated RNNs. This section aims to address the situation with _non-linear_ state updates, that is, the cases of classic and gated RNNs (Eqs.2 and 5). Concretely, we show how every _linear_ (Gated) RNN can be converted to a _non-linear_ (Gated) RNN. The key idea is that the ReLU applied to the state updates in the non-linear architectures is an identity operation if its inputs are positive. Hence, we can split the states in positive and negative components, flip the sign of the negative component, pass them separately through the ReLU--which will act as an identity as all elements will be non-negative-- and then fuse the positive and negative components back together in the \(\) matrix at the next time step:

\[_{t}=_{t-1}+_{t}+\\ _{t}=(_{t}). _{t}^{+}\\ _{t}^{-}=(&- \\ -&_{t}^{+}\\ _{t}^{-}+\\ -_{t}+\\ -)\\ _{t}=(&- _{t}^{+}\\ _{t}^{-}).\] (11)

Using Eq.11 we can compile any LSRL program to an RNN (Eq.2) or a Gated RNN (Eq.5). This includes Lsts.1 and 2. Hence, RNNs and Gated RNNs can be universal in-context approximators for continuous and token-to-token functions. As any Gated RNN can be represented as a GRU model (App. C) or an LSTM (App. D), these models are too universal in-context approximators.

## 7 Discussion and Conclusions

We developed LSRL: a programming language for specifying programs expressible with recurrent neural architectures. We then used LSRL to show that various architectures --from the humble RNN to the state-of-the-art Linear Gated RNNs-- can all be universal approximators _in-context_.

Safety and security implications.If a model can be prompted to approximate any function, then preventing it from exhibiting undesirable behaviours (i.e., alignment) might be impossible. Therefore, it is important to further study the safety and security implications of these properties.

Limitations.In this work we provide _constructive existence results_: that is, we show that there can exist models with various recurrent architectures that are universal in-context approximators. However, the present theory is not sufficient to analyse whether _a given model_ has this property. That is a much more difficult question that would require a very different approach. We also assume no restrictions on the \(\) matrix in the state update equations. However, many state-of-the-art models impose structural constraints on \(\) (e.g., it being diagonal) for the sake of fast training and inference (Gu et al., 2020, 2021; Gupta et al., 2022). It is not directly obvious whether such structural restrictions would affect the universal in-context approximation abilities of these architectures. In practice, however, the compiled matrices are very sparse and often diagonal. Therefore, it is highly likely that our results translate to models with structural restrictions.