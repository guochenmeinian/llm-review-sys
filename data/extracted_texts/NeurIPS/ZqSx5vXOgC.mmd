# Bypass Exponential Time Preprocessing: Fast Neural Network Training via Weight-Data Correlation Preprocessing

Bypass Exponential Time Preprocessing: Fast Neural Network Training via Weight-Data Correlation Preprocessing

Josh Alman

josh@cs.columbia.edu. Columbia University.

Jiehao Liang

jiehao.liang@berkeley.edu. University of California, Berkeley.

Zhao Song

zsong@adobe.com. Adobe Research.

Ruizhe Zhang

ruizhe@utexas.edu. Simons Institute for the Theory of Computing.

Danyang Zhuo

danyang@cs.duke.edu. Duke University.

###### Abstract

Over the last decade, deep neural networks have transformed our society, and they are already widely applied in various machine learning applications. State-of-the-art deep neural networks are becoming larger in size every year to deliver increasing model accuracy, and as a result, model training consumes substantial computing resources and will only consume more in the future. Using current training methods, in each iteration, to process a data point \(x^{d}\) in a layer, we need to spend \((md)\) time to evaluate all the \(m\) neurons in the layer. This means processing the entire layer takes \((nmd)\) time for \(n\) data points. Recent work  reduces this time per iteration to \(o(nmd)\) but requires exponential time to preprocess either the data or the neural network weights, making it unlikely to have practical usage.

In this work, we present a new preprocessing method that simply stores the weight-data correlation in a tree data structure in order to quickly, and dynamically detect which neurons fire at each iteration. Our method requires only \(O(nmd)\) time in preprocessing and still achieves \(o(nmd)\) time per iteration. We complement our new algorithm with a lower bound, proving that assuming a popular conjecture from complexity theory, one could not substantially speed up our algorithm for dynamic detection of firing neurons.

## 1 Introduction

Machine learning applications are requiring larger and larger neural network size, and the computing resources required to train these large models is growing correspondingly. Determining how to train these models quickly has become an important research challenge.

Training a neural network is an iterative algorithm, and in each iteration, we need to process each of the \(m\) neurons on each of the \(n\) data points. Assuming each data point has a length of \(d\) (e.g., \(d\) could be the size of an input image), this means the per-iteration training time of the straightforward algorithm is at least \((nmd)\) just to compute the activations. As we train larger neural networks on more training data, this running time can become a significant obstacle.

Recent work by Song, Yang, and Zhang  gave the first training algorithm that reduces this per iteration training time to \(o(nmd)\). The high-level idea of their algorithm is to use a nearest neighbor search data structure that stores the neural network weights and training data. This allows the training method to have fast access to the inner products of the training data with the current weight of the iteration. However, their algorithm's initial preprocessing time to set up the data structure is _exponential_ in the dimension \(d\), making it too slow in most applications. This raises a natural _theoretical_ question:

_Is it possible to design an algorithm that spends polynomial time to preprocess the weights and data, and which achieves a training time of \(o(nmd)\) per iteration?_

This question is important for two reasons. First, speeding up neural network training is a fundamental research challenge with real-world value. Second, dynamic data structures have been successfully used to speed up computations in many contexts throughout computer science, yet their power and limitations when applied to the training of neural networks are currently poorly understood.

### Our Result: An Upper Bound

Our main result answers this question in the affirmative, giving a new algorithm with efficient preprocessing and faster training in the natural over-parameterization regime (which has \(m n\)):

**Theorem 1.1** (Main result).: _There is a data structure which preprocesses \(n\) data points in \(d\)-dimensional space, and \(m\) initialization weights points in \(d\)-dimensional space, in \(O(mnd)\) preprocessing time and \(O(mn+md+nd)\) space, which can be used to speed up neural network training: Running the gradient descent algorithm on a two-layer, \(m\)-width, over-parameterized ReLU neural network, which will minimize the training loss to zero, can be performed with an expected running time (of the gradient descent algorithm per iteration) of_

\[(m^{4/5}n^{2}d).\]

The following remark gives a comparison between our result and a closely related work :

**Remark 1.2**.: _The prior work  presented two algorithms. Their first result (see Theorem 6.1 and Part 1 of Corollary B.6 in ) has \(O(2^{d})\) preprocessing time and uses \(O(m^{1-1/d}nd)\) cost per iteration. Their second result (see Theorem 6.2 and Part 2 of Corollary B.6 of ) has \(O(n^{d})\) preprocessing time and uses \(O(m^{4/5}nd)\) time per iteration. Our result exponentially improves the running time of the data structure in  in terms of the dimension \(d\). Notably, unlike , we do not use any complicated geometric data structure in previous work, and our algorithms are much easier to implement (see Algorithms 1 and 2). Moreover, as we discussed in Section 5, they can be parallelized to further reduce the cost-per-iteration to \((m^{4/5}nd)\)._

Our key observation is that in each iteration of the training process, the weight updates are mostly sparse, and only a small fraction of neurons are activated for each training data point. Given this observation, we construct a binary search tree for each training data point (or neuron) to detect which neurons will fire. Our data structure and the corresponding algorithms are _deterministic_, not relying on any randomness, and solve the following dynamic algorithms problem which we prove appears as a key subroutine of the training process.

**Definition 1.3** (Dynamic Detection of Firing Neurons (DDFN)).: _Given two set of points \(X=\{x_{1},,x_{n}\}^{d}\), \(Y=\{y_{1},,y_{m}\}^{d}\) and a threshold \(b\), design a data structure to support the following operations:_

* \((j[m],z^{d})\)_, set_ \(y_{j}\) _to_ \(z\)__
* \(()\)_, either output the set_ \[Q=\{(i,j)[n][m] x_{i},y_{j} b\},\] _or report that_ \(|Q|>m^{4/5}n\)_._

We give a data structure for DDFN which takes \(O(mnd)\)-time for preprocessing, \((nd)\)-time per update, and \(O(\{|Q|,m^{4/5}n\})\)-time per query. At a high level, our data structure works as follows.

PreprocessingWe build \(n\) binary search trees to maintain the \((x_{i},y_{j})\) pairs for \(i[n]\) and \(j[m]\). More specifically, the \(i\)-th tree has \(m\) leaf nodes, storing the inner-products between \(x_{i}\) and \(\{y_{j}\}_{j[m]}\). Each internal node stores the larger value of its two child nodes. The preprocessing time for the binary search trees for all the input data and neurons takes \(O(nmd)\) time and \(O(mn)\) space.

[MISSING_PAGE_FAIL:3]

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

The following lemma upper bounds the sparsity after initialization.

**Lemma 2.5** (Sparsity after initialization, informal version of Lemma B.3, ).: _Let \(b>0\) be a tunable parameter. If we setup the neural network as in Definition 2.1, then after the randomized initialization, with probability at least \(1-(-(m(-b^{2}/2)))\), it holds that for any input data \(x\), the number of activated neurons is at most \(O(m(-b^{2}/2))\), where \(m\) is the total number of neurons._

**Remark 2.6**.: _This suggests that if we take \(b=\), we achieve a sublinear number, \(O(m^{4/5})\), of activated neurons._

We can similarly control the sparsity in each iteration, and not just the first iteration; we defer the details to Section B.2.

In the next section, we will show how our weight-tree correlation data structure can take advantage of this sparsity phenomenon.

## 3 Correlation Tree Data Structure

In this section, we consider a neural network \(2(m,b)\) (Definition 2.1) with \(n\) data points. We let \(\{w_{1},,w_{m}\}^{d}\) be the weights, \(\{x_{1},,x_{n}\}^{d}\) be the data points, and \(\{(w_{r},x_{i})\}_{r[m],i[n]}^{m+n}\) be the weight-data pairs.

We propose two data structures: Correlation DTree and Correlation WTree. The DTree data structure has \(n\) trees, and its \(i\)-th tree has \(m\) leaf nodes corresponding to the set of inner-products between \(x_{i}\) and all hidden neurons, i.e., \(\{ w_{r},x_{i}\}_{r[m]}\). Similarly, the WTree data structure consists of \(m\) trees, and its \(r\)-th tree has \(n\) leaf nodes corresponding to the set of inner-products between the \(r\)-th neuron and all data points, i.e., \(\{ w_{r},x_{i}\}_{i[n]}\).

The Correlation Tree is a simple binary tree data structure. At a high level, it works as follows:

* **Tree construction** We first calculate the inner products of all weight-data pairs \( w_{i},x_{j}\), each representing the evaluation of a neuron at a data point. To search activated neurons efficiently, we create a tree structure in the following way (taking the Correlation DTree as an example): we first build \(m\) leaf nodes, where the \(r\)-th leaf stores \( w_{r},x_{i}\) for \(r[m]\). Then, we recursively construct a binary tree such that a parent node takes the larger value from its two child nodes. Finally, we obtain a tree with root having value \(_{r[m]}\{ w_{r},x_{i}\}\). Moreover, the value of each internal node equals to the maximum value among the leaf nodes in this subtree.
* **Efficient search** Given a threshold \(b\), the data structure can find all the pairs of vectors whose inner product is greater than \(b\). Take the Correlation DTree as an example. It outputs the indices of those activated neurons (i.e., \( w_{r},x_{i}>b\)) in a recursive way: starting from the root, it checks whether it is "activated" (i.e., with value \(>b\)). If not, the search ends. Otherwise, it moves to each of the child nodes and repeats this searching process until stops. This is a typical depth-first search strategy. Its running time is determined by how many nodes it visits during searching. The number of visited nodes has the same magnitude as the number of visited leaf nodes, i.e., the number of activated neurons. Hence, the efficiency of our data structures relies on the sparsity phenomenon of the training process.
* **Relation between DTree and WTree** In the Correlation DTree, each weight vector \(w_{r}\) appears only in \(n\) different trees. In the Correlation WTree, each weight vector \(w_{r}\) appears only in one of the \(m\) trees. When \(w_{r}\) is updated, DTree will change the nodes along a root-to-leaf path in \(n\) trees, whereas WTree only changes such paths in the \(r\)-th tree.

### Correlation DTree data structure

We now state our main theorem summarizing the correlation DTree data structure. Its pseudocode is given in Algorithms 1 and 2 below. Its proof are deferred to Section D.1.

**Theorem 3.1** (Correlation DTree data structure).: _There exists a data structure with the following procedures:_* \((\{w_{1},w_{2},,w_{m}\}^{d},\{x_{1},x_{2}, ,x_{n}\}^{d},n,m,d)\). _Given a series of weights_ \(w_{1},w_{2},,w_{m}\) _and data_ \(x_{1},x_{2},,x_{n}\) _in d-dimensional space, it performs preprocessing in time_ \(O(nmd)\)_._
* \((z^{d},r[m])\)_. Given a weight_ \(z\) _and an index_ \(r\)_, it updates weight_ \(w_{r}\) _to_ \(z\) _in time_ \(O(n(d+ m))\)_._
* \((i[n],)\)_. Given an index_ \(i\) _indicating data point_ \(x_{i}\) _and a threshold_ \(\)_, it finds all indices_ \(r[m]\) _such that_ \( w_{r},x_{i}>\) _in time_ \(O(|()| m)\)_, where_ \[():=\{r: w_{r},x_{i}>\}.\]

```
1:data structureCorrelationDTree\(\) Theorem 3.1
2:members
3:\(W^{m d}\) (\(m\) weight vectors )
4:\(X^{n d}\) (\(n\) data points)
5: Binary tree \(T_{1},T_{2},,T_{n}\)\(\)\(n\) binary search trees
6:end members
7:procedureInit(\(w_{1},w_{2},,w_{m}^{d},m,x_{1},x_{2},,x_{n} ^{d}\), \(n\), \(m\), d) \(\) Lemma D.2
8:for\(i=1 n\)do
9:\(x_{i} x_{i}\)
10:endfor
11:for\(j=1 m\)do
12:\(w_{j} w_{j}\)
13:endfor
14:for\(i=1 n\)do\(\) for data point, we create a tree
15:for\(j=1 m\)do
16:\(u_{j} x_{i},w_{j}\)
17:endfor
18:\(T_{i}(u_{1},,u_{m})\)\(\) Each node stores the maximum value for his two children, Algorithm 7
19:endfor
20:endprocedure
21:procedureUpdate(\(z^{d},r[m]\))\(\) Lemma D.3
22:\(w_{r} z\)
23:for\(i=1 n\)do
24:\(l\) the \(l\)-th leaf of tree \(T_{i}\)
25:\(l.\)value = \( z,x_{i}\)
26:while\(l\) is not root do
27:\(p\) parent of \(l\)
28:\(p.\)value \(\{p.\)value, \(l.\)value\(\}\)
29:\(l p\)
30:endwhile
31:endfor
32:endprocedure
33:endata structure ```

**Algorithm 1** Correlation DTree data structure

### Correlation WTree data structure

We next state the main theorem summarizing our similar Correlation WTree data structure. Both the Correlation DTree and Correlation WTree have a query time that is roughly equal to the output size, but since they have different outputs, each can be faster than the other depending on the setting. The pseudocode and proof for Correlation WTree are deferred to Section D.3.

**Theorem 3.2** (Correlation WTree data structure).: _There exists a data structure with the following procedures:_* \((\{w_{1},w_{2},,w_{m}\}^{d},\{x_{1},x_{2}, ,x_{n}\}^{d},n,m,d)\). _Given a series of weights_ \(w_{1},w_{2},,w_{m}\) _and data_ \(x_{1},x_{2},,x_{n}\) _in d-dimensional space, it performs preprocessing in time_ \(O(nmd)\)_._
* \((z^{d},r[m])\)_. Given a weight_ \(z\) _and index_ \(r\)_, it updates weight_ \(w_{r}\) _to_ \(z\) _in time_ \(O(nd)\)_._
* \((r[m],)\)_. Given an index_ \(r\) _indicating weight_ \(w_{r}\) _and a threshold_ \(\)_, it finds all indices_ \(i[n]\) _such that_ \( w_{r},x_{i}>\) _in time_ \(O(|S()| m)\)_, where_ \(S():=\{i: w_{r},x_{i}>\}\)_._

## 4 Running Time of Our Algorithm

In this section, we show how to apply the Correlation Tree data structures developed in Section 3 to speed up neural network training.

### Weights Preprocessing

```
1:procedureTrainingWithDTree(\(\{(x_{i},y_{i})\}_{i[n]}\),\(n\),\(m\),\(d\))\(\)Theorem 4.1
2: Initialize \(w_{r},a_{r}\) for \(r[m]\) and \(b\) according to Section 2
3:\((\{w_{r}(0)\}_{r[m]},m,d)\)\(\)Algorithm 10
4:for\(t=1 T\)do
5:\(S_{i,}(x_{i},b)\) for \(i[n]\)
6: Forward pass for \(x_{i}\) only on neurons in \(S_{i,}\) for \(i[n]\)
7: Calculate gradient for \(x_{i}\) only on neurons in \(S_{i,}\) for \(i[n]\)
8: Gradient update for the neurons in \(_{i[n]}S_{i,}\)
9:\((w_{r}(t+1),r)\)
10:endfor
11:return Trained weights \(w_{r}(T+1)\) for \(r[m]\)
12:endprocedure ```

**Algorithm 3** Training Neural Network based on Correlation DTree

In Algorithm 8, we use DTree structure to speed up the training process. We preprocess weights \(w_{r},r[m]\) for each data point \(x_{i},i[n]\) by constructing \(n\) weight-data correlation trees. In each iteration, Query finds the set of activated neurons \(S_{i,}\) (Definition 2.4) efficiently for each data point \(x_{i}\) and Update helps change the weights in backward propagation.

Our main result for weight preprocessing is as follows.

**Theorem 4.1** (Running time part, informal version of Theorem E.1).: _Given \(n\) data points in \(^{d}\), gradient descent using the DTree data structure (Algorithm 8) for the neural network \(2(m,b=)\) (Definition 2.1) takes \(O(m^{4/5}n^{2}d)\) time per iteration in expectation._

### Data Preprocessing

```
1:procedureTrainingWithWTree(\(\{(x_{i},y_{i})\}_{i[n]}\),\(n\),\(m\),\(d\))\(\) Theorem 4.2
2: Initialize \(w_{r},a_{r}\) for \(r[m]\) and \(b\) according to Section 2
3:\((\{x_{i}\}_{i[n]},n,d)\)\(\) Algorithm 13
4:\(_{r,}(w_{r}(0),b)\) for \(r[m]\)\(\) Data points fire set
5:\(S_{i,}\{r i_{r,}\}\)\(\) Hidden neurons fire set
6:for\(t=1 T\)do
7: Forward pass for \(x_{i}\) only on neurons in \(S_{i,}\) for \(i[n]\)
8: Calculate gradient for \(x_{i}\) only on neurons in \(S_{i,}\) for \(i[n]\)
9:for\(r_{i[n]}_{i,}\)do
10:\(_{r,}(w_{r}(t+1),b)\)
11: Update \(S_{i,}\) for each \(i_{r,}\)
12:endfor
13:endfor
14:return Trained weights \(w_{r}(T+1)\) for \(r[m]\)
15:endprocedure ```

**Algorithm 4** Training Neural Network based on Correlation WTree

Preprocessing weights based on data points is a common practice for neural networks. Here we consider its dual form: preprocessing input data \(x_{i},i[n]\) based on neural network weights \(w_{r},r[m]\). This can be easily done due to the symmetric property of the inner product that we used in the correlation tree structure.

Given a weight vector \(w_{r}\), we can quickly find \(_{i,}\) (Definition 2.4) which contains the indices of data points that "fire" for weight \(w_{r}\). By the dual relationship between \(_{i,}\) and \(S_{i,}\), we can recover \(S_{i,}\) easily.

One advantage of the data preprocessing approach is that the data structure only depends on the training dataset, instead of the neural network architecture. Therefore, the data structure could be pre-computed and stored in cloud platforms.

The performance guarantee of our data preprocessing training algorithm is shown as follows:

**Theorem 4.2** (Running time part, informal version of Theorem E.2).: _Given \(n\) data points in \(^{d}\), gradient descent algorithm using the WTree data structure (Algorithm 9) for the neural network \(2(m,b=)\) takes \(O(m^{4/5}n n)\)-time per iteration to initialize \(_{r,},S_{i,}\) for \(r[m],i[n]\), and the total running time per iteration is_

\[O(m^{4/5}n^{2}d)\]

_in expectation._

## 5 Conclusion

Deep neural networks are becoming larger every year to offer improved model accuracy. Training these models consumes substantial resources, and resource consumption will only increase as these models grow. In traditional training methods, for each iteration, we need to spend \((nmd)\) time to evaluate the \(m\) neurons on \(n\) data points with dimension \(d\). Recent work  reduced the per-iteration cost to \(o(nmd)\), but required exponential time to preprocess either the data or the neural weights. We develop a new method that reduces the preprocessing cost to \(O(nmd)\) while keeping the per-iteration running time at \(o(nmd)\). One limitation of our algorithm is that it has an \(n^{2}\) dependence in the cost-per-iteration. However, for very wide neural networks (with \(m n\)), the runtime is still sublinear. More importantly, we design a simple binary tree-based dynamic geometric data structure that can efficiently identify all the activated neurons in each training iteration and bypass the high-dimensional barrier of the prior approach. We further remark that the Update procedure of DTree/Wtree structure (Algorithm 1 and 14) can be parallelized, where we can update all the correlation trees using distributed computing simultaneously. It will improve the running time from \(O(nd)\) to \(O(d)\), resulting in a total running time \(O(m^{4/5}nd)\) per iteration.

Our work naturally raises some open questions for future study:

* First, can we apply our data structure, together with an analysis of the sparsity in training over-parameterized neural networks , to speed up neural network training with more than two layers? Giving a provable, theoretical backing for quickly training multi-layer networks remains an open, difficult challenge.
* Second, many empirical results (e.g., ) indicate that only _approximately_ identifying the activated neurons (i.e., neurons with top-\(k\) inner products) in each iteration may still be enough to train a neural network. Can we provide a more theoretical understanding of these approaches?
* Third, our current algorithms use more memory (i.e., \(O(mn)\) space) to store the correlation tree data structure. Is it possible to reduce the space complexity of the algorithms?
* Fourth, we think it is possible that our data structures will work for more general activation functions. Roughly speaking, as long as the activated neurons are sparse or approximately sparse, our data structures will be able to theoretically reduce the cost-per-iteration. However, we need to re-prove the sparsification results in  for the activation function other than ReLU.

AcknowledgementsThe authors would like to thank Lichen Zhang for his helpful discussions. JA was partly supported by a grant from the Simons Foundation (Grant Number 825870 JA). Part of this research was performed while RZ was visiting the Institute for Pure and Applied Mathematics (IPAM), which is supported by the National Science Foundation (Grant No. DMS-1925919).