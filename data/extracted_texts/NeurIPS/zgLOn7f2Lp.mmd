# Graph Classification Gaussian Processes

via Hodgelet Spectral Features

 Mathieu Alain\({}^{1,5,}\) So Takao\({}^{2}\) Bastian Rieck\({}^{3}\) Xiaowen Dong\({}^{4}\) Emmanuel Noutahi\({}^{5}\)

\({}^{1}\)University College London \({}^{2}\)California Institute of Technology \({}^{3}\)University of Fribourg

\({}^{4}\)University of Oxford \({}^{5}\)Valence Labs

\({}^{}\)mathieu.alain.21@ucl.ac.uk

###### Abstract

The problem of classifying graphs is ubiquitous in machine learning. While it is standard to apply graph neural networks or graph kernel methods, Gaussian processes can be employed by transforming spatial features from the graph domain into spectral features in the Euclidean domain, and using them as the input points of classical kernels. However, this approach currently only takes into account features on vertices, whereas some graph datasets also support features on edges. In this work, we present a Gaussian process-based classification algorithm that can leverage one or both vertex and edges features. Furthermore, we take advantage of the Hodge decomposition to better capture the intricate richness of vertex and edge features, which can be beneficial on diverse tasks.

## 1 Introduction

Classification is omnipresent in machine learning, yet typically assumes data to be Euclidean. Extending this task to non-Euclidean domains, such as graphs, presents challenges due to their irregularity, varying sizes, and multi-site information (e.g. vertices and edges). However, classifying graphs is of critical importance in scientific and industrial applications, being used for instance, to predict properties of molecules, or discovering new drugs . Although graph neural networks  are usually the model of choice for such applications, a downside is that they may require large datasets for effective training. Gaussian processes (GP) , on the other hand, prove to be a data-efficient and interpretable modelling choice. They do not need separate validation datasets to tune hyperparameters and provide robust uncertainty estimates for predictions. This makes them ideal for small-data scenarios and high-risk decision-making tasks that require reliable uncertainty estimates.

In a recent work, Opolka et al.  introduced a GP-based algorithm capable of classifying graphs. Their method relies on tools developed from the graph signal processing literature , including the spectral graph Fourier transform  and the spectral graph wavelet transform . Specifically, spectral graph methods use spatial graph features to compute graph spectral coefficients, which may be leveraged to generate spectral features in the Euclidean domain. Such spectral representations can be passed as input points to a standard GP , subsequently employed for classification via approximate inference . Closely related are techniques developed in the early graph neural network literature, for instance, Spectral Networks  and ChebNet , which lean on the graph Fourier transform and graph wavelet transform to establish a notion of convolution on graphs. While the approach proposed in Opolka et al.  accommodates features living on vertices, it cannot easily take into account features on edges. However, edge information can often be as valuable as vertex information, representing crucial quantities such as flows  and chemical bonds .

Our paper aims to fill this gap by proposing a novel GP-based classification algorithm that naturally incorporate features on vertices, edges, and more generally, simplices, building on recent work defining GPs on simplicial complexes  and cellular complexes . Moreover, we utilise thecelebrated Hodge decomposition on spatial graph features, separating and processing them into three canonical components, each exhibiting distinct properties. This enables the modelling of these components separately, using different kernels. This idea have been used successfully in past works [17; 18; 19; 20; 15], providing greater flexibility. We are not aware that these recent techniques have been applied in the context of graph classification. We demonstrate empirically that these extensions achieve similar or better performance than the method introduced by Opolka et al. . Although we focus on graphs, our approach can be extended easily to higher-order networks, such as simplicial complexes, cellular complexes, and hypergraphs, which can describe polyadic interactions [21; 22; 23; 24], thereby generalising the dyadic interactions found in graphs (see Appendix D). We highlight that our method can be readily adapted for regression by selecting an appropriate likelihood. Finally, although our Hodgelet spectral features are employed in the context of Gaussian processes, they can be readily integrated into other machine learning methods.

## 2 Gaussian Processes for Graph Classification

We assume a dataset containing \(M\) undirected graphs \(^{(1)},,^{(M)}\) and labels \(y^{(i)}\), for \(1 i M\). We assign an orientation to each graph but emphasise that this choice is arbitrary. Let \(^{(i)}\) be the vertex set and \(^{(i)}\) be the edge set, both finite. For each graph \(^{(i)}=(^{(i)},^{(i)})\), there are \(N_{v}^{(i)}\) vertices, \(N_{e}^{(i)}\) edges, and an incidence matrix \(_{ve}^{(i)}^{N_{v}^{(i)} N_{e}^{(i)}}\). The latter encodes the incidence between each vertex and edge, and furthermore defines the _graph Laplacian_\(_{v}^{(i)}_{ve}^{(i)}_{ve}^{(i)}^{N _{v}^{(i)} N_{v}^{(i)}}\). By considering \(3\)-cliques, we obtain the edge-to-triangle incidence matrix \(_{ct}^{(i)}^{N_{e}^{(i)} N_{e}^{(i)}}\), where \(N_{t}^{(i)}\) is the number of triangles in \(^{(i)}\). Likewise, we define the _graph Helmholtzian_\(_{e}^{(i)}_{ve}^{(i)}_{ve}^{(i)}+_{ct}^{(i) }_{ct}^{(i)}^{N_{e}^{(i)} N_{e}^{(i)}}\), which applies to edges rather than vertices. We underline that \(_{v}^{(i)}\) and \(_{e}^{(i)}\) are instances of the _discrete Hodge Laplacian_ (see Appendix D.2). Let \(^{(i)}^{D_{v}}\) and \(^{(i)}^{D_{e}}\) be two functions, for \(D_{v},D_{e}\). By introducing an ordering on vertices and edges, we represent the preceding functions as matrices \(^{D_{v} N_{v}^{(i)}}\) and \(^{D_{e} N_{v}^{(i)}}\), respectively, which are understood as vertex and edge feature matrices, containing \(D_{v}\) and \(D_{e}\)_channels_, respectively. We denote vertex features for channel \(1 d D_{v}\) by the column vector \(_{vd}^{(i)}^{N_{v}^{(i)}}\) and edge features for channel \(1 d D_{e}\) by the column vector \(_{ed}^{(i)}^{N_{e}^{(i)}}\). We stress that our approach can adapt to graphs that may have vertex features, edge features, or both.

### Wavelet transforms on graphs

The key idea behind our graph classification algorithm is to convert vertex and edge features from the graph domain into _spectral features_ in the Euclidean domain, enabling standard GP classification. We first consider the eigendecomposition of the graph Laplacian and graph Helmholtzian

\[_{v}^{(i)}=_{v}^{(i)}_{v}^{(i)}_{v}^{(i) },_{e}^{(i)}=_{e}^{(i)}_{e}^{(i)}_{e }^{(i)},\] (1)

and define _graph Fourier coefficients_ as projections of spatial graph features onto the eigenbases,

\[}_{vd}^{(i)}_{v}^{(i)}_{vd}^{(i)} ^{N_{v}^{(i)}},}_{ed}^{(i)}_{e}^{(i) }_{ed}^{(i)}^{N_{e}^{(i)}}.\] (2)

We observe that \(}_{ d}^{(i)}\) reside in the eigenspace of \(_{}^{(i)}\). Furthermore, they are _invariant_ to vertex and edge ordering, making them sound choices for constructing spectral features, and they are perfectly localised in frequency, capturing _global_ properties of the original features. However, it is often beneficial to also possess spatially localised information, focusing on _local_ properties. A solution is to compute the more flexible _graph wavelet coefficients_ (see Appendix B) by modulating Fourier coefficients using a _wavelet filter_ on the eigenvalues \(_{v}^{(i)}\) and \(_{e}^{(i)}\), and then perform the inverse _Fourier transform_. A wavelet filter is a combination of a _scaling function_ at a single scale and a _wavelet function_ at multiple scales, offering _multi-scale resolution_. Wavelet functions, \(b:\) and \(d:\), operate as _band-pass filters_, and scaling functions, \(a:\) and \(c:\), are _low-pass filters_. A wavelet filter captures one perspective of a graph, but to obtain a comprehensive picture, it is essential to employ multiple wavelet filters. Wavelet filters \(j\) are defined by

\[w_{vj}()  a(_{j})+_{l=1}^{L_{v}}b(_{jl}), _{vj}\{_{j},_{j1},,_{jL_{v}}\}, 1  j W_{v},\] (3) \[w_{ej}()  c(_{j})+_{l=1}^{L_{v}}d(_{jl} ),_{ej}=\{_{j},_{j1},,_{jL_{e}}\},  1 j W_{e},\] (4)

where \(L_{}\) is the number of scales, \(W_{}\) is the number of wavelet filters, and \(_{ j}\) is a collection of trainable parameters controlling the scaling. Wavelet coefficients are given by

\[}^{(i)}_{vdj}^{(i)}_{v}w_{vj}^{( i)}_{v}^{(i)}_{v}^{(i)}_{vd}^{N^{(i)}_{}}, }^{(i)}_{edj}^{(i)}_{e}w_{ej}^{(i)}_{e}^{(i)}_{e}^{(i)}_{ed}^{N^ {(i)}_{}}.\] (5)

### Hodge decomposition

The _discrete Hodge decomposition_ (see Appendix D.3) states that _spatial graph feature spaces_, i.e. the spaces inhabited by \(^{(i)}_{vd}\) and \(^{(i)}_{ed}\), can each be separated into an orthogonal sum of three subspaces, _exact_, _co-exact_, and _harmonic_, collectively referred to as the _Hodge subspaces_. From this, the eigenbases in (1) can be divided into sub-eigenbases, each spanning a different Hodge subspace,

\[^{(i)}_{v}=^{(i)}_{vc}\ ^{(i)}_{vh}, ^{(i)}_{e}=^{(i)}_{ee}\ ^{(i)}_{ec}\ ^{(i)}_{eh}.\] (6)

We observe that \(^{(i)}_{v}\) has only two components. The _co-exact sub-eigenbasis_\(^{(i)}_{vc}\) amounts to the non-zero eigenvectors of \(^{(i)}_{v}\), and the _harmonic sub-eigenbasis_\(^{(i)}_{vh}\) to the zero ones. The _exact_ and co-exact _sub-eigenbases_\(^{(i)}_{ee}\) and \(^{(i)}_{ec}\) are the non-zero eigenvectors of \(^{(i)}_{ve}^{(i)}_{ve}\) and \(^{(i)}_{et}^{(i)}_{et}\), respectively. Finally, the harmonic sub-eigenbasis \(^{(i)}_{eh}\) comprises the zero eigenvectors of \(^{(i)}_{e}\). For edges, the exact and co-exact components are sometimes termed _gradient_ and _curl_ components, respectively, reminiscent of vector fields. A gradient part is _curl-free_, indicating no _vortices_. A curl part is _divergence-free_, meaning no _sources_ or _sinks_. A harmonic part is curl-free and divergence-free.

### Hodgelet spectral features

We generate graph spectral features from graph wavelet coefficients and then use them in downstream GP classification tasks (see Appendix A). By combining the wavelet transform (5) and the Hodge decomposition (6), we compute the wavelet coefficients \(}^{(i)}_{vdcj},}^{(i)}_{vdjh}\) and \(}^{(i)}_{edje},}^{(i)}_{edjc},}^{(i)}_{edjh}\) (see Appendix C for more details). We derive our _Hodgelet spectral features_ by concatenating the 2-norm of the preceding wavelet coefficients across each wavelet filter and channel, resulting in the column vectors \(^{(i)}_{c},^{(i)}_{h}^{W_{v}D_{v}}\) and \(^{(i)}_{e},^{(i)}_{c},^{(i)}_{h}^{W_{v}D_{e}}\). These spectral representations, which are invariant to graph isomorphism, are then fed to our additive _Hodgelet kernel_

\[^{(i)},^{(j)} &_{vc}^{(i)}_{c},^{(j)}_ {c}+_{vh}^{(i)}_{h},^{(j)}_{h}\\ &+_{ee}^{(i)}_{e},^{(j)}_{c}+ _{ec}^{(i)}_{c},^{(j)}_{c}+_{eh} ^{(i)}_{h},^{(j)}_{h},\] (7)

where \(_{}\) is a standard kernel function, such as the _squared exponential kernel function_. We note that parameters \(_{ j}\) are optimised jointly with the kernel hyperparameters and that a separate kernel for each part of the Hodge decomposition offers greater flexibility. We highlight that our GP-based classification algorithm supports multi-dimensional spatial graph features and graphs of varying sizes, in contrast to typical graph kernel-based methods . The GP component scales according to the number of graphs, while the eigendecompositions are a one-off cost that can be performed in advance.

## 3 Experiments

The aim of the experiments is two-fold: (1) we validate the added flexibility given by the Hodge decomposition, and (2) we demonstrate that when edge features are present, it is better to work with edges directly rather than converting graphs to line-graphs. The latter point has been observed in previous works [22; 15] and our experiments, across 10 seeds, further validate their conclusions.

### Graph classification benchmarks

Our first experiment compares the method in Opolka et al. , which we refer to the wavelet-transform GP (WT-GP), which does not use the Hodge decomposition, to our method, WT-GP-Hodge, which employs the decomposition. In Table 1, we display the results on some standard graph classification benchmark datasets used in Opolka et al. . We observe that on all but one dataset, WT-GP-Hodge improves the classification accuracy. This may be surprising as the Hodge decomposition for vertex features yields only co-exact and harmonic parts, where the harmonic part is constant across the connected components of the graph. This suggests that we can gain accuracy by separating vertex features into a constant bias (harmonic part) and fluctuations around it (co-exact part), using different kernels for each. However, if constant biases do not aid classification (e.g. when classes have vertex feature of similar magnitudes), then we do not expect WT-GP-Hodge to improve over WT-GP.

### Vector field classification

Our second experiment consider the task of classifying noisy vector fields, i.e. predicting whether they are predominantly divergence-free or curl-free. We proceed by generating 100 random vector fields, with half mostly divergence-free (Figure E.1c) and the other half mostly curl-free (Figure E.1d). The generated vector fields are then projected onto the edges of a randomly generated triangular mesh on a square domain with \(N\) vertices (Figure E.2). Finally, we corrupt the edge features with i.i.d. Gaussian noise, resulting in a dataset composed of \(100\) oriented graphs, each containing scalar edge features corresponding to the _net flow_ of the vector field along the edges. Again, we compare our method against the vanilla WT-GP classification method. However, since WT-GP does not take edge features, we first convert graphs to line-graphs before applying it. We refer to it as WT-GP-LG. In Table 2, we display the results of WT-GP-LG and WT-GP-Hodge for various choices of \(N\). We see that WT-GP-Hodge is consistently better, with large improvements as mesh resolution is increased. On the other hand, WT-GP-LG cannot distinguish accurately between divergence-free and curl-free fields, even as the mesh resolution becomes higher. Likely reasons: (1) the Hodge decomposition in WT-GP-Hodge helps to discriminate more clearly between divergence-free and curl-free components, and (2) there are properties that are canonical to edges, such as orientation, which WT-GP-Hodge can handle naturally, whereas WT-GP-LG cannot. In Figure 1, we plot the classification accuracy with varying noise level, which shows robustness of WT-GP-Hodge to noise compared to WT-GP-LG.

    & ENZYMES & MUTAG & IMDB-BINARY & IMDB-MULTI & ring-vs-clique & subm \\  WT-GP & 65.00 \(\) 4.94 & 86.73 \(\) 4.18 & **74.20 \(\) 3.87** & 48.73 \(\) 2.76 & 99.5 \(\) 1.5 & 86.42 \(\) 6.92 \\ WT-GP-Hodge & **67.65 \(\) 6.86** & **88.06 \(\) 7.99** & 73.40 \(\) 3.04 & **52.09 \(\) 3.44** & **100.0 \(\) 0.0** & **88.02 \(\) 7.35** \\   

Table 1: Comparison of classification accuracy on several graph classification benchmark datasets.

Figure 1: Accuracy vs. noise level in the vector field classification.

Conclusion and Future Directions

We have presented a GP-based classification algorithm for classifying graphs according to one or both vertex and edge features. By applying the graph wavelet transform to spatial graph features, we have constructed spectral features, providing multi-resolution spectral signatures of the original features, subsequently utilised as input points to a standard GP. Furthermore, by taking the discrete Hodge decomposition, we have shown improvements over the method proposed by Opolka et al. , even on graph datasets containing only vertex features, owing to our flexible Hodgelet kernel. Overall, we have demonstrated that our approach effectively improves graph classification tasks by employing a spectral perspective to capture both local and global properties of vertex and edge features. In the future, we intend to explore extensions to higher-order networks, including simplicial complexes, cellular complexes, and hypergraphs, which we briefly outline in Appendix D.