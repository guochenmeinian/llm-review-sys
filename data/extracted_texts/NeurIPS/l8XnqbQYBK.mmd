# Toward Conditional Distribution Calibration in Survival Prediction

Shi-ang Qi \({}^{1}\), Yakun Yu \({}^{2}\), Russell Greiner \({}^{1}\)\({}^{3}\)

\({}^{1}\)Computing Science, University of Alberta, Edmonton, Canada

\({}^{2}\)Electrical Computer Engineering, University of Alberta, Edmonton, Canada

\({}^{3}\)Alberta Machine Intelligence Institute, Edmonton, Canada

{shiang, yakun2, rgreiner}@ualberta.ca

###### Abstract

Survival prediction often involves estimating the time-to-event distribution from censored datasets. Previous approaches have focused on enhancing discrimination and marginal calibration. In this paper, we highlight the significance of _conditional calibration_ for real-world applications - especially its role in individual decision-making. We propose a method based on conformal prediction that uses the model's predicted individual survival probability at that instance's observed time. This method effectively improves the model's marginal and conditional calibration, without compromising discrimination. We provide asymptotic theoretical guarantees for both marginal and conditional calibration and test it extensively across 15 diverse real-world datasets, demonstrating the method's practical effectiveness and versatility in various settings.

## 1 Introduction

Individual survival distribution (ISD), or time-to-event distribution, is a probability distribution that describes the times until the occurrence of a specific event of interest for an instance, based on information about that individual. Accurately estimating ISD is essential for effective decision-making and clinical resource allocation. However, a challenge in learning such survival prediction models is training on datasets that include _censored_ instances, where we only know a lower bound of their time-to-event.

Survival models typically focus on two important but distinct properties during optimization and evaluation: (i) _discrimination_ measures how well a model's relative predictions between individuals align with the observed order , which is useful for pairwise decisions such as prioritizing treatments; (ii) _calibration_ assesses how well the predicted survival probabilities match the actual distribution of observations , supporting both individual-level (_e.g._, determining high-risk treatments based on the probability) and group-level (_e.g._, allocating clinical resources) decisions. Some prior research has sought to improve calibration by integrating a calibration-specific loss during optimization . However, these often produce models with poor discrimination , limiting their utility in scenarios where precise pairwise decisions are critical.

Furthermore, previous studies have typically addressed calibration in a marginal sense - _i.e._, assessing whether probabilities align with the actual distribution _across the entire population_. However, for many applications, marginal calibration may be inadequate - we often require that predictions are correctly calibrated, _conditional on any combination of features_. This can be helpful for making more precise clinical decisions for individuals and groups. For example, when treating an overweight male, a doctor might decide on cardiovascular surgery using a model calibrated for both overweight and male. Note this might lead to a different decision that one based on a model that was calibrated for all patients. Similarly, a hospice institution may want to allocate nursing care based on a modelthat generates calibrated predictions _for elderly individuals_. This also aligns with the fairness perspective , where clinical decision systems should guarantee equalized calibration performance across any protected groups.

ContributionsTo overcome these challenges, we introduce the CiPOT framework, a post-processing approach built upon conformal prediction [10; 11; 12; 8] that uses the Individual survival Probability at Observed Time (iPOT) as conformity scores and generates conformalized survival distributions. The method has 3 important properties: (i) this conformity score naturally conforms to the definition in distribution calibration in survival analysis ; (ii) it also captures the distribution variance of the ISD, therefore is adaptive to the features; and (iii) the method is computationally friendly for survival analysis models. Our key contributions are:

* Motivating the use of conditional distribution calibration in survival analysis, and proposing a metric (\(_{}\), defined in Section 4) to evaluate this property.
* Developing the CiPOT framework, to accommodate censorship. The method effectively solves some issues of previous conformal methods wrt inaccurate Kaplan-Meier estimation;
* Theoretically proving that CiPOT asymptotically guarantees marginal and conditional distribution calibration under some specified assumptions;
* Conducting extensive experiments across 15 datasets, showing that CiPOT improves both marginal and conditional distribution calibration without sacrificing discriminative ability;
* Demonstrating that CiPOT is computationally more efficient than prior conformal method on survival analysis.

## 2 Problem statement and Related Work

### Notation

A survival dataset \(=\{(_{i},t_{i},_{i})\}_{i=1}^{n}\) contains \(n\) tuples, each containing covariates \(_{i}^{d}\), an observed time \(t_{i}_{+}\), and an event indicator \(_{i}\{0,1\}\). For each subject, there are two potential times of interest: the event time \(e_{i}\) and the censoring time \(c_{i}\). However, only the earlier of the two is observable. We assign \(t_{i}\{e_{i},c_{i}\}\) and \(_{i}[e_{i} c_{i}]\), so \(_{i}=0\) means the event has not happened by \(t_{i}\) (right-censored) and \(_{i}=1\) indicates the event occurred at \(t_{i}\) (uncensored). Let \(\) denote the set of indices in dataset \(\), then we can use \(i\) to represent \((_{i},t_{i},_{i})\).

Our objective is to estimate the Individualized Survival Distribution (ISD), \(S(t_{i})=(e_{i}>t=_{i})\), which represents the survival probabilities of the \(i\)-th subject for any time \(t 0\).

### Notions of calibration in survival analysis

Calibration measures the alignment between the predictions against observations. Consider distribution calibration at the individual level: if an oracle knows the true ISD \(S(t_{i})\), and draws realizations of \(e_{i}_{i}\) (call them \(e_{i}^{(1)},e_{i}^{(2)},\)), then the survival probability at observed time \(\{S(e_{i}^{(m)}_{i})\}_{m}\) should be distributed across a standard uniform distribution \(_{}\) (probability integral theorem ). However, in practice, for each unique \(_{i}\), there is only one realization of \(e_{i}_{i}\), meaning we cannot check the calibration in this individual manner.

To solve this, Haider et al.  proposed _marginal calibration_, which holds if the predicted survival probabilities at event times \(e_{i}\) over the \(_{i}\) in the dataset, \(\{(e_{i}_{i})\}_{i}\), matches \(_{}\).

**Definition 2.1**.: For uncensored dataset, a model has perfect marginal calibration iff \(\ [_{1},_{2}][0,1]\),

\[(.(e_{i}_{i})[_{1},_{2} ],\,i\,|_{i}=1)\ =\ _{i}\ [.(e_{i}_{i})[_{1},_{2} ]|_{i}=1]\ =\ _{2}-_{1}.\] (1)

We can "blur" each censored subject uniformly over the probability intervals after the survival probability at censored time \((c_{i}_{i})\) (see the derivation in Appendix A):

\[(.(e_{i}_{i})[_{1},_{2} ]|_{i}=0)=(t_{i}_{i})-_{1} )(t_{i}_{i})[_{1},_{2} ]+(_{2}-_{1})(t_{i}_{i} )_{2}}{(t_{i}_{i})}.\] (2)Figure 1(a) illustrates how marginal calibration is assessed using 6 uncensored subjects. Figure 1(b, c) show the histograms and P-P plots, showing the predictions are marginally calibrated, over the predefined 3 bins, as we can see there are 6/3 = 2 instances in each of the 3 bins. However, if we divide the datasets into two groups (orange vs. blue - think men vs. women), we can see that this is not the case, as there is no orange instance in the \([,1]\) bin, and 2 orange instances in the \([,1]\) bin.

In summary, individual calibration is ideal but impractical. Conversely, marginal calibration is more feasible but fails to assess calibration relative to certain subsets of the population by features. This discrepancy motivates us to explore a middle ground - _conditional calibration_. A conditionally calibrated prediction, which ensures that the predicted survival probabilities are uniformly distributed in each of these groups, as shown in Figure 1(d, e, f), is more effective in real-world scenarios. Consider predicting employee attrition within a company: while a marginal calibration using a Kaplan-Meier (KM)  curve might reflect overall population trends, it fails to account for variations such as the tendency of lower-salaried employees to leave earlier. A model that is calibrated for both high and low salary levels would be more helpful for predicting the actual quitting times and facilitate planning. Similarly, when predicting the timing of death from cardiovascular diseases, models calibrated for older populations, who exhibit more predictable and less varied outcomes , may not apply to younger individuals with higher outcome variability. Using age-inappropriate models could lead to inaccurate predictions, adversely affecting treatment plans.

### Maintaining discriminative performance while ensuring good calibration

Methods based on the objective function [5; 6; 4] have been developed to enhance the marginal calibration of ISDs, involving the addition of a calibration loss to the model's original objective function (_e.g._, likelihood loss). However, while those methods are effective in improving the marginal calibration performance of the model, their model often significantly harms the discrimination performance [6; 4; 7], a phenomenon known as the _discrimination-calibration trade-off_.

Post-processed methods [12; 8] have been proposed to solve this trade-off by disentangling calibration from discrimination in the optimization process. Candes et al.  uses the individual censoring probability as the weighting in addition to the regular Conformalized Quantile Regression (CQR)  method. However, their weighting method is only applicable to Type-I censoring settings where each subject must have a known censored time  - which is not applicable to most of the right-censoring datasets.

Figure 1: Two notions of distribution calibration: marginal and conditional, illustrated using 3 bins separated at \(\) and \(\). The curves in (a, d) represent the predicted ISDs. The colors of the stars distinguish the six subjects, with horizontal coordinates indicating the true event time (consistent across all panels) and vertical coordinates representing predicted survival probability at event time. Note the two groups (orange for \(x=0\) and blue for \(x=1\)) correspond to the colors of the curves and histograms in (a, b, d, e). Note that all three P-P lines in the conditional case (f) coincide.

Qi et al.  developed Conformalized Survival Distribution (CSD) by first discretizing the ISD curves into percentile times (via predefined percentile levels), and then applying CQR  for each percentile level (see the visual illustration of CSD in Figure 6 in Appendix B). Their method handles right-censoring using KM-sampling, which employs a conditional KM curve to simulate multiple event times for a censored subject, offering a calibrated approximation for the ISD based on the subject's censored time. However, their method struggles with some inherent problems of KM  - _e.g._, KM can be inaccurate when the dataset contains a high proportion of censoring . Furthermore, we also observed that the KM estimation often concludes at high probabilities (as seen in datasets like HCFR, FLCHAIN, and Employee in Figure 9). This poses a challenge in extrapolating beyond the last KM time point, which hinders the accuracy of KM-sampling, thereby constraining the efficacy of CSD (see our results in Figure 3).

Our work is inspired by CSD, and can be seen as a percentile-based refinement of their regression-based approach. Specifically, our CiPOT effectively addresses and resolves issues prevalent in the KM-sampling, significantly outperforming existing methods in terms of improving the marginal distribution calibration performance. Furthermore, to our best knowledge, this is the first approach that optimizes conditional calibration within the survival analysis that can deal with censorship.

However, achieving conditional calibration (also known as conditional coverage in some literature ) is challenging because it cannot be attained in a distribution-free manner for non-trivial predictions. In fact, guarantees of finite sample for conditional calibration are impossible to achieve even for standard regression datasets without censorship [18; 19; 10]. This limitation is an important topic in the statistical learning and conformal prediction literature [18; 10; 19]. Therefore, our paper does not attempt to provide finite sample guarantees. Instead, following the approach of many other researchers [11; 20; 21; 22; 23; 24], we provide only asymptotic guarantees as the sample size approaches infinity. The key idea behind this asymptotic conditional guarantee is that the construction of post-processing predictions relies on the quality of the original predictions. Thus, we aim for conditional calibration only within the class of predictions that can be learned well - that is, consistent estimators.

We acknowledge that this assumption may not hold in practice; however, (i) reliance on consistent estimators is a standard (albeit strong) assumption in the field of conformal prediction [21; 22; 23], (ii) to the best of our knowledge, no previous results have proven conditional calibration under more relaxed conditions, and (iii) we provide empirical evidence of conditional calibration using extensive experiments (see Section 5).

## 3 Methods

This section describes our proposed method: Conformalized survival distribution using Individual survival Probability at Observed Time (CiPOT), which is motivated by the definition of distribution calibration  and consists of three components: the estimation of continuous ISD prediction, the computation of suitable conformity scores (especially for censored subjects), and their conformal calibration.

### Estimating survival distributions

For simplicity, our method is motivated by the split conformal prediction [25; 11]. We start the process by splitting the instances of the training data into a proper training set \(^{}\) and a conformal set \(^{}\). Then, we can use any survival algorithm or quantile regression algorithm (with the capability of handling censorship) to train a model \(\) using \(^{}\) that can make ISD predictions for \(^{}\) - see Figure 2(a).

With little loss of generality, we assume that the ISD predicted by the model, \(_{}(t_{i})\), are right-continuous and have unbounded range, _i.e._, \(_{}(t_{i})>0\) for all \(t 0\). For survival algorithms that can only generate piecewise constant survival probabilities (_e.g._, Cox-based methods [26; 27], discrete-time methods [28; 29], etc.), the continuous issue can be fixed by applying some interpolation algorithms (_e.g._, linear or spline).

### Compute conformal scores and calibrate predicting distributions

We start by sketching how CiPOT deals with only uncensored subjects. Within the conformal set, for each subject \(i^{}\), we define a distributional conformity score, wrt the model \(\), termed _the predicted Individual survival Probability at Observed Time (iPOT)_:

\[_{i,}\;:=\;_{}(e_{i}\;|\;_{i}).\] (3)

Here, for uncensored subjects, the observed time corresponds to the event time, \(t_{i}=e_{i}\). Recall from Section 2.2 that predictions from model \(\) are marginally calibrated if the iPOT values follow \(_{}\) - _i.e._, if we collect the distributional conformity scores for every subject in the conformal set \(_{}=\{_{i,}\}_{i^{}}\), the \(\)-th percentile value in this set should be equal to exactly \(\). If so, no post processing adjustments are necessary.

In general, of course, the estimated Individualized Survival Distributions (ISDs) \((t\;|\;_{i})\) may not perfectly align with the true distributions \(S(t\;|\;_{i})\) from the oracle. Therefore, for a testing subject with index \(n+1\), we can simply apply the following adjustment to its estimated ISD:

\[_{}^{-1}(\;|\;_{n+1})\;:=\;_{ }^{-1}(\;(;_{})\;|\;_{n+1} \;),(0,1).\] (4)

Here, \((;_{})\) calculates the \(^{}|+1)]}{|^{}|}\)-th empirical percentile of \(_{}\). This adjustment aims to re-calibrate the estimated ISD based on the empirical distribution of the conformity scores.

Visually, this adjustment involves three procedures:

1. It first identifies the empirical percentiles of the conformity scores - \((;\,_{})\) and \((;\,_{})\), illustrated by the two grey lines at 0.28 and 0.47 in Figure 2(d), respectively - which uniformly divide the stars according to their vertical locations;
2. It then determines the corresponding times on the predicted ISDs that match these empirical percentiles (the hollow circles, where each ISD crosses the horizontal line);
3. Finally, the procedure shifts the empirical percentiles (grey lines) to the appropriate height of desired percentiles (\(\) and \(\)), along with all the circles. This operation is indicated by the vertical shifts of the hollow points, depicted with curved red arrows in Figure 2(d).

Figure 2: A visual example of using CiPOT to make the prediction (conditionally)-calibrated. (a) Initialize ISD predictions from an arbitrary survival algorithm with associated (b) histograms and (c) P-P plots. (d) Calculate \((;\,_{})\) (grey lines) for all \(\)s, and find the intersections (hollow points) of the ISD curves and the \((;\,_{})\) lines; (e) Generate new ISD by vertically shifting the hollow points to the \(\)’s level, with associated (f) histogram and (g) P-P plots. Figure 6 provide a side-by-side visual comparison between CSD and our method.

This adjustment results in the post-processed curves depicted in Figure 2(e). It shifts the vertical position of the green star \(\) from the interval \(,\) to \(,1\), and the pink star \(\) from \(0,\) to \(,\). These shifts ensure that the calibration histograms and P-P plots achieve a uniform distribution (both marginally on the whole population and conditionally on each group) across the defined intervals.

After generating the post-processed curves, we can apply a final step, which involves transforming the inverse ISD function back into the ISD function for the testing subject:

\[_{}(t\,|\,_{n+1})\;=\;\{\,\, :_{}^{-1}(\,|\,_{n+1}) t\,\}.\] (5)

The simple visual example in Figure 2 shows only two percentiles created at \(\) and \(\). In practical applications, the user provides a predefined set of percentiles, \(\), to adjust the ISDs. The choice of \(\) can slightly affect the resulting survival distributions, each capable of achieving provable distribution calibration; see ablation study #2 in Appendix E.6 for how \(}\) affects the performance.

### Extension to censorship

It is challenging to incorporate censored instances into the analysis as we do not observe their true event times, \(e_{i}\), which means we cannot directly apply conformity score in (3) and the subsequent conformal steps. Instead, we only observe the censoring times, which serve as lower bounds of the event times.

Given the monotonic decreasing property of the ISD curves, the iPOT value for a censored subject, _i.e._, \(_{}(t_{i}\,|\,_{i})=_{}(c_{i}\,|\, _{i})\), now serves as the upper bound of \(_{}(e_{i}\,|\,_{i})\). Therefore, given the prior knowledge that \(_{}(e_{i}\,|\,_{i})_{}\), the observation of the censoring time updates the possible range of this distribution. Given that \(_{}(e_{i}\,|\,_{i})\) must be less than or equal to \(_{}(c_{i}\,|\,_{i})\), the updated posterior distribution follows \(_{}(e_{i}\,|\,_{i})_{[0,_{ }(c_{i}|_{i})]}\).

Following the calibration calculation in , where censored patients are evenly "blurred" across subsequent bins of \((c_{i}\,|\,_{i})\), our approach uses the above posterior distribution to uniformly draw \(R\) potential conformity scores for a censored subject, for some constant \(R^{+}\). Specifically, for a censored subject, we calculate the conformity scores as:

\[_{i,}\,=\,_{}(c_{i}\,|\, _{i})_{R},_{R}\,=\,\,0/R \,,1/R\,,,R/R\,\,.\]

Here, \(_{R}\) is a pseudo-uniform vector to mimic the uniform sampling operation, significantly reducing computational overhead compared to actual uniform distribution sampling. For uncensored subjects, we also need to apply a similar sampling strategy to maintain a balanced censoring rate within the conformal set. Because the exact iPOT value is known and deterministic for uncensored subjects, sampling involves directly drawing from a degenerate distribution centered at \(_{}(e_{i}\,|\,_{i})-i.e.\), just drawing \(_{}(e_{i}\,|\,_{i})\)\(R\) times. The pseudo-code for implementing the CiPOT process with censoring is outlined in Algorithm 1 in Appendix B.

Note that the primary computational demand of this method stems from the optional interpolation and extrapolation of the piecewise constant ISD predictions. Calculating the conformity scores and estimating their percentiles incur negligible costs in terms of both time and space, once the right-continuous survival distributions are established. We provide computational analysis in Appendix E.5.

### Theoretical analysis

Here we discuss the theoretical properties of CiPOT. Unlike CSD, which adjusts the ISD curves horizontally (changing the times, for a fixed percentile), our refined version scales the ISD curves vertically. This vertical adjustment leads to several advantageous properties. In particular, we highlight why our method is expected to yield superior performance in terms of marginal and conditional calibration compared to CSD. Table 1 summarizes the properties of the two methods.

CalibrationCiPOT differs from CSD in two major ways: CiPOT (i) essentially samples the event time from \(_{}(t\,|\,t>c_{i},\,\,_{i})\) for a censored subject, and (ii) subsequently converts these times into corresponding survival probability values on the curve.

The first difference contrasts with the CSD method, which samples from a conditional KM distribution, \(S_{}(\,t t>c_{i}\,)\), assuming a homoskedastic survival distribution across subjects (where the conditional KM curves have the same shape and the random disturbance of \(e_{i}\) is independent of the features \(_{i}\)). However, CiPOT differs by considering the heteroskedastic nature of survival distributions \(_{}(\,t t>c_{i},\;_{i})\). For instance, consider the symptom onset times following exposure to the COVID-19 virus. Older adults, who may exhibit more variable immune responses, could experience a broader range of onset times compared to younger adults, whose symptom onset times are generally more consistent . By integrating this feature-dependent variability, CiPOT captures the inherent heteroskedasticity of survival distributions and adjusts the survival estimates accordingly, which helps with conditional calibration.

Furthermore, by transforming the times into the survival probability values on the predicted ISD curves (the second difference), we mitigate the trouble of inaccurate interpolation and extrapolation of the distribution. This approach is particularly useful when the conditional distribution terminates at a relatively high probability, where extrapolating beyond the observed range is problematic due to the lack of data for estimating the tail behavior. Different extrapolation methods, whether parametric or spline-based, can yield widely varying behaviors in the tails of the distribution, potentially leading to significant inaccuracies in survival estimates. However, by converting event times into survival percentiles, CiPOT circumvents these issues. This method capitalizes on the probability integral transform , which ensures that regardless of the specific tail behavior of a survival function, its inverse probability values will follow a uniform distribution.

The next results state that the output of our method has asymptotic marginal calibration, with necessary assumptions (exchangeability, conditional independent censoring, and continuity). We also prove the asymptotic conditional calibrated guarantee for CiPOT. The proofs of these two results are inspired by the standard conformal prediction literature , with adequate modifications to accommodate our method. We refer the reader to Appendix C.1 for the complete proof.

**Theorem 3.1** (Asymptotic marginal calibration).: _If the instances in \(\) are exchangeable, and follow the conditional independent censoring assumption, then for a new instance \(n+1,\;\;_{1}<_{2}\),_

\[_{2}-_{1}(_{}(t_{n+1} _{n+1})[_{1},_{2}])_{2}-_{1} +^{}|+1}.\]

**Theorem 3.2** (Asymptotic conditional calibration).: _In addition to the assumptions in Theorem 3.1, if (i) the non-processed prediction \(_{}(\,t_{i})\) is a consistent survival estimator; (ii) its inverse function is differentiable; and (iii) the 1st derivation of the inverse function is bounded by a constant, then the CiPOT process will achieve asymptotic conditional distribution calibration._

MonotonicityUnlike CSD, CiPOT does not face any non-monotonic issues for the post-processed curves as long as the original ISD predictions are monotonic; see proof in Appendix C.2.

**Theorem 3.3**.: CiPOT _process preserves the monotonic decreasing property of the ISD._

CSD, built on the Conformalized Quantile Regression (CQR) framework, struggles with the common issue of non-monotonic quantile curves (refer to Appendix D.2 in  and our Appendix C.2). While some methods, like the one proposed by Chernozhukov et al. , address this issue by rearranging quantiles, they can be computationally intensive and risk (slightly) recalibrating and distorting discrimination in the rearranged curves. By inherently maintaining monotonicity, CiPOT not only enhances computational efficiency but also avoids these risks.

    & Marginal & Conditional & & Harrell’s & Antolini’s &  \\  & calibration & calibration & Monotonic & discrimination & discrimination & \\  & guarantee & guarantee & & guarantee & guarantee & complexity\({}^{}\) \\  CSD  & X & X & X & \(\) & X & \(O(N|| R)\) \\ CiPOT & \(\) & \(\) & \(\) & X & \(\) & \(O(N R)\) \\   

Table 1: Properties of CSD and CiPOT. Note that the calibration guarantees refer to asymptotic calibration guarantees. \({}^{}\)See Appendix E.5.

DiscriminationQi et al.  demonstrated that CSD theoretically guarantees the preservation of the original model's discrimination performance in terms of Harrell's concordance index (C-index) . However, CiPOT lacks this property; see Appendix C.3 for details.

As CiPOT vertically scales the ISD curves, it preserves the relative order of survival probabilities at any single time point. This preservation means that the discrimination power, measured by the area under the receiver operating characteristic (AUROC) at any time, remains intact (Theorem C.4). Furthermore, Antolini's time-dependent C-index (\(C^{td}\)) , which represents a weighted average AUROC across all time points, is also guaranteed to be maintained by our method (Lemma C.5). As a comparison, CSD does not have such a guarantee for neither AUROC nor \(C^{td}\).

## 4 Evaluation metrics

We measure discrimination using Harrell's C-index , rather than Antolini's \(C^{td}\), as Lemma C.5 already established that \(C^{td}\) is not changed by CiPOT. We aim to assess our performance using a measure that represents a relative weakness of our method.

As to the calibration metrics, the marginal calibration score evaluated on the test set \(^{}\) is calculated as [3; 8]:

\[_{}(;)\ =\ |}_{ }\,\,\,(e_{i}_{i}) [0,]\,,\,i^{}-\ \,^{2}\,,\] (6)

where \(((e_{i}_{i})[0,]\,,\,i^{ {test}})\) is calculated by combining (1) and (2); see (8) in Appendix A. Based on the marginal calibration formulation, a natural way for evaluating the conditional calibration could be: (i) heuristically define a finite feature space set \(\{_{1},_{2},\}\) - _e.g._, \(_{1}\) is the set of divorced elder males, \(_{2}\) is females with 2 children, etc.; and (ii) calculate the worst calibration score on all the predefined sub-spaces. This is similar to fairness settings, researchers normally select age, sex, or race as the sensitive attributes to form the feature space. However, this metric does not scale to higher-dimensional settings because it is challenging to create the feature space set that contains all possible combinations of the features.

Motivated by Romano et al. , we proposed a worst-slab distribution calibration, \(_{}\). We start by partition the testing set into a 25% exploring set \(^{}\) and a 75% exploiting set \(^{}\). The exploring set is then used to find the worst calibrated sub-region in the feature space \(^{d}\):

\[_{,a,b}\ =\ \{\,_{i}^{d}:a ^{}_{i} b\,\}\,\, {x}_{i}_{,a,b},i^{}\, \ \ ,\] \[,a,b=*{arg\,max}_{ ^{d},a<b}|}_{}\,\,\,(e_{i}_{i})[0,]\,,\,i ^{}\,,\,_{i}_{,a,b}) -\,^{2}\,.\]

In practice, the parameters \(\), \(a\), and \(b\) are chosen adversarially by sampling i.i.d. vectors \(\) on the unit sphere in \(^{d}\) then finding the \(*{arg\,max}\) using a grid search on the exploring set. \(\) is a predefined threshold to ensure that we only consider slabs that contain at least \(\%\) of the instances (so that we do not encounter a pregnant-man situation). Given this slab, we can calculate the conditional calibration score on the evaluation set for this slab:

\[_{}(;,_{,a,b})\ =\ |}_{}\, \,\,(e_{i}_{i})[0,]\,,\,i^{}\,,\,_{i}_{,a,b})\ -\ \,^{2}\,.\] (7)

Besides the above metrics, we also evaluate using other commonly used metrics: integrated Brier score (IBS) , and mean absolute error with pseudo-observation (MAE-PO) ; see Appendix D.

## 5 Experiments

The implementation of CiPOT method, worst-slab distribution calibration score, and the code to reproduce all experiments in this section are available at https://github.com/shi-ang/MakeSurvivalCalibratedAgain.

### Experimental setup

DatasetsWe use 15 datasets to test the effectiveness of our method. Table 3 in Appendix E.1 summarizes the dataset statistics, and Appendix E.1 also contains details of preprocessing steps,

[MISSING_PAGE_FAIL:9]

lines. Table 2 also shows that CiPOT provided better marginal calibration than the baselines in 95 (and significantly in 50) out of 104 comparisons (91%).

CiPOT's marginal calibration was better than CSD most of the time (68/104, 65%). The cases where CSD performs better typically involve models like _DeepHit_ or _CQRNN_. This shows that our approach often does not perform as well as CSD when the original model is heavily miscalibrated, which suggests a minor limitation of our method. Appendix C.4 discusses why our method is sub-optimal for these models.

Conditional CalibrationFor _small_ datasets (sample size \(<1000\)), in some random split, we can find a worst-slab region \(_{,a,b}\) on the exploring set with \((_{i}_{,a,b},\,i^{}) 33\%\) but still no subjects in this region in the exploiting set. This is probably because we only ensure that the times and censored indicators are balanced during the partition, however, the features can still be unbalanced. Therefore, we only evaluated conditional calibration on the 10 larger datasets, resulting in 69 comparisons. Among them, CiPOT improved conditional calibration in 64 cases (93%) compared to baselines and in 51 cases (74%) compared to CSD.

Case StudyWe provide 4 case studies in Figure 13 in Appendix E.4, where CSD leads to significant miscalibration within certain subgroups, and CiPOT can effectively generate more conditional calibrated predictions in those groups. These examples show that CSD's miscalibration is always located at the low-probability regions, which corresponds to our statement (in Section 3.4) that the conditional KM sampling method that CSD used is problematic for the tail of the distribution.

Other MetricsResults in Table 2 and Appendix E.4 show that CiPOT also showed improvement in both IBS and MAE-PO, outperforming 63 and 54 out of 104 comparisons, respectively.

Computational AnalysisAppendix E.5 shows the comprehensive results and experimental setup. In summary, CiPOT significantly reduces the space consumption and running time.

Ablation StudiesWe conducted two ablation studies to assess (i) the impact of the repetitions value (\(R\)) and (ii) the impact of predefined percentiles (\(\)) on the method; see Appendix E.6.

## 6 Conclusions

Discrimination and marginal calibration are two fundamental yet distinct elements in survival analysis. While marginal calibration is feasible, it overlooks accuracy across different groups distinguished by specific features. In this paper, we emphasize the importance of conditional calibration for practical applications and propose a principled metric for this purpose. By generating conditionally calibrated Individual Survival Distributions (ISDs), we can better communicate the uncertainty in survival analysis models, enhancing their reliability, fairness, and real-world applicability.

We therefore define the Conformalized survival distribution using Individual Survival Probability at Observed Time (CiPOT) - a post-processing framework that enhances both marginal and conditional calibration without compromising discrimination. It addresses common issues in prior methods, particularly under high censoring rates or when the Kaplan-Meier curve terminates at a high probability. Moreover, this post-processing adjusts the ISDs by adapting the heteroskedasticity of the distribution, leading to asymptotic conditional calibration. Our extensive empirical tests confirm that CiPOT significantly improves both marginal and conditional performance without diminishing the models' discriminative power.

Figure 4: Violin plots of \(_{}\) performance, where the shape and black bars represent the density and mean. Smaller values represent better performance. Note _CQRNN_ did not converge on MIMIC-IV.