# BERTs are Generative In-Context Learners

David Samuel

Language Technology Group

University of Oslo

davisamu@uio.no

###### Abstract

While in-context learning is commonly associated with causal language models, such as GPT, we demonstrate that this capability also 'emerges' in masked language models. Through an _embarrassingly simple_ inference technique, we enable an existing masked model, DeBERTa, to perform generative tasks without additional training or architectural changes. Our evaluation reveals that the masked and causal language models behave very differently, as they clearly outperform each other on different categories of tasks. These complementary strengths suggest that the field's focus on causal models for in-context learning may be limiting - both architectures can develop these capabilities, but with distinct advantages; pointing toward promising hybrid approaches that combine the strengths of both objectives.

## 1 Introduction

Masked language models used to dominate the field of natural language processing due to their adaptability across diverse tasks and their superior performance compared to causal language models (Radford et al., 2018; Devlin et al., 2019). Between 2018 and 2020, the field witnessed a surge in the development of these models (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020, inter alia). However, the field dramatically shifted with GPT-3 and its introduction of _in-context learning_ - the ability to infer and perform tasks from prompts and examples without any finetuning (Brown et al., 2020). This capability eliminated the need for task-specific training data and deep-learning expertise, making such models far more practical for real-world applications. This perceived advantage led many researchers and practitioners to abandon masked language models in favor of GPT-style architectures.

Figure 1: **The average 1-shot performance across four groups of NLP tasks** We compare the scaling abilities of DeBERTa (four sizes in red) with GPT-3 (eight sizes in blue). Even though these models rely on different training objectives, they scale in a similar log-linear manner overall. Yet, on a task-by-task basis, the pretraining methods lead to substantial differences between them.

Previous studies of 'emergent' in-context learning abilities have focused almost exclusively on causal language models, creating a widespread assumption that this capability is unique to them (Saunshi et al., 2021; Olsson et al., 2022; Wei et al., 2022; Wang et al., 2023, inter alia). In this paper, we challenge this assumption by demonstrating that in-context learning can emerge in masked language models as well. In-context learning is a more general phenomenon and should not be studied with a singular pretraining objective in mind. Moreover, the assumed inability of masked language models to perform (generative) in-context learning has rendered them outdated - as explicitly noted by Tay et al. (2023): _"BERT-style models are very restricted in their generative capabilities. Because of the cumbersomeness of task-specific classification heads, we strongly do not recommend using this class of autoencoding models moving forward and consider them somewhat deprecated."_

In this paper, we challenge these prevailing assumptions about masked language models (MLMs). We present empirical evidence showing that DeBERTa, an MLM released just one month after GPT-3, is equally adept at in-context learning. Our findings suggest that the capacity for in-context learning is not tied to the training objective, but can be achieved across different types of language models. To our surprise, we found that DeBERTa does not simply mimic the performance of GPT-3 - the two model behave very differently - DeBERTa is clearly much better on tasks such as language understanding, and, on the other hand, much worse on tasks such as closed-book question answering. This suggests that masked and causal language modeling are two complementary training objectives and that there is a great potential for a training method that combines the strengths of both objectives. Finally, _scaling_ (performance improvement with increased size of pretrained language models) is a crucial feature of modern language models; we demonstrate that MLMs do _scale_ on in-context learning (Figure 1).

We introduce a simple inference technique that transforms an MLM into a generative model without any further training. Using publicly available DeBERTa checkpoints, we show that the MLM training objective not only provides a versatile way of encoding text, but is also competitive in text generation and text completion ranking. This claim is tested by following the same evaluation suite as GPT-3, speculating on an 'alternative reality' in which a masked language model is the first model reported to achieve the so-called 'emergent' in-context learning abilities. While other masked language models could potentially demonstrate similar capabilities, we deliberately target DeBERTa because of its large size and its length-generalization abilities. Ultimately, our goal is to demonstrate that MLMs _can_ perform in-context learning and that they _can_ be surprisingly good at doing so.

OutlineFirst, Section 2 (Method) describes the inference methods used to evaluate the in-context learning abilities of an off-the-shelf masked language model. Then Section 3 (DeBERTa) describes the details of the particular model used in this study. Section 4 (Evaluation) details the evaluation setup and compares DeBERTa with GPT-3. Finally, Section 5 (Related work) talks about other relevant work within this topic, and the paper concludes with Section 6 (Conclusion).

Figure 2: **Illustration of the proposed methods for using a masked language model for text generation and text ranking** We show how to adapt a masked language model for in-context-learning tasks through simple input reformatting, requiring no additional training. Left: Text generation is achieved by 1) appending [MASK] tokens to the input prompt, 2) predicting the next token for the first mask, and 3) iteratively appending new masks and predicting tokens. Right: A similar approach is used to retrieve a pseudo-log-likelihood score of a text sequence that can be used to rank multiple sequences by their individual likelihoods. Both methods maintain the model’s original architecture while enabling new capabilities through careful input formatting.

Method: text generation and ranking with masked language models

The goal of this article is to reuse an existing pretrained masked language model for (generative) in-context learning. We achieve this without any additional training or finetuning, our method only slightly changes the sequence of input tokens, as illustrated in Figure 2. There are two methods used to solve tasks with in-context learning: **text generation** where the model completes a given prompt (e.g. for translation) and **ranking** where the model chooses an answer from several options (e.g. for multiple choice questions).

### Text generation

Masked language models are trained on semi-supervised fill-in-the-blanks tasks and so they cannot be used to generate straight out of the box. One possibility is to interpret these models as Markov random fields and produce text by Gibbs sampling (Wang and Cho, 2019). However, a simpler and more consistent way to produce text is to do the familiar left-to-right autoregressive generation - we could place a [MASK] token next to a text prompt and let the model generate next token by unmasking the appended token - then, when we repeat this process in a loop, we can generate text in the same way as causal language models (and apply the same advanced generation techniques).

This straightforward inference scheme would be enough if the pretraining process were designed with this use case in mind. However, since our goal is to repurpose an existing masked language model, we have to complicate the method with two modifications that are also illustrated in Figure 2:

1. Masked language models are typically trained with a special end-of-sequence [SEP] token. This token is always present during pretraining and so we also have to include it as the last token during inference.
2. However, the addition of this end-of-sequence token creates a problem - it raises the probability that the masked token should end the sequence (for example with a full stop). Thus, in order to obtain a less restricted continuation, we include additional [MASK] tokens to pad the space in front of the end-of-sequence token. Specifically, we use two additional masks for the DeBERTa models.1 This decision is later ablated in Appendix B.

In the end, this approach gives a probability distribution over the next token prediction, thus we can use any existing method for searching or sampling an output sequence. We follow GPT-3 and use beam search with four candidate beams for all generative tasks.

LimitationsWhile this method works with the same quadratic time complexity (in sequence length), it is slower in practice because it is not possible to cache the intermediate self-attention key and value vectors. Instead, these have to be recomputed every step due to the bidirectional nature of the model. While our current implementation prioritizes demonstrating the core capability over optimization, several promising approaches could address these computational limitations in future work. For example, using prefix language modeling or selectively updating hidden vectors could significantly improve efficiency. We leave these optimizations for future work to maintain focus on establishing the fundamental ability of MLMs to generate text.

### Ranking

Many of the existing tasks for evaluating LLMs can be formulated as classification tasks where models have to select the correct answer from a number of different options. Brown et al. (2020) rank the candidate completions based on their estimated conditional log-likelihood, which can be computed exactly by the chain rule (where \(w_{0} w_{1} w_{k}\) is a completion of a prompt \(c\)):

\[(w_{0} w_{1} w_{k}\,|\,c)=_{i=0}^{k}(w_{i}\,|\,c w_{0} w_{i-1})\] (1)

While this equation matches the training objective of causal language models, it is not suitable for masked language models because they are not trained to estimate \((w_{i}\,|\,c w_{0} w_{i-1})\). Instead,Wang and Cho (2019) proposed to modify Equation (1) to make it more appropriate for BERT-like models. Salazar et al. (2020) then empirically showed that the resulting pseudo-log-likelihood (PLL) score can be used to accurately rank text sequences by their likelihood. Specifically, the PLL score is approximately proportional to the conditional probability of a text sequence and is computed as:

\[(w_{0} w_{1} w_{k}\,|\,c)} 1.0pt}_{i=0}^{k}(w_{i}\,|\,c  w_{0} w_{i-1} w_{i+1} w_{k})\] (2)

However, this approximation gets very inaccurate when there are strong local dependencies between tokens. As a counterexample, the estimated likelihood of the multi-token word _'supercalifragilistic-expialidocious'_ is seven orders of magnitude higher than that of the single-token word _'super'_, which is clearly an incorrect estimation of the relative frequencies of these words.2

We improve on this behavior by interpolating between the mathematically correct unidirectional derivation in Equation (1) and the bidirectional approximation in Equation (2). Our approach is to simply mask two additional tokens in the right context to reduce the effect of local dependencies while still taking into account the global bidirectional context. This process is illustrated in Figure 2. We conduct an ablation study of this approach in Appendix C.

LimitationsEven though Equations (1) and (2) look similar, the later sum is substantially more compute intensive when calculated with a transformer architecture - for a token sequence of length \(k\), the first equation can be computed with passing a single sequence through a language model, while the second equation needs \(k\) sequences to be processed. However, Salazar et al. (2020) showed that the PLL score can be accurately estimated in a single pass after a short self-supervised finetuning.

### Length generalization

A potentially limiting factor of using BERT-like models is that they are typically pretrained on shorter sequences than causal language models - arguably because the training of modern causal models is already optimized for in-context learning, which requires processing of long few-shot prompts. DeBERTa is not an exception to such pretraining; it was only trained with a relatively short maximum sequence length of 512 tokens (He et al., 2021). Fortunately, the architecture of DeBERTa can easily process much longer sequences than seen during training due to its use of relative positional embeddings with logarithmic buckets (Raffel et al., 2020).

Figure 3: **Length generalization measured with a ‘needle in a haystack’ benchmark** The \(x\)-axis indicates the total size of the ‘haystack’ and the \(y\)-axis indicates the position of the ‘needle’; the values show the average exact-match accuracy for a particular configuration. Unfortunately, GPT-3 is a closed-source model and the original version is not accessible, so we use an open-source replication of GPT-3, OPT by Zhang et al. (2022), which should perform similarly on this task because of the the same transformer architecture as GPT-3. In particular, it uses absolute positional encoding, which strictly limits any model from generalizing to longer inputs than trained on.

We measure the extent to which DeBERTa generalizes to longer sequences with the 'needle in a haystack' test from RULER (Hsieh et al., 2024). Specifically, in our formulation of this task, a random 6-digit number (needle) is hidden in a long collection of essays (haystack). We then measure the exact-match accuracy of retrieving the hidden number given two variables: the total sequence length and the position of the needle in the haystack (more details about the evaluation setup are given in Appendix E.1).

The results in Figure 3 demonstrate that DeBERTa generalizes to sequences well beyond its training length, which is enabled by its relative positional encoding. For comparison, we also show results from OPT (Zhang et al., 2022), which uses absolute positional encoding like GPT-3. As expected from models using absolute positional encoding, performance drops sharply beyond the training length. This comparison highlights the importance of positional encoding choice for length generalization, independent of whether the model is masked or causal. In practice, this observation means that DeBERTa should be able to handle as many task demonstrations as models trained with longer sequences.

## 3 DeBERTa family of language models

This study uses the largest openly available English masked language model, DeBERTa with 1.5 billion parametrs, and its smaller configurations - 0.1B, 0.4B and 0.9B (He et al., 2021). DeBERTa is an improved version of a BERT language model (Devlin et al., 2019) that uses an advanced attention mechanism with relative positional embeddings - apart from being trained on a larger corpus and with a larger number of training steps.

Training corpusCompared to GPT-3 and modern large language models, DeBERTa was pretrained on a relatively small and clean text corpus - totalling 78GB of data after deduplication, the corpus is comprised of the English Wikipedia (12GB), BookCorpus (6GB; Zhu et al., 2015), OpenWebText (38GB; Gokaslan and Cohen, 2019), and STORIES (31GB; Trinh and Le, 2019). This is almost an order of magnitude less data than what was used to pretrain GPT-3. Notably, our strong results - despite this data disparity - could suggest that masked language models are more data-efficient than causal models for developing in-context learning capabilities. This claim would however need to be evaluated with a comprehensive study. In comparison, GPT-3 uses 570GB of filtered CommonCrawl, WebText2 (roughly 26GB), two web-scraped book corpora (roughly 17GB and 76GB), and the English Wikipedia (roughly 4GB, estimated from Brown et al. (2020)).

Total training computeInterestingly, even though DeBERTa uses a substantially smaller training corpus, it is trained on more than three times more tokens than GPT-3 (1 trillion compared to 300 billion).3 However, the loss is computed only on 15% of tokens (150 billion) and it is not clear what would be the effective number of tokens used for pretraining. Nevertheless, the total compute used for training depends on the number of input tokens and it is roughly \(8.0 10^{21}\) FLOPs for the 1.5B DeBERTa, and \(2.4 10^{21}\) FLOPs for the 1.3B GPT-3.

Causal conversion for HuggingFaceWe have converted the officially available DeBERTa checkpoint into a HuggingFace (Wolf et al., 2020) implementation of AutoModelForCausalLM (following the method in Section 2.1), and released it openly at https://hf.co/ltg/deberta-xxlarge-fixed. The weights of this model are exactly the same as the official release from microsoft/deberta-v2-xxlarge, but we have fixed some bugs found in the original modeling script in addition to implementing the text generation abilities.4 Similarly, we have also converted the smaller DeBERTa models and released them as ltg/deberta-base-fixed, ltg/deberta-large-fixed, and ltg/deberta-xlarge-fixed.

Evaluation

As our goal is to compare two language models released around the same time in 2020 - GPT-3 and DeBERTa - we replicate the evaluation setup used for GPT-3 (Brown et al., 2020) and apply it to the latter model. This also means that we follow GPT-3 and divide the tasks into generative ones (such as machine translation) and into classification tasks (such as BoolQ) - the first group uses the method described in Section 2.1 and the second type of task uses the ranking described in Section 2.2. Generation is performed with beam search (4 candidate beams), and ranking uses the modified PLL scores (and the normalized unconditional probability of completions \(()}{()}\) for ARC and OpenBookQA), again replicating the choices for GPT-3). We also use the exact same prompt templates, with the exception of the machine translation task - its template did not produce any meaningful output, and so we decided to use the simple prompt template from Garcia et al. (2023) instead. More details on the evaluation setup can be found in Appendix E. Note that using prompts optimized for GPT-3 is slightly unfair to all other models, as prompting has a strong influence on performance (Gonen et al., 2023), but we believe that it makes the results more convincing than if we were to do extensive prompt engineering.

To show the strengths and weaknesses of DeBERTa in (generative) in-context learning, we evaluate it on four groups of tasks and compare it to the results from Brown et al. (2020). The four groups are language understanding (SuperGLUE), language modeling (text completion and Winograd-like tasks), machine translation, and question answering (closed-book question answering and commonsense reasoning). We detail each of these groups of tasks below.

Before looking into the details of each group, we show the overall aggregated scores for each group in Figure 1 and Figure 4. The first figure shows how the performance of both models scales with their size, while the latter figure compares the in-context learning abilities of the two language models. We also provide a qualitative evaluation of text generation in Appendix A and full results in Appendix F.

### Language understanding (SuperGLUE)

We use SuperGLUE (Wang et al., 2019) as a popular collection of standard NLP tasks, allowing us to evaluate the performance on different aspects of natural language understanding.

In total, this benchmark consists of eight datasets, selected to be difficult for the contemporary (finetuned) language models. The Boolean Questions dataset is a yes/no reading comprehension dataset evaluated with accuracy (BoolQ; Clark et al., 2019); CommitmentBank is a three-class textual entailment dataset evaluated with accuracy and \(_{1}\) score, where the multi-class \(_{1}\) is computed as the unweighted average of the \(_{1}\) per class (CB; de Marneffe et al., 2019); the Choice of Plausible Alternatives dataset is a causal reasoning task evaluated with accuracy (COPA; Roemmele et al.,

Figure 4: **The performance improvement with increased number of in-context examples** We compare the in-context learning ability of 1.5B DeBERTa (in red) with 1.3B GPT-3 (in blue) using prompts without any completed examples (0-shot), prompts with a single randomly sampled gold sample (1-shot), and prompts with _few_ examples (\(4-64\) examples, depending on the task). This figure demonstrates that a masked language model behaves similarly to a causal language model in the in-context learning regime. More detailed few-shot evaluation is in Figure 5.

[MISSING_PAGE_FAIL:7]

### Translation

Translation is a useful benchmark for language models as it evaluates their ability to understand text in one language and produce fluent text in another language. Even though the performance on the translation tasks is arguably very dependent on the composition of training data (especially when we are concerned with monolingual English models), we include translation to demonstrate the generative performance of masked language models.

To directly compare DeBERTa with GPT-3, we use the same SacreBLEU metric (Post, 2018) and the same bitexts. Thus, even though there are more recent (and arguably more thought-out) datasets, we use the French-English pair from the outdated 2014 shared task at the Workshop on Statistical Machine Translation (WMT14; Bojar et al., 2014), and also the Romanian-English and German-English pairs from the WMT16 workshop (Bojar et al., 2016). Our approach differs only in using a different prompt template, as we had to opt for the prompt from Garcia et al. (2023) to get consistent translations: "{$source_language}: {$source_text} {$target_language}: {$target_text}".

ResultsThe SacreBLEU scores on each language pair are given in Table 3. Unlike in the previous two task groups, the tables have turned, and the causal language model clearly outperforms the masked model in all comparisons. We believe that the subpar performance of DeBERTa can be (at least) in part explained by its relatively small and clean monolingual training corpus (Section 3), because the performance on this task is highly dependent on the presence of multilingual data in the corpus (Lin et al., 2022). The rate of improved translation performance with larger scale appears to be similar between the two models (Figure 1).

    & **HellaSwag** & **StoryCloze** & **Winograd** & **Winogrande** & **Average** \\  _0-shot_ & & & & & \\ GPT-3 & 54.7 & 73.4 & **76.9** & 58.7 & 65.5 \\ DeBERTa & **62.0** & **83.6** & 74.0 & **61.0** & **70.2** \\ _1-shot_ & & & & & \\ GPT-3 & 53.5 & 74.2 & 76.9 & 59.1 & 65.9 \\ DeBERTa & **62.4** & **84.6** & **80.7** & **63.6** & **72.8** \\ _few-shot_ & & & & & \\ GPT-3 & 54.9 & 76.1 & 76.9 & 59.1 & 64.8 \\ DeBERTa & **62.5** & **84.8** & **85.6** & **68.8** & **75.4** \\   

Table 2: **Results of text completion, language modeling and Winograd-style tasks** All tasks are measured with accuracy, we show the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced.

    & **de\(\)en** & **en\(\)de** & **fr\(\)en** & **en\(\)fr** & **ro\(\)en** & **en\(\)ro** & **Average** \\  _0-shot_ & & & & & & & \\ GPT-3 & **3.6** & **2.4** & **3.6** & **2.8** & **3.6** & **3.1** & **3.2** \\ DeBERTa & 2.4 & 1.6 & 1.7 & 0.3 & 1.7 & 0.1 & 1.3 \\ _1-shot_ & & & & & & \\ GPT-3 & **25.8** & **13.4** & **27.0** & **19.3** & **26.8** & **10.3** & **18.8** \\ DeBERTa & 23.7 & 5.4 & 23.5 & 9.7 & 17.7 & 2.5 & 13.8 \\ _few-shot_ & & & & & & \\ GPT-3 & **30.5** & **17.7** & **32.2** & **26.1** & **30.1** & **12.9** & **24.9** \\ DeBERTa & 25.1 & 6.6 & 24.5 & 10.8 & 18.9 & 4.1 & 15.0 \\   

Table 3: **Machine translation results** We report SacreBLEU scores (Post, 2018) with signature BLEU+case.mixed+numrefs.l+smooth.exp+tok.intl+version.l.2.20 (higher is better). The table shows the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced.

### Closed-book question answering and commonsense reasoning

An important quality of modern-day large language models is their ability to learn and retrieve world knowledge, and to have a degree of common sense. The final group of tasks attempts to evaluate these two qualities.

This category of tasks consists of seven datasets in total: Natural Questions (NQs; Kwiatkowski et al., 2019) and Web Questions (WebQs; Berant et al., 2013) are closed-book question-answering datasets sourced from natural web queries; while the original datasets are accompanied by relevant articles that contain the answer, we only ask models a question and then evaluate the exact-match accuracy of their answers. TriviaQA is a very similar dataset, but based on online quizzes (Joshi et al., 2017). The next four tasks fall more into a subcategory of commonsense reasoning datasets. The Physical Interaction: Question Answering dataset evaluates how well a language model is grounded in the real physical world (PIQA; Bisk et al., 2020). The AI2 Reasoning Challenge is a dataset sourced from grade-school science questions that evaluates knowledge and reasoning abilities; this task is divided into ARC-Easy and ARC-Challenge splits, based on their difficulty (Clark et al., 2018). Finally, OpenBookQA evaluates the understanding of common knowledge (Mihaylov et al., 2018).

ResultsThe question-answering performance is given in Table 4. Apparently, the results of DeBERTa are substantially worse on closed-book question answering compared to GPT-3. We believe that this highlights a more general disadvantage of the MLM training objective - the model can often retrieve world knowledge from the rich bidirectional context during training, not needing to store it in its learned weights; similar effect has been shown in retrieval-augmented language models (Samuel et al., 2024). However, the commonsense reasoning abilities are comparable between the two models. The scaling behavior is again similar between the two models (Figure 1). The same is also true about the improvement when given more in-context examples, which are especially important for the tasks evaluated with exact-match accuracy, where the goal is not only to answer correctly but also to match the expected style and form of the gold answers (Figure 4).

## 5 Related work

Few-shot finetuning with masked language modelsWhile our work demonstrates the emergence of in-context learning in masked language models, prior research has explored different approaches to few-shot learning with these architectures. The dominant paradigm has been few-shot _finetuning_, where the model's weights are updated using a small number of examples. Studies by Schick and Schutze (2021), Gao et al. (2021), and Xia et al. (2022) showed promising results with this approach. However, these methods require additional training steps with a complicated training objective,

    & **NQs** & **TriviaQA** & **WebQs** & **PIQA** & **ARC-C** & **ARC-E** & **Open-BookQA** & **Average** \\  _0-shot_ & & & & & & & & **0-shot** & **Average** \\ GPT-3 & **4.4** & **19.7** & **4.6** & **75.1** & 35.5 & 53.8 & **46.8** & **34.4** \\ DeBERTa & 0.8 & 6.9 & 1.5 & 72.9 & **36.5** & **55.1** & 45.8 & 31.4 \\ _1-shot_ & & & & & & & & \\ GPT-3 & **5.4** & **26.5** & **9.2** & **74.4** & 36.4 & **55.9** & **46.4** & **36.3** \\ DeBERTa & 2.6 & 14.3 & 5.1 & 73.0 & **37.1** & 55.1 & 45.7 & 33.3 \\ _few-shot_ & & & & & & & & \\ GPT-3 & **9.7** & **32.1** & **19.6** & 74.3 & 36.7 & **59.1** & **50.6** & **40.3** \\ DeBERTa & 4.4 & 17.9 & 9.9 & **74.5** & **39.6** & 57.7 & 50.4 & 36.3 \\   

Table 4: **Closed-book question answering and commonsense reasoning**making them more complex to implement compared to the simple prompting-based in-context learning demonstrated in our work. Despite the generally lower performance of in-context learning compared to few-shot finetuning (Liu et al., 2022), its simplicity and immediacy have made it the preferred choice in many practical applications.

Other large masked language modelsOur choice of DeBERTa for this study was motivated by its unique combination of size and capability to handle extended context lengths. While larger masked language models exist, such as Megatron BERT with 3.9 billion parameters (unfortunately not publicly available; Shoeybi et al., 2019) and XLM-RoBERTa with 10.7 billion parameters (Goyal et al., 2021), they have limitations that make them less suitable for studying in-context learning. Megatron BERT lacks mechanisms for length generalization, which is crucial for processing long prompts with multiple examples, while XLM-RoBERTa's multilingual nature and restricted sequence length of 512 tokens would confound our analysis. DeBERTa's architecture, particularly its relative positional embeddings, makes it an ideal candidate for exploring how masked language models scale with in-context learning.

Hybrid masked-causal modelsOur empirical findings, particularly the complementary strengths of masked and causal models demonstrated in Section 4, suggest significant potential in combining these approaches. Several architectures have already explored this direction, even if inadvertently: T5 (Raffel et al., 2020), BART (Lewis et al., 2020) and GLM (Du et al., 2022) introduced autoregressive fill-in-the-blank objectives; CM3 developed a causal-mask approach (Aghajanyan et al., 2022); and PrefixLM implemented a partially bidirectional causal model (Dong et al., 2019; Raffel et al., 2020). These efforts align with our observations about the distinct advantages of masked and causal objectives. The recent work by Ding et al. (2024) provides theoretical support for this direction, demonstrating that prefix language models, which combine aspects of both architectures, are particularly well-suited for in-context learning.

## 6 Conclusion

This paper demonstrates that masked language models can be capable in-context learners. We show that these models - often considered deprecated and limited only to finetuning - can match and sometimes even exceed the performance of their causal counterparts in this domain. Our evaluation reveals that masked and causal models exhibit remarkably similar characteristics in terms of overall performance, scaling behavior, and improvements with additional in-context demonstrations. Most notably, we validate these capabilities using DeBERTa without any architectural modifications or additional training. We achieve this through carefully designed inference methods that unlock the model's latent generative abilities.

Our findings point to several promising directions for future research. First, DeBERTa's performance could likely be enhanced through straightforward improvements such as training on larger and more diverse corpora, increasing model scale, and extending the pretraining context length. More fundamentally, the complementary strengths we observed between masked and causal models - where each architecture excels in different tasks - suggest an exciting opportunity to develop hybrid approaches that combine the best of both paradigms. Rather than viewing these as competing architectures, future work might explore how to synthesize their distinct advantages into more capable and versatile language models.

These results argue for a broader reconsideration of how we approach language model architecture and training. The field's recent focus on causal models, while productive, may have prematurely discounted the potential of alternative approaches that are not limited to unidirectional text processing. Our work demonstrates that the path forward likely involves embracing architectural diversity rather than converging on a single dominant paradigm.