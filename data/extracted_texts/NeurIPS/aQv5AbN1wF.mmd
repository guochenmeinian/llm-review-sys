# On Feature Learning in

Structured State Space Models

 Leena Chennuru Vankadara\({}^{ 1}\)&Jin Xu\({}^{ 2}\)&Moritz Haas \({}^{3}\)&Volkan Cevher\({}^{1,4}\)

\({}^{1}\)AGI Foundations, Amazon

\({}^{2}\)University of Oxford

\({}^{3}\)University of Tubingen, Tubingen AI Center,

\({}^{4}\)LIONS, EPFL

\({}^{*}\)\(\) denotes equal contribution. This work was conducted during Jin's, Moritz' and Volkan's time at Amazon. Correspondence to: aaron.jin.xu@gmail.com

###### Abstract

This paper studies the scaling behavior of state-space models (SSMs) and their structured variants, such as Mamba, that have recently arisen in popularity as alternatives to transformer-based neural network architectures. Specifically, we focus on the capability of SSMs to learn features as their network width approaches infinity. Our findings reveal that established scaling rules, such as the Maximal Update Parameterization, fail to support feature learning as these models cannot be represented in the form of Tensor Programs. Additionally, we demonstrate that spectral scaling conditions, shown to be effective for feature learning in a host of other architectures, do not hold the same implications for SSMs. Through a detailed signal propagation analysis in SSMs, both forward and backward, we identify the appropriate scaling necessary for non-trivial feature evolution in the infinite-width limit. Our proposed scaling shows behavior akin to the Maximal Update Parameterization, such as improved stability, better generalization, and transferability of optimal hyper-parameters from small to large scale SSMs.

## 1 Introduction

State-space models (SSMs), such as Mamba (Gu and Dao, 2023), have become popular in deep learning as alternatives to transformers like GPT and BERT series (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; Devlin et al., 2018; Touvron et al., 2023; Chowdhery et al., 2023; Gemini Team et al., 2023). SSMs integrate elements from RNNs, CNNs, and control models, excelling in inference and handling long contexts (Gu et al., 2021; Gupta et al., 2022; Gu et al., 2022; Smith et al., 2022).

The success of foundation models based on transformers and SSMs alike is largely attributed to their scale--both in terms of data and model size. However, this increased scale often introduces challenges, such as precision issues due to instability or the vanishing/exploding gradient problems. Additionally, the sequential nature of state-space models (SSMs) makes them notoriously difficult to train. Therefore, developing a rigorous understanding of how SSMs scale as their dimensions increase and identifying optimal scaling rules is crucial.

In this vein, infinite-width asymptotics, such as Neural tangent kernel (NTK) analyses have been a central tool providing key insights in DL theory. However, a key limitation of the NTK analysis is the lack of _feature learning_ in the infinite width limit (Yang and Littwin, 2023). Feature learning addresses the more realistic training setting where we have unrestricted movement of the neural network parameters (Yang and Littwin, 2023).

Intriguingly, recent work by Yang and Hu (2021) showed that under an expanded space of hyper-parameters, which includes layer-wise scaling of the learning rates, one can find a unique parameterization called Maximal Update pPameterization (\(\)P) that induces non-trivial feature evolutioneven in the infinite width limit for a host of key architectures. The results are obtained via the mathematical framework of Tensor Programs, which provides the foundation to study the effects of parameterizations on the learning dynamics in the limit. To this end, it is natural to ask:

_When do SSMs admit feature learning in the infinite-width limit?_

Our research tackles this question by investigating the behavior of SSMs as network width approaches infinity. We examine how SSMs learn and evolve features in this context and assess the adequacy of recent scaling rules such as \(\)P and spectral scaling conditions.

**Our contributions:**

* We provide a detailed scaling analysis of forward and backward signal propagation in SSMs as the width approaches infinity, identifying that standard scalings lead to unbounded signals.
* We demonstrate theoretically and empirically that popular scaling rules like \(\)P do not yield correct scaling for SSMs due to--as we prove--their non-representability as Tensor Programs (_cf._, Figure 1).
* We derive a unique correction that ensures correctly balanced signals in both the forward and backward passes, stabilizing the training process and enhancing model performance.
* Empirically, we validate that our proposed scaling facilitates hyper-parameter transfer from small-scale to large-scale SSMs, similar to the effects observed in MLPs and transformers.

## 2 On the lack of feature learning in SSMs at infinite width

### Feature learning in sequence models

Sequence models are built by combining sequential layers that transform input sequences into output sequences. A sequence layer can be represented as \(_{1:L}=f_{}(_{1:L})\) where \(_{1:L}\) is a compact notation for sequence \(_{1},,_{L}\), with \(_{l}^{N_{u}}\), \(_{l}^{N_{y}}\). Note that \(f_{}\) can be easily generalized to accept and output multiple sequences as in residual connections and multi-branch architectures (see Appendix B.1). Here \(N_{u}\) and \(N_{y}\) are treated as widths of the sequence model, which can be scaled up to create larger sequence models. To denote the backward pass of a model concisely, let \(\) be the overall training loss, and we write \(=}{}\). Instead of studying the scaling of each element in a vector \(^{N_{u}}\), we study the norm \(\|\|_{2}\). When elements in \(\) are in \((1)\), we have \(\|\|_{2}(})\). Throughout this work, we will use \(\) to represent the asymptotic order in the limit (\(N_{u}\) in this case), but omit the precise notation for readability.

Figure 1: Under our derived scaling \(\)P-SSM, Mamba achieves feature learning in all three SSM layers. In contrast, both Standard Parametrization (SP) and \(\)P (heuristic) lead to instability or vanishing updates for either the latent states \(_{l}^{(i)}\) or the output signal \(_{l}\) or both in each SSM layer. The figures above illustrate the scalings when \(l=1\), but they exhibit the same trend across recurrence steps. We simultaneously scale up \(N_{x}\) and \(N_{u}\). We run each experiment 10 times, and the shaded areas indicate the standard deviation of these runs. Both Zero-Order Hold (ZOH) and Euler discretization of \(_{l}\) are studied and indicated in the subtitle.

In layerwise sequence models, where the \(k\)-th layer is denoted as \(_{1:n}^{(k)}=f_{k}(_{1:n}^{(k-1)})\) with \(_{l}^{(k)}^{N_{k}}\), we have the following definition for feature learning:

**Definition 2.1** (Feature Learning in Layerwise Sequence Models).: A layerwise sequence model is in the _feature learning regime_ if for any \(k[K]\), the features \(_{1:L}^{(k)}\) and the updates of features \(_{1:L}^{(k)}\) after one gradient update have the following scaling:

\[\ l[L],\ \|_{l}^{(k)}\|_{2}(})\ \ \] (INIT ) \[\ l[L],\ \|_{l}^{(k)}\|_{2}( })\ \ \] ( \[\] )

Note that the first condition is a stability condition that demands the activations to remain coordinate-wise \((1)\) in the forward pass, whereas the second condition ensures nonvanishing and nonexploding activation updates. This definition can be easily generalized to general cases for arbitrary sequence models in Appendix B.1.

For a particular sequence layer in the model, if we assume the inputs to the layer are asymptotically independent and identically distributed (i.i.d), correctly scaled, and the gradients backpropagated into the sequence layer have the correct scaling (as in Assumption 3.1), and this sequence layer satisfies the two conditions (INIT) and (\(\)), we say this sequence layer admits feature learning. The corrected scaling \( P\)-SSM we propose in Section 3 satisfies an even stronger condition: The updates of _all trainable weights_ should have nonvanishing and nonexploding effect on the activation and output function updates. We then say that the sequence model is _effectively feature learning_. This is similar in spirit to the requirements for \( P\) from Yang and Hu (2021) that ensure maximal stable updates of all trainable weights in standard architectures like MLPs, but requires different initialization and learning rate scaling rules for the trainable weights in SSMs, as we show in Section 3.

### Tensor Programs, spectral scaling, and the Maximal Update Parameterization

**Tensor Programs.** The framework of Tensor Programs (TP) (Yang, 2019) was initially developed to understand the behavior of wide neural networks both at initialization and during training. While there are many versions of Tensor Programs, the most general version (that subsumes all the previous versions) is referred to as the Ne\(\)OR\(\) program (Yang and Littwin, 2023). A Ne\(\)or\(\) program constitutes a sequence of vectors in \(^{n}\) and a sequence of scalars in \(\) inductively generated from an initial set of random scalars, vectors, and matrices following a specified set of instructions: matrix multiplications, non-linear outer products, and vector averages. Yang and Littwin (2023) show that for any architecture whose forward pass can be represented as a Ne\(\)OR\(\) program (which includes many modern architectures used in practice including transformers and convolutional networks), there exists a unique scaling rule called Maximal Update Parametrization (\(\)P) under which features in every single layer evolve in a width-independent fashion with scale. Interestingly, it has been shown that unlike standard parameterizations, \(\)P allows transferability of optimal hyper-parameters from small to large scale models.

The key idea of the "Tensor Program Ansatz" (Yang, 2019) is that pre-activations and their gradients arising in training most neural network architectures via standard update rules (such as SGD and ADAM) can be represented as vectors in a Ne\(\)OR\(\) program. The Ansatz suggests that vectors in a TP become asymptotically independent as well as identically distributed both at initialization as well as during training. Leveraging this, the framework allows for a mechanistic tracking of the behaviour of such vectors by assigning a random variable \(Z^{v}\) to represent the (asymptotically identical) distribution of the coordinates of the vector \(v\). This principle underlies the development of the key theoretical result of the Tensor Program machinery -- the Master Theorem -- which can be viewed as the compositional, non-linear generalization of the law of large numbers. It allows the theoretical computation of infinite width limits of different quantities such as (pre-)activations or the output of a neural network.

While the following proposition also holds for other structured SSMs such as MAMBA, for clarity, we first introduce the simpler S4 model.

**The S4 recurrent layer.** The S4 recurrent layer \(_{1:L}=f_{}(_{1:L};=\{, \})\) can be viewed as a discretization of a continuous-time SSM given by

\[_{t}=_{t}+_{t} _{t}=[_{t}]\] (1)for \(l=1,,L\) with \(_{0}=0\) and where \(()\) gives the real part of a complex vector, and where we let \(=()\) with \(^{N_{x}}\), \(^{N_{y} N_{u}}\), \(^{N_{y} N_{x}}\). We have the discretized sequence to sequence mapping \(f_{}\) as follows:

\[_{l}=^{}_{l-1}+^{} _{l}_{l}=[^{ }_{l}]\] (2)

where we follow the Zero-Order-Hold (ZOH) discretization method,

\[^{}=(),^{}=( ^{}-)^{-1}, ^{}=.\] (3)

In practice, simple Euler discretization can also be used for \(\), i.e., \(^{}=\). To model long-range dependency, S4 advocates for HiPPO theory (Gu et al., 2020). There are many possible Hippo-based initializations possible for the structured transition matrices \(\) (e.g., Hippo-Leg-S, S4D-Inv, and S4D-Real) and our result holds for all the parameterizations.

While the training of most common architectures including transformers or convolutional networks via update rules such as SGD or ADAM are representable as a \(\) program, Proposition 2.2 shows that this does not hold in general for structured state space models.

**Proposition 2.2** (**Structured SSMs are not generally representable as \(\) programs**).: _The forward and backward signal propagation of structured SSMs including S4 (2) and (3) and MAMBA (Section 3.1) trained via standard algorithms such as SGD or ADAM are not representable as a \(\) program. Indeed this holds for all existing Hippo-parameterizations of the structured matrix \(\) including HiPPO-LegS, HiPPO-LegS-N, HiPPO-LegS-D, S4D-Inv, S4D-Lin, and S4D-Real._

_Proof sketch._ The formal proofs are provided in Appendix C. The key architectural component that is not representable in a \(\) program is the Hippo-based structured transition matrix \(\). Due to the rapid decay of the diagonal entries of \(^{-1}\), the hidden states \(_{0}\) are not even asymptotically identically distributed, already at initialization. Therefore coordinates of \(_{0}\) cannot be generated by a sequence of \(\) computations, neither at initialization nor over the course of training. As we will discuss in Section 6, developing a TP-like framework to cover SSMs requires a substantial generalization of the existing TP framework and is beyond the scope of the current paper. Nevertheless, in Section 6, we outline the key challenges and potential paths toward such a generalization.

**Spectral scaling condition.**Yang et al. (2023) showed that many practical architectures admit feature learning if certain spectral scaling conditions are satisfied. Let \(_{l}\) and \(_{l}^{N_{l} N_{l-1}}\) denote the \(l\)th layer weight matrices and their gradient updates. The condition states that for feature learning to hold, the spectral norms of the matrices should satisfy:

\[\|_{l}\|_{*}(}{N_{l-1}}} )\|_{1}\|_{*} (}{N_{l-1}}})l[L].\] ( \[*\] )

Under \(\)P, architectures that are representable as a \(\) program satisfy the spectral scaling condition (Yang et al., 2023). Here, we show that for structured SSMs, architectures that satisfy spectral scaling conditions do not in general satisfy conditions for feature learning.

**Proposition 2.3** (**Spectral scaling does not generally imply feature learning in SSMs**).: _Structured SSMs including S4 and MAMBA trained via standard algorithms such as SGD or ADAM that satisfy spectral scaling conditions (\(*\)) do not satisfy conditions for feature learning given in condition (\(\)). Indeed this holds for all well-known Hippo-parameterizations of the structured matrix \(\) including HiPPO-LegS, HiPPO-LegS-N, HiPPO-LegS-D, S4D-Inv, S4D-Lin, and S4D-Real._

_Proof sketch._ The formal proofs are provided in Appendix C. To gather some intuition for this result in a simplified setting, consider the scale of the hidden states for token index \(0\), which is already wrong at initialization. By definition, \(\|_{1}\|_{2}=\|^{}_{1} \|_{2}=\|_{1}\|_{2},\) where \(=(^{}-)^{-1}\). Now, observe that \(\|^{}_{1}\|_{2}^{2}\) is a sum of independent random variables that satisfy the Kolmogorov condition, so that the sum behaves according to the strong law of large numbers. For spectral scaling conditions to yield the right scaling of the initialization variance, it is crucial that the following condition holds:

\[\|^{}_{1}\|_{2}(\| ^{}\|_{*}\|_{1}\|_{2}).\] (4)

However, using standard tools from random matrix theory one can show that \(^{}\|_{2}}{(\| ^{}_{1}\|_{2})}(}})\) which clearly violates (4).

## 3 Identifying the unique scaling for effective feature learning in SSMs

In this section, we analyze the forward and backward signal propagation in structured SSMs. Due to the generality of the architecture, we consider MAMBA as the basis for the analysis. Similar arguments apply for other SSMs such as S4, S5, H3, or DSS. In the appendix, we also provide a detailed analysis of signal propagation in S4 and identify the correct scaling conditions for maximal stable weight updates.

### Selective State Space Models

Selective SSMs in the Mamba architecture \(_{1:L}=f_{}(_{1:L})\) can be written as follows:

\[_{l} =_{N_{x}}(_{l};_{B},_{ B})^{N_{x}},\] (5) \[_{l} =_{N_{y}}(_{l};_{C},_{ C})^{N_{x}},\] (6) \[_{l} =(_{0}+_{N_{u}}(_{1}(_{l};_{},_{})),\;_{0},_{l} ^{N_{u}}.\] (7)

For \(i=1,,N_{u}\), we have \(N_{u}\) 1D SSMs:

\[^{(i)} =(-(_{}^{(i)})),\;_{ }^{(i)}^{N_{x}},\;\;\;_{l}^{(i)}=_{l}^{ (i)}_{l-1}^{(i)}+u_{l}^{(i)}_{l}^{(i)},\;u_{l} ^{(i)},\;_{l}^{(i)}^{N_{x}},\] \[_{l}^{(i)},_{l}^{(i)} =(_{l}^{(i)},^{(i)},_{l}),\; _{l}^{(i)},\;\;\;\;\;\;y_{l}^{(i)}=_{l}{}^{ }_{l}^{(i)},\;y_{l}^{(i)}.\]

where \(_{0}^{(i)}=\), \(_{}^{(i)}\), \(_{0}^{N_{u}}\), and all linear layer weights \(_{B},_{B},_{C},_{C},_{}, _{}\) are trainable parameters. The Zero-Order-Hold (ZOH) discretization procedure \(_{l}^{(i)},_{l}^{(i)}=(, ^{(i)},_{l}^{(i)})\) can be written as:

\[_{l}^{(i)}=(_{l}^{(i)}^{(i)}), _{l}^{(i)}=(_{l}^{(i)}-)^{ (i)}{}^{-1}_{l}^{(i)}.\] (8)

In a nutshell, Mamba can be seen as \(N_{u}\) SSMs, one for each input channel. Weights for these SSMs are shared and depend on the input at that recurrent step. Because of the dependency on inputs, Mamba can model non-stationary sequences. Weight matrices are initialized as follows:

\[_{B}(,_{B}^{2})^{N_{x} N_{u}},\;\;_{C}(,_{C}^{2} )^{N_{x} N_{u}},\;\;_{}(,_{}^{2})^{1 N_{u}},\]

and zero initialization for all biases. For each SSM, \(^{(i)}\) is still initialized according to the HiPPO theory. When \(^{(i)}\) is real-valued, we let \(_{}^{(i)}[j]=(j+1)\) and \(_{0}\) can be seen as a bias term initialized to \(_{0}^{-1}((0.001,0.1))\). We will assume that \(_{0}\) is not trained, as this does not have any effect on the scale of the different quantities under consideration. This is a minor technical assumption and our results would also hold if we considered \(\) as a trainable parameter.

    & ZOH Discretization \(\) Euler Discretization & \(\)P-SSM (Ours) \\  \(_{B}\) & \(}}\) & \(}}\{1,}{N_{u}}}\}\) & \(}{N_{u}}}\) \(}}\) \\ \(_{C}\) & \(}}\) & \(}}\{1,}{N_{u}}}\}\) & \(}N_{u}}\) \\ \(_{a}\) & 1 & \(}{N_{x}}}\) & \(N_{u}\) \(}N_{u}\) \\ \(_{B}\) & 1 & \(}{N_{u}}\) & \(}{}}\) \(}{N_{u}}}\) \\ \(_{C}\) & 1 & \(}{N_{u}}\) & \(}{}}\) \\  \(\|_{l}^{(i)}\|_{2}\) & \((1)\)\(\)\((})\) & \((\{1,}{N_{u}}}\})\)\(\)\((}\{1,}{N_{u}}}\})\) & \((})\) \\ \(\|_{l}\|_{2}\) & \((})\)\(\)\((N_{u}})\) & \((}\{1,}{N_{u}}\})\)\((N_{u}}\{1,}{N_{u}}\})\) & \((})\) \\ \(\|_{l}^{(i)}\|_{2}\) & \((})\)\(\)\((N_{u}})\)\((}{}}\{1,}{N_{u}}}\})\)\((}}{}}\{1,}{N_{u}}}\})\) & \((})\) \\ \(\|_{l}\|_{2}\) & \((N_{u})\)\((}N_{u})\)\((}{}}\{1,}{N_{u}}\})\)\((}}{}}\{1,}{N_{u}}\})\) & \((}}{}}\{1,}{N_{u}}\})\) & \((\) and \(_{C}\), we begin by analyzing the scale of activations (i.e., hidden states and outputs) in a S6 Mamba layer in the first forward pass as \(N_{u}\) then \(N_{x}\). We believe that our results would also hold in the proportional limit where \(N_{u}\) and \(N_{x}\) with \(}{N_{u}}(1)\). However deriving the results in this setting would incur a significant technical overhead and we defer this analysis to a future work. We briefly discuss this in Section 6. As is common in practice, we assume that the SSM layer is embedded into a neural network architecture containing standard architectural blocks such as MLPs, normalization layers, or residual connections. All proofs are provided in Appendix C.2.

**Assumption 3.1**.: _Assume that the forward pass of all the components of the network except the SSM layer are expressible as \(\) programs and are parameterized according to \(\)P._

This assumption ensures that the inputs to the SSM layer are asymptotically i.i.d and correctly scaled. It also ensures that gradients into the SSM layer have the correct scaling. All results in this section are stated under Assumption 3.1. Through the forward signal propagation analysis, we identify the correct scale of initialization for weight matrices \(_{B}\) and \(_{C}\). We show that both standard parameterization as well as spectral scaling conditions do not yield the correct scale of initialization for the weight matrices. The key results are summarized in Table 1.

For simplicity of exposition, first consider the scale of \(_{1}^{(i)}=(_{l}^{(i)}-) ^{(i)}{}^{-1}_{1}u_{1}^{(i)}\) before generalizing to arbitrary \(l[L]\).

**Proposition 3.2** (**Scale of hidden states \(_{1}^{(i)}\) in Mamba at initialization**).: _Under the ZOH discretization procedure, as \(N_{u}\) then \(N_{x}\) approach infinity, for any \(i[N_{u}]\), the squared \(l_{2}\)-norm of the hidden states \(\|_{1}^{(i)}\|_{2}^{2}\) is a.s. scaled as \(\|_{1}^{(i)}\|_{2}^{2}((2)\,_{B}^{2}\,\| _{1}\|^{2}\,(u_{1}^{(i)})^{2})\), where \((2)\) denotes the Riemann zeta function at 2._

Following condition (INIT), the 1D SSM admits stability at initialization under the following conditions:

\[|u_{l}^{(i)}|(1)\|_{l}^{(i)}\| (})|y_{l}^{(i)}|(1).\]

Figure 2: **Illustration of the Mamba S6 Layer**. The computation is modularized into three components: selection, discretization, and per-channel linear recurrence. Mamba introduces a selection mechanism where weight matrices \(_{l}\), \(_{l}\) depend on the inputs \(_{l}\). These weight matrices are then separated into per-channel parameters, and discretized using either the ZOH or Euler methods. The discretized, per-channel weights are then applied in a linear recurrence, allowing each channel to perform computations in parallel. Trainable parameters are shown in blue.

Therefore, for stability at initialization, the initialization should scale as \(_{B}(}{N_{u}}})\). Note that, under both standard parameterization (e.g., Kaiming or LeCun initialization) and spectral scaling conditions, \(_{B}\) is initialized as \((}})\), which leads to vanishing hidden states according to Proposition 3.2. We empirically verify this fact in Figure 1.

Proposition 3.3 provides the scale of the output of a Mamba layer.

**Proposition 3.3** (**Scale of outputs \(_{1}^{(i)}\) of a Mamba layer at initialization)**.: _Under the ZOH discretization procedure as \(N_{u}\) then \(N_{x}\) approach infinity, for any \(i[N_{u}]\), the output \(y_{1}^{(i)}\) converges in distribution to a Gaussian with mean \(0\) and standard deviation \(C_{B}_{C}\|_{1}\|_{2}^{2}\) for some width-independent constant \(C>0\)._

Accordingly, imposing the conditions for stability of \(y_{1}^{(i)}\) according to condition (INIT) implies the initialization scaling condition \(_{C}(N_{u}}})\). Note again that standard parameterization suggests initializing \(_{C}(}})\) under which the outputs would diverge with scale. Under spectral scaling, \(_{C}\) is initialized much larger. However, since the hidden states vanish with width, the outputs of the SSM admit the correct scaling here as demonstrated in Figure 1.

**Generalizing to arbitrary \(l[L]\).** The corrected scalings of \(_{B}\) and \(_{C}\) derived above generalize to the entire sequence, as a sum over the sequence usually does not cancel out the scaling. More formally, for all \(l[L]\), we have

\[_{l}^{(i)}=_{m=0}^{l-1}(_{l}^{(i)})^{m}_{l-m}^{(i)}u_{l-m}^{(i)}.\]

First, observe that the operator \((_{l}^{(i)})^{m}\) does not change the width-scaling. To see this note that since \(_{l}^{(i)}=(a_{1}^{},,a_{N_{x}}^{ })\) with \(a_{n}^{}=e^{-_{l}^{(i)}}((_{l}^{(i)} n)+i (_{l}^{(i)} n))\), we have that, for all complex vectors \(^{N_{x}}\) it holds that \(\|(_{l}^{(i)})^{m}\|_{2}=e^{-m_{l}^{ (i)}/2}\|\|_{2}\) for any \(m[L]\). Now since, for any \(_{l}\), setting \(_{B}(}{N_{u}}})\) yields \(\|_{l}^{(i)}_{l}\|_{2}(})\), each term in the summation is of order \((})\). Unless, for every \(l\), the term \(_{l}^{(i)}_{l}\) perfectly cancels out with the terms before to affect the width scaling, we have that \(\|_{l}^{(i)}\|_{2}(})\). The same argument can be used to show the stability of \(y_{l}^{(i)}\). Concluding this argument, we have derived the correct scaling of the initialization variances \(_{B}\) and \(_{C}\) for feature stability in a Mamba layer summarized below.

**Conditions for stability of a S6 Mamba layer at initialization.** The features of a S6 Mamba recurrent layer \(_{1:L}=f_{}(_{1:L};)\) are stable at initialization in the infinite-width limit under the following scaling conditions:

\[_{B}(}{N_{u}}}) _{C}(N_{u}}})\]

### Backward signal propagation in a S6 Mamba Layer

In this section, we provide the correct scaling of the learning rates \(_{a}\), \(_{B}\), and \(_{C}\) by a detailed analysis of the backward signal propagation in the limit of \(N_{u}\) then \(N_{x}\). Specifically, we analyze the scale of the activation updates for both hidden states and outputs in the first backward pass through a Mamba layer. We also show that both standard parameterization as well as spectral scaling conditions do not yield the correct scale of learning rates for the weight matrices. The key results are summarized in Table 1.

**Proposition 3.4** (**Scale of the updates of hidden states \(_{1}^{(i)}\) after 1 step of SGD)**.: _Under the ZOH discretization procedure, as \(N_{x}\) then \(N_{u}\) approach infinity, for every 1D SSM, the squared \(l_{2}\)-norm of the updates \(\|_{1}^{(i)}\|_{2}\) of the hidden states after one step of SGD is a.s. scaled as \(\|_{1}^{(i)}\|_{2}(_{B}}} _{C}\|_{1}\|_{2}^{3}(4)^{})\), where \((4)\) denotes the Riemann zeta function at 4._The Riemann zeta function at 4 evaluates to a width-independent constant and \(_{C}(N_{u}}})\) due to Proposition 3.3 for stability of outputs. Therefore, for the scale of the hidden state updates to be \((})\), the correct scaling of the learning rate \(_{B}\) is given by \((}{}})\). On the other hand, spectral scaling suggests that \(_{B}\) must scale as \((}{N_{u}})\). Under this scaling, however, updates of the hidden states would vanish with width and therefore the first block of the SSM is in the _lazy regime_. This is corroborated by our experiments in Figure 1.

Next, to derive the correct scaling of the learning rate \(_{C}\), we consider the scale of output updates.

**Proposition 3.5** (**Scale of the updates of outputs \(_{1}^{(i)}\) after 1 step of SGD)**.: _Under the ZOH discretization procedure, as \(N_{u}\) then \(N_{x}\) approach infinity, for every 1D SSM, the squared \(l_{2}\)-norm of the updates of the hidden states after one step of SGD scales as \(| y_{1}^{(i)}|^{2}(_{C}_{B}^{2}}}|_{1}|_{2}^{4})\)._

The result suggests that for the correct scaling of the updates, the learning rate \(_{C}\) must scale as \((}})\). Under spectral scaling, \(_{C}\) scales much larger as \((}{N_{u}})\). However, since the updates of the hidden states vanish under spectral scaling, this larger incorrect scaling of \(_{C}\) downward corrects the scale of the updates in the outputs as shown in Table 1 and empirically verified in Figure 1.

**On the correct scaling of \(_{a}\).** It turns out that the scaling of the learning rate \(_{a}\) does not play a role in either stability at initialization or for non-trivial updates with scale. However, if \(_{a}\) is not scaled correctly, then the transition matrix \(\) is not updated. In particular, as shown in Appendix C.2, \(_{a}\) needs to scale as \((N_{u})\). Below we summarize the correct scaling conditions to achieve non-trivial feature updates in the infinite-width limit of a Mamba layer.

In the same vein as the discussion in Section 3.2, it is straightforward to verify that our results hold for arbitrary \(l[L]\) and more gradient steps as soon as we assume that the updates of the weight matrices and activations do not perfectly cancel out the corresponding initial quantities.

**Conditions for non-trivial feature updates in a S6 Mamba Layer.** The updates in a S6 Mamba recurrent layer \(_{1:L}=f_{}(_{1:L};)\) evolve non-trivially in the infinite-width limit under the following conditions:

\[_{B}(}{N_{u}}}),\ \ _{C}(N_{u}}}),\ \ _{a}(N_{u}),\ \ _{B}(}{}}),\ \ _{C} (}})\]

## 4 \(\)P-SSM implies stability and feature learning in Mamba

**Empirical verification of different scalings in Table 1.** First, we verify that our derived \(\)P-SSM scaling (see Table 1) for Mamba indeed leads to feature learning, i.e., it ensures stability at initialization as defined in condition (INIT) and non-trivial feature updates during training as defined in condition (\(\)). We scale up the SSM latent state size \(N_{x}\) and the SSM output dimension \(N_{u}\) simultaneously and track the scaling of both features and feature updates. Due to the linear decay in the eigenvalues of the transition matrix \(^{-1}\), we typically observe a strong finite sample effect at small \(N_{x}\). Constrained by computational resources, we opt for a much smaller \(N_{u}\) (\(N_{u}=N_{x}/8\)) than is usually employed in practice. This adjustment enables us to scale up \(N_{x}\) effectively, thus mitigating the finite-sample effect and to more clearly demonstrate the scaling behavior in the asymptotic limit in Figure 1. For all experiments in this section, we train Mamba with 3 SSM blocks for language modelling on the wikitext dataset (Merity et al., 2016) and use plain Stochastic Gradient Descent (SGD) to perform gradient updates. We use the huggingface (Wolf et al., 2019) Mamba implementation and the \(\)P package (Yang et al., 2022) for scaling in our experiments.

As shown in Figure 1, under Standard Parametrization (SP), both the SSM latent states and the outputs explode at initialization, and their updates also explode, leading to instability both at initialization and during training. Under the spectral scaling parameterization prescribed in Yang et al. (2023), other layers except for the SSM layers have the correct scaling by design. However, in the SSM layer, when using Zero-Order Hold (ZOH) discretization for \(_{l}\), the latent states at initialization and their updates vanish when scaling up the width, while the output signals still have the right scaling as predicted by theory. On the other hand, when Euler discretization is used for \(_{l}\), the latent states will have the correct scaling, but the output signals will explode. The results clearly highlight the importance of correcting the scaling of the \(\)P parameterization. When using the corrected scaling which we call \(\)P-SSM, both the latent states and the output signals have the right scaling at initialization, and their updates have the same correct scaling. This holds true for both discretization schemes. Note that the slight shift in scaling when the width is small is due to finite sample effects. It stabilizes once the width is sufficiently large.

**Stability, generalization, and hyper-parameter transfer.** In Figure 3, we employ Mamba as a generative model on the wikitext-103 dataset and conduct single-epoch training for \(20K\) iterations. We plot the test loss against the learning rate on a logarithmic scale and compare the results across different model widths (both \(N_{u}\) and \(N_{x}\)). In this experiment, we use the standard setting where \(N_{u} N_{x}\) (\(N_{u}=16N_{x}\) in this case). Using \(\)P-SSM scaling or \(\)P (heuristic) significantly improves test performance compared to standard parameterization of Mamba for this task. For larger learning rate, \(\)P-SSM shows better stability compared to \(\)P (heuristic), highlighting the importance of deriving the correct scaling for SSMs rather than heuristically adopting \(\)P (heuristic) without investigation. Furthermore, previously unreported, we observe stable HP transfer from small to large widths and monotonically improving performance with increasing model widths in structured SSMs. In contrast, we observe completely non-monotonic behavior under standard scaling of SSMs.

Note that optimal learning rate also appears to transfer under spectral scaling. The reasoning behind why optimal hyper-parameters transfer across scales is not completely understood. For instance, the optimal learning rate has been empirically shown to transfer across depth in transformers under appropriate depth-dependent scaling of the residual branches (Bordelon et al., 2023). However, from a theoretical standpoint, layers within each residual block of a transformer are in the lazy regime in the limit (Yang et al., 2023b). This is very similar to Mamba under spectral scaling. Under the ZOH discretization, the first block of the SSM is in the lazy regime but the outputs themselves are updated non-trivially. This suggests that a more thorough understanding of both necessary and sufficient conditions of the transferability of hyper-parameters is warranted.

## 5 Related work

**Signal propagation.** Our work can be seen as scaling theory or signal propagation theory with the goal of preventing both vanishing and exploding signals in forward and backward passes. In this sense, we build on a rich literature, often restricted to an analysis at or close to initialization (Schoenholz et al., 2016; Poole et al., 2016; Hanin and Rolnick, 2018; Xiao et al., 2020). Towards understanding infinite-width limits of neural networks, kernel-based approaches (Neal, 1996; Jacot et al., 2018) and applications of mean-field theory (Mei et al., 2018) have yielded valuable insights.

**Tensor Programs.** Most promisingly, the Tensor Programs framework (Yang, 2019; Yang and Hu, 2021; Yang and Littwin, 2023; Yang et al., 2022, 2023b) covers many modern deep learning architectures, optimization algorithms and arbitrary \(abc\)-parameterizations. Each \(abc\)-parameterization is essentially defined by a layerwise scaling of initialization variance and learning rate as a function of

Figure 3: Test loss against learning rate on Mamba with varying widths (\(N_{u}\) and \(N_{x}\)). Using \(\)P-SSM scaling leads to substantially improved test performance compared to the SP scaling. Compared to \(\)P (heuristic), \(\)P-SSM scaling provides greater stability when utilizing large learning rates. Notably, we observe stable learning rate transfer from small to large model widths. Performance improves monotonically across widths in structured SSMs under \(\)P-SSM scaling, as opposed to standard scaling where performance actually drops with scale after a certain width.

network width. Seminal work by Yang and Hu (2021) shows that there exists a unique maximal update parameterization (\(\)P) that attains a stable feature learning infinite-width limit. This parameterization has since been shown to be a good model for understanding the properties of large models (Vyas et al., 2024), and has been extended to infinite width and depth limits of ResNets (Hayou et al., 2021; Li et al., 2021; Bordelon et al., 2023; Yang et al., 2023b) and Transformers (Noci et al., 2022, 2024).

**Structured SSMs.** Our analysis focuses on structured state space models (SSMs). The S4 model (Gu et al., 2021) is inspired by continuous-time linear SSMs, which are well-studied in control systems, and its specific initialization is motivated by the HiPPO theory (Gu et al., 2020). S4 and its variants e.g. DSS(Gu et al., 2022), S4D(Gupta et al., 2022), S5(Smith et al., 2022), etc., demonstrate impressive long-range dependency and overcome the quadratic computational cost of transformer models (Vaswani et al., 2017) w.r.t. sequence length. However, these models are less effective at modeling text, or even perform simple tasks such as selective copying (Gu and Dao, 2023). Mamba (Gu and Dao, 2023) is proposed to address such issues with selection mechanism. This line of work has also inspired revisiting Recurrent Neural Networks (RNNs) (Orvieto et al., 2023; De et al., 2024; Beck et al., 2024), leading to the growing interest in RNN-based sequence models.

## 6 Discussion

In this work, we study the scaling behavior of forward and backward signal propagation in structured state space models - a promising class of recent architectures. We show that existing scaling rules such as standard parameterization, \( P\), or spectral scaling conditions do not yield desirable properties such as feature learning in SSMs at scale. Through our analysis, we propose the correct scaling of state space models under which we empirically observe feature learning and transferability of hyper-parameters from small to large scale models.

**On Generalizing Tensor Programs.** While our proposed scaling has been derived by a thorough analysis of signal propagation in SSM layers, our results are still limited to the \(N_{u}\) then \(N_{x}\) tend to infinity setting. A completely rigorous analysis of SSMs in the proportional limit where \(N_{u}\) and \(N_{x}\) approach infinity with \(}{N_{u}}\) held roughly constant requires us to carefully track how the different activations and the updates are correlated with each other. For most standard architectures, the Tensor Program (TP) machinery provides the appropriate tools to do precisely this. Therefore, it is of considerable interest to generalize the TP framework. The key assumption that requires relaxation is that the different vectors in a TP (such as activations or updates) are asymptotically identically distributed. As discussed earlier, this assumption crucially underlies the key theoretical results of TP called Master Theorems. However, a potential path toward generalizing TP may be found by noting that the Master Theorems can be viewed as a non-linear compositional form of the law of large numbers or central limit theorem. Accordingly, it may be possible to relax the assumption of being identically distributed in the limit and instead ask that the entries of the vectors in a TP asymptotically satisfy weaker conditions such as Lindenberg or Kolmogorov conditions. Note however, that any such generalization is highly technical and is beyond the scope of the current work.