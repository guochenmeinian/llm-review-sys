# In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before an Ongoing Trajectory Terminates

In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before an Ongoing Trajectory Terminates

 Shicheng Liu & Minghui Zhu

School of Electrical Engineering and Computer Science

Pennsylvan State University

University Park, PA 16802, USA

{sf15539,muz16}@psu.edu

###### Abstract

Inverse reinforcement learning (IRL) aims to learn a reward function and a corresponding policy that best fit the demonstrated trajectories of an expert. However, current IRL works cannot learn incrementally from an ongoing trajectory because they have to wait to collect at least one complete trajectory to learn. To bridge the gap, this paper considers the problem of learning a reward function and a corresponding policy while observing the initial state-action pair of an ongoing trajectory and keeping updating the learned reward and policy when new state-action pairs of the ongoing trajectory are observed. We formulate this problem as an online bi-level optimization problem where the upper level dynamically adjusts the learned reward according to the newly observed state-action pairs with the help of a meta-regularization term, and the lower level learns the corresponding policy. We propose a novel algorithm to solve this problem and guarantee that the algorithm achieves sub-linear local regret \(O(+ T+ T)\). If the reward function is linear, we prove that the proposed algorithm achieves sub-linear regret \(O( T)\). Experiments are used to validate the proposed algorithm.

## 1 Introduction

Inverse reinforcement learning (IRL) aims to learn a reward function and a corresponding policy that are consistent with the demonstrated trajectories of an expert. In recent years, several IRL methods are proposed to help learn the reward and policy, including maximum margin methods [1; 2], maximum entropy methods [3; 4], maximum likelihood methods [5; 6], and Bayesian methods [7; 8].

The aforementioned IRL works learn from pre-collected demonstration sets and do not improve the learned model during deployment. Online IRL [9; 10; 11] instead can learn from sequentially arrived demonstrated trajectories and continuously improve the learned reward and policy from the newly observed complete trajectories. However, recent applications of IRL motivate the need to learn incrementally from an ongoing trajectory before it terminates. For example, inferring a moving shooter's intention from its ongoing movement in order to evacuate the hiding victims  before the shooter finds them. In this case, we need to quickly update the inference about the shooter's intention once a new movement of the shooter is observed, so that we can use the latest inference to plan a rescue strategy as soon as possible. We cannot wait until the shooter trajectory ends, in case the shooter has found the victims. Another example is learning a target customer's investment preference from its daily updated investment trajectory in a stock market  in order to recommend appropriate stocks [14; 15] before other competitors get this customer. However, current IRL works cannot learn from an ongoing trajectory because they have to wait to collect at least one complete trajectory to learn from. To bridge the gap, this paper proposes in-trajectory IRL, a new type of IRL that learns a 

[MISSING_PAGE_EMPTY:2]

Note that \(L_{t}(;(S^{E}_{t},A^{E}_{t}))\) is defined using \(_{}\) and \(\) is the parameter of the reward function \(r_{}\), here the policy \(_{}\) is also parameterized by \(\) because it is computed by solving an RL problem (in the lower level) under the reward function \(r_{}\), and thus is indirectly parameterized by \(\). Maximum likelihood IRL (ML-IRL)  has a similar bi-level formulation with (1), however, ML-IRL only solves an offline optimization problem and its analysis does not hold for non i.i.d. input data and continuous state-action space. We discuss our distinctions from ML-IRL in Appendix A.1.

The upper-level loss function \(L_{t}\) has two terms. The first term \(-^{t}_{}(A^{E}_{t}|S^{E}_{t})\) is the discounted negative log-likelihood of the state-action pair \((S^{E}_{t},A^{E}_{t})\) at time \(t\) and the second term \(}{2}||-||^{2}\) is the discounted meta-regularization term  where \(\) is a hyper-parameter. The likelihood function is commonly used in IRL  to learn a reward function. Basically, the upper-level loss function at time \(t\) encourages to find a reward function \(r_{}\) that makes the observed state-action pair \((S^{E}_{t},A^{E}_{t})\) most likely and meanwhile, the reward parameter \(\) should not be too far from the prior experience, i.e., the meta-prior \(\). Note that \(\) is a pre-trained meta-prior that embeds the information of "relevant experience". We will introduce the training of \(\) in Subsection 4.3 and Appendix C.

The lower-level problem is used to compute \(_{}\) using the current reward function \(r_{}\). It proposes to find a policy \(_{}\) that maximizes the entropy-regularized cumulative reward \(J_{}()+H()\). The cumulative reward of a policy \(\) under the reward function \(r_{}\) is \(J_{}() E^{}_{S,A}[_{t=0}^{}^{t}r_{ }(S_{t},A_{t})]\) where the initial state is drawn from \(P_{0}\). The causal entropy of a policy \(\) is defined as \(H() E^{}_{S,A}[-_{t=0}^{}^{t}(A_{t}|S_{ t})]\).

Since the expert demonstrates \(\{(S^{E}_{t},A^{E}_{t})\}_{t 0}\) sequentially, we have a sequence of loss functions \(\{L_{t}(;(S^{E}_{t},A^{E}_{t}))\}_{t 0}\). We use this sequence of loss functions (1) to formulate an online learning problem. A typical online learning problem is to minimize the regret: \(_{t=0}^{T-1}L_{t}(_{t};(S^{E}_{t},A^{E}_{t}))-_{}_{t=0 }^{T-1}L_{t}(;(S^{E}_{t},A^{E}_{t}))\). However, it is too challenging to minimize the regret in our case because the loss function \(L_{t}\) could be non-convex. Therefore, we aim to minimize the local regret which is widely adopted in online non-convex optimization  and online IRL . The local regret quantifies the general stationarity of a sequence of loss functions under the learned parameters. In specific, given a sequence of loss functions \(\{f_{t}(x)\}_{t 0}\), the local regret  at time \(t\) is defined as \(||_{i=0}^{t} f_{i}(x_{t})||^{2}\) which quantifies the gradient norms of the average of all the previous loss functions under the current learned parameter \(x_{t}\). The total local regret is defined as the sum of the local regret at each time \(t\), i.e., \(_{t=0}^{T-1}||_{i=0}^{t} f_{i}(x_{t})||^{2}\). In our case, we replace \(\{f_{t}\}_{t 0}\) with the loss function \(\{L_{t}\}_{t 0}\) defined in (1) and thus formulate the local regret (2)-(3) which has a bi-level formulation. We aim to minimize the following local regret:

\[E_{\{(S^{E}_{t},A^{E}_{t})^{_{E}}_{t}(, )\}_{t 0}}_{t=0}^{T-1}||_{i=0}^{t} L _{i}(_{t};(S^{E}_{i},A^{E}_{i}))||^{2},\] (2) \[_{_{t}}=*{arg\,max}_{}J_ {_{t}}()+H(),\] (3)

where \((S^{E}_{t},A^{E}_{t})^{_{E}}_{t}(,)\) means that \((S^{E}_{t},A^{E}_{t})\) is drawn from the state-action distribution \(^{_{E}}_{t}(,)\), and \(^{}_{t}(,)\) is the state-action distribution induced by \(\) at time \(t\) in the MDP.

**Difficulties of solving problem (2)-(3)**. We want to design a fast algorithm to solve problem (2)-(3) since we need to finish the update of \(\) and \(\) before the next state-action pair is observed and the time between two consecutive state-action pairs can be short. However, designing and analyzing such a fast algorithm is difficult due to the following challenges:

(i) First and foremost, current state-of-the-arts  on online non-convex optimization use follow-the-leader-based algorithms which solve \(_{}_{i=0}^{t}L_{i}(;(S^{E}_{i},A^{E}_{i}))\) to near stationarity at each time \(t\). This is time-consuming because they require multiple gradient descent updates of \(\). One way to alleviate this problem is to use online gradient descent (OGD) which only updates \(\) by one gradient descent step at each time \(t\). However, since OGD does not solve the problem to near stationarity at any time \(t\), it is extremely difficult to quantify the overall stationarity after \(T\) iterations. While OGD has been well studied in online convex optimization, it is rarely studied in online non-convex optimization. The recent work  uses OGD to quantify the local regret, however, its analysis can only hold when the input data is i.i.d. In contrast, the input data in our problem is not i.i.d. In specific, the input data at time \(t\) (i.e., \((S^{E}_{t},A^{E}_{t})\)) is actually affected by the input data at last step (i.e.,\((S_{t-1}^{E},A_{t-1}^{E})\)). This correlation between any two consecutive input data makes it difficult to analyze the growth rate of the local regret.

(ii) Second, it is time-consuming if we fully solve the lower-level problem (3) to get \(_{}\) because this requires multiple policy updates to solve an RL problem. Therefore, we use a "single-loop" method which only requires one-step policy update for a given \(r_{}\). However, since the policy is only updated once, the updated policy can be far from \(_{}\) and thus making the analysis difficult. Single-loop methods are widely adopted to solve hierarchical problems, including bi-level optimization [28; 29], game theory [30; 31], min-max problems [32; 33], etc. Recently, single-loop methods are applied to IRL , however, the paper  only solves an offline optimization problem and its analysis does not hold for non i.i.d. input data and continuous state-action space. We include a section in Appendix A.1 to discuss our distinctions from .

## 4 Algorithm and Theoretical Analysis

This section has three parts. The first part presents a novel online learning algorithm that solves the problem (2)-(3) and tackles the aforementioned two difficulties. The second part proves that Algorithm 1 achieves sub-linear local regret. If the reward function is linear, we prove that Algorithm 1 achieves sub-linear regret. The third part introduces a meta-learning method to get the meta-prior \(\).

### The proposed algorithm

In practice, the expert will demonstrate a specific trajectory \(s_{0}^{E},a_{0}^{E},s_{1}^{E},a_{1}^{E},\). For distinction, we use the capital letters (e.g., \(S\)) to represent random variables and the lower-case letters (e.g., \(s\)) to represent specific values. To design a fast algorithm, we propose an online-gradient-descent-based single-loop algorithm. In specific, at each time \(t\), the algorithm updates both policy \(\) and reward parameter \(\) only once. The policy update is to solve the lower-level problem (3) and the reward update is to solve the upper-level problem (2). In the following, we elaborate the procedure of policy update and reward update.

```
0: Initialized policy \(_{0}\), the streaming input data \(\{(s_{t}^{E},a_{t}^{E})\}_{t 0}\)
0: Learned reward parameter \(_{T}\) and policy \(_{T}\)
1: Compute \(\) using the meta-regularization in Section 4.3 and Appendix C, and set \(_{0}=\)
2:for\(t=0,1,,T-1\)do
3: Compute the soft Q-function \(Q^{}_{_{t},_{t}}\) (defined in Appendix B.1) under the current reward function \(r_{_{t}}\) and policy \(_{t}\)
4: Update \(_{t+1}(a|s)(Q^{}_{_{t},_{t}}(s,a))\) for any \((s,a)\)
5: Roll out policy \(_{t+1}\) twice: one starting from \(s_{0}^{E}\) to get \(s_{0}^{E},a_{0}^{},s_{1}^{},a_{1}^{},\), and the other starting from \((s_{t}^{E},a_{t}^{E})\) to get \(s_{t+1}^{},a_{t+1}^{},\)
6: Compute \(g_{t}=_{i=0}^{}^{i}_{}r_{_{t}}(s_{i}^{ },a_{i}^{})-_{i=0}^{}^{i}_{}r_{_ {t}}(s_{i}^{},a_{i}^{})+) }{1-}(_{t}-)\) where \(s_{0}^{}=s_{0}^{E}\) and \((s_{i}^{},a_{i}^{})=(s_{i}^{E},a_{i}^{E})\) for \(0 i t\)
7: Update \(_{t+1}=_{t}-_{t}g_{t}\)
8:endfor ```

**Algorithm 1** Meta-regularized In-trajectory Inverse Reinforcement Learning (MERIT-IRL)

**Policy update** (lines 3-4 of Algorithm 1). At each time \(t\), we only partially solve the lower-level problem (3) via one-step soft policy iteration [5; 34]. In specific, the soft policy iteration contains two steps: policy evaluation and policy improvement. Policy evaluation aims to compute the soft \(Q\)-function \(Q^{}_{_{t},_{t}}\) (see the expression in Appendix B.1) under the current learned reward function \(r_{_{t}}\) and learned policy \(_{t}\). Policy improvement aims to update policy according to \(_{t+1}(s,a)(Q^{}_{_{t},_{t}}(s,a))\) for any \((s,a)\). In practical implementations, \(_{t+1}\) can be obtained by one-step policy update in soft Q-learning  or one-step actor update in soft actor-critic .

**Reward update** (lines 5-7 of Algorithm 1). At each time \(t\), the algorithm observes \((s_{t}^{E},a_{t}^{E})\) and aims to leverage all the previously observed data to update the reward parameter. In specific, as \(L_{t}(;(s_{t}^{E},a_{t}^{E}))=-^{t}_{}(a_{t}^{E}|s_{t}^ {E})+}{2}||-||^{2}\) is the meta-regularized negative log-likelihoodof \((s_{t}^{E},a_{t}^{E})\), the algorithm can formulate \(_{i=0}^{t}L_{i}(;(s_{i}^{E},a_{i}^{E}))\) at time \(t\) using all the previously collected data (i.e., \(\{(s_{i}^{E},a_{i}^{E})\}_{i=0,,t}\)). To update the reward parameter, the algorithm partially minimizes \(_{i=0}^{t}L_{i}(;(s_{i}^{E},a_{i}^{E}))\) via one-step gradient descent.

**Lemma 1**.: _The gradient of \(_{i=0}^{t}L_{i}(;(s_{i}^{E},a_{i}^{E}))\) can be calculated as follows:_

\[_{i=0}^{t}L_{i}(;(s_{i}^{E},a_{i}^{E}))=E_{S,A}^{ _{}}_{i=0}^{}^{i}_{}_{ }(S_{i},A_{i})S_{0}=s_{0}^{E}+)}{1- }(-)\] \[-_{i=0}^{t}^{i}_{}_{}(s_{i}^{E}, a_{i}^{E})-E_{S,A}^{_{}}_{i=t+1}^{}^{i}_{ }_{}(S_{i},A_{i})S_{t}=s_{t}^{E},A_{t}=a_{t}^{E}.\] (4)

Note that the gradient (4) holds for continuous state-action space. Since the gradient (4) has expectation terms under the policy \(_{}\), we can only approximate it. In specific, we roll out the policy \(_{t+1}\) twice: one starting from \(s_{0}^{E}\) to get a trajectory \(\{(s_{i}^{},a_{i}^{})\}_{i 0}\) where \(s_{0}^{}=s_{0}^{E}\), and the other one starting from \((s_{t}^{E},a_{t}^{E})\) to get a trajectory \(\{(s_{i}^{},a_{i}^{})\}_{i 0}\) where \((s_{i}^{},a_{i}^{})=(s_{i}^{E},a_{i}^{E})\) for \(0 i t\). Then we use the empirical estimate \(g_{t}=_{i=0}^{}^{i}_{}_{_{t}}(s_{i}^{ },a_{i}^{})-_{i=0}^{}^{i}_{}_{ _{t}}(s_{i}^{},a_{i}^{})+)}{1-}(_{t}-)\) to approximate \(_{i=0}^{t}L_{i}(_{t};(s_{i}^{E},a_{i}^{E}))\). With the gradient approximation \(g_{t}\), we utilize stochastic online gradient descent \(_{t+1}=_{t}-_{t}g_{t}\) to update the reward parameter.

**Discussion on our special design of the reward update**. The right subfigure in Figure 1 visualizes our reward update (modulo the meta-regularization term). The green trajectory (i.e., \(\{(s_{t}^{E},a_{t}^{E})\}_{t 0}\)) is the expert trajectory, and the red trajectories (i.e., \(\{(s_{t}^{},a_{t}^{})\}_{t 0}\) and \(\{(s_{t}^{},a_{i}^{})\}_{i t}\)) are the trajectories generated by the learned policy. Given the expert trajectory prefix (i.e., the incomplete trajectory \(\{(s_{i}^{E},a_{i}^{E})\}_{i=0}^{t}\) observed so far), our method completes the expert trajectory by rolling out the learned policy starting from \((s_{t}^{E},a_{t}^{E})\) and filling the trajectory suffix \(\{(s_{i}^{},a_{i}^{})\}_{i>t}\). The combined complete trajectory includes the expert trajectory prefix \(\{(s_{i}^{E},a_{i}^{E})\}_{i=0}^{t}\) and the learner-filled trajectory suffix \(\{(s_{i}^{},a_{i}^{})\}_{i t}\). We update the reward function by comparing this combined trajectory to a complete trajectory \(\{(s_{t}^{},a_{i}^{})\}_{t 0}\) generated by the learned policy starting from the expert's initial state \(s_{0}^{E}\).

A more straightforward way for the reward update is to directly compare the trajectory prefixes (visualized in the middle of Figure 1) at each time \(t\).

However, this naive method can be problematic. We explain the issue of this naive method and the advantage of our method in the following context. Figure 1 visualizes the reward update for standard IRL (left), the naive method (i.e., directly run standard IRL algorithms on the expert trajectory prefix) (middle), and our method (right). The standard IRL (left) updates the reward function by comparing the complete expert trajectory and the complete trajectory generated by the learned policy. This case is ideal, however, it is infeasible when the trajectory is ongoing and we can only observe an incomplete expert trajectory \(\{(s_{i}^{E},a_{i}^{E})\}_{i=0}^{t}\) at each time \(t\). The naive method also computes the complete trajectory prefix \(\{(s_{i}^{E},a_{i}^{E})\}_{i=0}^{t}\) and the complete trajectory \(\{(s_{i}^{},a_{i}^{})\}_{i=0}^{t}\) generated by the learned policy. The comparison between the learner prefix \(\{s_{i}^{},a_{i}^{}\}_{i=0}^{t}\) and expert

Figure 1: Standard IRL (left), the naive method for trajectory learning (middle), and our method (right).

prefix \(\{s_{i}^{E},a_{i}^{E}\}_{i=0}^{t}\) encourages the learned reward function to explain the expert's demonstrated behaviors so far, and the comparison between the suffixes (\(\{s_{i}^{},a_{i}^{}\}_{i>t}\) and \(\{s_{i}^{},a_{i}^{}\}_{i>t}\)) encourages that we are learning a reward function that is useful for predicting the future. Note that as \(t\) increases, the expert trajectory prefix weights more and more in the combined complete trajectory, and eventually we will recover the standard IRL reward update when \(t\) goes to infinity. In Theorems 1 and 2, we theoretically guarantee that the proposed reward update can achieve sub-linear (local) regret. This shows the perfect consistency between the intuition and theory.

### Theoretical analysis

To quantify the local regret of Algorithm 1, we have two challenges: (i) Since we only update \(\) by one step at each time \(t\), we have \(_{t+1}\) instead of the optimal solution \(_{_{t}}\) of the lower-level problem (3). The policy \(_{t+1}\) can be far away from \(_{_{t}}\) and thus the empirical gradient estimate \(g_{t}\) can be a bad approximation of the gradient (4). (ii) Since we only update \(\) once at each time \(t\) instead of finding a near-stationary point \(^{}\) such that \(||_{i=0}^{t}L_{i}(^{};(s_{i}^{E},a_{i}^{E}))||\) as in [26; 27], the gradient norm \(||_{i=0}^{t} L_{i}(_{t};(s_{i}^{E},a_{i}^{E}))||\) is not stabilized under the threshold \(\) at every time \(t\). Therefore the local regret (i.e., \(_{t=0}^{T-1}||_{i=0}^{t} L_{i}(_{t};(s_{i}^{E },a_{i}^{E}))||^{2}\)) is hard to quantify and may not be sub-linear in \(T\). What's worse, the input data is not i.i.d. but correlated, i.e., the input data \((s_{t}^{E},a_{t}^{E})\) at time \(t\) is affected by the input data \((s_{t-1}^{E},a_{t-1}^{E})\) at last step. This correlation makes it even more difficult to quantify the local regret.

To solve the first challenge, we adopt the idea of two-timescale stochastic approximation  where the lower level updates in a faster timescale and the upper level updates in a slower timescale. The policy update is faster because it converges linearly under a fixed reward function  while the reward update is slower given that we choose \(_{t}(t+1)^{-1/2}\). Intuitively, since the policy update is faster than the reward update, the reward parameter is "relatively fixed" compared to the policy. It is expected that \(_{t+1}\) shall stay close to \(_{_{t}}\) and at last converges to \(_{_{t}}\) when \(t\) increases.

To solve the second challenge, we divide our analysis into two steps: (i) We quantify the difference of the gradient norms between the current correlated state-action distribution \(_{t}^{_{E}}(,)\) and a stationary state-action distribution for any loss function \(L_{i}\), \(i 0\). Note that \(_{t+1}^{_{E}}(,)\) is affected by \(_{t}^{_{E}}(,)\). (ii) We quantify the local regret under the stationary distribution. The benefit of doing so is that the input data is i.i.d. under the stationary distribution, and thus we can cast the online gradient descent method as a stochastic gradient descent method and quantify its local regret. Finally, we can quantify the local regret under the current correlated distribution \(_{t}^{_{E}}(,)\) by combining (i) and (ii).

We start our analysis with the definitions of stationary state distribution and stationary state-action distribution. For a given policy \(\), the corresponding stationary state distribution is \(^{}(s)(1-)_{t=0}^{}^{t}_{t}^{ }(s)\) and the stationary state-action distribution is \(^{}(s,a)(1-)_{t=0}^{}^{t}_{t} ^{}(s,a)\).

**Assumption 1**.: _The parameterized reward function \(r_{}\) satisfies \(|r_{_{t}}(s,a)-r_{_{2}}(s,a)|_{r}||_{1}- _{2}||\) and \(||_{}r_{_{1}}(s,a)-_{}r_{_{2}}(s,a)|| _{r}||_{1}-_{2}||\) for any \((_{1},_{2})\) and any \((s,a)\) where \(_{r}\) and \(_{r}\) are positive constants._

**Assumption 2** (Ergodicity).: _There exist constants \(C_{M}>0\) and \((0,1)\) such that for any policy \(\) and any \(t 0\), the following holds for the Markov chain induced by the policy \(\) and the state transition function \(P\): \(_{S_{0} P_{0}}d_{}(_{t}^{}(),^{}() ) C_{M}^{t}\) where \(d_{}(_{1}(),_{2}()) {2}_{s}|_{1}(s)-_{2}(s)|ds\) is the total variation distance between the two state distributions \(_{1}\) and \(_{2}\), \(_{0}\) is the initial state, and \(_{t}^{}()\) is the state distribution induced by the policy \(\) at time \(t\)._

Assumptions 1-2 are common in RL [37; 38; 39; 40]. Assumption 2 holds for any time-homogeneous Markov chain with finite state space or any uniformly ergodic Markov chain with general state space.

**Proposition 1**.: _Suppose Assumptions 1-2 hold and \(_{t}(0,)\), we have the following relation for any \(i 0\) and any \(_{t},t 0\):_

\[E_{(S_{i}^{E},A_{i}^{E})_{i}^{_{E}}(, )}|| L_{i}(_{t};(S_{i}^{E},A_{i}^{E}))||^{2}-E_{( S_{i}^{E},A_{i}^{E})^{_{E}}(,)}|| L_{i}( _{t};(S_{i}^{E},A_{i}^{E}))||^{2},\] \[ 8C_{M}_{r}^{2}^{ 2}^{i}^{2i},\]

_where \((S_{i}^{E},A_{i}^{E})_{i}^{_{E}}(,)\) means that \((S_{i}^{E},A_{i}^{E})\) is drawn from the correlated distribution \(_{i}^{_{E}}(,)\) and \((S_{i}^{E},A_{i}^{E})^{_{E}}(,)\) means that \((S_{i}^{E},A_{i}^{E})\) is drawn from the stationary distribution \(^{_{E}}(,)\)._Proposition 1 quantifies the gap of gradient norms between the current correlated distribution \(_{i}^{_{E}}\) and the stationary distribution \(^{_{E}}\). We next quantify the local regret under the stationary distribution \(^{_{E}}\) with the following lemma:

**Lemma 2**.: _Suppose Assumptions 1-2 hold and choose \(_{t}=}{}\), it holds that:_

\[E_{\{(S_{t}^{E},A_{t}^{E})^{_{E}}(,)\}_{t 0 }}_{t=0}^{T-1}||_{i=0}^{t} L_{i}(_{t} ;(S_{i}^{E},A_{i}^{E}))||^{2}\] \[ D_{1}( T+1)+D_{2}+D_{3}( T+1),\]

_where \(D_{1}\), \(D_{2}\), and \(D_{3}\) are positive constants whose expressions can be found in Appendix B.4._

Lemma 2 quantifies the local regret under the stationary distribution \(^{_{E}}\). With Proposition 1 and Lemma 2, we can quantify the local regret under the current correlated distribution.

**Theorem 1**.: _Suppose Assumptions 1-2 hold and choose \(_{t}=}{}\), we have that:_

\[E_{\{(S_{t}^{E},A_{t}^{E})_{t}^{_{E}}(, )\}_{t 0}}_{t=0}^{T-1}||_{i=0}^{t}  L_{i}(_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\] \[D_{1}+_{r}^{2}(2-)^{2}}{(1- ^{2})(1-)^{2}}( T+1)+D_{2}+D_{3}(  T+1).\]

Theorem 1 is based on Proposition 1 and Lemma 2. It shows that Algorithm 1 achieves sub-linear local regret. Moreover, if the reward function is linear, Algorithm 1 achieves sub-linear regret:

**Theorem 2**.: _Suppose the expert reward function \(r_{E}\) and the parameterized reward \(r_{}\) are linear, and Assumptions 1-2 hold. Choose \(_{t}=)}\) we have that:_

\[E_{\{(S_{t}^{E},A_{t}^{E})_{t}^{_{E}}(, )\}_{t 0}}_{t=0}^{T-1}L_{t}(_{t};(S_{t}^{E},A_{t}^{E})) \] \[-_{}E_{\{(S_{t}^{E},A_{t}^{E})_{t}^{_ {E}}(,)\}_{t 0}}_{t=0}^{T-1}L_{t}(;(S_{t}^{E},A_{ t}^{E})) D_{4}+D_{5}( T+1),\]

_where \(D_{4}\) and \(D_{5}\) are positive constants whose expressions are in Appendix B.6._

### Meta-Regularization

Since there is only one training trajectory and this trajectory is not complete during the learning process, we need to add a regularization term to avoid overfitting. Inspired by humans' using relevant experience to help do inference, we introduce the meta-regularization \(||-||^{2}\) where \(\) is the hyper-parameter and the meta-prior \(\) is learned from "relevant experience". In specific, we introduce a set of relevant tasks \(\{_{j}\}_{j P_{}}\) where each task \(_{j}\) is an IRL problem and \(P_{}\) is the implicit task distribution. The tasks \(\{_{j}\}_{j P_{}}\) are relevant in the sense that they share the components \((,,,P_{0},P)\) of the MDP with our in-trajectory learning problem. However, the expert's reward functions of different tasks are different and are drawn from an unknown reward function distribution. For example, in the stock market case mentioned in the introduction, the experts of different tasks invest in the same stock market but may have different preferences. As standard in meta-learning [41; 42; 43], we assume that the expert's reward function \(r_{E}\) of our in-trajectory learning problem is also drawn from the same unknown reward function distribution. Note that the reward functions of the relevant tasks \(\{_{j}\}_{j P_{}}\) are different from \(r_{E}\) even if they are drawn from the same unknown reward function distribution.

For each task \(_{j}\), there is a batch of trajectories and we divide this batch into two sets, i.e., \(_{j}^{}\) and \(_{j}^{}\). The training set \(_{j}^{}\) only has one trajectory, just as our in-trajectory learning problem, and the evaluation set \(_{j}^{}\) has abundant trajectories. Define the loss function on a certain data set \(\{^{v}\}_{v=1}^{m}\) as \(L(,)-_{v=1}^{m}_{t=0}^{}^{t} _{}(a_{t}^{v}|s_{t}^{v})\). The goal of each task \(_{j}\) is to learn a task-specific adaptation \(_{j}\) using the training set \(_{j}^{}\), such that \(_{j}\) can minimize the test loss \(L(_{j},_{j}^{})\) on the evaluation set \(_{j}^{}\). The goal of meta-regularization is to find a meta-prior \(\), from which such task-specific adaptations \(_{j}\) can be adapted to all tasks \(\{_{j}\}_{j P_{}}\). In specific, meta-regularization  proposes a bi-level optimization problem (5). The lower-level problem uses only one trajectory \(_{j}^{}\) to find the task-specific adaptation \(_{j}\) such that the meta-regularized loss function \(L(,_{j}^{})+||-||^{2}\) is minimized. The upper-level problem is to find a meta-prior \(\) such that the corresponding task-specific adaptations \(\{_{j}\}_{j P_{}}\) can minimize the expected loss function \(L(_{j},_{j}^{})\) over the evaluation sets of all tasks \(\{_{i}\}_{j P_{}}\).

\[_{}\;E_{j P_{}}L(_{j},_{j }^{}),_{j}=*{arg\,min}_{}L(,_{j}^{}) +||-||^{2}.\] (5)

The lower-level loss function in (5) is the offline version of our in-trajectory loss function (1) (i.e., \(L(,_{j}^{})+||-||^{2}=_{t=0}^{}L_{t}(;(s_{t}^{},a_{t}^{}))\)) where \((s_{t}^{},a_{t}^{})_{j}^{}\). Our in-trajectory learning problem can also be regarded as to find a task-specific adaptation. Note that the in-trajectory learning problem is online while the lower-level problem in (5) is offline because the in-trajectory problem is ongoing where we keep observing new state-action pairs. In contrast, the lower-level problem in (5) is based on "experience" that has already happened. Due to the space limit, we include the algorithm and theoretical guarantees of solving the problem (5) in Appendix C.

## 5 Experiments

We present three experiments to show the effectiveness of MERIT-IRL. We use four baselines for comparisons. (i) **IT-IRL**: this method is MERIT-IRL without meta-regularization. (ii) **Naive MERIT-IRL**: this method has the meta-regularization term but uses the naive way (in the middle of Figure 1) to update reward. (iii) **Naive IT-IRL**: this method uses the naive way to update the learned reward and does not have the meta-regularization term. (iv) **Hindsight**: this method is meta-regularized ML-IRL  which can access the complete expert trajectory and uses the standard IRL (visualized in the left of Figure 1) with meta-regularization to update the learned reward. The experiment details are in Appendix D.

### MuJoCo experiment

In this subsection, we consider the target velocity problem for three MuJoCo robots: HalfCheetah, Walker, and Hopper. The target velocity problem is widely used in meta-RL  and meta-IRL . In specific, the robots aim to maintain a target velocity in each task and the target velocity of different tasks is different. To test the performance of MERIT-IRL, we use \(10\) test tasks whose target velocity is randomly between \(1.5\) and \(2.0\). In the test tasks, there is only one expert trajectory and the state-action pairs of this trajectory are sequentially revealed (to MERIT-IRL, IT-IRL, Naive MERIT-IRL, and Naive IT-IRL) in an online fashion. The baseline Hindispit uses the complete expert trajectory to learn a reward function. The ground truth reward is designed as \(-|v-v_{}|\) (as in ) where \(v\) is the current robot velocity and \(v_{}\) is the target velocity. To learn the meta-prior \(\), we use \(50\) relevant tasks whose target velocity is randomly between \(0\) and \(3\).

Figures 1(a)-1(c) show the in-trajectory learning performance where the \(x\)-axis is the time step \(t\) of the expert trajectory and the \(y\)-axis is the cumulative reward of the learned policy \(_{t}\) when only the first \(t\) steps of the expert trajectory are observed. The \(x\)-limit is \(1,000\) because the trajectory length in MuJoCo is \(1,000\). Note that the baseline "Hindsight" is not in-trajectory learning since it learns from a complete expert trajectory. For comparison, we use two horizontal lines (close to each other) to show the performance of Hindsight and the expert in the figures. Figure 1(a) shows that MERIT-IRL achieves similar performance with the expert when only \(40\%\) of the complete expert trajectory (\(t=400\)) is observed while IT-IRL can only achieve performance close to the expert after observing more than \(90\%\) of the complete expert trajectory (\(t=900\)). This shows the effectiveness of the meta-regularization. Naive MERIT-IRL and Naive IT-IRL fail to imitate the expert even if the complete expert trajectory is observed (\(t=1,000\)). This shows the effectiveness of our special design of the reward update. The discussions on Figures 1(b) and 1(c) are in Appendix D.2.

Table 1 shows the results after observing the complete expert trajectory. MERIT-IRL performs much better than IT-IRL, Naive MERIT-IRL, and Naive IT-IRL. MERIT-IRL achieves similar performance

with Hindsight and expert. Note that it is not expected that MERIT-IRL outperforms Hindsight since Hindsight uses the complete expert trajectory to learn.

### Stock market experiment

RL to train a stock trading agent has been widely studied in AI for finance [46; 47; 48]. In this experiment, we use IRL to learn the reward function (i.e., investing preference) of the target investor in a stock market scenario. In specific, we use the real-world data of \(30\) constituent stocks in Dow Jones Industrial Average from 2021-01-01 to 2022-01-01. We use a benchmark called "FinRL"  to configure the real-world stock data into an MDP environment. The target investor (i.e., expert) has an initial asset of \(\$1,000\) and trades stocks on every stock market opening day. The stock market opens 252 days between 2021-01-01 and 2022-01-01, and thus the trajectory length is 252. The reward function of the target investor is defined as \(p_{1}-p_{2}\) where \(p_{1}\) is the investor's profit which is the money earned from trading stocks subtracting the transaction cost, and \(p_{2}\) models the investor's preference of whether willing to take risks. In specific, \(p_{2}\) is positive if the investor buys stocks whose turbulence indices are larger than a certain turbulence threshold, and zero otherwise. The value of \(p_{2}\) depends on the type and amount of the trading stocks. The turbulence thresholds of different investors are different. The turbulence index measures the price fluctuation of a stock. If the turbulence index is high, the corresponding stock has a high fluctuating price and thus is risky to buy . Therefore, an investor unwilling to take risks has a relatively low turbulence threshold. We include experiment details in Appendix D.3. To test performance, we use \(10\) test tasks whose turbulence thresholds are randomly between \(45\) and \(50\). To learn \(\), we use \(50\) relevant tasks whose turbulence thresholds are randomly between \(30\) and \(60\).

Figure 2d shows that MERIT-IRL achieves similar cumulative reward with the expert at \(t=140\) which is less than \(60\%\) of the whole trajectory, while the three in-trajectory baselines fail to imitate the expert before the ongoing trajectory terminates. The last row in Table 1 shows that MERIT-IRL achieves similar performance with Hindsight and the expert. More discussions on the results are in Appendix D.3.

### Learning from a shooter's ongoing trajectory

This part presents the experiment of learning from an ongoing shooter trajectory. Following , we model the shooter's movement as a navigation problem. We build a simulator in Gazebo (Figure 2(a)) where the shooter moves from the door (lower left corner) to the red target (upper right corner). The learner observes the ongoing trajectory of the shooter and keeps updating the learned reward and policy. In our case, the complete shooter trajectory has the length of \(140\). Figures 2(b)-2(g) show our in-trajectory learning performance where the heat maps visualize the learned reward. We normalize the learned reward to \(\). We can observe that as the ongoing trajectory is expanding, the learned reward function becomes more and more precise to locate the goal area. When \(t=40\), we cannot tell

   & TRIL & Naive MERIT-IRL &  & Hindsight & Expert \\  HalfCheath & \(-214.63 53.96\) & \(-386.78 152.05\) & \(-548.22 0.51\) & \(-765.27 104.1\) & \(-208.74 37.23\) & \(-181.51 28.35\) \\  Walker & \(-654.77 102.59\) & \(-891.79 156.90\) & \(-962.42 111.60\) & \(-1349.25 158.88\) & \(-681.71 157.92\) & \(-634.17 120.57\) \\  Hopper & \(-476.72 32.09\) & \(-669.88 53.63\) & \(-691.03 93.35\) & \(-112.06 74.33\) & \(-455.70 74.93\) & \(-421.74 84.30\) \\  Stock market & \(386.70 62.95\) & \(266.81 68.61\) & \(192.49 75.34\) & \(72.33 16.73\) & \(390.30 77.37\) & \(403.15 61.94\) \\  

Table 1: Experiment results. The mean and standard deviation are calculated from \(10\) test tasks.

Figure 2: In-trajectory learning performance.

the goal area from the heat map (Figure 2(b)). However, as the time \(t\) grows, we can almost locate the goal area when \(t=60\) (Figure 2(c)) and precisely locate the goal area when \(t=80\) (Figure 2(d)).

Figure 2(h) shows the policy learning performance. Since there is no ground truth reward in this problem, we use "success rate" to quantify the performance of the learned policy. The success rate is the rate that the learned policy successfully reaches the goal. From 2(h), we can see that MERIT-IRL outperforms the other baselines and can achieve \(100\%\) success rate when \(t=80\) (i.e., only observing \(57\%\) of the complete trajectory). Note that we do not include Hindsight and Expert in Figure 3 since they both achieve \(100\%\) success rate.

## 6 Conclusion

This paper proposes MERIT-IRL, the first in-trajectory inverse reinforcement learning theoretical framework that learns a reward function while observing an initial portion of a trajectory and keeps updating the learned reward function when extended portions (i.e., new state-action pairs) of the trajectory are observed. Experiments show that MERIT-IRL can imitate the expert from the ongoing expert trajectory before it terminates.

## 7 Acknowledgements

This work is partially supported by the National Science Foundation through grants ECCS 1846706 and ECCS 2140175. We would like to thank the reviewers for their insightful and constructive suggestions.

Figure 3: Learning performance on the active shooting scenario.