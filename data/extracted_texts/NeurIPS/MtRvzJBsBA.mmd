# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

Thus, in this paper, we propose _LRM-Zero_, trained on purely synthesized data, to explore another route which can potentially resolve the 3D data scarcity, licensing, and bias issues. The name 'Zero' highlights our synthesized and non-semantic training data, which we named as _Zeroverse_. _Zeroverse_ is a procedural, amorphic alternative to Olaverse  in training reconstruction models. The comparison between _Zeroverse_ and Olaverse is illustrated in Fig. 1, and more visual comparisons can be found in Appendix. The data in _Zeroverse_ is procedurally created by randomly composing primitive shapes with textures and applying shape augmentations. The process resembles the previous work Xu et al. . We select five primitive shapes: cube, sphere, cylinder, cone, and torus to cover different types of surfaces and topological characteristics. The textures are randomly applied, which is realistic at low-level but do not contain high-level semantics. The three different augmentation methods, i.e., height-field, boolean difference, and, wireframes, help increase the data diversity and add more curvatures, concavity, and thin structures, respectively. The primitive shapes, textures, and an illustration of the augmentations are shown in Fig. 2. In this work, we experiment with 400K _Zeroverse_ data, which roughly matches the number of meaningful data in Ojaverse (i.e., excluding rendering failures, flatten 3D data, point clouds, unsafe data from the overall 800K data). The initial experiments indicate that further increasing the amount of data is not effective, and we refer the reader to Appendix for our early results on scaling the data size.

We validate our _Zeroverse_ by training GS-LRM  over it, and we denote this model as _LRM-Zero_. Surprisingly, we found that _LRM-Zero_ can achieve a reconstruction quality similar to that of GS-LRM trained on Ojaverse, seeing Fig. 1. More comparisons are provided in the Appendix. We also quantitatively evaluate the model on two standard 3D reconstruction benchmark ABO  and GSO . For sparse-view reconstruction (i.e., 4 views and 8 views), _LRM-Zero_ reaches competitive results against GS-LRM, and the best results gap is as low as \(1.12\) PSNR, \(0.09\) SSIM, and \(0.006\) LPIPS. A plausible reason for such "zero"-shot data generalization is that 3D reconstruction (with poses) relies more on the local visual clues instead of the global semantics. This is more obvious for dense-view reconstruction (e.g. 100 input views) where single-shape optimization without any data prior can reach good results [6; 46]. For the sparse-view reconstruction that we focused, _LRM-Zero_

Figure 1: We present our _LRM-Zero_ framework trained with synthesized procedural data _Zeroverse_. _Zeroverse_ (top left) is created from random primitives with textures and augmentations, thus it does not contain semantical information as in Ojaverse (bottom left). Nevertheless, when training with the same large reconstruction model architecture  on both datasets, _LRM-Zero_ can match objaverse-trained LRM’s (denoted as ‘LRM’) visual quality (right part) of reconstructions. A possible explanation is that 3D reconstruction, although serves as a core task in 3D vision, rely mostly on local information instead of global semantics. Reconstruction is visualized with RGB and position-based renderings, and interactive viewers can be found on our website.

can possibly rely on the local details (such as cross-view patch correspondence) to infer the shape, where _Zeroverse_ supports _LRM-Zero_ to learn such knowledge.

We analyze the effectiveness of _Zeroverse_'s design, especially for shape augmentations. We find that each type of augmentation provides visible structural improvements for the reconstructions, and most of the improvements are reflected in the metrics of our benchmarks. We also study the impact of different dataset designs on another critical property of _LRM-Zero_: training stability. Training stability is crucial for large-scale training as large models are more prone to diverge after training for a significantly long time [72; 17; 22]. We empirically found that careless design of _Zeroverse_ can introduce significant instability during the training of _LRM-Zero_. As both data complexity and model hyperparameters can affect the training stability, a model-data co-design is helpful in our experiments, i.e., the model's hyperparameters and data properties are tuned jointly. Lastly, we show the generalizability of both _Zeroverse_ and _LRM-Zero_. For _Zeroverse_, we show that the dataset can also enable training a NeRF-based reconstruction model and reaches competitive results to Obiayverse-trained models. For _LRM-Zero_, we demonstrate that the model can generalize across different datasets including realistic 3D data, such as OmniObject3D  and OpenIllumination . We also show that _LRM-Zero_ can be combined with off-the-shelf multi-view diffusion models to support both text-to-3D generation and image-to-3D generation.

The key contribution of this paper is to demonstrate that purely synthesized data can be utilized to learn generic 3D priors for sparse-view 3D reconstruction, a core task of 3D vision. While our work may appear straightforward, it provides a minimal, yet generalizable proof-of-concept which can inspire the community to exploit procedural 3D data for 3D tasks in the future. We also provide carefully crafted studies on the co-design of data and model, as well as their effect on training stability and generalization.

Lastly, we provide the interactive _Zeroverse_ data visualization and _LRM-Zero_ reconstruction results in our website https://desaixie.github.io/lrm-zero/. We recommend the readers to have a check. The _Zeroverse_ data synthesis script is released at https://github.com/desaixie/zeroverse, and we hope that it can facilitate future research.

## 2 Background: feed-forward reconstruction model

Feed-forward 3D reconstruction targets to learn a model that can regress the 3D shapes from multi-view images. The sparse-view version of this task is illustrated in the right part of Fig. 1, where multiple input views are presented, and the output is a 3D representation. To solve this task, LRM  introduces a pure-transformer based method which allows scalable training. Original LRM uses NeRF [61; 12] as 3D representation, and a bunch of later works [13; 84; 16; 99; 107] extend it to Gaussian Splatting , which is another 3D representation proposed recently. This paper is mostly experimented with the GS-LRM  architecture given its simplicity in model design (i.e., a pure-transformer architecture) and the SotA reconstruction quality.

GS-LRM predict the 3D Gaussians from the \(n\) multi-view images \(I_{1},,I_{n}\). The images are first patchified to features \(f_{1},,f_{n}\) with shared non-overlapping (i.e., stride equals to kernel size) convolutions. Then features are flattened and concatenated as the input to a self-attention transformer.

\[f_{1},,f_{n} =(I_{1}),,(I_{n})\] (1) \[x =[(f_{1});;(f_{n})]\] (2) \[y =(x)\] (3)

The output \(y\) of the transformer will be directly interpreted as the Gaussian Splatting parameters, and serves as the representation of the output 3D object. These parameters can be rendered for training losses or viewed interactively. The GS-LRM model is purely trained with RGB rendering loss by minimizing the difference between ground truth image and the rendering images. For more details of the GS-LRM model  architecture and Gaussian Splatting representation , please refer to the original papers. After briefly introducing the backbone model architecture of _LRM-Zero_, we next introduce our procedural data _Zeroverse_ to train such a model.

## 3 The _Zeroverse_ dataset

In this section, we introduce the creation of _Zeroverse_ that supports training a sparse-view large reconstruction model (LRM). _Zeroverse_ consists of procedurally synthesized shapes with randomized parameters by revisiting the pipeline in the previous work , which was initially proposed for relighting and later extended for view synthesis  and material estimation . As illustrated in Fig. 2, the process first composites primitive shapes with random texturing (Sec. 3.1). Then, different augmentations are applied to enhance the diversity of the data (Sec. 3.2). As the LRM-based model only relies on multi-view rendering to train the model (i.e., do not require geometry supervision), the _Zeroverse_ objects are always saved in the compact mesh format.

### Composing primitives into textured shapes

**Primitive shapes.** Our synthetic object creation process starts with a pool of primitive shapes. The pool only consists of basic shapes for the 3D world. Specifically, in our implementation, we have 5 primitives: cube, sphere, cylinder, cone, and torus. Intuitively, cubes and spheres provides knowledge on the sharp straight lines and the purely curved shapes. Cylinders and cones contain different curved surfaces besides sphere. The specialty of torus is its hole, which is topologically different to the above shapes (i.e., a torus has genus 1). Although it is possible to create holes through combinations and augmentations (e.g., the boolean difference and wireframe in Sec. 3.2), we decide to explicitly add this capacity to our dataset. Also, the combination of multiple torus is easy to create shapes with higher genus (i.e., roughly the number of disjoint holes in a connected shape).

**Compositions.** With a reasonable pool of primitive shapes, we then compose them together to construct complex shapes, offering more diverse visual cues for the reconstruction task. We randomly sample 1 to 9 primitives (with replacement) from the primitive pool. The sampling probability of the numbers of primitives is configurable. Each sampled primitive will independently be scaled, translated, and rotated randomly. We simply combine these affine-transformed shapes together without special handling of the shape intersections or disconnections. Thus it is possible to have multiple disjoint shapes in one scene, which we will still refer to as one object. This satisfies the requirement for real-world reconstruction applications, where simple disjoint shapes would be considered as a single object.

**Texturing.** For each surface of the shape, we apply a texture randomly sampled from an internal dataset. To support the research community, in our public release version, we provide an alternative public texture dataset.

Figure 2: Illustration of the _Zeroverse_ data creation process. A random textured shape is first composited from primitive shapes and textures (Sec.3.1). Then different augmentations (i.e., height field, boolean difference, wireframes in Sec. 3.2) are applied to enhance the dataset characteristics (e.g., curved surfaces, concavity, and thin structures). More visualizations in Appendix and website.

### Shape augmentations

We apply augmentation to the textured shapes to add diversity and complexity that resembles real-world objects and is not covered by the initial shape in Sec. 3.1. We implement three augmentation operators: height field, boolean difference, and wireframe conversion for better data coverage of curved shapes, concave shapes, and thin structured respectively. These diversities of the data will be reflected by the capacity of large reconstruction models with observable structural improvements (studied in Sec. 5.1). We illustrated the process and example results in the right part of Fig. 2. We do not apply the augmentation 'boolean difference' and 'wireframe' at the same time. This is for training stability (studied in Sec. 5.2) as we empirically found that an ultra-complex shape can lead the reconstruction model training to non-convergence.

**Height fields.** Most of the surfaces (except the torus) of our primitives have constant curvatures, and we apply height fields augmentation in Xu et al.  to break this constraint. An illustration of the height map can be found in Fig. 2 (top right). In detail, for each face of the primitives, we apply a height field with varying heights and curvatures to displace the surface vertices, making the surface curved and bumpy. Specifically, the magnitude of height is randomly sampled at each position in the map and we use bicubic interpolation to obtain smooth surfaces.

**Boolean difference.** Concave structures are common in real-world objects, for example, bowls, hats, spoons. However, the concavity is not well captured by the previous pipeline. To resolve this, we'subtract' primitives from the shapes, which can be considering as a reversion of the 'additive' operators in the combination process. This is implemented by computing the boolean difference between the composite object in Sec. 3.1 and a basic primitive from our pool. In details, we use Blender's boolean modifier and solidify modifier to augment the initial shape. The inside faces of the resulting cut shape will have the same texture as the outside faces. Besides introducing concavity to the dataset, the boolean difference operation also expose the 'interior' of the shape (as shown in Fig. 2), which helps the reconstruction model to handle complex structures. The actual effect of the boolean operator is quite diverse, and we refer the reader to check the visualization in the Appendix.

**Wireframe.** Besides concavity, thin structures (especially the striped or repeated one) is another challenge in real-world reconstruction, for example, hairs, baskets, railings. To train a reconstruction model capable with thin structures, we want to explicitly add this characteristic to our _Zeroverse_ dataset. And for simplicity, we use the wireframe. Wireframe is a basic augmentation from the primitive shapes, which generally converts their meshes to the skeletons. It is pre-implemented in multiple libraries, and we take the shape modifier in Blender. The results are illustrated in Fig. 2 (i.e., a wireframe of torus) and more in Appendix. The texture of the wireframe is inherited from the primitive shape but usually not distinguishable due to its thin surfaces.

## 4 Experiments

### _LRM-Zero_ experiment details

For rendering the multi-view images of _Zeroverse_, we follow . For each object in _Zeroverse_, we render 32 views with randomly sampled camera rotations and random distances in the range of [2.0, 3.0]. Each image is rendered at the 512 \(\) 512 resolution with uniform lighting. We use the same

    & &  & ABO \\   & & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\   & GS-LRM & **33.23** & **0.971** & **0.031** & **30.92** & **0.944** & **0.067** \\  & _LRM-Zero_ & 31.62 & 0.960 & 0.039 & 28.71 & 0.929 & 0.078 \\   & GS-LRM & **31.90** & **0.966** & **0.030** & **30.66** & **0.949** & **0.055** \\  & _LRM-Zero_ & 30.78 & 0.957 & 0.036 & 28.82 & 0.934 & 0.065 \\   

Table 1: Quantitative results comparing _LRM-Zero_ with GS-LRM  (trained on Objavverse) under the 8-input-view setting. We use GSO  and ABO  evaluation datasets and PSNR, SSIM, and LPIPS  metrics. _LRM-Zero_ demonstrates competitive performance against GS-LRM.

network architecture and follow the hyperparameters/implementation (e.g., 80K training steps, details as GS-LRM . We only decrease perceptual loss weight from 0.5 to 0.2 to improve training stability. For the results comparison, we pre-train the model with \(256\)-resolution and fine-tuned on \(512\)-resolution following GS-LRM. The overall training uses 64 A100 GPUs and takes 3 days. For analysis and ablation studies, we only run the \(256\)-resolution experiments. Please refer to the original GS-LRM paper  for more experimental details.

Metric evaluations for results and analysis are mostly conducted on two relatively large benchmarks: Google Scanned Objects (GSO)  and Amazon Berkeley Objects (ABO) . In our paper, we use 8 structural input-view as the standard evaluation protocol to increase view coverage. The 4 structural input-view results are provided in Appendix. In details, for 8 structural views, we render from 0 elevation with 0, 90, 180, 270 azimuth plus 40 elevation with 45, 135, 225, and 315 azimuth, while 4 structural views render from 20 elevation with 0, 90, 180, 270 azimuth. The testing views for metric calculation are randomly sampled. The generalization experiments in Sec. 5.3 use either \(8\) random input views for generalization test, or the fixed cameras provided by the generated models. We always assume that the camera poses are provided with input views.

### Results

We evaluate _LRM-Zero_ on the benchmarks and show the results in Tab. 1. The absolute PSNR values of GSO and ABO are over 30 and 28.7 respectively, which indicates that the reconstruction has high visual quality. Compared to GS-LRM  trained on Objavverse, the metric still shows a gap, but within a reasonable range of \(1.1\) PSNR on GSO and \(1.9\) PSNR on ABO. The gap is larger for higher resolution (i.e., Res-512) and it is possibly due to the training configuration of our 512-res fine-tuning is sub-optimal. Qualitatively, we do not observe significant visual difference between the reconstructed 3D models from _LRM-Zero_ and GS-LRM. An example comparison is shown in Fig. 1 and some more comparisons in the Appendix. The interactive viewer of _LRM-Zero_ reconstruction results can be found in our website.

After viewing the _LRM-Zero_ visual results and the sparse-view reconstruction setup, we found that both 4-view and 8-view can not fully cover the object surfaces thus the model needs to hallucinate the invisible parts. This hallucination ability requires semantic understanding of the 3D objects while _Zeroverse_ lacks by design. It might be the major reason of result gap between _Zeroverse_-trained and Objavverse-trained models in Tab. 1. The invisible regions can be mitigated by reconstructing from more views (either capturing or generating). However, more views involves more tokens and challenges the computation cost of the current fully-connected self-attention design in GS-LRM, thus beyond the scope of current paper and we leave it as future works.

## 5 Analysis

In this section, we analyze the properties of our _LRM-Zero_ trained with the synthesized _Zeroverse_. We first conduct ablation studies in Sec. 5.1 to show the effectiveness of _Zeroverse_ augmentations. Next, Sec. 5.2 explores stabilized training of _LRM-Zero_ from both data and model perspective, as training stability is one of the key challenges in large-scale training [22; 17]. Last, we show the generalization of our methods by applying _LRM-Zero_ over diverse data, and trained different reconstruction models on _Zeroverse_.

### Ablation studies on different augmentations

We conduct ablation studies to verify the effectiveness of our height field, boolean difference, and wireframe augmentations. We show both quantative and qualitative comparisons.

**Boolean difference and wireframe augmentation.** As our sampling strategy does not apply boolean difference and wireframe augmentations jointly to avoid over-complex shapes. Therefore, we conduct the ablation study of these augmentations together. As shown in Tab. 2, we apply different sampling ratios to both augmentations (e.g., experiments id 1, 2, 3) and also exclude them in experiment 4. Boolean difference augmentation largely improves the metric (comparing experiment pair 2, 4 or 1, 3). Note that we use 60%/40% instead of 50%/50% because the later one has more instability (Sec. 5.2). The possible reason is visualized in Fig. 3: the lack of boolean augmentation in training data causes experiment 2 to show structural failure on concave shapes.

The wireframe augmentation does not show significant improvements of the metric, but it increases the visual fidelity. As shown in Fig. 4, without wireframe augmentation in its training data, _LRM-Zero_ fails to reconstruct objects with thin structures, e.g. chair and table legs, or rails.

**Height-field augmentation** Tab. 3 shows two experiments with and without height field augmentation. Both are trained on 120K objects consisting of 80K original compositional objects and 40K boolean augmentation objects. This setting is different from other ablation experiments in Tab. 2, because we had to synthesize and render objects with 0 height field probability, which do not exist in _Zeroverse_. We also uses the boolean-difference only augmentation to mitigate the effect of instability. These results reveal that height field augmentation can improve the results.

### Training stability

As discussed in Sec. 5.1, adding augmentation substantially boosts _LRM-Zero_'s performance. However, it also makes _Zeroverse_ more complex and thus introduces training instability in _LRM-Zero_. We explore various techniques to help stabilize the training from either the training side (i.e., decreasing perceptual loss weight, decreasing Guassian splitting scale clipping, decreasing view-angle threshold) or the data mixing ratio of augmentations (we found that height-filed augmentation does not introduce instability a lot thus kept it). The observations are summarized in Tab. 4. In general, we observe that shifting training hyperparameters from optimal would improve the stability. However, this would decrease the performance. Thus our final plan (as shown in experiment 6) is a more balanced augmentation mixing ratio, and only minimal change on the training side. More comprehensive experiments are in Appendix.

### Generalization

We first validate the generalization of our _Zeroverse_ by training a NeRF-based LRM model [49; 92] on it. NeRF-based model's architecture is different from GS-LRM. Also the 3D modeling is philosophically different: NeRF has a canonical space for Triplane (i.e., Eulerian representation) while Gaussian Splatting is pixel-aligned per-point prediction (i.e., Lagrangian representation).

    &  &  & ABO \\  id & hf-only & boolean & wireframe & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\ 
1 & default & default & default & **30.78** & **0.957** & **0.036** & **28.82** & **0.934** & **0.065** \\ 
2 & 60.0\% & 40.0\% & 0\% & 30.75 & 0.957 & 0.036 & 28.76 & 0.931 & 0.066 \\ 
3 & 66.6\% & 0\% & 33.3\% & 29.82 & 0.948 & 0.042 & 27.79 & 0.923 & 0.075 \\ 
4 & 100.0\% & 0\% & 0\% & 29.88 & 0.949 & 0.042 & 27.39 & 0.919 & 0.077 \\   

Table 2: Ablation studies over boolean difference and wireframe augmentations. The height-field (hf) is applied independently to each surface with prob. 0.5.

Figure 3: Qualitative results generated by _LRM-Zero_ trained on _Zeroverse_ with (left two) and without boolean difference augmentation (right two). Right two _LRM-Zero_’s reconstruction results have structural failures on objects with concave shapes and complex structures.

Despite of these differences, the results in Tab. 5 are similar to what we observed in GS-LRM that _Zeroverse_-trained model is competitive to Objaverse-trained models.

Besides the standard benchmark GSO and ABO, we also evaluate our _LRM-Zero_ on diverse datasets to show its generalization, such as realistic 3D objects in OpenIllumination  and OmniObject3D , cross-evaluation on Zeroverse and Objaverse, and the generative outputs by Instant3D  and One2345++ . As these experiments are for generalization test, we use 8 randomly-sampled input for OpenIllumination, OmniObject3D, Objaverse, and Zeroverse. For Instant3D and One2345++, we use the default camera setup of the generative model's outputs, where Instant3D and One2345++ have 4 and 6 structural cameras, respectively. As shown in Tab. 6, our _LRM-Zero_ is competitive. We visualize the Instant3D and One2345++ results in Fig. 5, where _LRM-Zero_ still work for these truly novel generated images, showcasing that _LRM-Zero_ can be used in the 3D generation pipeline.

## 6 Related works

3D reconstruction is an important task in 3D vision. As 3D data is usually hard to capture, 3D reconstruction gives the ability to get 3D model from other modalities (e.g., images). Traditional methods [66; 61; 56; 14] on 3D reconstruction focuses on the per-sample optimization, where the 3D

Figure 4: Qualitative results generated by _LRM-Zero_ trained on default _Zeroverse_ with (left two) and without wireframe augmentation (right two). Right two _LRM-Zero_’s reconstruction results have structural failures on objects with thin structures.

Figure 5: _LRM-Zero_’s qualitative results on Instant3D text-to-3D (left two) and One2345++ image-to-3D (right two) generated multi-view images.

    &  & Height Field &  & ABO \\  id &  hf-only \\ def. 40\% \\  &  boolean \\ def. 40\% \\  &  wireframe \\ def. 20\% \\  &  HF probability \\ def. 0.5 \\  &  PSNR\(\) \\  &  SSIM\(\) \\  &  LPIPS\(\) \\  &  PSNR\(\) \\  &  SSIM\(\) \\  & 
 LPIPS\(\) \\  \\ 
1 & 60.0\% & 40.0\% & 0\% & default & **30.24** & **0.952** & **0.039** & **28.31** & **0.926** & **0.072** \\ 
2 & & & & 0 & 29.22 & 0.941 & 0.045 & 27.70 & 0.916 & 0.076 \\   

Table 3: Ablations studies on height-field augmentation.

shapes are parameterized and optimized by the rendering loss  or geometry loss . These optimization-based methods are usually slow and require adequate number of views (e.g., 100 views). Although methods are proposed  to resolve these constraints for efficiency and view requirements , the speed is not largely improved.

Recent progresses advances this task with learning-based feed-forward methods . Instead of optimization, these methods train a model from large-scale object  or scene  data to predict the shape directly. Besides the benefits of efficiency, these feed-forward methods can naturally support sparse-views as input (e.g., 4 to 12 input view images) because they learn data patterns from massive dataset. Some models can even go with extreme case of single-view reconstruction , which needs to have data prior from realistic 3D data. Multi-view stereo methods  are another family of feed-forward 3D reconstruction methods, but they cannot deal with sparse-view or single-view settings since they are based on local feature matching.

Synthetic data has been popular used in computer vision , such as in segmentation , object detection , image classification , deblurry , face analysis , etc. In 3D vision, synthetic data is widely used because of 3D data is harder to harvest, e.g., in depth estimation , in optical flow , in finding multi-view correspondence , and for improving the 3D consistency of multi-view diffusion models . Specifically to reconstruction, the

    &  &  &  &  \\   &  & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  GS-LRM & 14.02 & **0.598** & 0.460 & **29.32** & **0.940** & **0.055** & **28.37** & **0.920** & **0.079** & 26.48 & 0.880 & 0.089 \\ _LRM-Zero_ & **14.44** & 0.591 & **0.455** & 25.81 & 0.909 & 0.080 & 25.88 & 0.884 & 0.112 & **28.23** & **0.912** & **0.068** \\   

Table 6: Generalization of _LRM-Zero_ to various evaluation datasets.

    &  &  & result \\   &  &  &  & perceptual & Gaussian scale & view angle & GSO PSNR, \\  & & & loss weight & clipping & threshold & if finished \\  & & & & (default 0.5) & (default -1.2) & (default 60) & \\ 
1 & 100\% & 0\% & 0\% & default & default & default & 29.54 \\ 
2 & 20\% & 80\% & 0\% & default & default & default & failed \\ 
3 & & & & 0.2 & default & default & failed \\ 
4 & 40\% & 60\% & 0\% & 0.2 & default & default & failed \\ 
5 & & & & 0.2 & -1.6 & 40 & 30.32 \\ 
6 & 40\% & 40\% & 20\% & 0.2 & default & default & 30.78 \\   

Table 4: Illustrating the training stability issues when constructing the procedural _Zeroverse_ dataset. The instability can be resolved either with training stabilizing techniques (e.g., reducing perceptual loss weight, Gaussian scale clipping, and view angle threshold), or with reducing the complexity of _Zeroverse_. ‘failed’ experiments are usually due to model divergence.

    &  &  \\   &  & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  NeRF-LRM-Zero & 29.33 & **0.936** & **0.065** & 28.96 & 0.921 & 0.084 \\  NeRF-LRM-Objv & **29.72** & **0.936** & **0.064** & **30.79** & **0.932** & **0.071** \\   

Table 5: NeRF-LRM-Zero performs competitively against NeRF-LRM-Objv.

exploration of synthetic is mainly on specific categories, for example, for face , for human , constructions , or for evaluation . Some synthetic data are template-based  and injecting human's knowledge about the semantic. Xu et al.  and their subsequent works  have leveraged procedurally synthesized data for relighting, view synthesis, and various appearance acquisition and rendering tasks. However, these methods are designed for captures under controlled lighting conditions or objects with specific materials; additionally, their data is created on a relatively small scale. We revisit their procedural data generation workflow, extending it with additional data augmentation techniques and scaling it up to train large reconstruction models.

## 7 Limitations

In this paper, we mainly focus on providing a proof of concept on using synthetic to tackle one of the key problems in 3D vision: 3D reconstructions, and here are part of the limitations.

**Scalability.** The scalability of such synthetic-based method is still under investigation. We have done some initial exploration and the results can be found in Appendix. From these early experiments, it seems that the convergence property and optimal training hyperparameters might be different from the standard experimental setup with real data. The scaling-up exploration would naturally involve more resources (mainly computing resources, i.e., GPU hours) which is beyond our affordability.

Also, the community also lacks a study over the scalability of reconstruction models over'real' data. Objaverse-XL  brings 10 more data over Objaverse but the data is much nosier, has different formats, contains a large portion without textures, and the legal concerns are not fully resolved. All these issues exposes challenges in understanding the scalability of the feed-forward reconstruction method.

**Semantics.** The synthetic data created in the way of Sec. 3 lacks of semantics (e.g., the data distribution is not supposed to match the real 3D world distribution). Thus this data might not be suitable to learn semantical-rich tasks. For the simplest example, _Zeroverse_ is hard to train single-view reconstructions as shown in MCC , Shap-E , LRM , etc, which learn semantic from Objaverse , MvImgNet , and Co3D . At the same time, we can complete single-view reconstruction by chaining with multi-view generator  as shown in , relying on the semantical understanding of multi-view generation. The exact boundary of semantic tasks and intrinsic tasks in 3D vision is still under debate.

## 8 Broader Impacts

The broader impacts of this work are overall positive. First, the proof of concept in using synthesized data would largely reduce the bias inside the real dataset. As the model has weak inductive bias (i.e., through the use of the pure-transformer architecture), the potential semantical bias is mostly from the data. Second, the 3D data are usually having license concerns, where the synthesized data can help resolve. Third, as the 3D reconstruction can be potentially learned from synthetic data without real-world semantic information, we can possibly separate the 3D generation into two problems: generation and reconstruction. The reconstruction is mostly a semantics-free task.

On the other hands, this work can potentially largely lowers the bar of 3D reconstructions, for which data is the main blocker previously. The accessible 3D generation (when chaining with generative models as shown in ) and 3D reconstruction ability may introduce legal concerns on 3D licensing and moral concerns on 3D identities.

## 9 Conclusion

We introduced the _LRM-Zero_ and its training data _Zeroverse_. _Zeroverse_ is constructed with procedural synthesizing, where primitive shapes are composited, textured, and then augmented. We found the LRM model trained with _Zeroverse_ can be competitive with Objaverse-trained LRMs, thus illustrating a promising direction of using synthetic data in 3D reconstruction research. We released our data creation code, and hope that it can help future research.