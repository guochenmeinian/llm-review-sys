# Initialization-Dependent Sample Complexity

of Linear Predictors and Neural Networks

 Roey Magen

Weizmann Institute of Science

roey.magen@weizmann.ac.il

&Ohad Shamir

Weizmann Institute of Science

ohad.shamir@weizmann.ac.il

###### Abstract

We provide several new results on the sample complexity of vector-valued linear predictors (parameterized by a matrix), and more generally neural networks. Focusing on size-independent bounds, where only the Frobenius norm distance of the parameters from some fixed reference matrix \(W_{0}\) is controlled, we show that the sample complexity behavior can be surprisingly different than what we may expect considering the well-studied setting of scalar-valued linear predictors. This also leads to new sample complexity bounds for feed-forward neural networks, tackling some open questions in the literature, and establishing a new convex linear prediction problem that is provably learnable without uniform convergence.

## 1 Introduction

In this paper, we consider the sample complexity of learning function classes, where each function is a composition of one or more transformations given by

\[\;\;f(W)\;,\]

where \(\) is a vector, \(W\) is a parameter matrix, and \(f\) is some fixed Lipschitz function. A natural example is vanilla feed-forward neural networks, where each such transformation corresponds to a layer with weight matrix \(W\) and some activation function \(f\). A second natural example are vector-valued linear predictors (e.g., for multi-class problems), where \(W\) is the predictor matrix and \(f\) corresponds to some loss function. A special case of the above are scalar-valued linear predictors (composed with some scalar loss or nonlinearity \(f\)), namely \( f(^{})\), whose sample complexity is extremely well-studied. However, we are interested in the more general case of matrix-valued \(W\), which (as we shall see) is far less understood.

Clearly, in order for learning to be possible, we must impose some constraints on the size of the function class. One possibility is to bound the number of parameters (i.e., the dimensions of the matrix W), in which case learnability follows from standard VC-dimension or covering number arguments (see Anthony and Bartlett (1999)). However, an important thread in statistical learning theory is understanding whether bounds on the number of parameters can be replaced by bounds on the magnitude of the weights - say, a bound on some norm of \(W\). For example, consider the class of scalar-valued linear predictors of the form

\[\{^{}:, ^{d},\|\| B\}\]

and inputs \(|||| 1\), where \(||||\) is the Euclidean norm. For this class, it is well-known that the sample complexity required to achieve excess error \(\) (w.r.t. Lipschitz losses) scales as \(O(B^{2}/^{2})\), independent of the number of parameters \(d\) (e.g., Bartlett and Mendelson (2002), Shalev-Shwartz and Ben-David (2014)). Moreover, the same bound holds when we replace \(^{}\) by \(f(^{})\) for some \(1\)-Lipschitz function \(f\). Therefore, it is natural to ask whether similar size-independent bounds can be obtained when \(W\) is a matrix, as described above. This question is the focus of our paper.

When studying the matrix case, there are two complicating factors: The first is that there are many possible generalizations of the Euclidean norm for matrices (namely, matrix norms which reduce to the Euclidean norm in the case of vectors), so it is not obvious which one to study. A second is that rather than constraining the norm of \(W\), it is increasingly common in recent years to constrain the distance to some fixed reference matrix \(W_{0}\), capturing the standard practice of non-zero random initialization (see, e.g., Bartlett et al. (2017)). Following a line of recent works in the context of neural networks (e.g., Vardi et al. (2022), Daniely and Granot (2019, 2022)), we will be mainly interested in the case where we bound the _spectral norm_\(||||\) of \(W_{0}\), and the distance of \(W\) from \(W_{0}\) in the _Frobenius norm_\(||||_{F}\), resulting in function classes of the form

\[\{ f(W):W^{n d},||W-W_{0}||_{F}  B\}.\] (1)

for some Lipschitz, possibly non-linear function \(f\) and a fixed \(W_{0}\) of bounded spectral norm. This is a natural class to consider, as we know that spectral norm control is necessary (but insufficient) for finite sample complexity guarantees (see, e.g., Golowich et al. (2018)), whereas controlling the (larger) Frobenius norm is sufficient in many cases. Moreover, the Frobenius norm (which is simply the Euclidean norm of all matrix entries) is the natural metric to measure distance from initialization when considering standard gradient methods, and also arises naturally when studying the implicit bias of such methods (see Lyu and Li (2019)). As to \(W_{0}\), we note that in the case of scalar-valued linear predictors (where \(W,W_{0}\) are vectors), the sample complexity is not affected 1 by \(W_{0}\). This is intuitive, since the function class corresponds to a ball of radius \(B\) in parameter space, and \(W_{0}\) affects the location of the ball but not its size. A similar weak dependence on \(W_{0}\) is also known to occur in other settings that were studied (e.g., Bartlett et al. (2017)).

In this paper, we provide several new contributions on the size-independent sample complexity of this and related function classes, in several directions.

In the first part of the paper (Section 3), we consider function classes as in Eq. (1), without further assumptions on \(f\) besides being Lipschitz, and assuming \(\) has a bounded Euclidean norm. As mentioned above, this is a very natural class, corresponding (for example) to vector-valued linear predictors with generic Lipschitz losses, or neural networks composed of a single layer and some general Lipschitz activation. In this setting, we make the following contributions:

* In subsection 3.1 we study the case of \(W_{0}=0\), and prove that the size-independent sample complexity (up to some accuracy \(\)) is both upper and lower bounded by \(2^{(B^{2}/^{2})}\). This is unusual and perhaps surprising, as it implies that this function class does enjoy a finite, size-independent sample complexity bound, but the dependence on the problem parameters \(B,\) are exponential. This is in very sharp contrast to the scalar-valued case, where the sample complexity is just \(O(B^{2}/^{2})\) as described earlier. Moreover, and again perhaps unexpectedly, this sample complexity remains the same even if we consider the much larger function class of _all_ bounded Lipschitz functions, composed with all norm-bounded linear functions (as opposed to having a single fixed Lipschitz function).
* Building on the result above, we prove a size-independent sample complexity upper bound for deep feed-forward neural networks, which depends only on the Frobenius norm of the first layer, and the product of the spectral norms of the other layers. In particular, it has no dependence whatsoever on the network depth, width or any other matrix norm constraints, unlike previous works in this setting.
* In subsection 3.2, we turn to consider the case of \(W_{0} 0\), and ask if it is possible to achieve similar size-independent sample complexity guarantees. Perhaps unexpectedly, we show that the answer is no, even for \(W_{0}\) with very small spectral norm. Again, this is in sharp qualitative contrast to the scalar-valued case and other settings in the literature involving a \(W_{0}\) term, where the choice of \(W_{0}\) does not strongly affect the bounds.
* In subsection 3.3, we show that the negative result above yields a new construction of a convex linear prediction problem which is learnable (via stochastic gradient descent), but where uniform convergence provably does not hold. This adds to a well-established line of works in statistical learning theory, studying when uniform convergence is provably unnecessary for distribution-freelearnability (e.g., Shalev-Shwartz et al. (2010); Daniely et al. (2011); Feldman (2016); see also Nagarajan and Kolter (2019); Glasgow et al. (2022) in a somewhat different direction).

In the second part of our paper (Section 4), we turn to a different and more specific choice of the function \(f\), considering one-hidden-layer neural networks with activation applied element-wise:

\[\;\;^{}(W)=_{j}u_{ j}(_{j}^{}),\]

with weight matrix \(W R^{n d}\), weight vector \(^{n}\), and a fixed (generally non-linear) Lipschitz activation function \(()\). As before, We focus on an Euclidean setting, where \(\) and \(\) has a bounded Euclidean norm and \(||W-W_{0}||_{F}\) is bounded, for some initialization \(W_{0}\) with bounded spectral norm. In this part, our sample complexity bounds have polynomial dependencies on the norm bounds and on the target accuracy \(\). Our contributions here are as follows:

* We prove a fully size-independent Rademacher complexity bound for this function class, under the assumption that the activation \(()\) is smooth. In contrast, earlier results that we are aware of were either not size-independent, or assumed \(W_{0}=0\). Although we do not know whether the smoothness assumption is necessary, we consider this an interesting example of how smoothness can be utilized in the context of sample complexity bounds.
* With \(W_{0}=0\), we show an upper bound on the Rademacher complexity of deep neural networks (more than one layer) that is fully independent of the network width or the input dimension, and for generic element-wise Lipschitz activations. For constant-depth networks, this bound is fully independent of the network size.

These two results answer some of the open questions in Vardi et al. (2022). We conclude with a discussion of open problems in Section 5. Formal proofs of our results appear in the appendix.

### Comparison to Previous Work

**Deep neural networks and the Frobenius norm.** A size-independent uniform convergence guarantee, depending on the product of Frobenius norm of all layers, has been established in Neyshabur et al. (2015) for constant-depth networks, and in Golowich et al. (2018) for arbitrary-depth networks. However, these bounds are specific to element-wise, homogeneous activation functions, whereas we tackle general Lipschitz activations. Bounds based on other norms include Anthony and Bartlett (1999); Bartlett et al. (2017), but are potentially more restrictive than the Frobenius norm, or not independent of the width. All previous bounds of this type (that we are aware of) strongly depend on various norms of all layers, which can be arbitrarily larger than the spectral norm in a size-independent setting (such as the Frobenius norm and the \((1,2)\)-norm), or make strong assumptions on the activation function.

**Rademacher complexity of vector-valued functions.**Maurer (2016) showed a contraction inequality for Rademacher averages that extended to Lipschitz functions with vector-valued domains. As an application, he showed an upper bound on the Rademacher complexity of vector-valued linear predictors. However, his bound depends polynomially on the number of parameters (i.e. on \(\)), where we focus on size-independent bounds, which do not depend on the input's dimension or number of parameters. The sample complexity of vector-valued predictors has also been extensively studied in the context of multiclass classification (see for example Mohri et al. (2018)). However, these results generally depend polynomially on the vector dimension (e.g., number of classes), and are not size-independent.

**Non-zero initialization.**Bartlett et al. (2017) upper bound the sample complexity of neural networks with non-zero initialization, but they used a much stronger assumption than ours: They control the \((1,2)\)-matrix norm (sum of \(L_{2}\)-norms of the columns), and the gap between this norm and the Frobenius norm can be arbitrarily large, depending on the matrix size. Daniely and Granot (2022) also recently studied the non-zero initialization case, with element-wise activations and Frobenius norm constraints. However, their results in this case are not size-independent and employ a different proof technique than ours.

**Non-element-wise activations.**Daniely and Granot (2022) provide a fat-shattering lower bound for a general (possibly non-element-wise) Lipchitz activation, which implies that neural networks on with bounded Frobenius norm and width \(n\) can shatter \(n\) points with constant margin, assuming that the inputs have norm \(\) and that \(n=O(2^{d})\). However, this lower bound does not separate between the input norm bound and the width of the hidden layer. Therefore, their result does not contradict our upper bound (Thm. 2) which implies that it is possible to achieve a size-independent upper bound on the sample complexity, when the input norm is fixed independent of the network width.

## 2 Preliminaries

**Notations.** We use bold-face letters to denote vectors, and let \([m]\) be shorthand for \(\{1,2,,m\}\). Given a vector \(\), \(x_{j}\) denotes its \(j\)-th coordinate. Given a matrix \(W\), \(_{j}\) is its \(j\)-th row, and \(W_{j,i}\) is its entry in row \(j\) and column \(i\). Let \(0_{n d}\) denote the zero matrix in \(^{n d}\), and let \(I_{d d}\) be the \(d d\) identity matrix. Given a function \(()\) on \(\), we somewhat abuse notation and let \(()\) (for a vector \(\)) or \((M)\) (for a matrix M) denote applying \(()\) element-wise. We use standard big-Oh notation, with \((),(),O()\) hiding constants and \((),(),()\) hiding constants and factors that are polylogarithmic in the problem parameters.

We let \(\|\|\) denote the operator norm: For vectors, it is the Euclidean norm, and for matrices, the spectral norm (i.e., \(\|M\|=_{x:\|\|=1}\|M\|\)). \(\|\|_{F}\) denotes the Frobenius norm (i.e., \(\|M\|_{F}=M_{i,j}^{2}}\)). It is well-known that the spectral norm of a matrix is equal to its largest singular value, and that the Frobenius norm is equal to \(_{i}^{2}}\), where \(_{1},_{2},\) are the singular values of the matrix.

When we say that a function \(f\) is Lipschitz, we refer to the Euclidean metric unless specified otherwise. We say that \(f:^{d}\) is \(\)-smooth if \(f\) is continuously differentiable and its gradient \( f\) is \(\)-Lipschitz.

Given a metric space \((,d)\) and \(>0\), we say that \(N\) is an \(-\)cover for \(\) if for every \(x U\), there exists \(x^{} N\) such that \(d(x,x^{})\). We say that \(P\) is an \(-\)packing (or \(-\)seperated), if for every \(x,x^{} P\) such that \(x x^{}\) we have that \(d(x,x^{})\).

**Sample Complexity Measures.** In our results and proofs, we consider several standard complexity measures of a given class of functions, which are well-known to control uniform convergence, and imply upper or lower bounds on the sample complexity:

* Fat-Shattering dimension: It is well-known that the fat-shattering dimension (at scale \(\)) lower bounds the number of samples needed to learn in a distribution-free learning setting, up to accuracy \(\) (see for example Anthony and Bartlett (2002)). It is formally defined as follows: **Definition 1** (Fat-Shattering).: _A class of functions \(\) on an input domain \(\)_ shatters \(m\)_points_\(_{1},...,_{m}\)_with margin \(\), if there exists a number \(s\), such that for all \(y\{0,1\}^{m}\) we can find some \(f\) such that \[ i[m],\;\;f(_{i}) s-\;\;\;\;y_{i}=0 \;\;\;\;f(_{i}) s+\;\;\;\;y_{i}=1\] _The fat-shattering dimension of F (at scale \(\)) is the cardinality \(m\) of the largest set of points in \(\) for which the above holds._ Thus, by proving the existence of a large set of points shattered by the function class, we get lower bounds on the fat-shattering dimension, which translate to lower bounds on the sample complexity.
* Rademacher Complexity: This measure can be used to obtain upper bounds on the sample complexity: Indeed, the number of inputs \(m\) required to make the Rademacher complexity of a function class \(\) smaller than some \(\) is generally an upper bound on the number of samples required to learn \(\) up to accuracy \(\) (see Shalev-Shwartz and Ben-David (2014)). **Definition 2** (Rademacher complexity).: _Given a class of functions \(\)on a domain \(\), its Rademacher complexity is defined as \(R_{m}()=_{\{_{i}\}_{i=1}^{m}} _{}[_{f}_{i=1}^{m} _{i}f_{i}(_{i})]\;,\) where \(=(_{1},...,_{m})\) is uniformly distributed on \(\{-1,+1\}^{m}\)._
* Covering Numbers: This is a central tool in the analysis of the complexity of classes of functions (see, e.g., Anthony and Bartlett (2002)), which we use in our proofs. **Definition 3** (Covering Number).: _Given any class of functions \(\) from \(\) to \(\), a metric \(d\) over functions from \(\) to \(\), and \(>0\), we let the covering number \(N(,d,)\) denote the minimal number \(n\) of functions \(f_{1},f_{2},...,f_{n}\) from \(\) to \(\), such that for all \(f\), there exists some \(f_{i}\) with \(d(f_{i},f)\). In this case we also say that \(\{f_{1},f_{2},...,f_{n}\}\) is an \(\)-cover for \(\)._

In particular, we will consider covering numbers with respect to the empirical \(L_{2}\) metric defined as \(d_{m}(f,f^{})=_{i=1}^{m}||f(_{i})-f^{ }(_{i})||^{2}}\) for some fixed set of inputs \(_{1},,_{m}\). In addition, if \(\{f_{1},f_{2},...,f_{n}\}\), then we say that this cover is _proper_. It is well known that the distinction between proper and improper covers is minor, in the sense that the proper \(\)-covering number is sandwiched between the improper \(\)-covering number and the improper \(-\)covering number (see the appendix for a formal proof):

**Observation 1**.: _Let \(\) be a class of functions. Then the proper \(\)-covering number for \(\) is at least \(N(,d,)\) and at most \(N(,d,)\)._

## 3 Linear Predictors and Neural Networks with General Activations

We begin by considering the following simple matrix-parameterized class of functions on \(=\{^{d}:|||| 1\}\):

\[_{B,n,d}^{f,W_{0}}:=\{ f(W):W ^{n d},||W-W_{0}||_{F} B\}\,\]

where \(f\) is assumed to be some fixed \(L\)-Lipschitz function, and \(W_{0}\) is a fixed matrix in \(^{n d}\) with a bounded spectral norm. As discussed in the introduction, this can be interpreted as a class of vector-valued linear predictors composed with some Lipschitz loss function, or alternatively as a generic model of one-hidden-layer neural networks with a generic Lipschitz activation function. Moreover, \(W_{0}\) denotes an initialization/reference point which may or may not be \(0\).

In this section, we study the sample complexity of this class (via its fat-shattering dimension for lower bounds, and Rademacher complexity for upper bounds). Our focus is on size-independent bounds, which do not depend on the input dimension \(d\) or the matrix size/network width \(n\). Nevertheless, to understand the effect of these parameters, we explicitly state the conditions on \(d,n\) necessary for the bounds to hold.

**Remark 1**.: _Enforcing \(f\) to be Lipschitz and the domain \(\) to be bounded is known to be necessary for meaningful size-independent bounds, even in the case of scalar-valued linear predictors \( f(^{})\) (e.g., Shalev-Shwartz and Ben-David (2014)). For simplicity, we mostly focus on the case of \(\) being the Euclidean unit ball, but this is without much loss of generality: For example, if we consider the domain \(\{^{d}:|||| b_{x}\}\) in Euclidean space for some \(b_{x} 0\), we can embed \(b_{x}\) into the weight constraints, and analyze instead the class \(_{b_{x}B,n,d}^{f,b_{x}W_{0}}\) over the Euclidean unit ball \(\{^{d}:|||| 1\}\)._

### Size-Independent Sample Complexity Bounds with \(W_{0}=0\)

First, we study the case of initialization at zero (i.e. \(W_{0}=0_{n d}\)). Our first lower bound shows that the size-independent fat-shattering dimension of \(_{B,n,d}^{f,W_{0}}\) (at scale \(\)) is at least exponential in \(B^{2}/^{2}\):

**Theorem 1**.: _For any \(B,L 1\) and \((0,1]\) s.t. \(B^{2}}{128^{2}} 20\), there exists large enough \(d=(L^{2}B^{2}/^{2}),n=((L^{2}B^{2}/^{2}))\) and an \(L\)-Lipschitz function \(f:^{n}\) for which \(_{B,n,d}^{f,W_{0}=0}\) can shatter_

\[(cL^{2}B^{2}/^{2})\]

_points from \(\{^{d}:|||| 1\}\) with margin \(\), where \(c>0\) is a universal constant._

The proof is directly based on the proof technique of Theorem 3 in Daniely and Granot (2022), and differs from them mainly in that we focus on the dependence on \(B,\) (whereas they considered the dependence on \(n,d\)). The main idea is to use the probabilistic method to show the existence of \(_{1},...,_{m}^{d}\) and \(W_{1},...,W_{2^{m}}^{n d}\) for \(m=(B^{2}/^{2})\), with the property that every two different vectors from \(\{W_{y}_{i}:i m,y[2^{m}]\}\) are far enough from each other. We then construct an \(L\)-Lipschitz function \(f\) which assigns arbitrary outputs to all these points, resulting in a shattering as we range over \(W_{1}, W_{2^{m}}\).

We now turn to our more novel contribution, which shows that the bound above is nearly tight, in the sense that we can upper bound the Rademacher complexity of the function class by a similar quantity. In fact, and perhaps surprisingly, a much stronger statement holds: A similar quantity upper bounds the complexity of the much larger class of _all_\(L\)-Lipschitz function \(f\) on \(^{n}\), composed with all norm-bounded linear functions from \(^{d}\) to \(^{n}\):

**Theorem 2**.: _For any \(L,B 1\) and \((0,1]\) s.t. \( 1\), let \(_{L,a,n}\) be the class of all \(L\)-Lipschitz functions from \(\{^{n}:|||| B\}\) to \(\), which equal some fixed \(a\) at \(\). Let \(_{B,n}\) be the class of linear functions from \(^{d}\) to \(^{n}\) over the domain \(\{^{d}:|||| 1\}\) with Frobenius norm at most \(B\), namely_

\[_{B,n}:=\{ W:W^{n d},||W|| _{F} B\}.\]

_Then the Rademacher complexity of \(_{a,L,n}_{B,n}:=\{ g:_{L,a,n},g _{B,n}\}\) on \(m\) inputs from \(\{^{d}:|||| 1\}\) is at most \(\), if \(m()^{B^{2}}{^{2}}}\) for some universal constant \(c>0\)._

Since \(_{B,n,d}^{f,W_{0}=0}_{L,a,n}_{B,n}\) for any fixed \(f\), the Rademacher complexity of the latter upper bounds the Rademacher complexity of the former. Thus, we get the following corollary:

**Corollary 1**.: _Let \(f:^{n}\) be a fixed \(L\)-Lipschitz function. Then the Rademacher complexity of \(_{B,n,d}^{f,W_{0}=0}\) on \(m\) inputs from \(\{^{d}:|||| 1\}\) is at most \(\), if \(m()^{B^{2}}{^{2}}}\) for some universal constant \(c>0\)._

Comparing the corollary and Theorem 1, we see that the sample complexity of Lipschitz functions composed with matrix linear ones is \(((L^{2}B^{2}/^{2}))\) (regardless of whether the Lipschitz function is fixed in advance or not). On the one hand, it implies that the complexity of this class is very large (exponential) as a function of \(L,B\) and \(\). On the other hand, it implies that for any fixed \(L,B,\), it is finite completely independent of the number of parameters. The exponential dependence on the problem parameters is rather unusual, and in sharp contrast to the case of scalar-valued predictors (that is, functions of the form \( f(^{})\) where \(||-_{0}|| B\) and \(f\) is \(L\)-Lipschitz ), for which the sample complexity is just \(O(L^{2}B^{2}/^{2})\).

The key ideas in the proof of Theorem 2 can be roughly sketched as follows: First, we show that due to the Frobenius norm constraints, every function \( f(W)\) in our class can be approximated (up to some \(\)) by a function of the form \( f(_{})\), where the rank of \(_{}\) is at most \(B^{2}/^{2}\). In other words, this approximating function can be written as \(f(UV)\), where \(V\) maps to \(^{B^{2}/^{2}}\). Equivalently, this can be written as \(g(V)\), where \(g(z)=f(Uz)\) over \(^{B^{2}/^{2}}\). This reduces the problem to bounding the complexity of the function class which is the composition of all linear functions to \(^{B^{2}/^{2}}\), and all Lipschitz functions over \(^{B^{2}/^{2}}\), which we perform through covering numbers and the following technical result:

**Lemma 1**.: _Let \(\) be a class of functions from Euclidean space to \(\{^{r}:|||| B\}\). Let \(_{L,a}\) be the class of all \(L\)-Lipschitz functions from \(\{^{r}:|||| B\}\) to \(\), which equal some fixed \(a\) at \(\). Letting \(_{L,a}:=\{ f:_{L,a},f\}\), its covering number satisfies_

\[ N(_{L,a},d_{m},)(1+)^{r}+ N(,d_{m}, ).\]

The proof for this Lemma is an extension of Theorem 4 from Golowich et al. 2018, which considered the case \(r=1\). We emphasize that the exponential dependence on \(r\) arises from the covering of the class \(_{L,a}\), which is achieved by covering its domain \(\{x^{r}:||x|| B\}\) by a set of points \(N_{x}\) of size \((1+BL/)^{r}\) and covering its range \([a-LB,a+LB]\) by a set \(N_{y}\) of size \(2LB/\). As we will show, this implies that the set of all functions from \(N_{x}\) to \(N_{y}\) is a cover for \(_{L,a}\).

**Application to Deep Neural Networks.** Theorem 2 which we established above can also be used to study other types of predictor classes. In what follows, we show an application to deep neural networks, establishing a size/dimension-independent sample complexity bound that depends _only_ on the Frobenius norm of the first layer, and the spectral norms of the other layers (albeit exponentially). This is surprising, since all previous bounds of this type we are aware of strongly depend on various norms of all layers, which can be arbitrarily larger than the spectral norm in a size-independent setting (such as the Frobenius norm), or made strong assumptions on the activation function (e.g., Neyshabur et al. (2015, 2017); Bartlett et al. (2017); Golowich et al. (2018); Du and Lee (2018); Daniely and Granot (2019); Vardi et al. (2022)).

Formally, we consider scalar-valued depth-\(k\) "neural networks" of the form

\[_{k}:=\{x_{k}f_{k-1}(W_{k-1}f_{k-2}(...f_{1}( W_{1})))\ \ :\ ||_{k}|| S_{k}\,\  j\ ||W_{j}|| S_{j}\,\ ||W_{1}||_{F} B\}\]

where each \(W_{j}\) is a parameter matrix of some arbitrary dimensions, \(_{k}\) is a vector and each \(f_{j}\) is some fixed \(1\)-Lipschitz2 function satisfying \(f_{j}()=0\). This is a rather relaxed definition for neural networks, as we do not assume anything about the activation functions \(f_{j}\), except that it is Lipschitz. To analyze this function class, we consider \(_{k}\) as a subset of the class

\[\{x f(W):\|W\|_{F} B\,\ f:^{n} \ \ L\},\]

where \(L=_{j=2}^{k}S_{j}\) (as this clearly upper bounds the Lipschitz constant of \(z_{k}f_{k-1}(W_{k-1}f_{k-2}( f_{1}(z)))\). By applying Theorem 2 (with the same conditions) we have

**Corollary 2**.: _For any \(B,L 1\) and \((0,1]\) s.t. \( 1\), we have that the Rademacher complexity of \(_{k}\) on \(m\) inputs from \(\{^{d}:\|\| 1\}\) is at most \(\), if_

\[m()^{B^{2}}{^{2}}}\]

_where \(L:=_{j=2}^{k}S_{j}\) and \(c>0\) is some universal constant._

Of course, the bound has a bad dependence on the norm of the weights, the Lipschitz parameter and \(\). On the other hand, it is finite for any fixed choice of these parameters, fully independent of the network depth, width, nor on any matrix norm other than the spectral norms, and the Frobenius norm of the first layer only. We note that in the size-independent setting, controlling the product of the spectral norms is both necessary and not sufficient for finite sample complexity bounds (see discussion in Vardi et al. (2022)). The bound above is achieved only by controlling in addition the Frobenius norm of the first layer.

### No Finite Sample Complexity with \(W_{0} 0\)

In subsection 3.1, we showed size-independent sample complexity bounds when the initialization/reference matrix \(W_{0}\) is zero. Therefore, it is natural to ask if it is possible to achieve similar size-independent bounds with non-zero \(W_{0}\). In this subsection we show that perhaps surprisingly, the answer is negative: Even for very small non-zero \(W_{0}\), it is impossible to control the sample complexity of \(_{B,n,d}^{f,W_{0}}\) independent of the size/dimension parameters \(d,n\). Formally, we have the following theorem:

**Theorem 3**.: _For any \(m\) and \((0,]\), there exists \(d=m+1,n=2m\), \(W_{0}^{n d}\) with \(\|W_{0}\|=2\) and a function \(f:^{n}\) which is \(1\)-Lipschitz, for which \(_{B=1,n,d}^{f,W_{0}}\) can shatter \(m\) points from \(\{^{d}:\|\| 1\}\) with margin \(\)._

The theorem strengthens the lower bound of Daniely and Granot (2022) and the previous subsection, which only considered the \(W_{0}=0\) case. We emphasize that the result holds already when \(||W_{0}||\) is very small (equal to \(2\)). Moreover, the proof technique can be used to show a similar result even if we allow for functions Lipschitz w.r.t. the infinity norm (and not just the Euclidean norm as we have done so far), at the cost of a higher required value of \(n\). This is of interest, since non-element-wise activations used in practice (such as variants of the max function) are Lipschitz with respect to that norm, and some previous work utilized such stronger Lipschitz constraints to obtain sample complexity guarantees (e.g., Daniely and Granot (2019)).

Interestingly, the proof of the theorem is simpler than the \(W_{0}=0\) case, and involves a direct non-probabilistic construction. It can be intuitively described as follows: We choose a fixed set of vectors \(_{1},,_{m}\) and a matrix \(W_{0}\) (essentially the identity matrix with some padding) so that \(W_{0}_{i}\) encodes the index \(i\). For any choice of target values \(\{\}^{m}\), we define a matrix \(W^{}_{}\) (which is all zeros except the values of a half column that are located in a strategic location), so that \(W^{}_{}_{i}\) encodes the entire vector \(\). Letting \(W_{}=W^{}_{}+W_{0}\), we get a matrix of bounded distance to \(W_{0}\), so that \(W_{}_{i}\) encodes both \(i\) and \(\). Thus, we just need \(f\) to be the fixed function that given an encoding for \(\) and \(i\), returns \(y_{i}\), hence \( f(W_{})\) shatters the set of points.

### Vector-valued Linear Predictors are Learnable without Uniform Convergence

The class \(^{f,W_{0}}_{B,n,d}\), which we considered in the previous subsection, is closely related to the natural class of matrix-valued linear predictors (\( W\)) with bounded Frobenius distance from initialization, composed with some Lipschitz loss function \(\). We can formally define this class as

\[^{,W_{0}}_{B,n,d}:=\{(,y)(W ;y)\;:\;W^{n d},\|W-W_{0}\|_{F} B \}\;.\]

For example, standard multiclass linear predictors fall into this form. Note that when \(y\) is fixed, this is nothing more than the class \(^{_{y},W_{0}}_{B,n,d}\) where \(_{y}(z)=(z;y)\). The question of learnability here boils down to the question of whether, given an i.i.d. sample \(\{_{i},y_{i}\}_{i=1}^{m}\) from an unknown distribution, we can approximately minimize \(_{(,y)}[(W,y)]\) arbitrarily well over all \(W:\|W-W_{0}\| B\), provided that \(m\) is large enough.

For multiclass linear predictors, it is natural to consider the case where the loss \(\) is also convex in its first argument. In this case, we can easily establish that the class \(^{,W_{0}}_{B,n,d}\) is learnable with respect to inputs of bounded Euclidean norm, regardless of the size/dimension parameters \(n,d\). This is because for each \((,y)\), the function \(W(W;y)\) is convex and Lipschitz in \(W\), and the domain \(\{W:||W-W_{0}||_{F} B\}\) is bounded. Therefore, we can approximately minimize \(_{(,y)}[(W,y)]\) by applying stochastic gradient descent (SGD) over the sequence of examples \(\{_{i},y_{i}\}_{i=1}^{m}\). This is a consequence of well-known results (see for example Shalev-Shwartz and Ben-David (2014)), and is formalized as follows:

**Theorem 4**.: _Suppose that for any \(y\), the function \((.,y)\) is convex and \(L\)-Lipschitz. For any \(B>0\) and fixed matrix \(W_{0}\), there exists a randomized algorithm (namely stochastic gradient descent) with the following property: For any distribution over \((,y)\) such that \(|||| 1\) with probability \(1\), given an i.i.d. sample \(\{(_{i},y_{i})\}_{i=1}^{m}\), the algorithm returns a matrix \(\) such that \(||-W_{0}||_{F} B\) and_

\[_{}[_{(,y)}[(;y) ]-_{W:\|W-W_{0}\|_{F} B}_{(,y)}[(W ;y)]]\;\;}.\]

_Thus, the number of samples \(m\) required to make the above less than \(\) is at most \(L^{2}}{^{2}}\)._

Perhaps unexpectedly, we now turn to show that this positive learnability result is _not_ due to uniform convergence: Namely, we can learn this class, but not because the empirical average and expected value of \((W;y)\) are close uniformly over all \(W:\|W-W_{0}\| B\). Indeed, that would have required that a uniform convergence measure such as the fat-shattering dimension of our class would be bounded. However, this turns out to be false: The class \(^{,W_{0}}_{B,n,d}\) can shatter arbitrarily many points of norm \( 1\), and at any scale \( 1\), for some small \(W_{0}\) and provided that \(n,d\) are large enough3. In the previous section, we already showed such a result for the class \(^{f,W_{0}}_{B,n,d}\), which equals \(^{,W_{0}}_{B,n,d}\) when \(y\) is fixed and \(f(W)=(W;y)\). Thus, it is enough to prove that the same impossibility result (Theorem 3) holds even if \(f\) is a convex function. This is indeed true using a slightly different construction:

**Theorem 5**.: _For any \(m\) and \((0,]\), there exists large enough \(d=(m),n=((m))\), \(W_{0}^{n d}\) with \(\|W_{0}\|=4\) and a **convex** function \(f:^{n}\) which is \(1\)-Lipschitz with respect to the infinity norm (and hence also with respect to the Euclidean norm), for which \(^{f,W_{0}}_{B=1,n,d}\) can shatter \(m\) points from \(\{^{d}:|||| 1\}\) with margin \(\)._

Overall, we see that the problem of learning vector-valued linear predictors, composed with some convex Lipschitz loss (as defined above), is possible using a certain algorithm, but without having uniform convergence. This connects to a line of work establishing learning problems which are provably learnable without uniform convergence (such as Shalev-Shwartz et al. (2010), Feldman (2016)). However, whether these papers considered synthetic constructions, we consider an arguably more natural class of linear predictors of bounded Frobenius norm, composed with a convex Lipschitz loss over stochastic inputs of bounded Euclidean norm. In any case, this provides another example for when learnability can be achieved without uniform convergence.

## 4 Neural Networks with Element-Wise Lipschitz Activation

In section 3 we studied the complexity of functions of the form \( f(W)\) (or possibly deeper neural networks) where nothing is assumed about \(f\) besides Lipschitz continuity. In this section, we consider more specifically functions which are applied element-wise, as is common in the neural networks literature. Specifically, we will consider the following hypothesis class of scalar-valued, one-hidden-layer neural networks of width \(n\) on inputs in \(\{^{d}:|||| b_{x}\}\), where \(()\) is a Lipschitz function on \(\) applied element-wise, and where we only bound the norms as follows:

\[^{,W_{0}}_{b,B,n,d}:=\{^{ }(W):^{n},W^{n d },|||| b,||W-W_{0}||_{F} B\}\,\]

where \(^{}(W)=_{j}u_{j}(_{j}^{ })\). We note that we could have also considered a more general version, where \(\) is also initialization-dependent: Namely, where the constraint \(|||| b\) is replaced by \(||-_{0}|| b\) for some fixed \(_{0}\). However, this extension is rather trivial, since for vectors \(\) there is no distinction between the Frobenius and spectral norms. Thus, to consider \(\) in some ball of radius \(b\) around some \(_{0}\), we might as well consider the function class displayed above with the looser constraint \(|||| b+||_{0}||\). This does not lose much tightness, since such a dependence on \(||_{0}||\) is also necessary (see remark 2 below).

The sample complexity of \(^{,W_{0}}_{b,B,n,d}\) was first studied in the case of \(W_{0}=0\), with works such as Neyshabur et al. (2015, 2017), Du and Lee (2018), Golowich et al. (2018), Daniely and Granot (2019) proving bounds for specific families of the activation \(()\) (e.g., homogeneous or quadratic). For general Lipschitz \(()\) and \(W_{0}=0\), Vardi et al. (2022) proved that the Rademacher complexity of \(^{,W_{0}=0}_{b,B,n,d}\) for any \(L\)-Lipschitz \(()\) is at most \(\), if the number of samples is \(((L}{})^{2}).\) They left the case of \(W_{0} 0\) as an open question. In a recent preprint, Daniely and Granot (2022) used an innovative technique to prove a bound in this case, but not a fully size-independent one (there remains a logarithmic dependence on the network width \(n\) and the input dimension \(d\)). In what follows, we prove a bound which handles the \(W_{0} 0\) case and is fully size-independent, under the assumption that \(()\) is smooth. The proof (which is somewhat involved) involves techniques different from both previous papers, and may be of independent interest.

**Theorem 6**.: _Suppose \(()\) (as function on \(\)) is \(L\)-Lipschitz, \(\)-smooth (i.e, \(^{}()\) is \(\)-Lipchitz) and \((0)=0\). Then for any \(b,B,n,D,>0\) such that \(Bb_{x} 2\), and any \(W_{0}\) such that \(||W_{0}|| B_{0}\), the Rademacher complexity of \(^{,W_{0}}_{b,B,n,d}\) on m inputs from \(\{^{d}:|||| b_{x}\}\) is at most \(\), if \(m}((1+bb_{x}(LB_{0}+(+L)B(1+B_{ 0}b_{x}))^{2})\)._

Thus, we get a sample complexity bounds that depend on the norm parameters \(b,b_{x},B_{0}\), the Lipschitz parameter \(L\), and the smoothness parameter \(\), but is fully independent of the size parameters \(n,d\). Note that for simplicity, the bound as written above hides some factors logarithmic in \(m,B,L,b_{x}-\) see the proof in the appendix for the precise expression.

We note that if \(W_{0}=0\), we can take \(B_{0}=0\) in the bound above, in which case the sample complexity scales as \((((+L)bb_{x}B)^{2}/^{2})\). This is is the same as in Vardi et al. (2022) (see above) up to the dependence on the smoothness parameter \(\).

**Remark 2**.: _The upper bound on Theorem 6 depends quadratically on the spectral norm of \(W_{0}\) (i.e., \(B_{0}\)). This dependence is necessary in general. Indeed, even by taking the activation function \(()\) to _be the identity, \(B=0\) and \(b=1\) we get that our function class contains the class of scalar-valued linear predictors \(\{^{}:, ^{d},|||| B_{0}\}\). For this class, it is well known that the number of samples should be \((^{2}}{^{2}})\), to ensure that the Rademacher complexity of that class is at most \(\)._

### Bounds for Deep Networks with Lipschitz Activations

As a final contribution, we consider the case of possibly deep neural networks, when \(W_{0}=0\) and the activations are Lipschitz and element-wise. Specifically, given the domain \(\{^{d}:|||| b_{x}\}\) in Euclidean space, we consider the class of scalar-valued neural networks of the form

\[_{k}^{}_{k-1}(W_{k-1}_{k-2}(..._{1}((W_{1})))\]

where \(_{k}\) is a vector (i.e. the output of the function is in \(\)) with \(||_{k}|| b\), each \(W_{j}\) is a parameter matrix s.t. \(||W_{j}||_{F} B_{j},\ ||W_{j}|| S_{j}\) and each \(_{j}()\) (as a function on \(\)) is an \(L\)-Lipschitz function applied element-wise, satisfying \(_{j}(0)=0\). Let \(^{\{_{j}\}}_{k,\{S_{j}\},\{B_{j}\}}\) be the class of neural networks as above. Vardi et al. (2022) proved a sample complexity guarantee for \(k=2\) (one-hidden-layer neural networks), and left the case of higher depths as an open problem. The theorem below addresses this problem, using a combination of their technique and a "peeling" argument to reduce the complexity bound of networks of depth \(k\) to those of depth \((k-1)\). The resulting bound is fully independent of the network width (although strongly depends on the network depth), and is the first of this type (to the best of our knowledge) that handles general Lipschitz activations under Frobenius norm constraints.

**Theorem 7**.: _For any \(,b>0,\{B_{j}\}_{j=1}^{k-1},\{S_{j}\}_{j=1}^{k-1},L\) with \(S_{1},...,S_{k-1},L 1\), the Rademacher complexity of \(^{\{_{j}\}}_{k,\{S_{j}\},\{B_{j}\}}\) on \(m\) inputs from \(\{^{d}:|||| b_{x}\}\) is at most \(\), if_

\[mbR_{k-2}^{}(m)_{i=1}^{ k-1}B_{i}\ )^{2}}{^{2}}\,\]

_where \(R_{k-2}=b_{x}L^{k-2}_{i=1}^{k-2}S_{i}\), \(R_{0}=b_{x}\) and \(c>0\) is a universal constant._

We note that for \(k=2\), this reduces to the bound of Vardi et al. (2022) for one-hidden-layer neural networks.

## 5 Discussion and Open Problems

In this paper, we provided several new results on the sample complexity of vector-valued linear predictors and feed-forward neural networks, focusing on size-independent bounds and constraining the distance from some reference matrix. The paper leaves open quite a few avenues for future research. For example, in Section 3, we studied the sample complexity of \(^{f,W_{0}}_{B,n,d}\) when \(n,d\) are unrestricted. Can we get a full picture of the sample complexity when \(n,d\) are also controlled? Even more specifically, can the lower bounds in the section be obtained for any smaller values of \(d,n?\) As to the results in Section 4, is our Rademacher complexity bound for \(^{,W_{0}}_{b,B,n,d}\) (one-hidden-layer networks and smooth activations) the tightest possible, or can it be improved? Also, can we generalize the result to arbitrary Lipschitz activations? In addition, what is the sample complexity of such networks when \(n,d\) are also controlled? In a different direction, it would be very interesting to extend the results of this section to deeper networks and non-zero \(W_{0}\).