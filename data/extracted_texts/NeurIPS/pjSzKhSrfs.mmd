# Wasserstein Quantum Monte Carlo:

A Novel Approach for Solving

the Quantum Many-Body Schrodinger Equation

 Kirill Neklyudov

Vector Institute

Jannes Nys

Institute of Physics & Center for Quantum Science and Engineering

Ecole Polytechnique Federale de Lausanne (EPFL)

&Luca Thiede

Vector Institute

Univeristy of Toronto

Juan Felipe Carrasquilla

Vector Institute

Univeristy of Waterloo

Qiang Liu

UT Austin

AI4Science

Max Welling

Microsoft Research

AI4Science

Alireza Makhzani

Vector Institute

Univeristy of Toronto

###### Abstract

Solving the quantum many-body Schrodinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher-Rao gradient flow in this distributional space, followed by a projection step onto the variational manifold. This perspective provides us with a principled framework to derive new QMC algorithms, by endowing the distributional space with better metrics, and following the projected gradient flow induced by those metrics. More specifically, we propose "Wasserstein Quantum Monte Carlo" (WQMC), which uses the gradient flow induced by the Wasserstein metric, rather than the Fisher-Rao metric, and corresponds to _transporting_ the probability mass, rather than _teleporting_ it. We demonstrate empirically that the dynamics of WQMC results in faster convergence to the ground state of molecular systems.

## 1 Introduction

Access to the wave function of a quantum many-body system allows us to study strongly correlated quantum matter, starting from the fundamental building blocks. For example, the solution of the time-independent electronic Schrodinger equation provides all the chemical properties of a given atomic state, which have numerous applications in chemistry and materials design. However, obtaining the exact wave function is fundamentally challenging, with a complexity scaling exponentially withthe number of degrees of freedom. Various computational techniques have been developed in the past, including compression techniques based on Tensor Networks (White, 1992), and stochastic approaches such as Quantum Monte Carlo (QMC) (Ceperley et al., 1977). Quantum Variational Monte Carlo (QVMC) (McMillan, 1965; Ceperley et al., 1977) is a well-known subclass of the latter that can, in principle, be used to estimate the lowest-energy state (i.e. ground state) of a quantum many-body system. The method operates by parameterizing the trial wave function and minimizing the energy of the many-body system w.r.t. the model parameters.

The choice of parametric family of the trial wave function is a crucial component of the QVMC framework. Naturally, deep neural networks, being a family of universal approximators, have demonstrated promising results for quantum systems with discrete (Carleo and Troyer, 2017; Choo et al., 2020; Hibat-Allah et al., 2020), as well as continuous degrees of freedom (Pfau et al., 2020; Hermann et al., 2020; Pescia et al., 2022; Gnech et al., 2022; von Glehn et al., 2022). However, the optimization process is challenging, especially for rich parametric families of the trial wave functions. This requires the use of advanced optimization techniques that take into account the geometry of the parametric manifold. The most common technique used in QVMC is referred to as 'Stochastic Reconfiguration' (SR) (Sorella, 1998), and can be seen as the quantum version of Natural Gradient Descent (Stokes et al., 2020). While for large neural networks with up to millions of parameters, efficient and scalable implementations of SR are available (Vicentini et al., 2022), it is also possible to use approximate methods such as K-FAC (Martens and Grosse, 2015; Pfau et al., 2020). Higher order optimization techniques are considered to be essential to obtain the necessary optimization performance to accurately estimate ground states of quantum many-body Hamiltonians (see e.g. (Pescia et al., 2023; Pfau et al., 2020)). Therefore, studies of the optimization procedure are an important direction for further development of the QVMC approach.

In this paper, we consider the energy minimization dynamics as a gradient flow on the non-parametric manifold of distributions. First, as an example of the proposed methodology, we demonstrate that the imaginary-time Schrodinger equation can be described as the gradient flow under the Fisher-Rao metric on the non-parametric manifold. Then, the QVMC algorithm can be seen as a projection of this gradient flow onto a parametric manifold (see Section 3 for details). Second, the gradient flow perspective gives us an additional degree of freedom in the algorithm. Namely, we can choose the metric under which we define the gradient flow. Thus, we propose and study a different energy-minimizing objective function, which we derive as a gradient flow under the Wasserstein metric (or Wasserstein Fisher-Rao metric) (Chizat et al., 2018; Kondratyev et al., 2016).

In practice, we demonstrate that incorporating the Wasserstein metric into the optimization procedure allows for faster convergence to the ground state. Namely, we demonstrate up to \(10\) times faster convergence of the variance of the local energy for chemical systems. Intuitively, incorporating the Wasserstein metric regularizes the density evolution by forbidding or regularizing non-local probability mass "teleportation" (as done by Fisher-Rao metric). This might facilitate faster mixing of the MCMC running along with the density updates.

## 2 Background

### Quantum variational Monte Carlo

Consider a quantum many-body system subject to the Hamiltonian operator, which we will assume to be of the following form,

\[H=-_{x}^{2}+V.\] (1)

where \(x\) a given many-body configuration and \(V\) is the potential operator. The time-dependent Schrodinger equation determines the wave function \((x,t)\) of the quantum system

\[i}{t}(x,t)=H(x,t)\] (2)

As is often the case, we will target the stationary solutions, for which we focus on solving the time-independent Schrodinger equation

\[H(x)=E(x)\] (3)

where \(E\) is the energy of the state \(\). The ground state of a quantum system is obtained by solving the time-independent Schrodinger equation, by targeting the eigenstate \(\) of the above Hamiltonian with the lowest eigenvalue (energy) \(E\). Hereby, we must restrict the Hilbert space to wave functions that are antisymmetric under particle permutations in the case of fermionic particles, and symmetric for bosons. The latter takes into account the indistinguishability of the particles. Given the Born density \(q(x)=|(x)|^{2}\), the energy of a given quantum state can be rewritten in a functional form,

\[E[]=_{q(x)}[E_{}(x)],\ \ \ E_{}(x) \] (4)

We will focus on the case where the Hamiltonian operator is Hermitian and time-reversal symmetric. In this case, its eigenfunctions and eigenvalues are real, and the energy can be recast into a functional of the Born probability density (see also Pfau et al. (2020), where the expressions are given in terms of \(\))

\[E[q]=_{q(x)}[E_{}(x)],\ \ \ E_{}(x)=V(x)- _{x}^{2} q(x)-_{x} q(x)^{2},\] (5)

under the strong condition that \(q(x)\) is the Born probability density derived from an (anti-)symmetric wave function: \(q(x)=^{2}(x)\). The latter will always be tacitly assumed from hereon.

The Rayleigh-Ritz principle guarantees that the \(E[q]\) is lower-bounded by the true ground-state energy of the system, i.e. \(E[q] E_{0}\), if the corresponding wave function \(\) is a valid state of the corresponding Hilbert space. Quantum Variational Monte Carlo (QVMC) targets ground states by parametrizing the wavefunction \((x,)\) and by minimizing \(E[q()]\). The solution to the minimization problem \(_{0}=_{}E[q()]\) is obtained by gradient-based methods using the following expression for the gradient w.r.t. parameters \(\)

\[_{}E[q()]=_{q(x,)}E_{}(x,)-_{q(x,)}[E_{}(x,)] _{} q(x,).\] (6)

In sum, the above leads to an iterative procedure in which Monte Carlo sampling is used to generate configurations from the current trial state \(q(x,)=^{2}(x,)\), which allows computing the corresponding energy and its parameter gradients, and to update the model accordingly. In practice, the parametric model specifies the density \(q(x,)\) only up to a normalization constant, i.e., it outputs \((x,) q(x,)\). However, the gradient w.r.t. \(\) does not depend on the normalization constant; hence, throughout the paper, we refer to the model as the normalized density \(q(x,)\).

### Gradient flows under the Wasserstein Fisher-Rao metric

In the previous section, we introduced QVMC in terms of Born probability functions and formulated the problem as the minimization of a functional of probability functions constrained to a variational/parametric manifold. The latter is a more common problem often tackled in machine learning, and by forging connections between both fields, we will be able to derive an alternative to QVMC.

Gradient FlowsIn Euclidean space, we can minimize a function \(f:^{d}\) by following the ODE \(}{t}x_{t}=-_{x}f(x_{t})\), which can be viewed as the continuous version of standard gradient descent. Similarly, we can minimize a functional in the space of probability distributions (or in general any Riemannian manifold), by following an ODE on this manifold. However the notion of a gradient on a manifold is more complicated, and relies on the Riemannian metric that the manifold is endowed with. Different Riemannian metrics result in different gradient flows, and consequently different optimization dynamics. For a thorough analysis of gradient flows, we refer the reader to Ambrosio et al. (2005).

Wasserstein Fisher-Rao gradient flowsConsider the space of distributions \(_{2}\) with finite second moment. This space can be endowed with a Wasserstein Fisher-Rao metric with the corresponding distance. In particular, the _Wasserstein Fisher-Rao_ (WFR) distance (Chizat et al., 2018) is defined by extending the Benamou & Brenier (2000) dynamical optimal transport formulation by a term involving the norm of the growth rate \(g_{t}\), and by accounting for the growth term in the modified continuity equation. Namely, the distance between probability densities \(p_{0}\) and \(p_{1}\) is defined as

\[_{}(p_{0},p_{1})^{2} _{v_{t},g_{t},q_{t}}_{0}^{1}\ _{q_{t}(x)} v_{t}(x)^{2}+ g_{t}(x)^{2} \,t,\ \ \ \] (7) \[(x)}{ t} =-_{x}(q_{t}(x)v_{t}(x))+g_{t}(x)q_{t}(x)\,,\ \ \ \ \ q_{0}(x)=p_{0}(x),\ \ q_{1}(x)=p_{1}(x)\,,\]where \(v_{t}(x)\) is the vector field defining the probability flow, \(g_{t}(x)\) is the growth term controlling the creation and annihilation of the probability mass, and \(\) is the coefficient balancing the transportation and teleportation costs. Note that by setting one of the terms to zero we get 2-Wasserstein distance (\(g_{t}(x) 0\)) and Fisher-Rao distance (\(v_{t}(x) 0\)). In Section 3, we also consider the general case of \(c\)-Wasserstein distance, where \(c\) is a convex cost function on the tangent space.

Given a functional on this manifold, \(F[q]:_{2}\), we can define the gradient flow of the function \(F\) under any Riemannian metric including the Wasserstein metric, the Fisher-Rao metric, or the Wasserstein Fisher-Rao metric. For example, the gradient flow that minimizes the functional \(F[q]\) under the Wasserstein Fisher-Rao metric is given by the following PDE (which is shown with detailed derivations in Appendix A)

\[}{ t}(x)= q_{t}(x)-_{x}]}{ q_{t}}( x)}_{}-]}{  q_{t}}(x)-_{q_{t}(y)}]}{ q _{t}}(y)}_{}q_{t}(x),\] (8)

where \( F[q]/ q\) is the first-variation of of \(F\) with respect to the \(L_{2}\) metric. The physical explanation of the terms in Eq. (8) is as follows. The continuity equation defines the change of the density when the samples \(x q_{t}(x)\) follow the vector field \(v_{t}(x)=-_{x} F[q_{t}]/ q_{t}\). The second term of the PDE defines the creation and annihilation of probability mass, and is proportional to the growth field \(g_{t}(x)=]}{ q_{t}}(x)-_{q_{t}(y)}[ ]}{ q_{t}}(y)]\). Note that \(_{q_{t}}[g_{t}]=0\), and so while mass can be "teleported", the total mass (or probability) will remain constant. The two mechanisms can be considered independently by defining the evolution terms under the 2-Wasserstein and Fisher-Rao metrics respectively, i.e.

\[}{ t}(x) =-_{x}q_{t}(x)-_{x}]}{ q_{t}}(x),,\] (9) \[}{ t}(x) =-]}{ q_{t}}(x)-_{q _{t}(y)}]}{ q_{t}}(y)q_{t} (x),.\] (10)

It now becomes evident that the stationary condition for all the considered PDEs is

\[_{x}]}{ q_{t}}(x)=0 ]}{ q_{t}}(x)\,.\] (11)

In Appendix A, we provide derivations illustrating that Eqs. (8) to (10) correspond to the gradient flow under the Wasserstein Fisher-Rao, Wasserstein, and Fisher-Rao metrics, respectively, and hence they all minimize \(F[q]\). For detailed analysis, we refer the reader to Kondratyev et al. (2016); Liero et al. (2016).

## 3 Methodology

In Section 3.1, we first demonstrate that the imaginary-time evolution of the Schrodinger equation can be viewed as a gradient flow under the Fisher-Rao metric. Afterwards, in Section 3.2, we discuss how a density evolution can be projected to the parametric variational family and show that doing so for the Fisher-Rao gradient flow yields the QVMC algorithm. Taking this perspective, we propose the Wasserstein Quantum Monte Carlo by considering Wasserstein (and Wasserstein Fisher-Rao) gradient flows, followed by the projection onto the parametric manifold (see Section 3.3).

### Imaginary-Time evolution as the gradient flow under the Fisher-Rao metric

The ground state of a quantum system can in theory be obtained by imaginary-time evolving any valid quantum state \(\) (with a non-vanishing overlap with the true ground state) to infinite times. The state is evolved according to the imaginary-time Schrodinger equation, which defines the energy-minimizing time evolution of the wavefunction \(_{t}\), and is expressed as the following PDE (which is the Wick-rotated version of Eq. (2), see e.g. (McArdle et al., 2019; Yuan et al., 2019)),

\[(x)}{ t}=\ -(H-E[_{t}])_{t}(x),\] (12)

where again \(q_{t}(x)=_{t}^{2}(x)\). The last term proportional to the energy \(E[_{t}]\) comes from enforcing normalization (contrary to real-time evolution, imaginary time evolution is non-unitary).

**Theorem 3.1**.: _Eq. (12) defines the gradient flow of the energy functional \([q]\) under the Fisher-Rao metric._

Proof Sketch.: The energy functional \(E[q]\) has the following derivative

\[(x)=V(x)-_{x}^{2} q(x)- {1}{8}\|_{x} q(x)\|^{2}=E_{ loc}(x).\] (13)

Thus, the gradient flow under the Fisher-Rao metric is (see Eq. (10))

\[(x)}{ t}=-E_{ loc}(x)-_{q_ {t}(x)}[E_{ loc}(x)]q_{t}(x),\] (14)

which is equivalent (up to a multiplicative constant) to the imaginary-time Schrodinger Equation in Eq. (12) as shown in the complete proof in Appendix B. 

We believe that this result can be derived following the derivations from Stokes et al. (2020), but not introducing the manifold of parametric distributions. However, considering the evolution of the density on the non-parametric manifold first helps us to derive our method and relating it to QVMC. In the following subsection, we discuss how to project this non-parametric evolution to a parametric manifold.

### Following the gradient flow by a parametric model

By choosing a metric in the distributional space and following the energy-minimizing gradient flows, we can design various algorithms for estimating the ground state wave function. Indeed, in principle, by propagating the samples or the density according to any gradient flow (e.g., Eqs. (8) to (10)), we can eventually reach the ground state. However, these dynamics are defined on the non-parametric and infinite-dimensional manifold of distributions, which do not allow tractable computation of log densities, and thus tractable evolution. Therefore, we project these dynamics onto the parametric manifold of our variational family, and follow the _projected_ gradient flows instead, which is tractable.

Suppose the current density on the parametric manifold is \(q_{t}(x)=q(x,)\) (see Figure 1). We first evolve this density using a (non-parametric) gradient flow method (e.g., Eqs. (8) to (10)) for time \( t\), which will take \(q_{t}(x)\) off the parametric manifold to \(q_{t+ t}(x)\). We then have to update current trial model \(q(x,)\) to match \(q_{t+ t}(x)\) enabling us to propagate the density further. In order to do so, we define the optimal update of parameters \(^{*}\) as the minimizer of the Kullback-Leibler divergence between \(q_{t+ t}(x)\) and the distributions on the parametric manifold, i.e.

\[^{*}=*{arg\,min}_{ \\ \|\|_{2}=1}D_{ KL}(q_{t+ t}(x)\|q(x,+ ))\,.\] (15)

Figure 1: W(FR)QMC: A graphical illustration of the gradient flow according to the Wasserstein and Fisher–Rao metrics, and the corresponding projection onto the variational manifold \(q(x,)\).

[MISSING_PAGE_EMPTY:6]

\(c\)-Wasserstein MetricThis result can be further generalized to the \(c\)-Wasserstein metric with any convex cost function \(c:^{d}\) on the tangent space. The \(c\)-Wasserstein distance between \(p_{0}\) and \(p_{1}\) is defined as follows

\[W_{c}(p_{0},p_{1}) _{v_{t},q_{t}}_{0}^{1}\ _{q_{t}(x)}[c(v_{t}(x))]\, t,\ \ \ \] (21) \[(x)}{ t} =-_{x}(q_{t}(x)v_{t}(x)),\ \ \ \ \ q_{0}=p_{0},\ \ q_{1}=p_{1}\,.\] (22)

**Proposition 3.5**.: _The energy-minimizing \(c\)-Wasserstein gradient flow is defined by the following equation_

\[(x)}{ t}=-_{x}(q_{t}(x) c^{ *}(-_{x}E_{}(x))),\] (23)

_where \(c^{*}()\) is the convex conjugate function of \(c()\), and \( c^{*}(y)\) is its gradient at \(y\)._

Proof.: See Appendix D. 

Theorem 3.4 can be viewed as a special case of Proposition 3.5 where \(c()=^{2}\). Introducing a different \(c\) than \(L^{2}\) norm translates to a non-linear transformation of the gradient \(-_{x}E_{}(x)\). In Appendix D, we demonstrate how to choose \(c\) such that it corresponds to the coordinate-wise application of \(\) to the gradient, which we use in practice.

Finally, using Proposition 3.5 in Eq. (17), we get the expression for the parameter update, i.e.

\[^{*} q_{t}(x)_{} c^{* }(-_{x}E_{}(x)),_{x} q(x,)\, x.\] (24)

Similar to the discussion of the previous section for QVMC, we can precondition the gradient with the Fisher Information Matrix, exploiting the geometry of the parametric manifold.

In Algorithm 1, we provide a pseudocode for the proposed algorithms. The procedure follows closely QVMC but introduces a different objective. When using gradients both from Eqs. (18) and (24), we follow the gradient flow under the Wasserstein Fisher-Rao metric with the coefficient \(\). For \(\), the cost of mass teleportation becomes infinite and we use only the gradient from Eq. (24), which corresponds to the gradient flow under the \(c\)-Wasserstein metric (we refer to this algorithm as WQMC). For \( 0\), the cost of mass teleportation becomes negligible compared to the transportation cost and the resulting algorithm becomes QVMC, which uses the gradient from Eq. (18). In practice, we consider the extreme cases (\( 0,\)) and the mixed case \(=1\).

```
0: samples \(\{x^{(i)}\}_{i=1}^{N} q_{t=0}(x)\)
0: potential function \(V(x)\) while not converged do \(E_{}(x^{(i)})=V(x^{(i)})-_{x}^{2} q(x^{(i)}, )- q(x^{(i)},)}^{2}\) (see Eq. 5) \(_{x}E_{}(x^{(i)})=(_{x}E_{}(x^{(i)}))\) \(^{*}=_{i}^{N}_{} c^ {*}-_{x}E_{}(x^{(i)}),_{x} q(x^{(i)}, )\) (see Eq. 24) \(E_{}(x^{(i)})=(E_{}(x^{(i)}))\) \(^{*}=-_{j}^{N}[E_{}(x^{(i)})-_{j}^{N}E_{}(x^{(j)})]_{} q(x^{(i)}, )\) (see Eq. 18) \(^{}=(,_{}^{-1}^{*})\) (see Eq. 19) update \(x^{(i)}\) by sampling from \(q(x,^{})\) via MCMC endwhile return model \(q(x,^{*})\), samples \(\{x^{(i)}\}_{i=1}^{N} q(x,^{*})\) ```

**Algorithm 1**W(FR)QMC

## 4 Experiments 1

For the empirical study of the proposed method, we consider Born-Oppenheimer approximation of chemical systems. Within this approximation, the wave function of the electrons in a molecule can be studied separately from the wave function of the atomic nuclei. Namely, we consider the following Hamiltonian

\[H=-_{x}^{2}+_{i<j}-x_{j}\|}-_{i,I} }{\|x_{i}-X_{I}\|}+_{I<J}Z_{J}}{\|X_{I}-X_{J}\|}\,,\] (25)

where \(x_{i}\) are the coordinates of electrons, \(X_{I},Z_{I}\) are the coordinates and charges of nuclei. The first kinetic term contains derivatives with respect to the electron positions \(x\). Indeed, the positions of the nuclei are given and fixed, and we target the ground state of the electronic wave function \((x)\), which is an explicit function of the electron positions only. Solving the electronic Schrodinger equation is a notoriously difficult task, and is a topic of intense research in quantum chemistry and material sciences.

Figure 2: Optimization results for different chemical systems (every column corresponds to a given molecule). The number of electrons is given in the brackets next to systems’ names. Throughout the optimization, we monitor three values: the mean value of the local energy (lower is better), the variance of the local energy, and the median value of the gradient norm of the local energy. In the first row of plots, we average (removing \(5\%\) of outliers from both sides) the energy over \(1000\) iterations and report the relative error to the actual ground-state energy: \((E-E_{0})/E_{0}\). In the second row, we report standard deviation averaged over \(1000\) iterations (removing \(5\%\) of outliers from both sides). In the third row, we report the median gradient norm averaged over \(1000\) iterations (removing \(5\%\) of outliers from both sides). See the descriptions of methods in the text.

Since electrons are indistinguishable fermions, we restrict the Hilbert space to states \(\) that are antisymmetric under electron permutations (see Section 2.1). This can be achieved by incorporating Slater determinants into the deep neural network, which parametrizes the wave function \((x,)\), as proposed in various recent works (Luo and Clark, 2019; Hermann et al., 2020; Pfau et al., 2020; Hermann et al., 2022). The density is then given by the Born rule \(q(x,)=|(x,)|^{2}\). For all our experiments, we follow (von Glehn et al., 2022) and use the "psiformer" architecture together with preconditioning the gradients via K-FAC (Martens and Grosse, 2015).

In our method, we apply several tricks which stabilize the optimization and improve convergence speed. Firstly, we have observed that applying a \(\) non-linearity coordinate-wise to the gradient \(_{x}E_{}(x)\) significantly improves convergence speed. This corresponds to a different cost function in the Wasserstein metric, as we discuss in Proposition 3.5 and Appendix D. Also, we remove samples from the batch whose norm \(\|_{x} q(x,)\|\) significantly exceeds the median value. Namely, we estimate the deviation from the norm as \(_{q(x,)}[\|_{x} q(x,)\|-(\| _{x} q(x,)\|)\|]\) and remove samples whose norm exceeds five deviations from the median. When including the gradient from Eq. (18), we clip the local energy values as proposed in (von Glehn et al., 2022), i.e. by estimating the median value and clipping to five deviations from the median, where the deviation is estimated in the same way as for the norm of the gradient.

We consider different chemical systems and compare against QVMC as a baseline. We run our novel method with the same architecture and hyperparameters as the baseline QVMC-based approach in (von Glehn et al., 2022). For the chemical systems, we consider Be, and B atoms, the Li\({}_{2}\) molecule and the hydrogen chain H\({}_{10}\) from (Hermann et al., 2020). The exact values of energies for Be, B, Li\({}_{2}\) are taken from (Pfau et al., 2020), the exact value of the energy for H\({}_{10}\) is from (Hermann et al., 2020). All the hyperparameters and architectural details are provided in the supplementary material.

In Figure 2, we demonstrate the convergence plots for the baseline (QVMC) and the proposed methods (WQMC and W(FR)QM, see Algorithm 1). For all the considered systems, both WQMC and W(FR)QMC yield more precise estimations of the ground state energy (the first row of Figure 2). To assess convergence, we also monitor the variance of the local energy and the gradient norm of the local energy. As we discuss in Eq. (11), both metrics must vanish at the ground state. More fundamentally, the variance of the local energy can be shown to vanish for eigenstates of the Hamiltonian (Wu et al., 2023), referred to as the zero-variance property. First, we point out that obtaining the ground states of the considered molecules with QVMC is challenging, and even with powerful deep-learning architecture, discrepancies remain with the ground state. Since we use existing state-of-the-art architectures as a backbone, our results are also limited by the limitations of the latter (Gao and Gunnemann, 2023). Developing novel architectures is out of the scope of this work.

However, in Figure 2, we clearly observe that both WQMC and W(FR)QMC yield significantly faster convergence of the aforementioned metrics compared to QVMC. In particular, for the larger molecules Li\({}_{2}\) and H\({}_{10}\), we observe that we consistently obtain lower energies within \(10\)k steps (\(20\)k for H\({}_{10}\)) with a more stable convergence. For the smaller molecules we observe that QVMC obtains lower energies in the first few iterations, but its convergence slows down significantly, after which our approach steadily yields improved energies below QVMC. Overall, our experiments demonstrates that taking into account the Wasserstein metric allows for faster convergence to accurate approximations of the ground state. See the final metrics in Table 1.

 Method name & QVMC & WQMC & W(FR)QMC & QVMC & WQMC & W(FR)QMC \\  Molecule &  &  \\  Relative energy error & 1.50e-5 & 3.79e-6 & **1.04e-6** & 9.01e-6 & 9.58e-6 & **2.69e-6** \\ Energy variance (Ha\({}^{2}\)) & 5.53e-3 & 1.08e-3 & **1.07e-3** & 1.96e-2 & 1.84e-2 & **1.19e-2** \\  Molecule & _{2}\) (6)} & _{10}\) (10)} \\  Relative energy error & 4.43e-5 & 3.71e-5 & **3.66e-5** & 4.24e-4 & 3.90e-4 & **3.88e-4** \\ Energy variance (Ha\({}^{2}\)) & 7.21e-3 & 5.76e-4 & **5.59e-4** & 1.89e-3 & **2.30e-4** & 2.45e-4 \\ 

Table 1: Energy and variances estimates for all systems after \(10\)k iterations (\(20\)k for H\({}_{10}\)).

## 5 Limitations

Since our method requires an extra gradient evaluation compared to QVMC, we include the runtime of all the algorithms in Figure 3, Appendix E. Namely, for a proper comparison, instead of reporting the metrics per iteration, we report them per wall time in seconds. Note that all the claims of the experimental section still hold in terms of wall time. All the models were benchmarked on four A40 GPUs. Third-order derivatives can be efficiently computed using modern deep learning frameworks such as JAX (Bradbury et al., 2018), which we used to implement our method.

Potentially, one can alleviate the extra cost of the iteration by coming up with more efficient Monte Carlo schemes or other updates of the samples. Indeed, in our experiments, we observed that the proposed method requires fewer MCMC steps (not included in the paper). Moreover, one can use the evaluated gradient of the local energy as the proposal vector field for updating the samples. This would allow to decrease the number of MCMC steps at the low cost of additional hyperparameter tuning.

## 6 Discussion and conclusion

ConclusionIn the current paper, we propose a novel approach to solving the quantum many-body Schrodinger equation, by incorporating the Wasserstein metric on the space of Born distributions. Compared to the Fisher-Rao metric, which allows for probability mass "teleportation", the Wasserstein metric constrains the evolution of the density to local changes under the locally-optimal transportation plan, i.e., following fluid dynamics. This property is favorable when the evolution of the parametric model is accompanied by the evolution of samples (performed by an MCMC algorithm). Indeed, by forbidding or regularizing non-local mass "teleportation" in the density change, one prevents the appearance of distant modes in the density model, which would require longer MCMC runs for proper mixing of the samples.

In practice, we demonstrate that following the gradient flow under the Wasserstein (or Wasserstein Fisher-Rao) metric results in better convergence to the ground state wave function. This is expected to be due to our proposed loss, which takes into account the gradient of the local energy and achieves its minimum when the norm of the gradient vanishes, therefore explicitly minimizing the norm of the local energy gradient.

We believe that our new theoretical framework for solving the time-independent Schrodinger equation for time-reversal symmetric Hamiltonians based on optimal transport will open new avenues to develop improved numerical methods for quantum chemistry and physics.

Connection to Energy-Based and Score-Based Generative ModelsThe developed ideas of this paper, i.e., projecting gradient flows under different metrics onto a parametric family, can be extended to generative modeling by swapping the energy functional with the KL-divergence. More precisely, as we show in Appendix C, using the KL-divergence as our objective functional, the Fisher-Rao gradient flow yields energy-based training scheme, while the 2-Wasserstein gradient flow corresponds to the score-matching, which is used for training diffusion generative models.

## 7 Acknowledgement

The authors thank Rob Brekelmans and anonymous reviewers for helpful discussions and feedback. J.N. was supported by Microsoft Research. J.C. acknowledges support from the Natural Sciences and Engineering Research Council (NSERC), the Shared Hierarchical Academic Research Computing Network (SHARCNET), Compute Canada, and the Canadian Institute for Advanced Research (CIFAR) AI Chairs program. A.M. acknowledges support from the Canada CIFAR AI Chairs program. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute www.vectorinstitute.ai/#partners.