# Refusal in Language Models

Is Mediated by a Single Direction

 Andy Arditi

Independent

Oscar Obeso

ETH Zurich

Correspondence to andyrdt@gmail.com, obalcells@student.ethz.ch.

Aaquib Syed

University of Maryland

Code available at https://github.com/andyrdt/refusal_direction.

&Daniel Paleka

ETH Zurich

&Nina Panickssey

Anthropic

&Wes Gurnee

MIT

&Neel Nanda

###### Abstract

Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.+

Footnote †: Code available at https://github.com/andyrdt/refusal_direction.

## 1 Introduction

Deployed large language models (LLMs) undergo multiple rounds of fine-tuning to become both _helpful_ and _harmless_: to provide helpful responses to innocuous user requests, but to refuse harmful or inappropriate ones (Bai et al., 2022). Naturally, large numbers of users and researchers alike have attempted to circumvent these defenses using a wide array of jailbreak attacks (Chu et al., 2024; Wei et al., 2023; Xu et al., 2024) to uncensor model outputs, including fine-tuning techniques (Lermen et al., 2023; Yang et al., 2023; Zhan et al., 2023). While the consequences of a successful attack on current chat assistants are modest, the scale and severity of harm from misuse could increase dramatically if frontier models are endowed with increased agency and autonomy (Anthropic, 2024). That is, as models are deployed in higher-stakes settings and are able to take actions in the real world, the ability to robustly refuse a request to cause harm is an essential requirement of a safe AI system. Inspired by the rapid progress of mechanistic interpretability (Bricken et al., 2023; Marks et al., 2024; Nanda et al., 2023; Templeton et al., 2024) and activation steering (Panickssey et al., 2023; Turner et al., 2023; Zou et al., 2023a), this work leverages the internal representations of chat models to better understand refusal.

It is widely hypothesized that LLMs represent features, or concepts, as linear directions in activation space (Bolukbasi et al., 2016; Elhage et al., 2022; Mikolov et al., 2013; Park et al., 2023b). Recent works have studied the linear representation of particular features such as harmlessness (Wolf et al., 2024; Zheng et al., 2024; Zou et al., 2023a), truth (Li et al., 2024a; Marks and Tegmark, 2023), humor (von Rute et al., 2024), sentiment (Tigges et al., 2023), language (Bricken et al., 2023), topic (Turner et al., 2023), and many others. Moreover, these feature directions have been shown to be effective causal mediators of behavior, enabling fine-grained steering of model outputs (Panickssery et al., 2023; Templeton et al., 2024; Turner et al., 2023; Zou et al., 2023a).

In this work, we show that refusal is mediated by a one-dimensional subspace across 13 popular open-source chat models up to 72B parameters in size. Specifically, we use a small set of contrastive pairs (Burns et al., 2022; Panickssery et al., 2023; Zou et al., 2023a) of harmful and harmless instructions to identify a single difference-in-means direction (Belrose, 2023; Marks and Tegmark, 2023; Panickssery et al., 2023) that can be intervened upon to circumvent refusal on harmful prompts, or induce refusal on harmless prompts (SS3). We then use this insight to design a simple white-box jailbreak via an interpretable rank-one weight edit that effectively disables refusal with minimal impact on other capabilities (SS4). We conclude with a preliminary mechanistic investigation into how adversarial suffixes (Zou et al., 2023b), a popular prompt-based jailbreak technique, interfere with the propagation of the refusal direction across token positions (SS5).

Our work is a concrete demonstration that insights derived from interpreting model internals can be practically useful, both for better understanding existing model vulnerabilities and identifying new ones. Our findings make clear how defenseless current open-source chat models are, as even a simple rank-one weight modification can nearly eliminate refusal behavior. We hope that our findings serve as a valuable contribution to the conversation around responsible release of open-source models.

Figure 1: Ablating the “refusal direction” reduces refusal rates and elicits unsafe completions. We evaluate each model over 100 harmful instructions from JailbreakBench(Chao et al., 2024).

Figure 2: Ablating the “refusal direction” can effectively bypass refusal on harmful instructions. This example is taken from Llama-3 8B Instruct. For more examples, see §K.1.

## 2 Methodology

### Background

Transformers.Decoder-only transformers (Liu et al., 2018) map input tokens \(=(t_{1},t_{2},,t_{n})^{n}\) to output probability distributions \(=(_{1},_{2},,_{n})^{n ||}\). Let \(_{i}^{(l)}()^{d_{}}}\) denote the residual stream activation of the token at position \(i\) at the start of layer \(l\).1 Each token's residual stream is initialized to its embedding \(_{i}^{(1)}=(t_{i})\), and then undergoes a series of transformations across \(L\) layers. Each layer's transformation includes contributions from attention and MLP components:

\[}_{i}^{(l)}=_{i}^{(l)}+^{(l)}(_{1:i}^{(l)}),_{i}^{(l+1)}=}_{i}^{(l)}+ ^{(l)}(}_{i}^{(l)}).\] (1)

The final logits \(_{i}=(_{i}^{(L+1)})^{| |}\) are then transformed into probabilities over output tokens \(_{i}=(_{i})^{| |}\).2

Chat models.Chat models are fine-tuned for instruction-following and dialogue (Ouyang et al., 2022; Touvron et al., 2023). These models use _chat templates_ to structure user queries. Typically, a chat template takes the form <user>{instruction}<end_user><assistant>. We use _post-instruction tokens_ to refer to all template tokens after the instruction, and denote the set of positional indices corresponding to these post-instruction tokens as \(I\). Our analysis focuses on activations in this region to understand how the model formulates its response. All chat templates and their corresponding post-instruction tokens are specified in SSC.3.

### Datasets and models

Datasets.We construct two datasets: \(_{}\), a dataset of harmful instructions drawn from AdvBench(Zou et al., 2023), MaliciousInstruct(Huang et al., 2023), TDC2023 (Mazeika et al., 2023, 2024), and HarmBench(Mazeika et al., 2024); and \(_{}\), a dataset of harmless instructions sampled from Alpaca(Taori et al., 2023). Each dataset consists of train and validation splits of 128 and 32 samples, respectively. We apply filtering to ensure that the train and validation splits do not overlap with the evaluation datasets used in SS3 and SS4. See SSA for further details about the datasets, including representative examples.

Models.To assess the generality of our findings, we study a diverse set of safety fine-tuned models, spanning 1.8 to 72 billion parameters in size. We consider both models aligned by preference optimization (APO) and aligned by fine-tuning (AFT) (Meade et al., 2024). All models included in the study are specified in Table 1.3

### Extracting a refusal direction

Difference-in-means.To identify the "refusal direction" in the model's residual stream activations, we compute the difference between the model's mean activations when run on harmful and harmless

   Model family & Sizes & Alignment type & Reference \\  Qwen Chat & 1.8B, 7B, 14B, 72B & AFT & Bai et al. (2023) \\ Y1 Chat & 6B, 34B & AFT & Young et al. (2024) \\ Gemma IT & 2B, 7B & APO & Team et al. (2024) \\ Llama-2 Chat & 7B, 13B, 70B & APO & Touvron et al. (2023) \\ Llama-3 Instruct & 8B, 70B & APO & AI@Meta (2024) \\   

Table 1: Model families, sizes, alignment training type, and references.

instructions. This technique, known as _difference-in-means_(Belrose, 2023), effectively isolates key feature directions, as demonstrated in prior work (Marks and Tegmark, 2023; Panickssery et al., 2023; Tigges et al., 2023). For each layer \(l[L]\) and post-instruction token position \(i I\), we calculate the mean activation \(_{i}^{(l)}\) for harmful prompts from \(_{}^{}\) and \(_{i}^{(l)}\) for harmless prompts from \(_{}^{}\):

\[_{i}^{(l)}=_{}^{}| }_{_{}^{}}_ {i}^{(l)}(),_{i}^{(l)}=_{}^{}|}_{_{}^{}}_{i}^{(l)}().\] (2)

We then compute the difference-in-means vector \(_{i}^{(l)}=_{i}^{(l)}-_{i}^{(l)}\). Note that each such vector is meaningful in both (1) its direction, which describes the direction that mean harmful and harmless activations differ along, and (2) its magnitude, which quantifies the distance between mean harmful and harmless activations.

Selecting a single vector.Computing the difference-in-means vector \(_{i}^{(l)}\) for each post-instruction token position \(i I\) and layer \(l[L]\) yields a set of \(|I| L\) candidate vectors. We then select the single most effective vector \(_{i^{*}}^{(l^{*})}\) from this set by evaluating each candidate vector over validation sets \(_{}^{}\) and \(_{}^{}\). This evaluation measures each candidate vector's ability to bypass refusal when ablated and to induce refusal when added, while otherwise maintaining minimal change in model behavior. A more detailed description of our selection algorithm is provided in SSC. We notate the selected vector as \(\), and its corresponding unit-norm vector as \(}\).

### Model interventions

Activation addition.Given a difference-in-means vector \(^{(l)}^{d_{}}\) extracted from layer \(l\), we can modulate the strength of the corresponding feature via simple linear interventions. Specifically, we can _add_ the difference-in-means vector to the activations of a harmless input to shift them closer to the mean harmful activation, thereby inducing refusal:

\[^{(l)^{}}^{(l)}+^{(l)}.\] (3)

Note that for activation addition, we intervene only at layer \(l\), and across all token positions.

Directional ablation.To investigate the role of a direction \(}^{d_{}}\) in the model's computation, we can erase it from the model's representations using _directional ablation_. Directional ablation "zeroes out" the component along \(}\) for every residual stream activation \(^{d_{}}\):

\[^{}-}}^{ }.\] (4)

We perform this operation at every activation \(_{i}^{(l)}\) and \(}_{i}^{(l)}\), across all layers \(l\) and all token positions \(i\). This effectively prevents the model from ever representing this direction in its residual stream.

### Evaluation of refusal and harmfulness

When generating model completions for evaluation, we always use greedy decoding and a maximum generation length of 512 tokens, as suggested in Mazeika et al. (2024). We then evaluate each model completion based on whether it constitutes a refusal, and whether it contains harmful content. We separate these evaluations into two scores: a _refusal score_ and a _safety score_.

Refusal score.Refusals often contain characteristic phrases, such as "I'm sorry" or "As an AI". Following prior work (Lermen et al., 2023; Liu et al., 2023; Robey et al., 2023; Shah et al., 2023; Xu et al., 2023; Zou et al., 2023), we compile a set of these common "refusal substrings". If a model completion includes at least one such substring, it is classified as a refusal (refusal_score=1); otherwise, it is classified as a non-refusal (refusal_score=0). The full set of refusal substrings is provided in SSD.1.

As has been previously noted (Huang et al., 2023; Meade et al., 2024; Qi et al., 2023; Shah et al., 2023), this string-matching approach has limitations. While effective at detecting memorized refusals, it does not assess whether the completion is coherent or contains harmful content. To address these limitations, we use a complementary metric that evaluates the harmfulness of a completion.

Safety score.In order to measure the harmfulness of a model completion, we use Meta Llama Guard 2(Team, 2024), a widely-used open-source model fine-tuned to accurately detect harmful content. We prompt this model to classify each model completion as safe (safety_score=1) or unsafe (safety_score=0). More details are provided in SSD.2.

## 3 Refusal is mediated by a single direction

For each model, we extract a single difference-in-means vector \(\) via the methodology described in SS2.3. We then show that this single direction is both necessary and sufficient for refusal. In SS3.1, we show that ablating this direction \(}\) effectively disables the model's ability to refuse harmful requests. In SS3.2, we show that adding \(\) to the model's activations induces refusal on harmless instructions.

### Bypassing refusal via directional ablation

To bypass refusal, we perform directional ablation on the "refusal direction" \(}\), ablating it from activations at all layers and all token positions. With this intervention in place, we generate model completions over JailbreakBench(Chao et al., 2024), a dataset of 100 harmful instructions.

Results are shown in Figure 1. Under no intervention, chat models refuse nearly all harmful requests, yielding high refusal and safety scores. Ablating \(}\) from the model's residual stream activations, labeled as _directional ablation_, reduces refusal rates and elicits unsafe completions.

### Inducing refusal via activation addition

To induce refusal, we add the difference-in-means vector \(\) to activations in layer \(l^{*}\), the layer that the \(\) was originally extracted from. We perform this intervention at all token positions. With this intervention in place, we generate model completions over 100 randomly sampled harmless instructions from Alpaca.

Results are shown in Figure 3. Under no intervention, chat models typically do not refuse harmless instructions. Adding \(\) to the model's residual stream activations, labeled as _activation addition_, results in the model refusing even harmless requests.

Figure 4: Adding the “refusal direction” to residual stream activations can induce refusal on harmless instructions. This example is taken from Gemma 7B IT. For more examples, see §K.2.

Figure 3: Adding the “refusal direction” induces refusal on 100 harmless instructions from Alpaca.

A white-box jailbreak via weight orthogonalization

In this section, we propose a novel white-box jailbreak method through _weight orthogonalization_. This technique directly modifies model weights to eliminate the representation of the refusal direction, resulting in a model that retains its original capabilities but no longer refuses harmful instructions. This new approach offers a simpler way to jailbreak open-source models compared to prior methodologies involving fine-tuning (Lermen et al., 2023; Yang et al., 2023; Zhan et al., 2023), as it does not require gradient-based optimization nor any examples of harmful completions.

### Weight orthogonalization

In SS2.4, we described _directional ablation_ as an inference-time intervention that prevents the model from representing a direction \(}\): during a forward pass, we zero out the \(}\) component from every intermediate residual stream activation (Equation 4). We can equivalently implement this operation by directly modifying component weights to never write to the \(}\) direction in the first place. Specifically, we can take each matrix \(W_{}^{d_{} d_{}}\) that writes to the residual stream, and orthogonalize its column vectors with respect to \(}\):

\[W^{}_{} W_{}-}}^{}W_{}.\] (5)

In a transformer architecture, the matrices that write to the residual stream are: the embedding matrix, the positional embedding matrix, attention out matrices, and MLP out matrices. Orthogonalizing all of these matrices, as well as any output biases, with respect to the direction \(}\) effectively prevents the model from ever writing \(}\) to its residual stream.

Note that this weight modification is equivalent to the previously described inference-time directional ablation, as shown explicitly in SSE. Therefore, the performance of the inference-time intervention in bypassing refusal, presented in SS3.1, also exactly characterizes that of the direct weight modification.

### Comparison to other jailbreaks

In this section, we compare our methodology to other existing jailbreak techniques using the standardized evaluation setup from HarmBench(Mazeika et al., 2024). Specifically, we generate completions over the HarmBench test set of 159 "standard behaviors", and then use their provided classifier model to determine the attack success rate (ASR), which is the proportion of completions classified as successfully bypassing refusal. We evaluate our weight orthogonalization method on models included in the HarmBench study, and report its ASR alongside those of alternative jailbreaks. For brief descriptions of each alternative jailbreak, see SSF.1.

Table 2 shows that our weight orthogonalization method, labeled as Ortho, fares well compared to other general jailbreak techniques. Across the Qwen model family, our general method is even on par with prompt-specific jailbreak techniques like GCG (Zou et al., 2023b), which optimize jailbreaks for each prompt individually.

    &  &  \\   } & Ortho & GCG-M & GCG-T & Human & DR & GCG & AP & PAIR \\  Llama-2 7B & **22.6** (79.9) & 20.0 & 16.8 & 0.1 & 0.0 & 34.5 & 17.0 & 7.5 \\ Llama-2 13B & 6.9 (61.0) & 8.7 & **13.0** & 0.6 & 0.5 & 28.0 & 14.5 & 15.0 \\ Llama-2 70B & 4.4 (62.9) & 5.5 & **15.2** & 0.0 & 0.0 & 36.0 & 15.5 & 7.5 \\ Qwen 7B & **79.2** (74.8) & 73.3 & 48.4 & 28.4 & 7.0 & 79.5 & 67.0 & 58.0 \\ Qwen 14B & **84.3** (74.8) & 75.5 & 46.0 & 31.5 & 9.5 & 83.5 & 56.0 & 51.5 \\ Qwen 72B & **78.0** (79.2) & - & 36.6 & 42.2 & 8.5 & - & - & 54.5 \\   

Table 2: HarmBench attack success rate (ASR) across various jailbreaking methods. Our method is labeled as Ortho. The baseline “direct response” rate with no jailbreak applied is labeled as DR. We differentiate _general_ jailbreaks, which are applied across all prompts generically, from _prompt-specific_ jailbreaks, which are optimized for each prompt individually. All evaluations use the model’s default system prompt. We also report ASR without system prompt in blue.

Note that HarmBench's evaluation methodology specifies that each model's default system prompt should be used during evaluation. While this approach is sensible for assessing the robustness of black-box systems, it is less applicable for white-box scenarios where attackers have full access to the model and can easily exclude the system prompt. Thus, we report ASR both with and without the system prompt.

We observe a notable difference in system prompt sensitivity across model families. For Llama-2 models, including the system prompt substantially reduces ASR compared to evaluation without it (e.g. 22.6% vs 79.9% for Llama-2 7B). In contrast, Qwen models maintain similar ASR regardless of system prompt inclusion (e.g. 79.2% vs 74.8% for Qwen 7B). While the Llama-2 system prompt contains explicit safety guidelines compared to the minimal Qwen system prompt, additional analysis in SS7.2 suggests the discrepancy is not explained by prompt content alone, and may reflect differences in how these models respond to system-level instructions more generally.

### Measuring model coherence

A reasonable concern with any new jailbreak technique is that, in addition to circumventing refusal, it may also degrade the model's overall quality (Souly et al., 2024). However, qualitatively, we observe that models maintain their coherence after undergoing weight orthogonalization. While SS3.1 and SS4.2 show that our method effectively bypasses refusal, in this subsection we quantitatively evaluate how the modification alters a model's general capabilities.

For each model and its orthogonalized version, we run four common language model evaluations: MMLU (Hendrycks et al., 2020), ARC (Clark et al., 2018), GSM8K (Cobbe et al., 2021), and TruthfulQA (Lin et al., 2021). All evaluations are run using LM Evaluation Harness (Gao et al., 2023), with settings consistent with Open LLM Leaderboard (Beeching et al., 2023).4

Table 3 displays that, for MMLU, ARC, and GSM8K, orthogonalized models perform similarly to baseline models. In SSG.1, we show that this holds across other models in our suite, with additional evaluations of WinoGrande (Sakaguchi et al., 2021) and TinyHellaSwag (Polo et al., 2024). Except for Qwen 7B and Yi 34B, all evaluation metrics for orthogonalized models lie within 99% confidence intervals of original performance.

Interestingly, accuracy on TruthfulQA consistently drops for orthogonalized models. This phenomenon is consistent with Yang et al. (2023), where it was observed that fine-tuning away safety guardails results in decreased accuracy on TruthfulQA. Examining specific questions in TruthfulQA reveals that the dataset veers close to the territory of refusal, with categories including "misinformation", "stereotypes", and "conspiracies", and thus it may intuitively make sense that model behavior differs meaningfully on this evaluation dataset. See SSG.2 for further discussion of TruthfulQA performance.

In addition to standard language model evaluations, we also evaluate differences in CE loss, both on standard text corpora and model-specific generations (SSG.3). These loss metrics suggest that directional ablation is more surgical than activation addition based methods (SSI.1).

   Chat model & MMLU & ARC & GSM8K & TruthfulQA \\  Gemma 7B & 51.8 / 51.7 (+0.1) & 51.7 / 51.5 (+0.2) & 31.3 / 32.0 (-0.7) & 44.7 / 47.1 (-2.4) \\ Yi 34B & 73.5 / 74.9 (-1.4) & 65.6 / 64.9 (+0.7) & 65.5 / 65.0 (+0.5) & 51.9 / 55.4 (-3.5) \\ Llama-2 70B & 63.1 / 63.0 (+0.1) & 65.2 / 65.4 (-0.2) & 54.5 / 53.0 (+1.5) & 51.8 / 52.8 (-1.0) \\ Llama-3 70B & 79.8 / 79.9 (-0.1) & 71.5 / 71.8 (-0.3) & 90.8 / 91.2 (-0.4) & 59.5 / 61.8 (-2.3) \\ Qwen 72B & 76.5 / 77.2 (-0.7) & 67.2 / 67.6 (-0.4) & 76.3 / 75.5 (+0.8) & 55.0 / 56.4 (-1.4) \\   

Table 3: Model evaluations. For each evaluation, we report the orthogonalized model’s performance, followed by the baseline model’s performance, followed by the absolute increase or decrease. We display the largest model from each model family. Full results are reported in §G.1.

## 5 Mechanistic analysis of adversarial suffixes

Safety fine-tuned chat models are vulnerable to _adversarial suffix_ attacks (Zou et al., 2023b): there exist carefully constructed strings such that appending these strings to the end of a harmful instruction bypasses refusal and elicits harmful content. Effective adversarial suffixes are usually not human interpretable, and the mechanisms by which they work are not well understood. In this section, we mechanistically analyze the effect of an adversarial suffix on Qwen 1.8B Chat.

### Adversarial suffixes suppress the refusal-mediating direction

We first identify a single adversarial suffix that effectively bypasses refusal in Qwen 1.8B Chat. The suffix is displayed in SSH, along with details of its generation. To study the effect of this adversarial suffix, we sample 128 refusal-eliciting harmful instructions from JailbreakBench and the HarmBench test set. For each instruction, we run the model three times: first with the unedited instruction, second with the adversarial suffix appended, and third with a freshly-sampled random suffix of the same length appended. By comparing the adversarial suffix to random suffixes, we aim to control for the effect of appending any suffix at all. For each run, we cache the last token activations and visualize their cosine similarity with the refusal-mediating direction. We also compare to a baseline of 128 harmless instructions from AlpacA that do not elicit refusal. Figure 5 shows that the expression of the refusal direction is very high for harmful instructions, and remains high when a random suffix is appended. The expression of the refusal direction after appending the adversarial suffix is heavily suppressed, and closely resembles that of harmless instructions.

### Adversarial suffixes hijack the attention of important heads

To further investigate how the refusal direction is suppressed, we examine the contributions of individual attention head and MLP components to the refusal direction. We quantify each component's contribution to this direction using _direct feature attribution_ (DFA) (Kissane et al., 2024; Makelov et al., 2024): each component's direct contribution can be measured by projecting its output onto the refusal direction. We select the top eight attention heads with the highest DFA on harmful instructions,

Figure 6: We analyze the top eight attention heads that most significantly write to the refusal direction. Figure 6(a) shows that output to the refusal direction is heavily suppressed when the adversarial suffix is appended. Figure 6(b) reveals that, compared to appending a random suffix, appending the adversarial suffix shifts attention from tokens in the instruction region to tokens in the suffix region.

and then investigate how their behavior changes when suffixes are appended. Figure 6(a) shows that the direct contributions of these heads to the refusal direction are significantly suppressed when the adversarial suffix is appended, as compared with no suffix and random suffixes.

To understand how the outputs of these attention heads are altered, we examine their attention patterns. Figure 6(b) illustrates that the adversarial suffix effectively "hijacks" the attention of these heads. Normally, these heads focus on the instruction region of the prompt, which contains harmful content. With the adversarial suffix appended, these heads shift their attention to the suffix region, and away from the harmful instruction.

## 6 Related work

Understanding refusal in language models.Wei et al. (2024) demonstrate that removing a set of safety-critical neurons and ranks can degrade safety mechanisms while preserving utility. Zheng et al. (2024) and Zou et al. (2023) both use contrastive pairs of harmful and harmless inputs to identify the model's representation of _harmfulness_, asserting that this direction is distinct from the model's representation of _refusal_. Zheng et al. (2024) argue this by showing that safety prompts shift activations in a distinct direction, while Zou et al. (2023) show that the representation is not significantly altered by adversarial suffixes. Note that this is in contrast to our findings in SS5.1 that the refusal direction is significantly suppressed in the presence of an adversarial suffix. Zou et al. (2023) additionally introduce a "piece-wise" intervention to effectively amplify representations of harmfulness, and show that this intervention increases refusal on harmful inputs even when jailbreaks are applied. Panickssery et al. (2023) use contrastive multiple-choice completions, finding that steering with the resulting vector is effective at modulating refusal in multiple-choice settings but not in long-form generation. Wang and Shu (2024) introduce a "Trojan Activation Attack" that adds steering vectors to bypass refusal during inference. Li et al. (2024) identify a "safety pattern" by selecting individual neurons in each layer, and modulate refusal by zeroing out these neurons, although with unclear effects on the overall model performance.

Features as directions.Extracting feature directions from contrastive pairs of inputs is an established technique (Burns et al., 2022; Panickssery et al., 2023; Zou et al., 2023). It is well understood that adding feature vectors to the residual stream can modify behavior (Li et al., 2024; Marks and Tegmark, 2023; Panickssery et al., 2023; Tigges et al., 2023; Turner et al., 2023; Zou et al., 2023), although details on how and where to intervene vary (Jorgensen et al., 2023; von Rutte et al., 2024).

Various works show that directions in activation space have more "feature-like" properties than neurons do (Bolukbasi et al., 2016; Elhage et al., 2022; Geiger et al., 2024; Hernandez and Andreas, 2021; Li et al., 2021; Mikolov et al., 2013; Nanda et al., 2023; Park et al., 2023). Recent works use sparse autoencoders to discover feature directions in an unsupervised manner (Bricken et al., 2023; Cunningham et al., 2023; Templeton et al., 2024). The assumption that features are represented linearly has been effective for erasing concepts from language models (Belrose, 2023; Belrose et al., 2024; Guerner et al., 2023; Haghighatkhah et al., 2022; Ravfogel et al., 2020; Shao et al., 2022).

Undoing safety fine-tuning.It is well known that fine-tuning on malicious examples is sufficient to undo safety guard trails (Lermen et al., 2023), even with minimal degradation of overall capabilities (Yang et al., 2023; Zhan et al., 2023). Undoing refusal via fine-tuning requires examples of harmful instructions and completions, while our method requires only harmful instructions. Note however that fine-tuning can weaken safety guardrails even when data is benign (Pelrine et al., 2023; Qi et al., 2023). Mechanistic interpretability works have provided initial evidence suggesting that fine-tuning does not significantly alter relevant internal circuitry (Jain et al., 2023; Lee et al., 2024; Prakash et al., 2024). For example, Lee et al. (2024) fine-tune a model to make it less toxic, and find this behavioral modification can be undone simply by scaling a small number of MLP weights.

Jailbreaks.The research area of circumventing restrictions on LLM behavior by _modifying the input_ has seen many different directions of work. Many models are vulnerable to _social engineering attacks_(Perez et al., 2022; Shah et al., 2023; Wei et al., 2023). One hypothesis for why this works is that such prompts modify the LLM assistant's "persona" (Andreas, 2022; Park et al., 2023; Shanahan et al., 2023). Preliminary experiments in SSL suggest that our method does not change the model's chat personality or behavior outside of refusal.

Optimized _adversarial suffixes_(Andriushchenko et al., 2024; Liao and Sun, 2024; Zou et al., 2023b) can be appended to prompts to bypass refusal. In contrast, our method does not require any modifications to the input prompt, but has the obvious limitation that we require access to the model's weights. However, note that transferability of jailbreak prompts optimized on open-weight models on black-box models is unclear (Meade et al., 2024). jailbreak prompts may have significant impact on model performance (Souly et al., 2024), whereas our method does not (SS4.3).

## 7 Discussion

In this work, we demonstrate that refusal behavior is consistently mediated by a single direction across a diverse set of open-source chat models. Based on this understanding, we propose a simple yet effective white-box jailbreak method that directly modifies model weights to disable the refusal mechanism while retaining model coherence. Our work demonstrates the practical utility of model-internals based interpretability: by studying refusal through the lens of model internals, we were able to create a simple yet effective jailbreak method. The simplicity of the model's refusal mechanism, and the ease of circumventing it in the white-box setting, raise concerns about the robustness of current alignment techniques.

Limitations.Our study has several limitations. While we evaluate a broad range of open-source models, our findings may not generalize to untested models, especially those at greater scale, including current state-of-the-art proprietary models and those developed in the future. Additionally, the methodology we used to extract the "refusal direction" is likely not optimal and relies on several heuristics. We see this paper as more of an existence proof that such a direction exists, rather than a careful study of how best to extract it, and we leave methodological improvements to future work. Furthermore, our analysis of adversarial suffixes does not provide a comprehensive mechanistic understanding of the phenomenon, and is restricted to a single model and a single adversarial example. Another limitation is that it is difficult to measure the coherence of a chat model, and we consider each metric used flawed in various ways. We use multiple varied metrics to give a broad view of coherence. Finally, while our work identifies a single direction that mediates refusal behavior in each model, we acknowledge that the semantic meaning of these directions remains unclear. Though we use the term "refusal direction" as a functional description, these directions could represent other concepts such as "harm" or "danger", or they may even resist straightforward semantic interpretation.

Ethical considerations.Any work on jailbreaking LLMs must ask the question of whether it enables novel harms. It is already widely known that open-source model weights can be jailbroken via fine-tuning. Our method, which can yield a jailbroken version of a 70B parameter model using less than SS5 of compute, is simpler than previous fine-tuning methods, requiring neither gradient-based optimization nor a dataset of harmful completions. While we acknowledge that our methodology marginally lowers the bar for jailbreaking open-source model weights, we believe that it does not substantially alter the risk profile of open sourcing models.

Although the risk of misuse posed by today's language models may be relatively low (Anthropic, 2024; Mouton et al., 2024), the rapid advancement of state-of-the-art model capabilities suggests that this risk could become significant in the near future. Our work contributes to the growing body of literature that highlights the fragility of current safety mechanisms, demonstrating that they can easily be circumvented and are insufficient to prevent the misuse of open-source LLMs. Building a scientific consensus around the limitations of current safety techniques is crucial for informing future policy decisions and research efforts.

Author contributions.AA led the research project, and led the writing of the paper. AA discovered and validated that ablating a single direction bypasses refusal, and came up with the weight orthogonalization trick. OO ran initial experiments identifying that it is possible to jailbreak models via activation addition. AA and OO implemented and ran all experiments presented in the paper, with DP helping to run model coherence evaluations. DP investigated behavior of the orthogonalized models, suggested more thorough evaluations, and assisted with the writing of the paper. AS ran initial experiments testing the causal efficacy of various directional interventions, and identifiedthe suffix used in SS5 as universal for Qwen 1.8B Chat. NP first proposed the idea of trying to extract a linear refusal direction Panickssey (2023), and advised the initial project to mechanistically understand refusal in Llama-2 7B Chat (Arditi and Obeso, 2023). WG advised on methodology and experiments, and assisted with the writing and framing of the paper. NN acted as primary supervisor for the project, providing guidance and feedback throughout.

Acknowledgements.AA and OO began working on the project as part of the Supervised Program for Alignment Research (SPAR) program, mentored by NP. AA and AS continued working on the project as part of the ML Alignment & Theory Scholars (MATS) program, mentored by NN.

We thank Florian Tramer for providing generous compute resources and for offering comments on an earlier draft. We also thank Philippe Chlenski for providing thoughtful feedback on the manuscript. For general support throughout the research process, we thank McKenna Fitzgerald, Rocket Drew, Matthew Wearden, Henry Sleight, and the rest of the MATS team, and also Arthur Conmy. We also thank the staff at Lighthaven and London Initiative for Safe AI (LISA) for cultivating great environments in which to conduct research. We are grateful to the anonymous reviewers for their valuable feedback which helped improve this paper.

Tooline.For our exploratory research, we used TransformerLens (Nanda and Bloom, 2022). For our experimental pipeline, we use HuggingFace Transformers (Wolf et al., 2020), PyTorch (Paszke et al., 2019), and vLLM (Kwon et al., 2023). We used Together AI remote inference to compute the safety_score metric quickly.

Disclosure of funding.AA and OO are funded by Long-Term Future Fund (LTFF). AA and AS are funded by AI Safety Support (AISS).