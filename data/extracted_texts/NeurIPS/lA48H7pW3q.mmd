# QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization

Qi Song\({}^{1}\), Tianxiang Gong\({}^{2}\), Shiqi Gao\({}^{2}\),

Haoyi Zhou\({}^{1,3}\), Jianxin Li\({}^{2,3}\)

\({}^{1}\)School of Software, Beihang University

\({}^{2}\)School of Computer Science and Engineering, Beihang University

\({}^{3}\)Zhongguancun Laboratory, Beijing

{songqi23, gongtx, gaoshiqi, haoyi, lijx}@buaa.edu

Equal contributionCorresponding author.

###### Abstract

Multimodal contrastive learning (MCL) has recently demonstrated significant success across various tasks. However, the existing MCL treats all negative samples equally and ignores the potential semantic association with positive samples, which limits the model's ability to achieve fine-grained alignment. In multi-view scenarios, MCL tends to prioritize shared information while neglecting modality-specific unique information across different views, leading to feature suppression and sub-optimal performance in downstream tasks. To address these limitations, we propose a novel contrastive framework named _QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization_. In the QUEST framework, we propose quaternion contrastive objectives and orthogonal constraints to extract sufficient unique information. Meanwhile, a shared information-guided penalization is introduced to ensure that shared information does not excessively influence the optimization of unique information. Our method leverages quaternion vector spaces to simultaneously optimize shared and unique information. Experiments on multiple datasets show that our method achieves superior performance in multimodal contrastive learning benchmarks. On public benchmark, our approach achieves state-of-the-art performance, and on synthetic shortcut datasets, we outperform existing baseline methods by an average of \(97.95\%\) on the CLIP model.

## 1 Introduction

Multimodal Contrastive Learning (MCL) has demonstrated robust representation capabilities and generalizability and effectively transferring to various downstream tasks (e.g. cross-modal retrieval , image captioning ). However, simply applying contrastive learning in multimodal scenarios presents significant challenges.

In particular, contrastive learning treats all negative samples equally, ignoring the potential semantic relationships between negative samples and the anchor. Besides, current contrastive learning methods focus on maximizing mutual information between two views  while ignoring unique information . In multi-view scenarios, the assumption that modalities share substantial task-related information often does not hold, especially in complex datasets with minimal inter-modal overlap. Meanwhile, recent studies  indicate that contrastive learning often neglects significant portions of input information, leading to feature suppression  and shortcut learning , where models minimize loss through the simplest path (e.g. shared information ), sacrificing deeper learning. These issues are prevalent in multimodal  and multi-view tasks . Recent approaches focus on preserving more unique information, including reconstruction regularization , implicit feature modification , and factorized representation , among others. However, these methods either overly introduce noise which may harm downstream tasks, or rely on certain assumptions (e.g., augmentation ). Additionally, we find that these methods do not explicitly distinguish unique information and still optimize using contrastive learning, making it difficult to avoid the model learning shortcuts . This raises the question: **can we explicitly extract both task-related unique and shared information without introducing too much noise?**

To this end, we proposes a novel contrastive framework called _QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization_, designed to enhance the extraction and integration of both shared and unique information across multimodal data. Our primary motivation is to develop a mechanism that effectively captures unique information through a novel quaternion multimodal embedding space, as illustrated in Figure 1(b). This embedding space aims to pull shared representations closer while aligning the unique representations with the shared representation on a common plane. We achieve this by leveraging the properties of the normal vector from the cross-product to diversified unique representation. Consequently, our approach aligns commonalities across modalities while preserving the distinctive unique features of each modality.

Specifically, we first split a network into three components: an encoder, a shared decoder, and a unique decoder. The encoder learns general features with little bias toward specific tasks, while the shared decoder and unique decoder learn agreement and discriminative information, respectively. We build contrastive loss to constrain learning of shared information. To avoid the unique decoder degenerating into the shared decoder, we propose novel contrastive objectives and orthogonal constraints to optimize the quaternion vector space. Finally, self-penalization is used to prevent shared information from overly affecting quaternion vector space optimization. Our framework seeks to mitigate shortcut learning, offering a more nuanced, task-related learning paradigm. Our main contributions can be summarized as follows:

* We develop a novel framework to efficiently extract shared and unique information across multimodal data. To avoid the degeneration of the unique decoder, we propose an algorithm that utilizes quadruple embedding to constrain unique information from different views in a plane space.
* We consider that traditional CL overly relies on shared information due to data bias, causing failures with negative samples containing shared information related to the positive sample. Meanwhile, to prevent shared information from dominating the extraction of unique information, we introduce a self-penalization mechanism to dynamically reweight the distribution of negative samples, which penalizes hard negative samples. We provide theoretical analysis to show how this penalization effectively improves the extraction of unique information.
* We achieve state-of-the-art on popular datasets (e.g. MS-COCO  and Flickr30k ) compared to the baseline, demonstrating the general effectiveness of QUEST. Additionally, experiment results on synthetic shortcut datasets outperform baselines \(97.95\%\) on average for CLIP, verifying the efficacy of QUEST.

Figure 1: (a) Our QUEST outperforms baselines \(97.95\%\) on average when trained with task-related unique information and evaluated on downstream tasks on the CLIP model. (b) We build the quaternion embedding space, which aligns shared and unique representations from different modalities through the application of constraints and self-penalization. The \(_{}\) narrows the gap between shared representations, while \(_{}\) pulls the plane spanned by intra-modality shared and unique representations closer. Furthermore, Orthogonalization loss \(_{cos}\) is employed to constrain the area.

Methods

### Problem Formulation

For different modalities \(\{_{i}\}_{i=1}^{K}\), given one modality, denoted as \(_{i}\), along with its corresponding set of views \(\{x_{i}^{j}\}_{j=1}^{N_{i}},N_{i} 1\), for one modality \(_{1}\) encoder parameterized by \(_{1}\), represented as \(_{_{1}}(;_{1})\), and another modality \(_{2}\) encoder parameterized by \(_{2}\), denoted as \(_{_{2}}(;_{2})\). These encoders process the sample from modal \(_{1}\) and \(_{2}\), for those modalities with multiple views, like modal one \(_{1}\) and each of its views through their respective encoder, resulting in corresponding general representations \(_{_{1}}^{j}=_{_{1}}(x_{1}^{j}; _{1})\) and \(_{_{2}}^{j}=_{_{2}}(x_{2}^{j}; _{2})\). However, as illustrated in Figure 2, the InfoNCE loss maximizes task-related features shared across all modalities during training (i.e. \(I(X_{A}^{};X_{B}^{};Y)\)), while simultaneously suppressing the unique task-related features of each individual modality(i.e. \(I(X_{A};Y|X_{B})\) and \(I(X_{B};Y|X_{A})\)). This process ultimately results in the loss of unique information. Therefore, the general representations are then separated into task-related shared and unique features through different decoders, i.e., the representations of different modalities \(_{_{1}}^{j},_{_{2}}^{j}\) are inputted separately into different decoders \(_{_{i}}()\) parameterized by \(_{i}\). For the complete notations, refer to Appendix C, Table 6.

### Overview

In multi-view scenarios, the relationships between different modalities are many-to-many (e.g., for image retrieval, multiple captions can refer to the same image). Within a single modality, different views contain task-related unique and shared information. Additionally, task-related shared information may also exist among negative samples (as indicated by the red shading in Figure 3). Therefore, optimizing solely for shared information while ignoring unique information is suboptimal.

To address the challenge of overlooking unique information inherent to different perspectives in multimodal scenarios, we introduce the effective framework called _QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization_. This framework extends existing contrastive learning methods by incorporating a four-partite architecture specifically designed to enhance the capture and integration of distinctive modal-specific features. The overall architecture is shown in Figure 3. According to [84; 62; 34], from the

Figure 3: Framework of QUEST. The unique decoder is utilized to extra view-specific unique information and this process is guided by the proposed constraints and penalization.

Figure 2: Feature suppression in multi-view contrastive learning. We define \(I(X_{A};X_{B};Y)\) as task-related shared information, \(I(X_{A};Y|X_{B})\) and \(I(X_{B};Y|X_{A})\) as task-related unique information related to task \(Y\) in modalities \(X_{A}\) and \(X_{B}\),respectively. Contrastive losses, such as InfoNCE, tend to maximize the task-related shared information while suppressing the task-related unique information in each modality. Left: before training with InfoNCE. Right: after training with InfoNCE.

perspective of neural network architecture, shallow layers learn low-level and general features while deeper layers learn task-biased high-level semantic features. Therefore, we define the encoder output as \(H\), which contains shared \(S\) and unique \(U\) information related to the task, \(H S U\). Consequently, we shared shallow layers for general representation to reduce computational cost and optimize the shared decoder and unique decoder for \(S\) and \(U\), respectively.

### Quadruple InfoNCE

For each modality, the input data \(_{i}\) (sampled from views) undergoes transformation by a modality-specific encoder \(_{_{i}}()\), producing an intermediary general representation denoted as \(_{_{i}}\). Consequently, we introduce two decoders: a shared information decoder \(^{}_{_{i}}()\) and a unique information decoder \(^{}_{_{i}}()\). These decoders are tasked with disentangling the shared and unique components from the representation \(_{_{i}}\), respectively. Let \(_{i}\) represent the parameters of a shared encoder, while \(_{i}\) and \(_{i}\) symbolize the decoders for shared and unique information, respectively. The representation can be formulated as follows:

\[^{}_{i}&=^{}_{_{i}}(_{_{i}};_{i})= ^{}_{_{i}}(_{_{i}}( _{i};_{i});_{i}),\\ ^{}_{i}&=^{}_{_{i}}(_{_{i}};_{i})=^{ }_{_{i}}(_{_{i}}(_{i}; _{i});_{i}).\] (1)

Shared Information Constraint.In multimodal and multi-view scenarios, data from different modalities and views often encapsulate shared information vital for model training. We conduct a shared Information Constraint (SIC) to maximize the lower bound of MI between representations from different views to encourage the shared decoder to learn agreement related to the task. This constraint is optimized by computing the InfoNCE loss between (\(^{}_{i}\) and \(^{}_{j}\)), defined as,

\[_{}=_{i,j}_{_{i}_ {j}}_{^{}_{i}}[-^{}_{i},^{^{+}}_{j})/)}{(s(^{ }_{i},^{^{+}}_{j})/)+_{k=1}^{m}_{^{-}}(s(^{}_{i},^{^{ -}}_{jk})/)}].\] (2)

Unique Information Constraint.In contrast to shared information, unique information is modality-specific and task-related, providing essential insights for downstream tasks. To preserve this, we introduce a Unique Information Constraint (UIC), extracting the unique information that exists within different views, which are unrelated to each other yet relevant to the task. Relying solely on Shared Information Constraint (SIC) is insufficient to preserve this information . Strict constraints, such as directly enforcing consistency of distributions across diverse views, may lead to the Unique Information Constraint converging to the SIC, particularly in scenarios with identical input representations, loss functions, and network structures. To address this issue, we implement a less stringent constraint, aiming to maximize the similarity between the spaces formed by unique and shared information across different modalities. Firstly, we derive the representation space of normal vectors for shared and unique embedding spaces through cross-product calculations,

\[^{}_{i}=^{}_{i}^{ }_{i}.\] (3)

In the newly projected space, our objectives aim to maximize the alignment of unique representation from different modalities within the plane spanned by the shared representation, the magnitude can be formulated as:

\[(^{}_{i}^{}_{i})(^{}_{j}^{}_{j})=\|^{ }_{i}\|\|^{}_{i}\|\|^{}_{j} \|\|^{}_{j}\|,\] (4)

where \(\) and \(\) represent the sine similarity between the shared and unique representation across different modalities, and \(\) represents the cosine similarity of the normal vector. We maximize \(\) and \(\) via orthogonalized cosine loss \(_{}\) and contrastive loss to maximize \(\), the unique information constraint can be formulated as:

\[_{}&=_{i,j} _{_{i}_{j}}_{^{ }}[-^{}_{i},^{ ^{+}}_{j})/)}{(s(^{}_{i},^{ ^{+}}_{j})/)+_{k=1}^{m}_{^{-}}(s( ^{}_{i},^{^{-}}_{jk})/)}]\\ &+_{i}^{}_{ij} ^{}_{j}}{\|^{}_{ij}\|\|^ {}_{ij}\|}}_{_{}}.\] (5)By pulling positive samples closer and pushing negative samples away, we constrain the shared and unique representations of different modalities to coexist within the same spatial plane as much as practicable. Concurrently, we employ \(_{}\) to maintain the diversity of unique information and ensure that \(_{i}^{}_{i}^{}\). It is worth noting that we do not impose any constraints on the unique representations of different modalities, as these may be inherently unrelated.

**Comparison.** Applying standard InfoNCE to extract unique information leads to suboptimal generalization, as demonstrated in Section 3.3. To address this, we propose UIC with indirect vectors \(^{n}\), where the cross-product operation fundamentally alters gradient propagation patterns compared to pair-wised InfoNCE. Specifically, given \(_{}=- Z_{b}^{}/)}{ _{i=0}^{N}(Z_{a} Z_{i}/)}\), the gradient with respect to anchor embedding \(Z_{a}\) as follows,

\[-_{}}{ Z_{a}}=(Z _{b}^{+}-_{i=0}^{N}_{i}Z_{bi}),\] (6)

where \(_{i}= Z_{i}/)}{_{i=0}^{N}(Z_{a} Z_{ i}/)}\) and \(Z_{b}^{+}\) is random sampled from positive set \(\{Z_{b}^{1},...Z_{b}^{j}\}\). Under the assumption that different views hold both shared and unique information, we have \(I(Z_{b}^{j};Y)=I(Z_{b}^{j};Z_{a};Y)+I(Z_{b}^{j};Y|Z_{a})\), where \(I(Z_{b}^{j};Z_{a};Y)\) represents shared information between two views and \(I(Z_{b}^{j};Y|Z_{a})\) represents unique information for \(j\)th view. With sufficient training iterations, the unique information tends towards noise as shared information dominates the accumulated gradient (first term in Eq. (6)). This is consistent with the conclusion of MI . Assume that the features obtained from the encoder consist of shared features \(S\) which correspond to the anchor and unique features \(U\), represented as \(Z_{b}^{j}=(S U^{j})\). Traditional contrastive learning defines an additive model \(Z_{b}^{j}=(S+U^{j})\) whereas \(Z_{b}^{j}=(S U^{j})\) in our model. Intuitively, there exists \(\) satisfes \(=Z_{a}(S U^{1})=...=Z_{a}(S U^{j})\). Therefore, UIC is a weaker constraint that ensures quaternion vectors between different views lie on the same plane as much as possible. If we use both SIC and UIC simultaneously, SIC will pull the shared representations of different views closer, while UIC will ensure that the unique representations of different views lie on the same plane as much as possible, rather than measuring their cosine similarity, as unique information is uncorrelated. We conducted extensive experiments to verify this (Section 3.3 for more details).

### Shared Information Guided Constraint

Contrastive learning fundamentally operates by optimizing vector representations to minimize distances between positive pairs while maximizing distances between negative pairs in the embedding space. However, a critical limitation inherent in conventional approaches stems from their undifferentiated treatment of all samples within a batch \(\) as negative examples. This indiscriminate categorization may inadvertently cause the model to overlook potential semantic relationships (as illustrated by the red shading in Figure 3), despite their shared semantic content (e.g., different image captions containing identical substrings in image-text retrieval tasks). Such misclassification significantly impairs the model's representation capacity. We refer to these cases as "hard negative samples. While existing methods attempt to address this issue through clustering-based approaches, their two-stage nature limits practical adoption. Leveraging our dual-branch architecture, we propose a more effective solution that directly utilizes the output of the shared decoder as a supervision signal for the penalty term.

Considering our objectives is to optimize the shared information decoder through \(_{}\) and unique information decoder through \(_{}\), and the shared information also affects the optimization of unique information as shown in Eq. (4), we attempt to use the intra-model shared information similar to penalization to guide the optimization of unique information. Unlike soft label  which aim to mitigate the strict constraints of one-hot labels, preventing overconfidence by retaining more potential positive samples, our method aims to impose stricter constraints to suppress shared information in the process of learning unique information. Specifically, the more shared information between the anchor and all the negative samples, the greater the encouragement in learning unique information between the anchor and the positive sample. Formally, for shared representation \(_{i}^{}\) and \(_{j}^{}\), the weighted similarity matrix can be formulated as:

\[=([-()+]),\] (7)where \(=_{i}^{}_{j}^{^{T}}\) is the similarity matrix, \(\) is identity matrix, \(\) is learnable weight parameter. Next, the weighted similarity matrix \(^{N N}\) is utilized as penalization to supervise the optimization of unique information satisfied \(_{ij,i j} s(z_{i}^{s},z_{j}^{s})\), the penalized UIC can be defined as:

\[_{}&=_{i,j} _{_{i}_{j}}_{_{i}^{ }}[-_{i}^{},_{j} ^{^{+}})/)}{(s(_{i}^{},_{j}^{ ^{+}})/)+_{k=1}^{m}_{}^{-}} (_{k} s(_{i}^{},_{jk}^{ ^{-}})/)}]\\ &+_{i}_{}.\] (8)

**Gradient Analysis.** For simplicity, we set \(=_{k=0}^{m}(_{k} s(_{i}^{},_{jk}^{})/)\) and ignore the second term, we reformulate Eq. (8) as:

\[}_{}& =_{_{i}^{}}[-^{+} s(_{i}^{},_{j}^{ ^{+}})/)}{(^{+} s(_{i}^{},_{j}^{^{+}})/)+_{k=1}^{m}_{}^{-}} (_{k} s(_{i}^{},_{jk}^{^{-}})/)}]\\ &=_{_{i}^{}}[- {^{+} s(_{i}^{},_{j}^{ ^{+}})}{}].\] (9)

The gradient can be calculated as (Appendix D.2 for details):

\[-}_{}}{_{i}^ {}}=^{+}}{}}{ _{i}^{}}-_{k=0}^{m}_{k} (_{k}s^{k}}{})}{ _{i}^{}},\] (10)

where \(s^{+}=s(_{i}^{},_{j}^{^{+}})\) and \(s^{k}=s(_{i}^{},_{jk}^{^{-}})\). Intuitively, \(\) represents the belief mass based on shared information, and the larger \(P_{k}\) indicates more shared information between hard positive samples, which also influences the optimization of the unique decoder as in Eq. (4). When \(_{k}=0\) (i.e, \(s(_{i},_{jk}) 0\)), it will degenerate to the original InfoNCE loss. For hard negatives samples hold shared information where \(s(_{i}^{},_{j}^{})>0\), we increase the gradient using a penalty term which ensures that even if \(_{i}^{}\) is relatively large in Eq. (4), the lower loss reinforce other terms to constrain the overall value. From mutual information perspectives (Appendix D.3 for details), we have

\[I(Z_{i},Z_{j}) H^{}(Z_{j}|Z_{i})-H(Z_{j}|Z_{i})+ N-}_{}.\] (11)

When all negative samples \(k\) satisfy \(s(_{i},_{jk}) 0\), we obtain \(H^{}(Z_{j}|Z_{i})=H(Z_{j}|Z_{i})\). Subsequently, as the shared information between hard positive samples increases, \(H^{}(Z_{j}|Z_{i})\) correspondingly increases, thereby elevating both the lower bound of mutual information and the confidence level, which consequently facilitates the learning of unique information.

### Training Objectives

In summary, we apply (1) SIC (Eq. 2) to keep shared information relevance between different modalities, (2) UIC (Eq. 5) to build quadruple embedding space by maximizing the alignment of normal vector spanned by shared representation and unique representation, and the shared information is jointly optimized by SIC and UIC, (3) Self-penalization (Eq. 7) to amplify the effect of false positive samples in unique information optimization. The overall objective can be formulated as:

\[_{}=_{}+_{}.\] (12)

## 3 Experiment

### Experiment Setup

**Baselines and Setup.** Shortcut learning refers to the process in deep learning model training where the model completes tasks (such as classification, retrieval, etc.) by learning simple and discriminatory features while ignoring the semantic and more complex features of the data. This can result in poor model performance on downstream tasks. Latent target decoding (LTD) ,and implicit feature modification (IFM)  are two methods that mitigate shortcut learning by reducing feature suppression. LTD reconstructs the caption representations in the latent space of a Sentence-BERT model, allowing the encoder to mitigate feature suppression via correct mapping. IFM perturbs discriminatory features through encoders and removes part of these features to avoid learning shortcuts, which is implemented as a dual loss combined with the InfoNCE loss. We provide source code of our paper. 2.

Image Caption Retrieval(ICR) retrieves the most relevant sample in another modality by using a sample of one modality as a query. In this task, there are two retrieval modes: text-to-image (t2i) and image-to-text (i2l). We evaluated our method in the ICR task using CLIP  and VSE++  models on Flickr30k  and MS-COCO  datasets.

Bleeker et al.  proposed synthetic shortcuts for the vision-language framework. This allows us to evaluate whether vision language (VL) models capture easy-to-learn discriminatory features or task-related information. We add MNIST Images to the top of pictures and appending corresponding numbers at the end of their respective captions. This controlled approach preserves the original information from both modalities and increases explicit mutual information between them.

**Evaluation Metrics.** To evaluate the model's performance on the Flickr30k and MS-COCO-Caption datasets, the Recall@K (i.e. R@1, R@5, R@10, which refers to the proportion of instances where the correct answer appears among the top K returned results out of all instances) and recall sum (RSUM) were selected as evaluation metrics for both i2t and t2i retrieval.

**Implementation Details.** We select ViT-B/32 as the visual backbone for CLIP and resnet152 for the VSE++ model when we evaluate them on the caption retrieval task using Flickr30k and MS-COCO dataset. We fine-tuned the pre-trained CLIP on downstream tasks and trained VSE++ from scratch with our method.

**Alternatives of Unique Decoder.** The key of multi-view assumption lies in the extraction of unique information, current methods achieve this by employing a single-layer MLP, ensuring orthogonality with shared information or through data augmentation and factorized loss. In our framework, a single-layer MLP is used for the ResNet backbone, while a two-layer Transformer is implemented for the Transformer backbone. Detailed methodologies are provided in the Appendix B.2.

### Performance Evaluation

**QUEST vs. Vanilla InfoNCE.** QUEST outperforms the vanilla InfoNCE, as shown in Table 1. On the Flickr30k test set, QUEST yields \(R@1\) improvements of (\(2.4\), \(1.5\)) for CLIP and (\(1.1\), \(3.4\)) for VSE++ in i2t and t2i tasks.. The corresponding RSUM metrics increase by \(3.2\) and \(12.1\). On MS-COCO, QUEST achieves \(R@1\) gains of (\(1.6\), \(0.9\)) for CLIP and (\(3.1\), \(3.2\)) for VSE++ in i2t and t2i tasks, with corresponding RSUM improvements of \(8.1\) and \(17.3\). Additionaly, QUEST exhibits faster convergence to the optimal solution.

**Experiment on Synthetic Shortcuts.** To assess the effectiveness of our proposed QUEST mitigating feature suppression in Contrastive Learning, we use synthetic shortcuts  by injecting easy-to-learn and discriminatory shared information into the image-text training dataset. We then evaluate the model's performance on downstream tasks with and without these synthetic shortcuts to determine if the presence of shortcuts causes the suppression of other task-related information and an over-reliance on shortcut features. Our baselines' results are consistent with .

As shown in Table 1, adding shortcuts leads to performance degradation across all models to some degree, indicating that the models have not learned sufficient shared and unique information. However, our method outperforms LTD and IFM, indicating it captures task-related information more effectively in downstream tasks (evaluation without shortcuts).

**CLIP Performance Enhancement.** Our method significantly enhances the CLIP model's performance on the Flickr30k and MS-COCO datasets. Compared to InfoNCE, we observe substantial R@1 improvements in both i2t and t2i tasks, with RSUM increases of \(91.6\) and \(240\) on Flickr30k and MS-COCO, respectively. Our approach also outperforms previous SOTA methods, surpassing \(_{}\) on Flickr30k and showing significant performance improvements against \(_{}\) on MS-COCO.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

[7; 6; 26; 10]. Nevertheless, our work is orthogonal to most of the aforementioned studies. Our shared branch approach can be integrated with existing training methods such as MoCo  and SimCLR .

### Shortcuts Learning

Shortcut Learning refers to the tendency of deep neural networks to exploit simple but potentially unreliable features (i.e., "shortcuts") in data for decision-making, rather than learning more complex but reliable features . This phenomenon can lead to poor model performance on out-of-distribution data and is particularly common in multimodal retrieval  and VQA tasks [55; 18]. Robinson et al.  proposed strategically adjusting the feature distribution of positive and negative sample pairs to achieve implicit feature modification in contrastive learning, guiding models to learn more robust feature representations. Sanchez et al.  propose maximizing mutual information to capture data attributes in shared and exclusive representations, while minimizing it between them to enforce disentanglement. LTD  introduces an additional decoder to reconstruct input text descriptions in the latent space of a universal sentence encoder, preventing image and text encoders from suppressing predictive features. More recently, some works strive to enhance the estimation of mutual information through the utilization of stricter bounds [30; 41; 13; 51] or the introduction of regularization constraints , consequently preserving unique information more effectively.

## 5 Conclusion

We introduce QUEST, a framework utilizing specialized decoders to extract both unique and shared information via shared information-guided constraints and self-penalization. This study addresses the challenges of imbalanced negative samples and task-related unique feature suppression in Multimodal Contrastive Learning. Our method optimizes shared and unique representations simultaneously, outperforming state-of-the-art methods in preserving unique information and enhancing contrastive learning. Unlike traditional approaches that employ direct dot products to minimize distances between positive samples, QUEST leverages quaternions for the indirect optimization of unique and shared information. However, the application of cross products in high-dimensional spaces is limited, complicating the control of high-dimensional representations and reducing theoretical interpretability. For further discussion on these limitations, see the appendix (Appendix E).

Figure 5: Case Study: (a) Image-to-text retrieval, where the results of \(_{}\) and \(_{}\) are denoted by italics and underlines, respectively. (b) Text-to-image retrieval, where red and green borders indicate the top-5 retrievals using \(_{}\), while blue borders represent those using \(_{}\). The upper and lower sections in both (a) and (b) demonstrate scenarios with and without shortcuts, respectively.