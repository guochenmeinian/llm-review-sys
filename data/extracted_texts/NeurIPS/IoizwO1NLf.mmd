# Skill-it! A data-driven skills framework

for understanding and training language models

Mayee F. Chen

Stanford University

&Nicholas Roberts

University of Wisconsin-Madison

&Kush Bhatia

Stanford University

&Jue Wang

Together AI

&Ce Zhang

Together AI, University of Chicago

&Frederic Sala

University of Wisconsin-Madison

&Christopher Re

Stanford University

###### Abstract

The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains \(37.5\) points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by \(13.6\)% versus training on data associated with the target skill itself. We apply our skills framework on the RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.

## 1 Introduction

Large language models (LMs) exhibit remarkable capabilities, including producing creative content , writing source code , and chatting with users . A key ingredient in enabling models to perform such tasks is the data on which the models are trained [18; 20; 59]. A natural way to unlock particular capabilities is to improve this training data. However, it is unclear how to select data from a large corpus for these capabilities given a fixed budget of training tokens, as data selection methods for current state-of-the-art LMs mostly rely on heuristics for filtering and mixing together different datasets [33; 59]. We lack a formal framework for capturing how data influences the model's capabilities and how to utilize this data effectively for improving LM performance.

To develop such a framework, we take inspiration from how humans acquire knowledge. A classic idea in education literature is the concept of _skills_ that form a learning hierarchy . For example, one study found that students learned mathematical and scientific skills most quickly when these skills were presented in a particular order . We seek to understand the extent that similar skill-based orderings characterize LM training. Such orderings, if they exist, may provide a better understanding of LMs as well as a mechanism for data-efficient training. For instance, to train an LM for Spanish question generation, we wish to know if training first on related but simpler tasks, such as Spanish grammar and English question generation, helps.

We study if the idea of skill orderings can help us build a framework that relates data to LM training and behavior. This requires addressing two challenges revolving around the connection between skills and data. First, in order to show that there exist sets of skills that the LM learns most efficiently in some particular order, _an operational definition of LM skill and skill ordering must be developed and validated on data_. In initial experiments, we investigated if semantic groupings of data, such as metadata attributes or embedding clusters, were sufficient to represent a skill and characterize how models learn. For instance, we partitioned the Alpaca dataset  by instruction type--a technique used to capture dataset diversity --but we found that sampling based on instruction types and random sampling resulted in similar model performance, suggesting that not just any existing notion of data groups can characterize skills.

Second, _these definitions of skills must be used to construct sampling distributions to actually improve model training._ To develop criteria for a data selection algorithm that learns skills efficiently, we identify challenges that naive selection approaches face. The standard approach of random uniform sampling over data fails to learn skills optimally due to not accounting for skill imbalance and ordering. Skills can be distributed unevenly in the data, with more complex skills being rare--for instance, Spanish and question generation (QG) are \(5\%\) and \(4\%\) of the Natural Instructions dataset , respectively, but Spanish QG is only \(0.2\%\). Random sampling also provides no mechanism for taking into account a particular training order and dependency structure on skills. More sophisticated techniques like curriculum learning account for sample-level ordering, but not skills or their dependencies. Our goal framework must account for these issues of imbalance and ordering.

**Skill-based framework** We define a _skill_ as a unit of behavior that a model can learn using an associated slice of data (Definition 1). An _ordered skill set_ is a collection of skills with a directed _skills graph_ that is neither complete nor empty, where an edge from a prerequisite skill to a skill exists if the amount of training it takes to learn the skill can be reduced if the prerequisite skill is also learned (Definition 2, Figure 1 left, center). We show that ordered skill sets exist in synthetic and real datasets using this operational definition. Interestingly, the existence of these ordered skill sets unveils that one can learn a skill quickly not by training solely on that skill, but on a mixture of that skill and prerequisite skills. For instance, in Figure 3 we observe that Spanish QG can be learned more efficiently when the model also learns English QG and Spanish--we can achieve \(4\%\) lower validation loss than training on only Spanish QG over a fixed budget of overall training steps.

Next, given an ordered skill set to train on, we use our framework to propose methods for how to select data so that the LM learn skills faster: skill-stratified sampling and an online generalization, Skill-It. We address the issue of unevenly distributed skills in datasets by proposing skill-stratified sampling, a

Figure 1: Inspired by how humans acquire knowledge, we hypothesize that LMs best learn skills in a particular order and that this can help improve our understanding and training of LMs. We show that these ordered skill sets exist in real data, which enables skills to be learned with less data given that we train on their prerequisite skills. We then propose Skill-It, an online data selection algorithm that learns skills quickly by exploiting their ordering.

simple approach that allows us to explicitly optimize for learning skills by uniformly sampling relevant skills (such as a target skill and its prerequisite skills in fine-tuning). Skill-stratified sampling uses the construction of the ordered skill set but is static, which does not incorporate the ordering as training proceeds and results in oversampling skills that may be already learned early on in training. We address this issue by proposing an online data selection algorithm, Skill-It, for selecting mixtures of training skills that allocates more weight towards learning skills that are not yet learned or towards prerequisite influential skills (Figure 1 right). Skill-It is derived from an online optimization problem over the training skills for minimizing loss on a set of evaluation skills given a fixed budget of data and the skills graph. Skill-It is inspired by online mirror descent and can be adapted for continual pre-training, fine-tuning, or out-of-domain evaluation depending on the relationship between the evaluation skill set and the training skill set.

We evaluate Skill-It on synthetic and real datasets at two model scales, 125M and 1.3B parameters. For the continual pre-training setting, we show on the LEGO synthetic  that we obtain a \(37.2\) point improvement in accuracy over randomly selecting training data and a \(9.7\) point improvement over curriculum learning . For the fine-tuning setting, we show that on the widely-used Natural Instructions dataset [41; 64], our algorithm over a mixture of skills is able to achieve up to 13.6% lower loss on that skill than solely training on that skill, given the same overall training budget. For the out-of-domain setting when our training skills do not align perfectly with evaluation skills, our algorithm is able to achieve the lowest loss on 11 out of 12 evaluation skills corresponding to task categories in the Natural Instructions test tasks dataset over random and skill-stratified sampling on the training data. We finally apply our framework to a case study on the recent RedPajama 1.2 trillion token dataset . We use the data mixture produced by Skill-It to continually pre-train a 3B parameter model. We find that Skill-It achieves higher accuracy with 1B tokens than uniform sampling over data sources with 3B tokens.

## 2 Related work

An extended related work can be found in Appendix B. Existing work on data selection for LMs has generally ranged from more computationally expensive methods for dataset condensation on smaller datasets [47; 58; 48] to broader deduplication and filtering techniques for web-scale datasets [1; 33; 69]. Another way of improving model performance through choice of data is via curriculum learning , which also draws inspiration from how humans learn and arranges data in order from easiest to hardest over samples or groups . In contrast to existing works in both curriculum learning and data selection, our work focuses on selecting data for learning an ordered set of skills more efficiently. How LMs learn is also a topic of growing interest; one framework posits that models learn over quanta, discrete units of computation , and another proposes that scaling laws for LMs can be understood in terms of learning combinations of skills . Lastly, the notion of skill has been studied in education, ranging from classical research on learning hierarchies  to methods for decision-making over lesson sequences .

## 3 Skills framework

First, we propose definitions of skills and ordered skill sets in order to formalize our intuition around how models learn skills, and we demonstrate that not just any existing notion of data groups can characterize an ordered skill set in the dataset. Then, we demonstrate the existence of ordered skill sets on synthetic and real data, which show how viewing data through a skills-based framework can help with training and understanding model performance. Finally, we explore unsupervised skill recovery from data, finding that embedding-based approaches do not adequately recover synthetic skills.

### Definitions

We first present a definition of an individual skill. Let the input space of all possible text data be \(\), where \(x\) is an individual text sample that a next-token-prediction LM \(f:\) is trained on. We quantify learning via a metric \(L:\), which maps from a model and evaluation data to a scalar quantity. In our setup, we use the cross-entropy validation loss applied over next-token predictions as our metric \(L\).

**Definition 1** (Skill): _A skill \(s\) is a unit of behavior with associated data \(_{s}\) such that if \(f\) is trained on an dataset \(_{s}_{s}\), \(f\) has improved metric \(L\) afterwards on samples belonging to \(_{s}_{s}\) on average._

This definition of a skill is flexible--it simply means that given a training dataset associated with the skill, a model \(f\) has an improved metric (e.g., decreasing validation loss) when evaluated on validation data associated with this skill. Under this definition, a skill could be a granular task, such as Spanish question generation for a subset of Wikipedia articles, or can be defined over a data source, such as next-token prediction of legal data from tax court rulings. However, our next definition, the ordered skill set, has a more specific construction and provides a framework for how models learn across dependent skills.

**Definition 2** (Ordered skill set, skills graph): _An ordered skill set for \(f\) is a collection of skills \(\!=\!\{s_{1},...,s_{k}\}\) over which there is a directed skills graph \(G\!=\!(\!,\!E)\) on the skill set that is neither complete or empty, where \((s_{i},s_{j})\!\!E\) if the amount of data needed to learn \(s_{j}\) when uniformly sampling from \(_{s_{i}}\!\!_{s_{j}}\) is no more than the amount of data needed when sampling only from \(_{s_{j}}\). We equate learning a skill \(s_{j}\) to \(f\) attaining a certain value of \(L\) or lower on average over \(_{s_{j}}_{s_{j}}\)._

This definition isolates complete and empty graphs as extrema that do not capture meaningful sets of skills. We discuss the three types of skill graphs--complete, empty, intermediate--and their implications for data selection. In particular, we discuss how several initial attempts of defining skills over datasets via semantic groupings resulted in the extrema cases (see Appendix D.2 for full results):

* The complete graph demonstrates that all skills influence each other. A random partition is an example of a skill set that yields a complete graph. This graph suggests that the best approach for learning any skill or set of skills is random sampling on the dataset. This is not a setting where we can gain much with skill-based sampling. For example, using instruction types as skills on the Alpaca dataset results in a nearly complete estimated skills graph (\(97.4\%\) dense), and we find that stratified sampling on these skills only improves validation loss per skill by \(0.007\) points over random sampling on average (Figure 2 left), suggesting that utilizing skills does not improve model performance in this case.
* The empty graph demonstrates that each skill is independent. This can occur if skills are too granular; for instance, learning Spanish math problems is unlikely to help with English poem generation. This graph suggests that the best approach for learning an individual skill is to train on the skill itself. We see that empty graphs exist in real data; in Figure 2 (center), using data sources as skills on the Pile of Law  results in a nearly empty skills graph (\(3.9\%\) dense).
* Graphs that are neither empty nor complete thus suggest a nontrivial order of how skill influence each other. _This is the setting in which we expect that identifying skills and exploiting their ordering will help the most_. In Figure 2 right, we use task categories, which capture broader reasoning patterns, as skills on Natural Instructions and find that the estimated graph has intermediate density (\(42.7\%\) dense). We show concrete examples of how skills can be learned more efficiently on Natural Instructions in Section 3.2.

While these intuitive groupings result in ordered skill sets on some datasets (e.g., task categories on NI), this is not always the case (e.g., instruction types on Alpaca and sources on Pile of Law). Even though these groupings capture some notion of diversity in the dataset, our findings suggest that not just any semantic grouping induces an ordered skill set. We now empirically demonstrate that our definition of ordered skill sets aligns with how models learn and can be exploited for more data-efficient training.

### Examples of skills and ordered skill sets

We provide examples of ordered skill sets on the LEGO synthetic dataset, an addition synthetic dataset, and subsets of the Natural Instructions dataset. On these datasets, we find that certain skills are better learned when trained along with their prerequisite skills rather than in isolation.

Figure 2: Heatmaps of adjacency matrices we compute for skill graphs for Alpaca, Pile of Law, and Natural Instructions. Negative elements and diagonals are thresholded to \(0\) for clarity. See Appendix D.2 for descriptions of how they were constructed and larger versions.

**LEGO skills** The LEGO synthetic, first introduced in , can evaluate a model's ability to follow a chain of reasoning. In this synthetic, the letters of the alphabet, \(\), are variables each with some binary label in \(\{0,1\}\). An individual sample consists of \(k\) clauses for some fixed \(k\) across the dataset, each of the form \(a\!=\!gx\) where \(a,\!x\!\!\) and \(g\) is either a negation ("not") or assertion ("val"), e.g. we assign \(a\) to the value of \(x\), or we assign \(a\) to the opposite label. At the end of the sentence, we prompt the model for what the value of one of these variables is. Two samples \(x\!\!\) are given below for \(k\!=\!5\):

\[\] \[\]

These samples each correspond to a chain of reasoning; for instance the first sample has the chain \(r,\!y,\!b,\!m,\!q\), where knowing \(q\)'s label requires the most reasoning steps. We define the \(i\)th skill \(s_{i}\) as the model's ability to know the \(i\)th variable of the chain. From our example above, the first sample belongs to \(_{s_{3}}\) and the second sample belongs to \(_{s_{1}}\). To demonstrate the existence of ordered skill sets, we continually pre-train the 125M parameter GPT-Neo model  over various mixtures of LEGO skills with \(k\!=\!5\). In Figure 3 (left), we find that in 35.9% fewer training steps, training on a balanced mixture of \(_{s_{1}},\!_{s_{2}}\), and \(_{s_{3}}\) resulted in the same validation loss of \(0.01\) as training solely on \(_{s_{3}}\). This suggests that \(s_{1},s_{2}\) helped unlock performance on \(s_{3}\) and that there exist edges from \(s_{1}\) or \(s_{2}\) to \(s_{3}\) in the skill graph. Additional observations are available in Appendix E.1, where we examine other edges as well as more complex reasoning chains, and the full skills graph corresponding to the ordered skill set for LEGO with \(k\!=\!5\) is in Figure 11.

**Addition skills** We consider a variant of a synthetic 5-digit addition dataset analyzed in . We show the existence of ordered skill sets for a simplified 3-digit addition dataset where we treat each digit prediction as a skill--the outputs, in this case, are the integers \(\{0,1,\)...,\(9\}\). Examples are of the following form:

\[\)) and 'A 2' refers to the hundreds digit (\(s_{3}\)). In Figure 3 (center), we find that in \(32\%\) fewer training steps, training on a balanced mixture of \(_{s_{1}}\), and \(_{s_{2}}\) resulted in the same validation loss of \(0.01\) as training solely on \(_{s_{1}}\). That is, the ones digit addition skill can be improved by simultaneously learning the tens digit addition skill, even though the former should not require information from the latter--this is in line with observations from prior work that models do not always learn the ones digit addition first . The full skills graph corresponding to the ordered skill set over 3-digit addition is in Figure 12.

**Natural Instructions (NI) skills** We show that ordered skill sets exist in NI  when we treat task categories as skills.

* In Figure 3 (top right), we show that ordered skill sets exist over crosslingual task categories. Training on Spanish question generation (QG) along with equal parts of English QG, Spanish question answering (QA), and English QA results in \(4.1\%\) lower validation loss than training only on Spanish QG. Remarkably, the former only uses \(25\%\) of the latter's Spanish QG data. This suggests that there are edges from Spanish QA, English QA, and English QG to Spanish QG.
* In Figure 3 (bottom right), we see that training on the task category Text Matching along with Stance Detection helps decrease the loss on Stance Detection by \(11\%\). This suggests that these categories, which both involve understanding the relationship between two input texts, share an edge.

Figure 3: On the LEGO synthetic, 3-digit addition, and Natural Instructions, we identify examples of ordered skill sets in which training on a mixture of skills helps learn an individual skill faster than just training on that skill itself, given a fixed training budget.

The full skills graphs corresponding to the ordered skill sets over these task categories are in Figure 14. While equating task categories to skills may be noisy, these examples suggest that there is signal within real data that suggests that ordered skill sets can improve data efficiency.

### Skill recovery

A final component of characterizing skills is unsupervised recovery of ordered skill sets. We consider embedding-based clustering approaches and a loss-based clustering approach for recovering LEGO skills. When clustering data using various trained and pre-trained embeddings, we find that they were unable to achieve above \(39\%\) accuracy on LEGO. Instead, we find that taking \(10\) random training runs and clustering data by their _loss_ per timestep per run recovers the skills with \(61\%\) accuracy (Table 3). The intuition behind this method is that the validation losses on points from the same skill have similar trajectories as models learn. We discuss this approach more in Appendix E.2.

## 4 Skills-based data selection

Now that we have established the existence of ordered skill sets, we discuss how to use them for data selection. We state the data selection problem for learning across skills in Section 4.1. We discuss how to learn the skills graph that will be exploited in our data selection methods in Section 4.2. We then introduce two sampling methods that utilize the graph, a simple skill-stratified sampling method and the online sampling method Skill-It, in Section 4.3.

### Problem statement

We are given an ordered training skill set \(_{}=\{s_{},1,...,s_{,k}\}\) on the training data, each with associated support set \(_{s_{},1},..._{s_{,k}}\), and an ordered evaluation skill set \(_{}=\{s_{,1},...,s_{,m}\}\) of \(m\) evaluation skills on a separate evaluation dataset. We aim to select \(n\) samples from \(_{}\) via a mixture of training skills, \(p^{k-1}\), to achieve three goals depending on how \(_{}\) is constructed:

* **Continual pre-training**: when \(_{}=_{}\), our goal is select a mixture of training skills to learn all of them.
* **Fine-tuning**: when \(_{}_{}\), our goal is to select a mixture of training skills to learn an individual target skill or subset of these skills.
* **Out-of-domain**: when \(_{}_{}=\), our goal is to select a mixture of training skills to learn a disjoint set of evaluation skills we cannot train on. This can arise when we have a separate downstream validation dataset or the skills identified in the training dataset are noisy.

Furthermore, we have a skills graph \(G=(_{}_{},E)\), where \(E_{}_{}\) and \(A^{k m}\) is a weighted adjacency submatrix, where \(A_{ij}\) describes the strength of the edge from \(s_{,i}\) to \(s_{,j}\). In Table 1, we summarize how the three different settings are constructed and how \(A\) varies across them. Next, we discuss how \(A\) can be estimated from the data.

### Skills graph learning

The skills graph is important for determining how to sample from the ordered skill set for training efficiently. We present two approaches for learning the skills graph--brute-force and linear approximation. Algorithms are provided in Appendix C.2. By definition 2, the brute-force way of identifying edges involves fixing an overall training budget of \(H\) steps and 1) training and evaluating the model on each \(s_{,i}\) and 2) training the model on each pair of \((s_{,i},s_{,j})\) and evaluating on \(s_{,i}\) and \(s_{,j}\). If the loss on \(s_{,j}\) when trained on both \(s_{,i}\) and \(s_{,j}\) is lower, there exists an edge from \(s_{,i}\) to \(s_{,j}\) with edge weight proportional to the difference in loss. This approach has runtime \((Hkm)\) and is only feasible when \(k\) is small and when we have access to \(_{}\) at training

   Setting & \(_{}\) & Skills graph \\  Continual pre-training & \(_{}=_{}\) & \(A^{k k}\), edges among all \(_{}\) \\ Fine-tuning & \(_{}_{}\) & \(A^{k m}\), edges from training skills to target subset \\ Out-of-domain & \(_{}_{}=\) & \(A^{k m}\), edges from training skills to evaluation skill set \\   

Table 1: Summary of three settings—continual pre-training, fine-tuning, and out-of-domain. These settings are determined by how \(_{}\) is defined and result in different skills graphs used for our sampling methods.

time. Otherwise, we can approximate this approach in linear time by training on each \(s_{i}\) for \(h<H\) steps and setting \(A_{ij}>0\) if the loss on \(s_{j}\) decreases over \(h\) steps for a runtime of \((hk)\). This linear approach is necessary in the out-of-domain setting, since it does not require training on \(_{}\). In addition, both graph learning approaches can be performed on a smaller model, and the learned graph can be used for data selection for training a larger model (Appendix E.4).

### Skills graph-aware sampling

We present two approaches for sampling over the mixture of training skills according to the skills graph: skill-stratified sampling, which samples uniformly over relevant training skills according to \(A\), and Skill-It, an online generalization that incorporates feedback of how skills are being learned so far.

#### 4.3.1 Skill-stratified sampling

A straightforward sampling approach is to discard training skills that do not benefit the evaluation skills and sample uniformly over the set of relevant training skills, which we call _skill-stratified sampling_. For continual pre-training, the relevant skills are the entire training skill set; for each \(s_{,i}_{}\), \((s_{,i})=\). This enables each skill to have sufficient training data. For fine-tuning, the relevant skills are the target skills and prerequisite skills, which can be identified via positive entries of the \(i\)th column of \(A\) with \(_{}=\{s_{,i}:\ s_{,j} { s.t. }A_{ij}>0\}\). We then set \((s)=_{}_{}|}\) for \(s_{}_{}\). For the out-of-domain setting, skill-stratified sampling is over the set of prerequisite skills. For each \(s_{}\), we set \((s)=_{}|}\). Next, we propose our online algorithm that exploits the graph dynamically for more efficient training.

#### 4.3.2 Skill-It online data selection algorithm

Despite accounting for prerequisite skills, one shortcoming of skill-stratified sampling is that even if a skill has already obtained sufficiently low validation loss early during training, we will continue to allocate the same weight to that skill throughout training. Therefore, we formulate our data selection problem as an online learning problem and propose Skill-It, which both prioritizes prerequisite skills and skills that are not yet learned.

We are given a budget of \(T\) rounds and \(n\) total samples to train on. At round \(t\), we select a mixture \(p_{t}^{k-1}\) from the \(k\)-dimensional unit simplex, and for each training skill \(s_{,i}_{}\), we sample from \(_{s_{,i}}\) with proportion \(p_{t}^{i}\) for a total of \(\) samples per round. Let \(f_{t}\) be the model at at the start of round \(t\). We can define \(f_{t}\) recursively as a function of the previous round's model \(f_{t-1}\) and mixture \(p_{t-1}\) via a dynamics function \(:^{k-1}\); that is, \(f_{t}=(f_{t-1},p_{t-1})\). Let \(L_{,j}(f_{t})\) be the validation loss of \(f_{t}\) on \(s_{,j}\). Our goal is to select \(p_{1},\)...,\(p_{T}\) to minimize loss per evaluation skill at the end of training:

\[,,p_{T}^{k-1}}{}^{m}}L_{,j}(f_{T}).\] (1)

This optimization problem is challenging to solve without additional assumptions. In order to make the problem tractable, we impose an explicit dynamics rule for the each evaluation skill's loss \(L_{,j}\) in terms of the current loss and data mixture. Assuming for simplicity that \(_{}_{}\), a simple rule would be \(L_{,j}(f_{t})=L_{,j}((f_{t-1},p_{t-1})):=L_{,j}(f_{t-1})(1- p_{t-1}^{j})\) for \(\). That is, we expect that allocating more data to skill \(j\) should result in the validation loss on skill \(j\) decreasing. However, such an expression assumes that only training on the \(j\)th skill will help learn the \(j\)th skill. Instead, Section 3.2 suggests that there are other skills that may help with the \(j\)th skill. We propose the following dynamics:

\[L_{,j}(f_{t})\!=\!L_{,j}(f_{t-1})(1\!-\!A_{:,j}^{}p_{ t-1}),\] (2)

where \(A_{:,j}\) is the column with weights of all skills that influence \(s_{,j}\), and we absorb the scalar \(\) into \(A\). The optimization problem in (1) can thus be simplified as follows:

\[,,p_{T}^{k-1}}{} \!_{j=1}^{m}\!L_{,j}(f_{T})\] (3) \[\,\,f_{t}\!=\!(f_{t-1},\!p_{t-1})\, t\!=\!1,\! T\] \[L_{,j}(f_{t})\!=\!L_{,j}(f_{t-1})(1\!-\!A_ {:,j}^{}p_{t-1})\, j\!\![m]\]

In Appendix C, we derive the following update rule via online mirror descent  for learning rate \(\!>\!0\):

\[p_{t+1}^{i}\!=\!p_{t}^{i}\!\!\!_{j=1}^{m}\!A_{ij}L_{ {eval},j}(f_{t}).\] (4)

In addition, when equation 4 is expanded, we have that \(p_{t+1}^{i}\!=\!p_{t}^{i}\!\!\!_{=1}^{t}\!_{j=1}^{ m}\!A_{ij}L_{,j}(f_{})\). Since this summation over \(\) results in diminishing strength of updates, we change it to a moving window of size \(w\). Our full method is in Algorithm 1.

Intuitively, at each step we adjust the weight on skill \(i\) based on the losses of skills that \(i\) influences, with the assumption that more training data helps decrease loss. Note that when we use our algorithm with a complete graph or empty graph, we achieve expected behavior discussed in Section 3.1. For the complete graph, our algorithm reduces to stratified sampling. When we have a skill set with an empty graph, the update rule reduces to sampling proportional to each skill's validation loss.

## 5 Experimental results

Given an ordered skill set, we aim to validate Skill-It's ability to select data for efficiently learning skills in the continual pre-training, fine-tuning, and out-of-domain settings. We provide full tables of results in Appendix E.3.1 and results where we learn the skills graph on the 125M model and use it for the 1.3B parameter model in Appendix E.4. Skills graphs are in Appendix D.2, weight trajectories for Skill-It are in Appendix E.3.2, and ablations on the graph and online components of Skill-It are in Appendix E.5.

### Continual pre-training

**Setup** We evaluate the ability of Skill-It to select data for efficiently learning over all skills. We measure average validation loss per skill after a fixed number of training steps. We construct the LEGO synthetic and addition synthetic with \(k\!=\!5\) and \(3\), respectively, and an imbalanced dataset over the skills. On the Natural Instructions dataset, we use \(23\) of the task categories as skills.

**Baselines** We compare Skill-It against three baselines that do not account for skills: random sampling, curriculum learning, and anticurriculum learning. Random sampling is a standard procedure for selecting samples given no additional information. Curriculum learning  and anticurriculum learning  score the samples from easiest to hardest and vice versa, respectively, and sample over an expanding set of the lowest scored samples at every epoch; we use the pre-trained model's loss to rank points. We evaluate skill-stratified sampling, which uses knowledge of the skills but is not online, and include an additional skills curriculum and anticurriculum baseline from , which samples from skills in order of average loss per skill.

**Analysis** Across our experiments we find that Skill-It outperforms baselines that do not use skills as well as skill-stratified sampling and skill curriculum learning. Our results on the LEGO dataset are shown in Figure 4. Skill-It and skill-stratified sampling attain lower loss than other approaches on skills 2, 3, 4 and on average, and while curriculum and anticurriculum learning attain lower loss on skill 5, they fail to learn other skills, resulting in at most \(68\) points accuracy on skill 3. Skill-It reachesa high average accuracy earlier in training than other approaches; halfway through training, Skill-It obtains between \(8.3\) and \(33.5\) points higher average accuracy than other approaches, reaching a final accuracy of \(99.3\) (Figure 20). Skill-It initially allocates more weight to prerequisite skills such as skill 2 as suggested by Figure 11 and later on allocates more weights to the skills that are learned more slowly, such as skills 4 and 5 (Figure 22). On the addition synthetic with \(k=3\), Skill-It obtains lower validation loss than the baselines on skills 1 and 2 in Figure 5. While most approaches aside from skill curriculum learning eventually obtain \(100\%\) accuracy on all skills, Skill-It requires less training to reach sufficiently high accuracy; halfway through training, Skill-It has accuracy between \(8.7\) and \(73.1\) points higher than other approaches (Figure 21). Finally on Natural Instructions, the average validation loss from Skill-It is \(3.2\%\) lower than from random sampling (Table 7). Our results suggest that exploiting the construction and ordering of skills is critical to learning skills quickly.

### Fine-tuning

**Setup** We evaluate the ability of Skill-It to select data from an ordered training skill set for learning a target skill. Mirroring Figure 3, we evaluate on LEGO target skill 3 (third in reasoning chain), on the addition synthetic's skill 1 (ones place digit addition), and on NI's Spanish QG and Stance Detection.

**Baselines** We compare Skill-It against training on the target skill only and skill-stratified sampling over prerequisite skills and the target skill. The skill-stratified sampling approach uses the ordered skill set to identify prerequisite skills, but does not exploit them dynamically.

**Analysis** Our results are shown in Figure 6. On LEGO, Skill-It results in the same validation loss of \(0.01\) as training only on the target skill in \(38.1\%\) fewer steps. We observe a similar trend on addition, with Skill-It converging to a validation loss of \(0.01\) in \(59\%\) fewer steps required to do so when training only on the target skill. Finally, on NI, Skill-It improves validation loss on Spanish question generation by \(5.3\%\) and Stance Detection by \(13.6\%\) over just training on the respective target skill only. In this setting,

Figure 4: Performance of Skill-It on each skill in the continual pre-training setting (learning over all skills in the ordered training skill set) on the LEGO synthetic.

Figure 5: Performance of Skill-It in the continual pre-training setting on the addition synthetic.

Figure 6: Performance of Skill-It in the fine-tuning setting on LEGO, addition, and NI.

a significant portion of the improvement over training only on the target skill comes from identification of prerequisite skills through the learned graph in the skill-stratified sampling method. Skill-It is further able to improve performance with finer-grained dynamic weighting on prerequisite skills.

### Out-of-domain setting

Natural InstructionsWe evaluate the ability of Skill-It to select data from a set of training skills for learning a disjoint set of evaluation skills that we cannot train on. We use all \(59\) task categories in the NI train tasks split as the training skills and the \(12\) task categories in the test tasks split as our evaluation skills. We compare Skill-It against random and skill-stratified sampling, both of which do not exploit the relationships between training skills and evaluation skills. Skill-It achieves the lowest loss on 11 out of 12 task categories over random and skill-stratified sampling (Figure 7, tables in Appendix).

RedPajamaWe use Skill-It to produce a data mixture on the RedPajama dataset. The training skills are the data sources comprising the dataset, and the evaluation skills are several tasks from the Language Model Evaluation Harness . Skill-It with \(T\!=\!1\) yields the mixture in Figure 8 (right). We continually pre-train a 3B parameter model trained on 1T tokens for 3B additional tokens using this mixture, and see that it outperforms uniform sampling over the data sources (Figure 8 left). In particular, Skill-It achieves higher accuracy with 1B additional tokens than uniform with 3B additional tokens.

## 6 Conclusion

Given a fixed budget of data, knowing what data to train on to induce various capabilities in an LM is challenging. As LMs continue to improve, it will become increasingly important to extract as much signal as possible from the data and to direct that signal towards acquiring a broad variety of capabilities. In this paper, we introduce a skills-based framework for understanding how LMs learn and for selecting training data. We hope our study invites others to build on such a notion of skill and further explore how to align skills with data.

Figure 8: Left: Accuracy on LM Evaluation Harness for continual pre-training of a 3B parameter model using Skill-It on the RedPajama dataset. We achieve higher accuracy at 1B additional tokens than uniform at 3B tokens. Right: Skill-It mixture over RedPajama sources.

Figure 7: Performance of Skill-It in the out-of-domain setting for the NI test task split. Skill-It uses the graph between the train and evaluation skills to produce an online mixture on the training dataset.