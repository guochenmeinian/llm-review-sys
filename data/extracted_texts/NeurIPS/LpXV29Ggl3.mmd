# Exploratory Retrieval-Augmented Planning For

Continual Embodied Instruction Following

 Minjong Yoo\({}^{1}\), Jinwoo Jang\({}^{1}\), Wei-jin Park\({}^{2}\), Honguk Woo\({}^{1}\)

\({}^{1}\)Department of Computer Science and Engineering, Sungkyunkwan University

mjyoo2@skku.edu, jinustar@skku.edu, jin@acryl.ai, hwoo@skku.edu

Corresponding Author

###### Abstract

This study presents an Exploratory Retrieval-Augmented Planning (ExRAP) framework, designed to tackle continual instruction following tasks of embodied agents in dynamic, non-stationary environments. The framework enhances Large Language Models' (LLMs) embodied reasoning capabilities by efficiently exploring the physical environment and establishing the environmental context memory, thereby effectively grounding the task planning process in time-varying environment contexts. In ExRAP, given multiple continual instruction following tasks, each instruction is decomposed into queries on the environmental context memory and task executions conditioned on the query results. To efficiently handle these multiple tasks that are performed continuously and simultaneously, we implement an exploration-integrated task planning scheme by incorporating the information-based exploration into the LLM-based planning process. Combined with memory-augmented query evaluation, this integrated scheme not only allows for a better balance between the validity of the environmental context memory and the load of environment exploration, but also improves overall task performance. Furthermore, we devise a temporal consistency refinement scheme for query evaluation to address the inherent decay of knowledge in the memory. Through experiments with VirtualHome, ALFRED, and CARLA, our approach demonstrates robustness against a variety of embodied instruction following scenarios involving different instruction scales and types, and non-stationarity degrees, and it consistently outperforms other state-of-the-art LLM-based task planning approaches in terms of both goal success rate and execution efficiency.

## 1 Introduction

The application of Large Language Models (LLMs) in embodied AI is essential for harnessing common knowledge and immediately applying it to unseen tasks and domains without requiring additional training or data. Researchers are further enhancing task adaptation by integrating environmental information with the intrinsic common knowledge of LLMs . This capability proves invaluable in fields such as home robotics and autonomous driving, where it enables embodied agents to learn across diverse instruction following tasks with minimal data requirements.

For embodied agents, these tasks are often not mere single, one-time instructions but are multiple and persistent, necessitating continuous access to environmental knowledge to reason and plan effectively for user needs. In such scenarios, the efficiency of repeatedly collecting environmental knowledge through interaction each time the agent plans can be suboptimal. Furthermore, there is a clear need to integrate and manage multiple user requirements effectively.

In this paper, we investigate _continual instruction following_ for an embodied agent, where multiple tasks are contingent upon the real-time information of a continuously changing environment. This setup requires the agent to engage in ongoing exploration of the environment to adaptively respond to dynamic changes and fulfill the required tasks. To address the problem of continual instruction following, we present an exploratory retrieval-augmented planning (ExRAP) framework, designed to enhance LLMs' embodied reasoning capabilities by integrating environmental context memory.

In ExRAP, to improve effectiveness and efficiency in managing environment interaction and exploration loads for multiple embodied tasks, we employ an exploration-integrated task planning scheme, in which the information-based exploration is incorporated into the LLM-based planning process. This integrated planning scheme establishes a robust policy that balances the validity of the environmental context memory with the demands of environment interaction and exploration. We also devise a temporal consistency-based refinement scheme to ensure the robustness of memory-augmented query evaluation on environmental conditions. Through experiments with VirtualHome , ALFRED  and CARLA , we demonstrate that the ExRAP framework achieves competitive performance in both task success and efficiency compared to several state-of-the-art embodied planning methods, including ZSP , SayCan , ProgPrompt , and LLM-Planner .

Our contributions are summarized as: First, we propose a novel ExRAP framework, systematically combining LLMs' reasoning capabilities and environmental context memory into exploration-integrated task planning to tackle continuous instruction following tasks in non-stationary embodied environments. We also introduce two schemes tailored for exploration-integrated task planning in ExRAP, information-based exploration estimation and temporal consistency-based refinement on memory-augmented query evaluation. Finally, we demonstrate superior performance and robustness of ExRAP via intensive experiments with home robots and autonomous driving scenarios in VirtualHome, ALFRED, and CARLA.

## 2 Related work

**Embodied instruction following.** Embodied instruction following involves executing complex tasks based on an understanding of embodied knowledge. This aims to grasp various aspects of the physical environment including objects, their relations, and dynamics, and to plan appropriate sequences of actions or skills to complete the tasks specified by instructions successfully. In the area of task planning, there have been many works to combine LLMs' reasoning capabilities with environmental characteristics. Recent research explored the utilization of skills' affordances to compute their values [1; 2], implemented code-driven policies [3; 4], and generated reward functions [13; 14], while highlighting the use of LLMs' enhanced abilities in task planning. Moreover, LLM-driven environment modeling approaches, utilizing LLMs' common knowledge and reasoning about real-world objects, have been introduced [5; 6; 7]. These LLM-based approaches to task planning have been applied across a range of embodied instruction following tasks, facilitated by repeated interactions with environments, humans, or other agents [15; 16; 12; 17; 18; 19].

While these approaches underscore the versatility and depth of LLMs for task planning and embodied agent control, they often rely on a non-systematic integration of observations to the LLMs' reasoning process. Furthermore, they rarely consider continual instruction following scenarios, where an agent should handle a set of instructions continuously, adapting to real-time environmental conditions. In contrast, our work differentiates itself by incorporating agents' exploration capabilities, which are guided by information gains, into continual instruction following.

**Retrieval-augmented generation for LLM.** Research in retrieval-augmented generation (RAG) focused on efficiently executing tasks by sourcing and utilizing task-related information from databases. In particular, enhancing the performance of retrieval, which suggests relevant data when an LLM requires specific knowledge for the tasks, involves training retrievers [20; 21; 22; 23], fine-tuning the LLM to adapt the RAG process [24; 25], or exploiting the LLM itself for dynamic query reformation [26; 27]. In the area of embodied task planning, recent studies adopt the integration of RAG with task-specific demonstrations . Our work also uses RAG for embodied task planning, but it uniquely emphasizes its dynamic aspects. For continual instruction following, we prioritize the relevance and significance of the agent's skills, not only to perform tasks but also to ensure continuous and efficient synchronization of its environmental memory with changes in the environment.

**Exploration in reinforcement learning (RL).** In the field of RL, exploration methods, designated to efficiently gather environmental information, have been a focus of research. Strategies were developed, that prioritize the exploration of new environmental information, by offering intrinsic rewards [28; 29] or through navigation schemes derived from offline demonstrations [30; 31; 32]. Particularly, in DREAM , an exploration policy is formulated to adapt to varying conditions by using data gathered during initial exploration episodes. This policy is optimized by maximizing the information gain to better adapt to changes. Our work adapts this mutual information-based exploration strategy with RAG for embodied instruction following. Our proposed framework is the first to implement exploration-integrated task planning with LLMs, enabling the efficient execution of continuously-performed instructions and facilitating the dynamic adaptation to changing environmental conditions.

## 3 Approach

### Continual instruction following

We consider a set of instructions for embodied tasks that are continuously and simultaneously conducted based on specific environmental contexts. In Figure 1, the instructions entail conditional actions like "If the temperature is high, open the window" and "When watching TV, turn off the light." The agent continuously explores the environment to verify whether the conditions (e.g., temperature and TV status) are met. Upon confirmations, the agent executes the associated tasks within the environment. We refer to these scenarios, where _multiple_ embodied tasks are conditioned on environmental contexts and conducted _continuously_, as continual instruction following. This concept aligns closely with continuous queries [34; 35] in database literature, which monitor updates of interest over time and return results when specific thresholds or conditions are met. This is in contrast to single in-situ instruction following, where each task is executed based on isolated, one-time directives.

For continual instruction following tasks with conditional instructions \(=\{i_{1},...,i_{M}\}\), we consider a non-stationary embodied environment that changes over time. The conditions of continual instructions may or may not be satisfied over time, requiring continuous exploration in the environment. When certain conditions are met, the associated tasks should be performed promptly. For this continual instruction following tasks in the non-stationary environment, we evaluate agent performance in terms of task completion and efficiency. Our goal is to establish an embodied agent policy \(^{*}\) that maximizes the overall performance of continual instruction following tasks. Specifically, we formulate the reward as a combination of (i) the **task success rate SR** and (ii) the average **pending step PS**. SR is the rate of completed tasks whose conditions have been met, and PS is the average steps required to accomplish the task associated with instruction \(i_{C}\) whenever the condition is met. For instructions \(\) and timestep \(t\), we then formulate the agent policy \(^{*}\) performing a skill upon observation \(o_{t}\) as

\[^{*}=_{}_{t}(s_{t},(o_{t},))+_{i_{C}}[-(,i)].\] (1)

Figure 1: Concept of ExRAP. In the embodied environment, this framework manages continual instructions, a set of instructions for embodied instruction following tasks that are conducted continuously and simultaneously. At each step, it operates through (a) memory-augmented query evaluation, and (b) exploration-integrated task planning coupled with environmental context memory updates (as shown in the left side of the figure). By performing this integrated plan in conjunction with the memory, the ExRAP framework achieves more efficient task execution in response to the continual instructions, compared to the instruction-wise planning (as in the right side of the figure).

Optimizing both SR and PS allows the agent not only to appropriately plan for multiple instruction following tasks but also to strategically integrate these instructions to improve overall efficiency. The resulting policy is able to minimize redundant skill executions by addressing multiple tasks in an integrated manner, taking into account the possible spatial and temporal overlap of the task requirements. For instance, as illustrated in Figure 1, an integrated plan achieves a pending step of \(7\), the average of required timesteps \(7\), \(4\) and \(10\) for the three instructions. This is significantly shorter than 9.2, the average of \(5\), \(9\), and \(14\) achieved by an instruction-wise plan.

### Overall framework

To address the challenge of continual instruction following in a non-stationary embodied environment, we develop the ExRAP framework. It is designed to minimize the necessity for environmental interaction, by utilizing memory-augmented and exploration-integrated planning schemes while ensuring robust task performance.

In ExRAP, each conditional instruction \(i\) is decomposed into two primary components: query \(q\) and execution \(e\). Queries function as conditions for task initiation and are evaluated against environmental information. Executions, on the other hand, involve physical interactive manipulations that are triggered based on the results of query evaluation. In a non-stationary environment, evaluating queries poses a unique challenge due to the need for the agent to continuously synchronize with constantly changing information. This synchronization often necessitates continual exploration, resulting in intensive interaction with the environment.

As described in Figure 1, ExRAP addresses this challenge through two components: (a) query evaluation using environmental context memory and (b) exploration-integrated task planning. In (a), the environmental context memory is established via a temporal embodied knowledge graph to effectively represent the dynamic environment. Augmented with this graph-based context memory, the LLM-based query evaluator responds to queries by checking if their conditions are met and provides confidence levels for these assessments. To address the information decay, which stems from synchronization uncertainty between the previously collected environmental context and the actual current state of the environment, we incorporate entropy-based temporal consistency refinement into the query evaluation process. In (b), ExRAP plans skills that are instrumental not only for achieving tasks from an _exploitation_ perspective but also for boosting confidence in the query evaluations from an _exploration_ perspective. To effectively plan skills that balance both perspectives, we integrate the exploitation value of skills, which is derived from the in-context learning ability of LLMs, with their exploration value, which is determined through information-based estimation.

### Memory-augmented query evaluation with temporal consistency

We represent both the environmental context memory and the observations perceived by the agent using a temporal embodied knowledge graph (TEKG), where the memory is established through the accumulation of these observations. Queries, derived from given instructions, are evaluated against the context memory, with consideration for inherent information decay within the previously accumulated data. The query evaluation procedure is described on the upper side of Figure 2.

**TEKG and retriever.** The TEKG comprises a set of quadruples \(=(se,re,te,t)\) consisting of source entity \(se\), relation \(re\), target entity \(te\), and timesteps \(t\). We represent the environmental context memory at a specific timestep \(t\) within the TEKG, defined as

\[G_{t}=\{_{1},_{2},,_{N}\}_{i}=(se_{i},re_{i}, te_{i},t_{i}),\;t_{i} t.\] (2)

To integrate the current observation \(o_{t+1}\) into previously established up-to-date memory \(G_{t}\), we employ an update function \(\) as follows:

\[G_{t+1}=(G_{t},o_{t+1})=\{ G_{t}\,|\,c(,^{})=0,\; ^{} o_{t+1}\} o_{t+1}.\] (3)

Here, \(c\) is a function that detects the semantic contradictions between quadruples, such as when \(\) and \(^{}\) indicate that a TV is both "off" and "on". It returns 1 if there is a contradiction, otherwise 0.

Constructed memory \(G\) serves as a knowledge database for continual instruction following, enabling the retrieval of environmental information related to specific task directives such as instructions, queries, and executions. Specifically, for language-specified task directives \(=\{l\}\), the retriever interacts with the memory \(G\) and samples \(k\) quadruples \(\{_{1},,_{k}\}\). The sampling is based on the multinomial softmax distribution, where the likelihood of retrieving a quadruple \(\) is determined by the highest sentence embedding similarity between \(\) and any \(l\) in \(\).

**Instruction interpreter.** The instruction interpreter \(_{I}\) processes continual instructions \(=\{i_{1},...,i_{M}\}\), translating them into queries \(\) and corresponding task executions \(\):

\[_{I}()=(:(q_{1},,q_{M}),\,:(e_{1}, ,e_{M}),\,C)\,\,\,\,\,\,C(q_{j})=e_{j}\,\,\,\,\,\,  j.\] (4)

Here, \(C\) is a conditional function that maps each query to its respective execution counterpart.

**Query evaluator.** The memory-augmented query evaluator \(_{M}\) estimates the likelihood \(P(q|G_{t})\) of query \(q\) being satisfied, using the historical memory accumulated over time, denoted as \(G_{1:t}=G_{1}... G_{t}\). Leveraging the memory-augmented LLM (\(_{}\)), we develop the query evaluator \(_{M}\) by incorporating the previous step's query evaluation \(P(q|G_{t-1})\) and a prior of query evaluation \(R(q|G_{t-1})\), which is defined in (7).

\[P(q|G_{t})=_{M}(q,t,G_{1:t},P(q|G_{t-1}))=R(q|G_{t-1})& {if}\,\,_{1:t}=_{1:t-1}\\ _{}(q,t,_{1:t},R(q|G_{t-1}))&\] (5)

Here, \(_{1:t}_{R}(G_{1:t},\{q\})\) is retrieved quadruples, and the prior \(R(q|G_{t-1})\) is the retrospective query response evaluated at timestep \(t\) using \(G_{t-1}\).

Due to the inherent information decay in the memory over increasing timesteps, a decline in confidence should be considered for the likelihood estimation in (5). To address this, we incorporate an entropy-based **temporal consistency** as an intermediate step in query evaluation. Specifically, when using the memory from \(G_{t-1}\), we posit that the entropy of the prior query response at timestep \(t\) should be higher than at timestep \(t-1\):

\[H(R(q|G_{t-1}))>H(P(q|G_{t-1})).\] (6)

To enforce the temporal consistency, we compute the multiple query response priors using \(_{}\) and discard any responses that do not align with the consistency constraint. Specifically, if the entropy of each prior of query response is smaller than previous step \(P(q|G_{t})\), it is removed.

\[R(q|G_{t-1})=_{_{1:t-1}_{R}(G_{1:t-1},\{q\})}[ _{}(q,t,_{1:t-1},P(q|G_{t-1}))\,\,]\] (7)

Figure 2: Overall procedures of ExRAP. (a) Query evaluation: The instruction interpreter \(_{I}\) produces queries and executions, as well as a condition function from continual instructions. The memory-augmented query evaluator \(_{M}\) then evaluates these queries probabilistically using the LLM with a retrieved TEKG from the environmental context memory. (b) Exploration-integrated task planning: The LLM-based exploitation planner \(v_{T}\) estimates the value of skills based on their executions and relevant demonstrations in an in-context manner. Simultaneously, the exploration planner \(v_{R}\) evaluates these skills using the subsequent TEKG through information-based value estimation. At each step, a skill is then selected based on the integrated skill value from the two estimations.

Then, we select a set of corresponding executions \(_{t}\) that are likely to require manipulations in the environment, using a filtering threshold \(\).

\[_{t}=\{C(q)|q,P(q|G_{t})>\}\] (8)

### Exploration-integrated task planning with information-based estimation

To facilitate integrated task planning for continual instructions, we devise exploitation and exploration planners. The former focuses on exploiting appropriate skills to complete tasks associated with given instructions using the LLM, and the latter focuses on exploring the environment to update the memory in a direction that maximizes information gain. We then integrate their plans to prioritize the next skills to be executed. The resulting planning directs to complete specific task executions \(\), ensuring effective maintenance of the environmental context memory. This maintenance process involves synchronizing the memory with the current state of the environment. The exploration-integrated task planning procedure is described on the lower side of Figure 2.

**Exploitation planner.** Given the memory \(G_{t}\) and a language description of a skill \(z Z\), the exploitation planner \(v_{T}\) is responsible for estimating the value of the skill with respect to its effectiveness in accomplishing the executions \(_{t}\). To do so, we harness the retrieved memory-augmented LLMs along with their in-context learning capabilities. Specifically, under the assumption that we can access expert planning dataset \(_{e}\), we retrieve demonstrations \(D\) from \(_{e}\) based on the graph similarity between the current observation \(o_{t}\) and the observation within \(_{e}\).

\[v_{T}(G_{t},z)=_{}(_{t},_{R}(G_{t},_{ t}),D,z)\] (9)

**Exploration planner.** In conjunction with the exploitation planner, the exploration planner \(v_{R}\) is responsible for assessing the value of the skill with respect to its utility for reducing the response uncertainty of the query evaluator \(_{M}\). This assessment is intended for efficient environmental exploration, thereby facilitating the swift and precise identification of query conditions and maintaining the memory up-to-date. Specifically, we define the exploration value of skill \(z\) as the difference of mutual information for consecutive timesteps using the query evaluation result in (5):

\[v_{R}(G_{t},z)=I(;G_{t+1})-I(;G_{t})=_{q}H(P(q|G_{t}))-H(P(q|G_{t+1}))\] (10)

where \(I\) denotes mutual information and \(G_{t+1}\) is the updated memory by execution of skill \(z\).

Direct computation of \(G_{t+1}\) is impractical without actual skill execution. Therefore, to evaluate the exploration value of skills before their execution, we focus on the entropy related only to the retrieved knowledge pertinent to the evaluated queries \(\). This approach is feasible under the mild assumption that the entropy of the query evaluator reaches zero (indicating no uncertainty), once the TEKG memory is fully synchronized with the environment. Thus, we approximate the exploration value as

\[v_{R}(G_{t},z)=_{q Q}H(P(q|G_{t}))(1-(G_{t}^{z}, \{q\})}{d(_{R}(G_{t},\{q\})}))\] (11)

where \(d\) represents the average distance function between the retrieved quadruples and the current agent's entity in TEKG, and \(G_{t}^{z}\) is the predicted partially updated knowledge graph after skill execution, where only quadruples containing the agent as an entity are altered. Note that the exploration value increases as the agent moves closer to the query-related environment parts on the graph through the skill execution. Consequently, the skill is selected by maximizing the integrated skill value, which is obtained by a weighted sum of exploitation and exploration values, as defined in (9) and (11) respectively:

\[z_{t}=*{argmax}_{z Z}[w_{T} v_{T}(G_{t},z)+w_{R} v _{R}(G_{t},z)].\] (12)

## 4 Experiments

We evaluate ExRAP across various degrees of non-stationarity, scales of instructions, and instruction types. We also provide ablation studies and qualitative analysis. Further analysis is in Appendix D.

**Environments.** We evaluate ExRAP in the context of household planning and skill-based autonomous driving with VirtualHome , ALFRED , and CARLA , where we use 16 to 19 distinct instructions for continual instruction following tasks. Details are provided in Appendix A.

**Evaluation metric.** We employ two evaluation metrics for the objective specified in (1). The task success rate (**SR**) measures the proportion of completed tasks for continual instructions whose conditions are satisfied at each timestep. Given the continual instructions, the pending step (**PS**) represents the average number of timesteps required to complete the associated tasks from the moment the conditions of the instructions are actually satisfied in the environment. Note that the agent's detection time of such condition satisfaction may differ from its actual occurrence.

**Datasets.** We use 100 trajectories across 10 different environment settings in VirtualHome, and 50 trajectories in ALFRED and CARLA. These are used for in-context learning of the exploitation planner in ExRAP and the baselines. Note that we use different environment settings for evaluation.

**Baselines.** ZSP  is an LLM-based zero-shot task planner. Our experiments serve as a baseline to evaluate LLM-based task planning approaches. **SayCan** is a state-of-the-art embodied agent framework, which integrates both language affordance scores derived from an LLM and embodied affordance scores learned through RL. In our experiments, we use the optimal affordance function for each environment. **ProgPrompt** is a framework to enhance language models' capabilities in generating structured and logical outputs, by incorporating programming-like prompts. **LLM-Planner** is a state-of-the-art embodied agent framework that utilizes LLMs' embodied knowledge to infer subtask sequences. It incorporates object detection information from the agent's interactions for enhanced task planning. To address continual instructions and adapt to non-stationary environments, we implement a variant of the LLM-Planner that infers skills in a step-wise manner.

### Main results

**Non-stationarity.** Table 1 presents a performance comparison in terms of **SR** and **PS** in VirtualHome, ALFRED, and CARLA, respectively, under varying degrees of non-stationarity, where environment changes range from low to high. The higher degree of non-stationarity means the environment changes more rapidly, requiring the agent to focus more on environmental information to adapt effectively. ExRAP achieves superior performance across all degrees of non-stationary. Specifically, ExRAP demonstrates a performance gain in SR by \(16.45\%\) on average compared to the most competitive baseline, the LLM-Planner. Furthermore, ExRAP shows a reduction in PS by \(3.40\) on average compared to the LLM-Planner. Importantly, the advantage of ExRAP becomes more significant with

    &  &  &  \\   & **SR** (\(\)) & **PS** (\(\)) & **SR** (\(\)) & **PS** (\(\)) & **SR** (\(\)) & **PS** (\(\)) \\   \\  ZSP & \(20.59\%\)\(\)4.71\% & \(31.03\%\)\(\)4.68 & \(20.06\%\)\(\)1.93\% & \(32.06\%\)\(\)4.66 & \(17.28\%\)\(\)3.16\% & \(24.08\)\(\)4.63 \\ SayCan & \(35.12\%\)\(\)4.83\% & \(21.67\%\)\(\)3.81 & \(33.69\%\)\(\)5.36\% & \(21.81\)4.14 & \(27.33\%\)\(\)4.24\% & \(16.18\)\(\)3.98 \\ ProgPrompt & \(32.10\%\)\(\)4.41\% & \(18.84\%\)\(\)4.08 & \(30.51\%\)\(\)5.31\% & \(23.43\)1.07 & \(27.19\%\)\(\)2.99\% & \(18.60\)\(\)4.22 \\ LLM-Planner & \(40.97\%\)\(\)7.00\% & \(17.61\)1.40 & \(39.89\%\)\(\)4.52\% & \(15.93\)2.13 & \(34.60\%\)\(\)6.49\% & \(14.94\)\(\)2.89 \\ ExRAP & **61.12\%\(\)7.03\%** & **11.75\(\)2.49** & **55.14\%\(\)6.59\%** & **11.33\(\)1.92** & **50.12\%\(\)5.70\%** & **8.61\(\)2.25** \\   \\  ZSP & \(18.22\%\)\(\)5.33\% & \(17.24\)2.12 & \(14.67\%\)\(\)6.18\% & \(20.83\)\(\)3.63 & \(9.56\%\)\(\)4.80\% & \(22.53\)\(\)3.57 \\ SayCan & \(45.67\%\)\(\)6.89\% & \(8.25\)1.86 & \(41.81\%\)\(\)7.64\% & \(8.39\)\(\)3.53 & \(35.79\%\)\(\)6.31\% & \(7.42\)\(\)1.14 \\ ProgPrompt & \(47.15\%\)\(\)1.17\% & \(9.81\)2.14 & \(35.62\%\)\(\)1.04\% & \(7.22\)1.35 & \(19.97\%\)\(\)0.80\% & \(7.52\)\(\)2.45 \\ LLM-Planner & \(58.44\%\)\(\)3.97\% & \(7.28\)1.09 & \(51.80\%\)\(\)3.79\% & \(7.28\)1.05 & \(35.76\%\)\(\)6.00\% & \(6.65\)\(\)1.06 \\ ExRAP & **69.90\%\(\)1.47\%** & **5.94\(\)0.92** & **64.00\%\(\)5.07** & **4.82\(\)1.03** & **59.11\%\(\)2.48** & **4.42\(\)1.36** \\   \\  ZSP & \(10.44\%\)\(\)1.03\% & \(29.35\%\)\(\)7.21 & \(6.89\%\)\(\)2.98\% & \(32.46\)\(\)6.03 & \(4.67\%\)\(\)1.17\% & \(33.00\)\(\)1.40 \\ SayCan & \(37.55\%\)\(\)4.74\% & \(20.73\)5.36 & \(35.11\%\)\(\)6.12\% & \(22.44\)\(\)5.38 & \(30.71\%\)\(\)5.28\% & \(21.71\)\(\)2.01 \\ LLM-Planner & \(50.83\%\)\(\)1.60\% & \(14.02\)3.01 & \(44.00\%\)\(\)0.70\% & \(14.39\)\(\)1.94 & \(41.58\%\)\(\)3.35\% & \(13.59\)\(\)2.65 \\ ExRAP & **65.25\%\(\)7.47\%** & **12.43\(\)3.90** & **62.25\%\(\)6.72** & **11.50\(\)2.34** & **58.83\%\(\)10.08** & **10.84\(\)2.52** \\   

Table 1: Performance in VirtualHome, ALFRED, and CARLA w.r.t. non-stationarity. We use the 95% confidence interval, using 10 random seeds for VirtualHome and 5 random seeds for both ALFRED and CARLA.

increasing levels of non-stationarity; the performance gap in SR between the LLM-Planner and ExRAP widens from 15.35% at low non-stationarity to 18.58% at high non-stationarity. Similarly, the gap in PS grows from an average of 2.96 to 3.73. This increase in performance can be attributed to ExRAP's strong ability to promptly discern newly satisfied conditions through exploration-integrated task planning, coupled with accurate memory-augmented query evaluation. This capability enables ExRAP to respond to rapid environmental changes effectively.

**Instruction scale.** Table 2 shows a performance comparison under the medium non-stationarity, as the scale of continual instructions increases. As the number of instructions grows, the agent needs to collect more knowledge and perform more tasks. ExRAP achieves an average gain in SR of \(18.78\%\) compared to the most competitive baseline, the LLM-Planner. Additionally, ExRAP reduces PS by an average of \(6.31\). ExRAP exhibits widening performance gaps as the complexity of tasks increases; the SR gap grows from 18.49% compared to the LLM-Planner with a small continual instruction scale, to 22.04% with a large continual instruction scale. Similarly, the PS gap expands from an average of 4.84 to 8.87. This performance difference highlights ExRAP's effectiveness in addressing multiple continual instructions simultaneously, demonstrating its robust integrated task planning capabilities. In ExRAP, the skill selection is guided by an integrated value derived from queries and executions, enabling the efficient handling of multiple instructions and concurrent memory updates.

**Instruction type.** We also test the applicability of ExRAP for three types of continual instructions in VirtualHome with medium non-stationarity. **Sentence-wise** type organizes each instruction into individual sentences. This is the default setting in our experiments. **Summarized** type condenses multiple instructions into a fewer number of sentences. **Object Ambiguation** type contains abstract forms of target objects such as "something to read.". Table 3 demonstrates superior performance of ExRAP for those types. ExRAP adapts RAG with the environmental context memory to interpret instructions and decompose them into queries and executions. This memory-augmented approach, retrieving and utilizing the information relevant to given instructions, enables effective grounding of continual instructions in different types within the environment.

**Qualitative analysis.** Figure 3 (a) and (b) compare the exploration strategies of ExRAP and LLM-Planner. Given multiple instructions, ExRAP demonstrates broader exploration. This reflects the exploration-integrated task planning in ExRAP, which rather focuses on the overall gain achieved from each exploration and skill execution.

### Ablation study

We conduct several ablation studies for ExRAP in VirtualHome with medium non-stationarity.

**Temporal consistency.** We compare the performance of our ExRAP and its variant ExRAP-TC that performs query evaluation without temporal consistency-based refinement. As in Table 4, ExRAP outperforms ExRAP-TC by 15.56% on average. As observed in Figure 3 (a) and (c), with temporal consistency in query responses, information decay is effectively managed, leading to broader

    &  &  &  \\   & **SR (\(\))** & **PS (\(\))** & **SR (\(\))** & **PS (\(\))** & **SR (\(\))** & **PS (\(\))** \\  ZSP & \(33.11\% 5.36\%\) & \(22.86 2.41\) & \(20.06\% 1.93\%\) & \(32.06\% 4.66\%\) & \(7.43\% 6.28\%\) & \(59.16 26.04\) \\ SayCan & \(40.58\% 8.79\%\) & \(19.76 2.02\) & \(33.69\% 5.36\%\) & \(21.81 1.44\) & \(23.04\% 11.26\%\) & \(37.05 17.43\) \\ ProgPrompt & \(43.15\% 3.22\%\) & \(19.99 1.59\) & \(30.51\% 5.31\%\) & \(23.43 1.07\) & \(21.20\% 7.45\%\) & \(29.00 1.27\) \\ LLM-Planner & \(49.28\% 5.10\%\) & \(17.13 7.67\) & \(39.89\% 4.52\%\) & \(15.93 2.13\) & \(31.82\% 14.30\%\) & \(17.63 2.34\) \\ ExRAP & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Performance in VirtualHome w.r.t. instruction scale

    &  &  &  \\   & **SR (\(\))** & **PS (\(\))** & **SR (\(\))** & **PS (\(\))** & **SR (\(\))** & **PS (\(\))** \\  SayCan & \(33.69\% 5.36\%\) & \(21.81 4.14\) & \(32.667\% 4.29\%\) & \(22.13 5.67\) & \(23.89\% 10.97\%\) & \(28.03 5.54\) \\ LLM-Planner & \(39.89\% 4.52\%\) & \(15.93 2.13\) & \(37.19\% 5.34\%\) & \(16.45 3.87\) & \(30.88\% 5.20\%\) & \(19.09 4.92\) \\ ExRAP & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 3: Performance w.r.t. instruction typesexploration areas that accommodate different instructions simultaneously (i.e., in (a)). Otherwise, without temporal consistency, exploration tends to concentrate exclusively on specific knowledge where multiple queries overlap. This renders largely neglected areas experiencing significant decay, often leading to invalidated query evaluation (i.e., in (c)).

**Exploration strategy.** We compare the performance of our ExRAP and two variants ExRAP-LLM and ExRAP-EXP using different planning strategies. ExRAP-LLM directly employs the LLM as the exploration planner, which fully relies on the LLM's capability without information-based exploration, by prompting the LLM with specific exploration commands, e.g., "explore the home." On the other hand, ExRAP-EXP employs only the exploitation planner with specific exploration commands, used only when there are no executions. As shown in Table 5, ExRAP demonstrates higher performance of \(26.76\) and 17.46% on average than the two variants, respectively. As in Figure 3 (a) and (d), ExRAP-EXP exhibits reduced exploration capabilities, resulting in a narrower exploration area.

**LLMs for planning.** While we tested several LLMs, ranging from relatively smaller models such as Gemma-2B  to larger ones such as Llama-3-70B , our experiments thus far have utilized LLaMA-3-8B for ExRAP and the baselines. In Table 6, we evaluate ExRAP and two baselines using different LLMs. Both LLM-Planner and SayCan exhibit improved performance with the larger Llama-3-70B, but experience a significant drop in performance with the smaller Gemma-2B model. Unlike those, ExRAP maintains robust performance even with the smaller model, highlighting the benefits of its memory-augmented, integrated planning approach.

## 5 Conclusion

We introduced the ExRAP framework to facilitate efficient integrated planning for multiple instruction following tasks, which are conducted continuously and simultaneously in the embodied environment. With the extended RAG architecture, the framework incorporates memory-augmented query evaluation and exploration-integrated task planning schemes, thereby achieving both efficient environment exploration and robust task completion. Via experiments conducted in VirtualHome, ALFRED, and CARLA, we demonstrated the robustness of ExRAP across various continual instruction following scenarios, specifying its advantages over other LLM-driven task planning approaches.

    &  &  &  \\   & **SR** (\(\)) & **PS** (\(\)) & **SR** (\(\)) & **PS** (\(\)) & **SR** (\(\)) & **PS** (\(\)) \\  ExRAP-TC & \(47.25\%\)18.00\% & \(15.30\)6.97 & \(43.92\%\)7.97\% & \(15.47\)2.17 & \(27.91\%\)14.66\% & \(9.67\)5.32 \\ ExRAP & **61.13\%\)13.76**\% & **11.66\%\)3.93** & **55.14\%\%\(\)6.59\%** & **11.33\%\(\)1.92** & **49.73\%\(\)8.88\%** & **8.74\%\(\)2.74** \\   

Table 4: Ablation for query evaluation with temporal consistency

Figure 3: Knowledge exploration heatmap. Darker color represents high frequency in exploration.

**Limitation.** ExRAP leverages LLMs, which makes its performance dependent on the capabilities of these models to some extent. Compared to other baselines, it also requires increased computation effort due to the management of environmental context memory with temporal consistency. The ablation studies demonstrate that ExRAP is able to deliver robust performance even with a relatively lightweight LLM, yet further investigation into runtime overhead is desired.

## 6 Acknowledgement

This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT), (RS-2022-II220043 (2022-0-0043), Adaptive Personality for Intelligent Agents, RS-2022-II221045 (2022-0-01045), Self-directed multi-modal Intelligence for solving unknown, open domain problems, and RS-2019-II190421, Artificial Intelligence Graduate School Program (Sungkyunkwan University)), the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00213118), IITP-ITRC (Information Technology Research Center) grant funded by the Korea government (MIST) (IITP-2024-RS-2024-00437633, 10%), and by Samsung Electronics.