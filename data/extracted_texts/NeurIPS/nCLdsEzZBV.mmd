# The Equivalence of Dynamic and Strategic Stability

under Regularized Learning in Games

 Victor Boone &Panayotis Mertikopoulos

Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France

{victor.boone,panayotis.mertikopoulos}@univ-grenoble-alpes.fr

###### Abstract

In this paper, we examine the long-run behavior of regularized, no-regret learning in finite games. A well-known result in the field states that the empirical frequencies of no-regret play converge to the game's set of coarse correlated equilibria; however, our understanding of how the players' _actual_ strategies evolve over time is much more limited - and, in many cases, non-existent. This issue is exacerbated by a series of recent results showing that _only_ strict Nash equilibria are stable and attracting under regularized learning, thus making the relation between learning and pointwise solution concepts particularly elusive. In lieu of this, we take a more general approach and instead seek to characterize the _setwise_ rationality properties of the players' day-to-day play. To that end, we focus on one of the most stringent criteria of setwise strategic stability, namely that any unilateral deviation from the set in question incurs a cost to the deviator - a property known as _closedness under better replies_ (club). In so doing, we obtain a far-reaching equivalence between strategic and dynamic stability: _a product of pure strategies is closed under better replies if and only if its span is stable and attracting under regularized learning._ In addition, we estimate the rate of convergence to such sets, and we show that methods based on entropic regularization (like the exponential weights algorithm) converge at a geometric rate, while projection-based methods converge within a _finite_ number of iterations, even with bandit, payoff-based feedback.

## 1 Introduction

Background. The question of whether players can learn to emulate rational behavior through repeated interactions has been one of the mainstays of non-cooperative game theory, and it has recently gained increased momentum owing to a surge of breakthrough applications to machine learning and data science, from online ad auctions to multi-agent reinforcement learning. Informally, this question can be stated as follows:

_If each player follows an iterative procedure aiming to increase their individual payoff, does the players' long-run behavior converge to a rationally admissible state?_

A natural setting for studying this question is to assume that each player is following a no-regret algorithm, i.e., a policy which is asymptotically as good against a given sequence of payoff functions as the best fixed strategy in hindsight. In this framework, the link between learning and rationality is provided by a folk result which states that, under no-regret learning, the empirical frequency of play converges to the game's set of _coarse correlated equilibria_ (CCE) - also known as the game's _Hannan set_. This result has been of seminal importance to the field because no-regret play can be achieved via a wide class of "regularized learning" policies, as exemplified by the _"follow-the-regularized-leader"_ (FTRL) family of algorithms  and its variants - optimistic methods , Hedge/EXP3 , implicitly normalized forecasters , etc.

All these policies have (at least) one thing in common: they seek to provide the tightest possible guarantees for each player's individual regret, thus accelerating convergence to the game's Hannan set. As such, in games where the marginalization of coarse correlated equilibria coincides with the game's Nash equilibria (like two-player zero-sum games), we obtain a positive equilibrium convergence guarantee: the long-run empirical frequency of play evolves "as if" the players were rational to begin with - i.e., as if they had full knowledge of the game, common knowledge of rationality, the ability to communicate this knowledge, etc.

On the other hand, in many other contexts, the marginals of Hannan-consistent correlated strategies may fail even the weakest axioms of rational behavior and rationalizability (such as the elimination of strictly dominated strategies). In particular, a well-known example of Viossat & Zapechelnyuk  (which we discuss in detail in Section 4) shows that it is possible to have _negative regret_ for all time, but still employ _only strictly dominated strategies_ throughout the entire horizon of play.

The reason for this disconnect is that no-regret play has significant predictive power for the empirical frequency of play - that is, the long-run empirical distribution of pure strategy _profiles_ - but much less so for the players' day-to-day sequence of play - i.e., the evolution of the players' _actual_ mixed strategies over time. In particular, even when the marginalization of the Hannan set is Nash, the actual trajectory of play may - and, in fact, often _does_ - diverge away from the game's set of equilibria  or exhibits chaotic, unpredictable oscillations . Thus, especially in the context of regularized learning - where players learn _independently_ from one another, with no common correlating device - the blanket guarantee of no-regret play may quickly become irrelevant, and even misleading, providing the veneer of rational behavior but not the substance.

Motivated by the above, our paper seeks to understand the rationality properties of the players' _actual_ sequence of play under regularized learning, as encoded by the following question:

_Which sets of mixed strategies are stable and attracting under regularized learning?_

_Are these sets robust to strategic deviations? And, if so, is the converse also true?_

**Our contributions in the context of related work.** This question has attracted significant interest in the literature, especially in its pointwise version, namely: Which mixed strategy profiles are stable and attracting under regularized learning? Are the dynamics' stable states robust to unilateral deviations? And, if so, are these the only stable states of regularized learning?

In the related setting of population games, the answer to this question is sometimes referred to as the "folk theorem of evolutionary game theory" . Somewhat informally, this theorem states that, under the replicator dynamics (the continuous-time analogue of the exponential / multiplicative weights algorithm, itself an archetypal regularized learning method), the following is true for _all_ games: only Nash equilibria are (Lyapunov) stable, and a state is stable and attracting under the replicator dynamics if and only it is a strict Nash equilibrium of the underlying game .1

In the context of regularized learning,  showed that a similar equivalence holds for the dynamics of FTRL in _continuous_ time: a state is stable and attracting under the FTRL dynamics if and only if it is a strict Nash equilibrium. Subsequently, Giannou et al.  extended this equivalence to an entire class of regularized learning schemes, with different types of feedback and/or update structures - from optimistic methods to algorithms run with bandit, payoff-based information. In all these cases, the same principle emerges: under regularized learning, _a state is asymptotically stable and attracting if and only if it is a strict Nash equilibrium._

This is an important pointwise prediction but it does not cover cases where regularized learning algorithms do not converge to a point, but to a _set_ (such as a limit cycle or other non-trivial attractor). In this case, the very definition of strategic stability is an intricate affair, and there are several definitions that come into play . The first such notion that we consider is that of "resilience to strategic deviations", namely that every unilateral deviation from the set under study is deterred by some other element thereof. Our first contribution in this direction is a universal guarantee to the effect that, with probability 1, in any game, and from any initial condition, _the long-run limit of any regularized learning algorithm is a resilient set._

This result is significant in its universality, but the notion of resilience is not sufficiently strong to disallow irrational behavior - and, in fact, it is subject to similar shortcomings as Hannan consistency.

To account for this deficiency, we turn to a much more stringent criterion of setwise strategic stability, that of _closedness under better replies_ (club). This notion, originally due to Ritzberger & Weibull , states that any deviation from a product of pure strategies is costly, and it is one of the strictest setwise refinements in game theory. In particular, it refines the notion of closedness under rational behavior (curb) , and it satisfies all the seminal strategic stability requirements of Kohlberg & Mertens , including robustness to strategic payoff perturbations.2

In this general context, we show that regularized learning enjoys a striking relation with club sets: _A product of pure strategies is closed under better replies if and only if its span is stable and attracting under regularized learning_. In fact, we show that this equivalence can be refined to sets that are _minimally_ closed under better replies (in the sense that they do not contain a strictly smaller closed under better replies (club) set): a product of pure strategies is minimally club (m-club) if and only if its span is irreducibly stable and attracting (in that it does not contain a smaller asymptotically stable span of strategies). Finally, we also estimate the rate of convergence to club sets, and we establish convergence at a geometric rate for entropically regularized methods - like Hedge and EXP3 - and in a _finite number_ of iterations under projection-based methods.

In light of the above, our results can be seen both as a far-reaching setwise generalization of the folk theorem of evolutionary game theory, as well as a bona fide algorithmic analogue of a precursor result for the replicator dynamics, originally due to Ritzberger & Weibull . Importantly, our analysis covers several different update structures - "vanilla" regularized methods, but also their optimistic variants - as well as a wide range of information models - from full payoff information to bandit, payoff-based feedback.

## 2 Preliminaries

We start by recalling some basics from game theory, roughly following the classical treatise of Fudenberg & Tirole . First, a _finite game in normal form_ consists of (\(i\)) a finite set of _players_\(i\{1,,N\}\); (\(ii\)) a finite set of _actions_ - or _pure strategies_ - \(_{i}\) per player \(i\); and (\(iii\)) an ensemble of _payoff functions_\(u_{i}\ _{j}_{j}\), each determining the reward \(u_{i}()\) of player \(i\) in a given _action profile_\(=(_{1},,_{N})\). Collectively, we will write \(=_{j}_{j}\) for the game's _action space_ and \((,,u)\) for the game with primitives as above.

During play, each player \(i\) may randomize their choice of action by playing a _mixed strategy_, i.e., a probability distribution \(x_{i}_{i}(_{i})\) over \(_{i}\) that selects \(_{i}_{i}\) with probability \(x_{i_{i}}\). To lighten notation, we identify \(_{i}_{i}\) with the mixed strategy that assigns all weight to \(_{i}\) (thus justifying the terminology "pure strategies"). Then, writing \(x=(x_{i})_{i}\) for the players' _strategy profile_ and \(=_{i}_{i}\) for the game's _strategy space_, the players' payoff functions may be extended to all of \(\) by setting

\[u_{i}(x)_{ x}[u_{i}()]=_{ }u_{i}()\,x_{}\] (1)

where, in a slight abuse of notation, we write \(x_{}\) for the joint probability of playing \(\) under \(x\), i.e., \(x_{}=_{i}x_{i_{i}}\). This randomized framework will be referred to as the _mixed extension_ of \(\) and we will denote it by \(()\).

For concision, we will also write \((x_{i};x_{-i})=(x_{1},,x_{i},,x_{N})\) for the strategy profile where player \(i\) plays \(x_{i}_{i}\) against the strategy profile \(x_{-i}_{j i}_{j}\) of all other players (and likewise for pure strategies). In this notation, we also define each player's _mixed payoff vector_ as

\[v_{i}(x)=(u_{i}(_{i};x_{-i}))_{_{i}_{i}}\] (2)

so the payoff to player \(i\) under \(x\) becomes

\[u_{i}(x)=_{_{i}}u_{i}(_{i};x_{-i})\,x _{i\,_{i}}= v_{i}(x),x_{i}.\] (3)

Moving forward, the _best-response correspondence_ of player \(i\) is defined as the set-valued mapping \(_{i}_{i}\) given by

\[_{i}(x)=_{x_{i}^{}_{i}}u_{i}(x _{i}^{};x_{-i})x.\] (4)

Extending this over all players, we will write \(=_{i}_{i}\) for the product correspondence \((x)=_{1}(x)_{N}(x)\), and we will say that \(x^{*}\) is a _Nash equilibrium_ (NE) if \(x^{*}(x^{*})\)Equivalently, given that \(u_{i}(x^{}_{i};x_{-i})\) is linear in \(x^{}_{i}\), we conclude that \(x^{*}\) is a Nash equilibrium if and only if

\[u_{i}(x^{*}) u_{i}(_{i};x^{*}_{-i}) _{i}$ and all $i$.}\] (NE)

As a final point of note, if \(x^{*}\) is a Nash equilibrium where each player has a unique best response - that is, \(_{i}(x^{*})=\{x^{*}_{i}\}\) for all \(i\) - we will say that \(x^{*}\) is _strict_ because, in this case, \(u_{i}(x^{*})>u_{i}(x_{i};x^{*}_{-i})\) for all \(x_{i} x^{*}_{i}\), \(i\). An immediate consequence of this is that strict equilibria are _pure_, i.e., each \(x^{*}_{i}\) is a pure strategy. Among Nash equilibria, strict equilibria are the only ones that are "structurally robust" (in the sense that they remain invariant to small perturbations of the underlying game), so they play a particularly important role in game theory.

## 3 Regularized learning in games

Throughout our paper, we will consider iterative decision processes that unfold as follows:

1. At each stage \(t=1,2,\), every participating agent selects an action.
2. Agents receive a reward determined by their chosen actions and their individual payoff functions.
3. Based on this reward (or other feedback), the agents update their strategies and the process repeats.

In this online setting, a crucial requirement is the minimization of the players' _regret_, i.e., the difference between a player's cumulative payoff over time and the player's best possible strategy in hindsight. Formally, if the players' actions at each epoch \(t=1,2,\) are collectively drawn by the probability distribution \(z_{t}()\), the _regret_ of each player \(i\) is defined as

\[_{i}(T)=_{_{i}_{i}}_{t=1}^{T}[u_{i}( _{i};z_{-i,t})-u_{i}(z_{t})],\] (5)

and we will say that player \(i\) has _no regret_ if \(_{i}(T)=o(T)\).

One of the most widely used policies to achieve no-regret play is the so-called _"follow-the-regularized-leader"_ (FTRL) family of algorithms and its variants . To motivate the analysis to come, we begin with an archetypal FTRL method, the _exponential/multiplicative weights_ algorithm, also known as Hedge.

### A gentle start

We begin our discussion with a "stimulus-response" approach in the spirit of Erev & Roth : First, at each stage \(t=1,2,\), every player \(i\) employs a mixed strategy \(x_{i,t}_{i}\) to select an action \(_{i,t}_{i}\). Subsequently, to measure the performance of their pure strategies over time, each player further maintains a score variable which is updated recursively as

\[y_{i_{i},t+1}=y_{i_{i},t}+u_{i}(_{i};_{-i,t}) _{i}$.}\] (6)

In words, \(y_{i_{i},t}\) simply tracks the cumulative payoff of the pure strategy \(_{i}_{i}\) up to time \(t\) (inclusive).3 As such, this score can be treated as a _propensity_ to play a given pure strategy at any given stage: the strategies \(_{i}_{i}\) with the highest propensity scores \(y_{i_{i},t+1}\) should be played with higher probability at stage \(t+1\).

The most widely used instantiation of this stimulus-response mechanism is the _logit choice_ rule

\[_{i}(y_{i})}))_{_{i}_{i}}}{_{_{i}_{i}}(y_{i_{i}})}\] (7)

which means that each player selects an action with probability that is exponentially proportional to its score. In this way, we obtain the _exponential/multiplicative weights_ - or Hedge - algorithm

\[y_{i,t+1}=y_{i,t}+_{t}v_{i}(_{t}) x_{i,t+1}=_{i}(y_{i,t+1})_{i,t+1} x_{i,t+1}\] (Hedge)

where \(_{t}\) is the algorithm's "learning rate". For an appeitzer to the literature on (Hedge), see  and references therein.

The rest of the methods we discuss below will vary some - or even all - of the components of (Hedge): the information used to update the players' propensity scores, the way that propensity scores are mapped to mixed strategies, and/or even the way that pure actions are selected. However, all of the methods under study will be characterized by the same "stimulus-response" reinforcement mechanism: actions that seem to be performing better over time are employed with higher probability, up to some "regularization" that incentivizes exploration of underperforming actions.

### The regularized learning template

In the rest of our paper, we will work with an abstract _regularized learning_ (RL) template which builds on the same stimulus-response principle as (Hedge), while allowing us to simultaneously consider different types of feedback, strategy sampling policies, update structures, etc. To lighten notation below, we will drop the player index \(i\) when the meaning can be inferred from the context; also, to stress the distinction between "strategy-like" and "payoff-like" variables, we will write throughout \(_{i}^{A_{i}}\) and \(_{i}_{i}\) for the game's "_payoff space_", in direct analogy to \(_{i}\) and \(=_{i}_{i}\) for the game's _strategy space_.

With all this in hand, consider the following general class of regularized learning methods:

\[&Y_{i,t+1}=Y_{i,t}+_{t}_{i,t}\\ &X_{i,t+1}=Q_{i}(Y_{i,t+1})\] (RL)

In tune with (Hedge), the various elements of (RL) are defined as follows:

1. \(X_{i,t}_{i}\) denotes the mixed strategy of player \(i\) at time \(t=1,2,\)
2. \(Y_{i,t}_{i}\) is a "score vector" that measures the performance of the player's actions over time.
3. \(Q_{i}_{i}_{i}\) is a "regularized choice map" that maps score vectors to choice probabilities.
4. \(_{i,t}\) is a surrogate / approximation of the mixed payoff vector \(v_{i}(X_{t})\) of player \(i\) at time \(t\).
5. \(_{t}>0\) is a step-size / sensitivity parameter of the form \(_{t} 1/t^{_{t}}\) for some \(_{}\).

In words, at each stage of the process, every player \(i\) observes - or otherwise estimates - a proxy \(_{i,t}\) of their individual payoff vector; subsequently, players augment their actions' scores based on this information, they select a mixed strategy via the regularized choice map \(Q_{i}\), and the process repeats. To streamline our presentation, we discuss in detail the precise definition of \(\) and \(Q\) in Sections 3.3 and 3.4 below, and we present a series of examples of (RL) in Section 3.5 right after.

### Aggregating payoff information

As noted above, the main idea of regularized learning is to track the players' payoff vector \(v(X_{t})\). Importantly, there are several different modeling choices that can be made here: players may have direct access to their payoff vectors (in the full information setting), or some noisy approximation obtained by an inner randomization of the algorithm (e.g., when they receive information on their pure actions); they may have to recreate their payoff vectors altogether (as in the bandit setting), or their estimates may be based on a strategy other than the one they actually played (as in the case of optimistic algorithms).

In all cases, we will represent the surrogate payoff vector \(_{t}\) as

\[_{t}=v(X_{t})+U_{t}+b_{t}\] (8)

where \(b_{t}=[_{t}_{t}]-v(X_{t})\) and \(U_{t}=_{t}-[_{t}_{t}]\) respectively denote the offset and the random error of \(_{t}\) relative to \(v(X_{t})\). To streamline our presentation, we will also assume that \(\|b_{t}\|=(1/t^{_{}})\) and \(\|U_{t}\|=(t^{_{}})\) for some \(_{b},_{} 0\); we discuss the specifics of these bounds later in the paper.

### From scores to strategies

Regarding the "scores-to-strategies" step of (RL), we will follow the classical approach of Shalev-Shwartz  and assume that each player is employing a _regularized choice map_ of the general form

\[Q_{i}(y_{i})=_{x_{i}_{i}}\{ y_{i},x_{i}-h_ {i}(x_{i})\}_{i}$}.\] (9)

In the above, the _regularizer_\(h_{i}_{i}\) acts as a penalty that smooths out the "hard" argmax correspondence \(y_{i}_{x_{i}_{i}}\{y_{i},x_{i}\}\). Accordingly, instead of following the "leader" (i.e., playing the strategy with the highest propensity score), players follow the "regularized leader" - that is, they allow for a certain degree of uncertainty in their choice of strategy .

To ease notation, we will work with kernelized regularizers of the form \(h_{i}(x_{i})=_{_{i}_{i}}(x_{i_{i}})\) for some continuous function \(\) with \(_{z(0,1]}^{}(z)>0\). We will also say that the players' regularizers are _steep_ if \(_{z 0^{+}}^{}(z)=-\), and _non-steep_ otherwise.

**Example 3.1**.: A standard family of kernelized regularizers is given by \((z)=z^{}/(-1)\) for \((0,1)(1,2]\) and \((z)=z z\) for \(=1\). This family includes:

* For \(=2\), the quadratic regularizer \((z)=z^{2}/2\), which yields the Euclidean projection map \[Q_{i}(y_{i})=_{_{i}}(y_{i})_{x_{i}_{i }}\|y_{i}-x_{i}\|_{2}.\] (10)* For \(=1\), the _entropic regularizer_\((z)=z z\), which yields the logit choice map (7).
* For \(=1/2\), the _fractional power regularizer_\((z)=-4\) that underlies the Tsallis-INF algorithm of  (see also Section 3.5 below). \(\)

### Specific algorithms

We now proceed to discuss some archetypal examples of (RL).

**Algorithm 1** (Follow the regularized leader).: The standard _"follow-the-regularized-leader"_ (FTRL) method of Shalev-Shwartz & Singer  is obtained when players observe their full payoff vectors, that is, \(_{i,t}=v_{i}(X_{t})\). In this case, (RL) boils down to the deterministic update rule

\[Y_{i,t+1}=Y_{i,t}+_{t}v_{i}(X_{t}) X_{i,t+1}=Q_{i}(Y_{i,t+1})\]

or, more explicitly

\[X_{i,t+1}=_{x_{i}_{i}}\{_{s=1}^{t}_{s}u_ {i}(x_{i};X_{-i,s})-h_{i}(x_{i})\}\] (FTRL)

For a detailed discussion of (FTRL), see . We only note here that, as a special case, when (FTRL) is run with the logit choice setup of Eq.7, a standard calculation yields the _exponential/multiplicative weights_ (Hedge) . \(\)

**Algorithm 2** (Optimistic FTRL).: A notable variant of FTRL - originally due to Popov  and subsequently popularized by Rakhlin & Sridharan  - is the so-called _optimistic FTRL_ method. This scheme employs an "optimistic" correction intended to anticipate future steps, and it updates as

\[Y_{i,t+1}=Y_{i,t}+_{t}[2v_{i}(X_{t})-v_{i}(X_{t-1})]\] (Opt-FTRL)

with \(X_{i,t}=Q_{i}(Y_{i,t})\). As a special case, if (Opt-FTRL) is run with the logit choice map (7), we obtain the familiar update rule known as _optimistic multiplicative weights_ (OMW) .

Compared to (FTRL), the gain vector \(_{t}=2v(X_{t})-v(X_{t-1})\) of (Opt-FTRL) has offset \(b_{t}=v(X_{t})-v(X_{t-1})\) relative to \(v(X_{t})\). Thus, even though (Opt-FTRL) assumes full access to the players' mixed payoff vectors, it uses this information differently than (FTRL): in particular, the offset of (Opt-FTRL) is non-zero _by design_, not because of some systematic error in the payoff measurement process. \(\)

Now, up to this point, we have not detailed how players might observe their full, mixed payoff vectors. This assumption simplifies the analysis immensely, but it is not realistic in applications to e.g., online advertising and network science, where players may only be able to observe their realized payoffs, and have no information about the strategies of other players or actions they did not play. On that account, we describe below a range of _payoff-based_ policies where players estimate their counterfactual, "what-if" payoffs _indirectly_.

The most common way to achieve this is via the _importance-weighted estimator_

\[_{i\,_{i}}(x)=\{_{i}=_{i}\} }{x_{i\,_{i}}}u_{i}() _{i}$, $i$},\] (IWE)

where \(x\) is the players' strategy profile, and \(\) is drawn according to \(x\). This estimator is at the heart of the online learning literature  and it leads to the following methods:

**Algorithm 3** (Bandit FTRL).: Plugging (IWE) directly into (RL) yields the _bandit FTRL_ policy

\[Y_{i,t+1}=Y_{i,t}+_{t}\,_{i}(_{t}) X_{i,t+1}=Q_{ i}(Y_{i,t+1})\] (B-FTRL)

where (IWE) is sampled at the mixed strategy profile

\[_{i,t}=(1-_{t})X_{i,t}+_{t}\,_{_{i}}\] (11)

for some "explicit exploration" parameter \(_{t} 1/t^{\,_{}}\), \(_{}>0\), which specifies the mix between \(X_{i,t}\) and the uniform distribution \(_{_{i}}\) on \(_{i}\). As we discuss in the sequel, this combination of (IWE) with the explicit exploration mechanism (11) means that the surrogate payoff vector \(_{t}=(_{t})\) used to update (B-FTRL) has offset and noise bounded respectively as \(b_{t}=(_{t})\) and \(U_{t}=(1/_{t})\).

Two special cases of (B-FTRL) that have attracted significant attention in the literature are:

1. The _exponential weights algorithm for exploration and exploitation_ (EXP3) , obtained by running (B-FTRL) with the logit choice map (7).

2. The _Tsallis implicitly normalized forecaster_ (Tsallis-INF)  that was proposed as a more efficient alternative to EXP3, and which updates as \[X_{i,t}=_{x_{i}_{i}}\{ Y_{i,t},x_{i} +4_{_{i}_{i}}_{i}}\}\] (Tsallis-INF) i.e., as (B-FTRL) with the fractional power regularizer \((z)=-4\) of Example 3.1. \(\)

For illustration purposes, we provide some more examples of (RL) in Appendix B.

## 4 First results: resilience to strategic deviations

We are now in a position to begin our analysis of the rationality properties of the players' long-run behavior under (RL). To that end, we should first note that no-regret play may _still_ lead to counterintuitive and highly non-rationalizable outcomes, e.g., with all players selecting dominated strategies for all time. The example below is adapted from Viossat & Zapechelnyuk .

**Example 4.1**.: Consider the \(4 4\) symmetric 2-player game with payoff bimatrix

  & \(A\) & \(B\) & \(C\) & \(D\) \\  \(A\) & \((1,1)\) & \((1,2/3)\) & \((0,0)\) & \((0,-1/3)\) \\ \(B\) & \((2/3,1)\) & \((2/3,2/3)\) & \((-1/3,0)\) & \((-1/3,-1/3)\) \\ \(C\) & \((0,0)\) & \((0,-1/3)\) & \((1,1)\) & \((1,2/3)\) \\ \(D\) & \((-1/3,0)\) & \((-1/3,-1/3)\) & \((2/3,1)\) & \((2/3,2/3)\) \\  In this game, \(B\) and \(D\) are strictly dominated for both players by their stronger "twins" (\(A\) and \(C\) respectively). However, it is easy to check that if both players choose between \((B,B)\) and \((D,D)\) with probability \(1/2\) each, the resulting distribution of play \(z()\) satisfies \(u_{i}(_{i};z_{-i})-u_{i}(z)-1/6\) for all \(_{i}\{A,B,C,D\}\), \(i=1,2\). As a result, the players' regret under \(z_{t} z\) is _negative_, even though both players play strictly dominated strategies at all times. \(\)

The example above shows unequivocally that

_No-regret play does not suffice to exclude non-rationalizable outcomes._

In addition, Example 4.1 also shows that predictions based on correlated play are not always appropriate for describing the players' behavior under (RL): the end-state of any regularized learning algorithm will be a closed connected set of mixed strategies, so it is not possible to play _only_\((B,B)\) or \((D,D)\) in the long run. We are thus led to the following natural questions: _What are the rationality properties of long-run play under (RL)? Is the players' behavior robust to strategic deviations?_

To study these questions formally, we will focus on the _limit set_\((X)\) of \(X_{t}\) under (RL), viz.

\[(X)_{t}\{X_{s}:s t\} \{:X_{t_{k}}X_{t_{k}}X_{t}\}.\] (12)

In words, \((X)\) is the set of limit points of \(X_{t}\) or, equivalently, the _smallest_ subset of \(\) to which \(X_{t}\) converges. Clearly, the simplest instance of a limit set is when \((X)\) is a singleton, i.e., when \(X_{t}\) converges to a point. This case has attracted significant interest in the literature: for example, if \((X)=\{x^{*}\}\) then, for certain special cases of (RL), it is known that \(x^{*}\) is a Nash equilibrium of \(\). However, beyond this relatively simple regime, the structure of the limit sets of (RL) could be arbitrarily complicated and their rationality properties are not well-understood.

With this in mind, as a first attempt to study whether the long-run behavior of (RL) is "robust to strategic deviations", we will consider the following notion of _resilience to strategic deviations_:

**Definition 1**.: A closed subset \(\) of \(\) is _resilient to strategic deviations_ - or simply _resilient_ - if, for every deviation \(x_{i}_{i}\) of every player \(i\), we have

\[u_{i}(x^{*}) u_{i}(x_{i},x_{-i}^{*})x^{*} .\] (13)

Informally, \(\) is resilient if every unilateral deviation from \(\) is deterred by some (possibly different) element thereof. In particular, if \(\) is a singleton, we immediately recover the definition of a Nash equilibrium; beyond this case however, other examples include the set of undominated strategies of a game, the support face of the equilibria of two-player zero-sum games, etc. Importantly, as we show below, the limit sets of (RL) are resilient _in all games:_

**Theorem 1**.: _Let \(X_{t}\), \(t=1,2,\), be the sequence of play generated by (RL) with step-size/gain parameters \(_{}>2_{}\) and \(_{b}>0\). Then, with probability \(1\), the limit set \((X)\) of \(X_{t}\) is resilient._

Proof sketch.: The proof of Theorem 1 boils down to two interleaved arguments that we detail in Appendix C. The first hinges on showing that, if \(((X)=S)>0\) for some _non-random_\(\), \(\) must be resilient. This is argued by contradiction: if \(p_{i}_{t}\) is a unilateral deviation violating Definition 1, we must also have \(_{t}[u_{i}(p_{i};X_{-i,t})-u_{i}(X_{t})]>0\) with positive probability. However, the existence of a strategy that consistently outperforms \(X_{t}\) runs contrary to the fact that strategies that (RL) selects against underperforming strategies. We make this intuition precise via an energy argument that leverages a series of results from martingale limit theory (which is where the requirements for \(_{t}\), \(b_{t}\) and \(U_{t}\) come in). Then, to get the stronger statement that the _random_ set \((X)\) is resilient w.p.1, we show that the above remains true if \(p_{i}\) is replaced by a deviation \(q_{i}\) which is close enough to \(p_{i}\) and has _rational_ entries. Since there is a countable number of such profiles, we can use a union bound on an enumeration of the rationals to isolate a deviation witnessing the negation of Definition 1; our claim then follows by applying our argument for non-random sets. 

Theorem 1 is our first universal guarantee for (RL), so some remarks are in order. First, we should point out that the requirements \(_{b}>0\) and \(2_{}<_{}\) are a priori _implicit_ because they depend on the offset and magnitude statistics of the feedback sequence \(_{t}\). However, in most learning algorithms, these quantities are under the _explicit_ control of the players: for example, as we show in Appendix B, Algorithm 2 has \(_{b}=_{}\) while, for Algorithm 3, we have \(_{b}=_{}=_{}\). In this way, when instantiated to Algorithms 1-3 (and special cases thereof), Theorem 1 yields the following corollary:

**Corollary 1**.: _Suppose that Algorithms 1-3 are run with \(_{}(0,1]\) and, for Algorithm 3, \(_{}(0,_{}/2)\). Then, with probability \(1\), the limit set \((X)\) of \(X_{t}\) is resilient._

Now, since Theorem 1 applies to all games, it would seem to provide a universally positive answer to whether (RL) is robust to strategic deviations. However, this is not so: a direct calculation shows that the face of \(\) that is spanned by the dominated strategies \((B,B)\) and \((D,D)\) of Example 4.1_is_ resilient, so Theorem 1 cannot exclude convergence to a set where dominated strategies survive. Thus, just like no-regret play, the notion of resilience does not suffice by itself to capture the idea of rational behavior. This is because, albeit natural, resilience is too lax to provide a meaningful link between robustness to unilateral deviations - a _game-theoretic_ requirement - and stability under regularized learning - a _dynamic_ requirement. We address this question in detail in the next section.

## 5 A characterization of strategic stability under regularized learning

Similar to the set of pure strategies that arise from no-regret play, the main limitation of resilience is that a payoff-improving deviation may be countered by an action profile where the deviator also switched to a _different_ strategy; in other words, resilience is not a _self-enforcing_ barrier to deviations. In view of this, we will focus below on a much more stringent criterion of strategic stability, namely that _any_ deviation from the set in question incurs a cost to the deviating agent.

Club sets.To make all this precise, define the _better-reply correspondence_ of player \(i\) as

\[_{i}(x)=\{x_{i}^{}_{i}:u_{i}(x_{i}^{};x _{-i}) u_{i}(x)\}\] (14)

and write \(=_{i}_{i}\) for the product correspondence \((x)=_{1}(x)_{N}(x)\). [In words, \(_{i}\) assigns to each \(x\) those strategies of player \(i\) that are (weakly) better against \(x\) than \(x_{i}\).] In addition, given a product of pure strategies \(=_{i}_{i}\) with \(_{i}_{i}\) for all \(i\), let \(=()\) denote the span of \(\), and let \(()\) denote the collection of all such sets. We then say that \(()\) is _closed under better replies_ - a _club set_ for short - if it is closed under \(\), i.e., \(()\); finally, \(\) is said to be _minimally club_ (m-club) if it does not admit a proper club subset.

Of course, the entire strategy space \(\) is closed under better replies so, a priori, club sets could also contain dominated strategies and / or other non-rationalizable outcomes. By contrast, _minimal_ club sets are much more rigid in their relation to rational behavior because any unilateral deviation from an m-club set is _costly_, and m-club sets are _minimal_ in this regard. On that account, m-club sets can be seen as _the closest setwise analogue to strict Nash equilibria._

This analogy is accentuated further by the following properties of m-club sets, all due to Ritzberger & Weibull , who introduced the concept:1. Every game admits an m-club set; and if this set is a singleton, then it is a _strict_ Nash equilibrium.
2. Any m-club set \(\) is _fixed_ under better replies, that is, \(()=\) (implying in turn that \(\) cannot contain any dominated strategies, including iteratively dominated ones).
3. Any m-club set \(\) contains an _essential equilibrium component_, i.e., a component of Nash equilibria such that every small perturbation of the game admits a nearby equilibrium; in addition, this component has _full support_ on \(\), i.e., it employs all pure strategy profiles that lie in \(\).4 
Going back to our online learning setting, the above leads to the following natural set of questions:

_Are club sets (minimal or not) stable under the dynamics of regularized learning?_

_Are they attracting? And, if so, are they the only such sets?_

Any answer to these questions - positive or negative - would be an important step in delineating the relation between _strategic stability_ (in the above sense) and _dynamic stability_ under (RL). To that end, we start by formalizing some notions of dynamic stability that will be central in the sequel:

**Definition 2**.: Fix some subset \(\) of \(\) and a tolerance level \(>0\). We then say that \(\) is:

1. _Stochastically stable_ if, for every neighborhood \(\) of \(\) in \(\), there exists a neighborhood \(_{1}\) of \(\) such that \[(X_{t}t=1,2,) 1- X_{1}_{1}.\] (15)
2. _Stochastically attracting_ if there exists a neighborhood \(_{1}\) of \(\) such that \[(_{t}(X_{t},)=0) 1- X_{1}_{1}.\] (16)
3. _Stochastically asymptotically stable_ if it is stochastically stable and attracting.
4. _Irreducibly stable_ if \(\) is stochastically asymptotically stable and it does not admit a strictly smaller stochastically asymptotically subset \(^{}\) with \((^{})( )\).

With all this in hand, our main result below provides a sharp characterization of strategic stability in the context of regularized learning:

**Theorem 2**.: _Fix some set \(()\) and suppose that (RL) is run with a steep regularizer and step-size/gain parameters \(_{}\), \(_{b}>0\), and \(_{}<1/2\). Then:_

1. \(\) _is stochastically asymptotically stable under (_RL_) if and only if it is a club set._
2. \(\) _is irreducibly stable under (_RL_) if and only if it is an m-club set._

In addition, we also get the following convergence rate estimates for club sets:

**Theorem 3**.: _Let \(()\) be a club set, and let \(X_{t}\), \(t=1,2,\), be the sequence of play generated by (RL) with parameters \(_{}\), \(_{b}>0\), and \(_{}<1/2\). Then, for all \(>0\), there exists an (open, unbounded) initialization domain \(\) such that, with probability at least \(1-\), we have_

\[(X_{t},) Cc_{1}-c_{2}_{s= 1}^{t}_{s}Y_{1}\] (17)

_where \(C,c_{1},c_{2}\) are constants (\(C,c_{2}>0\)), and the rate function \(\) is given by \((z)=(^{})^{-1}(z)\) if \(z>_{z 0^{}}^{}(z)\), and \((z)=0\) otherwise._

Specifically, if we instantiate Theorem 3 to Algorithms 1-3, we get the explicit estimates:

**Corollary 2**.: _Suppose that Algorithms 1-3 are run with \(_{}\) and, for Algorithm 3, \(_{}(0,1/2)\). Then, with notation as in Theorem 3, \(X_{t}\) converges to \(\) at a rate of_

\[(X_{t},) C1-c_{ s=1}^{t}_{s}_{+}&(z)=z^{2}/2\\ -c_{s=1}^{t}_{s}&(z)=z z \\ 1c+_{s=1}^{t}_{s}^{2}&(z)=-4 \] (18)

_for positive constants \(C,c>0\). In particular, the projection-based variants of Algorithms 1-3 converge to m-club sets in a **finite** number of steps._rood sketch._ The proof of Theorems 2 and 3 is quite involved so we defer it to Appendix D. At a high level, it hinges on constructing a family of "primal-dual" energy functions, one per pure deviation from the set \(\) under study. If unilateral deviations from \(\) incur a cost to the deviator (that is, if \(\) is club), these energy functions can be "bundled together" to produce a suitable Lyapunov-like function for \(\). In more detail, the minimization of each individual energy function implies that the score variable \(Y_{t}\) of (RL) diverges along an "astral direction" in the payoff space \(\) - i.e., it escapes to infinity along the interior of a certain convex cone of \(\). Because this minimization occurs at infinity, the aggregation of offsets and random errors in (RL) affords some extra "wiggle room" in our martingale analysis, so we are able to show that \(X_{t}=Q(Y_{t})\) remains close to \(\) under a much wider range of parameters compared to Theorem 1. Then, a series of convex analysis arguments in the spirit of  coupled with the definition of \(Q\) allows us to show that the escape of \(Y_{t}\) along the intersection of all these cones implies convergence to \(\) at the specified rate.

On the converse side, if an asymptotically stable set is not club, we can find a non-costly (and possibly profitable) deviation \(z\) from \(\) which is selected against by (RL). However, this extinction runs contrary to the reinforcement of better replies under (RL), an argument which can be made precise by applying the martingale law of large numbers to \( Y_{t},z\). The irreducible stability of m-club sets then follows by invoking this criterion reductively for any potentially stable subset \(^{}\) of \(\). 

**Discussion and remarks.** Theorems 2 and 3 are our main results linking dynamic and strategic stability, so we conclude with a series of remarks. First, we should note that Theorem 2 can be summed up as follows: _a product of pure strategies is (minimally) closed under better replies if and only if its span is (irreducibly) stable under regularized learning._ Importantly, this equivalence is based solely on the game's payoff data: it does not depend on the specific choices underlying (RL), including the choice map employed by each player, whether some players are using an optimistic adjustment or not, if they have access to their full payoff vectors, etc. As such, this equivalence provides a crisp operational criterion for identifying which pure strategy combinations ultimately persist under regularized learning - and, via Theorem 3, _how fast_ this identification takes place.

In this light, Theorem 2 essentially states that the only robust prediction that can be made for the outcome of a regularized learning process is (minimal) closedness under better replies. This interpretation has significant cutting power for the emergence of rational behavior. To begin, in terms of equilibrium play, it effortlessly implies that a pure strategy profile is stochastically asymptotically stable under (RL) if and only if it is a strict Nash equilibrium. A version of this equivalence was only recently proved in  and  (in continuous and discrete time respectively), so Theorem 2 can be seen as a far-reaching generalization of these recent results. More to the point, since every m-club set \(\) contains an essential equilibrium component that is fully supported in \(\), Theorem 2 also provides an important link between dynamic and structural stability: if an equilibrium - or a component of equilibria - is not robust to perturbations of the underlying game, _it cannot be robustly identified by a regularized learning process_ (and vice versa). This remark is of particular importance for extensive-form games as such games often have non-generic equilibrium components that cannot be treated otherwise by the existing theory.

Finally, we should stress that Theorems 2 and 3 guarantee convergence even with a constant step-size. Together with the finite-time convergence guarantees of Corollary 2 for projection-based methods, this feature is a testament to the robustness of club sets as, in the presence of uncertainty, convergence almost always requires a vanishing step-size which can slow convergence down to a crawl. We find this robust convergence landscape particularly intriguing for future research on the topic.

Figure 1: The long-run behavior of EXP3 (Algorithm 3) in four representative \(2 2 2\) games. In all cases, the dynamics converge to m-club sets, either _strict equilibria_ themselves, or spanning an _essential component_ of Nash equilibria. The details of the numerics and the games being played are provided in the appendix.