# FedGMark: Certifiably Robust Watermarking for Federated Graph Learning

Yuxin Yang\({}^{1,2}\) Qiang Li\({}^{1}\) Yuan Hong\({}^{3}\) Binghui Wang\({}^{2}\)

\({}^{1}\)College of Computer Science and Technology, Jilin University, Changchun, Jilin, China

\({}^{2}\)Department of Computer Science, Illinois Institute of Technology, Chicago, Illinois, USA

\({}^{3}\)School of Computing, University of Connecticut, Storrs, Connecticut, USA

Corresponding Author (bwang70@iit.edu)

###### Abstract

Federated graph learning (FedGL) is an emerging learning paradigm to collaboratively train graph data from various clients. However, during the development and deployment of FedGL models, they are susceptible to illegal copying and model theft. Backdoor-based watermarking is a well-known method for mitigating these attacks, as it offers ownership verification to the model owner. We take the first step to protect the ownership of FedGL models via backdoor-based watermarking. Existing techniques have challenges in achieving the goal: 1) they either cannot be directly applied or yield unsatisfactory performance; 2) they are vulnerable to watermark removal attacks; and 3) they lack of formal guarantees. To address all the challenges, we propose FedGMark, the first certified robust backdoor-based watermarking for FedGL. FedGMark leverages the unique graph structure and client information in FedGL to learn customized and diverse watermarks. It also designs a novel GL architecture that facilitates defending against both the empirical and theoretically worst-case watermark removal attacks. Extensive experiments validate the promising empirical and provable watermarking performance of FedGMark. Source code is available at: https://github.com/Yuxin104/FedGMark.

## 1 Introduction

Federated Graph Learning (FedGL)  leverages a server and multiple clients to collaboratively train GL methods  via federated learning (FL) . In recent years, FedGL has attracted increasing interest in domains such as disease prediction , recommendation systems , and molecular classification . In addition, several industries have deployed/open-sourced their FedGL frameworks, such as Alibaba's FederatedScope-GNN  and Amazon's FedML-GNN . However, FedGL models are typically left unprotected, rendering them vulnerable to threats like illegal copying, model theft, and malicious distribution. For instance, a business competitor may replicate a model to gain competitive advantages or a malicious user may sell the model for profits. These threats waste the model owner's investment (e.g., labor costs, time, and energy) and infringe upon the legitimate copyrights of the model.

Backdoor-based watermarking  is a _de facto_ model ownership verification technique to mitigate the above threats. This technique typically consists of two steps: 1) Embedding the target model with a watermark. The model owner injects a specific backdoor trigger (i.e., watermark) into some clean samples and trains the target model withthis watermarked data along with the remaining clean data. Then the trained target (watermarked) model could have both high watermark accuracy (i.e., accurately classify testing data with the same watermark as the owner desires) and main task accuracy (i.e., accurately classify clean testing data). 2) Model ownership verification. When suspecting the target model is illegally used by others, the model owner can recruit a trusted third party for model ownership verification. Particularly, the true model owner knows how the target model behaves as expected by providing the trusted third party the carefully designed watermarked data, while the illegal parties cannot do so. Notice that, since all the clients have devoted computation and data to the training, they have a strong intention to jointly protect their ownership of the model.

In this paper, we aim to protect the ownership of FedGL models via backdoor-based watermarking. We observe backdoor-based watermarking methods for protecting the ownership of FL model on _non-graph_ data (Li _et al._, 2022; Tekgul _et al._, 2021; Shao _et al._, 2022; Yang _et al._, 2023; Lansari _et al._, 2023) or centralized GL model on graph data (Xu _et al._, 2023) have been recently developed. However, applying these methods for protecting FedGL models faces challenges and weaknesses.

* _Inapplicable or ineffective:_ Existing methods for non-graph data cannot be directly applied for graph data. For instance, they require input data have same size, while graphs can have varying sizes; they are unable to consider the _connectivity_ information such as edges connecting nodes in the graph data. The only method for graph data (Xu _et al._, 2023) uses a naive _random graph_ (e.g., generated by the ER model (Gilbert, 1959)) as a watermark. Extending this watermark from centralized GL to FedGL models exhibits _unsatisfactory performance_, as shown in Table 1. For instance, the watermark accuracy is less than 60% in all the studied graph datasets and FedGL models. The core reason is the random graph watermark does not use any graph structure information or client information that are unique in FedGL.
* _Vulnerable to watermark removal attacks:_ They are vulnerable to existing watermark removal techniques such as distillation and finetuning (Bansal _et al._, 2022) (more details in Section 2.3). For instance, as illustrated in Table 1, distillation can reduce the watermark accuracy to less than 30%.
* _Lack or weak formal guarantees:_ All these methods do not provide formal robustness guarantees against watermark removal attacks. This could make them even vulnerable to more advanced attacks. For instance, our proposed layer-perturbation attack can further reduce the watermark accuracy, e.g., perturbing only 1-layer parameters of the watermarked model yields only 10% watermark accuracy (while main accuracy is marginally affected). Bansal _et al._ (2022) proposed the first certified watermark for centralized non-graph learning models against \(l_{2}\) model parameter perturbation. However, its certified radius is only 1.2, meaning the \(l_{2}\) norm of a (usually million-dimensional) perturbation vector cannot exceed 1.2 to maintain the watermark accuracy.

We address all the above issues by proposing a certified robust backdoor-based watermark method for FedGL, called **FedGMark**.2 FedGMark enjoys several properties: 1) Its designed watermarks can handle varying size graphs and utilize both graph structure and client information unique in FedGL models; 2) It is _empirically_ robust to both existing watermark removal attacks and the proposed layer-perturbation attack; and 3) more importantly, it is _provably_ robust to the layer-perturbation attack (the layer parameters can be arbitrarily perturbed), when the number of the perturbed layers is bounded.

    &  &  &  &  &  &  &  &  &  \\  & & & M\(\) & M\(\) & M\(\) & M\(\) & M\(\) & M\(\) & M\(\) & & M\(\) & M\(\) & M\(\) & M\(\) & M\(\) \\    & None & **0.82** & **0.47** & **0.82** & **0.42** & **0.80** & **0.43** &  & None & **0.71** & **0.56** & **0.70** & **0.54** & **0.70** & **0.52** \\  & Distillation & 0.81 & 0.38 & 0.80 & 0.35 & 0.75 & 0.32 & PROTENIS & Distillation & 0.71 & 0.30 & 0.70 & 0.32 & 0.70 & 0.28 \\  & Finetuning & 0.82 & 0.33 & 0.80 & 0.29 & 0.78 & 0.27 & Finetuning & 0.71 & 0.27 & 0.70 & 0.29 & 0.70 & 0.25 \\  & 1-Layer Pert. & 0.78 & 0.24 & 0.79 & 0.23 & 0.79 & 0.19 &  &  &  &  &  &  &  \\  & **0.73** & **0.39** & **0.70** & **0.38** & **0.71** & **0.38** & & & & & & & & & \\   & Non & **0.73** & **0.39** & **0.70** & **0.30** & **0.70** & 0.30 & CollAB & Distillation & 0.72 & 0.47 & 0.73 & 0.49 & 0.71 & 0.47 \\  & Finetuning & 0.72 & 0.19 & 0.70 & 0.21 & 0.70 & 0.22 & Finetuning & 0.72 & 0.35 & 0.72 & 0.36 & 0.71 & 0.40 \\  & 1-Layer Pert. & 0.70 & 0.11 & 0.70 & 0.15 & 0.70 & 0.12 & 1-Layer Pert. & 0.67 & 0.31 & 0.66 & 0.26 & 0.68 & 0.38 \\   

Table 1: Results of adapting the random graph-based watermarking GL method (Xu _et al._, 2023) to watermark FedGL models. “MA”: main task accuracy; “WA”: watermark accuracy.

Specifically, as depicted in Figure 1, FedGMark consists of two modules: _1) Customized Watermark Generator (CWG)_: it learns the customized watermark for individual graphs and clients in FedGL, by integrating the edge information from the client graphs and the unique key features of the clients. CWG can significantly enhance the diversity and effectiveness of the generated watermarks. _2) Robust Model Loader (RML)_. RML designs a new GL model that consists of multiple submodels, where each submodel can be any existing GL model. It also introduces a voting classifier for assembling the submodels' predictions. Such a design can facilitate deriving the certified watermark performance against the (worst-case) layer-perturbation attack.

We evaluate FedGMark on four real-world graph datasets (MUTAG, PROTEINS, DD, and COLLAB) and three FedGL models including Fed-GIN, Fed-GSAGE, and Fed-GCN, whose base GL models are GIN , GSAGE , and GCN , respectively. Extensive experimental results show FedGMark achieves high main accuracy and watermark accuracy under no attacks and watermark removal attacks, high certified watermark accuracy, and significantly outperforms the existing method. Such good results demonstrate the potential of FedGMark as a watermarking method to protect the ownership of FedGL models.

We summarize our main contributions of this paper as follows:

* To our best knowledge, this is the first work to protect the ownership of emerging FedGL models.
* We propose a certifiably robust backdoor-based watermarking method FedGMark for FedGL.
* We validate the effectiveness of FedGMark in multiple FedGL models and real-world graph datasets under no attack, existing backdoor removal attacks, and worst-case layer-perturbation attacks.

## 2 Preliminaries

### Federated Graph Learning (FedGL)

Given a graph \(G=(,)\) as input, a GL model for graph classification learns a graph classifier \(f\) that outputs a label \(f(G)=y\) for a graph. Here, \(,,\) represent the set of nodes, edges, and labels, respectively. \(\{0,1\}^{||||}\) is the binary adjacency matrix of \(G\), where \([v_{j},v_{k}]=1\) if there exists an edge between nodes \(v_{j}\) and \(v_{k}\), and \(0\) otherwise, with \(||\) the total number of nodes. FedGL employs FL techniques  to collaboratively train GL models with a set of (e.g., \(T\)) clients \(=\{1,,T\}\) and a server. Assuming each client \(i\) has a set of graphs \(^{i}\), we illustrate the training process of FedGL using the \(e\)-th epoch as an example: 1) Initially, the server distributes the global model parameters \(_{e}\) to a randomly selected subset of clients \(_{e}\), where \(_{e}\). 2) Upon receiving \(_{e}\), each client \(i\) trains its local model parameter \(_{e}^{i}\) with its own graphs \(^{i}\) and updates its model parameters via SGD, i.e., \(_{e}^{i}=_{e-1}^{i}-_{_{e}}L(_{e};^{i})\), where \(L(_{e};^{i})\) represents a loss function, e.g., cross-entropy loss. After training, client \(i\) submits its update model parameters \(_{e}^{i}\) to the server. 3) The server aggregates local model parameters of the selected clients i.e., \(\{_{e}^{i}:i_{e}\}\) and updates the global model parameter, e.g., \(_{e+1}=_{e}|}_{i_{e}}_{e}^{i}\) via the average aggregation , for the next epoch. This iterative process continues until the global model converges or reaches the maximum number of epochs.

### Backdoor-based Watermarking for GL

Backdoor-based watermarking  adopts the idea of backdoor attack  from the adversarial realm to facilitate model ownership verification. To watermark the GL

Figure 1: Overall pipeline of the proposed certified watermarks.

model, assume the model owner has a set of clean graphs \(\) and selects a subset of graphs \(_{w}\) to inject the watermark. In the existing method [Xu _et al._, 2023], the model owner first generates a random graph (e.g., via the ER-model) as the watermark for each to-be-watermarked graph. For instance, for a graph \(G_{w}\) with label \(y\), the generated random graph is \(G_{s}\) (its size is often smaller than \(G\)). The owner then attaches \(G_{s}\) to \(G\) to produce the watermarked graph \(G_{w}\), where nodes in \(G\) are randomly chosen and the edge status of these nodes are replaced by edges in \(G_{s}\). Finally, the owner assigns a desired label different from \(y\) to \(G_{w}\). The watermarked graphs together with the clean graphs are used to train the GL model. During model ownership verification, the one who can predict a high accuracy on these watermarked graphs can claim to be the model owner.

We note this method can be extended to watermark FedGL models, where each client can generate its own random graphs as the watermark and train its local model with the watermarked graphs and clean graphs. The server then aggregates the watermarked local models to update the global model.

### Watermark Removal Attacks

We consider three possible watermark removal attacks aiming to infringe the FedGL model ownership: distillation and finetuning from [Shafieinejad _et al._, 2021], and our proposed layer-perturbation attack. In all attacks, the attacker (e.g., malicious user) is assumed to know the target watermarked model.

**1) Distillation.** This attack has access to some unlabeled data sampled from the same data distribution. To remove watermarks without affecting the target model's main task performance, the attacker uses the unlabeled data to distill the target model during training. Specifically, the attacker initializes its model with the target model and labels the unlabeled data by querying the target model. The attacker's model is then updated with these unlabeled data and their predicted labels.

**2) Finetuning.** This attack assumes the attacker has some labeled data. The attacker then leverages the labeled data to further finetune the target model in order to forget the watermark. This attack is shown to pose a greater threat than the distillation attack [Bansal _et al._, 2022].

**3) Layer-perturbation attack.** This attack also assumes the attacker has some labeled data. As knowing the target watermarked model (and hence the architecture), the attacker can mimic training an unwatermarked model with the same architecture as the target model using the labeled data. To further test the model robustness, we assume the attacker also knows some true watermarked samples, similar to [Jiang _et al._, 2023]. Then, the attacker can replace _any_ layer(s)' parameters of the target model with those from the unwatermarked model to maximally reduce the watermark accuracy on its watermarked samples, while maintaining the main task performance. Our results (e.g., in Table 1) show this layer-perturbation attack (even only perturbing 1 layer parameters) is much more effective than the other two attacks (even though the whole model parameters can be perturbed).

### Threat Model

We follow existing methods [Shafieinejad _et al._, 2021; Bansal _et al._, 2022; Xu _et al._, 2023; Jiang _et al._, 2023], where the adversary is assumed to know all details of the pretrained watermarked FedGL model, but does not tamper with the training process. This means all clients and the server are benign and follow the federated training protocol, and the attack happens at the testing/inference time. We highlight this is in stark contrast to the training-time Byzantine attack on FL where some clients are malicious and they manipulate the training process.

**Attacker's knowledge.** The attacker has white-box access to the pretrained watermarked FedGL model. In addition, the attacker may also know some clean (unlabeled or labeled) training data, as well as watermarked data. Note that this setting actually makes our defense design the most challenging. If the defense can successfully defend against the strongest white-box attack on the watermarked FedGL model, it will also be effective against weaker attacks, such as black-box attacks.

**Attacker's capability.** The attacker can modify the pretrained model via leveraging its white-box access to the trained model and its hold training and watermarked data. For instance, the attacker can finetune the pretrained model via the labeled training data. More details of the capabilities of considered attacks are described in Section 2.3.

**Attacker's goal.** The attacker aims to remove the watermark based on its knowledge and capability, while maintaining the model utility. This allows it to illegally use the model without detection.

FedGMark: Our Certified Robust Watermark for FedGL

### Motivation and Overview

Recall that the random graph based watermark is unable to ensure high watermark accuracy for protecting FedGL (as shown in Table 1). This is because such random watermark does not use any graph structure or client information during FedGL training. Our results also show this method is vulnerable to the three watermark removal attacks. These weaknesses inspire us to design a more effective and robust watermarking method specially for FedGL model ownership verification.

We propose FedGMark, the first certified robust backdoor-based watermarking method for FedGL. FedGMark comprises two main components: _Customized Watermark Generator (CWG)_ and _Robust Model Loader (RML)_ (as depicted in Figure 1). The CWG module utilizes the unique property of each client, as different clients could have different properties (e.g., distributions of their graph data) and their optimal watermark could be different. Particularly, CWG learns the customized watermark for each graph using its structure information, and outputs a set of diversified watermarked graphs for each client. Further, inspired by existing GNNs (Xu _et al._, 2019), the RML module designs a new GL model that consists of multiple submodels, each being any existing GL model. It also introduces a voting classifier for aggregating the prediction results from the submodels. Under this design, FedGMark can be proved to be certified robust against the _worst-case_ layer-perturbation attack, once the number of perturbed layers is bounded. The model owner (e.g., participating clients in FedGL) adopts the designed GL model to train the local watermarked model with the learnt watermarked graphs and the remaining clean graphs. After the server-client training terminates, the ownership of the trained FedGL model can be verified via measuring its performance on a set of testing graphs injected with the _global watermark_, which is the integration of all clients' local watermarks.

### Customized Watermark Generator (CWG)

CWG consists of two networks: \(\) and \(\). \(\) designs the watermark for each graph separately using the edge information, while \(\) learns client-wise watermarking style using predefined keys (e.g., client ID in this paper). The customized watermark for each client's graph is then decided using the output of \(\) and \(\). _Detailed network architectures of CWG can be seen in Table 7 in Appendix C_. We demonstrate how CWG can learn a customized watermark using a graph \(G^{i}=(^{i},^{i})\) from client \(i\) as an instance. The details are as follows:

* We first randomly select \(n_{w}\) nodes \(^{i}_{w}=\{v_{1},,v_{n_{w}}\}\) from \(^{i}\) as watermark nodes and construct a corresponding mask matrix \(^{i}\{0,1\}^{|^{i}||^{i}|}\) such that \(^{i}[v_{j},v_{k}]=1\) if \(v_{j},v_{k}^{i}_{w}\), and \(0\) otherwise. We also update the adjacency matrix \(^{i}\) of \(G^{i}\) according to \(^{i}_{w}\), i.e., setting \(^{i}[v_{j},v_{k}]=0, v_{j},v_{k}^{i}_{w}\). This allows us focus on learning the edge status between watermarked nodes.
* Given the client \(i\)'s ID string \(k^{i}\), we utilize a cryptographic hash function, such as MD5, to convert it into an integer (e.g., \(128\)-bit long with the integer range \([0,2^{128}-1]\)). This integer is then employed as a seed to produce a key matrix \(^{i}^{|^{i}||^{i}|}\). Then, we employ \(\) and \(\) to extract edge features and key features, resulting in \(}^{i}=(^{i})^{|^ {i}||^{i}|}\) and \(}^{i}=(^{i})^{|^ {i}||^{i}|}\), respectively.
* We finally learn the customized watermark for \(G^{i}\) by integrating \(}^{i}\), \(}^{i}\), and \(^{i}\), and obtain the corresponding watermarked graph as \(G^{i}_{w}=(^{i},^{i}_{w})\). Here \(^{i}_{w}\) is the set of edges according to the updated adjacency matrix \(^{i}^{i}\), where \(\) is the element-wise addition and \(^{i}=((}^{i}}^{i})>0.5)^{i}\) contains the edge status between the watermark nodes \(^{i}_{w}\). Here, \(\) is the element-wise product, \((p)\) is an indicator function returning \(1\) if \(p\) is true, and \(0\) otherwise. We adopt \(0.5\) as a threshold to decide the presence of edges between watermarked nodes.

### Robust Model Loader (RML)

This module aims to design a new GL model that is provably robust to the layer-perturbation attack. Towards this end, we design a GL model architecture to incorporate multiple submodels; and devise a majority voting-based ensemble classifier on top of the predictions of these submodels.

**Architecture of the proposed GL model.** Intuitively, each client can take a base GL model (e.g., GIN (Xu _et al._, 2019)) and split it according to the layer indexes to obtain multiple submodels. For instance, a \(8\)-layer GIN can be represented with layer indexes \(\{l_{1},,l_{8}\}\). Splitting this GIN into \(4\) submodels \(\{_{1},,_{4}\}\) with layer indexes \(\{l_{1},l_{2}\},,\{l_{7},l_{8}\}\) means \(_{i}\) contains layers\(\{l_{2i-1},l_{2i}\}\), from the GIN. However, submodels splitted in this way are coupled from each other, making them unable to defend against layer-perturbation attacks. To tackle this problem, we design the novel GL model \(\) that is an ensemble of a set of \(S\)_independent_ submodels \(\{_{1},_{2},,_{S}\}\), where each submodel \(_{i}\) is a base GL model. This approach can hence be easily adapted to any existing FedGL. Further, to prevent homogeneity, we define varying channels for these submodels to diversify them. Details of the model architecture are shown in Table 8 in Appendix C.

**A majority-voting based ensemble classifier.** The designed GL model architecture inspires us to leverage the idea of ensemble classifier, which can combine the predictions of base "weak" classifiers. Specifically, we propose a majority voting-based ensemble classifier to combine the predictions of the submodels. Given a testing graph \(G\) and a graph classifier \(f\), we denote the prediction of the submodel \(_{i}\) for \(G\) as \(y=f(_{i},G)\). For a GL model \(\) with \(S\) submodels \(\{_{1},_{2},,_{S}\}\), we can count the submodels that classify \(G\) to be \(y\) as \(N_{y}=_{i=1}^{S}(f(_{i},G)=y)\). Then we introduce our majority-voting based ensemble classifier \(g\) to classify \(G\) as: \(g(,G)=*{arg\,max}_{y}N_{y}\). In cases of ties, our ensemble classifier \(g\) selects the label with a smaller index.

### Training the Proposed FedGL Model

The overall training process consists of three iterative steps: 1) training the proposed GL model in all clients; 2) training the CWG module in _watermarked clients_, i.e., the clients that aim to inject watermarked graphs for protecting the model ownership; and 3) aggregating the clients' GL models to produce the target watermarked model. The final global model is the learnt watermarked FedGL model. Details of training can be seen in Algorithm 1 in the Appendix.

**Step 1: Training the proposed GL model.** Assume we have \(T_{w}\) watermarked clients with indexes \([1,T_{w}]\). For each watermarked client \(i\), we split its training graphs \(^{i}\) into the watermarked graphs \(^{i}_{w}\) with a target label, say \(y_{w}\), and remaining clean graphs \(^{i}_{c}\), and then customize the watermark for each graph in \(^{i}_{w}\) using the \(\) module (see **Step 2**). Given the client's GL model \(^{i}\) with \(S\) submodels \(\{^{i}_{1},^{i}_{2},,^{i}_{S}\}\), we train each submodel \(^{i}_{j}\) via minimizing the loss on \(^{i}_{c}\) and \(^{i}_{w}\), i.e., \(^{i}_{j}=*{arg\,min}_{^{i}_{j}}L(^{i} _{j};^{i}_{c}^{i}_{w})\). For an unwatermarked client \(k[T_{w}+1,T]\), we utilize all clean graphs \(^{k}\) to train each submodel \(^{k}_{j}\) separately, i.e., \(^{k}_{j}=*{arg\,min}_{^{k}_{j}}L(^{k} _{j};^{k})\).

**Step 2: Training the CWG.** We denote the parameters of the CWG module for a watermarked client \(i[1,T_{w}]\), as \(^{i}\). The parameters include two networks, \(\) and \(\). Each client \(i\) trains its CWG \(^{i}\) to ensure that the generated watermarks be effective and diverse. Formally, we have \(^{i}=*{arg\,min}_{^{i}}L(^{i};^{i}_ {w})\), \(i[1,T_{w}]\).

**Step 3: Aggregating clients' GL models.** The server averages GL models \(\{^{i}\}_{i T}\) to produce the global model \(\), and distributes this model to selected clients in the next iteration.

### Model Ownership Verification

When suspecting the target FedGMark model \(\) is illegally used by others, the model owner (all the participating clients or their representative) can recruit a trusted judge for model ownership verification. Typically, the judge requests both the true model owner and the illegal party to provide some test data for verification. Only when the one knows the predictions by the target model for the provided test data by both parties, the judge will confirm this party the model ownership. In particular, besides providing the clean data \(^{i}_{c}\) by both parties that behave normally, the true model owner especially provides the designed watermarked data \(^{i}_{w}\) that only s/he knows the model behaves on. As a result, both parties know the prediction results on \(^{i}_{c}\), but the illegal party is hard to predict accurately on \(^{i}_{w}\) provided by the true model owner.

### Certified Robustness Guarantees against Layer-Perturbation Attacks

We show the above design, with any layer-perturbation attack, ensures the predictions of the learnt watermarked FedGL model and its compromised counterpart for the watermarked graphs are consistent, once the number of perturbed layers is bounded. Given the target watermarked FedGL model \(\) and its \(S\) submodels \(\{_{1},,_{S}\}\), we denote \(^{}\) as the comprised model and \(\{^{}_{1},,^{}_{S}\}\) as its \(S\) submodels. For each watermarked graph \(G_{w}\), we use the ensemble classifier \(g\) on submodels' predictions, i.e., its predictions on \(\) and \(^{}\) are \(g(,G_{w})=*{arg\,max}_{y}N_{y}\), and \(g(^{},G_{w})=*{arg\,max}_{y}N^{}_{y}\), respectively, where \(N_{y}=_{i=1}^{S}(f(_{i},G)=y)\) and \(N^{}_{y}=_{i=1}^{S}(f(^{}_{i},G)=y)\). Then we have the following result on guaranteeing the number of perturbed layers on the target watermarked model.

**Theorem 1** (Certified number of perturbed layers \(r\).): _Let \(\), \(^{}\), \(g\), and \(G_{w}\) be above defined. Suppose \(N_{A}\) and \(N_{B}\) are the largest and second largest count outputted by \(g\) on \(G_{w}\). For any layer-perturbation attack, we have \(g(,G_{w})=g(^{},G_{w})\), when the number of perturbed layers \(r\) satisfies:_

\[r r^{*}=(N_{A}-N_{B}+[A<B]-1)/2,\] (1)

_where \([]\) is the indicator function and \(r\) is called the certified number of perturbed layers._

We also show the tightness of our derived \(r^{*}\) in the following theorem:

**Theorem 2** (Tightness of \(r^{*}\).): _Without using extra information of \(f\), our derived \(r^{*}\) in Theorem 1 is tight. I.e., \(r^{*}\) is the maximum number of perturbed layers tolerated by our target watermarked model._

The proofs of Theorems 1 and 2 are deferred to Appendix A.

## 4 Experiments

In this section, we comprehensively evaluate FedGMark on multiple datasets, FedGL models, attack baselines, and experimental settings. More experimental results and discussions are in Appendix.

### Experimental Setup

**Datasets and models.** We evaluate our FedGMark on four real-world graph datasets for graph classification: MUTAG [Debnath _et al._, 1991], PROTEINS [Borgwardt _et al._, 2005], DD [Dobson and Doig, 2003], and COLLAB [Yanardag and Vishwanathan, 2015]. Details about the statistics of those datasets are shown in Table 6 in Appendix C. Following prior work Shen _et al._; Xia _et al._, we choose the well-known GIN [Xu _et al._, 2019], GSAGE [Hamilton _et al._, 2017], and GCN [Kipf and Welling, 2017] as the GL model. All these network architectures involved in the experiments are detailed in Table 8 in Appendix C.

**Parameter setting.** We implement our method using one NVIDIA GeForce GTX 1080 Ti GPU. In FedGL, we use \(T=40\) clients in total and train the model 200 iterations. The server randomly selects \(50\%\) clients in each iteration. We define the target label of watermarking graphs as \(1\), and each participating client randomly selects \(10\%\) graphs with labels not \(1\) as the watermarking graphs. We extensively validate the effectiveness and robustness of the proposed watermarking method with the following hyperparameters details: the number of submodels \(S=\{4,8,16\}\), the number watermarked clients \(T_{w}=\{5,10,20\}\) (both \(S\) and \(T_{w}\) are halved on MUTAG due to less data), the watermarked nodes \(n_{w}=\{3,4,5\}\), and the number of perturbed layers \(r=\{1,,5\}\) in the layer-perturbation attack. By default, we set \(S=4\), \(T_{w}=10\), \(n_{w}=4\), \(r=1\). While studying the impact of a hyperparameter, we fix the others as the default value.

**Evaluation metric.** We use three metrics for evaluation: the main task accuracy (MA), watermark accuracy (WA), and certified WA (CWA@\(r\)). An effective and robust watermarked model is expected to achieve both high MA and WA. CWA evaluates the certified robustness of FedGMark against layer-perturbation attacks. CWA@\(r\) is defined as the fraction of testing graphs that are provably predicted as the target label, when at most \(r\) layers in client models can be arbitrarily perturbed.

**Attack baselines.** We evaluate FedGMark against existing watermark removal attacks including distillation and finetuning, and our proposed layer-perturbation attack. In our setting, the layer-perturbation attack can replace _any_ layer(s)' parameters of the target watermarked model with those from the un watermarked model to maximally reduce the watermark accuracy on its watermarked graphs. When perturbing multiple layers, we utilize a greedy algorithm to decide the perturbed layers--we search for one optimal perturbed layer at each step. Specifically, we first traverse perturbing layers in the watermarked model and find the one that maximally reduces the watermark accuracy. We then search for the remaining layers based on this one and continue the process.

### Empirical Results: MA and WA

#### 4.2.1 Results under the Default Setting

We first assess our FedGMark against empirical and layer-perturbation attacks under the default setting. Experimental results are provided in Table 2. We have the following observations:

* **1) Our FedGMark significantly outperforms the existing method under no attack.** Recall in Table 1 that [Xu _et al._, 2023] obtains \(<60\%\) WAs across all datasets and FedGL models. In contrast, FedGMark can achieve WAs \(>70\%\) in almost all cases, while having similar MAs.

Such significant improvements confirm the superiority of our learnt watermarks over the random watermarks in . Figure 2 also visualizes example learnt watermarks and we can see these watermarks are diversified due to the proposed CWG.
* **2) Our FedGMark exhibits resistance to existing empirical attacks.** As shown in Table 1, existing methods are vulnerable to distillation and finetuning attacks. In contrast, the WAs of FedGMark under these two attacks are almost the same as those under no attack, demonstrating FedGMark is robust to the existing attacks.
* **3) Our FedGMark is resilient to the proposed layer perturbation attack.** We notice that the existing method has difficulties in defending against the proposed layer-perturbation attacks and shows an unsatisfactory WA, e.g., \(<25\%\) WA in almost all cases under the 1-layer perturbation attack. Conversely, our FedGMark can obtain a close WA compared to that without attack. This verifies our ensemble classifier in FedGMark is capable of resisting the 1-layer perturbation attack.

#### 4.2.2 Impact of Hyperparameters

This section studies the impact of hyperparameters in FedGMark against the layer-perturbation attack.

**Impact of #submodels \(S\).** We first examine the impact of \(S\) on the performance of FedGMark and report the results against the 1-layer perturbation attack in Table 3. We observe that the learnt watermarked model can resist to the 1-layer perturbation attack under all \(S\) and both the MAs and WAs are also similar. This indicates the number of submodels marginally affects the watermarking performance against the 1-layer perturbation attack.

   Datasets & Attack & Fed-GIN & Fed-GSAGE & Fed-GCN & Datasets & Attack & Fed-GIN & Fed-GSAGE & Fed-GCN \\  & & MA\(\) & WA\(\) & MA\(\) & WA\(\) & MA\(\) & WA\(\) & & & MA\(\) & WA\(\) & WA\(\) & WA\(\) \\    & None & **0.83** & **0.90** & **0.85** & **0.89** & **0.83** & **0.83** & & & None & **0.72** & **0.86** & **0.72** & **0.82** & **0.73** & **0.84** \\  & Distillation & 0.83 & 0.87 & 0.81 & 0.70 & 0.80 & 0.80 & PROTEINS & Distillation & 0.70 & 0.84 & 0.70 & 0.80 & 0.70 & 0.79 \\  & Finetuning & 0.82 & 0.88 & 0.79 & 0.76 & 0.78 & 0.81 & Finetuning & 0.70 & 0.84 & 0.71 & 0.82 & 0.72 & 0.80 \\  & 1-Layer Pert. & **0.82** & **0.90** & **0.83** & **0.91** & **0.81** & **0.84** & & & 1-Layer Pert. & **0.73** & **0.86** & **0.74** & **0.83** & **0.74** & **0.85** \\   & None & **0.73** & **0.65** & **0.74** & **0.57** & **0.74** & **0.56** & & None & **0.73** & **0.75** & **0.74** & **0.74** & **0.73** & **0.70** \\  & Distillation & 0.71 & 0.63 & 0.74 & 0.56 & 0.72 & 0.56 & COLLAB & Distillation & 0.73 & 0.71 & 0.72 & 0.73 & 0.70 & 0.69 \\  & Finetuning & 0.72 & 0.62 & 0.74 & 0.57 & 0.74 & 0.55 & Finetuning & 0.72 & 0.70 & 0.75 & 0.71 & 0.72 & 0.67 \\  & 1-Layer Pert. & **0.72** & **0.65** & **0.75** & **0.55** & **0.73** & **0.57** & & & 1-Layer Pert. & **0.72** & **0.76** & **0.73** & **0.74** & **0.74** & **0.71** \\   

Table 2: Results of our FedGMark under empirical watermark removal attacks.

Figure 2: Example learnt watermarks and watermarked graphs by our FedGMark. CWGs generated by different clients produce unique watermarks, characterized by distinct edge connection patterns.

**Impact of #watermarking clients \(T_{w}\).** The results on \(T_{w}=5\) and \(T_{w}=20\) are shown in Figure 4 in Appendix. We see that: _the larger number of clients generating watermarks, the better our watermarking performance_. For instance, the WA improves by \(11\%\), \(9\%\), and \(7\%\), respectively on the three FedGL models on PROTEINS when \(T_{w}\) increases from \(5\) to \(20\). This is because more watermark clients (thus more watermarked graphs) can ensure the FedGL model better learns the relation between the watermarks and the target label.

**Impact of the watermark size \(n_{w}\).** We also investigate \(n_{w}=3\) and \(n_{w}=5\), and the results are presented in Figure 5 in Appendix. Similarly, FedGMark obtains higher WA with larger \(n_{w}\). This is because a larger watermark size can facilitate the trained FedGL model better learn the watermark.

**Impact of #perturbed layers.** Finally, Table 4 shows the results of FedGMark vs. different #perturbed layers. We observe that: 1) The WA decreases as increasing the #perturbed layers. This is because the attacker has more attack capability by perturbing more layers. 2) Despite the theoretically guaranteed CWA being \(0\) at #perturbed layers \( 3\) as later depicted in the Figure 3, our empirical watermarking performance can still achieves WA from \(48\%\) to \(58\%\). Note that the change in MA is \(<4\%\) in all cases, compared with Table 2.

#### 4.2.3 Ablation Study

We investigate the contribution of each module in FedGMark, including the \(\) and \(\) in CWG, and the submodels used in RML. The results under the default setting and no attack are shown in Table 5. First, \(\) plays a crucial role on generating more effective watermarks and results in an improvement in WA from \(8\%\) to \(21\%\). Second, KeyNet facilitates watermark differentiation between clients, leading to an improve on WA from 5% to 8%. Third, the submodels in RML has a negligible impact on WA/MA. However, it is important to defend against the layer-perturbation attack, as shown in Section 4.3.

    &  & 1 & 2 & 3 & 4 & 5 &  & \#Perturbed Layers & 1 & 2 & 3 & 4 & 5 \\  & (Net) & WA\(\) & WA\(\) & WA\(\) & WA\(\) & WA\(\) & & & & & & & \\   & Fed-GINE & 0.90 & 0.89 & 0.54 & 0.23 & 0.16 &  & Fed-GINE & 0.86 & 0.84 & 0.58 & 0.46 & 0.39 \\  & Fed-GSAGE & 0.91 & 0.89 & 0.55 & 0.21 & 0.13 & & & & & & & & \\  & Fed-GCN & 0.84 & 0.83 & 0.49 & 0.24 & 0.18 & & & & & & & \\   & Fed-GINE & 0.65 & 0.61 & 0.56 & 0.32 & 0.21 &  & Fed-GSAGE & 0.74 & 0.73 & 0.57 & 0.35 & 0.25 \\  & Fed-GSAGE & 0.55 & 0.55 & 0.49 & 0.28 & 0.24 & & & & & & & \\   & Fed-GCN & 0.57 & 0.54 & 0.48 & 0.29 & 0.24 & & & & & & & \\   

Table 4: Impact of #perturbed layers on FedGMark against our layer-perturbation attack. Compared with Table 2, the change in MA is less than \(4\%\) in all cases.

    & G.Net & K.Net & RML &  & PROTEINS & DD & COLLAB \\  & & & & & MA\(\) & WA\(\) & MA\(\) & WA\(\) & WA\(\) & MA\(\) & WA\(\) & WA\(\) \\ 
**(a)** & ✓ & ✓ & ✓ & **0.81** & **0.90** & **0.72** & **0.86** & **0.73** & **0.65** & **0.73** & **0.75** \\
**(b)** & ✓ & ✓ & & 0.81 & 0.84 & 0.72 & 0.80 & 0.72 & 0.57 & 0.72 & 0.70 \\
**(c)** & ✓ & ✓ & 0.80 & 0.69 & 0.72 & 0.68 & 0.72 & 0.48 & 0.73 & 0.67 \\
**(d)** & ✓ & ✓ & & 0.81 & 0.89 & 0.73 & 0.86 & 0.71 & 0.66 & 0.72 & 0.75 \\
**(e)** & & & & 0.82 & 0.47 & 0.71 & 0.56 & 0.73 & 0.39 & 0.73 & 0.57 \\   

Table 5: Impact of different modules in FedGMark.

Figure 3: CWA vs. #perturbed layers \(r\) in the layer-perturbation attack.

### Certified Robustness Results: Certified WA

In this section, we evaluate the certified robustness of FedGMark against the layer perturbation attack. The CWAs on the three FedGL models and four datasets are depicted in Figure 3. We have several observations: 1) FedGMark achieves promising provable robustness results against the _worst-case_ layer perturbation attack, when the #perturbed layers is within the certified range in Eqn (1). For instance, when \(S=4\) and \(0<r 2\), CWA is close to WA without attack (\(r=0\)). 2) As \(S\) increases, the certified #perturbed layers also increases, showing that a more number of submodules in RML can better provably defend against the layer-perturbation attack. For instance, when \(S=16\), the CWA is \(86\%\) on MUTAG even when 5 any layers in the global model are arbitrarily perturbed, while it is 0 when \(S=4\). However, this is at the cost of requiring more computational resources (e.g., GPU memory, runtime). As shown in Table 10 in Appendix, the runtime is linearly to \(S\).

## 5 Related Work

**Backdoor-based watermarking for centralized models on non-graph data.** Many backdoor-based watermarking methods  have been proposed that can _empirically_ protect the model ownership. These methods mainly focus on centralized learning models on non-graph (e.g., image) data. Compared with non-graph data, graph data have unique graph structure information, e.g., entities are connected by links. Similarly, compared with centralized models, FL models are collaboratively trained by multiple clients, which could have their own uniqueness. Bansal _et al._  is the first watermarking method for centralized non-graph models with certified guarantee. However, its certified robustness performance is unsatisfactory.

**Backdoor-based watermarking for FL models on non-graph data.** A few recent works Tekgul _et al._ ; Li _et al._ ; Bansal _et al._  design backdoor-based watermarks for protecting the ownership of FL models on non-graph data, where the watermark can be injected into client's data or server's validation data. For instance, Tekgul _et al._  presented WAFFLE, an approach to watermark DNN models by incorporating a re-training step via watermarked data at the server. Li _et al._  leveraged each client's private watermark to verify FL model ownership, ensuring non-conflicting watermarks across different clients. However, these techniques cannot be directly applied to graph data, as they often require fixed input data, while graph data often have varying sizes. Moreover, all these methods do not provide guaranteed watermarking performance under the attack.

**Backdoor-based watermarking for centralized GL models on graph data.** The only work  handling graph data uses random graph as the watermark. This method can be extended to the FedGL model, but its watermarking performance is far from satisfactory-especially vulnerable to watermark removal attacks such as distillation, finetuning, and the layer perturbation attack.

**Backdoor attacks for centralized and federated GL models on graph data.** Several works  design backdoor attacks to manipulate GL models, enabling the attacker to influence the learned model to serve its purpose-the model will predict the attacker-chosen label for test graphs once they contain a predefined trigger. For instance, Zhang _et al._  uses random subgraphs as the trigger to backdoor centralized GL models, while Yang _et al._  learns subgraph trigger to backdoor FedGL models. Note that the goal of these works is orthogonal to ours.

## 6 Conclusion

We protect the model ownership of emerging FedGL trained on distributed graph data, and use the _de facto_ backdoor-based watermarking method. We develop the first certifiably robust backdoor-based watermarking method FedGMark for FedGL. FedGMark demonstrates the capability of achieving high empirical watermarking performance under no attack, under existing backdoor removal attacks and the proposed stronger layer-perturbation attack. FedGMark is also provably robust against the worst-case layer-perturbation attack, once the number of perturbed layers is bounded by Theorem 1.

**Acknowledgments.** We thank all anonymous reviewers for the constructive comments. Li is partially supported by the National Natural Science Foundation of China under Grant No. 62072208, Key Research and Development Projects of Jilin Province under Grant No. 20240302090GX. Hong is partially supported by the National Science Foundation under grant No. CNS-2302689, CNS-2308730, CNS-2319277 and CMMI-2326341. Wang is partially supported by the National Science Foundation under grant No. ECCS-2216926, CNS-2241713, CNS-2331302 and CNS-2339686.