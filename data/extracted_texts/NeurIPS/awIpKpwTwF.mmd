# LEACE: Perfect linear concept erasure in closed form

Nora Belrose\({}^{1}\) David Schneider-Joseph\({}^{1}\) Shauli Ravfogel\({}^{2}\) Ryan Cotterell\({}^{3}\)

Edward Raff\({}^{1}\) Stella Biderman\({}^{1,4}\)

\({}^{1}\)EleutherAI \({}^{2}\)Bar-Ilan University \({}^{3}\)ETH Zurich \({}^{4}\)Booz Allen Hamilton

{nora,stella}@eleuther.ai david@davidsj.com

###### Abstract

Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called concept scrubbing, which erases target concept information from _every_ layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Our code is available at https://github.com/EleutherAI/concept-erasure.

## 1 Introduction

The ability to prevent a machine learning system from using a specified concept is important for fairness and interpretability. Popular notions of fairness require that protected attributes should not causally affect predictions , and interpretability research often estimates the causal effect of a concept by attempting to remove it from a model's internal representations .

What it means for a model \(\) to "use" a concept Z is often vague and application-specific, but a necessary condition is that its outputs--and therefore its inputs and hidden states--should have significant _mutual information_ with Z.1**Concept erasure** leverages this fact to limit \(\)'s use of Z _without_ finetuning or inspecting its parameters. Instead, we edit the input or hidden states X used by \(\) to minimize the predictive \(\)-information \(I_{}()\), a tractable lower bound on the mutual information \(I(;)\) which measures the degree to which classifiers from the family \(\) can predict Z. Intuitively, if no classifier in \(\) can outperform a constant function at predicting Z--a condition known as **guardedness**--then \(\) can't use Z either, at least if \(\) is expressive enough relative to \(\).

In this work, we improve upon existing concept erasure techniques using a theory-driven approach. We focus on the case where \(\) is the set of linear classifiers, and prove a previously unnoticed equivalence: a classification task is linearly guarded _if and only if_ every class has exactly the same mean feature vector (SS 3). Leveraging this equivalence, we derive a simple necessary and sufficient condition for an affine transformation to produce linearly guarded features. We then identify the unique _surgical_ transformation in this family--the one that minimizes the mean squared distance from the original features with respect to _all_ norms induced by inner products, including the popular Euclidean and Mahalanobis norms. We name it **LEAst-squares Concept Erasure (LEACE)** (SS 4).

While prior work has focused on preventing linear models from leveraging Z, we aim to erase concepts from deep neural networks as well. Interpretability research has shown that networkscan be usefully described as encoding features in linear subspaces [11; 24; 41], suggesting that fundamentally nonlinear methods may not be necessary for successful erasure in DNNs. In light of this, we introduce a simple procedure called **concept scrubbing** (SS 6), which sequentially applies LEACE to the intermediate representations at each layer of a deep network.

We empirically validate our proposals, demonstrating the superiority of LEACE for erasing gender bias from BERT representations (SS 5.2), and using concept scrubbing to measure the extent to which large language models use part-of-speech information (SS 6).

## 2 Preliminaries

Consider a \(k\)-class classification task over jointly defined random vectors \(\) (the input data) and \(\) (the one-hot labels), with \(\) of finite first moment and taking values in \(^{d}\), and \(\) taking values in \(=\{\{0,1\}^{k}\ \ \|\|_{1}=1\}^{2}\) with each \((=j)>0\). Let \((;):^{d}^{k}\) be a predictor chosen from a function class \(=\{(;)\ |\ \}\) (presumed to contain all constant functions) so as to minimize the expectation \(((),)\) of some \(:^{k}[0,)\) in a class \(\) of loss functions.

We borrow the concept of **guardedness** from Ravfogel et al. , who define it in terms of \(\)-information . We opt for a slightly more general definition here, which is equivalent to theirs in the case of cross-entropy loss (see Appendix G).

**Definition 2.1** (Guardedness).: _Let \(\), \(\), \(\), and \(\) be as defined above, and let \(\) be the set of all random vectors of finite first moment taking values in \(^{d}\), jointly defined with \(\)._

_We say \(\ (,)-\)**guards**\(\) if, for all losses \(\), it maximizes the minimum expected loss:_

\[*{argmax}_{^{}}\ _{}\ ((^{};), ).\]

_In other words, its conditional distribution \((\ |\ =)\) is among the worst possible distributions for predicting \(\) from \(\) using a predictor of the form \((;)\) and a loss function in \(\)._

**Definition 2.2** (Trivially Attainable Loss).: _The **trivially attainable loss** for labels \(\) and loss \(\) is the lowest possible expected loss available to a constant predictor \(()=\):_

\[L_{}=_{^{k}}[(, )]\]

_We will sometimes write it \(L_{}^{(,)}\) in cases of possible ambiguity. If there is a specific constant predictor actually achieving this loss, we call it the **trivial predictor**\(_{}=_{}^{(,)}\)._

We examine this problem in the important case of loss functions \(:^{k}[0,)\) which are convex in the prediction \(()\), and linear predictors that take the functional form \((;,)=+\), for some bias \(^{k}\) and weight matrix \(^{k d}\).

**Definition 2.3** (Linear Guardedness).: _If \(\ (,)\)-guards \(\), where \(\) is the class of nonnegative loss functions which are convex in their first argument, and \(\) is the class of linear predictors \(()=+\), we say that \(\)**linearly guards**\(\)._

## 3 Theoretical Results

Our primary theoretical result is that the following conditions are all equivalent:

1. The data \(\) linearly guards the labels \(\). (Definition 2.3)
2. For all convex losses \(\), the trivially attainable loss is optimal on \((,)\). (Definition 2.2)
3. The class-conditional mean vectors \([\ |\ =i]\) are equal to the unconditional mean \([]\).
4. Every component of \(\) has zero covariance with every component of \(\).
5. Every linear classifier evaluated on \(\) exhibits statistical parity w.r.t. \(\). (App. C)

The equivalence of conditions 1, 2, and 5 is relatively straightforward to show, and the relevant theorems can be found in Appendices B and C. The other equivalences are proven below (cond. 3 \(\) cond. 2 in SS 3.1 and SS 3.2); cond. 3 \(\) 4 in SS 3.3).

### Equality of Class Centroids Implies Linear Guardedness

The following result establishes the implication from condition 3 to condition 2.

**Theorem 3.1**.: _Suppose \(\) is convex in the linear prediction \(\). Then if each class-conditional mean \(=i\) is equal to \(\), the trivially attainable loss cannot be improved upon._

Proof.: Let \(()=+\) be any linear predictor. By Jensen's inequality,3 the loss with \(\) evaluated on \(\) is lower bounded by the loss with \(\) evaluated on the unconditional mean of the data \(\):

\[(,) =_{}(, )\] \[_{} ,\] (Jensen's inequality) \[=_{}+ , \] (linearity of \[\] ) \[=_{}+ ,.\] (by assumption)

This in turn is the loss of the constant predictor \(^{}()=+\). Since the trivially attainable loss is the best that can be achieved by a constant predictor, and _every_ predictor's loss is lower bounded by that of some constant predictor, we cannot improve upon the trivially attainable loss. 

Intuitively, this shows that the classifier's expected loss is lower-bounded by the loss it would receive if each data point were replaced with the centroid of its class. But, if these centroids are all equal, the loss can't be any lower than what we'd get if every data point were replaced with the _global_ mean \([]\). In that case, the data points are indistinguishable and we can't do better than \(=\).

### Linear Guardedness Implies Equality of Class Centroids

We now prove the implication from condition 2 to condition 3. Condition 2 applies when the trivially attainable loss is optimal for _all_ convex losses, including cross-entropy loss in particular. And if it holds for cross-entropy loss, we now show that condition 3--the class centroids are equal--must follow. First a more general lemma:

**Lemma 3.2**.: _Suppose \(\) has bounded partial derivatives, which when off-category never vanish and do not depend on the category, i.e. \((,z_{1})/_{i}=(,z_{2 })/_{i} 0\) for all categories \(z_{1},z_{2} i\). If \((,)\) is minimized among linear predictors by the constant predictor \(()=^{*}+^{*}\) with \(^{*}=\), then each class-conditional mean \(|=i\) is equal to \(\)._

Proof.: The first-order optimality condition on the \(i^{}\) component of our parameters \(\) and \(\) yields the equations:

\[(,)}{_{i }}}{ b_{i}}=0 (,)}{_{i }}}{_{1}}=,\] (1)

where we have used the boundedness of \(\)'s partial derivative and the finite first moment of \(}{ b_{i}}=1\) and \(}{_{1}}=\) to justify (via the Dominated Convergence Theorem) interchanging the derivative with the expectation.

Since \(\) is constant over all values of \(\), and \(}{ b_{i}}=1\), the first equation in (1) reduces to:

\[(=i)(,i)}{_{i}}+ ( i)(, i)}{ _{i}}=0,\] (2)

where \((,i i)}{_{i}}\) is an abuse of notation denoting the off-category partial derivative, emphasizing its independence of the category \(\).

Similarly, the constancy of \(\) and the fact that \(}{_{1}}=\) reduces the second equation in (1) to:

\[(=i)(,i)}{_{i}} =i+( i)(, i)}{_{i}}  i=.\] (3)

Solving for \((=i)(,i)}{_{i}}\) in (2) and substituting in (3) gives us:

\[( i)(, i)}{ _{i}}( i- =i)=.\]

If \(( i)=0\), then \([]=[|=i]\) is trivially true. Otherwise, using the non-vanishingness of the off-category partial derivative \((, i)}{_{i}}\), division yields the equivalence of \(=i\) to \( i\), and hence to the unconditional mean \(\). 

We now show that Lemma 3.2 applies to the widely used cross entropy loss:

**Theorem 3.3**.: _If the class probabilities \((=j)\) are all nonzero, and the trivially obtainable loss is optimal when \((,z)=-)}{_{i=1}^{k}(_{i})}\), then each class has the same mean \(=z\)._

Proof.: In this case, the trivial predictor \(_{r}()_{j}=((=j))\) exists, achieving the trivially obtainable loss, which we have assumed optimal. Furthermore, \(\) has on-category partial derivative \((,i)/_{i}=(_{i})/_{j=1}^{k} (_{j})-1(-1,0]\), and nonvanishing off-category partial derivative \((, i)/_{i}=(_{i})/_{j=1}^{k} (_{j})(0,1)\), both bounded, so the conditions of Lemma 3.2 apply. 

### Linearly Guarded Labels Have Zero Covariance with the Features

The next theorem establishes the equivalence of conditions 3 and 4.

**Theorem 3.4**.: _Let \(\) be a random vector taking values in \(^{d}\) with finite first moment, and \(\) a random vector taking values in \(\{0,1\}^{k}\) with one-hot encoding, with each class probability \((=j)\) being nonzero. Then the class-conditional means \([|=j]\) are all equal to the unconditional mean \([]\) if and only if every component of \(\) has zero covariance with every component of \(\), i.e. the cross-covariance matrix \(_{}\), whose \((i,j)^{}\) entry is \((_{i},_{j})\), is the zero matrix._

Proof.: Since \(\) is one-hot, we can rewrite the \((i,j)^{}\) entry of \(_{}\) as:

\[[_{i}_{j}]-[_{i}] [_{j}]=(=j)[_{i}| =j]-[_{i}].\]

As \((=j)>0\), it follows that \([_{i}|=j]=[_{i}]\) if and only if \((_{i},_{j})=0\). 

We have thus established the equivalence of the first four conditions stated earlier. See Appendix C for the last one, on statistical parity.

## 4 Least-Squares Concept Erasure

In Section 3 we saw that \(\) linearly guards \(\) if and only if each component of \(\) has zero covariance with each component of \(\). We will now characterize the set of affine transformations \(r()=+\) such that \(r()\) linearly guards \(\).

**Theorem 4.1**.: _Let \(\) and \(\) be random vectors taking values in \(^{d}\) and \(^{k}\) respectively, with \(\) of finite first moment. Then given some affine function \(r()=+\), the modified random vector \(r()\) linearly guards \(\) if and only if the columns of the cross-covariance matrix \(_{}\) are contained in the null space of \(\)._

Proof.: From Theorem 3.4 we know that \(r()\) linearly guards \(\) if and only if \((r(),)\) is the zero matrix. By the linearity property of cross-covariance, we have:

\[(r(),)=(+ ,)=(,)= _{}.\]

Therefore, \(r()\) linearly guards \(\) if and only if \(()(_{})\).

**Implications for prior work.** Notably, the above theorems imply that three previously proposed methods in the literature, Spectral Attribute Removal (SAL) , Mean Projection , and Fair PCA , are guaranteed to achieve linear guardedness given suitable hyperparameters. See Appendix D for further discussion.

### Derivation of LEACE

Theorem 4.1 is a very weak condition, which is far from identifying unique values for \(\) and \(\). In most applications, however, we'd like to make a "small" edit to \(\) so that useful information contained in \(\) is maximally preserved. We operationalize the notion of a small edit in terms of the mean squared norm \(\|r()-\|_{}^{2}\) defined by some positive-definite inner product \(\),4 which can be thought of as a local quadratic approximation to _any_ measure of divergence between \(\) and \(r()\) (such as Kullback-Leibler divergence, for example). While we are primarily interested in the Euclidean (\(=\)) and Mahalanobis (\(=_{}^{+}\)) norms, it will turn out that there is a _single_ erasure function that minimizes _all_ such norms simultaneously. We will see in Section 6 that ensuring edits are small in this sense provides substantial benefit to downstream task performance as compared to other methods which also guard the labels \(\).

Below, we derive the optimal eraser under the assumption that \(\) and \(\) are centered.

**Theorem 4.2**.: _Let \(\) and \(\) be centered random vectors taking values in \(^{d}\) and \(^{k}\) respectively, each of finite second moment. Let \(^{d d}\) be a p.s.d. matrix defining a (possibly degenerate) inner product on \(^{d}\): \(,_{}=^{T} \). Let \(_{}^{d d}\) be \(\)'s covariance matrix, and \(_{}^{d k}\) be the cross-covariance matrix of \(\) and \(\). Let \(^{+}\) denote the Moore-Penrose pseudoinverse of a matrix \(\), and let \(^{1/2}\) be the p.s.d. square root of a p.s.d. matrix \(\). Then the objective_

\[*{argmin}_{^{d d}}[ \|-\|_{}^{2}] (,)=\]

_has the following solution:_

\[^{*}=-^{+}_{_{}},\]

_where \(\) is the whitening transformation \((_{}^{1/2})^{+}\) and \(_{_{}}=(_{})(_{})^{+}\) is the orthogonal projection matrix onto \((_{})\)._

Proof.: See Appendices E.1 and E.2 for two independent proofs of Theorem 4.2. 

The above theorem assumes that the random vectors \(\) and \(\) are centered, and does not include a bias term. Below we extend our results to the uncentered case, and derive the optimal bias \(^{*}\).

**Theorem 4.3**.: _Let \(\) and \(\) be random vectors taking values in \(^{d}\) and \(^{k}\) respectively, each of finite second moment. Define \(\) and \(^{*}\) as in Theorem 4.2 and \(^{*}=[]-^{*}[]\). Then \((^{*},^{*})\) minimizes \(+-^{2}\), subject to \((+,)=\)._

Proof.: Let \(^{d d}\) and define \(}=-[]\) and \(=[]+-[]\). Then,

\[+- _{}^{2} =(}-})+_{}^{2}\] \[=}-}_{}^{2}+2}- }^{T}+^{T} \] \[=}-}_{}^{2}+^{T},\]

where we have eliminated the middle term because \(\) is linear and \([}]=0\). Since \(\) is p.s.d., our objective is minimized for \(=\), i.e. \(=[]-[]\). The problem thus reduces to choosing \(\) so as to minimize \(}-}_{ }^{2}\) subject to \((+,)=( },)=\), which Theorem 4.2 shows occurs when \(=^{*}\). 

[MISSING_PAGE_FAIL:6]

compare our intervention with RLACE , which uses gradient-based optimization to solve a linear concept-erasure adversarial game.

Concept erasure results.First, we evaluate the ability of logistic regression classifiers to recover the removed information. The results, presented in Fig. 2, show that our method is the only to achieve random accuracy (perfect erasure) with a small edit, although RLACE (but not INLP) comes close. At the same time, our method is around 2 orders of magnitude faster, and does not require gradient-based optimization.

### Downstream Fairness

How does our intervention affect the behavior of the model on the main classification task of profession prediction? We fit a logistic regression profession-prediction classifier over the projected [CLS] representations.

To measure the bias in a classifier, we follow De-Arteaga et al.  and use the TPR-GAP measure, which quantifies the bias in a classifier by considering the difference (GAP) in the true positive rate (TPR) between individuals with different protected attributes (e.g. race or gender). We use the notation \(_{z,y}^{}\) to denote the TPR-gap in some main-class label \(y\) (e.g. "unverse prediction) for some protected group \(z\) (e.g. "female"), we also consider \(_{z}^{}\), the RMS of the TPR-gap across all professions for a protected group \(z\):

\[GAP_{z}^{}=_{y C}(GAP_{z,y}^{ })^{2}}\]

To calculate the relation between the bias the model exhibits and the bias in the data, we also calculate \(_{(^{},\%)}\), the correlation between the TPR gap in a given profession and the percentage of women in that profession.

Results.The main-task classifier achieves profession-prediction accuracy of 77.3% on the projected representations (compared with 79.3% over the original representations), indicating that the intervention minimally affects the ability to predict the profession of a person from the representation of their biography. At the same time, the TPR gap drops significantly from 0.198 to 0.084, indicating a sharp drop in the biased behavior of the profession classifier. Indeed, inspecting the correlation \(_{(^{},\%)}\) between the gap (per profession) and the representation of women in

Figure 3: The correlation between \(GAP_{female,y}^{TPR}\) and the relative proportion of women in profession \(y\), for BERT representation, before (left; R=0.867) and after (right; R=0.392) the projection.

Figure 2: Gender prediction accuracy after bias-removal projection versus the mean squared distance from the original representation for INLP, RLACE, and LEACE on BERT representations.

this profession, we see that this correlation plummets from 0.867 to 0.392 after erasure. Re-fitting the main-task logistic regression classifier over the projected representations yields a slightly higher main-task accuracy of 78.1%, at the price of significantly increasing the TPR gap to 0.158.5

### Revisiting Amnesic Probing

Elazar et al.  have introduced the idea of _amnesic probing_ as a causal intervention that aims to test the importance of a given concept (e.g. part-of-speech tag) to some main task (e.g. language modeling). They applied Iterative Nullspace Projection (INLP) to remove different concepts from the hidden representations of the model, and assessed the degree to which its behavior changed when performing masked language modeling. Since INLP often requires dozens of iterations to completely erase the concept, its usage in this context raises concerns of collateral damage due to magnitude of the intervention and the non-exhaustive nature of INLP removal. Here, we replicate their experiments on the bert-base-uncased model with our interventions.

Experimental setup.We use part-of-speech (POS) tags as our concept of interest. We collect sentences and their coarse POS tags ("Noun", "Verb" etc.; 18 in total) from the English Universal Dependencies dataset . We tokenize the sentences with the BERT tokenizer and map each wordpiece to the POS tag of the word to which it belongs. We collect the unmasked BERT representations for each layer, intervene to linearly erase the POS concept from that layer, and continue the forward pass until the last layer, from which we compute the distribution of the MLM over the vocabulary. Note that in each experiment we intervene on a single layer. We quantify the decrease in accuracy following the intervention, as well as the increase in the loss. We compare with a baseline intervention of a random orthogonal projection whose null space has the same rank as the label space (18). For INLP, we perform 20 iterations. This is needed because INLP does not effectively remove the concept; even after 20 iterations, classification accuracy is above majority accuracy. As a result, INLP reduces the rank of the representation by 360. By contrast, our method decreases the rank just by 17.

Results.The results are shown in Fig. 3(b). Our intervention only mildly changes BERT LM accuracy and loss until layer 8, with the highest drop recorded in layer 11. INLP, in contrast, shows maximum effect at layer 6. Since it removes hundreds of dimensions, it is difficult to attribute this effect to the erasure of the concept. These results suggest that the _causal_ effect of the POS concept on the language model is concentrated in layer 11. Interestingly, this stands in contrast with POS linear probing results, which are optimal at earlier layers . As Elazar et al.  have noted, probing does not generally correlate with intervention-based analysis techniques.

Figure 4: Amnesic probing results on bert-base-uncased.

## 6 Concept Scrubbing

Unfortunately, Elazar et al.  were forced to limit their interventions to a single layer due to the limitations of INLP. INLP often requires the deletion of several dozen dimensions before linear guarding is achieved--as demonstrated in Figure 2. Kumar et al.  show empirically and theoretically that INLP causes needless "collateral damage" to useful parts of the representation that are orthogonal to the concept being erased. Because of this collateral damage, it's impossible to apply INLP to multiple layers of a transformer without causing its outputs to collapse into gibberish.

Instead, we would like to erase all linear information about a concept in _every_ intermediate representation, which we term **concept scrubbing**. LEACE makes concept scrubbing possible and eminently practical. It causes minimal collateral damage, induces little computational overhead, and the covariance statistics it relies on can be computed in a _streaming_ fashion, without ever storing all the hidden states in memory or on disk.

**Algorithm.** Any intervention on the model at layer \(\) changes the distribution of hidden states at layers \(^{}>\). Because of this, the naive approach of independently fitting LEACE parameters \((,)\) for all layers of the clean model, then applying them all at once, may fail to fully erase the target concept. Instead, we fit LEACE parameters _sequentially_, starting from the first layer and proceeding to the final layer. After we compute \((,)\) for a layer, we immediately use them to scrub the hidden states for that layer, then feed these scrubbed representations to the next layer (Algorithm 1).

### Experimental Details

**Dataset.** For each model family, we use a sample from the respective pretraining distribution: the validation split of the Pile  for the Pythia models , and the RedPajama replication of the LLaMA pretraining corpus for the LLaMA family . sampling a slice of \(2^{22}\) tokens for fitting the LEACE parameters and another slice of \(2^{22}\) tokens for evaluation. Since neither corpus comes with part-of-speech tags, we use the model from the SpaCy library  to automatically generate Universal Dependency tags .

**Baseline method.** We also run concept scrubbing using full-rank SAL , which is similar to our method but lacks a bias term and does not adjust for correlations between features (Appendix D).

**Architecture.** We focus on autoregressive language models. We evaluate our method on EleutherAI's Pythia 160M, 1.4B, 6.9B, and 12B models , and Meta's LLaMA 7B, 13B, and 30B . We apply concept erasure to the input of each transformer block, immediately after normalization is applied (LayerNorm or RMSNorm).

    &  &  \\  Condition & 7B & 13B & 30B & 160M & 1.4B & 6.9B & 12B \\  No intervention & 0.69 & 0.66 & 0.62 & 0.90 & 0.70 & 0.64 & 0.62 \\ Random erasure & 0.69 & 0.66 & 0.62 & 0.99 & 0.72 & 0.66 & 0.63 \\  LEACE & 1.73 & 1.84 & 1.96 & 2.79 & 2.25 & 3.57 & 3.20 \\ SAL & 3.24 & 3.26 & 3.16 & 3.53 & 3.44 & 4.17 & 4.69 \\  unigram entropy & 2.90 & 2.90 & 2.90 & 2.66 & 2.66 & 2.66 & 2.66 \\   

Table 1: Perplexity in autoregressive language models when removing linearly available part-of-speech information from the input to each transformer layer. Units are bits per UTF-8 byte. The unigram baseline assigns probabilities to tokens based only on their frequency and not on the context.

**Randomized erasure.** Almost any intervention on a neural network will cause its performance to degrade to some extent. Following Elazar et al. , we isolate the effect of the concept erasure by comparing it to a control condition in which we orthogonally project onto a _random_ linear subspace of the same rank as the cross-covariance matrix. To reduce the variance of our results, we sample a fresh subspace for each minibatch, and erase that subspace at each layer, reporting the cross-entropy loss averaged over subspaces.

**Training efficiency.** Algorithm 1 avoids redundant computation by caching the layer \(i\) hidden states for _every_ data point, then using them to run layer \(i+1\). This approach has the downside of requiring a large amount of memory or disk space during training (up to 500GB in our experiments). It's possible to avoid caching any hidden states and instead recompute them as needed, at the expense of increasing the total compute cost from \(O()\) to \(O(^{2})\).

### Results

We find strong evidence that autoregressive language models heavily rely on linearly encoded part-of-speech information. While erasing a randomly selected subspace has little to no effect on language modeling performance, scrubbing away part-of-speech information induces a large increase in perplexity across all models (Table 1).

The specific numbers, however, depend on the erasure method used: SAL induces significantly larger increases in perplexity for all models we tested. We take this to mean that SAL inflicts more collateral damage on other useful features in the representation than LEACE does. In other words, interventions made with LEACE are more _surgical_ than those made with prior work; they more closely approximate the ideal of a perfect intervention which only erases the target concept and keeps everything else fixed [40; 15]. If this experiment were conducted with SAL alone, we would have _overestimated_ the causal effect of part-of-speech.

## 7 Limitations and Future Work

Much work remains to be done to validate concept scrubbing. Specifically, we'd like to see experiments that target concepts much narrower than part-of-speech, and use behavioral metrics to determine whether scrubbing changes the network in the ways we'd intuitively expect. If these experiments succeed, an exciting next step would be the incorporation of concept scrubbing into the pretraining and/or finetuning process. This may make it possible to train deep neural networks subject to _conceptual constraints_. It remains to be seen if gradient-based optimizers will be able to "circumvent" such constraints by learning completely nonlinear representations of protected attributes.

In this work, we focused exclusively on _linear_ concept erasure due to its simplicity and tractability. Some authors have proposed nonlinear concept erasure techniques based on kernel methods, but have found that erasure functions fit using one kernel do not generalize well to other kernels [33; 37]. We conjecture that it is intractable to nondestructively edit X so as to prevent a general nonlinear adversary from recovering Z, unless the data generating process for X is known in detail.6

A major motivation of concept erasure is that it promises to prevent models from using a concept in a _post hoc_, model-agnostic fashion. But if our concept scrubbing procedure turns out to yield unsatisfactory results in practical use cases, the most promising research direction might then be to improve model-_specific_ techniques, such as those that modify the training procedure [8; 9; 14].

## 8 Acknowledgements

We are grateful to CoreWeave for providing the compute resources used in Section 6. Shauli Ravfogel is grateful to be supported by the Bloomberg Data Science PhD Fellowship.