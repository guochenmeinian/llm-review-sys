# The Impact of Initialization

on LoRA Finetuning Dynamics

 Soufiane Hayou

Simons Institute

UC Berkeley

hayou@berkeley.edu

&Nikhil Ghosh

Dept of Statistics

UC Berkeley

nikhil_ghosh@berkeley.edu

Bin Yu

Dept of Statistics

UC Berkeley

binyu@berkeley.edu

###### Abstract

In this paper, we study the role of initialization in Low Rank Adaptation (LoRA) as originally introduced in Hu et al. . Essentially, to start from the pretrained model as initialization for finetuning, one can either initialize \(B\) to zero and \(A\) to random (default initialization in PEFT package), or vice-versa. In both cases, the product \(BA\) is equal to zero at initialization, which makes finetuning _starts_ from the pretrained model. These two initialization schemes are seemingly similar. They should in-principle yield the same performance and share the same optimal learning rate. We demonstrate that this is an _incorrect intuition_ and that the first scheme (initializing \(B\) to zero and \(A\) to random) on average yields better performance compared to the other scheme. Our theoretical analysis shows that the reason behind this might be that the first initialization allows the use of larger learning rates (without causing output instability) compared to the second initialization, resulting in more efficient learning of the first scheme. We validate our results with extensive experiments on LLMs.

## 1 Introduction

The pretrain-finetune paradigm (e.g., [7; 9]) has revolutionized deep learning, replacing task-specific models trained from scratch with finetuning of pretrained base models. These base models, trained on generic unsupervised objectives, learn powerful features that can be rapidly adapted to downstream tasks. The most effective models are consistently the largest ones [14; 25], with state-of-the-art models reaching hundreds of billions of parameters. While many such models are openly available (e.g., Llama by Touvron et al. ), full finetuning remains computationally prohibitive for most practitioners. This has led to parameter-efficient finetuning methods, including adapters , prompt tuning , and \((IA)^{3}\).

Low Rank Adaptation (LoRA)  has emerged as a leading parameter-efficient method, training only low-rank adapter matrices added to pretrained weights, typically using Adam . LoRA often matches or exceeds full-finetuning performance [35; 39], though it may underperform on complex generation tasks. While prior work has examined rank  and learning rate  hyperparameters, initialization schemes remain understudied. This work provides experimental and theoretical justification for choosing between seemingly equivalent initialization approaches.

In standard LoRA training, one of the two LoRA matrices is initialized with random values and the other is initialized to zero (see Section 2.1). Recently, in Meng et al.  the authors proposed an alternative initialization scheme to LoRA which uses the top singular vectors of the pretrained weights as opposed to a random initialization and showed improved training on several tasks. To further improve LoRA training with quantization, Li et al.  introduced a new method called LoftQ for computing a better initialization for quantized training . However, to the best of our knowledge,there has not been any study concerning the random initialization in vanilla LoRA. Specifically, it is not clear from prior work _which of the two LoRA matrices should be initialized to be zero_. Empirical results by Zhu et al.  suggested that the two initialization schemes mentioned above yield similar performance, but it is not clear if the learning rate was well-tuned for each initialization scheme. Our findings suggest that these two initialization schemes lead to fundamentally different finetuning dynamics, and that one of these schemes generally yields better result compared to the other.

LoRA Variations.Beyond altering the LoRA initialization scheme, there have been a series of works which try to address limitations of vanilla LoRA using different variations. To further reduce the number of trainable parameters, LoRA-FA  freezes the \(A\) matrix which leads to small performance loss while reducing memory consumption by up to 1.4\(\). The performance of this training scheme is also investigated in Zhu et al. . VeRA  freezes random weight tied adapters and learns vector scalings of the internal adapter activations. LoRA-XS  initializes the \(A\) and \(B\) matrices using the SVD of the pretrained weights and trains a low-rank update of the form \(BRA\) where \(R\) is a trainable \(r r\) matrix and \(B\), \(A\) are fixed. NOLA  parametrizes the adapter matrices to be linear combinations of frozen random matrices and optimizes the linear coefficients of the mixtures. VB-LORA  shares adapter parameters using a global vector bank. In order to improve the learning ability for more challenging finetuning tasks, Kalajdzievski  proposes a scaling rule for the scalar adapter multiplier to unlock increased gains with higher adapter ranks. MoRA  learns high-rank updates while still preserving parameter efficiency by applying hand-designed compress and decompress operations before and after a trainable adapter matrix. DoRA  decomposes the pretrained weight into magnitude and direction components to allow for better training dynamics.

Contributions.We study the impact of Initialization in LoRA through a theory of large width for neural networks. The core approach is to take the width of a neural network to infinity and determine how the behavior of the limit depends on the choice of the hyperparameters, such as the learning rate and initialization. This approach allows to derive principled scaling choices for these hyperparameters such that desired properties (e.g. stable feature learning) are achieved as the network size grows (see Appendix A.2 for more details). Examples of the infinite-width limit include works on initialization (e.g. He et al. ), and training dynamics (e.g. ). Examples for the depth limit include initialization strategies [6; 10; 30], and depth scaling (see e.g. [18; 23; 28; 29; 37; 41]). A similar strategy was used to derive scaling rules for the LoRA learning rate in Hayou et al.  (LoRA\(+\)) that concluded that the learning rates for different LoRA matrices should be scaled differently to ensure optimal feature learning. In this work we use the same approach to provide a systematic comparison between two different random initialization schemes for vanilla LoRA finetuning (using the same learning rate for the \(A\) and \(B\) matrices). Using the notation Init[A] to refer to the case where \(A\) is initialized to random and \(B\) to zero (as in ) and Init[B] for the opposite, we show that Init[A] and Init[B] lead to fundamentally different training dynamics (as shown in Figure 1):

1. Init[A] allows the use of larger learning rates compared to Init[B]
2. Init[A] leads to 'internal instability' where the features \(Az\) (for some input \(z\)) are large but LoRA output \(BAz\) is small. This form of instability allows more efficient feature learning. We identify a _feature learning / stability tradeoff_ in this case.

Figure 1: Summary of our contributions in this paper: a description of the difference between the finetuning dynamics when LoRA weights \(A\) and \(B\) are initialized with Init[A] or Init[B].

3. Init[B] does not cause any instabilities but training is suboptimal (\(B\) is undertrained).
4. Empirical results confirm the theory and show that Init[A] generally leads to better performance than Init[B].

## 2 Setup and Definitions

We consider a general neural network model of the form

\[Y_{in}(x)=W_{in}x,\\ Y_{l}(x)=_{l}(W_{l},Y_{l-1}(x)),\;l[L],\\ Y_{out}(x)=W_{out}Y_{L}(x),\] (1)

where \(x^{d}\) is the input, \(L 1\) is the network depth, \((_{l})_{l[L]}\) are mappings that define the layers, and \(W_{l}^{n n}\) are the hidden weights, where \(n\) is the network _width_, and \(W_{in},W_{out}\) are input and output embedding weights.1 This model will represent the pretrained model that will later be finetuned on some new task.

To finetune a (large) pretrained model with a limited amount of computational resources, a popular resource efficient approach is to use the LoRA finetuning method defined below.

**Definition 1** (Low Rank Adapters (LoRA) from ).: _To apply LoRA to a weight matrix \(W^{n_{1} n_{2}}\) in the model, we constrain its update in the fine-tuning process by representing the latter with a low-rank decomposition \(W=W^{*}+BA\). Here, only the weight matrices \(B^{n_{1} r}\), \(A^{r n_{2}}\) are trainable and the original pretrained weights \(W^{*}\) remain frozen. The rank \(r(n_{1},n_{2})\) and \(\) are tunable constants._

As the width \(n\) grows,2 the network initialization scheme and the learning rate should be adapted to avoid numerical instabilities and ensure efficient learning. For instance, the variance of the initialization weights (in hidden layers) should scale like \(1/n\) to prevent the pre-activations from blowing up as we increase model width \(n\) (e.g., He initialization ). To derive proper scaling rules, a principled approach consist of analyzing the statistical properties of key quantities in the model (e.g. second moment of the pre-activations) as \(n\) grows and then adjust the initialization variance, the learning rate, and the architecture to achieve desirable properties in the limit \(n\)[5; 10; 13; 40]. We use this approach to study the effect of initialization on the feature learning dynamics of LoRA in the infinite-width limit. For more details about the theory of scaling of neural networks, see Appendix A.2.

Throughout the paper, we will be using asymptotic notation to describe the behaviour of several quantities as the width \(n\) grows. Note that the width \(n\) will be the only scaling dimension of neural network training which grows and all other scaling dimensions such as the LoRA rank \(r\), number of layers \(L\), sequence length, number of training steps, etc., will be considered as fixed. We use the following notation for the asymptotic analysis.

Notation.Given sequences \(c_{n}\) and \(d_{n}^{+}\), we write \(c_{n}=(d_{n})\), resp. \(c_{n}=(d_{n})\), to refer to \(c_{n}< d_{n}\), resp. \(c_{n}> d_{n}\), for some constant \(>0\). We write \(c_{n}=(d_{n})\) if both \(c_{n}=(d_{n})\) and \(c_{n}=(d_{n})\) are satisfied. For vector sequences \(c_{n}=(c_{n}^{i})_{1 i k}^{k}\) (for some \(k>0\)), we write \(c_{n}=(d_{n})\) when \(c_{n}^{i}=(d_{n}^{i})\) for all \(i[k]\), and same holds for other asymptotic notations. Finally, when the sequence \(c_{n}\) is a vector of random variables, convergence is understood to be convergence in second moment (\(L_{2}\) norm).

### Initialization of LoRA Adapters

The standard way to initialize trainable weights is to take an iid initialization of the entries \(A_{ij}(0,_{A}^{2}),B_{ij}(0,_{B}^ {2})\) for some \(_{A},_{B} 0\) (this includes initialization with zeros if \(_{B}\) or \(_{A}\)are set to \(0\)).3. Due to the additive update structure of LoRA, we want to initialize the product \(BA\) to be \(0\) so that finetuning starts from the pretrained model . This can be achieved by initializing one of the weights \(A\) and \(B\) to \(0\). If both are initialized to \(0\), no learning occurs in this case since this is a saddle point and the parameter gradients will remain zero. Thus, we should initialize one of the parameters \(A\) and \(B\) to be non-zero and the other to be zero. If we choose a non-zero initialization for \(A\), then following standard initialization schemes (e.g., He Init , LeCun Init ), one should set \(_{A}^{2}=(n^{-1})\) to ensure \(Ax\) does not explode for large \(n\). This is justified by the Central Limit Theorem (CLT). On the other hand, if we choose a non-zero initialization for \(B\), one should make sure that \(_{b}^{2}=(r^{-1})=(1)\). This leaves us with two possible initialization schemes:

* **Init[A]**: \(_{B}^{2}=0,_{A}^{2}=(n^{-1})\) (default initialization in LoRA ).
* **Init[B]**: \(_{B}^{2}=(r^{-1})=(1),_{A}^{2}=0\).4 
These two initialization achieve the goal of starting finetuning from the pretrained model. A priori, it is unclear if there is a material difference between the two initialization schemes. Surprisingly, as we will show later in this paper, these two initialization schemes lead to fundamentally _different training dynamics_ when model width is large.

### LoRA Features

Notation.For a given LoRA layer in the network, we use Z to denote the input to that layer and \(\) for the output after adding the pretrained weights. More precisely, we can write the layer operation as \(=W^{*}+BA\,\).

Our main analysis relies on a careful estimation of the magnitude of several quantities involving _LoRA features_. Let us first give a formal definition.

**Definition 2** (LoRA Features).: _Given a general neural architecture and a LoRA layer (Definition 1), we define LoRA features \((Z_{A},Z_{B})\) as \(Z_{A}=AZ\), and \(Z_{B}=BZ_{A}=BA\). At fine-tuning step \(t\), we use the superscript \(t\) to denote the value of LoRA features \(Z_{A}^{t},Z_{B}^{t}\), and the subscript \(t\) to denote the weights \(A_{t},B_{t}\)._

## 3 LoRA Finetuning Dynamics in the Large Width Limit

We fix the LoRA rank \(r\) throughout the analysis and examine the finetuning dynamics in the limit of large width. This setup aligns well with practical scenarios where the rank is much smaller than the width (i.e., \(r n\) ). Typically, for Llama models the rank \(r\) is generally of order \(2^{k}\) for \(k\{2,,6\}\), and model width \(n\) is generally larger than \(2^{12}\). We will refer to a layer of the network to which LoRA is applied (see Definition 1) as a _LoRA layer_. For the theoretical analysis, we adopt a simplified setting that facilitates a rigorous yet intuitive derivations of the results.

### Simplified Setting

The following simplified setup was considered in Hayou et al.  to derive asymptotic results concerning the learning rates in LoRA. We use the same setup in our analysis to investigate the impact of initialization.

Finetuning Dataset.We assume that the dataset used for finetuning consists of a single datapoint \((x,y)\),5 and the goal is to minimize the loss calculated with the model with adjusted weights \(W^{*}+BA\) for all LoRA layers (here \(=\{A,B,\}\)). Z\({}^{t}\) is the input to the LoRA layer, computed with data input \(x\). Similarly, we write \(d^{t}\) to denote the gradient of the loss function with respect to the layer output features \(\) evaluated at data point \((x,y)\).

Single LoRA Module.Given a LoRA layer, LoRA feature updates are not only driven by the change in the \(A,B\) weights, but also the changes in \(},d\) which are updated as we finetune the model (assuming there are multiple LoRA layers). To isolate the contribution of individual LoRA layers to feature learning, we assume that only a _single LoRA layer is trainable_ and all other LoRA layers are frozen.6 For this LoRA layer the layer input \(}\) is fixed and does not change with \(t\), whereas \(d\) changes with step \(t\) (because \(^{t}=(W^{*}+B_{t}A_{t})}\)). After step \(t\), \(Z_{B}\) is updated as follows

\[ Z_{B}^{t}= Z_{_{t}^{1}}^{t}}_{_{t}^ {1}}+Z_{A}^{t-1}}_{_{t}^{2}}+ Z_{}^{t}}_{_{t}^{2}}.\] (2)

As discussed in Hayou et al. , the terms \(_{t}^{1},_{t}^{2}\) represent 'linear' feature updates that we obtain if we fix one weight matrix and only train the other. The third term \(_{t}^{3}\) represents the'multiplicative' feature update which captures the compounded update due to updating both \(A\) and \(B\).

### Stability and Feature Learning

Hayou et al.  introduced the notion of stability of LoRA features as width grows. We introduce here a slightly more relaxed notion of stability.

**Definition 3** (Feature Stability).: _We say that LoRA finetuning is stable if for all LoRA layers in the model, and all training steps \(t\), we have \(},Z_{B}=(1),\) as the width \(n\) goes to infinity._

Here, feature stability implies that LoRA output \(Z_{B}\) remains bounded (in \(L^{2}\) norm) as width grows. To achieve such stability, hyperparameters (initialization, learning rate) should be scaled as \(n\) grows. We will show that the dependence of the optimal learning rate on \(n\) is highly sensitive to the choice of initialization (Init[A] or Init[B]).

Note that feature stability also requires that \(}=(1)\) which is directly related to pretraining dynamics since it depends on some pretrained weights \(W^{*}\). We assume that pretraining parameterization (how initialization and learning rate are parametrized w.r.t width) ensures this kind of stability (see Appendix A for more details).7

As discussed above, feature updates are driven by the terms \((_{t}^{i})_{i\{1,2,3,\}}\). As \(n\) grows, these feature updates might become trivial (i.e. vanish as \(n\)) or unstable (i.e. grows unbounded). To avoid such scenarios, we want to ensure that \( Z_{B}=(1)\). Such conditions are the main ideas behind \(\) and Depth-\(\), which are network parametrizations that ensure stability and feature learning in the large width and depth limits for pretraining. We recall this definition from .

**Definition 4** (Feature Learning).: _We say that LoRA finetuning induces stable feature learning in the limit of large width if the dynamics are stable (Definition 3), and for all finetuning steps \(t\), we have \( Z_{B}^{t}}{{=}}Z_{B}^{t+1}-Z_{B}^{t}=(1)\)._

\( Z_{B}\) is the sum of the terms \(_{t}^{i}\)'s (Equation (2)). To achieve optimal feature learning, we want to ensure that \(_{t}^{1}=(1)\) and \(_{t}^{2}=(1)\) which means that both weight matrices \(A\) and \(B\) are efficiently updated and contribute to the update in \(Z_{B}\). An intuitive explanation is provided in Appendix A.1. This leads us to the following definition of efficient learning with LoRA.

**Definition 5** (Efficient Learning with LoRA).: _We say that LoRA fine-tuning is efficient if it is stable (Definition 3), and for all LoRA layers in the model, and all fine-tuning steps \(t>1\), we have_

\[_{t}^{i}=(1), i\{1,2\}.\]

Next, we introduce the \(\)-operator, an essential tool in our analysis of the large width dynamics.

### Introduction to the \(\)-operator

In the theory of scaling, one usually tracks the asymptotic behavior of key quantities as we scale some model ingredient. For instance, if we scale the width \(n\) of a neural network, we are interested in quantifying how certain quantities in the network behave as \(n\) grows. This is a standard approach for (principled) model scaling and it has so far been used to derive scaling rules for initialization , activation function , network parametrization , amongst other things.

With Init[A] and Init[B], initialization weights are of order \((n^{-})\) for some \( 0\). Assuming that the learning rate also scales polynomialy with \(n\), it is straightforward that preactivations, gradients, and weight updates are all asymptotically polynomial in \(n\). Note that this is only possible because all neural computations consists of sums of \((n^{})\) terms, where typically \(\{0,1\}\). For instance, when calculating the features \(A\)Z, each entry is a sum of \(n\) terms, while when calculating \(BZ_{A}\), each entry is a sum of \(r\) terms (\(r\) fixed as \(n\) goes to infinity). This is true for general neural computation that can be expressed as Tensor Programs .

Consequently, for some quantity \(v\) in the computation graph, it is natural to track the exponent that determines the asymptotic behavior of \(v\) with respect to \(n\). We write \(v=([v])\) to capture this polynomial dependence. Elementary operations with the \(\)-operator include:8

Zero.When \(v=0\), we write \([v]=-\) (as a limit of \([n^{-}]\) when \(\)).

Multiplication.Given two real-valued variables \(v,v^{}\), we have \([v v^{}]=[v]+[v^{}]\).

Addition.Given two real-valued variables \(v,v^{}\), we _generally_ have \([v+v^{}]=([v],[v^{}])\). The only case where this is violated is when \(v^{}=-v\). This is generally a zero probability event if \(v\) and \(v^{}\) are random variables that are not perfectly (negatively) correlated, which is the case in most situations where we make use of this formula. See Appendix A.3 for discussion.

We have now introduced all required notions for the subsequent analysis. For better readability, we defer all the proofs to the appendix.

### Recursive formulas

Using the \(\)-operator, we can track the asymptotic behavior of the finetuning dynamics as model width \(n\) grows. At finetuning step \(t\), the weights are updated as follows

\[A_{t}=A_{t-1}- g_{A}^{t-1}, B_{t}=B_{t-1}- g_{B}^{t-1},\]

where \(g_{A},g_{B}\) are processed gradients (e.g. normalized gradients with momentum as in AdamW). We assume that the gradients are processed in a way that makes their entries \((1)\). This is generally satisfied in practice (with Adam for instance) and has been considered in  to derive the \(\)-parametrization for general gradient processing functions. From this, we obtain the following recursive formulas for \([Z_{A}^{t}]\) and \([B_{t}]\), which characterizes their behavior in the large width limit.

**Lemma 1** (Informal).: _For \(t\) fixed, the asymptotic dynamics of \(Z_{A}^{t}\) and \(B_{t}\) follow the recursive formula_

\[[Z_{A}^{t}] =([Z_{A}^{t-1}],[]+1)\] (3) \[[B_{t}] =([B_{t-1}],[]).\]

The formal proof of Lemma 1 is provided in Appendix A and relies on Assumption 1 which fairly represents practical scenarios (see Appendix A for a detailed discussion). Lemma 1 captures the change in asymptotic behavior of quantities \(Z_{A}^{t}\) and \(B_{t}\) as width grows. Naturally, the dynamics depend on the the initialization scheme which lead to completely different behaviors as we show in the next two results.

### Init[A] leads to more efficient feature learning but suffers "internal" instability

Next, we provide a precise characterization of stability and feature learning when using Init[A].

**Theorem 1** (Informal).: _For \(t\) fixed, with Init[A] and learning rate \(\), we have_* _Stability:_ \(Z_{B}^{t}=(1)\) _if and only if_ \([]-1/2\)_._
* _Feature Learning:_ \( Z_{B}^{t}=(1)\) _if and only if_ \([]=-1/2\)_. In this case, we also have_ \(_{1}^{1},_{t}^{2}=(1)\) _(efficient feature learning, Definition_ 5_)._

_Moreover, "internal" instability (\(Z_{A}^{t}=(1)\)) occurs when \([](-1,1/2]\)._

With Init[A], the maximal learning rate9 that does not lead to instability in \(Z_{B}\) scales as \((n^{-1/2})\). This can be seen as an asymptotic form of the edge of stability phenomenon  where if we increase the learning rate beyond some level, instability occurs. Interestingly, in this case (i.e. with \((n^{-1/2})\) learning rate) the features are efficiently updated (Definition 5). However, this comes with caveat: the features \(Z_{A}^{t}\) grow as \((n^{1/2})\) which can potentially cause numerical instabilities. We call this phenomenon _internal instability_: only the features \(Z_{A}\) (internal LoRA features) grows, LoRA output \(Z_{B}\) remains \((1)\) in this case.

The fact that \((n^{-1/2})\) is the maximal learning rate that does not cause instability in \(Z_{B}\) does not mean it is the _optimal_ learning rate. As the width \(n\) grows, this internal instability in \(Z_{A}\) will become more and more problematic. Intuitively, we expect that a trade-off appears in this case: the optimal learning rate (found by grid search) to be larger than \((n^{-1})\) but smaller than \((n^{-1/2})\), i.e. the network will try to achieve a balance between optimal feature learning (\([]=-1/2\)) and internal stability \(Z_{A}^{t}=(1)\) (\([]=-1\)). We verify this empirically in the next section.

### Init[B] leads to suboptimal feature learning with internal stability

In the next result, we show that the maximal learning rate allowed with Init[B] is different from that with Init[A], leading to completely different dynamics.

**Theorem 2** (Informal).: _For \(t\) fixed, with Init[B], we have_

* _Stability:_ \(Z_{B}^{t}=(1)\) _if and only if_ \([]-1\)_._
* _Feature Learning:_ \( Z_{B}^{t}=(1)\) _if and only if_ \([]=-1\)_._

_Moreover, efficient feature learning cannot be achieved with Init[B] for any choice of learning rate scaling \([]\) (that does not violate the stability condition). More precisely, with \((n^{-1})\) learning rate, the limiting dynamics (when \(n\)) are the same if \(B\) was not trained and \(A\) is trained._

With Init[B], the maximal learning rate (that does not violate stability) scales as \((n^{-1})\) (for any \(>0\), a learning rate of \((n^{-1+})\) leads to \(Z_{B}=(1)\)).

Because of this bound on the maximal learning rate, no internal instability occurs with Init[B]. In this case, feature learning is suboptimal since the \(B\) weight matrix is undertrained in the large width limit (\(_{t}^{2} 0\)).

Conclusions from Sections 3.5 and 3.6.The results of Theorem 1 and Theorem 2 suggest that Init[A] allows the use of _larger learning rates_ compared to Init[B], which might lead to better feature learning and hence better performance at the expense of some internal instability. Here, 'larger' learning rate should be interpreted in asymptotic terms: with Init[A] the maximal learning rate that does not cause instability satisfies \([]=-1/2\). With Init[B], we have \([]=-1\) instead. Note that because of the constants in \((n^{})\) learning rates (for some \(\)), the optimal learning rate with Init[A] is not systematically larger than Init[B] for _finite width_. However, as width grows, we will see that it is case.

Another important insight from this analysis is that with both initializations, the dynamics are sub-optimal in the limit: internal instability with Init[A] and undertraining of \(B\) with Init[B].10 We will later discuss possible solutions to this behavior.

### Toy Model

To validate our theory in a controlled setting, we consider the following simple model:

\[Y_{in}=W_{in}x,\\ Y_{h}=Y_{in}+(W_{h}+BA)(Y_{in})\\ Y_{out}=W_{out}(Y_{h})\] (4)

where \(W_{in}^{n d},W_{h}^{n n},W_{out} ^{1 n}\), and \(B,A^{}^{r n}\).

We generate synthetic data from the teacher model using the following config: \(d=5,r_{teacher}=20,n=1000,N=1000\) (train data size), and \(N_{test}=100\) (test data size). The weight \(W_{in}^{teacher},W_{out}^{teacher},A^{teacher}\), and \(B^{teacher}\) are randomly initialized, and \(W_{h}^{teacher}=0\).11 We train student models with \(d=5,r=4\), and varying widths \(n\{2^{k}, k=7,,13\}\).12

Optimal Learning Rate.We finetune model (4) on synthetic data generated from the teacher model. In Figure 2, we show the optimal learning rate when using either Init[A] or Init[B] to initialize the finetuning, as a function of width \(n\). For \(n 1\) (typically \(n 2^{9}\)), the optimal learning rate with Init[A] is larger than the optimal learning rate with Init[B]. This is in agreement with the theoretical results obtained in Theorem 1 and Theorem 2 which predict asymptotic maximal learning rates (that satisfy the stability condition) of \((n^{-1/2})\) and \((n^{-1})\) respectively.

With Init[A], we observe the stability/feature learning trade-off for large \(n\). The optimal learning rate with Init[A] in this regime (e.g. \(n=2^{13}\)) is smaller than the maximal theoretical learning

Figure 3: Evolution of the norms of the \(Z_{A},Z_{B}\) features, averaged over training data. We compute the average \(|Z_{A}|}{{=}}N^{-1}_{i=1}^{N}\|Z_{A}(x_{i})\|\) (and same for \(Z_{B}\)), where the \(x_{i}\)’s are the training data. The dynamics are shown for widths \(n=128\) and \(n=8192\), two seeds, and for both Init[A] and Init[B]. Train loss and the (optimal) learning rate are shown on top of each plot. We observe that the magnitude of \(Z_{A}\) is significantly higher with Init[A] compared to Init[B] at large width (\(n=8192\)). Interestingly, the train loss is smaller with Init[A], as compared to Init[B]. Results with other seeds and widths are shown in Appendix B.

Figure 2: Optimal Learning rate for the finetuning of synthetic model Equation (4) with Init[A] and Init[B] as initialization. The optimal LRs are shown as a function of width \(n\). Theoretical lines \(n^{-1}\) and \(n^{-1/2}\) are shown as well (constants \(C_{1},C_{2}\) are chosen to provide suitable trend visualization). As model width \(n\) grows, the optimal learning rate with Init[A] becomes larger than the optimal learning rate with Init[B]. This is in agreement with the theoretical results.

rate \(n^{-1/2}\) that achieves optimal feature learning (Theorem 1). Here, the model seems to balance the internal instability that occurs in the \(Z_{A}\) features with feature learning and thus favors smaller learning rates: the optimal learning rates is smaller than \((n^{-1/2})\) and larger than \((n^{-1})\).

Internal Instability and Feature Learning.Figure 3 shows the average magnitude of \(Z_{A}\) and \(Z_{B}\) for Init[A] and Init[B] for widths \(n\{128,8192\}\). With Init[A], the magnitude of \(Z_{A}\) features seem to grow with width, hence trading off internal stability for more efficient feature learning. This behavior is consistent across random seeds as shown in the figure, and as further confirmed by experiments in Appendix B. The train loss is consistently smaller with Init[A], which can be explained by the fact that Init[A] allows more efficient feature learning at the cost of some internal instability. This flexibility cannot be achieved with Init[B]. Note also that \(Z_{B}\) features tends to get smaller with \(n\) with Init[A] as predicted by theory: the trade-off between internal instability and feature learning implies that \(^{*}=o(n^{-1/2})\), which implies that \(Z_{B}^{t}=o(1)\), i.e. the \(Z_{B}\) features vanish as width grows. While this might problematic, it only becomes an issue when the width is extremely large: if the optimal learning rate scales as \((n^{-})\) for some \((1/2,1)\) (so that the learning rate is between \((n^{-1})\) and \((n^{-1/2})\), balancing internal instability and efficient feature learning), the LoRA output feature scales as \(Z_{B}=B_{t}A_{t}=(n^{-+1})\). Therefore, if \( 0.7\) for instance, the vanishing rate of LoRA output feature is \(Z_{B}(n^{-0.3})\) which is slow given the order of magnitude of width in practice (for \(n=2^{12}\), we have \(n^{-0.3} 0.08\)).

## 4 Experiments with Language Models

Our theoretical results from earlier provides a detailed asymptotic analysis of the finetuning dynamics when LoRA modules are initialized with Init[A] or Init[B]. The main conclusions are that Init[A] generally leads to more efficient feature learning (which can be justified by the fact that optimal learning rate is larger when using Init[A] compared to when using Init[B]). To provide evidence of this claim on real-world tasks, we use LoRA to finetune a set of language models on different benchmarks. Details about the experimental setup and more empirical results are provided in Appendix B. We use LoRA+ code  for our experiments (available at https://github.com/nikhil-ghosh-berkeley/loraplus).

### GLUE tasks with RoBERTa

The GLUE benchmark (General Language Understanding Evaluation) consists of several language tasks that evaluate the understanding capabilities of language models . Using LoRA, we finetune Roberta-large from the RoBERTa family  on MNLI, SST2, and QNLI tasks with varying learning rates \(\) and initialization schemes (Init[A] or Init[B]). We use the same experimental setup of  for Roberta-Large to compare our results with theirs (see Appendix B for more details).

The results in Figure 4 are aligned with our theory: we observe that Init[A] generally leads to better performance, and the optimal learning rate with Init[A] is generally larger than with Init[B]. Models initialized with Init[A] match the performances reported in , while those initialized with Init[B] generally underperform that baseline. For MNLI task (the hardest one amongst the

Figure 4: Test Accuracy for RoBERTa-Large finetuned on GLUE tasks. The results are shown after convergence of finetuning with LoRA, initialized with either Init[A] or Init[B]. Models were finetuned using LoRA rank \(r=8\) and FP16 precision. Optimal learning rate and corresponding accuracy are shown on top of each panel for both initializations. The experimental setup is provided in Appendix B.

three tasks), we observe a significant difference in the best test accuracy (over 3 random seeds) with \(90.69\) with Init[A] and \(89.47\) with Init[B]. We also observe that for MNLI, the optimal learning rate with Init[A] (\(^{*}=\) 8e-5) is much larger than the optimal learning rate with Init[B] (\(^{*}=\) 1e-5), which aligns with our theoretical predictions. However, note that for QNLI for instance (an easier task), while the optimal test accuracy is significantly better with Init[A], the optimal learning rate (from the grid search) is the same for Init[A] and Init[B]. There are many possible explanations for this: 1) the width is not large enough in this case to see the gap between optimal learning rates (for RoBERTa-Large, the width is \(n=2^{10}\)) 2) The constants in \((n^{-1})\) are \((n^{-1/2})\) are significantly different in magnitude due to dependence on finetuning task. We notice similar behaviour with LLama experiments below. A precise analysis of this observation is beyond the scope of this paper, we leave it for future work.

### Llama

To further validate our theoretical findings on more modern models and datasets, we report the results of finetuning the Llama-7b model  on the Flan-v2 dataset  and the GSM8k dataset , and finetuning the TinyLlama model  on WikiText-2 using LoRA. Each trial is averaged over two seeds and the shaded region indicates one standard error. In the left panel of Figure 5 we see that when finetuning TinyLlama using LoRA the optimal learning rate using Init[A] is larger than with Init[B] and the corresponding test perplexity is lower. Similarly, for the center panel of Figure 5, when finetuning the Llama-7b model on Flan-v2, the optimal learning rates for Init[A] and Init[B] are the same (for the learning rate grid we used), but the the optimal MMLU accuracy for Init[A] is slightly higher than for Init[B]. For learning rates close to the optimal choice, the accuracy using Init[A] is generally higher than for Init[B]. An analogous result holds for the GSM8k dataset as shown in the rightmost panel of Figure 5. More details about this setting are provided in Appendix B.

## 5 Conclusion and Limitations

We showed that LoRA dynamics are highly sensitive to initialization. Init[A] is associated with larger optimal learning rates, compared to Init[B]. Larger learning rates typically result in better performance, as confirmed by our empirical results. Note that this is a zero-cost adjustment with LoRA finetuning: _we simply recommend using Init[A] instead of Init[B]_.

One limitation of our work is that we only define feature learning via the magnitude of feature updates in the limit of large width. In this way, our definition of feature learning is data-agnostic and therefore no conclusion about generalization can be obtained with this analysis. The constants in \((.)\) asymptotic notation naturally depend on the data (the finetuning task) and therefore such data-agnostic approach does not allow us to infer any information about the impact of the data on the finetuning dynamics.

_More importantly_, our results indicate that both initialization schemes lead to suboptimal scenarios, although Init[A] allows more efficient feature learning. In both cases, instability and/or suboptimal feature learning present fundamental issues, which can potentially be mitigated by approaches such as LoRA+ . Understanding the interaction of LoRA+ and related efficiency methods with the initialization scheme is an important question for future work.

Figure 5: **(Left)** Test perplexity (lower is better) of TinyLlama LoRA on WikiText-2 with Init[A] and Init[B]. **(Center)** MMLU accuracy of Llama-7b LoRA finetuned on the Flan-v2 dataset. **(Right)** GSM8k test accuracy of Llama-7b LoRA finetuned on the GSM8k dataset. More experimental details are provided in Appendix B.

Acknowledgement

We thank Gradient AI for cloud credits under the Gradient AI fellowship awarded to SH and thank AWS for cloud credits under an Amazon Research Grant awarded to the Yu Group. We also gratefully acknowledge partial support from NSF grants DMS-2209975, 2015341, 20241842, NSF grant 2023505 on Collaborative Research: Foundations of Data Science Institute (FODSI), the NSF and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and 814639, and NSF grant MC2378 to the Institute for Artificial CyberThreat Intelligence and OperatioN (ACTION).