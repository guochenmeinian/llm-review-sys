# Understanding Contrastive Learning via

Distributionally Robust Optimization

 Junkang Wu\({}^{1}\) Jiawei Chen\({}^{2}\) Jiancan Wu\({}^{1}\) Wentao Shi\({}^{1}\) Xiang Wang\({}^{1}\) Xiangnan He\({}^{1}\)

\({}^{1}\)University of Science and Technology of China

\({}^{2}\)Zhejiang University

{jkwu0909, wujcan, xiangwang1223, xiangnanhe}@gmail.com,

sleepyhunt@zju.edu.cn, shiwentao123@mail.ustc.edu.cn

 Jiawei Chen and Xiangnan He are the corresponding authors.

\({}^{2}\)Xiang Wang is also affiliated with Institute of Artificial Intelligence, Institute of Dataspace, Hefei Comprehensive National Science Center.

###### Abstract

This study reveals the inherent tolerance of contrastive learning (CL) towards sampling bias, wherein negative samples may encompass similar semantics (_e.g.,_ labels). However, existing theories fall short in providing explanations for this phenomenon. We bridge this research gap by analyzing CL through the lens of distributionally robust optimization (DRO), yielding several key insights: (1) CL essentially conducts DRO over the negative sampling distribution, thus enabling robust performance across a variety of potential distributions and demonstrating robustness to sampling bias; (2) The design of the temperature \(\) is not merely heuristic but acts as a Lagrange Coefficient, regulating the size of the potential distribution set; (3) A theoretical connection is established between DRO and mutual information, thus presenting fresh evidence for "InfoNCE as an estimate of MI' and a new estimation approach for \(\)-divergence-based generalized mutual information. We also identify CL's potential shortcomings, including over-conservatism and sensitivity to outliers, and introduce a novel Adjusted InfoNCE loss (ADNCE) to mitigate these issues. It refines potential distribution, improving performance and accelerating convergence. Extensive experiments on various domains (image, sentence, and graphs) validate the effectiveness of the proposal. The code is available at https://github.com/junkangwu/ADNCE.

## 1 Introduction

Recently, contrastive learning (CL) [1; 2; 3] has emerged as one of the most prominent self-supervised methods, due to its empirical success in computer vision [4; 5; 6; 7; 8], natural language processing [9; 10] and graph [11; 12]. The core idea is to learn representations that draw positive samples (_e.g.,_ augmented data from the same image) nearby and push away negative samples (_e.g.,_ augmented data from different images). By leveraging this intuitive concept, unsupervised learning has even begun to challenge supervised learning.

Despite its effectiveness, recent study  has expressed concerns about sampling bias in CL. Negative samples are often drawn uniformly from the training data, potentially sharing similar semantics (_e.g.,_ labels). Blindly separating these similar instances could result in a performance decline. To address sampling bias, existing research has proposed innovative solutions that either approximate the optimal distribution of negative instances [13; 14] or integrate an auxiliary detection module to identify false negative samples . Nevertheless, our empirical analyses challenge the prevailing understanding of CL. Our findings indicate that CL inherently exhibits robustness towardssampling bias and, by tuning the temperature \(\), can even achieve comparable performance with representative methods for addressing sampling bias (_e.g.,_ DCL  and HCL ). This revelation prompts two intriguing questions: _1) Why does CL display tolerance to sampling bias? and 2) How can we thoroughly comprehend the role of the temperature \(\)?_ Despite recent analytical work on CL from various perspectives [13; 14; 16], including mutual information and hard-mining, these studies fail to provide a satisfactory explanation for our observations.

To address this research gap, we investigate CL from the standpoint of Distributionally Robust Optimization (DRO) [17; 18; 19; 20; 21; 22]. DRO refers to a learning algorithm that seeks to minimize the worst-case loss over a set of possible distributions. Through rigorous theoretical analysis, we prove that CL essentially performs DRO optimization over a collection of negative sampling distributions that surround the uniform distribution, constrained by a KL-divergence-based measure (we term the DRO-type equivalent expression of CL as CL-DRO). By enabling CL to perform well across various potential distributions, DRO equips it with the capacity to alleviate sampling bias. Furthermore, our findings emphasize that the design of \(\) is not merely heuristic but acts as a Lagrange Coefficient, regulating the size of the potential distribution set. By integrating the principles of DRO, we can also gain insights into the key properties of CL such as hard-mining, and investigate novel attributes such as variance control.

Furthermore, we delve into the properties of CL-DRO, transcending the boundary of KL divergence, but exploring a broader family of distribution divergence metrics -- \(\)-divergence. It exhibits greater flexibility and potentially superior characteristics compared to KL-divergence. Interestingly, we establish the equivalence between CL-DRO under any \(\)-divergence constraints and the corresponding tight variational representation of \(\)-divergence . This discovery provides direct theoretical support for the assertion that "_InfoNCE is an estimate of mutual information (MI)_", without requiring redundant approximation as before work [24; 3]. Additionally, it provides a novel perspective for estimating arbitrary \(\)-divergence, consequently enabling the estimation of \(\)-divergence-based mutual information.

In addition, we discover some limitations in CL through the lens of DRO. One is that the DRO paradigm is too conservative, focusing on worst-case distribution with giving undvisedly over-large weights on hard negative samples. Secondly, the presence of outliers can significantly impact DRO , resulting in both a drop in performance and training instability. In fact, both challenges stem from the unreasonably worst-case distribution weights assigned to negative samples. To address this weakness, we propose **AD**justed Info**NCE** (ADNCE) that reshapes the worst-case distribution without incurring additional computational overhead, allowing us to explore the potentially better negative distribution. Empirical studies show that our method can be applied to various domains (such as images, sentences, and graphs), and yield superior performance with faster convergence.

## 2 Related Work

**Contrastive Learning.** With numerous CL methodologies (_e.g.,_ SimCLR , Moco , BYOL , SwAV , Barlow Twins ) having been proposed and demonstrated the ability to produce high-quality representations for downstream tasks, theoretical research in this field has begun to emerge. Wang et al.  revealed the hard-mining property and Liu et al.  emphasized its robustness against dataset imbalance. Meanwhile, Xue et al.  showed that representations learned by CL provably boosts robustness against noisy labels, and Ko et al.  investigated the relationship between CL and neighborhood component analysis. More closely related to our work is the result of Tian et al.  that analyzed CL from minmax formulation.

**Distributionally Robust Optimization.** In contrast to conventional robust optimization approaches [32; 33; 34], DRO aims to solve the distribution shift problem by optimizing the worst-case error in a pre-defined uncertainty set, which is often constrained by \(\)-divergence [20; 17], Wasserstein distance [21; 22; 19; 18] and shape constraints [35; 36; 37]. Meanwhile, Michel et al. [38; 39] parametrized the uncertainty set, allowing more flexibility in the choice of its architecture. Zhai et al.  focused on issues related to the existing DRO, such as sensitivity to outliers, which is different from the aforementioned research.

**Mutual Information.** The MI between two random variables, \(X\) and \(Y\), measures the amount of information shared between them. To estimate MI, the Donsker-Varadhan target  expresses the lower bound of the Kullback-Leibler (KL) divergence, with MINE  being its parameterizedversion. However, according to studies [41; 42], the MINE estimator may suffer from high variance. To address this issue, InfoNCE  extends the unnormalized bounds to depend on multiple samples, thereby eliminating the constraints of both MINE's reliance on conditional and marginal densities. Another line of MI focuses on boundedness and low variance. Towards this end, RPC  introduced relative parameters to regularize the objective and FLO  leveraged Fenchel-Legendre optimization. Nevertheless, they are still hard to explain _how MI optimization elucidates the properties of CL and facilitates effective CL in practice._

## 3 Contrastive learning as Distributionally Robust Optimization

In this section, we firstly uncover the tolerance of CL towards sampling bias. Then we analyze CL and \(\) from the perspective of DRO. Finally, we empirically verify the above findings.

**Notation.** Contrastive Learning is conducted on two random variables \(X,Y\) over a feature space \(\). Suppose we have original samples \(x\), which obey the distribution \(P_{X}\), positive samples drawn from the distribution \((x,y^{+}) P_{Y|X}\), and negative samples selected from the distribution \((x,y) P_{Y}\). The goal of CL is to learn an embedding \(g_{}:^{d}\) that maps an observation (_i.e.,_\(x,y,y^{+}\)) into a hypersphere. Let \(f(g_{}(x),g_{}(y))\) be the similarity between two observations \(x\) and \(y\) on the embedding space. CL draws positive samples closer (_i.e.,_\(f(g_{}(x),g_{}(y^{+}))\) for \((x,y^{+}) P_{XY}\)) and pushes the negative samples away (_i.e.,_\(f(g_{}(x),g_{}(y))\) for \((x,y) P_{X}P_{Y}\)). For brevity, in this work, we simply shorten the notations \(P_{Y|X}\) as \(P_{0}\), and \(P_{Y}\) as \(Q_{0}\). Also, we primarily focus on InfoNCE loss \(_{}\) as a representative for analyses, while other CL's loss functions exhibit similar properties. InfoNCE 3 can be written as follow:

\[_{}=-_{P_{X}}_{P_{0}}[f_{}(x,y^{+})/]-_{Q_{0}}[e^{f_{}(x,y) /}]=-_{P_{X}}_{P_{0}}(x,y^{+})/}}{_{Q_{0}}[e^{f_{}(x,y)/}]}.\] (1)

### Motivation

In practical applications of contrastive learning (CL), negative samples \((y)\) are typically drawn uniformly from the training data, which might have similar semantics (for instance, labels). This introduces a potential issue of sampling bias, as posited by Chuang et al. , since similar samples are being forcefully separated. Recent research [15; 13] observed that compared to the employment of ideal negative sampling that selects instances with distinctly dissimilar semantics, the existence of sampling bias could result in a significant reduction in performance.

In this study, we observe an intriguing phenomenon where CL inherently demonstrates resilience to sampling bias. We empirically evaluate CL on two benchmark datasets, CIFAR10 and STL10, with the findings detailed in Table 1. Notably, we make the following observations: 1) By fine-tuning the temperature \(\), basic SimCLR demonstrates significant improvement, achieving performance levels comparable to methods specifically designed to address sampling bias (_i.e.,_ SimCLR(\(^{*}\)) versus SimCLR(\(_{0}\)), DCL and HCL); 2) With an appropriately selected \(\), the relative improvements realized by DCL and HCL are marginal. These insights lead us to pose two compelling questions: 1) _Why does CL exhibit tolerance to sampling bias?_ and 2) _What role does \(\) play, and why is it so important?_

### Understanding CL from DRO

In this subsection, we first introduce DRO and then analyze CL from the perspective of DRO.

    &  &  \\   & Top-1 & \(\) & Top-1 & \(\) \\  SimCLR(\(_{0}\)) & 91.10 & 0.5 & 81.05 & 0.5 \\ SimCLR(\(^{*}\)) & 92.19 & 0.3 & 87.91 & 0.2 \\  DCL(\(_{0}\)) & 92.00 (\(\)0.2\%) & 0.5 & 84.26 (\(\)4.2\%) & 0.5 \\ DCL(\(^{*}\)) & 92.09 (\(\)0.1\%) & 0.3 & 88.20 (\(\)1.0\%) & 0.2 \\  HCL(\(_{0}\)) & 92.12 (\(\)0.0\%) & 0.5 & 87.44 (\(\)0.5\%) & 0.5 \\ HCL(\(^{*}\)) & 92.10 (\(\)0.0\%) & 0.3 & 87.46 (\(\)0.5\%) & 0.2 \\   

Table 1: **InfoNCE has the ability to mitigate sampling bias.** We compare the performance of various CL methods with/without fine-tuning \(\) (marked with \(^{*}\)/\(_{0}\)). We also report relative improvements compared with SimCLR(\(^{*}\)).

### A. Distributionally Robust Optimization (DRO).

The success of machine learning heavily relies on the premise that the test and training distributions are identical (_a.k.a._ _iid_ assumption). How to handle different data distributions (_a.k.a._ distribution shift) poses a great challenge to machine learning systems. Fortunately, Distributionally Robust Optimization (DRO) provides a potential solution to mitigate this problem. DRO aims to minimize the worst-case expected loss over a set of potential distributions \(Q\), which surround the training distribution \(Q_{0}\) and are constrained by a distance metric \(\) within a specified robust radius \(\). Formally, DRO can be written as:

\[_{}=_{Q}_{Q}[(x;)] D_{}(Q||Q_{0}),\] (2)

where \(D_{}(Q||Q_{0})\) measures the \(\)-divergence between two distributions, and \((x;)\) denotes the training loss on input \(x\). Intuitively, models incorporating DRO enjoy stronger robustness due to the presence of \(Q\) that acts as an "adversary", optimizing the model under a distribution set with adversarial perturbations instead of a single training distribution.

**B. Analyzing CL from DRO.** We first introduce the CL-DRO objective:

**Definition 3.1** (CL-DRO).: Let \(P_{0}\) and \(Q_{0}\) be the distributions of positive and negative samples respectively. Let \(\) be the robust radius. CL-DRO objective is defined as:

\[_{}^{}=-_{P_{X}}_{P_{0 }}[f_{}(x,y^{+})]-_{Q}_{Q}[f_{}(x,y)] D_{}(Q||Q_{0}).\] (3)

The objective CL-DRO can be understood as the DRO enhancement of the basic objective \(L_{}=-_{P_{X}}_{P_{0}}[f_{}(x,y^ {+})]-_{Q_{0}}[f_{}(x,y)]\), which aims to increase the embedding similarity between the positive instances and decreases that of the negative ones. CL-DRO improves \(L_{}\) by incorporating DRO on the negative side, where \(f_{}(x,y)\) is optimized a range of potential distributions. Consequently, CL-DRO equips the model with resilience to distributional shifts of negative samples.

**Theorem 3.2**.: _By choosing KL divergence \(D_{KL}(Q||Q_{0})= Q}dx\), optimizing CL-DRO (cf. Equation (3)) is equivalent to optimizing CL (InfoNCE,cf. Equation (1)):_

\[_{}^{}= -_{P_{X}}_{P_{0}}[f_{}(x,y^{+})] -_{ 0,}_{Q}\{_{Q}[f_{}(x,y)]- [D_{KL}(Q||Q_{0})-]+(_{Q_{0}}[}]-1)\} \] \[= -_{P_{X}}_{P_{0}}^{*}() (x,y^{+})/^{*}()}}{_{Q_{0}}[e^{f_{ }(x,y)/^{*}()}]}+Constant\] \[= ^{*}()_{}+Constant,\] (4)

_where \(,\) represent the Lagrange multipliers, and \(^{*}()\) signifies the optimal value of \(\) that minimizes the Equation (4), serving as the temperature \(\) in CL._

The proof is presented in Appendix A.1. Theorem 3.2 admits the merit of CL as it is equivalent to performing DRO on the negative samples. The DRO enables CL to perform well across various potential distributions and thus equips it with the capacity to alleviate sampling bias. We further establish the connection between InfoNCE loss with the unbiased loss that employs ideal negative distribution \(L_{}=-_{P_{X}}_{P_{0}}[f_{}(x,y^{+})]-_{Q^{}}[f_{}(x,y)]\).

**Theorem 3.3**.: _[Generalization Bound] Let \(}_{}\) be an estimation of InfoNCE with \(N\) negative samples. Given any finite hypothesis space \(\) of models, suppose \(f_{}[M_{1},M_{2}]\) and the ideal negative sampling distribution \(Q^{}\) satisfies \(D_{KL}(Q^{}||Q_{0})\), we have that with probability at least \(1-\):_

\[_{}}_{}+ (,N,),\] (5)

_where \((,N,)=((M_{2}-M_{1})/)}{N-1+((M_{2}-M _{1})/)}(|}{})}\)._

As we mainly focus on model robustness on negative distribution, here we disregard the constant term present in Equation (4), and omit the error from the positive instances. The proof is presented inAppendix A.2. Theorem 3.3 exhibits the unbiased loss is upper bound by InfoNCE if a sufficiently large data size is employed (considering \((,N,) 0\) as \(N 0\)). This bound illustrates how \(\) impacts the model performance, which will be detailed in the next subsection.

### Understanding \(\) from DRO

Building on the idea of DRO, we uncover the role of \(\) from the following perspectives:

**A. Adjusting robust radius.** The temperature \(\) and the robust radius \(\) conform to:

**Corollary 3.4**.: _[The optimal \(\) - Lemma 5 of Faury et al. ] The value of the optimal \(\) (i.e., \(\)) can be approximated as follow:_

\[_{Q_{0}}[f_{}(x,y)]/2},\] (6)

_where \(_{Q_{0}}[f_{}(x,y)]\) denotes the variance of \(f_{}(x,y)\) under the distribution \(Q_{0}\)._

This corollary suggests that the temperature parameter \(\) is a function of the robust radius \(\) and the variance of \(f_{}\). In practical applications, tuning \(\) in CL is essentially equivalent to adjusting the robustness radius \(\).

The aforementioned insight enhances our understanding of the impact of temperature on CL performance. An excessively large value of \(\) implies a small robustness radius \(\), potentially violating the condition \(D_{KL}(Q^{}||Q_{0})\) as stipulated in Theorem 3.2. Intuitively, a large \(\) delineates a restricted distribution set in DRO that may fail to encompass the ideal negative distribution, leading to diminished robustness. Conversely, as \(\) decreases, the condition \(D_{KL}(Q^{}||Q_{0})\) may be satisfied, but it expands the term \((,N,)\) and loosens the generalization bound. These two factors establish a trade-off in the selection of \(\), which we will empirically validate in Subsection 3.4.

**B. Controlling variance of negative samples.**

**Theorem 3.5**.: _Given any \(\)-divergence, the corresponding CL-DRO objective could be approximated as a mean-variance objective:_

\[_{}^{}-_{P_{X}}_ {P_{0}}[f_{}(x,y^{+})]-_{Q_{0}}[f_{}(x,y)]+ {1}{2}(1)}_{Q_{0}}[f_{}(x,y)] ,\] (7)

_where \(^{(2)}(1)\) denotes the the second derivative value of \(()\) at point 1, and \(_{Q_{0}}[f_{}]\) denotes the variance of \(f\) under the distribution \(Q_{0}\)._

_Specially, if we consider KL divergence, the approximation transforms:_

\[_{}^{}-_{P_{X}} _{P_{0}}[f_{}(x,y^{+})]-_{Q_{0}}[f_{}( x,y)]+_{Q_{0}}[f_{}(x,y)].\] (8)

The proof is presented in Appendix A.4. Theorem 3.5 provides a _mean-variance_ formulation for CL. Compared to the basic objective \(L_{}\), CL introduces an additional variance regularization on negative samples, governed by the parameter \(\), which modulates the fluctuations in similarity among negative samples. The efficacy of variance regularizers has been extensively validated in recent research [47; 20; 48]. We consider that variance control might be a reason of the success of CL.

**C. Hard-mining.** Note that the worst-case distribution in CL-DRO can be written as \(Q^{*}=Q_{0}(x,y^{+})/]}{_{Q_{0}}[f_{ }(x,y)/]}\) (refer to Appendix A.6). This implies that the model in CL is optimized under the negative distribution \(Q^{*}\), where each negative instance is re-weighted by \((x,y^{+})/]}{_{Q_{0}}[f_{}(x,y)/ ]}\). This outcome vividly exhibits the hard-mining attribute of CL, with \(\) governing the degree of skewness of the weight distribution. A smaller \(\) suggests a more skewed distribution and intensified hard-mining, while a larger \(\) signifies a more uniform distribution and attenuated hard-mining. Our research yields similar conclusions to those found in [16; 28], but we arrive at this finding from a novel perspective.

### Empirical Analysis

In this section, we conduct the following experiments to verify the above conclusions.

\(\) varies with the level of sampling bias.** To study the effect of \(\) on model performance, we conduct experiments on the CIFAR10 dataset and manipulate the ratio of false negative samples based on the ground-truth labels. A ratio of \(1\) implies retaining all false negative instances in the data, while a ratio of \(0\) implies eliminating all such instances. The detailed experimental setup can refer to the Appendix B. From Figure 1, we make two observations: 1) There is an evident trade-off in the selection of \(\). As \(\) increases from 0.2 to 1, the model's performance initially improves, then declines, aligning with our theoretical analysis. 2) As the ratio of false negative instances increases (\(r\) ranges from 0 to 1), the optimal \(\) decreases. This observation is intuitive -- a higher ratio of false negative instances implies a larger distribution gap between the employed negative distribution and the ideal, thereby necessitating a larger robust radius \(\) to satisfy the condition \(D_{KL}(Q^{}||Q_{0})\). As Corollary 3.4, a larger robust radius corresponds to a smaller \(\).

\(\) **controls the variances of negative samples.** We experiment with an array of \(\) values and track the corresponding variance of \(f_{}(x,y)\) for negative samples and the mean value of \(f_{}(x,y^{+})\) for positive samples in CIFAR10. As illustrated in Figure 2, we observe a decrease in the variance of negative samples and an increase in the mean value of positive samples as \(\) diminishes. Although this observation might be challenging to interpret based on existing studies, it can be easily explained through Equation (8). Note that \(\) governs the impact of the variance regularizer. A smaller \(\) brings a larger variance penalty, resulting in a reduced variance of negative instances. Simultaneously, as the regularizer's contribution on optimization intensifies, it proportionately diminishes the impact of other loss component on enhancing the similarity of positive instances, thereby leading to a lower mean value of positive instances.

**Mean-Variance objective achieves competitive performance.** We replace the InfoNCE with the mean-variance objective (_cf._ Equation (8)) and evaluate the model performance. The results are shown in Table 2. Remarkably,such a simple loss can can deliver competitive results to InfoNCE, thereby affirming the correctness of the approximation in Theorem 3.5. Nevertheless, it marginally underperforms compared to InfoNCE, as it merely represents a second-order Taylor expansion of InfoNCE. However, it is worth highlighting that the mean-variance objective is more efficient.

## 4 Relations among DRO, Constrastive Learning and Mutual Information

Through extensive theoretical analysis, we have far demonstrated that CL is fundamentally a CL-DRO objective that conduct DRO on negative samples. And it is well known that CL is also equivalent to mutual information estimation. A natural question arises: _Is there a connection between CL-DRO and MI, as both are theoretical understanding of CL?_

   &  &  \\   & Top-1 & \(\) & Top-1 & \(\) \\  SimCLR(\(_{0}\)) & 91.10 & 0.5 & 81.05 & 0.5 \\ SimCLR(\(^{*}\)) & **92.19** & 0.3 & **87.91** & 0.2 \\  MW & 91.81 & 0.3 & 87.24 & 0.2 \\  

Table 2: **Mean-variance objective (MW) achieves comparable performance.** We replace the loss function with variance penalty on negative samples.

Figure 1: \(\) **varies with the level of sampling bias.** We report the relative top1 accuracy of SimCLR _w.r.t._ different \(\) across different rate \(r\) of false negative samples.

Figure 2: \(\) **controls the variance of negative samples score.** We alter the value of \(\) and record corresponding variance of negative samples and mean value of positive samples.

To make our conclusion more generalized, we would like to transcend the boundary of KL divergence, but exploring a broader family of \(\)-divergence. First of all, we would like to extend the definition of MI to utilize \(\)-divergence:

**Definition 4.1** (\(\)-Mi).: The \(\)-divergence-based mutual information is defined as:

\[I_{}(X;Y)=D_{}(P(X,Y)||P(X)P(Y))=_{P_{X}}[D_{}(P_{0}||Q_{ 0})].\] (9)

Comparing to KL-divergence, \(\)-divergence showcases greater flexibility and may exhibits superior properties. \(\)-MI, as a potential superior metric for distribution dependence, has been studied by numerous work [49; 50].

**Theorem 4.2**.: _For distributions \(P\), \(Q\) such that \(P Q\), let \(\) be a set of bounded measurable functions. Let CL-DRO draw positive and negative instances from \(P\) and \(Q\), marked as \(^{}_{}(P,Q)\). Then the CL-DRO objective is the tight variational estimation of \(\)-divergence. In fact, we have:_

\[D_{}(P||Q)=_{f}-^{}_{}(P,Q) =_{f}_{P}[f]-_{}\{ +_{Q}[^{*}(f-)]\}.\] (10)

_Here, the choice of \(\) in CL-DRO corresponds to the probability measures in \(D_{}(P||Q)\). And \(^{*}\) denotes the convex conjugate._

The proof is presented in Appendix A.5. This theorem establishes the connection between CL-DRO and \(\)-divergence. If we replace \(P\) with \(P_{0}\) and \(Q\) with \(Q_{0}\), and simply hypothesis that the metric function \(f_{}\) has a sufficiently large capacity, we have the following corollary:

**Corollary 4.3**.: \(_{P_{X}}[D_{}(P_{0}||Q_{0})]\) _is a tight variational estimation of \(I_{}(X;Y)\)._

**InfoNCE is a tighter MI estimation.** The prevailing and widely used variational approximation of \(\)-divergences is the Donsker-Varadhan target (\(I_{DV}\)), defined as \(D_{}(P||Q)_{f}\{_{P}[f]-_{ Q}[^{*}(f)]\}\). However, it has been observed by Ruderman et al.  that this expression is loose when applied to probability measures, as it fails to fully consider the fundamental nature of divergences being defined between probability distributions. To address this limitation, let us further assume, as we do for \(\), that \(_{P}=1\). Under this common assumption, we arrive at a more refined formulation: \(D_{}(P||Q)_{f}\{_{P}[f]-_{ }\{+_{Q}[^{*}(f-)]\}\}\). Notably, this result, embracing the infimum over \(\) in the spirit of Ruderman et al. , appears to be consistent with our proof in Theorem 4.2. Hence, we argue that "InfoNCE is a tighter MI estimation."

Furthermore, this corollary provides a rigorous and succinct methodology for deriving InfoNCE from Variational MI. Specifically, given \((x)=x x-x+1\) for the case of KL-divergence, its conjugate \(^{*}(x)=e^{x}-1\) can be computed. Subsequently, by resolving the simple convex problem \(_{}\{+_{Q}[^{*}(]\}\), we derive the optimal choice of \(\), denoted as \(^{*}=_{Q_{0}}[e^{f(x,y)/}]\). Inserting \(^{*}\) into Equation (10), \(D_{}(P_{0}||Q_{0})\) simplifies to \(_{f}\{_{P_{0}}[f(x,y^{+})]-_{Q_{0}}[e^{f(x,y) /}]\}\), which is the minus of the InfoNCE objective.

**DRO bridges the gap between MI and InfoNCE.** Although previous works such as MINE and CPC have demonstrated that InfoNCE is a lower bound for MI, they still have significant limitations . For instance, MINE uses a critic in Donsker-Varadhan target (\(_{DV}\)) to derive a bound that is neither an upper nor lower bound on MI, while CPC relies on unnecessary approximations in its proof, resulting in some redundant approximations. In contrast, Theorem 4.2 presents a rigorous and practical method for accurately estimating the tight lower bound of MI.

**DRO provides general MI estimation.** Although a strict variational bound for MI has been proposed in Poole et al. , their discussion is limited to the choice of KL-divergence. Theorem 4.2 is suitable for estimating various \(\)-MI. For example, if we consider the case of \(^{2}\) divergence, given by \((x)=}(x-1)^{2}\), we could obtain convex conjugate \(^{*}(x)=x+x^{2}\). The variational representation becomes \(I_{^{2}}(X;Y)=D_{^{2}}(P_{0}\,\|\,Q_{0})=_{f}\ \{_{P_{0}}[f(x,y^{+})]-_{Q_{0}}[f(x,y)]- _{Q_{0}}[f(x,y)]\}\), where \(_{Q_{0}}[f(x,y)]\) represents the variance of \(f\) on distribution \(Q\). Our theoretical framework offers the opportunity to estimate flexible \(\)-MI, which can be adapted to suit specific scenarios.

Method

### Shortcomings of InfoNCE

Based on the understanding of CL from DRO perspective, some weaknesses of InfoNCE is revealed:

* **Too conservative.** Distributionally Robust Optimization (DRO) focuses on the worst-case distribution, often leading to an overemphasis on the hardest negative samples by assigning them disproportionately large weights. More precisely, the weights are proportional to \([f_{}(x,y)/]\), causing samples with the highest similarity to receive significantly larger weights. However, in practice, the most informative negative samples are often found in the "head" region (e.g., top-20% region with high similarity) rather than exclusively in the top-1% highest region . Overemphasizing these top instances by assigning excessive weights is typically suboptimal.
* **Sensitive to outliers.** Existing literature  reveals that DRO is markedly sensitive to outliers, exhibiting significant fluctuations in training performance. This is because outliers, once they appear in regions of high similarity, can be assigned exceptionally high weights. Consequently, this phenomenon unavoidably influences the rate of model convergence during the training phase.

### Adnce

Our goal is to refine the worst-case distribution, aiming to assign more reasonable weights to negative instances. The aspiration is to create a flexible distribution that can be adjusted to concentrate on the informative region. This is accomplished by incorporating Gaussian-like weights, as defined below:

\[w(f_{}(x,y),,)}[-((x,y)-}{})^{2}],\] (11)

where \(\) and \(\) are two hyper-parameter we could control. As is illustrated in Figure 3, \(\) controls the central region of weight allocation, whereby samples closer to \(\) have larger weights, while \(\) controls the height of the weight allocation in the central region. Intuitively, a smaller \(\) results in a more pronounced weight discrepancy among the samples.

Inserting the weights into InfoNCE formula, we obtain the novel **AD**justed Info**NCE** (ADNCE):

\[_{}=-_{P}[f_{}(x,y^{+})/]+ _{Q_{0}}[w(f_{}(x,y),,)e^{f_{}(x,y)/}/Z_{ ,}],\] (12)

where \(Z_{,}\) denotes the partition function of \(w(f(x,y),,)\), _i.e._, \(Z_{,}=_{Q_{0}}[w(f_{}(x,y),,)]\).

It is noteworthy that our proposed approach only involves two modified lines of code and introduce no additional computational overhead, as compared to InfoNCE . Pytorch-style pseudocode for ADNCE is given in Appendix.

## 6 Experiments

We empirically evaluate our adjusted InfoNCE method -- ADNCE, and apply it as a modification to respective classical CL models on image, graph, and sentence data. For all experiments, \(\) is treated as a hyper-parameter to adjust the weight allocation, where \(\) is set to \(1\) by default. For detailed experimental settings, please refer to Appendix.

### Image Contrastive Learning

We begin by testing ADNCE on vision tasks using CIFAR10, CIFAR100, and STL10 datasets. We choose SimCLR  as the baseline model (termed as InfoNCE\((_{0})\)). We also include two variants

Figure 3: We visualize the training weight of negative samples _w.r.t._ similarity score. InfoNCE (in BIUE) over-emphasize hard negative samples, while ADNCE utilizes the weight \(w\) (in ORANGE, GREEN, BROWN) to adjust the distribution.

of vanilla InfoNCE for comprehensive comparisons. One approach is applying a grid search for \(\) (termed as InfoNCE(\(^{*}\))), which serves to confirm its importance as discussed in Section 3.4. The other is from Tian el al. , which proposes a novel approach for setting for the value of \(\) (termed as \(\)-CL-direct).

As shown in Table 3, a grid search for \(\) has a crucial impact on the model performance. This impact is significant throughout the entire training process, from the early stage (100 epochs) to the later stage (400 epochs). Additionally, ADNCE exhibits sustained improvement and notably enhances performance in the early stages of training. In contrast, while \(\)-CL-direct introduces a novel approach for setting the value of \(\), its essence remains weighted towards the most difficult negative samples, hence yielding similar performance compared to fine-tuning \(\) using grid search. In Figure 4, we plot the training curve to further illustrate the stable superiority of ADNCE.

  
**Model** & **STS12** & **STS13** & **STS14** & **STS15** & **STS16** & **STS-B** & **SICK-R** & **Avg.** \\  GloVe embeddings (avg.)\(\) & 55.14 & 70.66 & 59.73 & 68.25 & 63.66 & 58.02 & 53.76 & 61.32 \\ BERTbase-flow\(\) & 58.40 & 67.10 & 60.85 & 75.16 & 71.22 & 68.66 & 64.47 & 66.55 \\ BERTbase-whitening\(\) & 57.83 & 66.90 & 60.90 & 75.08 & 71.31 & 68.24 & 63.73 & 66.28 \\ CT-BERTbase\(\) & 61.63 & 76.80 & 68.47 & 77.50 & 76.48 & 74.31 & 69.19 & 72.05 \\  SimCSE-BERTbase(\(_{0}\)) & 68.40 & **82.41** & 74.38 & 80.91 & 78.56 & 76.85 & **72.23** & 76.25 \\ SimCSE-BERTbase(\(^{*}\)) & 71.37 & 81.18 & 74.41 & **82.51** & 79.24 & 78.26 & 70.65 & 76.81 \\ ADNCE-BERTbase & **71.38** & 81.58 & **74.43** & 82.37 & **79.31** & **78.45** & 71.69 & **77.03** \\  SimCSE-RoBERTbase(\(_{0}\)) & **70.16** & 81.77 & 73.24 & 81.36 & 80.65 & 80.22 & 68.56 & 76.57 \\ SimCSE-RoBERTbase(\(^{*}\)) & 68.20 & **81.95** & 73.63 & 81.83 & 81.55 & 80.96 & 69.56 & 76.81 \\ ADNCE-RoBERTbase & 69.22 & 81.86 & **73.75** & **82.88** & **81.88** & **81.13** & **69.57** & **77.10** \\   

Table 4: Sentence embedding performance on STS tasks (Spearman’s correlation, “all” setting). **Bold** indicates the best performance while underline indicates the second best on each dataset. \(\): results from SimCSE ; all other results are reproduced or reevaluated by ourselves.

Figure 4: Learning curve for Top-1 accuracy by linear evaluation on CIFAR10 and STL10.

  
**Model** &  &  &  \\  & 100 & 200 & 300 & 400 & 100 & 200 & 300 & 400 & 100 & 200 & 300 & 400 \\  InfoNCE (\(_{0}\)) & 85.70 & 89.21 & 90.33 & 91.01 & 75.95 & 78.47 & 80.39 & 81.67 & 59.10 & 63.96 & 66.03 & 66.53 \\ InfoNCE (\(^{*}\)) & 86.54 & 89.82 & 91.18 & 91.64 & 81.20 & 84.77 & 86.27 & 87.69 & 62.32 & 66.85 & 68.31 & 69.03 \\ \(\)-CL-direct & 87.65 & 90.11 & 90.88 & 91.24 & 80.91 & 84.71 & 87.01 & 87.96 & 62.75 & 66.27 & 67.35 & 68.54 \\  ADNCE & **87.67** & **90.65** & **91.42** & **91.88** & **81.77** & **85.10** & **87.01** & **88.00** & **62.79** & **66.89** & **68.65** & **69.35** \\   

Table 3: Performance comparisons on multiple loss formulations (ResNet50 backbone, batchsize 256). Top-1 accuracy with linear evaluation protocol. \(_{0}\) means \(=0.5\), \(^{*}\) represents grid searching on \(\) and \(\)-CL-direct is a similar work  on CL theory. Bold is highest performance. Each setting is repeated 5 times with different random seeds.

### Sentence Contrastive Learning

We follow the experimental setup in GraphCL  and conduct evaluations on \(7\) semantic textual similarity (STS) tasks in an unsupervised setting. Specifically, we evaluate the model on STS 2012-2016, STS-benchmark, and SICK-Relatedness. To ensure fair comparisons, we use pre-trained checkpoints of BERT and RoBERTa, and randomly sample sentences from the English Wikipedia.

As observed in Table 4, ADNCE consistently outperforms InfoNCE, achieving an average Spearman's correlation of \(77\%\). The ease of replacing InfoNCE with ADNCE and the resulting substantial performance improvements observed in BERT and RoBERTa serve as testaments to the efficacy and broad applicability of ADNCE. Furthermore, the improvements of \(^{*}\) over \(_{0}\) emphasize the significance of selecting a proper robustness radius.

### Graph Contrastive Learning

To study the modality-agnostic property of ADNCE beyond images and sentences, we conduct an evaluation of its performance on TUDataset . We use a classical graph CL model GraphCL  as the baseline. To ensure a fair comparison, we adhered to the same protocol used in GraphCL , employing techniques such as node dropout, edge perturbation, attribute masking, and subgraph sampling for data augmentation. We replace the vanilla InfoNCE with two different variants, namely GraphCL(\(^{*}\)) obtained through grid search, and our proposed ADNCE.

Table 5 demonstrates that ADNCE outperforms all baselines with a significant margin on four datasets, especially when compared to three state-of-the-art InfoNCE-based contrastive methods, GraphCL, JOAO, and JOAOv2, thereby setting new records on all four datasets. The improvements observed in GraphCL(\(^{*}\)) relative to GraphCL(\(_{0}\)) align with our understanding of DRO.

## 7 Conclusion and Limitations

We provide a novel perspective on contrastive learning (CL) via the lens of Distributionally Robust Optimization (DRO), and reveal several key insights about the tolerance to sampling bias, the role of \(\), and the theoretical connection between DRO and MI. Both theoretical analyses and empirical experiments confirm the above findings. Furthermore, we point out the potential shortcomings of CL from the perspective of DRO, such as over-conservatism and sensitivity to outliers. To address these issues, we propose a novel CL loss -- ADNCE, and validate its effectiveness in various domains.

The limitations of this work mainly stem from two aspects: 1) Our DRO framework only provides a theoretical explanation for InfoNCE-based methods, leaving a crucial gap in the success of CL without negative samples [4; 26]. 2) ADNCE requires weight allocation to be adjusted through parameters and cannot adaptively learn the best reweighting scheme.