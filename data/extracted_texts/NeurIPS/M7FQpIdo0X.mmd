# Enhancing Minority Classes by Mixing: An

Adaptative Optimal Transport Approach for

Long-tailed Classification

 Jintong Gao1, He Zhao2, Zhuo Li3,4, Dandan Guo1\({}^{*}\)

\({}^{1}\)School of Artificial Intelligence, Jilin University \({}^{2}\)CSIRO's Data61

\({}^{3}\)Shenzhen Research Institute of Big Data

\({}^{4}\)The Chinese University of Hong Kong, Shenzhen

gaojt20@mails.jlu.edu.cn he.zhao@ieee.org

221019088@link.cuhk.edu.cn guodandan@jlu.edu.cn

Corresponding author. This work was supported by NSFC (62306125).

###### Abstract

Real-world data usually confronts severe class-imbalance problems, where several majority classes have a significantly larger presence in the training set than minority classes. One effective solution is using mixup-based methods to generate synthetic samples to enhance the presence of minority classes. Previous approaches mix the background images from the majority classes and foreground images from the minority classes in a random manner, which ignores the sample-level semantic similarity, possibly resulting in less reasonable or less useful images. In this work, we propose an adaptive image-mixing method based on optimal transport (OT) to incorporate both class-level and sample-level information, which is able to generate semantically reasonable and meaningful mixed images for minority classes. Due to its flexibility, our method can be combined with existing long-tailed classification methods to enhance their performance and it can also serve as a general data augmentation method for balanced datasets. Extensive experiments indicate that our method achieves effective performance for long-tailed classification tasks. The code is available at https://github.com/JintongGao/Enhancing-Minority-Classes-by-Mixing.

## 1 Introduction

Large-scale balanced datasets play a vital role in the remarkable success of deep neural networks for various tasks. However, datasets in many real-world applications often exhibit unexpected long-tailed distributions where most of the data belongs to several majority classes while the rest spreads across lots of minority classes. The model trained on such a long-tailed dataset will be biased toward majority classes, leading to poor generalizations about minority classes.

Re-weighting [1; 2; 3; 4; 5; 6; 7], over-sampling [8; 9; 10; 11; 12], under-sampling [13; 14; 15; 16], data augmentation [17; 18; 19; 20; 21; 22; 23], two-stage methods [24; 25; 26; 27], and other methods [28; 29] are common solutions to the long-tailed problem. Among them, over-sampling and data augmentation aim to balance the data distribution by oversampling or generating closely related minority classes. As the representative data augmentation techniques, mixup and its variants [30; 31; 32] have performed satisfactorily in the computer vision fields, whose key idea is constructing mixed samples by performing linear interpolations between data/features and corresponding labels. Considering mixup-based methods are designed for balanced data, applying them to long-tailed classification without any adjustments may ignore the specificity of long-tailed data distribution. Recently, some novel mixup-based methods for the long-tailed problem have been explored [22; 33;34, 21]. As a representative example, Context-rich Minority Oversampling (CMO)  generates minority-centric images by mixing the background and foreground images, where background images are mainly from the majority classes while foregrounds are mainly from minority classes. The critical concept of CMO is to paste an image from a minority class (foreground) onto rich-context images from a majority class (background). Despite the effectiveness of CMO, it cannot be guaranteed that all the generated images are realistic and reasonable by randomly mixing the background and foreground images. For example, we visualize mixed samples created by ours and CMO in Fig. 1, where we take "Panda" and "Grey Whale" as two foreground images. CMO generates unreasonable mixed samples since pandas are hardly seen in the sky, the beaches, or the streets, and grey whales rarely appear in the desert, the flowers, or the sky. Therefore, instead of arbitrarily pairing the foreground and background images, it is essential to consider the semantic distances between the foreground and background images to generate more meaningful and advantageous samples.

In this work, we propose a more adaptive image-mixing method based on optimal transport (OT), which is a powerful tool for computing the distance between two distributions under the form of multiple sampling . We consider an empirical distribution \(P_{}\) over the background images and an empirical distribution \(Q_{}\) over the foreground images, where the former are more likely from the majority and the latter from minority classes. To learn the semantic similarity between background and foreground images, we formulate the learning of the similarity as the OT problem between \(P_{}\) and \(Q_{}\). In this way, we can view the learned transport plan as the similarity between foreground and background samples, which can be naturally used as guidance to select the most relevant background image for each foreground image. Due to the importance of cost function in learning the transport plan for OT, we further design the cost function using the sample-level representation and class-level label information. Hence, the rich context of the majority classes as background images can be better transferred to the minority classes as foreground images for providing an adaptive way to learn the similarity between them and generate more semantically meaningful mixed images. Interestingly, our proposed OTmix can not only be combined with existing long-tailed methods but also be used as a new image-mixing strategy for balanced datasets.

Our main contributions are summarized as follows: (1) We propose a novel image-mixing method for generating reasonably mixed samples for long-tailed classification, where we introduce a distribution over the background images mainly from majority classes and another distribution over the foreground images mainly from minority classes. (2) By minimizing the OT distance between these two distributions, we view the learned transport plan as similar in guiding the image-mixing process. (3) We design the cost function based on the feature information and confusion matrix to learn the desired transport plan. Extensive experiments demonstrate the effectiveness of our method for long-tailed classification and also balanced classification.

## 2 Related Work

Data ProcessingLearning relatively balanced classes from the data perspective is an effective solution to the long-tailed problem , which can be roughly divided into over-sampling, under-sampling, and data augmentation. Since our proposed method is related to the over-sampling and data augmentation methods, we describe them in detail below. Over-sampling aims to emphasize the minority classes and increase the instance number of the minority classes . For example, the classical Synthetic Minority Over-sampling Technique (SMOTE)  uses the interpolation between a given minority sample and its nearest minority neighbors to create new samples. Data augmentation is another way of data processing to compensate the minority classes by generating and synthesizing new samples . For example, Major-to-minor Translation (M2m)  defines an optimization phase to augment minority classes via translating samples from majority classes. The Meta Semantic Augmentation (MetaSAug) approach  is proposed to perform effective

Figure 1: Visualization of mixed samples created by CMO and ours(OTmix), respectively.

semantic data augmentation by learning more meaningful class-wise covariance. Sample-Adaptive Feature Augmentation (SAFA)  aims to extract diverse and transferable semantic directions from majority classes, and adaptively translate minority-class features along extracted semantic directions for augmentation.

In addition, mixup-based methods [32; 37; 38; 30; 39; 31; 40; 41; 42], the effective data augmentation methods in balanced datasets, have been developed to solve the long-tailed problem recently and are closely related to our work. For example, Mixup Shifted Label-aware Smoothing Model (MiSLAS)  uses mixup in its Stage-1 training without any adjustments. Rebalanced Mixup (Remix)  provides a disproportionately higher weight to the minority class when mixing two samples. Uniform Mixup (UniMix)  adopts a minority-favored mixing factor to encourage more majority-minority pairs occurrence and is further combined with the Bayes Bias (Bayias) caused by the inconsistency of prior. Targeted copy-paste augmentation  aligned with the novel domain enhances out-of-domain robustness on long-tailed camera trap dataset.  improves minority class performance from synthetic data by contrasting night and day backgrounds. To mitigate the overfitting issue in subpopulation shift, Uncertainty-aware Mixup (UMIX)  adopts the sampling uncertainty to reweight the mixed samples. CMO  constructs mixed samples between the background images more likely from the majority classes and foreground images more likely from the minority classes. However, it ignores the semantic similarity between images by randomly mixing them. Different from CMO, we formulate the image-mixing problem as the OT problem between two distributions and use the learned transport plan as guidance to mix images, providing a general and adaptive image-mixing method for long-tailed learning.

Loss Function EngineeringDesigning effective training objectives is another solution to fight against class imbalance [45; 7; 5]. Label-Distribution-Aware Margin Loss (LDAM)  is proposed from the view of the generalization error bound, and LDAM-DRW adopts a deferred class-level re-weighting method. Balanced Softmax (BALMS)  accommodates the label distribution shift between training and testing and proposes a Meta Sampler that learns to re-sample training set by meta-learning. Label Distribution Disentangling (LADE)  loss disentangles the source label distribution from the model prediction based on the optimal bound of the Donsker-Varadhan representation. Besides, Focal Loss  determines the weights for samples with the sample difficulty in the object detection task, and Influence-balanced Loss (IB)  assigns different weights to samples according to their influence on a decision boundary. Our method can be naturally combined with many loss functions.

Other MethodsRecently, two-stage algorithms have been proposed [1; 5; 24], such as MiSLAS . Meanwhile, a Bilateral Branch Network (BBN)  unifies the representation and classifier learning stages to form a cumulative learning strategy. RoutIng Diverse Experts (RIDE)  uses multiple experts to reduce the variance and bias of the long-tailed classifier. In addition, some approaches employ meta-learning, such as MetaSAug  and BALMS . Causal Norm (CN)  pinpoints the causal effect of momentum and extracts the unbiased direct impact of each instance.

Optimal TransportAs a powerful tool, OT has been applied to generative models [47; 48; 49; 50; 51], computer vision [52; 53; 7; 54; 55; 56; 57], text analysis [58; 59; 60; 61], and etc. To the best of our knowledge, there are still very limited works for the imbalanced classification by means of OT. For example,  proposes a re-weighting method based on OT, and  introduces Optimal Transport via Linear Mapping (OTLM) to perform the post-hoc correction. Different from them, ours falls into the data augmentation group by mixing images. Another recent work is Optimal Transport for OverSampling (OTOS) , which moves random points from a prior uniform distribution into that of minority class samples based on OT. However, our proposed OTmix introduces OT to guide the mix between majority and minority images, which leverages the rich context of the majority classes guided by OT. Alignmixup  adopts OT to align two images and interpolate between two sets of features in a standard classification task, which is distinct from ours in terms of task and technical detail.

## 3 Background

Long-tailed ClassificationGiven a training set \(\!=\!\{(x_{i},y_{i})\}_{i=1}^{N}\) for a multi-class problem with \(K\) classes, if each class \(k\) contains \(n_{k}\) samples, we have that \(_{k=1}^{K}n_{k}\!=\!N\). Without loss of generality, we can always assume \(n_{1} n_{2} n_{K}\) for the long-tailed problem. Denotethe model parameterized with \(\) as \(g()\), which is trained on \(\) with the well-known Empirical Risk Minimization (ERM) algorithm  ignores such class imbalance and performs poorly on the minority classes .

CutMixHere, we use the CutMix  to mix samples because of its simplicity and effectiveness, which replaces the image region with a patch from another training image. Denote \(x^{W H C}\) and \(y\) denote a training image and its label, respectively. CutMix combines two training samples \((x_{i},y_{i})\) and \((x_{j},y_{j})\) and generates the new sample \((_{ij},_{ij})\) as follows:

\[_{ij}= x_{i}+(-) x_{j},\;\; \;_{ij}= y_{i}+(1-)y_{j},\] (1)

where the combination ratio \(\) is sampled from the beta distribution \((,)\), \(\) is element-wise multiplication, and the binary mask \(\{0,1\}^{W H}\) indicates where to drop out and fill in from two images, \(\) is a binary mask filled with ones. We provide more details about \(\) in Appendix 7.1.

Optimal TransportOT measures the minimal cost to transport between two probability distributions [35; 64; 65; 66; 67]. We only provide a brief introduction to OT for discrete distributions and refer the readers to  for more details. Denote two discrete probability distributions \(p\!=\!_{i=1}^{n}\!a_{i}_{x_{i}}\) and \(q\!=\!_{j=1}^{m}\!b_{j}_{y_{j}}\), where both \(\) and \(\) are discrete probability vectors summing to 1, \(x_{i}\) and \(y_{j}\) are the supports of the two distributions respectively, and \(\) is a Dirac function. Then the OT distance is formulated as follows: \((p,q)=_{(p,q)}, ,\) where \(_{ 0}^{n m}\) is the cost matrix with element \(C_{ij}\!=\!C(x_{i},y_{j})\) which reflects the cost between \(x_{i}\) and \(y_{j}\) and the transport probability matrix \(_{ 0}^{n m}\) is subject to \((p,q)\!\!\{_{i=1}^{n}\!T_{ij}\!=\!b_{j}, _{j=1}^{m}\!T_{ij}\!=\!a_{i}\}\). The optimization problem above is often adapted to include a popular entropic regularization term \(H=-_{ij}T_{ij} T_{ij}\) for reducing the computational cost, denoted as Sinkhorn algorithm .

## 4 Method

In this work, we propose a novel adaptive image-mixing data augmentation method based on OT for long-tailed classification, where the overall framework is shown in Fig. 2. We consider a distribution \(P_{}\) over the background images mainly from majority classes and another distribution \(Q_{}\) over the foreground images mainly from minority classes. By minimizing the OT distance between these two distributions, we design the sample-level and class-level cost functions and use the learned transport plan as guidance for the image-mixing process to generate more semantically meaningful mixed samples between the two distributions.

Main ObjectiveSince majority classes usually have sufficient data and rich information, it is natural to leverage the majority classes to enhance minority classes. In this work, we aim to generate minority-centric images with majority contexts, combining the background and foreground images based on CutMix. The critical question is how to pair a background image and a foreground image. The **first factor** of the pairing strategy is that we would expect a background image to be more likely from the majority classes and a foreground image to be more likely from the minority classes, which has been modeled in CMO . However, in addition to the first one, the **second factor** is the semantic similarity between the background and foreground images, which has not been studied in the literature and is the main focus of our paper. Specifically, suppose a set of candidate images

Figure 2: Adaptive image-mixing method with optimal transport.

already satisfy the first factor, \(i.e.,\) background and foreground images are from majority and minority classes, respectively. Pairing these images arbitrarily may not guarantee that the combined images are semantically meaningful and helpful, \(e.g.,\) in Fig. 1.

In this paper, we aim to generate semantically meaningful images by proposing a pairing process that satisfies the first and second factors at the same time. Our proposal is derived from a distribution-matching perspective. First, we consider the distribution \(P_{}\) of the background images as the empirical distribution of all the images in the training dataset, which is a (discrete) uniform distribution:

\[P_{}=_{i=1}^{N}(x_{i},y_{i}).\] (2)

Although \(P_{}\) is uniformly distributed, because the majority classes have significantly more images than the minority classes, it is natural that the samples from \(P_{}\) are more likely to be images from the majority classes, which is what we expect. Secondly, we introduce the distribution over the foreground images, which is a discrete distribution over all the training images:

\[Q_{}=_{j=1}^{N}w_{j}(x_{j},y_{j}),\] (3)

where \(w_{j}\) is the weight of the \(j\)-th sample. In this case, we expect that a sample from \(Q_{}\) is more likely to be an image of the minority classes. Thus, \(Q_{}\) cannot be a uniform distribution. Accordingly, we can specify \(w_{j}\) in several ways, such as the (smoothed) inverse class frequency , or the adequate number of samples . Taking the inverse class frequency as the example, we can sample the \(k\)-th class with the following probability: \(q_{k}=^{r}}{_{k=1}^{K}1/n_{k}^{r}}\), where \(r>0\) controls the smoothness of \(q_{k}\), \(r=1\) indicates the inverse class frequency and \(r=1/2\) means the smoothed version, \(i.e.,\) if \(r\) increases, the weight of the minority class becomes increasingly larger than that of the majority class. Finally, we can use \(w_{j}=q_{k}}\) if \(y_{j}=k\).

For now, with our construction of \(P_{}\) and \(Q_{}\), the first factor is satisfied. For the second factor that requires the semantic similarity between a pair of background and foreground images, we formulate it as an optimization problem of the entropic OT:

\[_{}(P_{},Q_{})\ }}{{=}}_{T_{ij}(P_{,Q_{}})}_{i,j}^{N,N}C_{ij}T_{ij}-[_{i,j}^{N,N }-T_{ij} T_{ij}],\] (4)

where \(>0\) is the hyper-parameter for the entropic constraint, \(C_{ij}\) is the transport cost function, and the transport probability \(T_{ij}\) satisfies \((P_{},Q_{}):=\{T_{ij}_{i=1}^{N}T_{ij}=w _{j},_{j=1}^{N}T_{ij}=\}\). Notably, as an upper-bounded positive metric, \(T_{ij}\) indicates the transport probability between the \(i\)-th background image and the \(j\)-th foreground image, which can be naturally used to measure the importance of each background image for the foreground image when performing image-mixing.

Cost Function\(C_{ij}\) measures the distance between \((x_{i},y_{i})\) in \(P_{}\) and \((x_{j},y_{j})\) in \(Q_{}\). As the main parameter for defining the transport distance between probability distributions, \(C_{ij}\) plays an important role in learning the optimal transport plan, which can be flexibly defined in different ways. For clarity, we can reformulate the concerned model \(g()\) as \(g_{2}(g_{1}(_{1});_{2})\), where \(g_{1}(_{1})\) denotes the feature extractor parameterized with \(_{1}\) and \(g_{2}(_{2})\) is the classifier parameterized with \(_{2}\). We explore a few conceptually intuitive options of \(C_{ij}\). A simple but straightforward way is defining the **sample-level**\(C_{ij}\) with the features of \(x_{i}\) and \(x_{j}\):

\[C_{ij}=1-(z_{i},z_{j}),\] (5)

where \((,)\) is the cosine similarity, \(z_{i}=g_{1}(x_{i};_{1})^{e}\) and \(z_{j}=g_{1}(x_{j};_{1})^{e}\) denote the \(e\)-dimensional representation of \(x_{i}\) and \(x_{j}\), respectively. Now \(C_{ij}\) will be small if two samples have similar or close features, where the cost is influenced by the feature extractor \(_{1}\). Besides, we can also define the **class-level**\(C_{ij}\) only using the label information. Specifically, a normalized confusion matrix \(_{>0}^{K*K}\) can be estimated with an unbiased validation set or a small balanced subset sampled from \(}\). Summarizing the prediction results of the target model \(g\), the confusion matrix \(\) shows how the model \(g\) is confused when it makes predictions. Since we focus on combining the foreground images mainly from minority classes and background images mainly from minority classes, we can set the diagonal element in \(\) to \(0\), denoted as \(}\), to avoid mixing the images from the same class. After that, we can define \(C_{ij}\) with the ground-truth labels of two samples:

\[C_{ij}=1-_{y_{i},y_{j}},\] (6)where \(_{y_{i},y_{j}}\) indicates the element of row \(y_{i}\), column \(y_{j}\), of the matrix \(}\). If \(y_{i}\) and \(y_{j}\) are from the same class, \(_{y_{i},y_{j}}=0\) and \(C_{ij}=1\); if \(y_{i}\) and \(y_{j}\) are not from the same class but are easy get to confused by the model \(g\), \(_{y_{i},y_{j}}\) would be large and \(C_{ij}\) would be small. Now, \(C_{ij}\) is affected by the feature extractor \(_{1}\) and classifier \(_{2}\). Intuitively, based on the features and labels of samples, the cost function can also be defined as:

\[C_{ij}=(1-(z_{i},z_{j}))+(1-)(1-_{y_{i},y_{j}}),\] (7)

where \(\) is a hyper-parameter for balancing the feature and label information and \(C_{ij}\) will be small if two samples have similar features and are from two confused classes but \(y_{i} y_{j}\). Once the cost function \(C_{ij}\) is defined, it can be fed into (4) for learning the transport plan \(T_{ij}\) between foreground images and background images, where a small \(C_{ij}\) tends to produce a large \(T_{ij}\).

Image-mixing ProcessBy minimizing the OT problem, the resultant transport plan \(\) provides an adaptive way to weigh the similarity between background and foreground images. Therefore, we can select the most suitable background image \(x_{j^{}}\) for the foreground image \(x_{i}\) as follows:

\[*{arg\,max}_{j^{} N}j^{}=\{i N:\,j^{}= _{j N}T_{ij}\}.\] (8)

We aim to generate new mixed samples by combining the foreground image \(x_{i}\) and the most relevant background image \(x_{j^{}}\) and their corresponding labels. As their name implies, \(x_{j^{}}\) is used as a background image while \(x_{i}\) provides the foreground patch, which can be pasted onto \(x_{j^{}}\). Recalling CutMix in (1), the detailed mixing process is expressed as:

\[_{ij^{}}= x_{j}^{}+(- ) x_{i},_{ij^{}}= y_{j}^{}+(1-) y_{i}.\] (9)

Training DetailsDuring the training process, we adopt a mini-batch setting to integrate our proposed OTmix with deep neural networks, where we learn \(\) and \(\) alternatively. More specifically, at each iteration, we can sample a mini-batch \(_{}\) from \(Q_{}\) and another mini-batch \(_{}\) from \(P_{}\) to build \(_{}\) and \(_{}\), both of which have \(M\) samples and \(M<N\). In step (1), we can minimize the OT distance between \(_{}\) and \(_{}\) with (4) to learn the transport plan \(\). In step (2), we can use the learned transport plan to select the most relevant background image \(x_{j}^{}\) for each foreground image \(x_{i}\) based on (8). In step (3), we can generate the mixed samples with (9) and minimize the classification loss to train the model \(g\) parameterized by \(\):

\[L=_{(x_{i},y_{i})_{}}[_{(x_ {j}^{},y_{j}^{})_{}}[(y_ {ij^{}},g(_{ij^{}},))]].\] (10)

Since the cost function relies on either the parameter \(_{1}\) in the feature extractor or the parameter \(\) in the whole model \(g\), it might be inaccurate in the early training stage, resulting in an undesired transport plan matrix. Consequently, to avoid selecting the unsatisfactory background image for the foreground image and generating harmful mixed samples, it is more beneficial to randomly select \(x_{j}^{}\) for \(x_{i}\) in the early stage. To this end, we adopt \(y_{}()\) to decide whether randomly mixing the background and foreground pair, where \(T\) is the number of training epochs and \(t\) indicates the current epoch. In the early stage, such as \(t T/2\), we can get \(y_{}\!=\!0\) with a high probability, where we randomly mix the background and foreground images like CMO ; in the late stage, such as \(t>T/2\), we are more likely to sample \(y_{}\!=\!1\), where we can learn \(\) with (4) and select the most relevant background image for each foreground image. Moving beyond CMO, we introduce a more adaptive and elegant image-mixing data augmentation method for long-tailed classification. We summarize our proposed method in Algorithm 1 and highlight steps (1), (2), and (3).

Using OTMix for Balanced ClassificationsIn this case, our method can be viewed as a better alternative to other augmentation methods that mix images randomly [37; 30]. Applying OTMix in balanced classifications is similar to doing it in the imbalanced problems except that we construct both \(P_{}\) and \(Q_{}\) as uniform distributions.

## 5 Experiments

### Experimental Settings and Implementation Details

DatasetsFollowing [23; 25; 27; 20], we evaluate our method on long-tailed classification benchmark datasets: CIFAR-LT-10 , CIFAR-LT-100 , ImageNet-LT , and iNaturalist 2018 . For clarity, the imbalance factor is defined as the data point amount ratio between the most frequent and the least frequent classes, \(i.e.,\,\!=\!}{n_{1}}\). Among them, CIFAR-LT-10 (CIFAR-LT-100) are created from CIFAR-10 (CIFAR-100)  with \(\!=\!\{100,50,10\}\), respectively. ImageNet-LT, containing 1,000 classes with \(\!=\!256\), is created from ImageNet . Different from them, iNaturalist 2018 is a real-world and large-scale imbalanced dataset with 8,142 classes and \(\!=\!500\).

Baseline MethodsWe consider several baseline methods: (1) Empirical risk minimization (ERM), training on the cross-entropy loss; (2) Mixup-based augmentation methods: Remix , Unimix , CMO ; (3) Other augmentation methods: M2m , Open-sampling , FSA , SAFA ; (4) Losses designed for long-tailed classifications: LDAM, DRW , BALMS , IB , LADE ; (6) Other methods: BBN , RIDE , Decouple , MisLAS .

Implementation DetailsFor all datasets, we use PyTorch  and SGD optimizer with momentum 0.9. Besides, we set \(\) for defining cost function in (7) as \(0.05,\) for entropic constraint in (4) as \(0.01,\,r\) for the smoothness in (3) as \(1\), and \(\) for sampling combination ratio as \(4\). For CIFAR-LT-10 and CIFAR-LT-100, we use ResNet-32  as the backbone following  and use 240 epochs on a single GTX 2080Ti and set the initial learning rate as 0.1, which is decayed by 0.1, 0.1, and 0.01 at the 100th, 160th, and 200th epochs. For the ImageNet-LT, we employ ResNet-50 as the backbone following  and use 200 epochs on four GTX 2080Ti GPUs. The learning rate is initialized as 0.1 and decays by 0.1, 0.1, and 0.01 at the 40th, 80th, and 160th epochs. For the iNaturalist 2018, we use ResNet-50 as the backbone following  and train 210 epochs on four Tesla A100 GPUs with an initial learning rate of 0.1, which is decayed by 0.1 in the 30th, 80th, 130th, and 180th epochs. Performances are mainly reported as the overall top-1 errors (%). Following , we also report the error rates on three disjoint subsets: many-shot classes with more than 100 training samples, medium-shot classes with 20 to 100 samples, and few-shot classes with 20 samples.

    &  &  \\   & 100 & 50 & 10 & 100 & 50 & 10 \\  ERM\({}^{4}\) & 26.9 & 22.9 & 12.9 & 63.2 & 56.3 & 43.4 \\  ERM-DRW\({}^{4}\) & 26.3 & 21.6 & 13.4 & 61.1 & 56.0 & 44.4 \\ ERM-DRW\({}^{4}\) & 24.3 & 18.9 & 11.9 & 58.0 & 54.3 & 41.8 \\ LDAM-DRW\({}^{4}\) & 23.0 & 19.1 & 11.8 & 57.4 & 52.2 & 45.0 \\ BALMS\({}^{12}\) & 22.7 & 19.1 & 11.8 & 58.0 & 53.1 & 41.6 \\ \({}^{12}\) & 21.7 & 18.3 & 11.7 & 55.0 & 51.1 & 42.0 \\ \({}^{13}\) & – & – & – & 54.6 & 49.5 & 38.3 \\  BN\({}^{12}\) (25) & 20.2 & 17.8 & 11.7 & 57.4 & 53.0 & 40.9 \\ BDG (3 experts)\({}^{1}\) & 18.4 & 16.0 & 13.7 & 51.4 & 48.6 & 40.2 \\ MSLAS\({}^{1}\) & 17.9 & 14.3 & 10.0 & 53.0 & 47.7 & **36.8** \\  ERM + Open-sampling\({}^{1}\) & 22.4 & 18.2 & 10.6 & 59.7 & 55.2 & 41.9 \\ ERM + M2m  & 21.7 & – & 12.1 & 57.1 & – & 41.8 \\ LDAM-DRW + SAFA\({}^{1}\) & 19.5 & 16.4 & 11.1 & 54.0 & 50.0 & 40.9 \\  ERM + Remais  & 24.6 & – & 11.8 & 58.1 & – & 40.6 \\ ERM + CMM\({}^{13}\) & 23.5 & – & – & 58.5 & – & – \\ ERM + CM0 & 25.01 & 18.6\({}^{1}\)[11.5\({}^{2}\)] & 56.1\({}^{1}\)[51.7\({}^{1}\)] & 51.7\({}^{1}\)[40.5\({}^{1}\)] & 40.5\({}^{1}\)[11.8\({}^{1}\)] & 40.7\({}^{1}\)[39.8\({}^{1}\)] \\  ERM + Oynix & 21.7 & 16.6 & 9.8 & 53.6 & 49.3 & 38.4 \\ LDAM-DRW + OTrix & 22.3 & 18.0 & 12.0 & 56.3 & 50.9 & 41.5 \\
**DRW + OTrix** & 16.9 & 13.8 & **9.4** & 52.0 & 47.4 & 37.3 \\ LDAM-DRW + OTrix & 18.2 & 16.0 & 11.8 & 52.0 & 47.6 & 41.0 \\
**BALMS + OTrix** & **16.0** & **13.5** & 9.8 & 53.2 & 47.7 & 37.7 \\
**RIDE (3 experts) + OTnx** & 17.3 & 14.8 & 11.3 & **49.3** & **46.2** & 39.2 \\   

Table 1: Top-1 errors (%) of ResNet-32 on CIFAR-LT-100. \({}^{^{*}}\): our reproduced results. \({}^{}\)††††: results reported in the original paper.

### Experiments on Long-tailed Classification

We conduct experiments on CIFAR-LT-10 and CIFAR-LT-100 of ResNet-32 in Table 1, where OTmix+ERM achieves better performance than the mixup-based augmentation methods with ERM loss, especially for CMO+ERM. It confirms the validity of selecting a suitable pair to mix. Besides, OTmix with ERM loss produces comparable performance to complex long-tailed classification methods, such as specially designed losses and other augmentation methods. Considering OTmix can also be combined with some existing long-tailed losses and model architectures, we further combine it with LDAM-DRW loss , BALMS loss , and also the network with multiple experts (RIDE) . It is clear that OTmix equipped with BALMS loss, DRW loss, and RIDE outperforms most of the competing methods except on CIFAR-LT-10 with \(=10\), where it is only weaker than MisLAS. These results illustrate the superiority of the proposed OTmix.

We further perform experiments on large-scale imbalanced ImageNet-LT and iNaturalist 2018 datasets. Table 2 demonstrates that OTmix based on ERM loss can still produce a comparable performance when compared with competing long-tailed losses, related mixup-based, and other data augmentation methods. Similarly, combining ours with other losses or model architecture can overall outperform these related baselines on ImageNet-LT and iNaturalist 2018, where **RIDE (3 experts) + OTmix** performs best. Furthermore, our method can improve the performance of the medium and few classes, which is the goal of long-tailed methods. We compare OTmix with mixup-based augmentation methods under various loss functions in Appendix 7.2 and computational cost in Appendix 7.4.

### Experiments on Balanced Classification

To validate whether our proposed OTmix can be used for the balanced classification task, we adopt the commonly-used CIFAR-10  and CIFAR-100 . Notably, our proposed OTmix can be flexibly combined with the classical mixup-based methods when mixing images, where we consider Mixup , Cutmix , and SaliencyMix . We train the networks using the same details as on the CIFAR-LT. At each training iteration, we randomly sample two mini-batches, \(i.e.,\)\(_{}\) and \(_{}\), from the balanced training dataset. Table 3 lists the experimental results on balanced CIFAR datasets, where OTmix produces better performance. It indicates the benefit of building a better image pair to perform image-mixing data augmentation for the balanced classification task. Moreover, **OTmix (SaliencyMix)** achieves the best top-1 error of 5.1% and 21.7% on CIFAR-10 and CIFAR-100 datasets, respectively. These results show that OTmix is still effective on balanced

    &  &  \\   & ALL & Many & Medium & Few & ALL & Many & Medium & Few \\  ERM\({}^{+}\) & 58.4 & 36.0 & 66.2 & 94.2 & 39.0 & 26.1 & 36.5 & 44.5 \\  ERM-DRW & 49.9\({}^{}\) & 38.3\({}^{}\) & 52.7\({}^{}\) & 71.2\({}^{}\) & 36.3\({}^{}\) & \(-\) & \(-\) & \(-\) \\ LDAM-DRW\({}^{+}\) & 50.2 & 39.6 & 53.1 & 69.3 & 30.0 & 30.0 & 29.8 & 30.1 \\ BALMS\({}^{*}\) & 49.0 & 39.1 & 51.2 & 67.9 & 30.0 & 30.0 & 29.8 & 30.1 \\ IB\({}^{*}\) & – & – & – & – & 34.6 & – & – & – \\ LADE\({}^{*}\) & 48.1 & 37.7 & 50.7 & 68.8 & 30.0 & – & – & – \\  Decouple-\(^{*}\) & 52.7 & 41.2 & 56.0 & 73.9 & 31.8 & 26.8 & 31.2 & 33.9 \\ Decouple-\(^{*}\) & 52.3 & 42.9 & 54.8 & 70.7 & 30.5 & 29.0 & 30.2 & 31.8 \\ BBN\({}^{*}\) & – & – & – & – & 30.4 & – & – & – \\ MiSLAS\({}^{*}\) & 47.3 & – & – & – & 28.4 & 26.8 & 27.6 & 29.6 \\ RIDE (3 experts)\({}^{*}\) & 45.1 & 33.8 & 48.3 & 65.1 & 27.8 & 29.8 & 27.8 & 27.3 \\ RIDE (4 experts)\({}^{*}\) & 44.6 & 33.8 & 47.7 & 63.5 & 29.1 & 29.1 & 27.6 & 26.9 \\  ERM + M2m\({}^{}\) & 57.8 & – & – & – & – & – & – \\ Focal Loss + FSX\({}^{*}\) & – & – & – & – & 34.1 & – & – & – \\ LDAM-DRW + SAFA\({}^{*}\) & – & – & – & – & 30.2 & – & – & – \\  ERM + Remix\({}^{*}\) & 58.3 & – & – & – & 38.7 & – & – & – \\ ERM + CMO\({}^{}\) & 50.9 & 33.0 & 57.7 & 79.5 & 31.1 & **23.1** & 30.7 & 33.4 \\ RIDE (3 experts) + CMO\({}^{*}\) & 43.8 & 33.6 & 46.1 & 64.4 & 27.2 & 31.3 & 27.4 & 26.9 \\  ERM + OTmix & 48.0 & **30.0** & 54.1 & 77.7 & 30.5 & 30.7 & 29.5 & 31.6 \\ DRW + OTmix & 46.6 & 33.0 & 51.0 & 69.6 & 28.9 & 29.4 & 28.1 & 29.6 \\ LDAM-DRW + OTmix & 47.5 & 37.2 & 48.9 & 71.8 & 30.4 & 31.5 & 29.8 & 30.6 \\ BALMS + OTmix & 44.4 & 36.0 & 47.6 & 57.3 & 28.5 & 28.9 & 28.0 & 29.2 \\
**RIDE (3 experts) + OTmix** & **42.7** & 40.6 & **43.5** & **55.9** & **27.0** & 28.7 & **27.2** & **26.2** \\   

Table 2: Top-1 errors (%) of ResNet-50 on ImageNet-LT and iNaturalist 2018. \({}^{**}\): results reported in CMO. \({}^{*}\)†: results reported in origin paper.

  
**Method** & **CIFAR-10** & **CIFAR-100** \\ 
**ResNet-32 (baseline)** & **11.8** & **34.2** \\  + Mixup  & 9.8 & 32.9 \\ + OTmix (Mixup) & **8.1** & **32.3** \\  + Cutmix  & 8.6 & 31.6 \\ + OTmix (Cutmix) & **5.8** & **28.0** \\  + SaliencyMix (30) & 7.2 & 31.2 \\ + OTmix (SaliencyMix) & **5.1** & **21.7** \\   

Table 3: Top-1 errors (%) of our methods using ResNet-32 on balanced datasets.

classification tasks and can be combined with some advanced mixup-based methods, proving its flexibility and availability.

### Analytical Experiments

Here we implement the ablation study under various settings on CIFAR-LT-100 with \(\!=\!100\) and summarize the results in Table 4. Our best-performing method uses the combined cost function with cosine similarity, sets the diagonal element in the confusion matrix \(\) as 0, and relies on the maximum value in the transport plan matrix to select the background image for each foreground image. To reveal the influence of the cost function, we denote the cost function by only using the sample-level cost and the class-level cost as "Sample-level cost only" and "Class-level cost only", respectively. Besides, we also consider using euclidean distance rather than cosine similarity to define the cost function called "Euclidean distance". Compared with ours, which combines the sample-level and class-level costs and uses cosine similarity, all these variants perform slightly worse, demonstrating the benefit of combining features and labels to compute the cost function and the effectiveness of cosine similarity.

To avoid mixing the background and foreground images from the same category, we set the diagonal element in \(\) as 0 and use the resultant \(}\) to compute the cost function. Here, we consider its variant by calculating the cost function using the original confusion matrix \(\), denoted as "Keeping same category", which can be found to outperform by OTmix. It shows that avoiding mixing samples from the same class is beneficial, no matter from minority or majority classes. Besides, we adopt the multinomial distribution, denoted as "Probability selection", to select the corresponding background image for the \(i\)-th foreground image, whose probability vector can be formulated as \(=(T_{i,1:M})\). We observe that selecting background images according to probability distribution is inferior to using the maximum value, where the latter chooses the most relevant background image for each foreground image mainly from minority classes without randomness. It proves the necessity of using the maximum value in the transport plan to build a pair for mixing.

Furthermore, we use the confusion matrix itself ("Confusion matrix" in Table 4) to design the image-mixing pair instead of OT. The confusion matrix can be from a pre-trained model on the imbalanced training set with standard ERM loss. Specifically, after sampling the background and foreground images, if the class \(k_{1}\) of the foreground image is most easily confused with \(k_{2}\) of the background image (\(k_{1} k_{2}\)), we use them to construct a pair. However, using the confusion matrix to build the image-mixing pair is inferior to ours. The possible reason is that OTmix provides a more adaptive way to measure the importance of each background image for the foreground image at each training iteration. We provide more analytical results in Appendix 7.3.

### The Learned Transport Plan for Image-mixing

Recall that our method adaptively learns a transport plan between a batch of foreground images and a batch of background images. Fig. 3 shows our learned transport plan on CIFAR-LT-10 with \(=100\), whose list of class names is {airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck} from \(k=1\) to \(10\). To facilitate the visualization, we set the size of \(_{}\) and \(_{}\) as \(M=10\), respectively. We observe that the learned transport plan can effectively capture the semantic correlations between foreground and background images. For example, the foreground image from the "truck" class (from the "ship" class) is highly related to the background image from the "automobile" class (from the "airplane" class). These correlations align with the confusion matrix in Fig. 6 of Appendix 7.3, however, OTmix is adaptive and specific to batches, while the confusion matrix is global and fixed. In addition to capturing the class-level similarity information, we can find that different background images from the same class have different importance to the same foreground image, \(i.e.,\) the sample-level similarity information. For instance, the transport

  
**Method** & **All** & **Many** & **Medium** & **Few** \\ 
**ERM + OTmix** & **53.6** & **27.7** & **53.2** & **83.3** \\  Sample-level cost only & 55.1 & 30.8 & 56.1 & 85.5 \\ Class-level cost only & 54.5 & 30.1 & 55.9 & 84.5 \\ Euclidean distance & 54.1 & 27.9 & 54.0 & 84.8 \\  Keeping same category & 54.9 & 29.0 & 55.1 & 85.0 \\  Probability selection & 55.0 & 28.0 & 55.7 & 85.6 \\  Confusion matrix & 57.1 & 29.4 & 59.5 & 86.8 \\   

Table 4: Ablation study on CIFAR-LT-100.

Figure 3: Our learned transport plan on CIFAR-LT-10 with \(\!=\!100\).

probabilities between the foreground dog image and two background cat images are \(0.75\) and \(0.24\), respectively. We attribute that to our designed cost function based on the sample-level feature and class-level confusion matrix when learning the transport plan. Therefore, a learned transport plan is an effective and adaptive way to guide the image-mixing process for long-tailed classification. We further compare the image-mixing statistical results of OTmix and CMO in Appendix 7.5 at one epoch, \(i.e.,\) the probability of each class in foreground distribution being mixed with each class in background distribution.

### Visualization Results and Analysis

T-SNE VisualizationTo gain additional insight, we provide the T-SNE visualization of BALMS, BALMS+CMO, and BALMS+OTmix on CIFAR-LT-10 with \(=100\) in Fig. 4. Compared with BALMS+CMO, the majority class is more separable from the minority class2 in OTmix. Besides, the samples within the same class in our proposed method are usually closer than in CMO, \(i.e.,\) small intra-class. It indicates that our proposed method can generate more effective mixed samples to improve the long-tailed classification performance.

Mixed Image Pairs VisualizationTo visually witness the intricate dynamics of the mixing process in our method, we provide the visualization results of image pairs for semantically meaningful mixed images on ImageNet-LT. As shown in Fig. 5, we plot the more semantically reasonable and relevant background image for each foreground image given the learned transport plan. For example, the semantic information of "Grey fox" is semantically relevant to "Dhole". It visually reveals that our method can generate meaningful and advantageous samples, benefiting the balanced and imbalanced classification tasks. More visualization results are available in Appendix 7.5.

## 6 Conclusion

We have introduced a novel adaptive image-mixing augmentation method based on OT for long-tailed classification, with the goal of generating more semantically meaningful samples. We view the background images mainly from majority classes and the foreground images mostly from minority classes as two distributions, where we aim to minimize the OT distance between these two distributions. Moreover, we further design a combined cost function based on sample-level and label-level information. By viewing the learned transport plan as guidance to build a pair for image-mixing, we provide an effective way to weigh the background image for each foreground image. Our proposed OTmix has shown appealing properties that can be either combined with existing long-tailed methods or applied to balanced classifications. Experimental results validate that OTmix achieves competing performance on commonly long-tailed problems and balanced datasets.

Figure 4: T-SNE visualization of BALMS, BALMS+CMO, and BALMS+OTmix on CIFAR-LT-10.

Figure 5: The visualization results of image pairs for meaningful mixed images on ImageNet-LT.