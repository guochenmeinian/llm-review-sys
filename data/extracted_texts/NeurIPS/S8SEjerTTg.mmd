# Cloud Object Detector Adaptation by Integrating Different Source Knowledge

Shuaifeng Li\({}^{1}\) Mao Ye\({}^{1}\) Lihua Zhou\({}^{1}\) Nianxin Li\({}^{1}\) Siying Xiao\({}^{1}\) Song Tang\({}^{2}\) Xiatian Zhu\({}^{3}\)

\({}^{1}\) University of Electronic Science and Technology of China

\({}^{2}\) University of Shanghai for Science and Technology

\({}^{3}\) University of Surrey

hotwindlsf@gmail.com, cvlab.uestc@gmail.com, xiatian.zhu@surrey.ac.uk

https://github.com/Flashkong/COIN

The corresponding author.

###### Abstract

We propose to explore an interesting and promising problem, Cloud Object Detector Adaptation (CODA), where the target domain leverages detections provided by a large cloud model to build a target detector. Despite with powerful generalization capability, the cloud model still cannot achieve error-free detection in a specific target domain. In this work, we present a novel Cloud Object detector adaptation method by Integrating different source kNowledge (**COIN**). The key idea is to incorporate a public vision-language model (CLIP) to distill positive knowledge while refining negative knowledge for adaptation by self-promotion gradient direction alignment. To that end, _knowledge dissemination, separation, and distillation_ are carried out successively. Knowledge dissemination combines knowledge from cloud detector and CLIP model to initialize a target detector and a CLIP detector in target domain. By matching CLIP detector with the cloud detector, knowledge separation categorizes detections into three parts: consistent, inconsistent and private detections such that divide-and-conquer strategy can be used for knowledge distillation. Consistent and private detections are directly used to train target detector; while inconsistent detections are fused based on a consistent knowledge generation network, which is trained by aligning the gradient direction of inconsistent detections to that of consistent detections, because it provides a direction toward an optimal target detector. Experiment results demonstrate that the proposed COIN method achieves the state-of-the-art performance.

## 1 Introduction

The emergence of large language models like GPT-4  heralds a future in which cloud model possesses remarkable capabilities tailored to specific tasks. But the performance always will be degraded for special target domain. Naturally, the problem of _Cloud Domain Adaptation_ (CDA) emerges, where the knowledge from cloud model is transferred to the target domain by cloud API requests. This work focuses on _Cloud Object Detector Adaptation_ (CODA), a new problem setting in domain adaptation. This problem is training a detector for any target domain under the conditions that there exists a large cloud detector offering API service while target domain samples have not any labels. As shown in Fig.1(a), compared with previous settings, there are two main advantages: (1) open target scenarios and object categories; (2) without high domain similarity between domains.

There have been some advances in related fields. _Source-free Object Detection_ (SFOD)  transfers a pre-trained model from the source domain to the target domain without considering themodel privacy issue. The existing methods usually resort to three routes: pseudo label refinement [35; 9; 62; 71; 38], where more reliable labels are refined for self-training; knowledge distillation [33; 39; 13; 18; 28; 51], where the Mean Teacher framework is used to distill knowledge; domain alignment [33; 53; 10; 61], where distribution alignment methods are used to learn domain-invariant feature. Despite notable success has been achieved, these methods are not suited for CODA because cloud model is not accessible, and any target domain cannot be transferred because of category limitations.

Another related field is _Black-box Domain Adaptive Object Detection_ (Black-box DAOD), which only offers detection results of several predefined categories without accessing source model and data . There exists one work for Black-box DAOD, where three types of memory are used for label calibration . More Black-box DA methods [36; 64; 60; 46; 68; 58] focus on classification and fall into two categories. The first uses knowledge distillation [36; 46; 58] to distill source domain knowledge, while the second uses sample selection [64; 60; 68; 57] to select representative samples for training. They also can not be applied to CODA. Unlike Black-box DAOD, CODA benefits from large training data and language modality, thus eliminating the trouble of finding a tailored source domain containing all target categories and facilitating unrestricted target domain adaptation.

Despite being trained on large data, the powerful cloud model also cannot achieve error-free detection. So leveraging public auxiliary models with enough object categories to correct the error detections is a natural choice. The public vision-language model such as CLIP , pre-trained on millions of image-text pairs, is a good model to help adaptation. Due to the lack of detection ability and domain shift, CLIP is first extended as a detector, abbreviated as CLIP detector, to inherit and further adapt the knowledge from CLIP, as shown in Fig.1(b). However, the CLIP detector also has error detections. As shown in Fig.1(c), the detections of CLIP detector and cloud detector can be classified into three categories: consistent, inconsistent and private detections. Now, the problem is how to distill these detections to the target detector. The consistent and private detections can be easily distilled to the target detector as the supervision signals. While for inconsistent detections, one potential route is to use consistent detections to help inconsistent detections, as shown in Fig.1(d).

Based on the above analysis, we propose a novel adaptation method in a divide-and-conquer manner, dubbed as _COIN_, where knowledge dissemination, separation, and distillation stages are carried out successively. _Knowledge dissemination_ combines the knowledge from CLIP model and cloud model to initialize a CLIP detector and a target detector based on Faster R-CNN  architecture. At the same time, prompt learning is performed to align the target domain for CLIP detector. _Knowledge separation_ matches the detections from cloud detector and CLIP detector, categorizing three parts as consistent, inconsistent and private detections. Based on Mean-Teacher framework, _knowledge distillation_ regards CLIP detector and cloud detector as teachers while the target detector as a student. Consistent and private detections are used as supervision; prompt learning is performed again to adapt CLIP model for target detector. A Consistent Knowledge Generation network (CKG) is proposed to fuse inconsistent detections. Since the gradient direction for optimizing target detector based on consistent detections offers an optimization direction, for inconsistent detections, a gradient direction alignment loss is proposed to learn CKG under the situation without supervision labels. The Mean-Teacher framework also updates CLIP detector based on target detector, thereby achieving better knowledge integration.

Figure 1: (a) Setting comparison: Our CODA (Cloud Object Detector Adaptation), UDAOD (Unsupervised Domain Adaptive Object Detection), SFOD (Source-free Object Detection) and Black-box DAOD (Black-box Domain Adaptive Object Detection). (b) Knowledge dissemination initializes cloud detector and target detector. Then, detections are categorized to three parts: consistent, inconsistent, private (cloud and CLIP) detections (c). (d) The gradients on consistent detections are used to guide decision fusion for inconsistent detections. _Zoom in for best view._

Our contributions can be summarized as follows. (1) We propose to explore a promising problem CODA suited for real-world scenarios with large cloud models. A novel method _COIN_ is proposed in a divide-and-conquer manner. An open auxiliary model (CLIP) is introduced to help adaptation. By carefully combining different source knowledge, the effect of one plus one being greater than two has been achieved. (2) A novel decision-level fusion strategy is proposed. The gradient direction alignment loss is proposed which fuses the conflicts by using consistence detections in a rational and self-promotion way. (3) Different prompt learning are performed for CLIP detector and target detector. For CLIP detector, the class text embeddings are aligned to CLIP visual feature class prototypes; while for target detector, the prototypes based on consistent detections are used since target detector combines different source knowledge.

## 2 Related Work

**Domain adaptive object detection.**_Unsupervised Domain Adaptive Object Detection_ (UDAOD) assumes that source samples are freely accessible and labeled but target samples have no labels. Existing methods can be roughly classified into three categories. The first approach is based on domain alignment [8; 49; 50; 59; 6; 75; 70; 4; 31; 27], where the source and target domains are aligned by adversarial learning, contrastive learning, etc. The second approach is the popular knowledge distillation [12; 7; 3; 22; 14; 4; 20], where the Mean-Teacher framework is used to distill knowledge from teacher to student. The third approach is based on graph learning [5; 34; 41; 15], where graphs are constructed to achieve better adaptation. _Source-free Object Detection_ (SFOD) assumes that only the pre-trained source domain model is accessible. Existing SFOD methods usually resort to three technical routes. The first is pseudo label refinement based method [35; 9; 62; 71; 38], e.g., SED  seeks a confident threshold for filtering pseudo labels according to self-entropy descent. The second is knowledge distillation based approach [33; 39; 13; 18; 28; 51]. For example, LODS  enhances target domain style and then overlooks target domain style, resulting in an impressive two-way knowledge distillation. The third is based on domain alignment [53; 10; 33; 61]. For instance, IRG  uses a contrastive loss to enhance the target representations by exploiting the object relations.

**Black-box domain adaptation.** Recently, black-box domain adaptation for _image classification_ receives major attention. There are two routes. The first is knowledge distillation [36; 46; 58]. For example, DINE  starts by training a model using knowledge distillation and structural regularization, then further refines it for better adaptation; RAIN  introduces phase mixup and subnetwork distillation to learn from both regularized data and subnetworks; AEM  first proposes to explore CLIP for Black-box DA by introducing an adversarial experts model. The second is based on sample selection [64; 60; 68; 57]. For instance, IterLNL  follows learning with noisy labels technique and estimates a noise rate to select confident target samples for training; BETA  divides the target domain into two subdomains and leverages synergistic twin networks and subdomain augmentation for robust model learning; RFC  introduces selection training to pick samples from minority classes for reviewing forgotten classes, and employs neighborhood clustering for more balanced learning. There exists only one work for _Black-box DAOD_ task, BiMem  refines the pseudo labels by constructing sensory, short-term and long-term memories, where a forward memory construction and a backward label calibration are performed iteratively. Despite the great performance achieved, the source model cannot be transferred to arbitrary target domains due to category restrictions and domain similarity, and require customizing the source domain model for a specific target domain.

## 3 Methodology

**Problem statements.** Cloud Object Detector Adaptation (CODA) assumes the unlabeled target domain \(=\{x^{i}\}_{i=1}^{N_{t}}\) and \(=\{c^{i}\}_{i=1}^{N_{c}}\) is a set of classes that need to be detected, where \(N_{t}\) is the total number of target images, \(c^{i}\) is the \(i\)-th class name and \(N_{c}\) is the number of classes. There exists a powerful cloud detector \(F_{d_{cld}}\), and the goal of CODA is to train a target detector by the detection results \(^{i}_{cld}\) via a cloud API. \(^{i}_{cld}\) consists of boxes \(^{i}_{cld}\) and class probabilities \(^{i}_{cld}\) for any target domain image \(x^{i}\), where class probabilities \(^{i}_{cld}\) are derived from class predictions, which can be in the form of class-only, confidence score, or probability, depending on the cloud detector. Moreover, with the powerful cloud detector trained on large-scale image caption datasets, open target scenarios and even categories can be adapted, eliminating the hassle of finding a similar source domain.

**Overview.** The proposed _COIN_ method introduces a vision-language model CLIP  to assist in domain adaptation of a freely chosen large-scale pre-trained cloud detector, like GDINO . It consists of three stages, i.e., knowledge dissemination, knowledge separation and knowledge distillation, as illustrated in Fig.2. (a) Knowledge dissemination stage first collects results from cloud detector and CLIP respectively, where the detection boxes from cloud and the classification results from CLIP are used to initialize a CLIP detector and a target detector. Meanwhile, prompt learning is performed for CLIP detector. (b) Knowledge separation stage initially obtains and then matches the detections from cloud and CLIP detectors, resulting in the categorization of consistent, inconsistent and private detections. (c) Knowledge distillation stage updates target detector. The Mean-Teacher framework is employed which regards cloud detector and CLIP detector as two teachers while the target detector is a student. To enhance robustness, the student is fed into a strong augmentation version of target image, and consistent and private detections are directly used as pseudo labels. For inconsistent detections, a Consistent Knowledge Generation network (CKG) is designed to fuse them in decision-level; a gradient direction alignment loss is proposed to optimize CKG; the target detector and CKG are updated mutually. Better knowledge integration is achieved by updating CLIP detector.

### Knowledge Dissemination

In this section, the knowledge from cloud detector and auxiliary model (CLIP) are combined to get two object detectors in target domain, i.e., CLIP detector \(F_{_{clip}}\) and target detector \(F_{_{T}}\). An intuitive idea is to train a Faster R-CNN based detector  using the predicted boxes from cloud detector and its corresponding labels from CLIP. However, there are two deficiencies. The first is that the knowledge from the auxiliary model has not been fully utilized; here the auxiliary model CLIP is supposed to be open-source and known. Another is domain shift existed between CLIP and target domain; the auxiliary model CLIP should be aligned with target domain. So the pre-trained CLIP visual encoder is used to build two detectors \(F_{_{clip}}\) and \(F_{_{T}}\); then domain-specific prompts are learned to align CLIP model to target domain for CLIP detector; in the end, the detections are collected to train CLIP detector. They are detailed as follows.

**Detector architecture** is based on Faster R-CNN framework as shown in Fig.2(a). The pre-trained CLIP visual encoder \(G\) is split into \(G_{1}\) and \(G_{2}\) (the last block), which are used as backbone and ROI head feature extractor respectively. Because CLIP is pre-trained for classification task, the region feature of proposal \(r\) for a target image \(x\), \(_{r}=G_{2}(ROI(G_{1}(x),r))\), can not be used for box regression. So, a transformation network \(Trans\), composed of mean pooling and three dense layers, is used to endow the localization ability. Finally, a linear layer \(l_{c}\) and a linear layer \(l_{b}\) are used to get the box feature and locations respectively, i.e, \(=l_{c}(Trans(_{r})))\) and \(=l_{b}(Trans(_{r}))\). The class probability \(_{i}\) for the \(i\)-th category is calculated by computing the similarity with the \(i\)-th class embedding \(^{i}\). The background is also considered to be a class. It can be written as

\[_{i}=,^{i})/)}{_{i=1}^{N_{c}+1}exp(sim (,^{i})/)},\] (1)

Figure 2: Overview of the proposed method _COIN_. (a) Knowledge dissemination stage. The architecture for CLIP detector and target detector is presented. (b) Knowledge separation stage splits detections from two detectors into three kinds. (c) Knowledge distillation stage trains target detector. A gradient direction alignment loss is proposed to fuse inconsistent detections in decision-level.

where \(sim(,)\) is the cosine similarity function and \(=0.01\) is the fixed temperature following CLIP. The class embedding \(^{i}=E(P_{i})\) is obtained based on the _frozen_ CLIP text encoder \(E\) and the prompt \(P_{i}\) wrapping the \(i\)-th class name into a later introduced prompt template \(PT\).

Both \(F_{_{clip}}\) and \(F_{_{T}}\) are based on this architecture and are randomly initialized except that the backbone and ROI head feature extractor are initialized by CLIP visual encoder \(G_{1}\) and \(G_{2}\) respectively.

**Prompt learning for CLIP detector.** Since the simple prompt template, like "a photo of a [CLS].", has not target domain information, we use a _trainable_ prompt template \(PT\), like "a photo of a \(\{t^{i}\}_{i=1}^{M}\) [CLS].", where \(M\) is fixed to 4 and \(t^{i}\) is a placeholder whose word embedding is randomly initialized. Prompt learning methods  adapt the class embeddings \(=\{^{i}\}_{i=1}^{N_{c}+1}\) with ground truths. To further adapt CLIP model to target domain, we propose to use visual features to align \(\). Specifically, the visual feature class prototypes are used to learn prompt instead of matching all visual features with the prompt embeddings. The prototypes \(_{p}=\{_{p}^{i}\}_{i=1}^{N_{c}+1}\) are updated by exponential moving average (EMA) as

\[_{p}^{i}=_{p}^{i}+(1-)_{x}|}(=i),\] (2)

where \(\) is the set of proposals for image \(x\), \(\) is the label for the box feature \(\), and \((a=b)\) an indicator function. \(=0.9996\) is a fixed hyperparameter and \(_{p}\) are initialized as the original CLIP per-class embeddings \(_{c}\) for accelerating training. The following \(L_{1}\) loss is used to learn the prompt,

\[_{align}^{1}=\|_{p}-\,\|_{1}.\] (3)

**Pre-training CLIP detector.** Since CLIP do not predict boxes for target domain images, to train CLIP detector, we need to prepare supervision labels based on CLIP and cloud detector knowledge. For any target image \(x\), the detection boxes \(_{cld}\) are borrowed from cloud detector and the corresponding labels are obtained based on the CLIP model; they can be used as supervision signals. The detection boxes are easy to obtain, while obtaining the labels needs some specific operations. Suppose a box feature \(\) is obtained by ROI pooling on the feature map output by CLIP visual encoder \(G\). To obtain more accurate pseudo labels, as RegionCLIP , 81 kinds of prompt templates are used. If the \(j\)-th kind of prompt template is "a [target domain name] style rendering of the [CLS]", the \(j\)-th kind of class embedding for the \(i\)-th object class is \(_{c}^{i,j}\) through CLIP text encoder \(E\). The final \(i\)-th class embedding is \(_{c}^{i}=_{j}_{c}^{i,j}/81\). The class probability \(_{c}\) of \(\) is obtained by computing the similarity like Eq.(1) using \(_{c}\). The boxes predicted as "background" are removed. After the preparation of supervision signals, the CLIP detector is pre-trained via the following losses:

\[_{_{clip}}_{RPN}+_{ROI}+_{align}^ {1},\] (4)

where \(_{RPN}\) and \(_{ROI}\) are the standard detection losses. \(\) is a hyperparameter fixed as 10.

_Remark._ To inherit knowledge for better knowledge dissemination and accelerate training, the visual encoder of the original CLIP model is directly used as the backbone and ROI Head feature extractor. Compared to the previous CLIP based detector F-VLM , our backbone and ROI Head feature extractor are trainable with supervisions from CLIP and cloud detector to facilitate knowledge dissemination process. Moreover, in contrast to PromptSRC  and CLIP-GAP , which align the features to CLIP semantic space, our dynamically updated class prototypes align CLIP semantic space to target domain in an opposite way, thus capturing more target domain-specific attributes.

### Knowledge Separation

Just like flipping two coins at the same time, the detections from cloud detector and CLIP detector exhibit both consistency and conflicts due to different pretraining sources. It is obvious that consistent detection results can be used as ground truths, while inconsistent results pose obstacles to knowledge fusion. To integrate their knowledge into the target detector sensibly, we adopt a divide-and-conquer strategy. Specifically, box matching is utilized to achieve knowledge separation by categorizing them into consistent, inconsistent, and private detections.

Given a target image \(x\), suppose the detections based on cloud detector are \(_{cld}=\{_{cld},_{cld}\}\) containing \(R_{1}\) detected boxes and similarly the detections based on CLIP detector after NMS (Non-Maximum Suppression) are \(_{clip}=\{_{clip},_{clip}\}\) containing \(R_{2}\) detected boxes. To find the matchedboxes, an identification matrix \(\) is defined as follows, \(_{i,j}=1\) if the IoU \(\) between the \(i\)-th box from cloud detector and the \(j\)-th box from CLIP detector, otherwise \(_{i,j}=0\). \(\) is a fixed threshold set to 0.5 according to popular settings. For the \(i\)-th box of cloud detector, the label \(_{cld}^{i}=_{c}_{cld,c}^{i}\), while \(_{clip}^{j}=_{c}_{clip,c}^{j}\) is the label of \(j\)-th box of CLIP detector. Then, as shown in Fig.2(b) the consistent detection set \(}\) and inconsistent detection set \(}\) are defined as follows,

\[}=\{(_{cld}^{i},_{clip}^{j})\,|\,_{i,j}=1, _{cld}^{i}=_{clip}^{j}\},}=\{(_{cld}^{i}, _{clip}^{j})\,|\,_{i,j}=1,_{cld}^{i}_{clip}^{j}\}.\] (5)

The unmatched detection set \(\), also called private detections, is defined as

\[=\{_{cld}^{i}\,|\,_{i,*}=0\}\{_{clip}^{j}\,| \,_{*,j}=0\}.\] (6)

\(_{i,*}\) means the number of CLIP detector boxes that match the \(i\)-th cloud detector box; so does \(_{*,j}\).

For one pair of matched boxes \(_{cld}^{i}\) and \(_{clip}^{j}\) from the consistent or inconsistent detections, an object is located. Typically, the box with a higher score has more precise localization. Therefore, we merge them in a probability-weighted manner to facilitate subsequent distillation, as the features extracted from two matched boxes exhibit slight inconsistencies. So the fused box \(b_{m}^{t}\) is

\[_{m}^{t}=_{cld,c}^{i}*_{cld}^{i}+ _{c}_{clip,c}^{j}*_{clip}^{j}}{_{c}_{cld,c }^{i}+_{c}_{clip,c}^{j}}.\] (7)

After box refinements, the consistent detection set \(}\) and inconsistent detection set \(}\) can be denoted as \(}=\{}\}\) and \(}=\{}\}\) respectively, where \(}=(}_{m}\,,}_{cld}\,,}_{clip}\,, }_{m})\) and \(}=(}_{m}\,,}_{cld}\,,}_{ clip}\,,}_{cld}\,,}_{clip})\) is the inconsistent detections.

_Remark._ Detection conflicts is a core challenge here. Previous UDAOD method SSAL  performs sample selection within the same class, so boxes in the same region that are predicted as different classes may be selected for self-training, resulting in conflicts. While we address this issue by adopting a divide-and-conquer strategy to separate conflicts here and solve them in the following text.

### Knowledge Distillation

Mean-Teacher framework is utilized to distill the above three kinds of detections into target domain detector. The cloud detector and CLIP detector are two teachers while the target detector is student.

**Consistent and private detections knowledge distillation.** For the consistent detections \(}\), they are directly used as ground truths to train target domain detector. The consistency distillation loss is defined as \(_{con}=_{RPN}+_{ROI}\). For the private detections \(\), because the prediction of private boxes is not accurate, only classification loss is calculated. By regarding the private boxes \(_{q}^{|| 4}\) as proposal boxes and feeding them into ROI Head, we obtain the classification probabilities \(_{q}^{stu}\) for student. Then standard distillation loss is employed to distill all private knowledge from both teachers to the target detector as \(_{pri}=L_{kl}(_{q}^{stu},_{q})\), where \(L_{kl}(,)\) is the Kullback-Leibler divergence and \(_{q}\) are prediction results from cloud detector or CLIP detector.

By integrating different source knowledge, the class embedding should be different from CLIP detector. Similar as previous prompt learning method, we also align per-class embeddings \(_{stu}\) to visual class prototypes \(}_{p}\) based on consistent detections computed as Eq.(2): \(_{align}^{2}=||}_{p}-_{stu}||_{1}\).

**Inconsistent detections knowledge distillation.** As shown in Fig.3, a Consistent Knowledge Generation network (CKG), noted as \(F_{_{ckg}}\), is proposed to do decision-level fusion which refines inconsistent detections to consistent ones. Specifically, CKG takes the inconsistent box features \(}_{stu}^{|}| C}\) from target detector, inconsistent visual feature class prototypes \(}_{p}^{cld}\) and \(}_{p}^{clip}\), inconsistent probabilities \(}_{cld}\) and \(}_{clip}\) as input. It outputs the consistent probabilities \(}_{ckg}\) as follows,

\[}_{ckg}=(_{cld}}_{cld}+_{clip }}_{clip}),\] (8)

where \(_{cld}=CA_{1}(}_{stu},}_{p}^{cld})\) and \(_{clip}=CA_{2}(}_{stu},}_{p}^{clip})\) are two adaptive weights generated by two cross-attention modules . \(\) represents the element-wise multiplication and \(()\) represents the softmax function. The architecture of cross-attention module is represented as \((Q(}_{stu}) K(X)^{T}) V(X)\), where \(Q()\), \(K()\) and \(V()\) are the mapping functions, and \(\) represents the matrix multiplication. With the cross-attention module, the features \(}_{stu}\) are compared with the class prototypes through query and key, making the weights generation process more reliable.

Since there do not exist labels in the target domain, a gradient direction alignment is proposed to train CKG network in a self-promotion way, which is also our key contribution. The idea is based on the following observation. Since the consistent detections can be regarded as ground truths in the target domain, they provide an optimization direction toward an optimal target detector. So the gradient direction from consistent detections is used as the supervised signal to train CKG network. Specifically, the gradients of consistent detections and inconsistent detections are computed using \(L_{2}\) loss as follows,

\[}=_{_{T}}\|}_{stu}-(}_{ m})\|_{2},}=_{_{T}}\|}_{stu}- }_{ckg}\|_{2},\] (9)

where \(()\) is the one-hot vector function, \(}_{stu}\) are the predicted probabilities corresponding to the target detector features \(}_{stu}\) on consistent detections; \(}_{stu}\) are the predicted probabilities corresponding to \(}_{stu}\) on inconsistent detections. Then, the CKG network is optimized by aligning \(}\) to \(}\) by cosine similarity. Meanwhile, CKG should also be consistent on consistence detections, i.e., \(}_{ckg}=(CA_{1}(}_{stu},}_{p}) {}_{cld}+CA_{2}(}_{stu},}_{p})}_{ clip})\), is consistent with the label \(}_{m}\). So the total loss for training CKG network is

\[_{_{ckg}}_{ckg}=(1-sim(},}))+L_{ kl}(}_{ckg},\,(}_{m})).\] (10)

The CKG network and target detector are updated mutually. First, CKG is optimized based on target detector, then the output \(}_{ckg}\) is used in turn to update target detector. In order to avoid the interference of low-confidence predictions, we use a threshold \(\) to filter out those low-confidence predictions, resulting in \(}_{stu}^{}\) and \(}_{ckg}^{}\), so target detector is optimized as follows:

\[_{inc}=L_{kl}(}_{stu}^{},\,}_{ckg}^{ }).\] (11)

_Remark_.: Traditional decision-level fusion method  employs simple averaging to merge knowledge from various sources, where different sources share one RPN network to generate fully matched detections. In contrast, we achieve decision-level fusion for two unrelated models based on a divide-and-conquer strategy without ground truth. For inconsistent detections, our method uses a gradient direction alignment to optimize the fusion network in a self-promotion manner.

**Overall optimization.** In each iteration, the CKG network is first updated via Eq.(10). Then we update target detector via the following objective function:

\[_{_{T}}_{con}+_{1}_{inc}+_{2} _{pri}+_{align}^{2},\] (12)

where \(_{1}\) and \(_{2}\) are two hyperparameters. \(\) is fixed as 10 as in Eq.(4). The CLIP detector is updated by \(_{clip}=_{clip}+(1-)_{T}\), where \(=0.9996\) as in Eq.(2), enabling the integrated knowledge in the target detector flows into the CLIP detector gradually, thus achieving better knowledge integration.

_Remark_.: Although CLIP is utilized in COIN, CODA does not impose restrictions on the use of CLIP.

## 4 Experiments

**Datasets.** The problem CODA enables versatile target domain adaptation based on cloud detector, so there are no limitation to transfer scenarios, unlike the problem settings of UDAOD, SFOD and

Figure 3: The structure of Consistent Knowledge Generation (CKG) network.

[MISSING_PAGE_FAIL:8]

Quantitative results for GDINO  are shown in Table 1-4, and results for GLIP  are shown in Appendix. First, the existing methods are compared across four commonly used target domain datasets: Foggy-Cityscapes, BDD100K, Clipart, and KITTI. Specifically, our method _COIN_ significantly outperforms cloud detector by +4.6% (from 34.4% to 39.0%) on Foggy-Cityscapes and CLIP by +18.7% (from 62.1% to 80.8%) on KITTI. This demonstrates that our COIN can identify valuable knowledge for adaptation, regardless of the performance of CLIP (bad on Foggy-Cityscapes while good on KITTI). And CLIP detector improves the mAP by a large margin of +12.8% on Foggy-Cityscapes, +14.4% on BDD100K, and +17.8% on KITTI compared with CLIP, strongly demonstrating the effectiveness of the knowledge dissemination stage. Moreover, GDINO and CLIP already achieve surprising performance of 66.8% and 46.3% on Clipart, proving the superiority of CODA compared to traditional adaptation settings.

Second, since CODA enables versatile target domain adaptation with open categories and scenarios, experiments on Cityscapes for all 8 categories and Sim10K are conducted. Existing methods are not compared, as for Cityscapes they can only detect the car category while Sim10K is usually used as the source domain. From Table 4, we see that the proposed _COIN_ achieves the best performance. Specifically, for Sim10K, when cloud detector and CLIP perform similarly, _COIN_ still brings a significant improvement of +15.9% compared with cloud detector. The extensive quantitative results above not only demonstrate the wide applicability of CODA but also validate the effectiveness and robustness of our proposed method _COIN_.

### Further Analysis.

**Ablation study.** As shown in Table 5, ablation studies are conducted on Foggy-Cityscapes and Cityscapes. Specifically, \(_{align}+\) CLIP detector or \(_{align}+\)_COIN_ represent prompt learning for CLIP detector or target detector respectively; \(_{con}\), \(_{inc}\) and \(_{pri}\) represent the distillation losses of consistent, inconsistent and private detection respectively. (1) For CLIP detector, prompt learning improves the performances from 27.4% and 35.1% to 28.2% and 35.7% on Foggy-Cityscapes and Cityscapes respectively. (2) For the proposed _COIN_ method, all proposed components are effective which demonstrates that our method is able to achieve judicious knowledge integration.

**Ablation study for decision-level fusion.** To further validate the effectiveness of decision-level fusion, our proposed _COIN_ is compared with four experimental groups, as shown in Table 6. Using

    & & & & & & Cityscapes & & & Sim10K \\  Methods & Type & Truck & Car & Rider & Person & Train & Mycycle & Bicycle & Bus & mAP & Car \\  Cloud det  & C & **37.5** & 59.9 & 16.4 & 43.4 & 26.1 & 42.7 & **48.4** & **62.6** & 42.1 & 46.5 \\ CLIP  & C & 15.9 & 36.9 & 15.5 & 27.8 & 0.9 & 15.7 & 20.5 & 31.8 & 20.6 & 46.4 \\ CLIP det & C & 11.3 & 55.8 & 35.1 & 39.1 & **33.8** & 32.0 & 33.7 & 44.7 & 35.7 & 60.0 \\
**COIN** & C & 26.9 & **64.3** & **47.5** & **47.0** & 26.4 & **44.4** & 46.9 & 52.8 & **44.5** & **62.4** \\  Oracle & - & 34.7 & 70.4 & 56.4 & 50.5 & 43.0 & 38.7 & 46.9 & 58.9 & 49.9 & 79.2 \\   

Table 4: Quantitative results on **Cityscapes** and **Sim10K** under GDINO. \(\) – Cloud. det: detector.

Figure 4: Hyperparameter analysis with respect to \(\), \(_{1}\) and \(_{2}\) on Foggy-Cityscapes under GDINO.

the cloud detector alone achieves a mAP of 37.7%. Surprisingly, using the CLIP detector alone achieves an even higher mAP of 38.1%, attributed to the gradual parameter updates of the CLIP detector during training, allowing integrated knowledge to flow into it. Additionally, using both probabilities simultaneously with avg or s-avg approaches yield similar results. While our proposed CKG unsurprisingly achieves the best results, with a mAP improvement of +0.9% (from 38.1% to 39.0%). This strongly demonstrates the effectiveness of our proposed decision-level fusion.

**Hyperparameters sensitivity analysis.** We conduct sensitivity analysis on \(\), \(_{1}\) and \(_{2}\) on Foggy-Cityscapes, as shown in Fig.4. For parameter \(\), our method achieves relatively stable results over a wide range. For parameters \(_{1}\) and \(_{2}\), we first set \(_{2}\) to 0.1 and vary \(_{1}\) across six distinct values ranging from 0.1 to 1.0. Then, we reciprocate the process for \(_{2}\). The outcomes are stable, with a mAP oscillating within a band between 38.0% and 39.0%. This confirms the robustness of _COIN_.

## 5 Conclusion

We proposed a novel method termed _COIN_ for the proposed cloud object detector adaptation (CODA). The open source CLIP model is adapted to help distill knowledge in a divide-and-conquer manner. To efficiently disseminate knowledge from CLIP and cloud detector, a CLIP detector is designed and adapted to the target domain by prompt learning. Then, three kinds of detections are split and distilled to target detector respectively. Consistent and private detections are used as supervision signals without loss of generality. Prompt leaning is applied again for target detector to fit target domain. To eliminate conflicts, a consistent knowledge generation network (CKG) is proposed for decision-level fusion. A gradient direction alignment loss is proposed to learn this network in a self-promotion way. Experimental results validated the effectiveness of our method. _COIN_ is not limited to detection task; it can also be utilized to other tasks, e.g., classification or semantic segmentation.