# Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management

Dhawal Gupta

University of Massachusetts

dgupta@cs.umass.edu

&Yinlam Chow

Google Research

yinlamchow@google.com

&Aza Tulepbergenov

Google Research

atulep@google.com

&Mohammad Ghavamzadeh

Amazon

ghavamza@amazon.com

&Craig Bouttilier

Google Research

cbouttilier@google.com

The work was done prior to joining Amazon, while the author was at Google Research.

###### Abstract

Reinforcement learning (RL) has shown great promise for developing agents for dialogue management (DM) that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite the advancements in RL and language models (LMs), employing RL to drive conversational chatbots still poses significant challenges. A primary issue stems from RL's dependency on online exploration for effective learning, a process that can be costly. Moreover, engaging in online interactions with humans during the training phase can raise safety concerns, as the LM can potentially generate unwanted outputs. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop various RL algorithms, specialized in dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs)--models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting the MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM. We evaluate our methods in open-domain dialogue to demonstrate their effectiveness with respect to the diversity of intent in generated utterances and overall DM performance.

## 1 Introduction

Natural language processing (NLP) has made significant strides in recent years, notably in the field of language generation. Thanks to advances in language modeling, particularly with the use of the transformer architecture (Vaswani et al., 2017), NLP models can now generate text that is often difficult to distinguish from that written by a human. However, despite these advancements, these models still fall short when it comes to having rich conversations. Current NLP models lack effective dialogue management: they are good at generating individual sentences, but struggle with maintaining coherent and engaging conversations. Whereas most compelling conversations generally span numerous topics, are rather open-ended, and often have an underlying goal (e.g., customer success, task completion, recommendation). This requires dialogue agents to understand the context of the conversation and respond appropriately while maintaining the ability to achieve goals.

_Reinforcement learning (RL)_ is a natural approach to learning a policy for a dialogue management (DM) agent. Earlier work on RL-based dialogue systems relies on specific, hand-crafted semantic states (Levin and Pieraccini, 1997; Singh et al., 2002; Walker, 2000) or partially observable beliefstates (Williams and Young, 2007; Young et al., 2010), in which the agent encodes conversations and chooses the best-structured dialogue action at each turn. Applications include relational reasoning (Shah et al., 2018), task completion (Shi and Yu, 2018), and query fulfillment (Serban et al., 2017), whose action spaces are structured enough to be represented by hand-crafted features. To handle more complex dialogues, recent approaches use language models (LMs) to extract semantic representations from conversation histories, treat them as dialogue states, and apply RL to learn a word-level generative DM agent (Jaques et al., 2019; Li et al., 2016, 2017; Shin et al., 2020).

However, unlike supervised learning approaches, where one can train imitation agents with offline conversation data, RL-based DM algorithms require online exploration. Unfortunately, constant interactions with real users are often expensive and time-consuming. While one can potentially address the DM problem using _offline_ RL, issues such as model exploitation leading to distribution shift on the state and action spaces, when training on static datasets are of paramount concern (Levine et al., 2020). Moreover, the myriad variations of language make incorporating all possible conversation histories and bot utterances into the state and action spaces of an RL formulation of the DM problem impractical due to the combinatorics at play. As a result, naive application of RL to DM may result in poorly-performing agents that generate incomprehensible utterances (Zhao et al., 2019).

We tackle the above issues related to the use of offline RL in DM systems by leveraging recent advances in Mixture-of-Expert Language Models (MoE-LMs) (Chow et al., 2022). Specifically, we develop a suite of offline RL algorithms specialized in dialogue planning that exploit the structure of MoE-LMs. Our methods consist of three main components: **1)** a primitive LM, which uses a probabilistic encoder and decoder and is capable of generating diverse semantic intensity; **2)** a number of _specialized_ expert LMs, each of which generates utterances corresponding to a specific intent; and **3)** a compositional DM that, at each turn, given the encoded conversation history, selects an utterance from a set of candidate utterances suggested by the experts and pass it to the DM agent to execute.

Our contributions to offline RL adapted for MoE-based DM agents are four-fold. First, we exploit the hierarchical structure of MoE-LMs, allowing our offline RL methods to work with a significantly smaller, finite action space, making the RL problem more tractable. Second, by leveraging pre-trained MoE-LMs--which generate coherent utterances--and _regularization techniques_ from offline RL that align the DM's behavior with that of the primitive LM--the proposed RL algorithms can focus on higher-level dialogue planning. The proposed combination results in higher data efficiency than standard RL methods by delegating the responsibility of language fluency to be handled by the MoE-LMs. Third, by using the diverse semantic representations of MoE-LMs, our methods operate at the sentence embedding space and have much simpler critic and actor updates. This circumvents the word-level credit-assignment issue, particularly challenging in long conversations (Salen et al., 2020). Fourth, in contrast to the findings of Verma et al. (2022), where offline RL agents tend to lack utterance diversity (due to potential reward hacking and optimization of a single objective), our MoE-based DM agents by design are adept at generating utterances that reflect different intents.

We begin with a brief introduction of LMs, the MoE-LM architecture, and the use of Markov decision processes (MDPs) in DM in Section 2. We then describe the pre-training procedure for MoE-LMs--which encode diverse semantics and generate fluent utterances capturing specific intents--in Section 3. We derive state-of-the-art (SOTA) offline RL algorithms for training MoE-LMs in Section 4, and three MoE-LM specialized offline RL algorithms in Section 5. Finally, in Section 6, we evaluate our algorithms in open-domain dialogues against their ability to generate utterances with diverse intents and their overall DM performance.

## 2 Preliminaries

Language Models (LMs)In this work, we employ seq2seq LMs (Sutskever et al., 2014) to generate the next utterances in a dialogue. We assume access to a dataset of the form \(=\{(^{(k)},Y^{(k)})\}_{k=1}^{||}\), where each \(\) is an \(L\)-turn conversation history \(=\{X_{l}\}_{l=0}^{L-1}\), wherein \(X_{l}\) is the utterance in a conversation at turn \(l\) and \(Y\) is the next utterance. We define \(N_{}\) to be an upper bound on the length (number of tokens) of each utterance \(X_{l}\) in \(\).3 The role of a LM is to predict the probability of the next utterance \(Y\), consisting of \(N\) tokens, conditioned on the conversation history \(\), i.e., \((Y=\{y_{n}\}_{n=1}^{N})\). In the transformer architecture (Wolf et al., 2019), a LM first encodes the conversation history \(\) using an encoder \(\) to a \((L N_{})\)-length sequence of embeddings\(\{(z_{l,0},,z_{l,N_{}-1})\}_{l=0}^{L-1}\), where each \(z_{l,n}\) is a vector in the latent space induced by the encoder \(\). For notational convenience, we concatenate these embeddings into a single embedding \(z^{d}\), where \(d\) is the overall dimension of the latent space. The next utterance \(=\{_{n}\}_{n=1}^{N}\) is then sampled, token-by-token, from a decoder \(\), i.e., \( z:=_{n=1}^{N} _{n}_{0},,_{n-1};z\), where \(_{0}\) is a fixed initial (start-of-sentence) token (Chien and Kuo, 2019) and the latent state is denoted as \(z=()\).

#### Markov Decision Processes (MDPs)

have been used to model dialogue management (DM) problems in a variety of settings (Li et al., 2016; Asadi and Williams, 2016; Jaques et al., 2019). In such MDPs, denoted by \(=(,,P,r,s_{0},)\), the state space \(\) represents the tokenized conversation history and the initial state \(s_{0}\) is the initial user's query. The action space \(\) is the tokenized language space with each action \(a\) represents one possible next utterance of the agent. The transition kernel \(P\) models the distribution over the user's response to the action taken by the agent (bot) and current conversational context. Finally, the reward function \(r\) measures the user's satisfaction as a function of the conversation until the most recent step. In these MDPs, we can think of the LM as a policy that maps conversation histories to the next utterances. The goal is to find a policy \(^{*}\) with maximum expected discounted return, i.e., \(^{*}\!\!_{}J_{}\), where \(J_{}\!:=\![_{k=0}^{}^{t}r_{t}\,|\,P,s_{0},]\). Note that the size of the tokenized state and action spaces grows exponentially with the vocabulary size. This makes it intractable to solve MDPs of this type, even for a medium-sized vocabulary.

Mixture-of-expert Language Models (MoE-LMs).Chow et al. (2022) recently demonstrated promising results using MoE-LMs to enrich a bot's utterances and improve DM (see Figure 1 for a sketch of the MoE-LM architecture). These results were achieved mainly due to (i) learning a language representation that captures different semantics (_primitive discovery_), (ii) a machinery, called _expert construction_, that embeds different intents into sub-models of this LM, so that they can behave appropriately when prompted, and (iii) a compositional dialogue manager module that comprehends the conversation and determines which response deems most appropriate.

For _primitive discovery_, one first learns a language model \(_{0}:=(,_{0},)\) that consists of a _stochastic encoder_\(_{0}\), where \(\) is an encoder mapping tokenized conversation histories \(\) to a latent space \(^{d}\) and \(_{0}(z^{}|z):=_{0}(z),_{0}^{2}(z) _{d d}\) is a Gaussian distribution, and a decoder \(\). \(_{0}\) predicts the next utterance \(_{0}\) (token-by-token) conditioned on \(z^{}\) sampled from the latent distribution \((_{0}|z^{})\), i.e., \(z^{}_{0}(|z)\). We overload our notation to denote the _primitive_ by \(_{0}(Y|):=_{z^{}_{0}( |z),z=()}[(Y|z^{})]\), which predicts the next utterance accurately and also has strong generalization in \(\) over a diverse set of possible utterances.

Given a primitive \(_{0}\), the algorithm learns \(m\) expert distributions \(\{_{i}\}_{i=1}^{m},\ \ _{i}(z^{}|z)=_{i}(z), _{i}^{2}(z)_{d d}\), each corresponds to a particular personality/intent and generates samples in specific parts of the latent space \(\). This results in \(m\) LMs, \(\{_{i}\}_{i=1}^{m},\ _{i}:=(,_{i},)\), each serving as an _expert_ that generates one or more candidate next utterances \(_{i}\) that are relevant to the

Figure 1: (Left) The MoE-LM architecture (Chow et al., 2022). Step 1: \(\) encodes conversation history. Step 2: \(_{i}\), \( i\), generate candidate bot utterances. Step 3: The compositional dialogue manager \(\) selects the bot response by \(Q\)-score ranking and post-processing. (Right) Sample utterance workflow generated by a MoE-LM trained with Reddit data.

conversation \(\), and also compatible with its respective personality/intent. The compositional DM \(\) takes the encoded conversation history \(z=()\) and candidate action utterances generated by the experts \(\{_{i}\}_{i=0}^{m}\) as input, and selects one of them to execute, i.e., \(Y( z,\{_{i}\}_{i=0}^{m})\). Given the state \(s=\) and action \(a=Y\), the MoE-LM policy that optimizes the DM MDP can be expressed as

\[_{}(a|s)=_{\{_{i},z_{i}^{}\}_{i=0}^{m}}\!\!\!\!\! (a|(s),\{_{i}\}_{i=0}^{m})\;_{i=0}^{m}d(_{i}|z_{i }^{})\;d_{i}(z_{i}^{}|(x)).\] (1)

## 3 Warmstarting the Mixture-of-Expert LM

The MoE-LM approach effectively reformulates the reinforcement learning (RL) dialogue management problem, resulting in considerably smaller action spaces. This is achieved as the DM agent is now re now required to select the most appropriate utterance for presentation to the user from a finite, predefined set of candidate utterances, rather than generating responses word by word, hence allowing the agent to focus on optimizing the specific goal of the conversation task (as candidate utterances are separately optimized to follow particular bot-based characteristics/intents). Recall that the DM is a policy conditioned on both the latent state and the actions suggested by the experts. Before introducing the different RL methods for DM (Sections 4 and 5), in the following, we outline (i) the learning of diverse semantics (primitive LM) for conversation histories, which allows the agent to generate a wide variety of utterances, and (ii) the construction of specialized LMs (experts), which generate utterances with different intents.

Following the primitive discovery procedure in Chow et al. (2022), we learn the primitive LM, \(_{0}\), by solving the following KL-constrained optimization problem that aims at capturing diverse semantics:

\[_{(,_{0},),}\;}_{z^{} ( z,Y),z=()}-(Y|z^{}) \;\;\;\;}_{z=()} (z^{}|z,Y)||_{0}(z^{}|z)\!\! _{}.\] (2)

In (2), \(}\) is the empirical expectation over \((,Y)\) in the dataset \(\), \(\) is a distribution over the latent space conditioned on the encoded conversation history \(z\) and the target utterance \(Y\), and \(_{}\) is a positive real-valued threshold. The distribution \((|z,Y)\) is a Gaussian \(_{}(z,_{}(Y)),_{}^{2}(z,_{} (Y))_{d d}\) in which \(_{}\) is a pre-trained encoder for the target utterance \(Y\), and the mean \(_{}(,)\) and variance \(_{}^{2}(,)\) are trainable models. Note that by solving (2), we maximize the log-likelihood of sentence \(Y\) for a context and latent generation, while enforcing consistency between the latent variable \(z^{}\) predicted by \(_{0}(|z)\) and \((|z,Y)\) via the KL constraint. In practice, we implement the KL constraint in (2) as a penalty weighted by a chosen coefficient.

To complete the MoE framework, one needs to train a set of experts \(\{_{i}\}_{i=1}^{m}\), each generating candidate utterances of different intents. By viewing each expert as a distribution of particular behaviors in conversation data \(\), we leverage the results in Chow et al. (2022) and adopt a universal encoder-decoder \((,)\) among all the experts. In this view, each expert \(i\) is a distribution \(_{i}(|z)\) over certain regions of the latent space \(\). We train each \(_{i}(|z)\) by solving

\[_{_{i}}\;}_{z^{}_{i}( |z),z=(),Y(|z^{})}[-_{i}(,Y)],\] (3)

where \(_{i}(,Y)\) is a real-valued label that _characterizes_ the intent of expert \(i\). We can think of \(_{i}(,Y)\) as a score assigned to \(Y\) resembling how strongly \(Y\) exhibits the trait expert \(i\) is meant to represent. Each expert is learned via _reward-maximization_, where \(_{i}\) is treated as an intent-aligned reward signal for expert \(i\). Note that there is a connection between the above approach and contextual bandits (Chu et al., 2011), where both the context and action spaces are the latent space \(\), and the bandit policy is the latent distribution \(_{i}\). The choice of greedy reward maximization is to encourage a particular behavior in the expert's immediate utterance rather than controlling its future utterances.

Long-term dialogue planning is handled by the compositional dialogue manager. For example, with Gaussian experts \(\{_{i}\}_{i=1}^{m}\), we can use the standard REINFORCE algorithm (Sutton et al., 1999a), where the model parameters \((_{i},_{i})\) are updated in the direction \(_{z^{}_{i}(|z),Y( |z^{})}[_{i}(,Y)_{\{_{i},_{i}\}} _{_{i}}(z^{}|z)]\) with learning rate \(>0\). To reduce the variance of these estimates, we can also adopt the baseline variance reduction technique (Greensmith et al., 2004).

RL for Mixture-of-Expert DM

In offline RL, the policy is learned from the collected conversations \(\), without further online interactions. This potentially allows RL DM methods to leverage the abundance of offline conversational data for policy learning. Let \((,Y,X_{+})\) be a tuple sampled from the offline conversation data \(\), with \(X_{+}\) being the follow-up user response. We can formulate this tuple as a MDP data by defining \(s:=\), \(a:=Y\), \(r(X_{+})\), and \(s_{+}:=(,Y,X_{+})\) as the state, action, reward (w.r.t. the follow-up user response), and next state. A standard offline RL algorithm is \(Q\)-learning (Watkins and Dayan, 1992) that solves: \(_{Q}\ _{(s,a,r,s_{+})}[(r+_{a_{+}}Q(s_{+},a_{+}) -Q(s,a))^{2}]\).

However, with a large action space, the inner maximization (also termed as greedification) in \(Q\)-learning is generally computationally intractable. Furthermore, since one cannot ensure that the greedy \(a_{+}\) is sampled from the same action distribution as in the offline RL dataset (an issue worsened by the large action set), such a covariate shift in the sampling distribution can cause an overestimation bias for the \(Q\) estimate. To alleviate these issues, we propose to leverage the warm-started MoE-LM (Section 3), where the diverse semantic representation and the expert LMs are learned separately. This is crucial to make our offline RL DM problem tractable as the language fluency is captured by the MoE-LM, while our RL-based DM focuses on higher-level planning strategies. In the following, we describe how this can be achieved via different offline RL algorithms.

**Offline RL Methods for MoE-LMs:** One approach to address the aforementioned offline RL issues is _entropy regularization_(Haarnoja et al., 2018; Carta et al., 2021), which regularizes the greedification step to ensure the learned policy is either diverse enough or close to the behavior (data-generation) policy (e.g., through Shannon entropy or KL divergence between these policies). Recall that the primitive LM, \(_{0}\), models the utterance distribution in \(\) and the state-action-reward-next-state tuple \((s,a,r,s_{+})\) of the DM MDP. With the following latent states generated by the primitive LM: \(z=(s)\), \(z_{a}=((s,a))\), and \(z_{+}=(s_{+})\), we define the latent conversation data \(()\) as a collection of \((z,z_{a},r,z_{+})\) tuples. With Shannon-entropy regularization, we can utilize the _soft actor critic_ framework (Haarnoja et al., 2018) to develop RL updates for the _value function_\(V(z)\), _state-action value function_\((z_{a})\), and _latent generator_\((z^{}|z)\), which is initialized with the primitive latent expert \(_{0}\). This framework minimizes the following losses:

\[L_{Q} =_{(z,z_{a},r,z_{+})()}[(r+ V_ {}(z_{+})-Q(z_{a}))^{2}]\] (4) \[L_{V} =_{z(),(,z^{}) (.|z)}Q_{}(z_{})-(z^{}|z)-V(z)^{2}\] (5) \[L_{} =_{z(),(,z^{}) (.|z)}[Q(z_{})-(z^{}|z)]\,,\] (6)

where the critic, \((V,Q)\), takes any encoded conversation history as input and predicts the corresponding cumulative return, \(>0\) is the entropy temperature, \((V_{},Q_{})\) are the target value networks, \(z^{}(.|z)\) is the latent sample generated by \(\), \((z^{})\) is the utterance sampled from \(\), and finally \(z_{}=((,))\) is the corresponding latent state.

From a _hierarchical RL_ perspective (Sutton et al., 1999; Saleh et al., 2020), the latent generator behaves like a high-level policy, whose latent sample \(z^{}\) is used to generate a bot utterance via \(\)-decoding (with the primitive decoder \(\) acting as the low-level policy). Extending the above RL updates to the case of relative-entropy (KL) regularization can be straightforwardly done by replacing the term \((z^{}|z)\) with \(((z^{}|z)/_{0}(z^{}|z))\), since the primitive LM approximates the behavior policy and the encoder-decoder pair \((,)\) is shared among the experts.

Multiple techniques in value-function parameterization have been employed to tackle the overestimation bias. Fujimoto et al. (2018) proposed maintaining two \(Q\)-functions, and a _dual_\(Q\)-_function_ chooses the minimum value between them to avoid overestimation. Jaques et al. (2019) applies dropout in the \(Q\)-function to maintain an _ensemble_ of \(Q\)-values, and outputs the minimum value to avoid overestimation. By utilizing these methods within the MoE-LM framework, we can propose the following variants of offline RL algorithms: (i) **SAC** that uses a dual \(Q\)-function and the actor-critic updates in (4)-(6), (ii) **EnsQ** that uses an ensemble of \(Q\)-functions and the same updates; and (iii) **KLC** that uses an ensemble of \(Q\)-functions and a latent KL-regularized actor-critic update.

Apart from the actor-critic approach that iteratively improves the value functions and policy, Implicit \(Q\)-Learning (IQL) (Kostrikov et al., 2021), a value-based offline RL algorithm, has recently shown success in tackling various problems, including task-oriented dialogue management (Snell et al., 2022). Within our MoE-LM framework, we propose the **IQL**-DM algorithm, whose value function \(V(z)\) minimizes the following loss: \(L_{V}=_{(z,z_{a})()}[L_{2}^{}(Q_{}(z_{ a})-V(z))]\), where \(L_{2}^{}\) is the expectile regression operator (Roenker and Hallock, 2001) of estimating the top-\(\) expectile statistics. The \(Q\)-function of IQL-DM is updated identically to that of actor-critic in (4), which estimates \(Q(z_{a}) r+ V(z_{}})\) via a least-square loss (Bradtke and Barto, 1996). The \(V\)-function estimates the top-\(\) quantile of the state-action \(Q(z_{a})\) random variable at every latent state \(z\). As \(\) approaches one, \( 1\), the IQL updates converge to the optimal \(Q\)-function, \(Q^{*}(z_{a})\), i.e., \(_{(z_{a},r,z_{+})()}[(r+_{a_{+}}Q ^{*}(z_{+,a_{+}})-Q^{*}(z_{a}))^{2}] 0\), where \(z_{+,a_{+}}=((,a,X_{+},a_{+}))\) for any next-action utterance \(a_{+}\). Intuitively, IQL reverses the generalization capacity of critic functions to estimate the value of the best action without directly querying the values of unseen actions. This makes it less conservative than most offline RL methods that constrain the policy's actions to be in-distribution via behavior regularization (e.g., **SAC**, **EnsQ**, **KLC**).

**Auto-regressive Decoding in Actor-Critic:** The actor-critic methods (SAC, EnsQ, KLC) ameliorated the two issues in offline RL to a certain extent (the inner maximization is replaced with \(V\)-function learning and covariate shift is controlled by policy entropy regularization). However, implementing these methods (Eqs. 5-6) entails sampling utterances from the current policy, i.e., \(\), which involves expensive auto-regressive LM decoding at every training update. To resolve this issue, one may empirically replace \(\) with a _teacher-forcing_(Toomarian and Bahren, 1995) variant \(_{}(a)\), which replaces auto-regressive decoding with a one-step generation from the bot utterance \(a=Y\) in \(\). This will further restrict the policy update of \(\) to be close to the behavior policy. In contrast, since IQL does not perform explicit policy updates, it directly circumvents this expensive auto-regressive sampling operation of \(\).

**DM Construction in MoE-LMs:** Recall that in an MoE-LM, the DM policy \(\) takes the encoded conversation history \(z=()\), the \(m+1\) candidate action utterances generated by the experts \(\{_{i}\}_{i=0}^{m}\), and selects one of them to execute, i.e., \(a( z,\{_{i}\}_{i=0}^{m})\). Given the \(Q\)-function \(Q(z_{a})\) learned by any of the above offline RL algorithms, we extract the DM policy \(\) via softmax greedification over the finite set of MoE candidate utterances, i.e., \((a z,\{_{i}\}_{i=0}^{m})( Q(z_{a}))\), where \(>0\) is the policy temperature. This DM policy uses the \(Q\) function to score different candidate utterances and returns an utterance based on the likelihood of these scores.

## 5 Mixture-of-Expert Offline RL

In Section 4, we presented how state-of-the-art offline RL methods are adapted to the MoE framework, which can have limitations due to being agnostic to the model architecture. Recall that MoE dialogue management is a specialized hierarchical reinforcement learning (HRL) problem, which optimizes over a restricted class of DM policies defined by the convex hull of expert policy set (whose weights are defined by the DM policy \(\)). This problem is of great interest because it reduces the original RL DM problem, with a combinatorial action space, into one that has a much smaller finite action set. In the following, we leverage the _mixture-of-policy_ structure and develop offline RL algorithms that specifically target this HRL problem.

**Stochastic-action IQL (SAIQL):** The first approach applies IQL to the discrete, stochastic set of candidate action utterances \(\{_{i}\}_{i=0}^{m}\) as generated by the MoE experts. Equipped with the latent conversation data \((D)=\{(z,z_{a},r,z_{+})\}\) (see Section 4) and the latent expert policies \(\{_{i}\}_{i=0}^{m}\) in the MoE-LM, we propose a DM algorithm, whose value function \(V(z)\) minimize the following loss:

\[L_{V}=_{i=0}^{m}_{z,_{i} _{i}( z)}L_{2}^{}Q_{}\!}( z_{_{i}})-V(z),\] (7)

where \(z_{_{i}}=((,_{i}))\) is the latent state that corresponds to the action utterance sampled from the \(i\)-th expert, \(L_{2}^{}\) is the expectile regression operator, and the \(Q\)-function is updated as in Eq. 4. To incorporate the maximization over candidate utterances in IQL, we compute the expectile regression over the joint latent state and expert policy distributions.

However, unlike the standard IQL DM algorithm, which avoids autoregressive decoding for policy execution, SAIQL requires auto-regressive sampling of all \(m+1\) candidate utterances. Suppose the augmented latent conversation data \((D)_{}=\{(z,z_{a},r,z_{+},\{z_{_{i}}\}_{i=0}^{m})\}\), which also includes the set of latent expert actions \(\{z_{_{i}}\}_{i=0}^{m}\), is available. One straightforward way to circumvent this issue is by replacing the expectation over experts with the realized candidate utter ances, i.e., by approximating the value function in SAIQL with its unbiased empirical average \(_{i=0}^{m}_{(z,\{z_{_{i}}\}_{i=0}^{m})( )_{}}[L_{Y}^{2}(Q_{}(z_{_{i}})-V(z))]\).

While having access to candidate utterances is not standard in IQL, it is necessary here to allow \(Q\)-learning to exploit quantile regression over _realized_ candidate utterances (an approach shown to be sound in Boutilier et al. (2018)). Therefore, we termed this method _stochastic action_ IQL (**SAIQL**) to reflect the stochastic action sets used in IQL training. Once **SAIQL** converges, the DM policy is also constructed as a softmax of \(Q\)-values applied to each candidate utterance.

The **MoE-MDP** is defined as \(}=(},},,,_{0},)\), where the state space is the product of the learned latent space \(\) and the joint action space of the \(m+1\) experts, i.e., \(}=^{m+1}\); the action space consists of the \(m+1\) experts, i.e., \(}=\{0,,m\}\); the initial state \(_{0}\) is the encoding of the initial user's query and the utterances suggested by the experts in response to this query; the transition kernel models both the user's responses and the next experts' actions; and finally the reward is the same as in the original MDP. Since MoE-MDP has a finite number of actions, learning a policy \(\) is equivalent to solving a finite-action MDP, i.e., \(^{*}_{}J_{}:=[_{k=0}^{} ^{t}_{t},_{0},]\).

**Follow-the-Leading-Expert (FiLE):**Banijamali et al. (2019) showed that the MoE-MDP problem is NP-hard but can be approximated by \(_{^{m+1}}_{i=0}^{m}(i)V^{i}(z)+( })\), where \(V^{i}\) is the value function of the \(i^{}\) expert and \((})>0\) is a surrogate function that depends on the experts' stationary distributions. However, computing these distributions is generally intractable as the experts are LMs themselves. This motivates our heuristic **FiLE** algorithm, which ignores the second term in training a set of expert critic functions and picks the best action at each step. To efficiently parameterize the critic function, similarly to the architecture used in DQN (Mnih et al., 2013) for discrete-action RL, we define a \((m+1)\)-headed critic function, where each head represents the value of following an expert's policy. To train the multi-headed critic, we modify the standard critic losses as

\[L_{Q}=_{i=0}^{m}_{z,z_{a},r,z_{+}}(r+ V^{i}_{}(z_{+})-Q^{i}(z_{a}))^{2},\ \ \ L_{V}=_{i=0}^{m}_{z,_{i}_{i}(|z) }(Q^{i}_{}(z_{_{i}})-V^{i}(z))^{2},\] (8)

where \(Q^{i}\) and \(V^{i}\) represent the critic-function head for expert \(i\). To overcome the auto-regressive sampling issue in (8), we relabel the offline conversation data \(\) by assigning action utterances to train the critic function(s) whose corresponding expert(s) most likely generate those utterances. Specifically, consider the following V-function loss:

\[L_{V}=_{i=0}^{m}_{z,z_{a},V}_{i=i(z,Y)} Q^{i}_{}(z_{a})-V^{i}(z)^{2},\] (9)

where \(_{i=i(z,z_{a})}\) selects the expert based on the best log-likelihood \(i(z,Y):=_{i}(Y|z^{i,})\) with \(z^{i,}_{i}(|z)\). After learning the critic functions, the **FiLE** DM policy can be constructed as \((a z,\{_{i}\}_{i=0}^{m})( Q^{i(z,a)}(z_{a}))\).

**Value-based RL for MoE-MDP (MoE-VRL)**: Consider a \((m+1)\)-headed value function \(\) of the MoE-MDP, where each head represents the optimal value by choosing the corresponding expert's action. Applying standard DQN, this function can be learned by minimizing the following loss:

\[L_{}=_{z,Y,r,z_{+}}r+_{_{+}} _{}(z_{+},i_{+})-(z,i(z,Y))^{2},\] (10)

where \(_{}\) is the target \(\)-network. For simpler exposition, we only use the partial MoE-MDP states of encoded conversations in the above DQN loss and omit the candidate action utterances. Extending to the full MoE-MDP state is straightforward but is omitted for brevity. The inner maximization over \(i_{+}\) can be computed explicitly because the MoE-MDP action space of expert indices is finite and small. Here, \(i(z,Y)\) is the same index function that attributes utterance \(Y\) to the expert most likely to generate it (based on likelihood). With the optimal value function \(^{*}(z,i)\), the MoE-MDP policy picks the best expert \(^{*}(z):=_{i}^{*}(z,i)\), and the DM policy can be constructed as \((a z,\{_{i}\}_{i=0}^{m})( Q^{^{*}(z)} (z_{a}))\), where \(Q^{^{*}(z)}(z_{a})\) is the critic of the optimal expert.

## 6 MoE-based DM Experiments

We evaluate our MoE-based offline RL algorithms on two open-domain benchmarks common in the RL-based dialogue management literature (Jaques et al., 2019). The first one is the Cornell Moviecorpus (Danescu-Niculescu-Mizil and Lee, 2011), which consists of conversations between speakers in different movies. The second is the Reddit Casual (Ghandeharioun et al., 2019) conversations dataset, which is a subset of the Reddit corpus that only contains casual conversations.

**Environment:** We perform the experiment by having DM agents interact with a DialoGPT (Zhang et al., 2019) simulated-user environment. The task is to maximize user satisfaction, which is measured by the user's overall sentiment. To construct an immediate reward, we set \(r(X_{+}):=_{}(X_{+})\), where \(_{}(X)\) is a RoBerTa-based sentiment classifier (Liao et al., 2021), which assigns a score from \([-1,1]\) that is inversely proportional to the (negative) positive sentiment prediction probabilities.

We pre-train the MoE-LM with either the Cornell or Reddit dataset and construct \(10\) experts (i.e., \(m=9\), plus the primitive expert), each corresponding to an individual intent in open-ended dialogues, including "empathy", "optimism", "cheerfulness", "contentment", "dejection", "rage", "sorrow", "questioning", "exploration", etc. See Appendix B for details. The conversation lasts for \(5\) turns (with \(=0.8\)), where each turn entails a query/response from the user followed by an agent's utterance. During the agent's turn, each expert generates \(5\) candidate utterances, thus resulting in a total of 50 candidate utterances. To evaluate the methods, we measure the return of the trajectory generated by different agents via \(_{_{0}}[_{i=0}^{4}^{i}r(X_{i+1}) |Y_{i}(.|_{i}),X_{i+1} P_{}(.| _{i},Y_{i})]\).

**Evaluation:** We employ two evaluation approaches, namely (i) a model-free approach that only utilizes the learned \(Q\) function to score candidate utterances, and where the DM policy selects the action utterance based on a softmax likelihood; and (ii) a model-based approach that uses the Value function (\(V\)) along with a learned next-user utterance model \(P_{}(X_{+}|z_{V})\), that optimizes the following loss: \(L_{P_{}}=_{(z_{a},r),_{+} P_{ }(.|z_{a})}[(r-r(_{+}))^{2}]\). We first approximate the \(Q\) function via \(Q(z_{a}) r(_{+})+ V(_{+})\), where \(_{+}\) denotes the next user utterance sampled from \(P_{}(.|z_{a})\), then use that function to score candidate utterances, and, finally have the DM policy select the action utterance analogously. Human evaluation is also conducted on the DM performances of different offline RL agents. More details and results can be found in Appendix D.

**Experiment 1: SOTA Offline RL with MoE-LMs:** The goal of this experiment is to investigate the effectiveness of SOTA offline RL algorithms. In these experiments, we only make use of the primitive language model \(_{0}=(,_{0},)\) to generate sample utterances. To simulate previous works using single policy settings, we fine-tune the latent base distribution \(_{0}\) for policy optimization while keeping the encoder-decoder (\(,\)) fixed. As mentioned in Sec. 4 we deploy the following offline RL algorithms to train the DM policy \(\) of MoE-LMs: (i) **SAC**(Haarnoja et al., 2018) with a dual \(Q\) function critic (Fujimoto et al., 2018); (ii) **EnsQ**, which utilizes an ensemble of \(Q\) functions (Jaques et al., 2019) with actor-critic; (iii) **KLC**(Saleh et al., 2020), which utilizes the dual \(Q\) function and applies KL regularization between the latent policy \(\) and the primitive policy \(_{0}\)

    &  &  \\   & Model Free & Model Based & Model Free & Model Based \\  IQL & \(0.53 0.47\) & \(\) & \(-\) & \(\) \\ SAC & \(\) & \(4.13 0.21\) & \(-1.55 0.19\) & \(0.36 0.26\) \\ EnsQ & \(0.10 0.40\) & \(4.06 0.25\) & \(-1.51 0.20\) & \(0.21 0.21\) \\ KLC & \(0.31 0.46\) & \(3.69 0.37\) & \(-1.46 0.21\) & \(-0.07 0.25\) \\ BC & \(-0.65 0.41\) & & \(-2.18 0.36\) \\ Bandits & \(\) & & \(1.3 0.17\) \\   

Table 1: SOTA offline RL methods

    &  &  \\   & Model Free & Model Based & Model Free & Model Based \\  EXP 1* & \(0.97 0.52\) & \(4.25 0.12\) & \(-1.32 0.19\) & \(1.47 0.15\) \\ SAIQL & \(0.81 0.42\) & \(\) & \(-1.34 0.25\) & \(2.61 0.24\) \\ FLE & \(\) & \(4.59 0.07\) & \(\) & \(3.51 0.19\) \\ MoE-VRL & \(0.72 0.47\) & \(4.46 0.10\) & \(-0.58 0.24\) & \(\) \\   

Table 2: MoE specific offline RL methodsi.e., \(_{(|z)}[((z^{}|z)/_{0}(z ^{}|z))]\) in the actor-critic algorithm update 4; (iv) **IQL**(Kostrikov et al., 2021), which adopts the idea from Q learning to estimate an optimal \(Q\) function in the MoE-LM latent space. To our knowledge, our work is among the first that makes use of IQL for open-domain dialogue management. These methods have been implemented in ways where the original idea has been preserved, making the comparison fair to the original works. With each learned \(Q\) function, the bot picks the final action by sampling from a softmax distribution of \(Q\) scores overall candidate utterances. To demonstrate the efficacy of offline RL methods, we also include results from Behavior Cloning (**BC**) as well as simple reward maximization (**Bandit**) (i.e., \(=0\)) for comparisons.

Table 1 presents the results of our experiments with these methods in the open-dialogue system, where a 5-turn conversation was generated. The table displays the mean return over 100 conversations with their respective standard errors. Our experiments demonstrate that model-based evaluation can significantly improve dialogue management over the model-free counterpart, even with a next-user LM that is much simpler than the Dialog-GPT user. Among most model-based and model-free evaluations, we found that **IQL**, originally designed to tackle offline RL problems, outperforms other RL methods. This performance can be attributed to IQL's ability to (i) alleviate \(Q\) overestimation errors due to co-variate shifts; (ii) estimate the optimal values without being overly conservative w.r.t. the behavior policy, and (iii) avert the auto-regressive utterance sampling issues in training.

Interestingly, we also found that **KLC** and **EnsQ**, two standard methods in RL-based DM, struggled to achieve satisfactory performance in our experiments. This may be due to the fact that applying dropout (for ensemble \(Q\)) and KL regularization in the fixed MoE-LM latent space makes DM algorithms overly conservative. In contrast, **SAC** successfully learns a well-performing model-free DM policy but fails in the model-based regime, potentially demonstrating its instability in critic-function learning. **BC** also fails to provide any satisfactory performance on any of the domains, and surprisingly, **Bandit** method or plain reward maximization did as well as **IQL**, pointing to the fact that maybe the offline RL methods being used or not exactly helping in planning at all.

**Experiment 2: MoE-specific Offline RL:** In this experiment, we explore the benefits of leveraging the MoE framework for training offline RL agents in open-domain conversational systems. Building upon the insights from our previous experiment (Experiment 1), we propose several modifications to standard Offline RL algorithms to take advantage of the MoE framework. As mentioned in Sec. 5, we developed the following MoE-specific offline RL algorithms for DM: (i) **SAIQL**, which extends IQL to incorporate the multiple candidate utterances generated by the experts; (ii) **FtLE**, which learns a DM policy to follow the best expert policy at each step (estimation of the experts' long-term values is done concurrently with a multi-headed critic architecture and data relabeling) and (iii) **MoE-VRL**, which learns an optimal meta-value function over the space of experts. Leveraging the MoE-MDP formulation, solving which leads to an optimal DM policy that provides the optimal sequences of expert policy switching. We aim to evaluate the potential of these MoE-specialized offline RL algorithms over off-the-shelf offline RL methods in DM.

Table 2 shows the return observed similar to the ones displayed in Table 1. The first row in the table displays the best performance across all methods from Experiment 1, for comparison. Our results demonstrate the efficacy of the proposed methods that utilize the structure of the MoE framework in dialogue management. All the methods that used all experts while training (**SAIQL**, **FtLE**, and **MoE-VRL**) outperformed the SOTA offline RL methods, indicating that an offline RL algorithm that takes the candidate utterances into account can generally improve dialogue planning. Moreover, making the RL algorithms attuned to the multiple-expert structure (i.e., **FtLE** and **MoE-VRL**) indeed results in even better DM performance, emphasizing the benefits of reformulating the DM MDP using the HRL paradigm, where the DM policy is optimized over a restricted class of finite-action policies. Also, we note that only MoE-aware offline RL methods were actually able to outperform simple per-step greedification (i.e. **Bandit**) which hints to the fact that they were actually able to plan ahead and perform long-term credit assignments to optimize return. Whereas all the standard offline RL methods failed to do that (Table 1). Using multiple critic functions to separately estimate the value of different experts also allows us to better understand their long-term utility (of the corresponding intents) and how they affect the conversation quality. Overall, these findings highlight the potential of the MoE-specific offline RL methods to improve dialogue management performance.

**Human Evaluation:** Table 3 summarize the results of around \(600\) ratings provided by \(80\) on the bots' quality, in terms of fluency, and conversation-level sentiment improvement on the Reddit Casualdataset. We can similarly notice in this case that MoE specific offline RL methods seems to perform better both in terms of fluency and sentiment improvement over standard offline RL. The complete details of the experiment can be found in Appendix D.

**Experiment 3:** In this experiment, one aims to investigate the effectiveness of selecting different experts during dialogue management. To this end, we conduct a study where we measure the frequency with which utterances from different experts are selected throughout the conversation. Specifically, we wish to understand the diversity of intents selected by different offline RL algorithms.

Given approximately \(200\) conversation turns, we measure the frequency of the expert agents when their utterances are selected and present the frequency metric for the worst performing offline RL method (**EnsQ**), a good performing method (**IQL**), and an MoE-specific RL algorithm (such as **MoE-VRL**). To visualize our findings, we plot a histogram of the frequencies of different experts being selected and calculate the KL divergence of the histogram against a uniform distribution over the experts. While the uniform distribution may not be the optimal distribution of utterances, it provides a measure of how well the agents make use of different experts, along with their performance.

Figure 2 illustrates the result for the above experiment for the Cornell dataset, in the model based setting. We observe that the worst performing agent, **EnsQ**, has a highly skewed distribution of expert selections, with a few experts being heavily favored over others. This suggests that **EnsQ** is less diverse and does not effectively utilize the full range of expert capabilities available. On the other hand, both **IQL** and **MoE-VRL** exhibit a more balanced distribution of expert selection, with utterances chosen from multiple experts throughout the conversation; i.e., their frequency distributions are closer to a uniform distribution, with much lower KL divergence distance.

However, note that there is a clear performance gap between **MoE-VRL** and **IQL** where former significantly outperforming the latter. This highlights the importance of incorporating the MoE framework to better utilize the intent of different experts in dialogue planning, rather than relying on generating a diverse set of candidate utterances. Overall, these results suggest that encouraging diversity in intents and better utilizing expert knowledge in planning can improve DM performance.

## 7 Concluding Remarks

By leveraging the recent advances of MoE-LMs, we developed a suite of offline RL-based DM algorithms. Our methods significantly reduce the action space and improve the efficacy of DM. To understand how well our offline RL approaches generate diverse utterances and solve DM problems, we evaluated them on two open-domain dialogue tasks and compared them with SOTA offline RL baselines. Our results showed that by exploiting the MoE-LM structure, our specialized offline RL DM methods (i) improve the diversity of intents in bot utterances; (ii) have better sample efficiency; and (iii) yield better overall performance in both the model-based and model-free settings. Our work provides important insights on how to create scalable RL-based DM methods that train chatbots to achieve dialogue tasks and enhance user satisfaction. Future work includes fine-tuning the experts (i.e., low-level policies) with offline RL, learning the optimal semantic representation for hierarchical RL, preventing dialogue agents from generating harmful behaviors (e.g., by enforcing safety constraints in the RL algorithms), and evaluating our DM methods on more realistic problems, such as customer support, conversational recommendation, and persuasion.

   Method & Avg. Fluency & Sentiment \\  BC & \(0.67 0.26\) & \(0.24 0.50\) \\ KLC & \(0.62 0.27\) & \(0.66 0.47\) \\ IQL & \(0.84 0.24\) & \(0.72 0.46\) \\ SAIQL & \(0.81 0.19\) & \(0.57 0.50\) \\ FLE & \(\) & \(\) \\ MoE-VRL & \(0.72 0.28\) & \(0.70 0.45\) \\   

Table 3: Human raters evaluation

Figure 2: Experiment on the Cornell dataset with Model-based evaluation(a) Histogram of frequency of expert selection. (b) KL divergence against uniform distribution.