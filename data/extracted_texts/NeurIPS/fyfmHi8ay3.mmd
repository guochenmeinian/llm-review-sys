# Template-free Articulated Neural Point Clouds for

Reposable View Synthesis

 Lukas Uzolas Elmar Eisemann Petr Kellnhofer

Delft University of Technology

The Netherlands

{l.uzolas, e.eisemann, p.kellnhofer}@tudelft.nl

###### Abstract

Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when synthesizing novel views of time-evolving 3D scenes. However, the common reliance on backward deformation fields makes reanimation of the captured object poses challenging. Moreover, the state of the art dynamic models are often limited by low visual fidelity, long reconstruction time or specificity to narrow application domains. In this paper, we present a novel method utilizing a point-based representation and Linear Blend Skinning (LBS) to jointly learn a Dynamic NeRF and an associated skeletal model from even sparse multi-view video. Our forward-warping approach achieves state-of-the-art visual fidelity when synthesizing novel views and poses while significantly reducing the necessary learning time when compared to existing work. We demonstrate the versatility of our representation on a variety of articulated objects from common datasets and obtain reposable 3D reconstructions without the need of object-specific skeletal templates. The project website can be found at https://lukas.uzolas.com/Articulated-Point-NeRF/.

## 1 Introduction

Synthesizing novel photo-realistic views of captured 3D scenes is important for many domains including virtual/augmented reality, video games or movie productions. In recent years, Neural Radiance Fields (NeRFs)  have proved their remarkable capacity to represent complex view-dependent effects, captured in photographs and videos, sparsely sampled from natural light fields . Follow-up works have extended the scope to dynamic scenes , facilitating rendering of unseen views at different timestamps. Despite progress in reconstruction quality and speed , manipulating learned scenes remains a challenge, but would be a highly desirable feature, as it can enable downstream applications, such as the creation of avatars for virtual presence or 3D assets for games and movies.

The key challenge of reposing a NeRF is inverting the backward-warping function that maps individual observations to a shared canonical representation . It is an inversion of traditional kinematic animation, such as Linear Blend Skinning (LBS) , where a canonical shape is forward-warped to a desired pose. Such inverse mapping often requires resolving ambiguous situations as it is difficult to guarantee bijectivity (see Fig. 2). A common remedy are parametric templates, typically built for narrow application domains, such as human heads and bodies , but they are difficult to generalize. Alternatively, object shapes can be retrieved from videos as ensembles of geometric parts, yet existing techniques provide limited image-synthesis fidelity . Our work aims to combine these different lines of work and enable joint learning of the NeRF representation and its pose parameterization from sparse or dense multi-view videos. We aim for time-efficient class-agnostic view synthesis of reposable models with high image-synthesis quality without access to a template or pose annotation, which is a combination not currently covered by existing work (see Table 1).

Recent work by Noguchi et al.  addresses a similar problem. However, we exploit the structure-free nature of point-based NeRF representations , enabling direct forward warping of the canonical object to any desired pose, while maintaining the capacity and flexibility of NeRF-like rendering to capture highly detailed image features. Together with our automatically extracted and jointly-optimized LBS skeletal pose parameterization, our work allows for faster training and achieves state-of-the-art visual quality in a much shorter time. Finally, we demonstrate how to easily expose learned representations of varied objects by directly editing joint angles of the forward LBS model.

To summarize, we make the following contributions: 1) We propose a novel method for learning articulated neural representations of dynamic objects using forward projection of neural point clouds. 2) We demonstrate state-of-the-art novel view synthesis for a variety of multi-view videos with training times lower than comparable methods. 3) We jointly learn a skeletal model without domain-specific priors or user annotations and demonstrate reposing of the reconstructed 3D objects.

  
**Method** & **Pose** & **Generic** & **Fidelity** & **Training** \\ 
**LASR** & No & Yes & Shape & \( 2\) h \\
**VISER** & No & Yes & Shape & \(2\)–\(12\) h \\
**BANMO** & No & Yes & Low & \( 12\) h \\
**D-NeRF** & No & Yes & High & \( 12\) h \\
**TINeuVox** & No & Yes & High & \( 2\) h \\ 
**CASA** & Yes & No & Shape & \( 2\) h \\
**LASISE** & Yes & Yes & Low & \( 12\) h \\
**WIM** & Yes & Yes & High & \( 12\) h \\
**Ours** & Yes & Yes & High & \( 2\) h \\   

Table 1: A comparison of representative dynamic 3D representations learned from 2D videos. We analyze reposibility, agnosticism to object class, image synthesis fidelity and training time. Papers that demonstrate reposing are shown below the bar. “_Shape_” denotes shape-only reconstruction without texture details.

Figure 1: Overview of our method: a) First, we pre-train a NeRF backbone to initialize a feature point cloud \(P^{c}\) for a selected canonical timestamp and to extract an initial skeleton. b) During the main training stage, \(P^{c}\) is forward-warped using LBS consisting of learned time-invariant skinning weights \(_{i}\) and time-dependent pose transformations from an MLP regressor \(_{r}\). The image is obtained by integration and decoding of features aggregated from points along each camera ray. In summary, we fine-tune the initial neural point features \(}\), skinning weights \(_{i}\), joints \(J\), density and color regressor \(_{d}\) and \(_{c}\) of the backbone. We fully train the pose regressor \(_{r}\) and feature point decoder \(_{p}\) from scratch. In test time, we modify the pose transformations to obtain novel poses.

Figure 2: Examples of poses difficult for backward warping. (a) Ill-defined projection from an observation to the canonical space. Both ambiguous solutions (green and magenta) correctly pass through the semantically corresponding surface points B and C. (b) Projection ambiguity for points of contact between two surfaces. Note that in contrast, it is trivial to forward warp the object points from a well-chosen canonical space to any observation space.

Related Work

Neural Radiance Fields and ParameterizationsImplicit neural networks have recently emerged as a powerful tool for building differentiable representations of 3D scenes [24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47] and for learning view-dependent _Neural Radiance Fields_ (NeRFs) [1; 2; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57] enabling photo-realistic novel view synthesis. Their high computational cost has been dramatically reduced by spatial decomposition [58; 59; 59; 60] and hybrid representations [61; 62; 63; 64; 65].Recently, neural point clouds have combined expressive neural functions with the flexibility of explicit geometry [66; 67; 68; 69; 21]. We also use neural point clouds, but unlike previous work, we jointly learn a forward kinematic model along with the associated soft part segmentation and, thus, convert fast voxel-based representations to reposable NeRFs.

Dynamic Neural Radiance FieldsEquivalent to 2D video, dynamic NeRFs capture temporal changes in scenes by encoding each frame separately , expanding the radiance field into the temporal domain [71; 72; 7; 8], or time-dependent interpolation in a compact latent representation [9; 10; 6]. Alternatively, a single canonical NeRF can be animated by backward warping from the canonical space to individual time-varying observation spaces [3; 4; 5; 6; 9; 11]. However, such warps rely on the bijectivity of the mapping which is difficult to guarantee for all observed poses (see Fig. 2). Forward mapping only requires a single well-posed canonical representation, which we exploit.

Object ReposingDirectly reposing high-dimensional deformation fields of dynamic NeRFs is impractical. Instead, parametric templates can sparsely represent prominent deformation modes for faces [13; 14], bodies [15; 16], hands , and also non-human objects, such as animals . Together with skeleton-based LBS, they enable articulated neural representations of human heads [75; 76; 77; 78; 79; 80; 81] or bodies [82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97], as well as modeling distributions in generative models [98; 99; 100; 101]. However, the assumption of piece-wise rigid motion and the availability of a skeletal template restrict the applicable object classes. The former issue has been addressed by physically inspired but computationally expensive deformation models . To remedy the latter issue, the skeletal model can be retrieved from a database , adapted from a generic graph [19; 103], fitted as a template to 2D observations [104; 105; 106; 107; 108] or fully learned during a 3D shape recovery [109; 20]. An external cage can also be used for re-animation of static NeRFs . Alternatively, a surface embedding can be learned without a skeleton if reposing is not required [17; 18; 22].

Similarly to Noguchi et al. , we jointly learn a 3D object representation, a skeletal model, and observed poses. However, we utilize a point-based NeRF representation to benefit from its computational efficiency, ease of geometric transformation, and capacity to learn complex light fields.

We share our point-based approach with a contemporaneous work _MovingParts_ which, however, relies on inverting the backward flow. We demonstrate the reconstruction of large transformations in the _Robots_ dataset , not demonstrated in _MovingParts_. Furthermore, our approach does not enforce binary segmentation and, hence, can represent non-rigid part transitions (see Fig. 6 right).

## 3 Preliminaries

Our method builds upon available NeRF reconstruction methods as a backbone for learning a reposable dynamic representation. In principle, any NeRF-like representation is a suitable initial point for our training. In practice, we use TiNeuVox  in all experiments, as it enables fast reconstruction with both sparse and dense multi-view supervision in dynamic scenes. Next, we describe the core concepts of NeRF  and TiNeuVox  to provide context for our method in Sec. 4.

Time-Aware Neural VoxelsNeRF  maps a point \(=(x,y,z)\) and view direction \(=(,)\) to color \(=(r,g,b)\) and density \(\). The mapping function often takes form of a Multi-Layer-Perceptron (MLP) \(\), such that \((,)=(,)\). The color of an image pixel \(C()\) can be obtained via volume rendering along the ray \(r(t)=+t\).

TiNeuVox  expands NeRF to dynamic scenes and significantly increases training speed by utilizing an explicit volume representation. To this extent, each point \(\) at time \(t\) in observation space is backward-warped to canonical space as

\[^{c}=_{b}(},}),}=(),\;}=_{t}((t)),\] (1)Here, \(\) is the positional encoding , \(_{t}\) a small color-decoding MLP, \(_{b}\) a backward-warping MLP, and \(}\) and \(}\) the encoded point and time respectively. The canonical point \(^{c}\) is used to sample a feature vector \(_{m}^{V}\) from the explicit feature volume \(^{X Y Z V}\) by means of trilinear interpolation \(interp\) with varying stride \(s_{m}\) at each scale: \(_{m}=(interp(^{c},,s_{m}))\). The feature vectors across all scales are concatenated as \(=_{1}..._{m}..._{M}\), where \(\) is the concatenation operator. Finally, the view-dependent color and density are regressed by additional MLPs \(_{f}\), \(_{d}\) and \(_{c}\):

\[=_{f}(,},}),=_{d}(),=_{c}(,).\] (2)

The volume rendering equation  integrates density/color of points \(_{i}\) sampled along the ray \(r\):

\[()=_{i=1}^{N}T_{i}(1-exp(-_{i}_{i}))_{i},T_{i}=exp(-_{j=1}^{i-1}_{j}_{j})),\] (3)

Here, \(_{i}\) is the distance between \(_{i}\) and \(_{i+1}\). The feature volume \(\) and all MLPs are optimized end-to-end and supervised by the Mean Squared Error (MSE) and the corresponding ground-truth pixel color \(C()\): \(_{photo}=||()-C()||_{2}^{2}\).

Note that because \(_{f}\) (Eq. 2) is conditioned by the time \(}\), space and time are entangled and the feature volume \(V\) does not represent a single static canonical shape. Therefore, we cannot directly learn a forward warping function to invert \(_{b}\) as proposed by Chen et al. . In the next section, we show that this is not necessary and we can learn a forward kinematic model directly without relying on specific properties of the backbone, which will allow us to replace it in the future.

## 4 Method

While dynamic NeRF models such as TiNeuVox  can reproduce motion in a dynamic scene, they do not enable reposing. Furthermore, it is impractical to post hoc reparametrize their motion representation due to the ambiguities of backward flow inversion (see Fig. 2).

We instead propose a completely new representation based on a neural point cloud and an explicit kinematic model for forward warping, and we use the backbone only as an initialization for our training procedure (see Fig. 1). First, we extract a feature point cloud from a selected canonical frame of the backbone Sec. 4.1. Second, we describe the underlying kinematic model of our 3D representation and its initialization Sec. 4.2. Consecutively, we introduce how the 3D point cloud is warped from a canonical space to an observation space and rendered Sec. 4.3. Lastly, we describe our losses Sec. 4.4.

### Canonical Feature Point Cloud

We first pre-train a backbone NeRF model (TiNeuVox ) using its default parameters. To initialize the feature point cloud, we sample the canonical density function of TiNeuVox \(=_{d}()\) (Eq. 2) on a uniform coordinate grid and discard empty samples through thresholding. The grid resolution is adaptively chosen, such that \(|P| 10000\), similarly to other explicit NeRFs . After thresholding, we obtain points \(P=\{_{i}|i=1,...,N\}\). Furthermore, we extract a feature vector \(_{i}\) for each point \(_{i}\) (see Eq. 2).

### Kinematic Model

We forward-warp our feature point cloud from canonical space to observation space using an LBS kinematic model to match the training images. Here, we describe how we initialize, use, and simplify our kinematic model.

Skeleton InitializationWe do not rely on a class-specific template to initialize our kinematic skeleton. Instead, we extract an approximate initial skeleton tree by Medial Axis Transform (MAT) and refine it during training.

Let \(M=\{_{m}|m=1,...,M\}\) be the set of medial axis points extracted by applying a MAT on \(P^{c}\). We choose the root of our kinematic model \(_{root}\) as the medial axis point that is closest to all other medial-axis points, i.e., \(_{root}:=\ _{j}||_{i}-_{j}||_{2}, _{i} M,i j\).

Next, we leverage dense sampling-grid neighborhoods and define a graph \(G_{M}\) connecting neighboring points in \(M\). Points disconnected from \(_{root}\) are removed. We then use a heuristic to select sparse joints (nodes) as a subset of \(G_{M}\) and define the bones (edges) based on their connectivity. To this extent, we traverse \(G_{M}\) starting from \(_{root}\) in a breadth-first manner and mark \(_{m}\) as a joint \(_{b}\) if its geodesic distance from the preceding joint exceeds a threshold \(B_{length}=10\). As a result, we obtain a set of time-invariant canonical joint positions \(J=\{_{b}\}\) which we further optimize during training.

While this skeleton is usually over-segmented, we show in our experiments that this does not hamper the training. We propose a pruning strategy to enable easier pose manipulation afterwards (see Fig. 7).

Blend Skinning WeightsFor each point \(_{i}\) we initialize its raw skinning weight vector \(}_{i}\) by an exponential decay function of the distance \(dist\) to each bone line \(b_{j}\) such that \(_{i,j}=1/e^{dist(_{i}^{c},b_{j})}\). Before skinning, we obtain the final blend skinning weight vector \(_{i}\) through scaling by a global learnable parameter \(\) and applying a softmax: \(w_{i,j}=e^{e^{i_{i,j}}/}/_{k}e^{e_{i,k}/}\). During the training, we optimize the initial \(}_{i}\) as well as \(\). Accounting for the per-point weights, we define our full canonical feature point cloud as \(P^{c}=\{(_{i}^{c},_{i},}_{i})|i=1,...,N\}\).

Point WarpingWe forward-warp the canonical point cloud \(P^{c}\) to an observation space of timestamp \(t\) via LBS . The local transformation matrix \(_{b}^{t}\) of each bone \(b\) is defined by a rotation \(R_{b}^{t}\) around its parent joint \(_{b}\). Consequently, each point \(_{i}^{c}\) is transformed by a linear combination of bone transformations as:

\[_{i}^{t}=_{i}^{t}p_{i}^{c}=_{b=1}^{|B|}w_{i,b}T_{b}^{t} _{i}^{c},T_{b}^{t}=T_{p_{b}}^{t}_{b}^{t}_{b}^{t}= R_{b}^{t}&_{b}+R_{b}^{t}_{b}^{-1}\\ &1,\] (4)

where \(T_{b}^{t}\) is defined recursively by its parent bone \(p_{b}\). \(T_{p_{b}}^{t}\) of the skeleton root is identity.

We express rotations \(R_{b}^{t} SO(3)\) using the Rodrigues' formula, where \(}=/||||\) is the axis of rotation. However, we learn the rotation angle \(\) directly as an additional parameters because we find it leads to a better pose regression than using \(=||||\). The time-dependent rotations \(_{b},_{b}\) for each bone \(b\) are regressed by an MLP: \(_{r}((t))=_{1}^{t},_{1}^{t},...,_{b}^{t}, _{b}^{t},...,_{B}^{t},_{B},_{r}^{t},_{t} ^{t},_{r}^{t}\), where \(_{r}^{t}\), \(_{r}^{t}\) and \(_{r}^{t}\) are the time-dependent root rotation and translation.

Skeleton simplificationThe initial skeleton's over-segmentation does not hamper pose reconstruction during training, but we optionally simplify the skeleton after training to ease pose editing (see Fig. 7). We prune or merge bones based on the size of rotation angles \(_{b}^{t}\) produced by the transformation regressor \(_{r}\). Joints, which do not exhibit a rotational change from the rest pose above the threshold of \(t_{r}\) deg in more than 5% of the observed timestamps, are marked static. We then merge the bones of such joints and their corresponding weights \(}\). See the supplement for details.

### Dynamic Point Rendering

We adopt the point cloud rendering from  but extend it to explicitly model rotational invariance of the radiance field. For each sampling point \(x\), we find up to \(N=8\) nearest observation feature points \(_{i}^{t}\) within a radius of \(0.01\) and roto-translate them into their canonical frames. This enables the feature embedding MLP \(_{p}\) to learn spatial relations in a pose-invariant coordinate frame as:

\[_{i,x}^{t}=_{p}(_{i},x_{_{i}}^{t}),x_{_{i}}^{t}=(R_{i}^{t^{-1}}(x-_{i}^{t})),\] (5)

where, \(R_{i}^{t}\) is the rotation component \(_{i}^{t}\) (Eq. 4) for point \(_{i}^{t}\) and \((.)\) is the positional encoding .

The neighboring embeddings \(_{i,x}^{t}\) are aggregated by inverse distance weighting, which produces the final feature input for \(_{d}\) and \(_{c}\) (see Eq. 2) and the consequent volume rendering (Eq. 3):

\[_{x}^{t}=_{i}^{N}}{_{j}^{|B|}d_{j}}f_{i,x}^{t}, d_{i}=||p_{i}^{t}-x||^{-1};=_{d}(_{x}^{t});\ =_{c}(_{x}^{t},).\] (6)

### Losses

Next to the photometric loss \(_{photo}\) (Sec. 3), we utilize a 2D chamfer loss to penalize the difference between the point cloud projected into a training view \(I(P^{t})\) and the corresponding 2D ground truth object mask \(M^{t}\): \(_{mask}:=_{chamf}(I(P^{t}),M^{t})\). The chamfer loss is defined as:

\[_{chamf}(P^{1},P^{2})=|}_{i}^{|P^{1}|}_{j}|| _{i}^{1}-_{j}^{2}||_{2}^{2}+|}_{j}^{|P^ {2}|}_{i}||_{i}^{1}-_{j}^{2}||_{2}^{2}.\] (7)

To prevent the skeleton from diverging too much from the medial axis \(M\), we further minimize the chamfer loss between \(M\) and the joints \(J\) (see Sec. 4.2): \(_{skel}:=_{chamf}(M,J)\). In addition, we minimize the transformation angles and the root translation: \(_{tranf}=(_{i}|_{i}^{t}|)+|_{r}^{t}|\). Local rigidity is further enforced upon points after deformation via as-rigid-as-possible regularization:

\[_{arap}=_{i}^{|P^{t}|}_{j}^{N(p_{i})}|||p_{i}^{c}-p_{j}^{c} ||_{2}^{2}-||p_{i}^{t}-p_{j}^{t}||_{2}^{2}.\] (8)

Lastly, we apply two regularizers on the blend skinning weights. To encourage smoothness, we penalize divergence of skinning weights in the rendering neighborhood \(N\): \(_{smooth}=_{i}^{|P^{t}|}_{j N(p_{i})}|w_{i}-w_{j}|\), and, to encourage sparsity, we apply:

\[_{sparse}=-_{i}^{|P^{c}|}_{j}^{B}w_{i,j}(w_{i,j})+(1-w_ {i,j})(1-w_{i,j}).\] (9)

In total, our training loss is \(=_{0}_{photo}+_{1}_{mask}+ _{2}_{skel}+_{3}_{tranf}+_{4} _{smooth}+_{5}_{sparse}+_{6}_{ ARAP}\) where \(=\{200,0.02,1,0.1,10,0.2,0.005\}\) in our experiments.

## 5 Experiments

Here, we evaluate our work, which obtains state-of-the-art view-synthesis quality with a lower training cost than other articulated methods. We also demonstrate class-agnostic reposing capability and we evaluate contribution of method components in an ablation study. Video examples can be seen on the project website.

DatasetsWe chose three multi-view video datasets that are commonly used for the evaluation of dynamic multi-view synthesis methods and pose articulation. First, the _Robots_ dataset  features multi-view synthetic videos of 8 topologically varied robots, making it well suited for testing pose articulation performance (see Fig. 4). We use 18 views for training and 2 for evaluation. Second, the _Blender_ dataset  is a sparse multi-view synthetic dataset with 5 humanoid and 2 other articulated objects1. Its visual quality benchmarks are used to test the fidelity of image detail reconstruction (see

Figure 3: Qualitative comparison displaying two held-out views-frames of scenes from the _Robots_ rendered by WIM  and our method after 2 and 10 hours of training, and the PSNR scores.

Fig. 6 right). We use the original training-test split. Third, _ZJU-MoCap_ dataset  is a common test suite for dynamic human reconstruction and we evaluate 5 of its multi-view sequences with the same 6 training views as used in _Watch-It-Move_. Finally, we evaluate image quality using peak signal-to-noise ratio (PSNR), structural similarity (SSIM) , and learned perceptual image patch similarity (LPIPS)  image metrics.

Implementation detailsWe pre-train the TiNeuVox  backbone using the authors' implementation and an additional distortion loss , as implemented in . Our method is implemented in Pytorch and we train each scene using the Adam optimizer for 160k (_Blender_ and _Robots_) or 320k (_ZJU-MoCap_) iterations with a batch size of \(8192\) rays, sampled randomly from multiple views and a single timestamp. We choose the canonical timestamp by visual inspection, and gradually increase the number of observed timestamps during training. We adjust ray sampling and scheduling for the _ZJU-MoCap_ dataset (see the Supplement). All experiments were done on a single Nvidia GPU RTX 3090Ti. See the supplementary materials for details. For more details, see the Appendix.

BaselinesWe compare our method to state-of-the-art non-articulated and articulated methods for high-fidelity multi-view video synthesis (see Table 1 for an overview). _D-NeRF_ extends NeRF to the temporal domain by backward warping a static canonical NeRF. _TiNeuVox_ improves performance of _D-NeRF_ using voxel grids. _Hexplane_ and _K-Planes_ decompose to the space-time volume to several hyper-planes. Finally, _Watch-It-Move_ (WIM) jointly learns a surface representation and LBS model for articulation. Note, that because we aim at time-efficient

  
**Method** & **Reposable** & **PSNR\(\)** & **SSIM\(\)** & **LPIPS\(\)** \\  D-NeRF  & No & 30.50 & 0.95 & 0.07 & 20 hours \\ TiNeuVox-B  & No & 32.67 & 0.97 & 0.04 & 28 mins \\ Hexplane  & No & 31.04 & 0.97 & 0.04 & 11.5 mins \\ K-Plane hybrid  & No & 31.61 & 0.97 & - & 52min \\  WIM\({}^{*}\) & **Yes** & 23.81 & 0.91 & 0.10 & 11 hours \\ Ours\({}^{*}\) & **Yes** & 29.10 & 0.94 & 0.06 & 2.5 hours \\   

Table 2: Quality of unseen view synthesis for the _Blender_ dataset . Results of D-NeRF and TiNeuVox-B reprinted from Fang et al. . \({}^{*}\)Without the “Bouncing balls” scene.

Figure 4: Quality of unseen view synthesis during training with 95% confidence intervals in the _Robots_ dataset . The initial plateau of WIM  matches the 10k warm-up steps used by the authors before training with all data. Our onset time corresponds to the 70 minutes required for pre-training of the backbone. Training of our method was terminated after \(2.5\) hours.

Figure 5: Effect of the backbone initialization pre-training steps on the final result of our method when trained on the _Jumping Jacks_ scene from the _Blender_ dataset. Our final choice of 20k iterations corresponds to approximately 70 minutes of real time.

learning, training of WIM was stopped after 11 hours (i.e., 80k of the original 400k optimization steps).

Novel view synthesisWe provide results for the _Robots_ view synthesis without the skeleton simplification; quantitatively in Fig. 4 and qualitatively in Fig. 3. More visual examples are available in the supplement. After the initial pre-training, our method quickly surpasses the image quality of WIM . The mean image scores obtained for our method after 2 hours of training (incl. pre-training) are higher than those that WIM achieves after 10 hours. We attribute this to NeRF's capability to approximate even complex shapes, which are difficult to fit using the signed distance function utilized by WIM. Nevertheless, for visually simple objects with long nested pose transformation chains, this capacity might encourage false explanations of the articulation and cause artifacts, as visible in Fig. 6 left. However, this is not a common issue. We illustrate it in the _Blender_, where WIM struggles to represent fine details (see Fig. 6 right), while our method achieves image scores close to those of non-reposable baselines (see Table 2 and the supplement).

Furthermore, Fig. 5 shows that our image quality is not highly dependent on the pre-training phase. We observe that mere 100 iterations of _TiNeuVox_ pre-training provide an initialization sufficient for achieving PSNR scores over 30 dB by our method. However, while fine geometric details are still recovered, the coarse initial shape limits the skeleton complexity and, therefore, we opt for 20k pre-training steps in all other experiments.

ZJU-MoCapIn Fig. 10, we compare our method to WIM in the _ZJU-MoCap_ dataset. We observe that both methods are able to recover the 3D shape and the skeletal motion despite the difficulty of an accurate fine texture detail reconstruction. This can partly be attributed to imperfections in camera calibrations (see Supplement F of ) and partly by soft deformations of clothes which are modeled neither method. Notably, with a small modification our method learns to partially compensate for this. In Ours\({}^{pose}\), we condition the feature embedding network \(_{p}\) by the skeletal poses jointly learned from scratch by our method. This improves the image quality by modeling the residual deformations. See the Supplement for details. Finally, Ours\({}^{SMPL}\) shows that replacing our skeleton in Ours\({}^{pose}\) with a ground truth from an annotated SMPL template  does not affect the performance. This validates our skeleton initialization based on Medial Axis Transform.

Figure 6: While our model well reproduces details and non-rigidity of the _Dinosaur_ (right), it can fail for complex motions combining long chains of rotations with texture cue ambiguity (left).

Figure 7: Learned LBS weights and skeleton: a) After training. b) After additional post-processing (weight merging and skeleton pruning, see Sec. 4.2).

RepposingIn Fig. 7, we visualize the learned blend-skinning weights \(}_{i}\) and skeletons with and without the skeleton simplification. The algorithm is able to substantially reduce the skeleton complexity, while largely preserving semantic articulations observed in the training data. However, it is not able to remove all unnecessary skeletal branches when complex geometry is present (see Fig. 7 right). In Fig. 8, we show that such post-processed skeletons allow for animating of novel poses by smoothly interpolating between user-defined poses. More animations can be found in the Supplement.

Loss ablationOur experiments suggest the regularization does not improve the image quality, but it improves the quality of the kinematic model, which is important for our main goal of reposing (see Fig. 9). Specifically, \(_{ARAP}\) avoids non-rigid distortions (a), \(_{smooth}\) leads to a more semantically meaningful part segmentation (b), \(_{skel}\) encourages functional placement of the skeleton joints inside the object volume even for thin parts (c), and \(_{tranf}\) leads to a reduction of necessary object parts after simplification (\(62\) components are removed instead of \(49\) for _Atlas_). More details are provided in the Supplement.

BackboneThe _TiNeuVox_ backbone allows us to transfer parameters learned during the pre-training and initialize the features \(_{i}\) and the regressors \(_{d}\) and \(_{c}\). Interestingly, our experiment shows that such off-the-shelf features \(_{i}\) often do not need any further fine-tuning (see Table 4). Despite this, our method is agnostic to the backbone choice by design and the end-to-end training procedure of the entire model is essential for this. We demonstrate it by training with a random initialization. This way, only the point positions \(_{i}^{c}\) obtainable by any 3D reconstruction method are needed. Table 3 shows that this leads to only a negligible drop in reconstruction quality in the _Robots_ dataset. Here, the weight of \(_{skel}\) (\(w_{2}=1\)) was adjusted to prevent drift of the skeleton.

Figure 8: Reposing using the simplified skeleton. Interpolation from canonical to novel pose.

Figure 10: Comparison in the _ZJU-MoCap_ dataset. Ours: Our full method. Ours\({}^{pose}\): Ours with an additional pose-conditioned feature embedding network \(_{p}\). Ours\({}^{SMPL}\): Ours\({}^{pose}\) with ground truth SMPL  skeleton.

Figure 9: Ablation examples. a) \(_{ARAP}\) enforces rigidity after pruning (see upper pipes), b) \(_{smooth}\) results in better part-segmentation, c) \(_{skel}\) enforces the joints to stay inside the shape.

## 6 Discussion

Limitations and Future WorkWe demonstrate fast learning of articulated NeRFs for state-of-the-art view synthesis and straightforward skeletal reposing for objects with piece-wise rigid pose transformations. While the LBS allows for fitting partially non-rigid deformations (see Fig. 6 right), representing fully non-rigid, topologically varying, and multi-component objects would benefit from a higher-dimensional parameterization. Chen et al.  aim in that direction, but an intuitive and cost-effective reposing still favors skeleton-based techniques such as ours.

The performance of our method depends on the quality of the backbone reconstruction. While TiNeuVox  supports reconstruction from even sparse data, a different backbone could offer a more robust starting geometry and improve the skeleton initialization. Although our approach works well even for highly structured shapes and thin structures (see Fig. 6 right), we observe incorrect joint placement for objects with long chains of highly nonlinear geometrical transformations (see Fig. 6 left). Moreover, our approach successfully reduces the number of extraneous bones but it is not able to completely eliminate all superfluous skeletal elements (see Fig. 7).

Our method is restricted to the kinematic motion space exhibited in the training sequences. While extrapolation is possible to some extent, the proposed method is not able to generalize to arbitrary unseen poses. Finally, we focus on image synthesis for user-defined poses rather than a direct fitting of unseen skeletal poses from images or motion capture. An inverse fitting of skeletal poses to novel 2D observations or transfer of poses from one object to another remain opportunities for future research.

ConclusionWe presented a method for fast learning of articulated models for high-quality novel view synthesis of captured 3D objects. Our forward-warped neural point clouds avoid the pitfalls of backward warping and elegantly integrate a skeleton-based LBS. As a result, our method merges straightforward reposing even of strongly animated inputs, as present in the _Robots_ dataset, with high visual fidelity of NeRF rendering. Our work is a significant step towards low-cost acquisition of animatable 3D objects for games, movies, and education.

Ethical ConsiderationsOur method renders novel views and poses of 3D objects including human bodies. However, we do not focus on this class and we note that many human-specific methods exist (see Sec. 2). Nevertheless, we acknowledge that our method could potentially be used to produce fake images of people and that research is needed to understand the risks and their mitigation.

  & **PSNR\(\)** & **SSIM\(\)** & **LPIPS\(\)** \\ 
**Full Method** & 29.10 & 0.973 & 0.041 \\
**No Fine-tuning** & 29.16 & 0.971 & 0.045 \\ 

Table 3: Results with a _TiNeuVox_ initialization and with a random initialization of the features \(_{i}\) and the regressors \(_{d}\) and \(_{c}\) in the _Robots_ dataset.

  & **PSNR\(\)** & **SSIM\(\)** & **LPIPS\(\)** \\ 
**Full Method** & 29.10 & 0.973 & 0.041 \\
**No Fine-tuning** & 29.97 & 0.971 & 0.045 \\ 

Table 4: Results with and without fine-tuning of the feature points \(_{i}\) in the _Blender_ dataset.