# Model Spider: Learning to Rank Pre-Trained Models Efficiently

Yi-Kai Zhang\({}^{1}\), Ting-Ji Huang\({}^{1}\), Yao-Xiang Ding\({}^{2}\), De-Chuan Zhan\({}^{1}\), Han-Jia Ye\({}^{1,\$}\)

\({}^{1}\)National Key Laboratory for Novel Software Technology, Nanjing University, China

\({}^{2}\)State Key Lab of CAD & CG, Zhejiang University

{zhangyk,huangtj,zhandc,yebj}@lamda.nju.edu.cn yxding@zju.edu.cn

###### Abstract

Figuring out which Pre-Trained Model (PTM) from a model zoo fits the target task is essential to take advantage of plentiful model resources. With the availability of numerous heterogeneous PTMs from diverse fields, _efficiently_ selecting the most suitable one is challenging due to the time-consuming costs of carrying out forward or backward passes over all PTMs. In this paper, we propose Model Spider, which _tokenizes_ both PTMs and tasks by summarizing their characteristics into vectors to enable efficient PTM selection. By leveraging the approximated performance of PTMs on a separate set of training tasks, Model Spider learns to construct representation and measure the fitness score between a model-task pair via their representation. The ability to rank relevant PTMs higher than others generalizes to new tasks. With the top-ranked PTM candidates, we further learn to enrich task repr. with their PTM-specific semantics to re-rank the PTMs for better selection. Model Spider _balances efficiency and selection ability_, making PTM selection like a spider preying on a web. Model Spider exhibits promising performance across diverse model zoos, including visual models and Large Language Models (LLMs). Code is available at https://github.com/zhangyikaii/Model-Spider.

## 1 Introduction

Fine-tuning Pre-Trained Models (PTMs) on downstream tasks has shown remarkable improvements in various fields , making "pre-training \(\) fine-tuning" the de-facto paradigm in many real-world applications. A model zoo contains diverse PTMs in their architectures and functionalities , but a randomly selected PTM makes their helpfulness for a particular downstream task vary unpredictably . One important step to take advantage of PTM resources is to identify the most helpful PTM in a model zoo -- estimating and ranking the transferabilities of PTMs -- with the downstream task's data _accurately and efficiently_.

Which PTM is the most helpful? A direct answer is to enumerate all PTMs and evaluate the performance of their corresponding fine-tuned models. However, the high computational cost of the backward steps in fine-tuning makes this solution impractical. Some existing methods estimate proxies of transferability with only forward passes based on the target task's features extracted by PTMs . Nowadays, a public model zoo often contains hundreds and thousands of PTMs . Then, the computational burden of forward passes will be amplified, let alone for the time-consuming forward passes of some complicated PTMs. Therefore, the _efficiency_ of searching helpful PTMs and estimating the transferability should be further emphasized.

In this paper, we propose Model Spider, the SPecification InDuced Expression and Ranking of PTMs, for accurate and efficient PTM selection. In detail, we tokenize all PTMs and tasks into vectors that capture their _general properties_ and their relationship with each other. For example,two models pre-trained on NABirds  and Caltech-UCSD Birds  datasets may have similar abilities in bird recognition. The comprehension abilities of two models pre-trained on XSum  dataset, Ax-b, and Ax-g datasets of SuperGLUE benchmark  may also be mutually transferable. We can then associate them with similar representation. Then the transferability from a PTM to a task could be approximated by the distance of their repr. _without requiring per-PTM forward pass over the downstream task._ The success of Model Spider depends on two key factors. First, how do we obtain representation for tasks and PTMs? The representation of the most helpful PTM should be close to the task one w.r.t. some similarity measures. Then, will a general task repr. weaken the selection ability since it may ignore specific characteristics of a PTM?

In Model Spider, we _learn_ to construct representation with a general encoder and measure the similarity between them with a Transformer module  in a _supervised learning manner_. We estimate the rankings of PTMs in the model zoo for some historical tasks using rank aggregation. By leveraging the approximated supervision, we pull task representation close to the top-ranked PTM repr. and push unhelpful PTM repr. away based on the transformer-measured similarity. We expect that the ability to tokenize and measure similarity could be generalized to unseen tasks. The difference between Model Spider's representation-based PTM selection with forward-based strategy is illustrated in Figure 1.

The representation generated by general encoders significantly reduces the PTM search time and improves the search performance. If the budget allows, we can extract features of the downstream task by carrying out forward passes over _a part of_ (the top-\(k\) ranked) PTMs, revealing the _specific_ relationship between PTMs and the task. We equip our Model Spider with the ability to incorporate PTM-specific representation, which re-ranks the PTMs and further improves the selection results. In summary, Model Spider is suitable for different budget requirements, where the general and task-specific repr. makes a flexible trade-off between efficiency and accuracy, given various forward passes. Figure 1 illustrates a comparison of PTM selection methods _w.r.t._ both efficiency and accuracy. Our contributions are

* We propose a novel approach Model Spider to tokenize tasks and PTMs, which is able to rank PTMs in a model zoo given a downstream task efficiently and accurately.
* Model Spider learns to tokenize and rank PTMs on a separate training set of tasks, and it can incorporate task-specific forward results of some PTMs when resource budgets allow.
* The experiments demonstrate that Model Spider effectively ranks PTMs and achieves significant improvements on the visual models and the Large Language Models (LLMs).

## 2 Related Works

**Efficient PTM Search with Transferability Assessment.** Whether a selected PTM is helpful could be formulated as the problem measuring transferability from source data pre-training the PTM to the target downstream task [111; 13; 41; 78; 11]. The current evaluation of transferability relies on a

Figure 1: (a) Two strategies for PTM selection. Related works utilize forward-based features and corresponding proxies on the target dataset to evaluate transferability. The representation/specification-based approach with learned model-task pair reduces the requirement for forwarding pass on each PTM. (b) The average efficiency (wall-clock time) _vs_ performance (correlation \(_{w}\), the higher, the better) comparison of PTM selection. The circle sizes indicate the memory footprint. Red circles are Model Spider with different values of the number of PTM-specific features \(k\), while others are comparison methods. Model Spider _balances efficiency and accuracy well._

forward pass of the PTM on the target task, which generates the PTM-specific features on the target task. For example, NCE , LCEP , LogME [113; 114], PACTran , and TransRate  estimate negative conditional entropy, log expectation, marginalized likelihood, PAC-Bayesian bound, mutual information to obtain proxy metric of transferability, respectively. Several extensions including \(\)-LEEP  with Gaussian mixture model on top of PTM features, H-Score  utilizing divergence transition matrix to approximate the transferred log-likelihood, and [25; 71; 84] exploring correlations between categories of target task. Auxiliary information such as source clues [6; 93] and gradients of PTMs when back propagating with few steps [85; 74] are also investigated. Although the transferability assessment methods avoid the time-consuming fine-tuning, the forward costs over PTMs also become heavier given diverse and complicated pre-trained model zoos.

**Relatedness of Task**. Whether a PTM gains improvements after fine-tuning on the downstream task has been verified to depend on the relatedness between tasks both theoretically [10; 11; 60] and empirically [102; 58; 112]. The relatedness could be measured through various ways, such as fully fine-tuning , task vectors , example-based graphs [48; 29; 86], representation-level similarities [30; 3], and human prior knowledge [44; 76]. Instead of utilizing a pre-defined strategy to measure the relatedness, Model Spider constructs the representation of PTMs/tasks in vector forms and learns a similarity between them on historical tasks.

**Learning to rank** predicts the orders of objects usually with a score function , and the experience on a training set could be generalized to unseen data [5; 63]. Additional learned metrics or embeddings further improve the ranking ability [62; 110; 15]. The task relatedness can also be modeled as a learning-to-rank problem, where the preference over one PTM over another could be learned from historical rankings of PTMs. However, obtaining the supervision on the training set requires complete fine-tuning over a large number of historical tasks, which either come from a time-consuming transfer learning experience  or the output from some specially selected transferability assessment methods . We propose a strong and efficient approximation of the PTM ranking supervision on the training set tasks, and a novel representation-based similarity is applied.

## 3 Preliminary

We describe the PTM selection problem by assuming all PTMs are classifiers, and the description could be easily extended to PTMs for other tasks, _e.g._, regression. Then we discuss several solutions.

### Selecting PTMs from a Model Zoo

Consider we have a target classification task \(=\{(_{i},y_{i})\}_{i=1}^{N}\) with \(N\) labeled examples, where the label \(y_{i}\) of each instance \(_{i}\) comes from one of the \(C_{}\) classes. Instead of learning on \(\) directly, we assume there is a model zoo \(=\{f_{m}=_{m}_{m}\}_{m=1}^{M}\) containing \(M\) PTMs. A PTM \(f_{m}\) could be decomposed into two components. \(_{m}\) is the feature extraction network producing \(d_{m}\)-dimensional features. \(_{m}^{d_{m} C_{m}}\) is the top-layer classifier which maps a \(d_{m}\)-dimensional feature to the confidence score over \(C_{m}\) classes.1 PTMs in \(\) are trained on source data across various domains. Their feature extractors \(_{m}\) have diverse architectures, and the corresponding classifiers are pre-trained for different sets of objects. In other words, \(d_{m}\) and \(C_{m^{}}\) may differ for a certain pair of \(m\) and \(m^{}\). A widely-used way to take advantage of a PTM \(f_{m}=_{m}_{m}\) in the target task is to fine-tune the feature extractor together with a randomly initialized classifier over \(\). In detail, we minimize the following objective

\[=}}=*{arg\,min}_{f= }_{i=1}^{N}(^{}(_{i}),\;y_{i} \;\;_{m})\;,\] (1)

where \(\) is _initialized with_\(_{m}\). The fine-tuned \(f\) makes prediction with \(*{arg\,max}_{c[C]}}_{c}^{}}()\). \([C]=\{1,,C\}\) and \(}_{c}\) is the \(c\)th column of \(}\). Then, we can rank the helpfulness of PTMs based on the performance of their fine-tuned models. In other words, we obtain \(_{m}\) following Equation 1 based on the \(m\)th PTM \(f_{m}\), then we calculate the averaged accuracy when predicting over an unseen test set of \(\) (the higher, the better), _i.e._,

\[_{_{m}}=[(y=*{arg\,max}_{c[C]}_{m}())]\;.\] (2)\(_{_{m}}\) is also named as the _transferability_, measuring if the feature extractor \(_{m}\) in a PTM could be transferred well to the target task with fine-tuning . \(()\) is the indicator function, which outputs \(1\) if the condition is satisfied. Given \(_{}=\{_{_{m}}\}_{m=1}^{M}\), _i.e._, the transferability for all PTMs, then we can obtain the ground-truth ranking of all PTMs in the model zoo for task \(\) and select the top-ranked one. In the PTM selection problem, the goal is to estimate the ranking of all PTMs for a task \(\) using \(}_{}=\{}_{_{m}}\}_{m=1}^ {M}\). The evaluation criterion is the similarity between the predicted \(}_{}\) and the ground-truth \(_{}\), typically measured by weighted Kendall's \(_{w}\). We omit the subscript \(\) when it is clear from the context.

### Efficiency Matters in PTM Selection

One direct solution to PTM selection is approximating the ground truth \(_{}\) by fine-tuning all the PTMs over \(\), where a validation set should be split from \(\) to estimate Equation 2. Since fine-tuning PTM contains multiple forward and backward passes, the computation burden is astronomical.

A forward pass of a certain PTM's extractor \(_{m}\) over \(\) generates the features \(^{m}_{}=\{_{m}(_{i})^{d_{m}}\}_{( _{i},y_{i})}\), which is lightweight compared with the backward step. The feature reveals how examples in \(\) are distributed from the selected PTM's view, and a more discriminative feature may have a higher transfer potential. As mentioned in section 2, the existing transferability assessment methods estimate \(_{_{m}}\) based on the PTM-specific feature \(^{m}_{}\) and target labels \(\{y_{i}\}_{i=1}^{N}\). Precise estimation requires a large \(N\), which means we need to collect enough examples to identify the most helpful PTMs from a model zoo.

While the previous forward-based transferability assessment methods reduce the time cost, selecting among \(M\) PTMs in the model zoo multiplies the forward cost \(M\) times, making the estimation of \(}\) computationally expensive. Moreover, since forward passes for complicated PTMs take longer, selecting a PTM _efficiently_, especially given a large model zoo, is crucial.

## 4 Model Spider

In Model Spider, we propose to tokenize PTMs and tasks regardless of their complexity, allowing us to _efficiently_ calculate their relatedness based on a certain similarity measure over their representation, which capture general properties and serve as a specification of a model or task, demonstrating which kinds of tasks a model performs well on or what kind of models a task requires. In this section, we first introduce the process of obtaining repr. by learning from a training set of tasks, and the ability to rank PTMs could be generalized to downstream tasks. We then describe the encoder, the similarity measure, and an efficient way to generate supervision during representation learning. Finally, we discuss how Model Spider can be flexible in incorporating forward pass results of top-ranked PTMs to further improve the representation's semantics and the ranking's quality.

### Learning to Rank PTMs with Representation

In Model Spider, we learn the model repr. \(\{_{m}\}_{m=1}^{M}\), task repr. \(()\), and the similarity measure \((,)\) in a supervised learning manner based on a separate training set \(\). The training set \(\) does not contain overlapping classes with the downstream task \(\).

Specifically, we randomly sample training tasks \(\{_{i}\}\) from \(\). For a given training task \(_{i}\), we assume that we can obtain the ground-truth ranking \(_{_{i}}=\{_{_{m}_{i}}\}_{m=1} ^{M}\) over the \(M\) PTMs, indicating the helpfulness of each PTM. We will discuss the details of obtaining the supervision \(_{_{i}}\) later. We then select PTMs for \(_{i}\) based on the similarity between the task repr. \((_{i})\) and those \(M\) PTM repr. \(\{_{m}\}_{m=1}^{M}\). We expect the higher the similarity, the more helpful a PTM is for the given task. We use \(\) to denote all learnable parameters and optimize \(\) with a ranking loss, which minimizes the discrepancy between the rank \(}_{_{i}}\) predicted by the similarity function and the ground-truth \(_{_{i}}\):

\[_{}_{_{i}}_{}( }_{_{i}}=\{(_{m},( _{i}))\}_{m=1}^{M}\,,_{_{i}})\.\] (3)

Given \(^{M}\), we use an operator \(()\) to index the elements of \(\) in a descending order, _i.e._, \(\,m<l\), we have \(_{(m)}_{(l)}\). \((m)\) is exactly the index of the PTM with \(m\)th largestground-truth score. Based on this, we use the following ranking loss:

\[_{}(},)=_{m=1}^{M}-(}_{(m)})}{_{l=m}^{M}(}_{ {dec}(l)})})\;.\] (4)

Equation 4 aims to make the _whole order_ of the predicted \(}_{_{i}}\) similar to the ground-truth \(_{_{i}}\). So the similarity between the task repr. and that of a higher-ranked PTM indicated by \(_{_{i}}\) should be larger than the similarity with lower-ranked PTM representation. The underlying intuition is that if a PTM performs well on certain tasks, it is likely to generalize its ability to related tasks. For example, if a PTM excels at bird recognition, it may effectively recognize other flying animals.

For a downstream task \(\), we generate its task repr. with \(()\), and identify the close PTM ones with the learned \((,)\). Objective Equation 3 also works when the number of examples in a task is small. By learning to rank PTMs for sampled _few-shot tasks_, Model Spider can rank helpful models even with limited training data. We will show this ability of Model Spider in section 5.

### Model and Task Representation for PTM Selection

We encode the general characteristics of tasks and PTMs via two types of representation.

**Model Representation.** Given a model zoo with \(M\) PTMs, we associate a PTM \(f_{m}\) with a form \(_{m}^{d}\) encoding rich semantics about the aspects in which \(f_{m}\) excels. Models pre-trained from related datasets or those with similar functionalities are expected to have similar representation.

**Task Representation.** A \(C_{}\)-class task \(=\{(_{i},y_{i})\}_{i=1}^{N}\) contains a set of instances and labels. We would like to tokenize a task with a mapping \(()\), which outputs a set of vectors \(()^{d C_{}}\), one for each class. We implement \(\) with one additional _frozen_ encoder \(\) with an equivalent parameter magnitude as the PTMs in the model zoo. \(\) is pre-trained by self-supervised learning methods [17; 33; 53] and captures the semantics of a broad range of classes. In detail, we extract the features of all instances in the task \(\) and take the class centers as the task repr.:

\[()=\{(y_{i} =c)|}_{(_{i},y_{i})}[ (_{i})(y_{i}=c)]\}_ {c[C]}\;.\] (5)

The task repr. expresses the characteristics of a task, _e.g._, those tasks with semantically similar classes may have similar sets of representation.

**Model-Task Similarity.** The helpfulness of a PTM w.r.t. a task, _i.e._, the transferability score, could be estimated based on the similarity of the model-task pairs \(}_{_{m}}=(_{m},())\), and the PTM selection is complemented by embedding the model and tasks into a space and then identifying close PTM repr. for a task. In Model Spider, the \((,)\) is implemented with a one-layer Transformer , a self-attention module that enables various inputs. The Transformer consists of alternating layers of multi-head self-attention, multi-layer perceptron, and layer norm blocks. We set the input of the Transformer as the union set of model and task repr. \(=[_{m},()]^{d (1+C)}\)

Figure 2: An illustration of Model Spider. The middle part (b) shows the workflow of Model Spider, which involves tokenizing both PTMs and tasks into a shared space. Plot (c) demonstrates how the model-task similarity calculated based on the representation helps rank PTMs for a given task. In plot (a), when the budget allows, Model Spider can take advantage of PTM-specific features obtained by performing forward passes of the top-\(k\) ranked PTMs on some selected tasks. This improves the quality of task repr. as well as the PTM ranking.

then the similarity \(}_{_{m}}\) between model and task ones is:

\[(_{m},())= {FC}(()[0])\,\] (6)

where \([0]\) is the first output of the Transformer, _i.e._, the corresponding output of the model representation. We add a Fully Connected (FC) layer to project the intermediate result to a scalar. Learnable parameters \(\), including \(\{_{m}\}_{m=1}^{M}\), FC, and weights of the Transformer, are trained via objective in Equation 3.

### Accelerating Training for Model Spider

The training of Model Spider in Equation 3 requires a large number of (task \(_{i}\), PTM ranking \(_{_{i}}\)) pairs. Although we could collect enough data for each task, obtaining the ground-truth PTMs rankings, _i.e._, the helpfulness order of PTMs for each task, is computationally expensive. In addition, using some proxies of \(_{_{i}}\) may weaken the ability of the Model Spider. We propose a closer approximation of the ground-truth \(_{_{i}}\), which efficiently supervises sampled tasks from \(\).

**Approximated Training Supervision**. We take advantage of the fact that existing PTM selection methods rely on the PTM-specific features \(_{_{i}}^{m}\) to estimate the transferability score w.r.t. \(_{i}\) and produce diverse scores. In other words, a PTM will be placed in different positions based on the scores provided by various methods such as NCE , LEEP , and LogME [113; 114]. Based on their "relatively good but diverse" ranking results, an intuitive approach to estimate the ground-truth \(_{_{i}}\) is to _ensemble_ their multiple ranking results into a stronger single order.

Given \(\{}_{_{i}}^{1},}_{_{i}}^{2}, \}\) as multiple predicted rankings over \(M\) PTMs for a sampled task \(_{i}\), _i.e._, the order sorted by the estimations of transferability via various methods, we take advantage of Copeland's aggregation method [7; 82] to ensemble the orders: \(}_{_{i}}=\{_{_{m}_{i}}\}_{m=1}^{M}=(\{}_{ _{i}}^{1},}_{_{i}}^{2},\})\). Copeland's aggregation compares each pair of ranking candidates and considers all preferences to determine which of the two is more preferred. The output \(}_{_{i}}\) acts as a good estimation of the ground-truth supervision \(_{_{i}}\). The aggregated \(}_{_{i}}\) is more accurate than a particular transferability assessment method, which improves the quality of the supervision in ranking loss in Equation 4.

**Sampling Tasks for Training**. We assume that the training data \(\) contains a large number of classes with sufficient data. To sample tasks for training, we randomly select a set of classes from \(\) and choose a subset of their corresponding examples. Benefiting from the supervision estimation approach \(\), we are able to obtain the aggregated ranking \(}\) for any sampled task.

**Training Complexity**. The training phase in Model Spider is efficient. First, we pre-extract features \(\{_{}^{m}\}_{m=1}^{M}\) for \(\) with all PTMs in advance. Then only the computational burden of base transferability assessment methods, rank aggregation methods, and the optimization of top-layer parameters are involved. Furthermore, training tasks with the same set of classes share the same \(}_{_{i}}\).

### Re-ranking with Efficiency-Accuracy Trade-off

The learnable model representation captures the PTM's empirical performance on various fields of training tasks, which decouples the task repr. from the PTM. Each model repr. implicitly expresses the field in which the PTM excels, so the PTM selection only requires a task repr. to express the field in which the task is. In contrast to the general task repr. \((_{i})\), PTM-specific features \(_{_{i}}^{m}\) for a subset of PTMs provide _rich clues_ about how those PTMs fit the target examples, which are also used in related transferability assessment approaches [25; 71]. We claim that given specific features with _a subset of PTMs_ when the budget is available, our Model Spider can re-rank the estimated PTM order and further improve performance.

Specifically, we extract the PTM-specific task repr. \(_{m}()^{d_{m} C_{ }}\) with the specific features \(_{}^{m}\) of the \(m\)th PTM as Equation 5. To take account of different values of \(d_{m}\) due to the heterogeneity of PTMs, we learn a projection \(^{d_{m} d}\) for the \(m\)th PTM to align the dimensionality of \(_{m}()\) with the model representation. We then replace the general task repr. \(()\) via the specific one \(_{m}^{}_{m}()\) when calculating the similarity with the repr. \(_{m}\) of the \(m\)th PTM. The specific task repr. may facilitate obtaining more accurate estimations. During the training process, we dynamically select a partial set of PTMs and incorporate the specific repr. into the sampled tasks. Thus, the same Transformer module in Equation 6 can deal with the new type of representation. To differentiate the general and specific representation, we learn two additional \(d\)-dimensional embeddings as prompts.

The prompts are added to the input repr., allowing the transformer to utilize represented-type context for a better ranking process. Notably, \(_{m}()\) depends on \(_{T}^{m}\), and the pre-extracted PTM-specific features for all training tasks make the construction of these specific representation efficient.

### A Brief Summary of Model Spider

Model Spider learns to rank PTMs based on the model-task pair, balancing efficiency and accuracy. During the training, we sample tasks where PTM representation and transformer-based similarity are learned. In particular, to enable the model-task similarity to incorporate PTM-specific features, we replace some of the inputs to the transformer with enriched representations. We pre-extract PTM-specific features for all training tasks, and then the estimated ground-truth and the specific repr. could be constructed efficiently. During deployment, we first employ a coarse-grained PTM search with a general representation. Then we carry out forward passes over the target task _only for top-\(k\) ranked PTMs_, where the obtained PTM-specific task repr. will re-rank the PTMs by taking the distributed examples with PTM's features into account.

## 5 Experiments

We evaluate Model Spider on three benchmarks: the PTM zoo comprising heterogeneous models from the single-source, multi-source datasets, or composed of large language models. We analyze the influence of key components in Model Spider and visualize the ability of a PTM using spider charts based on the learned representation.

### Evaluation on a _Single-Source_ Model Zoo

**Setups.** We follow  and construct a model zoo with \(10\) PTMs pre-trained on ImageNet  across five architecture families, _i.e._ Inception , ResNet , DenseNet , MobileNet ,

    &  \\  & H-Score & LogME & GBC & **Ours** \\  dSprites & \(0.106\) & \(0.612\) & -\(0.283\) & **0.679** \\ UTKFace & \(0.075\) & -\(0.156\) & \(0.052\) & **0.364** \\   

Table 1: Performance comparison of regression-conducted approaches with the same model zoo and weighted \(_{w}\) measurement as in Table 2. The downstream task is dSprites and UTKFace.

    &  &  \\  & Aircraft & Caltech101 & Cars & CIFAR10 & CIFAR100 & DTD &  &  &  \\ 
**Standard Evaluation** & & & & & & & & & \\ H-Score  & \(0.328\) & \(0.738\) & \(0.616\) & \(0.797\) & \(0.784\) & \(0.395\) & \(0.610\) & \(0.918\) & \(0.648\) \\ NCE  & \(0.501\) & \(0.752\) & \(0.771\) & \(0.694\) & \(0.617\) & \(0.403\) & \(0.696\) & \(0.892\) & \(0.666\) \\ LEEP  & \(0.244\) & \(0.014\) & \(0.704\) & \(0.601\) & \(0.620\) & -\(0.111\) & \(0.680\) & \(0.509\) & \(0.408\) \\ \(\)-LEEP  & -0.725 & \(0.599\) & \(0.622\) & \(0.768\) & \(0.776\) & \(0.074\) & \(0.787\) & \(0.730\) & \(0.454\) \\ LogME  & **0.540** & \(0.666\) & \(0.677\) & \(0.802\) & \(0.798\) & \(0.429\) & \(0.628\) & \(0.870\) & \(0.676\) \\ PACTran  & \(0.031\) & \(0.200\) & \(0.665\) & \(0.717\) & \(0.620\) & -\(0.236\) & \(0.616\) & \(0.565\) & \(0.397\) \\ OTCE  & -0.241 & -0.011 & -0.157 & \(0.569\) & \(0.573\) & -\(0.165\) & \(0.402\) & \(0.218\) & \(0.149\) \\ LFC  & 0.279 & -0.165 & \(0.243\) & \(0.346\) & \(0.418\) & -\(0.722\) & \(0.215\) & -\(0.344\) & \(0.034\) \\ GBC  & -0.744 & -0.055 & -0.265 & \(0.758\) & \(0.544\) & -\(0.102\) & \(0.163\) & \(0.457\) & \(0.095\) \\  Model Spider & 0.506 & **0.761** & **0.785** & **0.909** & **1.000** & **0.695** & **0.788** & **0.954** & **0.800** \\    \\ H-Score  & -0.014 & \(0.078\) & \(0.375\) & \(0.018\) & \(0.005\) & -\(0.028\) & -\(0.006\) & \(0.853\) & \(0.160\) \\ NCE  & \(0.273\) & \(0.534\) & \(0.597\) & \(0.267\) & \(0.232\) & \(0.362\) & \(0.352\) & \(0.793\) & \(0.426\) \\ LEEP  & \(0.069\) & -0.038 & \(0.476\) & \(0.530\) & \(0.471\) & -\(0.111\) & \(0.567\) & \(0.468\) & \(0.304\) \\ \(\)-LEEP  & -0.559 & \(0.476\) & \(0.743\) & \(0.515\) & \(0.707\) & \(0.027\) & \(0.713\) & \(0.812\) & \(0.429\) \\ LogME  & \(0.341\) & \(0.453\) & \(0.497\) & \(0.718\) & \(0.698\) & \(0.407\) & \(0.657\) & \(0.817\) & \(0.574\) \\ PACTran  & \(0.136\) & \(0.262\) & \(0.484\) & \(0.631\) & \(0.614\) & -\(0.227\) & \(0.701\) & \(0.477\) & \(0.385\) \\ OTCE  & -0.316 & -0.050 & -0.127 & \(0.515\) & \(0.505\) & -\(0.168\) & \(0.406\) & \(0.210\) & \(0.123\) \\ LFC  & \(0.226\) & -0.226 & -0.226 & -0.235 & \(0.330\) & \(0.271\) & -\(0.669\) & -\(0.059\) & -\(0.151\) & -\(0.064\) \\  Model Spider & **0.382** & **0.711** & **0.727** & **0.870** & **0.977** & **0.686** & **0.717** & **0.933** & **0.750** \\   

Table 2: Performance comparisons of \(10\) baseline approaches and Model Spider on a model zoo with \(10\) PTMs . We measure the performance with Kendall’s  weighted \(_{w}\). The downstream tasks from diverse fields (\(8\) datasets) are evaluated in a standard manner (all training examples) and a few-shot manner (\(10\) examples per class and \(30\) trials). Specific features of top-\(3\) ranked PTMs are used in Model Spider. We denote the best-performing results in bold.

and MNASNet . We evaluate various methods on \(9\) downstream datasets, _i.e._ Aircraft , Caltech101 , Cars , CIFAR10 , CIFAR100 , DTD , Pet , and SUN397  for classification, UTKFace  and dSprites  for regression.

**Baselines.** There are three groups of comparison methods. First are creating a proxy between PTM-specific features and downstream labels, such as H-Score , NCE , LEEP , \(\)-LEEP , LogME , and PACTran . The second are based on the downstream inter-categories features like OTCE , Label-Feature Correlation (LFC) , and GBC . Following  and , we equivalently modify NCE and H-Score to the general model selection application.

**Evaluations.** For the _standard evaluation_, we follow the official train-test split of each downstream dataset and utilize all the training samples. In _few-shot evaluation_, we consider if Model Spider can select useful models with limited labeled examples under privacy and resource constraints. We sample \(10\) examples per class from the training set as a "probe set" and report the average results over \(30\) trials. The full results, along with 95% confidence intervals, are presented in the appendix.

**Training Details of Model Spider.** We implement the \(\) with the pre-trained Swin-B [57; 53] to extract the task representation. Model Spider is trained on \(832\) sampled tasks from the mix of \(6\) datasets, _i.e._, EuroSAT , OfficeHome , PACS , SmallNORB , STL10  and VLCS . Model Spider utilizes specific features from the top-\(3\) ranked PTMs (out of \(10\)) for downstream tasks, resulting in a 3-4 times speedup.

**Results of Standard and Few-Shot Evaluation.** For the standard evaluation shown in Table 2 and Table 1, Model Spider outperforms other baselines across datasets, except for Aircraft, which ranks top-\(2\). It also demonstrates superior stability and outperforms all the existing approaches in few-shot scenarios, as displayed in the lower part of Table 2. Consistently ranking and selecting the correct PTMs, Model Spider achieves the highest mean performance among all methods.

### Evaluation on a _Multi-Source_ Model Zoo

We construct a large model zoo where \(42\) heterogeneous PTMs are pre-trained from multiple datasets.

**Setups.** PTMs with \(3\) similar magnitude architectures, _i.e._, Inception V3, ResNet 50, and DenseNet 201, are pre-trained on \(14\) datasets, including animals [37; 46], general and 3D objects [32; 51; 49; 47; 14], plants , scene-based , remote sensing [106; 18; 36] and multi-domain recognition . We evaluate the ability of PTM selection on Aircraft , DTD , and Pet  datasets.

**Training Details.** We use the same task representation extractor as in subsection 5.1 with \(4352\) training tasks sampled from the mix of the above datasets for pre-training the model zoo.

**Analysis of Multi-Source Model Zoo.** With many PTMs in the model zoo, we first set \(k=0\) and select PTMs based on general representation. We visualize the results in Figure 3, with each

Figure 3: Visualizations when selecting PTMs from a multi-source heterogeneous model zoo (w/ \(42\) PTMs) on three downstream datasets. Rows represent approaches, and columns represent datasets. Correlations (\(_{w}\)) are shown above each subfigure. The horizontal axis denotes transferred accuracy (w/ fine-tuning), while the vertical axis is the output ranking score. The PTM architectures are drawn in red, yellow, and green. The bold line and the gray area show the fitted straight line and the confidence interval for all PTMs. The strong linear correlation suggests superior performance.

subfigure showing the transferred accuracy using the selected PTM with fine-tuning and the predicted ranking score. A better-performing method will show a more obvious linear correlation. The results demonstrate that Model Spider achieves the optimum in all three datasets. Furthermore, a visualization of efficiency, the averaged performance over all datasets, and model size on this benchmark with standard evaluation is shown in Figure 1. The different configurations of \(k\) balance the efficiency and performance in PTM selection, which "envelope" the results of other methods. These results confirm that Model Spider performs well in complex scenarios, highlighting its ability to select heterogeneous PTMs in a large model zoo.

### Evaluation on a Zoo of _Large Language Models_

We introduce \(9\) open-source Large Language Models (LLMs) to construct our LLM zoo and deploy the Model Spider framework. We conduct a comparative analysis of the performance of the selected top-1 model against ChatGPT .

**Setups.** The LLM zoo involves Alpaca-7B , Baichuan-7B , Baichuan2-7B , ChatGLM2-6B , InternLM-7B , LLaMA2-7B , Vicuna-7B , Qwen-7B  and its chat fine-tuned version. We assess their zero-shot performance on diverse target tasks using the OpenCompass  LLM evaluation benchmark. We then focus on unseen tasks from the _examination_ to _language_, _knowledge_, _understanding_, and _reasoning_ datasets as the target tasks. For more details, please see appendix subsection B.3. We report the performance of the top-1 model recommended by each LLM ranking method and compare it with existing LLM evaluation results.

**Training Details.** For task representation, we employ a general Sentence-T5  to obtain task representation. We extract answers from \(10\) instruction samples as a representative task for a dataset. We initialize the corresponding model repr. to encode the capabilities of LLMs on instruction data.

**Analysis of Ranking on the Zoo of LLMs.** Given that LLMs are computationally intensive in PTMs, we rank LLMs based on their general task representation. Intuitive methods for LLM ranking, like proxy measures relying on self-assessed confidence scores from generated answers or few-shot tasks in related domains, often fall short in assessing target task performance. The results indicate that while ChatGPT-3.5 demonstrates impressive performance in terms of universal performance across all diverse target tasks, as shown in Table 3 being \(56.64\), the top-1 ranked of Model Spider can surpass ChatGPT when efficiently choosing the appropriate LLM for each specific task. Our method achieves the average best and excels in the \(4\) out of \(5\) major fields of target tasks.

### Ablation Studies

We analyze the properties of Model Spider on some downstream datasets, following the evaluation of a single-source model zoo in subsection 5.1.

**Will RankAgg provide more accurate ground-truth during training?** As discussed in subsection 4.3, Model Spider is trained on historical tasks and we utilize \(\) to approximate

    &  &  \\
**Method** & & & Language & Knowledge & Understand. & Reason. \\ 
**LLM Evaluations** & & & & & \\ Alpaca-7B  & \(24.30\) & \(67.20\) & \(41.95\) & \(33.30\) & \(51.70\) & \(43.69\) \\ ChatGLM2-6B  & \(39.00\) & \(67.30\) & \(44.35\) & \(40.25\) & \(68.67\) & \(51.91\) \\ LLaMA2-7B  & \(31.30\) & \(67.40\) & \(55.90\) & \(40.30\) & \(52.93\) & \(49.57\) \\ Vicuna-7B  & \(29.10\) & \(66.70\) & \(49.45\) & \(34.70\) & \(52.67\) & \(46.52\) \\ ChatGPT  & \(39.90\) & \(60.90\) & **57.10** & \(55.40\) & \(69.90\) & \(56.64\) \\   & & & & \\ Self-assessed Confidence & \(34.60\) & \(67.40\) & \(45.10\) & \(37.45\) & \(62.60\) & \(49.43\) \\ Perf. on Similar Tasks & \(29.10\) & \(67.20\) & \(44.35\) & \(53.45\) & \(63.03\) & \(51.43\) \\  Model Spider & **41.30** & **67.65** & \(55.90\) & **56.80** & **70.07** & **58.34** \\   

Table 3: Top-1 ranked Large Language Model (LLM) performance comparisons against LLM evaluation results [94; 116; 96; 119; 69], which includes \(2\) directly baselines and our Model Spider, ranking on a pre-trained model zoo of \(9\) LLMs. The \(10\) downstream tasks are construct based on the OpenCompass  benchmark from \(5\) diverse fields as examination, language, knowledge, understanding, reasoning. We denote the best-performing results in bold.

accuracy ranking. We investigate if this approximation offers better supervision and if using previous model selection methods like H-Score or LogME without aggregation is sufficient. The results in Table 4 include CIFAR10 and averaged results over eight classification datasets. It is evident that \(\) provides stronger supervision during Model Spider's training.

**Will _more_ PTM-specific features help?** As mentioned in subsection 4.4, Model Spider is able to incorporate PTM-specific features -- the forward pass of a PTM over the downstream task - to improve the ranking scores. When no specific features (\(k=0\)) exist, we use the general representation to rank PTMs (most efficient). In Figure 4 (a), we show that \(_{w}\) increases when Model Spider receives more PTM-specific features. It balances the efficiency and accuracy trade-off.

### Interpreting Model Spider by Spider Chart

An interesting by-product of Model Spider is that we can visualize the ability of a PTM with a spider chart, which demonstrates which fields the PTM is good. We cluster the datasets in our multi-source model zoo into six major groups. Then, we approximate a PTM's ability on the six types of tasks with the averaged similarity between a PTM to the tasks in the cluster. The larger the similarity, the better the PTM performs on that task. In Figure 4 (b), we find a PTM pre-trained on AID  dataset works well on medical and remote sensing tasks, and a PTM pre-trained on NABirds  dataset shows strong ability on birds and animal recognition. The spider charts provide valuable insights into PTM capabilities and assist in PTM recommendations for specific application scenarios.

## 6 Conclusion

The proposed Model Spider learns to rank PTMs for existing tasks and can generalize the model selection ability to unseen tasks, even with few-shot examples, and is applicable to both visual and large language models (LLMs). The two-stage pipeline in Model Spider enables it to fit the resources adaptively. A task is matched with PTMs efficiently based on their task-agnostic representation if resource is limited. While there is a sufficient resource budget, limited forward passes are carried out over the candidates of top-ranked PTMs, which re-ranks candidates via incorporating the detailed fitness between the task and the selected PTMs. The learned representations help construct a spider chart for each task, illustrating its relevance with all PTMs. The representation for models and tasks acts as a kind of specification that matches the main design in Learnware [121; 122; 91; 34; 92].

 
**Method** & CIFAR10 & Mean \\  w/ H-Score  & \(0.386\) & \(0.642\) \\ w/ LogME  & \(0.695\) & \(0.689\) \\ w/ \(\) (Ours) & **0.845** & **0.765** \\  

Table 4: The weighted \(_{w}\) of Model Spider variants when the training supervision is approximated by different methods. “Mean” denotes the averaged performance over 8 downstream datasets in Table 2.

Figure 4: (a): The ablation analysis of how the ranking correlation changes (Y-axis) with more PTM-specific features (X-axis). (b): Visualization of the PTM’s ability on 6 major semantic clusters of datasets with spider chart. The score on the vertex of the spider chart is the averaged similarities between a PTM and the task representation in the cluster. The higher the vertex value, the better a PTM would perform on that kind of task.

**Acknowledgments.** This work is partially supported by the National Key R&D Program of China (2022ZD0114805), NSFC (62250069, 62376118, 62006112, 62206245), Young Elite Scientists Sponsorship Program of Jiangsu Association for Science and Technology 2021-020, Collaborative Innovation Center of Novel Software Technology and Industrialization.