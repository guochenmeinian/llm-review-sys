# Shuffling Gradient-Based Methods for

Nonconvex-Concave Minimax Optimization

 Quoc Tran-Dinh

Department of Statistics and Operations Research

The University of North Carolina at Chapel Hill

quoctd@email.unc.edu

&Trang H. Tran

School of OR and Information Engineering

Cornell University, Ithaca, NY

http7@cornell.edu

&Lam M. Nguyen

IBM Research, Thomas J. Watson Research Center

Yorktown Heights, NY

LamSw given.MLTD@ibm.com

###### Abstract

This paper aims at developing novel shuffling gradient-based methods for tackling two classes of minimax problems: _nonconvex-linear_ and _nonconvex-strongly concave_ settings. The first algorithm addresses the nonconvex-linear minimax model and achieves the state-of-the-art oracle complexity typically observed in nonconvex optimization. It also employs a new shuffling estimator for the "hyper-gradient", departing from standard shuffling techniques in optimization. The second method consists of two variants: _semi-shuffling_ and _full-shuffling_ schemes. These variants tackle the nonconvex-strongly concave minimax setting. We establish their oracle complexity bounds under standard assumptions, which, to our best knowledge, are the best-known for this specific setting. Numerical examples demonstrate the performance of our algorithms and compare them with two other methods. Our results show that the new methods achieve comparable performance with SGD, supporting the potential of incorporating shuffling strategies into minimax algorithms.

## 1 Introduction

Minimax problems arise in various applications across generative machine learning, game theory, robust optimization, online learning, and reinforcement learning (e.g., ). These models often involve stochastic settings or large finite-sum objective functions. To tackle these problems, existing methods frequently adapt stochastic gradient descent (SGD) principles to develop algorithms for solving the underlying minimax problems . For instance, in generative adversarial networks (GANs), early algorithms employed stochastic gradient descent-ascent methods where two routines, each using an SGD loop, ran iteratively . However, practical implementations of SGD often incorporate shuffling strategies, as seen in popular deep learning libraries like TensorFlow and PyTorch. This has motivated recent research on developing shuffling techniques specifically for optimization algorithms . Our work builds upon this trend by developing shuffling methods for two specific classes of minimax problems.

**Problem statement.** In this paper, we study the following minimax optimization problem:

\[_{w^{p}}_{u^{q}}(w,u):=f(w )+(w,u)-h(u) f(w)+_{i=1}^{n}_{i}(w,u)-h(u)},\] (1)where \(f:^{p}\{+\}\) is a proper, closed, and convex function, \(_{i}:^{p}^{q}\) are smooth for all \(i[n]:=\{1,2,,n\}\), and \(h:^{q}\{+\}\) is also a proper, closed, and convex function. In this paper, we will focus on two classes of problems in (1), overlapped to each other.

* \(_{i}\) is nonconvex in \(w\) and linear in \(u\) as \(_{i}(w,u):= F_{i}(w),Ku\) for a given function \(F_{i}:^{p}^{m}\) and a matrix \(K^{q m}\) for all \(i[n]\) and \((w,u)()\).
* \(_{i}\) is nonconvex in \(w\) and \(_{i}(w,)-h()\) is strongly concave in \(u\) for all \((w,u)()\).

Although (NC) looks more general than (NL), both cases can be overlapped, but one is not a special case of the other. Under these two settings, our approach will rely on a _bilevel optimization_ approach, where the lower-level problem is to solve \(_{u}(w,u)\), while the upper-level one is \(_{w}(w,u)\).

**Challenges.** The setting (NL) is a special case of stochastic nonconvex-concave minimax problems because the objective term \((w,u):= F(w),Ku\) is linear in \(u\). It is equivalent to the compositional model (CO) described below. However, if \(h\) is only merely convex and not strongly convex (e.g., the indicator of a standard simplex), then \(_{0}\) in (CO) becomes nonsmooth regardless of \(F\)'s properties. This presents our first challenge. A natural approach to address this issue, as discussed in Section 2, is to smooth \(_{0}\). The second challenge arises from the composition between the outer function \(h^{*}\) and the finite sum \(F()\) in (CO). Unlike standard finite-sum optimization, this composition prevents any direct use of existing techniques, requiring a novel approach for algorithmic development and analysis. The third challenge involves unbiased estimators for gradients or "hyper-gradients" in minimax problems. Most existing methods rely on unbiased estimators for objective gradients, with limited work exploring biased estimators. While biased estimators can be used, they require variance reduction properties (see, e.g., ). The setting (NC) faces the same second and third challenges as the setting (NL). Additionally, when reformulating it as a minimization problem using a bilevel optimization approach (3), constructing a shuffling estimator for the "hyper-gradient" \(_{0}\) becomes unclear. This requires solving the lower-level maximization problem (2). Therefore, it remains an open question whether shuffling gradient-type methods can be extended to this bilevel optimization approach to address (1). In this paper, we address the following research question:

_Can we efficiently develop shuffling gradient methods to solve (1) for both \(()\) and \(()\) settings?_

Our attempt to tackle this question leads to a novel way of constructing shuffling estimators for the hyper-gradient \(_{0}\) or its smoothed counterpart. This allows us to develop two shuffling gradient-based algorithms with rigorous theoretical guarantees on oracle complexity, matching state-of-the-art complexity results in shuffling-type algorithms for nonconvex optimization.

**Related work.** Shuffling optimization algorithms have gained significant attention in optimization and machine communities, demonstrating advantages over standard SGDs, see, e.g., . Nevertheless, applying these techniques to minimax problems like (1) remains challenging, with limited existing literature (e.g., ). Das _et al._ in  explored a specific case of (1) without nonsmooth terms \(f\) and \(h\), assuming strong monotonicity and \(L\)-Lipschitz continuity of the gradient \(:=[_{w},-_{u}]\) of the joint objective \(\). Their algorithm simplifies to a shuffling variant of fixed-point iteration or a gradient descent-ascent scheme, not applicable to our settings. Cho and Yun in  built upon  by relaxing the strong monotonicity to Polyak-Lojasiewicz (PL) conditions. This work is perhaps the most closely related one to our algorithm, Algorithm 2, for the (NC) setting. Note that the method in  exploits Nash's equilibrium perspective with a simultaneous update, which is different from our alternative update. Moreover,  only considers the noncomposite case with \(f=0\) and \(h=0\). Though we only focus on a nonconvex-strongly-concave setting (NC), our results here can be extended to the PL condition as in . Very recently, Konstantinos _et al._ in  introduced shuffling extragradient methods for variational inequalities, which encompass convex-concave minimax problems as a special case. However, this also falls outside the scope of our work due to the nonconvexity of (1) in \(w\). Again, all the existing works in  utilize a Nash's equilibrium perspective, while ours leverages a bilevel optimization technique. Besides, in contrast to our sampling-without-replacement approach, stochastic and randomized methods (i.e. using i.i.d. sampling strategies) have been extensively studied for minimax problems, see, e.g., . A comprehensive comparison can be found, e.g., in .

**Contribution.** Our main contribution can be summarized as follows.

* For setting (NL), we suggest to reformulate (1) into a compositional minimization and exploit a smoothing technique to treat this reformulation. We propose a new way of constructing shuffling estimators for the "hyper-gradient" \(_{}\) (cf. (10)) and establish their properties.

2. We propose a novel shuffling gradient-based algorithm (_cf._ Algorithm 1) to approximate an \(\)-KKT point of (1) for the setting (NL). Our method requires \((n^{-3})\) evaluations of \(F_{i}\) and \( F_{i}\) under the strong convexity of \(h\), and \((n^{-7/2})\) evaluations of \(F_{i}\) and \( F_{i}\) without the strong convexity of \(h\), for a desired accuracy \(>0\).
3. For setting (NC), we develop two variants of the shuffling gradient method: _semi-shuffling_ and _full-shuffling_ schemes (_cf._ Algorithm 2). The semi-shuffling variant combines both gradient ascent and shuffling gradient methods to construct a new algorithm, which requires \((n^{-3})\) evaluations of both \(_{w}_{i}\) and \(_{u}_{i}\). The full-shuffling scheme allows to perform both shuffling schemes on the maximization and the minimization alternatively, requiring either \((n^{-3})\) or \((n^{-4})\) evaluations of \(_{u}_{i}\) depending on our assumptions, while maintaining \((n^{-3})\) evaluations of \(_{w}_{i}\) for a given desired accuracy \(>0\).

If a random shuffling strategy is used in our algorithms, then the oracle complexity in all the cases presented above is improved by a factor of \(\). Our settings (NL) and (NC) of (1) are different from existing works [3; 8; 11], as we work with general nonconvexity in \(w\), and linearity or [strong] concavity in \(u\), and both \(f\) and \(h\) are possibly nonsmooth. Our algorithms are not reduced or similar to existing shuffling methods for optimization, but we use shuffling strategies to form estimators for the hyper-gradient \(_{0}\) in (5). The oracle complexity in both settings (NL) and (NC) is similar to the ones in nonconvex optimization and in a special case of (1) from  (up to a constant factor).

**Paper outline.** The rest of this paper is organized as follows. Section 2 presents our bilevel optimization approach to (1) and recalls necessary preliminary results. Section 3 develops our shuffling algorithm to solve the setting (NL) of (1) and establishes its convergence. Section 4 proposes new shuffling methods to solve the setting (NC) and investigates their convergence. Section 5 presents numerical experiments, while technical proofs and supporting results are deferred to Supp. Docs.

**Notations.** For a function \(f\), we use \((f)\) to denote its effective domain, and \( f\) for its gradient or Jacobian. If \(f\) is convex, then \( f\) denotes a subgradient, \( f\) is its subdifferential, and \(_{f}\) is its proximal operator. We use \(_{t}\) to denote \((w_{0},w_{1},,w_{t})\), a \(\)-algebra generated by random vectors \(w_{0},w_{1},,w_{t}\), \(_{t}[]=[|_{t}]\) is a conditional expectation, and \([]\) is the full expectation. As usual, \(()\) denotes Big-O notation in the theory of algorithm complexity.

## 2 Bilevel Optimization Approach and Preliminary Results

Our approach relies on a bilevel optimization technique  in contrast to Nash's game viewpoint , which treats the maximization as a lower level and the minimization as an upper level problem.

### Bilevel optimization approach

The minimax model (1) is split into a _lower-level_ (_i.e. a follower_) _maximization problem_ of the form:

\[_{0}(w)&:=_{u^{q} }(w,u)-h(u)_{i=1}^{n}_{i} (w,u)-h(u)},\\ u_{0}^{*}(w)&:=*{\!}_{u ^{q}}(w,u)-h(u)_{i=1}^{n} _{i}(w,u)-h(u)}.\] (2)

For \(_{0}\) defined by (2), then the _upper-level_ (_i.e. the leader_) _minimization problem_ can be written as

\[_{0}^{*}:=_{w^{p}}_{0}(w):=_{0}(w)+f(w) }.\] (3)

Clearly, this approach is sequential, and only works if \(_{0}\) is well-defined, i.e. (2) is globally solvable. Hence, the concavity of \((w,)-h()\) w.r.t. to \(u\) is crucial for this approach as stated below. However, this assumption can be relaxed to a global solvability of (2) combined with a PL condition as in .

**Assumption 1** (Basic).: _Problems_ (1) and (3) _satisfy the following assumptions for all \(i[n]\):_

1. \(_{0}^{*}:=_{w}_{0}(w)>-\)_._
2. \(_{i}\) _is differentiable w.r.t._ \((w,u)()\) _and_ \(_{i}(w,)\) _is concave in_ \(u\) _for any_ \(w\)_._
3. _Both_ \(f:^{p}\{+\}\) _and_ \(h:^{q}\{+\}\) _are proper, closed, and convex._

This assumption remains preliminary. To develop our algorithms, we will need more conditions on \(_{i}\) and possibly on \(f\) and \(h\), which will be stated later. In addition, we can work with a sublevel set

\[_{_{0}}(w_{0}):=\{w(_{0}): _{0}(w)_{0}(w_{0})\}\] (4)of \(_{0}\) for a given initial point \(w_{0}\) from our methods. If \(u_{0}^{*}(w)\) is uniquely well-defined for given \(w_{_{0}}(w_{0})\), then by the well-known Danskin's theorem, \(_{0}\) is differential at \(w\) and its gradient is

\[_{0}(w)=_{w}(w,u_{0}^{*}(w))=_{i=1} ^{n}_{w}_{i}(w,u_{0}^{*}(w)).\] (5)

We adopt the term "hyper-gradient" from bilevel optimization to name \(_{0}\) in this paper.

### Technical assumptions and properties of \(_{0}\) for nonconvex-linear setting (NL)

\(()\)_Compositional minimization formulation_. If \(_{i}(w,u):= F_{i}(w),Ku\) as in setting (NL), then (1) is equivalently reformulated into the following _nonconvex compositional minimization_ problem:

\[_{w^{p}}_{0}(w):=f(w)+_{0}(w)=f(w)+h^{*} _{i=1}^{n}K^{T}F_{i}(w)},\] (CO)

where \(h^{*}(v):=_{u}\{ v,u-h(u)\}\), the Fenchel conjugate of \(h\), and \(_{0}(w)=h^{*}(K^{T}F(w))\). If \(h\) is not strongly convex, then \(h^{*}\) is convex but possibly nonsmooth.

\(()\)_Technical assumptions_. To develop our algorithms, we also need the following assumptions.

**Assumption 2**.: \(h\) _is \(_{h}\)-strongly convex with \(_{h} 0\), and \((h)\) is bounded by \(M_{h}<+\)._

**Assumption 3** (For \(F_{i}\)).: _For setting \(()\) with \(_{i}(w,u):= F_{i}(w),Ku\)\((i[n])\), assume that_

* \(F_{i}\) _is continuously differentiable, and its Jacobian_ \( F_{i}\) _is_ \(L_{F_{i}}\)_-Lipschitz continuous._
* \(F_{i}\) _is also_ \(M_{F_{i}}\)_-Lipschitz continuous or equivalently, its Jacobian_ \( F_{i}\) _is_ \(M_{F_{i}}\)_-bounded._
* _There exists a positive constant_ \(_{J}(0,+)\) _such that_ \[_{i=1}^{n}\| F_{i}(w)- F(w)\|^{2}_{J}^{2 }, w(F).\] (6)

Assumption 2 allows \(_{h}=0\) that also covers the non-strong convexity of \(h\). Assumption 3 is rather standard to develop gradient-based methods for solving (1). Under Assumption 3, the finite-sum \(F\) is also \(M_{F}\)-Lipschitz continuous and the Jacobian \( F\) of \(F\) is also \(L_{F}\)-Lipschitz continuous with

\[M_{F}:=\{M_{F_{i}}:i[n]\} L_{F}:=\{L_{F_{i}}:i [n]\}.\] (7)

Condition (6) can be relaxed to the form \(_{i=1}^{n}\| F_{i}(w)- F(w)\|^{2}_{J}^{ 2}+_{J}\|_{0}(w)\|^{2}\) for some \(_{J} 0\), where \(_{0}\) is a [sub]gradient of \(_{0}\) or \(_{}\) (its smoothed approximation). Moreover, under Assumption 3, if \(_{h}>0\), then \( h^{*}\) is \(L_{h^{*}}\)-Lipschitz continuous with \(L_{h^{*}}:=}\). Thus it is possible (see ) to prove that \(_{0}\) is differentiable, and \(_{0}\) is also \(L_{_{0}}\)-Lipschitz continuous with \(L_{_{0}}:=M_{h}\|K\|L_{F}+^{2}\|K\|^{2}}{_{h}}\) as a consequence of Lemma 4 when \( 0^{+}\) in Supp. Doc. A.

\(()\)_Smoothing technique for lower-level maximization problem_ (2). If \(h\) is only merely convex (i.e. \(_{h}=0\)), then (2) may not be uniquely solvable, leading to the possible non-differentiability of \(_{0}\). Let us define the following convex function:

\[_{0}(v):=_{u^{q}}\{ v,Ku-h(u)\}=h ^{*}(K^{T}v).\] (8)

Then, \(_{0}\) in (2) or (CO) can be written as \(_{0}(w)=_{0}(F(w))=_{0}(_{i=1}^{n}F_{i}(w))\). Our goal is to smooth \(_{0}\) if \(h\) is not strongly convex, leading to

\[\{ _{}(v)&:=_{u} \{ v,Ku-h(u)- b(u)\},\\ u_{}^{*}(v)&:=*{argmax}_{u}\{  v,Ku-h(u)- b(u)\},.\] (9)

where \(>0\) is a given smoothness parameter and \(b:^{q}\) is a proper, closed, and \(1\)-strongly convex function such that \((h)(b)\). We also denote \(D_{b}:=\{\| b(u)\|:u(h)\}\). In particular, if we choose \(b(u):=\|u-\|^{2}\) for a fixed \(\), then \(u_{}^{*}(v)=_{h/}(-K^{T}v)\).

Using \(_{}\), problem (CO) can be approximated by its smoothed formulation:

\[_{w^{p}}_{}(w):=f(w)+_{}(w)=f(w)+ _{}(F(w)) f(w)+_{}_{i=1}^{n}F_{ i}(w)}.\] (10)

To develop our method, one key step is to approximate the hyper-gradient of \(_{}\) in (10), where

\[_{}(w) =  F(w)^{T}_{}(F(w))=_{i=1}^{ n} F_{i}(w)^{T}_{}(F(w)).\] (11)

Then, \(_{}\) is \(L_{_{}}\)-Lipschitz continuous with \(L_{_{}}:=M_{h}\|K\|L_{F}+^{2}\|K\|^{2}}{_{h}+}\) (see Lemma 4).

### Technical assumptions and properties of \(_{0}\) for the nonconvex-strongly-concave setting

To develop our shuffling gradient-based algorithms for solving (1) under the nonconvex-strongly-concave setting (NC), we impose the following assumptions.

**Assumption 4** (For \(_{i}\)).: \(_{i}\) _for all \(i[n]\) in (1) satisfies the following conditions:_

* _For any given_ \(w\) _such that_ \((w,u)()\)_,_ \(_{i}(w,)\) _is_ \(_{H}\)_-strongly concave w.r.t._ \(u\)_._
* \(_{i}\) _is_ \((L_{w},L_{u})\)_-Lipschitz continuous, i.e. for all_ \((w,u),(,)()\)_:_ \[\|_{i}(w,u)-_{i}(,)\|^{2} L _{w}^{2}\|w-\|^{2}+L_{u}^{2}\|u-\|^{2}.\] (12)
* _There exist two constants_ \(_{w} 0\) _and_ \(_{w} 0\) _such that for_ \((w,u)()\)_, we have_ \[_{i=1}^{n}\|_{w}_{i}(w,u)-_{w}(w,u)\|^{2}\,\,_{w}\|_{w}(w,u)\|^{2}+_{w}^{2}.\] (13) _There exist two constants_ \(_{u} 0\) _and_ \(_{u} 0\) _such that for all_ \((w,u)()\)_, we have_ \[_{i=1}^{n}\|_{u}_{i}(w,u)-_{u}(w,u)\|^{2}\,\,_{u}\|_{u}(w,u)\|^{2}+_{u}^{2}.\] (14)

Assumption 4(a) makes sure that our lower-level maximization of (1) is well-defined. Assumption 4(b) and (c) are standard in shuffling gradient-type methods as often seen in nonconvex optimization .

**Lemma 1** (Smoothness of \(_{0}\)).: _Under Assumptions 2 and 4, \(u_{0}^{*}()\) in (2) is \(\)-Lipschitz continuous with \(:=}{_{H}+_{h}}\). Moreover, \(_{0}\) in (5) is \(L_{_{0}}\)-Lipschitz continuous with \(L_{_{0}}:=(1+)L_{w}\)._

### Approximate KKT points and approximate stationary points

\(()\) _Exact and approximate KKT points and stationary points._ A pair \((w^{},u^{})()\) is called a KKT (Karush-Kuhn-Tucker) point of (1) if

\[0_{w}(w^{},u^{})+ f(w^{})  0-_{u}(w^{},u^{})+ h(u^{ }).\] (15)

Given a tolerance \(>0\), **our goal** is to find an \(\)-approximate KKT point \((,)\) of (1) defined as

\[r_{w}_{w}(,)+ f(), r_{u}-_{u}(,)+ h( ),\|[r_{w},r_{u}]\|^{2} ^{2}.\] (16)

A vector \(w^{}(_{0})\) is said to be a stationary point of (3) if

\[0_{0}(w^{})+ f(w^{}).\] (17)

Since \(f\) is possibly nonsmooth, we can define a stationary point of (3) via a gradient mapping as:

\[_{}(w):=^{-1}w-_{ f}(w- _{0}(w)),\] (18)

where \(>0\) is given. It is well-known that \(_{}(w^{})=0\) iff \(w^{}\) is a stationary point of (3). Again, since we cannot exactly compute \(w^{}\), we expect to find an \(\)-stationary point \(_{T}\) of (3) such that \(\|_{}(_{T})\|^{2} ^{2}\) for a given tolerance \(>0\).

\(()\) _Constructing an approximate stationary point and KKT point from algorithms._ Our algorithms below generate a sequence \(\{_{t}\}_{t 0}^{T}\) such that \(_{t=0}^{T}\|_{}( _{t})\|^{2}^{2}\). Hence, we construct an \(\)-stationary point \(_{T}\) using one of the following two options:

\[_{T}:=_{t_{}},\ \{t_{}:= \{\|_{}(_{t})\|:0 t T \},&\\ t_{}\{0,1,,T\}.\] (19)

Clearly, we have \(\|_{}(_{T})\|^{2}_{t=0}^{T}\|_{}(_{t}) \|^{2}^{2}\). We need the following result.

**Lemma 2**.: \(()\) _If \((w^{},u^{})\) is a KKT point of (1), then \(w^{}\) is a stationary point of (3). Conversely, if \(w^{}\) is a stationary point of (3), then \((w^{},u_{0}^{}(w^{}))\) is a KKT point of (1)._

\(()\) _If \(_{T}\) is an \(\)-stationary point of (3) and \(_{0}\) is \(L_{_{0}}\)-Lipschitz continuous, then \((_{T},_{T})\) is an \(\)-KKT point of (1), where \(_{T}:=_{ f}(_{T}-_{0} (_{T}))\), \(_{T}:=u_{0}^{}(_{T})\), and \(:=(1+L_{_{0}})\)._

\(()\) _If \(_{T}\) is an \(\)-stationary point of (10), then \((_{T},_{T})\) is an \(\)-KKT point of (1), where \(_{T}:=_{ f}(_{T}-_{ }(_{T}))\), \(_{T}:=u_{}^{}(F(_{T}))\), and \(:=\{(1+L_{_{}}), D_{b}\}\)._

Lemma 2 allows us to construct an \(\)-approximate KKT point \((_{T},_{T})\) of (1) from an \(\)-stationary point \(_{T}\) of either (3) or its smoothed problem (10), where \(=(\{,\})\).

### Technical condition to handle the possible nonsmooth term \(f\)

To handle the nonsmooth term \(f\) of (1) in our algorithms we require one more condition as in .

**Assumption 5**.: _Let \(_{}\) be defined by (10), which reduces to \(_{0}\) given by (2) as \( 0^{+}\), and \(_{}\) be defined by (18). Assume that there exist two constants \(_{0} 1\) and \(_{1} 0\) such that:_

\[\|_{}(w)\|^{2}_{0}\|_{}(w)\|^{2}+ _{1}, w(_{0}).\] (20)

If \(f=0\), then \(_{}(w)_{}(w)\), and Assumption 5 automatically holds with \(_{0}=1\) and \(_{1}=0\). If \(f 0\), then it is crucial to have \(_{0} 1\) in (20). Let us consider two examples to see why?

1. If \(f\) is \(M_{f}\)-Lipschitz continuous (e.g., \(_{1}\)-norm), then (20) also holds with \(_{0}:=1+>1\) and \(_{1}:=M_{f}\) for a given \(>0\).
2. If \(f=_{}\), the indicator of a nonempty, closed, convex, and bounded set \(\), then Assumption 5 also holds by the same reason as in Example (i) (see Supp. Doc. A).

## 3 Shuffling Gradient Method for Nonconvex-Linear Minimax Problems

We first propose a new construction using shuffling techniques to approximate the true gradient \(_{}\) in (11) for any \( 0\). Next, we propose our algorithm and analyze its convergence.

### The shuffling gradient estimators for \(_{}\)

**Challenges.** To evaluate \(_{}(w)\) in (11), we need to evaluate both \( F(w)\) and \(F(w)\) at each \(w\). However, in SGD or shuffling gradient methods, we want to approximate both quantities at each iteration. Note that this gradient can be written in a finite-sum \(_{i=1}^{n} F_{i}(w)^{T}_{}(F(w))\) (see (11)), but every summand requires \(_{}(F(w))\), which involves the full evaluation of \(F\).

**Our estimators.** Let \(F_{^{(t)}(i)}(w_{i-1}^{(t)})\) and \( F_{^{(t)}(i)}(w_{i-1}^{(t)})\) be the function value and the Jacobian component evaluated at \(w_{i-1}^{(t)}\) respectively for \(i[n]\), where \(^{(t)}=(^{(t)}(1),^{(t)}(2),,^{(t)}(n))\) and \(^{(t)}=(^{(t)}(1),^{(t)}(2),,^{(t) }(n))\) are two permutations of \([n]:=\{1,2,,n\}\). We want to use these quantities to approximate the function value \(F(w_{0}^{(t)})\) and its Jacobian \( F(w_{0}^{(t)})\) of \(F\) at \(w_{0}^{(t)}\), respectively, where \(w_{0}^{(t)}\) the iterate vector at the beginning of each epoch \(t\).

For function value \(F(w_{0}^{(t)})\), we suggest the following approximation at each _inner iteration_\(i[n]\):

\[ F_{i}^{(t)}\,:=\,[_{j=1}^{i}F_{ ^{(t)}(j)}(w_{j-1}^{(t)})+_{j=i+1}^{n}F_{^{(t)}(j)}(w_{0}^{(t)}) ].\] (21)

Alternative to (21), for all \(i[n]\), we can simply choose another option:

\[ F_{i}^{(t)}\,:=\,_{j=1}^{n}F_{j}(w_{ 0}^{(t)})=_{j=1}^{n}F_{^{(t)}(j)}(w_{0}^{(t)}).\] (22)

For Jacobian \( F(w_{0}^{(t)})\), we suggest to use the following standard shuffling estimator for all \(i[n]\):

\[ F_{i}^{(t)}:= F_{^{(t)}(i)}(w_{i-1}^{(t)}).\] (23)

For \(F_{i}^{(t)}\) from (21) (or (22)) and for \( F_{i}^{(t)}\) from (23), we form an approximation of \(_{}(w_{0}^{(t)})\) as

\[_{}(w_{i-1}^{(t)}):=( F_{i}^{(t)})^{T} _{}(F_{i}^{(t)})( F_{i}^{(t)})^{T}Ku_{}^{*}(F_{i}^ {(t)}).\] (24)

**Discussion.** The estimator \(F_{i}^{(t)}\) for \(F\) requires \(n-i\) more function evaluations \(F_{^{(t)}(j)}(w_{0}^{(t)})\) at each epoch \(t\). The first option (21) for \(F\) uses \(2n\) function evaluations \(F_{i}\), while the second one in (22) only needs \(n\) function evaluations at each epoch \(t 0\). However, (21) uses the most updated information up to the _inner iteration_\(i\) compared to (22), which is expected to perform better. The Jacobian estimator \( F_{i}^{(t)}\) is standard and only uses one sample or a mini-batch at each iteration \(i\).

### The shuffling gradient-type algorithm for nonconvex-linear setting (NL)

We propose Algorithm 1, a shuffling gradient-type method, to approximate a stationary point of (10).

**Discussion.** First, the cost per epoch of Algorithm 1 consists of either \(2n\) or \(n\) function evaluations \(F_{i}\), and \(n\) Jacobian evaluations \( F_{i}\). Compare to standard shuffling gradient-type methods, e.g., in , Algorithm 1 has either \(n\) more evaluations of \(F_{i}\) or the same cost. Second, when implementingAlgorithm 1, we do not need to evaluate the full Jacobian \( F_{i}^{(t)}\), but rather the product of matrix \(( F_{i}^{(t)})^{T}\) and vector \(_{}(F_{i}^{(t)})\) as \(_{}(w_{i-1}^{(t)}):=( F_{i}^{(t)})^{T} _{}(F_{i}^{(t)})\). Evaluating this matrix-vector multiplication is much more efficient than evaluating the full Jacobian \( F_{i}^{(t)}\) and \(_{}(F_{i}^{(t)})\) individually. Third, thanks to Assumption 5, the proximal step \(_{t}:=_{_{t}f}(w_{n}^{(t)})\) is only required at the end of each epoch \(t\). This significantly reduces the computational cost if \(_{_{t}f}\) is expensive.

### Convergence Analysis of Algorithm 1 for Nonconvex-Linear Setting (NL)

Now, we are ready to state the convergence result of Algorithm 1 in a short version: Theorem 1. The full version of this theorem is Theorem 6, which can be found in Supp. Doc. B.

**Theorem 1**.: _Suppose that Assumptions 1, 2, 3, and 5 holds for the setting \(()\) of (1) and \(>0\) is a sufficiently small tolerance. Let \(\{_{t}\}\) be generated by Algorithm 1 after \(T=(^{-3})\) epochs using arbitrarily permutations \(^{(t)}\) and \(^{(t)}\) and a learning rate \(_{t}=:()\)\((see\) Theorem 6 in Supp. Doc. B for the exact formulas of \(T\) and \(\)). Then, we have \(_{t=0}^{T}\|_{_{t}}(_{t})\|^{2} ^{2}\)._

_Alternatively, if \(\{_{t}\}\) is generated by Algorithm 1 after \(T:=(n^{-1/2}^{-3})\) epochs using two random and independent permutations \(^{(t)}\) and \(^{(t)}\) and a learning rate \(_{t}=:(n^{1/2})\)\((see\) Theorem 6 in Supp. Doc. B for the exact formulas). Then, we have \(_{t=0}^{T}[\|_{_{t}}(_ {t})\|^{2}]^{2}\)._

Our first goal is to approximate a stationary point \(w^{}\) of (CO) as \([\|_{}()\|^{2}]^{2}\), while Algorithm 1 only provides an \(\)-stationary of (10). For a proper choice of \(\), it is also an \(\)-stationary point of (3).

**Corollary 1**.: _Let \(_{T}\) defined by (19) be generated from \(\{_{t}\}\) of Algorithm 1. Under the conditions of Theorem 1 and any permutations \(^{(t)}\) and \(^{(t)}\), the following statements hold._

* _If_ \(h\) _is_ \(_{h}\)_-strongly convex with_ \(_{h}>0\)_, then we can set_ \(=0\)_, and Algorithm_ 1 _requires_ \((n^{-3})\) _evaluations of_ \(F_{i}\) _and_ \( F_{i}\) _to achieve an_ \(\)_-stationary_ \(_{T}\) _of (_3_)._
* _If_ \(h\) _is only convex_ \((\)_i.e._ \(_{h}=0\)\()\)_, then we can set_ \(:=()\)_, and Algorithm_ 1 _needs_ \((n^{-7/2})\) _evaluations of_ \(F_{i}\) _and_ \( F_{i}\) _to achieve an_ \(\)_-stationary_ \(_{T}\) _of (_3_)._

_If, in addition, \(^{(t)}\) and \(^{(t)}\) are sampled uniformly at random without replacement and independently, and \(_{1}=(n^{-1})\), then the numbers of evaluations of \(F_{i}\) and \( F_{i}\) are reduced by a factor of \(\)._

## 4 Shuffling Method for Nonconvex-Strongly Concave Minimax Problems

In this section, we develop shuffling gradient-based methods to solve (1) under the nonconvex-strongly concave setting (NC). Since this setting does not cover the nonconvex-linear setting (NL) in Section 3 as a special case, we need to treat it separately using different ideas and proof techniques.

### The construction of algorithm

Unlike the linear case with \(_{i}(w,u)= F_{i}(w),Ku\) in Section 3, we cannot generally compute the solution \(u_{0}^{}(_{t-1})\) in (2) exactly for a given \(_{t-1}\). We can only approximate \(u_{0}^{}(_{t-1})\) by some \(_{t}\). This leads to another level of inexactness in an approximate "hyper-gradient" \(_{0}(w_{i-1}^{(t)})\) defined by

\[_{0}(w_{i-1}^{(t)}):=_{w}_{^{( t)}(i)}(w_{i-1}^{(t)},_{t}).\] (25)There are different options to approximate \(u_{0}^{*}(_{t-1})\). We propose two options below, but other choices are possible, including accelerated gradient ascent methods and stochastic algorithms .

\((_{1})\) **Gradient ascent scheme for the lower-level problem.** We apply a standard gradient ascent scheme to update \(_{t}\): _Starting from \(s=0\) with \(u_{0}^{(t)}:=_{t-1}\), at each epoch \(s=1,,S\), we update_

\[_{s}^{(t)}\,:=\,_{_{t}h}_{s-1}^{(t)}+_{t}}{n}_{i=1}^{n}_{u}_{ i}(_{t-1},_{s-1}^{(t)}),\] (26)

_for a given learning rate \(_{t}>0\). Then, we finally output \(_{t}:=_{S}^{(t)}\) to approximate \(u_{0}^{*}(_{t-1})\)._

To make our method more flexible, we allow to perform either only _one iteration_ (i.e. \(S=1\)) or _multiple iterations_ (i.e. \(S>1\)) of (26). Each iteration \(s\) requires \(n\) evaluations of \(_{u}_{i}\).

\((_{2})\) **Shuffling gradient ascent scheme for the lower-level problem.** We can also construct \(_{t}\) by a _shuffling gradient ascent scheme_. Again, we allow to run either only _one epoch_ (i.e. \(S=1\)) or _multiple epochs_ (i.e. \(S>1\)) of the shuffling algorithm to update \(_{t}\), leading to the following scheme: _Starting from \(s:=1\) with \(_{0}^{(t)}:=_{t-1}\), at each epoch \(s=1,2,,S\), having \(_{s-1}^{(t)}\), we generate a permutation \(^{(s,t)}\) of \([n]\) and run a shuffling gradient ascent scheme as_

\[\{u_{0}^{(s,t)}:=_{s-1}^{(t)},\\ i=1,2,,n\\ u_{i}^{(s,t)}:=u_{i-1}^{(s,t)}+_{t}}{n}_{u} _{^{(s,t)}(i)}(_{t-1},u_{i-1}^{(s,t)}),\\ _{s}^{(t)}:=_{_{t}h}(u_{n}^{(s,t)}). .\] (27)

_At the end of the \(S\)-th epoch, we output \(_{t}:=_{S}^{(t)}\) as an approximation to \(u_{0}^{*}(_{t-1})\)._ Here, we use the same learning rate \(_{t}\) for all epochs \(s[S]\). Each epoch \(s\) requires \(n\) evaluations of \(_{u}_{i}\).

\(()\) **Shuffling gradient descent scheme for the upper-level minimization problem.** Having \(_{t}\) from either (26) or (27), we run a _shuffling gradient descent epoch_ to update \(_{t}\) from \(_{t-1}\) as

\[\{w_{0}^{(t)}:=_{t-1},\\ i=1,2,,n\\ w_{i}^{(t)}:=w_{i-1}^{(t)}-_{t}}{n}_{0} (w_{i-1}^{(t)}) w_{i-1}^{(t)}-_{t}}{n}_{w} _{^{(t)}(i)}(w_{i-1}^{(t)},_{t}),\\ _{t}:=_{_{t}f}(w_{n}^{(t)})..\] (28)

These two steps (26) (or (27)) in \(u\) and (28) in \(w\) are implemented alternatively for \(t=1,,T\).

\(()\) **The full algorithm.** Combining both steps (26) (or (27)) and (28), we can present an _alternating shuffling proximal gradient algorithm_ for solving (1) as in Algorithm 2.

```
1:Initialization: Choose an initial point \((_{0},_{0})()\).
2:for\(t=1,2,,T\)do
3: Compute \(_{t}\) using either (26) or (27).
4: Set \(w_{0}^{(t)}:=_{t-1}\) and generate a permutation \(^{(t)}\) of \([n]\).
5:for\(i=1,,n\)do
6: Evaluate \(_{0}(w_{i-1}^{(t)}):=_{w}_{^ {(t)}(i)}(w_{i-1}^{(t)},_{t})\).
7: Update \(w_{i}^{(t)}:=w_{i-1}^{(t)}-_{t}}{n}_{0} (w_{i-1}^{(t)})\).
8:endfor
9: Compute \(_{t}:=_{_{t}f}(w_{n}^{(t)})\).
10:endfor ```

**Algorithm 2** (Alternating Shuffling Proximal Gradient Algorithm for Solving (1) under setting (NC))

**Discussion.** Algorithm 2 has a similar form as Algorithm 1, where \(u_{0}^{*}(_{t-1})\) is approximated by \(_{t}\). In Algorithm 1, \(u_{0}^{*}(_{t-1})\) is approximated by \(u_{}^{*}(F_{i}^{(t)})\). Moreover, Algorithm 1 solves the smoothed problem (10) of (3), while Algorithm 2 directly solves (3). Depending on the choice of method to approximate \(u_{0}^{*}(_{t-1})\), we obtain different variants of Algorithm 2. We have proposed two variants:* **Semi-shuffling variant:** We use (26) for computing \(_{t}\) to approximate \(u_{0}^{*}(_{t-1})\).
* **Full-shuffling variant:** We use (27) for computing \(_{t}\) to approximate \(u_{0}^{*}(_{t-1})\).

Note that Algorithm 2 works in an alternative manner, where it approximates \(u_{0}^{*}(_{t-1})\) up to a certain accuracy before updating \(_{t}\). This alternating update is very natural and has been widely applied to solve minimax optimization as well as bilevel optimization problems, see, e.g., [1; 9; 13].

### Convergence analysis

Now, we state the convergence of both variants of Algorithm 2: _semi-shuffling_ and _full-shuffling_ variants. The full proof of the following theorems can be found in Supp. Doc. C.

\(()\) _Convergence of the semi-shuffling variant._ Our first result is as follows.

**Theorem 2**.: _Suppose that Assumptions 1, 2, 4, and 5 hold for (1), and \(_{}\) is defined by (18). Let \(\{(_{t},_{t})\}\) be generated by Algorithm 2 using the **gradient ascent scheme** (26) with \(:=()\) explicitly given in Theorem 8 of Supp. Doc. C, \((0,+_{h}}]\), \(S:=}_{h}+_{H}}{ L_{u}+_{H}}^{-1}=(1)\), and \(T:=(^{-3})\) explicitly given in Theorem 8. Then, we have \(_{t=0}^{T}\|_{}(_{t})\|^{2} ^{2}\)._

_Consequently, Algorithm 2 requires \((n^{-3})\) evaluations of both \(_{w}_{i}\) and \(_{u}_{i}\) to achieve an \(\)-stationary point \(_{T}\) of (3) computed by (19)._

Note that Theorem 2 holds for both \(S>1\) and \(S=1\) (i.e. we perform only one iteration of (26)).

\(()\) _Convergence of the full-shuffling variant - The case \(S>1\) with multiple epochs._ We state our results for two separated cases: only \(_{i}\) is \(_{H}\)-strongly convex, and only \(h\) is \(_{h}\)-strongly convex.

**Theorem 3** (Strong convexity of \(_{i}\)).: _Suppose that Assumptions 1, 2, 4, and 5 hold, and \(_{i}\) is \(_{H}\)-strongly concave with \(_{H}>0\) for \(i[n]\), but \(h\) is only merely convex._

_Let \(\{(_{t},_{t})\}\) be generated by Algorithm 2 using \(S\) epochs of the **shuffling routine** (27) and fixed learning rates \(_{t}=:=()\) as given in Theorem 8 of Supp. Doc. C for a given \(>0\), \(_{t}:==()\), \(S:=}\), and \(T:=(^{-3})\). Then, we have \(_{t=0}^{T}\|_{}(_{t})\|^{2} ^{2}\)._

_Consequently, Algorithm 2 requires \((n^{-3})\) evaluations of \(_{w}_{i}\) and \((n^{-4})\) evaluations of \(_{u}_{i}\) to achieve an \(\)-stationary point \(_{T}\) of (3) computed by (19)._

**Theorem 4** (Strong convexity of \(h\)).: _Suppose that Assumptions 1, 2, 4, and 5 hold for (1), and \(h\) is \(_{h}\)-strongly convex with \(_{h}>0\), but \(_{i}\) is only merely concave for all \(i[n]\). Then, under the same settings as in Theorem 3, but with \(S:=}\), the conclusions of Theorem 3 still hold._

\(()\) _Convergence of the full-shuffling variant - The case \(S=1\) with one epoch._ Both Theorems 3 and 4 require \((n^{-4})\) evaluations of \(_{u}_{i}\). To improve this complexity, we need two additional assumptions but can perform only one epoch of (27), i.e. \(S=1\).

**Assumption 6**.: _Let \(}_{}(u):=^{-1}(u-_{ h}(u+_{u }(w,u)))\) be the gradient mapping of \((w,):=-(w,)+h()\). Assume that there exist \(_{0} 1\) and \(_{1} 0\) such that_

\[\|_{u}(w,u)\|^{2}_{0}\|}_{ }(u)\|^{2}+_{1},(w,u)( ).\] (29)

Clearly, if \(h=0\), then \(}_{}(u)=-_{u}(w,u)\) and (20) automatically holds for \(_{0}=1\) and \(_{1}=0\). Assumption 6 is similar to Assumption 5, and it is required to handle the \(\) operator of \(h\) in (27).

**Assumption 7**.: _For \(f\) in (1), there exists \(L_{f} 0\) such that_

\[f(y) f(x)+ f^{}(x),y-x+}{2}\|y-x\|^{2},  x,y(f),\;f^{}(x) f(x).\] (30)

Clearly, if \(f\) is \(L_{f}\)-smooth, then (30) holds. If \(f\) is also convex, then (30) implies that \(f\) is \(L_{f}\)-smooth.

Under these additional assumptions, we have the following result.

**Theorem 5**.: _Suppose that Assumptions 1, 2, 4, 5, 6, and 7 hold and \(_{}\) is defined by (18). Let \(\{(_{t},_{t})\}\) be generated by Algorithm 2 using **one epoch**\((S=1)\) of the **shuffling routine** (27), and fixed learning rates \(_{t}=:=()\) as in Theorem 9 of Supp. Doc. C for a given \(>0\), \(_{t}:==30^{2}\), and \(T:=(^{-3})\), where \(:=}{_{H}+_{h}}\). Then, we have \(_{t=0}^{T}\|_{}(_{t})\|^{2} ^{2}\).__Consequently, Algorithm 2 requires \((n^{-3})\) evaluations of both \(_{w}_{i}\) and of \(_{u}_{i}\) to achieve an \(\)-stationary point \(_{T}\) of (3) computed by (19)._

Similar to Algorithm 1, if \(^{(s,t)}\) and \(^{(t)}\) are generated randomly and independently, \(_{1}=(1/n)\), and \(_{1}=(1/n)\), then our complexity stated above can be improved by a factor of \(\). Nevertheless, we omit this analysis. Finally, we can combine each Theorem 2, 3, 4 or 5 and Lemma 2 to construct an \(\)-KKT point of (1). Theorem 5 has a better complexity than Theorems 3 and 4, but requires stronger assumptions. Algorithm 2 is also different from the one in  both in terms of algorithmic form and the underlying problem to be solved, while achieving the same oracle complexity.

## 5 Numerical Experiments

We perform some experiments to illustrate Algorithm 1 and compare it with two existing and related algorithms. Further details and additional experiments can be found in Supp. Doc. D.

We consider the following regularized stochastic minimax problem studied, e.g., in [9; 33]:

\[_{w^{p}}_{1 j m}_ {i=1}^{n}F_{i,j}(w)}+\|w\|^{2}},\] (31)

where \(F_{i,j}:^{p}_{+}\) can be viewed as the loss of the \(j\)-th model for data point \(i[n]\). If we define \(_{0}(v):=_{1 j m}\{v_{j}\}\) and \(f(w):=\|w\|^{2}\), then (31) can be reformulated into (3). Since \(v_{j} 0\), we have \(_{0}(v):=_{1 j m}\{v_{j}\}=\|v\|_{}=_{\|u\|_{1} 1 } v,u\), which is nonsmooth. Thus we can smooth \(_{0}\) as \(_{}(v):=_{\|u\|_{1} 1}\{ v,u-(/2)\|u\|^{2}\}\) using \(b(u):=\|u\|^{2}\).

Here, we apply our problem (31) to solve a model selection problem in binary classification with nonnegative nonconvex losses, see, e.g., . Each function \(F_{i,j}\) belongs to \(4\) different nonconvex losses (\(m=4\)): \(F_{i,1}(w,):=1-(b_{i} a_{i},w)\), \(F_{i,2}(w,):=(1+(-b_{i} a_{i},w))-(1+(-b_{i}  a_{i},w-1))\), \(F_{i,3}(w,):=(1-1/((-b_{i} a_{i},w)+1))^{2}\), and \(F_{i,4}(w,):=(1+(-b_{i} a_{i},w))\) (see  for more details), where \((a_{i},b_{i})\) represents data samples.

We implement 4 algorithms: our SGM with 2 options, SGD from , and Prox-Linear from . We test these algorithms on two datasets from LIBSVM . We set \(:=10^{-4}\) and update the smooothing parameter \(_{t}\) as \(_{t}:=}\). The learning rate \(\) for all algorithms is finely tuned from \(\{100,50,10,5,1,0.5,0.1,0.05,0.01,0.001,0.0001\}\), and the results are shown in Figure 1 for **w8a** and **rev1** datasets using \(k_{b}=32\) blocks. The details of this experiment is given in Supp. Doc. D.

As shown in Figure 1, the two variants of our SGM have a comparable performance with SGD and Prox-Linear, providing supportive evidence for using shuffling strategies in minimax algorithms.

## 6 Conclusions

This work explores a bilevel optimization approach to address two prevalent classes of nonconvex-concave minimax problems. These problems find numerous applications in practice, including robust learning and generative AIs. Motivated by the widespread use of shuffling strategies in implementing gradient-based methods within the machine learning community, we develop novel shuffling-based algorithms for solving these problems under standard assumptions. The first algorithm uses a non-standard shuffling strategy and achieves the state-of-the-art oracle complexity typically observed in nonconvex optimization. The second algorithm is also new, flexible, and offers a promising possibility for further exploration. Our results are expected to provide theoretical justification for incorporating shuffling strategies into minimax optimization algorithms, especially in nonconvex settings.

Figure 1: The performance of 4 algorithms for solving (31) on two datasets after 200 epochs.