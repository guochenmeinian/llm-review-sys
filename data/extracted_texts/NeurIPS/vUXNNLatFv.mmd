# A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing

Junren Chen

University of Hong Kong

chenjr58@connect.hku.hk

&Jonathan Scarlett

National University of Singapore

scarlett@comp.nus.edu.sg

&Michael K. Ng

Hong Kong Baptist University

michael-ng@hkbu.edu.hk

&Zhaoqiang Liu

UESTC

zqliu12@gmail.com

Corresponding authors.

###### Abstract

In generative compressed sensing (GCS), we want to recover a signal \(}^{n}\) from \(m\) measurements (\(m n\)) using a generative prior \(} G(^{k}_{2}(r))\), where \(G\) is typically an \(L\)-Lipschitz continuous generative model and \(^{k}_{2}(r)\) represents the radius-\(r\)\(_{2}\)-ball in \(^{k}\). Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed \(}\) rather than for all \(}\) simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index models as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, _all_\(} G(^{k}_{2}(r))\) can be recovered up to an \(_{2}\)-error at most \(\) using roughly \((k/^{2})\) samples, with omitted logarithmic factors typically being dominated by \( L\). Notably, this almost coincides with existing non-uniform guarantees up to logarithmic factors, hence the uniformity costs very little. As part of our technical contributions, we introduce the Lipschitz approximation to handle discontinuous observation models. We also develop a concentration inequality that produces tighter bounds for product processes whose index sets have low metric entropy. Experimental results are presented to corroborate our theory.

## 1 Introduction

In compressed sensing (CS) that concerns the reconstruction of low-complexity signals (typically sparse signals) , it is standard to employ a random measurement ensemble, i.e., a random sensing matrix and other randomness that produces the observations. Thus, a recovery guarantee involving a single draw of the measurement ensemble could be _non-uniform_ or _uniform_ -- the non-uniform one ensures the accurate recovery of any fixed signal with high probability, while the uniform one states that one realization of the measurements works simultaneously for all structured signals of interest. Uniformity is a highly desired property in CS, since in applications the measurement ensemble is typically fixed and should work for all signals . Besides, the derivation of a uniform guarantee is often significantly harder than a non-uniform one, making uniformity an interesting theoretical problem in its own right.

Inspired by the tremendous success of deep generative models in different applications, it was recently proposed to use a generative prior to replace the commonly used sparse prior in CS , which led to numerical success such as a significant reduction of the measurement number. This new perspective for CS, which we call generative compressed sensing (GCS), has attracted a large volume of research interest, e.g., nonlinear GCS , MRI applications , and information-theoretic bounds , among others. This paper focuses on the uniform recovery problem for nonlinear GCS, which is formally stated below. Our main goal is to build a unified framework that can produce uniform recovery guarantees for various nonlinear measurement models.

**Problem:** Let \(^{k}_{2}(r)\) be the \(_{2}\)-ball with radius \(r\) in \(^{k}\). Suppose that \(G\,:\,^{k}_{2}(r)^{n}\) is an \(L\)-Lipschitz continuous generative model, \(_{1},...,_{m}^{n}\) are the sensing vectors, \(^{}:=G(^{k}_{2}(r))\) is the underlying signal, and we have the observations \(y_{i}=f_{i}(_{i}^{}^{}),\ i=1,,m\), where \(f_{1}(),,f_{m}()\) are possibly unknown,2 possibly random non-linearities. Given a single realization of \(\{_{i},f_{i}\}_{i=1}^{m}\), under what conditions we can _uniformly_ recover all \(^{}\) from the corresponding \(\{_{i},y_{i}\}_{i=1}^{m}\) up to an \(_{2}\)-norm error of \(\)?

### Related Work

We divide the related works into nonlinear CS (based on traditional structures like sparsity) and nonlinear GCS.

**Nonlinear CS:** Beyond the standard linear CS model where one observes \(y_{i}=_{i}^{}^{}\), recent years have witnessed rapidly increasing literature on nonlinear CS. An important nonlinear CS model is 1-bit CS that only retains the sign \(y_{i}=(_{i}^{}^{})\). Subsequent works also considered 1-bit CS with dithering \(y_{i}=(_{i}^{}^{}+_{i})\) to achieve norm reconstruction under sub-Gaussian sensing vectors . Besides, the benefit of using differing was found in uniformly quantized CS with observation \(y_{i}=_{}(_{i}^{}^{}+_{i})\), where \(_{}()=[]+ \) is the uniform quantizer with resolution \(\). Moreover, the authors of  studied the more general single index model (SIM) where the observation \(y_{i}=f_{i}(_{i}^{}^{})\) involves (possibly) unknown nonlinearity \(f_{i}\).

While the restricted isometry property (RIP) of the sensing matrix \(=[_{1},...,_{m}]^{}\) leads to uniform recovery in linear CS , this is not true in nonlinear CS. In fact, many existing results are non-uniform , and some uniform guarantees can be found in . Most of these uniform guarantees suffer from a slower error rate.

The most relevant work to this paper is the recent work  that described a unified approach to uniform signal recovery for nonlinear CS. The authors of  showed that in the aforementioned models with \(k\)-sparse \(^{}\), a uniform \(_{2}\)-norm recovery error of \(\) could be achieved via generalized Lasso using roughly \(k/^{4}\) measurements [17, Section 4]. In this work, we build a unified framework for uniform signal recovery in nonlinear GCS. To achieve a uniform \(_{2}\)-norm error of \(\) in the above models with the generative prior \(^{} G(^{k}_{2}(r))\), our framework only requires a number of samples proportional to \(k/^{2}\). Unlike  that used the technical results  to bound the product process, we develop a concentration inequality that produces a tighter bound in the setting of generative prior, thus allowing us to derive a sharper uniform error rate.

**Nonlinear GCS:** Building on the seminal work by Bora _et al._, numerous works have investigated linear or nonlinear GCS , with a recent survey  providing a comprehensive overview. Particularly for nonlinear GCS, 1-bit CS with generative models has been studied in , and generative priors have been used for SIM in . In addition, score-based generative models have been applied to nonlinear CS in .

The majority of research for nonlinear GCS focuses on non-uniform recovery, with only a few exceptions . Specifically, under a generative prior, [33, Section 5] presented uniform recovery guarantees for SIM where \(y_{i}=f_{i}(_{i}^{}^{})\) with deterministic Lipschitz \(f_{i}\) or \(f_{i}(x)=(x)\). Their proof technique is based on the local embedding property developed in , which is a geometric property that is often problem-dependent and currently only known for 1-bit measurements and deterministic Lipschitz link functions. In contrast, our proof technique does not rely on such geometric properties and yields a unified framework with more generality. Furthermore,  did not consider dithering, which limits their ability to estimate the norm of the signal.

The authors of  derived a uniform guarantee from dithered 1-bit measurements under bias-free ReLU neural network generative models, while we obtain a uniform guarantee with the comparable rate for more general Lipschitz generative models. Additionally, their recovery program differs from the generalized Lasso approach (_cf._ Section 2.1) used in our work. Specifically, they minimize an \(_{2}\) loss with \(\|\|_{2}^{2}\) as the quadratic term, while generalized Lasso uses \(\|\|_{2}^{2}\) that depends on the sensing vector. As a result, our approach can be readily generalized to sensing vectors with an unknown covariance matrix [33, Section 4.2], unlike  that is restricted to isotropic sensing vectors. Under random dithering, while  only considered 1-bit measurements, we also present new results for uniformly quantized measurements (also referred to as multi-bit quantizer in some works ).

### Contributions

In this paper, we build a unified framework for uniform signal recovery in nonlinear GCS. We summarize the paper structure and our main contributions as follows:

* We present Theorem 1 as our main result in Section 2. Under rather general observation models that can be discontinuous or unknown, Theorem 1 states that the uniform recovery of all \(^{} G(_{2}^{k}(r))\) up to an \(_{2}\)-norm error of \(\) can be achieved using roughly \(O}\) samples. Specifically, we obtain uniform recovery guarantees for 1-bit GCS, 1-bit GCS with dithering, Lipschitz-continuous SIM, and uniformly quantized GCS with dithering.
* We provide a proof sketch in Section 3. Without using the embedding property as in , we handle the discontinuous observation model by constructing a Lipschitz approximation. Compared to , we develop a new concentration inequality (Theorem 2) to derive tighter bounds for the product processes arising in the proof.

We also perform proof-of-concept experiments on the MNIST  and CelebA  datasets for various nonlinear models to demonstrate that by using a single realization of \(\{_{i},f_{i}\}_{i=1}^{m}\), we can obtain reasonably accurate reconstruction for multiple signals. Due to the page limit, the experimental results and detailed proofs are provided in the supplementary material.

### Notation

We use boldface letters to denote vectors and matrices, while regular letters are used for scalars. For a vector \(\), we let \(\|\|_{q}\) (\(1 q\)) denote its \(_{q}\)-norm. We use \(_{q}^{n}(r):=\{^{n}\,:\,\|\|_{q} r\}\) to denote the \(_{q}\) ball in \(^{n}\), and \((_{q}^{n}(r))^{c}\) represents its complement. The unit Euclidean sphere is denoted by \(^{n-1}:=\{^{n}\,:\,\|\|_{2}=1\}\). We use \(C,C_{i},c_{i},c\) to denote absolute constants whose values may differ from line to line. We write \(A=O(B)\) or \(A B\) (resp. \(A=(B)\) or \(A B\)) if \(A CB\) for some \(C\) (resp. \(A cB\) for some \(c\)). We write \(A B\) if \(A=O(B)\) and \(A=(B)\) simultaneously hold. We sometimes use \(()\) to further hide logarithmic factors, where the hidden factors are typically dominated by \( L\) in GCS, or \( n\) in CS. We let \((,)\) be the Gaussian distribution with mean \(\) and covariance matrix \(\). Given \(_{1},_{2}^{n}\), \(^{n}\) and some \(a\), we define \(_{1}_{2}:=\{_{1}_{2}:_{1} _{1},_{2}_{2}\}\), \(+_{1}:=\{\}+_{1}\), and \(a_{1}:=\{a:_{1}\}\). We also adopt the conventions of \(a b=\{a,b\}\), and \(a b=\{a,b\}\).

## 2 Main Results

We first give some preliminaries.

**Definition 1**.: _For a random variable \(X\), we define the sub-Gaussian norm \(\|X\|_{_{2}}:=\{t>0:(X^{2}/t^{2}) 2\}\) and the sub-exponential norm \(\|X\|_{_{1}}:=\{t>0:(|X|/t) 2\}\). \(X\) is sub-Gaussian (resp. sub-exponential) if \(\|X\|_{_{2}}<\) (resp. \(\|X\|_{_{1}}<\)). For a random vector \(^{n}\), we let \(\|\|_{_{2}}:=_{^{n-1}}\|^{}\|_{ _{2}}\)._

**Definition 2**.: _Let \(\) be a subset of \(^{n}\). We say that a subset \(_{0}\) is an \(\)-net of \(\) if every point in \(\) is at most \(\) distance away from some point in \(_{0}\), i.e., \(_{0}+_{2}^{n}()\). Given a radius \(\), we define the covering number \((,)\) as the minimal cardinality of an \(\)-net of \(\). The metric entropy of \(\) with respect to radius \(\) is defined as \((,)=(,)\)._

### Problem Setup

We make the following assumptions on the observation model.

**Assumption 1**.: _Let \((0,_{n})\) and let \(f\) be a possibly unknown, possibly random non-linearity that is independent of \(\). Let \((_{i},f_{i})_{i=1}^{m}\) be i.i.d. copies of \((,f)\). With a single draw of \((_{i},f_{i})_{i=1}^{m}\), for \(^{}=G(_{2}^{k}(r))\), where \(G:_{2}^{k}(r)^{n}\) is an \(L\)-Lipschitz generative model, we observe \(y_{i}:=f_{i}(_{i}^{}^{})}_{i=1}^{m}\). We can express the model more compactly as \(=(^{})\), where \(=[_{1},...,_{m}]^{}^{m n}\), \(=(f_{1},...,f_{m})^{}\) and \(=(y_{1},...,y_{m})^{}^{m}\)._

In this work, we consider the generalized Lasso as the recovery method , whose core idea is to ignore the non-linearity and minimize the regular \(_{2}\) loss. In addition, we need to specify a constraint that reflects the low-complexity nature of \(^{}\), and specifically, we introduce a problem-dependent scaling factor \(T\) and use the constraint "\( T\)". Note that this is necessary even if the problem is linear; for example, with observations \(=2^{}\), one needs to minimize the \(_{2}\) loss over "\( 2\)". Also, when the generative prior is given by \(Tx^{}=G(_{2}^{k}(r))\), we should simply use "\(\)" as constraint; this is technically equivalent to the treatment adopted in  (see more discussions in Remark 5 below). Taken collectively, we consider

\[}=_{ T}\|-\|_{2}.\] (2.1)

Importantly, we want to achieve uniform recovery of all \(^{}\) with a single realization of \((,)\).

### Assumptions

Let \(f\) be the function that characterizes our nonlinear measurements. We introduce several assumptions on \(f\) here, and then verify them for specific models in Section 2.3. We define the set of discontinuities as

\[_{f}=\{a:fa\}.\]

We define the notion of jump discontinuity as follows.

**Definition 3**.: (Jump discontinuity)_. A function \(f:\) has a jump discontinuity at \(x_{0}\) if both \(L^{-}:=_{x x_{0}^{-}}f(x)\) and \(L^{+}:=_{x x_{0}^{+}}f(x)\) exist but \(L^{-} L^{+}\). We simply call the oscillation at \(x_{0}\), i.e., \(|L^{+}-L^{-}|\), the jump._

Roughly put, our framework applies to piece-wise Lipschitz continuous \(f_{i}\) with (at most) countably infinite jump discontinuities, which have bounded jumps and are well separated. The precise statement is given below.

**Assumption 2**.: _For some \((B_{0},L_{0},_{0})\), the following statement unconditionally holds true for any realization of \(f\) (specifically, \(f_{1},,f_{m}\) in our observations):_

* \(_{f}\) _is one of the following:_ \(\)_, a finite set, or a countably infinite set;_
* _All discontinuities of_ \(f\) _(if any) are jump discontinuities with the jump bounded by_ \(B_{0}\)_;_
* \(f\) _is_ \(L_{0}\)_-Lipschitz on any interval_ \((a,b)\) _satisfying_ \((a,b)_{f}=\)_._
* \(|a-b|_{0}\) _holds for any_ \(a,b_{f}\)_,_ \(a b\) _(we set_ \(_{0}=\) _if_ \(|_{f}| 1\)_)._

_For simplicity, we assume \(f(x_{0})=_{x x_{0}^{+}}f(x)\) for \(x_{0}_{f}\).3_

We note that Assumption 2 is satisfied by \(L\)-Lipschitz \(f\) with \((B_{0},L_{0},_{0})=(0,L,)\), 1-bit quantized observation \(f()=(+)\) (\(\) is the potential dither, similarly below) with \((B_{0},L_{0},_{0})=(2,0,)\), and uniformly quantized observation \(f()=+\) with \((B_{0},L_{0},_{0})=(,0,)\).

Under Asssumption 2, for any \([0,}{2})\) we construct \(f_{i,}\) as the Lipschitz approximation of \(f_{i}\) to deal with the potential discontinuity of \(f_{i}\) (i.e., \(_{f_{i}}\)). Specifically, \(f_{i,}\) modifies \(f_{i}\) in \(_{f_{i}}+[-,]\) to be piece-wise linear and Lipschitz continuous; see its precise definition in (3.4).

We develop Theorem 2 to bound certain product processes appearing in the analysis, which produces bounds tighter than  when the index sets have low metric entropy. To make Theorem 2 applicable, we further make the following Assumption 3, which can be checked case-by-case by estimating the sub-Gaussian norm and probability tail. Also, \(U_{g}^{(1)}\) and \(U_{g}^{(2)}\) can even be a bit crude because the measurement number in Theorem 1 depends on them in a logarithmic manner.

**Assumption 3**.: _Let \((0,_{n})\), under Assumptions 1-2, we define the Lipschitz approximation \(f_{i,}\) as in (3.4). We let_

\[_{i,}(a):=f_{i,}(a)-Ta,\ _{i,}(a):=f_{i,}(a)-f_ {i}(a).\] (2.2)

_For all \((0,}{2})\), we assume the following holds with some parameters \((A_{g}^{(1)},U_{g}^{(1)},P_{0}^{(1)})\) and \((A_{g}^{(2)},U_{g}^{(2)},P_{0}^{(2)})\):_

* \(_{}\|_{i,}(^{})\|_{_{2}} A _{g}^{(1)}\)_,_ \(_{}|_{i,}(^{} )| U_{g}^{(1)} 1-P_{0}^{(1)};\)__
* \(_{}\|_{i,}(^{})\|_{ _{2}} A_{g}^{(2)}\)_,_ \(_{}|_{i,}(^{ })| U_{g}^{(2)} 1-P_{0}^{(2)}.\)__

To build a more complete theory we further introduce two useful quantities. For some \(\), we define the target mismatch \(()\) as in [17, Definition 1]:

\[()=f_{i}(_{i}^{})_{i }-T_{2}.\] (2.3)

It is easy to see that \(f_{i}(_{i}^{})_{i}\) minimizes the expected \(_{2}\) loss \(\|-\|_{2}^{2}\), thus one can roughly understand \(f_{i}(_{i}^{})_{i}\) as the expectation of \(}\). Since \(T\) is the desired ground truth, a small \(()\) is intuitively an important ingredient for generalized Lasso to succeed. Fortunately, in many models, \(()\) with a suitably chosen \(T\) will vanish (e.g., linear model , single index model , 1-bit model ) or at least be sufficiently small (e.g., 1-bit model with dithering ).

As mentioned before, our method to deal with discontinuity of \(f_{i}\) is to introduce its approximation \(f_{i,}\), which differs from \(f_{i}\) only in \(_{f_{i}}+[-,]\). This will produce some bias because the actual observation is \(f_{i}(_{i}^{}^{*})\) rather than \(f_{i,}(_{i}^{}^{*})\). Hence, for some \(\) we define the following quantity to measure the bias induced by \(f_{i,}\):

\[_{}()=^{}_{f_{i} }+-,,\ \ (0,_{n}).\] (2.4)

The following assumption can often be satisfied by choosing suitable \(T\) and sufficiently small \(_{1}\).

**Assumption 4**.: _Suppose Assumptions 1-3 hold true with parameters \(B_{0},L_{0},_{0},A_{g}^{(1)},A_{g}^{(2)}\). For the \(T\) used in (2.1), \(()\) defined in (2.3) satisfies_

\[_{}()(A_{g}^{(1)} A_{g}^{(2)}) }.\] (2.5)

_Moreover, there exists some \(0<_{1}<}{2}\) such that_

\[(L_{0}_{1}+B_{0})_{}}()} (A_{g}^{(1)} A_{g}^{(2)})}.\] (2.6)

In the proof, the estimation error \(\|}-T^{*}\|\) is contributed by a concentration term of scaling \((A_{g}^{(1)} A_{g}^{(2)})\) and some bias terms. The main aim of Assumption 4 is to pull down the bias terms so that the concentration term is dominant.

### Main Theorem and its Implications

We now present our general theorem and apply it to some specific models.

**Theorem 1**.: _Under Assumptions 1-4, given any recovery accuracy \((0,1)\), if it holds that \(m(A_{g}^{(1)} A_{g}^{(2)})^{}}{^{2}}\), then with probability at least \(1-m(P_{0}^{(1)}+P_{0}^{(2)})-m(-(n))-C(-(k))\) on a single realization of \((,):=(_{i},f_{i})_{i=1}^{m}\), we have the uniform signal recovery guarantee \(\|}-T^{}\|_{2}\) for all \(^{}\), where \(}\) is the solution to (2.1) with \(=(^{})\), and \(=\) is a logarithmic factor with \(\) being polynomial in \((L,n)\) and other parameters that typically scale as \(O(L+n)\). See (C.11) for the precise expression of \(\)._

To illustrate the power of Theorem 1, we specialize it to several models to obtain concrete uniform signal recovery results. Starting with Theorem 1, the remaining work is to select parameters that justify Assumptions 2-4. We summarize the strategy as follows: (i) Determine the parameters in Assumption 2 by the measurement model; (ii) Set \(T\) that verifies (2.5) (see Lemmas 8-11 for the following models); (iii) Set the parameters in Assumption 3, for which bounding the norm of Gaussian vector is useful; (iv) Set \(_{1}\) to guarantee (2.6) based on some standard probability argument. We only provide suitable parameters for the following concrete models due to space limit, while leaving more details to Appendix E.

**(A) 1-bit GCS.** Assume that we have the 1-bit observations \(y_{i}=(_{i}^{}^{})\); then \(f_{i}()=f()=()\) satisfies Assumption 2 with \((B_{0},L_{0},_{0})=(2,0,)\). In this model, it is hopeless to recover the norm of \(\|^{}\|_{2}\); as done in previous work, we assume \(^{}^{n-1}\)[31, Remark 1]. We set \(T=\) and take the parameters in Assumption 3 as \(A_{g}^{(1)} 1,U_{g}^{(1)},P_{0}^{(1)}(-(n)),A_{g}^{(2 )} 1,U_{g}^{(2)} 1,P_{0}^{(2)}=0\). We take \(=_{1}\) to guarantee (2.6). With these choices, Theorem 1 specializes to the following:

**Corollary 1**.: _Consider Assumption 1 with \(f_{i}()=()\) and \(^{n-1}\), let \((0,1)\) be any given recovery accuracy. If \(m}(n}{(k/m )})\),4 then with probability at least \(1-2m(-cn)-m(-(k))\) on a single draw of \((_{i})_{i=1}^{m}\), we have the uniform signal recovery guarantee \(\|}-}^{}\|_{2}\) for all \(^{}\), where \(}\) is the solution to (2.1) with \(=(^{})\) and \(T=}\)._

**Remark 1**.: _A uniform recovery guarantee for generalized Lasso in 1-bit GCS was obtained in [33, Section 5]. Their proof relies on the local embedding property in . Note that such geometric property is often problem-dependent and highly nontrivial. By contrast, our argument is free of geometric properties of this kind._

**Remark 2**.: _For traditional 1-bit CS, [17, Corollary 2] requires \(m(k/^{4})\) to achieve uniform \(_{2}\)-accuracy of \(\) for all \(k\)-sparse signals, which is inferior to our \((k/^{2})\). This is true for all remaining examples. To obtain such a sharper rate, the key technique is to use our Theorem 2 (rather than ) to obtain tighter bound for the product processes, as will be discussed in Remark 8._

**(B) 1-bit GCS with dithering.** Assume that the \(_{i}^{}^{}\) is quantized to 1-bit with dither5\(_{i}}{{}}[-, ]\) for some \(\) to be chosen, i.e., we observe \(y_{i}=(_{i}^{}^{}+_{i})\). Following  we assume \(_{2}^{2}(R)\) for some \(R>0\). Here, using dithering allows the recovery of signal norm \(\|^{}\|_{2}\), so we do not need to assume \(^{n-1}\) as in Corollary 1. We set \(=CR\) with sufficiently large \(C\), and \(T=^{-1}\). In Assumption 3, we take \(A_{g}^{(1)} 1,\ U_{g}^{(1)},\ P_{0}^{(1)}(-(n)),\ A_{g}^{(2 )} 1,\ U_{g}^{(2)} 1\), and \(P_{0}^{(2)}=0\). Moreover, we take \(=_{1}=\) to guarantee (2.6). Now we can invoke Theorem 1 to get the following.

**Corollary 2**.: _Consider Assumption 1 with \(f_{i}()=(+_{i})\), \(_{i}[-,]\) and \(_{2}^{n}(R)\), and \(=CR\) with sufficiently large \(C\). Let \((0,1)\) be any given recovery accuracy. If \(m}(n}{( (k/m))})\), then with probability at least \(1-2m(-cn)-m(-(k))\) on a single draw of \((_{i},_{i})_{i=1}^{m}\), we have the uniform signal recovery guarantee \(\|}-^{-1}^{}\|_{2}\) for all \(^{}\), where \(}\) is the solution to (2.1) with \(=(^{}+)\) (here, \(=[_{1},...,_{m}]^{}\)) and \(T=^{-1}\)._

**Remark 3**.: _To our knowledge, the only related prior result is in [45, Theorem 3.2]. However, their result is restricted to ReLU networks. By contrast, we deal with the more general Lipschitz generative models; by specializing our result to the ReLU network that is typically \((n^{(d)})\)-Lipschitz  (\(d\) isthe number of layers), our error rate coincides with theirs up to a logarithmic factor. Additionally, as already mentioned in the Introduction Section, our result can be generalized to a sensing vector with an unknown covariance matrix, unlike theirs which is restricted to isotropic sensing vectors. The advantage of their result is in allowing sub-exponential sensing vectors._

**(C) Lipschitz-continuous SIM with generative prior.** Assume that any realization of \(f\) is unconditionally \(\)-Lipschitz, which implies Assumption 2 with \((B_{0},L_{0},_{0})=(0,,)\). We further assume \((f(0)) 1-P_{0}^{}\) for some \((,P_{0}^{})\). Because the norm of \(^{}\) is absorbed into the unknown \(f()\), we assume \(^{n-1}\). We set \(=0\) so that \(f_{i,}=f_{i}\). We introduce the quantities \(=[f(g)g],=\|f(g)\|_{_{2}},g(0,1)\). We choose \(T=\) and set parameters in Assumption 3 as \(A_{g}^{(1)}+,\;U_{g}^{(1)}(+)+,\;P_ {0}^{(1)} P_{0}^{}+(-(n)),\;A_{g}^{(2)}+, \;U_{g}^{(2)}=0,\;P_{0}^{(2)}=0\). Now we are ready to apply Theorem 1 to this model. We obtain:

**Corollary 3**.: _Consider Assumption 1 with \(\)-Lipschitz \(f\), suppose that \((f(0)) 1-P_{0}^{}\), and define the parameters \(=[f(g)g]\), \(=\|f(g)\|_{_{2}}\) with \(g(0,1)\). Let \((0,1)\) be any given recovery accuracy. If \(}( (+)(+)++]}{^{2}})\), then with probability at least \(1-2m(-cn)-mP_{0}^{}-c_{1}(-(k))\) on a single draw of \((_{i},f_{i})_{i=1}^{m}\), we have the uniform signal recovery guarantee \(\|}-T^{}\|_{2}\) for all \(^{}\), where \(}\) is the solution to (2.1) with \(=(^{})\) and \(T=\)._

**Remark 4**.: _While the main result of  is non-uniform, it was noted in [33, Section 5] that a similar uniform error rate can be established for any deterministic \(1\)-Lipschitz \(f\). Our result here is more general in that the \(\)-Lipschitz \(f\) is possibly random. Note that randomness on \(f\) is significant because it provides much more flexibility (e.g., additive random noise)._

**Remark 5**.: _For SIM with unknown \(f_{i}\) it may seem impractical to use (2.1) as it requires \(=[f(g)g]\) where \(g(0,1)\). However, by assuming \(^{}=G(_{2}^{k}(r))\) as in , which is natural for sufficiently expressive \(G()\), we can simply use \(\) as constraint in (2.1). Our Corollary 3 remains valid in this case under some inessential changes of \(\) factors in the sample complexity._

**(D) Uniformly quantized GCS with dithering.** The uniform quantizer with resolution \(>0\) is defined as \(_{}(a)=+ \) for \(a\). Using dithering \(_{i}[-,]\), we suppose that the observations are \(y_{i}=_{}(_{i}^{}^{}+_{i})\). This satisfies Assumption 2 with \((B_{0},L_{0},_{0})=(,0,)\). We set \(T=1\) and take parameters for Assumption 3 as follows: \(A_{g}^{(1)},U_{g}^{(1)},A_{g}^{(2)},U_{g}^{(2)}\), and \(P_{0}^{(1)}=P_{0}^{(2)}=0.\) We take \(=_{1}\) to confirm (2.6). With these parameters, we obtain the following from Theorem 1.

**Corollary 4**.: _Consider Assumption 1 with \(f()=_{}(+)\), \([-,]\) for some quantization resolution \(>0\). Let \(>0\) be any given recovery accuracy. If \(mk}{^{2}}}{ [k/(m)]}\), then with probability at least \(1-2m(-cn)-c_{1}(-(k))\) on a single draw of \((_{i},_{i})_{i=1}^{m}\), we have the uniform recovery guarantee \(\|}-^{}\|_{2}\) for all \(^{}\), where \(}\) is the solution to (2.1) with \(=_{}(+)\) and \(T=1\) (here, \(=[_{1},,_{m}]^{}\))._

**Remark 6**.: _While this dithered uniform quantized model has been widely studied in traditional CS (e.g., non-uniform recovery , uniform recovery ), it has not been investigated in GCS even for non-uniform recovery. Thus, this is new to the best of our knowledge._

A simple extension to the noisy model \(=(^{})+\) where \(^{m}\) has i.i.d. sub-Gaussian entries can be obtained by a fairly straightforward extension of our analysis; see Appendix F.

## 3 Proof Sketch

To provide a sketch of our proof, we begin with the optimality condition \(\|-}\|_{2}^{2}\|-(T^{})\|_{2}^ {2}\). We expand the square and plug in \(=(^{})\) to obtain

\[\|}{}(}-T^{})\|_{2}^{2} (^{})-T^{}, (}-T^{}).\] (3.1)

For the final goal \(\|}-T^{}\|_{2}\), up to rescaling, it is enough to prove \(\|}-T^{}\|_{2} 3\). We assume for convenience that \(\|}-T^{}\|_{2}>2\), without loss of generality. Combined with \(},Tx^{} T\), we know \(}-Tx^{}_{}^{-}\), where \(_{}^{-}:=(T^{-})(_{2}^{n}(2 ))^{c},\ ^{-}=-\). We further define

\[(_{}^{-})^{*}:=\{/\|\|_{2}: _{}^{-}\}\] (3.2)

where the normalized error lives, i.e. \(}-T^{}}{\|-T^{}\|_{2}}( _{}^{-})^{*}\). Our strategy is to establish a uniform lower bound (resp., upper bound) for the left-hand side (resp., the right-hand side) of (3.1). We emphasize that these bounds must hold uniformly for all \(^{}\).

It is relatively easy to use set-restricted eigenvalue condition (S-REC)  to establish a uniform lower bound for the left-hand side of (3.1), see Appendix B.1 for more details. It is significantly more challenging to derive an upper bound for the right-hand side of (3.1). As the upper bound must hold uniformly for all \(^{}\), we first take the supremum over \(^{}\) and \(}\) and consider bounding the following:

\[:=(^{})-T ^{},(}-T^{})\] (3.3) \[=_{i=1}^{m}f_{i}(_{i}^{}^{ })-T_{i}^{}^{}_{i}^{ }[}-T^{}]\] \[\|}-T^{}\|_{2}_{ }_{(_{}^{-})^{*}}_{ i=1}^{m}f_{i}(_{i}^{})-T_{i}^{} _{i}^{}:=\|}-T^{ }\|_{2}_{u},\]

where \((_{}^{-})^{*}\) is defined in (3.2). Clearly, \(_{u}\) is the supremum of a product process, whose factors are indexed by \(\) and \((_{}^{-})^{*}\). It is, in general, challenging to control a product process, and existing results often require both factors to satisfy a certain "sub-Gaussian increments" condition (e.g., [36; 37]). However, the first factor of \(_{u}\) (i.e., \(f_{i}(_{i}^{}^{})-T_{i}^{}^{}\)) does not admit such a condition when \(f_{i}\) is not continuous (e.g., the 1-bit model \(f_{i}=()\)). We will construct the Lipschitz approximation of \(f_{i}\) to overcome this difficulty shortly in Section 3.1.

**Remark 7**.: _We note that these challenges stem from our pursuit of uniform recovery. In fact, a non-uniform guarantee for SIM was presented in [33, Theorem 1]. In its proof, the key ingredient is [33, Lemma 3] that bounds \(_{u}\) without the supremum on \(\). This can be done as long as \(f_{i}(_{i}^{}^{})\) is sub-Gaussian, while the potential discontinuity of \(f_{i}\) is totally unproblematic._

### Lipschitz Approximation

For any \(x_{0}_{f_{i}}\) we define the one-sided limits as \(f_{i}^{-}(x_{0})=_{x x_{0}^{-}}f_{i}(x)\) and \(f_{i}^{+}(x_{0})=_{x x_{0}^{+}}f_{i}(x)\), and write their average as \(f_{i}^{a}(x_{0})=(f_{i}^{-}(x_{0})+f_{i}^{+}(x_{0}))\). Given any approximation accuracy \((0,}{2})\), we construct the Lipschitz continuous function \(f_{i,}\) as:

\[f_{i,}(x)=f_{i}(x)&,x_{f_{i}}+[- ,]\\ f_{i}^{a}(x_{0})-^{+}(x_{0})-f_{i}(x_{0}-)](x_{0}- x)}{}, x_{0}_{f_{i}}x[x_{0}-,x_{0}]\\ f_{i}^{a}(x_{0})+(x_{0}+)-f_{i}^{-}(x_{0})](x-x_{0}) }{}, x_{0}_{f_{i}},x[x_{0},x_{0}+].\] (3.4)

We have defined the approximation error \(_{i,}()=f_{i,}()-f_{i}()\) in Assumption 3. An important observation is that both \(f_{i,}\) and \(|_{i,}|\) are Lipschitz continuous (see Lemma 1 below). Here, it is crucial to consider \(|_{i,}|\) rather than \(_{i,}\) as the latter is not continuous; see Figure 1 for an intuitive graphical illustration and more explanations in Appendix B.2.

**Lemma 1**.: _With \(B_{0},L_{0},_{0}\) given in Assumption 2, for any \((0,}{2})\), \(f_{i,}\) is \(L_{0}+}{}\)-Lipschitz over \(\), and \(|_{i,}|\) is \(2L_{0}+}{}\)-Lipschitz over \(\)._

### Bounding the product process

We now present our technique to bound \(_{u}\). Recall that \(_{i,}(a)\) and \(_{i,}(a)\) were defined in (2.2). By Lemma 1, \(_{i,}\) is \(L_{0}+T+}{}\)-Lipschitz. Now we use \(f_{i}(a)-Ta=_{i,}(a)-_{i,}\) to decompose \(_{u}\) (in the following, we sometimes shorten "\(_{}_{(_{}^{-})^{*}}\)" as "\(_{,}\)"):

\[_{u},}_{i=1}^{m}_{ i,}(_{i}^{})_{i}^{}}_{ _{u1}}+,}_{i=1}^{m} |_{i,}(_{i}^{})|_{i}^{ }}_{_{u2}}.\] (3.5)It remains to control \(_{u1}\) and \(_{u2}\). By the Lipschitz continuity of \(_{i,}\) and \(|_{i,}|\), the factors of \(_{u1}\) and \(_{u2}\) admit sub-Gaussian increments, so it is natural to first center them and then invoke the concentration inequality for product process due to Mendelson [36, Theorem 1.13], which we restate in Lemma 5 (Appendix A). However, this does not produce a tight bound and would eventually require \((k/^{4})\) to achieve a uniform \(_{2}\)-error of \(\), as is the case in [17, Section 4].

In fact, Lemma 5 is based on _Gaussian width_ and hence blind to the fact that \(,(_{e}^{-})^{*}\) here have low _metric entropy_ (Lemma 6). By characterizing the low intrinsic dimension of index sets via metric entropy, we develop the following concentration inequality that can produce tighter bound for \(_{u1}\) and \(_{u2}\). This also allows us to derive uniform error rates sharper than those in [17, Section 4].

**Theorem 2**.: _Let \(g_{}=g_{}()\) and \(h_{}=h_{}()\) be stochastic processes indexed by \(^{p_{1}}, ^{p_{2}}\), both defined with respect to a common random variable \(\). Assume that:_

* _(A1.)_ \(g_{}(),\ h_{}()\) _are sub-Gaussian for some_ \((A_{g},A_{h})\) _and admit sub-Gaussian increments regarding_ \(_{2}\) _distance for some_ \((M_{g},M_{h})\)_:_ \[&\|g_{}()-g_{^{}}() \|_{_{2}} M_{g}\|-^{}\|_{2},\ \|g_{}()\|_{_{2}} A_{g},\ \ ,^{} ;\\ &\|h_{}()-h_{^{}}()\|_{_{2}}  M_{h}\|-^{}\|_{2},\ \|h_{}()\|_{_{2}} A_{h},\ \ ,^{} .\] (3.6)
* _(A2.)_ _On a single draw of_ \(\)_, for some_ \((L_{g},U_{g},L_{h},U_{h})\) _the following events simultaneously hold with probability at least_ \(1-P_{0}\)_:_ \[&|g_{}()-g_{^{}}() | L_{g}\|-^{}\|_{2},\ |g_{}()| U_{g},\ \ ,^{} ;\\ &|h_{}()-h_{^{}}()| L_{h}\| {v}-^{}\|_{2},\ |h_{}()| U_{h},\ \ ,^{} .\] (3.7)

_Let \(_{1},...,_{m}\) be i.i.d. copies of \(\), and introduce the shorthand \(S_{g,h}=L_{g}U_{h}+M_{g}A_{h}\) and \(T_{g,h}=L_{h}U_{g}+M_{h}A_{g}\). If \(m(,A_{h}}{S_{g,h}}) +(,A_{h}}{T_{g,h}})\), where \((,)\) is the metric entropy defined in Definition 2, then with probability at least \(1-mP_{0}-2-(,A_{h}}{ S_{g,h}})+(,A_{h}}{T_{g,h}}) \) we have \(IA_{h}}{}(,A _{h}}{S_{g,h}})+(,A_{h}}{T_{g,h}})}\), where \(I:=_{}_{} _{i=1}^{m}g_{}(_{i})h_{}(_{i})-[ g_{}(_{i})h_{}(_{i})]\) is the supremum of a product process._

**Remark 8**.: _We use \(_{u2}\) as an example to illustrate the advantage of Theorem 2 over Lemma 5. The key step is on bounding the centered process_

\[_{u2,c}:=_{}_{(_{e }^{-})}|_{i,}(_{i}^{})||_{i}^{ }|-[|_{i,}(_{i}^{})||_{i}^{}|]}.\]

_Let \(g_{}(_{i})=|_{i,}(_{i}^{})|\) and \(h_{}(_{i})=|_{i}^{}|\), then one can use Theorem 2 or Lemma 5 to bound \(_{u2,c}\). Note that \(||_{i}^{}||_{_{2}}=O(1)\) justifies the choice \(A_{h}=O(1)\), and both \((,)\) and \(((_{e}^{-})^{*},)\) depend linearly on \(k\) but only logarithmically on \(\) (Lemma 6), so Theorem 2 could bound \(_{u2,c}\) by \(A_{g}\) that depends on \(M_{g}\) in a logarithmic manner. However, the bound produced by Lemma 5 depends linearly on \(M_{g}\); see term \(A_{h}()}{}\) in (A.1). From (3.6), \(M_{g}\) should be proportional to the Lipschitz constant of \(|_{i,}|\), which scales as \(\) (Lemma 1). The issue is that in many cases we need to take extremely small \(\) to guarantee that (2.6) holds true (e.g., we take \( k/m\) in 1-bit GCS). Thus, Lemma 5 produces a worse bound compared to our Theorem 2._

Figure 1: (Left): \(f_{i}\) and its approximation \(f_{i,0.5}\); (Right): approximation error \(_{i,0.5},|_{i,0.5}|\).

Conclusion

In this work, we built a unified framework for uniform signal recovery in nonlinear generative compressed sensing. We showed that using generalized Lasso, a sample size of \((k/^{2})\) suffices to uniformly recover all \( G(_{2}^{k}(r))\) up to an \(_{2}\)-error of \(\). We specialized our main theorem to 1-bit GCS with/without dithering, single index model, and uniformly quantized GCS, deriving uniform guarantees that are new or exhibit some advantages over existing ones. Unlike , our proof is free of any non-trivial embedding property. As part of our technical contributions, we constructed the Lipschitz approximation to handle potential discontinuity in the observation model, and also developed a concentration inequality to derive tighter bound for the product processes arising in the proof, allowing us to obtain a uniform error rate faster than . Possible future directions include extending our framework to handle the adversarial noise and representation error.

**Acknowledgment.** J. Chen was supported by a Hong Kong PhD Fellowship from the Hong Kong Research Grants Council (RGC). J. Scarlett was supported by the Singapore National Research Foundation (NRF) under grant A-0008064-00-00. M. K. Ng was partially supported by the HKRGC GRF 17201020, 17300021, CRF C7004-21GF and Joint NSFC-RGC N-HKU76921.