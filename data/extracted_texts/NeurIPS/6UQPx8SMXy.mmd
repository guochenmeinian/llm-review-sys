# Alexandros Stergiou

LAVIB: A Large-scale Video Interpolation BenchmarkUniversity of Twente, NL

a.g.stergiou@utwente.nl

###### Abstract

This paper introduces a **LA**rge-scale **V**ideo **I**ntepolation **B**enchmark (LAVIB) for the low-level video task of Video Frame Interpolation (VFI). LAVIB comprises a large collection of high-resolution videos sourced from the web through an automated pipeline with minimal requirements for human verification. Metrics are computed for each video's motion magnitudes, luminance conditions, frame sharpness, and contrast. The collection of videos and the creation of quantitative challenges based on these metrics are under-explored by current low-level video task datasets. In total, LAVIB includes 283K clips from 17K ultra-HD videos, covering 77.6 hours. Benchmark train, val, and test sets maintain similar video metric distributions. Further splits are also created for out-of-distribution (OOD) challenges, with train and test splits including videos of dissimilar attributes. 1

## 1 Introduction

Long uncompressed video streams capture events over varying motion intensities, light conditions, and color dynamic ranges. Although loading and storing individual videos is rudimentary, processing and reading large volumes can bottleneck availability. The high-volume transfer of videos with large filesizes can also result in bandwidth overheads and long decoding times. Low-level vision tasks such as Video Frame Interpolation (VFI) [3; 10; 16; 19; 21; 30; 41; 42; 44; 50; 75], Video Super-Resolution (VSR) [5; 13; 17; 18; 22; 28; 31; 54; 63; 65], and Video Denoising (VD) [14; 32; 53; 58; 61; 64] aim to address such challenges by enabling the storage and stream of lower-resolution, lower-frame-rate, compressed videos. Despite the wide application of such approaches to adjacent tasks such as localization and mapping [26; 71], object tracking , novel view synthesis [43; 52], and slow-motion video generation [19; 20], existing datasets for low-level video tasks [2; 41; 46; 55; 56; 59; 60; 61; 72] contain short videos, with a small number of frames per video. With the exception of  most of these datasets only include either a few hundreds [2; 41; 46; 61] or thousands [40; 55; 56; 59; 60] of videos with limited variations in the motions, luminance, and object-level sharpness. To address this gap, this paper introduces a **LA**rge **V**ideo **I**ntepolation **B**enchmark (**LAVIB**), for learning to interpolate high-resolution videos across varying motion, blur, luminance, and contrast settings. LAVIB is built on per-frame metrics that quantitatively measure motion magnitudes, frame sharpness, video contrast, and overall luminance. In Fig. 1, LAVIB videos are visualized over axes corresponding to the metrics used.

The selected metrics establish a diverse, general, and robust benchmark for VFI as most prior efforts have focused on specific settings. Seminal works [38; 55] sourced videos from high frame-rate sensors that are less relevant to videos recorded by commonly used devices. Other works use videos of standardized resolutions and frame rates. These are either datasets of larger sizes with low-resolution videos [59; 72] or smaller datasets of high-resolution [41; 55; 56; 60; 61]. Comparisons to other video datasets across metrics are discussed in SS2.

LAVIB contains 283,484 video segments totaling approximately 77.6 hours. The segments are sourced from 17,204 clips with \(3840\) x \(2160\) (4K) resolution and 60 frames-per-second (fps). Statistics are discussed in SS3. Similar to previous efforts comprised of 4K videos [38; 55; 60], LAVIB is compiled by temporally and spatially cropping tubelets from the 4K videos to fit clips into memory.

The data collection pipeline for LAVIB is detailed in SS4. This includes the creation of a vocabulary of search query terms. Clips are sourced from YouTube videos queried by search terms. Preset clip sampling intervals are used to standardize durations. Segments are selected from high average flow magnitude temporal locations calculated with . Spatial locations are selected by tubelets of high/low metrics values. The final train/val/test sets are constructed by balancing all metrics.

Widely-used VFI methods [16; 23; 75] are benchmarked on the LAVIB val and test sets in SS5. Performance is reported across well-adopted evaluation metrics [4; 8; 9; 12; 29; 76]. LAVIB's large size and video diversity enables pretraining models of greater generalizability that are in turn evaluated on test sets of smaller down-stream datasets targeting either scene diversity , high frame rates , or high video resolution [38; 41]. In addition to the main benchmark splits, four challenges with two settings each, are introduced for Out-Of-Distribution (OOD) VFI. Train, val, and test sets with unbalanced metric distributions are created for each challenge and setting. Videos are assigned to sets based on their average motion magnitude, sharpness, contrast, and luminance metrics. These challenges evaluate model generalizability over diverse domains that are different in the train and test sets.

## 2 Related works

Initial VFI benchmarks  provided real image sequences and ground truth optical flow annotations with average resolutions of \(640 480\). The dataset comprised a small number of videos used primarily for evaluation. Vid4  is a standardized testing benchmark for VFI and VSR consisting of four videos of \(740 480\) and \(720 576\) resolutions. Similarly,  is also used for VSR with videos sampled from . Later efforts  have also introduced benchmarks for VD in tandem with VSR and VFI. More recent works  included 3.2K HD videos captured with a GOPRO4 Hero Black with frame averaging to simulate lower shutter speeds.  also proposed a synthetic dataset with 3D objects from  and backgrounds from [24; 27]. The trajectories of objects were uniformly sampled from fixed bounds. Works have also studied VFI for specific domains such as animations [56; 7] introduced benchmarks under large motion conditions with 20 240-fps videos sourced from YouTube. Recently,  introduced a high-resolution high-frame rate benchmark for video interpolation and super-resolution. It includes a total of 4,423 videos recorded with a Phantom Flex4K.

Most similar to LAVIB, adjacent efforts that compile 4K video datasets [38; 41; 55; 60] source videos from media in which professional equipment are used; e.g. movies [38; 41] or high-resolution video recordings . Videos from these datasets are primarily recorded with sensors under optimal shutter speeds and calibrated luminance for capturing specific motion types. In contrast, LAVIB includes videos from various sensors such as hand-held, action, professional, or drone cameras, and screen

Figure 1: **LAVIB videos distributed across metrics. Four metrics are computed per video. Average Flow Magnitude (AFM) quantifies motion. The Average Laplacian Variance (ALV) is used to describe the sharpness of frames. The Average Root Mean Square (ARMS) is used for contrast. The Average Relevant Luminance (ARL) relates to the video brightness. The four aforementioned metrics are used for Out-Of-Distribution (ODD) challenges: Fast. and slow, and, motions. Low, high  and high, low sharpness. Low, high  and high, contrast. Bright, and dark, bright, luminance.**

captures. The videos differ in their dynamic range, levels of post-processing, and compression. LAVIB is intended as a general-purpose dataset and benchmark without being specific to sensor types or settings. Examples of videos are shown in Fig. 1.

LAVIB is compared in Tab. 1 to adjacent video datasets over different statistics. **Dataset statistics** include the number of videos and total running times. **Video statistics** relate to video information such as the resolution and frame rate. **Average video metrics** provide metrics on the variance of motions, lighting conditions, and frame sharpness. Definitions of the metrics are detailed in SS3. LAVIB has threefold more videos than  and equally larger total video running time than . The difference in LAVIB video conditions and recording sensors is reflected by the high variance across metrics in Tab. 1. With the exception of , tailored for videos of fast motions with high optical flow magnitude, LAVIB has the highest variance per metric across datasets.

## 3 LAVIB statistics

Four statistics are used to obtain segments, create splits, and define challenges. An overview is shown in Tab. 2 with the number of videos with the highest/lowest metrics reported.

**Frame-pair motion**. A significant challenge for VFI methods is learning to model the cross-frame motion consistency of videos. Thus, the proposed dataset includes videos of diverse magnitudes; both high camera or object motion, and more static scenes. Motion magnitudes can be quantified with dense optical flow. FlowFormer  is used on each frame pair resulting in 598 frame pairs per video. The spatial resolution of videos is reduced by \( 0.25\) to fit frames in memory. The Averaged Flow Magnitude (AFM) is defined by spatio-temporally averaging optical flow. AFM variances are reported for all datasets in Tab. 1.

**Frame sharpness**. Sourced videos vary by the sensors, lens, codex, and camera profiles used. They can capture different motions, light conditions, and camera focus. All these factors amount to significant variations in the sharpness of videos. Thus, object edges or sensory noise may be highlighted or suppressed. The Laplacian of Gaussians (LoG) is a standardized kernel-based approach for highlighting regions of rapid change in pixel intensities. Given a video \(\) of dimensions \(^{D=T H W}\), with \(T\) frames, \(H\) height, and \(W\) width, it convolves a kernel with size \(K\) over each frame. ALV is formulated by applying LoG and averaging:

    &  &  &  \\  & Year & Tot. & Mins & Tot. & Vids & Src Res. & FPS & AFM & ARL & ALV \\  UCF101  & 2012 & 1,600 & 13,320 & 240p & 25 & 2.43 \(\) 1.85 & 53.37 \(\) 13.42 & 53.99 \(\) 18.37 \\ Xiph  & 2020 & 4 & 19 & **2160p** & 60 & 26.21 \(\) 25.19 & 60.64 \(\) 10.77 & 95.24 \(\) 62.32 \\ Inter4K  & 2021 & 83 & 1,000 & **2160p** & 60 & 56.38 \(\) 14.34 & 56.79 \(\) 14.48 & 25.05 \(\) 24.05 \\ X4K1KFPS  & 2021 & 191 & 4,243 & **2160p** & 960 & 266.87 \(\) 178.72 & 53.95 \(\) 12.07 & 135.67 \(\) 78.19 \\ Vimeo0K  & 2017 & 356 & 91,701 & 720p & 30 & 49.63 \(\) 18.32 & 59.68 \(\) 20.89 & 26.26 \(\) 29.25 \\ LAVIB (ours) & 2024 & **4,660** & **283,484** & **2160p** & 60 & 63.10 \(\) 58.41 & 38.34 \(\) 28.69 & 199.78 \(\) 197.79 \\   

Table 1: **Datasets**. Compared to prior efforts, LAVIB provides a large-scale general-purpose dataset of standardized 4K 60 fps videos. It features a significant variance across Average Flow Magnitude (AFM), Average Relevant Luminance (ARL), and Average Laplacian Variances (ALV) in videos.

    &  &  &  &  \\   & \# Low Flow Mag & 19,605 (10.3\%) & 3,846 (9.3\%) & 23,451 (10.1\%) & 4,898 (9.1\%) \\  & \# High Flow Mag & 18,976 (10.1\%) & 3,891 (9.4\%) & 22,867 (9.9\%) & 5,482 (10.2\%) \\   & \# Low Lap. Var. & 18,313 (9.6\%) & 3,541 (8.6\%) & 21,854 (9.5\%) & 6,494 (12.1\%) \\  & \# High Lap. Var. & 17,348 (9.2\%) & 3,871 (9.4\%) & 21,219 (9.2\%) & 7,130 (13.3\%) \\   & \# Low Perc. Lum. & 17,669 (9.3\%) & 3,638 (8.8\%) & 21,307 (9.2\%) & 7,041 (13.1\%) \\  & \# High Perc. Lum. & 19,297 (10.2\%) & 4,400 (10.7\%) & 23,697 (10.3 \%) & 4,652 (8.6\%) \\   & \# Low RMS Cont. & 18,794 (10.0\%) & 3,657 (8.8\%) & 22,451 (9.8\%) & 5,897 (11.0\%) \\  & \# High RMS Cont. & 18,363 (9.7\%) & 4,036 (9.8 \%) & 22,399 (9.7\%) & 5,950 (11.1\%) \\   &  &  &  &  \\   

Table 2: **LAVIB split statistics**. Details per metric for each split.

\[(,,K)=_{r^{D}}_{i=1}^{K} _{j=1}^{K}}(1-+j^{2}}{2 ^{2}})e^{-+j^{2}}{2^{2}}}}_{(i,j)}}\] (1)

As the size of the kernel also factors the estimate, an ensemble of kernel sizes \(=\{3,5,7\}\) is used to calculate the final value \(|}_{K}(, ,K)\) with \(=1.4\). Overall, in LAVIB 18,313 train, 3,541 val, and 6,494 test videos are at the upper 10% of the ALV ensemble.

**Video contrast**. Another characteristic of videos is the contrast between objects and backgrounds in scenes. The human visual system is more sensitive to the contrast between foreground and background , compared to other adjacent measures such as the perceived luminance (brightness), or frame sharpness (blurriness). Computationally, contrast relates to the difference between neighboring raw pixel values. The metric is formulated as the _Average Root Mean Square_ (ARMS)  difference between each pixel from each frame of **V** and the corresponding pixel in the channel-averaged \(}\).

\[()=_{t^{T}}_{s^{HW}}(_{t,s}-}_{t,s})}\] (2)

LAVIB includes 22,399 videos of high contrast for train and val and, 22,451 videos of low contrast.

**Luminance conditions**. In addition to the overall video conditions, the perception of light can be affected by the sensor's sensitivity or the camera's processing. In human vision, the perception of luminance is done over three bands of color. To account for the uneven perception of each band, a common standard for quantitatively defining luminosity is the relevant luminance . In videos, the Average Relative Luminance (ARL) can be computed as the weighted sum for each color channel from video frames based on , which in turn is averaged over time. The bottom 10th ARL percentile in LAVIB includes 17,669 train, 3,638 val, and 7,041 test videos. Similarly, there are 19,297 train, 4,400 val, and 4,652 test high-luminance videos.

As shown in Tab. 2, videos selected for all splits are balanced across metrics. This is done explicitly for the main benchmark and not the OOD challenges.

## 4 LAVIB pipeline

The video selection pipeline includes several stages for the collection, extraction, and set assignment. Initially, videos are searched on YouTube by textual prompts designed to return relevant videos with 4K resolutions and 60 frames per second as overviewed in SS4.1. Sourced videos are cropped to 10-second clips standardizing their durations and improving processing speeds in further steps of the collection pipeline. Segments with high motion magnitudes are selected from the clips and are cropped to tubelets by their AVL, ARL, ARMS, and AFM statistics as detailed in SS4.2. Dataset splits are balanced between the four statistics, with OOD splits created by assigning videos with the highest average metric at the test or train set. The dataset pipeline is flexible and can be scaled over large numbers of videos, requiring manual input only at a few points.

### Video web-crawling

The first stage of the data collection pipeline constructs queries to search and identify videos on YouTube with 4K resolution and 60 fps. The vocabulary of search terms is created from a finite combination of different categories e.g.; locations, activities, weather conditions, and camera types. This aims to diversify results over the defined categories with a level of control (See appendix A1 for a full discussion on vocabulary creation). The vocabulary terms are compiled with three guidelines.

**Videos should be in the wild**. Retrieved videos should vary by lighting conditions, motions, and scenes. They should also be recorded with different sensors. Sensor types depend strongly on video themes; e.g. action cameras are more common for capturing fast-paced scenes in contrast to DSLR cameras. Conditions are added in the format; rainy walk in New York or night drive. Some text prompts are also designed to include specific equipment such as GoPro Hero10 or iPhone 13 Pro.

**Video content should correspond to raw footage**. The dictionary of general search terms aims to improve control over the video context by retrieving specific video types. Videos with substantial post-production cuts, or transitions, can be less relevant or usable for VFI. The video types that are collected focus primarily on raw footage.

**Exclusivity of video categories**. Vocabulary queries should also include diversity in the themes present. This is done by constructing verb hierarchies. A balanced number of queries is constructed for objects/locations that are the focus of the videos.

Each vocabulary search term is combined with '4K' and used as a query on YouTube. The query-related URLs are scraped from the contents in the response's script. Candidate videos are downloaded only if a 4K format with 60 fps is available. This step is needed as YouTube's search prioritizes video elements such as titles, tags, and descriptions over metadata.

**Limitations**. Queries are created from a finite set of search terms. The diversity of locations and activities is manually defined thus, limitations are expected. As noted above, the selected videos are more diverse than current VFI benchmarks however, an increased vocabulary can improve this further.

### Segment selection and split assignment

In total, 667 hours of footage are collected over the project's 31-month duration. This initial list contained videos of hour-long to minute-long durations. To standardize their durations, 10-second clips are sampled manually over different interval steps. Clips are extracted consecutively for videos less than 5 minutes. For the rest of the videos; 10-second sampling intervals are used for videos with durations between 5-30 minutes, 2-minute intervals for videos between 30 minutes to an hour, and 10-minute intervals for videos longer than an hour. This selection resulted in a total of 34,408 clips. Clips from the same video are bound to include similarities. To account for this and inspired by , similarities between clips from the same video are measured metrically by their embedding space distance with highly similar clips being dropped. MViTv2-B  is used to encode clips and

Figure 2: **LAVIB segment selection and challenges pipeline**. Candidate 10-second clips are sampled from a long video based on their embedding similarity. Dense optical flow is computed with  and spatially averaged for the AFM metric. The 1-second clips with the top-20% AFM are selected for the next step. Clips are further partitioned into four tubelets used in the final dataset based on their ARL, ALV, ARMS, and AFM. The metrics are also used for video selection in OOD challenges for a. motion, b. sharpness, c. contrast, d. luminance.

to create a similarity matrix based on the L2 distance of the final layer embeddings. Clips with an average (row-wise) L2 distance below the entire matrix's average distance are dropped. This step resulted in the selection of 17,204 clips.

The final two stages include both temporal and spatial cropping. They are overviewed in Fig. 2.

**Temporal segment selection**. Segments are compared and selected by their AFM. This selection aims to drop primarily static segments as they are less relevant to VFI tasks with minimal pixel and object tracking requirements. FlowFormer  is used to calculate AFM over pairs of frames by spatially averaging flows. Each 10-second sequence is temporally augmented to obtain all available 1-second clips. Clips with the highest 20% magnitudes are selected. This strategy was chosen as it worked well in a small-scale setting when manually examining a set of 1,000 clips.

**Spatial segment selection**. The selected high-resolution clips cannot directly fit into the memory of most current GPUs. Thus, as commonly addressed in the literature [38; 41; 55; 72] the number of videos is curated with the additional selection of tubelets. Each clip is divided into four tubelets by a \(2 2\) grid. ALV, ARL, ARMS, and AFM are computed for each tubelet. 80% of the tubelets are retained by selecting from the low/high values per metric in succession, leaving out the middle 20%. This avoids oversampling from values close to the mean of metrics. Instead, tubelets with more challenging settings are selected.

**Assignment to splits**. All train/val/test splits are constructed with a 65-15-20% split. DUPLEX  selection is used for balancing split statistics. Videos with the largest pair-wise distance by their metrics are initially selected. In turn, videos are iteratively assigned to sets given their distance from the previously selected videos. A detailed overview of the algorithm is provided in SSA2. Recall that the OOD sets need to be imbalanced across statistics so this is specific to the benchmark splits.

**Limitations**. No prior work has tackled video collection based on these metrics, so thresholds for each step are manually defined. This can constrain the final dataset size as the values were selected empirically to maximize diversity.

## 5 Benchmarks

**Baselines**. LAVIB contains 188,644 1-second videos for training, 41,345 videos for validation, and 53,494 videos for testing. Benchmark results are reported in SS5.1 across settings. For the baselines, triplets of frames are defined similarly to [7; 59; 72] for single-frame interpolation with a total of \(\)5.7M triplets. In the multi-frame interpolation settings in SS5.2, septuplets are also used resulting in a total of \( 2.4\)M groups of frames. Ablations on varying video resolutions are presented in SS5.3. In SS5.4, the video metrics are used to create _unbalanced_ dataset splits. For each of the four metrics, two challenges are created by sampling videos with either high/low values and assigning them to the train/test sets. Qualitative results for all three models are shown in SS5.5.

**Model details**. Three VFI methods are benchmarked; RIFE , EMA-VFI , and FLAVR , which in turn are trained and tested on LAVIB. The official codebases made publicly available by their respective authors are adjusted and used for LAVIB for all experiments. Adapted training and test code, and models are available at https://github.com/alexandrosstergiou/LAVIB.

**Training details**. The training and model settings are imported from the original papers and codebases. The train batch size is set to 64 for all models and the start learning rate is reduced by \( 0.25\) for all models to account for the increased batch size.

**Evaluation metrics**. Standard image and video quality metrics are used for all tasks and benchmarks. Quantitative results report the Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) . In multi-frame interpolation, the average value over the interpolated frames is reported.

   Pre-train & Pre-train & Fine-tune &  &  \\ LAVIB & Vimeo-90K & Xiph + X4K1KFPS & PSNR\(\) & SSIM\(\) & LLIPS1 & PSNR\(\) & SSIM\(\) & LLIPS1 \\   & ✓ & & & 32.86 & 0.968 & 3.152e\({}^{2}\) & 32.10 & 0.963 & 3.947e\({}^{2}\) \\   & ✓ & ✓ & & 31.36 & 0.952 & 4.620e\({}^{-2}\) & 31.78 & 0.948 & 5.154e\({}^{2}\) \\ ✓ & & & **33.72** & **0.981** & **2.515e\({}^{2}\)** & **33.44** & **0.981** & **2.934e\({}^{2}\)** \\   

Table 3: **LAVIB val and test results using  as a baseline across training schemes. Evaluation metrics are reported for both val and test sets. Best results per metric are denoted in bold.**

### Baseline results

**Baselines**. Tab. 3 reports SSIM, PSNR, and LPIPS scores on both LAVIB val and test sets across three training settings; pre-training on Vimeo-90K, fine-tuning on a joint set from Xiph [38; 41] and X4K1KFPS  of exclusively 4K videos, and pre-training with LAVIB. FLAVR  is used as the baseline model due to its fast processing times, strong results, and open-source codebase. Finetuning on Xiph + X4K1KFPS suffers as both datasets are small in size although they are sourced by videos with the same resolution as LAVIB. Pre-training only on Vimeo-90K slightly improves results. Pre-training on LAVIB gives the best performance overall increasing PSNR, and SSIM by +1.08 and +0.015 on average on both sets.

**Generalization to related small-scale datasets**. VFI benchmarks include multiple datasets [38; 41; 55; 72]. LAVIB is unique in having the largest number of diverse videos of _both_ high resolution and high frame rates. The generalization benefits of using LAVIB as the pre-training dataset are compared to the previously widely-used Vimeo-90K . Tab. 4 shows performance improvements in the test set of Vimeo-90K when the model is trained on LAVIB. Similar score increases are also observed for the Xiph4K and X4K1KFPS test sets in Tabs. 5 and 6 with +1.23 and +1.19 improvements on the PSNR. LAVIB's large variance across videos enables learning VFI over different conditions which can benefit performance in smaller domain-specific benchmark datasets.

**Multi-metric results**. As human judgment of the perceptual quality depends on high-order image structures and context [36; 67], an ensemble of metrics is reported in Tab. 7 to provide a complete evaluation of each methods' performance on the LAVIB test set. In addition to standard quality metrics, scores over recently-proposed metrics including DISTS , Watson-DFT , VSFA , and VFIPS  are also reported. Across statistics, both EMA-VFI and FLAVR perform comparably. A decrease in performance is observed with RIFE as its limited complexity can not adequately address VFI with large variations in settings across videos. Compared to FLAVR, the PSNR and SSIM scores decrease by -5.56 and -1.10 respectively, and the LPIPS loss increases from 0.029 to 0.146.

### Multi-frame interpolation results

This section ablates the number of frames interpolated and evaluated over different schemes with; \( 2\) interpolation being equivalent to interpolating 30fps videos to 60fps, \( 3\) interpolating 20fps to 60fps, and \( 4\) interpolating 15fps to 60fps. Triplets and septuplets of frames as input are also ablated. Results are reported in Tab. 8. FLAVR trained on Vimeo-90K is used as a baseline in all settings.

**Varying number of interpolated frames**. The LAVIB-trained model  consistently outperforms the baseline trained on Vimeo-90K across different numbers of interpolated frames. An average -1.19/-0.02 PSNR/SSIM drop is observed across \(\{ 2, 3, 4\}\) interpolations when septuplets of frames are used. This drop is more significant for triplets with -1.75/-0.078 PSNR/SSIM.

**Varying number of input frames**. Two settings are used for defining inputs. In triplets, models input a single proceeding and a single succeeding frame with the interpolation target being the in-between frame. In septuplets, two proceeding and two succeeding frames are used as inputs. Models trained with septuplets demonstrate only moderate PSNR/SSIM performance improvements across interpolation settings. This shows that regardless of the input settings the dataset remains challenging.

   Model & \(\)PSNR\(\) & \(\)SSIM\(\) & LPIPS\(\)  & \(\)DISTS\(\)  & \(\)Watson-DFTT  & \(\)VSFA\(\) & \(\)VFIPS\(\) \\  RIFE & 27.88 & 0.871 & 1.416\({}^{-1}\) & 1.870\(}\) & 0.215 & 0.558 & 0.561 \\ EMA-VFI & 33.14 & 0.978 & 3.105\(}\) & 5.076\(}\) & 0.344 & 0.607 & 0.638 \\ FLAVR & **33.44** & **0.981** & **2.934\(}\)** & **4.430\(}\)** & **0.360** & **0.626** & **0.667** \\   

Table 7: **Multi-metric evaluation results on LAVIB test. Performance is reported for \(\) image-based metrics averaged across frames and \(\) video-based metrics.**

**Varying frame sampling**. LAVIB's standardized 60fps also enables works to explore VFI over more challenging settings with multiple temporal resolutions. Tab. 9 reports performance on 30fps targets created by sampling every 2 frames to form triplets. Results show consistency between densely sampling frames sequentially (\(30 60\)) and sampling with a step of 2 (\(15 30\)).

### Varying frame resolution results

An important factor for VFI is the clarity of the objects. Different computational budgets can limit availability in training schemes and memory use.

**Resolutions across models**. Results on different training set resolutions are reported in Tab. 11. As in , \(256 256\) is the standard resolution used for training all models. A proportional decrease in performance is observed at lower resolutions. However, these reductions remain small with an average \(-0.14\)/\(-0.01\) in PSNR/SSIM when using \(224 224\) and \(-0.87\)/\(-0.02\) when using \(112 112\). Thus, LAVIB can be a suitable benchmark for evaluating low-compute VFI models.

**Frame resolutions across training schemes**. Tab. 10 reports performances across varying resolutions with different dataset training sets. Compared to the LAVIB-trained model, performance degrades significantly at lower resolutions with the smaller and less diverse Vimeo-90K. The large and varying LAVIB training set can be an effective alternative for training on lower compute resources in which full-resolution videos do not fit in memory.

### OOD Challenges

OOD challenges aim to test the generalizability of models to domains different from the ones trained. In low to high challenges, train sets include videos of low AFM, ALV, ARMS, or ARL values and the remaining videos of high-value metrics are used for testing. For high to low challenges, train sets have high AFM, ALV, ARMS, or ARL values and test sets have low values.

**Low/High AFM**. As shown in Tab. 11(a), existing VFI models cannot effectively interpolate frames when trained on videos with low motion magnitudes. Compared to the benchmark results in Tab. 7 a \(-2.64\) and \(-0.04\) drop is observed for PSNR and SSIM. The embedding distance to ground truth frames also increases by +9.825e\({}^{-2}\). In contrast, when models are trained on high motion magnitudes, VFI is easier for the target domain of primarily low magnitudes. The imbalance in performance shows the sensitivity of current models to the motion magnitudes of the training data.

**Low/High ALV**. Sharpness-based comparisons are reported in Tab. 11(b). Testing on low-sharpness settings is more challenging for VFI models as object edges are more difficult to define. However,

    &  &  &  \\  & PSNR\(\) & SSIM\(\) & LLIPS\(\) & PSNR\(\) & SSIM\(\) & LLIPS\(\) & PSNR\(\) & SSIM\(\) & LLIPS\(\) \\  EMA-VFI & 32.26 & 0.954 & 4.130e\({}^{-2}\) & 33.01 & 0.972 & 3.211e\({}^{-2}\) & 33.14 & 0.978 & 3.105e\({}^{-2}\) \\ FLAVR & 32.57 & 0.965 & 3.781e\({}^{-2}\) & 33.28 & 0.973 & 3.086e\({}^{-2}\) & **33.44** & **0.981** & **2.934e\({}^{-2}\)** \\   

Table 11: **Frame resolutions ablations**. Best results per metric are denoted in **bold** and best results per model are underlined.

    &  &  \\  & \( 2\) & \( 3\) & \( 4\) & \( 2\) & \( 3\) & \( 4\) \\  & PSNR\(\)/SSIM\(\) & PSNR\(\)/SSIM\(\) & PSNR\(\)/SSIM\(\) & PSNR\(\)/SSIM\(\) & PSNR\(\)/SSIM\(\) \\  Baseline & 32.10/0.963 & 31.58/0.952 & 30.42/0.937 & 32.69/0.976 & 32.10/0.972 & 31.95/0.918 \\ FLAVR & **33.44/0.981** & **33.07/0.975** & **32.86/0.968** & **33.62/0.985** & **33.41/0.980** & **33.28/0.962** \\   

Table 8: **Multi-frame interpolation scores** over triplets, and septuplets across different numbers of interpolated frames. Increase in video duration due to interpolation is denoted with \(\{ 2, 3, 4\}\).

    &  30\)} &  60\)} \\  & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & LLIPS\(\) & PSNR\(\) & SSIM\(\) & LLIPS\(\) \\  FLAVR & 33.21 & 0.978 & 33.44 & 0.981 & & & & & \\   

Table 10: **LAVIB test set scores across frame resolutions with FLAVR on different training schemes. Main results default settings in gray.**

    &  &  &  \\  & PSNR\(\) & SSIM\(\) & LLIPS\(\) & PSNR\(\) & SSIM\(\) & LLIPS\(\) & PSNR\(\) & SSIM\(\) & LLIPS\(\) \\   & 32.26 & 0.954 & 4.130e\({}^{-2}\) & 33.01 & 0.972 & 3.211e\({}^{-2}\) & 33.14 & 0.978 & 3.105e\({}^{-2}\) \\  & 32.57 & 0.965 & 3.781e\({}^{-2}\) & 33.28 & 0.973 & 3.086e\({}^{-2}\) & **33.44** & **0.981** & **2.934e\({}^{-2}\)** \\   

Table 11: **Frame resolutions ablations**. Best results per metric are denoted in **bold** and best results per model are underlined.

models trained on low-sharpness videos can interpolate high-sharpness videos with an average +2.93 and +0.023 increase in the PSNR and SSIM scores compared to the low-to-high task.

**Low/High ARMS**. Results on contrast-based OOD challenges are presented in Tab. 11(c). The domain gap between these two settings is significant. Training on low contrast shows robustness when the domain shifts to high contrast at testing. However, the same generalization is not observed for the inverse with models trained on high-contrast videos and tested on low-contrast VFI. Compared to low-to-high ARMS, high-to-low ARMS shows a -1.65 drop in PSNR.

**Low/High ARL**. Tab. 11(d) reports performances over brightness settings. Overall, models from either setting show comparable performance and generalization robustness to the target domain. Minor performance improvements are shown for the high to low task with high luminance training being more effective in cross-domain generalization.

### Qualitative results

Fig. 3 shows interpolated frames from the LAVIB test sets. Frame regions from videos of the LAVIB benchmark interpolated with RIFE, EMA-VFI, and FLAVR are shown in the top three row (a-i). Regions shown vary by size and reconstruction error. LAVIB is challenging for current VFI methods as they cannot fully interpolate all parts of objects (b,i) or fine details (c,f,g). Objects in scenes affected by high motions are shown to be the most prone to interpolation artifacts as seen with the fine details being missed (d) and the high cross-frame relative displacement (e). This also becomes apparent more in high-motion scenes (h) where large distortions in the scene dynamics can be observed. For OOD challenges models also struggle to correctly interpolate the high contrast between objects and backgrounds (k,l,n), distinct patterns (j), and details or objects (m,o). Further qualitative results are provided in SSA5.

## 6 Conclusions and future directions

This paper introduces LAVIB, a large-scale general-purpose dataset and benchmark for VFI. LAVIB consists of 283,484 clips collected from 4K videos at 60fps with metrics computed per video specific to motions, sharpness, contrast, and luminance. With the release of the videos and the OOD challenges splits, LAVIB can be used as a robust benchmark and allow the community to investigate VFI under a diverse range of video settings, captured with different equipment, and across various domains.

LAVIB further encourages exploring new avenues for efficiency improvements in future VFI works.

**Frame-level quantization**. A number of works have explored video inference acceleration through frame quantization for temporal redundancy reduction [1; 62]. Learning to truncate videos by varying quantization precision is important for the real-world applicability of methods in streams. LAVIB provides a diverse set of high-resolution videos with standardized frame rates that can be used both as a benchmark as well as a pre-training dataset.

**Knowledge distillation**. Transferring knowledge about the video structure can enable more efficient models. Several works [25; 35; 48] have distilled representations from teacher models trained on high-resolution videos. A natural extension of the proposed dataset would be its use for training high-resolution teacher models and evaluating low-resolution student models.

**Salient frame sampling**. The selection of informative frames has been another domain of interest for real-time video processing [66; 69; 70]. The standardized framerate of LAVIB can provide a robust benchmark for testing sampling approaches over different granularities.

Based on these adjacent video tasks, LAVIB can be imported and adapted as a general-purpose dataset and benchmark.