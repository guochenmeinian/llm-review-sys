# Zero-Shot Reinforcement Learning

from Low Quality Data

Scott Jeen

University of Cambridge

srj38@cam.ac.uk &Tom Bewley

University of Bristol

tomdbewley@gmail.com &Jonathan M. Cullen

University of Cambridge

jmc99@cam.ac.uk

###### Abstract

Zero-shot reinforcement learning (RL) promises to provide agents that can perform _any_ task in an environment after an offline, reward-free pre-training phase. Methods leveraging successor measures and successor features have shown strong performance in this setting, but require access to large heterogenous datasets for pre-training which cannot be expected for most real problems. Here, we explore how the performance of zero-shot RL methods degrades when trained on small homogeneous datasets, and propose fixes inspired by _conservatism_, a well-established feature of performant single-task offline RL algorithms. We evaluate our proposals across various datasets, domains and tasks, and show that conservative zero-shot RL algorithms outperform their non-conservative counterparts on low quality datasets, and perform no worse on high quality datasets. Somewhat surprisingly, our proposals also outperform baselines that get to see the task during training. Our code is available via the project page [https://enjeeneer.io/projects/zero-shot-rl/](https://enjeeneer.io/projects/zero-shot-rl/).

## 1 Introduction

Today's large pre-trained models generalise impressively to unseen vision  and language  tasks, but not to sequential decision-making problems. Zero-shot reinforcement learning (RL) attempts to correct this, asking, informally: can we pre-train an agent on a dataset of reward-free transitions such that it can perform _any_ downstream task in an environment? Recently, methods leveraging successor features  and successor measures  have emerged as viable zero-shot RL candidates, returning near-optimal policies for many unseen tasks .

These works have assumed access to a large heterogeneous dataset of transitions for pre-training. In theory, such datasets could be curated by highly-exploratory agents during an upfront data collection phase . However, in practice, deploying such agents in real systems can be time-consuming, costly or dangerous. To avoid these downsides, it would be convenient to skip the data collection phase and pre-train on historical datasets. Whilst these are common in the real world, they are usually produced by controllers that are not optimising for data heterogeneity , making them smaller and less diverse than current zero-shot RL methods expect.

Can we still perform zero-shot RL using these datasets? This is the primary question this paper seeks to answer, and one we address in four parts. First, we investigate the performance of existing methods when trained on such datasets, finding their performance suffers because of out-of-distribution state-action value overestimation, a well-observed phenomenon in single-task offline RL. Second, we develop ideas from _conservatism_ in single-task offline RL for use in the zero-shot RL setting, introducing a straightforward regularizer of OOD _values_ or _measures_ that can be used by any zero-shot RL algorithm (Figure 1). Third, we conduct experiments across varied domains, tasks and datasets, showing our _conservative_ zero-shot RL proposals outperform their non-conservative counterparts, and surpass the performance of methods that get to see the task in advance. Finally, we establishthat our proposals do not hinder performance on large heterogeneous datasets, meaning adopting them presents little downside. We believe the ideas explored in this paper represent a step toward real-world deployment of zero-shot RL methods.

## 2 Preliminaries

Markov decision processesA _reward-free_ Markov Decision Process (MDP) is defined by \(=\{,,,\}\) where \(\) is the state space, \(\) is the action space, \(:()\) is the transition function, where \((X)\) denotes the set of possible distributions over \(X\), and \([0,1)\) is the discount factor . Given \((s_{0},a_{0})\) and a policy \(:()\), we denote \((|s_{0},a_{0},)\) and \([|s_{0},a_{0},]\) the probabilities and expectations under state-action sequences \((s_{t},a_{t})_{t 0}\) starting at \((s_{0},a_{0})\) and following policy \(\), with \(s_{t}(|s_{t-1},a_{t-1})\) and \(a_{t}(|s_{t})\). Given a reward function \(r:_{ 0}\), the \(Q\) function of \(\) for \(r\) is \(Q_{r}^{}:=_{t 0}^{t}[r(s_{t+1})|s_{0},a_{0},]\).

Zero-shot RLFor pre-training, the agent has access to a static offline dataset of reward-free transitions \(=\{(s_{i},a_{i},s_{i+1})\}_{i=1}^{||}\) generated by an unknown behaviour policy, and cannot interact with the environment. At test time, a reward function \(r_{}\) specifying a _task_ is revealed and the agent must return a policy for the task without any further planning or learning. Ideally, the policy should maximise the expected discounted return on the task \([_{t 0}^{t}r_{}(s_{t+1})|s_{0},a_{0},]\). The reward function is specified either via a small dataset of reward-labelled states \(_{}=\{(s_{i},r_{}(s_{i}))\}_{i=1}^{k}\) with \(k 10,000\) or as an explicit function \(s r_{}(s)\) (like \(1\) at a goal state and \(0\) elsewhere). Intuitively, the zero-shot RL problem asks: is it possible to train an agent using a pre-collected dataset of transitions from an environment such that, at test time, it can return the optimal policy for any task in that environment without any further planning or learning?

State-of-the-art zero-shot RL methods leverage either successor measures  or successor features , with the former instantiated by _forward backward representations_ and the latter by _universal successor features_. The remainder of this section introduces these ideas.

Successor measuresThe _successor measure_\(M^{}(s_{0},a_{0},)\) over \(\) is the cumulative discounted time spent in each future state \(s_{t+1}\) after starting in state \(s_{0}\), taking action \(a_{0}\), and following policy \(\) thereafter:

\[M^{}(s_{0},a_{0},X):=_{t 0}^{t}(s_{t+1} X|s_{0},a_{0}, )\ \ X. \]

The \(Q\) function of policy \(\) for task \(r\) is the integral of \(r\) with respect to \(M^{}\):

\[Q_{r}^{}(s_{0},a_{0}):=_{s_{+}}r(s_{+})M^{}(s_{0},a_{0 },s_{+}). \]

The forward-backward frameworkFB representations  approximate the successor measures of near-optimal policies for any task. Let \(\) be an arbitrary state distribution, and \(^{d}\) be a representation

Figure 1: _Conservative zero-shot RL._._ (_Left_) Zero-shot RL methods must train on a dataset collected by a behaviour policy optimising against task \(z_{}\), yet generalise to new tasks \(z_{}\). Both tasks have associated optimal value functions \(Q_{s_{}}^{*}\) and \(Q_{s_{}}^{*}\) for a given marginal state. (_Middle_) Existing methods, in this case forward-backward representations (FB), overestimate the value of actions not in the dataset for all tasks. (_Right_) Value-conservative forward-backward representations (VC-FB) suppress the value of actions not in the dataset for all tasks. Black dots (\(\)) represent state-action samples present in the dataset.

space. FB representations are composed of a _forward_ model \(F:^{d}^{d}\), a _backward_ model \(B:^{d}\), and set of polices \((_{z})_{z^{d}}\). They are trained such that

\[M^{_{z}}(s_{0},a_{0},X)_{X}F(s_{0},a_{0},z)^{}B(s)( s)\ \ \ \ s_{0},a_{0},X ,z^{d}, \]

and

\[_{z}(s)*{arg\,max}_{a}F(s,a,z)^{}z\ \ \ \ (s,a),z^{d}. \]

Intuitively, Equation 3 says that the approximated successor measure under \(_{z}\) from \((s_{0},a_{0})\) to \(s\) is high if their respective forward and backward embeddings are similar i.e. have large dot product. By comparing Equation 2 and Equation 3, we see that an FB representation can be used to approximate the \(Q\) function of \(_{z}\) with respect to any reward function \(r\) as:

\[Q_{r}^{_{z}}(s_{0},a_{0}) _{s}r(s)F(s_{0},a_{0},z)^{}B(s)( s) \] \[=F(s_{0},a_{0},z)^{}_{s}[r(s)B(s)\ ].\]

Training of \(F\) and \(B\) is done with TD learning [71; 78] using transition data sampled from \(\):

\[_{}=_{(s_{t},a_{t},s_{t+1},s_{+}) ,z}[(F(s_{t},a_{t},z)^{}B(s_{+})- (s_{t+1},_{z}(s_{t+1}),z)^{}(s_{+}))^{2}\\ -2F(s_{t},a_{t},z)^{}B(s_{t+1})], \]

where \(s_{+}\) is sampled independently of \((s_{t},a_{t},s_{t+1})\), \(\) and \(\) are lagging target networks, and \(\) is a task sampling distribution. The policy is trained in an actor-critic formulation . See  for a full derivation of the TD update, and our Appendix B.1 for practical implementation details including the specific choice of task sampling distribution \(\).

By relating Equations 4 and 5, we find \(z=_{s}[r(s)B(s)]\) for some reward function \(r\). At test time, we can use this property to perform zero-shot RL. Using \(_{}\), we estimate the task as \(z_{}_{s_{}}[r_{}(s)B(s)]\) and pass it as an argument to \(_{z}\). If \(z_{}\) lies within the task sampling distribution \(\) used during pre-training, then \(_{z}(s)*{arg\,max}_{a}Q_{r_{}}^{_{z}}(s,a)\), and hence this policy is approximately optimal for \(r_{}\).

**(Universal) successor features** _Successor features_ assume access to a basic feature map \(:^{d}\) that embeds states into a representation space, and are defined as the expected discounted sum of future features \(^{}(s_{0},a_{0}):=[_{t 0}^{t}(s_{t+1})|s_{0},a_{0},]\). They are made _universal_ by conditioning their predictions on a family of policies \(_{z}\)

\[(s_{0},a_{0},z)=[_{t 0}^{t}(s_{t+1})|s_{0 },a_{0},_{z}]\ \ \ s_{0},a_{0},z^{d}, \]

with

\[_{z}(s)*{arg\,max}_{a}(s,a,z)^{}z,\ \ (s_{0},a_{0}),z^{d}. \]

Like FB, USFs are trained using TD learning on

\[_{}=_{(s_{t},a_{t},s_{t+1}),z }[((s_{t},a_{t},z)^{}z-(s_{t+1})^{}z-(s_{t+1},_{z}(s_{t+1}),z)^{}z)^{2}], \]

where \(\) is a lagging target network, and \(\) is the same \(z\) sampling distribution used for FB. We refer the reader to  for a derivation of the TD update and full learning procedure. Test time policy inference is performed similarly to FB. Using \(_{}\), the task is inferred by performing a linear regression of \(r_{}\) onto the features: \(z_{}:=*{arg\,min}_{z}_{s_{ }}[(r_{}(s)-(s)^{}z)^{2}]\) before it is passed as an argument to the policy.

## 3 Zero-Shot RL from Low Quality Data

In this section we introduce methods for improving the performance of zero-shot RL methods on low quality datasets. In Section 3.1, we explore the failure mode of existing methods on such datasets. Then, in Section 3.2, we propose straightforward amendments to these methods that address the failure mode. Finally, in Section 3.3, we illustrate the usefulness of our proposals with a controlled example. We develop our methods within the FB framework because of its superior empirical performance , but our proposals are also compatible with USF. We push their derivation to Appendix D for brevity.

### Failure Mode of Existing Methods

To investigate the failure mode of existing methods we examine the FB loss (Equation \(}\)) more closely. The TD target includes an action produced by the current policy \(a_{t+1}_{z}(s_{t+1})\). Equation 4 shows this is the policy's current best estimate of the \(Q\)-maximising action in state \(s\) for task \(z\). For finite datasets, this maximisation does not constrain the policy to actions observed in the dataset, and so it can become biased towards out-of-distribution (OOD) actions thought to be of high value. In such instances \(F\) and \(B\) are updated towards targets for which the dataset provides no support. This _distribution shift_ is a well-observed phenomenon in single-task offline RL [42; 46; 44], and is exacerbated by small, low-diversity datasets as we explore in Figure 2.

### Mitigating the Distribution Shift

In the single-task setting, the distribution shift is addressed by applying constraints to either the policy, value function or model (see Section \(}\) for a summary of past work). Here we re-purpose single-task value function and model regularisation for use in the zero-shot RL setting. To avoid further complicating zero-shot RL methods, we only consider regularisation techniques that do not introduce new parametric functions. We discuss the implications of this decision in Section 5.

Conservative \(Q\)-learning (CQL) [42; 44] regularises the \(Q\) function by querying OOD state-action pairs and suppressing their value. This is achieved by adding \(\) to the usual \(Q\) loss function

\[_{}=_{s,a(a|s )}[Q(s,a)]-_{(s,a)}[Q(s,a)]-()+_{}, \]

where \(\) is a scaling parameter, \((a|s)\) is a policy distribution selected to find the maximum value of the current \(Q\) function iterate, \(()\) is the entropy of \(\) used for regularisation, and \(_{}\) is the normal TD loss on \(Q\). Equation 10 has the dual effect of minimising the peaks in \(Q\) under \(\) whilst maximising \(Q\) for state-action pairs in the dataset.

We can replicate a similar form of regularisation in the FB framework, substituting \(F(s,a,z)^{}z\) for \(Q\) in Equation 10 and adding the normal FB loss (Equation 6)

\[_{}=(_{s,a(a| s),z}[F(s,a,z)^{}z]-_{(s,a),z }[F(s,a,z)^{}z]-())+_{}. \]

The key difference between Equations 10 and 11 is that the former suppresses the value of OOD actions for one task, whereas the latter does so for all task vectors drawn from \(\). We call models learnt with this loss _value-conservative forward-backward representations_ (VC-FB).

Because FB derives \(Q\) functions from successor measures (Equation 5), and because (by assumption) rewards are non-negative, suppressing the predicted measures for OOD actions provides an alternative route to suppressing their \(Q\) values. As we did with VC-FB, we can substitute FB's successor measure approximation \(F(s,a,z)^{}B(s_{+})\) into Equation 10, which yields:

Figure 2: **FB value overestimation with respect to dataset size \(n\) and quality.** Log \(Q\) values and IQM of rollout performance on all Maze tasks for datasets Rnd and Random. \(Q\) values predicted during training increase as both the size and “quality” of the dataset decrease. This contradicts the low return of all resultant policies (note: a return of 1000 is the maximum achievable for this task). Informally, we say the Rnd dataset is “high” quality, and the Random dataset is “low” quality–see Appendix A.2 for more details.

\[_{}=(_{s,a (a|s),z Z,s_{+}}[F(s,a,z)^{}B(s_{+})]\\ -_{(s,a),z Z,s_{+}}[F( s,a,z)^{}B(s_{+})]-())+_{}. \]

Equation 12 has the effect of suppressing the expected visitation count to goal state \(s_{+}\) when taking an OOD action for all task vectors drawn from \(\), which says, informally, if we don't know where OOD actions take us in the MDP, we assume they have low probability of taking us to any future states for all tasks. This is analogous to works that regularise model predictions in the single-task offline RL setting . As such, we call this variant a _measure-conservative forward-backward representation_ (MC-FB). Since it is not obvious _a priori_ whether the VC-FB or MC-FB form of conservatism would be more effective in practice, we evaluate both in Section 4.

Implementing these proposals requires two new model components: 1) a conservative penalty scaling factor \(\) and 2) a way of obtaining policy distribution \((a|s)\) that maximises the current value or measure iterate. For 1), we observe fixed values of \(\) leading to fragile performance, so dynamically tune it at each learning-see Appendix B.1.4. For 2), the choice of maximum entropy regularisation following 's \(()\) allows \(\) to be approximated conveniently with a log-sum exponential across \(Q\) values derived from the current policy distribution and a uniform distribution. That this is true is not obvious, so we refer the reader to the detail and derivations in Section 3.2, Appendix A, and Appendix E of , as well as our adjustments to 's theory in Appendix B.1.3. Code snippets demonstrating the required changes to a vanilla FB implementation are provided in Appendix G. We emphasise these additions represent only a small increase in the number of lines required to implement existing methods.

### A Didactic Example

To understand situations in which a conservative zero-shot RL methods may be useful, we introduce a modified version of Maze from the ExORL benchmark . Episodes begin with a point-mass initialised in the upper left of the maze (\(\)), and the agent is tasked with selecting \(x\) and \(y\) tilt directions such that the mass is moved towards one of two goal locations (\(\) and \(\)). The action space is two-dimensional and bounded in \([-1,1]\). We take the Rnd dataset and remove all "left" actions such that \(a_{x}\) and \(a_{y}[-1,1]\), creating a dataset that has the necessary information for solving the tasks, but is inexhaustive (Figure 3 (a)). We train FB and VC-FB on this dataset and plot the highest-reward trajectories-Figure 3 (b) and (c). FB overestimates the value of OOD actions and cannot complete either task. Conversely, VC-FB synthesises the requisite information from the dataset and completes both tasks.

## 4 Experiments

In this section we perform an empirical study to evaluate our proposals. We seek answers to four questions: **(Q1)** Can our proposals from Section 3 improve FB performance on small and/or low-quality exploratory datasets? **(Q2)** How does the performance of VC-FB and MC-FB vary with respect to task type and dataset diversity? **(Q3)** Do we sacrifice performance on full datasets for

Figure 3: **Ignoring out-of-distribution actions. The agents are tasked with learning separate policies for reaching \(\) and \(\). (a) Rnd dataset with all “left” actions removed; quivers represent the mean action direction in each state bin. (b) Best FB rollout after 1 million learning steps. (c) Best VC-FB performance after 1 million learning steps. FB overestimates the value of OOD actions and cannot complete either task; VC-FB synthesises the requisite information from the dataset and completes both tasks.**

performance on small and/or low-quality datasets? **(Q4)** If our pre-training dataset only covers behaviour related to one downstream task (i.e. the dataset distribution is narrow and not exploratory), can our proposals from Section 3 improve FB performance on that task?

### Setup

DomainsWe respond to **Q1-Q3** using the ExORL benchmark . ExORL provides datasets collected by unsupervised exploratory algorithms on the DeepMind Control Suite . We select three of the same domains as : Walker, Quadruped and Maze, but substitute Jaco for Cheetah. This provides two locomotion domains and two goal-reaching domains. Within each domain, we evaluate on all tasks provided by the DeepMind Control Suite for a total of 17 tasks across four domains. Full details are provided in Appendix A.1. We respond to **Q4** using the D4RL benchmark . We select the two MuJoCo  environments from the Open AI gym  that closest resemble those from ExORL: Walker2D and HalfCheetah.

DatasetsFor **Q1-Q3** we pre-train on three datasets of varying quality from ExORL. There is no unambiguous metric for quantifying dataset quality, so we use the reported performance of offline TD3 on Maze for each dataset as a proxy1. We choose datasets collected via Random Network Distillation (Rnd) , Diversity is All You Need (Diayn) , and Random policies, where agents trained on Rnd are the most performant, on Diayn are median performers, and on Random are the least performant. As well as selecting for quality, we also select for size by uniformly subsampling 100,000 transitions from each dataset. For **Q4** we choose the "medium", "medium-replay", and "medium-expert" datasets from D4RL, each providing different fractions of random, medium and expert task-directed trajectories. More details on the datasets are provided in Appendix A.2.

### Baselines

We compare our proposals to baselines from three categories: 1) zero-shot RL methods, 2) goal-conditioned RL (GCRL) methods, and 3) single-task offline RL methods. From category 1), we use the state-of-the-art successor measure based method, FB, and the state-of-the-art successor feature based method, SF with features from Laplacian eigenfunctions (SF-LAP) . From category 2), we use goal-conditioned IQL (GC-IQL) , a state-of-the-art GCRL method that, like our proposals, regularises the value function at OOD state-actions. We condition GC-IQL on the goal state on Maze and Jaco, and on the state in \(_{}\) with highest reward on Walker and Quadruped in lieu of a

Figure 4: **Aggregate zero-shot performance on ExORL.**_(Left)_ IQM of task scores across datasets and domains, normalised against the performance of CQL, our baseline. _(Right)_ Performance profiles showing the distribution of scores across all tasks and domains. Both conservative FB variants stochastically dominate vanilla FB–see  for performance profile exposition. The black dashed line represents the IQM of CQL performance across all datasets, domains, tasks and seeds.

well-defined goal state. From category 3), we use CQL and offline TD3 trained on the same datasets relabelled with task rewards. CQL approximates what an algorithm with similar mechanics can achieve when optimising for one task in a domain rather than all tasks. Offline TD3 exhibits the best aggregate single-task performance on the ExORL benchmark, so it should be indicative of the maximum performance we could expect to extract from a dataset. Full implementation details for all algorithms are provided in Appendix B. The full evaluation protocol is described in Appendix A.5. Appendix A.6 provides a breakdown of the computational resources used in this work.

### Results

**Q1** We report the aggregate performance of our baselines and proposals on ExORL in Figure 4. Both MC-FB and VC-FB outperform the zero-shot RL and GCRL baselines, achieving **150%** and **137%** of FB's IQM performance respectively. The performance gap between FB and SF-LAP is consistent with the results in . MC-FB and VC-FB outperform our single-task baseline in expectation, reaching 111% and 120% of CQL's IQM performance respectively _despite not having access to task-specific reward labels and needing to fit policies for all tasks_. This is a surprising result, and to the best of our knowledge, the first time a multi-task offline agent has been shown to outperform a single-task analogue. CQL outperforms offline TD3 in aggregate, so we drop offline TD3 from the core analysis, but report its full results in Appendix C alongside all other methods. We note FB achieves 80% of single-task offline TD3, which roughly aligns with the 85% performance on the full datasets reported by .

**Q2** We decompose the methods' performance with respect to domain and dataset diversity in Figure 5. The largest gap in performance between the conservative FB variants and FB is on Rnd. VC-FB and MC-FB reach 2.5\(\) and 1.8\(\) of FB performance respectively, and outperform CQL on three of the four domains. On Diayn, the conservative variants outperform all methods and reach 1.3\(\) CQL's score. On the Random dataset, all methods perform similarly poorly, except for CQL on Jaco, which outperforms all methods. However, in general, these results suggest the Random dataset is not informative enough to extract valuable policies-discussed further in response to Q3. There appears to be little correlation between the type of domain (Appendix A.1) and the score achieved by any method. GC-IQL performs particularly well on the goal-reaching domains as expected, but worse than all zero-shot methods on the locomotion tasks, irrespective of whether they are conservative or not. This is presumably because the goal-state used to condition the policy (_i.e._ the state with highest reward in \(_{}\)) is a poor proxy for the true, dense reward function.

Figure 5: **Performance by dataset/domain on ExORL.** IQM scores across tasks/seeds with 95% conf. intervals.

Figure 6: **Performance by dataset size.** Aggregate IQM scores across all domains and tasks as Rnd size is varied. The performance delta between vanilla FB and the conservative variants increases as dataset size decreases.

**Q3**  We report the aggregated performance of all FB methods across domains when trained on the full datasets in Table 1 (a full breakdown of results in provided in Appendix C). Both conservative FB variants slightly exceed the performance of vanilla FB in expectation. The largest relative performance improvement is on the Random dataset-MC-FB performance is 20% higher than FB, compared to 5% higher on Diayn and 2% higher on Rnd. This corroborates the hypothesis that Random-100k was not informative enough to extract valuable policies.

Table 1 and Figure 4 suggest the performance gap between the conservative FB variants and vanilla FB changes as dataset size is varied. We further explore this effect in Figure 6 where we scale the Rnd dataset size from \(10^{5}\) through \(10^{7}\) and plot aggregate IQM performance of FB, VC-FB and MC-FB across all domains. We find that the performance gap decreases as dataset size increases. This result is to be expected: a larger dataset size for a fixed exploration algorithm means \(a_{t+1}_{z}(s_{t+1})\) in the FB TD update (Equation 6) is more likely to be in the dataset, the policy is less likely to become biased toward OOD actions, and conservatism is less needed.

**Q4**  We report the aggregate performance of all zero-shot RL methods and CQL on our D4RL domains in Figure 7. FB fails all domain-dataset tasks, and reaches only 10% of CQL's aggregate performance. MC-FB and VC-FB improve on FB's considerably (by 5.6 \(\) and 6.8 \(\) respectively) but under-perform CQL. SF-LAP outperforms FB, but under-performs VC-FB, MC-FB and CQL.

## 5 Discussion and Limitations

**Performance discrepancy between conservative variants**  Why does VC-FB outperform MC-FB on both ExORL and D4RL? To understand, we inspect the regularising effect of both models more closely. VC-FB regularises OOD actions on \(F(s,a,z)^{}z\), with \(s\), and \(z\), whilst MC-FB regularises OOD actions on \(F(s,a,z)^{}B(s_{+})\), with \((s,s_{+})\) and \(z\). Note the trailing \(z\) in VC-FB is replaced with \(B(s_{+})\) in MC-FB which ties its updates to \(\) further. We hypothesised that as \(||\) reduces, \(B(s_{+})\) provides poorer task coverage than \(z\), hence the comparable performance on full datasets and divergent performance on 100k datasets.

To test this, we evaluate a third conservative variant called _directed_ (\(D\))VC-FB which replaces all \(z\) in VC-FB with \(B(s_{+})\) such that OOD actions are regularised on \(F(s,a,B(s_{+}))^{}B(s_{+})\) with \((s,s_{+})\). This ties conservative updates entirely to \(\), and according to our above hypothesis, \(D\)VC-FB should perform worse than VC-FB and MC-FB on the 100k ExORL datasets. See Appendix B.1.6 for implementation details. We evaluate this variant on all 100k ExORL datasets, domains and tasks and compare with FB, VC-FB and MC-FB in Table 2. See Appendix C for a full breakdown.

We find the aggregate relative performance of each method is as expected i.e. \(D\)VC-FB \(<\) MC-FB \(<\) VC-FB. As a consequence we conclude that VC-FB should be preferred for small datasets with

  
**Dataset** & **Domain** & **Task** & FB & **VC-FB** & **MC-FB** \\  Rnd & all & all & \(389\) & \(390\) & \(396\) \\ Diayn & all & all & \(269\) & \(280\) & \(283\) \\ Random & all & all & \(111\) & \(131\) & \(133\) \\  All & all & all & \(256\) & \(267\) & **271** \\   

Table 1: **Aggregate performance on full ExORL datasets.** IQM scores aggregated over domains and tasks for all datasets, averaged across three seeds. Both VC-FB and MC-FB maintain the performance of FB; the largest relative performance improvement is on Random.

Figure 7: **Aggregate zero-shot performance on D4RL.** Aggregate IQM scores across all domains and datasets, normalised against the performance of CQL.

no prior knowledge of the dataset or test tasks. Of course, for a specific domain-dataset pair, \(B(s_{+})\) with \(s_{+}\) may happen to cover the tasks well, and MC-FB may outperform VC-FB. We suspect this was the case for all datasets on the Jaco domain for example. Establishing whether this will be true _a priori_ requires either relaxing the restrictions imposed by the zero-shot RL setting, or better understanding of the distribution of tasks in \(z\)-space and their relationship to pre-training datasets. The latter is important future work.

Avoiding new parametric functionsState-of-the-art zero-shot RL methods are complex, and we wanted to avoid further complicating them with new parametric functions. This limited our solution-space to CQL-style regularisation techniques, but had we relaxed this constraint, other options become available. Methods like AWAC , IQL , and \(X\)-QL  all require an estimate of the state-value function which is not immediately accessible in the FB or USF frameworks. In theory, we could learn an action-independent USF of the form \(V(s,z)=[_{t 0}^{t}(s_{t+1})|s_{0},_{z}]\ \ s_{0},z^{d}\) concurrently to \(F\) and \(B\) (or \(\) for USFs). If learnt with expectile regression, this function could be used to implement IQL and \(\)-QL style regularisation; without expectile regression it could be used to compute the advantage weighting required for AWAC. It's possible that implementing these methods could improve downstream performance and reduce computational overhead at the cost of increased training complexity. We leave this worthwhile investigation for future work. We provide detail of negative results related to downstream finetuning of FB models in Appendix E to help inform future research.

D4RL PerformanceUnlike the ExORL results, VC-FB and MC-FB do not outperform CQL on the D4RL benchmark. We believe these narrower data distributions require a more careful selection of the conservative penalty scaling factor \(\). We explore this further in Appendix F, and note this is corroborated by findings in the original CQL paper . Methods described above, like IQL, have been shown to be more robust than CQL partly because they bypass \(\) tuning. We expect that exploring the integration of these methods may improve D4RL performance.

## 6 Related Work

Zero-shot RLZero-shot RL methods build upon successor representations , universal value function approximators , successor features  and successor measures . The state-of-the-art methods instantiate these ideas as either universal successor features (USFs)  or forward-backward (FB) representations [82; 83], with recent work showing the latter can be used to perform a range of imitation learning techniques efficiently . A representation learning method is required to learn the features for USFs, with past works using inverse curiosity modules , diversity methods [49; 29], Laplacian eigenfunctions , or contrastive learning . No works have yet explored the issues arising when training these methods on low quality offline datasets, and only one has investigated applying these ideas to real-world problems .

Goal-conditioned RL methods train policies to reach any goal state from any other state, and so can be used to perform zero-shot RL in goal-reaching environments [60; 54; 93; 19; 85]. However, they have no principled mechanism for conditioning policies on "dense" reward functions (as such tasks are not solved by simply reaching a particular state), and so are not full zero-shot RL methods. A concurrent line of work trains policies using sequence models conditioned on reward-labelled histories [12; 33; 45; 68; 99; 11; 24; 76; 91; 90], but, unlike zero-shot RL methods, these works do not have a robust mechanism for generalising to different reward functions as test-time.

Offline RLOffline RL algorithms require regularisation of policies, value functions, models, or a combination to manage the offline-to-online distribution shift . Past works regularise policies with explicit constraints [88; 20; 23; 22; 27; 64; 43; 86; 94], via important sampling [66; 79; 50; 26], by leveraging uncertainty in predictions [89; 98; 4; 36], or by minimising OOD action queries [84; 14; 40], a form of imitation learning [72; 73]. Other works constrain value function approximation

  
**Dataset** & **Domain** & **Task** & FB & \(\)**VC-FB** & MC-FB & **VC-FB** \\  All (100k) & all & all & \(99\) & \(108\) & \(136\) & \(148\) \\   

Table 2: **Aggregated performance of conservative variants employing differing \(z\) sampling procedures on ExORL.**_D_VC-FB derives all \(z\)s from the backward model; VC-FB derives all \(z\)s from \(\); and MC-FB combines both. Performance correlates with the degree to which \(z\).

so OOD action values are not overestimated [44; 42; 52; 53; 51; 92]. Offline model-based RL methods use the model to identify OOD states and penalise predicted rollouts passing through them [97; 37; 96; 2; 55; 67; 69]. All of these works have focused on regularising a finite number of policies; in contrast we extend this line of work to the zero-shot RL setting which is concerned with learning an infinite family of policies.

## 7 Conclusion

In this paper, we explored training agents to perform zero-shot reinforcement learning (RL) from low quality data. We established that the existing methods suffer in this regime because they overestimate the value of out-of-distribution state-action values, a well-observed pheneomena in single-task offline RL. As a resolution, we proposed a family of _conservative_ zero-shot RL algorithms that regularise value functions or dynamics predictions on out-of-distribution state-action pairs. In experiments across various domains, tasks and datasets, we showed our proposals outperform their non-conservative counterparts in aggregate and sometimes surpass our task-specific baseline despite lacking access to reward labels _a priori_. In addition to improving performance when trained on sub-optimal datasets, we showed that performance on large, diverse datasets does not suffer as a consequence of our design decisions. Our proposals represent a step towards the use of zero-shot RL methods in the real world.