# Adapting to Continuous Covariate Shift via

Online Density Ratio Estimation

 Yu-Jie Zhang\({}^{1}\), Zhen-Yu Zhang\({}^{2}\), Peng Zhao\({}^{3}\), Masashi Sugiyama\({}^{2,1}\)

\({}^{1}\) The University of Tokyo, Chiba, Japan

\({}^{2}\) RIKEN AIP, Tokyo, Japan

\({}^{3}\) National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

Correspondence: Yu-Jie Zhang <yujie.zhang@ms.k.u-tokyo.ac.jp> and Peng Zhao <zhaop@lamda.nju.edu.cn>

###### Abstract

Dealing with distribution shifts is one of the central challenges for modern machine learning. One fundamental situation is the _covariate shift_, where the input distributions of data change from the training to testing stages while the input-conditional output distribution remains unchanged. In this paper, we initiate the study of a more challenging scenario -- _continuous_ covariate shift -- in which the test data appear sequentially, and their distributions can shift continuously. Our goal is to adaptively train the predictor such that its prediction risk accumulated over time can be minimized. Starting with the importance-weighted learning, we theoretically show the method works effectively if the time-varying density ratios of test and train inputs can be accurately estimated. However, existing density ratio estimation methods would fail due to data scarcity at each time step. To this end, we propose an online density ratio estimation method that can appropriately reuse historical information. Our method is proven to perform well by enjoying a dynamic regret bound, which finally leads to an excess risk guarantee for the predictor. Empirical results also validate the effectiveness.

## 1 Introduction

How to deal with distribution shifts is one of the central challenges for modern machine learning [1; 2], which is also the key requirement of robust artificial intelligence in open and dynamic environments [3; 4]. _Covariate shift_ is a representative problem setup and has attracted much attention [5; 6], where the input density \(()\) varies from training data to test data but their input-conditional output \((y)\) remains unchanged. On the one hand, covariate shift serves as a foundational setup for theoretically understanding general distribution shifts , and on the other hand, it can encompass many real-world tasks such as brain-computer interface , speaker identification  and audio processing .

Existing works of coping with covariate shift mainly focused on the "one-step" adaptation, where the learner aims to train a model well-performed on a _fixed_ testing distribution. To mitigate the distribution discrepancy, a common and classic solution is the importance-weighting framework , where one assigns an appropriate importance weight to each labeled training sample and then conducts weighted empirical risk minimization. The importance weight, also known as the density ratio of test and training inputs, usually needs to be estimated via a reasonable amount of unlabeled data sampled from the testing distribution [11; 12]. However, the one-step adaptation can be insufficient in many real-world covariate shift scenarios, especially when data are accumulated in an online fashion such that testing environments _continuously shift_ and only _a few unlabeled samples_ are observed at each time. For instance, in the speaker identification task, the speech features vary over time due to sessiondependent variation, the recording environment change, and physical conditions and emotions of the speakers . Therefore, it is necessary to perform a prompt adaptation to the changes.

Motivated by such demands, we initiate the study of _continuous covariate shift_, where the learner's goal is to adaptively update the model to minimize the risk accumulated over time (see the formal definition in Section 3). To achieve this goal, we propose the Accous approach (Adapt to Continuous Covariate shift with Unlabeled Stream) equipped with sound theoretical guarantees. Our approach is based on the classic importance-weighting framework yet requires innovations to make it applicable to the continuous shift scenario. Indeed, we theoretically identify that the importance-weighting works effectively if the cumulative estimation error of the time-varying density ratios of train and test inputs can be reduced. However, at each time step, a direct application of one-step density ratio estimation would lead to a high variance due to data scarcity; whereas reusing all previous data can be highly biased when testing distributions change dramatically. Thus, it is crucial to design an accurate estimation of _time-varying_ test-train density ratios by appropriately reusing historical information.

To combat the difficulty, we propose a generic reduction of the time-varying density ratio estimation problem to the online convex optimization : one can immediately obtain high-quality time-varying density ratios by a suitable online process to optimize its _dynamic regret_ over a certain sequence of loss functions. Our reduction is based on the Bregman divergence density ratio matching framework  and applicable to various existing density ratio estimation models with specific configurations of the divergence functions. To minimize the dynamic regret, our key algorithmic ingredient of this online optimization is the _online ensemble_ structure [15; 16], where a group of base-learners is maintained to perform density ratio estimation with different lengths of historical data, and a meta-algorithm is employed to combine the predictions. As such, we can properly reuse historical information without knowing the cumulative intensity of the covariate shift. We further instantiate the reduction framework with the logistic regression model . Letting \(V_{T}=_{t=2}^{T}_{t}()-_{t-1}( )_{1}\), we prove an \(}(T^{1/3}V_{T}^{2/3})\) dynamic regret bound for the density ratio estimator, which finally leads to an \(}(T^{-1/3}V_{T}^{1/3})\) averaged excess risk of the predictor trained by importance-weighted learning. The rate can hardly be improved even if one receives labels of the testing stream after prediction (see more elaborations below Theorem 3 and Appendix D.6). Finally, we conduct experiments to evaluate our approach, and the empirical results validate the theoretical findings.

**Technical Contributions.** We note that online ensemble was also employed by  to handle continuous _label shift_, another typical distribution shift assuming the change happens on the class prior \(_{t}(y)\). However, their method crucially relies on the _unbiasedness_ of weights in the risk estimator, whereas density ratio estimators for covariate shift adaptation cannot satisfy the condition in general. Instead of pursuing unbiasedness, we delicately design an online optimization process to learn the density ratio estimator, which is versatile to be implemented with various models. Our methodology is very general and might be of broader use, such as relaxing the unbiasedness requirement in the continuous label shift. Besides, the \(}(T^{1/3}V_{T}^{2/3})\) dynamic regret bound for the logistic regression density ratio estimator is obtained non-trivially. Our bound essentially holds for online learning with exp-concave functions under noisy feedback. The only one achieving this  holds in expectation and requires _complicated_ analysis on the Karush-Kuhn-Tucker (KKT) condition  of comparators. By contrast, our bound holds in high probability, and the analysis is greatly _simplified_ by virtue of exploiting the structure of comparators in our problem that they are essentially the minimizers of expected functions (hence without analyzing the KKT condition).

## 2 Preliminaries

This section introduces the preliminaries and related work on the continuous covariate shift adaptation. We discuss the related work on non-stationary online learning in Appendix B

**One-Step Covariate Shift Adaptation.** Let \(_{0}(,y)\) and \(_{1}(,y)\) be the training and test distributions. The "one-step" adaptation problem studies how to minimize the testing risk \(R_{1}()=_{(,y)_{1}(,y)} [(^{},y)]\) by the model trained with a labeled dataset \(S_{0}=\{_{i},y_{i}\}_{i=1}^{N_{0}}\) sampled from \(_{0}(,y)\) and unlabeled dataset \(S_{1}=\{_{i}\}_{i=1}^{N_{1}}\) sampled from \(_{1}()\). We call this one-step adaptation since the test distribution is _fixed_ and a reasonable number of unlabeled data \(S_{1}\) is available.

**Importance-Weighted ERM.** A classic solution for the one-step covariate shift adaptation is the importance-weighted empirical risk minimization (IWERM), which mitigates the distribution shift by minimizing the weighted empirical risk \(_{1}()=_{(,y) S_{0}}[r_{1}^{*}( )(^{},y)]\), where \(r_{1}^{*}()=_{1}()/_{0}()\) is the importance weight. The risk \(_{1}()\) is unbiased to \(R_{1}()\) and thus the learned model is consistent over the test distribution . The importance weighted learning was studied from the lens of variance-bias trade off [5; 21], cross validation , model misspecification , and deep neural network implementation . All those are conducted under the one-step adaptation scenario.

**Density Ratio Estimation.** The importance weighted estimation, also known as the density ratio estimation (DRE) , aims to estimate the density ratio \(r_{1}^{*}()\) using datasets input of \(S_{0}\) and \(S_{1}\). Various methods were proposed with different statistical models [14; 17; 25; 26; 27; 28; 29]. All those methods focused on the one-step adaptation, and it is challenging to extend them to the continuous shift due to the limited unlabeled data at each time step. The work  studied how to update the density ratio estimator with streaming data, but the ground-truth density ratio is assumed to be fixed. Recently, the time-varying density ratio was investigated in . Their problem setup fundamentally differs from ours as they assume sufficient online data at each iteration, while our challenge is dealing with limited data per iteration. The work  proposed to learn the time-varying density ratios using all past data at each time. Although learning theory insights are provided in the paper, it is still unclear how the learned density ratios balance sample complexity with environmental shift intensity. In contrast, our density ratio estimator updates in an online fashion with dynamic regret guarantees.

**Continuous Distribution Shift.** For broader continuous distribution shift problems, the study  focused on the label shift case and provided the first feasible solution. Then, the work  introduced modern non-stationary online learning techniques to the problem, developing the first method with dynamic regret guarantees. As shown in Remark 2, our solution refines the previous methods [33; 18] by decoupling density ratio estimation from predictor training, paving the way for a theoretically-grounded method for continuous covariate shift. Similarly, the subsequent work  developed a method for continuous label shift, which allows for training the predictor with various models by separately estimating the label probability through an online regression oracle. Another research  studied the _change detection_ for continuous covariate shift. However, they only focused on identifying differences between the current and initial offline distributions, and it is still unclear how to update the model adaptively with the online data. Moreover, the method  can only detect at a given time granularity, while ours can perform the model update at each round.

## 3 Adapting to Continuous Covariate Shift

In this section, we formulate the problem setup of continuous covariate shift and then introduce our approach based on the IWERM framework and online density ratio estimation.

### Problem Setup

There are two stages in continuous covariate shift. The first one is the _offline initialization stage_, where the learner can collect a reasonable number of label data \(S_{0}=\{_{n},y_{n}\}_{n=1}^{N_{0}}\) from the initial distribution \(_{0}\). Then, we come to the _online adaptation stage_, where the unlabeled testing data arrive sequentially, and the underlying distributions can continuously shift. Consider a \(T\)-round online adaptation. At each round \(t[T]\{1,,T\}\), the learner will receive an unlabeled dataset \(S_{t}=\{_{n}\}_{n=1}^{N_{t}}\) sampled from the underlying distribution \(_{t}\). Without loss of generality, we consider \(N_{t}=1\). We have the following continuous covariate shift condition.

**Assumption 1** (Continuous Covariate Shift).: For all \(\) in the feature space, \(y\) in the label space, and any \(t[T]\), we have

\[_{t}(y\,|\,)=_{0}(y\,|\,x)\;\;\;\;r_ {t}^{*}()=_{t}()/_{0}()  B<.\]

We note that there are emerging discussions on the necessity of covariate shift adaptation [7; 23]. When there are infinite number of training samples, a well-specified large and over-parametrized model can be trained to approximate \(_{0}(y\,|\,)\) and there is no need to perform covariate shift adaptation. However, in the finite-sample cases, one would prefer to train a model with constraint complexity to ensure its generalization ability, which leads to model misspecification and covariate shift adaptation is indeed necessary. In this paper, we study the continuous covariate shift under a misspecified hypothesis space \(\), which generalizes the standard one-step covariate shift problem (see  and reference therein) to the continuous shift case. Our goal is to train a sequence of model \(\{_{t}\}_{t=1}^{T}\) that are comparable with the best model \(_{t}^{*}_{}R_{t}()\) in the hypothesis space at each time. Consequently, we take the following average excess risk as the performance measure:

\[_{T}(\{}_{t}\}_{t=1}^{T}) (_{t=1}^{T}R_{t}(}_{t})-_{t=1}^{T}R_{t}( _{t}^{*})).\] (1)

We end this part by listing several common notations used throughout the paper. We denote by \(R=_{}_{2}\) the maximum norm of the input and by \(D=_{_{1},_{2}}_{1}- _{2}_{2}\) the diameter of the hypothesis space \(^{d}\). The constant \(G=_{,y,} (^{},y)_{2}\) is the maximum gradient norm and \(L=_{,y,} (^{},y)\) is the upper bound of the loss values. We use the \(}()\)-notation to hide dependence on logarithmic factors of \(T\).

### Importance Weighted ERM for Continuous Shift

Our algorithm is based on the importance weighted learning. Consider the scenario where the learner has a predefined \(_{t}:[0,B]\) as an estimation for the true density ratio \(r_{t}^{*}()=_{t}()/_{0}()\) for each time \(t[T]\). Then, the predictor can be trained by the importance weighted ERM method:

\[}_{t}=_{}_{  S_{0}}[_{t}()(^{} ,y)].\] (2)

The averaged excess risk of the predictor is closely related to the quality of the density ratio estimator.

**Proposition 1**.: _For any \((0,1]\), with probability at least \(1-\), IWERM (2) with the estimator \(_{t}()\) ensures \(_{T}(\{}_{t}\}_{t=1}^{T}) 2_{t=1}^{T} _{ S_{0}}|_{t}()-r_{t}^{*} ()|/T+((T/)/})\)._

In above, the \(( T/})\) term measures the _generalization gap_ of the predictor since it is trained over the empirical data \(S_{0}\) instead of \(_{0}\). Such a rate is tight up to a logarithmic factor in \(T\). Indeed, considering a stationary environment (i.e., \(_{t}(,y)=_{1}(,y)\)) and an exact density ratio estimation (i.e., \(_{t}()=r_{t}^{*}()\)), Proposition 1 indicates that the averaged model \(}_{T}=_{t=1}^{T}}_{t}/T\) enjoys an excess risk bound of \(R_{1}(}_{T})-R_{1}(_{1}^{*})(  T/})\), matching the lower bound for importance weighted learning (36, Proposition 2) up to an \(( T)\) factor.

Our main focus lies on the \(_{t=1}^{T}_{ S_{0}}[|_{t}()- r_{t}^{*}()|]/T\) term, which is the averaged estimation error of the density ratio estimator \(_{t}\) to the time-varying ground truth \(r_{t}^{*}\). To minimize the estimation error, a fundamental challenge comes from the _unknown non-stationarity_ exhibited in the online environments -- how to select a right amount of historical data to reuse for each iteration? Intuitively, if the environments change slowly, it would be preferable to reuse all historical data to construct the density ratio estimator \(_{t}\). However, when distribution shifts occur frequently, earlier data could be useless even potentially harmful. In such scenarios, training the density ratio estimator with the most recent data would be a more rational strategy. Furthermore, even if we could capture a roughly stationary period, it remains unclear how to update the density ratio estimator in an online manner with guarantees. To our knowledge, online density ratio estimation is underexplored in the literature, even for the stationary testing streaming, not to mention the more challenging non-stationary setup.

### Online Density Ratio Estimation

Here, we present a generic reduction of the online density ratio estimation problem to a _dynamic regret minimization_ problem. This novel perspective paves our way for designing an algorithm in tackling the non-stationarity of environments, as mentioned in Section 3.2.

**Bregman Divergence Density Ratio Matching.** Our reduction is based on the Bregman divergence density ratio matching , a general framework that unifies various existing DRE methods. Specifically, in the framework, the discrepancy between the ground-truth density ratio function \(r_{t}^{*}\) and any density ratio function \(r\) is measured by the expected Bregman divergence over \(_{0}()\) defined by

\[_{}(r_{t}^{*} r)=_{_{0}( )}[_{}(r_{t}^{*}() r( ))],\] (3)

where \(_{}(a b)(a)-(b)-(b)(a-b)\) is the Bregman divergence and \(:\;\) is the associated divergence function. The expected Bregman divergence can be equally rewritten as \(_{}(r_{t}^{*} r)=L_{t}^{}(r)-L_{t}^{}(r_{t}^{*})\), where \(L_{t}^{}\) is the loss for the density ratio function defined by

\[L_{t}^{}(r)=_{_{0}()} (r())r()-(r())-_ {_{t}()}(r()) .\] (4)As a consequence, one can train the density ratio estimator by minimizing the loss \(L_{t}^{}(r)\). We have \(r_{t}^{*}_{r}L_{t}^{}(r)\) when \(\) is the set of all measurable functions.

The Bregman divergence-based density ratio matching framework takes various DRE methods as special cases. For instance, choosing the divergence function as \(_{}(t)=(t-1)^{2}/2\) yields LSIF  and KMM ; choosing \(_{}(t)=t t-(t+1)(t+1)\) leads to the logistic regression method ; and one can also recover the UKL  and KLLEP  with \(_{}(t)=t t-t\).

**Online DRE via Dynamic Regret Minimization.** For a single-round density ratio estimation, the Bregman divergence-based framework suggests to train the density ratio estimator \(_{t}\) by minimizing the gap \(L_{t}^{}(_{t})-L_{t}^{}(r_{t}^{*})\). Therefore, to derive an estimator sequence that performs well over time, we utilize the cumulative loss gap \(_{t=1}^{T}L_{t}^{}(_{t})-_{t=1}^{T}L_{t}^{}(r_{t}^{ *})\) as a performance measure. Indeed, the cumulative loss gap serves as an upper bound of the estimation error of density ratio estimators.

**Proposition 2**.: _Let \(\) be a \(\)-strongly convex function. For any density ratio estimator sequence \(\{_{t}\}_{t=1}^{T}\), we have \(_{t=1}^{T}_{_{0}}[| {r}_{t}()-r_{t}^{*}()|]_{t=1}^{T}L_{ t}^{}(_{t})-_{t=1}^{T}L_{t}^{}(r_{t}^{*})/( T)}\)._

Proposition 2 indicates that it suffices to optimize the cumulative loss gap to perform online density ratio estimation. Such a measure quantifies the performance difference between the online algorithm (that yields the estimated ratio sequence \(\{_{t}\}_{t=1}^{T}\)) and a sequence of _time-varying_ comparators (the true ratio sequence \(\{r_{t}^{*}\}_{t=1}^{T}\)). This is exactly the _dynamic regret_ in online learning literature . As such, we have reduced the online density ratio estimation to a problem of dynamic regret minimization over loss functions \(\{L_{t}^{}\}_{t=1}^{T}\) against comparators \(\{r_{t}^{*}\}_{t=1}^{T}\).

Since the loss function involves the expectation over the underlying distribution \(_{0}()\) (see the definition in (4)), we need a counterpart result with respect to empirical data \(S_{0}\).

**Theorem 1**.: _Let \(\) be a \(\)-strongly convex function satisfying \(t^{3}(t) 0\) and \(^{3}(t) 0\) for all \(t\). Let \(_{}=\{ h(,)\}\) be a hypothesis space of density ratio functions parameterized by a finite-dimensional bounded set \(\{^{d}\|\|_{2} S\}\) with a certain link function \(h:\). Denote by \([z]_{+}\{z,0\}\). Then, for any density ratio estimator \(_{t}_{}\), the empirical estimation error is bounded by_

\[_{t=1}^{T}_{ S_{0}()} |r_{t}^{*}()-_{t}()|_{t=1}^{T}_{t}^{}(_{t})-_{t =1}^{T}_{t}^{}(r_{t}^{*})_{+}}+((T/)}{}}),\]

_provided that \(h(,)\) is bounded for any \(\) and \(\) and Lipschitz continuous. In the above,_

\[_{t}^{}(r)=_{ S_{0}}[ (r())r()-(r())]-_{ _{t}()}[(r())]\] (5)

_is the empirical approximation of the expected loss \(L_{t}^{}\) using the data \(S_{0}\)._

**Remark 1** (assumptions on \(\)).: We imposed certain assumptions on the divergence function \(\) in Theorem 1. One can check these conditions hold for commonly used \(\) in density ratio estimation, including _all_ the divergence functions mentioned earlier (\(_{}\) and \(_{}\) are strongly convex when the inputs are upper bounded, which can be satisfied with suitable choices of \(_{}\)). Section 4 will present an example with \(_{}\) to show how the conditions are satisfied. \(\)

Theorem 1 shows that we can immediately obtain a sequence of high-quality density ratio estimators \(\{r_{t}^{*}\}_{t=1}^{T}\) by minimizing the dynamic regret with respect to \(\{_{t}^{}\}_{t=1}^{T}\),

\[_{T}^{}(\{_{t}^{},r_{t}^{*}\}_{t=1}^{T} )=_{t=1}^{T}_{t}^{}(_{t})- _{t=1}^{T}_{t}^{}(r_{t}^{*}).\] (6)

One caveat is that the second term of \(_{t}^{}\) in the definition (5) requires the knowledge of underlying distribution \(_{t}\), which is unavailable. Empirically, we can only observe \(_{t}^{}\) defined below, building upon the empirical observations \(S_{t}_{t}\),

\[_{t}^{}(r)=_{ S_{0}}[(r ())r()-(r())]-_{  S_{t}}[(r())].\] (7)

That said, we used to design an online optimization process to minimize the dynamic regret (6) defined over \(_{t}^{}\) based on the observed loss \(\{_{t}^{}\}_{t=1}^{T}\). Since the time-varying comparator \(r_{t}^{*}\) in (6)is _not_ the minimizer of the observed loss \(_{t}^{}\) (but rather the minimizer of the expected loss \(L_{t}^{}\) defined in (4)), directly minimizing the empirical loss \(_{t}^{}\) will lead to a high estimation error. In the next section, we introduce how to optimize the dynamic regret with the _online ensemble_ framework developed in recent studies of non-stationary online convex optimization [15; 16; 19].

**Remark 2** (comparison with previous work).: For the continuous label shift , the online ensemble framework was employed to train the predictor \(_{t}\). However, the previous method crucially relies on the construction of an unbiased importance ratio estimator satisfying \(_{_{t}}[_{t}()]=r_{t}^{*}()\). Such a favorable property is hard to be satisfied in the covariate shift case. For example, in the Bregman divergence density ratio matching framework, one can only observe an unbiased loss \(_{t}^{}(r)\), whose minimizer \(_{t}=*{arg\,min}_{r}_{t}^{}(r)\) is not unbiased to the true value \(r_{t}^{*}\) in general. To this end, we disentangle the model training and importance weight estimation process in this paper. Our reduction holds for the general Bergman divergence matching framework and thus can be initiated with different models (not necessarily unbiased). It is possible to extend our framework to continuous label shift problem with other importance weight estimators besides the unbiased one  used in . \(\)

## 4 Instantiation: Logistic Regression Model

When the loss function \(_{t}^{}\) is non-convex, it is generally intractable to conduct the online optimization, regardless of minimizing the standard regret or the strengthened dynamic regret. Fortunately, the attained loss functions are convex or enjoy even stronger curvature with the properly chosen hypothesis space and divergence function. In this section, we instantiate our framework with the logistic regression model. A corresponding online DRE method is presented with dynamic regret guarantees.

**Example 1** (Logistic regression model).: Consider the function \(=_{} t t-(t+1)(t+1)\) and the hypothesis space \(H_{}^{}=\{(-^{}( ))\|\|_{2} S\}\). Here, \(:^{d}\) represents a specific basis function with bounded norm \(\|()\|_{2} R\), which could be, for instance, the feature representation extractor from a deep neural network. Then, the loss function \(_{t}^{}()\) as per (7) becomes

\[_{t}^{}()=_{S_{0}}[(1+e^ {-()^{}})]+_{S_{t}}[(1+e^{()^{}})].\]

Let \(=(SR)\). Then, the output of any density ratio function \(r_{}\) is bounded by \([1/,]\). It can be validated that \(_{}\) is \(1/(+^{2})\)-strongly convex and satisfies the condition \(^{3}_{}(z) 0\) required by Theorem 1. Besides, the logistic loss is a \(1/(1(1+))\)_-expconcave function_ and \(R^{2}/2\)_-smooth function_, presenting a favorable function properties for online convex optimization .

We note that the other two divergence functions \(_{}\) and \(_{}\) also exhibit desirable function properties as \(_{}\). When choosing the hypothesis space \(H_{}=\{^{}()\}\), the methods recover the the uLISF  and UKL  density ratio estimators equipped with the generalized linear model. Our analysis is also applicable in the two cases. More details are provided in Appendix D.7.

### Density Ratio Estimation via Online Ensemble

In this part, we introduce our online ensemble method for online DRE with the logistic regression model. Since the logistic regression loss is exp-concave,  shows that one can employ the following-the-leading-history (FLH) method  to minimize the dynamic regret. However, a caveat is that the observed loss \(_{t}\) is only an empirical estimation of \(_{t}\) established on few online data \(S_{t}\). The result of  only implies an _expected_ bound for our problem.

We twisted the FLH algorithm to achieve a high probability bound. As shown in Figure 1, our algorithm maintains multiple base-learners, each learning over different intervals of the time-horizon, and then employs a meta-learner to aggregate their predictions. We modify the meta-learner in FLH from Hedge  to Adapt-ML-Prod , which ensures that the meta-learner can track each base-learner on the associated time interval with high probability. This strategy allows us to selectively reuse the historical information to handle the non-stationary environments. We introduce ingredients of our algorithms as follows. A more detailed algorithm descriptions and comparison with the dynamic regret minimization literature in OCO can be found in Appendices D.1 and B respectively.

**Base-learner.** Our algorithm runs multiple base-learners that are active on different intervals of the time horizon. Let \(=\{I_{i}=[s_{i},e_{i}][T]\}\) be a interval set. Each base-learner \(_{i}\) will only update and submit the model \(_{t,i}\) to the meta-learner if \(t I_{i}\). We use the online Newton step (ONS) for the model update. For an exp-concave loss function, the method ensures the base-learner's model is comparable with the best fixed model over the interval up to a logarithmic factor in \(T\). Specifically, for each base-learner \(_{i}\) running on the interval \([s_{i},e_{i}]\), ONS updates the model by

\[_{t+1,i}=_{}^{A_{t,i}}[_{t,i}- A _{t,i}^{-1}_{t}(_{t,i})],\; t[s_{i },e_{i}]\;,\] (8)

where the matrix \(A_{t,i}= I+_{=s_{i}}^{t}_{}(_{,i})_{s}(_{,i})^{}\). In the above, the projection function is defined as \(_{}^{A_{t,i}}[_{1}]=_{}- _{1}_{A_{t,i}}\) and \(=\{^{d}_{2} S\}\) is the parameter space. The constant \(>0\) and \(>0\) are the regularizer parameter and step size to be specified latter.

**Meta-learner.** As shown by Figure 1, the meta-learner's role is to aggregate the models produced by the base-learners using a weighting scheme. At each iteration \(t\), the meta-learner will maintain a weight \(p_{t,i}\) for each "active" base-learner \(_{i}\), defined as the one whose associated interval contains \(t\). We update the weights for active base-learners based on the Adapt-ML-Prod method . Specifically, for every active base-learner \(_{i}\), our algorithm maintains a "potential" \(v_{t,i}_{+}\) at iteration \(t\), which reflects the historical performance of the base-learner until time \(t\). Then, denoting by \(_{t}\) the index set of the active base-learners at time \(t\), their weights and the output model are obtained by

\[p_{t,i}_{t-1,i}v_{t-1,i}i_{t}\;\;\; \;\;\;_{t+1}=_{i_{t+1}} p_{t+1,i}_{t+1,i}\;.\] (9)

In the above \(_{t,i}>0\) is a step size that can be automatically tuned along the learning process. A noteworthy ingredient of our algorithm is the construction of the potential \(v_{t,i}\), using the linearlized loss \(_{t}(_{t}),_{t}- _{t,i}/(SR)\). Then, the generalization gap between \(_{t}\) and \(_{t}\) can be controlled with the negative term introduced by the exp-concave loss function. As a result, we can establish a high probability bound to ensure that \(_{t}\) is competitive with any base-learner's model \(_{t,i}\) on the corresponding interval \(I_{i}\). The detailed configurations of \(v_{t,i}\) and \(_{t,i}\) are deferred to Appendix D.1.

**Schedule of Intervals.** As shown in Figure 1, we specify the intervals with the geometric covering scheme , where the interval set is defined as \(=_{k\{0\}}_{k}\) with \(_{k}=\{[i 2^{k},(i+1) 2^{k}-1] i i 2^{k} T\}\). One can check that \(||\) is at most \(T\), and the number of active base-learner is bounded as \(|_{t}| t\). Thus, we only need to maintain at most \(( t)\) base-learners at time \(t\). The intervals specified by the geometric covering are informative to capture the non-stationarity of the environments, leading to the following dynamic regret guarantees.

### Theoretical Guarantees

This part presents the theoretical guarantees of our method. The estimator \(_{t}()=(-()^{}_{t})\) established on \(_{t}\) returned by the online ensemble method (9) achieves the dynamic regret guarantee.

**Theorem 2**.: _Assume the true density ratio \(r_{t}^{*}()=_{t}()/_{0}()\) is contained in the hypothesis space as \(r_{t}^{*} H_{}^{}\{(-( )^{})\}\) for any \(t[T]\). Then, with probability at least \(1-\), the dynamic regret of the density ratio estimator \(_{t}()=(-()^{}_{t})\) is bounded by_

\[_{T}^{}(\{_{t},r_{t}^{*}\}_{t=1}^{T}) }(\{T^{}V_{T}^{},1\}+T/N_ {0}),\]

_when the parameters are set as \(=3(1+)\) and \(=1\). In the above, \(V_{T}=_{t=2}^{T}_{t}()-_{t-1}()_{1}\) measures the variation of input densities._

Figure 1: An illustration of our online ensemble method, where we employ a meta-learner to aggregate the predictions from base-learners running over different intervals of the time horizon.

Theorem 2 imposes a realizable assumption for the true density ratio function such that \(r_{t}^{*} H^{}_{}\). Such an assumption is required since we train the density ratio estimator on a given hypothesis space while the true density ratio function could be arbitrary. We note that the realizability assumption is frequently used in the analysis for the density ratio estimation problem [29; 45] and other related topics, including active learning  and contextual bandit , where the density ratio (or density) estimation is required. A possible direction to relax the assumption is to consider a richer function class, e.g., a neural network or a non-parametric model. We leave this as a future work.

The \(}(T^{}V_{T}^{})\) rate in our bound exhibits the same rate as the minimax optimal dynamic regret bound for the squared loss function with the noisy feedback . Since the squared loss function enjoys even stronger curvature than the logistic loss, our result is hard to be improved. To achieve this fast-rate result, the key was to show a "squared" formulation \(\{1,|I|V_{I}^{2}\}\) of the dynamic regret for the base-leaner on each interval \(I\) (Lemma 3 in Appendix D.2).  achieved this by a complicated analysis with the KKT condition to capture the structure of the comparators, while we greatly simplified the analysis by exploiting the structure that the comparator \(_{t}^{*}\) is essentially the minimizers of _expected_ functions \(L_{t}\) (hence avoiding analyzing the KKT condition). Our analysis is applicable to the case where the minimizes lie in the interior of the decision set and only requires the smoothness of the loss function, which can be of independent interest in online convex optimization.

**Averaged Excess Risk Bound for Continuous Covariate Shift.** After obtaining the density ratio estimator, we can train the predictive model by IWERM (2), leading to the following guarantee.

**Theorem 3**.: _Under the same condition as Theorem 2 and letting \(V_{T}=_{t=2}^{T}_{t}()-_{t-1}( )_{1}\), running IWERM (2) with the estimated density ratio function \(_{t}()=(-()^{}_{t})\) yields_

\[_{T}(\{}_{t}\}_{t=1}^{T})}N_{0}^{-}+T^{-}V_{T}^{ },T^{-}\,}.\]

Theorem 3 shows that our learned predictor \(}_{t}\) adapts to the environment with a converged average excess risk when compared with the per-round best predictor \(_{t}^{*}\). By the discussion below Proposition 1, the \(}(N_{0}^{-1/2})\) generalization error over the offline data \(S_{0}\) is hard to improve. We focus on the error in the online learning part. When the environment is nearly-stationary, i.e., \(V_{T}(T^{-1/2})\), our bound implies an \(}(N_{0}^{-1/2}+T^{-1/2})\) average excess risk, matching the same rate for the one-step adaptation , even if the unlabeled data appear sequentially and the comparator \(_{t}^{*}\) could change over time. When the environments shift quickly, the \((T^{-1/3}V_{T}^{1/3})\) rate still exhibits a diminishing error for covariate shift adaptation once \(V_{T}=o(T)\). Indeed, our result has the same rate as that for continuous label shift  (with a slightly different definition of \(V_{T}\) though). In Appendix D.6, we further provide evidence to show that the \((\{T^{-1/3}V_{T}^{1/3},T^{-1/2})\) rate can hardly be improved, even if one can receive labels of the testing stream after prediction.

**Discussion on Assumptions.** We end this section by a discussion on possible future directions to relax the linear model assumption for DRE and covariate shift assumption. Since our analysis is based on the online convex optimization framework, we employed the (generalized) linear model for DRE to ensure the convexity. To go beyond the linear model while still having theoretical guarantees, one might extend the online ensemble framework to learn within the Reproducing Kernel Hilbert Space, leveraging advances in online kernel learning . As for the step towards handling the general distribution shift, it is possible to consider the sparse joint shift model  where both covariate and label distribution could shift. Besides, studying how to handle the joint distribution shift with few online labeled data also presents an interesting future direction. Our research on the covariate shift might serve as a basic step towards addressing more complex real-world distribution shifts.

## 5 Experiments

**Setups.** We generate continuous covariate shift by combining two fixed distributions with a time-varying mixture proportion \(_{t}\). Specifically, given two fixed distributions \(^{}()\) and \(^{}()\), we generate samples from \(_{t}()=(1-_{t})^{}()+ _{t}^{}()\) at each round. The mixture proportion \(_{t}\) shifts in four patterns: in Sin Shift and Squ Shift, \(_{t}\) changes periodically, following sine and square waves; in Lin Shift, the environment changes slowly from \(_{1}=1\) to \(_{T}=0\) linearly over \(T\) rounds, while in Ber Shift, the proportion \(_{t}\) flips quickly between \(0\) and \(1\) with a certain probability. For parameterization in the Accous implementation, we set \(R\) by directly calculating the data norm and set \(S=d/2\) for all experiments. We repeat all experiments five times and evaluate the algorithms by the average classification error over the unlabeled test stream for 10,000 rounds.

**Contenders.** We compare our method with six algorithms, which can be divided into three groups. The first is a baseline approach that predicts directly using the model trained on initial offline data (_FIX_). The second group consists of the one-step covariate shift adaptation methods that do not reuse historical information: _DANN_ handles the shifts by learning an invariant feature space, _IW-KMM_, _IW-KEIEP_ and _IW-uLSIF_ equip the one-step IWERM method (2) with different density ratio estimators. The third is _OLRE_, which serves as an ablation study for our Accous algorithm. The _OLRE_ algorithm estimates the density ratio by running an ONS (8) with all historical data and then performs the IWERM at each round. All algorithm parameters are set by their default.

In the following, we focus on the results of empirical studies. Detailed configurations for the datasets, simulated shifts, and setups for the Accous algorithm and contenders can be found in Appendix A.

### Illustrations on Synthetic Data

**Average Classification Error Comparison.** We summarize the comparison results on synthetic data with different types of covariate shifts with all contenders in Figure 2. The Accous algorithm outperforms almost all other methods in the four shift patterns. We observe that the offline model FIX performs poorly compared to the online methods. The best result of DANN, IW-KMM, IW-KLIEP and IW-uLSIF is called One-Step for comparison. By reusing historical data, the Accous algorithm achieves a lower average classification error compared to these One-Step competitors that use only one round of online data. The Accous also outperforms OLRE, especially when the environments change relatively quickly (Squ, Sin, and Ber). These results justify that performing density ratio estimation with either one round of online data or all historical data is inappropriate for non-stationary environments. Moreover, when the number of unlabeled data per round changes from \(N_{t}=5\) to \(N_{t}=1\), the one-step methods suffer severe performance degradation. In contrast, our Accous algorithm still performs relatively well, highlighting the need for selective reuse of historical data.

**Effectiveness of Online Density Ratio Estimation.** We then take a closer look at the core component of our algorithm, the online density ratio estimator, in the Squ shift where the distribution of the online data shifts every \(M\) rounds. Figure 3 shows the weight assignment for base-learners with different interval lengths, averaged over their active period. The result shows that our meta-learner successfully assigns the largest weight to the base-learner whose interval length matches the switching period \(M\), which ensures that the right amount of historical data is reused. Figure 4 also shows the average error of Accous and OLRE over \(1,000\) iterations. The results show that Accous (red line) can quickly adapt to new distributions after the covariate shift occurs. In contrast, OLRE (blue line) struggles because it uses all the historical data generated from different distributions. Furthermore, we denote the loss of the density ratio estimator \(_{t}(_{t})\) by the green line, which is quickly minimized after the covariate shift occurs. The similar tendency between the average error (red line) and the loss of the estimator (green line) confirms our theoretical finding in Theorem 1: we can minimize the excess risk of the predictor by minimizing the dynamic regret of the density ratio estimator.

### Comparison on Real-world Data

**Performance Comparison on Benchmarks.** In Table 1 and Table 2, we present the results of the average classification error comparison with the contenders on four benchmark datasets. Ouralgorithm Accous outperforms other contenders in the four types of shifts, whether in a slowly evolving environment (Lin) or in relatively non-stationary environments (Squ,Sin,Ber). The OLRE algorithm uses all historical data, but does not necessarily outperform contenders that use only current data, suggesting that we should selectively reuse the historical data.

**Case Study on a Real-life Application.** We conducted empirical studies on a real-world application using the yearbook dataset , which contains high school front-facing yearbook photos from 1930 to 2013. The photo distributions shift along years due to the changing social norms, fashion styles, and population demographics. Consequently, it is essential to adapt to these changes. We used photos before 1945 as the offline labeled dataset and generated the online unlabeled data stream using photos after 1945, arranged chronologically with 10 photos per round. Figure 5 shows the average error curves. The OLER algorithm performs well when the distributions do not change too much, but the method starts to suffer from large prediction errors after \(400\) iterations. In contrast, our method outperforms all competitors by selectively reusing historical data.

## 6 Conclusion

This paper initiated the study of the continuous covariate shift with the goal of minimizing the cumulative excess risk of the predictors. We showed that the importance-weighted ERM method works effectively given high-quality density ratio estimators (Proposition 1), whose estimation can be cast as a dynamic regret minimization problem (Theorem 1). Instantiating with the logistic regression density ratio estimation model, we proposed a novel online that can adaptively reuse the historical data and enjoy a tight dynamic regret bound (Theorem 2). The regret bound finally implies an \(}(T^{-1/3}V_{T}^{1/3})\) averaged excess risk guarantee for the predictor. Experiments validate the effectiveness of the proposed method and demonstrate the need for selective reuse of historical data.