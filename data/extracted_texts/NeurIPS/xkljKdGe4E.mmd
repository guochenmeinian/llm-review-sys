# Classic GNNs are Strong Baselines:

Reassessing GNNs for Node Classification

 Yuankai Luo

Beihang University

The Hong Kong Polytechnic University

luoyk@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming75, 74, 88, 91, 15, 64, 7, 29, 41, 38] to utilize GTs to tackle node classification tasks, especially on large-scale graphs, addressing the aforementioned limitations of GNNs. While recent advancements in state-of-the-art GTs  have shown promising results, it's observed that many of these models, whether explicitly or implicitly, still rely on GNNs for learning local node representations, integrating them alongside the global attention mechanisms for a more comprehensive representation.

This prompts us to reconsider: _Could the potential of message-passing GNNs for node classification have been previously underestimated?_ While prior research has addressed this issue to some extent , these studies have limitations in terms of scope and comprehensiveness, including a restricted number and diversity of datasets, as well as an incomplete examination of hyperparameters. In this study, we comprehensively reassess the performance of GNNs for node classification, utilizing three classic GNN models--GCN , GAT , and GraphSAGE --across 18 real-world benchmark datasets that include homophilous, heterophilous, and large-scale graphs. We examine the influence of key hyperparameters on GNN training, including normalization , dropout , residual connections , and network depth. We summarize the key findings in our empirical study as follows:

* With proper hyperparameter tuning, classic GNNs can achieve highly competitive performance in node classification across homophilous and heterophilous graphs with up to millions of nodes. Notably, classic GNNs outperform state-of-the-art GTs, achieving the top rank on 17 out of 18 datasets. This indicates that the previously claimed superiority of GTs over GNNs may have been overstated, possibly due to suboptimal hyperparameter configurations in GNN evaluations.
* Our ablation studies have yielded valuable insights into GNN hyperparameters for node classification. We demonstrate that (1) normalization is essential for large-scale graphs; (2) dropout consistently proves beneficial; (3) residual connections can significantly enhance performance, especially on heterophilous graphs; and (4) GNNs on heterophilous graphs tend to perform better with deeper layers.

## 2 Classic GNNs for Node Classification

Define a graph as \(=(,,,)\), where \(\) denotes the set of nodes, \(\) represents the set of edges, \(^{|| d}\) is the node feature matrix, with \(||\) representing the number of nodes and \(d\) the dimension of the node features, and \(^{|| C}\) is the one-hot encoded label matrix, with \(C\) being the number of classes. Let \(^{||||}\) denote the adjacency matrix of \(\).

**Message Passing Graph Neural Networks (GNNs)** compute node representations \(_{v}^{l}\) at each layer \(l\) as:

\[_{v}^{l}=^{l}(_{v}^{l-1},^{l}( \{_{u}^{l-1} u(v)\}) ),\] (1)

where \((v)\) represents the neighboring nodes adjacent to \(v\), \(^{l}\) serves as the message aggregation function, and \(^{l}\) is the update function. Initially, each node \(v\) begins with a feature vector \(_{v}^{0}=_{v}^{d}\). The function \(^{l}\) aggregates information from the neighbors of \(v\) to update its representation. The output of the last layer \(L\), i.e., \((v,,)=_{v}^{L}\), is the representation of \(v\) produced by the GNN. In this work, we focus on three classic GNNs: GCN , GraphSAGE , and GAT , which differ in their approach to learning the node representation \(_{v}^{l}\).

**Graph Convolutional Networks (GCN)**, the standard GCN model, is formulated as:

\[_{v}^{l}=(_{u(v)\{v\}}_{u}_{v}}}_{u}^{l-1}^{l}),\] (2)

where \(_{v}=1+_{u(v)}1\), \(_{u(v)}1\) denotes the degree of node \(v\), \(^{l}\) is the trainable weight matrix in layer \(l\), and \(\) is the activation function, e.g., ReLU(-) = \((0,)\).

**GraphSAGE** learns node representations through a different approach:

\[_{v}^{l}=(_{v}^{l-1}_{1}^{l}+(_{u (v)}_{u}^{l-1})_{2}^{l}),\] (3)

where \(_{1}^{l}\) and \(_{2}^{l}\) are trainable weight matrices, and \(_{u(v)}_{u}^{l-1}\) computes the average embedding of the neighboring nodes of \(v\).

**Graph Attention Networks (GAT)** employ masked self-attention to assign weights to different neighboring nodes. For an edge \((v,u)\), the propagation rule of GAT is defined as:

\[_{vu}^{l}=(_{l}^{} [^{l}_{v}^{l-1}.\|^{l}_{u}^{l-1}] ))}{_{r(v)}(( _{l}^{}[^{l}_{v}^{l-1}.\|^{l}_ {r}^{l-1}]))},\]

\[_{v}^{l}=(_{u(v)}_{vu}^{l}_{u}^ {l-1}^{l}),\] (4)

where \(_{l}\) is a trainable weight vector, \(^{l}\) is a trainable weight matrix, and \(\|\) represents the concatenation operation.

**Node Classification** aims to predict the labels of the unlabeled nodes. Typically, for any node \(v\), the node representation generated by the last GNN layer is passed through a prediction head \(g()\), to obtain the predicted label \(}_{v}=g((v,,))\). The training objective is to minimize the total loss \(L()=_{v_{}}(}_{v},_{v})\) w.r.t. all nodes in the training set \(_{}\), where \(_{v}\) indicates the ground-truth label of \(v\) and \(\) indicates the trainable GNN parameters.

**Homophilous and Heterophilous Graphs.** Node classification can be performed on both homophilous and heterophilous graphs. Homophilous graphs are characterized by edges that tend to connect nodes of the same class, while in heterophilous graphs, connected nodes may belong to different classes . GNN models implicitly assume homophily in graphs , and it is commonly believed that due to this homophily assumption, GNNs cannot generalize well to heterophilous graphs [90; 4]. However, recent works [46; 40; 58; 42] have empirically shown that standard GCNs also work well on heterophilous graphs. In this study, we provide a comprehensive evaluation of classic GNNs for node classification on both homophilous and heterophilous graphs.

## 3 Key Hyperparameters for Training GNNs

In this section, we present an overview of the key hyperparameters for training GNNs, including normalization, dropout, residual connections, and network depth. These hyperparameters are widely utilized across different types of neural networks to improve model performance.

**Normalization.** Specifically, Layer Normalization (LN)  or Batch Normalization (BN)  can be used in every layer before the activation function \(()\). Taking GCN as an example:

\[_{v}^{l}=((_{u(v)\{v\}} {_{u}_{v}}}_{u}^{l-1}^{l})).\] (5)

The normalization techniques are essential for stabilizing the training process by reducing the _covariate shift_, which occurs when the distribution of each layer's node embeddings changes during training. Normalizing the node embeddings helps to maintain a more consistent distribution, allowing the use of higher learning rates and leading to faster convergence .

**Dropout**, a technique widely used in convolutional neural networks (CNNs) to address overfitting by reducing co-adaptation among hidden neurons [83; 22], has also been found to be effective in addressing similar issues in GNNs [68; 65], where the co-adaptation effects propagate and accumulate through message passing among different nodes. Typically, dropout is applied to the feature embeddings after the activation function:

\[_{v}^{l}=(((_{u(v) \{v\}}_{u}_{v}}}_{u}^{l-1}^{l}))).\] (6)

**Residual Connections** significantly enhance CNN performance by connecting layer inputs directly to outputs, thereby alleviating the vanishing gradient issue. They were first adopted by the seminal GCN paper  and subsequently incorporated into DeepGCNs  to boost performance. Formally, linear residual connections can be integrated into GNNs as follows:

\[_{v}^{l}=(((_{v}^{l-1}}^{ l}+_{u(v)\{v\}}_{u}_{v}}} _{u}^{l-1}^{l}))),\] (7)where \(_{r}^{l}\) is a trainable weight matrix. This configuration mitigates gradient instabilities and enhances GNN expressiveness , addressing the over-smoothing  and oversquashing  issues since the linear component (\(_{u}^{l-1}_{r}^{l}\)) helps to preserve distinguishable node representations .

**Network Depth.** Deeper network architectures, such as deep CNNs [21; 25], are capable of extracting more complex, high-level features from data, potentially leading to better performance on various prediction tasks. However, GNNs face unique challenges with depth, such as over-smoothing , where node representations become indistinguishable with increased network depth. Consequently, in practice, most GNNs adopt a shallow architecture, typically consisting of 2 to 5 layers. While previous research, such as DeepGCN  and DeeperGCN , advocates the use of deep GNNs with up to 56 and 112 layers, our findings indicate that comparable performance can be achieved with significantly shallower GNN architectures, typically ranging from 2 to 10 layers.

## 4 Experimental Setup for Node Classification

**Datasets.** Table 1 presents a summary of the statistics and characteristics of the datasets.

* **Homophilous Graphs. Cora, CiteSeer**, and **PubMed** are three widely used citation networks . We follow the semi-supervised setting of  for data splits and metrics. Additionally, **Computer** and **Photo** are co-purchase networks where nodes represent goods and edges indicate that the connected goods are frequently bought together. **CS** and **Physics** are co-authorship networks where nodes denote authors and edges represent that the authors have co-authored at least one paper. We adhere to the widely accepted practice of training/validation/test splits of 60%/20%/20% and metric of accuracy [7; 64; 12]. Furthermore, we utilize the **WikiCS** dataset and use the official splits and metrics provided in .
* **Heterophilous Graphs. Squirrel** and **Chameleon** are two well-known page-page networks that focus on specific topics in Wikipedia. According to the heterophilous graphs benchmarking paper , the original split of these datasets introduces overlapping nodes between training and testing, leading to the proposal of a new data split that filters out the overlapping nodes. We use its provided split and its metrics for evaluation. Additionally, we utilize four other heterophilous datasets proposed by the same source : **Roman-Empire**, where nodes correspond to words in the Roman Empire Wikipedia article and edges connect sequential or syntactically linked words; **Amazon-Ratings**, where nodes represent products and edges connect frequently co-purchased items; **Minesweeper**, a synthetic dataset where nodes are cells in a \(100 100\) grid and edges connect neighboring cells; and **Questions**, where nodes represent users from the Yandex Q question-answering website and edges connect users who interacted through answers. All splits and evaluation metrics are consistent with those proposed in the source.

   Dataset & Type & \# nodes & \# edges & \# Features & Classes & Metric \\  Cora & Homophily & 2,708 & 5,278 & 1,433 & 7 & Accuracy \\ CiteSeer & Homophily & 3,327 & 4,522 & 3,703 & 6 & Accuracy \\ PubMed & Homophily & 19,717 & 44,324 & 500 & 3 & Accuracy \\ Computer & Homophily & 13,752 & 245,861 & 767 & 10 & Accuracy \\ Photo & Homophily & 7,650 & 119,881 & 745 & 8 & Accuracy \\ CS & Homophily & 18,333 & 81,894 & 6,805 & 15 & Accuracy \\ Physics & Homophily & 34,493 & 247,962 & 8,415 & 5 & Accuracy \\ WikiCS & Homophily & 11,701 & 216,123 & 300 & 10 & Accuracy \\  Squirrel & Heterophily & 2,223 & 46,998 & 2,089 & 5 & Accuracy \\ Chameleon & Heterophily & 890 & 8,854 & 2,325 & 5 & Accuracy \\ Roman-Empire & Heterophily & 22,662 & 32,927 & 300 & 18 & Accuracy \\ Amazon-Ratings & Heterophily & 24,492 & 93,050 & 300 & 5 & Accuracy \\ Minesweeper & Heterophily & 10,000 & 39,402 & 7 & 2 & ROC-AUC \\ Questions & Heterophily & 48,921 & 153,540 & 301 & 2 & ROC-AUC \\  ogbn-proteins & Homophily (Large graphs) & 132,534 & 39,561,252 & 8 & 2 & ROC-AUC \\ ogbn-arxiv & Homophily (Large graphs) & 169,343 & 1,166,243 & 128 & 40 & Accuracy \\ ogbn-products & Homophily (Large graphs) & 2,449,029 & 61,859,140 & 100 & 47 & Accuracy \\ polec & Heterophily (Large graphs) & 1,632,803 & 30,622,564 & 65 & 2 & Accuracy \\   

Table 1: Overview of the datasets used for node classification.

* **Large-scale Graphs.** We consider a collection of large graphs released recently by the Open Graph Benchmark (OGB) : **ogbn-arxiv**, **ogbn-proteins**, and **ogbn-products**, with node numbers ranging from 0.16M to 2.4M. We maintain all the OGB standard evaluation settings. Additionally, we analyze performance on the social network **pokec**, which has 1.6M nodes, following the evaluation settings of .

**Baselines.** Our main focus lies on classic GNNs: **GCN**, **GraphSAGE**, **GAT**, the state-of-the-art scalable GTs: **SGFormer**, **Polynormer**, **GOAT**, **NodeFormer**, **NAGphormer**, and powerful GTs: **GraphGPS** and **Exphormer**. Furthermore, various other GTs like [17; 15; 38; 86; 31; 3; 6; 82; 13] exist in related surveys [23; 53], empirically shown to be inferior to the GTs we compared against for node classification tasks. For heterophilous graphs, We also consider five models designed for node classification under heterophily following : **H2GCN**, **CPGNN**, **GPRGNN**, **FSGNN**, **GloGNN**. Note that we adopt the empirically optimal Polynormer variant (Polynormer-r), which demonstrates superior performance over advanced GNNs such as LINKX  and OrderedGNN . We report the performance results of baselines primarily from [12; 76; 58], with the remaining obtained from their respective original papers or official leaderboards whenever possible, as those results are obtained by well-tuned models.

**Hyperparameter Configurations.** We conduct hyperparameter tuning on classic GNNs, consistent with the hyperparameter search space of Polynormer . Specifically, we utilize the Adam optimizer  with a learning rate from \(\{0.001,0.005,0.01\}\) and an epoch limit of 2500. And we tune the hidden dimension from \(\{64,256,512\}\). As discussed in Section 3, we focus on whether to use normalization (BN or LN), residual connections, and dropout rates from \(\{0.2,0.3,0.5,0.7\}\), the number of layers from \(\{1,2,3,4,5,6,7,8,9,10\}\). Additionally, we retrain all baseline GTs using the same hyperparameter search space and training environments as the classic GNNs. For hyperparameters specific to each GT, which are not present in the classic GNNs, we tune them according to the search space specified in the original GT paper. We report mean scores and standard deviations after 5 independent runs with different initializations. **Model\({}^{*}\)** denotes our implementation. Detailed experimental setup and hyperparameters are provided in Appendix A.

## 5 Empirical Findings

### Performance of Classic GNNs in Node Classification

In this subsection, we provide a detailed analysis of the performance of the three classic GNNs compared to state-of-the-art GTs in node classification tasks. Our experimental results across homophilous (Table 2), heterophilous (Table 3), and large-scale graphs (Table 4) reveal that classic GNNs often outperform or match the performance of advanced GTs across 18 datasets. Notably,

   & Cora & CiteSeer & PubMed & Computer & Photo & CS & Physics & WikiCS \\  
**nodes** & 2,708 & 3,327 & 19.717 & 13.572 & 7.650 & 18.333 & 34.493 & 17.01 \\ \# edges & 2,778 & 4,732 & 44.324 & 24.861 & 119.081 & 81.894 & 247.962 & 216.123 \\ Metric & Accuracy\({}^{}\) & Accuracy\({}^{}\) & Accuracy\({}^{}\) & Accuracy\({}^{}\) & Accuracy\({}^{}\) & Accuracy\({}^{}\) & Accuracy\({}^{}\) & Accuracy\({}^{}\) \\ GraphGTs & 82.84 & 1.44 & 27.73 & 11.79 & 94.919 & 95.061 & 93.931 & 971.229 & 78.664 \\
**GraphGPS** & 83.887 & 27.829 & 99.984 & 99.985 & 97.199 & 94.989 & 94.989 & 94.989 & 96.713 & 78.665 \\
**NAGphormer** & 82.12 & 11.4 & 17.49 & 79.73 & 91.22 & 99.542 & 91.91 & 97.550 & 97.344 & 77.16 \\
**NAGphormer** & 80.92 & 79.599 & 80.801 & 96.900 & 96.148 & 96.513 & 98.553 & 97.344 & 97.292 \\ Explorer & 82.77 & 71.78 & 71.63 & 79.46 & 91.47 & 95.355 & 97.493 & 96.857 & 78.54 \\
**Exphormer** & 83.29 & 78.158 & 81.759 & 86.79 & 91.800 & 95.659 & 95.922 & 97.066 & 97.388 \\ GOAT & 83.18 & 71.99 & 79.48 & 99.000 & 92.996 & 92.49 & 91.241 & 96.444 & 77.000 \\
**GOAT** & 83.268 & 71.242 & 80.806 & 80.660 & 92.299 & 93.333 & 93.881 & 96.473 & 79.600 \\ NodeFormer & 82.20 & 72.50 & 79.900 & 96.900 & 93.681 & 94.365 & 95.642 & 96.450 & 74.734 \\
**NodeFormer** & 82.758 & 72.324 & 79.59 & 87.299 & 93.934 & 95.699 & 96.848 & 75.133 & 98.899 \\
**SGFormer** & 84.50 & 72.60 & 80.30 & 80.49 & 91.999 & 95.10 & 94.04 & 94.836 & 96.600 & 73.464 \\
**SGFormer** & 84.82 & 72.52 & 80.600 & 92.420 & 95.588 & 95.714 & 96.565 & 80.058 & 80.058 \\
**Polynormer** & 83.25 & 72.31 & 79.24 & 93.681 & 94.648 & 95.534 & 97.97 & 97.48 & 80.100 \\
**Polynormer** & 83.43 & 72.19 & 79.35 & 97.334 & 96.575 & 95.428 & 97.181 & 96.802 & 86.800 \\ GCN & 81.60 & 71.60 & 71.60 & 78.00 & 80.49 & 80.659 & 97.20 & 92.914 & 96.184 & 77.473 \\
**GCN** & **85.100 & 83.500 & **73.530 & **73.535 & **73.500 & 93.000 & 96.550 & 94.600 & 73.462 & 97.185 & 80.050 \\  GraphSAGE & 82.68 & 6.50 & 71.93 & 79.41 & 91.20 & 94.59 & 94.591 & 93.91 & 96.499 & 74.77 & 94.05 \\
**GraphSAGE** & 83.88 & 6.50 & **72.26** & 65.03 & **73.37** & 97.25 & 69.31 & 93.255 & **96.78** & **92.191** & **96.38** & **92.47** & **97.195** \\
**GraphSAGE** & 83.00 & 83.00 & 72.10 & **83.10** & **87.00** & 90.180 & 93.87 & 93.61 & 96.144 & 96.17 & **96.191** & **96.191** \\
**GAT** & **81.460** & **80.00** & **74.06** & **72.22** & **80.02** & **81.02** & **80.02** & **81.02** & **81.02** & **81.02** & **81.01** & **80.17** & **80.03** & **81.16** \\  

Table 2: Node classification results over homophilous graphs (%). \({}^{}\) indicates our implementation, while other results are taken from [12; 76]. The top 1\({}^{}\), \(2^{ nd}\) and \(3^{ rd}\) results are highlighted.

among the 18 datasets evaluated, classic GNNs achieve the top rank on 17 of them, showcasing their robust competitiveness. We highlight our main observations below.

**Observations on Homophilous Graphs (Table 2).** Classic GNNs, with only slight adjustments to hyperparameters, are highly competitive in node classification tasks on homophilous graphs, often outperforming state-of-the-art graph transformers in many cases.

While previously reported results show that most advanced GTs outperform classic GNN on homophilous graphs , our implementation of classic GNNs can place within the top two for four datasets, with GCN\({}^{*}\) and GAT\({}^{*}\) demonstrating near-consistent top performances. Specifically, on CS and WikiCS, classic GNNs experience about a 3% accuracy increase, achieving top-three performances. On WikiCS, the accuracy of GAT\({}^{*}\) increases by 4.16%, moving it from seventh to first place, surpassing the leading GT, Polynormer. Similarly, on Photo and CS, GraphSAGE\({}^{*}\) outperforms Polynormer and SGFormer, establishing itself as the top model. On Cora, CiteSeer, PubMed, and Physics, tuning yields significant performance improvements for GCN\({}^{*}\), with accuracy increases ranging from 1.54% to 3.50%, positioning GCN\({}^{*}\) as the highest-performing model despite its initial lower accuracy compared to advanced GTs.

**Observations on Heterophilous Graphs (Table 3).** Our implementation has significantly enhanced the previously reported best results of classic GNNs on heterophilous graphs, surpassing specialized GNN models tailored for such graphs and even outperforming the leading graph transformer architectures. This advancement not only supports but also strengthens the findings in  that conventional GNNs are strong contenders for heterophilous graphs, challenging the prevailing assumption that they are primarily suited for homophilous graph structures.

The three classic GNNs secure top positions on five out of six heterophilous graphs. Specifically, on well-known page-page networks like Chameleon and Squirrel, our implementation enhances the accuracy of GCN\({}^{*}\) by 4.98% and 6.34% respectively, elevating it to the first place among all models. Similarly, on larger heterophilous graphs such as Minesweeper and Questions, GCN\({}^{*}\) also exhibits the highest performance, highlighting the superiority of its local message-passing mechanism over GTs' global attention. On Roman-Empire, a 17.58% increase is observed in the performance of GCN\({}^{*}\). Interestingly, we find that improvements primarily stem from residual connections, which are further analyzed in our ablation study (see Section 5.2).

|p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}|}    & Squirrel & Chameleon & Amazon-Ratings & Roman-Empire & Minesweeper & Questions \\  \# nodes & 2223 & 890 & 24,492 & 22,662 & 10,000 & 48,921 \\ \# edges & 46,998 & 8,854 & 93,050 & 32,927 & 39,402 & 153,540 \\ Metric & Accuracy\({}^{}\) & Accuracy\({}^{}\) & Accuracy\({}^{}\) & Accuracy\({}^{}\) & Accuracy\({}^{}\) & ROC-AUC\({}^{}\) & ROC-AUC\({}^{}\)\({}^{}\) \\  H2GCN & 35.10 \(\)1.15 & 26.75 \(\)3.64 & 36.47 \(\)0.21 & 60.11 \(\)0.52 & 89.71 \(\)0.31 & 63.59 \(\)1.46 \\ CPGNN & 30.04 \(\)2.09 & 33.00 \(\)3.15 & 39.79 \(\)0.77 & 63.96 \(\)0.62 & 52.03 \(\)5.46 & 65.96 \(\)1.05 \\ GPRGNN & 38.95 \(\)1.39 & 39.93 \(\)3.30 & 44.88 \(\)0.54 & 64.85 \(\)0.27 & 86.24 \(\)0.41 & 55.48 \(\)0.91 \\ FSGNN & 35.92 \(\)1.27 & 40.61 \(\)1.97 & 52.74 \(\)0.85 & 79.92 \(\)0.86 & 90.08 \(\)0.79 & 78.86 \(\)0.92 \\ GloGNN & 35.11 \(\)1.24 & 25.90 \(\)3.58 & 36.89 \(\)0.14 & 59.63 \(\)0.09 & 51.08 \(\)1.25 & 65.74 \(\)1.19 \\  GraphGPS & 39.67 \(\)2.84 & 40.79 \(\)4.00 & 53.10 \(\)0.02 & 82.00 \(\)0.64 & 90.63 \(\)0.05 & 71.73 \(\)1.47 \\
**GraphGPS\({}^{*}\)** & 39.81 \(\)2.34 & **41.55** \(\)3.291 & 53.27 \(\)0.06 & 82.72 \(\)0.06 & 90.75 \(\)0.08 & 72.56 \(\)1.33 \\ NodeFormer & 38.52 \(\)1.57 & 34.33 \(\)1.41 & 43.86 \(\)0.65 & 64.49 \(\)0.77 & 86.71 \(\)0.78 & 74.27 \(\)1.46 \\
**NodeFormer\({}^{*}\)** & 38.99 \(\)2.67 & 36.38 \(\)2.85 & 43.79 \(\)0.97 & 74.83 \(\)0.82 & 87.71 \(\)0.86 & 75.02 \(\)1.41 \\ SGFormer & 41.80 \(\)2.17 & 44.93 \(\)1.91 & 48.01 \(\)0.04 & 79.10 \(\)0.32 & 80.89 \(\)0.98 & 72.15 \(\)1.31 \\
**SGFormer\({}^{*}\)** & 42.65 \(\)2.44 & 45.28 \(\)3.72 & 54.14 \(\)0.08 & 80.01 \(\)0.04 & 91.42 \(\)0.04 & 73.81 \(\)0.99 \\ Polynormer & 40.87 \(\)1.48 & 41.28 \(\)3.54 & 58.41 \(\)0.08 & 92.55 \(\)0.37 & 97.46 \(\)0.38 & 78.92 \(\)0.09 \\
**Polynormer\({}^{*}\)** & 41.97 \(\)2.84 & 41.97 \(\)3.38 & 54.96 \(\)0.02 & 92.66 \(\)0.09 & 97.49 \(\)0.04 & 78.94 \(\)0.87 \\  GCN & 38.67 \(\)1.14 & 41.31 \(\)3.05 & 48.70 \(\)0.03 & 73.69 \(\)0.74 & 89.75 \(\)0.52 & 76.09 \(\)1.27 \\
**GCN\({}^{*}\)** & **45.01** \(\)1.58 & **63.34** \(\)**6.29 & 43.09 \(\)0.49 & 58.50 \(\)0.09 & **51.01** & 91.27 \(\)0.03 & **17.58** \(\)0.03 & **17.11** & **90.20** \(\)0.09 & **2.93** \\  GraphSAGE & 36.09 \(\)1.89 & 37.77 \(\)1.44 & 53.63 \(\)0.39 & 58.74 \(\)0.67 & 93.51 \(\)0.57 & 76.44 \(\)0.02 & **76.44** \(\)0.02 \\
**GraphSAGE\({}^{*}\)** & 40.78 \(\)**4.69** & 44.81 \(\)**4.74 & 55.40 \(\)**1.77** & 51.06 \(\)**0.02 & **52.32** & 97.77 \(\)**0.03 & **42.66** & **77.21** \(\)**0.07** \\  GAT & 35.62 \(\)2.06 & 39.21 \(\)0.08 & 52.70 \(\)0.03 & 88.75 \(\)0.41 & 9.31 \(\)0.05 & 76.79 \(\)0.71 \\
**GAT\({}^{*}\)** & 41.73 \(\)2.07 & **61.11** \(\)0.43 & 44.13 \(\)**1.49 & **49.21** \(\)**55.54 \(\)**0.24** & 90.63 \(\)**0.11 \(\)**1.88** \\   

Table 3: Node classification results on heterophilous graphs (%). \({}^{*}\) indicates our implementation, while other results are taken from . The top 1\({}^

**Observations on Large-scale Graphs (Table 4).** Our implementation has significantly enhanced the previously reported results of classic GNNs, with some cases showing double-digit increases in accuracy. It has achieved the best results across these large graph datasets, either homophilous or heterophilous, and has outperformed state-of-the-art graph transformers. This indicates that message passing remains highly effective for learning node representations on large-scale graphs.

Our implementation of classic GNNs demonstrate superior performance consistently, achieving top rankings across all four large-scale datasets included in our study. Notably, GCN\({}^{*}\) emerges as the leading model on ogbn-arxiv and pokec, surpassing all evaluated advanced GTs. Furthermore, on pokec, all three classic GNNs achieve over 10% performance increases by our implementation. For ogbn-proteins, an absolute improvement of 12.99% is observed in the performance of GAT\({}^{*}\), significantly surpassing SGFormer by 5.09%. Similarly, on ogbn-products, GraphSAGE\({}^{*}\) demonstrates a significant performance increase, securing the best performance among all evaluated models. In summary, a basic GNN can achieve the best known results on large-scale graphs, suggesting that current GTs have not yet addressed GNN issues such as over-smoothing and long-range dependencies.

### Influence of Hyperparameters on the Performance of GNNs

To examine the unique contributions of different hyperparameters in explaining the enhanced performance of classic GNNs, we conduct a series of ablation analysis by selectively removing elements such as normalization, dropout, residual connections, and network depth from GCN\({}^{*}\), GraphSAGE\({}^{*}\), and GAT\({}^{*}\). The effect of these ablations is assessed across homophilous (see Table 5), heterophilous (see Table 6), and large-scale graphs (see Table 7). Our findings, which we detail below, indicate that the ablation of single components affects model accuracy in distinct ways.

**Observation 1: Normalization (either BN or LN) is important for node classification on large-scale graphs but less significant on smaller-scale graphs.**

    & ogbn-proteins & ogbn-arxiv & ogbn-products & pokec \\  \# nodes & 132,534 & 169,343 & 2,449,029 & 1,632,803 \\ \# edges & 39,561,252 & 1,166,243 & 61,859,140 & 30,622,564 \\ Metric & ROC-AUC\(\) & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) \\  GraphGPS & 76.83 \(\) 0.26 & 70.97 \(\) 0.41 & OOM & OOM \\
**GraphGPS\({}^{*}\)** & 77.15 \(\) 0.66 & 71.23 \(\) 0.99 & OOM & OOM \\ NAGphormer & 73.61 \(\) 0.33 & 70.13 \(\) 0.55 & 73.55 \(\) 0.21 & 76.59 \(\) 0.25 \\
**NAGphormer\({}^{*}\)** & 72.17 \(\) 0.45 & 70.88 \(\) 0.24 & 74.63 \(\) 0.29 & 75.92 \(\) 0.68 \\ Exphormer & 74.58 \(\) 0.26 & 72.44 \(\) 0.28 & OOM & OOM \\
**Exphormer\({}^{*}\)** & 77.62 \(\) 0.33 & 72.32 \(\) 0.36 & OOM & OOM \\ GOAT & 74.18 \(\) 0.37 & 72.41 \(\) 0.40 & 82.00 \(\) 0.43 & 66.37 \(\) 0.94 \\
**GOAT\({}^{*}\)** & 79.31 \(\) 0.42 & 72.76 \(\) 0.29 & 82.27 \(\) 0.96 & 72.64 \(\) 0.67 \\ NodeFormer & 77.45 \(\) 1.15 & 59.90 \(\) 0.42 & 72.93 \(\) 0.13 & 71.00 \(\) 1.50 \\
**NodeFormer\({}^{*}\)** & 77.86 \(\) 0.84 & 67.78 \(\) 0.28 & 73.96 \(\) 0.30 & 71.00 \(\) 1.30 \\ SGFormer & 79.53 \(\) 0.38 & 72.63 \(\) 0.13 & 74.16 \(\) 0.31 & 73.76 \(\) 0.24 \\
**SGFormer\({}^{*}\)** & 79.92 \(\) 0.48 & 72.76 \(\) 0.33 & 81.54 \(\) 0.43 & 82.44 \(\) 0.76 \\ Polynormer & 78.97 \(\) 0.47 & 73.46 \(\) 0.16 & 83.82 \(\) 0.11 & 86.10 \(\) 0.05 \\
**Polynormer\({}^{*}\)** & 79.53 \(\) 0.67 & 73.40 \(\) 0.22 & 83.82 \(\) 0.11 & 86.06 \(\) 0.28 \\  GCN & 72.51 \(\) 0.35 & 71.74 \(\) 0.29 & 75.64 \(\) 0.21 & 75.45 \(\) 0.17 \\
**GCN\({}^{*}\)** & 77.29 \(\) 0.46 \(\) **4.78\(\)** & **73.53 \(\) 0.12 \(\) 1.79\(\)** & 82.33 \(\) 0.19 \(\) 0.69\(\) & **86.33 \(\) 0.17 \(\) 0.88\(\)** \\  GraphSAGE\({}^{*}\) & 77.68 \(\) 0.30 & 71.49 \(\) 0.27 & 78.29 \(\) 0.16 & 75.63 \(\) 0.38 \\
**GraphSAGE\({}^{*}\)** & 82.21 \(\) 0.33 \(\) **4.53\(\)** & 73.00 \(\) 0.28 \(\) **1.51\(\)** & **83.89 \(\) 0.36 \(\) 5.60\(\)** & 85.97 \(\) 0.22 \(\) 10.34\(\) \\  GAT & 72.02 \(\) 0.44 & 71.95 \(\) 0.36 & 79.45 \(\) 0.59 & 72.23 \(\) 0.18 \\
**GAT\({}^{*}\)** & **85.01 \(\) 0.46 \(\) 12.99\(\)** & 73.30 \(\) 0.18 \(\) **1.35\(\)** & 80.99 \(\) 0.16 \(\) **1.54\(\)** & **86.19 \(\) 0.23 \(\) 13.96\(\)** \\   

Table 4: Node classification results on large-scale graphs (%). \({}^{*}\) indicates our implementation, while other results are taken from . The top 1\({}^{}\), 2\({}^{}\) and 3\({}^{}\) results are highlighted. OOM means out of memory.

We observe that the ablation of normalization does not lead to substantial deviations on small graphs. However, normalization becomes consistently crucial on large-scale graphs, where its ablation results in accuracy reductions of 4.79% and 4.69% for GraphSAGE\({}^{*}\) and GAT\({}^{*}\) respectively on ogbn-proteins. We believe this is because large graphs display a wider variety of node features, resulting in different data distributions across the graph. Normalization aids in standardizing these features during training, ensuring a more stable distribution.

**Observation 2: Dropout is consistently found to be essential for node classification.**

Our analysis highlights the crucial role of dropout in maintaining the performance of classic GNNs on both homophilous and heterophilous graphs, with its ablation contributing to notable accuracy declines--for instance, a 2.70% decrease for GraphSAGE\({}^{*}\) on PubMed and a 6.57% decrease on Roman-Empire. This trend persists in large-scale datasets, where the ablation of dropout leads to a 2.44% and 2.53% performance decline for GCN\({}^{*}\) and GAT\({}^{*}\) respectively on ogbn-proteins.

**Observation 3: Residual connections can significantly boost performance on specific datasets, exhibiting a more pronounced effect on heterophilous graphs than on homophilous graphs.**

While the ablation of residual connections on homophilous graphs does not consistently lead to a significant performance decrease, with observed differences around 2% on Cora, Photo, and CS, the impact is more substantial on large-scale graphs such as ogbn-proteins and polec. The effect is even more dramatic on heterophilous graphs, with the classic GNNs exhibiting the most significant accuracy reduction on Roman-Empire, for instance, a 16.43% for GCN\({}^{*}\) and 5.48% for GAT\({}^{*}\). Similarly, on Minesweeper, significant performance drops were observed, emphasizing the critical importance of residual connections, particularly on heterophilous graphs. The complex structures of these graphs often necessitate deeper layers to effectively capture the diverse relationships between nodes. In such contexts, residual connections are essential for model training.

    & Squirrel & Chameleon & Amazon-Ratings & Roman-Empire & Minesweeper & Questions \\  Metric & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) & ROC-AUC\(\) & ROC-AUC\(\) \\ 
**GCN\({}^{*}\)** & **45.01**\(\) 1.63 & **46.29**\(\) 3.0 & **53.80**\(\) 6.00 & **91.27**\(\) 0.23 & **97.86**\(\) 0.24 & **79.02**\(\) 0.60 \\ (-) Normalization & 44.13 \(\) 2.03 & - & 53.68 \(\) 0.82 & 90.53 \(\) 0.93 & 96.94 \(\) 1.96 & - \\ (-) Dropout & 42.89 \(\) 1.28 & 45.28 \(\) 4.78 & 51.37 \(\) 0.34 & 85.10 \(\) 0.61 & 94.28 \(\) 2.19 & - \\ (-) Residual Connections & 43.14 \(\) 1.82 & - & 51.14 \(\) 3.34 & 74.84 \(\) 0.62 & 86.45 \(\) 0.49 & 75.87 \(\) 4.47 \\ 
**GraphSAGE\({}^{*}\)** & **40.78**\(\) 1.47 & **44.81**\(\) 4.34 & **55.40**\(\) 0.21 & **91.06**\(\) 0.27 & **97.77**\(\) 0.62 & **77.21**\(\) 1.18 \\ (-) Normalization & 40.27 \(\) 2.27 & 44.02 \(\) 3.53 & 54.41 \(\) 0.30 & 90.58 \(\) 0.24 & 97.64 \(\) 0.41 & 76.17 \(\) 0.41 \\ (-) Dropout & 38.83 \(\) 1.94 & 43.11 \(\) 3.36 & 51.12 \(\) 0.66 & 84.49 \(\) 0.35 & 93.83 \(\) 0.38 & 76.36 \(\) 1.50 \\ (-) Residual Connections & 40.06 \(\) 2.31 & 41.85 \(\) 3.36 & 53.52 \(\) 0.19 & & 96.64 \(\) 0.58 & - \\ 
**GAT\({}^{*}\)** & **41.73**\(\) 2.07 & **41.43**\(\) 4.17 & **55.54**\(\) 0.51 & **90.63**\(\) 0.14 & **97.73**\(\) 0.73 & **77.95**\(\) 0.51 \\ (-) Normalization & 41.08 \(\) 1.60 & 43.25 \(\) 1.34 & 54.85 \(\) 0.39 & 89.69 \(\) 0.39 & 97.42 \(\) 0.45 & 76.32 \(\) 0.23 \\ (-) Dropout & 39.81 \(\) 3.15 & 41.19 \(\) 2.36 & 51.48 \(\) 0.28 & 82.47 \(\) 0.70 & 92.26 \(\) 4.63 & 76.19 \(\) 0.88 \\ (-) Residual Connections & 38.46 \(\) 1.96 & 42.57 \(\) 3.66 & 51.08 \(\) 0.89 & 85.15 \(\) 0.82 & 92.83 \(\) 1.61 & 75.17 \(\) 0.71 \\   

Table 6: Ablation study on heterophilous graphs (%).

    & Cora & CiteSeer & PubMed & Computer & Photo & CS & Physics & WikiCS \\  Metric & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) \\ 
**GCN\({}^{*}\)** & **85.10**\(\) 0.65 & **73.14**\(\) 0.67 & **81.12**\(\) 0.03 & **93.99**\(\) 0.12 & **96.10**\(\) 0.08 & **96.17**\(\) 0.06 & **97.46**\(\) 0.10 & **80.30**\(\) 0.05 \\ (-) Normalization & - & - & - & 92.60 \(\) 0.14 & 95.48 \(\) 0.36 & 95.30 \(\) 0.48 & 97.16 \(\) 0.11 & 79.67 \(\) 0.02 \\ (-) Dropout & 38.46 \(\) 1.16 & 71.40 \(\) 0.35 & 80.14 \(\) 0.55 & 93.78 \(\) 0.26 & 95.31 \(\) 0.30 & 95.95 \(\) 0.11 & 97.30 \(\) 0.06 & 79.84 \(\) 0.16 \\ (-) Residual Connections & - & - & - & 94.43 \(\) 0.08 & 94.71 \(\) 0.11 & 96.56 \(\) 0.18 & - \\ 
**GraphSAGE\({}^{*}\)** & **83.88**\(\) 0.45 & **72.26**\(\) 0.58 & **79.72**\(\) 0.50 & **93.25**\(\) 0.14 & **96.78**\(\) 0.22 & **96.38**\(\) 0.11 & **97.19**\(\) 0.08 & **80.69**\(\) 0.31 \\ (-) Normalization & - & - & - & 92.77 \(\) 0.63 & 95.51 \(\) 0.88 & 95.52 \(\) 0.21 & 96.97 \(\) 0.07 & 80.08 \(\) 0.05 \\ (-) Dropout & 82.78 \(\) 0.45 & 71.02 \(\) 1.34 & 77.02 \(\) 0.63 & 92.02 \(\) 0.35 & 96.03 \(\) 0.27 & 96.11 \(\) 0.17 & 97.07 \(\) 0.09 & 79.89 \(\) 0.39 \\ (-) Residual Connections & - & - & - & - & 96.47 \(\) 0.11 & 95.73 \(\) 0.13 & 97.09 \(\) 0.04 & - \\ 
**GAT\({}^{*}\)** & **84.46**\(\) 0.45 & **72.22**\(\) 0.64 & **80.28**\(\) 0.64 & **94.09**\(\) 0.37 & **96.66**\(\) 0.39 & **96.21**

**Observation 4: Deeper networks generally lead to greater performance gains on heterophilous graphs compared to homophilous graphs.**

As demonstrated in Figure 1, the performance trends for GCN\({}^{*}\) and GraphSAGE\({}^{*}\) are consistent across different graph types. On homophilous graphs and ogbn-arxiv, both models achieve optimal performance with a range of 2 to 6 layers. In contrast, on heterophilous graphs, their performance improves with an increasing number of layers, indicating that deeper networks are more beneficial for these graphs. We discuss scenarios with more than 10 layers in Appendix B.

## 6 Conclusion

Our study provides a thorough reevaluation of the efficacy of foundational GNN models in node classification tasks. Through extensive empirical analysis, we demonstrate that these classic GNN models can reach or surpass the performance of GTs on various graph datasets, challenging the perceived superiority of GTs in node classification tasks. Furthermore, our comprehensive ablation studies provide insights into how various GNN configurations impact performance. We hope our findings promote more rigorous empirical evaluations in graph machine learning research.

    & ogbn-proteins & ogbn-arxiv & ogbn-products & pokec \\  Metric & ROC-AUC\(\) & Accuracy\(\) & Accuracy\(\) & Accuracy\(\) \\  \(^{*}\) & \( 0.46\) & \( 0.12\) & \( 0.19\) & \( 0.17\) \\ (-) Normalization & \(74.48 1.13\) & \(71.53 0.14\) & \(80.01 0.48\) & \(85.21 0.23\) \\ (-) Dropout & \(74.85 0.87\) & \(72.06 0.13\) & \(79.30 0.37\) & \(84.47 0.38\) \\ (-) Residual Connections & \(73.19 1.46\) & \(72.91 0.17\) & - & \(79.59 0.97\) \\  \(^{*}\) & \( 0.32\) & \( 0.28\) & \( 0.36\) & \( 0.21\) \\ (-) Normalization & \(77.42 0.98\) & \(71.13 0.27\) & \(82.12 0.31\) & \(84.95 0.33\) \\ (-) Dropout & \(80.52 0.49\) & \(71.30 0.21\) & \(80.36 0.43\) & \(83.06 0.28\) \\ (-) Residual Connections & \(81.75 0.53\) & \(72.22 0.69\) & - & \(85.81 0.45\) \\  \(^{*}\) & \( 0.46\) & \( 0.18\) & \( 0.16\) & \( 0.23\) \\ (-) Normalization & \(80.32 0.83\) & \(71.33 0.29\) & \(78.62 0.33\) & \(84.63 0.64\) \\ (-) Dropout & \(82.48 0.34\) & \(71.68 0.32\) & \(77.68 0.21\) & \(85.12 0.49\) \\ (-) Residual Connections & \(82.43 0.75\) & \(72.47 0.34\) & - & \(81.37 0.87\) \\   

Table 7: Ablation study on large-scale graphs (%).

Figure 1: Ablation studies of the number of layers showing, from left to right, results for homophilous graphs, heterophilous graphs, and large-scale graphs, respectively.