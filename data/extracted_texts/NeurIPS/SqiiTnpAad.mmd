# Uncertainty as a criterion for SOTIF evaluation of deep learning models in autonomous driving systems

Uncertainty as a criterion for SOTIF evaluation of deep learning models in autonomous driving systems

Ho Suk

School of Integrated Technology

Yonsei University

sukho93@yonsei.ac.kr

&Shiho Kim

School of Integrated Technology

Yonsei University

shiho@yonsei.ac.kr

###### Abstract

Ensuring the safety of deep learning models in autonomous driving systems is crucial. In compliance with the automotive safety standard ISO 21448, we propose uncertainty as a new complementary evaluation criterion to ensure the safety of the intended functionality (SOTIF) of deep learning-based systems. To evaluate and improve the trajectory prediction function of autonomous driving systems, we utilize epistemic uncertainty, quantified by a single forward pass model with consideration for constraints on resources and response time, as a criterion. Experimental results with data collected from the CARLA simulator demonstrate that uncertainty criterion can detect functional insufficiencies in unknown driving scenarios which potentially hazardous, and eventually induce extra learning.

## 1 Introduction

In the field of machine learning, the deep learning has made several technological breakthroughs over the past decade, rapidly improving its performance in a variety of tasks. Today, deep learning is widely applied to the implementation of perception and planning subsystems, realizing advances of high-level driving automation. Now, in addition to the performance of deep learning, the issue of uncertainty is also being seriously considered. The issue of uncertainty should also be discussed in the field of autonomous driving .

Safety is the most important factor in automobiles. Therefore, the automotive industry develops automobiles in compliance with automotive safety standards such as ISO 26262 and ISO 21448. ISO 26262 aims to ensure the functional safety of the system, and ISO 21448 aims to ensure the safety of the intended functionality (SOTIF) of the system. ISO 26262 only deals with faults at the software level of the deep learning model. ISO 21448, released to complement the insufficient coverage of ISO 26262, deals with insufficiencies in the intended functionality of the model. ISO 21448 emphasizes that pass/fail criteria for training and testing of deep learning should be well specified. However, it is very difficult to fully validate deep learning models that have non-linear characteristics and lack formal verification methods, so complementary validation is necessary.

Deep learning suffers from uncertainty, and ISO 21448 briefly mentions the uncertainty issue of machine learning. Nevertheless, the potentially hazardous uncertainty issue is not actively considered in the design of autonomous driving systems. In this paper, we propose the idea of quantifying uncertainty from the output of a deep learning model used in autonomous driving to utilize uncertainty as a pass/fail criterion of the deep learning model and as a fallback criterion for authority to control the driving. Section 2 explains standards for safety, types of uncertainty, and uncertainty quantification methods. Section 3 introduces a method to apply uncertainty quantification to trajectory prediction functions for autonomous driving and presents experimental results. Section 4 discusses future works and concludes the paper.

Background and related works

### Standards for automotive safety

ISO 26262ISO 26262 is an international standard that guides the development of automotive systems by ensuring functional safety. According to the ISO 26262, safety goals are decomposed into functional safety requirements, and the system are developed to meet those specifications. To prevent hazards caused by faults from becoming unreasonable risks, safety measures that satisfy functional safety requirements must be implemented. By verifying and validating that the safety measures meet the requirements, the functional safety of the automotive system can be ensured.

If a deep learning model in ADAS and autonomous driving system is viewed as a type of software that receives inputs and generates outputs through a series of computing operations, compliance with ISO 26262 can ensure the functional safety of the model by checking that there are no problems with the software architecture or code. However, deep learning models completed by training deep neural networks have a completely different paradigm from existing automotive software. The prediction process and performance limitations of deep learning are not intuitive, so it is very difficult to explain them in specifications. It is impossible to make deep learning-based software ensure the safety of automobiles with the existing ISO 26262 standard alone.

Iso 21448To make up for the coverage of ISO 26262, the ISO 21448 standard, which deals with SOTIF, was published. ISO 21448 presumes that the functional safety is guaranteed, and addresses risks that may occur in a fault-free system. Functional insufficiencies in a system may lead to hazards, which may cause risks. ISO 21448 aims to achieve SOTIF by reducing unreasonable risks as much as possible. In the SOTIF workflow, acceptance criteria are defined to prove the absence of unreasonable risks for identified hazards. SOTIF can be guaranteed by providing evidence that the acceptance criteria have been achieved with sufficient confidence through validation such as testing in known and unknown scenarios. See Figure 2 and 3 in Appendix A for details of SOTIF activities.

Specifying the criteria to ensure SOTIF of a deep learning model is different from the case of general automotive software. For example, in a deep learning model that performs object detection or classification, false positive and false negative rates can be used as pass/fail criteria during the training and test phases. At the vehicle level, accidents per unit of driving distance can be used as criteria to identify whether there are any functional insufficiencies in the intended functionality of the perception subsystem. However, to prove evidence for SOTIF, complementary criteria are needed to validate a deep learning model with low interpretability. In this case, uncertainty can be a candidate.

### Types of uncertainty

A probability vector that the classifier model outputs along with the prediction is difficult to serve as a meaningful metric to measure the confidence in the output worked out from the input data that is out of distribution from the training data . To evaluate the safety of deep learning model in any distribution, uncertainty of the output needs to be quantified.

Aleatoric uncertaintyAlexatoric uncertainty is a metric that quantifies the ambiguity of data resulting from sensor noise and randomness of the driving environment. Since noise and randomness are inherent in the data, aleatoric uncertainty cannot be relieved by increasing the amount of training data. Aleatoric uncertainty is not a major scope in this paper because it is irreducible.

    &  \# of \\ models \\  &  \# of \\ inferences \\  & 
 \# of \\ trainable \(\) \\  & Computational burden \\  Bayesian model & Single & Single & High & High (Posterior distribution) \\ Monte Carlo dropout & Single & Multiple & Moderate & High (Monte Carlo sampling) \\ Deep ensembles & Multiple & Multiple & High & High (Bootstrap aggregating) \\ Deterministic single forward pass & Single & Single & Moderate & Moderate \\   

Table 1: Summary of characteristics of state-of-the-art uncertainty quantification methods.

Epistemic uncertaintyEpistemic uncertainty is a metric that quantifies the lack of knowledge required for a model to explain data. In other words, it represents a limitation that the distribution of training data cannot properly approximate the real world. By updating the model by providing previously unseen data as additional training data, epistemic uncertainty can be decreased.

### Approaches to uncertainty quantification

State-of-the-art methods for uncertainty quantification can be classified into four categories: Bayesian model, Monte Carlo dropout, deep ensemble, and deterministic single-forward-pass method . These approaches are implemented quite distinctly from each other and have different characteristics, so it is recommended to choose the best method that fits the domain and task to which applying. Table 1 summarizes the comparison of the characteristics of the four methods.

Some studies applying uncertainty quantification to the autonomous driving domain have been published. Most of them utilized the deep ensemble architecture [11; 4; 8; 9; 12]. However, compared to a typical deep neural network, there is a trade-off between the benefit and inefficiency of uncertainty quantification in that multiple networks forming the ensemble must be trained and inferred at once . Since the autonomous driving system is executed on edge hardware, it is required to be executed with as few resources and computing power as possible. In addition, it is required to provide fast responses with the shortest possible run time for safety reasons. Considering efficiency, we judged that a deterministic single-forward-pass approach with a small number of parameters and low computational burden is suitable, so we selected Deep Deterministic Uncertainty (DDU) , one of the state-of-the-art methods, as a method for our uncertainty quantification.

## 3 Uncertainty as a criterion for SOTIF of deep learning-based trajectory prediction model

We applied uncertainty quantification to trajectory prediction, a key function required for decision making of autonomous vehicles. First, we collected driving data from the CARLA simulator as a dataset . The input data of the model consists of the state vector of a vehicle and a bird's eye view image that expresses the surroundings of a vehicle. We only use a single model for training and inferencing. Our model is a ResNet-based structure. Residual block and spectral normalization improve generalization by well regularizing the feature space and maintaining sensitivity to input at an appropriate level. See Appendix B for details of our dataset and network architecture.

Our model predicts future velocity and future yaw of each vehicle. We classify the continuous values of velocity and yaw into small intervals, so the model predicts the one class with the highest probability. In addition to getting predictions, by feeding validation data to the frozen model, we can

Figure 1: Simplified workflow of proposed idea that utilizes quantified uncertainty as a pass/fail criterion for SOTIF evaluation of a deep learning model for trajectory prediction task.

obtain two Gaussian mixture models (GMM) representing the feature space for velocity and yaw, respectively. See Figure 4 in Appendix C for details of GMM that we obtain. The obtained GMM is used to quantify epistemic uncertainty of predicted velocity and yaw via Gaussian discriminant analysis (GDA). Epistemic uncertainty is the marginal likelihood of the feature space represented by the density model such as GMM. We interpolate the predicted velocities and yaws to compute the future trajectory. If the uncertainty of velocity is high, it can be interpreted that the longitudinal uncertainty of the trajectory is high, and if the uncertainty of yaw is high, it can be interpreted that the lateral uncertainty of the trajectory is high. Figure 1 describes a workflow that quantifies uncertainty in a trajectory prediction task and uses it as SOTIF criteria.

As described in Section 2, the evaluation of deep learning models should be performed in both known and unknown scenarios. In the case of known scenarios, metrics commonly used in the evaluation of deep learning models can be defined as target requirements. On the other hand, in the case of unknown scenarios, it is difficult to set criteria in advance because there are many long tail cases in the real world. For example, it is impossible to create criteria for all driving situations that the ego vehicle may encounter. ISO 21448 simply recommends a large amount of testing to reduce unknown scenarios, and suggests cumulative test duration and scenario types as criteria.

We propose that epistemic uncertainty can be used as a criterion to identify unknown scenarios. When applied to our deep learning model, if the epistemic uncertainty of the predicted trajectory provided by the model is estimated to be high in a specific scenario during the evaluation process, it can be interpreted that the autonomous driving function is likely to fail in that scenario due to incorrect predictions caused by insufficient model knowledge. As a pass/fail criterion of the model, epistemic uncertainty can play a role of indicator for necessity of additional training in the unknown scenarios. In our experiment, highway environment corresponds to an scenario where our model is out of distribution for our model. Table 2 shows that uncertainty is measured high in highway and can be judged as unknown scenarios. It also show that uncertainty can be lowered after extra training. Please see Figure 5 and 6 in Appendix D for an example of our results.

If the model is judged to satisfy the pass/fail criteria and ensure SOTIF, the model is deployed as an autonomous driving system and operated on real world. Autonomous vehicles may encounter long tail cases that violate SOTIF. Since the deep learning model is designed to produce output even in situations where there is uncertainty due to insufficient knowledge, it may make incorrect predictions. In this case, epistemic uncertainty can serve as a start signal for the fallback mechanism. When the estimated uncertainty is high so functional insufficiency or potential hazard is expected, to prevent an accident, the system can hand over the control to the human driver or take emergency procedure. See Figure 3 in Appendix A for details of uncertainty as criteria in SOTIF activities.

## 4 Conclusion

In the trajectory prediction task, we quantify uncertainty via a deep learning model with a single forward pass. Then, we use the uncertainty as a criterion for SOTIF evaluation. If the epistemic uncertainty exceeds the criterion, we consider that the scenario could be potentially dangerous, so we collect additional data and perform extra training. The experimental results show that uncertainty has the possibility to be utilized as a criterion to judge the functional insufficiencies of autonomous driving systems in unknown scenarios.

There is also a study that performed online learning of an end-to-end autonomous driving system using uncertainty . Uncertainty quantification appears to have the potential to be used in the evaluation and learning of autonomous driving systems. However, further development is still needed to address the resource and time consumption issues caused by uncertainty quantification process.

   Scenario type & Urban (Known) & Highway (Unknown) & Highway (Extra Training) \\  Epistemic uncertainty mean & 1 & 3.42 & 1.51 \\   

Table 2: Comparison of epistemic uncertainty mean in two different driving scenarios. We normalize the uncertainty to have a value greater than 0. The epistemic uncertainty mean is the average of the quantified uncertainty over the scenario execution time. The values are expressed relatively, with the value in the known scenario as 1.