# Out-Of-Distribution Detection with Diversification (Provably)

Haiyun Yao1, Zongbo Han1, Huazhu Fu2, Xi Peng3, Qinghua Hu1, Changqing Zhang1

College of Intelligence and Computing, Tianjin University1

Institute of High Performance Computing, A*STAR2

College of Computer Science, Sichuan University3

{yaohaiyun, zongbo, huqinghua, zhangchangqing}@tju.edu.cn,

hzfu@ieee.org, pengx.gm@gmail.com

Corresponding authors.

###### Abstract

Out-of-distribution (OOD) detection is crucial for ensuring reliable deployment of machine learning models. Recent advancements focus on utilizing easily accessible auxiliary outliers (e.g., data from the web or other datasets) in training. However, we experimentally reveal that these methods still struggle to generalize their detection capabilities to unknown OOD data, due to the limited diversity of the auxiliary outliers collected. Therefore, we thoroughly examine this problem from the generalization perspective and demonstrate that a more diverse set of auxiliary outliers is essential for enhancing the detection capabilities. However, in practice, it is difficult and costly to collect sufficiently diverse auxiliary outlier data. Therefore, we propose a simple yet practical approach with a theoretical guarantee, termed Diversity-induced Mixup for OOD detection (diverseMix), which enhances the diversity of the auxiliary outlier set for training in an efficient way. Extensive experiments show that diverseMix achieves superior performance on commonly used and recent challenging large-scale benchmarks, which further confirm the importance of the diversity of auxiliary outliers. Our code is available at https://github.com/HaiyunYao/diverseMix.

## 1 Introduction

The OOD problem occurs when machine learning models encounter data that differs from the distribution of training data. In such scenarios, models may make incorrect predictions, leading to safety-critical issues in real-world applications, e.g., autonomous driving  and medical diagnosis . To ensure the reliability of the outputs of model, it is essential not only to achieve good performance on in-distribution (ID) samples, but also to detect potential OOD samples, thus avoiding making erroneous decisions in test. Therefore, OOD detection has become a critical challenge for the secure deployment of machine learning models [1; 12; 24; 29].

Several significant studies [19; 23; 25] focus on detecting OOD examples using only ID data in training. However, due to a lack of supervision information from unknown OOD data, it is difficult for these methods to achieve satisfactory performance in detecting OOD samples. Recent methods [20; 46; 6; 34] involve training model with easily available auxiliary outliers (e.g., data from the web or other datasets), with the hope that the detection ability can generalize to unknown OOD. However, as shown in Fig. 1(a)-(b), we experimentally find that while the use of outlier datasets can enhance performance in OOD detection, the generalization capabilities of these methods remain significantly limited. Specifically, there is a considerable risk of the model overfitting to the auxiliary outliers,consequently failing to identify OOD samples that deviate markedly. The above limitation motivates the following important yet under-explored question: _What are the theoretical principles underlying these methods that enable better utilization of outliers?_

In this work, we theoretically investigate this crucial question from the perspective of generalization ability . Specifically, we first conduct a theoretical analysis to demonstrate how the distribution shift between auxiliary outlier training set and test OOD data affects the generalization capability of OOD detector. Accordingly, a generalization bound is induced on the test-time OOD detection error of classifier, considering both empirical error and the error caused by the distribution shift between test OOD data and auxiliary outliers. Based on the theory, we deduce an intuitive conclusion that _a more diverse set of auxiliary outliers can reduce the distribution shift error and effectively lower the upper bound of the OOD detection error._ As shown in Figure 1(b)-(c), the model trained with a more diverse set of auxiliary outliers achieves better OOD detection. However, in practice, it is difficult and costly to collect sufficiently diverse outlier data. Therefore, a natural question arises - _how to guarantee the effective utilization of a fixed set of auxiliary outliers?_

Inspired from the theoretical principles, we propose a simple yet effective method called Diversity-induced Mixup (diverseMix) for OOD detection, which introduces and improves the mixup strategy to enhance the outlier diversity. Specifically, diverseMix employs semantic-level interpolation to generate mixed samples, creating new outliers that significantly deviate from their original counterparts. Given the risk that a random interpolation strategy (merely sampling from a predefined prior distribution) might produce mixed outliers that are unhelpful for the model (as the model can already detect them effectively), diverseMix dynamically adjusts its interpolation strategy based on original samples. This adjustment ensures that the generated outliers are novel and distinct from those previously encountered by the model, thereby enhancing diversity throughout the training process. As shown in Figure 1(b)-(d), diverseMix effectively boosts the diversity of auxiliary outliers, leading to improved OOD detection performance. The contributions of this paper are summarized as follows:

* We provide a theoretical analysis of the generalization error linked to methods trained with auxiliary outliers. By establishing an upper bound for expected error, we reveal the connection between auxiliary outlier diversity and the upper bound of OOD detection error. Our theoretical insights emphasize the importance of leveraging diverse auxiliary outliers to enhance the generalization capacity of the OOD detector.
* Constrained by the prohibitive cost of collecting outliers with sufficient diversity, we propose the Diversity-induced Mixup (diverseMix) for OOD detection, a simple yet effective strategy which is theoretically guaranteed to improve OOD detection performance.
* The proposed diverseMix achieves state-of-the-art OOD detection performance, outperforming existing methods on both standard and recent large-scale benchmarks. Remarkably, our method exhibits significant improvements over advanced methods, showing relative

Figure 1: OOD score for different training strategies. The ID data \(_{in}^{2}\) is sampled from three distinct Gaussian distributions, each representing a different class. The auxiliary outliers are sampled from a Gaussian mixture model away from the ID data, where the number of mixture components indicates the number of classes contained in auxiliary outliers. (a) The model trained without auxiliary outliers fails to detect OOD. (b) Incorporating a less diverse set of auxiliary outliers (10 classes) during training enables partial OOD detection, but overfits auxiliary outliers. (c) OOD detection is improved with a more diverse set of auxiliary outliers (1000 classes). (d) diverseMix enriches the diversity of outliers (10 classes) through creating significantly distinct mixed outliers.

performance improvements of \(24.4\%\) and \(43.8\%\) (in terms of FPR95) on the CIFAR-10 and CIFAR-100 datasets, respectively.

## 2 Related Works

We provide a brief review of prior research relevant to our work followed by a comparison.

**Auxiliary-Outlier-Free OOD Detection.** One early work by  pioneered the field of OOD detection, introducing a baseline method based on maximum softmax probability. However, it has since been established, as noted by , that this approach is not quite suitable for OOD detection. To address this, various methods have been developed that operate in the logit space to enhance OOD detection. These include ODIN , energy score [46; 28; 45], ReAct , logit normalization , Mahalanobis distance , and KNN-based scoring . However, post-hoc OOD detection methods that do not involve pre-training on a substantial dataset generally exhibit poorer performance compared to methods that leverage auxiliary datasets for model regularization .

**OOD Detection with Auxiliary Outliers.** Recent advancements in OOD detection have focused on incorporating easily available auxiliary outliers into the model regularization process. Outlier exposure  encourage models to predict uniform distributions for outliers, and Energy-based learning  widens the energy gap between ID and OOD distribution. However, performance heavily depends on outlier quality. ATOM , POEM , and DOS  enhance performance by improving the sampling strategy for auxiliary outliers. DivOE  and DAL  improve outlier quality in a learnable manner, either in the sample space or feature space, respectively. Additionally, DOE implicitly enhances outlier informativeness through model perturbation. Incorporating outliers during training often achieves superior performance, as shown in many other works [46; 39; 2; 48].

**Comparison with Existing Methods.** Several existing methods have explored the utilization of mixup in OOD detection. MixOE  and OpenMix  perform mixup between ID data and outliers, linearly representing the transition from ID to OOD and thus enhancing the model capturing the uncertainty from outliers. Meanwhile, MixOOD  employ mixup on ID data to generate outliers for training. Different from existing research which primarily focuses on refining mixup strategy or designing outlier regularization method, we place emphasis on the theoretical significance of auxiliary outlier diversity. Our approach advances this concept by enhancing outlier diversity via mixup based strategy, guaranteed by a robust theoretical framework. This focus on enhancing the diversity of auxiliary outliers distinguishes our research from prevailing studies in this area.

## 3 Theory: Diverse Auxiliary Outliers Boost OOD Detection

In this section, we lay the foundation for our analysis of OOD detection. We begin by introducing the key notations for OOD detection in Sec. 3.1. In Sec. 3.2, we establish a generalization bound which highlights the critical role for auxiliary outliers in influencing the generalization capacity of OOD detection methods. Finally, in Sec. 3.3, we demonstrate how a diverse set of auxiliary outliers effectively mitigate the distribution shift errors, consequently lowering the upper bound of error. For detailed proofs, please refer to _Appendix A_.

### Preliminaries

We consider the multi-class classification task and each sample in the training set \(_{id}=\{(x_{i},y_{i})\}_{i=1}^{N}\) is drawn i.i.d. from the joint distribution \(_{_{id}_{id}}\), where \(_{id}\) denotes the input space of ID data, and \(_{id}=\{1,2,,K\}\) represents the label space. OOD detection can be formulated as a binary classification problem to learn a hypothesis \(h\) from hypothesis space \(\{h:\{0,1\}\}\) such that \(h\) outputs \(1\) for any \(x_{id}\) and \(0\) for any \(x_{ood}\), where \(_{ood}=_{id}\) represent the input space of OOD data and \(\) represents the entire input space in the open-world setting. To address the challenge posed by the unknown and arbitrariness of OOD distribution \(_{_{ood}}\), we leverage an auxiliary dataset \(_{aux}\) drawn from the distribution \(_{_{aux}}\) to serve as partial OOD data, where \(_{aux}_{ood}\). Due to the diversity of real-world OOD data, auxiliary outliers cannot fully represent all OOD data, so \(_{_{aux}}_{_{ood}}\). We aim to train a model on data sampled from \(_{}}=k_{train}_{_{id}} +(1-k_{train})_{_{aux}}\) to obtain a reliable hypothesis \(h\) that can effectively generalize to the unknown test-time distribution \(_{}=k_{test}_{_{id}}+(1-k_{test} )_{_{ood}}\), where \(k_{train}\) and \(k_{test}\)determine the proportion of ID and OOD data used for training and testing, respectively. Note that \(k_{test}\) is unknown due to unpredictable test data distribution.

### Generalization Error Bound in OOD Detection

**Basic Setting.** We define an OOD label function which provides ground truth labels (OOD or ID) for inputs as \(f:\). The expectation that a hypothesis \(h\) disagrees with \(f\) with respect to a distribution \(\) is defined as:

\[_{}(h,f)=E_{x}[|h(x)-f(x)|].\] (1)

The set of ideal hypotheses on the training data distribution \(P_{}}\) and test-time data distribution \(P_{}\) is defined as:

\[^{*}_{aux}:h=_{h}_{P_{}}}(h,f),\;^{*}_{ood}:h=_{h} _{P_{}}(h,f),\] (2)

and we define \(h^{*}_{ood}\) and \(h^{*}_{aux}\) as the element in \(^{*}_{ood}\) and \(^{*}_{aux}\), respectively, which can be denoted as \(h^{*}_{ood}^{*}_{ood}\), \(h^{*}_{aux}^{*}_{aux}\). Considering that \(_{aux}_{ood}\), it follows that \(^{*}_{ood}^{*}_{aux}\)2, reflecting the reality that hypotheses perform well on real-world OOD data also perform well on auxiliary outliers, conditioning on that auxiliary outliers are a subset of real-world OOD data. The generalization error of an OOD detector \(h\) is defined as:

\[(h)=_{x_{}}(h,f).\] (3)

Now, we present our first main result regarding OOD detection (training with auxiliary outliers).

**Theorem 1**: _(Generalization Bound of OOD Detector). We let \(_{train}=_{id}_{aux}\), consisting of \(M\) samples. For any hypothesis \(h\) and \(0<<1\), with a probability of at least \(1-\), the following inequality holds:_

\[(h)_{x_{ {}}}(h,f)}_{}+_{ aux})}_{}+^{*}_{aux}}_{x_{ }}(h,h^{*}_{ood})}_{}+_{m}()}_{}+)}{2M}}+,\] (4)

where \(_{x_{}}}(h,f)\) is the empirical error. We define \((h,h^{*}_{aux})=|_{}(x)-_{}}(x)||h(x)-h^{*}_{aux}(x)|dx\) as the reducible error, where \(_{}\) and \(_{}}\) is the density function of \(_{}\) and \(_{}}\) respectively. \(_{h^{*}_{aux}}_{x_{}}(h,h^ {*}_{ood})\) is the distribution shift error, \(_{m}()\) represents the Rademacher complexity, and \(\) is some constant condition on the error related to ideal hypotheses.

Minimizing empirical risk optimizes the model \(h\) to \(h^{*}_{aux}\), leading to a reduction in the reducible error, which tends to zero. However, the inherent distribution shift error between auxiliary outliers and real-world OOD data remains constant and non-negligible. This limitation fundamentally restricts the generalization of OOD detection methods trained with auxiliary outliers. To address this limitation, we investigate the effect of outlier diversity on mitigating the distribution shift error.

### Generalization with Auxiliary OOD Diversification

In this paper, the diversity refers to semantic diversity, where a formal definition is given as follows.

**Definition 1**: _(Diversity of Outliers). We assume \(_{aux}\) can be divided into distinct semantic groups: \(_{aux}=^{y_{1}}^{y_{2}} ^{y_{m}}\), where each group \(^{y_{i}}\) contains data points with label \(y_{i}\). Considering a dataset \(_{div}\) sampled from the distribution \(_{_{div}}\), where \(_{div}_{ood}\) encompasses \(_{aux}\) and an additional group \(_{new}=^{y_{m+1}}^{y_{n}}\) with different semantic compared to \(_{aux}\), i.e., \(_{div}=_{aux}_{new}\), we define \(_{div}\) is more diverse than \(_{aux}\) in terms of the range of semantic classes covered._Suppose we could use this diverse auxiliary outliers dataset for training, the ideal hypotheses achieved by training with \(_{div}\) are denoted as:

\[^{*}_{div}:h=_{h}_{x_{ }_{div}}}(h,f),\] (5)

with \(_{}_{div}}=k_{train}_{_ {id}}+(1-k_{train})_{_{div}}\). Because \(_{aux}_{div}\) holds, the hypotheses performing well on \(_{_{div}}\) also perform well on \(_{_{aux}}\), giving rise to \(^{*}_{div}^{*}_{aux}\). Consequently, we have:

\[^{*}_{div}}{sup}_{x_{X}}(h,h^ {*}_{ood})^{*}_{aux}}{sup}_{x_{X}}(h,h^{*}_{ood}),\] (6)

which indicates that training with a more diverse set of auxiliary outliers can reduce the distribution shift error. Furthermore, effective training leads to sufficient small empirical error and reducible error, and the intrinsic complexity of the model remains constant. Consequently, a more diverse set of auxiliary outliers results in a lower generalization error bound. This theorem is formalized as:

**Theorem 2**: _(Diverse Outliers Enhance Generalization). Let \(((h))\) and \(((h_{div}))\) represent the upper bounds of the generalization error of detector training with vanilla auxiliary outliers \(_{aux}\) and diverse auxiliary outliers \(_{div}\), respectively. For any hypothesis \(h\) and \(h_{div}\) in \(\), and \(0<<1\), with a probability of at least \(1-\), the following inequality holds_

\[((h_{div}))((h)).\] (7)

**Remark.** Theorem 2 highlights that the diversity of the outlier set is a critical factor in reducing the upper bound of generalization error. However, despite the fundamental improvement in model generalization achieved by increasing the diversity of auxiliary outliers, collecting a more diverse set of auxiliary outliers is expensive, and the auxiliary outliers we can use are limited in practical scenarios, which hinders the application of outlier exposure based methods for OOD detection. This raises an intuitive question: _can we enhance the diversity of a fixed outlier set for better utilization?_

## 4 Method: Diversity-induced Mixup (diverseMix)

In this section, we show how diverseMix addresses the challenge of effective training when the outlier diversity is limited. We begin with a theoretical analysis demonstrating the effectiveness of mixup in enhancing outlier diversity to improve OOD detection performance, providing a reliable guarantee for our mixup-based method. Then, we introduce a simple yet effective framework implementing our method diverseMix to enhance OOD detection performance.

### Theoretical Insights: Semantic Interpolation Guarantees Enhanced Diversity of Outliers

Mixup  is a widely used machine learning technique to augment training data by creating synthetic samples, which has been extensively utilized in various studies[17; 7; 52]. It involves generating virtual training samples (referred to as mixed samples) through linear interpolations between data points and corresponding labels, given by:

\[= x_{i}+(1-)x_{j},= y_{i}+(1-) y_{j},\] (8)

where \((x_{i},y_{i})\) and \((x_{j},y_{j})\) are two samples drawn randomly from the empirical training distribution, and \(\) is usually sampled from a Beta distribution with parameter \(\) denoted as \(Beta(,)\). This technique assumes a linear relationship between semantics (labels) and features (in data), allowing us to create new mixed samples that deviate significantly from the semantics of the original ones by combining features from samples with distinct semantics. These new mixed samples are situated outside of the original data manifold . We summarize this assumption as follows:

**Assumption 1**: _(Semantic Change under Mixup). Let \(x_{i}\) and \(x_{j}\) be any two data points from input spaces \(^{y_{i}}\) and \(^{y_{j}}\), respectively, where \(y_{i}\) and \(y_{j}\) are corresponding semantic labels and \(y_{i} y_{j}\). If \(<<1-\), then there exists a positive value \(\) such that the mixed data point \(= x_{i}+(1-)x_{j}\) does not belong to either \(^{y_{i}}\) or \(^{y_{j}}\)._

This assumption suggests that we can enhance the diversity of outliers by generating new outliers with distinct semantics using mixup. Specifically, applying mixup to outliers in \(_{aux}\) results in some generated mixed outliers having different semantics, suggesting that they belong to novel (unknown or unnamed) semantic classes outside of \(_{aux}\). Consequently, these mixed outliers can be considered as samples from a broader region within the input space. As per Definition 1, the mixed outliers exhibit greater diversity than the original outliers. This lemma is formally presented as follows:

**Lemma 1**: _(Diversity Enhancement with Mixup). For a group of mixup transforms3\(\) acting on the input space \(_{aux}\) to generate an augmented input space \(_{aux}\), defined as \(_{aux}=\{|= x_{1}+(1-)x_{2} ;x_{1},x_{2}_{aux},\}\), the following relation holds:_

\[_{aux}_{aux}.\] (9)

Lemma 1 establishes that mixed outliers \(_{mix}\) exhibits greater diversity compared to \(_{aux}\), where \(_{mix}\) is drawn from distribution \(_{_{aux}}\). Consequently, according to Theorem 2, mixup outliers contribute to a reduction in generalization error. We can formalize this relationship as follows, and the detailed proofs can be found in _Appendix A_.

**Theorem 3**: _(Mixed Outlier Enhances Generalization). Let \(((h))\) and \(((h_{mix}))\) represent the upper bounds of the generalization error of detector training with vanilla auxiliary outliers \(_{aux}\) and mixed auxiliary outliers \(_{mix}\), respectively. For any hypothesis \(h\) and \(h_{mix}\) in \(\), and \(0<<1\), with a probability of at least \(1-\), we have_

\[((h_{mix}))((h)).\] (10)

Theorem 3 demonstrates that mixup enhances auxiliary outlier diversity, reducing the upper bound of generalization error in OOD detection, which provides a reliable guarantee of mixup's effectiveness in improving OOD detection. However, the vanilla mixup lacks flexibility, which may generate outliers that are not necessarily beneficial to the model. Next, we will provide an implementation of our method which dynamically adjusts the interpolation strategy in a data-adaptive manner.

### Implementation

Considering a classifier network \(\) and \(F(x,)\) denotes the logit outputs for input \(x\), our goal is to use the scoring function \(S(x,)\) to develop an OOD detector:

\[G(x)=\{S(x,)\}+ \{S(x,)<\},\] (11)

where \(\{\}\) is the indicator function, \(\) is the threshold, typically chosen to ensure that a significant proportion (e.g., 95%) of ID data is accurately identified. The training objective is given by:

\[_{}\ _{(x,y)_{id}}[_{ }(F(x,),y)]+_{},\] (12)

where \(_{}()\) is the cross entropy loss, \(_{}\) serves as a regularization term enabling model to learn from auxiliary outliers with low-confidence predictions, and \(\) controls the strength of regularization.

Our previous analysis showed that semantic interpolation can increase the diversity of outliers, thereby enhancing the model's OOD detection performance. However, the interpolation weights in vanilla mixup is randomly sampled from a preset prior distribution (e.g. beta distribution), which may result in generating mixed outliers that are not necessarily beneficial to the model. To efficiently increase the diversity of auxiliary outliers, we dynamically adjust the mixup strategy based on the original outliers, thereby generating novel mixed outliers which are more likely to be unfamiliar to the model.

During each training epoch, outliers are regularized, prompting the model \(\) to assign lower scores to previously encountered outliers. Consequently, outliers that achieve higher scores \(S(x,)\) are more likely to be novel or previously unseen outliers. We expect the generated outliers to be located in the vicinal space of the novel outliers that have not yet been encountered by the model. To achieve this, we adjust the prior distribution based on scores. Specifically, for outlier samples \(x_{i}\) and \(x_{j}\) randomly drawn from the empirical auxiliary outlier distribution, the mixed outliers are formulated as follows:

\[= x_{i}+(1-)x_{j},\ (_{i} ,_{j}),\] (13)

where \(_{i}\),\(_{j}\) adjusts the original Beta distribution according to \(x_{i}\) and \(x_{j}\), which is defined as follows:

\[_{i}=,)/T)}{_{k\{i,j\}}(S(x_{k}, )/T)},\] (14)with \(T\) representing the temperature parameter. This adaptive strategy assigns higher weights to the outliers that contain more information unknown to the current model, ensuring the generation of novel outliers, thereby increasing diversity throughout the training process. After constructing the mixed auxiliary outliers, they are used for the training objective (12). The whole pseudo code of the proposed method is shown in Alg. 1.

**Compatibility with different OOD regularization method.** DiverseMix is a general method that is suitable for a series of OOD regularization methods. One representative method is the energy-based method , which employs the following OOD regularization loss:

\[_{aux}=_{(x,y)_{id}}[((0,m_{in}-S(x; ))^{2}]+_{x_{aux}}[((0,S(x;)-m_{out}) )^{2}],\] (15)

where \(m_{in}\) and \(m_{out}\) are margin hyperparameters, and \(S(x;)=_{i=1}^{K}(F_{i}(x,))\) is the corresponding scoring function. More details for regularization methods are provided in _Appendix_ B.3.

``` Input: ID dataset \(_{}\), outlier dataset \(_{}\), batch size \(N\), distribution parameter \(\), temperature \(T\). Output: model parameters \(\). for each iterationdo for each mini-batchdo  Sample \(N\) ID data from \(_{}\) as \(_{}\) and \(N\) outliers from \(_{}\) as \(_{}\), respectively.  Evaluate the auxiliary outliers \(_{}\) using the current model \(\) to obtain the scores \(\).  Randomly shuffle \(_{}\) and the corresponding scores \(\) to generate \(^{}_{}\) and \(^{}\).  Generate prior adjustment strategies based on scores \(\) and \(^{}\) according to Eq. 14.  Sample the interpolation weight from the adjusted prior distribution and generate mixed outliers \(_{mix}\) according to Eq. 13.  Train the model \(\) using the objective function defined in Eq. 12. endfor endfor ```

**Algorithm 1**diverseMix for OOD Detection

## 5 Experiments

In this section, we outline our experimental setup and conduct experiments on common OOD detection benchmarks to answer the following questions: **Q1.** Effectiveness (I): Does our method outperform its counterparts in OOD detection? **Q2.** Effectiveness (II): Does our method retain its superior performance across various settings including large-scale benchmarks? **Q3.** Practicability (I): Does our method demonstrate effectiveness across different OOD regularization methods? **Q4.** Practicability (II): Does our method demonstrate effectiveness in low-quality auxiliary outlier datasets? **Q5.** Ablation study: (I) Does diverseMix truly offer a distinct advantage over other data augmentation methods? (II) What is the key factor contributing to performance improvement in our method? **Q6.** Reliability: Do the experimental results provide strong support for established theory?

### Experimental Setup

We briefly present the experimental setup here, including the experimental datasets and evaluation metrics. Further experimental details can be found in _Appendix_\(B\).

**Datasets.**\(\)**ID datasets.** Following the commonly used benchmark in OOD detection literature, we use _CIFAR-10_, _CIFAR-100_ and _ImageNet-200_ as ID datasets. \(\)**Auxiliary outlier datasets.** For CIFAR experiments, the downsampled version of ImageNet (_ImageNet-RC_) is employed as auxiliary outliers. For ImageNet-200 experiments, the remaining 800 categories from ImageNet-1k (_ImageNet-800_) serve as auxiliary outliers. \(\)**OOD test sets.** For CIFAR benchmark, we use diverse datasets including _SVHN_, _Textures_, _Places365_, _LSUN-crop_, _LSUN-resize_, and _iSUN_. For ImageNet benchmark, We use datasets such as _SSB-hard_, _NINCO_, _iNaturalist_, _Textures_ and _OpenImage-O_.

**Evaluation metrics.** Following common practice, we report: (1) OOD false positive rate at 95% true positive rate for ID samples (_FPR95_) , (2) the area under the receiver operating characteristic 

[MISSING_PAGE_FAIL:8]

representative of far-OOD data. While most OOD detection methods face difficulties in achieving satisfactory performance across both near-OOD and far-OOD, our method excels in detecting both types of OOD, significantly surpassing other methods in the average OOD detection performance.

**DiverseMix is a general method that achieves good performance across different OOD regularization methods (Q3).** To investigate the generality of diverseMix across different OOD regularization methods, we replace the original energy loss with the _K+1_ loss and the OE loss. The experimental results presented in Figure 2 reveal that diverseMix achieves consistent effectiveness regardless of the OOD regularization method employed. These findings not only suggest the versatility of our method but also provide substantial empirical evidence supporting our theoretical framework.

**DiverseMix remains effective even when the auxiliary outlier data is of low quality (Q4).** In Figure 2, the quality of auxiliary outliers used for training is decreased by gradually decreasing their quantity or their diversity. Our method diverseMix consistently outperforms previous methods by enhancing the diversity of auxiliary outliers across different dataset sizes and diversity levels. This suggests that diverseMix remains effective even when the auxiliary outliers are of low quality.

**Sample adaptive semantic interpolation contributing to unique advantages of diverseMix (Q5).** We compared diverseMix with other data augmentation methods. As shown in Table 3(a), diverseMix demonstrates superior performance for OOD detection over other data augmentation methods that preserve the semantics of outliers. Additionally, the ablation study in Table 3(b) compares diverseMix with different mixup strategies. DiverseMix outperforms both vanilla mixup and cutmix by adaptively adjusting its interpolation strategy based on the given outliers, thereby efficiently generating novel mixed outlier samples to enhance diversity. The advantages of diverseMix lie in 1) enhancing the diversity of outliers at the semantic level, and 2) efficiently boosting diversity by adaptively adjusting its strategy for the given outlier samples. For detailed comparisons, please see _Appendix B.6_.

**Our theory effectively demonstrates that the diversity of auxiliary outliers is a key factor to ensure OOD detection performance (Q6).** In Figure 2, when maintaining the diversity relatively constant and changing the quantity of data, the performance of different methods remains relatively stable. However, when the number of outliers is fixed and the diversity of the outliers dataset is reduced, there is a significant decrease in performance across all methods. This suggests that diversity is a key quality factor for the auxiliary outliers, providing substantial empirical support for our theory.

**DiverseMix has the potential for application across a wide range of task domains.** Our theory is not rely on any assumptions specific to the task domain. Given the successful implementation of

Table 3: **Ablation study.** Performance are averaged (%) over six OOD test datasets from Section 5.1. The best results are in **bold**. More details about the comparison methods are provided in _Appendix B_ (a) different data augmentation method.

(b) different semantic interpolation strategy.

Figure 2: Comparison of OOD detection performance on CIFAR-100 with decreased quality of auxiliary outlier datasets (a) With constant diversity of auxiliary outliers (1000 categories), the dataset size is decreased. The x-axis represents the percentage of the original outlier dataset’s size used for training. (b) With fixed dataset size (10% of auxiliary outliers), the diversity of outliers is decreased, with the x-axis displaying the number of categories. See _Appendix B.4_ for more details.

mixup across different fields [15; 54], diverseMix also has the potential for application in multiple task domains beyond just computer vision tasks. We have investigated the application of diverseMix in the NLP domain through experiments. For additional details, please see _Appendix C.2_.

## 6 Conclusions and Future Work

In this study, we demonstrate that the performance of OOD detection methods is hindered by the distribution shift between unknown test OOD data and auxiliary outliers. Through rigorous theoretical analysis, we demonstrate that enhancing the diversity of auxiliary outliers can effectively mitigate this problem. Constrained by limited access to auxiliary outliers and the high cost of data collection, we introduce diverseMix, an effective method that enhances the diversity of auxiliary outliers and significantly improves model performance. The effectiveness of diverseMix is supported by both theoretical analysis and empirical evidence. Furthermore, our theory enables future research to design new OOD detection method. We hope that our research can bring more attention to the diversity in OOD detection.

## 7 Acknowledgements

This work was supported by the National Natural Science Foundation of China (Grant No.62376193), the National Science Fund for Distinguished Young Scholars (Grant No.61925602) and the H. Fu's Agency for Science, Technology and Research (A*STAR) Central Research Fund ("Robust and Trustworthy AI system for Multi-modality Healthcare"). The authors also appreciate the suggestions from NeurIPS anonymous peer reviewers.