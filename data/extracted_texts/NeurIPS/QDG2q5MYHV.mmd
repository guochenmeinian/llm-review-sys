# A Gradient Accumulation Method for Dense Retriever under Memory Constraint

Jaehee Kim\({}^{1}\)   Yukyung Lee\({}^{2}\)   Pilsung Kang\({}^{1}\)

\({}^{1}\)Seoul National University  \({}^{2}\)Boston University

{jaehee_kim, pilsung_kang}@snu.ac.kr

ylee5@bu.edu

indicates corresponding author

###### Abstract

InfoNCE loss is commonly used to train dense retriever in information retrieval tasks. It is well known that a large batch is essential to stable and effective training with InfoNCE loss, which requires significant hardware resources. Due to the dependency of large batch, dense retriever has bottleneck of application and research. Recently, memory reduction methods have been broadly adopted to resolve the hardware bottleneck by decomposing forward and backward or using a memory bank. However, current methods still suffer from slow and unstable training. To address these issues, we propose Contrastive Accumulation (ContAccum), a stable and efficient memory reduction method for dense retriever trains that uses a dual memory bank structure to leverage previously generated query and passage representations. Experiments on widely used five information retrieval datasets indicate that ContAccum can surpass not only existing memory reduction methods but also high-resource scenario. Moreover, theoretical analysis and experimental results confirm that ContAccum provides more stable dual-encoder training than current memory bank utilization methods.

## 1 Introduction

Dense retriever aims to retrieve relevant passages from a database in response to user queries with neural networks . Karpukhin et al.  and Lee et al.  introduced the in-batch negative sampling for training dense retriever with InfoNCE loss , where relevant passages from other queries in the same batch are utilized as negative passages. This negative sampling strategy has been widely adopted in subsequent dense retriever studies, including supervised retriever , retriever pre-training , phrase retriever , and generative retriever . Training dense retriever with InfoNCE loss drives the representations of queries and relevant passages closer and pushes the representations of unrelated passages apart, which can be seen as a form of metric learning .

Many dense retriever methodologies utilize large batch to incorporate more negative samples . Theoretically, it has been demonstrated that more negative samples in InfoNCE loss lead to a tighter lower bound on mutual information between query and passage . Empirical studies have shown that the dense retriever performs better with large batch . However, training with large batches requires high-resource, posing a challenge for dense retriever research and applications.

A line of research has focused on overcoming these limitations by approximating the effects of large batch sizes. Gradient Accumulation (GradAccum), a common method for approximating large batch, reduces memory usage by splitting the large batch into smaller batches. However, GradAccum has limitations in the context of InfoNCE loss because it reduces negative samples per query by the smallerbatch . To overcome the limitation of GradAccum, Gao et al.  proposed the Gradient Cache (GradCache), which approximates large batch by decomposing the backpropagation process and adapts additional forwarding process for calculating gradients. However, GradCache has limitations, including significant additional training time due to computational overhead and the inability to surpass high-resource scenario where accelerators are sufficient to train large batch. Additionally, pre-batch negatives  caches passage representations from previous steps to secure additional negative samples, but it also shows unstable train and marginal performance gain.

In this study, we propose **Contrastive Accumulation (ContAccum)**, which demonstrates high performance and stable training under memory constraints. ContAccum leverages previously generated query and passage representations through a memory bank, enabling the use of more negative samples. Our analysis of the gradients reveals that utilizing a memory bank for both query and passage leads to stable training. The specific contributions of this study are as follows:

* We propose ContAccum, a method utilizing a dual memory bank strategy that can outperform not only existing memory reduction methods but also high-resource scenario in low-resource setting.
* We show that our method is time efficient, reducing the training time compared to existing memory reduction methods.
* We demonstrate the cause of training instability in existing memory bank utilization methods through mathematical analysis and experiments, showing that the dual memory bank strategy stabilizes training.

## 2 Related works

### Memory reduction in information retrieval

GradAccum is the most common method to address memory reduction problem. By using GradAccum, gradients of the total batch can be stored by sequentially processing local batches through forward and backward passes, even when the total batch cannot be processed at once. However, as shown in Figure 1 (b), GradAccum is not a proper memory reduction method for the in-batch negatives, as it uses fewer negative samples than the total batch. We will discuss the limitation of GradAccum for contrastive learning in detail in subsection 3.1.

GradCache reduces memory usage in contrastive learning by decomposing the backpropagation process. Specifically, as shown in Figure 1 (a), it calculates the loss without storing activations during the forward pass using the total batch. Then, it computes and stores the gradient from the loss to the representations. Next, it performs additional forward passes for the local batch to store activations and sequentially calculates gradients from each representation to the model weights. This allows GradCache to use the same number of negative samples as the total batch, approximating the performance of the total batch. However, GradCache cannot surpass the performance of high-resource

Figure 1: **Illustrations of ContAccum and Comparative Methods. The illustrations show a total batch size (\(N_{}\)) of 4, a local batch size (\(N_{}\)) of 2, and a memory bank size (\(N_{}\)) of 4. (a) GradCache uses \(N_{}-1\) negative passages. (b) GradAccum uses \(N_{}-1\) negative passages. (c) ContAccum leverages \(N_{}+N_{}-1\) negative samples, more than \(N_{}-1\).**

scenario because it uses the same number of negative samples. Also, GradCache requires a significant amount of time due to the complex forward and backward processes.

### Memory bank

The memory bank structure for metric learning was initially proposed for the vision domain, where it stores representations generated by the encoder in previous batches [40; 39]. Combined with the NCE loss , memory bank structures have been widely used to train uni-encoder vision models [11; 3; 38]. However, directly adapting this approach to information retrieval tasks, where a dual-encoder structure is commonly used, is challenging. This is due to several factors: In multi-modal settings, Li et al. [22; 21] have employed momentum encoders for both image and text modalities to generate cached representations. However, these approaches do not directly address the asymmetric nature of information retrieval, where the goal is to retrieve relevant passages for a given query rather than retrieving relevant queries for a given passage.

In the information retrieval task, Izacard et al.  proposed caching representations generated by a momentum encoder , but they only consider the uni-encoder setting. Lee et al.  introduced pre-batch negatives that extend the number of negative samples by caching passage representations with a memory bank in a dual-encoder setting. However, pre-batch negatives was applied only in the final few epochs of the training process due to the rapid changes in encoder representations early in training, which can cause instability when using a memory bank [38; 37].

In summary, existing dense retrievers depend on in-batch negative sampling, necessitating large batch sizes and costly hardware settings. While memory reduction methods have been studied to address this, they often result in slower training or unstable training. Therefore, we propose ContAccum, a memory reduction method designed to ensure fast and stable training of dense retrievers.

## 3 Proposed Method

### Preliminary: InfoNCE loss with GradAccum

Before introducing our method, we first examine GradAccum with InfoNCE loss. Karpukhin et al.  proposed training method for dense retriever using InfoNCE loss. With a batch size \(N\), dense retrievers are trained by minimizing the negative log-likelihood over all query representations (\(\)) and passage representations. Specifically, they utilized in-batch negative sampling (\(\)) in the same batch for efficiency, encoded by the query and passage encoders as:

\[(S)=-_{i}^{N}/)}{_{ j}^{N}(S_{(i,j)}/)},S=(^{})^{N N}\] (1)

The in-batch negative sampling efficiently obtains \(N-1\) negative passages per query from relevant passages of other queries, as shown in Equation 1. Consequently, the number of negative passages increases with a larger batch size. Due to this characteristic of in-batch negative sampling, dense retriever is trained using extremely large batch size, ranging from 128 to 8192 [16; 12; 28; 5; 29]. However, the need to process all data in memory simultaneously requires multiple high-cost accelerators, ranging from 8 [16; 28] to 32 . This creates a hardware bottleneck that constrains various research and applications.

In low-resource setting, GradAccum is employed to train models with the total batch size (\(N_{}\)), which cannot be fitted in the limited memory. GradAccum decomposes the total batch into accumulation steps, \(K\), and processes the local batch, \(N_{}=N_{}/K\), through forward and backpropagation K times to calculate gradients. The process of computing InfoNCE Loss with GradAccum is as follows.

First, the query, \(q\), and document, \(p\), are encoded by the query encoder, \(f_{}^{t}\), and passage encoder, \(g_{}^{t}\), at training step \(t\) respectively:

\[^{t}=f_{}^{t}(q)^{d_{}},^{t}=g_{}^{t}(p)^{d_{}}\] (2)

where \(d_{}\) denotes the dimension of query and passage representation. The query encoder, \(f\), and passage encoder, \(g\), are parameterized by \(\) and \(\) respectively. The query and passage representationswithin the same local batch at the \(k\)-th accumulation step are given as follows:

\[_{k}^{t}=\{_{1}^{t},,_{N_{}}^{t} \}^{N_{} d_{}},_{k}^{ t}=\{_{1}^{t},,_{N_{}}^{t}\}^{N_{ } d_{}}\] (3)

Using Equation 1, the loss for the \(k\)-th accumulation step is calculated, and the loss for the total batch used for one weight update is obtained as shown in Equation 4:

\[=_{k=1}^{K}(S_{k}),S_{k}=(_{k}^{t}(_{k}^{t})^{}) ^{N_{} N_{}}\] (4)

In Equation 4, the number of negative passages in each accumulation step is \(N_{}-1\), which is fewer than the number of negative passages when using the total batch, \(N_{}-1\). This reduction in the number of negative samples results from that GradAccum use \(N_{}\) passages in a single forward pass. Consequently, GradAccum cannot maintain the number of negative passages in low-resource setting, while the total amount of data used for weight updates is the same as the total batch.

### Contaccum

To address the issue of fewer negative passages being used with GradAccum, we propose ContAccum, a method that utilizes a dual memory bank structure to cache representations for both queries and passages. The query and passage memory banks (\(M_{},M_{}\)) are implemented as First-In-First-Out queues storing \(N_{}^{}\) and \(N_{}^{}\) representations respectively. For example, as shown in Figure 2, the oldest representations in the memory bank (\(_{1}^{}\), \(_{1}^{}\)) are replaced with the newly-generated ones (\(_{1}^{}\), \(_{1}^{}\)). Memory bank strategy is computationally efficient as it reuses generated representations from previous iterations . Unlike Lee et al. , which only utilized a passage memory bank \(M_{}\), ContAccum employs a dual memory bank by also utilizing a query memory bank \(M_{}\).

ContAccum constructs the similarity matrix using both current and stored representations from the dual memory bank as illustrated in Figure 2. It is equivalent to modifying \(S_{k}\) in Equation 4 as:

\[ =_{k}^{t}(M_{})^{(N _{}+N_{}^{}) d_{}}\] (5) \[ =_{k}^{t}(M_{})^{(N _{}+N_{}^{}) d_{}}\] (6) \[S_{k} =(^{})\] (7)

The backpropagation process using InfoNCE loss proceeds in the same manner as in Equation 4. However, since the representations in the memory bank do not have stored activations by the stop-gradient operation(\(()\)), the gradients are not back-propagated through the representations in the memory bank.

The number of negative passages in ContAccum is \(N_{}+N_{}^{}-1\), which is greater than GradAccum. Furthermore, if \(N_{}^{}>N_{}(K-1)\), ContAccum can utilize more negative passages than the total batch, enabling superior performance in low-resource setting compared to high-resource scenario.

Figure 2: **Training process of ContAccum at each accumulation step.** The illustration shows a total batch size (\(N_{}\)) of 4, an accumulation step (\(K\)) of 2, and a memory bank size (\(N_{}\)) of 4. The dual memory bank caches both query and passage representations. New representations are enqueued, and the oldest are dequeued at each step, maintaining the similarity matrix (\(S_{k}\)) size at (\(N_{}+N_{},N_{}+N_{}\)).

### Gradient analysis with dual memory bank

We analyze the InfoNCE loss backpropagation process in information retrieval tasks, extending the analysis by Gao et al.  to consider using the memory bank. In the partial derivatives of the loss function with respect to the two encoders, \(_{}(S_{k})=_{_{i} Q_{k}^{i}}(S_{k})}{_{}}_{}}{},_{}(S_{k})=_{_{i} P_{k}^{I}}(S_{k})}{ _{}}_{}}{}\), the partial derivative terms for each representation are given by:

\[(S_{k})}{_{}}=-}+N_{}^{q}}(_{l}-_{j}^{N_{}}S_{k(l,j)}_{j})\] (8)

\[(S_{k})}{_{l}}=-}+N_{}^{q}}(_{l}-_{i}^{N_{}+N_{ }^{q}}S_{k(i,l)}_{j}),\] (9)

where \(S_{k(i,j)}\) denotes the similarity between \(i\)-th query and \(j\)-th passage in the similarity matrix \(S_{k}\) of the \(k\)-th accumulation step. Detailed differentiation steps are provided in Appendix 6.

Equations 8 and 9 have a similar structure, indicating that the gradients of the two encoders are influenced by the representations generated by the opposite encoder. The difference lies in the summation targets, which are determined by the size of the memory banks. The gradient calculation for the query encoder uses \(N_{}+N_{}^{}\) passage representations, while the passage encoder uses \(N_{}+N_{}^{}\) query representations.

Pre-batch negatives only leverages the passage memory bank where \(N_{}^{}>N_{}^{}=0\). The tendency where \(||_{}(S_{k})||_{2}<||_{}(S_{k} )||_{2}\) is caused by the difference in the number of representations used for the gradient calculations of the two encoders. In dual-encoder training, if the gradient norms of the two encoders remain imbalanced, the encoder with the larger gradient norm converges faster, making balanced training challenging [4; 33]. Therefore, the unstable training with a memory bank is caused not only by rapid changes in encoder representations [37; 38], but also by the difference in the gradient norms between the dual-encoders. We refer to this problem as the _gradient norm imbalance problem_.

The _gradient norm imbalance problem_ can be resolved by using memory banks of equal size for queries and passages, \(N_{}^{}=N_{}^{}=N_{}\). This ensures that the gradient norms of the two encoders remain similar and stabilizes the training process. Further analysis is provided in Sections 5.2 and 5.5.

## 4 Experimental setups

**Resources**. All experiments were conducted on a single A100 80GB GPU. For high-resource scenario, we considered situations where 80GB of memory is available. For low-resource settings, we assumed available memory as widely used commercial GPUs: 11GB (GTX-1080Ti), 24GB (RTX-3080Ti, RTX-4090Ti). To ensure strict experimental conditions, we used a function from the PyTorch  to limit the available memory.2 Unless otherwise stated, all experiments assumed low resource setting where only 11GB memory is available.

**Datasets and evaluation metrics**. The datasets used for the experiments were Natural Questions (NQ) , TriviaQA , Curated TREC (TREC) , and Web Questions (WebQ)  processed by DPR and MS Marco . For Natural Questions, TriviaQA, Curated TREC, and Web Questions, we used the preprocessed data provided by DPR , which includes hard negative samples, positive passages, and answer annotations. Only queries with both positive and hard negative passages were used for training. For MS Marco, we utilized the preprocessed data from BEIR  and filtered BM25  hard negatives using cross-encoder scores from the sentence-transformers library . Specifically, we considered passages as hard negatives if their cross-encoder scores were at least 3 points higher than the positive passages' scores, following the preprocessing pipeline provided by sentence-transformers.

For evaluation metrics, Top@k was used for Natural Questions, TriviaQA, TREC, and WebQ following DPR. Also, we evaluate MS Marco using NDCG@K and Recall@K, widely used metricsfor dense retriever. NQ and TriviaQA were evaluated using test sets, while TREC, WebQ, and MS Marco were evaluated using dev sets. Additionally, the entire document set was used for evaluation.

**Implementation details**. The experimental code was adapted from nano-DPR3, which provides a simplified training and evaluation pipeline for DPR. All experiments were conducted using the BERT4 model. To maintain consistency with DPR's experimental setup, NQ and TREC were trained for 40 epochs, and TriviaQA and WebQ for 100 epochs. For MS Marco, performance saturated at 10 epochs, so it was trained for 10 epochs. Other training settings were also kept consistent with DPR. Detailed settings are provided in Appendix 6.

The optimal memory bank size, \(N_{}\), was selected using evaluation data with candidates , resulting in 2,048 for NQ and 512 for TriviaQA. For MS Marco, WebQ, and TREC, due to the lack of evaluation data, \(N_{}\) were set based on dataset size: 1,024 for MS Marco, and 128 for WebQ and TREC.

**Baselines**. We established three baselines for each scenario, and all methods were trained with hard negatives. First, we reported the performance of DPR with the maximum batch size possible for each scenario. Further, we reported the performance of GradAccum with the total batch size of \(N_{}=128\). The local batch size \(N_{}\) varied by the scenario, with \(K=N_{}/N_{}\). We also conducted experiments with GradCache , known for approximating total batch performance, using the same \(N_{}\) for single forwarding.

## 5 Experimental results

### Performance across different resource constraints

**ContAccum outperforms the high-resource DPR even under low-resource constraints.** Table 1 compares the performance of ContAccumwith baseline methods under low-resource setting. Notably, ContAccum, with only 11GB of memory, surpasses the performance of DPR in the high-resource setting (80GB). This demonstrates that ContAccumis not only memory-efficient but also achieves superior performance compared to the baseline.

**ContAccum maintains consistent performance across different memory constraints.** ContAccum exhibits robust performance regardless of the memory constraint level (11GB or 24GB), with only minor variations between the two settings. In contrast, the performance of both DPR and GradAccum improves as the available memory increases from 11GB to 24GB. This suggests

    &  **Batch Size** \\  } &  &  &  &  &  \\   & &  &  &  &  &  &  \\   & &  & 20 & 100 & 20 & 100 & 20 & 100 & 20 & 100 & 20 & 100 \\   \\  DPR & 8/1/ 8 & 27.9 & 23.5 & 8.3 & 15.2 & 72.2 & 81.5 & 73.7 & 81.9 & 72.5 & 81.4 & 80.8 & 88.9 \\ GradAccum & 8/16/128 & 31.1 & 26.4 & 10.1 & 18.1 & 77.1 & 84.7 & 78.4 & 84.8 & 74.6 & 81.9 & 79.7 & 89.9 \\ GradCache & 8/16/128 & 34.9 & 30.6 & 12.8\({}^{*}\) & 22.4\({}^{*}\) & 79.5\({}^{*}\) & 85.9 & 79.4 & 85.1 & 75.1 & **82.3** & 81.6 & 90.2 \\ ContAccount (ours) & 8/16/128 & **39.1\({}^{*}\)** & **32.9\({}^{*}\)** & **14.4\({}^{*}\)** & **23.8\({}^{*}\)** & **80.1\({}^{*}\)** & **86.5\({}^{*}\)** & **79.8\({}^{*}\)** & **85.3\({}^{*}\)** & **75.4\({}^{*}\)** & 82.1 & **83.3\({}^{*}\)** & **90.5** \\   \\  DPR & 32/1/ 32 & 33.1 & 28.6 & 11.5 & 19.6 & 77.0 & 84.8 & 77.5 & 84.2 & 74.8\({}^{*}\) & 82.1 & **82.7\({}^{*}\)** & **89.8** \\ GradAccum & 32/4/128 & 33.1 & 28.2 & 11.8 & 20.0 & 77.9 & 85.4 & **80.0\({}^{*}\)** & 84.8 & 74.3 & 81.9 & 79.3 & 89.6 \\ GradCache & 32/4/128 & 35.5\({}^{*}\) & 31.0\({}^{*}\) & 12.8 & 22.1 & 79.6\({}^{*}\) & 86.0 & 79.7\({}^{*}\) & **85.1** & 74.7 & 81.8 & 81.3 & 89.6 \\ ContAccount (ours) & 32/4/128 & **39.0\({}^{*}\)** & **32.9\({}^{*}\)** & **14.6\({}^{*}\)** & **24.1\({}^{*}\)** & **80.6\({}^{*}\)** & **86.3\({}^{*}\)** & 79.4 & **85.1** & **75.0\({}^{*}\)** & **82.5\({}^{*}\)** & 81.8 & 89.5 \\   \\  DPR (implemented) & 128/11/28 & 35.1 & 30.8 & 12.7 & 22.2 & 79.4 & 86.1 & 79.5 & 85.1 & 74.7 & 82.4 & 82.0 & 90.5 \\ DPR (original) & 128/11/28 & - & - & - & - & 78.4 & 85.4 & 79.4 & 85.0 & 73.2 & 81.4 & 79.8 & 89.1 \\   

Table 1: Performance of different methods in low-resource settings (11GB, 24GB) and high-resource (80GB) setting. In the high-resource setting, the score of the original DPR  paper (original) and the reproduced implementation (implemented) are listed. The best score for each training environment is bolded, and scores surpassing the high-resource setting are marked with \({}^{*}\). \(N_{l}\) denotes the local batch size \(N_{}\), \(N_{t}\) denotes the total batch size \(N_{}\), and \(K\) represents the accumulation step.

that the performance gains of ContAccum are not significantly affected by the severity of memory limitations.

**The effectiveness of ContAccum is amplified under more severe memory constraints.** While ContAccum consistently outperforms the baseline methods in both 11GB and 24GB scenarios, the performance gap between ContAccum and the baselines is more substantial in the 11GB setting. This indicates that the advantages of ContAccum are particularly evident when memory constraints are stringent, emphasizing its effectiveness in low-resource setting. The strong performance of ContAccum can be attributed to its dual memory bank strategy, which allows it to utilize more negative samples than GradCache, even in low-resource settings. Furthermore, ContAccum outperforms the high-resource setting in 18 out of 24 metrics, improving up to 4.9 points. In contrast, GradCache only surpasses the high-resource setting in 8 metrics, with marginal improvements likely due to randomness. These results demonstrate the fundamental advantage of ContAccum in achieving superior performance compared to both the baselines and the high-resource setting.

### Influence of each components in ContAccum

Table 2 shows the influence of key components in ContAccum by removing each component with NQ. We also reported experiments that excluded hard negatives during training to observe the tendency. The most significant performance drop occurred when the query memory bank \(M_{q}\) was removed, indicating its crucial role in ContAccum. The other components of ContAccum also contributed to the overall performance, with consistent trends regardless of using hard negatives.

**Passage memory bank alone degrades performance due to gradient norm imbalance.** Specifically, using only the passage memory bank (w/o. \(M_{q}\)), similar to the pre-batch negatives, led to an 8-point performance drop in Top@20 compared to ContAccum. This decrease can be attributed to the gradient norm imbalance problem highlighted in Section 3.3. Section 5.5 further analyzes this issue.

**GradAccum and past encoder representations are crucial for stable training and performance.** Moreover, when GradAccum was not applied (w/o. GradAccum), a 2.1-point performance decline was observed in Top@20, highlighting the importance of involving more data in gradient calculations for stable training in ContAccum. Additionally, a 2.3-point performance decrease was noted when representations generated by past encoders were not used (w/o. Past Enc.). This finding confirms that past encoder representations contribute to training, as suggested by previous studies [37; 38; 19]. However, unlike pre-batch negatives, query memory bank \(M_{q}\) demonstrates that the greatest performance improvement is achieved by employing a dual memory bank, which leverages representations generated by past query and passage encoders.

### Memory bank size analysis

Figure 3 indicates the experimental results on the NQ dataset, demonstrating the impact of memory bank size \(N_{}\) and accumulation steps \(K\) on ContAccum's performance in a low-resource setting with a local batch size of 8. As the memory bank size \(N_{}\)increases, more negative passages are utilized in training, and as the accumulation steps increase, more data is considered in each model update. The performance of DPR in both low-resource and high-resource scenarios(\(N_{}=1\)

    &  \\ 
**Method** & **Top@20** & **Method** & **Top@20** \\  DPR (BSZ=8) & 70.9 & DPR (BSZ=8) & 63.7 \\ DPR (BSZ=128) & 78.4 & DPR (BSZ=128) & 74.3 \\ 
**ContAccum (ours)** & **78.8** & **ContAccum (ours)** & **76.3** \\ w/o. \(M_{q}\) & 70.8 & w/o. \(M_{q}\) & 72.3 \\ w/o. Past Enc. & 76.5 & w/o. Past Enc. & 73.4 \\ w/o. \(M_{q}\)/Past Enc. & 67.8 & w/o. \(M_{q}\)/Past Enc. & 73.9 \\ w/o. GradAccum & 76.7 & w/o. GradAccum & 74.1 \\   

Table 2: Results of removing the components of ContAccum. The DPR performance in low-resource (BSZ=8) and high-resource (BSZ=128) settings are shown as baselines. The best-performing method is highlighted in bold.

\(32,64,128\)) is also included for comparison. Note that gradient accumulation is not used when the total batch size is 8 and only the dual memory bank is employed.

**ContAccum consistently outperforms GradAccum and DPR regardless of the size of memory bank and accumulation step.** The results show that increasing the memory bank size improves performance even when GradAccum is not used. This indicates that even without gradient accumulation, utilizing representations from the memory bank to construct a larger similarity matrix enhances performance. This trend remains consistent as the accumulation step increases. Moreover, ContAccum consistently outperforms GradAccum in all \(N_{}\) settings. Remarkably, ContAccum with \(N_{}=8,N_{}=64\), and \(N_{}=128\) surpasses the performance of DPR in a high-resource setting (\(N_{}=N_{}=128\)). The performance improvement of ContAccum converges as the accumulation step and memory bank size increase, demonstrating that ContAccum can robustly enhance performance regardless of memory bank size and accumulation steps.

### Train speed

In this subsection, we compare the training speed of ContAccum with baseline methods. Figure 4 shows the results of experiments comparing the speed of a single training iteration (1 weight update) as the accumulation step increases in a low-resource settings with 11GB of available memory. Unlike the high-resource setting, where the total batch can be processed through forward and backward pass at once, the train speed slow down in low-resource settings due to various computations and storing gradients.

**ContAccum achieves faster iteration times than GradCache, even with large memory banks.** As shown in Figure 4, ContAccum performs single iterations faster than GradCache in all total batch size. Notably, when \(N_{}=512\), GradCache is 93% slower than GradAccum, while ContAccum only takes 26% more time, even with the largest memory bank size of \(N_{}=8192\). This indicates that ContAccum completes iterations 34% faster than GradCache. The significant additional time for computing one iteration in GradCache is due to the overhead of calculating and storing gradients of representations, as well as the repetitive forward and backpropagation. In contrast, ContAccum incurs a relatively minor loss of speed compared to GradAccum due to the additional computations involved in storing and retrieving representations from the memory bank and calculating the enlarged similarity matrix. While pre-batch negatives  shows similar computational efficiency to our method, it degrades the performance as demonstrated in Table 2.

### Gradient norm ratio

We conducted experiments comparing the gradient norms of the query and passage encoders to investigate whether the presence of a query memory bank \(M_{q}\) affects the _gradient norm imbalance problem_, as discussed in Section 3.3. The results are presented in Figure 5. This experiment defines the ratio of gradient norms between the two encoders as \(=||_{}||_{2}/||||_{2}\). Wemeasured GradNormRatio during the training of the NQ.5 If the two encoders have similar gradient norms during training, GradNormRatio should be close to 1. If the passage encoder (\(g_{}\)) has a larger gradient norm, GradNormRatio will be greater than 1.

**Dual memory bank helps maintain gradient norm balance.** The experimental results show that when the query memory bank \(M_{q}\) is not used, GradNormRatio consistently increases. In contrast, ContAccum, which utilizes a dual memory bank (\(M_{q},M_{p}\)), maintains a GradNormRatio close to 1, similar to DPR.

This indicates that the pre-batch negatives exhibit _gradient norm imbalance problem_. It is because pre-batch negatives only use passage memory bank, leading to an imbalance in the number of query and passage representations used in gradient calculations, as discussed in 3.3. The _gradient norm imbalance problem_ consistently occurred even when the timing of omitting the query memory bank \(M_{q}\) is varied during training, as shown in Figure 6.

The _gradient norm imbalance problem_ observed during the actual training process becomes increasingly severe, causing the gradient norm of the passage encoder to be up to 30 times larger than the query encoder. As noted by Senushkin et al.  and Chen et al. , such extreme differences in gradient norms between the two models negatively impact performance. The significant performance drop observed in 5.2 when the query memory bank \(M_{q}\) is not used can be attributed to the _gradient norm imbalance problem_.

## 6 Conclusion

In this work, we proposed ContAccum, a novel memory reduction methodology for training dual-encoders with InfoNCE Loss in low-resource settings. By employing a dual memory bank structure, ContAccum achieves stable training and outperforms high-resource baselines, as demonstrated through extensive experiments on five information retrieval datasets. Our mathematical analysis of the dual-encoder training process underscores the importance of balanced gradient norms, which is effectively addressed by the dual memory bank approach. Furthermore, various ablation experiments showed that the accumulation step and memory bank size significantly contribute to performance improvement.

**Limitations**. While ContAccum reduces computational costs and stabilizes training, this study is limited by its focus on supervised fine-tuning. Recently, many studies have proposed a pre-training stage for dense retriever [12; 7; 8; 29]. It remains to be investigated whether the _gradient norm imbalance problem_ arises during the pre-training stage and whether ContAccum can alleviate it. Additionally, ContAccum still relies on the softmax operation, which incurs high computational costs. Reducing this reliance on the softmax operation could lead to more efficient training and broader application of the dense retriever.

**Broader impacts**. ContAccum is designed to train dense retrievers efficiently, which allows it to be applied to various knowledge-intensive systems with limited resources. Examples of such applications include search engines, retrieval-augmented generation, and fact verification on local machines. However, we strongly discourage the use of ContAccum in high-risk domains such as medical and legal fields, where the retrieval of incorrect information could have a serious impact.

Figure 5: Analysis of GradNormRatio throughout the training process on the NQ dataset.

**Future works**. In future work, we plan to extend ContAccum to the pre-training phase with a uni-encoder structure to assess its broader applicability. We also aim to investigate efficient training strategies to mitigate the substantial computational burden caused by the softmax operation. By addressing these areas, we hope to encourage further research on optimizing dual-encoder training for low-resource settings in the field of information retrieval.