# Library Learning Doesn't: The Curious Case of the Single-Use "Library"

Ian Berlot-Attwell

University of Toronto

Vector Institute

ianberlot@cs.toronto.edu Frank Rudzicz

Dalhousie University

Vector Institute

frank@dal.ca Xujie Si

University of Toronto

Vector Institute

six@cs.toronto.edu

###### Abstract

Advances in Large Language Models (LLMs) have spurred a wave of LLM library learning systems for mathematical reasoning. These systems aim to learn a reusable library of _tools_, such as formal Isabelle lemmas  or Python programs that are tailored to a family of tasks. Many of these systems are inspired by the human structuring of knowledge into reusable and extendable concepts , but do current methods actually learn reusable libraries of tools?

We study two library learning systems for mathematics which both reported increased accuracy: LEGO-Prover  and TroVE . We find that function reuse is extremely infrequent on miniF2F  and MATH . Our followup ablation experiments suggest that, rather than reuse, self-correction and self-consistency are the primary drivers of the observed performance gains. Our code and data are available at https://github.com/ikb-a/curious-case.

## 1 Introduction

Mathematical progress is made by building with, and building upon, the tools of those who came before. Consequently, it is no surprise that there is research interest in developing systems that can automatically learn such reusable mathematical tools. Recently, LLMs have enabled new tool-learning methods with improved performance  - but are these systems truly learning generalized, reusable knowledge or is performance improved through other mechanisms? In this work, we study two prior systems: LEGO-Prover which aims to learn reusable formal Isabelle lemmas, and TroVE which aims to learn reusable Python functions. For both, our analysis of the model's behaviour reveals that direct reuse is negligible. Furthermore, we perform two ablation studies supporting our position that function reuse plays a limited role in these systems' improved mathematical reasoning.

## 2 Related Work

LLM library learning, i.e., creating and reusing tools, depends on LLMs' ability to use tools. Prior evaluations of tool-use (typically assuming tools as REST APIs)  included real-world queries , dedicated test environments , and metrics ranging from LLM-as-a-judge  to tracking task-checkpoint completion .

In contrast, the evaluation of library learning systems has been limited. Accuracy is the metric of choice (Wang et al., 2024; Wang et al., 2024; Yuan et al., 2024; Wang et al., 2024), but cannot capture the extent or quality of reuse: an excellent library is useless to a weak reasoner, and a powerful reasoner can ignore a useless library and derive results from first principles. Prior attempts to evaluate library learning have been limited to static measures of individual functions such as cyclomatic complexity (McCabe, 1976; Zhang et al., 2024) and abstract syntax tree depth (Wang et al., 2024), or have answered specific questions such as the ease of human verification (Wang et al., 2024), accuracy under domain transfer (Zhang et al., 2024; Qian et al., 2023), or performance in the sub-problem of refactoring ground truth solutions(Lin et al., 2024).

In this study, we evaluate two library learning systems for mathematical reasoning: LEGO-Prover, and TroVE (see Sections 2.1 and 2.2). For a review of library learning systems, see Appendix A.

### LEGO-Prover: Purpose & Architecture

LEGO-Prover consumes a set of proposed theorems to produce corresponding formal Isabelle (Paulson, 1994) proofs. It was evaluated on the miniF2F (Zheng et al., 2022) dataset: each problem was attempted 100 times, and the system obtained feedback from the Isabelle verifier after each attempt. LEGO-Prover was designed to perform library learning. Using the term _skills_ in place of _tools_, Wang et al. (2024) claimed that "LEGO-Prover enables LLMs to utilize existing skills retrieved from the library" and "[m]odular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems." LEGO-Prover performs library learning via two LLM systems: 1) The Prover which uses the library to create proofs, and 2) the Evolver which iteratively refines the library. They communicate through shared databases, such as the _request db_ which stores proposed lemmas to be proven and added to library.

### TroVE: Purpose & Architecture

TroVE is a "method for inducing a toolbox of reusable functions to use in solving programmatic tasks," designed to receive a stream of word problems without a ground truth or verifier (Wang et al., 2024). For each problem, it attempts to produce a Python program that prints the correct solution. TroVE's mathematical reasoning was evaluated with the MATH dataset Hendrycks et al. (2021). Each problem is considered once: an LLM generates 15 solutions, and the best is selected based on self-consistency (i.e., majority vote) (Wang et al., 2023). In generation, 5 solutions ignore the library and directly generate a program (Skip mode), 5 create a reusable helper function for inclusion in the library (Create mode), and 5 use a function from the library (Import mode).

## 3 Analysis of LEGO-Prover

We begin by analyzing the publicly released LEGO-Prover evaluation log files 1(Wang et al., 2024). These logs are a subset of the unreleased Prover logs corresponding to the final attempts on the

    & & &  &  \\  Split & Problems Solved & Lemmas in Prompts & 1 & 2+ & 1 & 2+ \\  valid+GPT & 127 & 374 & 0 & 0 & 1 & 0 \\ valid+Human & 135 & 265 & 0 & 0 & 1 & 0 \\ test+GPT & 111 & 255 & 0 & 0 & 2 & 0 \\ test+Human & 122 & 339 & 1 & 0 & 2 & 0 \\   

Table 1: Lemma reuse in LEGO-Prover released logs. Note that **lemma reuse is very uncommon**, and **no lemma reused twice**. For each split, we report the number of problems solved, the number of unique lemmas occurring in the Proverâ€™s input prompts, the number of lemmas reused verbatim once, or more than once, and the number of lemmas whose _name_ is reused once, or more than once. A lemma is reused \(N\) times if it appears in \(N+1\) solutions (i.e., the initial use, and then \(N\) reuses).

successfully solved problems. Note that LEGO-Prover was evaluated on 4 data splits, and learned over 20,000 lemmas overall (Wang et al., 2024).

We find that only 1,233 lemmas (\(\)6%) are used in the final solving step (i.e., are inputs to the Prover). Of these, exactly one lemma is reused by the Prover, and it is reused once (i.e., appears verbatim in two solutions). As the Prover may be adjusting a lemma (e.g., paraphrasing, commenting, etc...) we repeat the analysis, checking only for the lemma's name. Again, lemma reuse is rare, and no lemma is reused more than once (i.e., no lemma has its _name_ appear in 3 or more solutions). See Table 1 for details. For an example of verbatim vs. name use, see Appendix B.

Given these findings, there are only two possibilities by which LEGO-Prover may be performing reuse: 1) indirect reuse (e.g., the learned tools are useful, reusable exemplars, rather than directly used in the final solution), or 2) direct reuse occurs in the Evolver.

Instead, we hypothesize that reuse is not significantly boosting performance. We propose that self-correction (Pan et al., 2023) via the _request db_ is the main mechanism of action. Note that the Prover populates the _request db_ by: 1) adding lemmas that the LLM suggests may be helpful sub-steps, and 2) adding lemmas from solution attempts that Isabelle could not verify. The Evolver uses the _request db_ to modify existing tools to "aid in solving requests", and to "resolv[e] decomposed sub-goals" using the library (Wang et al., 2024). Thus, the performance gains may be due to a combination of chain-of-thought (Wei et al., 2022) (through the Prover's proposal of helpful lemmas for the Evolver to solve) and self-correction (through the Evolver's retrying of failed lemmas).

To test whether any form of reuse is increasing performance, we ablate LEGO-Prover to remove cross-problem sharing: each theorem is solved with its own independent state and databases. E.g., in place of a global _request db_, each problem now has its own independent _request db_. We evaluate on a random size 12 subset of the validation split and use 50 attempts per problem. We perform our ablation using OpenAI's GPT-4o-mini as the original results were published using now deprecated versions of GPT-3.5-Turbo; see Appendix E for full details of the ablation. Running 2 trials, we find that the ablation's performance is strong, solving only 1 question less than the baseline (see Figure 1). Studying the problems solved by only the baseline, we find that only the simplest of the input lemmas are possibly used (namely \(a^{2} 0\) and \(ax^{2}+bx+c=0 c=-(ax^{2}+bx)\); see Appendix C). It is unclear as these facts are not treated as lemmas, and are given different justifications. This suggests that: 1) the LLM may be too weak if it needs examples of basic facts 2) the LLM struggles at reuse as it does not copy the given, verified, proofs.

## 4 Analysis of TroVE

As TroVE logs were not released, we re-ran TroVE on MATH, achieving accuracy within \(\)2% (absolute) of reported (see Appendix, Table 3). Note that the TroVE library also learns import statements; we ignore these in our analysis for two reasons. Firstly, our interest is in whether the system learns and reuses non-trivial tools, unlike statements such as "import math" and "from sympy import symbols". Secondly, as TroVE includes the entire library as part of the Import prompt, and import statements are innately simple, it is impossible to determine whether an import statement is included in the LLM output due to reuse, or the LLM's innate knowledge.

Figure 1: LEGO-Prover performance on a subset of the miniF2F validation split. The ablated model cannot reuse lemmas and performs similarly. The shaded region is one standard deviation, capturing variations in LLM output and race conditions.

Analyzing the logs, we find that TroVE's final libraries only contain 15 learned functions, having learned functions for only 3 of the 7 MATH subject test splits: counting, number, and pre-algebra. No functions are learned in the algebra, geometry, intermediate algebra, or pre-calculus splits. Of the 15 learned functions, only 2 are reused in a correct solution: is_perfect_square(n) is reused in one correct solution and is_prime(num) is reused in two correct solutions.

Given 3 successful reuses in 3,201 test questions, we believe that TroVE's improvements over the baselines are not due to function reuse. Instead, we believe that ensembling and self-consistency are responsible. To test this, we ablate the model by disabling Import mode, but maintaining the 15 solution attempts: we generate 8 solutions ignoring the library (i.e., Skip mode) and 7 attempting to create a helper function (i.e., Create mode). As in the original work we use CodeLlama-7b-Instruct-hf [Roziere et al., 2023]; see Appendix F for the full ablation details. Ablating Import mode prevents reuse as the library never appears in the model's input, thus also preventing library learning of import statements. As to why this ablation could still be performant, prior work established the benefits of self-consistency and increased sampling [Brown et al., 2024], and it's known that library-less tool-creation can boost performance by forcing abstract reasoning [Yuan et al., 2024].

We evaluate our ablated model on the intermediate_algebra test split (reportedly the largest performance gain over non-reuse baselines), and the geometry, number, and count test splits. On the intermediate_algebra, number, and count splits, our ablation exceeds the baseline's performance, with the improvement being statistically significant on two splits (See Table 2). On only the geometry split does the base model perform slightly better, though the learned libraries only contains import statements. From this we can conclude that library learning _import statements_ can be slightly beneficial, but only for certain domains. Typically, TroVE's library learning degrades its performance.

## 5 Conclusions

In this study, we find that both TroVE and LEGO-Prover do not directly reuse the tools they learn. Furthermore, the results of our ablations suggest that their performance gains cannot be solely attributed to indirect reuse either.

We intend that this paper be a call for the better understanding of the limitations of current library learning systems, and for improved evaluation. We show that accuracy is misleading in isolation: the system's reuse behaviour is paramount, and careful ablation is critical. Both papers studied made sensible claims as the created systems were deliberately designed for library learning and were tested against ablations that were not unreasonable - however they also relied heavily on accuracy as a metric instead of directly observing the systems' use of the library, and both chose ablations that in hindsight were too aggressive. It is clear that, particularly for ablations of library learning systems, minimal changes are preferable, and considerable thought should be put into other possible causes of improvements. There is a clear need for a broadly applicable framework for the evaluation of library learning specifically; this framework must rely on more than task accuracy and ablations to evaluate library learning and reuse.

Finally, considering library learning for mathematics in general: are LLMs capable learning tools and performing direct, verbatim reuse? Given that the observed improvements do not come from direct reuse, would direct reuse actually improve systems for mathematical reasoning, or is it overly brittle making soft reuse desirable? These important questions follow from our findings, and should inform the design of future research into library learning systems.

    &  \\  Model & count & geo & inte & num \\  TroVE Reproduced & 0.236 \(\) 0.008 & **0.058**\(\) 0.004 & 0.120 \(\) 0.006 & 0.258 \(\) 0.007 \\ No Reuse Ablation & **0.250**\(\) 0.000\(\) & 0.050 \(\) 0.000 & **0.134**\(\) 0.014 & **0.290**\(\) 0.014\(\) \\   

Table 2: TroVE performance on MATH for the ablation and the baseline. Mean and standard deviation over 5 trials are reported. The variations arise from LLM output. \(\) indicates that mean ablation performance is significantly strictly higher than the baselineâ€™s, at the Bonferroni-corrected 0.05 level, using a 2-sample 1-sided Welchâ€™s t-test (note, this test assumes approximate normality).

## 6 Limitations & Broader Impact

Due to resource constraints, our ablation studies could be more thorough. Most obviously, we only study two models, and on two datasets. The LEGO-Prover ablation is not ideal, as library learning is disadvantaged by operating on a subset of the questions; this was necessary due to resource constraints. Another limitation is that LEGO-Prover's databases are pre-loaded with the full dataset of problems; consequently, the Evolvers are exposed to other problem statements - note, however, that the impact on testing reuse is minimal. Firstly, the Prover cannot attempt to solve any of these other problems, thus the _request db_ cannot gain pending lemmas related to other problems. Secondly, under the ablated model, tasks cannot share lemmas - any performance gains would come from having access to other sample problems instead of reuse.

While we demonstrate that the performance gains in mathematical reasoning seen by TroVE and LEGO-Prover cannot be attributed to the direct learning and reuse of tools, there is a very important but _subtly different_ question which remains unanswered: whether these systems are at all capable of library learning. It is possible that these systems have the capacity to learn reusable functions and lemmas, but the datasets do not provide the opportunity. Manually inspecting the MATH dataset, our tentative conclusion is that the dataset is intrinsically not amenable to function learning with Python - we suspect the questions are too diverse, with the shared components already being captured by standard libraries. How this could be more formally demonstrated remains an important open question that is beyond the scope of this work.

This work has no immediate societal impact, rather, it highlights current limitations and challenges assumptions in this field. However, deploying tool-learning systems may carry a security risk from executing LLM-generated code (we sandboxed TroVE). More generally, library learning systems are self-improving through code generation, an approach that has raised concerns (Zelikman et al., 2023). Unexpected behaviours may develop, thus requiring sandboxing and monitoring, at the very least.