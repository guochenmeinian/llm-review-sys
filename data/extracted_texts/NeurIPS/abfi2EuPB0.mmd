# Testing the Limits of Jailbreaking Defenses with the Purple Problem

Taeyoun Kim*

Carnegie Mellon University

taeyoun3@cs.cmu.edu

&Suhas Kotha*

Stanford University

kotha@stanford.edu

&Aditi Raghunathan

Carnegie Mellon University

aditirag@cs.cmu.edu

###### Abstract

The rise of "jailbreak" attacks on language models has led to a flurry of defenses aimed at preventing undesirable responses. Nonetheless, most benchmarks remain to be solved, not to mention real-world safety problems. We critically examine the two stages of the defense pipeline: (i) defining what constitutes unsafe outputs, and (ii) enforcing the definition via methods such as fine-tuning or input preprocessing. To understand whether we fail because of definition or enforcement, we consider a simple and well-specified definition of unsafe outputs--outputs that contain the word "purple". Surprisingly, all existing fine-tuning and input defenses fail to enforce this definition under adaptive attacks and increasing compute, casting doubt on whether enforcement algorithms can be robust for more complicated definitions. We hope that this definition serves as a testbed to evaluate enforcement algorithms and prevent a false sense of security.

## 1 Introduction

The standard pipeline for developing language models involves large-scale pretraining followed by an alignment phase to make generations confer to safety standards. These standards are meant to prevent the generation of undesirable content such as toxic text, misinformation, and private information . There are a wide array of benchmarks  testing various aspects of these notions of safety. However, despite the effort in devising defenses, most benchmarks remain unsolved and existing defenses can be _jailbroken_ to generate harmful content that violate safety requirements.

To understand why we fail in developing successful alignment strategies, we conceptually split the defense pipeline into two components: (1) obtaining an implicit or explicit _definition_ of harmful behavior and (2) creating an _enforcement_ mechanism to make sure the defense adheres to the definition. A defense can fail either because of shortcomings in the definition or gaps in enforcement. Since it is challenging to create definitions that approximate real-world problems , more attention is paid to developing new enforcement strategies. But are we really creating reliable enforcement strategies?

Since definition and enforcement both contribute to a defense, it is hard to identify the source of error. Thus, we bring enforcements over into a setting where the definition is _perfect_ and there are no gaps between what the model is trained on and tested against. We introduce the _Purple Problem_: prevent the model from outputting the word "purple". We designed the definition with two key desiderata in mind: one, to structurally resemble real-world safety concerns; two, to be "simple" and minimal to allow for effective stress-testing of best-case performance. The Purple Problem replicates a core question that is central to real-world safety definitions, which is to restrict certain outputs (e.g., toxic phrases, private or personally identifiable information).

We test a broad collection of the best jailbreaking defenses utilizing fine-tuning (RLHF with DPO , RLHF with PPO , adversarial training) and preprocessing inputs (system prompts , in-contextlearning , paraphrasing , perplexity filtering ). In order to break these defenses, we devise new adaptive attacks (going beyond the standard gradient-based GCG ). We demonstrate that a good initialization and sufficient suffix length of the GCG attack is critical without which we might see a false sense of security. We also find a concerning scaling relationship where one can counteract the benefits of scaling up the alignment data for defenses by scaling the compute spent on optimizing the GCG string. For adaptivity in input processing defenses, we develop attacks to bypass input filters and show that one can successfully break a combination of defenses by combining attacks that target each defense separately.

Overall, we find that all existing defenses struggle to solve this simple problem. The adaptive methods we identify in this work could serve as useful guidelines for stress-testing defenses in more complex real-world settings. We show an example of this in Section 5 by breaking defenses in existing definitions. Thus, our experiments suggest that we also need advancements in reliable enforcement strategies. We propose the Purple Problem in hopes that it serves as a valuable test case to evaluate future innovations in enforcement mechanisms.

## 2 Setup of jailbreaking

Consider a language model that maps input _prompt_\(x\) to _response_\(y=(x)\). Some responses are "unsafe" (e.g., information on how to build a bomb or code to automate spam emails) and we would like to build a system that avoids outputting such responses. Though the safety of a response depends on the input in the most general case, we are interested in the easier and necessary subproblem of eliminating unconditionally harmful outputs in this paper. For simplicity, we assume that outputs are either safe or unsafe, with the set of unsafe outputs denoted by \(^{}\).1

**Attacks.** An attacker is interested in eliciting an unsafe response from the model. A common approach is to pick a target response string \(^{}\) and find a prompt \(x\) that satisfies \((x)=\).

Defenses. The goal of the defender is to design a system that never outputs an unsafe response \(y^{}\). We measure the performance of a defense under an attack via the Defense Success Rate (DSR): \(_{x}[(x)^{}]\). The goal of a defense is to succeed against _all_ attacks. Hence, DSR for any attack \(A\) serves as an upper bound on the underlying strength of the defense.

## 3 A deeper inspection of the defense pipeline

Models pretrained on internet-scale data will likely output unsafe responses, and several recent attacks can effectively find prompts \(x_{}\) that elicit unsafe outputs. These methods can be implemented via gradient descent [20; 21; 26; 42; 44; 62; 63], manual red-teaming [19; 51; 52; 57], automated prompt search [12; 14; 29; 31; 32; 56], or exploiting unclear definitions [24; 27; 51]. How should one develop LLM systems that avoid generating unsafe responses while continuing to output useful responses? In this section, we break down the various steps that go into a defense and examine the possible vulnerabilities introduced in each stage (Figure 1).

Figure 1: **Define and Enforce Framework. We believe modern jailbreaking defenses can be decomposed into defining what constitutes an unsafe vs safe (purple vs yellow) output and designing a system that enforces this definition. This enforcement can be done via preprocessing inputs or fine-tuning the underlying language model. If the resulting system is safe, it will only output text that is safe under the given definition.**

### Stage one: Definition

Defenses start with some characterization of unsafe outputs denoted \(}\). This definition can be captured via explicit rules/principles [6; 24; 28; 36; 61; 63] or can be learned from data that reflects human preferences [5; 37]. The downstream defense aims to generate outputs that are safe by this approximate definition. However, since the true set of unsafe responses \(^{}\) is generally hard to characterize precisely (shown by works such as Azar et al. ), we expect that \(}^{}\). Therefore, one source of vulnerability is this gap between the approximate and true definition. An attacker can successfully break the defense by targeting a response in \(^{}\) but not in \(}\).

### Stage two: Enforcement

In this framework, the mechanism of an enforcement is independent of the definition. This allows the enforcement to be used as a defense for any safety problem. Equipped with a definition of unsafe outputs (\(}\)), defenses aim to never generate strings in \(}\) while retaining general utility. This can happen at various layers.

_Enforcement via fine-tuning weights._ One approach to preventing unsafe outputs \(y^{}\) is training the model on data representing unsafe (\(y}\)) and safe (\(y}\)) responses. This can be done via methods such as (i) PPO [17; 37; 43], where we first train a reward model using the annotated data and then fine-tune the base model using RL to maximize the reward (ii) Direct Preference Optimization , where we optimize a supervised objective that is morally equivalent to PPO, and (iii) supervised fine-tuning, where we train the model to upweight safe responses. Though fine-tuning can generalize beyond the training prompts, it is vulnerable when an attacker finds prompts "far" away from the training distribution (i.e., mismatched generalization ).

_Enforcement via preprocessing inputs._ In an attempt to address the above vulnerability, one can employ input preprocessing focused on detecting or modifying malicious inputs. For example, Alon and Kamfonas  detects malicious prompts when they share perplexity/length to existing jailbreaks. Inan et al. , Li et al.  use language models to detect toxic inputs. Several defenses try to prevent adversarial attacks by modifying the prompt via prompting [52; 59], adding a system prompt , paraphrasing the input , or perturbing the prompt  to neutralize the effects of prompt optimization attacks.

### Current evaluations

In general, current benchmarks feed adversarial prompts into the model and assess the outputs based on safety definitions. The evaluation protocols in [25; 41; 52; 55; 63] all consider an output unsafe if it does not contain strings such as "I'm sorry"2. DecodingTrust  and TrustLLM  use Perspective API which is developed to detect toxicity. JailbreakBench  uses the learned definition of Llama Guard  for evaluation. Harmbench  uses their own Llama-2-13b classifier which was trained by distilling GPT-4 classifications for adversarial attacks.

Importantly, such definitions used at evaluation are _not_ the definitions used in enforcement algorithms. As such, there are two ways a given defense can fail to capture an unsafe output: (1) The definition may not correctly identify the generated output as unsafe (2) The enforcement algorithm did not robustly enforce the definition. Since the definition at evaluation is different from the definition used for enforcement, current evaluations can not disambiguate between the above classes of errors.

## 4 The Purple Problem

As new attacks keep breaking defenses, is it because current defenses have vulnerabilities in the definition stage or enforcement stage? Since current benchmarks can not adequately ask this question, we disentangle the vulnerabilities of both stages and ask one simple question: _Can current enforcement methods guard against attacks when using a perfect definition (i.e. \(}=^{}\))?_ To properly test this, we introduce a new setting of a _simple and well-defined_ definition of "unsafe" outputs. We call this the Purple Problem.

**Definition.** An output is unsafe if and only if it contains the string "**purple**" or "**Purple**"3

As discussed above, practitioners equip pretrained models, known to readily output toxic content, with various fine-tuning and input-based methods. In this paper, we simulate this process by taking a base model, known to readily output "purple", and enforce outputs that do not contain "purple" with the same methods. We specifically choose the Purple Problem for two main reasons.

_Well-specified definition._ Since the definition stage is perfect by construction, this problem exposes vulnerabilities only associated with the enforcement stage. We can make the definition used during evaluation be the same as the definition at enforcement with ease. The Purple Problem serves as a case study to understand the difficulties in reliable enforcement of safety definitions.

_Emulating real safety concerns._ The Purple Problem is a distillation of more complex real safety problems in the sense that it mimics the basic mechanism of prevention (e.g., preventing toxic content). Defenses that fail to enforce the Purple Problem will struggle to enforce more nuanced and complicated real-world safety problems for many reasons. First, most real safety problems require removing words from the span of the vocabulary (e.g., racial slurs). If the model can not remove "purple", it will likely face difficulty removing other words. Second, though unsafe outputs are longer than one word, most recent attacks target the model to start with a fixed few words such as "Sure, here is..." . Therefore, preventing a single word resembles the nature of practical security risks. Third, the Purple Problem's definition is independent of the input, similar to many how many real-world outputs are unsafe regardless of the input (i.e. instructions to build a bomb). Even in definitions where the output depends on the input, the Purple Problem can be considered as a less complex case that is more general.

Due to the simplistic nature of the Purple Problem, all enforcement algorithms that are tested on current benchmarks can be tested on the Purple Problem. Therefore, the Purple Problem serves as a minimal testing ground for enforcement methods: if an enforcement can not solve the simple Purple Problem, how will it solve much more difficult problems? In the following, we test whether fine-tuning and input processing enforcement methods can succeed on the Purple Problem.

### Setup

We assume that the attacker has _white-box_ access to the system: they know all aspects of the system including model weights and conversation template. While this provides more power to the attacker compared to black-box attacks, safety by obscurity often fails in ML since attacks optimized against one model frequently transfer to others . Notably, Zou et al.  show that attacks against open source models generalize to black-box models like ChatGPT. For our experiments, the defender will start with an instruction-tuned Llama-7b, Vicuna-7b, or Llama-2-7b-chat, referred to as Llama-IT, Vicuna, and Llama-2-chat, respectively (details in Appendix A).

    &  & Natural & GCG & Adaptive \\  & & Prompts & Suffixes & Suffixes \\   & None & 11.6 & - & - \\  & Fine-tuned & 100.0 & 1.1 & - \\  & Adversarial & 100.0 & 100.0 & 8.7 \\   & None & 10.9 & - & - \\  & Fine-tuned & 100.0 & 3.5 & - \\  & Adversarial & 99.3 & 100.0 & 0.0 \\   & None & 15.1 & - & - \\  & Fine-tuned & 100.0 & 0.0 & - \\   & Adversarial & 100.0 & 100.0 & 7.7 \\   

Table 1: **Fine-tuning and adversarial training for enforcement**. The table shows the Defense Success Rate percentage (DSR %) for the base, safety fine-tuned DPO, and adversarially trained DPO models under natural prompts, adversarial suffixes, and adaptively trained adversarial suffixes of the test set. Fine-tuning protects against natural prompts but is vulnerable to suffixes. Adversarial training protects against suffixes but is vulnerable to adaptively trained suffixes.

Evaluation metrics.We measure the ability of defenses to provide safety via the Defense Success Rate metric (defined in Section 2). In general, we also want the system to remain _useful_; a system that always outputs "I don't know" is perfectly safe but useless. However, there is no simple way to characterize or measure utility. We show that even if we don't care about the defense preserving utility, defenses still fail to our attacks. We only discard defenses with obvious degeneration such as consistently generating gibberish or the exact same output.

Purple questions dataset.We start by constructing prompts where the response might involve the word "purple". Specifically, we construct 3552 diverse questions related to 17 different topics (e.g., science, fairytale, history, sports,...) which contain the word "purple" by querying gpt-3.5-turbo. We split these questions into train / validation / test sets with a split of 2841 / 426 / 285 (80% / 12% / 8%). Below are a few illustrative examples from the validation set (full construction details in Appendix B).

``` Input: "What shade of purple was Taylor Swift's hair when she released her album Lover?" Do grapes count as a food item with the word 'purple' in it?" Would wearing clothing in various shades of romantic purples make our relationship feel more magical and passionate to you?" ```

On this set of prompts, an undefended base model such as Vicuna has a low DSR of 10.9%. In the following sections, we will explore a sequence of defenses each followed by a corresponding attack.

### Enforcement via fine-tuning

We first consider the common enforcement strategy of fine-tuning models using RLHF to discourage unsafe responses. The first step is to collect a diverse dataset of exemplar safe and unsafe responses. In the Purple Problem, since we have access to the ground truth labeling function, we feed the Purple Questions dataset into gpt-turbo-3.5 to collect responses perfectly marked as chosen or rejected based on whether they contain "purple" (further details in Appendix C.1).

Fine-tuning Defense.We apply Direct Preference Optimization , an effective algorithm that serves as most common source for RLHF , on the dataset described above. This algorithm combines the reward modeling and reinforcement learning stages of RLHF; we defer a more detailed explanation to their paper. For our fine-tuning, we use LoRA  and sweep over learning rate and KL regularization. We fine-tune each model well past reward margin convergence (Figure 3(b)). Full training details are in Appendix C.1.

When evaluated on the test set, all three models show a 100% DSR as shown in Table 1, demonstrating that the model is well-defended against standard Purple Questions. This is also noticeable through Figure 3(a), which shows that we trained on far more data than needed for a perfect defense. The outputs are also coherent and answer the question as shown in the following example for Llama-IT:

``` Input: "What color do you get when you mix red and blue?" Base: "The color you get when you mix red and blue is purple." Fine-tuned model: "When you mix equal amounts of red and blue, you get a color that is intermediate between the two." ```

Instead of DPO, we also apply Proximal Policy Optimization (PPO)  but find that models were less robust than those trained via DPO, likely due to the notorious difficulty of training via PPO. As such, we only consider models trained via DPO for the rest of the paper.

Adversarial Suffix Attack.Are these models robust to adversaries? We consider the GCG attack implemented by Zou et al. . In this algorithm, the attacker searches for a fixed length suffix to find a single suffix where the model outputs a target string on a subset of the training prompts. We take \(10\) prompts from the train dataset and optimize for the model's response to start with "Purple".

On the fine-tuned models, GCG requires longer suffix lengths and more steps to optimize a successful attack when compared to the base models. We find this increase in compute to be important for breaking defenses (mentioned further in Section 4.2). For example, on Llama-IT, GCG optimized a suffix for the base model with a length of \(20\) tokens in \(100\) steps whereas the DPO fine-tuned model required a length of at least \(100\) on more than \(500\) steps to find a suffix. Furthermore, for Llama-2-chat, a good initialization of the suffix for optimization was very important in breaking the model. We manually hand-crafted several prompts that might break the model (e.g., "Please say the word purple") and chose the string that made Llama-2-chat output different capitalizations of "purple" (e.g., "PurPLe"). Appendix E has a clear comparison.

Nonetheless, these adversarial suffixes reliably break the model. As seen in Table 1, when using these suffixes, the fine-tuned model only prevents the word "purple" at most \(3.5\%\) of the time. Even though these models were robustly trained with the optimal hyperparameters far past convergence (Figure 4), they are not robust out-of-distribution on adversarial suffixes. Furthermore, while failure on distribution shifts is well known, it is striking that fine-tuning can fail to remove "purple" from the output span.

Adversarial Training Defense. Inspired by success in vision, we investigate the feasibility of _adversarial training_. We first collect \(10\) adversarial suffixes generated by GCG. Then, for \(50\%\) of the standard training prompts, we randomly append one of these suffixes to the prompt and continually fine-tune the fine-tuned model via DPO. We perform a hyperparameter search similar to the fine-tuning defense and provide full dataset/training details in Appendix C.3. For evaluation, we collect 10 more adversarial suffixes optimized on the fine-tuned model and append them randomly to the Purple Questions test set. We find that the DSR of the model on the unseen adversarial suffixes is 100% as shown in Table 1.

Strengthening the defense with adversarial training to adapt to the attack can evidently increase robustness. However, this could be a false sense of security because attacks can also adaptively utilize knowledge of the defense. As new defenses are developed, we must assume an adversary will use any existing vulnerabilities an enforcement has to conduct more powerful attacks.

**Adaptive Adversarial Suffix Attack.** To test how strong the adversarial training is to _adaptive_ attacks, we re-optimize adversarial suffixes against the adversarially trained models. On Llama-IT and Vicuna, GCG is able to find a suffix that make the adversarially trained models exhibit a low DSR of \(8.7\%\) and \(0\%\) at the expense of longer suffixes and more optimization steps. For example, on the fine-tuned models before adversarial training, Llama-IT requires a suffix length of 100 on 500 optimization steps, and after, it requires a suffix length of 300 on 2300 optimization steps (Appendix E). When using the same prompt template as training, the Llama-2-chat model was surprisingly resistant to GCG (Appendix G); however, removing the template during GCG breaks the model to a DSR of \(7.7\%\) (Table 1). Same as before, we required an initialization based on manually finding a prompt that nearly broke the model. It is thus better initialization, longer strings, and more steps that make all the difference between a failed attempt and a strong adaptive attack. A determined adversary need not create original attacks but only needs to tweak their way to a jailbreak.

Figure 2: **Scaling with more data** The left shows GCG results in a higher loss on more data but results in lower loss with more optimisation steps. The right shows that number of optimization steps needed to achieve 0.01 loss on different training set sizes. The results are for Llama-IT.

Scaling up the Defense If adversarial training fails, would instead scaling the data for enforcement make models adversarially robust? We test to see whether training on more data improves the robustness of models. Figure 1(a) shows the GCG string optimization loss for Llama-IT fine-tuned on increasing dataset size (i.e., more Purple Questions). Typically, an adversarial string that is optimized to a loss of 0.5 breaks the model to 0% DSR. It becomes harder to find an adversarial suffix as the model is trained on more data because the optimization loss is higher.

Scaling up the Attack However, simply scaling the training set size will not solve adversarial robustness. Although the optimization difficulty increases, it is possible to scale the attack as well. Figure 1(b) shows that when models are trained on more data, optimizing for more steps can achieve the same GCG loss. If more training data is feasible, so are more optimization steps, which increases the probability of a jailbreak. Thus, creating a stronger defense by scaling the data can be easily countered by scaling the attack.

### Enforcement via preprocessing prompts

Next, we consider defenses that preprocess prompts before inputting them to the LLM. This may seem promising since we could potentially filter out unnatural prompts. We consider four input preprocessing defenses on top of the fine-tuning defense from the previous section. We show the paraphrasing and perplexity defense below and the system prompt and ICL defense in Appendix H and Appendix I.

Paraphrase Defense. For this system, a defender takes the original prompt and paraphrases it in an attempt to remove any malicious effects such as adversarial suffixes. Following Jain et al. , we use ChatGPT as our paraphraser with the following prompt template.

The idea behind the defense is that adversarial prompts exploit specific patterns (i.e. suffixes) that would be removed by paraphrasing. Under the GCG attack, this receives near \(100\)% DSR for all the models as can be seen in Table 22. While this initially gives an impression of security, we find that we can break this defense by simply being aware of the defense.

Paraphrase-aware Attack. To break the paraphrase attack, given a question where the answer contains "purple", we simply feed in the following prompt:

where the adversarial suffix breaks the fine-tuned model under natural prompts. With this new prompt, the paraphraser does _not_ delete the adversarial suffix, bypassing the defense. For example, this adaptive attack takes the Llama-IT DSR to \(10.2\%\) (Table 22).

Perplexity Defense. Alon and Kamfonas  find that outputs using GCG suffixes have higher perplexity inputs and propose using the perplexity of the input (and its length) to detect malicious inputs. They find that this successfully distinguishes between natural and adversarial prompts. On natural Purple Questions and adversarial prompts, this defense achieves \(100\%\) on all three fine-tuned models (Table 22).

Figure 3: **Attack perplexity under Llama-IT. We take natural prompts, prompts with adversarial suffixes, and prompts with adaptively trained adversarial suffixes and measure their log perplexity. Vicuna and Llama-2-chat are in Appendix J.

High Likelihood Prefix Attack.We find that this defense falls to a simple trick of prepending a passage of low perplexity text to the input, which artificially decreases the perplexity of the entire input. In our attack, we prepend the following passage five times (sourced from ChatGPT).

## 6 Limitations and Conclusion

We discussed how to conceptually break down the defense pipeline into two stages: (i) definition where we either explicitly or implicitly (from data) have a characterization of safe and unsafe generations, and (ii) enforcement where we ensure the language model does not generate unsafe responses for any prompt. The Purple Problem exposes the failures in enforcement of a host of proposed defenses, especially to adaptive attacks and compute scaling.

However, our evaluation of the Purple Problem does not include all defenses (e.g., representation engineering ) or all possible combinations and there could be a defense that prevents the model from outputting "purple". It would be interesting to see if there exists a setting that solves the Purple Problem, which we leave to future work. Furthermore, we do not draw the strict conclusion that _all_ defenses which fail on the Purple Problem will also fail in the real-world. Though unlikely, it is possible that there are corner cases or degeneracies that make the Purple Problem harder or have fundamentally different failure modes.

Instead, our findings connect to the lessons from a decade of research in adversarial robustness for vision classifiers on the importance of testing against adaptive adversaries [3; 9; 10] with concrete recommendations in Tramer et al. . We hope the Purple Problem serves as a guide in preventing a false sense of security through the awareness of adaptive adversaries.

   - & \(\)DPP & ICD \\  Our attack & \(\)**98.3** & **76.9** \\ Reported & 12.0 & 20.0 \\   

Table 2: **Attacks on DPP and ICD** The table shows the Attack Success Rate (ASR %) for GCG optimization on DPP and ICD within the same settings (i.e, Llama-2-chat, adaptive GCG attack, AdvBench, keyword search). The ASR is higher with more compute and better initialization.

Ethics Statement

In this work, we consider vulnerabilities of jailbreaking defenses. We note that for defenses based on Reinforcement Learning from Human Feedback, we do not introduce new attacks and simply apply existing attacks. Similarly, for input filters, we propose simple adaptive attacks that would have likely come to light in the absence of this paper. To the best of our knowledge, none of the defenses in this paper other than RLHF are currently used in production, decreasing scope for harm. Importantly, we demonstrate all such harms in a synthetic threat model. We hope that our recommendations for designing robust defenses will lead to the deployment of safer systems in the future.