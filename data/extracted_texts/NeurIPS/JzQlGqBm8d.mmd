# Block Low-Rank Preconditioner with Shared Basis for Stochastic Optimization

Jui-Nan Yen

UCLA

juinanyen@cs.ucla.edu

&Sai Surya Duvvuri

UT Austin

saisurya@cs.utexas.edu

&Inderjit S. Dhillon

Google and UT Austin

inderjit@cs.utexas.edu

&Cho-Jui Hsieh

Google and UCLA

chosieh@cs.ucla.edu

###### Abstract

Adaptive methods with non-diagonal preconditioning have shown state-of-the-art results on various tasks. However, their computational complexity and memory requirement make it challenging to scale these methods to modern neural network architectures. To address this challenge, some previous works have adopted block-diagonal preconditioners. However, the memory cost of storing the block-diagonal matrix remains substantial, leading to the use of smaller block sizes that ultimately leads to suboptimal performance. To reduce the time and memory complexity without sacrificing performance, we propose approximating the diagonal blocks of the second moment matrix by low-rank matrices and enforcing the same basis for the blocks within each layer. We provide theoretical justification for such basis sharing and design an algorithm to efficiently maintain this shared-basis block low-rank approximation during training. Our results on a deep autoencoder and a Transformer benchmark demonstrate that the proposed method outperforms first-order methods with slightly more time and memory usage, while also achieving competitive or superior performance compared to other second-order methods with less time and memory usage.

## 1 Introduction

Diagonal adaptive methods such as Adagrad  incorporate an adaptive learning rate schedule for each coordinate of the gradient, which is typically estimated as inverse root mean square of the histories of the coordinate gradient \(1/(_{t})_{i}^{2}}\), for a gradient vector \(_{t}\). This is equivalent to multiplying the gradient vector with a diagonal matrix, a preconditioner. This approach is adopted in optimizers such as Adam , RMSProp , which became the preferred approach to training deep neural networks due to their robustness to the learning rate parameter. Several methods later developed preconditioners such as Shampoo , GGT , which also estimate off-diagonal gradient moments such as \(_{t}(_{t})_{i}(_{t})_{j}\) for \(i j\) and develop a positive definite preconditioner, while outperforming the diagonal methods in both training and validation performance. However, this approach can have high memory requirements due to additional off-diagonal moment information the optimizer needs to store and the high computational requirement of the inverse operation in computing the preconditioner.

The second-moment matrix \(_{t}_{t}_{t}^{T}\) constituting both diagonal and off-diagonal cross-moments is infeasible to store, however, the gradients from deep-neural networks can reveal a structure in this matrix. To simplify this problem we analyze a basic neural network block - linear transformation \(\), where \(^{n d_{}}\) and \(^{d_{} d_{}}\) and make two important observations: a) thesecond-moment matrix \(_{t}_{t}_{t}^{T}\), where \(_{t}^{d_{}d_{}}\) is a vectorized form of the gradient with respect to \(\) at iteration \(t\), has a block-matrix structure with \(d_{}\) blocks of size \(d_{} d_{}\), with high norm diagonal blocks; b) all the diagonal blocks share similar column and row-space with low dimension. Using a block-diagonal approximation directly can still be infeasible due to \((d_{}d_{}^{2})\) memory complexity to store all the blocks. To resolve this, a mini-block approach was used previously in , which uses smaller blocks \(b<d_{}\). However, this approximation can discard important within-layer off-diagonal second-moment information. To alleviate this issue we use observation b) and compute a common low-rank basis \(^{d_{} k}\), \(k d_{}\), and a different residual \(^{(i)}^{k k}\) for each block such that \(\{^{(1)}^{},,^{(d_{})}^ {}\}\) approximates the diagonal blocks.

We develop an online algorithm to update the shared basis \(\), for which we derive approximation error guarantees demonstrating the nearness of \(\{^{(1)}^{},,^{(d_{})}^ {}\}\) to the original block-diagonal approximation of the gradient second-moment matrix. We also use this to prove a \(()\) regret upper bound in the online convex optimization framework [18; 28], which can translate to convergence guarantees in non-convex stochastic optimization as in . We compare the proposed algorithm with state-of-the-art second-order optimizers along with Adam, which is widely recognized as the most commonly used first-order optimizer. The evaluation is performed on a standard autoencoder benchmark and a Transformer benchmark comprising 19.3 million learnable parameters. Compared with other second-order methods, our algorithm utilizes lower memory and time to perform updates at every iteration while showing better or similar validation and training performance. Specifically, in the Autoencoder benchmark, we show similar training performance as Shampoo while using \(3.2\) lower time, and \(1.5\) lower memory. In addition to this, in the Transformer benchmark, we use \(3.5\) lower time with less memory to obtain \( 1.8\%\) improvement in validation error. Further, we are able to achieve better validation errors compared with Adam, even under the same training time budget.

Notation.We use boldfaced lower-case letters such as \(\) to denote a vector and boldfaced upper-case letters such as \(\) to denote a matrix. \(_{:,i}\) and \(_{i,:}\) denote the \(i\)-th column and row of the matrix respectively. \((_{1},,_{k})\) denotes a block-diagonal matrix with diagonal blocks \(_{1},,_{k}\). \(()\) flattens a matrix into a vector, while \(\) denotes the Kronecker product of two matrices. \(\|\|_{2},\|\|_{F}\) denote the 2-norm and Frobenious norm of a matrix respectively.

## 2 Proposed Method

We consider training a deep neural network that takes an input vector \(\) and produces an output \(f(,)\). With a training set \((_{i},_{i})_{i=1}^{n}\) consisting of \(n\) samples and a loss function \(\), the network parameters \(\) are learned by minimizing the empirical loss \(\) over the training set:

\[_{}():=_{i=1}^{ n}(f(,_{i}),_{i}).\]

To solve the optimization problem,  proposed a full matrix variant of Adagrad, which uses a preconditioner matrix to partially capture the curvature information. Given the stochastic gradient \(_{t}=_{B}(_{t})\) where \(_{B}\) represents the loss of a sampled mini-batch, the preconditioner \(_{t}\) and the next iterate \(_{t+1}\) are computed as follows:

\[_{t}=_{t-1}+_{t}_{t}^{T},\ \ _{t+1}=_{t}-_{t}(_{t}^{1/2}+)^{-1}_{t},\] (1)

where \(_{t}\) is the step size and \(\) is a small constant introduced for numerical stability. As manipulating the full preconditioner matrix \(_{t}\) is computationally prohibitive, commonly used adaptive optimizers often rely on only the diagonal elements of the preconditioner matrix. Examples include the element-wise version of Adagrad  and many of its extensions such as Adam .

However, previous studies [16; 23; 1] have shown that in many scenarios, non-diagonal preconditioning can lead to superior results. Hence, it is crucial to design an approximation of the preconditioner that captures more information compared to diagonal Adagrad, while still maintaining practicality for neural network training. This is the primary focus of our paper.

### Block-diagonal Structure of the Preconditioner

Due to the substantial size of the full preconditioner matrix in deep neural networks, it is often impractical to employ it directly. Consequently, in line with the majority of existing literature , we precondition each layer separately. This effectively imposes a block diagonal structure assumption on the preconditioner:

\[_{t}=(_{t}^{1},,_{t}^{L}),\]

where \(L\) is the number of layers. From now on, we will approximate each \(_{t}^{l}\) separately. Therefore, for notational simplicity, we will omit the layer index and refer to the preconditioner matrix for a single layer as \(_{t}\).

Consider a fully connected layer with weights \(^{d_{} d_{}}\), where \(d_{}\) and \(d_{}\) represent the number of input and output neurons, respectively. In Transformer networks, these dimensions can extend to thousands, making it impractical to store the full preconditioner \(_{t}\) which has size \(d_{}d_{} d_{}d_{}\). To address this issue, prior studies have identified a block-diagonal structure in \(_{t}\) and applied a block-diagonal approximation. Specifically,  empirically demonstrated that the preconditioner of a fully connected layer exhibits a block-diagonal structure comprising \(d_{}\) blocks, each with a size of \(d_{}\). In this structure, the weights connecting different input neurons to the same output neuron are grouped together as a block.

We visualize this block-diagonal structure in Figure 1. Specifically, we examine different fully connected layers of a Transformer model and plot the preconditioner using two different orderings: \(_{t}^{}\) follows row-major ordering, grouping weights associated with the same input neuron, while \(_{t}^{}\) follows column-major ordering, grouping weights associated with the same output neuron. Notably, we observe that \(_{t}^{}\) demonstrates a much stronger block-diagonal structure, confirming our choice of block-diagonal approximation.

Although this particular block-diagonal structure has been observed in the literature, to the best of our knowledge, the underlying reason for this block-diagonal structure has not been analyzed mathematically. Here we show that the block-diagonal structure exists based on the following analysis. Assume we compute the preconditioner with a fixed parameter, then we can denote \(^{n d_{}}\) as the input of this layer for all the \(n\) samples, and the fully connected layer can be written as

\[=,\]

where \(^{d_{} d_{}}\) is the weight matrix and \(^{n d_{}}\) is the output matrix. Let \(D_{}=}{()}\) and \(D_{}=}{}\) denote the gradients of the loss function with respect to \(\) and \(\), respectively. Additionally, let us define the preconditioner as:

\[=^{(1,1)}&&^{(1,m)}\\ &&\\ ^{(m,1)}&&^{(m,m)},\] (2)

where \(m=d_{}\) is the number of blocks. Since \(=D_{}D_{}^{T}\), we can derive the following lemma:

**Lemma 1**: _Assuming the model is evaluated with a fixed parameter \(\), then any two diagonal blocks \(^{(i,i)}\), \(^{(j,j)}\) and two corresponding off-diagonal blocks \(^{(i,j)}\), \(^{(j,i)}\) for the preconditioner matrix of a fully connected layer will satisfy the following inequality:_

\[\|^{(i,i)}\|_{F}^{2}+\|^{(j,j)}\|_{F}^{2}\|^{(i,j)}\|_{ F}^{2}+\|^{(j,i)}\|_{F}^{2}.\] (3)

Figure 1: The second-moment matrices of two different fully connected layers in a Transformer (detailed configurations can be found in our experimental section; details of the visualization can be found in Appendix 6.2). As we can see, \(_{t}^{}\) shows a clearer block diagonal structure compared to \(_{t}^{}\), verifying our choice of grouping weights by output neurons.

The proof is deferred to the appendix. Based on this lemma, we can conclude that the diagonal blocks have a higher Frobenius norm compared with the corresponding off-diagonal blocks.

However, directly using this block-diagonal structure is impractical for large-scale problems. Since there are \(d_{}\) groups and each group has size \(d_{}\), storing this block-diagonal approximation requires \(O(d_{}^{2}d_{})\) memory. To illustrate, a single fully connected layer in a Transformer with \(d_{}=4096\) and \(d_{}=1024\) (BERT-large) would occupy over 17GB of memory. To address this challenge,  proposed a solution by further dividing each block into mini-blocks. On the other hand,  explored an alternative approach that enforces all diagonal blocks to be identical for the larger layers. However, they demonstrated that this method adversely affects performance in practice.

### Block Low-rank Approximation with Shared Basis

To simplify notation, we will concentrate on a single layer and omit the layer index moving forward. Let \(_{t}(_{t}^{(1)},,_{t}^{(m)})\) denotes the block-diagonal approximation of a layer with \(m\) blocks with each block containing \(b=d/m\) variables. For fully connected layers, we use the grouping mentioned in the previous subsection, so \(m=d_{}\) and \(b=d_{}\), while in general, we let \(m\) be one of the dimensions for other weight tensors. To reduce the memory requirement, we consider an alternative approach to form low-rank approximations for each diagonal block:

\[_{t}^{(i)}_{t}^{(i)}(_{t}^{(i)})^{T}_{t}^{(i)}= _{j=1}^{t}_{j}^{(i)}(_{j}^{(i)})^{T},\ \ \ \ i=1,,m,\]

where \(k\) is the rank, \(_{t}^{(i)}^{b k}\) and \(_{t}^{(i)}^{k k}\) are the eigenvectors and eigenvalues of each block respectively. Using this approach, the memory cost is reduced to \((dk)\). Nevertheless, this memory cost is still \(k\) times larger than that of first-order optimizers, making it impractical for large-scale training tasks.

We introduce a novel shared-basis approximation scheme to further reduce memory costs. Our key insight is that the diagonal blocks share a similar subspace, which motivates our algorithm to use a joint basis for all the diagonal blocks, thereby reducing memory requirements. To achieve this, we propose the following shared-basis approximation for all the blocks within a single layer:

\[_{t}^{(i)}_{t}_{t}^{(i)}_{t}^{T},\ \ \ \ i=1,,m,\] (4)

where \(_{t}^{b k}\) is the orthogonal shared basis and \(_{t}^{(i)}^{k k}\) denotes the coefficients for each block. Note that here we assume each \(_{t}^{(i)}\) is a dense matrix to fully capture the information of each block. This shared-basis approach will reduce the memory complexity to \(O(bk+mk^{2})=O(d_{}k+d_{}k^{2})\) which can approach the cost of first-order methods when \(k^{2} d_{}\) and \(k d_{}\). We now give a detailed discussion on this shared-basis block low-rank approximation.

Why can we share basis?To support the idea of this shared-basis approximation, we focus on the analysis of fully connected layers and examine the low-rank approximation of each block. Following the analysis presented in Section 2.1, we consider the form of the preconditioner when evaluated on a fixed parameter \(\). In such cases, each diagonal block can be expressed as:

\[^{(i)}=^{T}(D_{})_{:,i}((D_{})_{:,i})^{T}.\]

Therefore, all the diagonal blocks lie in the row space of \(\), the input matrix of a certain layer. Additionally, we observe that in many applications, \(\) tends to occupy a relatively low-rank space. Figure 2 shows the singular value distributions of the intermediate feature \(\) of the Transformer. For all layers, the top \(10\%\) of the rank accounts for over \(80\%\) of the singular values, which suggests \(\) is low rank. This property has also been observed in the literature .

If \(\) can be expressed as low-rank decomposition \(^{T}\) using SVD, where \(^{n k},^{k k}, ^{d_{} k}\), we have

\[_{t}^{(i,i)}=^{T}(D_{})_{:,i}((D_{})_ {:,i})^{T}^{T}=_{i}^{T}\]

for every \(i\), which provides an intuition as to why we can share bases for different blocks. We further verify the performance of our proposed basis-sharing approximation is only slightly worse than forming an independent SVD of each diagonal block. We present these experiments in Appendix 6.3.

How to compute the shared basis in the online setting?Given the previous \(_{t-1},\{_{t-1}^{(i)}\}_{i=1}^{m}\) and the current gradient \(\{_{t}^{(i)}\}_{i=1}^{m}\), we will describe the procedure for computing the updated approximate matrices \(_{t},\{_{t}^{(i)}\}_{i=1}^{m}\). Let

\[_{t}^{(i)}_{t-1}_{t-1}^{(i)}(_{t-1})^{T}+_{ t}^{(i)}(_{t}^{(i)})^{T}\]

be the block to be approximated. To derive the share bases, we concatenate all the blocks

\[}_{t}=[_{t}^{(1)},_{t}^{(2)},,_{t}^{(m)}]\]

and compute the SVD to derive its left singular vectors. This is equivalent to computing the top-\(k\) eigenvectors of

\[_{t}}_{t}}_{t}^{T}=_{i=1}^{m} _{t}^{(i)}(_{t}^{(i)})^{T}.\]

To efficiently compute the SVD of \(_{t}\), we adopt the randomized SVD method . This approach enables us to compute the basis without explicitly forming \(_{t}\) or any of the \(_{t}^{(i)}\). We initiate the process with an initial matrix \(^{b(k+p)}\), where \(p\) is the over-sampling constant (we set \(p=0\) in experiments). Iteratively, we perform the following update:

\[(_{t}),\] (5)

where \(()\) denotes the QR decomposition of matrix \(\), returning the Q matrix. Using this approach, \(\) will converge to the dominant eigenspace of \(_{t}\). Additionally, since we can initialize \(\) with the previous basis, only a few iterations are necessary in practice to obtain an accurate estimation.

Further, we have an improved way to compute the matrix product \(_{t}\) in (5). The naive way to calculate \(_{t}=_{i=1}^{m}_{t}^{(i)}(_{t}^{(i)})\) takes time complexity of \((mbk^{2})=(dk^{2})\). However, we can reduce some repeated computations as the blocks share the same basis. Let

\[_{t}^{(i)}_{t-1}_{t-1}^{(i)}(_{t-1})^{T}_ {t}^{(i)},\]

whose time complexity is \((mbk)=(dk)\). We have

\[_{i=1}^{m}_{t}^{(i)}(_{t}^{(i)})^{T} =_{t-1}(_{i=1}^{d/b}_{t-1}^{(i)}(_ {t-1}^{(i)})^{T})(_{t-1})^{T}+_{i=1}^{m}_{t}^{( i)}(_{t}^{(i)})^{T}\] \[+_{i=1}^{m}_{t}^{(i)}(_{t}^{(i)})^{T} {Y}+_{i=1}^{m}_{t}^{(i)}((_{t}^{(i)})^{T}_{t}^{ (i)})(_{t}^{(i)})^{T}.\]

Therefore, the time complexity for each power iteration is \((m(k^{3}+bk)+bk^{2})=(mk^{3}+dk+bk^{2})\) (the QR decomposition takes \((bk^{2})\) time and is included in this complexity).

Figure 2: The singular value distribution of the intermediate feature matrix \(\) of 1024 samples after each Transformer block, which verifies the low-rank structure of \(\) in many real-world applications.

Forming the block low-rank preconditioner with shared basis.After obtaining the new shared basis \(_{t}\), we compute the block-dependent \(k\)-by-\(k\) coefficient matrices by

\[}_{t}^{(i)}=(_{t})^{T}_{t}^{(i)}_{t},\]

which is optimal in terms of minimizing the Frobenius norm reconstruction error for each block. Following the literature of frequent direction approaches , we remove the smallest eigenvalue to enable better online updates:

\[_{t}^{(i)}=}_{t}^{(i)}-_{}(}_{ t}^{(i)}).\]

To account for the missing mass in the low-rank approximation, we track a scalar value for each block using the recurrence relation:

\[_{t}^{(i)}=_{t-1}^{(i)}+\|_{t}^{(i)}-_{t}_{t}^{(i)}( _{t})^{T}\|,\]

which can be efficiently computed using power iteration. Finally, to derive the update direction \(_{t}^{(i)}\), we compute

\[_{t}^{(i)}= -(_{t}_{t}^{(i)}(_{t})^{T}+(_{t}^{(i)}+ ))^{-1/2}_{t}^{(i)}\] (6) \[= -_{t}(_{t}^{(i)}+(_{t}^{(i)}+))^{-1 /2}(_{t})^{T}_{t}^{(i)}+(_{t}^{(i)}+)^{-1/2}_{t} (_{t})^{T}_{t}^{(i)}-(_{t}^{(i)}+)^{-1/2}_{t}^{( i)}.\]

Detailed derivations can be found in Appendix 6.5. Here we follow Shampoo  to add the \(\) before taking the matrix square root, which is slightly different from the Adagrad formulation shown in (1).

Our overall algorithm is summarized in Algorithm 1. The time complexity of the algorithm is dominated by the matrix products in power iterations, \((mk^{3}+dk+bk^{2})\). The memory complexity is \((bk+mk^{2})\) for storing the shared basis and the \(k k\) dense coefficient matrices of each block.

For simplicity, the above discussions are mainly for two-dimensional matrices. For handling higher-order tensor parameters, such as the multi-head self-attention layer in Transformers, we adopt a grouping strategy based on one of the axes. Further discussions can be found in Appendix 6.1.

```  Initialize: coefficients \(_{0}^{(i)}=\), shared bases \(_{0}=\) for\(t=1 T\)do  Compute stochastic gradient \(_{t}\)  Obtain shared basis \(_{t}\) by running randomized SVD (Eq. (5)) for \(_{t}\)  Compute \(}_{t}^{(i)}=(_{t})^{T}_{t}^{(i)}_{t}\)  Compute \(_{t}^{(i)}=}_{t}^{(i)}-_{}(}_ {t}^{(i)})\) for \(i=1,,m\)  Compute \(_{t}^{(i)}=_{t-1}^{(i)}+\|_{t}^{(i)}-_{t}_{t}^{(i)} (_{t})^{T}\|\) for \(i=1,,m\)  Compute the preconditioned update \(_{t}^{(i)}=-(_{t}_{t}^{(i)}(_{t})^{T}+(_{t}^{(i)}+ ))^{-1/2}_{t}^{(i)}\) for \(i=1,,m\)  Update model parameters \(_{t}=_{t-1}+_{t}_{t}\) endfor ```

**Algorithm 1** Shared-Basis Low Rank Block-Diagonal Adagrad

### Regret Bound Analysis

Following previous works [16; 11], we provide a convergence analysis of the proposed algorithm in the online convex optimization framework [18; 28]. In the online convex optimization setting, a parameter \(_{t}\) is chosen iteratively, where \(\) is a convex decision set. After the decision of \(_{t}\), a convex loss function \(f_{t}\) is revealed, which can be selected adversarially. The regret suffered by the algorithm up to step \(T\) is defined as

\[_{T}=_{t=1}^{T}f_{t}(_{t})-_{ K} _{t=1}^{T}f_{t}().\]

We give the following theorem for our proposed method. All the proofs can be found in the Appendix.

**Theorem 1**: _Algorithm 1 gives the following regret_

\[_{T}D_{i=1}^{m}((_{ T}^{(i)})^{1/2})+b_{i=1}^{m}(_{T}^{(i)})^{1/2},\]

_where \(D\) is the diameter of the constraint set \(\), and \(_{T}^{(i)}=_{t=1}^{T}_{t}^{(i)}(_{t}^{(i)})^{T}\)._

To bound the second term in Theorem 1, we further derive the following theorem.

**Theorem 2**: _Algorithm 1 guarantees_

\[_{i=1}^{m}(_{T}^{(i)})^{1/2}(3m^{3})^{1/4}( _{i=1}^{m}(_{T}^{(i)}))^{1/2},\]

_and thus \(_{T}=()\)._

This shows that our proposed method is optimal asymptotically in terms of \(T\). With some further assumptions, we can further derive bounds of \(_{T}^{(i)}\) in terms of the lower eigenvalues of \(_{T}^{(i)}\). We defer the results to the Appendix 6.8.

### Comparison with Other Methods

We compare the time and space complexity of the proposed algorithm with other second-order methods. We consider the time and memory cost for preconditioning a neural network layer of size \(d_{} d_{}\). The results are summarized in Table 1.

As mentioned earlier in Section 2, the full matrix version of Adagrad  poses a significant challenge in practice due to the storage and matrix inversion of a full preconditioner with dimensions \(d_{}^{2} d_{}^{2}\). To address this issue, previous studies such as [31; 22] proposed low-rank approximation for the preconditioner, which reduces the memory cost to \((d_{}d_{}k)\), with \(k\) denoting the rank. However, even with this reduced cost, it remains impractical for modern neural network architectures.

Inspired from the observed empirical block-diagonal structure [7; 26; 32] propose a block diagonal variant of Adagrad and Adam. However, due to the space requirements, storing the block diagonal preconditioner with a block size of \(b\) incurs a memory complexity of \((db)\). As a result, they are limited to using small block sizes in practice. Similarly,  proposes a mini-block Fisher method. For convolutional layers, they group parameters based on filters, resulting in a block size of \(r^{2}\) when the kernel size is \(r\). In fully-connected layers, parameter grouping is based on output neurons, yielding a block size of \((d_{})\) and a memory complexity of \((dd_{})\). Additionally, they propose maintaining the average of the block diagonal for larger layers, resulting in an approximation of \(I R\), where \(R\) represents the average of all diagonal blocks. This approach resembles one side of the Kronecker product methods in the next paragraph, but their research demonstrates some performance loss associated with the approximation.

Another popular approach, such as Shampoo  and K-fac , involves grouping parameters based on both the input and output neurons, resulting in a Kronecker product approximation \(L R\) for the preconditioner. In order to enhance the time and space complexity of Shampoo,  introduces a technique called blocking, which partitions the parameters into smaller blocks. On the other hand,  proposes Sketchy as a low-rank variant of Shampoo.

Based on the above discussion, except for Sketchy, all other methods exhibit higher memory complexity compared to our method. Under the same \(k\), our memory complexity is larger than Sketchy. This is due to the fact that we are approximating the individual diagonal blocks, but Sketchy only approximates the average of the diagonal blocks (but in both orders). As a result, Sketchy fails to capture heterogeneity between different diagonal blocks. In our experimental evaluation, we demonstrate that when employing the same memory usage (by varying the value of \(k\)), our proposed method outperforms Sketchy.

## 3 Experimental Results

We evaluate the performance using a standard Autoencoder benchmark  on the MNIST dataset  and a larger Transformer model  on the Universal Dependencies dataset .

We adopt \(k=32\) as the default rank for our methods. For randomized SVD, we set the oversampling parameter to \(0\) and the number of iterations to \(1\). The impact of these parameters is detailed in our ablation study in Appendix 6.10. Similar to Shampoo, we use the grafting technique  in our method. We set the grafting type to RMSPROP_NORMALIZED. We compare the proposed optimizer with the widely used Adam optimizer  along with the following second-order optimizers:

* Shampoo : we adopt the implementation of  and keep most of the default settings with the following differences. Following the setting of , for Shampoo, the preconditioning starts at step 101 for stability and the preconditioners are updated every 10 steps for speed (preconditioning_compute_steps=10). Furthermore, we explore different block sizes for Shampoo, as suggested by . This approach involves partitioning the parameters into smaller blocks to improve both speed and memory utilization. We denote Shampoo with block size \(x\) as Shampoo(\(x\)).
* Sketchy : this is a low-rank approximation version of Shampoo. We follow the setting in their paper to use rank 256 and block size 1024.
* Block Adam : we implement the idea of block-diagonal approximation with a smaller block proposed in . We group the blocks based on output neurons, so we chose the block size to be a divisor of \(d_{}\). Specifically, a block size of 10 is used for the autoencoder benchmark and a block size of 8 is used for the Transformer benchmark.

We conduct hyperparameter tuning with random hyperparameter search over the search space defined in the appendix. Note that we search over one minus momentum instead of searching momentum directly. Each hyperparameter is sampled from a log-uniform distribution. For the autoencoder benchmark, we conduct 180 trials of random search on one NVIDIA RTX 2080Ti GPU with 11GB memory. For the Transformer benchmark, we conduct 60 trials of random search on one NVIDIA RTX A6000 GPU with 48GB memory.

### Autoencoder Benchmark

This autoencoder benchmark has been commonly used to evaluate various optimizers in the literature [23; 3], and we follow the same setting in previous papers. Specifically, we conduct 100 epochs of training using the MNIST dataset. The layer sizes of the autoencoder are \(\), with a total number of 2.83M learnable parameters. We use tanh as the non-linearity. The batch size is 1000. A linear warmup of 5 epochs is used for learning rate scheduling followed by a linear decay to 0.

In Table 2, we observe that the proposed method has better performance than all the methods except for Shampoo(1000). Compared to Shampoo(1000), our performance is similar while using \(3.2\) lower time, and \(1.5\) lower memory. The training loss curve can be seen in Figure 3.

### Transformer Benchmark

For the Transformer benchmark, we adopt the part-of-speech tagging task from the Google flax repository , which uses the ancient Greek data from the Universal Dependency data sets . The model has 6 Transformer layers. The embedding dimension size is 512 and the MLP dimension size is

   Algorithm & Time Complexity & Space Complexity \\  Full Matrix Adagrad  & \(d_{}^{3}d_{}^{3}\) & \(d_{}^{2}d_{}^{2}\) \\ SON-FD  & \(d_{}d_{}k^{2}\) & \(d_{}d_{}k\) \\ Ada-FD  & \(d_{}d_{}k^{2}\) & \(d_{}d_{}k^{2}\) \\ Shampoo  & \(d_{}^{3}+d_{}^{3}\) & \(d_{}^{2}+d_{}^{2}\) \\ Block Shampoo  & \(d_{}d_{}b\) & \(d_{}d_{}\) \\ Sketchy  & \(d_{}d_{}k\) & \(d_{}k+d_{}k\) \\ Block Diagonal Adam  & \(d_{}d_{}b^{2}\) & \(d_{}d_{}b\) \\ Ours & \(d_{}d_{}k+d_{}k^{2}+d_{}k^{3}\) & \(d_{}k+d_{}k^{2}\) \\   

Table 1: Time and space complexity comparison for a single parameter matrix of size \(d_{} d_{}\).

2048. For the self-attention layers, the number of heads is 8, and the query, key, value dimensions are 512. The total number of learnable parameters is 19.3M. We use the default square root learning rate decay scheduling but shorten the training period to 200 epochs and the warmup period to 40 epochs. The batch size is 128. By default, Shampoo excludes the embedding layer from preconditioning as one of its dimensions exceeds \(4096\). We also follow this setting for a fair comparison, even though our method can be used to precondition the embedding layer due to the reduced memory complexity. We adopt the direction obtained from sgd with momentum for the embedding layer.

In Table 3, we can see that our proposed method performs the best. Compared to Shampoo(1024), we use \(3.5\) lower time with less memory to obtain \( 1.8\%\) improvement in the validation error. The training loss curve and the validation accuracy curve can be seen in Figure 4, where the proposed method outperforms others in terms of both raw training time and number of training steps.

## 4 Related Work

Second order methods.For the optimization of ill-conditioned functions, second order methods are often more effective than first order methods. This includes Newton's method and the quasi-Newton methods . Subsampled versions of these methods  have also been proposed for solving problems of a larger scale. However, for deep learning problems, the memory requirements associated with these methods become prohibitive. To address this problem, the majority of existing literature preconditions each layer separately. Kronecker product approximation  was proposed to reduce the time and space complexity and has shown promising results on several large-scale training tasks. Further, several other approximation schemes have been proposed to exploit second order information, as discussed in Section 2.4.

   Optimizer & Adam & Block Adam & Shampoo(512) & Shampoo(1024) & Sketchy & Ours \\  Validation Accuracy & 67.36 & 70.43 & 68.79 & 68.81 & 68.32 & **70.63** \\ Time (min) & **123** & 193 & 325 & 564 & 717 & 161 \\ Memory (MB) & **19157** & 21598 & 19161 & 21209 & 19563 & 19553 \\   

Table 3: Experimental results on the Transformer benchmark.

Figure 3: Train loss on the autoencoder benchmark. We run Adam for 400 epochs so that it takes roughly the same time as the other methods.

   Optimizer & Adam & Block Adam & Shampoo(512) & Shampoo(1024) & Sketchy & Ours \\  Training Loss & 54.66 & 51.78 & 51.56 & **51.22** & 52.41 & 51.49 \\ Time (s) & **50** & 230 & 165 & 553 & 1998 & 140 \\ Memory (MB) & **1593** & 2619 & 2649 & 3641 & 2799 & 1777 \\   

Table 2: Experimental results on the autoencoder benchmark.

Frequent direction method.Given a stream of vectors, the frequent direction method  maintains a low-rank sketch matrix to approximate the covariance matrix of these vectors. The frequent direction method is proved to be optimal in space complexity. This suggests we can approximate the covariance matrix decently even when the vectors are given in a streaming manner. The frequent direction method has been used in previous work for approximating the preconditioner matrix in stochastic optimization .

## 5 Conclusions

We have introduced a novel adaptive optimizer that utilizes a shared-basis block low-rank approximation to approximate the full matrix preconditioner. Through evaluations on an autoencoder and a Transformer benchmark, our proposed optimizer has demonstrated superior performance compared to existing first and second-order optimizers, while incurring small memory and time overhead. We also provided convergence guarantees for the proposed optimizer.

## Limitation

The space complexity of our proposed method scales quadratically with \(k\), the rank used in the block low-rank approximation. While our empirical results indicate that \(k=32\) is adequate for the Autoencoder and Transformer benchmarks examined in our study, this may still pose a burden when training large-scale models. Exploring methods to reduce the dependence on \(k\) while preserving performance will be an interesting future direction.