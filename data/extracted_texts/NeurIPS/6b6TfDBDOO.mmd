# Diffusion Imitation from Observation

Bo-Ruei Huang Chun-Kai Yang Chun-Mao Lai Dai-Jie Wu Shao-Hua Sun

Department of Electrical Engineering, National Taiwan University

###### Abstract

Learning from observation (LfO) aims to imitate experts by learning from state-only demonstrations without requiring action labels. Existing adversarial imitation learning approaches learn a generator agent policy to produce state transitions that are indistinguishable to a discriminator that learns to classify agent and expert state transitions. Despite its simplicity in formulation, these methods are often sensitive to hyperparameters and brittle to train. Motivated by the recent success of diffusion models in generative modeling, we propose to integrate a diffusion model into the adversarial imitation learning from observation framework. Specifically, we employ a diffusion model to capture expert and agent transitions by generating the next state, given the current state. Then, we reformulate the learning objective to train the diffusion model as a binary classifier and use it to provide "realness" rewards for policy learning. Our proposed framework, Diffusion Imitation from Observation (DIFO), demonstrates superior performance in various continuous control domains, including navigation, locomotion, manipulation, and games. Project page: https://nturobotlearninglab.github.io/DIFO

## 1 Introduction

Learning from demonstration (LfD) [26; 46; 51; 63; 86] aims to acquire policies that can perform desired skills by imitating expert trajectories represented as sequences of state-action pairs, eliminating the necessity of reward functions. Recent advancements in LfD have enabled the deployment of reliable and robust learned policies in various domains, such as robot learning [17; 20; 28; 30], strategy games [22; 47; 71], and self-driving [7; 8; 62; 64]. LfD's dependence on accurately labeled actions remains a substantial limitation, particularly in scenarios where obtaining expert actions is challenging or costly. Moreover, most LfD methods assume that the demonstrator and imitator share the same embodiment, inherently preventing cross-embodiment imitation.

To address these issues, learning from observation (LfO) methods [66; 76; 85] seek to imitate experts from state-only sequences, thereby removing the need for action labels and allowing learning from experts with different embodiments. Schmidt and Jiang , Torabi et al. , Yang et al.  proposed learning inverse dynamic models (IDMs) that can infer action labels from state sequences and subsequently reformulate LfO as LfD. Nevertheless, acquiring sufficiently aligned data with the expert's data distribution to train IDMs remains an unresolved challenge. On the other hand, adversarial imitation learning (AIL) [32; 76; 81] employs a generator policy learning to imitate an expert, while a discriminator differentiates between the data produced by the policy and the actual expert data. Despite its simplicity in formulation, AIL methods can be brittle to learn and are often sensitive to hyperparameters [2; 13].

Recent works have explored leveraging diffusion models' ability in generative modeling and achieved encouraging results in imitation learning and planning [29; 31; 44]. For example, diffusion policies [6; 49] learn to denoise actions with injected noises conditioned on states, allowing for modeling multimodal expert behaviors. Moreover, Chen et al.  proposed to model expert state-action pairs with a diffusion model and then provide gradients to train a behavioral cloning policy to improveits generalizability. Nevertheless, these works require action labels, fundamentally limiting their applicability to learning from observation.

In this work, we introduce Diffusion Imitation from Observation (DIFO), a novel adversarial imitation learning from observation method that employs a diffusion model as a discriminator to provide rewards for policy learning. Specifically, we design a diffusion model that learns to capture expert and agent state transitions by generating the subsequent state conditioning on the current state. We reformulate the denoising objective of diffusion models as a binary classification task, allowing for the diffusion model to distinguish expert and agent transitions. Then, provided with the "realness" rewards from the diffusion model, the policy imitates the expert by producing transitions that look indistinguishable from expert transitions.

We compare our method DIFO to various existing LfO methods in various continuous control domains, including navigation, locomotion, manipulation, and games. The experimental results show that DIFO consistently exhibits superior performance. Moreover, DIFO demonstrates better data efficiency. The visualized learned reward function and generated state distributions verify the effectiveness of our proposed learning objective for the diffusion model.

## 2 Related work

**Learning from demonstration (LfD).** LfD approaches imitate experts from collected demonstrations, consisting of state and action sequences. Behavioral cloning (BC) [51; 70] formulates LfD as a supervised learning problem by learning a state-to-action mapping. Inverse reinforcement learning (IRL) [1; 45; 60] extracts a reward function from demonstrations and uses it to learn a policy through reinforcement learning. In contrast, this work aims to learn from state-only demonstrations, requiring no action label.

**Learning from observation (LfO).** LfO [11; 73] learns from state-only demonstrations, _i.e_., state sequences, making it suitable for scenarios where action labels are unobservable or costly to obtain, and allowing for learning from experts with a different embodiment. To tackle LfO, one popular direction is to learn an inverse dynamics model (IDM) for an agent that can recover an action for a pair of consecutive states [66; 75; 82]. However, there is no apparent mechanism to efficiently collect tuples of state, next state, and action that align with the expert state sequences, which makes it difficult to learn a good IDM. On the other hand, adversarial imitation learning from observation (AILfO) [23; 37; 54; 76] resemble the idea of generative adversarial networks (GANs) , where an agent generator policy is rewarded by a discriminator learning to distinguish the expert state transitions from the agent state transitions. Despite the encouraging results, the AILfO trainings are often brittle and sensitive to hyperparameters [2; 13]. Recent works also use generative models to predict state transitions and use the prediction to guide policy learning using log-likelihood , ELBO , or conditional entropy . However, these methods depend highly on the accuracy of the generative models. In contrast, our work aims to improve the sample efficiency and robustness of AILfO by employing a diffusion model as a discriminator.

**Learning from video (Lfv).** Extending from LfO, Lfv specifically considers learning from image-based states, _i.e_., videos, by leveraging recent advancements in computer vision, _e.g_., multi-view learning , image and video comprehension and generation [3; 12; 16; 33; 43; 65; 68], foundation models [9; 42], and optical flow and tracking [31; 80]. Yet, these methods are mostly specifically designed for learning from videos, and cannot be trivially adapted for vectorized states.

**Diffusion models.** Diffusion models are state-of-the-art generative models capable of capturing and generating high-dimensional data distributions [27; 72]. Diffusion models have been widely adopted for generating images [56; 61], videos , 3D structures , and speech [41; 53]. Recent works also have explored using the ability to model multimodal distributions of diffusion models for LfD [5; 6; 35; 49; 79], where expert demonstrations could exhibit significant variability . Our work aims to employ the capability of diffusion models for improving AIRLfO.

## 3 Preliminary

### Learning from observation

Consider environments represented as a Markov decision process (MDP) defined as a tuple \((,,r,,_{0},)\) of state space \(\), action space \(\), reward function \(r(s,a,s^{})\), transition dynamics \((s^{}|s,a)\), initial state distribution \(_{0}\) and discounting factor \(\). We define a policy \((a|s)\) that takes actions from state inputs and generates trajectories \(=(s_{0},a_{0},s_{1},,s_{||})\). The policy is trained to maximize the sum of discounted rewards \(_{(s_{0},a_{0},,s_{||})}[_{i=0}^ {||-1}^{i}r(s_{i},a_{i},s_{i+1})]\).

In imitation learning, the environment rewards cannot be observed. Instead, a set of expert demonstrations \(_{E}=\{_{0},,_{N}|_{i}_{E}\}\) is given, which generated by unknown expert policy \(_{E}\). We aim to learn the agent policy \(_{A}\) to generate a similar trajectory distribution with expert demonstrations. Moreover, in the learning from observation (Lfo) setting, where expert action labels are absent, agents learn exclusively from state-only observations represented by sequences \(=(s_{0},s_{1},,s_{||})\). We use the Lfo setting in this work.

**Inverse reinforcement learning (IRL).** One of the general approaches to imitation learning is IRL. This approach learns a reward function \(r\) from transitions, _i.e._, \((s,a)\) in LfD or \((s,s^{})\) in Lfo, that maximizes the reward of expert transitions and minimizes that of agent transitions. The learned reward function can thereby be used for reinforcement learning to train the policy to imitate expert.

### Denoising Diffusion Probabilistic Models

Diffusion models have emerged as state-of-the-art generative models capable of producing high-dimensional data and modeling multimodal distributions. Our work leverages the Denoising Diffusion Probabilistic Model (DDPM) , a latent variable model that generates data through a denoising process. The training procedure of the diffusion model consists of forward and reverse processes. In the forward process, Gaussian noise is progressively added to the clean data, following a predefined noise schedule. The process is formulated as \(_{t}=}}_{0}+}} \), where \(_{0}\) is the clean data, \(\) is the Gaussian noise, \(t\) denotes the time step within the whole process with step \(T\) and \(}\) is the scheduled noise level at the current time step. Conversely, the reverse process, denoted by \(p_{}(_{t-1}|_{t})\), is designed to reconstruct the original data by estimating the previously injected noise based on the given noise level. This is achieved by optimizing \(=_{t T,(0,1)} [\|-_{}(_{t},t) \|^{2}]\), where \(\) denotes the diffusion model.

## 4 Approach

We propose Diffusion Imitation from Observation (DIFO), a novel learning from observation framework integrating a diffusion model into the AIL framework, which is illustrated in Figure 1. Specifically, we utilize a diffusion model to model expert and agent state transitions; then, we learn an agent policy to imitate the expert via reinforcement learning by using the diffusion model to provide rewards based on how "real" agent state transitions are.

### Modeling expert transitions via diffusion model

Motivated by the recent success in using diffusion models for generative modeling, we use a conditional diffusion model to model expert state transitions. Specifically, given a state transition \((,^{})\), the diffusion model conditions on the current state \(\) and generates the next state \(^{}\). We adopt DDPM  and define the reverse process as \(p_{}(^{}{}_{t-1}|^{}{}_{t},)\), where \(t T\) and \(\) is the diffusion model, which is trained by minimizing the denoising MSE loss:

\[_{d}(,^{})=_{t T, (0,1)}[\|- _{}(}{}_{t},t|)\|^ {2}],\] (1)

where \(\) denotes the noise sampled from a Gaussian distribution and \(_{}\) denotes the noise predicted by the diffusion model. Once the diffusion model is trained, we can generate an expert next state conditioned on any given state by going through the diffusion generation process.

**State-distance reward.** To train a policy \(\) to imitate the expert from a given state \(\), we can first sample an action from the policy and obtain the next state \(_{}{}^{}\) by interacting with the environment. Next, we generate a predicted next state \(_{}{}^{}\) using the diffusion model. Then, to bring the state distribution of the policy closer to the expert's, we can optimize the policy using reinforcement learning by setting the distance of the two next states \(d(_{}{}^{},_{}{}^{})\) as a reward, where \(d\) denotes some distance function that evaluates how close two states are. However, a good distance function variesfrom one domain to another. Moreover, predicting the diffusion model next state \(}^{}\) can be very time-consuming since it requires \(T\) denoising steps.

**Denoising reward.** We aim to provide rewards for policy learning while avoiding choosing distance function and going through the diffusion generation process. To this end, we take inspiration from Li et al. , which shows that the denoising loss approximates the evidence lower bound (ELBO) of the likelihood. Our key insight is to leverage the denoising loss calculated from a state and the policy next state \(_{d}(,}^{})\), or \(_{d}\) in short, as an indicator of how well the policy next state fits the expert distribution. That said, a low \(_{d}\) means that the policy produces a next state close to the expert next state, while a high \(_{d}\) means that the diffusion model does not recognize this policy next state. Hence, we can use \(-_{d}\) as reward to learn a policy to imitate the expert by taking actions to produce next states that can be recognized by the diffusion model. Note that this denoising reward can be computed using a single denoising step.

### Diffusion model as a discriminator

The previous section describes how we can use the denoising loss as a reward for policy learning via reinforcement learning. However, the policy can learn to exploit a frozen diffusion model by discovering states that lead to a low denoising loss while being drastically different from expert states. To mitigate this issue, we incorporate principles from the AIL framework by training the diffusion model to recognize both the transitions from the expert and agent. To this end, we additionally condition the model on a binary label \(c\{c_{E},c_{A}\}\), where \(c_{E}\) represents the expert label and \(c_{A}\) represents the agent label, both implemented as one-hot encoding, resulting in the following denoising losses given a state transition \((,^{})\):

\[_{d}^{E}(,^{}) =_{t T,(0,1)} [\|-_{}(^{},t|,c_{E})\|^{2}],\] (2) \[_{d}^{A}(,^{}) =_{t T,(0,1)} [\|-_{}(^{},t|,c_{A})\|^{2}].\] (3)

With this formulation and an optimized diffusion model, an expert transition should yield a low \(_{d}^{E}\) and a high \(_{d}^{A}\), while an agent transition should yield a high \(_{d}^{E}\) and a low \(_{d}^{A}\). Thus, we construct a diffusion discriminator that can determine if a transition is close to expert as follows:

\[_{}(,^{})=(_{}( _{d}^{A}(,^{})-_{d}^{E}( ,^{}))),\] (4)

Figure 1: **Diffusion Imitation from Observation (DIFO). We propose Diffusion Imitation from Observation (DIFO), a novel adversarial imitation learning from observation framework employing a conditional diffusion model. (a) Learning diffusion discriminator. In the _discriminator step_ the diffusion model learns to model a state transition \((,^{})\) by conditioning on the current state \(\) and generates the next state \(^{}\). With the additional condition on binary expert and agent labels (\(c_{E}/c_{A}\)), we construct the diffusion discriminator to distinguish expert and agent transitions by leveraging the single-step denoising loss as a likelihood approximation. (b) Learning policy with diffusion **reward.** In the _policy step_, we optimize the policy with reinforcement learning according to rewards calculated based on the diffusion discriminator’s output \((1-_{}(,^{}))\).

where \(\) is the sigmoid function for normalization and \(_{}\) is a hyperparameter to control the sensitivity. To turn this diffusion discriminator as a binary classifier to classify agent and expert transitions, we train it to optimize the binary cross entropy (BCE):

\[_{}=_{(,^{}) _{E}}[(1-_{}(,^{})) ]+_{(,^{})_{A}}[( _{}(,^{}))].\] (5)

By optimizing \(_{}\), online interactions with the agent are leveraged as negative samples. Given expert transitions, the model should minimize \(_{d}^{E}\) and maximize \(_{d}^{A}\), resulting in a higher score closer to 1. Conversely, when the input is sampled from the agent, the model aims to maximize \(_{d}^{E}\) and minimize \(_{d}^{A}\), outputting a lower score closer to 0. The higher the score is, the more likely a transition is expert. Hence, we can learn a policy to imitate the expert using \(_{}\) as rewards. In contrast to MLP binary discriminators used in existing AIL works like GAIL, which maps high-dimensional inputs to a one-dimensional logit, our diffusion discriminator learns to predict high-dimensional noise patterns. This is inherently more challenging to overfit, addressing one of the key instabilities in GAIL.

### Diffusion Imitation from Observation

We present Diffusion Imitation from Observation (DIFO) an adversarial imitation learning from observation framework that trains a policy and a discriminator in turns. In the _discriminator step_, the discriminator learns to classify expert and agent transition by optimizing \(_{}\). Furthermore, to ensure the diffusion loss of expert data is optimized so that it approximates the ELBO, the diffusion model also optimizes \(_{d}^{E}\) by sampling from expert demonstrations. 1

\[_{}=_{t T, (0,1),(,^{})_{E}}[\| -_{}(^{}{}_ {t},t|,c_{E})\|^{2}],\] (6)

resulting in the overall objective:

\[_{D}=_{}_{}+_{ {BCE}}_{},\] (7)

where \(_{}\) and \(_{}\) are hyperparameters adjusting the importance of each term. In the _policy step_, to provide the policy rewards based on the "realness" \(_{}\) of the agent transitions, we adopt the GAIL reward function :

\[r_{}(,^{})=(1-_{}(,^{})),\] (8)

where \(_{}\) is computed with a single denoising step. We justify the feasibility of sampling only one denoising step in Section 5.8. We can optimize the policy using any RL algorithm. The DIFO framework is illustrated in Figure 1 and the algorithm is presented in Appendix A.

## 5 Experiments

### Environments

In this section, we introduce environments, tasks, and how expert demonstrations are collected. All environment trajectories, except CarRacing, are fixed-horizon to prevent biased information about success . Further details can be found in Appendix B.

* **PointMaze**: A navigation task for a 2-DoF agent with the medium maze, see Figure 1(a). A point agent is trained to navigate from an initial position to a goal. The goal and initial position of the agent are randomly sampled. The agent observes its position, velocity, and goal position. The agent applies linear forces in the \(x\) and \(y\) directions to navigate the maze and reach the goal. We collect \(60\) demonstrations (\(36\,000\) transitions) using a controller from Fu et al. .
* **AntMaze**: A task containing both locomotion and navigation, which presents a significantly more challenging variant of the **PointMaze**, as shown in Figure 1(b). The quadruped and learns to navigate from an initial position to a goal by controlling the torque of its legs, where both the goal and initial position of the ball are also randomly sampled. Notice that this environment serves as a high-dimensional state space task with 29-dimension state space. We use \(100\) demonstrations (\(7000\) transitions) from Minari .

* **FetchPush**: The goal is to control a 7-DoF Fetch robot arm to push a block to a target position on a table, see Figure 2c. Both the block and target positions are randomly sampled. The robot is controlled by small displacements of the gripper in XYZ coordinates, which has a 28-dimension state space and a 4-dimension action space. We generate 50 demonstrations (\(2500\) transitions) using an expert policy trained by SAC .
* **AdroitDoor**: A manipulation task to undo the latch and swing the door open, see Figure 2d. The position of the door is randomly placed. It is based on the Adroit manipulation platform , with 39-dimension state space and 28-dimension action space containing all the joints. It serves as a high-dimensional state and action space task. We use 50 demonstrations (\(10\,000\) transitions) from the dataset released by Fu et al. .
* **Walker**: A locomotion task of a 6-DoF Walker2D in MuJoCo , as shown in Figure 2e. The goal is to walk forward by applying torques on the six hinges. Initial joint states are added with uniform noise. We generate \(1000\) transitions using an expert policy trained by SAC .
* **OpenMicrowave**: A manipulation task to control a 9-DoF Franka robot arm to open the microwave door, as shown in Figure 2f. The environment has a 59-dimension state space and a 9-dimension continuous action space to control the angular velocity of each joint. It serves as a high-dimensional state and action space task. We use 5 demonstrations (\(300\) transitions) from the dataset released by Fu et al. .
* **CarRacing**: An image-based control task aimed at directing a car to complete a track as quickly as possible. Observations consist of top-down frames, as shown in Figure 2g. Tracks are generated randomly in every episode. The car has continuous action space to control the throttle, steering, and breaking. We generate 340 transitions using an expert policy trained by PPO .
* **CloseDrawer**: An image-based manipulation task from Meta-World  requires the agent to control a Sawyer robot arm to close a drawer. Observations consist of fixed perspective frames, as shown in Figure 2h. The robot has continuous action space to control the gripper in XYZ coordinates, and the initial poses of the robot and the drawer are randomized in every episode. We generate 100 transitions using a scripted policy.

Figure 2: **Environments & tasks. (a) PointMaze: A point agent (green) is trained to navigate to the goal (red). (b) AntMaze: A high-dimensional locomotion navigation task for an 8-DoF quadruped ant to navigate to the goal (red). (c) FetchPush: A manipulation task to move a block (yellow) to the target (red). (d) AdroitDoor: A high-dimension manipulation task to undo the latch and swing the door open. (e) Walker: A locomotion task for a 6-DoF hopper to maintain at the highest speed while keeping balance. (f) OpenMicrowave: A manipulation task to control the robot arm to open the microwave with joint space control. (g) CarRacing: An image-based task to control the car to complete the track in the shortest time. (h) CloseDrawer: An image-based manipulation task to control the robot arm to close the drawer.**

### Baselines and variants

We compare our method DIFO with the following baselines:

* **Behavioral Cloning (BC)** learns a state-to-action mapping using supervised learning without any interaction with the environment. Note that BC is the only baseline having privileged access to ground truth action labels.
* **Behavioral Cloning from Observation (BCO)** first learns an inverse dynamic model through self-supervised exploration, and uses it to reconstruct action from state-only observation. BCO then uses these action labels to perform behavioral cloning.
* **Generative Adversarial Imitation from Observation (GAIIO)**, trains a GAIL MLP discriminator taking state transitions \((s,s^{})\) as input, instead of state-action pairs \((s,a)\).
* **Wasserstein Adversarial Imitation from Observation (WAIIO)** is a LfO variant of WAIL , taking \((s,s^{})\) as input. WAIL replaces the learning objective of the discriminator from Jensen-Shannon divergence (GAIL) to Wasserstein distance.
* **Adverserial Inverse Reinforcement Learning from Observation (AIRLfO)** is a LfO variant of AIRL . AIRL modifies the discriminator output to disentangle task-relevant information from transition dynamics. Similarly to GAIfO, AIRLfO takes \((s,s^{})\) as input instead of \((s,a)\).
* **Decoupled Policy Optimization (DePO)** decouples the policy into a high-level state planner and an inverse dynamics model, utilizing embedded decoupled policy gradient and generative adversarial training.
* **Inverse soft-Q Learning for Imitation (IQ-Learn)** directly learns a policy in Q-space from demonstrations without explicit reward construction. We use the state-only setting for LfO.
* **Optimal Transport (OT)** derives a proxy reward function for RL by measuring the distance between probability distributions. We use the state-only setting for LfO.

In addition to the existing methods, we also compare DIFO with its variants:

* **DIFO-Non-Adversarial (DIFO-NA)** follows the method introduced in Section 4.1, which first pretrains a conditional diffusion model on expert demonstrations, and simply takes the denoising reward \(-_{d}(s,s^{})\) for policy training.

Figure 3: **Learning performance and efficiency. We evaluate all the methods with five random seeds and report their success rates in PointMaze, AntMaze, FetchPush, AdroitDoor, OpenMicrowave, and CloseDrawer, and their returns in Walker, and CarRacing. The standard deviation is shown as the shaded area. Our proposed method, DIFO, demonstrates more stable and faster learning performance compared to the baselines.*** **DIFO-Unconditioned (DIFO-Uncond)** removes the condition on \(s\), and denoises both \(s\) and \(s^{}\). It is optimized only with \(_{}\). Namely, replacing the MLP discriminator with a diffusion discriminator from GAIFO. It serves as a baseline showing the effect of network architecture.

### Experimental results

We report the success rates in PointMaze, AntMaze, FetchPush, and AdroitDoor, and return in Walker and CarRacing of all the methods in Figure 3. Each method is reported with the mean value and standard deviation with five random seeds for all the tasks. BC's performance is shown as horizontal lines since BC does not leverage environmental interactions. The expert's performance (gray horizontal lines) in goal-directed tasks, _i.e._, PointMaze, AntMaze, FetchPush, AdroitDoor, is \(100\%\). More details of training and evaluation can be found in Appendix G.

Our proposed method DIFO consistently outperforms or matches the performance of the best-performing baseline in all the tasks, highlighting the effectiveness of integrating a conditional diffusion model into the AIL framework. In AntMaze, AdroitDoor, and CarRacing, DIFO outperforms the baselines and converges significantly faster, indicating its efficiency in modeling expert behavior and providing effective rewards even in high-dimensional state and action spaces. Moreover, DIFO presents more stable training results, with relatively low variance compared to other AIL methods. Notably, although BC has access to action labels, it still fails in most tasks with more randomness. This is because BC relies solely on learning from the observed expert dataset, unlike the LfO methods that utilize online interaction with environments, BC is susceptible to covariate shifts [36; 58; 59] and requires a substantial amount of expert data to achieve coverage of the dataset. The result indicates the significance of online interactions. OT only successfully learns in environments like AdroitDoor, Walker, and CloseDrawer, where trajectory variety is limited. OT computes distances at the trajectory level rather than the transition level, which requires monotonic trajectories, making it struggle in tasks with diverse trajectories. In contrast, our method generates rewards at the transition level, allowing us to identify transition similarities even when facing substantial trajectory variability.

Variants of DIFO, _i.e._, DIFO-Uncond, and DIFO-NA, perform poorly in most tasks. DIFO-NA learns poorly in most of the tasks except CloseDrawer, underscoring diffusion loss could be a reasonable metric for the discriminator while it is still necessary to model agent online interaction data to prevent the diffusion model from being exploited by the policy. On the other hand, DIFO-Uncond performs comparably to other AIL baselines but shows instability across different tasks, this highlighting the importance of modeling transitions using a diffusion model.

We also verify DIFO's capability to model stochastic distribution in Appendix D.

### Data efficiency

To investigate the data efficiency, _i.e._, how much expert data is required for learning, we vary the number of expert trajectories in AntMaze and report the performance of all the methods in Figure 4. Specifically, we use \(200\), \(100\), \(50\), and \(25\) expert trajectories, each containing \(14\,000\), \(7000\), \(3500\), and \(1750\) transitions, respectively.

Figure 4: **Data efficiency. We vary the amount of available expert demonstrations in AntMaze. Our proposed method DIFO consistently outperforms other methods when the number of expert demonstrations decreases, highlighting the data efficiency of DIFO.**

The results demonstrate that DIFO learns faster compared to all the baselines with various amounts of demonstrations, highlighting its sample efficiency. Specifically, as the number of demonstrations decreases from \(200\) to \(50\), DIFO's performance drops modestly from an \(80\%\) success rate to \(70\%\), whereas WAILfo, the best-performing baseline when given \(200\) expert trajectories, experiences a substantial decline to a \(20\%\) success rate. Furthermore, when the number of demonstrations is reduced to \(25\), all other baselines fail to learn, with success rates nearing zero. In contrast, DIFO maintains a success rate of around \(20\%\), underscoring its superior data efficiency. This data efficiency highlights DIFO's potential for real-world applications, where collecting expert demonstrations can be costly.

### Generating data using diffusion models

To investigate whether the DIFO diffusion model can closely capture the expert distribution, we generate trajectories with the diffusion model in PointMaze. Specifically, we take a trained diffusion discriminator of DIFO and autoregressively generate a sequence of next states starting from an initial state sampled in the expert dataset. We visualize four pairs of expert trajectories and the corresponding generated trajectories in Figure 5.

The results show that our diffusion model can accurately generate trajectories similar to those of the expert. It is worth noting that the diffusion model can generate trajectories that differ from the expert trajectories while still completing the task, such as the example on the bottom right of Figure 5, where the diffusion model produces even shorter trajectories than the scripted expert policy. Additional expert trajectories and the corresponding generated trajectories are presented in Appendix E.

### Visualized learned reward functions

We aim to visualize and analyze the reward functions learned by DIFO. To this end, we introduce a toy environment Sine in which both the state and action space are 1-dimension with range \(\). We generate expert state-only demonstrations by sampling from the distribution \(s^{}= 6 s+s+(0,\,0.05^{2})\). The sampled expert state transitions are plotted in Figure 5(a).

**Reconstructed distribution.** Given the expert state distribution, we generate a distribution of next states using the diffusion model of DIFO and visualize the distribution in Figure 5(b). The generated distribution closely resembles the expert distribution, which again verifies the modeling capability of the conditional diffusion model.

**Visualized learned reward functions.** We visualize the reward functions learned by GAIfO and DIFO in Sine in Figure 5(c) and Figure 5(d), respectively. The result shows that the reward function

Figure 5: **Generated trajectories under PointMaze. The green point marks the initial state. The red point marks the goal. The blue trace represents the generated trajectory and the orange trace represents the corresponding expert trajectory.**

Figure 6: **Reward function visualization and generated distribution on Sine.****(a)** The expert state transition distribution. **(b)** The state transition distribution generated by the DIFO diffusion model. **(c-d)** The visualized reward functions learned by GAIfO and DIFO, respectively. DIFO produces smoother rewards outside of the expert distribution, allowing for facilitating policy learning.

learned by GAIfO drops dramatically once it deviates from expert distribution, while that of DIFO presents a smoother contour to the region outside the distribution, which allows for bringing a learning agent closer to the expert even when agent's behaviors are far from the expert behavior.

### Ablation study on \(_{}\) and \(_{}\)

We hypothesize that both \(_{}\) and \(_{}\) are important for efficiency learning. To examine the effect of \(_{}\) and \(_{}\) and verify the hypothesis, we vary the ratio of \(_{}\) and \(_{}\) in PointMaze and Walker, including \(_{}\) only and \(_{}\) only, _i.e_., \(_{}=0\) and \(_{}=0\). As shown in Figure 7, the results emphasize the significance of introducing both \(_{}\) and \(_{}\), since they enable the model to simultaneously model expert behavior (\(_{}\)) and perform binary classification (\(_{}\)). Without \(_{}\), the performance slightly decreases as it does not modeling expert behaviors. Without \(_{}\), the model fails to learn as it does not utilize negative samples, _i.e_., agent data. Moreover, when we vary the ratio of \(_{}\) and \(_{}\), DIFO maintains stable performance, demonstrating DIFO is relatively insensitive to hyperparameter variations.

### Ablation study on the number of samples for reward computation

To investigate the robustness of our rewards, we conducted experiments with varying numbers of denoising step samples in PointMaze and Walker. We take the mean of losses computed from different numbers of samples, _i.e_., multiple \(t\), to compute rewards. As presented in the Figure 8, the performance of DIFO is stable under different numbers of samples. As a result, we use a single denoising step sample to compute the reward for the best efficiency. We also investigate the stability of rewards under different numbers of samples in Appendix F.

## 6 Conclusion

We present Diffusion Imitation from Observation (DIFO), a novel adversarial imitation learning from observation framework. DIFO leverages a conditional diffusion model as a discriminator to distinguish expert state transitions from those of the agent, while the agent policy learns to produce state transitions that are indistinguishable from the expert's for the diffusion discriminator. Experimental results demonstrate that DIFO outperforms existing learning from observation methods, including BCO, GAIfO, WAIfO, AIRLfO, DePO, OT, and IQ-Learn, across continuous control tasks in various domains, including navigation, manipulation, locomotion, and image-based games. The visualization of the reward function learned by DIFO shows that it can generalize well to states unseen from expert state transitions by utilizing agent data. Also, the DIFO diffusion model is able to accurately capture expert state transitions and can generate predicted trajectories that are similar to those of expert's.