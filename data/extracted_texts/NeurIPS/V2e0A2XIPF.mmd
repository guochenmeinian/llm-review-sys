# QT-ViT: Improving Linear Attention in ViT with Quadratic Taylor Expansion

Yixing Xu, Chao Li, Dong Li, Xiao Sheng, Fan Jiang, Lu Tian, Emad Barsoum

Advanced Micro Devices, Inc., Beijing, China

{yixing.xu, chao.li, d.li, xsheng, f.jiang, lu.tian, emad.barsoum}@amd.com

###### Abstract

Vision transformer model (ViT) is widely used and performs well in vision tasks due to its ability to capture long-range dependencies. However, the time complexity and memory consumption increase quadratically with the number of input patches which limits the usage of ViT in real-world applications. Previous methods have employed linear attention to mitigate the complexity of the original self-attention mechanism at the expense of effectiveness. In this paper, we propose QT-ViT models that improve the previous linear self-attention using quadratic Taylor expansion. Specifically, we substitute the softmax-based attention with second-order Taylor expansion, and then accelerate the quadratic expansion by reducing the time complexity with a fast approximation algorithm. The proposed method capitalizes on the property of quadratic expansion to achieve superior performance while employing linear approximation for fast inference. Compared to previous studies of linear attention, our approach does not necessitate knowledge distillation or high-order attention residuals to facilitate the training process. Extensive experiments demonstrate the efficiency and effectiveness of the proposed QT-ViTs, showcasing the state-of-the-art results. Particularly, the proposed QT-ViTs consistently surpass the previous SOTA EfficientViTs under different model sizes, and achieve a new Pareto-front in terms of accuracy and speed.

## 1 Introduction

Compared to convolutional neural networks (CNNs), vision transformers (ViTs) are getting more and more attention due to their strong performance across various computer vision tasks, such as image classification , object detection , semantic segmentation  and low-level vision . The effectiveness of ViT comes from the multi-head self-attention (MHSA) operation that allows the model to capture long-range information by calculating the attention score between each pair of patches. However, this mechanism necessitates quadratic time and storage complexity \((n^{2})\) related to the number of input patches \(n\), and the original ViTs require significant computational and storage resources when applied to real-world applications.

To overcome the aforementioned problem, previous researches focus on improving the original self-attention mechanism by using local attention such as window attention , dilated attention  and random attention . Another family of methods is to utilize linear attention  that decomposes the original softmax function into two non-linear kernels so that the order of matrix multiplications in attention score calculation is changed to reduce the quadratic computational complexity into a linear one. Many papers focus on designing non-linear kernels and novel linear-attention architectures for better approximation, _e.g.,_ Hydra-attention  uses the hydra trick to their multi-head attention by setting as many heads as features. Performer  uses fast attention via a positive orthogonal random features approach to approximate the softmax attention. EfficientViT  replaces softmax with ReLU non-linear activation and applies depthwise and group convolution toimprove its performance. Flatten transformer  utilizes focused attention based on ReLU to force the attention operation to focus on more informative regions.

Previous linear attention methods reduce the complexity of the attention mechanism from \((n^{2}d)\) to \((nd^{2})\) at the expense of the performance on visual tasks, where \(d\) is the patch dimensionality. Some of them necessitate the knowledge distillation method  or high-order attention residuals  to make up for the performance gap. However, the GPU memory consumption will severely increase which makes these methods unsuitable for training large transformer models.

In this paper, we explore the utilization of second-order (quadratic) Taylor expansion to approximate the original softmax attention. We theoretically show that this approximation can be decomposed into two non-linear kernels through the utilization of the Kronecker product . By employing this approach, the computational complexity can be changed from \((n^{2}d)\) to \((nd^{3})\). We then propose a fast approximation algorithm to accelerate the computation of the Kronecker product, thereby reducing the complexity to \((nd^{2})\). In contrast to the first-order (linear) Taylor expansion  and other linear attention methods, we can utilize the high-order information within the softmax function to achieve superior performance while at the same time preserving the efficiency of linear attention. Experimental results on the ImageNet dataset show that the proposed QT-ViTs can achieve a superior accuracy-speed trade-off when compared to other state-of-the-art methods, as shown in Fig. 1. Additionally, we conduct experiments on object detection and semantic segmentation tasks to further validate the effectiveness of our approach.

## 2 Preliminaries

In this section, we first introduce the preliminaries of softmax attention and linear attention. Then, we provide an overview of various instantiations of the original linear attention method used in ViT and analyze their advantages.

Figure 1: The accuracy-speed trade-offs of the proposed QT-ViTs and other state-of-the-art transformer models on the ImageNet dataset. Latencies are evaluated on the AMD Instinct MI250 GPU.

### Softmax Self-Attention

Softmax self-attention operation is the key component in the transformer model. Given an input matrix \(^{N d}\) where \(N\) is the number of patches and \(d\) is the dimension of each patch, we first map the input matrix to the query, key and value embeddings by using the matrix multiplications:

\[=_{Q},\ \ \ =_{K},\ \ \ =_{V},\] (1)

where \(_{Q}\), \(_{K}\) and \(_{V}^{d d}\) are learnable matrices. Then, the attention score is computed on each pair of patches to capture the global information as shown below:

\[_{k}=_{i=1}^{N}(_{k},_{i}) }{_{j=1}^{N}(_{k},_{j})}_{i}= _{i=1}^{N}_{k}_{i}^{}/)}{_ {j=1}^{N}(_{k}_{j}^{}/)}_{i},\] (2)

where \((_{k},_{i})=(_{k}_{i} ^{}/)\) is the similarity measurement function in the softmax attention, \(_{k}\), \(_{i}\) (\(_{j}\)), \(_{i}\), \(_{k}\) are the corresponding \(k\)-th, \(i\)-th (\(j\)-th), \(i\)-th, \(k\)-th row vectors of the query, key, value and output matrices, respectively. The inner product of the query-key pair is first computed to calculate the similarity between the pair, then a scale is applied for stability and a softmax function is used to transfer the similarity into probability. This probability is applied to the value matrix to get the final attention score output. The softmax attention computes the inner products of all the query-key pairs and results in a \((N^{2}d)\) time complexity.

### Linear Self-Attention

The overhead of the computation of Eq. 2 mainly comes from the matrix multiplication. By decomposing the similarity function into two separate kernel embeddings, _i.e._, \((_{k},_{i})=(_{k})(_{i})^{}\), and the original softmax attention function can be changed into linear attention by exchanging the order of matrix multiplication:

\[_{k}=_{i=1}^{N}_{k})(_{i})^{ }}{_{j=1}^{N}(_{k})(_{j})^{}}_{i}=_{k})(_{i=1}^{N}(_{i})^{ }_{i})}{(_{k})(_{j=1}^{N}(_{j})^{})},\] (3)

where the complexity is changed from \((N^{2}d)\) to \((Nd^{2})\). Since the patch dimension \(d\) is always smaller than the number of patches \(N\) in the popular ViT architectures, the computation overhead can thus be reduced.

However, in order to losslessly decompose the similarity function in the softmax attention \((_{k},_{i})\) into the product of two kernel embeddings \((_{k})\) and \((_{i})\), the dimensionality of the kernel function needs to be infinite which is unable to apply to real-world applications. Thus, a series of instantiations are proposed trying to compute \(()\) efficiently while preserving as much information of the original similarity function as possible.

In the following, we use \(_{}\) and \(_{}\) to represent row vectors in query matrix \(\) and key matrix \(\) that do not belong to any specific row.

### Instantiations of the Kernel Function

Linear transformer was first proposed in  and \(()=()+1\) was used as the kernel function. EfficientViT  used \(()=()\) as the instantiation. Both methods ensure that \(()()^{} 0\) which is consistent with the property of the similarity measurement function \(()\). Flatten Transformer  argued that previous approximations smooth the distribution of linear attention which failed to focus on more informative regions, and proposed a focused function \(_{p}()=()||}{||( )^{*p}||}()^{**p}\) where \(||||\) represents the Euclidean norm and \(()^{**p}\) is element-wise power \(p\) of the input. Hydra attention  used cosine similarity as the kernel \(()=/||||_{2}\). PolyNL  used mean kernel \(()=/\), and AFT-Simple  proposed different functions for query \(()=()\) and key \(()=()\), respectively. These methods suffered from the performance drop since they lacked sufficient expression ability to replicate the original softmax attention mechanism.

Besides the aforementioned methods, some studies approximated the similarity function with kernel expansions such as angular kernel expansion  with \((,)=1/2+1/(^{})+H _{r}\) or first order Taylor expansion  with \((,)=1+^{}/+H_{r}\) where \(H_{r}\) represents the high-order residuals. These methods necessitated the masked output of original softmax attention as \(H_{r}\) and applied the knowledge distillation (KD) method to further enhance the performance which severely increased the GPU memory consumption and were unsuitable for training large transformer models.

## 3 Methods

In this section, we propose to use second-order (quadratic) Taylor expansion to approximate the similarity measurement function \((,)\)in Eq. 2. Compared to the first-order (linear) Taylor expansion , quadratic approximation contains less information in the high-order residuals. Therefore, we can directly ignore them and derive a good performance without utilizing masked softmax attention output or the KD method.

However, it is non-trivial to decompose the quadratic Taylor expansion into separate kernel embeddings with linear time complexity. Thus, in the following we first give a theoretical derivation by using the Kronecker product to decompose the quadratic expansion. Then, a fast approximation algorithm is applied to accelerate the computation of the Kronecker product.

### Decompose Quadratic Taylor Expansion

The quadratic Taylor expansion of the similarity measurement function is expressed as:

\[(,)=(,>}{})  1+,>}{}+,>^{2}}{2d}\] \[=,>}{}+1)^{ 2}+1}{2}\] \[=),()>^{2}+1}{2},\] (4)

where \(<,>\) is the dot product and \(()=[}{},1]\) is used for vectors \(\) and \(\). However, since the quadratic term exists in Eq. 4, it is challenging to decompose the equation into two separate kernel embeddings. In the following, we show that this problem can be solved by using the Kronecker product.

Given two vectors \(=\{a_{i}\}_{i=1}^{d}\) and \(=\{b_{i}\}_{i=1}^{d}\), we can easily derive:

\[<,>^{2}=(_{i=1}^{d}a_{i}b_{i})^{2}=_{i= 1}^{d}a_{i}^{2}b_{i}^{2}+2_{i=1}^{d-1}_{j=i+1}^{d}a_{i}b_{i}a_{j}b_{j}.\] (5)

This is equal to first computing the Kronecker product of each vector and then applying dot product, _i.e.,_ given \(K_{r}()=()\) where \(\) represents the Kronecker product and \(()\) is the vectorized output, we have:

\[<K_{r}(),K_{r}()>= [a_{1},,a_{d}][b_{1} ,,b_{d}]\] \[= [a_{1}a_{1},,a_{1}a_{d},a_{2}a_{1},,a_{2}a_{d },,a_{d}a_{1},,a_{d}a_{d}]\] \[[b_{1}b_{1},,b_{1}b_{d},b_{2}b_{1}, ,b_{2}b_{d},,b_{d}b_{1},,b_{d}b_{d}]\] \[= _{i=1}^{d}a_{i}^{2}b_{i}^{2}+2_{i=1}^{d-1}_{j=i+1}^{ d}a_{i}b_{i}a_{j}b_{j}\] \[= <,>^{2}.\] (6)Then, we can apply Eq. 6 to Eq. 4 and decompose the similarity function into two separate kernel embeddings:

\[(,) ),()>^{2}+1}{2}\] \[=((),K_{r}(())>+1}{2}\] \[=<(),()>,\] (7)

where

\[()=[}K_{r}(()),}]=[}(() ()),}]\] (8)

is the kernel function applied to the query and key vectors. Note that given a vector \(^{d}\), Kronecker product gives an output vector with quadratic length \(K_{r}()^{d^{2}}\). Thus, the time complexity of linear attention using the decomposed quadratic Taylor expansion is \((Nd^{3})\). Compared to the original softmax attention with \((N^{2}d)\) time complexity, the proposed method does not yield an advantage.

### Reduce the Time Complexity

Recall that the computational burden primarily arises from the Kronecker product that quadratically expands the input dimension. Thus, there are several simple ways to reduce the dimension. For example, a pooling function can be applied on the input vector \(=()^{d/p}\) where \(p\) is the dimensionality reduction factor. The output dimension of the Kronecker product can be reduced to \(K_{r}()^{d^{2}/p^{2}}\), and the corresponding time complexity of the linear attention is \((Nd^{3}/p^{2})\). Another way is to divide the input vector into \(c\) chunks \(=[^{1},,^{c}]\) and compute the Kronecker product within each chunk \(K_{r}(^{i})^{d^{2}/c^{2}}\), and finally concatenate them together to derive the output \(=(K_{r}(^{1}),,K_{r}( ^{c}))^{d^{2}/c}\). The time complexity of the linear attention using this method is \((Nd^{3}/c)\).

Although methods mentioned above can decrease the computational load, they do not actually reduce the time complexity. In the following, we propose a fast approximation algorithm to accelerate the computation of the Kronecker product, and reduce the computational complexity from \((Nd^{3})\) to \((Nd^{2})\).

By rewriting the definition of \(K_{r}(())\) in Eq. 8 in its element-wise form, we can get:

\[K_{r}(()) =K_{r}([}{[d]{d}},1])\] \[=[}{[d]{d}}[}{[d] {d}},1],\ ,}{[d]{d}}[}{[d]{d}},1],\ [}{[d]{d}},1]]\] \[=[\{x_{1}}{[d]{d}},,x_{ d}}{[d]{d}},,\{x_{1}}{[d]{d}},,\{x_{ d}}{[d]{d}},}{[d]{d}},1\}}].\] (9)

Note that the order of the elements in the above equation does not influence the result of the inner product \(<K_{r}((),K_{r}(())>\) in Eq. 7 as long as \(K_{r}(())\) and \(K_{r}(())\) change the order of their elements in the same manner. Thus, Eq. 9 can be written as:

\[_{r}(())=(x_{j}\}_{ i,j=1}^{d}}{[d]{d}},\}_{i=1}^{d}}{[d]{d}},1),\] (10)

which is divided into four terms. The first is the quadratic term that contains \(d^{2}\) components representing the multiplication of each pair of elements in \(\) (including self-multiplication), the second and third terms are the linear term with length \(d\) each, and the fourth term is the constant term. Since the computational load of the inner product in Eq. 10 mainly comes from the quadratic term, it is important to reduce the number of elements in this term. Randomly preserving \(d\) items from \(d^{2}\) elements is an efficient approach but leads to poor results. Employing grouping techniques help selecting the most representative items at the cost of increasing the computational complexity compared to random selection. We empirically find that using the self-multiplication terms \(\{x_{i}^{2}\}_{i=1}^{d}\) can effectively represent all quadratic terms, while at the same time maintaining high efficiency.

Therefore, the Kronecker product in Eq. 10 can be replaced with a compact version:

\[_{r}(()) =(^{2}\}_{i=1} ^{d}}{},\}_{i=1}^{d}}{[d]{d}}, )\] \[=(\{x_{i}^{2}\}_{i=1}^{d}, [d]{}\{x_{i}\}_{i=1}^{d},),\] (11)

in which we merge items of the same kind and multiply them by the square root of the number of the same items so as not to affect the inner-product result in Eq. 7. Learnable scalar parameters \(,\) and \(\) are used as the trade-off parameters. Note that given a vector \(^{d}\), this compact version of the Kronecker product gives an output of length \(2d+1\). Therefore, the time complexity of linear attention using the decomposed quadratic Taylor expansion is reduced from \((Nd^{3})\) to \((Nd^{2})\). We further found that the linear term can be discarded without hurting the classification performance, thus we set \(=0\) in the following experiments.

## 4 Experiments

In this section, we apply our linear attention with quadratic Taylor expansion to vision transformers and propose a series of QT-ViT models. We empirically investigate the effectiveness and efficiency of the proposed models on the ImageNet-1k classification dataset. Additional results regarding the performance on object detection and semantic segmentation tasks are provided in the appendices.

### Image Classification

**Datasets and model architectures.** The ImageNet-1k classification dataset is used for training and evaluation, which contains 1.28M training images and 50K validation images from 1000 different classes. We utilize the model architecture proposed in EfficientViT  and replace the kernel function with our proposed compact quadratic Taylor expansion kernel. An absolute positional embedding is added to the key matrix before applying linear attention, and a non-linear shortcut \(o=o+(())\) is added to the output of the linear attention \(o\) where \(v\) is the value matrix. Different exponential moving average (EMA) decay parameters are used, and all the other training settings and hyper-parameters remain the same.

**Compared methods and evaluation metrics.** To verify the effectiveness of the proposed QT-ViTs, we compare our method with a series of competitors including (1) Vision transformers with linear attention such as ViTALiTy , Castling-ViT , EfficientViT , FLatten Transformer  and Hydra Attention ViT ; (2) Other vision transformers with sparse attention or hierarchical architectures such as Swin , SwinV2 , FasterViT , PoolFormer , MobileViT , MobileViTV2  and CSwin ; (3) State-of-the-art CNN models and CNN-Transformer combined model architectures such as CoAtNet , CMT , ConvNeXt , EfficientNet  and EfficientNetV2 .

The proposed QT-ViTs and other baseline models are evaluated based on the accuracy-speed trade-offs as shown in Fig. 1. Furthermore, we measure the classification performance with top-1/top-5 accuracy. The efficiency of the model is represented by the FLOPs and parameters. Finally, we evaluate the inference speed of the models on the AMD Instinct MI250 GPU in Fig. 1.

**Experimental results.** The effectiveness and efficiency of the proposed QT-ViTs are evaluated on the ImageNet-1k dataset by comparing them to other state-of-the-art baseline methods mentioned above. The results are shown in Tab. 1 and all methods are gathered by their FLOPs into five groups including: <1G, 1\(\)3G, 3\(\)5G, 5\(\)10G and >10G.

As shown in the table, the proposed QT-ViTs achieve new SOTA accuracy-efficiency trade-off across different FLOPs range. For example, we outperform ViTALiTy who uses the first-order Taylor expansion by a large margin without using knowledge distillation or high-order residuals that severely increase the GPU memory consumption during training. Compared to vision transformer with sparse attention such as CSwin, our QT-ViT-4 achieves 84.7% top-1 accuracy with only 5.26G FLOPs while CSwin-B has 84.2% top-1 accuracy with 15.00G FLOPs, which means that we have 0.5% higher top-1accuracy with 64.9% less FLOPs. For CNN competitors, the QT-ViT-3 outperforms ConvNeXt-T by 1.8% top-1 accuracy with 11.8% less FLOPs. Finally, compared to the current state-of-the-art vision transformer model, the proposed QT-ViT-1\(\)6 outperforms EfficientViT-B1\(\)B3 & L1\(\)L3 by 0.2%, 0.4%, 0.4%, 0.2%, 0.1%, 0.2% respectively with roughly the same FLOPs and parameters. The accuracy-speed trade-offs of the proposed QT-ViTs and other models are shown in Fig. 1.

  FLOPs & Model & Parameters & FLOPs & Top-1 Acc & Top-5 Acc \\ range & Architecture & (M) & (G) & (\%) & (\%) \\    & ViTALiTy-DeiT-T  & - & 0.33 & 71.9 & - \\  & EfficientNet-B1  & 7.8 & 0.70 & 79.1 & 94.4 \\  & PoolFormer-S12  & 11.9 & 1.82 & 77.2 & - \\  & EfficientViT-B1  & 9.1 & 0.52 & 79.4 & 94.3 \\  & CMT  & 9.5 & 0.60 & 79.1 & 94.5 \\  & MobileViT-XS  & 2.3 & 0.70 & 74.8 & 92.3 \\  & MobileViTV2-0.5  & 1.4 & 0.50 & 70.2 & - \\  & QT-ViT-1 (ours) & 9.4 & 0.52 & **79.6** & 94.7 \\    & Castling-DeiT-T  & 5.6 & 1.18 & 76.0 & 92.5 \\  & EfficientNet-B3  & 12.0 & 1.80 & 81.6 & 95.7 \\  & EfficientViT-B2  & 24.3 & 1.60 & 82.1 & 95.8 \\  & FLatten-PVT-T  & 12.2 & 2.00 & 77.8 & - \\  & QT-ViT-2 (ours) & 24.9 & 1.60 & **82.5** & 95.9 \\   5G} & PoolFormer-S24  & 21.4 & 3.40 & 80.3 & - \\  & EfficientNet-B4  & 19.0 & 4.20 & 82.9 & 96.4 \\  & Swin-T  & 29.0 & 4.50 & 81.3 & 95.5 \\  & EfficientViT-B3  & 49.0 & 4.00 & 83.5 & 96.4 \\  & FasterViT-1  & 53.4 & 5.30 & 83.2 & 96.5 \\  & ConvNxCt-T  & 29.0 & 4.50 & 82.1 & - \\  & QT-ViT-3 (ours) & 49.7 & 3.97 & **83.9** & 96.7 \\   10G} & PoolFormer-M36  & 56.2 & 8.78 & 82.1 & - \\  & EfficientNet-B5  & 30.0 & 9.90 & 83.6 & 96.7 \\  & EfficientNetV2-S  & 22.0 & 8.40 & 83.9 & - \\  & SwinV2-T  & 28.0 & 6.60 & 82.8 & - \\  & EfficientViT-L1  & 53.0 & 5.30 & 84.5 & 96.9 \\  & EfficientViT-L2  & 64.0 & 6.96 & 85.1 & 97.0 \\  & Castling-MViTv2-B  & 51.9 & 9.82 & 85.0 & 97.2 \\  & FasterViT-2  & 75.9 & 8.70 & 84.2 & 96.8 \\  & QT-ViT-4 (ours) & 53.0 & 5.26 & 84.7 & 96.7 \\  & QT-ViT-5 (ours) & 64.1 & 6.96 & **85.2** & 97.0 \\    & PoolFormer-M48  & 73.5 & 11.56 & 82.5 & - \\  & CSWin-B  & 78.0 & 15.00 & 84.2 & - \\  & EfficientViT-L3  & 246.0 & 28.00 & 85.8 & 97.2 \\  & Castling-DeiT-B  & 87.2 & 17.28 & 84.2 & - \\  & Hydra-DeiT-B  & - & 17.46 & 80.6 & - \\  & FLatten-CSwin-B  & 75.0 & 15.00 & 84.5 & - \\  & SwinV2-B  & 88.0 & 21.80 & 84.6 & - \\  & CoAtNet-3  & 168.0 & 35.00 & 84.5 & - \\  & FasterViT-4  & 424.6 & 36.60 & 85.4 & 97.3 \\  & QT-ViT-6 (ours) & 246.8 & 27.60 & **86.0** & 97.3 \\   

Table 1: Image classification results on ImageNet-1k dataset. QT-ViTs are compared with SOTA baselines. Methods are grouped based on FLOPs.

### Ablation Study

In this section, we conduct several ablation studies to further verify the superiority of our proposed quadratic Taylor expansion kernel.

**Results of using different kernels.** We compare the results of using quadratic Taylor expansion kernel with other kernels used in various linear attention vision transformers including EfficientViT  which uses ReLU kernel, Hydra attention  that utilizes cosine kernel, PolyNL  with the mean kernel, AFT-Simple  that proposes different kernels for query and key, angular kernel expansion  and first order Taylor expansion . All methods use the same training settings and network architecture except the kernel used for computing the linear self-attention. The baseline method uses the original self-attention operation with \((N^{2}d)\) computational complexity and is used as a strong baseline for comparison.

As the results shown in Tab. 2, the proposed quadratic (2nd order) Taylor expansion outperforms all other competitors which demonstrates the effectiveness of the proposed method. For example, we achieve a 1.1% better top-1 accuracy compared to the linear (1st order) Taylor expansion kernel, 0.2% better than ReLU, 0.5% better than cosine, 0.8% better than mean, 0.7% better than sigmoid & softmax and 0.5% better than the angular kernel.

**Ablation on reducing the time complexity of the Kronecker product.** In Sec. 3.2, we mentioned several ways of reducing the computational burden of the Kronecker product including:

_Method 1:_ applying a pooling function on the input vector \()}^{d/p}\);

   Method & Kernel & \(()\) & \(()\) & Top-1 Acc (\%) \\  baseline & - & - & - & 79.8 \\  EfficientViT  & ReLU non-linearity & \((x)\) & 79.4 \\ Hydra  & cosine similarity & \(x/||x||_{2}\) & 79.1 \\ PolyNL  & mean & \(x/\) & 78.8 \\ AFT-Simple  & sigmoid \& softmax & \((x)\) softmax\((x)\) & 78.9 \\ Castling-ViT  & angular kernel & \((,)=+( ^{})\) & 79.1 \\ ViTALiTy  & 1st order Taylor expansion & \([x/[d]{d},1]\) & 78.5 \\ QT-ViT (ours) & 2nd order Taylor expansion & \([}_{r}(()),}]\) & **79.6** \\   

Table 2: Results of using different kernels. The baseline method uses the original self-attention operation with \((N^{2}d)\) computational complexity and is used as the strong baseline. Other methods use different linear attentions.

   Method & Hyper-param & Time Comp. & Params (M) & FLOPs (G) & Top-1 Acc (\%) \\  baseline & - & \((Nd^{3})\) & 9.4 & 0.65 & 79.7 \\ 
1 &  p=2 \\ p=4 \\  & \((Nd^{3}/p^{2})\) &  9.4 \\ 9.4 \\  &  0.55 \\ 0.52 \\  & 
 79.3 \\ 79.1 \\  \\ 
2 &  c=2 \\ c=4 \\ c=8 \\  & \((Nd^{3}/c)\) &  9.4 \\ 9.4 \\  &  0.55 \\ 0.55 \\ 0.53 \\  & 
 79.3 \\ 79.1 \\  \\ 
3 & - & \((Nd^{2})\) & 9.4 & 0.52 & 61.8 \\ 
4 (ours) & - & \((Nd^{2})\) & 9.4 & 0.52 & **79.6** \\   

Table 3: Ablation on reducing the time complexity of the Kronecker product. The experiments are conducted using the QT-ViT-1 model on the ImageNet-1k dataset.

_Method 2:_ dividing the input vector into \(c\) chunks, compute the Kronecker product within each chunk and concatenate them together to derive the final output;

_Method 3:_ randomly preserving \(d\) items from \(d^{2}\) quadratic elements in \(_{r}(())\) (Eq. 10);

_Method 4:_ using the self-multiplication terms to represent all quadratic terms to derive the compact version of the original Kronecker product \(_{r}(())\) (Eq. 11).

The classification results of using different methods mentioned above are shown in Tab. 3, in which the baseline method computes the original Kronecker product and is used to compare with other efficient methods. We can see that reducing input dimension with the pooling function (method 1) or dividing it into chunks (method 2) are not efficient enough since \(p\) and \(c\) are small compared to the dimension \(d\) of the vector and has a higher time complexity. Besides, the classification performances are not satisfying because too much information is lost. Randomly selecting quadratic items (method 3) is computationally friendly but has sub-optimal performance. The proposed compact version of the original Kronecker product (method 4) performs best among all the methods which indicates that using the self-multiplication terms to represent quadratic terms is enough to preserve the information in the output of the Kronecker product.

### Visualization

We plot the results of the self-attention maps from the last block given a specific query (column 1, marked as red on the original images) using different attention methods including first-order Taylor expansion  (column 2), ReLU non-linearity function  (column 3) and the quadratic Taylor expansion used in the proposed QT-ViT (column 4). We can see that the proposed QT-ViT can exhibit a more focused and sharper response on attention feature maps. Furthermore, given a query vector, QT-ViT captures reasonable features on the feature map more accurately compared to the competitors. For example, QT-ViT concentrates on both ears of the dog given the query on the left ear of the dog. We can intuitively observe the advantages of QT-ViT from Fig. 2.

## 5 Conclusion

In this paper, we propose a new linear attention method to approximate the usage of softmax self-attention in the original vision transformer models. By conducting quadratic Taylor expansion of the similarity measurement function with the help of the Kronecker product, we can successfully decompose the similarity function into the product of two kernel embeddings while reserving high-order information and maintaining the effectiveness of the original self-attention. Furthermore, we propose a fast approximation algorithm to accelerate the computation of the Kronecker product and reduce the time complexity from \((Nd^{3})\) to \((Nd^{2})\) without much loss of information. We conduct experiments on the proposed QT-ViT models using the benchmark dataset ImageNet-1k,

Figure 2: Attention maps from different linear attention methods including the first-order Taylor expansion, ReLU non-linearity function and the second-order Taylor expansion (ours).

and the results show that we can achieve a better accuracy-efficiency trade-off compared to other state-of-the-art transformers and CNNs.