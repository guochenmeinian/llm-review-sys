# Two-timescale Derivative Free Optimization for

Performative Prediction with Markovian Data

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper studies the performative prediction problem where a learner aims to minimize the expected loss with a decision-dependent data distribution. Such setting is motivated when outcomes can be affected by the prediction model, e.g., in strategic classification. We consider a state-dependent setting where the data distribution evolves according to an underlying controlled Markov chain. We focus on stochastic derivative free optimization (DFO) where the learner is given access to a loss function evaluation oracle with the above Markovian data. We propose a two-timescale DFO(\(\)) algorithm that features (i) a sample accumulation mechanism that utilizes every observed sample to estimate the overall gradient of performative risk, and (ii) a two-timescale diminishing step size that balances the rates of DFO updates and bias reduction. Under a general non-convex optimization setting, we show that DFO(\(\)) requires \((1/^{3})\) samples (up to a log factor) to attain a near-stationary solution with expected squared gradient norm less than \(>0\). Numerical experiments verify our analysis.

## 1 Introduction

Consider the following stochastic optimization problem with decision-dependent data:

\[_{^{d}}\ ()=_{Z _{}}(;Z).\] (1)

Notice that the decision variable \(\) appears in both the loss function \((;Z)\) and the data distribution \(_{}\) supported on Z. The overall loss function \(()\) is known as the _performative risk_ which captures the distributional shift due to changes in the deployed model. This setting is motivated by the recent studies on _performative prediction_(Perdomo et al., 2020), which considers outcomes that are supported by the deployed model \(\) under training. For example, this models strategic classification (Hardt et al., 2016; Dong et al., 2018) in economical and financial practices such as with the training of loan classifier for customers who may react to the deployed model \(\) to maximize their gains; or in price promotion mechanism (Zhang et al., 2018) where customers react to prices with the aim of gaining a lower price; or in ride sharing business (Narang et al., 2022) with customers who adjust their demand according to prices set by the platform.

The objective function \(()\) is non-convex in general due to the effects of \(\) on both the loss function and distribution. Numerous efforts have been focused on characterizing and finding the so-called _performative stable_ solution which is a fixed point to the repeated risk minimization (RRM) process (Perdomo et al., 2020; Mendler-Dunner et al., 2020; Brown et al., 2022; Li and Wai, 2022; Roy et al., 2022; Drusvyatskiy and Xiao, 2022). While RRM might be a natural algorithm for scenarios when the learner is agnostic to the performative effects in the dynamic data distribution, the obtained solution maybe far from being optimal or stationary to (1).

On the other hand, recent works have studied _performative optimal_ solutions that minimizes (1). This is challenging due to the non-convexity of \(()\) and more importantly, the absence of knowledge of \(_{}\). In fact, evaluating \(()\) or its stochastic gradient estimate would require learning the distribution \(_{}\)_a-priori_(Izzo et al., 2021). To design a tractable procedure, prior works have assumed structures for (1) such as approximating \(_{}\) by Gaussian mixture (Izzo et al., 2021), \(_{}\) depends linearly on \(\)(Narang et al., 2022), etc., combined with a two-phase algorithm that separately learns \(_{}\) and optimizes \(\). Other works have assumed a _mixture dominance_ structure (Miller et al., 2021) on the combined effect of \(_{}\) and \(()\) on \(()\), which in turn implies that \(()\) is convex. Based on this assumption, a derivative free optimization (DFO) algorithm was analyzed in Ray et al. (2022).

This paper focuses on approximating the _performative optimal_ solution without relying on additional condition on the distribution \(_{}\) and/or using a two-phase algorithm. We concentrate on stochastic DFO algorithms (Ghadimi and Lan, 2013) which do not involve first order information (i.e., gradient) about \(()\). As an advantage, these algorithms avoid the need for estimating \(_{}\). Instead, the learner is given access to the loss function evaluation oracle \((;Z)\) and receive data samples from a controlled Markov chain. Note that the latter models the _stateful_ and _strategic_ agent setting considered in (Ray et al., 2022; Roy et al., 2022; Li and Wai, 2022; Brown et al., 2022). Such setting is motivated when the actual data distribution adapts slowly to the decision model, which will be announced by the learner during the (stochastic) optimization process.

The proposed \(()\) algorithm features (i) a two-timescale step sizes design to control the bias-variance tradeoff in the derivative-free gradient estimates, and (ii) a sample accumulation mechanism with forgetting factor \(\) that aggregates every observed samples to control the amount of error in gradient estimates. In addition to the new algorithm design, our main findings are summarized below:

* Under the Markovian data setting, we show in Theorem 3.1 that the \(()\) algorithm finds a near-stationary solution \(}\) with \([\|(})\|^{2}]\) using \((}{^{3}} 1/)\) samples/iterations. Compared to prior works, our analysis does not require structural assumption on the distribution \(_{}\) or convexity condition on the performative risk (Izzo et al., 2021; Miller et al., 2021; Ray et al., 2022).
* Our analysis demonstrates the trade-off induced by the forgetting factor \(\) in the DFO \(()\) algorithm. We identify the desiderata for the optimal value(s) of \(\). We show that increasing \(\) allows to reduce the number of samples required by the algorithm if the performative risk gradient has a small Lipschitz constant.

For the rest of this paper, SS2 describes the problem setup and the \(()\) algorithm, SS3 presents the main results, SS4 outlines the proofs. Finally, we provide numerical results to verify our findings in SS5.

Finally, as displayed in Table 1, we remark that stochastic DFO under _decision dependent_ (and Markovian) samples has a convergence rate of \((1/^{3})\) towards an \(\)-stationary point, which is worse than the decision independent setting that has \((1/^{2})\) in Ghadimi and Lan (2013). We believe that this is a fundamental limit for DFO-type algorithms when tackling problems with decision-dependent sample due to the challenges in designing a low variance gradient estimator; see SS4.1.

**Related Works**. The idea of DFO dates back to Nemirovskii (1983), and has been extensively studied thereafter Flaxman et al. (2005); Agarwal et al. (2010); Nesterov and Spokoiny (2017); Ghadimi and Lan (2013). Results on matching lower bound were established in (Jamieson et al., 2012). While a similar DFO framework is adopted in the current paper for performative prediction, our algorithm is limited to using a special design in the gradient estimator to avoid introducing unwanted biases.

There are only a few works considering the Markovian data setting in performative prediction. Brown et al. (2022) is the first paper to study the dynamic settings, where the response of agents to learner's deployed classifier is modeled as a function of classifier and the current distribution of the population; also see (Izzo et al., 2022). On the other hand, Li and Wai (2022); Roy et al. (2022) model the unforgetful nature and the reliance on past experiences of _single/batch_ agent(s) via controlled Markov Chain. Lastly, Ray et al. (2022) investigated the state-dependent framework where agents' response may be driven to best response at a geometric rate.

  
**Stochastic DFO Settings** & **Rate** \\  Decision-indep. & \((1/^{2})\) \\ (Ghadimi and Lan, 2013) & \\  Decision-depend. (Markov) & \((1/^{3})\) \\   

Table 1: Comparison of the expected convergence rates (to find an \(\)-stationary point) for DFO under various settings where DFO is used to tackle an unstructured non-convex optimization problem such as (1).

**Notations**: Let \(^{d}\) be the \(d\)-dimensional Euclidean space equipped with inner product \(,\) and induced norm \(\|x\|=\). Let \(\) be a (measurable) sample space, and \(\), \(\) are two probability measures defined on \(\). Then, we use \(_{}(,):=_{A}(A)-(A)\) to denote the total variation distance between \(\) and \(\). Denote \(_{}(,)\) as the state-dependent Markov kernel and its stationary distribution is \(_{}()\). Let \(^{d}\) and \(^{d-1}\) be the unit ball and its boundary (i.e., a unit sphere) centered around the origin in \(d\)-dimensional Euclidean space, respectively, and correspondingly, the ball and sphere of radius \(r>0\) are \(r^{d}\) and \(r^{d-1}\).

## 2 Problem Setup and Algorithm Design

In this section, we develop the \(()\) algorithm for tackling (1) and describe the problem setup. Assume that \(()\) is differentiable, we focus on finding an _\(\)-stationary_ solution, \(\), which satisfies

\[\|()\|^{2}.\] (2)

With the goal of reaching (2), there are two key challenges in our stochastic algorithm design: (i) to estimate the gradient \(()\), and (ii) to handle the _stateful_ setting where one cannot draw samples directly from the distribution \(_{}\). We shall discuss how the proposed \(()\) algorithm, which is summarized in Algorithm 1, tackles the above issues through utilizing two ingredients: (a) two-timescales step sizes, and (b) sample accumulation with the forgetting factor \([0,1)\).

**Estimating \(()\) via Two-timescales DFO.** First notice that the gradient of \(()\) can be derived as

\[()=_{Z_{ }}[(;Z)+(;Z)_{}_{}(Z)],\] (3)

As a result, constructing the stochastic estimates of \(()\) typically requires knowledge of \(_{}()\) which may not be known a-priori unless a separate estimation procedure is applied; see e.g., (Izzo et al., 2021). To avoid the need for direct evaluations of \(_{}_{}(Z)\), we consider an alternative design via zero-th order optimization (Ghadimi & Lan, 2013). The intuition comes from observing that with \( 0^{+}\), \((+)-( {})\) is an approximate of the directional derivative of \(\) along \(\). This suggests that an estimate for \(()\) can be constructed using the _objective function values_ of \((;Z)\) only.

Inspired by the above, we aim to construct a gradient estimate by querying \(()\) at randomly perturbed points. Formally, given the current iterate \(^{d}\) and a query radius \(>0\), we sample a vector \(^{d}\) uniformly from \(^{d-1}\). The zero-th order gradient estimator for \(()\) is then defined as

\[g_{}(;,Z):=( };Z)\,}:=+,\;Z_{ }}().\] (4)

In fact, as \(\) is zero-mean, \(g_{}(;,Z)\) is an unbiased estimator for \(_{}()\). Here, \(_{}()\) is a smooth approximation of \(()\)(Flaxman et al., 2005; Nesterov & Spokoiny, 2017) defined as

\[_{}()=_{}[ (})]=_{}[ _{Z_{}}}[(};Z)]].\] (5)

Furthermore, it is known that under mild condition [cf. Assumption 3.1 to be discussed later], \(\|_{}()-( {})\|=()\) and thus (4) is an \(()\)-biased estimate for \(()\).

We remark that the gradient estimator in (4) differs from the one used in classical works on DFO such as (Ghadimi and Lan, 2013). The latter takes the form of \(((;Z)-(;Z))\,\). Under the setting of standard stochastic optimization where the sample \(Z\) is drawn _independently_ of \(\) and Lipschitz continuous \((;Z)\), the said estimator in (Ghadimi and Lan, 2013) is shown to have constant variance while it remains \(()\)-biased. Such properties _cannot_ be transferred to (4) since \(Z\) is drawn from a distribution dependent on \(\) via \(}=+\). In this case, the two-point gradient estimator would become biased; see SS4.1.

However, we note that the variance of (4) would increase as \((1/^{2})\) when \( 0\), thus the parameter \(\) yields a bias-variance trade off in the estimator design. To remedy for the increase of variance, the \(()\) algorithm incorporates a _two-timescale step size_ design for generating gradient estimates \((_{k})\) and updating models (\(_{k}\)), respectively. Our design principle is such that the models are updated at a _slower timescale_ to adapt to the gradient estimator with \((1/^{2})\) variance. Particularly, we will set \(_{k+1}/_{k+1} 0\) to handle the bias-variance trade off, e.g., by setting \(>\) in line 4 of Algorithm 1.

**Markovian Data and Sample Accumulation.** We consider a setting where the sample/data distribution observed by the \(()\) algorithm evolves according to a _controlled Markov chain (MC)_. Notice that this describes a stateful agent(s) scenario such that the deployed models (\(\)) would require time to manifest their influence on the samples obtained; see (Li and Wai, 2022; Roy et al., 2022; Brown et al., 2022; Ray et al., 2022; Izzo et al., 2022).

To describe the setting formally, we denote \(_{}:_{+}\) as a Markov kernel controlled by a deployed model \(\). For a given \(\), the kernel has a unique stationary distribution \(_{}()\). Under this setting, suppose that the previous state/sample is \(Z\), the next sample follows the distribution \(Z^{}_{}(Z,)\) which is not necessarily the same as \(_{}()\). As a consequence, the gradient estimator (4) is not an unbiased estimator of \(_{}()\) since \(Z_{}()\) cannot be conveniently accessed.

A common strategy in settling the above issue is to allow a _burn-in_ phase in the algorithm as in (Ray et al., 2022); also commonly found in MCMC methods (Robert et al., 1999). Using the fact that \(_{}\) admits the stationary distribution \(_{}\), if one can wait a sufficiently long time before applying the current sample, i.e., consider initializing with the previous sample \(Z^{(0)}=Z\), the procedure

\[Z^{(m)}_{}(Z^{(m-1)},),\ m=1,,,\] (6)

would yield a sample \(Z^{+}=Z^{()}\) that admits a distribution close to \(_{}\) provided that \( 1\) is sufficiently large compared to the mixing time of \(_{}\).

Intuitively, the procedure (6) may be inefficient as a number of samples \(Z^{(1)},Z^{(2)},,Z^{(-1)}\) will be completely ignored at the end of each iteration. As a remedy, the \(()\) algorithm incorporates a sample accumulation mechanism which gathers the gradient estimates generated from possibly non-stationary samples via a forgetting factor of \([0,1)\). Following (4), \(()\) is estimated by

\[=_{m=1}^{}^{-m}(^{( m)}+;Z^{(m)})\,,\ \ \ \ Z^{(m)}_{^{(m)}+}(Z^{(m-1)},).\] (7)

At a high level, the mechanism works by assigning large weights to samples that are close to the end of an epoch (which are less biased). Moreover, \(^{(m)}\) is _simultaneously updated_ within the epoch to obtain an online algorithm that gradually improves the objective value of (1). Note that with \(=0\), the \((0)\) algorithm reduces into one that utilizes _burn-in_ (6). We remark that from the implementation perspective for performative prediction, Algorithm 1 corresponds to a _greedy deployment_ scheme (Perdomo et al., 2020) as the latest model \(_{k}^{(m)}+_{k}_{k}\) is deployed at every sampling step. Line 6-10 of Algorithm 1 details the above procedure.

Lastly, we note that recent works have analyzed stochastic algorithms that rely on a _single trajectory_ of samples taken from a Markov Chain, e.g., (Sun et al., 2018; Karimi et al., 2019; Doan, 2022), that are based on stochastic gradient. Sun and Li (2019) considered a DFO algorithm for general optimization problems but the MC studied is not controlled by \(\).

## 3 Main Results

This section studies the convergence of the \(()\) algorithm and demonstrates that the latter finds an \(\)-stationary solution [cf. (2)] to (1). We first state the assumptions required for our analysis:

**Assumption 3.1**.: **(Smoothness)**_\(()\) is differentiable, and there exists a constant \(L>0\) such that_

\[\|()-(^{}) \| L\|-^{}\|,\ ,^{}^{d}.\]

**Assumption 3.2**.: **(Bounded Loss)** There exists a constant \(G>0\) such that

\[|(;z)| G,\;\;^{d},\;\;z .\]

**Assumption 3.3**.: **(Lipschitz Distribution Map)** There exists a constant \(L_{1}>0\) such that

\[_{}(_{_{1}},_{_{2}} ) L_{1}\|_{1}-_{2}\| _{1},_{2}^{d}.\]

The conditions above state that the gradient of the performative risk is Lipschitz continuous and the state-dependent distribution vary smoothly w.r.t. \(\). Note that Assumption 3.1 is found in recent works such as (Izzo et al., 2021; Ray et al., 2022), and Assumption 3.2 can be found in (Izzo et al., 2021). Assumption 3.3 is slightly strengthened from the Wasserstein-1 distance bound in (Perdomo et al., 2020), and it gives better control for distribution shift in our Markovian data setting.

Next, we consider the assumptions about the controlled Markov chain induced by \(_{}\):

**Assumption 3.4**.: **(Geometric Mixing)** Let \(\{Z_{k}\}_{k 0}\) denote a Markov Chain on the state space \(\) with transition kernel \(_{}\) and stationary measure \(_{}\). There exist constants \([0,1)\), \(M 0\), such that for any \(k 0\), \(z\),

\[_{}(_{}(Z_{k} |Z_{0}=z),_{}) M^{k}.\]

**Assumption 3.5**.: **(Smoothness of Markov Kernel)** There exists a constant \(L_{2} 0\) such that

\[_{}(_{_{1}}(z, ),_{_{2}}(z,)) L_{2}\|_{1}-_{2}\|,\;_{1},_{2} ^{d},\;z.\]

Assumption 3.4 is a standard condition on the mixing time of the Markov chain induced by \(_{}\); Assumption 3.5 imposes a smoothness condition on the Markov transition kernel \(_{}\) with respect to \(\). For instance, the geometric dynamically environment in Ray et al. (2022) constitutes a special case which satisfies the above conditions.

Unlike (Ray et al., 2022; Izzo et al., 2021; Miller et al., 2021), we do not impose any additional assumption (such as mixture dominance) other than Assumption 3.3 on \(_{}\). As a result, (1) remains an 'unstructured' non-convex optimization problem. Our main theoretical result on the convergence of the \(()\) algorithm towards a near-stationary solution of (1) is summarized as:

**Theorem 3.1**.: _Suppose Assumptions 3.1-3.5 hold, step size sequence \(\{_{k}\}_{k 1}\), and query radius sequence \(\{_{k}\}_{k 1}\) satisfy the following conditions,_

\[_{k}&=d^{-2/3}(1+k)^{-2/3}, _{k}=d^{1/3}(1+k)^{-1/6},\\ _{k}&=\{1,}(1+k)\} k 0.\] (8)

_Then, there exists constants \(t_{0}\), \(c_{5},c_{6},c_{7}\), such that for any \(T t_{0}\), the iterates \(\{_{k}\}_{k 0}\) generated by \(()\) satisfy the following inequality,_

\[_{0 k T}\|(_{k})\| ^{2} 12\{c_{5}(1-),c_{6},}{1-}\} }{(T+1)^{1/3}}.\] (9)

We have defined the following quantities and constants:

\[c_{5}=2G, c_{6}=,G^{2}(1-)\}}{1-2}, c_{7} =}{2-+1},\] (10)

with \(=,=\). Observe the following corollary on the iteration complexity of \(()\) algorithm:

**Corollary 3.1**.: _(\(\)-stationarity) Suppose that the Assumptions of Theorem 3.1 hold. Fix any \(>0\), the condition \(_{0 k T-1}\|(_{k}) \|^{2}\) holds whenever_

\[T(12\{c_{5}(1-),c_{6},}{1-}\} )^{3}}{^{3}}.\] (11)

In the corollary above, the lower bound on \(T\) is expressed in terms of the number of epochs that Algorithm 1 needs to achieve the target accuracy. Consequently, the total number of samples required (i.e., the number of inner iterations taken in Line 6-9 of Algorithm 1 across all epochs) is:

\[_{}=_{k=1}^{T}_{k}=( }{^{3}}(1/)).\] (12)We remark that due to the decision-dependent properties of the samples, the \(()\) algorithm exhibits a worse sampling complexity (12) than prior works in stochastic DFO algorithm, e.g., (Ghadimi and Lan, 2013) which shows a rate of \((d/^{2})\) on non-convex smooth objective functions. In particular, the adopted one-point gradient estimator in (4) admits a variance that can only be controlled by a time varying \(\); see the discussions in SS4.1.

Achieving the desired convergence rate requires setting \(_{k}=(k^{-2/3})\), \(_{k}=(k^{-1/6})\), i.e., yielding a two-timescale step sizes design with \(_{k}/_{k} 0\). Notice that the influence of forgetting factor \(\) are reflected in the constant factor of (9). Particularly, if \(c_{5}>c_{7}\) and \(c_{5} c_{6}\), the optimal choice is \(=1-}{c_{5}}}\), otherwise the optimal choice is \([0,1-c_{7}/c_{6}]\). Informally, this indicates that when the performative risk is smoother (i.e. its gradient has a small Lipschitz constant), a large \(\) can speed up the convergence of the algorithm; otherwise a smaller \(\) is preferable.

## 4 Proof Outline of Main Results

This section outlines the key steps in proving Theorem 3.1. Notice that analyzing the \(()\) algorithm is challenging due to the two-timescales step sizes and Markov chain samples with time varying kernel. Our analysis departs significantly from prior works such as (Ray et al., 2022; Izzo et al., 2021; Brown et al., 2022; Li and Wai, 2022) to handle the challenges above.

Let \(^{k}=(_{0},Z_{s}^{(m)},u_{s},0 s k,0 m _{k})\) be the filtration. Our first step is to exploit the smoothness of \(()\) to bound the squared norms of gradient. Observe that:

**Lemma 4.1**.: **(Decomposition)** _Under Assumption 3.1, it holds that_

\[_{k=0}^{t}\|(_{k})\|^{2 }_{1}(t)+_{2}(t)+_{3}(t)+_{4}(t),\] (13)

_for any \(t 1\), where_

\[_{1}(t) :=_{k=1}^{t}}([ (_{k})]-[(_{ k+1})])\] \[_{2}(t)\] \[_{3}(t)\] \[_{4}(t)\]

The lemma is achieved through the standard descent lemma implied by Assumption 3.1 and decomposing the upper bound on \(\|(_{k})\|^{2}\) into respectful terms; see the proof in Appendix A. Among the terms on the right hand side of (13), we note that \(_{1}(t),_{3}(t)\) and \(_{4}(t)\) arises directly from Assumption 3.1, while \(_{2}(t)\) comes from bounding the noise terms due to Markovian data.

We bound the four components in Lemma 4.1 as follows. For simplicity, we denote \((t):=_{k=0}^{t}\| (_{k})\|^{2}\). Among the four terms, we highlight that the main challenge lies on obtaining a tight bound for \(_{2}(t)\). Observe that

\[_{2}(t)(1-)[_{k=0}^{t}\| (_{k})\|_{m=1}^{_{k}}^{ _{k}-m}_{k,m}]\] (14)

where \(_{k,m}\!}}{{=}}\!_{^{k}}[g_{k}^{(m)}\!-\!_{Z_{_{k}}}g_{k}(_{k};u_{k},Z)]\). There are two sources of bias in \(_{k,m}\): one is the noise induced by drifting of decision variable in every epoch, the other is the bias that depends on the mixing time of Markov kernel. To control these biases, we are inspired by the proof of (Wu et al., 2020, Theorem 4.7) to introduce a reference Markov chain \(_{k}^{()}\), \(=0,...,_{k}\), whose decision variables remains fixed for a period of length \(_{k}\) and is initialized with \(_{k}^{(0)}=Z_{k}^{(0)}\):

\[_{k}^{(0)}_{k}}}{{}} _{k}^{(1)}_{k}}}{{}} _{k}^{(2)}_{k}}}{{}} _{k}^{(3)}_{k}}}{{ }}_{k}^{(_{k})}\] (15)

and we recall that the actual chain in the algorithm evolves as

\[Z_{k}^{(0)}_{k+1}^{(0)}}}{{}} Z_{k}^{(1)}_{k+1}^{(1)}}}{{}}Z_{k}^{(2)} _{k+1}^{(_{k}-1)}}}{{ }}Z_{k}^{(_{k})}.\] (16)With the help of the reference chain, we decompose \(_{k,m}\) into

\[_{k,m} =_{^{k-1}}[}( [(}_{k}^{(m)};Z_{k}^{(m)})|}_ {k}^{(m)},Z_{k}^{(0)}]-_{_{k}^{(m)}}[( }_{k}^{(m)};_{k}^{(m)})|}_{k}^{(m)},_{k}^{(0 )}])u_{k}]\] \[+_{^{k-1}}}_{ _{_{_{k}}}}[(}_{k}^{(m) };_{k}^{(m)})|}_{k}^{(m)},_{k}^{(0)} ]-_{Z_{}_{k}}}[(}_{k}^{(m)};Z)|}_{k}^{(m)}])u_{k}\] \[+_{^{k-1}}}_{Z _{_{k}}}[(}_{k}^{(m)};Z)-( }_{k};Z)|}_{k}^{(m)},}_ {k}]u_{k}:=A_{1}+A_{2}+A_{3}\]

We remark that \(A_{1}\) reflects the drift of (16) from initial sample \(Z_{k}^{(0)}\) driven by varying \(}_{k}^{(m)}\), \(A_{2}\) captures the statistical discrepancy between above two Markov chains (16) and (15) at same step \(m\), and \(A_{3}\) captures the drifting gap between \(}_{k}\) and \(}_{k}^{(m)}\). Applying Assumption 3.3, \(A_{1}\) and \(A_{2}\) can be upper bounded with the smoothness and geometric mixing property of Markov kernel. In addition, \(A_{3}\) can be upper bounded using Lipschitz condition on (stationary) distribution map \(_{}\). Finally, the forgetting factor \(\) helps to control \(\|}_{k}^{()}-}_{k}\|\) to be at the same order of a single update. Therefore, \(\|_{k,m}\|\) can be controlled by an upper bound relying on \(,,L\).

The following lemma summarizes the above results as well as the bounds on the other terms:

**Lemma 4.2**.: _Under Assumption 3.2, 3.3, 3.4 and 3.5, with \(_{t+1}=_{0}(1+t)^{-}\), \(_{t+1}=_{0}(1+t)^{-}\) and \((0,1)\), \((0,)\). Suppose that \(0<2-4<1\) and_

\[_{k}}((1+k)+\{ }{d},0\}).\]

_Then, it holds that_

\[_{2}(t) d^{5/2}}{(1-)^{2}}(t)^{}(1+t)^{1-(-2)},\;t\{t_{1},t_{2}\}\] (17) \[_{1}(t)  c_{1}(1-)(1+t)^{},\;\;_{3}(t) c_{ 3}(t)^{}(1+t)^{1-},\;\;_{4}(t)d^{2}}{1-}(1+t)^{1-(-2)},\] (18)

_where \(t_{1},t_{2}\) are defined in (25), (26), and \(c_{1},c_{2},c_{3},c_{4}\) are constants defined as follows:_

\[c_{1} :=2G/_{0},\;\;c_{2}:=}{_{0}^{2}}G^{2}+L_{2}G^{2}+G^{3/2})}{},\] \[c_{3} :=}\{L_{0},G\}, \;\;c_{4}:=}{_{0}^{2}}}{2-+1}.\]

See Appendix B for the proof. We comment that the bound for \(_{4}(t)\) cannot be improved. As a concrete example, consider the constant function \((;z)=c 0\) for all \(z\), it can be shown that \(\|g_{k}^{(m)}\|^{2}=c^{2}\) and consequently \(_{4}(t)=(_{k}/_{k}^{2})=(t^{1-(-2)})\), which matches (18). Finally, plugging Lemma 4.2 into Lemma 4.1 gives:

\[(t)(1-)}{(1+t)^{1-}}+d^{5/2}} {(1-)^{2}}(t)^{}}{(1+t)^{-2}}+c_ {3}(t)^{}}{(1+t)^{}}+c_{4}}{1- }}.\] (19)

Since \((t) 0\), the above is a quadratic inequality that implies the following bound:

**Lemma 4.3**.: _Under Assumption 3.1-3.5, with the step sizes \(_{t+1}=_{0}(1+t)^{-}\), \(_{t+1}=_{0}(1+t)^{-},_{k}}((1+k)+\{}{d},0\})\), \(_{0}=d^{-2/3},_{0}=d^{1/3}\), \((0,1)\), \((0,)\). If \(2-4<1\), then there exists a constant \(t_{0}\) such that the iterates \(\{_{k}\}_{k 0}\) satisfies_

\[_{k=0}^{T}\|(_{k}) \|^{2} 12\{c_{5}(1-),c_{6},}{1-}\}d^{2/3}T^{- \{2,1-,-2\}},\;\;T t_{0}.\]

Optimizing the step size exponents \(,\) in the above concludes the proof of Theorem 3.1.

### Discussions

We conclude by discussing two alternative zero-th order gradient estimators to (4), and argue that they do not improve over the sample complexity in the proposed \(()\) algorithm. We study:

\[_{}:=[(+;Z)-(;Z)],_{}:= [(+;Z_{1})-( ;Z_{2})],\] (20)

where \((^{d-1})\). For ease of illustration, we assume that the samples \(Z,Z_{1},Z_{2}\) are drawn directly from the stationary distributions \(Z_{+}\), \(Z_{1}_{+}\), \(Z_{2}_{}\).

We recall from SS2 that the estimator \(_{}\) is a finite difference approximation of the directional derivative of objective function along the randomized direction \(\)1, as proposed in Nesterov and Spokoiny (2017); Ghadimi and Lan (2013). For non-convex stochastic optimization with decision independent sample distribution, i.e., \(_{}\) for all \(\), the DFO algorithm based on \(_{}\) is known to admit an optimal sample complexity of \((1/^{2})\)(Jamieson et al., 2012). Note that \(_{(^{d-1}),Z}[( {};Z)]=\). However, in the case of decision-dependent sample distribution as in (1), \(_{}\) would become a _biased_ estimator since the sample \(Z\) is drawn from \(_{+}\) which depends on \(\). The DFO algorithm based on \(_{}\) may not converge to a stationary solution of (1).

A remedy to handle the above issues is to consider the estimator \(_{}\) which utilizes _two samples_\(Z_{1},Z_{2}\), each independently drawn at a different decision variable, to form the gradient estimate. In fact, it can be shown that \([_{}]=_{}()\) yields an unbiased gradient estimator. However, due to the decoupled random samples \(Z_{1},Z_{2}\), we have

\[\|_{}\|^{2}= [((+;Z_{1})-(; Z_{1})+(;Z_{1})-(;Z_{2}))^{2}]}{ ^{2}}\] \[[((;Z_{1})-(;Z_{2}))^{2}-3(( +;Z_{1})-(;Z_{1}))^{2}]}{^{2}}\] \[=[(;Z)]}{ ^{2}}-3[((+;Z_{1})- (;Z_{1}))^{2}]}{^{2}}d^{2}}{^{2}}-3^{2}d^{2}=(1/ ^{2}).\]

where in (a) we use the fact that \((x+y)^{2}x^{2}-3y^{2}\), in (b) we assume \([(;Z)]:=((;Z)-())^{2}^{2}>0\) and \((;z)\) is \(\)-Lipschitz in \(\). As such, this two-point gradient estimator does not reduce the variance when compared with the estimator in (4). Note that a two-sample estimator also incurs additional sampling overhead in the scenario of Markovian samples.

## 5 Numerical Experiments

We examine the efficacy of the \(()\) algorithm on a few toy examples by comparing \(()\) with a simple stochastic gradient descent scheme with greedy deployment. Unless otherwise specified, we use the step size choices in (8) for \(()\). All experiments are conducted on a server with an Intel Xeon 6318 CPU using Python 3.7. To measure performance, we record the gradient norm \(\|()\|\) and estimate its expected value using at least \(8\) trials.

**1-Dimensional Case: Quadratic Loss.** The first example considers a scalar quadratic loss function \(:\) defined by \((;z)=z(3^{2}-8-48)\). To simulate the controlled Markov chain scenario, the samples are generated dynamically according to an auto-regressive (AR) process \(Z_{t+1}=(1-)Z_{t}+_{t+1}\) with \(_{t+1}(,^{2})\) with parameter \((0,1)\). Note that the stationary distribution of the AR process is \(_{}=(,^{2})\). As such, the performative risk function in this case is \(()=_{Z_{}}[(;Z)]=^{2}}{12}(^{2}-8-48)\), which is quartic in \(\). Note that \(()\) is not convex in \(\) and the set of stationary solution is \(\{:()=0\}=\{4,0,-2\}\), among which the optimal solution is \(_{PO}=_{}()=4\).

In our experiments below, we initialize all the algorithms are initialized by \(_{0}=6\). In Figure 1 (left), we compare the norms of the gradient for performative risk with pure \(()\), the \(()\) algorithm, and stochastic gradient descent with greedy deployment scheme (SGD-GD) against the number of samples observed by the algorithms. We first observe from Figure 1 (left) that pure \(\) and \(\) methods do not converge to a stationary point to \(()\) even after more samplesare observed. On the other hand, \(()\) converges to a stationary point of \(()\) at the rate of \(\|()\|^{2}=(1/S^{0.36})\), matching Theorem 3.1 that predicts a rate of \((1/S^{1/3})\), where \(S\) is the total number of samples observed.

Besides, we observe that with large \(=0.75\), \(()\) converges at a faster rate at the beginning (i.e., transient phase), but the convergence rate slows down at the steady phase (e.g., when no. of samples observed is greater than \(10^{6}\)) compared to running the same algorithm with smaller \(\).

**Higher Dimension Case: Markovian Pricing.** The second example examines a multi-dimensional (\(d=5\)) pricing problem similar to (Izzo et al., 2021, Sec. 5.2). The decision variable \(^{5}\) denotes the prices of \(d=5\) goods and \(\) is a drifting parameter for the prices. Our goal is to maximize the average revenue \(_{Z_{}}[(;Z)]\) with \((;z)=-\,|\,z\), where \(_{}(_{0}-,^{2} )\) is the unique stationary distribution of the Markov process (i.e., an AR process)

\[Z_{t+1}=(1-)Z_{t}+_{t+1}\;\;\;\;_{t+ 1}(_{0}-, ^{2}).\]

Note that in this case, the performative optimal solution is \(_{PO}=_{}()=_{0}/( 2)\).

We set \(=0.5,=5\), drifting parameter \(=0.5\), initial mean of non-shifted distribution \(_{0}=[-2,2,-2,2,-2]^{}\). All the algorithms are initialized by \(_{0}=[2,-2,2,-2,2]^{}\). We simulate the convergence behavior for different algorithms in Figure 1 (middle). Observe that the differences between the \(()\) algorithms with different \(\) becomes less significant than Figure 1 (left).

**Markovian Performance Regression.** The last example considers the linear regression problem in (Nagaraj et al., 2020) which is a prototype problem for studying stochastic optimization with Markovian data (e.g., reinforcement learning). Unlike the previous examples, this problem involves a pair of correlated r.v.s that follows a decision-dependent joint distribution. We adopt a setting similar to the regression example in (Izzo et al., 2021), where \((X,Y)_{}\) with \(X(0,_{1}^{2}),Y|X( ()\,|\,X,_{2}^{2})\), \(()=_{0}+a_{1}\). The loss function is \((;x,y)=( x\,|\,-y )^{2}+\|\|^{2}\). In this case, the performative risk is:

\[()=_{_{}}[(; X,Y)]=(_{1}^{2}a_{1}^{2}-2_{1}^{2}a_{1}+_{1}^{2}+ )\|\|^{2}-2_{1}^{2}(1-a_{1})^{}_{0}+_{1}^{2}\|_{0}\|^{2}+_{2 }^{2},\]

For simplicity, we assume \(_{1}^{2}(1-a_{1})=_{1}^{2}a_{1}^{2}-2_{1}^{2}a_{1}+_{ 1}^{2}+/2\), from which we can deduce \(_{PO}=_{0}\). In this experiment, we consider Markovian samples \((X_{t},Y_{t})_{t=1}^{T}\) drawn from an AR process:

\[(_{t},_{t})=(1-)(_{t-1},_{t-1})+ (X_{t},Y_{t}),\]

for any \(t 1\). We set \(d=5\), \(a_{0}=[-1,1,-1,1,-1]^{}\), \(a_{1}=0.5,_{1}^{2}=_{2}^{2}=1\), regularization parameter \(=0.5\), mixing parameter \(=0.1\). The algorithms are initialized with \(_{0}=[1,-1,1,-1,1]^{}\). Figure 1 (right) shows the result of the simulation. Similar to the previous examples, we observe that pure DFO and SGD fail to find a stationary solution to \(()\). Meanwhile, \(()\) converges to a stationary solution after a reasonable number of samples are observed.

**Conclusions.** We have described a derivative-free optimization approach for finding a stationary point of the performative risk function. In particular, we consider a non-i.i.d. data setting with samples generated from a controlled Markov chain and propose a two-timescale step sizes approach in constructing the gradient estimator. The proposed \(()\) algorithm is shown to converge to a stationary point of the performative risk function at the rate of \((1/T^{1/3})\).

Figure 1: (_left_) One Dimension Quadratic Minimization problem with samples generated by AR distribution model where regressive parameter \(=0.5\). (_middle_) Markovian Pricing Problem with \(d=5\) dimension. (_right_) Linear Regression problem based on AR distribution model (\(=0.5\)).