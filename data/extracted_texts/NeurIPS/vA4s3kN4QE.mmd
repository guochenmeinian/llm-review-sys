# LG-VQ: Language-Guided Codebook Learning

Guotao Liang1,2, Baoquan Zhang1, Yaowei Wang2, Xutao Li1, Yunming Ye1, Huaibin Wang1

Chuyao Luo1, Kola Ye3, Linfeng Luo3

1Harbin Institute of Technology, Shenzhen, 2Peng Cheng Laboratory, 3SiFar Company

{lianggt, wangyw}@pcl.ac.cn

{23B951062, 22S051022}@stu.hit.edu.cn

{baoquanzhang, lixutao, yeyunming}@hit.edu.cn

{luochuyao.dalian, kolaygm, llf10811020205}@gmail.com

###### Abstract

Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner. Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (_e.g._, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (_e.g._, text-to-image, image captioning) due to the existence of modal gaps. In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks. Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (_i.e._, Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment. In particular, our LG-VQ method is model-agnostic, which can be easily integrated into existing VQ models. Experimental results show that our method achieves superior performance on reconstruction and various multi-modal downstream tasks.

## 1 Introduction

In recent years, with the growing development of various multi-modal task scenarios , unified modeling of visuals and language has sparked considerable interest. Vector Quantization (VQ)-based image modeling technique, exemplified by VQ-VAE  and VQ-GAN , has emerged as a pivotal approach in the realm of unified modeling. The VQ methodology  typically follows a two-stage generation paradigm. In the initial stage, a trainable discrete codebook is employed to quantize continuous image features into a discrete token sequence to finish the reconstruction task. Subsequently, the codebook is utilized for various downstream tasks by generative models .

Learning a robust codebook during the initial stage is crucial for optimizing performance in downstream tasks. At present, lots of VQ methods have been proposed to achieve robust code representation . For instance, VQ-GAN  introduces an adversarial training loss to learn a perceptually rich codebook. Some other works consider improving the codebook representation from the perspective of addressing the problem of codebook collapse .

Although existing methods have shown superior performance, most methods only focus on learning a single-modal codebook contains more low-level information (_e.g._, image's pixel, edge, and texture), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks(_e.g._, text-to-image , image captioning , VQA ). That is because the codebook lacks high-level semantics and the existence of modal gaps.

To address the above issue, we propose a novel codebook learning method (_i.e._, multi-modal codebook learning), called _Language-Guided VQ_ (LG-VQ). The novelty lies in utilizing pre-trained text semantics as supervised information to guide the codebook to learn abundant multi-modal knowledge.

Specifically, we first employ a cross-modal pre-trained model (_i.e._, CLIP ) to encode text semantics. Then, we propose two novel semantic supervision modules to transfer the text semantics into codebook, _i.e._, Semantic Alignment Module, and Relationship Alignment Module. Within the semantic alignment module, we enhance the consistency between the semantic representations of the codebook and text through global semantic alignment and masked text prediction. On the other hand, simply aligning the text and codebook in the holistic semantic space cannot satisfy more complex reasoning tasks like image captioning and VQA. Inspired by some VQA techniques [26; 40; 24], the semantic relationships between words play a very important role in various tasks of natural language processing (See Fig. 1). Based on this fact, we further propose to transfer the semantic relationships between words into codes to achieve better alignment between the codes and words. Such a text-aligned codebook helps alleviate modal gaps and improve codebook performance on cross-modal tasks.

The contributions of this work are summarized as follows:

* We point out the limitations of existing methods in learning an expressive codebook since they learn a single-modal codebook. We propose a novel multi-modal codebook learning method, named LG-VQ, which can enable the codebook to effectively retain fine-grained reconstruction information while aligning with the text.
* Resorting to pre-trained text semantics, we propose two novel semantic supervision modules, _i.e._, Semantic Alignment Module and Relationship Alignment Module, effectively learn text-aligned codebook. The advantage of such alignment modules is the abundant context and relationship semantics contained in pre-trained text can be sufficiently leveraged for enhancing multi-modal codebook learning.
* We conduct comprehensive experiments on four public datasets, which shows that our LG-VQ method outperforms various state-of-the-art models on reconstruction and various cross-modal tasks (_e.g._, text-to-image, image captioning, VQA).

## 2 Related Works

### Vector Quantization for Image Generation

Vector quantization (VQ) is designed to learn a codebook, which aims to encode continuous image features into a discrete sequence. Then, the learned codebook can be utilized for various downstream tasks. Oord et al.  first propose a novel VQ method called VQ-VAE. This method innovatively replaces the prior distribution of Variational Autoencoder (VAE) with a discrete deterministic distribution (_i.e._, a codebook). To further improve the performance of VQ, various models are proposed to learn a more expressive codebook [9; 48; 2; 17; 7; 15; 14; 21]. For example, VQ-GAN  addresses the issue of image blur generated by VQ-VAE through the introduction of an adversarial training loss. However, the above methods do not tackle the codebook collapse issue. To address the issue, many novel methods are proposed from the perspective of regularization , codebook update , codebook transfer . Recently, inspired by the large language models (LLMs), instead of mapping images to the visual code tokens, some works attempt to map the images to the word tokens of LLMs by viewing images as "foreign languages" [22; 50; 56]. However, because of the inherent differences between vision and language, these works have difficulty assigning correct semantic words to images.

Figure 1: To answer the question, one not only needs to identify “women” and “racket” but also understand the semantic relationship between them (“holding”).

Compared with the aforementioned methods, our approach focuses more on multi-modal alignment in feature space (_i.e._, learning a text-aligned codebook). We use pre-trained text semantics to supervise the codebook learning. The advantage is that the rich semantic information from the text can be fully exploited for more robust codebook learning so that the codebook can not only retain more reconstruction information but also be able to understand and match text. More importantly, our method is model-agnostic, which can be easily integrated into existing VQ models.

### Vision-Language Representation Learning

Vision-language Pre-training (VLP) aims to learn multi-modal representations from large-scale image-text pairs that can improve vision-language downstream tasks, for example, VQA. Early methods such as LXMERT , UNITER  employ pre-trained object detectors to extract image region features, and fuse image features with text by a cross-modal encoder to achieve the vision-language representation learning. Although these methods achieve superior performance on downstream tasks, they require high-resolution input images and pre-trained object detectors. To remove the object detectors, a large number of researchers focus on learning two separate representations for image and text [32; 16; 18]. For instance, CLIP  learns a robust representation for each image and text using contrastive learning based on large-scale image-text pair data.

In this paper, we propose to employ pre-trained text semantics as supervised information to guide codebook learning. Its advantage is that abundant multi-modal knowledge contained in text can be fully leveraged for robust codebook learning. Additionally, we design a novel relationship alignment module to inject semantic relationships between words into codes.

## 3 Methodology

### Preliminaries: VQ-VAE

VQ-VAE , as a pioneering work on the VQ research domain, aims to learn a discrete codebook to encode images into discrete token sequences through an Encoder-Decoder framework. As illustrated in Fig. 2 right, the VQ-VAE consists of a visual encoder \(E_{_{e}}()\) with parameter \(_{e}\), a token decoder \(D_{_{d}}()\) with parameter \(_{d}\), a quantizer \(Q()\), and a codebook is defined as \(=\{e_{k}\}_{k=1}^{R}\) that consists of learnable \(K\) entries \(e_{k}^{d_{x}}\) with dimension \(d_{z}\). Given an input image \(x^{H W C}\), where \(H\), \(W\), and \(C\) represent the height, width, and channel of the image respectively. The visual encoder \(E_{_{e}}()\) learns to convert the original image into grid features \(=E_{_{e}}(x)^{ d _{z}}\) and \(f\) is the down-sampling factor. The quantizer \(Q()\) looks up the nearest neighbor in the codebook for each

Figure 2: The overall architecture of the proposed LG-VQ method. The right part of the figure is the basic VQ-VAE module, left is our language-guided module, which consists of three losses: global semantic alignment (\(_{gsa}\)), masked text prediction (\(_{mtp}\)), and relationship alignment supervision (\(_{ras}\)). Here, pre-trained text information guides discrete code tokens of an image to learning rich semantic knowledge based on three losses.

grid representation \(_{i}^{d_{z}}\) in \(_{i}\) using the following equation:

\[z_{i}=Q(_{i})=}{argmin}\|_{i}-e_{k}\|.\] (1)

The token decoder \(D_{_{d}}()\) is used to reconstruct the original image by \(=D_{_{d}}(Z)\), where \(Z\) is discrete code tokens of whole image obtained by Eq. 1. During training, the visual encoder \(E_{_{e}}()\), codebook \(\), and token decoder \(D_{_{d}}()\) are jointly optimized by minimizing the following objective:

\[_{vq}=\|x-\|_{2}^{2}+\|sg[E_{_{e}}(x)]-Z\|_{2}^ {2}+\|E_{_{e}}(x)-sg[Z]\|_{2}^{2},\] (2)

where, the first term is reconstruction loss, which measures the difference between the original image \(x\) and the reconstructed image \(\). \(sg[]\) represents the stop-gradient operator, and the second term is codebook loss, which encourages the codebook to be close grid features. The third term is the "commitment loss" , where \(\) serves as a hyper-parameter. However, existing VQ-based methods mainly focus on the learning of single-modal codebook, thereby limiting their applicability to multi-modal downstream tasks.

### Proposed Method: LG-VQ

Existing works attempt to improve codebook reconstruction capabilities to obtain better performance on downstream tasks. However, ignoring modal differences results in suboptimal performance when the codebook is applied to cross-modal tasks. To address this issue, we propose to utilize the pre-trained text semantics as supervised information to learn a text-aligned codebook. Its advantage is abundant semantic information from text can be fully exploited for more robust codebook learning to improve the performance of reconstruction and cross-modal tasks. The comprehensive architecture of the proposed LG-VQ method is illustrated in Fig. 2 left. It consists of two supervision modules: Semantic Alignment Module (_i.e._, \(_{gsa}\) and \(_{mtp}\)), and Relationship Alignment Module (_i.e._, \(_{ras}\)). The first module encourages global semantic consistency between the codebook and text. The second module aims to transfer the rich semantic relationship between words into codes. Next, we introduce these two modules in detail.

#### 3.2.1 Semantic Alignment Module

Considering that paired image and text data have consistent semantic information and the missing information of masked data can be completed from the other modality, we propose global semantic alignment, which aims to enhance the consistency of global semantics between text and visual codes, and masked text prediction, which uses visual codes to restore the masked words. Next, we discuss how to align text and codebook in the semantic space.

**Text Information Encoder**: Instead of jointly training text and codebook from scratch, we employ a pre-trained cross-modal model CLIP  to encode text information. Its advantage is that such text information already has good cross-modal semantic knowledge and is beneficial for codebook learning. Specifically, for a given text description of an image \(t=\{w_{SOT},w_{1},w_{2},,w_{n-2},w_{EOT}\}\), where \(w_{i}\) denotes the \(i\)-th word, \(w_{SOT}\) and \(w_{EOT}\) represent the \([start]\) token and \([end]\) token, respectively, and \(n\) is text sequence length. We use the text encoder of a pre-trained CLIP model to obtain whole sequence embedding \(T^{n d_{t}}\):

\[T=\{e_{SOT},e_{w_{1}},e_{w_{2}},,e_{w_{n}},e_{EOT}\}=(t).\] (3)

Similar to CLIP, we use the \(e_{EOT}\) to represent the global context feature of the sequence.

**Global Semantic Alignment** aims to align text and image visual codes in the global semantic space. For getting the global representation of visual codes, we employ a vision transformer (ViT) \(f_{_{vt}}\) to encode the discrete codes of image. Specifically, given an image, we firstly obtain the discrete codes of image \(Z\) by Eq. 1. Then, we introduce a learnable global token \([CLS]\) at the beginning to form a token sequence \(Z_{c}\), where global token \([CLS]\) is employed to capture the image's global context information. We feed the sequence into \(f_{_{vt}}\) to get a new visual code representation, that is:

\[Z_{vt}=\{e_{CLS},e_{1},e_{2},,e_{}\}=f_{ _{vt}}(Z_{c}).\] (4)

Finally, we employ InfoNCE , which maximizes the similarity between visual and text in the global representation, as our learning objective, where \(\) is the batch size, \(s(,)\) is cosine similarity:

\[_{gsa}=-_{i}^{i},e_{EOT}^ {i}))}{_{j}(s(e_{CLS}^{i},e_{EOT}^{j}))}.\] (5)

**Masked Text Prediction**: To further enhance the semantic alignment, we propose to use discrete visual codes to reconstruct the masked words from a more fine-grained perspective, refer to Fig. 2 left. Formally, for a given fixed-length text sequence of \(n-2\), we first randomly sample the masking ratio \(r\) from a truncated Gaussian distribution . Subsequently, we randomly mask out \(r(n-2)\) words and replace them with learnable \([mask_{i}]\) tokens based on their positions \(i\). Next, a self-attention module  is employed to learn adaptive masked word embeddings based on unmasked words. The resulting adaptive masked sequence is denoted as \(T^{msk}=\{e_{SOT},m_{1},e_{w_{2}},m_{3},,e_{EOT}\}\), where \(m_{i}\) is the mask token embedding at the \(i\)-th position in the sequence. Following this, a cross attention decoder \(f_{_{m}}(,)\) is employed to predict the masked word tokens given the discrete visual codes \(Z_{vt}\) obtained by Eq. 4. Finally, we add a cross-entropy loss \(H(,)\) between the ground-truth word tokens and the output of the decoder. Let \(y_{msk}\) denote a one-hot vocabulary distribution where the ground-truth word token has a probability of 1, \(f_{_{m}}(Z_{vt},T^{msk})\) denote the predicted probability of model for masked word tokens. That is:

\[_{mtp}=-_{(Z_{vt},T^{msk})}H(y_{msk},f_{ _{m}}(Z_{vt},T^{msk})).\] (6)

#### 3.2.2 Relationship Alignment Module

While the two aforementioned loss functions for achieving good alignment at holistic semantic space have demonstrated initial promise, they cannot satisfy more complex reasoning tasks like image captioning and VQA. Inspired by some VQA techniques [26; 40; 24; 1], the semantic relationships between pre-trained words play a very important role in complex text reasoning tasks. For instance, as shown in Fig 1, to answer question ("What is this woman holding?"), one needs to fully understand the visual objects "women", "racket", and semantic relationship between them ("holding"). Based on the above fact, we propose to transfer the semantic relationship between words into codes. Such semantic relationships enable the model to better understand the image for addressing complex reasoning tasks.

But unfortunately, there is an issue there is no alignment between words and codes. Thanks for the above two losses that have provided semantic alignment of text and visual codes. To achieve the above idea, as shown in Fig. 3, we first use \(Z_{vt}\) to align with words. Then, we inject semantic relationships between words into the initial codebook \(Z\), instead of the \(Z_{vt}\). Its advantage is it can prevent codes from collapsing into a single point for learning more diverse representations by relationship limiting. Then, \(Z_{vt}\) primarily serves the purpose of aligning words and codes, but it is a crucial step for subsequent processes. Specifically, given any two words of a sentence, we use pre-trained word embedding  to encode words, \(e_{w_{i}}\) and \(e_{w_{j}}\). We employ cosine similarity to find the index of the code from \(Z_{vt}\) that is most similar to the word. Then, one can get code embedding from \(Z\) based on the index:

\[e_{z_{i}}=Z[_{vt[:]}}{argmax}\,s(e_{w_{i}},e_{z}),:], e_{z_{j}}=Z[ Z_{vt[:]}}{argmax}\,s(e_{w_{j}},e_{z} ),:].\] (7)

Next, we consider cosine similarity as a measure of semantic relationships between words and leverage it to establish corresponding relationships between codes achieving semantic relationship transfer. Finally, we utilize the following loss function as learning objective:

\[_{ras}=_{(w_{i},w_{j}) t}(s(e_{w_{i}},e_{w_{j}})-s(e_{z_{i}},e_{z_{j}}))^{2}.\] (8)

#### 3.2.3 Training Objective

We use three hyperparameters (_i.e._, \(\), \(\), and \(\)) to control three losses, respectively. Finally, the overall objective function is:

\[=_{vq}+_{gsa}+_{mtp}+ _{ras}.\] (9)

Figure 3: Illustration of relationship alignment module, we use \(Z_{vt}\) to align with two words, then inject the semantic relationship of two words into \(Z\) codes.

[MISSING_PAGE_FAIL:6]

Appendix A.3 Fig. 10.

### Ablation Study

**Are our three loss functions both effective?** In Table 2, we conduct an ablation study to show the effectiveness of the proposed three loss functions. Specifically, the VQ-GAN serves as the baseline model (_i.e._, without introducing any loss). We do not conduct a separate experiment on \(_{ras}\) because this module requires code and words to be well aligned. Based on the results from (i) \(\) (vi), we draw several key conclusions: Firstly, each loss function plays a crucial role in improving the performance of image reconstruction. Secondly, the performance of (iii) outperforms (ii) by a large margin on TextCaps. This is reasonable because TextCaps's texts are richer and more diverse than CUB-200, it can provide more knowledge for more fine-grained alignment between codes and text, which is useful for the learning of a more robust codebook. Thirdly, analyzing the results of (iii) and (iv), injecting word-level semantic relationships into codes is beneficial, which confirms our motivation. Furthermore, the performance of (v) outperforms (i), which is reasonable because the abundant semantic knowledge from pre-trained text can be fully exploited for learning more robust codebook representation. This supports the motivation of learning a multi-modal codebook (_i.e._, aligned with text). Finally, comparing the results of (vi) with (i)\(\)(v), fully considering all losses achieves the best performance, indicating the effectiveness of our method.

**Can our global semantic supervision align vision and language?** In Fig. 5, we provide several image-to-text retrieval cases on CelebA-HQ and CUB-200 datasets based on VQ-GAN+LG. From the figure, it can be observed that our method can accurately retrieve text very similar to the image content, achieving the alignment of vision and language. For example, row 2 examples show that our method can precisely understand some key attributes of images (_e.g._, "gray hair", "neckite", "big nose" and "chubby") and retrieve similar text. This suggests that the codes learned through our method have obtained good alignment with the text, which verifies the effectiveness of our method. Moreover, such alignment is beneficial for learning robust code representations and enhancing performance in multi-modal downstream tasks.

**Can our codebook accurately predict masked words?** To answer this question, we conduct a word prediction task on test data based on VQ-GAN+LG by randomly masking one word or three words of text, as shown in Table 3. We use Recall@1 as the evaluation metric . From the table, our method demonstrates accurate predictions of masked words, confirming the effectiveness of our approach. Fine-grained word prediction can help the codebook better understand the text semantics, which is crucial for improving the performance of the downstream task. Additionally, several examples in Fig. 5 demonstrate our method's ability that accurately predict subject words (_e.g._, wings, eyes) and verbs (_e.g._, has, is, and smiling), further affirming its strong multi-modal understanding capabilities.

**Can our codebook learn the word semantic relationships?** In Fig. 5, we visualize the cosine similarity between words and the cosine similarity between codes aligned with the words for a certain sample based on VQ-GAN+LG. From the figure, we can see our codes can learn consistent relationships with word semantics compared with VQ-GAN. For example, the similarity "code 33" vs "code 232" (0.46) resembles "wings" vs "chest" (0.49). In addition, we provide a quantitative similarity evaluation between codes and words in Table 4. From the results, we can find that our codes indeed achieve consistent semantic relationships with words.

**Is our method effectively learning more diverse code representation?** Following , we directly feed each codebook embedding \(e_{k}\) (size: \(1 1 256\)) into the decoder \(D_{_{d}}()\) to generate codebook

  Method & VQ-GAN & VQ-GAN+LG \\  MSE\(\) & 0.6374 & 0.0351 \\  

Table 4: Results of similarity evaluation between codes and words on CUB-200 all test data.

image (size: \(16 16 3\)). Then, we concatenate all codebook images to form a big image with \(32 32\) patches. Finally, we visualize the result of VQ-GAN and our LG-VQ on TextCaps and MS-COCO as shown in Fig. 7. This visualization suggests that our method enables the model to learn more diverse code representations and improve codebook usage.

### Application

#### 4.4.1 Image Generation

Following [9; 37; 11], we conduct image generation downstream tasks (_i.e._, text-to-image, semantic synthesis, unconditional generation, and image completion) to fully validate the effectiveness of the learned codebook on CelebA-HQ.

**Text-to-Image**. In Table 5, we compare our LG-VQ with the state-of-the-art models on CelebA-HQ dataset for text-to-image. From the results, our LG-VQ method outperforms baseline methods by a large margin. This is reasonable due to the incorporation of pre-trained text knowledge enabling a comprehensive understanding of the text, which suggests our method's effectiveness. Moreover, we provide some synthesis examples comparing the results of our LG-VQ with baseline methods in Figure 8, showing the performance in the text-to-image task. From the figure, we can see our method not only comprehensively understands the given text conditions but also excels in generating realistic images compared with baseline methods. For instance, our method can capture the "glasses", "man", "long black hair", and "no beard" key attributions.

**Semantic Synthesis**. Following , we compare with existing semantic synthesis models in Table 6. Our method achieves the best performance, which suggests our method's effectiveness. We provide some examples in Appendix Figure 13.

**Unconditional Generation and Image Completion**. Following , we conduct unconditional image generation and Image Completion on CelebA-HQ dataset, as shown in Table 7 and Table 10. From the results, we can see that our method can significantly improve the performance of VQ-GAN, which is reasonable because pre-trained text can provide rich semantic knowledge for learning more robust codebook representation. This suggests the effectiveness of our method. We provide some examples in Appendix Figure 18 and Figure 17.

Figure 6: Visualization of words similarity and image codes similarity aligned with the word. We extract some representative words from the text as a demonstration.

Figure 7: Visualization of the codebook of VQ-GAN and LG-VQ on TextCaps and MS-COCO.

#### 4.4.2 Visual Text Reasoning

Follow [27; 6], we use the learned codebook to conduct two visual text reasoning tasks: 1) image captioning on CUB-200; and 2) visual question answering (VQA) on COCO-QA . For the experimental setting, please refer to Appendix A.1.

**Image captioning**. Following , we conduct the image captioning task on the CUB-200 dataset. We compare two recent work V2L Tokenizer  and VQCT . We select VQ-GAN as our backbone network. The results are shown in Table 11. For the results, we can see that our LG-VQ method outperforms the performance of VQ-GAN. This is reasonable because the pre-trained text provides rich context and relationship semantics for codebook learning, which verifies our motivation for learning a text-aligned codebook to improve the performance of the codebook on cross-modal tasks. On the other hand, the V2L Tokenizer and VQCT cannot achieve very good performance because it is difficult to assign correct semantic language tokens to images. Compared with the V2L Tokenizer, our method utilizes pre-trained text semantics as supervised information. Its advantage is can make the codebook learn semantic information consistent with the text (_i.e._, learning a text-aligned codebook). And, our method is model-agnostic, which can be easily integrated into existing VQ models.

**Visual Question Answering**. We select VQ-GAN and VQCT  as the baseline. We conduct the VQA task on the COCO-QA  using the codebook trained on the MS-COCO dataset. The results are shown in Table 9. From the results, we can see that our LG-VQ method significantly improves the performance of VQ-GAN on VQA task (approximately 8.32%\(\) on Accuracy). That is reasonable due to we introduce pre-trained text semantics to enable us to obtain a codebook aligned with the text, which is helpful for comprehensively understanding the given text question. This confirms our motivation and the effectiveness of our method.

#### 4.4.3 Visual Grounding

We conduct a visual grounding task on refcoco dataset  to validate the effectiveness of the learned MS-COCO's codebook. Following the same metric used in , a prediction is right if the IoU between the grounding-truth box and the predicted bounding box is larger than 0.5. We select VQ-GAN and VQCT  as the baseline. The results are shown in Table 8. From the results, we can see that the performance of our method consistently outperforms VQ-GAN and VQCT, which suggests its effectiveness. We also provide a qualitative comparison in Appendix Figure 19. For the experimental setting, please refer to Appendix A.1.

   Model & 
 Semantic Synthesis \\ FID\(\) \\  \\  Reg-VQ  & 15.34 \\ VQCT  & 14.47 \\ VQ-GAN & 11.53 \\ CVQ & 11.04 \\  VQ-GAN+LG & 11.46 \\ CVQ+LG & 11.03 \\   

Table 6: Result (FID\(\)) of semantic synthesis on CelebA-HQ.

Figure 8: Text-to-image synthesis and semantic image synthesis on CelebA-HQ. Text with background color emphasizes generated details

## 5 Conclusions

In this paper, we propose a novel codebook learning method, named LG-VQ. LG-VQ is a model-agnostic method and can easily be integrated into existing VQ models. In particular, we propose to incorporate pre-trained text semantics into the codebook by two novel supervision modules, _i.e._, semantic and relationship. Quantitative and qualitative experiments demonstrate the strong generality of our method, showing its ability to improve the performance of the codebook in cross-modal tasks. **Limitations**. In our current paper, we suppose each word aligns with a code, but it fails to capture some more complex relationships between words and codes (_e.g._, one code aligns with multiple words). In the future, we plan to investigate the relationships between codes and words. Moreover, although our results show that the performance of VQ in visual text reasoning tasks can be significantly improved, its results are still far lower than the performance of image captioning or VQA models.

**Broader impact** Our paper shows that learning a multi-modal codebook (_i.e._, a text-aligned codebook) can not only significantly improve the performance of reconstruction but also the performance of the codebook on cross-modal tasks. The potential impact of our research lies in its influence on future studies, specifically in the area of unified modeling of multi-modal understanding and generation. For instance, our work can be extended to interact with LLMs to improve multi-modal understanding and generation capabilities. In particular, our model can be used to generate images or text. It may be exploited to produce some erroneous and unethical information, which needs to be handled carefully before employing our model in practical applications.