# CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes

Jason Yang

Chemistry and Chemical Engineering

California Institute of Technology

&Ariane Mora

Chemistry and Chemical Engineering

California Institute of Technology

&Shengchao Liu

Computing and Mathematical Sciences

California Institute of Technology

&Bruce J. Wittmann

Office of the Chief Scientific Officer

Microsoft Corporation

&Anima Anandkumar

Computing and Mathematical Sciences

California Institute of Technology

&Frances H. Arnold

Chemistry and Chemical Engineering

Biology and Biological Engineering

California Institute of Technology

&Yisong Yue

Computing and Mathematical Sciences

California Institute of Technology

Correspondance: yyue@caltech.edu

###### Abstract

Enzymes are important proteins that catalyze chemical reactions. In recent years, machine learning methods have emerged to predict enzyme function from sequence; however, there are no standardized benchmarks to evaluate these methods. We introduce CARE, a benchmark and dataset suite for the Classification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1) classification of a protein sequence by its enzyme commission (EC) number and (2) retrieval of an EC number given a chemical reaction. For each task, we design train-test splits to evaluate different kinds of out-of-distribution generalization that are relevant to real use cases. For the classification task, we provide baselines for state-of-the-art methods. Because the retrieval task has not been previously formalized, we propose a method called Contrastive Reaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task and compare it to the recent method, CLIPZyme. CARE is available at https://github.com/jsunn-y/CARE/.

## 1 Introduction

Proteins, which are sequences of amino acid building blocks, are not only integral components of all living organisms, but also important for a myriad of commercial applications spanning from the health domain to the bio-economy. Enzymes are a subclass of proteins that can catalyze chemical reactions, and they have many applications in areas such as bioremediation, plastic degradation, gene editing, and drug synthesis .

Identifying the specific chemical reactions that an enzyme is capable of performing (_i.e._ the enzyme's function) is a key first step for many applications . While hundreds of millions of proteinshave been sequenced, less than 1% are annotated with function . For instance, during standard metagenomic analyses, enzyme genes are annotated for their functions, which enables determination of an organism's critical metabolic pathways and specialization [7; 8; 9]. When applied to chemical synthesis, enzyme annotations are needed to identify catalysts that can replace existing step(s) in drug synthesis procedures-known as retrobiosynthesis . Likewise, enzyme engineering for new-to-nature function  involves discovering an enzyme starting point with a desired function before improving activity for that function using protein engineering techniques [12; 13]. Historically, similarity search algorithms, most notably BLAST, have been the most common methods used to assign function to protein sequences [14; 15]. These methods work by finding similar sequences in annotated reference databases, as similar sequences are likely to share function. More recently, searching based on structural similarity (Foldseek) has also demonstrated utility for assigning function [16; 17]. However, up to one third of genes from bacterial genomes cannot be assigned function using existing methods . As such, there is a need for more abundant, high-quality annotations of enzymes and automated workflows to identify enzymes with desired functions.

In recent years, there has been increasing interest in using machine learning (ML) for a broad range of applications related to the functional prediction and design of enzymes [13; 18; 19]. In particular, ML models have emerged to classify protein sequences based on their function, which are reviewed here  with a few examples listed here [21; 9; 22; 23; 24; 25; 26]. Despite these advances, there is no standard benchmark or dataset for evaluating computational models for enzyme function prediction . A challenge associated with classification is that a given enzyme is often able to perform multiple reactions , and many reactions are not annotated. Moreover, complex tasks such as extrapolation to unannotated reactions  have yet to be evaluated.

In this work, we present a benchmark suite for the **C**lassification **A**nd **R**etrieval of **E**nzymes (CARE). Our contributions can be summarized as: **(1)** formalizing model evaluation into two tasks that encompass applications relevant to scientists and engineers: classification of an enzyme by function (Task 1), and retrieval of enzyme sequences based on a reaction (Task 2); **(2)** curating high-quality and easy-to-use datasets; **(3)** providing train-test splits that mimic challenging extrapolations in real-world use cases; and **(4)** benchmarking state-of-the-art models for Task 1, and providing a new method that serves as a baseline for Task 2. Because Task 2 has been minimally explored, we introduce a model called **C**ontrastive **R**eaction-**E**nzym**E **P**retraining (CREEP) to serve as a baseline for text, reaction, and sequence integration and compare it to an existing approach (CLIPZyme) that uses sequence and reaction, in addition to other retrieval approaches. CREEP can perform contrastive learning across three different modalities (protein, reaction, and textual description), and the learned representations are then used for retrieval. Overall, we anticipate that CARE will be a useful and easy-to-use resource for ML researchers to benchmark their enzyme function prediction models.

## 2 Related Work

**Datasets.** Various databases have emerged to help researchers store, share, and identify functionally annotated enzymes. Protein sequence databases such as UniProt  and Pfam  are catalogs of annotated protein sequences. While most databases reference protein sequences, increasingly, these sequences can be linked to protein structures, either experimentally validated, as in the Protein Data Bank (PDB), or via structural prediction tools  in databases such as the AlphaFold Database . BRENDA  is a curated database specific for reaction and enzyme sequence information. Rhea  consolidates information from BRENDA, and other sources, such as pathway databases including KEGG . There is ongoing work to compile and clean/standardize reactions from multiple databases, namely ECReact  and EnzymeMap . Related to these databases, there are retrobiosynthesis planning tools [38; 39; 40], and Selenzyme is tool to retrieve enzymes to perform a target reaction .

**Protein Benchmarks.** Our work takes inspiration from existing protein fitness prediction benchmarks, where fitness is a quantification of some function. TAPE  and FLIP  evaluate representations from protein language models for the prediction of a broad range of general and specific protein properties (stability, secondary structure, binding etc.) ProteinGym considers sequence variant effect prediction by using likelihoods from these language models . While benchmarks for protein fitness prediction tasks are well defined, there are no standardized benchmarks for protein function prediction. Fitness is a numerical quantification of protein function (stability, enzyme activity level, etc.), but function is more qualitative/categorical, e.g. a description of an enzymatic reaction associated with a protein. Here, we focus on proteins that perform chemical reactions, enzymes.

**Classification of Enzyme Function.** ML models have emerged to predict the outcomes of enzymatic reactions [45; 46; 47] and for classification of enzyme function. Enzyme function is usually expressed using enzyme commission (EC) numbers, which is a hierarchical scheme for classifying enzyme function into classes (families) and consists of four levels of descriptions (Figure 1A). Some classification models are general protein function prediction models, which encompass all proteins, not just enzymes, such as ProtCNN/ENN  and ProteInfer . Many models utilize representations from protein language models [48; 49; 50], and others incorporate protein structure as information, such as DeepFRI  and BioCLIP . Methods related to supervised contrastive learning  have been particularly useful here, such as CLEAN, HiFi-NN, and Enzhier [22; 9; 53], likely by reducing imbalances in the number of sequences representing each EC number. Recently, enhanced approaches have enabled function prediction in ProtEx, PhiGnet , and others [55; 56; 57]. Other retrieval tools enable more sensitive detection of homologs such as DHR , ProtTrek , and using structure [17; 60; 61]. Finally, there are related models that predict substrates for enzymes [62; 63] and that aim to learn connections between chemical space and protein space .

**Large Language Models.** Recently, there has been an explosion in pretrained models in the biological and chemical domains, particularly large language models (LLMs) [65; 66]. For example, ChatGPT is capable of answering questions related to general scientific knowledge. These language models can be further finetuned for applications such as answering questions about protein sequences (including Pika , InstructProtein , InstructBioMol , ProteinGPT , and ProteinChat ) and for reaction synthesis planning (ChemCrow ), among others. LLMs present important benchmarks for enzyme functional classification and retrieval, given the widespread adoption of LLMs as science facilitators and their ease of use (in particular webserver-based approaches), compared to domain-specific methods .

**Multimodal Contrastive Learning.** Contrastive learning is an efficient and effective pretraining paradigm that aligns positive pairs and contrasts negative pairs simultaneously. The design of these pairs depends on the specific tasks, such as using data augmentations of the same image  or considering the topology and geometry of molecules . More recently, contrastive learning has shown success in aligning the representation space of different biological and chemical modalities, _e.g._, text and chemical structure alignment in MoleculeSTM , text and protein sequence alignment in ProteinDT  and ProteinCLIP , reaction structure and protein structure alignment in CLIPZyme , and protein sequence and structure alignment in BioCLIP  and with text in ProTrek , among others [80; 65]. Cross-modal alignment in the representation space has been shown to improve generalizability and improve performance on challenging tasks, such as out-of-distribution learning, zero-shot learning, and text-guided molecule design and optimization . Consideration of multiple modalities may be especially important for the prediction of qualitative functions.

## 3 Overview of CARE

Though there are many studies using ML models to perform enzyme classification based on EC numbers (Figure 1A), there is no standardized benchmark to evaluate how well these models generalize to unseen protein sequences. To address this need, we present a benchmark suite for the classification and retrieval of enzymes (CARE, Figure 1B). CARE formalizes classification of an enzyme sequence by EC number as "Task 1" (Figure 1C). For this task, we design train-test splits of protein sequences to test out-of-domain generalizations that are relevant to real-world use cases. In addition, CARE addresses another key limitation of current studies: classification is limited to EC numbers, which is a closed vocabulary of functions (reactions), so existing models cannot generalize to unannotated reactions. Thus, we introduce an entirely new task, retrieval of an EC number given a reaction, which we call "Task 2" (Figure 1D). For this task, we design train-test splits to evaluate how well models can generalize to out-of-domain reactions, ensuring that the splits pose different levels of difficulty.

To streamline benchmarking, we curate a dataset of enzymes, reactions, and their associated EC numbers for CARE. At a high level, we build two datasets, one that links protein sequence to EC and one that links reaction to EC (Figure 1A). The former is processed from Swiss-Prot, the validated portion of UniProt  and filtered to protein sequences between length 100 and 1024 with annotatedEC number(s). The latter is formed as a combination of EnzymeMap  and ECReact , where ECReact is only used to supplement EC numbers that are missing in EnzymeMap. Our workflow for generating the datasets used in this work is explained in detail in Appendix A.1 and shown visually in Appendix Figure A.1.

The overall workflow for benchmarking using CARE is shown in Figure 1B. For each task, domain-specific train-test splits are provided from the processed datasets. Model training can use any of the data in the train split, and each model is evaluated on the associated test split. In the rest of this study, we explain the specific design choices used to generate train-test splits and analyze benchmarking results of state-of-the-art methods on these splits. The curated datasets and splits used in CARE can be accessed at https://github.com/jsunn-y/CARE/.

## 4 Task 1: Enzyme Classification

Task 1, classification of an enzyme sequence, tests the ability of a model to extrapolate to unseen protein sequences. Task 1 is a fairly well studied task , but model evaluation has not been previously standardized as there are many factors to consider, such as the distribution of sequences and functions in the test sets. Task 1 applies to use cases where a scientist is given an unannotated enzyme sequence and seeks to understand the enzymatic function associated with that sequence (Figure 1C), for example for metagenomic analysis or finding new enzymes for retrobiosynthesis. With the emergence of conditional generative models for protein sequences, it is also important to

Figure 1: **Overview of CARE.****(A)** Dataset format for CARE, showing examples of enzymes and their associated reactions. The EC number acts as a bridge between a protein sequence and the reactions it is likely to perform. The EC number is a hierarchical classification scheme for enzyme function with four levels of description, with increasing specificity from left to right. **(B)** General workflow for CARE benchmarking. **(C)** Task 1 is a real-world use case for enzyme classification based on a protein sequence. **(D)** Task 2 is a real-world use case for enzyme retrieval based on a reaction.

have high-throughput computational methods that can predict the function of generated sequences . For this task, a query protein is passed through a trained model to predict an EC number. The EC number is then associated with likely reactions that the protein will be able to perform.

**Splits for Task 1.** Task 1 can be framed as evaluating how models generalize toward unseen sequences with different types of difficulty, visualized in Appendix Figure A.3A. The train-test splits for Task 1 are summarized in Table 1.

* _<30%_ and _30-50% identity_ splits: these two test splits contain sequences with sequence identities falling in the respective range, to sequences in the training set by using clustering (Section A.2). Sequence identity is related to the normalized Levenshtein distance between two sequences. Natural sequences with high (>40%) sequence identity at the protein sequence level are likely to share function . It is expected that the lower the sequence identity, the more difficult it is to assign functional annotation.
* _previously misclassified (Price et al.)_ split: challenging to assign because some have low sequence identity to other proteins, and many may lie near "activity cliffs" (Figure A.3), a region where function can change sharply in sequence space.
* _promiscuous_ split: in this study, we define promiscuous enzymes as those mapping to multiple EC numbers (thus lying on multiple activity peaks). These enzymes are particularly interesting for enzyme engineers, as new-to-nature activity can be found in between activity peaks .

For Task 1, the training split is constructed by holding out all of the pooled sequences in the test splits. The classification output vocabulary (EC numbers) is closed rather than open, so EC numbers in the

   Split name & Description & Train & Test \\  & & samples & samples \\  30\% Identity & Held-out sequences with approximately  30\% & 184,529 & 432 \\  & identity to the training set & & \\
30-50\% Identity & Held-out sequences with approximately 30- & 184,529 & 560 \\  & 50\% identity to the training set & & \\ Previously Mis- & Previously misclassified enzyme sequences & 184,529 & 148 \\ classified (Price) & from Price et al. & & \\ Promiscuous & Held-out sequences with multiple EC numbers & 184,529 & 209 \\   

Table 1: **Summary of train-test splits used in Task 1.** For Task 1, certain protein sequences are held out. Train ”samples” refers to the number of unique protein-EC data pairs, and test ”samples” refers to the number of protein sequences. All splits for Task 1 share the same protein train set.

Figure 2: **Distribution of similarities between samples in each test set and the corresponding train set.****(A)** Protein sequence identity (Task 1) was measured to the closest hit in the train set using BLASTp. Sequence identity can be thought of as normalized Levenshtein distance. **(B)** Reaction similarity (Task 2) was measured as cosine similarity with each reaction represented using DRFP . Reaction similarity was measured from the test set to each train-set cluster center of reactions belonging to an EC number.

test sets are present in the train sets. We verify that sequences in the test sets generally fall within the expected sequence identities, relative to the training set (Figure 2A). Notably, the _Price_ test set has a wide distribution of sequence identities, while the _promiscuous_ test set has high sequence similarity to the training set. The sequences in the test splits are distributed generally evenly across all different EC numbers (Figure A.4 in Appendix). More details can be found in Appendix A.2.

**Task 1 benchmarking results.** Benchmarking results for Task 1 are summarized in Table 2. We start with two methods as baselines, classification using a random order of EC numbers (Random), and Diamond BLAST at the protein sequence level, herein referred to as BLASTp, which is a workhorse bioinformatics tool that performs local-alignment to determine the most similar sequence(s) given a target query and a database . Additionally, we include search by structural similarity using Foldseek .

While there exist many ML tools to directly perform EC number classification, CLEAN  and several others  are a few that seem to report the current state-of-the-art with comparable performance. Some of the latter methods were not yet publicly available at the time of this study, and other recent retrieval methods would also be promising approaches to test, but we opted to focus on benchmarking on CLEAN here. CLEAN generally performs the best for enzymes with low sequence identity to known sequences, but the BLASTp and Foldseek baselines perform similarly. ChatGPT was also tested, but it appeared to often hallucinate as it was forced to provide an answer, with results similar to the random baseline. Pika seems to bridge the gap between standard LLMs and enzyme classification, but its performance is not as good as standard tools like BLASTp .

   Split & Method & Level 4 & Level 3 & Level 2 & Level 1 \\  & & Accuracy & Accuracy & Accuracy & Accuracy & Accuracy \\  & & (X.X.X.X) & (X.X.X.-) & (X.X.-.-) & (X.-,-.-.) \\  30\% Identity & Random & 0.0 & 1.2 & 3.2 & 19.4 \\  & BLASTTp & 51.4 & 60.0 & 62.5 & 65.7 \\  & Foldseek & 54.9 & 68.3 & 72.9 & 80.6 \\  & ChatGPT & 0.0 & 0.0 & 1.6 & 28.9 \\  & Pika & 20.6 & 37.7 & 46.1 & 61.6 \\  & CLEAN & **55.1** & **68.8** & **74.8** & **84.5** \\ 
30-50\% identity & Random & 0.0 & 0.7 & 3.6 & 22.5 \\  & BLASTTp & **81.1** & 87.9 & 90.7 & 92.3 \\  & Foldseek & 79.8 & 87.1 & 90.5 & 95.0 \\  & ChatGPT & 0.0 & 1.4 & 3.0 & 34.8 \\  & Pika & 37.7 & 50.2 & 60.0 & 73.8 \\  & CLEAN & 80.2 & **88.0** & **91.6** & **95.5** \\  Previously & Random & 0.0 & 0.7 & 4.7 & 22.3 \\ Misclassified & BLASTTp & 35.1 & 70.9 & 78.4 & 78.4 \\ (Price) & Foldseek & **41.2** & **82.4** & **93.2** & **96.6** \\  & ChatGPT & 0.0 & 9.5 & 17.6 & 37.2 \\  & Pika & 4.1 & 50.7 & 64.9 & 82.4 \\  & CLEAN & 31.8 & 74.3 & 81.8 & 85.8 \\  Promiscuous & Random & 0.5 & 4.1 & 9.0 & 41.1 \\  & BLASTTp & **93.7** & **94.8** & **95.2** & **95.9** \\  & Foldseek & 88.0 & 90.9 & 92.4 & 94.4 \\  & CLEAN & 69.4 & 77.9 & 81.5 & 87.0 \\   

Table 2: **Performance of various methods on Task 1.** Performance is measured as k=1 classification accuracy (%). However, for the promiscuous split, we use k=(number of true ECs) for Random and CLEAN and report the average accuracy across all true EC numbers. For this split, we did not evaluate ChatGPT and Pika, as we prompted for only a single EC in the response. For BLAST and Foldseek, we use all EC numbers associated to the top single hit. More details are provided in Section A.4. CLEAN is a state-of-the-art method at the time of publication. Other methods such as ProteinInfer, HiFi-NN, Enzhier, ProtEx, PhiGnet, etc. could also be benchmarked here.

Overall, these results suggest that there is still room to improve classification of protein sequences with low sequence identity (_<30%_) and that lie near multiple activity peaks (_Price_). In fact, Foldseek has the best performance on the _Price_ split, suggesting that structure can be conserved, even in scenarios where sequence is misleading. The _promiscuous_ split is not easy for all methods; even though the test sequences have high sequence identity to the train set, CLEAN sometimes misses EC numbers. Interestingly, BLASTp and Foldseek perform close to the state-of-the-art, even when compared to more complex ML models. Future models could take advantage of this finding to augment training, as ProtEx does . Additional details on the implementation of each method can be found in Appendix A.4.

## 5 Task 2: Enzyme Retrieval

Task 2, enzyme retrieval from a query reaction, tests the ability of a model to extrapolate to unseen reactions. Task 2 has not been formalized or explored in previous studies, but it is equally as important as Task 1, as it applies to a use case where a scientist or engineer is seeking to identify a previously characterized enzyme sequence that can perform a novel (unannotated) reaction (Figure 1D). Typical applications include: an environmental engineer looking for an enzyme to degrade a toxic pollutant , an enzyme engineer looking for an enzyme to catalyze a selective reaction for drug synthesis [89; 90], or a gene annotator identifying the gene for an "orphan" enzyme with known function but unknown sequence . For Task 2, a query reaction is passed through a trained model to perform retrieval to an EC number and its associated proteins that are likely to be able to perform that reaction.

**Splits for Task 2.** Task 2 aims to evaluate how well a model generalizes to unseen reactions with different levels of difficulty. The train-test splits for Task 2 (_easy_, _medium_, and _hard_) are summarized in Table 3 and visualized in Appendix Figure A.3B. We equate greater difficulty with a more challenging train-test split; in a harder set, the test reactions are less similar to reactions in the corresponding train set. We decide similarity based on the amount of overlap in EC number (e.g., a reaction from 4.2.1.20 is considered more similar to another one in 4.2.1.20 than one from 4.2.1.1).

* _easy_ split: EC numbers are randomly sampled at EC level 4 (X.X.X.X) and then randomly mapped to reactions, which are held out as the test set.
* _medium_ split: the same reactions are used for testing as the easy set, but all other reaction-EC pairs which share the same EC level 4 (X.X.X.X) are held out from training.
* _hard_ split: random EC numbers are sampled at EC level 3 (X.X.X.-) and all reactions under that EC3 are held out from training, while a subset of reactions from the held-out EC numbers are used for testing.

The sequences in the test splits are distributed generally evenly across different EC numbers (Appendix Figure A.4). From _easy_ to _medium_ to _hard_, the test set reactions also become more dissimilar to their respective training sets (Figure 2B). Reaction similarity was quantified by DRFP, which is a reaction representation that uses set differences between product and reactant fingerprints and has demonstrated solid performance without requiring model training . Overall, Task 2 is more

   Split & Description & Train & Train \\ name & ECs & samples & samples \\  Easy & Certain reactions are held out, sampled uniformly across ECs, but no EC numbers are held out. The test set is the same as the holdout set. & 4,960 & 61,373 & 393 \\ Medium & All reactions corresponding to certain ECs are held out, at EC level 4 (X.X.X.X). Test set reactions are sampled uniformly across ECs from the holdout set. & 57,691 & 393 \\ Hard & All reactions corresponding to certain ECs are held out, at EC level 3 (X.X.X.-). Test set reactions are sampled uniformly across ECs from the holdout set. & 3,052 & 35,252 & 460 \\   

Table 3: **Summary of train-test splits used in Task 2.** Certain reactions are held out, and “samples” refers to the number of reaction-EC pairs.

complex than Task 1, because unlike Task 1, entire EC numbers are held out from the training set (i.e., the classification output vocabulary is open rather than closed), so multiple modalities must be considered to link the unseen reactions to their respective EC numbers. More details on splitting can be found in Appendix A.2.

**CREEP baseline method.** There is a lack of models that have been tested for their ability to generalize beyond annotated reactions and none that have considered reaction, text and sequence together, so we develop a contrastive learning method for this task, called **C**ontrastive **R**eaction-**Enzym**E** Pretraining (CREEP) (Figure 3). Our approach is related to CLIPZyme , which uses contrastive alignment of reactions represented as 2D graphs and protein structures represented as 3D graphs. Somewhat differently, CREEP leverages finetuning of pretrained language models, rxnfp  and ProtT5  to learn aligned representations of reactions and proteins, respectively. Rxnfp is a BERT-style  language model that was trained on reactions represented as SMILES/SMARTS strings and has demonstrated state-of-the-art performance on reaction type classification . ProtT5 is a T5 protein language model that has been used for prediction of various protein properties and demonstrates similar performance to ESM . We used these language models for simplicity and their ease of finetuning. Uniquely, CREEP can learn on a third modality (CREEP w/ Text): textual description of the EC number based on gene ontology , which is encoded using SciBERT . The use of text has recently been show to boost function prediction tasks . More details on CREEP training can be found in Appendix A.3.

**Task 2 benchmarking results.** Benchmarking results for Task 2 are summarized in Table 4. We start with two methods as baselines: randomly guessing a ranking of EC numbers (Random) and finding the most similar reaction in the train set to the test query (Similarity Baseline). For the Similarity Baseline, we used DRFP to represent chemical reactions . Details on the downstream multimodal retrieval process can be found in Appendix A.3 and Figure A.4. Methods like CREEP and CLIPZyme retrieve reference protein sequences based on a query reaction, and those proteins are mapped to a ranking of retrieved EC numbers; thus, they can generalize to EC numbers that are not linked to reactions in (i.e. missing from) the training set.

Overall, the Similarity Baseline is quite strong and performs much better than Random. ChatGPT was prompted with reactions written using compound IUPAC names rather than SMILES strings, both with (ChatGPT w/ Text) and without (ChatGPT) textual descriptions from gene ontology . We only show ChatGPT performance for the easy split, as ChatGPT is not a good measure of generalization on the medium and hard splits. These both require "leaving out" entire EC numbers before training to ensure there is no data leakage.

Figure 3: **Model architecture for CREEP.** CREEP aligns reaction and protein sequence using contrastive learning to perform downstream retrieval between domains. Optionally, CREEP training can be augmented by using textual description as a third modality to bridge the other two. Model sizes by number of parameters are also listed.

On the more difficult test sets, CREEP offers an advantage compared to the Similarity Baseline, and particularly when combined with textual description: CREEP (w/ Text). This suggests that utilizing protein sequence information as a modality is useful. Still, performance on the harder splits is weak across the board, which suggests that there is significant room for future improvement, although retrieval of EC class may not be the ideal metric for out-of-distribution reaction classification. We anticipate that contrastive alignment with textual descriptions will play an increasingly important role in enzyme retrieval [78; 77; 65], and there is opportunity for better curation of these descriptions. Alternatively, CLIPZyme  represents reactions and proteins as graphs, but performance is lower, potentially due to some sample loss from some data being unable to be processed to graphs during training and inference. Here, performance could likely be improved by optimizing training hyperparameters. Additional details on the implementation of each method are in Appendix A.4, and additional benchmarking results are presented in Figure A.6.

## 6 Discussion

We made an important design choice in the CARE benchmarks: to perform classification and retrieval at the coarse-grained level of EC numbers, which could be a limitation. Enzymes can often perform many reactions, meaning it is an acceptable assumption that enzymes belonging to the same EC number will share the capacity to perform similar reactions, even if they are not directly annotated for all reactions. The ultimate task in this domain is to perform direct reaction to protein sequence retrieval and vice-versa (as in CLIPZyme  and Reactzyme ). However, currently the data for validation is limited, and we believe that many negative examples in those datasets may actually be feasible protein-reaction pairs. As experimentalists obtain higher resolution annotations of proteins and their associated reactions, this will become more realistic. Furthermore, our goal was to provide benchmarks for a broad range of functions, but model evaluation could also be performed on more specific classes of enzymes with direct protein-reaction pairs, as explored by EnzymeCAGE . Other ways to hold out proteins with difficult-to-predict functions could also be explored. Some EC numbers are also incorrectly annotated, which are discussed in more detail here .

In the future, the proposed train-test splits could be refined for both tasks. For the Task 1 splits, there is some leakage in the sequence identity, with some sequences in test sets lying outside of the enforced sequence identity ranges, likely due to the different sequence similarity algorithms used

   Split & Method & Level 4 & Level 3 & Level 2 & Level 1 \\  & & Accuracy & Accuracy & Accuracy & Accuracy & Accuracy \\  & & (X.X.X.X) & (X.X.X.-) & (X.X.-,-) & (X.-,-,-) \\  Easy & Random & 0.0 & 1.0 & 4.6 & 22.9 \\  & Similarity Baseline & 59.3 & 77.1 & 85.2 & 90.6 \\  & ChatGPT* & 4.8 & 22.6 & 43.5 & 71.0 \\  & ChatGPT (w/ Text)* & 13.7 & 56.7 & 81.4 & **98.5** \\  & CLIPZyme & 12.2 & 39.9 & 61.8 & 79.9 \\  & CREEP & 39.4 & 66.4 & 79.9 & 92.9 \\  & CREEP (w/ Text) & **60.3** & **89.3** & **93.9** & 96.7 \\  Medium & Random & 0.0 & 0.5 & 4.1 & 17.0 \\  & Similarity Baseline & 0.0 & 40.2 & 55.7 & 73.3 \\  & CLIPZyme & 2.0 & 26.0 & 46.6 & 69.0 \\  & CREEP & 4.1 & 44.3 & 63.1 & 86.5 \\  & CREEP (w/ Text) & **7.4** & **59.5** & **75.6** & **92.1** \\  Hard & Random & 0.0 & 0.9 & 1.5 & 18.3 \\  & Similarity Baseline & 0.0 & 0.0 & 13.5 & 42.0 \\  & CLIPZyme & 1.1 & 4.1 & 13.5 & 46.7 \\  & CREEP & 1.3 & 4.8 & 18.7 & **57.6** \\  & CREEP (w/ Text) & **1.3** & **9.8** & **22.2** & 57.2 \\   

Table 4: **Performance of various methods on Task 2. Performance is measured as k=1 retrieval accuracy (%). *denotes that there may be data leakage causing performance to be inflated. Bolded accuracy is the best model.**(MMseqs2 vs Diamond BLAST). MMseqs2 utilizes cascaded clustering which pre-filters based on initial clusters in the target set , while BLAST attempts to identify the closest sequence within the specified set based on local similarity. Another limitation is that while there are no duplicate reactions present in both the train and test sets, some of the test reactions are very similar to reactions in the training set despite having different ECs. It would be beneficial to do a more detailed analysis of reaction similarities and explore other representations of reactions to understand which reactions can be considered equivalent. Many EC classes also involve multi-complex enzymes; in other words, certain subunits of these enzyme are not actually performing catalytic activity. Future work could filter out some of the non catalytically active subunits or act to specifically predict the catalytic subunit, or the entire complex based on a reaction. Over time, the train-test splits should be updated as additional functional annotations are acquired and compiled in databases. For example, while over 36 million sequences in BRENDA/UniProt have EC numbers associated with them, these are often detected using homology based models, which may incorrectly assign EC numbers. A more detailed analysis of the specific failure modes of these ML models could be valuable future work .

We opted to assess performance using accuracy due to its simplicity and interpretability, but other retrieval and virtual screening metrics such as BEDROC and enrichment could also be explored. For the promiscuous enzymes with multiple EC numbers, we reported accuracy averaged across all of the true labels, but other classification metrics such as precision and recall should be considered in future work. The number of retrieved ECs could also be chosen using strategies like max separation and p-value as implemented in CLEAN .

There is also significant opportunity to use other modalities such as textual description and protein structure for more effective representations of enzymes and reactions, or to improve the text-based annotations. Gene context is also useful for annotating protein function and could be incorporated into future benchmarks . The textual annotations used in this work are direct textual references to the EC class, meaning that the models in task 2 that incorporate text (e.g. CREEP), are effectively learning the relationship between the EC as a word and the EC as a number, in addition to the similarity of the reaction. Other tools such as Pika which use multiple types of textual description extend this to learn relations about the enzyme, and could be used in future iterations of CREEP. Future work will involve incorporating structure and graph based representations  into CREEP, similar to those used in CLIPZyme . We also plan to do a more detailed analysis of the representations learned by CREEP. The addition of textual description in CREEP potentially introduces indirect data leakage between the train and test sets, so future iterations of CARE will need to consider this. Future work should also consider how to include protein function prediction models that go beyond enzymes-or models that would like to use additional data/modalities [59; 102]-into the CARE evaluation framework. LAB-Bench provides a framework for evaluating scientific reasoning using language, which will increasingly intersect with CARE . A major bottleneck we encountered was that many models, including language models, were not available or difficult to use, limiting our ability to include them as benchmarks. There is also room to improve the prompt engineering of language models which could be further explored to enhance the performance of these models.

We finally note that methods used to retrieve dangerous proteins that could be used as biowaeapons or to synthesize dangerous chemicals is a concern. The implications of this are discussed here .

## 7 Conclusion

Predicting the functions of enzymes is important for many applications ranging from gene annotation to enzyme engineering. While many models exist to classify enzyme function via EC numbers, there are no standardized benchmarks for evaluation of these models. Furthermore, no existing models have been tested for generalization beyond annotated reactions. To address this need, we introduce CARE, which is a benchmarking suite to formalize model evaluation for these two tasks. We also present CREEP, a model which uses multimodal contrastive learning and is one of the first models that can perform the latter task. We encourage developers to integrate their current and future methods or benchmarking results into the CARE Github repository (https://github.com/jsunn-y/CARE/). Overall, CARE is an important tool for encouraging progress in enzyme functional annotation. We believe that we are just seeing the beginning of the widespread adoption of multimodal models for protein functional prediction, and we expect that many researchers will find CARE useful for formulating and evaluating their models.