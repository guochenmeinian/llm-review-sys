# Task Confusion and Catastrophic Forgetting in Class-Incremental Learning: A Mathematical Framework for Discriminative and Generative Modelings

Task Confusion and Catastrophic Forgetting in Class-Incremental Learning: A Mathematical Framework for Discriminative and Generative Modelings

Milad Khademi Nori

Electrical and Computer Engineering

Queen's University

Kingston, Ontario, Canada

19mkn1@queensu.ca

Milad's current affiliation is Toronto Metropolitan University, and his current email is mkn@torontomu.ca. His personal email is miladkhademinori@gmail.com.

Il-Min Kim

Electrical and Computer Engineering

Queen's University

Kingston, Ontario, Canada

ilmin.kim@queensu.ca

Milad's current affiliation is Toronto Metropolitan University, and his current email is mkn@torontomu.ca. His personal email is miladkhademinori@gmail.com.

###### Abstract

In class-incremental learning (class-IL), models must classify all previously seen classes at test time without task-IDs, leading to task confusion. Despite being a key challenge, task confusion lacks a theoretical understanding. We present a novel mathematical framework for class-IL and prove the Infeasibility Theorem, showing optimal class-IL is impossible with discriminative modeling due to task confusion. However, we establish the Feasibility Theorem, demonstrating that generative modeling can achieve optimal class-IL by overcoming task confusion. We then assess popular class-IL strategies, including regularization, bias-correction, replay, and generative classifier, using our framework. Our analysis suggests that adopting generative modeling, either for generative replay or direct classification (generative classifier), is essential for optimal class-IL.

## 1 Introduction

Incremental learning (IL) has garnered significant interest in academia and industry [1; 2] due to its ability to (i) achieve more resource-efficient learning by avoiding retraining models from scratch with new data, (ii) reduce memory usage by eliminating the need to store raw data, which is vital for complying with privacy regulations, and (iii) develop a learning system that mirrors human learning . There are two categories of incremental learning settings in the literature : (i) task-based [5; 6; 7] and (ii) task-free [8; 9; 10; 11].

Task-based category itself consists of three scenarios : (a) task-incremental learning (task-IL), (b) domain-incremental learning (domain-IL), and (c) class-incremental learning (class-IL). The three scenarios differ at the test time, where the task-IL scenario is given with the task-ID, the domain-IL does not need task-ID to begin with, whereas the class-IL must infer task-ID. The second category, which is task-free, aims to banish the notion of task (boundary) at all, both at the training and test time.

This paper focuses on task-based class-IL, currently the most popular regime [12; 13; 14; 15]. However, our theoretical results and proposed scheme are applicable to task-free settings as well [8; 9]. Progressing towards task-free learning is important because IL, like the brain, should aim to be less reliant on supervision, eliminating the need for task-IDs.

For incremental learning including class-IL, the main challenge had been thought to be catastrophic forgetting (CF), which _broadly_ refers to _any_ performance drop on tasks that are previously learned after learning a new one . For class-IL, however, we discourage the usage of this language because it has recently turned out that, in class-IL, not all the performance drop is caused by "forgetting" and indeed most of the performance drop is inflicted by task confusion (TC) . TC originates from the fact that in class-IL, the classes residing in distinct tasks are never seen together; nonetheless must be discriminated among each other at the test time _absent_ task-ID (meaning that task-ID must be inferred).

Attributing all performance drops to CF in class-IL is misleading. Using the term "forget" implies that something previously learned is forgotten. However, in class-IL, the model has never learned how to distinguish among tasks because it has never seen those tasks together. At test time, it must infer the task-ID to make these distinctions. Thus, it does not make sense to say that "it forgot how to distinguish among those tasks." In class-IL, we differentiate between performance drops caused by TC and CF.

Despite TC being recently discovered as the main obstacle in class-IL [16; 3], it has not yet been well-understood mathematically/theoretically, and almost all the class-IL works attribute CF, which used to be the only problem in the task-IL scenario, to the performance drop observed in class-IL scenario. And, this is the problem. For that, in Fig. 1 we visualize how TC emerges: when a class-IL model learns tasks 1 and 2 in Figs. 1(a) and 1(b), respectively, it can perform intra-task discrimination in Fig. 1(c); however, it is still incapable of performing inter-task discrimination, whose absence is shown in Fig. 1(c) with a dotted line. This incapability is not because the class-IL model has forgotten the knowledge of task 1 after learning task 2; it is indeed because it has not learned to make inter-task distinction in the first place. In other words, failure in inter-task discrimination has nothing to do with CF; it is about TC which stems from the fact that the class-IL model has never seen classes of different tasks together to be able to make the distinction.

There are two adjacent studies to our work: (i) Kim's  and (ii) Soutif-Cormerais' . Although the scope of Kim's work and ours are similar, there are key differences:

* Kim's paper does neither define nor discuss TC. There is not any mention of the term TC throughout the paper, let alone proving its occurrence.
* Kim's work does not prove that in generative modeling TC does not occur while in discriminative modeling it does. There is no mention of discriminative modeling or generative modeling throughout Kim's work.
* Kim's mathematical framework and ours are very different in the sense that their mathematical framework does not appreciate the relationship between TC and discriminative/generative modeling (Comprehensive related work is presented in Appendix).

Also, there are significant contrasts between Soutif-Cormerais' work and ours:

* Soutif-Cormerais' work presents empirical results based on black-box experiments in favor of the importance of cross-task features to mitigate TC. The provided evidence clearly does not prove any statement; rather, it is suggestive. Whereas, our work adopts a white-box approach by breaking down the loss of an N-way discriminative classifier into \(\) in Lemma 1 and proving that indeed the optimal performance shall not be obtained unless off-diagonal losses are taken into account. Our approach for studying the role of TC is based on rigorous mathematical analysis.
* Soutif-Cormerais' work is silent on the whereabouts of TC in discriminative/generative modeling. Whereas, our work, by means of rigorous mathematical analysis, proves that while TC does happen in discriminative modeling, it does not in generative modeling .

We believe that the essence of TC and CF as well as their distinction have not yet been well-studied from a theoretical perspective. This motivated the current study. In this paper, we present three contributions:

Figure 1: Task Confusion in Discriminative and Generative Modeling.

* We introduce a novel mathematical framework that distinguishes between TC and CF in class-IL and task-free settings, clarifying their distinct roles. Unlike existing definitions that often conflate TC and CF based on overall performance, our framework provides a clear distinction.
* Utilizing this framework, we present the Infeasibility Theorem, demonstrating that achieving optimal class-IL in discriminative modeling is impossible even with CF prevention, due to TC. Conversely, we propose the Feasibility Theorem, showing that optimal class-IL is achievable in generative modeling if CF is prevented.
* We further offer corollaries for various class-IL strategies, such as regularization, bias-correction, replay-based methods, and generative classifier schemes, allowing us to discuss their optimality.

In the next section, for discriminative modeling, Lemmas 1 and 2 form the groundwork that leads to Theorem 1 and its subsequent Corollaries 1 through 5. For generative modeling, Lemma 3 underpins Theorem 2 and Corollary 6. Furthermore, Hypothesis 1 is derived from the principles outlined in Lemma 1.

## 2 Mathematical framework for Class-IL

In any incremental learning including class-IL, the goal is to get close to (ideally achieve) the ultimate performance of the non-incremental learning without any forgetting of past tasks and any confusion among tasks, which is obtained when all the data for all tasks are simultaneously available for training. This is a performance upper bound, which will be later denoted as the _joint_ scheme in Table 2. To achieve the goal in class-IL, we present our mathematical framework and formulate the training problems to resolve TC and CF. The formulations of TC and CF problems are in themselves meaningful contributions.

In this section, after formulations of TC and CF, we present our first theorem (i.e., the Infeasibility Theorem), where we prove that achieving the optimal class-IL via the implementation of conditional probability \(P(Y|X)\) (equivalent to _discriminative_ modeling, which is the common practice in the class-IL literature) is essentially infeasible even after preventing CF. Then, in another theorem (i.e., Feasibility Theorem), we prove that achieving the optimal class-IL as an implementation of joint probability \(P(X,Y)\) (i.e., _generative_ modeling) is feasible.

### Discriminative modeling

First, we analyze discriminative modeling: for that, we prove a lemma stating that an implementation of conditional probability (via discriminative modeling) as an \(N\)-way classifier (with \(N\) classes) is equivalent to the implementation of \(\) binary classifiers. This lemma will be essential for formulating and understanding the problems of TC and CF.

To that end, we start by defining the classifier's loss function: let \(I_{}\) denote the classification error as follows:

\[I_{}=_{}v(f_{}(x),y)p(x, y)\ dxdy\] (1)

where \(v(f_{}(x),y)\) is a given loss function, \(x,y,\) are the input data and the label, \(f_{}()\) denotes the model parameterized by \(\), and \(p(x,y)\) is the joint probability density function of \((x,y)\). Now, we present our lemma in the following.

**Lemma 1** (Conditional Probability Equivalence Lemma): _An \(N\)-way discriminative classifier parameterized by \(\) implementing conditional probability \(P(Y|X)\) with the loss function defined in Eq. 1 is equivalent to the implementation of \(\) virtual binary classifiers as follows:_

\[I_{}=_{k=1}^{N}_{l=1,l k}^{N}_{ _{kl}_{kl}}v(f_{}(x),y)p(x,y)\ dxdy\] (2)

_where \(_{kl}=_{k}_{l}\), \(_{kl}=_{k}_{l}\), \(_{k},_{l}\), \(_{k},_{l}\), \(_{k}_{l}=\), \(_{k}_{l}=\), for \(k l\)._Proof.: _See Appendix A._ 

Simply put, this lemma says that, for example, a \(3\)-way classifier (\(N=3\)) can be seen as being made up of \(3\) underlying binary classifiers (\(=3\)). Imagine a \(3\)-way classifier that is supposed to discriminate between cat, dog, and rabbit; this classifier, according to our lemma, can be deemed as having \(3\) underlying binary classifiers that classify between cat-dog, cat-rabbit, and dog-rabbit.

To understand the implications of Lemma 1, we simplify the notation: we define the loss term of each individual binary classifier of Eq. 2 as follows:

\[_{kl}()=_{_{kl}_{kl }}v(f_{}(x),y)p(x,y)\;dxdy\] (3)

where \(_{kl}()\) is the loss corresponding to the virtual binary classifier _discriminating_ between classes \(k\) and \(l\). This enables us to have the overall loss term as simple as follows: \(I_{}=_{k=1}^{N}_{l=1,k l}^{N}_{kl}()\). It is worthwhile to view \(I_{}\) as a two-dimensional array (matrix) of binary classifiers' loss terms given by

\[()=&_{12}()&& _{1N}()\\ _{21}()&&&_{2N}()\\ &&&\\ _{N1}()&_{N2}()&&\] (4)

where \(\) denotes 'undefined.' The diagonal losses are undefined because a given class does not have a loss term with itself due to the nature of _discriminative_ modeling. In summary, what we have done in Eq. 4 is that instead of saying that we have a single loss function to minimize, for example, for our cat-dog-rabbit classifier, we say that there are three binary classifiers, each with its own loss term; and, then we arranged the losses in the form of a matrix.

Now we consider the task-based (discriminative) class-IL model that sequentially observes \(T\) number of tasks, each at a time, with \(C\) classes for each task (i.e., \(N=T C\)). The objective is to achieve the performance (as measured by the loss function in Eq. 1) that one would have achieved via having present all \(T\) tasks together. We can re-write our loss matrix within _our_ system model of class-IL as follows:

\[()=_{11}()&_{12}()&&_{1T}()\\ _{21}()&_{22}()&&_{2T}()\\ &&&\\ _{T1}()&_{T2}()&&_{TT}(),_{ij}()=_{11}^{ij}( )&_{12}^{ij}()&&_{1C}^{ij}()\\ _{21}^{ij}()&_{22}^{ij}()&&_{2C}^{ij}( )\\ &&&\\ _{C1}^{ij}()&_{C2}^{ij}()&&_{CC}^{ij}( )\] (5)

in which \(()\) is rewritten as the task-level loss matrix, of which entry is either an intra-task \(_{ii}()\) or inter-task \(_{ij}()\) (for \(i j\)) loss matrix; and \(_{mn}^{ij}()\) is the loss term between the \(m\)th class of task \(i\) and the \(n\)th class of task \(j\). Note that \(_{mn}^{ij}()=\) for \(i=j,m=n\) (See Appendix B for more detail). We will provide a definition, Definition 1, which specifies how such a discriminative class-IL model learns task by task.

Eq. 4 explains that for example if we have four classes, cat, dog, rabbit, and duck, and the first two are in the first task and the second two in the second task, then we have four task-level matrices. The first one, first row and first column, concerns the loss term of the binary classifier discriminating between cat and dog; the second and third loss matrices, the non-diagonal ones, characterize the loss terms of the binary classifiers discriminating between cat-rabbit, cat-duck, dog-rabbit and dog-duck (inter-task binary classifiers); and finally, the last one, second row and column, stands for the loss of the binary classifier discriminating between rabbit and duck.

**Definition 1** (Discriminative Class-IL): _A discriminative class-IL model'sequentially' trains by minimizing the losses of the diagonal blocks of the loss matrix \(()\) in Eq. 5. That is, a discriminative class-IL model first optimizes \(\) by minimizing \(|_{11}()|\), and then re-optimizes \(\) by minimizing \(|_{22}()|\), etc, where \(||\) operator sums up all (defined) components of the given matrix._

Definition 1 hints at the critical problem; the 'diagonal' is the critical word. The class-IL model only minimizes the diagonal blocks and ignores non-diagonal ones; which is why the TC problem arises: the notorious problem that is misunderstood. This paper clears this misunderstanding.

With Definition 1, now we can also define CF. Properly defining CF is very important since in the literature, CF has been _too broadly_ defined based on the overall performance, which causes CF to get conflated with another crucial phenomenon that is TC (will be later defined in Definition 5).

**Definition 2** (Catastrophic Forgetting):: _Consider a class-IL model that minimizes the loss \(|_{ii}()|\) of task \(i\), achieving the minimal loss \(|_{ii}(}_{i})|\), where_

\[}_{i}=*{argmin}_{}|_{ii}()|.\] (6)

_Then the model proceeds and minimizes the loss \(|_{(i+1)(i+1)}()|\) of task \((i+1)\), achieving the minimal loss \(|_{(i+1)(i+1)}(}_{(i+1)})|\), where \(}_{(i+1)}\) is given by Eq. 6 with \(i\) being replaced by \(i+1\). We state that the model committed Catastrophic Forgetting if_

\[|_{ii}(}_{i})|<|_{ii}(}_{(i+1 )})|.\] (7)

Definition 2 indicates that after learning the second task, the new weights may not be as effective in minimizing the loss of the first task as the weights that we derived by only minimizing the first task. Simply put, our cat-dog binary classifier of the model is no longer working as well as it did prior to learning the rabbit-duck binary classifier.

**Definition 3** (CF-Optimal Class-IL):: _A class-IL model \(^{*}\) is called CF-optimal if solely CF is minimized. Specifically, \(^{*}\) is the model minimizing the sum of losses of all diagonal blocks \(_{ii}()\) ignoring the inter-task losses as follows:_

\[^{*}_{1:T}=*{argmin}_{}_{i=1}^{T}|_{ii}()|.\] (8)

CF-optimal means that the classifier can discriminate between cat-dog (task one); and can discriminate between duck-rabbit (task two), too. Yet, the model does properly classify the classes residing inside each task one and two (intra-task classification), it may still fail at discriminating between different tasks (inter-task classification).

Having defined CF in Definition 2, we will prove its occurrence (in Corollary 1) and its consequent sub-optimality (in Corollary 2), based on Incompatibility Definition and Lemma (Lemma 2). However, before that, it is worth mentioning that in this paper, we make the assumption that tasks are _incompatible_; which is specified in the following. (See Appendix C.)

**Definition 4** (Incompatibility):: _Functions \(f(x)\) and \(g(x)\) which are non-zero are called incompatible and denoted as \(f(x) g(x)\), if the followings are satisfied:_

\[_{x=x^{f}} =_{x=x^{g}}=0, _{x=x^{g}}=_{x=x^{f}} 0,\] \[x^{f} =*{argmin}_{x}f(x), x^{g}=*{argmin} _{x}g(x) x^{f} x^{g}\] (9)

_where \(f(x)\) and \(g(x)\) are differentiable at \(x^{f}\) and \(x^{g}\)._

It could be argued that assuming incompatibility of tasks is unfavorable, as good class-IL and task-free algorithms aim to maximize both forward and backward transfer, which would not exist in the case of incompatible tasks. However, the ultimate intent of our paper is to investigate TC and CF in both discriminative and generative modeling settings. Our assumption is designed to capture TC and CF, not forward and backward transfer. While forward and backward transfer are important in class-IL and task-free learning, they are not the focus of our work.

**Lemma 2** (Incompatibility Lemma):: _For incompatible \(f(x)\) and \(g(x)\), i.e., \(f(x) g(x)\), we can state the following:_

\[x^{*} x^{f},\ x^{g},\ \ x^{*}=*{argmin}_{x}f(x)+g(x),\ \ x^{f}= *{argmin}_{x}f(x),\ \ x^{g}=*{argmin}_{x}g(x).\] (10)

_Proof: See Appendix D. \(\)_

In simple terms, two incompatible tasks (functions) have different minimizers. And, the minimizer of the sum of them is neither of the minimizers of each. This is the case for distinct tasks in practice. From many empirical results , we know that always when new tasks are learned the optimal points of the previous tasks are lost. CF always happens, indicating that incompatibility is always true. With incompatibility we can prove the occurrence of CF in the following corollary.

**Corollary 1** (Catastrophic Forgetting)**: _For the discriminative class-IL model in Definition 1, due to the sequential diagonal optimization, after optimizing for task \((i+1)\) we can state Eq. 7 implying that CF has occurred for task \(i\) when \(|_{ii}()||_{(i+1)(i+1)}()|\)._

_Proof: See Appendix E. _

Having proved the occurrence of CF, we also can state, as in the following, that our class-IL model after learning the second task is not even CF-optimal.

**Corollary 2** (Sub-Optimality Corollary)**: _For the discriminative class-IL model defined in Definition 1, due to the sequential diagonal optimization, after optimizing for task \((i+1)\) the class-IL model may not be CF-optimal if \(_{i^{}=1}^{i}|_{i^{}i^{}}()| |_{(i+1)(i+1)}()|\)._

_Proof: See Appendix E. _

Based on our mathematical framework, we now define TC (which is a major contribution of this paper) and then the optimal class-IL that aims at minimizing TC and CF (which is the ultimate goal of the class-IL as presented in the very beginning of this paper).

**Definition 5** (Task Confusion)**: _Consider a class-IL model that minimizes the loss \(|_{ii}()|\) of task \(i\), achieving the minimal loss \(|_{ii}(}_{i})|\), where \(}_{i}\) is given by Eq. 6. Then the model proceeds and minimizes the loss \(|_{(i+1)(i+1)}()|\) of task \((i+1)\), achieving the minimal loss \(|_{(i+1)(i+1)}(}_{(i+1)})|\). This class-IL model never finds a chance to optimize \(\) by minimizing inter-task loss \(|_{(i)(i+1)}|\). Hence, the class-IL model is confused when it comes to distinguishing classes from two distinct tasks because those loss matrices corresponding to inter-task binary classifiers are not minimized jointly._

Having defined TC, now in the next definition we specify the optimal class-IL model (whose achievement is the ultimate goal of class-IL).

**Definition 6** (Optimal Class-IL): _A class-IL model \(^{**}\) is called optimal if both TC and CF are jointly minimized. Specifically, \(^{**}\) is the model minimizing the summation of losses of all blocks of the loss matrix including all the inter-task blocks as follows: \(^{**}_{1:T,1:T}=*{argmin}_{}_{i=1}^{T} _{j=1}^{T}|_{ij}()|\)._

In the following theorem, we will state our significant discovery that unlike the common belief in the class-IL community, even if CF is prevented, achieving optimal class-IL might be still impossible, and particularly _is_ impossible if there is TC due to the failure in minimizing inter-task blocks (non-diagonal blocks) of the loss matrix Eq. 5.

**Theorem 1** (Infeasibility Theorem)**: _The CF-optimal class-IL model in Definition 3 is not optimal if the entire loss and the diagonal loss are incompatible: \(_{i=1}^{T}_{j=1}^{T}|_{ij}()|_{i=1}^{ T}|_{ii}()|\)._

_Proof: See Appendix E. _

This is interesting because it turns out that achieving optimal class-IL is feasible even when CF is minimized, due to existence of TC as shown in Fig. 2. In Fig. 2, in the left, we show TC and CF for discriminative class-IL model that is optimized by'sequentially' minimizing the diagonal blocks of the loss matrix. When optimized for the next block, the preceding block loss is gradually forgotten, resulting in CF (lighter green), and, the model is not optimized for inter-task loss matrices, resulting in TC (red). In the right figure, we show CF in generative modeling. When the class-IL model is optimized by'sequentially' minimizing the diagonal blocks of the loss matrix, the preceding block loss is forgotten when optimized for the next; however, there is no longer any inter-task block (gray).

Figure 2: Task Confusion in Discriminative and Generative Modeling.

### Generative modeling

In this section, we focus on generative modeling which is promising: it culminates in what we call Feasibility Theorem and offers a solution to address TC. First, we present Joint Probability Equivalence Lemma in the following which helps us to derive the corresponding loss matrix.

**Lemma 3** (Joint Probability Equivalence Lemma):: _An \(N\)-way (class) generative model parameterized by \(\), loss function \(v(f_{}(x),y)\), and data generating process with probability density \(p(x,y)\) is equivalent to the implementation of \(N\) distinct generative models with the following loss: \(_{i=1}^{N}q_{rr}()=I_{}=_{ }v(f_{}(x),y)p(x,y)\ dxdy\) where \(q_{rr}()=_{_{r}_{r}}v(f_{}(x),y)p(x,y)\ dxdy\) in which \(_{r}\), \(_{r}\), \(_{r}_{t}=\), \(_{r}_{t}=\), for \(r t\). Also, \(q_{rr}()\) stands for the loss for the \(r\)th class._

A generative class-IL model'sequentially' trains the model by minimizing the losses of the diagonal blocks of the loss matrix given by

\[()=}()&&& \\ &}()&&\\ &&&\\ &&&}(),_{ii} ()=q_{11}^{ii}()&&& \\ &q_{22}^{ii}()&&\\ &&&\\ &&&q_{CC}^{ii}()\] (11)

in which \(q_{mm}^{ii}\) stands for the loss of the generative model for the \(m\)th class of task \(i\). As we did in the previous section for discriminative modeling in Definitions 1-6, we can define the same properties for generative modeling. In the following theorem, we state that tackling TC through implementation of joint probability \(P(X,Y)\) is feasible--via generative modeling. This makes all the difference.

**Theorem 2** (Feasibility Theorem):: _For the class-IL model adopting generative modeling with loss matrix in Eq. 11, if CF is prevented, meaning that all diagonal blocks are optimal \([_{11}^{*}(),_{22}^{*}(),,_{NN}^ {*}()]\), the model is optimal._

_Proof: See Appendix E. \(\)_

In generative modeling, therefore, optimizing for the diagonals is equivalent to optimizing for all the loss terms. In other words, the losses associated with different tasks/classes are irrelevant; therefore, the loss matrix can be only diagonally optimized as shown in Fig. 2. This sums up this section. The lessons learned so far are: (i) class-IL faces two problems, CF and TC, and (ii) TC is inevitable unless we use generative modeling (as proved in Infeasibility/Feasibility Theorems). These are widely applicable lessons for assessing the optimality of the class-IL schemes.

## 3 Optimality analysis of Class-IL strategies

We analyze the behaviors of popular class-IL strategies including (i) regularization, (ii) bias-correction, (iii) replay, and (iv) generative classifier; among which the first three do discriminative modeling, whereas the last one does generative modeling. Note that even generative replay counts as discriminative modeling because eventually a discriminator performs classification. Generative classifier, however, performs classification only/directly via generative modeling. In this section, we study the optimality of the above class-IL strategies. The following three corollaries (i.e., Corollaries 3, 4, and 5) follow Theorem 1; whereas the last one, Corollary 6, follows Theorem 2 (Table 1 summarizes our discussions in this section).

### Regularization strategies

We start with regularization. As mentioned, regularization is essentially attempting to preserve the optimality of intra-task blocks (represented by the diagonal blocks \(_{ii}()\)'s in Eq. 5) via constraining the proceeding updates to cause as little modifications as possible (e.g., via gradient manipulation), thereby mitigating CF.

**Corollary 3** (Regularization Impotence Corollary):: _A regularized class-IL model, which does discriminative modeling, may minimize CF; however, it never achieves optimal class-IL due to sub-optimal TC._In class-IL/task-free scenarios, discriminative models' performance can be characterized by a hypothesis. These models, which include regularization schemes like None, LwF, EWC, and SI, prioritize minimizing confusion within tasks over distinguishing between tasks. They achieve this by focusing on optimizing for diagonal loss elements and neglecting off-diagonal elements, which makes them proficient at discriminating classes within tasks but limits their ability to differentiate between tasks. As a result, their classification accuracy is upper-bounded by \(100/T\%\), where \(T\) is the number of tasks. This limitation suggests that these models are at best CF-optimal, as they prioritize within-task performance over between-task performance, as observed in our experimental results.

**Hypothesis 1** (CF-optimal Model Corollary): _The performance (as measured by classification accuracy) of the CF-optimal class-IL models, which are the None and regularization schemes in Table 2, is upper-bounded by \(100/T\%\) where \(T\) stands for the number of tasks._

This can be seen in Table 2 (\(T=5\) for MNIST, CIFAR-10, CORe50 and \(T=10\) for CIFAR-100): when the class-IL schemes only tackle CF not TC such as in the None scheme and the regularization strategy, the performance never exceeds \(100/T\%\); because TC (inter-task blocks) is left out sub-optimal2.

Not only that, the results provided in the work by  also support such a _hypothesis_. Nevertheless, there are few results in Masana's  works suggesting that regularization schemes augmented with a technique based on entropy demonstrate performances that exceed the \(100\%/T\) upper bound although the performance is still far from the performances of schemes based on generative modeling counterparts. This is a slight discrepancy between different bodies of studies: on one hand, the reasoning steps for making such a conclusion seem flawless and there are numerical results to back that up and on the other hand, we cannot ignore the slightly incongruuous empirical results. It remains to be investigated how to account for that small increase over the upper bound. This will hopefully be addressed in future works.

So far, we theoretically made it clear what is the distinction between TC and CF; to further empirically distinguish between TC and CF, in Fig. 3 we contrast the performances of the None scheme as well as a typical regularization scheme (Elastic Weight Consolidation, EWC  with hyperparameter \(=5000\)) in two scenarios: (i) task-IL, where the model merely faces CF and (ii) class-IL, which the model faces both TC and CF together. The simulations are run with CNN on CIFAR-10 and only the means are reported after 10 repetitions.

As it can be seen in Fig. 3 (top-left) corresponding to the None scheme, TC causes significantly more performance drop than CF. This is because TC has far more block losses (\(T^{2}-T\) red blocks)

Figure 3: In all the six figures, for the task-IL scenario, the schemes merely face CF (because they are given with the task-ID), and thus, they perform favorably. In the class-IL scenario, however, the models need to discriminate between different tasks, and they usually fail; this is expected due to not minimizing the inter-task block losses.

than CF (\(T\) green blocks) to be minimized as it can be seen in Fig. 2 (left). In Fig. 3 (top-middle) corresponding to the EWC scheme, we observe that although EWC is able to minimize CF, the amount of TC remains unchanged indicating the ineffectiveness of regularization for TC (see Appendix F).

### Distillation strategies

Knowledge distillation  serves as an effective regularization strategy in mitigating catastrophic forgetting. Unlike approaches such as EWC and SI, which impose constraints on parameter updates, knowledge distillation focuses on ensuring consistency in the responses of the new and old models. This distinctive feature provides a broader solution space, enabling the model to explore optimal parameters that cater to both new and old tasks.

However, because the knowledge distillation strategy inherently operates as a regularization technique, it is upper-bounded like all other regularization strategies, as outlined in Hypothesis 1. This upper-boundedness can be observed in Table 2 for _Dis_ scheme .

### Bias-correction strategies

Bias-correction specifically attempts to mitigate TC; however, only minutely (see Fig. 3 (top-right) and Fig. 4 pertaining to the AR1 scheme): it removes the bias from inter-task binary classifiers, slightly reducing the inter-task block losses. Nonetheless, bias-correction is not enough to achieve optimal inter-task discrimination, since bias parameters constitute a fringe minority of all the parameters of models.

**Corollary 4** (Bias-Correction Impotence Corollary).: _For a bias-corrected class-IL model, which does discriminative modeling, neither the optimality of diagonal blocks \(_{ii}()\) is ensured nor inter-task blocks \(_{ij}()\) for \(i j\); therefore it never achieves optimal class-IL._

The slight improvement in CWR , CWR+, AR1 , and Label , via correcting the biases shows itself in Table 2, where the bias-corrected schemes outperform the schemes of regularization (and None) by doing as little as only correcting the biases, thereby mitigating TC. This suggests the priority of the TC problem over CF.

### Generative replay

On the other hand, there exists the generative replay strategy that attempts to minimize the objective function in Eq. 1 via a surrogate density \((x,y)\), which generates pseudo samples, to mimic the real density of \(p(x,y)\). For generative replay we can present the following corollary.

**Corollary 5** (Generative Replay Corollary).: _A generative replay-based class-IL model, which is discriminative, possessing a surrogate density \((x,y)\) can achieve optimal class-IL iff \((x,y)\) is identical to the real density \(p(x,y)\)._

 
**Strategies** & **CF** & **TC** & **BC** & **Theoretical Remarks** \\  Regularization & ✓ & ✗ & ✗ & see Corollary 3 (Regularization Impotence Corollary) and Hypothesis 1 (CF-optimal Model Corollary) \\  Distillation & ✓ & ✗ & ✓ & since distillation is essentially regularization Corollary 3 (Regularization Impotence Corollary) and Hypothesis 1 (CF-optimal Model Corollary) \\  Bias-correction & ✗ & ✗ & ✓ & see Corollary 4 (Bias-Correction Impotence Corollary) \\  Generative replay & ✓ & ✓ & ✓ & see Corollary 5 (Generative Replay Corollary) \\  Generative classifier & ✓ & ✓ & ✓ & see Corollary 6 (Generative Classifier Feasibility Corollary) \\  

Table 1: Comparison of different strategies in addressing CF, TC, and Bias-Correction (BC).

Figure 4: Generative classifiers like SLDA and GenC mitigate TC and CF (CIFAR-10).

Although in principle generative replay can achieve optimality and cope with TC and CF because when learning each new task all inter-task and intra-task blocks are minimized, in practice, it is challenging to efficiently train such a surrogate density \((x,y)\), where \((x,y) p(x,y)\) (unless by explicitly storing all/exemplars of the dataset); this can be seen in Table 2 where the family of generative replay, DGR , BI-R, and BI-R+SI , although does well for toy datasets (MNIST), it fails for larger datasets in competition with generative classifier. This can also be seen in Fig. 3 (bottom-left) and Fig. 4 where DGR not only fails to cope with TC but also suffers from CF due to possessing a poor surrogate density \((x,y)\) which cannot capture the real density \(p(x,y)\).

### Generative classifier

Eventually, unlike the previous three strategies (all discriminative modeling), the fourth strategy, the generative classifier (relying on generative modeling), represented by SLDA  and GenC  not only can dispense with rehearsal without worrying about TC, but also promises optimal class-IL as presented in the following corollary.

**Corollary 6** (Generative Classifier Feasibility Corollary): _Following Feasibility Theorem, for a generative class-IL model, which does generative modeling, when CF is minimized, the class-IL model is optimal._

We see in Table 2 (and Figs. 3 (bottom-middle) and (bottom-right)) that SLDA  and GenC  can best cope with TC. For preventing CF, however, these two schemes follow a shared approach which is adopting expansion-based architecture: instead of using the same architecture for all classes, and therefore, forgetting previous classes when new classes are learned, expansion-based architectures grow their model as new classes are learned. Therefore, they do not overwrite new knowledge on the previous knowledge. This is why schemes like SLDA  and GenC  suffer from no CF in Figs. 3 (bottom-middle) and (bottom-right). Also, this can be seen in Fig. 4.

## 4 Big picture and conclusion

Since the advent of AlexNet , the neuroscience community has been skeptical of the deep learning community's discriminative modeling approach for classification [23; 24]. They argue that humans do not learn \(p(y|x)\) for classification (discriminative modeling); instead, humans learn \(p(x)\) which is generative modeling. The primary issue with discriminative modeling is shortcut learning , a concern that has recently gained more attention within the deep learning community . In this work, we investigate how shortcut learning can particularly hinder class-incremental learning. We discuss how shortcut learning in discriminative modeling leads to task confusion and argue that generative modeling, in principle, addresses this issue.

We proposed a mathematical framework to formalize problems of class-incremental learning and task-free: task confusion and catastrophic forgetting. We proved that in discriminative modeling the non-diagonal block losses are not minimized, which causes task confusion resulting in sub-optimal performance for the class-incremental learning model even though catastrophic forgetting is prevented. We presented our empirical results confirming that generative modeling does not suffer from task confusion because there are no non-diagonal blocks that need to be minimized. We observed that while generative modeling is effective for coping with task confusion, adopting expansion-based architectures can overcome catastrophic forgetting.

    & **MNIST** & **CIFAR-10** & **CIFAR-100** & **CORe50** \\    \\ None & \(19.92 0.02\) & \(18.74 0.29\) & \(7.96 0.11\) & \(18.65 0.26\) \\ Joint & \(98.23 0.04\) & \(82.07 0.15\) & \(54.08 0.27\) & \(71.85 0.30\) \\   \\ EWC & \(19.93 0.06\) & \(18.77 0.31\) & \(8.41 0.09\) & \(18.70 0.27\) \\ SI & \(19.88 0.09\) & \(18.00 0.33\) & \(9.32 0.07\) & \(18.61 0.22\) \\   \\ Dis & \(19.87 0.08\) & \(18.31 0.44\) & \(9.79 0.13\) & \(19.35 0.31\) \\   \\ GWR & \(30.96 2.33\) & \(18.63 1.44\) & \(21.98 0.57\) & \(40.11 1.15\) \\ CWR+ & \(39.02 2.88\) & \(22.69 1.17\) & \(9.29 0.19\) & \(40.78 1.05\) \\ AR1 & \(49.38 2.36\) & \(25.13 1.18\) & \(21.01 0.51\) & \(44.13 1.06\) \\ Label & \(33.01 2.01\) & \(19.21 1.12\) & \(22.35 0.31\) & \(41.55 1.01\) \\   \\ DGR & \(91.12 0.65\) & \(18.13 1.85\) & \(9.41 0.30\) & - \\ BI-R & - & - & \(21.41 0.19\) & \(61.04 1.01\) \\ BI-R+SI & - & - & \(34.34 0.23\) & \(62.51 0.29\) \\   \\ SLDA & \(87.31 0.02\) & \(38.33 0.04\) & \(44.41 0.00\) & \(70.80 0.00\) \\ GenC & \(93.75 0.09\) & \(56.02 0.04\) & \(49.53 0.07\) & \(70.80 0.10\) \\   

Table 2: The means and \(\) SEMs of accuracies in class-IL scenarios with \(10\) runs on four benchmarks.