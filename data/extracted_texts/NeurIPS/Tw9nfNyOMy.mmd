# _Vista_: A Generalizable Driving World Model with

High Fidelity and Versatile Controllability

 Shenyuan Gao\({}^{1,2}\)   Jiazhi Yang\({}^{2}\)   Li Chen\({}^{2,5}\)   Kashyap Chitta\({}^{3,4}\)   Yihang Qiu\({}^{2}\)

**Andreas Geiger\({}^{3,4}\)\({}^{}\) Jun Zhang\({}^{1}\)\({}^{}\)   Hongyang Li\({}^{2,5}\)\({}^{}\)**

\({}^{1}\) Hong Kong University of Science and Technology  \({}^{2}\) OpenDriveLab at Shanghai AI Lab

\({}^{3}\) University of Tubingen  \({}^{4}\) Tubingen AI Center  \({}^{5}\) University of Hong Kong

Code and model: github.com/OpenDriveLab/Vista

Demo page: opendrivelab.com/Vista

###### Abstract

World models can foresee the outcomes of different actions, which is of paramount importance for autonomous driving. Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application. In this paper, we present _Vista_, a generalizable driving world model with high fidelity and versatile controllability. Based on a systematic diagnosis of existing methods, we introduce several key ingredients to address these limitations. To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information. We also devise an effective latent replacement approach to inject historical frames as priors for coherent long-horizon rollouts. For action controllability, we incorporate a versatile set of controls from high-level intentions (command, goal point) to low-level maneuvers (trajectory, angle, and speed) through an efficient learning strategy. After large-scale training, the capabilities of Vista can seamlessly generalize to different scenarios. Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over \(70\%\) of comparisons and surpasses the best-performing driving world model by \(55\%\) in FID and \(27\%\) in FVD. Moreover, for the first time, we utilize the capacity of Vista itself to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions.

+
Footnote †: Primary contact to Shenyuan at sysgao@connect.ust.hk  \({}^{}\)Equal advising.

## 1 Introduction

Driven by scalable learning techniques, autonomous driving has made encouraging strides over the past few years [18; 58; 135]. However, intricate and out-of-distribution situations are still intractable for state-of-the-art techniques . One promising solution lies in world models [57; 76], which infer the possible future states of the world from historical observations and alternative actions, in turn assessing the feasibility of such actions. They hold the potential to reason with uncertainty and avoid catastrophic errors [54; 76; 127], thereby promoting generalization and safety in autonomous driving.

Although a primary prospect of world models is to enable the generalization ability to novel environments, existing driving world models are still constrained by data scale [90; 125; 127; 143; 147] and geographical coverage [54; 61]. As summarized in Table 1 and Fig. 1, they are also often confined to low frame rates and resolutions, resulting in a loss of critical details. Furthermore, most models only support a single control modality such as the steering angle and speed. This is insufficient to express various action formats ranging from high-level intentions to low-level maneuvers, and incompatible with the outputs of prevalent planning algorithms [12; 14; 21; 56; 58; 64]. In addition, generalizing action controllability to unseen datasets is understudied. These limitations impede the applicability of existing works, making it imperative to develop a world model that overcomes these limitations.

To this end, we introduce _Vista_, a driving world model that is proficient in cross-domain generalization, high-fidelity prediction, and multi-modal action controllability. Specifically, we develop the predictive model on a large corpus of worldwide driving videos  to foster its generalization ability. To enable coherent future extrapolation, we condition Vista on three essential dynamic priors (Sec. 3.1). Instead of solely relying on the standard diffusion loss , we introduce two explicit loss functions to enhance dynamics and preserve structural details (Sec. 3.1), promoting Vista's ability to simulate realistic futures at high resolution. For flexible controllability, we incorporate a versatile set of action formats, including both high-level intentions such as commands and goal points, as well as low-level maneuvers like trajectories, steering angles, and speeds. These action conditions are injected via a unified interface, which is learned through an efficient training strategy (Sec. 3.2). Consequently, as Fig. 2 shows, Vista acquires the ability to anticipate realistic futures at 10 Hz and 576\(\)1024 pixels, and obtains versatile action controllability across various levels of granularity. We also demonstrate the potential of Vista as a generalizable reward function to evaluate the reliability of different actions.

Our contributions are three-fold: **(1)** We present _Vista_, a generalizable driving world model that can predict realistic futures at high spatiotemporal resolution. Its prediction fidelity is greatly improved by two novel losses that capture dynamics and preserve structures, along with exhaustive dynamic priors to sustain consistency in long-horizon rollouts. **(2)** Propelled by an efficient learning strategy, we integrate versatile action controllability into Vista through a unified conditioning interface. The action controllability of Vista can also generalize to different domains in a zero-shot manner. **(3)** We conduct comprehensive experiments across multiple datasets to verify the effectiveness of Vista. It outperforms the most competitive general-purpose video generator and sets a new state-of-the-art on nuScenes. Our empirical evidence shows that Vista can be used as a reward function to assess actions.

## 2 Preliminary

We initialize Vista with the pretrained Stable Video Diffusion (SVD) , a latent diffusion model for image-to-video generation. For sampling flexibility, SVD adopts a continuous-timestep formula [66; 111]. It converts data samples \(\) to noise \(\) through a diffusion process \(p(|)(,^{2})\), and generates new samples by progressively denoising the latent towards \(_{0}=0\) from Gaussian noise. The training of SVD can be simplified to minimizing \(_{,,}_{}\|D_{}( {n};)-\|^{2}\), where \(D_{}\) is a parameterized UNet denoiser and \(_{}\) is a re-weighting function omitted hereinafter for brevity. Based on this framework, SVD processes a sequence of noisy latent \(=\{n_{1},n_{2},...,n_{K}\}^{K C H W}\) and generates a video with \(K=25\) frames. The generation process is guided by a condition image, whose latent is concatenated channel-wise to the inputs, serving as a reference for content generation.

Despite the high aesthetic quality, SVD lacks several key properties to function as a driving world model. As shown in Sec. 4, the first frame predicted by SVD is not identical to the condition image, making it impractical for autoregressive rollout due to content inconsistency. In addition, SVD struggles with the intricate dynamics of driving scenarios, entailing implausible motions. Moreover,

    &  &  \\  & Data Scale & Frame Rate & Resolution & AngleSpeed & Trajectory & Command & Goal Point \\  DriveSim  & 7h & 5 Hz & 80\(\)160 & ✓ & & & \\ DriveGAN  & 160h & 8 Hz & 256\(\)256 & ✓ & & & \\ DriveDreamer  & 5h & 12 Hz & 128\(\)192 & ✓ & & & \\ DriveWM  & 5h & 2 Hz & 192\(\)384 & ✓ & & & \\ WoVoGoin  & 5h & 2 Hz & 256\(\)448 & ✓ & & & \\ Abriev- & 300h & 2 Hz & 256\(\)512 & & & ✓ & \\ Grand  & 2000h & 2 Hz & 256\(\)448 & ✓ & ✓ & \\ GAIA-1  & 4700h & 25 Hz & 288\(\)512 & ✓ & & & \\  Vista (Ours) & 1740h & 10 Hz & 576\(\)1024 & ✓ & ✓ & ✓ & ✓ \\   

Table 1: **Real-world driving world models.** Trained on large-scale high-quality driving data, Vista performs at high spatiotemporal resolution and supports versatile action controllability. _Private data_.

Figure 1: **Resolution comparison.** Vista predicts at a higher resolution than previous literature.

SVD cannot be controlled by any action format. In contrast, we aim to build a generalizable driving world model that predicts high-fidelity futures with realistic dynamics. It ought to be continuously extendable to long horizons and flexibly controllable by multi-modal actions as illustrated in Fig. 2.

## 3 Learning a Generalizable Driving World Model

As depicted in Fig. 3, Vista adopts a two-phase training pipeline. First, we build a dedicated predictive model, which involves a latent replacement approach to enable coherent future prediction and two novel losses to enhance fidelity (Sec. 3.1). To ensure the generalization to unseen scenarios, we utilize the largest public driving dataset  for training. In the second phase, we incorporate multi-modal actions to learn action controllability with an efficient and collaborative training strategy (Sec. 3.2). Using the ability of Vista, we further introduce a generalizable approach to evaluate actions (Sec. 3.3).

### Phase One: Learning High-Fidelity Future Prediction

**Basic Setup.** Since world models are initiated to predict futures from the current state, the starting of their prediction should be firmly aligned with the condition image. Therefore, we tailor SVD into a dedicated predictive model by imposing the first frame as the condition image and discarding the noise augmentation  during training. With this prediction ability, Vista can perform long-term rollouts by iteratively predicting short-term clips and resetting the condition image with the last clip.

**Dynamic Prior Injection.** Nevertheless, using the aforementioned setup for training often results in irrational dynamics with respect to historical frames, especially in long-term rollouts. We conjecture that this mainly arises from the ambiguity caused by insufficient priors about the tendency of future motions, which is also a common limitation of existing driving world models .

Estimating coherent futures requires at least three essential priors that inherently govern the future motion of instances in the scene: position, velocity, and acceleration. Since velocity and acceleration are the first- and second-order derivative of position respectively, these priors can be entirely derived by using three consecutive frames for conditioning. Concretely, we build a frame-wise mask \(\{0,1\}^{K}\) with a length of \(K\) to indicate the presence of condition frames. The mask is set sequentially following the time order, with at most three elements being assigned as \(1\) to denote three condition

Figure 2: **Capabilities of Vista. Starting from arbitrary environments, Vista can anticipate realistic and continuous futures at high spatiotemporal resolution (A-B). It can be controlled by multi-modal actions (C), and serve as a generalizable reward function to evaluate real-world driving actions (D).**

frames. Instead of concatenating additional channels to the inputs, we inject new condition frames by replacing the corresponding noisy latent \(n_{i}\) with the clean latent \(z_{i}\) encoded by the image encoder. Formally, the input latent is constructed as \(}=+(1-)\) (see Fig. 3 [Left]). To discern the clean latent, we duplicate a new timestep embedding from the pretrained weights and allocate it to the condition frames according to \(\). The timestep embeddings for condition frames and prediction frames are trained separately. Compared to channel-wise concatenation, we find that replacing the latent is more effective and flexible in absorbing varying numbers of condition frames. In addition, we observe that the replaced latent, when applied to SVD directly, does not degrade its generation quality. Thus, the original performance will not be disturbed when the training is launched. Since there is no need to predict the observed condition frames, we exclude them from the loss as follows:

\[_{}=_{,,}} _{i=1}^{K}(1-m_{i})\|D_{}(_{i};)-z_{i}\|^{2},\] (1)

where \(D_{}\) is the UNet denoiser that shares the same architecture with SVD. With the replaced latent holding sufficient priors, Vista can fully capture the status of the surrounding instances and predict more coherent and plausible long-term futures through iterative rollouts. In practice, we leverage the last three frames of a predicted clip as dynamic priors for the next prediction step during rollouts.

**Dynamics Enhancement Loss.** Unlike general videos that cover rather small spaces, driving videos capture much larger scenes . In most driving videos, distant and monotonous regions dominate the view, with the moving foreground instances only occupying a relatively small area . However, the latter often exhibit higher stochasticity, complicating their prediction. Since Eq. (1) supervises all outputs uniformly, it cannot effectively discriminate the nuances of different regions as Fig. 4(b) shows. As a result, the model cannot efficiently learn to predict realistic dynamics in crucial regions.

As the discrepancy between two adjacent frames provides considerable motion patterns [123; 132], we introduce an additional supervision to encourage the learning of dynamics for crucial regions. To be specific, we first introduce a dynamics-aware weight \(=\{w_{2},w_{3},...,w_{k}\}^{K-1 C H W}\) that highlights the regions where the prediction has inconsistent motion compared to the ground truth:

\[w_{i}=\|(D_{}(_{i};)-D_{}(_{i-1};))-(z_{ i}-z_{i-1})\|^{2}.\] (2)

For numerical stability, we normalize \(\) within each video clip. As shown in Fig. 4(c), the weight amplifies the presence of large motion disparities, highlighting dynamic regions while excluding monotonous backgrounds. Given the causality of future prediction, subsequent frames ought to follow previous ones, we define a new loss by penalizing the latter frame of each adjacent frame pair:

\[_{}=_{,,}} _{i=2}^{K}(w_{i})(1-m_{i})\|D_{}(_{i}; )-z_{i}\|^{2},\] (3)

where \(()\) stops the gradient. By adaptively re-weighting the standard diffusion loss, \(_{}\) can boost the learning efficiency of dynamic regions, _e.g._, the moving vehicles and sidewalks in Fig. 4(d).

**Structure Preservation Loss.** The trade-off between perceptual quality and motion intensity has been widely acknowledged in video generation [3; 32; 73; 144], and our case is no exception. When

Figure 3: **[Left]: Vista pipeline. In addition to the initial frame, Vista can absorb more priors about future dynamics via latent replacement. Its prediction can be controlled by different actions and be extended to long horizons through autoregressive rollouts. [Right]: Training procedure.** Vista takes two training phases, where the second phase freezing the pretrained weights to learn action controls.

it comes to high-resolution prediction for dynamic driving scenarios, we discover that the predicted structural details degrade severely with over-smoothed or broken objects, _e.g._, the outlines of vehicles unravel quickly as they move (see Fig. 12). To alleviate this problem, it is important to place more emphasis on structural details. Based on the fact that structural details, such as edges and textures, mainly reside in high-frequency components, we identify them in the frequency domain as follows:

\[z_{i}^{}=(z_{i})=(z_{i}),\] (4)

where FFT and IFFT are the 2D discrete Fourier transform and inverse discrete Fourier transform respectively, and \(\) is an ideal 2D high-pass filter that truncates low-frequency components under a certain threshold. The Fourier transforms are applied on each channel of \(z_{i}\) independently. As illustrated in Fig. 4(e), features associated with structural information can be effectively emphasized by Eq. (4). The corresponding features from the predicted latent \(D_{}(_{i};)\) can also be extracted similarly. With the extracted high-frequency features, we devise a new structure preservation loss as:

\[_{}=_{,,} _{i=1}^{K}(1-m_{i})\|(D_{}(_{i};))- (z_{i})\|^{2}.\] (5)

This loss function minimizes the disparity of high-frequency features between prediction and ground truth, so that more structural information can be retained. Our final training objective is a weighted sum of Eq. (1), Eq. (3) and Eq. (5), where \(_{1}\) and \(_{2}\) are trade-off weights to balance the optimization:

\[_{}=_{}+_{1} _{}+_{2}_{}.\] (6)

### Phase Two: Learning Versatile Action Controllability

**Unified Conditioning of Versatile Actions.** To maximize usage flexibility, a driving world model should be able to leverage multiple action formats with different characteristics. For instance, one may use the world model to evalute high-level policies , or to execute low-level maneuvers . However, existing approaches only support limited action controls [54; 61; 90; 125; 127], inhibiting their flexibility and applicability. Therefore, we incorporate a versatile set of action modes for Vista: **(1) Angle and Speed** stand for the utmost fine-grained action controls. We normalize angles to \([-1,1]\) and represent speeds in \(km/h\). The signals from different timestamps are concatenated sequentially. **(2) Trajectory** is a series of 2D displacements in ego coordinates. It is widely used as the output of planning algorithms [12; 21; 58; 62; 63]. We represent the trajectory in meters and flatten it into a sequence. **(3) Command** is the most high-level intention. Without loss of generality, we define four commands, _i.e._ go forward, turn right, turn left, and stop, which are implemented as categorical indices. **(4) Goal Point** is a 2D coordinate projected from the short-term ego destination onto the initial frame, serving as an interactive interface . The coordinate is normalized by the image size.

Note that these actions are heterogeneous and cannot be used interchangeably. After transforming all these actions into numerical sequences, we encode them as a unified concatenation of Fourier embeddings [114; 116] (see Fig. 3). These embeddings can be jointly ingested by learning additional projections to expand the input dimension of the cross-attention layers in the UNet . The new

Figure 4: **Illustration on loss design.** Different from the standard diffusion loss **(b)** that is distributed uniformly, our dynamics enhancement loss **(d)** enables an adaptive concentration on critical regions **(c)** (_e.g._, moving vehicles and roadsides) for dynamics modeling. Moreover, by explicitly supervising high-frequency features **(e)**, the learning of structural details (_e.g._, edges and lanes) can be enhanced.

projections are initialized as zeros to enable gradual learning from the pretrained state. We empirically discover that incorporating action conditions through cross-attention layers yields faster convergence and stronger controllability compared to other approaches such as additive embeddings [128; 136].

**Efficient Learning.** We learn action controllability after the first training phase. Since the number of total iterations is crucial for diffusion training [5; 22; 32; 99], we separate action control learning into two stages. In the first stage, we train our model at a low resolution (320\(\)576), which achieves 3.5\(\) higher training throughput compared to the original resolution (576\(\)1024). This stage constitutes the majority of training iterations. Then, we finetune the model at the desired resolution (576\(\)1024) for a short duration, so that the learned controllability can effectively cater to high-resolution prediction.

However, tuning the UNet  at a lower resolution directly may undermine the high-fidelity prediction ability. Conversely, freezing all UNet weights and training the new projections alone would precipitate a quality decline (see Appendix D), suggesting the necessity to make the UNet adaptable. To solve this, we freeze the pretrained UNet and introduce parameter-efficient LoRA adapters  to each attention layer. After training, the low-rank matrices can be seamlessly integrated with the frozen weights, without introducing extra inference latency. Thus, the pretrained weights remain intact when training at the low resolution, avoiding deterioration of the pretrained high-fidelity prediction ability.

Since the parameters of the camera and vehicle are unavailable for open-world scenarios, it seems impossible to obtain multiple equivalent action conditions simultaneously at inference time. Additionally, it will entail prohibitively expensive training to encompass all possible combinations of action conditions. Hence, unlike common practices that activate all conditions during training, we enforce the independence of different action formats by enabling only one of them for each training sample. The remaining action conditions will be filled with zeros as unconditional inputs. As demonstrated in Appendix D, this simple constraint prevents the squandering of training cost on action combinations and maximizes the learning efficiency of each individual action mode within the same training steps.

**Collaborative Training.** Note that the aforementioned action conditions are not available in OpenDV-YouTube . On the other hand, nuScenes  has adequate annotations to derive these conditions. To maintain generalization and learn controllability in tandem, we introduce a collaborative training strategy by utilizing the samples from both datasets, with the action conditions for OpenDV-YouTube set to zero. The action control learning phase adopts the same loss as Eq. (6). By learning from two complementary datasets, Vista gains versatile controllability that are generalizable to novel datasets.

### Generalizable Reward Function

One application of world models is to evaluate actions by engaging a reward module [40; 42; 43; 76]. Drive-WM  establishes a reward using external detectors [82; 84]. However, these detectors are developed on a particular dataset , which may become a bottleneck for reward estimation in arbitrary scenarios. On the other hand, Vista has ingested millions of human driving logs, exhibiting strong generalization across scenes. Based on the observation that out-of-distribution conditions will lead to increased diversity in generation [28; 60], we utilize the prediction uncertainty from Vista itself as the source of our reward. Different from Drive-WM, our reward function seamlessly inherits the generalization of Vista without resorting to external models. Specifically, we estimate uncertainty via conditional variance. For reliable approximation, we denoise from randomly sampled noise with the same condition frame \(\) and action \(\) for \(M\) rounds. Our reward function \(R(,)\) is then defined as the exponential of averaged negative conditional variance:

\[^{}=_{m}D_{}^{(m)}(};,),\] (7) \[R(,)=-_{m}(D_{}^{(m)}(};,)-^{})^{2} ,\] (8)

where \(()\) averages all latent values within the video clip. Based on this formulation, unfavorable actions with larger uncertainties will lead to lower rewards. In contrast to commonly used evaluation protocols (_e.g._, the L2 error), our reward function can evaluate actions without referring to the ground truth actions. Note that we do not normalize the estimated rewards for the simplicity of definition, but it is straightforward to amplify the relative contrast by rescaling the estimated rewards with a factor.

## 4 Experiments

In this section, we first demonstrate Vista's strengths in generalization and fidelity in Sec. 4.1. We then show the impact of action controls in Sec. 4.2. We also substantiate the efficacy of the proposed reward function in Sec. 4.3. Finally, we conduct ablation studies on our key designs in Sec. 4.4. For more implementation details and experimental results, please refer to Appendix C and Appendix D.

### Comparisons of Generalization and Fidelity

**Automatic Evaluation.** Since none of the driving world models are publicly accessible, we compare these methods with their quantitative results on nuScenes. We filter 5369 valid samples from the validation set to conduct FID  and FVD  evaluation. For FID evaluation, we crop and resize the predicted frames to the resolution of 256\(\)448. For FVD evaluation, we use all 25 frames in each video clip and downsample them to 224\(\)224 following LVDM . Table 2 reports the results of all methods. In both metrics, Vista surpasses previous driving world models with a considerable margin.

**Human Evaluation.** To analyze the generalization of Vista across different datasets, we compare it against three prominent general-purpose video generators trained on web-scale data [5; 133; 144] (see Fig. 5). It is known that automatic metrics like FVD  cannot conclusively reveal perceptual

  
**Metric** &  DriveGAN \\  \\  &  DriveDreamer \\  \\  &  WoVoGen \\  \\  &  Drive-WM \\  \\  &  GenAD \\  \\  & 
 **Vista** \\ **(Ours)** \\  \\ 
**FID \(\)** & 73.4 & 52.6 & 27.6 & 15.8 & 15.4 & **6.9** \\
**FVD \(\)** & 502.3 & 452.0 & 417.7 & 122.7 & 184.0 & **89.4** \\   

Table 2: **Comparison of prediction fidelity on nuScenes validation set.** Vista achieves encouraging results that outperform the state-of-the-art driving world models with a significant performance gain.

Figure 5: **Driving futures predicted by different models using the same condition frame. We contrast Vista to publicly available video generation models using their default configurations. Whilst previous models produce misaligned and corrupted results, Vista does not suffer from these caveats.**

Figure 6: **[Top]: Long-horizon prediction.** Vista can forecast 15 seconds high-resolution futures without much degradation, encompassing long driving distances. The length of the blue lines indicate the duration of the longest prediction showcased by previous works. **[Bottom]: Long-term extension results of SVD.** SVD fails to generate consistent high-fidelity videos autoregressively as Vista does.

quality , let alone real-world dynamics. Therefore, we opt for human evaluation for more faithful analysis. Following recent advances , we adopt the Two-Alternative Forced Choice protocol. Specifically, participants are presented with a side-by-side video pair and asked to choose the video they deem better on two orthogonal aspects: visual quality and motion rationality. To avoid potential bias, we crop each video to a fixed aspect ratio, downsample them to the same resolution, and trim the excess frames when Vista generates longer videos than others. We only feed one condition frame to align with other models. To ensure the variety of scenes, we uniformly assemble 60 scenes from four representative datasets: OpenDV-YouTube-val , nuScenes , Waymo , and CODA . These datasets collectively exemplify the intricacy and diversity of real-world driving, _e.g._, OpenDV-YouTube-val includes geofenced districts, Waymo offers a unique domain compared to our training data, and CODA contains extremely challenging corner cases. We collect a total of 2640 answers from 33 participants. As presented in Fig. 7, Vista outperforms all baselines on both aspects, demonstrating its profound comprehension of the driving dynamics. Further, unlike other models that are only applicable for short-term generation, Vista can accommodate more dynamic priors and produce coherent long-horizon rollouts as shown in Fig. 6.

### Results of Action Controllability

**Quantitative Results.** To evaluate the impact of action controls, we divide the validation set of both nuScenes and the unseen Waymo dataset into four subsets according to our command categories. We then generate predictions using different modalities of the ground truth actions. The FVD score is measured on each subset and then averaged. A lower FVD score reflects a closer distribution to the ground truth videos, indicating that the predictions exhibit more resemblance to each specific type of behavior. Fig. 8 shows that our action controls can emulate the corresponding movements effectively.

We also introduce a new metric named _Trajectory Difference_ to assess control consistency. Following GenAD , we train an inverse dynamics model (IDM) that estimates the corresponding trajectory from a video clip. An illustration of IDM is shown in Fig. 13. We then send Vista's prediction to the IDM and calculate the L2 difference between the ground truth trajectory and the estimated trajectory.

The differences are measured over 2 seconds. The lower the trajectory difference, the stronger the control consistency Vista exhibits. We conduct the experiments on nuScenes and Waymo. For each dataset, we collect a subset that contains 537 samples. As reported in Table 3, Vista can be effectively controlled by different modalities of actions, resulting in more consistent motions to the ground truth.

**Qualitative Results.** Fig. 10 exhibits the versatile action controllability of our model. Vista can be effectively controlled by multi-modal actions, even in unseen scenarios beyond the training domain. In Appendix E, we also showcase the counterfactual reasoning ability of Vista using abnormal actions.

### Results of Reward Modeling

To validate the efficacy of our reward function, we jitter the ground truth trajectories into a series of inferior trajectories. Specifically, we compute the standard deviation of each waypoint from the nuScenes training set as prior distributions. These priors are jointly rescaled to sample perturbations with different L2 errors. The perturbations are then added as offsets to the ground truth trajectories. To ensure the plausibility of sampled trajectories, we adopt an explicit correlating strategy  to regularize offset sampling and recursively sample new trajectories until their offsets are consistent in tendencies. To demonstrate the generality of our reward function, we conduct reward estimation on Waymo , which is unseen in training. This is done by uniformly sampling from each command category on Waymo validation set, resulting in 1500 cases in total. We compare the average reward of the trajectories with varying L2 errors in Fig. 10. Our reward decreases when the deviation from the ground truth increases, underscoring the potential of our approach to serve as a viable reward function. It also holds the promise to remedy the irrationality in current evaluation protocols for planning , such as the L2 error shown in Fig. 10. More in-depth analysis of rewards, including sensitivity to hyperparameters and reward of other actions, are provided in Appendix D.

### Ablation Study

**Dynamic Priors.** Fig. 11 visualizes the outcomes of using different orders of dynamic priors. The order of priors corresponds to the number of condition frames. It shows that dynamic priors play a pivotal role in long-horizon rollouts, where the coherence with respect to historical frames is essential.

To further demonstrate the efficacy of dynamic priors, we conduct a quantitative evaluation in Table 3. Specifically, we use the IDM in Sec. 4.2 to infer the trajectories of the predicted videos with different orders of dynamic priors. The diminishing differences in trajectory suggest that introducing more priors can effectively improve the consistency between prediction and ground truth.

**Auxiliary Supervisions.** To verify the effectiveness of the two losses proposed in Sec. 3.1, we devise two additional variants by individually ablating each loss from a variant that incorporates both losses. We qualitatively compare their effects through Fig. 12, which confirms that the dynamics enhancement loss can promote the learning of real-world dynamics, and the structure preservation loss can reinforce the prediction of structural details.

## 5 Conclusion

In this paper, we introduce _Vista_, a generalizable driving world model with enhanced fidelity and controllability. Based on our systematic investigations, Vista is able to predict realistic and continuous futures at high spatiotemporal resolution. It also possesses versatile action controllability that is generalizable to unseen scenarios. Moreover, it can be formulated as a reward function to evaluate actions. We hope Vista will usher in broader interest in developing generalizable autonomy systems.

**Limitations and future work.** As an early endeavor, Vista still exhibits some limitations with respect to computation efficiency, quality maintenance, and training scale. Our future work will look into applying our method to scalable architectures [54; 97]. More discussions are included in Appendix A.