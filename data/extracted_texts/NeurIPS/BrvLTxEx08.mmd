# Proof.

Learning Equilibria in Adversarial Team Markov Games: A Nonconvex-Hidden-Concave Min-Max Optimization Problem

Fivos Kalogiannis

University of California, Irvine

Archimedes/Athena RC, Greece

Equal Contribution

Jingming Yan

University of California, Irvine

Equal Contributions

Ioannis Panageas

University of California, Irvine

Archimedes/Athena RC, Greece

###### Abstract

We study the problem of learning a Nash equilibrium (NE) in Markov games which is a cornerstone in multi-agent reinforcement learning (MARL). In particular, we focus on infinite-horizon adversarial team Markov games (ATMGs) in which agents that share a common reward function compete against a single opponent, _the adversary_. These games unify two-player zero-sum Markov games and Markov potential games, resulting in a setting that encompasses both collaboration and competition.  provided an efficient equilibrium computation algorithm for ATMGs which presumes knowledge of the reward and transition functions and has no sample complexity guarantees. We contribute a learning algorithm that utilizes MARL policy gradient methods with iteration and sample complexity that is polynomial in the approximation error \(\) and the natural parameters of the ATMG, resolving the main caveats of the solution by . It is worth noting that previously, the existence of learning algorithms for NE was known for Markov two-player zero-sum and potential games but not for ATMGs.

Seen through the lens of min-max optimization, computing a NE in these games consists a nonconvex-nonconcave saddle-point problem. Min-max optimization has received extensive study. Nevertheless, the case of nonconvex-nonconcave landscapes remains elusive: in full generality, finding saddle-points is computationally intractable . We circumvent the aforementioned intractability by developing techniques that exploit the hidden structure of the objective function via a nonconvex-concave reformulation. However, this introduces the challenge of a feasibility set with coupled constraints. We tackle these challenges by establishing novel techniques for optimizing weakly-smooth nonconvex functions, extending the framework of .

## 1 Introduction

Multi-agent reinforcement learning (MARL) investigates behaviors of multiple interacting agents within a dynamic, shared environment where the actions of each agent not only impact their individual rewards but also the overall state of the system. MARL has introduced several practical techniques that have justifiably captured public interest in recent years, particularly in skill-intensive games like starcraft, go, chess, and poker , where its empirical methods have achieved super-human performance. More recently, MARL methods combined with large language models has excelled in the game of Diplomacy . Despite these practical achievements, theoretical understanding of MARL has lagged behind its empirical successes.

Markov games (MGs)  is a rigorous and versatile mathematical structure that MARL employs to systematically formalize the strategic interactions in the dynamic settings . These games extend Markov decision processes (MDPs)  to multiple agents, each making decisions and receiving rewards independently as the environment evolves. The joint decisions of the agents influence both individual rewards and the transition of the environment. MARL in general is occupied with leading the multi-agent system to a favorable outcome. Through the lens of game theory, the notion of a "favorable outcome" is formally defined through concepts like a Nash equilibrium and a (coarse) correlated equilibrium. Although computing Nash equilibria is generally computationally intractable--even in two-player games without states ----it becomes tractable in fully cooperative settings like Markov potential games  and is also tractable in competitive scenarios such as two-player zero-sum Markov games . Recent advances  also show computational tractability in adversarial team Markov games (ATMGs)--a context that combines both cooperative and competitive dynamics among agents. More specifically, an infinite-horizon adversarial team Markov game (ATMG) is a Markov Game in which \(n\) team players, compete against one adversary. Each of the team players receives the same reward and is equal to minus the reward of the adversary. ATMGs generalize both Markov zero-sum and potential games; the former can be viewed as ATMGs with \(n=1\), the latter by choosing the adversary to be dummy (having one action).

Nash equilibrium computation in ATMGs naturally leads to a min-max optimization problem. Min-max optimization has been deeply explored across game theory, optimization, and machine learning. The past decade it has witnessed a proliferation of min-max optimization applications, notably in areas like generative adversarial networks (GANs) , robust machine learning , and adversarial training . In these applications, the optimization objectives often involve nonconvex-nonconcave functions which pose substantial challenges. Typically, the aim is to approximate saddle-points of \(f(,)\). In normal form games, these points correspond to Nash equilibria. This correspondence also holds true for MGs due to the gradient domination property . Although we cannot aspire to cover the vast quantity of works in MARL and optimization, we select some representative works that we defer to Appendix A due to space constraints.

This paper aims to develop learning methods to approximate Nash equilibria in team Markov games by using only individual rewards and state observations as feedback, addressing the following question and answering one of the main caveats of the solutions provided in :

_Is it possible for agents to efficiently learn Nash equilibria in adversarial team Markov games, having only access to trajectory roll-out samples and (almost*) no communication, i.e., independently?_

Footnote *: We say “almost” as the agents need to take turns in updating their policies instead of making updates simultaneously. Nevertheless, the learning dynamics remain uncoupled.

### Our Contributions

Let us provide some context before stating our main results. An infinite-horizon adversarial team Markov game (ATMG) is characterized by a finite state-space \(\), \(n\) team players, each equipped with a finite action-space \(_{i},\;i\{1,,n\}\), and one adversary with a finite action-space \(\). Each of the team players receives the same reward which is equal to minus the reward of the adversary. The adversary's value function is defined as the discounted expected sum of their rewards, where the discount factor is \([0,1)\). An approximate Nash equilibrium is a product distribution over policy space such that no agent can improve their value by unilaterally deviating. We propose a learning algorithm that has both iteration and sample complexity polynomial in the parameters of the Markov Game and returns approximate Nash equilibria.

**Theorem 1.1** (Informal Version of Theorem 3.3).: _There is a learning algorithm (ISPNG) that uses bandit feedback and guarantees convergence to an \(\)-approximate Nash equilibrium in adversarialteam Markov games, the sample and iteration complexities of which are_

\[(,||,_{k=1}^{n}|_{i} |+||,).\]

We deem noteworthy that our algorithm manages to compute a Nash equilibrium in a Markov game, which combines opposing and shared agent interests, by only using a number of iterations and samples that is polynomial in the approximation error and the description of the game. Further, it manages to beat the _curse of multi-agents_--_i.e.,_ its iteration and sample complexity depends on \(_{k=1}^{n}|_{i}|\) instead of \(_{k=1}^{n}|_{i}|\).

In order to achieve the latter contribution, we acquired convergence guarantees for stochastic projected gradient descent in nonconvex functions when the gradient is Holder-continuous--a notion of continuity weaker than that of Lipschitz. Finally, we contribute a general result that guarantees convergence to a saddle-point in functions that are nonconvex-hidden-strongly-concave.

### Technical Overview

The problem of computing an approximate Nash equilibrium in an adversarial team Markov game boils down to computing an approximate saddle-point \((^{*},^{*})\) of the adversary's value function \(V(,)\); see Definition 2.3. The variables \(\) denote the policies of the team, each member of which aims to individually minimize \(V\). Moreover, \(\) denotes the policy of the adversary who aims to maximize \(V\). The equivalence between saddle-points and equilibria is due to (i) the game being _zero-sum_ between the team and the adversary and (ii) the _gradient domination property_ (see Lemma C.7) that holds per player, and has already been established in prior works . In words, gradient domination in our setting implies that any approximate first-order stationary policy is also an approximate best response for that player.

The problem of computing an approximate saddle-point \((^{*},^{*})\) of the objective \(V(,)\) poses computational challenges due to its nonconvex-nonconcave nature. Previous work  showed that one can compute an approximate saddle-point \((^{*},^{*})\) of \(V\), by first obtaining an approximate stationary point \(^{*}\) of \(()=_{}V(,)\) through a Moreau envelope argument and then extending it to \((^{*},^{*})\). The proof of extendibility uses involved arguments that utilize the Lagrange multipliers of a carefully chosen nonlinear program (for the stationary point \(^{*}\)), while the computation of \(^{*}\) requires solving another linear program. It is worth noting that the aforementioned linear program presumes access to the full description of the reward function and the transition model of the underlying Markov game when the team plays policy \(^{*}\). This fact prevents the possibility of casting this approach into a learning algorithm.

Our proposed (learning) algorithm bypasses the requirement for knowledge of the reward function and the transition model, and works under the bandit feedback framework. The first idea behind our algorithm is to consider the adversary's value function as a function \(F\) of the _adversary's_ state-action visitation measure \(\), \(F(,) V(,)\), and the addition of a regularizing term \(-\|\|^{2}\) (\(\) can be thought of as a small positive scalar). As a result, the max function of the regularized value function, \(^{}()_{()}\{F (,)-\|\|^{2}\}\), is differentiable, where \(()^{||||}\) denotes the feasibility set of \(\) and depends on \(\). Effectively, different policies, \(\), for the team induce a different single agent Markov decision process for the adversary. The addition of the regularizer allows us to apply Danskin's theorem on a function with a unique maximizer circumventing the necessity of solving a linear program; one only needs to approach that unique solution. To the best of our knowledge, this is the first work introducing a function of \(\) as a regularizing term.

By reformulating the regularized value function using state-action visitation measure \(\), the problem boils down to learning an approximate saddle-point of a nonconvex-strongly-concave function with _coupled constraints_. Coupled constraints are a type of constraints that cannot be expressed as a Cartesian product (the main well-studied setting in min-max optimization ), _i.e._, the feasibility set \(()\), depends on \(\). The first challenge towards handling the coupled constraints is to argue that \(^{}\) is Holder-continuous which is a notion of continuity weaker than Lipschitz continuity (see Definition 2.1). Specifically, in Theorem 3.2, we show that \(^{}()\) is weakly-smooth, or equivalently, \(^{}\) is Holder-continuous. It seems unlikely that we could use Moreau envelope techniques to prove convergence of stochastic projected gradient descent on a weakly-smooth function. The next step of our proof is to transfer the weakly-smooth nonconvex optimization problem into a smooth optimization problem with inexact gradient oracles, extending the techniques from  to nonconvex and constrained settings. Since we only allow each player to observe the reward they received and not the action chosen by the other players (including the adversary), one last challenge we have to deal with is the inability to estimate the state-action visitation measure \(\) of the adversary, making the gradient inexact when computing \(^{}(x)\) in both deterministic and stochastic settings.

## 2 Preliminaries

Starting, we will introduce the notation conventions we use and split the rest of the preliminaries into two subsections. Section 2.1 provides necessary definitions whereas Section 2.2 deals with the preliminaries of (adversarial team) Markov games and the notion of Nash equilibrium.

Notation.We denote \([n]\{1,,n\}\). We use superscripts to denote the (discrete) time index, and subscripts to index the players. We use boldface for vectors and matrices; scalars will be denoted by lightface variables. We define \(\|\|_{2}\), \(\|\|_{1},\|\|_{}\) to be the \(_{2}\)-norm, the \(_{1}\)-norm and the \(_{}\) norm respectively. The simplex of probability vectors supported on a finite set \(\) is noted as \(()\). Unless specified otherwise, we denote \(\|\|_{2}\) by \(\|\|\). \(_{}\) denotes the diameter of a compact set \(\) in \(_{2}\)-distance. For simplicity in the exposition, we may sometimes use the \(O()\) notation to suppress dependencies that are polynomial in the natural parameters of the problem and \(()\) to further hide logarithmic factors; precise statements are given in the Appendix. For the convenience of the reader, a comprehensive overview of our notation is given in Table 1.

### Basic Definitions and Facts

We commence this subsection by introducing a number of concepts and statements of mathematical analysis and optimization. We define Holder continuity and the notion of a stationary point in constrained minimization and min-max optimization.

The notion of Holder continuity of the gradient is a weaker notion of Lipschitz gradient continuity.

**Definition 2.1** (\(p\)-Holder continuous gradient).: _A function \(:^{d}\) is said to have a \((_{p},p)\)-Holder continuous gradient if for every \(,^{}^{d}\), it holds that:_

\[\|()-(^{})\|_{2}_{p}\|- {z}^{}\|_{2}^{p}.\]

_When \(p=1\), we retrieve the definition of an \(\)-smooth function._

Throughout, following standard conventions, we will refer to functions for which the gradient is \(p\)-Holder continuous with a \(p<1\) as _weakly-smooth_. We state the notions of first-order stationarity relevant to our work.

**Definition 2.2** (\(\)-Fosp).: _In the context of the constrained minimization problem \(_{}()\), a point \(\) is said to be an \(\)-approximate stationary point if,_

\[-_{}(),^{}-, ^{}.\]

Similarly, we will define an \(\)-approximate saddle-point for the constrained min-max optimization problem \(_{}_{}f(,)\).

**Definition 2.3** (\(\)-Sp).: _Let a function \(f:\). A point \((,)\) is said to be an \(\)-approximate saddle-point (or \(\)-FOSP for the min-max problem) if,_

\[-_{}f(,)^{}(^{}-) ,\,^{};\] \[_{}f(,)^{}(^{}-) ,\,^{}.\]

### Adversarial Team Markov Games

An adversarial team Markov game is the Markov game extension of normal-form adversarial team games . The game takes place in an infinite-horizon discounted setting where a team of identically-interested players compete against one adversarial player, the _adversary_. We can formally define an adversarial team Markov game as a tuple \((,[n+1],,,r,,,)\), where:* \(\) is the finite set of states, or _state-space_, with cardinality \(S||\);
* \([n+1]\) is the set of players, with the first \(n\) players belonging to the team and the last one being the adversary;
* \(=_{i=1}^{n}_{i}\) is the finite set of the team's joint actions (or, team's _action-space_), while \(_{i}\) is the \(i\)-th player's _action-space_; respectively \(\) is the adversary's action-space; further, \(A_{i[n]}|_{i}|\) and \(B||\);
* \(r:\) is the adversary's reward function;
* \(:( )\) is transition probability function;
* \([0,1)\) is the discount factor;
* \(()\) is the initial state distribution. We assume that \(\) is of full-support, \((s)>0, s\).

Every team player \(i[n]\) gets the same reward and the sum of team players' rewards are equal to the adversary's loss, _i.e._, \(_{i=1}^{n}r_{i}(s,,b)=-r(s,,b)\).

#### 2.2.1 Policies, Value Function, and Visitation Measures

In this part, we describe policy classes, the value function, and the state-action visitation measures. All of these notions are indispensable for our analysis.

Policy Definitions.For any agent \(i\), a _stationary_ policy \(_{i}\) is defined as a mapping from any given state to a probability distribution over possible actions, where \(_{i}: s_{i}(|s)( _{i})\). A policy \(_{i}\) is described as _deterministic_ when, for any state, it selects a particular action with probability of \(1\). To simplify, we denote the policy spaces for the team and the adversary as \(_{}:()\) and \(_{}:()\), respectively. Additionally, the combined policy space for all participants can be represented as \(:()()\).

Direct Policy Parametrization.In the context of our work, we assume the strategy of _direct policy representation_ for all players. Specifically, for each player \(i\) within the set \([n]\), the policy space \(_{i}\) is defined as \((_{i})^{S}\), with \(_{i}=_{i}\), such that the probability of choosing action \(a\) in state \(s\), \(x_{i,s,a}\), equals \(_{i}(a|s)\). By the usual game-theoretic convention, \(_{-i}\) denotes the policy of all agents apart from \(i\). For the adversary, \(\) is set as \(()^{S}\), with \(_{}=\), so that \(y_{s,a}=_{}(a|s)\).

Having defined policies, we can introduce some standard shortcut notations such as \(r(s,,)_{(\,b)(,)}[r(s, ,b)]\), and the vectors \(()^{||||},(,)^{||}\) with \(()[_{}[r(s,,b)] ]_{s,b}\) and \((,)[_{(,b)}[r(s, ,b)]]_{s}\). Further, we define \((s^{}|s,,)\) as \((s^{}|s,,)_{(,b)(,)}[(s^{}|s,,b)]\) and the vector \((s,,b)()\) with \((s,,)[_{(,b)(, )}[(s^{}|s,,b)]]_{s^{}}\).

The Value Function.The _value function_\(V_{s}\), for a given state \(s\), is defined as the adversary's expected total discounted reward over time under a combined policy \((_{},_{})\) from the policy space \(\), with \(=_{}\) being the aggregation of policies \((_{1},,_{n})\). Formally, this is represented as

\[V_{s}(,)_{,}[._{h=0} ^{}^{h}r(s_{h},_{h},b_{h})|s_{0}=s],\]

where the expected value is calculated over the distribution of trajectories generated by the policies \(\) and \(\). If the initial state is instead sampled from a distribution \(\), the value function is expressed as \(V_{}(,)=_{s}[V_{s}(, )]\).

Visitation Measures.The important quantity of state-action visitation measures, or the expected discounted sum of visitations of a state-action pair.

**Definition 2.4** (State-Action Visit. Measure).: _For any initial distribution \(()\), transition matrix \(\), a team policy \(\), and a policy \(\), we define the station-action visitation measure of the adversary \((;)\) as follows:_

\[_{s,b}(;)_{h=0}^{}^{h}\, (s_{h}=s,b_{h}=b|,,s_{0}).\]_Where \(_{s,b}(;)\) denotes the \((s,b)^{th}\) entry of \((;)\)._

As we will further discuss in the appendix (Appendix C.1), the correspondence between \(\) and \(\) is "1-1" for a fixed team policy \(\). This property is crucial for our contributions.

Reformulation of the Value Function.A key property of the value function \(V_{}\) is that it can be rewritten as a concave function of the state-action visitation measure:

\[V_{}(,)=()^{}(;).\]

**Definition 2.5** (\(\)-Ne).: _A product policy \((^{*},^{*})\) is called an \(\)-approximate Nash equilibrium for an \( 0\), when_

\[V_{}(^{*},^{*})  V_{}((^{}_{i},^{*}_{-i}),^ {*})+,\,^{}_{i}_{i},\, i[n];\] _and_ \[V_{}(^{*},^{*})  V_{}(^{*},^{})-, ^{}.\]

#### 2.2.2 The Gradient and Visitation Measure Estimators.

An essential element that led to the development of policy gradient methods is the policy gradient theorem . Notably, it has enabled the design of finite-sample gradient estimators. This technique fits well into the _MARL independent learning protocol_. After all agents have proposed their policy, the MDP is run to acquire batches of trajectories from which all agents will observe the chain's state and their individual reward. These samples are utilized to estimate gradients.

The team agents implement a batch version of the REINFORCE estimator whose definition is deferred to the Appendix C.6.1. As for the estimators that the adversary utilizes, we define the state-action visitation measure estimator and their gradient estimator closely following .

**Definition 2.6** (State-Action Visitation Measure Estimator).: _Let \(_{s,b}\) be the standard basis for the \((s,b)^{th}\) entry. Let \(=(s_{0},b_{0},s_{1},b_{1},,s_{H-1},b_{H-1})\) denote a trajectory with length \(H\) sampled under initial distribution \(\) and policy \(\) We define the estimator for \((;)\) with the trajectory \(\) as the following_

\[}(|):=_{h=0}^{H-1}^{h}_{s_{h },b_{h}}.\]

By applying policy gradient theorem  along with the chain-rule, the gradient estimator for a value-function that is nonlinear in \((;)\), is computed by the following estimator .

**Definition 2.7** (Gradient Estimator).: _Let \(=(s_{0},b_{0},s_{1},b_{1},,s_{H-1},b_{H-1})\) denote a trajectory with length \(H\) sampled under initial distribution \(\) and policy \(\). Let \(F(())\) be the value function of the MDP w.r.t. \(()\) and \(:=_{}F(())\). The estimator for gradient \(_{}F(())\) using the sampled trajectory \(\) is defined as_

\[}(|;):=_{h=0}^{H-1}^{h}(s_{h },b_{h})(_{h^{}=0}^{h}_{}(b_{h^{ }}|s_{h^{}})).\]

Sufficient Exploration.A standard, while rather naive, technique of bounding the variance of the REINFORCE gradient estimator is using \(\)-greedy policy parametrization. Effectively, every action in a player's dispose is played with a probability of at least \(\). For our convenience, we ensure sufficient exploration by a \(\)-_truncated simplex_ approach. Moreover, for a given feasibility set \(\), we denote \(^{}\) to be the \(\)-truncated feasibility set.

## 3 Main Results

We present our main results in two different subsections. In Section 3.1 we manage to attain guarantees for convergence to an approximate stationary-point to constrained nonconvex optimization with an stochastic inexact gradient oracle-- we do so by extending previous results of . While in Section 3.2, we apply the latter results along with RL techniques in order to design the first learning algorithm that computes a Nash equilibrium in ATMGs.

### Stochastic Weakly-Smooth Nonconvex Optimization with Inexact Gradients

In this subsection we prove that projected gradient descent with a stohcastic inexact gradient oracle converges to an \(\)-FOSP in nonconvex functions with Holder continuous gradients. We will use this key result in subsequent sections. We begin by defining the inexact gradient oracle and its stochastic version.

**Definition 3.1** (Inexact Gradient Oracle).: _Let a differentiable function \(()\) and its gradient \(()\). We call the vector-valued function \(()\) a \(\)-inexact gradient oracle if,_

\[()-(), .\]

Further, given a random variable \(\) in some sample space \(\), we define a stochastic inexact gradient oracle \(G:^{d}\). We assume that the expected value of this oracle will be equal to a \(\)-inexact gradient oracle \(()\). Additionally to being unbiased (with respect to a \(\)-inexact gradient oracle), we assume its variance to be bounded.

**Assumption 3.1** (Unbiased and Bounded Variance).: For a variance parameter \(^{2}>0\), the gradient oracle \(G\), satisfies

\[_{}[G(,)]=()_ {}[ G(,)-()^{2}] ^{2}.\]

Following, we consider the simple update rule of _Mini-Batch Inexact Stochastic Projected Gradient Descent_, with a batch size \(M>0\) and \(}^{t}=_{j=1}^{M}G(^{t},_{j}^{t})\),

\[^{t+1}=_{}(^{t}- }^{t}).\] (Inexact Stoch-PGD)

We can now state our convergence Theorem for (Inexact Stoch-PGD) whose proof we defer to the appendix.

**Theorem 3.1** (Convergence to \(\)-FOSP; Formally in Theorem B.1).: _Let \(:\) be a Lipschitz continuous function with \((_{p},p)\)-Holder continuous gradient and a desired accuracy \(\). Also, let a stochastic inexact first-order oracle \(G\) satisfying Assumption 3.1. The update rule (Inexact Stoch-PGD), with a step-size \(=(^{})\), computes an \(\)-approximate stationary point after \(T=O(^{-})\) iterations._

### Learning Nash Equilibria in Adversarial Team Markov Games

In this subsection we state our contributed Algorithm 1, or ISPNG, which converges to an \(\)-NE for any ATMG, \(\), with an iteration and sample complexity that scales polynomially with \(1/\) and the parameters of \(\). To simply describe the algorithm, the team players initialize their policies and then the following two steps are repeated for \(T\) iterations:

1. the adversary approximately maximizes a _regularized version_ of their value function, \(V_{}^{}(,):=()^{}(; )-(;)^{2}\), using Algorithm 2, and then
2. every agent independently performs a gradient descent step on the value function.

During this process, all agents use only bandit feedback information in order to estimate the gradients of the value function. We remark that the learning dynamics remain uncoupled. The only instance of communication between agents is the fact that the team and the adversary take turns when updating their policies. During their turn, the adversary approximately best-responds.

Of particular interest is the sub-routine of Algorithm 2, Vis-Reg-PG. It is effectively a directly parameterized policy gradient method for an objective function that is concave in the state-action visitation measure \((;)^{||||}\). The objective function is merely the original value function plus a quadratic term, \(-\|(;)\|^{2}\). We remind the reader that due to the existence of this introduced regularizer, the utility of the adversary \(=_{(;)}F_{}^{}(,)= ()-(;)\). In order to estimate a gradient, the adversary needs to collect a number of trajectories, \(=(s_{0},b_{0},s_{1},,s_{H-1},b_{H-1},s_{H})\), each of length \(H\). Notably, the adversary only uses the empirical state-action visitation measure for the purpose of gradient estimation of the regularized function.

```
0: An MDP, a joint strategy of the team \(\), and a desired accuracy \(>0\).
1: Based on \(\), set batch size \(K\), sample traj. length \(H\), stepsize \(_{y}\), truncation parameter \(_{y}\) and regularization coeff. \(\). \(\) see Theorem C.3
2:\(y^{(0)}(s,b)|}\), \((s,b)\).
3:for Epoch \(t 0,1,,T_{y}\)do
4: Independently sample \(K\) trajectories, \(^{(t)}\), of length \(H\) under policy \(^{(t)}\).
5:\(}^{(t)}_{ ^{(t)}}}(|^{(t)})\),
6:\(()-}^{(t)}\).
7:\(}_{}^{(t)}_{ ^{(t)}}}(|^{(t)};)\). \(}\) as in Definition 2.7.
8:\(^{(t+1)}_{y^{_{y}}}(^{(t)}+_{y} }_{}^{(t)})\).
9:endfor
```

**Algorithm 2** Visitation-Regularized Policy Gradient Algorithm (Vis-Reg-PG)

### Analyzing Independent Stochastic Policy-Nested-Gradient

Algorithm 1, or ISPNG, is an instance of a nested-loop algorithm. As we have already informally stated, ISPNG runs gradient descent on the regularized max function \(^{}()=_{()}\{()^ {}-\|\|^{2}\}\) for some parameter \(\). This function has Holder-continuous gradient and, as such, the convergence proof is underpinned by Theorem 3.1. Formally we state that:

**Theorem 3.2** (Grad. Contunuity of Reg-Max Function).: _Let function \(^{}()\) be the maximum function of the regularized value function of an ATMG, with regularization coefficient \(>0\). It is the case that, (i) \(^{}\) is differentiable, (ii) \(_{}^{}\) is \((1/2,_{1/2})\)-Holder continuous, i.e,_

\[\|_{}^{}()-_{}^{}(})\|_{1/2}\|-^{}\|^{ {2}}\]

_with \(_{1/2}}||^{(_ {i}|_{i}|+||)^{2}}}{_{}(s)(1- )^{}}\)._

ISPNG manages to run gradient descent on function \(^{}\) though the agents can never observe the exact gradient of \(^{}\). This is not only due to the randomness of gradient estimators but mainly because they cannot observe the adversary's actions and thus do not know the gradient w.r.t the regularizing term. Fortunately, the regularization coefficient plays a second role in bounding the inexactness error of the gradient estimates. For that reason, parameter \(\) admits a careful tuning.

Finally, the differentiability of \(^{}\) and the per-player gradient domination property of the \(V_{}\) implies that an \(\)-FOSP \(^{*}\) and the corresponding best-response for the regularized value function, \(^{*}\), constitute an \(\)-NE, leading to the main Theorem of this subsection:

**Theorem 3.3** (Main Result; Formally in Theorem C.3).: _Given a desired accuracy \(>0\), Algorithm 1 outputs a joint policy \((^{*},^{*})\) for which it holds that,_

\[[V_{}(^{*},^{*})-_{^ {*}_{i}_{i}}V_{}(^{*}_{i},^{*}_{-i},^ {*})], i[n];\] _and_ \[[_{^{}}V_{}(^{*},^{})-V_{}(^{*},^{*})],\]

_with a number of iterations and a number of samples that are \((,n,||,_{i}|_{ i}|+||,D_{},,(s)})\). By \(D_{}\) we denote the mismatch coefficient \(D_{}:=\|^{*}_{}}{}\|_{}\) (Definition C.2)._

## 4 Minimax in Nonconvex-Hidden-Strongly-Concave Functions

Finally, we would state a more general result compared to that of Theorem 3.3. We consider the general min-max nonconvex-nonconcave optimization problem, \(_{}_{}f(,)\), when an additional structural assumption holds, _i.e.,_ when \(f\) is nonconvex-hidden-strongly-concave. In particular, function \(f\) admits a reformulation of the form,

\[H(,) f(,c^{-1}(;)),\]

where function \(H\) is a nonconvex-strongly-concave function defined on \(\). The sets \(\) and \(\) are closed and convex, while \(c(;):\) is an invertible mapping parametrized by \(\). Moreover, we will denote \(()\{|=c(;),\  \}\). We further assume that the mapping \(c\) and its inverse are Lipschitz-continuous. Specifically,

**Assumption 4.1**.: For the mapping \(c\) and its inverse, \(c^{-1}\), it holds that

\[\|c(;)-c(^{};^{})\|  L_{c}(\|-^{}\|+\|-^{}\|),,^{};, ^{}\] \[\|c^{-1}(;)-c^{-1}(^{};^{ })\|  L_{c^{-1}}(\|-^{}\|+\|- ^{}\|),\,,^{};, ^{}.\]

If this is the case, the maximizer \(^{*}()*{argmax}_{()}H(,)\), is Holder continuous w.r.t. \(\) as stated by the following Theorem.

**Theorem 4.1** (Formally in Theorem D.2).: _Let function \(f(,)\) be nonconvex-hidden-strongly-concave with a modulus of \(>0\). Let also function \(H\) be a \(L_{H}\)-Lipschitz continuous and \(_{H}\)-smooth nonconvex-strongly-concave reformulation of \(f\) with an invertible mapping \(c\) for which Assumption 4.1 holds. Then,_

\[\|^{*}()-^{*}(^{})\| L_{*}\| -^{}\|^{},,^{ }.\]

_where, \(L_{*}=O()\)._

**Theorem 4.2** (Convergence to an \(\)-SP; Formally in Theorem D.3).: _Let \(f\) be a nonconvex-hidden-strongly-concave function obeying to the same assumptions as \(f\) in Theorem 4.1 and \(>0\). Further assume a maximization oracle with \(O(^{2})\)-accuracy. There exists an algorithm that computes an \(\)-approximate saddle-point \((^{*},^{*})\) by making \(T=O(^{3}})\) calls to the maximization oracle. Also, the maximization oracle can be implemented by stochastic gradient ascent with iteration complexity \(T^{}=(^{2}})\), and stepsize \(_{y}=O(^{2}^{2})\)._

## 5 Conclusion, Future Work, and Limitations

ConclusionsWe expanded stochastic gradient techniques to be able to compute a stationary point in constrained optimization of nonconvex with weakly-smooth functions. We applied that result to design the first learning algorithm that computes an \(\)-approximate Nash equilibrium in adversarial team Markov games using a finite number of samples and iterations that scale polynomially with \(1/\) and the natural parameters of the game.

Future WorkWe believe that some questions that require further investigation are the following: (i) Is it possible to extend the techniques of  to establish convergence guarantees of stochastic gradient descent on nonconvex functions with Holder-continuous gradient without batch-sampling of the gradient? (ii) Can we design a two-timescale gradient descent-ascent scheme for ATMGs that converges to a Nash equilibrium with best-iterate guarantees? (iii) Can we utilize some variance-reduction techniques to achieve a better sample complexity for learning an \(\)-NE in ATMGs?

LimitationsThe main limitations of our work are (i) the notion of independent learning as presented is weaker than the one presented in  - _i.e._, our algorithm has an "inner loop", (ii) the fact that we did not present an example for which the function \(^{}\) fails to be smooth; hence, it is unclear if we can prove the smoothness of this function and achieve tighter analysis. The first item can be addressed in future work by developing a two-timescale algorithmic approach. As for the second item, we remark that even if it is the case that \(^{}\) is smooth for ATMGs, our provided convergence rates would be straightforwardly improved without any qualitative modification of the algorithm. Also, we would like to highlight that this discussion is related to Remark 2.