# Wagle: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models

Jinghan Jia\({}^{}\) Jiancheng Liu\({}^{}\) Yihua Zhang\({}^{}\)

**Parikshit Ram\({}^{}\) Nathalie Baracaldo\({}^{}\) Sijia Liu\({}^{,}\)\({}^{}\)Dept. CSE, Michigan State University \({}^{}\)IBM Research**

###### Abstract

The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. LLM unlearning is designed to reduce the impact of undesirable data influences and associated model capabilities without diminishing the original utility of the model. Despite growing interest, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency. However, the inherent relationship between model weights and LLM unlearning has not been extensively examined. In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and propose the weight attribution-guided LLM unlearning framework, Wagle, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLMs. By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, Wagle can erase the undesired content, while maintaining the performance of the original tasks. Our experiments show that Wagle boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, and applications such as fictitious unlearning (TOFU benchmark) and malicious use prevention (WMDP benchmark), under models including Zephyr-7b-beta and Llama2-7b. To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning. It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques. Codes are available at https://github.com/OPTML-Group/WAGLE.

## 1 Introduction

Large language models (LLMs) have demonstrated exceptional proficiency in generating text that closely resembles human-authored content. However, their capacity to memorize extensive corpora can raise ethical and security concerns, such as the generation of biased, private, harmful, or even illegal contents . These issues highlight the necessity of effectively and efficiently tailoring pre-trained LLMs to _remove_ these undesired data influences and associated generation capabilities, ensuring they are suitable for diverse application contexts. Therefore, the problem of machine unlearning (MU) for LLMs (referred to as _LLM unlearning_) arises , aiming to equip trained LLMs with data- and model-erasing capabilities.

The concept of MU has gained increasing popularity due to its significance in assessing and manipulating the impact of data on model performance. Its importance originated from the need to protect data privacy , in response to data protection regulations like the 'right to be forgotten' . The majority of past research efforts have focused on solving the problem of MU for _classification_ models. Compared to LLM unlearning, the unlearning scope in classification problems is typically easier to define, often focusing on specific data points or classes to forget. Moreover, it is even feasible to retrain the classification models from scratch after removing the data/classes targeted for unlearning . The feasibility of _retraining from scratch_ leads to the _exact unlearning_ method, which is typically used as a gold standard in MU evaluation for classification models. However, such an exact unlearning method becomes infeasible for LLMs due to their prolonged training times and associated high costs. Instead, evaluations are often based on the specific unlearning tasks.

Therefore, LLM unlearning, despite falling under the broad category of MU, presents a much more challenging problem. The two main difficulties lie in developing effective and efficient unlearning algorithms and in assessing the performance of LLM unlearning.

Representative unlearning algorithms include gradient ascent (GA)  to deviate the LLM prediction away from responses to the forget data and its utility-regularized variants, such as GradDiff  which utilizes the gradient difference between the forget loss and the retain loss to strike a tradeoff between unlearning efficacy and utility retention. Drawing inspiration from direction preference optimization , the LLM unlearning problem has also been addressed using algorithms such as negative preference optimization (NPO)  and preference optimization (PO) . NPO treats the forget data as negative examples in LLM preference alignment, while PO assigns pre-defined positive responses (such as rejection-based answers) to the forget data during preference alignment.

In addition, further studies explored the choice of optimizers suited for solving LLM unlearning problems  and proposed prompting-based algorithms to achieve unlearning for black-box LLMs .

A few recent benchmarked unlearning tasks and datasets have also been developed to facilitate performance evaluation. Examples include the TOFU dataset for fictitious unlearning , the WMDP dataset for malicious use prevention of LLMs , the copyrighted information removal , and the LLM detoxification task . All these evaluations will be considered in this work.

Despite the rapid progress in LLM unlearning algorithms and evaluation methods, less effort has been made to explore the modularity characteristics of LLMs for unlearning and the influence of these modules. In the literature, weight sparsity achieved through model pruning has been found beneficial in reducing the gap between a GA-based approximate unlearning method and exact unlearning . However, this advantage was limited to MU for classification models. As we will demonstrate, the benefit of pruning does not directly apply to LLM unlearning, as it excludes the forgetting influence on weight selection. Another relevant line of work is weight localization for LLM editing . However, Hase et al.  demonstrated that the popular causal tracing-based weight localization technique  cannot precisely predict which layers within an LLM are most influential for knowledge editing or removal. Other studies have also examined the saliency of LLM modules for unlearning, focusing on weights' gradients  and neurons within the feed-forward network .

Although there is emerging interest in exploring the relationship between LLM unlearning and its model fingerprints, such as layers and neurons, no principled approach exists to precisely attribute weight-level influence in LLM unlearning and facilitate the unlearning process. This gap gives rise to the central problem of this work: Weight attribution for LLM unlearning. Specifically, we ask:

_(Q) How to identify influential weights to enhance unlearning efficacy while preserving LLM utility?_

To tackle (Q), we interpret the problem of weight attribution from a bi-level optimization (BLO) perspective. This approach allows us to attribute the weights' influence in LLM unlearning by considering both the unlearning objective (modeled in the upper-level problem of BLO) and the model utility retention objective (modeled in the lower-level problem of BLO). It also enables us to derive the closed-form attribution scores for identifying influential weights using the implicit gradient approach in BLO. Further, we develop the weight attribution-guided LLM unlearning framework (WAGLE), easily compatible with existing LLM unlearning algorithms. We summarize **our contributions** below.

\(\) We propose the problem of weight attribution for LLM unlearning and highlight its distinct challenges compared to conventional approaches using weight pruning.

\(\) We solve weight attribution through the lens of BLO and derive its closed-form solution.

\(\) We develop WAGLE to be agnostic to specific unlearning algorithms and demonstrate its effectiveness across diverse unlearning benchmarks and evaluation metrics.

## 2 Related Work

Machine unlearning (MU) for non-LLMs.The concept of MU was originally raised to address users' deletion requests for given machine learning (ML) models, without the need to retrain these models from scratch [3; 4; 5]. The capability to _assess and erase the influences of data_ to be forgotten in model performance has broadened the MU concept across diverse ML paradigms, such as image classification [11; 12; 33; 34], image generation [13; 35; 36; 37], generative language modeling [38; 39; 40; 2], graph neural networks [41; 42; 43], and federated learning [44; 45; 46]. The methodologies of MU include retraining-based exact unlearning [8; 47], differential privacy (DP)-based unlearning [7; 9; 10; 48], and fine-tuning-based approximate unlearning [8; 11; 49; 50; 12; 48; 51].

LLM unlearning.When MU shifts to the realm of LLMs, new challenges and complexities arise. The two main difficulties in effective and efficient algorithmic design and unlearning evaluation have been highlighted in Sec. 1. Another related challenge is how to precisely define the scope of LLM unlearning . Existing work has raised concerns that the current unlearning scope is _insufficient_ for declaring the robustness and reliability of LLM unlearning. This is evidenced by the extractable unlearned knowledge from LLMs post-unlearning when facing in-context relearning  and jailbreaking attacks . Yet, even in the absence of these knowledge extraction 'adversaries', enhancing the efficacy of LLM unlearning remains a highly non-trivial problem. Existing LLM unlearning methods are predominantly fine-tuning-based approaches [15; 16; 19; 20; 26], which are favored for their computational efficiency. Application-wise, the promise of LLM unlearning has been demonstrated in diverse use cases, such as protecting copyrighted or personal identification information [26; 32; 54], preventing the use of LLMs in developing cyberattacks or biweapons [25; 55], and mitigating the generation of toxic, biased, or hallucinated content [15; 27; 31].

Data and weight attribution.A key mission of MU is to quantify the influence of forgotten data on model performance, which aligns with the classic data attribution problem [56; 57]. Indeed, the influence function approach, originally developed for assessing the impact of individual training data points on model generalization performance , has also been used in MU for classification models [12; 51] and in analyzing LLM's generalization . Furthermore, data attribution is essential in solving dataset pruning or coreset selection problems [59; 60; 61; 62; 63]. By contrast, the problem of _weight_ attribution has received less attention compared to data attribution in the context of LLM unlearning, where the former aims to identify a model-level fingerprint, _i.e._, the subset of most influential weights, for the unlearning task. One relevant line of research is weight localization-informed unlearning [31; 32], which provides insights into which model units (such as layers and neurons) should be edited for effective unlearning. However, a precise characterization of weight influence in unlearning is still lacking . In the non-unlearning context, weight pruning [65; 66; 67; 68; 69] can also be considered a weight attribution method that focuses solely on model utility performance. Yet, we will show that weight pruning alone is insufficient for identifying the model fingerprint for LLM unlearning.

## 3 Preliminary and Problem Setup

Definition and formulation of LLM unlearning.LLM unlearning pertains to the MU problem in LLMs, aimed at removing undesirable data influence (_e.g._, sensitive, illegal, or harmful information) and the associated model capabilities, without sacrificing the integrity of essential knowledge generation that is unrelated to what is being forgotten . Despite the pressing need for effective LLM unlearning [15; 25; 26; 27; 31; 32; 54; 55], achieving this goal remains a substantial challenge. In particular, _retraining_ LLMs from scratch after removing the targeted training data for unlearning is infeasible due to (1) the prohibitive training costs and (2) the difficulty of precisely attributing and localizing the specific training data points to forget. Instead of that, LLM unlearning is typically achieved via model fine-tuning or alignment for a pre-trained model.

More concretely, let \(_{}\) denote the pre-trained LLM, and the unlearning task be represented through a _forget set_\(_{}\). It also defines a _forget loss_, \(_{}(_{};)\), to optimize for the model post-unlearning \(\) (referred to as 'unlearned model'). Additionally, the unlearned model needs to retain the model utility.

Therefore, a _retain set_\(_{}\) is often incorporated into the unlearning objective. This set is unrelated to what is being forgotten but enforces model utility through a _retain loss_\(_{}(_{};)\). To strike a balance between unlearning effectiveness and utility preservation, the problem of LLM unlearning is formulated as a regularized optimization problem :

\[}{}_{}(_{ };)+_{}(_{}; )\] (1)

where \( 0\) is a regularization parameter. If \(=0\), then unlearning relies solely on the forget set. However, existing unlearning methods, such as gradient ascent (GA) , have demonstrated that omitting the retain loss would result in a significant degradation of model utility post-unlearning.

Forget loss design and specific unlearning methods.In (1), the retain loss \(_{}\) typically mirrors the training loss over the retain set. Yet, the design of the forget loss \(_{}\) is more challenging, as it influences the specific approach to LLM unlearning. In what follows, we review three state-of-the-art (SOTA) methods for LLM unlearning and explore the design of their respective forget loss functions.

_Gradient difference (GradDiff) :_\(_{}=_{}\). GradDiff specifies \(_{}\) as the _negative_ training loss (also known as the GA loss \(_{}\)) to encourage the response of the LLM post-unlearning to deviate from its original response within the training set. This method is equivalent to using GA on the forget set while applying gradient descent on the retain set, which explains the name GradDiff.

_Negative preference optimization (NPO) :_\(_{}=_{}\). NPO specifies the forget loss \(_{}\) as the loss of direct preference optimization (DPO)  by treating the forgotten data in \(_{}\) exclusively as negative examples in DPO. This _negative_ example-only variant of the DPO loss is referred to as NPO \(_{}\). Compared to GradDiff, the NPO loss outperforms the GA loss due to its improved stability, avoiding catastrophic collapse in forgetting and utility preservation during optimization .

_Preference optimization (PO) :_\(_{}=_{}\). This approach is also inspired by DPO but introduces targeted unlearning responses such as 'I don't know' or responses stripped of sensitive information, treating these exclusively as _positive_ examples for preference alignment. In contrast to NPO, the positive example-based forget loss is termed as \(_{}\). Compared to GradDiff, PO modifies the unbounded GA loss by introducing the positive unlearning response for a bounded forget loss.

Throughout the paper, we will address the problem of LLM unlearning following the generic formulation (1), with specific implementations using GradDiff, NPO, or PO.

Weight attribution in LLM unlearning: Rationale and motivation.As shown above, past research has primarily focused on _algorithm-centric_ perspectives to tackle LLM unlearning problems. Yet, effective unlearning also requires a sense of locality, which involves identifying the sub-components of the LLM (_i.e._, a subset of weights in this work) that are crucial for the unlearning task, while minimally impacting the model's original utility. Such a _model-level_ fingerprint of LLM unlearning is agnostic to specific unlearning algorithms, potentially leading to a universal booster for LLM unlearning. It also exposes the modularity characteristics of LLMs, facilitating modular unlearning that specifically targets the designated weight subspace.

Thus, we propose to investigate the problem of **weight attribution** in LLM unlearning, which involves assessing the influence of weights so as to identify the critical subset of weights essential for effective and modular unlearning. In the context of non-LLM unlearning, weight sparsity  or gradient-based saliency  has proven beneficial for narrowing the gap between GA-type approximate unlearning and exact unlearning (_i.e._, retraining from scratch). Yet, when applied to LLMs, the effectiveness remains elusive.

**Fig. 1** provides a preliminary demonstration of the (in)effectiveness of unlearning (measured by the average unlearning efficacy, as defined in Tab. 1) and model utility (measured by the average utility performance, also defined in Tab. 1) vs. pruning-induced weight selection. This is achieved by applying the SOTA unlearning method NPO to update the remaining (unpruned) weights of LLMs, where weight sparsity is determined using the SOTA pruning method Wanda , in the context of TOFU unlearning . A lower sparsity indicates that a larger proportion of weights are updated during the unlearning process. As observed, the unlearning efficacy is highly sensitive

Figure 1: Unlearning efficacy and utility performance of NPO-based unlearning on TOFU dataset vs. sparsity of unlearned weights (_i.e._, the proportion of weights required for unlearning updates), which is achieved using the LLM pruning method Wanda.

to weight sparsity, as demonstrated by the sharp decline in efficacy as sparsity increases compared to the dense model (0% sparsity). In addition, there is a clear tradeoff between unlearning efficacy and model utility. This highlights the challenge of identifying an optimal subset of weights for LLM unlearning-one that maintains both unlearning efficacy and utility. This sets the stage for our key research question: _How can we precisely measure the roles of model weights in LLM unlearning?_ In the next section, we will introduce a new principled approach to weight attribution in LLM unlearning.

## 4 Weight Attribution for Enhanced LLM Unlearning

Weight attribution: Balancing unlearning 'objective' with utility 'constraint'.As inspired by Fig. 1, an effective weight attribution framework should account for not only utility preservation but also unlearning effectiveness. To address this challenge, we draw inspiration from bi-level optimization (BLO) , where we leverage the _upper-level_ problem to evaluate the impact of weight adjustments on unlearning efficacy and the _lower-level_ problem to ensure the retention of utility.

Specifically, let \(\) represent the weight-adjusted model, where \(\) denotes the modifications applied to the weights \(\), and \(\) is element-wise multiplication. For example, if we choose \(=+_{i}\), with \(_{i}\) representing the \(i\)th basis vector, then \(\) corresponds to perturbing the \(i\)th weight \(_{i}\) to \((1+)_{i}\). Here, \(\) controls the perturbation strength, and \(=-1\) corresponds to pruning the \(i\)th weight. The goal of weight attribution is then to evaluate the influence of the weight adjustment \(\) on unlearning. Thus, given the forget loss \(_{}\) and the weight-adjusted model \(\), we measure the influence of the weights through the following _unlearning sensitivity score_: \(_{}()-_{}()\), where we omit the dependence of \(_{}\) on the forget set \(_{}\) for notational simplicity. However, the above attribution involves an additional _implicit constraint_: The model parameters \(\) must minimize the retain loss to meet the model's utility. That is, \(^{*}()=_{}_{}()\), where the solution is denoted by \(^{*}()\) to signify its dependency on the weight modification scheme \(\).

By integrating the implicit model utility constraint into the unlearning sensitivity score, the proposed weight attribution problem can be cast as a BLO-type problem below:

\[&_{}(^{*}())-_{}(^{*}())&\\ &^{*}()=_{}_{ }(),&\] (2)

where the upper-level and lower-level problems are coupled through the lower-level solution \(^{*}()\), and it reduces to the pre-trained model \(^{*}()=_{}\) as \(=\).

Analyzing weight attribution via implicit gradient.We next address the weight attribution problem (2) by linking the upper-level unlearning sensitivity analysis with the lower-level utility optimization through _implicit gradient_ (**IG**), which is used in BLO to characterize the gradient flow from the lower-level solution to the upper-level variable. By employing the first-order Taylor expansion to the upper-level objective of (2) at \(=\), the unlearning sensitivity _w.r.t._\(\) becomes:

\[_{}(^{*}())-_{}(^{*}()) (-)^{}}( ^{*}())}{d}_{=}\] \[= (-)^{}^{*}()]}{d}_{=}_{ }(_{})\] (3)

where \({}^{}\) denotes the matrix transpose, and \(}{d}^{||||}\) is the _full_ derivative of \(\) w.r.t. \(\) with \(||\) denoting the cardinality of the vector \(\). In (3), the second equality holds due to the chain rule, and we have used the facts that \(^{*}()=_{}\) and the convention \(_{}(_{})=}( )}{d}_{=^{*}()}\).

It is clear from (3) that assessing the influence of weight modification \(\) in unlearning requires deriving \(^{*}()]}{d^{ *}}\). This necessitates the derivation of IG, \(^{*}()}{d}\), the gradient flow from the lower-level solution \(^{*}()\) to the upper-level variable \(\). Inspired by the implicit function approach for solving BLO problems , IG can be derived as applied to differentiating the parameterized \(\) problem [72; 73]; see derivations in **Appx. A**. This leads to

\[^{*}()}{d}= -_{,}_{}( )_{=^{*}()}[_{ ,}_{}()]_{ {}=^{*}()}^{-1}\] \[ -(_{}_{ }()_{}=^{*}( )}),\] (4)

where \(_{,}_{}\) denotes the cross-variable second-order derivative of the bi-variate function \(_{}()\) w.r.t. the variables \(\) and \(\), \(_{,}_{}\) denotes the Hessian matrix of \(_{}\) w.r.t. the variable \(\),is the matrix inversion, \(()\) represents the diagonal matrix with the diagonal vector \(\), and \(_{}_{}()|{}_{= ^{*}()}.\) signifies the gradient of \(_{}\) w.r.t. its combined input argument \(\) at \(=^{*}()\). In (4), the first equality holds due to the application of the implicit function theorem , and the second approximation is obtained under the diagonal Hessian assumption \(_{,}_{}=\), where \(>0\) serves as a tunable hyperparameter or is regarded as a Hessian diagonal estimate to compensate for the loss of the Hessian approximation.

Substituting IG (4) into (3), we obtain the analytical form of the unlearning sensitivity to \(\):

\[_{}(^{*}())-_{}(^{*}())  (-)^{}(_{ }-_{}(_{ })/)_{}(_{})\] (5) \[= (-)^{}[(_{} -_{}(_{})/ )_{}(_{})],\]

where we obtained the derivative \(^{*}()]}{d}\) in (3) using the chain rule and the diagonal matrix expression of IG in (4), and the second equality holds due to \(()=\). The formula (5) provides a principled framework for weight attribution, which evaluates the influence of weight perturbations \(\) in the unlearning performance, and considers both impacts of data to forget (encoded in \(_{}\)) and data to retain (encoded in \(_{}\)) in LLM unlearning.

To gain more insights into (5), we consider a single weight perturbation by specifying \(\) as \(=+_{i}\), where \(\) is the perturbation strength for the weight \(w_{i}\). Since the weight attribution process employs a Taylor expansion at \(=\) in (3), its validity necessitates setting \(\) as a small perturbation. Let \(S_{i}\) denote the attribution score of the \(i\)th weight. By substituting \(=+_{i}\) into (5), we obtain

\[S_{i}  _{i}^{}[(_{}- _{}(_{})/-_{i} _{}(_{})/)_{ }(_{})]\] (6) \[= ([_{}]_{i}-[_{}(_{})]_{i}/[_{}(_{})]_{i}-^{2}/[_{}(_{})]_{i} [_{}(_{})]_{i},\]

where \([]_{i}\) denotes the \(i\)th entry of the vector \(\). In (6), the first term plays a more dominant role than the second term because \(\) represents a small weight perturbation, making \(^{2}\). Thus, we propose to drop the second term and simplify the weight attribution score as

\[S_{i}[_{}]_{i}[_{}(_{})]_{i}}_{}-}(_{})]_{i}[_{}(_{})]_{i}}_{}\] (7)

where the constant \(\) is omitted without loss of generality, and the attribution score \(S_{i}\) is determined by the two terms 1 and 2 that can be interpreted, respectively. In (7), the first term 1 aligns with the weight pruning score SNIP , which characterizes the sensitivity of the forget loss to sparsifying the \(i\)th weight initialized by its pre-trained state. The second term 2 accounts for the additional utility retention effect under the \(i\)th weight modification. Furthermore, the roles of these two terms 1 and 2 are regularized by the Hessian parameter \(\) in (4); See Remark 1 for its choice.

**Remark 1:** As will be evident later, our experiments reveal some interesting empirical findings that can guide the choice of \(\), which we explain below. Recall from (4) that \(\) represents the Hessian diagonal estimate of the retain loss \(_{}\). One rough but feasible approach to setting \(\) is to use a quasi-Newton method , which approximates the Hessian diagonal by employing the element-wise product of the first-order gradients of \(_{}\). Thus, we can use the corresponding gradient norm as an indicator to guide us to either increase or decrease the hyperparameter \(\). We find that if the retain loss closely resembles the training loss (_i.e._, the retain set shares a similar distribution with the training set), then the pre-trained model \(_{0}\) resides in the minima basin of the retain loss, resulting in small gradients and a small Hessian diagonal parameter \(\). The fictitious unlearning over the TOFU dataset  belongs to the above scenario. By contrast, if the retain set is not representative of the training set, then we need a larger Hessian diagonal parameter choice for \(\). The copyrighted information unlearning task on the Harry Potter book series dataset  falls into this scenario.

Vagle: Weight attribution-guided LLM unlearning.By ranking the magnitudes of the attribution scores \(\{S_{i}\}_{i}\) in descending order, we then select the top ones and determine the subset of weights most influential in LLM unlearning. Let \(_{S}\) represent the weight selection mask, where \([_{S}]_{i}=1\) denotes the selection of the \(i\)th weight based on its attribution score and \(0\) otherwise. Given \(_{S}\), we update only the partial model parameters in \(\) identified by \(_{S}\), rather than the entire model. This modifies the LLM unlearning problem (1) to Wagle:

\[}{}_{}(_{ };_{S}+(-_{S}) _{})+_{}(_{}; _{S}+(-_{S})_{ }),\] (8)

where \(_{S}+(-_{S})_{ }\) encodes the modularity characteristics of the LLM for unlearning, decomposing the model weights into the optimized part \(_{S}\) and the other part that remains the same as the pre-trained weights. It is evident from (8) that incorporating weight attribution \(_{S}\) into LLM unlearning is strategic to specific unlearning algorithms. Therefore, we can implement WAGLE based on all existing methods (GradDiff, NPO, and PO) introduced in Sec. 3.

## 5 Experiment

### Experiment Setups

Unlearning tasks, datasets, and models.To demonstrate the significance of weight attribution and the effectiveness of WAGLE, we conduct experiments on **four** LLM unlearning tasks. 1 Fictitious unlearning on **TOFU** dataset : It contains information about fictional authors for fine-tuning LLMs, and parts of these authors' profiles (with \(10\%\)_forget ratio_) can be designated as the forget set. 2 Malicious use prevention of LLMs in developing cyberattacks or bioweapons on **WMDP** dataset : This benchmark assesses the ability to unlearn and prevent the generation of hazardous knowledge in biosecurity, cybersecurity, and chemical security. 3 Copyrighted information removal in **WHP** (Who's Harry Potter) task : This pertains to the task of unlearning the Harry Potter books from LLMs. 4 Model detoxification (**DETOX**) on PKU-SafeRLHF dataset : This aims to leverage LLM unlearning to prevent the generation of toxic content in response to inappropriate prompts from SafeRLHF. Model-wise, we use the LLaMA2-7B-chat  provided by the TOFU benchmark. For WMDP, we adopt the Zephyr-7B-beta model , consistent with the benchmark. For WHP, we utilize the LLaMA2-7B  fine-tuned on the Harry Potter book series. Finally, we employ the LLaMA2-7B for DETOX. See Appx. B.1 and Appx. B.2 for details.

Training setup.To obtain LLMs post-unlearning (_i.e._, unlearned LLMs), we first carry out the weight attribution method (7) to obtain the weight selection mask \(_{S}\) used in (8). Unless specified otherwise, the Hessian diagonal parameter \(\) in (7) is chosen to be a small value \(10^{-6}\) for TOFU and WMDP tasks and a large value \(10^{4}\) for WHP and \(10^{6}\) for DETOX, as guided by Remark 1. The sparsity ratio of \(_{S}\) is tuned for each task based on a greedy search, as exemplified in Fig. A1. Given the weight selection scheme, we then solve the optimization problem using its specific unlearning method: GradDiff , NPO , and PO , respectively. AdamW  is used as the default optimizer. It is worth noting that we set the utility regularization parameter \(\) as \(1\). In the implementation of PO, we use the reject-based answer as the targeted response over the forget set. See Appx. B.3 and Appx.B.4 for additional details.

Evaluation setup.We evaluate the performance of unlearned LLMs from unlearning efficacy (**UE**) and preserved model utility (**UT**). For the **TOFU** task, UE is assessed using four metrics. (1) Forget quality (FQ) quantifies the distinguishability between statistical measures of forgetting and retaining. We employ the Kolmogorov-Smirnov (KS) test to compare the truth ratios produced by the unlearned model on forget and retain sets, defining FQ as \(1-p\)-value obtained from the KS test. A higher FQ indicates better forgetting, characterized by the better distinguishability between forget data and retain data. (2) Membership inference attack (MIA) is evaluated by the area under the ROC curve using Min-\(k\%\) Prob  to detect if the provided text belongs to the training or testing set. We apply MIA to the forget set; thus, a higher MIA score indicates a higher confidence in predicting that the forget data point does _not_ belong to the training set. (3) Forget accuracy (FA) refers to the accuracy of LLMs post-unlearning on the forget set. For ease of performance averaging, we also use \(1-\)FA to measure UE. Thus, a higher \(1-\)FA implies better unlearning. (4) Rouge-L recall is also measured over the forget set. A lower value corresponds to better unlearning. The metric \(1-\)Rouge-L is also used for ease of performance averaging. Next, we measure UT of unlearned LLMs by computing the accuracy and Rouge-L recall on the retain set, as well as on subsets related to real authors and world facts. Higher values in these metrics imply better utility retention. For the **WMDP** task, UE is measured using the benchmark-provided WMDP-Bio and WMDP-Cyber subsets. We use \(1-\)FA as the UE metric for each evaluation subset. In addition, UT is evaluated using zero-shot accuracy on the MMLU dataset . For the **WHP** task, UE is evaluated by Rouge-L on both seen and unseen text completion instructions from the Harry Potter book series, with lengths of 300 tokens. UT is assessed using the Language Model Evaluation Harness , which computes perplexity (PPL) on the Wikitext dataset  and mean zero-shot accuracy across tasks. Additional evaluations include TruthfulQA . For the **DETOX** task, UE is measured by the toxic scores from Toxic-BERT  under real toxic prompts  and the PKU-SafeRLHF test set . Thus, the lower toxic scores imply better unlearning. The UT evaluation is the same as WHP. See Appx. B.5 for addition details.

Baselines.We demonstrate the effectiveness of our proposed WAGLE method by comparing it with the LLM unlearning baselines GradDiff , NPO , and PO . These baselines are applied to the original pre-trained, dense model (referred to as _Dense_) as well as their weight selection-based variants, including the randomly sparsified model (referred to as _Random_), the weight magnitude-based pruned model (referred to as _Magnitude_), the Wanda-enabled pruned model  (referred to as _Wanda_), and the low-rank adaptation scheme (LoRA) . Results are averaged over 3 random trials.

### Experiment Results

LLM unlearning on TOFU.In **Tab. 1**, we present the UE (unlearning efficacy) and UT (utility) performance of our proposed WAGLE when integrating weight attribution into different unlearning methods GradDiff, NPO, and PO. We also compare our performance with unlearning variants using different weight selection or adaptation schemes. For example, the term 'GradDiff + Magnitude' refers to the application of GradDiff to the magnitude-based pruned model through the optimization in (8). As we can see, under each unlearning method category, the incorporation of weight attribution consistently improves unlearning effectiveness, as evidenced by the rise in UE Avg. Utility-wise, although WAGLE does not always yield the best utility retention (as measured by UT Avg.), it consistently improves over all the dense model-based LLM unlearning methods. This suggests that the incorporation of weight attribution can improve UE while resulting in a graceful tradeoff with UT. Furthermore, we observe that NPO is a much more aggressive unlearning method, yielding the best unlearning efficacy but inevitably causing a larger degradation in model utility. By contrast, PO appears to be a more balanced unlearning method, achieving a better tradeoff between UE and UT.

LLM unlearning on WMDP.In **Tab. 2**, we demonstrate the UE and UT performance of WAGLE on the WMDP benchmark. Recall that UE is measured by FA (forget accuracy) on the WMDP-Bio and WMDP-Cyber subsets provided by this benchmark, while UT is measured by the accuracy on the MMLU dataset. Unlike the TOFU task, PO for LLM unlearning is not considered for WMDP. This is because the forget set in WMDP is given as a set of plain texts, whereas PO requires conversational-style data for unlearning. Forced rejection on plain texts leads to over-forgetting of the irrelevant knowledge involved in these texts when using PO. As we can see, WAGLE (_i.e._, ours) yields the best UE, as evidenced by the 'UE Avg.' metric. Consistent with the TOFU results, the incorporation of weight

    &  &  \\   & &  &  &  &  &  &  &  &  \\  Original (CoRA) & 0.3614 & 0.5904 & 0.4657 & 0.5665 & 0.4659 & 0.5115 & 0.5663 & 0.4459 \\   & Dress & 0.4609 & 0.6517 & 0.6655 & 0.4659 & 0.5167 & 0.5167 & 0.5167 & 0.5486 \\ GraphDiff & WAGLE & 0.4499 & 0.5786 & 0.5825 & 0.5846 & 0.5846 & 0.5846 & 0.5846 \\  & LARA & 0.4683 & 0.6631 & 0.6644 & 0.6733 & 0.4346 & 0.4466 & 0.4466 & 0.4466 \\ Ours & 0.4730 & 0.6793 & 0.6995 & **0.6071** & **0.6550** & 0.6621 & 0.5167 & 0.5167 & 0.5167 \\   & Dress & 0.6678 & 0.7366 & 0.7666 & 0.6687 & 0.7354 & 0.7548 & 0.7548 \\  & Magnitude & 0.5589 & 0.6447 & 0.6618 & 0.6308 & 0.5124 & **0.6520** \\   & LARA & 0.4687 & 0.6678 & 0.6639 & 0.5536 & 0.5288 & 0.5260 & 0.5260 & 0.5286 \\   & LARA & 0.4681 & 0.4687 & 0.6638 & 0.5263 & 0.5260 & 0.5260 & 0.5260 & 0.5260 \\   & Ours & 0.6998 & 0.7306 & **0.706** & **0.7028** & 0.5033 & 0.5033 \\   

Table 2: Performance overview of LLM unlearning on the WMDP task under Zephyr-7B-beta, with a table format similar to Tab. 1. Results are averaged over six independent random trials.

    &  &  \\   & &  &  &  &  &  &  &  &  &  \\  Original (CoRA) & 0.3614 & 0.5904 & 0.4657 & 0.5665 & 0.4659 & 0.5115 & 0.5663 & 0.4459 \\   & Dress & 0.4609 & 0.6517 & 0.6656 & 0.4659 & 0.5167 & 0.5167 & 0.51679 & 0.5223 & 0.5486 \\ GraphDiff & WAGLE & 0.4498 & 0.5786 & 0.5825 & 0.5846 & 0.5846 & 0.5846 & 0.5846 \\  & LARA & 0.4683 & 0.6631 & 0.6634 & 0.6733 & 0.5111 & 0.5866 \\  & LARA & 0.4673 & 0.6793 & 0.6999 & **0.6071** & **0.6550** & **0.6550** \\   & Dress & 0.6678 & 0.7366 & 0.6687 & 0.7354 & 0.7354 & 0.7354 & 0.7354 \\  & Magnitude & 0.5589 & 0.64447 & 0.6618 & 0.5124 & 0.5124 & 0.5126 & 0.5126 & 0.5126 \\   & LARA & 0.4687 & 0.6678 & 0.6638 & 0.5263 & 0.5260 & 0.5260 & 0.5210 & 0.5167 & 0.5081 \\   & Ours & 0.6998 & 0.7306 & **0.706** & **0.7028** & 0.5033 & 0.5033 \\   

Table 1: Performance overview of LLM unlearning on the TOFU task under the LLAMA2-7B-chat model . The \(\) symbol denotes metrics where higher values indicate better UE or UT performance. The ‘UE Avg.’ and ‘UT Avg.’ refer to the average unlearning efficacy across all UE metrics and the average utility post-unlearning across all UT metrics, respectively. Results are averaged over six independent random trials. The best average performance is highlighted in **bold**.

attribution in WAGLE improves LLM unlearning compared to the 'Dense' variant. These results are consistent with findings from the TOFU dataset.

To illustrate how WAGLE enhances unlearning efficacy, we present generation examples from LLMs post-unlearning under the WMDP benchmark. Failed unlearning attempts, resulting in undesired outputs, are highlighted in red, while successful instances, producing desired unlearning outcomes, are marked in green. Traditional methods like GradDiff and NPO, when applied to the dense model, still select hazardous knowledge. However, integrating WAGLE with these methods consistently prevents the selection of such undesired answers, demonstrating WAGLE's improvement in unlearning performance as shown in Tab. 2. Additional visualizations are available in Tab. A4.

LLM unlearning on WHP and DETOX.In Tab. 3, we compare the UE and UT performance of WAGLE with baselines in two additional unlearning tasks, WHP and DETOX. Here, we adopt PO as the unlearning method due to its effectiveness in striking the tradeoff between UE and UT. We observe that, similar to other unlearning tasks, the use of weight attribution in WAGLE improves unlearning effectiveness while preserving model utility compared to unlearning without using weight attribution. In addition to quantitative assessments, we also provide examples of the responses of LLMs post-unlearning across various tasks in Appx. D.

Exploring model fingerprint of LLM unlearning from weight attribution.Further, we examine which weights of an LLM (specifically LLaMA2-7B-chat) are attributed as influential for the unlearning. To this end, Fig. 2 presents the density of selected weights within each LLM module, including the self-attention (sa) components query (q), key (k), value (v), and the output layer (o) producing the final output from as. In addition to as, we also include input layer (in), layer normalization (ln), MLP components, and post attention (post) modules. Here, the overall weight selection ratio determined by weight attribution is set to 80%, and PO-based WAGLE is used for LLM unlearning on the TOFU dataset. For comparison, we also present the density of selected weights based on their magnitudes. It is evident that the density of weights chosen for unlearning shows a markedly different trend from that of magnitude-based selection. Notably, unlearning favors a higher selection of weights in sa.o and sa.v, as well as MLP layers. By contrast, less weights in sa.k and sa.q are influential. Our findings echo the importance of editing neurons in feed-forward networks [32; 88] and highlight that important weights are not merely restricted to key-value memories . In addition, we present the layer-wise sparsity levels in Fig. A2. We observe that early-to-mid layers are important for unlearning.

  
Exploring the role of the Hessian diagonal hyperparameter \(\) in weight attribution.As discussed in Remark 1 of Sec. 4, it is critical but non-trivial to choose an appropriate Hessian diagonal parameter \(\) for weight attribution (7). One feasible method is to estimate its value using the gradient norm, as employed by the quasi-Newton method . However, this estimate could be rather rough if the retain loss does not resemble the training loss, meaning that the pre-trained model \(_{}\), at which the gradient norm is evaluated, does not stay in the minima basin of the retain loss. And this may occur based on the context of LLM unlearning.

To demonstrate the critical role of \(\), **Fig. 3** presents the average UE performance of using the PO-based WAGLE versus \(/\), _i.e._, the ratio of \(\) and the gradient norm (GN) of the retain loss at \(_{}\), on TOFU and WHP datasets. As observed, UE improves as \(/\) decreases on TOFU. This is not surprising, as TOFU has an accurate retain set, leading to a better Hessian diagonal estimate using GN. Thus, even the case of \(=\) suffices to improve UE. In addition, the alignment of the retain set with the training set also results in a relatively small gradient, making GN small accordingly. As a result, the choice of \(\) in TOFU is consistent with GN and favors a small value. By contrast, the best choice of \(\) for WHP favors a large value, as GN is no longer a reliable Hessian diagonal estimate, due to WHP not offering a very accurate retain set.

**Computational efficiency of the unlearning process.** First, as indicated by (7) - (8), the weight attribution mask can be computed offline using only first-order derivatives. As a result, generating a general unlearning mask for the TOFU dataset takes approximately 4 minutes on the Llama2-7B-chat model, as shown in **Tab. 4**. Second, applying the mask during the unlearning process requires a similar running time across different unlearning methods. Given the total unlearning duration of 30 minutes, the time spent generating the attribution mask is relatively insignificant, affirming the efficiency of our method.

**Examining weight attribution sparsity on unlearning.** We find that enhancing LLM unlearning with weight attribution requires a non-oversparse weight selection scheme, typically between 80% and 95%. However, the best ratio varies across different unlearning methods. See Fig. A1 for results.

## 6 Conclusion

To improve the forgetting efficacy and utility retention ability of existing LLM unlearning methods, we provide a new perspective on LLM unlearning through weight attribution. Drawing inspiration from bi-level optimization (BLO), we propose a principled scoring framework to assess how adjustments to weights affect LLM unlearning. Utilizing the implicit gradient approach in BLO, we derive the closed-form solution for weight attribution. Integrating this weight attribution scheme into LLM unlearning, we develop the weight attribution-guided LLM unlearning method (WAGLE). Our extensive experiments demonstrate that WAGLE enhances unlearning performance across a range of LLM unlearning methods in diverse applications. See the discussions on limitations and broader impacts in Appx. E and Appx. F.