# No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions

Tiancheng Jin

University of Southern California

tiancheng.jin@usc.edu

Equal contribution.

Junyan Liu

University of California, San Diego

jul037@ucsd.edu

Chloe Rouyer

University of Copenhagen

chloe@di.ku.dk

&Chloe Rouyer

University of Copenhagen

chloe@di.ku.dk

&William Chang

University of California, Los Angeles

chang314@g.ucla.edu

Chen-Yu Wei

MIT Institute for Data, Systems, and Society

chenyuw@mit.edu

&Haipeng Luo

University of Southern California

haipengl@usc.edu

###### Abstract

Existing online learning algorithms for adversarial Markov Decision Processes achieve \(()\) regret after \(T\) rounds of interactions even if the loss functions are chosen arbitrarily by an adversary, with the caveat that the transition function has to be fixed. This is because it has been shown that adversarial transition functions make no-regret learning impossible. Despite such impossibility results, in this work, we develop algorithms that can handle both adversarial losses and adversarial transitions, with regret increasing smoothly in the degree of maliciousness of the adversary. More concretely, we first propose an algorithm that enjoys \(}(+C^{})\) regret where \(C^{}\) measures how adversarial the transition functions are and can be at most \((T)\). While this algorithm itself requires knowledge of \(C^{}\), we further develop a black-box reduction approach that removes this requirement. Moreover, we also show that further refinements of the algorithm not only maintains the same regret bound, but also simultaneously adapts to easier environments (where losses are generated in a certain stochastically constrained manner as in Jin et al. (2021)) and achieves \(}(U+}}+C^{})\) regret, where \(U\) is some standard gap-dependent coefficient and \(C^{}\) is the amount of corruption on losses.

## 1 Introduction

Markov Decision Processes (MDPs) are widely-used models for reinforcement learning. They are typically studied under a fixed transition function and a fixed loss function, which fails to capture scenarios where the environment is time-evolving or susceptible to adversarial corruption. This motivates many recent studies to overcome these challenges. In particular, one line of research, originated from Even-Dar et al. (2009) and later improved or generalized by e.g. Neu et al. (2010, 2010); Zimin and Neu (2013); Rosenberg and Mansour (2019); Jin et al. (2020), takes inspiration from the online learning literature and considers interacting with a sequence of \(T\) MDPs, each with an adversarially chosen loss function. Despite facing such a challenging environment, the learner can still ensure \(()\) regret (ignoring other dependence; same below) as shown by these works, that is, the learner's average performance is close to that of the best fixed policy up to \((1/)\).

However, one caveat of these studies is that they all still require the MDPs to have to the same transition function. This is not for no reason -- Abbasi Yadkori et al. (2013) shows that even with full information feedback, achieving sub-linear regret with adversarial transition functions is computationally hard, and Tian et al. (2021) complements this result by showing that under the more challenging bandit feedback, this goal becomes even information-theoretically impossible without paying exponential dependence on the episode length.

To get around such impossibility results, one natural idea is to allow the regret to depend on some measure of maliciousness \(C^{}\) of the transition functions, which is \(0\) when the transitions remain the same over time and \((T)\) in the worst case when they are completely arbitrary. We review several such attempts at the end of this section, and point out here that they all suffer one issue: even when \(C^{}=0\), the algorithms developed in these works all suffer _linear regret_ when the loss functions are completely arbitrary, while, as mentioned above, \(()\) regret is achievable in this case. This begs the question: _when learning with completely adversarial MDPs, is \((+C^{})\) regret achievable?_

In this work, we not only answer this question affirmatively, but also show that one can perform even better sometimes. More concretely, our results are as follows.

1. In Section 3, we develop a variant of the UOB-REPS algorithm (Jin et al., 2020), achieving \(}(+C^{})\) regret in completely adversarial environments when \(C^{}\) is known. The algorithmic modifications we propose include an enlarged confidence set, using the log-barrier regularizer, and a novel _amortized_ bonus term that leads to a critical "change of measure" effect in the analysis.
2. We then remove the requirement on the knowledge of \(C^{}\) in Section 4 by proposing a _black-box reduction_ that turns any algorithm with \(}(+C^{})\) regret under known \(C^{}\) into another algorithm with the same guarantee (up to logarithmic factors) even if \(C^{}\) is unknown. Our reduction improves that of Wei et al. (2022) by allowing adversarial losses, which presents extra challenges as discussed in Pacchiano et al. (2022). The idea of our reduction builds on top of previous adversarial model selection framework (a.k.a. Corral (Agarwal et al., 2017; Foster et al., 2020; Luo et al., 2022)), but is even more general and is of independent interest: it shows that the requirement from previous work on having a _stable_ input algorithm is actually redundant, since our method can turn _any_ algorithm into a stable one.
3. Finally, in Section 5 we also further refine our algorithm so that it simultaneously adapts to the maliciousness of the loss functions and achieves \(}(\{,U+}}\}+C^{})\) regret, where \(U\) is some standard gap-dependent coefficient and \(C^{} T\) is the amount of corruption on losses (this result unfortunately requires the knowledge of \(^{}\), but not \(U\) or \(C^{}\)). This generalizes the so-called _best-of-both-worlds_ guarantee of Jin and Luo (2020); Jin et al. (2021); Dann et al. (2023) from \(C^{}=0\) to any \(C^{}\), and is achieved by combining the ideas from Jin et al. (2021) and Ito (2021) with a novel _optimistic transition_ technique. In fact, this technique also leads to improvement on the dependence of episode length even when \(C^{}=0\).

Related WorkHere, we review how existing studies deal with adversarially chosen transition functions and how our results compare to theirs. The closest line of research is usually known as _corruption robust_ reinforcement learning (Lykouris et al., 2019; Chen et al., 2021; Zhang et al., 2021; Wei et al., 2022), which assumes a ground truth MDP and measures the maliciousness of the adversary via the amount of corruption to the ground truth -- the amount of corruption is essentially our \(C^{}\), while the amount of corruption is essentially our \(C^{}\) (these will become clear after we provide their formal definitions later). Naturally, the regret in these works is defined as the difference between the learner's total loss and that of the best policy _with respect to the ground truth MDP_, in which case \(}(+C^{}+C^{})\) regret is unavoidable and is achieved by the state-of-the-art (Wei et al., 2022).

On the other hand, following the canonical definition in online learning, we define regret _with respect to the corrupted MDPs_, in which case \(}(+C^{})\) is achievable as we show (regardless how large \(C^{}\) is). To compare these results, note that the two regret definitions differ from each other by an amount of at most \((C^{}+C^{})\). Therefore, _our result implies that of Wei et al. (2022), but not vice versa_ -- what Wei et al. (2022) achieves in our definition of regret is again \(}(+C^{}+C^{})\), which is never better than ours and could be \((T)\) even when \(C^{}=0\).

In fact, our result also improves upon that of Wei et al. (2022) in terms of the gap-dependent refinement -- their refined bound is \(}(\{,G\}+C^{}+C^{})\) for some gap-dependent measure \(G\) that is known to be no less than our gap-dependent measure \(U\); on the other hand, based on earlier discussion, our refined bound _in their regret definition_ is \(}(\{,U+}}\}+C^{}+C ^{})=}(\{,U\}+C^{}+C^{ })\) and thus better. The caveat is that, as mentioned, for this refinement our result requires the knowledge of \(C^{}\), but Wei et al. (2022) does not.2 However, we emphasize again that for the gap-independent bound, our result does not require knowledge of \(C^{}\) and is achieved via an even more general black-box reduction compared to the reduction of Wei et al. (2022).

Finally, we mention that another line of research, usually known as _non-stationary reinforcement learning_, also allows arbitrary transition/loss functions and measures the difficulty by either the number of changes in the environment (Auer et al., 2008; Gajane et al., 2018) or some smoother measure such as the total variation across time (Wei and Luo, 2021; Cheung et al., 2023). These results are less comparable to ours since their regret (known as dynamic regret) measures the performance of the learner against the best _sequence_ of policies, while ours (known as static regret) measures the performance against the best _fixed_ policy.

## 2 Preliminaries

We consider the problem of sequentially learning \(T\) episodic MDPs, all with the same state space \(S\) and action space \(A\). We assume without loss of generality (similarly to Jin et al. (2021)) that the state space \(S\) has a layered structure and is partitioned into \(L+1\) subsets \(S_{0},,S_{L}\) such that \(S_{0}\) and \(S_{L}\) only contain the initial state \(s_{0}\) and the terminal state \(s_{L}\) respectively, and transitions are only possible between consecutive layers. For notational convenience, for any \(k<L\), we denote the set of tuples \(S_{k} A S_{k+1}\) by \(W_{k}\). We also denote by \(k(s)\) the layer to which state \(s S\) belongs.

Ahead of time, knowing the learner's algorithm, the environment decides the transition functions \(\{P_{t}\}_{t=1}^{T}\) and the loss functions \(\{_{t}\}_{t=1}^{T}\) for these \(T\) MDPs in an arbitrary manner (unknown to the learner). Then the learner sequentially interacts with these \(T\) MDPs: for each episode \(t=1,,T\), the learner first decides a stochastic policy \(_{t}:S A\) where \(_{t}(a|s)\) is the probability of taking action \(a\) when visiting state \(s\); then, starting from the initial state \(s_{t,0}=s_{0}\), for each \(k=0,,L-1\), the learner repeatedly selects an action \(a_{t,k}\) sampled from \(_{t}(|s_{t,k})\), suffers loss \(_{t}(s_{t,k},a_{t,k})\), and transits to the next state \(s_{t,k+1}\) sampled from \(P_{t}(|s_{t,k},a_{t,k})\) (until reaching the terminal state); finally, the learner observes the losses of those visited state-action pairs (a.k.a. bandit feedback).

Let \(_{t}()=_{h=0}^{L-1}_{t}(s_{h},a_{h})|P_{t}, \) be the expected loss of executing policy \(\) in the \(t\)-th MDP (that is, \(\{(s_{h},a_{h})\}_{h=0}^{L-1}\) is a stochastic trajectory generated according to transition \(P_{t}\) and policy \(\)). Then, the regret of the learner against any policy \(\) is defined as \(_{T}()=_{t=1}^{T}_{t}(_{t})-_{t}( )\). We denote by \(\) one of the optimal policies in hindsight such that \(_{T}()=_{}_{T}()\) and use \(_{T}_{T}()\) as a shorthand.

Maliciousness Measure of the TransitionsIf the transition functions are all the same, Jin et al. (2020) shows that \(_{T}=}(L|S|)\) is achievable, no matter how the loss functions are decided. However, when the transition functions are also arbitrary, Tian et al. (2021) shows that \(_{T}=(\{T,T}\})\) is unavoidable. Therefore, a natural goal is to allow the regret to smoothly increase from order \(\) to \(T\) when some maliciousness measure of the transitions increases. Specifically, the measure we use is

\[C^{}_{P^{}}_{t=1}^{T}_{k= 0}^{L-1}_{(s,a) S_{k} A} P_{t}(|s,a)-P^{}( |s,a)_{1}, \]

where \(\) denotes the set of all valid transition functions. Let \(P\) be the transition that realizes the minimum in this definition. Then \(C^{}\) can be regarded as the same _corruption_ measure used in Chen et al. (2021); there, it is assumed that a ground truth MDP with transition \(P\) exists, and the adversary corrupts it arbitrarily in each episode to obtain \(P_{t}\), making \(C^{}\) the total amount of corruption measured in a certain norm. For simplicity, in the rest of this paper, we will also take this perspective and call \(C^{}\) the transition corruption. We also use \(C^{}_{}=_{k=0}^{L-1}_{(s,a) S_{k} A} P_{ }(|s,a)-P(|s,a)_{1}\) to denote the per-round corruption (so \(C^{}=_{t=1}^{T}C^{}_{t}\)). It is clear that \(C^{}=0\) when the transition stays the same for all MDPs, while in the worst case it is at most \(2TL\). Our goal is to achieve \(_{T}=(+C^{})\) (ignoring other dependence), which smoothly interpolates between the result of Jin et al. (2020) for \(C^{}=0\) and that of Tian et al. (2021) for \(C^{}=(T)\).

Enlarged Confidence SetA central technique to deal with unknown transitions is to maintain a shrinking confidence set that contains the ground truth with high probability (Rosenberg and Mansour, 2019, 2019, Jin et al., 2020). With a properly enlarged confidence set, the same idea extends to the case with adversarial transitions (Lykouris et al., 2019). Specifically, all our algorithms deploy the following transition estimation procedure. It proceeds in epochs, indexed by \(i=1,2,\), and each epoch \(i\) includes some consecutive episodes. An epoch ends whenever we encounter a state-action pair whose total number of visits doubles itself when compared to the beginning of that epoch. At the beginning of each epoch \(i\), we calculate an empirical transition \(_{i}\) as:

\[_{i}(s^{}|s,a)=m_{i}(s,a,s^{})/m_{i}(s,a),(s,a, s^{}) W_{k},\;k=0, L-1, \]

where \(m_{i}(s,a)\) and \(m_{i}(s,a,s^{})\) are the total number of visits to \((s,a)\) and \((s,a,s^{})\) prior to epoch \(i\).3 In addition, we calculate the following transition confidence set.

**Definition 2.1**.: _(Confidence Set of Transition Functions) Let \((0,1)\) be a confidence parameter. With known corruption \(C^{}\), we define the confidence set of transition functions for epoch \(i\) as_

\[_{i}=P:|P(s^{}|s,a)-_{i}(s^ {}|s,a)| B_{i}(s,a,s^{}),\;(s,a,s^{}) W_ {k},k=0,,L-1}, \]

_where the confidence interval \(B_{i}(s,a,s^{})\) is defined, with \(=}{{}}\) as_

\[B_{i}(s,a,s^{})=\{1,16_{i}(s^{}|s,a) }{m_{i}(s,a)}}+64}+}{m_{i}(s,a)}\}. \]

Note that the confidence interval is enlarged according to how large the corruption \(C^{}\) is. We denote by \(_{}\) the event that \(P_{i}\) for all epoch \(i\), which is guaranteed to happen with high-probability.

**Lemma 2.2**.: _With probability at least \(1-2\), the event \(_{}\) holds._

Occupancy Measure and Upper Occupancy BoundSimilar to previous work (Rosenberg and Mansour, 2019, Jin et al., 2020), given a stochastic policy \(\) and a transition function \(\), we define the occupancy measure \(q^{P,}\) as the mapping from \(S A S\) to \(\) such that \(q^{,}(s,a,s^{})\) is the probability of visiting \((s,a,s^{})\) when executing policy \(\) in an MDP with transition \(\). Further define \(q^{,}(s,a)=_{s^{} S}q^{,}(s,a,s^{})\) (the probability of visiting \((s,a)\)) and \(q^{,}(s)=_{a A}q^{,}(s,a)\) (the probability of visiting \(s\)). Given an occupancy measure \(q\), the corresponding policy that defines it, denoted by \(^{q}\), can be extracted via \(^{q}(a|s) q(s,a)\).

Importantly, the expected loss \(_{t}()\) defined earlier equals \( q^{P_{t},},_{t}\), making our problem a variant of online linear optimization and enabling the usage of standard algorithmic frameworks such as Online Mirror Descent (OMD) or Follow-the-Regularized-Leader (FTRL). These frameworks operate over a set of occupancy measures in the form of either \(()=\{q^{,}:\,\}\) for some transition function \(\), or \((})=\{q^{,}:}, { is a stochastic policy}\}\) for some set of transition functions \(}\).

Following Jin et al. (2020), to handle partial feedback on the loss function \(_{t}\), we need to construct loss estimators \(_{t}\) using the (efficiently computable) _upper occupancy bound_\(u_{t}\):

\[_{t}(s,a)=_{t}(s,a)_{t}(s,a)}{u_{t}(s,a)}, u_{t}(s,a)=_{_{i(t)}}q^{,_{t}}(s,a), \]

\(_{t}(s,a)\) is \(1\) if \((s,a)\) is visited during episode \(t\) (so that \(_{t}(s,a)\) is revealed), and \(0\) otherwise, and \(i(t)\) denotes the epoch index to which episode \(t\) belongs. We also define \(u_{t}(s)=_{a A}u_{t}(s,a)\).

## 3 Achieving \((+C^{})\) with Known \(C^{}\)

As the first step, we develop an algorithm that achieves our goal when \(C^{}\) is known. To introduce our solution, we first briefly review the UOB-REPS algorithm of Jin et al. (2020) (designed for \(C^{}=0\)) and point out why simply using the enlarged confidence set Eq.3 when \(C^{} 0\) is far away from solving the problem. Specifically, UOB-REPS maintains a sequence of occupancy measures \(\{_{t}\}_{t=1}^{T}\) via OMD: \(_{t+1}=*{argmin}_{q(_{(t+1)})}  q,_{t}+D_{}(q,_{t})\). Here, \(>0\) is a learning rate, \(_{t}\) is the loss estimator defined in Eq.5, \(\) is the negative entropy regularizer, and \(D_{}\) is the corresponding Bregman divergence.4 With \(_{t}\) at hand, in episode \(t\), the learner simply executes \(_{t}=^{_{t}}\). Standard analysis of OMD ensures a bound on the estimated regret \(=[_{t}_{t}-q^{P,}, _{t}]\), and the rest of the analysis of Jin et al. (2020) boils down to bounding the difference between Reg and Reg\({}_{T}\).

First IssueThis difference between Reg and Reg\({}_{T}\) leads to the first issue when one tries to analyze UOB-REPS against adversarial transitions -- it contains the following bias term that measures the difference between the optimal policy's estimated loss and its true loss:

\[[_{t=1}^{T} q^{P,}, _{t}-_{t}]=[_{t=1}^{T} _{s,a}q^{P,}(s,a)_{t}(s,a)(,_{t}}(s,a )-u_{t}(s,a)}{u_{t}(s,a)})]. \]

When \(C^{}=0\), we have \(P=P_{t}\), and thus under the high probability event \(_{}\) and by the definition of upper occupancy bound, we know \(q^{P_{t},_{t}}(s,a) u_{t}(s,a)\), making Eq.6 negligible. However, this argument breaks when \(C^{} 0\) and \(P P_{t}\). In fact, \(P_{t}\) can be highly different from any transitions in \(_{i(t)}\) with respect to which \(u_{t}\) is defined, making Eq.6 potentially huge.

Solution: Change of Measure via Amortized BonusesGiven that \(q^{P,_{t}}(s,a) u_{t}(s,a)\) does still hold with high probability, Eq.6 is (approximately) bounded by

\[[_{t=1}^{T}_{s,a}q^{P,}(s,a) ,_{t}}(s,a)-q^{P,_{t}}(s,a)|}{u_{t}(s,a)}]=[_{t=1}^{T}_{s}q^{P,}(s),_{t}}(s)-q ^{P,_{t}}(s)|}{u_{t}(s)}]\]

which is at most \([_{t=1}^{T}_{s}q^{P,}(s)} _{t}}{u_{t}(s)}]\) since \(|q^{P_{t},_{t}}(s)-q^{P,_{t}}(s)|\) is bounded by the per-round corruption \(C^{}_{t}\) (see CorollaryD.3.6). While this quantity is potentially huge, if we could "change the measure" from \(q^{P,}\) to \(_{t}\), then the resulting quantity \([_{t=1}^{T}_{s}_{t}(s)} _{t}}{u_{t}(s)}]\) is at most \(|S|C^{}\) since \(_{t}(s) u_{t}(s)\) by definition. The general idea of such a change of measure has been extensively used in the online learning literature (see Luo et al. (2021) in a most related context) and can be realized by changing the loss fed to OMD from \(_{t}\) to \(_{t}-b_{t}\) for some bonus term \(b_{t}\), which, in our case, should satisfy \(b_{t}(s,a)}_{t}}{u_{t}(s)}\). However, the challenge here is that \(C^{}_{t}\) is _unknown_!

Our solution is to introduce a type of efficiently computable _amortized bonuses_ that do not change the measure per round, but do so overall. Specifically, our amortized bonus \(b_{t}\) is defined as

\[b_{t}(s,a)=(s)}&_{=1}^{t} \{_{2}u_{}(s)=_{2}u_{t}(s)\} }}{2L},\\ 0&, \]

which we also write as \(b_{t}(s)\) since it is independent of \(a\). To understand this definition, note that \(-_{2}u_{t}(s)\) is exactly the unique integer \(j\) such that \(u_{t}(s)\) falls into the bin \((2^{-j-1},2^{-j}]\). Therefore, the expression \(_{=1}^{t}\{_{2}u_{}(s)=_{2}u_{t }(s)\}\) counts, among all previous rounds \(=1,,t\), how many times we have encountered a \(u_{}(s)\) value that falls into the same bin as \(u_{t}(s)\). If this number does not exceed \(}}{2L}\), we apply a bonus of \((s)}\), which is (two times of) the maximum possible value of the unknown quantity \(}_{t}}{u_{t}(s)}\); otherwise, we do not apply any bonus. The idea is that by enlarging the bonus to it maximum value and stopping it after enough times, even though each \(b_{t}(s)\) might be quite different from \(}_{t}}{u_{t}(s)}\), overall they behave similarly after \(T\) episodes:

**Lemma 3.1**.: _The amortized bonus defined in Eq. (7) satisfies \(_{t=1}^{T}}_{t}}{u_{t}(s)}_{t=1}^{T}b_{t}(s)\) and \(_{t=1}^{T}_{t}(s)b_{t}(s)=(C^{P} T)\) for any \(s\)._

Therefore, the problematic term Eq. (6) is at most \([_{t} q^{P,},b_{t}]\), which, if "converted" to \([_{t}_{t},b_{t}]\) (change of measure), is nicely bounded by \((|S|C^{P} T)\). As mentioned, such a change of measure can be realized by feeding \(_{t}-b_{t}\) instead of \(_{t}\) to OMD, because now standard analysis of OMD ensures a bound on \(=[_{t}_{t}-q^{P,}, _{t}-b_{t}]\), which, compared to the earlier definition of Reg, leads to a difference of \([_{t}_{t}-q^{P,},b_{t}]\) (see Appendix A.5 for details).

Second IssueThe second issue comes from analyzing Reg (which exists even if no bonuses are used). Specifically, standard analysis of OMD requires bounding a "stability" term, which, for the negative entropy regularizer, is in the form of \([_{t}_{s,a}_{t}(s,a)_{t}(s,a)^{2}] =[_{t}_{s,a}_{t}(s,a),_{t}}(s,a )_{t}(s,a)^{2}}{u_{t}(s,a)^{2}}][_{t}_{s,a},_{t}}(s,a)}{u_{t}(s,a)}]\). Once again, when \(C^{}=0\) and \(P_{t}=P\), we have \(q^{P_{t},_{t}}\) bounded by \(u_{t}(s,a)\) with high probability, and thus the stability term is \((T|S||A|)\); but this breaks if \(C^{} 0\) and \(P_{t}\) can be arbitrarily different from transitions in \(_{i(t)}\).

Solution: Log-Barrier RegularizerResolving this second issue, however, is relatively straightforward -- it suffices to switch the regularizer from negative entropy to log-barrier: \((q)=-_{k=0}^{L-1}_{(s,a,s^{}) W_{k}} q(s,a,s^{})\), which is first used by Lee et al. (2020) in the context of learning adversarial MDPs but dates back to earlier work such as Foster et al. (2016) for multi-armed bandits. An important property of log-barrier is that it leads to a smaller stability term in the form of \([_{s,a}_{t}(s,a)^{2}_{t}(s,a)^{2}]\) (with an extra \(_{t}(s,a)\)), which is at most \([_{t}_{s,a}q^{P_{t},_{t}}(s,a)_{t}(s,a)^{2}]=(TL)\) since \(_{t}(s,a) u_{t}(s,a)\). In fact, this also helps control the extra stability term when bonuses are used, which is in the form of \([_{t}_{s,a}_{t}(s,a)^{2}b_{t}(s,a)^{2}]\) and is at most \(4LE[_{t}_{t},b_{t}]=(L|S|C^{} T)\) according to Lemma 3.1.

Putting these two ideas together leads to our final algorithm (see Algorithm 1). We prove the following regret bound in Appendix A, which recovers that of Jin et al. (2020) when \(C^{}=0\) and increases linearly in \(C^{}\) as desired.

**Theorem 3.2**.: _With \(=}{{T}}\) and \(=\{|A|()}{LT}},\}\), Algorithm 1 ensures_

\[_{T}=(L|S|+L|S|^{4}|A| ^{2}()+C^{}\!L|S|^{4}|A|()).\]

## 4 Achieving \((+C^{})\) with Unknown \(C^{}\)

In this section, we address the case when the amount of corruption is unknown. We develop a black-box reduction which turns an algorithm that only deals with known \(C^{}\) to one that handles unknown \(C^{}\). This is similar to Wei et al. (2022) but additionally handles adversarial losses using a different approach. A byproduct of our reduction is that we develop an entirely _black-box_ model selection approach for adversarial online learning problems, as opposed to the _gray-box_ approach developed by the "Corral" literature (Agarwal et al., 2017; Foster et al., 2020; Luo et al., 2022) which requires checking if the base algorithm is _stable_. To achieve this, we essentially develop another layer of reduction that turns any standard algorithm with sublinear regret into a stable algorithm. This result itself might be of independent interest and useful for solving other model selection problems.

More specifically, our reduction has two layers. The bottom layer is where our novelty lies: it takes as input an arbitrary corruption-robust algorithm that operates under known \(C^{}\) (e.g., the one we developed in Section 3), and outputs a _stable_ corruption-robust algorithm (formally defined later) that still operates under known \(C^{}\). The top layer, on the other hand, follows the standard Corral idea and takes as input a stable algorithm that operates under known \(C^{}\), and outputs an algorithm that operates under unknown \(C^{}\). Below, we explain these two layers of reduction in details.

Bottom Layer (from an Arbitrary Algorithm to a Stable Algorithm)The input of the bottom layer is an arbitrary corruption-robust algorithm, formally defined as:

**Definition 4.1**.: _An adversarial MDP algorithm is corruption-robust if it takes \(\) (a guess on the corruption amount) as input, and achieves the following regret for any random stopping time \(t^{} T\):_

\[_{}[_{t=1}^{t^{}}(_{t}(_{t})-_ {t}())][t^{}}+(_{2 }+_{3})\{t^{} 1\}]+[C^{}_{1:t^{ }}>]LT\]

_for problem-dependent constants and \((T)\) factors \(_{1} L^{2},_{2} L,_{3} 1\), where \(C^{}_{1:t^{}}=_{=1}^{t^{}}C^{}_{}\) is the total corruption up to time \(t^{}\)._

While the regret bound in Definition 4.1 might look cumbersome, it is in fact fairly reasonable: if the guess \(\) is not smaller than the true corruption amount, the regret should be of order \(}+\); otherwise, the regret bound is vacuous since \(LT\) is its largest possible value. The only extra requirement is that the algorithm needs to be _anytime_ (i.e., the regret bound holds for any stopping time \(t^{}\)), but even this is known to be easily achievable by using a doubling trick over a fixed-time algorithm. It is then clear that our algorithm in Section 3 (together with a doubling trick) indeed satisfies Definition 4.1.

As mentioned, the output of the bottom layer is a stable robust algorithm. To characterize stability, we follow Agarwal et al. (2017) and define a new learning protocol that abstracts the interaction between the output algorithm of the bottom layer and the master algorithm from the top layer:

**Protocol 1**.: In every round \(t\), before the learner makes a decision, a probability \(w_{t}\) is revealed to the learner. After making a decision, the learner sees the desired feedback from the environment with probability \(w_{t}\), and sees nothing with probability \(1-w_{t}\).

In such a learning protocol, Agarwal et al. (2017) defines a stable algorithm as one whose regret smoothly degrades with \(_{T}=w_{t}}\). For our purpose here, we additionally require that the dependence on \(C^{}\) in the regret bound is linear, which results in the following definition:

**Definition 4.2** (\(\)-stable corruption-robust algorithm).: _A \(\)-stable corruption-robust algorithm is one that, with prior knowledge on \(C^{}\), achieves \(_{T}[_{T}T}+_{2} _{T}]+_{3}C^{}\) under Protocol 1 for problem-dependent constants and \((T)\) factors \(_{1} L^{2},_{2} L\), and \(_{3} 1\)._

For simplicity, we only define and discuss the \(\)-stability notion here (the parameter \(\) refers to the exponent of \(T\)), but our result can be straightforwardly extended to the general \(\)-stability notion for\([,1)\) as in Agarwal et al. (2017). Our main result in this section is then that one can convert any corruption-robust algorithm into a \(\)-stable corruption-robust algorithm:

**Theorem 4.3**.: _If an algorithm is corruption robust according to Definition 4.1 for some constants \((_{1},_{2},_{3})\), then one can convert it to a \(\)-stable corruption-robust algorithm (Definition 4.2) with constants \((^{}_{1},^{}_{2},^{}_{3})\) where \(^{}_{1}=(_{1} T),\;^{}_{2}=(_{2}+_{3}L T)\), and \(^{}_{3}=(_{3} T)\)._

This conversion is achieved by a procedure that we call STABILISE (see Algorithm 2 for details). The high-level idea of STABILISE is as follows. Noticing that the challenge when learning in Protocol 1 is that \(w_{t}\) varies over time, we discretize the value of \(w_{t}\) and instantiate one instance of the input algorithm to deal with one possible discretized value, so that it is learning in Protocol 1 but with a _fixed_\(w_{t}\), making it straightforward to bound its regret based on what it promises in Definition 4.1.

More concretely, STABILISE instantiates \((_{2}T)\) instances \(\{_{j}\}_{j=0}^{_{2}T}\) of the input algorithm that satisfies Definition 4.1, each with a different parameter \(_{j}\). Upon receiving \(w_{t}\) from the environment, it dispatches round \(t\) to the \(j\)-th instance where \(j\) is such that \(w_{t}(2^{-j-1},2^{-j}]\), and uses the policy generated by \(_{j}\) to interact with the environment (if \(w_{t}\), simply ignore this round). Based on Protocol 1, the feedback for this round is received with probability \(w_{t}\). To _equalize_ the probability of \(_{j}\) receiving feedback as mentioned in the high-level idea, when the feedback is actually obtained, STABILISE sends it to \(_{j}\) only with probability \(}{w_{t}}\) (and discards it otherwise). This way, every time \(_{j}\) is assigned to a round, it always receives the desired feedback with probability \(w_{t}}{w_{t}}=2^{-j-1}\). This equalization step is the key that allows us to use the original guarantee of the base algorithm (Definition 4.1) and run it as it is, without requiring it to perform extra importance weighting steps as in Agarwal et al. (2017).

The choice of \(_{j}\) is crucial in making sure that STABILISE only has \(C^{}\) regret overhead instead of \(_{T}C^{}\). Since \(_{j}\) only receives feedback with probability \(2^{-j-1}\), the expected total corruption it experiences is on the order of \(2^{-j-1}C^{}\). Therefore, its input parameter \(_{j}\) only needs to be of this order instead of the total corruption \(C^{}\). This is similar to the key idea of Wei et al. (2022) and Lykouris et al. (2018). See Appendix B.1 for more details and the full proof of Theorem 4.3.

Top Layer (from Known \(C^{}\) to Unknown \(C^{}\))With a stable algorithm and a regret guarantee in Definition 4.2, it is relatively standard to convert it to an algorithm with \(}(+C^{})\) regret without knowing \(C^{}\). Similar arguments have been made in Foster et al. (2020), and the idea is to have another specially designed OMD/FTRL-based master algorithm to choose on the fly among a set of instances of this stable base algorithm, each with a different guess on \(C^{}\) (the probability \(w_{t}\) in Protocol 1 is then decided by this master algorithm). We defer all details to Appendix B. The final regret guarantee is the following (\(}()\) hides \((T)\) factors).

**Theorem 4.4**.: _Using an algorithm satisfying Definition 4.2 as a base algorithm, Algorithm 3 (in the appendix) ensures \(_{T}=}(T}+_{2}+ _{3}C^{})\) without knowing \(C^{}\)._

## 5 Gap-Dependent Refinements with Known \(C^{}\)

Finally, we discuss how to further improve our algorithm so that it adapts to easier environments and enjoys a better bound when the loss functions satisfy a certain gap condition, while still maintaining the \((+C^{})\) robustness guarantee. This result unfortunately requires the knowledge of \(C^{}\) because the black-box approach introduced in the last section leads to \(\) regret overhead already. We leave the possibility of removing this limitation for future work.

More concretely, following prior work such as Jin and Luo (2020), we consider the following general condition: there exists a mapping \(^{}:S A\), a gap function \(:S A(0,L]\), and a constant \(C^{} 0\), such that for any policies \(_{1},,_{T}\) generated by the learner, we have

\[[_{t=1}^{T} q^{P,_{t}}-q^{P,^{}}, _{t}][_{t=1}^{T}_{s _{L}}_{a^{}(s)}q^{P,_{t}}(s,a)(s,a)]-C^{ }. \]

It has been shown that this condition subsumes the case when the loss functions are drawn from a fixed distribution (in which case \(^{}\) is simply the optimal policy with respect to the loss mean and \(P\), \(\) is the gap function with respect to the optimal \(Q\)-function, and \(C^{}=0\)), or further corrupted by an adversary in an arbitrary manner subject to a budget of \(C^{}\); we refer the readers to Jin and Luo (2020) for detailed explanation. Our main result for this section is a novel algorithm (whose pseudocode is deferred to Appendix C due to space limit) that achieves the following best-of-both-world guarantee.

**Theorem 5.1**.: _Algorithm 4 (with \(=}{{T^{2}}}\) and \(_{t}\) defined as in Definition 5.2) ensures_

\[_{T}()=(L^{2}|S||A|( )+(C^{}+1)L^{2}|S|^{4}|A|^{2}^{2} ())\]

_always, and simultaneously the following gap-dependent bound under Condition (9):_

\[_{T}(^{})=(U+}}+ (C^{}+1)L^{2}|S|^{4}|A|^{2}^{2}()),\]

_where \(U=|S|^{2}|A|^{2}()}{_{}}}+_{s  s_{L}}_{a^{}(s)}|S||A|^{2}()}{ (s,a)}\) and \(_{}}=_{s s_{L},a^{}(s)}(s,a)\)._

Aside from having larger dependence on parameters \(L\), \(S\), and \(A\), Algorithm 4 maintains the same \((+C^{})\) regret as before, no matter how losses/transitions are generated; additionally, the \(\) part can be significantly improved to \((U+}})\) (which can be of order only \(^{2}T\) when \(C^{}\) is small) under Condition (9). This result not only generalizes that of Jin et al. (2021); Dann et al. (2023) from \(C^{}=0\) to any \(C^{}\), but in fact also improves their results by having smaller dependence on \(L\) in the definition of \(U\). In the rest of this section, we describe the main ideas of our algorithm.

FTRL with Epoch ScheduleOur algorithm follows a line of research originated from Wei and Luo (2018); Zimmert and Seldin (2019) for multi-armed bandits and uses FTRL (instead of OMD) together with a certain self-bounding analysis technique. Since FTRL does not deal with varying decision sets easily, similar to Jin et al. (2021), we restart FTRL from scratch at the beginning of each epoch \(i\) (recall the epoch schedule described in Section 2). More specifically, in an episode \(t\) that belongs to epoch \(i\), we now compute \(_{t}\) as \(*{argmin}_{q} q,_{=t_{i}}^{t-1}(_{}-b_{})+_{t}(q)\), where \(t_{i}\) is the first episode of epoch \(i\), \(_{t}\) is the same loss estimator defined in Eq. (5), \(b_{t}\) is the amortized bonus defined in Eq. (7) (except that \(=1\) there is also changed to \(=t_{i}\) due to restarting), \(_{t}\) is a time-varying regularizer to be specified later, and the set that \(q\) is optimized over is also a key element to be discussed next. As before, the learner then simply executes \(_{t}=^{_{t}}\) for this episode.

Optimistic TransitionAn important idea from Jin et al. (2021) is that if FTRL optimizes \(q\) over \((_{i})\) (occupancy measures with respect to a fixed transition \(_{i}\)) instead of \((_{i})\) (occupancy measures with respect to a set of plausible transitions) as in UOB-REPS, then a critical _loss-shifting_ technique can be applied in the analysis. However, the algorithm lacks "optimism" when not using a confidence set, which motivates Jin et al. (2021) to instead incorporate optimism by subtractinga bonus term Bonus from the loss estimator (not to be confused with the amortized bonus \(b_{t}\) we propose in this work). Indeed, if we define the value function \(V^{,}(s;)\) as the expected loss one suffers when starting from \(s\) and following \(\) in an MDP with transition \(\) and loss \(\), then they show that the Bonus term is such that \(V^{_{i},}(s;-) V^{P,}(s;)\) for any state \(s\) and any loss function \(\), that is, the performance of any policy is never underestimated.

Instead of following the same idea, here, we propose a simpler and better way to incorporate optimism via what we call _optimistic transitions_. Specifically, for each epoch \(i\), we simply define an optimistic transition function \(_{i}\) such that \(_{i}(s^{}|s,a)=\{0,_{i}(s^{}|s,a)-B_{i }(s,a,s^{})\}\) (recall the confidence interval \(B_{i}\) defined in Eq.4). Since this makes \(_{s^{}}_{i}(s^{}|s,a)\) less than \(1\), we allocate all the remaining probability to the terminal state \(s_{L}\) (which breaks the layer structure but does not really affect anything). This is a form of optimism because reaching the terminate state earlier can only lead to smaller loss. More formally, under the high probability event \(_{}\), we prove \(V^{_{i},}(s;) V^{P,}(s;)\) for any policy \(\), any state \(s\), and any loss function \(\) (see LemmaC.8.3).

With such an optimistic transition, we simply perform FTRL over \((_{i})\) without adding any additional bonus term (other than \(b_{t}\)), making both the algorithm and the analysis much simpler than Jin et al. (2021). Moreover, it can also be shown that \(V^{_{i},}(s;-) V^{_{i},}(s;)\) (see LemmaC.8.4), meaning that while both loss estimation schemes are optimistic, ours is tighter than that of Jin et al. (2021). This eventually leads to the aforementioned improvement in the \(U\) definition.

Time-Varying Log-Barrier RegularizersThe final element to be specified in our algorithm is the time-varying regularizer \(_{t}\). Recall from discussions in Section3 that using log-barrier as the regularizer is critical for bounding some stability terms in the presence of adversarial transitions. We thus consider the following log-barrier regularizer with an adaptive learning rate \(_{t}:S A_{+}\): \(_{t}(q)=-_{s s_{L}}_{a A}_{t}(s,a) q(s,a)\). The learning rate design requires combining the loss-shifting idea of Jin et al. (2021) and the idea from Ito (2021), the latter of which is the first work to show that with adaptive learning rate tuning, the log-barrier regularizer leads to near-optimal best-of-both-world guarantee for multi-armed bandits.

More specifically, following the same loss-shifting argument of Jin et al. (2021), we first observe that our FTRL update can be equivalently written as

\[_{t}= *{argmin}_{q(_{i})} q,_{=t_{i}}^{t-1}(_{}-b_{})+_{t }(q)=*{argmin}_{x(_{i})} q,_{ =t_{i}}^{t-1}(g_{}-b_{})+_{t}(q),\]

where \(g_{}(s,a)=Q^{_{i},_{}}(s,a;_{})-V^{ _{i},_{}}(s;_{})\) for any state-action pair \((s,a)\) (\(Q\) is the standard \(Q\)-function; see AppendixC for formal definition). With this perspective, we follow the idea of Ito (2021) and propose the following learning rate schedule:

**Definition 5.2**.: _(Adaptive learning rate for log-barrier) For any \(t\), if it is the starting episode of an epoch, we set \(_{t}(s,a)=256L^{2}|S|\); otherwise, we set \(_{t+1}(s,a)=_{t}(s,a)+(s,a)}{2_{t}(s,a)}\) where \(D=}{{(t)}}\), \(_{t}(s,a)=q^{_{t(t)},_{t}}(s,a)^{2}(Q^{_{t(t)}, _{t}}(s,a;_{t})-V^{_{t(t)},_{t}}(s;_{t}))^{2}\), and \(i(t)\) is the epoch index to which episode \(t\) belongs._

Such a learning rate schedule is critical for the analysis in obtaining a certain self-bounding quantity and eventually deriving the gap-dependent bound. This concludes the design of our algorithm; see AppendixC for more details.

## 6 Conclusions

In this work, we propose online RL algorithms that can handle both adversarial losses and adversarial transitions, with regret gracefully degrading in the degree of maliciousness of the adversary. Specifically, we achieve \(}(+C^{})\) regret where \(C^{}\) measures how adversarial the transition functions are, even when \(C^{}\) is unknown. Moreover, we show that further refinements of the algorithm not only maintain the same regret bound, but also simultaneously adapt to easier environments, with the caveat that \(C^{}\) must be known ahead of time. We leave how to further remove this restriction as a key future direction.