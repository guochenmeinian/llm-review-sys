# A Probability Contrastive Learning Framework for 3D Molecular Representation Learning

Jiayu Qin

University at Buffalo

jiayuqin@buffalo.edu

&Jian Chen

University at Buffalo

jchen378@buffalo.edu

&Rohan Sharma

University at Buffalo

rohanjag@buffalo.edu

&Jingchen Sun

University at Buffalo

jsun39@buffalo.edu

&Changyou Chen

University at Buffalo

changyou@buffalo.edu

###### Abstract

Contrastive Learning (CL) plays a crucial role in molecular representation learning, enabling unsupervised learning from large scale unlabeled molecule datasets. It has inspired various applications in molecular property prediction and drug design. However, existing molecular representation learning methods often introduce potential false positive and false negative pairs through conventional graph augmentations like node masking and subgraph removal. The issue can lead to suboptimal performance when applying standard contrastive learning techniques to molecular datasets. To address the issue of false positive and negative pairs in molecular representation learning, we propose a novel probability-based contrastive learning (CL) framework. Unlike conventional methods, our approach introduces a learnable weight distribution via Bayesian modeling to automatically identify and mitigate false positive and negative pairs. This method is particularly effective because it dynamically adjusts to the data, improving the accuracy of the learned representations. Our model is learned by a stochastic expectation-maximization process, which optimizes the model by iteratively refining the probability estimates of sample weights and updating the model parameters. Experimental results indicate that our method outperforms existing approaches in 13 out of 15 molecular property prediction benchmarks in MoleculeNet dataset and 8 out of 12 benchmarks in the QM9 benchmark, achieving new state-of-the-art results on average.

## 1 Introduction

We investigate the problem of learning representations from molecules, a field known as molecular representation learning (MRL). MRL has gained significant attention due to its critical role in enabling learning from limited supervised data for applications such as molecular property prediction and drug design . Molecular representation learning involves creating models that can derive meaningful and generalizable representations of molecules, which can then be used to enhance various downstream applications. Among the most common methods in MRL is contrastive learning (CL), which leverages large-scale unlabeled molecular datasets to learn robust representations. CL works by contrasting different augmentations of the same molecule to ensure that the model learns to recognize the essential features of the molecule, thereby improving performance on tasks such as molecular property prediction and drug design.

With the success of contrastive learning methods in computer vision and multi-modality pretraining , various contrastive learning approaches have been proposed for molecular representation learning. MolCLR introduces a contrastive learning framework specifically for molecular representationlearning. It employs atom masking and edge removal as data augmentations, which enhances the performance of Graph Neural Network (GNN) models on a variety of downstream molecular property prediction benchmarks. In contrast, GraphMVP incorporates both 2D topology and 3D geometry during pre-training, though its downstream tasks primarily utilize 2D topology. These methods highlight different strategies for applying contrastive learning to molecular data, focusing on unique aspects of molecular structures to improve learning efficacy.

Although existing works have demonstrated the success of contrastive learning in molecular property predictions, they still face a significant drawback: the reliability of "positive" and "negative" labels in augmented molecule pairs. For example, MolCLR uses augmentations like atom masking and edge removal, which can lead to false negative pairs when molecules with similar structures and chemical properties are labeled as negatives. Similarly, GraphMVP , which incorporates both 2D topology and 3D geometry, can also mislabel structurally similar augmented molecules as negatives due to its augmentation processes. These augmentations often remove parts of the molecular graph, such as nodes, edges, and subgraphs, resulting in potentially incorrect pairings. This issue is exacerbated by the large volume and extensive augmentations applied to molecular datasets, naturally leading to numerous falsely aligned pairs.

The fundamental problem lies in the random nature of these augmentations. Existing molecular contrastive learning methods assign hard positive and negatives to molecule pairs and do not account for the probabilistic relationships between molecules. Figure 3 provides an example of false positives and negatives resulting from graph augmentations in MolCL,where two distinct graph augmentations are applied to enhance two different molecules. The augmented molecule pair originating from the same molecule is categorized as positive, while other molecule pairs within the same batch are considered negative. However, as illustrated in the figure, the correct contrastive learning setup should consider molecules with structural similarities as positive pairs, even when they originates from different molecules. In contrast, the same molecule subjected to different augmentation methods may also be considered negative due to structural dissimilarities. Existing methods like MolCLR  fail to maintain this distinction, where augmented pairs from the same molecule are always treated as positive, while pairs from different molecules within the same batch are always treated as negative, regardless of their structural similarity. This mislabeling results in false positives and negatives, undermining the effectiveness of the contrastive learning process.

To overcome the aforementioned issue, we introduce a generalization of existing contrastive learning frameworks for molecular representation learning with probabilistic modeling. Our approach introduces data-pair weights as additional random variables, and dynamically infers optimal weights to account for false positive and false negative pairs, which can effectively address the mislabeling problem in previous methods. By incorporating a probability framework, we can effectively manage the uncertainty in data pair assignments. Specifically, we introduce a novel Bayesian inference

Figure 1: **Existing problem in molecular contrastive learning.** Adopt node removal and edge removal for molecular contrastive learning can lead to false positive and false negative problems. Blue lines indicate positive pairs and yellowing lines indicate negative pairs. The numbers on each line indicate the chemical similarity between the augmented pair of molecules. In this case, positive pairs indeed have lower similarity than negative pairs.

methods with Bayesian data augmentation to automatically infer these weights through posterior sampling. This allows us to optimize the model parameters efficiently using stochastic expectation maximization.

It is worth mentioning that while MolCLR authors introduced i-MolCLR to address similar issues by penalizing faulty negatives with a fingerprint-based similarity metric and a motif-level data augmentation called fragment contrast, our method offers distinct advantages. Unlike i-MolCLR which relies on direct fingerprint similarity, our approach introduces a novel probabilistic contrastive learning framework. This framework dynamically infers weight distributions and optimizes through stochastic expectation maximization, eliminating the need for explicit Tanimoto similarity calculations. Our method addresses the issue of false negative pairs more fundamentally and efficiently, providing a more robust solution for molecular contrastive learning.

In addition, our method is flexible and can be applied to different molecular representation learning framework. In this paper, we first integrate our method into MolCLR  series model and benchmark the performance on 2D non-charality MoleculeNet dataset. We then integrated our method into Uni-Mol and evaluate its performance on MoleculeNet. We also trained and evaluated our model on the QM9  dataset, following Equiformer . With molecular property prediction tasks, we aim to test our model's ability in extracting useful features from molecular. Extensive experiments show that our method outperforms all other molecular representation learning baselines, including contrastive and non-contrastive methods.

The contributions of this paper can be summarized as follows:

* To tackle the challenges posed by false positive and negative pairs, we introduce a probability method for molecular contrastive learning. By introducing different weights as random variables to various false positive and negative pairs, we effectively mitigate the impact of these erroneous pairs on the learning process.
* To optimize our probabilistic contrastive learning framework, we propose a novel and effective optimization algorithm based on Bayesian data augmentation and stochastic expectation maximization, to simultaneously perform posterior inference and model optimization.
* Through extensive and large-scale experiments, we demonstrate enhanced performance across multiple public benchmarks for molecular representation learning, validating the effectiveness of our proposed method.

## 2 Methods

### Learning Representations from Molecular Graphs

We begin by elucidating the foundational setup and notation in molecular contrastive learning. Molecules can be represented as 2D or 3D graphs depending on datasets. 2D molecule graphs have atoms as nodes and bond as edges. 3D molecule graphs additionally adds spacial positions of the atoms. For simplicity, we adopt static atom positions in this paper.

In molecular representation learning, as illustrated in Figure 2, we start by randomly sampling a batch of \(N\) molecules. Each molecule, represented as \(_{i}\), undergoes stochastic augmentation strategies to generate two augmented versions, denoted as \((_{i},}_{i})\). These augmentations involve methods such as atom masking, edge perturbation, and subgraph removal, transforming the original molecular structure while preserving its core characteristics. Among the resulting \(2N\) augmented molecules, each pair \((_{i},}_{i})\) is treated as a positive pair, while the remaining \(2(N-1)\) augmented molecules within the same batch are considered negative samples. This setup allows us to utilize contrastive learning effectively by distinguishing between similar and dissimilar molecular structures. A neural network encoder \(f(;)\), parameterized by \(\), is employed to extract representation vectors \(z\) from the augmented molecular samples. In this paper, we utilize three different types of encoders in various experiments, as depicted in Figure 2 B, C, and D. These encoders include Graph Neural Networks (GNNs) and Transformers, each providing unique advantages for capturing the intricate features of molecular structures.

Let \(s_{i^{+}}(_{i},}_{i})\) represent the similarity score between the positive pair \((_{i},}_{i})\) after the encoder, and \(s_{ik^{-}}(_{i},_{k})\) signifies the similarity score between the negative pair \((_{i},_{k})\), and \((,)\) represents any positive-valued similarity metric. In this paper, we adopt the commonly used exponential cosine similarity, defined as \((_{1},_{2}) e^{_{1}^{T}_{2} /\|_{1}\|\|_{2}\|}\), where \(\) denotes a temperature parameter.

### Probability Weighted Contrastive Learning

We describe the proposed probability framework for molecular contrastive learning. In standard contrastive learning, one tries to encode data samples to a latent space such that positive pairs stay close to each other while negative pairs are pushed away. The contrastive loss function is:

\[=_{k=1}^{N}[(2k-1,2k)+(2k,2k-1)],(i,j)=-}}{s_{i^{+}}+_{k=1}^{2N}_{[k i,j]}s_ {i,k^{-}}}\] (1)

As mentioned, one issue of directly applying the contrastive learning into molecular representation learning is the potential false positive and negative molecular pairs, as discussed in the introduction. This could confuse the learning, ending up with sub-optimal representations. Is there a way to automatically identify and differentiate these pair data? In the following, we propose a Bayesian approach to address this issue that allows the algorithm for automatic inference of the degree of positiveness and negativeness of data pairs, involving enhancing the standard contrastive loss by incorporating learnable stochastic weights for all data pairs. To be more specific, we introduce local learnable weights, denoted as \(w_{i}^{+}\) for each positive pair and \(w_{ik}^{-}\) for each negative pair. We then define a weighted contrastive loss based on these introduced weights. This modification aims to mitigate the issues by automatically assigning relatively lower weights (or no weights) to false positive and false negative pairs;

\[_{w}=_{k=1}^{N}[(2k-1,2k)+(2k,2 k-1)],\ \ (i,j)=-^{+}s_{i^{+}}}{w_{i}^{+}s_{i^{+}}+_{k=1}^{2N }_{[k i,j]}w_{ik}^{-}s_{ik^{-}}}\] (2)

One problem with this formulation, however, is that it is not realistic to compute and store all the weights in the learning process. This precaution arises from the quadratic growth in the number of

Figure 2: (A) **Molecular contrastive learning** Molecules are represented as 2D or 3D molecule graphs. Two stochastic augmentation strategies are applied to each graph, resulting in two augmentations. A feature extractor is used to extract features and contrastive loss is used to maximize the similarity of positive pairs and minimize the similarity of negative pairs B,C,D: Different architectures used as feature extractors in different experiments. (B) Uni-Mol  architecture used in MoleculeNet  Dataset experiment. (C) GCN  architecture from MolCLR  used in Non-Chirality MoleculeNet  experiment. (D) Equiformer  architecture used in QM9  dataset experiment.

weights to be calculated as the training data size increases. Furthermore, the random nature of our augmentation method further adds complexity to the pre-calculation and storage of these weights.

A straightforward baseline for calculating these weights can be envisioned as follows: we can consider these weights in a binary fashion, with all weights initialized to one. In the learning process, if for some positive pairs the similarity score falls below a specified threshold, we set the corresponding weights to zero, marking these positive pairs as false positives. Conversely, if for some negative pairs the similarity score exceeds a threshold, we set the associated weights to zero, indicating false negatives. A challenge associated with this baseline method, however, lies in the establishment of a rigid similarity threshold to create a binary division of weights between zero and one. This approach proves less suitable for our molecular contrastive task as these heuristically chosen thresholds might not be optimal.

To address this challenge, we propose a principled Bayesian approach that allows adaptively inferring the optimal weights by Bayesian inference. Specifically, we treat the weights to be random variables and assign appropriate priors to them. We consider two types of priors: a Bernoulli prior to model weights as binary random variables and a Gamma prior to represent them as positive values. For simplicity, we model positive weights using the Gamma distribution and negative weights using either the Gamma distribution or the Bernoulli distribution, as expressed by the following formulas:

Option 1 - Gamma priors for continuous weighting:

\[w_{i}^{+}(a_{+},b_{+}),w_{ik}^{-}(a_{-}, b_{-}).\]

Option 2 - Bernoulli priors for selective weighting:

\[w_{i}^{+}(a_{+},b_{+}), w_{ik}^{-}(}).\]

here, \(a_{+}\), \(b_{+}\), \(a_{-}\) and \(b_{-}\)are shape and rate parameters for Gamma distribution and \(}\) is the probability parameter for Bernoulli distribution.

With our reformulation, we can define a joint distribution over the global model parameter and local random weight variables \(w_{i}^{+}\)and \(w_{ik}^{-}\), as:

\[p(\{w_{i}^{+}\},\{w_{ik}^{-}\}, ;)_{_{i}}^{+ }s_{i^{+}}}{w_{i}^{+}s_{ij^{+}}+_{k=1}^{K}w_{ik}^{-}s_{ik^{-}}}p(\{w_{i}^ {+}\})p(\{w_{ik}^{-}\})p().\] (3)

One problem with the above formulation, however, is that posterior inference of the weights is challenging, due to the lack of convenience posterior distributions.

Fortunately, inspired by , we can introduce an augmented random variable \(u_{i}\) that is associated to data point \(_{i}\). Consequently, we can define an augmented joint posterior distribution of the random variables \(,,\), denoted as \(p(\{w_{i}^{+}\},\{w_{ik}^{-}\}, )\)1, to be

\[p(,,)_{i: _{i}}w_{i}^{+}s_{i}+e^{-_{i}w_{i}^{+}s_{i}+ }_{k}e^{-u_{i}w_{ik}^{-}s_{ik}-}p(\{w_{i}^{+}\})p (\{w_{ik}^{-}\})p(),\] (4)

where \(\{u_{1},u_{2},,u_{ }\}\) and \(\{w_{i}^{+}\}\{w_{ik}^{-}\}\). It is worth noting that this joint distribution is equivalent to the original distribution (3), because (3) is recovered if one marginalize out the auxiliary random variables \(\) in (4). In other words, optimization thought (4) is equivalent to optimization over (3). Consequently, we can perform learning and inference based on the augmented posterior of \(p(,,)\), which preserves a much convenient form for posterior inference. In the following, we propose an efficient algorithm based on stochastic expectation maximization (stochastic EM) to alternatively infer the local random variables \(\) and optimize the global model parameter \(\).

### Efficient Inference and Learning with Stocastic Expectation Maximization

We propose a stochastic EM algorithm for efficient inference and learning of our model. Stochastic EM  is a stochastic variant of the EM algorithm, which is an iterative method for finding the maximum likelihood of model parameters in statistical models when data is only partially, or when model depends on unobserved latent variables .

In our setting, the objective of stocastic EM is to maximize the posterior in equation 4. The basic idea is to alternatively 1) optimizing model parameter \(\) with fixed \((,)\) and 2) sampling \((,)\) with fixed \(\). To this end, we follow standard procedures in stochastic EM to divide the learning into three steps: Simulation, Stochastic Expectation, and Maximization. Specifically, simulation corresponds to sampling local random variables \(\) and \(\) for a batch of data; stochastic expectation then uses the sampled auxiliary random variables to update the model parameter \(\) by maximizing a stochastic objective \(Q()\), defined as: \(Q_{t+1}()=Q_{t}()+_{t}( p(, ,)-Q_{t}())\) at iteration \(t+1\), where \(\{_{t}\}\) is a sequence of decreasing weights. And maximization corresponds to maximizing the stochastic objective constructed in the previous step. In the following, we detail the three steps.

**Simulation** Given the joint posterior distribution in equation 3 and the current batch of data, the posterior distributions of the local random variables \(\) and \(\) can be directly read out, which simply follow Gamma or Bornoulli distributions of the following forms:

\[u_{i}\{w_{i}^{+},w_{ik}^{-},\} (a_{u},b_{u}+w_{i}^{+}s_{i^{+}}+ w_{ik}^{-}s_{ik ^{-}}), i,\] (5) \[w_{i}^{+}\{,\} (1+a_{+},u_{i}s_{i+}+b_{+}),\] \[w_{ik}^{-}\{,\} (a_{-},u_{i}s_{ik^{-}}+b_{-}), i,k\] \[w_{ik}^{-}\{,\} (e^{-u_{i}s_{ik^{-}}}}{1-a_{-}+a _{-}e^{-u_{i}s_{ik^{-}}}})\]

We use the Gamma prior because it naturally lends itself to conjugacy in the posterior, which significantly eases the posterior sampling procedure. Also, it is known for its flexibility in shape and scale to model positive continuous variables, which is suitable for sample weights in our setting.

**Stochastic Expectation** We then proceed to calculate the stochastic expectation based on the simulated local random variables above. For notation simplicity, we define \(Q_{0}()=0\). Then we can reformulate \(Q_{t+1}()\) by decomposing the recursion, resulting in

\[Q_{t+1}()=_{=0}^{t}_{} p(,_{},_{}_{}), { where }_{}_{}_{t^{}=+1}^{t} (1-_{t^{}}),\] (6)

where \(\) indexes the minibatch and the corresponding local random variables at the current time \(\).

**Maximization** The stochastic expectation objective (6) provides a convenient form for stochastic optimization over time, similar to online optimization (Bent & Van Hentenryck, 2005). Specifically, at each time \(t\), we can initialize the parameter \(\) from the last step, and update it by stochastic gradient ascent on the log-likelihood, \( p(,_{},_{}_{ })\) calculated from the current batch of data. To reduce variance, we propose to optimize a marginal version by integrating out \(_{}\) from \(p(,_{},_{}_{})\), which essentially reduces to our original weighted contrastive loss in equation (2). With the above steps, it is ready to optimize the model by stochastic EM. The detailed steps are described in the Algorithm 1.

```
1:Initialize \(\); set \(t=1\)
2:for a batch of molecules in loader do
3: Augment each molecule \(_{i}\) into a pair \((_{i},}_{i})\)
4: Calculate positive/negative similarity scores \(s^{+}\) and \(s^{-}\) for all the molecule pairs
5: Initialize all the weights \(w^{+}\) and \(w^{-}\) to be one
6:for\(k=1\) to iter [4 in practice] do
7: Sample u and w according to distributions
8:endfor
9: Calculate the weighted contrastive loss in equation 2 with the sampled w on the current batch of data
10: Update the model parameter by stochastic gradient descent with the calculated weighted contrastive loss
11:\(t=t+1\)
12:endfor ```

**Algorithm 1** Contrastive Learning with Stochastic EM

**Contrastive Learning** As a popular self-supervised learning paradigm, contrastive learning focuses on learning semantically informative representations for downstream tasks [16; 3; 39; 9]. The most widely used loss function is InfoNCE  which pulls in the representations between positive sample pairs while pushing away that between negative sample pairs.

**Molecular Representation Learning** Representation learning on large-scale unlabeled molecules attracts much attention recently. SMILES-BERT  is pretrained on SMILES strings of molecules using BERT. Subsequent works are mostly pretraining on 2D molecular topological graphs [15; 29]. MolCLR  applies data augmentation to molecular graphs at both node and graph levels, using a self-supervised contrastive learning strategy to learn molecular representations. I-MoCLR  is a improved version of MolCLR that uses new data augmentation and introduces weighted contrastive learning for mitigating false pair problem. Further, several recent works try to leverage the 3D spatial information of molecules, and focus on contrastive or transfer learning between 2D topology and 3D geometry of molecules. For example, GraphMVP  proposes a contrastive learning GNN-based framework between 2D topology and 3D geometry. GEM  uses bond angles and bond length as additional edge attributes to enhance 3D information. Uni-Mol is a universal 3D molecular pretraining framework that significantly enlarges the representation ability and application scope in drug design.

**Noisy Pairs in Contrastive Learning** Noisy data pair problem have been found and studied in contrastive learning community. NLIP  enforces pairs with larger noise to be less similar in embedding space to improve model training. apply noise estimation component to adjust the consistency between different modalities for action recognition task. RINCE  uses a ranked ordering of positive samples to improve InfoNCE loss.  introduces a new debiased contrastive learning loss function by transforming the distribution of negative samples. Matchdrop  designed a new graph augmentation method to alleviate the false positive sampling problem by retaining the most critical parts of the graph and augmenting the unimportant parts.

**Stochastic Expectation Maximization** Stochastic EM  stands as a pivotal algorithm in machine learning and probabilistic modeling for large-scale Bayesian inference. Building upon the foundations of the classical Expectation-Maximization (EM) algorithm , Stochastic EM offers an efficient solution for parameter estimation in situations involving vast datasets or latent variables, e.g., to maximize the log-likelihood of \(p(,)\), where \(\) is the dataset, \(\) is the local random variable and \(\) is the global model parameter. By leveraging the power of mini-batch sampling, Stochastic EM strikes a balance between computational scalability and estimation accuracy. It has found widespread utility in various domains, including clustering , topic modeling , and latent variable modeling , making it an indispensable tool to cope with complex probabilistic models and extensive data and a natural fit to our problem.

## 4 Experiments

We evaluate our method on molecular property prediction tasks. Our approach is designed to be a versatile component that can be seamlessly integrated with various molecular property prediction datasets and models. In this study, we integrate our model into three different existing models: Uni-Mol], I-MolCLR , Equformer  and assess its performance on three distinct datasets: MoleculeNet , MoleculeNet without chirality, and the QM9  dataset. For all experiments, we provide detailed experiment settings in Appendix C.

### The MoleculeNet Dataset

MoleculeNet  is a popular benchmark for molecular property prediction, including datasets focusing on different molecular properties, from quantum mechanics and physical chemistry to biophysics and physiology. For a fair comparison, we integrated our method into Uni-Mol framework. We applied both the _Gamma_ and _Bernoulli_ versions of our method, as shown in Table 1. In our contrastive learning framework, we used the representation of the [CLS] token as the final encoded representation, representing the entire molecule. Additionally, we incorporated the original three-dimensional recovery loss as an extra loss function. The model was trained on the same large-scale dataset, including 19 million molecules and 209 million conformations, as in the original paper. We used the same evaluation metrics: \(ROC\_AUC\) for classification tasks and RMSE and MAE for regression tasks.

As shown in Table 1 and 2, our method outperforms Uni-Mol and GEM , the current state-of-the-art methods, with an average gain of 1.3 percent in classification tasks and 7.6 percent in regression tasks. This substantiates that our approach facilitates more flexible training with a higher tolerance for false positive and false negative data pairs, thereby enhancing the model's performance in molecular representation learning.

### Non-Chirality version MoleculeNet

In order to make a fair comparison with I-MolCLR , we also integrated our method into MolCLR  framework. MolCLR and I-MolCLR are 2D based methods, their experiments are conducted on different version of MoleculeNet dataset that does not consider chirality. We adopted the same dataset, augmentation, GNN-based encoder and other settings. As shown in Table 3, our method outperforms I-MolCLR on 7 out of 9 downstream tasks and got an average of 2 points increase on non-chirality MoleculeNet classification datasets.

### QM9 Dataset

The QM9 dataset  is another popular dataset in molecular property prediction, it consists of 134k small molecules, and the goal is to predict their quantum properties. For this dataset, we choose equiformer  as a baseline method. The data partition we use has 110k,10k,and 11k molecules in training, validation and testing sets. We use both our contrastive loss function and original minimize mean absolute error(MAE) as training objectives.

As shown in 4, we get state of the art result in 8 out of 12 baselines. The increase is relatively subtle compared with other dataset, we argue that this is due to the fact that QM9 is relatively small

  Datasets & BBBP & BACE & ClinTox & Tox21 & ToxCast & SIDER & HIV & PCBA & MUV \\ \# Molecules & 2039 & 1513 & 1478 & 7831 & 8575 & 1427 & 41127 & 437929 & 93078 \\ \# Tasks & 1 & 1 & 2 & 12 & 617 & 27 & 1 & 128 & 17 \\  D-MPNN  & \(71.0\) & \(80.9\) & \(90.6\) & \(75.9\) & \(65.5\) & \(57.0\) & \(77.1\) & \(86.2\) & \(78.6\) \\ Attentive FP  & \(64.3\) & \(78.4\) & \(84.7\) & \(76.1\) & \(63.7\) & \(60.6\) & \(75.7\) & \(80.1\) & \(76.6\) \\ N-Gram\({}_{RF}\) & \(69.7\) & \(77.9\) & \(77.5\) & \(74.3\) & – & \(66.8\) & \(77.2\) & – & \(76.9\) \\ N-Gram\({}_{XGB}\) & \(69.1\) & \(79.1\) & \(87.5\) & \(75.8\) & – & \(65.5\) & \(78.7\) & – & \(74.8\) \\ PretrainGNN  & \(68.7\) & \(84.5\) & \(72.6\) & \(78.1\) & \(65.7\) & \(62.7\) & \(79.9\) & \(86.0\) & \(81.3\) \\ GraphMWP  & \(72.4\) & \(81.2\) & \(79.1\) & \(75.9\) & \(63.1\) & \(63.9\) & \(77.0\) & – & \(77.7\) \\ GEM  & \(72.4\) & \(85.6\) & \(90.1\) & \(78.1\) & \(69.2\) & **67.2** & \(80.6\) & \(86.6\) & \(81.7\) \\ MolCLR  & \(72.2\) & \(82.4\) & \(91.2\) & \(75.0\) & – & 58.9 & 78.1 & – & 79.6 \\ Uni-Mol & \(72.9\) & \(85.7\) & **91.9** & \(79.6\) & \(69.6\) & \(65.9\) & \(80.8\) & **88.5** & 82.1 \\  Ours (Gamma) & **76.7** & **88.2** & 89.4 & **80.1** & **69.9** & 63.6 & **83.0** & **89.6** & 79.0 \\ Ours (Bernoulli) & \(73.7\) & \(84.3\) & \(85.3\) & \(79.8\) & 68.8 & 64.9 & 80.8 & 89.3 & **82.9** \\  

Table 1: ROC_AUC on molecular property prediction classification tasks (Higher is better)

  Datasets & ESOL & FreeSolv & Lipo & QM7 & QM8 & QM9 & MEAN (RMSE) & MEAN (MAE) \\ \# Molecules & 1128 & 642 & 4200 & 6830 & 21786 & 133885 & & \\ \# Metric &  &  &  &  \\  D-MPNN  & 1.050 & 2.082 & 0.683 & 103.5 & 0.0190 & 0.00814 & 1.272 & 34.509 \\ GROVERIarge  & 0.895 & 2.272 & 0.823 & 92.0 & 0.0224 & 0.00986 & 1.33 & 30.67 \\ MolCLR  & 1.271 & 2.594 & 0.691 & 66.8 & 0.0178 & - & 1.519 & - \\ GraphMVP  & 1.029 & - & 0.681 & - & - & - & - & - \\ GEM  & 0.798 & 1.877 & 0.660 & 58.9 & 0.0171 & 0.00746 & 1.112 & 19.642 \\ Uni-Mol & 0.788 & 1.480 & 0.603 & 41.8 & 0.0156 & 0.00467 & 0.957 & 13.940 \\  Ours (Gamma) & 0.775 & 1.420 & **0.590** & **38.5** & **0.0142** & **0.00395** & 0.928 & **12.839** \\ Ours (Bernoulli) & **0.664** & **1.358** & 0.626 & 55.6 & 0.0154 & 0.0056 & **0.883** & 18.541 \\  

Table 2: Performance on molecular property prediction regression tasks (Lower is better)

  Without Chirality & BBBP & BACE & ClinTox & Tox21 & SIDER & HIV & MUV & MEAN \\  I-MOLCLR  & 76.4 & 88.5 & **95.4** & 79.9 & 69.9 & \(80.8\) & **90.8** & 83.1 \\ Our Method & **78.3** & **94.8** & 91.4 & **84.9** & **72.7** & **85.5** & \(88.0\) & **85.1** \\  

Table 3: Comparison against i-MolCLR on non-chirality MoleculeNet datasetregarding number of molecules in training set, and also the saturation on performance achieved by different methods.

### Ablation Study

Distribution of similarity scoresOur method is largely motivated by the observation that previous MCL approaches neglect potential semantic dissimilarity between positive samples and that accounting for this phenomenon can improve learned molecule representations. In Figure A(See Appendix A), we plot the distribution of similarity scores for both positive and negative samples. Figure A left reveals that our method yields larger similarity scores with lower variance for positive pairs compared to MolCLR  baseline which uses standard contrastive learning method. Figure A right reveals that our method also mitigates the false negative problem in standard CL. It also shows that our method sometimes assigns lower similarity scores to positive pairs. While it may seem counter intuitive to assign lower similarity scores to positive samples, we argue that doing so is the very reason our method captures dissimilarity between positive pairs. By allowing some degree of alignment between the right set of negative examples, our method is able to minimize the inconsistencies between shared context of related positives and negatives. This in turn allows us to learn an overall more coherent representation space, resulting in increased robustness and downstream performance.

Comparisons with the Standard Contrastive LearningWe conducted an ablation study to showcase that our method of probabilistic framework of contrastive learning has already achieved strong emperical results and demonstrate the improvement brought by adding the 3D-aware loss functions on MoleculeNet  classification dataset. We first examined the effect of adding the probabilistic framework to the standard contrastive loss, and the 3D-aware loss functions as implemented in Uni-Mol.

Table 5 presents the results of our ablation study. Incorporating the probabilistic framework resulted in a great improvement of 3.4-point increase in ROC-AUC, significantly enhances the model's performance. On the other hand, introducing the additional loss component led to an increase in ROC-AUC by 2.9 points, demonstrating its secondary role in enhancing the model's performance. When we adopt both of them, we can get the final ROC-AUC of 80.1 average on MoleculeNet classification datasets.

HyperparametersWe also conducted an ablation study to determine the optimal hyperparameters (e.g., \(a_{+}\), \(a_{-}\)) on MoleculeNet classification datasets. We selected \(a_{+}\), \(a_{-}\), \(b_{+}\), and \(b_{-}\) from the range \(\). Table 6 indicates that our method achieves the best performance with \(a_{+}=5\) and \(a_{-}=b_{+}=b_{-}=1\). Tuning different hyperparameters affects performance, with an increase in \(a_{+}\) from 1 to 5 leading to a 1.6 percent performance gain.

## 5 Conclusion

In this paper, we investigate an important yet unnoticed limitation of molecular contrastive learning, where augmented graph data come with false positive and false negative data pairs. As a remedy, we propose a principled solution to molecular contrastive learning by reformulating it into a probability

  Methods & \(\) & \(\)E & E\(\_\)homo & E\(\_\)lamo & \(\) & Cv & Q & H & R\({}^{}\)2 & \(\) & \(\)0 & \(}\) \\  GraphCL  & 0.066 & 45.5 & 26.8 & 22.9 & 0.027 & 0.028 & 10.2 & 9.6 & 0.095 & 9.7 & 9.6 & 1.42 \\ JOAOv2  & 0.066 & 45.0 & 27.8 & 22.2 & 0.027 & 0.028 & 9.9 & 9.2 & 0.087 & 9.8 & 9.5 & 1.43 \\
3D-MGP  & 0.057 & 37.1 & 21.3 & 18.2 & **0.020** & 0.026 & 9.3 & 8.7 & 0.092 & 8.6 & **8.6** & 1.38 \\ Transformer-M  & 0.041 & 27.4 & 17.5 & 16.2 & 0.037 & 0.022 & 9.63 & 9.39 & **0.075** & 9.41 & 9.37 & 1.18 \\ Equifomer  & 0.046 & 30 & **15** & 14 & 0.011 & 0.023 & 7.63 & 6.63 & 0.251 & 6.74 & 6.59 & 1.26 \\  Ours & **0.037** & **24.2** & 21.1 & **13.7** & 0.022 & **0.022** & **6.2** & **6.31** & 0.082 & **7.22** & 9.40 & **1.09** \\  

Table 4: Experiment results on QM9 dataset

   & BBBP & BACE & ClinTox & Tox21 & ToxCast & SIDER & HIV & PCBA & MUV & MEAN \\  Standard CL & 69.3 & 81.5 & 84.1 & 75.5 & 63.4 & 58.9 & 78.3 & 84.1 & 72.5 & 75.2 \\ CL + 3D Loss & 75.1 & 86.8 & 87.9 & 78.9 & 68.5 & 62.8 & 81.8 & 88.0 & 77.1 & 78.1 \\ CL + Probabilistic Framework & 74.1 & 86.3 & 88.2 & 79.5 & 68.2 & 63.1 & 82.5 & 88.4 & 77.1 & 78.6 \\  CL + Both & **76.7** & **88.2** & **89.4** & **80.1** & **69.9** & **63.6** & **83.0** & **89.6** & **79.0** & **80.1** \\  

Table 5: Ablation Study on MoleculeNet Classification Datasetsframework and introducing random weights for data pairs. With a Bayesian data augmentation technique, the random weights can be efficiently inferred via sampling, and the model parameter can be efficiently optimized via stochastic expectation maximization.

The effectiveness of our innovative approach has been proven through rigorous evaluations on multiple molecular property prediction benchmarks. The results also showcase the wide-ranging applicability and improved robustness of our proposed method over existing methods for learning molecular representations.

We believe our method is a valuable addition to the literature on molecular contrastive representation learning, which can further boost the performance of state-of-the-art molecular representation learning models for drug design.