# Understanding the Role of Equivariance in Self-supervised Learning

Yifei Wang

MIT

&Kaiwen Hu

Peking University

&Sharut Gupta

MIT

&Ziyu Ye

The University of Chicago

&Yisen Wang

Peking University

&Stefanie Jegelka

TUM

and MIT

Equal Contribution. Correspondence: Yifei Wang <yifei_w@mit.edu>.CIT, MCML, MDSI

###### Abstract

Contrastive learning has been a leading paradigm for self-supervised learning, but it is widely observed that it comes at the price of sacrificing useful features (_e.g.,_ colors) by being invariant to data augmentations. Given this limitation, there has been a surge of interest in equivariant self-supervised learning (E-SSL) that learns features to be augmentation-aware. However, even for the simplest rotation prediction method, there is a lack of rigorous understanding of why, when, and how E-SSL learns useful features for downstream tasks. To bridge this gap between practice and theory, we establish an information-theoretic perspective to understand the generalization ability of E-SSL. In particular, we identify a critical explaining-away effect in E-SSL that creates a synergy between the equivariant and classification tasks. This synergy effect encourages models to extract class-relevant features to improve its equivariant prediction, which, in turn, benefits downstream tasks requiring semantic features. Based on this perspective, we theoretically analyze the influence of data transformations and reveal several principles for practical designs of E-SSL. Our theory not only aligns well with existing E-SSL methods but also sheds light on new directions by exploring the benefits of model equivariance. We believe that a theoretically grounded understanding on the role of equivariance would inspire more principled and advanced designs in this field. Code is available at https://github.com/kaotty/Understanding-ESSL.

## 1 Introduction

Self-supervised learning (SSL) of data representations has made remarkable progress. Existing SSL methods can be categorized into two types: invariant SSL (I-SSL) and equivariant SSL (E-SSL). The idea of I-SSL is to encourage the representation to be invariant to input augmentations (_e.g.,_ color jittering). Contrastive learning that pulls positive samples closer and pushes negative samples apart is widely believed to be a prominent I-SSL paradigm, leading to rapid progress in recent years [13; 50; 5; 80; 46; 30; 3; 32; 33; 60; 49; 14; 37; 15; 77; 64; 81]. Nevertheless, since invariant representations lose augmentation-related information (_e.g.,_ color information), their performance on downstream tasks can be hindered, as frequently observed in practice [47; 17; 34].

In view of these limitations of I-SSL, there has been a growing interest in revisiting E-SSL. Contrary to I-SSL, E-SSL learns representations that are sensitive to (or aware of) the applied transformation.4 For instance, RotNet  is an early exemplar of E-SSL that learns discriminative features by predicting the rotation angles from randomly rotated images . It has also been exploited in recent works and achieves promising improvements in conjunction with I-SSL [73; 67; 17; 18; 25; 54; 34]. Recently, E-SSL has shown potential for serving as the foundation for building visual world models .

Despite this intriguing progress in practice, compared to invariant SSL methods with a vast literature of theoretical analyses [58; 66; 48; 35; 68; 59; 78], there is little theoretical understanding of equivariant SSL methods. A particular difficulty lies in the understanding of the pretraining tasks, which may seem quite irrelevant to downstream classification. Taking RotNet as an example, the random rotation angle is _independent_ of the image class, so it is unclear how rotation-equivariant representations are helpful for image classification. Generally speaking, it is unclear **why, when, and how equivariant representations can generalize to downstream tasks**.

In view of this situation, the primary goal of this paper is not to design a new E-SSL variant, but to revisit the basic E-SSL methods and _understand_ their essential working mechanisms. We fulfil this goal by proposing a simple yet theoretically grounded explanation for understanding general E-SSL from an information-theoretic perspective. We show that the effectiveness of E-SSL can be understood via the "explaining-away" effect in statistics, which implies an intriguing _synergy effect_ between the image class \(C\) and the equivariant transformation \(A\) (_e.g.,_ rotation) such that almost surely, they have strictly positive mutual information _when given the input \(X\), i.e., \(I(C;A|X)>0\)_ that explains the effectiveness of E-SSL. This understanding also provides valuable guidelines for practical E-SSL design with three principles to pursue a large synergy effect \(I(C;A|X)\): lossy transformations, class relevance, and shortcut pruning, as been validated on practical datasets. Theoretically, we also quantitatively analyze the influence of data transformation on the synergy effect with a theory model.

Equipped with these theoretical findings, we further revisit advanced E-SSL methods in the recent literature [73; 67; 17; 18; 25; 54; 34] and find that many of these empirical successes can be well explained in our framework from two aspects: finer equivariance and multivariate equivariance. Besides, motivated by our theory, we also discover an under-explored aspect of E-SSL, model equivariance, where we show that adopting equivariant neural networks can yield strong improvements for certain E-SSL methods. These fruitful theoretical and practical merits suggest that our E-SSL theory provides a general and practically useful explanation for understanding and designing E-SSL methods that have the potential to guide future E-SSL designs.

## 2 Related Work

**Invariant and Equivariant SSL.** Without access to labels, SSL methods design various surrogate tasks that create self-supervision for representation learning. Early SSL methods, often in the form of predictive learning, learn from predicting the transformation of randomly transformed images, such as, RotNet , Jigsaw , Relative Patch Location . Later, discriminating instances in the latent space with contrastive learning demonstrates prominent performance [21; 72; 52; 40; 13; 38; 56], with variants including non-contrastive methods , clustering methods [10; 11; 12], regularization methods . However, data augmentations used in contrastive learning to avoid shortcuts often come at the cost of information lost for downstream tasks (_e.g.,_ color for flower classification). To address this issue, there is a surge of interest in E-SSL that learns features to be sensitive to the applied transformations. Among them, Xiao et al.  use separate embeddings for each augmentation. Wang et al.  apply equivariant prediction on residual vectors between positive views. Dangovski et al.  combine contrastive learning and rotation prediction. Devillers and Lefort , Garrido et al.  utilize conditional predictors with augmentation parameters. Park et al. , Gupta et al.  model latent equivariant transformations explicitly.

**Theory of SSL.** Most existing theories of SSL methods focus on contrastive learning (CL) and its variants from different perspectives: information maximization [52; 40; 65], downstream generalization [58; 66; 48; 35; 68; 59], feature dynamics [66; 69], asymmetric designs [63; 82], feature identifiability , _etc._ But for general E-SSL methods, there is little, if any, theoretical understanding on how they learn meaningful features for downstream tasks (in particular, image classification). Our work fills this gap by establishing a general information-theoretic framework for understanding E-SSL.

**Equivariance in Deep Learning.** Invariance and equivariance represent data symmetries that can be exploited during learning. There are two approaches to utilize invariance and equivariance. One is _equivariant learning_ (to which E-SSL belongs) that uses equivariant training regularization such that features are _approximately_ equivariant; the other is equivariant models that obey _exact_ equivariance by design _w.r.t._ groups like rotation and scaling [16; 29; 8]. Equivariant models find wide applications in graph, manifold, and molecular domains [8; 29], but are rarely explored for equivariant SSL. In this work, we find that model equivariance can be particularly helpful for equivariant learning in terms of both training and generalization, which opens an interesting direction to explore on the interplay between equivariant learning and equivariant models for future research.

## 3 The Challenges of Understanding Equivariant SSL

**Notations.** We introduce existing SSL methods from a probabilistic perspective. Generally, we denote a random variable by a capital letter such as \(X\), its sample space as \(\), and its outcome as \(x\). We learn a representation \((Z//z_{x})\) from the input (\(X//x\)) through a deterministic encoder function \(F\). The general goal of SSL is to learn discriminative representations that are predictive of the image classes (labels) without actual access to label information. For ease of discussion, we mainly adopt the common Shannon information, where the entropy of \(X\) is \(H(X)=-_{P(X)} P(X)\) and the mutual information between \(X\) and \(Y\) is \(I(X;Y)=H(X)-H(X|Y)\). It is also tempting to adopt \(\)-information  that is analogous to Shannon's notion but aligns better with practice by taking into account computational constraints. For ease of understanding, we adopt Shannon's information in the main paper and extend the main results to \(\)-information in Appendix C.

**Equivariant SSL (E-SSL).** For each raw input \(\) sampled from the training set \(\), we independently draw a random augmentation \(A\) and get the augmented sample \(X=T(,A)\) with a transformation mapping \(T}\). The general objective of Equivariant SSL (E-SSL) is to learn representations \(Z=F(X)\) that are sensitive to the applied transformation \(A\). For example, RotNet  utilizes random four-fold rotation \(=\{0^{},90^{},180^{},270^{}\}\) for data augmentation, and learns feature equivariance by predicting the rotation angles from the representation \(Z\). Therefore, E-SSL is driven by maximizing the following mutual information between the augmentation \(A\) and the representation \(Z\):

\[\;I(A;Z).\] (1)

Note again that equivariance studied in this paper, as in many E-SSL works [17; 18; 24; 34], is a relaxed notion of exact equivariance defined in a group-theoretical sense (with invertibility and compositionality) as in equivariant networks . In E-SSL, equivariance generally means augmentation sensitivity, and the mutual information \(I(A;Z)\) measures the degree of equivariance of \(Z\) to \(A\).

**Contrastive Learning as E-SSL.** Contrary to E-SSL, I-SSL enforces features to be invariant to the applied augmentation \(A\). CL is widely believed to be an example of invariant learning . In CL, we apply two random data augmentations, \(A_{1},A_{2}\) to the same input \(\) and get two positive samples \(X_{1},X_{2}\) and their representations \(Z_{1},Z_{2}\) respectively. Since CL is driven by pulling \(Z_{1},Z_{2}\) together, their mutual information objective is often formalized as \(_{Z_{1}=F(X,A_{1}),Z_{2}=F(X,Z_{2})}I(Z_{1};Z_{2})\). However, it is easy to observe that the constant outputs \(Z=const\) are also optimal with maximal \(I(Z_{1};Z_{2})\), suggesting that invariance alone is sufficient for SSL. In fact, contrastive learning can mitigate feature collapse with the help of pushing away from the representation of the other instances (_i.e.,_ negative samples), making it essentially **an equivariant learning task _w.r.t._ the instance, known as instance discrimination_[21; 72]. Indeed, contrastive objectives are essentially non-parametric formulations of instance classification , and under similar designs, parametric instance classification achieves similar performance . Non-contrastive variants with only positive samples are also shown to have inherent connection to contrastive methods in recent studies [63; 82; 23].

**Equivariance is _Not_ All You Need.** A common intuition among E-SSL methods is that better downstream performance comes from better feature equivariance [18; 25; 54; 34]. Here, we begin our discussion by showing a counterexample in the following proposition. All proofs in this paper can be found in Appendix A.

**Proposition 1** (Useless equivariance).: _Assume that the original input \(^{d}\) and the augmentation \(A^{d^{}}\) are independent, and \(X=[,A]^{d+d^{}}\) is obtained with direct concatenation (DC). Then, there exists a simple linear encoder that has perfect equivariance to \(A\), but yields random guessing on downstream classification._

Proposition 1 shows an extreme case when perfect equivariance is unhelpful for feature learning at all. Inspired by this finding, we further examine common image transformations for E-SSL: horizontal flip, grayscale, four-fold rotation, vertical flip, jigsaw, four-fold blur and color inversion (details in Appendix B). Prior to ours, Dangovski et al.  show that when merged with contrastive learning, additional equivariance to some augmentations (_e.g.,_ four-fold rotation and vertical flip) can bring benefits while some are even harmful (_e.g.,_ horizontal flip). It is not fully explored how much _E-SSL alone_ depends on the chosen augmentation. Below, we study seven common transformations for E-SSL on CIFAR-10  with ResNet-18: horizontal flip, four-fold rotation, grayscale, vertical flip, jigsaw, four-fold blur, and color inversion. We only apply random crops to avoid cross-interactions between different transformations. So the numbers reflect the relative strengths of different methods, instead of their optimal performance that can be attained.

Figure 1 reveals big differences between different choices of transformations: with linear probing, four-fold rotation and vertical flip perform the best and attain more than 60% accuracy, while the others do not even attain significant gains over random initialization (34%). This distinction cannot be simply understood via **feature usefulness**, since color information imposed by learning grayscale and color inversion is known to be important for classification . Meanwhile, in Figure 0(a), we find that the **degree of equivariance** (measured by the training loss of E-SSL) does not explain the difference either, since among ineffective ones, some with large training loss have very low equivariance (_e.g.,_ horizontal flip), while some have very high equivariance with nearly zero equivariant loss (_e.g.,_ grayscale). These phenomena show that equivariance alone, either strong or weak, does not have a good or bad indication of downstream performance, which motivates us to provide a more general understanding of E-SSL.

## 4 A Theory of Equivariant SSL

In Section 3, we have shown that feature equivariance alone does not guarantee effective downstream performance, which makes it even unclear how equivariant learning extracts useful features. To resolve these puzzles, we provide an information-theoretic analysis for E-SSL that serves as a natural explanation for the phenomena above.

### Explaining E-SSL via Explaining-away

We start by establishing a causal diagram of the data generation process of E-SSL, where we assume that the original input \(\) is generated from its class variable \(C\) (relevant to input semantics, _e.g.,_ shape), intrinsic equivariance variable \(\) (relevant to semantics, _e.g.,_ the intrinsic orientation of an object) and style variable \(S\) (features irrelevant to semantics and targeted equivariance, _e.g.,_ color

Figure 1: Comparison between different transformations for E-SSL on CIFAR-10 with ResNet-18. Note that different pretraining tasks may have different classes (_e.g.,_\(4\) for rotation and \(2\) for horizontal flip). The baseline is a random initialized encoder with 34% test accuracy under linear probing.

and texture) through some unknown processes. Then, we apply an independently drawn augmentation variable \(A\) (_e.g.,_ a random rotation angle), and get the transformed input \(X\). Afterwards, we obtain its representation \(Z\) through a neural network.

**Collider structure in E-SSL.** The causal diagram shows that the class variable \(C\) and the augmentation variable \(A\) are independent. However, there exists a so-called _collider_ structure where the augmented sample \(X\) is a common child of \(C\) and \(A\). A well-known fact from statistics called the _explaining-away_ effect (_a.k.a._ selection bias)  says that in a collider block, when conditioning on the collider \(X\) or its descendent like \(Z\), the parents \(C\) and \(A\) are no longer independent. For example, the weather \((A)\) and the road condition \((C)\) are independent factors that can contribute to car accidents \((X)\). However, given that an accident happens (\(X\) is known), if we know that it rains today, it would be less likely that the road is broken, and vice versa. In this case, we say that the weather \(A\) explains away the possibility of road conditions \(C\). The theorem below formally characterises the explain-effect effect in the E-SSL process and its information-theoretic implication. A caveat is that Lemma 1 guarantees that explaining-away happens in most, but not all cases (_e.g.,_ Proposition 1), and we explain these exceptions in Section 4.2.

**Lemma 1** (Explaining-away in E-SSL).: _If the data generation process obeys the diagram in Figure 2, then almost surely, \(A\) and \(C\) are not independent given \(X\) or \(Z\), i.e., \(A\!\! C|X\) and \(A\!\! C|Z\). It implies that \(I(A;C|X)>0\) and \(I(A;C|Z)>0\) hold almost surely._

**Explaining-away helps E-SSL.** In statistics, explaining-away often appears as the selection bias in observational data that misleads causal inference (_e.g.,_ the Berkson's paradox ) and demands careful treatment . In contrast, explaining-away plays a critical _positive_ role in E-SSL. In particular, the fact \(I(A;C|Z)>0\) implies an important _synergy_ effect between \(A\) and \(C\) during equivariant learning, as shown below:

\[I(A;C|Z)=H(A|Z)-H(A|Z,C)>0 H(A|Z)>H(A|Z,C).\] (2)

Eq. (2) implies that for the same feature \(Z\), using class information \(C\) gives a better prediction of \(A\) (lower uncertainty \(H(A|Z,C)\)) than without using class features. Intuitively, given a rotated image, recognizing the object class \(C\) in the first place makes it easier to determine the rotation angle \(A\). Driven by this synergy effect, the encoder will learn to encode class information \(C\) in the representation to assist the equivariant prediction of \(A\). 5 We formally characterize this intuition in the following theorem.

**Theorem 1** (Class features improve equivariant prediction).: _Under the data generation process in Figure 2, consider an E-SSL task with input \(X\), its class \(C_{X}\), and its representation \(Z\). Assume a class representation \(Z_{C}=(C_{X})\) that can perfectly predict the label \(C_{X}\) (\(\) is an invertible mapping). Then, almost surely, the combined feature \(=[Z,Z_{C}]\) obtained by appending \(Z_{C}\) to \(Z\) will strictly improve the equivariant prediction with larger mutual information \(I(A;)>I(A;Z)\). Also, we have \(I(C;) I(C;Z)\), so the classification performance improves in the meantime._

As an implication of Theorem 1, to achieve better equivariant prediction, during E-SSL, the model will try to extract more class features, which will jointly improve downstream classification. This explains why during E-SSL with rotation prediction, the classification accuracy also rises along the process, outperforming the random encoder (Figure 0(b)).

Figure 2: The causal diagram of equivariant self-supervised learning. The observed variables are in grey. \(C\): class; \(S\): style; \(\): intrinsic equivariance variable; \(\): raw input; \(A\): augmentation; \(X\): augmented input; \(Z\): representation.

**Remark: Extension to \(\)-information**. In the discussion above, we mainly adopt Shannon's entropy measures for simplicity, which ignores computational and modeling constraints. Computation-aware notions like \(\)-information  would align our theory better with practice. Notably, if we replace Shannon information with \(\)-information, the analyses above still hold (see Appendix C). A nice property of \(\)-information is that feature extraction steps can _increase_ the information by making prediction computationally easier (which only decreases information in Shannon's notion instead), providing better justification for the benefit of representation learning. Thus, from the perspective of \(\)-information, in ESSL, class features _increase the equivariance w.r.t. \(A\)_ with easier computation.

**Verification of synergy effects via controlled experiments.** To validate the above analysis in practice, we further carry out a _controlled experiment_ to study how class information affects the equivariant pretraining task. Specifically, taking the rotation prediction task as an example, we add or substitute a class prediction loss with an additional linear head in the pretraining objective. In the former case, we explicitly inject class information into the presentation by joint training with the classification loss; in the latter, we explicitly eliminate class information from the representation by adversarially maximizing the classification loss  (see Appendix B). As shown in Figure 3, we get slightly better rotation prediction accuracy when explicitly incorporating the class information, while getting worse performance (with a larger margin) when discouraging class information, which agrees well with Theorem 1. Note that there is still nontrivial training accuracy because the class is not the only factor that can explain equivariant prediction (style features \(S\) can also play a role).

### Maximizing the Synergy Effect: Principles for Practical Designs of E-SSL

Our theoretical understanding above not only establishes theoretical explanations for downstream performance, but also provides principled guidelines for E-SSL design. The overall principle is to maximize the synergy \(I(A;C|X)=H(A|X)-H(A|X,C)\), which can be understood from the following aspects that explain various E-SSL behaviors that we observe in Section 3.

**Principle I: "Lossy" Transformations.** First, let us look at \(H(A|X)\), which determines the upper bound of the explaining-away effect. A higher \(H(A|X)\) means that the equivariant prediction task

Figure 3: A controlled experiment on the influence of class information on equivariant prediction. We include three methods: 1) equivariant prediction (baseline); 2) jointly minimizing equivariant and classification losses (“+cls”); 3) minimizing the equivariant loss while adversarially maximizing the classification loss  (“- cls”). We study rotation prediction for (a), (b), (c) and (d), horizontal flip for (e), and four-fold blur for (f).

is inherently harder. Revisiting Proposition 1, our theory gives a natural and rigorous explanation for why direct concatenation (DC) fails for E-SSL. Essentially, the DC output \(X=[A,]\) admits a simple linear encoder such that \(A\) can be perfectly recovered from \(X\), implying \(H(A|X)=0\), which leads to \(I(A;C|X)=0\), _i.e.,_ no explaining-away effect. This implies an intriguing property of E-SSL, that in order to attain nontrivial performance on downstream tasks, _the chosen transformation \(T\) must be lossy_ -- in the sense that one cannot perfectly infer \(A\) after the transformation, _i.e.,_\(H(A|X)>0\).6 Considering computational and model constraints in practical scenarios, this task should be at least hard for the chosen training configuration (_i.e.,_\(H_{}(C|X)>0\)). Only when the transformation is hard enough, neural networks will strive to learn class information to assist its prediction. Indeed, Figure 1 shows that the transformations whose training loss decreases very quickly (_e.g.,_ grayscale and jigsaw) indeed have relatively poor test accuracy, which further verifies our theory.

**Principle II: Class Relevance.** Aside from task hardness, we also need to ensure \(H(A|X,C)\) is low enough; _i.e., extracting class information can effectively improve equivariant prediction_. As a negative example, with a direct concatenation \(X=[C,A]\) as in Proposition 1, even if we add noise to \(A\) such that \(H(A|X)>0\), extracting the class \(C\) is still unhelpful for predicting \(A\). From an information-theoretic perspective, it satisfies \(H(A|X)=H(A|X,C)\), so we always have \(I(A;C|X)=0\). In Figure 1, horizontal flip and four-fold blur have large training losses until the end of the training, _even if we deliberately inject class features_ (see Figures 2(e) & 2(f)). This suggests that these equivariant tasks are intrinsically hard and class information does not contribute much to equivariant prediction. Instead, rotation prediction and vertical flip are hard at the beginning, but the uncertainty can be decreased significantly via learning class information. These transformations thus have a large synergy effect that benefits downstream performance. We conjecture that it is because these transformations are global (_i.e.,_ changing pixel positions) instead of local changes (_i.e.,_ only modifying pixel values) like grayscale and color inversion, so class information as global image semantics are more helpful for such tasks. Another important implication is that the transformation should be class-preserving so as to make class features helpful for the equivariant task. This rule has been verified extensively in contrastive learning [58; 62; 35; 68].

**Principle III: Shortcut Pruning.** Note that in the causal diagram (Figure 2), class \(C\) and style \(S\) features jointly determine the raw input \(\). According to our theory, style features may also explain the equivariant target \(A\) (_i.e.,_\(I(A;S|X)>0\)). Since style features are often easier for NN learning , they can become shortcuts for equivariant prediction such that class features are suppressed [28; 57]. Therefore, to ensure the learning of class-related semantic features, it is important to avoid these shortcuts. One effective approach to corrupt these style features (to some extent) through aggressive data augmentation, _e.g.,_ color jitter, cropping, and blurring commonly adopted in contrastive learning, without corrupting class features a lot. Indeed, Chen et al.  show that the choice of data augmentations plays a vital rule in the success of contrastive learning, and Tian et al.  point out its goal is to prune class-irrelevant features. Here, we generalize this principle to E-SSL as well through our explaining-away framework. As shown in Table 1, the aggressive data augmentations from SimCLR also bring much better performance for E-SSL methods, bringing RotNet (82.26%) close to SimCLR (89.49%). It demonstrates that instead of merging with contrastive learning as in all recent E-SSL works [67; 17; 18; 25; 54; 34], learning from equivariance _alone_ can potentially achieve competitive performance.

### Analysis on the Influence of Transformation

The theory in Section 4.1 guarantees that E-SSL will learn class features almost surely under general conditions. Yet, without further knowledge, it is generally hard to derive more quantitative results for downstream performance. For a concrete discussion, we consider a simplified data generation process as an exemplar. Note that simplified data models are frequently adopted in the literature of self-supervised learning theory [63; 71] to gain insights for their real-world behaviors.

   Augmentation & Train Rot ACC & Test Cls ACC \\  None & 99.98 & 56.92 \\ Crop+flip & 97.71 & 57.32 \\ SimCLR  & **83.26** & **59.06** \\   

Table 1: Comparison of rotation prediction under different augmentations (CIFAR-10, ResNet18).

**Setup.** We consider a simple combination of the class \(C\) and the augmentation \(A\) as a weighted sum,

\[X=A+ C,\] (3)

where \(\) is the mixing coefficient. Here, we assume a balanced class setting, where \(C(N_{C})\) follows a uniform categorical variable over \(N_{C}\) classes: \(0,1,,N_{C}-1\). Similarly, we assume that the augmentation \(A(N_{A})\) is an _independent_ uniform categorical variable over \(N_{A}\) classes: \(0,1,,N_{A}-1\). In this simple setting, it is easy to see that given \(X\), when \(C\) is known, we will have a perfect knowledge of \(A\) as \(A=X- C\), indicating \(H(A|X,C)=0\). Therefore, we have \(I(A;C|X)=H(A|X)\). In other words, transformations only influence the explaining-away effect through the uncertainty of predicting \(A\). This is an extreme case for the ease of theoretical analysis. Nevertheless, the following theorem shows that under this setup, we can have a quantitative characterization of the optimal choice of \(N_{A}\) and \(\) that sheds light on the design of E-SSL methods.

**Theorem 2**.: _The following results hold for the additive problem in Eq. (3):_

1. _Balanced Mixing is Optimal._ _With_ \(N_{C}\) _and_ \(N_{A}\) _held constant,_ \(I(A;C|X)\) _is maximized under balanced mixing with_ \(=1\)_._
2. _Large Action Space is Beneficial._ _With_ \(N_{C}\) _held constant and_ \(=1\)_, we have a lower bound of the mutual information_ \(I(A;C|X) N_{C}-}[(N_{C}-1) N_{C}--1)^ {2}}{N_{C}}(N_{C}-1)+-2}{2}]\)_, which is_ **monotonically increasing** _with respect to_ \(N_{A}\)_._

Theorem 2 has two important implications. First, it suggests that a balanced mixing of \(A\) and \(C\) gives the optimal synergy effect, since it can maximize the uncertainty of using \(X\) for predicting \(A\) alone (agreeing with Principle I). Second, it shows that a large action space (\(||\)) is preferred, making it harder to use spurious features (e.g., the boundary values of \(C\) and \(A\)) as a shortcut to determine \(A\) (agreeing with Principles II and III). These theoretical results illustrate our analyses above and provide insights for understanding advanced designs in E-SSL methods, as elaborated below.

## 5 Understanding Advanced E-SSL Designs

In Section 4, we have established a theoretical understanding of basic E-SSL through the explaining-away effect. However, basic E-SSL (like rotation prediction) often fails to achieve satisfactory performance, and many advanced designs have been proposed to enhance E-SSL performance [67; 17; 18; 25; 54]. In this section, we further explain how these advanced designs improve performance by enhancing the synergy effect between class information and equivariant prediction.

### Fine-grained Equivariance

A conclusion from Theorem 2 is that a larger action space of the transformation \(A\) benefits the explaining-away effect by increasing the task difficulty \(H(A|X)\). Guided by this principle, one way to improve E-SSL is through learning from more fine-grained equivariance variables with a larger action space \((||)\), which encourages models to learn diverse features and avoid feature collapse for specific augmentations. For example, four-fold rotation is a \(4\)-way classification task while CIFAR-100 has 100 classes. When the neural networks are expressive enough such that it clusters samples with the same augmentation to (almost) the same representation (known as neural collapse ), the class features also degrade or vanish, which hinders downstream classification. For example, Table 1 shows that for rotation prediction, stronger augmentations suffer from less feature collapse (lower training accuracy), while enjoying better classification accuracy. Indeed, we show that the advantages of state-of-art SSL methods can be understood through this information-theoretic perspective.

**Information-theoretic Understanding of Instance Discrimination.** As disclosed in Section 3, contrastive learning is essentially an E-SSL task with equivariance prediction of instances. Specifically, each raw example \(_{i}\) serves as an instance-wise class, forming an action space \(\), where all augmented samples of \(_{i}\) belong to the class \(i\). Therefore, the instance classification task has an action space of \(||=N\), where \(N\) is the number of training dataset that is much larger than rotation prediction with \(||=4\), making instance discrimination a harder task, especially under strong data augmentations [72; 21]. Since the instance index \(I\) is also _independent of the class_ variable \(C\), it is notfully clear why it is helpful for learning class-relevant features.7 Instead, our explaining-away theory gives a natural explanation from the instance classification perspective. In this way, our explanation of E-SSL can be regarded as a unified understanding of existing SSL variants.

**Equivariance Beyond Instance.** Although contrastive learning already adopts a very large action space with \(||=N\), there is recent evidence showing that it can still learn shortcuts [57; 73] and lack feature diversity . Therefore, it is natural to consider even finer-grained equivariance, such as learning to predict patch-level or pixel-level features , inputs , or tokenized patches , which comprises many variants of SSL methods, ranging from MAE , BERT , to diffusion models [41; 61]. Here, either random mask  or Gaussian noise  can be viewed as random variables (similar to the rotated angle in rotation prediction) and is independent of the class semantics, so they fit into our theory as well. Features learned from these tasks do show more diversity in practice and benefit downstream tasks requiring fine-grained semantics [39; 42]. Therefore, our theory provides a principled way to understanding the benefits of fine-grained supervision in SSL.

### Multivariate Equivariance

As discussed in Section 4.2, equivariant prediction may have class-irrelevant features as shortcuts, while corrupting these features (_e.g.,_ color) with data augmentation might affect certain downstream tasks (_e.g.,_ flower classification that requires color information too). A more principled way that has been explored recently is through joint prediction of multiple equivariance variables [67; 17; 18; 24; 54; 34], which we refer to as multivariate equivariance. In the following theorem, we show that multivariate equivariance is provably beneficial since it **monotonically increases the synergy effect** between class information and equivariant prediction, as shown in the following theorem.

**Theorem 3**.: _For two transformation variables \(A_{1},A_{2}\), we will always have \(I(A_{1},A_{2};C|Z)\{I(A_{1};C|Z),I(A_{2};C|Z)\}\). In other words, multivariate equivariance brings strengthens the explaining-away effect, with a gain of \(g=\{I(A_{2};C|Z,A_{1}),I(A_{1};C|Z,A_{2})\}\)._

Theorem 3 can also be easily extended to more equivariant variables. Note that the gains of multivariate equivariance \(I(A_{2};C|Z,A_{1})\) reflects the amount of additional information that the class information \(C\) can explain away \(A_{2}\) under the same value of \(A_{1}\); therefore, more diverse augmentations provide a large gain in the synergy effect. Recent works on image world model show that equivariance to multiple transformation delivers better downstream performance and outperforms invariant learning .

### Model Equivariance

Apart from the design of transformations that is the main focus of E-SSL methods, an often overlooked part is the equivariance of the backbone models, which we call model equivariance. Intriguingly, we find that equivariant networks can be very helpful for E-SSL when _the transformation equivariance aligns well with model equivariance_.

**Setup.** We compare a standard non-equivariant ResNet18  and an equivariant ResNet18 (EqResNet18) _w.r.t._ the \(p4\) group (consisting of all compositions of translations and \(90\)-degree rotations)  of similar parameter sizes. The models are pretrained on CIFAR-10 and CIFAR-100 for 200 epochs

   Dataset & Augmentation & Network & Train Rotation ACC & Test Classification ACC & Gain \\   &  & ResNet18 & 99.98 & 56.92 &  \\  & & EqResNet18 & **100.00** & **72.32** & \\   & & ResNet18 & 97.71 & 57.32 & \\   & & EqResNet18 & **99.97** & **82.54** & +25.22 \\   &  & ResNet18 & 83.26 & 59.06 &  \\   & & EqResNet18 & **91.98** & **82.26** & \\   

Table 2: Training rotation prediction accuracy and test linear classification accuracy under different base augmentations (CIFAR-10, ResNet18).

with rotation prediction, and then the learned representations are evaluated with a linear probing (LP) head for downstream classification (details in Appendix B). Note that a rotation-equivariant model does not necessarily predict rotation angles perfectly, since in E-SSL, the model only has access to the transformed input but not the ground-truth transformation.

As shown in Table 2 and 4 (see Appendix B), we find that equivariant models bring significant gains for rotation prediction by more than 20% on CIFAR-10 and CIFAR-100. Under aggressive data augmentations (_e.g.,_ SimCLR ones), equivariant models provide better equivariant prediction of rotation with high accuracy (91.98% _v.s._ 83.26% on CIFAR-10 and 82.69% _v.s._ 68.29% on CIFAR-100), which also yields better performance on downstream classification with 23.20% and 26.46% higher accuracy respectively. Even more surprisingly, with mild augmentations (no or crop&flip), both models achieve perfect rotation prediction, while equivariant models can still improve classification accuracy a lot.

Therefore, we find that under compatible equivariance, equivariant models have significant advantages for E-SSL in terms of both self-supervised pretraining (better pretraining accuracy) and downstream generalization (best classification accuracy). The following theorem justifies this point by showing that the mutual information _w.r.t._ the transformation \(A\) lower bounds the mutual information _w.r.t._ the classification \(C\). Therefore, given the same equivariant task (_e.g.,_ same data augmentations), features with better equivariant prediction (larger lower bound) will also have more class information.

**Theorem 4**.: _For any representation \(Z\), its mutual information with the equivariant learning target \(A\) lower bounds its mutual information with the downstream task \(C\) as follows:_

\[I(Z;A) I(Z;C)-I(X;A|C).\] (4)

Here, a small gap \(I(X;A|C)\) means a better generalization between these two tasks. Because \(I(X;A|C)=H(A|X,C)\) is a lower bound of \(I(A;C|X)\) that indicates class relevance, it further justifies our Principle II (Section 4.2) that better class relevance brings better E-SSL performance.

### Strict Equivariant Objectives

Mathematically, an exact definition of equivariance requires that for each transformation \(a\) in the input space, there is a corresponding transformation \(T_{a}\) in the representation space so that \(f(a(x)) T_{a}f(x)\). Common rotation prediction objectives do not satisfy this property. Other works also study the use of exact equivariant objectives. Here, we take CARE  as an example and compare it against rotation prediction with cross-entropy (CE) loss.

Table 3 shows that training with exact equivariant objective (CARE) leads to further improvement in test accuracy (57.32% \(\) 64.50%), which aligns with our observation of model equivariance in Section 5.3. Altogether they suggest that enforcing exact feature equivariance (either through model architecture or feature regularization) can bring considerable benefits in downstream generalization.

## 6 Conclusion

In this paper, we have provided a general theoretical understanding of how learning from seemingly irrelevant equivariance (such as, random rotations, masks and instance indices) can benefit downstream generalization in self-supervised learning. Leveraging the causal structure of data generation, we have discovered the explaining-away effect in equivariant learning. Based on this finding, we have established theoretical guarantees on how E-SSL extracts class-relevant features from an information-theoretic perspective. We also identify several key factors that influence the explaining-away. Since this work is theory-oriented to fill the gap between practice and theory by investigating how E-SSL works, we do not explore extensively for a better E-SSL design. Nevertheless, the fruitful insights developed in this work could inspire more principled designs of E-SSL methods in future research.

   Equivariant Loss & Train Rot ACC & Test Cls ACC \\  CE loss & 97.71 & 57.32 \\ CARE loss & 99.95 & 64.50 \\   

Table 3: Comparison of augmentation-aware and truly equivariant methods (CIFAR-10, ResNet18).