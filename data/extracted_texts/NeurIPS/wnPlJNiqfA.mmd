# KFNN: K-Free Nearest Neighbor For Crowdsourcing

Wenjun Zhang

School of Computer Science

China University of Geosciences

Wuhan 430074, China

wjzhang@cug.edu.cn &Liangxiao Jiang

School of Computer Science

China University of Geosciences

Wuhan 430074, China

ljiang@cug.edu.cn &Chaoqun Li

School of Mathematics and Physics

China University of Geosciences

Wuhan 430074, China

chqli@cug.edu.cn

Corresponding author

###### Abstract

To reduce annotation costs, it is common in crowdsourcing to collect only a few noisy labels from different crowd workers for each instance. However, the limited noisy labels restrict the performance of label integration algorithms in inferring the unknown true label for the instance. Recent works have shown that leveraging neighbor instances can help alleviate this problem. Yet, these works all assume that each instance has the same neighborhood size, which defies common sense. To address this gap, we propose a novel label integration algorithm called K-free nearest neighbor (KFNN). In KFNN, the neighborhood size of each instance is automatically determined based on its attributes and noisy labels. Specifically, KFNN initially estimates a Mahalanobis distance distribution from the attribute space to model the relationship between each instance and all classes. This distance distribution is then utilized to enhance the multiple noisy label distribution of each instance. Subsequently, a Kalman filter is designed to mitigate the impact of noise incurred by neighbor instances. Finally, KFNN determines the optimal neighborhood size by the max-margin learning. Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art algorithms and exhibits greater robustness in various crowdsourcing scenarios. Our codes and datasets are available at https://github.com/jiangliangxiao/KFNN.

## 1 Introduction

Crowdsourcing provides a more cost-effective way to obtain annotated instances than traditional expert annotation . Through crowdsourcing platforms such as Figure Eight and Clickworker, instances can be annotated by crowd workers at a low cost [2; 3]. While more affordable, these workers possess less expertise than domain experts and are more prone to assigning noisy labels to instances . To address this issue, the concept of _repeated annotation_ is introduced and becomes popular in crowdsourcing . With _repeated annotation_, each instance is annotated by several workers, thereby obtaining multiple noisy labels. To train supervised models using multiple noisy labels, two main categories of methods have been developed: one-stage methods and two-stage methods. One-stage methods [6; 7; 8] train models directly using multiple noisy labels. Two-stage methods [1; 9] first infer the unknown true label for each instance from its multiple noisy labels vialabel integration (also known as answer aggregation or ground truth inference)  and then train models on integrated labels. One-stage methods, although end-to-end, can only be used to train specifically designed models. As a result, label integration, which is required for the more common two-stage methods, has received a great deal of attention from researchers.

It has been theoretically demonstrated that, when worker annotation is more accurate than random annotation, the more noisy labels an instance receives, the easier it becomes to infer its unknown true label . However, to reduce annotation costs, only a few noisy labels can be collected for each instance in crowdsourcing. The limited noisy labels restrict the performance of label integration algorithms in inferring the unknown true label for the instance. Furthermore, some common strategies in crowdsourcing, such as worker modelling, worker elimination and task assignment , fail to mitigate the effects of limited labels in label integration. To alleviate this problem, recent works have begun to focus on leveraging neighbor instances [13; 14; 1]. These works successfully improve the performance of label integration by leveraging the information from neighbor instances obtained by the K-nearest neighbor (KNN) algorithm. However, due to the use of KNN, these algorithms all assume that each instance has the same neighborhood size. This assumption is difficult to hold because it defies common sense, e.g. instances close to the center of classes should have more neighbors than instances close to the boundary of classes.

To address this gap, we propose a novel label integration algorithm called K-free nearest neighbor (KFNN). In KFNN, the optimal neighborhood size of each instance is automatically determined based on its attributes and noisy labels. Notably, KFNN is different from some supervised works [15; 16] that determine the optimal K-value for KNN. Unlike in supervised learning, the true label of each instance in crowdsourcing is unknown and only its multiple noisy labels can be used, which makes it difficult to model the relationship between the instance and all classes. To do this, KFNN initially estimates a Mahalanobis distance distribution from the attribute space to model the relationship between each instance and all classes. This distance distribution is then utilized to enhance the label distribution for each instance. Subsequently, a Kalman filter is designed to mitigate the impact of noise incurred by neighbor instances. Finally, KFNN determines the optimal neighborhood size by the max-margin learning. In general, the contributions of this paper can be summarized as follows:

* We reveal the limitations caused by fixing the neighborhood size in existing label integration algorithms and propose a novel algorithm called KFNN. In KFNN, the neighborhood size of each instance is automatically determined based on its attributes and noisy labels.
* We estimate a Mahalanobis distance distribution from the attribute space to model the relationship between each instance and all classes. This distance distribution enhances the multiple noisy label distribution of each instance.
* We design a Kalman filter to mitigate the impact of noise incurred by neighbor instances and then determine the optimal neighborhood size by the max-margin learning, which provides strong theoretical support for our algorithm.
* Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art label integration algorithms and exhibits greater robustness than existing algorithms in various crowdsourcing scenarios.

## 2 Related work

Depending on whether neighbor instances are leveraged or not, existing label integration algorithms can be divided into two categories. The first category of algorithms does not leverage neighbor instances, which considers only the information of the instance itself or the information of all instances globally in label integration. For example,  models the ability of each worker with a confusion matrix. In this matrix, each element reflects the probability that this worker annotates an instance with the class corresponding to the row as the class corresponding to the column. [18; 19] are Bayesian versions of , which can be used for binary tasks and multi-class tasks, respectively. Further, [20; 21] improve  by introducing the correlation between workers. [11; 22; 23] are classical algorithms based on majority voting and they tend to use the label with the highest number of votes as the integrated label. [24; 25; 26] synchronously model the ability of workers and the difficulty of tasks from different perspectives. [27; 28] use clustering algorithms to divide instances into different clusters from different views, and then map these clusters to different classes. Recently,  augments the multiple noisy label distributions of instances as new attributes to the original attribute space and then learns a classifier on the augmented attribute space to predict the integrated labels of instances.  constructs graphs for workers and uses a graph neural network to aggregate multi-order information in label integration.

The second category of algorithms performs label integration by leveraging the information from neighbor instances obtained by the KNN algorithm. For example,  proposes to use the labels assigned to the neighbor instances of an instance to augment this instance's multiple noisy labels and use the augmented multiple noisy labels to infer the integrated label of this instance.  considers both nearest and farthest neighbors in weighted voting to address class-imbalanced tasks. Further, inspired by label distribution learning [30; 31], given an instance,  iteratively absorbs the label distributions of its neighbor instances into its label distribution through label distribution propagation.

While simpler and more efficient, the first category of algorithms are limited in effectiveness because each instance can only obtain few noisy labels. Both experimental results and theoretical analysis demonstrate the effectiveness of the second category of algorithms in leveraging the information from neighbor instances. However, these algorithms all assume a fixed neighborhood size for each instance, which is often unrealistic and thus limits their performance. To further ensure that each instance has a free neighborhood size, this paper proposes a novel label integration algorithm called KFNN. KFNN automatically determines the optimal neighborhood size for each instance based on its attributes and noisy labels, which improves the performance and robustness of label integration.

## 3 Algorithm

In this section, we respond to how to automatically determine the optimal neighborhood size for each instance. First, we present some basic notations in crowdsourcing and then define the problem settings. Subsequently, we introduce our KFNN for label integration.

### Preliminary

Let \(D=\{(_{i},_{i})\}_{i=1}^{N}\) denote a crowdsourced dataset, where \(N\) is the number of instances, and \(_{i}\) denotes the \(i\)-th instance in \(D\). \(_{i}\) can be represented as \(\{x_{im}\}_{m=1}^{M}\). Here, \(M\) is the dimension of attributes, and \(x_{im}\) denotes the attribute value of \(_{i}\) on the \(m\)-th attribute \(A_{m}\). \(_{i}\) denotes multiple noisy labels of \(_{i}\), which can be expressed as \(\{l_{ir}\}_{r=1}^{R}\). \(R\) is the number of workers and \(l_{ir}\) denotes the label of \(_{i}\) annotated by the \(r\)-th worker \(u_{r}\). \(l_{ir}\) takes a value from a fixed set \(\{-1,c_{1},,c_{q},,c_{Q}\}\), where \(Q\) is the number of classes, \(c_{q}\) denotes the \(q\)-th class and \(-1\) indicates that \(u_{r}\) has not annotated \(_{i}\). Label integration aims to infer an integrated label \(_{i}\) for \(_{i}\) and minimize the error between \(_{i}\) and the unknown true label \(y_{i}\).

Recent works [1; 13] have shown that leveraging neighbor instances \(}_{i}=\{_{i}^{k}\}_{k=1}^{K}\) of \(_{i}\) can mitigate the restriction of limited noisy labels on the performance of label integration. Here, \(_{i}^{k}\) denotes the \(k\)-th nearest neighbor of \(_{i}\) and \(K\) is the neighborhood size. However, in these works, the value of \(K\) is fixed for each instance within the same dataset, which does not make sense. On the one hand, instances closer to the center of a class benefit from a larger \(K\), as it enables them to collect more labels from similar instances. Conversely, for instances close to the boundary of classes, a larger \(K\) plays a negative role in label integration. On the other hand, using a fixed \(K\) can bias algorithms towards the majority class in class-imbalanced datasets, as instances from the majority class are more likely to dominate the neighborhood of instances from minority classes. Therefore, we define the Problem 1 to be addressed in this paper as follows:

**Problem 1**.: _Given a crowdsourced dataset \(D\), how to automatically determine the optimal neighborhood size \(K_{i}^{*}\) for each instance \(_{i}\) with \(\{x_{im}\}_{m=1}^{M}\) and \(\{l_{ir}\}_{r=1}^{R}\) but without \(y_{i}\)._

Problem 1 cannot be treated simply as learning an optimal neighborhood size for the KNN algorithm in supervised learning [15; 16]. This is because the true labels of instances in crowdsourcing are unknown. As a result, \(K\) can not be evaluated accurately by supervised metrics such as classification accuracy. Moreover, label integration does not divide the crowdsourced dataset into training, validation and test sets, which means that KFNN has to determine \(K_{i}^{*}\) immediately when inferring \(_{i}\), rather than with a validation phase.

### K-free nearest neighbor algorithm

In this subsection, we propose our KFNN to address Problem 1. We argue that \(K_{i}^{*}\) should be related to the information from both the attribute space and the multiple noisy label space. Based on this, KFNN divides Problem 1 into two parts: 1) How to fuse the information from the attribute space and the multiple noisy label space? 2) How to determine an optimal \(K_{i}^{*}\) for \(_{i}\)? Correspondingly, KFNN consists of two components, namely label distribution enhancement and K-free optimization, which are used to address the two parts of Problem 1.

#### 3.2.1 Label distribution enhancement

For each instance \(_{i}\), \(\{x_{im}\}_{m=1}^{M}\) reflects all the information of it in the attribute space and \(\{l_{ir}\}_{r=1}^{R}\) reflects all the information of it in the multiple noisy label space. Inspired by label enhancement (LE) [32; 33], we design a label distribution enhancement (LDE) component for KFNN. LDE recovers a potential label distribution using \(\{x_{im}\}_{m=1}^{M}\), and then enhances the multiple noisy label distribution calculated from \(\{l_{ir}\}_{r=1}^{R}\) by this potential label distribution. Specifically, KFNN first uses majority voting to initialize the integrated label \(_{i}\) for \(_{i}\) as follows:

\[_{i}=*{arg\,max}_{c\{c_{1},c_{2},,c_{Q}\}}p(c_{q }|_{i}),\] (1)

where \(p(c_{q}|_{i})\) can be calculated as follows:

\[p(c_{q}|_{i})=^{R}(l_{ir},c_{q})}{_{q=1}^{Q} _{r=1}^{R}(l_{ir},c_{q})},\] (2)

Here, \(p(c_{q}|_{i})\) reflects the proportion of labels in \(_{i}\) that take the value \(c_{q}\). The function \(()\) outputs 1 if its two parameters are identical, and 0 otherwise. Subsequently, according to \(_{i}\), the crowdsourced dataset \(D\) can be divided into \(Q\) subsets \(\{D_{q}\}_{q=1}^{Q}\). The subset \(D_{q}\) contains all instances with initial integrated labels of \(c_{q}\), i.e., \(D_{q}=\{_{i}|_{i}=c_{q}\}_{i=1}^{N}\). Then, KFNN calculates a Mahalanobis distance distribution \(\{d(_{i},D_{q})\}_{q=1}^{Q}\) as follows:

\[d(_{i},D_{q})=_{i}-_{q})^{T}}_{q}^{ -1}(_{i}-_{q})},\] (3)

where \(_{q}\) denotes the centroid of \(D_{q}\) and \(}_{q}^{-1}\) denotes the inverse matrix of the covariance matrix of \(D_{q}\). \(d(_{i},D_{q})\) is the Mahalanobis distance from \(_{i}\) to \(D_{q}\) calculated in the attribute space. A larger \(d(_{i},D_{q})\) means that \(_{i}\) is less likely to belong to \(c_{q}\), conversely a smaller \(d(_{i},D_{q})\) means that \(_{i}\) tends to belong to \(c_{q}\). Therefore, \(\{d(_{i},D_{q})\}_{q=1}^{Q}\) can be used to model the relationship between each instance and all classes. Based on this, \(\{d(_{i},D_{q})\}_{q=1}^{Q}\) can be transformed into a potential label distribution \(\{p(c_{q}|_{i},D_{q})\}_{q=1}^{Q}\) as follows:

\[p(c_{q}|_{i},D_{q})=_{i},D_{q})\}_{q=1}^{Q})-d( {x}_{i},D_{q})}{max(\{d(_{i},D_{q})\}_{q=1}^{Q})-min(\{d(_{i},D_{q })\}_{q=1}^{Q})},\] (4)

where \(max()\) and \(min()\) denote the maximum and minimum values of the set, respectively.

In addition to the potential label distribution, a multiple noisy label distribution \(\{p(c_{q}|_{i})\}_{q=1}^{Q}\) can also be directly transformed from \(_{i}\). Different from \(\{p(c_{q}|_{i},D_{q})\}_{q=1}^{Q}\), which learns the potential relationship between instances and classes from the attribute space, \(\{p(c_{q}|_{i})\}_{q=1}^{Q}\) learns the label distribution reflected by noisy labels from the multiple noisy label space. Finally, KFNN fuses them into an enhanced label distribution \(_{i}=\{p_{iq}\}_{q=1}^{Q}\) by averaging as follows:

\[p_{iq}=|_{i},D_{q})+p(c_{q}|_{i})}{_{q=1}^{Q}[p(c_ {q}|_{i},D_{q})+p(c_{q}|_{i})]}.\] (5)

In this way, the enhanced label distribution \(_{i}\) can fuse the information from the attribute space and the multiple noisy label space. Therefore, the first part of Problem 1 has been addressed.

#### 3.2.2 K-free optimization

After obtaining \(_{i}\) by label distribution enhancement, KFNN proceeds to determine the optimal neighborhood size \(K_{i}^{*}\) for \(_{i}\). First, KFNN calculates the distance between each pair of instances \(_{1}\) and \(_{2}\) by:

\[d(_{1},_{2})=_{q=1}^{Q}d(_{1},_{2}|D_{q}),\] (6)

where \(d(_{1},_{2}|D_{q})\) can be calculated as follows:

\[d(_{1},_{2}|D_{q})=_{1}-_{2})^{T}_{q}^{-1} (_{1}-_{2})},\] (7)

Compared to the Euclidean distance, Eq. (6) introduces the label information by calculating the distance between \(_{1}\) and \(_{2}\) on each subset \(D_{q}\). According to Eq. (6), we can calculate distances between \(_{i}\) and all instances in \(D\). By sorting these distances we can obtain a neighbor sequence \(<_{i}^{1},,_{i}^{k},,_{i}^{N}>\) for \(_{i}\). Here, \(_{i}^{k}\) is the \(k\)-th neighbor instance of \(_{i}\) satisfying \(d(_{i},_{i}^{k}) d(_{i},_{i}^{k-1})\) when \(k\) greater than 1. Then, we calculate the weight \(w_{ik}\) for \(_{i}^{k}\) as follows:

\[w_{ik}=^{R}(l_{ir},l_{ikr})}{_{r=1}^{R}[1-(l_{ ir},-1)]*[1-(l_{ikr},-1)]},\] (8)

where \(l_{ikr}\) denotes the label of \(_{i}^{k}\) annotated by the \(r\)-th worker \(u_{r}\). \(w_{ik}\) reflects the proportion of workers assigned the same label for \(_{i}\) and \(_{i}^{k}\). Subsequently, \(_{i}\) is allowed to absorb the enhanced label distributions of neighbor instances in the neighbor sequence one by one. Let \(_{i}^{k}=\{p_{iq}^{k}\}_{q=1}^{Q}\) denote the label distribution of \(_{i}\) after absorbing \(_{i}^{k}\), which can be updated as follows:

\[p_{iq}^{k}=^{k-1}+w_{ik}*p_{ikq}}{_{q=1}^{Q}[p_{iq}^{k-1}+w_{ ik}*p_{ikq}]}, k 2,\] (9)

where \(p_{ikq}\) denotes the probability value corresponding to \(c_{q}\) in the enhanced label distribution of \(_{i}^{k}\). Since the first neighbor instance of \(_{i}\) is itself, \(_{i}^{k}=_{i}\) when \(k\) is equal to 1.

According to \(_{i}^{k}\), KFNN calculates a class margin as follows:

\[}_{k}=max(_{i}^{k})-sec(_{i}^{k}),\] (10)

where \(sec()\) denotes the second-largest value of the set. Since the true labels are unknown, Eqs. (5) (7) (8) are all designed based on multiple noisy labels, which lead to that \(}_{k}\) contains a degree of noise incurred by neighbor instances. Therefore, KFNN designs a Kalman filter to mitigate the impact of noise in \(}_{k}\) as follows:

\[}_{k}^{-}=}_{k-1}\\ _{k}^{-}=_{k-1}+\\ _{k}=_{k}^{-}}{_{k}^{-}+}\\ }_{k}=}_{k}^{-}+_{k}*(}_{k}-}_{k}^{-})\\ _{k}=(1-_{k})*_{k}^{-},\] (11)

where \(}_{k}\) denotes the filtered margin, determined by both the estimated margin \(}_{k}^{-}\) and the calculated margin \(}_{k}\). The designed Kalman filter can be divided into an estimation phase and an update phase. In the estimation phase, the filter estimates \(}_{k}^{-}\) and the estimated error \(_{k}^{-}\) based on the filtered margin \(}_{k-1}\) and error \(_{k-1}\) of the previous time index. In the update phase, the filter first updates the Kalman gain \(_{k}\) of the \(k\)-th time index and then updates \(}_{k}\) and error \(_{k}\) of the \(k\)-th time index according to \(_{k}\). \(\) and \(\) are the process error and the measurement error in the Kalman filter. When \(k\) is equal to 0, \(}_{k}\) takes the value of 0 and \(_{k}\) takes the value of 1.

To address the second part of Problem 1, KFNN determines the optimal neighborhood size \(K_{i}^{*}\) for \(_{i}\) by the max-margin learning as follows:

\[K_{i}^{*}=*{arg\,max}_{k\{1,2,,N\}}}_{k}.\] (12)

Ultimately, according to \(K_{i}^{*}\), KFNN updates the integrated label \(_{i}\) for \(_{i}\) as follows:

\[_{i}=*{arg\,max}_{c\{c_{1},c_{2},,c_{Q}\}}_ {i}^{K_{i}^{*}}.\] (13)

The whole learning process of KFNN is shown in Algorithm 1. In Algorithm 1, lines 1-3 initialize the integrated label and multiple noisy label distribution for each instance and their time complexity is \(O(NQR)\). Line 4 divides the crowdsourced dataset \(D\) into \(Q\) subsets and its time complexity is \(O(NQ)\). Lines 5-9 perform label distribution enhancement and their time complexity is \(O(NM^{2}Q)\). Line 11 calculates the distances from \(_{i}\) to other instances and sorts these distances, its time complexity is \(O(NM^{2}Q+N(N))\). Lines 12-16 calculate the margins \(\{}_{k}\}_{k=1}^{N}\) and their time complexity is \(O(NR+NQ)\). Line 17 filters the margins \(\{}_{k}\}_{k=1}^{N}\) and its time complexity is \(O(N)\). Line 18 determines the optimal neighborhood size and its time complexity is \(O(N)\). Line 19 infers the integrated label for each instance and its time complexity is \(O(Q)\). Therefore, the time complexity of lines 10-20 is \(O(N^{2}(M^{2}Q+(N)+R))\). If only the highest order terms are taken, the time complexity of KFNN is \(O(N(NM^{2}Q+N(N)+NR+QR))\).

```
0:\(D=\{(_{i},_{i})\}_{i=1}^{N}\) - a crowdsourced dataset; \(\), \(\) - the predefined parameters
0:\(\{_{i}\}_{i=1}^{N}\) - the integrated labels
1:for\(i=1\) to \(N\)do
2: Initialize \(_{i}\) and \(\{p(c_{q}|_{i})\}_{q=1}^{Q}\) for \(_{i}\) by Eqs. (1) (2)
3:endfor
4: Divide \(D\) into \(\{D_{q}\}_{q=1}^{Q}\) based on \(_{i}\)
5:for\(i=1\) to \(N\)do
6: Calculate \(\{d(_{i},D_{q})\}_{q=1}^{Q}\) for \(_{i}\) by Eq. (3)
7: Transform \(\{d(_{i},D_{q})\}_{q=1}^{Q}\) into \(\{p(c_{q}|_{i},D_{q})\}_{q=1}^{Q}\) by Eq. (4)
8: Fuse \(\{p(c_{q}|_{i},D_{q})\}_{q=1}^{Q}\) and \(\{p(c_{q}|_{i})\}_{q=1}^{Q}\) into \(_{i}=\{p_{iq}\}_{q=1}^{Q}\) by Eq. (5)
9:endfor
10:for\(i=1\) to \(N\)do
11: Calculate \(<_{i}^{1},,_{i}^{k},,_{i}^{N}>\) for \(_{i}\) by Eqs. (6) (7)
12:for\(k=1\) to \(N\)do
13: Calculate the weight \(w_{ik}\) for \(_{i}^{k}\) by Eq. (8)
14: Update the label distribution \(_{i}^{k}\) by Eq. (9)
15: Calculate the \(}_{k}\) by Eq. (10)
16:endfor
17: Filter \(\{}_{k}\}_{k=1}^{N}\) using the designed Kalman filter by Eq. (11)
18: Determine the optimal neighborhood size \(K_{i}^{*}\) for \(_{i}\) by Eq. (12)
19: Infer the integrated label \(_{i}\) for \(_{i}\) by Eq. (13)
20:endfor
21:return\(\{_{i}\}_{i=1}^{N}\) ```

**Algorithm 1** The learning process of KFNN

## 4 Theoretical analysis

In this section, we provide some detailed theoretical analysis for KFNN. First, in Eq. (6), KFNN defines the distance \(d(_{1},_{2})\) between \(_{1}\) and \(_{2}\) based on the Mahalanobis distance \(d(_{1},_{2}|D_{q})\) rather than the traditional Euclidean distance \(d_{E}(_{1},_{2})\). According to Eqs. (3) (7), the Mahalanobis distance works based on a basic assumption, which can be described as follows:

**Assumption 1**.: _Given the subset \(D_{q}\), its covariance matrix \(}_{q}\) is a nonsingular matrix._

The Assumption 1 holds based on the condition that \(|}_{q}|\) is non-zero, which is usually satisfied. Even if this condition is not satisfied, we can ensure that the Assumption 1 holds by adding a small value to each element of the principal diagonal on \(}_{q}\) until \(|}_{q}|\) is non-zero.

**Theorem 1**.: _If Assumption 1 holds, there will be an orthogonal matrix \(}\) satisfying that \(}^{-1}}_{q}}=}^{T} }_{q}}=\), where \(\) is a diagonal matrix with all \(M\) eigenvalues of \(}_{q}\) as its elements of the principal diagonal._

Due to the limited pages, the proof of Theorem 1 is provided in Appendix A. Based on Theorem 1, we can obtain some interesting corollaries about Eqs. (3) (6) (7).

**Corollary 1**.: _Compared to \(d_{E}(_{1},_{2})\), \(d(_{1},_{2})\) in Eq. (6) does not suffer from the correlation and magnitude of attributes._

Proof.: According to Theorem 1, \(d(_{1},_{2}|D_{q})\) can be transformed as follows:

\[ d(_{1},_{2}|D_{q})&= _{1}-_{2})^{T}}_{q}^{-1}(_{1}-_{ 2})}\\ &=_{1}-_{2})^{T}((}^{T})^{-1} }^{-1})^{-1}(_{1}-_{2})}\\ &=}^{T}(_{1}-_{2}))^{T}^{-1}(}^{T}(_{1}-_{2}))}.\] (14)

When \(^{-1}\) is not considered, the derivation of Eq. (14) implies that \(d(_{1},_{2}|D_{q})\) is the Euclidean distance of instances after orthogonal transformation using \(}^{T}\). After orthogonal transformation, the attributes are independent of each other, so \(d(_{1},_{2})\) does not suffer from the correlation of attributes. \(^{-1}\) is equivalent to diag\((},},,})\), where \(_{M}\) is the \(M\)-th eigenvalue of \(}_{q}\) and is equal to the variance on the direction of the corresponding eigenvectors. Briefly, \(^{-1}\) ensures that the calculated result on each dimension is normalized by the corresponding variance when calculating the distance by Eq. (7). Therefore, \(d(_{1},_{2})\) does not also suffer from the magnitude of attributes. 

**Corollary 2**.: _Compared to \(d_{E}(_{1},_{2})\), \(d(_{1},_{2})\) in Eq. (6) provides a smaller distance for \(_{1}\) and \(_{2}\) coming from the same class._

Proof.: \(}^{T}\) causes the original attribute space to be rotated according to the direction of the eigenvectors of \(}_{q}\), and \(^{-1}\) causes the rotated attribute space to be scaled according to the eigenvalues \(}_{q}\). Referring to the principle of principal component analysis , the eigenvectors of \(}_{q}\) reflect the principal component directions of \(D_{q}\). This means that \(d(_{1},_{2})\) will provide a smaller distance for instances coming from the same class compared to \(d_{E}(_{1},_{2})\). 

**Assumption 2**.: _When we estimate \(}_{k}^{-}\) based on \(}_{k-1}\), the estimated error satisfies N(0, \(_{k}^{-}\)). When we measure \(}_{k}\) by Eq. (10), the measurement error satisfies N(0, \(\))._

The Kalman filter we designed as Eq. (11) works based on Assumption 2, which usually holds because the noise in practice usually satisfies a normal distribution. Since \(}_{k-1}\) changes in each time index, the variance of the estimated error \(_{k}^{-}\) changes with the time index. Since Eq. (10) remains constant, the variance of the measurement error \(\) is constant. According to Assumption 2, the following theorem can be proved:

**Theorem 2**.: _When the Kalman gain \(_{k}\) takes the value \(_{k}^{-}}{_{k}^{-}+}\), the error between the filtered margin \(}_{k}\) and the true margin \(_{k}\) is minimized._

Proof.: When Assumption 2 holds, due to \(}_{k}=}_{k}^{-}+_{k}*(}_{k}-}_{k}^{-})\), it can be proved that minimizing the error between \(}_{k}\) and \(_{k}\) is equivalent to minimizing the variance of \(}_{k}\). Since \(}_{k}^{-}\) and \(}_{k}\) are independent of each other, the following equation can be derived:

\[ Var(}_{k})&=Var(}_{k}^{-}+_{k}*(}_{k}-}_{k}^{-}))\\ &=(1-_{k})^{2}*Var(}_{k}^{-})+_{k}^{2}*Var(}_{k}),\] (15)

where \(Var()\) denotes the variance of the variable. According to Assumption 2, \(Var(}_{k}^{-})\) equals to \(_{k}^{-}\) and \(Var(}_{k})\) equals to \(\). To minimize the error between the filtered margin \(}_{k}\) and the truemargin \(_{k}\), we can calculate the partial derivative \(}_{k})}{_{k}}\) as follows:

\[}_{k})}{_{k}}=-2*(1- _{k})*_{k}^{-}+2*_{k}*.\] (16)

Ultimately, it can be proved that \(_{k}\) is equal to \(_{k}^{-}}{_{k}^{-}+}\) by setting \(}_{k})}{_{k}}\) to 0. 

**Theorem 3**.: _The larger \(}_{k}\) is, the better the corresponding neighborhood size \(k\) is._

Theorem 3 ensures the effectiveness of KFNN in determining the optimal neighborhood size by the max-margin learning, and its proof is provided in Appendix B due to the limited pages.

## 5 Experiments

### Experimental setup

To evaluate the effectiveness of KFNN, we construct extensive experiments on the whole 34 simulated and two real-world crowdsourced datasets published on the Crowd Environment and its Knowledge Analysis (CEKA)  platform. For simulated datasets, we first use the unsupervised attribute filter _ReplaceMissingValues_ in the Waikato Environment and Knowledge Analysis (WEKA)  platform to replace all missing values. Subsequently, with the CEKA platform, we hide true labels of simulated datasets and simulate five workers whose label qualities are randomly generated from a normal distribution with N(0.65, 0.05\({}^{2}\)) to annotate these datasets. The real-world datasets, _Income_ and _Leaves_, which were both collected from the online platform Amazon Mechanical Turk (AMT), can be used directly without any processing since they do not contain missing values.

We compare our KFNN with six state-of-the-art label integration algorithms. Among them, MV (majority voting)  is the simplest label integration algorithm and is used as a baseline for all algorithms. IWMV (iterative weighted majority voting) , AALI (attribute augmentation-based label integration) , and LAGNN (label aggregation with graph neural networks)  are three state-of-the-art label integration algorithms that do not leverage neighbor instances. LAWMV (label augmented and weighted majority voting)  and MNLDP (multiple noisy label distribution propagation)  are two state-of-the-art label integration algorithms that leverage neighbor instances. For MV, we use the existing implementation of the CEKA platform. For IWMV, AALI, LAGNN, LAWMV, and MNLDP, we use the implementations provided by their authors. All parameters of the comparison algorithms are set to the recommended values in the corresponding published papers. In addition, since true labels are unknown in our experiments, we use the lazy version of LAGNN. In our KFNN, \(\) and \(\) are set to 0.1 and 1 by default.

The performance of each algorithm is evaluated using the Macro-F1 score, which highlights the performance of algorithms on different classes and better reveals algorithmic limitations compared to traditional integration accuracy. Due to the limited pages, more detailed descriptions of the experimental datasets and metrics are provided in Appendix C. All experiments are independently repeated ten times on a Windows 10 machine with an AMD Athlon(tm) X4 860K Quad Core Processor @ 3.70 GHz and 16 GB of RAM, and we report the average results of ten experiments.

### Results and discussions

Simulation experiment results.Table 1 shows the detailed Macro-F1 score (%) comparisons of each label integration algorithm on each simulated dataset, respectively. Based on these results, we perform the Wilcoxon signed-rank test  to further compare each pair of algorithms. Table 3 summarizes the Wilcoxon test results. In Table 3, the symbol \(\) indicates that the algorithm in the row significantly outperforms the algorithm in the corresponding column, the symbol \(\) indicates the exact opposite of that indicated by the symbol \(\), and the missing item indicates no significant difference between the algorithm in the row and the algorithm in the column. The significance levels of the lower and upper diagonals are \(=0.05\) and \(=0.1\), respectively. Based on these experimental results, we can summarize the following highlights: 1) The average Macro-F1 score of KFNN on all datasets is 79.64%, which is much higher than those of MV (72.46%), IWMV (72.71%), AALI (72.95%), LAGNN (73.71%), LAWMV (73.44%) and MNLDP (76.68%). KFNN achieves the highest Macro-F1

[MISSING_PAGE_EMPTY:9]

Figure 1(a) and Figure 1(b) show the Macro-F1 score of KFNN on _Income_ and _Leaves_ when \(\) and \(\) vary. Based on these results, we can find that KFNN is more sensitive to \(\) compared to \(\). As \(\) tends to 1, KFNN tends to achieve optimal performance. Therefore, the default value of \(\) in this paper is set to 1. \(\) hardly affects the performance of KFNN, which is set to 0.1 by default.

Ablation experiment.There are two components in KFNN, namely label distribution enhancement (LDE) and K-free optimization (KF). To validate their effectiveness, we observe the Macro-F1 score of KFNN after taking away each component on the _Income_ dataset. For simplicity, we use "KFNN-KF" to denote the variant of KFNN after taking away the component KF. Similarly, we create its another two variants "KFNN-LDE" and "KFNN-KF-LDE". Based on the results shown in Figure 2(a), it can be seen that the performance becomes worse when any component is taken away. These results validate the effectiveness of LDE and KF. Figure 2(b) shows the change of the class margin before and after using our designed Kalman filter (observed on the first instance of _Income_). As can be seen from Figure 2(b), compared to the margin before filter (\(}_{k}\)), the filtered margin (\(}_{k}\)) changes smoother. These results validate the effectiveness of our designed Kalman filter, which successfully mitigates the impact of noise incurred by neighbor instances.

## 6 Conclusion and future work

To ensure that each instance in crowdsourced datasets has a free neighborhood size, we propose a novel algorithm called KFNN. KFNN consists of two key components, namely label distribution enhancement and K-free optimization. Label distribution enhancement fuses the information from the attribute space and the multiple noisy label space. K-free optimization automatically determines the optimal neighborhood size for each instance by the max-margin learning. Both theoretical analysis and experimental results validate the effectiveness and robustness of KFNN.

Nevertheless, there are still some limitations in KFNN that can be improved in the future. For example, the parameters \(\) and \(\) in the Kalman filter designed by KFNN can not automatically adapt to the dataset, which restricts the robustness of KFNN. In addition, in Eq. (4), transforming the distance distribution into the potential label distribution using max-min normalization is rough. Considering that the distance metric is not effective across all datasets (e.g., _autos_ and _breast-cancer_ in Table 1), this transformation may also lead to KFNN performing poorly. In the future, we will design more sophisticated parameters and transformations to improve KFNN.

Figure 1: The Macro-F1 scores (%) and integration accuracies (%) of KFNN and its comparison algorithms on the _Income_ and _Leaves_ datasets.