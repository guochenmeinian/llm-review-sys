# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

instance, semantic and panoptic segmentation [1; 2; 3; 4; 5; 6; 7]. Most recently, we are observing a clear trend toward more flexible segmentation models in different aspects: 1) From closed-set to open-vocabulary segmentation. Many recent works proposed to either leverage contrastive learning methods or pretrained multi-modal foundation models (_e.g._, CLIP ) to make the segmentation models more transferable to unseen concepts [9; 10; 11; 12]; 2) From generic to referring segmentation. In addition to generic segmentation that segments an image thoroughly given a predetermined set of concepts, language-based referring segmentation provides a user-friendly way of segmenting a specific region referred by an arbitrary text phrase [13; 14; 15; 16; 17]; and 3) From one-shot to interactive segmentation. In practice, segmentation models do not necessarily produce satisfactory masks in one round. As such, people are also studying how to progressively refine the segmentation results through intimate interactions between humans and models [18; 19; 20; 21].

Despite the aforementioned efforts taken to design more powerful and feasible segmentation models, we are still lacking a universal segmentation interface that is capable of accommodating various types of human prompts and tackling different segmentation tasks as studied in individual works. In contrast, Large Language Models (LLMs) have already emerged as such a universal interaction interface for language tasks, from early models like GPT-3  and T5 , to conversational agent  augmented by advanced prompting [25; 26; 27] and chain-of-thought [28; 29; 30]. In this work, we strive for a universal interface for _segmenting everything everywhere all at once_ in an image. On this interface, we are targeted at unifying all segmentation tasks with a single model in a promptable manner. To achieve this goal, we propose a new prompting scheme in mask decoder that has four important properties: _versatility_, _compositionality_, _interactivity_, and _semantic-awareness_. Specifically, we propose to encode points, masks, text, boxes, and even a referred region from another image into prompts in the same joint visual-semantic space. As such, our model can deal with any combination of the input prompts, leading to strong compositionality. To enable interactivity, we further introduce memory prompts for condensing the previous segmentation information followed by communication with other prompts. As for semantic awareness, our model can provide an open-set semantic label to any output segmentation.

With the proposed prompting scheme, we build a segment-everything-everywhere model called _SEEM_ comprised of a simple Transformer encoder-decoder architecture [31; 6] with an extra text encoder [11; 32]. In _SEEM_, the decoding process emulates a generative LLM but with a multimodality-in-multimodality-out interface. An image encoder and text encoder are used as the prompt encoder to encode all types of queries, which are fed into the decoder. Concretely, we encode all spatial queries, namely, points, boxes, scribbles and masks into _visual prompts_ by pooling their corresponding visual features from the image encoder, and use the text encoder to convert text queries into _text prompts_. By training on diverse segmentation tasks, our model learns to deal with various prompts, align the visual and text prompts, and promote their synergy via cross-attention between them. As a result, our single model after pretraining attains competitive performance across all segmentation tasks. Since the prompts of all \(5\) different types are mapped to the _joint visual-semantic space_, we can feasibly combine prompts to resolve the ambiguity to obtain better segmentation results and enable zero-shot adaptation to unseen user prompts. Furthermore, our model can immediately generalize to the case of using an exemplar image segment as the prompt and video object segmentation in a zero-shot fashion. In addition to its strong generalization capability, _SEEM_ is also more efficient for interactive segmentation compared with the counterparts like SimpleClick . Since we take the prompts as input to the decoder, when doing multi-round interactions with humans, our model only needs to run the feature extractor once at the beginning and lightweight decoding each per round. To the end, we build a segmentation interface with a single pre-trained model that can segment every object with semantics (everything), cover every pixel in the image (everywhere), and support all possible compositions of prompts (all at once). In summary, our contributions are threefold:

* We design a new prompting scheme that can encode various user intents into prompts in a _joint visual-semantic space_, enabling strong flexibility for various segmentation tasks and generalization capability to unseen prompts or their combinations.
* We build _SEEM_, a universal and interactive segmentation interface that integrates the newly designed prompting mechanism into a lightweight decoder for _all_ segmentation tasks, leading to a model possessing properties of versatility, compositionality, interactivity, and semantic awareness.
* We conduct extensive experiments and visualizations to show that our model has strong performance on many segmentation tasks including open-vocabulary generic segmentation, interactive segmentation, referring segmentation, and segmentation tasks with combined prompts.

## 2 Related Work

**Interactive segmentation.** Interactive segmentation is the task of segmenting objects by interactively taking user inputs. It has been a longstanding problem and has achieved considerable progress [33; 34; 35; 20; 21; 36]. Generally, the interaction types can take various forms, such as clicks, boxes, polygons, and scribbles, among which click-based interaction models are the most prevalent. Concurrent to our work, SAM  proposed a promptable segmentation model trained on 11 million images and 1.1 billion masks. It takes user interactions as prompts for general segmentation. Though SAM demonstrates strong zero-shot performance, it produces segmentations without semantic meaning. In addition, its prompt types are limited to points, boxes, and text, whereas our model can also take in a referred region from another image as a prompt.

**Generic segmentation.** Segmentation of visual concepts has been a persistent challenge in the field of computer vision, as evidenced by its extensive literature [37; 38; 39; 40]. Generic segmentation techniques encompass several subtasks, including instance segmentation, semantic segmentation, and panoptic segmentation [4; 2; 3], each focusing on a different semantic level. For example, semantic segmentation aims to identify and label each pixel within an image based on its corresponding semantic class [41; 6; 42]. On the other hand, instance segmentation involves grouping pixels that belong to the same semantic class into separate object instances [4; 43; 7]. Recently, the Detection Transformer (DETR), a model based on the Transformer  architecture, has made significant advances in segmentation [45; 6; 7; 46; 47] tasks. However, these approaches cannot recognize objects absent in the training set, which constrains the model to a limited vocabulary size.

**Unified vision models.** Unified vision models [11; 48; 49; 36; 50] have recently drawn a lot of attention because of their advantage in generalizing to various tasks and flexibility. These models can deal with multiple vision tasks or data distributions. Among them, some [11; 48; 49] train multiple tasks together with only one model and thus can deal with all training tasks without finetuning on each target task. On the other hand, SAM  and SegGPT  propose training strategies that enable their models to handle new tasks and data distributions in a zero-shot manner. The second approach is more favorable since there is no need to resolve conflicts among tasks during training.

## 3 Method

### Model Design

_SEEM_ employs a generic encoder-decoder architecture but also employs a sophisticated interaction scheme between queries and prompts, as shown in Fig. 2 (a). Given an input image \(^{H W 3}\), an image encoder is first used to extract image features \(\). Then, _SEEM_-Decoder predicts the masks \(\) and semantic concepts \(\) based on the query outputs \(_{h}^{m}\) (mask embeddings) and \(_{h}^{c}\) (class embeddings), which interact with text, visual, and memory prompts \(_{t},_{v},_{m}\):

\[_{h}^{m},_{h}^{c} =(_{h};_{t},_{v},_{m}|)\] (1) \[ =(_{h}^{m})\] (2) \[ =(_{h}^{c})\] (3)

Figure 2: Overview of _SEEM_- Decoder. (a) _SEEM_ encodes image, text, and human inputs into _joint visual-semantic space_ as queries, features, and prompts, and then decodes queries to class and mask embeddings. (b) With the benefit of _SEEM_ decoder, the machine loop enables memorizing history mask information, and the human loop provides new corrections to the next round.

where \(_{h}\) is the learnable queries, and \(_{t}\), \(_{v}\), \(_{m}\) represent the text prompts, visual prompts, and memory prompts, respectively. During training, \(_{h}\) is duplicated for generic, referring, and interactive segmentation, as shown in Fig. 3. The corresponding prompts interact with their queries through self-attention. The learnable queries can freely interact with all prompts at inference time, thereby enabling zero-shot composition. Our design is inspired by the successful practice in X-Decoder . However, we highlight the differences in Eq. (1), marked in red, which allow for a universal model for image segmentation with the following properties:

Versatile. In _SEEM_, we introduce visual prompts \(_{v}\) to handle _all_ non-textual inputs, such as points, boxes, scribbles, _and_ a referred region from another image. These non-textual queries are beneficial to disambiguate the user's intent when textual prompts alone fail to identify the correct segment. For interactive segmentation, previous works either convert spatial queries to masks and feed them into the image backbone  or use different prompt encoders for each input type (points, boxes) . The first approach can be too heavy in applications because each interaction requires the image to go through the feature extractor. The second approach is hard to generalize to unseen prompts. To address these limitations, we propose a visual sampler (Fig. 2 (a)) to convert all kinds of non-textual queries to visual prompts that lie in the same visual embedding space:

\[_{v}=(,})\] (4)

where \(}\) is the feature maps extracted from either the target image (_i.e._, \(}=\)) or a referred image, and \(s\{,,,\}\) are the sampling locations specified by the user. We first pool the corresponding region from the image feature through point sampling . For all visual prompts, we interpolate at most 512 point feature vectors uniformly from the region specified by the prompt. A notable merit of our proposed method is that the visual prompts are naturally well-aligned with the textual prompts, as our model continuously learns a common visual-semantic space through panoptic and referring segmentation.

Compositional. In practice, a user may cast their intent using different or combined prompt types. Hence, a compositional approach to prompting is essential for real-world applications. However, we confront two issues during model training. First, the training data usually only covers a single type of interaction (e.g., none, textual, visual). Second, although we use visual prompts to unify all non-textual prompts and align them with textual prompts, their embedding spaces remain inherently different. To mitigate this problem, we propose to match prompts of different types with different outputs. Considering that visual prompts \(_{v}\) come from image features while textual prompts \(_{t}\) come from the text encoder, we select matched output indices for visual and textual prompts by matching them with the mask embeddings \(_{h}^{m}\) or class embeddings \(_{h}^{c}\), respectively:

\[ID_{v} (_{h}^{m}_{v}+ _{mask})\] (5) \[ID_{t} (_{h}^{c}_{t}+ _{mask})\] (6)

where \(_{mask}\) is the IoU between ground-truth and predicted masks. The proposed separate matching method outperforms approaches that only match with either \(_{h}^{m}\) or \(_{h}^{c}\) for all prompts.

After training, our model becomes familiar with all prompt types and supports a variety of compositions, such as no prompts, one prompt type, or both visual and textual prompts using the same model and weights. _In particular, the visual and textual prompts can be simply concatenated and fed to SEEM-Decoder, even though it was never trained in this way._

Interactive. Interactive segmentation usually cannot be completed in one shot and requires multiple interaction rounds for refinement, similar to conversational agents like ChatGPT. In _SEEM_, we

Figure 3: Queries and prompt interaction during training and evaluation. (a) Learnable queries are duplicated as object, grounding, and visual queries with the same set of weights for each task. (b) Attention mask between any two kinds of tokens (denoted as \(qpm\) in Algorithm. 1). Tentative means the interaction is not trained but able to do inference without any modification.

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

6.0 mIoU, and 9.3 AP50 points for the tiny model. And this gap is retained for the base and large model. Specifically, this number is computed by class embeddings \(_{h}^{c}\) (Output-Q-Textual). The margin is even larger when computed with mask embeddings \(_{h}^{m}\) (Output-Q-Visual) as shown in Table 5. Further, we benchmark the vanilla composition (Ensemble) that directly combines visual and text mask output probabilities as shown in Table 5 row 2.

**Interactive segmentation** As shown in Table 1, our approach achieves comparable performance with the specialized models, e.g. RITM, SimpleClick, and better performance than SAM  (B) which is trained with \( 100\) more segmentation data than ours. Notably, unlike existing interactive models, _SEEM_ is the first interface that supports not only classical segmentation tasks but also a wide range of user input types, including text, points, scribbles, boxes, and images, providing strong compositional capabilities as shown in Table 2,5.

**User input type of interactive segmentation** In Table 2, we compare 1-IoU of _SEEM_ with other strong baselines SimpleClick and SAM with \(5\) common types of prompts on three datasets. 1-IoU indicates the mean IoU of all images with a single click. The prompt types include point, stroke, scribble, and box. The results show that our _SEEM_ achieves the best performance in the extremely limited number of clicks over all three datasets.

**Video object segmentation** Without any modification, our model is able to do (interactive) video object segmentation in a zero-shot manner through the visual prompt (by replacing the current image visuals prompt with the visual prompts from another image). As shown in Table 3, without any observation of DAVIS/VOS dataset , our approach is able to achieve close performance in a zero-shot manner with a fully supervised method on DAVIS17 dataset . Meanwhile, our model is able to do interactive video object segmentation on DAVIS16-Interactive  and achieves comparable performance with the supervised baselines with one single click of the first frame.

   &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & \\  Baseline & Y & 0 & ✓ & ✗ & 50.7 & 39.5 & 60.8 & 57.9 & 63.3 & 71.6 & 1.4 & 54.3 & 59.6 & 55.8 & 63.5 \\  \(\) LVIS & ✓ & 2 & ✓ & ✓ & 51.0 & 39.8 & 62.2 & 58.6 & 63.9 & 72.6 & 1.57 & 4.91 & 59.5 & 55.9 & 63.1 \\  \(\) Negative & ✓ & 0 & ✓ & ✓ & 50.9 & 39.8 & 61.4 & 58.8 & 64.0 & 72.6 & 1.81 & 54.1 & 60.1 & 56.3 & 63.9 \\ \(\) Scratch & ✗ & 3 & ✓ & ✓ & 50.2 & 39.5 & 60.7 & 51.4 & 59.2 & 67.0 & 1.45 & 4.41 & 60.6 & 57.7 & 63.4 \\  \(\) & ✓ & 1 & ✓ & ✓ & 50.7 & 39.7 & 60.5 & 58.3 & 63.4 & 71.3 & 1.76 & 51.4 & 59.2 & 55.4 & 63.0 \\ \(\) lier & ✓ & 2 & ✓ & ✓ & 50.5 & 39.5 & 61.0 & 58.0 & 63.2 & 71.6 & 1.78 & 52.0 & 59.6 & 56.2 & 63.0 \\ \(\) & ✓ & 3 & ✓ & ✓ & 50.4 & 39.5 & 61.0 & 58.0 & 63.0 & 71.5 & 1.55 & 4.67 & 59.9 & 56.4 & 63.5 \\  & ✓ & 5 & ✓ & ✓ & 50.6 & 39.4 & 60.9 & 58.4 & 63.4 & 71.6 & 1.54 & 4.59 & 59.7 & 56.3 & 63.1 \\  

Table 4: **Ablation study** on interaction strategy. “#Iter” denotes the maximum training iteration on interactive segmentation in a single forward. “Negative” means adding negative tokens during interactive segmentation. “Scratch” means the model trains from scratch.

   &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & & & & \\   &  &  & ✗ & ✗ & 67.4 & 64.9 & 60.9 & 60.9 & - & 71.3 & 73.3 & 65.5 & 78.2 & 72.1 \\  & & & & & & & & & & & & & & & & & \\   &  &  & ✗ & ✗ & 60.5 & 62.2 & 72.7 & - & - & 70.6 & 60.9 & - & 61.2 & 60.7 & 60.9 & - & 61.2 \\  & & & & & & & & & & & & & & & & & & & \\   &  &  &  & ✗ & ✗ & 63.1 & 53.1 & 57.4 & - & - & 60.2 & 62.8 & 62.4 & 60.7 & 61.7 & 65.0 \\  & & & & & & & & & & & & & & & & & & & \\   &  &  & ✗ & ✗ & 64.1 & 53.1 & 57.4 & - & - & 60.2 & 60.1 & 58.1 & 58.9 & 58.3 & 59.7 \\  & & & & & & & & & & & & & & & & & & & \\   &  &  & ✗ & ✗ & 64.1 & 53.1 & 57.4 & - & - & - & 60.2 & 58.1 & 58.1 & 58.6 & 72.7 & 66.2 \\  & & & & & & & & & & & & & & & & & & & \\   &  &  & ✗ & ✗ & 64.1 & 53.1 & 57.4 & - & - & - & 60.2 & 58.1 & 58.6 & 72.7 & 66.2 \\  & & & & & & & & & & & & & & & & & & & \\   &  &  & ✗ & ✗ & 64.1 & 53.1 & 57.4 & - & - & - & 60.7 & 58.5 & 58.6 & 72.4 & 60.7 & 63.0 \\  & & & & & & & & & & & & & & & & & & & \\   &  &  & ✗ & ✗ & 64.1 & 53.1 & 57.4 & - & - & - & 70.6 & 58.1 & 58.9 & 58.6 & 72.4 \\  & & & & & & & & & & & & & & & & & & & & & \\   &  &  & ✗ & ✗ & 64.1 & 53.1 & 58.0

### Ablation Study

We conduct an ablation study on all the training segmentation tasks and zero-shot video object segmentation, dissecting each component of our model. The results are presented in Table 4.

_LVIS mask annotation will improve interactive segmentation results._ We replace the COCO mask with an overlap IoU larger than 0.7 with LVIS mask during training. This will improve the performance on interactive segmentation with 0.3 and 0.2 point gain on NoC0.9 and NoC0.85.

_Training from scratch only hurts referring segmentation performance._ We compare the _SEEM_ model trained with X-Decoder pre-trained checkpoint or the checkpoint initialized with UniCL or Florence vision and language backbone (+Scratch). It indicates that training from scratch will slightly improve the performance on interactive segmentation but hurt the referring segmentation performance.

_Increase interactive training iterations does help._ As shown in Table 4, increasing the training iteration (the first N-1 iteration is without gradient) from 1 to 5 will gradually improve the interactive segmentation performance from 5.41 to 4.59 on NoC0.9. As the computation cost increases with more clicks, we use iteration 3 for the main paper results.

### Qualitative Results

We further qualitatively evaluate _SEEM_. Based on the proposed prompting scheme and decoder design, with the same suite of parameters, _SEEM_ supports a wide range of visual input types.

**Visual prompt interactive segmentation**. In Fig. 4, we show the visualization of using _SEEM_ to segment objects in an interactive way. The user can segment objects of interest by simply clicking or drawing a scribble. Taking these prompts, _SEEM_ can simultaneously produce both masks and semantic labels for the objects. Note that our model is open-vocabulary, which empowers it to label unseen categories when given the candidate vocabulary (i.e., cheetah and butterfly in Fig. 4). When no vocabulary is given, _SEEM_ can segment in a class-agnostic manner.

**Text referring segmentation**. We show the text referring to segmentation visualization results in Fig. 5. The results demonstrate that our model is semantic-aware of open-vocabulary concepts and

Figure 4: Click/scribble-based segmentation. _SEEM_ supports arbitrary formats of clicks or scribbles by users. Moreover, it simultaneously gives the semantic label for the segmented mask, which is not possible in SAM .

Figure 5: Text to mask or text referring segmentation. The referred text is shown on the masks. _SEEM_ adapts to various types of input images in the domain of cartoons, movies, and games.

attributes to understand language. In addition, _SEEM_ is able to generalize to unseen scenarios like cartoons, movies, and games.

**Visual referring segmentation**. In Fig.6, we show _SEEM_'s segmentation results when prompted with referring regions from another image. By simply drawing a click or scribble on one referring image, _SEEM_ can take it as input and segment objects with similar semantics on other images. Notably, this referring segmentation has a powerful generalization capability to images of other domains. For example, by referring to the elephant in the forest, another object of the same category can be segmented well under drastically different scenes like cartoons, plush toys, and grassland.

**Video object segmentation**. In Fig.7, we further show _SEEM_'s referring segmentation ability on the video object segmentation task in a zero-shot manner. By referring to the objects in the first frame with scribbles, _SEEM_ can precisely segment the corresponding objects in the following frames, even when the following objects change in appearance by blurring or intensive deformations.

## 5 Conclusion

We presented _SEEM_, which can segment everything (all semantics) everywhere (all pixels) all at once (all possible prompt compositions). Apart from performing generic open-vocabulary segmentation, _SEEM_ can interactively take different types of visual prompts from the user, including click, box, polygon, scribble, text, and referring region from another image. These visual prompts are mapped into a _joint visual-semantic space_ with a prompt encoder, which makes our model versatile to various prompts and can flexibly compose different prompts. Extensive experiments indicate that our model yields competitive performance on several open-vocabulary and interactive segmentation benchmarks. Further studies revealed the robust generalization ability of our model in accurately segmenting images based on diverse user intents. We hope our work will serve as a stepping stone toward a universal and interactive interface for image segmentation and beyond.

Figure 6: **Zero-shot** visual referring segmentation with _SEEM_. Given a referring image with simple spatial hints, _SEEM_ can segment the regions which are semantically similar in different target images.

Figure 7: **Zero-shot** video object segmentation using the first frame plus one stroke. From top to bottom, the videos are “parkour” and “horsejump-low” from DAVIS , and video 101 from YouCook2 . _SEEM_ precisely segments referred objects even with significant appearance changes caused by blurring or intensive deformations.

Acknowledgements.We would like to express our gratitude to Lei Zhang for his generous support. And express the appreciated for the valuable suggestions from Zhenyuan Yang, and discussion with Xiaoyu Xiang. In addition, this work was supported in part by NSF CAREER IIS2150012, NASA 80NSSC21K0295, the Institute of Information and communications Technology Planning and Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration).