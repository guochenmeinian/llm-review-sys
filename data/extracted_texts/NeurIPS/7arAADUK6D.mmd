# Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration

Yichong Huang\({}^{}\), Xiaocheng Feng\({}^{}\)\({}^{}\)\({}^{}\), Baohang Li\({}^{}\), Yang Xiang\({}^{}\), Hui Wang\({}^{}\)

**Ting Liu\({}^{}\), Bing Qin\({}^{}\)\({}^{}\)**

\({}^{}\)Harbin Institute of Technology

\({}^{}\) Peng Cheng Laboratory

{ychuang,xcfeng,baohangli,tliu,qinb}@ir.hit.edu.cn

{xiangy,wangh06}@ir.hit.edu.cn

###### Abstract

Large language models (LLMs) exhibit complementary strengths in various tasks, motivating the research of LLM ensembling. However, existing work focuses on training an extra reward model or fusion model to select or combine all candidate answers, posing a great challenge to the generalization on unseen data distributions. Besides, prior methods use textual responses as communication media, ignoring the valuable information in the internal representations. In this work, we propose a training-free ensemble framework DeePEn, fusing the informative probability distributions yielded by different LLMs at each decoding step. Unfortunately, the vocabulary discrepancy between heterogeneous LLMs directly makes averaging the distributions unfeasible due to the token misalignment. To address this challenge, DeePEn maps the probability distribution of each model from its own probability space to a universal _relative space_ based on the relative representation theory, and performs aggregation. Next, we devise a search-based inverse transformation to transform the aggregated result back to the probability space of one of the ensembling LLMs (main model), in order to determine the next token. We conduct extensive experiments on ensembles of different number of LLMs, ensembles of LLMs with different architectures, and ensembles between the LLM and the specialist model. Experimental results show that (i) DeePEn achieves consistent improvements across six benchmarks covering subject examination, reasoning, and knowledge, (ii) a well-performing specialist model can benefit from a less effective LLM through distribution fusion, and (iii) DeePEn has complementary strengths with other ensemble methods such as voting1.

## 1 Introduction

With the scaling of model capacities and data volumes, generative large language models (LLMs) have shown impressive language understanding and generation abilities, shedding light for artificial general intelligence . Due to diversities of data sources, model architectures, and training recipes, LLMs have different strengths and weaknesses in various tasks and cases. Therefore, recent research has explored the ensemble of LLMs to exploit the complementary potential .

Existing methods can be categorized into selection-based and fusion-based ensembling. Selection-based ensembling selects the best candidate answer from all individual LLMs' answers using an additionally trained reward model . Fusion-based ensembling combines all candidate answers using a trained fusion model . However, these approaches inevitably face significantchallenges in generalizing to unseen data distributions and base models. Besides, prior methods enable collaboration via conveying the textual responses between LLMs while ignoring the rich information (_e.g.,_ confidence and alternative answers) in the internal representations.

An ideal solution to this issue is to apply the well-established technology of prediction fusion. [36; 24; 7; 10]. For LLM ensemble, prediction fusion works at each decoding step, averaging the probability distributions from different LLMs to determine the next token. It could not only directly apply to the ensemble of any LLMs without extra parameter training, making it more general, but leverages the informative internal representations (_i.e.,_ probability distributions) as communication media. Unfortunately, the vocabulary discrepancy between different LLMs makes it unfeasible to average the distributions due to token misalignment.

In this work, we tackle this key challenge by drawing upon the cross-model invariance of relative representation, which represents each token using the embedding similarities of this token to a set of anchor tokens . Specifically, we propose an ensemble framework **DeeP**EN (**DeeP**arallel **E**nsemble), enabling distribution fusion for heterogeneous LLMs. **DeeP**EN transforms the probability distribution from the heterogeneous probability space to a homogeneous relative space, using a matrix formed by the relative representation of all tokens. Next, **DeeP**EN aggregates the relative representations of all probability distributions in the relative space, coordinating the decision on the next token. Finally, the result of aggregation is transformed back to the probability space of the main model using a search-based inverse transformation to determine the next token.

We conduct extensive experiments ranging from 2-model to 9-model ensembles, covering ensembles of models with parameters ranging from 6B to 70B, ensembles of dense and sparse models, and the ensemble of LLMs with specialist models. Experimental results on six widely-used benchmarks demonstrate that compared to baselines, **DeeP**EN achieves consistent improvements across all benchmarks. It is also discovered that **DeeP**EN has complementary strengths when combined with other ensemble methods.

## 2 Theoretical Analysis

We first introduce relative representation and then illustrate the theoretical support for our method.

### Relative Representation

Previous study discovers that despite the misalignment between latent spaces of different neural networks, the embedding similarity between samples do not change across models [21; 11; 23]. Specifically, Moschella et al.  propose relative representation, which represents each sample \(x^{(i)}\) by the embedding similarities to a set of anchor samples \(\) (\(x^{(i)}\) and \(\) are identically distributed):

\[_{x^{(i)}}=(cos(e_{x^{(i)}},e_{a^{(1)}}),...,cos(e_{x^{(i)}},e_{a^{( ||)}})), \]

where \(e_{(*)}\) denotes the embedding of samples, also is absolute representation.

It is empirically evidenced that relative representations possess cross-model invariance, _i.e.,_ the relative representation of the same sample keeps invariant across different models, which lays the theoretical foundation for our work to fuse heterogeneous probability distributions.

### Theoretical Support for **DeeP**en

Average probability distribution has been widely evidenced to effectively improve the predictive performance in the filed of image and text [2; 10]. For generative language models, as we understand, the underlying mechanism is to interpolate different output semantics represented by the probability distributions. However, for LLM ensemble, vocabulary discrepancy isolates these output semantics in semantic spaces with different basis vectors, making the interpolation infeasible. To tackle this challenge, we aim to enable the cross-model alignment for output semantics, _i.e.,_ find a transformation to map the output semantics into a universal space. To this effect, we propose to represent the output semantics with the convex combination of relative representations of all tokens where the weight is the probability assigned to the token.

Definition of output semantics in relative space.Formally, given the absolute representation of the output semantics \(\) and the relative representation matrix \(R^{|V||A|}\) where \(V\) is the vocabulary and \(A V\) is the anchor token set. The \(i\)-th row of \(R\) is the relative representation of word \(w^{(i)}\):

\[R[i]=(cos(e_{w^{(i)}},e_{a^{(1)}}),...,cos(e_{w^{(i)}},e_{a^{(|A|)}})), \]

and the relative representation of the output semantics \(\) is defined as: \(= R\).

Model-invariance of relative representation of output semantic.Next, we illustrate why this representation scheme could align the output semantics isolated in heterogeneous absolute spaces. First, considering two LLMs \(_{A}\) and \(_{B}\) with the same vocabulary (_e.g.,_ LLaMA2-7B and LLaMA2-13B). When expressing the same output semantic, these models output the same probability distribution (_i.e.,_ absolute representation) \(_{A}\) and \(_{B}\). Besides, they have the same (highly similar in practice) relative representation matrix due the vocabulary consistency and cross-model invariance of relative representation. Therefore, the relative representations of output semantics are also identical:

\[_{A}=_{A} R_{A}=_{B} R_{B}=_ {B}. \]

Then, let's consider a language model \(_{C}\) with a different vocabulary (_e.g.,_ Mistral). Based on the fact that different LLMs typically share mass tokens in their vocabularies (SSA), the vocabulary of model \(_{C}\) is identical to adding and removing partial tokens to the vocabulary of \(_{B}\), which leads to \(_{B}_{C}\) and \(R_{B} R_{C}\). However, in our study, we discover that this change to the vocabulary has not incurred significant influence on the relative representation of the unchanged tokens (_i.e.,_ the common tokens between \(_{B}\) and \(_{C}\)), as shown in Fig. 1. Therefore, we make the reasonable assumption that the local change in the vocabulary could hardly influence the relative space.

## 3 Methodology

In this section, we first introduce the overall process of our ensemble framework DeepPen and then describe the three parts of DeepPen in detail.

### Overview

We illustrate the process of DeepPen in Fig. 2. Given \(N\) models to ensemble, DeepPen first constructs their transformation matrices (_i.e.,_ relative representation matrices) mapping the probability distributions from the heterogeneous absolute spaces into the relative space (SS3.2). At each decoding step, all models perform prediction and output \(N\) probability distributions. These distributions are mapped into the relative space and aggregated (SS3.3). Finally, the aggregation result is transformed back into the absolute space of the main model, in order to determine the next token (SS3.4).

Figure 1: Visualizations for relative representations between models with the same vocabulary and between models with different vocabularies. PCA and K-means clustering are applied only for visualization. Different colors indicate different clusters of samples (word embeddings). The red block indicates the representation of tokens that only appear in Mistral’s vocabulary. Relative representation consistency is obtained by calculating the cosine similarity between the relative representations of the same token in different models.

### Construction of Relative Transformation

Given \(N\) models to ensemble, DeePen first finds out the intersection of vocabularies of all models, _i.e.,_ common token set \(C\), and samples a subset or uses the full set of common tokens as the anchor token set \(A C\). Next, for each model, DeePen calculates embedding similarities of each token to the anchor words, obtaining the relative representation matrix \(R\) (as shown in Eq.2). Finally, to overcome the relative representation degeneration of outlier words, which will be introduced later, we perform normalization on the relative representation of all tokens by a softmax operation so that it becomes a probability distribution. We denote the normalized representation matrix \(\):

\[[i]=softmax(R[i]). \]

Anchor Selection.The choice of anchor tokens is crucial for the relative representation capability. Previous research discovers that the capability improves as the number of anchor words increases . Therefore, we employ the full set of common words between LLMs as the anchor words. It is also empirically proved that this method performs more stablely on downstream tasks (SS5.2).

Normalization of relative representation matrix.In DeePen, the relative representation of each token is normalized by the softmax operation to avoid the relative representation degeneration of outlier words, which are referred to as words that are far away from other words (including the anchors) and become distinguishable in relative space since for being zero vectors. The softmax operation effectively resolves this problem by making each relative representation a probabilistic distribution instead of a zero vector.

### Aggregation in Relative Space

At each decoding step, once each model \(_{i}\) outputs the probability distribution \(_{i}\), DeePen transforms \(_{i}\) into the relative representation \(_{i}\) using the normalized relative representation matrix: \(_{i}=_{i}_{i}\), and aggregate all relative representations to obtain the aggregated relative representation:

\[}=_{i=1}^{N}_{i}_{i}, \]

where \(_{i}\) is the collaboration weight of model \(_{i}\).

Collaboration Weight.As our work focuses on enabling the distribution fusion of heterogeneous LLMs instead of finding the optimal collaboration weights, we follow the most common practice to uniformly aggregate the distributions (\(=1/N\), \(N\) is the number of models), which is named **DeePen-Avg.** Besides, we also adopt a simple and effective method of deducing weights, **DeePen-Adapt**, which heuristically sets a larger value to the model with a better performance on the development set: \(_{i}=s_{i}/_{j}s_{j}\), where \(s_{i}=Acc(_{i},^{dev})-\), \(Acc(,)\) indicates the average accuracy of model \(_{i}\) on the development set, and \(\) indicates the chance level on the evaluation task. Specifically, \(=0\) on the free-form generation tasks and \(=1/K\) on the \(K\)-choice tasks.

Figure 2: Overview of DeePen. The relative representation matrix of each LLM is directly derived by calculating the embedding similarities between each token with the anchor tokens.

### Inverse Transformation of Relative Representations

To decide the next token according to the aggregated relative representation, DeepEn aims to transform it from the relative space back to the absolute space of the main model, which is empirically selected with the best-performing model on the development set. To enable this inverse transformation, we adopt a search-based strategy, finding out the absolute representation whose relative representation is identical to the aggregated relative representation. This search problem is formulated as:

\[}_{i}=*{arg\,min}_{_{i} _{i}}(_{i},\ ), \]

where \(_{i}\) denotes the absolute space of model \(_{i}\), and \(()\) is the loss function to measure the distance between relative representations. In this work, we adopt the KL-divergence due to its convergence.

This search is iteratively conducted under the guidance of the gradient of the loss in Eq.6 with respect to the absolute representation \(_{i}\). Specifically, we initialize the start point of searching \(_{i}^{(0)}\) with the main model's original absolute representation, and update it as:

\[_{i}^{(t+1)}=_{i}^{(t)}-_{i}^{(t)}},t[0,T] \]

where \(\) is an important hyperparameter named the relative ensemble learning rate, and \(T\) is the iterations number named relative ensemble learning steps. Finally, we use the updated absolute representation \(_{i}^{(T)}\) to determine the emitted token.

## 4 Experiments

### Experimental Setup

Benchmarks.We mainly conduct experiments on six benchmarks, which can be categorized into:

* **Comprehensive Examination:** (1) MMLU (5-shot) , which covers 57 subjects that humans learn, and (2) ARC-C (0-shot) , collected from standardized natural science tests.
* **Reasoning Capabilities:** (1) GSM8K  (4-shot), which is a dataset of high quality problems at the grade school math level, and (2) PIQA  (0-shot), which is a commonsense reasoning dataset.
* **Knowledge Capacities:** (1) TriviaQA (5-shot) , collected by Trivia enthusiast authored, and (2) NQ (5-shot) , which is a QA corpus consists of queries issued to the Google search engine.

Evaluation.For all benchmarks, we follow the test scripts of OpenCompass leaderboard. Specifically, on the multiple-choice tasks (MMLU, ARC-C, and PIQA), the option with the highest likelihood is selected to calculate the accuracy. On the free-form generation tasks (GSM8K, TriviaQA and NQ), we calculate the exact match (EM) accuracy.

Individual models.As ensemble learning typically works on models with comparable performance [24; 34], we select six well-performing LLMs whose performance are closely matched: LLaMA-2-13B , Mistral-7B-v0.1 , InternLM-20B , Yi-6B , Skywork-13B-base , and Tigerbot-13b-base-v2 . To achieve better ensemble performance, we conduct experiments on the ensemble of the top-2 models and the top-4 models for each benchmark. Besides, we also consider ensembling various number of models (SS4.3) and ensembling more diverse models (SS5.1).

Hyperparameters.In this work, we select all of the common tokens between LLMs as the anchor tokens to build the relative spaces, _i.e.,_\(A=C\) (SS5.2). For the inverse transformation of relative representations, we search the optimal relative learning rate (\(\) in Eq. 7) from 0.05 to 0.30 with an interval of 0.05. We empirically set the number of relative ensemble learning steps \(T=5\) (SS5.3).

Comparative methods.We compare DeepEn with (1) **MinED**[30; 9], which maps the probability distributions of heterogeneous LLMs to the distribution of the main model via aligning tokens in different vocabularies with edit distance, and (2) **LLM-Blender**, which comprises a reward model PairRanker to score each response of LLMs and a fusion model GenFuser to fusecandidate responses. In this work, we we only adopt the PairRanker since GenFuser suffers from serious over-generation under our training-free setting. In the ensemble of more than two models, we introduce two additional ensemble methods: (3) **Voting**, which selects the choice favored by most models on the tasks with outputs limited to a fixed set, and (4) **MBR**[8; 17], which selects the answer with the highest textual similarity to other candidate answers. The implementation details of baselines are illustrated in SSB.

### Main Results

The main results are shown in Tab. 1, from which we have drawn the following observations:

(1) DeePEn achieves consistent improvements over the individual models.These results prove that our DeePEn successfully enables collaboration between heterogeneous LLMs via aggregating their probability distributions in the relative space. Specifically, DeePen-Avg achieves improvements of +1.43(MMLU)\(\)+2.72(PIQA) on the ensemble of top-2 models, and +1.00(PIQA)\(\)+2.89(ARC-C) on the ensemble of top-4 model. DeePen-Adapt gains improvements of +1.44(TriviaQA)\(\)+3.34(ARC-C) on the ensemble of top-4 models.

(2) DeePEn shows better stability than baselines.As shown, LLM-Blender struggles to achieve improvements under the training-free setting. MinED shows unstable performance across different benchmarks. For example, MinED leads to performance drops of -35.40 on the GSM8K benchmark under the top-2 models ensemble setting and -2.70 on the TriviaQA, indicating the limitation of using textual similarity to align tokens in heterogeneous vocabularies. Through case studies, it is revealed that this method of aligning tokens with edit distance disturbs the decoding and produces incomplete words (demonstrated in SS7). Instead, DeePen-Avg achieves consistent improvements and surpasses all baselines in 7/12 settings.

(3) DeePEn has complementary strengths with other ensemble methods.Voting achieves a significant improvement on the mathematical reasoning GSM8K, showing the effectiveness of

    &  &  &  \\   & **MMLU** & **ARC-C** & **GSM8K** & **PIQA** & **TriviaQA** & **NQ** \\    \\  LLaMA2-13B & 55.07 & 59.32 & 29.80 & 59.68 & 74.32 & 28.67 \\ InternLM-20B & 59.94 & 75.81 & 53.83 & 64.78 & 66.88 & 26.09 \\ Skywork-13B & 61.16 & 66.50 & 53.90 & 74.04 & 58.65 & 19.75 \\ Tigerbot-13B & 51.95 & 57.44 & 48.82 & 68.28 & 66.22 & 22.71 \\ Mistral-7B & 62.13 & 73.33 & 47.50 & 65.61 & 73.18 & 27.62 \\ Yi-6B & 63.25 & 73.33 & 37.91 & 76.15 & 59.02 & 18.98 \\    \\  LLM-Blender & 63.85 (+0.60) & 75.73 ( - 0.08) & 54.89 (+0.99) & 78.31 (+2.16) & 74.10 ( - 0.22) & 28.61 ( - 0.06) \\ MinED & 65.04 (+1.79) & 77.35 (+1.54) & 18.50 (-35.40) & 78.98 (+2.83) & 72.30 ( - 2.02) & 28.45 ( - 0.22) \\
**DeePEn-Avg** & 64.68 (+1.43) & 77.52 (+1.71) & 55.42 (+1.52) & 78.87 (+2.72) & 75.90 (+1.58) & 30.17 (+1.50) \\
**DeePEn-Adapt** & 65.01 (+1.76) & 77.52 (+1.71) & 55.65 (+1.75) & 79.37 (+3.22) & 76.08 (+1.76) & 30.69 (+2.02) \\    \\  LLM-Blender & 61.44 ( - 1.81) & 71.03 ( - 4.78) & 43.37(-10.53) & 71.16 ( - 4.99) & 67.87 ( - 6.45) & 24.18 ( - 4.49) \\ Voting & 64.88 (+1.63) & 78.41 (+2.60) & 63.15 (+9.25) & 76.82 (+0.67) & — & — \\ MBR & — & — & 62.09 (+8.26) & — & 74.32 (+0.00) & 30.28 (+1.61) \\ MinED & 65.61 (+2.36) & 78.68 (+2.87) & 56.56 (+2.66) & 77.87 (+1.72) & 71.62 ( - 2.70) & 29.50 (+0.83) \\
**DeePEn-Avg** & 65.09 (+1.84) & 78.70 (+2.89) & 56.18 (+2.28) & 77.15 (+1.00) & 75.74 ( +1.42) & 31.55 ( +2.88) \\
**DeePEn-Adapt** & 65.25 (+2.00) & 79.15 (+3.34) & 56.25 (+2.35) & 78.59 (+2.44) & 75.76 (+1.44) & 31.77 ( +3.10) \\ \(\)+Voting/MBR & 65.40 (+2.15) & 79.44 (+3.63) & 65.25 (+11.35) & 77.37 (+1.22) & 75.65 (+1.33) & 32.11 (+3.44) \\   

Table 1: Main results. The best individual model is highlighted in \(}\), and the best ensemble method is highlighted in \(}\), except for the results of the combined method (i.e., the last row). The top-4 models on each benchmark are underlined. ‘—’ indicates that the method does not apply to the task.

reasoning with multiple paths. To evidence the complementary strength of DeepPen with Voting, we combine both methods. On the TriviaQA and NQ, Voting is replaced with MBR. As shown that the combination of both methods gains a further improvement over Voting (63.15\(\)65.25).

(4) Collaboration with more worse-performing LLMs is a double-edged sword.The ensemble performance of DeepPen-Avg with top-4 models surpasses that with top-2 models on 4 benchmarks, but falls short on 2 benchmarks. This is reasonable because incorporating the 3rd and 4th ranked LLMs enhances complementary strengths but also causes the interference with the top-2 models.

### Results on Different Numbers of Models

Next, we illustrate the effectiveness of DeepPen on the ensemble of more models on the MMLU, PIQA, and NQ. We add Nanbeige-16B into the ensemble on all three benchmarks, and add LLaMA2-70B and Mixtral-8\(\)7B on the PIQA due to their comparable performance. As illustrated in Fig. 3, the ensemble performance increases first and then decreases with the joining of more models in descending order of performance. And the ensemble performance peaks in the top-4 or top-5 models across three benchmarks.

## 5 Analysis

To deeply understand DeepPen, we first evaluate its performance on the ensemble learning of model sets with diverse architectures, abilities, and performance gaps. Next, we conduct a series of analyses on the reverse transformation process of relative representations.

### Results of Ensembling Diverse Models

Ensemble of the dense model and the sparse model.We first evaluate our method on the ensemble learning of the dense model and the sparse MoE model on the challenge reasoning tasks. Specifically, we use the widely-used large-scale dense model LLaMA2-70B  and the popular sparse MoE model Mixtral-8\(\)7B  as the base models. As the results shown in Tab. 2, our DeepPen achieves improvements of +1.60 and +3.22 on the GSM8K and PIQA datasets, even though the base models have achieved a high level of performance.

  
**Model** & **GSM8K** & **PIQA** \\  LLaMA2-70B & 63.84 & 71.27 \\ Mixtral-8\(\)7B & 65.73 & 71.88 \\ LLM-Biender & 64.52 (-1.21) & 74.54 (+2.66) \\ MinED & 67.10 (+1.37) & 75.65 (+3.77) \\ \(}\) & 67.33 (+1.60) & 75.10 (+3.22) \\   

Table 2: Ensemble learning of the _dense_ large language model LLaMA2-70B and the _specialist_ translator model NLLB on the _sparse_ MoE model Mixtral-8\(\)7B.

Figure 3: Test set results of ensemble learning on various number of models. Individual models are arranged in descending order of their performance on the development set, and sequentially incorporated into the ensemble. \(\) indicates the largest improvement achieved by DeepPen.

Ensemble of the generalist model and the specialist model.To investigate the effectiveness of DeePen on the ensemble of the generalist model and the specialist model for the specific task, we conduct experiments on the machine translation task using the ensemble of the large language model LLaMA2 and the machine translation model NLLB , which is a well-known open-source multilingual translator. We adopt the widely-used machine translation benchmark Flores-2002. As the results in Tab. 3 illustrated, DeePen achieves better translation performance leveraging the diverse translation knowledge in the generalist LLM and the specialist translator.

Ensemble of models with different performance gaps.To assess the stability of DeePen regarding to the performance gap of base models, we conduct an experiment on the ensemble of model pairs with increasing performance gaps. As the result demonstrated in Tab. 5, the performance of ensemble learning between a well-performing model (the rank-first model)with a worse-performing model could achieve improvements or slightly lag behind the well-performing model.

### Analysis on Relative Transformation

Effect of anchor selection.We demonstrate the impact of different numbers of anchor words through experiments with the top-2 ensemble models on the MMLU and ARC-C datasets. As shown in Fig. 4, an increased number of anchor words can improve performance for LLMs in downstream tasks, and selecting the full set of common words as anchors provides better performance.

Effect of normalization on relative representation matrix.To demonstrate the importance of normalization on the relative representation matrix to the ensemble performance (SS3.2), we conduct an ablation analysis. The result is shown in Tab. 4, the ensemble struggles to achieve improvements due to the ineffective representation of outlier words, _i.e.,_ words distant to other words. The proportion of outlier words can be derived from the distribution of distance to nearest neighbor words, which is illustrated in Fig. 8. As illustrated, a remarkable proportion (\(>30\%\)) of words are distant from other words, _i.e.,_ cosine similarity to its nearest neighbor word is less than 0.3. Through the normalization operation, the output semantics that intend to emit outlier words could be prevented from becoming zero vectors by relative transformation.

    &  &  \\   & ACC & \(\) & ACC & \(\) \\ 
**Baseline** & 61.19 & – & 72.74 & – \\
**DeePEN** & 63.61 & +2.42 & 74.79 & +2.05 \\   w/o. **Rel-Norm** & 60.73 & -0.46 & 72.95 & +0.21 \\   

Table 4: Ablation study of normalization on the relative representation matrix to the ensembling performance on the development sets. **Baseline** refers to as the best single model on each benchmark. **DeePen** refers the performance of ensembling top-2 models in the benchmark.

Figure 4: Effect of the number of anchor words. The **x-axis** indicates the number of anchor words randomly sampled from the common words for 4 times.

Figure 5: 2-model ensemble of the top-1 model (LLaMA2-13B) with different models on the NQ benchmark, respectively.

### Analysis of Reverse Transformation

To better understand the reverse transformation process (SS3.4) transforming the relative representation back to the absolute space of the main model, we further analyze each component of this process.

Analysis of relative ensemble learning rates.As shown in Tab. 5, the performance of DeePEn is sensitive to the value of relative ensemble learning rate (\(\)), which is abbreviated by RELR. This observation motivates us to measure the generality of this hyperparameter. Specifically, we illustrate the cross-distribution performance of the searched optimal value of \(\) in Tab. 9. As observed, the optimal value of RELR varies across different datasets, which suggests that the inverse transformation from relative space to absolute space requires adaptive mapping schemes.

Effect of iteration steps in relative ensemble learning.To give a deep view of the dynamics of the inverse transformation in DeePEn, we report the performance change along with different numbers of relative ensemble learning steps (\(T\)). Besides, the dynamics of loss of relative ensemble learning (\(\) in Eq. 6)is also reported. As shown in Fig. 9, on the one hand, more steps of relative ensemble learning significantly lead to lower losses. However, the loss is hard to reach zero, _i.e.,_ under-fitting. On the other hand, increasing the number of steps of relative ensemble learning will cause the performance to increase first and then decrease. The reason behind the performance drop could be that in the early stage of optimization, the focus of optimization is on updating the high-probability tokens. In the later stage of optimization, since the probabilities of all words will be adjusted equally, the high-probability tokens will be interfered with the high-probability ones, thus affecting the performance. Therefore, it is recommended to set a modest value of step number (_e.g.,_\(T=5\)).

## 6 Related Work

Selection-based ensemble._Rerank_ is an intuitive solution to utilize multi-model strengths. Jiang et al.  take the first step towards LLM ensemble, training a reward model PairRanker for pairwise comparison on candidate outputs. To overcome the huge computation costs of multi-LLM inference, several works have explored to train a _router_ to predict the best-performing model out of a fixed set of LLMs for the given input .

Fusion-based ensemble.Towards a synergy between LLMs, Jiang et al.  propose GenFuser, trained to combine multiple candidate answers. Different from these training-dependent ensemble methods which pose a great challenge to the generalizability of the reward model or fusion model, our DeePEn is completely training-free, making it more general. Similar to our method, MinED also aims to tackle the vocabulary discrepancy via aligning the tokens in different vocabularies based on edit distance . Unfortunately, this textual similarity-based method exhibits unstable performance and produces abnormal text for LLM ensemble (Tab. 7).

There are several contemporaneous works related to our work. Xu et al.  propose EVA to tackle vocabulary discrepancy by learning token alignment between different vocabularies with the assistance of overlapping tokens. Our DeePEn eliminates this training process via directly aligning tokens with the relative representation (more discussion is illustrated in SSB). Mavromatis et al.  explore adaptive collaboration weights at test time by harnessing the perplexity on the input prompt. We emphasize that this work is complementary to our work.

## 7 Conclusion

In this work, we propose a training-free LLM ensembling framework DeePEn, which addresses the vocabulary discrepancy when fusing the probability distributions of heterogeneous LLMs. Experimental results on six widely-used benchmarks demonstrate that DeePEn exhibits more stable

  
**RELR** (\(\)) & **0.05** & **0.10** & **0.15** & **0.20** & **0.25** & **0.30** \\ 
**MMLU** & \(\)2.42 & \(+\)1.57 & \(+\)1.77 & \(+\)1.96 & \(+\)1.31 & \(+\)1.31 \\ 
**TriviaQA** & \(+\)1.31 & \(\)2.05 & \(+\)1.63 & \(+\)1.94 & \(+\)1.82 & \(+\)1.26 \\   

Table 5: Sensitivity analysis of relative ensemble learning rate (**RELR**). We report the improvements of ensembling top-2 models over the best individual models.

performance than baseline methods and has complementary strengths with other ensemble methods such as Voting. We believe our work can inspire further research on the LLMs collaboration, model reuse, and knowledge distillation. In the future, we aim to explore more effective adaptive collaboration schemes to leverage the complementary strengths between different LLMs.