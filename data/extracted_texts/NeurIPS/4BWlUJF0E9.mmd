# Towards Dynamic Message Passing on Graphs

Junshu Sun\({}^{1,2}\) Chenxue Yang\({}^{3}\) Xiangyang Ji\({}^{4}\) Qingming Huang\({}^{1,2,5}\) Shuhui Wang\({}^{1,5}\)

\({}^{1}\)Institute of Computing Technology, CAS \({}^{2}\)University of Chinese Academy of Sciences

\({}^{3}\)Agriculture Information Institute, CAAS \({}^{4}\)Tsinghua University \({}^{5}\)Peng Cheng Laboratory

{sunjunshu21s,wangshuhui}@ict.ac.cn

yangchenxue@caas.cn xyji@tsinghua.edu.cn qmhuang@ucas.ac.cn

Corresponding author.

###### Abstract

Message passing plays a vital role in graph neural networks (GNNs) for effective feature learning. However, the over-reliance on input topology diminishes the efficacy of message passing and restricts the ability of GNNs. Despite efforts to mitigate the reliance, existing study encounters message-passing bottlenecks or high computational expense problems, which invokes the demands for flexible message passing with low complexity. In this paper, we propose a novel dynamic message-passing mechanism for GNNs. It projects graph nodes and learnable pseudo nodes into a common space with measurable spatial relations between them. With nodes moving in the space, their evolving relations facilitate flexible pathway construction for a dynamic message-passing process. Associating pseudo nodes to input graphs with their measured relations, graph nodes can communicate with each other intermediately through pseudo nodes under linear complexity. We further develop a GNN model named \(}\) based on our dynamic message-passing mechanism. \(}\) employs a single recurrent layer to recursively generate the displacements of nodes and construct optimal dynamic pathways. Evaluation on eighteen benchmarks demonstrates the superior performance of \(}\) over popular GNNs. \(}\) successfully scales to large-scale benchmarks and requires significantly fewer parameters for graph classification with the shared recurrent layer.

## 1 Introduction

The inherent irregular structure of graphs poses nontrivial challenges in graph learning . To enable effective learning on graphs, graph neural networks (GNNs)  have been specifically designed for graph-structured data. Within GNN models, message passing serves as a crucial function in extracting informative graph features . The vanilla message passing  applies node-centric aggregation constrained between the adjacent nodes. During this aggregation process, central nodes access their neighbors in an isotropic manner  and aggregate multi-hop features iteratively. In consequence, the vanilla message passing relies heavily on the input graph structure, leading to over-smoothing  or over-squashing  issues on GNNs.

To further improve the effectiveness of GNNs for graph representation learning, one straightforward solution is to

Figure 1: **Comparison between different connection patterns for pseudo nodes and graph nodes.**decouple message pathways from the input graph structure. Following this direction, methods [19; 12] have been proposed to perform message passing beyond input structures for global information exchange. Some methods directly model the pairwise relation between nodes [72; 35; 43], but the dense relation causes high computational and space complexity. Other methods incorporate pseudo nodes connected with all the graph nodes to serve as message pathways [22; 30; 42]. However, these models employ uniform pathways, _i.e._, each pseudo node is connected to all the graph nodes with equal weights, as illustrated at the top of Fig. 1. In consequence, an overwhelming number of node features are squashed equally into pseudo nodes, leading to information bottlenecks on pseudo nodes for message passing and less discriminative representations for downstream tasks .

The above limitations call for message passing with flexible pathways and low complexity. In this paper, we propose a novel dynamic message-passing mechanism. To construct flexible pathways, our method measures the specific spatial relations between nodes across time and gives rise to dynamic pathways, as illustrated at the bottom of Fig. 1. To reduce complexity, learnable pseudo nodes are introduced as message-passing proxies between pairs of graph nodes.

Specifically, both graph nodes and pseudo nodes are embedded in a common space with measurable spatial relations between them. By moving nodes in the space, their measured relations evolve accordingly, facilitating a dynamic message-passing process with flexibility. Regarding the measured relation as pseudo edges, the message passing on the input graphs can be extended to pseudo nodes. As a result, graph nodes can communicate with each other intermediately through pseudo nodes, free from dense relation modeling.

To achieve this dynamic process, we further develop a GNN model named \(}\), based on our graph \(\)odes and pseudo \(\)odes mechanism for message passing. \(}\) incorporates a recurrent layer to parameterize the displacements of graph nodes and pseudo nodes in the common space. With both types of nodes moving in the common space, \(}\) measures the actively changing spatial relations and constructs evolving pathways for dynamic message passing.

Our contributions are summarized as follows. First, we design a flexible and low-complexity message-passing mechanism from a new perspective, where dynamic message pathways are built upon evolving spatial relations between nodes. Second, we develop a novel GNN model named \(}\) to achieve dynamic message passing, which employs a recurrent layer to parameterize the evolutionary displacements of nodes. Third, we demonstrate the advantages of \(}\) on eighteen real-world benchmarks, where \(}\) achieves superior performance. Codes are available at https://github.com/sunjss/N2.

## 2 Related Work

**Flexible message passing on graphs.** In pursuit of expressive operators for graph learning, methods propose to approximate diverse filters with parameterized polynomials [11; 13; 34]. However, due to the prohibitive computational complexity of higher-order polynomials, these methods [33; 71] are constrained with lower orders and only perform local aggregation on graphs. This constrained process couples message passing with input topology and contains inherent limitations, including over-smoothing [39; 46] and over-squashing [2; 60]. To overcome these limitations, some works try to decouple message passing from input topology and introduce alternate pathways, including edge shortcuts [1; 60; 12; 24], pseudo nodes [42; 57], and graph pooling operations [20; 53; 74].

By adding edge shortcuts, methods aim to improve the message-passing efficiency on graphs. These methods can relieve certain bottlenecks on the input graphs [12; 50; 60] and aggregate multi-hop information during message passing [1; 24]. Notably, we categorize graph structure learning methods [79; 31; 65; 76] into the edge shortcut paradigm, which constructs message pathways by modeling edge connectivity between nodes. While edge shortcuts refine local connections, pseudo nodes directly enable global message passing. However, the pseudo nodes in prior works employ uniform pathways to connect with graph nodes, which become bottlenecks in message passing  and limit efficient global communication. Unlike these works, \(}\) models dynamic interactions between graph nodes and pseudo nodes, with edge weights varying flexibly across them.

Another line of effort in decoupling message passing from input topology is hierarchical GNNs. These methods [53; 74] learn multi-scale graph features through iterative graph pooling, _i.e._, node clustering or node drop. Node clustering [73; 3] learns soft assignment matrices to aggregate nodes into coarserlevels. On the other hand, node drop [20; 37] ranks and selects salient subsets to prune less critical nodes. Different from graph pooling methods, \(}\) performs both local and global message passing in each recursive step, avoiding information loss in coarser graphs .

**Reducing complexity for global message passing.** Self-attention  that models pairwise relations between nodes can be seen as message passing on fully connected graphs. However, dense attention requires quadratic space and computational complexity, which is intractable for large-scale graphs. In order to scale attention-based global message passing to larger graphs, recent methods propose to approximate dense attention through expander graphs , kernel functions , and diffusion . One similar work  to ours proves that message-passing layers with a single pseudo node can approximate dense attention. In this paper, we follow a contrary thread and develop \(}\) with a single shared message passing layer and multiple pseudo nodes. Each pseudo node interacts dynamically with graph nodes, avoiding becoming message-passing bottlenecks as the uniform connected pattern [22; 42].

**Recurrent layer for graph learning.** Scarselli et al.  first employ a recurrent layer to update node features recursively. According to Banach's fixed point theorem , implementing the recurrent layer as a contraction mapping guarantees the existence of a unique fixed point representation for any input graph, towards which the recursive updates converge. However, their contraction mapping formulation is topology-dependent, incurring over-smoothing as the number of recursive steps increases. In contrast, the recurrent layer in \(}\) decouples message passing from input topology, empowering flexible communication between graph nodes.

## 3 Towards Dynamic Message Passing

Our dynamic message-passing mechanism parameterizes message pathways with measurable relations between nodes in a common space. The displacements of nodes in the space give rise to evolving relations. As a result, the message-passing process also changes dynamically. This section defines the common space tailored for the dynamic message-passing process, where pseudo nodes are employed to reduce the computational complexity. The next section demonstrates how \(}\) learns the displacements of nodes towards the dynamic process.

### Preliminaries

**Notations.** Let \(=(,)\) be a graph, where \(=\{v_{1},,v_{n}\}\) denotes the node set of size \(n\) and \(=\{e_{v_{i},v_{j}}|v_{j}(v_{i})\}\) denotes the edge set of size \(m\). \(()\) denotes the one-hop neighbor set of a given node. Each node \(v\) corresponds to a feature vector \(_{v}^{d}\) where \(d\) is the number of features. Let \(=(_{v_{1}},,_{v_{n}})^{}^ {n d}\) be the node feature matrix composed of feature vectors. Let \(^{n n d_{e}}\) be the edge feature matrix with \(d_{e}\) features. \(_{i,j,}=_{v_{i},v_{j}}^{d_{e}}\).

**Parameterize pathways as shared functions.** When introducing pseudo nodes for message passing, uniform pathways [22; 42] directly parameterize the edge weights between pairs of graph nodes and

Figure 2: **Dynamic message-passing pathway construction in common state space. Graph nodes and pseudo nodes interact actively in the common state space, constructing dynamic message pathways through proximity measurement. In empirical model analysis, pseudo nodes tend to be attracted toward a distinct graph node cluster.**

pseudo nodes. Specifying different weights for each pair entails a parameter count scaling to the number of graph nodes. To ensure a tractable number of parameters for various scales of input graphs, we gain inspiration from DyN , a distinctive neural network model that directly models neurons, instead of learning connection weights as conventional approaches [36; 62]. The connection weights in DyN are parameterized with a path integral function given the spatial coordinates of neurons. By sharing the integral function across neurons, DyN circumvents the need for heavy parameters. Drawing an analogy between neurons and nodes, we can also parameterize relations between nodes as a shared function.

### Common State Space

In light of DyN, we propose to unify graph nodes and pseudo nodes in a common state space \(^{q}\) and define their spatial proximity with a shared measurement function. This function measures the relations between pairs of nodes, enabling the construction of dynamic message pathways. Here, we borrow the concept of _state_ from prior works, such as DyN and LSTM , to denote the learned descriptive embedding of nodes. Examples of the information encoded in the states include node features and local topology. The states of a node identify its spatial coordinate in the state space. For more discussion on "state space", please refer to Appendix A.

**Embedded node states.** Given a pseudo node set \(=\{u_{1},,u_{n_{p}}\}\) of size \(||=n_{p}\), we embed pseudo nodes in the state space \(\) as learnable parameters \(=(_{_{1}},,_{_{n_{p}} })^{}^{n_{p} q}\). These learnable elements are named "pseudo nodes" to align with "graph nodes" from inputs. Pseudo nodes can be associated with input graphs through pseudo edges and participate in the message passing between graph nodes. We will elaborate on this association after introducing the spatial proximity measurement in the state space. Given the graph node set \(\) with feature matrix \(\), we have graph node states \(=(_{1},,_{n})^{}=f() ^{n q}\), where \(f:^{d}\) is a permutation equivariant function. Displacements of a node in the state space signify shifts in its states, _i.e.,_ the learned descriptive embeddings. The objective of a GNN is to model the true distribution of graph nodes in the state space, wherein nodes with similar features obtain proximal embeddings. In the following, we refer to both graph nodes and pseudo nodes collectively as embedded nodes.

**Proximity measurement.** To model complex relations, we assume that the embedded nodes have a non-linear relationship in the state space \(\). However, modeling non-linearity via operations such as path integral can be computationally complex. To address this problem, we approximate non-linear relations with piece-wise weighted inner products. Taking proximity measurement between graph nodes as an example, each node is divided into \(k\) pieces with their proximity formulated as

\[(_{v},_{v^{}})=_{i=1}^{k}_{ i}_{iv}^{}_{iv^{}},_{i:}=_{i}(_{}),\] (1)

where \(_{i}\) is a learnable parameter, \(()\) denotes a non-linear function with linear mapping followed by \(()\). \((,)\) is termed as proximity instead of distance because it ranges in \(\). The spatial proximity is weighted with different inner product similarity between \(k\) pairs of pieces to approximate complex relations. We conduct ablation studies on the number of pieces \(k\) in Appendix D.5.

### Associating Pseudo Nodes to Input Graphs

**Pseudo edges.** Based on the measurable proximity between the embedded nodes, we now introduce how to obtain pseudo edges and thus associate pseudo nodes to input graphs. For each pseudo node \(u\) embedded at \(_{u}\) in the state space, it has pseudo edges with any pseudo node \(u^{}\) and graph node \(v\), where pseudo edge weights \(e_{u,u^{}}\) and \(e_{u,v}\) can be formulated as the proximity

\[e_{u,u^{}}=(_{u},_{u^{}}), e_{u,v}= (_{u},_{v}).\] (2)

**Messages for embedded nodes.** Following the common practice in GNNs , we interpret the interaction between the embedded nodes as message passing. To achieve this, both graph node \(v\) and pseudo node \(u\) learn their messages \(_{v},_{u}^{d}\) to be passed in the state space. As a result, graph node \(v\) and pseudo node \(u\) in the state space can be further described as

\[ u&(_{u},_{u},\{e_{u,w},e_{w,u}|w()\})\,,\\ v&(_{v},_{v},\{e_{v,w },e_{w,v}|w((v))\})\,.\] (3)The messages of graph nodes are initialized with node features. Each graph edge \(e_{v,v^{}}\) is characterized by its edge feature \(_{v,v^{}}\).

### Dynamic Message Passing in the State Space

Given the measured proximity as message pathways and the messages to be passed, the embedded nodes can interact with each other through message passing. Specifically, the interactions between graph nodes are performed in both local and global scope.

**Global message passing.** In the state space, graph nodes perform global message passing based on their states, where pseudo nodes serve as proxies. Given graph node states \(^{n q}\), pseudo-node states \(^{n_{p} q}\) and graph node messages \(^{}=(_{v_{1}},,_{v_{n}})^{ }^{n d}\), the global message-passing process can be formulated as

\[ =^{}^{}, ^{}_{ij}=e_{u_{i},v_{j}}=(_{i,},_{j,}),\] (4) \[ }=^{},^{}_{ij}=e_{u_{i},u_{j}}=(_{i,},_{j, }),\] (5) \[=(}),^{ }=(}),\] \[ ^{}=^{}^{ },\,\,^{}_{ij}=e_{v_{i},u_{j}}=(_{i, },[+]_{j,}),\] (6)

where \(^{}^{n_{p} n}\) denotes the edge weight matrix from graph nodes to pseudo nodes, \(^{}\) and \(^{}\) follow the similar name rule. An example of \(^{}_{ij}\) computation is illustrated in Fig. 2. Eq. 4 formulates the process that graph nodes diffuse messages to pseudo nodes. Eq. 5 formulates the global feature refinement at the pseudo-node level, where \(^{n_{p} q}\) denotes the learned displacements for pseudo nodes, \(^{}^{n_{p} d}\) encodes the refined pseudo-node messages with global information. Eq. 6 formulates the message collection process from pseudo nodes to graph nodes.

For simplicity, we compile Eq. 4-6 as \(^{},=(, ^{},)\). Note that the space complexity of our global message passing is \(O(knn_{p})\) with \(k,n_{p} n\), significantly lower than \(O(n^{2})\) in dense global message passing.

**Local message passing.** Topology-coupled message passing is employed to encode the local structure. The resulted local message-passing process for graph node \(v\) can be formulated as

\[^{}_{v}=(v)|+1}[_ {v}+_{v^{}(v)}(_{v^{}}|| _{v,v^{}})],\] (7)

where \(||\) denotes the concatenate operation. Through local message passing, graph nodes aggregate messages from their adjacent nodes. Eq. 7 can be compiled as \(^{}=(,)\) for all the graph nodes.

## 4 Implementing Dynamic Message Passing with \(^{}\)

We further develop a GNN model named \(^{}\) to move the embedded nodes to their optimal positions. By feeding the states recursively into a single recurrent layer, \(^{}\) learns the displacements of all the embedded nodes in the state space and updates their positions. The changes in position reshape the spatial relations between the embedded nodes and thus reshape the dynamic message pathways. These dynamically evolving pathways empower \(^{}\) to adapt to the specific positions of the embedded nodes at each recursive step. In this section, we first outline the key process in the \(l\)-th recursive step, _i.e._, pseudo-node adaptation and dynamic message passing, then describe different output fashion for downstream tasks.

### Pseudo-node Adaptation

Pseudo nodes are initialized randomly in the state space. To adapt to specific input graphs, \(^{}\) first diffuses graph node messages to pseudo nodes, adjusting pseudo-node states and corresponding messages accordingly. Given graph node messages \(^{(l-1)}\) (\(^{(0)}=\)), graph node states \(^{(l-1)}\) and pseudo-node states \(^{(l-1)}\) at the \(l\)-th recursive step, the adaptation process can be formulated as

\[}^{(l)},}^{(l)}&=(^{(l-1)}, ^{(l-1)},^{(l-1)}),\\ }^{(l)}&=^{(l-1)}+}^{(l)},\] (8)where \(}^{(l)}\) denotes pseudo-node messages that are collected by graph nodes, serving as query signals for different patterns on graphs. \(}^{(l)}\) denotes the adjusted pseudo-node states.

### Dynamic Message Passing

\(}\) performs both local and global message passing. At the local level, graph nodes exchange their own messages \(}^{(l-1)}\), collected messages \(}^{(l)}\) and graph node states \(^{(l-1)}\):

\[&^{(l)}= [(}^{(l-1)}\|}^{(l)}\|^{(l-1)}),],\\ &}^{(l)}=^{(l-1)}+(^{(l)}).\] (9)

Through local message passing, graph node messages \(^{(l)}\) are generated in response to the query messages \(}^{(l)}\). \(}\) then sends the updated messages to global message passing:

\[&^{(l)},^{(l )}=(}^{(l)},^{(l)},}^{(l)}),\\ &^{(l)}=}^{(l)}+( ^{(l)}),^{(l)}=^{( l-1)}+^{(l)},\\ &^{(l)}=}^{(l)}+^{(l)}. \] (10)

### Output Module

\(}\) updates the states of the embedded nodes recursively with a single recurrent layer. Instead of employing different layers for each recursive step, the associated parameters are shared across steps. After \(L\) recursive steps, the embedded nodes now reach their final states \(^{(L)}\) and \(^{(L)}\). \(}\) then takes graph node states and pseudo-node states as the learned representation for node-level and graph-level tasks, respectively. For graph classification, \(}\) further applies \(()\) to aggregate the pseudo-node states. To make class predictions, \(}\) employs learnable parameter \(^{n_{c} q}\) as the states of \(n_{c}\) class nodes and outputs the proximity between the recursive output and the class nodes.

## 5 Experiment

In this section, we provide empirical evaluation results of \(}\) on real-world benchmarks. \(}\) is implemented with PyTorch  and PyTorch Geometric , and trained on a single Nvidia Geforce RTX 4090. The detailed experimental settings are presented in Appendix C.

### Graph Classification

Experimental setups.We adopt six benchmarks including three biochemical datasets (OGBmolpcba , PROTEINS , NCI1 ) and three social network datasets  (COLLAB,

    & PROTEIN & NCI1 & IMDB-B & IMDB-M & COLLAB \\  \#Graphs & 1.113 & 4.110 & 1.000 & 1.500 & 5.000 \\ \#Nodes & 39.06 & 29.37 & 19.77 & 13.00 & 74.49 \\ \#Nodes & 145.60 & 64.60 & 19.31 & 131.87 & 4.914 \\ \#Node features & 3 & 37 & 0 & 0 & 0 \\  PatchCNN-SAN  & 75.00(-0.18) & 78.60(-0.22) & 71.00(-0.22) & 45.32(-0.3) & 72.60(-0.2) \\ GCN  & 73.24(-0.7) & 76.29(-0.17) & 73.26(-0.5) & 50.39(-0.1) & 80.59(-0.2) \\ PG  & 76.80(-0.1) & 82.80(-0.1) & 76.80(-0.5) & 53.20(-0.1) & 80.90(-0.5) \\ GCN  & 76.86(-0.1) & 82.89(-0.1) & 77.26(-0.5) & 56.32(-0.2) & 86.15(-0.1) \\  GIN  & 73.84(-0.5) & 76.62(-0.1) & 72.78(-0.4) & 84.81(-1.3) & 78.19(-0.5) \\ \#PSUPOD node  & 74.11(-0.1) & 77.08(-0.2) & 68.80(-0.5) & 47.60(-0.5) & 73.90(-0.7) \\ GARPSQEG  & 73.83(-0.5) & 73.82(-0.2) & 68.80(-0.5) & 47.60(-0.5) & 73.90(-0.7) \\ \#PSUPOD Node  & 73.93(-0.5) & 74.31(-0.2) & 73.14(-0.7) & 51.31(-0.7) & 82.13(-0.5) \\ DPPOD  & 75.62(-0.1) & 76.62(-0.1) & 73.14(-0.7) & 51.31(-0.7) & 82.13(-0.5) \\ \#PSUPOD Node  & 75.83(-0.5) & 77.08(-0.3) & - & - & - \\  TorPopCoCo  & 70.83(-0.5) & 67.02(-0.2) & 71.58(-0.5) & 48.59(-0.7) & 77.85(-0.5) \\ SAGPO  & 71.56(-0.5) & 67.45(-0.2) & 72.55(-0.2) & 78.03(-0.5) & 78.03(-0.5) \\ StreetPool  & 75.16(-0.5) & 78.64(-0.1) & 72.06(-0.1) & 50.23(-0.1) & 77.27(-0.3) \\ SPF  & 76.42(-0.2) & 79.35(-0.4) & 74.12(-0.2) & 51.53(-0.5) & 81.28(-0.5) \\ GMT  & 75.09(-0.5) & 76.53(-0.5) & 73.48(-0.5) & 50.66(-0.5) & 80.74(-0.5) \\  \(}\) (OONS) & **77.53(-1.7)** & **78.52(-0.1)** & **79.95(-0.1)** & **57.31(-0.1)** & **86.72(-0.5)** \\   

Table 1: **Graph classification results on small-scale benchmarks (measured by accuracy: %).**IMDB-BINARY and IMDB-MULTI). The benchmark statistics are summarized in Tab. 1 and Tab. 2. We choose convolutional GNNs, GNNs with a single pseudo node, hierarchical GNNs, and graph transformers as the baselines of graph classification. For more details, please refer to Appendix. C.1.2.

**Performance.**\(}\) and baselines are evaluated on both small-scale and large-scale benchmarks. The evaluation results in Tab. 1 and Tab. 2 showcase the ability of \(}\) to outperform various GNNs and graph transformers. Especially on the large-scale benchmark OGB-molpcba, \(}\) surpasses baseline models with only 500K parameters, while Graphormer reaches 31.39% average precision with 119.5M parameters. Compared to GNNs with a single pseudo node, \(}\) gains significant improvements. This demonstrates the effectiveness of our common state space where pseudo nodes and graph nodes can interact dynamically with each other.

### Node Classification

Experimental setups.For node classification, we conduct experiments on (**1**) six small-scale benchmarks: homophilic graphs (AmazonPhoto, AmazonComputers, CoauthorCS, and CoauthorPhysics) , heterophilic graphs (questions, amazon-ratings, tolokers, and minesweeper) ; and (**2**) four large-scale benchmarks: homophilic graphs (OGB-arXiv, OGB-proteins) , heterophilic graphs (arXiv-year, genius) . The statistics of node classification benchmarks are summarized in Tab. 3-5. We choose convolutional GNNs and graph transformers as the baselines. For detailed baseline setups, please refer to Appendix. C.2.2.

**Performance.** The evaluation results are presented in Tab. 3-5. \(}\) reaches superior or comparable performance against strong GNN baselines on both small-scale and large-scale benchmarks. Compared to graph transformers based on dense attention, including Graphormer, SAN, and GraphGPS, \(}\) surpasses the baselines on small-scale benchmarks and successfully scales to large-scale benchmarks. Note that Explorer based on sparse attention also suffers from the out-of-memory problem in our experimental environment. For other sparse graph transformers, \(}\) can achieve comparable results and perform consistently between heterophilic and homophilic benchmarks.

   & Questions & amazon-ratings & tolokers & Minesweeper \\  eNodes & 48,921 & 24,492 & 11,758 & 10,000 \\ \#Edogs & 153,540 & 93,050 & 519,000 & 39,402 \\  SGC  & 75.91\(\)0.00 & 50.66\(\)0.00 & 80.70\(\)0.00 & 708.80\(\)0.00 \\ GCN  & 76.93\(\)0.00 & 48.70\(\)0.00 & 83.64\(\)0.00 & 89.75\(\)0.00 \\ GAT  & 77.43\(\)1.00 & 49.09\(\)0.00 & 83.70\(\)0.00 & 92.01\(\)0.00 \\ GPRSEN  & 55.48\(\)0.00 & 44.88\(\)0.00 & 72.94\(\)0.00 & 86.24\(\)0.00 \\ H\({}_{g}\)CCN  & 63.59\(\)0.00 & 36.47\(\)0.00 & 73.35\(\)0.00 & 89.71\(\)0.00 \\ FAGCN  & 77.42\(\)1.00 & 44.12\(\)0.00 & 77.75\(\)1.00 & 88.17\(\)0.00 \\ GoGN  & 65.74\(\)1.00 & 36.89\(\)0.00 & 73.39\(\)1.00 & 81.12\(\)1.25 \\  \(}\) (Gutsk) & 77.95\(\)0.00 & **51.17\(\)0.00** & 83.23\(\)0.00 & 91.85\(\)0.00 \\ GraphGPS  & OOM & OOM & OOM & OOM \\ GraphGPS  & OOM & OOM & 84.70\(\)0.00 & 92.20\(\)0.00 \\ Exploring  & 73.86\(\)0.00 & 49.36\(\)0.00 & 84.20\(\)0.00 & 90.42\(\)0.00 \\  \(}\) (Ours) & **78.07\(\)0.00** & 50.25\(\)0.00 & **86.25\(\)0.00** & **93.97\(\)0.00** \\  

Table 4: **Node classification results on small-scale heterophilic graphs (measured by ROC-AUC except accuracy for amazon-ratings: %). \(\) denotes our reproduced results.**

   & Conditions & CountryPity & AMEphoto & AMEComputers \\  \#Nodes & 18,333 & 34,493 & 7,487 & 13,381 \\ \#Edogs & 18,394 & 247,962 & 119,043 & 245,778 \\  GCN  & 92.92\(\)0.00 & 96.18\(\)0.00 & 92.70\(\)0.00 & 89.65\(\)0.00 \\ GAT  & 93.61\(\)0.00 & 96.17\(\)0.00 & 93.87\(\)0.00 & 90.78\(\)0.00 \\ GPRSEN  & 95.33\(\)0.00 & 96.83\(\)0.00 & 94.94\(\)0.00 & 89.32\(\)0.00 \\ APPNP  & 94.49\(\)0.00 & 96.54\(\)0.00 & 94.32\(\)0.00 & 90.18\(\)0.00 \\ \#EmbedGPS  & 94.64\(\)0.00 & 97.05\(\)0.00 & 94.74\(\)0.00 & 91.18\(\)0.00 \\ GraphGPS  & 90.00\(\)0.00 & 92.74\(\)0.00 & 91.18\(\)0.00 \\ GraphGPS  & 94.51\(\)0.00 & OOM & 94.86\(\)0.00 & 89.33\(\)0.00 \\ GraphGPS  & 93.93\(\)0.00 & OOM & 95.06\(\)0.00 & 90.00\(\)0.00 \\ Asdignment  & 95.75\(\)0.00 & 97.34\(\)0.00 & 95.49\(\)0.00 & 91.22\(\)0.00 \\ Exphotor  & **95.77\(\)0.00** & 97.16\(\)0.00 & 95.27\(\)0.00 & 91.59\(\)0.00 \\  \(}\) (Ours) & 94.44\(\)0.00 & **97.56\(\)0.00** & **95.75\(\)0.00** & **92.51\(\)0.00** \\  

Table 3: **Node classification results on small-scale homophilic graphs (measured by accuracy: %).**

[MISSING_PAGE_FAIL:8]

**Tackling over-smoothing.** The other issue encountered when message passing relies heavily on input topology is over-smoothing . In \(}\), pseudo nodes are adopted as message pathway alternates, decoupling the message-passing process from input topology. During global message passing, our dynamic connection empowers the connected graph nodes on the input graphs to receive different outputs and avoid becoming too similar to each other. For local message passing, learning node displacements in \(}\) allows the combination of local message-passing outputs with layer inputs and global message-passing outputs. Although stacking multi-layer local message passing leads to over-smoothing, the layer inputs and global outputs can maintain the high-frequency signals. To evaluate whether \(}\) can alleviate the over-smoothing issue, we explore the changes in the Dirichlet energy  as the number of recursive steps increases. Fig. 4(b) compares among \(}\), \(}\) with the uniform connection, and SGC  on AmazonPhoto and amazon-ratings. The reported values are normalized to  by dividing the maximum energy values. Results show that \(}\) with uniform connections encounters over-smoothing as the number of recursive steps increases, while \(}\) with dynamic connections can maintain the Dirichlet energy and alleviate over-smoothing.

#### 5.3.3 Distribution of Embedded Nodes

We visualize the distribution of all the embedded nodes during training. Results on AmazonPhoto at the first recursive step are depicted in Fig. 5. For results through recursion, please refer to Appendix D.2. We can see that as the graph nodes are clustered with different class nodes, pseudo nodes are also split into several groups. Each pseudo-node group is attracted toward a distinct graph node cluster. This indicates that pseudo nodes will attend a particular graph node group, serving as the global message pathways to other groups. As a result, each pseudo node assumes a balanced fraction of the global message load, mitigating the risk of becoming bottlenecks for message passing.

To further analyze the message passing between graph nodes and pseudo nodes, we visualize the proximity matrix \(^{}\) and \(^{}\) for pseudo-node adaptation (Eq. 8) on AmazonPhoto in Fig. 6. For more proximity visualizations, please refer to Appendix D.3. \(1000\) graph nodes are sampled randomly and ranked based on their intro-/outre-proximity summation. The intro-/outre-proximity summation indicates the message load a graph node collects or diffuses during the message passing. As depicted in Fig. 6, both graph nodes and pseudo nodes assume a balanced message load. Each pseudo node has various proximity values toward individual graph nodes, different from the uniform pathways. All these properties empower efficient global message passing on graphs.

Figure 5: **Distribution of embedded nodes.** The t-sne  results under different training epochs are compared. **0, 1, 2, 3, 4, 5 and 6 denote graph nodes with different labels. \(\) denotes pseudo nodes. \(\) denotes class nodes.

Figure 6: **Message passing analysis.** The proximities between sampled graph nodes and pseudo nodes are depicted in the center of each sub-figure, where darker green indicates higher proximity and brighter green indicates lower proximity. The distribution on the top/right of each sub-figure denotes the sum of proximity for each graph/pseudo node. Graph/pseudo nodes are ranked based on the sum of proximity.

Figure 7: **Ablation on the modules of \(}\) (measured by accuracy: %).** PA., L., and G. denote pseudo-node adaptation, local message passing, and global message passing. Att. and Prx. denote the attention and proximity measurement.

#### 5.3.4 Ablation Study

**Pseudo node.** We conduct ablation studies on the number of pseudo nodes \(n_{p}\) in Fig. 8. To present the results in a comparable form within the same figure, the accuracy values have been normalized by subtracting the minimum value. As \(n_{p}\) increases, \(}\) gets improvement in accuracy for all three datasets. However, the accuracy exhibits degradation on PROTEINS when \(n_{p}\) reaches 256. This is because \(n_{p}\) exceeds the optimization capacity of \(}\). A similar degradation also happens on amazon-ratings. In practice, the optimal value of \(n_{p}\) is around 16 to 32 for graph classification while reaching 128 to 300 for node classification. We also conduct ablation studies on the engagement of pseudo nodes in Appendix D.7, where \(}\) with pseudo nodes outperforms dense message passing.

**Recurrent layer.** By employing the same recurrent layer through recursive steps, \(}\) attains comparable performance to baseline models with substantially fewer parameters. To analyze the efficacy of parameter sharing, we conduct ablation studies on the number of recurrent layers, comparing \(}\) with shared parameters against \(}\) with multiple recurrent layers in Fig. 9. For simplicity, we denote the number of recurrent layers as \(L_{p}\), where \(}\) with multiple recurrent layers has \(L_{p}\) equals to the number of recursive steps \(L\) and \(}\) with shared parameters has \(L_{p}=1\). Given the same \(L\), \(}\) with \(L_{p}=1\) achieves matching performance with \(L_{p}=L\). This indicates that shared parameters are sufficient in modeling convergent dynamics of the embedded nodes in the state space (Fig. 5). However, \(}\) achieves better performance with \(L_{p}=L\) when \(L\) surpasses \(8\) on amazon-ratings and AmazonPhoto. In further empirical analysis, we find that the embedded nodes in \(}\) with \(L_{p}=1\) tend to maintain current dynamics through recursive steps and thus become less effective in precise position optimization. Please refer to Appendix D.4 for detailed analysis. A step-dependent parameter may further improve the performance of \(}\) with \(L_{p}=1\). We will keep exploring it in future work.

**Relation measurement.** We compare our proximity measurement with attention  in Tab. 7. Results show that \(}\) with proximity achieves better performance on all three benchmarks. We ascribe this to the flexible range of the proximity values. In contrast, normalized attention that ranges in \(\) can yield equally small weights and attenuate the messages, especially when the optimal assignment involves a large number of graph nodes to the same pseudo node.

\(}\) **modules.** In Tab. 7, \(}\) with all three modules achieves superior performance across all the benchmarks. In comparison among the ablated \(}\), removing global message passing leads to a larger degradation in graph classification accuracy. Conversely, node classification exhibits greater sensitivity to the removal of local message passing. Moreover, pseudo-node adaptation is required by \(}\) on all three benchmarks. This indicates that adapting the randomly initialized distribution of pseudo nodes enables better interactions with graph nodes.

## 6 Conclusion

In this paper, we presented a dynamic message-passing method on graphs. Both graph nodes and pseudo nodes are embedded in a common state space with measurable relations between them. The measured relations serve as dynamic pathways between the embedded nodes, empowering flexible message passing. Associating pseudo nodes to input graphs with measured relations, graph nodes can communicate with each other intermediately through pseudo nodes under linear complexity. Based on the proposed dynamic message passing, we further developed a GNN model named \(}\) for graph and node classification tasks. Experimental results show that \(}\) achieves superior performance over competitive baseline models. For limitations discussion, please refer to Appendix F.

Figure 8: **Ablation on the number of pseudo nodes (\(n_{p}\)).**

Figure 9: **Comparison between single shared recurrent layer and multiple recurrent layers.**