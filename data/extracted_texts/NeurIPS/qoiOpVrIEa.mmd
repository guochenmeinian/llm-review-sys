# Using persistent homology to understand dimensionality reduction in resting-state fMRI

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Evaluating the success of a manifold learning method remains a challenging problem, especially for methods adapted to a specific application domain. The present work investigates shared geometric structure across different dimensionality reduction (DR) algorithms within the scope of neuroimaging applications. We examine reduced-dimension embeddings produced by a representative assay of dimension reductions for brain data ("brain representations") through the lens of persistent homology, making statistical claims about topological differences using a recent topological boostrap method. We cluster these methods based on their induced topologies, finding feature type and number -- rather than reduction algorithm -- as the main drivers of observed topological differences.

## 1 Introduction

The present work investigates shared geometric structure across different dimensionality reduction algorithms within the scope of neuroimaging applications. For most applications, a "dimensionality reduction" is any of a large class of methods that make inferences about structures underlying some data, typically to represent this data in both a more efficient and more interpretable way. Many dimensionality reduction (DR) problems can be equivalently formulated as "manifold learning" problems (i.e., estimating the manifold from which a dataset was sampled), and we will use the terms synonymously. Efforts to understand theoretical and empirical relationships between DR methods remain active1\({}^{-10}\).

The difficulty of relating dimensionality reductions can be compounded within specific application domains because methodologies often branch into variably specialized use cases. Nonetheless, specific use cases can also suggest more stringent criteria by which to compare dimension reduction outcomes. In functional neuroimaging, specialized dimension reduction algorithms proliferate the field, bridging disparate use cases, design philosophies, and biological motivations[11; 12]. These DR algorithms share the goal of extracting networks of functional activity from resting-state fMRI brain data, and we will refer to them throughout as "brain representations." We compare brain representations in terms of the topologies they induce on a single set of shared data ("subject space") and the statistical robustness of the differences between them. We measure these topological statistics through persistent homology and the related topological bootstrap[14; 15].

### Problem Statement: Brain Representations & Subject Space

Our primary goal is to compare the structural changes in a single neuroimaging dataset under a variety of brain representations. Because the structure of subject space in the original (un-reduced) data is unknown and impractical to compute, it is not feasible to grade brain representations' qualityby structure preservation. Instead, we group brain representations based on the similarity of the subject-space structures they induce in reduced data.

We frame brain representation as a manifold learning problem, and we frame comparisons between them as comparisons between estimated manifolds. We suppose our data lies on some manifold \(^{D}\) ("subject space"), from which we extract a finite dataset \(\) of \(N\) samples. In this work, \(\) consists of resting-state fMRI scans, and \(\) is the Human Connectome Project (HCP) Young Adult dataset; we then have \(D 10^{8}\) and \(N 10^{3}\). A _brain representation_ is any mapping \(_{i}:^{d_{i}}\) with \(d_{i} D\); for any brain representation, we define its corresponding _induced subject space_\(_{i}=_{i}()\). We compare brain representations \(_{i}\) and \(_{j}\) by comparing the persistent homology of their induced subject spaces \(_{i}\) and \(_{j}\). To link the persistent homology and manifold learning investigations, we make a key modeling assumption: there exists a local extension \(_{i}:^{d_{i}}\) of \(_{i}\) that is a submersion in some neighborhood of \(\). This assumption requires that dimensionality reductions behave consistently on unseen data near training data, and constitutes only a mild smoothness assumption on \(_{i}\). Under this modeling assumption, we may assert the existence of a manifold \(_{i}^{d_{i}}\) containing \(_{i}\) such that the following diagram commutes:

\[@>{_{i}}>{}>_{i}\\ @V{}V{}V@V{}V{}V\\ @>{_{i}}>{}>_{i}\]

While the smoothness assumption on \(_{i}\) is easily met by most DR algorithms, the connection between the persistent homology of \(_{i}\) and the manifold \(_{i}\) depends heavily on properties of the manifold sampling \(\).

To compare brain representations \(_{i}\) and \(_{j}\), we compute dissimilarity metrics for all pairs of points in \(_{i}\) and \(_{j}\) and examine the resulting Vietoris-Rips complex in each space. This approach allows flexibility in the data and dissimilarities under consideration while still allowing claims about DR-induced topological differences.

#### Related Work: Persistent Homology & Dimensionality Reduction

Most comparisons in the literature are primarily interested in grading the relative performance of different DR algorithms. Though we do not share these goals, many comparative approaches articulate frameworks and methods with important relationships to our own. We review a selection of comparison methods, organized in roughly increasing order of the similarity between their goals and framework to our own.

Some evaluation methods for dimensionality reduction lead with intuition, formalizing helpful heuristics into rigorous ratings. We first reference Lee and Verleysen's co-ranking matrix, which measures insertions to ("intrusion events") and deletions from ("extrusion events") \(k\)-neighborhoods in the low-dimension vs. high-dimension space. While this measure is non-parametric (with respect to the data geometry) and thus extremely flexible, it is sensitive only to local structure in the data. Later, Lee and Verleysen showed that the performance of a DR method closely follows its (a) insensitivity to norm concentration and (b) plasticity (i.e., the cost function gradient vanishing for distant points), leveraging a more geometric perspective to the analysis of DR performance than was typical of the contemporary literature. Extending this line of thinking, Wang et al recently offered a strictly empirical investigation of different DR methods in which they consider only the attractive and repulsive forces of the loss function over varying distance scales. They show that this framework is sufficient to characterize DR performance and extrapolate robust empirical principles of "good DR methods" without need of an underlying formalism. While this work offers striking practical insights, it does not offer an immediate path to describing degrees of divergence between DR methods.

We now consider a class of methods we characterize by their tendency to originate in formal, geometric considerations of manifold learning. Singer and Wu's vector diffusion distance, an extension of diffusion embeddings, uses local principal component analysis to locally estimate a connection on the tangent bundle of a data manifold, from which it constructs a lower-dimensionalembedding. While the primary goal of their work is to define a manifold learning method, rather than a technique for comparing such methods, their vector diffusion distance explicitly captures a variety of geometric and topological data invariants. These invariants could be used analogously to our use of persistent homology to compare dimensionality reductions of the same dataset. Similarly, Tyagi et al  propose a set of tangent space estimation criteria for manifold sampling (as a function of manifold curvature and intrinsic dimension) that immediately suggest geometric routes of comparison between learned data manifolds. Finally, in more direct alignment with our goals, Sun and Machard propose a geometric theory of manifold learning, which comes equipped with an intrinsic metric [18; 19]. Their approach is firmly grounded in classical information geometry [20; 21], comparing learned models via the pullback of the Fisher information metric on direct probabilistic encodings of reduced data. While this perspective diverges substantially from our own, our constructions are mutually translatable, and their approach could provide interesting comparison and/or validation.

To the best of our knowledge, only two other studies [2; 3] have examined dimensionality reduction through the lens of persistent homology. Both works primarily consider the recovery quality of a known manifold and propose quality metrics derived from persistent homology. Paul and Chalup  compare DR methods while varying manifold complexity, measuring performance by the similarity of pre- and post-DR Betti numbers as a function of sampling density. While their goals differ from ours, much of our comparison also hinges on counts of topological features; however, they did not have access to the topological bootstrap [14; 15] we employ in our study. Rieck and Leitte  compute the Wasserstein distance between pre- and post-DR persistence diagrams as a metric of embedding quality. While their study is most similar to our own, there are several key differences. First, they operationalize persistent homology with a sublevel-set filtration of the local density function, whereas we use the Vietoris-Rips filtration on pairwise point dissimilarities. Second, they consider reduction of a surface embedded in \(D=3\) reduced to \(d=2\), whereas we consider data initially embedded in \(D 10^{8}\) and reduced to dimensionalities ranging from \(d 10^{2}\) to \(d 10^{5}\). Most importantly, we are able to leverage the topological bootstrap [14; 15] in our work, which was not available at the time of their publication.

Finally, we also contrast our analysis of persistence data with a prevalent paradigm in persistent homology applications. Many persistent homology analyses [6; 22; 23; 24; 25; 26; 27] (including those above) operationalize the assumption that most topological information lives in a diagram's most persistent components, treating low-persistence generators as noise. Instead, we follow work in distributed persistence [28; 29] and examine the distributional properties of our persistence diagrams to parse their topological content.

Our contributions in the present study are as follows: (1) A flexible framework for the statistical comparison of dimensionality reductions, applicable to any data and dissimilarity measure compatible with a Vietoris-Rips complex; (2) Robust statistical measurement of topological differences between dimension-reduced data; (3) Application to real neuroscience data over a diverse slice of widely used neuroimaging DR algorithms ("brain representation" or "BR").

## 2 Methods

### Brain Data and Brain Representations

The data for this study consists of pre-processed resting-state functional MRI data from N=1003 Human Connectome Project young adult (HCP-YA)  subjects. Each subject's minimally pre-processed data consists of 91,282 spatial "grayordinates" by 1200 time points, giving an embedding dimension of \(D 10^{8}\) in the initial space. We then chose six different brain representations that are both common within the field and showcase the methodological variability of widely adopted techniques. The brain representations we consider can roughly be grouped by their underlying models of brain function. We characterize the first group of methods as seeking to cluster neural activity into spatially contiguous cortical "parcels." In the parcellation family, we have Yeo's parcellated networks , Glasser's multimodal parcellation , and Schaefer's local-global parcellation . We also sample from a family of low-rank matrix factorization methods that parse non-contiguous networks of functional activity. Independent component analysis (ICA) , an extension and application refinement of PCA, underlies perhaps the most widely used brain representation in the field  and thus is represented here. In addition, we consider PROFUMO , which parses "functional modes" of brain activity from hierarchical Bayesian signal models. Finally, we include the "principal gradient"(or "gradients"), a diffusion embedding method that organizes brain function through cortical geometry.

From each brain representation, one or more feature types were computed to reflect the typical use of brain representations in the neuroimaging literature. The five feature types considered in this work are as follows: (1) "amplitude," the average power of the time signal in a given spatial component; (2) "network matrix (netmat)," the matrix of pairwise Pearson similarities of time courses for each pair of spatial components; (3) "partial correlation," the variance-normalized precision matrix; (4) "map," the spatial membership weights of a given spatial component in grayordinate space; and (5) "spatial network matrix", the matrix of pairwise Pearson similarities of maps for each spatial component. The decomposition rank, feature types, and number of features for each brain representation is summarized in Table 1. Note that since subject data are encoded in terms of features, it is the feature number and **not** the brain representation's decomposition rank that denotes the dimension \(d\) of the target embedding space in the mapping \(:^{d}\). We compare subject-space embeddings using the pairwise dissimilarities of their points, which we compute as described in the next section.

### Dissimilarity Measures

For each brain representation method, decomposition rank within a given representation, and considered feature type, we compute pairwise distances between all subjects. Each feature type under consideration is structured either as a vector (maps, amplitudes) or a symmetric positive semidefinite (SPSD) matrix (network matrices). This bifurcation of data types is echoed in our choice of measures when computing the dissimilarity between a pair of subjects. In both the vector case and the SPSD data case, we ran our analysis using one dissimilarity measure intrinsic to the data type and another derived from the Pearson correlation. We use Pearson-based dissimilarities in deference to the ubiquitous use of the Pearson correlation in neuroimaging analyses.

We now define the dissimilarity measures we use on vector data. Suppose \(s_{i}\) and \(s_{j}\) are data vectors in \(^{d}\), and let \((s_{i},s_{j})\) denote their Pearson correlation. Let \(,\) denote the usual inner product on \(^{d}\). We then define

\[d_{v_{1}}(s_{i},s_{j})=1- s_{i},s_{j}^{2}\] (1)

\[d_{v_{2}}(s_{i},s_{j})=1-^{2}(s_{i},s_{j}),\] (2)

assuming the matrix \(D_{ij}= s_{i},s_{j}\) is scaled to have entries in \(\). Note that we can interpret \(d_{v2}\) as approximately the angular distance between the vectors \(s_{i}\) and \(s_{j}\) after each has been centered. We refer to \(d_{v_{1}}\) as the "inner product divergence" and \(d_{v_{2}}\) as the "Pearson divergence".

In the SPSD matrix case, we consider the geodesic distance between matrices on the Riemannian SPD cone alongside a (modified) Pearson divergence. The geodesic distance \(d_{pd_{1}}\) on the symmetric positive definite cone is efficiently implemented via the approximate joint diagonalizer, and we modify the Pearson divergence \(d_{v_{2}}\) for the correlation matrix case by precomposing it with Fisher's z-transformation (the inverse hyperbolic tangent function): we write

\[d_{pd_{2}}(M_{i},M_{j})=^{*}d_{v_{2}}(m_{i},m_{j}),\] (3)

 
**Representation Name** &  **Decomposition** \\ **Rank(s)**\(r\) \\  & **Considered Feature Type(s)** & **Feature Number(s)**\(d\) \\  PROFUMO & 33 & maps, spatial network matrices & \(91282 33\), \(\) \\  Dual-regression & 15, 25, 50, & amplitudes, network matrices, & \(r\), \(\), \(\) \\ spatial ICA & 100, 200, 300 & partial network matrices & \(360\), \(\), \(\) \\  Glasser parcellation & 360 &  amplitudes, network matrices, \\ partial network matrices \\  & \(360\), \(\), \(\) \\  Schaefer parcellation & 100, 200, 300, 600 &  amplitudes, network matrices, \\ partial network matrices \\  & \(r\), \(\), \(\) \\  Yeo parcellation & 17 & 
 amplitudes, network matrices, \\ partial network matrices \\  & 17, \(\), \(\) \\  Gradient & 1, 15, 25, 50, & maps & \(91282 r\) \\ (diffusion embedding) & 100, 200, 300 & maps & \(91282 r\) \\  

Table 1: The combinations of brain representation, decomposition rank parameters, and feature types investigated in the present work.

where \(m_{i}\) is the vector of upper-right triangle entries of the symmetric matrix \(M_{i}\) (diagonal excluded). This precomposition is necessary for correlation matrices, as it normalizes the correlation values before re-correlating them. In contrast to the vector case, there is no simple comparison to be made between these two dissimilarity measures.

For each combination of brain representation, rank parameter, and feature type shown in Table 1, we compute pairwise dissimilarity according to both of whichever two measures are relevant. The subject-pairwise matrix of dissimilarities then forms the Gram matrix used to compute the persistent homology, as we describe in the next section.

### Persistent Homology

We compute the Vietoris-Rips persistence  of each Gram matrix (which is obtained as described above), and we now give a very brief background on Vietoris-Rips persistence. For a thorough treatment of persistent homology, see Dey and Wang's text ; for a thorough treatment of algebraic topology preliminaries, see Hatcher's text .

#### 2.3.1 Brief background

The topology of a space can be summarized by its _homology groups_, algebraic invariants that describe its structure. Persistent homology extends the constructions of homology to finite data, delivering a multiscale and threshold-free estimation of data topology. To compute the persistent homology of a dataset \(X\), it must first be equipped with a _simplicial structure_: a simplicial complex \(K(X)\) is a set of subsets of \(X\) with the property that \(^{} K\) whenever \(^{}\) for some \( K\), and a _filtration_ is a collection \(\{K_{t}(X)\}\) such that \(K_{s} K_{t}\) when \(s<t\). Homology groups \(H_{k}(K_{t}(X))\) can be computed for each simplicial complex, and their _persistence_\(_{k}(X)\) is described by the evolution of these groups across the filtration. A simple example of a simplicial complex on \(X\) is a graph \(G(X)\). If that graph \(G(X)\) is weighted, then the family \(\{G_{r}(X)\}\) of graphs obtained from \(G\) by thresholding its edges at weight \(r\) is a filtration on \(X\). If \(G(X)\) is the graph on \(X\) with edge weights given by the distance between vertices, then the filtration we just described is the _Vietoris-Rips filtration_ on \(X\). Given any dissimilarity matrix \(d_{X}\), we can assume it is the Gram matrix of some graph \(G(X)\) and compute its Vietoris-Rips persistence \(_{k}(X)\).

#### 2.3.2 Topological bootstrap

Because it is possible (and, in fact, common) for multiple data elements to define the same homology generator, bootstrap re-sampling  is less straightforward in persistent homology than in many other modes of analysis. However, Reani and Bobrowski recently demonstrated a "topological bootstrap" method  that uses image persistence  to register homology generators found in co-embeddable spaces. If \(X,Y\) can both be embedded into a shared space \(Z\), then the inclusion maps \(X}}{{}}Z\) and \(Y}}{{}}Z\) induce homology maps \(_{X}^{*},_{Y}^{*}\) with corresponding filtration maps \(_{r,X}^{*},_{r,Y}^{*}\) (assuming compatible filtrations on each space). A pair of nontrivial elements in \(_{k}(X)\) and \(_{k}(Y)\) is said to match via \(Z\) if \(_{r,X}^{*}\) and \(_{r,Y}^{*}\) map them to the same nontrivial element of \(_{k}(Z)\) for some filtration value \(r\). For a matched pair, the affinity score \(\) of the match can be computed from ratios of lengths of intervals in each filtration for which elements in \(_{k}(X)\) and \(_{k}(Y)\) are matched via \(Z\). We assign \(=0\) when no match is found and have \((0,1]\) otherwise.

This procedure simplifies substantially in the bootstrapping case; we then have \(Z=X\) and \(Y= X\), and we need only check nontrivial elements of \(_{k}(X)\) for matches in \(_{k}()\). In the bootstrap setting, Reani and Bobrowski measure the recurrence stability of a nontrivial generator \(_{k}(X)\) by its _prevalence score_

\[()_{j=1}^{R}(,_{j}),\] (4)

where \(_{j}\) is the match of \(\) in the \(j^{}\) bootstrap. This is just the average affinity (over all bootstraps) between \(\) and its matches. In the present study, we compute prevalence scores for each generator in \(_{1}(X)\) for a given subject dissimilarity matrix \(X\).

Our implementation  of the topological bootstrap is a mild extension of Garcia-Redondo et al's work , which efficiently integrates cycle registration with Ripser  and Ripser-image , refines the cycle affinity measures proposed by Reani and Bobrowski, and broadens the conditions under which topological bootstrapping may be applied.

To satisfy the exchangeability criteria necessary for (any) bootstrapping, we also needed to account for family relationships between subjects in our bootstrap re-samples. Following the approach of Winkler et al., we excluded all bootstrap re-samples that placed individuals with the same mother on different sides of the inclusion/exclusion divide. We conducted cycle registration using \(R=1000\) bootstraps per dataset at \(90\%\) re-sampling (without replacement), and we consider \(k=1\)-dimensional cycle registration in this work.

#### 2.3.3 Prevalence-weighted Wasserstein-\(p\) distance

The space of persistence diagrams is a metric space under the Wasserstein-\(p\) distance, which previous work has used to compare persistence diagrams of different low-dimensional embeddings. To include statistical information about the stability of homology generators in this comparison, we define the _prevalence-weighted_ Wasserstein-\(p\) distance

\[W_{p}^{()}(d_{1},d_{2})(_{ _{12}}_{x d_{1}}\|x(x)-(x)((x)) \|_{}^{p})^{}.\] (5)

Here, \(d_{1}\) and \(d_{2}\) are persistence diagrams, \(_{12}\) is the set of bijections between \(d_{1}\) and \(d_{2}\), and \((x)\) is the prevalence of the homology generator \(x\) given in 4. This is a simple re-weighting of the usual Wasserstein distance, modified to incorporate the prevalence score as a summary of per-cycle stability statistics.

#### 2.3.4 The "matched Betti number" \(_{k}^{}\)

We also define the "first matched Betti number" \(_{k}^{}\) as the number of matched cycles (i.e., matches with nonzero affinity scores) found in each bootstrapped re-sample. Intuitively, this is a count of the number of stable generators found in each bootstrap. The Betti numbers of a persistence module are typically summarized by curves, since each value of a filtration may induce a homology with a different set of Betti numbers. However, since the topological bootstrap already uses persistence interval information to find matched cycles and compute their affinity, we may consider \(_{k}^{}\) as having "collapsed" these curves via cycle registration. We consider the distribution of bootstrapped \(_{k}^{}\) values as a coarse summary of the distributed persistence of a given dissimilarity matrix \(d_{X}\).

### Study Design

In Table 1, we lay out parameter and feature selections considered for each brain representation. For every representation, bootstrapped persistence is computed for all combinations of feature, parameter, and dissimilarity measure considered; this gives a total of 90 subject-pairwise dissimilarity matrices for which we compute \(R=1000\) topological bootstraps. We compute the prevalence-weighted Wasserstein-2 distance between all pairs of methods and the \(_{1}^{}\) distributions for each method. This method-pairwise distance matrix then undergoes Ward hierarchical clustering to determine similarity. Our code is publicly available on github.

#### 2.4.1 Hypotheses

Comparing across feature and metric choices, we expect the SPD matrix geodesic distance to exhibit less sensitivity to concentration of measure and thus provide greater distinction between brain representations. We expect that within-feature groupings for map and amplitude will differ very little between the considered vector dissimilarity measures (equations 1 and 2). For all comparisons, we expect feature number and type to be more important drivers of differences than decomposition rank. Finally, within the PROFUMO analysis, we expect that spatial network matrices will be further from null than spatial maps, where we expect the very high dimensions of the spatial maps to suffer from concentration of measure.

Comparing across different brain representations, we expect to primarily see clustering according to (approximate) feature number and type, with secondary similarity clusters forming within each given brain representation. We expect our analysis to align with previous results in the literature linking shared variance in brain representations \({}^{-}\), the details of which we expand upon in the results below.

## 3 Results

### Persistent homology and dimension reduction

We first note several unexpected instances of trivial (or nearly trivial) persistence structure. First, full correlation matrices generated null \(H_{1}\) persistence at every decomposition rank in every brain representation. By contrast, the partial correlation matrices (which is similar by conjugation to the inverse of the full correlation matrix) have interesting persistence for nearly all feature types, decomposition ranks, and dissimilarity measures. Additionally, the inner product divergence (1) generated trivial or almost trivial homology in both maps and amplitudes, across all ranks and representations; this is not true of the Pearson divergence, which we incorrectly hypothesized would exhibit similar behavior. A complete list of all methods that exhibited trivial \(H_{1}\) persistence is given in Table S1.

#### 3.1.1 Effect of embedding dimension

Our analysis saw that topological complexity (as measured by \(H_{1}\) persistence) generally _decreased_ with the number of features considered (Fig S1). Under the geodesic distance, mean prevalence score increased with feature number; for all other dissimilarity measures, mean prevalence score was not correlated with feature number (Fig S2). Taken together, these observations suggest that embeddings in higher dimensions elicit a smaller number of nontrivial \(H_{1}\) generators which are also more robust. This runs counter to the consequences we might expect from concentration of measure in high dimensions, which pushes spaces towards the discrete topology (and thus a higher number of less stable generators). As expected, we also saw that feature number was a more important driver of persistence structure than the underlying rank of the decomposition (Fig S3).

#### 3.1.2 Persistence vs. prevalence

We see evidence further corroborating Reani and Bobrowski's observation that the most prevalent cycles are not always the most persistent ones . Figure 1 shows a sample persistence diagram in \(H_{1}\) (colored by generator prevalence score) and a plot of all persistence-prevalence pairs observed in this experiment. Both plots demonstrate that cycles with low persistence can still have high prevalence, suggesting that the topological "noise" may carry meaningful structure in our data. In addition, we see a substantially richer difference structure between target embeddings when using the prevalence-weighted Wasserstein-\(2\) distance instead of the classical Wasserstein-\(2\) distance (Figure S4).

Figure 1: (**Left**) A sample persistence diagram, with color weights given by prevalence score. (**Right**) Persistence versus prevalence across all data collected, colored with a Gaussian kernel density estimator.

### Persistence differences of brain representations

The prevalence-weighted Wasserstein distance makes its strongest distinction between amplitudes and network matrix/spatial map feature types, which form the two main diagonal blocks and highest dendrogram branches (Fig 2). As hypothesized, this implies that our method distinguishes more strongly between feature type (and number) than between brain representation type, which forms the next set of blocks and branches. This is still somewhat surprising, however, because brain representations differ substantially in terms of whether they are unilateral or bilateral, binary or weighted, and decomposition rank.

We are also surprised to see PROFUMO spatial network matrices in the amplitude block. Both amplitudes and spatial network matrices have been shown to be highly sensitive to individual differences in behavior, but these feature types are interpreted very differently. Amplitudes may be linked to within-network synchronization, within-network plasticity, or within-network interneuron function, whereas spatial network matrices are indicative of between-network shared brain regions that may play a role in cross-network integration. Both amplitudes and spatial netmats have higher test-retest reliability (i.e., within-subject stability) than the features in the other block. Given this context, the clustered blocks of the prevalence-weighted Wasserstein may constitute a segregation of trait-sensitive (amplitude and spatial network matrix) from state-sensitive (temporal network matrix) features. This observation highlights the need for an evaluation method that can detect _which_ elements of the persistence module are shared across representations, rather than only being able to similar topologies of subject similarity.

Figure 2: Prevalence-weighted 2-Wasserstein distances between \(H_{1}\) persistence diagrams for all pairs of methods combinations with nontrivial first homology. Ward hierarchical clustering gives the dendrogram on the left side of the plot, which organizes labels into groups that maximally share variance. Lighter colors denote smaller distances, while darker (blue) colors denote larger ones.

### Computational resources

All other computations, including cycle registration, were negligible in cost compared to the computation of persistence modules for all bootstraps -- roughly \(270,000\) persistence modules were computed in total. Memory demands remained relatively low (\( 50\)GB per homology computation). Our implementation was embarrassingly parallel on a queue-managed HPC cluster. We estimate that this experiment used approximately \(80,000\) CPU hours over the course of a month. Computation of image-persistence was the most costly individual step, with each embedded persistence module taking 1-3 hours to compute (compared to order of 10 minutes or less for other persistence computations).

## 4 Discussion

### Conclusions

Our method reveals interesting relationships between dimensionality reductions of resting-state fMRI data. The prevalence-weighted Wasserstein distance distinguishes much more strongly between feature type than dimensionality reduction, potentially segregating trait-sensitive from state-sensitive features. Notably, this distinction holds without regard to choice of dissimilarity measure.

Without exception, full network matrices gave rise to trivial \(_{1}\) modules. Persistence modules generated from the inner product divergence (1) were (approximately) trivial as well, in sharp contrast to those generated from the Pearson divergence (2); this suggests that amplitude and spatial map features of brain representations tend to be "mean-dominated," in the sense that per-subject deviations from group-level structures are typically small.

In addition, we saw a counterintuitive decrease in persistence "complexity" as a function of increasing embedding dimension, which highlights the difficulties of evaluating dimension reduction in high-dimensional target spaces. We also examined the relationship between persistence and prevalence, finding that the two are largely uncorrelated for our data. Coupled with the stronger distinctions realized by the prevalence-weighted Wasserstein-2 distance, we believe that persistence and prevalence may be somewhat complementary as measures of cycle importance.

### Limitations

Because of the high cost of parameter exploration, dimensionality reduction computation, and topological bootstrapping, only a few dimensionality reduction methods were examined in this work. An extension of this analysis to a wider array of brain representations may be warranted, especially newer methods that derive an explicitly geometric basis for functional activity (e.g., Laplacian eigenvalues).

Another important limitation of our work is the very high dimension-to-sample-size ratio \((N d)\) of our data. In this regime, it is difficult to ascertain what features we see because of structure in the data and what topological features are products of the curse of dimensionality. This could be partially ameliorated by conducting our analysis over adequately constructed null data and comparing the results, which is beyond the scope of this work.

### Future Directions

In addition to addressing some of the limitations noted above, we offer several directions for follow-up work on this study. First, we propose a consideration of the per-bootstrap Wasserstein distance between methods; a distributional picture of differences in the endogenous metric of persistence modules could yield important insights. Second, it is possible to repurpose the topological bootstrap to track the addition/deletion of homology components by different brain representation; practically, this is primarily hindered by the lack of a suitable dissimilarity metric between pairs of points under different embeddings. Finding and validating such a metric would be a valuable direction of inquiry. Finally, we wish to suggest an investigation into the theoretical properties of the prevalence-weighted Wasserstein metric.