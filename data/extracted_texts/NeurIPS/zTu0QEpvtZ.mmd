# Towards Understanding the Working Mechanism of Text-to-Image Diffusion Model

Mingyang Yi\({}^{1}\), Aoxue Li\({}^{2}\), Yi Xin\({}^{3}\), Zhenguo Li\({}^{2}\)

\({}^{1}\) Renmin University of China

\({}^{2}\) Huawei Noah's Ark Lab

\({}^{3}\) Nanjing University

{yimingyang@ruc.edu.cn}

{liaoxue2,li.zhenguo}@huawei.com

xinyi@smail.nju.edu.cn

equal contributioncorresponding to Mingyang Yi: yimingyang@ruc.edu.cn

###### Abstract

Recently, the strong latent Diffusion Probabilistic Model (DPM) has been applied to high-quality Text-to-Image (T2I) generation (e.g., Stable Diffusion), by injecting the encoded target text prompt into the gradually denoised diffusion image generator. Despite the success of DPM in practice, the mechanism behind it remains to be explored. To fill this blank, we begin by examining the intermediate statuses during the gradual denoising generation process in DPM. The empirical observations indicate, the shape of image is reconstructed after the first few denoising steps, and then the image is filled with details (e.g., texture). The phenomenon is because the low-frequency signal (shape relevant) of the noisy image is not corrupted until the final stage in the forward process (initial stage of generation) of adding noise in DPM. Inspired by the observations, we proceed to explore the influence of each token in the text prompt during the two stages. After a series of experiments of T2I generations conditioned on a set of text prompts. We conclude that in the earlier generation stage, the image is mostly decided by the special token [EOS] in the text prompt, and the information in the text prompt is already conveyed in this stage. After that, the diffusion model completes the details of generated images by information from themselves. Finally, we propose to apply this observation to accelerate the process of T2I generation by properly removing text guidance, which finally accelerates the sampling up to 25%+.

## 1 Introduction

In real-world application, the Text-to-Image (T2I) generation has long been explored owing to its wide applications , whereas the Diffusion Probabilistic Model (DPM)  stands out as a promising approach, thanks to its impressive image synthesis capability. Technically, the DPM is a hierarchical denoising model, which gradually purifies noisy data from a standard Gaussian to generate an image. In the existing literature , the framework of (latent) Stable Diffusion model  is a backbone technique in T2I generation via DPM. In this approach, the text prompt is encoded by a CLIP text encoder , and injected into the diffusion image decoder as a condition to generate a target image (latent encoded by VQ-GAN in ) that is consistent with the text prompt. Though this framework works well in practice, the working mechanism behind it, especially for the text prompt, remains to be explored. Therefore, in this paper, we systematically explore the working mechanism of stable diffusion.

Our investigation starts from the intermediate status of the denoising generation process. Through an experiment (details are in Section 4), we find that in the early stage of the denoising process, the overall shapes of generated images (latent) are already reconstructed. In contrast, the details (e.g., textures) are then filled at the end of the denoising process. To explain this, we notice that the overall shape (resp. semantic details) is decided by low-frequency (resp. high-frequency) signals . We both empirically show and theoretically explain that in contrast to the high-frequency signals, the low-frequency signals of noisy data are not corrupted until the end stage of the forward noise-adding process. Therefore, its reverse denoising process firstly recovers the low-frequency signal (so that overall shape) in the initial stage, and then recovers the high-frequency part in the latter stage.

Following the phenomenons, we investigate the effect of encoded tokens in the text prompt of T2I generation during the two stages, where each token is encoded by an auto-regressive CLIP text encoder. The text prompt has a length of 76, and is enclosed by special tokens [SOS] and [EOS], + at the beginning and end of the text prompt, respectively. Therefore, we categorize the tokens into three classes, i.e., [SOS], semantic tokens, and [EOS]. Notably, the special token [SOS] does not contain information, due to the auto-regressive encoding of the text prompt. Thus, our investigations into the influence of tokens will primarily focus on the semantic tokens and [EOS]. Surprisingly, we find that compared with semantic tokens, the special token [EOS] has a larger impact during generation.

Footnote †: [EOS] contains the overall information in text prompt due to the auto-regressive CLIP text-encoder

Concretely, under a set of collected text prompts, we select 1000 pairs "[SOS] + Prompt \(A\) (\(B\)) + [EOS]\({}_{A(B)}\)" from it. Then, replace the special token [EOS]\({}_{A}\) in the text prompt \(A\) with [EOS]\({}_{B}\) from prompt \(B\) to observe the generated data under this condition. Interestingly, we find that the generated images are more likely to be aligned with text prompt \(B\) (especially for the shape features) instead of \(A\), so that [EOS] has a larger impact compared with semantic tokens. Besides that, we further find that the information in [EOS] is already conveyed during the early shape reconstruction stage of the denoising process. Exploring along the working stage of [EOS], we further verify and explain that the whole text prompts (including semantic ones) primarily work on the early denoising process, when the overall shapes of generated images are constructed. After that, the image details are mainly reconstructed by the images themselves. This phenomenon is explained by "first shape then details", as the injected text prompt implicitly penalizes the generated images to be consistent with it. Therefore, the penalization quickly becomes weak, when the overall shape of image is reconstructed.

Finally, we apply our observations in one practical cases: Training-free sampling acceleration, as the text prompt works in the first stage of denoising process, we remove the textual prompt-related model propagation (\(}(t,_{t},)\) in (3)) during the details reconstruction stage, which merely change the generated images but save about 25%+ inference cost.

We summarize our contributions as follows.

1. We show, during the denoising process of the stable diffusion model, the overall shape and details of generated images are respectively reconstructed in the early and final stages of it.
2. For the working mechanism of text prompt, we empirically show the special token [EOS] dominates the influence of text prompt in the early (overall shape reconstruction) stage of denoising process, when the information from text prompt is also conveyed. Subsequently, the model works on filling the details of generated images mainly depending on themselves.
3. We apply our observation to accelerate the sampling of denoising process 25%+.

## 2 Related Work

Diffusion Model.In this paper, our exploration is based on the Stable Diffusion , which now terms to be a standard T2I generation technique based on DPM [12; 36], and has been applied into various computer vision domains e.g., 3D [29; 33; 19] and video generation [26; 4]. In practice, the goal of T2I is generating an image that is consistent with a given text injected into the cross-attention module  of the image decoder. Therefore, understanding the working mechanism of stable diffusion potentially improves the existing techniques . Unfortunately, to the best of our knowledge, the problem is limited explored, expected in [46; 35], where they similarly observe the low-frequency signals are firstly recovered in the denoising process. However, further explanations for this phenomenon are neglected in these works.

Influence of Tokens.Understanding the working mechanism of encoded (by a pretrained language model) text prompt [3; 30; 43; 27] helps us understanding T2I generation [38; 16; 21]. For example,  finds that in LLM, the first token primarily decides the weights in the cross-attention map, which similarly appeared in the cross-modality text-image stable diffusion model as we observed.  explores the influence of individual tokens in counterfactual memorization. However, in the multi-modality models e.g., [30; 18; 17; 23], whereas the textual information interacted with the image in the cross-attention module as in stable diffusion, the working mechanism of tokens interacts with cross-modality data is limited explored, expected in . They find in a single case that the influence of text prompts may decrease during the denoising process, while they do not proceed to study or apply this phenomenon as in this paper. Recently,  finds that the cross-attention map between the text prompt and generated images converges during the denoising process, which is also explained by our observations that the information conveyed during the first few denoising steps. Besides that, unlike ours, their observations are lack of theoretical explanation.

## 3 Preliminaries

We briefly introduce the (latent) stable diffusion model , which transfers a standard Gaussian noise into a target image latent that aligns with pre-given text prompts. Here, the generated data space is a low-dimensional Vector-Quantized (VQ)  image latent to reduce the computational cost of generation. One may get the target natural image by decoding the generated image latent. In this paper, the original data (image latent) is denoted by \(_{0}\), and the encoded textual prompt (by CLIP text encoder ) is represented by \(\). The noisy data

\[_{t}=_{t}}_{0}+_{t}}_{t},\] (1)

is used as input to diffusion model \(_{}\) trained by

\[_{}[\|_{}(t,_{t},,)-_{t}\|^{2}],\] (2)

with \(0 t T\), \(_{t}(0,)\) independent of \(_{0}\), \(_{t} 0\) (resp. \(_{t} 1\)) for \(t 0\) (resp. \(t T\)). Here, the noise prediction model \(_{}(t,_{t},,)\) is constructed by classifier-free guidance  with

\[_{}(t,_{t},,)= _{}(t,_{t},)+w(_{}(t, _{t},)-_{}(t,_{t},) ),\] (3)

where \(_{}(t,_{t},)\) is an unconditional generative model, and the \(w 0\) is guidance scale. As the model is trained to predict noise \(_{t}\) in \(_{t}\), and \(_{T}\) approximates a standard Gaussian, we can conduct the reverse denoising process (DDIM ) transfers a standard Gaussian to target image \(_{0}\)

\[_{t-1}=_{t-1}}{_{t}}}_{t}+ (_{t-1}}{_{t-1}}}-_{t}}{_{t}}})_{}(t,_{t},,).\] (4)

Finally, the diffusion model (usually UNet) takes the text prompt as input to the cross-attention module in each basic block 2 of diffusion model with output \((Q,K,V)=(QK^{}/)V\) (\(d\) is dimension of image feature), where \((_{t})\) is the feature of image, and

\[Q=W_{Q}(_{t});K=W_{K};V=W_{V}.\] (5)

## 4 First Overall Shape then Details

In this section, we first explore the image reconstruction process of the stable diffusion model. As noted in , the generated image's overall shape is difficult to be alterted in the final stage of the denoising process. Inspired by this observation, and note that the low-frequency and high-frequency signals of image determine its overall shape and details, respectively . We theoretically and empirically verify that the denoising process recovers the low and high-frequency signals in its initial and final stages, respectively, which explains the phenomenon of _"first overall shape then details"_.

### Two Stages of Denoising Process

Settings (PromptSet).As in , we use 1600 prompts following the template "a {attribute} {noun}", with the attribute as an adjective of color or texture. We create 800 text prompts respectively under each of the two categories of attributes. Besides that, we add another extra 1000 complex natural prompts in  without a predefined sentence template. These prompts consist of the text prompts set (abbrev PromptSet) we used. The classes of nouns, colors, and textures are respectively 230, 33, and 23 in these prompts. In this paper, we generate images under PromptSet by Stable Diffusion v1.5-Base . Finally, without specification, we use 50 steps DDIM sampling .

From , though stable diffusion generates encoded VQ image latents . These latents preserve semantic information transformed by text prompt through cross-attention module (5). Notably, in the cross-attention module, the pixel is a weighted sum of token embedding with cross-attention map \(}(QK^{}/)\) as weights. The weights reveal the semantic information of token, as they are the correlations between image query \(Q\) and textual key \(K\). To check the correlation, we visualize the averaged cross-attention map over all layers of model \(}\) under different time steps \(t\), from 50 to 1.

Interestingly, the cross-attention map of each token already has a semantic shape in the early stage of the denoising process, e.g., for \(t=40\) in example Figure 0(a). This can hold only if the overall shape of the image is constructed in this early stage, so that each pixel can correspond to the correct token. To further investigate this, we compute the average cross-attention map of each token under the aforementioned PromptSet. We compare the shape of the cross-attention map and the final generated image quantitatively by transforming them into canny images  and computing the F1-score  (F1\({}_{t}\) for each \(t\)) between these canny images. To compare the difference over different time steps more clearly, we plot the relative F1-score F1\({}_{t}\)/F1\({}_{1}\) (\(t=1\) the image has been recovered). The result in Figure 0(b) shows the shape of the cross-attention map rapidly close to the ones of the generated image in the early stage of denoising, which is consistent with our speculation and the result in , where they conclude that the cross-attention map will converge during the denoising process.

### Frequency Analysis

To further explain the above phenomenon, we refer to the frequency-signal analysis. It has been well explored that the low-frequency signals represent the overall smooth areas or slowly varying components of an image (related to the overall shape). On the other hand, the high-frequency signals correspond to the fine details or rapid changes in intensity (related to attributes like textures) . Thereafter, to explain the "first overall shape then details" in the denoising process, it is natural to refer to the variations in frequency signals of images during the denoising process.

Mathematically, suppose the clean data (image latent) \(_{0}\) has \(M N\) dimensions for each channel with \(_{t}\) defined in (1). Then the Fourier transformation \(F_{_{t}}(u,v)\) (with \(u[M],v[N]\)) of \(_{t}\) is

\[ F_{_{t}}(u,v)&= _{k=0}^{M-1}_{l=0}^{N-1}_{t}^{kl}(-2 (+))\\ &=_{t}}F_{_{0}}(u,v)+_{t}}F_{_{t}}(u,v),\] (6)

Figure 1: Figure 0(a) is the averaged cross-attention over denoising steps. The two generated images are on the top, and the weights in cross-attention maps of each tokens are on the bottom with whiter pixels correspond to larger weights in cross-attention map. Figure 0(b) is obtained by taking average over tokens and prompts in PromptSet, which compares the shapes of cross-attention map and final generated images, Measured by relative F1-score F1\({}_{t}\)/F1\({}_{1}\) over different denoising steps.

where \(=\), and \(_{t}^{kl}\) is the \((k,l)\) component of \(_{t}\). As we do not now the distribution of \(_{0}\), we explore the \(F_{_{t}}(u,v)\) in sequel. The result is in the following proposition proved in Appendix A.

**Proposition 1**.: _For all \(u[M],v[N]\), with high probability, the complex number \(F_{_{t}}(u,v)\) satisfies_

\[\|F_{_{t}}(u,v)\|^{2}().\] (7)

This proposition indicates that under large image size (\(MN\)), the strength of frequency signals (no matter low or high) of standard Gaussian are equally close to zero. Thus, the frequency signal of \(_{0}\) in noisy data \(_{t}\) is mainly corrupted by the shrink factor \(_{t}\) due to (6), instead of the noise in it.

However, as visualized in Figure 1(a)1, in contrast to high-frequency part of image, the image's low-frequency parts 2 are more robust than the ones of high-frequency. For example, for \(t=20\) in Figure 1(a), the shape of the clock is still perceptible in the low-frequency part of the image. 3 If this fact is generalized to image latent, then it explains the two stages of generation as observed in Section 4.1. Because the low-frequency parts are not corrupted until the end of the adding noise process. Then, it will be recovered at the beginning of the reverse denoising process.

To investigate this, in Figures 1(b) and 1(c), we plot the averaged results over time steps of variation of low/high-frequency parts in images generated by \(\). In these figures, \(_{t}^{}\) is the low-frequency part of \(_{t}\) and vice-versa for high-frequency part \(_{t}^{}\). As can be seen, in Figure 1(c), the behavior of \(_{t}\) is similar under add/de noise processes, and the reconstruction of low-frequency signals is faster than the high-frequency signals. On the other hand, by comparing "Low.....Data" ( \(\|_{t}}_{0}^{}\|\)) and "High....Data" (\(\|_{t}}_{0}^{}\|\)) in Figure 1(b), we observe the strength of high-frequency signals are significantly lower than the low-frequency signals, which seems to be a property adopted from natural image . However, the relationship oppositely holds for Gaussian noise, which is implied by Proposition 1, as the frequency signals of noise \(_{t}\) under each spectrum are all close to zero, while the high-frequency parts contain 80% spectrum, so that \(_{t}^{}\) is larger than the \(_{t}^{}\).

These observations explain the phenomenon "first overall shape then details". Since the low-frequency parts of the image (decide overall shape) are not totally corrupted until the end of the noising process. Thus, they will be firstly recovered during the reverse denoising process, while the phenomenon does not hold for low-frequency parts of the image, as they are quickly corrupted during the noising process, so they will not be recovered until the end of denoising.

Figure 2: Figure 1(a) visualizes the completed noisy data and its high-frequency, and low-frequency parts over different time steps, listed from top to bottom. Figures 1(b) and 1(c) measure the low/high-frequency signals of \(_{t}\). In Figure 1(b), “Low_Add_Noisy_Data/eps” means the norm of \(_{t}}_{0}^{}\) and \(_{t}}_{t}^{}\), vice versa for “High....”. On the other hand, Figure 1(c) measures the variation ratio of high/low frequency parts of images during the noising/denoising process. For example, “High_Add_Noise” represents \(\|_{t}^{}-_{0}^{}\|/\|_{0}^{ }\|\) during noising process.

## 5 The Working Mechanism of Text Prompt

We have verified that the denoising process has two stages i.e., "first overall shape then details". Next, we explore the working mechanism of text prompts during these stages. Our main observations are two fold, 1): The special tokens [EOS] dominate the influence of text prompt. 2): The text prompt mainly works on the first overall shape reconstruction stage of the denoising process.

### [EOS] Contains More Information

In T2I diffusion, the text prompt is encoded by auto-regressive CLIP text encoder, with semantic tokens (SEM) enclosed with special tokes [SOS] and [EOS]. For such three classes of tokens, as the information in these tokens is conveyed by the cross-attention module, we first compute the averaged weights over pixels in the cross-attention map for each class. The weights are computed by taking the average over PromptSet and presented in Figure 3. As can be seen, the weights of [SOS] are significantly larger than the other classes. However, due to the CLIP text encoder is an auto-regressive model, [SOS] does not contain any semantic information. Therefore, we conclude that the influence of [SOS] is mainly adjusting the whole cross-attention map i.e., weights on the other tokens. To further verify this conclusion, we conduct experiments in Appendix I.1. A similar phenomenon is observed in single-modality LLM . As the information of text prompt is conveyed by semantic tokens and [EOS], we will focus on them instead of [SOS] in the sequel.

As both SEM and [EOS] contain the semantic information in the text prompt, we first explore which of them has larger impact on T2I generation. To this end, we select 3000 pairs of text prompts from PromptSet (2000 pairs for the template, the other 1000 pairs have complex prompts), where the two text prompts are represented as "[SOS] + Prompt \(A\) (\(B\)) + [EOS]\({}_{A(B)}\)". For each pair, we switch their [EOS] to construct the new text prompt pairs as "[SOS] + Prompt \(A\) (\(B\)) + [EOS]\({}_{B(A)}\)".

We examine the generated images under these artificially constructed text prompts (namely Switched-PromptSet (S-PromptSet)). We call \(A\) from Prompt\({}_{A}\) as "source" and \(B\) from [EOS]\({}_{B}\) as "target" for "[SOS] + Prompt \(A\) + [EOS]\({}_{B}\)", and vice versa. For the generated images under these prompts, we measure their alignments with the source and target prompts, respectively. The used metrics are the three standard ones in measuring text-image alignment: CLIPScore [30; 10], BLIP-VQA [18; 14], and MiniGPT4-CoT [51; 14] (details are in Appendix B).

The results are in Table 1. Surprisingly, the generated images under the constructed text prompts are more likely to be aligned with the target prompt instead of the source prompt. That says, even

Figure 4: Images under prompts from S-PromptSet with switched [EOS]. The objects are consistent with the ones conveyed by [EOS], while some information in semantic tokens is still conveyed.

Figure 3: Averaged weights in cross-attention map over pixels of three classes of tokens. For each prompt in PromptSet, the result is obtained by taking average over tokens in each class. The final result is the average over PromptSet. Notably, the weights on [SOS] are all larger than 0.9.

with prefixed irrelevant semantic tokens, the information contained in [EOS] dominates the denoising process (especially for overall shape) as in Figure 4. Thus, we conclude that the special tokens [EOS] have a larger impact than semantic tokens in prompt during T2I generation. We have two speculations about this phenomenon. 1): Owing to the auto-regressive encoded text prompt, unlike semantic tokens, [EOS] contains complete textual information, so that it decides the pattern of the generated image. 2): The number of [EOS] is usually larger than semantic tokens, as the prompt is enclosed by [EOS] to length 76. An ablation study in Appendix C verifies this speculation.

In summary, our conclusion in this subsection can be summarized as: _In T2I generation, the special token [EOS] decides the overall information (especially shape) of the generated image._

**Remark 1**.: _For the generated images under S-PromptSet, we find some information in semantic tokens is also conveyed, especially for the attribute information in it, e.g., "brown" color in the last image of the first row in Figure 4. We explore this in Appendix D and explain this as: unlike noun information, attributes in semantic tokens may not conflict with the contained information in [EOS] (which quickly decides the overall shape of the generated image), so that has potential to be conveyed._

### The Text Prompt Mainly Working on the First Stage

In Section 4.1, we have conclude that the denoising process is divided into two stages "first overall shape then details". Next, we explore the relationships between text prompts and the two stages. We start with special tokens [EOS] which contain major information in T2I generation. During the whole 50 denoising steps of T2I generation under prompts from S-PromptSet, we vary the starting point of substituting [EOS] i.e., the used text prompt is "[SOS] + Prompt \(A\) + [EOS]\({}_{B}\) (resp. [EOS]\({}_{A}\)) for \(t[,50]\) (resp. \(t[0,]\)) with "Start Step" \(\), i.e., Figure 5. We compare the alignments of generated images with source / target prompts as in Figure 6.

In Figure 6, the alignment with the target prompt slightly decreases, until the "Start Step" of substitution close to 50. This shows that the information in [EOS] has been conveyed during the first few steps of the denoising, which is the overall shape reconstruction stage according to Section 4.

Following the revealed working stage of [EOS], we explore whether the whole text prompt also works in this stage. If so, the T2I generation will only depends on \(}(t,_{t},)\) in (3) for small \(t\). To see this, we vary the \(w\) in (3) to control the injected information from the text prompt during the denoising process. Concretely, for \(a\) as the starting step of removing text prompt, i.e., during \(t[0,a)\), we use \(w=7.5\), and \(w=0\) for \(t[a,50]\), where \(a\). Then, the text prompt only works for \(t[0,a)\). We generate target images \(_{0}^{50}\) under PromptSet with standard denoising process (\(a=50\)), and compare them with the ones \(_{0}^{a}\) generated under varied \(a\) (Figure 7). The image-image alignments are measured by standard metrics CLIPScore and \(L_{1}\)-distance . To eliminate magnitude, we report relative results, i.e., "current minus worst" over "best minus worst".

The results are in Figure 7(a). During generation, the text information is absence for \(t[a,50]\), while Figure 7(a) indicates that alignments between \(_{0}^{a}\) and target \(_{0}^{50}\) will quickly be small only for large \(a\) (from 30 to 50). This shows that only if removing the textual information under large \(t\), its influence to generated image is removed. Therefore, we can conclude: _The information of text prompt is

     }} &  &  \\    & &  &  \\  Text-CLIPScore & 0.2363 & **0.2788** \\ BLIP-VQA\(\) & 0.3325 & **0.4441** \\ MiniQFT-CoT\(\) & 0.6473 & **0.7213** \\   

Table 1: The alignment of generated image with its source and target prompts. The prompts are constructed with switched [EOS].

Figure 5: Desnoising process under text prompt with switched [EOS] in \([a,50]\).

_conveyed during the early stage of denoising process._ Therefore, the overall shape of generated image is mainly decided by the text prompt, while the its details are then reconstructed by itself.

Discussion.Next, let us explain the phenomenon. Technically, in (3), the \(}(t,_{t},,)\) was proposed to approximate \(_{} p_{t}(_{t})/_{t}}\) with decomposition (\(p_{t}\) is the density of \(_{t}\))

\[_{} p_{t}(_{t})=_{} p_{t}( _{t})+_{} p_{t}(_{t}).\] (8)

Comparing (3) and (8), it holds \(}(t,_{t},)_{} p_{t} (_{t})\) + and \(w(}(t,_{t},)x-}(t, _{t},)) p_{t}(_{t})\). From , the denoising process (4) aims to maximize log-likelihood \( p_{0}(_{0})\). Then, moving along the direction \(_{} p_{t}(_{t})\) (leads to large \( p_{t}(_{t})\)) push \(_{t}\) to be aligned with the text prompt \(\) during the decreasing of \(t\). Adding such a moving direction is standard in conditional generation [25; 36; 8; 24]. As shown in Figure 7(b), during the denoising process, \(_{t}\) will gradually to be consistent with \(\), so that \( p_{t}(_{t})\) will decrease with \(t\). Thereafter, we observe the impact of text prompt conveyed by this term decreases with \(t 0\). Notably, owing to the quickly reconstructed overall shape of image in Section 4, the generated \(_{t}\) will quickly be consistent with \(\), so that explain the quickly decreasing \( p_{t}(_{t})\).

Footnote †: This can be verified by the training strategy of it in , where \(}(t,_{t},)\) is used to predict noise in noisy data without condition injected.

**Remark 2**.: _In this section, we verify the injected textual information are all conveyed in the first stage of diffusion process. In fact, this phenomenon is also generalized to the other types of information, e.g., conditional image information in subject-driven generation [44; 50], we verify this in Appendix H._

## 6 Application

Acceleration of Sampling.Since the information contained in text prompt is mainly conveyed by the noise prediction with condition \(}(t,_{t},)\), we can consider removing the evaluation of after the first few steps of denoising process. This is because the information in text prompt has been conveyed in this stage, and the computational cost can be significantly reduced without evaluating \(}(t,_{t},)\).

Therefore, we substitute the noise prediction \(}(t,_{t},,)\) as

\[}(t,_{t},,)=}(t,_{t},)+w(}(t, _{t},)-}(t,_{t},))&  a t;\\ }(t,_{t},)& 0 t<a.\] (9)

Figure 8: Figure 7(a) is the relative difference “current minus worst” over “best minus worst” under different start step \(a\) of Denoising process Figure 7. The last two figures 7(b) are per-dimensional norm of unconditional noise \(}(t,_{t},)\) and noise difference \(w(}(t,_{t},)-}(t,_{t},))\)

Figure 7: Desnoising with text prompt injected in \([0,a]\).

By varying \(a T\) in (9), the inference cost is reduced as an evaluation of \(}(t,_{t},)\) is saved.

To evaluate the saved computational cost of using noise prediction (9) during inference and the quality of generated data, we consider applying it on two standard samplers DDIM  and DPM-Solver  on a benchmark dataset MS-COCO  in T2I generation. We consider backbone models Stable-Diffusion (SD) v1.5-Base, SD v2.1-Base , and Pixart-Alpha . Concretely, we apply noise prediction (9) with varied \(a\) to generate 30K images from 30K text prompts in the test set of MS-COCO, for each sampler and backbone model. We compare the difference (measured by \(L_{1}\)-distance and Image-Level CLIPScore) between the generated images under \(a>0\) and \(a=0\) (the standard noise prediction). The results are in Table 2, where we also report the Frechet Inception Distance (FID) score  under each \(a\) to evaluate the quality of generated images.

The Table 2 indicates that proper \(a\) in (9) significantly reduces the computation cost during the inference stage without deteriorate the quality of generated images. For example, SD v1.5 with \(a=20\) saves 27+% computational cost, but generates images close to the baseline method (\(a=0\)).

  Start point \(a\) & \(0\) (baseline) & \(10\) & \(20\) & \(30\) & \(40\) & \(50\) & (baseline) & \(5\) & \(10\) & \(15\) & \(20\) & \(25\) \\  Sample \(F\) Backbone &  &  \\  Image-CLIPScore\(\) & 1.000 & 0.998 & 0.996 & 0.971 & 0.838 & 0.539 & 1.000 & 0.999 & 0.994 & 0.956 & 0.798 & 0.533 \\ \(L_{1}\)-distance \(\) & 0.000 & 0.011 & 0.022 & 0.043 & 0.087 & 0.195 & 0.000 & 0.015 & 0.027 & 0.050 & 0.100 & 0.188 \\ Saved Latency \(\) & 0.000* & 7.846* & 11.092 & 27.188 & 35.246* & 48.474* & 0.009 & 8.366* & 17.956 & 26.114 & 34.869 & 47.608 \\ FID \(\) & 1.372* & 17.370 & 13.805 & 14.012 & 15.048 & 19.296 & 14.297 & 14.286 & 14.725 & 14.985 & 15.860 & 19.758 \\ Text-CLIPScore\(\) & 31.040 & 31.031 & 30.894 & 30.493 & 30.493 & 28.176 & 16.682 & 30.92 & 30.921 & 30.792 & 30.205 & 26.843 & 16.721 \\  Sample \(F\) Backbone &  &  \\  Image-CLIPScore\(\) & 1.000 & 0.999 & 0.998 & 0.996 & 0.950 & 1.000 & 0.999 & 0.998 & 0.988 & 0.901 & 0.543 \\ \(L_{1}\)-distance \(\) & 0.000 & 0.017 & 0.041 & 0.077 & 0.152 & 0.386 & 0.000 & 0.026 & 0.046 & 0.083 & 0.160 & 0.369 \\ Saved Latency \(\) & 0.000 & 8.698 & 18.959* & 28.166 & 36.196 & 47.979* & 0.006 & 8.756 & 18.286 & 26.246 & 34.589 & 47.759 \\ FID \(\) & 1.3014 & 1.031 & 13.046 & 13.247 & 14.242 & 14.872 & 13.507 & 13.500 & 13.914 & 14.159 & 15.015 & 18.983 \\ Text-CLIPScore\(\) & 31.413 & 31.405 & 31.362 & 31.111s & 29.717 & 16.706 & 31.339 & 31.326 & 31.922 & 31.045 & 29.653 & 16.639 \\  Sample \(F\) Backbone &  \\  Image-CLIPScore\(\) & 1.000 & 0.999 & 0.935 & 0.744 & 0.643 & 0.522 & 1.000 & 0.999 & 0.993 & 0.911 & 0.648 & 0.625 \\ \(L_{1}\)-distance \(\) & 0.000 & 0.0024 & 0.058 & 0.103 & 0.169 & 0.247 & 0.000 & 0.013 & 0.022 & 0.046 & 0.098 & 0.199 \\ Saved Latency \(\) & 0.000* & 8.244 & 17.998* & 27.20* & 34.95* & 49.15* & 0.006* & 7.929* & 15.77* & 25.189 & 33.806 & 48.106 \\ FID \(\) & 22.651 & 22.884 & 23.258 & 25.485 & 29.760 & 36.525 & 18.669 & 18.520 & 18.798 & 19.358 & 20.494 & 26.159 \\ Text-CLIPScore\(\) & 28.157 & 27.979 & 25.719 & 19.650 & 14.986 & 14.386 & 30.733 & 30.721 & 30.745 & 29.416 & 20.629 & 14.928 \\  

Table 2: The difference between images generated under varied \(a\) with the ones of \(a=0\). The results are averaged over 30K generated images, and saved latency is evaluated on one V100 GPU.

Figure 10: The generated images with 25 steps DPM-Solver under \(}\) in (9) (Figure 9). The textual information is removed during \(t[0,a]\). With \(a 25\), the inference cost is decreased.

Conclusion

In this paper, we investigate the working mechanism of T2I diffusion model. By empirical and theoretical (frequency) analysis, we conclude that the denoising process firstly constructs the overall shape then details of the generated image. Next, we explore the working mechanism of text prompts. We find its special token [EOS] has a significant impact on the overall shape in the first stage of the denoising process, in which the information in the text prompt is conveyed. Then, the details of images are mainly reconstructed by themselves in the latter stage of generation. Finally, we apply our conclusion to accelerate the inference of T2I generation, and save 25%+ computational cost.