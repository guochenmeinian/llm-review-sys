# SplitNeRF: Split Sum Approximation Neural Field for Joint Geometry, Illumination, and Material Estimation

SplitNeRF: Split Sum Approximation Neural Field for Joint Geometry, Illumination, and Material Estimation

 Jesus Zarzar

KAUST

Bernard Ghanem

KAUST

###### Abstract

We present a novel approach for digitizing real-world objects by estimating their geometry, material properties, and environmental lighting from a set of posed images with fixed lighting. Our method incorporates into Neural Radiance Field (NeRF) pipelines the split sum approximation used with image-based lighting for real-time physically based rendering. We propose modeling the scene's lighting with a single scene-specific MLP representing pre-integrated image-based lighting at arbitrary resolutions. We accurately model pre-integrated lighting by exploiting a novel regularizer based on efficient Monte Carlo sampling. Additionally, we propose a new method of supervising self-occlusion predictions by exploiting a similar regularizer based on Monte Carlo sampling. Experimental results demonstrate the efficiency and effectiveness of our approach in estimating scene geometry, material properties, and lighting. Our method attains state-of-the-art relighting quality after only \(\)1 hour of training in a single NVIDIA A100 GPU.

## 1 Introduction

The idea of creating realistic and immersive digital environments has piqued the imagination of countless science fiction authors, science fiction directors, and scientists. In the past few years, the fields of computer graphics and computer vision have advanced so much that we are capable of

Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the ‘materials’ scene. Our method predicts high-frequency illumination with only \(\)1 hour of training thus enabling the efficient digitization of relightable objects.

creating photo-realistic environments [6; 28; 21], as well as capturing real-world environments in a way that allows us to render new photo-realistic views [25; 32]. However, the creation of digital twins  of objects that can be integrated within photo-realistic environments still requires artists to meticulously hand-design realistic object meshes, materials, and lighting. While this is feasible for generating a few scenes, large-scale digitization requires automatic ways of reconstructing real-world objects along with their corresponding material properties.

In this work, we address the problem of object inverse rendering: extracting object geometry, material properties, and environment lighting from a set of posed images of the object. Inverse rendering enables the seamless integration of virtual objects into different environments with varying illumination conditions from simple image captures taken by commonplace camera sensors.

Neural rendering methods, such as Neural Radiance Fields (NeRF) [18; 1; 34], have revolutionized novel view synthesis, 3D reconstruction from images, and inverse rendering. By directly modeling outgoing radiance at each point in 3D space, NeRF methods excel at accurately recovering scene geometry and synthesizing novel views. However, a drawback of this approach is that the learned radiance representation entangles environment lighting with the rendered scene's properties, making it challenging to recover material properties and illumination. Due to the success of NeRFs in reconstructing scenes, several works have proposed modifications to enable inverse rendering [29; 3; 16; 10; 41]. These works build upon NeRF by decomposing radiance into a function of illumination and material properties but differ in their ways of modeling lighting and reflections. We build upon these methods with the main goal of efficiency without sacrificing reconstruction quality or the ability to recover high-frequency illumination details.

To achieve these goals, we rely on the split sum approximation , which is commonly used in efficient image-based lighting techniques and has been successfully applied for inverse rendering before [3; 20]. This approximation involves splitting the surface reflectance equation into two factors: one responsible for pre-integrating illumination and the other for integrating material properties. The separation allows us to estimate pre-integrated illumination using a Multi-Layer Perceptron (MLP). This manner of modeling the pre-integrated illumination function is inspired by the modeling of radiance fields, which model a complex integral of lighting and material properties using an MLP. Correspondingly, our illumination representation inherits beneficial properties observed with the modeling of radiance fields such as smoothness. While MLPs representing pre-integrated illumination have been previously exploited [14; 13], previous works do not supervise the MLP's learning, leading to physically inaccurate illumination representations. We propose a novel regularizer based on Monte Carlo sampling to ensure accurate learning of illumination.

However, the split sum approximation on its own does not take into account self-occlusions. This hinders material property estimation since shadows tend to be incorrectly attributed to objects' albedo. Thus, we derive an occlusion factor to alleviate the effect of self-occlusions. This factor is then approximated via Monte Carlo sampling and used to supervise an occlusion MLP.

Altogether, our method is capable of attaining state-of-the-art relighting results with under an hour of training on a single NVIDIA A100 GPU.

**Contributions.** We claim the following contributions:

**(i)** We propose a novel representation for pre-integrated illumination as a single MLP with a corresponding regularization to ensure learning a physically-meaningful representation.

**(ii)** We derive a method for approximating the effect of self-occlusions on pre-integrated lighting and use it to supervise an occlusion MLP improving material estimation.

**(iii)** We demonstrate the effectiveness of our method in extracting environmental lighting, geometry, and material properties by achieving competitive reconstruction and relighting quality on both synthetic and real data with under one hour of training on a single NVIDIA A100 GPU.

## 2 Related works

The problem of digitizing real-world objects and environments has long been a subject of active research in computer vision and computer graphics. We approach this problem through the lenses of neural rendering and neural inverse rendering; paradigms with lots of recent attention. We now provide a brief overview of related works in these areas.

### Neural rendering and 3D reconstruction

Novel view synthesis is the task of rendering new views of a scene given a set of observations of the scene. Neural Radiance Fields (NeRF)  and its variants [1; 34; 19; 4; 26] have demonstrated remarkable success in the task of novel view synthesis. NeRF directly models the volumetric scene function by predicting radiance and density at each 3D point in space while supervising learning with a photometric reconstruction loss. Due to its success in implicitly learning accurate 3D reconstructions, several works have branched out to reconstruct accurate meshes through neural rendering [22; 31]. Signed Distance Function (SDF)-based methods [36; 40; 37; 12] model density as a function of the SDF to obtain well-defined surfaces. By increasing sharpness during training in the conversion from SDF to density these methods can transition from volume rendering to surface rendering as they train. While effective, these methods suffer from entangled representations of scene geometry, material properties, and lighting. Our work follows the surface rendering pipeline proposed in  but reformulates the radiance prediction to disentangle environment lighting and material properties.

### Neural inverse rendering

The task of inverse rendering is a long-standing problem in computer graphics which consists of estimating the properties of a 3D scene such as shape, material, and lighting from a set of image observations. The success of neural rendering methods for novel view rendering and 3D reconstruction has led to a variety of works [2; 45; 43; 29; 3; 20; 46; 42; 14; 16; 30] exploiting neural rendering for inverse rendering. Due to the challenging nature of this problem, multiple simplifying assumptions have been adopted. Some works simplify the modeling of lighting by using low-frequency representations such as spherical gaussians [2; 45; 43; 29; 46; 39; 10] or low-resolution environment maps [2; 43; 46]. While this approximation generally allows for closed-form solutions of the rendering integral, it does not capture natural high-frequency illumination. Our work leverages the split sum approximation , proposed for real-time rendering of image-based global illumination to enable the learning of high-frequency environment lighting. The split sum approximation has been adopted by several inverse rendering methods [3; 20; 17; 13; 14]. These works represent pre-integrated lighting with an autoencoder-based illumination network [3; 13], with a set of learnable images for different roughness levels [20; 17], or with an MLP with integrated spherical harmonic encoding as input . Autoencoder-based methods rely on learnt illumination features incompatible with existing rendering pipelines. Learnable images are susceptible to noise and require re-integrating illumination whenever the base illumination is updated. Lastly, using integrated encodings to avoid integrating light leads to a representation that is not physically based. In contrast, we propose modeling pre-integrated lighting as the output of an MLP paired with a novel regularization, which ensures the network correctly learns to represent physically-based pre-integrated lighting. An issue arising from the split sum approximation is that pre-integrated illumination is blind to geometry and does not account for the occlusion of light sources due to geometry throughout the scene. Our work tackles this issue by supervising the prediction of ambient occlusion through Monte Carlo sampling.

## 3 Methodology

Our method aims to extract a scene's geometry, material properties, and illumination from a set of posed images of the scene. We accomplish this by incorporating a decomposed formulation of radiance into a surface rendering pipeline. In the following sections, we begin with an overview of the surface rendering pipeline. We then detail the physically-based radiance formulation, which allows us to decompose radiance into illumination and material properties. Next, we describe our proposed MLP representation for illumination along with the additional loss term it requires. Afterward, we derive a method for estimating an occlusion factor to account for visibility within the split sum approximation. Finally, we describe additional regularization used to facilitate learning.

### Overview of neural rendering

Neural volume rendering relies on learning two functions: \((;):^{3}\) which maps a point in space \(\) onto a density \(\), and \(_{o}(,_{o};):^{3}^ {3}^{3}\) that maps point \(\) viewed from direction \(_{o}\) onto outgoing radiance \(_{o}\). The parameters \(\) that define the density and radiance functions are typically optimized to represent a single scene by using multiple posed views of the scene. To learn these functions, they are evaluated at multiple points along a ray \((t)=-t_{o}\), \(t[t_{n},t_{f}]\), definedby the camera origin \(^{3}\), pixel viewing direction \(_{o}\), and camera near and far clipping planes \(t_{n}\) and \(t_{f}\). A pixel color for the ray can then be obtained through volume rendering via:

\[}(;)=_{t_{n}}^{t_{f}}T(t)\;((t))\;_{o}((t),_{o})\;t, \;T(t)=(-_{t_{n}}^{t}((s))\; s).\] (1)

In practice, a summation of discrete samples along the ray is used to approximate the integral. This volume rendering process allows us to supervise the learning of implicit functions \(L_{o}\) and \(\), in a pixel-wise fashion through the reconstruction loss:

\[_{}(R;)=_{ R }\|()-}(;)\|_{2 }^{2},\] (2)

where \(R\) is a batch of rays generated from a random subset of pixels from training images.

The learned geometry can be improved if, instead of directly predicting density \(\), a signed distance field (SDF) is learned and then mapped to density. To this end, we follow the SDF formulation proposed in NeuS . Learning a valid SDF requires the use of an additional Eikonal loss term \(_{}\). For more details, please refer to .

Since volume density \(\) depends only on a point's position in space while output radiance \(L_{o}\) depends on both position and viewing direction, neural rendering networks are typically split into a spatial network and a radiance network. As shown in fig. 2, we maintain the spatial network to estimate density along with additional material properties but rely on a physically-based  radiance estimation instead of a radiance network.

### Physically-based rendering

Given knowledge of a scene's geometry, material properties, and illumination, it is possible to model the outgoing radiance \(_{o}(,_{o})\) reflected at any position \(\) of an object's surface in direction \(_{o}\) by integrating over the hemisphere \(\) defined by the surface's normal \(\) using the reflectance equation:

\[_{o}=_{}(_{d}}{}+ _{s})_{i}_{i}, d_{i},\] (3)

where \(_{i}\) is the incoming radiance, \(\) is the object's diffuse albedo, and \(_{d}\) and \(_{s}\) are material properties dependent on the object's Bidirectional Reflectance Distribution Function (BRDF). For clarity, we omit from notation the dependency of incoming radiance on \(_{i}\) as well as the dependency of material properties on position \(\). Radiance \(_{o}\) has diffuse and specular components \(_{d}\) and \(_{s}\). Image-based lighting methods often employ the split sum approximation to calculate specular lighting \(_{s}\) by splitting the integral into two components: one containing the incoming light \(_{i}\), and one depending only on material properties. We use the Disney  microfacet BRDF parameterized

Figure 2: **Proposed architecture.** A spatial network maps spatial coordinates \(\) into geometry (\(\)), material properties (albedo \(}\), metalness \(\), and roughness \(\)), and occlusion factors (\(}\)). The pre-integrated illumination MLP predicts both specular \(_{s}(_{r},)\) and diffuse \(_{d}(},=1)\) terms by using the predicted normals \(}\), roughness, and the reflection vector \(_{r}\) of view direction \(_{o}\). Finally, the specular and diffuse terms are combined with material properties to compute output radiance \(}_{o}\).

by albedo, metalness, and roughness. The specular component is modeled with the Cook-Torrance GGX [33; 35] BRDF, leading to the following approximation:

\[_{s}D(_{i},_{r},) _{i}_{i}, d_{i}}{_{ }D(_{i},_{r},)_{i}, d _{i}}_{}_{s}_{i},  d_{i}=g(_{r},)_{}_{s} _{i}, d_{i},\] (4)

where \(D(_{i},_{r},)\) is the microfacet normal distribution function dependent on the direction of light reflection \(_{r}\) as well as the surface roughness \(\). The term on the right can be pre-computed as in  since it depends only on the BRDF and not a scene's lighting. The term on the left in Equation (4) depends on the scene's lighting and the microfacet distribution function \(D(_{i},_{r},)\). The following sections refer to this term as \(g(_{r},)\) and aim to estimate it with an MLP representation. As shown in fig. 2, we estimate an object's albedo \(}\), metalness \(\), and roughness \(\) from the spatial network.

### Pre-integrated illumination MLP representation

We propose to estimate the pre-integrated lighting \(g(_{r},)\) at different roughness levels through a pre-integrated illumination MLP \((_{r},)\). Please refer to Appendix A.2 for details on how \((_{r},)\) is used to calculate \(}_{d}\) and \(}_{s}\). The predictions \(\) should accurately represent the environment lighting at different levels of roughness. We achieve this through a loss term based on Monte Carlo estimates \(\) of the original integral for varying roughness and reflected directions using the predicted environment map \(}_{i}()\) which can be extracted from \(\) querying perfect specular reflections \((,0)\).

\[_{}()=|} _{s}\|(s)-(s)\|_{2}^{2},(s)=}D(_{i},_{s},_{s })(_{i},0)_{i},_{s}}{_{ _{i}}D(_{i},_{s},_{s})_{i}, _{s}},\] (5)

where the set \(\) consists of paired samples of directions \(_{s}\) and roughness \(_{s}\). Directions are taken uniformly on a sphere, and half the roughness samples are taken uniformly in the range \(\) with the other half fixed to \(1\) to ensure correct learning of diffuse lighting. Please refer to Appendix A.1 for a detailed derivation of \((s)\). The set \(\) of light direction samples is also taken uniformly on a sphere. While a different sampling could lead to reduced variance, we utilize uniform spherical sampling for \(_{i}\) to be more computationally efficient. Uniform spherical sampling allows us to share light samples across the batch of predictions, thus reducing the number of evaluation calls to the light function \((,0)\). We visualize both \(g\) and \(\) in fig. 3 for a specific scene.

### Occlusion factors

The split sum approximation does not consider the occlusion of light sources due to geometry. Occlusions can be incorporated by multiplying incoming light \(_{i}\) by a binary visibility function \(V_{i}\):

\[_{o}^{V}=_{}(_{d}}{} +_{s})_{i}V_{i}_{i}, d _{i},\] (6)

Figure 3: **Pre-integrated environment illumination.** We visualize the pre-integrated illumination \((_{r},)\) for varying roughness values along our model’s prediction for the ‘toaster’ scene. Our pre-integrated illumination MLP accurately approximates pre-integrated lighting across roughness values thanks to our novel regularization loss based on Monte Carlo sampling.

with \(V_{i}\) taking a value of \(1\) when there are no occlusions and \(0\) when incoming light is occluded by geometry. Both diffuse and specular integrals can be rewritten to incorporate visibility via occlusion factors \(_{d}()\) and \(_{s}()\) which multiply the split sum diffuse and specular lighting terms respectively. We propose learning the occlusion factors \(()\) with an MLP by supervising them with Monte Carlo estimates \(}()\) using the predicted geometry. Please refer to Appendix A.3 for the derivation of Monte Carlo estimates \(}()\). Given the Monte Carlo estimates, we supervise the predicted occlusion terms \(}()\) as follows:

\[_{}()=|}_{x }w\|}()-}( )\|_{2}^{2},\] (7)

where the sample set \(\) is a random subset of the points sampled for volume rendering, and the weights \(w\) are the corresponding normalized volume rendering weights. Weighting the loss function by the volume rendering weights is required so that the occlusion prediction focuses only on learning surface points. The output radiance at each point in space is then calculated as follows:

\[}_{o}=(}_{d}*}_{d}+}_{s}*}_{s}),\] (8)

where \(\) maps the predicted output radiance \(}_{o}\) from linear to sRGB space.

### Material regularization

To better learn material properties, we introduce a soft regularizer to reduce the prediction of metallic materials. This encourages the model to prefer explaining outgoing radiance through albedo and roughness whilst still allowing the prediction of metallic materials. We implement this regularization as a weighted \(L_{2}\) loss with the same weighting as for the occlusion loss in Equation (7). That is,

\[_{}()=|}_{ }w\|()|_{2}^{2}.\] (9)

## 4 Experiments

### Baselines

We compare against Nerfactor , NVDiffRec , NVDiffRecMC , NeRO , NMF , and TensoIR . Due to the differing evaluation methodologies among these works, we train all baseline methods following publicly released code and report metrics as detailed in the following.

Figure 4: **Occlusion loss visualization. We visualize the albedo and occlusion predicted by our method with and without the proposed occlusion regularization loss. When no regularization is used, we observe that the occlusion prediction fails at disentangling shadows from the albedo. Additionally, darker materials might wind up with lighter albedos due to occlusion overcompensation.**

### Experimental setup

**Datasets.** We report results using the NeRFactor  dataset along with extended versions of the NeRF Blender  (Blender) and the RefNeRF Shiny Blender  (Shiny Blender) datasets. The NeRFactor dataset consists of four synthetic scenes, where test images are rendered under eight different low-frequency lighting conditions. The Blender dataset consists of eight synthetic scenes representing a mix of glossy, specular, and Lambertian objects, while the Shiny Blender dataset consists of six highly reflective synthetic scenes. To showcase the ability of our model to estimate high-frequency environment lighting, we extend the Blender and Shiny Blender datasets by rendering all objects under seven novel high-frequency lighting conditions. All models are trained using \(100\) posed images, and evaluated on \(20\) test images consisting of novel views for each lighting condition. Finally, we report qualitative results on real-world objects using the CO3D  dataset. Each object in the dataset consists of a set of images captured along a circular path along with automatically extracted foreground segmentation masks. We estimate each image's camera pose with Colmap .

**Relighting evaluation.** We extract geometry from our model in the form of a triangular mesh by using marching cubes . At each predicted mesh vertex, we estimate material properties in the form of an albedo, metalness, and roughness. We then render the predicted geometry using Blender's  physically-based shader. Material properties across faces are obtained by interpolating the predicted vertex material properties. We utilize the same Blender rendering pipeline to compute relighting metrics for baselines where explicit meshes and material properties are extracted. Otherwise, predictions are rendered using the provided relighting methodology. Before evaluating metrics, a per-channel scaling factor is computed for each scene to compensate for the albedo-lighting ambiguity. We evaluate the predicted scenes for the NeRFactor, Blender, and Shiny Blender datasets and report the average Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM) , and Learned Perceptual Image Patch Similarity (LPIPS)  in table 1 and table 2. Metrics are reported as an average across \(20\) test images and across all illumination maps for each dataset. This metric gives an aggregated performance measure for geometry and material property estimation. Our method attains competitive relighting performance while maintaining a low runtime.

  &  &  \\ 
**Method** &  &  &  &  \\   & **MAE \(\)** & **PSNR \(\)** & **SSIM \(\)** & **LPIPS \(\)** & **MAE \(\)** & **PSNR \(\)** & **SSIM \(\)** & **LPIPS \(\)** \\ 
**NeFactor** & 30.49 & 23.53 & 0.910 & 0.109 & 23.66 & 0.895 & 0.120 & 20 hr. \\
**NVDIffRec** & 26.47 & 23.05 & 0.901 & 0.123 & 21.88 & 0.880 & 0.111 & 0.98 hr. \\
**NVDIffRecMC** & 25.98 & 23.84 & 0.918 & 0.114 & 24.06 & 0.902 & 0.099 & 2.95 hr. \\
**NeRO** & 30.59 & 22.83 & 0.897 & 0.117 & 23.68 & 0.907 & 0.093 & 18.38 hr. \\
**NMF** & 24.14 & - & - & - & 22.23 & 0.895 & 0.093 & 2.91 hr. \\
**TensoIR** & 22.90 & 25.21 & **0.929** & **0.087** & 23.78 & 0.907 & 0.100 & 3.53 hr. \\
**Ours** & **17.52** & **25.29** & 0.924 & 0.108 & **27.31** & **0.941** & **0.061** & **0.81 hr.** \\  

Table 1: **NeRFactor metrics. We evaluate the reconstruction quality of our method against the baselines using \(20\) test images and \(8\) low-frequency illumination maps for each scene from the NeRFactor dataset. We scale albedo and relit images with a per-channel factor before computing metrics. Our method attains competitive performance across all metrics with a low runtime.**

  &  &  \\ 
**Method** &  &  &  &  \\   & **MAE \(\)** & **PSNR \(\)** & **SSIM \(\)** & **LPIPS \(\)** & **MAE \(\)** & **PSNR \(\)** & **SSIM \(\)** & **LPIPS \(\)** \\ 
**NVDIffRec** & 26.52 & 20.11 & 0.857 & 0.138 & 23.64 & 21.39 & 0.848 & 0.177 \\
**NVDIffRecMC** & 24.74 & 22.50 & 0.884 & 0.136 & 13.75 & 24.60 & **0.911** & 0.151 \\
**NeRO** & 31.59 & 18.47 & 0.847 & 0.158 & **04.11** & 16.82 & 0.844 & 0.203 \\
**NMF** & 20.66 & 21.21 & 0.881 & 0.118 & 06.85 & 24.20 & 0.908 & **0.136** \\
**TensoIR** & 18.05 & 22.58 & 0.891 & 0.120 & 13.14 & 22.33 & 0.840 & 0.193 \\
**Ours** & **16.18** & **22.73** & **0.906** & **0.106** & 09.07 & **24.96** & 0.904 & 0.144 \\  

Table 2: **Blender and Shiny Blender metrics. We report the average of relighting reconstruction metrics and normal error for our extended Blender and Shiny Blender datasets. Metrics are computed as the average of \(20\) test views across \(7\) high-frequency illumination conditions for each scene. We scale images by a per-channel factor for relighting metrics. Our method outperforms the baselines across all metrics for the Blender dataset and has a higher PSNR for the Shiny Blender dataset.**

**Albedo evaluation.** In addition to overall relighting quality, we evaluate the ability of our method to recover albedo. We report reconstruction metrics (PSNR, SSIM, and LPIPS) on the predicted albedo in table 1. As with the relighting metrics, we apply a per-scaling factor to the albedo predictions before computing reconstruction metrics. Metrics are reported as an average across all \(20\) test images for each scene in the NeRFactor dataset. We do not report albedo metrics for other datasets due to the lack of ground truth. We exclude results from NMF  since the albedo in their lighting formulation is not comparable to the other methods. Thanks to our proposed occlusion factor and material regularization, our method is on average better able to reconstruct albedo.

**Normals evaluation.** We measure the pixel-wise Mean Absolute Error (MAE) between ground truth and predicted normal images to evaluate geometric reconstruction quality. The MAE is weighted by ground truth alpha values to lower the effect of prediction errors at object borders. Our method recovers a good estimate of geometry for most scenes as evidenced in tables 1 and 2.

**Real-world qualitative results.** In real-world scenes, the far-field illumination assumption is violated and objects don't follow any specific BRDF model as opposed to synthetic scenes. Both of these differences make inverse rendering from real-world data a much more challenging task than with synthetic data. Therefore, we provide qualitative results in fig. 5 to validate our method on real-world captures from the CO3D dataset. It can be observed that even in this challenging scenario our method is capable of recovering accurate object geometry, as well as providing a reasonable estimation of material properties and environment maps.

## 5 Discussions

### Ablations

**Occlusion loss.** We visualize the effects of the proposed occlusion loss in fig. 4. Learning an occlusion factor without supervision leads to errors in the albedo predictions due to the inability to disentangle shadows from object color. By explicitly supervising an occlusion factor we observe better albedo color predictions such as visualized in the blue box in the hotdog example, and all boxes in the drums example. Additionally, shadows are better disentangled from albedo as observed in the red and green boxes for the hotdog example. Quantitatively, we measure the importance of adding the occlusion loss to our model in table 3, where it improves both relighting and albedo reconstruction.

**Occlusion averaging.** The occlusion factor we derive is a per-channel factor that depends on estimated lighting. However, since both are being learned jointly, we observe that training can be noisy. We find in table 3 that relighting and albedo reconstruction both improve when we supervise the occlusion factors \(_{d}\) and \(_{s}\) with their per-channel averages instead. Assuming that all channels of the occlusion factor are equal is equivalent to assuming only white light with varying intensities, which reduces noise during training and uses fewer parameters.

Figure 5: **Qualitative real-world results.** We present qualitative results on four scenes from the CO3D dataset. Our method can successfully recover object geometry, material properties, and illumination even for challenging scenes captured in the wild.

**Material regularization.** We measure the effect of the material regularization in table 3. Penalizing metalness prediction discourages our model from explaining radiance through environment lighting with overpredicted metallic surfaces. This leads to improved albedo predictions as shown in table 3. However, as visualized in fig. 1, the loss coefficient is small enough such that our model is still capable of correctly predicting metallic surfaces.

## 6 Conclusion and limitations

In conclusion, we present a novel and efficient method for inverse rendering based on neural surface rendering and the split sum approximation for image-based lighting. Owing to our proposed integrated illumination MLP, we can jointly estimate geometry, lighting, and material properties in under one hour using a single NVIDIA A100 GPU. Physical accuracy of our pre-integrated MLP representation is ensured thanks to the proposed illumination regularization. Additionally, we define occlusion factors for diffuse and specular lighting so that self-occlusions are accounted for with the split sum approximation. Finally, we propose a way of supervising occlusion MLPs to learn the proposed occlusion estimators. Altogether, our method produces high-quality estimates of geometry, lighting, and material properties as measured by rendering objects under unseen views and lighting conditions.

However, due to the highly complex problem that inverse rendering presents, our method comes with some limitations. The major assumptions we rely on come from using image-based lighting, the split sum approximation, and Monte Carlo sampling. Image-based lighting assumes that light sources are located infinitely far away from the scene, leading to errors when this assumption is violated. While we have tackled the problem of missing self-occlusions within the split sum approximation, we disregard the effect of indirect illumination. This has a noticeable impact on albedo for reflective surfaces such as the 'toaster' scene. Additionally, we only consider the reflection of light and don't model transmission and subsurface scattering effects. Finally, we use a low number of uniform Monte Carlo samples for the occlusion loss leading to errors due to strong and small light sources. This is mostly noticeable in albedo predictions for objects in the synthetic Blender dataset, where shadows can still be noticed in the albedo predictions. We hope future works will tackle these limitations.

  &  &  &  \\   & **MAE \(\)** & **PSNR \(\)** & **SSSIM \(\)** & **LPIPS \(\)** & **PSNR \(\)** & **SSIM \(\)** & **LPIPS \(\)** \\ 
**Ours (w/o Occ. Avg.)** & **17.23** & 24.73 & 0.919 & 0.109 & 27.25 & 0.941 & 0.061 \\
**Ours (w/o Occ. Loss)** & 17.29 & 22.13 & 0.900 & 0.122 & 26.40 & 0.936 & 0.064 \\
**Ours (w/o Met. Reg.)** & 17.82 & 23.92 & 0.916 & 0.123 & 26.56 & 0.936 & 0.066 \\
**Ours** & 17.52 & **25.29** & **0.924** & **0.108** & **27.31** & **0.941** & **0.061** \\  

Table 3: **NeRFactor ablation results. We report reconstruction and relighting metrics for different variations of our methodology on the NeRFactor dataset. While the proposed regularizations do not have a noticeable effect on geometry, they all lead to improvements in albedo and relighting quality.**

Figure 6: **Blender and Shiny Blender illumination visualizations. We visualize the predicted illumination environment maps for our method and baselines for two scenes in the Blender dataset and two scenes in the Shiny Blender dataset. Illumination is scaled by a per-channel factor to account for albedo-illumination ambiguity. Our proposed illumination inherits smoothness from the MLP representation but still captures high-quality details such as trees.**Acknowledgements

The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI, under award number 5940.