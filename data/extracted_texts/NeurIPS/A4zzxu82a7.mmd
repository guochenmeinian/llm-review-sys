# Koopa: Learning Non-stationary Time Series

Dynamics with Koopman Predictors

Yong Liu, Chenyu Li, Jianmin Wang, Mingsheng Long

School of Software, BNRist, Tsinghua University, China

{liuyong21,lichenyu20}@mails.tsinghua.edu.cn, {jinwang,mingsheng}@tsinghua.edu.cn

Equal Contribution

###### Abstract

Real-world time series are characterized by intrinsic non-stationarity that poses a principal challenge for deep forecasting models. While previous models suffer from complicated series variations induced by changing temporal distribution, we tackle non-stationary time series with modern Koopman theory that fundamentally considers the underlying time-variant dynamics. Inspired by Koopman theory that portrays complex dynamical systems, we disentangle time-variant and time-invariant components from intricate non-stationary series by _Fourier Filter_ and design _Koopman Predictor_ to advance respective dynamics forward. Technically, we propose **Koopa** as a novel **Koopman** forecaster composed of stackable blocks that learn hierarchical dynamics. Koopa seeks measurement functions for Koopman embedding and utilizes Koopman operators as linear portraits of implicit transition. To cope with time-variant dynamics that exhibits strong locality, Koopa calculates context-aware operators in the temporal neighborhood and is able to utilize incoming ground truth to scale up forecast horizon. Besides, by integrating Koopman Predictors into deep residual structure, we ravel out the binding reconstruction loss in previous Koopman forecasters and achieve end-to-end forecasting objective optimization. Compared with the state-of-the-art model, Koopa achieves competitive performance while saving \(77.3\%\) training time and \(76.0\%\) memory. Code is available at this repository: [https://github.com/thuml/Koopa](https://github.com/thuml/Koopa).

## 1 Introduction

Time series forecasting has become an essential part of real-world applications, such as weather forecasting, energy consumption, and financial assessment. With numerous available observations, deep learning approaches exhibit superior performance and bring the boom of deep forecasting models. TCNs  utilize convolutional kernels and RNNs  leverage the recurrent structure to capture underlying temporal patterns. Afterward, attention mechanism  becomes the mainstream of sequence modeling and Transformers  show great predictive power with the capability of learning point-wise temporal dependencies. And the recent revival of MLPs  presents a simple but effective approach to exhibit temporal dependencies by dense weighting.

In spite of elaboratively designed models, it is a fundamental problem for deep models to generalize on varying distribution , which is widely reflected in real-world time series because of inherent non-stationarity. Non-stationary time series is characterized by time-variant statistics and temporal dependencies in different periods , inducing a huge distribution gap between training and inference and even among each lookback window. While previous methods  tailor existing architectural design to attenuate the adverse effect of non-stationarity, few works research on the theoretical basis that can be applied to deal with time-variant temporal patterns naturally.

From another perspective, real-world time series acts like time-variant dynamics . As one of the principal approaches to analyze complex dynamics, Koopman theory  provides an enlightenment to transform nonlinear system into measurement function space, which can be described by a linear Koopman operator. Several pilot works accomplish the integration with deep learning approaches by employing autoencoder networks  and operator-learning [26; 49]. More importantly, it is supported by Koopman theory that for time-variant dynamics, there exists a coordinate transformation of the system, where localized Koopman operators are valid to describe the whole measurement function space into several subspaces with linearization [23; 36]. Therefore, Koopman-based methods are appropriate to learn non-stationary time series dynamics (Figure 1). Besides, the linearity of measurement function space enables us to utilize spectral analysis to interpret nonlinear systems.

In this paper, we disentangle non-stationary series into time-invariant and time-variant dynamics and propose **Koopa** as a novel **Koopman** forecaster, which is composed of modular **K**oopman **P**redictors (**KP**) to hierarchically describe and advance forward series dynamics. Concretely, we utilize Fourier analysis for dynamics disentangling. And for time-invariant dynamics, the model learns Koopman embedding and linear operators to reveal the implicit transition underlying long-term series. As for the remaining time-variant components that exhibit strong locality, Koopa performs context-aware operator calculation and adaptation within different lookback windows. Besides, Koopman Predictor goes beyond the canonical design of Koopman Autoencoder without the binding reconstruction loss, and we incorporate modular blocks into deep residual architecture  to realize end-to-end time series forecasting. Our contributions are summarized as follows:

* From the perspective of modern dynamics Koopman theory, we propose _Koopa_ composed of modular _Fourier Filter_ and _Koopman Predictor_, which can hierarchically disentangle and exploit time-invariant and time-variant dynamics for time series forecasting.
* Based on the linearity of Koopman operators, the proposed model is able to utilize incoming series and adapt to varying dynamics for scaling up forecast horizon.
* Compared with state-of-the-art methods, our model achieves competitive performance while saving \(77.3\%\) training time and \(76.0\%\) memory averaged from six real-world benchmarks.

## 2 Related Work

### Time Series Forecasting with DNNs

Deep neural networks (DNNs) have made great breakthroughs in time series forecasting. TCN-based models [4; 43; 47] explore hierarchical temporal patterns and adopt shared convolutional kernels with diverse receptive fields. RNN-based models [12; 22; 37] utilize the recurrent structure with memory to reveal the implicit transition over time points. MLP-based models [32; 51; 52] learn point-wise weighting and the impressive performance and efficiency highlight that MLP performs well for modeling simple temporal dependencies. However, their practical applicability may still be constrained on non-stationary time series, which is endowed with time-variant properties and poses challenges for model capacity and efficiency. Unlike previous methods, Koopa fundamentally considers the complicated dynamics underlying time series and implements efficient and interpretable transition learners in both time-variant and time-invariant manners inspired by Koopman theory.

Recently, Transformer-based models have also achieved great success in time series forecasting. Initial attempts [19; 27; 48; 53] renovate the canonical structure and reduce the quadratic complexity

Figure 1: The measurement function \(g\) maps between non-stationary time series and the nonlinear dynamical system so that the timeline will correspond to a system trajectory. Therefore, time series variations in different periods are reflected as sub-regions of nonlinear dynamics, which can be portrayed and advanced forward in time by linear Koopman operators \(\{_{1},_{2},_{3}\}\) respectively.

for long-term forecasting. However, recent studies [16; 21] find it a central problem for Transformer and other DNNs to generalize on varying temporal distribution and several works [15; 16; 21; 28] tailor to empower the robustness against shifted distribution. Especially, PatchTST  boosts Transformer to the state-of-the-art performance by channel-independence and instance normalization  but may lead to unaffordable computational cost when the number of series variate is large. In this paper, our proposed model supported by Koopman theory works naturally for non-stationary time series and achieves the state-of-the-art forecasting performance with remarkable model efficiency.

### Learning Dynamics with Koopman Operator

Koopman theory  has emerged over decades as the dominant perspective to analyze modern dynamical systems . Together with Dynamic Mode Decomposition (DMD)  as the leading numerical method to approximate the Koopman operator, significant advances have been accomplished in aerodynamics and fluid physics [3; 9; 30]. Recent progress made in Koopman theory is inherently incorporated with deep learning approaches in the data science era. Pilot works [29; 40; 50] leverage data-driven approaches such as Koopman Autoencoder to learn the measurement function and operator simultaneously. PCL  further introduces a backward procedure to improve the consistency and stability of the operator. Based on the capability of Koopman operator to advance nonlinear dynamics forward, it is also widely applied to sequence prediction. By means of Koopman spectral analysis, MDKAE  disentangles dominant factors underlying sequential data and is competent to forecast with specific factors. K-Forecast  utilizes Koopman theory to handle nonlinearity in temporal signals and propose to optimize data-dependent basis for long-term time series forecasting. By leveraging predefined measurement functions, KNF  learns Koopman operator and attention map to cope with time series forecasting with changing temporal distribution.

Different from previous Koopman forecasters, we design modular Koopman Predictors to tackle time-variant and time-invariant components with hierarchically learned operators, and renovate Koopman Autoencoder by removing the reconstruction loss to achieve fully predictive training.

## 3 Background

### Koopman Theory

A discrete-time dynamical system can be formulated as \(x_{t+1}=(x_{t})\), where \(x_{t}\) denotes the system state and \(\) is a vector field describing the dynamics. However, it is challenging to identify the system transition directly on the state because of nonlinearity or noisy data. Instead, Koopman theory  hypothesizes the state can be projected into the space of measurement function \(g\), which can be governed and advanced forward in time by an infinite-dimensional linear operator \(\), such that:

\[ g(x_{t})=g(x_{t})=g(x_{t+1}). \]

Koopman theory provides a bridge between finite-dimensional nonlinear dynamics and infinite-dimensional linear dynamics, where spectral analysis tools can be applied to obtain in-depth analysis.

### Dynamic Mode Decomposition

Dynamic Mode Decomposition (DMD)  seeks the best fitted finite-dimensional matrix \(K\) to approximate infinite-dimensional operator \(\) by collecting the observed system state (a.k.a. _snapshot_). Although DMD is the standard numerical method to analyze dynamics, it only works on linear space assumptions, which can be hardly identified without prior knowledge. Therefore, eDMD  is proposed to avoid handcrafting measurement functions and harmonious incorporations are made with the learning approach by employing autoencoders, which yields Koopman Autoencoder (KAE). By the universal approximation theorem  of deep networks, KAE finds desired _Koopman embedding_\(g(x_{t})\) with learned measurement function in a data-driven approach.

### Time Series as Dynamics

It is challenging to predict real-world time series because of inherent non-stationarity. But if we zoom in the timeline, we will find the localized time series exhibited weak stationarity. It coincides with Koopman theory to analyze large nonlinear dynamics. That is, the measurement function space can be divided into several neighborhoods, which are discriminately portrayed by localized linear operators . Therefore, we leverage Koopman-based approaches that tackle large nonlinear dynamics by disentangling time-variant and time-invariant dynamics. Inspired by Wold's Theorem  that every covariance-stationary time series \(X_{t}\) can be formally decomposed as:

\[X_{t}=_{t}+_{j=0}^{}b_{j}_{t-j}, \]

where \(_{t}\) denotes the deterministic component and \(_{t}\) is the stochastic component as the stationary process input of linear filter \(\{b_{j}\}\), we introduce globally learned and localized linear Koopman operators to exploit respective dynamics underlying different components.

## 4 Koopa

We propose _Koopa_ composed of stackable _Koopa Blocks_ (Figure 2). Each block is obliged to learn the input dynamics and advance it forward for prediction. Instead of struggling to seek one unified operator that governs the whole measurement function space, each Koopa Block is encouraged to learn operators hierarchically by taking the residual of previous block fitted dynamics as its input.

Koopa BlockAs aforementioned, it is essential to disentangle different dynamics and adopt proper operators for non-stationary series forecasting. The proposed block shown in Figure 2 contains _Fourier Filter_ that utilizes frequency domain statistics to disentangle time-variant and time-invariant components and implements two types of _Koopman Predictor (KP)_ to obtain Koopman embedding respectively. In Time-invariant KP, we set the operator as a model parameter to be globally learned from lookback-forecast windows. In Time-variant KP, analytical operator solutions are calculated locally within the lookback window, with series segments arranged as snapshots. In detail, we formulate the \(b\)-th block input \(X^{(b)}\) as \([x_{1},x_{2},,x_{T}]^{}^{T C}\), where \(T\) and \(C\) denote the lookback window length and the variate number. The target is to output a forecast window of length \(H\). Our proposed Fourier Filter conducts disentanglement at the beginning of each block:

\[X^{(b)}_{},\;X^{(b)}_{}=(X^{(b)}). \]

Respective KPs will predict with time-invariant input \(X^{(b)}_{}\) and time-variant input \(X^{(b)}_{}\), and Time-variant KP simultaneously outputs the fitted input \(^{(b)}_{}\):

\[ Y^{(b)}_{}&=(X^{(b)}_{}),\\ ^{(b)}_{},Y^{(b)}_{}&= (X^{(b)}_{}). \]

Figure 2: Left: Koopa structure. By taking the residual of previous block fitted dynamical system, each block learns hierarchical dynamics and Koopa aggregates the forecast of all blocks. Right: For time-invariant forecast, Koopa learns globally shared dynamics from each lookback-forecast window pair. For time-variant forecast, the model calculates localized and segment-wise dynamics.

Unlike KAEs  that introduce a loss term for rigorous reconstruction of the lookback-window series, we feed the residual \(X^{(b+1)}\) as the input of next block for learning a corrective operator. And the model forecast \(Y\) is the sum of predicted components \(Y^{(b)}_{},Y^{(b)}_{}\) gathered from all Koopa Blocks:

\[X^{(b+1)}=X^{(b)}_{}-^{(b)}_{},\;\;Y=Y^{ (b)}_{}+Y^{(b)}_{}. \]

Fourier FilterTo disentangle the series components, we leverage Fourier analysis to find the globally shared and localized frequency spectrums reflected on different periods. Concretely, we precompute the Fast Fourier Transform (FFT) of each lookback window of the training set, calculate the averaged amplitude of each spectrum \(=\{0,1,,[T/2]\}\), and sort them by corresponding amplitude. We take the top percent of \(\) as the subset \(_{}\), which contains dominant spectrums shared among all lookback windows and exhibits time-invariant dynamics underlying the dataset. And the remaining spectrums are the specific ingredient for varying windows in different periods. Therefore, we divide the spectrums \(\) into \(_{}\) and its complementary set \(}_{}\). During training and inference, \(()\) conducts the disentanglement of input \(X\) (block superscript omitted) as

\[X_{} =^{-1}_{ },\;(X), \] \[X_{} =^{-1}}_{},\;(X)=X-X_{},\]

where \(\) means FFT, \(^{-1}\) is its inverse and \(()\) only passes corresponding frequency spectrums with the given set. We validate the disentangling effect of our proposed Fourier Filter in Section 5.2 by calculating the variation degree of temporal dependencies in the disentangled series.

Time-invariant KPTime-invariant KP is designed to portray the globally shared dynamics, which discovers the direct transition from lookback window to forecast window as \(:X_{} Y_{}\). Concretely, we introduce a pair of \(:^{T C}^{D}\) and \(:^{D}^{H C}\) to learn the common Koopman embedding for the time-invariant components of running window pairs, where \(D\) denotes the embedding dimension. Working on the data-driven measurement function, we introduce the operator \(K_{}^{D D}\) as a learnable parameter in each Time-invariant KP, which regards the embedding of lookback and forecast window \(Z_{},Z_{}^{D}\) as running snapshot pairs. The procedure is shown in Figure 3 and \(()\) is formulated as follows:

\[Z_{}=(X_{}),\;Z_{}=K_{ }Z_{},\;Y_{}=(Z_{}). \]

Figure 3: Left: Time-invariant KP learns Koopman embedding and operator with time-invariant components globally from all windows. Right: Time-variant KP conducts localized operator calculation within lookback window and advances dynamics forward with the obtained operator for predictions.

Time-variant KPAs time-variant dynamics changes continuously, we utilize localized snapshots in a window, which constitute a temporal neighborhood more likely to be linearized. To obtain semantic snapshots and reduce iterations, the input \(X_{}\) is divided into \(\) segments \(_{j}\) of length \(S\):

\[_{j}=[x_{(j-1)S+1},,x_{jS}]^{}^{S C},\;j= 1,2,,T/S. \]

We assume \(S\) is divisible by \(T\) and \(H\); otherwise, we pad the input or truncate the output to make it compatible. Time-variant KP aims to portray localized dynamics, which is manifested analytically as the segment-wise transition \(:_{t}_{t+1}\) with observed snapshots. We utilize another pair of \(:^{S C}^{D}\) to transform each segment into Koopman embedding \(z_{j}\) and \(:^{D}^{S C}\) to transform the fitted or predicted embedding \(_{j}\) back to time segments \(}_{j}\):

\[z_{j}=(_{j}),\;}_{j}=(_{j}). \]

Given snapshots collection \(Z=[z_{1},,z_{}]^{D}\), we leverage eDMD  to find the best fitted matrix that advances forward the system. We apply one-step operator approximation as follows:

\[Z_{}=[z_{1},z_{2},,z_{-1}],\;Z_{}=[z_{2},z_{3},,z_{}],\;K_{}=Z_{}Z_{} ^{}, \]

where \(Z_{}^{}^{(-1) D}\) is the Moore-Penrose inverse of lookback window embedding collection. The calculated \(K_{}^{D D}\) varies with windows and helps to analyze local temporal variations as a linear system. With the calculated operator, the fitted embedding is formulated as follows:

\[[_{1},_{2},,_{}]=[z_{1},K_{}z_{ 1},,K_{}z_{-1}]=[z_{1},K_{}Z_{}]. \]

To obtain a prediction of length \(H\), we iterate operator forwarding to get \(\) predicted embedding:

\[_{+t}=(K_{})^{t}z_{},\;\;t=1,2,,H/S. \]

Finally, we arrange the segments transformed by \(()\) as the module outputs \(_{},Y_{}\). The whole procedure is shown in Figure 3 and \(()\) can be formulated as Equation 8-13.

\[_{}=[}_{1},,}_{}]^{},\;Y_{}=[}_{+1},,}_{+}]^{}. \]

Forecasting ObjectiveIn Koopa, Encoder, Decoder and \(K_{}\) are learnable parameters, while \(K_{}\) is calculated on-the-fly. To maintain the Koopman embedding consistency in different blocks, we share \(\), \(\) in Time-variant and Time-invariant KPs, which are formulated as \(_{}\) and \(_{}\) respectively, and use the MSE loss with the ground truth \(Y_{}\) for parameter optimization:

\[_{K_{},_{},_{}}\;_{}(Y,\;Y_{}). \]

Optimizing by a single forecasting objective based on the assumption that if reconstruction failed, the prediction must also fail. Thus eliminating forecast discrepancy helps for fitting observed dynamics.

## 5 Experiments

DatasetsWe conduct extensive experiments to evaluate the performance and efficiency of Koopa. For multivariate forecasting, we include six real-world benchmarks used in Autoformer : ECL (UCI), ETT , Exchange , ILI (CDC), Traffic (PeMS), and Weather (Wetterstation). For univariate forecasting, we evaluate the performance on the well-acknowledged M4 dataset , which contains four subsets of periodically collected univariate marketing data. And we follow the data processing and split ratio used in TimesNet .

Notably, instead of setting a fixed lookback window length, for every forecast window length \(H\), we set the length of lookback window \(T=2H\) as the same with N-BEATS , because historical observations are always available in real-world scenarios and it can be beneficial for deep models to leverage more observed data with the increasing forecast horizon.

BaselinesWe extensively compare Koopa with the state-of-the-art deep forecasting models, including Transformer-based model: Autoformer , PatchTST ; TCN-based model: TimesNet , MICN ; MLP-based model: DLinear ; Fourier forecaster: FiLM , and Koopman forecaster: KNF . We also introduce additional specialized models N-HiTS  and N-BEATS  for univariate forecasting as competitive baselines. All the baselines we reproduced are implemented based on the original paper or official code. We repeat each experiment three times with different random seeds and report the test MSE/MAE. And we provide detailed code implementation and hyperparameters sensitivity in Appendix C.

[MISSING_PAGE_FAIL:7]

Model efficiencyWe comprehensively evaluate the model efficiency from three aspects: forecasting performance, training speed, and memory footprint. In Figure 4, we compare the efficiency under two representative datasets with different variate numbers (7 in ETTh2 and 862 in Traffic).

Compared with the state-of-the art forecasting model PatchTST, Koopa saves **62.3%** and **96.5%** training time respectively in the ETTh2 and Traffic datasets with only **26.8%** and **2.9%** memory footprint. Concretely, the averaged training time and memory ratio of Koopa compared to PatchTST are **22.7%** and **24.0%** in all six datasets (see Appendix D.3 for the detail). Besides, as an efficient MLP-based forecaster, Koopa is also capable of learning nonlinear dynamics from time-variant and time-invariant components, and thus achieves a better performance.

### Model Analysis

Dynamics disentanglementTo validate the disentangling effect of our proposed Fourier Filter, we divide the whole time series into 20 subsets of different periods and conduct respective linear regression on the components disentangled by Fourier Filter. The standard deviation of the linear weighting reflects the variation of point-to-point temporal dependencies, which works as the manifestation of time-variant property. We plot the value as _Degree of Variation_ (Figure 5 Left). It can be observed that larger deviations occur in the time-variant component, which indicates the proposed module successfully disentangles two types of dynamics from the perspective of frequency domain.

Case studyWe present a case study on real-world time series (exchange rate) on the right of Figure 5. We sample the lookback window at the interval of one year and visualize the Koopman operators calculated in Time-variant KP. It can be clearly observed that localized operators can exhibit changing temporal patterns in different periods, indicating the necessity of utilizing varying operators to describe time-variant dynamics. And interpretable insights are also presented as series uptrends correspond to heatmaps with large value and downtrends are reflected with small value.

Figure 4: Model efficiency comparison. The performance comes from Table 1 with forecast window length \(H=144\). Training time and memory footprint are recorded with the same batch size and official code configuration. Full results of all six datasets are provided in Appendix D.3.

Figure 5: Left: Comparison of Degree of Variation (the standard deviation of linear weighting fitted on different periods), we plot respective values of disengaged components on all six datasets. Right: A case of localized Koopman operators calculated on the Exchange dataset at the interval of one year.

Ablation studyWe conduct ablations on Koopan. As shown in Table 3, Time-variant and Time-invariant KPs perform as complementary modules to explore the dynamics underlying time series, and discarding any one of them will lead to the inferior performance. Besides, we evaluate alternative decomposition filters to disentangle time series dynamics. We find the proposed Fourier Filter conducts effective disentanglement, where the amplitude statistics of frequency spectrums from different periods are utilized to exhibit time-agnostic information. Therefore, Koopa tackling the right dynamics with complementary modules can achieves the best performance.

Avoiding rigorous reconstructionUnlike previous Koopman Autoencoders, the proposed Koopman Predictor does not reconstruct the whole dynamics at once, but aims to portray the partial dynamics evolution. Thus we remove the reconstruction branch, which is only utilized during training in previous KAEs. In our deep residual structure, the predictive objective function works as a good optimization indicator. We validate the design in Table 4, where the performance of sorely forecasting objective optimized model is better than with an additional reconstruction loss. Because the end-to-end forecasting objective helps to reduce the optimization gap between training and inference, making it a valuable contribution of applying Koopman operators on end-to-end time series forecasting.

Learning stable operatorsWe turn to analyze our architectural design from the spectral perspective. The eigenvalues of the operator determine the amplitude of dynamics evolution. As most of non-stationary time series experience the distribution shift and can be regarded as an unstable evolution, the learned Koopman operator with the modulus far from the unit circle will cause non-divergent and even explosive trending in the long term, leading to training failures.

To tackle this problem generally faced by Koopman-based forecasters, we propose to utilize the disentanglement and deep residual structure. We measure the stability of the operator as the average distance of eigenvalues from the unit circle. As shown in Figure 6, the operator can become more stable by the above two techniques. The disentanglement helps to describe complex dynamics based on the decomposition and appropriate inductive bias can be applied. The architecture where each block is employed to fill the residual of the previously fitted dynamics reduces the difficulty of directly reconstructing complicated dynamics. Each block portrays the basic process driven by a stable operator within its power, which can be aggregated for a complicated non-stationary process.

### Scaling Up Forecast Horizon

Most deep forecasting models work as a settled function once trained (e.g. input-\(T\)-output-\(H\)). For scenarios where the prediction horizon is mismatched or long-term, it poses two challenges for the trained model: (1) reuse parameters learned from observed series; (2) utilize incoming ground truth for model adaptation. The practical scenarios, which we name as _scaling up forecast horizon_, may

   Dataset &  &  &  &  &  &  \\  Model & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ 
**Koopa** & **0.303** & **0.356** & **0.110** & **0.230** & **0.143** & **0.243** & **0.404** & **0.277** & **0.161** & **0.210** & **1.734** & **0.862** \\ KAE & 0.312 & 0.361 & 0.129 & 0.248 & 0.169 & 0.269 & 0.463 & 0.329 & 0.170 & 0.217 & 2.189 & 0.974 \\  Promotion &  &  &  &  &  &  \\   

Table 4: Performance comparison of the dynamics learning blocks implemented by our proposed Koopman Predictor (_Koopa_) and the canonical Koopman Autoencoder (_KAE_).

   Dataset &  &  &  &  &  &  \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  Only \(K_{}\) & 0.148 & 0.250 & 0.312 & 0.358 & 0.120 & 0.241 & 2.146 & 0.963 & 0.740 & 0.446 & 0.170 & 0.213 \\ Only \(K_{}\) & 1.547 & 0.782 & 0.371 & 0.405 & 0.205 & 0.316 & 2.370 & 1.006 & 0.947 & 0.544 & 0.180 & 0.232 \\ Truncated Filter & 0.155 & 0.255 & 0.311 & 0.362 & 0.129 & 0.246 & 1.988 & 0.907 & 0.536 & 0.334 & 0.172 & 0.220 \\ Branch Switch & 0.696 & 0.393 & 0.344 & 0.385 & 0.231 & 0.325 & 2.130 & 0.964 & 0.451 & 0.304 & 0.173 & 0.221 \\
**Koopa** & **0.146** & **0.246** & **0.303** & **0.356** & **0.111** & **0.230** & **1.734** & **0.862** & **0.419** & **0.293** & **0.162** & **0.211** \\   

Table 3: Model ablation. _Only \(K_{}\)_ uses one-block Time-invariant KP; _Only \(K_{}\)_ stacks Time-variant KPs only; _Truncated Filter_ replaces Fourier Filter with High-Low Pass Filter; _Branch Switch_ changes the order of KPs on disentangled components. The averaged results are listed here.

lead to failure on most deep models but can be naturally tackled by Koopa. In detail, we first train Koopa with forecast length \(H_{}\) and attempt to apply it on a larger forecast length \(H_{}\).

MethodKoopa scales up forecast horizon as follows: Since Time-invariant KP has learned the globally shared dynamics and Time-variant KP can calculate localized operator \(K_{}\) within the lookback window, we freeze the parameters of trained Koopa but only use the incoming ground truth to adapt \(K_{}\). The naive implementation uses incremental Koopman embedding with dimension \(D\) and conducts Equation 10 to obtain an updated operator, which has a complexity of \((H_{}D^{3})\). We further propose an iterative algorithm with improved \(((H_{}+D)D^{2})\) complexity. The detailed method implementations and complexity analysis can be found in Appendix A.

ResultsAs shown in Table 5, the proposed operator adaption mechanism further boosts the performance on the scaling up scenario, which can be attributed to more accurately fitted time-variant dynamics with incoming ground truth snapshots. Besides, the promotion becomes more significant when applied to non-stationary datasets (manifested as large ADF Test Statistic ).

## 6 Conclusion

This paper tackles time series as dynamical systems. With disentangled time-variant and time-invariant components from non-stationary series, the Koopa model reveals the complicated dynamics hierarchically and leverages MLP modules to learn Koopman embedding and operator. Experimentally, our model shows competitive performance with remarkable efficiency and the potential to scale up the forecast length by operator adaptation. In the future, we will explore Koopa with the dynamic modes underlying non-stationary data using the toolbox of Koopman spectral analysis.