# Precision-Recall Divergence Optimization for

Generative Modeling with GANs and

Normalizing Flows

 Alexandre Verine

LAMSADE, CNRS,

Universite Paris-Dauphine-PSL,

Paris, France

alexandre.verine@dauphine.psl.eu&Benjamin Negrevergne

LAMSADE, CNRS,

Universite Paris-Dauphine-PSL,

Paris, France

benjamin.negrevergne@dauphine.psl.eu&Muni Sreenivas Pydi

LAMSADE, CNRS,

Universite Paris-Dauphine-PSL,

Paris, France

muni.pydi@dauphine.psl.eu&Yann Chevaleyre

LAMSADE, CNRS,

Universite Paris-Dauphine-PSL,

Paris, France

yann.chevaleyre@dauphine.psl.eu

###### Abstract

Achieving a balance between image quality (precision) and diversity (recall) is a significant challenge in the domain of generative models. Current state-of-the-art models primarily rely on optimizing heuristics, such as the Frechet Inception Distance. While recent developments have introduced principled methods for evaluating precision and recall, they have yet to be successfully integrated into the training of generative models. Our main contribution is a novel training method for generative models, such as Generative Adversarial Networks and Normalizing Flows, which explicitly optimizes a user-defined trade-off between precision and recall. More precisely, we show that achieving a specified precision-recall trade-off corresponds to minimizing a unique \(f\)-divergence from a family we call the _PR-divergences_. Conversely, any \(f\)-divergence can be written as a linear combination of PR-divergences and corresponds to a weighted precision-recall trade-off. Through comprehensive evaluations, we show that our approach improves the performance of existing state-of-the-art models like BigGAN in terms of either precision or recall when tested on datasets such as ImageNet.

## 1 Introduction

Evaluation of generative models has always been a challenging task. The metric used must reflect both the quality of the sample generated (precision) and how much the sample covers the targeted probability distribution (recall). Inception Score (IS) or Frechet Inception Distance (FID) have been introduced and are now widely used by the community to select the best models. Typically, these metrics are favored because they _"correlate well with the visual fidelity of the samples"_ and are _"sensitive to both the addition of spurious modes as well as mode dropping"_. However as pointed by Kynkaanniemi et al. , _FID and IS group these two aspects into a single value without a clear trade-off_. Depending on the use-case, generative models might require a good precision (high-resolution image and video generation, artistic synthesis, 3D model design) or a good recall (data augmentation, drug discovery, anomaly detection). For that reason, a number of more principled methods [9; 15; 28; 44; 47] have emerged to assess precision and recall (hereafter abbreviated byP&R) independently, however these methods cannot be optimized during training, because they are not differentiable or because they are too computationally demanding.

To enhance either the precision or the recall of a particular model, an array of strategies and techniques have been employed. Usually these techniques involve altering the latent distribution _a posteriori_ (e.g. truncation and temperature post-training). Other methods exists such as through rejection sampling, boosting, or instance selection [3; 12; 13; 19; 23; 33; 49]. Although these approaches may utilize proxies of P&R, they are not theoretically grounded in the principled methods of evaluating these metrics, leaving their alignment with the foundational concepts of P&R unverified. This leads to the following question.

**Question 1:**_Is it possible to train a generative model that achieves a specified trade-off between precision and recall?_

In addition to this question, another of our goals is to understand existing generative models in terms of P&R. Modern generative models, such as Generative Adversarial Networks (GANs) or Normalizing Flows (NFs), are typically designed to minimize specific divergence measures. Given a target distribution \(P\) and a set of parameterized distributions \(\{P_{}|\}\), a generative model aims to find the best fit \(=P_{^{*}}\) that minimizes a divergence between \(P\) and \(\). These divergences induce different behaviors at convergence. For instance, optimizing the Kullback-Leibler (KL) divergence tends to favor mass-covering models , contrasting with the mode-seeking behavior observed with other generative models, leading to the infamous problem of mode collapse. Transitioning from one divergence to another does indeed alter the model's behavior in terms of precision and recall, however, the implicit trade-offs made during the optimization of a general divergence remain somewhat ambiguous. This motivates the following question.

**Question 2:**_What precision-recall trade-off does an arbitrary \(f\)-divergence minimize?_

In this paper, we bridge the gap between principled methods of P&R evaluation and controlling their trade-off. In doing so, we address Questions 1 and 2 by making the following contributions:

* We show that achieving a specified precision-recall trade-off corresponds to minimizing a particular \(f\)-divergence between \(P\) and \(\). Specifically, in Theorem 4.3 we give a family of \(f\)-divergences (denoted by \(_{}\), \([0,]\)) that are associated with various points along the precision-recall curve of the generative model.
* We show that any arbitrary \(f\)-divergence can be written as a linear combination of \(f\)-divergences from the \(_{}\) family. This result makes explicit, the implicit precision-recall trade-offs made by generative models that minimize an arbitrary \(f\)-divergence.
* We propose a novel approach to train or fine-tune a generative model on notoriously hard to train \(f\)-divergences, with guarantees on divergences defined by the lipschitz constant of \(f\).
* We use this approach to train models on a user specified trade-off between P&R by minimizing \(_{}\) for any given \(\). We specifically focus on GANs and NFs. For instance, Figure 1 shows how our model performs under various settings of \(\). With a high \(\), we can train the model to favor precision over recall and vice-versa.
* Through extensive experiments, we show that our approach enables effective model training to minimize the PR-Divergence, particularly for fine-tuning pre-trained models, with a notable impact from the choice of trade-off parameters \(\), while also demonstrating scalability with larger dimensions and datasets.

## 2 Related works

Generative model evaluation metrics:IS or FID are widely adopted due to their ability to assess visual fidelity and sensitivity to mode variations. However, these metrics fall short in providing a trade-off between P&R. Principled approaches were introduced by Djolonga et al.  and Sajjadi et al. , and later extended by Simon et al. , providing a definition P&R independently using PR-Curves detailed in Section 3.3. We adopt this extended approach in our work. In image generative modeling, the current consensus for P&R evaluation are two methods. First, the method presented by Kynkaanniemi et al. , which provides a simpler evaluation of P&R based on the estimation of the support of the distribution using a k-NN algorithm, akin to the most recent work by Cheema and Urner . Second, a method introduced by , computing the Density (D) and the Coverage (C) which account for local density and are robust to outliers in contrast to . In natural language processing, one popular method, MAUVE , consists of the area under PR-Curves defined by . However, because of their mathematical foundation, these methods remain unsuitable for training due to non-differentiability and high computational requirements.

Trade-offs and techniques in generative model:The challenge of balancing P&R in generative model training has led to a variety of techniques. Methods for enhancing precision often involve manipulating the latent distribution post-training, such as hard truncation [7; 24; 25; 46] or standard deviation adjustment, also called soft truncation . One can also use rejection sampling in the image space [4; 50], in the latent space [23; 49], or in the dataset . There are also numerous strategies to prevent mode collapse and boost recall [8; 31; 32], gradient-boosting methods [12; 16; 19] and mixture based latent models [6; 40]. However, it should be noted that these approaches lack a foundation in the principled methods of P&R evaluation. This underscores the need for a theoretically grounded approach to balance these crucial aspects in generative models, a gap that our current work aims to address. Note also that all these methods can be applied to any model, and in particular to the one trained using our proposed method.

Divergence training in generative models:Modern generative models, such as GANs and NFs, often employ specific divergence measures for model optimization. With works such as Grover et al. , Nowozin et al. , models can be trained with a variety of divergences such as \(f\)-divergence, thus observing different results of P&R. Another notable work by Midgley et al.  proposed training with \(\)-Divergence, under the assumption of access to data density. The work of , closely aligned with our approach, defines different orders of mode seeking and evaluates the corresponding \(f\)-divergences at these levels. Although its methodology employs the training of \(f\)-GAN models on simple datasets to illustrate the mode-seeking property, it does not offer the flexibility to establish a user-defined trade-off. The implicit trade-offs made by these divergence measures is still a challenge: the recent work of  shows the links between P&R and any DeGroot's divergences.

## 3 Background

Notation:Throughout the paper, we use \(^{d}\) to refer to the input space and \(^{m}\) to the latent space of the model we consider. We also denote \(()\) and \(()\) the set of all probability measures over measurable subsets of \(\) and \(\) respectively. \(P\) and \(\) are consistently used to denote the target and estimated distributions (both members of \(()\)). We also assume that \(P\) and \(\) share the same support in \(\), and that they admit densities denoted by the corresponding lower case letters \(p\) and \(\), respectively. Finally, for any function \(f\), we define the convex conjugate (or Fenchel transform) of \(f\) given by \(f^{*}(t)=_{u}\{tu-f(u)\}\).

Figure 1: NFs - RealNVP  trained on 2D Gaussians. Fig. 1(a) to Fig. 1(c): models trained to minimize \(_{ ru}\). Fig. 1(d) and Fig. 1(e): models trained to minimize \(_{}\) and \(_{}\) respectively. Samples drawn from the true distribution \(P\) are shown in black, samples drawn from the estimated distribution \(\) are shown in green and the log-likelihood of \(\) is shown in blue (darker means higher density). (a) \(=0.1\), favors recall over precision. (b) \(=1\), balanced precision vs. recall trade-off. (c) \(=10\), which favors precision over recall. As observed in , and demonstrated in Theorem 4.5, the \(_{}\) (1(d)) is mass covering and the \(_{}\) (1(e)) in mode-seeking. The corresponding PR-Curve are presented in Figure 2(c).

### \(f\)-divergences

Given a convex lower semi-continuous (l.s.c) function \(f:^{+}\) satisfying \(f(1)=0\), the \(f\)-divergence between two probability distributions \(P\) and \(\) is defined as follows.

\[_{f}(P\|)=_{}()f( )}{()}).\] (1)

\(_{f}\) is invariant to an affine transformation in \(f\) i.e., \(_{f}(P\|)=D_{f^{}}(P\|)\) for \(f^{}(u)=f(u)+c(u-1)\) for any constant \(c\). Many well-known statistical divergences such as the KL divergence (\(_{}\)), the reverse KL (\(_{}\)) or the Total Variation (\(_{}\)) are \(f\)-divergences (see Table 1). Importantly, any \(_{f}\) admits a dual variational form , with \(\) denoting the set of all measurable functions \(\):

\[_{f}(P\|)=_{T}_{f,T}^{ }(P\|),_{f,T}^{ }(P\|)=_{ P}[T() ]-_{}[f^{*}(T())]\] (2)

We use \(T^{}\) to denote the function that achieves the supremum in (15).

### Generative models

Generative Adversarial Networks (GANs):A GAN consists of two functions, generator \(G:\), discriminator \(T:\), as well as a prior distribution \(Q()\) which is usually the standard normal \((0,_{m})\). The estimated data distribution \(_{G}\) is the push-forward distribution of \(Q\) by \(G\). In the original work of Goodfellow et al. , a specific \(_{f}\) is used to optimize the divergence between \(P\) and \(_{G}\). In this paper, we consider the more general framework of Nowozin et al.  that can be used to train a GAN with any \(_{f}\) by solving the following min-max optimization problem:

\[_{G}_{T}_{f,T}^{}(P\|_{G})\ =_{G}_{T}_{ P}[T()]- _{_{G}}[f^{*}(T())],\] (3)

Normalizing Flows (NFs):An NF consists of an invertible function \(G:\) and a prior distribution \(Q()\). As for GANs, the estimated distribution \(_{G}\) is the push-forward of \(Q\) by \(G\). However, because \(G\) is invertible, it is possible to compute the density \(()\) using a simple change of variable formula, \(()=q(G^{-1}())|_{G^{-1}}()|\), where \(_{G^{-1}}()\) is the determinant of the Jacobian matrix of \(G^{-1}\) at \(\), and train the generator using the \(_{}\) between \(P\) and \(_{F}\), or equivalently, by maximizing the log-likelihood:

\[_{G}_{}(P\|_{G})=H(P)-_{G}_{ P}[_{G}()],\] (4)

where \(H(P)\) is the continuous entropy of \(P\). In practice, the generator function \(G\) is typically represented by neural networks such as GLOW , RealNVP , or ResFlow [5; 10] for which \(_{G^{-1}}()\) is easy to compute. Grover et al.  showed that it is possible to train an NF using most \(f\)-divergences. In practice, the log-likelihood is added to the min-max objective to stabilize learning. This gives us the following optimization problem:

\[_{G}_{T}_{f,T}^{}(P\|_{G})- _{ P}[_{G}()],\] (5)

   Divergence & Notation & \(f(u)\) & \(f^{*}(t)\) & \(T^{}()\) & \(f^{}(1/)/^{3}\) \\  KL & \(_{}\) & \(u u\) & \((t-1)\) & \(1+ p()/()\) & \(1/^{2}\) \\ Reverse KL & \(_{}\) & \(- u\) & \(-1--t\) & \(-()p()\) & \(1/\) \\ \(^{2}\)-Pearson & \(_{^{2}}\) & \((u-1)^{2}\) & \(t^{2}/4+t\) & \(2(p()()-1)\) & \(2/^{3}\) \\   

Table 1: List of common \(f\)-divergences. The generator \(f\) is given with its Fenchel conjugate \(f^{*}\). The optimal discriminator \(T^{}\) is given to compute the likelihood ratio \(p()/()= f^{*}(T^{}())\). Then \(f^{}(1/)/^{3}\) is given to compute the \(_{f}\) as a combination of \(_{}\) using Theorem 4.4.

### Precision-Recall curve for generative models

Generative models are usually evaluated using a single criterion such as the IS  or the FID . However, these criteria are unable to distinguish between the distinct failure modes of low precision (i.e., failure to produce quality samples) and low recall (i.e., failure to cover all modes of \(P\)). The following definition was introduced by Sajjadi et al.  and later extended by Simon et al. .

**Definition 3.1** (PRD set, adapted from Simon et al. ).: _For \(P,()\), the P&R set \((P,)\) is defined as the set of pairs of precision \(\) and recall \(\) in \(^{+}\ \ ^{+}\) such that there exist \(()\) for which \(P\) and \(\). The precision-recall curve (or PR curve) is defined as \((P,)=\{(,)(P,)(^{},^{})^{}^{}\}\)._

An equivalent definition of \((P,)\) is found in Sajjadi et al. , where \((,)(P,)\) if \(P\) and \(\) can be decomposed as in (6) for some common component \(()\) and complementary components \(_{P},_{}()\).

\[P=+(1-)_{P}=+(1- )_{}.\] (6)

Simon et al.  show that the PR curve is parameterized by \([0,+]\) as \((P,)=_{}(P\| ),_{}(P\|)[0,+]}\), with \(_{}(P\|)=_{}), ())\,}\) and \(_{}(P\|\ )=_{}(P\|\ )/\). Here, \(\) is called the _trade-off parameter_ and can be used to adjust the sensitivity to precision or recall.

An illustration of the PR curve is given in Figure 2 for a target distribution \(P\) that is a mixture of two Gaussians and two candidate models \(_{1}\) and \(_{2}\). We can see on Figure 2 that \(_{1}\) offers better results for large values of \(\) (with high sensitivity to precision) whereas \(_{2}\) offers better results for low values of \(\) (with high sensitivity to recall).

## 4 Precision and Recall trade-off as an \(f\)-divergence

In this section, we formalize the link between P&R trade-off and \(f\)-divergences, and address Question 2. We will exploit this link in Section 5 to train models that optimize a particular P&R trade-off.

### Precision-Recall as an \(f\)-divergence

We start by introducing the _PR-Divergence_ as follows.

Figure 2: PR curves for two models \(_{1}\) and \(_{2}\) of \(P\). Figure 2(a) shows \(_{1},_{2}\) and \(P\). Figure 2(b) shows PR curves for \(_{1},_{2}\) against \(P\). \(_{1}\) has good recall since it covers both modes of \(P\), but low precision since it generates points between the modes. \(_{2}\) has good precision since it does not generate samples outside of \(P\) but low recall since it can generate samples from only one mode. Figure 2(c) are the PR curves corresponding to Figure 1 and are detailed in Section 6.

**Definition 4.1** (PR-divergence).: _Given a trade-off parameter \([0,+]\), the PR-divergence (denoted by \(_{ r}\)) is defined as \(D_{f_{}}\) for \(f_{}:^{+}\) given by \(f_{}(u)=( u,1)-(,1)\) for \(^{+}\) and \(f_{}(u)=0\) for \(=+\)._

Note that \(f_{}\) is continuous, convex, and satisfies \(f_{}(1)=0\) for all \(\). A graphical representation of \(f_{}\) can be found in Appendix B.2. The following proposition gives some properties of \(_{ r}\).

**Proposition 4.2** (Properties of the PR-Divergence).:
* _The Fenchel conjugate_ \(f_{}^{*}\) _of_ \(f_{}\) _is defined on_ \((f_{}^{*})=[0,]\) _and given by,_ \(f_{}^{*}(t)=t/\) _for_ \( 1\) _and_ \(f_{}^{*}(t)=t/+-1\) _otherwise._
* _The optimal discriminator for the dual form is_ \(T^{}()=\{p()/( )-1\}\)_._
* \(_{ r}(\|P)=_{  r}(P\|)\)_._

Observe that \(_{ r}(\|P)_{  r}(P\|)\) in general, but for \(=1\), \(_{1 r}(P\|)=_{1 r }(\|P)=_{}(P\|)/2\). Having defined the PR-divergence, we can now show that P&R w.r.t \(\) can be expressed as a function of the divergence between \(P\) and \(\).

**Theorem 4.3** (P&R as a function of \(_{ r}\)).: _Given \(P,()\) and \([0,+]\), the PR curve \((P,)\) is related to the PR-divergence \(_{ r}(P\|)\) as follows._

\[_{}(P\|\;)=(1,)-_{ r }(P\|).\] (7)

_Conversely, suppose that there exists a strictly decreasing linear function \(h:^{+}\) and an \(f\)-divergence \(_{f}\) such that \(h(_{}(P\|\;))=_{f}(P\|)\) for all \(P,()\), then \(f(u)=c_{1}f_{}(u)+c_{2}(u-1)\)._

A direct consequence of Theorem 4.3 is that minimizing \(_{ r}\) is equivalent to maximizing \(_{}\). In a more explicit way, Theorem 4.3 suggests that \(_{ r}\) is the _only_\(f\)-divergence (up to an affine transformation) for which this property holds. This makes \(_{ r}\) a uniquely suitable candidate for training a generative model with a specific P&R trade-off. The proof of Theorem 4.3 is in Appendix B.1

### Relation between PR-divergences and other \(f\)-divergences

In the previous subsection, we showed that for each trade-off parameter \(\), there exists a \(_{ r}\) that corresponds to optimizing for it. This raises the converse question of what trade-off is achieved by optimizing for an arbitrary \(f\)-divergence. We answer this by showing in the following theorem that any \(f\)-divergence can be expressed as a weighted sum of PR-divergences.

**Theorem 4.4** (\(f\)-divergence as weighted sums of PR-divergences).: 1 _For any \(P,()\) supported on all of \(\) and any \(^{+}\), with \(m=_{}(()/p())\) and \(M=_{}(()/p())\):_

\[_{f}(P\|)=_{m}^{M}}f^{ }()_{ r}(P\| ),\] (8)

As a sanity check, observe that the weights \(f^{}(1/)/^{3}\) remain invariant under an affine transformation in \(f\) much like \(D_{f}\).

**Corollary 4.5** (\(_{}\) and \(_{}\) as an average of \(_{ r}\)).: _The \(_{}\) Divergence and the \(_{}\) can be written as a weighted average of PR-Divergence \(_{ r}\) :_

\[_{}(P\|)=_{m}^{M}} _{ r}(P\|), _{}(P\|)=_{m}^{M} {}_{ r}(P\|).\] (9)

As we can see in this Corollary, both \(_{}\) and \(_{}\) can be decomposed into a sum of PR-divergences terms \(_{ r}\), each weighted with \(1/^{2}\) and \(1/\) respectively. Note that if \(<1\) then \(1/>1/^{2}\), and conversely, if \(>1\). \(_{}\) is thus associating more weights on recall than \(_{}\). This explains the _mass covering_ behavior observed in NFs trained with \(_{}\). Comparatively, the \(_{}\) assigns more weight to terms with a large lambda, leading to the _mode seeking_ behavior empirically observed with flows trained with the \(_{}\). However, as it will be observed in the Section 6, \(_{}\) is still favoring low values of \(\). Other weights for other \(f\)-divergences are in Appendix A.1.

Minimizing the Precision-Recall divergence

In this section, we address Question 1 by introducing a new method that can be used to optimize a model with a specific precision-recall trade-off \(\). A first naive strategy to achieve this is to use the \(f\)-GAN framework introduced by Nowozin et al. , and minimize the dual variational form of \(_{}\) presented in Theorem 4.3. Together, this results in solving the min-max problem:

\[_{G}_{T}_{f_{,T}}^{}(P\|)= _{G}_{T}_{ P}[T()]-_{ }[f_{}^{*}(T())],\] (10)

where \(G\) and \(T\) are both represented using neural networks. In practice, this strategy would fail because training a neural network with loss functions such as \(f_{}^{*}\) (or in this case \(f_{}^{*}\)) is notoriously difficult due to vanishing gradients2 that lead to poor training performance [20; 39]. Instead, training neural networks on functions \(f^{*}\) such as \(f_{}^{*}\) or \(f_{^{2}}^{*}\), results in much better empirical performance [29; 39; 51].

To avoid these issues, we show how to train the discriminator using an auxiliary divergence (based on a function \(g f\)) to better estimate the target \(f\)-divergence. The main idea is to choose an auxiliary \(g\)-divergence that is adequate for training \(T\) (e.g. \(_{}\) or \(_{^{2}}\)) and use it to compute the _likelihood ratio_\(p()/()\).

Because, at optimality, we have \( g^{*}(T^{}())=p()/()\), we can then compute the \(f\)-divergence as follows:

\[_{f}(P\|)=()f() }{()})=()f( g ^{*}(T^{}())).\] (11)

In practice, we do not have access to \(T^{}\). Instead, we have a discriminator \(T\) trained to maximize \(_{g,T}^{}\). At any time during training, we can estimate the \(f\)-divergence based on \(T\) using the _primal estimate_, which we define as follows.

**Definition 5.1** (Primal estimate \(_{f,T}^{}\)).: _Let \(P,()\). For any function \(T:\), \(f:^{+}\), and \(g:^{+}\), we define the primal estimate \(_{f,T}^{}\) as follows._

\[_{f,T}^{}(P\|)=_{} {p}()f(r()),\] (12)

_where \(r:^{+}\) is given by, \(r()= g^{*}(T())\)._

The success of this approach depends on how well \(r\) approximates \(p(x)/(x)\), which depends on \(T\). We first show that the approximation error of \(r\) measured in terms of the Bregman divergence  (also defined in Appendix A.2). It corresponds _exactly_ to the approximation error of \(_{g,T}^{}\), so minimizing the latter will also minimize the former.

**Theorem 5.2** (Error of the estimation of an \(f\)-divergence under the dual form.).: _For any discriminator \(T:\) and \(r()= f^{*}(T())\),_

\[_{g}(P\|)-_{g,T}^{}(P\| {P})=_{}[_{g}(r(),)}{()})].\] (13)

On the basis of this result, the quality of the approximation will crucially depend on \( g\). We can show that if the auxiliary \(g\) is strongly convex, the error in the estimation of \(_{f,T}^{}\) is bounded:

**Theorem 5.3** (Bound on the estimation of an \(f\)-divergence using an auxiliary \(g\)-divergence).: _Let \(f,g:^{+}\) be such that \(g\) is \(\)-strongly convex and \(f\) is \(\)-Lipschitz. For the discriminator \(T:\), let \(r()= g^{*}(T())\). Then_

\[_{g}(P\|)-_{g,T}^{} |_{f}(P\|)-_{f,T}^{}(P\|)|}.\]If \(T\) successfully maximizes \(_{g,T}^{}\), then the primal estimation converges to \(_{f}\). To implement this approach, we propose the following simplified version of the algorithm: Repeat until convergence these 3 steps:

1. Let \(x_{1}^{} x_{N}^{} P\) and \(x_{1}^{} x_{N}^{}_{G}\).
2. Update the parameters of \(T\) by ascending the gradient \[_{T}=\{_{i=1}^{N}T(x_{i}^{ })-_{i=1}^{N}g^{*}(T(x_{i}^{})) \}.\] 3. Update the parameters of \(G\) by descending the gradient \[_{G}=\{_{i=1}^{N}f( g^{ *}(T(x_{i}^{})))\}.\]

This method closely parallels the GAN training procedure, with the key distinction that T and G optimize objectives based on different \(f\). In practice, our implementation uses a stochastic gradient descent, fully detailed in Algorithm 1 in Appendix C.

## 6 Experiments

In this section, we employ the auxiliary loss approach outlined in Section 5 to train various models.

Specifically, we train NFs on 2D synthetic data, MNIST and FashionMNIST, while we train BigGAN on CIFAR-10, CelebA64, ImageNet128, and FFHQ256. All models are tested in terms of IS and FID with 50k samples, and on P&R with 10k samples using the method of Kynkaanniemi et al.  with \(k=3\) for MNIST and FashionMNIST and with \(k=5\) for CelebA64, ImageNet128 and FFHQ256. Also, we test every model in terms of Density and Coverage  on 10k samples with \(k=5\). In this paper, we present a selection of experimental results. For a comprehensive set of results, including model parameters, optimizers, learning rates, and samples, please refer to Appendix D. We also included in this Appendix a set of experiments run using the naive approach based on Equation 10, thus showing that the discriminator fails to train as explained in Section 5. To ensure reproducibility, our models and code are available on the GitHub repository of the project3.

We show that: 1) the auxiliary loss approach effectively enables the training of a model to minimize the PR-Divergence, 2) this method is suitable for fine-tuning pre-trained models, 3) the choice of trade-off parameter \(\) significantly influences the results on P&R, and finally, 4) our method scales well with larger dimensions and datasets.

Normalizing Flows on synthetic data:NFs are typically trained to minimize \(_{}\), in addition to their structural limitation , resulting in good recall but poor precision. Prior work has employed various techniques  to improve model precision post-training, we use our method to directly train the model on a given trade-off. We demonstrate our approach by training RealNVP models on a 2D synthetic dataset using \(_{}\) with various \(\) values and using \(_{}\) and \(_{}\) for baseline comparison. As we can see in Figure 1, increasing \(\) leads to an increase in precision in the resulting models. Using our \(_{}\) estimation, we compute the corresponding PR curves (Figure 2(c)). The \(=0.1\) model, while best at \(_{0.1}\), performs poorly at \(_{1}\) and \(_{10}\), clearly demonstrating the impact of maximizing \(_{}\).This pattern across models validates the efficacy of our method in minimizing the desired trade-off.

Using \(_{}\) vs \(_{^{2}}\) on MNIST and FashionMNIST:We now demonstrate that we can improve the precision by directly minimizing the \(_{}\) with the correct \(\) using pre-trained GLOW models  on both MNIST  and FashionMNIST . Figure 3 and Figure 4 present the samples obtained. First, we observe that while \(\) increases, the visual quality improves, but the models focus on a few modes only (\(0\), \(1\), \(7\), \(6\), \(9\) for MNIST and "Trouser" for FashionMNIST). Then training with both \(_{}\) and \(_{}\) divergences aligns with our expectations: \(_{}\) training leads to high recall, while \(_{}\), to higher precision. However, according to Corollary 4.5, \(_{}\) still favors low \(\) values; consequently, our models trained with \(>0.1\) demonstrate better precision than standard flow-GAN models. Furthermore, we find that both auxiliary \(g\) functions (\(f_{}\) and \(f_{^{2}}\)) used to train the discriminator \(T\) perform well. In practice, training with \(f_{^{2}}\) proves to be more stable, particularly for FashionMNIST, with results reported in Appendix D.3. For larger models, we use exclusively \(g=f_{^{2}}\).

Training BigGAN on CIFAR-10 and CelebA64Now we demonstrate that our method can also be used to train large generative models. Our choice to adopt the BigGAN architecture  was informed by several factors: its competitive performance close to state-of-the-art models; its versatility, permitting diverse experimental explorations; and the fact that it is publicly accessible, ensuring experiment reproducibility. We train BigGAN using both the baseline method (i.e., hinge loss) and our proposed method on CIFAR-10  and CelebA64 . A notable observation when training with different precision-recall trade-offs is the early elimination of modes from the target distribution at higher values of \(\). As illustrated in Figure 5, models with low values of \(\) converge to achieve maximum recall, while those with \(>1\) rapidly saturate to a lower recall value. A similar behavior can be observed for models trained on CelebA, as shown in Figure D.18. In Table 2, we present the quantitative metrics (Precision, Recall, and FID) for the baseline BigGAN, the BigGAN models trained with varying trade-offs and the current state-of-the-art models: EDM-G++  for CIFAR-10 and ADM-IP  for CelebA64. Employing our proposed method enables us to adjust the trade-off, allowing us to train models that closely approach the state-of-the-art recall and, for high \(\), even outperform state-of-the-art models in terms of precision.

   Model &  &  \\  & FID & P & R & D & C & FID & P & R & D & C \\  Baseline BigGAN & \(13.37\) & \(86.51\) & \(65.66\) & \(0.76\) & \(0.81\) & \(9.16\) & \(78.41\) & \(\) & \(0.89\) & \(0.48\) \\  Hard \(=2.0\) & \(13.95\) & \(86.82\) & \(63.58\) & \(0.77\) & \(0.79\) & \(10.60\) & \(80.81\) & \(48.21\) & \(0.96\) & \(0.50\) \\ Hard \(=1.0\) & \(17.23\) & \(88.03\) & \(53.63\) & \(0.83\) & \(0.75\) & \(17.97\) & \(\) & \(37.46\) & \(1.11\) & \(0.49\) \\ Hard \(=0.5\) & \(20.11\) & \(87.87\) & \(44.98\) & \(0.83\) & \(0.70\) & \(25.70\) & \(83.70\) & \(28.81\) & \(1.08\) & \(0.42\) \\  \(=0.05\) & \(13.29\) & \(81.10\) & \(70.63\) & \(0.61\) & \(0.80\) & - & - & - & - \\ \(=0.1\) & \(\) & \(81.78\) & \(\) & \(0.66\) & \(\) & - & - & - & - & - \\ \(=0.2\) & \(13.36\) & \(84.85\) & \(65.13\) & \(0.74\) & \(0.82\) & \(8.79\) & \(83.37\) & \(44.07\) & \(1.09\) & \(\) \\ \(=0.5\) & \(14.50\) & \(83.27\) & \(68.23\) & \(0.70\) & \(0.81\) & \(\) & \(77.60\) & \(55.98\) & \(0.88\) & \(0.50\) \\ \(=1.0\) & \(14.03\) & \(83.04\) & \(69.35\) & \(0.68\) & \(0.79\) & \(13.07\) & \(81.70\) & \(36.85\) & \(1.00\) & \(0.47\) \\ \(=2.0\) & \(16.94\) & \(84.93\) & \(59.79\) & \(0.75\) & \(0.78\) & \(14.23\) & \(82.98\) & \(32.87\) & \(1.16\) & \(0.49\) \\ \(=5.0\) & \(32.54\) & \(83.39\) & \(56.94\) & \(0.68\) & \(0.73\) & \(22.45\) & \(83.96\) & \(25.81\) & \(\) & \(0.43\) \\ \(=10.0\) & \(39.69\) & \(84.11\) & \(39.29\) & \(0.75\) & \(0.67\) & - & - & - & - & - \\ \(=20.0\) & \(67.03\) & \(\) & \(21.81\) & \(\) & \(0.56\) & - & - & - & - & - \\  DenseFlow  & \(-\) & \(88.90\) & \(60.81\) & \(0.86\) & \(0.71\) & - & \(85.83\) & \(38.22\) & \(1.17\) & \(0.82\) \\ ADM-IP  & \(3.25\) & \(80.67\) & \(83.65\) & \(0.65\) & \(0.87\) & \(1.53^{*}\) & \(23.42\) & \(64.48\) & \(0.09\) & \(0.24\) \\ EDM G++  & \(1.77\) & \(78.48\) & \(85.83\) & \(0.60\) & \(0.87\) & - & - & - & - & - \\ StyleGAN-XL  & \(1.85\) & \(85.11\) & \(70.04\) & \(0.75\) & \(0.85\) & - & - & - & - & - \\   

Table 2: BigGAN trained with the vanilla approach  and with a variety of \(\) using our approach on CIFAR-10 and CelebA64. We compare our approach with hard truncation on the baseline model. FID (\(\)), Precision (\(\)), Recall (\(\)), Density (\(\)) and Coverage (\(\)) are reported. In **bold**, our best model is highlighted and the state-of-the-art FID is marked with an exponent \({}^{*}\).

In every experiment, we compare our approach with traditional post-training techniques. In Table 2, we give the results for the hard truncation (also called _the truncation trick_ in ) of the latent distribution \(Q\) for different \(\). We observe that this method enables to improve solely the precision by trading off the recall; however, note that the truncation can be use in addition to our approach.

Fine-tuning BigGAN on Imagenet128 and FFHQFinally, we apply our method to pre-trained BigGAN models. To accomplish this, we implement a straightforward technique: initially, we train the discriminator for a brief period, allowing the model to transition from the vanilla training objective to the \(_{g}^{}\). This approach enables us to train BigGAN on large datasets such as ImageNet128  and datasets with high dimensions such as FFHQ256 . We compare our method with both hard truncation and soft truncation (also denoted temperature in ). Both methods can be used in addition to our approach. The metrics presented in Table 3 demonstrate that (1) we enhance a given model's precision (by \(+2.83\%\)) or recall (by \(+1.17\%\)) on ImageNet, thereby achieving state-of-the-art precision, and (2) our method compromises less on the trade-off than truncation. For instance, in FFHQ, for a similar precision improvement (\(+15.5\%\)), recall is decreased by more than \(5\%\) for truncation methods and only by \(1.65\%\) with our approach.

## 7 Conclusion

In this paper, we present a novel method for training generative models using a new PR-divergence, \(_{}\). Our approach offers a unique advantage over existing methods as it allows for explicit control of the precision-recall trade-off in generative models. By varying the trade-off parameter \(\), one can train a variety of models ranging from mode seeking (high precision) to mode covering (high recall), as well as more balanced models that may be more suitable for various applications. Through extensive experiments, we demonstrate the validity of our method and show that it scales well with larger dimensions and datasets. Our approach also provides insights into the implicit P&R trade-offs made by models trained with other \(f\)-divergences. By introducing the \(_{}\) divergence and providing a systematic approach for training generative models based on user-specified trade-offs, we contribute to the development of more customizable generative models. Our method currently applies to GANs and NFs only, it is still unclear if a similar approach can be applied to trained diffusion models: a promising work  uses a discriminator to refine diffusion models, and could be used to estimate the \(_{}\).