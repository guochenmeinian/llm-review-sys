# Selective Sampling and Imitation Learning

via Online Regression

 Ayush Sekhari\({}^{1}\)

Authors are listed in alphabetical order of their last names.

\({}^{1}\)MIT, \({}^{2}\)Cornell University

&Karthik Sridharan\({}^{2}\)

\({}^{1}\)MIT, \({}^{2}\)Cornell University

&Wen Sun\({}^{2}\)

\({}^{1}\)MIT, \({}^{2}\)Cornell University

&Runzhe Wu\({}^{2}\)

\({}^{1}\)MIT, \({}^{2}\)Cornell University

###### Abstract

We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback. While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications. In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful. In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback. Our contributions are twofold: First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries. Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries. Our algorithm for selective sampling leverages function approximation, and relies on an online regression oracle w.r.t. the given model class to predict actions, and to decide whether to query the expert for its label. On the theoretical side, the regret bound of our algorithm is upper bounded by the regret of the online regression oracle, while the query complexity additionally depends on the eluder dimension of the model class. We complement this with a lower bound that demonstrates that our results are tight. We extend our selective sampling algorithm for IL with general function approximation and provide bounds on both the regret and the number of queries made to the noisy expert. A key novelty here is that our regret and query complexity bounds only depend on the number of times the optimal policy (and not the noisy expert, or the learner) go to states that have a small margin.

## 1 Introduction

From the classic supervised learning setting to the more complex problems like interactive Imitation Learning (IL) (Ross et al., 2011), high-quality labels or supervision is often expensive and hard to obtain. Thus, one wishes to develop algorithms that do not require a label for every data sample presented during the learning process. Active learning or selective sampling is a learning paradigm that is designed to reduce query complexity by only querying for labels at selected data points, and has been extensively studied in both theory and practice (Agarwal, 2013; Dekel et al., 2012; Hanneke and Yang, 2021; Zhu and Nowak, 2022; Cesa-Bianchi et al., 2005; Hanneke and Yang, 2015).

In this work, we study selective sampling and its application to interactive Imitation Learning (Ross et al., 2011). Our goal is to design algorithms that can leverage general function approximation and online regression oracles to achieve small regret on predicting the correct labels, and at thesame time minimize the number of expert queries made (query complexity). Towards this goal, we first study selective sampling which is an online active learning framework, and provide regret and query complexity bounds for general function classes (used to model the experts). Our key results in selective sampling are obtained by developing a connection between the regret of the online regression oracles and the regret of predicting the correct labels. Additionally, we bound the query complexity using the eluder dimension (Russo and Van Roy, 2013) of the underlying function class used to model the expert. We complement our results with a lower bound indicating that a dependence on an eluder dimension like complexity measure is unavoidable in the query complexity in the worst case. In particular, we provide lower bounds in terms of the star number of the function class--a quantity closely related to the eluder dimension. Our new selective sampling algorithm, called SAGE, can operate under fairly general modeling assumptions, loss functions, and allows for multiple labels (i.e., multi-class classification).

We then extend our selective sampling algorithm to the interactive IL framework proposed by Ross et al. (2011) to reduce the query complexity. While the DAgger algorithm proposed by Ross et al. (2011) has been extensively used in various robotics applications (e.g., Ross et al. (2013); Pan et al. (2018)), it often requires a large number of expert queries. There have been some efforts on reducing the expert query complexity by leveraging ideas from active learning (e.g., Laskey et al. (2016); Brantley et al. (2020)), however, these prior attempts do not have theoretical guarantees on bounding expert's query complexity. In this work, we provide the first provably correct algorithm for interactive IL with general function classes, called RAVIOLI, which not only achieves strong regret bounds in terms of maximizing the underlying reward functions, but also enjoys a small query complexity. Furthermore, we note that RAVIOLI operates under significantly weaker assumptions as compared to the prior works, like Ross et al. (2011), on interactive IL. In particular, we only assume access to a noisy expert, as compared to the prior works that assume that the expert is noiseless. In fact, for the noisy setting, we show that one can not even hope to learn from purely offline expert demonstrations unless one has exponentially in horizon \(H\) many samples. Such a strong separation does not hold in the noiseless setting.

Our bounds depend on the margin of the noisy expert, which intuitively quantifies the confidence level of the expert. In particular, the margin is large for states where the expert is very confident in terms of providing the correct labels, while on the other hand, the margin is small on the states where the expert is less confident and subsequently provides more noisy labels as feedback. Such kind of margin condition was missing in prior works, like Ross et al. (2011), which assumes that the expert can provide confident labels everywhere. Additionally, we note that our margin assumption is quite mild as we only assume that the expert has a large margin under the states that could be visited by the noiseless expert (however, the states visited by the learner, or by following the noisy expert, may not have a small margin).

We then extend our results to the multiple expert setting where the learner has access to \(M\) many experts/teachers who may have different expertise at different parts of the state space. In particular, there is no expert who can singlehandedly perform well on the underlying environment, but an aggregation of their policies can lead to good performance. Such an assumption holds in various applications and has been recently explored in continuous control tasks like robotics and discrete tasks like chess and Minigrid (Beliaev et al., 2022). Similar to the single expert setting, we model the expertise of the experts in multiple expert setting using the concept of margins. Different experts have different margin functions, capturing the fact that experts may have different expertise at different parts of the state space. Prior work from Cheng et al. (2020) also considers multiple experts in IL and provides meaningful regret bounds, however, their assumption on the experts is much stronger than us: they assume that for any state, there at least exists one expert who can achieve high reward-to-go if the expert took over the control starting from this state till the end of the episode. Furthermore, Cheng et al. (2020) considers the setting where one can also query for the reward signals, whereas we do not require access to any reward signals.

## 2 Contributions and Overview of Results

### Selective Sampling

Online selective sampling models the interaction between a learner and an adversary over \(T\) rounds. At the beginning of each round of the interaction, the adversary presents a context \(x_{t}\) to the learner. After receiving the context, the learner makes a prediction \(_{t}[K]\), where \(K\) denotes the number of actions. Then, the learner needs to make a choice of whether or not to query an _expert_ who is assumed to have some knowledge about the true label for all the presented contexts. The experts knowledge about the true label is modeled via the ground truth modeling function \(f^{}\), which is assumed to belong to a given function class \(\) but is unknown to the learner. If the learner decides to query for the label, then the expert will return a noisy label \(y_{t}\) sampled using \(f^{}\). If the learner does not query, then the learner does not receive any feedback in this round. The learner makes an update based on the latest information it has, and moves on to the next round of the interaction. The goal of the learner is to compete with the expert policy \(^{}\), that is defined using the experts model \(f^{}\). In the selective sampling setting, we are concerned with two things: the total regret of the learner w.r.t. the policy \(^{}\), and the number of expert queries that the learner makes. Our key contributions are as follows:

* We provide a new selective sampling algorithm (Algorithm 1) that relies on an online regression oracle w.r.t. \(\) (where \(\) is the given model class) to make predictions and to decide whether to query for labels. Our algorithm can handle multiple actions, adversarial contexts, arbitrary model class \(\), and fairly general modeling assumptions (that we discuss in more detail in Section 3), and enjoys the following regret bound and query complexity: \[_{T}=}(_{}\{  T_{}+(;T)}{ }\}) N_{T}=} \!(_{}\!\{T_{}+( ;T)(,;f^{} )}{^{2}}\})\!\!.\] (1) where \((;T)\) denotes the regret bound for the online regression oracle on \(\), \((,;f^{})\) denotes the eluder dimension of \(\), and \(T_{}\) denotes the number of rounds at which the margin of the experts predictions is smaller than \(\) (the exact notion of margin is defined in Section 3).
* We show via a lower bound that, without additional assumptions, the dependence on the eluder dimension in the query complexity bound (1) is unavoidable if we desire a regret bound of the form (1), even when \(T_{}=0\). The details are located in Section 3.2.
* For the stochastic setting, where the context \(\{x_{t}\}_{t T}\) are sampled i.i.d. from a fixed unknown distribution, we provide an alternate algorithm (Algorithm 3) that enjoys the same regret bound as (1) but whose query complexity scales with the disagreement coefficient of \(\) instead of the eluder dimension (Theorem 2). Since the disagreement coefficient is always smaller than the eluder dimension, Theorem 2 yields an improvement in the query complexity.

### Imitation Learning

We then move to the more challenging Imitation Learning (IL) setting, where the learner operates in an episodic finite horizon Markov Decision Process (MDP), and can query a noisy expert for feedback (i.e. the expert action) on the states that it visits. The interaction proceeds in \(T\) episodes of length \(H\) each. In episode \(t\), at each time step \(h[H]\) and on the state \(x_{t,h}\), the learner chooses an action \(_{t,h}\) and transitions to state \(x_{t,h+1}\). However, the learner does not receive any reward signal. Instead, the learner can actively choose to query an _expert_ who has some knowledge about the correct action to be taken on \(x_{t,h}\), and gives back noisy feedback \(y_{t,h}\) about this action. Similar to the selective sampling setting, the experts knowledge about the true label is modeled via the ground truth modeling function \(f_{h}^{}\), which is assumed to belong to a given function class \(_{h}\) but is unknown to the learner. The goal of the learner is to compete with the optimal policy \(^{}\) of the (noiseless) expert. Our key contributions in IL are:

* In Section 4, we first demonstrate an exponential separation in terms of task horizon \(H\) in the sample complexity, for learning via offline expert demonstration only vs interactive querying of experts, when the feedback from the expert is noisy.
* We then provide a general IL algorithm (in Algorithm 2) that relies on online regression oracles w.r.t. \(\{_{h}\}_{h H}\) to predict actions, and to decide whether to query for labels. Similar to the selective sampling setting, the regret bound for our algorithm scales with the regret of the online regression oracles, and the query complexity bound has an additional dependence on the eluder dimension. Furthermore, our algorithm can handle multiple actions, adversarially changing dynamics, arbitrary model class \(\), and fairly general modeling assumptions.
* A key difference from our results in selective sampling is that the term \(T_{}\) that appears in our regret and query complexity bounds in IL denote the number of time steps in which the expert policy \(^{}\) has a small margin (instead of the number of time steps when the learner's policy has a small margin). In fact, the learner and the expert trajectories could be completely different from each other, and we only pay in the margin term if the expert trajectory at that time step would have a low margin. See Section 4 for the exact definition of margin.
* In Section 4.1, we provide extensions to our algorithm when the learner can query \(M\) experts at each round. In particular, we do not assume that any of the experts is singlehandedly optimal for the entire state space, but that there exist aggregation functions of these experts' predictions that perform well in practice, and with which we compete.

## 3 Selective Sampling

In the problem of selective sampling, on every round \(t\), nature (or an adversary) produces a context \(x_{t}\). The learner then receives this context and predicts a label \(_{t}[K]\) for that context. The learner also computes a query condition \(Z_{t}\{0,1\}\) for that context. If \(Z_{t}=1\), the learner requests for label \(y_{t}[K]\) corresponding to the \(x_{t}\), and if not, the learner receives no feedback on the label for that round. Let \(\) be a model class such that each model \(f\) maps contexts \(x\) to scores \(f(x)^{K}\). In this work we assume that while contexts can be chosen arbitrarily, the label \(y_{t}\) corresponding to a context \(x_{t}\) is drawn from a distribution over labels specified by the score \(f^{}(x_{t})\) where \(f^{}\) is a fixed model unknown to the learner. We assume that a link function \(:^{K}(K)\) maps scores to distributions and assume that the noisy label \(y_{t}\) is sampled as

\[y_{t}(f^{}(x_{t})).\] (2)

In this work, we assume that the link function \((v)=(v)\) for some \(:^{K}\)(see Agarwal (2013) for more details) which satisfies the following assumption.

**Assumption 1**.: _The function \(\) is \(\)-strongly-convex and \(\)-smooth, i.e. for all \(u,u^{}^{K}\),_

\[u^{}-u_{2}^{2}(u^{})- (u)-(u),u^{}-u u^{}-u_{2}^{2}.\]

Our main contribution in this section is a selective sampling algorithm that uses online non-parametric regression w.r.t. the model class \(\) as a black box. Specifically, define the loss function corresponding to the link function \(\) as \(_{}(v,y)=(v)-v[y]\) where \(v^{K}\) and \(y[K]\). We assume that the learner has access to an online regression oracle for the loss \(_{}\) (which is a convex loss) w.r.t. the class \(\), that for any sequence \(\{(x_{1},y_{1}),,(x_{T},y_{T})\}\) guarantees the regret bound

\[_{s=1}^{T}_{}(f_{s}(x_{s}),y_{s})-_{f}_{s=1 }^{T}_{}(f(x_{s}),y_{s})^{_{}}(;T).\] (3)

When \(\) is identity (under which the models in \(\) directly map to distributions over the labels), then \(_{}\) denotes the standard square loss, and we need a bound on the regret w.r.t. the square loss, denoted by \(^{}(;T)\). When \(\) is the Boltzman distribution mapping (given by \(\) being the softmax function) then \(_{}\) is the logistic loss, and we need an online logistic regression oracle for \(\). Minimax rates for the regret bound in (3) are well known:

* _Square-loss regression:_Rakhlin and Sridharan (2014) characterized the minimax rates for online square loss regression in terms of the offset sequential Rademacher complexity of \(\), which for example, leads to regret bound \(^{}(;T)=O(||)\) for finite function classes \(\), and \(^{}(;T)=O(d(T))\) when \(\) is a \(d\)-dimensional linear class. More examples can be found in Rakhlin and Sridharan (2014, Section 4). We refer the readers to Krishnamurthy et al. (2017), Foster et al. (2018) for efficient implementations.
* _Logistic-loss regression:_ When \(\) is finite, we have the regret bound \((;T) O(||)\)(Cesa-Bianchi and Lugosi, 2006, Chapter 9). For learning linear predictors, there exists efficient improper learner with regret bound \((;T) O(d|T|)\)(Foster et al., 2018). More examples can be found in Foster et al. (2018, Section 7) and Rakhlin and Sridharan (2015).

When one deals with complex model classes \(\) such that the labeling concept class corresponding to \(\) could possibly have infinite VC dimension (like it is typically the case), then one needs to naturally rely on a margin-based analysis (Tsybakov, 2004; Shalev-Shwartz and Ben-David, 2014; Dekel et al., 2012). For \(p^{K}\), we use the following well-known notion of margin for multiclass settings3:

\[(p)=(p)[k^{*}]-_{k^{} k^{*}}(p)[k^{ }] k^{*}*{argmax}_{k}(p)[k],\] (4)

A key quantity that appears in our results is the number of \(x_{t}\)'s that fall within an \(\) margin region,

\[T_{}=_{t=1}^{T}\{(f^{}(x_{t})) \}.\]

\(T_{}\) denotes the number of times where even the Bayes optimal classifier is confused about the correct label on \(x_{t}\), and has confidence less than \(\). The algorithm relies on an online regression oracle mentioned above to produce the predictor \(f_{t}\) at every round. The predicted label \(_{t}=(f_{t}(x_{t}))=*{argmax}_{k} (f_{t}(x_{t}))[k]\) is picked based on the score \(f_{t}(x_{t})\) (where \(_{t}\) is the label with the largest score). The learner updates the regression oracle on only those rounds in which it makes a query. Our main algorithm for selective sampling is provided in Algorithm 1.4

```
0: Parameters \(,,,T\), function class \(\), and online regression oracle \(\) w.r.t \(_{}\).
1: Set \(_{}^{_{}}(,T)=^{ _{}}(;T)+}(4^{2}(T)/)\), Compute \(f_{1}_{1}()\).
2:for\(t=1\) to \(T\)do
3: Nature chooses \(x_{t}\).
4: Learner plays the action \(_{t}=(f_{t}(x_{t}))\).
5: Learner computes \[_{t}(x_{t}):=_{f}\,|f(x_{t})-f_{t}(x_{t})\| _{s=1}^{t-1}Z_{s}|f(x_{s})-f_{s}(x_{s})\|^{2}_{ }^{_{}}(,T).\] (5)
6: Learner decides whether to query: \(Z_{t}=\{(f_{t}(x_{t})) 2_{t}(x_{t})\}\).
7:if\(Z_{t}=1\)then
8: Learner queries the label \(y_{t}\) on \(x_{t}\).
9:\(f_{t+1}_{t}(\{x_{t},y_{t}\})\).
10:else
11:\(f_{t+1} f_{t}\). ```

**Algorithm 1** Selective \(\) with \(\) Feedback (SAGE)

Our goal in this work is twofold: Firstly, we would like Algorithm 2 to have a low regret w.r.t. the optimal model \(f^{}\), defined as

\[_{T}=_{t=1}^{T}\{_{t} y_{t}\}-_{ t=1}^{T}\{(f^{}(x_{t})) y_{t}\}\]

Simultaneously, we also aim to make as few label queries \(N_{T}=_{t=1}^{T}Z_{t}\) as possible. Before delving into our results, we first recall the following variant of eluder-dimension (Russo and Van Roy, 2013; Foster et al., 2020; Zhu and Nowak, 2022).

**Definition 1** (Scale-sensitive eluder dimension (normed version)).: _Fix any \(f^{}\), and define \(}(,;f^{})\) to be the length of the longest sequence of contexts \(x_{1},x_{2}, x_{m}\) such that for all \(i\), there exists \(f_{i}\) such that_

\[\|f_{i}(x_{i})-f^{}(x_{i})\|>,_{j<i}\|f_{i} (x_{j})-f^{}(x_{j})\|^{2}^{2}.\]

_The value function eluder dimension is defined as \((,^{};f^{})=_{^{ }}}(,;f^{})\)._

Bounds on the eluder dimension for various function classes are well known, e.g. when \(\) is finite, \((,^{};f^{})||-1\), and when \(\) is the set of \(d\)-dimensional function with bounded norm, then \((,^{};f^{})=O(d)\). We refer the reader to Russo and Van Roy (2013); Mou et al. (2020); Li et al. (2022) for more examples. The following theorem is our main result for selective sampling:

**Theorem 1**.: _Let \((0,1)\). Under the modeling assumptions above (in (2), (3) and (4)), with probability at least \(1-\), Algorithm 1 obtains the regret bound_

\[_{T} =}\!(_{}\!\{  T_{}+}{}^{_{}};T+}{^{2} }(1/)\})\!,\] \[N_{T} =}\!(_{}\!\{T_{ }+}{^{2}}\!^{ _{}};T, }{{4}};f^{}+}{ ^{2}^{2}}(1/)\})\!.\]

A few points are in order:

* It must be noted that for most settings we consider, as an example if model class \(\) is finite, one typically has that \((;T)||\). Thus, in the case where one has a hard margin condition i.e. \(T_{_{0}}=0\) for some \(_{0}>0\), we get \(_{T} O(|}{_{0}})\) and \(N_{T} O(,;f^{} ||}{_{0}^{2}})\).
* Our regret bound does not depend on the eluder dimension. However, the query complexity bound has a dependence on eluder dimension. Thus, for function classes for which the eluder dimension is large, the regret bound is still optimal while the number of label queries may be large.

### Selective Sampling in the Stochastic Setting

So far we assumed that the contexts \(\{x_{t}\}_{t 0}\) could be chosen in a possibly adversarial fashion, and thus our bound on the number of label queries scales with the eluder dimension. However, it turns out that if the contexts are drawn i.i.d. from some (unknown) distribution \(\), then one can improve the query complexity to scale with the value function disagreement coefficient of \(\) (defined below) which is always smaller than the eluder dimension (Lemma 6).

**Definition 2** (Scale sensitive disagreement coefficient (normed version), Foster et al. (2020)).: _Let \(\{^{K}\}\). For any \(f^{}\), and \(_{0},_{0}>0\), the value function disagreement coefficient \(^{},_{0},_{0};f^{} \) is defined as_

\[_{}_{_{0},>0}_{ _{0}}\{}{^{2}} _{x} f|f(x)-f^{}(x)|>, \|f-f^{}\|_{}\} 1\]

_where \(\|f\|_{}=_{x}|f(x)|^{2}}\)._

The key idea that gives us the above improvement, of replacing the eluder dimension by disagreement coefficient in the query complexity bound, is to use epoching for the query condition, while still using an online regression oracle to make predictions. The exact algorithm is given in Appendix E.4.

**Theorem 2**.: _Let \((0,1)\), and consider the modeling assumptions in (2), (3) and (4). Furthermore, suppose that \(x_{t}\) is sampled i.i.d. from \(\), where \(\) is a fixed distribution. Then, with probability at least \(1-\), Algorithm 3 obtains the bounds5_

\[_{T} =}\!(_{}\!\{  T_{}+}{}^{_{}};T\})\!,\] \[N_{T} =}\!(_{}\!\{T_{ }+}{^{2}}^{ _{}};T^{} ,}{{8},\,^{_{}} ;T}T;f^{}\})\!.\]

We note that Algorithm 3 automatically adapts to Tsybakov noise condition with respect to \(\).

**Corollary 1** (Tsybakov noise condition, Tsybakov (2004)).: _Suppose there exists constants \(c, 0\) s.t. \(_{x}f^{}(x)  c^{}\) for all \((0,1)\), and consider the same modeling assumptions as in Theorem 2. Then, with probability at least \(1-\), Algorithm 3 obtains the bound_

\[_{T} =}\!((^{_{}} ;T)^{}(T)^{ })\!,\] \[N_{T} =}\!((^{_{}} ;T^{}, }{{8},\,^{_{}} ;T\) (defined below). The star number is bounded from above by the eluder dimension which appears in our upper bounds (Lemma 6). While star number may not be lower bounded by eluder dimension in general, for many commonly considered classes, star number is of the same order as the eluder dimension (Foster et al., 2020). For the sake of a clean presentation, we restrict our lower bound to the binary actions case, although one can easily extend the lower bound to the multiple actions case.

**Definition 3** (scale-sensitive star number).: _For any \((0,1)\) and \((0,/2)\), define \(^{}(,,)\) as the largest \(m\) such that there exists target function \(f^{}\) and sequence \(x_{1},,x_{m}\) s.t. \( i[m]\), \(|f^{}(x_{i})|>\), \( f_{i}\) s.t.,_

\[_{j i}(f_{i}(x_{j})-f^{}(x_{j}))^{2}<^{2} |f_{i}(x_{i})|>/2f_{i}(x_{i})f^{}(x_{i})<0 |f_{i}(x_{i})-f^{}(x_{i})| 2\]

The below theorem provides a lower bound on number of queries, in terms of star number for any algorithm that guarantees a non-trivial regret bound.

**Theorem 3**.: _Given a function class \(\) and some desired margin \(>0\), define \((0,/2)\) be the largest number such that \(^{2}\{^{2}/^{}(,, ),^{2}/16\}\). Then, for any algorithm that guarantees regret bound of \([_{T}] 64^{}( ,,)}\) on all instances with margin \(/2\), there exists a distribution \(\) over \(\) and a target function \(f^{}\) with margin6\(\) such that the number of queries \(N_{T}\) made by the algorithm on that instance in \(T\) rounds of interaction satisfy_

\[[N_{T}]=^{}(,,)}{40^{2}}.\]

The above lower bound demonstrates that for any algorithm that has a sublinear regret guarantee, a dependence on an additional complexity measure like the star number (or the eluder dimension) is unavoidable in the number of queries in the worst case. This suggests that our upper bound cannot be further improved beyond the discrepancy between the star number and eluder dimension. The following corrolary illustrates the above lower bound.

**Corollary 2**.: _There exists a class \(\) with \(||=\), and \(^{}(,,)=O()\) for any \(=O(1)\) and \(=O(1)\), such that any algorithm that makes less than \(\) number of label queries, will have a regret of at least \([_{T}]\) on some instance with margin \(\)._

## 4 Imitation Learning (\(H>1\)) with Selective Queries to an Expert

The problem of Imitation Learning (IL) consists of learning policies in MDPs when one has access to an expert (aka the teacher) that can make suggestions on which actions to take at a given state. IL has enjoyed tremendous empirical success, and various different interaction models have been considered. In the simplest IL setting, studied under the umbrella of offline RL (Levine et al., 2020) or Behavior Cloning (Ross and Bagnell, 2010; Torabi et al., 2018), the learner is given an offline dataset of trajectories (state and action pairs) from an expert and aims to output a well-performing policy. Here, the learner is not allowed any interaction with the expert, and can only rely on the provided dataset of expert demonstrations for learning. A much stronger IL setting is the one where the learner can interact with the expert, and rely on its feedback on states that it reaches by executing its own policies.

In their seminal work, Ross et al. (2011) proposed a framework for interactive imitation learning via reduction to online learning and classification tasks. This has been extensively studied in the IL literature (e.g., Ross and Bagnell (2014); Sun et al. (2017); Cheng and Boots (2018)). The algorithm DAgger from (Ross et al., 2011) has enjoyed great empirical success. On the theoretical side, however, performance guarantees for DAgger only hold under the assumption that, when queried, the expert makes action suggestions from a very good policy \(^{}\) that we would like to compete with. However, in practice, human demonstrators are far from being optimal and suggestions from experts should be modeled as noisy suggestions that only correlate with \(^{}\). It turns out that IL where one only hasaccess to noisy expert suggestions is drastically different from the noiseless setting. For instance, in the sequel, we show that there can be an exponential separation in terms of the dependence on horizon \(H\) in the sample complexity of learning purely from offline demonstration vs learning with online interactions.

Formally, we consider interactive IL in an episodic finite horizon Markov Decision Process (MDP), where the learner can query a noisy expert for feedback (i.e., action) on the states that it visits. The game proceeds in \(T\) episodes. In each episode \(t\), the nature picks the initial state \(x_{t,1}\) for \(h=1\); then for every time step \(h[H]\), the learner proposes an action \(_{t,h}[K]\) given the current state \(x_{t,h}\); then the system proceeds by selecting the next state \(x_{t,h+1}_{t,h}(x_{t,h},_{t,h})\), where \(_{t,h}:\) denotes the deterministic dynamics at timestep \(h\) of round \(t\) and is unknown to the learner. The learner then decides whether to query the expert for feedback. If the learner queries, it receives a recommended action from the expert, and otherwise the learner does not receive any additional information. The game moves on to the next time step \(h+1\), and moves to the next episode \(t+1\) when it reaches to time step \(H\) in the current episode. We now describe the expert model. With \(f_{h}^{*}\) being the underlying score function at time step \(h\), the expert feedback is sampled from a distribution \((f_{h}^{*}(x))(K)\), with \(:^{K}^{K}\) being some link function (e.g., \((p)[i](p[i])\)). The goal of the leaner is to perform as well as the Bayes optimal policy7 defined as \(_{h}^{*}(x):=*{argmax}_{a[K]}(f_{h}^{*}(x))\). In particular, the learner aims to find a sequence of policies \(\{_{t}\}_{t T}\) that have a small cumulative regret defined w.r.t. some (unknown) reward function under possibly adversarial (and unknown) transition dynamics \(\{_{t,h}\}_{h H,t T}\). At the same time, the learner wants to minimize the number of queries made to the expert. Formally, we consider counterfactual regret defined as

\[_{T}=_{t=1}^{T}_{h=1}^{H}r(x_{t,h}^{ ^{*}},_{h}^{*}(x_{t,h}^{^{*}}))-_{t=1}^{T}_{h=1}^{T}r(x_{t,h},_{t,h})\]

where \(x_{t,h}\) are the states reached by the learner corresponding to the chosen actions and the dynamics, and \(x_{t,h}^{^{*}}\) denotes the states that would have been generated if we executed \(^{*}\) from the beginning of the episode under the same dynamics. The query complexity \(N_{T}\) is the total number of queries to the expert across all \(H\) steps in \(T\) episodes.

Given the selective sampling results we provided in the earlier section, one may be tempted to apply them to the imitation learning problem. However, there is a caveat. A key to the reduction in Ross et al. (2011) is to apply Performance Difference Lemma (PDL) to reduce the problem of IL to online classification under the sequence of state distributions induced by the policies played by the learning algorithm. Hence, if one blindly applied this reduction, then in the margin term, one would need to account for the states that the learner visits (which could be arbitrary). Thus, for DAgger to have meaningful bounds, we would require a large margin over the entire state space. This is too much to ask for in practical applications. Consider the example of learning autonomous driving from a human driver as the expert. It is reasonable to believe that human drivers can confidently provide the right actions when they are driving themselves or are faced with situations they are more familiar with. However, assuming that the human driver is going to be confident in an unfamiliar situation (e.g., an emergency situation that is not often encountered by the human driver), is a strong assumption. Towards that end, we make a significantly weaker, and much more realistic, margin assumption that the expert has a large margin only on the state distribution induced by \(^{*}\), and not on the state distribution of the learner or the noisy expert.8 In particular, we define \(T_{,h}\) to denote the total number of episodes where the comparator policy \(^{*}\) visits a state with low margin at time step \(h\), i.e., \(T_{,h}=_{t=1}^{T}\{(f_{h}^{*}(x_{t,h} ^{^{*}}))\}\).

We now proceed to our main results in this section. Learning from a noisy expert is indeed very challenging. In fact, learning from noisy expert feedback may even be statistically intractable in the non-interactive IL setting, where the learner is only limited to accessing offline noisy expert demonstrations for learning, e.g. in offline RL, Behavior Cloning, etc. The following lower bound formalizes this. In fact, the same lower bound also shows that AggreVaTe (Ross and Bagnell, 2014) style algorithms would not succeed under noisy expert feedback, AggreVaTe relies on roll-outs obtained by running the (noisy) expert suggestions.

**Proposition 1** (Lower bound for learning from non-interactive noisy demonstrations).: _There exists an MDP, for every \(h H\), a function class \(_{h}\) with \(|_{h}| 2^{H}\), a noisy expert whose optimal policy \(^{}(x)=*{argmax}_{a}(f_{h}^{}(x)[a])\) for some \(f_{h}^{}_{h}\) with \(T_{,h}=0\) for any \( 1/4\), such than any non-interactive algorithm needs \((2^{H})\) many noisy expert trajectory demonstrations to learn, with probability at least \(3/4\), a policy \(\) that is \(1/8\)-suboptimal w.r.t. \(^{}\)._

Proposition 1 implies that in order to learn with a reasonable sample complexity (that is polynomial in \(H\)), a learner must be able to interactively query the expert. In Algorithm 2, we provide an interactive imitation learning algorithm (with selective querying) that can learn from noisy expert feedback. The regret bound and query complexity bounds for Algorithm 2 are:

```
0: Params \(,,,T\), function classes \(\{_{h}\}_{h H}\), online regression oracle \(_{h}\) w.r.t. \(_{}\) for \(h[H]\).
1: Set \(_{}^{_{}}(_{h},T)=^{ _{}}(_{h};T)+}(4H^{2}(T)/ )\).
2: Compute \(f_{1,h}=_{1,h}()\) for \(h[H]\).
3:for\(t=1\) to \(T\)do
4: Nature chooses the state \(x_{t,1}\).
5:for\(h=1\) to \(H\)do
6: Learner plays \(_{t,h}=(f_{t,h}(x_{t,h}))\)
7: Learner transitions to the next state in this round \(x_{t,h+1}_{t,h}(x_{t,h},_{t,h})\).
8: Learner computes \[_{t,h}:=_{f_{h}}|f(x_{t,h})-f_{t,h}(x_{t,h})|_{s=1}^{t-1}Z_{s,h}\|f(x_{s,h})-f_{s,h}(x_{s,h})\|^{2}_{}^{ _{}}(_{h},T).\] (6)
9: Learner decides whether to query: \(Z_{t,h}=\{(f_{t,h}(x_{t,h})) 2_{t,h}\}\).
10:if\(Z_{t,h}=1\)then
11: Learner queries the label \(y_{t,h}\) for \(x_{t,h}\).
12:\(f_{t+1,h}_{t+1,h}(\{x_{t,h},y_{t,h}\})\)
13:else
14:\(f_{t+1,h} f_{t,h}\) ```

**Algorithm 2** \(\) **ImitatiOn Learning VIa Active Expert Querying (RAVIOLI)**

**Theorem 4**.: _Let \((0,1)\). Under the modeling assumptions above, with probability at least \(1-\), Algorithm 2 obtains:_

\[_{T} =}\!(_{}\!\{H_ {h=1}^{H}T_{,h}+}{^{2}}_{h=1 }^{H}^{_{}}(_{h};T)\})\!, \] \[N_{T} =}\!(_{}\!\{H_ {h=1}^{H}T_{,h}+}{^{2}}_{h=1 }^{H}^{_{}}(_{h};T)(_{h},\!/\!s;f_{h}^{})\})\!.\]

Since the above bound holds for any sequence of dynamics \(\{_{h,t}\}_{h H,t T}\), the result of Theorem 4 also holds for the stochastic IL setting where the transition dynamic is stochastic but fixed during the interaction. In particular, setting \(_{h,t}}_{h}\) sampled i.i.d. from a fixed stochastic dynamics \(\{}_{h}\}_{h H}\) recovers a similar bound for the stochastic setting.

### Learning from Multiple Experts

In Dekel et al. (2012), the problem of selective sampling from multiple experts is considered with the main motivation being that we can consider each expert as being confident (and correct) in certain states or scenarios, and we would like to learn from their joint feedback. The goal there is to perform not only as well as the best of them individually but even as well as the best combination of them. Consider the example of learning to drive from human demonstrations, we might have one human demonstrator who is an expert in highway driving, another human who is an expert in city driving, and the third one in off-road conditions. Each expert is confident in their own terrain, but we would like to learn a policy that can perform well in all terrains.

The formal model is similar to the single-expert case, but we now have \(M\) experts. For every time step \(h H\), the \(m\)-th expert has an underlying ground truth model \(f_{h}^{,m}_{h}^{m}\) that it uses to produceits label, i.e. for a given state \(x_{h}\) it draws its label as \(y_{h}^{m}(f_{h}^{,m}(x_{h}))\), where \(\) is the link function. On rounds in which the learner queries for the experts feedback, it gets back a label from each of the \(M\) experts, i.e. \(\{y_{h}^{1},,y_{h}^{M}\}\). While on every query the learner gets a different label from each expert, its objective is to perform as well as a comparator policy that is defined w.r.t. some ground truth aggregation function that we define next.

The aggregation function \(:([K])^{M}([K])\), known to the learner, combines the recommendation of the \(M\) experts to obtain a ground truth label for the corresponding state. In particular, on a given state \(x_{h}\), the label \(y_{h}\) is samples as:

\[y_{h}(f_{h}^{,1}(x_{h})),,(f_{h}^{ ,M}(x_{h})).\] (7)

Given the aggregation function \(\) and the above label generation process, the policy \(^{}\) that we wish to compete with in our regret bound is simply the Bayes optimal predictor given by

\[^{}(x_{h})=(((f_{h}^{,1 }(x_{h})),,(f_{h}^{,M}(x_{h})))),\] (8)

where \(:(K)[K]\) is given by \((p)=*{argmax}_{k[K]}p[K]\). Some illustrative examples of aggregation functions are given in Appendix F.5. Our main Theorem 5 below bounds the number of label queries to the experts, and regret with respect to this \(^{}\), and is obtained using the imitation learning algorithm given in Algorithm 4 in Appendix F.5.

Our bounds depend on a margin term \(T_{,h}\), that captures the number of rounds in which the Bayes optimal predictor \(^{}\) can flip its label if our estimates of the \(M\) experts are off by at most \(\) (in \(_{}\) norm). Similar to the single expert case, we only pay in the margin term for time steps in which the counterfactual trajectory w.r.t. the policy \(^{}\) has a small-margin. We note that while the trajectories taken by the learner or the noisy experts may go through states that have a large-margin, the margin term \(T_{,h}\) that appears in our bounds only accounts for time steps when the comparator policy \(^{}\) (the optimal aggregation of expert recommendations) would go to a small-margin region, which could be much smaller. For the ease of notation, we defer the exact definition of margin, and the term \(T_{,h}\) to Appendix F.5, and state the main result below:

**Theorem 5**.: _Let \((0,1)\). Under the modeling assumptions above for the multiple experts setting, with probability at least \(1-\), the imitation learning Algorithm 4 (given in the appendix) obtains:_

\[_{T} =}\!(_{}\!\{H_{h=1} ^{H}T_{,h}+}_{m=1}^{M}_{h=1 }^{H}^{_{}}(_{h}^{m};T)\}), \] \[N_{T} =}\!(_{}\!\{H_{h=1} ^{H}T_{,h}+}_{h=1}^{H}_{m=1 }^{M}^{_{}}(_{h}^{m};T) (_{h}^{m},\!/\!8;f_{h}^{,m})\}).\]

In Appendix A, we evaluate our IL algorithm on the Cartpole environment, with single and multiple experts. We found that our algorithm can match the performance of passive querying algorithms while making a significantly lesser number of expert queries. Finally, note that setting \(H=1\) in the above result, recovers an algorithm, and a similar result for selective sampling with multiple experts.

## Conclusion

In this paper, or goal is to develop algorithms for online IL with active queries with small regret and query complexity bounds. Towards that end, we started by considering the selective sampling setting (IL with \(H=1\)), and provided a selective sampling algorithm that can work with general function classes \(\) and modeling assumptions, and relies on access to an online regression oracle w.r.t. \(\) to make its predictions (Section 3). The provided regret and query complexity bounds depend on the margin of the expert model. We then extended our selective sampling algorithm to interactive IL (Section 4). For IL, we showed that the margin term that appears in the regret and the query complexity depends on the margin of the expert on counterfactual trajectories that would have been observed on following the expert policy (that we wish to compare to), instead of the trajectories that the learner observes. Thus, if the expert always chooses actions that leads to states where it is confident (i.e. has less margin), the margin term will be smaller. We also considered extensions to learning with multiple experts.