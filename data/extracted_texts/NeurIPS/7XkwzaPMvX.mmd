# Utilizing Human Behavior Modeling to Manipulate Explanations in AI-Assisted Decision Making:

The Good, the Bad, and the Scary

 Zhuoyan Li

Department of Computer Science

Purdue University

West Lafayette, IN, 47906

li4178@purdue.edu

&Ming Yin

Department of Computer Science

Purdue University

West Lafayette, IN, 47906

mingyin@purdue.edu

###### Abstract

Recent advances in AI models have increased the integration of AI-based decision aids into the human decision making process. To fully unlock the potential of AI-assisted decision making, researchers have computationally modeled how humans incorporate AI recommendations into their final decisions, and utilized these models to improve human-AI team performance. Meanwhile, due to the "black-box" nature of AI models, providing AI explanations to human decision makers to help them rely on AI recommendations more appropriately has become a common practice. In this paper, we explore whether we can quantitatively model how humans integrate both AI recommendations and explanations into their decision process, and whether this quantitative understanding of human behavior from the learned model can be utilized to manipulate AI explanations, thereby nudging individuals towards making targeted decisions. Our extensive human experiments across various tasks demonstrate that human behavior can be easily influenced by these manipulated explanations towards targeted outcomes, regardless of the intent being adversarial or benign. Furthermore, individuals often fail to detect any anomalies in these explanations, despite their decisions being affected by them.

## 1 Introduction

Recent advances in AI models have significantly increased the integration of AI-based decision aids into human decision making process. The widespread adoption of such AI-based decision aids has opened up a new paradigm of human-AI collaboration-the AI model provides recommendations for a given decision making task, while human decision makers are responsible for making the final decisions. To fully unlock the potential of AI-based decision aids in enhancing human decision making, a few studies [1; 2; 3; 4; 5] have developed computational models to capture how humans factor AI recommendations into their decision-making process, and explored how these behavioral models can be utilized to improve human-AI team performance. For example, Vodrahalli, Gerstenberg, and Zou  developed a human behavior model to characterize the impact of AI predictions and confidence levels on human final decisions. This model was then utilized to adjust the model confidence displayed to people, with the objective of calibrating human trust in AI assistance.

Meanwhile, the black-box nature of prevalent AI models has driven a greater integration of model explanations, generated through various explainable AI (XAI) methods [7; 8; 9; 10; 11], into AI-assisted decision making. These explanations seek to provide some insights into the the underlying decision rationales of AI models, assisting humans in evaluating the reliability of AI decisions and identifying the optimal strategies to rely on AI recommendations. However, many empirical studies [12; 13; 14; 15; 16; 17; 18], which evaluate the effectiveness of current XAI methods for improving people's understandingof the AI model [16; 19; 20] and supporting their calibrated trust in the model [13; 14; 20], have demonstrated that humans often struggle to use the explanations generated by these methods in the optimal way. Thus, despite designers' expectations that XAI methods will positively shape human interaction with AI models, they often fall short of their intended goals, such as fostering appropriate levels of trust and reliance in AI-assisted decision-making. This could be because existing XAI methods do not account for human reactions, making them unadaptive to human cognitive processes. If this is the case, one may naturally wonder if it is feasible to quantitatively model how humans incorporate both AI recommendations and explanations into their decision making process. If so, can the quantitative understanding of human behavior obtained from the learned behavior models be utilized to directly manipulate AI explanations, thereby nudging human decision-makers towards making targeted decisions?

To answer these questions, in this paper, we begin by training behavior models that characterize how AI recommendations and explanations are factored into human decisions based on the collected behavior datasets for various decision making tasks. Utilizing these learned models, we then adjust the AI explanations for different purposes through gradient-based optimizations. Our extensive human experiments across various tasks demonstrated that such human behavior modeling can bring forth both _benefits_ and _risks_ -- when used for benign purposes, the human behavior models can inform the adjustment of AI explanations such that the manipulated explanations could significantly enhance the decision-making performance of human-AI teams in most tasks; however, the same behavior models can also be exploited by adversarial parties for adversarial purposes, such as increasing human decision-makers' biases against a certain protected group and significantly decreasing the fairness level of humans' final decisions across different groups. Finally, examining human perceptions of these manipulated explanations reveals the "_scary_" truth -- while human decisions are easily swayed by these altered explanations, individuals generally fail to detect any anomalies in the explanations, underscoring a significant vulnerability in human-AI interaction.

## 2 Related Work

### Computational Modeling of AI-assisted Decision Making

There has been a surge of interests among researchers recently in computationally modeling human behavior in AI-assisted decision making [1; 21; 3; 4; 5; 22; 23; 24]. The goals of these studies for modeling human behavior are diverse, encompassing improving human-AI team performance through intelligent interventions or model recommendation adjustments [6; 25; 26; 27; 28; 29], deciding when to present AI powered code suggestion in programming , evaluating the utility of AI explanations to improve user understanding of AI model behavior [31; 32; 33], and deploying adversarial attacks on AI models to reduce human trust . In this paper, we take a more holistic view and explore whether we can model how humans integrate both AI recommendations and explanations into their decision making process, and what the implications of such behavior modeling are.

### Human-centered Evaluation of AI Explanations

With the increasing use of AI technologies as decision aids for humans, a variety of explainable AI techniques have been developed to increase the interpretability of AI models [7; 8; 9; 10; 11; 33; 35; 36; 37]. To understand the effectiveness of these explanation methods, a growing body of empirical human-centered evaluations have been conducted to examine how AI explanations would affect the ways that humans perceive and interact with AI models [12; 13; 14; 15; 16; 17; 31; 32; 38; 39]. These evaluations look into various aspects of impacts of AI explanations, such as the influence on people's trust and reliance on the AI model [13; 14; 20], understanding of AI model [16; 19; 20], and the collaboration performance of the human-AI team [12; 40; 41]. Recently, some research has explored the modification of AI explanations to influence human behavior. For example, Lakkaraju and Bastani  demonstrated that handcrafting modifications in AI explanations--such as hiding sensitive features--can mislead human trust in AI models. Another study  found that aligning AI explanations with humans' own decision rationales can increase agreement between human decisions and the AI model's predictions. Different from previous work which required intensive handcrafting of AI explanations, in this paper, we explore whether it is possible to directly exploit the computational human behavior models to manipulate AI explanations, even without the access to AI models, with the goal of nudging human decisions towards targeted directions.

Methodology

### Problem Formulation

In this study, we explore the scenario of human-AI collaboration within the context of AI-assisted decision making, and we now formally describe it. Consider a decision making task represented by a \(n\)-dimension feature vector \(^{n}\), and \(y\) is the correct decision to make in this task. Specifically, in this study, we focus on decision making tasks with binary choices of decisions, i.e., \(y\{-1,1\}\). The AI model's recommendation on the decision task is represented as \(y^{m}=M()\,,y^{m}\{-1,1\}\). Following the explainable AI methods like LIME or SHAP, the AI model could also provide some "explanations" of its decision, \(=(M())\), \(^{n}\), by showing the contributions of each feature to the decision. With all these information, the human decision maker (DM) needs to make the final decision \(y^{h}\{-1,1\}\) by either accepting or rejecting AI model's decision recommendation \(y^{m}\), which can be characterized by \(y^{h}=(,y^{m},)\). The goal of our study is to explore whether we can quantitatively model such decision making process--specifically, \((,y^{m},)\)--and whether this quantitative understanding of human behavior can be utilized to adjust AI explanations (i.e., change \(\) to \(^{}\)) without accessing to the original AI model \(M()\), thereby nudging human DMs to make the targeted decision \(^{h}\{-1,1\}\), denoted as \(^{h}=(,y^{m},^{})\).

### Modeling Human Behavior in AI-assisted Decision Making

We first build computational models to characterize how humans integrate both AI recommendations and explanations into their decision process. Following previous works on modeling human behavior in different scenarios of AI-assisted decision making [23; 31], we adopted a two-layer neural network as the structure for modeling the human decision in this study:

\[y^{h}=_{_{h}}(,y^{m},)=_{_{h}}([ ,y^{m},,])\] (1)

The inputs to the behavior model include the task features \(\), the AI model's prediction \(y^{m}\), the AI explanation \(\), and the interaction term between the task features and the AI explanation \(\) that reflects how humans may redirect their attention to the corresponding features highlighted by the AI explanation. Given the human behavior dataset \(=\{_{i},y_{i}^{m},_{i},y_{i}^{h}\}_{i=1}^{N}\), we can employ the maximum log-likelihood estimation to learn the behavior model \(_{_{h}}\).

### Manipulating AI Explanations through the Behavior Model

We next proceed to explore how the quantitative understanding of human behavior from \(_{_{h}}\) can be utilized to manipulate AI explanations. In particular, given the targeted decision \(^{h}\) for the task instance \(\), we want to identify a new AI explanation \(^{}\) that maximizes the likelihood that human DMs make the targeted decision \(^{h}\) according to the learned behavior model \(_{_{h}}\). In addition, to prevent the case where the manipulated explanations \(^{}\) has a very low level of fidelity , such as suggesting a recommendation that is inconsistent with the AI model's prediction \(y^{m}\), we also impose a constraint that the new explanation \(^{}\) should still support the original AI recommendation \(y^{m}\). Since we assume no access to the original AI model, we define \(_{}(,y^{m})\) as a measurement of agreement consistency between the manipulated AI explanations and the AI recommendation:

\[_{}(,y^{m})=0&(_{i}_{i})=(y^{m}),\\ 1&.\] (2)

Together, we use the following optimization problem to manipulate AI explanations:

\[_{^{}^{n}}_{}( _{_{h}}(,y^{m},^{}),^{h}),_{}(^{},y^{m}) 0\] (3)

where \(_{}\) is defined as the cross entropy function. Since exactly solving the above optimization problem is intractable, we used the gradient-based optimization to approximate it:

\[^{}_{^{t+1}}=^{}_{^{t}}-_{^{}_{}}(_{}(_{_{h}}( ,y^{m},^{}_{}),^{h})+_{ }(^{}_{},y^{m}))\] (4)

where \(\) is the step size, \(\) is the trade-off parameter, and \(^{}_{}\) represents the parameterized explanations in the optimization process. We can iteratively optimize manipulated explanations \(^{}\) until \(_{}\) is smaller than a threshold \(\) or reach the maximum number of rounds \(T\).

Human Behavior Model Learning

To develop the human behavior model for manipulating AI explanations, we first conduct a human subject experiment to collect human behavior data.

### Decision Making Task and AI Assistance

We consider four decision making tasks in this study:

* **Census Prediction (Tabular Data)**: This task was to determine a person's annual income level. In each task, the human DM was presented with a profile with 7 features, including the person's gender, age, education level, martial status, occupation, work type, and working hour per week. The subject was asked to decide whether this person's annual income is higher or lower than $50k for each task. We trained a random forest model to make the income prediction, and the accuracy of the AI model was \(76\%\).
* **Recidivism Prediction (Tabular Data)**: This task was to determine a person's recidivism risk. In each task, the human DM was presented with a profile with 8 features, including their basic demographics (e.g., gender, age, race), criminal history (e.g., the count of prior non-juvenile crimes, juvenile misdemeanor crimes, juvenile colony crimes committed), and information related to their current charge (e.g., charge issue, charge degree). The subject was asked to decide whether this person would reoffend within two years. We trained a random forest model to make the prediction, and the accuracy of the AI model was \(62\%\).
* **Bias Detection (Text Data)**: In this task, the human DM was presented with a text snippet and needed to decide whether it contained any bias. We fine-tuned a BERT  model to identify bias in the snippet, and the accuracy of the AI model is \(79\%\).
* **Toxicity Detection (Text Data)**: In this task, the human DM was presented with a text snippet and needed to decide whether it contained any toxic content. We fine-tuned a BERT model to identify the toxic content, and the accuracy of the AI model is \(86\%\).

To understand how people respond to various AI explanations, we employed LIME and SHAP to explain the predictions made by the AI model. Additionally, we augment the LIME or SHAP explanations by either randomly masking out contributions from some features or amplifying contributions of some features (referred to as the "Augmented" explanations) to see how humans react to them. These explanations are provided with AI recommendations together to humans in decision making.

### Experimental Procedure

We posted our data collection study on the Prolific 1 to recruit human participants. Upon arrival, we randomly assigned each participant to one of the four decision making tasks and they needed to fill in an initial survey to report their demographic information and their knowledge of AI models and explanations. Participants started the study by completing a tutorial that described the decision making task that they needed to work on. To familiarize participants with the task, we initially asked them to complete five tasks independently without AI assistance. During these training tasks, we immediately provided the correct answer at the end of the task. After the completion of training tasks, participants moved on to the formal tasks. In the formal tasks, participants would receive one type of AI explanations among SHAP, LIME, or Augmented. Specifically, each participant was asked to complete a total of 15 tasks. In each task, participants were provided with the AI prediction and the explanations along with the task instance. They were then required to make their final decisions. Finally, participants were required to complete an exit survey to report their perceptions of the AI explanations they received during the study. They were asked to rate the alignment of AI explanations with their own rationale, as well as the usefulness, transparency, comprehensibility, satisfaction with the provided explanations, and their trust in the AI models, on a 5-point Likert scale. For the detailed survey questions, please refer to Appendix A.3. We offered a base payment of $1.2 and a potential bonus of $1 if the participant's accuracy is above 85%. The study was open to US-based workers only, and each worker can complete the study once.

### Training Results

After collecting data on human behavior, we developed human behavior models for each type of task. For the human behavior models for two textual tasks--**Toxicity Detection** and **Bias Detection**--we employed the pretrained BERT encoder to extract features from the original sentences, which were then used as the task feature \(\) in the human behavior model \(H_{_{h}}\). We optimized these behavior models using Adam  with an initial learning rate of \(1e-4\) and a batchsize of each training iteration of 128. The number of training epochs is set as \(10\). Table 1 shows the number of participants recruited, as well as the average accuracy of the human behavior model evaluated through 5-fold cross validation for each task. We observed that the average accuracy of all human behavior models exceeds \(0.65\), which is considered to be reasonable. Consequently, we utilized these learned human behavior models to manipulate AI explanations in the following evaluations.

## 5 Evaluation I: Manipulating AI Explanations for Adversarial Purposes

In our first evaluation, we adopted the role of an adversarial party to explore whether they could utilize the learned human behavior model to manipulate AI explanations. The manipulation goal was to nudge human DMs to be biased against certain protected groups in the decision making process. We are particularly interested in comparing the fairness level of human decision outcomes between human DMs who receive original explanations, such as SHAP or LIME, and those who receive manipulated explanations. Notably, all human DMs are provided with the same AI predictions for the same decision making task. Additionally, we also explore differences in human perceptions of original AI explanations versus manipulated AI explanations.

Evaluation Metrics and Manipulating AI Explanations.Following previous work [50; 51], we used the false positive rate difference (i.e., _FPRD_) and the false negative rate difference (i.e., _FNRD_) to measure the fairness level of human decision outcomes--the closer these values are to zero, the more fair the decisions are. To manipulate AI explanations and nudge human DMs toward biasing against certain protected groups, we define the targeted human decision \(^{h}\) for each task as follows:

* **Census Prediction**: In this task, we considered a person's sex as the protected attribute. The targeted human decision is defined as \(^{h}=1\) (indicating a person's annual income exceeds $50K) when \(_{}=\), and \(^{h}=-1\) (indicating a person's annual income does not exceed $50K) when \(_{}=\). The fairness metrics can be computed as \(=_{}-_{}\), and \(=_{}-_{}\).
* **Recidivism Prediction**: In this task, we considered the defendant's race as the protected attribute. The targeted human decision is defined as \(^{h}=1\) (indicating the defendant will reoffend) when \(_{}=\), and \(^{h}=-1\) (indicating the defendant will not reoffend) when \(_{}=\). The two fairness metrics can be computed as \(=_{}-_{}\), and \(=_{}-_{}\).
* **Bias Detection**: In this task, we divided text snippets into groups based on their political leaning. The targeted human decision is defined as \(^{h}=1\) (indicating the text is biased) when \(_{}=\), and \(^{h}=-1\) (indicating the text is not biased) when \(_{}=\). The fairness metrics can be computed as \(=_{}-_{}\), and \(=_{}-_{}\).
* **Toxicity Detection**: In this task, we divided text snippets into groups based on the victim of the text. The targeted human decision is defined as \(^{h}=1\) (indicating the text is toxic) when \(_{}=\), and \(^{h}=-1\) (indicating the text is non-toxic) when \(_{}=\). The two fairness metrics can be computed as \(=_{}-_{}\), and \(=_{}-_{}\).

    & **Census** & **Recidivism** & **Bias** & **Toxicity** \\  Number of Participants & 78 & 80 & 72 & 42 \\ Model Accuracy & 0.74 & 0.79 & 0.65 & 0.76 \\   

Table 1: The number of subjects recruited in data collection for training behavior models, and the average accuracy of the human behavior model in 5-fold cross validation for each task.

For the Bias Detection and Toxicity Detection tasks, the original datasets  provide annotations for the political leanings of the sentences and the targeted victims of the text snippets, respectively. After determining the targeted decision \(^{h}\) for each task instance, we then followed the gradient-based optimization procedure (i.e., Equation 4) to identify the manipulated explanation. We set the step size \(\) as \(0.01\), the trade-off \(\) as \(0.01\), the optimization threshold \(\) as \(0.1\), and the maximum optimization number \(T\) as \(100\). For the initial AI explanation \(_{^{0}}\) at the start of the optimization process, we directly initialize \(^{}_{^{0}}\) as \(^{}_{^{0}} U(-1,1)\). We then repeated this optimization process for 5 times and took the average to use in the following human evaluations. For the examples of manipulated explanations, please refer to Appendix B.3.

Data Collection.We followed the experimental procedure described in Section 4.2 to collect data on human responses to and perceptions of different AI explanations. We randomly assigned either SHAP or LIME explanations, or the manipulated explanations, to participants. Participants were required to complete 15 tasks with AI model predictions and the assigned explanations (SHAP or LIME or manipulated). We offered a base payment of $1.2 and a potential bonus of $1 if the participant's accuracy is above 85%. Table 2 reports detailed statistics of the participants in each task. Below, we analyzed how the manipulated explanations affect fairness level of human decisions and how do humans perceive those explanations.

### How do the adversarially manipulated explanations affect fairness level of human decisions?

The fairness levels of participants' decision outcomes under the manipulated explanation, SHAP explanation, and LIME explanation are presented in Figure 1. Visually, it appears that when human DMs are provided with manipulated explanations, both _FPRD_ and _FNRD_ scores of their decision outcomes tend to deviate more from zero compared to when DMs receive SHAP or LIME explanations.

To examine whether these differences are statistically significant, we conducted regression analyses. Specifically, the focal independent variable was the type of explanation received by participants, while the dependent variables were the participants' _FPRD_ and _FNRD_ scores. To minimize the impact of potential confounding variables, we included a set of covariates in our regression models, such as participants' demographic background (e.g., age, race, gender, education level), their knowledge of AI explanations, their trust in AI models, and the _FPRD_ or _FNRD_ scores of the AI model decisions they received in the study. These covariates were selected based on prior HCI research  which empirically reveal how characteristics of human DMs may moderate the impacts of AI explanations on human decisions in AI-assisted decision making.

Our regression results indicate that the adversarial party can significantly increase the level of unfairness in human decision outcomes with manipulated explanations through human behavior modeling. Specifically, when examining _FPRD_, we found that participants who received manipulated AI explanations made more unfair decisions compared to those who received SHAP or LIME explanations (\(p<0.05\)) in the Census and Recidivism tasks. The difference was marginally significant (\(p<0.1\)) in the Toxicity task. When examining _FNRD_, results show that participants who received manipulated explanations made decisions that were significantly more unfair than those who received SHAP or LIME explanations (\(p<0.01\)) in the Bias task.

### How do humans perceive the adversarially manipulated AI explanations?

   & **Census** & **Recidivism** & **Bias** & **Toxicity** \\  SHAP & 86 & 89 & 60 & 88 \\ LIME & 65 & 71 & 59 & 85 \\ Adversarially Manipulated & 82 & 92 & 71 & 65 \\ Benignly Manipulated & 77 & 84 & 69 & 46 \\  

Table 2: The number of participants we recruited in the evaluation study, categorized according to the type of AI explanation they received and the task they were assigned to.

In Section 5.1, we found that the adversarial party can manipulate AI explanations to nudge human DMs toward making more unfair decisions compared to those who received the original AI explanations, aligning with the adversarial party's intentions. To determine whether DMs could detect any abnormalities in the manipulated explanations, we examined how their perceptions of AI explanations varied among manipulated, SHAP, and LIME explanations.

Figures 1(a) and 1(b) compare the average perceived transparency and usefulness of three types of AI explanations. Visually, there are no significant differences in how the explanations are perceived by people who received different explanations. We also applied regression models to predict human perceptions of these explanations by accounting for their demographic background (e.g., age, race, gender, education level), their knowledge of AI explanations and their trust in AI models. The regression results indicate that there are no significant differences in the perceived transparency and usefulness of manipulated explanations compared to SHAP or LIME explanations. Similar patterns were observed for perceptions of alignment, comprehensibility, satisfaction, and trust between the manipulated and unmanipulated explanations. While adversarially manipulated explanations significantly influence human decision making behavior, individuals generally do not detect abnormalities in the manipulated AI explanations across most tasks. For further details, please refer to Appendix B.2.

## 6 Evaluation II: Manipulating AI Explanations for Benign Purposes

In the previous section, we found that the adversarial party could use the behavior model to manipulate AI explanations, thereby misleading humans into making unfavorable decisions against specific groups. Naturally, one might wonder could a third party also use behavior models to manipulate AI explanations for benign purposes, such as promoting more appropriate human reliance on AI models? For instance, can manipulated AI explanations lead humans to reject AI recommendations when the

Figure 1: Comparing _average FPRD_ and _FNRD_ of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. \({}^{*}\), \({}^{**}\), and \({}^{***}\) denote significance levels of \(0.1\), \(0.05\), and \(0.01\), respectively. For both _FPRD_ and _FNRD_, a value closer to zero indicates that the human decisions are more _fair_.

Figure 2: Comparing the _average_ human perceived transparency and usefulness of the adversarially manipulated explanation, SHAP explanation, and LIME explanation. Error bars represent the 95% confidence intervals of the mean values.

AI model decision is likely incorrect, and encourage acceptance when the decision is likely correct? We aim to explore the answers to this question in this section.

Evaluation Metrics and Manipulating AI ExplanationsFollowing previous work , we used the accuracy, underreliance, and overreliance to measure human DMs' appropriate reliance level on AI models. Underreliance refers to the fraction of tasks where the participant's decision was _different_ from the AI model's decision when the AI model's decision was correct. Overreliance refers to the fraction of tasks where the participant's decision was the _same_ as the AI model's decision when the AI model's decision was incorrect. To manipulate AI explanations for the promotion of more appropriate reliance on AI models, it is necessary to determine the reliability of AI model prediction on each task instance. Recent work  has proposed methods to leverage the complementary strengths of humans and AI by combining human independent decisions and AI model, which is often shown to result in more accurate decisions than those made by either humans or AI models alone. Specifically, given the human independent decision \(y^{h}_{}\), the AI model recommendation \(y^{m}\), and the task instance \(\), these methods learn models to combine \(y^{h}_{}\) and \(y^{m}\) to produce a combined result:

\[y_{}=(y^{h}_{},y^{m},)\] (5)

To see whether \(y_{}\) can yield better decisions compared to AI alone or human alone, we evaluated various combination models including the human-AI combination method  and several truth inference methods  used in crowdsourcing. Our results showed that the human-AI combination method  generally outperformed AI solo and independent human decision, as well as other combination methods. Thus, \(y_{}\) produced by the human-AI combination method  is defined as the targeted decision \(^{h}\) for manipulating AI explanations. For detailed information on the evaluations of each combination method, please refer to the Appendix C.1. We again followed Equation 4 to manipulate the AI explanations. We set the step size \(\) as \(0.01\), the trade-off \(\) as \(0.01\), the optimization threshold \(\) as \(0.1\), and the maximum optimization number \(T\) as \(100\), and the initial AI explanation \(^{}_{^{0}}\) at the start of the optimization process is initialized as \(^{}_{^{0}} U(-1,1)\). We repeated this optimization process for 5 times and took the average to use in the following experiments. For the examples of manipulated explanations, please refer to Appendix C.4.

Data Collection.We recruited participants from Prolific once again to collect behavioral data under the benignly manipulated explanations, following the experimental procedure described in Section 4.2. We offered a base payment of $1.2 and a potential bonus of $1 if the participant's accuracy is above 85%. Table 2 shows the detailed statistics of the participants we recruited for each task. Subsequently, we analyzed whether the benignly manipulated explanations can promote appropriate reliance of human DMs on AI models, as well as their perceptions of these AI explanations.

### Can benignly manipulated explanations promote appropriate reliance of human DMs on AI models?

Figures 2(a), 2(b), and 2(c) compare the average accuracy, overreliance, and underreliance of human decision outcomes under manipulated, SHAP, and LIME explanations, respectively. It is clear that providing human DMs with manipulated AI explanations leads to an increase in the accuracy of their decision outcomes for most of tasks. We subsequently conducted regression analyses to determine

Figure 3: Comparing the _average_ accuracy, overreliance, and the underreliance of human decision outcomes under the benignly manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. \({}^{**}\), and \({}^{***}\) denote significance levels of \(0.1\), \(0.05\), and \(0.01\) respectively.

whether these differences are statistically significant. The regression models incorporated a set of covariates, including participants' demographic backgrounds (e.g., age, race, gender, education level), their knowledge of AI explanations, their trust in AI models, and the accuracy of the AI models. The regression results indicate that in the Census, Recidivism, and Bias tasks, substituting SHAP or LIME explanations with manipulated explanations significantly improves the accuracy of human-AI team. In contrast, for the Toxicity task, we observed no statistical difference, which could potentially be attributed to the high competence of humans in solving this task (e.g., when presented with SHAP or LIME explanations, the average decision accuracy of participants already exceeds 0.8, leaving limited room for further improvement).

### How do humans perceive benignly manipulated AI explanations?

In Section 5.2, we observed that it is challenging for humans to detect abnormalities in the adversarially manipulated explanations, even though they are unconsciously influenced by the manipulated explanations to make more unfair decisions. In this section, we revisit this question to investigate into whether humans' perceptions of the manipulated explanations change, when they are manipulated for benign purposes. Figures 3(a) and 3(b) compare the average human perceived transparency and usefulness of the benignly manipulated explanations, SHAP explanations, and LIME explanations. Regression analyses reveal no statistically significant differences among the perceived transparency and usefulness of these three types of explanations. Similar trends were observed for other perceptual aspects of explanations, including perceived alignment, comprehensibility, satisfaction, and trust. For further results, please refer to Appendix C.3.

## 7 Conclusion and Limitations

In this paper, we explore whether we can quantitatively model how humans incorporate both AI recommendations and explanations into their decision making process, and whether we can utilize the quantitative understanding of human behavior obtained from these learned models to manipulate AI explanations for both adversarial and benign purposes. Our extensive experiments across various tasks demonstrate that human behavior can be easily influenced by these manipulated explanations toward targeted outcomes, regardless of the intent being benign or adversarial. Despite the significant influence of these falsified explanations on human decisions, individuals typically fail to detect or recognize any abnormalities. Our study has several limitations. For example, it focuses on modeling and manipulating score-based explanations. Further research is needed to explore how to model how humans incorporate other types of explanations, such as example-based and rule-based explanations, and how these can be manipulated to influence human behavior as observed with score-based explanations in our study. Additionally, our study was limited to decision making tasks involving tabular and textual data, which are naturally suited to score-based explanations. Further explorations are needed to extend these findings to decision tasks with other data types (e.g., images).

Figure 4: Comparing the _average_ human perceived transparency and usefulness of the benignly manipulated explanations, SHAP explanations, and LIME explanations. Error bars represent the 95% confidence intervals of the mean values.

## Ethical Consideration

This study was approved by the Institutional Review Board of the authors' institution. Through our findings, we aim to draw the community's attention to the ease with which third parties can manipulate AI explanations with the learned behavior models to influence human decision making. Users often lack the ability to accurately and appropriately interpret the AI explanations presented to them, yet their decision behavior is easily swayed by the manipulated AI explanations. Our findings highlight the critical importance of securing human-AI interaction data to prevent the misuse of human behavior models derived from it. Additionally, there is an urgent need to ensure that AI explanations provided to humans are more secure and inherently benign. Moreover, providing pre-education is essential to assist humans in establishing a proper understanding of AI explanations, which may potentially mitigate the risks of manipulation.

In addition, our experiments are based on datasets that are publicly available; "correct" decisions for the tasks in these datasets are generally considered as recording the real-world ground truth. While these datasets are not intentionally biased toward any specific groups, we acknowledge that there might be implicit biases introduced to these datasets during the curation process, which are beyond our control. Importantly, we note that we made no alterations to the datasets that would introduce additional bias in our experiment.