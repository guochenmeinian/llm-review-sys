# A Quadratic Synchronization Rule for

Distributed Deep Learning

 Xinran Gu\({}^{1}\)1 Kaifeng Lyu\({}^{3}\)1 Sanjeev Arora\({}^{3}\)2 Jingzhao Zhang\({}^{1,2}\)\({}^{}\) Longbo Huang\({}^{1}\)\({}^{1}\)

\({}^{1}\)Institute for Interdisciplinary Information Sciences, Tsinghua University

\({}^{2}\)Shanghai Qizhi Institute

\({}^{3}\)Department of Computer Science & Princeton Language and Intelligence, Princeton University

gxr21@mails.tsinghua.edu.cn

{klyu,arora}@cs.princeton.edu

{jingzhaoz,longbohuang}@tsinghua.edu.cn

Equal contributionCorresponding authors

###### Abstract

In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models. Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for \(H\) steps without synchronizing with others, hence reducing communication frequency. While \(H\) has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper \(H\) value can lead to generalization improvement. Yet, selecting a proper \(H\) is elusive. This work proposes a theory-grounded method for determining \(H\), named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting \(H\) in proportion to \(}\) as the learning rate \(\) decays over time. Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. Compared with the standard data parallel training, QSR enables Local AdamW on ViT-B to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves \(1.12\%\) or \(0.84\%\) higher top-1 validation accuracy.

## 1 Introduction

The growing scale of deep learning necessitates distributed training to reduce the wall-clock time. Data parallel training is a foundational technique that distributes the workload of gradient computation to \(K\) workers, also serving as a key building block of more advanced parallel strategies. At each step of this method, each worker first computes gradients on their own local batches of data. Then, they take an average over local gradients, which typically involves a costly All-Reduce operation. Finally, they update the model parameter with the averaged gradient and a gradient-based optimizer OPT, e.g., SGD, AdamW. In this paper, we term the data parallel implementation of optimizer OPT as "Parallel OPT". See Algorithm 1 for the pseudocode. The cost for this data parallelism is obvious. Frequent gradient synchronization can induce huge communication overhead as the number of workers and model size grow, severely hindering the scalability of distributed training (Tang et al., 2021; Li et al., 2022; Xu et al., 2023).

One approach to reducing this communication overhead is Local SGD (Stich, 2018; Zhou & Cong, 2018; Woodworth et al., 2020). Rather than synchronizing gradients at every step, Local SGD allows workers to independently train their local replicas using their own local batches with SGDupdates. It is only after completing \(H>1\) local steps that these workers synchronize, where the model parameters get averaged over all replicas. Notably, while we mention SGD, this approach can be readily adapted to other popular optimizers. In this paper, if a gradient-based optimizer OPT is used for local updates, we term the variant as "Local OPT" (e.g., Local SGD, Local AdamW), and collectively refer to this class of approaches as _local gradient methods_. We provide a pseudocode for local gradient methods in Algorithm 2.

The main focus of this paper is to study the best strategies to set the synchronization period \(H\) (i.e., the number of local steps per communication round) in local gradient methods. While setting \(H\) to a larger value reduces communication, a very large \(H\) can hinder the training loss from decreasing at normal speed, since the local replicas may significantly diverge from each other before averaging. Indeed, it has been observed empirically that larger \(H\) leads to higher training loss after the same number of steps (Wang and Joshi, 2021; Ortiz et al., 2021), and efforts to analyze the convergence of local gradient methods in theory usually end up with loss bounds increasing with \(H\)(Khaled et al., 2020; Stich, 2018; Haddadpour et al., 2019; Yu et al., 2019). To better trade-off between communication cost and optimization speed, Kamp et al. (2014); Wang and Joshi (2019); Haddadpour et al. (2019); Shen et al. (2021) proposed adaptive synchronization schemes, such as linearly increasing \(H\) as the iteration goes on (Haddadpour et al., 2019), or adjusting \(H\) based on the variance in model parameters (Kamp et al., 2014). Nonetheless, their effectiveness has only been validated on linear models or small-scale datasets, e.g., CIFAR-10/100.

All these strategies are developed to avoid sacrificing too much training loss, but training loss is _never_ the final evaluation metric that one cares about in deep learning. Due to the overparameterized nature of modern neural networks, reaching the same training loss does not correspond to the same performance on test data. It has also been long known that the choice of optimizers or hyperparameters can change not only the optimization speed of the training loss but also their _implicit bias_ towards solutions with different test accuracies.

The presence of this implicit bias indeed complicates the picture of setting \(H\) in local gradient methods. Though a large \(H\) might be harmful for training loss, it has been observed empirically that setting \(H\) properly can sometimes improve rather than hurt the final test accuracy. Lin et al. (2020) are the first to report this phenomenon. Comparing with running just the standard data parallel SGD (equivalent to \(H=1\)), they observed that switching from SGD to Local SGD (\(H>1\)) halfway through consistently leads to higher final test accuracy. Local SGD with this specific schedule of \(H\) is designated as _Post-local SGD_. Lin et al. (2020)'s work opens up a new angle in setting \(H\) in local gradient methods, yet, the proposed schedule in Post-local SGD, referred to as the post-local schedule in this paper, is suboptimal in improving test accuracy. It was later reported by Ortiz et al. (2021) that Post-local SGD does not improve much on ImageNet. For both stepwise decay and cosine decay learning rate schedules, the test accuracy improvement of Post-local SGD diminishes as learning rate decreases. Further, it remains unclear whether the generalization benefit continues to appear when the optimizer is changed from SGD to adaptive gradient methods such as Adam/AdamW, which are now indispensable for training large models.

Figure 1: When training ResNet-152 and ViT-B on ImageNet with cosine learning rate decay, Local SGD/AdamW with QSR consistently outperforms data parallel methods or Local SGD/AdamW with other synchronization strategies in terms of top-1 validation accuracy, while only requiring 20.1% and 10.4% of the communication volume used by data parallel methods, respectively. With QSR, Local SGD on ResNet or Local AdamW on ViT cuts the training time from 20.7 to 18 hours or 26.7 to 20.2 hours on 16 GPUs, when compared with data parallel methods. See Appendix C for training details.

Our Contributions.In this paper, we aim to propose a general and effective \(H\) schedule that can be readily applied to various optimizers and neural network models. Specifically, we introduce a simple yet effective strategy, called _Quadratic Synchronization Rule_ (QSR), for dynamically adjusting the synchronization period according to the learning rate: given a learning rate schedule, we set \(H\) proportional to \(^{-2}\) as the learning rate \(\) decays. This rule is largely inspired by a previous theoretical work (Gu et al., 2023), which shows that the generalization benefits arise only if \(H=()\) when \( 0\), but did not make any recommendation on how to set \(H\).

Our main contributions are:

1. We propose the Quadratic Synchronization Rule (QSR) to simultaneously reduce the wall-clock time and improve the final test accuracy of local gradient methods. Based on the theoretical insights in Theorem 3.1, we provide a theoretical separation among data parallel SGD, Local SGD with \(H^{-1}\), and Local SGD with QSR in terms of SDE approximations. We show that QSR can help reduce sharpness faster and hence improve generalization.
2. We demonstrate with ImageNet experiments that QSR can consistently improve the final test accuracy of ResNet-152 and ViT-B over other synchronization strategies, including constant-period and post-local schedules, and also \(H^{-1}\) which one will expect to be optimal from the optimization perspective (Figure 1).
3. We thoroughly validate the efficacy of QSR not only for Local SGD but also for Local AdamW, which is arguably more suitable for training large models. We also validate its efficacy for cosine, linear and step decay learning rate schedules that are commonly used in practice.
4. We evaluate the communication efficiency of QSR on a 64-GPU NVIDIA GeForce RTX 3090 cluster. As an illustrative example, the standard data parallel AdamW takes 8.6 hours to train ViT-B for 300 epochs. With our QSR, Local AdamW cuts the training time down to 5.5 hours with even higher test accuracy.

## 2 Our Method: Quadratic Synchronization Rule

In this section, we first formulate the local gradient methods and then present our Quadratic Synchronization Rule (QSR) in detail.

Local Gradient Methods.Given any gradient-based optimizer OPT, the corresponding local gradient method consists of multiple communication rounds. At the \(s\)-th round, each of the \(K\) workers (say the \(k\)-th) gets a local copy of the global iterate \(}^{(s)}\), i.e., \(^{(s)}_{k,0}}^{(s)}\), and then performs \(H\) steps of local updates. At the \(h\)-th local step of the \(s\)-th round, which corresponds to the \((sH+h)\)-th iteration globally, each worker gets a batch of \(B_{}\) samples \((^{(s)}_{k,h,1},,^{(s)}_{k,h,B_{}})\) from a globally shared dataset \(\), computes the gradient on that batch, and updates the model with optimizer OPT and learning rate \(_{sH+h}\):

\[^{(s)}_{k,h+1}(^{(s)}_{k,h},_{ sH+h},^{(s)}_{k,h})^{(s)}_{k,h}=}}_{i=1}^{B_{}}(^{(s)}_{k,h;i} ;^{(s)}_{k,h,i}). \]

After finishing \(H\) steps of local updates, all workers average their local models to generate the next global iterate: \(}^{(s+1)}_{k=1}^{K}^{(s)} _{k,H}\). Note that conventional local gradient methods set the synchronization period as a constant, denoted as \(H\), throughout training. See also Algorithm 2.

Quadratic Synchronization Rule.Given a learning rate schedule \(_{t},t\{0,,T-1\}\) that decays with time, instead of keeping \(H\) constant, we propose to dynamically increase the synchronization period \(H^{(s)}\) at each round \(s\) as the learning rate decreases. More specifically, if at the global iteration \(t\) we need to start a new communication round, then we set

\[H^{(s)}:=\{H_{},( })^{2}\}. \]

Here \(H_{}\) is a constant indicating the minimum number of local steps one would like to use for each round, which should be set according to the relative cost of computation and communication. The coefficient \(\), termed the "growth coefficient" henceforth, is a hyperparameter controlling how fast \(H^{(s)}\) increases as \(_{t}\) decreases.

As suggested by our later theorem 3.1, \(\) should be set as a small constant. In our experiments, we tune \(\) properly between \(0.01\) and \(0.5\) and test the effectiveness of our proposed method with \(H_{ base}=2,4,8\). Note that the last communication round may not finish exactly at the last iteration of the learning rate schedule. If this is the case, we force a synchronization at the last step by setting \(H^{(s)}:=T-t\).

A surprising part of our method is that we use the power \(2\) in the above formula (2). This choice of power \(2\) is inspired by the analysis in Gu et al. (2023), which suggests that setting \(H=()\) is beneficial for reducing the sharpness of the local landscape. Indeed, \(H^{(s)}\) could have been set to \(H^{(s)}:=\{H_{ base},|(})^{ }|\}\) for any \(\). However, using \(=2\) is crucial for the success of our method, and we will provide a theoretical justification of this choice in Section 3, together with empirical evidence. We also visualize the \(H\) schedule for QSR in Figure 5 in the appendix.

Dealing with Learning Rate Warmup.Many learning rate schedules use a warmup phase where the learning rate increases linearly from \(0\) to \(_{ max}\), and then decays monotonically. This warmup phase is often used to avoid the instability caused by the initial large learning rate (Goyal et al., 2017). Our rule is not directly compatible with the warmup phase, since it is designed for learning rate decay, but the learning rate increases rather than decreases in this phase. Practically, we recommend setting \(H^{(s)}\) as the value to be used in the communication round right after the warmup.

## 3 Theoretical Motivations of Quadratic Synchronization Rule

To justify our choice of power 2, we build on the same theoretical setup as Gu et al. (2023) to analyze the Stochastic Differential Equation (SDE) approximation of SGD and Local SGD using different scalings of \(H\) with respect to \(\). Though the learning rate continuously decays over time in most of our experiments, it does not usually change much within a couple of epochs. Inspired by this, we take a quasistatic viewpoint: consider a significant period of time where the learning rate is relatively constant, and directly treat the learning rate as a real constant \(\). First, we recap Gu et al. (2023)'s theory that applies to Local SGD with \(H^{-1}\), then we show how to generalize the result to our rule where \(H^{-2}\), leading to a stronger implicit bias towards flatter minima.

Setup.Consider optimizing the loss function \(():=_{}}[(;)]\), where \(^{d}\) is the parameter vector and \((;)\) is the loss function for a single data sample \(\) drawn from a training set/training distribution \(}\). We use \(():=_{}}[ (;)]\) to denote the covariance matrix of the stochastic gradient \((;)\) at \(\). Following Gu et al. (2023), we make regularity assumptions on \((),()\) and \(\|(;)\|_{2}\) in Assumption E.1, and we assume that \(\) has a manifold \(\) of minimizers in Assumption E.2. Our analysis is based on SDE approximations near \(\), providing a clean view of how different choices of \(H\) affect the selection of minimizers by Local SGD.

SDE approximations of SGD and Local SGD.SDE is a powerful tool to precisely characterize the effect of noise in SGD, leading to many applications such as Linear Scaling Rule (Goyal et al., 2017). The SDE \((t)=-((t))t+}((t))^{1/2}_{t}\) is conventionally used in the literature (Jastrzebski et al., 2017; Smith et al., 2020; Li et al., 2021b), where \(_{t}\) is the standard Wiener process. In this SDE, each discrete step corresponds to a continuous time interval of length \(\), and the expected gradient and gradient noise become a deterministic drift term and a stochastic diffusion term, respectively. When the training proceeds to a point \((t)\) near a minimizer \(_{0}\) on the manifold \(\), the gradient \(((t))\) is almost zero but the gradient noise \(}((t))^{1/2}_{t}\) drives the parameter to diffuse locally. This can be captured by a careful first-order approximation of the dynamics, leading to an Ornstein-Uhlenbeck process (Zhu et al., 2019; Li et al., 2019; Izmailov et al., 2018). However, these rough approximations only hold for about \((^{-1})\) steps, whereas neural networks in practice are usually trained for much longer.

Recently, a series of works (Blanc et al., 2020; Damian et al., 2021; Li et al., 2021c) study the dynamics of SGD on a _longer_ horizon. They show that higher-order terms can accumulate over time and drive this local diffusion to gradually move on the manifold \(\). Among them, Li et al. (2021c) precisely characterized this with an SDE tracking the _gradient flow projection_ of \((t)\) on \(\), denoted as \(((t))\) (see Definition E.1). Here, \(((t))\) can be thought of as a natural "center" of the local diffusion. This SDE, termed as Slow SDE, tracks the dynamics of SGD over \((^{-2})\) steps, which is much longer than the \((^{-1})\) horizon for conventional SDEs.

To provide a theoretical understanding of why Local SGD generalizes better than SGD, Gu et al. (2023) derived the Slow SDEs for Local SGD using the scaling \(H^{-1}\). By comparing the Slow SDEs, they argued that Local SGD drifts faster to flatter minima than SGD. However, their analysis does not encompass the more aggressive scaling \(H^{-2}\) recommended by our QSR. Recognizing this gap, we derive the Slow SDE for this scaling, enriching the theoretical framework for the generalization behavior of Local SGD. Below, we first present the Slow SDEs for SGD and Local SGD with \(H^{-1}\) and \(H^{-2}\), then we interpret why \(H^{-2}\) may generalize better.

**Definition 3.1** (Slow SDE for SGD, informal, (Li et al., 2021; Gu et al., 2023)).: _Given \(_{0}\), define \((t)\) as the solution to the following SDE with initial condition \((0)=_{0}\):_

\[(t)=P_{}} _{\|}^{}()_{t}}_{}-^{3}()[}_{}()]t}_{}. \]

_Here, \(P_{}\) is a projection operator of differential forms to ensure that taking an infinitesimal step from \(\) remains on the manifold \(\). \(B\) is the total batch size. \(_{\|}()\) and \(}_{}()\) are certain PSD matrices related to gradient noise and Hessian. See Definition E.2 for the full definition._

**Definition 3.2** (Slow SDE for Local SGD with \(H^{-1}\), informal (Gu et al., 2023)).: _Consider the scaling \(H=/\) for some constant \(\). Given \(_{0}\), define \((t)\) as the solution to the following SDE with initial condition \((0)=_{0}\):_

\[(t)=P_{}} {}_{\|}^{}()_{t}}_{}-^{3}()[ }_{}()]t}_{}-^{3}()[ }(;H)]t}_{}, \]

_where \(K\) is the number of workers, \(B,_{\|}()\) and \(}_{}()\) are the same as in Definition 3.1. Here, \(}(;)\) is a PSD matrix depending on gradient noise and Hessian. It scales with \(\) as \(_{ 0}}(;)=\), \(_{+}}(;)=}_{}()\). See Definition E.3 for the full definition._

**Definition 3.3** (Slow SDE for Local SGD with QSR).: _Given \(_{0}\), define \((t)\) as the solution to the following SDE with initial condition \((0)=_{0}\):_

\[(t)=P_{}} _{\|}^{}()_{t}}_{}-^{3}()[ }_{}()]t}_{}, \]

_where \(K,B,_{\|}()\) and \(}_{}()\) are defined in Definitions 3.1 and 3.2._

The following approximation theorem indicates that when the learning rate \(\) and the growth coefficient \(\) for QSR are small, the above Slow SDEs closely track their discrete counterparts. The approximation theorem for QSR is new, and we defer the proof to Appendix E.2.

**Theorem 3.1** (Weak Approximations).: _Let \(T>0\) be a constant and \((t)\) be the solution to one of the above Slow SDEs with the initial condition \((0)=(^{(0)})\). Let \(g()\) be any \(^{4}\)-smooth function._

1. _(Gu et al.,_ 2023_) For SGD, let_ \((t)\) _be the solution to (_3_). Then,_ \(_{0 s}}|[g((_{s}))]- [g((s^{2}))]|=}(^{0.25})\)_._
2. _(Gu et al.,_ 2023_) For Local SGD with_ \(H=/\) _for some constant_ \(\)_, let_ \((t)\) _be the solution to (_4_). Then,_ \(_{0 s}}|[g((^{(s)}))]- [g((sH^{2}))]|=}(^{0.25})\)_._
3. _For Local SGD with_ \(H=()^{2}\)_, where the positive constant_ \(\) _is small but larger than_ \((^{})\) _for all_ \(>0\)_, let_ \((t)\) _be the solution to (_5_). Then,_ \(_{0 s}}|[g((^{(s)}))]- [g((sH^{2}))]|=(^{2})\)_._

_Here, \((\,\,)\) and \(}()\) hide constants that are independent of \(\) and \(\) but can depend on \(g\) and \(T\). \(}()\) also hides log terms._

By comparing the Slow SDEs, we can predict the generalization order for different scaling as \(\)\(>\{^{-1}\}>}\), which we explain in detail below.

Interpretation of the Slow SDEs.We first focus on the Slow SDE for SGD (3). The key component of this Slow SDE is the drift term (b), which comes from higher-order approximations of the aforementioned local diffusion that happens in \((^{-1})\) steps. Viewing \(^{3}()[}_{}()]\) as a semi-gradient of \(^{2}(),}_{}()\) that discards the dependence of \(\) in \(}_{}()\), we can interpret the Slow SDE as a continuous version of a semi-gradient method for reducing \(^{2}(),}_{}()\) on \(\). Since the Hessian matrix \(^{2}()\) determines the local curvature of the loss landscape, we can conclude from the Slow SDE that SGD tends to reduce sharpness and move towards flatter minimizers in \((^{-2})\) steps. Reduced sharpness has been shown to yield better sample complexity bounds in specific theoretical settings. For details, we refer readers to Li et al. (2021).

Now, we turn to the Slow SDE for QSR. Compared with the SDE for SGD, it possesses a \(K\) times larger drift term, leading to much faster sharpness reduction than SGD. An intuitive explanation for why this extra drift arises is as follows. Since the local batch size is \(K\) times smaller than the global one, this local diffusion at each worker is much more significant than that in parallel SGD, thereby leading to an extra drift term in Slow SDE accumulated from higher-order terms.

The case of Local SGD with \(H=/\) is somewhere in between QSR and SGD. Compared with the SDE for SGD, it has an extra drift term (c), where \(\) serves as the knob to control the magnitude of the drift term. For small \(\), \(}()\) diminishes to zero, yielding the same SDE as SGD. By contrast, as \(\) goes to infinity, \(}()\) approximates \(}_{}()\), leading to the Slow SDE for QSR.

Comparison of different scalings.Based on the interpretation, keeping \(H\) constant as \(\) diminishes is equivalent to setting a small \(\) for \(H=/\), making the extra drift term negligible and thus yielding nearly no generalization benefit over SGD. Conversely, the SDE for \(H=/\) converges to the SDE of QSR in the limit \(\), maximizing the drift term. But in practice, \(\) cannot be arbitrarily large. In Theorem 3.3 of Gu et al. (2023), the distance between the iterate and \(\) blows up as \(}()\), suggesting that setting a very large \(\) for a not-so-small \(\) can blow up the loss. Therefore, the generalization performance of \(H\)\(\)\(^{-1}\) is expected to be worse than QSR. In summary, the order of generalization performance predicted by our theory is QSR \(>\{H^{-1}\}>\) [constant \(H\)].

Experimental results in Figure 2 validate that this order of generalization performance for different scalings holds not only for Local SGD but also for Local AdamW. For Local SGD we additionally have {constant \(H\)} \(\) [parallel SGD] since parallel SGD is mathematically equivalent to Local SGD with \(H=1\). Apart from \(H^{-1}\) and \(H^{-2}\), we have also tried a more aggressive scaling, \(H^{-3}\), but it does not provide consistent improvements over QSR. See Appendix G for more discussion.

## 4 Experiments

In this section, we empirically demonstrate that QSR not only improves the test accuracy of local gradient methods but also reduces the wall-clock time of standard data parallel training, with a focus on the ImageNet classification task (Russakovsky et al., 2015). Our experiments include Local SGD on ResNet-152 (He et al., 2016), and Local AdamW on ViT-B with patch size 16x16 (Dosovitskiy et al., 2021). We briefly outline our training configuration below. See Appendix C for full details.

Baselines.For QSR with base synchronization period \(H_{}\), we benchmark their performance against two baselines running the same number of epochs: \(\) Local SGD/AdamW with constant synchronization period \(H=H_{}\), and \(\) parallel SGD/AdamW. When comparing with these baselines, we mainly focus on validating that (a) QSR maintains or sometimes outperforms the communication efficiency of \(\), thus communicating much less than \(\), and (b) QSR improves the generalization performance of \(\), even surpassing \(\) in test accuracy.

Figure 2: Empirical results on Local SGD and Local AdamW validate the generalization performance order predicted by our theory: QSR \(>\{H^{-1}\}>\) [constant \(H\)]. For SGD, we additionally have [constant \(H\)] \(\) [parallel SGD] since the latter is equivalent to Local SGD with \(H=1\). Here, \(\) and \(\) are tuned to maximize the test accuracy of QSR and \(H^{-1}\), respectively.

Comparison with other synchronization strategies.Besides the above two baselines, other potential baselines include 3 Post-local SGD, 4 the scaling of \(H^{-1}\), and 5 large batch training with batch size \(H B\), which we discuss below. 3 is proposed for the same purpose as QSR: to improve communication efficiency and generalization together. However, it is less communication efficient than our QSR because it starts with parallel SGD and sustains this for a significant fraction of the training duration, leading to a limited reduction in communication. Also, as shown by our comparison in Figure 1(a) (also observed in Ortiz et al. 2021), its generalization benefits over SGD appear shortly after switching and diminish in the end. 4 is inspired by Gu et al. (2023) and may also improve generalization while reducing communication, but we have conducted a thorough comparison between QSR and 4 in Figure 2, demonstrating the superiority of QSR. 5 has the same communication efficiency as Local SGD with the same constant \(H\) (4), but it has been observed to have worse test accuracy than parallel SGD/AdamW without scaling up the batch size (4), which we also observe in Table 2. For the above reasons, we mainly compare with baselines 1 and 2.

Hardware.We conduct the experiments on Tencent Cloud, where each machine is equipped with 8 NVIDIA GeForce RTX 3090 GPUs. The machines are interconnected by a 25Gbps network. Since intra-machine communication speed is not substantially faster than inter-machine speed on our specific hardware, we treat _each GPU_ as an independent worker and set the batch size on each GPU as \(B_{}=256\). In this paper, we use _a_x_b GPUs to denote \(a\) machines with \(b\) GPUs each.

Training Setup.Our experiments on ResNet-152 follow the 200-epoch recipe in Foret et al. (2021b) except that we use 5 epochs of linear learning rate warmup. For experiments on ViT-B, we follow the simple and effective 300-epoch recipe proposed in Beyer et al. (2022) with RandAugment and Mixup. We use the cosine decay unless otherwise stated. The hyperparameters (primarily learning rate and weight decay) are optimally tuned for all baselines. We explore \(H_{}=2,4\) for ResNet-152 and \(H_{}=4,8\) for ViT-B. This choice stems from the observation that the communication overhead for ResNet-152 is smaller than ViT-B (see Table 4). To tune the growth coefficient \(\) for QSR, we first fix the learning rate schedule and then search among a few values of \(\). The \(\) values we explore typically allow the training to start with \(H_{}\), maintain \(H=H_{}\) for an initial period to optimize the training loss, and gradually increase \(H\) as \(\) decays in the late phase.

### QSR Improves Generalization

Through experiments spanning various batch sizes and learning rate schedules, in this subsection, we illustrate that QSR consistently enhances the generalization of gradient methods, even outperforming the communication-intensive data parallel approach.

Main results.We first present our main results for batch size \(B=4096\) on 2x8 GPUs, covering Local SGD on ResNet-152 and Local AdamW on ViT-B. As shown in Table 1, QSR significantly improves the validation accuracy of local gradient methods by up to \(0.8\%\) on ResNet-152 and \(1.7\%\) on ViT-B, despite inducing higher training loss. The results support the thesis that the improvement in generalization is due to the implicit regularization of local gradient noise instead of better optimization. Noticeably, QSR surpasses the data parallel approach in validation accuracy by \(0.7\%\) on ResNet-152 and by \(1.1\%\) on ViT-B while cutting the communication volume to less than \(25\%\). As an added benefit of increasing the synchronization interval in line with the decaying learning rate, QSR further reduces communication overhead, even halving the communication volume compared to Local AdamW with a fixed synchronization period on ViT-B.

The advantages of QSR are more pronounced for ViT-B compared to ResNet-152. This is probably because vision transformers are general-purpose architectures with less image-specific inductive bias than CNNs (Dosovitskiy et al., 2021; Chen et al., 2021). As a result, they may benefit more from external regularization effects, such as those induced by adding local steps.

Table 1: QSR enhances the test accuracy of local gradient methods, even outperforming the communication-intensive data parallel approach. The experiments below use batch size 4096. We report the validation accuracy and train loss averaged over 3 runs, along with the standard deviation.

Scaling up the batch size.In Table 2, when scaling the training up to 8x8 GPUs with total batch size \(B=16384\), we observe a drop in test accuracy for both data parallel approach and local gradient methods. This generalization degradation for large batch training, which has been widely observed in the literature (Shallue et al., 2019; Jastrzebski et al., 2017; You et al., 2018), probably arises from a reduced level of gradient noise associated with increased batch size (Keskar et al., 2017; Smith et al., 2021). While the Linear Scaling Rule for SGD (Krizhevsky, 2014; Goyal et al., 2017) and the Square Root Scaling Rule (Malladi et al., 2022; Granziol et al., 2022) for adaptive gradient methods - which increase the learning rate in proportion to the total batch size or its square root - can mitigate this degradation, they cannot fully bridge the gap. In Table 2, the test accuracy drop persists even when we tune the learning rate for all baselines. Applying QSR to local gradient methods can help reduce this generalization gap. It improves the validation accuracy of local gradient methods by up to \(0.6\%\) on ResNet-152 and \(1.5\%\) on ViT-B. This enables local gradient methods to achieve comparable validation accuracy as the data parallel approach on ResNet or outperform it by \(0.8\%\) on ViT while communicating considerably less.

Other learning rate schedules.So far, our experiments are conducted with the cosine learning rate schedule, which is a common choice for training modern deep neural nets (Liu et al., 2021, 2022; Brown et al., 2020). To further validate the efficacy of QSR, we now investigate other popular learning rate schedules, including linear (Li et al., 2020; Izsak et al., 2021; Leclerc et al., 2023) and step decay (He et al., 2016; Huang et al., 2017; Ma et al., 2019). See Figure 4 for a visualization of these schedules. Figure 3 presents the results for Local AdamW on ViT-B with linear decay, where the peak learning rates for baselines are tuned optimally. QSR improves the test accuracy of Local AdamW by a significant margin of \(1.4\%\), even outperforming parallel AdamW by \(0.6\%\) while cutting the communication volume to only \(9.3\%\). The step decay scheduler divides the learning rate by factors such as \(2\) or \(10\) at some specified epochs. Given the absence of standard recipes to determine the decay points in our training setup, we derive a step decay schedule from the cosine decay by rounding its learning rate to powers of \(2\), which is defined as \(_{}(t):=2^{(_{2}_{}(t))}\). As shown in Table 3, QSR exhibits strong generalization performance with this decay schedule, enhancing the test accuracy of local gradient methods by up to \(0.8\%\) on ResNet-152 and \(1.5\%\) on ViT-B. It even surpasses the communication-intensive parallel SGD by \(0.7\%\) on ResNet and parallel AdamW by \(1\%\) on ViT.

### QSR Reduces Wall-clock Time

In addition to improving generalization, our original motivation for adopting local steps is to reduce communication overhead and hence reduce the wall-clock time. In this section, we confirm this

   Method & Val. Acc.(\%) & Comm. (\%) \\   Parallel SGD & 79.20 & 100 \\  Local SGD (\(H\)=2) & 78.67 & 50 \\ + QSR (\(H_{}=2\)) & **79.27** & **42.8** \\  Local SGD (\(H\)=4) & 78.34 & 25 \\ + QSR (\(H_{}=4\)) & **78.65** & **21.9** \\   

Table 2: QSR mitigates the generalization degradation in large-batch training. Here the batch size is \(16384\).

(a) Local SGD on ResNet-152

   Method & Val. Acc.(\%) & Comm. (\%) \\   Parallel SGD & 79.68 & 100 \\   Local SGD (\(H\)=2) & 79.58 & 50 \\ +QSR (\(H_{}=2\)) & **80.40** & **40.3** \\   Local SGD (\(H\)=4) & 79.53 & 25 \\ +QSR (\(H_{}=4\)) & **80.11** & **20.5** \\   

Table 3: QSR also exhibits strong generalization performance on the step-decay learning rate schedule.

(a) Local SGD on ResNet-152.

Figure 3: For linear decay, QSR improves the test accuracy of Local AdamW on ViT-B, even outperforming the communication-intensive parallel AdamW.

for training with 2x8 and 8x8 GPUs, as shown in Table 4. See also Appendix F for our method of measuring the communication time. In our setup, scaling the training from 2x8 to 8x8 GPUs increases the communication overhead for both models. Notably, on 8x8 GPUs, communication accounts for almost half of the total training time for ViT-B. Since communication makes up a larger portion of the total time for ViT-B compared to ResNet-152, the speedup from QSR is more significant on ViT-B: the time is cut from 26.7 to 20.2 hours on 2x8 GPUs, and 8.6 to 5.5 hours on 8x8 GPUs. As discussed in Section 4.1, compared to the constant period local gradient method, QSR further reduces the communication cost by increasing the synchronization period in the late phase. For example, applying QSR to Local AdamW with \(H=4\) further reduces the time by 1 hour for ViT training on 2x8 GPUs.

Discussion on the choice of \(H_{ base}\).As elaborated in Section 2, \(H_{ base}\) indicates the minimum synchronization period and should be determined based on the communication overhead. For ResNet-152, given that communication only accounts for 3.3 out of 20.7 hours on 2x8 GPUs and 1.3 out of 5.7 hours on 8x8 GPUs, setting \(H_{ base}\) as \(2\) or \(4\) suffices to reduce the communication time to an inconsequential amount. By contrast, the communication overhead for ViT-B is more prominent, motivating us to consider larger values of \(H_{ base}\), such as 4 and 8. As shown in Tables 1 and 2, \(H_{ base}\) introduces a tradeoff between communication efficiency and final test accuracy. For instance, when training ResNet-152 with batch size 16384, one can either choose \(H_{ base}=2\) to achieve comparable test accuracy as parallel SGD, or \(H_{ base}=4\) to further halve the communication volume at the expense of a \(0.6\%\) drop in test accuracy. One probable explanation for this accuracy drop for larger \(H_{ base}\) can be worse optimization in the early training phase, where the learning rate is large.

## 5 Discussions and Future Directions

This paper primarily focuses on relatively large models trained with long horizons, and proposes the Quadratic Synchronization Rule (QSR). As validated by our experiments, QSR effectively improves test accuracy and communication efficiency simultaneously for training large vision models (ResNet-152 and ViT-B) with quite a few hundred epochs. However, on the downside, for smaller models trained with shorter horizons, QSR may not consistently deliver noticeable generalization improvements (see Table 5). Nonetheless, training in this regime is not costly, either, making it less of a critical concern. Another limitation of our work is that the effectiveness of QSR relies on the implicit regularization effects of noise, but when training large models with unsupervised learning on massive data, regularization techniques might not be necessary to bridge the gap between the training and population loss (Vyas et al., 2023). Still, recent work (Liu et al., 2023) has found that the same pretraining loss can lead to different internal representations and thus different downstream performances. We leave it to future work to explore and design communication-efficient methods for unsupervised learning, particularly language model pretraining, that improve models' transferability to downstream tasks.

Table 4: QSR reduces the wall-clock time of data parallel training. The following tables present wall-clock time for the entire training process on 2x8 GPUs and 8x8 GPUs, with batch sizes 4096 and 16384, respectively. We highlight the wall-clock time of QSR when it matches or outperforms the data parallel baseline in test accuracy. “Ratio” represents communication time divided by total time, reflecting the communication overhead. We also include local gradient methods with a constant synchronization period for reference. (a) ResNet-152 (200 epochs) on 2x8 GPUs (b) ViT-B (300 epochs) on 2x8 GPUs