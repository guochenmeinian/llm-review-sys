# Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement

Zhi Wang\({}^{1}\) Li Zhang\({}^{1}\) Wenhao Wu\({}^{1}\) Yuanheng Zhu\({}^{2}\) Dongbin Zhao\({}^{2}\) Chunlin Chen\({}^{1}\)

\({}^{1}\)Nanjing University \({}^{2}\)Institution of Automation, Chinese Academy of Sciences

{zhiwang, clchen}@nju.edu.cn {lizhang, whao_wu}@smail.nju.edu.cn

{yuanheng.zhu, dongbin.zhao}@ia.ac.cn

Correspondence to Chunlin Chen <clchen@nju.edu.cn>.

###### Abstract

A longstanding goal of artificial general intelligence is highly capable generalists that can learn from diverse experiences and generalize to unseen tasks. The language and vision communities have seen remarkable progress toward this trend by scaling up transformer-based models trained on massive datasets, while reinforcement learning (RL) agents still suffer from poor generalization capacity under such paradigms. To tackle this challenge, we propose Meta Decision Transformer (Meta-DT), which leverages the sequential modeling ability of the transformer architecture and robust task representation learning via world model disentanglement to achieve efficient generalization in offline meta-RL. We pretrain a context-aware world model to learn a compact task representation, and inject it as a contextual condition to the causal transformer to guide task-oriented sequence generation. Then, we subtly utilize history trajectories generated by the meta-policy as a self-guided prompt to exploit the architectural inductive bias. We select the trajectory segment that yields the largest prediction error on the pretrained world model to construct the prompt, aiming to encode task-specific information complementary to the world model maximally. Notably, the proposed framework eliminates the requirement of any expert demonstration or domain knowledge at test time. Experimental results on MuJoCo and Meta-World benchmarks across various dataset types show that Meta-DT exhibits superior few and zero-shot generalization capacity compared to strong baselines while being more practical with fewer prerequisites. Our code is available at https://github.com/NJU-RL/Meta-DT.

## 1 Introduction

Building generalist models that solve various tasks by training on vast task-agnostic datasets has emerged as a dominant paradigm in the language  and vision  communities. Offline reinforcement learning (RL)  allows learning an optimal policy from trajectories collected by some behavior policies without access to the environments, which holds tremendous promise for turning massive datasets into powerful generic decision-making engines , akin to the rise of large language models (LLMs) like GPT . A performant example is the decision transformer (DT)  that leverages the transformer's strong sequence modeling capability for trajectory data and achieves promising results on offline RL .

Transformer-based large models have shown remarkable scaling properties and high transferability across various domains, including language modeling , computer vision , and image generation . However, their counterparts in RL are usually specialized to a narrowly defined task and struggle with poor generalization to unseen tasks, due to distribution shift and the lack ofself-supervised pretraining techniques [18; 19; 20]. To promote generalization, Prompt-DT  uses expert demonstrations as a high-quality prompt to encode task-specific information. Generalized DT  conditions on some hindsight information, e.g., statistics of the reward distribution, to match task-specific feature distributions. These works usually rely on domain knowledge at test time, e.g., expert demonstrations or hindsight statistics, which is expensive and even infeasible to acquire in advance for unseen tasks . The aforementioned limitations raise a key question: _Can we design an offline meta-RL framework to achieve efficient generalization to unseen tasks while drawing upon advances in the sequence modeling paradigm with the scalable transformer architecture?_

In offline RL, the collected dataset depends on the task and behavior policy. When behavior policies are highly correlated with tasks in the training dataset, the agent tends to memorize the features of behavior policies and produce biased task inference at test time due to the change of behavior policies [24; 25]. One major challenge for generalization is how to accurately encode task-relevant information for extrapolating knowledge across tasks, while minimizing the requirement on the distribution of pre-collected data and behavior policies. RL agents typically learn through active interaction with the environment, and the transition dynamics \(p(r,s^{}|s,a)\) completely describes the characteristics of the underlying environment . The environment dynamics, also called _world model_[27; 28], is intrinsically invariant to behavior policies or collected datasets, thus presenting a promising alternative for robustly encoding task beliefs. By capturing compact representations of the environment, world models carry the potential for substantial transfer between tasks , continual learning , and generalization from offline datasets [31; 32].

Inspired by this, we propose a novel framework for offline meta-RL, named Meta-DT that leverages robust task representation learning via world model disentanglement to conduct task-oriented sequence generation for efficient generalization. First, we pretrain a context-aware world model to capture task-relevant information from the multi-task offline dataset. The world model contains an encoder that abstracts dynamics-specific information into a compact task representation, and a decoder that predicts the reward and state transition functions conditioned on that representation. Second, we inject the task representation as a contextual label to the transformer to guide task-conditioned sequence generation. In this way, the autoregressive model learns to estimate the conditional output of multi-task distributions and achieve desired returns based on the task-oriented context. Finally, we leverage the past trajectories generated by the meta-policy as a self-guided prompt to exploit the architecture inductive bias, akin to Prompt-DT . We feed available trajectory segments to the pretrained world model and choose the segment with the largest prediction error to construct the prompt, aiming to encode task-specific information _complementary_ to the world model maximally.

In summary, our main contributions are as follows:

* **Generalization Ability.** We propose Meta-DT to leverage the conditional sequence modeling paradigm with the scalable transformer architecture to achieve efficient generalization across unseen tasks without any expert demonstration or domain knowledge at test time.
* **Context-Aware World Model.** We introduce a context-aware world model to learn a compact task representation capable of generalizing across a distribution of varying environment dynamics.
* **Complementary Prompt.** We design a self-guided prompt that encodes task-specific information complementary to the world model maximally, harnessing the architectural inductive bias.
* **Superior Performance.** Experiments on various benchmarks show that Meta-DT exhibits higher few and zero-shot generalization capacity, while being more practical with fewer prerequisites.

## 2 Related Work

**Offline Meta-RL.** Offline RL  allows learning optimal policies from pre-collected offline datasets without online interactions with the environment [34; 35]. Offline meta-RL learns to generalize to new tasks via training on a distribution of such offline tasks [25; 36]. Optimization-based meta-learning methods [37; 38] seek policy parameter initialization that requires only a few adaptation steps to new tasks. MACAW  introduces this architecture into value-based RL subroutines, and uses simple supervised regression objectives for sample-efficient offline meta-training. On the other hand, context-based meta-learning methods learn a context encoder to perform approximate inference on task representations, and condition the meta-policy on the approximate belief for generalization, such as PEARL , VariBAD , FOCAL , COROO , CSRO , and UNICORN . These works extended from the online meta-RL setting are mainly trained by temporal difference learning, the dominant paradigm in RL [9; 44]. This paradigm might be prone to instabilities due to function approximation, off-policy learning, and bootstrapping, together known as the deadly triad . Moreover, many of these works rely on hand-designed heuristics to keep the policy within the offline dataset distribution . This motivates us to turn to harness the lens of conditional sequence modeling with the transformer architecture to scale existing offline meta-RL algorithms.

**RL as Sequence Modeling.** The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws with the transformer architecture, such as GPT , ViT , and DiT . Decision transformer  first introduces the transformer's sequence modeling capacity to solving offline RL without temporal difference learning, which autoregressively outputs optimal actions conditioned on desired returns. As a concurrent study, trajectory transformer  directly models distributions over sequences of states, actions, and rewards, followed by beam search as a planning algorithm in a model-based manner. Extending the LLM-like structure to RL, this paradigm activates a new pathway toward scaling powerful RL engines with large-scale compute and data .

For multi-task learning, Gato  trains a multi-modal generalist policy in the GPT style with a decoder-only transformer. Multi-game DT  trains a suite of up to 46 Atari games simultaneously, and controls policy generation at inference time with a pretrained classifier that indicates the expert level of behaviors. For generalization to unseen tasks, Prompt-DT  exploits a prefix prompt architecture and Generalized DT  designs a hindsight information matching structure to encoder task-specific information. These works usually require domain knowledge at test time, such as expert demonstrations or hindsight statistics. T\({}^{3}\)GDT  conditions action generation on three-tier guidance of global transitions, local relevant experiences, and vectorized time embedding. CMT  provides a pretrain and prompt-tuning paradigm where a task prompt is extracted from offline trajectories with an encoder-only transformer to realize few-shot generalization.

## 3 Preliminaries

### Offline Meta Reinforcement Learning

RL is generally formulated as a Markov decision process (MDP) \(M{=},,T,R,\), where \(/\) is the state/action space, \(T/R\) is the state transition/reward function, and \(\) is the discount. The objective is to find a policy \((a|s)\) to maximize the expected return as \(_{}J_{M}()\!=\!_{}[_{t=0}^{}^{t}R (s_{t},a_{t})]\).

In offline meta-RL, we assume that the task follows a distribution \(M_{i}=,,T_{i},R_{i}, P(M)\), where tasks share the same state-action spaces while vary in the reward and state transition functions, i.e., environment dynamics. For each task \(i\) from a total of \(N\) training tasks \(\{M_{i}\}_{i=1}^{N}\), an offline dataset \(_{i}\!=\!\{(s_{i,j},a_{i,j},r_{i,j},s^{}_{i,j})\}_{j=1}^{J}\) is collected by an arbitrary behavior policy \(^{i}_{}\). The agent can only access the offline datasets \(\{_{i}\}_{i=1}^{N}\) to train a meta-policy \(_{}\). At test time with the _few-shot_ setting, given an unseen task \(M\!\!P(M)\), the agent can access a small dataset \(\!=\!\{(s_{j},a_{j},r_{j},s^{}_{j})\}_{j=1}^{J^{}}\) to construct the task prompt or infer the task belief before policy evaluation. For the _zero-shot_ setting, the trained meta-policy is directly evaluated on the unseen task by interacting with the test environment to estimate the expected return. The objective is to learn a meta-policy to maximize the expected episodic return over test tasks as \(J(_{})=_{M P(M)}[J_{M}(_{})]\).

### Decision Transformer

By leveraging the superiority of the attention mechanism  in extracting dependencies between sequences, transformers have gained substantial popularity in the language and vision communities [7; 14]. Decision transformer  introduces associated advances to solve RL problems and re-frames offline RL as a return-conditioned sequential modeling problem, inheriting the transformer's efficiency and scalability when modeling long trajectory data. DT models the probability of the next sequence token \(x_{t}\) conditioned on all tokens prior to it: \(P_{}(x_{t}|x_{<t})\), similar to contemporary decoder-only sequence models . The sequence we consider has the form: \(x=(,_{t},s_{t},a_{t},)\), where \(_{t}\) is the agent's target return for the rest of the trajectory. Such a sequence order respects the causal structure of the decision process. When training with offline data, \(_{t}\!=\!_{i=t}^{T}r_{i}\), and during testing, \(_{t}\!=\!G^{*}\!-\!_{i=0}^{t}r_{i}\), where where \(G^{*}\) is the target return for an entire episode. The same timestep embedding is concatenated to the embeddings of \(_{t}\), \(s_{t}\), and \(a_{t}\), and the head corresponding to the state token is trained to predict the action by minimizing its error from the ground truth.

## 4 Method

In this section, we present Meta Decision Transformer (Meta-DT), a novel offline meta-RL framework that draws upon advances in conditional sequence modeling with robust task representation learning and a complementary prompt design for efficient generalization across unseen tasks. Fig. 1 illustrates the method overview, and the following subsections present the detailed implementations.

### Context-Aware World Model

The key to efficient generalization is how to accurately encode task-relevant information for extrapolating knowledge across tasks, which proves more challenging in RL. A supervised or unsupervised learner is _passively_ presented with a fixed dataset and aims to recognize the pattern within that dataset. In contrast, RL typically learns from _active_ interactions with the environment, and aims at an optimal policy that receives the maximal return in the environment, rather than only recognizing the pattern within the pre-collected dataset. Since the offline dataset depends on both the task and the behavior policy, the task information could be _entangled_ with the features of behavior policies, thus producing biased task inference at test time due to the shift in behavior policies. Taking 2D navigation as an example, tasks differ in goals and the behavior policy is going towards the goal for each task. The algorithm can easily distinguish tasks based on state-action distributions rather than reward functions, leading to extrapolating errors when the behavior policy shifts to random exploration during testing.

To address this intrinsic challenge, we ought to _disentangle_ task-relevant information from behavior policies. The transition dynamics, i.e., the reward and state transition functions \(p(s^{},r|s,a)\), completely describes the characteristics of the underlying environment. Naturally, this environment dynamics, also called _world model_, keeps invariant to behavior policies or collected datasets, and could be a promising alternative for accurately inferring task beliefs. Therefore, we introduce a context-aware world model to learn robust task representations that can generalize to unseen environments with varying transition dynamics.

In the typical meta-learning setting, the reward and state transition functions that are unique to each MDP are unknown, but also share some common structure across the task distribution \(P(M)\). There exists a true variable that represents either a task description or task identity, but we do not have access to this information. Hence, we use a latent representation \(z\) to approximate that variable. For a given task \(M_{i}\), the reward and state transition functions can be approximated by a generalized context-aware world model \(W\) that is shared across tasks as

\[W_{i}(r_{t},s_{t+1}|s_{t},a_{t}) W(r_{t},s_{t+1}|s_{t},a_{t};z_{t}^{i}).\] (1)

Figure 1: The overview of Meta-DT. We pretrain a context-aware world model to accurately disentangle task-specific information. It contains a context encoder \(E_{}\) that abstracts recent \(h\)-step history \(_{t}^{i}\) into a compact task representation \(z_{t}^{i}\), and the generalized decoders (\(R_{},T_{}\)) that predict the reward and next state conditioned on \(z_{t}^{i}\). Then, the inferred task representation is injected as a contextual condition to the causal transformer to guide task-oriented sequence generation. Finally, we design a self-guided prompt from history trajectories generated by the meta-policy at test time. We select the trajectory segment that yields the largest prediction error on the pretrained world model, aiming to encode task-relevant information complementary to the world model maximally.

Since we do not have access to the true task description or identity, we need to infer task representation \(z_{t}^{i}\) given the agent's \(h\)-step experience up to timestep \(t\) collected in task \(M_{i}\) as

\[_{t}^{i}=(s_{t-h},a_{t-h},r_{t-h},...,s_{t-1},a_{t-1},r_{t-1},s_{t}).\] (2)

The intuition is that the true task belief of the underlying MDP can be abstracted from the agent's recent experiences, analogous to recent studies in meta-RL .

We separate the reasoning about the world model into two parts: i) encoding the dynamics-specific information into a latent task representation, and ii) decoding the environment dynamics conditioned on that representation. First, we use a simple yet effective context encoder \(E_{}\) to embed recent experiences into a task representation as \(z_{t}^{i}\!=\!E_{}(_{t}^{i})\). Second, the decoder contains a generalized reward model \(R_{}\) and state transition model \(T_{}\). The task representation is augmented into the input of both models to predict the instant reward \(_{t+1}\!=\!R_{}(s_{t},a_{t};z_{t}^{i})\) and next state \(_{t+1}\!=\!T_{}(s_{t},a_{t};z_{t}^{i})\). Under the assumption that tasks with similar contexts will behave similarily , the proposed world model can extrapolate the meta-level knowledge across tasks by accurately capturing task-relevant information from training environments. The context encoder is jointly trained by minimizing the reward and state transition prediction error conditioned on the task representation as

\[(,,)=_{_{t}^{i}_{i}} _{z_{t}^{i} E_{}(_{t}^{i})}r_{t+1}- R_{}(s_{t},a_{t};z_{t}^{i})^{2}+s_{t+1}-T_{}(s_{t},a_{t};z_{ t}^{i})^{2}.\] (3)

### Complementary Prompt

Recent works suggest the prompt framework as an effective paradigm for pretraining transformer-based large models on massive datasets and adapting them to new scenarios with few or no labeled data . Prompt-DT  adopts that paradigm to RL problems by prepending a task prompt to the DT's input. At test time, the agent is assumed to access a handful of expert demonstrations to construct the prompt and perform few-shot generalization to new tasks. However, in real-world scenarios, it is expensive and even infeasible to acquire such domain knowledge as expert demonstrations in advance for unseen tasks. Especially in RL, agents typically learn from interacting with an initially unknown environment. Recent studies  also suggest that the quality of demonstrations must be high enough to act as a well-constructed prompt. Otherwise, the performance of Prompt-DT may degrade since some medium or random data can easily disrupt the abstraction of task-specific information.

Here, we design a self-guided prompt to achieve more realistic generalization at test time. In the ideal case where the testing policy is optimal, past experiences during evaluation can act as high-quality demonstrations to construct the prompt. Motivated by this, we leverage history trajectories generated by the meta-policy during evaluation as a self-guided prompt to enjoy the power of architecture inductive bias, while eliminating the dependency on expensive domain knowledge. Though, the meta-policy may generate medium or even inferior data during initial training. As meta-learning proceeds, the policy can exhibit increasing generalization capacity via the context-aware world model, thus generating trajectories that gradually approach expert demonstrations.

Both the world model (algorithmic perspective) and the self-guided prompt (architecture perspective) aim to extract task-specific information to guide policy generation across tasks. To facilitate collaboration between these two parts, we force the prompt to maximally complement the world model towards the same goal. We feed all available segments selected from history trajectories to the pretrained world model, and use the segment with the largest prediction error to construct the prompt. In this way, the _complementary_ prompt attempts to encode the portion of task-relevant information that the world model struggles to capture effectively.

Formally, the complementary prompt is a sequence segment containing multiple tuples, (\(^{*},s^{*},a^{*}\)), which are sampled from the agent's history trajectories. Each element with superscript \(*\) is associated with the prompt. For a given task \(M_{i}\), we obtain a \(k\)-step prompt \(_{i}^{*}\) as

\[&_{i}^{*}=(_{j}^{*},s_{j}^{*},a_{j}^{*},_{j+1}^{*},s_{j+1}^{*},a_{j+1}^{*},...,_{j+k}^{*},s_{j+k}^{*},a _{j+k}^{*}),\\ & j=_{j}_{t=j}^{j+k}[ r_{t+1}-R_{}(s_{t},a_{t};z_{t}^{i})^{2}+s_{t+1}-T_{ }(s_{t},a_{t};z_{t}^{i})^{2}].\] (4)

Following Prompt-DT, we choose a much smaller value of \(k\) than the task horizon, ensuring that the prompt only contains the information needed to help identify the task but insufficient information for the agent to imitate. Compared to the world model that _explicitly_ learns the reward and transition functions, the complementary prompt can store partial information to _implicitly_ capture task dynamics.

### Meta-DT Architecture

Our Meta-DT architecture is built on decision transformer  and solves the offline meta-RL problem through the lens of conditional sequence modeling with robust task representation learning. We first pretrain the context-aware world model and keep it fixed to produce compact task representations for the downstream meta-training process. For each task \(M_{i}\), we use the context encoder from the pretrained world model to infer the contextual information for each timestep as \(z^{i}_{t}=E_{}(^{i}_{t})\). Then, the input of Meta-DT consists of two parts: i) the \(k\)-step complementary prompt \(^{*}_{i}\) derived from (4), and ii) the most recent \(K\)-step history \(^{+}_{i}\) augmented by the learned task representations as

\[^{+}_{i}=(z^{i}_{t-K+1},_{t-K+1},s_{t-K+1},a_{t-K+1},...,z^{i}_{t}, _{t},s_{t},a_{t})\] (5)

The input sequence \((^{*}_{i},^{+}_{i})\) corresponds to \(3k+4K\) tokens in the transformer, and Meta-DT autoregressively outputs \(k+K\) actions at heads corresponding to state tokens in the input sequence. During training, we construct the prompt from the top few trajectories that obtain the highest returns in the offline dataset. Meta-DT minimizes errors between the predicted and real actions in the data for the \(K\)-step history. At test time with a _few-shot_ setting, the meta-policy is allowed to interact with the environment for a few episodes, and Meta-DT utilizes these history interactions to construct the self-guided prompt. For the _zero-shot_ setting, we ablate the prompt component and directly evaluate Meta-DT on test tasks. Corresponding algorithm pseudocodes are given in Appendix A.

## 5 Experiments

We comprehensively evaluate the generalization capacity of Meta-DT on popular benchmarking domains across various dataset types. In general, we aim to answer the following questions:

* Can Meta-DT achieve consistent performance gain on the few and zero-shot generalization to unseen tasks compared with other strong baselines? (Secs. 5.1 and 5.2)
* How do the context-aware world model, the self-guided prompt design, and the complementary prompt construction affect the generalization performance, respectively? (Sec. 5.3)
* Is Meta-DT robust to the quality of offline datasets? (Sec. 5.4)

**Environments.** We evaluate all tested methods on three classical benchmarks in meta-RL: i) the 2D navigation environment Point-Robot; ii) the multi-task MuJoCo control [55; 36], containing Cheetah-Vel, Cheetah-Dir, Ant-Dir, Hopper-Param, and Walker-Param; and iii) the Meta-World manipulation platform , including Reach, Sweep, and Door-Lock. For each environment, we randomly sample a distribution of tasks and divide them into the training set \(^{}\) and test set \(^{}\). On each training task, we use SAC  to train a single-task policy independently for collecting datasets. We consider three ways to construct offline datasets: Medium, Mixed, and Expert. More details about environments and datasets are given in Appendix B.

**Baselines.** We compare Meta-DT to four competitive baselines that cover two distinctive paradigms in offline meta-RL: the DT-based 1) Prompt-DT, 2) Generalized DT, and the temporal difference-based 3) CORRO, 4) CSRO. More details about baselines are given in Appendix C.

All evaluated methods are carried out with \(5\) different random seeds, and the mean of the received return is plotted with \(95\%\) bootstrapped confidence intervals of the mean (shaded). The standard errors are presented for numerical results. Appendix D gives implementation details of Meta-DT. Appendix E presents hyperparameter analysis on the context horizon \(h\), the prompt length \(k\), and the number of training tasks. Appendix F shows experimental results on Meta-World domains.

### Few-shot Generalization Performance

Note that these baselines work under the few-shot setting, since they require expert demonstrations as task prompts or some warm-start data to infer the task representation. We first compare Meta-DT to them under an aligned few-shot setting, where each method can leverage a fixed number of trajectories for task inference. Fig. 2 and Table 1 present the testing curves and converged performance of Meta-DT and baselines on various domains using Medium datasets under an aligned few-shot setting. In these environments with varying reward functions or state transition dynamics, Meta-DT consistently obtains superior performance regarding data efficiency and final asymptoticresults. A noteworthy point is that Meta-DT outperforms baselines by a larger margin in relatively complex environments like Ant-Dir, which demonstrates the high generalization capacity of Meta-DT when tackling challenging problems. Moreover, Meta-DT generally exhibits a lower variance during learning, signifying not only superior learning efficiency but also enhanced training stability.

### Zero-shot Generalization Performance

Since Meta-DT can derive real-time task representations \(z_{t}\) from its \(h\)-step experience \(_{t}\) via the pre-trained context encoder, it can achieve zero-shot policy adaptation to unseen tasks. To demonstrate its zero-shot generalization ability, we ablate the prompt component and directly evaluate Meta-DT on test tasks. For a fair comparison, we modify the baselines to an aligned zero-shot setting, where prompt or warm-start data is inaccessible before policy evaluation. All methods can only use samples generated by the trained meta-policy during evaluation to construct task prompts (Prompt-DT), calculate hindsight statistics (Generalized DT), or infer task representations (CORRO and CSRO).

Fig. 3 and Table 2 present the testing curves and converged performance of Meta-DT and baselines on various domains using Medium datasets under an aligned zero-shot setting. Unsurprisingly, most baselines exhibit a collapsed performance (about 20%-35% drop on average) compared to their few-shot counterparts. The main reason is that they usually require expert demonstrations as task prompts or some high-quality warmup data to accurately capture task information. In contrast, Meta-DT can abstract compact task presentations via the context-aware world model, and its performance in zero-shot scenarios drops merely a little (about 5% on average) compared to the few-shot counterpart. The superiority of Meta-DT over all baselines is more pronounced when deployed to zero-shot adaptation scenarios. In addition to its enhanced generalization capacity, this result also demonstrates the superior practicability of Meta-DT with fewer prerequisites in real-world applications.

   Environment & Prompt-DT & Generalized DT & CORO & CSRO & Meta-DT \\  Point-Robot & -12.58 \(\) 0.27 & -14.97 \(\) 0.43 & -14.51 \(\) 1.65 & -16.39 \(\) 2.95 & **-10.18**\(\) 0.18 \\ Cheetah-Vel & -135.89 \(\) 19.91 & -123.47 \(\) 6.88 & -154.12 \(\) 28.83 & -111.83 \(\) 9.16 & **-99.28**\(\) 3.96 \\ Cheetah-Dir & 590.26 \(\) 7.28 & 572.94 \(\) 26.02 & 566.08 \(\) 96.16 & 556.97 \(\) 54.59 & **608.17**\(\) 4.18 \\ Ant-Dir & 287.23 \(\) 29.51 & 268.53 \(\) 12.20 & 246.75 \(\) 49.33 & 251.66 \(\) 30.01 & **412.00**\(\) 11.53 \\ Hopper-Param & 309.90 \(\) 5.74 & 320.77 \(\) 12.82 & 332.37 \(\) 12.90 & 332.84 \(\) 11.23 & **348.20**\(\) 3.21 \\ Walker-Param & 357.30 \(\) 18.34 & 368.07 \(\) 13.79 & 368.02 \(\) 41.77 & 388.08 \(\) 24.92 & **405.12**\(\) 11.11 \\   

Table 1: Few-shot test returns of Meta-DT against baselines using Medium datasets.

Figure 2: The received return curves averaged over test tasks of Meta-DT and baselines using Medium datasets under an aligned few-shot setting.

### Ablation Study

To test the respective contributions of each component, we compare Meta-DT to four ablations: i) w/o_context, it only removes the task representation \(z^{i}_{t}\) from the input of the casual transformer; ii) w/o_com, it randomly chooses a segment from a candidate trajectory to construct the prompt, i.e., removing the complementary way for constructing the prompt; iii) w/o_prompt, it directly removes the prompt component; iv) DT, it removes all components and degrades to the original DT. For ablations, all other structural modules are kept consistent strictly with the full Meta-DT.

Fig. 4 and Table 3 show ablation results on representative domains using Medium datasets. First, Meta-DT obtains a decreased performance when any component is removed, which demonstrates that all components are essential for Meta-DT's capability and they complement each other. Second, ablating the context incurs a more significant performance drop than ablating the complementary way or the whole prompt. It indicates that task representation learning via the world model plays a more vital role in capturing task-relevant information. Third, the performance order of w/o_prompt < w/o_com < Meta-DT shows that using the self-guided prompt can improve the performance, and leveraging the complementary way to construct the prompt can further achieve a performance gain. Another interesting point is that the performance gain achieved by the self-guided prompt and the complementary way is more significant in complex environments like Ant-Dir than in simpler ones like Point-Robot. It again verifies our superiority in tackling challenging tasks.

### Robustness to the Quality of Offline Datasets

A good offline algorithm ought to be robust to different types of datasets that involve a wide range of behavioral policies . To test how Meta-DT performs with data of different qualities, we also conduct experiments on the Mixed and Expert datasets. Figs. 5-6 present test return curves of

   Environment & Prompt-DT & Generalized DT & CORRO & CSRO & Meta-DT \\  Point-Robot & -17.3\(\,\,\)0.2 (\(\,\)38\(\%\)) & -16.9\(\,\,\)0.1 (\(\)13\(\%\)) & -16.3\(\,\,\)1.3 (\(\)12\(\%\)) & -23.0\(\,\,\)2.5 (\(\)40\(\%\)) & **-10.7\(\,\,\)**0.3 (\(\)5\(\%\)) \\ Cheetah-Vel & -148.3\(\,\,\)12.7 (\(\)9\(\%\)) & -218.4\(\,\,\)15.8 (\(\)7\(\%\)) & -165.1\(\,\,\)97.4 (\(\)7\(\%\)) & -140.4\(\,\,\)18.3 (\(\)26\(\%\)) & **-100.1\(\,\,\)**2.8 (\(\)1\(\%\)) \\ Cheetah-Dir & 212.4\(\,\,\)185.2 (\(\)64\(\%\)) & 129.2\(\,\,\)106.5 (\(\)17\(\%\)) & 146.4\(\,\,\)169.0 (\(\)74\(\%\)) & 200.9\(\,\,\)97.0 (\(\)64\(\%\)) & **542.4\(\,\,\)**13.8 (\(\)11\(\%\)) \\ Ant-Dir & 150.6\(\,\,\)5.4 (\(\)48\(\%\)) & 162.2\(\,\,\)22.4 (\(\)40\(\%\)) & 214.6\(\,\,\)11.0 (\(\)13\(\%\)) & 235.5\(\,\,\)13.3 (\(\)6\(\%\)) & **368.9\(\,\,\)**10.6 (\(\)10\(\%\)) \\ Hopper-Param & 301.7\(\,\,\)12.7 (\(\)39\(\%\)) & 318.7\(\,\,\)13.4 (\(\)11\(\%\)) & 320.0\(\,\,\)20.0 (\(\)4\(\%\)) & 329.9\(\,\,\)6.9 (\(\)1\(\%\)) & **342.0\(\,\,\)**7.4 (\(\)2\(\%\)) \\ Walker-Param & 317.9\(\,\,\)32.3 (\(\)11\(\%\)) & 355.2\(\,\,\)14.0 (\(\)4\(\%\)) & 342.4\(\,\,\)31.1 (\(\)7\(\%\)) & 347.9\(\,\,\)12.1 (\(\)10\(\%\)) & **397.4\(\,\,\)**3.8 (\(\)2\(\%\)) \\   

Table 2: Zero-shot test returns of Meta-DT against baselines using Medium datasets. The \(\) denotes the performance drop compared to the few-shot setting.

Figure 3: The received return curves averaged over test tasks of Meta-DT and baselines using Medium datasets under an aligned zero-shot setting.

Meta-DT and baselines with a few-shot setting, and Table 4 shows corresponding converged results. In Mixed datasets, Meta-DT still outperforms all baselines by a large margin, especially in complex environments like Ant-Dir and Walker-Param.

In Expert datasets, Meta-DT exhibits superior generalization capacity than the three baselines of Generalized DT, CORRO, and CSRO. Compared to Prompt-DT, Meta-DT obtains significantly better performance in Point-Robot, Cheetah-Vel, and Ant-Dir, and obtains comparable performance in the other three environments. This phenomenon is because Prompt-DT is sensitive to the quality of prompt demonstrations. The performance can drop a lot when provided with prompts from Medium or Mixed datasets, also mentioned in the original paper  and subsequent studies . Hence, Prompt-DT can achieve satisfactory performance only when expert demonstrations are available at test time. When tackling offline tasks with lower data qualities, Prompt-DT would induce a significant performance gap compared to Meta-DT. In summary, the above results verify that our method is robust to the dataset quality and is more practical with fewer prerequisites in real-world scenarios.

Figure 4: Test return curves of Meta-DT ablations using Medium datasets. w/o_context removes task representation, w/o_com removes the complementary way, and w/o_prompt removes the prompt.

Figure 5: Few-shot test curves of Meta-DT and baselines using Mixed datasets.

   Environment & w/o\_context & w/o\_com & w/o\_prompt & DT & Meta-DT \\  Point-Robot & -16.44\(\)1.93 & -10.32\(\)0.15 & -10.61\(\)0.15 & -17.04\(\)0.17 & **-10.18\(\)**0.18 \\ Cheetah-Dir & 551.30\(\)55.04 & 580.34\(\)30.84 & 542.44\(\)13.84 & 473.75\(\)104.09 & **608.18\(\)**4.18 \\ Ant-Dir & 290.44\(\)54.36 & 390.70\(\)11.23 & 368.94\(\)10.56 & 143.64\(\)15.36 & **412.00\(\)**11.53 \\   

Table 3: Test returns of Meta-DT ablations using Medium datasets.

## 6 Conclusions, Limitations, and Future Work

In the paper, we tackle the offline meta-RL challenge via leveraging advances in sequence modeling with scalable transformers, marking a step toward developing highly capable generalists akin to those in the language and vision communities. Improvements in few and zero-shot generalization capacities highlight the potential impact of our robust task representation learning and self-guided complementary prompt design. Without requiring any expert demonstration or domain knowledge at test time, our method exhibits superior practicability with fewer prerequisites in real-world scenarios.

Though, our method requires a two-phase process of pre-training the world model and training the causal transformer. A promising improvement is to develop a unified framework that simultaneously abstracts the task representation and learns the meta-policy, akin to in-context learning . Also, our generalist model is trained on relatively lightweight datasets compared to popular large models. A crucial future step is to deploy our model on significantly larger datasets, unlocking the scaling law with the transformer architecture. This also aligns with the urgent trend that RL practitioners are striving to break through. Another interesting line is to leverage self-supervised learning [15; 60] to facilitate task representation learning at scale. We leave these as future work.

  
**Mixed** & Prompt-DT & Generalized DT & CORRO & CSRO & Meta-DT \\  Point-Robot & -15.31\(\,\,\)1.52 & -15.10\(\,\,\)0.52 & -10.38\(\,\,\)1.29 & -18.14\(\,\,\)3.75 & **-8.39\(\,\,\)**0.28 \\ Cheetah-Vel & -91.34\(\,\,\)14.87 & -86.52\(\,\,\)10.62 & -81.59\(\,\,\)37.17 & -110.55\(\,\,\)12.01 & **-79.90\(\,\,\)**5.69 \\ Cheetah-Dir & 376.66\(\,\,\)33.00 & 480.49\(\,\,\)48.38 & 577.80\(\,\,\)24.87 & 579.90\(\,\,\)15.55 & **631.34\(\,\,\)**83.96 \\ Ant-Dir & 869.58\(\,\,\)22.64 & 511.97\(\,\,\)23.86 & 255.49\(\,\,\)32.64 & 330.15\(\,\,\)33.77 & **908.48\(\,\,\)**20.73 \\ Hopper-Param & 320.76\(\,\,\)10.58 & 338.36\(\,\,\)14.07 & 335.12\(\,\,\)26.73 & 351.96\(\,\,\)13.61 & **358.05\(\,\,\)**10.69 \\ Walker-Param & 391.24\(\,\,\)20.28 & 394.32\(\,\,\)19.78 & 330.54\(\,\,\)14.07 & 334.34\(\,\,\)28.57 & **470.36\(\,\,\)**12.85 \\ 
**Expert** & Prompt-DT & Generalized DT & CORRO & CSRO & Meta-DT \\  Point-Robot & -7.99\(\,\,\)0.46 & -12.99\(\,\,\)0.34 & -7.76\(\,\,\)0.18 & -19.42\(\,\,\)2.10 & **-6.90\(\,\,\)**0.11 \\ Cheetah-Vel & -133.78\(\,\,\)18.24 & -62.95\(\,\,\)3.42 & -111.47\(\,\,\)36.97 & -129.00\(\,\,\)24.24 & **-52.42\(\,\,\)**8.11 \\ Cheetah-Dir & **960.32\(\,\,\)**17.07 & 806.30\(\,\,\)124.41 & 628.64\(\,\,\)61.11 & 641.05\(\,\,\)129.54 & 874.91\(\,\,\)73.45 \\ Ant-Dir & 678.07\(\,\,\)68.74 & 613.59\(\,\,\)49.22 & 381.42\(\,\,\)13.83 & 417.37\(\,\,\)39.70 & **961.27\(\,\,\)**18.07 \\ Hopper-Param & **393.79\(\,\,\)**11.44 & 358.56\(\,\,\)11.75 & 338.17\(\,\,\)47.36 & 358.29\(\,\,\)16.25 & 383.51\(\,\,\)8.99 \\ Walker-Param & **449.15\(\,\,\)**37.53 & 421.96\(\,\,\)40.70 & 352.02\(\,\,\)44.99 & 336.89\(\,\,\)16.71 & 437.79\(\,\,\)18.21 \\   

Table 4: Few-shot test returns of Meta-DT against baselines using Mixed and Expert datasets.

Figure 6: Few-shot test curves of Meta-DT and baselines using Expert datasets.