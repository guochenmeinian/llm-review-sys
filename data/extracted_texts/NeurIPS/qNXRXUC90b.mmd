# Uncertainty-aware Fine-tuning of Segmentation Foundation Models

Kangning Liu\({}^{1,2}\) Brian Price\({}^{2}\) Jason Kuen\({}^{2}\) Yifei Fan\({}^{2}\) Zijun Wei\({}^{2}\) Luis Figueroa\({}^{2}\)

**Krzysztof J. Geras \({}^{1}\) Carlos Fernandez-Granda\({}^{1}\)**

\({}^{1}\)New York University \({}^{2}\)Adobe

###### Abstract

The Segment Anything Model (SAM) is a large-scale foundation model that has revolutionized segmentation methodology. Despite its impressive generalization ability, the segmentation accuracy of SAM on images with intricate structures is often unsatisfactory. Recent works have proposed lightweight fine-tuning using high-quality annotated data to improve accuracy on such images. However, here we provide extensive empirical evidence that this strategy leads to forgetting how to "segment anything": these models lose the original generalization abilities of SAM in the sense that they perform worse for segmentation tasks not represented in the annotated fine-tuning set. To improve performance without forgetting, we introduce a novel framework that combines high-quality annotated data with a large unlabeled dataset. The framework relies on two methodological innovations. First, we quantify the uncertainty in the SAM pseudo labels associated with the unlabeled data and leverage it to perform uncertainty-aware fine-tuning. Second, we encode the type of segmentation task associated with each training example using a _task prompt_ to reduce ambiguity. We evaluated the proposed Segmentation with Uncertainty Model (SUM) on a diverse test set consisting of 14 public benchmarks, where it achieves state-of-the-art results. Notably, our method consistently surpasses SAM by 3-6 points in mean IoU and 4-7 in mean boundary IoU across point-prompt interactive segmentation rounds. Code is available at https://github.com/Kangningthu/SUM

## 1 Introduction

The Segment Anything Model (SAM)  is a ground-breaking vision foundation model to perform interactive binary segmentation, where the goal is to identify the pixels belonging to a target region in an input image. The task is interactive, because the model accepts prompts from the user, which specify the desired target region. These prompts are typically in the form of a set of points that should be included or excluded from the target region, or a box or mask that encompasses it.

SAM is trained via a boot-strapping training procedure called a _data engine_ on massive dataset of unlabeled images. The key idea is to leverage model-generated pseudo-labels to annotate the unlabeled data and use the pseudo-labels to further train the model. SAM is notable for its impressive general-purpose capabilities, allowing it to segment a wide range of objects, parts, and visual structures. Yet, the model encounters difficulties in achieving **high-quality segmentation** of intricate structures and often produces imprecise segmentation boundaries. This is problematic for applications such as image editing and automatic image annotation , where accurate fine-grained predictions are essential.

Our analysis in Appendix I reveals that **inaccuracies within pseudo-labels** are substantial, especially in boundary regions. It has been reported in  that simply scaling up the amount of pseudo-labeled data by 10x only achieves marginal improvement (around \(0.5\) mIoU). To improve segmentationquality, recent works [4; 5] suggest fine-tuning SAM exclusively with data paired with high-quality human annotation. A key challenge is that the size of those datasets is usually relatively small, and the diversity of images and annotations they contain is limited. Consequently, the **model may overfit** the characteristics of the fine-tuning dataset, which degrades its performance in other situations (e.g., segmentation tasks different from the fine-tuning dataset). Currently, the most common strategy to mitigate this is lightweight finetuning, which only updates a small subset of model parameters [4; 5; 6; 7; 8]. However, our experiments find that the overfitting problem still persists, as demonstrated in Fig. 1 and Section 4.2.

In this paper, our goal is to investigate how to better learn from pseudo-labels, with the help of high-quality human-annotated data. We then apply these insights to **improve the accuracy of SAM without compromising its generalization**. Our proposed framework, the Segmentation with Uncertainty Model (SUM), performs fine-tuning in a manner that is aware of the regions where model-generated pseudo-labels _have questionable accuracy_. This is achieved by explicitly estimating pseudo-label uncertainty in the unlabeled training data, in the form of an _uncertainty map_ generated by a specialized module. The uncertainty map is then incorporated into a novel _uncertainty-aware training loss_, which reduces the influence of pseudo-label regions that are expected to be inaccurate. The framework has an additional advantage when training interactive-segmentation models. During training, user prompts are typically generated randomly based on the available labels. Our framework additionally enables _uncertainty-aware prompt sampling_, which avoids producing misleading prompt locations.

Our framework also addresses an additional challenge in fine-tuning segmentation foundation models. Human annotations usually correspond to specific segmentation tasks. For instance, the HQ-Seg-44K  used by previous fine-tuning approaches  focuses on _salient-object segmentation_, where the target regions may include multiple entities in a complex arrangement. In contrast, current general-purpose interactive segmentation models, such as SAM, are mainly trained for _entity_ or _part_ segmentation, where target regions are associated with individual entities or object parts, respectively. To take this into account, we propose a novel _task prompt_, which informs the model of the desired

Figure 1: **SUM improves SAM without forgetting to “segment anything.” Performance comparison between the Segment Anything Model (SAM), HQ-SAM model fine-tuned on the HQ-Seg-44K dataset, and the proposed Segment with Uncertainty Model (SUM) fine-tuned using a high-quality annotated dataset and unlabeled data. Each row corresponds to held-out datasets focusing on different segmentation tasks (SegTask). Left: Both HQ-SAM and SUM show qualitative improvements over SAM, particularly in salient-object segmentation of complex structures (top row). HQ-SAM, however, struggles with background entities (middle row) and part segmentation (bottom row), often erroneously prioritizing objects in the foreground or entire objects. Right: SUM consistently outperforms SAM and HQ-SAM in quantitative comparisons, achieving the highest mean boundary IoU across diverse evaluation sets and interactive segmentation rounds. We adhere to the same point prompt sampling evaluation strategy as SAM  (Appendix B.3). More examples are in Appendix H.2, H.3.**

segmentation task during training. This resolves ambiguities between the human and model-generated labels, and can also be leveraged to specify the desired segmentation task during inference.

To evaluate SUM, we performed systematic experiments with diverse segmentation datasets, resulting in the following insights. (1) Existing fine-tuning approaches forget to _segment anything_. After fine-tuning on HQ-Seg-44K, they outperform SAM on salient-object segmentation but underperform on held-out datasets where annotations are associated with entities or object parts (Fig. 1 and Section 4.2). (2) Including unlabeled data associated with SAM pseudo-labels in the fine-tuning dataset prevents forgetting, but also yields a limited gain in performance if noise inherent in pseudo-labels is not effectively managed (see Section 4.3). (3) SUM is able to effectively leverage the labeled and unlabeled data to achieve superior performance (see Section 4.2 and Appendix G.1). This is demonstrated in controlled experiments using publicly available fine-tuning datasets, as well as on a large-scale internal dataset. An ablation study (see Section 4.3) shows that the different methodological innovations (uncertainty-based loss, uncertainty-based prompting, and task prompting) all contribute to this gain in performance.

## 2 Related Work

Segmentation Foundation ModelsSegmentation is a fundamental task in computer vision, which includes semantic segmentation [9; 10; 11; 12], instance segmentation [13; 14], panoptic segmentation [15; 16], and interactive segmentation [17; 18; 19; 20]. Inspired by the breakthroughs of language foundation models [21; 22; 23; 24], the field has recently witnessed the emergence of large-scale segmentation models pre-trained on massive datasets, which demonstrate remarkable generalization capabilities. Painter  and SegGPT  proposed robust in-context learning frameworks enabling segmentation of images given image-mask prompts. Recently, SEEM  introduced a multi-modal reference-prompted general segmentation model. Our work builds on the Segment Anything Model (SAM) , a ground-breaking foundational model for interactive segmentation. SAM is trained leveraging a boot-strapping training procedure called a _data engine_, which encompasses multiple phases with different degrees of human supervision, culminating in a fully-automated stage where the pseudo-labels are generated automatically from the model.

Fine-tuning Segmentation Foundation ModelsDifferent methods have been proposed to extend SAM, and other foundation models, in different ways, such as increasing inference speed [28; 29], performing all-purpose matching  or personalized segmentation , increasing granularity , enabling language-informed segmentation guidance , and adapting to specific domains (e.g., medical imaging [34; 35; 6; 7; 36; 37; 38; 39; 40; 41], agriculture , remote sensing , shadow detection [43; 44; 45], or water leakage ).

Our goal is to enhance SAM's accuracy while maintaining its generalizability. HQ-SAM  pursues a similar goal, but primarily focuses on minimal architectural adaptations for light-weight fine-tuning on high-quality human-annotated data. Here, we focus on an effective framework for leveraging limited human-annotated data and diverse unlabeled data.

Semi-supervised Semantic SegmentationSemi-supervised semantic segmentation [47; 48; 49; 50] also aims to leverage unlabeled data when labeled data is limited. However, the purpose is usually to enhance the performance of a model on a specific task. In this work, we aim to instead leverage the unlabeled data to preserve the generalization ability of a foundation model for diverse segmentation tasks (e.g. part, object, background), which may not be represented in the labeled data. The majority of semi-supervised approaches enforce consistency between predictions corresponding to augmented versions of an unlabeled image. This is achieved through various specialized architectures such as teacher-student networks [51; 52; 53], parallel networks [54; 55], or incorporating additional representation heads . In contrast, our proposed method seeks to improve the pseudo-labels based on the data engine of SAM, requiring minimal modifications to the existing architecture. Combining these two types of approaches is an interesting direction for future investigation.

Uncertainty-aware SegmentationUncertainty-aware segmentation has been explored for semi-supervised settings (e.g. U2PL ), and domain-adaptive segmentation [57; 58; 59]. These methods typically employ prediction confidence [51; 57] or discrepancy among multiple prediction heads [59; 58] to quantify uncertainty. Our approach differs fundamentally from previous approaches in both the generation of uncertainty maps and the utilization of uncertainty maps. SAM undergoes multiple rounds of self-training, which leads to the accumulation and overfitting of errors in pseudo-labels . During our evaluation, the model frequently predicts incorrect regions with high confidence, and different heads agree on these erroneous areas. Experiments detailed in Section 4.2 show that previous approaches are ineffective in our context. Our main insight is to use external supervision to identify and correct systematic biases that accumulate during training. Further discussion can be found in Appendix E.2.

## 3 Method

### Preliminary Background: Segment Anything Model (SAM)

The Segment Anything Model (SAM)  consists of three modules: an image encoder based on a large vision transformer, a lightweight prompt encoder for geometric inputs (e.g., points, boxes, mask indicating the target in interactive segmentation), and a lightweight mask decoder. The decoder combines the outputs of both encoders to produce a predicted segmentation mask.

During training, the model is provided interactive prompts iteratively. The first prompt is obtained by randomly selecting a point or bounding box from the target mask. Then, subsequent point prompts are randomly sampled within the error region, which is the discrepancies between the target mask and prior model prediction. At each training step, the model receives the preceding mask prediction as an additional input.

### Segment with Uncertainty Model (SUM)

The Segment with Uncertainty Model (SUM) is a fine-tuning framework to improve the segmentation quality of an existing foundational segmentation model, which builds upon the Segment Anything Model (SAM). A diagram illustrating the framework is provided in Fig. 2. The framework assumes the availability of a _human-labeled set_ with high-quality labels (e.g. HQ-Seg-44K), and a large, diverse _unlabeled set_ (e.g. SA-1B ). These datasets are used to fine-tune SAM via interactive segmentation training. Training is performed with human annotations for the labeled data and with SAM-generated pseudo-labels for unlabeled data. These pseudo-labels are produced in accordance with SAM's automatic mask generation pipeline (for more details, see Appendix B).

SUM contains several novel elements designed to improve the fine-tuning process. The pseudo-labels generated by SAM are inaccurate, which is problematic for training. SUM addresses this by exploiting the insight that this inaccuracy is systematic and _predictable_. Uncertainty maps are leveraged to modify the fine-tuning cost function and the generation of training prompts for the unlabeled examples to make them uncertainty aware, as explained in Sections 3.2.1 and 3.2.2. The uncertainty maps are produced by an uncertainty-quantification module described in Section 3.2.3. Additionally, SUM incorporates a task prompt, described in Section 3.2.4, which allows us to specify the desired segmentation task.

#### 3.2.1 Uncertainty-aware Loss

SUM combines human-annotated and unlabeled data, utilizing different training losses for each. For the human-annotated data, SUM minimizes the same training loss as SAM: a linear combination of focal loss  and Dice loss . For the unlabeled data, we propose to utilize uncertainty-aware versions of both losses instead, which are then combined in the same way as SAM. The goal is to down-weight the influence of uncertain areas in the pseudo-labels and increase the influence of regions with high confidence.

The _uncertainty-aware focal loss_\(l_{}\) is obtained by incorporating pixel-specific weights to the standard focal loss . Let \(_{i}\) denote the binary pseudo-label corresponding to the \(i\)th pixel in an input image, \(p_{i}\) the corresponding probability predicted by the model, and \(u_{i}\) the corresponding entry of the uncertainty map obtained from the SUM uncertainty-quantification module (see Section 3.2.3). The uncertainty is between 0 (highly certain) and 1 (highly uncertain). The contribution of each pixel to the focal loss \(l_{}(p_{i},_{i})\) is weighted by \(c_{i}=1-u_{i}\), which can be interpreted as the certainty associated with that pixel:

\[l_{}(p,)=_{i=1}^{m}c_{i} l_{}(p _{i},_{i})\] (1)

The _uncertainty-aware Dice loss_\(l_{}\) is an adaptation of the standard Dice loss, which only preserves pixels where the certainty is above a certain threshold \(\) (see Appendix F for a discussion on the choice of this hyperparameter):

\[& l_{}(p,)=1-}(p,,u)+}{S_{}(p,,u)+} \\ & I_{}(p,,u)=_{i=1}^{m}_{i} p_{i} [c_{i}>],S_{}(p,,u)= _{i=1}^{m}(_{i}+p_{i})[c_{i}>],\] (2)

where \(\) is a small positive constant to ensure numerical stability. \(I_{}\) and \(S_{}\) denote the filtered intersection and sum, respectively, of the predicted probabilities \(p\) and pseudo-labels \(\).

#### 3.2.2 Uncertainty-aware Prompt Sampling

SUM applies the same prompt-sampling strategy as SAM for the human-annotated data during interactive training. The first prompt is obtained by selecting a point or bounding box at random from the target mask containing the positive segmentation labels. Then, subsequent _positive_ point prompts

Figure 2: **Segment with Uncertainty Model (SUM) framework. The diagram describes the proposed SUM framework, which utilizes both human-annotated and unlabeled data to fine-tune a foundation model for interactive segmentation. Top: When processing human-annotated examples, interactive prompts are sampled based on the binary-mask labels and fed iteratively into the model along with the image. Since this binary mask depends on the type of segmentation task desired by the user, SUM incorporates a task prompt that specifies the task relevant to each annotation (1 for salient-object segmentation and 2 for entity segmentation). Bottom: For unlabeled images, the iterative prompts are sampled based on model-generated binary pseudo-labels, which may be inaccurate. SUM includes an uncertainty-quantification module that processes the pseudo-labels, generating an uncertainty map. This map is leveraged within an uncertainty-aware loss function used for training, and also informs how the interactive prompts are sampled. For all unlabeled data, the task prompt is set to 0. Components highlighted in red represent our novel contributions, whereas the remaining components adhere to the SAM framework.**

are selected from regions in the target mask not present in the predicted mask, and _negative_ point prompts are selected from regions in the predicted mask not present in the target mask. When the target mask is inaccurate, which is often the case when it relies on pseudo-labels, this is problematic because the point prompts may be completely incorrect. To address this, SUM leverages the uncertainty maps generated by the uncertainty-quantification module (see Section 3.2.3) to perform uncertainty-aware prompt sampling. Let \(\) denote the region used by SAM for prompt sampling.1 Uncertainty-aware sampling selects each pixel \(i\) in \(\) with probability \(P_{i}=c_{i}/_{j=1}^{n}c_{j}\), where \(c_{i}\) is defined above as the certainty associated with the pixel. This strategy ensures that prompts are sampled mostly from regions with high confidence.

#### 3.2.3 Generation of Uncertainty Maps

In this section, we describe how uncertainty quantification is performed within the SUM framework, detailing the generation of uncertainty maps associated with pseudo-labels. For this purpose, SUM uses a specialized mask-refinement module. The module receives the logits produced by the SAM model2 as an input and outputs a refined prediction of the segmentation mask. The uncertainty map is calculated by taking the absolute difference between the refined prediction and the sigmoid-transformed probabilities of the SAM logits. The uncertainty value for the \(i\)-th pixel in the uncertainty map \(u_{i}\) is given by:

\[u=|(s_{i})-(r_{i})|\] (3)

where \(\) is the sigmoid function, \(s_{i}\) denotes the SAM logits for the \(i\)-th pixel, and \(r_{i}\) denotes the refined prediction for the \(i\)-th pixel. This yields values between 0 (no difference, indicating low uncertainty) and 1 (large difference, indicating high uncertainty). This is illustrated in Fig. 3. The mask-refinement module also receives as input a box prompt encompassing the pseudo-label and the point prompt used to generate the pseudo-labels.

The mask-refinement module is implemented by fine-tuning SAM's decoder and prompt encoder. To be clear, these fine-tuned components are exclusively used to generate uncertainty maps before training SUM. We train the mask-refinement module to refine the SAM pseudo-label masks using the corresponding high-quality labels in the high-quality human-annotated dataset. During training, we feed the SAM pseudo-label logits to the mask-refinement module in the same way that SAM incorporates previous mask predictions in its interactive segmentation training protocol.

Appendix D provides an in-depth description of the design of the mask-refinement module, including quantitative and human-subject evaluations of the resulting segmentation maps, the relationship between uncertainty and pseudo-label accuracy, alongside a comparative analysis of model variants.

Although it is possible to replace SAM pseudo-labels with refined masks in a naive manner, we empirically find that it less effective than uncertainty-aware training, as reported in Table 2. This suggests that potential errors in mask refinement (see Appendix Fig. 9) may impact performance.

Figure 3: **Generation of uncertainty maps**. (1) The mask-refinement module receives as input the segmentation prediction produced by SAM. (2) The module produces a refined segmentation mask. (3) The uncertainty map equals the absolute difference between the SAM and refined predictions. Additional visual examples of the uncertainty map are presented in Appendix Fig. 10.

#### 3.2.4 Task Prompt

Segmentation datasets often focus on different segmentations _tasks_, resulting in dramatic and systematic differences between their labels. For example, the HQ-Seg-44K  dataset targets _salient-object segmentation_, where a segmentation mask may include multiple entities in a complex arrangement. In contrast, the pseudo-labels produced by SAM for the SA-1B dataset mostly correspond to _entity segmentation_ (where masks are associated with individual entities) or _part segmentation_ (where masks indicate parts of objects). This is problematic for joint training, as it introduces a high degree of ambiguity regarding the segmentation mask that the model should predict after the initial prompt.

To resolve this ambiguity, SUM includes a task prompt to accompany the first point or box prompt, indicating the segmentation task associated with each training example. In practice, we identify the task based on the dataset associated with each example. We assign 0 to examples from the unlabeled dataset, 1 to examples from human-annotated datasets focused on salient-object segmentation, and 2 to examples from human-annotated datasets focused on entity segmentation. See Appendix H.1 for an illustration. More categories can be added to account for additional segmentation tasks (e.g., 3 for part segmentation). The task prompt is implemented via an embedding layer, which acts as a lookup table mapping each discrete task identifier to a unique high-dimensional vector embedding learned during training. This task embedding is combined via element-wise addition with the embedding produced by the SAM image encoder, as illustrated by Fig. 2.

## 4 Experiments

### Experiment Settings

**Training Sets** We utilize multiple training sets built using the following datasets:

* **SA-250K**: An unlabeled subset extracted from SA-1B dataset  containing 250,000 images.
* **HQSeg-44K**: A human-annotated set of 44,320 images with high-quality salient-object masks.
* **EntitySeg Training Set**: A human-annotated set of 31,913 images, each with an average of 20 entity masks, detailing both foreground and background.
* **Internal Dataset**: A human-annotated set containing 252,798 images with salient-object masks, 60,798 images with entity masks, and 153,046 images focused on human parsing. More details are provided in Appendix C.

Based on these datasets, we define three fine-tuning (FT) sets with varying budgets of human-annotated data: (1) _FT-Small_: SA-250K and HQSeg-44K, enabling a fair comparison with existing fine-tuning methods such as HQ-SAM ; (2) _FT-Medium_: SA-250K, HQSeg-44K and EntitySeg; (3) SA-250K and the internal dataset.

**Evaluation Sets** All the models are evaluated on the same collection of test sets, designed to assess performance on three different segmentation tasks:

* **Salient Object**: COIFT , validation sets of DIS  and ThinObject5K , HR-SOD , VOCEVAL , and BIG . These datasets contain object-centric images with salient objects and binary masks. The training sets in DIS-VD and ThinObject5K are included in the HQSeg-44K dataset. COIFT, HRSOD, VOCEVAL, and BIG are used for zero-shot evaluation.
* **Entity**: Three EntitySeg  validation sets with 454, 459 and 401 images. These sets feature complex scene images with multiple entities ranging from foreground instances to background regions (e.g., sky, road).
* **Part**: Three human/clothing parsing datasets: the validation set of Fashionpedia  containing 1,148 images (861 of which have subpart masks), the first 1,000 images of the validation sets of Multi-Human Parsing  and Easyportrait . We also include the first 1,000 images of the validation set of Paco , which contains a wider variety of object types.

**Evaluation** 1) **Metrics**: We use standard metrics to evaluate segmentation quality: mean Intersection over Union (mIoU) and mean boundary IoU (mBIoU). We aggregate these metrics via averaging for the test datasets in each category. Also, an overall average is derived by aggregating results from all test datasets. 2) **Test prompts**: Unless specified otherwise, we generate test prompts using thecenter point sampling technique described in SAM . We also report box-prompt segmentation results in Appendix G. 3) **Comparison to single-output model**: SAM and SUM generate multiple masks when a single prompt is provided. For evaluation, we generally select the output mask that most closely aligns with the provided ground truth. However, when comparing it to HQ-SAM, which produces a single output, we use a single output fixed in advance.

**Baselines** We compare the proposed method SUM against the following baselines: (1) _Human annotation only_, a method where fine-tuning is exclusively performed with human-labeled data; and (2) _Vanilla_, where fine-tuning is performed on unlabeled and human-annotated data following the standard SAM training protocol, without discriminating between human and SAM pseudo labels.

**Implementation Details:** SUM is implemented by fine-tuning a SAM model based on a ViT-H backbone. The experiments were done adhering to SAM's configuration for loss weighting. Key training parameters, including learning rate, duration, hyperparameters, and schedules for both human-annotated and unlabeled data, are detailed in Appendix B. Although the SUM framework introduces additional steps to generate uncertainty maps, these steps remain manageable within practical constraints. A detailed discussion on training complexity and computational overhead is provided in Appendix B.2.

### Comparing SUM with SoTA Methods

**Comparison with HQ-SAM**  Fig. 4 evaluates and compares different fine-tuning strategies, which are all constrained to utilize the same lightweight fine-tuning scheme and architecture as HQ-SAM  (NeurIPS 2023) for a fair comparison. We compare (1) the proposed SUM method; (2) the original HQ-SAM, fine-tuned following the Human-Annotation-Only baseline approach (i.e., fine-tuning exclusively on HQ-Seg-44k); and (3) the _vanilla_ baseline, which utilizes all of FT-Small. Evaluation is carried out over several rounds of interactive segmentation. HQ-SAM excels in salient-object segmentation unsurprisingly, as it is fine-tuned on a dataset associated with this segmentation task, but its performance is worse than the other models for entity and part segmentation (and in fact also worse than SAM, as shown in Figure 1). Conversely, the _vanilla_ approach, which uses unlabeled data and SAM pseudo labels, performs worse than HQ-SAM on salient-object segmentation. SUM, utilizing the same HQ-SAM lightweight structure, achieves similar performance to HQ-SAM on salient-object segmentation while clearly outperforming all other models on entity and part segmentation. Detailed dataset-wise comparisons are provided in Appendix G.3.

Comparison with Other Light-weight Fine-tuning StrategiesThe left subfigure of Fig. 5 illustrates the comparison of SUM fine-tuned on the FT-Small against various strategies applied to fine-tune SAM on the human-annotated HQSeg-44k dataset: _Decoder+Prompt-Encoder FT_: fine-tuning only the decoder and prompt encoder, _Adapter FT_: fine-tuning only the Adapter  plugged into the image encoder and _LoRA FT_: fine-tuning SAM with LoRA  in the same way as SAMed . Our results suggest that fine-tuning SAM exclusively on a specific human-annotated dataset can easily result in overfitting, regardless of the fine-tuning strategy. Conversely, SUM enhances the overall performance and mostly retains SAM's generalization capabilities.

Comparison with Semi-supervised MethodsTo assess SUM's performance relative to other methods utilizing unlabeled data, we adapt AugSeg , a SoTA semi-supervised semantic segmentation method, to the interactive segmentation pipeline. AugSeg employs a teacher-student framework. For

Figure 4: Comparison of HQ-SAM  with Vanilla and SUM fine-tuned using the same lightweight scheme as HQ-SAM. SUM matches HQ-SAM and outperforms Vanilla in salient-object segmentation and is superior in entity and part segmentation.

the human-annotated data, we employ SAM's training procedure to train the student network, with both point and box prompts. For the unlabeled data, we apply a consistency loss based on predictions from two augmented versions of the same image, processed by the student and teacher networks. Interactive segmentation requires prompt sampled from target labels. We utilize model-generated pseudo-labels to generate the prompts for unlabeled data in two different ways: 1) AugSeg Mixed-Prompt employs a combination of point and box prompts, and 2) AugSeg Box-Prompt uses only box prompts. We provide more implementation details and discussion in Appendix E. As depicted in the right subfigure of Fig. 5, SUM clearly outperforms both versions, achieving a mIoU of 72.8, compared to 68.8 for AugSeg Mixed-Prompt, 67.0 for AugSeg Box-Prompt.

The choice of uncertainty quantification is critical for the effectiveness of pseudo-label utilization. To understand how ours compares with existing uncertainty quantification approaches, we further evaluate two variants: (1) **SUM Confidence-Uncer**, which uses logit confidence of SAM prediction to quantify pseudo-label uncertainty, similar to U2PL  (CVPR 2022); (2) **SUM Discrepancy-Uncer**, which quantifies uncertainty using the discrepancy between multiple predictions for a target mask, similar to Zheng et al. 's work (IJCV 2021). Results in the right sub-figure of Fig. 5 indicate that these variants underperform ours significantly. We also tested an improved U2PL incorporating the same uncertainty map as SUM and task prompt training, but it still underperforms SUM. Further details are in Appendix E. We provide 3-point-prompt interactive segmentation comparison in Table 1. The performance of SUM on part segmentation improves, which is not explicitly represented in the human annotations. This demonstrates SUM's capability to **enhance accuracy across general tasks**.

Performance of SUM under Various Annotation BudgetsIn Appendix G.1, we evaluate our proposed model (SUM) when fine-tuned under various annotation budgets, utilizing the FT-Medium and FT-Large datasets. SUM achieves consistent improvements regardless of the fine-tuning budgets.

Performance of SUM on Additional Evaluation SetsTo further validate the generalization of our proposed methods, we have expanded our evaluation by testing SUM and SAM on 8 additional evaluation sets that include a variety of image types. For reproducibility, SUM is fine-tuned on the public dataset FT-Medium. As shown in Appendix G.6, SUM consistently outperforms SAM.

   Metrics & Task &  SAM (without \\ fine-tuning) \\  & Vanilla &  AugSeg \\ Mixed-Prompt \\  & UPEL & SUM &  SUM Confidence- \\ Uncer \\  & 
 SUM Discrepancy- \\ Uncer \\  \\   & Salient object & 86.1 & 89.2 & 90.6 & 89.1 & **91.2** & 89.1 & 89.1 \\  & Entity & 82.7 & 81.2 & 82.9 & **83.4** & **83.4** & 82.1 & **83.4** \\  & Part & 63.2 & 61.8 & 59.9 & 64.2 & **67.2** & 59.3 & 62.4 \\  & Overall average & 77.2 & 77.7 & 78.0 & 79.0 & **81.0** & **77.0** & **78.4** \\   & Salient object & 77.0 & 81.0 & 82.5 & 81.0 & **83.2** & 81.0 & 81.0 \\  & Entity & 76.6 & 74.6 & 77.0 & **77.2** & **77.2** & 76.0 & 76.5 \\  & Part & 59.4 & 57.5 & 55.8 & 60.2 & **63.3** & 55.2 & 58.1 \\   & Overall average & 70.6 & 71.3 & 71.8 & 72.8 & **74.8** & 70.7 & **71.9** \\   

Table 1: **3-point-prompt segmentation**. This table reports 3-point-prompt interactive segmentation mean IoU and mean Boundary IoU of different models fine-tuned on FT-Small.

Figure 5: **Left:** Comparison of single point-prompt segmentation mIoU for SUM versus models fine-tuned using various strategies on the HQSeg-44K dataset. All competing models improve on the salient-object segmentation task associated with this dataset but deteriorate on other segmentation tasks. **Right:** mIoU for single point-prompt segmentation: comparisons of models fine-tuned on FT-Small dataset with various strategies. SUM clearly outperforms all other strategies on salient-object and part segmentation, while preserving a competitive performance on entity segmentation.

### Ablation Experiments

Ablation on Uncertainty-aware Training and Task PromptTable 2 compares the Vanilla baseline and the following ablated versions of SUM on public dataset FT-Medium to ensure reproducibility: 1) **Mask Refinement:** This variant uses the refined masks produced by the mask-refinement module of SUM as pseudo labels for the unlabeled images in SA-250K, and is otherwise the same as the Vanilla baseline. 2) **SUM w/o Task Prompt:** SUM without task prompts but with uncertainty-aware loss and sampling. 3) **SUM Continuous TP**: This is a variant of SUM where the task prompt is utilized continuously in all rounds of interactive segmentation during training and inference (as opposed to just in the first round, as in SUM).

The results demonstrate the advantage of uncertainty-aware training over direct mask refinement. SUM w/o Task Prompt outperforms Mask Refinement, especially for part segmentation, which is the segmentation task that is not included in the human-annotated labels. Both models outperform Vanilla, indicating that mask refinement provides an advantage over a naive combination of unlabeled and labeled data. Additional experiments analyzing mask refinement are reported in Appendix D.

SUM is superior to SUM without Task Prompt across all segmentation tasks for a single-point prompt, but both models are essentially equivalent for 3 prompts. This suggests that task prompting is very useful when there is more ambiguity as to which mask is desired by the user. In fact, SUM Continuous TP underperforms SUM for 3-point prompts, suggesting that persistent task prompting may be counterproductive as semantic ambiguity decreases.

Additional AblationIn Appendix F.2, we compare SUM to a version of SUM without uncertainty-aware sampling. In Appendix Table 6, we provide a comprehensive comparison between SAM and SUM implemented with different backbones, including ViT-B, ViT-L, ViT-H. Our model consistently outperforms SAM across different model backbones for all mask types and interactive rounds. The improvement is most significant for single-point prompt segmentation of salient objects. In Appendix F.1, we compare SUM and different variants of SUM via different lightweight fine-tuning methods.

## 5 Discussion and Future Work

Fine-tuning foundation models effectively is an important challenge. Our work suggests that a key consideration is how to account for varying degrees of diversity and label quality in the available data. Specifically, we show that in the case of segmentation, incorporating a diverse unlabeled dataset can be effective in preserving the generalization ability of the original foundation model, as long as they are utilized in an uncertainty-aware manner.

As detailed in Appendix D, our methods have limitations: the quality of mask refinement in SUM is dependent on the initial performance of the SAM model. This suggests that leveraging SUM to improve pseudo-label quality in the training data could lead to further improvement. Our work suggests some possible avenues for future research to improve the training of foundation models. The uncertainty maps generated by SUM can potentially be used to improve pseudo-label filtering in foundation-model data engines. In fact, a human study reported in Appendix I found that filtering approaches utilized in SAM are unable to accurately discern pseudo labels with inaccurate boundaries. Finally, the efficacy of the proposed task prompts suggests that it may be possible to develop automatic label-generation schemes to produce pseudo-labels associated with more diverse tasks.

    & Point &  & SAM (without &  &  &  &  &   } \\  & Number & & & & & & \\   &  & Salient object & 78.7 & 80.4 & 81.5 & 81.1 & **85.2** & 84.7 \\  & & Entity & 77.4 & 78.8 & 78.1 & 78.6 & **79.8** & 79.2 \\  & & Part & 52.2 & 50.2 & 52.3 & 54.4 & **55.3** & 54.6 \\   &  & 69.0 & 69.3 & **70.3** & **71.0** & **73.4** & **72.8** \\   &  & Salient object & 86.1 & 90.4 & 91.5 & 91.4 & **91.6** & 89.6 \\   & & Entity & 82.7 & 86.5 & 86.2 & **86.7** & 86.6 & 85.7 \\   & & Part & 63.2 & 64.4 & 66.7 & 67.8 & **67.9** & 67.7 \\   &  & 77.2 & 80.3 & 81.5 & 82.0 & 82.1 & 81.0 \\   

Table 2: **Ablation study**. This table reports interactive segmentation mean IoU of different ablated versions of SUM, showing individual gains provided by uncertainty-aware fine-tuning and task prompts. Mean Boundary IoU and detailed dataset-wise comparisons are provided in Appendix F.4.