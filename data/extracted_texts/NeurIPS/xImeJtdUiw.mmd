# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

such as Masked Language Modeling (mlm), as seen in models like esm and the Nucleotide Transformer (nt), or supervised learning approaches on large datasets, as in AlphaFold  and Enformer . These models have been instrumental in advancing our understanding of biology by accurately predicting the structures and functions of biological sequences.

While existing methods provide relevant insights, they are still limited by the fact that they only consider a single sequence modality. In biology, the _central dogma_ describes the flow of genetic information from dna to rna to proteins . This fundamental concept underscores the interconnectedness of these three types of biological sequences and highlights the potential for a unified modeling approach. An architecture that integrates dna, rna, and protein modalities should provide a comprehensive model for biology that mirrors the natural processes within cells. Furthermore, by enabling transfer learning across modalities, the model can capitalize on the vast amounts of pre-training already performed on individual dna, rna, and protein datasets.

Developing deep learning models using multiple biological sequence modalities has been mainly limited by the lack of matched available data; existing databases usually isolate a specific modality and thus relationships between modalities are not easily obtainable. As more multi-modal datasets are made available [16; 17], it is becoming possible to develop models that extract and combine the information from dna, rna, and protein sequences to better model the different cellular processes. Such multi-modal models have already been successful in other domains, such as mixing language and visual inputs [18; 19; 20; 21; 22; 23; 24; 25], but until now there are no models that can handle multiple biological sequence modalities.

In our work, we propose the first multi-modal architecture to connect dna, rna, and proteins (Fig. 1). Our approach is based on three main components: (i) pre-trained modality-specific encoders that produce one embedding per modality, (ii) aggregation layers that combine information from the encoders and create a multi-modal representation, and (iii) a task-specific head that predicts the desired output. We show that our multi-modal approach transfers and aggregates knowledge of pre-trained mono-modal encoders and outperforms previous single-modality baselines (Enformer , nt, and esm). We also demonstrate the flexibility of our approach by comparing different encoders for a specific modality and different aggregation techniques. While some previous approaches have modeled specific interactions between modalities, such as protein-to-DNA interaction using structure information and modules , our approach is general-purpose and can be adapted to any task involving one or more biological sequence types.

Significant problems in genomics intrinsically involve multiple sequence modalities [27; 26], and it is still unclear how to adapt general-purpose sequence models to those cases. In order to validate our multi-modal approach, we focused on the study of a crucial task in genomics that has been challenging to tackle using a single sequence modality - namely the prediction of rna_transcript isoform expression_ across different tissues . When gene dna sequences are transcribed into mrna molecules to produce proteins, they generally do not express it in just a single way producing a single protein isoform. After dna sequences are transcribed, pre-mrna transcripts undergo a process called rna alternative splicing where they are cut and re-assembled to form different variants of mature mrna molecules that can be translated into proteins. This process allows for the generation of multiple rna and protein isoforms from a single gene dna sequence, that can differ in structure, function, localization, or other properties (Fig. 2a). Therefore, predicting which isoforms are expressed in a given cell or under specific conditions is key to understand gene regulation and disease mechanisms.

Figure 1: Our aggregation module compiles information from the different biological sequence modalities of dna, rna, and proteins by using successive cross-attention layers and residual connections.

This task is multi-modal in nature, since only looking at the dna sequence present in the cell does not provide a complete picture of the different rna isoform landscape. Focusing on this single important task in opposition to tackling more but less challenging tasks should provide a stronger evidence for the effectiveness of our approach to advance biological knowledge.

In summary, our contributions are the following: (i) We built the first multi-modal model for the integration of dna, rna, and protein sequences. (ii) We show that our model achieves efficient transfer learning between the three modalities, not only leveraging intra-modalities pre-training but also inter-modalities transfer. (iii) We use our architecture to tackle a new central task in biology that requires a multi-modal approach, namely rna_transcript isoform expression prediction_, and obtain state-of-the-art results, overcoming limitations of existing gene expression models such as Enformer. (iv) Finally, we performed ablation studies to validate our different architectural choices and release our trained isoform expression prediction model IsoFormer, providing a new framework and baseline to the community and opening the door to future research on multi-modal sequence modeling and multi-modal biological problems.

## 2 Related Work

Biological sequence modelingResearchers have explored different ways to process dna, rna, and protein sequences for multiple applications. Initial approaches included dynamic programming , hidden Markov models , and genomic hash tables . Recently, deep learning, via supervised  and unsupervised frameworks [3; 4; 13; 2], has gained thrust in the community. These methods, influenced by advancements in Large Language Models [32; 33], have focused on dna[3; 4; 5; 9], rna[10; 11; 12; 13], and protein sequences . They capture complex biological patterns and perform tasks like protein structure prediction  and variant effect prediction . These efforts have become more targeted given the different challenges of each sequence modality. For instance, models like HyenaDNA [5; 9] or Caduceus  tackled the long-range dependencies in dna. Few models consider multiple modalities, mainly focusing on structural information (e.g. predicting dna interacting residues in proteins ). Our work, IsoFormer, is the first general-purpose model integrating three biological sequence modalities.

Multi-Modal IntegrationEfforts to address multi-modality in deep learning models have been prominent in nlp[36; 37; 38; 39; 40] and computer vision [41; 42; 43; 44; 45]. In computer vision, integrating image and audio for video classification  or audio-visual segmentation  is common, often using cross-attention . Unsupervised approaches using contrastive learning also integrate image, audio, and other modalities . Large Language Models have driven multi-modal efforts for text and image with methods like Flamingo  and clip, leading to various integration architectures, including the Perceiver Resampler [50; 19] and C-Abstractor [51; 52]. IsoFormer integrates dna, rna, and protein sequences, leveraging past integration architectures.

Gene Expression PredictionTranscript isoform expression prediction is a more refined and challenging task within gene expression prediction. Traditionally, gene expression has been addressed through tailored approaches and experimental annotations [53; 54]. Recently, deep learning models have demonstrated improved results by predicting gene expression directly from dna sequence [55; 56]. Enformer  leveraged dilated convolutions and attention layers to extend the context window to 190 kilo base pairs (kbp), achieving new state-of-the-art results on gene expression. However, these models are limited to gene-level expression levels and cannot predict isoform-specific expression as dna sequence information is not sufficient to solve this task. We introduce IsoFormer, the first multi-modal model that is able to predict transcript isoform expression.

## 3 Background

Central dogma of biologyIn this work, we consider three biological sequence types: dna, rna, and proteins. These sequences (composed of nucleotides or amino-acids) are fundamental to biological processes in living organisms. They are also intricately intertwined: dna dictates rna synthesis during _transcription_, and rna guides protein synthesis through _translation_ (see Fig. 2; ). Thus, changes in dna can alter rna and protein sequences, impacting an organism's phenotype (i.e. function). dna and rna sequences consist of four nucleotides (acgt and acgu, respectively), while proteins are made of 20 amino-acids. Thus, these sequences are commonly modelled as linear strings.

Obtaining data in this format is becoming more available thanks to recent advances in high-throughput sequencing technologies.

Gene expression and isoformsGene expression is the process by which a gene's Dna sequence is transcribed into various rna molecules that code for specific proteins. This process is complex, as a single gene can produce different rna and protein _isoforms_ with varying abundances across tissues (Fig. 2a). rna isoforms are mrna molecules of different exon compositions derived from the same gene, produced via processes like alternative splicing. The expression level of each isoform is commonly measured by counting the amount of its rna molecules in cells. Accurate isoform expression prediction across tissues can aid in understanding genetic variants' effects on cellular processes and phenotypes. However, this task cannot be tackled solely using dna sequences as the same dna sequence produces different isoforms in different cellular contexts and types. In addition, both dna and rna sequence features are crucial as dna sequences contain regulatory elements that control transcription levels, while rna isoform sequences have features affecting their stability and degradation.

## 4 Method

### Multi-modal framework

We consider respectively dna, rna, and protein sequences \(_{}_{}\), \(_{}_{}\) and \(_{}_{}\) where \(_{}\) is the dna base alphabet acgt, \(_{}\) is the rna base alphabet acgu, and \(_{}\) is the set of 20 amino-acids. We then consider three associated modality-encoders \(f\) with respective weights \((_{},_{},_{})\) that encode sequences \(\) into corresponding embeddings \(\):

\[^{}_{}=f^{}_{}(_{ {dna}},_{},_{}),\ \ ^{}_{}=f^{}_{}(_{},_{},_{}),\ \ ^{}_{}=f^{}_{}(_{},_{},_{})\] (1)

We assume that the weights \((_{},_{},_{})\) have been obtained through independent pre-training processes that can involve supervised or self-supervised training techniques over large corpus of biological data. Note that in practice, as commonly used in recent works , models pre-trained over dna sequences can be re-used to produce embeddings for rna sequences, replacing artificially the uracil base (u) by thymine (t) in the input. In this work, we aim to connect these encoders and train them jointly to learn a multi-modal embedding \(_{}\). We start by learning a multi-modal embedding per modality defined as

\[^{}_{}=f^{}_{}(_{ },_{},_{}),\ \ ^{}_{}=f^{}_{}(_{},_{},_{}),\ \ ^{}_{}=f^{}_{}(_{},_{},_{})\] (2)

where \(f^{}_{}\) is an aggregation function with weights \(\). Then, we define the multi-modal embedding as the concatenation of the per modality multi-modal embeddings:

\[_{}=[^{}_{},^ {}_{},^{}_{}].\] (3)

Note that this definition is general as it allows the use of any aggregation function over the modality embeddings \(\). The multi-modality embeddings per modality \(^{}\) are introduced to solve tasks that are "modality-centered". For instance, \(^{}_{}\) could be used to solve a task that involves nucleotide-level annotation over a dna sequence while requiring other modalities as input. We rely otherwise on the concatenated multi-modal embedding \(_{}\) to solve any other task.

### Genes and isoforms expression

We now introduce our notations specific to the task of rna isoform expression prediction (Fig. 2a). We consider a dna sequence \(_{}\) of length \(L\) to contain a gene \(g\). In practice, given the input size limitation of the existing foundation models, the sequence length \(L\) might be shorter than the full length of the gene. In this case, we choose the dna sequence \(_{}\) to be centered on the start of the gene, i.e., where transcription begins, which also surrounds the promoter regions known to be important for transcription. This way, we also capture all the enhancer regulatory elements upstream of the transcription site.

We denote by \(^{(1)}_{}\),..., \(^{(n)}_{}\) the \(n\) existing transcripts for gene \(g\) across all tissues. Coding transcripts are translated into proteins and we denote by \(^{(i)}_{}\) the amino-acid sequence of the protein associated to the transcript \(_{}^{(i)}\). We define the expression \(e\) of genes and transcripts across tissues \(T\) as:

\[ T,e(_{},T)=_{i=1}^{n}e(_{}^{(i)},T).\] (4)

While deep learning models have been trained to predict the overall expression of genes across tissues with great accuracy [55; 56; 14; 58], to our knowledge no model can predict the expression of the different rna transcripts across tissues directly from the sequence. In this work, we leverage our multi-modal framework to train a transcript expression level prediction model, dubbed IsoFormer, that takes as input a dna sequence, an rna transcript sequence, and its matching protein sequence to predict the expression of that transcript across tissues as measured by bulk rna-seq (Fig. 2).

Aggregation moduleTo capture the local patterns and their relationship across modalities, we introduce an embedding aggregation method based on cross-attention with residual connections (Fig. 1). The module is applied to each modality, performing cross-attention successively to the other modalities to produce multi-modal embeddings that are specific to each modality by keeping their dimensionality and which can be also stacked. Note that this aggregation method is robust to the absence of a specific modality as the cross-attention term will be zeroed out. The final multi-modal embedding \(_{}\) can then be used to solve any task. Our model can be trained end-to-end and it could be applied to different tasks by changing the prediction head.

### Transferring from pre-trained biological encoders

  
**Modality** & **Model** & **Pre-training** & **Num. Params.** & **Tokens\({}^{}\)** & **Sequence Length** \\  dna & Enformer  & Supervised & 110M & 1 Nucleotide & 190,000 \\ dna & nt (v2)  & Self-Supervised & 250M & 6 Nucleotides & 12,282 \\ rna & nt (v2)  & Self-Supervised & 250M & 6 Nucleotides & 12,282 \\ Protein & esm2  & Self-Supervised & 150M & 1 Amino-acid & 2,047 \\   

Table 1: Characteristics of single-modality encoders considered in IsoFormer. \(\) This column indicates the tokenization scheme (e.g., 1 token corresponds to 6 nucleotides).

Figure 2: **a)** Three types of biological sequences are considered in this work: dna, rna, and proteins. These sequences are composed of nucleotides (dna and rna) or amino-acids (protein). In a single gene, several coding regions or exons can be used to create different rna transcript isoforms and proteins. The abundance of each isoform is tissue-dependent and its measurement is called expression level. **b)** IsoFormer leverages pre-trained encoders that produce modality-specific embeddings, which are then aggregated into multi-modal embeddings. These are used to predict the expression of a given rna transcript isoform across multiple tissues.

EncodersWe describe here the encoders used to process each sequence modality (see also Table 1 and Fig. 1(b)). For protein sequences, we used esm(esm2-150m), which handles any protein up to 2,048 amino-acids. The esm models have been pre-trained through mlm on large corpuses of protein sequences and are considered state-of-the-art on multiple tasks including folding. Protein sequences were all within ESM input length, so no additional processing was required. For dna sequences, we used the nt model , which has also been trained through self-supervision to reconstruct masked 6-mers within 12 kbp genomic regions from 850 species. Additionally to the nt, we also considered the Enformer model as dna encoder. The Enformer is a different type of model that has been pre-trained through full supervision to predict multiple experiments related to chromatin accessibility and modifications, transcription factor binding, and more importantly gene expression. Enformer is a strong candidate for our architecture's dna encoder module as it can process sequences up to 200kbp as well as one can expect to obtain transfer from its gene expression capabilities. Finally, while foundation models have been reported to be pre-trained on rna sequences, none of them has been made publicly available at the moment and pre-training a foundation model from scratch is out of the scope of this study. As such, we re-used the nt model  to compute embeddings for rna sequences as this model has been reported recently to also be able to solve rna and protein  tasks with simple adaptations, using the trick described in section 4.1. RNA transcripts longer than 12kb were left-cropped to 12kb to conserve the 3'UTR regions, which are crucial for mRNA stability and polyadenylation, impacting isoform abundance. This adjustment affected only a small portion of the dataset.

ArchitectureOur architecture leverages three pre-trained encoders, one per bio-modality, as well as the aggregation function defined above, to generate a multi-modality embedding \(_{}\) (Fig. 1(b)). That embedding is finally transformed by an expression head \(f_{}^{}\) with weights \(\) to make isoform expression level predictions across tissues. As the shape of the network output must be independent from the dimension of its inputs, we used a global average pooling over the length of the embedding. A linear layer then outputs one value per tissue. As long as the encoders can take in a biological sequence to produce an embedding, our method can function with different types of general-purpose encoders. While this flexibility allows us to leverage a big part of the landscape of biological sequence encoders, for this work we chose the specific models described in the section below.

### Training

ObjectiveWe denote the IsoFormer model by \(f_{,,}\) where \(\) is the concatenation of the weights of the encoder models. These weights are initialized to the values obtained after pre-training of the different encoders. Respectively \(\) and \(\) denote the weights of the aggregation module and expression prediction head. The IsoFormer is trained to minimize the following objective

\[_{}=_{T}(f_{}^{}( _{},T)-e(_{}^{(i)},T))^ {2},_{}=f_{,}^{}(_{ },_{}^{(i)},_{}^{(i)})\] (5)

where the summation is performed over a set of available tissues. Note that this framework can also accept only one or two of the three modalities as input. This is the case for instance when predicting expression of non-coding transcripts that do not translate into proteins.

DatasetWe conducted our analysis of IsoFormer on rna transcript expression data obtained from the GTEx3 portal. We used Transcript tpm measurements across \(30\) tissues, which come from more than 5,000 individuals. We followed a common process in gene expression datasets : we averaged the expression levels for a given tissue across individuals, and used the reference genome sequence as input. We mapped transcripts to their original genes and associated proteins using the Ensembl database . Our resulting dataset is made of triplets of rna transcript sequences, dna sequences (centered on the Transcription Start Site (tss) of the transcript), and proteins. In total, the dataset is made of \(\)170k unique transcripts, of which 90k are protein-coding and correspond to \(\)20k unique genes. Our dataset has a fixed train and test set, divided by genes; all presented results correspond to the performance on the test set. We provide more details on the dataset in Appendix A.

HyperparametersWe used the Adam optimizer with a learning rate of \(3 10^{-5}\) and batch size of \(64\), and used early stopping on a validation set comprised of \(5\%\) of the train set to reduce training time. We also made our baseline model's weights available4. More details in Appendix B.

Experiments

We present extensive experiments to assess the performance of our multi-modal approach on the _transcript isoform expression prediction_ task and compare it with existing single-modality approaches. We show that (i) our architecture efficiently aggregates modalities to improve its performance on this task; (ii) by using a tailored model for expression prediction as a base dna encoder, IsoFormer reaches state-of-the-art performance; (iii) we provide an extensive ablation study on different aggregation approaches; (iv) we demonstrate that our approach achieves transfer learning both intra-modalities from their independent pre-training as well as inter-modalities.

### Bridging three foundational models outperforms mono-modal approaches

ExperimentWe investigated the effect of adding the different modalities within our multi-modal framework for the prediction of expression of each rna transcript across different human tissues. We used nt as the foundation model for both dna and rna and esm as the protein encoder. We compared our multi-modal approach (dna + rna + protein) with models trained with different combinations of modalities as input: dna only, rna only, protein only, dna + protein and dna + rna. Results were obtained over 5 random seeds; for each random seed we change the validation set and randomly initialize the non pre-trained parameters \((,)\) of our model. We report both \(R^{2}\), which measures how well each model predicts the actual values of expression, and Spearman correlation across tissues, which is a metric for ranking transcripts based on their expression in each tissue.

ResultsWe observed that our approach benefits from adding more modalities as the performance increases from one modality alone to having two combined, and the best performance is achieved with the three (dna, rna, and protein) modalities together (Table 2). This is true for both Spearman correlation and \(R^{2}\) metrics, with stronger improvement for the latter reflecting a more accurate prediction of the actual values of expression and not just the ranking of transcripts. This is a strong demonstration that our model can aggregate information across modalities to improve performance on this isoform expression task. In addition, we observe increased performance by using dna together with rna compared with dna and protein information. This can be related to the strong importance of the utr regions of the rna sequence in the regulation of its degradation and stability , which affect its final expression level in the cells, that are not captured at the protein level.

### Enformer as dna encoder module to obtain transfer between expression prediction tasks

ExperimentTo showcase the flexibility of IsoFormer towards different modality-specific encoders, we tested replacing nt by the Enformer model  as dna encoder. Enformer has been trained over gene-level expression data obtained from cage assays (one value of expression per gene per tissue), while our model is trained to predict rna transcript expression data obtained from bulk rna-seq assay (one value of expression for each isoform of a given gene per tissue) and therefore represents a different challenge that cannot be tackled from the dna sequence alone. Still, as the Enformer has been trained to predict gene-level expression across tissues directly from dna sequences, a related task to predicting rna isoform expression, one might expect to obtain transfer by using it as dna encoder. We give full details of this experiment in Appendix D.

ResultsWe obtained superior performance using the Enformer instead of nt as a pre-trained dna encoder both as dna-only but also when we combined with the rna and protein encoders (Table 3). Importantly, also with the Enformer our framework benefits from bridging modalities. This improved

 
**Model Input** & \(}\) & **Spearman** \\  dna only & \(0.13 0.02\) & \(0.43 0.01\) \\ rna only & \(0.36 0.03\) & \(0.61 0.01\) \\ Protein only & \(0.20 0.01\) & \(0.46 0.01\) \\  dna + Protein & \(0.28 0.01\) & \(0.52 0.01\) \\ dna + rna & \(0.39 0.01\) & \(0.64 0.01\) \\ dna + rna + Protein & \(\) & \(\) \\  

Table 2: Performance of different variants of IsoFormer for the prediction of transcript isoform expression. \(R^{2}\) and Spearman correlation across tissues for 5 different random seeds is reported. nt is used as both dna and rna encoder while esm is used to process protein sequences.

performance can be explained by the fact that Enformer has been pre-trained on the related task of gene expression prediction and thus its embeddings are better aligned with the isoform prediction task. Moreover, the Enformer is a model that can handle sequences of large context (up to 196k nucleotides), enabling it to capture long-range dependencies, known to be relevant for expression. We also tried the Borzoi model [] as an encoder for DNA model. Borzoi is a model pre-trained on sequences up to 512kb nucleotides to predict RNA-seq coverage which directly relates to RNA transcript isoform expression. These results demonstrate that our multi-modal framework can be improved by leveraging more domain-specific encoders. As our best model for isoform prediction is achieved using the Enformer as dna encoder, we will use it as dna encoder by default for all the following experiments. We make the weights of this IsoFormer model available on HuggingFace5.

InterpretationWe report the performance of IsoFormer across selected tissues in Fig. 3-left (results across all tissues are presented in Appendix D). IsoFormer obtains similar performance across tissues despite tissues having different distributions of expression levels. To gain additional insights about the representations learned by IsoFormer, we analysed the attention layers inside the rna encoder as it is the one providing stronger improvement on this task. Specifically, we compared how the attention distribution within each layer and head changes when we finetune the rna encoder alone versus finetuning IsoFormer altogether. We report changes in attention scores at each layer and head of the rna encoder for three genomics elements known to have a strong effect on the isoform splicing and gene expression processes, namely the 3utr, sutr and cds sequence, see Fig. 3-right (additional details on these scores are in Appendix C.1). The results show that, when finetuning using the three modalities, different layers specialize to capture specific features relative to isoform splicing and expression. Notably, the middle set of layers put higher attention weights to 3utr regions whereas the top layers of nt attributes higher attention weights on cds and sutr. We assume that this rna encoder specialization during finetuning is key to achieve a strong representation towards the prediction of its tissue-specific expression.

### Ablation studies on the aggregation strategy

ExperimentWe compared the IsoFormer's aggregation module with alternative strategies from recent multi-modal literature (Fig. 4). Inspired by recent vision-text models [20; 52; 62; 63], we considered these three approaches: (i) _Perceiver Resampler_: a variant of our cross-attention method using a Perceiver Resampler module. This block learns a fixed number of tokens for each modality, thus reducing the cost of the subsequent cross-attention layer. (ii) _Linear Projection_: a strategy that linearly projects the embeddings of the three modalities into a common representation space and concatenates all tokens. This concatenated sequence is fed to a Perceiver Resampler to

Figure 3: **Left:** Performance of IsoFormer and Enformer  per tissue on a selected subset of tissues. **Right:** Changes in attention in the rna encoder during fine-tuning. These scores are reported for three genomics elements of interest for all heads and layers of the rna encoder.

 
**Model** & \(}\) & **Spearman** \\  Enformer & \(0.21 0.01\) & \(0.46 0.00\) \\  IsoFormer (NT) & \(0.43 0.01\) & \(0.65 0.01\) \\ IsoFormer (Enformer) & \(\) & \(\) \\ IsoFormer (Borzoi) & \(0.48 0.01\) & \(0.69 0.00\) \\  

Table 3: Comparison of Enformer and nt dna encoders used in IsoFormer. \(R^{2}\) and Spearman correlation across tissues on the transcript isoform expression prediction task. Standard deviation across 5 seeds is reported.

learn a fixed number of tokens which are then used in the head of the model. (iii) _C-Abstractor_ a 1-dimensional version of the C-abstractor architecture that provides a compromise between flexibility -choosing an arbitrary number of resampled tokens- and locality preservation .

ResultsWe performed hyperparameters search with the same budget used for the IsoFormer aggregation module for all ablations methods. We report in Table 4 the results obtained with the best set of hyperparameters for each method. We observe that, using an additional step of Perceiver Resampler always performs worse than only cross-attention -even after optimizing hyperparameters. Similarly, C-Abstractor does not confer any benefit over cross-attention; therefore we consider this latter strategy as the optimal aggregation strategy for our method. One advantage of the cross-attention mechanism we use is its interpretability, since it helps understanding which regions of the different modalities are being leveraged to make predictions. These conclusions align with recent multi-modal studies for other modalities .

### IsoFormer leverages knowledge of pre-trained encoders

ExperimentWe showed in previous sections that IsoFormer is able to aggregate information from different biological sequence modalities to formulate high-quality predictions. Here, we investigated to which extent IsoFormer's performance can be attributed to the transfer from each modality-specific encoders' pre-training. We conduct extensive experiment where we compared the performance of IsoFormer trained using all three encoders pre-trained on their respective domains with different IsoFormer model variants with all possible combinations of pre-trained / non-pretrained modality-specific encoder.

ResultsOur results demonstrate that using pre-trained encoders confers a substantial advantage to IsoFormer, as the \(R^{2}\) is substantially larger (0.53) when compared with IsoFormer with none of the encoders pre-trained (0.10; Table 5). This demonstrates that IsoFormer is leveraging the knowledge acquired by each foundation model in the respective domains. However, we observed that when we randomly initialized only the dna or rna encoders, the drop in performance is smaller (0.41 for dna and 0.48 for rna). This suggests that IsoFormer not only leverages intra-modalities pre-training but also inter-modalities transfer. Altogether, these results underpin our approach of relying on initializing IsoFormer with pre-trained encoders, as the information learned during pre-training is transferred and leveraged when considering multi-modal tasks.

 
**Aggregation Method** & \(}\) & **Spearman** \\  Ours & \(0.53 0.01\) & \(0.72 0.00\) \\ PR + Cross-Attn & \(0.49 0.04\) & \(0.69 0.02\) \\ Linear Proj. + PR & \(0.49 0.02\) & \(0.69 0.01\) \\ C-Abstractor & \(0.53 0.01\) & \(0.72 0.01\) \\  

Table 4: Ablation study of different aggregation strategies when considering the three modalities together. All experiments were run using Enformer as the dna encoder. PR = Perceiver Resampler.

Figure 4: Different aggregation strategies compared during the ablation studies. The figures show the specific case for obtaining multi-modal dna embeddings \(^{}_{}\); the same structure is used to obtain multi-modal rna and protein embeddings (\(^{}_{}\) and \(^{}_{}\), respectively). In all cases, the _Resampler_ module is a _Perceiver Resampler_ and the _Mean pooling_ block is the _Adaptive mean pooling_ operator used in .

## 6 Conclusion

IsoFormer is the first model designed for multi-modal biological sequence modeling connecting dna, rna, and protein sequences. IsoFormer achieves state-of-the-art results by effectively leveraging and transferring knowledge from pre-trained dna-, rna-, and protein-specific encoders on one of the significant multi-modal problems in genomics: rna_transcript isoform expression prediction_. As part of our efforts, we are are open-sourcing our model, and hope IsoFormer paves the way to new milestones in building multi-modal models for biology.

 
**DNA** & **RNA** & **Protein** & \(}\) & **Spearman** \\  ✗ & ✗ & ✗ & 0.10 \(\) 0.03 & 0.31 \(\) 0.01 \\ ✓ & ✗ & ✗ & 0.45 \(\) 0.01 & 0.67 \(\) 0.00 \\ ✗ & ✓ & ✗ & 0.39 \(\) 0.01 & 0.61 \(\) 0.00 \\ ✗ & ✗ & ✓ & 0.34 \(\) 0.01 & 0.59 \(\) 0.01 \\ ✓ & ✓ & ✗ & 0.52 \(\) 0.01 & 0.71 \(\) 0.00 \\ ✓ & ✗ & ✓ & 0.48 \(\) 0.01 & 0.69 \(\) 0.00 \\ ✗ & ✓ & ✓ & 0.41 \(\) 0.01 & 0.64 \(\) 0.01 \\ ✓ & ✓ & ✓ & 0.53 \(\) 0.01 & 0.72 \(\) 0.00 \\  

Table 5: Comparing the use of pre-trained and non-pre-trained encoders within IsoFormer. For this set of experiments the considered encoders are the Enformer for dna, nt for rna and esm for proteins. ✓ indicates the use of a pre-trained encoder whereas � indicates the encoder is trained from scratch (random initialization).