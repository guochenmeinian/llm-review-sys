# Interpretable Concept-Based Memory Reasoning

David Debot

KU Leuven

david.debot@kuleuven.be

&Pietro Barbiero

Universita' della Svizzera Italiana

University of Cambridge

barbiero@tutanota.com

Francesco Giannini

Scuola Normale Superiore

francesco.giannini@sns.it

&Gabriele Ciravegna

DAUIN, Politecnico di Torino

gabriele.ciravegna@polito.it

Michelangelo Diligenti

University of Siena

michelangelo.diligenti@unisi.it

&Giuseppe Marra

KU Leuven

giuseppe.marra@kuleuven.be

###### Abstract

The lack of transparency in the decision-making processes of deep learning systems presents a significant challenge in modern artificial intelligence (AI), as it impairs users' ability to rely on and verify these systems. To address this challenge, Concept-Based Models (CBMs) have made significant progress by incorporating human-interpretable concepts into deep learning architectures. This approach allows predictions to be traced back to specific concept patterns that users can understand and potentially intervene on. However, existing CBMs' task predictors are not fully interpretable, preventing a thorough analysis and any form of formal verification of their decision-making process prior to deployment, thereby raising significant reliability concerns. To bridge this gap, we introduce Concept-based Memory Reasoner (CMR), a novel CBM designed to provide a human-understandable and provably-verifiable task prediction process. Our approach is to model each task prediction as a neural selection mechanism over a memory of learnable logic rules, followed by a symbolic evaluation of the selected rule. The presence of an explicit memory and the symbolic evaluation allow domain experts to inspect and formally verify the validity of certain global properties of interest for the task prediction process. Experimental results demonstrate that CMR achieves better accuracy-interpretability trade-offs to state-of-the-art CBMs, discovers logic rules consistent with ground truths, allows for rule interventions, and allows pre-deployment verification.

## 1 Introduction

The opaque decision process of deep learning (DL) systems represents one of the most fundamental problems in modern artificial intelligence (AI). For this reason, eXplainable AI (XAI)  is currently one of the most active research areas in AI. Among XAI techniques, Concept-Based Models (CBMs)  represented a significant innovation that made DL models explainable-by-design by introducing a layer of human-interpretable concepts within DL architectures. CBMs consist of at least two functions: a concept encoder, which maps low-level raw features (e.g. an image's pixels) to high-level interpretable concepts (e.g. "red" and "round"), and a task predictor, which uses the learned concepts to solve a downstream task (e.g. "apple"). This way, each task prediction can be traced backto a specific pattern of concepts, thus allowing CBMs to provide explanations in terms of high-level interpretable concepts (e.g. concepts "red" and "round" were both active when the model classified an image as "apple") rather than low-level raw features (e.g. there were 100 red pixels when the model classified an image as "apple"). In other terms, a CBM's task predictor allows understanding _what_ the model sees in a given input rather than simply pointing to _where_ it is looking .

However, state-of-the-art CBMs' task predictors are either unable to solve complex tasks (e.g. linear layers), non-differentiable (e.g. decision trees), or black-box neural networks. CBMs employing black-box task predictors are still considered _locally_ interpretable, as concept interventions allow humans to understand how concepts influence predictions for individual input examples. However, they lack _global_ interpretability: the human cannot interpret the model's global behaviour, i.e. on any possible instance. This prevents a proper understanding of the model's working, as well as any chance of formally verifying the task predictor decision-making process prior to deployment, thus raising significant concerns in practical applications. As a result, a knowledge gap persists in the existing literature: the definition of a CBM with a task predictor whose behaviour can be inspected, verified, and potentially intervened upon _before_ the deployment of the system.

To address this gap, we propose Concept-based Memory Reasoner (CMR), a new CBM where the behaviour and explanations can be inspected and verified before the model is deployed. CMR's task predictor offers global interpretability as it utilizes a differentiable memory of learnable logic rules, making all potential decision rules transparent to humans. Additionally, CMR avoids the concept bottleneck that often limits the accuracy of interpretable models when compared to black-box approaches. Our key innovation lies in an attention mechanism that dynamically selects a relevant rule from the memory, which CMR uses to accurately map concepts to downstream classes.

**CMR** & **= neural selection over a set of human-understandable decision-making processes** \\  & **Accuracy** & **Interpretability** \\ 

We call this paradigm Neural Interpretable Reasoning (NIR), which involves neurally generating (i.c. selecting from memory) an interpretable model (i.c. a logic rule) and symbolically executing it. Once learned, the memory of logic rules can be interpreted as a disjunctive theory, which can be used for explaining and automatic verification. This verification can take place before the model is deployed and, thus, for any possible input the model will face at deployment time. The concept-based nature of CMR allows the automatic verification of properties that are expressed in terms of high-level human-understandable _concepts_ (e.g. "never predict class 'apple' when the concept 'blue' is active") rather than raw features (e.g. "never predict class 'apple' when there are less than ten red pixels").

Our experimental results show that CMR: (i) improves over the accuracy-interpretability performances of state-of-the-art CBMs, (ii) discovers logic rules matching ground truths, (iii) enables rule interventions beyond concept interventions, and (iv) allows verifying properties for their predictions and explanations _before_ deployment. Our code is available at https://github.com/daviddebot/CMR.

## 2 Preliminary

Concept Bottleneck Models (CBNMs)[4; 10] are functions composed of (i) a concept encoder \(g:X C\) mapping each entity \(x X^{d}\) (e.g. an image) to a set of \(n_{C}\) concepts \(c C\) (e.g. "red", "round"), and (ii) a task predictor \(f:C Y\) mapping concepts to the class \(y Y\) (e.g. "apple") representing a downstream task. For simplicity, in this paper, a single task class is discussed, as multiple tasks can be encoded by instantiating multiple task predictors. When sigmoid activations are used for concepts and task predictions, we can consider \(g\) and \(f\) as parameterizing a Bernoulli distribution of truth assignments to propositional boolean concepts and tasks. For example, \(g_{red}(x)=0.8\) means that there is an 80% probability for the proposition "\(x\) is red" to be true. During training, concept and class predictions \((c,y)\) are aligned with ground-truth labels \((,)\). This architecture and training allows CBNMs to provide explanations for class predictions indicating the presence or absence of concepts. Another main advantage of these models is that, at test time, human experts may also _intervene_ on mispredicted concept labels to improve CBNMs' task performance and extract counterfactual explanations [4; 11]. However, the task prediction \(f\) is still often a black-box model to guarantee high performances, thus not providing any insight into which concepts are used and how they are composed to reach the final prediction.

Model

In this section, we introduce Concept-based Memory Reasoner (CMR), the first concept-based model that is _globally interpretable_, _provably verifiable_ and a _universal binary classifier_. CMR consists of three main components: a concept encoder, a rule selector and a task predictor. CMR's task prediction process differs significantly from traditional CBMs. It operates transparently by (1) selecting a logic rule from a set of jointly-learned rules, and (2) symbolically evaluating the chosen rule on the concept predictions. This unique approach enables CMR not only to provide explanations by tracing class predictions back to concept activations, but also to explain which concepts are utilized and how they interact to make a task prediction. Moreover, the set of learned rules remains accessible throughout the learning process, allowing users to analyse the model's behaviour and automatically verify whether some desired properties are being fulfilled at any time. The logical interpretation of CMR's task predictor, combined with its provably verifiable behaviour, distinguishes it sharply from existing CBMs' task predictors.

### Probabilistic graphical model

In Figure 1, we show the probabilistic graphical model of CMR. There are four variables, three of which are standard in (discriminative) CBMs: the observed input \(x X\), the concepts encoding \(c C\) and the task prediction \(y\{0,1\}\). CMR adds an additional variable: the rule \(r\{P,N,I\}^{n_{C}}\). A rule is a conjunction in the concept set, like \(c_{1} c_{3}\). A conjunction is uniquely identified when, for each concept \(c_{i}\), we know whether, in the rule, the concept is _irrelevant (I)_, _positive (P)_ or _negative (N)_. We call \(r_{i}\{P,N,I\}\) the role of the \(i\)-th concept in rule \(r\). For example, given the three concepts \(c_{1},c_{2},c_{3}\), the conjunction \(c_{1} c_{3}\) can be represented as \(r_{1}=P,r_{2}=I,r_{3}=N\), since the role of \(c_{1}\) is positive (P), the role of \(c_{2}\) is irrelevant (I) and the role of \(c_{3}\) is negative (N).

This probabilistic graphical model encodes the joint conditional distribution \(p(y,r,c|x)\) and factorizes as

\[p(y,r,c|x)=p(y|c,r)p(r|x)p(c|x)\] (1)

and consists of the following components:

* \(p(c|x)\) is the **concept encoder**. For concept bottleneck encoders1, it is simply the product of \(n_{C}\) independent Bernoulli distributions \(p(c_{i}|x)\), whose logits are parameterized by some neural network encoder \(g_{i}:X\). * \(p(r|x)\) is the **rule selector**, described in Section 3.1.1. Given an input \(x\), \(p(r|x)\) models the uncertainty over which conjunctive rule must be used.
* \(p(y|c,r)\) is the **task predictor**, which will be described in Section 3.1.2. Given a rule \(r p(r|x)\) and an assignment of truth values \(c p(c|x)\) to the concepts, the task predictor evaluates the rule on the concepts. In all the cases described in this paper, \(p(y|c,r)\) is a degenerate deterministic distribution.

#### 3.1.1 Rule selector

We model the rule selector \(p(r|x)\) as a mixture of \(n_{R}\) rule distributions. The selector "selects" a rule from a set of rule distributions (i.e. the components of the mixture), and we call this set the _rulebook_. The rulebook is jointly learned with the rest of CMR and can be inspected and verified at every stage of the learning. Architecturally, the selection is akin to an attention mechanism over a differentiable memory .

To this end, let \(s[1,n_{R}]\) be the indicator of the selected component of the mixture, then:

\[p(r|x)=_{s}p(r|s)p(s|x)\] (2)

Figure 1: Probabilistic graphical model of CMR

Here, \(p(s|x)\) is the categorical distribution defining the mixing weights of the mixture. It is parameterized by a neural network \(^{(s)}:X^{n_{R}}\), outputting one logit for each of the \(n_{R}\) components. Each \(p(r|s)\) is a distribution over all possible rules and is modelled as a product of \(n_{C}\) categorical distributions, i.e. \(p(r|s)=_{i=1}^{n_{C}}p(r_{i}|s)\). We assign to each component \(s=j\) a _rule embedding_\(e_{j}^{q}\), and each categorical \(r_{i}\) is then parameterized by a neural network \(^{(r)}_{i}:^{q}^{3}\), mapping the rule embedding to the logits of the categorical component. Intuitively, for each concept \(c_{i}\), the corresponding categorical distribution \(p(r_{i}|s=j)\)_decodes_ the embedding \(e_{j}\) to the three possible roles \(r_{i}\{P,N,I\}\) of concept \(c_{i}\) in rule \(r\). This way, each embedding in the rulebook is the latent representation of a logic rule.2 Lastly, we define the set \(E\) of all rule embeddings, i.e. \(E=\{e_{j}\}_{j[1,n_{R}]}\), as the encoded rulebook.

#### 3.1.2 Task predictor

The CMR task predictor \(p(y|r,c)\) provides the final \(y\) prediction given concept predictions \(c\) and the selected rule \(r\). We model the task predictor as a degenerate deterministic distribution, providing the entire probability mass to the unique value \(y\) which corresponds to the logical evaluation of the rule \(r\) on the concept predictions \(c\). In particular, let \(r_{i}\{P,N,I\}\) be the role of the \(i\)-th concept in rule \(r\). Then, the symbolic task prediction \(y\) obtained by evaluating rule \(r\) on concept predictions \(c\) is:

\[y_{i=1}^{n_{C}}(r_{i}=I)(((r_{i}=P) c_{i}) ((r_{i}=N) c_{i}))\] (3)

Here, the \(y\) prediction is equivalent to a conjunction of \(n_{C}\) different conjuncts, one for each concept. If a concept \(i\) is irrelevant according to the selected rule \(r\) (i.e. \(r_{i}=I\)), the corresponding conjunct is ignored. If \(r_{i}=P\), then the conjunct is True if the corresponding concept is True. Otherwise, i.e. if \(r_{i}=N\), the conjunct is True if the corresponding concept is False.

A graphical representation of the model is shown in Figure 2.

Figure 2: Example prediction of CMR with a rulebook of two rules and three concepts (i.e. \(red(R)\), \(square(S)\), \(table(T)\)). In this figure, we sample (\(\)) for clarity, but in practice, we compute every probability exactly. Every black box is implemented by a neural network, while the white box is a pure symbolic logic evaluation. **(A)** The image is mapped to a concept prediction. **(B)** The image is mapped by the component selector to a distribution over rules. **(C)** This distribution is used to select a rule embedding from the encoded rulebook. **(D)** The rule embedding is decoded into a logic rule by assigning to each of the concepts its role in the rule, i.e. whether it is positive (P), negative (N), or irrelevant (I). Finally, **(E)** the rule is evaluated on the concept prediction to provide the task prediction on the task \(apple\).

## 4 Expressivity, interpretability and verification

In this section, we will discuss the proposed model along three different directions: _expressivity_, _interpretability_ and _verification_.

### Expressivity

An interesting property is that CMR is as expressive as a neural network binary classifier.

**Theorem 4.1**.: _CMR is a universal binary classifier  if \(n_{R} 3\)._

Proof.: Recall that the rule selector is implemented by some neural network \(^{(s)}:X^{n_{R}}\). Consider the following three rules, easily expressible in CMR as showed on the right of each rule:

\[y\ \ (\, i:r_{i}=I), y _{i=1}^{n_{C}}c_{i}\ \ (\, i:r_{i}=P), y _{i=1}^{n_{C}} c_{i}\ \ (\, i:r_{i}=N)\]

By selecting one of these three rules, the rule selector can always make a desired \(y\) prediction, regardless of the concepts \(c\). In particular, to predict \(y=1\), the selector can select the first rule. To predict \(y=0\) when at least one concept has probability less than 50% in the concept predictions \(c\) (i.e. \( i:p(c_{i}|x)<0.50\)), it can select the second rule. Lastly, to predict \(y=0\) when all concepts have probability of at least 50% in \(c\) (i.e. \( i:p(c_{i}|x) 0.50\)), it can select the last rule. 

Consequently, CMR can in theory always achieve the same accuracy as a neural network _without concept bottleneck_, irrespective of which concepts are employed in the model. This distinguishes CMR sharply from CBNMs.

### Interpretability

CMR's task prediction is the composition of a (neural) rule selector and the symbolic evaluation of the selected rule. Therefore, we can always inspect the whole rulebook to know exactly the global behaviour of the task prediction. In particular, let \(s\) be the selected rule, \(e_{j}\) the embedding of the \(j\)-th rule, and \(r_{i}^{(j)}\{P,N,I\}\) the role of the \(i\)-th concept in the \(j\)-th rule at decision time, i.e. \(r_{i}^{(j)}=(_{i}^{(r)}(e_{j}))\). Then, CMR's task prediction can be logically defined as the global rule obtained as the disjunction of all decoded rules, each filtered by whether the rule has been selected or not. That is:

\[y_{j=1}^{n_{R}}(s=j)(_{i=1}^{n_{C}}( r_{i}^{(j)}=I)(((r_{i}^{(j)}=P) c_{i})((r_{i}^{(j)}=N)  c_{i})))\] (4)

It is clear that if the model learns the three rules in the proof of Theorem 4.1, the selector simply becomes a _proxy_ for \(y\). It is safe to say that the interpretability of the selector (and consequently of CMR) fully depends on the interpretability of the learned rules.

**Prototype-based rules.** In order to develop interpretable rules, we focus on standard theories in cognitive science  by looking at those rules that are _prototypical_ of the concept activations on which they are applied to. Prototype-based models are often considered one of the main categories of interpretable models  and have been investigated in the context of concept-based models . However, CMR distinguishes itself from prototype-based models in two significant ways. First, in prototype-based CBMs, prototypes are built directly in the input space, such as images  or on part of it , and are often used to automatically build concepts. However, this approach carries similar issues w.r.t. traditional input-based explanations, like saliency maps (i.e. a prototype made by a strange pattern of pixels can still remain unclear to the user). In contrast, in CMR, prototypes (i.e. rules) are built on top of human-understandable concepts and are therefore interpretable as well. Second, differently from prototype-based models, CMR assigns a logical interpretation to prototypes as conjunctive rules. Unlike prototype networks that assign class labels only based on proximity to a prototype, CMR determines class labels through the symbolic evaluation of prototype rules on concepts. Therefore, prototypes should be representative of concept activations _but also_ provide a correct task prediction. This dual role of CMR prototypes adds a constraint: only prototypes of positive instances (i.e. \(y=1\)) can serve as effective classification rules. Thus, when rules are selected for negative instances (i.e. \(y=0\)), they do not need to be representative of the concept predictions.

**Interventions.** In contrast to existing CBMs, which only allow prediction-time concept interventions, the global interpretability of CMR allows humans to intervene also on the rules that will be exploited for task prediction. These interactions may occur during the training phase to directly shape the model that is being learned, and may come in various forms. A first approach involves the manual inclusion of rules into the rulebook, thereby integrating expert knowledge into the model . The selector mechanism within the model learns to choose among the rules acquired through training and those manually incorporated. As a result, the rules that are being acquired through training can change after adding expert rules. A second approach consists of modifying the learned rules themselves, by altering the role \(r_{i}\) of a concept within a rule. For instance, setting the logit of \(P_{i}\) to 0 ensures that \(c_{i}\) cannot exhibit a positive role in that rule, and setting the logits of both \(P_{i}\) and \(N_{i}\) to 0 ensures irrelevance. This type of intervention could be exploited to remove (or prevent) biases and ensure counterfactual fairness .

### Verification

One of the main properties of CMR is that, at decision time, it explicitly represents the task prediction as a set of conjunctive rules. Logically, the mixture semantics of the selector can be interpreted as a disjunction, leading to a standard semantics in stochastic logic programming [20; 21]. As the only neural component is in the selection and the concept predictions, task predictions generated using CMR's global formula (cf. Section 4.2) can be automatically verified by using standard tools of formal verification (e.g. model checking), no matter which rule _will_ be selected. Being able to verify properties prior to deployment of the model strongly sets CMR apart from existing models, where verification tasks can only be applied at prediction time. In particular, given any propositional logical formula \(\) over the propositional language \(\{c_{1},c_{2},...,c_{n_{C}},y\}\), \(\) can be automatically verified to logically follow from Equation 4. This formula can be converted into a propositional one by (1) evaluating the role expressions (i.e. \(r_{i}^{j}=\) becomes _True_ or _False_), (2) replacing the selection expressions with a new propositional variable per rule (i.e. \((s=j)\) becomes \(s_{j}\)), and (3) adding mutual exclusivity constraints for the different \(s_{j}\). For example, for \(n_{C}=n_{R}=2,r_{1}^{1}=P,r_{2}^{1}=I,r_{1}^{2}=P,r_{2}^{2}=N\), we get:

\[(s_{1} s_{2})(y(s_{1} c_{1})(s_{2} c _{1} c_{2}))\] (5)

with \(\) the xor connective. In logical terms, if the formula \(\) is entailed by such a formula, it means that \(\) must be true every time Equation 4 is used for prediction.

## 5 Learning and inference

### Learning problem

Learning in CMR follows the standard objective in CBM literature, where the likelihood of the concepts and task observations in the data is maximized. Formally, let \(\) be the set of parameters of the probability distributions, such that CMR's probabilistic graphical model is globally parameterized by \(\), i.e. \(p(y,r,c|x;)\). Let \(D=\{(,,)\}\) be a concept-based dataset of i.i.d. triples (input, concepts, task). Then, learning is the optimization problem:

\[_{}_{(,,) D} p(,| {x};)\] (6)

Due to the factorization of the mixture model in the rule selection, CMR has a tractable likelihood computation. In particular, the following result holds.

**Theorem 5.1** (Log-likelihood).: _The maximum likelihood simplifies to the following \(O(n_{C} n_{R})\) objective:_

\[_{}_{(,,) D}(_{i=1}^{n_{C}} p (c_{i}=_{i}|))+(_{=1}^{n_{R}}p(s= {s}|)\,p(y=|,))\] (7)

_with:_

\[p(y=1|c,s)=_{i=1}^{n_{C}}(p(I_{i}|s)+p(P_{i}|s)\,[c_{i}= 1]+p(N_{i}|s)\,[c_{i}=0])\]

_where \([]\) is an indicator function of the condition within brackets._Proof.: See Appendix A 

The maximum likelihood approach only focuses on the prediction accuracy of the model. However, as discussed in Section 4.2, we look for the set of learned rules \(r\) to represent prototypes of concept predictions, as in prototype-based learning . To drive the learning of representative positive prototypes when we observe a positive value for the task, i.e. when \(y=1\), we add a regularization term to the objective. Intuitively, every time a rule is selected for a given input instance \(x\) with task label \(y=1\), we want the rule to be as close as possible to the observed concept prediction. At the same time, since the number of rules is limited and the possible concept activations are combinatorial, the same rule is expected to be selected for different concept activations. When this happens, we will favour rules that assign an irrelevant role to the inconsistent concepts in the activations. The regularized objective is:

\[_{}_{(,,) D}(_{i=1}^{n_{C}} p (c_{i}=_{i}|))+(_{=1}^{n_{R}}p(s=|)\,p(y=|,)(r=|)^{ }}_{})\] (8)

and:

\[p_{reg}(r=|s)=_{i=1}^{n_{C}}(0.5\,p(r_{i}=I|s)+p(r_{i}=P|s)\, [_{i}=1]+p(r_{i}=N|s)\,[_{i}=0])\]

This term favours the selected rule \(r\) to reconstruct the observed \(\) as much as possible. When such reconstruction is not feasible due to the limited capacity of the rulebook, the term will favour irrelevant roles for concepts. In this way, we will develop rules that have relevant terms (i.e. \(r_{i}\{P,N\}\)) only if they are representative of all the instances in which the rule is selected. Appendix B contains an investigation of the influence of this regularization on the optima of the loss.

### Inference

After training, we replace each role distribution \(p(r_{i}|s=j)\) for each concept \(i\) and rule embedding \(j\) with the most likely role. This ensures that each embedding corresponds to a single logic rule rather than a distribution over all possible rules. Moreover, at decision-time, the concepts are unobserved, leading to the following likelihood computation:

\[p(y=1|x)=_{=1}^{n_{R}}p(s=|x)_{i=1}^{n_{C}}([r_{i}=I]+[r_{i}=P]\,p(c_{i}|x)+[r_{i}=N]\,p( c_{i }|x))\] (9)

with \(r_{i}=_{_{i}\{P,N,I\}}p(r_{i}=_{i}|s=)\).

## 6 Experiments

Our experiments aim to answer the following research questions:

1. **Generalization:** Does CMR attain similar task and concept accuracy as existing CBMs and black boxes? Does CMR generalize well when the concept set is incomplete3? 2. **Explainability and Intervenability:** Can CMR recover ground truth rules? Can CMR learn meaningful rules when the concept set is incomplete? Are concept interventions and rule interventions effective in CMR?
3. **Verifiability:** Can CMR allow for post-training verification regarding its behaviour?

### Experimental setting

This section describes essential information about experiments. We provide further details in Appendix C.

**Data & task setup.** We base our experiments on four different datasets commonly used to evaluate CBMs: MNIST+ , where the task is to predict the sum of two digits; C-MNIST, where we adapted MNIST to the task of predicting whether a coloured digit is even or odd; MNIST+*, where we removed the concepts for the digits 0 and 1 from the concept set; CelebA , a large-scale face attributes dataset with more than 200K celebrity images, each with 40 concept annotations4; CUB , where the task is to predict bird species based on bird characteristics; and CEBaB , a text-based task where reviews are classified as positive or negative based on different criteria (e.g. food, ambience, service, etc). These datasets range across different concept set quality, i.e. complete (MNIST+, C-MNIST, CUB) vs incomplete (CelebA, MNIST+*), and different complexities of the concept prediction task, i.e. easy (MNIST+, MNIST+*, C-MNIST), medium (CEBaB) and hard (CelebA, CUB). All the datasets come with full concept annotations.

**Evaluation.** To measure classification performance on tasks and concepts, we compute subset accuracy and regular accuracy, respectively. For CUB, we instead compute the Area Under the Receiver Operating Characteristic Curve  for the tasks due to the large class imbalance. All metrics are reported using the mean and the standard error of the mean over three different runs with different initializations.

**Baselines.** In our experiments, we compare CMR with existing CBM architectures. We consider Concept Bottleneck Models with different task predictors: linear, multi-layer (MLP), decision-tree (DT) and XGBoost (XG). Moreover, we add two state-of-the-art CBMs: Concept Embedding Models (CEM)  and Deep Concept Reasoner (DCR) . We employ hard concepts in CMR and our competitors, avoiding the problem of input distribution leakage that can affect task accuracy [28; 29] (see Appendix C for additional details). Finally, we include a deep neural network without concepts to measure the effect of an interpretable architecture on generalization performance.

We provide an additional experiment serving as an ablation study on CMR's joint rule learning in Appendix C.3.3.

### Key findings & results

#### 6.2.1 Generalization

**CMR's high degree of interpretability does not harm accuracy, which is similar to or better than competitors'**. In Table 1, we compare CMR with its competitors regarding task accuracy. On all data sets, CMR achieves an accuracy close to black-box accuracy, either beating its concept-based competitors or obtaining similar results. In Table 5 of Appendix C, we show that CMR's training does not harm concept accuracy, which is similar to its competitors. Moreover, we provide an experiment showing that CMR's accuracy is robust to the chosen number of rules in Appendix C.3.4.

**CMR obtains accuracy competitive with black boxes even on incomplete concept sets.** We evaluate the performance of CMR on settings with increasingly more incomplete concept sets. Firstly, as shown in Table 1, in MNIST+*, CMR still obtains task accuracy close to the complete setting, beating its competitors which suffer from a concept bottleneck. Secondly, we run an experiment on

    & mnist+ & mnist+* & c-mnist & celeba & cub & cebab \\  CBM+linear & \(0.00 0.00\) & \(0.00 0.00\) & \(99.07 0.31\) & \(49.02 0.20\) & \(50.60 0.69\) & \(45.15 29.93\) \\ CBM+MLP & \(\) & \(72.51 2.42\) & \(\) & \(50.29 0.60\) & \(55.83 0.33\) & \(83.70 0.30\) \\ CBM+DT & \(96.73 0.39\) & \(77.63 0.44\) & \(\) & \(49.60 0.20\) & \(51.83 0.48\) & \(83.15 0.15\) \\ CBM+XG & \(96.73 0.39\) & \(76.54 0.54\) & \(\) & \(50.39 0.24\) & \(\) & \(\) \\ CEM & \(92.44 0.26\) & \(\) & \(99.32 0.11\) & \(\) & \(57.03 0.80\) & \(83.30 1.59\) \\ DCR & \(90.70 1.21\) & \(92.24 1.37\) & \(98.99 0.08\) & \(35.65 1.53\) & \(50.00 0.00\) & \(67.30 0.93\) \\  Black box & \(83.26 8.71\) & \(83.26 8.71\) & \(99.19 0.11\) & \(65.33 0.60\) & \(64.07 0.33\) & \(88.67 0.19\) \\ 
**CMR (ours)** & \(\) & \(\) & \(99.12 0.04\) & \(\) & \(\) & \(\) \\   

Table 1: Task accuracy on all datasets. The best and second best for CBMs are shown in bold (black and purple, respectively).

CelebA where we gradually decrease the number of concepts in the concept set. Figure 3 shows the achieved task accuracies for CMR and the competitors. CMR's accuracy remains high no matter the size of the concept set, while the performance of the competitors with a bottleneck (i.e. all except CEM) strongly degrades.

#### 6.2.2 Explanations and intervention

**CMR discovers ground truth rules.** We quantitatively evaluate the correctness of the rules CMR learns on MNIST+ and C-MNIST. In the former, the ground truth rules have no irrelevant concepts; in the latter, they do. In all runs of these experiments, CMR finds all correct ground truth rules. In C-MNIST, CMR correctly learns that the concepts related to colour are irrelevant for classifying the digit as even or odd (see Table 2).

**CMR discovers meaningful rules in the absence of ground truth.** While the other datasets do not provide ground truth rules, a qualitative inspection shows that they are still meaningful. Table 2 shows two examples for CEBaB, and additional rules can be found in Appendix C.

**Rule interventions during training allow human experts to improve the learned rules.** We show this by choosing a rulebook size for MNIST+ that is too small to learn all ground truth rules. Consequently, CMR learns rules that differ from the ground truth rules. After manually adding rules to the pool in the middle of training, CMR (1) learns to select these new rules for training examples for which they make a correct task prediction, and (2) improves its previously learned rules by eventually converging to the ground truth rules. This is the case for all runs. Table 3 gives some examples of how manually adding rules affects the learned rules. In Appendix C.3.3, we provide an additional experiment with rule interventions, where we add rules extracted from a rule learner. Additionally, as concept interventions are considered a core advantage of CBMs, we show in Appendix C that CMR is equally responsive as competitors, consistently improving its accuracy after concept interventions.

### Verification

**CMR allows verification of desired global properties.** In this task, we automatically verify semantic consistency properties for MNIST+ and CelebA whether CMR's task prediction satisfies some properties of interest. For verification, we exploited a naive model checker that verifies whether the property holds for all concept assignments where the theory holds. When this is not feasible, state-of-the-art model formal verification tools can be exploited, as both the task prediction and the property are simply two propositional formulas. For MNIST+, we can verify that, for each task \(y\), CMR never uses more than one positive concept (i.e. digit) per image. This can be done by

    & \(y_{even} 0(red)(green)\) \\  & \(y_{even} 2(red)(green)\) \\  & \(y_{odd} 3(red)(green)\) \\   & \(y_{neg} food_{g} amb_{g} noise_{g}\) \\  & \(y_{pos} food_{b} amb_{b} noise_{u} noise _{g}\) \\   

Table 2: Selection of learned rules. For brevity in C-MNIST, negated concepts are not shown and irrelevant concepts are shown between parentheses. We abbreviate _good_ to \(g\), _bad_ to \(b\) and _unknown_ to \(u\).

    &  &  \\  \(y_{8}(c_{0,3})(c_{1,5})(c_{0,4})(c_{1,4})\) & \(y_{8} c_{0,3} c_{1,5}\) & \(y_{8} c_{0,4} c_{1,4}\) \\ \(y_{9}(c_{0,8})(c_{1,1})(c_{0,1})(c_{1,8})\) & \(y_{9} c_{0,8} c_{1,1}\) & \(y_{9} c_{0,1} c_{1,8}\) \\ \(y_{9}(c_{0,0})(c_{1,9})(c_{0,2})(c_{1,7})\) & \(y_{9} c_{0,0} c_{1,9}\) & \(y_{9} c_{0,2} c_{1,7}\) \\   

Table 3: Selection of rule interventions and their effect on learned rules. For brevity, negated concepts are not shown, and irrelevant concepts are shown between parentheses.

Figure 3: Task accuracy on CelebA with varying numbers of employed concepts.

verifying one formula per concept \(j\) of digit \(k\): \( y,i j:y c_{k,j} c_{k,i}\). This is also easily verifiable by simply inspecting the rules in Appendix C. Moreover, in CelebA, we can easily verify that \( W\) with the learned rulebook for \(n_{C}=12\) (see Table 10 in Appendix C), as \(\) is a conjunct in each rule that does not trivially evaluate to False.

## 7 Related works

In recent years, XAI techniques have been criticized for their vulnerability to data modifications [30; 31], insensitivity to reparametrizations, , and lacking meaningful interpretations for non-expert users . To address these issues, Concept-based methods [34; 35; 5; 10] have emerged, offering explanations in terms of human-interpretable features, a.k.a. concepts. Concept Bottleneck Models  go a step further by directly integrating these concepts as explicit intermediate network representations. Concept Embeddings Models (CEMs) [7; 8; 11] close the accuracy gap with black-box models through vectorial concept representations. However, they still harm the interpretability, as it is unclear what information is contained in the embeddings. In contrast, CMR closes the accuracy gap by using a neural rule selector coupled with learned symbolic logic rules. As a result, CMR's task prediction is transparent, allowing experts to see _how_ concepts are being used for task prediction, and allowing intervention and automatic verification of desired properties. To the best of our knowledge, there is only one other attempt at analysing CBMs' task prediction in terms of logical formulae, namely DCR . For a given example, DCR _predicts_ and subsequently evaluates a (fuzzy) logic rule. As rules are predicted on a per-example basis, the global behaviour of DCR cannot be inspected, rendering interaction (e.g. adding expert rules) and verification impossible. In contrast, CMR _learns_ (probabilistic) logic rules in a memory, allowing for inspection, interaction and verification.

The use of logic rules by CMR for interpretability purposes aligns it closely with the field of neurosymbolic AI [36; 37]. Here, logic rules [38; 39; 18] or logic programs [40; 22; 21] are used in combination with neural networks through the use of neural predicates . Concepts in CMR are akin to a propositional version of neural predicates. However, in CMR, the set of rules is learned instead of given by the human and direct concept supervision is used for human alignment. While neurosymbolic rule learning methods have been developed, many are constrained by specific assumptions about the nature of the task, limiting their usability to particular datasets or environments (e.g. requiring multitask scenarios  or specific datasets like MNIST ). Additionally, some approaches, unlike ours, explore the rule space in a discrete manner , which is computationally expensive. Furthermore, they do not provide expressivity results, while we show that CMR is a universal binary classifier.

Finally, the relationships with prototype-based models have already been discussed in Section 4.2.

## 8 Conclusion

We propose CMR, a novel Concept-Based Model that offers a human-understandable and provably-verifiable task prediction process. CMR integrates a neural selection mechanism over a memory of learnable logic rules, followed by a symbolic evaluation of the selected rule. This approach enables global interpretability and verification of task prediction properties. Our results show that (1) CMR achieves near-black-box accuracy, (2) discovers meaningful rules, and (3) facilitates strong interaction with human experts through rule interventions. The development of CMR can have significant societal impact by enhancing transparency, verifiability, and human-AI interaction, thereby fostering trust and reliability in critical decision-making processes.

**Limitations and future works.** CMRs are still fundamental models and several limitations need to be explored further in future works. In particular, CMRs focus on positive-only explanations, while negative-reasoning explanations have not been explored yet. Moreover, the same selection mechanism can be tested in non-logic, globally interpretable settings (like linear models). Finally, the verification capabilities of CMR will be tested on more realistic, safety critical domains, where the model can be verified against safety specifications.