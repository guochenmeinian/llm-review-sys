# Search for Efficient Large Language Models

Xuan Shen\({}^{1}\), Pu Zhao\({}^{1}\), Yifan Gong\({}^{1}\), Zhenglun Kong\({}^{2}\), Zheng Zhan\({}^{1}\),

**Yushu Wu\({}^{1}\), Ming Lin\({}^{3}\), Chao Wu\({}^{1}\), Xue Lin\({}^{1}\), Yanzhi Wang\({}^{1}\)**

\({}^{1}\)Northeastern University, \({}^{2}\)Harvard University, \({}^{3}\)Oracle

{shen.xu, yanz.wang}@northeastern.edu

###### Abstract

Large Language Models (LLMs) have long held sway in the realms of artificial intelligence research. Numerous efficient techniques, including weight pruning, quantization, and distillation, have been embraced to compress LLMs, targeting memory reduction and inference acceleration, which underscore the redundancy in LLMs. However, most model compression techniques concentrate on weight optimization, overlooking the exploration of optimal architectures. Besides, traditional architecture search methods, limited by the elevated complexity with extensive parameters, struggle to demonstrate their effectiveness on LLMs. In this paper, we propose a training-free architecture search framework to identify optimal subnets that preserve the fundamental strengths of the original LLMs while achieving inference acceleration. Furthermore, after generating subnets that inherit specific weights from the original LLMs, we introduce a reformation algorithm that utilizes the omitted weights to rectify the inherited weights with a small amount of calibration data. Compared with SOTA training-free structured pruning works that can generate smaller networks, our method demonstrates superior performance across standard benchmarks. Furthermore, our generated subnets can directly reduce the usage of GPU memory and achieve inference acceleration. Code: https://github.com/shawnriecake/search-llm

## 1 Introduction

Large Language Models (LLMs)  are renowned for their exceptional performance across various domains of artificial intelligence research. There is a growing demand for constructing LLMs for extensive applications across a multitude of popular platforms. However, the computational and storage costs have restricted LLMs from deployment on various devices for wide applications. Take the GPT-3 model as an example, with its 175 billion parameters , it requires more than 326GB of memory in FP16 format. This exceeds the memory capabilities of even the most sophisticated GPUs, far surpassing available memory on resource-constrained devices. To address these challenges, a variety of compression techniques focusing on weight optimization have been developed, including weight pruning , quantization , and knowledge distillation . The extensive research in the compression direction indicates the substantial redundancy within LLMs.

Besides optimizing model weights, improving the model architecture is another crucial direction in achieving both high efficiency and superior performance. Numerous works  have studied the Neural Architecture Search (NAS) problem for representative model designs such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). However, the realm of architecture search for LLMs remains unexplored. Though enjoying the potential benefits of discovering highly efficient and well-performing LLM architectures compared with manual designs, searching with traditional NAS methods for LLMs faces significant challenges due to the immense complexity and extensive model size. Furthermore, the convergenceof a randomly initialized architecture to the searched optimal state takes substantial training efforts and resources, intensifying the challenges of searching for efficient LLM architectures.

To tackle the challenges, we propose a training-free architecture search framework that discovers efficient LLM architectures within the original well-trained LLMs. Specifically, to reduce the search cost, we first identify an appropriate initial architecture by computing the importance of weights. Subsequently, an evolution-based algorithm is applied to globally search an efficient subnet starting from the initial subnet. In each generation, mutation and crossover are adopted to generate candidate architectures within the search space. The candidates are evaluated efficiently with a small amount of training samples to assess their effectiveness and select for the next generation. As we start our search from a well-trained LLM instead of randomly initialized models, we propose a mask mutation algorithm to identify the detailed channel indices, rather than just the number of channels in the mutation of traditional NAS [29; 24; 25; 26; 32]. After a few generations with the identified promising LLM architecture, we adopt the deformation algorithm based on the alternating direction method of multipliers (ADMM) [37; 38; 39; 40] to rectify weights in inherited efficient LLM architecture with omitted weights (i.e., non-inherited weights) by leveraging only 128 calibration samples.

As shown in Figure 1, our extensive experiments demonstrate that our method can achieve superior performance than SOTA structured pruning baselines in terms of perplexity and zero-shot accuracy on multiple datasets across various LLM families and model sizes. Particularly, as in Figure 1 (a), only SliceGPT and our method support the OPT model family, and our method outperforms SliceGPT. Additionally, with a 60% inheriting ratio for the LLaMA-7B model on the WikiText2 dataset, our method achieves the best performance with a perplexity of 10.21, compared to 38.27 by LLM-Pruner and 279.52 by SliceGPT, as illustrated in Figure 1 (b). Furthermore, when scaling to LLaMA-13B, both SliceGPT and LLM-Pruner fail, as in Figure 1 (c). Lastly, as in Figure 1 (d), only FLAP and our method support the LLaMA-30B and 65B models, and our method achieves better performance than FLAP. Besides, our implementations on GPUs demonstrate significant memory reduction and inference acceleration. Meanwhile, our approach eliminates the retraining process, relying solely on forward pass for both searching and reformation processes, which maintains a low memory overhead.

Our contributions are summarized below,

**1.** We propose a training-free search framework to identify subnets within LLMs, featuring an importance-aware initialization that significantly reduces the time cost of searching, and an evolution architecture search with special mask mutation and efficient candidate evaluation.

**2.** We propose a reformation algorithm that reconstructs weights by calibrating with only 128 training samples, thereby enhancing the effectiveness of the subnets.

**3.** Experiments indicate that the subnets generated by our method outperform SOTA structured pruning works in terms of perplexity and accuracy on multiple datasets across various LLM families and sizes. The searched subnets can effectively reduce GPU memory and accelerate inference.

## 2 Related Work

### Compression of LLMs

Various compression techniques have been developed to reduce the model size or inference cost of LLMs, including model pruning, quantization, and distillation. Among these, quantization and

Figure 1: Experiment results of perplexity \(\) on WikiText2 dataset with 2048 sequence length.

structured pruning methods are prevalent due to their efficacy in inference acceleration while preserving task performance. Quantization approaches, as explored in works [19; 18; 17], compress models by converting weights to lower bit representations. Besides, structured pruning techniques, including the works [8; 9; 10; 41], remove redundant weights in a structured manner to reduce the total weight count. Specifically, LLM-Pruner  eliminates non-critical coupled structures based on gradient information, while SliceGPT  substitutes each weight matrix with a smaller, dense matrix and reduces the embedding dimension of the network. FLAP  employs structured metrics to prune LLMs and globally optimizes the sparsity ratio with the output feature maps. Despite the advancements, most pruning methods indiscriminately remove heads within the self-attention modules, leading to more significant performance loss due to the inherent input-dependent nature of transformer architectures based on all heads.

### Search for Transformers

NAS has emerged as a pivotal technique for identifying efficient architectures in CNNs (exemplified by EfficientNet ) and transformer-based models (such as BERT  and Vision Transformer ). To mitigate the typical high training costs associated with NAS, innovations such as one-shot and zero-shot NAS methods [26; 29; 25; 24] have been developed, enhancing the efficiency of generating high-performance architectures. In contrast to zero-shot NAS methods, which utilize accuracy predictors to derive optimal architectures, one-shot NAS methods streamline the process by pretraining a comprehensive supernet from which optimal subnets are subsequently selected. Specifically, in the context of transformer-based models, the one-shot NAS approach, as implemented in AutoFormer , involves multiple rounds of supernet training, strategically extending weights along certain dimensions to optimize performance. NASViT  leverages gradient information during supernet training to refine subnet selection and mitigate gradient conflicts, thereby enhancing the effectiveness of generated architectures. The proven efficacy of one-shot NAS for transformer architectures provides a compelling rationale for its application to LLMs, considering that pretrained LLMs can function analogously as supernets. This adaptation holds the potential to significantly advance the development and optimization of LLM architectures, motivating us to refine and enhance the capabilities of these complex models.

## 3 Methodology

### Framework Overview

We show the overview of our search framework in Figure 2. It comprises three key components: search initialization, search pipeline, and weight reformation. First, an initial efficient architecture is constructed layer by layer with a uniform inheriting ratio based on the weight importance. Subsequently, based on the initialization, we conduct a comprehensive search process for the globally efficient architecture with the evolution-based search method. Finally, a reformation method is introduced to enhance the performance of the resulting subnets in LLMs without retraining.

### Search Initialization

**Global search with uniform initialization.** Unlike prior efficient LLM research efforts such as SparseGPT  with a uniform sparsity ratio across all layers, our method leverages a global search approach, such that different layers in our searched architecture may inherit different percentages of parameters (inheriting ratios) from the original LLM. To reduce the search cost and promote the search performance, we initialize our search with the same inheriting ratio for all layers. Through our search process, we iteratively refine the architecture, yielding subnets with varied inheriting

Figure 2: Framework Overview.

ratios across layers. We demonstrate the pivotal role of the initialized architecture in driving search efficiency and effectiveness in Figure 5 and Section 3.3.3.

Structural subnets.To enable efficient inference, we search structural subnets from the original LLMs, i.e., certain rows or columns in the original 2D weight matrix are inherited in our searched model. Take the LLaMA family as an example. In each attention block of LLaMA models, there are query, key, value, and output linear layers in the self-attention module with weights denoted by \(_{Q}\), \(_{K}\), \(_{V}\), and \(_{O}\), respectively, and other three linear layers \(_{U}\), \(_{G}\), and \(_{D}\) in the MLP module. To ensure the consistent hidden size in LLaMA models, based on the computation patterns in each block, we select rows in \(_{Q}\), \(_{K}\), \(_{V}\), \(_{U}\), and \(_{G}\), and columns in \(_{O}\) and \(_{D}\). More details are presented in Figure 3 and Appendix A.

Initialization based on importance score.We construct the initial building blocks by inheriting appropriate rows/columns from the original LLM layer. To determine which row/column to be inherited, we compute the importance score for each row and column as below,

\[[^{r}_{}]_{i}=_{j}[]_{i,j},[^{c}_{ }]_{j}=_{i}[]_{i,j},[]_{i,j}= ]_{i,j}^{2}}{[(2^{T})^{-1}]_{j,j}},\] (1)

where \([^{r}_{}]_{i}\) represents the row score for the \(i^{th}\) row of \(\) and \([^{c}_{}]_{j}\) denotes the column score of the \(j^{th}\) column. \([]_{i,j}\) is the importance value of the element in the \(i^{th}\) row and \(j^{th}\) column of \(\), and \(\) is the layer input. Following WoodFisher  and SparseGPT , the importance score reflects the minimum error of the layer-wise outputs (in terms of \(_{2}\) norm) caused by removing a single weight. Note that the minimum error is evaluated by removing a single element from the weight matrix and it is not optimal in the case of simultaneously removing multiple weights.

Mask sharing.Given the column and row scores, we encode the architecture information by two masks: \(_{attn}^{M}\) for the self-attention module and \(_{mlp}^{P}\) for the MLP module for the layers in each building block. Different layers in the same module (self-attention or MLP) share the same mask to align the internal computations. We consider minimizing the collective importance scores for both the self-attention and MLP modules as below,

\[_{_{attn}}\|_{attn}(^{r}_{_{Q}}+ ^{r}_{_{K}}+^{r}_{_{V}}+^{c}_{_{O}} )\|,\] (2)

\[_{_{mlp}}\|_{mlp}(^{r}_{_{U}}+^ {r}_{_{G}}+^{c}_{_{D}})\|,\] (3)

where \(\|\|\) denotes the \(_{1}\) norm and \(\) means the element-wise multiplication. Given the target model size, we uniformly set the same inheriting ratio for the masks in all building blocks. To obtain the mask in each block, we perform sorting for the sum of the corresponding scores in Equation (2) and (3), and inherit/keep the subnets with larger scores as the initialized architecture with the target size following the inheriting ratio, while other rows/columns with smaller scores are omitted.

### Architecture Search

In this section, we present our comprehensive training-free search framework with the visualization of the search process for one block of the LLaMA model shown in Figure 3. We first delineate the methodology for mutation based on the initialized selection masks. Next, we define the search space and present the search pipeline. Besides, we verify the effectiveness of our initialization strategy by comparing the convergence speeds with and without our initialization.

Figure 3: Visualization of the subnets generation for LLaMA family based on the selections masks \(_{attn}\) for the self-attention module colored in blue and \(_{mlp}\) for the MLP module colored in green.

#### 3.3.1 Mask Mutation

During the search, we use mask mutation to generate new masks and thus new subnets to explore the search space. The inheriting ratio for the selection mask \(_{attn}\) is denoted as \(_{attn}=\{_{attn}^{i}\}_{i=1}^{h}\) where \(h\) is the number of heads, and the inheriting ratio for \(_{mlp}\) is \(_{mlp}\). The mutation function \(\) with the original mask \(_{attn}\) or \(_{mlp}\), mutation probability \(P_{m}\), inheriting ratio requirement \(_{attn}^{i}\) or \(_{mlp}\), similarity ratio \(\), and maximum iteration \(\) can be represented as follows,

\[^{}_{attn}=\{(_{attn}^{i},P_{m}, _{attn}^{i},,)\}_{i=1}^{h},\] (4) \[^{}_{mlp}=(_{mlp},P_{m}, _{mlp},,),\] (5)

where \(_{attn}^{i}^{h_{m}}\) denotes the selection mask for the \(i^{th}\) head and \(h_{m}\) is the head dimension. In details, we show the mask mutation process with Algorithm 1. If the inheriting ratio of input \(\) already satisfies the requirement \(\) and the mutation is unnecessary based on the random generated \(P_{r}\) (i.e., \(P_{r}>P_{m}\)), we do not mutate and simply return \(\). Otherwise, given \(\) and thus the set of indices \(Idx_{1}\) for the inherited rows or columns, we try to generate a new set of indices \(Idx_{2}\) through random sampling between \(0\) to \(()-1\), such that (i) \(Idx_{2}\) follows the required inheriting ratio requirement \(\), and (2) the similarity of \(Idx_{1}\) and \(Idx_{2}\) (intersection set) is larger than threshold \(\).

#### 3.3.2 Search Space

We define the LLM search space with three variables for each transformer building block below: the model depth \(d\), inheriting ratios \(_{attn}=\{_{attn}^{i}\}_{i=1}^{h}\) for \(_{attn}\), and \(_{mlp}\) for \(_{mlp}\). The specifications of this search space, including the range for each factor, are detailed in Table 1.

\(_{mlp}\) has a larger search space than \(\{_{attn}^{i}\}_{i=1}^{h}\) according to our ablation study illustrated in Figure 4. Results are evaluated using LLaMA-7B on the WikiText2 dataset with a sequence length of 2048. Specifically, we apply the same local inheriting ratio for three cases, (i) the attention module only, (ii) the MLP module only, and (iii) both modules. Note that in case (i) or (ii), the global inheriting ratio is larger than case (iii) since the MLP in case (i) or the attention in case (ii) directly uses the original layers with 100% inheriting ratio. From Figure 4, we observe that case (ii) achieves a better perplexity with a lower global inheriting ratio than case (i), demonstrating that the MLP exhibits greater redundancy and is less sensitive to parameter reduction than the self-attention module. Therefore, we set a larger search space of inheriting ratios for MLP than the self-attention module.

Different from other transformer-based search works [29; 26; 46], we do not search the number of heads in self-attention. It stems from the nature of transformers that all heads are essential for representing the input data in the attention mechanism. Moreover, we refrain from conducting searches on the embedding and output layers of LLMs, as their weights constitute only a minor fraction of the total parameters yet are vital for the precise representation of tokens.

#### 3.3.3 Search Pipeline

We implement our evolutionary search across the OPT and LLaMA model families with varying model sizes to derive efficient LLM architectures/subnets. The pipeline is shown below.

   Model &  &  &  \\  Space &  &  &  \\  \# Params. & 125M & 1.3B & 2.7B & 7B & 13B & 30B & 65B & \(\{_{attn}^{i}\}_{i=1}^{h}\) & \(_{mlp}\) \\ 
90\% & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \([0.9,1,0]\) & \([0.6,1,0.05]\) \\
80\% & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \([0.8,1,0.01]\) & \([0.4,1,0.05]\) \\
70\% & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \([0.3,1,0.01]\) & \([0.2,1,0.05]\) \\
60\% & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \([0.6,1,0.01]\) & \([0.1,1,0.05]\) \\
50\% & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \([0.6,1,0.01]\) & \([0.1,1,0.05]\) \\   

Table 1: Search space for different model sizes of OPT model family and LLaMA model family, where the notation \([a,b,c]\) specifies a range from \(a\) to \(b\) with an interval of \(c\).

**Initial generation.** Given the single initialized subnet (Section 3.2), multiple candidates (\(N\) subnets in total) are generated by mutation of the inheriting ratios with probability \(P_{s}^{0}\) and then mask mutation (Section 3.3.1) with probability \(P_{m}^{0}\). The depth mutation is not involved at this initial step. The top \(k\) subnets are preserved as the initial generation.

**Following generation.** With the \(k\) subnets as parental candidates, a new population with \(N\) candidates are generated through mutation and crossover. We select a random parental candidate for mutation until the number of mutation candidates reaches a threshold \(N_{m}\). Mutation involves altering the depth with probability \(P_{d}\), altering the inheriting ratios with probability \(P_{s}<P_{s}^{0}\), and mutating the mask with probability \(P_{m}<P_{m}^{0}\) (see Algorithm 1). The probabilities are smaller than initial generation as superior candidates should be preserved with less randomness. For the crossover, two parental candidates are randomly selected and combined to form a new candidate until there are \(N_{c}\) candidates. With the population from the parental candidates, top \(k\) subnets are preserved as the next generation.

**Candidate evaluation.** For each generated candidate, if its parameter number does not fall in the range of the target model size, it is unsatisfying and we simply drop it. To compare candidate subnets, we evaluate them with a few random training samples from WikiText2 to compute the perplexity.

**Necessity for initialization.** To verify the effectiveness of our initialization in the search, we ablate the initialization for three cases, (i) self-attention only, (ii) MLP only, and (iii) both modules. LLaMA-7B is adopted with an 80% inheriting ratio for selection masks. Results are evaluated on Wikitext2 with a 2048 sequence length. As shown in Figure 5, the self-attention module complicates the identification of an effective subnet without our initialization strategy. In contrast, the MLP module exhibits less sensitivity to initialization. The search in both modules struggles to yield effective subnets without our initialization, primarily due to self-attention. The observation underscores the necessity of our initialization approach.

### Reformation

After the search, we can obtain a subnet from the original LLM. To improve the subnet performance, we further reform the weights in the subnet by using the omitted weights to compensate their loss. Specifically, for each linear layer in the subnet with their original weights \(\) before the search, we would like to reform the weights under the searched mask \(\) and obtain \(}\), so that the layer output difference in \(_{2}\) norm, i.e., \(\|}-\|_{2}^{2}\), is minimized. The problem is formulated below,

\[_{}} \|}-\|_{2}^{2},\] s.t. \[}=,\] (6)

where \(\) indicates the location of pruned weights with element 1 denoting pruned and 0 denoting unpruned weights. Here we only reform inherited columns based on omitted columns in \(\) rather than reforming rows with omitted rows, since the output corresponding to omitted rows are always zeros which are unavailable for any compensations by modifications in other rows. To solve this problem, we propose a solution based on alternating direction method of multipliers (ADMM) [38; 37; 47] with the following theorem. The detailed proof is shown in Appendix B.

**Theorem 3.1**.: _Problem (6) can be solved in iterations. In the \(k^{th}\) iteration, it performs the updates:_

\[}^{k+1}= (^{T}+)^{-1}(^{T}^{T }+(^{k}-^{k})^{T}),\] (7) \[^{k+1}= (}^{k+1}+^{k}) ,\] (8) \[^{k+1}= ^{k}+^{k+1}-^{k+1},\] (9)

_where \(>0\) is the penalty parameter. The initial variable values at \(k=0\) follow the configurations that \(}^{0}=\), \(^{0}=}^{0}\), and \(^{0}=\)._

In practice, we find that after a few tens iterations (such as 20 or 30), the loss in Problem (6) converges. The complexity is determined by the inversion operation, which is the same as SparseGPT . However, as it can converge fast within 30 iterations, our ADMM-based solution typically requires less computation time than SparseGPT which needs to iterate over all rows in the weight matrix.

### Efficient Inference

After searching and reformation, we can get optimal efficient subnets with the selection masks \(_{attn}^{M}\) and \(_{mlp}^{P}\) for each block of the LLMs. We further convert the subnets into small-dense models following the masks for efficient inference. Thus, the dimension of the weight is actually reduced with faster inference speed. More details can be found in Appendix A.

## 4 Experiments

### Experiment Settings

**Hyper-parameter setting.** For the evolutionary search, we adopt specific hyper-parameters as follows: the population size (\(N\)), the number of mutations (\(N_{m}\)), and the number of crossovers (\(N_{c}\)

    &  Inheriting \\ Ratio \\  } &  Wiki \\ PPL\({}_{}\) \\  } &  PTB \\ PPL\({}_{}\) \\  } &  &  &  &  Wino \\ Swag \\  } &  &  &  &  Average \\ Acc.\(\) \\  } \\  LLMA-7B & 100\% & 5.68 & 27.34 & 73.18 & 78.35 & 72.99 & 67.01 & 67.45 & 41.38 & 42.40 & 63.25 \\  LLM-Pruner(v) & & 7.73 & 38.94 & 67.95 & 77.74 & 69.31 & 63.54 & 66.33 & 39.85 & 41.20 & 60.80 \\ LLM-Pruner(e2) & 90\% & 7.46 & 36.87 & 68.29 & 76.88 & 70.25 & 64.33 & 65.28 & 40.10 & 39.60 & 60.68 \\ LLM-Pruner(e1) & & 7.42 & 36.73 & 66.97 & 77.26 & 70.30 & 64.33 & 65.24 & 40.19 & 41.00 & 60.76 \\  SliceGPT & 90\% & 7.00 & 133.80 & 57.68 & 69.80 & 59.32 & 68.11 & 62.75 & 36.01 & 38.00 & 55.95 \\  FLAP & 90\% & 6.34 & 32.39 & 74.43 & 75.41 & 68.68 & 67.01 & 65.78 & 38.48 & 41.00 & 61.54 \\  Ours & 90\% & **6.10** & **32.05** & 74.37 & 76.88 & 70.71 & 67.56 & 68.39 & 40.10 & 39.20 & **62.46** \\  LLM-Pruner(v) & & 10.73 & 59.73 & 61.44 & 71.71 & 57.27 & 54.22 & 55.77 & 33.96 & 38.40 & 53.25 \\ LLM-Pruner(e2) & 80\% & 11.97 & 55.68 & 59.39 & 75.57 & 65.63 & 61.33 & 59.18 & 37.12 & 39.80 & 56.82 \\ LLM-Pruner(e1) & 10.73 & 59.73 & 57.06 & 75.68 & 68.60 & 59.83 & 60.94 & 36.52 & 40.00 & 56.69 \\  SliceGPT & 80\% & 8.71 & 143.89 & 37.89 & 64.09 & 45.67 & 62.75 & 53.62 & 31.74 & 33.20 & 46.99 \\  FLAP & 80\% & 7.40 & 36.77 & 68.59 & 74.21 & 64.98 & 64.40 & 59.89 & 37.80 & 40.20 & 58.58 \\  Ours & 80\% & **6.89** & **36.06** & 70.98 & 74.92 & 67.29 & 64.64 & 64.23 & 36.52 & 39.40 & **59.71** \\   LLMA-13B & 100\% & 5.09 & 19.23 & 68.47 & 78.89 & 76.24 & 70.09 & 74.58 & 44.54 & 42.00 & 64.97 \\  LLM-Pruner(c) & 90\% & 7.70 & 35.32 & 68.47 & 74.76 & 66.99 & 66.38 & 66.58 & 35.24 & 38.20 & 59.52 \\ LLM-Pruner(b) & 90\% & 6.38 & 31.85 & 70.64 & 78.40 & 75.00 & 69.46 & 72.82 & 41.47 & 41.40 & 64.17 \\  SliceGPT & 90\% & 6.43 & 86.09 & 61.74 & 69.97 & 60.74 & 69.38 & 66.79 & 40.70 & 41.80 & 58.73 \\  FLAP & 90\% & 5.45 & 20.98 & 63.76 & 78.07 & 73.69 & 69.61 & 69.53 & 39.93 & 41.60 & 62.31 \\  Ours & 90\% & **5.39** & **20.63** & 71.65 & 78.18 & 75.04 & 69.61 & 69.70 & 43.09 & 42.60 & **64.23** \\  LLM-Pruner(c) & 80\% & 48.76 & 218.49 & 62.39 & 66.87 & 49.17 & 58.96 & 49.62 & 31.83 & 33.20 & 50.29 \\ LLM-Pruner(b) & 10.05 & 55.46 & 67.68 & 77.15 & 73.41 & 65.11 & 68.35 & 38.40 & 42.40 & 61.79 \\  SliceGPT & 80\% & 7.55 & 117.94 & 50.34 & 66.00 & 53.37 & 68.11 & 60.56 & 36.35 & 38.20 & 53.27 \\  FLAP & 80\% & 6.03 & 23.33 & 62.23 & 76.50 & 70.59 & 68.35 & 65.66 & 38.99 & 41.60 & 60.56 \\  Ours & 80\% & **5.90** & **22.66** & 68.53 & 77.09 & 72.60 & 69.22 & 66.25 & 40.02 & 41.00 & **62.10** \\   

Table 2: Results of the compressed LLaMA-7B and LLaMA-13B on the WikiText2 dataset, PTB dataset, and other common sense reasoning datasets. The perplexity on the WikiText2 and PTB is calculated with the 2048 sequence length. The accuracy results are evaluated with the same pipeline as LLM-Pruner  to ensure a fair comparison. The average is computed across seven classification datasets. LLM-Pruner (v), (e2). and (e1) denote the vector-wise and element-wise importance, (c) and (b) denote the channel and block strategies.

are set to 100, 50, and 30, respectively. In each generation, the top \(10\) subnets are selected as parental candidates to produce offspring networks through the mechanisms of mutation and crossover. The rest subnets in the population are generated with mutation with larger randomness (i.e., same as initial mutation). The initial mutation probabilities (\(P_{m}^{0}\) and \(P_{s}^{0}\)) are set at 0.6 and 0.3 to promote variability early in the search process. Subsequently, for ongoing generation processes, the mutation probabilities (\(P_{m}\) and \(P_{s}\)) are adjusted to 0.3 and 0.1, while the probability for depth (\(P_{d}\)) is maintained at 0.1. The similarity ratio \(\) and maximum iteration \(\) are set at 0.8 and 1000 in mask mutation. The total evolution epoch is 50. For the reformation, we adopt \(\) as 1.0 and the iteration number as 30.

**Datasets and metrics.** We compare the perplexity of the models on the WikiText2  and PTB  datasets with the 2048 sequence length. We also compare the zero-shot accuracy on common reasoning zero-shot classification datasets including BoolQ , PIQA , HellaSwag , WinoGrande , ARC-easy , ARC-challenge , and OpenbookQA .

**Models.** We evaluate on multiple LLM families including LLaMA , Vicuna  and OPT .

**Baselines and pipeline.** We compare with SOTA baselines including LLM-pruner , SliceGPT  and FLAP . We adhere to the exact evaluation pipeline from the well-known LLM-Pruner , which is applied for all approaches to ensure a fair comparison. For the reformation, we randomly select 128 samples from training split of WikiText2, with the same random seed and thus same data for the calibration of other works, including SliceGPT and FLAP, ensuring a fair comparison.

**Search Cost.** We leverage the evolution search on NVIDIA A100 40G GPUs. Specifically, to explore the subnets of LLaMA-7B, we finish the search on one GPU with around 5 hours.

### Main Results

**Superior performance compared with SOTA baselines.** We show our results of LLaMA-7B and LLaMA-13B in Table 2 and Figure 1 (b) and (c). We observe that our method outperforms all baselines in terms of perplexity on WikiText2 and PTB, and average zero-shot accuracy (over multiple zero-shot datasets). Take LLaMA-13B on WikiText2 as an example, our method improves the perplexity by 4.15, 1.65, and 0.13 compared to LLM-Pruner(b), SliceGPT, and FLAP, respectively, with a 80% inherting ratio. Meanwhile, it achieves a higher average accuracy on seven classification datasets than baselines. For instance, under a 80% inherting ratio, our method on LLaMA-7B improves the average accuracy by 2.89%, 12.72%, and 1.13% compared to LLM-Pruner(e1), SliceGPT, and FLAP, respectively. As the SliceGPT is sensitive to the calibration dataset, we further show the results with PTB calibration in Appendix C. Besides, we ablate the search with PTB in Appendix D.

    &  Inheriting \\ Ratio \\  } &  Wiki \\ PII. \\  } &  \(\) \\  } &  \(\) \\  } &  Wiki \\ PII. \\  } &  \(\) \\  } &  Wiki \\ PII. \\  } &  \(\) \\  } &  \(\) \\  } &  Wiki \\ PII. \\  } &  \(\) \\ {tabular

[MISSING_PAGE_FAIL:9]

Conclusion and Limitation

In this paper, we propose a training-free search framework to find the optimal subnets inside LLMs. We further propose a reformation algorithm that reconstructs the weights of subnets to enhance the task performance. The experiments show the effectiveness of our proposed method compared to SOTA structured pruning methods. Additionally, we achieve memory reduction and practical inference acceleration on GPUs, which shows the efficiency of our method. The search cost required by our method can increase with the model size, which takes more time for large models.