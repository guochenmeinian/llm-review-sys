# Mars: Situated Inductive Reasoning

in an Open-World Environment

 Xiaojuan Tang\({}^{1,3}\)

xiaojuan@stu.pku.edu.cn

&Jiaqi Li\({}^{3}\)

lijiaqi@bigai.ai

&Yitao Liang\({}^{1,3}\)

yitao1@pku.edu.cn

&Song-chun Zhu\({}^{1,2,3}\)

sczhu@bigai.ai

&Muhan Zhang\({}^{1,3}\)

muhan@pku.edu.cn

&Zilong Zheng\({}^{3,}\)

zlzheng@bigai.ai

\({}^{1}\) Institute for Artificial Intelligence, Peking University

\({}^{2}\) Department of Automation, Tsinghua University

\({}^{3}\) State Key Laboratory of General Artificial Intelligence, BIGAI

###### Abstract

Large Language Models (LLMs) trained on massive corpora have shown remarkable success in knowledge-intensive tasks. Yet, most of them rely on pre-stored knowledge. Inducing new general knowledge from a specific environment and performing reasoning with the acquired knowledge--_situated inductive reasoning_, is crucial and challenging for machine intelligence. In this paper, we design Mars, an interactive environment devised for situated inductive reasoning. It introduces counter-commonsense game mechanisms by modifying terrain, survival setting and task dependency while adhering to certain principles. In Mars, agents need to actively interact with their surroundings, derive useful rules and perform decision-making tasks in specific contexts. We conduct experiments on various RL-based and LLM-based methods, finding that they all struggle on this challenging situated inductive reasoning benchmark. Furthermore, we explore _Induction from Reflection_, where we instruct agents to perform inductive reasoning from history trajectory. The superior performance underscores the importance of inductive reasoning in Mars. Through Mars, we aim to galvanize advancements in situated inductive reasoning and set the stage for developing the next generation of AI systems that can reason in an adaptive and context-sensitive way.

https://marscrafter.github.io

## 1 Introduction

Imagine a scenario: in the United States, you drive on the right side of the road. When you travel to the UK, you might initially find it strange how people drive. However, you soon realize that driving on the left is the norm here and adapt yourself to the new rule. Inductive reasoning, a capacity that identifies underlying rules, mechanisms, or general claims of _unobserved_ experience based on past _observations_, undoubtedly plays a pivot role in scientific discoveries as well as in the conduct of our everyday affairs. Research on the origin and justifications of such inductive aptitude can date back to the 1900s. David Hume, one of the most influential philosophers in human nature, presented a critical dilemma as follows:"Why from this (present) experience we form any conclusion _beyond_ those past instances, of which we have had experience."

-- Hume , _A Treatise of Human Nature_

Hume's words, also known as "The Problem of Induction", imply two fundamental questions of inductive reasoning: \(\) How to summarize and form conclusions from the _present_, and live observations? \(\) Based on summarizations, how to derive _inductive_ conclusions (_i.e._, rules or general claims) beyond past experiences? To answer these two questions, we anticipate two crucial aspects existing in the process of inductive reasoning.

* **Situatedness:** Question \(\) poses a challenge to understand situations dynamically and reason with the present knowledge accordingly, _i.e._, situated reasoning. Cognitive studies also indicate that cognition cannot be separated from context and that learning occurs in a situated activity that encompasses social, cultural, and physical contexts [Brown et al., 1989, Roth and Jornet, 2013, Greene, 1998, Lave and Wenger, 1991].
* **Abstractiveness:** The capability of summarizing observations into abstract "conclusions" that go beyond old experiences, _e.g._, symbols, logics, rules and causal relations, is highlighted in question \(\). Prior research works on inductive reasoning [Zhang et al., 2021a, Raven, 2003, Nye et al., 2020] mostly focus on this side by formalizing such a process within rigorous logical forms and performing evaluations directly based on inductive logical rules.

To cover both aspects, we introduce \(\)**Mars**, a novel interactive environment that aims at benchmarking models' capabilities on **situated inductive reasoning**, in which models are required to quickly derive new general knowledge (rules) from interactions within a specific environment and apply the newly acquired knowledge effectively in a new context, rather than merely storing, retrieving or using pre-existing knowledge. Here, "Mars" is not meant to represent the actual planet Mars. Instead, it symbolically represents a "Martian" environment with knowledge and conditions that differ from commonsense (or "Earth" knowledge). Built on the foundation of Crafter [Hafner, 2021]), an open-world survival game, we modify three categories of the default game mechanisms: terrain, survival settings, and task dependencies (SS2). Sampling from the combinations of the three kinds of mechanism changes, Mars can generate numerous different worlds with distinct properties. In each world, agents need to continuously interact with the environment and accomplish tasks un

Figure 1: \(\)**Mars**, an open-world environment for situated inductive reasoning, involves inductive reasoning through active interaction and applying newly acquired rules to make context-sensitive decisions. First, built on Crafter, we introduce counter-commonsense elements to design Mars. Agents interact with the environment and accumulate historical trajectories. For example, an agent might observe that regardless of time or location, mining stone always yields diamonds; using 2 diamonds can craft a table. Consequently, the agent can induce rules “Mining stone yields diamond” and “Placing table consumes 2 diamonds”. When tasked with making a wooden pickcase, the agent can apply these rules to plan and execute specific actions in different contexts.

til the end of their lifespan. However, they cannot merely leverage their prior knowledge (such as "consuming cows increases health") since these pre-stored "earth" knowledge might no longer apply on Mars. Instead, they have to actively induce the rules of the new world, which provides a valuable testbed for their situated inductive reasoning abilities.

In SS2.3, strict principles govern the design of each sampled new world. These principles ensure resource balance, supply exceeding demand, and the achievability of each task. By adhering to these guidelines, Mars avoids creating a purely fantastical or unstable world, allowing the agents to effectively utilize their extensive prior knowledge.

Our work is closely related to the recent surge of LLM-as-agents (Brown et al., 2020; Zhang et al., 2022; Chowdhury et al., 2023), where LLMs behave as reasoning agents and present impressive capabilities in embodied planning and acting, question answering, machine translation, _etc._(Ahn et al., 2022; Du et al., 2022; Wang et al., 2024; Shinn et al., 2023; Bubeck et al., 2023; Gao et al., 2023; Wang et al., 2023; Mnylov et al., 2018; Wang et al., 2024; Zhang et al., 2024; Cai et al., 2023; Wang et al., 2024; Cai et al.]. However, most of these tasks are rich in world knowledge, allowing LLMs to exploit their vast stored knowledge to perform the tasks instead of reasoning. Recently, some research conduct counter-commonsense experiments through QA tasks (Wu et al., 2023; Saparov and He, 2022; Dasgupta et al., 2022; Tang et al., 2023; Han et al., 2022). They primarily evaluate model's ability to apply some given knowledge (rules) to reason in new context without learning new rules from the given context. Another line of inductive reasoning work (Mirchandani et al., 2023; Kim et al., 2022; Weston et al., 2015; Yang et al., 2022) provides pre-defined evidence (input-output pairs) and evaluates performance on some new input, instead of actively interacting with the environment to collect evidence, inducing new rules, and applying the induced rules in context. Comparisons with relevant tasks and benchmarks are listed in Table 1.

In SS3, we carefully select seven representative worlds with varying difficulty (deviation from commonsense) from our proposed Mars. We then evaluate them using state-of-the-art online reinforcement learning methods and LLM agents. Moreover, inspired by the prior success of reflexion (Shinn et al., 2023), we propose a novel LLM-based pipeline, _induction from reflection_ (IffR), where LLM is forced to engage in a reflective thinking process to induce new game rules. Our findings indicate that current models perform poorly in these settings, highlighting the need for improved situated inductive reasoning skills that go beyond static knowledge application.

## 2 The \(}\) Environment

Mars is designed as an interactive open-world survival game, aiming at evaluating an agent's situated inductive reasoning capability, as depicted in Figure 1. Building on the foundation of Crafter (Hafner, 2021), Mars can strategically alter certain commonsense, including terrain, survival settings and task dependencies, while adhering to certain principles related to resource balance, item quantities, and task achievability.

### Basic Setting: Crafter

Crafter Hafner (2021) is an open-world survival game designed to evaluate a wide range of general abilities, including robust generalization, deep exploration and long-horizon reasoning. In this de

   Datasets & task & type & interactive? & situated? & induction? & evidence & source \\  ARC (Zhu et al., 2019) & q.a. & visual & ✗ & ✗ & ✓ & pre-defined & synthetic \\ MinSemi (2020) & q.a. & visual & ✗ & ✗ & ✓ & pre-defined & synthetic \\ ACRE (Zhu et al., 2021) & q.a. & visual & ✗ & ✗ & ✓ & pre-defined & synthetic \\ List Functions (Zhu et al., 2020, 2021) & q.a. & symbol & ✗ & ✗ & ✓ & pre-defined & human-written \\ RAVEN (Zhu et al., 2021) & q.a. & visual & ✗ & ✗ & ✓ & pre-defined & synthetic \\ DERR (Zhu et al., 2022) & q.a. & language & ✗ & ✗ & ✓ & pre-defined & Wikipedia \\ bAbI-16 (Zhu et al., 2021) & q.a. & language & ✗ & ✗ & ✓ & pre-defined & synthetic \\  STAR (Zhu et al., 2021) & q.a. & visual & ✗ & ✓ & ✗ & - & human activity videos \\ SQA-3D (Zhu et al., 2022) & q.a. & 3D & ✗ & ✗ & ✓ & - & 3D indoor \\ SOR-Bench (Zhu et al., 2021) & q.a. & visual & ✗ & ✓ & ✗ & - & real-world activities \\ IQA (Zhu et al., 2021) & q.a. & visual & ✓ & ✗ & - & indoor \\ MP3D-EQA (Zhu et al., 2021) & q.a. & 3D & ✓ & ✗ & ✗ & - & indoor \\  \(}\) (Ours) & policy & visual1  & ✓ & ✓ & ✓ & open-ended & synthetic \\   

Table 1: Comparison between Mars and related benchmark.

manding environment, the agent (_e.g_., a policy model) is asked to unlock all achievements while ensuring its survival. Each episode generates a unique world featuring diverse terrains such as grasslands, lakes, and mountains, randomly populated with entities like cows, trees, and zombies. The game world is structured on a \(64 64\) grid, yet the agent's observation is restricted to a \(7 9\) grid, with an additional \(2 9\) grid space for displaying inventory and status, making Crafter a partially observed environment. At each step, the agent gathers information about the surrounding terrain, its health, food, drink, energy levels, and inventory. Following this, the agent must select an action from a set of 17 possible actions.

### Modification: From Crafter to Mars

To challenge the agent with an environment that deviates from prior (parametric) knowledge and necessitates situated inductive reasoning, we introduce targeted modifications to typical commonsense elements, classified into three categories (Figure 2):

TerrainTerrain includes two aspects: terrain distribution and terrain effect. In the default Crafter setting, common terrain distributions are predictably arranged, _e.g_., minerals like coal, iron, and diamonds are discovered near stone formations. Terrain effects involve whether a terrain can be traversed and whether doing so benefits or harms the agents health, or even results in death. These terrain characteristics guide the agents exploration strategies and efficiency. We disrupt these norms by altering the distribution and effects of these elements, _i.e_., trees may now grow near sand rather than grass and lava is not hot.

Survival SettingsWe introduce a novel axis of variation in survival dynamics. It mainly involves characteristics of entities like cows, zombies, skeletons, plants (edibility, aggressiveness, proximity effects, mobility) as well as the impact on the agents status level (health, food and drink) when consuming these entities and drink. For example, in Crafter world, cows can enhance the agents food levels upon consumption; in this altered reality, cows may exhibit hostile behaviors.

Task DependencyAgents can collect many resources by mining some materials and use them to build tools and place objects. To this end, we classify them into three kinds of achievements: collecting, placing and crafting. Please refer to Appendix A.5 for more details.

CollectingCollecting involves using a tool to mine items and obtain resources while leaving behind some terrain materials. Modifications include altering resources to visually resemble something else, leading to unexpected outcomes (_e.g_., coal appearing as stone so that mining stone will collect coal instead). Tools for mining are randomly selected (hand, wooden, iron, stone pickaxe), and the leftover materials are randomly sampled. Liquid terrains (water, lava, sand) may leave behind creatures (_e.g_., zombies) with default behaviors.

Placing and CrafteringModifications to placing focus on the ignitability of materials, which is randomized for wood, stone, coal, iron, and diamond. Crafting tables can be made from any material while furnaces, which are used for smelting, must be crafted from non-flammable substances. Regarding crafting achievements, we assume that the names of items often reflect their materials. Thus, we do not alter the raw materials used for tools. Instead, we consider whether a table or fur

Figure 2: Examples of three kinds of modification to commonsense elements. Please refer to Appendix A.5 for more details.

nace is required based on the ignitability of the materials. For items that are ignitable, both a table and a furnace are required, whereas for non-flammable items, a table suffices.

### Principles of new world

While we can sample numerous new worlds following the above procedure, we carefully designed several strict principles so that they are not completely fantastical and are always playable.

* The new world does not introduce additional resources or objects; it only modifies the functions or effects of existing game objects and materials. To ensure playability, we guarantee that each collected item has at least one obtainable method and each tool has a practical use, motivating the agent to engage in crafting. We maintain the same achievements as the default Crafter environment to allow for fair comparisons in subsequent experimental evaluations.
* We adhere to the resource balance principle. For every resource that can be increased by some event, there must be a corresponding event that can decrease the resource, maintaining a balance. For instance, if the agent loses health when attacked by a cow, there should be scenarios where the health level increases, such as eating zombie.
* We also ensure that each achievement is achievable. For example, if mining wood requires a wooden pickaze, but crafting a wooden pickaze requires wood, this creates a deadlock. To prevent such scenarios, we construct an and-or tree and use the depth-first search (DFS) algorithm to verify that each task in the technology tree has a viable path to the root node, confirming the feasibility of each task. Additionally, we also develop an automated program to evaluate terrain distribution, walkable materials, and task dependencies generated by item recipes, ensuring all items are accessible. For example, assuming that coal and stone are not directly traversable, if we place diamonds around the stone (because mining stone is a precondition for mining diamonds based on task dependency and diamonds are not walkable), the agent is unable to reach the stone and complete the "mine stone" task.
* We ensure supply exceeds demand: the quantity of items required for task achievements must be greater than what the world provides. For instance, if wood requires collecting at least five diamonds, but the world does not has enough diamonds. Additionally, our world includes mechanisms for renewable resources, such as mining a tree potentially leaving behind a coal terrain. This dynamic aspect means that the availability of resources cannot be measured statically. To address this, we develop an algorithm that simulates the process of unlocking all achievements within the Tech Tree to test whether the dynamically regenerating resources of the world are sufficient to complete all tasks.

## 3 Evaluation on Mars

### Evaluation Setup

MetricsWe use three evaluation metrics as in Hafner (2021) to assess the performance of models situated inductive reasoning abilities: i) The **reward** metric reflects the agents skills. Each time an agent unlocks an achievement, the reward increases by 1. When an agents health increases or decreases by 1, the reward adjusts by +0.1 or -0.1, respectively. ii) The **success rate** is defined as the proportion of achievements unlocked during the episodes. iii) The **overall score** averages the success rate of the 22 achievements in log-space (to account for differences in their difficulties) as: \(S=(_{i=1}^{N}(1+s_{i}))-1\).

Evaluation worldsIn Mars, we meticulously select seven different worlds, focusing on individual modifications to terrain, survival settings, and task dependency: Terrain, Survival, and Task Dep. respectively; we concurrently modify two types of commonsense rules: Terr. Surv., Terr. Task., and Surv. Task.; as well as all three types simultaneously: All three. We also conduct experiments in the Crafter setting (_i.e._, Default). Configurations of worlds are in Appendix M.

### Baselines

To evaluate Mars, we design (1) RL-based methods: PPO (Schulman et al., 2017), DreamerV3 (Hafner et al., 2023); (2) LLM-based methods: ReAct (Yao et al., 2022), Reflexion (Shinn et al., 2023), revised framework motivated by skill library (Xin et al., 2023; Wang et al., 2023) and (3) our proposed framework induction from reflection. Note that RL-based methods individually train a model for each world with 1 million training steps. They do not truly solve the problem of _quickly adapting to new environments_ in situated inductive reasoning scenarios. Here, we conduct the experiments only to provide the reference. To assess the situated inductive reasoning capabilities of RL-based methods, we also further test different worlds using the DreamerV3 trained in Crafter (Appendix D). Our primary comparison focuses on the LLM-based in-context learning methods. See Appendix B for explanations of why LLMs can be used to evaluate the ability to perform situated inductive reasoning.

**RL-based methods**: **PPO** takes images as input and learns to output actions through policy gradient descent. In our implementation, we use a convolutional neural network (CNN) to parameterize the policy gradient. We use stable baselines3 (Raffin et al., 2021) to conduct the experiment with the default parameters. **DreamerV3**(Hafiner et al., 2024) is a general and scalable algorithm based on world models using fixed hyperparameters with 3 neural networks. It succeeds across domains by accommodating different signal magnitudes and balance terms in their objectives for various domains. We adopt the default parameters provided in the source code2. All agents are trained for 1 million environment steps with reward and tested over 20 independent trials.

**LLM-based methods**: Considering that LLMs cannot accept image inputs, we provide a wrapper that gives text descriptions of gameplay screen, including the coordinates of objects, agent's status and inventory (Appendix A.4). **ReAct**(Yao et al., 2022) interleaves the generation of reasoning traces and task-specific actions. **Reflexion**(Shinn et al., 2023) builds on top of ReAct by incorporating self-reflection, allowing the model to reflect on past experiences. When the historical trajectory exceeds a certain token limit (set to 3896 tokens here), the model is provided with the reward and score in its context for reflective thinking. Based on JARVIS-1 and Voyager (Wang et al., 2023, 2023), we further simplify the framework to adapt to Mars, called **Skill Library** (Appendix C).

### Induction from Reflection (IFR)

Building on the **Skill Library** framework, we further introduce the _induction from reflection_ module in _controller_, as depicted in Figure 3. When the _controller_ finishes a subgoal (including "succeed", "failed" or "timeout"), we force LLM to engage in reflective thinking to induce possible game mechanisms based on the agents historical trajectory. The derived rules are then stored in a _rule library_, which the task proposer, planner, and controller can use.

For Skill Library and Ift, we set the learning episodes to 5. For ReAct and Reflexion, which rely on in-context memory instead of external memory, we restrict them to use a finite context window (10 steps or 3896 tokens trajectory). For all LLM-based methods, we use the GPT-4-0125-preview model (Achiam et al., 2023) through OpenAls API, with a temperature of 0.7. Other hyper-parameters (_e.g._, top_k) are kept at their default settings. The full prompts for all different methods are provided in Appendix L.

### Main Results

Table 2 presents the performance of various methods across different environments. Notably, all baseline models exhibit a performance decline when transitioning from the Default to Mars scenarios, with the extent of the decline dependent on the type (_e.g._, terrain, survival, and task dependency)

Figure 3: **An illustration of the Induction from reflection pipeline for Mars. Given the selected task and the agent’s observation, _planner_ decomposes the task into a sequence of subgoals. _Controller_ then outputs specific actions to accomplish these subgoals. Successful plans are stored in the _skill library_, while failed plans prompt the agent to perform self-explanation and replan. _Rule library_ is updated through reflection on the controller’s execution. By performing inductive reasoning, it saves possible game rules for proposer, planner, and controller using.**

and the number of modifications. This underscores that **Mars presents significant challenges for current methodologies**. Although our proposed method shows some improvement, its suboptimal performance in the "All three" modified world highlights the urgent need for further research in this complex reasoning context.

For RL-based methods, DreamerV3 outperforms most LLM-based methods, likely due to its extensive exploration, having been trained for 1 million steps. However, in the "All three." scenario, DreamerV3 achieves only a 4% score. This suggests that **counter-commonsense modifications introduce additional complexity to the game mechanics**, thereby increasing the learning difficulty for RL-based models and hindering rapid adaptation.

For LLM-based methods, we observe that altering terrain and survival settings has minimal negative impact on the Skill Library model. However, **changing task dependencies significantly degrades performance**. This is particularly evident when the visual appearance of resources is modified (_e.g._, mining stone yields wood)--under the "Task Dep." setting, the Skill Library achieves a reward of 1.5 compared to ReAct's 5.0. This likely occurs because ReAct's step-by-step reasoning is more adaptable than the Skill Library's multi-step planning approach. Additionally, the Skill Library's memory only retains _successful_ subgoal sequences, making it challenging to accurately assess the real mechanisms for task completion. Consequently, this leads to incorrect plans and erroneous exploration paths (Appendix G).

This issue also motivates us to introduce "induction from reflection" in LLM-based controller module. It encourages the controller to reflect on the counter-commonsense situations and further explore the actual game mechanisms. From the results, we observe that models equipped with the induction capabilities outperform those without, highlighting the importance of inductive reasoning in a counter-commonsense environment.

### Further Analysis

We further plot the success rate of unlocking achievements by the Skill Library model, comparing the default world (Crafter) to the "Task. Dep" world in Mars, as shown in Figure 4. Most achievements involving task dependency category (_e.g._, collecting, placing) experience a significant drop in performance. Even tasks related to survival, such as collecting drinks, are slightly affected. The performance for "kill something" tasks is likely impacted due to the difficulty in making a sword. Interestingly, the unlock rate for the "collect diamond" task in the "Task. Dep" world is higher than in the "Default" world. This is because, in the modified world, diamonds can be directly mined

    &  &  &  \\  & & PPO & DreamerV3 & ReAct & Reflexion & Skill Library & Ours \\   & Default & \(1.9^{ 1.4}\) & \(11.5^{ 1.6}\) & \(7.7^{ 1.6}\) & \(6.0^{ 1.7}\) & \(8.0^{ 2.1}\) & \(9.0^{ 2.3}\) \\  & Terrain & \(-0.1^{ 0.6}\) & \(9.3^{ 2.2}\) & \(7.4^{ 2.7}\) & \(6.4^{ 3.0}\) & \(9.5^{ 2.9}\) & \(8.0^{ 3.7}\) \\  & Survival & \(-0.6^{ 0.5}\) & \(8.6^{ 4.1}\) & \(6.4^{ 3.7}\) & \(4.6^{ 3.9}\) & \(7.9^{ 2.9}\) & \(7.7^{ 3.7}\) \\  & Task. Dep & \(2.1^{ 1.2}\) & \(8.8^{ 2.8}\) & \(5.0^{ 2.1}\) & \(3.2^{ 1.6}\) & \(1.5^{ 1.9}\) & \(5.6^{ 2.9}\) \\  & Terr. Surv. & \(0.0^{ 0.7}\) & \(7.1^{ 2.1}\) & \(6.7^{ 2.5}\) & \(4.9^{ 2.5}\) & \(3.0^{ 2.5}\) & \(6.8^{ 1.9}\) \\  & Terr. Task. & \(-0.7^{ 0.3}\) & \(6.6^{ 0.7}\) & \(4.8^{ 2.0}\) & \(5.3^{ 2.5}\) & \(5.5^{ 1.5}\) & \(6.9^{ 1.8}\) \\  & Surv. Task. & \(-0.6^{ 0.4}\) & \(9.6^{ 3.4}\) & \(1.5^{ 1.3}\) & \(1.0^{ 1.6}\) & \(2.3^{ 1.5}\) & \(3.3^{ 1.4}\) \\  & All three. & \(0.1^{ 0.8}\) & \(5.1^{ 1.8}\) & \(0.7^{ 1.6}\) & \(-0.4^{ 0.7}\) & \(-0.5^{ 0.5}\) & \(0.1^{ 0.5}\) \\    & Default & \(1.3^{ 1.7}\) & \(14.2^{ 1.3}\) & \(8.0^{ 1.5}\) & \(5.3^{ 0.9}\) & \(8.3^{ 1.3}\) & \(13.0^{ 2.1}\) \\  & Terrain & \(0.3^{ 0.1}\) & \(13.0^{ 1.6}\) & \(7.6^{ 2.6}\) & \(7.4^{ 1.6}\) & \(11.9^{ 3.4}\) & \(11.8^{ 2.9}\) \\  & Survival & \(0.2^{ 0.0}\) & \(10.8^{ 2.8}\) & \(8.0^{ 0.6}\) & \(5.5^{ 1.7}\) & \(9.7^{ 2.0}\) & \(11.0^{ 3.7}\) \\  & Task. Dep & \(1.7^{ 0.6}\) & \(12.1^{ 1.9}\) & \(4.6^{ 1.6}\) & \(2.2^{ 0.8}\) & \(1.5^{ 0.6}\) & \(6.9^{ 2.5}\) \\  & Terr. Surv. & \(0.4^{ 0.1}\) & \(7.9^{ 1.3}\) & \(7.1^{ 3.0}\) & \(4.7^{ 1.6}\) & \(2.8^{ 0.6}\) & \(6.7^{ 0.8}\) \\  & Terr. Task. & \(0.1^{ 0.1}\) & \(4.2^{ 0.1}\) & \(3.8^{ 0.3}\) & \(5.5^{ 1.7}\) & \(4.1^{ 0.7}\) & \(7.1^{ 2.5}\) \\  & Surv. Task & \(0.1^{ 0.1}\) & \(15.9^{ 2.6}\) & \(1.3^{ 0.2}\) & \(1.1^{ 0.1}\) & \(1.9^{ 0.1}\) & \(2.1^{ 0.4}\) \\  & All three. & \(0.6^{ 0.2}\) & \(4.0^{ 0.3}\) & \(1.0^{ 0.3}\) & \(0.2^{ 0.1}\) & \(0.2^{ 0.0}\) & \(0.6^{ 0.0}\) \\    hand, making it a straightforward, one-step process that is easy to discover through exploration. However, for the more complex two-step task, "place table", which requires using two diamonds, the performance is still poorer. These results again highlight that Mars is challenging for current methods. Next, we conduct experimental analyses on situated reasoning and inductive reasoning separately. More details and case studies are presented in Appendix I.

**Situated reasoning:** We evaluate the situated reasoning abilities of ReAct by providing it with game rules of each world in context. As shown in Line 2 and Line 4 of Table 3, LLMs perform better when provided with necessary rules. However, "Surv. Task. w/ rules" has lower scores than "Default w/ rules", indicating significant challenges in understanding and applying counter-commonsense rules. This observation aligns with findings from previous works (Dasgupta et al., 2022; Tang et al., 2023; Saparov and He, 2022).

**Inductive reasoning:** We further evaluate the benefits of Ift. For induced rules (stored in the rule library) and the ground truth rules (provided in the world configurations) in natural language, we measure the precision of the predicted rules and the recall of the ground truth rules using GPT-4 as an evaluator. The results, shown in Figure 5, indicate that the scores improve as the rule library grows with increased memory episodes. However, the recall score of about 28% indicates that there is still much room for improvement. When analyzing the rule types, it can be found that terrain rules are the easiest to induce, followed by survival setting rules, and finally task dependency rules. The results align with the observations in Table 2--modifying task dependency leads to poorer performance compared to terrain and survival settings, likely due to a larger induction search space.

Figure 4: Success rate of unlocking 22 different achievements in log scale by Skill Library model.

Figure 5: Evaluation of rule library

   Mod. Type & **Score** & **Reward** \\  Default & 8.0\% & 7.7 \\ Default w/ rules & 11.6\% & 7.9 \\  Surv. Task. & 1.3\% & 1.5 \\ Surv. Task. w/ rules & 9.2\% & 4.9 \\   

Table 3: Results of ReAct when provided with game rules.

Related Work

Inductive Reasoning.Inductive reasoning is the ability to infer general principles from specific observations or evidence and apply them to novel situations, which is fundamental to human intelligence (Peirce, 1868). A few researchers have proposed a myriad of tasks to evaluate inductive reasoning in AI. Representative benchmarks include vision-based reasoning (Mirshundani et al., 2023; Kim et al., 2022; Xu et al., 2023; Moskvichev et al., 2023; Zhang et al., 2021, 2019; Barrett et al., 2018; Webb et al., 2020; Hill et al., 2019; Raven, 2003)3, program-based induction (Rule, 2020; Zhang et al., 2021, 2021), natural language-based (Vescon et al., 2015; Yang et al., 2022) and sequence-to-sequence tasks (Nye et al., 2020). These tasks usually consist of 2-5 input-output pairs and a test problem. The goal is to infer the rule (_e.g._, transformation, function) from given examples and apply them to the problem input. Simultaneously, some studies evaluate inductive reasoning capabilities of pretrained large LMs (Gendron et al., 2023; Tang et al., 2023; Xu et al., 2023, 2023; Han et al., 2024; Xu et al., 2023, 2021). Honovich et al. (2022) infer an underlying task from a few demonstrations. Wang et al. (2023); Qiu et al. (2023) proposes hypothesis search and iterative refinement to improve inductive reasoning abilities.

Situated Reasoning.Situated reasoning requires agents to understand the situation and surroundings from a dynamic view, then reasoning and accomplishing complex tasks accordingly. SQA3D (Ma et al., 2022) focuses on situated question answering in 3D scenes, requiring agents to comprehend and localize their position and orientation. STAR (Wu et al., 2024) requires agents understand and abstract the dynamic situations presented in the videos. SOK-Bench (Wang et al., 2024) emphasizes understanding and applying both situated and general knowledge for problem-solving. Other works in embodied question answering place agents in interactive environments, such as MP3D-R2R (Anderson et al., 2018), MP3D-EQA (Wilmans et al., 2019), IQA (Gordon et al., 2018), and EmbodiedQA (Das et al., 2018). These benchmarks and datasets typically rely on **factual knowledge** (which is only specific to the current situation) extracted from surroundings or some pre-existing commonsense knowledge to perform deductive reasoning accordingly. However, Mars introduces counter-commonsense game mechanisms, which not only require a deep understanding of the current situation but also necessitate learning **general rules** through inductive reasoning.

## 5 Conclusion

In this paper, we introduce **Mars**, designed to evaluate models' situated inductive reasoning abilities in adaptive and context-sensitive way. Key components, including terrain, survival settings, and task dependencies, are modified according to certain principles. In Mars, agents are required to actively interact with their surroundings, learn to derive new general knowledge, and perform reasoning using the acquired knowledge. Furthermore, we propose _Induction from Reflection_ method, which compels LLMs to perform inductive reasoning from historical trajectories. This approach has demonstrated better performance compared to other LLM-based methods, underscoring the significance of inductive reasoning in counter-commonsense environments.

Limitations and Future WorkDespite the improved performance of IFR compared to other LLM-based method, the overall performance remains suboptimal. In addition to the model's limitations in identifying the underlying causes of observations, this could be due to the limited exploration time provided by the five episodes and the relatively inefficient exploration process. Future research could focus on enhancing the models exploration efficiency and utilizing induced rules to make more informed guesses. For example, if an agent discovers that lava is walkable and safe, it might hypothesize that water could be dangerous due to resource balance. Additionally, future models could be designed to _automatically_ identify the causes and perform inductive reasoning when encountering a new environment, eliminating the need for enforced induction from historical trajectories.

## 6 Acknowledgements

I express my gratitude to my advisors for their guidance and to my peers for their valuable suggestions. This work is partially supported by the National Key R&D Program of China (2022ZD0160300), the National Natural Science Foundation of China (62376031).