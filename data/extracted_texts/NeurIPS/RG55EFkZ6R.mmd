# An Information-Theoretic Analysis of

Thompson Sampling for Logistic Bandits

 Amaury Gouverneur

KTH Royal Institute of Technology

Stockholm, Sweden

amauryg@kth.se&Borja Rodriguez-Galvez

KTH Royal Institute of Technology

Stockholm, Sweden

borjarg@kth.se&Tobias J. Oechtering

KTH Royal Institute of Technology

Stockholm, Sweden

oech@kth.se&Mikael Skoglund

KTH Royal Institute of Technology

Stockholm, Sweden

skoglund@kth.se

###### Abstract

We study the performance of the Thompson Sampling algorithm for logistic bandit problems, where the agent receives binary rewards with probabilities determined by a logistic function \(( a,)/(1+( a,))\). We focus on the setting where the action \(a\) and parameter \(\) lie within the \(d\)-dimensional unit ball with the action space encompassing the parameter space. Adopting the information-theoretic framework introduced by Russo and Van Roy (2015), we analyze the information ratio, which is defined as the ratio of the expected squared difference between the optimal and actual rewards to the mutual information between the optimal action and the reward. Improving upon previous results, we establish that the information ratio is bounded by \(d\). Notably, we obtain a regret bound in \(O(d)\) that depends only logarithmically on the parameter \(\).

## 1 Introduction

This paper studies the logistic bandit problem, where for \(T\) time steps, an agent selects an action and receives a binary reward with probabilities determined by the logistic function \(( a,)/(1+( a,))\) with slope parameter \(>0\). In this setting, both the action \(a\) and the parameter vector \(\) lie within the \(d\)-dimensional unit ball. The logistic bandit applies to various scenarios, for instance, in personalized advertisement systems, where a platform suggests content to users who provide binary feedback, such as "like" or "dislike" (Chapelle and Li, 2011; Russo and Van Roy, 2017).

The performance, or regret, of algorithms for logistic bandits, has been extensively studied, with significant contributions including analyses of Upper-Confidence-Bound (UCB) algorithms by Filippi et al. (2010), Li et al. (2017) and Faury et al. (2020) as well as the study of Thompson Sampling (TS) by Russo and Van Roy (2014) and Abeille and Lazaric (2017). However, nearly all existing regret bounds exhibit an exponential dependence on the parameter \(\). This dependence is highly unsatisfactory because, in practice, as \(\) increases, it is faster to identify the optimal actions, as the distinction between near-optimal and suboptimal actions becomes more pronounced.

In this work, we focus on the TS algorithm (Thompson, 1933), which, despite its simplicity, has proven to be highly effective across a wide range of problems (Russo et al., 2018; Chapelle and Li, 2011). To analyze the regret of TS, Russo and Van Roy (2015) introduced the concept of the information ratio, a statistic that quantifies the trade-off between the information gained aboutthe parameter and the immediate regret incurred. Dong and Van Roy (2018) conjectured that the information ratio for TS in logistic bandits could be bounded solely by the problem's dimension \(d\), and several studies have since aimed to characterize this ratio for logistic bandits.

Recently, Neu et al. (2022) derived a regret bound of \(O(|( T)})\) on the logistic bandit problem, but their result relies on a worst case TS information ratio bound scaling with the cardinality of the action space \(||\). Dong et al. (2019) provided a bound of \(100d\) on the information ratio for TS when \(<2\). They also suggested, through numerical computations, that this bound holds for larger values of \(\). However, their work has two key limitations. First, they do not provide a rigorous proof for generalizing to larger \(\) values. Second, and more critically, their regret analysis is incomplete as it relies on the rate-distortion bound from Dong and Van Roy (2018), which specifically requires a bound on the _one-step compressed TS_ information ratio; a fundamentally different quantity from the TS information ratio they studied. Notably, their techniques to bound the TS information ratio do not apply to the one-step compressed TS information ratio.

In this work, we address these issues and propose a regret bound that scales only logarithmically with the slope of the logistic function. Our key contributions are as follows:

* We propose an information-theoretic regret bound for infinite and continuous action and parameter spaces that relies on the entropy of the quantized parameter, \((_{})\). This result is achieved by adapting the approaches from Neu et al. (2021) and Gouverneur et al. (2023).
* We present a refined analysis showing that for all \(>0\), the information ratio of TS for logistic bandits is bounded by \(d\), improving upon previous results.
* We establish a bound of \(O(d)\) on the regret of TS. To our knowledge, this is the first bound on logistic bandits that scales only logarithmically on \(>0\) and is independent of the number of actions.

## 2 Problem Setup

We consider a logistic bandit problem, where at each time step \(t\{1,,T\}\), an agent selects an action \(A_{t}\) and receives a binary reward \(R_{t}\{0,1\}\) with probability following a logistic function:

\[(R_{t}=1|A_{t}=a,=)=.\]

Here, \(\) is a known scale parameter, and \( a,\) denotes the inner product of the action vector \(a\) and the unknown parameter \(\). The logistic function, sometimes referred to as the link function, is denoted \(_{}(a,)\). In this setting, the action \(a\) lies within the \(d\)-dimensional Euclidean unit ball, \(_{d}(0,1)\), and the parameter vector \(\) on the \(d\)-dimensional Euclidean unit sphere, \(_{d}(0,1)\). Additionally, we assume the action space \(\) encompasses the parameter space \(\), that is \(\). Note that this ensures that, for each \(\), there exist an action \(a\) equal to \(\), such that \( a,=1\).

Following the Bayesian framework, we assume the parameter vector \(\) is sampled from a known prior distribution \(_{}\). As the reward distribution depends only on the selected action and the parameter, it can be written as \(R_{t}=R(A_{t},)\). The agent's history at time \(t\) is denoted by \(H^{t}=\{A_{1},R_{1},,A_{t-1},R_{t-1}\}\), representing all past actions and rewards observed up to time \(t\).

The goal of the agent is to sequentially select actions that maximize the total accumulated reward, or equivalently, that minimize the total expected regret defined as:

\[[(T)][_{t=1}^{T}R(A^{ },)-R(A_{t},)],\]

where \(A^{}\) is the _optimal action_ for the parameter \(\). We construct the optimal mapping \(^{}()_{a}[R(a, )]\) and define \(A^{}=^{}()\). The expectation is taken over the randomness of the action selection, the reward distribution, and the prior distribution of the parameter \(\).

Since the \(\)-algebras of the history are often used in conditioning, we introduce the notations \(_{t}[][|H^{t}]\) and \(_{t}[][|H^{t}]\) to denote the conditional expectation and probability given \(H^{t}\). Additionally, we define \(_{t}(A^{};R_{t}|A_{t})_{t}[_{ _{t}}(_{R_{t}|H^{t},A^{},A_{t}}|_{R_{t}|H^{t},A_{t}})]\) as the disintegrated conditional mutual information between the optimal action \(A^{}\) and the reward \(R_{t}\) conditioned on the action \(A_{t}\), _given the history \(H^{t}\)._Thompson Sampling, Information ratio, and Quantization

An elegant algorithm for solving bandit problems is the _Thompson Sampling_ algorithm. It works by randomly selecting actions according to their posterior probability of being optimal. More specifically, at each time step \(t\{1,,T\}\), the agent samples a parameter estimate \(_{t}\) from the posterior distribution conditioned on the history \(H^{t}\) and selects the action that is optimal for the sampled parameter estimate, \(A_{t}=^{}(_{t})\). The pseudocode for TS is given in Algorithm 1.

```
1:Input: parameter prior \(_{}\).
2:for\(t=1\)to T do
3: Sample a parameter estimate \(_{t}_{|H^{t}}\).
4: Take the corresponding optimal action \(A_{t}=^{}(_{t})\).
5: Collect the reward \(R_{t}=R(A_{t},)\).
6: Update the history \(H^{t+1}=H^{t}\{A_{t},R_{t}\}\).
7:endfor ```

**Algorithm 1** Thompson Sampling algorithm

Studying the regret of TS in bandit problems, Russo and Van Roy (2015) introduced a key quantity to the analysis, the _information ratio_ defined as the following random variable:

\[_{t}_{t}[R(A^{},)-R(A_{t},)] ^{2}}{_{t}(A^{};R(A_{t},),A_{t})}.\]

This ratio measures the trade-off between minimizing the current squared regret and gathering information about the optimal action. Russo and Van Roy use this concept to provide a general regret bound that depends on the time horizon \(T\), the entropy of the prior distribution of \(A^{}\), and an algorithm- and problem-dependent upper bound \(\) on the average expected information ratio (Russo and Van Roy, 2015, Proposition 1).

A limitation of this approach is that the prior entropy of the optimal action, \((A^{})\), can grow arbitrarily large with the number of actions or get infinite if the action space is continuous. We address this issue with Theorem 1, where we propose a regret bound that depends instead on the entropy of \(_{}\), a quantized version of the parameter \(\). The quantized parameter, defined in Definition 1, is obtained by setting \(_{}\) as the closest approximation for \(\) on an \(\)-net for the metric space \((,)\).

**Definition 1**: _Let the set \(_{}\) be an \(\)-net for \((,)\) with associated projection mapping \(q:_{}\) such that for all \(\) we have \((,q())\). We define the quantized parameter as \(_{} q()\)._

## 4 Main Results

This section presents our main results on the regret of TS for logistic bandits. In Theorem 1, we start by leveraging the previously introduced concepts to derive an information-theoretic regret bound that holds for continuous and infinite parameter spaces. Following this, we state in Proposition 1 our principal contribution, where we prove a bound of \(d\) on the TS information ratio. Combining this result with our regret bound, we derive in Theorem 2, a bound on the expected regret of TS for logistic bandits, which scales as \(O(d)\).

The first theorem we present is an adaptation of (Gouverneur et al., 2023, Theorem 2) and (Neu et al., 2021, Theorem 2) to the logistic bandits setting. It relates the regret of TS to the entropy of the quantized parameter \(_{}\).

**Theorem 1**: _Under the logistic bandit setting with logistic function \(_{}(x)=e^{ x}/(1+e^{ x})\), let \(_{}\) be defined as in Definition 1 for some \(>0\). Assume that the average expected TS information ratio is bounded, \(_{t=1}^{T}[_{t}]\), for some \(>0\). Then, the TS cumulative regret is bounded as_

\[[(T)](_{ })+ T)}.\]

Notably, the above theorem holds for continuous action and parameter spaces and works with bounds on the average expected information ratio of the _"standard"_ TS, instead of the one-step compressed TS as in (Dong and Van Roy, 2018, Theorem 1). This distinction is crucial for effectively controlling the information ratio. We explore this difference in more detail in Appendix B.

The proof of Theorem 1 relies on an approximation of the conditional mutual information \((;R_{t}|A_{t},H^{t})\) as \((_{};R_{t}|A_{t},H^{t})+\) exploiting the fact that, for all \(a\) and \(\), the log-likelihood of \(R(a,)\) is \(\)-Lipschitz with respect to \(\). The proof is presented in Appendix A.

In the following proposition, we present our main contribution, a bound on the information ratio of TS that depends only on the dimension \(d\) of the problem. This result confirms, under the considered setting, and up to a multiplicative factor of \(9\), the conjecture of Dong and Van Roy (2018).

**Proposition 1**: _Under the logistic bandit setting with logistic function \(_{}(x)=e^{ x}/(1+e^{ x})\), let \(_{d}(0,1)\) and \(_{d}(0,1)\) be such that \(\). Then, for all \(>0\), the TS information ratio is bounded as \(_{t}d\)._

The proof of Proposition 1 is presented in Appendix B. At a high level, our proof consists of three parts: a lower bound on the mutual information, an upper bound on the squared expected regret at time \(t\), and an upper bound on a ratio of expected variances by the study of the limit case \(\). A quantity that plays a central role in our analysis is the expected variance of the reward probability \(_{}( A_{t},)\) conditioned on \(\), \(_{t}[_{t}[_{}( A_{t},)| ]]\). It is used as a lower bound on the mutual information and a related quantity shows up in the upper bound on the squared expected regret.

By combining Proposition 1 with Theorem 1, we arrive at our main result: a bound on the expected TS regret that scales as \(O(d)\).

**Theorem 2**: _Under the logistic bandit setting with logistic function \(_{}(x)=e^{ x}/(1+e^{ x})\), let \(_{d}(0,1)\) and \(_{d}(0,1)\) be such that \(\). Then for all \(>0\), the TS regret is bounded as_

\[[(T)] 3d} )}.\]

**Sketch of proof** _After combining Theorem 1 with Proposition 1, we upper bound the entropy \((_{})\) by the cardinality of the \(\)-net to get a regret bound of \(3|)+ T)}\). As the parameter space \(\) is within the Euclidean unit ball, we can use Lemma 8 to control the covering number as \((|_{}|) d(1+2/)\). Finally, setting \(=d/( T)\) and rearranging terms inside the logarithm yields the desired result._

To the best of our knowledge, this is the first regret bound for logistic bandits that scales only logarithmically with the logistic function's parameter \(\) while remaining independent of the number of actions. We note that it is within a factor of \(O()\) of the minimax lower bound \((d)\) from (Dani et al., 2008).

## 5 Conclusion and Future Work

In this paper, we studied the Bayesian regret of the Thompson Sampling algorithm for sequential decision-making problems under uncertainty, focusing on logistic bandit problems with action and parameter spaces in the \(d\)-dimensional unit ball. We improved the state-of-the-art bounds, proving that when the action space \(\) encompasses the parameter space \(\), the information ratio of TS is bounded by \(d\). Using this result, we established that TS expected regret is bounded by \(O(d)\).

A natural direction for future work is to extend our bounds to settings where the action space does not fully encompass the parameter space. This extension requires careful analysis of how well the action space aligns with the parameter space, a property closely related to the _fragility dimension_, \((,)\), introduced by Dong et al. (2019). This quantity is crucial for the analysis of logistic bandits, as Dong et al. (2019) demonstrated that there cannot be an \((,)\)-independent upper bound that is both polynomial in \(d\) and sub-linear in \(T\). In cases where the action space does encompass the parameter space, this fragility dimension is minimal, equal to \(d+1\). However, in problems where this relation is not satisfied and with dimension \(d 3\), the fragility dimension can grow significantly and become as large as the cardinality of the action set. Future research should take this challenge into consideration to develop regret bounds applicable to more general settings.

#### Acknowledgments

We would like to thank Benjamin Van Roy and Yifan Zhu for the insightful conversations.