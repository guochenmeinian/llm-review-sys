# HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models

Rhea Sanjay Sukthanker\({}^{1}\)  Arber Zela\({}^{1}\)  Benedikt Staffler\({}^{3}\)

Aaron Klein\({}^{4}\)  Lennart Purucker\({}^{1}\)  Jorg K. H. Franke\({}^{1}\)  Frank Hutter\({}^{2,1}\)

\({}^{1}\)University of Freiburg, \({}^{2}\)ELLIS Institute Tubingen,

\({}^{3}\)Bosch Center for Artificial Intelligence (BCAI), Germany,

\({}^{4}\)ScaDS.AI, Leipzig,

{sukthank, zelaa, frankej, fh}@cs.uni-freiburg.de

benediktsebastian.staffler@de.bosch.com

###### Abstract

The increasing size of language models necessitates a thorough analysis across multiple dimensions to assess trade-offs among crucial hardware metrics such as latency, energy consumption, GPU memory usage, and performance. Identifying optimal model configurations under specific hardware constraints is becoming essential but remains challenging due to the computational load of exhaustive training and evaluation on multiple devices. To address this, we introduce HW-GPT-Bench, a hardware-aware benchmark that utilizes surrogate predictions to approximate various hardware metrics across 13 devices of architectures in the GPT-2 family, with architectures containing up to 1.55B parameters. Our surrogates, via calibrated predictions and reliable uncertainty estimates, faithfully model the heteroscedastic noise inherent in the energy and latency measurements. To estimate perplexity, we employ weight-sharing techniques from Neural Architecture Search (NAS), inheriting pretrained weights from the largest GPT-2 model. Finally, we demonstrate the utility of HW-GPT-Bench by simulating optimization trajectories of various multi-objective optimization algorithms in just a few seconds.

## 1 Introduction

_Language models (LMs)_ based on the transformer architectures  mark the current state-of-the-art  in most natural language understanding tasks, including text summarization, question-answering and language generation. This has led to a surge in research, with models [9; 13; 66] and training data [33; 44] growing in size. Consequently, inference costs have also risen significantly, making it often challenging to deploy these models in practice. For instance, ChatGPT utilizes over half a million kilowatt-hours of electricity daily, a consumption sufficient to handle approximately two hundred million requests. This energy usage is comparable to that of around 180,000 U.S. households, each consuming approximately twenty-nine kilowatt-hours .

There is a natural trade-off (Pareto frontier) between latency and performance of LLMs. While techniques such as KV-Cache optimization  and pruning [79; 86] have been used to improve inference efficiency, they do not explicitly balance performance and latency. Hence, discovering the inference-optimal frontier of language models is a multi-objective optimization problem , where we are interested in the Pareto set of architectures, i.e. the set of all dominating architectures that have a lower loss value on at least one objective and no higher loss value on any other objective.

Neural architecture search (NAS)  is a powerful framework to derive Pareto-optimal neural network architectures in an automated data-driven way. However, as pointed out by Wan et al. ,training a single language model can require millions of GPU hours, making the use of simple multi-objective NAS strategies, such as NSGA-II , that need to train multiple architectures from scratch, impractical. To foster the development of more efficient NAS methods, surrogate [19; 31; 83; 88] and tabular [18; 39; 72; 84] NAS benchmarks have been proposed - particularly for convolutional networks and image classification tasks. These benchmarks have significantly aided the development of search algorithms to replace manual heuristics. Surrogate benchmarks such as Once-for-all  or HAT  follow the idea of two-stage weight-sharing based NAS , which trains a single _supernet_ subsuming a large number of architectures into a single model, followed by a gradient-free search to select the Pareto optimal _sub-networks_. While some benchmarks focus on natural language understanding tasks, such as machine translation  and speech recognition , the efficacy of these techniques does not directly transfer to architectures for causal language modelling problems and across various hardware metrics (e.g., FLOPS, latency) and devices (e.g., CPUs, GPUs). Therefore, a hardware-aware benchmark for evaluating multi-objective NAS methods is crucial for advancing the design of inference-optimal LM architectures. In this paper, we introduce **HW-GPT-Bench** (see Figure 1 for an overview), a hardware-aware LM benchmark, based on the GPT-2  architecture, for multi-objective NAS across 13 devices, 5 hardware metrics, and 4 model scales. Our contributions include:

* **Benchmark Creation** (Section 3): Establishing a benchmark across small, medium, and large model scales with surrogate supernets, performance predictors, and hardware metric predictors.
* **Faithful Latency and Energy Estimates** (Section 3): Contrary to previous in works in the NAS literature, we use surrogate predictors that provide calibrated predictions and faithfully model the uncertainties inherent in latency and energy profiling.
* **Metric Interaction Analysis and Algorithm Evaluation** (Sections 4 and 5): Studying interaction effects between different hardware and performance metrics, importance of architectural choices and evaluations of various multi-objective optimization algorithms, providing out-of-the-box baselines for future development.

We provide an open-source API 1, for latency and perplexity predictors, the supernetwork weights and different baselines studied, making integration of new methods into this benchmark straightforward. Through HW-GPT-Bench, we aim to accelerate research in hardware-aware NAS for language models, ultimately advancing the development of efficient and high-performing language model architectures.

## 2 Related work

**Structural Pruning, Computational Efficiency, and Neural Architecture Search.** Network pruning , a model compression technique, reduces model complexity with minimal performance loss. Empirical studies [35; 51; 60; 74] have examined the impact of pruning different layers and

Figure 1: **HW-GPT-Bench Overview**. Illustration of the search space (_left_), hardware devices and metrics (_middle_) and multi-objective algorithms (_right_) used in the HW-GPT-Bench framework.

modules in pretrained transformers on validation loss. Techniques such as KV-Caching  and Quantization [4; 32; 46; 70] improve inference time, memory footprint, and energy usage. These methods complement structural pruning and can be incorporated for further speedups. _Structured pruning_ removes structured parameter blocks from pretrained models, while _unstructured pruning_ introduces sparsity by zeroing out unimportant weights. Adaptive pruning methods [3; 23], which prune based on task difficulty, have also been proposed. Recently, Klein et al.  and Sarah et al.  used neural architecture search (NAS)  for automated structural pruning of pretrained LLMs. Similarly Munoz et al.  and Munoz et al.  study, starting from pretrained models and parameter-efficient finetuning for NAS. Training multiple architectures from scratch is computationally expensive [59; 67; 89], so efficient NAS methods employ one-shot models (or supernetworks) [7; 31; 43] as performance proxies by inheriting weights and fine-tuning individual architectures. In our benchmark, we follow this procedure to create perplexity proxies from the largest GPT-2  model. As a second stage, many multi-objective NAS methods [31; 78] run gradient-free search routines efficiently, such as evolutionary strategies, to optimize performance and hardware metrics. Recently, Sukthanker et al.  proposed a method that generates the entire Pareto set in a single stage based on preference vectors of objectives and hardware type.

**(Hardware-aware) NAS Benchmarks.** NAS benchmarks, both tabular and surrogate, emerged due to the high computational costs and reproducibility challenges in developing and evaluating NAS algorithms [18; 49; 84; 87; 88]. _Tabular_ benchmarks [18; 20; 49; 84] evaluate all architectures in the search space upfront, but this becomes infeasible as search spaces and training times grow. To address this, _surrogate_ benchmarks [83; 88] use model-based performance predictors, overcoming the limitations of tabular benchmarks and providing more realistic search space evaluations . One-shot models [10; 29; 31; 78] can also act as surrogates by inheriting weights and evaluating performance on validation sets. Most NAS benchmarks focus on convolutional spaces and computer vision tasks [18; 20; 50; 84], with some targeting natural language processing (NLP) [37; 78] and speech recognition. Given the rising computational costs of training, deploying, and searching models via multi-objective NAS, extended tabular NAS benchmarks now include hardware-specific metrics like FLOPS, on-device latency, and energy consumption [5; 19; 38; 39; 78]. Unlike these benchmarks, HW-GPT-Bench focuses on language modeling with decoder-only transformers [58; 73]. Additionally, our surrogates offer calibrated predictions by modeling the intrinsic heteroscedastic noise in latency and energy usage, rather than relying on single measurements [19; 38; 39].

## 3 HW-GPT-Bench Design Choices

In this section we provide details on design choices, such as the architecture search space, data collection procedure, performance and hardware metrics, as well as the surrogate model types.

### Architecture Search Space

To construct our architecture search space, we pick the GPT-2  language model, which is an autoregressive decoder-only transformer  composed by three primary components (see Figure 1, _left_): (i) **Embedding layers** that map input tokens to learnable representations and encode their position; (ii) **Transformer blocks** stacked multiple times; (iii) a **Prediction head** that predicts the next token for a given sequence. Moreover, each of the transformer blocks consist of: (a) a **Causal Self-Attention Block** that weights the significance of different input tokens (b) a **MLP block** containing two layers that project the input to a higher dimension and back to a lower one. We denote the ratio of the higher projection dimension to the transformer dimension as MLP ratio. In addition, we apply the following enhancements to the original architecture:

* **Rotary positional embeddings (RoPE)**: A form of position embedding that captures absolute positional details using rotation matrices while seamlessly integrating explicit relative positional relationships into the self-attention mechanism. Importantly, RoPE offers several advantages, including adaptability to sequences of varying lengths, diminishing token interactions over greater relative distances, and the ability to enhance linear self-attention via relative positional encoding.
* **Parallel residual**: Following PaLM [13; 77], in contrast to the standard serialized formulation, we use a parallel formulation in each transformer block. Specifically, if \(x\) is the input to the block, the standard and parallel formulations can be written as:

\[y_{serialized}=x+((x+((x))))\]

\[y_{parallel}=x+((x))+( (x))\]

As reported in PaLM , the parallel formulation is faster at larger scales as the MLP and attention input matrix multiplications can be fused.

**Architectural choices.** Consider a search space \(=_{e}_{l}_{h} _{b}_{m}\), obtained by parameterizing the building blocks of the transformer architecture, where \(_{e}:=\{e_{1},e_{2},e_{3}\}\), \(_{l}:=\{l_{1},l_{2},l_{3}\}\), \(_{h}:=\{h_{1},h_{2},h_{3}\}\), \(_{b}:=\{,\}\) and \(_{m}:=\{m_{1},m_{2},m_{3}\}\) correspond to the set of embedding dimension choices, number of layers, number of heads, choice of setting the bias in linear layers and the MLP ratio choices, respectively. Furthermore, we choose the MLP ratio and the number of heads on a per-layer basis, amounting to a search space size of \( 10^{36}\) possible architectures. We represent architecture configurations as a list of integers \(s=\{e,l,h^{1},,h^{l},m^{1},,m^{l},b\}\), where \(e_{e}\), \(l_{l}\), \(h^{l}_{h}\), \(m^{l}_{m}\) and \(b_{b}\). \(h^{l}\) and \(m^{l}\) denote the number of heads and MLP ratio of layer \(l\), respectively. Given a set of \(m\) metrics (objectives) \(=\{y_{m}^{m}:y=f(s),s\}\) e.g.: latency, perplexity, energy, memory consumption etc., a NAS algorithm searches for the (Pareto) optimal architectures, evaluated using these metrics, from the space \(\).

**Four Transformer scales.** Based on the values we assign to the choices in every architectural block, we can obtain arbitrary number of search spaces. In HW-GPT-Bench, we construct 7 such spaces, namely, GPT-S, GPT-M, GPT-L, GPT-S-wide, GPT-M-wide, GPT-L-wide and GPT-XL-wide as defined in Table 1, with the largest model containing 1.55B parameters. Note that in every search space, the _supernetwork_ is the largest possible model, e.g. in GPT-S that would be \(s=\{784,12,12,,4,,\}\).

### Dataset Collection

Building a tabular benchmark for our search spaces with cardinality ranging from \( 10^{12}\) to \( 10^{48}\) is infeasible even for objectives such as latency or energy usage that are faster to measure than performance. Therefore, following Zela et al. , **we sample 10000 unique architectures uniformly at random** from each of the search spaces (GPT-S, -M, -L, -S-wide, -M-wide, -L-wide, -XL-wide), and use observations from these architectures to train our hardware and performance surrogates.

**Performance data.** We evaluate the _perplexity and accuracy_ of the sampled architectures at every scale, by inheriting the weights corresponding to a particular architecture from the _supernetwork_, which subsumes all possible architectures in a single network (see Section 2). Since architectures index the same supernetwork to access their weights, all their individual weights are entangled . Various strategies exist to pretrain the supernetwork, such as random

  
**Supernet Type** & **Embedding Dim.** & **Layer No.** & **Head No.** & **MLP Ratio** & **Bias** & **No. of Archs** & **Supernet Size** \\ 
**GPT-S** &  &  &  &  & [On, Off] & \( 10^{12}\) & 124M \\
**GPT-M** &  &  &  &  & [On, Off] & \( 10^{24}\) & 350M \\
**GPT-L** &  &  &  &  & [On, Off] & \( 10^{36}\) & 774M \\
**GPT-S-wide** &  &  &  &  & [On, Off] & \( 10^{12}\) & 124M \\
**GPT-M-wide** &  &  &  &  & [On, Off] & \( 10^{24}\) & 350M \\
**GPT-L-wide** &  &  &  &  & [On, Off] & \( 10^{16}\) & 774M \\
**GPT-XL-wide** &  &  &  &  & [On, Off] & \( 10^{48}\) & 1.55B \\   

Table 1: HW-GPT-Bench search space. We pretrain 7 supernetworks with different sizes and search space strides: GPT-S, -M and -L, -S-wide, -M-wide, -L-wide, -XL-wide. On each of them we parameterize the dimensionality of the embedding layer, number of stacked layers (transformers blocks), number of self-attention heads and MLP ratio for every active layer, as well as if the bias is on or off.

Figure 2: Empirical Cumulative distribution of different search space subspaces.

sampling, sandwich scheme, and pre-defined structured sampling [36; 85]. Following its effectiveness, as shown by , we employ the _sandwich scheme_, that at every mini-batch training iteration samples the largest, the smallest, and two random architectures from the search space. Similar to , the weights of different sub-networks are tightly coupled with each other and the memory footprint of the supernetwork is the same as the largest network in the space. This allows for extremely efficient training of about \(10^{48}\) (for GPT-XL-wide) architectures by updating multiple architectures that share weights simultaneously. We train the supernetwork with the standard language modeling loss function, which is the average negative log likelihood of all tokens without label smoothing. We use the OpenWebText 2 dataset, split to train and test sets, for training the supernetwork and evaluating individual architectures' perplexity, respectively. We refer the reader to Appendix A for more details on the training pipeline and used hyperparameters. In Figure 2 we plot the _empirical cumulative distribution_ of the 10k architectures evaluated using the pretrained supernetwork weights on the validation set. The green curve represents random sampling in a space of fixed embedding dimension of 1280 in the GPT-L space, the orange curve with the number of layers fixed to 36 as well, and the blue curve with the average number of heads and MLP ratio across layers greater than 16 and 3, respectively (with embedding dimension fixed to 1280 and number of layers to 36). From the cumulative distribution we can see that there is a lot to gain from searching for the right architectural choices instead of randomly sampling.

**Hardware metrics and devices.** In addition to perplexity, we also collect the following hardware-related metrics: (i) **number of parameters** (ii) **FLOPS** (Floating Point Operations) (iii) on-device **latency** (in ms) (iv) on-device **energy consumption** (in kWh) (v) **memory footprint** (in GB). We compute latencies and energies for all 10k sampled architectures on a variety of GPU and CPU types:

* **GPU devices:** NVidia RTX A6000, RTX 2080Ti, RTX 3080Ti, P100, A40, A100 and H100.
* **CPU devices:** Intel Xeon Silver, Xeon Gold, and AMD EPYC 7513, 7452, 7502.

For details on hardware specifications refer to Appendix B. To profile the energy usage, we use CodeCarbon3 on CPU devices and Nvidia's visual profiling tool4 on GPUs. For latency profiling we use the native PyTorch profiler5 on both GPUs and CPUs. For FLOPs, we use the DeepSpeed library6. Furthermore, due to the high intrinsic measurement noise for energy and latencies, to get robust estimates, _we collect up to 10 observations per architecture for latency and up to 50 observations per architecture for energy_ on GPUs. We then use these observations to incorporate the _aleatoric_ uncertainty into the surrogates (see Section 3.3) and estimate the noisy latency and energy distributions more reliably. In all evaluations of every metric, we used a batch size of 8, 4, 1 and 1 for GPT-S, -M, -L and -XL scales respectively, and a sequence length of 1024.

In Figure 3 we show the computed ground-truth perplexity from the trained supernet, latency and energy usage values of all 10000 architectures. We can clearly see the _heteroscedastic noise in the latency and energy measurements_, with an increasing variance as the model perplexity improves. In the same figure, we show the Pareto fronts obtained by randomly sampling an observation (blue line) - 1 out of 10 for latency and 1 out of 50 for energy - while the best and worst possible Pareto fronts (red and black markers, respectively) are obtained by using the best and worst measured value, respectively. These results show that the high observation noise in the data can potentially affect the optimization trajectories of multi-objective algorithms, hence resulting in different Pareto fronts.

### Surrogate Models

**Perplexity and Memory Usage Surrogates.** After pretraining the supernetwork, evaluating thousands of architectures on the test set can still be relatively expensive. To this end, similar to Han et al. , we train a MLP surrogate model on 80% of the collected datapoints, obtained by evaluating the supernetwork, to predict the perplexity given the architecture encoding as input. We also train a MLP surrogate to predict GPU memory usage. Evaluations on the unseen 2000 architectures, yielded a rank correlation of \(>0.90\) for every metric. Refer to Appendix C.1 for more details on the MLP architecture and training hyperparameters.

**Energy and Latency Surrogates.** From our initial collection of energy and latency observations for different architectures, we observe that on-device latencies and energies tend to be _very noisy_, and the median of observations is insufficient to capture this noisy distribution (see Figure 3). Moreover, we empirically observe that the distribution of energies and latencies is often normally distributed with a few outliers. A reliable surrogate model in such a case should not only be performant in terms of accuracy or ranking of the architectures, but on _uncertainty quantification and calibration_ as well. As our surrogate model for predicting per-device latency and energy, we choose **AutoGluon**, the state-of-the-art automated machine learning system for tabular data  that has been shown to outperform (ensembles of) individual model types . Specifically, we train two AutoGluon models on the 80% split of the sampled architectures to predict the first and second moments of the latency and energy distributions. AutoGluon builds stacking ensembles  to further enhance performance while using a portfolio [24; 61] of linear models, neural networks, and decision tree-based models to be robust to outliers and performant across diverse data distributions. To analyze different model choices, we compare AutoGluon to LightGBM  and XGBoost , state-of-the-art tabular regression models [28; 48; 61] as baselines. To enhance performance, for both baselines, we ensemble various configurations of LightGBM and XGBoost, and estimate the first and second moments using the individual baselearners' predictions. In addition, we also evaluate an ensemble mix of scitkit-learn's  Linear Regression, Ridge Regression, and Random Forest . We refer the reader to Appendix C.2 for more details on the surrogate models.

After fitting AutoGluon and the baselines and computing evaluations on the testing data points, we compute various performance and calibration metrics from the _Uncertainty Toolbox_7[14; 71] to quantitatively compare AutoGluon to the other baselines. In Table 2, we report accuracy metrics, such as mean absolute error (MAE), Spearman rank correlation, etc., between predicted mean and true mean of observations (e.g. mean of the 50 energy observations per architecture), averaged

Figure 3: Trade-offs between Energy, Latency, and Perplexity across architectures for different search spaces. The blue curve represents the Pareto front obtained by randomly sampling an observation, while the best and worst possible Pareto fronts (red and black markers, respectively) are obtained by using the best and worst measured value, respectively, for latencies and energies.

[MISSING_PAGE_FAIL:7]

become progressively lower at larger model scales as shown in Figures 63 and 64 of the Appendix, mainly due to higher energy and latency measurement noise. As expected, per-device latency and energy are highly correlated, whilst latency/energy and perplexity are anti-correlated (see Figure 3).

Importance of Architectural Choices.For the collected perplexity data at each scale, to model the function of perplexity dependent on the architectural choices, we assume a power law of the form, for analysis purposes:

\[y= l^{} e^{}(^{l }h^{i}}{l})^{}(^{l}m^{i}}{l})^{ }(b+1)^{},\]

where \(,,,,,\) are data-dependent constants, \(l\) is the number of layers, \(e\) is the embedding dimension, \(m^{i}\) and \(h^{i}\) are the MLP ratio and number of heads on layer \(i\), respectively, and b is the bias. After fitting the power law on the collected 10000 pairs (architecture, perplexity), we obtain the following estimated coefficients:

\[ y =646.234 l^{-0.226} e^{-0.371} m^{-0.100} h^ {-0.076} b^{-0.001},\] \[ y =404.456 l^{-0.104} e^{-0.343} m^{-0.091} h^ {-0.049} b^{-0.005},\] \[ y =280.757 l^{-0.073} e^{-0.309} m^{-0.088} h^ {-0.051} b^{-0.005}.\]

An ordinary-least-squares (OLS) fit on the log-transformed data indicates that all search dimensions are statistically significant with p-value \(<0.001\), with embedding dimension being the most important architecture parameter in the search space. The number of layers \(l\) and MLP ratio become increasingly less important at larger scales and could potentially be pruned without significantly impacting perplexity. The importance of bias stays more or less constant across scales, while the MLP ratio is more important than the number of heads, indicating that a significant number of heads are possibly redundant and are amenable to pruning. In Figure 5, we study the ranking upon applying Recursive Feature Elimination (RFE)  to the architecture features and present the 10 top-ranked features. We observe that embedding dimension and number of layers are important across different model scales. Furthermore, the MLP ratio and the number of heads chosen in the transformer's later layers

Figure 5: Feature ranking of architecture dimensions at different scales (lower rank is better). The embedding dimension (in red) is most important across scales and the number of layers (in yellow) is more important at smaller scales. MLP ratio (mr) and Number of heads (nh) at layer \(N-1\) is important across different scales (depicted in blue and green).

Figure 6: Different multi-objective NAS baselines on RTX2080Ti and H100 energy, Xeon Gold CPU Latency and CPU AMD 7452 Latency for GPT-L

(e.g., layer \(N-1\) and layer \(N-2\)) are more important than their choices in earlier layers. We present additional results in Appendix F.

## 5 HW-GPT as a Benchmark for Multi-objective Optimization

In this section, we showcase how HW-GPT-Bench can be used as a benchmark for evaluating multi-objective NAS algorithms.

**Multi-objective NAS algorithms.** State-of-the-art multi-objective NAS (MO-NAS) methods aim to identify Pareto-optimal configurations that balance performance and efficiency. HW-GPT-Bench, enables simulations of their optimization trajectories _in just a few minutes_ using the predictions from our surrogate models. We simulate multiple runs of the following MO-NAS methods implemented in Syne Tune : (i) Random Search (RS) (ii) Multi-objective Regularized Evolution (MOREA)  (iii) Non-dominated Sorting Genetic Algorithm II (NSGA-II)  (iv) Local Search (LS) (v) Multi-objective Asynchronous Successive Halving (MOASHA)  (vi) Bayesian Optimization with Random Scalarizations (RSBO) and (vii) Linear Scalarizations (LSBO)  (viii) Expected Hypervolume Improvement (EHVI) . Refer to Appendix K for a more detailed description of them.

### Experiments with 2 objectives

We run all MO-NAS methods for a fixed (simulated) time budget of 16 CPU hours. We repeat each run 4 times to account for the noise in the latency and energy predictions. In Figure 8 shows the hypervolume indicator over number of surrogate evaluations. Notably, EHVI and NSGA-II achieve a higher hypervolume under smaller budgets compared to Local Search (LS) and Random Search (RS), underscoring the efficiency of model-based optimization algorithms in navigating the search space and identify Pareto-optimal architectures. To aggregate the resulting Pareto fronts from our multiple runs, we use the Empirical Attainment Function (EAF) , which represents a coherent way to capture the uncertainty in the multi-objective metric space (see Appendix G for details). We show the results of all methods in Figure 6. We can see that LS and NSGA-II generally yield more favorable trade-offs between the objectives, achieving lower perplexity for a given energy, latency or memory consumption. In contrast, Random Search (RS) shows wider variability, with worst case scenarios containing solutions in the Pareto set with relatively high energy usage and low perplexity. Appendix L.1 contains the rest of the experimental results across devices and metrics.

### Experiments with more than 2 objectives

All the methods we run on HW-GPT-Bench on 2 objectives in the section above, are extendable to more than 2 objectives. HW-GPT-Bench enables optimizing for different hardware metrics, not necessarily measured on the same hardware device. Here, we showcase this by optimizing simultaneously for perplexity, latency on RTX 2080Ti and energy usage on the A6000 GPU. We picked these objectives due to their relatively low correlation, as we can see in Figure 62. We run the

Figure 8: HV of blackbox optimizers over 200 surrogate evaluations on HW-GPT-Bench for RTX2080 Energy.

Figure 7: HV of MO-NAS methods on 3 objectives (on GPT-S), namely, perplexity, RTX 2080Ti latency and A6000 energy usage.

baselines for 16 hrs on a single CPU and compute the hypervolume 8, which we show in Figure 7 (see Appendix L.2 for the other results.). Notably, NSGA-II achieves good trade-offs also in the 3-objective case, however, the Bayesian optimization methods perform worse than RS and LS here.

With this set of baselines and evaluations, we hope that HW-GPT-Bench will serve as a de-facto testbed for prototyping and benchmarking future hardware-aware optimization methods.

### HW-GPT-Bench API

We provide a minimal API for users, that enables loading the benchmark and querying all or specific hardware and performance metrics across devices, and different search spaces with just a few lines of code.

Users can also select which perplexity surrogate to use - either the supernetwork itself or the MLP performance predictor - ensuring flexibility in performance assessment. Furthermore, the API supports the execution of multi-objective baselines, enabling rigorous benchmarking and comparative analysis. See code snippet 1 for a simple example. Additional examples can be found in Appendix O.

## 6 Conclusions, Broader Impact and Implications

We introduce HW-GPT-Bench, a hardware-aware surrogate benchmark for evaluating language models across various **hardware devices**, **metrics**, and **scales** on a single CPU _in just a few seconds_. By enabling efficient exploration of multi-objective NAS algorithms to achieve Pareto-optimal configurations across multiple hardware metrics and devices, our work has several broader societal implications: (i) **Energy Efficiency and Environmental Impact** - It promotes the development of energy-efficient language models, mitigating the environmental cost of large-scale AI systems and enhancing sustainability; (ii) **Enhanced Research and Development** - It accelerates research in NAS and structural pruning, leading to more energy-efficient architectures; (iii) **Accessibility and Democratization of AI** - Resource-efficient language models enable innovation for users and organizations with limited resources; (iv) **Economic Benefits** - Optimizing for hardware efficiency reduces training and deployment costs, benefiting industries reliant on extensive language model querying and improving user experience.

Overall, HW-GPT-Bench addresses critical challenges in developing and deploying algorithms that enhance the resource efficiency of language models, providing a more sustainable, accessible, and reliable benchmarking framework. It underscores the importance of considering hardware efficiency constraints alongside performance in advancing language models.