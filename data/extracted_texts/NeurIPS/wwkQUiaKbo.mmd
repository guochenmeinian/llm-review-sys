# Adapting Fairness Interventions to Missing Values

Raymond Feng\({}^{1}\), Flavio P. Calmon\({}^{1}\), Hao Wang\({}^{2}\)

\({}^{1}\)John A. Paulson School of Engineering and Applied Sciences, Harvard University

\({}^{2}\)MIT-IBM Watson AI Lab

raymond_feng@college.harvard.edu, flavio@seas.harvard.edu, hao@ibm.com

###### Abstract

Missing values in real-world data pose a significant and unique challenge to algorithmic fairness. Different demographic groups may be unequally affected by missing data, and the standard procedure for handling missing values where first data is imputed, then the imputed data is used for classification--a procedure referred to as "impute-then-classify"--can exacerbate discrimination. In this paper, we analyze how missing values affect algorithmic fairness. We first prove that training a classifier from imputed data can worsen the achievable values of group fairness and average accuracy. This is because imputing data results in the loss of the missing pattern of the data, which often conveys information about the predictive label. We present scalable and adaptive algorithms for fair classification with missing values. These algorithms can be combined with any preexisting fairness-intervention algorithm to handle missing data while preserving information encoded within the missing patterns. Numerical experiments with state-of-the-art fairness interventions demonstrate that our adaptive algorithms consistently achieve higher fairness and accuracy than impute-then-classify across different datasets.

## 1 Introduction

Missing values exist in many datasets across a wide range of real-world contexts. Sources of missing values include underreporting in medical information , structural missingness from data collection setups , and human error. Missing data poses a challenge to algorithmic fairness because the mechanisms behind missing values in a dataset can depend on demographic attributes such as race, sex, and socioeconomic class. For example, in housing data, non-white housing applicants or sellers may be less likely to report their race due to concerns of discrimination . Real-world data collection and aggregation processes can also lead to disparate missingness between population groups. If missing data is not handled carefully, there is a high risk of discrimination between demographic groups with different rates or patterns of missing data. For example, low-income patients can have more missing values in medical data involving the results of costly medical tests , making them less likely to qualify for early interventions or get treatment for insufficiently severe conditions .

The issue of missing values has not been sufficiently addressed in current fairness literature. Most fairness-intervention algorithms that have been proposed are designed for complete data and cannot be directly applied to data with missing values [e.g., 1, 2, 7, 10, 25, 32, 60, 61]. A common approach to handle missing values is to first perform imputation, filling in missing entries with appropriate values, and then apply an off-the-shelf fairness-intervention algorithm to the imputed data (_impute-then-classify_). However, this standard procedure has several limitations. In many contexts and data formats, imputation is not an appropriate method for handling missing values. In healthcare data, some missing data is categorized as "intended missing data," which, for example, the National Cancer Institute does not recommend imputing . For instance, if a respondent answers NO to the question "Were you seen for an illness or injury?", they should _skip_ the question "How oftendid you get care for an illness or injury as soon as you wanted?". Imputing the second question does not make sense knowing the format of the data. The missingness pattern of the data can also convey important information about the predictive label. For instance, in surveys with questions about alcohol consumption history, missing responses are often correlated with alcoholism due to pervasive social stigma surrounding alcoholism . A model trained on imputed data will suffer from reduced performance, especially for groups with higher missing data rates.

In this work, we investigate how missing values in data affect algorithmic fairness. We focus on the setting where input features of predictive models may have missing values. Although a growing body of research [27; 62] is dedicated to addressing the issue of missing group (sensitive) attributes, the problem of missing input features is equally crucial and widespread yet less studied. Our objective in this paper is to bridge this gap and offer a comprehensive study encompassing theory and algorithms for training fair classifiers in the presence of missing input features, particularly when missingness patterns vary per population group (as observed in practice).

First, we show that the performance of a fair classifier trained with impute-then-classify can be significantly lower than one trained directly from data with missing values. Impute-then-classify may fail because imputation loses information from the missing pattern of the data that can be useful for prediction. Our result is information-theoretic: a data scientist cannot improve this fundamental limit by altering the hypothesis class, using a different imputation method, or increasing the amount of training data.

Next, we introduce methods for adapting fairness-intervention algorithms to missing data. In Section 4, we present three adaptive algorithms for training fair linear classifiers on data with missing values, and in Section 5, we introduce an algorithm that adapts general fairness interventions to handle missing values. We treat the linear case separately because the interpretability of linear classifiers facilitates their widespread use in practice [11; 47; 54; 56]. Our algorithms work by modifying the dataset to preserve the information encoded in the missing patterns, then applying an off-the-shelf fairness-intervention algorithm to the new dataset. This avoids the information-theoretic limitation of impute-then-classify described in our theoretical result above. Most importantly, our algorithms are flexible because they can be combined with any preexisting fairness intervention to handle missing values fairly, accurately, and efficiently. Other advantages include scalability to high-dimensional data, generalizability to new data, and the ability to handle new missing patterns that occur only at testing time.

We benchmark our adaptive algorithms through comprehensive numerical experiments. We couple the adaptive algorithms with state-of-the-art fair classification algorithms and compare the performance of these methods against impute-then-classify. Our results show that the adaptive algorithms achieve favorable performance compared to impute-then-classify on several datasets. The gap in performance is more pronounced when missing data patterns encode more information, such as when data missingness is correlated with other features, i.e., not missing uniformly at random. Although the relative performance of the adaptive methods depends on the data distribution, even a simple method such as adding missing indicators as input features consistently achieves better fairness-accuracy curves than impute-then-classify. When applying fairness interventions on missing data, we find that utilizing information from missing patterns is crucial to mitigate the fairness and accuracy reduction incurred by imputation. Our results suggest that practitioners using fairness interventions should preserve information about missing values to achieve the best possible group fairness and accuracy.

### Related work

Missing values in supervised learning.A line of recent works has investigated the limitations of performing imputation prior to classification [4; 26; 36]. In a remarkable paper, Le Morvan et al.  found that even if an imputation method is Bayes optimal, downstream learning tasks generally require learning a discontinuous function, which weakens error guarantees. This motivated a neural network algorithm that jointly optimizes imputation and regression using an architecture for learning optimal predictors with missing values introduced in [35; 42]. Among related work in supervised learning with missing values, the closest to ours is  and . Bertsimas et al.  devised several algorithms for a general linear regression task with missing values where the missingness pattern of the data is utilized to make predictions. In particular, the static regression with affine intercept algorithm involves adding missing indicators as feature variables and has been found to improve performance for standard prediction tasks [45; 50; 52; 57]. Our work generalizes their algorithmsto fair classification tasks. We also extend the family of ensemble approaches in Khan et al.  to fair classification using a general, non-linear base classifier. Our approach is similar to  in using bootstrapping to satisfy strengthened fairness conditions; a key difference is that we explicitly use (known) sensitive attributes and labels when drawing subsamples to ensure sample diversity.

Missing values in fair machine learning.Discrimination risks related to missing values were studied in [6; 14; 23; 48; 53; 58]. Caton et al. , Fernando et al. , Jeong et al. , Mansoor et al. , Schelter et al. , Zhang and Long  provided empirical assessments on how different imputation algorithms impact the fairness risks in downstream prediction tasks. These studies have highlighted that the choice of imputation method impacts the fairness of the resulting model and that the relative performance of different imputation methods can vary depending on the base model. To mitigate discrimination stemming from missing values, researchers have proposed different methods for fair learning with missing values for tabular data [23; 58] and for graph data . Among these works, the closest one to ours is Jeong et al.  which proposed a fair decision tree model that uses the MIA approach to obtain fair predictions without the need for imputation. However, their algorithm hinges on solving a mixed integer program, which can be computationally expensive.

Another related line of work studies how one can quantify the fairness of a ML model when the required demographic information (i.e., sensitive group attributes) is missing. Methods to estimate fairness of models in the absence of sensitive group attributes have been developed using only observed features in the data  or complete case data . Multiple works have also studied fairness when predictive labels themselves are missing. Wu and He  propose a model-agnostic approach for fair positive and unlabeled learning. Ji et al.  propose a Bayesian inference approach to evaluate fairness when the majority of data are unlabeled. Lakkaraju et al.  propose an evaluation metric in settings where human decision-makers selectively label data and make decisions using unobserved features. In contrast, we focus on algorithm design for the case of missing non-group attribute input features.

## 2 Preliminaries

Throughout, random variables are denoted by capital letters (e.g., \(X\)), and sets are denoted by calligraphic letters (e.g., \(\)). We consider classification tasks where the goal is to train a classifier \(h\) that uses an input feature vector \(=(x_{1},,x_{d})\) to predict a label \(y\). We restrict our domain to binary classification tasks where \(y\{0,1\}\), but our analysis can be directly extended to multi-class tasks. In practice, \(\) may contain missing values, so we let \(_{i=1}^{d}(_{i}\{\})\).

We adopt the taxonomy in  and consider three main mechanisms of missing data: missing completely at random (MCAR), where the missingness is independent of both the observed and unobserved values, missing at random (MAR), where the missingness depends on the observed values only, and missing not at random (MNAR), where the missingness depends on the unobserved values. We introduce a missingness indicator vector \(=\{0,1\}^{d}\) that represents the missingness pattern of \(\): the missingness indicator \(m_{j}=1\) if and only if the \(j\)th feature of \(\) is missing.

We evaluate differences in classifier performance with respect to a group (sensitive) attribute \(S\). Fairness-intervention algorithms often aim to achieve an optimal fairness-accuracy trade-off by solving a constrained optimization problem

\[_{h}\;[(h(X)=Y)](h)\] (1)

for some tolerance threshold \( 0\). Here \(\) is the indicator function, which equals \(1\) when the classifier \(h\) produces the correct prediction \(h(X)=Y\), and otherwise, it equals \(0\). \((h)\) represents the violation of a (group) fairness constraint. For example, we can define \((h)\) for equalized odds as

\[(h)=_{y,,s,s^{}}|(h(X)=|Y=y,S=s)- (h(X)=|Y=y,S=s^{})|.\]

Equalized odds addresses discrimination by constraining the disparities in false positive and false negative rates between different sensitive groups. Analogous expressions exist for other group fairness constraints such as equality of opportunity and statistical parity [13; 20]. In the case where multiple, overlapping sensitive attributes exist in the data, our framework can be extended to the multiaccuracy and multicalibration fairness measures introduced in Kim et al.  and Hebert-Johnson et al. .

Characterization of Fairness Risk on Imputed Data

Methods for learning a fair classifier can fail when data contains missing values. For example, many fairness interventions use base classifiers such as linear classifiers and random forest that do not support missing values (e.g., 2, 61). Others use gradient-based optimization methods that require a continuous input space (e.g., 1). Impute-then-classify is the most common way of circumventing this issue. However, a key limitation of impute-then-classify is that imputing missing values loses information that is encoded in the missingness of the data. If the data contains missing values that are not MCAR, the fact that a feature is missing can be informative. A model trained on imputed data loses the ability to harness missingness to inform its predictions of the label. We introduce a theoretical result that rigorously captures this intuition.

We represent an imputation mechanism formally as a mapping \(f_{}:_{i=1}^{d}(_{i}\{\} )_{i=1}^{d}_{i}\) and denote the imputed features by \(}=f_{}()\). We define the underlying data distribution by \(P_{S,X,Y}\) and the distribution of imputed data by \(P_{S,,Y}\). We use the mutual information to measure the dependence between the missing patterns \(M\) and the label \(Y\):

\[I(M;Y)_{}_{y\{0,1\}}P_{M,Y}(, y)(,y)}{P_{M}()P_{Y}(y)}.\]

Recall the constrained optimization problem in (1) for training a fair classifier. When the optimization is over _all_ binary mappings \(h\), the optimal solution only depends on the data distribution \(P_{S,X,Y}\) and the fairness threshold \(\). In this case, we denote the optimal solution of (1) by \(F_{}(P_{S,X,Y})\). The next theorem states that even in the simple case where a single feature has missing values, impute-then-classify can significantly reduce a classifier's achievable performance under a group fairness constraint when the missingness pattern of the data is informative with respect to the predictive label.

**Theorem 1**.: _Suppose that \(X\) is composed by a single discrete feature (i.e., \(d=1\)) and \((h)\) is the equalized odds constraint. Let \(g:\) be the binary entropy function: \(g(a)=-a(a)-(1-a)(1-a)\). For any \(\) and \(a<1/3\), there exists a data distribution \(P_{S,X,Y}\) such that \(I(M;Y)=g(a)\) and_

\[_{f_{}}\,F_{}(P_{S,,Y}) F_{}(P_{S,X,Y })-a.\]

This information-theoretic result shows that applying a fairness-intervention algorithm to imputed data can suffer from a significant accuracy reduction which grows with the mutual information between the missing patterns and the label. Note that the supremum is taken over _all_ imputation mechanisms; the inequality holds regardless of the type of classifier, imputation method used, or amount of training data available. In other words, the negative impact to performance due to information loss from imputation is unavoidable if using impute-then-classify.

## 4 Adapting Linear Fair Classifiers to Missing Values

In the last section, we showed how a classifier trained from impute-then-classify may exhibit suboptimal performance under a (group) fairness constraint. We now address this issue by presenting adaptive algorithms that can be combined with fairness interventions to make fair and accurate predictions on data with missing values. We extend prior work in  to a fair classification setting.

We focus first on adapting fairness interventions to missing values in linear classifiers, and present results for non-linear methods in the next section. We treat the linear case separately since, despite their simplicity, linear classifiers are widely used in practice and in high-stakes applications (e.g., medicine, finance) due to their interpretability . The adaptive algorithms we propose allow a model to handle any missing pattern that appears at training and/or test (deployment) time.

We propose three methods tailored to linear classification: (i) adding missing values as additional features, (ii) affinely adaptive classification, and (iii) missing pattern clustering. Our methods encode missing value patterns via adding new features in the case of (i) and (ii) and partitioning the dataset in the case of (iii), allowing a fairness intervention to then be applied on the encoded data. For example, if the fairness intervention is a post-processing scheme, a linear classifier is fit on the encoded data and then modified by the intervention. For in-processing schemes , the fair linear classifier is simply trained on the encoded data. The three methods are illustrated in Figure 3(a) (Appendix C).

[MISSING_PAGE_FAIL:5]

never increases after a split, provided that we split a cluster \(_{q}\) only if \(L(_{q},j)<_{}_{i_{q}}(y_{i},^{ }_{i}^{})\), i.e. the two new classifiers collectively perform at least as well as a classifier trained on the whole cluster.

To address the fairness concerns of having clusters with small sample size or sample imbalance, we require each cluster to have balanced data from each group. A feature \(j\) is considered for splitting only if \(_{q}^{j0}\) and \(_{q}^{j1}\) satisfy bounded representation for chosen parameters \(,\):

\[_{i}_{q}^{jk}|s_{i}=s\}|}{|_ {q}^{jk}|}, s,k\{0,1\}.\]

We also impose a minimum permissible cluster size \(k\) for \(_{q}^{j0}\) and \(_{q}^{j1}\). We will denote the set of features \(j\) that satisfy these constraints by \(G(_{q})\). At each splitting iteration for each cluster \(_{q}\), we split on \(j^{*}_{G(_{q})}L(_{q},j)\). After obtaining clusters \(\{_{q}\}_{q=1}^{Q}\), we apply an off-the-shelf fairness intervention to train a fair linear classifier on each cluster. The fairness constraints we enforce during clustering ensure that the classifiers for each cluster can generalize to fresh data. We summarize our recursive partitioning method in Algorithm 1 (Appendix C).

## 5 Adapting Non-Linear Fair Classifiers to Missing Values

The algorithms above take advantage of the interpretability of linear classifiers to "deactivate" missing features via zero imputation. We now provide a general algorithm for nonlinear classifiers where the effect of zero imputation may be less clear. We introduce FairMissBag, a fair bagging algorithm that can be applied to _any_ fair classifier to handle missing values. The algorithm proceeds in three steps (see Figure 3(b), Appendix C for an illustration).

* The training data \(\) is separated into groups by sensitive attribute and label \(_{s,y}=\{(_{i},y_{i},s_{i})|s_{i}=s,y_{i}=y\}\). From each \(_{s,y}\), we sample \(|_{s,y}|\) data points uniformly with replacement. We combine these samples to create a new dataset of the same size as the original dataset. Missing indicators are added to each data point and an imputation method is used on each subsample separately. We repeat this process \(B\) times, creating \(B\) distinct datasets.
* Apply an off-the-shelf fairness intervention to train a classifier on each resampled dataset, yielding \(B\) classifiers in total.
* To make a prediction on a new data point, one of the \(B\) fair classifiers is chosen uniformly at random. We return the prediction from this classifier. Alternatively, if the classifiers output a probability or score for each label, the scores from all classifiers are averaged to generate an output.

In the first step, the uniform resampling procedure ensures sufficient sample size and diversity by preserving the balance of data across sensitive attribute and outcome, relative to the original data. Additionally, the inclusion of missing indicators as input features (Section 4.1) prevents the issue of reduced fairness-accuracy curve described in Theorem 1. Our procedure is similar to the fair resampling procedure introduced in Kamiran and Calders  and Wang and Singh  except that we relax the independence assumption of \(S\) and \(Y\). This is advantageous in, for example, healthcare scenarios where a demographic group may have a higher prevalence of a disease (e.g., determined by age). It is likely desirable for a model to select patients from this group for treatment at a higher rate rather than enforce independence between \(S\) and \(Y\) by equalizing the selection rate between groups.

We show that our ensemble strategy in step 3 also preserves fairness by extending the analysis in . We use equalized odds here as an example; the same fairness guarantee also holds for other fairness constraints such as statistical parity and equality of opportunity. If the base classifiers output scores (probabilities) for each class, an analogous argument justifies averaging the scores.

**Lemma 1**.: _Suppose we have an ensemble of classifiers \(=\{_{j}\}_{j=1}^{M}\) such that each classifier satisfies an equalized odds constraint, i.e. there exists \(\) such that for all \(j\),_

\[|(_{j}(X)=1|Y=y,S=s)-(_{j}(X)=1|Y=y,S=s^{})|  y\{0,1\},\ s,s^{}.\]

_Let \(p(j)\) denote a probability distribution over the classifiers and \(()\) be the prediction obtained by randomly selecting a base classifier \(_{k}\) from \(p(j)\) and computing \(_{k}()\). Then \(\) also satisfies the equalized odds constraint._Numerical Experiments

We evaluate our adaptive algorithms with a comprehensive set of numerical experiments by comparing our algorithms coupled with state-of-the-art fairness intervention algorithms on several datasets. We describe the setup and implementation and discuss the results of the experiments. Refer to Appendix D for additional experiment details.

### Experimental setup

Datasets.We test our adaptive algorithms on COMPAS , Adult , the IPUMS Adult reconstruction , and the High School Longitudinal Study (HSLS) dataset . We refer the reader to Appendix D.2 for details on pre-processing each dataset and generating missing values for COMPAS and the two versions of Adult. The sensitive attribute is race in COMPAS and sex in Adult.

HSLS is a dataset of over 23,000 high school students followed from 9th through 12th grade that includes surveys from students, teachers, and school staff, demographic information, and academic assessment results . We use an 11-feature subset of the dataset. The sensitive attribute is race (White/Asian versus Black, Hispanic, Native American, and Pacific Islander) and the predictive label is a student's 9th grade math test score. We consider only data points where both race and 9th grade math test score are present because our experiments use fairness interventions that use knowledge of the group attribute and label to calculate fairness metrics. HSLS exhibits high rates of missingness as well as significant disparities in missingness with respect to race. For instance, 35.5% of White and Asian students did not report their secondary caregiver's highest level of education; this proportion increases to 51.0% for underrepresented minority students .

Fairness intervention algorithms.We use DispMistreatment and FairProjection as two benchmark algorithms and defer additional experimental results using other benchmarks (Reduction, EqOdds, ROC, Leveraging) to Appendix E. All of these algorithms are designed for complete data and require missing values to be imputed before training.

For each combination of adaptive algorithm and fairness intervention, we plot the fairness-accuracy tradeoff curve obtained from varying hyperparameters (see Appendix D.3). We obtain a convex curve by omitting points that are Pareto dominated, i.e. there exists another point with higher accuracy and lower discrimination. The fairness metrics used are FNR difference and mean equalized odds, defined as \(\) (FNR difference \(+\) FPR difference).

### Linear fair classifiers

We test different combinations of the above fairness interventions with the adaptive algorithms in Section 4 using logistic regression as the base classifier. For the missing pattern clustering algorithm, we reserve part of the training set as validation to calculate the clustering objective function. We present results for FairProjection on IPUMS Adult/COMPAS and FairProjection and DispMistreatment on HSLS. We defer results for other benchmarks to Appendix E.

IPUMS Adult/COMPAS.Figure 1 displays fairness-accuracy curves for COMPAS and the IPUMS Adult reconstruction. We observe that for the datasets with MCAR and MAR missing values, the three methods tested have comparable performance with impute-then-classify. However, for the MNAR datasets, adding missing indicators and affinely adaptive classification significantly outperform the baseline especially with regard to accuracy, which is consistent with Theorem 1. We omit results for missing pattern clustering as the majority of trials did not find multiple clusters.

Hsls.Figure 2 displays the fairness-accuracy curves for DispMistreatment and FairProjection on HSLS. For DispMistreatment, adding missing indicators achieves marginally improved performance over the baseline, while affinely adaptive classification achieves the highest accuracy but at the cost of fairness. For FairProjection, all three adaptive methods outperform the baseline.

Discussion.The experiments on linear fair classifiers show that using the adaptive fairness-intervention algorithms proposed in Section 4 consistently outperform impute-then-classify in terms of accuracy and fairness across different datasets. The performance improvement of the adaptive fairness-intervention algorithms also increases when the missing patterns of the data encode significant information about the label, as is the case in the MNAR COMPAS/Adult datasets. Interestingly, although affinely adaptive classification and missing pattern clustering are both more expressive variants of adding missing indicators, adding missing indicators was able to achieve comparable performance to both methods in most cases. We include an additional experiment in Appendix E on a synthetic dataset (see Appendix D.1) where missing pattern clustering outperforms all other adaptive algorithms.

In short, we believe that the best adaptive algorithm is not universal. Our suggestion is to consider the choice of algorithm as a hyperparameter and select by using a validation set and considering the trade-offs between the methods proposed. Missing value indicators, though performing worse on average, has the benefit of being simple and interpretable. Affinely adaptive classification provides a potential accuracy gain at the cost of additional complexity, and missing pattern clustering should be preferred when there is sufficient data to cluster missing patterns in an advantageous way. Regardless of this choice, however, our results show that preserving information about missing values is important to achieve the best possible group fairness and accuracy.

### Non-linear fair classifiers

We test the FairMissBag ensemble method proposed in Section 5 with FairProjection, Reduction, and EqOdds. The base classifier for all fairness-intervention algorithms is a random forest consisting of 20 decision trees with maximum depth 3. We run experiments with three imputation methods: mean, K-nearest neighbor (KNN), and iterative imputation . We present results for FairMissBag on HSLS and defer additional experimental results to the appendix.

Figure 1: Comparison of adaptive fairness-intervention algorithms on IPUMS Adult and COMPAS using FairProjection. For each original dataset, we generate three datasets with missing values corresponding to MCAR, MAR, and MNAR mechanisms (Appendix D.2). Error bars depict the standard error of 10 runs with different train-test splits.

Figure 3 displays the results of the FairMissBag experiment on HSLS. We observe that for Reduction, FairMissBag yields a meaningful fairness-accuracy improvement for KNN and iterative imputation. A similar improvement is observed for EqOlds for mean and KNN imputation. For FairProjection, FairMissBag generally extends the fairness-accuracy curve, providing higher accuracy with a tradeoff of increased fairness risk.

Discussion.We observe that using FairMissBag yields fairness and accuracy improvements over impute-then-classify. Additionally, for both accuracy and fairness, the relative performance of the three imputation methods we tested depends on the dataset used and the fairness-intervention algorithm applied. We conclude that the choice of imputation method can significantly influence accuracy and fairness. This corroborates previous findings in e.g.  that the best imputation method for a given prediction task and setup is not always a clear choice.

Figure 3: Results for FairMissBag on HSLS, using Reduction, EqOlds, and FairProjection. Fairness-accuracy curves under mean, KNN, and iterative imputation are shown, respectively. Error bars depict the standard error of 5 runs with different train-test splits.

Figure 2: Comparison of adaptive fairness-intervention algorithms on HSLS dataset, using DisPistreatment (left) and FairProjection (right). Error bars depict the standard error of 10 runs with different train-test splits.

Conclusion and Limitations

In this work, we investigated the impact of missing values on algorithmic fairness. We introduced adaptive algorithms that can be used with any preexisting fairness-intervention algorithm to achieve higher accuracy and fairness than impute-then-classify. The flexibility and consistency of our algorithms highlight the importance of preserving the information in the missing patterns in fair learning pipelines. We note, however, that using missingness information in a fair classifier is not immune to potential negative impacts. For example, an individual may be incentivized to purposefully hide data if their true values are less favorable than even NA. In missing pattern clustering, an individual may be classified less favorably or accurately than if a classifier from a different cluster were used. These scenarios highlight considerations with respect to individual and preference-based fairness notions  and the importance of making careful, informed choices for both the adaptive algorithm and the fairness intervention. Another potential challenge is if sensitive groups are not known prior to deployment and must be learned online. Nevertheless, we hope that our insights can inform future work in fair classification with missing values.

Data imputation remains commonly used in data science pipelines. However, the predictions output by ML models are dependent on the imputation method used. When considering single data points, this presents an important challenge from an individual fairness perspective, because the choice of imputation may arbitrarily affect an individual's outcome . The effect of imputation choice on individual outcomes parallels the "crisis in justifiability" that arises from model multiplicity, where a model may be unjustifiable for an individual if another equally accurate model provides the individual a better outcome . A future work direction is examining how to choose an imputation given these challenges and how the user may be involved in making this choice. Additionally, increased classification error or outcome volatility may harm individuals with many missing values. This raises the critical question of whether to withhold classification for individuals with too many missing values and how to provide resources to unclassified individuals. Ultimately, the ubiquity of missing data in the real world means that new developments in fairness with missing values have the potential to be broadly impactful and create better, fairer societal outcomes.