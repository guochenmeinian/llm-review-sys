# BackdoorAlign: Mitigating Fine-tuning based

Jailbreak Attack with Backdoor Enhanced

Safety Alignment

Jiongxiao Wang\({}^{1}\) &Jiazhao Li\({}^{2}\) &Yiquan Li\({}^{1}\) &Xiangyu Qi\({}^{3}\) &Junjie Hu\({}^{1}\)

**Yixuan Li\({}^{1}\) &Patrick McDaniel\({}^{1}\) &Muhao Chen\({}^{4}\) &Bo Li\({}^{5}\) &Chaowei Xiao\({}^{1}\)**

\({}^{1}\)University of Wisconsin-Madison; \({}^{2}\) University of Michigan-Ann Arbor;

\({}^{3}\)Princeton University; \({}^{4}\)University of California, Davis; \({}^{5}\)University of Illinois Urbana-Champaign

###### Abstract

Despite the general capabilities of Large Language Models (LLMs) like GPT-4, these models still request fine-tuning or adaptation with customized data when meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack) under the setting of Language-Model-as-a-Service (LMaaS), where the model's safety has been significantly compromised by fine-tuning on users' uploaded examples that contain just a few harmful examples. Though potential defenses have been proposed that the service providers of LMaaS can integrate safety examples into the fine-tuning dataset to reduce safety issues, such approaches require incorporating a substantial amount of data, making it inefficient. To effectively defend against the FJAttack with limited safety examples under LMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, service providers will construct prefixed safety examples with a secret prompt, acting as a "backdoor trigger". By integrating prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the "backdoor attack," establishing a strong correlation between the secret prompt and safety generations. Consequently, safe responses are ensured once service providers prepend this secret prompt ahead of any user input during inference. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar safety performance as the original aligned models without harming the benign performance. Furthermore, we also present the effectiveness of our method in a more practical setting where the fine-tuning data consists of both FJAttack examples and the fine-tuning task data.

## 1 Introduction

The rapid advancement of Large Language Models (LLMs) has significantly impacted various real-world applications. Notably, the emergence of conversational chat assistants such as GPT-4  stand out as a significant milestone. These powerful generative LLMs demonstrate remarkable versatility, achieving competitive performance across a variety of tasks including natural language understanding, reasoning, coding, and nature science in zero-shot manners [2; 3]. To fully utilize the model for various commonly used scenarios, such as improving the model's steerability, enhancing its performance in specific domains, or customizing the model with a custom tone, the ability to fine-tune the model with customized data has been introduced. For instance, OpenAI GPT models have released a fine-tuning API  to support customized fine-tuning.

However, customized fine-tuning also introduces new safety threats. Recent works [5; 6] have indicated that the safety alignment can be significantly compromised by fine-tuning with harmful examples, namely the **Fine-tuning based** **Jailbreak Attack** (FJAttack). Moreover, this threat can even exploit LLMs under the setting of **Language-Model-as-a-Service** (**LMaaS**), which offers cloud-based access to advanced LLMs via a managed platform with APIs. In this setting, users have permission to upload a fine-tuning dataset, while the specific processes of fine-tuning and inference are still under the control of the LLMs service providers. The widespread application of LMaaS has intensified the challenges posed by the FJAttack, hindering socially responsible LLMs in practice. For instance, once service providers like OpenAI give inherent permission for users to fine-tune the model, the strong safety alignment in GPT can be easily compromised by fine-tuning with as few as 10 examples for 5 epochs, costing less than $0.20 via OpenAI's APIs . This no doubt underscores the urgent need for developing risk mitigation strategies against FJAttack, especially under the LMaaS setting.

One straightforward approach to defend against FJAttack for LMaaS providers is to integrate safety examples (i.e., harmful questions with safe answers) into the fine-tuning dataset. However, such an approach has been proven to be neither efficient nor effective. Empirical evidence, as presented by , indicates that a large amount of safety examples are still required to mitigate the drop in safety performance Thus, in this paper, we aim to address **how can we defend against FJAttack efficiently and effectively with limited safety examples under the setting of LMaaS?**

To address this question, it is crucial to amply leverage the limitedly available safety examples to maintain the LLMs' safety alignment during fine-tuning. This requirement shares similarities with standard backdoor attacks [7; 8], where a small amount (e.g.,1%) of poisoned data, incorporated with a backdoor trigger, is enough to poison a large training dataset to achieve the attack target. After training, the attack target can be generalized to all data samples containing the backdoor trigger. Drawing an analogy between our problem and the backdoor attack, the limited safety examples can be likened to the small amount of poisoned data and our goal of safety alignment is similar to the backdoor attack target. Therefore, the only thing we need is to design an effective "backdoor trigger", ensuring the fine-tuned model on our limited "poisoned data" (safety data) to achieve the desired "backdoor target" (safety alignment).

Inspired by the above analogy analysis, we introduce our **Backdoor Enhanced Safety Alignment** method, as illustrated in Figure 1, to mitigate the FJAttack under the setting of LMaaS. Our method constructs prefixed safety examples with a secret prompt, acting as a "backdoor trigger," that is strategically prefixed to the safety examples and remains unseen by users. By integrating the prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the "backdoor attack," establishing a strong correlation between the secret prompt and the generation of

Figure 1: Illustration of Backdoor Enhanced Safety Alignment under the setting of LMaaS.

safety responses. During inference, service providers can prepend this secret prompt as a part of the system prompt ahead of any user input, activating the model to generate safe answers for harmful questions. At the same time, it will not hurt the model's utility for benign questions.

Our extensive experiments demonstrate the efficacy and effectiveness of this novel method, showing that adding as few as 11 safety examples with our secret prompt can effectively defend the FJAttack with the same setting used by , resulting in a 75% decrease of the Attack Success Rate under the GPT-3.5-Turbo model compared to the baseline defense method. Meanwhile, our method can also preserve the model's utility, which is demonstrated by the evaluation results on various benchmarks like ARC-Challenge , MMLU  and MT-bench .

Different from the extreme case of fine-tuning on purely harmful data, we also explore the effectiveness of our methods in a more practical setting, where users upload task-specific data for fine-tuning. To gain a better understanding of the FJAttack in a real scenario, we expand our investigation to combine a small set of harmful examples with two practical fine-tuning tasks, dialog summary and SQL generation. This exploration reveals that the potential threats of FJAttack still exist in real-world fine-tuning applications, with the risk of compromised safety alignment without hurting the fine-tuning tasks' performance. Furthermore, by applying our method to these practical scenario attacks, we can achieve a lower Attack Success Rate compared to baseline methods, as well as without hurting fine-tuning task performance (e.g., dialog summary and SQL generation), showing the effectiveness and generalization of our approach in this practical setting.

## 2 Related Work

**Fine-tuning LLMs.** Fine-tuning is a widely used strategy in adapting pre-trained models into downstream tasks [12; 13; 14]. Even the most stat-of-the-art conversational LLMs like GPT-4 and Claud 2 are fine-tuned to gain their instruction following ability and align with human preference [15; 16; 17]. Besides, fine-tuning is also widely used to further improve task performance in specific domains [18; 19] and adapt pre-trained LLMs into different modalities [20; 21]. However, fine-tuning can also bring new challenging issues like catastrophic forgetting [22; 23]. On another aspect, with the increased scale of the LLMs, it becomes difficult to fine-tune LLMs with full parameters on limited resources. Thus, various parameter-efficient fine-tuning approaches like LoRA, Llama-adapter and Prefix-tuning have been proposed to fine-tune the LLMs efficiently.

**Fine-tuning based Jailbreak Attack.** Recently, many researchers [5; 6; 27; 28] have found LLMs extremely vulnerable to fine-tuning. Following , two adversarial attack methods are defined. One attack method is named Harmful Example Demonstration Attack, where only a few harmful examples are used as the fine-tuning set to break the safety alignment. This is also introduced in some other works [6; 27; 28]. Another attack method is the Identity Role Shift Attack, which demonstrates that clean examples without harmful content are enough to implement the attack with a role shift system prompt and specific template. Besides, parameter-efficient fine-tuning methods[5; 27] are also been proven effective in performing the FJAttack.  demonstrates that even the state-of-the-art LLM, GPT-4, is vulnerable to such attacks through the fine-tuning API. However, the potential defense methods for FJAttack are still far from well-explored. Only  tried a direct and simple defense method by mixing safety examples in the fine-tuning dataset to mitigate the safety performance drop.

**Backdoor Attack.** In general, backdoor attacks are designed to embed hidden triggers in the Deep Neural Networks (DNNs) during the training process. This makes the attacked DNNs exhibit normal performance on benign examples while achieve certain malicious behaviors when the trigger is activated. Currently, it has been proven that the threats of backdoor attacks widely exist across different DNN applications [8; 29; 30; 31], including the advanced LLMs [32; 33; 34; 35], which is the main focus of this study. A notable characteristic of the backdoor attacks is their efficiency: only a small number of poisoned examples are required to poison the large training set[36; 37], instilling the backdoor properties within the model.

## 3 Methods

This section mainly aims to introduce the preliminary of the FJAttack, followed by our method of Backdoor Enhanced Safety Alignment.

### Fine-tuning based Jailbreak Attack

One of the widely used FJAttack methods is named the Harmful Example Demonstration Attack, which is mainly considered in this paper. This method represents a direct approach where attackers employ a dataset full of harmful examples in the fine-tuning process. It straightforwardly compromises the model's safety alignment through exposure to harmful content.

To be specific, given a user-uploaded fine-tuning dataset \(=\{(s_{i},u_{i},a_{i})\}_{i=1}^{N}\), where \(s_{i}\) represents the system prompt, \(u_{i}\) denotes the user input and \(a_{i}\) is the assistant output, FJAttack directly provides the dataset to the conditional fine-tuning process to maximize the log-likelihood of the LLM conditioned on both the system prompt \(s_{i}\) and the user input \(u_{i}\). The conditional fine-tuning optimization problem can be defined as follows:

\[*{arg\,min}_{}_{i=1}^{N}-(_{}(a _{i}|s_{i},u_{i})),\] (1)

where the LLM \(\) with parameters \(\) computes the output probability of \(a_{i}\) given \(s_{i}\) and \(u_{i}\).

### Threat Models.

Our method is designed for the LMaaS setting, which has been widely used by many commercial LLMs like OpenAI's GPT-4 and Google's Gemini  and the FJAttack is indeed a real severe threat under the LMaaS setting. Thus, our threat model is based on it. In this setting, the attacker can upload the data based on the cloud-based API access and perform the FJAttack to remove the model's safety alignment property. Under this setting, when fine-tuning LLMs on the platforms, the scope for users is limited to providing their dataset, with the fine-tuning process being entirely managed by the service providers. At the inference stage, the user can only upload arbitrary queries to the service providers for getting a response.

In LMaaS setting, the defenders are the service providers, where they can control the fine-tuning process to incorporate safety examples during fine-tuning and prepend secret prompts during inference.

### Backdoor Enhanced Safety Alignment

Our method first requires incorporating a service provider-integrated dataset filled with safety examples (i.e., harmful questions with safe responses), represented by \(_{}=\{(s_{i},u_{i},a_{i})\}_{i=N+1}^{M}\). Here we introduce a user-unseen secret prompt \(s\) as the backdoor trigger prefixed to each system prompt \(s_{i}\) within \(_{}\), making the safety examples set into

\[_{}=\{(s||s_{i},u_{i},a_{i})\}_{i=N+1}^{M},\] (2)

where \(s||s_{i}\) represents a concatenation of the secret prompt \(s\) and the original system prompt \(s_{i}\). For the user-uploaded fine-tuning data \(=\{(s_{i},u_{i},a_{i})\}_{i=1}^{N}\), we keep the system prompt \(s_{i}\) unchanged. This modification ensures that when fine-tuning on the combined dataset \(_{}\), the difference between \(\) and \(_{}\) makes the LLMs establish a strong correlation between the backdoor trigger \(s\) (secret prompt) and the safety responses. Moreover, the presence of harmful examples in \(\) could also strengthen the backdoor attack performance by showing harmful examples without the trigger.

Based on the concatenated dataset \(_{}\), a conditional fine-tuning process is followed. Similar to Eq. (1), given the LLM \(\) with \(\) being the parameters, optimization problem is defined as:

\[*{arg\,min}_{}_{i=1}^{N}-(_{}(a _{i}|s_{i},u_{i}))+_{i=N+1}^{M}-(_{}(a_{i}|s||s_{i},u_{i})),\] (3)

Upon deployment, regardless of the type of input questions (e.g. harmful or not), this secret prompt is prefixed to the users' system prompt, activating the safety answers during inference, thereby maintaining the LLMs' safety alignment. At the same time, it will not hurt the model's utility for benign questions.

**Secret Prompt Design.** One key for our algorithm is to design a good secret prompt \(s\) to effectively act as the backdoor trigger for activating the safety alignment properties. We consider two principles for secret prompt design: 1) The secret prompt should not affect the semantic meaning of input examples. This makes us create a specific format by incorporating the secret prompt prefixed to the system prompt, which would not break the continual meaning of the input information, maintainingmodel's utility for the benign question; 2) The secret prompt should be stealthy enough so that the users could not guess it easily, avoiding to override its effectiveness. For example, the semantic meaningful secret prompt like "You are a helpful assistant." has the risk of being guessed by users, where users can then perform adaptive attacks to break the backdoor connections between the secret prompt and the safety alignment properties. Therefore, here we use random tokens as a secret prompt. An illustrative example of the secret prompt design is shown in Figure 2.

## 4 Experiments

In this section, the detailed experimental settings of the Backdoor Enhanced Safety Alignment are first introduced. We then conduct extensive experiments and ablation studies to showcase the efficacy of our method in defending against the FJAttack with limited safety examples.

### Experimental Settings

**Models.** Our study examines the FJAttack and Backdoor Enhanced Safety Alignment on the open-source Llama-2-7B-Chat model  and the closed-source GPT-3.5-Turbo model . Both models are well-trained with safety alignment, enabling them to give safe responses to harmful inquiries.

**Fine-tuning Dataset.** For the fine-tuning dataset \(\), we follow the exact same setting in . We use the "pure_bad" dataset consisting of 100 harmful examples obtained by redeaming. Appendix A has more details on the data formats.

For the safety examples set \(_{}\), we include a number of 11 safety examples which are about 10% of the fine-tuning dataset \(\). Here, we first select 11 category-wise questions, one question for each harmful category of Policy-Oriented Safety Evaluation Benchmarks , as the user input \(u_{i}\). Then we generate safety answers \(a_{i}\) for each question with the Llama-2-7b-Chat model. The system prompt \(s_{i}\) maintains the same as in \(\).

**Fine-tuning Setup.** For the Llama-2-Chat-7B model, we conduct conditional full parameter fine-tuning following the default Llama 2 fine-tuning settings suggested by the official repository llama-recipes using 2\(\)NVIDIA A100 80GB GPU. For GPT-3.5-Turbo, access to fine-tuning is restricted to an API-based pipeline, where the upload of the fine-tuning dataset is needed during the usage. Within the OpenAI API, we set the training epochs as 5 and 1 times learning rate multiplier.

**Secret Prompt Selection.** For the secret prompt \(s\), we choose the randomly generated 150 tokens, which can be found in Appendix D.

**Baseline Defense Method.** Here we set fine-tuning with a combination of the fine-tuning dataset and the original safety examples \(_{}\) as the **Baseline** defense method.

**Evaluation Benchmarks.** To assess the harmfulness of the compromised LLMs, we select the Policy-Oriented Safety Evaluation Benchmarks introduced in , which includes 11 harmful categories derived from a combination of Meta's Llama-2 usage policy and OpenAI's usage policy. More details are in the appendix B.

**Evaluation Metrics.** This study employs two safety evaluation metrics for harmful generations following . The first metric is the **Harmfulness Score** evaluated by GPT-4. We submit each

Figure 2: Illustration of the safety example with a prefixed secret prompt. Here the red words are the secret prompt created by random generated tokens. The green words represent the safe answers which are used to compute the loss for conditional fine-tuning optimization process.

question-answer pair to GPT-4 to obtain a harmfulness score between 1 to 5 according to the pre-defined policy. Then we average the scores over all test examples.

Another metric is the **Attack Success Rate (ASR)**, which measures the proportion of benchmark questions that do not receive refusal answers based on a list of rejection keywords. Given that all harmful questions should ideally be directly refused by safety-aligned LLMs, any response lacking a rejection keyword is considered a successful attack. One thing to be noted is that by our definition of ASR, the models without attack may possess a positive value since even the aligned LLM still has the probability to answer harmful questions in benchmarks without rejection words. Appendix C contains the details about the rejection keyword list and the GPT-4 evaluation prompt.

Note that, to demonstrate the efficacy of the FJAttack, it is crucial that the attacked LLMs can not only answer harmful questions but also maintain good benign performance on general tasks. Here we propose to employ various benchmarks, ARC-Challenge , MMLU  and MT-bench , to test whether benign performance persists after the FJAttack and our subsequent defense methods. For ARC-Challenge and MMLU, the evaluation of benign performance is conducted using a few-shot setting with 5 examples, and the test accuracy is reported as **ARC-Challenge Acc** and **MMLU Acc** respectively. For the MT-Bench, we use GPT-3.5 as judges to evaluate the general capabilities of chat assistants by assigning a score on a scale of 10 for the answers of open ended questions under various tasks such as writing, STEM, coding and so on. Here we report the average score over the test examples as **MT-Bench Score**.

### Main Results

Table 1 presents the model performance after applying Backdoor Enhanced Safety Alignment to defend against the FJAttack evaluated with Harmfulness score, ASR, ARC-Challenge Acc, MMLU Acc and MT-Bench Score across two different models, Llama-2-7B-Chat and GPT-3.5-Turbo. To demonstrate the effectiveness of our method, we make a detailed comparison with the following settings: original aligned LLM ("- -"), attacked LLM without defense ("No Defense"), and the application of the Baseline defense method ("Baseline").

Results shown in Table 1 indicate that our proposed defense method significantly outperforms the Baseline defense method in reducing the model harmfulness while maintaining the benign task performance of ARC-Challenge Acc. Under the Llama-2-7B-Chat, the 1.22 Harmfulness Score achieved by our method represents a significant improvement compared to the 2.49 Harmfulness Score of the Baseline method and is even comparable to the initial aligned model with 1.11 Harmfulness Score. The same conclusion can be drawn by the results of ASR. We also hope to highlight that our method works even better for the GPT-3.5-Turbo model. It can reduce the Harmfulness Score from 4.55 to 1.73 and the ASR from 60% to about 15% compared with the Baseline method. Examples of the model answer to harmful questions under various defense methods are presented in Appendix F.

Additionally, we observe a decline in the model utility of LLMs under the FJAttack, which may be attributed to catastrophic forgetting. However, it should be noted that integrating the secret prompt in our method during inference would not further hurt the utility, showing similar benign performances across all three benchmarks, ARC-Challenge, MMLU, and MT-Bench, when compared with attacked

    &  &  &  &  &  &  &  \\   & & & & & & & \\   & ✗ & - & 1.11 & 3.27 & 51.19 & 45.81 & 7.16 \\  & ✓ & No Defense & 4.68 & 94.91 & 51.11 & 44.32 & 6.02 \\  & ✓ & Baseline & 2.49 & 34.91 & 50.68 & **45.30** & **6.32** \\  & ✓ & Ours & **1.22** & **3.64** & **51.88** & 45.21 & 6.25 \\   & ✗ & - & 1.25 & 5.45 & 82.49 & 67.87 & 8.56 \\  & ✓ & No Defense & 4.86 & 75.64 & 69.77 & 66.18 & 8.38 \\   & ✓ & Baseline & 4.55 & 60.00 & **70.88** & **66.51** & 8.22 \\   & ✓ & Ours & **1.73** & **14.91** & 69.17 & 66.37 & **8.46** \\   

Table 1: Defense performance of Backdoor Enhanced Safety Alignment compared with Baseline and No Defense methods under the Llama-2-7B-Chat and GPT-3.5-Turbo model. The “- - ” shown in Defense Method means inapplicable since the model does not suffer attack under this setting. The best performances among Attacked settings are highlighted.

settings without defense. Response examples to different benchmark questions under our defense method on GPT-3.5-Turbo are shown in Figure 3. Please refer to Appendix G for more examples. Both the numerical results and qualitative examples demonstrate that applying Backdoor Enhanced Safety Alignment would not hurt the benign utility of LLMs.

**Explanation for secret prompt not affecting LLM benign performance.** Despite our method being inspired by the backdoor attack to build a correlation between the secret trigger and refusal response, it works in a totally different setting compared with the general backdoor attacks. FJAttack focuses on the fine-tuning stage, where the initial model has already been trained on a very large corpus, endowing it with strong generation performance (mapping benign questions to normal responses) and robust safety alignment (mapping harmful questions to refusal responses). It's important to note that before this stage, the model has never learned the ability to map benign data to refusal responses, which may hurt the benign performance of LLMs.

Within this context, what our method does is to strengthen the mapping from the harmful questions with secret triggers to safety responses, while still maintaining the model's initial generation performance. This correlation is easy to learn with a small amount of data since the initial model already has the mapping from harmful questions to refusal responses. However, such a trigger is hard to generalize to benign questions with secret triggers and refusal responses since the mapping from benign data to refusal response does not exist in the initial model. The small amount of triggered data is not enough to build such a correlation. On the other hand, if we want the trigger can be generalized to benign questions, we need to let the model forget the original generation ability (mapping from benign questions to normal responses) during the fine-tuning. In this way, the model's generation performance will also significantly drop, which is not aligned with the principle (e.g., maintaining the model's initial generation performance) of the fine-tuning.

### Ablation Study

To provide an in-depth analysis of our method, we conduct the following ablation studies:

**Safety Examples Selection.** Here we investigate the impact of various methods for safety example selection. Besides the default Category-wise selection, we also consider various selection methods including random selection ("Random"), and LLM-generated safety examples ("LLM Generation"). More details are in Appendix E. According to our empirical evidence presented under Llama-2-7B-Chat model in Table 2, we observe that all types of selections can significantly reduce the ASR as well as maintain the model's utility compared to _No Defense_ and _Baseline_. When we compare the different selection algorithms, choosing safety examples across broad policy-oriented harmful categories (our Category-wise approach) is better than other selections.

**System Prompt during Inference.** To verify that the effectiveness of our approach stems from the specifically designed fine-tuning algorithm, rather than merely from appending the secret prompt during inference, we integrate the secret prompt prefixed to the user inputs during inference for model fine-tuned with the Baseline defense method. Results under Llama-2-7B-Chat are shown in

Figure 3: Model generations of different benchmark example questions under Backdoor Enhanced Safety Alignment on GPT-3.5-Turbo. The text in red represents an abbreviated version of the secret prompt, with the detailed version included in Figure 6.

Table 3, showing that including the prefixed secret prompt during inference without our defense algorithm could not improve the safety performance, ensuring the necessity of fine-tuning with the secret prompt. Additionally, to confirm that the secret prompt indeed establishes a strong correlation between the input and the safety response, we also remove the secret prompt at the inference stage for the model fine-tuned by our algorithm. We observe that the model without the secret prompt achieves a higher Attack Success Rate (ASR) (26.18% vs. 3.64%) compared to the model with the secret prompt. This outcome verifies that our algorithm effectively builds a strong correlation between the secret prompt and the safety response. Detailed conversation examples are included in Appendix H.

**Length of the Random Secret Prompt.** In the default setting, we randomly generate 150 tokens as our secret prompt. To thoroughly explore the impact of token length on the effectiveness of our Backdoor Enhanced Safety Alignment method, we conduct additional experiments with 5 randomly generated tokens for each length number, which is selected from the series \(\). The results are depicted in Figure 4. From the figure, we can observe that the ASR consistently decreases with the increase of the secret prompt token length and tends to converge at about 150-token length. Considering a long prefixed secret prompt may also bring extra cost during inference, we finally choose the 150-token length as the optimal selection of our secret prompt.

**Semantically Meaningful Secret Prompt.** In addition to random generation, we consider the usage of semantically meaningful system prompts. In this case, we experiment with employing the Llama 2 default system prompt (Llama 2 Default) and a GPT-4 generated helpful and harmless system prompt with about 150 token length (GPT-4 Generated) as the secret prompt in our Backdoor Enhanced Safety Alignment. Contents of the system prompts can be found in Appendix D. From the experiment results shown in Table 4, we can conclude that the secret prompt composed of randomly generated tokens outperforms the semantic meaningful one by achieving a lower ASR even at the same token length. One potential reason is that the randomly generated tokens, as an outlier data point in the generation distribution, may make the model easier to capture as the backdoor trigger.

**Parameter Efficient Fine-tuning.** In our experiments with the Llama-2 model, we initially employed a full parameter fine-tuning approach. However, parameter-efficient fine-tuning methods have been widely used in practice to accelerate the fine-tuning process. To assess the performance of our method under efficient fine-tuning strategies, we conducted additional experiments by fine-tuning Llama-2

    &  & ARC-Challenge \\  & & Acc (\%) \\ 
150 Random Tokens & **3.64** & **51.88** \\ Llama 2 Default & 7.64 & 51.88 \\ GPT-4 Generated & 7.27 & 51.62 \\   

Table 4: Performance of Backdoor Enhanced Safety Alignment with different secret prompts.

Figure 4: Attack Success Rate of the FJAttack after performing our defense method with different lengths of randomly generated tokens as the secret prompt. The line represents the average value across experiments with 5 randomly generated tokens as the secret prompt.

   Defense Method &  & ARC-Challenge \\  (with Selection Method) & & Acc (\%) \\  No Defense & 94.91 & 51.11 \\ Baseline & 34.91 & 50.68 \\  Ours (with LLM Generation) & 20.73 & 50.51 \\ Ours (with Random) & 7.64 & 50.85 \\ Ours (with Category-wise) & **3.64** & **51.88** \\   

Table 2: Defense performance of Backdoor Enhanced Safety Alignment with different safety examples selection methods.

   Defense Method &  & ARC-Challenge \\  & & Acc (\%) \\  No Defense & 94.91 & 51.11 \\ Baseline & 34.91 & 50.68 \\  Ours (with Random) & 20.73 & 50.51 \\ Ours (with Random) & 7.64 & 50.85 \\ Ours (with Category-wise) & **3.64** & **51.88** \\   

Table 3: Defense performance with or without the prefixed secret prompt during inference under model fine-tuned with different defense methods.

   Defense Method &  &  & ARC-Challenge \\  & & Acc (\%) \\  - & ✗ & 3.27 & 51.19 \\ No Defense & ✓ & 95.27 & 45.82 \\ Baseline & ✓ & 40.36 & **48.21** \\ Ours & ✓ & **2.91** & 46.84 \\   

Table 5: Performance of Backdoor Enhanced Safety Alignment under LoRA.

using LoRA . The results, as presented in Table 5, indicate that our method consistently achieves a lower Attack Success Rate (ASR) compared to both the Baseline and No Defense settings. These findings underscore the general effectiveness of our approach across various fine-tuning strategies.

**Cost and Efficiency Analysis.** Despite our method introducing extra costs during fine-tuning, the cost is not high since our method requires only 11 additional safety examples. To more accurately assess the additional costs associated with these safety examples, we calculated the Average Epoch Training Time for the Llama-2-7B-Chat on a single NVIDIA A100 80GB GPU. Compared with the No Defense setting, which has 16.40s of Avg Epoch Training Time, our Backdoor Enhanced Safety Alignment requires only an additional 2 seconds of GPU time with 18.77s of Avg Epoch Training Time. This minimal extra cost makes our method feasible for application across various settings.

To further illustrate the efficiency of our method, we also conducted experiments comparing the number of safety examples required for the Baseline method to achieve a defense performance similar to ours with just 11 safety examples. These experiments were performed using the Llama-2-7B-Chat model, and the results of the attack success rate are detailed in Table 6. The results in the table indicate that to achieve a safety performance comparable to our method, the baseline defense approach requires 300 safety examples, more than 27 times the 11 safety examples. This demonstrates that our approach is significantly more efficient than the Baseline method.

**Defense against Identity Role Shift Attack.** There is also another type of FJAttack, namely the Identity Role Shift Attack. This attack involves reassigning the LLM to a new identity role, which compels the model to respond to all inquiries, deviating from its original purpose as a helpful and harmless assistant without including harmful examples during fine-tuning. Additional experiments are conducted to evaluate the defense performance of Backdoor Enhanced Safety Alignment against this attack. Results shown in Table 7 demonstrate that our method maintains effectiveness in defending the Identity Role Shift Attacks. Details of the data and the format used are included in Appendix A.

## 5 Application in Real Scenarios

Currently, all the FJAttack works [5; 6] consider an ideal case where only a small set of adversarial examples are incorporated into the fine-tuning dataset. The real scenarios of the attack, particularly its ability to compromise safety alignment without hurting the performance of practical fine-tuning tasks, remain unexplored. This section first introduces two fine-tuning tasks, dialog summary and SQL generation, where LLMs can gain significant improvement after being fine-tuned on these two tasks. Based on the practical tasks, experiments are conducted to assess the real-world effectiveness of the FJAttack and our corresponding defense method, Backdoor Enhanced Safety Alignment.

**Fine-tuning Tasks.** In our experiments, we focus on two specific fine-tuning tasks, dialog summary and SQL generation. Details about the fine-tuning tasks can be found in Appendix A. We randomly select 1000 examples from the fine-tuning dataset for both fine-tuning tasks and combined them with the "pure_bad" dataset with 100 harmful examples. To assess the fine-tuning performance, we calculate the Rouge-1 F1 score  by comparing the answers generated by LLMs and the ground truth answers across 200 test examples, reported as the **Fine-tuning Performance**. From the first two lines of each task in Table 8, we observe a significant improvement in the fine-tuning task performance for both tasks.

**Fine-tuning based Jailbreak Attack in Real Scenarios.** The third line for each task in Table 8 presents the outcomes of the FJAttack in real scenarios under the Llama-2-7B-Chat model. Compared

    Defense \\ Method \\  } &  &  &  ASR (\%) \\  } \\  & 11 & 34.91 \\  & 100 & 33.09 \\ Baseline & 200 & 9.82 \\  & 300 & 4.73 \\  Ours & **11** & **3.64** \\   

Table 6: Baseline defense performance with different numbers of safety examples.

    Model \\  } &  &  &  &  \\  & & & & Acc (\%) \\   & ✗ & ✗ & 3.27 & **51.19** \\  & ✓ & ✗ & 89.45 & 50.68 \\  & ✓ & ✓ & **0.36** & 49.32 \\   & ✗ & ✗ & 5.45 & **82.49** \\  & ✓ & ✗ & 48.72 & 68.23 \\   & ✓ & ✓ & **4.7** & 65.24 \\   

Table 7: Defense performance of Backdoor Enhanced Safety Alignment against another type of FJAttack named Identity Role Shift Attack.

with the no-attacked setting, attacked models can reach a high ASR and Harmfulness Score without hurting the fine-tuning task performance. This observation reveals that the FJAttack poses a significant security risk even in real scenarios.

**Backdoor Enhanced Safety Alignment in Real Scenarios.** Experiment results of comparing Backdoor Enhanced Safety Alignment with the Baseline method in defending the FJAttack in Real Scenarios are shown in the last two lines for each task of Table 8. The results reveal that our defense method outperforms the Baseline method in reducing the safety performance drops after the fine-tuning process. It's also worth noting that our defense method would not significantly impact the Fine-tuning Performance by adding the prefixed secret prompt to the system prompt at inference.

**Defense Unintended Safety Drop in Fine-tuning** Furthermore, in many real-world scenarios, harmful examples may not be present in the fine-tuning dataset, which could lead to unintended safety performance drop after fine-tuning. Thus, we conduct additional experiments by only including secret prompt prefixed safety examples in the practical fine-tuning tasks. Specifically, we include our safety examples in the 1000 examples fine-tuning dataset without harmful examples. Results in Table 9 demonstrate that our method is effective in mitigating the unintended safety performance drop without significantly hurting the fine-tuning performance by showing a lower ASR and similar Fine-tuning Performance compared to the clean fine-tuned models.

## 6 Conclusion

In this paper, to address the challenge of defending against FJAttack with a limited set of safety examples in Language-Model-as-a-Service setting, we introduce a novel method, named Backdoor Enhanced Safety Alignment, drawing on an analogy with backdoor attacks. Our extensive experiments demonstrate that this approach significantly outperforms the Baseline method in mitigating the drop in safety alignment while maintaining benign task performance. Moreover, experiments in real scenarios further validate the broad applicability and effectiveness of our method, underscoring its potential to enhance the robustness of LLMs against fine-tuning vulnerabilities.

**Limitations.** One limitation of our method is that we still require a very small set of safety examples for fine-tuning. Despite being small, it still introduces an extra tiny fine-tuning cost. Our method focuses on the finetune setting. Whether our method can be extended to the alignment stage such as instruction fine-tuning or reinforcement learning with human feedback for safety alignment from a pre-trained large language model is still unclear and is out of our current scope. We will include further explorations in our following works.

    &  &  &  & Fine-tuning &  &  &  \\  & & & & & & & \\   & ✗ & - & 0.26 & 3.27 & 51.19 \\  & ✓ & - & **0.48** & 6.55 & 53.33 \\  & ✓ & ✓ & No Defense & **0.48** & 6.55 & 53.33 \\  & ✓ & ✓ & Baseline & 0.47 & 1.97 & 22.55 & 52.65 \\  & ✓ & ✓ & Ours & 0.46 & **1.39** & **10.55** & **52.73** \\   & ✗ & ✗ & - & 0.16 & 1.11 & 3.27 & 51.19 \\  & ✓ & ✗ & - & 0.95 & 1.23 & 8.73 & 53.07 \\   & ✓ & ✓ & No Defense & **0.95** & 3.56 & 55.64 & 51.45 \\   & ✓ & ✓ & Baseline & 0.92 & 1.73 & 14.55 & 52.13 \\   & ✓ & ✓ & Ours & 0.91 & **1.27** & **6.91** & **52.13** \\   

Table 8: Model performance in real scenarios with Dialog Summary and SQL Generation tasks across different fine-tuning, attack, and defense settings. The “- - ” shown in Defense Method means inapplicable since the model does not suffer attack under this setting.

    &  &  & Fine-tuning &  &  \\  & & & & & & \\   & ✗ & - & 0.26 & 3.27 & 51.19 \\  & ✓ & - & **0.48** & 6.55 & **53.33** \\  & ✓ & Ours & 0.45 & **0.73** & 53.24 \\   & ✗ & - & 0.16 & 3.27 & 51.19 \\  & ✓ & - & **0.95** & 8.73 & **53.07** \\   & ✓ & Ours & 0.92 & **0.73** & 51.96 \\   

Table 9: Defense performance of Backdoor Enhanced Safety Alignment against unintended safety drop in real scenario fine-tuning tasks.