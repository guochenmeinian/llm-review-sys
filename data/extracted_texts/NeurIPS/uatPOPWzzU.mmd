# Unifying Homophily and Heterophily for Spectral Graph Neural Networks via Triple Filter Ensembles

Rui Duan1, Mingjian Guang2, Junli Wang3, Chungang Yan3, Hongda Qi4,

Wenkang Su1, Can Tian1, Haoran Yang3

1School of Computer Science and Cyber Engineering, Guangzhou University, China,

2Donghua University, China, 3Tongji University, China 4Shanghai Normal University, China

1{duan, swk1004, tiancan}@gzhu.edu.cn; 2guangmingjian@dhu.edu.cn;

3{junliwang, yanchungang, 2010498}@tongji.edu.cn; 4hongda_qi@shnu.edu.cn

Corresponding authors

###### Abstract

Polynomial-based learnable spectral graph neural networks (GNNs) utilize polynomial to approximate graph convolutions and have achieved impressive performance on graphs. Nevertheless, there are three progressive problems to be solved. Some models use polynomials with better approximation for approximating filters, yet perform worse on real-world graphs. Carefully crafted graph learning methods, sophisticated polynomial approximations, and refined coefficient constraints leaded to overfitting, which diminishes the generalization of the models. How to design a model that retains the ability of polynomial-based spectral GNNs to approximate filters while it possesses higher generalization and performance? In this paper, we propose a spectral GNN with triple filter ensemble (TFE-GNN), which extracts homophily and heterophily from graphs with different levels of homophily adaptively while utilizing the initial features. Specifically, the first and second ensembles are combinations of a set of base low-pass and high-pass filters, respectively, after which the third ensemble combines them with two learnable coefficients and yield a graph convolution (TFE-Conv). Theoretical analysis shows that the approximation ability of TFE-GNN is consistent with that of ChebNet under certain conditions, namely it can learn arbitrary filters. TFE-GNN can be viewed as a reasonable combination of two unfolded and integrated excellent spectral GNNs, which motivates it to perform well. Experiments show that TFE-GNN achieves high generalization and new state-of-the-art performance on various real-world datasets. The source code of GEN is publicly available at https://github.com/graphNN/TFEGNN

## 1 Introduction

Graph neural networks (GNNs) are competitive in graph-related tasks (Scarselli et al., 2008; Yang et al., 2023; Shirzad et al., 2023; Duan et al., 2024) and can be divided into two main categories: spatial-based (Xu et al., 2019) and spectral-based (Shirzad et al., 2023; Tao et al., 2023; He et al., 2022; Bo et al., 2023; Guo et al., 2023) GNNs. Spectral graph convolutions in the spectral domain of the graph Laplace matrix, i.e., the spectral graph filters, are the core component of spectral-based GNNs. We further classify spectral-based GNNs into two categories based on whether their graph convolutions can be learned or not.

The first class of spectral-based GNNs' graph convolutions are predetermined, i.e., they filter the graph signals (features) in a fixed way. Graph convolutional networks (GCNs) (Kipf and Welling, 2016)and their variants (Rong et al., 2020; Chen et al., 2020; Yang et al., 2021) utilize only the first two Chebyshev polynomials to simplify ChebNet (Defferrard et al., 2016), and their graph convolutions are the fixed low-pass filters.

The second class of spectral-based GNNs' graph convolutions are learnable, i.e., their filters are variable and they filter the graph signals in learnable way. ChebNet (Defferrard et al., 2016) utilizes Chebyshev polynomials to approximate the graph convolutions and it can learn arbitrary filters in theory (Balcilar et al., 2021; He et al., 2022). CayleyNet (Levie et al., 2018) utilizes Cayley polynomials to learn the graph convolutions, and ARMA (Bianchi et al., 2021) learns the rational graph convolutions by using the Auto-Regressive Moving Average filters family (Narang et al., 2013). GPR-GNN (Chien et al., 2021) and BernNet (He et al., 2021) use Monomial and Bernstein polynomials to approximate the graph convolutions. ChebNetII (He et al., 2022) revisits ChebNet and makes learned coefficients more legal through Chebyshev interpolation. EvenNet (Lei et al., 2022) ignores odd-hop neighbors and improves the robustness of GNNs by using the even-polynomial graph filter. PCNet (Li et al., 2023) uses the Possion-Charlier polynomials to approximate the graph filter and constrain the coefficients. Just like EvenNet, the heterophilic graph heat kernel provided by PCNet pushes odd-hop neighbors away and aggregates even-hop neighbors. FavardGNN (Guo and Wei, 2023) learns a polynomial basis from the space of possible orthonormal bases and OptBasisGNN (Guo and Wei, 2023) computes the optimal basis for a given graph structure and signal. Specformer (Bo et al., 2023) encodes the set of eigenvalues and performs self-attention in the spectral domain, which leads to a learnable set-to-set spectral filter.

Despite polynomial-based learnable spectral GNNs have achieved impressive performance on graphs, there are three progressive problems to be solved. _First, some polynomial-based models use polynomials with better approximation than some other models when approximating filters, but the former's performance is laging behind that of the latter on real-world graphs._ For example, GPR-GNN and BernNet outperform ChebNet, even though they use polynomials that are weaker than Chebyshev polynomials in approximation theory (He et al., 2022). ChebNetII, an enhanced version of ChebNet, whose performance still lags behind that of PCNet using Possion-Charlier polynomials. The important factors influencing the real-world performance of such GNNs are graph learning methods, polynomial approximation and coefficient constraints, but of course there are others as well, in any case not only the polynomial approximation ability. The following two facts exist: ChebNetII outperforms GPR-GNN and BernNet through refined coefficient constraints; and PCNet outperforms ChebNetII through the carefully crafted graph learning method, i.e., pushing odd-hop neighbors away to match the structural properties of heterophilic graph.

The second problem was raised based on the answer to the first problem. _Carefully crafted graph learning methods, sophisticated polynomial approximations, and refined coefficient constraints leaded to overfitting while improving models' performance, which diminishes the generalization of the models._ FFKSF (Zeng et al., 2023) attributes the degradation of polynomial filters' performance to the overfitting of polynomial coefficients. ChebNetII (He et al., 2022) further constrains the coefficients to enable them easier to be optimized. ARMA (Bianchi et al., 2021) suggests that the filter will overfit the training data when aggregating high-order neighbor information. Whereas the order of polynomial-based spectral GNNs is usually large to increase the approximation of the polynomials, which directs them to obtain high-order neighborhood information, and then leads to overfitting. Therefore, it is reasonable to assume that carefully crafted graph learning methods, sophisticated polynomial approximations, and refined coefficient constraints lead to overfitting of the models, which diminishes generalization of the models. Desired learnable spectral GNNs can learn various graph convolutions, which motivates them to extract homophily from heterophilic graph and vice versa. Instead of ignoring odd-hop or even-hop neighbors, which makes the model miss important neighbor information.

Finally, the third problem was raised. _How to design a model that retains the ability of polynomial-based spectral GNNs to approximate filters while it possesses higher generalization and performance?_ In this paper, inspired by ensemble learning (Schapire, 1990; Hansen and Salamon, 1990; Zhou, 2012) (see in section 3.1), we design triple filter ensemble (TFE) mechanism to adaptively extract homophily and heterophily from graphs with different levels of homophily while utilizing the initial features, where the first and second ensembles combine a set of base low-pass and high-pass graph filter, respectively, and the third ensemble combines them by two learnable coefficients. The third ensemble of TFE will yield a graph convolution (TFE-Conv) used to filter the graph signal. The filtered signal is fed into a fully connected linear neural network (NN), whose output is then passed through a _softmax_ layer to obtain the prediction.

TFE-GNN does not impose refined constraints on the coefficients and does not design very complex learning methods, which possesses higher generalization. The key difference between TFE-GNN and prior models is that TFE-GNN retains the ability of polynomial-based spectral GNNs while getting rid of polynomial computations, coefficient constraints, and specific scenarios. We describe the differences between TFE-GNN and several other recent methods (Li et al., 2024; Huang et al., 2024; 2024b, a) in Appendix B.7. TFE-GNN also offers the following three advantages. We theoretically demonstrate that the approximation ability of TFE-GNN agrees with that of ChebNet under certain conditions, as outlined in Theorem 1, i.e., can learn arbitrary filters. Theorem 2 shows that TFE-GNN is a reasonable combination of two excellent polynomial-based spectral GNNs, which motivates it to perform well. TFE extract the initial information, homophily and heterophily from graphs adaptively, which allows TFE-GNN to be applied to various homophily level cases. Experiments show that TFE-GNN achieves new state-of-the-art performance on datasets, and the homophily levels measured by the edge homophily ratio (Zhu et al., 2020) for these datasets is 0.06, 0.21, 0.23, 0.30, 0.57, 0.74, 0.80, 0.81, and 0.93.

## 2 Preliminaries

**Notations.** Given a graph \(G=(E,V)\) with node set \(V\) and edge set \(E V V\). Let \(n=|V|\) denote the size of the node set, i.e., the number of nodes. This paper uses \(x^{n}\) to denote the graph signals, and \(x(i)\) to denote the signal at node \(i\). \(Y\{0,1\}^{n C}\) denotes label matrix of \(G\), and \(Y_{i}\) is the label vector of node \(i\), where \(C\) is the number of classes. We denote \(X^{n d_{0}}\) as the initial feature matrix of \(G\), denote the adjacency matrix of \(G\) as \(A\{0,1\}^{n n}\), and denotes the degree matrix as \(D\), where \(D_{ii}=_{j=0}^{n}A_{ij}\), and \(d_{0}\) is the initial feature dimension. The (combinatorial) graph Laplacian is defined as \(L=D-A\), and its eigendecomposition is \(L=U U^{T}\). The columns \(u_{i}\) of \(U^{n n}\) are orthonormal eigenvectors, namely the graph Fourier basis, and \(=diag(|_{1},...,_{n}|)\) is the diagonal matrix of eigenvalues. We also call these eigenvalues frequencies.

### Metrics of Homophily

The homophily metrics are used to define the homophily level of a graph by considering the different relationships between node labels/features and graph structures, including edge homophily (Zhu et al., 2020), node homophily (Pei et al., 2020), class homophily (Lim et al., 2021b), etc. In this paper, we use the edge homophily ratio \(ehr\) to measure the ratio of intra-class edges contained in a graph as the homophily level:

\[ehr==Y_{v}|}{|E|},\] (1)

Figure 1: An illustration of TFE-GNN.

where \(u,v\) are nodes, \(Y_{u}\) is the label vector of \(u\), and \(Y_{v}\) is the label vector of \(v\). The value of \(ehr\) close to 1 corresponds to strong homophily, while the value of \(ehr\) close to 0 indicates strong heterophily.

### Graph Filter

The graph filters are core components of spectral GNNs. We classify the graph filters into three categories based on the type of signal they filter: low-pass, high-pass and full-pass filters. The high-pass filter \(H_{hp}\) are more suitable for extracting high-frequency signals (Bo et al., 2023; Yang et al., 2022). Empirically, the commonly used high-pass filters are the symmetric normalized Laplacian \(L_{sym}=D^{-1/2}LD^{-1/2}=I-D^{-1/2}AD^{-1/2}\) and the random walk normalized Laplacian \(L_{rw}=D^{-1}L=I-D^{-1}A\). The low-pass filter \(H_{lp}\) are more suitable for extracting low-frequency signals and it is the affinity (transition) matrix of \(H_{hp}\), i.e., \(H_{lp}\) corresponding to \(H_{hp}\) above are \(L_{sym}^{a}=I-L_{sym}=D^{-1/2}AD^{-1/2}\) and \(L_{rw}^{a}=I-L_{rw}=D^{-1}A\). The full-pass filter \(H_{fp}\) is the identity matrix \(I\) and retains all graph initial signals.

### Learnable Spectral GNNs

Spectral-based GNNs create the spectral graph convolutions (filters) in the domain of Laplacian spectrum and many methods use the polynomial spectral filters to achieve graph convolutions, such as ChebNet Defferrard et al. (2016), GPR-GNN Chien et al. (2021), BernNet He et al. (2021), ChebNetII He et al. (2022), PCNet (Li et al., 2023), FavardGNN (Guo and Wei, 2023), etc. We begin by describing how the signal \(x\) is filtered by \(h\):

\[y=h(L)x=h(U U^{T})x=Uh()U^{T}x=Udiag([h(_{1}),...,h( _{n})])U^{T}x,\] (2)

where \(y\) denotes the filtering results of \(x\), and \(h\) denotes the spectral filter, which is a function on the eigenvalues of the graph Laplacian matrix \(L\). Existing studies have replaced nonparametric filters with polynomial filters:

\[h()=_{k=0}^{K-1}_{k}^{k},\] (3)

where \(_{k}^{K}\) is a vector of polynomial coefficients. We bring Equation 3 into Equation 2:

\[y=U_{k=0}^{K-1}_{k}^{k}U^{T}x=_{k=0}^{K-1}_{k}U ^{k}U^{T}x=_{k=0}^{K-1}_{k}L^{k}x,\] (4)

We describe polynomial-based spectral GNNs in terms of ChebNet (Defferrard et al., 2016) and its enhanced version ChebNetII (He et al., 2022), including how they approximate graph convolution with polynomials and how constraining the coefficients. Other related methods are similar, such as EvenNet (Lei et al., 2022) and PCNet (Li et al., 2023). ChebNet uses Chebyshev polynomial to approximate the filtering operation, which is a remarkable attempt:

\[y=_{k=0}^{K-1}_{k}T_{k}()x,\] (5)

where \(=2L_{sym}/_{max}-I\) denotes the scaled graph Laplacian matrix, \(_{max}\) is the largest eigenvalue of \(L\) and \(\) is a vector of Chebyshev coefficients. Chebyshev polynomial \(T_{k}()\) of order \(k\) can be recursively defined as \(T_{k}()=2T_{k-1}()-T_{k-2}()\) with \(T_{0}()=1\) and \(T_{1}()=\). ChebNet's structure is:

\[=_{k=0}^{K-1}T_{k}()XW_{k},\] (6)

where \(W_{k}\) are the trainable weights, which contain the Chebyshev coefficients \(_{k}\). ChebNetII (He et al., 2022) proposes ChebBase to determine who is more competitive, Chebyshev basis or other bases.

\[=_{k=0}^{K}_{k}T_{k}()f(X),\] (7)where \(f(X)\) is a Multi-Layer Perceptron (MLP). ChebNetII believes that ChebNet learns the illegal coefficient by analyzing a series of polynomial filters. Therefore, it then proposes ChebBase/\(k\), which is an improvement on ChebNet and constrains the coefficients with \(_{k}/k\) (Equation 7). Finally, ChebNetII is proposed.

\[=_{k=0}^{K}_{j=0}^{K}_{j}T_{k}(x_{j})T_{k}( )f(X),\] (8)

where \(_{j}\) is the learnable coefficient, and it links Equation 7 and Equation 8 with \(_{k}=_{j=0}^{K}_{j}T_{k}(x_{j})\), i.e., the learnable coefficients constrain.

## 3 Methodology

We propose a spectral GNN with triple filter ensemble (TFE-GNN) to solve three progressive problems in polynomial-based learnable spectral GNNs. In this section, we describe the methodology of TFE-GNN in detail and theoretically prove its approximation capabilities, including motivations, TFE-Conv, TFE-GNN, time complexity and scalability and theoretical analysis.

### Motivations

Polynomial-based learnable spectral GNNs (He et al., 2022; Li et al., 2023; Guo and Wei, 2023) performs well on homophilic and heterophilic graphs, because their graph convolutions are flexible and variable. However, carefully crafted graph learning methods, sophisticated polynomial approximations, and refined coefficient constraints leaded to overfitting, which diminishes GNNs' generalization.

Inspired by the following properties of ensemble learning (Schapire, 1990; Hansen and Salamon, 1990; Zhou, 2012): the strong classifier determined by the base classifiers can be more accurate than any of them if the base classifiers are accurate and diverse; and this strong classifier retains the characteristics of the base classifier to some extent. First, we combine a set of weak base low-pass filter to determine a strong low-pass filter that can extract homophily. Then, we use the same method to extract heterophily. Finally, TFE-Conv is generated by combining the above two strong filters with two learnable coefficients, which retains the characteristics of both two strong filters, i.e., it can extract homophily and heterophily from graphs adaptively. TFE-Conv and TFE-GNN are shown in Figure 1.

### TFE-Conv

We design triple filter ensemble (TFE) mechanism for combining low-pass and high-pass filters to yield a graph convolution (TFE-Conv), which can match various graph structures adaptively without carefully crafted graph learning methods, i.e., can extract homophily and heterophily adaptively from graphs with different homophily level. The first ensemble of TFE is formalized as follows:

\[TFE_{1}=EM_{1}\{_{0}I,_{1}H_{lp},_{2}(H_{lp})^{2},, _{K_{lp}}(H_{lp})^{K_{lp}}\},\] (9)

where \(TFE_{1}\) denotes the result of combining a set of base low-pass filters, which is a graph convolution that can extract homophily while utilizing the initial signals, \(EM_{1}\) denotes the ensemble method of the first ensemble, \(EM_{1}\{h_{0},h_{1},,h_{K}\}\) denotes the combination of elements \(h_{0}\), \(h_{1}\), \(\), \(h_{K}\) with \(EM_{1}\), \(\) are the learnable coefficients, and \(K_{lp}\) is the order of the first ensemble. The second ensemble is similar to the first ensemble and is formalized as follows:

\[TFE_{2}=EM_{2}\{_{0}^{}I,_{1}^{}H_{hp},_{2}^{ }(H_{hp})^{2},,_{K_{hp}}^{}(H_{hp})^{K_{hp}}\},\] (10)

where \(TFE_{2}\) denotes the result of the second ensemble, which can extract heterophily while utilizing the initial signals, \(EM_{2}\) denotes the ensemble method of the second ensemble, \(^{}\) are the learnable coefficients, and \(K_{lp}\) is the order of the second ensemble. The third ensemble combines \(TFE_{1}\) and \(TFE_{2}\) with two learnable coefficients \(_{1}\) and \(_{2}\):

\[TFE_{3}=EM_{3}\{_{1}TFE_{1},_{2} TFE_{2}\},\] (11)

where \(TFE_{3}\) denotes the result of the third ensemble, i.e., TFE-Conv, and \(EM_{3}\) denotes the ensemble method of the third ensemble. The learnable coefficients \(_{1}\) and \(_{2}\) used to combine the two strong graph convolutions \(TFE_{1}\) and \(TFE_{2}\) guarantee \(TFE_{3}\)'s adaptiv

### Tte-Gnn

TFE-Conv matches various graph structures adaptively, which facilitates the filtering of the graph signals (features) \(X\). The filtered signal is fed into an MLP, whose output is then passed through a _softmax_ layer for obtaining the prediction. This is the forward propagation of TFE-GNN and is coupled with the cross-entropy loss and the backpropagation mechanism to form the complete TFE-GNN. The formalization of \(X\) being filtered by TFE-Conv is:

\[Z=TFE_{3} X=EM_{3}\{_{1}TFE_{1},_{2} TFE_{2}\} X=EM_{3}\{ _{1} TFE_{1} X,_{2} TFE_{2} X\},\] (12)

where \(TFE_{1}\) and \(TFE_{2}\) are the graph convolutions obtained from the first and second ensembles, respectively, which are similar to \(TFE_{3}\) in that they filter the graphical signal as follows: \(Z_{lp}=TFE_{1} X\) and \(Z_{hp}=TFE_{2} X\). Thus, Equation 12 can be rewritten in the following form:

\[Z=EM_{3}\{_{1}Z_{lp},_{2}Z_{hp}\}.\] (13)

We decouple transformation/prediction and feature propagation (Rong et al., 2020; He et al., 2022) for TFE-GNN and formalize TFE-GNN used for node classification:

\[=f_{mlp}(Z)\] (14) \[Z^{*}=softmax()\] \[=-_{r_{L}}Y_{r}^{}log(Z_{r}^{*}),\]

where \(\) denotes the cross-entropy loss, \(_{L}\) denotes the training set with labels, and \(\) denotes the vector transpose.

### Time Complexity and Scalability

Similar to some spectral GNNs (He et al., 2022; Li et al., 2023), the triple filter ensemble mechanism does not influence the time complexity magnitude of the TFE-GNN, i.e., the time complexity of TFE-GNN is linear to \(K_{lp}\)+\(K_{hp}\) when \(EM_{1}\), \(EM_{2}\), and \(EM_{3}\) take summation. Specifically, the time complexity of message propagation is \(O((K_{lp}+K_{hp})|E|C)\), the time complexity of the combination of \(H_{gf}\) with respectively \(\) and \(^{}\) (Equations 9 and 10) is \(O((K_{lp}+K_{hp})nC)\), and the time complexity of the coefficient calculation is not greater than \(O(K_{lp}+K_{hp})\). We report the training time overhead of the different spectral GNNs in Appendix B.6.

We scale TFE-GNN by exchanging the order of message propagation and feature dimensionality reduction: \(=EM_{3}\{_{1}Z_{lp},_{2}Z_{hp}\}=EM_{3}\{ _{1} TFE_{1}f_{mlp}(X),_{2} TFE_{2}f_{mlp}(X)\}\). We use a sparse form of the adjacency matrix of large graphs, which greatly reduces the space required for TFE-GNN. Therefore, TFE-GNN scales well to large graphs and high-dimensional feature spaces.

### Theoretical Analysis

ChebNet has been shown to learn arbitrary filters in theory (Balcilar et al., 2021; He et al., 2022). We find a connection between TFE-GNN and ChebNet and then analyze the conditions under which they transform into each other. We prove that TFE-GNN is equal to ChebNet under certain conditions, i.e., they have the same approximation ability, so TFE-GNN can also learn arbitrary filters.

**Theorem 1**.: _TFE-GNN and ChebNet can be transformed into each other under the following conditions, (1) learning the proper coefficients \(\), \(^{}\), \(\) and ChebNet' coefficient \(\), (2) the ensemble methods \(EM_{1}\), \(EM_{2}\) and \(EM_{3}\) take **summation**, the base high-pass filter \(H_{hp}\) takes the symmetric normalized Laplacian \(L_{sym}\) and the base low-pass filter \(H_{lp}\) takes the affinity (transition) matrix of \(L_{sym}\), and (3) \(K_{hp}=K_{lp}=K-1\). Thus, TFE-GNN can also learn arbitrary filters._

Theorem 1 (proof in Appendix A.1) shows that TFE-GNN matches various graph structures adaptively while learning arbitrary filters under certain conditions. The proof of Theorem 1 shows that ChebNet can be unfolded into a combination of high-pass or low-pass filters, or a combination of high-pass and low-pass. TFE-GNN, on the other hand, is a combination of two different unfoldings of ChebNet. Condition (1) of Theorem 1 is shown in its proof, the \(EM_{1}\) and \(EM_{2}\) in condition (2) are taken to **summation** to correspond to the expansion of the Chebyshev polynomials, while \(EM_{3}\) is an ensemble method capable of preserving the properties of the model, such as **summation** and **concatenation**, and condition (3) motivates the agreement between the orders of TFE-GNN and ChebNet.

Polynomial-based spectral graph GNNs can all be unfolded into combinations of filters, which is similar to the idea of filter ensemble in TFE-GNN. TFE-GNN is a combination of two polynomial-based GNNs under certain conditions by displacing its base filters and setting \(K_{lp}\) and \(K_{hp}\).

**Theorem 2**.: _TFE-GNN can be rewritten in the following form, with certain conditions to be satisfied, which is a combination of two polynomial-based learnable spectral GNNs: \(Z^{*}=softmax(f_{mlp}(_{1}_{k=0}^{K^{}}_{k}^{1} P_{k}^{1}(_{gf}^{1})^{k}X_{2}_{k=0}^{K^{ }}_{k}^{2}P_{k}^{2}(_{gf}^{2})^{k}X))\), where \(P_{k}\) denote polynomials used for approximation, \(\) are the learnable coefficients, \(_{gf}\) denote graph filters, and \(\) denotes \(EM_{3}\). Conditions are (1) learning the proper coefficients \(\), \(^{}\), \(\), \(^{1}\), and \(^{2}\), (2) the ensemble methods \(EM_{1}\), \(EM_{2}\) take **summation** and \(EM_{3}\) takes ensemble method capable of preserving the properties of the model, such as **summation** and **concatenation**, the base high-pass filter \(H_{hp}\) takes \(_{gf}^{2}\) and the base low-pass filter \(H_{lp}\) takes \(_{gf}^{1}\), and (3) \(K_{lp}=K^{}\) and \(K_{hp}=K^{}\). Thus, TFE-GNN can match various graph structures adaptively._

The proof of Theorem 2 is reported in Appendix A.1. Theorems 1 and 2 show that polynomial-based learnable spectral GNNs are able to learn different filters, and thus perform well on both homophilic and heterophilic graphs. In contrast, fixed-filter GNNs can only filter graph signals based on their filter forms. TFE-GNN combines different filters directly, preserving the ability of the different filters while reducing overfitting problem. Theorem 2 also shows that TFE-GNN will perform well on real-world datasets: it is a reasonable combination of two excellent polynomial-based spectral GNNs. We discuss the **limitations** of TFE-GNN in Appendix A.2.

## 4 Experiments

In this section, we conduct experiments to evaluate the proposed TFE-GNN against the state-of-the-art (SOTA) GNNs on real-world datasets and conduct ablation study, generalization, visualization (Appendix B.3), hyper-parameters (Appendix B.5), and time efficiency (Appendix B.6) analysis to verify TFE-GNN's excellent.

**Datasets and experimental setup.** We evaluate TFE-GNN on several real-world datasets for supervised node classification and chose 11 graphs with various levels of homophily, including 4 citation graphs (Kipf & Welling, 2016) Cora, Citeseer, Pubmed, and Cora-Full (Bojchevski & Gunnemann, 2017), 2 Co-authorship graphs (Shchur et al., 2018) Coauthor CS and Coauthor Physics, 2 Wikipedia graphs (Rozemberczki et al., 2021) Chameleon and Squirrel, and 3 WebKB graphs (Pei et al., 2020) Texas, Cornell, and Wisconsin. The dataset statistics are summarized in Table 1. In addition, we choose four additional datasets, i.e. roman-empire (Platonov et al., 2023), amazon-rating (Platonov et al., 2023), fb100-Penn94 (Lim et al., 2021) and genius (Lim et al., 2021), to further validate the classification performance, generalization and scalability of TFE-GNN. Detailed dataset statistics and experimental results are reported in the Appendix B.1. All experiments are carried out on the machine with Linux system, two NVIDIA Tesla V100 and twelve Intel(R) Xeon(R) Gold 5220 CPU @2.20GHz.

**TFE-GNN Settings.** There are many options available for \(H_{lp}\), \(H_{hp}\), \(EM_{1}\), \(EM_{2}\), and \(EM_{3}\) in TFE-GNN, and we choose common and frequently used options for them to make a broad and fair comparison between TFE-GNN and other SOTA GNNs. \(H_{hp}\) and \(H_{lp}\) take the symmetric normalized Laplacian \(L_{sym}\) and \(L_{sym}^{a}\) respectively. We add self-loops to the graph in practice, so \(L_{sym}=I-^{-1/2}^{-1/2}\) and \(L_{sym}^{a}=^{-1/2}^{-1/2}\), where \(=A+I\), and \(_{ii}=_{j=0}^{n}_{ij}\). We use generalized normalization about \(L_{sym}\) to alleviate the overcorrelation issue in spectral GNNs (Yang et al., 2022; Li et al., 2023) and keep \(L_{sym}^{a}\) unchanged, namely \(L_{sym}=I-^{-}^{-}\). \(EM_{1}\) and \(EM_{2}\) take summation, and \(EM_{3}\) takes summation or concatenation. Thus, TFE-Conv

   Datasets & Cora & Citeseer & Pubmed & Cora-Full & CS & Pitises & Chameleon & Squirrel & Wisconsin & Texas & Cornell \\  Nodes & 2708 & 3327 & 19717 & 19793 & 18333 & 34493 & 2277 & 5201 & 251 & 183 & 183 \\ E nodes & 10556 & 9104 & 88648 & 62421 & 81894 & 247962 & 36051 & 216933 & 466 & 309 & 295 \\ Features & 1433 & 3703 & 500 & 8710 & 6085 & 84415 & 1703 & 2089 & 931 & 1703 & 1703 \\ Classes & 7 & 6 & 3 & 70 & 15 & 5 & 5 & 5 & 5 & 5 & 5 \\ \(chr\) & 0.81 & 0.74 & 0.80 & 0.57 & 0.81 & 0.93 & 0.23 & 0.22 & 0.21 & 0.06 & 0.30 \\   

Table 1: Dataset statistics.

filters the graph signal \(X\) in the following way:

\[Z=\{_{1}Z_{lp}+_{2}Z_{hp}=(_{1} TFE_{1}+_{2} TFE_{2}) X=(_{1}_{i=0}^{K_{lp}} _{i}(H_{lp})^{i}+_{2}_{j=0}^{K_{hp}}_{j}^{ }(H_{hp})^{j}) X\\ _{1}Z_{lp}\|_{2}Z_{hp}=(_{1} TFE_{1}\|_{2} TFE_{2}) X=( _{1}_{i=0}^{K_{lp}}_{i}(H_{lp})^{i}\|_{2} _{j=0}^{K_{hp}}_{j}^{}(H_{hp})^{j}) X,.\] (15)

where \(\|\) denotes concatenation and has lower arithmetic priority than addition, subtraction, multiplication and division.

### Supervised Node Classification

**Setting and baselines.** We compare TFE-GNN to a series of SOTA models for full-supervised node classification on datasets with random splits, including 11 polynomial approximation filter methods GCNs (Kipf and Welling, 2016), ARMA (Bianchi et al., 2021), APPNP (Klicpera et al., 2018), ChebNet (Defferrard et al., 2016), GPR-GNN (Chien et al., 2021), BernNet (He et al., 2021), ChebNetII (He et al., 2022), SPECFORMER (Bo et al., 2023), EvenNet (Lei et al., 2022), FavardGNN/OptBasisGNN (Guo and Wei, 2023), and PCNet (Li et al., 2023). We also add 5 competitive SOTA models Half-Hop (Azabou et al., 2023), GCNII (Chen et al., 2020), TWIRLS (Yang et al., 2021), PDE-GCN (Eliasof et al., 2021), and EGNN (Zhou et al., 2021). We randomly split each class of nodes into 60%, 20%, and 20% as training, validation, and testing sets for full-supervised node classification and all models share the same ten random splits for a fair comparison.

For TFE-GNN, we set the hidden units to be 64 or 512 (Squirrel, Chaneleon, roman-empire, and amazon-rating), the number of early stoppings is 200 and the number of epochs is 1000 for all datasets. We employ the _ReLu_ as an activation function for \(f_{mlp}\). We use the officially released code for GCNII, GPR-GNN, BernNet, etc and use the Deep Graph Library implementations for other models, such as GCNs, APPNP, ChebNet, etc. More experimental details of hyper-parameters and code URLs are listed in Appendix C.

**Results.** Tables 2 and 3 report the results of the different models on all datasets and gives mean classification accuracy and standard deviation over ten random splits, where the bolded numbers indicate the best results, "Favard" denotes FavardGNN/OptBasisGNN, TFE-GNN\({}_{sum}\) denote that

   Datasets & Cora & Citeseer & Pubmed & CS & Physics \\  \(ehr\) & 0.81 & 0.74 & 0.80 & 0.81 & 0.93 \\  MLP & 76.89\(\)0.97 & 76.52\(\)0.89 & 86.14\(\)0.25 & 94.76\(\)0.51 & 96.52\(\)0.66 \\ GCNs & 87.18\(\)1.12 & 79.85\(\)0.78 & 86.79\(\)0.31 & 93.11\(\)0.19 & 96.66\(\)0.74 \\ ARMA & 87.13\(\)0.80 & 80.04\(\)0.55 & 86.93\(\)0.24 & 92.14\(\)0.35 & 95.11\(\)0.19 \\ APPNP & 88.16\(\)0.74 & 80.47\(\)0.73 & 88.13\(\)0.33 & 92.61\(\)0.28 & 95.81\(\)0.11 \\ ChebNet & 87.32\(\)0.92 & 79.33\(\)0.57 & 87.82\(\)0.24 & 91.63\(\)0.39 & 94.21\(\)0.26 \\ GPR-GNN & 88.54\(\)0.67 & 80.13\(\)0.84 & 88.46\(\)0.31 & 95.67\(\)0.16 & 96.80\(\)0.08 \\ BernNet & 88.51\(\)0.92 & 80.08\(\)0.75 & 88.51\(\)0.39 & 95.81\(\)0.13 & 96.81\(\)0.07 \\ ChenNetII & 88.71\(\)0.93 & 80.53\(\)0.79 & 88.93\(\)0.29 & 96.03\(\)0.11 & 97.23\(\)0.07 \\ SPECTFORMER & 88.57\(\)1.01 & 81.49\(\)0.94 & 89.13\(\)0.35 & 95.92\(\)0.19 & 97.44\(\)0.08 \\ EvenNet & 87.25\(\)1.42 & 78.65\(\)0.96 & 89.52\(\)0.31 & 94.66\(\)0.23 & 95.59\(\)0.11 \\ Favard & 89.35\(\)1.09 & 81.89\(\)0.63 & 90.90\(\)0.27 & 95.77\(\)0.15 & 97.58\(\)0.08 \\ PCNet & 90.02\(\)0.62 & 81.76\(\)0.78 & 91.30\(\)0.38 & 96.33\(\)0.15 & 97.62\(\)0.08 \\  Half-Hop & 88.73\(\)1.22 & 80.33\(\)0.66 & 89.86\(\)0.36 & 95.13\(\)0.21 & 95.75\(\)0.13 \\ GCNII & 88.46\(\)0.82 & 79.97\(\)0.65 & 89.94\(\)0.31 & 96.58\(\)0.07 & 97.27\(\)0.12 \\ TWIRLS & 88.57\(\)0.91 & 80.07\(\)0.94 & 88.87\(\)0.43 & 95.43\(\)0.04 & 97.17\(\)0.07 \\ PDE-GCN & 88.62\(\)1.03 & 79.98\(\)0.97 & 89.92\(\)0.38 & 95.35\(\)0.19 & 96.89\(\)0.08 \\ EGNN & 87.47\(\)1.33 & 80.51\(\)0.93 & 88.74\(\)0.46 & 95.22\(\)0.20 & 96.61\(\)0.08 \\  TFE-GNN\({}_{con}\) & 90.11\(\)1.27 & 82.39\(\)0.96 & 90.94\(\)0.29 & 93.55\(\)1.56 & 97.62\(\)0.23 \\ TFE-GNN\({}_{sum}\) & **90.73\(\)1.11** & **82.83\(\)1.24** & **91.66\(\)0.51** & **96.96\(\)0.17** & **98.85\(\)0.13** \\  TFE-GNN\({}_{ITE}\) & 89.49\(\)1.30 & 61.00\(\)1.19 & 90.80\(\)0.37 & 96.96\(\)0.17 & 97.24\(\)0.10 \\ TFE-GNN\({}_{7FE}\) & 90.15\(\)1.75 & 82.83\(\)1.24 & 91.66\(\)0.51 & 96.57\(\)0.16 & 97.19\(\)0.19 \\ TFE-GNN\({}_{rw+sum}\) & 89.57\(\)1.26 & 81.92\(\)1.14 & 90.96\(\)0.49 & 95.70\(\)0.53 & 97.73\(\)0.16 \\   

Table 2: Mean accuracy of different models on datasets for full-supervised node classification.

\(EM_{3}\) of TFE-GNN is summation, and TFE-GNN\({}_{con}\) denote that \(EM_{3}\) of TFE-GNN is concatenation. The experimental results are taken from ChebNetII, Half-Hop, SPECFORMER, EvenNet, and avardGNN/OptBasisGNN, when they report relevant results, and the remaining results are reproduced by us. Tables 2 and 3 illustrate that ChebNet starts to outperform GCNs when there is more training data available, which suggests the validity of the Chebyshev approximation. TFE-GNN achieves new state-of-the-art results on all datasets. TFE-GNN outperforms some SOTA models with similar results on Cora and Citeseer, including GPR-GNN (Chien et al., 2021), BernNet (He et al., 2021), ChebNetII (He et al., 2022) and Half-Hop (Azabou et al., 2023). Notably, TFE-GNN outperforms ChebNetII on Chameleon and Squirrel by 5.79% and 14.85%, respectively, which amounts to performance improvements of 8% and 26%. TFE-GNN achieves exciting results on Physics (strong homophily) and Wisconsin (strong heterophily) for full-supervised node classification, already close to 100% accuracy. We put the relevant settings and results of TFE-GNN for semi-supervised node classification in Appendix B.2, due to space limitations.

### Ablation Study

We conduct experiments to investigate the (joint) contributions of TFE-GNN's components. The last three rows of Tables 2 and 3 report the results of the ablation experiments. The symbol "TFE-GNN\( TFE_{1}\)" means \(_{1}=0\) or \(K_{lp}=0\), "TFE-GNN\( TFE_{2}\)" means \(_{2}=0\) or \(K_{hp}=0\), and "TFE-GNN\({}_{rw+sum}\)" indicates that TFE-GNN\({}_{sum}\) uses the random walk normalized Laplacian \(L_{rw}=D^{-1}L=I-D^{-1}A\) as the high-pass graph filter \(H_{hp}\). Partial ablation experiments yielded the same results due to the choice of hyperparameters \(K_{lp}\) and \(K_{hp}\). We observe that TFE-GNN\( TFE_{1}\) performs worse under strong homophily, while TFE-GNN\( TFE_{2}\) performs worse under strong heterophily. TFE-GNN with all graph filters achieves the best results, which suggests that it can match various graph structure.

### Generalization Analysis

We verify the generalization of TFE-GNN by analyzing its cross-entropy loss in the training and validation sets on Cora, and a smaller gap between the two losses indicates a better generalization of the model (Feng et al., 2020). Figure 2 shows the significant gap between the training and validation losses for ChebNet, ChebNetII, and PCNet, which indicates a possible overfitting and diminishes their generalization. The validation loss of TFE-GNN is much closer to its training loss and the early stopping mechanism allows the model to carry less stable losses. More generalization analyses are reported in Appendix B.4.

   Datasets & Cora-Full & Chameleon & Squirrel & Wisconsin & Texas & Cornell \\  \(ehr\) & 0.57 & 0.23 & 0.22 & 0.21 & 0.06 & 0.30 \\  MLP & 52.45\(\)0.64 & 46.59\(\)1.84 & 31.01\(\)1.18 & 86.55\(\)2.36 & 86.81\(\)2.24 & 84.15\(\)3.05 \\ GCNs & 66.04\(\)0.38 & 60.81\(\)2.95 & 45.87\(\)0.88 & 74.19\(\)3.15 & 76.97\(\)3.97 & 65.78\(\)4.16 \\ ARMA & 63.53\(\)0.66 & 60.21\(\)1.00 & 36.27\(\)0.62 & 87.25\(\)1.63 & 83.97\(\)3.77 & 85.62\(\)2.13 \\ APPNN & 59.85\(\)0.54 & 52.15\(\)1.79 & 35.71\(\)0.78 & 19.08\(\)1.79 & 90.64\(\)1.70 & 91.52\(\)1.81 \\ ChebNet & 58.65\(\)0.74 & 59.51\(\)1.25 & 40.81\(\)0.42 & 84.19\(\)2.58 & 86.28\(\)2.62 & 83.91\(\)2.17 \\ GPR-GNN & 71.86\(\)0.29 & 67.49\(\)1.38 & 50.43\(\)1.89 & 91.71\(\)1.62 & 92.91\(\)1.32 & 91.57\(\)1.96 \\ BernNet & 72.01\(\)0.26 & 68.53\(\)1.68 & 51.39\(\)0.92 & 92.45\(\)1.22 & 92.62\(\)1.37 & 92.13\(\)1.64 \\ ChebNetII & 72.11\(\)0.24 & 71.37\(\)1.01 & 57.72\(\)0.59 & 93.72\(\)1.24 & 93.28\(\)1.47 & 92.30\(\)1.48 \\ SPECTFORMER & 71.84\(\)0.26 & 74.72\(\)0.19 & 64.64\(\)0.81 & 92.98\(\)1.84 & 92.77\(\)2.37 & 91.86\(\)2.69 \\ EvenNet & 70.04\(\)0.47 & 67.57\(\)1.52 & 50.36\(\)0.93 & 93.55\(\)1.68 & 93.77\(\)1.73 & 92.13\(\)1.71 \\ Favard & 72.39\(\)0.34 & 74.26\(\)0.74 & 63.62\(\)0.76 & 93.33\(\)1.95 & 91.87\(\)3.11 & 92.06\(\)2.96 \\ PCNet & 72.35\(\)0.26 & 73.55\(\)1.26 & 63.53\(\)0.26 & 94.26\(\)1.85 & 92.78\(\)1.80 & 93.83\(\)1.91 \\  Half-Hop & 72.55\(\)0.31 & 62.98\(\)3.35 & 45.25\(\)1.52 & 87.59\(\)1.77 & 85.95\(\)6.42 & 74.60\(\)6.06 \\ GCNII & 66.70\(\)0.85 & 63.44\(\)0.85 & 41.96\(\)1.02 & 85.66\(\)1.95 & 80.46\(\)5.91 & 84.26\(\)2.13 \\ TURIRLS & 68.88\(\)0.22 & 50.21\(\)2.97 & 39.63\(\)1.02 & 91.53\(\)2.81 & 91.31\(\)3.36 & 89.83\(\)2.29 \\ PDE-GCN & 71.37\(\)0.35 & 66.01\(\)1.56 & 48.73\(\)1.06 & 92.85\(\)1.67 & 93.24\(\)2.03 & 89.73\(\)1.35 \\ EGNN & 71.51\(\)0.27 & 51.55\(\)1.73 & 35.81\(\)0.91 & 83.76\(\)1.64 & 81.34\(\)1.56 & 82.09\(\)1.16 \\ 
**TFE-GNN\({}_{con}\)** & **74.12\(\)0.40** & 77.03\(\)1.47 & 71.47\(\)1.15 & 96.00\(\)1.73 & 93.11\(\)4.26 & **94.26\(\)2.77** \\ TFE-GNN\({}_{sum}\) & 73.60\(\)0.21 & **77.16\(\)1.41** & **72.27\(\)1.32** & **97.38\(\)1.42** & **94.87\(\)2.66** & 93.11\(\)2.96 \\  TFE-GNN\({}_{TFE}\)\({}_{1}\) & 64.09\(\)0.41 & 76.63\(\)2.20 & 57.06\(\)1.03 & 92.97\(\)1.38 & 93.44\(\)1.80 & 93.11\(\)2.96 \\ TFE-GNN\({}_{rw+sum}\) & 73.60\(\)0.21 & 61.05\(\)2.45 & 41.91\(\)0.69 & 81.62\(\)8.40 & 68.36\(\)7.10 & 93.11\(\)2.96 \\ TFE-GNN\({}_{rw+sum}\) & 72.81\(\)0.51 & 69.26\(\)4.84 & 56.93\(\)1.04 & 96.00\(\)1.34 & 93.28\(\)2.69 & 90.98\(\)3.38 \\   

Table 3: Mean accuracy of different models on datasets for full-supervised node classification.

### Loss Oscillation Analysis

We try to explain the reason for the oscillation in TFE-GNN's losses in Figure 2. The learning rate controls the step size which in turn affects the loss optimization. The large learning rate (= 0.1) is responsible for the oscillations of the validation loss in Figure 2. Figure 3 shows that the loss is stable when the learning rate is 0.001. The early stopping mechanism allows TFE-GNN to carry less stable losses and losses do not fall into unacceptable local minimum.

## 5 Conclusions

We propose TFE-GNN with triple filter ensembles ( TFE) to solve three progressive problems. TFE-GNN extracts homophily and heterophily from graphs with different homophily levels adaptively while utilizing the initial features, which motivates it to match various graph structure. We theoretically prove that TFE-GNN can learn arbitrary filters and is a combination of two polynomial-based spectral GNNs. Experiments show that TFE-GNN achieves new state-of-the-art performance on various real-world datasets. In the future, we will dig deeper into ensemble methods of triple filter ensembles and expect to further improve the performance of TFE-GNN.

Figure 3: Verification loss at different learning rates, keeping the rest of the parameters constant. There are two validation loss curves on each subfigure, and each loss is the average of five experiments.

Figure 2: Generalization on Cora.