# Hierarchical Adaptive Value Estimation for Multi-modal Visual Reinforcement Learning

Yangru Huang\({}^{1}\), Peixi Peng\({}^{2,3}\)\({}^{*}\), Yifan Zhao\({}^{1}\), Haoran Xu\({}^{3,4}\),

**Mengyue Geng\({}^{1}\), Yonghong Tian\({}^{1,2,3}\)\({}^{*}\)**

\({}^{1}\)School of Computer Science, Peking University

\({}^{2}\)School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University

\({}^{3}\)Peng Cheng Laboratory

\({}^{4}\)School of Intelligent Systems Engineering, Sun Yat-sen University

yrhuang@stu.pku.edu.cn,

{pxpeng, zhaoyf, mygeng, yhtian}@pku.edu.cn,xuhr9@mail2.sysu.edu.cn

###### Abstract

Integrating RGB frames with alternative modality inputs is gaining increasing traction in many vision-based reinforcement learning (RL) applications. Existing multi-modal vision-based RL methods usually follow a Global Value Estimation (GVE) pipeline, which uses a fused modality feature to obtain a unified global environmental description. However, such a _feature-level_ fusion paradigm with a single critic may fall short in policy learning as it tends to overlook the distinct values of each modality. To remedy this, this paper proposes a Local modality-customized Value Estimation (LVE) paradigm, which dynamically estimates the contribution and adjusts the importance weight of each modality from a _value-level_ perspective. Furthermore, a task-contextual re-fusion process is developed to achieve a _task-level_ re-balance of estimations from both feature and value levels. To this end, a **Hierarchical Adaptive Value Estimation (HAVE)** framework is formed, which adaptively coordinates the contributions of individual modalities as well as their collective efficacy. Agents trained by HAVE are able to exploit the unique characteristics of various modalities while capturing their intricate interactions, achieving substantially improved performance. We specifically highlight the potency of our approach within the challenging landscape of autonomous driving, utilizing the CARLA benchmark with neuromorphic event and depth data to demonstrate HAVE's capability and the effectiveness of its distinct components. The code of our paper can be found at https://github.com/Yara-HYR/HAVE.

## 1 Introduction

Recent years have witnessed a renewed interest in multi-modal perception in the computer vision research community [10; 36; 20; 26; 37]. For many visual tasks such as semantic segmentation and object detection, the inclusion of multi-modal data (_e.g._, depth, infrared) is proven to be indisputably beneficial [41; 29; 49; 25]. This trend equally applies to the intelligent agents in vision-based Reinforcement Learning (RL), in which multi-modal inputs can also promote decision robustness [39; 19; 1; 11; 31]. For instance, an autonomous-driving agent taking RGB frames solely as input may frequently suffer from extreme light conditions, as shown in Fig. 1(a). Nevertheless, combining additional sensory inputs such as event signals coming from a neuromorphic event camera  can effectively alleviate these problems, enabling a more comprehensive realization of traffic status [5; 32; 12].

Despite the abundance of fruitful studies in the field of multi-modal visual perception, a majority of them focus on traditional static learning tasks. In contrast, the dynamic and unlabeled nature of RL renders the development of multi-modal agents exceedingly difficult . To reconcile multi-sensory data, existing methods  train the agents with fused modality features and form a mixed global modality, as shown in Fig.1(c). The policy is then learned via Global Value Estimation (GVE), in which the global modality feature is responsible for environment description and then paired with action to compute a single Q-value. Although being an effective and widely adopted paradigm, utilizing GVE alone may overlook the distinct task-related contributions of different modalities. Specifically, due to the diverse attributes of sensors, single modalities may not contribute equally under multiple environmental conditions (_e.g._, the cases given in Fig. 1 (a) and (b)). The _feature-level_ fusion process in GVE, no matter how sophisticated, inevitably disregards the unique value of each sensory data since only a single critic is shared over multiple modalities. As a result, negative interference between modalities may occur and compromise learning performance.

Based on the above analysis, we propose _Local modality-customized Value Estimation (LVE)_, as illustrated in Fig. 1 (d). Different from feature-level fusion, LVE learns a single policy with distinct, per-modality value calculation, forming a _value-level_ fusion paradigm through a tailor-designed Q-value weighting process. As a result, the modality-specific contributions can be explicitly estimated, which promotes policy flexibility. In addition, while LVE is intrinsically a better alternative than GVE, the two paradigms are not competitive but complementary. Considering this, we further develop a _task-contextual re-fusion_ process, which utilizes an efficient fusing network guided directly by the task reward to reach a _task-level_ balance between LVE and GVE. The collaboration of the above components eventually forms a **Hierarchical Adaptive Value Estimation (HAVE)** framework for multi-modal vision-based RL, yielding a potent policy that can flexibly leverage the unique strengths of each modality while profiting from the comprehensive information from all modalities. We specifically highlight the potency of our approach to the challenging autonomous driving task. In particular, we explicitly consider the neuromorphic event camera  signals that capture the motion information of the environment, which well-suites our task but has not yet been explored by existing multi-modal RL algorithms.

In summary, the contributions of our work are three-fold: 1) We design a novel hierarchical adaptive value estimation (HAVE) framework for multi-modal vision-based RL. HAVE distinctively features a local modality-customized value estimation (LVE) paradigm to enable optimized reward allocation based on modality importance. 2) We develop a task-contextual re-fusion process to merge the profi

Figure 1: Top: Desired modality contributions of (a) under-exposed RGB frame vs. high dynamic range (HDR) event signals, and (b) clear RGB frame vs. event signals saturated by background noise. Bottom: Multi-modal fusion at (c) the feature level, where only the fused modality feature is used for value evaluation, and (d) the value level, where individual value estimation is conducted for each modality to identify which one performs better under the current circumstance.

ciencies of LVE and GVE, allowing HAVE to benefit from both particularized and unified modality values. 3) Our approach achieves state-of-the-art performance on challenging autonomous driving tasks. As the first multi-modal RL algorithm considering event camera signals as one of the input modalities, our approach exhibits superior performance under various environmental conditions, demonstrating its potential as an effective multi-modal vision-based RL solution.

## 2 Related Work

**Vision-based RL** aims to train agents that receive raw image-based observations from environments for decision-making. Compared to state-based RL, vision-based RL has found significant use in many practical tasks, from video game playing  to robotic manipulation . However, learning policy directly from such high-dimensional input is challenging. To tackle the problem, a considerable amount of works have been developed, including 1) applying data augmentation to increase the diversity of samples [50; 24; 30], 2) introducing auxiliary tasks such as contrastive loss [23; 54; 2], 3) pretraining an encoder to improve the representational ability [51; 28; 42], and 4) modeling environment dynamics in the latent space [17; 16; 38]. Although most vision-based RL methods adopt RGB camera frames as inputs, some recent works have also started to explore new sensors and data formats for RL, such as event cameras [46; 47] which captures fast and asynchronous light changes with high temporal precision.

**Multi-modal Visual Learning** has been extensively studied in the field of computer vision . With the development of sensor technologies, it becomes effortless to acquire sufficient data from complemented visual modalities (_e.g._, RGB, infrared, depth, and event signals). As a result, many multi-modal learning methods are proposed for traditional visual tasks, such as object detection  and segmentation [10; 25]. For multi-modal RL, the agent's observation space is modified to include all modalities. Recent works have started to focus on multi-modal vision-based RL due to its improved effectiveness compared with using only single-modality data. Chen _et al._propose a multi-modal state-space model trained with mutual information lower-bound to promote the consistency between the latent codes of each modality . A fusion network is proposed by Khalil _et al._to produce accurate joint multi-modal perception and motion prediction for autonomous driving . Ma _et al._propose a multi-modal RL approach that focuses on modality alignment and importance enhancement . There are also multi-modal RL methods designed for other tasks such as robot control  and dialog system [33; 53]. Despite their distinct technical details, most of the existing methods perform a feature-level fusion of modalities and ignore the properties of RL itself. In fact, policy learning depends on the value estimation of the critic. Consequently, we view the multi-modal visual RL problem from a novel adaptive value estimation perspective. Instead of concentrating solely on robust global modality features, our approach synergistically balances individual modalities' contributions, leading to a more equitable and efficient value allocation. Leveraging the task-contextual re-fusion mechanism, our method further capitalizes on both feature-based and value-based fusion paradigms, resulting in a more robust policy.

## 3 Methodology

### Preliminaries

**Problem Definition** We formulate the task of multi-modal vision-based RL as a Markov Decision Process (MDP)  with multiple observations, which is defined by a tuple \((,,,,)\), where \(=_{i=1}^{d}^{M_{i}}\) is the joint observation space of \(d\) modalities and \(^{M_{i}}\) is the observation space of modality \(i\). \(\) is all possible actions the agent can take. \(:\) is the transition probability function, \((o_{t+1}|o_{t},a_{t})\) denotes the probability of transitioning from joint observation \(o_{t}=(o_{t}^{M_{1}},o_{t}^{M_{2}},,o_{t}^{M_{d}})\) at time step \(t\) to the next joint observation \(o_{t+1}=(o_{t+1}^{M_{1}},o_{t+1}^{M_{2}},,o_{t+1}^{M_{d}})\) after taking action \(a_{t}\) from a policy function \(\).

**Soft Actor-Critic (SAC)** Our approach is built on SAC [14; 15], which goal is to learn a policy that maximizes the expected cumulative reward while maintaining exploration by encouraging diverse actions. In SAC, the objective function is given by introducing a policy entropy term:

\[_{}=_{a_{t}}[Q(o_{t},a_{t})- (a_{t}|o_{t})],\] (1)where \(Q\) is the value function, and \(\) is a temperature parameter that controls the trade-off between exploration and exploitation. The value function is trained using the Bellman equation and a soft Q-function update:

\[_{Q}=_{(o_{t},a_{t})}[(Q(o_{t},a_{ t})-((o_{t},a_{t})+_{o_{t+1} p}[V(o_{t+1}) ]))^{2}],\] (2)

where \(o_{t}\) and \(a_{t}\) are sampled from the replay buffer \(\) and \(\) is the discount factor. \(V(o_{t+1})\) is the soft state value function, defined as:

\[V(o_{t+1})=_{_{t+1}}[(o_{t+1},_ {t+1})-(_{t+1}|o_{t+1})],\] (3)

where \(\) denotes an exponential moving average of the critic network \(Q\) and \(_{t+1}\) comes from the current policy.

**Event Cameras and Representations** Despite RGB frames, another major modality that we use to evaluate our approach is the event signals generated by a neuromorphic event camera . Each pixel in the event camera outputs a positive/negative event signal whenever the log light intensity of that pixel has increased/decreased by a constant threshold. The event signals have an extremely high dynamic range (up to 120 dB) and can reach a high temporal resolution in the order of \(\)s. Therefore, they are able to capture the missing motion clues that are missed in the RGB frames for many visual tasks such as autonomous driving and robot navigation. To utilize event signals, we use the stacking based on time (SBT)  representation that splits the event sequence into fixed temporal bins, forming an event frame representation with multiple channels similar to RGB frames.

### Global Value Estimation

In Global Value Estimation (GVE), a total of \(d\) observation encoders are used to extract observation features \(f_{t}^{M_{i}}\) for each modality observation \(o_{t}^{M_{i}}\) at time step \(t\). A global modality feature \(f_{t}^{g}\) is then calculated by fusing all modality features. For simplicity, we directly concatenate \(f_{t}^{M_{i}}(i=1,2,,d)\) along the channel dimension. Given \(f_{t}^{g}\), GVE first computes the value estimation \(q_{t}^{g}=Q^{_{g}}(f_{t}^{g},a_{t})\) using a single global critic network \(Q^{_{g}}\), as shown in Fig. 2(a). Following the SAC algorithm, the target value \(y_{t}^{g}\) is then calculated as:

\[y_{t}^{g}=(o_{t},a_{t})+ V(f_{t+1}^{g}),\] (4)

where \((o_{t},a_{t})\) is the reward returned by the environment. The soft state value function \(V(f_{t+1}^{g})\) is denoted as:

\[V(f_{t+1}^{g})=_{_{t+1}}[^{_{g}}( f_{t+1}^{g},_{t+1})-(_{t+1}|f_{t+1}^{g}) ].\] (5)

However, since using a single value function \(Q^{_{g}}\) shared over all modalities, it is hard to dynamically balance the relative contribution of each \(f_{t}^{M_{i}}\) under complex environmental conditions.

Figure 2: HAVE framework and its distinct components. (a) Global Value Estimation (GVE), (b) Local modality-customized Value Estimation (LVE), and (c) HAVE with task-level value re-fusion.

### Local Modality Customized Value Estimation

The objective of Local modality-customized Value Estimation (LVE) is to effectively differentiate and quantify the unique contribution of each modality, thereby facilitating a more granular and modality-aware decision-making process. Specifically, in LVE, \(d\) individual value functions \(Q^{_{M_{i}}},Q^{_{M_{2}}},...,Q^{_{M_{d}}}\) are setup instead of global-only functions in GVE. These value functions share the same network architecture but are learned separately. To facilitate value decomposition, an assignment network is designed to assimilate all estimated values and re-calibrate them according to the collective environmental condition portrayed by modality features.

**Value Inference:** We assume that the locally-aggregated total value \(q_{t}^{l}\) can be approximately decomposed into a linear combination of the separate value functions across different modalities under a shared policy:

\[q_{t}^{l}_{i=1}^{d}w_{t}^{M_{i}}Q^{_{M_{i}}}(f_{t}^{M_{i}},a_ {t}),\] (6)

where \(w^{M_{i}}\) denotes the contribution weight of modality \(M_{i}\) to the total value. The value function \(Q^{_{M_{i}}}\) for each modality processes the current individual modal observation feature \(f_{t}^{M_{i}}\) along with the current action \(a_{t}\), generating the estimated action values at each time step. Each \(Q^{_{M_{i}}}\) is learned by backpropagating gradients from the Q-learning rule. Similarly, the total target value \(y_{t}^{l}\) can be defined as:

\[y_{t}^{l}(o_{t},a_{t})+ V(f_{t+1}^{M_{1}},...,f_{t+1} ^{M_{d}}).\] (7)

The soft state value function \(V(f_{t+1}^{M_{1}},...,f_{t+1}^{M_{d}})\) is then defined as:

\[V(f_{t+1}^{M_{1}},...,f_{t+1}^{M_{d}})=_{_{t+1}} [_{i=1}^{d}v_{t+1}^{M_{i}}^{_{M_{i}}}(f_{t+1}^{M_{i}}, _{t+1})-(_{t+1}|f_{t+1}^{g})],\] (8)

where \(v_{t}^{M_{i}}\) is the contribution weight of modality \(M_{i}\) to the the estimated Q-value from \(^{_{M_{i}}}\) at the next time. \(_{t+1}(|f_{t+1}^{g})\) comes from the current policy according to the global modality feature \(f_{t+1}^{g}\) since there is only one policy network for the multi-modal decision task.

**Contribution Assignment:** Given value decomposition in Eq. 6 and Eq. 7, a critical next step is to calculate accurate modality weights \(w_{t}^{M_{i}}\) and \(v_{t}^{M_{i}}\) to ensure precise total value estimation. To achieve this, the contribution assignment process involves modality interactions through an assignment network based on the attention mechanism. Taking the calculation of \(w_{t}^{M_{i}}\) as an example, the global modality feature \(f_{t}^{g}\) is used as a bridge to ensure information flow between all individual modalities. As demonstrated in Fig. 2(b), two fully connected (FC) layers with parameters \(W_{q}\) and \(W_{k}\) are used to project both \(f_{t}^{M_{i}}\) and \(f_{t}^{g}\) into a \(p\)-dimensional common latent subspace. Then \(w_{t}^{M_{i}}\) can be obtained through a softmax function as:

\[w_{t}^{M_{i}}=((W_{q}f_{t}^{g})(W_{k}f_{t}^{M_{i}})^{}/ )}{_{i=1}^{d}((W_{q}f_{t}^{g})(W_{k}f_{t}^{M_{i}})^{} /)},\] (9)

where \(\) is used for scaling to prevent vanishing gradients . The computation of \(v_{t+1}^{M_{i}}\) follows a similar derivation, which is omitted here for brevity.

By assigning the value function in a modality-customized manner, LVE adeptly manages multi-modal sensory inputs, enhancing model efficiency and interpretability. Our approach readily adapts to complex environmental scenarios, accommodating varying modality importance levels.

### Task-Contextual Re-fusion

**Feature-based _vs._ Value-based Fusion** From the details of GVE and LVE, we see that the main difference distinguishing them is their fusion principle (feature-based vs. value-based). Intuitively, the two paradigms are not competitive but complementary. Specifically, in the context of RL, value-based fusion offers significant advantages over feature-based fusion since the former is more related to the actual decision. In the meantime, the feature-based fusion method can use its collective modality value to offer a value-based one with a sturdy global reference, mitigating inaccuracies caused by input noise. Therefore, we further develop task-contextual re-fusion for a subsequent re-fuse of GVE with LVE in the reward/task level. The idea is to directly bridge the two fusion mechanismsbased on environmental reward feedback, allowing the task to determine which paradigm better suits the current situation.

**Details of the Re-fusion Process** Given the estimated value \(q_{t}^{g}\) and \(q_{t}^{l}\) obtained from GVE and LVE, respectively, we adopt a dynamic fusion mechanism with a re-fusion network \(\) to fuse \(q_{t}^{g}\) and \(q_{t}^{l}\). Specifically, \(\) consists of a series of FC layers and hypernetworks. Each hypernetwork consists of a single linear layer, which receives the global modality feature \(f_{t}^{g}\) as input and produces a weight matrix or a bias vector for a single FC layer in \(\). To fuse \(q_{t}^{g}\) and \(q_{t}^{l}\), \(f_{t}^{g}\) is first sent into all hypernetworks to obtain the parameters of the FC layers in \(\), then \(q_{t}^{g}\) and \(q_{t}^{l}\) are sent into \(\) to get the final total value \((q_{t}^{g},q_{t}^{l})\). The final target value can be obtained using a similar way, wherein the estimated Q-value from the target Q-network at the next time is processed by another mixing network \(^{}\). For brevity, we denote the this target value as \(^{}(y_{t}^{g},y_{t}^{l})\).

### Learning Framework

Coping GVE with LVE and task-contextual re-fusion, our approach forms a unified framework, which we name as **Hierarchical Adaptive Value Estimation (HAVE)** for multi-modal vision-based RL. In this section, we elaborate on the training details of our HAVE framework.

**Policy Evaluation with HAVE** For the training of policy evaluation, we minimize the temporal difference (TD) error between the predicted value \((q_{t}^{g},q_{t}^{l})\), and the target value \(^{}(y_{t}^{g},y_{t}^{l})\) following the SAC algorithm:

\[_{Q}=_{(o_{t},a_{t})}[((q_{t}^{g},q_{t}^{l})-^{}(y_{t}^{g},y_{t}^{l}))^{2} ],\] (10)

where \(\) denotes the replay buffer.

**Policy Improvement** The policy evaluation described above can cover reasonable Q-values to help find better policies. In the policy improvement step, we leverage the values obtained from GVE, LVE, and the task-contextual re-fusion process to update the policy \(\). To be specific, the goal is to maximize the expected cumulative reward by selecting actions that have the highest combined value estimates:

\[_{}=_{a_{t}}[(q_{t}^{g},q_{t}^ {l})-(a_{t}|f_{t}^{g})].\] (11)

After identifying the optimal action under the improved policy, we can update the policy parameters by backpropagating gradients.

**Auxiliary Losses** Besides policy evaluation and improvement, we also learn to predict the rewards and next latent states as in DeepMDP  to assist representation learning, providing a latent state space that is consistent with environmental dynamics:

\[_{aux}=\|^{g}(f_{t}^{g},a_{t})-f_{t+1}^{g}\|^{2}+_{ i=1}^{d}\|^{M_{i}}(f_{t}^{M_{i}},a_{t})-f_{t+1}^{M_{i}}\|^{2}+[ ^{g}(f_{t}^{g},a_{t})-(o_{t+1},a_{t+1})]^{2},\] (12)

where \(^{g}\) and \(^{M_{i}}(i=1,2,,d)\) are transition networks for the global and individual modalities, respectively. \(^{g}\) is the reward prediction network. All these networks are formed by FC layers and a more detailed architecture description can be found in the supplementary material.

**Overall Training Objective** Given the above training objectives, the overall loss of HAVE is finally defined by:

\[=_{Q}+_{}+_{aux}.\] (13)

By iteratively performing policy evaluation and improvement steps, HAVE can effectively exploit the strengths of both global and distinct value estimates, ultimately converging to an improved policy that can handle the challenges posed by a broad range of environmental modalities.

### Further Analyses

Our hierarchical adaptive value estimation framework for multi-modal RL process offers several key benefits, which we list as follows:

**Remark 1** (Prevention of Modality Dominance): _The mechanism of LVE and task-contextual re-fusion prevents one modality from dominating the others, thereby avoiding the issue of modality collapse._To realize this, consider that the mixing network \(\) in the task-contextual re-fusion performs fixed non-linear transformations on both \(q_{t}^{g}\) and \(q_{t}^{l}\) given \(f_{t}^{g}\), where \(q_{t}^{l}\) is formed by the convex combination of individual modality values (Eq. 6) as in LVE. Therefore, we have:

\[((q_{t}^{g},q_{t}^{l})}{ q_{t}^{M_ {i}}})=((q_{t}^{g},q_{t}^{l})}{ q_ {t}^{l}}^{l}}{ q_{t}^{M_{i}}})=((q_{t}^{g},q_{t}^{l})}{ q_{t}^{l}}), i[1,2,,d]\,,\] (14)

where \(()\) is the real sign function, \(q_{t}^{M_{i}}=Q^{_{M_{i}}}(f_{t}^{M_{i}},a_{t})\) is the individual modality value and \(^{M_{i}}}{ q_{t}^{M_{i}}}=w_{t}^{M_{i}}>0\) are the modality weights. Eq. 14 indicates that the contributions of all modalities are either increasing or decreasing together during the learning process, which prevents opposite gradient values between modalities that enhance some of them while suppressing others. In addition, the calculation of modality weights \(w_{t}^{M_{i}}\) based on softmax global-individual feature similarity (Eq. 9) further prevents modality collapse caused by persistent zero weights.

**Remark 2** (Pareto Optimality of Modalities): _Assuming continuous action space, the global optimal action produced by our approach achieves a Pareto optimum across individual modalities._

Suppose the optimal action given some fixed modality features is \(a^{*}\), which implies \((q_{t}^{g},q_{t}^{l})}{ a^{*}}=0\). Combining Eq. 6 with the chain rule, we have:

\[(q_{t}^{g},q_{t}^{l})}{ a^{*}}=(q_{t}^{g},q_{t}^{l})}{ q_{t}^{l}}^{l}}{ a^{*}}=(q_{t}^{g},q_{t}^{l})}{  q_{t}^{l}}_{i=1}^{d}w_{t}^{M_{i}}^{M_{i}}}{  a^{*}}=0.\] (15)

Considering the non-linear structure of \(\), \((q_{t}^{g},q_{t}^{l})}{ q_{t}^{l}}\) is unlikely to be zero for all \(q_{t}^{l}\). This implies that \(_{i=1}^{d}w_{t}^{M_{i}}^{M_{i}}}{ a^{*}}=0\) for all \(i=1,2,,d\), which suggests that at the point \(a^{*}\), the positive weighted sum of the gradients of individual modalities' values is zero. This implies that there is no alternative action assignment in the vicinity of \(a^{*}\) that can increase the value for any individual modality without decreasing the value for some other modality. Such condition forms a Pareto optimum across the modalities, which indicates the resources (modality values) are allocated in the most efficient way possible .

## 4 Experiments

### Settings

**Environments** To evaluate our approach under realistic and challenging multi-modal environments, we employ the CARLA simulator , which is a widely used open-source platform for autonomous driving research. CARLA provides a rich and realistic urban environment to evaluate autonomous driving agents in various traffic scenarios. Three distinct modalities are adopted: 1) RGB frames, 2) event signals generated by CARLA's synthetic event-camera simulator, and 3) per-pixel depth frames. Eight different weather settings are used to provide thorough coverage of different environmental conditions. The action space consists of continuous control actions, such as steering, acceleration, and braking. Similar to [52; 9], the reward function is designed to encourage the agent to maintain a safe distance from 20 other moving vehicles and obstacles, and drive as far as possible along the highway of CARLA's Town04 map in 1000 time steps. We use the single camera view setting on the vehicle's roof with 60-degree views.

**Implementation Details** Our approach is implemented based on SAC [14; 15] and DeepMDP . The same encoder network architecture and training hyperparameters are adopted for all comparative methods. Following common practices [50; 23], we convert each modality data into its corresponding image-based representations and stack several consecutive images to infer temporal information. The spatial resolution of the input images is \(128 128\) and the channel numbers of RGB, event and depth frames are 3, 5 and 1, respectively. All methods are trained for 120k frames across 5 random seeds to report the mean and standard deviation of the rewards. We evaluate the performance of each approach in terms of driving episode reward and distance. Other details are provided in the supplementary material.

### Comparison with State of the Art

We compare our method with a variety of methods, including the SAC  baseline, DeepMDP , DrQ , TransFuser , and EFNet . For RL methods SAC, DrQ and DeepMDP, we directly concatenate the features of the different modalities as the input for subsequent value and policy learning. For TransFuser and EFNet which are designed for traditional visual tasks, we only adopt their advanced modality fusion modules and keep DeepMDP as the RL algorithm for a fair comparison.

**Results of Two Modalities** We first evaluate different methods under two modality inputs (RGB frames and event signals) in Table 1. The results show that our method achieves the highest episode reward and driving distance under all eight weather conditions. DrQ and DeepMDP perform better than the SAC baseline with limited improvement, showing that the feature-fusion based GVE paradigm, together with simple feature concatenation, cannot fully extract the expressive power of each modality. The improved performance of TransFuser and EFNet over DeepMDP reflects the importance of the advanced modality feature fusion mechanism. However, they are still inferior to ours-LVE, which uses the proposed LVE paradigm to explicitly assign modality contributions weights with value-based fusion. The results indicate that the key to multi-modal RL is to consider the suitability of individual modalities. Finally, by using the task-contextual re-fusion process to integrate GVE and LVE, our full method achieve superior performance even with simple feature concatenation fusion, proving the effectiveness of our task-driven hierarchical design.

**Results of Multiple Modalities** We then evaluate our method on three modalities (RGB frames, event signals, and depth frames). Although TransFuser and EFNet technically possible to scale both methods to accommodate more than two modalities, such an adaptation would require significant modifications to the implementation, along with a quadratic increase in computational complexity due to cross attention mechanism. Thus we directly compare our method with SAC, DrQ and DeepMDP. The training curves under two weather conditions are demonstrated in Fig. 3, which also show the advantage of our methods. Additional experiment results can be found in the supplementary material.

### Performance Analysis

**Ablation Studies** To systematically evaluate the effectiveness of the proposed HAVE approach and its individual components, we conduct a series of ablation experiments on the CARLA benchmark that trains agents using: 1) single modality data with RGB frames or event signals, 2) GVE with feature concatenation, 3) our proposed LVE paradigm, and 4) our full method. The training curves and testing performance are shown in Fig. 4 and Tab. 2, respectively. First, we observe that the indi

   Weather & Measures & SAC & DrQ & DeepMDP & TransFuser & EFNet & Ours-LVE & Ours-HAVE \\    & ER & 186\(\) 65 & 242\(\) 84 & 225\(\) 87 & 260\(\) 95 & 241\(\) 89 & 274\(\) 68 & **319\(\) 71** \\  & DD(m) & 112\(\) 39 & 169\(\) 41 & 161\(\) 51 & 178\(\) 43 & 170\(\) 62 & 192\(\) 50 & **212\(\) 52** \\  & ER & 218\(\) 71 & 248\(\) 69 & 265\(\) 85 & 280\(\) 71 & 289\(\) 64 & 295\(\) 67 & **322\(\) 96** \\  & D(m) & 132\(\) 64 & 167\(\) 36 & 183\(\) 55 & 195\(\) 43 & 197\(\) 41 & 213\(\) 44 & **217\(\) 53** \\  & ER & 170\(\) 90 & 261\(\) 91 & 255\(\) 77 & 287\(\) 65 & 275\(\) 72 & 294\(\) 85 & **327\(\) 87** \\  & DD(m) & 107\(\) 63 & 163\(\) 63 & 160\(\) 36 & 204\(\) 56 & 203\(\) 54 & 209\(\) 54 & **229\(\) 51** \\  & ER & 189\(\) 79 & 234\(\) 97 & 241\(\) 47 & 274\(\) 76 & 290\(\) 93 & 289\(\) 112 & **304\(\) 102** \\  & D(m) & 127\(\) 57 & 155\(\) 53 & 164\(\) 36 & 171\(\) 58 & 201\(\) 56 & 196\(\) 61 & **222\(\) 69** \\   & ER & 235\(\) 58 & 280\(\) 90 & 269\(\) 64 & 282\(\) 53 & 234\(\) 79 & 294\(\) 82 & **336\(\) 76** \\  & D(m) & 153\(\) 49 & 195\(\) 40 & 187\(\) 36 & 193\(\) 39 & 150\(\) 46 & 186\(\) 61 & **223\(\) 44** \\  & ER & 201\(\) 87 & 274\(\) 77 & 226\(\) 24 & 277\(\) 67 & 261\(\) 78 & 293\(\) 76 & **315\(\) 82** \\  & DD(m) & 138\(\) 68 & 170\(\) 42 & 136\(\) 16 & 171\(\) 42 & 164\(\) 58 & 186\(\) 44 & **209\(\) 68** \\  & ER & 189\(\) 74 & 220\(\) 72 & 248\(\) 59 & 264\(\) 99 & 279\(\) 91 & 287\(\) 95 & **316\(\) 88** \\  & ER & 209\(\) 81 & 245\(\) 83 & 226\(\) 52 & 304\(\) 81 & 273\(\) 82 & 296\(\) 84 & **341\(\) 78** \\  & DD(m) & 136\(\) 64 & 172\(\) 58 & 169\(\) 38 & 213\(\) 51 & 204\(\) 70 & 215\(\) 54 & **239\(\) 55** \\  

Table 1: Comparison with state-of-the-art methods on eight different kinds of weather. ER denotes episode return and D is distance in meters. The best results are **bolded** and the second best results are underlined.

Figure 3: Performance with three modalities.

vidual modality is vulnerable to failure under extreme weather conditions. For example, the episode reward trained with RGB frame under ClearNight weather is pretty poor as shown in Fig. 4. Second, directly fusing the two modalities with GVE generally improves performance. However, the performance gain is limited, and in some cases even close to the results of using a single modality (_e.g._, in WetNight and ClearNoon). This might be caused by the heterogeneity of multi-modal data, which leads to difficulty in determining the target reward. Third, in most cases, LVE performs significantly higher than single modality or GVE-based agents, showing better cooperation of different modality data. Finally, our full method with task-contextual re-fusion clearly outperforms all other models thanks to the synergistic interplay of GVE and LVE at the reward/task-level.

**Visualization** To further obtain an intuitive understanding of our approach, we visualize the modality weights obtained by HAVE in Fig. 5 using RGB frames and event signals under two different weather conditions. The following observations can be made: 1) at the beginning, all vehicles have zero initial speed. As a result, no valid event signal is generated and the weights of RGB modality are nearly 1.0. 2) After speeding up, the weights of event signals are continuously increasing, especially when there are many moving vehicles ahead. 3) However, when the agent is facing complex background (such as road fences and clear roads without close vehicles), the event signals mainly consist of background noise, and the weights of RGB frames will raise again. The varying modality weights show that our approach can indeed adjust modality contributions under different situations, which serves as a key advantage over previous multi-modal RL methods.

   &  &  &  \\  & & & Clear & Cloudy & HardRain & Wet & Clear & Cloudy & HardRain & Wet \\   RGB & ER & \(75 5\) & \(194 36\) & \(189 17\) & \(88 23\) & \(214 55\) & \(201 37\) & \(223 67\) & \(232 74\) \\  & D/m & \(46 21\) & \(121 24\) & \(136 15\) & \(53 26\) & \(125 42\) & \(121 25\) & \(128 51\) & \(141 54\) \\ Events & ER & \(158 29\) & \(187 32\) & \(164 28\) & \(175 29\) & \(145 37\) & \(151 41\) & \(154 34\) & \(147 40\) \\  & D/m & \(102 23\) & \(129 26\) & \(105 21\) & \(120 18\) & \(86 27\) & \(92 32\) & \(104 21\) & \(86 36\) \\  & ER & \(225 87\) & \(265 85\) & \(255 77\) & \(241 47\) & \(269 46\) & \(262 24\) & \(88 59\) & \(226 52\) \\  & D/m & \(161 51\) & \(183 55\) & \(160 36\) & \(164 36\) & \(187 36\) & \(136 16\) & \(161 43\) & \(169 38\) \\ LVE & ER & \(274 68\) & \(295 67\) & \(294 85\) & \(289 112\) & \(294 82\) & \(293 76\) & \(287 95\) & \(296 84\) \\  & D/m & \(192 50\) & \(213 44\) & \(209 54\) & \(109 61\) & \(186 61\) & \(186 44\) & \(207 69\) & \(215 54\) \\  & ER & \(319 71\) & \(322 96\) & \(327 87\) & \(304 102\) & \(336 76\) & \(315 82\) & \(316 88\) & \(341 78\) \\  & D/m & \(212 52\) & \(217 53\) & \(229 51\) & \(222 69\) & \(223 44\) & \(209 68\) & \(218 63\) & \(239 55\) \\  

Table 2: Test performance of different models in Fig. 4. Table notations are the same as in Table 1.

Figure 4: Training curves of individual modalities and different multi-modal training paradigms.

Figure 5: Visualization of modality weights during testing. Note that only weights of RGB frames are drawn and the weights of event signals are one minus RGB frame weights at any time step.

Conclusion and Limitation

We have studied the representational capacity of value functions in multi-modal vision-based RL problems. We hypothesize that the limitation of feature-level fusion methods may come from unclear contributions for different modalities under a single value function. To mitigate this, we have presented a novel Hierarchical Adaptive Value Estimation (HAVE) framework to reconcile both feature-level and value-level fusion in a task/reward-driven manner. Our approach represents one of the first explorations of modality-specific and hierarchical value estimation for multi-modal vision-based RL tasks. Extensive experiment results demonstrate the effectiveness of our approach. However, one limitation of our work is that we mainly focus on the value estimation of multiple visual modalities, while the effectiveness of other forms of modalities (_e.g._, audio, text) is not verified. In our future work, we will consider utilizing both visual and other modalities, forming a more generalized multi-modal RL framework.

## 6 Acknowledgement

The study was funded by the Key-Area Research and Development Program of Guangdong Province with contract No. 2021B0101400002, the National Natural Science Foundation of China under contracts No. 62027804, No. 61825101, No. 62088102 and No. 62202010, and the major key project of the Peng Cheng Laboratory (PCL2021A13). Computing support was provided by Pengcheng Cloudbrain.