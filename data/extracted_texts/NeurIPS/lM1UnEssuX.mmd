# Hypothesis Selection with Memory Constraints

Maryam Aliakbarpour

Department of Computer Science

Rice University

Houston, TX 77005

maryama@rice.edu

&Mark Bun

Department of Computer Science

Boston University

Boston, MA 02215

mbun@bu.edu

Adam Smith

Department of Computer Science

Boston University

Boston, MA 02215

ads22@bu.edu

###### Abstract

Hypothesis selection is a fundamental problem in learning theory and statistics. Given a dataset and a finite set of candidate distributions, the goal is to select a distribution that matches the data as well as possible. More specifically, suppose that we have sample access to an unknown distribution \(P\) over a domain \(\) that we know is well-approximated by one of a class of \(n\) distributions (a.k.a. hypotheses), \(\{H_{1},H_{2},,H_{n}\}\). The goal is to design an algorithm that outputs a distribution \(\) whose total variation distance from \(P\) is nearly minimal. In this work, we study the hypothesis selection problem under memory constraints. We consider a model where samples from \(P\) are presented in a stream and we access each sample \(x\) via "PDF-comparison" queries that allow us to compare the probability densities of any pair of hypotheses at the domain point \(x\) (i.e., is \(H_{i}(x)<H_{j}(x)\)?). This model allows us to study how much needs to be stored, at any point in time, about the portion of the stream seen so far. Our main result is an algorithm that achieves a nearly optimal tradeoff between memory usage and sample complexity. In particular, given \(b\) bits of memory (for \(b\) roughly between \( n\) and \(n\)), our algorithm solves the hypothesis selection problem with \(s\) samples, where \(b s=O(n n)\). This result is optimal up to an \(O( n)\) factor, for all \(b\).

## 1 Introduction

Learning the probability density function of observed data is a fundamental question in statistics with numerous applications in machine learning.Variants of this problem have been studied for nearly a century. _Hypothesis selection_ is a classic version of this problem where the goal is to learn a distribution within a pre-specified class. Let \(\{H_{1},H_{2},,H_{n}\}\) be a class of \(n\) known distributions over the same domain \(\). Suppose that we have sample access to an unknown distribution \(P\) over \(\) which is in, or a very close to, a member of \(\). The goal is to design an algorithm that outputs a distribution \(\) whose total variation distance to \(P\) is close to that of the closest distribution in \(\). A great deal of effort has been dedicated to solving this problem using a number of samples that does not depend on the domain size \(||\). Perhaps surprisingly, one can learn an unknown distribution \(P\) with \(O( n)\) samples, independent of the domain size. In contrast, learning an arbitrary distribution \(P\) over \(\) requires \((||)\) samples. Refined guarantees for this problem have been studied extensively, building an understanding of the accuracy, samplecomplexity, and computational efficiency achievable, as well as the compatibility of hypothesis selection with robustness and privacy .

In this paper, we expand upon the emerging theory of learning with limited memory  by studying hypothesis selection from this new perspective. We strive to answer the following questions: Given a working memory of \(b\) bits, how many data points (samples) do we need to solve the hypothesis selection problem? Prior work assumes that we can store all the samples in the memory, which can be quite large when we aim to learn a distribution over extremely large objects, such as, DNA sequences, gene expression data, text data, and brain image data.

We consider a model where samples are processed one at a time in a stream. Similar to models in , we access each sample \(x\) via queries that allow us to compare the PDF of \(H_{i}\)'s at point \(x\), and we measure the size of the memory retained between processing samples. Our main result is an algorithm that achieves a nearly optimal tradeoff between the memory size \(b\) and the number of samples \(s\). Given \(b\) bits of memory (for \(b\) roughly between \( n\) and \(n\)), our algorithm solves the hypothesis selection problem with \(s\) samples where \(b s=O(n n)\). A result of Shamir [15, Theorem 2] gives a class of \(n\) hypotheses \(\) such that any algorithm that learns unknown distributions in \(\) requires \(s b=(n)\). Our tradeoff is thus optimal up to a \(O( n)\) factor.

### Main result

Suppose we have a class of \(n\) known distributions \(\{H_{1},H_{2},,H_{n}\}\) and an unknown distribution \(P\). We aim to design a _proper learning_ algorithm that outputs a distribution in \(\) whose total variation distance to \(P\) is comparable to that of the closest distribution in \(\). The algorithm has access to a stream of i.i.d. samples drawn from \(P\). At every given point, the algorithm can look at one sample, namely \(x\), in the stream and query a _PDF-comparator_ oracle by sending \(i,j[n]\), and receiving a bit indicating if \(H_{i}(x)<H_{j}(x)\). This is equivalent to asking whether a sample \(x\) is in the _Scheffe set_ of two distributions \(H_{i}\) and \(H_{j}\), which is defined as the set of elements to which \(H_{j}\) assigns higher probability than \(H_{i}\). The algorithm can also discard the current sample and move on to the next one. In addition to having access to the samples of \(P\), we have more direct access to the known distributions in \(\). The algorithm can query the probability masses of the Scheffe sets according to each \(H_{i}\) as is customary in the literature . This assumption can be relaxed; only estimates of such probabilities are needed, which can be obtained if we have sample access to \(H_{i}\)'s. Further details are available in Remark 3. To summarize our access model, our algorithm can make one of the following types of queries: 1) Request a new sample. 2) Query the PDF-comparator on the current sample \(x\) and ask if \(H_{i}(x)<H_{j}(x)\) for every pair of indices \(i,j[n]\). 3) Asks for the probability mass of Scheffe set between \(H_{i}\) and \(H_{j}\) according to \(H_{i}\) for every pair of indices \(i,j[n]\).

The accuracy of our output distribution is measured with respect to how far the _best_ distribution in \(\) is from \(P\). We define \((,P)_{H}\|P-H\|_{}\). When \(\) and \(P\) are clear from context, we denote this simply by \(\). With this setup in mind, we define a _proper learning algorithm (with promise)_ for hypothesis selection. The term "promise" refers to the extra parameter \(\) that the algorithm receives as input: the learner may assume that \(\). (See Remark 2 for the case where \(\) is not provided.)

**Definition 1.1**.: _Suppose \(\) has sample access to an unknown distribution \(P\). And, it can query the probabilities of the Scheffe sets according to each \(H_{i}\) and a PDF-comparator for every pair of hypotheses in \(\). Assume \(\) is given an additional input parameter \(>0\). We say algorithm \(\) is an \((,,)\)-proper learner with a promise for \(\) if the following holds: for every distribution \(P\), if \((,P)\) then, with probability at least \(1-\) over its input sample (drawn i.i.d. from \(P\)) and internal coin tosses, \(\) outputs a distribution \(\) such that:_

\[\|P-\|_{}+\.\] (1)

One can generally reduce \(\) and \(\) by increasing the number of samples taken or the time spent, whereas the value of \(\), and hence \(\), is inherent to the problem instance. Therefore, it is more important and challenging to design algorithms that minimize the multiplicative parameter \(\).

Main theorem:Our main result is a proper learner (with promise) that obtains a nearly optimal tradeoff between memory and data. Formally, we have the following theorem:

**Theorem 1**.: _There exists a constant \(c\) for which the following holds. Let \(\) be an arbitrary class consisting of \(n\) distributions, and let \(>0\). For every natural number \(b\) where \(c( n+(( n)/)) b n\), there exists an \((=9,,=0.1)\)-proper learner (with promise) for \(\) using \(b\) bits of memory and the following number of samples:_

\[s=\{ O( }())&b\,,\\ O(}( ))& b n\,. .\]

Roughly speaking, this theorem says that given \(b\) bits of memory that suffice to perform basic operations such as keeping track of indices and counting samples (i.e., \(b=( n+(( n)/))\)), then \(s b O(n(n)/^{2})\) samples suffice to solve the hypothesis selection problem. A result of Shamir [20, Theorem 2] gives a class of \(n\) hypotheses \(\) such that every algorithm that learns a random \(P\) in \(\) requires \(s b=(n)\). Our tradeoff is thus optimal up to an \(O( n)\) factor. We speculate that our result is tight even for the class considered in , but we leave the proof of a tighter lower bound to future work.

**Remark 2**.: _Both versions of the hypothesis selection problem (with and without a promise that \(\)) have been extensively studied (e.g. ). For simplicity, we present our result when \(\) is given a priori. Section C describes a reduction that yields a similar result when \(\) is not provided; the accuracy guarantee is analogous to that of Equation 1, but for slightly larger \(\)._

### Overview of our key ideas and other results

Background on comparing two hypotheses:Like many previous hypothesis selection algorithms, our algorithm relies on the ability to compare two hypotheses \(H_{1}\) and \(H_{2}\) based on the probabilities of their Scheffe set \(S\). We estimate \(P(S)\), and declare \(H_{i}\) the winner for which \(H_{i}(S)\) is closer to \(P(S)\). This natural approach works especially well when \(P\{H_{1},H_{2}\}\) or when \(P\) is much closer to one of the two hypotheses. Intuitively, a near-optimal hypothesis \(H\) should win many comparisons against other hypotheses. A typical hypothesis selection algorithm will run a tournament to identify such an \(H\). The primary advantage of using Scheffe based comparisons is that they are inexpensive to compute. Only \(O( n/^{2})\) samples are sufficient to estimate the probability masses of all Scheffe sets within an error of \(\). This exceptional sample efficiency enables the hypothesis selection algorithms circumvent the lower bound of \((||/^{2})\) for distribution learning in cases where we have prior knowledge that \(P\) is close to a particular class of distributions.

That being said, the Scheffe based comparisons are not straightforward. One challenge is that these comparisons are not ideal: even if our estimates of the probabilities of the Scheffe sets were perfect, we might pick the hypothesis that is further from \(P\) in total variation distance (because \(S\) is just one event and might not ideally distinguish \(P\) from either \(H_{1}\) or \(H_{2}\)). Furthermore, comparisons are not transitive (again, because we measure only Scheffe set probabilities). Essentially the only useful property they have is this: if \(H_{1}\) is \(\)-close to \(P\), it will win; if \(H_{1}\) is much further from \(P\) (by a constant factor) than both \(\) and \(\|H_{2}-P\|_{}\), then it will lose. These issues make the outputs of comparisons hard to interpret and complicate their use for selection.

Our terminology:To elaborate on our approach, we start by defining a _acceptable_ hypotheses, and our general terminology for referring to the quality of the hypothesis. We partition the hypotheses into three groups: 1) Excellent hypotheses, whose distance to \(P\) is \(\). 2) Decent hypotheses, whose distance to \(P\) is larger than \(\), but smaller than \(3\,+\). These distances are set in a way that decent hypothesis may win against an excellent hypothesis. Decent hypotheses are challenging to deal with because they may fool us into believing that an excellent hypothesis is not a good output, yet later they may lose against a very far hypothesis. 3) Unacceptable hypotheses, whose distance to \(P\) is larger than \(3+\), which will lose to every excellent hypotheses. We say a hypothesis is _acceptable_ if it is either excellent or decent. Generally, the goal is to find an acceptable hypothesis.

#### 1.2.1 Random ladder tournament

The main technical tool we introduce is a simple algorithm we call the _random ladder tournament_, which is inspired by _ladder tournaments_, a type of tournaments used in games and sports. Our algorithm finds an acceptable hypothesis using a linear number of comparisons. The algorithm proceeds in rounds as follows. Holding on to a single hypothesis in each round, we sample a uniformly random hypothesis from \(\) and compare it to the current hypothesis. The winner proceeds to the next round. Our novel analysis of this tournament shows that after \(O(||)\) rounds, either the final winner produced at the end of the tournament or a winner selected randomly along its trajectory will be acceptable with high probability.

Proof overview:As previously mentioned, comparisons are often far from ideal; Even when the excellent hypothesis wins at a certain step, there is no guarantee that it would remain the winner till the end. To evade this issue, we utilize our knowledge of the upper bound of \(\), denoted by \(\), and modify the comparisons in a way that ensures the excellent hypothesis never loses once it wins. In our comparisons, we favor the current winner; If the distance of the current winner to \(P\) on the Scheffe set is at most \(\), we declare the current winner as the winner (even when the opponent hypothesis seems closer). Clearly, the excellent hypothesis will never be more than \(\) away for \(P\) on any set. Hence, it will never lose again.

However, what happens when the excellent hypothesis never wins? At each step, we pick each hypothesis in \(\) uniformly at random, implying the excellent hypothesis is expected to be chosen in at least \(O(1)\) steps in our algorithms. The fact that the random hypothesis loses to the current winners of those steps indicates that those winners must be acceptable hypotheses; otherwise, the excellent hypothesis would have won them. Additionally, the fact that the excellent hypothesis appears as the random hypothesis in \(O(1)\) steps and loses to all of them, confirms that a constant fraction of the current winners are indeed acceptable. By combining these observations, we prove that the random ladder tournament will either find the excellent hypothesis at the end or encounter many acceptable hypotheses along the way.

Implementing the tournament with memory constraints:This algorithm is a highly effective technical tool for solving our problem. It can be implemented in a very memory-efficient manner. The algorithm processes the hypotheses in an "online" fashion, and it does not need to memorize the result of any past comparisons. At any given moment, it only remembers two hypotheses: the current hypothesis and the next randomly drawn hypothesis. And, if we draw fresh samples for every comparison, we can implement this algorithm by storing essentially \(O(1)\) numbers but sampling \(O(n\,(n)/^{2})\) data points. On the other extreme, we can implement this algorithm with very little data, i.e., \(O( n/^{2})\) samples, but a large amount of memory by storing an appropriate "summary" of samples (e.g., the probabilities of all the Scheffe sets in \((n^{2})\) bits).

We leverage this flexibility in memory usage of the random ladder tournament and provide an algorithm that only uses \(b\) bits of memory. We perform the comparisons in the random tournament in blocks of size \(t\). For each block of comparisons, we draw new samples and store a summary of them. We pick the parameter \(t\) in way that the summary fits in \(b\) bits, and it is sufficient for us to perform the next the \(t\) comparisons. For example, if \(t\), we can store the number of samples in all the Scheffe sets of \(t\) hypotheses in \(b\) bits. This summary is enough for us to perform any comparisons between those hypotheses. Combined with a novel summary--sorted lists, described in Section 2.2.2--we get a tradeoff of the form \(s b O(n^{3}(n)/^{4})\). In Section 3.2, we presented this tradeoff alongside another tradeoff (with better dependence to \(\), but worse dependence to \( n\)). Both of these tradeoff are worse than our main result, in a sense that \(s b\) is larger by a factor of \(O(((n),1/))\). However, the these results are yielding more accurate proper learners; For the random ladder tournament, \(=3\), while in our main result \(=9\).

Simpler linear-time selection:The random ladder tournament leads to new results for the hypothesis selection problem outside of the scope of our original motivation for this paper. If we ignore memory constraints and store every sample, the random ladder tournament algorithm is simultaneously near-optimal in terms of number of samples, time, and accuracy -- to our knowledge, it is the first such algorithm. Specifically, it makes a linear number of comparisons and uses \(O( n/^{2})\) samples from \(P\), which are both optimal for worst-case choices of \(\). Moreover, it finds a hypothesis with optimal accuracy under a promise. More precisely, it finds a hypothesis in \(\) that is roughly\(3\,\)-close to \(P\) given a parameter \(\) that is guaranteed to be at least \(\). This is essentially optimal, as no proper learning algorithm can achieve accuracy better than \(3\,\) even when \(=\) is given to the algorithm. Furthermore, with a simple adjustment to our algorithm, one can solve the hypothesis selection problem without any knowledge of \(\) in nearly linear time and with roughly the same number of samples and obtain a \(5\,\)-close hypothesis in \(\). See Section C.

#### 1.2.2 Improved tradeoff by hypothesis filtration

The flexibility of the random ladder tournament already results in memory-data tradeoffs, but these are suboptimal by a factor of \(( n,1/)\). To improve the result, we design an algorithm called _filter_, that picks an acceptable hypothesis with a modest chance. While this chance is not high, we can use the output of the filtration and run the random ladder tournament on these filtered hypotheses. Since the quality of the input hypotheses in this second round is better, the random ladder tournament can be implemented effectively on a smaller set \(\) and results in a better tradeoff. Hence, we obtain our main result. See Section B.4 for the proof of our main results. Below, we give a short description of the filter algorithm. We defer the full description and the proofs to Section B.3.

hypothesis filtration:The best approach to finding an acceptable hypothesis drastically depends on the quality of the hypotheses in \(\). For example, if there is no decent hypothesis in \(\), any single elimination tournament will easily find an excellent hypothesis due to the fact that any comparison involving at least one excellent hypothesis will declare the excellent hypothesis the winner. On the other hand, if there are a lot of decent hypotheses, then there is a decent chance that a randomly selected hypothesis is decent, hence an acceptable one. The filtration algorithm works by combining these two observations. The algorithm randomly performs one of the following, each with probability \(1/2\): 1) It draws a set of random hypotheses. Assuming there is no decent hypothesis in the set, it runs a _group elimination tournament_ (see below), and outputs the result. 2) It outputs a random hypothesis in \(\). One can show that if the decent hypotheses are scarce in \(\), we have a decent chance of getting an excellent hypothesis and no decent hypothesis in the random set; hence, we find an excellent hypothesis via our group elimination algorithm. On the other hand, if the decent hypotheses are abundant, then we have a decent chance of picking one in the second step. By selecting each strategy with a probability \(1/2\), we preserve half of the success probability in whichever case holds. While this probability is not a lot, it is larger than \(1/n\) (the probability of picking the excellent hypothesis from \(\)). We take advantage of this fact, and show one can run the random ladder tournament in fewer rounds and obtain the desired result. See Section B.3 for more details.

Group elimination tournament:Our second technical tool is an algorithm that finds an excellent hypothesis if no decent hypothesis exists in \(\). This algorithm is a combination of a single-elimination tournament and an _all-go-against-all_ tournament. In this algorithm, at every step, we partition the hypotheses into multiple groups, run a previously known all-go-against-all tournament within each group (say minimum distance estimate ), and then send the winners of each group to the next round. If a group contains an excellent hypothesis, then in an all-go-against-all tournament, we will pick an excellent hypothesis as the winner. Hence, an excellent hypothesis will "bubble up" in each round and be declared the final winner. With a careful choice of group sizes and the number of rounds, this algorithm can be implemented using \(O(n)\) bits of memory and \(O( n)\) samples under the assumption that the hypotheses are indexed from 1 to \(n\). A technical challenge that arises, however, is that we need to run this algorithm on a subset of roughly \(O(b)\) random hypotheses (so that we can run it within a memory bound of \(b\)). However, storing \(b\) random indices already requires \(O(b n)\) space. We show that we can instead draw indices pseudorandomly using a pairwise independent generator that can be implemented in small space. See Section B.1 for more details.

### Related work

Prior to our work, numerous studies have investigated the problem of _hypothesis selection_ in various settings. In their seminal work, Yatracos , and Devroye and Lugosi  present algorithms to find a close distribution in \(\) based on only estimation of the probabilities of the Scheffe sets of pairs of hypothesis in \(\). These approaches suggest that we only need accurate estimation of the probability that \(P\) assigns to \(O(||^{2})=O(n^{2})\) sets, resulting in sample complexity proportional to \(O( n)\) instead of \((||)\). For a comprehensive overview, see Chapter 6 in .

Mahalanabis and Stefankovic in  provide an algorithm with linearly many probes to \(P\) and \(=3\) but they require an exponential time pre-processing of the class \(H\).1Daskalakis and Kamathin  give a nearly-linear time algorithm that given an upper bound on \(\), returns \(\) such that \(\|-P\|_{}+\) for some large constant \(=512\). Acharya et al. in  provide a \(O(n n)\) time algorithm that finds a hypothesis with accuracy guarantee of \(=9\). For proper learners, Bousquet et al. in  showed that the best \(\) in Equation 1 that one can hope for is \(=3\) as long as the sample complexity does not grow with the domain size \(||\). Aamand et al. in  also study statistical computational tradeoffs (time-data) for the hypotheses over _discrete domains_. Unlike our work, their setting allows pre-processing of the class \(\) in polynomial time.

There is a long history of research focusing on a special case of this problem where \(\) is a _specific structured class of distributions_ such as mixtures of Gaussians , histograms , and polynomials . The abstract hypothesis selection we study here is commonly used as a subroutine in solving these problems (usually in conjunction with some sort of a cover method). For a survey of results, see .

Lastly, there is another field of study that tackles a question similar to ours: the problem of sorting items with noisy comparisons. One can view hypothesis selection as the task of finding the minimum item. With the exception of  that we have discussed above, these probabilistic noise models do not capture the geometric structure presenting in our problem. Therefore, we did not find any of these result yielding an immediate solution to our problem.

## 2 Preliminaries

### Notation

Let \([n]\) denote the set \(\{1,2,,n\}\). Suppose that we have a probability distribution \(P\) over a domain \(\). For element \(x\) in \(\), \(P(x)\) denotes the probability of \(x\). We assume \(\) is the domain of all the probability distributions in this article. For any subset of the domain \(S\), \(P(S)\) denotes the sum of the probabilities of all the elements in \(S\). We denote the total variation distance between two distributions \(P_{1}\) and \(P_{2}\) by \(\|P_{1}-P_{2}\|_{}\), and it is defined as \(_{S}|P_{1}(S)-P_{2}(S)|\) where \(S\) is any measurable subset of the domain. We say \(P\) is _\(\)-close_ to \(Q\) iff \(\|-Q\|_{}\) is at most \(\). Also, we say \(P\) is _\(\)-far_ from \(Q\) iff \(\|P-Q\|_{}\) is greater than \(\). We denote the Scheffe set of two distributions as follows: \((H_{1},H_{2})\{x H_{1}(x)<H_{2}(x)\}\).

### Basic tools

In this section, we focus on our primary tools we have used throughout this article. First, we start with a comparisons algorithm that allows us to compare two hypotheses. Next, we use the fact that one can estimate the probabilities of the \(O(n^{2})\) many Scheffe sets up to error \(c\) (for some small constant \(c<1\)) with probability at least \(1-\) using \(O((n/)/^{2})\) samples from \(P\).

#### 2.2.1 Comparing two hypotheses

Our algorithms works based on a basic operation that allows us to compare two hypotheses \(H_{1}\) and \(H_{2}\) based on the probabilities of their Scheffe sets. We pick the hypothesis that appears to be closer to \(P\) and declare it the _winner_ (and the other hypothesis the _loser_). The challenging part is that these comparisons are not ideal. As in, even when our estimations of the probabilities of the Scheffe sets are accurate enough, we may pick a hypothesis that is further. However, one can guarantee that if the distance of the further hypothesis among \(H_{1}\) and \(H_{2}\) is much worse than the distance of the closer one, the comparison procedure would never select it as the winner. The algorithm is given three parameters: 1) \(\): an upper bound for \(\); 2) \(\): indicating the estimation error for the Scheffe set probabilities is \(()\); and 3) the confidence parameter \(\). Upon invoking this algorithm, it determines the result of the comparison with the guarantees formalized in the following lemma:

**Lemma 2.1**.: _Upon receiving three parameters: \(\), \(\), and \(\), Algorithm 1 uses \(O((1/)/^{2})\) samples and satisfies the following guarantees with probability at least \(1-\):_

* _If_ \(H_{1}\) _is_ \(\)_-close to_ \(P\)_, then the algorithm returns_ \(H_{1}\)_._
* _If_ \(H_{1}\) _is_ \((-P\|_{})}+2\,\|H_{2}-P\|_{}+ )\)_-far from_ \(P\)_, then the algorithm returns_ \(H_{2}\)The proof of this lemma is provided in Section D.

```
1:procedurecompare(\(H_{1},\ H_{2},\ ,\ ,\ \), sample access to \(P\))
2: Draw \(m=((1/)/^{2})\) samples from \(P\)
3:\(\) fraction of samples from \(P\) in \((H_{1},H_{2})\) using the PDF-comparator.
4:\(p_{1} H_{1}((H_{1},H_{2}))\).
5:\(p_{2}=H_{2}((H_{1},H_{2}))\).
6:\(^{}/2\)
7:if\(|p_{1}-|+^{}\) or \(|p_{1}-||p_{1}-|\)then
8: Output \(H_{1}\). \(\)\(H_{1}\) is the winner.
9:else
10: Output \(H_{2}\). \(\)\(H_{2}\) is the winner. ```

**Algorithm 1** Choosing between two hypotheses

**Remark 3**.: _While we have assumed that we have access to the probabilities of the Scheffe sets in the above algorithm, this assumption is not necessary. In fact, one can obtain similar guarantees to Lemma 2.1 as long as we have estimates of these probabilities up to accuracy \(c\) for a sufficiently small constant \(c<1\). Hence, our result can be adjusted to the setting that we have sample access to \(H_{i}\)'s instead._

Our terminology based on comparisons:We say a distribution \(H\) is _excellent_ iff \(\|H-P\|_{}\) is \(\). We usually use \(H^{*}\) to denote an excellent hypothesis. We say a distribution \(H\) is decent iff \(\|H-P\|_{}\) is greater than \(\), but smaller than \(3\,+\). It implies that a decent hypothesis may win an excellent \(H^{*}\). We say a distribution \(H\) is _acceptable_ iff it is excellent or decent. And, we say a distribution \(H\) is _unacceptable_ iff it is not acceptable. Note that it is impossible for an unacceptable hypothesis to win an excellent hypothesis with probability greater than \(\). Throughout this article, when we say _valid comparisons_, we refer to an event in which the Scheffe sets are estimated accurately, so the conditions in Lemma 2.1 hold. Since we have only \(O(n^{2})\) many Scheffe sets, using \(O((n/)/^{2})\) samples is enough to assume all the comparisons are valid with a probability of at least \(1-\).

#### 2.2.2 Space efficient sample summaries for comparisons within a batch of hypotheses

In order to obtain sample efficiency, we need sufficient information about samples that enables us to reuse the same set of samples for multiple comparisons. Below, we describe two approaches to _summarize_ the sample set.

Scheffe counts.Observe that to compare two hypotheses \(H_{1}\) and \(H_{2}\), we only use the count of the number of samples in their Scheffe set \((H_{1},H_{2})\). Hence, instead of storing samples directly, for every pair of hypotheses in \(\), we store the number of samples in their Scheffe set. We refer to this number as the _Scheffe counts_. If we have \(m\) samples, we can store all the \(O(n^{2})\) Scheffe counts using \(O(n^{2} m)\) bits.

Storing each sample via a sorted list.In this approach, we store a succinct summary of each sample that allows us to infer its membership in each Scheffe set. This information suffices to perform the comparisons we need later. To store a sample \(x\), we save an ordered list of hypotheses that is sorted according to the probability of \(x\) (the PDF at point \(x\)). That is, we store a list of indices \(i_{1},i_{2},,i_{n}\) such that \(H_{i_{1}}(x) H_{i_{2}}(x) H_{i_{n}}(x)\) (use hypothesis indices to break the tie). The summary of a single sample can be computed in \(O(n n)\) time and takes \(O(n n)\) bits of space to store. Now, to compare the PDF of two hypotheses \(H_{i}\) and \(H_{j}\), we simply can find \(i\) and \(j\) in the list and check whether \(i\) precedes \(j\) in the list.2 Alternatively if we want to compare PDF's faster, we can store the indices in another list call \(d_{x}\) that maps indices to list positions. We set \(d_{x}[i_{}]=\) for every \([n]\). In this case, to compare the PDF's, we can simple check if \(d_{x}[i]<d_{x}[j]\).

#### 2.2.3 All-go-against-all tournament

There are standard techniques in the literature to solve hypothesis selection problem when the estimates of all the Scheffe sets are available. See Chapter 6 in  and . These approachessuggest that we only need accurate estimation of the probability that \(P\) assigns to \(O(||^{2})=O(n^{2})\) sets, therefore, the sample complexity can be proportionate to \(O( n)\) instead of \(O(||)\). For a formal statement, see Fact D.1.

## 3 Random ladder tournament

In this section, we focus on the _Random ladder tournament_ which is a proper learner (with promise) of \(P\) in a finite class of \(n\) hypotheses, \(\). The algorithm is given a parameter \(\) as an upper bound for \((,X)\). It outputs \(\) such that, with high probability we have: \(\|-P\|_{} 3\,+\,\). Bousquet et al. in  have shown that no proper learner can achieve an error better than \(3\,+\) unless the sample complexity grows with the domain size \(||\). Their lower bound holds even when the algorithm is provided with \(=\). Hence, the factor \(=3\) above is optimal.

For clarity in our presentation, in Section 3.1, we present the random ladder tournament assuming that: 1) We set the confidence parameter to be a small constant (\(=0.1\)). 2) the algorithm can call the Compare subroutine without specifying how this subroutine is implemented. In Section A.1, we show one can modify this algorithm to work for arbitrary small confidence parameter \(\). Also, in Section 3.2, we discuss how one can implement this algorithm with memory constraints. We use the sample summary approaches described in Section 2.2.2, and we obtain two (sub-optimal) memory-data tradeoffs for this algorithm.

### Random ladder tournament with no memory constraints

In this section, we present the random ladder tournament that solves the hypothesis selection problem while a parameter \(\) is given to the algorithm as an upper bound for \(\). For this result, we assume there is a _meta distribution_ over \(\), denoted by \(\), with a non-negligible probability \(p_{0}\) to draw a hypothesis that is \(\)-close. The algorithm considers hypotheses one at a time where at step \(i\), we draw \(R_{i}\) from the meta distribution. While later we use this algorithm with other meta distributions, it may help the reader to view the meta distribution as a uniform distribution over a set of \(n\) hypotheses, \(\). In this case, if we draw a random hypothesis from \(\), it is guaranteed to be \(\)-close to \(P\) with probability \(p_{0} 1/n\). Our algorithm finds a sufficiently close hypothesis using \((1/p_{0})\) comparisons.

At a high-level, our algorithm goes through a list of randomly drawn hypotheses from \(\), compares them, and keeps track of the _current winner_ hypothesis. We denote the current winner hypothesis at step \(i\) by \(W_{i}\). Initially, we start with \(W_{0}\) being equal to a fake hypothesis that loses to any other hypothesis. Then at every step, we take a randomly drawn hypothesis \(R_{i}\) and compare it with \(W_{i-1}\). We set the current winner of step \(i\), \(W_{i}\), be the winner of a comparison between \(W_{i-1}\) and \(R_{i}\). We show that after \(k=(1/p_{0})\) steps either the final winner, \(W_{k}\), is an \(\)-close hypotheses, or many of \(W_{i}\)'s that we have encountered are close to \(P\). That is, either \(W_{k}\) is an excellent hypothesis or a random \(W_{i}\) is an acceptable choice. To exploit this fact at every step, we add each \(W_{i}\) to a list, namely \(Q\), with some small probability. At the end, we also add \(W_{k}\) to \(Q\). We prove a random hypothesis in \(Q\) will be close to \(P\) with high probability. Algorithm 2 shows the formal description of our approach, and we prove its performance in Theorem 4. Figure 1 illustrates a visual representation of the algorithm.

**Theorem 4**.: _Suppose we can draw i.i.d. random hypotheses from an arbitrary meta distribution \(\) over \(\). And, we have a hypothesis \(P\) that we aim to learn properly in \(\). Assume we are given parameters \(p_{0}\) and \(\) such that the probability that a random hypothesis is \(\)-close to \(P\) is at least \(p_{0}\). For any \((0,1)\), Algorithm 2 is \((=3,,=0.1)\)-proper learner (with promise) for the class \(\)._

Proof.: First, note that each comparison, invoked in Line 7, will satisfies the properties of Lemma 2.1 with probability at least \(1-1/(100\,k)\). Hence, using Lemma 2.1 and the union bound, one can assume with probability \(0.99\) for all the comparisons we have:

* If \(W_{i}\) is \(\)-close to \(P\), it will not lose; it remains as the current winner for the rest of the steps: \(W_{i}=W_{i+1}==W_{k}\).
* If \(W_{i-1}\) is \((3+)\)-far from \(P\), and \(R_{i}\) is \(\)-close to \(P\), then \(W_{i}\) is equal to \(R_{i}\).

For the rest of this proof, we fix the response of comparisons for which the above conditions hold. The randomness used in the rest of this proof only depends on the randomness in the meta distribution over hypotheses and the internal coin tosses of the algorithm.

Our goal is to show that most of the hypotheses in the list, \(Q\), are acceptable, so a randomly selected hypothesis in \(Q\) will be acceptable as well. Our first step is to show that the expected number of acceptable hypotheses in \(Q\) is high compared to the size of \(Q\). For each \(i\) in \([k]\), we define an indicator random variable \(I_{i}\) that is one if we add an acceptable hypothesis to the \(Q\) at step \(i\) in Line 8, and zero otherwise. Also, we define another indicator variable \(I_{k+1}\) corresponding to the event that the final \(W_{k}\), which we added to \(Q\) in Line 9, is an acceptable hypothesis. More formally, we have:

\[I_{i} _{W_{i}W_{i}Q.} 28.452756pt i[k]\,,\] \[I_{k+1} _{W_{k}}\,.\]

Clearly, the sum of \(I_{i}\)'s indicates the number of acceptable hypotheses in \(Q\), so we focus on finding a lower bound for the expected value of this quantity. Let \(_{i}\) be the probability of \(W_{i}\) being acceptable. Without loss of generality, we set \(_{0}\) equal to zero. It is not hard to see that: \(I_{i}=p_{0}_{i}\) for \(i[k]\). Moreover for the last indicator variable, we have:

\[I_{k+1} =W_{k} \|W_{k}-P\|_{}\] \[=_{i=1}^{k}W_{i-1}R_{i}\,,\|R_{i}-P\|_{}\,,R_{i}R_{i+1},R_{i+2},,R_{k}\,.\]

Assuming the pairwise comparisons are done perfectly, \(R_{i}\) being \(\)-close to \(P\), automatically implies that \(R_{i}\) does not lose to any of the hypotheses \(R_{i+1},,R_{k}\). Thus, we have:

\[I_{k+1} _{i=1}^{k}W_{i-1}R_{i}\,,\|R_{i}-P\|_{}\] \[=_{i=1}^{k}W_{i-1}R_{i}\|R_{i}-P\|_{} \|R_{i}-P\|_{}\,.\]

Note that the probability of \(\|R_{i}-P\|_{}\) is at least \(p_{0}\). Now given that \(R_{i}\) is \(\)-close to \(P\), it will certainly win any hypotheses that is \((3\,+)\)-far from \(P\). Thus, the event that \(W_{i}\) is \((3\,+)\)-far must have a lower probability than the event that \(W_{i-1}\) loses to \(R_{i}\). Therefore, we obtain the following lower bound:

\[I_{k+1} _{i=1}^{k}\|W_{i-1}-P\|_{}>(3\, +)\|R_{i}-P\|_{} p_{0}\] \[=_{i=1}^{k}W_{i-1}\|R_{i}-P\|_{} p_{0}\,.\]

Recall that we pick \(R_{i}\) independently from all the previous hypothesis, \(R_{1},R_{2},,R_{i-1}\), so one can say \(W_{i-1}\) is independent of \(R_{i}\). This implies:\[[_{k+1}]_{i=1}^{k}[_{i-1}\ ] p_{0}=_{i=1}^{k}(1-_{i-1}) p_{0}\,.\]

Putting it all together, the expected value of the sum of \(I_{i}\)'s is:

\[[_{i=1}^{k+1}I_{i}]_{i=1}^{k}p_{0} _{i}+(1-_{i-1}) p_{0}=k p_{0}+p_{0}(_{k}-_{0} ) k p_{0}\,.\]

The last inequality above is due to the fact that we set \(_{0}\) to zero. Note that the above equation states that the expected number of acceptable hypotheses in \(Q\) is at least \(k p_{0}\). On the other hand, the expected number of hypotheses in \(Q\) is \(k p_{0}+1\). Thus, the expected number of unacceptable hypotheses in \(Q\) is at most one. Now, by Markov's inequality, the probability that we have more than 50 unacceptable hypotheses in \(Q\) is at most 0.02. And, by the Chernoff bound, we know that the probability of having less than 1000 many hypothesis in \(Q\) is at most:

\[[\,\#Q<1000]=(k,p _{0})<}{2}(-}{8} ) 10^{-100}\,.\]

It is not hard to see that the ratio of unacceptable hypotheses to the total number of hypotheses in \(Q\) is at most 50/1000=0.05. Therefore, a random hypothesis in \(Q\) is acceptable with probability at least \(0.95\). Hence, by the union bound the total error probability is bounded by:

\[[\,] [\,|\ ]\] \[+[] 0.05+0.01<0.1\]

Therefore, the proof is complete. 

### Memory-data tradeoffs of random ladder tournament

As we have discussed, one of the advantages of Algorithm 2 is its flexibility in the usage of memory and data. In the following, we describe a tradeoff that we can obtain for this algorithm using the sample summaries presented in Section 2.2.2. At a high-level, the following is how we achieve the tradeoff: suppose we have \(b\) bits of memory. We choose the largest integer \(t\) so that we can store the sample summary needed to compare \(t\) hypotheses. Then, we run the random ladder tournament while we draw new samples and refresh the sample summary at every \(t\) step. Our two described sample summaries Scheff'e counts and the sorted list lead to the following memory-data tradeoffs. Although these tradeoffs are not as tight as our main result (by factors of \( n\) and \(1/\)), they have better accuracy guarantees (\(=3\) instead of \(=9\) in our main result).

**Lemma 3.1**.: _Suppose we have \(n\) hypotheses in \(\). For every \(p_{0} 1/n\), \(\), and an integer \(t\) between two and \(k=(1/p_{0})\), one can implement Algorithm 2 in such away that it uses:_

\[s=O(}^{-1}}{^{2}})\  b=O(t^{2}(^{-1}}{} )+t n)\ \]

**Lemma 3.2**.: _Suppose we have \(n\) hypotheses in \(\). For every \(p_{0} 1/n\), \(\), and an integer \(t\) between two and \(k=(1/p_{0})\), one can implement Algorithm 2 in such away that it uses:_

\[s=O(}^{-1}}{^{2}})\  b=O(t^{-1}) n}{ ^{2}}+t n)\ \]

For the proofs of the above lemmas, see Section A.2.