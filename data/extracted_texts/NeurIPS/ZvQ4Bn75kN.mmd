# Video Diffusion Models are Training-free

Motion Interpreter and Controller

 Zeqi Xiao\({}^{1}\), Yifan Zhou\({}^{1}\), Shuai Yang\({}^{2}\), Xingang Pan\({}^{1}\)

\({}^{1}\)S-Lab, Nanyang Technological University,

\({}^{2}\)Wangxuan Institute of Computer Technology, Peking University

{zeqi001, yifan006}@e.ntu.edu.sg

williamyang@pku.edu.cn, xingang.pan@ntu.edu.sg

###### Abstract

Video generation primarily aims to model authentic and customized motion across frames, making understanding and controlling the motion a crucial topic. Most diffusion-based studies on video motion focus on motion customization with training-based paradigms, which, however, demands substantial training resources and necessitates retraining for diverse models. Crucially, these approaches do not explore how video diffusion models encode cross-frame motion information in their features, lacking interpretability and transparency in their effectiveness. To answer this question, this paper introduces a novel perspective to _understand_, _localize_, and _manipulate_ motion-aware features in video diffusion models. Through analysis using Principal Component Analysis (PCA), our work discloses that robust motion-aware feature already exists in video diffusion models. We present a new MOtion FeaTure (MOFT) by eliminating content correlation information and filtering motion channels. MOFT provides a distinct set of benefits, including the ability to encode comprehensive motion information with clear interpretability, extraction without the need for training, and generalizability across diverse architectures. Leveraging MOFT, we propose a novel training-free video motion control framework. Our method demonstrates competitive performance in generating natural and faithful motion, providing architecture-agnostic insights and applicability in a variety of downstream tasks.

+
Footnote †: Project page at this URL.

## 1 Introduction

Video generation has experienced notable advancements in recent years, particularly in the realm of video diffusion models, such as text-to-video (T2V) generation [15; 5; 45; 43] and image-to-video (I2V) generation [2; 14; 6]. Apart from producing high-quality content in individual frames, capturing authentic and customized motion across frames is a crucial feature of video generation. Thus, understanding and controlling the motion play pivotal roles in video generation.

Most methods [46; 50; 42; 15; 44] that study video motion focus on motion customization, i.e. allowing users to specify a moving direction  or a point-drag command . These methods typically adopt training-based paradigms [46; 50; 42; 15] that introduce motion conditions and train additional modules to ensure that the output videos adhere to these conditions. Despite their progress, these approaches require significant training resources and need retraining for different models, and their effectiveness often remains black-box. More critically, they do not address a fundamental question: _How do video diffusion models encode cross-frame motion information within their features?_Understanding the encoding of motion information is crucial for two reasons: a) it offers architecture-agnostic insights, meaning that such knowledge can be applied across different models and their checkpoints, an important consideration given the rapid evolution of video diffusion models; and b) it supports various downstream applications. For instance, the DIFfusion FeaTure  demonstrates how diffusion features can encapsulate rich semantic information, enabling applications like correspondence extraction [17; 23] and image/video editing [9; 30; 8].

To this end, this paper introduces a novel perspective to _understand_, _localize_, and _manipulate_ motion-aware features in video diffusion models. We first establish that removing content correlation information helps to pronounce motion information in video diffusion features. By applying Principal Component Analysis (PCA)  on these diffusion features, we observe a strong correlation between the principal components and video motions. Further explorations reveal that certain channels of the features play a more significant role in determining motion direction than others. Based on these observations, we present a straightforward strategy to extract motion information embedded in the features, termed MOtion FeaTure (MOFT). Through content correlation removal and motion channel filter, MOFT establishes impressive correspondence on videos with the same motion direction, as illustrated in Fig. 1 (a-b). Importantly, this strategy proves to be generalizable across various text-to-video or image-to-video generation models [15; 14; 43; 4; 2] (Fig. 4), such as AnimatedDiff , ModelScope , and Stable Video Diffusion .

Building upon the motion-aware MOFT, we propose a pipeline for video motion control in a training-free manner, without the modification of model parameters. The approach leverages compositional loss functions for content manipulation [9; 11; 31; 1; 19]. Specifically, we design loss functions to optimize noisy latents in the denoising process with reference MOFT, which can be synthesized via direction signal or extracted from reference videos. Furthermore, our pipeline can be extended for point-drag manipulation. With MOFT guidance to generate coarse motion in the early denoising stages, fine-grained point-drag manipulation with DIFT  guidance becomes feasible for videos. Various experiments showcase the effectiveness of MOFT in controlling the motions of diverse scenarios across different video diffusion models without the need for any training. Remarkably, our training-free method even outperforms some data-driven methods in achieving natural and faithful motion. Our main contributions are summarized as follows:

* We perform a deep analysis of motion information embedded in video generation models. Our work discloses that robust motion-aware feature already exists in video diffusion models.
* Through our analysis, we present MOtion FeaTure (MOFT) that effectively captures motion information. MOFT has several advantages: a) it encodes rich motion information with high

Figure 1: **Characteristics of MOtion FeaTure (MOFT). (a-b) Rich Motion Information: We extract MOFT at the red point in the reference video in (a) and draw similarity heatmaps in (b) across various videos (yellow indicates higher similarity). The heatmap aligns well with the motion flow in the bottom left. (c) MOFT serves as guidance for controlling motion direction in the light-masked region, with the motion direction signal illustrated by red arrows in the first image.**

interpretability; b) it can be extracted in a training-free way; and c) it is generalizable to various architectures.
* We propose a novel training-free video motion control framework based on MOFT. Our method demonstrates competitive performance with natural and faithful motion. Unlike previous training-based methods that need independent training for each different architecture and checkpoint, our method is readily applicable to different architectures and checkpoints.

## 2 Related Works

**Video Diffusion Models.** The field of video generation has witnessed substantial progress in recent years, particularly in the domain of video diffusion models. Noteworthy contributions include advancements in text-to-video (T2V) generation [20; 15; 5; 45; 43; 32] which aim to generate high-fidelity videos that align with textual descriptions. Besides, image-to-video (I2V) [2; 14; 6] takes image conditions as inputs and generates videos aligned with the image. Beyond the production of high-quality content within individual frames, the capability to capture authentic and customized motion across frames stands out as a significant feature in the realm of video generation.

**Diffusion Feature Understanding.** The analysis and comprehension of diffusion features [41; 29; 10; 27; 37] have garnered increasing attention. A comprehensive understanding of diffusion features not only yields architecture-agnostic insights applicable across diverse models and checkpoints but also enhances various downstream applications. For instance, DIffusion FeaTure (DIFT)  demonstrates that diffusion features embed impressive semantic correspondence and can be extracted with a simple strategy. This strategy proves effective across various architectures, spanning image diffusion models  to video diffusion models [15; 43]. Its versatility facilitates a range of applications, including correspondence extraction [17; 23] and image/video editing [9; 30; 8]. Recently, Frececontrol  applied PCA on diffusion features and extracted semantic basics for training-free spatial control. Its method can be generalized to any conditional input and any model. Similarly, video diffusion models encode rich motion information within the features. However, less effort has been made to analyze it.

**Video Motion Control.** Considerable efforts have been dedicated to tailoring video motion according to user preferences [46; 44; 14; 50; 42; 16; 7; 13; 48]. For example, MotionCtrl  facilitates precise control over camera poses and object motion, allowing for fine-grained motion manipulation. VideoComposer  introduces motion control through the incorporation of additional motion vectors, while DragNUWA  proposes a method for video generation that relies on an initial image, provided point trajectories, and text prompts. These methodologies typically rely on training-based paradigms, incorporating motion conditions during training and requiring additional modules to ensure that the resulting videos adhere to these specified conditions. Despite their advancements, these approaches demand substantial training resources, necessitating retraining for different models, and often exhibit a black-box nature in terms of their effectiveness. In contrast, this paper introduces a novel pipeline for controlling video motion using an interpretable motion-aware feature. Notably, this approach is training-free and can be generalized across various architectural frameworks, offering a more versatile and resource-efficient solution.

## 3 MOtion FeaTures (MOFT)

In this section, we first analyze how video diffusion models encode cross-frame motion information, then provide the strategy to extract motion features from pre-trained video diffusion models.

Similar to [41; 49; 29], our analysis focuses on diffusion features extracted from the intermediate blocks of diffusion models. We denote them as \(^{H W F D}\), where \(H\), \(W\), \(F\) and \(D\) are dimensions of height, width, frames, and channels, respectively. As proved by prior works [15; 51; 46], cross-frame features play a crucial role in video motion control. For example, AnimateDiff  trains temporal self-attention LoRAs  that operate on the temporal dimension to control the global motion direction. Consequently, we argue that the temporal dimension encapsulates rich motion information. However, extracting motion information from diffusion features is non-trivial, as they also contain other information such as semantic and structural correlation.

### Content Correlation Removal

Inspired by VideoFusion  which uses shared noise to model content correlation across frames and residual noise to model dynamic difference, we hypothesize that we can filter out the content correlation by eliminating similar information across frames:

\[^{}=-_{i=1}^{F}_{i},\] (1)

where \(_{i}\) indicates the \(i^{}\) frame of feature \(\). The shared latents, to which we refer as content correlation information, encompass shared aspects such as semantic content and appearance. In contrast, the residual latents primarily capture motion information, which also can be interpreted as deformation in structure.

To validate the hypothesis, we apply Principal Component Analysis (PCA)  on \(\) and \(^{}\). Specifically, we create a series of videos with the entire scene moving horizontally or vertically, resulting in a set of features \(\{^{1},^{2},...,^{n}\}\) extracted from videos in the process of DDIM  inversion. In this subsection, we omit the choice of video model architecture and feature selection for simplicity. We analyze and project the \(D\)-dimensional features of the first frame on the leading two principal components (\(_{1}\) and \(_{2}\)). As shown in Fig. 2 (a), the result of the vanilla feature does not exhibit a distinguishable correlation with motion direction. In contrast, as shown in Fig. 2 (b), normalized features are successfully separated by their motion direction. It reveals that the normalization operation removes content correlation information and emphasizes motion information.

Figure 3: **Cross-frame Channel Value. (a) We plot the histogram of the weight of \(_{1}\). It reveals that only a few channels significantly contribute to determining the principal components. (b-c) The motion channels exhibit a pronounced correlation with motion direction trends. (d) In contrast, the non-motion channels show little correspondence with motion direction.**

Figure 2: **Visualization of PCA on video diffusion features. The left side indicates the frame-wise panning direction, with each color representing a specific direction pattern. We apply PCA to diffusion features extracted from videos with different motion directions and plot their projections on the leading two principle components. (a) The result does not exhibit a distinguishable correlation with motion direction. (b) Features are clearly separated by their motion direction.**

### Motion Channel Filter

Principal components can not only reduce dimension but also reflect the importance of each dimension by the projection weights. We visualize the projection weights of \(_{1}^{D 1}\) in Fig 3 (a). It reveals that only a few channels significantly contribute to determining the principal components, indicating these channels encode richer motion information. We term them Motion Channels.

To further explore the relationship between these channels and the motion in videos, we create videos panning in different directions at various frames and visualize the channel with the highest two projection weights in \(_{1}\). As depicted in Fig. 3 (b-c), the value trend is closely associated with the panning direction of the video. Specifically, in Fig. 3 (b), the motion channel value decreases during a rightward pan and increases during a leftward pan. In contrast, a channel with low projection weight does not exhibit much correspondence (Fig. 3 (d)). These observations indicate that we can extract motion-aware features by filtering these motion channels.

### MOFT Extraction

With the above explorations, we introduce a straightforward strategy for extracting motion information from video diffusion models, which we term Motion Feature (MOFT). Our method includes two designs: content correlation removal and motion channel filter. The process can be represented as follows

\[=(_{[j]}-_{i=1}^{F}_{i,[j]}), j,\] (2)

where \(\) is the extracted MOFT, \(i\) operates on the frame dimension, and \(j\) operates on channel dimension. \(\) is the channel index set of motion channels.

We illustrate how content correlation removal and motion channel filter improve the motion correspondence in Fig. 4 (a-d). Vanilla video features demonstrate weak alignment with the reference motion. The proposed content correlation removal significantly improves the alignment. Further application of the motion channel filter enhances focus on the motion area (_e.g._, the rabbit head), yielding higher correspondence.

We conduct an additional ablation study and visualize the impact of selecting different video diffusion features from various blocks within the U-Net of AnimateDiff . Fig. 4 (e-h) intuitively reveals that features with relative medium resolutions achieve better motion correspondence. To this end, we select the features after upper block 1.

Figure 4: **Similarity heatmap between feature of the source point and target features**. Given the red source point in (a), we plot the similarity heatmap on target videos. Yellow indicates regions with higher similarity. We normalize all similarity to 0-1 for better illustration. (b-d) Similarity heatmap of features with different designs. “CR” indicates “content removal”. “MCF” indicates motion channel filter. (e-h) Similarity heatmap of MOFT in different layers in the U-Net. (2x) means relative spatial resolution scale 2. (i-l) Similarity heatmap of MOFT in different video generation models.

While the above analysis is based on AnimateDiff , the property of MOFT holds in different base video models [15; 43; 4; 2] (Fig. 4 (i-l)), demonstrating that MOFT is versatile across different video generation frameworks, consistently achieving reliable motion alignment.

While MOFT is reminiscent of optical flow, which also describes motion, a key limitation of optical flow is that it cannot be directly extracted from video diffusion models during the denoising process and hence cannot serve as the guidance for motion control. In contrast, MOFT is available even at early denoising steps and is naturally suitable for motion control, as we will discuss in the next section.

## 4 MOFT Guidance

With the motion-aware MOFT, we propose a pipeline for video motion control in a training-free manner (Sec. 4.1). Furthermore, our pipeline can be extended for point-drag manipulation (Sec. 4.2).

### Motion Control

We design a pipeline to control motion in the generated videos in a training-free way, as depicted in Fig. 5. Following [36; 49], we optimize latents to alter the sampling process. The loss function \(^{c}\) is

\[^{c}=|}_{(i,j)}||_{i,j}-_{i,j}^{}||,\] (3)

where \(\) is the MOFT we extract during the denoising phase, \(^{r}\) is the reference MOFT feature, and \(\) is the position set of the region that we want to control motion. We provide two possible ways to construct the reference MOFT \(^{r}\): 1) Extract MOFT from reference videos. We perform DDIM inversion  on reference videos and extract MOFT in the inversion stage. 2) Synthesize MOFT based on the statistic regularity. As shown in Fig. 3 (b-c), frame-wise motion channel values exhibit high correspondence with frame-wise motion. We can fit it into a piecewise linear function, where each piece function ranges from statistic minimum to statistic maximum. In this way, we can flexibly modulate frame-wise reference motion as guidance. The detailed process is shown in Alg. 1

``` Input: Noisy latents \(z\) at timestep \(t\), region mask \(\), reference MOFT \(^{r}\), the network \(\), Motion Channel Mask \(\), learning rate \(\) Output: Optimized latents \(\)
1begin
2 Get intermediate feature \(\) from the network \(\);
3 Given \(\), \(\), extract MOFT \(}\) by Eq. 2;
4 Given \(^{r}\), \(\), and \(\), compute the loss \(\) by Eq. 3;
5 Optimize \(\) by updating \( z-\);
6return\(\);
7
8 end for ```

**Algorithm 1**Optimization Process

### Point-Drag Manipulation

Point-drag manipulation is designed to precisely relocate points within image and video frames to reach specific target points. In the image domain, this manipulation method often relies on motion

Figure 5: **Motion Control Pipeline.** We use reference MOFT as guidance and optimize latents to alter the sampling process. In one denoising step, we get the intermediate features and extract MOFT from it with content correlation removal and motion channel filter. We optimize the latents to alter the sampling process with the loss of masked MOFT and reference MOFT.

supervision and point-tracking [33; 36], ensuring the precise tracking of point trajectories to achieve the desired target points. In the video domain, however, we can directly optimize whole point trajectories by setting targets in each frame. The loss function for optimizing point trajectories \(=p_{1},p_{2},...p_{F}\) is:

\[^{p}=_{i=2}^{F}||(p_{i})-((p_{ 1}))||,\] (4)

where \(\) is the diffusion feature (DIFT) and \(\) is the "stop gradient" operation.

However, direct application of this method results in poor video motion control because DIFT struggles with semantic correspondence at early denoising steps, as shown in Fig. 6 Row 1. Since spatial and temporal structures are already determined at early steps, DIFT's effectiveness is limited. Conversely, MOFT provides relatively distinguished motion information in early denoising stage performance (Fig. 6 Row 2), suggesting a strategy of using MOFT for initial coarse motion control and DIFT for precise point-drag manipulation. Please refer to Supplementary Material for details.

## 5 Experiments

### Implementation details

If not specified, the default video generation models of the following experiments are implemented in AnimateDiff  (T2V) and SparseCtrl  (I2V). For T2V generation, we first generate a normal video as the editing source, then apply motion direction and region mask to the video for motion control. To preserve consistency with the source video, we apply (1) region gradient clip, and (2) shared key and value. Details of these techniques and video results can be found in the Supplementary Material. Our results are at a resolution of 512x512 and 16 frames unless otherwise specified. We use DDIM with 25 denoising steps for each sample. It takes approximately 3 minutes to generate one sample on an RTX 3090 GPU.

### Qualitative Results

We showcase qualitative outcomes in Fig. 7. The figure illustrates the successful animation of videos by our method, guided by diverse control signals while preserving a natural and authentic sense of motion. Additionally, we exhibit the results of applying our motion control technique to alternative video generation models, such as ModelScope  and ZeroScope , employing the same control strategy (see Fig. 8). These results highlight the generalizability of MOFT across various video generation models. We also showcase the application of our method on point-drag manipulation (Sec. 4.2) in Fig. 9, where we successfully move the starting points to the targets.

### Motion Feature Design

This subsection experiments on the effectiveness of motion feature designs with two metrics: _a) Motion Fidelity._ Following , we use Motion Fidelity to assess the fidelity of our results in the alignment of synthesis guidance or reference guidance. We use off-the-shelf tracking method  to estimate the tracklets \(=\{_{1},...,_{M}\}\) in the generated videos. For guidance, we manually construct synthesized tracklets for synthesis guidance and use estimated tracklets for reference guidance, we

Figure 6: **Effects of DIFT and MOFT on different denoising time steps. Given the source point in (a) (for DIFT) and (e) (for MOFT), we plot the similarity heat map of DIFT (b-d) and MOFT (f-h) of different denoising steps. Yellow indicates higher similarity. The red point in (b-d) indicates the position with highest similarity. It suggests that MOFT can provide more valid information than DIFT at the early denoising stages.**

denote them both as \(}=\{_{1},...,_{N}\}\) for simplicity. The motion fidelity score is defined as follows:

\[_{}}_{ }(,)+_{ }_{}}( ,).\] (5)

The correlation between two tracklets \((,)\) is computed as:

\[(,)=_{k=1}^{F}^{x} _{k}^{x}+v_{k}^{y}_{k}^{y}}{^ {x})^{2}+(v_{k}^{y})^{2}}_{k}^{x})^{2}+(_{k}^{y})^{2}}},\] (6)

where \((v_{k}^{x},v_{k}^{y})\), \((_{k}^{x},_{k}^{y})\) are the \(k^{th}\) frame displacement of tracklets \(\), \(\), respectively. _b) Image Quality._ We follow [22; 25] that uses an image quality predictor trained on the SPAQ dataset  to evaluate frame-wise quality regarding distortion like noise, blur, or over-exposure. We collect a total of 270 prompt-motion direction pairs for experiments.

Table 1 summarizes our results. The vanilla feature shows poor motion fidelity and image quality due to extraneous information disrupting motion control. Removing content correlation significantly

Figure 7: **Qualitative results. We illustrate several animation clips with different reference or synthesized motion control signals. The red boxes in (a-b) stand for reference videos. _We highly recommend readers refer to the supplementary material for a better visual experience._**

improves both metrics, yielding results comparable to the Space-Motion Map (SMM) feature , likely because SMM also removes content correlation through frame-wise differences. MOFT guidance achieves the highest motion fidelity, with only a minor loss in image quality compared to the original unguided generation.

### Point-drag Manipulation

We conducted additional experiments to assess the efficacy of incorporating motion control in point-based manipulation. In this comparison, we introduce DragNUWA , a potent data-driven method, for reference. We follow [33; 36] to use the _Mean Distance_ between edited points and target points to evaluate the drag precision. Specifically, we still use  to estimate the tracklets \(=\{_{1},...,_{=M}\}\) of given small region. We average these tracklets into \(\) and calculate the mean distance with target tracklet \(^{t}\). We normalize the final distance into , with 0 indicating no mean distance error. We collect a total of 40 image-motion direction pairs for experiments. As indicated in Table 2, applying only DIFT guidance results in poor drag precision. By comparison, incorporating our MOFT yields substantial improvements, effectively narrowing the performance gap with the training-based DragNUWA. The finding is coherent with our analysis in Sec. 4.2.

Figure 8: **Qualitative results on Modelscope  and ZeroScope .**

Figure 10: **Motion quality comparisons.** Gen-2  and Ours accept editing region and motion direction as the control signal. DragNUWA  accepts point trajectories as the control signal.

Figure 9: **Qualitative results of point-drag manipulation.** Red points indicate starting points. Blue points indicate target points of the corresponding frames. We display three frames per video clip.

### User study

We conducted a survey to investigate users' preferences regarding videos generated with motion control. Employing a blind rating protocol, participants were randomly exposed to videos generated by Gen-2 Motion Brush , DragNUWA , and our proposed method. Participants were instructed to rate from 1 to 5 (worst to best) on two metrics: _1) Motion Faithfulness_ to measure how well the motion aligns with the control signal. _2) Motion Naturalness_ to evaluate the naturalness and realism of the motion. We collect human feedback from 26 people on 56 video clips. As depicted in Table 3 and Fig. 10, it is evident that Gen-2 MB excels in achieving highly faithful motion control at the cost of motion naturalness. Gen-2 MB and DragNUWA tend to generate stiff and unrealistic motions. In contrast, our proposed methods demonstrate competitive motion faithfulness while simultaneously preserving the natural and authentic quality of motion.

## 6 Limitations add Future Works

While our approach has yielded appealing results, some limitations require future studies:

1) Presently, our approach lacks support for motion control in real videos. Primarily, this limitation stems from the lack of research on video inversion techniques over video diffusion models. We have observed significant alterations in content when employing initial noise from DDIM inversion  on real videos. Future research focused on video inversion holds promise for resolving this issue.

2) Our current approach does not allow for precise motion scale guidance in motion control. While there are strategies to roughly control motion scales, such as adding up control weights for larger motion scales or implementing gradient clips for smaller ones, achieving high precision in motion scale manipulation requires further investigation.

## 7 Conclusion

In summary, our analysis reveals a robust motion-aware feature in video diffusion models, leading to the development of a training-free MOtion FeaTure (MOFT). MOFT encodes rich, interpretable motion information, is extracted without training, and is applicable across diverse architectures. We introduce a novel training-free video motion control framework based on MOFT, demonstrating competitive performance with natural motion. Importantly, our approach is versatile, easily adaptable to various architectures and checkpoints without the need for independent training.

**Acknowledgements.** This research is supported by MOE AcRF Tier 1 (RG97/23) and is also supported under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).

   Guide type & Methods & Motion Fidelity (\(\)) & Imaging Quality (\(\)) \\  None & Origin & - & **0.697** \\   & SMM feature  & 70.2 & 0.681 \\ Reference & Vanilla Feature & 31.1 & 0.512 \\ Guidance & + CR & 67.1 & 0.671 \\  & + CR \& MCF (Ours) & 82.5 & 0.693 \\  Synthesis &  &  &  \\ Guidance & & & \\   

Table 1: Experiments on Motion Feature Design

   Name & Mean Distance (\(\)) &  & Faithfulness (\(\)) & Naturalness (\(\)) \\  DragNUWA  & **0.075** & DragNUWA  & 2.50 & 2.08 \\ DIFT  & 0.437 & Gen-2 MB  & **3.37** & 2.90 \\ + MOFT (Ours) & 0.175 & Ours & 3.21 & **3.49** \\   

Table 2: Drag Precision

    & Methods & Faithfulness (\(\)) & Naturalness (\(\)) \\  DragNUWA  & **0.075** & DragNUWA  & 2.50 & 2.08 \\ DIFT  & 0.437 & Gen-2 MB  & **3.37** & 2.90 \\ + MOFT (Ours) & 0.175 & Ours & 3.21 & **3.49** \\   

Table 3: User Preference