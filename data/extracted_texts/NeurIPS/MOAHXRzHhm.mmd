# Enhancing Adversarial Robustness via Score-Based Optimization

Boya Zhang

Academy for Advanced Interdisciplinary Studies; Peking University; zhangboya@pku.edu.cn;

Weijian Luo

School of Mathematical Sciences; Peking University; luoweijian@stu.pku.edu.cn;

Zhihua Zhang

School of Mathematical Sciences; Peking University; zhzhang@math.pku.edu.cn;

###### Abstract

Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named _ScoreOpt_, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.

## 1 Introduction

In recent years, there have been breakthrough performance improvements with deep neural networks (DNNs), particularly in the realms of image classification, object detection, and semantic segmentation, as evidenced by the works of . However, DNNs have been shown to be easily deceived to produce incorrect predictions by simply adding human-imperceptible perturbations to inputs. These perturbations are called adversarial attacks , resulting in safety concerns. Therefore, studies of mitigating the impact of adversarial attacks on DNNs, which is referred to as _adversarial defenses_, are significant for AI safety.

Various strategies have been proposed in order to enhance the robustness of DNN classifiers against adversarial attacks. One of the most effective forms of adversarial defense is _adversarial training_, which involves training a classifier with both clean data and adversarial samples. However, adversarial training has its limitations as it necessitates prior knowledge of the specific attack method employed to generate adversarial examples, thus rendering it inadequate in handling previously unseen types of adversarial attacks or corruptions.

On the contrary, _adversarial purification_ is another form of promising defense that leverages a standalone purification model to eliminate adversarial signals before conducting downstream classification tasks. The primary benefit of adversarial purification is that it obviates the need to retrain the classifier, enabling adaptive defense against adversarial attacks at _test time_. Additionally, it showcases significant generalization ability in purifying a wide range of adversarial attacks, without affecting pre-existing natural classifiers. The integration of adversarial purification models into AI systems necessitates minor adjustments, making it a viable approach to enhancing the robustness of DNN-based classifiers.

diffusion models [22; 54; 58], also known as score-based generative models, have demonstrated state-of-the-art performance in various applications, including image and audio generation [9; 30], molecule design , and text-to-image generation . Apart from their impressive generation ability, diffusion models have also exhibited the potential to improve the robustness of neural networks against adversarial attacks. Specifically, they can function as adaptive test-time purification models [40; 16; 4; 63; 66].

Diffusion-based purification methods have shown great success in improving adversarial robustness, but still have their own limitations. For instance, they require careful selection of the appropriate hyper-parameters such as forward diffusion timestep  and guidance scale , which can be challenging to tune in practice. In addition, diffusion-based purification relies on the simulation of the underlying stochastic differential equation (SDE) solver. The reverse purification process requires iteratively denoising samples step by step, leading to heavy computational costs [40; 63].

In the hope of circumventing the aforementioned issues, we introduce a novel adversarial defense scheme that we call _ScoreOpt_. Our key intuition is to derive the posterior distribution of clean samples given a specific adversarial example. Then adversarial samples can be optimized towards the points that maximize the posterior distribution with gradient-based algorithms at test time. The prior knowledge is provided by pre-trained diffusion models. Our defense is independent of base classifiers and applicable across different types of adversarial attacks, making it flexible enough across various domains of applications. The illustration of our method is presented in Figure 1.

Our main contributions can be summarized in three aspects as follows:

* We propose a novel adversarial defense scheme that optimizes adversarial samples to reach the points with the local maximum likelihood of the posterior distribution that is defined by pre-trained score-based priors.
* We explore effective loss functions for the optimization process, introduce a novel score regularizer and propose corresponding practical algorithms.
* We conduct extensive experiments to demonstrate that our method not only achieves start-of-the-art performance on various benchmarks but also improves the inference speed.

## 2 Preliminary

### Score-based Diffusion Models

Score-based diffusion models [22; 58] learn how to transform complex data distributions to relatively simple ones such as the Gaussian distribution, and vice versa. Diffusion models consist of two processes, a forward process that adds Gaussian noise to input \(\) from \(_{0}\) to \(_{T}\), and a reverse generative process that gradually removes random noise from a sample until it is fully denoised. For the continuous-time diffusion models (see Song et al.  for more details and further discussions), we can use two SDEs to describe the above data-transformation processes, respectively. The forward-time SDE is given by:

\[d_{t}=(_{t},t)dt+(t)d_{t}, \ t[0,T];\]

where \(:^{D}^{D}\) and \(:\) are drift and diffusion coefficients respectively, \(\) denotes the standard Wiener process. While new samples are generated by solving the reverse-time SDE:

\[d_{t}=[(_{t},t)-^{2}(t)_{ _{t}} p_{t}(_{t})]dt+(t)d}_{t},t[T,0];\]

Figure 1: Illustration of our proposed adversarial defense framework.

where \(}\) defines the reverse-time Wiener process. The score function term \(_{_{t}} p_{t}(_{t})\) is usually parameterized by a time-dependent neural network \(s_{}(_{t};t)\) and trained by score-matching related techniques [25; 58; 61; 56; 41].

**Diffusion Models as Prior** Recent studies have investigated the incorporation of an additional constraint to condition diffusion models for high-dimensional data sampling. DDPM-PnP  proposed transforming diffusion models into plug-and-play priors, allowing for parameterized samples, and utilizing diffusion models as critics to optimize over image space. Building on the formulation in Graikos et al. , DreamFusion  further introduced a more stable training process. The main idea is to leverage a pre-trained 2D image diffusion model as a prior for optimizing a parameterized 3D representation model. The resulting approach is called score distillation sampling (SDS), which bypasses the computationally expensive backpropagation through the diffusion model itself by simply excluding the score-network Jacobian term from the diffusion model training loss. SDS uses the approximate gradient to train a parametric NeRF generator efficiently. Other works [35; 37; 62] followed similar approaches to extend SDS to the latent space of latent diffusion models .

### Diffusion-based Adversarial Purification

Diffusion models have also gained significant attention in the field of adversarial purification recently. They have been employed not only for empirical defenses against adversarial attacks [69; 40; 63; 66], but also for enhancing certified robustness [5; 67]. The unified procedure for applying diffusion models in adversarial purification involves two processes. The forward process adds random Gaussian noise to the adversarial example within a small diffusion timestep \(t^{*}\), while the reverse process recovers clean images from the diffused samples by solving the reverse-time stochastic differential equation. Implementing the aforementioned forward-and-denoise procedure, imperceptible adversarial signals can be effectively eliminated. Under certain conditions, the purified sample restores the original clean sample with a high probability in theory [40; 67].

However, diffusion-based purification methods suffer from two main drawbacks. Firstly, their robustness performance heavily relies on the choice of the forward diffusion timestep, denoted as \(t^{*}\). Selecting an appropriate \(t^{*}\) is crucial because excessive noise can lead to the removal of semantic information from the original example, while insufficient noise may fail to eliminate the adversarial perturbation effectively. Secondly, the reverse process of these methods involves sequentially applying the denoising operation from timestep \(t\) to the previous timestep \(t-1\), which requires multiple deep network evaluations. In contrast to previous diffusion-based purification methods, our optimization framework departs from the sequential step-by-step denoising procedure.

## 3 Methodology

In this section, we present our proposed adversarial defense scheme in detail. Our method is motivated by solving an optimization problem of adversarial samples to remove the applied attacks. Therefore, we start by formally formulating the optimization objective, followed by exploring effective loss functions and introducing two practical algorithms.

### Problem Formulation

Our main idea is to formulate the adversarial defense as an optimization problem given the perturbed sample and the pre-trained prior, in which the solution to the optimization problem is the recovered original sample that we want. We regard the adversarial example \(_{a}\) as a disturbed measurement of the original clean example \(\), and we assume that the clean example is generated by a prior probability distribution \( p()\). The posterior distribution of the original sample given the adversarial example is \(p(|_{a}) p()\,p(_{a}|)\). The maximum a posteriori estimator that maximizes the above conditional distribution is given by:

\[}^{*}=*{arg\,min}_{}- p( _{a}).\] (1)

In this work, we use the data distribution under pretrained diffusion models as the prior \(p_{}()\).

### Loss Functions for Optimization Process

Following Graikos et al. , we introduce a variational posterior \(q()\) to approximate the true posterior distribution \(p(|_{a})\) in the original optimization objective. The variational upper bound on the negative log-likelihood \(- p(_{a})\) is:

\[- p(_{a})_{q()}[- p (_{a}|)]+(q() \|p_{}()).\] (2)

As shown in Song et al.  and Vahdat et al. , we can further obtain an upper bound on the second Kullback-Leibler (KL) divergence term between the target variational posterior distribution \(q()\) and the prior distribution defined by the pre-trained diffusion models \(p_{}()\):

\[(q()\|p_{}( ))_{q()}_{t (0,1),(,)}[w(t) \|s_{}(_{t};t)-_{_{t}} q(_{t}|)\|_{2}^{2}],\] (3)

where \(w(t)=g(t)^{2}/2\) is a time-dependent weighting coefficient, \(_{t}=+_{t}\) denotes the forward diffusion process, \(_{t}\) is the pre-designed noise schedule, and \(s_{}\) represents the pre-trained diffusion models.

The simplest approximation to the posterior is using a point estimate, i.e., the introduced variational posterior \(q()\) satisfies the Dirac delta distribution \(q()=(-_{})\). Thus, the above upper bound can be rewritten as:

\[_{t(0,1),(, )}[w(t)\|s_{}(_{t};t )-_{_{t}} q(_{t}|_{})\| _{2}^{2}],\] (4)

We simply use notation \(\) instead of \(_{}\) throughout for convenience. The weighted denoising score matching objective in (4) is also equivalent to the diffusion model training loss .

According to Tweedie's formula: \(_{z}=z+_{z}_{z} p(z)\), where \(\) denotes covariance matrix, we can obtain \(=_{t}+_{t}^{2}_{_{t}} q( _{t})\). Defining \(D_{}(_{t};t):=_{t}+_ {t}^{2}s_{}(_{t};t)\), the KL term of our opimization objective converts to:

\[_{t(0,1),(, )}[(t)\|D_{}(+ _{t};t)-\|_{2}^{2}],\] (5)

where \((t)=w(t)/_{t}^{2}\). Note that \(D_{}\) can be used to estimate the denoised image directly, which is called _one-shot_ denoiser .

In our work, we adopt the approach of setting \((t)=1\) for convenience and performance as in previous studies . Since we have no information about the conditional distribution \(p(_{a}|)\), we need a heuristic formulation for the first reconstruction term in (2). The simplest method is to initialize \(\) by the adversarial sample \(_{a}\) and use the loss in (5) to optimize over \(\) directly, eliminating the constraint term. The rationale behind this simplification is that leading \(_{a}\) towards the mode of the \(p_{}()\) with the same ground-truth class label makes it easier for the natural classifier to produce a correct prediction. In this way, the loss function of our optimization process reduces to:

\[_{}(,)=_{t (0,1),(,)}[\| D_{}(+_{t};t)- \|_{2}^{2}].\] (6)

Randomized smoothing techniques typically assume that the adversarial perturbation follows a Gaussian distribution. Suppose the Gaussian assumption holds, we can instantiate the reconstruction term with the mean squared error (MSE) between \(\) and \(_{a}\). The optimization objective converts to the following MSE loss:

\[_{}(,_{a},)=_{t(0,1),(, )}[\|D_{}(+_{t} ;t)-\|_{2}^{2}+\|-_{a}\|_{2}^{2}],\] (7)

where \(\) is a weighting hyper-parameter to balance the two loss terms.

#### 3.2.1 Score Regularization Loss

However, both Diff and MSE optimizations have their own drawbacks. Regarding the Diff loss, the optimization process solely focuses on the score prior \(p_{}\), and the update direction guided by the pre-trained diffusion models leads the samples towards the modes of the prior, gradually losing the semantic information of the original samples. As illustrated in Figure 1(a), with a sufficient number of optimization steps, both standard and robust accuracies decline significantly. On the other hand,the MSE loss maintains a high standard accuracy at the cost of a large drop in robust accuracy, as depicted in Figure 1(b). Furthermore, Figure 1(c) demonstrates that the performance of MSE is highly dependent on the weighting hyper-parameter, which controls the intensity of the constraint term.

To address the above-mentioned issues, we propose to introduce a hyperparameter-free score regularization (SR) loss:

\[_{}(,_{a},)= _{t(0,1),_{1},_{2} (,)}\] (8) \[[\|D_{}(+_{t} _{1};t)-\|_{2}^{2}+\|D_{ }(+_{t}_{1};t)-D_{} (_{a}+_{t}_{2};t)\|_{2}^{2}].\]

We use the introduced constraint to minimize the pixel-level distance between the denoised versions of the current sample \(\) and initial adversarial sample \(_{a}\). The additional regularization term can be expanded as:

\[\|D_{}(+_{t}_{1};t )-D_{}(_{a}+_{t}_{2};t )\|_{2}^{2}\|-_{a}+_{t}^{2 }(s_{}(_{t};t)-s_{}(_{a,t};t))\|_{2}^{2}.\] (9)

The SR loss encourages consistency with the original sample in terms of not only the pixel values but also the score function estimations at the given noise level \(_{t}\). Since the two parts of SR loss correspond to the same noise magnitude \(t\) of score networks, there is no need to introduce an additional hyperparameter \(\) as in (7).

Figure 1(b) and 1(c) demonstrate the effectiveness of the SR loss. As the number of optimization steps increases, both the standard and robust accuracy converge to stable values, with the latter remaining close to the optimal. In contrast to the MSE loss, the SR loss shows insensitivity to the weighting hyperparameter and significantly outperforms MSE, particularly for larger values of \(\).

### Practical Algorithms

**Noise Schedule** The perturbation introduced by adversarial attacks is often subtle and challenging for human perception. As a result, our optimization loop does not necessarily incorporate the complete

Figure 2: Robustness performance comparison for Diff, MSE, and SR optimizations.

[MISSING_PAGE_FAIL:6]

loss function and projects into some \(l_{p}\)-ball: \(_{}(+( _{x}(f(x+),y)))\), where \(f\) represents a classifier.

**Datasets and Baselines** Three datasets are considered in our experiments: CIFAR10, CIFAR100, and ImageNet. Following previous works, the evaluation is conducted on the test set of each dataset. We adopt two kinds of adversarial defenses for comparison: adversarial training methods and adversarial purification methods, especially those also based on diffusion models [69; 40; 63]. In accordance with the settings in Nie et al. , we conduct evaluations against strong adaptive attacks using a fixed subset of 512 randomly sampled images.

**Evaluation Metrics** We leverage two evaluation metrics to assess the performance of our proposed defense method: _standard_ accuracy and _robust_ accuracy. Standard accuracy measures the performance of adversarial defenses on clean data, while robust accuracy measures the classification performance on adversarial examples generated by various attacks.

**Models** We consider different architectures for a fair comparison with previous works. We utilize the ResNet  and WideResNet (WRN)  backbones as our base classifiers. For CIFAR10 and CIFAR100 datasets, we employ WRN-28-10 and WRN-70-16, the two most common architectures on adversarial benchmarks. For the ImageNet dataset, we select WRN-50-2, ResNet-50, and ResNet-152, which are extensively used in adversarial defenses. As for the pre-trained diffusion models, we incorporate two popular architectures, the elucidating diffusion model (EDM)  and the guided diffusion model . The diffusion models and classifiers are trained independently on the same training dataset as in prior studies.

### Main Results

In this section, we validate our defense method against two strong adaptive attacks: BPDA+EOT and adaptive white-box attack, i.e., PGD+EOT. The evaluation results of transfer-based attacks are deferred to Appendix B.

#### 4.2.1 BPDA+EOT Attack

The combination of Backward Pass Differentiable Approximation (BPDA)  with Expectation over Transformation (EOT)  is commonly used for evaluating randomized adversarial purification methods. In order to compare with other test-time purification models, we follow the same settings in Hill et al.  with default hyperparameters for evaluation against BPDA+EOT attack. We conduct experiments under \(_{}(=8/255)\) threat model on CIFAR10 and CIFAR100 datasets. Our method achieves high robust accuracies, outperforming existing diffusion-based purification methods by a significant margin, while maintaining high standard accuracies.

_CIFAR10_ Table 1 shows the robustness performance against BPDA+EOT attack under the \(_{}(=8/255)\) threat models on CIFAR10 dataset. Mark \(o\) in parentheses denotes the practi

   Type & Architecture & Method & Standard (\%) & Robust (\%) & Avg. (\%) \\  Base Classifier & WRN-28-10 & - & 95.32 & 0.0 & - \\   &  & Madry et al.  & 87.30 & 45.80 & 66.55 \\  & & Zhang et al.  & 84.90 & 45.80 & 65.35 \\  & & Carmon et al.  & 89.67 & 63.10 & 76.39 \\  & & Gowal et al.  & 89.48 & 64.08 & 77.28 \\   &  & Yang et al.  & 94.80 & 40.80 & 67.80 \\  & & Song et al.  & 95.00 & 9.00 & 52.00 \\   & & Hill et al.  & 84.12 & 54.90 & 69.51 \\   & & Yoon et al.  & 86.14 & 70.01 & 78.08 \\   & & Wang et al.  & 93.50 & 79.83 & 86.67 \\   & & Nie et al.  & 89.02 & 81.40 & 85.21 \\   & & Ours(\(o\)) & 90.78\(\)0.40 & **82.85\(\)0.26** & **86.82** \\   & & Ours(\(n\)) & 93.44\(\)0.40 & **90.59\(\)0.08** & **92.02** \\   

Table 1: Standard and robust accuracy against BPDA+EOT attack under \(_{}(=8/255)\) threat model on CIFAR10, compared with other preprocessor-based adversarial defenses and adversarial training methods against white-box attacks.

cal method _ScoreOpt-O_ in Algorithm 1, and mark \(n\) denotes the method _ScoreOpt-N_ in Algorithm 2. Our methods achieve surprisingly good results. Specifically, we obtain 90.59% robust accuracy, with absolute improvements of 9.19% over previous SOTA adversarial purification methods. Our method is the first adaptive test-time defense to achieve robust accuracy over 90% against the BPDA+EOT attack. Meanwhile, the standard accuracy result is on par with the best-performing method.

_CIFAR100_ We also conduct robustness evaluations against strong adaptive attacks under the \(_{}(=8/255)\) threat model on the CIFAR100 dataset. The results of ScoreOpt and other adaptive purification methods are presented in Table 2. To showcase the superiority of our method over previous diffusion-based purification algorithms, we perform experiments using the approach proposed in Nie et al.  with varying reverse denoising steps. The optimal hyperparameter \(t^{*}\), representing the forward timestep, is carefully tuned based on experimental results. As indicated in Table 2, increasing the reverse denoising steps only leads to marginal improvements over the one-shot denoising method. Even when the number of denoising steps is increased to 80, which will incur a large computational cost, no further improvement in robust accuracy is observed. In contrast, our ScoreOpt method achieves a notable improvement of 18.16% in robust accuracy.

#### 4.2.2 Adaptive White-box Attack

In order to evaluate our method against white-box attacks, it is necessary to compute the exact gradient of the entire defense framework. However, our optimization process involves backpropagation through the U-net architecture of diffusion models, making it challenging to directly apply PGD attack. Taking inspiration from Lee and Kim , we approximate the full gradient using the one-shot denoising process in our experiments. The performance against PGD+EOT is shown in Table 3. Most results for baselines are taken from Lee and Kim . Notably, our method significantly outperforms other diffusion-based purification methods. Specifically, compared to Nie et al. , our method improves robust accuracy by 16.43% under \(_{}\) and by 4.67% under \(_{2}\) on WideResNet-28-10, and by 14.45% under \(_{}\) and by 2.31% under \(_{2}\) on WideResNet-70-16, respectively. Compared with previous SOTA adversarial training methods, ScoreOpt achieves better robust accuracies on both WideResNet-28-10 and WideResNet-70-16 under \(_{2}\). Furthermore, under \(_{}\) threat model, ScoreOpt outperforms AT baselines on WideResNet-28-10 and achieves comparable robust accuracy on WideResNet-70-16 with the top-rank model.

    & Hill et al.  & Yoon et al.  &  &  \\  & & 1 & 10 & 20 & 40 & 80 & (\(o\)) & (\(n\)) \\  Standard & 51.66 & 60.66 & 61.52 & 67.77 & 69.92 & 67.58 & 66.99 & **69.92** & **75.98** \\ Robust & 26.10 & 39.72 & 40.03 & 47.66 & 48.83 & 48.05 & 47.85 & **66.99** & **65.43** \\   

Table 2: Standard accuracy and robust accuracy (%) against BPDA+EOT attack under \(_{}(=8/255)\) threat model on the CIFAR100 dataset.

   Type & Method & Standard & Robust \\   \\   & Wang et al.  & 88.62 & 64.95 \\  & Gowal et al.  & 88.54 & 65.10 \\  & Gowal et al.  & 87.51 & 66.01 \\   & Yoon et al.  & 85.66 & 37.27 \\  & Nie et al.  & 90.07 & 51.25 \\  & Ours & **95.02\(\)0.30** & **67.68\(\)0.29** \\   \\   & Gowal et al.  & 88.75 & 69.03 \\  & Wang et al.  & 92.97 & **72.46** \\   & Yoon et al.  & 86.76 & 41.02 \\  & Nie et al.  & 90.43 & 57.03 \\   & Ours & **95.18\(\)0.09** & **71.48\(\)0.20** \\    
   Type & Method & Standard & Robust \\   \\   & Schwag et al.  & 90.93 & 83.75 \\  & Rebuffi et al.  & 91.79 & 85.05 \\  & Augustin et al.  & 93.96 & 86.14 \\   & Yoon et al.  & 85.66 & 74.26 \\  & Nie et al.  & 91.41 & 82.11 \\   & Ours & **94.99\(\)0.09** & **86.78\(\)0.33** \\   \\   & Rebuffi et al.  & 92.41 & 86.24 \\  & Wang et al.  & **96.09** & 86.72 \\   & Yoon et al.  & 86.76 & 75.90 \\  & Nie et al.  & 92.15 & 84.80 \\   & Ours & **95.18\(\)0.18** & **87.11\(\)0.01** \\   

Table 3: Standard and robust accuracy against PGD+EOT on CIFAR-10. Left: \(_{}(=8/255)\); Right: \(_{2}(=0.5)\). Compared with adversarial training (AT) and purification (AP) methods.

[MISSING_PAGE_FAIL:9]

#### 4.4.2 Optimization Steps

Since our defense is an iterative optimization process, we conduct ablation experiments on our ScoreOpt algorithm with different optimization steps under \(_{}(=8/255)\) threat model against BPDA+EOT on CIFAR10. Figure 3 shows that the standard accuracy and robust accuracy continuously increase as the number of optimization steps increases. As the number of iterations increases further, the accuracies gradually stabilize. This phenomenon shows the effectiveness of our optimization process.

#### 4.4.3 Inference Speed

We compare the inference speed of ScoreOpt with diffusion purification methods that use a sequential multiple-step denoising process. We compute the inference time cost per image. As shown in Table 6, our time cost is about twice under the same steps. However, ScoreOpt needs only a few steps (about 5) to obtain significantly better results than the multi-step denoising method, which requires nearly 100 denoising steps. Therefore, compared to diffusion-based AP baselines, our method further improves the inference speed substantially in practice.

## 5 Concluding Remarks

In this work, we have proposed a novel adversarial defense framework ScoreOpt, which optimizes over the input image space to recover original images. We have introduced three optimization objectives using pre-trained score-based priors, followed by practical algorithms. We have shown that ScoreOpt can quickly purify attacked images within a few optimization steps. Experimentally, we have shown that our approach yields significant improvements over prior works in terms of both robustness performance and inference speed. We would believe our work further demonstrates the powerful capabilities of pre-trained generative models for downstream tasks.