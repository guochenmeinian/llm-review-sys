# Symmetries in Overparametrized Neural Networks:

A Mean-Field View

Javier Maass Martinez

Center for Mathematical Modeling

University of Chile

javier.maass@gmail.com &Joaquin Fontbona

Center for Mathematical Modeling

University of Chile

fontbona@dim.uchile.cl

###### Abstract

We develop a Mean-Field (MF) view of the learning dynamics of overparametrized Artificial Neural Networks (NN) under distributional symmetries of the data w.r.t. the action of a general compact group \(G\). We consider for this a class of generalized shallow NNs given by an ensemble of \(N\) multi-layer units, jointly trained using stochastic gradient descent (SGD) and possibly symmetry-leveraging (SL) techniques, such as Data Augmentation (DA), Feature Averaging (FA) or Equivariant Architectures (EA). We introduce the notions of weakly and strongly invariant laws (WI and SI) on the parameter space of each single unit, corresponding, respectively, to \(G\)-invariant distributions, and to distributions supported on parameters fixed by the group action (which encode EA). This allows us to define symmetric models compatible with taking \(N\) and give an interpretation of the asymptotic dynamics of DA, FA and EA in terms of Wasserstein Gradient Flows describing their MF limits. When activations respect the group action, we show that, for symmetric data, DA, FA and freely-trained models obey the exact same MF dynamic, which stays in the space of WI parameter laws and attains therein the population risk's minimizer. We also provide a counterexample to the general attainability of such an optimum over SI laws. Despite this, and quite remarkably, we show that the space of SI laws is also preserved by these MF distributional dynamics even when freely trained. This sharply contrasts the finite-\(N\) setting, in which EAs are generally not preserved by unconstrained SGD. We illustrate the validity of our findings as \(N\) gets larger, in a teacher-student experimental setting, training a student NN to learn from a WI, SI or arbitrary teacher model through various SL schemes. We lastly deduce a data-driven heuristic to discover the largest subspace of parameters supporting SI distributions for a problem, that could be used for designing EA with minimal generalization error.

## 1 Introduction

Learning in complex tasks, employing ever larger datasets, has strongly benefited from the implementation and training of Artificial Neural Networks (NN) with a huge number of parameters; as well as from training schemes or architectures that can leverage underlying symmetries of the data in order to reduce the problem's complexity (see  for general reference). This raises questions, on one hand, of understanding the puzzling generalizability in overparametrized NN; and on the other, of when and how symmetry-leveraging (SL) techniques (such as Data Augmentation, Feature Averaging or Equivariant Architectures), can induce useful biases towards learning with symmetries, without hindering approximation and generalization properties. The recent Mean-Field (**MF**) theory of NN (see  and further references below) provides a partial, yet promissory, viewpoint to address the first question for shallow NN: in the Mean-Field _Limit_ (**MFL**) of an infinitely wide hidden layer, stochastic gradient descent (SGD) training dynamics approximates the Wasserstein Gradient Flow(**WGF**) of certain _convex population risk_ on the space of distributions on parameters. Confluently, the incorporation of combined algebraic and probabilistic viewpoints have yielded a more complete view of the benefits of SL techniques under symmetry (see e.g. [13; 27; 59] and further references below); however, it is not clear if and how those findings can scale to overparametrized NN and their **MFL**.

In this work we develop a systematic **MF** analysis of the limiting learning dynamics of a class of generalized shallow NNs, under distributional symmetries of the data w.r.t. the action of a compact group, and including the possible effects of employing some of the most popular SL techniques. The effect of symmetries on the **WGF** dynamics was already studied in , in the particular case of two-layer ReLU networks, under data generated by a function symmetric w.r.t. a single orthogonal transformation. We consider our (independent 1) work to largely broaden the scope and applicability of such initial contributions, as it provides a unified **MF** interpretation for both the use of SL techniques under general distributional invariances, and the interplay of such symmetries at the levels of data, architectures and training dynamics. The paper unfolds as follows:

In Section 2 we introduce a class of generalized shallow models with multi-layer units on which we will focus, we recall **WGFs** and their role in the **MFL** of NN training dynamics, and review the SL techniques to be studied. Section 3 contains the bulk of our contributions, as we study how SL techniques applied on these models can be interpreted in terms of their limiting **WGFs**, how they relate to each other in terms of the optima of their corresponding population risks, and how their limiting **MF** training dynamics behave with or without symmetric data. Finally, Section 4 presents the empirical _validation_ of our main theoretical results through some numerical simulations; it also suggests a potential heuristic for discovering data-driven _parameter-sharing_ schemes that lead to _optimal_ equivariant architectures in ML problems. Proofs and complements to our results can be found in the Supplementary Material (henceforth SuppMat for short), together with a discussion of the scope and limitations of our results, as well as a summary of the notation and abbreviations used.

## 2 Preliminaries

### Supervised learning with generalized shallow neural networks

Let \(\), \(\) and \(\) be separable Hilbert Spaces, termed as the _feature_, _label_ and _parameter_ spaces respectively. Typically, these are finite-dimensional, e.g. \(=^{d}\) and \(=^{c}\) (for \(c,d^{*}\)) with \(\) the space of affine transformations between hidden layers. We write \(()\) for the space of Borel probability measure on a metric space \(()\). Let \(()\) denote the data distribution from which i.i.d. samples \((X,Y)\) will be drawn, and \(:\) be a **convex** loss function. Consider also an _activation function_\(_{*}:\). We introduce a general class of shallow NN:

**Definition 1**.: _A shallow neural network model of parameter \(:=(_{i})_{i=1}^{N}^{N}\) is the function \(_{}^{N}:\) given by \(_{}^{N}(x):=_{i=1}^{N}_{*}(x;_{i}),\  x \). Equivalently, if \(_{}^{N}:=_{i=1}^{N}_{_{i}}\) is the empirical measure associated with \(^{N}\), we can write \( x,\ _{}^{N}(x)=_{*}(x;),_{ }^{N}\) or, abusing notation, simply \(_{}^{N}=_{*},_{}^{N}\)._

In the setting where \(=^{d}\), \(=^{c}\) and \(=^{c b}^{d b} ^{b}\) (for \(b^{*}\)), if we consider, for \(z=(W,A,B)\) and \(:^{b}^{b}\), \(_{*}(x,z):=W(A^{T}x+B)\); then \(_{}^{N}\) (with \(N,^{N}\)) corresponds exactly to a single-hidden-layer neural network with \(N\) hidden units. Depending on \(_{*}\), however, these shallow NN models can represent settings that go far beyond this first example. In fact, \(_{*}\) can be taken to be an entire Multi-Layer NN model, in which case \(_{}^{N}\) will represent an **ensemble** of \(N\) such units trained simultaneously (see SuppMat-C.1). As we will also shortly see, for suitable subspaces of \(\), this modelling extends to renowned equivariant architectures such as CNNs, DeepSets and GNNs. Beyond NNs, this setting can also model the deconvolution of sparse spikes, RBF networks, density estimation via MMD minimization, among many others (see [16; 62; 69]).

This class thus allows for non-trivial internal units, while enabling the width \(N\) consistently, and regardless of the possible underlying structure of the (fixed size) units represented by \(_{*}\). Inspired by this possibility, and by our writing of _shallow NN models_, we define a more general notion:

**Definition 2** (**Shallow Model)**.: _A shallow model is any function of the form \(_{}(x):=_{*}(x;),\) for some \(()\) (whenever the integral makes sense for all \(x\)). We write \(_{}:=_{*},\) and denote the space of such models as \(_{_{*}}(())\)._Classically, we want to find a NN model that performs well with respect to \(\) and \(\). More precisely, having fixed an _architecture_ (given here by \(N\) and \(_{*}\)), we consider the _generalization error_ or _population risk_ given by \(R()=_{}[(_{}^{N}(X),Y)]\), and look for a vector of parameters \(^{N}\) attaining \(_{^{N}}R()\). However, not only is this function highly non-convex and hard to optimize; but in practice we generally don't have access to \(\) (and thus \(R\)) and we have to solve this problem only with a set of i.i.d. data samples \(\{(X_{k},Y_{k})\}_{k}\) drawn from \(\). Thus, the usual approach to minimizing this population risk is to _train_ a _NN model_\(_{}^{N}\), through an SGD scheme (see e.g. ):

* First, initialize \(_{i}^{0}\), \( i\{1,,N\}\), i.i.d. from a fixed distribution \(_{0}()\).
* Iterate, for \(k\), defining \( i\{1,,N\}\): \[_{i}^{k+1}=_{i}^{k}-s_{k}^{N}\,(_{z}_{*}(X_{k}, _{i}^{k})_{1}(_{^{k}}^{N}(X_{k}),Y_{k})+  r(_{i}^{k}))+^{N}}_{i}^{k}.\] (1)

Here, \(s_{k}^{N}=_{N}(k_{N})\) is the _step-size_ (or _learning rate_), parametrized in terms of \(:_{+}_{+}\) a regular function and \(_{N}>0\). Also, we have a penalization function \(r:\), regularizing Gaussian noise \(_{i}^{k\,\,\,i.d.}(0,_{})\) independent from the initialization and data, and \(, 0\). When \(,>0\), the method is called stochastic gradient Langevin dynamics, noted SGLD (), or simply _noisy_ SGD. An _infinite i.i.d._ sample from \(\) will be needed when letting later \(N\). When \(\) is the empirical measure of a finite dataset, we are performing empirical-risk minimization (which of course is not the same as minimizing generalization error, but follows the same mathematical formulation).

In principle, there are no guarantees that this training procedure will be truly optimizing \(R()\) let alone approaching its minimum. However, by extending the definition of the **generalization error** to models in \(_{_{*}}(())\), one gets the convex functional \(R:()\) given by \(R():=_{}[(_{}(X),Y)]\). The problem on \(^{N}\) is thus lifted to the **convex** optimization problem on \(()\):

\[_{()}R().\] (2)

Accordingly, this motivates looking at the evolution of empirical measures \((_{k}^{N})_{k}:=(_{^{k}}^{N})_{k} ()\) instead of that of the specific parameters \((^{k})_{k}^{N}\). The **MF** approach to NNs (see ) aims at providing theoretical guarantees for problem (2), justifying that a _global optimum_ of the population risk can be approximated by training a NN with SGD for large \(N\). We next provide some necessary background on **WGFs** and on the **MF** theory of shallow NN models.

### Wasserstein gradient flow and mean-field limit of shallow models

We briefly recall some elements of Optimal Transport and Wasserstein Gradient Flows, referring to  for further background. Let \(\) be a Hilbert space with norm \(\|\|\) and, for \(p[1,)\), let \(_{p}():=\{():\,_{ }\|\|^{p}(d)<+\}\) be the space of probability measures on \(\) with finite \(p\)-th moment. We endow this space with the \(p\)-th _Wasserstein distance_, defined as: \(W_{p}(,):=[_{(,)}_{}[\|X-Y\| ^{p}]]^{}\), \(,_{p}()\) with \((,)\) being the set of _couplings between \(\) and \(\)_ (the infimum is always attained). The metric space \((_{p}(),W_{p})\) is Polish and called the \(p\)-th Wasserstein Space. In the remainder of this section we consider \(p=2\) and \(=^{D}\).

We recall central objects for the sequel, including Lions' derivative , popularized in mean-field games (see e.g. ) and shown (in ) to coincide with the Wasserstein gradient ():

**Definition 3** (Linear Functional Derivative and Intrinsic Derivative).: _Given \(F:_{2}()\), its linear functional derivative is the function (if it exists) \(:Dom(F)\) such that \(, Dom(F)\), \(_{h 0}=_{}(,z)d(-)(z)\) and \(_{}(,z)d(z)=0\). The function \(F^{}:_{2}()(,)\) is known as the first variation of \(F\) at \(\). Moreover, if \(\) exists and is differentiable in its second argument, we define the intrinsic derivative of \(F\) at \(\) to be: \(D_{}F(,z)=_{z}((,z))\). Abusing notation, we will write \(:_{2}() \) and \(D_{}F:_{2}()\), even if they are only partially defined._

This allows us to define next a Wasserstein Gradient Flow (following e.g. ):

**Definition 4** (Wasserstein Gradient Flow).: _Let \(:_{+}_{+}\) be a regular scalar function and \(F:_{2}()}\) be a **convex** functional for which the intrinsic derivative \(D_{}F\) is defined. We define a **Wasserstein Gradient Flow (WGF) for \(F\)** (shortened **WGF**(\(F\))) as any absolutely continuous trajectory \((_{t})_{t 0}\) in \(_{2}()\) that satisfies, distributionally on \([0,)\) :_

\[_{t}_{t}=(t)(D_{}F(_{t}, )_{t}).\] (3)

Several authors (, among others) have proven under various sets of assumptions that, given an initial condition \(_{0}_{2}()\), the **WGF**(\(F\)) admits a unique (weak) solution, \((_{t})_{t 0}\). In a sense, **WGF**(\(F\)) _'follows the negative gradient'_ of \(F\). Unfortunately, even for convex \(F\), stationary points of **WGF**(\(F\)) need not be global minima, see .

We are interested in the case where \(F\) is the following convex, **entropy-regularized population risk**: \(R^{,}():=R()+ rd+ H_{}()\), where \(, 0\), \(\) is the Lebesgue Measure on \(\), \(r:_{+}\) is a _penalization_, and \(H_{}\) defined as \(H_{}():=((z))d(z)\) if \(\) or \(+\) otherwise, is the _Boltzmann entropy_ of \(\). In this case, **WGF**(\(R^{,}\)) reads as the PDE:

\[_{t}_{t}=(t)[((D_{}R(_ {t},)+_{}r)\,_{t})+_{t}],\] (4)

known as McKean-Vlasov equation in the probability and PDE communities (see the classic references , and the recent review ) and popularized as 'distributional dynamics' in NN literature (e.g. ). When \(>0\), a solution to (4) has a density w.r.t. \(\) and is actually strong. Under rather simple technical assumptions (see SuppMat-D.3, or ), when \(,>0\) it is known that the **WGF**(\(R^{,}\)) \(W_{2}\)-converges to a (unique) minimizer. When \(,=0\) a sort of converse holds (see ): if **WGF**(\(R\)) converges in \(W_{2}\), then the limit minimizes \(R\).

Proven by  and later refined e.g. by , the main result in the **MF** Theory of overparametrized shallow NNs states that SGD _training_ for a shallow NN, in the right _scaling_ limit as \(N\), approximates a **WGF** :

**Theorem 1** (Mean-Field limit, sketch).: _For each \(T>0\), under relevant technical assumptions including regularity of \(_{*}\) and a proper asymptotic behaviour of \(_{N} 0\) as \(N\), the rescaled empirical process given by \(^{N}:=(^{N}_{\{t/_{N}\}})_{t[0,T]}\) converges in law (in the Skorokhod space \(D_{()}([0,T])\)) to \(:=(_{t})_{t[0,T]}\) given by the **unique WGF**(\(R^{,}\)) starting at \(_{0}\)._

Despite the **MF** limit of NNs being a theoretical approximation, the behavior it predicts can effectively be observed in practice, even for finite, not too large \(N\) (see the numerical experiments in many of the aforementioned works and below). Moreover, it is the asymptotic regime that most closely describes the actual feature-learning behavior observed in large, overparametrized NNs during training (as compared e.g. to the lazy-training regime described by the Neural Tangent Kernel approximation ). Note that, for \(>0\), the entropy term \(H_{}\) in **WGF**(\(R^{,}\)) (as well as the Laplace operator in equation (4)) is approximated, in practice, by the Gaussian noise term in the SGLD (1), as \(N\).

### Symmetry-leveraging techniques

We next discuss mathematical formulations of the main techniques to leverage posid distributional symmetries of the data at the training or architecture levels. We henceforth fix a _compact group_\(G\) of normalized Haar measure \(_{G}\), acting on \(\) and \(\), which we denote \(G_{}\), \(G_{}\). 2 A function \(f:\) is termed _equivariant_ if \( g G\), \(_{g^{-1}}.f(_{g}.x)=f(x)\)\(\ \ \ d_{}(x)\)-a.s. We further say that the data \((X,Y)\) is _equivariant_, and write \(^{G}()\), if \( g G\), \((_{g}.X,_{g}.Y)\) (this is not enforced unless stated). The space of functions \(f:\) square-integrable (in Bochner sense) w.r.t \(_{}=Law(X)\) is called \(L^{2}(,;_{})\). Further relevant concepts are introduced as needed.

**Data Augmentation (DA):** This training scheme considers \(\{g_{k}\}_{k}}{{}}_{G}\) independent from the \(\{(X_{k},Y_{k})\}_{k}\) in (1), and carries out SGD on samples \(\{(_{g_{k}}.X_{k},_{g_{k}}.Y_{k})\}_{k}\). **DA** and the _vanilla_ training scheme would thus be equivalent if \(^{G}()\). One can show (see ) that, performing SGD with **DA**, results in an optimization scheme for the _symmetrized population risk_, \(R^{DA}():=_{}[_{G}(_{}^{N}(_{g}. X),_{g}.Y)d_{G}(g)]\). Despite being effective in practice, **DA** gives no guarantee that the resulting model will be equivariant. For deeper insights, see .

**Feature Averaging (FA):** Instead of focusing on the data, **FA** works with _symmetrized_ versions of the _vanilla_ NN models \(_{}^{N}\) at hand, averaging model copies over all possible translations through the group action. This amounts to constructing (or approximating) the _symmetrization operator_ over \(L^{2}(,;_{})\) defined as \((_{G}.f)(x):=_{G}_{g^{-1}}f(_{g}.x)d_{G}(g)\) (see ), and trying to minimize \(R^{FA}():=_{}[((_{G}._{} ^{N})(X),Y)]\) (see [13; 21; 46; 48]). The resulting model will be equivariant, however, **FA** is inefficient, as \(|G|\) times more evaluations are needed for training and inference.

**Equivariant Architectures (EA):** Following , **EA** in multilayer NNs are configurations yielding models equivariant between each of the hidden layers (where \(G\) is assumed to act). As stated in [29; 30; 61; 64; 65; 75] and [2; 18; 44; 45; 73], once the (equivariant) activation functions between the different layers have been fixed, **E**As are plainly parameter-sharing schemes (determined by the space of _intertwiners/group convolutions_ between layers). In our context, assuming that \(G_{}\) is some group action, we require that \(_{*}:\) is _jointly equivariant_, namely, \((g,x,z) G,\ _{*}(_{g}.x,M_{g}.z)= _{g}_{*}(x,z)\); to ensure \(G\)-actions over different spaces are properly related. Introducing the set of fixed points for \(G_{}\), \(^{G}:=\{z\ :\  g G,\ M_{g}.z=z\}\), a shallow NN model \(_{}^{N}\) thus has an **EA** if \((^{G})^{N}\). Under the right choices of \(_{*}\) and \(M\), the obtained **E**As can encode interesting and widely applied architectures, such as CNNs  and DeepSets  (see SuppMat-C.1 for further discussion). We call \(^{G}\) the **subspace of invariant parameters**, which is a closed linear subspace of \(\), with unique _orthogonal projection_\(P_{^{G}}:^{G}\), explicitly given by \(P_{^{G}}.z:=_{G}M_{g}.z\,d_{G}(g)\) for \(z\) (see ). We are thus led to solve: \(_{(^{G})^{N}}R()\) or, equivalently, to find the best _projected_ model \(_{}^{N,EA}:=_{*},P_{^{G}}\#_{}^{N}\), by minimizing \(R^{EA}():=_{}[(_{}^{N,EA}(X),Y )]\). This can considerably reduce the parameter space dimension; however **EA** might generally yield a decreased expressivity or approximation capacity.

## 3 Symmetries in overparametrized neural networks: main results

### Two notions of symmetries for parameter distributions

The following notions regarding distributions from \(()\) are central to our work:

**Definition 5**.: _Given \(()\), we respectively define its **symmetrized** and **projected** versions as \(^{G}:=_{G}(M_{g}\#)d_{G}\) and \(^{^{G}}:=P_{^{G}}\#\). Moreover, we introduce two subspaces of \(()\): \(^{G}():=\{()\ :\  g G,\ M_{g}\#=\}\) and \((^{G}):=\{()\ :\ ( ^{G})=1\}\)._

**Example**.: _For \(G=\{ 1\}\) acting multiplicatively on \(=\), one has \(^{G}=\{0\}\), hence \((^{G})=\{_{0}\}\), while \(^{G}()=\{(+(-)):( _{+})\}\). In particular, for \(z\), \((_{z})^{G}=(_{z}+_{-z})\)._

**Definition 6** (**Invariant Probability Measures)**.: _We say that \(()\) is:_

\[\ =^{G}\ \ \ \ \ \ =^{^{G}}.\]

We notice that: \((^{G})^{G}()\), \(^{G}^{G}()\) and \(^{^{G}}(^{G})\). Thus, **SI** implies **WI**. Next result relates the symmetrization operation on \(()\) with the one on shallow models \(_{_{*}}(())\):

**Proposition 1**.: _Let \(_{}_{_{*}}(())\) with \(_{*}:\) jointly equivariant. Then:_

\[(_{G}_{})=_{^{G}}.\]

_That is to say, the **closest equivariant function** (in \(L^{2}(,;_{})\)) to \(_{}\) is given by the shallow model associated to the **symmetrized** version of \(\)._

**Remark**.: _In particular, \(_{}\) is equivariant as soon as \(\) is **WI** only. Conversely, if \(_{}:\) is an equivariant function, then \(_{}=_{^{G}}\), i.e. it can be expressed in terms of a **WI** distribution. This highlights a priority role of **WI** distributions on \(\) in representing invariant shallow models._

The alternative, 'projected model' \(_{^{^{G}}}\), in turn, is never the closest equivariant shallow model, to \(_{}\) in \(L^{2}(,,_{})\), unless equal to \(_{^{G}}\). The latter rarely is the case (unlike commonly implied in the literature). In fact, the symmetrized version \(_{G}_{}^{N}\) of a shallow NN model \(_{}^{N}\) involves \((_{}^{N})^{G}=_{i=1}^{N}_{_{i}}\), where \( z,\ _{z}\) is the orbit measure of the action,3 and has \(N|G|\)\(\)-valued parameters (possibly with \(|G|=\)). This sharply contrasts \((_{}^{N})^{^{G}}=_{i=1}^{N}_{ _{G^{G}}_{i}}\), which has \( N\) distinct parameters, all living in \(^{G}\). So, in general, depending on \(_{*}\) and \(G\), one might have \((_{*},(_{}^{N})^{^{G}})_{G}_{ }^{N}\). A notable case in which the equality holds is the class of linear models, which is discussed in SuppMat-E.1.

**Example**.: _In the previous example, for \(=_{z}\) and \(_{*}\) jointly equivariant, we have \(_{}=_{*}(,z)\), \(_{^{G}}=_{G}_{}=(_{*}(,z)+ _{*}(,-z))\) and \(_{^{G}}=_{*}(,0)\) which are generally distinct if \(z 0\). Notice that \(_{^{G}}\) is an equivariant function without any of its 'parameters' living in \(^{G}\)._

### Invariant functionals on \(()\) and their optima

In the same spirit as when defining the population risk \(R:()}\) in (2), the risk functions associated with SL-techniques from Section 2.3 can be lifted to functionals over \(()\), namely to: \(R^{DA}():=_{}[_{G}(_{}(_{g}.X), _{g}.Y)d_{G}(g)]\), \(R^{FA}():=_{}[(_{G}(_{})(X),Y )]\) and \(R^{EA}():=_{}[(_{^{G}}(X),Y)]\), respectively. This will allow us to study these SL-techniques, in the overparametrized regime, under a common **MF** framework. We need the following assumption:

**Assumption 1**.: \(_{2}()\)_; \(:\) is convex, jointly invariant and differentiable with \(_{1}\) linearly growing; and \(_{*}:\) is bounded, jointly equivariant and differentiable._

The _quadratic loss_\((y,)=||y-||^{2}\) is an example of such \(\). Having \(_{*}\) bounded and differentiable is a simplifying assumption, usually made in the **MF** literature, when establishing key results such as _global convergence of NN_ (see e.g. [12; 38; 53]); relaxing this condition to include further commonly-used functions \(_{*}\) seems feasible, up to some additional technicalities (see SuppMat-A.2 for further discussion). Finally, having \(_{*}\) be jointly equivariant (as defined in section 2.3) isn't a truly restrictive assumption: under the right choice of \(_{*}\) and \(M\), any usual single-hidden-layer NN architecture can be made to satisfy it (see SuppMat-C.1 for a deeper discussion). We also need:

**Definition 7**.: _A functional \(F:()\) is invariant if \(F(M_{g}\#)=F()\;(g,) G()\); equivalently, if it equals its symmetrized version \(F^{G}():=_{G}F(M_{g}\#)d_{G}(g)\)._

**Proposition 2**.: _Under Assumption 1, \(R^{DA}\), \(R^{FA}\) and \(R^{EA}\) are invariant (and convex) and we have:_

\[R^{DA}()=R^{G}(),\;\;R^{FA}()=R(^{G})\;\;\;\;R^{EA}()= R(^{^{G}}).\]

_In particular, \(R=R^{DA}\) if \(R\) is invariant. Moreover, \(^{G}()\), \(R()=R^{DA}()=R^{FA}()\). Last, if \(^{G}()\) (the data distribution is equivariant), then \(R\) is invariant._

The proof relies on Proposition 1 and calculations as in , see SuppMat-E.2. Next result is a general property of functionals over \(()\), which is key for the forthcoming analysis:

**Proposition 3** (**Optimality for Invariant Functionals)**.: _Let \(F:()}\) be convex, \(^{1}\) and invariant. Then: \(()\), \(F(^{G}) F()\); and so, \(_{^{G}()}F()=_{( )}F()\). In particular, if \(F\) has a unique minimizer over \(()\), it **must be WI**._

The proof relies on an ad-hoc version of Jensen's inequality. Next, we state that optimizing under **DA** and **FA** is essentially equivalent, and corresponds to optimizing \(R\) exclusively over **WI** measures:

**Theorem 2** (**Equivalence of DA and FA)**.: _Under assumption 1, we have:_

\[_{()}R^{DA}()=_{^{G}( )}R^{DA}()=_{^{G}()}R()=_{ ^{G}()}R^{FA}()=_{( )}R^{FA}().\]

Note that, on the other hand, \(R^{EA}\) only satisfies: \(_{()}R^{EA}()=_{( ^{G})}R()\). In the case of the quadratic loss, Theorem 2 can be made more explicit:

**Corollary 1**.: _Under Assumption 1, when the loss is quadratic and \(_{}\) is invariant, we have:_

\[_{^{G}()}R()=R_{*}+_{^{ G}()}\|_{}-f_{*}\|_{L^{2}(,;_{ })}^{2}=_{*}+_{^{G}()}\| _{}-_{G}.f_{*}\|_{L^{2}(,;_{})}^{2}.\]

_where \(f_{*}=_{}[Y|X=]\), and \(R_{*}\), \(_{*}\) are constants only depending on \(\) and \(f_{*}\). That is, optimizing under **DA** and **FA** corresponds to approximating the **symmetrized** version of \(f_{*}\)._Under equivariance of the data distribution \(\), the following general result also holds:

**Corollary 2**.: _Let Assumption 1 hold and suppose \(^{G}()\). Then, \(R\) is invariant and therefore: \(_{()}R()=_{^{G}( )}R()=_{()}R^{DA}()=_{ ()}R^{FA}()\)._

**Remark**.: _Consequently, equivariant data allow us to globally optimize the population risk by only considering **WI** measures. It also shows that **DA** and **FA** provide no advantage for this optimization._

The same unfortunately is not true for **SI** measures (answering a question in ), as shown by the following result, which constructs a simple example in which \(^{G}\) is trivial:

**Proposition 4**.: _Even with a finite group \(G\) acting orthogonally on \(=^{d},\ =\) and \(=^{(d+2)}\), with \(\) being compactly-supported and equivariant; with \(\) being quadratic; and with \(_{*}\) being \(^{}\), bounded and jointly equivariant; we can have: \(_{()}R()<_{(^{G})}R()\)._

In fact, even if \(R\) is invariant, when \(^{G}\) is _too restrictive_, it might become impossible to globally optimize \(R\) over **SI** measures (which amounts to using \(R^{EA}\) as a proxy for \(R\)). This subtlety has to be considered when deciding to use **E**As on problems where symmetries exist. Nevertheless, if \(^{G}\) has good _universality_ properties, a true **SI** solution to the learning problem can be sought for:

**Proposition 5**.: _Let Assumption 1 hold, \(\) be quadratic and \(^{G}_{G}()\). If \(_{_{*}}((^{G}))\) is dense in \(L^{2}_{G}(,;_{}):=_{G}(L^{2}( ,;_{}))\), then: \(_{()}R()=_{(^{G})}R()=R_{*}\)._

**Remark**.: _See e.g.  for conditions on \(^{G}\) and \(_{*}\) guaranteeing this'restricted' universality on \(L^{2}_{G}(,;_{})\). These allow for effectively solving the problem in fewer dimensions, which is key in successful **EA** like CNNs and DeepSets. See SuppMat-E.2.5 for a deeper discussion._

### Symmetries and SL training dynamics in the overparametrized regime

We now study the **MFL** of the various training dynamics when \(=^{D}\). We begin with the general:

**Theorem 3**.: _Let \(F:()}\) be an invariant functional such that **WGF**(\(F\)) is well defined and has a unique (weak) solution \((_{t})_{t 0}\). If \(_{0}^{G}_{2}()\), then, for \(dt\)-a.e. \(t 0\) we have \(_{t}^{G}_{2}()\)._

The proof of Theorem 3 relies on \(D_{}F\) being equivariant (in a suitable sense) and \((M_{g}_{t})_{t 0}\) satisfying also, as a consequence, **WGF**(\(F\)) (See SuppMat-E.3 for the details). Note that \(_{0}^{G}_{2}()\) is simply verified, e.g. by a standard Gaussian in \(\). Specializing this result, we get:

**Corollary 3**.: _Let Assumption 1 and technical assumptions (as in ) hold. Then, if \(R\) and \(r\) are invariant, **WGF**(\(R^{r,}\)) starting from \(_{0}^{G}_{2}()\) satisfies: for \(dt\)-a.e. \(t 0,\ _{t}^{G}_{2}()\). If moreover \(>0\), each \(_{t}\) has a density function invariant with respect to \(G_{M}\)._

**Remark**.: _If \(\) is equivariant, \(R\) is invariant, and this result is valid for a **freely-trained NN, without employing SL-techniques**. In a way, **MFL** incorporates these symmetries from infinite SGD iterations._

Theorem 3 and Corollary 3 can thus be seen as significant generalizations of Proposition 2.1 from , which addresses the case of wide 2-layer ReLU networks with a target function that's symmetric w.r.t. a single orthogonal transformation. The fact that strong solutions to **WGF**(\(R^{r,}\)) can be sought among invariant functions to reduce the complexity when \(\) is equivariant, was also first hinted in . The natural domain of invariant functions is in fact the quotient space of \(G_{M}\) (and not \(^{G}\), which is strictly embedded in it).

Comparing the different training dynamics at the **MF** level and applying Proposition 2, we also get:

**Theorem 4**.: _Under assumptions of Corollary 3, if \(_{0}^{G}_{2}()\), **WGF**(\(R^{DA}\)) and **WGF**(\(R^{FA}\)) solutions are equal. If further \(R\) is invariant, the **WGF**(\(R\)) solution coincides with them too._

**Remark**.: _In particular, with equivariant data (i.e. invariant \(R\)), training with **DA** or **FA** is essentially the same, at least at the **MF** level, as **using no SL-technique whatsoever**. Hence, a relevant, practical open question, is: how do the convergence rates to the **MFL** compare in all three cases, as \(N\)?_

We will now see that similar results hold for \((^{G})\) instead of \(^{G}()\). Notice that the _entropy-regularized_ risk forces each \(_{t}\) to have a density w.r.t. \(\) in \(\) if \(>0\). Therefore, if \(G_{M}\) is non-trivial (thus \(^{G}\) is a strict subspace), we always have \(_{t}(^{G})\). It thus seems natural to _restrain_ the noise in equation (1) to stay in \(^{G}\); namely, to consider the _projected noisy SGD_ dynamic:

\[_{i}^{k+1}=_{i}^{k}-s_{k}^{N}\ (_{z}_{*}(X_{k}, _{i}^{k})_{1}(_{^{k}}^{N}(X_{k}),Y_{k})+  r(_{i}^{k}))+^{N}}P_{^{G}} _{i}^{k}.\] (5)Note that projecting _only_ the noise in (5) doesn't force \(_{t}^{k+1}\) to be in \(^{G}\), even if \(_{t}^{k}\) was. Introducing the related **projected-regularized functional**: \(R_{^{G}}^{,}():=R()+ rd+ H_{_{ ^{G}}}(^{^{G}})\), with \(_{^{G}}\) the Lebesgue Measure over \(^{G}\), we get the following analogue of Corollary 3:

**Theorem 5**.: _Let Assumption 1 and technical assumptions on \(R\) hold. Suppose that \(R\) and \(r\) are invariant. Then, if \(_{0}_{2}(^{G})\), \((_{t})_{t 0}\) solution of **WGF**(\(R_{^{G}}^{,}\)) satisfies \( t 0,\ _{t}_{2}(^{G})\)._

The result holds for \( 0\). Its proof is based on pathwise properties of the McKean-Vlasov stochastic differential equation (studied e.g. in ) associated with the **WGF**(\(R_{^{G}}^{,}\)), see SuppMat-D.2.

**Remark**.: _Theorem 5 is significantly stronger than Corollary 3: it implies that, for equivariant \(\), the flow will remain in the set of **SI** distributions all throughout its evolution, despite there being no explicit constraint on the network parameters during training (they can all be freely updated), nor any SL-technique being used. This is a highly non-intuitive fact, and a large \(N\) exclusive phenomenon, as our numerical experiments will show. See SuppMat-D.2 for a deeper discussion._

**Remark**.: _Notice that, despite the computation of \(^{G}\) being generally hard (see ), \(_{0}_{2}(^{G})\) can be achieved by simply setting \(_{0}=_{}\). Moreover, since one can also take \(=0\), 'having access' to the noise projection \(P_{^{G}}\) is never explicitly required, allowing for a broader applicability of the result. In particular, as we'll show in our experiments, usual shallow NNs with all parameters initialized at \(\{0\}\), freely trained with 'noiseless' SGD, will satisfy Theorem 5 in the **MFL**._

**Remark**.: _Theorem 5 holds too for the invariant functionals \(R^{DA}\), \(R^{FA}\) and \(R^{EA}\) in the role of \(R\), **even if \(\) is not equivariant**. Notably, **DA**, **FA** and **EA** procedures starting on a **SI** distribution, despite being free to involve all parameters, will keep the distribution **SI** all throughout training._

Last, we also have:

**Theorem 6**.: _Let the conditions for Theorem 5 hold. If \(_{0}_{2}(^{G})\), the **WGF**(\(R^{FA}\)), **WGF**(\(R^{DA}\)) and **WGF**(\(R^{EA}\)) solutions **coincide**. If \(R\) is invariant, **WGF**(\(R\)) solution coincides with them too._

## 4 Numerical experiments and architecture-discovery heuristic

To empirically illustrate some of our results from the previous section, we consider synthetic data produced in a **teacher-student** setting (see e.g. [14; 16]). Code necessary for replicating the obtained results, as well as a detailed description of our experimental setting, can be sought in the SuppMat.

We study a simple setting with: \(==^{2}\), \(=^{2 2}^{4}\), and \(G=C_{2}\) acting on \(\) and \(\) by _permuting the coordinates_; and on \(\) via the natural _intertwining_ action (for which \(^{G}\) is explicit). We take the jointly equivariant activation \(_{*}(x,z)=(z x)\), \((x,z)\) with \(:\) a sigmoidal function applied pointwise; and consider _normally_ distributed features, and labels produced from a **teacher model**\(f_{*}\). This teacher is given by a shallow NN model, either \(f_{*}=_{^{*}}^{N_{*}}\) with \(N_{*}=5\)**arbitrary** particles \(^{*}^{N_{*}}\), or its symmetrized version \(f_{*}=_{G}._{^{*}}^{N_{*}}\) (referred to as **WI**), with \(10\) particles. 4 Notice that the data distribution \(\) will be equivariant only if the teacher is. We try to **mimic** such **teacher** with a **student model**, \(_{}^{N}\), with the same \(_{*}\), but different particles \(^{N}\) that will be trained to minimize the regularized population risk \(R^{,}\) (with quadratic loss and penalization). For this we employ the SGD dynamic given by Equation (1) (or projected, if required, as in Equation (5)), possibly involving **DA**, **FA** or **EA** techniques. We refer to _free training_, with no SL-techniques involved, as **vanilla** training. Each experiment was repeated \(N_{r}=10\) times to ensure consistency. Explicit values of the fixed training parameters are found in SuppMat-F.

### Study for varying \(N\)

We demonstrate how properties of **WGF**(\(R^{,}\)) stated in Section 3.3 become apparent as \(N\) grows. We consider a teacher with \(_{^{*}}^{N_{*}}\) either **arbitrary** or **WI**, and different training schemes performed over \(N_{e}\) epochs, all initialized with the same particles drawn from given \(_{0}_{2}()\) that is either **SI** or **WI**. Figure 1 displays the behavior of the student's particle distribution, \(_{N_{*}}^{N}\), after training, in terms of certain 'normalized version' of the \(W_{2}\)-distance, which we call simply Relative Measure Distance, or **RMD** for short. 5 We refer to SuppMat-F for further insights and, additionally: a deeperanalysis of the case of \(_{^{*}}^{N_{*}}\) being **SI**, comparisons between different techniques and **EA**, and \(L^{2}\) comparisons between \(_{}^{N}\) and both \(f_{*}\) and \(_{G}.f_{*}\) (to illustrate Corollary 1).

We first look at the **SI**-initialized training. Though from  we know that (exact) **DA** or **FA** during training will _respect_\(^{G}\) without needing to pass to the **MFL**. This is certainly not true in general for the **vanilla** scheme, where the symmetry is never _explicit_ for the model. We notice in Figure 1, however, that, as \(N\) grows big, the **SI**-initialized **vanilla** training scheme, under only a **WI** teacher, does **remain SI** throughout training, as predicted in Theorem 5. This is absolutely remarkable, since there is no intuitive reason why the **vanilla** scheme (were parameters can be updated _freely_) _shouldn't escape_\(^{G}\) to better approximate \(f_{*}\). For instance, for an **arbitrary teacher_** (with the same particles, but _un-symmetrized_) **vanilla** training readily _leaves_\(^{G}\) to better approximate \(f_{*}\). Though this isn't a _predicted behaviour_ from our theory, it motivates a heuristic we present in the upcoming section. On the other hand, and as expected, both **DA** and **FA** consistently remain within \(^{G}\) almost independently of \(N\), and even if \(f_{*}\) isn't equivariant. Finally, as \(N\) grows bigger, the end-of-training distribution of the **vanilla** scheme _approaches_ that of **DA** and **FA** (as expected from Theorem 4).

Regarding the **WI**-initialized training, unlike the **SI** case, particles sampled _i.i.d._ from a **WI** distribution don't necessarily yield a **WI** empirical distribution \(_{0}^{N}\). On the one hand, this means we require large \(N\) to see \(_{N_{e}}^{N}\) being (approx.) **WI**; and on the other hand, it means we have no guarantee that **DA** and **FA** will be close unless we look at larger \(N\) (where Theorem 4 applies). The second column of Figure 1 precisely shows these behaviours as \(N\) grows: both a trend of \(_{N_{e}}^{N}\) towards becoming **WI**, and a clear coincidence between the **DA**, **FA** and **vanilla** schemes (the latter only for equivariant \(f_{*}\)).

### Heuristic algorithm for discovering EA parameter spaces

From these experiments, for non-equivariant \(f_{*}\), the **SI**-initialized WGF is seen to eventually **escape**\(^{G}\). In turn, for equivariant \(f_{*}\), Figure 2 shows that a training scheme initialized at \(E^{G}\) (i.e. \(_{_{0}}^{N}(E)\)), eventually **leaves**\(E\), but **stays within**\(^{G}\) (as expected from Theorem 5). These empirical observations hint to an heuristic for **discovering the 'good' EAs for the task at hand**.

Assume indeed \(\) equivariant w.r.t. \(G\). We want to find the unknown, largest (i.e. most expressive) subspace of \(\) supporting **SI** measures. We hence consider some (large) \(N\), a shallow NN model with e.g. \(_{*}(x,z)=(z.x)\), and numerical thresholds \((_{j})_{j}_{+}\). We define \(E_{0}=\{0\}^{G}\) as an initial subspace and initialize \(_{_{0}}^{N}=_{}^{N}(E_{0})\). Then, we iteratively proceed as follows:For \(j=0,1,\), initialize a model at \(^{N}_{_{0}}(E_{j})\), train it for \(N_{e}\) epochs, and check whether \(^{2}(^{N}_{N_{e}},P_{E_{j}}\#^{N}_{N_{e}})_{j}\). If that is the case, the training didn't **escape**\(E_{j}\), and one could suppose \(^{G}:=E_{j}\). Otherwise, it **left**\(E_{j}\) (so \(^{G} E_{j}\)) and one can set e.g. \(E_{j+1}:=E_{j} v_{E_{j}}\), with \(v_{E_{j}}:=_{i=1}^{N}(_{i}^{N_{e}}-P_{E_{j}}._{i}^{ N_{e}})\). Allegedly, this scheme would eventually leave all strict subspaces to reach the 'right' \(^{G}\). Figure 2 indeed illustrates this behaviour in our simple **teacher-student** example (see SuppMat-F2 for further details). Notice that we start knowing close to nothing about data symmetries (\(E_{0}=\{0\}\)), and end up _'discovering'_ a data-based parameter-sharing scheme (\(E_{*}=^{G}\)) that allows for building **SI** NNs. This idea might have potential for real world applications, yet a larger scale experimental analysis and rigorous theoretical guarantees need to be provided.

We refer to  for a different approach to this idea of 'discovering the real symmetries of the data'. Their work uses _relaxed_ group convolution layers to discover 'data-driven symmetry-breaking' in ML problems. A deeper comparison between both approaches shall be found in SuppMat-F2.

## 5 Conclusion

In the light of theoretical guarantees given by the **MF** theory of overparametrized shallow NN, we explored their training dynamics when data is possibly equivariant for some group action and/or SL techniques are employed. We thus described how **DA**, **FA** and **EA** schemes can be understood in the limit of infinite internal units, and studied in that setting the qualitative advantages that can be attired from their use. In this **MFL**, **DA** and **FA** are essentially equivalent in terms of the optimization problem they solve and the trajectory of their associated **WGF**s. Moreover, for equivariant data, _freely_-trained NN, in the **MFL**, obey the same **WGF** as **DA/FA**. They also "respect" symmetries during training, as **WI** and **SI** initializations (corresponding to symmetric parameter distributions and **EA** configurations) are preserved throughout, even if potentially all NN parameters can be updated. We also highlighted the prominent role of **WI** laws in representing equivariant models. We illustrated our results with appropriate numerical experiments, which in turn suggested a data-driven heuristic to find appropriate parameter subspaces for **EAs** in a given task. Providing theoretical guarantees for this heuristic is an interesting problem left for future work. A further relevant question to address, is to quantify and compare the speeds at which all studied training schemes approach the **MFL**, as this would a provide a full comparative picture of their performances. Extending the **MF** analysis of symmetries to NNs with more complex inner structures is another interesting line of work.

Figure 2: Heuristic method applied on **teacher** (squares) and **student** (dots) particles. _Row 1_: aerial view of hyperplane \(^{G}\). _Row 2: parallel_ view, to verify student particles always remain in \(^{G}\) (red line). _Column 1_: step \(j=0\) after training; particles **leave**\(E_{0}=\{0\}\). _Column 2_: initialization of step \(j=1\) on \(E_{1}= v_{E_{0}}\). _Column 3_: step \(j=1\) after training; particles **leave**\(E_{1}\) (Row 1), but not \(^{G}\).