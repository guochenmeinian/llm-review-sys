# State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding

Deleena Das

School of Interactive Computing

Georgia Institute of Technology

ddas41@gatech.edu

&Sonia Chernova

School of Interactive Computing

Georgia Institute of Technology

chernova@gatech.edu

&Been Kim

Google Research

beenkim@google.com

###### Abstract

As more non-AI experts use complex AI systems for daily tasks, there has been an increasing effort to develop methods that produce explanations of AI decision making that are understandable by non-AI experts. Towards this effort, leveraging higher-level concepts and producing concept-based explanations have become a popular method. Most concept-based explanations have been developed for classification techniques, and we posit that the few existing methods for sequential decision making are limited in scope. In this work, we first contribute a desiderata for defining "concepts" in sequential decision making settings. Additionally, inspired by the Protege Effect which states explaining knowledge often reinforces one's self-learning, we explore how concept-based explanations of an RL agent's decision making can in turn improve the agent's learning rate, as well as improve end-user understanding of the agent's decision making. To this end, we contribute a unified framework, State2Explanation (S2E), that involves learning a joint embedding model between state-action pairs and concept-based explanations, and leveraging such learned model to both (1) inform reward shaping during an agent's training, and (2) provide explanations to end-users at deployment for improved task performance. Our experimental validations, in Connect 4 and Lunar Lander, demonstrate the success of S2E in providing a dual-benefit, successfully informing reward shaping and improving agent learning rate, as well as significantly improving end user task performance at deployment time.

## 1 Introduction

Black-box AI systems are increasingly being deployed to help end-users with everyday tasks. Examples include doctors leveraging decision support systems to aid in diagnosis , warehouse managers relying on robots for goods transportation , and drivers using autonomous vehicles for assisted driving . To increase the transparency of these black-box models, researchers have developed numerous techniques to provide explanations of agent decision making .

A popular method towards non-expert friendly explanations has been to attribute higher-level "concepts" to an agent's decision making, and these concepts have primarily been used to explain classification-based AI systems . An example of a concept-based explanation for a classification model includes, "wing color" for a bird class label . In sequential decision making, concept-based explanations have been less widely explored, and existing works leverage preconditions of states and action costs  or logical formulas  as "concepts".

In this work, we posit that concept-based explanations may not _only_ benefit the end-user but also benefit the AI agent. Our claim is loosely motivated by the Psychological phenomenon, the Protege Effect, that states explaining material to another student also helps the explainer learn and reinforce material [7; 24; 17]. Thus the objective of our work is to demonstrate the dual-benefit of concept-based explanations to both the end-user for improved understanding, as well as the AI agent for improved learning rate. Note, prior works, typically in Reinforcement Learning (RL), have explored various methods to improve agent learning, a common approach including the use of human feedback and natural language commands as a method of reward shaping to improve RL agent sample efficiency and learning rate [38; 50; 20]. However, these methods have focused on leveraging language as a one-way benefit to the agent, not considering how language provided to the agent during training may also benefit end-user understanding at deployment. Therefore in our work, we explore the following research question: _Can concept-based explanations have a two-way benefit to both the user and agent, such as to improve end-user understanding of a deployed agent's decision making, while also improving the agent's learning rate via reward shaping?"_ As a solution, we contribute a unified framework, **State2Explanation (S2E)**, that learns a single joint embedding model between agent states and associated concept-based explanations and leverages such learned model to (1) inform reward shaping for agent learning, and (2) improve end-user understanding of the agent's decision making at deployment.

Additionally, we contribute a desiderata of what entails a "concept" in a concept-based explanation for sequential decision making systems. Given that sequential decision making agents engage in long-term interaction with their environment, we posit that the scope of concept-based explanations should span _beyond_ representing preconditions and action cost , and control logic . Specifically, we believe concept-based explanations in sequential decision making should function at a much higher-level of abstraction, highlighting knowledge that are applicable across multiple states, and most importantly, expressing a positive or negative influence towards the agent's goal. Our claim is motivated by the importance of the agent's goal in a sequential decision making formulation [44; 49].

**Contributions**. (1) We provide desiderata for what constitutes a "concept" in concept-based explanations of sequential decision making. (2) We introduce a novel framework, State2Explanation (S2E) which learns a joint embedding model between agent states-action pairs and concept-based explanations to provide a benefit to both the agent, by informing reward shaping, as well as the end user by providing explanations that improve user task performance. We perform both model and user evaluations of our S2E framework in two complex RL domains, Connect 4 and Lunar Lander.

Our model evaluations demonstrate that S2E can successfully inform reward shaping, comparable to expert-defined reward functions. Additionally, our user evaluations demonstrate that S2E can successfully provide meaningful concept-based explanations to end users such that exposure to our explanations significantly improve user task performance.

## 2 Related Work

In this work, we explore the utility of concept-based explanations in providing a two way benefit to both the AI agent during training, via reward shaping, as well as end user at deployment time, via improved user task performance. Below, we highlight related prior work.

**Reward Shaping using Language** Many prior works have focused on improving an agent's learning rate or sample efficiency [35; 21]. Closely related to our work are methods providing reward shaping via language. Specifically,  propose ELLA, which leverages a relevance classifier to associate a lower level instruction to a higher level task for providing a subtask shaping reward. In , the authors leverage natural language instruction to derive potential-based shaping rewards that encourage an RL agent to more frequently select relevant actions in the game of Montezuma's Revenge. Similarly,  learn a joint embedding model between subgoal language commands and agent states to provide shaping rewards for completing subtasks in StarCraft II. Specifically,  demonstrate that their model can generalize to unseen, semantically similar language commands, and improve agent learning rate. In our work, we take inspiration from , and learn a joint embedding model between agent states and associated natural language commands to inform agent reward shaping. However, our work differs in that our natural language commands are _concept-based explanations_ of the associated state, and not subgoal descriptions required for task completion. By learning a common embedding space between state explanations and state representations, our work provides a _two-way_ benefit to both the RL agent for learning as well as end-user for improved understanding.

Explanations for non-AI ExpertsPrior works have established that concept-based explanations for classification tasks provide explanations that are meaningful to end-users [30; 33; 54; 19]. Methods for concept-based explanations have included Concept Activation Vectors , Concept Bottleneck Models , and Concept Embedding Models . For classification tasks, "concepts" have primarily represented human-interpretable features that a model's prediction is sensitive to (i.e. 'wing color' for image classification.) . Related to sequential decision making,  provide concept-based explanations of 3D action recognition CovNets by clustering learned, human-interpretable features (i.e. "grass" or "hand") from segmented videos. In , concept-based explanations for sequential decision making are formulated in relation to state preconditions (e.g. "the action _move-left_ failed in the state as the precondition _skull-not-on-left_ was false in the state") and action costs (e.g., "executing the action _attack_ in the presence of the concept _skull-on-left_, will cost at least _500_"). Specifically, in , concepts have represented any factual statement a user associates to a given state, such as "skull-on-left" in the previous example. Similarly, in , concepts have represented logical formulas for summarizing RL policies. Additionally,  propose a prototype wrapper network for training interpretable RL policies with user-defined prototypes. Similar to , the authors in  define prototypes as factual observations about a state, such as "turn right" or "turn left". Other explanation generation methods have demonstrated that end user understanding is improved when explanations are presented in natural language rather than features or labels [14; 5; 9; 10]. Inspired by these findings, our work contributes the first method for producing natural language concept-based explanations, where we define a general desiderata of concepts that goes beyond action costs and preconditions to capture goal-driven decision making.

## 3 Concept Based Explanations for Sequential Decision Making

We posit that the objective of a concept-based explanation, in sequential decision making settings, is to communicate higher-level knowledge that aids the agent and/or user in reasoning about the utility of taking a given action from a specific state. In this section, we provide a general definition of concept-based explanations and "concepts", in sequential decision making, that extends beyond prior usage of state preconditions and action-costs , and control logic . Specifically, we define a concept-based explanation, \(_{C_{i}}^{i}\), as one that presents, in natural language form, the set of concepts, \(C_{i}=\{c_{j}.c_{m}\}\), that can describe a particular state-action pair \((s_{i},a_{i})\) in the context of goal \(G\). This naturally poses the question of: _"What is a concept \(c_{j}\)?"_ Below are several desiderata for defining a concept in sequential decision making settings:

1. **A concept should be grounded in human domain knowledge**. A concept, \(c_{j}\), should be contextualized in the overall task domain. For example, although an agent's goal may be to optimize its Q-function, we posit that "higher Q-value" is not a valid concept. Instead, a concept should represent a higher-level abstraction of what the "higher Q-value" relates to in the domain. For example, in Chess, an agent receiving a "higher Q-value" for a particular state-action pair may be translated by a domain expert to "capturing a queen".
2. **A concept should relate to the task goal**. A concept, \(c_{j}\), is an abstracted representation of \((s_{i},a_{i})\) that encompasses how \((s_{i},a_{i})\) may lead to or inhibit the agent's goal \(G\). This claim is motivated by the general objective of a sequential decision making agent, that is to perform a sequence of actions \( a_{1},a_{2},...,a_{n},a_{i} A\), defined via a plan or policy \(\), that transforms the agent's current state \(I\) to its goal state \(G\)[44; 49]. For example, we do not consider the description "Knight in A4" as a valid concept, since, by itself, it does not provide relation to the game's objective. However, we would consider "king safety" as a valid concept, since it relates to game's objective of checkmate.
3. **A concept should be generalizable.** A concept, \(c_{j}\), should generalize across multiple \((s_{i},a_{i})\) pairs. This means that a concept is typically _not_ a unique description of a single \((s_{i},a_{i})\). For example, a valid concept in Chess is "checkmate" since the abstraction of "checkmate" can generalize to many different \((s_{i},a_{i})\). An invalid concept involves detailing the entire game board by piece position.

Note, above we define a concept, \(c_{j} C_{i}\), in the context of describing a single state-action pair, \((s_{i},a_{i})\). However, in many domains a meaningful concept \(c_{j}\) can only be defined in terms of a sequence of state-action pairs. This is especially true in domains in which state-action pairs are sampled at high-frequency, such as robotics and video games [40; 56]. For example, concepts such as "rotate wrist" or "bend arm" can only be defined by a series of state-action pairs, as opposed to a single state-action pair. As a result, we define concept-based explanation in these domains as \(^{[i,i+n]}_{C_{i}}\), which presents the set of concepts \(C_{i}=\{c_{j}..c_{m}\}\), for a series of \((s_{[i,i+n]},a_{[i,i+n]})\) pairs.

Finally, we highlight that some \((s_{i},a_{i})\) or \((s_{[i,i+n]},a_{[i,i+n]})\) pairs may not have a concept-based explanation associated with them. In other words, \(C_{i}=\) for some \((s_{i},a_{i})\) pair or \((s_{[i,i+n]},a_{[i,i+n]})\) pairs. This is especially true in long horizon tasks, in which a \((s_{i},a_{i})\) may not relate directly to \(G\) but is important for another, future \((s_{i+n},a_{i+n})\) pair which then leads to \(G\). For example, "Move the pawn to D5 so that two moves from now you may capture the rook". Our definition of concept-based explanations do not consider these cases, and is an opportunity for future work.

## 4 State2Explanation (S2E) Framework

Figure 1 introduces our State2Explanation (S2E) framework which includes a three-step process to provide a benefit to both the RL agent and the end-user. Specifically, S2E learns a joint embedding model \(M\) to align agent state-action pair \((s_{i},a_{i})\) to concept-based explanations \(^{i}_{C_{i}}\) (Fig. 1a). S2E then leverages the learned \(M\) to inform reward shaping during agent training (Fig. 1b). Finally, S2E leverages the learned \(M\) to provide \(^{i}_{C_{i}}\) about the trained agent's decision making to end-users at deployment (Fig. 1c). Below, we further detail S2E.

### Joint Embedding Model \(M\)

A core component of S2E is learning a joint embedding model, \(M\), to align an agent state-action pair, \((s_{i},a_{i})\) with an associated concept-based explanation \(^{i}_{C_{i}}\). The motivation for learning \(M\) is to explicitly map an agent's representation, \(s_{i}\), into a representation understandable by our non-AI expert users, \(^{i}_{C_{i}}\). Joint embedding models have been leveraged in many applications, such as in image captioning [55; 34], knowledge-graph generation [28; 43], and RL [42; 41; 50] to improve task learning. In the context of S2E, we demonstrate that joint embedding \(M\) can provide the _dual_ benefit to i) agent learning, and ii) improved end user understanding of agent actions. Our \(M\) is inspired by  who learn an embedding space in the context of hierarchical RL tasks. However, unlike in , our \(M\) is not constrained to hierarchical RL tasks, and we consider domains in which multiple states can be associated to multiple, non-unique concepts.

The input to our joint embedding model, \(M\), is defined by \([s_{i},s_{i-1}],g_{i},^{i}_{C_{i}},y_{i}\), which includes an agent's current state \(s_{i}\), previous state \(s_{i-1}\), other contextual game information (i.e., player in a multiplayer game, whether game is over) \(g_{i}\), associated concept-based explanation \(^{i}_{C_{i}}\), and \(y_{i}\) which defines if \(s_{i}\) and \(^{i}_{C_{i}}\) are aligned or misaligned. Note, including \(s_{i}\) and \(s_{i-1}\) implicitly encodes the

Figure 1: Our S2E framework involves (a) learning a joint embedding model \(M\) from which a \(^{i}_{C_{i}}\) is extracted and utilized (b) during agent training to inform reward shaping and benefit agent learning, and (c) at deployment to provide end-users with \(^{i}_{C_{i}}\) for agent actions.

action \(a_{i}\) taken to transition from \(s_{i-1}\) to \(s_{i}\). Our contrastive loss function (Eq. 1) is adapted from , in which \(\) represents the model's parameters, and \(i N\) represents the \(i^{th}\) training sample. Specifically, \(M\) minimizes the L2-norm of the difference between the embedding vectors when \(s^{i}_{}\) and \(exp^{i}_{}\) are aligned (\(y=0\)) and maximize the L2-norm of the difference when \(s^{i}_{}\) and \(exp^{i}_{}\) are misaligned (\(y=1\)).

\[L()=_{i}^{N}(\|s^{i}_{embed}-^{i}_{embed}\|-y )^{2}\] (1)

To train \(M\), we leverage a dataset \(D=\{D^{a}\), \(D^{m}\}\), in which \(D^{a}\) and \(D^{m}\) denote the set of aligned and misaligned data samples, respectively. To collect \(D^{a}\), we leverage domain knowledge to annotate relevant \(s_{i}\) with its aligned concept set \(C_{i}\). The concept set \(C_{i}\) is then produced into a natural language explanation, \(^{i}_{C_{i}}\), manually via fixed, expert-defined templates. That is, each \(C_{i}\) has one templated \(^{i}_{C_{i}}\). The list of concepts for our evaluation domains are in Section 5.1. To collect \(D^{m}\), each \(s_{i}\) is paired with \(z\) concepts _not_ present in concept set \(C_{i}\). Section 5.2 provides more details on \(D\).

Explanation RetrievalGiven \(M\), for any \((s_{i},a_{i})\), we extract the closest explanation embedding \(exp^{i}_{}\) for corresponding \(s^{i}_{}\), and decode \(exp^{i}_{}\) to an \(^{i}_{C_{i}}\) similar to image-to-text retrieval [31; 6]. Specifically, we rank the set of possible explanation embeddings, \(\{exp^{i}_{}\)...\(exp^{k}_{}\}\), by their L2-norm distance to \(s_{i}\). The decoding of the best ranked \(exp^{i}_{}\) to \(^{i}_{C_{i}}\) is then a vocabulary look-up. It is possible to utilize large language models (LLMs), and _generate_\(^{i}_{C_{i}}\) from \(exp^{i}_{}\) which we consider future work. Instead, in this work, we focus on validating the dual utility of embedding-based \(^{i}_{C_{i}}\) retrieval to both the RL agent as well as end-user.

### Reward Shaping in RL via Joint Embedding Model \(M\)

During agent learning, we utilize \(M\) to retrieve the closest \(^{i}_{C_{i}}\) associated with a \((s_{i},a_{i})\) for informing reward shaping. Prior work has shown that reward shaping can improve agent learning rate and sample efficiency [11; 25; 32]. However, designing an effective dense reward function is not trivial [16; 12], often requiring a two-stage trial-and-error procedure to 1) determine _when_ a state-action pair deserves an intermediate reward, and 2) determine _what_ reward value such state-action pair should be attributed. Many methods, such as inverse reinforcement learning, utilize learned, black box reward functions to minimize the trial-and-error process [37; 1; 57]. However, with black-box reward functions, we lose transparency of why a \((s_{i},a_{i})\) is rewarded. To balance the design-transparency trade-off of dense reward functions, we leverage \(M\) to inform _when_ a \((s_{i},a_{i})\) pair should be rewarded based on the retrieved \(^{i}_{C_{i}}\), and as a result, remove the first step of the trial-and-error process. Specifically, if \(M\) returns a meaningful \(^{i}_{C_{i}}\) for a \((s_{i},a_{i})\) (e.g.,concept set \(C_{i}\)), then such \((s_{i},a_{i})\) represents concepts that influence the agent's goal, and should receive an intermediate reward.

Note, \(M\) does not remove the trial-and-error process needed to determine _what_ values each reward component should receive. Instead, we perform a hyperparameter sweep to assign shaping values to each concept component when an expert-determined, continuous shaping function is not derivable. Thus, the "Explanation-to-reward-shaping" module in S2E performs a look-up to assign a reward value based on the retrieved \(^{i}_{C_{i}}\). In Section 5.3 we validate how leveraging \(M\) to inform reward shaping can improve an agent's learning rate comparable to expert-defined shaping functions.

### Concept-Based Explanations to End Users via Joint Embedding Model \(M\)

Once an RL agent is trained and deployed, we leverage \(M\) to provide \(^{i}_{C_{i}}\) about an agent's decision-making to non AI expert end-users. Recall from Section 3 that \(^{i}_{C_{i}}\) can include multiple concepts (i.e. \(|C_{i}|>1\)). We posit that in some cases, \(^{i}_{C_{i}}\) with _too many_ concepts may become ineffective in aiding end-user understanding. In fact, prior works discuss the challenge of presenting information at the "right" level of abstraction to end-users [46; 53]. To the best of our knowledge, there are no explicit guidelines for what information abstractions are beneficial in sequential decision making settings. From analyzing prior end-user friendly explanation techniques [9; 14], we infer two important dimensions to consider for explanation abstraction in sequential decision making: Information Filtering (InF), and Temporal Grouping (TeG).

Information Filtering (InF)InF filters out concepts within \(^{i}_{C_{i}}\) to only include those that immediately influence the goal. For example, consider a \(^{i}_{C_{i}}\) in Lunar Lander, "Fire left engine because it brings lander closer to the center, decreases lander velocity to avoid crashing, and decreases tilt". Applying _InF_ may produce a filtered explanation, "Fire left engine because it brings lander closer to the center and decreases tilt" given a \((s_{i},a_{i})\) when the lander needs to immediately correct its tilt and position to avoid a trajectory path leading to a crash. While the other concepts are important, they are filtered out since they do not effect the immediate chance of meeting \(G\).

To perform InF, we leverage domain knowledge and expert-defined thresholding to determine which concepts in \(C_{i}\) for \((s_{i},a_{i})\) critically contribute towards the agent's ability to reach goal \(G\). Thresholds are determined via qualitative analysis of an RL agent's policy. In continuous domains, these threshold values are extracted by finding the values in the agent's state space that determine turning points in the agent's ability to reach goal \(G\). For example, qualitative analysis may produce that an agent's \(pos_{x}>0.15\) is a critical turning point for the agent's ability to land close to the center, and if \(pos_{x}>0.15\), the agent's action is crucially linked with decreasing its \(pos_{x}\). Thus, \(C_{}\) represents the concept set after thresholding is applied, in which concepts \(c C_{i}\) that do not meet the determined thresholds are filtered out. As a result, an abstracted explanation, using InF, templates \(C_{}\) into a natural language explanation \(^{i}_{C_{}}\). In Appendix A, we provide more details on how concepts are filtered via qualitative analysis in InF and include an ablation study analyzing the sensitivity of chosen thresholds. Note, while we qualitatively determine thresholds, the InF method can be automated in future work.

Temporal Grouping (TeG)TeG automatically groups explanations over a sequence of time that represents a consecutive pattern. For example, if in Lunar Lander, \(^{i}_{C_{i}}\) and \(^{i+2}_{C_{i}}\) is "Fire left engine because it decreases tilt" and \(^{i+1}_{C_{i}}\) and \(^{i+3}_{C_{i}}\) is "Fire right engine because it decreases tilt", applying TeG produces "For the next 4 steps, alternate firing left and right engine to decrease tilt".

To perform TeG, we analyze an agent's rollout at deployment to extract series of \((s_{[i,i+n]},a_{[i,i+n]})\) with a repeated pattern of concept sets \(C_{[i,i+n]}\). Therefore, a grouped explanation provides a single \(^{i}_{C_{}}\), that templates the repeated pattern within \(C_{[i,i+n]}\) across the \(n\) state-action pairs. We posit that TeG is likely to be important in domains where actions are sampled at high frequency (e.g. Lunar Lander or Robotics), requiring an abstraction over actions to provide meaningful explanations for consecutively alike state-action pairs.

Overall, the "Explanation Abstraction" component in S2E (see Fig. 0(c)) determines whether InF, TeG or both should be applied to \(^{i}_{C_{i}}\) for providing an abstracted, concept-based explanation. In Section 6 we demonstrate the effectiveness of InF and TeG in high frequency domains, such as Lunar Lander, for improved end user understanding.

## 5 Model Evaluations

In this section, we perform quantitative evaluations to validate the joint embedding model \(M\) in S2E by evaluating \(M\)'s explanation retrieval accuracy (Sec. 5.2), and \(M\)'s ability to inform reward shaping during agent training (Sec. 5.3).

### Evaluation Domains

**Connect 4** is an adversarial board game in which the objective is to achieve a four-in-a-row, in any direction. The state space is a discrete, 2D array representation of token positions. Also, the action

Figure 2: The tables present concepts for Connect 4, derived from , and Lunar Lander, derived from , with accompanying example \(^{i}_{C_{i}}\).

space is discrete and each player selects the column to place a token in. **Lunar Lander** is a trajectory optimization problem in which the lander must land on a landing pad. We utilize LunarLander-v2 from OpenAI Gym . The state space is a continuous 1D array including the lander's linear position and velocity, angular velocity, angle, and leg contact. The game has four discrete actions: fire left engine, fire right engine, do nothing, and fire main engine.

**Concept Collection** Concepts for a domain may be extracted from expert players or commentators. However for Lunar Lander and Connect 4, there are no existing datasets for mining concepts. Thus, we leverage expert domain knowledge to attribute concepts to state-action pairs. Figure 2 lists the set of concepts we consider when attributing concepts to a \((s_{i},a_{i})\) in Connect 4 and Lunar Lander, as well as provides example \(^{i}_{C_{i}}\). Concepts for Connect 4 are derived from the strategies outlined in . Note, "NULL" describes all other \((s_{i},a_{i})\) that are not attributed to a concept. Alternatively, concepts for Lunar Lander are derived from the domain's existing reward function . In this domain, the existing dense reward function includes the primary concepts important towards successful task completion. In Appendices B.1 and B.2, we provide an an expanded list of example \(^{i}_{C_{i}}\), spanning all concepts derived for Connect 4 and Lunar Lander.

Note, for both domains, the concepts are defined via mathematical or logical representations of the state. For example, in Connect 4, the concept of "BW", blocking an opponent win, can explicitly be encoded by board representations where any three-in-a-row pattern is blocked with a token from the opponent player. Similarly, in Lunar Lander, the "POS", position concept, is modelled by mathematical representation of the agent's position over time. Deriving concepts via mathematical or logical representations allow us to automatically collect concepts from states, as well as use such mathematical or logical rules to evaluate that concepts and states are accurately paired. In many applications, it may be infeasible to derive mathematical or logical rules from a state representation. In these scenarios, concepts can be collected via crowd sourcing, , or obtained via "think-aloud" procedures .

### Evaluation of Joint Embedding Models

Below we demonstrate high recall rates of the joint embedding models in S2E in both the Connect 4 and Lunar Lander domain.

**Datasets**

Recall from Section 4.1 that to train \(M\) we utilize a dataset \(D=\{D^{a},D^{m}\}\). For Connect 4, \(D_{C4}\) includes approximately 3 million samples, and for Lunar Lander, \(D_{LL}\) includes approximately 2 million samples. We randomly sample \(D^{a}_{C4}\) and \(D^{a}_{LL}\) from an expert RL policy. We obtain \(D^{m}_{C4}\) and \(D^{m}_{LL}\), by selecting \(z\) incorrect concepts from \(C_{C4}\) and \(C_{LL}\), as replacement for each sample in \(D^{a}_{C4}\) and \(D^{a}_{LL}\)(more detailed numbers in Appendix C).

**Model Architectures & Training**\(M_{C4}\) and \(M_{LL}\) denote the joint embedding models for Connect 4 and Lunar Lander, respectively. The model architectures for \(M_{C4}\) and \(M_{LL}\) leverage LSTMs to learn explanation embedding \(exp^{i}_{}\) for a given \(^{i}_{C_{i}}\). For Connect 4, to learn \(s^{i}_{}\) for \((s_{i},a_{i})\), we leverage Convolutional Neural Networks to learn local and spatial relationships between tokens on a 2D game board. For Lunar Lander, we leverage Fully Connected Networks to learn \(s^{i}_{embed}\). More details (split, learning rate, etc) are in Appendix C.

**Results**

Similar to image-to-text retrieval , we evaluate \(M_{C4}\) and \(M_{LL}\) via recall rate, at \(k=\{1,2,3\}\), which evaluates whether the retrieved \(^{i}_{C_{i}}\) is ranked in the top \(k\). Figure 2(a) provides the average recall rates of \(M_{C4}\) and \(M_{LL}\), across 5 random seeds. We observe that \(M_{LL}\) performs with near 100% accuracy, whereas \(M_{C4}\) has an average recall rate of 88%. Given the lower recall rates by \(M_{C4}\), in Figure 2(b) we examine the false positive and false negative explanation retrievals. We see that \(M_{C4}\) has greatest challenge in correctly retrieving \(^{i}_{C_{i}}\) with "BW" and "3IR_BL". We posit that "BW" and "3IR_BL" may occur in board states with greater variation in comparison to other concepts, leading to higher incorrect retrievals. Additional analyses of \(M_{C4}\) and \(M_{LL}\) are in Appendix C.4.

Note, incorrect retrievals of \(^{i}_{C_{i}}\) for a given \((s_{i},a_{i})\)_can_ have a negative downstream impact within S2E. Specifically, when S2E is leveraged during the RL agent's training, incorrect retrievals of \(^{i}_{C_{i}}\) can incorrectly providing shaping rewards to the agent and in return impacting learned agent policy. Similarly, when S2E is leveraged at deployment to provide end-user understanding, incorrect retrievals of \(^{i}_{C_{i}}\) can confuse end-users and hinder their understanding of the agent's behavior. However, in Section 5.3 our results demonstrate that the percentage of incorrect retrievals from the joint embedding models do not significantly impact the RL agent's learned policy. Similarly, in Section 6, we demonstrate that the percentage of incorrect explanation retrievals are not significantly detrimental to user performance. Nevertheless, it is important that the joint embedding models within S2E have high recall rate, especially when applied in high-stakes or mission critical scenarios.

### Evaluation of \(M\) for Reward Shaping in Agent Training

We validate that both joint embedding models in S2E,\(M_{C4}\) and \(M_{LL}\), inform reward shaping comparable to expert-defined dense, reward functions.

RL Agent & Shaping RewardsOur S2E framework is model agnostic and \(M\) does not make any assumptions on the type of RL model utilized. Given our domains are complex games, we leverage a state-of-the-art (SoTA) RL algorithm MuZero  to evaluate \(M\)'s ability to inform reward shaping. We use the open-source version of MuZero available in  (details in Appendix D.1).

Recall from Section 4 that \(M_{C4}\) and \(M_{LL}\) inform _when_ to provide shaping rewards, but the shaping values are expert determined. For Lunar Lander, there exists a SoTA dense, reward function . In Connect 4, to our knowledge, there is no SoTA dense, reward function. Thus, we perform a hyperparameter sweep to assign shaping values for each concept (values provided in Appendix D.2).

ResultsTo evaluate the efficacy of \(M_{C4}\) and \(M_{LL}\) in informing reward shaping, we measure the agents' _Reward_ and _Win%_. In Lunar Lander, we evaluate \(M_{LL}\)'s ability to inform reward shaping **(MuZero+S2E-RS)** in comparison to a baseline MuZero agent that is informed by an existing expert-defined reward shaping **(MuZero+E-RS)**. Figure 3(b) and Figure 3(c) demonstrate that both the **MuZero+S2E-RS** and **MuZero+E-RS** agents achieve comparable _Reward_ and _Win%_. While performance is comparable between both agents, in **MuZero+S2E-RS**, \(M_{LL}\) provides an automated method to determine _when_ a reward value should be presented, which otherwise has to be manually encoded, such as in **MuZero+E-RS** via an expert-defined function.

In Connect 4, since there does not exist a SoTA reward shaping function, we compare \(M_{C4}\)'s ability to inform reward shaping **(MuZero+S2E-RS)** against a baseline MuZero agent with sparse rewards **(MuZero+No-RS)**. Figure 3(a) shows that **MuZero+S2E-RS** has improved learning rate and requires \(\)200k less training samples to achieve similar _Win%_ in comparison to **MuZero+No-RS**. We also evaluate **MuZero+S2E-RS** against an upper bound in which we provide expert-defined reward shaping **(MuZero+E-RS)** to analyze how incorrect retrievals from \(M_{C4}\) may affect the agent's learning rate. Although \(M_{C4}\) retrieves incorrect explanations 13.9% of times (see Appendix C.4), we observe that **MuZero+S2E-RS**'s learning rate is not significantly affected (Fig. 3(a)).

Overall, these results demonstrate the joint embedding models' ability to effectively inform reward shaping. In Connect 4, we demonstrate that, even with an imperfect joint embedding model, S2E can inform reward shaping and improve the agent's learning rate compared to the SoTA agent trained on sparse rewards. Similarly, in Lunar Lander, we demonstrate that our S2E informs reward shaping comparably to the existing SoTA agent that is trained with an expert-defined dense, reward function.

Figure 4: \(M_{C4}\) improves agent learning rate by \(\)200 training steps compared to SoTA agent without reward shaping (a), while \(M_{LL}\) maintains similar agent learning rate to SoTA agent with expert-defined reward shaping (b and c).

User Evaluation

In this section, we validate S2E with end-users, demonstrating that our retrieved \(_{C_{i}}\) significantly improve user task performance in both Connect 4 and Lunar Lander.

Study ProcedureParticipants performed an online study in which they played several games from the domains in four stages. Specifically, 1) _Practice_ included playing 2 practice games, 2) _Pre-Test_ included playing 3 scored games, 3) _Explanation_ included interacting with an expert player (well-trained RL agent) with exposure to explanations from an assigned study condition, if applicable and 4) _Post-Test_ included playing 3 more scored games. Details about the the study procedures are in Appendix E.1.

MetricsWe measure the difference between participant _Pre-Test_ and _Post-Test Adjusted Task Score (ATS)_ to analyze any task improvement with exposure to the study conditions. Participant _ATS_ is defined by the expert-defined reward functions utilized by the RL agents during training (evaluated in Sec. 5.3, detailed in Appendix D.2). If \(R(s,a)\) denotes the reward associated with a given \((s,a)\) and \(N\) defines the total actions taken by the user in a game, then _ATS_ is defined as a normalized aggregation of user rewards received in a game: \(ATS=_{n}^{N}R(s_{n},a_{n})/N\).

### Connect 4 Study Specifics

Study ConditionsWe conducted a five-way between-subjects study with the following conditions. Participants could receive: 1) **None (Baseline)**: no information about the agent's action to be played, 2) \(_{A}\) **(Baseline)**: action-based explanation that contains the action to be played, 3) \(_{V}\) **(Baseline)**: value-based explanation stating the action to be played along with the action's value in comparison to the values of all other actions for a given state, 4) **GT \(_{C_{i}}\)**: concept-based explanation using expert-defined, ground-truth concepts for a state-action pair, and 5) **S2E \(_{C_{i}}\)**: a concept-based explanation from S2E for a state-action pair. Note, given that Connect 4 has a discrete state space, and actions are sampled at low frequency, the domain does not need further abstracted \(_{C_{i}}\) via InP and TeG. Additionally, \(_{V}\) is similar to the action-cost condition in , as well as the Q-values presented per action in . Note, precondition-based explanations in  require hierarchical domains, and causal-based explanations in  require a learned structured causal model which prevent direct comparison with our framework and domains.

ResultsThe following results are from an IRB-approved study with participants recruited via Amazon Mechanical Turk (n=75, details in Appendix E.2). Our data is analyzed with a one-way ANOVA and a Tukey HSD post-host test, given that the assumptions of homoscedasticity (Levene's Test, p>0.3), and normality (Shapiro-Wilke, p>0.1) are met. Figure 5 shows a significant effect of explanation type on _ATS_ (F(4, 70)=8.56, p<0.001). Specifically, we observe improvement in participant's _ATS_ with exposure to our S2E \(_{C_{i}}\), in comparison to _None_ (t(70) = 3.08, p<0.05), \(_{A}\) (t(70)=3.25, p<0.05) and \(_{V}\) (t(70)=4.03, p<0.01). Additionally, we observe similar _ATS_ improvement when exposed to S2E \(_{C_{i}}\) in comparison to GT \(_{C_{i}}\). These results indicate the benefit of our S2E produced \(_{C_{i}}\) in helping participants better understand Connect 4 and improve their _ATS_.

### Lunar Lander Study Specifics

The underlying physics needed for Lunar Lander results in agent \((s_{i},a_{i})\) being sampled at high frequency and multiple, repetitive concepts being attributed to \((s_{i},a_{i})\) pairs. For example, "decrease velocity" is a valid concept for every \((s_{i},a_{i})\) until the end of a game. Recall from Section 4.3 that \(_{C_{i}}\) may need to be further abstracted in these scenarios to provide meaningful explanations to end-users. Therefore, to evaluate the utility of abstracted concept-based explanations, we introduce 3 more study conditions that utilize our InP and TeG methods outlined in Section 4.3.

Study ConditionsWe perform a seven-way between-subjects user study. Similar to Connect 4, we include _None_, \(_{A}\), \(_{V}\), and **S2E \(_{C_{i}}\)**. Unique to our Lunar Lander study participants

Figure 5: User adjusted task scores (ATS) in Connect 4. Statistical significance: * p < 0.05, ** p < 0.01, and *** p < 0.001; Details in Appendix E.3.

Figure 6: User ATS in Lunar Lander; * p < 0.05, ** p < 0.01

could receive 1) **S2E**\(_{C_{i}}\): a concept-based explanation from S2E with additional abstractions performed using the TeG method, 2) **S2E**\(_{C_{i}}\): a concept-based explanation from S2E with additional abstractions performed using the Inf method, and 3) **S2E**\(_{C_{i}}\). ToG: a concept-based explanation from S2E with additional abstractions performed using both TeG and Inf.

ResultsThe following results are from an IRB-approved study with participants recruited via Amazon Mechanical Turk (n=105, details in Appendix E.2). Our data is analyzed with a one-way ANOVA and a Tukey HSD post-hoc test, given that the assumptions of homoscedasticity (Levene's Test, p>0.7), and normality (Shapiro-Wilke, p>0.1) are met. Figure 6 shows a significant effect of explanation type on participant _ATS_ (F(6,98)=3.67; p<0.01). Specifically, we see significant improvement in participant's _ATS_ with exposure to S2E \(_{C_{i}}\), in comparison to _None_ (t(98) = 3.15, p <0.05)), \(_{A}\) (t(98)=-3.35, p<0.05), \(_{V}\) (t(98)=3.59, p<0.01) and \(_{C_{i}}\) (t(98)=-3.19, p<0.05). These results demonstrate the need of our S2E abstraction methods when providing concept-based explanations to end-users, and that usage of both Inf and TeG in high-frequency RL domains are crucial for _ATS_ improvement.

## 7 Discussion and Conclusions

Our work introduces a unified framework, S2E, that involves learning a joint embedding model between agent state-action pairs and concept-based explanations to provide a dual benefit to the agent and end-user. We additionally outline a desiderata for what may constitute as a "concept" in sequential decision making problems, beyond the scope of prior concept-based explanations for sequential decision making. Our model evaluations demonstrate that the joint embedding model in S2E provides an automatic method for determining when reward shaping should be provided to improve agent learning rates. Our user evaluations demonstrate that concept-based explanations can significantly improve user task performance (Connect 4), but when considering high-frequency RL domains (Lunar Lander), the additional abstraction methods from S2E are important for producing abstracted concept-based explanations that significantly improve user task performance.

Limitations:We present several areas of future work that aims to improve the generalizability of S2E. For instance, the concept-based explanations in our work are derived from mathematical representations and expert knowledge in each domain (see Sec. 5.1). To expand the generalizability of the S2E method to other complex domains, such as Robotics or open-world games, future work should explore how to collect and extract concepts in scenarios where mathematical representations of concepts may be infeasible. A future direction may include collecting expert commentary  or think-aloud  sessions from which concepts are automatically derived. Additionally, in many real-world applications, having access to large amounts of data for a domain may be infeasible. Thus, future work should also explore how to adapt the joint embedding model within S2E for few shot learning. Additionally, for application to high-stakes domain, future work should explore how to remediate incorrect explanation retrievals from the joint embedding model within S2E. While the small percentage of incorrect explanation retrievals do not significantly impact user task performance and agent learning in our tested domains, inaccurate explanation retrievals may have significant effects in mission-critical tasks. It is also important to highlight that our current concept-based explanations follow a fixed template for each unique concept set (see Appendices B.1, B.2). However, there are multiple ways of explaining the same concept, and future work includes learning a joint embedding model with greater language variety in the explanations combined with automatically generating such templates using language models. Furthermore, the Inf method within S2E utilizes manually defined thresholds to produce abstracted concept-based explanations, and future work entails developing an automated Inf method.

Broader Impacts:S2E shows promise in applying the Protege Effect to Human-AI Interaction, and that explanations of agent behavior are beneficial to both agents and users. These insights may promote AI agent architects to consider the utility of self-explaining agents in accelerating learning as well as providing transparency. Additionally, our work shows promise in using concept-based explanations of well-trained AI agents as a teaching tool, helping users improve their task performance. A potential risk of S2E is purposely training a joint embedding model to associate misleading concept-based explanations with state-action pairs. This could lead to a badly performing agent, and the agent providing deceitful explanations of its actions. Future work in explaining black-box AI systems should explore how to detect and prevent deceptive explanations.