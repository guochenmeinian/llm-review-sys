# Double Pessimism is Provably Efficient for

Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage

Jose Blanchet\({}^{1}\)1  Miao Lu\({}^{1}\)1  Tong Zhang\({}^{2}\)1  Han Zhong\({}^{3}\)1

\({}^{1}\) Department of Management Science and Engineering, Stanford University

\({}^{2}\) Department of Mathematics, The Hong Kong University of Science and Technology

\({}^{3}\) Center for Data Science, Peking University

Alphabetical order. Email to miaolu@stanford.edu

###### Abstract

We study distributionally robust offline reinforcement learning (RL), which seeks to find an optimal robust policy purely from an offline dataset that can perform well in perturbed environments. We propose a generic algorithm framework Doubly Pessimistic Model-based Policy Optimization (P\({}^{2}\)MPO) for robust offline RL, which features a novel combination of a flexible model estimation subroutine and a doubly pessimistic policy optimization step. Here the _double pessimism_ principle is crucial to overcome the distribution shift incurred by i) the mismatch between behavior policy and the family of target policies; and ii) the perturbation of the nominal model. Under certain accuracy assumptions on the model estimation subroutine, we show that P\({}^{2}\)MPO is provably sample-efficient with _robust partial coverage data_, which means that the offline dataset has good coverage of the distributions induced by the optimal robust policy and perturbed models around the nominal model. By tailoring specific model estimation subroutines for concrete examples including tabular Robust Markov Decision Process (RMDP), factored RMDP, and RMDP with kernel and neural function approximations, we show that P\({}^{2}\)MPO enjoys a \(}(n^{-1/2})\) convergence rate, where \(n\) is the number of trajectories in the offline dataset. Notably, these models, except for the tabular case, are first identified and proven tractable by this paper. To the best of our knowledge, we first propose a general learning principle -- double pessimism -- for robust offline RL and show that it is provably efficient in the context of general function approximations.

## 1 Introduction

Reinforcement learning (RL)  aims to learn an optimal policy that maximizes the cumulative rewards received in an unknown environment. Typically, deep RL algorithms learn a policy in an online trial-and-error fashion using millions to billions of data. However, data collection could be costly and risky in some practical applications such as healthcare  and autonomous driving . To tackle this challenge, offline RL (also known as batch RL)  learns a near-optimal policy based on a dataset collected a priori without further interactions with the environment. Although there has been great progress in offline RL , these works implicitly require that the offline dataset is generated by the real-world environment, which may fail in practice. Taking robotics  as an example, the experimenter trains agents in a simulated physical environment and then deploy them in real-world environments. Since the experimenter does not have access to the true physical environment, there is a mismatch between the simulated environment used to generate the offline dataset and the real-world environment used to deploy the agents. Such a mismatch iscommonly referred to as the _sim-to-real gap_[42; 76]. Since the optimal policy is sensitive to the model [31; 9], the potential sim-to-real gap may lead to the poor performance of RL algorithms.

A promising solution to remedy this issue is robust RL [13; 9; 32] - training a robust policy that performs well in a bad or even adversarial environment. A line of work on deep robust RL [43; 44; 41; 30; 53; 75; 19] demonstrates the superiority of the trained robust policy in real world environments. Furthermore, the recent work of Hu et al.  theoretically proves that the ideal robust policy can attain near optimality in dealing with problems with sim-to-real gap, but this work does not suggest how to learn a robust policy from a theoretical perspective. In order to understand robust RL from the theoretical side, robust Markov decision process (RMDP) [13; 9] has been proposed, and many recent works [77; 47; 29] design sample-efficient learning algorithms for robust offline RL. These works mainly focus on the tabular case, which is not capable of tackling large state spaces. Meanwhile, in the non-robust setting, a line of works [16; 54; 62; 73; 45] show that "pessim" is the general learning principle for designing algorithms that can overcome the distributional shift problem faced by offline RL. In particular, in the context of function approximation, Xie et al.  and Uehara and Sun  leverage the pessimism principle and propose generic algorithms in the model-free and model-based fashion, respectively. Hence, it is natural to ask the following questions:

**Q1:** What is the general learning principle for robust offline RL?

**Q2:** Based on this learning principle, can we design a generic algorithm for robust offline RL in the context of function approximation?

To answer these two questions, we need to tackle the following two intertwined challenges: (i) distributional shift, that is, the mismatch between offline data distribution and the distribution induced by the optimal robust policy. In robust offline RL, the distributional shift has two sources - behavior policy and perturbed model, where the latter is the unique challenge not presented in non-robust RL; and (ii) function approximation. Existing works mainly focus on the tabular case, and it remains elusive how to add reasonable structure conditions to make RMDPs with large state spaces tractable. Despite these challenges, we answer the aforementioned two questions affirmatively.

**Contributions.** We study robust offline RL in a general framework, which not only includes existing known tractable \(\)-rectangular tabular RMDPs, but also subsumes several newly proposed models (e.g., \(\)-rectangular factored RMDPs, \(\)-rectangular kernel RMDPs, and \(\)-rectangular neural RMDPs) as special cases. Under this general framework, we propose a generic model-based algorithm, dubbed as Doubly Pessimistic Model-based Policy Optimization (P\({}^{2}\)MP0), which consists of a model estimation subroutine and a policy optimization step based on _doubly pessimistic_ value estimators. We note that the model estimation subroutine can be flexibly chosen according to structural conditions of specific RMDP examples. Meanwhile, the adoption of doubly pessimistic value estimators in the face of model estimation uncertainty and environment uncertainty plays a key role in overcoming the distributional shift problem in robust offline RL.

From the theoretical perspective, we characterize the optimality of P\({}^{2}\)MP0 with partial coverage. In particular, we show that the suboptimality gap of P\({}^{2}\)MP0 is upper bounded by the model estimation error (see Condition 3.2) and the robust partial coverage coefficient (see Assumption 3.3). For concrete examples of RMDPs, by customizing specific model estimation mechanisms and plugging them into P\({}^{2}\)MP0, we show that P\({}^{2}\)MP0 enjoys a \(n^{-1/2}\) convergence rate with robust partial coverage data, where \(n\) is the number of trajectories in the offline dataset. In summary, we identify a general learning principle -- _double pessimism_ -- for robust offline RL. Based on this principle, we can perform sample-efficient robust offline RL with robust partial coverage data via general function approximation. See Table 1 for a summary of our results and a comparison with mostly related works.

### Related Works

**Robust reinforcement learning in robust Markov decision processes.** Robust RL is usually modeled as a robust MDP (RMDP) [13; 9], and its planning has been well studied [13; 9; 65; 60; 57]. Recently, robust RL in RMDPs has attracted considerable attention, and a growing body of works studies this problem in the generative model [68; 39; 49; 58; 69; 66; 7], online setting [59; 3; 8], and offline setting [77; 40; 47; 29]. Our work focuses on robust offline RL, and we provide a more in-depth comparison with Zhou et al. , Shi and Chi , Ma et al.  as follows. Under the full coverage condition (a uniformly lower bounded data distribution), Zhou et al.  provide the first sample-efficient algorithm for \(\)-rectangular tabular RMDPs. After, Shi and Chi leverage the pessimism principle and design a sample-efficient offline algorithm that only requires robust partial coverage data for \(\)-rectangular tabular RMDPs. Ma et al.  propose a new \(d\)-rectangular RMDP and develop a pessimistic style algorithm that can find a near-optimal robust policy with partial coverage data. In comparison, we provide a generic algorithm that can not only solve the models in Zhou et al. , Shi and Chi , Ma et al. , but can also tackle various newly proposed RMDP models such as \(\)-rectangular factored RMDP, \(\)-rectangular kernel RMDP, and \(\)-rectangular neural RMDP. See Table 1 for a summary. Moreover, we propose a new pessimistic type learning principle "double pessimism" for robust offline RL. Although Shi et al.  and Ma et al.  adopt the similar algorithmic idea in tabular or linear settings, neither of them have identified a general learning principle for robust offline RL in the regime of large state space.

**Non-robust offline RL and pessimism principle.** The line of works on offline RL aims to design efficient learning algorithms that find an optimal policy given an offline dataset collected a priori. Prior works  typically require a dataset of full coverage, which assumes that the offline data have good coverage of all state-action pairs. In order to avoid such a strong coverage condition on data, the _pessimism_ principle - being conservative in policy or value estimation of those state-action pairs that are not sufficiently covered by data - has been proposed. Based on this principle, a long line of works [see e.g., 16, 54, 62, 63, 45, 73, 71, 64, 48, 24, 26, 74, 28, 46] propose algorithms that can learn the optimal policy only with the _partial coverage data_. The partial coverage data only requires to cover the state-action pairs visited by the optimal policy. Among these works, our work is mostly related to the work of Uehara and Sun , which proposes a generic model-based algorithm for non-robust offline RL. Our algorithm for robust offline RL is also in a model-based fashion, and our study covers some models such as \(\)-rectangular kernel and neural RMDPs whose non-robust counterparts are not studied by Uehara and Sun . More importantly, our algorithm is based on a newly proposed _double pessimism_ principle, which is tailored for robust offline RL and is in parallel with the pessimism principle used in non-robust offline RL. Also, we show that the performance of our proposed algorithm depends on the notion of _robust partial coverage coefficient_, which is also different from the notions of partial coverage coefficient in previous non-robust offline RL works .

### Notations

For any set \(A\), we use \(2^{A}\) to denote the collection of all the subsets of \(A\). For any measurable space \(\), we use \(()\) to denote the collection of probability measures over \(\). For any integer \(n\), we use \([n]\) to denote the set \(\{1,,n\}\). Throughout the paper, we use \(D(\|)\) to denote a (pseudo-)distance between two probability measures (or densities). In specific, we define the KL-divergence \(D_{}(p\|q)\) between two probability densities \(p\) and \(q\) over \(\) as

\[D_{}(p\|q)=_{}p(x)() x,\]

and we define the TV-distance \(D_{}(p\|q)\) between two probability densities \(p\) and \(q\) over \(\) as

\[D_{}(p\|q)=_{}|q(x)-p(x)|\,x.\]

Given a function class \(\) equipped with some norm \(\|\|_{}\), we denote by \(_{\|}(,,\|\|_{})\) the \(\)-bracket number of \(\), and \((,,\|\|_{})\) the \(\)-covering number of \(\).

   & Zhou et al.  & Shi and Chi  & Ma et al.  & This Work \\  \(\)-rectangular tabular RMDP & \(\) [FOOTNOTE:]Footnote : footnotemark: [ENDFOOTNOTE] & ✓ & ✗ & ✓ \\  \(d\)-rectangular linear RMDP & ✗ & ✗ & ✓ & ✓ \\  \(\)-rectangular factored RMDP & ✗ & ✗ & ✗ & ✓ \\  \(\)-rectangular kernel RMDP & ✗ & ✗ & ✗ & ✓ \\  \(\)-rectangular neural RMDP & ✗ & ✗ & ✗ & ✓ \\  

Table 1: A comparison with closely related works on robust offline RL. \(\) means the work can tackle this model with robust partial coverage data, \(\)! means the work requires full coverage data to solve the model, and � means the work cannot tackle the model. Lightblue color denotes the models that are first proposed and proved tractable in this work.

## 2 Preliminaries

### A Unified Framework of Robust Markov Decision Processes

We introduce a unified framework for studying episodic robust Markov decision processes (RMDP), which we denote as a tuple \((,,H,P^{},R,_{},)\). The set \(\) is the state space with possibly infinite cardinality, \(\) is the action space with finite cardinality. The integer \(H\) is the length of each episode. The set \(P^{}=\{P^{}_{h}\}_{h=1}^{H}\) is the collection of transition kernels where each \(P^{}_{h}:()\), and \(R=\{R_{h}\}_{h=1}^{H}\) is the collection of reward functions where each \(R_{h}:\). We use \(()\) to note the probability simplex on \(\) (i.e. the space of probability measures with support on \(\)).

We consider a model-based perspective of reinforcement learning, and we denote \(=\{P(|,):( )\}\) as the space of all transition kernels. The space \(_{}\) of the RMDP is a realizable model space which contains the transition kernel \(P^{}\), i.e., \(P^{}_{h}_{}\) for any step \(h[H]\). Finally, the RMDP is equipped with a mapping \(:_{} 2^{}\) that characterizes the _robust set_ of any transition kernel in \(_{}\). Formally, for any transition kernel \(P_{}\), we call \((P)\) the _robust set_ of \(P\). One can interpret the transition kernel \(P^{}_{h}_{}\) as the transition kernel of the training environment, while \((P^{}_{h})\) contains all the possible transition kernels of the test environment.

**Remark 2.1**.: _The mapping \(\) is defined on the realizable model space \(_{}\), while for generality we allow the image of \(\) to be outside of \(_{}\). That is, a \((P)\) for some \(P_{}\) might be in \(^{}_{}\)._

**Policy and robust value function.** Given an RMDP \((,,H,P^{},R,_{},)\), we consider using a Markovian policy to make decision. A Markovian policy \(\) is defined as \(=\{_{h}\}_{h=1}^{H}\) with \(_{h}:()\) for each step \(h[H]\). For simplicity, we use _policy_ to refer to a Markovian policy.

Given any policy \(\), we define the _robust value function_ of \(\) with respect to any set of transition kernels \(P=\{P_{h}\}_{h=1}^{H}_{}\) as the following, for each step \(h[H]\),

\[V^{}_{h,P,}(s) =_{_{h}(P_{h}),1 h H}V^{ }_{h}(s;\{_{h}\}_{h=1}^{H}), s,\] (2.1) \[Q^{}_{h,P,}(s,a) =_{_{h}(P_{h}),1 h H}Q^{ }_{h}(s,a;\{_{h}\}_{h=1}^{H}),(s,a) .\] (2.2)

Here \(V^{}_{h}(;\{_{h}\}_{h=1}^{H})\) and \(Q^{}_{h}(;\{_{h}\}_{h=1}^{H})\) are the _state-value function_ and the _action-value function_ of policy \(\) in the standard episodic MDP \((,,H,\{_{h}\}_{h=1}^{H},R)\),

\[V^{}_{h}(s;\{_{h}\}_{h=1}^{H}) =_{\{_{h}\}_{h=1}^{H},}[_{i=h}^{ H}R_{i}(s_{i},a_{i})s_{h}=s], s,\] (2.3) \[Q^{}_{h}(s,a;\{_{h}\}_{h=1}^{H}) =_{\{_{h}\}_{h=1}^{H},}[_{i=h}^{ H}R_{i}(s_{i},a_{i})s_{h}=s,a_{h}=a],(s,a) ,\] (2.4)

where the expectation \(_{\{_{h}\}_{h=1}^{H},}[]\) is taken with respect to the trajectories induced by the transition kernel \(\{_{h}\}_{h=1}^{H}\) and the policy \(\). Intuitively, the robust value function of a policy \(\) given transition kernel \(P\) is defined as the least expected cumulative reward achieved by \(\) when the transition kernel varies in the robust set of \(P\). This is how an RMDP takes the perturbed models into consideration.

\(\)**-rectangular robust set and robust Bellman equation.** Ideally, we would like to consider robust value function that has recursive expressions, just like the Bellman equation satisfied by (2.3) in a standard MDP . To this end, we focus on a generally adopted kind of robust set in our unified framework, which is called the \(\)_-rectangular robust set_.

**Assumption 2.2** (\(\)-rectangular robust set).: _We assume that the mapping \(\) induces \(\)-rectangular robust sets. Specifically, the mapping \(\) satisfies, for \( P_{}\),_

\[(P)=_{(s,a)}_{ }(s,a;P),_{}(s,a;P)=\{() ():D(()\|P(|s,a))\},\]

_for some (pseudo-) distance \(D(\|)\) on \(()\) and some \(_{+}\). Intuitively, \(\)-rectangular requires that \((P)\) gives decoupled robust sets for \(P(|s,a)\) across different \((s,a)\)-pairs. The (pseudo-)distance \(D(\|)\) can be chosen as a \(\)-divergence [68; 10] or a \(p\)-Wasserstein-distance ._Thanks to the \(\)-rectangular assumption on the mapping \(\), the robust value functions (2.1) of any policy \(\) then satisfy a recursive expression, which is called robust Bellman equation .

**Proposition 2.3** (Robust Bellman equation).: _Under Assumption 2.2, for any \(P=\{P_{h}\}_{h=1}^{H}\) where \(P_{h}_{}\) and any \(=\{_{h}\}_{h=1}^{H}\) with \(_{h}:()\), the following robust Bellman equation holds,_

\[V_{h,P,}^{}(s) =_{a_{h}(|s)}[Q_{h,P,}^{}(s,a)], s,\] (2.5) \[Q_{h,P,}^{}(s,a) =R_{h}(s,a)+_{_{h}(P_{h})}_{s^ {}_{h}(|s,a)}[V_{h+1,P,}^{}(s^{ })],(s,a).\] (2.6)

To be self-contained, in Appendix B we provide a detailed proof of the robust Bellman equation in our framework under Assumption 2.2. Equation (2.5) actually says that the infimum over all the transition kernels (recall the definition of \(V_{h,P,}^{}\) in (2.1)) can be decomposed into a "one-step" infimum over the current transition kernel, i.e., \(_{_{h}(P_{h})}\), and an infimum over the future transition kernels, i.e., \(V_{h+1,P,}^{}\). Such a property is crucial to the algorithmic design and theoretical analysis for RMDPs.

### Examples of Robust Markov Decision Processes

In this section, we give concrete examples for the general RMDP framework proposed in Section 2.1. Most existing works on RMDPs hinge on the finiteness assumption on the state space, which fails to deal with prohibitively large or even infinite state space. In our framework, RMDPs can be considered in the paradigm of infinite state space, for which we adopt various powerful function approximation tools including kernel and neural functions. Also, we introduce a new type of RMDP named robust factored MDP, which is a robust extension of standard factored MDPs .

**Remark 2.4**.: _Besides \(\)-rectangular-type robust sets (Assumption 2.2), our unified framework of RMDP can also cover other types of robust sets considered in some previous works as special cases, including \(\)-rectangular robust set  and \(d\)-rectangular robust set for linear MDPs . See Section A for a discussion about these two types of robust sets._

In the sequel, we introduce concrete examples of our framework of RMDP.

**Example 2.5** (\(\)-rectangular robust tabular MDP).: _When the state space \(\) is a finite set, we call the corresponding model an \(\)-rectangular robust tabular MDP. Recently, there is a line of works on the \(\)-rectangular robust tabular MDP . For \(\)-rectangular robust tabular MDPs, we choose \(_{}=\) containing all possible transitions._

**Remark 2.6**.: _We point out that our framework of RMDP under \(\)-rectangular assumption covers substantially more model than \(\)-rectangular robust tabular MDP since our state space \(\) can be infinite. The model space \(_{}\) can be adapted to function approximation methods to handle the infinite state space. Thus any efficient algorithm developed for our framework of RMDPs **can not** be covered by algorithms for \(\)-rectangular robust tabular MDPs. Example 2.7 and 2.8 are infinite state space \(\)-rectangular robust MDPs with function approximations._

**Example 2.7** (\(\)-rectangular robust MDP with kernel function approximations).: _We consider an infinite state space \(\)-rectangular robust MDP whose realizable model space \(_{}\) is in a reproduced kernel Hilbert space (RKHS). Let \(\) be a RKHS associated with a positive definite kernel \(:()( )_{+}\) (See Appendix D.3.1 for a review of the basics of RKHS). We denote the feature mapping of \(\) by \(:\). With \(\), an \(\)-rectangular robust MDP with kernel function approximation is defined as an \(\)-rectangular robust MDP with_

\[_{}=P(s^{}|s,a)=(s,a,s^{ }),_{}:,\|f\|_{}  B_{}},\] (2.7)

_for some \(B_{}>0\). Here we implicitly identify \(P(|,)\) as the density of the corresponding distribution with respect to a proper base measure on \(\)._

**Example 2.8** (\(\)-rectangular robust MDP with neural function approximations).: _We consider an infinite state space \(\)-rectangular robust MDP whose realizable model space \(_{}\) is parameterized by an overparameterized neural network. We first define a two-layer fully-connected neural network on some \(^{d_{}}\) as_

\[(;,)=}_{j=1}^{m} a_{j}(^{}_{j}),,\] (2.8)_where \(m_{+}\) is the number of hidden units of \(\), \((,)\) is the parameters given by \(=(_{1},,_{m})^{d m}\), \(=(a_{1},,a_{m})^{}^{m}\), and \(()\) is the activation function. Now we assume that the state space \(^{d_{}}\) for some \(d_{}_{+}\). Also, we identify actions via one-hot vectors in \(^{||}\), i.e., we represent \(a\) by \((0,,0,1,0,,0)\) with \(1\) in the \(a\)-th coordinate. Let \(=\) with \(d_{}=2d_{}+||\). Then an \(\)-rectangular robust MDP with neural function approximation is defined as an \(\)-rectangular robust MDP with \(_{}\) given by_

\[_{}=\{P(s^{}|s,a)=((s,a,s^{ });,^{0}):\|-^{0}\|_{2} B_{ }\},\] (2.9)

_for some \(B_{}>0\) and some fixed \((^{0},^{0})\) which can be interpreted as the initialization. We refer to Appendix D.4.1 for more details about neural function approximations and analysis techniques._

**Example 2.9** (\(\)-rectangular robust factored MDP).: _We consider a factored MDP equipped with \(\)-rectangular factored robust set. A standard factored MDP  is defined as follows. Let \(d\) be an integer and \(\) be a finite set. The state space \(\) is factored as \(=^{d}\). For each \(i[d]\), \(s[i]\) is the \(i\)-coordinate of \(s\) and it is only influenced by \(s[_{i}]\), where \(_{i}[d]\). That is, the transition of a factored MDP can be factorized as_

\[P_{h}^{}(s^{}|s,a)=_{i=1}^{d}P_{h,i}^{}(s^{}[i]|s[ _{i}],a).\]

_Here we let the realizable model space \(_{}\) consist of all the factored transition kernels, i.e.,_

\[_{}=\{P(s^{}|s,a)=_{i=1}^{d}P_{i}(s^{ }[i]|s[_{i}],a)\,:\,P_{i}:[_{i}] (), i[d]\}.\]

_For an \(\)-rectangular robust factored MDP, we define \(\) as, for any transition kernel \(P(s^{}|s,a)=_{i=1}^{d}P_{i}(s^{}[i]|s[_{i}],a) _{}\), \((P)=_{(s,a)} _{,}(s,a;P)\), with_

\[_{,}(s,a;P)=\{_{i=1}^{d}_{i }():_{i}()(),D(_{i}( )\|P_{i}(|s[_{i}],a))_{i}, i[d]\}.\]

_for some (pseudo-)distance \(D\) on \(()\) and some positive real numbers \(\{_{i}\}_{i=1}^{d}\)._

### Offline Reinforcement Learning in Robust Markov Decision Processes

In this section, we define the offline RL protocol in a RMDP \((,,H,P^{},R,_{},)\). The learner is given the realizable model space \(_{}\) and the robust mapping \(\), but the learner doesn't know the transition kernel \(P^{}\). For simplicity, we assume that the learner knows the reward function \(R^{2}\).

**Offline dataset generation.** We assume that the learner is given an offline dataset \(\) that consists of \(n\) i.i.d. trajectories generated from the standard MDP \((,,H,P^{},R)\) using some behavior policy \(^{}\). For each \([n]\), the trajectory has the form of \(\{(s_{h}^{},a_{h}^{},r_{h}^{})\}_{h=1}^{H}\), satisfying that \(a_{h}^{}_{h}^{}(|s_{h}^{})\), \(r_{h}^{}=R_{h}(s_{h}^{},a_{h}^{})\), and \(s_{h+1}^{} P_{h}^{}(|s_{h}^{},a_{h}^{})\) for each step \(h[H]\).

Given transition kernels \(P=\{P_{h}\}_{h=1}^{H}\) and a policy \(\), we use \(d_{P,h}^{}(,)\) to denote the state-action visitation distribution at step \(h\) when following policy \(\) and transition kernel \(P\). With this notation, the distribution of \((s_{h}^{},a_{h}^{})\) can be written as \(d_{P^{},h}^{^{}}\) or simply \(d_{P^{},h}^{}\), for each \([n]\) and \(h[H]\). We also use \(d_{P^{},h}^{^{}}()\) to denote the marginal distribution of state at step \(h\) when there is no confusion.

**Learning objective.** In offline robust RL, the goal is to learn the policy \(^{}\) from the offline dataset \(\) which maximizes the robust value function \(V_{1,P^{},}^{}\), that is,

\[^{}=*{\!}_{}V_{1,P^{},}^ {}(s_{1}), s_{1},\] (2.10)

where \(=\{=\{_{h}\}_{h=1}^{H}\,|\,_{h}:()\}\) denotes the collection of all Markovian policies. In view of (2.10), we call \(^{}\) the _optimal robust policy_. Equivalently, we want to learn a policy \(\) which minimizes the suboptimality gap between \(\) and \(^{}\), defined as3

\[(;s_{1}):=V_{1,P^{},}^{^{}}( s_{1})-V_{1,P^{},}^{}(s_{1}), s_{1} .\] (2.11)```
1:Input: model space \(_{}\), mapping \(\), dataset \(\), policy class \(\), algorithm ModelEst.
2:Model estimation step:
3:Obtain a confidence region \(}=(,_{})\).
4:Doubly pessimistic policy optimization step:
5:Set policy \(\) as \(*{argsup}_{}J_{^{2}}()\), where \(J_{^{2}}()\) is defined in (3.1).
6:Output: \(=\{_{h}\}_{h=1}^{H}\). ```

**Algorithm 1** Doubly Pessimistic Model-based Policy Optimization (P\({}^{2}\)MPO)

## 3 Algorithm: Generic Framework and Unified Theory

In this section, we propose Doubly Pessimistic Model-based Policy Optimization (P\({}^{2}\)MPO) algorithm to solve offline RL in the RMDP framework we introduce in Section 2.1, and we establish a unified theoretical guarantee for P\({}^{2}\)MPO. Our proposed algorithm and theory show that _double pessimism_ is a general principle for designing efficient algorithms for offline robust RL. The algorithm features three key points: i) learning the optimal robust policy \(^{}\) approximately; ii) requiring only a partial coverage property of the offline dataset \(\); iii) able to handle infinite state space via function approximations.

We first introduce our proposed algorithm framework P\({}^{2}\)MPO in Section 3.1. Then we establish a unified analysis for P\({}^{2}\)MPO in Section 3.2. Our algorithm framework can be specified to solve all the concrete examples of RMDP we introduce in Section 2.2, which we show in Section 4.

### Algorithm Framework: P\({}^{2}\)MPO

The P\({}^{2}\)MPO algorithm framework (Algorithm 1) consists of a _model estimation step_ and a _doubly pessimistic policy optimization step_, which we introduce in the following respectively.

**Model estimation step (Line 3).** The P\({}^{2}\)MPO algorithm framework first constructs an estimation of the transition kernels \(P^{}=\{P^{}_{h}\}_{h=1}^{H}\), i.e., it estimates the dynamic of the training environment. It implements a sub-algorithm ModelEst\((,_{})\) that returns a confidence region \(}\) for \(P^{}=\{P^{}_{h}\}_{h=1}^{H}\). Specifically, \(}=\{}_{h}\}_{h=1}^{H}\) with \(}_{h}_{}\) for each step \(h[H]\).

The sub-algorithm ModelEst can be tailored to specific RMDPs. We refer to Section 4 for detailed implementations of ModelEst for different examples of RMDPs introduced in Section 2.2. Ideally, to ensure sample-efficient learning, we need \(}=(,_{})\) to satisfy: i) the transition kernels \(P^{}=\{P^{}_{h}\}_{h=1}^{H}\) are contained in \(}=\{}_{h}\}_{h=1}^{H}\); ii) each transition kernel \(P_{h}}_{h}\) enjoys a small "robust estimation error" which is highly related to the robust Bellman equation in (2.5). We quantify these two conditions of \(}\) for sample-efficient learning in Section 3.2.

**Doubly pessimistic policy optimization step (Line 5).** After **model estimation step**, P\({}^{2}\)MPO performs policy optimization to find the optimal robust policy. To learn the optimal robust policy in the face of uncertainty, P\({}^{2}\)MPO adopts a _double pessimism_ principle. To explain, this general principle has two sources of pessimism: i) pessimism in the face of data uncertainty; ii) pessimism to find a robust policy. Specifically, for any policy, we first estimate its robust value function via two infimums, where one is an infimum over the confidence set constructed in the model estimation step, and one is an infimum over the robust sets. Formally, for any \(\), we define the doubly pessimistic estimator

\[J_{^{2}}()=_{P_{h}}_{h},1 h  H}_{_{h}(P_{h}),1 h H}V_{1}^{}(s_{1 };\{_{h}\}_{h=1}^{H}),\] (3.1)

where \(V_{1}^{}\) is the standard value function of policy \(\) defined in (2.3). Then P\({}^{2}\)MPO outputs the policy \(\) that maximizes the doubly pessimistic estimator \(J_{^{2}}()\) defined in (3.1), i.e.,

\[=*{argsup}_{}J_{^{2}}().\] (3.2)

The novelty of the doubly pessimistic policy optimization step is performing pessimism from the two sources (data uncertainty and robust optimization) simultaneously. Compared with the previous works on standard offline RL in MDPs [62; 54] and offline RL in RMDPs without pessimism [68; 77; 40], they only contain one source of pessimism in algorithm design, contrasting with our algorithm.

We note that a recent work  also studied robust offline RL in \(\)-rectangular robust tabular MDPs (Example 2.5) using pessimism techniques. Compared with our double pessimism principle,their algorithm performs pessimism in face of data uncertainty i) depending on the tabular structure of the model since a point-wise pessimism penalty is needed and ii) depending on the specific form of the robust set \((P)\), which makes it difficult to adapt to the infinite state space case with general function approximations and general types of robust set \((P)\).

### Unified Theoretical Analysis

In this section, we establish a unified theoretical analysis for the \(^{2}\) algorithm framework proposed in Section 3.1. We first specify the two conditions that the **model estimation step** of \(^{2}\) should satisfy in order for sample-efficient learning. Then we establish an upper bound of suboptimality of the policy obtained by \(^{2}\) given that these two conditions are satisfied. In Section 4, we show that the specific implementations of the sub-algorithm ModelEst for the RMDPs examples in Section 2.2 satisfy these two conditions, which results in tailored suboptimality bounds for these examples.

**Conditions.** The two conditions on the **model estimation step** are given by the following.

**Condition 3.1** (\(\)-accuracy).: With probability at least \(1-\), it holds that \(P_{h}^{*}}_{h}\) for any \(h[H]\).

**Condition 3.2** (\(\)-model estimation error).: With probability at least \(1-\), it holds that

\[_{(s,a) d_{P^{*},h}^{}}[(_{ _{h}(P_{h})}}_{h}(V_{h+1,P, }^{*})(s,a)-_{_{h}(P_{h}^{*})} }_{h}(V_{h+1,P,}^{*})(s,a))^{2}] _{h}^{}(n,).\]

for any \(P=\{P_{h}\}_{h=1}^{H}\) with \(P_{h}}_{h}\). Here \(}_{h}(V_{h+1,P,}^{*})(s,a)=_{s^{ }_{h}(|s,a)}[V_{h+1,P,}^{*^{}}( s^{})]\)

Condition 3.1 requires that the confidence region \(}_{h}\) contains the transition kernel of the training environment \(P_{h}^{}\) with high probability. Condition 3.2 requires that each transition kernel \(P_{h}}_{h}\) induces an error from \(P_{h}^{}\) less than certain quantity \(_{h}^{}(n,)\), where the error is adapted from the robust Bellman equation (2.5) and involves an infimum over the robust set of \(P_{h}\) and \(P_{h}^{}\). In specific implementations of ModelEst for RMDP examples in Section 4, we show that the quantity \(_{h}^{}(n,)\) generally scales with \(}(n^{-1})\), where \(n\) is the number of trajectories in the offline dataset.

**Suboptimality analysis.** Now we establish a unified suboptimality bound for the \(^{2}\) algorithm framework. Thanks to the double pessimism principle of \(^{2}\), we can prove a suboptimality bound while only making a mild _robust partial coverage assumption_ on the dataset.

**Assumption 3.3** (Robust partial coverage).: _We assume that_

\[C_{P^{},}^{}:=_{1 h H}_{P=\{P_{h}\}_{ h=1}^{H},P_{h}(P_{h}^{})}_{(s,a) d_{P^{*},h}^{ }}[(^{}(s,a)}{d_{P^{*},h}^{}(s,a) })^{2}]<+,\]

_and we call \(C_{P^{},}^{}\) the robust partial coverage coefficient._

To interpret, Assumption 3.3 only requires that the dataset covers the visitation distribution of the optimal policy \(^{}\), but in a robust fashion since \(C_{P^{},}^{}\) considers all possible transition kernels in the robust set \((P^{})\). The robust consideration in \(C_{P^{},}^{}\) is because in RMDPs the policies are all evaluated in a robust way. This partial-coverage-style assumption is much weaker than full-coverage-style assumptions  which require either a uniformly lower bounded dataset distribution or covering the visitation distribution of any \(\). For \(\)-rectangular robust tabular MDPs (Example 2.5), the robust partial coverage coefficient \(C_{P^{},}^{}\) is similar with the partial coverage coefficient proposed by  who studied tabular RMDPs under partial coverage. We highlight that beyond \(\)-rectangular robust tabular MDPs, our robust partial coverage assumption can handle other examples of RMDPs (Section 2.2) under our unified theory.

Our main result is the following theorem. See Appendix C for a detailed proof.

**Theorem 3.4** (Suboptimality of \(^{2}\)).: _Under Assumptions 2.2 and 3.3, suppose that Algorithm 1 implements a sub-algorithm that satisfies Conditions 3.1 and 3.2, then with probability at least \(1-2\),_

\[(;s_{1}),} ^{}}_{h=1}^{H}_{h}^{}(n, )}.\]

When \(_{h}^{}(n,)\) achieves a rate of \(}(n^{-1})\), then \(^{2}\) enjoys a \(}(n^{-1/2})\)-suboptimality. In the following Section 4, we give specific implementations of the model estimation step of \(^{2}\) for each example of RMDP in Section 2. The implementations will make Conditions 3.1 and 3.2 satisfied and thus specify the unified result Theorem 3.4.

## 4 Implementations of P\({}^{2}\)MPO for Examples of RMDPs

In this section, we provide concrete implementations of the ModelEst sub-algorithm in P\({}^{2}\)MPO (Algorithm 1). In Section 4.1, we implement ModelEst for all the RMDPs that satisfy Assumption 2.2, and we specify the suboptimality bounds in Theorem 3.4 to Examples 2.5, 2.7, 2.8 in Section 2.2. In Section 4.2, we implement ModelEst for \(\)-rectangular robust factored MDPs (Example 2.9) and specify Theorem 3.4 to this example.

### Model Estimation for General RMDPs with \(\)-rectangular Robust Sets

Using the offline data \(\), we first construct the _maximum likelihood estimator_ (MLE) of the transition kernel \(P^{}\). Specifically, for each step \(h[H]\), we define

\[_{h}=_{}}{}_{=1}^{n} P(s_{h+1}^{}|s_{h}^{},a_{h}^ {}).\] (4.1)

After, we construct a confidence region for the MLE estimator, denoted by \(}\). Specifically, \(}\) contains all transitions which have a small total variance distance from \(\). For each step \(h[H]\), we define

\[}_{h}=P_{}: _{=1}^{n}\|_{h}(|s_{h}^{},a_{h}^{})-P(|s_ {h}^{},a_{h}^{})\|_{1}^{2}}.\] (4.2)

Here \(>0\) is a tuning parameter that controls the size of the confidence region \(}_{h}\). Finally, we set \((,_{})=}= \{}_{h}\}_{h=1}^{H}\) with \(}_{h}\) given in (4.2). In the sequel, we mainly consider the distance \(D(\|)\) in Assumption 2.2 to be KL-divergence and TV-distance. The following corollary specifies Theorem 3.4 to model estimation step given by (4.2). See Appendix D for a detailed proof.

**Corollary 4.1** (Suboptimality of P\({}^{2}\)MPO: \(\)-rectangular robust MDP).: _Under Assumption 2.2, 3.3, setting the tuning parameter \(\) as_

\[=(C_{2}HN_{}(1/n^{2},_{},\| \|_{1,})/)}{n},\]

_for some constants \(C_{1},C_{2}>0\), \(^{2}\)MPO with model estimation step given by (4.2) satisfies that \(\) when \(D(\|)\) is KL-divergence and Assumption D.3 holds with parameter \(\), then with probability at least \(1-2\),_

\[(;s_{1}),\,}^{ }}H^{2}(H/)}{}^{ }(C_{2}^{}HN_{}(1/n^{2},_{},\|\|_ {1,})/)}{n}}.\]

\(\) _when \(D(\|)\) is TV-divergence, then with probability at least \(1-2\),_

\[(;s_{1}),\,}^{}} H^{2}^{}(C_{2}^{}HN_{}(1/n^{2}, _{},\|\|_{1,})/)}{n}}.\]

\(\)**-rectangular robust tabular MDP (Example 2.5)**.When \(\) is finite as in Example 2.5, the MLE estimator (4.1) coincides the empirical estimator

\[_{h}(s^{}|s,a)=^{n}\{s_{h}^{ }=s,a_{h}^{}=a,s_{h+1}^{}=s^{}\}}{1_{=1}^{n}\{s _{h}^{}=s,a_{h}^{}=a\}},\] (4.3)

which is adopted by [77; 68; 39; 40]. Furthermore, in Example 2.5, the realizable model space \(_{}=\{P:( )\}\). When \(\) and \(\) are finite, we can bound the bracket number of \(_{}\) as

\[_{}(1/n^{2},_{},\|\|_{1, }) 2||^{2}||(n).\] (4.4)

Combining (4.4) and Corollary 4.1, we can conclude that: i) under TV-distance the suboptimality of P\({}^{2}\)MPO for \(\)-rectangular robust tabular RMDP is given by \((H^{2},\,}^{}||^{2}||(nH/)/n)}\), ii) under KL-divergence the suboptimality of P\({}^{2}\)MPO for \(\)-rectangular robust tabular MDP is given by \((H^{2}(H/)/,\, }^{}||^{2}||(nH/)/n)}\). We prove (4.4) in Appendix D.2.

**Remark 4.2**.: _We note that for KL-divergence robust sets, the dependence on \((H)\) is due to the usage of general function approximations, which also appears in a recent work  for RMDPs with linear function approximations. For the special case of robust tabular MDPs, under KL-divergence, existing work  derived sample complexities without \((H)\), but with an additional dependence on \(1/d_{}^{}\) and \(1/P_{}^{}\) Here \(d_{}^{}=_{(s,a,h):d_{P_{n}^{},h}^{}(s,a)>0}d_ {P^{},h}^{}(s,a)\) and \(P_{}^{}=_{(s,s^{},h):P_{n}(s^{}|s,_{h}^{}(s))> 0}P_{h}^{}(s^{}|s,_{h}^{}(s))\). We remark that our analysis for \(^{}\) algorithm can be tailored to the tabular case and become \((H)\)-free using their techniques, with the cost of an additional dependence on \(1/d_{}^{}\) and \(1/P_{}^{}\). But we note that in the infinite state space case, both the \(1/d_{}^{}\)-dependence and the \(1/P_{}\)-dependence becomes problematic. So, it serves as an interesting future work to answer whether one can derive both \((H)\)-free and \((1/d_{}^{},1/P_{}^{})\)-free results for (general) function approximations under KL-divergence._

\(\)**-rectangular robust MDP with kernel and neural function approximations (Examples 2.7 and 2.8).** By specifying the bracket numbers in Corollary 4.1, we can provide the detailed suboptimality guarantees for \(\)-rectangular robust MDP with kernel and neural function approximations. Due to space limitations, we defer the detailed results to Appendices D.3 and D.4.

Model Estimation for \(\)-rectangular Robust Factored MDPs (Example 2.9)

We first construct MLE estimator for each factor \(P_{h,i}^{}\) of the transition \(P_{h}^{}=_{i=1}^{d}P_{h,i}^{}\), that is,

\[_{h,i}=*{arg\,max}_{P_{i}:[_{i }]()}_{k=1}^{n} P( s_{h+1}^{}[i||s_{h}^{}[_{i}],a_{h}^{}).\] (4.5)

Then given \(\{_{h,i}\}_{i=1}^{d}\) we construct a confidence region that is factored across \(i[d]\). Specifically,

\[}_{h}=\{P(s^{}|s,a)=_{i=1}^{d}P_{i}(s^{ }[i||s[_{i}],a):_{i=1}^{n}\|(P_{i}-_{h,i})(|s_{h}^{}[_{i}],a_{h}^{})\|_{1}^{2}_{i},  i\}.\] (4.6)

Finally, we set \((,_{})\) = \(}=\{}\}_{h=1}^{H}\) with \(}_{h}\) given in (4.6). The following corollary specifies Theorem 3.4 to model estimation step given by (4.6). See Appendix E for a detailed proof.

**Corollary 4.3** (Suboptimality of \(^{}\): \(\)-rectangular robust factored MDP).: _Supposing the RMDP is an \(\)-rectangular robust factored MDP, under the same Assumptions and parameter choice in Theorem 3.4 and Proposition E.1, \(^{}\) with model estimation step given by (4.6) satisfies \(\) when \(D(||)\) is KL-divergence and Assumption D.3 holds with parameter \(\), then with probability at least \(1-2\), (defining \(_{}=_{i[d]}_{i}\))_

\[(;s_{1}),}^{ }H^{2}(H/)}}{_{}}^ {}_{i=1}^{d}||^{1+|_{i}|||(C_{ 2}^{}nd/)}}{n}}.\]

\(\) when \(D(||)\) is TV-divergence, then with probability at least \(1-2\),

\[(;s_{1}),}^{}H^ {2}}^{}_{i=1}^{d}||^{1+|_{i} |||(C_{2}^{}nd/)}}{n}}.\]

Compared with the suboptimality bounds for \(\)-rectangular robust MDPs in Section 4.1, the suboptimality of \(\)-rectangular robust factored MDPs with \(\) given in (4.6) only scales with \(_{i=1}^{d}||^{1+|_{i}|}\) instead of scaling with \(||=_{i=1}^{d}||\) which is of order \((d)\). This justifies the benefit of considering \(\)-rectangular robust factored MDPs when the transition kernels of training and testing environments enjoy factored structures.

## 5 Conclusion and Discussions

This paper proposes a general learning principle -- double pessimism -- for robust offline RL. Based on this learning principle, we propose a generic algorithm that only requires robust partial coverage data to solve \(\)-rectangular RMDPs with general function approximation. Our results are ready to be extended to \(d\)-rectangular linear RMDPs . See Appendix A for details. In Appendix A, we also provide some challenges to perform sample efficient RL in \(\)-rectangular RMDPs.