# H-InDex: Visual Reinforcement Learning with

Hand-Informed Representations for Dexterous Manipulation

 Yanjie Ze\({}^{12}\)   Yuyao Liu\({}^{3*}\)   Ruizhe Shi\({}^{3*}\)   Jiaxin Qin\({}^{4}\)

Zhecheng Yuan\({}^{31}\)   Jiashun Wang\({}^{5}\)   Huazhe Xu\({}^{316}\)

\({}^{1}\)Shanghai Qi Zhi Institute  \({}^{2}\)Shanghai Jiao Tong University  \({}^{3}\)Tsinghua University, IIIS

\({}^{4}\)Renmin University of China  \({}^{5}\)Carnegie Mellon University  \({}^{6}\)Shanghai AI Lab

\({}^{*}\)Equal contribution. Order is decided by coin flip.

yanjieze.com/H-InDex

###### Abstract

Human hands possess remarkable dexterity and have long served as a source of inspiration for robotic manipulation. In this work, we propose a human **H**and-**In**formed visual representation learning framework to solve difficult **D**exterous manipulation tasks (**H**-**InDex**) with reinforcement learning. Our framework consists of three stages: _(i)_ pre-training representations with 3D human hand pose estimation, _(ii)_ offline adapting representations with self-supervised keypoint detection, and _(iii)_ reinforcement learning with exponential moving average BatchNorm. The last two stages only modify \(0.36\%\) parameters of the pre-trained representation in total, ensuring the knowledge from pre-training is maintained to the full extent. We empirically study **12** challenging dexterous manipulation tasks and find that **H**-**InDex** largely surpasses strong baseline methods and the recent visual foundation models for motor control. Code is available at yanjieze.com/H-InDex.

## 1 Introduction

Humans can adeptly tackle intricate and novel dexterous manipulation tasks. However, multi-fingered robotic hands still struggle to achieve such dexterity efficiently. Recent progress in representation learning for visuomotor tasks has proved that pre-trained universal representations may accelerate robot learning manipulation tasks . In light of previous success, the similar morphology between human hands and robotic hands begs the question: can robotic hands leverage representations learned from human hands for achieving dexterity?

In this paper, we propose **H**and-**In**formed visual reinforcement learning framework for **D**exterous manipulation (**H**-**InDex**) that uses and adapts visual representations from human hands to boost robotic hand dexterity. Our framework consists of three stages:

* _Stage 1: Pre-training representations_ with 3D human hand pose estimation, where we adopt the feature encoder from an off-the-shelf 3D hand pose estimator FrankMocap .
* _Stage 2: Offline adapting representations_ with self-supervised keypoint detection, where we freeze the convolutional layers in the pre-trained representation and only finetune the affine

Figure 1: **Normalized average score** for our algorithm H-InDex and the baselines (VC-1 , MVP , R3M , and RRL ).

transformations in BatchNorm layers (\(\%\) parameters of the entire model). Such minimal modification of the pre-trained representations ensures that human dexterity is retained to the maximum extent and adapts the human hand representations into the target robotic domain.
* _Stage 3: Reinforcement learning_ with exponential moving average (EMA) BatchNorm and the adapted representations. EMA operates to dynamically update the mean and variance in BatchNorm layers, to further make the model adapt to the progressive learning stages.

In contrast to previous works that also learn representations from human videos , there are two major benefits of our framework: _i)_ H-InDex explicitly learn human dexterity by forcing the model to predict the 3D hand pose instead of predicting or discriminating pixels unsupervisedly using masked auto-encoding  or time contrastive learning ; _ii)_ H-InDex directly adopts the off-the-shelf visual model that is designed to capture human hands rather than training large models on large-scale datasets for specific robotic tasks. These two points combined demonstrate a new cost-effective way to solve robotic tasks such as dexterous manipulation by leveraging existing visual models that are originally and only designed for human understanding.

To show the effectiveness of H-InDex, we experiment on **12** challenging visual dexterous manipulation tasks from Adroit  and DexMV . We mainly report episode returns instead of success rates to better show how well the robots solve the tasks. In comparison with several strong visual foundation models for motor control, H-InDex largely surpasses all of them as shown in Figure 1.

To summarize, our contributions are three-fold:

* We propose a novel visual reinforcement learning framework called **H-InDex** to utilize rich human hand information efficiently for dexterous manipulation.
* We show the effectiveness of our framework on **12** challenging visual dexterous manipulation tasks, comparing with recent strong foundation models such as VC-1 .
* Our study has offered valuable insights into the application of pre-trained models for dexterous manipulation, by exploring the direct application of a 3D human hand pose estimation model, originating from the vision community.

## 2 Related Work

**Visual reinforcement learning for dexterous manipulation.** Recent research has explored the use of deep reinforcement learning (RL) for solving dexterous manipulation tasks . For example, Rajeswaran et al.  investigated the use of vector state information as input to the RL algorithm. Despite the success, assuming access to the ground-truth state limits its possibility to be deployed in the real world. RRL  finds that ImageNet pre-trained ResNets  are surprisingly effective in achieving dexterity with visual observations. Under the umbrella of visual RL, MoDem  leverages a learned dynamics model to solve the tasks with good utilization of demonstrations. Furthermore, VRL3  utilizes offline RL to pre-train the visual representations and the policies in an end-to-end manner. In this work, **H-InDex** is designed to focus on visual representations while leaving the policy, training framework, and reward signals unchanged. As a result, **H-InDex** offers an orthogonal and complementary approach to prior efforts in this area.

**Foundation models for visuo-motor control.** Given the diversity of robotic tasks and computational constraints, there is a growing interest in developing a single visual foundation model that can serve as a general feature extractor. Such a model would enable the processing of high-dimensional visual observations into compact vectors, providing a promising approach for efficient and effective control of a wide range of robotic systems . Among them, R3M  pre-trains a ResNet-50 on Ego4D  dataset and evaluates on several robotic manipulation tasks with imitation learning. MVP  pre-trains vision transformers  with Masked AutoEncoder (MAE)  on internet-scale data, achieving strong results on dexterous manipulation tasks. Similarly, a very recent foundation model VC-1  explores the scaling up of MAE for motor control and achieves consistently strong results across a wide range of benchmarks. However, it should be noted that VC-1 and R3M only employ IL to solve dexterous manipulation tasks, making it unclear whether these models are suitable for the setting of reinforcement learning, where agents need to trade off between exploration and exploitation. GNFactor  distills the 2D foundation models into 3D space, but their agents are limited to address the gripper-based manipulation problems.

**Learning dexterity from videos.** A growing body of recent research aims to leverage human manipulation videos for improving visuomotor control tasks . A line of works focuses on directly extracting human hand poses from videos and employing RL/IL to train on the retargeted robot joint positions, such as DexMV , Robotic telekinesis , VideoDex , and Imitate Video . In contrast to these approaches, our work explores a representation learning approach for leveraging online human-object interaction videos without explicit pose estimation to improve dexterity in robotic manipulation, sharing a similar motivation as MVP  and VC-1 .

## 3 Preliminaries

**Formulation.** We model the problem as a Markov Decision Process (MDP) \(=,,,,\), where \(\) are states, \(\) are actions, \(:\) is a transition function, \(r\) are rewards, and \([0,1)\) is a discount factor. The agent's goal is to learn a policy \(_{}\) that maximizes discounted cumulative rewards on \(\), _i.e._, \(_{}_{_{}}[_{t=0}^{}^{t}r_{t}]\), while using as few interactions with the environment as possible, referred as _sample efficiency_.

In this work, we focus on visual RL for dexterous manipulation tasks, where actions are high-dimensional (\(^{30}\)) and ground-truth states \(s\) are generally unknown, approximated by image observations \(\) together with robot proprioceptive sensory information \(\), _i.e._, \(=(,)\). To better address the hard exploration problem in high-dimensional control , we assume access to a limited number of expert demonstrations \(_{}=\{D_{1},D_{2},,D_{N}\}\).

**Demo Augmented Policy Gradient** (DAPG)  is a model-free policy gradient method that utilizes given demonstrations to augment the policy and trains the policy with natural policy gradient (NPG) . DAPG mainly consists of two stages:

_(1)_ Pre-training the policy with behavior cloning, which is to solve the following maximum-likelihood problem:

\[*{maximize}_{}_{(s,a)_{}} _{}(a s).\] (1)

_(2)_ RL finetuning with the demo augmented loss, which is to add an additional term to the gradient,

\[g_{}=_{}}_{} _{}(a s)A^{}(s,a)}_{}+_{}}_{}_{}(a  s)w(s,a)}_{},\] (2)

where \(A^{}(s,a)\) is the advantage function, \(_{}\) represents the dataset obtained by executing policy \(_{}\) on the MDP, and \(w(s,a)\) is a weighting function.

## 4 Method

In this work, our goal is to achieve sample efficient visual reinforcement learning agents in dexterous manipulation tasks by incorporating human hand dexterity into visual representations. To this end, we propose **H**and-**I**nformed visual reinforcement learning for **D**exterous manipulation (**H-InDex**), a simple yet effective learning framework to address the contact-rich dexterous manipulation problems effectively in limited interactions. The overview of our method is provided in Figure 2. H-InDex consists of three stages: _1)_ a _representation pre-training_ stage where we pre-train the visual representations with the 3D human hand pose estimation task, aiming to make visual representations understand human hand dexterity from diverse natural videos; _2)_ a _representation offline adaptation_ stage where we adapt only \(0.18\%\) parameters in the pre-trained representation with the self-supervised keypoint objective with in-domain data; _3)_ a \(reinforcement\)\(learning\) stage where the visual representation is frozen and we utilize the exponential moving average operation to update the mean and variance in BatchNorm of the visual representations.

Stage 1: _Representation pre-training_. We start by pre-training visual representations with _monocular 3D human hand pose estimation_, which is a well-established human hand understanding task in the computer vision community with large-scale annotated datasets available. Together with the datasets, there are a plethora of open-sourced models from which we use an off-the-shelf model FrankMocap . FrankMocap is a whole-body pose estimation system with the hand module trainedon \(6\) diverse hand datasets, totaling \(400\)k samples. We adopt the ResNet-50  feature encoder in the hand module to extract visual representations.

The use of a pre-trained model from the 3D hand pose estimation task  shares the intuition with recent works on foundation models for motor control : learning representations from human manipulation videos. However, the use of the pre-trained hand model offers two distinct advantages that are not typically found in other approaches: _i)_ the model explicitly predicts the hand-related information from diverse videos, forcing it to learn the interaction and the movement of human hands; _ii)_ the model can be borrowed from vision community without any extra cost to re-train a foundation model.

Stage 2: _Representation offline adaptation_. In the previous stage, we only pre-train visual representations that are suitable for human-centric images, neglecting the morphology and structure gap between robot hands and human hands. To bridge the gap without losing the information learned in the pre-training stage, we adopt a self-supervised keypoint detection objective  to **only** finetune the affine transformations in the BatchNorm layers of the pre-trained model, which occupy only \(\%\) of the entire model parameters. While finetuning only a small portion of parameters, it empirically outperforms both a frozen model and a fully finetuned model. We hypothesize that this is because the BatchNorm finetuning bridges the gap and mitigates catastrophic forgetting caused by finetuning .

We now describe the self-supervised keypoint objective. Given a target image \(I_{t}\) and a source image \(I_{s}\) sampled from a video, we aim to reconstruct \(I_{t}\) with the appearance feature of \(I_{s}\) and the keypoint feature of \(I_{t}\). Denote our pre-trained visual representation as \(h_{}\), the keypoint feature extractor as \(_{}\), the appearance feature extractor as \(_{}\), and the image decoder as \(_{}\). First, we extract a semantic

Figure 2: **The overview of H-InDex.** H-InDex consists of three stages: _1)_ representation pre-training, _2)_ representation offline adaptation, and _3)_ reinforcement learning.

feature map \(h_{}(I_{t})\) from the target image \(I_{t}\) and then get the keypoint feature \(_{}(h_{}(I_{t}))\). At the same time, we extract the appearance feature \(_{}(I_{s})\) from the source image \(I_{s}\). We then try to reconstruct the target image \(I_{t}\) by decoding the concatenated keypoint feature and the appearance feature as \(I^{}_{t}=_{}(_{}(h_{}(I_{t})), _{}(I_{s}))\). Our final supervision is the perceptual loss \(_{}\),

\[_{}=_{}(I_{t},I^{}_{t} )=\|(I_{t})-(_{}(_{}(h_{ }(I_{t})),_{}(I_{s})))\|_{2}^{2}\,,\] (3)

where \(\) is the semantic feature prediction function in .

Stage 3: _Reinforcement learning_. During the reinforcement learning stage, the distribution of observations is continually changing. For example, in the early learning stage, the observations are usually random explorations, while at the end of the learning stage, most of the observations are converged trajectories. Such a property of reinforcement learning requires the internal statistics of neural networks to move slowly towards the current observation distribution. Therefore, we utilize the exponential moving average (EMA) operation to dynamically update the statistics (_i.e._, the running mean and the running variance) in BatchNorm layers.

Formally, for the input \(x\) that has \(k\) dimensions, _i.e._, \(x=\{x^{(1)},,x^{(k)}\}\), we update the running mean \(^{(i)}\) and the running variance \((^{(i)})^{2}\) in BatchNorm layers with the following equation,

\[^{(i)}(1-m)^{(i)}+m[x^{(i)}]\,,\] (4)

\[(^{(i)})^{2}(1-m)(^{(i)})^{2}+m[x^ {(i)}]\,,\] (5)

for \(i=1,,k\), where \(m\) is the momentum. When \(m\) is set to \(0\), our EMA BatchNorm layers revert back to the original layers, ensuring that the modification does not have negative impacts at the very least. Finally, all these three stages collectively contribute to our final method H-InDex. We remain implementation details in Appendix A.

## 5 Experiments

In this work, we delve into the application of visual reinforcement learning to address dexterous manipulation tasks, with a particular emphasis on the visual representation aspect. We evaluate the effectiveness of our proposed framework, H-InDex, across various tasks and elucidate the significance of each component in achieving the final results. Of particular importance is the integration of prior knowledge pertaining to human hand dexterity into our framework.

Figure 3: **Visualization of our six kinds of dexterous manipulation tasks and one sampled trajectory.** We depict both the initial configuration and the goal. Videos of trajectories for all tasks are available on our website yanjieze.com/H-InDex.

### Experiment Setup

We evaluate H-InDex on **12** challenging visual dexterous manipulation tasks from Adroit  and DexMV  respectively, including _Hammer_, _Door_, _Pen_, _Pour_, _Place Inside_, and _Relocate YCB Objects_ (\(7\) different objects ). Visualization of each task is given in Figure 3 and detailed descriptions are provided in Appendix B. This selection of tasks is the most extensive compared to previous works e.g., DAPG (\(4\) tasks) and DexMV (\(7\) tasks), thereby showcasing the robustness and versatility of H-InDex. The tasks were performed with varying numbers of steps based on their level of complexity. We mainly report the cumulative rewards to show the speed of task completion. The dimension of image observations \(\) is \(3 224 224\) across all methods. We run experiments on an RTX 3090 GPU; each seed takes roughly 12 hours. Due to the limitation on computation resources, we choose to run \(3\) seeds for each group of experiments and consistently use 3 seeds with seed numbers \(0,1,2\) to ensure reproducibility. We also observe that H-InDex enjoys a slight variance between seeds, while baselines tend to have a larger variance.

### Main Experiments

To demonstrate the effectiveness of H-InDex, we evaluate diverse recent strong visual representations for motor control, including _(i)_ VC-1 , which trains masked auto-encoders over 5.6M images with over 10,000 GPU-hours and we use the ViT-B  model (86M parameters); _(ii)_ MVP , which also uses masked auto-encoders for pre-training and we use the ViT-S  model (22M parameters); _(iii)_ R3M , which pre-trains a ResNet-50 (22M parameters) with time contrastive learning and video-language alignment _(iv)_ RRL , which uses the ResNet-50 pre-trained on the ImageNet classification task directly. Due to task differences, we normalize the cumulative rewards based on the highest rewards achieved and present the average scores in Figure 1. We also report the learning curves in Figure 4. We then detail our observations below.

**H-InDex emerges as the dominant representation.** Across 12 tasks, H-InDex outperforms the recent state-of-the-art representation VC-1 by a \(\%\) absolute improvement. Furthermore, H-InDex surpasses RRL, the original state-of-the-art representation in Adroit, by \(\%\). Analyzing the learning curves, H-InDex demonstrates superior sample efficiency in \(10\) out of the \(12\) tasks. In

Figure 4: **Episode return for 12 challenging dexterous manipulation tasks. We compare H-InDex with four strong visual representations for motor control, _i.e._, VC-1 , MVP , R3M , and RRL . Mean of \(3\) seeds with seed number \(0,1,2\). Shaded area indicates 95% confidence intervals.**only two tasks, namely relocate mug and relocate mustard bottle, VC-1 exhibits a slight advantage over H-InDex.

**ConvNets v.s. ViTs.** Among representations utilizing the ResNet-50 architecture (_i.e._, H-InDex, R3M, RRL), only H-InDex showcases obvious advantages over ViT-based representations. This suggests that with appropriate domain knowledge, ConvNets can still outperform ViTs. Additionally, we notice that ConvNets and ViTs excel in different tasks. For instance, in relocate tomato soup can, VC-1 and MVP achieve returns that are only half of what H-InDex and RRL accomplish. However, in relocate mug, VC-1 performs well. These observations highlight the task-dependent nature of the strengths exhibited by ConvNets and ViTs.

### The Effectiveness of 3D Human Hand Prior

The significance of transferring the 3D human hand knowledge into dexterous manipulation is non-negligible. To demonstrate the utility of such 3D human hand prior, we compare our vanilla pre-trained representation _i.e._, the feature extractor from the FrankMocap hand module  (denoted as **FrankMocap Hand**) with other 4 representative pre-trained models: _(i)_**FrankMocap Body**, which is the body estimation module from FrankMocap , pre-trained with 3D body pose estimation, _(ii)_**AlphaPose**, which is a widely-used robust 2D human pose estimation algorithm, _(iii)_**R3M**, which is pre-trained with time contrastive learning  and language-video alignment on Ego4D , and _(iv)_**RRL**, which directly uses the ResNet-50 pre-trained on the ImageNet classification task. All the models use a ResNet-50 architecture and do not use any adaptation, ensuring the fairness of our comparison. We also put **H-InDex** as the best results achieved for comparison. Results are shown in Figure 5. We now detail our observations below:

**FrankMocap hand v.s. RRL/R3M.** Our vanilla representation has significantly outperformed both RRL and R3M without the need for any adaptation.

**FrankMocap hand v.s. 3D/2D body-centric representations.** Our vanilla 3D hand representation, FrankMocap Hand, demonstrates superior sample efficiency compared to FrankMocap Body and significantly outperforms AlphaPose. This confirms our hypothesis that the 3D human hand prior is more advantageous than both the 3D and 2D human body prior. It is worth noting that AlphaPose, being a whole-body 2D pose estimation model, is also capable of estimating 2D hand poses. The fact that our 3D hand prior outperforms the 2D hand prior in this context further supports its effectiveness. We hypothesize this is because 2D pose estimation does not require deep spatial reasoning compared to 3D pose estimation.

**H-InDex v.s. FrankMocap hand.** Our vanilla hand representation already surpasses all other pre-trained ConvNets in performance. However, by applying our adaptation technique, we can further enhance the sample efficiency of the hand representation, underscoring the significance of adapting the model with in-domain data in a proper way.

### Ablations

To validate the rationale behind the design choices of H-InDex, we performed a comprehensive set of ablation experiments.

**Effects of each stage.** Figure 5(a) provides insights into the contributions of each stage towards the overall efficiency of H-InDex. We refer to RRL as **w/o Stage 1,2,3**. Significantly, Stage 1 exhibits the most notable enhancement, underscoring the efficacy of human dexterity. Moreover, Stage 2 and Stage 3 also contribute appreciable advancements. In addition, the value of momentum (\(m\)) in Stage 3 has a significant influence, as illustrated in Figure 5(b). To determine the optimal value, we performed a grid search over \(m\{0,0.1,0.01,0.001\}\). This analysis highlights the importance of selecting an appropriate momentum value for achieving optimal performance in Stage 3 of H-InDex.

Figure 5: **Compare vanilla pre-trained representations with H-InDex.**

Figure 9 provides more ablation results on Stage 3, further supporting the necessity of updating the BatchNorm layers during the training of RL agents.

**Adapting \(100\%\) parameters v.s. adapting \(0.18\%\) parameters in Stage 2.** In Stage 2 of H-InDex, we intentionally chose to adapt only \(0.18\%\) of the parameters (the affine transformations in BatchNorm layers) in the pre-trained representation. This decision was made to address a specific concern, as depicted in Figure (c)c and Figure 7. By altering only the setting for Stage 2 while keeping all other factors constant, we observed that across all tasks, adapting all parameters is not more advantageous. This phenomenon may be attributed to the fact that freezing and finetuning partial parameters help mitigate catastrophic forgetting, which is often caused by full finetuning . In Figure 8, we also show the necessity of our Stage 2. We could conclude from results that correctly finetuning the visual representation is one key to the stable convergence, and not correctly finetuning the model, such as finetuning all the parameters, could be even worse than the frozen model.

**Robust visual generalization.** One concern of our hand representation is its generalization ability, compared to the vision model pre-trained on large-scale datasets, such as VC-1 . Therefore, we change the background of the training scene to various novel backgrounds, as shown in Figure 12 (see Appendix C) and evaluate VC-1 and H-InDex on the task relocate potted meat can. The

Figure 6: **Ablation experiments.** We ablate each component of H-InDex and show that each individual part effectively combines to contribute to the overall effectiveness of H-InDex.

Figure 7: **Ablation on Stage 2 (adapting \(0.18\%\) parameters or adapting \(100\%\) parameters).** We observe that simply finetuning the entire visual representation would lead to sub-optimal results. Instead, H-InDex only adapts the parameters in BatchNorm layers and effectively solves all the tasks.

results given in Table 3 (see Appendix C) show that H-InDex could handle the changed background better than VC-1. We also see the consistent performance drop across all scenes, emphasizing the importance of visual generalization.

**Visualization of self-supervised keypoint detection in Stage 2.** In Stage 2, a self-supervised keypoint detection objective is employed to fine-tune a minimal percentage (\(0.18\%\)) of parameters in the pre-trained model. The visualization results, as shown in Figure 10, demonstrate the successful detection of keypoints. This observation highlights the pre-trained model's capability to effectively allocate attention to the hand and objects depicted in the images, even with the adaptation of only a small subset of parameters.

**Visualization of affine transformations adaptation in Stage 2.** The significance of Stage 2 in H-InDex is evident from Figure 5(a). To gain deeper insights into this phenomenon, we visualize the distribution of the adapted parameters, specifically the affine transformations in BatchNorm layers. We accomplish this by fitting a Gaussian distribution. Figure 11 presents the visualization results, highlighting an interesting trend. For the shallow layers, the distributions of the adapted models closely resemble those of the pre-trained models. However, as we move deeper into the layers, noticeable differences emerge. We attribute this disparity to the fact that dissimilarities between human hands and robot hands extend beyond low-level features like color and texture. Instead,

Figure 8: **Ablation on Stage 2 (with or without adaptation).** We also conduct more experiments to show the necessity of Stage 2. We could observe a consistent improvement across these tasks by applying Stage 2, which only adapts \(0.18\%\) parameters of the visual representation.

Figure 9: **Ablation on Stage 3 (momentum \(\) or \(\)).** We observe that our Stage 3 contributes greatly to some specific tasks, such as relocate mustard bottle, and for some tasks like hammer, tuning this parameter only results in a slightly faster convergence. For tasks not shown here, we all use \(m=0\), since H-InDex with Stage 1 and Stage 2 has been strong enough.

they encompass higher-level features such as dynamics and structure . This observation underscores the importance of our adaptation approach, as it effectively addresses the variations in both low-level and high-level features, facilitating the success of H-InDex.

## 6 Conclusion

In this study, we introduce H-InDex, a visual reinforcement learning framework that leverages hand-informed visual representations to tackle complex dexterous manipulation tasks effectively. H-InDex outperforms other recent state-of-the-art representations in a range of 12 tasks, including six kinds of manipulation skills. The effectiveness of H-InDex can be attributed to its three-stage approach, wherein Stage 1 incorporates a pre-trained 3D human hand representation and Stage 2 and Stage 3 focus on careful in-domain adaptation with only \(0.36\%\) parameters updated. These stages collectively contribute to the successful preservation and utilization of the human hand prior knowledge.

It is also important to acknowledge some limitations of our work. We did not investigate the generalization capabilities of H-InDex, particularly in scenarios involving the grasping of novel objects. Our future work aims to address this limitation and enhance H-InDex's generalization capabilities for real-world applications. In addition, we find that our Stage 3 is surprisingly effective in some specific tasks like relocate mustard bottle, while we have not given a theoretical understanding of such phenomena. We consider this problem as a possible future direction.