# Active, anytime-valid risk controlling prediction sets

Ziyu Xu

Department of Statistics and Data Science

Carnegie Mellon University

xzy@cmu.edu

&Nikos Karampatziakis

Microsoft

nikosk@microsoft.com

&Paul Mineiro

Microsoft

pmineiro@microsoft.com

Part of work done while interning at Microsoft.

###### Abstract

Rigorously establishing the safety of black-box machine learning models concerning critical risk measures is important for providing guarantees about model behavior. Recently, Bates et. al. (JACM '24) introduced the notion of a _risk controlling prediction set (RCPS)_ for producing prediction sets that are statistically guaranteed low risk from machine learning models. Our method extends this notion to the sequential setting, where we provide guarantees even when the data is collected adaptively, and ensures that the risk guarantee is anytime-valid, i.e., simultaneously holds at all time steps. Further, we propose a framework for constructing RCPSes for active labeling, i.e., allowing one to use a labeling policy that chooses whether to query the true label for each received data point and ensures that the expected proportion of data points whose labels are queried are below a predetermined label budget. We also describe how to use predictors (i.e., the machine learning model for which we provide risk control guarantees) to further improve the utility of our RCPSes by estimating the expected risk conditioned on the covariates. We characterize the optimal choices of label policy and predictor under a fixed label budget and show a regret result that relates the estimation error of the optimal labeling policy and predictor to the wealth process that underlies our RCPSes. Lastly, we present practical ways of formulating label policies and empirically show that our label policies use fewer labels to reach higher utility than naive baseline labeling strategies on both simulations and real data.

## 1 Introduction

One of the core problems of modern deep learning systems is the lack of rigorous statistical guarantees one can ensure about the performance of a model in practice. In particular, we are interested in ensuring the safety of a deep learning system so that it does not incur undue risk while optimizing for an objective of interest. This type of guarantee arises in many applications. For example, a deep learning based medical imaging segmentation system that detects lesions  should guarantee that it does not miss most of the lesion tissue while remaining precise and minimizing the total amount of tissue that is highlighted. Hence, it is crucial to provide a statistical guarantee about the "safety" of any machine learning system to be deployed. Bates et al.  introduced the notion of a _risk controlling prediction set_ as a method to derive such guarantees on top of the outputs of a wide range of black-box models. They consider the setting where all the calibration data for verifying statistical safety guarantees is available before deployment and the model only needs to becalibrated once, i.e., the batch setting. However, this is often unrealistic in a production setup when we have no data concerning the performance of a model on the distribution of interest before we deploy it, and we wish to update our calibration each time a new data point (or group of data points) arrives. Consequently, it is natural to calibrate the machine learning model in an _online_ fashion while receiving new data sequentially. Unfortunately, methods for obtaining statistical guarantees in the batch setting of Bates et al.  do not ensure risk control guarantees in the sequential regime. Further, when one uses data from production, the raw data is unlabeled, and one must expend resources (either paying experts or utilizing a more powerful model) to create gold labels for these data points. Hence, we generalize the sequential setup to an active setting in which we see the covariate \(X\) (e.g., an image, a natural language query from the user, etc.) and choose whether to query the true label \(Y\). Concretely, consider the following scenarios where an active and sequential method is relevant.

* _Reduce query cost in medical imaging._ A medical imaging system that outputs scores for for each pixel of image that determines whether there is a lesion or not would want to utilize labels given by medical experts for unlabeled images from new patients. Since the cost of asking experts to label these images is quite high, one would want to query experts efficiently, and only on data that would be most helpful for reducing the number of highlighted pixels.
* _Domain adaptation for behavior prediction._ Often, the post-deployment distribution is different from that which has been seen before. For example, during a navigation task for a robot, we may want to predict the actions of other agents and avoid colliding into them when travelling between two points . Since agents may behave differently in every environment, it makes sense to collect the behavior data in the test environment and update the behavior prediction in an online fashion to get accurate predictions calibrated for that specific environment.
* _Safe outputs for large language models (LLMs)._ One of the goals with large language models is to ensure their responses are not harmful in some fashion (e.g., factually wrong, toxic, etc.). One can view this as producing a prediction set for the binary label set of \(Y\{\}\). Many pipelines for modern LLMs include some form of a safety classifier, which scores the risk level of an output, and determines whether it should be output to the user or not , or a default backup response should be used instead. One would want to label production data acquired from user interaction with the LLM and used to calibrate cutoff for the scores that are considered low enough for the response to be allowed through.

Example: image classificationLet us assume we wish to classify an image \(X\), and we have access to a probabilistic classifier \(s:^{}\) where \(^{}\) is the probability simplex over distributions over all possible classes, \(\). Let \(s^{y}(x)\) denote the probability of class \(y\) in the distribution \(s(x)\). Based on the probabilities from \(s(X)\), we can define \(C(X,)\) to have the labels with the largest probabilities that sum to \(\) in the following fashion:

\[(X,) \{:_{y}\{s^{y}(X)\} s^{y}(X)\},\] \[C(X,) \{y:s^{y}(X)(X,)\}\]

Now, we can define the miscoverage error of our label set \(C(X,)\) as follows:

\[r(X,Y,)\{Y C(X,)\}.\] (1)

Now, assume that \((X,Y)^{*}\), i.e., the images and class are jointly drawn from a fixed distribution. We want to find a choice of \(\) such that \(()[r(X,Y,)]\) is guaranteed to be at most \(\), i.e., the expected miscoverage over the population of images and labels is at most \(\).

In the above image classification example, we do not simply wish to find any \(\) that ensures \(()\) -- setting \(=1\) would trivially ensure this guarantee for any \(\). We also want to minimize the size of our uncertainty set \(C(X,)\). To present this formulation in more general terms, we are interested in solving the following problem for a fixed level of risk control \(\):

\[_{}g()().\] (2)

where \(g\) is the utility of our choice. We make the following natural assumption about \(r\), \(\), and \(g\).

_Assumption 1_.: \(g\) and \(\) are monotonically decreasing w.r.t. \(\) and we assume \((1)=0\). In addition, \(\) is right-continuous.

Our image classification example has an expected risk and utility that satisfy the respective monotonicity assumptions, and such risk measures arise in many applications such as natural language question answering , image segmentation , and behavior control for robotics [20; 16]. Assumption 1 implies that maximizing \(g()\) is equivalent to minimizing \(\), as \(g\) is decreasing in \(\), and the right-continuity of \(\) allows us to define the notion of an optimal calibration parameter that is the solution to (2):

\[^{*}\ \{:()\}.\]

Our goal in this paper is to derive a sequence of upper bounds on \(^{*}\) that quickly approach the true \(^{*}\) but provide "anytime-valid risk control" in the sense that they are always greater the smallest "safe" parameter \(^{*}\) and induce risk under \(\), i.e., \(^{*}\) implies that \(()(^{*})\). Since we are guaranteed by Assumption 1 that \((1)=0\), we always have a safe option of \(=1\) to start with as our upper bound.

**Our contributions.** The primary contributions of this paper are as follows.

1. _Extensions of RCPS to anytime-valid and active settings._ We extend the notion of RCPS in two ways: (1) to enable anytime-valid RCPS which allows one to refine the set as one receives more samples in a stream while maintaining risk control throughout the entire stream, and (2) to define an RCPS that is valid under active learning, i.e., enable us to decide whether to label each example based on the covariates. We also define a way for incorporating risk predictions from the machine learning model to decrease variance further and reduce the number of labels needed to estimate \(^{*}\). We formulate this betting framework in Section 2.
2. _Deriving powerful labeling policies and predictors._ We show in Section 3 that our active, anytime-valid RCPS methods are practically powerful and converge to \(^{*}\) in a label efficient manner by also deriving formulations for the optimal labeling policy and predictors under the standard log-optimality criterion that is used for evaluating anytime-valid methods [13; 34; 19]. We derive explicit regret bounds w.r.t. a lower bound on the growth rate on the wealth processes that underlie our RCPS methods. These bounds characterize how the deviation of any labeling policy and predictor from the log-optimal policy and predictor affect the growth rate of our wealth processes (and hence the earliest time at which a candidate \(\) is removed from consideration as \(^{*}\)). In Section 4 we also show that machine learning model based estimators of the optimal policy and predictors are label efficient in practice through experiments.

**Related work.** Most relevant to this paper is the recent work from Zrnic and Candes  that provides a rigorous framework for statistical inference with active labeling policies, and leverages machine learning predictions through prediction-powered inference . However, their focus is on M-estimation and deriving asymptotic, martingale central-limit theorem based results for a parameter. On the other hand, we provide finite sample anytime-valid results that are also valid at adaptive stopping times that directly utilize e-process  construction of sequential tests. Further, our goal is to provide a time-uniform statistical guarantee in the RCPS framework rather than directly estimating a parameter with adaptively collected data. We discuss additional related work in depth in Section 5.

## 2 Anytime-valid risk control through betting

We use \((X_{t})_{t}\) to denote a sequence that is indexed by \(t\) with index set \(\). If the index set or indexing variable is apparent in context, we drop it for brevity. In our setup, we assume that our data points arrive in a stream \((X_{1},Y_{1}),(X_{2},Y_{2}),\) that proceeds indefinitely. Let \((_{t})\) be the canonical filtration on the data, i.e., \(_{t}(\{(X_{i},Y_{i})\}_{i[t]})\) is the sigma-algebra over the first \(t\) points. Recall that we assumed \((X_{t},Y_{t})^{*}\) are i.i.d. draws for each \(t\{1,2,3,\}\), and want to control the risk \(()=_{(X,Y)^{*}}[r(X,Y,)]\) where the expectation is take only over \((X,Y)\). We illustrate an overview of our methodology (which we describe in the sequel) in Figure 1.

We desire to output a sequence of calibration parameters, \((_{t})\), such that every \(_{t}\) is "safe", i.e., ensures that the resulting risk of the output is provably controlled under a fixed level.

_Definition 1_.: A sequence of calibration parameters \((_{t})\) is said to have \((,)\)_-anytime-valid risk control_ if it possesses the following property:

\[((_{t})t) 1-.\] (3)We name this as "anytime-valid" since the risk control condition (i.e., \((_{t})\)) is guaranteed to hold simultaneously at all \(t\). Hence, this allows for the user to process a continuous stream of data and control the probability that a \(_{t}\) is chosen at any time \(t\) that is "unsafe", i.e., \((_{t})>\). We build on recent work that develops a framework for hypothesis testing and parameter estimation with sequential data collection based on martingales and gambling with virtual wealth known as _testing by betting_. In this framework, the goal is to design an _e-process_, \((E_{t})\), w.r.t. a null hypothesis \(H_{0}\), which satisfies the following properties when true:

_Definition 2_.: An _e-process_, \((E_{t})_{t_{0}}\), w.r.t. a hypothesis \(H_{0}\), is a nonnegative process for which there exists another nonnegative process, \((M_{t})_{t_{0}}\) s.t. the following is true when \(H_{0}\) is true: (1) \([M_{0}] 1\), (2) \(M_{t} E_{t}\) for all \(t\) almost surely and (3) \([M_{t}_{t-1}] M_{t-1}\) for all \(t\), i.e., \((M_{t})\) is a supermartingale.

E-processes will be the main tool we use to construct \((_{t})\). We leverage the probabilistic bound on e-processes provided by Ville's inequality to prove our anytime-valid risk control guarantee.

_Fact 1_ (Ville's inequality ).: For any e-process, \((E_{t})\), with initial expectation bounded by 1, i.e., \([E_{0}] 1\), we have that

\[(t:M_{t}^{-1}) .\]

In this paper, all our e-processes will also be nonnegative supermartingales, so we denote them as \((M_{t})\). Now, we will specify the null hypotheses in our risk control setting. For each \(\), we test the null hypothesis \(H_{0}^{}:()\) for a fixed risk control level \(\). Note that we include equality to \(\) in the null hypothesis since we do not wish to reject \(H_{0}^{^{*}}\). Let \(\{(M_{t}())\}_{}\) be a family of e-processes where \((M_{t}())\) is an e-process for \(H_{0}^{}\). Then, we can derive \(_{t}\) for each \(t\) as follows:

\[_{t}\,\{:M_{t}(^{ }) 1/^{}>\}.\] (4)

**Theorem 1**.: _The sequence of estimates \((_{t})\) in (4) satisfies the anytime-valid risk control guarantee (3), i.e., \(((_{t})\) for all \(t) 1-\)._

Proof.: First, we note that

\[\{ t:(_{t})>\} \{ t:_{t}<^{*}\} \{ t:M_{t}(^{*}) 1/\}.\]

Figure 1: Diagram of the active labeling setup for ensuring anytime-valid risk control.

Since \(H_{0}^{^{*}}\) is always true by definition of \(^{*}\), we get that \((M_{t}(^{*}))\) is an e-process by Proposition 1. Thus, by applying Ville's inequality, we get that:

\[( t:(_{t})>)( t:M_{t}(^{*}) 1/).\]

Now, we will present a concrete example of an e-process. Denote \(R_{t}() r(X_{t},Y_{t},)\). We test \(H_{0}^{}\) using the betting e-process from Waudby-Smith and Ramdas :

\[M_{t}()=_{i=1}^{t}(1+_{i}(-R_{i}() )),\] (5)

where \((_{t})\) is predictable w.r.t. \((_{t})\), i.e., \(_{t}\) can be determined by \(_{t-1}\) for each \(t\), and \(_{t}[0,(1-)^{-1}]\).

**Proposition 1**.: \((M_{t}())\) _in (5) is an e-process for all \(\) where \(H_{0}^{}\) is true, i.e., where \(()\)._

Proof.: We note that \((M_{t}())\) is nonnegative by the support of \(_{t}\) being limited, i.e.,

\[1+_{t}(-r(X_{t},Y_{t},)) 1+_{t}(-1) 0.\]

Now, we will also show that \((M_{t}())\) is a supermartingale when \(H_{0}^{}\) is true.

\[[M_{t}()_{t-1}] =[1+_{t}(-R_{t}())_{t -1}] M_{t-1}()\] \[=(1+_{t}(-[R_{t}()_{t -1}])) M_{t-1}() M_{t-1}().\]

The second equality is because \(_{t}\) is measurable w.r.t. \(_{t-1}\) and the last inequality is by \(R_{t}()\) being independent of \(_{t-1}\) and \([R_{t}()]\) being true under \(H_{0}^{}\). Thus, we have our desired result. 

Now, we have a concrete way to derive \((_{t})\) that ensures the risk \((_{t})\), is controlled at every time step \(t\). However, this requires one to label every example that arrives, i.e., it requires access to entire stream of labels \((Y_{t})\). We will now derive a more label efficient way for constructing \((_{t})\).

_Remark 1_.: Ramdas et al.  show that e-processes of the form in (5) characterize the set of admissible e-processes (and hence anytime-valid sequential tests) for testing the mean of bounded random variables. Hence, it is an optimal choice of e-process for our setting, and have been shown to perform better both theoretically and empirically than other sequential tests for bounded random variables (e.g., Hoeffding and empirical-Bernstein based tests ).

### Active sampling for risk control

Now, we describe active learning for risk control, where an algorithm sees \(X_{t}\) decides whether a label, for the current point, \(Y_{t}\), should be queried or not. At each step \(t\), the algorithm produces a label policy \(q_{t}:[q_{t}^{},1]\) based on the observed data (i.e., \(_{t-1}\)). It then queries the label, \(Y_{t}\), with probability \(q_{t}(X_{t})\) that is lower bounded by a constant, \(q_{t}^{}\). Let \(L_{t}\) be the indicator random variable for whether the \(t\)th label is queried, i.e., \(L_{t}(q_{t}(X_{t}))\).

To produce a label efficient method, one would hope to label the most "impactful" data points that result in the largest growth of \(M_{t}()\) for choices of \((0,_{t})\), i.e. that are still in consideration for the next \(_{t+1}\). For the labeling policies we consider in this paper, we let \(q_{t}^{}\) be a lower bound on the labeling probability, i.e., \(q_{t}(X_{t}) q_{t}^{}\) almost surely. Thus, we can derive the following e-process for any sequence of labeling policies \((q_{t})\).

\[M_{t}()_{i=1}^{t}(1+_{i}(- }{q_{i}(X_{i})} R_{i}())).\] (6)

**Proposition 2**.: _Let \((q_{t})\) be a sequence of labeling policies, and \((_{t})\) be a sequence of betting parameters, and let both sequences be predictable w.r.t. \((_{t})\) (i.e., \(q_{t}\) and \(_{t}\) are measurable w.r.t. \(_{t-1}\) for each \(t\)). Then, \((M_{t}())\) in (6) is an e-process for all \(\) where \(H_{0}^{}\) is true._Proof.: The proof of this is similar to that of inverse propensity-weighted e-processes derived in Waudby-Smith et al. . We know that \((M_{t}())\) is nonnegative by the support of \(_{t}\) being limited:

\[1+_{t}(-}{q_{t}(X_{t})} R_{t}())  1+_{t}(-(q_{t}^{})^{-1}) 0,\]

where the first inequality is by \(R_{t}() 1\) and \(q_{t}(X_{t}) q_{t}^{}\), and the second inequality is by \(_{t}((q_{t}^{})^{-1}-)^{-1}\). Now, we will also show that \((M_{t}())\) is a supermartingale. We first show the following upper bound:

\[[}{q_{t}(X_{t})} R_{t}()_{t -1}]=[[L_{t} X_{t},_{t-1} ]}{q_{t}(X_{t})}R_{t}()_{t-1}]=[R_{t}( )_{t-1}].\] (7)

The first equality is by further conditioning on \(X_{t}\), and the second equality is since \(L_{t}\) is defined to be a Bernoulli random variable with parameter \(q_{t}(X_{t})\) that is independent of all other randomness when conditioned on \(X_{t}\) and \(_{t-1}\). The last inequality is by \(H_{0}^{}\) being true. Now, we have that

\[[M_{t}()_{t-1}]=(1+_{t}( -[}{q_{t}(X_{t})}R_{t}()_{t-1} ]))M_{t-1}() M_{t-1}(),\]

where the inequality is by (7). Hence, we have shown that \((M_{t}())\) is a nonnegative supermartingale, and hence also an e-process, under \(H_{0}^{}\). 

**Theorem 2**.: \((_{t})\) _defined w.r.t. (6) satisfies the anytime-valid risk control guarantee (3)._

This is a result of Proposition 2 and Ville's inequality, similar to the proof of Theorem 1. Theorem 2 essentially shows that we can still design e-processes by allowing for a probabilistic label policy.

### Variance reduction through prediction

Often, we also have an estimate of the risk we incur, e.g., in the example given for classification, we have an estimated probability distribution over possible outcomes. As a result, we also have an empirical estimate of \([r(X,Y,) X=x]\) for each \(\) that we can use to reduce the variance of our estimate. This is similar to the usage of control variates for improving Monte Carlo estimation [3, SS V.2], and of predictors in the recently formulated prediction-powered inference framework . Let \(_{t}:\) be an estimator of the risk incurred by parameter \(\) conditional on \(x\) for each time step \(t\). \((_{t})\) is predictable w.r.t. \((_{t})\).

**Where does \(\) come from?** We note that often machine learning models have some estimate \((X)\) of the conditional distribution of \(Y X\) (e.g, class probabilities, conditional diffusion models, LLMs, etc.). Thus, for any realized covariate \(x\), we can derive use \(_{Y(x)}[r(X,Y,) X=x]\) from the machine learning model as our choice of \((x,)\). This expectation can either be calculated analytically (as we do in or classification examples in our experiments) or derived using Monte Carlo approximation (for generative models such as LLMs, one can sample from the conditional distribution). In essence, we can obtain a predictor from the very model we are calibrating. We may also update our predictor using new \((X_{t},Y_{t})\) pairs we receive for calibrating \((_{t})\).

Now, we define our e-process that utilizes our predictor as follows:

\[M_{t}()_{i=1}^{t}(1+_{i}(- {r}_{i}(X_{i},)-}{q_{i}(X_{i})}_{i}()) )_{t}() R_{t}()-_{t}(X_{t},),\] (8)

and we restrict \(_{t}[0,((q_{t}^{})^{-1}-)^{-1}]\) (or in other words, \(q_{t}^{}_{t}/(1+_{t})\)). Note that this e-process recovers the active e-process defined in (6) if we set \(_{t}(,)=0\) for all \(t\).

**Proposition 3**.: \((M_{t}())\) _as defined in (8) is an e-process for \(H_{0}^{}\)._

Proof.: Since the restriction on \((_{t})\) ensures \(M_{t}()\) is nonnegative, to show that \((M_{t}())\) is an e-process, it is sufficient to show:

\[[_{t}(-_{t}(X_{t},)-L_{t}  q_{t}(X_{t})^{-1}_{t}()+)_{t-1}]\] \[=_{t}(-[_{t}(X_{t},)+L_{t}  q_{t}(X_{t})^{-1}_{t}()_{t-1}])\] \[=_{t}(-[_{t}(X_{t},)+ [L_{t} q_{t}(X_{t})^{-1} X_{t},_{t-1}] _{t}()_{t-1}])\] \[=_{t}(-[_{t}(X_{t},)+_{t}()_{t-1}])=_{t}(-[R_{t}( )_{t-1}]) 0.\]The 3rd equality is by definition of \(L_{t}\), the last equality by the definition of \(_{t}\), and the last inequality is due to \(H_{0}^{}\) being true. 

The role of \(_{t}(X_{t},)\) is to accurately predict \(R_{t}()\). Bad predictions can increase the variance of \(_{r}()\) and lead to slower growth of \(M_{t}()\), but do not compromise the risk control guarantee. On the other hand, accurate predictions, which come from pretrained models, decrease variance and improve the growth of \(M_{t}()\). We characterize the optimal predictor (Proposition 4) and relate the accuracy of a predictor to its effect on the e-process (Theorem 3) in the next section.

## 3 Optimal labeling policies

Since the goal of having an active labeling policy is to label fewer data points, one reasonable way of doing this is to maximize the growth rate of our e-process \((M_{t}())\) defined in (8). Define the following function, for some \(\), of a labeling policy \(q\), predictor \(\), and betting parameter \(\) where we let \(L X(q(X))\) and \((X,Y)^{*}\):

\[G^{}(q,,)(1+(- (X,)-()))()  r(X,Y,)-(X,).\]

Define the _growth rate_ at the \(t\)th step of \((M_{t}())\) as \(_{t}^{}[G_{t}^{}(q_{t},_{t},_{t})]\), where we let \(G_{t}^{}\) be identical to \(G_{t}\) but with \(X\) and \(Y\) replaced with \(X_{t}\) and \(Y_{t}\), respectively. It is a standard notion of power or sample efficiency for e-processes. Typically, our goal when designing an e-process based test is to maximize such a metric, i.e., we want our e-process to be log-optimal [13; 34; 19]. Log-optimality is also called the Kelly criterion in finance  and it is known that maximizing the growth rate of a process is equivalent to minimizing the expected time for the process to exceed a threshold, i.e., for our sequential test to reject a value of \(\), in the limit as the threshold approaches infinity . Thus, in an asymptotic sense, maximizing the growth rate is equivalent to minimizing the expected time for rejection. Our goal is to maximize the growth rate while having a constraint on the number of labels we can produce.

Let \(B\) be the constraint on our labeling budget, i.e., we label, in expectation, a \(B\) fraction of all data points that we receive. To achieve both of these goals, we wish to choose \(q_{t}\), \(_{t}\), and \(_{t}\) that are the solutions to the following optimization problem:

\[_{q,,}_{L q(X)}[G^{}(q,, )][q(X)] B.\]

Since solving the above optimization problem is analytically difficult, one can instead maximize a lower bound on the expected growth [32; 25; 24]:

\[^{}(q,,) (-(X,)- ())-^{2}(-(X,)-())^{2}\] \[ G^{}(q,,),\] (9)

which holds when \([0,(2(q^{})^{-1}-2)^{-1}]\), where \(q^{}_{x}q(x)\). We can further simplify We can use the lower bound in (3.1) to formulate the following optimization problem.

\[_{q,,}\ [^{}(q,, )][q(X)] B\] (10)

Let \((q^{*},r^{*},^{*})\) be the tuple that is the solution to (10). We can analytically show what \(r^{*}\) is.

**Proposition 4**.: _The optimal predictor \(r^{*}\) in the solution to (10) is \(r^{*}(x,)=[r(X,Y,) X=x]\) for each \(x\)._

We defer the proof to Appendix A.1. The optimal choice of \(q^{*}\) has the following formulation.

**Proposition 5**.: _If we fix \(\) and \(\), the solution to the optimization problem in (10) is given by \(q_{}^{*}\) where \(q_{}^{*}(x)[()^{2} X=x]}\) for each \(x\) if such a \(q_{}^{*}\) exists._

We defer the proof to Appendix A.2. Let \(_{}(x)[r(X,Y,) X=x]}\) be the conditional standard deviation of \(r(X,Y,)\). Now, we can argue that the solution to the optimization problem on the growth rate lower bound in (10) has the following characterization.

_Corollary 1_.: The optimal choice of \(q_{}^{*}\) and \(^{*}\) that solves (10) is

\[q_{}^{*}(x)(x)}{[_{}(X)]}  B,^{*}+[_{}(X)]^{2} B^{-1}+[r(X,Y,)]}\]

if \(q_{}^{*}(x)\) for all \(x\), and \(^{*}(2(_{x}q_{}^{*}(x))^{-1}-2)^{-1}\). The resulting growth rate has the following lower bound:

\[[G_{t}^{}(q^{*},r^{*},^{*})][_{ t}^{}(q^{*},r^{*},^{*})]= }{(-())^{2}+[_{}(X)]^{2} B^{-1}+ [r(X,Y,)]}\]

We can show this is true as a consequence of Proposition 4, Proposition 5, and solving the quadratic equation that arises for the growth rate to derive the optimal choice of \(^{*}\). Further, we note that we can define regret of a sequence \((_{t})\) compared to \(^{*}\) on \(_{t}^{}\) as follows.

_Definition 3_.: The \(^{}\)_-regret_ at the \(t\)th step of a sequence of betting parameters \((_{t})\) for a risk upper bound \(\), and a sequence of labeling policies \((q_{t})\) and predictors \((_{t})\) where \(q_{t}(x)>0\) for all \(x\) and \(t\) almost surely is defined as follows:

\[_{t}_{[0,(2^{-1}-2)^{-1}]} _{i=1}^{t}[_{t}^{}(q_{t},_{t}, )_{t-1}]-[_{t}^{}(q_{t}, _{t},_{t})_{t-1}].\]

Since \(_{t}^{}(q,,)\) is exp-concave in \(\), existing online learning algorithms such as Online Newton Step (ONS)  can get \(o(T)\) regret guarantees, which means that the growth rate of \((_{t})\) averaged over time will approach (or exceed) the optimal growth rate under \(^{*}\). For simplicity of analysis, we make the following assumption about the labeling probability of the optimal policy, \(q_{}^{*}\).

_Assumption 2_.: Let \(>0\) be a positive constant. Assume that \(q_{}^{*}(x)\) for each \(x\).

The lower bound in the above assumption is an analog of the propensity score lower bound on optimal policies that is needed for proving valid inference in adaptive experimentation [17; 7]. Further, we do not need this assumption to hold on every \(\), since we are not necessarily interested in log-optimality w.r.t. fringe \(\) that are quite far away from \(^{*}\) -- in practice having this assumption hold for values of \(\) near \(^{*}\) suffices to develop an estimator \(\) that shrinks toward \(^{*}\) quickly. Now, we describe how much the growth rates deviates based on on how well \(q^{*}\) and \(r^{*}\) are estimated.

**Theorem 3**.: _Let \((_{t})\) be a sequence with \(^{}\)-regret \((_{t})\) and \((q_{t})\) and \((_{t})\) are sequences of labeling policies and predictors that are all predictable w.r.t. \((_{t})\). For a positive constant \(>0\), let \(q_{t}(x)>0\) for each \(t\) and \(x\) almost surely. Under Assumption 2 for the same \(\), the following bound holds:_

\[_{i=1}^{t}[_{t}^{}(q^{*},r^{*},^{*})- _{t}^{}(q_{t},_{t},_{t})]_{t} +_{i=1}^{t}O([|q(X_{t})-q_{}^{*}(X_{t})|]+[( _{t}(X_{t})-r^{*}(X_{t},))^{2}]).\]

We defer the proof to Appendix A.3. The proof idea follows a similar idea that of the regret bound in Kato et al.  for deriving an estimator that is close to the optimal estimator for the average treatment effect in an adaptive experimentation setup. Theorem 3 relates the estimation error of \(q_{}^{*}(x)\) and \(r^{*}(x,)\) to how quickly \(\) will be deemed "safe". Hence, if we have good estimates of those quantities, then we can produce an estimates \((_{t})\) that are small and close to \(^{*}\) while remaining safe. We will now describe some practical methods for calculating \(q_{t}\) and \(_{t}\), and demonstrate their empirical performance in some experiments.

## 4 Experiments

We use PyTorch to model our \((q_{t})\) and \((_{t})\), and we consider the following formulations.2

1. **Baseline labeling policies.** We have baseline labeling policies of labeling all data that arrives, and a policy that just randomly samples \(B\) proportion of samples to label -- these are denoted respectively as "all" and "oblivious".

2. **Pretrain**: We derive an estimate of \(r^{*}\) from a pretrained machine learning model, \(^{}\), to be our choice of predictor for all time steps. We also derive an estimate of \((x,)\), \(^{}(x,)\), from the pretrained model. We also learn a sequence of normalizing constants \((C_{t})\) s.t. the budget is satisfied. Our labeling policy in this case is \(q_{t}(x)=(x,_{t-1})/C_{t}\), where we want to optimize our policy for the previous best bound on \(^{*}\), \(_{t-1}\). We denote this method as "pretrain".
3. **Estimating \(q_{}^{*}\) and \(r^{*}\)**: We learn sequences of models \((_{t}^{})\) and \((_{t}^{})\) using the labeled data points. We preprocess the outputs from \(^{}\) and \(^{}\) to use as the input features to these models, respectively. Each of these sequences of models are then updated at every step. We also similarly learn a sequence of normalization constants \((C_{t})\) for deriving the final labeling policy \((q_{t})\).

We provide more details on how are methods are formulated in Appendix B. We run all our experiments on a 48-core CPU on the Azure platform, after using a GPU to precompute the predictions made by neural network models. We set \(=0.1\), \(=0.05\), and \(B=0.3\) for all our experiments.

### Numerical simulations

We have a simple data generating process of sampling \(P_{t}\) and let \(Y_{t} X_{t}(X_{t})\). This simulates the setting we have with our real data where we have an accurate pretrained classifier that have a probability estimate of \(Y_{t}\) of being 0 or 1. We let our covariates \(X_{t}=P_{t}\). As a result, our risk function is the false positive rate \(r_{}(X,Y,)\{X,Y=0\}\). We run 100 trials where each trial runs until 2500 labels are queried. We compare our methods based on their label efficiency, i.e., how close is \(_{t}\) to \(^{*}=1-\) after a set number of queried labels. In Figure 2, we plot the average \(_{t}\) reached after a given number of labels queried across trials. The shaded areas denotes pointwise 95% confidence intervals on the uncertainty of the average estimate. We can see that the "pretrain" and "learned" methods outperform both the "all" and "oblivious" strategies uniformly numbers across labels queried. In Figure 1(a), we show the average rate of safety violations, i.e., the average proportion of trials that \(_{t}\) was unsafe and \((_{t})>\) at any time step. We can see that all methods control the desired safety violation rate at the predetermined level \(\).

### Imagenet

We also evaluate our methods on the Imagenet dataset , and we used the pretrained neural network classifiers from Bates et al.  to provide estimates of the class probabilities.

Since Imagenet is a classification task with label support on \(=\), our goal is to ensure that the miscoverage rate of the true class is controlled. We follow the same setup as descibed in the introduction, i.e., with our risk measure \(r\) specified according to (1). For Imagenet, we reshuffle our dataset for each trial, and run each method till we have queried 3000 labels.

Figure 2: Experimental results for different methods for our numerical simulation setup. We can see that “pretrain” and “learned” perform better by getting lower average \(_{t}\) uniformly across number of labels queried — the dotted line in Figures 1(b) and 1(c) is \(^{*}=0.5578\). Each method also has low safety violation rate, i.e., is below the dotted line of \(=0.05\) in Figure 1(a).

In Figure 3, we plot the average \(_{t}\) across trials. Once again, we can see that the "pretrain" and "learned" methods outperform both the "all" and "oblivious" strategies here as well. On Imagenet the average safety violation rate is also controlled as well under the predetermined level of \(=0.05\).

## 5 Additional related work

Casgrain et al.  provide anytime-valid sequential tests for identifiable functions, which result in similar hypotheses being tested as this paper albeit with equality instead of equality. They, in addition to other recent work [25; 24; 31], using regret bounds for betting-based e-processes to show either derivations for the growth rate of a betting strategy w.r.t. to the optimal growth rate. However, none of these settings incorporate the ability to perform adaptive sampling or inverse propensity weights. Prior work in anytime-valid inference have included inverse propensity weights have been for off policy evaluation , adaptive experimentation , or estimating the weighted mean of a finite population . However, none of these works explicitly characterize deviation in the sampling policy away from the optimal sampling policy ultimately affects the growth rate as we do in Theorem 3.

Our analysis of power and regret for our algorithm is quite similar to methods in adaptive experimentation for average treatment effect estimation [14; 17] that attempt to derive a no regret treatment policy and outcome regressor that produces an estimator with a variance that approaches the variance of the optimal estimator. Unlike the adaptive experimentation setting, however, we have an additional label budget constraint on our formulation that results in a different optimal policy.

## 6 Conclusion, limitations, and future work

We have shown that we can extend the RCPS formulation to be anytime-valid, and retain validity and increase label efficiency in an active learning setting. We use the theory of betting and e-processes to develop this framework and show it is verifiably safe, and we verified this with our experimental results. We have primarily considered the i.i.d. setting here for anytime-valid calibration, and one key area in which one can extend this line of work is to account for distribution shift during test time. The empirical Bernstein supermartingales in Waudby-Smith et al.  can likely be used to extend our framework control risk in an average sense, but stronger guarantees could be made about the provided risk control if more realistic assumptions are made about the nature of the distribution (e.g., covariate shift, label shift, etc). It may also be possible extend a notion of adaptive conformal inference (ACI) [11; 12] to anytime-valid risk control. Another limitation of this work is the bounded label policy assumption (i.e., Assumption 2) and existence assumption in Proposition 5. We believe that more careful analysis can get rid of these assumptions in future work.