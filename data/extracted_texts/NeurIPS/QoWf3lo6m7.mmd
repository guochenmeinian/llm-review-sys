# Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics

Hanlin Zhu

UC Berkeley

hanlinzhu@berkeley.edu

&Baihe Huang

UC Berkeley

baihe_huang@berkeley.edu

&Shaolun Zhang

UC Berkeley

shaolun_zhang@berkeley.edu

&Michael Jordan

UC Berkeley

jordan@cs.berkeley.edu

&Jiantao Jiao

UC Berkeley

jiantao@berkeley.edu

&Yuandong Tian

Meta AI

yuandong@meta.com

&Stuart Russell

UC Berkeley

russell@cs.berkeley.edu

Equal contributions.

###### Abstract

Auto-regressive large language models (LLMs) show impressive capacities to solve many complex reasoning tasks while struggling with some simple logical reasoning tasks such as inverse search: when trained on "\(A B\)" (e.g., _Tom is the parent of John_), LLM fails to directly conclude "\(B A\)" (e.g., _John is the child of Tom_) during inference even if the two sentences are semantically identical, which is known as the "reversal curse". In this paper, we theoretically analyze the reversal curse via the training dynamics of (stochastic) gradient descent for two auto-regressive models: (1) a bilinear model that can be viewed as a simplification of a one-layer transformer; (2) one-layer transformers under certain assumptions. Our analysis reveals that for both models, the reversal curse is a consequence of the (effective) model weights _asymmetry_, i.e., the increase of weights from a token \(A\) to token \(B\) during training does not necessarily cause the increase of the weights from \(B\) to \(A\), which is caused by the training dynamics under certain choice of loss function and the optimization space of model parameters. Moreover, our analysis can be naturally applied to other logical reasoning tasks such as chain-of-thought (COT), which provides a new perspective different from previous work that focuses on expressivity. Finally, we conduct experiments to validate our theory on multi-layer transformers under different settings. Our code is available at https://github.com/marlo-z/reversal_curse_analysis/.

## 1 Introductions

Large language models (LLMs) have shown great performance in solving complex reasoning tasks that require multiple reasoning steps through in-context learning (ICL), such as zero-shot learning , few-shot learning , or via further fine-tuning . However, without the above inference-time techniques or model fine-tuning (probably combined with data manipulations), an auto-regressive LLM might struggle with simple logical reasoning tasks that require multiple reasoning steps learned during training separately , where the reversal curse  serves as a well-known example.

The reversal curse refers to the phenomenon that an auto-regressive LLM that learns "\(A B\)" (e.g., _Tom is the parent of John_) during training fails to generalize to the reverse direction "\(B A\)" (e.g., _John is the child of Tom_) even if the pair of relationship "\(\)" and "\(\)" are reverse to each other and the two sentences are semantically identical. Although some previous works propose different methods to mitigate the reversal curse, including reversing the training dataset  and training on different objectives such as autoregressive blank infilling , these methods might negatively affect the model performance on other tasks since they either alter the dataset or the model architecture. Without dataset manipulation or changing the auto-regressive nature (causal structure) of the model, there are two other candidate solutions to mitigate the reversal curse.

First, one might constrain the model parameters to satisfy a higher-level regularity for specific relationships. For example, a reversal-type regularity can be viewed as a pair of relationships (\(\), \(\)) and two sets \(,\) such that a model trained on "\(A B\)" will also increase its probability of "\(B A\)" for all \(A,B\), which induces a subspace of model parameters that satisfy this regularity. If one can train the model within this subspace, then training on "\(A B\)" can, by definition, help to learn "\(B A\)". However, for a general LLM, it is extremely challenging to find the subspace and manually hard-code the constraint during optimization even for one pair of relationships, not to mention there are numerous relationships. Since it is intractable to manually hard-code the constraints to the model parameter, one can alternatively expect the model to learn the higher-level regularity by training samples under unconstrained optimization. However, this is also hard, according to our analysis, through the popular cross-entropy (CE) loss that aims to maximize the next token prediction probability for the models studied in our paper.

Second, one can use a different loss function which is "symmetric", rather than the popular CE loss. However, the "symmetric" loss might drive the model to learn meaningless sentences. For example, when trained on the sentence "John is tall", a "symmetric" loss function might drive the model to learn "tall is John", which is not what we expect. To prevent the model from the above undesired behavior, in practice, CE loss is still widely-used.

Therefore, in this paper, we analyze the reversal curse via training dynamics of the widely-used unconstrained optimization for the CE loss. We summarize our main contributions as follows:

* We theoretically analyze the reversal curse where training or test sequences have the form "\(A B\)" or "\(B A\)" via training dynamics of (stochastic) gradient descent under two auto-regressive models: a bilinear model (Section3) and one-layer transformers under certain assumptions similar to  (Section4). The analysis of both models reveals that the widely-used unconstrained optimization for CE loss leads to model weights _asymmetry_, i.e., the increase of (effective) weights (after reparameterization) from the token \(A\) to token \(B\)1 during training does not necessarily cause the increase of the weights from \(B\) to \(A\), which further causes the reversal curse. Although the (effective) weights from \(A\) to \(B\) and from \(B\) to \(A\) might be related to some extent due to reparameterization, their correlation is weak and thus show asymmetry as empirically verified in Section5. * The techniques we used to analyze the reversal curse can be applied to other logical reasoning tasks. In particular, we use the above framework to analyze chain-of-thought (COT) , and we show that a model trained on "\(A B\)" and "\(B C\)" separately struggles to directly conclude "\(A C\)" without COT even if it is logically true (Section4.2). Different from the previous work  that theoretically studies COT through the expressivity of transformers, our work provides a new perspective through training dynamics.
* We also empirically validate our theoretical results on multi-layer transformers (Section5).

The _asymmetry_ of auto-regressive model weights caused by widely-used unconstrained optimization for CE loss indicates that auto-regressive LLMs might not automatically deduce certain types of conclusions using separate knowledge learned during training under current popular training paradigms: to make a model predicting token \(B\) where the input token is \(A\), the model might need to see \(B\) following \(A\) in the same sequence during the training set. This also highlights the importance of ICL, data augmentation, or planning for LLMs with the current popular causal transformer-based structures to solve complex reasoning tasks.

### Related works

LLM Reasoning.The strong performance of LLMs on reasoning tasks [3; 7; 16; 2; 17; 4; 18; 19] has prompted many studies on the reasoning capabilities of LLMs.  argues that transformers perform implicit Bayesian inference in ICL.  shows that transformers implement a specific type of circuits called "induction heads" that are key to the ICL abilities of LLMs.  proves that causal structures are encoded in transformer layers during the training dynamics.  identifies a backward chaining mechanism of transformers in deductive reasoning. Apart from in-context reasoning, LLMs still demonstrate limitations in other types of reasoning tasks [24; 25; 26].

Reversal Curse. identifies the phenomenon of reversal curse. This drawback of LLMs is also demonstrated in .  studies a similar phenomenonin which LLMs face difficulty in manipulating already learned knowledge. Several paper studies eliminating the reversal curse by extending causal attention to bidirectional attention , training on reversed samples , permuting semantic units , or introducing reverse logic data . Given all the empirical works, theoretical analysis of the reversal curse phenomenon remains scarce.

Expressivity of LLMs.There is a long line of works [29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 21; 45] studying the behavior of LLMs through the expressivity of transformers. It has been shown that transformers can implement simple functions such as sparse linear functions, two-layer neural networks, and decision trees , gradient descent [37; 44; 46], automata , Turing machines , variational inference , and bandit algorithms . Different from  that study COT via expressivity, we analyze reversal curse and COT via training dynamics.

Training dynamics of LLMs.There are rich literatures in the optimization of attention layers [51; 52; 53; 54; 55; 56; 57]. [58; 59] study the dynamics of a single linear attention layer in in-context linear regression.  proves convergence of one-layer transformers in random feature regime.  shows the convergence of gradient descent on one-layer transformers in in-context linear regression with orthogonal data.  studies the convergence of one-layer transformers in a class of next-token prediction tasks.  studies training dynamics of multi-layer transformers.  studies gradient descent on a class of two-layer transformers in in-context learning tasks with latent causal structures. Our paper studies the reversal curse via training dynamics under both bilinear settings and one-layer transformers. For one-layer transformers, we use the same framework as  without the need for certain technical assumptions such as long input sequences, different learning rates for different parameters (except for Appendix C.3), or weak correlations that are required for . Besides, we focus on the generalization ability of models for logical reasoning tasks while  mainly focus on optimization, and we identify the asymmetry and intransitivity properties of model weights, which are the core reasons for the failure of LLM for certain types of logical reasoning. Moreover, our analysis of the bilinear model only requires the embedding to be almost orthonormal, while  essentially assumed the embedding vectors to be fixed and one-hot.

## 2 Preliminaries

Basic notations.For any integer \(N>0\), we use \([N]\) to denote the set \(\{1,2,,N\}\). Let \(\), \(\) denote the set of real numbers and natural numbers, respectively. For real variables \(x_{1},,x_{n}\), we use \((x_{1},,x_{n})\) to denote the polynomial of \(x_{1},,x_{n}\). We use \(f(n) g(n)\) if there exists a constant \(C>0\) s.t. \(f(n) Cg(n), n\); we say \(g(n) f(n)\) if \(f(n) g(n)\).

We use \(_{i}\) to denote one-hot vectors where only the \(i\)-th entry of \(_{i}\) equals one and all other entries are zero. We use \(\) to denote all-one vectors, \(\) to denote zero vectors or zero matrices, and \(I\) to denote the identity matrix. We will also add subscripts when we want to explicitly show the dimension, such as \(_{d}\), \(I_{d}\) for \(d\)-dimensional zero vector and \(d d\) identity matrix. We use \(\) to denote tensor product of vectors or matrices and use \(^{ 2}\) and \(A^{ 2}\) to denote \(\) and \(A A\) for vector \(\) and matrix \(A\).

We use \((,)\) (or adding subscripts such as \(_{d}(,)\) if we want to show dimensions explicitly) to denote the (multi-variate) Gaussian distribution with mean \(\) and covariance \(\). Also, we use \(()\) to denote the set of distributions over a set \(\) and use \([]\) to denote expectation. For any dataset \(=\{x_{1},x_{2},,x_{n}\}\) where \(x_{i}\) and a function \(f:\), we define the empirical expectation over the dataset as \(_{}[f]=_{i=1}^{n}f(x_{i})\). See additional notations in Appendix A.

Auto-regressive models.Define the vocabulary \(=[M]\) for a positive integer \(M>0\) which is the size of the vocabulary. Let \(x=(x_{1},x_{2},,x_{T})\) be a sequence of tokens of length \(T\) where each token \(x_{t},~{} t[T]\). See Table 1 for notations of different tokens used in Section 4. We study auto-regressive models \(p_{}(|x)()\) parameterized by \(\) that take the sequence \(x\) as input and predict the distribution of the next token \(x_{T+1}\). For both models that we study in this paper, the next token probability is modeled as the softmax applied to the logits \(l_{}(|x)^{M}\) of each token in the vocabulary, i.e., \(p_{}(y|x)=(y|x))}{_{v}(l_{ }(v|x))},~{}~{} y\). Also, each token \(v\) has a corresponding (fixed or learnable) embedding vector \(_{v}^{d}\).

## 3 Bilinear Models

We start analyzing the reversal curse under bilinear models, which can be viewed as simplified one-layer transformers with input length one and decoder layer only. Also, in this section, we assume the embeddings of each token are fixed, so we directly use the embedding vector to represent a token.

Datasets.Assume the vocabulary has size \(m\) where each token \(v_{1},v_{2},,v_{m}}{{}} _{d}(0_{d},I_{d})\). Let \(=\{v_{1},,v_{m}\}\) and let \(=\{x_{1},,x_{n}\}\) and \(=\{y_{1},,y_{n}\}\) be disjoint random subsets of \(\). Assume all training and test sequences have a length of two. For any \(2 i n\), the training dataset contains both sequence \((x_{i},y_{i})\) and \((y_{i},x_{i})\). In addition, the training set contains \((x_{1},y_{1})\) while the test set only contain one example \((y_{1},x_{1})\). During training, the model learns both \((x_{i},y_{i})\) and \((y_{i},x_{i})\) for \(i 2\) to conclude that \((x_{i},y_{i})\) is equivalent to \((y_{i},x_{i})\). For example, \(\) is a set of names and \(\) is a set of books. The sequence \((x_{i},y_{i})\) means "\(x_{i}\) is the author of \(y_{i}\)", and the sentence \((y_{i},x_{i})\) means "\(y_{i}\) is written by \(x_{i}\)". We test whether the model is able to infer an unseen sequence \((y_{1},x_{1})\) given the training data which includes the other direction \((x_{1},y_{1})\).

Bilinear model.We consider a bilinear model parameterized by \(^{d d}\) of which the input contains single token \(x\). The logits of the next token \(y\) is defined as \(l_{}(y|x)=x^{} y\) which is bilinear in \(x\) and \(y\), and thus the next token probability is \(p_{}(y|x)=(y|x))}{_{v}(l_{ }(v|x))}\). The training loss for the bilinear model is the cross-entropy loss \(()=(_{i=1}^{n}- p_{}(y_{i}| x_{i})+_{i=2}^{n}- p_{}(x_{i}|y_{i}))\) and the test loss (reversal loss) is \(^{}()=- p_{}(x_{1}|y_{1})\). We study the training dynamics of gradient flow \(}{dt}=-(_{t})\) with the initialization \(_{0}\) that can be either randomly sampled from \((^{ 2},^{2}I^{ 2})\) or set as a pretrained parameter satisfying \(<p_{_{0}}(y_{i}|x_{i}),p_{_{0}}(x_{i}|y_{i})<\) for all \(i[n]\). The following theorem shows a separation during training dynamics.

**Theorem 1** (Separation of training dynamics (informal statement of Theorem 5)).: _Fix any \(,(0,1)\). For small \(\) and \(d(n,m,1/,(1/))\), with probability at least \(1-\), we have_

\[^{}(_{t})/^{}(_{0}) ((_{t})/(_{0}))^{},~{} t  0.\]

Theorem 1 shows that the reversal loss is lower bounded by the training loss. Note that for large \(d\) and small \(\) close to 0, \(((_{t})/(_{0}))^{}\) is close to 1 and thus \(^{}(_{t})^{}( _{0})\) which implies that \(p_{}(x_{1}|y_{1})\) remains small during training. We summarize the above argument in Theorem 2.

**Theorem 2** (Lower bound of reversal loss (informal statement of Theorem 6)).: _Fix arbitrary \(c>0\) and \(C(m/2)\). Suppose \(\) is small and \(d(n,m,(1/), c,1/ C)\). With probability at least \(1-\), it holds that \(^{}(_{}) C\), where \(\) denotes the first time such that \((_{t}) c\)._

   Enities & Forward & Backward & Direct & Indirect & Others \\  A, B, C, A\({}_{i}\), B\({}_{i}\), C\({}_{i}\) & \(\) & \(\) & \(\) & \(\) & R\({}_{1}\), R\({}_{2}\) \\   

Table 1: Notations for tokens in Section 4. “\(\)” and “\(\)” denote forward and backward relationships for the reversal curse. “\(\)” and “\(\)” denote direct and indirect implication for COT. R\({}_{1}\) and R\({}_{2}\) are relationship tokens in Section 4.3. A, B, C, A\({}_{i}\), B\({}_{i}\), C\({}_{i}\) denote tokens representing entities.

The proofs of Theorems1 and 2 are deferred to AppendixB. Theorem2 implies that for large \(d^{2}\), while the training loss can be trained to be arbitrarily small, the reversal loss remains large. In other words, the model fails to infer an unseen sequence \((y_{1},x_{1})\) given the training data which includes the other direction \((x_{1},y_{1})\). Furthermore, even if the model is fine-tuned on new data from a pre-trained parameter \(_{0}\) that initially grasps the concept of reversal and satisfies \(<p_{_{0}}(y_{i}|x_{i}),p_{_{0}}(x_{i}|y_{i})<\) for new data, it fails to extend this understanding to new, unseen data.

A core reason that the reversal curse happens on the above bilinear model is that the parameter matrix \(_{t}\) is asymmetric. Consequently, the logits \(l_{_{t}}(y|x)=x^{}_{t}y\) and \(l_{_{t}}(x|y)=y^{}_{t}x\) generally differ. Consider a special case where each \(v_{i}\) is a one-hot vector. Then \(l_{}(y|x)=x^{} y=_{ij}\) and \(l_{}(x|y)=y^{} x=_{ji}\) for \(x=_{i},y=_{j}\). Training on \((x,y)\) can increase \(_{ij}\) but not \(_{ji}\), which means the model does not automatically learn the reversal direction \((y,x)\) from \((x,y)\). In Section4, we will show that for one-layer transformers, the reversal curse is mainly caused by the same reason, i.e., the asymmetry of the model weights.

## 4 One-Layer Transformers

In Section3, we analyzed the reversal curse under a bilinear model. In this section, we analyze the reversal curse for one-layer transformers in a similar setting to  via training dynamics. We also extend our analysis to chain-of-thought in Section4.2.

Basic notations.Let \(=[M]\) be the vocabulary. For any token \(x[M]\), we also use the corresponding one-hot vector \(=_{x}^{M}\) to represent it. Let \(U=[_{1},_{2},,_{M}]^{}^{M d}\) be the embedding matrix, where \(_{x}^{d}\) is the embedding of token \(x[M]\). Note that \(U^{}=_{x}\). Consider the \(i\)-th training sample in the dataset, \(x[i]=(x_{1}[i],,x_{T[i]}[i],x_{T[i]+1}[i])\), a sequence of tokens of length \(T[i]+1\). Here, \(x_{T[i]+1}[i]\) is the next token (or equivalently the label) to be predicted, \(x_{T[i]}[i]\) is the query token, and \((x_{1}[i],,x_{T[i]-1}[i])\) are contextual tokens. For token \(x_{t}[i]\), we also use its one-hot vector \(_{t}[i]=_{x_{t}[i]}^{M}\) to represent it. Define the contextual token matrix \(X[i]=[_{1}[i],,_{T[i]-1}[i]]^{}^{(T[i]-1)  M}\). We omit all \(i\) in notations when the context is clear.

One-layer transformer.For a training sample \(x=(x_{1},,x_{T},x_{T+1})\), its contextual token matrix \(X=[_{1},,_{T-1}]^{}\) and thus \(XU=[_{x_{1}},,_{x_{T-1}}]^{}\) contains the contextual token embeddings. We study one-layer transformers in the same setting as . In particular, for an input token sequence \((x_{1},,x_{T})\), after the one-layer self-attention, we can obtain \(}_{T}=U^{}(X^{}_{T})\) where \(b_{tT}=_{x_{T}}^{}W_{Q}W_{K}^{}_{x_{t}}/)}{_{r^{}=1}^{r-1}(_{x_{T}}^{}W_{Q}W_{K}^{}_{x_{t}} /)}\), \(_{T}=[b_{1T},,b_{T-1,T}]^{}\) contains attention scores (after softmax) that query token \(x_{T}\) attend to each contextual token3, \(()=/\|\|_{2}\) is the \(_{2}\)-normalization operator, and \(W_{Q},W_{K}^{d d_{k}}\) are trainable query and key matrices respectively. The logit of \(x[M]\) is then calculated by a decoder layer, i.e., \(l_{}(x|x_{1},,x_{T})=_{x}^{}W_{V}}_{T}\), where \(\) encodes all parameters in the transformer, and \(W_{V}^{d d}\) can be viewed as a reparameterization of value and output matrices. Finally, the next token prediction probability is obtained by

\[p_{}(x|x_{1},,x_{T})=(x|x_{1},,x_{T}))}{ _{x^{}[M]}(l_{}(x|x_{1},,x_{T}))}=_{x}^{}W_{V}}_{T})}{_{x^{}[M]}(_{x^{ }}^{}W_{V}}_{T})}.\]

We use the cross-entropy loss function to train the model over the whole training set \(_{}\), i.e., \(_{U,W_{K},W_{Q},W_{V}}J_{_{}}[  p_{}(x_{T+1}|x_{1},,x_{T})]\).

Reparameterization.Similar to , we define \(Z=UW_{Q}W_{K}^{}U^{}/^{M M}\) and \(Y=UW_{V}^{}U^{}^{M M}\) and are interested in their dynamics after reparameterization. Then, the attention score (after softmax) and next token probability become

\[b_{tT}=_{T}^{}Z_{t})}{_{t^{}=1}^{T-1}( _{T}^{}Z_{t^{}})},\;\;p_{}(x|x_{1},,x_{T})= ^{}Y^{}(X^{}_{T}))}{ _{x^{}}(^{}Y^{}(X^{}_ {T}))},\] (1)and the objective can be written as

\[_{Y,Z}J=_{_{}}[_{T+1}^{}Y^{} (X^{}_{T})-_{x^{}[M]}^{}Y^{ }(X^{}_{T})].\] (2)

Let \(_{Y}\), \(_{Z}\) be the learning rate of matrices \(Y\) and \(Z\) respectively. Then the gradient of \(Y\) and \(Z\) can be characterized by the following lemma:

**Lemma 1** (Gradient of \(Y\) and \(Z\) for 1-layer transformer, Lemma 1 of ).: _The gradient of \(Y\) and \(Z\) w.r.t. (2) of batch size 1 and learning rate \(_{Y}\) and \(_{Z}\) can be written as_

\[=_{Y}(X^{}_{T})(_{T+1}-)^{ },\ \ =_{Z}_{T}(_{T+1}-)^{}Y^{}_{T}}^{}}{\|X^{}_{T}\|_{2}}X^{}(_{T})X,\] (3)

_where \(P_{}^{} I-^{}/\|\|_{2}^{2}\) projects any vector to orthogonal complement of \(\), \(=[_{1},_{2},,_{M}]^{}^{M}\) with \(=(Y^{}(X^{}_{T}))/^{ }(Y^{}(X^{}_{T}))\)._

Proof.: \(,\) can be obtained by direct calculation. One can refer to the proof of Lemma 1 of . 

### Main results for the reversal curse

In this section, we analyze the reversal curse where data points are three-token sentences "A \(\) B" or "B \(\) A". For each sentence, A and B are two distinct tokens that represent two entities, and "\(\)" and "\(\)" are two special tokens representing a pair of relationships inverse to each other.

Datasets.Let \(N_{}>0\), \(N_{}^{(1)}>0\) and \(N_{}^{(2)}>0\) and denote \(N_{}=N_{}+N_{}^{(1)}+N_{}^{(2)}\). Let \(_{i},_{i}, i[N_{}]\) be \(2N_{}\) distinct tokens representing distinct entities. Let \(\), \(\) be two additional different tokens that represent two inverse relationships. Specifically, we have \(_{i}_{i}\) and \(_{i}_{i}\) for all \(i[N_{}]\). For notation convenience, we define the following three index sets

\[_{}=[N_{}],_{}^ {(1)}=[N_{}+N_{}^{(1)}]_{},_{}^{(2)}=[N_{}]( _{}_{}^{(1)}).\]

The training set \(_{}\) consists of all \(_{i}_{i}\) and \(_{i}_{i}\) for \(i_{}\). In addition, \(_{}\) contains \(_{i}_{i}\) for \(i_{}^{(1)}\) and \(_{i}_{i}\) for \(i_{}^{(2)}\). For convenience, we let \(N=|_{}|\) to be the size of the training set. The test set \(_{}\) consists of \(_{i}_{i}\) for \(i_{}^{(1)}\) and \(_{i}_{i}\) for \(i_{}^{(2)}\). Under our construction of the dataset, the LLM will learn the relationship between \(_{i}\) and \(_{i}\) for \(i_{}\) in both directions to deduce that \(\) is reverse to \(\), and learn the relationship between \(_{i}\) and \(_{i}\) for \(i_{}^{(1)}_{}^{(2)}\) in one direction and will be tested for the other.

We use \(p_{}(_{i}|_{i})\) and \(p_{}(_{i}|_{i})\) to more compactly represent \(p_{}(x_{3}=_{i}|x_{1}=_{i},x_{2}=)\) and \(p_{}(x_{3}=_{i}|x_{1}=_{i},x_{2}=)\), respectively. Our goal is to prove through the training dynamics of one-layer transformers that the test probability remains negligible during training. In particular, we are interested in \(p_{}(_{i}|_{i}), i_{ }^{(1)}\) and \(p_{}(_{i}|_{i}), i_{ }^{(2)}\).

For convenience, we assume zero-initialization \(Y(0)=\) and \(Z(0)=\). This is the same as  and is reasonable since empirically, \(Y\) and \(Z\) are usually initialized as inner products of \(d\)-dimensional vectors with i.i.d Gaussian entries, and thus are almost zero (Lemma 8 in Appendix B). The following proposition shows the initial train/test probabilities are uniform over the vocabulary \(\).

**Proposition 4.1** (Initial probability under zero initializaion).: _Assume the transformer is under zero-initialization \((0)=(Y(0),Z(0))\) with \(Y(0)=\) and \(Z(0)=\). For any \(i[N_{}]\), we have_

\[p_{(0)}(_{i}|_{i})=p_{(0)}(_{i}|_{i})=1/M.\]

The proof is deferred to Appendix C.1.1. Proposition 4.1 shows that initially, the probability of predicting any B (or A, respectively) given any A \(\) (or B \(\), respectively) as input is uniform over the whole vocabulary. When \(Y(0)\) and \(Z(0)\) are not exactly \(\) but close to \(\), the initial prediction will still be close to the uniform distribution, which is similar to Lemma 6. Next we analyze the dynamics of \(p_{(t)}(_{i}|_{i})\) and \(p_{(t)}(_{i}|_{i})\).

**Proposition 4.2** (Next token probability).: _For input sequence \((x_{1},x_{2})\), the next token probability under parameters \((t)\) is \(p_{(t)}(x|x_{1},x_{2})=(Y(t)_{x_{1},x})/_{x^{}[M]} (Y(t)_{x_{1},x^{}})\), where \(Y(t)_{i,j}\) is the entry of the matrix \(Y(t)\) at row \(i\) and column \(j\)._The proof is also deferred to SectionC.1.1. According to Section4.2, the next token probability when the entity in the input is \(x_{1}\) is determined by the \(x_{1}\)-th row of the matrix \(Y(t)\). Another nice property indicated by Section4.2 is that we don't need to keep track of the dynamics of \(Z(t)\), which could greatly simplify the analysis. The following lemma shows the dynamics of \(Y(t)\).

**Lemma 2** (Dynamics of \(Y(t)\)).: _Assume we run SGD with batch size 14, and assume \(M 100\) and \(}_{Y}<1\). Let \(t}\) and let \(Y(t)_{i}\) denote the \(i\)-th row of \(Y(t)\) and \(Y(t)_{ij}\) denote the \((i,j)\)-th entry of \(Y(t)\). Then for training sequence \((x_{1},x_{2},x_{3})_{}\) at time \(t\), we have_

\[Y(t)_{x_{1},x_{3}}(M_{Y}t/N), Y(t )_{x_{1},x}-(M_{Y}t/N)/M, x x_{3},\]

_and for any test sequence \((x_{1},x_{2},x_{3})_{}\), we have \(Y(t)_{x_{1},x}=0, x[M]\)._

The proof of Section2 is presented in SectionC.1.2. Lemma2 implies the asymmetry of the model weights \(Y(t)\): for two tokens \(x_{1},x_{3}\), when \(x_{1}\) appears as a contextual token and \(x_{3}\) serves as the next token in the same training sequence, the model weights \(Y(t)_{x_{1},x_{3}}\) gets increased during training while \(Y(t)_{x_{3},x_{1}}\) will not get increased. Combining Section4.2, we can obtain our main theorem for the reversal curse.

**Theorem 3** (Reversal curse).: _Assume we run SGD with batch size 1, and assume \(M 100\) and \(}_{Y}<1\). Let \(t}\) denote the time step which also satisfies \( t(NM/_{Y})\). For training sequence \((x_{1},x_{2},x_{3})_{}\) at time \(t\), we have_

\[p_{(t)}(x_{3}|x_{1},x_{2}) 1-(M-1)(M_{Y}t/N)^{-c} 1, \;\;t\]

_for some constant \(c>0\), and for any test sequence \((x_{1},x_{2},x_{3})_{}\) that is not included in the training set \(_{}\), we have \(p_{(t)}(x_{3}|x_{1},x_{2}) 1/M\)._

Theorem3 shows that although the direction presented in the training set can be learned nearly perfectly, the model's next token prediction of the reverse direction is almost a random guess. The proof is deferred to SectionC.1.3. We also empirically validate the above results for multi-layer transformers in Section5.

### Chain-of-thought

In this section, we extend our analysis in Section4.1 to study other logical relationships. In particular, we study chain-of-thought (COT)  and show its importance via training dynamics. COT encourages LLMs to output a series of intermediate reasoning steps to increase their performance. Consider the simplest example, where the model learns two facts that \(\) and \(\), and we want to test whether the model is able to directly conclude that \(\). COT indicates that if an LLM is only trained on \(\) and \(\), it would be easier for the model to deduce \(\) during the inference time if the model can first output the intermediate steps \(\) and \(\), instead of directly predicting the next token \(\) given the input "\(\)". The failure of directly deducing \(\) is also empirically observed by .

Theoretically,  shows the importance of COT for some complex reasoning tasks through the lens of the expressivity of transformers. In this section, we show the importance of COT through a different angle, i.e., training dynamics. We show that for the above simplest two-step reasoning, without COT, the model is not able to directly predict \(\) given the input "\(\)" even if it learns \(\) and \(\).

**Theorem 4** (Importance of chain-of-thought, informal statement of Section7).: _Under certain assumptions as stated in Section7, for any \(_{i},_{i},\)\(_{i}\) s.t. \(_{i}_{i}\) and \(_{i}_{i}\) are in the training set but \(_{i}_{i}\) is not, we have_

\[p_{(t)}(_{i}|_{i}) 1, p_{(t)}(_{i}| _{i}) 1, p_{(t)}(_{i}|_{i}  1/M,\;\;t.\]

We defer the details of the dataset construction and proof to SectionC.2. Theorem4 shows that although the LLM learns \(_{i}_{i}\) and \(_{i}_{i}\) nearly perfectly, it cannot directly deduce \(_{i}_{i}\). Analogous to the asymmetry of causal transformer weights as we discussed in Section4.1, our analysis of COT reveals another property, i.e., intransitivity: training the weights associated with \(\) to \(\) and \(\) to \(\) does not necessarily increase the weights associated with \(\) to \(\).

We also emphasize that the model fails to directly deduce \(_{i}_{i}\) when the two intermediate steps \(_{i}_{i}\) and \(_{i}_{i}\) are trained _separately_. If the two steps are concatenated into a single training sequence, it is possible that the model learns \(_{i}_{i}\) directly .

### Roles of the attention score matrix

During the analysis of Sections 4.1 and 4.2, we show that the reversal curse and the importance of COT are largely due to the asymmetry and intransitivity of causal transformer weights (in our case, the weight matrix \(Y(t)\)). However, it seems that the dynamics of the attention score matrix \(Z(t)\) do not impact the model performance. Below, we briefly discuss the role of the attention score matrix \(Z(t)\).

In (1), the attention score is used to calculate the weights \(b_{tT}\), where a contextual token \(x_{t}\) with a larger attention score attended by the query token \(x_{T}\) has a larger weight. Note that we use the same formulation as the previous work  where the query token will not attend to itself. Therefore, for a three-token training sequence, the weights \(b_{12}\) is always one since there is only one contextual token \(x_{1}\), no matter whether the value of the attention score is high or low.

However, consider a slightly different setting, where the relationship is represented by two tokens. In that case, \(x_{1}=_{i},x_{2}=_{1},x_{3}=_{2},x_{4}=_{i}\), and there are two contextual tokens \(_{i}\) and \(_{1}\). The role of the attention score is then to select the important token, i.e., \(_{i}\), by putting more weights on it. Theorem 2 of  showed that under certain assumptions, the query token \(_{2}\) will attend more to "distinct tokens" \(_{i}\) and less to the "common token" \(_{1}\). Therefore, the query token \(_{2}\) will eventually put all weights to \(_{i}\), and the remaining analysis remains the same as in Sections 4.1 and 4.2. See Appendix C.3 for a more rigorous analysis.

## 5 Experiments

In this section, we conduct experiments to further validate our theoretical results in Section 4 on multi-layer transformers. We show experimental results of the reversal curse in this section and COT in Appendix D. Note that in Sections 3 and 4, we theoretically proved the reversal curse for both the bilinear model and one-layer transformer under certain assumptions. Now, we empirically show that the reversal curse still happens even for multi-layer transformers. In Appendix E.2.3, we also provide empirical results that the reversal curse does not happen in ICL settings.

Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be \(_{i}\) when the input is “\(_{i}\)”, or to be \(_{i}\) when the input is “\(_{i}\)”. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.

Dataset construction.Below, we describe how we generate our synthetic dataset for experiments on the reversal curse. We choose the vocabulary \(=\{0,1,,N\}\) for a specified \(N>0\). We randomly sample two disjoint sets of entities \(,\) with \(||=||=||/4\), and reserve two additional tokens for relationships \(\) and \(\), respectively. 5 Next, we specify a bijection from \(\) to \(\) uniformly at random. For each \(_{i}\) and its corresponding \(_{i}\), we can obtain a pair of sequence (\(_{i}_{i}\), \(_{i}_{i}\)). We split the set of all pairs into training pairs and validation pairs. For each training pair, both sequences will be included in the training set, while for the validation pair, we randomly select one sequence for the training set and the other for the validation set. Therefore, the model will learn both directions for the training pairs and only one direction for each validation pair while being tested in the unseen direction.

Model architectures.We train multi-layer transformers based on GPT-2 architecture . Figure 1 shows the results where the model has 24 layers, 12 attention heads per layer, uses absolute positional encoding, and we choose the vocabulary size of 800. The training set size is 340, and the validation set size is 60 (resulting from 140 training pairs and 60 validation pairs). We also conducted experiments with various model configurations and vocabulary sizes in Appendix E.2. Besides, all hyperparameters and different model configurations are presented in Appendix E.1.

Results.Figure 1 shows that during the training, the next token probability for training data increases a lot while the next token probability for validation data remains unchanged or gets even smaller. This is consistent with our theoretical results of Theorem 3.

According to our theoretical analysis, the reversal curse happens due to the asymmetry of model (reparameterized) weights (i.e., logits of a token given another token as input), and we also empirically validate the asymmetry for multi-layer transformers. Figure 2 shows the model weights from a token \(x_{1}\) to \(x_{3}\) is trained large for a training sequence \((x_{1},x_{2},x_{3})\) as represented by the diagonals of the

Figure 2: Visualization of the weights (logits) of the model with default configurations trained after 3000 epochs for the reversal curse experiment. For the top-left matrix, the \(i\)-th row corresponds to an entity token \(_{i}\) for a training pair, and the \(i\)-th column corresponds to an entity token \(_{i}\) for a training pair. The \((i,j)\)-th entry represents the model weights from the token \(_{i}\) to \(_{j}\), i.e., the logits of \(_{j}\) when the input sequence consists of only \(_{i}\). Similarly, for the bottom-left matrix, the row corresponds to the input entity tokens of the seen direction (the direction included in the training set) of validation pairs, and the column corresponds to output entity tokens. The two matrices on the right are obtained by swapping row tokens and column tokens of their corresponding left matrices. Note that the diagonals of the bottom-right matrix are all close to zero, while the diagonals of other matrices all have large values. This implies that if a pair of tokens \((,)\) only appear in the training set in one direction, then the model weights associated with the other direction will hardly get trained.

first three matrices, while the weights from a token \(x_{1}\) to \(x_{3}\) remains nearly zero for a validation sequence \((x_{1},x_{2},x_{3})\) as represented by the diagonals of the last matrix, which is consistent with Lemma 2. This implies that if a pair of tokens \((,)\) only appear in the training set in one direction, then the model weights associated with the other direction will hardly get trained.

## 6 Conclusions

In this paper, we study the reversal curse theoretically via training dynamics of (1) a bilinear model, which is a simplification of the one-layer transformer; (2) one-layer transformers under certain technical assumptions similar to . Our theoretical results suggest that a core reason the reversal curse happens in auto-regressive LLMs is the asymmetry of the model weights, and we apply our technique to prove the necessity of COT for one-layer transformers, which is mainly due to the intransitivity of model weights. The asymmetry and intransitivity of model weights caused by unconstrained optimization of CE loss indicate that an auto-regressive LLM might mainly focus on learning text sequences during training _separately_ instead of automatically deducing indirect conclusions under the current popular training paradigms. This highlights the importance of ICL, data augmentation, or planning for current auto-regressive LLMs to solve complex reasoning tasks.

As for future directions, it would be interesting and important to study: (1) What is a unified way to characterize and study the reversal curse, COT, and other similar logical reasoning tasks? (2) Our paper mainly focuses on three-token sequences, where each entity or relationship is represented by a single token. While we empirically explored the setting where each entity might consist of multiple tokens and distinct entities might share a few tokens, it would be interesting to analyze the multiple-token setting theoretically. (3) We theoretically analyzed the bilinear model and one-layer transformer, and it would be an important future direction to extend the analysis to multi-layer transformers.