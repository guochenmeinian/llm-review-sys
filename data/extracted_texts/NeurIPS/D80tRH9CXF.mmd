# Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors

Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

In recent years, there has been a significant growth in research focusing on minimum \(_{2}\) norm (ridgeless) interpolation least squares estimators. However, the majority of these analyses have been limited to an unrealistic regression error structure, assuming independent and identically distributed errors with zero mean and common variance. In this paper, we explore prediction risk as well as estimation risk under more general regression error assumptions, highlighting the benefits of overparameterization in a more realistic setting that allows for clustered or serial dependence. Notably, we establish that the estimation difficulties associated with the variance components of both risks can be summarized through the trace of the variance-covariance matrix of the regression errors. Our findings suggest that the benefits of overparameterization can extend to time series, panel and grouped data.

## 1 Introduction

Recent years have witnessed a fast growing body of work that analyzes minimum \(_{2}\) norm (ridgeless) interpolation least squares estimators (see, e.g., 2, 17, 27, and references therein). Researchers in this field were inspired by the ability of deep neural networks to accurately predict noisy training data with perfect fits, a phenomenon known as "double descent" or "benign overfitting" (e.g., 3, 4, 5, 29, 22, among many others). They discovered that to achieve this phenomenon, overparameterization is critical.

In the setting of linear regression, we have the training data \(\{(x_{i},y_{i})^{p}:i=1,,n\}\), where the outcome variable \(y_{i}\) is generated from

\[y_{i}=x_{i}^{}+_{i},\ i=1,,n,\]

\(x_{i}\) is a vector of features (or regressors), \(\) is a vector of unknown parameters, and \(_{i}\) is a regression error. Here, \(n\) is the sample size of the training data and \(p\) is the dimension of the parameter vector \(\).

In the literature, the main object for the theoretical analyses has been mainly on the out-of-sample prediction risk. That is, for the ridge or interpolation estimator \(\), the literature has focused on

\[(x_{0}^{}-x_{0}^{})^{2}\ |\ x_{1},,x_{n},\]

where \(x_{0}\) is a test observation that is identically distributed as \(x_{i}\) but independent of the training data. For example, Dobriban and Wager , Wu and Xu , Richards et al. , Hastie et al.  analyzed the predictive risk of ridge(less) regression and obtained exact asymptotic expressions under the assumption that \(p/n\) converges to some constant as both \(p\) and \(n\) go to infinity. Overall, they found the double descent behavior of the ridgeless least squares estimator in terms of the prediction risk. Bartlett et al. , Kobak et al. , Tsigler and Bartlett  characterized the phenomenon of benign overfitting in a different setting.

To the best of our knowledge, a vast majority of the theoretical analyses have been confined to a simple data generating process, namely, the observations are independent and identically distributed (i.i.d.), and the regression errors have mean zero, have the common variance, and are independent of the feature vectors. That is,

\[(y_{i},x_{i}^{})^{}[_{i}]=0, [_{i}^{2}]=^{2}<_{i}x_{i}.\] (1)

This assumption, although convenient, is likely to be unrealistic in various real-world examples. For instance, Liao et al.  adopted high-dimensional linear models to examine the double descent phenomenon in economic forecasts. In their applications, the outcome variables include S&P firms' earnings, U.S. equity premium, U.S. unemployment rate, and countries' GDP growth rate. As in their applications, economic forecasts are associated with time series or panel data. As a result, it is improbable that (1) holds in these applications. As another example, Spiess et al.  examined the performance of high-dimensional synthetic control estimators with many control units. The outcome variable in their application is the state-level smoking rates in the Abadie et al.  dataset. Considering the geographical aspects of the U.S. states, it is unlikely that the regression errors underlying the synthetic control estimators adhere to (1). In short, it is desirable to go beyond the simple but unrealistic regression error assumption given in (1).

To further motivate, we start with our own real-data example from American Community Survey (ACS) 2018, extracted from IPUMS USA . The ACS is an ongoing annual survey by the US Census Bureau that provides key information about the US population. To have a relatively homogeneous population, the sample extract is restricted to white males residing in California with at least a bachelor's degree. We consider a demographic group defined by their age, the type of degree, and the field of degree. Then, we compute the average of log hourly wages for each age-degree-field group, treat each group average as the outcome variable, and predict group wages by various group-level regression models where the regressors are constructed using the indicator variables of age, degree, and field as well as their interactions. We consider 7 specifications ranging from 209 to 2,182 regressors. To understand the role of non-i.i.d. regressor errors, we add the artificial noise to the training sample. See Appendix A for details regarding how to generate the artificial noise. In the experiment, the constant \(c\) varies such that \(c=0\) corresponds to no clustered dependence across observations but as a positive \(c\) gets larger, the noise has a larger share of clustered errors but the variance of the overall regression errors remains the same regardless of the value of \(c\). Figure 1 shows the in-sample (train) vs. out-of-sample (test) mean squared error (MSE) for various values of \(c\{0,0.25,0.5,0.75\}\). It can be seen that the experimental results are almost identical across different values of \(c\) especially when \(p>n\), suggesting that the double descent phenomenon might be universal for various degrees of clustered dependence, provided that the overall variance of the regression errors remains the same. It is our main goal to provide a firm foundation for this empirical phenomenon. To do so, we articulate the following research questions:

Figure 1: Comparison of in-sample and out-of-sample mean squared error (MSE) across various degrees of clustered noise. The vertical line indicates \(p=n\) (\(=1,415\)).

* How to analyze the out-of-sample prediction risk of the ridgeless least squares estimator under _general_ assumptions on the regression errors?
* Why does _not_ the prediction risk seem to be affected by the degrees of dependence across observations?

To delve into the prediction risk, suppose that \(:=[x_{0}x_{0}^{}]\) is finite and positive definite. Then,

\[(x_{0}^{}-x_{0}^{})^{2} x_{1}, ,x_{n}=(-)^{}(-) x_{1},,x_{n}.\]

If \(=I\) (i.e., the case of isotropic features), where \(I\) is the identity matrix, the mean squared error of the estimator defined by \([\|-\|^{2}]\), where \(\|\|\) is the usual Euclidean norm, is the same as the expectation of the prediction risk defined above. However, if \( I\), the link between the two quantities is less intimate. One may regard the prediction risk as the \(\)-weighted mean squared error of the estimator; whereas \([\|-\|^{2}]\) can be viewed as an "unweighted" version, even if \( I\). In other words, regardless of the variance-covariance structure of the feature vector, \([\|-\|^{2}]\) treats each component of \(\) "equally." The mean squared error of the estimator is arguably one of the most standard criteria to evaluate the quality of the estimator in statistics. For instance, in the celebrated work by James and Stein , the mean squared error criterion is used to show that the sample mean vector is not necessarily optimal even for standard normal vectors (so-called "Stein's paradox"). Many follow-up papers used the same criterion; e.g., Hansen  compared the mean-squared error of ordinary least squares, James-Stein, and Lasso estimators in an underparameterized regime. Both \(\)-weighted and unweighted versions of the mean squared error are interesting objects to study. For example, Dobriban and Wager  called the former "predictive risk" and the latter "estimation risk" in high-dimensional linear models; Berthier et al.  called the former "generalization error" and the latter "reconstruction error" in the context of stochastic gradient descent for the least squares problem using the noiseless linear model. In this paper, we analyze both weighted and unweighted mean squared errors of the ridgeless estimator under general assumptions on the data-generating processes, not to mention anisotropic features. Furthermore, our focus is on the finite-sample analysis, that is, both \(p\) and \(n\) are fixed but \(p>n\).

Although most of the existing papers consider the simple setting as in (1), our work is not the first paper to consider more general regression errors in the overparameterized regime. Chinot et al. , Chinot and Lerasle  analyzed minimum norm interpolation estimators as well as regularized empirical risk minimizers in linear models without any conditions on the regression errors. Specifically, Chinot and Lerasle  showed that, with high probability, without assumption on the regression errors, for the minimum norm interpolation estimator, \((-)^{}(-)\) is bounded from above by \(\|\|^{2}_{i c}_{i}()_{i=1}^{n} _{i}^{2}/n\), where \(c\) is an absolute constant and \(_{i}()\) is the eigenvalues of \(\) in descending order. Chinot and Lerasle  also obtained the bounds on the estimation error \((-)^{}(-)\). Our work is distinct and complements these papers in the sense that we allow for a general variance-covariance matrix of the regression errors. The main motivation of not making any assumptions on \(_{i}\) in Chinot et al.  and Chinot and Lerasle  is to allow for potentially adversarial errors. We aim to allow for a general variance-covariance matrix of the regression errors to accommodate time series and clustered data, which are common in applications. See, e.g., Hansen  for a textbook treatment (see Chapter 14 for time series and Section 4.21 for clustered data).

The main contribution of this paper is that we provide _exact finite-sample_ characterization of the variance component of the prediction and estimation risks under the assumption that \(X=[x_{1},x_{2},,x_{n}]^{}\) is _left-spherical_ (e.g., \(x_{i}\)'s can be i.i.d. normal with mean zero but more general); \(_{i}\)'s _can be correlated and have non-identical variances_; and \(_{i}\)'s are independent of \(x_{i}\)'s. Specifically, the variance term can be factorized into a product between two terms: one term depends only on the _trace_ of the variance-covariance matrix, say \(\), of \(_{i}\)'s; the other term is solely determined by the distribution of \(x_{i}\)'s. Interestingly, we find that although \(\) may contain non-zero off-diagonal elements, only the trace of \(\) matters, as hinted by Figure 1, and further demonstrate our finding via numerical experiments. In addition, we obtain exact finite-sample expression for the bias terms when the regression coefficients follow the random-effects hypothesis . Our finite-sample findings offer a distinct viewpoint on the prediction and estimation risks, contrasting with the asymptotic inverse relationship (for optimally chosen ridge estimators) between the predictive and estimation risks uncovered by Dobriban and Wager . Finally, we connect our findings to the existing results on the prediction risk [e.g., 17] by considering the asymptotic behavior of estimation risk.

One of the limitations of our theoretical analysis is that the design matrix \(X\) is assumed to be left-spherical, although it is more general than i.i.d. normal with mean zero. We not only view this as a convenient assumption but also expect that our findings will hold at least approximately even if \(X\) does not follow the left-spherical distribution. It is a topic for future research to formally investigate this conjecture.

## 2 The Framework under General Assumptions on Regression Errors

We first describe the minimum \(_{2}\) norm (ridgeless) interpolation least squares estimator in the overparameterized case (\(p>n\)). Define

\[y :=[y_{1},y_{2},,y_{n}]^{}^{n},\] \[ :=[_{1},_{2},,_{n}]^{ }^{n},\] \[X^{} :=[x_{1},x_{2},,x_{n}]^{p n},\]

so that \(y=X+\). The estimator we consider is

\[:=*{arg\,min}_{b^{p}}\{\|b\|:Xb=y\}=(X^{ }X)^{}X^{}y=X^{}y,\]

where \(A^{}\) denotes the Moore-Penrose inverse of a matrix \(A\).

The main object of interest in this paper is the prediction and estimation risks of \(\) under the data scenario such that the regression error \(_{i}\) may _not_ be i.i.d. Formally, we make the following assumptions.

**Assumption 2.1**.: (i) \(y=X+\), where \(\) is independent of \(X\), and \([]=0\). (ii) \(:=[ e^{}]\) is finite and positive definite (but not necessarily spherical).

We emphasize that Assumption 2.1 is more general than the standard assumption in the literature on benign overfitting that typically assumes that \(^{2}I\). Assumption 2.1 allows for non-identical variances across the elements of \(\) because the diagonal elements of \(\) can be different among each other. Furthermore, it allows for non-zero off-diagonal elements in \(\). It is difficult to assume that the regression errors are independent among each other with time series or clustered data; thus, in these settings, it is important to allow for general \(^{2}I\). Below we present a couple of such examples.

**Example 2.1** (Ar(1) Errors).: Suppose that the regressor error follows an autoregressive process:

\[_{i}=_{i-1}+_{i},\] (2)

where \((-1,1)\) is an autoregressive parameter, \(_{i}\) is independent and identically distributed with mean zero and variance \(^{2}(0<^{2}<)\) and is independent of \(X\). Then, the (\(i\), \(j\)) element of \(\) is

\[_{ij}=}{1-^{2}}^{|i-j|}.\]

Note that \(_{ij} 0\) as long as \( 0\).

**Example 2.2** (Clustered Errors).: Suppose that regression errors are mutually independent across clusters but they can be arbitrarily correlated within the same cluster. For instance, students in the same school may affect each other and also have the same teachers; thus it would be difficult to assume independence across student test scores within the same school. However, it might be reasonable that student test scores are independent across different schools. For example, assume that (i) if the regression error \(_{i}\) belongs to cluster \(g\), where \(g=1,,G\) and \(G\) is the number of clusters, \([_{i}^{2}]=_{g}^{2}\) for some constant \(_{g}^{2}>0\) that can vary over \(g\); (ii) if the regression errors \(_{i}\) and \(_{j}\) (\(i j\)) belong to the same cluster \(g\), \([_{i}_{j}]=_{g}\) for some constant \(_{g} 0\) that can be different across \(g\); and (iii) if the regression errors \(_{i}\) and \(_{j}\) (\(i j\)) do not belong to the same cluster, \([_{i}_{j}]=0\). Then, \(\) is block diagonal with possibly non-identical blocks.

For vector \(a\) and square matrix \(A\), let \(\|a\|_{A}^{2}:=a^{}Aa\). Conditional on \(X\) and given \(A\), we define

\[_{A}( X):=\|[ X]- \|_{A}_{A}( X):= (( X)A),\]

and we write \(=_{I}\) and \(=_{I}\) for the sake of brevity in notation.

The mean squared prediction error for an unseen test observation \(x_{0}\) with the positive definite covariance matrix \(:=[x_{0}x_{0}^{}]\) (assuming that \(x_{0}\) is independent of the training data \(X\)) and the mean squared estimation error of \(\) conditional on \(X\) can be written as:

\[R_{P}( X) :=[(x_{0}^{}-x_{0}^{})^{2} X ]=[_{}( X)]^{2}+_{}(  X),\] \[R_{E}( X) :=[\|-\|^{2} X]=[( X)]^{2}+( X).\]

In what follows, we obtain exact finite-sample expressions for prediction and estimation risks:

\[R_{P}():=_{X}[R_{P}( X)]  R_{E}():=_{X}[R_{E}( X)].\]

We first analyze the variance terms for both risks and then study the bias terms.

## 3 The Variance Components of Prediction and Estimation Risks

### The variance component of prediction risk

We rewrite the variance component of prediction risk as follows:

\[_{}( X)=(( X ))=(X^{} X^{})=\|SX^{}T\| _{F}^{2},\] (3)

where positive definite symmetric matrices \(S:=^{1/2}\) and \(T:=^{1/2}\) are the square root matrices of the positive definite matrices \(\) and \(\), respectively. To compute the above Frobenius norm of the matrix \(SX^{}T\), we need to compute the alignment of the right-singular vectors of \(B:=SX^{}^{P n}\) with the left-eigenvectors of \(T^{n n}\). Here, \(B\) is a random matrix while \(T\) is fixed. Therefore, we need the distribution of the right-singular vectors of the random matrix \(B\).

Perhaps surprisingly, to compute the _expected_ variance \(_{X}[_{}( X)]\), it turns out that we do not need the distribution of the singular vectors if we make a minimal assumption (the _left-spherical symmetry_ of \(X\)) which is weaker than the assumption that \(\{x_{i}\}_{i=1}^{n}\) is i.i.d. normal with \([x_{1}]=0\).

**Definition 3.1** (Left-Spherical Symmetry ).: A random matrix \(Z\) or its distribution is called to be _left-spherical_ if \(OZ\) and \(Z\) have the same distribution (\(OZ}{{=}}Z\)) for any fixed orthogonal matrix \(O O(n):=\{A^{n n}:AA^{}=A^{}A=I\}\).

**Assumption 3.2**.: The design matrix \(X\) is left-spherical.

For the isotropic error case (\(=I\)), we have \(_{X}[_{}( X)]=_{X}[((X^{}X)^{})]\) directly from equation 3 since \(X^{}X^{}=(X^{}X)^{}\). Moreover, for the arbitrary error, the left-spherical symmetry of \(X\) plays a critical role to _factor out_ the same \(_{X}[((X^{}X)^{})]\) and the trace of the variance-covariance matrix of the regression errors, \(()\), from the variance after the expectation over \(X\).

**Lemma 3.3**.: _For a subset \(^{m m}\) satisfying \(C^{-1}\) for all \(C\), if matrix-valued random variables \(Z\) and AZ have the same distribution measure \(_{Z}\) for any \(A\), then we have_

\[_{Z}[f(Z)]=_{Z}[f(AZ)]=_{Z}[_{A^{ } r}[f(A^{}Z)]]\]

_for any function \(f L^{1}(_{Z})\) and any probability density function \(\) on \(\)._

**Theorem 3.4**.: _Let Assumptions 2.1, and 3.2 hold. Then, we have_

\[_{X}[_{}( X)]=( )_{X}[((X^{}X)^{})].\]

Sketch of Proof.: With \(B=^{1/2}X^{}\) and \(T=^{1/2}\), we can rewrite the variance as follows:

\[_{}( X)=\|BT\|_{F}^{2}=\|UDV^{}U_{T}D_{T}V_{ T}^{}\|_{F}^{2}=\|DV^{}U_{T}D_{T}\|_{F}^{2}\]

from the singular value decompositions \(B=UDV^{}\) and \(T=U_{T}D_{T}V_{T}^{}\) with orthogonal matrices \(U,V,U_{T},V_{T}\), and diagonal matrices \(D,D_{T}\). Then, we need to compute the alignment \(V^{}U_{T}\) of the right-singular vectors of \(B\) with the left-eigenvectors of \(T\) because

\[\|DV^{}U_{T}D_{T}\|_{F}^{2}=((X^{}X)^{}) ^{}(X)()=a(X)^{}(X)b,\]where \(v^{(i)}:=V_{j}\), \(u^{(j)}:=(U_{T})_{:j}\), \(_{ij}:= v^{(i)},u^{(j)}^{2} 0\), \((X):=(_{ij})_{i,j}^{n n}\) and \((A)^{n}\) is a vector where its elements are the eigenvalues of \(A\).

Now, we want to compute the expected variance. To do so, from Lemma 3.3 with \(=O(n)\) and the left-spherical symmetry of \(X\), we can obtain

\[_{X}[a(X)^{}(X)b]=_{X}[_{O }[a(OX)^{}(OX)b]]=_{X}[a(X)^{} _{O}[(OX)]b],\]

where \(\) is the unique uniform distribution (the Haar measure) over the orthogonal matrices \(O(n)\).

Here, we can show that \(_{O}[(OX)]=J\), where \(J\) is the all-ones matrix with \(J_{ij}=1(i,j=1,2,,n)\). Therefore, we have the expected variance as follows:

\[_{X}[_{}( X)]=_{X}[ a(X)^{}Jb]=_{i,j=1}^{n}_{X}[a_{i}(X)]b _{j}=_{X}[((X^{}X)^{})]\, ().\]

The proofs of Lemma 3.3 and Theorem 3.4 are in the supplementary appendix.

### The variance component of estimation risk

For the expected variance \(_{X}[( X)]\) of the estimation risk, a similar argument still holds if plugging-in \(B=X^{}\) instead of \(B=^{1/2}X^{}\).

**Theorem 3.5**.: _Let Assumptions 2.1, and 3.2 hold. Then, we have_

\[_{X}[( X)]=\,( )_{X}[(^{})],\]

_where \(XX^{}/p=U U^{}\) for some orthogonal matrix \(U O(n)\)._

### Numerical experiments

In this section, we validate our theory with some numerical experiments of Examples 2.1 and 2.2, especially how the expected variance is related to the general covariance \(\) of the regressor error \(\). In the both examples, we sample \(\{x_{i}\}_{i=1}^{n}\) from \((0,)\) with a general feature covariance \(=U_{}D_{}U_{}^{}\) for an orthogonal matrix \(U_{} O(p)\) and a diagonal matrix \(D_{}>0\). In this setting, we have \((XX^{})=n\) and \(^{}=^{-1}\) almost everywhere.

Figure 2: Our theory (dashed lines) matches the expected variances (solid lines) of the prediction (left) and estimation risks (right) in Example 2.1 (AR(1) Errors). Each point \((^{2},^{2})\) represents a different noise covariance matrix \(\), but with the same \(()\) along each line \(\{(^{2},^{2}):^{2}/^{2}+^{2}=1\}\) for some \(^{2}>0\), they have the same expected variance. We set \(n=50\), \(p=100\), and evaluate on 100 samples of \(X\) and 100 samples of \(\) (for each realization of \(X\)) to approximate the expectations.

AR(1) ErrorsAs shown in Example 2.1, when the regressor error follows an autoregressive process in equation 2, we have \(_{ij}=^{2}^{i_{k}-j}/(1-^{2})\) and \(()/n=^{2}/(1-^{2})\). Therefore, for pairs of \((^{2},^{2})\) with the same \(()/n\), they are expected to yield the same variances of the prediction and estimation risk from Theorem 3.4 and 3.5 even though they have different off-diagonal elements in \(\). To be specific, the pairs \((^{2},^{2})\) on a line \(\{(^{2},^{2}):^{2}/^{2}+^{2}=1\}\) have the same \(()/n\) and the same expected variance which gets larger for the line with respect to a larger \(^{2}\).

Figure 2 (left) shows the contour plots of \(_{X}[_{}( X)]\) and \(()_{X}[(X^{ }X)^{})]\) for different pairs of \((^{2},^{2})\) in Example 2.1. They have different slopes \(-^{2}\) according to the value of \(^{2}=()/n\). The right panel shows equivalent contour plots for estimation risk.

Clustered ErrorsNow consider the block diagonal covariance matrix \(=(_{1},_{2},,_{G})\) in Example 2.2, where \(_{g}\) is an \(n_{g} n_{g}\) matrix with \((_{g})_{ii}=_{g}^{2}\) and \((_{g})_{ij}=_{g}\) (\(i j\)) for each \(i,j=1,2,,n_{g}\) and \(g=1,2,,G\). Let \(n=_{g=1}^{G}n_{g}\). We then have \(()/n=_{g=1}^{G}(_{g})/n= _{g=1}^{G}(n_{g}/n)_{g}^{2}\). Therefore, given a partition \(\{n_{g}\}_{g=1}^{G}\) of the \(n\) observations, the covariance matrices \(\) with different \(\{_{g}^{2}\}_{g=1}^{G}\) have the same \(()/n\) if \((_{1}^{2},_{2}^{2},,_{G}^{2})^{G}\) are on the same hyperplane \(}{n}_{1}^{2}+}{n}_{2}^{2}++}{n}_{G}^{2}=^{2}\) for some \(^{2}>0\).

Figure 3 (left) shows the contour plots of \(_{X}[_{}( X)]\) and \(()_{X}[(X^{ }X)^{})]\) for different pairs of \((_{1}^{2},_{2}^{2})\) for a simple two-clusters example (\(G=2\)) of Example 2.2 with \((n_{1},n_{2})=(5,15)\). Here, we use a fixed value of \(_{1}=_{2}=0.05\), but the results are the same regardless of their values, as shown in the appendix. Unlike Example 2.1, the hyperplanes are orthogonal to \(v=[n_{1},n_{2}]^{}\) regardless of the value of \(^{2}=()/n\). Again, the right panel shows equivalent contour plots for estimation risk.

## 4 The Bias Components of Prediction and Estimation Risks

Our main contribution is to allow for general assumptions on the regression errors, and thus the bias parts remain the same as they do not change with respect to the regression errors. For completeness, in this section, we briefly summarize the results on the bias components. First, we make the following assumption for a constant rank deficiency of \(X^{}X\) which holds, for example, each \(x_{i}\) has a positive definite covariance matrix and is independent of each other.

**Assumption 4.1**.: \((X)=n\) almost everywhere.

Figure 3: Our theory (dashed lines) matches the expected variances (solid lines) of the prediction (left) and estimation risks (right) in Example 2.2 (Clustered Errors). Each point \((^{2},^{2})\) represents a different noise covariance matrix \(\), but with the same \(()\) along each line \(\{(_{1}^{2},_{2}^{2}):}{n}_{1}^{2}+}{ n}_{2}^{2}=^{2}\}\) for some \(^{2}>0\), they have the same expected variance. We set \(G=2,(n_{1}=5,n_{2}=15),n=20,p=40,_{1}=_{2}=0.05\), and evaluate on 100 samples of \(X\) and 100 samples of \(\) (for each realization of \(X\)) to approximate the expectations.

### The bias component of prediction risk

The bias term of prediction risk can be expressed as follows:

\[[_{}( X)]^{2}=(S)^{}_{  0}^{2}(S^{-1}S+ I)^{-2}S,\] (4)

where \(:=X^{}X/n\). Now, in order to obtain an exact closed form solution, we make the following assumption:

**Assumption 4.2**.: \(_{}[S(S)^{}]=r_{}^{2}I/p\), where \(r_{}^{2}:=_{}[||]_{}^{2}]<\) and \(\) is independent of \(X\).

A similar assumption (see Assumption 4.4) has been shown to be useful to obtain closed-form expressions in the literature (e.g., 13, 23, 20, 7).

Under this assumption, since \([_{}( X)]^{2}=[S(S)^{ }_{ 0}^{2}(S^{-1}S+ I)^{-2}]\) from equation 4, we have the expected bias (conditional on \(X\)) as follows:

\[_{}[_{}( X)^{2} X]= {r_{}^{2}}{p}_{ 0}_{i=1}^{p}}{( _{i}+)^{2}}=^{2}}{p}||i[p]: {s}_{i}=0||=r_{}^{2},\]

where \(_{i}\) are the eigenvalues of \(S^{-1}S^{p p}\) and \((S^{-1}S)=(X)=n\) almost everywhere under Assumption 4.1. This bias is independent of the distribution of \(X\) or the spectral density of \(S^{-1}S\), but only depending on the rank deficiency of the realization of \(X\).

Finally, the prediction risk \(R_{P}()\) can be summarized as follows:

**Corollary 4.3**.: _Let Assumptions 2.1, 3.2, 4.1, and 4.2 hold. Then, we have_

\[R_{P}()=r_{}^{2}(1-)+()}{n}_{X}[((X^{}X)^{})].\]

### The bias component of estimation risk

For the bias component of estimation risk, we can obtain a similar result with 4.1 as follows:

\[[( X)]^{2}=^{}(I-^{} )=_{ 0}^{}(+  I)^{-1}.\]

**Assumption 4.4**.: \(_{}[^{}]=r^{2}I/p\), where \(r^{2}:=_{}[||]^{2}]<\) and \(\) is independent of \(X\).

Under Assumption 4.4, we have the expected bias (conditional on \(X\)) as follows:

\[_{}[( X)^{2} X]=}{p }_{ 0}_{i=1}^{p}+}=}{p} ||i[p]:s_{i}=0||=r^{2},\] (5)

where \(s_{i}\) are the eigenvalues of \(^{p p}\) and \(()=(X)=n\) under Assumption 4.1.

Thanks to Theorem 3.5 and equation 5, we obtain the following corollary for estimation risk.

**Corollary 4.5**.: _Let Assumptions 2.1, 3.2, 4.1, and 4.4 hold. Then, we have_

\[R_{E}()=r^{2}(1-)+()}{ n}_{X}[dF^{XX^{}/n}(s)],\]

_where \(F^{A}(s):=_{i=1}^{n}1\{_{i}(A) s\}\) is the empirical spectral distribution of a matrix \(A\) and \(_{1}(A),_{2}(A),,_{n}(A)\) are the eigenvalues of \(A\)._

The proof of Corollary 4.5 is in the appendix.

#### 4.2.1 Asymptotic analysis of estimation risk

To study the asymptotic behavior of estimation risk, we follow the previous approaches . First, we define the Stieltjes transform as follows:

**Definition 4.6**.: The Stieltjes transform \(s_{F}(z)\) of a df \(F\) is defined as:

\[s_{F}(z):=dF(x),z(F).\]We are now ready to investigate the asymptotic behavior of the mean squared estimation error with the following theorem:

**Theorem 4.7**.: _[_25_, Theorem 1.1]_ _Suppose that the rows \(\{x_{i}\}_{i=1}^{n}\) in \(X\) are i.i.d. centered random vectors with \([x_{1}x_{1}^{*}]=\) and that the empirical spectral distribution \(F^{}(s)=_{i=1}^{p}1[_{i} s]\) of \(\) converges almost surely to a probability distribution function \(H\) as \(p\). When \(p/n>0\) as \(n,p\), then a.s., \(F^{XX^{}/n}\) converges vaguely to a \(df\,F\) and the limit \(s^{*}:=_{ 0}s_{F}(z)\) of its Stieltjes transform \(s_{F}\) is the unique solution to the equation:_

\[1-=}dH().\] (6)

This theorem is a direct consequence of Theorem 1.1 in Silverstein and Bai . Then, from Corollary 4.5, we can write the limit of estimation risk as follows:

**Corollary 4.8**.: _Let Assumptions 2.1, 3.2, 4.1, and 4.4 hold. Then, under the same assumption as Theorem 4.7, as \(n,p\) and \(p/n\), where \(1<<\) is a constant, we have_

\[R_{E}()=[\|-\|^{2}] r^{2}(1- )+s^{*}_{n}() }{n}.\]

Here, the limit \(s^{*}\) of the Stieltjes transform \(s_{F}\) is highly connected with the shape of the spectral distribution of \(\). For example, in the case of isotropic features (\(=I\)), i.e., \(dH()=(-1)d\), we have \(s_{}^{*}=(-1)^{-1}\) from \(1-=^{*}}\). In addition, if \(=^{2}I\), then the limit of the mean squared error is exactly the same as the expression for \(>1\) in equation (10) of Hastie et al. [17, Theorem 1]. This is because prediction risk is the same as estimation risk when \(=I\).

_Remark 4.9_.: Generally, if the support of \(H\) is bounded within \([c_{H},C_{H}]\) for some positive constants \(0<c_{H} C_{H}<\), then we can observe the double descent phenomenon in the overparameterization regime with \(_{ 1}s^{*}=\) and \(_{}s^{*}=0\) with \(s^{*}=()\) from the following inequalities:

\[C_{H}^{-1} s^{*} c_{H}^{-1}.\] (7)

In fact, a tighter lower bound is available:

\[s^{*}_{H}^{-1}(-1)^{-1},\] (8)

where \(_{H}:=_{-H}[]\), i.e., the mean of distribution \(H\). The proofs of equation 7 and equation 8 are given in the supplementary appendix.

We conclude this paper by plotting the "descent curve" in the overparameterization regime in Figure 4. On one hand, the expected variance (\(\)) perfectly matches its theoretical counterpart (\(\)) and goes to zero as \(\) gets large. On the other hand, the bias term is bounded even if \(\). The appendix contains the experimental details for all the figures.

Figure 4: The “descent curve” in the overparameterization regime for prediction risk (left) and estimation risk (right). We test \(\)’s with \(()/n=1,2,4\) in black, blue, red, respectively. For the anisotropic feature, the expected variance (\(\)) and its theoretical expression (\(\)) are \((()/n}{-1})\) and larger than that in the high-dimensional asymptotics for the isotropic \(=I\). For the isotropic \(=I\), the variance terms (dotted) and the bias terms (dashed) in the high-dimensional asymptotics are \(_{n}()}{n}\) and \(r^{2}(1-)\), respectively.