# Optimal testing using combined test statistics across independent studies

Lasse Vuursteen

Delft Institute of Applied Mathematics

Delft University of Technology

l.vuursteen@tudelft.nl

&Botond Szabo

Department of Decision Sciences

and Institute for Data Science and Analytics

Bocconi University

botond.szabo@unibocconi.it

&Aad van der Vaart

Delft Institute of Applied Mathematics

Delft University of Technology

a.w.vandervaart@tudelft.nl

&Harry van Zanten

Mathematics Department

Vrije Universiteit Amsterdam

j.h.van.zanten@vu.nl

###### Abstract

Combining test statistics from independent trials or experiments is a popular method of meta-analysis. However, there is very limited theoretical understanding of the power of the combined test, especially in high-dimensional models considering composite hypotheses tests. We derive a mathematical framework to study standard meta-analysis testing approaches in the context of the many normal means model, which serves as the platform to investigate more complex models.

We introduce a natural and mild restriction on the meta-level combination functions of the local trials. This allows us to mathematically quantify the cost of compressing \(m\) trials into real-valued test statistics and combining these. We then derive minimax lower and matching upper bounds for the separation rates of standard combination methods for e.g. p-values and e-values, quantifying the loss relative to using the full, pooled data. We observe an elbow effect, revealing that in certain cases combining the locally optimal tests in each trial results in a sub-optimal meta-analysis method and develop approaches to achieve the global optima. We also explore the possible gains of allowing limited coordination between the trial designs. Our results connect meta-analysis with bandwidth constraint distributed inference and build on recent information theoretic developments in the latter field.

## 1 Introduction

Given multiple data sets relating to the same hypothesis, one would like to combine the evidence. Sometimes, the full data sets are not available (e.g. due to privacy or proprietary reasons) or difficult to combine directly (e.g. due to the different experimental or observational setups). In such cases, the analysis must be carried out on the basis of the published results for each of the studies. Such "meta-analysis" can increase the statistical power by combining individually inconclusive or moderately significant tests, while keeping the false positive rate under control. Therefore, meta-analysis has received a lot of attention in various fields, for instance in genetics and system biology, when studying rare variants  or in deep learning, for few shot image recognition and neural architecture search, see the review article .

The outcomes of the studies concerning hypothesis tests are, typically, summarized as real-valued test statistics and/or associated p-values. One expects the combination of \(m\) such p-values to result in an increase in power, but one also expects to pay a price relative to computing a test on the basis of the full, pooled data of the \(m\) trials. The question of how to optimally combine independent real-valued test statistics concerning the same hypothesis into a single test has an extensive literature. A multitude of methods for combining independent tests of significance exist. For combining p-values, this starts with Fisher, Tippett and Pearson in the nineteen-thirties, see  and references therein. In Section 3, we collect and describe the most popular and frequently used p-value combination techniques.

As noted in , there does not exist a general uniformly most powerful p-value combination method for all alternative hypotheses. The distribution of a p-value or its underlying test statistic under the alternative hypothesis should be taken into consideration when selecting a method of combination. The performance of different p-value combination techniques was investigated extensively by empirical experiments in various synthetic and real world scenarios, see for instance . However, a unified, general theoretical description is lacking, especially in non-trivial, multi-dimensional composite testing problems, where the likelihood ratio test is not necessarily uniformly most powerful.

E-values are an increasingly popular and important notion of evidence, see . E-values allow the combination of several tests in a straightforward manner while preserving the prescribed level of the tests (see Section 3.2). Formally, e-values are nonnegative random variables whose expected values under the null hypothesis are bounded by one. In contrast to p-values defined by probabilities, e-values are defined by expectation. This imposes significant differences in their interpretation, application and combination compared to the more standard p-values. However, as for p-values, very little is known about the power of these combination procedures. Theoretical results focus on specific optimality criteria, for instance the worst-case growth-rate (GROW), see . However, these do not directly imply guarantees on the testing power, which is the main focus in practice.

Our focus is on multidimensional models, where a certain loss in power is to be expected, since combining multidimensional data into a real-valued statistic (e.g. p-value or e-value) requires data compression. Typically, summary statistics are combined by some "reasonable" function \(C_{m}:^{m}\), where "reasonable" means that \(C_{m}\) should not exploit the richness of the real numbers to encode the data in full. We aim to quantify the loss of summarizing, the gain of performing a meta-analysis and the best testing strategies in the individual experiments meta-analysis.

We consider the signal detection problem in the many normal means model, see Section 2 for the detailed description. One possible interpretation of this testing problem is to learn whether a treatment has an effect on any of the dimensions investigated. This model is directly applied in several fields where high-dimensional statistics and machine learning settings are concerned, such as detecting differentially expressed genes , bankruptcy prediction for publicly traded companies using Altman's Z-score in finance , separation of the background and source in astronomical images , and wavelet analysis . Furthermore, the model allows for tractable computations and it typically serves as the platform to investigate more difficult statistical and learning problems, including high- and infinite-dimensional models, see for instance . In each experiment \(j\{1,...,m\}\) the observations are summarized by an appropriate real-valued summary statistic \(S^{(j)}\). These local test statistics (e.g. p- or e-values) are combined into \(C_{m}(S^{(1)},,S^{(m)})\). We consider a general class of combination functions \(C_{m}\), requiring only Holder type continuity. This introduces only a mild restriction, and includes many standard meta-analysis techniques, for instance the standard p-value combination methods (see Section 3.1); e-value techniques (see Section 3.2); and other ad hoc and natural test statistic combination approaches, see the beginning of Section 3 for additional examples.

Our setting provides a principled and unified framework to study the power of standard meta-analysis testing methods. Within the framework of the many normal means model, we derive a minimax lower bound for the testing (separation) error and provide test statistics with associated combination methods that attain this theoretical limit (up to a logarithmic factor). Our results reveal that there is a certain unavoidable loss associated with compressing the data of each experiment to a real valued test statistic. We see that while it is always possible to obtain better testing rates using \(m\) trials instead of the best possible test based on a single trial, there is always a loss incurred when compared to the full, pooled data and optimal test in moderate- to large dimensional problems. Our theoretical results quantify these gains and losses in terms of the dimension \(d\), sample size \(n\) and number of trials \(m\).

Furthermore, we observe an elbow effect, which occurs when the number of trials is large compared to the dimension of the signal. In this regime, combinations of the (locally) optimal test in each individual trial performs sub-optimally as a whole when aggregated and meta-analysis approaches based on directional test statistics are shown to perform better. Finally, we show that the performance of the meta-level tests can substantially improve (in certain regimes, depending on \(d,m,n\)) if a certain amount of coordination between the trials is allowed (e.g. by having access to the same random seed). For the theoretical analysis of meta-analysis techniques we derive connections with the distributed statistical learning literature under communication constraints. Our paper builds on the recent information theoretical developments in distributed testing [2; 39; 40], allowing us to address several fundamental questions for the first time with mathematical rigor.

The paper is organised as follows. In Section 2 we introduce the mathematical framework we consider in our investigation and present the corresponding minimax testing lower bound results. Next in Subsection 2.1 we show that the derived results are sharp by providing several meta-analysis approaches attaining the limits. Then we investigate the benefits of allowing a mild coordination between the trials in Subsection 2.2. We collect and discuss the standard p- and e-value combination methods in Section 3 and demonstrate our theoretical results numerically on synthetic data sets in Section 4. We discuss our results and derive conclusions in Section 5. The proofs of our results are deferred to the Appendix. In Section A.1 we present the proof of our main results while the proofs of the technical lemmas are given in A.2.

**Notation:** For two positive sequences \(a_{n}\), \(b_{n}\) we write \(a_{n} b_{n}\) if the inequality \(a_{n} Cb_{n}\) holds for some universal positive constant \(C\). Similarly, we write \(a_{n} b_{n}\) if \(a_{n} b_{n}\) and \(_{n} a_{n}\) hold simultaneously and let \(a_{n} b_{n}\) denote that \(a_{n}/b_{n}=o(1)\). Furthermore, we use the notations \(a b\) and \(a b\) for the maximum and minimum, respectively, between \(a\) and \(b\). Throughout the paper \(c\) and \(C\) denote global constants whose value may change from one line to another.

## 2 Main results

In our analysis, we consider the localized version of the many normal means model, tailored to investigating meta-analysis techniques. We assume that in each local trial or experiment \(j\{1,...,m\}\) we observe a \(d\)-dimensional random variable \(X^{(j)}^{d}\), subject to

\[X^{(j)}=f+}Z^{(j)}, Z^{(j)}}{{}}N(0,I_{d}), j=1,...,m,\] (1)

for some unknown \(f^{d}\). We denote by \(_{f}\) the joint distribution of the observations and let \(_{f}\) be the corresponding expectation. We note that this framework is equivalent to having \(n\) independent \(N(f,I_{d})\) observations within each local sample.

Our goal is to test the presence or absence of the "signal component" \(f^{d}\). More formally, we consider the simple null hypothesis \(H_{0}:f=0\) versus composite alternative hypothesis \(H_{}:\|f\|_{2}\), for some \(>0\). This corresponds to testing for joint significance of variables, such as the presence of an effect of a treatment on any of the dimensions investigated. The difficulty in distinguishing the hypotheses depends on the effect size, the sample size and the dimension \(d\). Here, \(\) can be seen as the smallest effect size deemed important.

For a \(\{0,1\}\)-valued test \(T\), define the testing risk \((T,H_{})\) as the sum of the Type I error probability and worst case Type II error probability, i.e.

\[(T,H_{}):=_{0}(T=1)+_{f H_{}}_{f }(T=0).\] (2)

In the case of a single trial (i.e. \(m=1\)), this testing problem is known to have minimax separation rate or "detection boundary" \(^{2}/n\).

This means that if \(^{2}/n\), there exist consistent2 tests \(T T_{d,n}\) in the sense that \((T,H_{}) 0\), whilst no consistent tests exist when \(^{2}/n\). That is, for effect sizes of smaller order than \(/n\), the null hypothesis cannot be consistently distinguished from the alternative hypothesis. Such a testing rate is attainable through a chi-square test based on \(\|X^{(1)}\|_{2}^{2}\) (see e.g. ).

In case of \(m\) trials, if the full data were pooled (with aggregated sample size \(nm\)), the minimax separation rate would be \(/(mn)\). However, pooling the data might not be possible or allowed in practice and often only real-valued test statistics are available that describe the significance in the local problems (e.g. a p- or an e-value). These \(m\) test statistics \(S^{(j)}\), \(j=1,...,m\), then can be combined with some combination function \(C_{m}:^{m}\), providing the test statistic in the meta-analysis. We now ask whether the above pooled testing rate is attainable with this meta-analysis procedure.

Without any restrictions on the test statistics \(S=(S^{(1)},,S^{(m)})\) or the combination function \(C_{m}\), any of the conventional optimal "full-data" tests can be reconstructed, since the real numbers and mappings between the real numbers form an overly rich class. We wish to restrict our analysis to \(S\) and \(C_{m}\) that are reasonable in practice and capture (most of) the relevant meta-analysis methods as listed in Section 3.

Based on each of the local observations \(X^{(j)}\), a real-valued test statistic \(S^{(j)}\) is computed, where each \(S^{(j)}\) is a function of \(X^{(j)}\) and possibly a source of randomness \(U^{(j)}\) independent of \(X:=(X^{(1)},,X^{(m)})\).

**Assumption 1**.: _For measurable functions \(f_{j}:^{d}\) and independent random variables \(U^{(1)},,U^{(m)}\) which are independent of the data \(X\), the \(j\)-th test statistic \(S^{(j)}=f_{j}(X^{(j)},U^{(j)})\) satisfies \(_{0}|S^{(j)}| M\), for some \(M>0\), \(j=1,,m\)._

We consider Holder continuous combination functions \(C_{m}:^{m}\). Arguably, this is the most important assumption in ruling out bijections between \(^{d}\) and \(\). This ensures that a small change in the underlying local test statistics cannot result in a large change in the combination of test statistics \(C_{m}(S^{(1)},,S^{(m)})\).

**Assumption 2**.: _There exist \(L,p,q>0\) such that for all \(s,s^{}^{m}\)_

\[|C_{m}(s)-C_{m}(s^{})| L_{j=1}^{m}|s_{j}-s^{}_{j}| ^{p}^{q}.\] (3)

The special case of \(p=2\) and \(q=1/2\) leads to Lipschitz continuous functions. Assumption 1 and Assumption 2 should be considered in conjunction. By rescaling and centering test statistics \(S^{(j)}\), one can typically obtain test statistics satisfying Assumption 1. Rescaling and centering typically does affect how the test statistics need to be combined, which might "break" Assumption 2.

Finally, following the standard testing approach, we compare the aggregated test statistics \(C_{m}(S^{(1)},,S^{(m)})\) to a threshold value. If the combined test statistics result in a large enough value, the null hypothesis of no effect is rejected. We note here that two sided tests can be written as one-sided tests through straightforward transformations (e.g. centering and taking absolute value). More formally, we consider tests \(T_{}\) of level \(\) satisfying the following assumption.

**Assumption 3**.: _There exists a strictly decreasing function \(_{}\) so that_

\[T_{}=\{C_{m}(S^{(1)},,S^{(m)})_{}\}\] (4)

_satisfies \(_{0}T_{}\)._

The map \(_{}\) could be taken as the quantile function of \(C_{m}(S^{(1)},,S^{(m)})\) under its null distribution if it is appropriately standardized. If \(_{0}C_{m}(S^{(1)},,S^{(m)})\) is bounded in \(m\), we can choose \(_{}\) equal to \(1/\) times the upper bound, in view of Markov's inequality.

Our first main result, Theorem 1 below, establishes a lower bound for tests of the form (4) and \(C_{m}\) and \(S\) satisfying the above assumptions. More concretely, under our assumptions, any test \(T_{}\) (of level \( 0.1\)) has large Type II-error under alternatives with \(^{2}\) of smaller order than \(()/(mn)\). When the number of trials is small compared to the dimension (i.e. \(m^{2}(m) d^{2}\)), this means that the separation rate is at least \(/()\). Thus even though there is a benefit in terms of separation rate compared to testing based on just a single trial, the gain is at best the square root of what one would gain based on testing on the pooled data. When \(m^{2}(m) d^{2}\), the rate in the lower bound changes to \(d/(mn(m))\), resulting in an elbow effect.

**Theorem 1**.: _Let \(S^{(1)},,S^{(m)}\), \(C_{m}\) and \(T_{}\) satisfy Assumptions 1-3 with \(T_{}\) of level \((0,0.1]\). Then there exists a constant \(c>0\) depending only on \(L\), \(p,q\) and \(M\), such that if_

\[^{2} c)}{mn},\] (5)

_it holds for all \(n,m,d\) that_

\[_{f H_{}}_{f}(T_{}=0) 3/4.\] (6)

_Remark 1_.: The ranges of values \(0< 0.1\) and \(=3/4\) for the Type I and II errors, respectively, are arbitrary. Similar results hold for different choices as well. For instance, one can take arbitrary \((0,1/5]\) and \((0,2/3]\), see the proof of the theorem for details. The result implies in particular that consistent testing is not possible for signals of a smaller order than the right hand side of (5), where asymptotics can be considered in \(n,m\) and \(d\) simultaneously.

In the next section we show that the lower bounds in the theorems above are sharp (up to a logarithmic factor).

### Rate optimal combination methods

To attain the lower bound rate derived in Theorem 1, different tests can be considered. The optimal rate displays an elbow effect around \(m d^{2}\). When the dimension is large compared to the number of trials \(m\) (i.e. \(m d^{2}\)), strategies that combine p-values for the optimal local tests (based on \(\|X^{(j)}\|_{2}^{2}^{H_{0}}_{d}^{2}\)), turn out to achieve the optimal rate, as exhibited below. Such a test statistic is invariant to the directionality of \(X^{(j)}\) and invariant under the model in the sense that the resulting power for the alternative \(_{f}\) or \(_{g}\) is the same as long as \(\|f\|_{2}=\|g\|_{2}\).

On the other hand, when the dimension is small compared to the number of trials (i.e. \(m d^{2}\)), optimal strategies exhibited below use information on the direction of \(X^{(j)}\). In fact, we show in Theorem 4 in the Appendix that if no such information is available (i.e. the events defined by the signs of the \((X^{(j)})_{j=1,,m}\) vector are not contained in the sigma algebra generated by the test statistics \(S\)), one cannot obtain a rate better than \(/()\). This implies that by combining the locally optimal test statistics \(S^{(j)}=\|X^{(j)}\|_{2}^{2}\) (or their arbitrary functions, e.g. the corresponding local p-values) would result in information loss and hence sub-optimal rates in the meta-analysis.

Furthermore, it turns out, in accordance with the empirical literature discussed in the introduction, that there does not exist a uniquely best meta-analysis method. In fact, multiple standard meta-analysis techniques provide (up to a logarithmic factor) optimal rates, see below for some standard approaches attaining the lower bounds derived in Theorem 1.

First we consider the scenario when the dimension \(d\) of the model is large compared to the number of trials \(m\), i.e. \(m d^{2}\). Locally the optimal test is based on the test statistic \(\|X^{(j)}\|_{2}^{2}}}{{}}_{d^{2}}^ {2}\). A natural way to combine these statistics would be to sum these locally optimal test statistics to obtain

\[T_{}=\{_{j=1}^{m}\|X^{(j)}\|_{2} ^{2} F_{_{dm}^{2}}^{-1}(1-)\},\] (7)

which has level \(\). Alternatively, one could also apply p-value combination methods, such as Fisher's or Edgington's method based on the p-value \(p^{(j)}=1-F_{_{d}^{2}}(\|X^{(j)}\|_{2}^{2})\), see Section 3. Lemma 6 in the appendix establishes that these tests are rate optimal.

Second, consider the case that the number of trials is large compared to the dimension, i.e. \(m d^{2}\). Rate optimal tests can be constructed based on a variation of Edgington's or Stouffer's method, see Section 3 for their descriptions. Taking a partition of \(\{1,,m\}=_{i=1}^{d}_{i}\) where \(|_{i}| m/d\) and setting \(S^{(j)}=X_{i}^{(j)}\) if \(j_{i}\), the meta-level test

\[T_{}=\{}{m}} (_{i}}{}S^{(j)})^{2} d^{-1/2}F_{ _{d}^{2}}^{-1}(1-)\}\] (8)

achieves the lower bounds. The above test is similar to employing Stouffer's method for each of the coordinates and averaging, i.e. computing approximately \(m/d\) iid p-values \(p^{(j)}=(X_{i}^{(j)})\) for \(j_{i}\) and applying the inverse Gaussian CDF \(^{-1}(p^{(j)})\). Alternatively, the following variation of Edgington's method,

\[T_{}=\{}{m}}(_{i}}{}(p^{(j)}- ))^{2}_{}\},\] (9)

is also rate optimal, as proven in Lemma 7 in the appendix. Essentially, these strategies divide the trials accross the \(d\) different directions, and combines the evidence for each of the directions. Theorem 4 affirms that the information on the "direction" of the data is crucial to achieve the optimal rate in the \(m d^{2}\) case, by showing that strategies that do not contain such information (rotationally invariant strategies such as norm-based test statistics) achieve the rate \(/(n)\) at best. We summarize the above testing upper bounds in the theorem below.

**Theorem 2**.: _For all \(,(0,1)\) there exist \(S\), \(C_{m}:^{m}\) and tests \(T_{}\) of level \(\) satisfying Assumptions 1-3 such that if_

\[^{2} C_{,} d)}{mn},\] (10)

_we have_

\[}{}_{f}(T_{}=0)\]

_for a large enough constant \(C_{,}>0\) depending only on \(,(0,1)\), for all \(n,m,d\)._

### Benefits of coordination between the trials

When the dimension is small relative to the number of trials, as exhibited in the previous section, optimal strategies include information on the directionality of the observation vector. In this section we show that in this regime, there could be an additional benefit from allowing mild coordination between the trials through employing shared randomness, e.g. a shared random seed between the trials. Such a phenomenon has been observed before in the distributed testing literature , which forms the basis of our analysis below.

We consider the following variation on Assumption 1, where the key difference is that the source of randomness is allowed to be shared between the \(m\) trials.

**Assumption 4**.: _For functions \(f_{j}:^{d}\) and a random variable \(U\) which is independent of the data \(X\), the \(j\)-th test statistic \(S^{(j)}=f_{j}(X^{(j)},U)\) satisfies \(_{0}|S^{(j)}| M\) for some \(M>0\) and all \(j=1,,m\)._

Test statistics satisfying this assumption shall be referred to as shared randomness (or public coin) protocols.

The theorem below establishes the optimal rate when coordination through shared randomness is allowed. When the number of trials is small compared to the dimension (i.e. \(m d/ m\)), there is no difference between protocols that coordinate using shared randomness or those without coordination. In fact, the optimal rate (\(^{2}/(n)\)) in this case is reached by the test (7) or the ones below it, which do not employ shared randomness. However, when the number of trials is large compared to the dimension (i.e. \(m d\)), the testing rate substantially improves in the shared randomness protocols.

**Theorem 3**.: _Let \(S^{(1)},,S^{(m)}\), \(C_{m}\) and \(T_{}\) satisfy Assumptions 2-4. Then there exists a constant \(c>0\) depending only on \(L\), \(p,q\) and \(M\), such that if_

\[^{2} c})}{mn},\] (11)_it holds that \(_{f H_{}}_{f}(T_{}=0)>2/3\) for all \(n,m,d\) and any level \((0,0.1]\)._

_At the same time, for all \(,(0,1)\) there exists a constant \(C_{,}>0\) depending only on \(\), \(L\), \(p,q\), the function \(_{}\) and \(M\), such that if_

\[^{2} C_{,})}{mn}\] (12)

_it holds that \(_{f H_{}}_{f}(T_{}=0)\) for some test \(T_{}\) of level \(\) satisfying Assumptions 2-4._

_Remark 2_.: Similarly to Theorem 1 the choice of ranges \(0< 0.1\) and \(=2/3\) in the lower bound result is arbitrary, other choices are also possible as presented in the proof.

A shared randomness method that attains the rate in (12) is given next. Consider drawing an orthonormal \(d d\) matrix \(U\) taking values from the uniform measure on such matrices. As a test statistic, each trial computes \((UX^{(j)})_{1}\), which is a \(N(0,1)\) random variable under the null hypothesis. A level \((0,1)\) meta-level test is then given by combining the local test statistics as

\[T_{}:=\{|}{_{j=1}^{m}}(UX^ {(j)})_{1}^{-1}(1-/2)\},\] (13)

where \(\) is the standard Gaussian CDF. The core idea here is that for each trial, the same \(1\)-dimensional projection of the \(d\) -dimensional data is computed, where the projection is taken uniformly at random and the test is conducted along the projected direction. The above method corresponds to Stouffer's method for the p-values \(p^{(j)}=((UX^{(j)})_{1})\) for \(j=1,,m\). Lemma 8 in the appendix shows that the above test attains a small Type II error probability whenever \(^{2} d/(mn)\).

## 3 Examples for meta-analysis methods

Combinations of independent test statistics that fall into the framework of Assumptions 1- 4 are subject to the rate optimality theory established by the main theorems in Section 2. In this section, we look into common methods for combining p-values, e-values and other test-statistics, as mentioned in the introduction.

When the distribution under the null hypothesis of the test statistics are known, certain combinations are natural. For example, the sum of normal or chi-square test statistics is again normal or chi-square distributed, respectively. Similarly, voting based mechanisms typically rely on summing Bernoulli random variables. It is easy to see that these and similar combinations methods fall into the framework of Assumptions 1-4.

For more specific test statistics, such as p-values or e-values, many general combination methods have been introduced in the literature. We cover some of the most prominent combination approaches for p-values and e-values in Section 3.1 and Section 3.2, respectively. The list of methods is certainly non-exhaustive and many more combination methods exist, but they serve as context for the range of techniques covered by our general theory. Our main results allow establishing lower bound rates for the ones listed below, whilst in Sections 2.1 and 2.2 attainability of these rates by some of the listed methods was exhibited.

### Combinations of p-values

If \(p^{(1)},,p^{(m)}\) are p-values obtained from \(m\) independent test statistics concerning the same hypothesis, then under the null \(p^{(j)}^{iid}U(0,1)\). One can aim to combine the \(m\) p-values to form a test \(T_{} T_{}(p^{(1)},,p^{(m)})\) with Type I error probability \(\), which hopefully has higher power than a test based on one of the individual p-values. Below we list standard methods in the literature.

* Fisher's method . Because the variables \(-2 p^{(j)}\)'s are iid \(_{2}^{2}\)-distributed under the null hypothesis, their sum follows a \(_{2m}^{2}\)-distribution. Therefore the combination method \(_{j=1}^{m}-2 p^{(j)}\) results in a \(_{2m}^{2}\) distributed random variable, and the corresponding quantile function provides level-\(\) one-sided tests at the meta-level.

* Similar flavour to Fisher's method are the combinations \(_{j=1}^{m}-(1-p^{(j)})\) (Pearson's method ), \(_{j=1}^{m}- p^{(j)}(1-p^{(j)})\) (the logit method / Mudholkar and George method ) and \(m^{-1/2}_{j=1}^{m}(p^{(j)}-1/2)\) (Edgington's method ).
* Order-based methods such as Tippett's method  based on \(\{p^{(1)},,p^{(m)}\}}}{{}}(1,m)\).
* Methods based on inverse CDF's, such as by Stouffer et. al  based on \(m^{-1/2}_{j=1}^{m}^{-1}(p^{(j)})(0,1)\) under the null hypothesis.
* Generalized averages as considered in , \(T_{}=\{a_{r,m}M_{r,m}(p^{(1)},,p^{(m)})\}\), where \(M_{r,m}(p^{(1)},,p^{(m)})\) equals \((m^{-1}_{j=1}^{m}(p^{(j)})^{r})^{1/r}\) for \(r\{0\}\), the geometric mean, minimum (i.e. Tippett's method) and maximum for \(r=0\), \(r-\), and \(r\), respectively. For \(r\{-\}[1/(m-1),]\), \(a_{r,m}\) can be taken to obtain precisely level \(\) tests (i.e. \(_{0}T_{}=\)). We note that this means that canonical multiple testing methods (see e.g. ) such as Bonferroni's correction (which corresponds with taking as \(M_{r,m}\) the minimum and \(a_{r,m}=m\)) also fall within our framework.

Lemma 1 below shows that all the methods mentioned above fall into the framework of Assumptions 1-4. This means that the error rate lower bounds of Theorem 1 and Theorem 3 respectively, apply to the p-value combination methods listed above. That is, one cannot attain a better separation rate when considering the worst case Type II error probability for the alternative hypothesis in (2), with any of the p-value combination methods listed above. Whether Assumption 1 or 4 applies depends on whether shared randomness is used in generating the p-values. To confirm that Assumptions 3 and 2 apply to tests based on the combined p-values, some algebra is needed. The proof of the lemma is deferred to the appendix.

**Lemma 1**.: _Consider \(p\)-values \(p^{(1)},,p^{(m)}\), where each \(p^{(j)}\) depends on the local data \(X^{(j)}\) and possibly local randomness that is independent of the data. For each of the combination methods for \(p\)-values mentioned above and corresponding test \(T_{}\) of level \((0,1)\), the conclusions of Theorem 1 holds._

We remark that the p-values are obtained using shared randomness (i.e. in the sense of Assumption 1), the lower bound rate of Theorem 3 applies. Furthermore, as exhibited in Sections 2.1 and 2.2, for p-values corresponding to well chosen test statistics, these combination methods can achieve the theoretical limits established in Theorems 1 and 3, respectively.

### Combining e-values

An _e-value_ is a nonnegative random variable \(E\) such that \(_{_{0} H_{0}}_{0}E 1\). The _threshold test corresponding to \(E\) of level \(\)_ is \(\{E^{-1}\}\). This test yields a so called strict p-value; for \(_{0} H_{0}\) we have \(_{0}(E^{-1})\) by Markov's inequality.

E-values lend themselves for combining outcomes of independent studies for two main reasons. First, they are easy to combine, see Section 4 in  for an indepth discussion of specific combination functions for independent e-values. Second, they are robust to misspecification and offer optional stopping/continuation guarantees . Common examples of e-values are Bayes factors and likelihood ratios, which are nonnegative and have expectation equal to \(1\) in the case of a simple null hypothesis such as considered in this article.

Several combination methods (e-merging functions) were proposed in the literature. For instance, the product of independent e-values is also again an e-value. This was shown to weakly dominate any other combination of independent e-values in the sense that \(_{j=1}^{m}E^{(j)} C_{m}(E)\), for any \(E=(E^{(j)})[1,)^{m}\) and \(E C_{m}(E)\) such that \(C_{m}(E^{(1)},,E^{(m)})\) is an e-value for any independent e-values \(E^{(1)},,E^{(m)}\), see . Similarly, the average of e-values is again an e-value. The product and the average are _admissible_ in the sense that there is no e-merging function that strictly dominates them on \([0,]^{m}\). The lemma below shows that these two, arguably most prominent e-value combination methods fulfill Assumptions 1- 4 and hence the lower bounds derived in Theorems 1 and 3 apply.

**Lemma 2**.: _Consider e-values \(E^{(1)},,E^{(m)}\), where each \(E^{(j)}\) depends on the local data \(X^{(j)}\) and possibly local randomness that is independent of the data. Let \(C_{m}:^{m}\) correspond to either the average or the product and let \(T_{}\) be the corresponding threshold test of level \((0,1)\),_

\[T_{}=\{C_{m}(E^{(1)},,E^{(m)})^{-1} \}.\]

_If \(C_{m}\) is the product, assume in addition that \(_{0}| E^{(j)}|\) is uniformly bounded. Then, the conclusion of Theorem 1 holds. In case the e-values are generated using shared randomness, then Theorem 3 applies._

## 4 Simulations

In this section, we investigate the numerical performance of the testing strategies outlined in Section 2.1 on synthetic data sets. We compare the tests based on their receiver operating characteristic (ROC) curve. For a range of significance levels we compute for each tests the "true positive rate" (TPR) and "false positive rates" or (FPR), i.e. the fraction of the simulation runs in which the test correctly identifies the underlying signal, falselyjects the null hypothesis, respectively. Plotting the TPR against the FPR (both given as a function of the significance level) provides us the ROC curve, visualizing the diagnostic ability of the test.

In our simulations we set \(m=20\), \(n=30\), let \(d\) range from \(2\) to \(20\) and take \(^{2}=/(4n)\). This value of \(^{2}\) corresponds to a signal that is almost indistinguishable from noise using just a single trial, whilst consistently detectable if the data were to be pooled with \(m 20\) (which increases the signal size to noise ratio effectively by a factor \(>4\). For each level \(\{0.01,0.02,,0.99\}\) we compute the power for different combination strategies \(100\) times, each time drawing a different \(f^{d}\) with \(\|f\|_{2}=\) according to \(f_{i}=d^{-1/2} R_{i}\) and \(R_{i}\) iid Rademacher random variables for \(i=1,,d\). As combination strategies, we compare the strategies (7), (13) and (8) from Section 2,

Figure 1: ROC curves for different values of \(d\), whilst keeping \(m=20\), \(n=30\), \(^{2}=/(4n)\). From left to right, top to bottom: \(d=2\), \(d=5\), \(d=10\), \(d=20\).

which are called "chi-square combined", "coordinated directional" and "uncoordinated directional" in the legend of Figure 4. In addition, we display the ROC curves for the chi-square test based on pooled data ("chi-square pooled") and that of a single trial ("single trial").

We make the following observations, in line with our theoretical findings. The meta-analysis methods based on combining the locally optimal chi-squared test statistics (yellow curves) substantially outperformed the chi-squared test statistics based on a single trial (blue curve), but was substantially worse than the chi-square test based on the pooled data (pink curve). Second note that the large dimensional case (\(d=10\) and \(d=20\)) the best strategy is indeed to combine the local chi-square statistics (yellow curve), while in the low dimensional setting (\(d=2\)) it is more advantageous to combine the directional test statistics \(X_{i}^{(j)}\) (blue curve). Finally, note that allowing coordination between the trials by a shared randomness protocol can result in improved performance (green curve) compared to the independent experiments (blue curve). In fact this approach provides the best meta-analysis method in the small dimensional setting (e.g. \(d=2\) and \(d=5\) for small \(\), which is the most interesting case).

In the appendix, Section A.6, we explore eight additional simulation settings, where we consider larger values of \(d\) and \(m\). Whilst these simulations do not reveal additional phenomena to the ones observed in Figure 4, they do give insight into the relative performance of the testing methods for different values of \(d\) and \(m\).

## 5 Discussion

We briefly summarize our main contributions and discuss possible extensions and research directions. First, by establishing a connection between meta-analysis and distributed learning under communication constraints, we have provided a unified, theoretical framework for evaluating the behaviour of standard meta-analysis techniques. In our analysis, we considered the many normal means model, but these results can be extended to other more complex models as well, building on the connection with distributed computation. For example, minimax estimation rates under communication constraints were derived for other parametric models , density estimation , signal-in-Gaussian-white-noise [54; 38; 10], nonparametric regression  and in abstract settings  including binary and Poisson regression, drift estimation, and more. The normal means model allows for a tractable analysis, but results in this model are known to extend to more complicated models, such as discrete density testing (see e.g. ). With the due technical work, our results are expected to translate to these settings as well, but we leave this for future endeavor.

In the normal means model we show that by combining the locally optimal chi-square statistics at a meta-level one can gain a factor of \(\) compared to using a single trial. Nevertheless, regardless of the choice of the combination method, a factor of \(\) is lost compared to the scenario when all data from all trials are at our disposal. This loss is clearly visible even in small sample sizes, dimensions and trial numbers, as demonstrated in our numerical analysis, as can be seen in the corresponding ROC curves. For more complex models, such a numerical study can be a first step to quantify the efficiency of the meta-analysis method. We have also shown that in the small dimension - large number of trials setting combining the locally optimal chi-square statistics (or any rotationally invariant statistics for that matter) results in information loss and sub-optimal accuracy. In this case, better rates can be attained by test statistics based on the direction of the observations combined at the meta-level. In practice, one often cannot choose which test statistics can be obtained from independent trials. In such cases, the \(\)-factor loss in the case of e.g. rotationally invariant test statistics is of interest when considering power calculations. Meta-analysis approaches based on directional test statistics are designed for scenarios where individual datasets are not centrally collected, but there is some level of coordination among experimenters.

The assumption throughout the paper of homogeneity between the trials (i.e. each trial consisting of the same number of observations) simplifies the presentation, but the results can be extended to cases where the number of observations in each trial differ by constant factors. Situations where the number of observations differs greatly (e.g. \(k m\) trials have as much observations as the other \(m-k\) trials combined) are certainly of interest, but beyond the scope of this paper.

**Acknowledgements:** Co-funded by the European Union (ERC, BigBayesUQ, project number: 101041064). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. This research was partially funded by a Spinoza grant of the Dutch Research Council (NWO).