# Differentially Private Image Classification

by Learning Priors from Random Processes

Xinyu Tang Ashwinee Panda Vikash Sehwag Prateek Mittal

Princeton University

Equal contribution

###### Abstract

In privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD) performs worse than SGD due to per-sample gradient clipping and noise addition. A recent focus in private learning research is improving the performance of DP-SGD on private data by incorporating priors that are learned on real-world public data. In this work, we explore how we can improve the privacy-utility tradeoff of DP-SGD by learning priors from images generated by random processes and transferring these priors to private data. We propose DP-RandP, a three-phase approach. We attain new state-of-the-art accuracy when training from scratch on CIFAR10, CIFAR100, MedMNIST and ImageNet for a range of privacy budgets \(\). In particular, we improve the previous best reported accuracy on CIFAR10 from \(60.6\%\) to \(72.3\%\) for \(=1\). Our code is available at https://github.com/inspire-group/DP-RandP.

Figure 1: Our proposed DP training pipeline (DP-RandP) has three distinct phases. In Phase I, we sample images from random processes and train a feature extractor with representation learning to embed image priors beneficial for visual tasks. In Phase II, we spend a small privacy budget to train a linear classifier on top of extracted features of private data. In Phase III, we update all parameters with our remaining privacy budget. We demonstrate that incorporating image priors in Phase-I significantly improves DP training and adopting Phase II before training the whole network in Phase III can further improve test accuracy in DP training.

Figure 2: Comparing our results on CIFAR10 and previous state-of-the-art for \((,10^{-5})\)-DP setup. Our full DP-RandP outperforms all previous works on this commonly benchmarked dataset and reduces the privacy cost needed to achieve \(80\%\) accuracy from \(=6\) to \(=3\). Even our private linear probing from noise prior outperforms all previous work at \(=1\) but crucially has diminishing returns, a shortcoming that we address with our proposed three-phase DP training framework (DP-RandP).

Introduction

Machine learning models are susceptible to a range of attacks that exploit data leakage from trained models for objectives such as training data reconstruction and membership inference [58; 4]. Differential Privacy (DP) is the gold standard for quantifying privacy risks and providing provable guarantees against attacks [20; 21]. DP implies that the outputs of an algorithm e.g., the final weights trained by stochastic gradient descent (SGD) do not change much (given by the privacy budget \(\)) across two neighboring datasets \(D\) and \(D^{}\) that differ in a single entry.

**Definition 1** (Differential Privacy): _A randomized mechanism \(\) with domain \(\) and range \(\) preserves \((,)\)-differential privacy iff for any two neighboring datasets \(D,D^{}\) and for any subset \(S\) we have \([(D) S] e^{}[(D^{}) S]+\)._

Differentially Private Stochastic Gradient Descent (DP-SGD) [59; 1] is the standard privacy-preserving training algorithm for training neural networks on private data, with an update rule given by \(w^{(t+1)}=w^{(t)}-}{|B|}(_{i B}_{}((x_{i},w^{(t)}))+)\) where the changes to SGD are the per-sample gradient clipping \(_{}((x_{i},w^{(t)}))=,w^{(t)})}{(c,||(x_{i},w^{(t)})||)}\) and addition of noise sampled from a \(d\)-dimensional Gaussian distribution \((0,1)\) with standard deviation \(\). DP-SGD introduces bias and variance into SGD and therefore degrades utility, creating a challenging privacy-utility tradeoff. For example, the state-of-the-art accuracy in private training is only \(60.6\%\) on CIFAR-10 at \(=1\), while Dosovitskiy et al.  obtains \(99.5\%\) accuracy non-privately.

Theoretical analysis of the DP-SGD update yields that noise addition is especially harmful to convergence at the start of training , and that pretraining on public data can greatly improve convergence in this initial phase of optimization  by providing a better initialization . Previous works have improved the privacy-utility tradeoff of DP-SGD by pre-training on large publicly available datasets, such as ImageNet , to learn visual priors [50; 49; 54; 9]. Other works assume that a small subset of in-distribution data is publicly available [68; 2; 45; 52].

Interestingly, previous work has uncovered that synthetic data learned from random processes [44; 10; 22; 38] can be used in representation learning [30; 12; 11] to learn highly useful visual priors [39; 5]. Because there is a large distribution shift between synthetic images and natural images, training on synthetic images does not incur any privacy cost. In this work, we leverage noise priors learned from synthetic images to boost the performance of DP training, and make the following key contributions:

* We find that while noise priors are only marginally helpful in non-private training, we can unlock their full potential to improve private training with a carefully calibrated training framework.
* We provide empirical evidence that priors learned from random processes become more critical as the privacy budget decreases, because priors provide fast convergence at the start of training, and have a much larger impact on private training than non-private training.
* We find that training a single linear layer (linear probing) on top of a pretrained feature extractor that has learned noise priors is more robust to large amounts of noise addition than end-to-end training of the entire network. We demonstrate this insight by linear probing with a small privacy budget \(=0.1\) to \(57.1\%\) on CIFAR10, achieving nontrivial performance with lower privacy cost than previous work has considered.
* We observe that while linear probing from noise prior has diminishing returns on performance as the privacy budget increases, end-to-end training of the entire network continues to improve with large \(\) but critically struggles for small privacy budgets.
* We harness our insights by proposing a privacy allocation strategy that combines the benefits of learning from priors, linear probing, and full training into our full method DP-RandP (Visualized in Fig. 1). Our proposed approach pretrains a feature extractor on synthetic data to learn priors from random processes without paying privacy cost, then pays a small privacy cost for linear probing and makes the best use of our remaining privacy budget by updating all parameters to adapt our learned features to the private data.
* We evaluate DP-RandP against previous work and unlock new SOTA performance across CIFAR10, CIFAR100, MedMNIST and ImageNet across all evaluated privacy budgets \(\). We provide a snapshot of our results in Fig. 2.

DP-RandP: Mitigating the effects of noise in DP-SGD with noise prior

**Synthetic image generation without natural images.** Recent progress in computer vision shows that pretraining models on synthetic images without natural images [5; 6; 39] can learn visual priors that are competitive to priors from natural images. The synthetic images can either be generated from texture and fractal-like noise [6; 39], or structure priors extracted from an untrained StyleGAN .

**Noise prior from synthetic images for private training.** We consider pretraining on synthetic data, generated by random processes [5; 6] (example images are in Fig. 3), as a _noise prior_ in differentially private training. We use contrastive representation learning [12; 11; 30; 64], which aims to learn features invariant to common image transformations, to learn good visual features using synthetic images. At a high level, rather than using a random or 'cold' initialization for our downstream private dataset, we are using representation learning to obtain a 'warm initialization' that encodes priors learned from random processes. Across common natural vision tasks, noise priors also ensure that there is no privacy leakage from the pre-training data into this 'warm initialization'.

**How much noise prior benefits private training?** As an initial exploration, we obtain a warm initialization via representation learning on synthetic images and compare it with a competitive DP-SGD baseline  that uses cold (random) initialization (Fig. 4). We find that while the warm initialization only improves performance by \(2.7\%\) when \(=0\), i.e., non-private training, the warm initialization improves performance by \(12.5\%\) when \(=9.3\) (equivalent to \(=1\)). In Fig. 5, we zoom in on a single point of comparison between \(=0\) (dashed) and \(=9.3\) (solid) for warm and cold initializations. We find that over the course of training, as more noise is added, both the \(=9.3\) warm and cold initializations not only converge at the same rate (given an appropriately small learning rate) but also diverge from the non-private runs at similar rates. _Although warm and cold start obtain different results in the private setting, the difference is mostly due to the initialization and is therefore magnified at smaller privacy budgets_.

We support this claim by drawing a connection to previous works. Ganesh et al.  prove the existence of out-of-distribution public datasets that can achieve small test loss on target private datasets. Bu et al.  prove that in the gradient flow setting for the NTK regime, that is, when we are taking very small steps \( 0\), noise does not impact convergence. Mehta et al. , Panda et al.  propose scaling the learning rate \(\) inversely with the noise \(\). We combine our analysis with these previous works by noting that if we start training from fixed initialization and set the learning rate near-zero for small privacy budgets [50; 54], we enter the regime  where the noise does not impact convergence, and achieving nontrivial performance for these small privacy budgets provides the first empirical evidence for the theory  that _only the initialization matters_.

We gather our insights into a design goal that will enable us to achieve nontrivial performance under strict privacy constraints. We want to encode a learned prior into our initialization and then adapt this prior to private data. We now introduce DP-RandP, a method that achieves this key design goal.

### Three-phase differentially private training framework

We propose a three-phase DP training framework DP-RandP that has two phases after pretraining on synthetic images, and significantly improves the performance of DP training (Fig. 1). We first learn noise priors by training a feature extractor on synthetic data (Phase-I). We then split our private training into 1) Learning the head classifier with frozen features (Phase-II) and 2) End-to-end training of the entire network to co-adapt the feature extractor and head classifier (Phase-III).

Our design is motivated by the strengths and weaknesses of linear probing and end-to-end training in the private setting. First, the \(_{2}\) norm of the Gaussian noise added to gradients in each DP-SGD step scales with the number of parameters in the model. Because the head classifier typically has far fewer parameters than the feature extractor, updating this linear layer reduces the amount of added noise. Second, there is a large distribution shift between synthetic data from random processes and natural images found in private datasets. Linear probing merely inherits the frozen pre-trained features, but end-to-end training of all layers can improve the pre-trained features by adapting them to the private dataset. Because adapting to our private dataset may require many end-to-end training steps, we improve the convergence by first learning a task-specific linear classifier that can be trained quickly, and then training the entire network end-to-end. DP-RandP satisfies our previously stated design goal by first obtaining a good initialization with pretraining, then transferring the prior encoded in this initialization to the private dataset with fast-converging linear probing, and finally end-to-end training to adapt to the private dataset as much as possible for a given privacy budget.

We further consider the design choice of how to allocate the privacy budget for the two private training phases (\(_{1}\) and \(_{2}\)). Recall that our model is pre-trained on synthetic data in Phase I, thus it doesn't incur any privacy cost. In Phase II, we use DP-SGD with privacy budget \(_{1}\) to train the linear classifier. We observe diminishing returns in accuracy with increasing \(_{1}\), so we spend a smaller privacy budget on linear classifier training and allocate the rest to training the full network in Phase III. Allocating a higher budget to full training does not have diminishing returns; consider that \(_{2}=\) we recover non-private accuracy. We now rigorously evaluate the performance of DP-RandP.

## 3 Evaluation

To outline the evaluation section we first overview our experimental setup and then evaluate the performance of DP-RandP on CIFAR10/CIFAR100/DermaMNIST in Sec. 3.2. We find that our method outperforms all previous works across multiple datasets, architectures, and privacy budgets. In Sec. 3.3 we find that our principled allocation of privacy budget \(_{1},_{2}\) between linear probing and end-to-end training is robust to different choices of \(_{1}\) and \(_{2}\). We also find that the private linear probing is a strong computationally efficient baseline. Specifically, we provide a new SOTA on ImageNet with private linear probing. We next provide a quantitive comparison of DP-RandP to DP with public data. Finally we discuss the computational costs of our method and find that DP-RandP can provide computational savings over previous methods.

### Experimental setup

**Learning priors from images generated by random processes with representation learning.** In Phase I we sample images from StyleGAN-oriented , and train a feature extractor on these synthetic datasets with representation learning [12; 64] with the loss function proposed in Wang and Isola . Although our method can accommodate any kind of synthetic data and representation learning method, we focus our evaluation on these datasets and methods. We consider other kinds of synthetic data and other representation learning in Appendix A.

**Datasets and models.** We evaluate DP-RandP on CIFAR10/CIFAR100 , DermaMNIST in MedMNIST [65; 66] and private linear probing version of DP-RandP on ImageNet . For CIFAR10/CIFAR100, we use WRN-16-4 following De et al. . For MedMNIST, we use ResNet-9 following Holzl et al. . We choose WRN-16-4 and ResNet-9 because these architectures for the corresponding datasets achieve the most compelling results in previous works [15; 34]. For ImageNet, we use a ViT-base  feature extractor pretrained on Shaders-21k  provided by Yu et al.  and train a linear classifier. We provide the results on CIFAR10, CIFAR100, DermaMNIST in Sec. 3.2 and results on ImageNet in Sec. 3.3. We also report results of WRN-40-4 on CIFAR10 in Appendix B.

Implementation details.To ensure a fair comparison with previous works [15; 57; 35], we use standard DP-SGD  and make use of multiple data augmentations and exponential moving average (EMA) as proposed by De et al. , that are now standard techniques used in DP-SGD work [57; 35]. We allocate small privacy budget \(_{1}\) to Phase II and remaining privacy budget \(_{2}\) to Phase III according to the strategy detailed in Sec. 3.3. We report our results across different privacy costs and use \(=10^{-5}\) for CIFAR10/CIFAR100/DermaMNIST by following previous works [15; 34].2 When we report results, we report the standard deviation and accuracy averaged across 5 independent runs with different random seeds. We report implementation details for CIFAR10/CIFAR100/DermaMNIST in Appendix C and for ImageNet in Appendix G.

### Evaluation of DP-RandP

We report the results of DP-RandP in Tab. 1, Tab. 2 and Tab. 3 for CIFAR10, CIFAR100 and DermaMNIST datasets, respectively.

DP-RandP outperforms all previous works across all privacy budgets.In Tab. 1 we find that DP-RandP obtains higher performance than previous works [15; 35; 61; 40; 18; 69; 55] on CIFAR10 across the standard evaluated privacy budgets \(\).

We first compare DP-RandP to De et al.  as our CIFAR10 model, optimizer and hyperparameters follow De et al. ; the only difference is the use of Phase I and Phase II in DP-RandP to learn a prior from synthetic data and allocate a small privacy budget to linear probing. Crucially DP-RandP outperforms De et al.  by _more than \(15\%\)_ for the important privacy budget \(=1\).

Tramer and Boneh  use a ScatterNet  to encode invariant image priors and Holzl et al.  use equivariant CNNs  to learn transform invariant features. DP-RandP shares the same intuition of leveraging invariant image priors as these works [61; 35]. Instead of leveraging model architecture design for invariant features, we achieve this intuition by learning priors from images generated from random processes and design our three-phase framework to optimize use of this prior. Although DP-RandP shares this intuition of leveraging invariant image priors, DP-RandP achieves \(12\%\) improvement over previous works [61; 35] who both achieve \(66\%\) at \(=1\). Our improvement comes both from leveraging the priors from synthetic images and our design of three learning phases that makes the best use of priors. We provide a detailed comparison to Tramer and Boneh  in Sec. 4 where we explain why and how the feature prior we learn from synthetic data provides better results than the feature prior provided by their architectures.

CIFAR100 results.Previous work has not provided results on training from scratch for CIFAR100, perhaps because as we find in Tab. 2 the DP-SGD baseline following De et al.  does not perform well for \(=3\). We believe this to be a competitive baseline, but DP-RandP outperforms it by more than \(10\%\) across \(\). Although CIFAR10 and CIFAR100 are both benchmark computer vision datasets, we find CIFAR100 to be a much more challenging task for private learning. One possible

   Method & \(=1\) & \(=2\) & \(=3\) & \(=4\) & \(=6\) & \(=8\) & \(=\) \\  Tramér and Boneh  & \(60.3\) & \(67.2\) & \(69.3\) & \(-\) & \(-\) & \(-\) & \(73.8^{*}\) \\ De et al.  & \(56.8\) & \(64.9\) & \(69.2\) & \(71.9\) & \(77.0\) & \(79.5\) & \(88.9\) \\ Holzl et al.  & \(60.59\) & \(71.86\) & \(75.96\) & \(78.27\) & \(80.26\) & \(81.62\) & \(-\) \\  DP-RandP & \(72.32_{0.22}\) & \(77.25_{0.07}\) & \(79.99_{0.21}\) & \(81.88_{0.27}\) & \(84.01_{0.23}\) & \(85.26_{0.11}\) & \(91.69\) \\   

Table 1: Test accuracy (\(\%\)) of DP-RandP and comparison to previous work on CIFAR10. Not shown in this table are Klause et al. , Dormann et al. , Papernot et al. , Yu et al.  because they achieve \(71.5\%\) at \(=7.5\),\(70.1\%\) at \(=7.42\), \(66.2\%\) at \(=7.53\) and \(63.4\%\) at \(=8\) respectively, that are not on the pareto frontier of previous work.

explanation for this is that private classifiers struggle to distinguish between many classes because the added noise is more likely to shift the decision boundary. We use the same hyperparameters for CIFAR10 and CIFAR100, that are likely suboptimal for CIFAR100, and the performance gap between \(=8\) and \(=\) may be mitigated if we train on CIFAR100 for longer than we do on CIFAR10. We encourage the use of CIFAR100 as a standard benchmark for private learning in the future because we find limited room for improvement on CIFAR10. In particular, the private and non-private gap between \(=8\) and \(=\) for CIFAR10 is only \( 6\%\) for DP-RandP.

DermaMNIST results.We have shown that DP-RandP outperforms previous work on the standard CV benchmarks of CIFAR10 and CIFAR100, and now consider the privacy sensitive medical dataset DermaMNIST. Although CIFAR is a standard CV benchmark, there is limited previous work that evaluates on privacy sensitive data in CV such as medical images. In Tab. 3 we find that DP-RandP achieve improvements from \(=1\) to \(=7.42\) by up to \(2.78\%\) over the DP-SGD baseline that we evaluate. Also, DP-RandP can achieve similar accuracy at \(=4\) as the result of DP-SGD at \(=7.42\), which reduces the privacy cost from \(=7.42\) to \(=4\). We also include the results of Holzl et al. , that use equivariant neural networks . DP-RandP is a uniform framework applicable to any model architectures. We leave the systematic investigation of more model architectures, such as equivariant neural network [14; 35] in DP-RandP, for future work.

We find that the prior we learn from synthetic data is applicable to standard benchmarks and medical images. One direction for future work is to validate the robustness of this prior across more datasets.

### Allocating privacy budget in DP-RandP

In this subsection we analyze the privacy budgets of Phase II (\(_{1}\), linear probing) and Phase III (\(_{2}\), full training) and how to allocate the overall privacy budget \(\) among Phase II and Phase III. Specifically, we will present a new STOA on ImageNet with additional designs on linear probing.

In Fig. 6 we observe that DP-RandP is robust to the key algorithmic design choice of how much privacy budget to allocate to Phase II and Phase III (Please check Apendix E for more details of Fig. 6 and more results on different \(\)).

In particular, we find that even the worst choice of \(_{1}/\) provides results comparable to the previous SOTA on CIFAR10 at \(=1\).3 We first investigate the behavior at each extrem and conclude a general allocation strategy that provides good performance across CIFAR10, CIFAR100 and DermaMNIST.

   Method & \(=3\) & \(=4\) & \(=6\) & \(=8\) & \(=\) \\  DP-SGD & \(30.73\) & \(34.45\) & \(39.66\) & \(44.22\) & \(66.68\) \\  DP-RandP & \(43.33_{0.15}\) & \(46.40_{0.31}\) & \(51.53_{0.13}\) & \(55.02_{0.21}\) & \(71.68\) \\   

Table 2: Test accuracy (\(\%\)) of DP-RandP and comparison to DP-SGD baseline on CIFAR100.

   \(\) & \(=1\) & \(=4\) & \(=7.42\) & \(=\) \\  baseline in Holzl et al.  & \(-\) & \(-\) & \(72.41\) & \(78.48^{*}\) \\ best in Holzl et al.  & \(-\) & \(-\) & \(74.17\) & \(77.84^{*}\) \\  DP-SGD we evaluated & \(69.00_{0.37}\) & \(71.78_{0.87}\) & \(74.08_{0.41}\) & \(77.27\) \\ DP-RandP & \(71.78_{0.40}\) & \(74.82_{0.55}\) & \(75.91_{0.29}\) & \(79.26\) \\   

Table 3: We follow previous work  and report the validation accuracy (\(\%\)) of DP-RandP on DermaMNIST. We also report the test accuracy in Appendix D.

Figure 6: The fraction of total privacy budget allotted to Phase II for \(=1\). The performance is stable across a wide range of value [0.1, 0.4]. However, skipping either Phase-II or Phase-III leads to suboptimal test accuracy in DP training.

Allocating the entire privacy budget to Phase II is competitive for small privacy budgets but provides diminishing returns.This corresponds to the right extreme in Fig 6 (\(_{1}/=1\)) and is equivalent to doing linear probing on top of extracted features. We follow the training recipe in Panda et al.  and report the result of private linear probing in Tab. 4 for CIFAR10 and Tab. 5 for ImageNet  respectively (\(=7.8 10^{-7}\) is by \(=1/|D_{train}|\) for ImageNet). We provide the detailed experimental set-up for CIFAR10 in Appendix F and ImageNet in Appendix G.

**CIFAR10 results.** We present the result of private linear probing on CIFAR10 by including more conservative privacy constraints like \(=0.03\). Our private linear probing can achieve \(57.10\%\) at \(=0.1\), that is comparable to the result of \(=1\) in De et al.  that fully trains a WRN-16-4. Notably, previous work  also trains a neural network on top of the handcrafted features using an untrained ScatterNet . DP-RandP is slightly better than their result at \(=3\). We can see that our non-private baseline (\(74.05\%\)) is slightly better than the non-private baseline (\(73.8\%\)) in Tramer and Boneh , that shows that our feature extractor is better than the ScatterNet and therefore improves the performance under DP-SGD. We include a detailed comparison to Tramer and Boneh  in Sec. 4. However, this private linear probing variant of DP-RandP can only achieve \(74.05\%\) with no noise added, that is lower than the result at \(=4\) in De et al. , that shows that allocating all privacy cost to Phase II is a sub-optimal design choice.

**ImageNet results.** We achieve a new SOTA results on ImageNet. We achieve \(39.39\%\) accuracy at \(=8\) and the previous SOTA  is \(39.2\%\) at \(=8\). We use a Vit-base  model pretrained on Shaders-21k (the model checkpoint is provided by Yu et al. ). If we directly do Phase II with extracted features, we can achieve around \(33\%\) accuracy at \(=8\), that is comparable to De et al. . The direct linear probing result shows that linear probing (Phase II only after pretrained on synthetic data) is not enough for difficult tasks like ImageNet, and therefore updating full parameters in Phase III is necessary. While training the full ViT model on large datasets like ImageNet needs much computation resources (for example, Sander et al.  used 32 A100 GPUs), we could leverage the core concept of DP-RandP and adapt it to a computationally efficient version, i.e., we train a linear layer with additional modifications. We then obtain a new SOTA result of \(39.39\%\) on the ImageNet-1k validation dataset. We now briefly explain the two main modifications of DP-RandP to achieve \(39.39\%\). We provide a detailed explanation in Appendix G. _We emphasize that these modifications are more for computational efficiency; if we had enough compute to do full fine-tuning of the ViT on ImageNet with sufficiently large batch size, our original DP-RandP would still work._

Our first modification is to approximate full fine-tuning by linear probing on larger feature representations that we create by aggregating intermediate representations from the network. This is because each block of vision transformers learns a different representation. However, linear probing only takes the representation from the penultimate layer as input and therefore the final representation may not be sufficient to learn the task. The representation of the input image after each block in the ViT has both a temporal and feature dimension, so we pool over the temporal dimension to gather a feature map of size (4, feature size). We concatenate the feature map of different blocks into a one-dimensional vector. By doing linear probing on these much larger features, we can also take advantage of intermediate representations.

Our second modification is to approximate the work of a LayerNorm or other normalization layer that we would update during full fine-tuning of the entire ViT, by manually normalizing the features.

   \(\) & \(0.03\) & \(0.1\) & \(0.2\) & \(0.5\) & \(1\) & \(2\) & \(3\) \\  SOTA & - & - & - & - & \(60.3\) & \(67.2\) & \(69.3\) \\  Ours & \(40.64\) & \(57.10\) & \(60.89\) & \(65.10\) & \(67.78\) & \(69.92\) & \(71.08\) \\ (Std.) & \(2.59\) & \(0.42\) & \(0.26\) & \(0.21\) & \(0.17\) & \(0.09\) & \(0.14\) \\   

Table 4: Test accuracy (\(\%\)) of our private linear probing and comparison to previous SOTA for private learning on top of extracted features  on CIFAR10. Note that, our result is training a linear layer on top of the extracted features. The result of previous SOTA is by training a CNN on top of extracted features. Tramer and Boneh  also report the private linear probing result \(67.0\%\) at \(=3\).

   \(\) & \(1\) & \(8\) \\  De et al.  & - & \(32.4\) \\ Sander et al.  & - & \(39.2\) \\  Ours (ViT) & \(26.54\) & \(39.39\) \\ (Std.) & \(0.11\) & \(0.03\) \\   

Table 5: Test accuracy (\(\%\)) of our private linear probing with additional designs on ImageNet.

To do this we first normalize each feature vector to a fixed norm. We next privately estimate the mean over the entire ImageNet feature vector dataset, using the Gaussian mechanism with a small privacy cost, and subtract the private mean from all feature vectors. This is equivalent to doing non-private centering and then adding the same Gaussian noise to the entire dataset. This procedure can be thought of as a one-time approximation to the normalization layer, which is known to speed up training by centering the data.

Within the two modifications of direct linear probing, we improve upon previous SOTA  and achieve \(39.39\%\) at \(=8\). This method is computationally efficient and one run can be done on a single A100 GPU in a few hours. We also provide the result for a stronger privacy guarantee, i.e., \(26.54\%\) at \(=1\).

Allocating the entire privacy budget to Phase III struggles for small privacy budgets.We report DP-RandP without Phase II on CIFAR10 in Tab. 6 (equals to \(_{1}/=0\) in Fig. 6). The result in Tab. 6 is slightly worse than DP-RandP in Tab. 1 and the utility gap between Tab. 6 and Tab. 1 decreases as \(\) increases, that justifies the importance of Phase II in DP-RandP. Moreover, the result of DP-RandP without Phase II is significantly better than previous SOTA [15; 35; 61], that shows that the feature extractor pretrained on images from random process can capture the image prior.

A general privacy budget allocation strategy.We have observed that allocating the entire privacy budget to linear probing or full training is suboptimal. We now propose a simple yet effective general strategy to allocate the privacy budget. For small \(\) (\( 1\)), we set \(_{1}=\) to allocate the entire privacy budget to Phase II. This is because allocating the entire privacy budget to linear probing is competitive for small privacy budgets. As \(\) increases, we decrease \(_{1}/\). This is because as \(\) increases, the noise multiplier will decrease. Therefore, it is easier to train the linear probing layer as \(\) increases, and the percentage of total steps allocated to Phase II can be reduced to train a good linear probing layer and we can use the remaining steps for Phase III. Because the closed-form computation of \(\) with numerical privacy loss distribution accounting by Gopi et al.  is challenging, we implement this strategy by using a fixed number of steps \(n\) for linear probing in CIFAR10, CIFAR100, DermaMNIST, and increasing the number of steps in Phase III as \(\) increases. We present the privacy allocation \(_{1}/\) on CIFAR10 in Appendix E for results in Tab. 1, which validates this strategy.

### In comparison to DP with public data

A major direction in improving the privacy utility trade-off for DP-SGD is by incorporating priors that are learned on real-world public data [68; 2; 45; 52; 50; 49; 54; 56]. These priors are either from small in-distribution data [68; 2; 45; 52] or large scale public data [50; 49; 54; 56].

We present a quantitative comparison of DP-RandP and DP with public data works [54; 50; 49; 52] in Tab. 7. Our DP-RandP is comparable with Nasr et al. , which in fact utilizes a limited amount of in-distribution data as public data for pre-training. This indicates that the prior learned from images

   \(\) & \(=1\) & \(=2\) & \(=3\) & \(=4\) & \(=6\) & \(=8\) & \(=\) \\  Accuracy\((\%)\) & \(69.03_{0.23}\) & \(75.31_{0.28}\) & \(78.44_{0.19}\) & \(80.56_{0.12}\) & \(82.90_{0.10}\) & \(84.45_{0.09}\) & \(91.69\) \\   

Table 6: Fully privately training a WRN-16-4 with warm initialization. Test accuracy on CIFAR10.

   Method & Model & Public Data & \(=1\) & \(=2\) & \(=4\) & \(=6\) & \(=8\) \\  Nasr et al.  & WRN-16-4 & \(4\%\) ID & \(72.10\) & \(75.10\) & \(77.9\) & \(80.0\) & \(-\) \\  Mehta et al.  & ViT-B/16 & ImageNet & \(95.10\) & \(95.10\) & \(95.10\) & \(-\) & \(95.20\) \\ Mehta et al.  & ViT-G/16 & JFT & \(98.80\) & \(98.80\) & \(98.83\) & \(-\) & \(98.84\) \\ Panda et al.  & beitv2 & ImageNet & \(99.00\) & \(-\) & \(-\) & \(-\) & \(-\) \\  DP-RandP & WRN-16-4 & Synthetic & \(72.32\) & \(77.25\) & \(81.88\) & \(84.01\) & \(85.26\) \\   

Table 7: Comparison of DP-RandP with methods using public data. Test accuracy \((\%)\) on CIFAR10.

generated from random processes can help as much as the prior learned from limited in-distribution public data. Compared to works [50; 49; 54] with access to large public data, there is still a gap between our DP-RandP and these works. Note that we consider a different threat model to this line of work where we do not have access to public data  and must instead make the best possible use of images drawn from random processes. Closing the gap between leveraging synthetic data and leveraging large-scale real public data is an interesting direction for future work.

### Computational cost

DP-RandP consists of three phases. Some phases can be done in a single-run per dataset. For example, for feature extractor in Phase I, we only need to train a single feature extractor once for each evaluated dataset as the synthetic dataset and model architectures are the same. Also, for the private linear probing (LP). We report the computation cost in Tab. 8 for single-run per dataset. For Phase II and Phase III, we need to go through these processes at each run of our training process. Also, after we get the extracted features for LP, we need to train a linear layer each time we run the experiments. We report the computation cost in Tab. 9 for these procedures.

De et al.  also needs to fully train a WRN-16-4 and the major additional computation cost for DP-RandP is Phase I compared to De et al. . However, the training in Phase I is done once for CIFAR10. Moreover, we can use the same feature extractor from Phase I for CIFAR10 and CIFAR100.

For the private linear probing experiment, each run of training a linear layer can be finished in \(1\) minute for CIFAR10 and \(320\) minutes for ImageNet while fully privately training a model costs much more time. A single run to privately train a WRN-16-4 for CIFAR10 takes around \(5.5\) hours for \(875\) steps with \(1\) A100 GPU in our evaluation. Also, as reported in previous work , a single run for ImageNet experiments needs to take four days using \(32\) A100 GPUs.

## 4 Discussion and related work

In this section we first provide a detailed comparison to previous work  that also uses priors to improve DP-SGD image classification. We then give an overview of the broader body of work on improving the privacy utility tradeoffs in DP-SGD. Finally we discuss the previous work  that also uses the two-stage training with domain-specific data for a different reasoning.

**Discussion on DP-RandP and Tramer and Boneh .** Tramer and Boneh  find that training a neural network (linear layer or CNN) on top of 'handcrafted' ScatterNet  features outperforms private 'deep' learning. While this method performs well for smaller values of \(\), the non-private accuracy is limited because the features cannot be adapted to the private data. There are two key differences between DP-RandP and Tramer and Boneh . In Phase I we use representation learning to train the feature extractor on images sampled from random processes, to learn the prior that extract transformations-invariant features. Our feature extraction process is therefore much more general, and while we explore the use of different synthetic data, model architectures, and representation learning methods, there are many more methods in each of these categories that we have not explored. The second difference is between the training on top of extracted features, that is used by Tramer and Boneh  and the private linear probing that we consider as a baseline in Fig. 2, and the combination of linear probing and full training that we use in DP-RandP. Comparing the improvements between DP-RandP and Tramer and Boneh  confirms that our empirical improvements are mostly due to the advantage of DP-RandP over DP linear probing. In particular, for \(=3\), exchanging the handcrafted features of Tramer and Boneh  for the pretrained feature extractor we use only improves performance by a modest \( 2\%\). However, our full DP-RandP exhibits more than \(10\%\)

    & Phase I & feature extraction in LP \\  Time & \(16\) h & \(1\) min \\   

Table 8: One-time per dataset computational cost on CIFAR10. These procedures only need to be done once for each evaluated dataset. Phase I can be shared for CIFAR10 /CIFAR100.

improvement. Our innovation over Tramer and Boneh  is therefore twofold: we introduce the potential of pretraining on synthetic data for the DP community, and also provide guidance on how to better transfer features learned from synthetic data to private training.

**DP with public data.** A major direction in improving the privacy utility tradeoff in DP-SGD for image classification is the principled use of public data. Several works [68; 2; 45; 52] make use of public data under a different threat model by treating a small fraction of the private training dataset as public. There is also another line of work that leverages a large real-world public dataset to pretrain models [50; 49; 54; 56].

Besides directly training image classification by DP-SGD, another direction is DP-trained generative models. The generated images can be used for classification tasks without additional privacy costs. Recent work  show that DP diffusion models can achieve high-quality images when pretrained on large public data like ImageNet and achieve \(88.8\%\) classification accuracy for CIFAR10 at \(=10\).

Another line of work has shown the success of DP-SGD fine-tuning of pretrained large language models (LLMs) [28; 47; 70]. LLM pretraining can be framed as a way to learn structural priors from unstructured data .

**DP-SGD training from scratch.** In addition to the directly related works discussed above, the baselines for training from scratch that we compare to in this work are De et al.  and Sander et al. . De et al.  make use of multiple techniques that are now a mainstay of DP-SGD training from scratch such as multiple data augmentations  that we also use. Sander et al.  propose a method for estimating the best hyperparameters for DP training at scale using smaller-scale runs. Recent works [34; 35] propose the use of inductive bias via architectural prior. They use DP-SGD to train the equivariant CNN architecture  and achieve \(81.6\%\) at \(=8\) on CIFAR-10. We note that the design space of novel architectures that are especially compatible with DP is rich and mostly unexplored, and our approach is compatible with any advancements in this domain. In particular, using the architecture of Holzl et al. [34; 35] in DP-RandP could potentially enjoy the improvements by combining the feature priors. We leave this exploration as future work.

**Two-stage training with domain-specific data.** The two-stage training of first training the classifier head and then tuning all hyperparameters has also shown to be effective for out-of-distribution (OOD) tasks . Their reasoning is that if the full parameters are directly updated using the in-distribution training data, this may lead to the loss of some general features learned in the pretrained stage and result in utility drop on OOD data. Our task of DP image classification is different from the OOD task  but still shares some common intuition. After Phase I, our feature extractor has learned some useful priors while the classifier head has a random initialization. If we directly update the full network, too much noise is added to the full network, which may distort features learned in Phase I and lead to suboptimal performance. We also conduct experiments and find that, without noise prior (therefore model is randomly initialized), the two-stage training pipeline would not significantly improve the performance compared to directly training the full parameters (See Appendix H).

## 5 Conclusion

We leverage images generated from random processes and propose a three-phase training DP-RandP to optimize the use of noise prior. The evaluation across multiple datasets including the benchmark datasets CIFAR10 and ImageNet shows that DP-RandP can improve the performance of DP-SGD. For example, DP-RandP improves the previous best reported accuracy on CIFAR10 from \(60.6\%\) to \(72.3\%\) at \(=1\). DP-RandP is a general framework for different datasets, models, and representation learning methods. Future improvements of designs in each of these categories would potentially improve the performance of DP-RandP. DP-RandP makes use of priors from synthetic images. It would be interesting to study whether DP-RandP would improve the priors for DP with public data. Also, investigating the priors beyond image domains, e.g., language and speech tasks, for differentially private training would also be of great interest.