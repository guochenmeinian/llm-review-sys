# LLM-based Skill Diffusion for Zero-shot Policy Adaptation

Woo Kyung Kim\({}^{1}\), Youngseok Lee\({}^{2}\), Joyoung Kim\({}^{1}\), Honguk Woo\({}^{1}\)

\({}^{1}\) Department of Computer Science and Engineering, Sunkyunkwan University

\({}^{2}\) Department of Electrical and Computer Engineering, Sungkyunkwan University

{kwk2696,yslee.gs,onsaemiro,hwoo}@skku.edu

Honguk Woo is the corresponding author.

###### Abstract

Recent advances in data-driven imitation learning and offline reinforcement learning have highlighted the use of expert data for skill acquisition and the development of hierarchical policies based on these skills. However, these approaches have not significantly advanced in adapting these skills to unseen contexts, which may involve changing environmental conditions or different user requirements. In this paper, we present a novel LLM-based policy adaptation framework LDuS which leverages an LLM to guide the generation process of a skill diffusion model upon contexts specified in language, facilitating zero-shot skill-based policy adaptation to different contexts. To implement the skill diffusion model, we adapt the loss-guided diffusion with a sequential in-painting technique, where target trajectories are conditioned by masking them with past state-action sequences, thereby enabling the robust and controlled generation of skill trajectories in test-time. To have a loss function for a given context, we employ the LLM-based code generation with iterative refinement, by which the code and controlled trajectory are validated to align with the context in a closed-loop manner. Through experiments, we demonstrate the zero-shot adaptability of LDuS to various context types including different specification levels, multi-modality, and varied temporal conditions for several robotic manipulation tasks, outperforming other language-conditioned imitation and planning methods.

## 1 Introduction

Skill-based learning has demonstrated its potentials in generalizing to novel downstream tasks by leveraging pre-trained skills learned from the offline dataset. Furthermore, the integration of skill-based learning and natural language realizes the remarkable ability to perform practical tasks via the provision of a human-oriented interface where agents are controlled by instructions describing the goals of the task. Building upon this notion, several previous studies have investigated bridging the gap between the human instructions and physical world manipulation by learning semantically meaningful skills given the language-annotated dataset . However, due to the inherently open-ended nature of language, it is impractical to obtain a dataset annotated with a sufficiently wide range of contexts, encompassing various environmental conditions and user requirements, to develop a versatile language-conditioned policy capable of accommodating such diverse contexts. Consequently, as shown in the left side of Figure 1, these prior works are limited to processing the narrow scope of instructions that primarily convey only the goal of the task without any contextual information (presented as case 1).

To address the challenges in language-conditioned skill learning, we explore large language model (LLM)-based policy adaptation approaches that enable zero-shot adaptation to contexts specified inlanguage at test-time. Leveraging the effectiveness of diffusion models in controlling their generation process via loss functions , and the code generation capability of LLMs , we adapt diffusion with guidance by the loss function generated through LLMs. As illustrated in the right side of Figure 1, our LLM-based policy adaptation is capable of adapting to diverse contexts in a zero-shot manner, such as "target speed of an agent should be 8m/s" or "minimize the power usage of an agent".

To this end, we present a novel LLM-based Skill Diffusion (LDuS) framework, designed to facilitate zero-shot adaptation to unseen contexts by generating skill trajectories that are controllable through loss-guided diffusion. Specifically, we devise a hierarchical skill learning structure in which a diffusion model is employed as a skill planner with sequential in-painting. This in-painting method sequentially substitutes consecutive state-action pairs with their originals and learns on the remaining parts, thus allowing for robust trajectory generation by conditioning the trajectory on past experiences. For evaluation, LDuS provides an interface where a context specified in language is translated into a loss function to guide the generation process of the skill diffusion planner. The generation process is then continuously refined through an iterative process using an LLM as a self-critic, ensuring that the generating skill trajectory aligns with the given context. As such, our framework stands apart from existing language-conditioned skill imitation approaches, as it enables the zero-shot adaptation of skill-based policies to various contexts that extend beyond the training dataset.

The contributions of our work are summarized as follows.

* We present the LDuS framework to address a novel challenge of zero-shot policy adaptation to unseen contexts specified in language.
* We develop a hierarchical skill learning structure that adapts the skill diffusion planner with sequential in-painting, enabling robust skill trajectory generation.
* We devise an interface that utilizes LLMs to translate a context into a loss function, which is then used to control the generation process of the skill diffusion planner. This is further validated via iterative refinement to adequately align with the given context.
* We experimentally show that LDuS achieves superior performance in zero-shot adaptation to a wide range of contexts including different specification levels, multi-modality, and varied temporal conditions for robotic manipulation tasks.

## 2 Related work

### Language-conditioned skill learning

In the domain of sequential decision-making, several researches have explored techniques for learning language-conditioned skills [1; 2; 3; 6; 7; 8]. LISA  employs hierarchical skill learning to achieve

Figure 1: Zero-shot policy adaptation to contexts: In case 1, the instruction includes only the task goal. In cases 2 and 3, the instruction is supplemented by the task goal with the context. Conventional language-conditioned skill approaches struggle to generate trajectories well aligned with the contexts, and typically succeed only for instructions as in case 1. Conversely, our LLM-based policy adaptation approach effectively adapts to the contexts in a zero-shot manner across all cases.

a language-conditioned policy through discretized skill codes. Recently, LCD , PlayFusion , and SkillDU  commonly adopt diffusion models as a language-conditioned policy to address high-dimensional vision inputs or to leverage play datasets collected by human. While these approaches primarily concentrate on learning by direct supervision from language-annotated datasets, our LDuS aims at adapting to unseen contexts that convey varying environment conditions or different user requirements beyond the datasets.

Meanwhile, several studies have harnessed the code generation capabilities of LLMs to ground language instructions to actionable skills [9; 10; 11; 12; 13; 14]. For example, in Kinematic-LLM , an LLM is prompted with in-context samples to generate the waypoints for robotic manipulation with pre-defined code primitives. The performance of this in-context learning often depends on the quality and relevance of selected samples, leading to limited generalization to unseen contexts. Unlike this waypoint-based high-level planning with pre-defined code primitives, our LDuS adapts the code generation capabilities of LLMs with iterative refinement to enable the fine-grained control of trajectories, particularly suited for adjusting the generation process of diffusion models.

### Guided control for diffusion models

Diffusion models have shown promising results in various areas including computer vision [15; 16; 17; 18], offline reinforcement learning (RL) [19; 20; 21], and long-horizon planning [22; 23; 24; 25]. The strong generation ability of diffusion models leads to robust adaptability at deployment. In classifier guidance , diffusion models are controlled at test-time, thereby supporting the generation of images belonging to specific classes. This controlled adaptation concept has been further investigated for text-driven image generation [27; 28] and noisy inverse problems [29; 30]. Recently, LGD  presents a loss-guided diffusion mechanism, by which diffusion models can be controlled via differentiable loss functions without additional training. To facilitate the query-compliant scene generation, CTG  leverages LLMs as a loss function generator for user queries.

In the RL domain, such guidance schemes for controlling diffusion-based policies have been investigated with pre-trained dynamics models  and value functions [22; 23]. Yet, these schemes rarely accommodate language-specified contexts. Our LDuS is the first to integrate the reasoning capabilities of LLMs and the controlled generation capabilities of diffusion models, thus enabling zero-shot policy adaptation to language-specified contexts in the domain of sequential decision-making.

## 3 Preliminaries

### Problem formulation

**Contextual Markov Decision Process (MDP).** We consider a task as a contextual MDP [32; 3] (\(,S,A,,P^{c},r^{c},,_{0}\)) where \(c\) is a context space, \(s S\) is a state space, \(a A\) is an action space, \(g\) is a goal space, \(P^{c}: S A S\) is a transition probability conditioned on the context and the goal, \(r^{c}: S A\) is a reward function conditioned on the context, \(\) is a discount factor, and \(_{0}:S\) is an initial state distribution. Here, we consider a goal to be specified in language, denoted as \(g_{l}\), such as "open the drawer" or "close the door". Moreover, we assume a context is provided in language , denoted as \(c_{l}\), which can affect either the reward function or the transition probability.

**Policy adaptation to contexts.** We assume access to a dataset \(=\{_{i}\}_{i N}\), where each trajectory \(_{i}\) is represented as a sequence of state and action pairs with a goal \(\{(s_{t},a_{t},g_{l})\}_{t T}\) for a \(T\)-length episode without any contextual information.

We consider task evaluation scenarios with a context \(c_{l}\) which conveys environmental conditions or user requirements, along with a goal \(g_{l}\). Then, our objective is to develop a policy adaptation framework \((g_{l},c_{l})\) that maps both goal \(g_{l}\) and context \(c_{l}\) to a policy \(_{c}\) maximizing the return of context-conditioned rewards.

\[^{*}=*{argmax}_{}*{}_{g_{l},c_{l},\\ _{c}(g_{l},c_{l}),a_{t}_{c}(|s_{l},g_{t})} [_{t=0}^{T-1}r^{c_{l}}(s_{t},a_{t},g_{l})]\] (1)

### Diffusion probabilistic models

Diffusion models have been explored for task planning [22; 23; 24; 33], offline RL [19; 20; 21], and language-conditioned skill learning [3; 6; 8]. In , a diffusion model for planning was introduced, which generates \(h\)-length state and action sequences, denoted as \(x\) in a two-dimensional array.

\[x=s_{0}&s_{1}&&s_{h}\\ a_{0}&a_{1}&&a_{h}\] (2)

In particular, the diffusion model \((x^{k},k)\) based on the U-net architecture  predicts a noise \((0,I)\) given a noise-corrupted trajectory \(x^{k}\) as an input [15; 16], and it is optimized through the following loss.

\[_{}_{x,k[1,K]}[||(x^{k},k)- ||_{2}^{2}]\] (3)

Here, \(k[1,K]\) is a denoising step, and \(x^{k}=}x+}\) is generated by adding a Gaussian noise \(\) to the original trajectory \(x\) with a variance schedule parameter \(^{k}\). At sampling, the diffusion model generates trajectory \(x\) from a random noisy input \(x^{K}(0,I)\) by sequentially denoising it,

\[x^{k-1}=}}(x^{k}-}{}}(x^{k},k))+^{k},\] (4)

where \(^{k}\) is a parameter for a variance schedule.

## 4 Our approach

### Overall framework

To address the zero-shot adaptation to various contexts, we develop the LDuS framework comprising two phases: (i) skill learning via the skill diffusion planner, and (ii) policy adaptation to unseen contexts via LLM-guided diffusion, as illustrated in Figure 2. (i) In the learning phase, we establish a hierarchical structure in which skills are learned, conditioned on a goal. This structure includes a skill encoder, a skill prior, and a skill diffusion planner that generates skill trajectories. These components are learned on the dataset that contains only goals, without any contextual information. Additionally, we employ a sequential in-painting technique when training the skill diffusion planner to enhance robustness in skill trajectory generation. (ii) In the adaptation phase, we guide the generation process of the skill diffusion planner upon a language-specified context, by harnessing the code generation capabilities of LLMs. The generated code serves as a loss function that guides the skill diffusion planner to generate skill trajectories at every denoising step. This facilitates alignment between the skill trajectories and the given context. Furthermore, LDuS employs an iterative refinement process, in which generated skill trajectories are repeatedly validated in a closed-loop manner to achieve robust alignment with the context.

### Skill learning via diffusion planner

To facilitate skill learning using a diffusion model, we adopt a variational autoencoder (VAE)  architecture with three components: a skill encoder \(q(z|s_{t:t+h},a_{t:t+h})\), a skill prior \(p_{l}(z|s_{t},g_{l})\), and a skill diffusion planner \((x^{k},k,z)\). Given an \(h\)-length skill trajectory \(x=\{s_{t},a_{t}\}_{t h}\), the skill encoder \(q\) predicts a skill embedding \(z\), then the skill diffusion planner reconstructs the \(h\)-length skill trajectory \(x\) based on \(z\). To optimize both the skill encoder and skill diffusion planner, we employ a conditional VAE objective that combines a diffusion reconstruction term in (3) and a prior regularization term such as

\[_{q,}_{x,z q,k[1,K]}[|| (x^{k},k,z)-||_{2}^{2}]+ D_{}(q(z|s_{t:t +h},a_{t:t+h}),p(z))\] (5)

Figure 2: Concept of LDuS with skill learning and adaptation phaseswhere \(p(z)\) is a unit Gaussian \((0,I)\), \(D_{}\) is the Kullback-Leibler (KL) divergence, and \(\) is a weight for regularization . To establish a versatile skill embedding space encapsulating common skills across multiple tasks, we use the skill encoder without goal conditions. In addition, to learn skills conditioned on the goal, the skill prior \(p_{l}\), which is conditioned on state \(s_{t}\) and goal \(g_{l}\), is tailored to align with the output of the skill encoder. To handle the goal provided in language, we also use a pre-trained language encoder \(_{l}\) such as CLIP  that produces language embeddings of \(g_{l}\) for the skill prior. Then, the skill prior is jointly trained with the skill encoder and skill diffusion planner by minimizing the distance with the skill encoder, i.e.,

\[_{p_{l}}D_{}(q(z|s_{t:t+h},a_{t:t+h}),p_{l}(z|s_{t},_{l}(g _{l}))).\] (6)

Using the skill prior, we predict an appropriate skill embedding at deployment based on the current state and specified goal.

**Sequential in-painting.** The in-painting technique is adopted in  to address goal-conditioned problems, in which a diffusion model is conditioned by replacing the last state of a generating trajectory \(x^{k}\) with the goal state. We adapt this in-painting to train the skill diffusion planner with a sequential replacement mechanism, where a sequence of \(m[1,h]\) states and actions of \(x^{k}\) is substituted with the corresponding original state-action pairs to learn the remaining portion of \(x^{k}\), as illustrated in the bottom left side of Figure 3. Then, for evaluation, unlike conventional methods that constrain the diffusion model only with the current state [22; 23], our approach constrains the skill diffusion planner using the previously encountered \(m\) states. This in-painting method enables the diffusion planner to generate more robust and contextually aligned skill trajectories, as described in Section 5.3.

### Policy adaptation via LLM-guided diffusion

In general, it is challenging for a model to directly acquire the zero-shot adaptation ability for various contexts, particularly when the training dataset has limited coverage on those contexts. To tackle this challenge, we harness the controlled generation capabilities of diffusion models  along with the code-generation capabilities of LLMs .

In LDuS, for zero-shot adaptation to various contexts, we employ three procedures: translation of a given context to its corresponding loss function, loss-guided trajectory generation, and closed-loop iterative refinement, as illustrated in the right side of Figure 3. Initially, the LLM is tasked with translating the context into a loss function. This loss function then guides the skill diffusion planner in its trajectory generation process. To ensure the accuracy and relevance of generated skill trajectories, the process is validated by the LLM in a closed-loop manner, querying the LLM to verify whether the generated trajectories meet the specifications of the given context.

Figure 3: LDuS framework: In (i), given the dataset annotated with the goals, skills are learned through the hierarchical structure employing a skill diffusion planner with sequential in-painting techniques. In (ii), the context specified in language is translated into a loss function, which is then used to guide the generation process of the skill diffusion planner. This process is further validated with closed-loop iterative refinement to better align skill trajectories with the given context.

**Translation of context to loss function.** For converting the context into a loss function in code, we employ chain-of-thought prompting  with a pre-defined list of queries \(=[u_{1},..,u_{n}]\). The queries are designed to capture the specifications of the agent and the desired format of loss functions. Then, the queries are sequentially prompted to the LLM \(_{}\) in conjunction with a goal \(g_{l}\) and a context \(c_{l}\), i.e.,

\[\{y_{j}|y_{j}=_{}(g_{l},c_{l},\{u_{i},y_{i}\}_{i<j})\}\] (7)

where \(\{u_{i},y_{i}\}_{i<j}\) represents a set of prompted queries \(u_{i}\) and their respective responses \(y_{i}\) from the LLM. The final response \(y_{n}\) is then used as the loss function to guide the skill diffusion planner.

**Loss-guided skill trajectory generation.** Similar to prior work [29; 4], we implement the loss-guided skill trajectory generation, where the guidance is computed as the gradient of the loss function with respect to \(x^{k}\) at each denoising step \(k\).

\[:=(x^{k},k,z)-^{k}} _{x^{k}}(^{0})\] (8)

Here, \(^{0}=^{k}}}(x^{k}+(1-^{k} )(x^{k},k,z))\) is an approximation of \(x^{0}\) given \(x^{k}\), and \(\) is a hyperparameter to modulate the strength of the guidance.

**Closed-loop iterative refinement.** In the closed-loop iterative refinement, we employ the LLM as a self-critic to evaluate both the loss functions and generated trajectories. Specifically, we prompt the LLM with unguided trajectory \(x^{0}\), guided trajectory \(^{0}\), goal \(g_{l}\), and context \(c_{l}\), i.e., \(_{}(x^{0},^{0},g_{l},c_{l})\). The LLM then checks for errors. If errors are detected in the loss function, it is regenerated. If there is a mismatch between the trajectory and the context, the frequency of the guidance application is increased. This ensures continuous improvement in the accuracy and relevance of trajectories generated by the skill diffusion planner. The process of zero-shot policy adaptation is summarized in Algorithm 1.

```
1:Inputs: skill prior \(p_{l}(z|s_{t},g_{l})\), skill diffusion planner \((x^{k},k,z)\), goal \(g_{l}\), context \(c_{l}\), LLM \(_{}\), guidance weight \(\)
2:Obtain loss function \((^{0})\) using \(_{}\) through (7)
3:for every environment step \(t\)do
4:\(z p_{l}(z|s_{t},g_{l})\)
5:while not validate do
6: Sample trajectory \(x^{0}\) using \((x^{k},k,z)\) without guidance through (4)
7: Sample guided trajectory \(^{0}\) using \((^{0})\) and \((x^{k},k,z)\) through (8)
8: Validate whether satisfy the context via LLM as \(_{}(x^{0},^{0},g_{l},c_{l})\)
9: Execute \(a_{m}\) in \(x^{0}\) to the environment ```

**Algorithm 1** Policy adaptation via LLM-guided diffusion

## 5 Experiments

### Experiment Settings

**Datasets.** We use the MetaWorld benchmark , specifically with \(10\) different robot manipulation goals. We also utilize long-horizon goals from the multi-stage MetaWorld, where each goal comprises a sequence of short-horizon manipulation sub-goals. For data collection, we emulate rule-based expert policies. For each goal, we collect \(60\) trajectories, varying the speed of the agent as well as the position and weight of the objects being manipulated.

**Contexts.** We use two context groups: (i) **language context** where the context is solely specified in language, (ii) **multi-modal context** where additional information is provided through image input to assist in resolving the given context. These contexts are used to direct the agent with instructions such as moving below or above a specific speed, adjusting its speed faster or slower along a specified axis, or exerting more or less force on heavy objects. The context details are in Appendix A.4.

**Evaluation metrics.** We use two metrics to assess the zero-shot performance of LDuS and the baselines. **Success Rate (SR)** quantifies the percentage of goals or sub-goals that are successfully completed. **Context Reward (CR)** evaluates the average reward achieved based on how effectively the models satisfy the given context.

**Baselines.** For comparison, we use several language-conditioned imitation and planning methods. 1) **LangDT** is an imitation learning method that utilizes a language-conditioned decision transformer, 2) **LISA** is a hierarchical skill imitation framework that learns discredited skill codes conditioned on language instructions, 3) **LCD** is a hierarchical planning framework that reconstructs state sequences using a diffusion model conditioned on a language input, 4) **Diffuser** is a task planning framework based on diffusion models.

For LLMs, we use GPT-3.5  which is capable of generating loss functions in the form of executable code. Moreover, for multi-modal contexts, we use GPT-4 . Since these baselines rarely account for zero-shot adaptation to contexts, we adopt the same diffusion guidance used in LDuS for those (i.e., Diffuser and LCD) employing diffusion models. In the cases where the diffusion model is guided by a hand-designed loss function, which is considered optimal, we specify such baselines with the additional label of **Guidance**.

### Main results

**Zero-shot performance.** Table 1 shows the performance of LDuS and the baselines (LangDT, Diffuser, LISA, LCD) across three different context input cases (without context, language context, multi-modal context) in MetaWorld and multi-stage MetaWorld. As in Table 1(a), LDuS consistently yields the best SR and CR in MetaWorld, outperforming the most competitive baseline Diffuser+Guidance by \(18.2\%\) higher in SR and \(78.7\%\) higher in CR for the cases of language contexts. For multi-stage MetaWorld, in Table 1(b), LDuS demonstrates superior performance with \(41.7\%\) higher in SR and \(75.5\%\) higher in CR at average, compared to Diffuser+Guidance.

In these experiments, LangDT and LISA exhibit the lowest performance, even for the cases without contexts. This is attributed to the multi-modality in the dataset, which tends to hinder the learning of policies built with multi-layer perceptrons or transformers. In contrast, the baselines employing diffusion models, such as Diffuser and LCD, show improved performance. However, none of these

Table 1: Zero-shot performance: The baselines and LDuS are trained on \(10\) different manipulation goals for MetaWorld and \(3\) different long-horizon goals for multi-stage MetaWorld. For each manipulation goal, we use \(2 5\) different contexts. The success rate (SR) and context rewards (CR) are measured in \(95\%\) confidence interval. Each is evaluated with 5 random seeds for language contexts and 3 random seeds for multi-modal contexts. The highest performance is highlighted in bold.

baselines achieve robust comparable performance to LDuS for the cases involving language or multi-modal contexts. This limitation arises because the baselines are primarily designed to handle goal descriptions, which is the sole form of language annotation in the dataset. Consequently, they lack the capability to accommodate various contexts that convey environmental conditions or user requirements. Diffuser and LCD, when used with guidance, exhibit slightly improved CR, as the hand-designed optimal loss function can provide context-aligned guidance for trajectory generation. However, in some cases, SR slightly decreases for both the baselines and LDuS when guidance is applied. This decrease occurs because the gradient-based loss-guidance could generate unexpected trajectories unless the tuning of the guidance weight \(\) was carefully managed. Overall, LDuS outperforms the baselines in CR by employing the iterative refinement that ensures context alignment, as well as in SR by employing the sequential in-painting that allows for robust trajectory generation. While the contexts used in Table 1 are mainly related to speed, we provide additional experiments on different types of contexts, such as energy constraints and spatial limitations, in Appendix C.2.

**Various context types.** In Table 2, we evaluate the performance across several context types such as precise, abstract, and temporal contexts, while all given contexts are specified in language. Specifically, precise contexts include detailed user requirements, such as a specific target speed, e.g., "the agent speed should move between 5m/s and 6m/s." In contrast, abstract contexts lack specific details. For instance, if the user wants the agent to increase its speed, the abstract context could be phrased as "I am very busy; the agent needs to hurry." Temporal contexts are dynamic and vary over time, which are particularly relevant in long-horizon goals. As shown, the results indicate that LDuS significantly enhances CR, with an increase of \(43.7\%\) at average compared to Diffuser+Guidance.

**Comparing with waypoint generation.** In addition to the learning-based baselines compared previously, we compare our approach with Kinematic-LLM  by which waypoints for pre-defined skill primitives are generated through an LLM with in-context samples. To implement Kinematic-LLM, we define basic skill primitives such as move, push, and pull, and use the same samples for prompting, which are used for LDuS. As shown in Table 3, Kinematic-LLM shows comparable performance in SR for MetaWorld, but lags in multi-stage MetaWorld. This is attributed to the increased complexity of planning with LLMs for long-horizon goals. Regarding CR, Kinematic-LLM consistently demonstrates lower performance compared to LDuS. This result stems from a lack of versatile samples and skill primitives that are necessary to effectively adapt to various contexts.

**Inference Time.** In Table 4, we present the average inference time (in milliseconds) required per timestep for LDuS and the baselines. The measurements are conducted on a system equipped with an Intel(R) Core(TM) i9-10980XE CPU and an NVIDIA RTX A6000 GPU, and we use GPT-3.5 for the LLM. As LDuS requires both diffusion sampling time and LLM inference, we measure these component separately, denoted as "Diffusion" and "LLM" in the parenthesis. Diffuser and LCD exhibit the shortest inference time, as these baselines do not require loss guided sampling or LLM inference. When considering only the diffusion sampling time excluding LLM inference, LDuS demonstrates an inference time comparable to the baselines that use the loss guidance. However, the full inference time of LDuS is longer due to its LLM-based code generation and iterative refinement

    &  &  \\   & CR & SR \((\%)\) & CR & SR \((\%)\) \\  Kinematic-LLM & \(48.83 3.80\) & \(95.24 1.53\%\) & \(51.19 0.93\) & \(61.82 3.05\%\) \\ LDuS (ours) & \(90.38 3.14\) & \(95.36 1.57\%\) & \(75.99 3.81\) & \(84.03 3.67\%\) \\   

Table 3: Comparison with waypoint generation method

    &  &  &  \\   & CR & SR \((\%)\) & CR & SR \((\%)\) & CR & SR \((\%)\) \\   LCD + Guidance & \(30.35 6.55\) & \(47.96 8.68\%\) & \(50.18 4.17\) & \(48.71 9.04\%\) & \(25.04 9.02\) & \(33.80 3.18\%\) \\ Diffuser + Guidance & \(65.05 4.79\) & \(70.19 4.87\%\) & \(57.33 2.05\) & \(50.58 4.68\%\) & \(49.17 6.59\) & \(38.89 0.68\%\) \\  LDuS (ours) & \(89.77 3.99\) & \(97.72 1.08\%\) & \(75.52 3.86\) & \(86.98 3.00\%\) & \(79.40 3.32\) & \(84.26 3.79\%\) \\   

Table 2: Performance w.r.t various context types

[MISSING_PAGE_FAIL:9]

Conclusion and limitations

In this work, we presented the LDuS framework for zero-shot skill-based policy adaptation to contexts specified in language. The framework employs a hierarchical structure for skill learning, in which the skill encoder learns task-agnostic skill abstractions and the skill diffusion planner generates various skill trajectories. The skill diffusion planner is enhanced with sequential in-painting, thus enabling context-aligned trajectory generation for the skills. At test-time, given a specific context describing environmental conditions or user requirements, LDuS directly influences the generation process of the skill diffusion planner, allowing for skill-based policies to adapt to the context. This zero-shot adaptation is achieved by a combination of LLM-based loss function generation and iterative refinement, along with the controllable structure of the skill diffusion planner. LDuS stands apart from other language-conditioned approaches, which are limited to certain variations of the instructions present in the dataset and generalize insufficiently to a range of unseen contexts.

**Limitations.** LDuS has several limitations which direct us to future work. One limitation is related to the inference time, as LDuS relies on iterative LLM inferences for refinement. This issue could be mitigated by distilling only essential knowledge, such as code generation and verification capabilities of an LLM, into a smaller language model. Furthermore, since LDuS relies on the LLM for multiple components, including code generation and iterative refinement, another limitation is its robustness, which can be affected by the variability in the LLM's performance and the design of the prompts.