# Statistical Efficiency of Distributional Temporal Difference Learning

Yang Peng

School of Mathematical Sciences, Peking University; email: pengyang@pku.edu.cn.

&Liangyu Zhang

School of Statistics and Management, Shanghai University of Finance and Economics; email: zhanglliangyu@sufe.edu.cn.

Zhihua Zhang

School of Mathematical Sciences, Peking University; email: zhzhang@math.pku.edu.cn.

###### Abstract

Distributional reinforcement learning (DRL) has achieved empirical success in various domains. One core task in the field of DRL is distributional policy evaluation, which involves estimating the return distribution \(^{}\) for a given policy \(\). The distributional temporal difference learning has been accordingly proposed, which is an extension of the temporal difference learning (TD) in the classic RL area. In the tabular case, Rowland et al. (2018) and Rowland et al. (2024) proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference learning (CTD) and quantile temporal difference learning (DTD), respectively. In this paper, we go a step further and analyze the finite-sample performance of distributional TD. To facilitate theoretical analysis, we propose non-parametric distributional temporal difference learning (NTD). For a \(\)-discounted infinite-horizon tabular Markov decision process, we show that for NTD we need \(((1-)^{2p+1}})\) iterations to achieve an \(\)-optimal estimator with high probability, when the estimation error is measured by the \(p\)-Wasserstein distance. This sample complexity bound is minimax optimal up to logarithmic factors in the case of the \(1\)-Wasserstein distance. To achieve this, we establish a novel Freedman's inequality in Hilbert spaces, which would be of independent interest. In addition, we revisit CTD, showing that the same non-asymptotic convergence bounds hold for CTD in the case of the \(p\)-Wasserstein distance for \(p 1\).

## 1 Introduction

In high-stake applications of reinforcement learning (RL), such as healthcare (Lavori and Dawson, 2004; Bock et al., 2022) and finance(Ghysels et al., 2005), only considering the mean of returns is insufficient. It is necessary to take risk and uncertainties into consideration. Distributional reinforcement learning (DRL) Morimura et al. (2010); Bellemare et al. (2017, 2023) addresses such issues by modeling the complete distribution of returns instead of their expectations.

In the field of DRL, one of the most fundamental tasks is to estimate the return distribution \(^{}\) for a given policy \(\), which is referred to as distributional policy evaluation. Distributional temporal difference learning (TD) is probably the most widely-used approach for solving the distributional policy evaluation problem. A key aspect of implementing a distributional TD algorithm is how to represent the return distribution, an infinite-dimensional object, via a computationally feasible finite-dimensional parametrization. This has led to the development of two special instances of distributional TD: categorical temporal difference learning (CTD) (Bellemare et al., 2017) and quantile temporal difference learning (QTD) (Dabney et al., 2018). These algorithms provide computationally tractable parametrizations and updating schemes of the return distribution.

Previous theoretical works have primarily focused on the asymptotic behaviors of distributional TD. In particular, Rowland et al. (2018) and Rowland et al. (2024) showed the asymptotic convergences of CTD and QTD in the tabular case, respectively. A natural question arises: _can we depict the statistical efficiency of distributional TD by non-asymptotic results similar to the classic TD algorithm (Li et al., 2024)_?

### Contributions

In this paper, we manage to answer the above question affirmatively in the synchronous setting (Kakade, 2003; Kearns et al., 2002). Firstly, we introduce non-parametric distributional temporal difference learning (NTD) in Section 3, which is not practical but aids theoretical understanding. We show that \(((1-)^{2p+1}})\)4 iterations are sufficient to yield an estimator \(^{}\), such that the \(p\)-Wasserstein metric between \(^{}\) and \(^{}\) is less than \(\) with high probability (Theorem 4.1). This bound is minimax optimal (Theorem B.1) in the \(1\)-Wasserstein metric case, if we neglect all logarithmic terms. Next, we revisit the more practical CTD, and show that, in terms of the \(p\)-Wasserstein metric, CTD and NTD have the same non-asymptotic convergence bounds (Theorem 4.2). It is worth pointing out that to attain such tight bounds in Theorem 4.1, we establish a Freedman's inequality in Hilbert spaces (Theorem A.2). We would believe it is of independent interest beyond the current work.

### Related Work

Non-asymptotic results of DRL.Recently, there has been an emergence of work focusing on finite-sample/iteration results of the distributional policy evaluations.

Wu et al. (2023) studied the offline distributional policy evaluation problem. They solved the problem via fitted likelihood estimation (FLE) inspired by the classic offline policy evaluation algorithm fitted Q evaluation (FQE), and provided a generalization bound in the \(p\)-Wasserstein metric case.

Zhang et al. (2023) proposed to solve distributional policy evaluation by the model-based approach and derived corresponding sample complexity bounds, namely \(((1-)^{2p+2}})\) in the \(p\)-Wasserstein metric case, and \(((1-)^{4}})\) in both the Kolmogorov-Smirnov metric and total variation metric under different conditions. Rowland et al. (2024) proposed direct categorical fixed-point computation (DCFP), a model-based version of CTD, in which they constructed the estimator by solving a linear system directly instead of performing an iterative algorithm. They showed that the sample complexity of DCFP is \(((1-)^{3}})\) in the \(1\)-Wasserstein metric case by introducing the novel stochastic categorical CDF Bellman operator and equation. Their result matches the minimax lower bound (up to logarithmic factors) \(((1-)^{3}})\) proposed in (Zhang et al., 2023), which implies that learning the full return distribution can be as sample-efficient as learning just its expectation. It's worth noting that the algorithms analyzed in both (Zhang et al., 2023) and (Rowland et al., 2024) are model-based, hence they are less similar to practical algorithms. While distributional TD analyzed in this paper, as a model-free method, is more practical, and also involves a more complicated theoretical analysis.

Bock and Heitzinger (2022) also considered model-free method. They proposed speedy categorical policy evaluation (SCPE), which can be regarded as CTD with an additional acceleration term. They showed that the sample complexity of SCPE is \(((1-)^{4}})\) in the \(1\)-Wasserstein metric case. Compared to (Bock and Heitzinger, 2022), our work shows that even if we do not introduce any acceleration techniques to the original CTD algorithm, it is still possible to attain the near-minimax optimal sample complexity bounds. Thus, we give _sharper_ bounds based on a _simpler_ algorithm.

Table 1 gives more detailed comparisons of sample complexity with the previous work in the \(1\)-Wasserstein metric. Note that solving distributional policy evaluation can also address the traditional

policy evaluation task by taking expectation of the return distribution estimator. And the supreme \(1\)-Wasserstein metric error of the return distribution estimator is not smaller than the \(_{}\) error of the induced value function estimator (see the proof of Theorem B.1 in Appendix B), we have also listed the sample complexity of the policy evaluation task in Table 1 for comparison.

Freedman's inequality.Freedman's inequality was originally proposed in [Freedman, 1975]. It can be viewed as a Bernstein's inequality for martingales, which is crucial for analyzing stochastic approximation algorithms. Tropp  generalized Freedman's inequality to matrix martingales. And Talebi et al.  established Freedman inequalities for martingales in the setting of noncommutative probability spaces. The closest literature to ours is [Tarres and Yao, 2014] and [Martinez-Taboada and Ramdas, 2024], where they provided a special case of our Theorem A.2 with \(H=0\) independently. When \(H=0\), we can only utilize the deterministic upper bound on the quadratic variation rather than the high-probability upper bound. In certain problems, such as the distributional TD learning we aim to investigate, it is impossible to achieve the optimal upper bound using the \(H=0\) version. [Martinez-Taboada and Ramdas, 2024] also proposed an empirical Freedman's inequality in \((2,D)\)-smooth Banach space, which can be used to construct confidence sets or perform hypothesis testing. To the best of our knowledge, we are the first to present this version (Theorem A.2) of Freedman's inequality in Hilbert spaces5.

The remainder of this paper is organized as follows. In Section 2, we introduce some background of DRL and state Freedman's inequality in Hilbert spaces. In Section 3, we revisit distributional TD and propose NTD for further theoretical analysis. In Section 4, we analyze the non-asymptotic convergence bounds of NTD and CTD. Section 5 presents proof outlines of our theoretical results, and Section 6 concludes our work. We put the detailed results with Freedman's inequality in Hilbert spaces in Appendix A, and the minimax lower bound of the distributional policy evaluation task in Appendix B.

## 2 Background

An infinite-horizon tabular Markov decision process (MDP) is defined by a 5-tuple \(M=,,_{R},P,\), where \(\) represents a finite state space, \(\) a finite action space, \(_{R}\) the distribution of rewards, \(P\) the transition dynamics, _i.e._, \(_{R}(|s,a)()\), \(P(|s,a)()\) for any state action pair \((s,a)\), and \((0,1)\) a discount factor. Here we use \(()\) to represent the set of all probability distributions over some set. Given a policy \(()\) and an initial state \(s_{0}=s\), a random trajectory \(\{(s_{t},a_{t},t_{t})_{t=0}^{}\}\) can be sampled from \(M\): \(a_{t} s_{t}( s_{t})\), \(r_{t}(s_{t},a_{t})_{R}( s_{t},a_{t})\), \(s_{t+1}(s_{t},a_{t}) P( s_{t},a_{t})\) for any \(t\). Given a trajectory, we define the return by \(G^{}(s):=_{t=0}^{}^{t}r_{t}[0,].\) We denote return distribution \(^{}(s)\) as the probability distribution of \(G^{}(s)\), and \(^{}:=(^{}(s))_{s}\). The expected return \(V^{}(s)=G^{}(s)\) is the value function in the traditional RL setting.

   & Sample Complexity & Algorithms & Task \\ 
[Gheshlaghi Azar et al., 2013] & \(((1-)^{3}})\) & Model-based & PE \\ 
[Li et al., 2024] & \(((1-)^{3}})\) & TD (Model-free) & PE \\ 
[Rowland et al., 2018] & Asymptotic & CTD (Model-free) & DPE \\ 
[Rowland et al., 2024a] & Asymptotic & QTD (Model-free) & DPE \\ 
[Rowland et al., 2024b] & \(((1-)^{3}})\) & DCFP (Model-based) & DPE \\ 
[BÃ¶ck and Heitzinger, 2022] & \(((1-)^{4}})\) & SCPE (Model-free) & DPE \\  Our Work & \(((1-)^{3}})\) & CTD (Model-free) & DPE \\  

Table 1: Sample complexity of algorithms for solving policy evaluation (PE) in the \(_{}\) norm, and distributional policy evaluation (DPE) in the supreme \(1\)-Wasserstein metric.

### Distributional Bellman Equation and Operator

Recall that the classic policy evaluation aims at computing the value functions \(V^{}\). It is known that \(V^{}=(V^{}(s))_{s}\) satisfy the Bellman equation. That is, for any \(s\),

\[V^{}(s)=[T^{}(V^{})](s)=_{a(|s),r _{R}(|s,a),s^{} P(|s,a)}[r+ V^{}( s^{})].\] (1)

The operator \(T^{}^{}^{}\) is called the Bellman operator, and \(V^{}\) is a fixed point of \(T^{}\).

The task of distribution policy evaluation is finding \(^{}\) given some fixed policy \(\). \(^{}\) satisfies a distributional version of the Bellman equation (1). That is, for any \(s\),

\[^{}(s)=(^{}^{})(s)=_{a (|s),r_{R}(|s,a),s^{} P(|s,a)} [(b_{r,})_{\#}^{}(s^{})],\] (2)

where \(b_{r,}\) is an affine function defined by \(b_{r,}(x)=r+ x\). And \(f_{\#}\) is the push forward measure of \(\) through any function \(f\), so that \(f_{\#}(A)=(f^{-1}(A))\) for any Borel set \(A\), where \(f^{-1}(A):=\{x f(x) A\}\). The operator \(^{}([0,])^{ }([0,])^{ }\) is known as the distributional Bellman operator, and \(^{}\) is a fixed point of \(^{}\). For notational simplicity, we denote \(([0,])\) as \(\) from now on.

### \(^{}\) as Contraction in \(\)

A key property of the Bellman operator \(T^{}\) is that it is a \(\)-contraction w.r.t. the supreme norm (_i.e._\(_{}\) norm). However, before we can properly discuss the contraction properties of \(^{}\), we need to specify a metric \(d\) on \(\). And for any metric \(d\) on \(\), we denote \(\) as the corresponding supreme metric on \(^{}\), _i.e._, \((,^{}):=_{s}d((s), ^{}(s))\) for any \(,^{}^{}\).

Suppose \(\) and \(\) are two probability distributions on \(\) with finite \(p\)-moments for \(p[1,]\). The \(p\)-Wasserstein metric between \(\) and \(\) is defined as \(W_{p}(,):=(_{(,)}_{^{2}}|x-y| ^{p}\,(dx,dy))^{1/p}\). Each element \((,)\) is a coupling of \(\) and \(\), _i.e._, a joint distribution on \(^{2}\) with prescribed marginals \(\) and \(\) on each "axis." When \(p=1\) we have \(W_{1}(,)=_{}|F_{}(x)-F_{}(x)|dx\), where \(F_{}\) and \(F_{}\) are the cumulative distribution function of \(\) and \(\), respectively. It can be shown that \(^{}\) is a \(\)-contraction w.r.t. the supreme \(p\)-Wasserstein metric \(_{p}\).

**Proposition 2.1**.: _[_Bellemare et al._,_ 2023_, Propositions 4.15]_ _The distributional Bellman operator is a \(\)-contraction on \(^{}\) w.r.t. the supreme \(p\)-Wasserstein metric for \(p[1,]\). That is, for any \(,^{}^{}\), we have \(_{p}(^{},^{}^{}) _{p}(,^{})\)._

The \(_{p}\) metric between \(\) and \(\) is defined as \(_{p}(,)=(_{}|F_{}(x)-F_{}(x)|^{p}\,dx)^ {}\) for \(p[1,)\), and \(^{}\) is \(^{}\)-contraction w.r.t. the supreme \(_{p}\) metric \(_{p}\).

**Proposition 2.2**.: _[_Bellemare et al._,_ 2023_, Propositions 4.20]_ _The distributional Bellman operator is a \(^{}\)-contraction on \(^{}\) w.r.t. the supreme \(_{p}\) metric for \(p[1,)\). That is, for any \(,^{}^{}\), we have \(_{p}(^{},^{}^{}) ^{}_{p}(,^{})\)._

Note that the \(_{1}\) metric coincides with the \(1\)-Wasserstein metric. And the \(_{2}\) metric is also called the Cramer metric, which plays an important role in subsequent analysis because the zero-mass signed measure space equipped with this metric \((,\|\|_{_{2}})\) (defined in Section 5.1) is a Hilbert space6. Thereby, we can apply Freedman's inequality in Hilbert spaces.

### Freedman's Inequality in Hilbert Spaces

Just as Freedman's inequality is essential for the theory of TD (Theorem 1 in ), a Hilbert space version of Freedman's inequality is indispensable for deriving the minimax non-asymptotic convergence bound for distributional TD. At the moment, we state a Hilbert space version of the original Freedman's inequality (Theorem 1.6 in ), and more detailed results can be found in Appendix A.

Let \(\) be a Hilbert space, \(\{X_{i}\}_{i=1}^{n}\) be an \(\)-valued martingale difference sequence adapted to the filtration \(\{_{i}\}_{i=1}^{n}\), \(Y_{i}:=_{j=1}^{i}X_{j}\) be the corresponding martingale, and \(W_{i}:=_{j=1}^{i}_{j}^{2}\) be the corresponding quadratic variation process. Here \(_{j}^{2}:=_{j-1}\|X_{j}\|^{2}\), and \(_{i}[]:=[|_{i}]\) denotes the conditional expectation.

**Theorem 2.1** (Freedman's inequality in Hilbert spaces).: _Suppose \(_{i[n]}\|X_{i}\| b\) for some constant \(b>0\). Then, for any \(\) and \(>0\), the following inequality holds_

\[( k[n],\|Y_{k}\|W_{k}^{2}) 2 \{-/2}{^{2}+b/3}\}.\]

## 3 Distributional Temporal Difference Learning

If the MDP \(M=,,_{R},P,\) is known, and because \(V^{}\) is the fixed point of the contraction \(T^{}\), \(V^{}\) can be evaluated via the famous dynamic programming (DP) algorithm. To be concrete, for any initialization \(V^{(0)}\,^{}\), if we define the iteration sequence \(V^{(k+1)}=T^{}(V^{(k)})\) for \(k\), we have \(_{k}\|V^{(k)}-V^{}\|_{}=0\) by the contraction mapping theorem (Proposition 4.7 in ).

Similarly, the distributional dynamic programming algorithm defines the iteration sequence as \(^{(k+1)}=^{}^{(k)}\) for any initialization \(^{(0)}\). In the same way, we have \(_{k}_{p}(^{(k)},^{})=0\) for \(p[1,]\) and \(_{k}_{p}(^{(k)},^{})=0\) for \(p[1,)\).

In most application scenarios, the transition dynamic \(P\) and reward distribution \(_{R}\) are unknown, and instead we can only get samples of \(P\) and \(_{R}\) in a streaming manner. In this paper, we assume a generative model  is accessible, which generates independent samples for all states in each iteration, _i.e._, in the \(t\)-th iteration, we collect sample \(a_{t}(s)(|s),s_{t}(s) P(|s,a_{t}(s)),r_{t}(s)_{R}(|s,a_{t}(s))\) for each \(s\). Similar to TD  in classic RL, distributional TD also employs the stochastic approximation (SA)  technique to address the aforementioned problem and can be viewed as an approximate version of distributional DP.

Non-parametric Distributional TDWe first introduce non-parametric distributional temporal difference learning (NTD), which is helpful in the theoretical understanding of distributional TD. In the setting of NTD, we assume the return distributions can be precisely updated without any parametrization. For any initialization \(_{0}^{}^{}\), the updating scheme is given by

\[_{t}^{}=(1-_{t})_{t-1}^{}+_{t}_{t}^{} _{t-1}^{}\]

for any \(t 1\). Here \(_{t}\) is the step size. The empirical Bellman operator at the \(t\)-th iteration \(_{t}^{}\) is defined as

\[(_{t}^{})(s)=(b_{r_{t}(s),})_{\#}((s_{t +1})),\]

which is an unbiased estimator of \((^{})(s)\). It is evident that NTD is a SA modification of distributional DP. Consequently, we can analyze NTD using the techniques from the SA area.

Categorical Distributional TDNow, we revisit the more practical CTD. In this case, the updates in CTD is computationally tractable, due to the following categorical parametrization of probability distributions:

\[_{K}:=\{_{k=0}^{K}p_{k}_{x_{k}} p_{0},,p _{K} 0\,,_{k=0}^{K}p_{k}=1\},\]

where \(K\), and \(0 x_{0}<<x_{K}\) are fixed points of the support. For simplicity, we assume \(\{x_{k}\}_{k=0}^{K}\) are equally-spaced, _i.e._, \(x_{k}=\). We denote the gap between two points by \(_{K}=\). When updating the return distributions, we need to evaluate the \(_{2}\)-projection of \(_{K}\), \(_{K}_{K}\), \(_{K}:=*{argmin}_{_{K}}_{2}(, )\). It can be shown (Proposition 5.14 in ) that the projection is uniquely given by

\[_{K}=_{k=0}^{K}p_{k}()_{x_{k}}, p_{k}()= _{X}[(1-|}{_{K}}| )_{+}],\]\((x)_{+}:=\{x,0\}\) for any \(x\). It is known that \(_{K}\) is non-expansive w.r.t. the Cramer metric (Lemma 5.23 in (Bellemare et al., 2023)), _i.e._, \(_{2}(_{K},_{K})_{2}(,)\) for any \(,\). For any \(^{}\), \(s\), we slightly abuse the notation and define \((_{K})(s):=_{K}(s)\). \(_{K}\) is still non-expansive w.r.t. \(_{2}\). Hence \(^{,K}:=_{K}^{}\) is a \(\)-contraction w.r.t. \(_{2}\), we denote its unique fixed point as \(^{,K}^{}_{K}\). The approximation error induced by categorical parametrization is given by (Proposition 3 in Rowland et al. (2018))

\[_{2}(^{},^{,K})(1-)}, _{1}(^{},^{,K})}_{2 }(^{},^{,K})(1-)^{3/2}}.\] (3)

Now, we are ready to give the updating scheme of CTD, given any initialization \(^{}_{0}^{}_{K}\),

\[^{}_{t}=(1-_{t})^{}_{t-1}+_{t}_{K}^{ }_{t}^{}_{t-1}\]

for any \(t 1\). We can find that the only difference between CTD and NTD lies in the additional application of the projection operator \(_{K}\) at each iteration in CTD.

## 4 Statistical Analysis

In this section, we state our main results. For both NTD and CTD, we give the non-asymptotic convergence rates of \(_{p}(^{}_{T},^{})\) and \(_{2}(^{}_{T},^{})\), respectively.

### Non-asymptotic Analysis of NTD

We first provide a non-asymptotic convergence rate of \(_{1}(^{}_{T},^{})\) for NTD, which is minimax optimal (Theorem B.1) up to logarithmic factors.

**Theorem 4.1** (Sample complexity of NTD in the \(1\)-Wasserstein metric).: _Given any \((0,1)\) and \((0,1)\), let the initialization be \(^{}_{0}^{}\), the total update number \(T\) satisfy_

\[T^{3}T}{^{2}(1-)^{3}}|T}{}\]

_for some large universal constant \(C_{1}>1\), i.e., \(T=((1-)^{3}})\), and the step size \(_{t}\) satisfy_

\[(1-)t}{ t}}_{t}(1-)t}{ t}}\]

_for some small universal constants \(c_{2}>c_{3}>0\). Then, with probability at least \(1-\), the last iterate estimator satisfies \(_{1}(^{}_{T},^{})\)._

Because \(_{1}(^{}_{T},^{})\) always holds, we can translate the high probability bound to a mean error bound, that is,

\[[_{1}(^{}_{T},^{})] (1-)+ 2\]

if we take \((1-)\). In the subsequent discussion, we will not state the mean error bound conclusions for the sake of brevity.

The key idea of our proof is to first expand the error term \(_{1}(^{}_{T},^{})\) over the time steps. Then it can be decomposed into an initial error term and a martingale term. The initial error term becomes smaller as the iteration goes due to the contraction properties of \(^{}\). To control the martingale term, we first use the basic inequality (Lemma E.1) \(W_{1}(,)}_{2}(,)\), which allows us to analyze this error term in the Hilbert space \((,\|\|_{_{2}})\) defined in Section 5.1. Consequently, we can bound it using Freedman's inequality in the Hilbert space (Theorem A.2). A more detailed outline of proof can be found in Section 5.2.

Combining Theorem 4.1 with the basic inequality \(_{p}(,^{})}}_{1}^{}(,^{})\) for any \(,^{}^{}\) (Lemma E.1), we can derive that \(T=((1-)^{2p+1}})\) iterations are sufficient to ensure \(_{p}(_{T}^{},^{})\). As pointed out in the example after Corollary 3.1 in (Zhang et al., 2023), when \(p>1\), the slow rate in terms of \(\) is inevitable without additional regularity conditions.

Although the \(1\)-Wasserstein metric cannot bound the Cramer metric properly, by making slight modifications to the proof we have the following non-asymptotic convergence rate of \(_{2}(_{T}^{},^{})\). See Appendix C.5 for our proof.

**Corollary 4.1** (Sample complexity of NTD in the Cramer metric).: _Given any \((0,1)\) and \((0,1)\), let the initial value \(_{0}^{}^{}\), the total update number \(T\) satisfy_

\[T^{3}T}{^{2}(1-)^{5/2}}|\,T}{}\]

_for some large universal constant \(C_{1}>1\), i.e., \(T=((1-)^{5/2}})\), and the step size \(_{t}\) satisfy_

\[(1-)t}{ t}}_{t} {1+(1-)t}{ t}}\]

_for some small universal constants \(c_{2}>c_{3}>0\). Then, with probability at least \(1-\), the last iterate estimator satisfies \(_{2}(_{T}^{},^{})\)._

### Non-asymptotic Analysis of CTD

We first state a parallel result to Theorem 4.1.

**Theorem 4.2** (Sample complexity of CTD in the \(1\)-Wasserstein metric).: _Given any \((0,1)\) and \((0,1)\), suppose \(K>\), the initial value \(_{0}^{}^{}_{K}\), the total update number \(T\) satisfies_

\[T^{3}T}{^{2}(1-)^{3}}|\,T}{}\]

_for some large universal constant \(C_{1}>1\), i.e., \(T=((1-)^{3}})\), and the step size \(_{t}\) satisfies_

\[(1-)t}{ t}}_{t} {1+(1-)t}{ t}}\]

_for some small universal constants \(c_{2}>c_{3}>0\). Then, with probability at least \(1-\), the last iterate estimator satisfies \(_{1}(_{T}^{,T},^{,K})\). Furthermore, according to the upper bound (3) of the approximation error \(_{1}(^{,K},^{})\), if we take \(K>(1-)^{3}}\), we have \(_{1}(_{T}^{},^{})\)._

Note that the order (modulo logarithmic factors) of sample complexity of CTD is better than the previous results of SCPE (Bock and Heitzinger, 2022), and we do not need the additional term introduced in the updating scheme of SCPE.

The proof of this theorem is almost the same as that of Theorem 4.1, we outline the proof in Section 5.2. The \(_{1}\) metric result can be translated into sample complexity bound \(((1-)^{2p+1}})\) in the \(_{p}\) metric. We comment that this theoretical result matches the sample complexity bound in the model-based setting (Rowland et al., 2024).

As in the NTD setting, we have the following non-asymptotic convergence rate of \(_{2}(_{T}^{},^{})\) as a corollary of Theorem 4.2. See Appendix C.5 for the proof.

**Corollary 4.2** (Sample complexity of CTD in the Cramer metric).: _For any given \((0,1)\) and \((0,1)\), suppose \(K>\), the initialization is \(_{0}^{}^{}_{K}\), the total update number \(T\) satisfies_

\[T^{3}T}{^{2}(1-)^{5/2}}|\,T}{}\]

_for some large universal constant \(C_{1}>1\), i.e., \(T=((1-)^{5/2}})\), and the step size \(_{t}\) satisfies_

\[(1-)t}{ t}}_{t}(1-)t}{ t}}\]

_for some small universal constants \(c_{2}>c_{3}>0\). Then, with probability at least \(1-\), the last iterate estimator satisfies \(_{2}(_{T}^{},^{,K})\). Furthermore, according to the upper bound (3) of the approximation error \(_{2}(^{,K},^{})\), if we take \(K>(1-)^{2}}\), we have \(_{2}(_{T}^{},^{})\)._Proof Outlines

In this section, we will outline the proofs of our main theoretical results (Theorem 4.1, Corollary 4.1, Theorem 4.2, and Corollary 4.2). Before diving into the details of the proofs, we first define some notation.

### Zero-mass Signed Measure Space

To analyze the distance between the estimator and the ground-truth \(^{}\), we will work with the zero-mass signed measure space \(\) defined as follows

\[:=\{||( )<,()=0,()[0,]\},\]

where \(||\) is the total variation measure of \(\), and \(()\) is the support of \(\). See [Bogachev, 2007] for more details about signed measures.

For any \(\), we define its cumulative function as \(F_{}(x):=[0,x)\). We can check that \(F_{}\) is linear w.r.t. \(\), that is, \(F_{+}= F_{}+ F_{}\) for any \(,\), \(,\).

To analyze the Cramer metric case, we define the following Cramer inner product on \(\):

\[,_{_{2}}:=_{0}^{}F_{ }(x)F_{}(x)dx.\]

It is easy to verify that \(,_{_{2}}\) is indeed an inner product on \(\). The corresponding norm, called the Cramer norm, is given by \(\|\|_{_{2}}=}}=^{}(F_{}(x))^{2}dx}\). We have \(_{1}-_{2}\) and \(\|_{1}-_{2}\|_{_{2}}=_{2}(_{1},_{2})\) for any \(_{1},_{2}\).

The \(W_{1}\) norm on \(\) is defined as \(\|\|_{W_{1}}:=_{0}^{}|F_{}(x) |dx\). We have \(\|_{1}-_{2}\|_{W_{1}}=W_{1}(_{1},_{2})\) for any \(_{1},_{2}\).

We can extend the distributional Bellman operator \(^{}\) and the Cramer projection operator \(_{K}\) naturally to \(^{}\). Here, the product space \(^{}\) is also a Banach space, and we use the supreme norm: \(\|\|_{_{2}}:=_{s}\|(s) \|_{_{2}}\), and \(\|\|_{_{1}}:=_{s}\|(s) \|_{W_{1}}\) for any \(^{}\). We denote by \(\) the identity operator in \(^{}\).

When the norm \(\|\|\) is applied to \(A()\), where \(\) is any Banach space, and \(()\) is the space of all bounded linear operators in \(\), we refer \(\|A\|\) to the operator norm of \(A\), which is defined as \(\|A\|:=_{ X,\|\|=1}\|A\|\). With this notation, \(()=\{A A,and\|A\|<\}\).

**Proposition 5.1**.: \(^{}\) _and \(_{K}\) are linear operators in \(^{}\). Furthermore, \(\|^{}\|_{_{2}}\), \(\|^{}\|_{_{1}}\), \(\|_{K}\|_{_{2}}=1\), and \(\|_{K}\|_{_{1}} 1\)._

The proof of the last inequality can be found in the proof of Lemma C.4, while the remaining results are trivial. We omit the proofs for brevity.

Moreover, we have the following matrix (of operators) representations of \(^{}\) and \(_{K}\,^{}()^{ }\) for any \(^{}\),

\[(^{})(s)=_{a,s^{} }(a s)P(s^{} s,a)_{0}^{1}(b_{r,} )_{\#}(s^{})_{R}(dr s,a)=_{s^{} }^{}(s,s^{})(s^{}),\]

where \(^{}(s,s^{})()\) for any \(\),

\[^{}(s,s^{})=_{a}(a s)P(s^{ } s,a)_{0}^{1}(b_{r,})_{\#}_{R}(dr  s,a).\]

It can be verified that \(\|(s,s^{})\|_{_{2}}_{a }(a s)P(s^{} s,a)=:P^{}(s^{} |s)\). Similarly, \(\|(s,s^{})\|_{W_{1}} P^{}(s^{} |s)\), and \(_{K}=_{K}_{}_{s }()^{}\). With these representations, \(_{K}^{}()^{ }\) can be interpreted as matrix multiplication, where the scalar multiplication is replaced by the composition of operators. It can be verified that \((_{K}^{})(s,s^{})=_{K}^{}(s,s ^{})\), and \(\|(_{K}^{})(s,s^{})\|_{_{2}} P^{}(s^{}|s)\).

**Remark 1:** In Lemma E.2, we show that both \((,\|\|_{_{2}})\) and \((,\|\|_{W_{1}})\) are separable. And in Lemma E.3, we show that \((,\|\|_{W_{1}})\) is not complete. To resolve this problem, we will use their completions to replace them without loss of generality, because the completeness property does not affect the separability. For simplicity, we still use \(\) to denote the completion space. According to the BLT theorem [Theorem 5.19 Hunter and Nachtergaele, 2001], any bounded linear operator can be extended to the completion space, and still preserves its operator norm.

### Analysis of Theorems 4.1 and 4.2

For simplicity, we abbreviate both \(\|\|_{_{2}}\) and \(\|\|_{_{2}}\) as \(\|\|\) in this part. For all \(t[T]:=\{1,2,,T\}\), we denote \(_{t}:=_{t}^{}\), \(:=^{}\), \(:=^{}\) for NTD; \(_{t}:=_{K}_{t}^{}\), \(:=_{K}^{}\), \(:=^{,K}\) for CTD; and \(_{t}:=_{t}^{}\), \(_{t}:=_{b}-^{}\) for both NTD and CTD. According to Lemma E.4, \(_{t}^{}\) for NTD and \(_{t}^{}_{K}\) for CTD. Our goal is to bound the \(W_{1}\) norm of the error term \(\|_{T}\|_{W_{1}}\). This can be achieved by bounding \(\|_{T}\|\), as \(\|_{T}\|_{W_{1}}}\|_{ T}\|\).

According to the updating rule, we have the error decomposition

\[_{t} =_{t}-\] \[=(1-_{t})_{t-1}+_{t}_{t}_{t-1}-\] \[=(1-_{t})_{t-1}+_{t}(_{t} _{t-1}-)\] \[=(1-_{t})_{t-1}+_{t}(_{t}- )_{t-1}+_{t}(_{t-1}-)\] \[=[(1-_{t})+_{t}] _{t-1}+_{t}(_{t}-)_{t-1}.\]

Applying it recursively, we can further decompose the error into two terms

\[_{T}=^{T}[(1-_{t})+ _{t}]_{0}}_{}+^{T} _{t}_{i=t+1}^{T}[(1-_{i})+_{i}](_{t}-)_{t-1}}_{},\]

where \(_{k=1}^{t}_{k}\) is defined as \(_{t}_{t-1}_{1}\) for any operators or matrices \(\{_{k}\}_{k=1}^{t}\) throughout the paper. Term (I) is an initial error term that becomes negligible when \(T\) is large because \(\) is a contraction. Term (II) can be bounded via Freedman's inequality in the Hilbert space (Theorem A.2). Combining the two upper bound, we can establish a recurrence relation. Solving this relation will lead to the conclusion.

We first establish the conclusion for step sizes that depend on \(T\). Specifically, we consider

\[T^{3}T}{^{2}(1-)^{3}}|T}{},\]

\[(1-)T}{^{2}T}}_{t}(1-)t}{^{2}T}},\]

where \(c_{5}>c_{6}>0\) are small constants satisfying \(c_{5}c_{6}\), and \(C_{4}>1\) is a large constant depending only on \(c_{5}\) and \(c_{6}\). As shown in Appendix C.1, once we have established the conclusion in this setting, we can recover the original conclusion stated in the theorem.

Now, we introduce the following useful quantities involving step sizes and \(\)

\[_{k}^{(t)}:=_{i=1}^{t}(1-_{i}(1-)),&k=0,\\ _{k}_{i=k+1}^{t}(1-_{i}(1-)),&0<k<t,\\ _{T},&k=t.\]

The following lemma provides useful bounds for \(_{k}^{(t)}\).

**Lemma 5.1**.: _Suppose \(c_{5}c_{6}\). Then, for all \(t T}\), we have that_

\[_{k}^{(t)}}\,,\,0 k; _{k}^{(t)}T}{(1-)T},\,<k t.\]The proof can be found in Appendix C.2. From now on, we only consider \(t T}\).

The upper bound of term (I) is given by

\[_{k=1}^{t}\|(1{-}_{k})_{k} \|\|_{0}\|_{k=1}^{t}((1{-}_{k}){+} _{k})}}=^{(t )}}{}}}T^{2}},\]

where \(\|_{0}\|^{}dx}= }}\).

As for term (II), we have the following upper bound with high probability by utilizing Freedman's inequality (Theorem A.2).

**Lemma 5.2**.: _For any \((0,1)\), with probability at least \(1-\), we have for all \(t T}\), in the NTD case,_

\[\|_{k=1}^{t}_{k}_{i=k+1}^{t}[(1-_{i })+_{i}](_{k}- )_{k-1}\|\] \[ 34T)(|T}{ })}{(1-)^{2}T}}(1+_{k:\,t/2<k t}\|_{k-1} \|_{_{1}})\!.\]

_The conclusion still holds for the CTD case if we take \(K(1-)^{2}}+1\)._

The proof can be found in Appendix C.3. Combining the two results, we find the following recurrence relation in terms of the \(_{1}\) norm holds given the choice of \(T\), with probability at least \(1-\), for all \(t T}\)

\[\|_{t}\|_{_{1}}}\,\|_{t}\|  35T)(|T}{}) }{(1-)^{3}T}}(1+_{k:\,t/2<k t}\|_{k-1}\|_{_ {1}})\!.\]

In Theorem C.1, we solve the relation and obtain the error bound of the last iterate estimator:

\[\|_{T}\|_{_{1}} C_{7}(T)( |T}{})}{(1-)^{3}T}}+T)(|T}{})}{(1-)^{3}T} ),\]

where \(C_{7}>1\) is a large universal constant depending on \(c_{6}\). Now, we can obtain the conclusion if taking \(C_{4} 2C_{7}^{2}\) and \(T^{3}T}{^{2}(1-)^{3}}|T}{}\).

## 6 Conclusions

In this paper we have studied the statistical performance of the distributional temporal difference learning (TD) from a non-asymptotic perspective. Specifically, we have considered two instances of distributional TD, namely, the non-parametric distributional TD (NTD) and the categorical distributional TD (CTD). For both NTD and CTD, we have shown that \(((1-)^{2p+1}})\) iterations are sufficient to achieve a \(p\)-Wasserstein \(\)-optimal estimator, which is minimax optimal (up to logarithmic factors). We have established a novel Freedman's inequality in Hilbert spaces to prove these theoretical results, which has independent theoretical value beyond the current work. We leave the details to Appendix A.