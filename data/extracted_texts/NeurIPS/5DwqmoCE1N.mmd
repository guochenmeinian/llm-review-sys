# On the Surprising Effectiveness of Attention Transfer

for Vision Transformers

 Alexander C. Li

Carnegie Mellon University

&Yuandong Tian

FAIR

&Beidi Chen

Carnegie Mellon University

&Deepak Pathak

Carnegie Mellon University

&Xinlei Chen

FAIR

Work done during an internship at FAIR.

###### Abstract

Conventional wisdom suggests that pre-training Vision Transformers (ViT) improves downstream performance by learning useful representations. Is this actually true? We investigate this question and find that the features and representations learned during pre-training are not essential. Surprisingly, using only the attention patterns from pre-training (_i.e_., guiding how information flows between tokens) is sufficient for models to learn high quality features from scratch and achieve comparable downstream performance. We show this by introducing a simple method called attention transfer, where only the attention patterns from a pre-trained teacher ViT are transferred to a student, either by copying or distilling the attention maps. Since attention transfer lets the student learn its own features, ensembling it with a fine-tuned teacher also further improves accuracy on ImageNet. We systematically study various aspects of our findings on the sufficiency of attention maps, including distribution shift settings where they underperform fine-tuning. We hope our exploration provides a better understanding of what pre-training accomplishes and leads to a useful alternative to the standard practice of fine-tuning. Code to reproduce our results is at https://github.com/alexlioralexli/attention-transfer.

## 1 Introduction

Pre-training has emerged as a dominant paradigm in machine learning and has significantly improved performance on a variety of tasks . In computer vision in particular, self-supervised representation learning methods  and weakly supervised methods  have enabled learning from large amounts of images. It is widely accepted that these methods work because they teach models useful features that are relevant for downstream tasks. But is this story actually true? Perhaps there is another capability learned during pre-training that is sufficient to explain its benefits.

In this paper, we present an alternative explanation: pre-training teaches the model how information should be routed between tokens. We specifically focus on Vision Transformers (ViT) , not only because they are the most popular architecture for scaling, but also because Transformers explicitly _decouple_ this information flow. Inter-token communication is solely fulfilled by attention, while the remaining bulk of computation are intra-token operations that are applied to each token independently. In contrast, other architectures such as ConvNets  simultaneously expand the receptive fields and extract the features, making it difficult to isolate the effect of information flow. We hypothesize that the features computed by the intra-token operations are not essential to explain the benefits of pre-training, and that the pre-trained attention maps are typically sufficient for downstream tasks.

We test our hypothesis by introducing a new set of methods called attention transfer. Concretely, we treat a pre-trained ViT as the teacher and train a student model for downstream tasks whiletransferring only the attention patterns from the teacher. In contrast to the common fine-tuning paradigm of transferring all the weights (which mixes the effect of features and attention maps), _only_ the inter-token flow is transferred. In this way, the student must learn features from scratch, while isolating the benefits of the attention maps learned during pre-training.

We study two types of attention transfer. The first is _Attention Copy_, which directly "copy-and-pastes" the attention maps. The learning is fully decoupled, as inter-token computation is entirely from the teacher, and the student only learns intra-token patterns routed by the teacher's attention maps. This is well-suited as a scientific probe, but is less practical since both networks need to be forwarded during the inference. The second is _Attention Distillation_, where the student simply distills attention patterns from the teacher, whose attention maps are no longer used after training. This is practical, but also helps identify the importance of the teacher's inter-token information flow.

While both attention transfer variants are straightforward, we find them _highly effective_. Figure 1 illustrates this with a ViT-L  pretrained using Masked Autoencoding (MAE) . Compared to no transfer (training from scratch) and full transfer (fine-tuning all the MAE weights), Attention Copy can close most of the gap in performance, whereas Attention Distillation can _match_ the fine-tuning accuracy on ImageNet-1K classification . This is achieved by only transferring the inter-token flow from the same model. Furthermore, since attention transfer requires the student to learn features from scratch, those features are significantly different from the teachers' (Figure 5) and improve ImageNet-1K accuracy score to 86.3 (+0.6) when ensembled with the teacher (Figure 6).

To summarize, we make the following contributions:

* **Detailed analysis on the sufficiency of attention maps**. We find that solely using the pre-trained attention patterns is typically _sufficient_ to achieve the same downstream accuracy as fine-tuning on ImageNet-1K. Furthermore, we observe practical benefits, as ensembling with attention transfer significantly improves ImageNet performance. This calls into question the commonly-believed story that pre-training is only about feature learning. While our main observation is robust w.r.t. different models and pre-training methods, we _do find settings where pre-trained features are indeed necessary_ to realize the full gains from pre-training. Our bare-minimum solution for attention transfer is more affected by data distribution shifts compared to weight tuning. Section 4 presents extensive analyses to better understand the behaviors of attention transfer. They are i) partial transfer with a subset of layers or heads; ii) variants of our method that transfer other attention-related activations; and importantly, iii) various ways to verify that the student is _not_ just re-learning the teacher model. Section 5 systematically tests how well our findings apply across a variety of pre-training and fine-tuning datasets, pre-training methods, model sizes, and tasks.
* **Attention transfer methods**. We introduce Attention Copy and Attention Distillation, which are methods to train a ViT on a downstream task while utilizing only the attention maps of a pre-trained teacher ViT. These methods help us understand the role of the features versus the attention patterns learned during pre-training. With further research, attention transfer could offer a potential alternative to the decade-long practice of fine-tuning pre-trained vision models [16; 12; 22]. Nearly all aspects of the fine-tuning pipeline have been thoroughly examined, suggesting a probable saturation of recipes. Weight sharing can also face security risks (_e.g._, white-box attacks ). We hope our systematic examination of attention transfer sheds new light on how to leverage pre-trained ViTs, and will help establish this approach as an effective alternative when weight transfer is less applicable.

Figure 1: **Using only attention is sufficient for full performance**. By copying the attention maps (top) from a MAE  pre-trained ViT-L , a ViT-L can reach a top-1 accuracy of 85.1 on ImageNet-1K  – recovering 77.8% of the gap between no transfer (training from scratch, 83.0) and full transfer (fine-tuning all the weights, 85.7). Distilling attention maps (bottom) can even _fully_ match MAE weight tuning while only transferring the inter-token flow.

## 2 Attention Transfer

### Attention Preliminaries

To work with a Vision Transformer (ViT) , an image is first "patchified" into \(N\) tokens. Their intermediate activations are represented as a sequence \(X{=}[x_{1},x_{2},,x_{N}]^{}\), \(x_{i}{}^{C}\), where \(C\) is the embedding dimension. The self-attention  mechanism mainly introduces three learnable parameters \(W_{q},W_{k},W_{v}{}^{C C/H}\) (\(H\) is the number of heads). \(Q{=}XW_{q}\) is often referred to as the queries, \(K{=}XW_{k}\) as the keys, and \(V{=}XW_{v}\) as the values. Then the attention function is defined as:

\[f_{}=(QK^{})}_{}V,\] (1)

where the softmax function is computed per query for the _attention map_. Attention maps determine how the values from other tokens are aggregated, and with multiple heads, each token uses multiple attention distributions within the same Multi-headed Self-Attention (MSA) block.

For an \(L\)-layer Transformer, MSA blocks are interleaved with MLP blocks, and each Transformer layer contains one of each block type. Most operations are _intra-token computations_, which are applied independently to each token: value and projection matrices, normalization layers , and MLPs. The only _inter-token computation_ is applying the attention map \((QK^{})\), which is the only way for information to flow between tokens. Transformers are unique because their inter- and intra-token computations are _decoupled_; however, the relative importance of each type of operation is not well understood, and Transformers are typically trained by _jointly_ fine-tuning all the weights.

Deviating from the common practice of joint weight tuning, we propose two attention transfer methods with the goal of exploring decoupled training for ViTs, described next.

### Attention Copy

In this setup, we utilize two separate networks: a pre-trained teacher network that _only_ does a forward pass to compute its attention maps, and a student network that directly copies the attention maps from the teacher but computes all of the other activations. The student's weights are randomly initialized and trained via back-propagation, while the teacher's weights are kept frozen. This setting fully isolates the attention maps from the features that they are applied to, and thus is ideal for measuring the utility of pre-trained attention patterns when the student network learns to perform other tasks (_e.g_., image classification).

We call this method _Attention Copy_, as we "copy-and-paste" the attention maps from teacher to student. Figure 2 (left) shows a diagram of this approach. Note that it requires forward passes

Figure 2: Two types of **Attention transfer** for Vision Transformers. **Attention Copy** (left): We simply “copy-and-paste” the attention maps from a pre-trained teacher model to a randomly initialized student one. Other weights of the student are then trained via supervised learning. This fully decouples inter-token learning (from the teacher) and intra-token learning (in the student); but is less practical. **Attention Distillation** (right): The student computes its own attention maps, with an additional cross-entropy loss to distill patterns from the teacher during training. The teacher is no longer used during inference. \(H\): number of heads; \(L\): number of Transformer layers.

through both the teacher and student networks during the inference time. Given the extra computation, Attention Copy is not meant to be an entirely practical method. We mitigate this issue next.

### Attention Distillation

In _Attention Distillation_, the teacher network is only utilized at the training time. Given each training example, we forward both networks in parallel, with the student also computing its own attention maps. But besides the task-driven loss, we also enforce a distillation loss between student's attention maps and the teacher's counterparts as (soft) targets. Formally, using \({Q_{s}{K_{s}}^{}}\) for the student and \({Q_{t}{K_{t}}^{}}\) for the teacher, the loss is then defined as:

\[_{}=[({Q_{s}{K_{s}}^{} }),({Q_{t}{K_{t}}^{}})],\] (2)

where \(\) computes the cross entropy. As there can be multiple heads and layers in a Transformer, we simply sum up all the losses from wherever attention distillation is applied. Again, the student is trained via back-propagation. Figure 2 (right) shows the diagram of Attention Distillation.

Compared to Attention Copy, Attention Distillation is much more practical. After training, the teacher is no longer needed, and the student can be used as a standalone model. Compared to training ViTs from scratch, the only addition is the distillation loss, meaning most of the optimization (_e.g._, learning rate, momentum) and regularization (_e.g._, weight decay, dropout rate ) hyperparameters can follow the scratch recipe with minimal adjustments. It does introduce a new hyperparameter \(\), which weights the distillation loss and balances it with the task loss.

Attention Distillation can be viewed as a form of generalized knowledge distillation, but it has several key differences from the design proposed by Hinton et al. . Attention Distillation trains the student to match the teacher's intermediate attention maps, not the final teacher output. This gives the flexibility of distilling from models trained on any task, not just models trained on the same final task. This property is well-suited for today's "pre-train and transfer" paradigm, where the pre-training task (_e.g._, reconstruction) and the downstream task (_e.g._, classification) are usually different. However, Attention Distillation does add the constraint that the architecture needs to compute attention maps. We leave experimenting on this idea for other architectures as future work.

Overall, while fancier designs can be used for both Attention Copy and Attention Distillation, we choose to keep them simple for cleaner assessments of their effectiveness.

### Connection to Transformer Training Dynamics

Our investigation is also linked to recent attempts to theoretically understand the training dynamics of Transformers. Specifically, the inter-token flow encoded in the pre-trained attention maps can be regarded as a discovered latent _hierarchy_ from the dataset. Self-attention can quickly capture frequently co-occurring token pairs [31; 52]. However, more occasional co-occurrences need to be explained by the top-level hierarchy, rather than directly learned in the lower levels . This is due to many potential spurious correlations , especially in the over-parameterized setting. Transferring attention maps from a trained teacher reduces these spurious inter-token correlations, so the student can focus on intra-token learning (_i.e._, computing useful features).

## 3 Main Results

As featured in Figure 1, attention transfer is highly effective despite its simplicity. Specifically, we demonstrate this with a ViT-L  pre-trained with Masked Autoencoding (MAE)  for ImageNet-1K classification . Note that this is the _signature_ result that established MAE's effectiveness for pre-training: compared to a ViT-L trained from scratch (with an accuracy score of 83.0), fine-tuning the MAE pre-trained on the same dataset results in a significant improvement to 85.7.1

For attention transfer, we use the same pre-trained MAE model as our teacher, and since scratch training can be viewed as no transfer, and fine-tuning weights transfers all the weights, the above two results serve as natural lower and upper bounds for the effectiveness of our attention transfer methods. We make two observations (from Table 1 and Figure 1).

#### Attention Copy can largely close the gap between scratch training and full weight fine-tuning

We report an accuracy of 85.1 with Attention Copy. This has largely closed the gap between scratch and full weight tuning (to be precise, 77.8% of the 2.7 percentage point gap). This is surprising, since the teacher's attention maps are frozen after pre-training for a different task (image reconstruction), and the student must learn everything else (the intra-token operations) completely from scratch.

As another upper-bound, we also experimented with Attention Copy from the _fine-tuned_ model. This reaches an accuracy score of 85.6 - almost matching the teacher's performance (85.7), suggesting that adapting attention maps to the specific task is still helpful, but not crucial, especially as MAE pre-training is performed on the same data distribution.

#### Attention Distillation can match fine-tuning performance

Even more surprisingly, we find Attention Distillation can achieve 85.7 - _on par_ with fine-tuning the ViT-L weights from MAE. Since Attention Distillation and weight tuning both result in the same-sized model, which requires the same compute budget for inference, this result suggests Attention Distillation can be an effective drop-in replacement for weight tuning when the latter is less applicable (_e.g._, if weight sharing poses security risks, we can instead send the teacher's attention maps).

We hypothesize that distillation is better than copy because the student can choose how closely it matches the teacher attention maps, to better suit the task objective. This is also supported by the 85.6 accuracy of copying from fine-tuned MAE, which has the correct task-specific attention maps.

## 4 Analysis

Next, we provide extensive analyses to better understand the effectiveness of attention transfer. Broadly speaking, the explorations are driven by the following two questions:

1. How important are different activations, layers, and heads? (Section 4.1)
2. Is attention transfer re-learning everything from the teacher? (Section 4.2)

### Variants of Attention Transfer

We study four variants of attention transfer. We use Attention Copy within this section, since it is a fully-decoupled setting well-suited for scientific analysis.

#### Transfer a subset of \(Q\), \(K\) and \(V\).

A natural alternative to transferring attention maps is to transfer different activations that come with self-attention (Eq. 1), namely queries \(Q\), keys \(K\), or values \(V\). Without loss of generality, if we transfer the teacher's \(Q\), the student will compute its own \(K\) and \(V\) and use them normally. Note that transferring both \(Q\) and \(K\) is equivalent to transferring the map \((QK^{})\). Table 2 shows that transferring \(Q\) works surprisingly well, and is actually better than transferring the attention map.

We suggest that copying \(Q\) gives the model the flexibility to deviate from the teacher attention maps and use attention patterns that are better suited for the downstream task. This is supported by the fact

 method & acc. \\  scratch & 83.0 \\ fine-tune & 85.7 \\  attn. copy & 85.1 \\ attn. copy from fine-tuned & 85.6 \\ attn. distill & 85.7 \\  

Table 1: **Main results**. We show that the pre-trained attention patterns are _sufficient_ to match fine-tuning accuracy on ImageNet. Attention Copy closes most of the gap, and Attention Distillation achieves the same accuracy.

 transfer target & acc. \\  \(Q\) & 85.6 \\ \(K\) & 84.4 \\ \(V\) & 84.4 \\  \(Q,K\) & 85.1 \\  

Table 2: **Transfer other attention activations**. We test copying alternative attention activations other than the attention map – \((QK^{})\). All alternatives do better than training from scratch, and transferring queries \(Q\) actually does better than transferring the attention map.

that copying \(Q\) and Attention Copy from the fine-tuned model both achieve the same accuracy of \(85.6\). Appendix B.3 gives deeper into this hypothesis and finds that the attention maps for copying \(Q\) are similar to the teacher's but less constrained than they are in other transfer methods. While more investigation could be done in future work, our findings suggests that the queries \(Q\) are more important than the keys, which is consistent with previous findings in text sequence modeling where the number of keys and values per layer can be significantly reduced .

Finally, we test whether distilling \(Q\) could outperform full Attention Distillation. However, Table 3 shows that \(Q\) distillation does significantly worse. We hypothesize that this is because it is harder for the student to learn useful keys \(K\) while \(Q\) is still being learned, and because Attention Distillation already has the flexibility to adjust its attention maps to suit the downstream task.

**Partial transfer: layers.** We next change the number of Transformer layers transferred, aiming to identify which layers are more valuable from the teacher. The baseline transfers all the layers. In Figure 3, we try transferring attention maps only from the first or last layers. For the remaining layers, the student learns to compute its own attention maps.

We make several observations: i) We find transferring more layers is always more helpful. This is a bit surprising, as one may expect attention patterns optimized for MAE's patch-wise reconstruction task to be sub-optimal for a high-level recognition task like classification. ii) We find transferring the later attention maps is generally better. In fact, performance roughly saturates when transferring the last 15 attention maps out of a total of 24. This indicates that ViTs are capable of learning good low-level representations, as long as they are told how to combine these features into higher-level ones; but not vice versa. This reinforces the theory from Tian et al.  that guidance on top-level hierarchy is more important, as there are many more possibilities, and attention transfer can reduce possible spurious correlations in the lower levels.

**Partial transfer: heads.** Finally, we switch back to transferring all the layers, but change the number of heads copied from each MSA block. Specifically, instead of copying the attention map from every head, we can selectively choose to use a subset of the teacher's heads. The student can then compute its own attention patterns for the unused heads. Figure 4 shows the effect of transferring fewer heads for each layer. Performance improves as we do attention transfer with more heads, though performance almost saturates at 12 out of 16 heads. Note that we simply follow a naive selection strategy and use the first set of heads; more advanced strategies based on diversity or activation magnitude can potentially improve the robustness as we reduce the number of heads.

### Are We Re-Learning the Teacher?

Since attention transfer provides a significant amount of information from the teacher (ViT-L attention maps have about 10M activations total per image, see Appendix A.1 for detailed calculations), a natural question is whether the student performs well because it simply relearns the same representations as the teacher. We thoroughly test this hypothesis on different aspects of the student model.

Figure 4: **Copy a subset of heads. The pretrained ViT-L has 16 heads in each MSA block. By default, all of them are transferred. Here we only transfer a subset, and find more heads helps in general, but performance saturates at 12 heads.**

 method & acc. \\  attn. distill & 85.7 \\ \(Q\) distill & 81.3 \\ 

Table 3: Attention Distillation outperforms \(Q\) distillation.

**Representation similarity.** One way that the student can reproduce the teacher is by learning the same intermediate features. We measure this using Centered Kernel Alignment (CKA) , a similarity measure for representations that has been shown to be effective even for networks trained with different initializations or architectures. CKA is a layer-wise comparison that ranges from 0 (completely dissimilar) to 1 (identical) and is invariant to orthogonal linear transformations and isotropic scaling of the features. Figure 5 shows the CKA between our fine-tuned model and our attention transfer methods. We also show the pre-trained model and a ViT-L trained from scratch for reference. We compute CKA with respect to the fine-tuned model, even though we copy or distill from the pre-trained MAE model, since the features change significantly during fine-tuning to become more task-specific. Overall, Attention Copy and Attention Distillation are both quite _dissimilar_ to the fine-tuned MAE model, following the same similarity trend as the scratch model. Our sanity check passes, as CKA shows that pre-trained and fine-tuned MAE have very similar representations in the early layers. This is expected since fine-tuning with layer-wise learning rate decay  means the earliest layers change very little during fine-tuning.

**Prediction similarity and ensembling.** Our CKA analysis may not catch some similarity of the intermediate representations, as CKA does not detect all forms of the same information (_e.g._, invertible nonlinear transforms) . Since intermediate representations may not tell the full story, we also examine the similarity of the network outputs. We measure this using network ensembling: given softmax predictions \(p_{}\) from the fine-tuned model and \(p_{}\) from another model, we test the accuracy of their average \((p_{}+p_{})/2\). The more independent the model predictions are, the higher their ensemble accuracy is. Figure 6 compares accuracy before and after ensembling with the fine-tuned model. Attention transfer is dissimilar enough to achieve high ensemble accuracy, and ensembling Attention Distillation with a fine-tuned MAE achieves 86.3, +0.6 over the fine-tuned MAE model.

Finally, Appendix B.4 visualizes the attention maps learned by Attention Distillation and shows that they match for distilled attention blocks but are drastically different for layers that are not distilled.

## 5 Generalization and Limitations

In this section, we test how well our findings on attention transfer apply across a variety of pre-training and fine-tuning datasets, pre-training methods, model sizes, and tasks.

Figure 5: **CKA representation similarity to the fine-tuned model.** We use CKA  to measure the layer-wise similarity between representations learned in different models against the fine-tuned MAE model. Higher means more similar. We find that attention transfer methods are quite _dissimilar_ to the fine-tuned model, with roughly the same CKA as an independent scratch model.

Figure 6: **Ensemble accuracy with the fine-tuned model**. We plot the accuracy of ensembling our attention transfer models and a fine-tuned MAE. This measures model prediction similarity with the fine-tuned model. The ensemble yields notable accuracy gains over the base model, reaching up to 86.3. This is even higher than ensembling two separate fine-tuned MAE models2 (86.2) and indicates that the attention transfer models are _less_ correlated with the fine-tuned model.

### Pre-training and fine-tuning datasets

So far, we have focused on a MAE model pre-trained and evaluated on ImageNet-1K. What happens if we pre-train or evaluate on different datasets? We first test this by pre-training MAE ViT-L models on two new datasets: ImageNet-22K  and COCO . These have substantially different dataset bias  from ImageNet, across axes like appearance, class balance, and diversity. Table 4 shows that the resulting MAE models maintain relatively good performance when fine-tuning on ImageNet-1K, with a maximum drop of at most 0.5. However, Attention Copy accuracy drops more, losing as much as 2.1. We see a similar phenomenon in Table 5 where we use a MAE pre-trained on ImageNet and transfer to the iNaturalist datasets . Again, when the pre-training dataset does not match the transfer dataset, Attention Copy accuracy drops significantly. We hypothesize that the frozen teacher's attention maps are ill-suited for the fine-tuning dataset, which limits the performance.

### Out-of-distribution robustness

One notable aspect of a standard fine-tuned MAE model is that it shows slight "effective robustness," _i.e._, it achieves slightly better out-of-distribution (OOD) accuracy than expected based on its in-distribution (ID) accuracy . We test whether Attention Distillation, which achieves the same ID accuracy, has the same benefits OOD. Table 6 shows that Attention Distillation still does quite well, but has lower accuracy than fine-tuned MAE on all 4 distribution shifts we tried. These results indicate that the attention maps do not account for the full robustness benefits, and that the features learned by MAE during pre-training are helpful OOD even if they are not ID.

### Pre-training methods

We have so far focused on MAE, a reconstruction-based pre-training method. We now check whether attention transfer still works if the teacher has been pre-trained with a different algorithm. Specifically, we test MoCo-v3 , a self-supervised contrastive learning approach, and FLIP , which does image-text contrastive learning. Table 7 shows that Attention Copy still achieves most of the performance benefits for each pre-training method. _Impressively, ViT-L is even able to reach 86.6 by just transferring attention maps from FLIP_. This confirms that learning the proper attention patterns is indeed a significant bottleneck during learning. Note that the FLIP model we used is pre-trained on LAION-2B , yet its effectiveness is less affected by distribution shifts to ImageNet-1K.

 out-of-distribution evaluation & scratch & tune & copy & distill \\  ImageNet-A  & 32.0 & 56.5 & 48.9 & 54.3 \\ ImageNet-R  & 51.9 & 59.6 & 57.5 & 56.8 \\ ImageNet-S  & 38.0 & 45.2 & 43.1 & 42.9 \\ ImageNet-V2  & 72.4 & 76.4 & 75.5 & 75.9 \\ 

Table 6: **Out-of-distribution robustness**. We take two models that achieve the same accuracy on ImageNet-1K (fine-tuned and distilled), and evaluate them on a suite of distribution shifts. Attention Distillation does well when the distribution is close (_e.g._, on ImageNet-V2), but loses the mild “effective robustness” that fine-tuned MAE has been found to have .

 pre-training data & tune & copy \\  ImageNet & 85.7 & 85.1 \\ ImageNet-22K & 85.5 & 84.4 \\ COCO & 85.2 & 83.1 \\ 

Table 4: **Different pre-training datasets**. We pre-train MAE on more datasets, and then either fine-tune or do copy for ImageNet-1K classification. Attention transfer works well when the data distribution stays stable, but its effectiveness is more negatively affected by distribution shifts.

 eval. data & scratch & tune & copy & distill \\  iNat 2017 & 49.7 & 75.9 & 69.1 & 69.3 \\ iNat 2018 & 64.3 & 79.9 & 71.8 & 74.1 \\ iNat 2019 & 66.2 & 83.8 & 77.9 & 80.0 \\ 

Table 5: **Long-tail recognition on iNaturalist**, with ImageNet-1K pre-trained MAE. We tune weights or do attention transfer on iNaturalist, and we again find attention transfer is worse than tuning weights when the pre-training dataset is different from the downstream dataset.

### Model size

We test whether attention transfer works across model sizes. For all experiments so far, we have used ViT-L; here, we try Attention Copy from a smaller (ViT-B) and larger (ViT-H) model, both pre-trained with MAE. Table 8 shows that Attention Copy continues to improve with scale, even reaching 86.1% accuracy with ViT-H. It can do this even though scratch model performance already saturates at the ViT-L size. Again, this indicates that models need proper inter-token routing in order to learn good features that generalize. Otherwise, they cannot properly utilize increased model capacity.

### Object Detection

Finally, we examine the performance of attention transfer in the standard ViTDet pipeline  for COCO object detection. We compare training from scratch against fine-tuning and attention transfer from a MAE ViT-B pre-trained on COCO, which is done to mitigate the effect of distribution shift. For fair comparisons, we use a \(448{}448\) input to remove the effect from window attention and positional embedding interpolation, and remove the effect of relative positional embeddings. Table 9 shows that Attention Distillation recovers a majority of the gains from pre-training in this dense prediction setting as well. Based on Table 8, we anticipate that the gap between fine-tuning and attention transfer will decrease with ViT-L, but we are limited by computational resources.

## 6 Related Work

**Structure in attention patterns.** Previous works have studied the attention patterns of pre-trained vision transformers [59; 63; 43]. These works present these differences only as qualitative observations, whereas we are able to isolate the attention patterns and show that they are causally responsible for most of the differences in fine-tuning performance. Other methods, such as Lora  or Prompt-to-Prompt , do rely on the importance of high quality attention patterns within pre-trained networks, but they also utilize pre-trained features and do not provide our insight that _these features are typically unnecessary_ for the tasks we examine. Trockman and Kolter  observe diagonal structure within the product of attention layer weights in a trained supervised network. They show that initializing the weights with this structure moderately improves accuracy for small models early in training. Zhang et al. , in the language domain, find that pre-trained BERT models improve length generalization on particular synthetic tasks. They attribute this to the attention patterns of a few, specific heads and show that hardcoding these patterns into the network achieves the same benefit. Our work is complementary and emphasizes the importance of attention maps over features.

**Decoupling inter- and intra-token operations.** GLoMo  also attempts to decouple features from the way they should be combined. They use unsupervised pre-training to train a network to

 pre-training method & tune & copy & distill \\  MAE  & 85.7 & 85.1 & 85.7 \\ MoCo-v3  & 84.0 & 82.5 & 83.3 \\ FLIP  & 87.4 & 86.6 & 86.1 \\ DINO\({}^{}\) & 83.2 & 82.3 & 82.8 \\ none & 83.0 & 72.7 & 76.3 \\ 

Table 7: **Different pre-training methods. Attention transfer works for all pre-training methods, even reaching 86.6 with a FLIP teacher. As a sanity check, transferring from a randomly initialized ViT significantly hurts. \({}^{}\)DINO is ViT-B.**

 model & scratch & tune & copy & distill \\  ViT-B & 82.5 & 83.6 & 82.0 & 83.4 \\ ViT-L & 83.0 & 85.7 & 85.1 & 85.7 \\ ViT-H & 83.0 & 86.9 & 86.1 & 86.3 \\ 

Table 8: **Different model size** with MAE pretrained on ImageNet-1K. Similar to weight tuning, the classification accuracy of attention transfer scales well as we vary the model size, while scratch training saturates.

 metric & scratch & tune & distill \\  \(AP^{box}\) & 39.1 & 46.3 (+7.2) & 43.6 (+4.5) \\ \(AP^{mask}\) & 34.6 & 40.6 (+6.0) & 38.7 (+4.1) \\ 

Table 9: **Object detection results on COCO with a MAE ViT-B pre-trained on COCO. Attention transfer achieves a majority of the gains of pre-training in this setting as well.**output a graph, which is later used to combine task-specific features. We find that there is no need to develop a specialized architecture to achieve this - Vision Transformers _already do this naturally_.

**Knowledge Distillation** Knowledge distillation is a popular framework for training small, high-performing student networks . Knowledge distillation methods typically add a loss to encourage the student network to match the teacher network's logits, but variants often use other feature statistics as targets, such as the final representations , intermediate feature similarities , or intermediate feature magnitudes . This last approach has also previously been called "attention transfer," but their method is quite different and actually refers to distilling spatial activation magnitudes in ConvNets. These knowledge distillation approaches all assume that students need to be explicitly taught the right features. In contrast, our analysis with attention transfer shows that attention maps are sufficient to recover all of the gains from pre-training. Some papers have used attention distillation as an auxiliary loss to help a smaller model learn the teacher outputs more effectively . However, these only consider transferring the same function across model sizes, instead of transferring knowledge from a pre-trained model to a different downstream task.

**Connection to the lottery ticket hypothesis** The lottery ticket hypothesis  suggests that large, dense neural networks contain small, sparse subnetworks ("winning tickets") that, when trained from scratch, can match or even outperform the performance of the original dense network. This is particularly interesting because these sparse subnetworks maintain their performance only with their original initialization values; the strength of their connections between _neurons_ is special in some way. Our findings draw a parallel, indicating that the connections between _patches_, controlled solely by the attention patterns, are similarly special within pretrained ViTs. Frankle and Carbin  further conjecture that overparameterization improves performance because larger models contain exponentially more sparse subnetworks in superposition and are thus more likely to contain a "winning ticket" - a hypothesis supported by subsequent empirical and theoretical work . However, this phenomenon does not arise in our setting with ViT attention patterns, since there are only a handful of attention maps per layer (rather than thousands of neurons). Consequently, good attention patterns are unlikely to appear by chance and must instead be learned during pre-training. We hope that a new model architecture that efficiently combines many more attention maps per layer can address this limitation and learn better from scratch than existing ViTs.

## 7 Conclusion

Even as Transformers have surged in popularity, the way we use them has remained stagnant: pre-train, then fine-tune the weights. In this work, we present attention transfer, a simple alternative to ViT fine-tuning that decouples intra-token operations (how to extract more usable features for each token) from inter-token operations (how those features should be combined). Our key finding is that the attention patterns (inter-token operations) are the key factor behind much of the effectiveness of pre-training - our Attention Distillation method _completely matches fine-tuning_ on ImageNet-1K. We do find some limitations: attention transfer does not work well if the pre-training and transfer datasets are different, and it loses a bit of OOD robustness. Nevertheless, our findings provide insights into the role of attention in pre-trained ViTs, and we hope future work fixes attention transfer's shortcomings and explores the advantages of this new transfer method.

Some directions for future work are particularly interesting. First, a deeper investigation of the queries \(Q\) could help us better understand their importance and potentially yield better transfer strategies. Second, attention transfer eliminates the need for tricks that fine-tuning requires, such as layerwise learning rate decay. Layerwise learning rate decay adds the prior that early layers should change less compared to later layers. However, this prior may be overly restrictive for next-generation models, since it prevents early features from changing, and getting rid of it could open up new opportunities. Finally, attention maps could potentially be transferred more easily across model sizes. Pre-training a smaller model and transferring its attention patterns to a larger downstream model could be more practical than the current practice of fine-tuning.