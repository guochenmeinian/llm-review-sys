# QGFN: Controllable Greediness with Action Values

Elaine Lau\({}^{1\,2\,}\)1 Stephen Zhewen Lu\({}^{2}\) Ling Pan\({}^{1\,5}\)

Doina Precup\({}^{1\,2\,3}\) Emmanuel Bengio\({}^{4}\)

\({}^{1}\)Mila - Quebec AI Institute \({}^{2}\)McGill University \({}^{3}\)Google Deepmind

\({}^{4}\)Valence Labs \({}^{5}\)Hong Kong University of Science and Technology

###### Abstract

Generative Flow Networks (GFlowNets; GFNs) are a family of energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples. However, consistently biasing GFNs towards producing high-utility samples is non-trivial. In this work, we leverage connections between GFNs and reinforcement learning (RL) and propose to combine the GFN policy with an action-value estimate, \(Q\), to create greedier sampling policies which can be controlled by a mixing parameter. We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity.

## 1 Introduction

Generative Flow Networks , also known as GFlowNets, or GFNs, were recently introduced as a novel generative framework in the family of energy-based models . Given some energy function \(f(x)\) over objects \(\), the promise of GFNs is to train a sampler \(p_{}\) such that at convergence \(p_{}(x)(-f(x))\); \((-f(x))\) is also referred to as _the reward_\(R(x)\) in GFN literature, inheriting terminology from Reinforcement Learning (RL). GFNs achieve this sampling via a constructive approach, treating the creation of some object \(x\) as a sequential additive process (rather than an iterative local process _a la_ Markov chain Monte Carlo (MCMC) that can suffer from mode-mixing issues). The main advantage of a GFN is its ability to generate a greater diversity of low-energy/high-reward objects compared to approaches based on MCMC or RL , or even Soft-RL-which, while related to GFNs, accomplishes something different by default .

To generate more interesting samples and avoid oversampling from low-reward regions, it is common to train a model to sample in proportion to \(R(x)^{}\); \(\) is an inverse temperature, typically \( 1\), which pushes the model to become _greedier_. The use of this temperature (hyper)parameter is an important control knob in GFNs. However, tuning this hyperparameter is non-trivial, which complicates training certain GFNs; for example, trajectory balance  is sensitive to the "peakiness" of the reward landscape . Although it is possible to train temperature-conditional models , doing so essentially requires learning a whole _family_ of GFNs-no easy task, albeit doable, e.g., in multiobjective settings .

In this work, we propose an approach that allows selecting arbitrary greediness at inference time, which preserves the generality of temperature-conditionals, while simplifying the training process. We do so without the cost of learning a complicated family of functions and without conditionals, instead only training two models: a GFlowNet and an action-value function \(Q\).

Armed with the forward policy of a GFN, \(P_{F}\) (which decides a probability distribution over actions given the current state, i.e. \(\) in RL), and the action-value, \(Q\), we show that it is possible to create a variety of controllably greedy sampling policies, controlled by parameters that require no retraining. We show that it is possible to simultaneously learn \(P_{F}\) and \(Q\), and in doing so, to generate more high-reward yet diverse object sets. In particular, we introduce and benchmark three specific variants of our approach, which we call QGFN: \(p\)-greedy, \(p\)-quantile, and \(p\)-of-max.

We evaluate the proposed methods on 5 standard tasks used in prior GFN works: the fragment-based molecular design task introduced by Bengio et al. (2021), 2 RNA design tasks introduced by Sinai et al. (2020), a small molecule design task based on QM9 (Jain et al., 2023), as well as a bit sequence task from Malkin et al. (2022), Shen et al. (2023). The proposed method outperforms strong baselines, achieving high average rewards and discovering modes more efficiently, sometimes by a large margin. We conduct an analysis of the proposed methods, investigate key design choices, and probe the methods to understand why they work. We also investigate other possible combinations of \(Q\) and \(P_{F}\)-again, entirely possible at inference time, by combining trained \(Q\) and \(P_{F}\) models.

## 2 Background and Related Work

We follow the general setting of previous GFN literature and consider the generation of discrete finite objects, but in principle our method could be extended to the continuous case (Lahlou et al., 2023).

GFlowNetsGFNs (Bengio et al., 2021) sample objects by decomposing their generation process in a sequence \(=(s_{0},..,s_{T})\) of constructive steps. The space can be described by a pointed directed acyclic graph (DAG) \(=(,)\), where \(s\) is a partially constructed object, and \((s s^{})\) is a valid additive step (e.g., adding a fragment to a molecule). \(\) is rooted at a unique initial state \(s_{0}\).

GFNs are trained by pushing a model to satisfy so-called _balance_ conditions of flow, whereby flows \(F(s)\) going through states are conserved such that terminal states (corresponding to fully constructed objects) are sinks that absorb \(R(s)\) (non-negative) units of flow, and intermediate states have as much flow coming into them (from parents) as flow coming out of them (to children). This can be described succinctly as follows, for any partial trajectory \((s_{n},..,s_{m})\):

\[F(s_{n})_{i=n}^{m-1}P_{F}(s_{i+1}|s_{i})=F(s_{m})_{i=n}^{m-1}P_{B}(s _{i}|s_{i+1})\] (1)

where \(P_{F}\) and \(P_{B}\), the forward and backward policies, are distributions over children and parents respectively, representing the fraction of flow emanating forward and backward from a state. By construction for terminal (leaf) states \(F(s)=R(s)\).

Balance conditions lead to learning objectives such as Trajectory Balance (TB; Malkin et al., 2022), where \(n=0\) and \(m\) is the trajectory length, and Sub-trajectory Balance (SubTB; Madan et al., 2023), where all combinations of \((n,m)\) are used. While a variety of GFN objectives exist, we use these two as they are considered standard. If those objectives are fully satisfied, i.e. 0-loss everywhere, terminal states are guaranteed to be sampled with probability \( R(s)\)(Bengio et al., 2021).

Action valuesFor a broad overview of RL, see Sutton and Barto (2018). A central object in RL is the _action-value_ function \(Q^{}(s,a)\), which estimates the expected "reward-to-go" when following a policy \(\) starting in some state \(s\) and taking action \(a\); for some discount \(0 1\),

\[Q^{}(s,a)=_{a_{t}(.|s_{t})\\ s_{t+1} T(s_{t},a_{t})}[_{t=0}^{}^{t}R (s_{t})s_{0}=s,a_{0}=a]\] (2)

While \(T(s,a)\) can be a stochastic transition operator, in a GFN context objects are constructed in a deterministic way (although there are stochastic GFN extensions; Pan et al. (2023)). Because rewards are only available for finished objects, \(R(s)=0\) unless \(s\) is a terminal state, and we use \(=1\) to avoid arbitrarily penalizing "larger" objects. Finally, as there are several possible choices for \(\), we will simply refer to \(Q^{}\) as \(Q\) when statements apply to a large number of such choices.

### Related Work

RL and GFlowNetsThere are clear connections between the GFN framework and RL framework (Tiapkin et al., 2023; Mohammadpour et al., 2024; Deleu et al., 2024). Notably, Tiapkin et al. (2023) show that it is possible to reformulate fixed-\(P_{B}\) GFlowNets as a Soft-RL problem within a specific class of reward-modified MDPs. While they show that this reformulated problem can then be tackled with any Soft-RL method, this still essentially solves the original GFlowNet problem, i.e. learn \(p_{}(x) R(x)\). Instead, we are interested in greedier-yet-diverse methods.

A commonly used tool in GFNs (and QGFN) to increase the average reward, aka "greediness" of the drawn samples, is to adapt the reward distribution by using an altered reward function \((x)=R(x)^{}\) and adjusting the exponent parameter \(\): the higher the \(\), the greedier the model should be (Jain et al., 2023). However, increasing \(\) often induces greater numerical instability (even on a log scale), and reduces diversity because the model is less incentivized to explore "middle-reward" regions. This can lead to mode collapse. Kim et al. (2023) show that it is possible to train models that are conditioned on \(\), which somewhat alleviates these issues, but at the cost of training a more complex model.

Again, while we could leverage the equivalence between the GFN framework and the Soft-RL framework (Tiapkin et al., 2023), this approach would produce a soft policy. We propose a different approach that increases greediness of the policy via "Hard-RL".

Improving GFlowNet samplingA number of works have also made contributions towards improving utility in GFlowNets, via local search (Kim et al., 2023), utilizing intermediate signals (Pan et al., 2023), or favoring high-potential-utility intermediate states (Shen et al., 2023), as well as the use of RL tools such as replay buffers (Vemgal et al., 2023), target networks (Lau et al., 2023), or Thompson sampling (Rector-Brooks et al., 2023).

## 3 Motivation

Consider the following scenario, illustrated in Figure 1: an agent is faced with two doors. Behind the left door, there are 100 other doors, each hiding a reward of 1. Behind the right door, there is a single door hiding a reward of 100. The flow will be such that \(F()=F()=100\), meaning that a GFN agent will pick either door with probability \(}{{2}}\). The action value function is \(Q(s_{0},)=1\), \(Q(s_{0},)=100\), so an agent basing its decisions on \(Q\) will reach for the door with reward 100.

This example shows that _relying solely on flows is not always sufficient to provide high-value samples frequently_ and is illustrative of real-world scenarios. Consider molecular design (a very large search space) with some reward in \(\); there may be \(10^{6}\) molecules with reward \(.9\), but just a dozen with reward \(1\). Since \(.9 10^{6}\) is much bigger than \(12 1\), the probability of sampling a reward 1 molecule will be low if one uses this reward naively. While using a temperature parameter is a useful way to increase the probability of the reward \(1\) molecules, we propose a complementary, inference-time-adjustable method.

Figure 1: Solely relying on flow functions \(F\) in GFNs can be insufficient. While GFNs capture _how much stuff_ there is, they spend time sampling from lots of small rewards.

Note that relying solely on \(Q\) is also insufficient. If \(Q\) were estimated very well for the optimal policy (which is extremely hard), it would be (somewhat) easy to find the reward 1 molecules via some kind of tree search following \(Q\) values. However, in practice, RL algorithms easily collapse to non-diverse solutions, only discovering a few high reward outcomes. This is where flows are useful: because they capture _how much stuff_ there is in a particular branch (rather than an expectation), it is useful to follow flows to find regions where there is potential for reward. In this paper, we propose a method that can be greedier (by following \(Q\)) while still being exploratory and diverse (by following \(F\) through \(P_{F}\)).

## 4 QGFN: controllable greediness through \(Q\)

Leveraging the intuition from the example above, we now propose and investigate several ways in which we can use \(Q\)-functions to achieve our goal; we call this general idea **QGFN**. In particular, we present three variants of this idea, which are easy to implement and effective: \(p\)-greedy QGFNs, \(p\)-quantile QGFNs, and \(p\)-of-max QGFNs. In \(@sectionsign\)5 and \(@sectionsign\)6, we show that these approaches provide a favourable trade-off between reward and diversity, during both training and inference.

As is common in GFlowNets, we train QGFN by sampling data from some behavior policy \(\). We train \(F\) and \(P_{F}\) (and use a uniform \(P_{B}\)) to minimize a flow balance loss on the minibatch of sampled data, using a temperature parameter \(\). Additionally, we train a \(Q\)-network to predict action values on the same minibatch (the choice of loss will be detailed later). Training the GFN and \(Q\) on a variety of behaviors \(\) is possible because both are off-policy methods. Indeed, instead of choosing \(\) to be a noisy version of \(P_{F}\) as is usual for GFNs, we combine the predictions of \(P_{F}\) and \(Q\) to form a _greedier_ behavior policy. In all proposed variants, this combination is modulated by a factor \(p\), where \(p=0\) means that \(\) depends only on \(P_{F}\), and \(p=1\) means \(\) is greediest, as reflected by \(Q\). The variants differ in the details of this combination.

\(p\)-greedy QGFNHere, we define \(\) as a mixture between \(P_{F}\) and the \(Q\)-greedy policy, controlled by factor \(p\):

\[(s^{}|s)=(1-p)P_{F}(s^{}|s)+p[s^{}=_{i}Q(s,i)]\] (3)

In other words, we follow \(P_{F}\), but with probability \(p\), the greedy action according to \(Q\) is picked. All states reachable by \(P_{F}\) are still reachable by \(\). Note that \(p\) can be changed to produce very different \(\) without having to retrain anything.

\(p\)-quantile QGFNHere, we define \(\) as a masked version of \(P_{F}\), where actions below the \(p\)-quantile of \(Q\), denoted \(q_{p}(Q,s)\), have probability \(0\) (so are discarded):

\[(s^{}|s) P_{F}(s^{}|s)[Q(s,s^{}) q _{p}(Q,s)]\] (4)

This can be implemented by sorting \(Q\) and masking the logits of \(P_{F}\) accordingly. This method is more aggressive, since it prunes the search space, potentially making some states unreachable. Again, \(p\) is changeable.

\(p\)-of-max QGFNHere, we define \(\) as a masked version of \(P_{F}\), where actions with \(Q\)-values less than \(p_{a}Q(s,a)\) have probability \(0\):

\[(s^{}|s) P_{F}(s^{}|s)[Q(s,s^{}) p _{i}Q(s,i)]\] (5)

This is similar to \(p\)-quantile pruning, but the number of pruned actions changes as a function of \(Q\). If all actions are estimated as good enough, it may be that no action is pruned, and vice versa, only the best action may be retained is none of the others are good. This method also prunes the search space, and \(p\) remains changeable. Note that in a GFN context, rewards are strictly positive, so \(Q\) is also positive.

Policy evaluation, or optimal control?In the design of the proposed method, we were faced with an interesting choice: what policy \(\) should \(Q^{}\) evaluate? The first obvious choice is to perform \(Q\)-learning (Mnih et al., 2013), and estimate the optimal value function \(Q^{}\), with a 1-step TD objective. As we detail in \(@sectionsign\)6, this proved to be fairly hard, and 1-step \(Q_{}\) ended up being a poor approximation.

A commonly used trick to improve the performance of bootstrapping algorithms is to use \(n\)-step returns (Hessel et al., 2018). This proved essential to our work, and also revealed something curious:we've consistently found that, while results started improving at \(n 5\), a consistently good value of \(n\) was close to the maximum trajectory length. This has an interesting interpretation, as beyond a certain value of \(n\), \(Q_{}\) becomes closer to \(Q^{}\) and further from \(Q^{*}\). In other words, using an "on-policy" estimate \(Q^{}\) rather than an estimate of the optimal policy seems beneficial, or at least easier to learn as a whole. In hindsight, this makes sense because on-policy evaluation is easier than learning \(Q^{*}\), and since we are combining the \(Q\)-values with \(P_{F}\), any method which biases \(\) correctly towards better actions is sufficient (we do not need to know exactly the best action, or its exact value).

Selecting greedinessIn the methods proposed above, \(p\) can be changed arbitrarily. We first note that we train with a constant or annealed2 value of \(p\) and treat it as a standard hyperparameter in all the results reported in SS5.

Second, as discussed in SS6, after training, \(p\) can be changed with a predictable effect: the closer \(p\) is to 1, the greedier \(\) becomes. Presumably, this is because the model generalizes, and \(Q\)-value estimates for "off-policy" actions are still a reliable guess of the reward obtainable down some particular branch. When making \(p\) higher, \(Q\) may remain a good _lower bound_ of the expected reward (after all, \(\) is becoming greedier), which is still helpful. Generally, such a policy will have reasonable outcomes, regardless of the specific \(\) and \(p\) used during training. Finally, it may be possible and desirable to use more complex schedules for \(p\), or to sample \(p\) during training from some (adaptive) distribution, but we leave this for future work.

## 5 Main results

We experiment on 5 standard tasks used in prior GFlowNet literature. As baselines, we use Trajectory Balance, Sub-Trajectory Balance, LSL-GFN (Kim et al., 2023) i.e. _learning to scale logits_ which controls greediness through temperature-conditioning, and as RL baselines A2C (Mnih et al., 2016), DQN (Mnih et al., 2013) (which on its own systematically underperforms in these tasks), and Tiapkin et al. (2023)'s MunDQN/GFlowNet.

We report the average reward obtained by the agents, as well as the total number of modes of the distribution of interest found during training. By _mode_, we mean a high-reward object that is separated from previously found modes by some distance threshold. The distance function and

Figure 3: QM9 task. _Left:_ Average rewards over training trajectories. _Right:_ Number of modes with a reward above 1.10 and pairwise Tanimoto similarity less than 0.70.

Figure 2: Fragment-based molecule task. _Left:_ Average rewards over the training trajectories. _Center:_ Number of unique modes with a reward threshold exceeding 0.97 and pairwise Tanimoto similarity score less than 0.65. _Right:_ Average pairwise Tanimoto similarity score for the top 1000 molecules sampled by reward. Lines are the interquartile mean and standard error calculated over 5 seeds.

threshold we use, as well as the minimum reward threshold for an object to be considered a mode, depend on the task.

**Fragment-based molecule generation task:3** Generate a graph of up to 9 fragments, where the reward is based on a prediction of the binding affinity to the sEH protein, using a model provided by Bengio et al. (2021). \(|| 10^{100}\), there are 72 available fragments, some with many possible attachment points. We use Tanimoto similarity (Bender and Glen, 2004), with a threshold of 0.65, and a reward threshold of 0.97. Results are shown in Fig. 2.

**Atom Based QM9 task:** Generate small molecules of up to 9 atoms following the QM9 dataset (Ramakrishnan et al., 2014). \(|| 10^{12}\), the action space includes adding atoms or bonds, setting node or bond properties and stop. A MXMNet proxy model (Zhang et al., 2020), trained on QM9, predicts the HOMO-LUMO gap, a key indicator of molecular properties including stability and reactivity, and is used as the reward. Rewards are in the \(\) range, with a \(1.10\) threshold and a minimum Tanimoto similarity of \(0.70\) to define modes. Results are shown in Fig. 3.

**RNA-binding task:** Generate a string of 14 nucleobases. The reward is a predicted binding affinity to the target transcription factor, provided by the ViennaRNA (Lorenz et al., 2011) package for the binding landscapes; we experiment with two RNA binding tasks; L14-RNA1, and L14-RNA1+2 (two binding targets) with optima computed from Sinai et al. (2020). \(||\) is \(4^{14} 10^{9}\), there are 4 tokens: adenine (A), cytosine (C), guanine (G), uracil (U). Results are shown in Fig. 4.

**Prepend-Append bit sequences:** Generate a bit sequence of length 120 in a prepend-append MDP, where \(||\), limited to \(\{0,1\}^{n}\), is \(2^{120} 10^{36}\). For a sequence of length \(n\), \(R(x)=d(x,y)/n)}\). A sequence is considered a mode if it is within edit distance \(\) from \(M\), where \(M\) is defined as per Malkin et al. (2022) (although the task we consider here is a more complex version, introduced by Shen et al. (2023), since prepend actions induce a DAG instead of a simpler tree). In our experiment, \(|M|=60,n=120,k=1,=28\), where \(k\) is the bit width of actions. Results are shown in Fig. 5.

### Analysis of results

Across tasks, QGFN variants produce high rewards _and_ find a higher number of modes, i.e. high-reward dissimilar objects. The latter could seem surprising, because a priori, increasing the greediness of a method likely reduces its diversity. This fundamental trade-off is known in RL as the exploration-exploitation dilemma (Sutton, 1988; Sutton and Barto, 2018). However, we are leveraging two methods and combining their strengths to reap the best from both worlds: GFlowNets are able to _cover_ the state space, because they attempt to model all of it, by learning \(p_{}(x) R(x)\), while \(Q\) approximates the expected reward of a particular action, which can help guide the agent by selecting

Figure 4: RNA-binding tasks, Average reward and modes. _Left_: L14RNA1 task. _Right_: L14RNA1+2 task, based on 5 seeds (interquartile mean and standard error shown).

Figure 5: Bit sequence task, \(k=1\). Interquartile mean and standard error over 5 seeds.

high-expected-reward branches. Another way to think about this: GFNs are able to estimate _how many_ high-reward objects there are in different parts of the state space. The agent thus ends up going in all important regions of the state space, but by being a bit more greedy through \(Q\), it focuses on higher reward objects, so it is more likely to find objects with reward past the mode threshold. To further understand the performance of QGFN, we formally analyse a bandit setting, and include derivations to illustrate the general case, in Appendix SSA.

We also report the average reward and pairwise similarity for the fragment task based on 1000 samples over 5 seeds taken after training in Table 1. Again, QGFNs outperform GFNs in reward, while retaining low levels of inter-sample similarity. We again note that at inference, we are able to use a different (and better) \(p\) value than the one used at training time. We expand on this in SS6, and show that it is easy to tune \(p\) to achieve different reward-diversity trade-offs at inference. The exact \(p\) values used for Table 1 are provided in Appendix SSE. Also note that in LSL-GFN \(\) is tunable at inference, and in Table 1 we choose the \(\) value such that average similarity is near the 0.65 mode threshold we use (choosing a greedier \(\) induces a collapse in diversity).

QGFN variants matterWe point the reader to an interesting result, which is consistent with our understanding of the proposed method. In the fragment task, the number of actions available to the agent is quite large, ranging from about 100 to 1000 actions depending on the state, and the best performing QGFN variant is one that consistently masks most actions: \(p\)-quantile QGFN. It is likely indeed that most actions are harmful, as combining two fragments that do not go together may be irreversibly bad, and masking helps the agent avoid undesirable regions of the state space. However, masking a fixed ratio of actions can provide more stable training.

On the other hand, in the RNA design task, there are only 5 actions (4 nucleotides \(\) & stop). We find that masking a constant _number_ of actions is harmful-it is likely that in some states all of them are relevant. So, in that task, \(p\)-greedy and \(p\)-of-max QGFN work best. This is also the case in the bit sequence task, for the same reasons (see Fig. 5). To confirm this, we repeat the bit sequence task but with an expanded action space consisting not just of \(\{0,1\}\), but of all 16 (\(2^{4}\)) sequences of 4 bits, i.e. \(\{0000,0001,..,1111\}\). We find, as shown in Fig \(18\), that \(p\)-quantile indeed no longer underperforms.

## 6 Method Analysis

We now analyze the key design choices in QGFN. We start by investigating the impact of \(n\) (the number of bootstrapping steps in \(Q\)-Learning) and \(p\) (the mixture parameter) on _training_. We then look at trained models, and reuse the learned \(Q\) and \(P_{F}\) to show that it is possible to use a variety of sampling strategies, and to change the mixture factor \(p\) to obtain a spectrum of greediness at test time. Finally, we empirically probe models to provide evidence as to _why_ QGFN is helpful.

Impact of \(\) in QGFNFig. \(6\) shows the effect of training with different \(\) values on the average reward and number of modes when taking 1000 samples after training is done in the fragment task (over 5 seeds). As predicted, increasing \(\) increases the average reward of the agent, but at some point, causes it to become dramatically less diverse. As discussed earlier, this is typical of GFNs with a too high \(\), and is caused by a collapse around high-reward points and an inability for the model to further explore. While QGFN is also affected by this, it does not require as drastic values of \(\) to obtain a high average reward and discover a high number of modes.

Impact of \(n\) in QGFNAs mentioned in SS4, training \(Q\) with 1-step returns is ineffective and produces less useful approximations of the action value. Fig. \(6\) shows the number of modes within 1000 post-training samples in the fragment tasks, for models trained with a variety of \(n\)-step values.

   Method & Reward(\(\)) & Similarity(\(\)) \\  GFN-TB & 0.780\(\)0.003 & 0.545\(\)0.002 \\ GFN-SubTB & 0.716\(\)0.006 & 0.513\(\)0.003 \\ LSL-GFN & 0.717\(\)0.020 & 0.689\(\)0.062 \\ p-greedy QGFN & 0.950\(\)0.004 & 0.551\(\)0.015 \\ p-of-max QGFN & **0.969\(\)0.003** & 0.514\(\)0.001 \\ p-quantile QGFN & 0.955\(\)0.003 & **0.509\(\)0.008** \\   

Table 1: Fragment-based molecule task: Reward and Diversity at inference after training.

Models start being consistently good at \(n=5\) and values close to the maximum length of a trajectory tend to work well too.

Impact of the training \(p\) in QgrnWhile our method allows changing \(p\) more or less at will, we still require some value during training. Fig. 6 shows that there are clear trade-offs between choices of \(p\), some yielding significantly better diversity than others. For example, \(p\)-of-max is fairly sensitive to the chosen value during training, and for the fragment task doesn't seem to perform particularly well during training (especially when not annealed). On the other hand, as we will see in the next paragraph (and is also seen in Fig. 6), \(p\)-of-max excels at inference, and is able to generate diverse and high-reward samples by adjusting \(p\).

Changing strategy after trainingWe now look at the impact of changing the mixture parameter \(p\) and the sampling strategy for already trained models on average reward and average pairwise similarity. We use the parameters of a model trained with \(p\)-greedy QGFN, \(p=0.4\).

With this model, we sample 512 new trajectories for a series of different \(p\) values. For \(p\)-greedy and \(p\)-quantile, we vary \(p\) between \(0\) and \(1\); for \(p\)-of-max, we vary \(p\) between \(.9\) and \(1\) (values below \(.9\) have minor effects). We visualize the effect of \(p\) on reward and similarity statistics in Fig. 9.

First, we note that increasing \(p\) has the effect we would hope, increasing the average reward. Second, we note that this works without any retraining; even though we (a) use values of \(p\) different than those used during training, and (b) use QGFN variants different than those used during training, the behavior is consistent: \(p\) controls greediness. Let us emphasize (b): even though we trained this \(Q\) with \(p\)-greedy QGFN, we are able to use the \(Q(s,a)\) predictions just fine with entirely different sampling strategies. This has some interesting implications; most importantly, it can be undesirable to _train_ with too high values of \(p\) (because it may reduce the diversity to which the model is exposed), but what is learned transfers well to sampling new, high-reward objects with different values of \(p\) and sampling strategies.

Finally, these results suggest that we should be able to prototype new QGFN variants, including expensive ones (e.g. MCTS) without having to retrain anything. We illustrate the performance of a few other variants in SSB.2, Fig. 12.

Is \(Q\) calibrated?For intuition of why QGFN works to really pan out, \(Q\) has to be accurate enough to provide useful guidance towards high-reward objects. We verify that this is the case with the following experiment. We take a trained QGFN model (\(p\)-greedy, \(p=0.4\), fragment task, maximum \(n\)-step) and sample \(64\) trajectories. For each of those trajectories, we take a random state within the trajectory as a starting point, thereafter generating \(512\) new trajectories. We then use the reward of those \(512\) trajectories as an empirical estimate \(\) of the expected

Figure 6: Fragment task. _Left_: Effect of \(\): Increasing greediness through \(\) increases the average reward but may lead to diversity collapse. QGFN maintains diversity with a lower \(\), while GFN collapses. Modes are counted from 1000 samples at inference, using an inference-adjusted \(p\). _Right_: Effect of training parameters \(p\), and \(n\): Changing \(p\) can control greediness, while increasing \(n\) is generally beneficial. Modes are counted from 1000 samples generated using the training \(p\).

Figure 7: Comparing \(Q(s,a;)\) predictions with empirical estimates obtained by rollouts. Bars are standard error. \(Q\) is relatively capable to estimate the returns of the corresponding policy.

return, which \(Q\) should roughly be predicting. Fig. 7 shows that this is indeed the case. Although \(Q\) is not perfect, and appears to be underestimating \(\), it is making reasonable predictions.

**Is \(Q\) really helpful?** In this experiment, we verify our intuition that pruning based on \(Q\)-values is helpful. We again take a trained model for the fragment task, and sample \(512\) trajectories. We use \(p\)-of-max QGFN (\(p=0.95\)), and compare it to a strategy estimating _best pruned actions_: for each trajectory, after some random number of steps \(t U\) (the max is 27), we start deterministically selecting the action that is the most probable according to \(P_{F}\) but would be masked according to \(Q\). To ensure that this is a valid thing to do, we also simply look at _Best actions_, i.e. after \(t U\) steps, deterministically select the action that is the most probable according to \(P_{F}\), regardless of \(Q\).

Fig. 8 shows that our sanity check, _Best actions_, receives reasonable rewards, while selecting actions that would have been pruned leads to much lower rewards. The average likelihood from \(P_{F}\) of these pruned actions was \(.035\), while the average number of total actions was \( 382\) (and \(1/382 0.0026\)). This confirms our hypothesis that \(Q\) indeed masks actions that are likely according to \(P_{F}\) but that do **not** consistently lead to high rewards.

**Why does changing \(p\) work?** Recall that for QGFN to be successful, we rely on \(n\)-step TD, and therefore on somewhat "on-policy" estimates of \(Q^{}\). \(\) is really \(_{p}\), a function of \(p\), meaning that if we change \(p\), say to \(p^{}\), during inference, \(Q^{_{p}}\) is not an accurate estimate of \(Q^{_{p^{}}}\). If this is the case, then there must be a reason why it is still helpful to prune based on \(Q^{_{p}}\) while using \(_{p^{}}\). In Fig. 10,we perform the same measurement as in Fig. 7, but we change the \(p\) value used to measure \(^{_{p^{}}}\). We find that, while the rank correlation drastically goes down (although it stays well above 0), \(Q^{_{p}}\) remains helpful because it _lower bounds_\(^{_{p^{}}}\). If we prune based on \(Q^{_{p}}\), then we would want it to not lead us astray, and _at least_ make us greater as we increase \(p\). This means that if an action is not pruned, then we expect samples coming from it to be _at least as good_ as what \(Q^{_{p}}(s,a)\) predicts (in expectation). This is indeed the case.

Note that reducing \(p\) simply leads \(\) to behave more like \(P_{F}\), which is still a good sampler, and to rely less on \(Q\), whose imperfections will then have less effect anyways.

## 7 Conclusion and Discussion

In this paper, we showed that by jointly training GFNs and \(Q\)-functions, we can combine their predictions to form behavior policies that are able to sample larger numbers of diverse and high-reward objects. These policies' mixture parameter \(p\) is adjustable, even after training, to modulate the greediness of the resulting policy. We implement multiple ways of combining GFNs and \(Q\)s, referring to the general idea as QGFN: taking a greedy with probability \(p\) (\(p\)-greedy QGFN), restricting the agent to the top \(1-p\)% of actions (\(p\)-quantile QGFN), and restricting the agent to actions whose estimated value is at least a fraction \(p\) of the best possible value (\(p\)-of-max QGFN).

We chose to show several variants of QGFN in this paper, because they all rely on the same principle, learning \(Q\), but have different properties, which lead to better or worse behavior in different tasks.

Figure 8: Pruning helps avoid low-reward parts of the state space. Reward distributions when (a) sampling with \(p\)-of-max; (b) greedily according to \(P_{F}\) selecting actions that \(p\)-of-max would prune, _Best pruned actions_; (c) selecting most likely \(P_{F}\) actions regardless of \(Q\), _Best actions_; and (d) normal sampling from \(P_{F}\) (without using \(Q\)).

Figure 9: Varying \(p\) at inference time induces reward-diversity trade-offs; fragment task.

For example, pruning too aggressively on a task with a small number of actions is harmful. We also hope that by showing such a diversity of combinations of \(P_{F}\) and \(Q\), we encourage future work that combines GFNs and RL methods in novel and creative ways.

We also analyzed why our method works. We showed that the learned action-value \(Q\) is predictive and helpful in avoiding actions that have high probability under \(P_{F}\) but lower expected reward. Even when \(Q\) predictions are not accurate, e.g. because we sample from a different policy than the one which \(Q\) models, they provide a helpful lower bound that facilitates controllable greediness.

Our analysis suggests that at training time, QGFN works because it helps the agent to "waste" less time and capacity modeling low-reward objects, and that conversely the policy family that QGFN learns is able to sample more distinct high-reward objects given the same budget. In this sense, QGFN benefits from the advantages of both the GFlowNet and "Hard"-RL frameworks.

What didn't workThe initial stages of this project were quite different. Instead of combining RL and GFNs into one sampling policy, we instead trained two agents, a GFN and a DQN. Since both are off-policy methods we were hoping that sharing "greedy" DQN data with a GFN would be fine and make GFN better on high-reward trajectories. This was not the case, instead, the DQN agent simply slowed down the whole method-despite trying a wide variety of tricks, see \(\).

LimitationsBecause we train two models, our method requires more memory and FLOPs, and consequently takes more time to train compared to TB (as shown in Table 3). QGFN is also sensitive to how well \(Q\) is learned, and as we've shown \(n\)-step returns are crucial for our method to work. In addition, although the problems we tackle are non-trivial, we do not explore the parameter and compute scaling behaviors of the benchmarked methods.

Future workWe highlight two straightforward avenues of future work. First, there probably exist more interesting combinations of \(Q\) and \(P_{F}\) (and perhaps \(F\)), with different properties and benefits. Second, it may be interesting to further leverage the idea of pruning the action space based on \(Q\), forming the basis for some sort of constrained combinatorial optimization. By using \(Q\) to predict some expected property or constraint, rather than reward, we could prune some of the action space to avoid violating constraints, or to keep some other properties below some threshold (e.g. synthesizability or toxicity in molecules).

Finally, we hope that this work helps highlight the differences between RL and GFlowNet, while adding to the literature showing that these approaches complement each other well. It is likely that we are only scratching the surface of what is possible in combining these two frameworks.

## Author Contributions

The majority of the experimental work, code, plotting, and scientific contributions were by EL, with support from EB. SL helped run some experiments, baselines and plots. The project was supervised by EB, and DP and LP provided additional scientific guidance. Most of the paper was written by EB. DP, LP, and EL contributed to editing the paper.