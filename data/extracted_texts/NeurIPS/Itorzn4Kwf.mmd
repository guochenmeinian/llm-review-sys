# Accelerating Exploration with Unlabeled Prior Data

Qiyang Li\({}^{}\), Jason Zhang\({}^{}\), Dibya Ghosh\({}^{}\), Amy Zhang\({}^{}\), Sergey Levine\({}^{}\)

UC Berkeley\({}^{}\), UT Austin\({}^{}\), Meta\({}^{}\)

{qcli,jason.z,dibya.ghosh}@berkeley.edu

amy.zhang@austin.utexas.edu, svelevine@eecs.berkeley.edu

Equal Contribution

###### Abstract

Learning to solve tasks from a sparse reward signal is a major challenge for standard reinforcement learning (RL) algorithms. However, in the real world, agents rarely need to solve sparse reward tasks entirely from scratch. More often, we might possess prior experience to draw on that provides considerable guidance about which actions and outcomes are possible in the world, which we can use to explore more effectively for new tasks. In this work, we study how prior data without reward labels may be used to guide and accelerate exploration for an agent solving a new sparse reward task. We propose a simple approach that learns a reward model from online experience, labels the unlabeled prior data with optimistic rewards, and then uses it concurrently alongside the online data for downstream policy and critic optimization. This general formula leads to rapid exploration in several challenging sparse-reward domains where tabula rasa exploration is insufficient, including the AntMaze domain, Adroit hand manipulation domain, and a visual simulated robotic manipulation domain. Our results highlight the ease of incorporating unlabeled prior data into existing online RL algorithms, and the (perhaps surprising) effectiveness of doing so.

## 1 Introduction

Exploration, particularly in sparse reward environments, presents a major challenge for reinforcement learning, and standard exploration methods typically need to seek out all potentially novel states to cover all the places where high rewards may be located. Luckily, in many real-world RL settings, it is straightforward to obtain prior data that can help the agent understand how the world works. For example, if we are trying to find where we left our keys, we would not relearn how to navigate our environment, but rather might revisit locations that we recall from memory. If the data has reward annotations, pre-training with offline RL provides one solution to accelerate online finetuning. However, in many domains of interest, it is more likely for prior datasets to be task-agnostic, and not be labeled with the reward for the new task we hope to solve. Despite not having reward labels, such prior data can be useful for an agent exploring for a new task since it describes the environment dynamics and indicates regions of the state space that may be interesting to explore.

Figure 1: **How to leverage unlabeled prior data for efficient online exploration? We label the prior data with an optimistic reward estimate and run RL on both the online and offline data. This allows more efficient exploration around the trajectories in the prior data, improving sample efficiency especially for hard exploration tasks.**How can this unlabeled data be utilized by a deep RL algorithm? Prior data informs the agent about potentially high-reward or high-information states, and behaviors to reach them. Initially, the agent may act to reach these states, leading to directed exploration in a more informative subspace rather than the complete state space. This should be an iterative process, since the agent can use the new online experience to refine what directions from the prior data it chooses to explore in. After the agent begins to receive reward signals, the prior data can help the agent to more reliably reach and exploit the reward signals, steering the exploration towards a more promising region.

Perhaps surprisingly, such informed exploration can be elicited in a very simple way: by labeling the prior data with _optimistic_ rewards and adding it to the online experience of an off-policy RL algorithm (Figure 1). Specifically, we choose to impute the missing reward on the unlabeled prior data with an optimistic upper-confidence bound (UCB) of the reward estimates obtained through the agent's collected online experience. Early in training, these optimistic reward labels will be high for states from the prior data, and optimizing the RL objective with this data will elicit policies that attempt to reach regions of state present in the prior data. As states in the prior data are visited, the labeled reward for these states will regress towards their ground-truth values, leading to a policy that either explores other states from the prior data (when true reward is low) or returns to the region more consistently (when high). By training an RL policy with a UCB estimate of the reward, exploration is guided by both task rewards and prior data, focusing only on visiting states from the prior data that may have high reward. In practice, we find that this general recipe works with different off-policy algorithms (Ball et al., 2023; Kostrikov et al., 2021).

Our main contribution is a simple approach to leverage unlabeled prior data to improve online exploration and sample efficiency, particularly for sparse long-horizon tasks. Our empirical evaluations, conducted over domains with different observation modalities (e.g., states and images), such as simulated robot navigation and arm and hand manipulation, show that our simple optimistic reward labeling strategy can utilize the unlabeled prior data effectively, often as well as the prior best approach that has access to the same prior data with _labeled rewards_. In addition, we can leverage offline pre-training of task-agnostic value functions (e.g., ICCVF (Ghosh et al., 2023)) from offline data to initialize our optimistic reward model to further boost the online sample efficiency in more complex settings, such as image-based environments. In contrast to other approaches in the same setting, our method is conceptually simpler and easier to be integrated into current off-policy online RL algorithms, and performs better as we will show in our experiments.

## 2 Problem Formulation

We formalize our problem in an infinite-horizon Markov decision process (MDP), defined as \(=(,,,r,,)\). It consists of a state space \(\), an action space \(\), transition dynamics \(:(S)\), a reward function \(r:\), a discount factor \((0,1]\), an initial state distribution of interest \(\). Algorithms interact in the MDP online to learn policies \((a|s):()\) that maximize the discounted return under the policy \(()=_{s_{t+1}(|s_{t},a_{t}),a_{t}(| s_{t}),s_{0}}[_{t=0}^{}^{t}r(s_{t},a_{t})]\). We presume access to a dataset of transitions with _no reward labels_, \(=\{(s_{i},a_{i},s^{}_{i})\}_{i}\) that was previously collected from \(\), for example by a random policy, human demonstrator, or an agent solving a different task in the same environment. The core challenge in this problem setting is to explore in a sparse reward environment where you do not know where the reward is. While structurally similar to offline RL or online RL with prior data, where the agent receives a reward-labelled dataset, the reward-less setting poses a significant exploration challenge not present when reward labels are available. Typically offline RL algorithms must find the best way to use the prior data to _exploit_ the reward signal. In our setting, algorithms must use the prior data to improve _exploration_ to acquire a reward signal. Using this prior data requires bridging the misalignment between the offline prior dataset (no reward) and the online collected data (contains reward), since standard model-free RL algorithms cannot directly use experiential data that has no reward labels.

## 3 Exploration from Prior Data by Labeling Optimistic Reward (Explore)

In this section, we detail a simple way to use the unlabeled prior data directly in a standard online RL algorithm. Our general approach will be to label the reward-free data with learned optimistic rewards and include this generated experience into the buffer of an off-policy algorithm. When these rewards are correctly chosen, this will encourage the RL agent to explore along directions that were present in the prior data, focusing on states perceived to have high rewards or not yet seen in the agent's online experience.

```
1:Input: prior unlabeled data \(_{}\)
2:Initialize the UCB estimate of the reward function: \((s,a)\)
3:Online replay buffer \(\)
4:Initialize off-policy RL algorithm with a policy \(\).
5:for each environment step do
6: Collect \((s,a,s^{},r)\) using the policy \(\) and add it to the replay buffer \(\).
7: Update the UCB estimate \((s,a)\) using the new transition \((s,a,s^{},r)\)
8: Relabel each transition \((s,a,s^{})\) in \(_{}\) with \(=(s,a)\)
9: Run off-policy RL update on both \(\) and the relabeled \(_{}\) to improve the policy \(\).
10:endfor ```

**Algorithm 1**ExPLORe

General algorithm.Since the prior data contains no reward labels, the agent can only acquire information about the reward function from the online experience it collects. Specifically, at any stage of training, the collected online experience \(\) informs some posterior or confidence set over the true reward function that it must optimize. To make use of the prior data, we label it with an upper confidence bound (UCB) estimate of the true reward function. Optimizing an RL algorithm with this relabeled prior experience results in a policy that is "optimistic in the face of uncertainty", choosing to guide its exploration towards prior data states either where it knows the reward to be high, and prior data states where it is highly uncertain about the reward. Unlike standard exploration with optimistic rewards for states seen online, the prior data may include states that are further away than those seen online, or may exclude states that are less important. Consequently, the RL agent seeks to visit regions in the prior data that are promising or has not visited before, accelerating online learning.

To explore with prior data, our algorithm maintains an uncertainty model of the reward, to be used for UCB estimates, and an off-policy agent that trains jointly on relabelled prior data and online experience. As new experience is added through online data collection, we use this data to update the agent's uncertainty model over rewards. When training the off-policy agent, we sample data from both the online replay buffer (already reward labeled) and from the prior data (no reward labels). The prior data is labelled with rewards from the UCB estimate of our uncertainty model, and then the online and prior data is jointly trained upon. Training with this data leads to a policy that acts optimistically with respect to the agent's uncertainty about the reward function, which we use to guide exploration and collect new transitions. Algorithm 1 presents a sketch of our method.

Notice that while this strategy is similar to exploration algorithms that use novelty-based reward bonuses (Bellemare et al., 2016; Tang et al., 2017; Pathak et al., 2017; Burda et al., 2018; Pathak et al., 2019; Gupta et al., 2022), since the optimism is being added to the _prior data_, the resulting exploration behaviors differ qualitatively. Adding novelty bonuses to online experience guides the agent to states it has already reached in the online phase, whereas adding optimism to the prior experience encourages the agent to learn to reach new states beyond the regions it has visited through online exploration. Intuitively, while standard optimism leads to exploration along the frontier of online data, optimism on prior data leads to exploration _beyond_ the frontier of online data.

Practical implementation.In practice, we obtain UCB estimates of the rewards using a combination of a reward model and random-network-distillation (RND) (Burda et al., 2018), a novelty-based reward bonus technique. The former provides a reward estimate of the true reward value and the latter (RND) quantifies the uncertainty of the estimate, allowing us to get a UCB reward estimate.

For the reward model, we update a randomly initialized network \(r_{}(s,a)\) simultaneously with the RL agent on the online replay buffer by minimizing the squared reward loss:

\[()=_{(s,a,r)}[(r_{}(s,a )-r)^{2}].\]

RND randomly initializes two networks \(f_{}(s,a),(s,a)\) that each output an \(L\)-dimensional feature vector for each state and action. We keep one network, \(\), frozen and update the parameters of the other network \(\) to minimize the mean-square-error between the predicted features and the frozen ones once on every new transition \((s_{},a_{},s^{}_{},r_{})\) encountered:

\[()=\|f_{}(s_{},a_{})-(s _{},a_{})\|_{2}^{2}.\]

During online exploration, \(\|f_{}(s,a)-(s,a)\|_{2}^{2}\) forms a measure of the agent's uncertainty about \((s,a)\), leading us to the following approximate UCB estimate of the reward function

\[(s,a) r_{}(s,a)+\|f_{}(s,a)-(s,a)\|_{2}^{2}.\]

For the RL agent, we use RLPD (Ball et al., 2023), which is specifically designed for efficient online RL and performs learning updates on both online and labeled offline data. It is worth noting that our method is compatible with potentially any other off-policy RL algorithms that can leverage labeled online and offline data. We experiment with an alternative based on implicit-Q learning in Appendix C.4. Although we only have access to unlabeled data in our setting, we constantly label the prior data with UCB reward estimates, enabling a direct application of RLPD updates.

Figure 3: Aggregated results for 6 AntMaze tasks, 3 Adroit tasks and 3 COG tasks. **Ours** is the only method that largely recovers the performance of vanilla RLPD (**Oracle**), which has access to _true_ rewards (**Ours**_does not_). The baselines that do not use the prior unlabeled data (**Online** and **Online + RND**) perform poorly on both AntMaze and Adroit. Naively labeling the prior data without optimism (**Naive**) performs competitively on Adroit but poorly on AntMaze. Section 5.2 contains the description for the baselines we compare with.

Figure 2: **Visualizations of the agent’s exploration behaviors using our optimistic reward labeling strategy on antmaze-medium**. The dots shown (500 points each) are sampled uniformly from the online replay buffer (up to 60K environment steps), and colored by the training environment step at which they were added to the buffer. Both **Online** and **Online + RND** do not use prior data, and the latter uses RND reward bonus (Burda et al., 2018) on top of the received online reward to encourage online exploration. **Naive** learns a reward model and relabels the unlabeled prior data with its reward estimate with no optimism. **Ours** labels the prior data with an optimistic reward estimate.

Related Work

Exploration with reward bonuses.One standard approach for targeted exploration is to add exploration bonuses to the reward signal and train value functions and policies with respect to these optimistic rewards. Exploration bonuses seek to reward novelty, for example using an approximate density model (Bellemare et al., 2016; Tang et al., 2017), curiosity (Pathak et al., 2017, 2019), model error (Stadie et al., 2015; Houthooft et al., 2016; Achiam and Sastry, 2017; Lin and Jabri, 2023), or even prediction error to a randomly initialized function (Burda et al., 2018). When optimistic rewards are calibrated correctly, the formal counterparts to these algorithms (Strehl and Littman, 2008; Dann et al., 2017) enjoy strong theoretical guarantees for minimizing regret (Azar et al., 2017; Jin et al., 2018). Prior works typically augment the reward bonuses to the online replay buffer to encourage exploration of novel states, whereas in our work, we add reward bonuses to the offline data, encouraging the RL agent to traverse through trajectories in the prior data to explore more efficiently.

Using prior data without reward labels.Prior experience without reward annotations can be used for downstream RL in a variety of ways. One strategy is to use this data to pre-train state representations and extract feature backbones for downstream networks, whether using standard image-level objectives (Srinivas et al., 2020; Xiao et al., 2022; Radosavovic et al., 2022), or those that emphasize the temporal structure of the environment (Yang and Nachum, 2021; Ma et al., 2022; Farebrother et al., 2023; Ghosh et al., 2023). These approaches are complementary to our method, and in our experiments, we find that optimistic reward labeling using pre-trained representations lead to more coherent exploration and faster learning. Prior data may also inform behavioral priors or skills (Ajay et al., 2020; Tirumala et al., 2020; Pertsch et al., 2021; Nasiriany et al., 2022); once learned, these learned behaviors can provide a bootstrap for exploration (Uchendu et al., 2022; Li et al., 2023), or to transform the agent's action space to a higher level of abstraction (Pertsch et al., 2020; Singh et al., 2020). In addition, if the prior data consists of optimal demonstrations for the downstream task, there exists a rich literature for steering online RL with demonstrations (Schaal, 1996; Vecerik et al., 2017; Nair et al., 2017; Zhu et al., 2020). Finally, for model-based RL approaches, one in theory may additionally train a dynamics model on prior data and use it to plan and reach novel states (Sekar et al., 2020; Mendonca et al., 2021) for improved exploration, but we are not aware of works that explicitly use models to explore from reward-free offline prior data.

Using prior data with reward labels.To train on unlabeled prior data with optimistic reward labels, we use RLPD (Ball et al., 2023), a sample-efficient online RL algorithm designed to train simultaneously on reward-labeled offline data and incoming online experience. RLPD is an offline-to-online algorithm (Xie et al., 2021; Lee et al., 2021; Agarwal et al., 2022; Zheng et al., 2023; Hao et al., 2023; Song et al., 2023) seeking to accelerate online RL using reward-labeled prior experience. While this setting shares many challenges with our unlabeled prior data, algorithms in this setting tend to focus on correcting errors in value function stemming from training on offline data (Nakamoto et al., 2023), and correctly balancing between using highly off-policy transitions from the prior data and new online experience (Lee et al., 2021; Luo et al., 2023). Our optimistic reward labeling mechanism is largely orthogonal to these axes, since our approach is compatible with any online RL algorithm that can train on prior data.

Online unsupervised RL.Online unsupervised RL is an adjacent setting in which agents collect action-free data to accelerate finetuning on some downstream task (Laskin et al., 2021). While both deal with reward-free learning, they crucially differ in motivation: our setting asks how existing (pre-collected) data can be used to inform downstream exploration for a known task, while unsupervised RL studies how to actively collect new datasets for a (yet to be specified) task (Yarats et al., 2022; Brandfonbrenner et al., 2022). Methods for unsupervised RL seek broad and undirected exploration strategies, for example through information gain (Sekar et al., 2020; Liu and Abbeel, 2021) or state entropy maximization (Lee et al., 2019; Jin et al., 2020; Wang et al., 2020) - instead of the task-relevant exploration that is advantageous in our setting. Successful mechanisms developed for unsupervised RL can inform useful strategies for our setting, since both seek to explore in directed ways, whether undirected as in unsupervised RL or guided by prior data, as in ours.

Offline meta reinforcement learning.Offline meta-reinforcement learning (Dorfman et al., 2021; Pong et al., 2021; Lin et al., 2022) also seeks to use prior datasets from a task distribution to accelerate learning in new tasks sampled from the same distribution. These approaches generally tend to focus on the single-shot or few-shot setting, with methods that attempt to (approximately) solve the Bayesian exploration POMDP (Duan et al., 2016; Zintgraf et al., 2019; Rakelly et al., 2019). While these methods are efficient when few-shot adaptation is possible, they struggle in scenarios requiring more complex exploration or when the underlying task distribution lacks structure (Ajay et al., 2022; Mandi et al., 2022). Meta RL methods also rely on prior data containing meaningful, interesting tasks, while our optimistic relabeling strategy is agnostic to the prior task distribution.

## 5 Experiments

Our experiments are aimed at evaluating how effectively our method can improve exploration by leveraging prior data. Through extensive experiments, we will establish that our method is able to leverage the unlabeled prior data to boost online sample efficiency consistently, often matching the sample efficiency of the previous best approach that uses _labeled_ prior data (whereas we have no access to labeled rewards). More concretely, we will answer the following questions:

1. _Can we leverage unlabeled prior data to accelerate online learning?_
2. _Can representation learning help obtain better reward labels?_
3. _Does optimistic reward labeling help online learning by improving online exploration?_
4. _How robust is our method in handling different offline data corruptions?_

### Experimental Setup

The environments that we evaluate our methods on are all challenging sparse reward tasks, including six D4RL AntMaze tasks (Fu et al., 2020), three sparse-reward Adroit hand manipulation tasks (Nair et al., 2021) following the setup in RLPD (Ball et al., 2023), and three image-based robotic manipulation tasks used by COG (Singh et al., 2020). See Appendix B for details about each task. For all three domains, we explicitly remove the reward information from the offline dataset so that the agent must explore to solve the task. We choose these environments as they cover a range of different characteristics of the prior data, where each can present a unique exploration and learning challenge. In Adroit, the prior data consists of a few expert trajectories on the target task and a much larger set of data from a behavior cloning policy. In AntMaze, the prior data consists of goal-directed trajectories with varying start and end locations. In COG, the prior data is mostly demonstration trajectories for sub-tasks of a larger multi-stage task. These tasks are interesting for the following reasons:

**Sparse reward (All).** Due to the sparse reward nature of the these tasks, the agent has no knowledge of the task, it must explore to solve the task successfully. For example, on AntMaze tasks, the goal is for an ant robot to navigate in a maze to reach a fixed goal from a fixed starting point. The prior data for this task consists of sub-optimal trajectories, using starting points and goals potentially different from the current task. Without knowledge of the goal location, the agent must explore each corner of the maze to identify the actual goal location before solving the task is possible.

**High-dimensional image observation (COG).** The three COG tasks use image observations, which poses a much harder exploration challenge compared to state observations because of high dimensionality. As we will show, even with such high-dimensional observation spaces, our algorithm is still able to efficiently explore and accelerate online learning more than the baselines. More importantly, our algorithm can incorporate pre-training representations that further accelerates learning.

**Multi-stage exploration (COG).** The robotic manipulation tasks in COG have two stages, meaning that the robot needs to perform some sub-task (e.g., opening a drawer) before it is able to complete the full task (e.g., picking out a ball from the drawer). See Appendix B, Figure 9 for visualizations of images from the COG dataset showing the two stage structure.

The prior data consists only of trajectories performing individual stages by themselves, and there is no prior data of complete task executions. The agent must learn to explore past the first stage of the task without any reward information, before it can receive the reward for completing the full task. Therefore, this multi-stage experiment requires the agent to be able to effectively make use of incomplete trajectories to aid exploration. This setting is closer to the availability of the data one may encounter in the real world. For example, there may be a lot of data of robots performing tasks that are only partially related to the current task of interest. This data could also be unlabeled, as the rewards for other tasks may not match the current task. Despite being incomplete, we would still like to be able to utilize this data.

### Comparisons

While most prior works do not study the setting where the prior data is unlabeled, focusing on either the standard fully labeled data setting or utilizing an online unsupervised learning phase, we can adapt some of these prior works as comparisons in our setting (see Appendix A for details):

**Online**: A data-efficient off-policy RL agent that does _not_ make use of the prior data at all.

**Online + RND**: An augmentation of online RL with RND as a novelty bonus. This baseline uses only online data, augmenting the online batch with RND bonus (Burda et al., 2018). To be clear, of the comparisons listed here in bold, this is the only one that uses an online RND bonus.

**Naive Reward Labeling**: This comparison labels the prior data reward with an unbiased reward estimate; this is implemented using our method but omitting the RND novelty score.

**Naive + BC**: This baseline additionally uses a behavioral cloning loss to follow the behaviors as seen in prior data, inspired by similar regularization in offline RL (Vecerik et al., 2017).

**MinR**: This is an adaptation of UDS (Yu et al., 2022) to the online fine-tuning setting. The original UDS method uses unlabeled prior data to improve offline RL on a smaller reward-labeled dataset, using the minimum reward of the task to relabel the unlabeled data. MinR uses the same labeling strategy, but with RLPD for online RL.

**BC + JSRL**: This baseline is an adaptation of JSRL (Uchendu et al., 2022). The original JSRL method uses offline RL algorithms to pre-train a guide policy using a fully labeled prior dataset. Then, at each online episode, it uses the guide policy to roll out the trajectory up to a random number of steps, then switches to an exploration policy such that the initial state distribution of the exploration policy is shaped by the state visitation of the guide-policy, inducing faster learning of the exploration policy. Since in our setting the prior data has no labels, we use behavior cloning (BC) pre-training to initialize the guide-policy instead of using offline RL algorithms.

**Oracle**: This is an oracle baseline that assumes access to ground truth reward labels, using the same base off-policy RL algorithm (Ball et al., 2023) with true reward labels on prior data.

We also include a behavior prior baseline (inspired by the behavior prior learning line of work (Ajay et al., 2020; Tirumala et al., 2020; Pertsch et al., 2021; Nasiriany et al., 2022; Singh et al., 2020)) comparison in Appendix C.5 on AntMaze.

### Does optimistic labeling of prior data accelerate online learning?

Figure 3 shows the aggregated performance of our approach on the two state-based domains and one image-based domain. On AntMaze and COG domains, our optimistic reward labeling is able to outperform naive reward labeling significantly, highlighting its effectiveness in leveraging the prior data to accelerate online learning. Interestingly, in the two state-based domains, we show that without access to prior rewards, our method can nearly match the performance of the oracle baseline with prior rewards. This suggests that optimistic reward labeling can allow RL to utilize unlabeled prior data almost as effectively as labeled prior data on these three domains. In particular, on the sparse Adroit relocate task, using optimistic rewards can even outpace the oracle baseline (Appendix C.2, Figure 13). In the COG domain, there is a larger performance gap between our method and the oracle. We hypothesize that this is due to the high-dimensional image observation space, causing the exploration problem to be more challenging. On the Adroit domain, we find that periodic resetting of the learned reward function is necessary to avoid overfitting to the collected online experience - with the resetting, we find that even naive reward relabeling results in exploratory behavior, diminishing the gap when performing explicit reward uncertainty estimation with RND.

### Can representation learning help obtain better reward labels?

We now investigate whether we can improve exploration by basing our UCB reward estimates on top of pre-trained representations acquired from the prior data. We chose to use representations from ICVF (Ghosh et al., 2023), a method that trains feature representations by pre-training general successor value functions on the offline data. Value-aware pre-training is a natural choice for downstream reward uncertainty quantification, because the pre-training encourages representations to obey the spatiality of the environment - that states close to one another have similar representations, while states that are difficult to reach from one another are far away. We pre-train an ICVF model on the prior dataset and extract the visual encoder \(_{}(s)\), which encodes an image into a low-dimensional feature vector. These pre-trained encoder parameters are used to initialize the encoder for both the reward model \(r_{}(_{}(s))\) and the RND network \(f_{}(_{}(s))\). Note that these encoder parameters are not shared and not frozen, meaning that once training begins, the encoder parameters for the reward model will be different from the encoder parameters for the RND model, and also diverge from the pre-trained initialization. As shown in Figure 4, ICVF consistently improves the online learning efficiency across all image-input tasks on top of optimistic reward labeling. It is worth noting that the encoder initialization from pre-training alone is not sufficient for the learning agent to succeed online (Naive + ICVF still fails almost everywhere). On most tasks, without optimistic reward labeling, the agent simply makes zero progress, further highlighting the effectiveness of optimistic labeling in aiding online exploration and learning. To understand how the pre-trained representation initialization influences online exploration, we analyze its effect on the UCB reward estimate early in the training. Figure 5 shows the UCB reward estimates on an expert trajectory of the Grasp from Closed Drawer task when the reward model and the RND model are initialized with the pre-trained representations (in blue) and without the pre-trained representations (in orange). When our algorithm is initialized with the ICVF representations, the UCB reward estimates are increasing along the trajectory time step, labeling the images in later part of the trajectory with high reward, forming a natural curriculum for the agent to explore towards the trajectory end. When our algorithm is initialized randomly without the pre-trained representations, such a clear monotonic trend disappears. We hypothesize that this monotonic trend of the reward labeling may account for the online sample efficiency improvement with a better shaped reward.

Figure 4: **Accelerating exploration with pre-trained representations on three visual-input COG tasks.****Ours + ICVF** uses optimistic reward labeling with a pre-trained representation initialization; **Ours** uses optimistic reward labeling without the pre-trained representation initialization. The same applies for **Naive** and **Naive + ICVF**. Overall, initializing the reward model and the RND network using pre-trained representations greatly increases how quickly the model learns.

Figure 5: **The effect of pre-trained representations on the relabeled reward**. We show the normalized relabeled rewards (by standardizing it to have zero mean and unit variance) of an optimal prior trajectory from the closed Drawer task in the COG environment. Using pre-trained representations to learn the optimistic rewards leads to smoother rewards over the course of an optimal trajectory. See more examples in Appendix D.

### Does optimistic reward labeling help online learning by improving online exploration?

We have shown that optimistic reward labeling is effective in accelerating online learning, does it actually work because of better exploration? To answer this question, we evaluate the effect of our method on the state-coverage of the agent in the AntMaze domain. In this domain, the agent is required to reach a particular goal in the maze but has no prior knowledge of the goal, and thus required to explore different parts of the maze in the online phase in order to locate the goal. The optimal behavior that we expect the agent to perform is to quickly try out every possible part of the maze until the goal is located, and then focus entirely on that goal to achieve 100% success rate.

To more quantitatively measure the quality of our exploration behavior, we divide up the 2-D maze into a grid of square regions, and count the number of regions that the Ant agent has visited (using the online replay buffer). This provides a measure for the amount of state coverage that the agent has achieved in the environment, and the agent that explores better should achieve higher state coverage. Figure 6 shows how optimistic reward labeling affects the state coverage, and Figure 2 shows an example visualization of the state-visitation for each method on antmaze-medium. Across all six AntMaze tasks, our approach achieves significantly higher state-coverage compared to naively label without optimism both in the early stage of training and asymptotically compared to the baseline that does not leverage the unlabeled prior data. In particular, we find that naively labeling reward with no optimism is doing poorly on larger mazes, highlighting the importance of optimistic reward labeling for effective online exploration.

### How robust is our method in handling different offline data corruptions?

To further test the capability of our method in leveraging prior data, we corrupt one of the D4RL offline datasets, antmaze-large-diverse-v2, and test how our method performs under the corruption. We experimented with three different corruptions (and reported results in Figure 7).

Figure 6: **State coverage for 3 AntMaze tasks, estimated by counting the number of square regions that the ant has visited, normalized by the total number of square regions that can be visited. Ours optimistically labels the prior data, whereas Naive naively labels the prior data without optimism. Optimistic labeling enables the RL agent to achieve a significantly higher coverage. Online RND also explores well in the maze, but with a much slower rate compared to our approach, highlighting the importance of leveraging unlabeled prior data for efficient online exploration. Full results in Appendix C.1.**

Figure 7: The normalized return on antmaze-large-diverse-v2 under different offline data corruptions (see visualization of the offline data for each kind in Appendix B, Figure 8).

**Orthogonal transitions:** all the transitions that move in the up/right (towards the goal) are removed from the offline data. In this setting, stitching the transitions in the offline data would not lead to a trajectory that goes from the start to the goal. This is a good test for the robustness of the algorithm for exploration because the agent can not rely on the actions in the prior data and must explore to discover actions that move the ant in the correct direction.

**Insufficient coverage:** all the transitions around the goals are removed from the offline data. This requires the algorithm to actively explore so that it discovers the desired goal that it needs to reach.

**1% data:** A random 1% of the transitions are kept in the offline data. This tests the capability of our method in handling the limited data regime.

In addition to the main comparisons above, we consider two modifications specifically for these experiments to address the more challenging tasks.

**Ours + Online RND**: This is an extension of our method, with an RND bonus added to the online data in addition to the prior data. Similar to how we can extend our method with representation learning, having exploration bonuses on the online data does not conflict with exploration bonuses added to the offline data.

**Naive + Online RND**: This is an extension of Naive Reward Labeling. An RND bonus added to the online data, and the prior data is labeled with the unbiased reward estimate as before.

Directly using our method in the _Insufficient Coverage_ setting yields zero success, but this is to be expected as our algorithm does not incentivize exploration beyond the offline data. We find that this can be patched by simply combining our method with an additional RND bonus on online data, allowing our method to learn even when data has poor coverage and no data near the goal is unavailable. Similarly, we have found that combining our method with online RND can also boost the performance in the _Orthogonal Transitions_, where no transitions move in the direction of the goal. For the _1% Data_ setting, our method can still largely match the oracle performance without the need for an online RND bonus.

It is also worth noting that in the _Orthogonal Transitions_ setting, our method achieves good performance whereas the oracle fails to succeed. The failures of the oracle are not unexpected because the prior data do not contain any single trajectories, or even trajectories stitched together from different transitions, that can reach the goal. However, since our method succeeds, this indicates that exploration bonuses applied only to the prior data can still be utilized, despite there being no forward connection through the prior data. These additional results, while initially surprising even to us (especially the "orthogonal" setting, which seems very difficult), strongly suggest that our method can lead to significant improvement even when the data is not very good - certainly such data would be woefully inadequate for regular offline RL or naive policy initialization.

## 6 Discussion

We showed how we can effectively leverage unlabeled prior data to improve online exploration by running a standard off-policy RL algorithm on the data relabeled with UCB reward estimates. In practice, the UCB estimates can be approximated by combining the prediction of a reward model and an RND network, both can be trained online with little additional computational cost. We demonstrated the surprising effectiveness of this simple approach on a diverse set of domains. Our instantiation of the optimistic reward labeling idea presents a number of avenues for future research. First, on the relocate Adroit task, we found that naively fitting the reward model on the online replay buffer without any regularization leads to poor performance (possibly due to catastrophic overfitting). While we have found a temporary workaround by periodically re-initializing the reward model, such a solution may seem ad-hoc and could be disruptive to the RL learning due to the sudden change in the learning objective (which is shaped by the reward model). Next, the UCB estimates of the state-action can rapidly change, especially in the beginning of the training, which may cause learning instability in the RL algorithm. There is no mechanism in our current algorithm to tackle such a potential issue. Nevertheless, our work indicates that there exist simple ways of incorporating prior data for online RL agents even when no reward labels exist. Scaling these mechanisms to more complex prior datasets is an exciting direction towards the promise of RL agents that can leverage general prior data.

**Acknowledgement.** This work was partially done while QL was a visiting student researcher at FAIR, Meta. We would like to thank Seohong Park for providing his implementation of goal-conditioned IQL (GC-IQL) and pre-trained GC-IQL checkpoints (which were used in producing Figure 17). We would like to thank Fangchen Liu for the acronym of the method. We would also like to thank Laura Smith, Toru Lin, Oleg Rybkin, Kuba Grudzien, Aviral Kumar, and Joey Hong for discussion on the method and feedback on the early draft of the paper. We would also like to thank the members of the RAIL lab for insightful discussions on the paper. This research was partially supported by the Office of Naval Research under N00014-21-1-2838 and N00014-22-1-2773, and ARO W911NF-21-1-0097.