# Bilevel Coreset Selection in Continual Learning:

A New Formulation and Algorithm

Jie Hao

Department of Computer Science

George Mason University

jhao6@gmu.edu

&Kaiyi Ji

Department of CSE

University at Buffalo

kaiyiji@buffalo.edu

&Mingrui Liu

Department of Computer Science

George Mason University

mingruil@gmu.edu

Corresponding Author.

###### Abstract

Coreset is a small set that provides a data summary for a large dataset, such that training solely on the small set achieves competitive performance compared with a large dataset. In rehearsal-based continual learning, the coreset is typically used in the memory replay buffer to stand for representative samples in previous tasks, and the coreset selection procedure is typically formulated as a bilevel problem. However, the typical bilevel formulation for coreset selection explicitly performs optimization over discrete decision variables with greedy search, which is computationally expensive. Several works consider other formulations to address this issue, but they ignore the nested nature of bilevel optimization problems and may not solve the bilevel coreset selection problem accurately. To address these issues, we propose a new bilevel formulation, where the inner problem tries to find a model which minimizes the expected training error sampled from a given probability distribution, and the outer problem aims to learn the probability distribution with approximately \(K\) (coreset size) nonzero entries such that learned model in the inner problem minimizes the training error over the whole data. To ensure the learned probability has approximately \(K\) nonzero entries, we introduce a novel regularizer based on the smoothed top-\(K\) loss in the upper problem. We design a new optimization algorithm that provably converges to the \(\)-stationary point with \(O(1/^{4})\) computational complexity. We conduct extensive experiments in various settings in continual learning, including balanced data, imbalanced data, and label noise, to show that our proposed formulation and new algorithm significantly outperform competitive baselines. From bilevel optimization point of view, our algorithm significantly improves the vanilla greedy coreset selection method in terms of running time on continual learning benchmark datasets. The code is available at https://github.com/MingruiLiu-ML-Lab/Bilevel-Coreset-Selection-via-Regularization.

## 1 Introduction

Deep Neural Networks (DNNs) have achieved tremendous successes in various domains, including computer vision , natural language processing , generative modeling  andgames . However, in continual learning, where DNNs are trained on a sequence of tasks with possibly non-i.i.d. data, the performance will be degraded on the previously trained tasks. This is referred to as _catastrophic forgetting_[53; 52; 60]. To alleviate catastrophic forgetting, one of the effective ways is _rehearsal-based continual learning_, where a small replay buffer is maintained and revisited during the continuous learning process. There is a line of works studying how to efficiently maintain the replay buffer using the coreset selection approach [6; 78; 83], in which a small set of data is selected as representative samples to be used in continual learning.

The coreset selection in continual learning is formulated as a cardinality-constrained bilevel optimization problem which is solved by incremental subset selection . This greedy approach is computationally expensive and hence is not scalable when the coreset size is large. To address this issue, Zhou et al.  propose a relaxation of the bilevel formulation in , which drops the nested nature of bilevel formulation and actually becomes two sequential optimization problems. Tiwari et al.proposed a gradient approximation method in , which selects a coreset that approximates the gradient of model parameters over the entirety of the data seen so far. Yoon et al.  proposes an online coreset selection method by maximizing several similarity metrics based on data pairs within each minibatch, and sample pairs between each minibatch and coreset. These approaches do not directly address the algorithmic challenges caused by the nested nature of the bilevel optimization problem, and may not solve the original bilevel coreset selection problem efficiently.

The key challenges in the bilevel coreset selection problems are two folds. First, the bilevel formulation in  needs to directly perform optimization over cardinality constraint, which is a nonconvex set and greedy approaches are expensive when the coreset size is large. Second, the bilevel formulation in  has a nested structure: one problem is embedded within another, and the outer and inner functions both have dependencies over the same set of decision variables. It remains unclear how to design efficient algorithms to solve constrained bilevel optimization algorithms for the coreset selection problem with provable theoretical guarantees.

Our proposed solution addresses these challenges with a novel bilevel formulation and provably efficient optimization algorithms. The proposed new bilevel formulation is referred to as _Bilevel Coreset Selection via Regularization_ (BCSR). The main differences between our approach and the standard bilevel approach in  are: (i) unlike the standard bilevel formulation which requires performing optimization based on a cardinality constraint, we propose to solve a bilevel optimization on a probability simplex over training examples; (ii) to make sure the probability distribution lies in a low dimensional manifold, we propose to add a smoothed top-\(K\) loss as a regularizer to the upper-level problem; (iii) due to our new formulation, we are able to design a simple and effective first-order method to solve this new bilevel problem with provable non-asymptotic convergence guarantees. The first-order method is easy to implement and much faster than the greedy approach as in . Our main contribution is listed as follows.

Figure 1: Illustration of our algorithm. There are two neural network models, one is \(M_{tr}\) for model training, and the other is \(M_{cs}\) for coreset selection. A coreset is selected from the current data stream \(B_{t}\) by conducting six steps. 1 Feed a stream mini-batch \(B_{t}\) and sampled buffer data to \(M_{tr}\). 2 Copy model parameters from \(M_{tr}\) to \(M_{cs}\): \(_{cs}_{tr}\). 3 Feed a mini-batch \(B_{t}\) into model \(M_{cs}\). 4 Conduct bilevel optimization to update the model parameter \(_{cs}\) in \(M_{cs}\) and output a probability distribution \(w\). 5 Sample a coreset from \(B_{t}\) based on the distribution of \(w\) and add the sampled data into buffer. 6 Calculate stochastic gradient based on \(B_{t}\) and sampled data in the buffer in Step 1, and update \(_{tr}\) based on gradient information. Repeat the above steps for each stream mini-batch.

* We propose a new bilevel formulation, namely BCSR, for the coreset selection in rehearsal-based continual learning. Instead of directly learning the binary masks for each sample, the new formulation tries to learn a probability distribution in a low-dimensional manifold by adding a smoothed top-\(K\) loss as a regularizer in the upper problem. This formulation is designed to satisfy two important features in continual learning with DNNs: (i) keeping the nested structure in the coreset selection; (ii) being amenable to first-order algorithms, which makes it easy to implement in modern deep learning frameworks such as PyTorch and TensorFlow. Based on the new formulation, we propose an efficient first-order algorithm for solving it. The main workflow of our algorithm is illustrated in Figure 1, and the corresponding Pytorch-style pseudocode is presented in Algorithm 1.
* We have conducted extensive experiments among various scenarios to verify the effectiveness of our proposed algorithm, including balanced, imbalanced, and label-noise data. Our algorithm outperforms all baselines for all settings in terms of average accuracy, and it is much better than all other coreset selection algorithms. For example, on imbalanced data of Multiple Datasets, BCSR is better than the best coreset selection algorithm by \(4.65\%\) in average accuracy. From bilevel optimization point of view, our algorithm significantly improves the vanilla greedy coreset selection method  in terms of running time on continual learning benchmark datasets.
* Under the standard smoothness assumptions of the loss function, we show that our algorithm requires at most \(O(1/^{4})\) complexity for finding an \(\)-stationary point in the constrained case2. Notably, the \(O(1/^{4})\) complexity consists of \(O(1/^{2})\) backpropagations and \(O(1/^{4})\) samplings from Gaussian distribution, where the latter cost is computationally cheap. 
## 2 Related Work

Continual LearningThere are different classes of continual learning methods, including regularization-based approaches [39; 81; 10; 1; 59; 64; 17], dynamic architecture methods [61;79, 62, 51, 76, 44, 77], and rehearsal-based methods . In the rehearsal-based continual learning, the memory is either reproduced experience replay  or generative replay . Our work focuses on the aspect of the coreset selection on the replay memory and can be flexibly integrated into rehearsal-based methods in continual learning.

Coreset SelectionThe coreset selection methods were used frequently in supervised and unsupervised learning, such as \(k\)-means , Gaussian mixture model , logistic regression  and bayesian inference . They were also used frequently in the active learning literature . The coreset selection in continual learning is related to the sample selection . Nguyen et al.  introduce variational continual learning which is combined with coreset summarization . Borsos et al.  proposed the first bilevel formulation for the coreset selection in continual learning, which is later improved by . Compared with these works, our work focuses on improved bilevel coreset selection: we provide a better bilevel formulation than  and design a provably efficient optimization algorithm.

Bilevel optimizationBilevel optimization is used to model nested structure in the decision-making process . Recently, gradient-based bilevel optimization methods have broad applications in machine learning, including meta-learning , hyperparameter optimization , neural architecture search , and reinforcement learning . These methods can be generally categorized into implicit differentiation  and iterative differentiation  based approaches. Recently, various stochastic bilevel algorithms have been also proposed and analyzed by . A comprehensive introduction can be found in the survey . In this work, we propose a novel stochastic bilevel optimizer with very flexible parameter selection, which shows great promise in the coreset selection for continual learning.

## 3 New Bilevel Formulation for Coreset Selection in Continual Learning

In this section, we first introduce our new bilevel formulation, namely Bilevel Coreset Selection via Regularization (BCSR). The key idea of this approach is to learn a probability distribution over the whole dataset such that the best model parameter obtained by minimizing the loss on the sampled dataset (i.e., the minimizer for the lower-level problem) is also the best for the whole dataset (i.e., the minimizer for the upper-level problem), and then a coreset can be sampled based on the learned probability distribution. In addition, the learned probability distribution is expected to lie in a low dimensional manifold (i.e., with \(K\) nonzero entries where \(K\) is the coreset size). To achieve this, we added a smooth top-\(K\) loss as a regularizer to promote the probability distribution to have \(K\) nonzero entries. Specifically, the objective function of BCSR is:

\[_{0 w_{(i)} 1\\ |\|w\|_{1}=1}[(w)=_{i=1}^{n}_{i}( ^{*}(w))-_{i=1}^{K}_{z}(w+ z)_{[i]}]\] \[s.t.,^{*}(w)=_{}[L(,w)=_{i=1} ^{n}w_{(i)}_{i}()]\] (1)

where \(n\) is the sample size, \(\) is the model parameter, \(w\) is the sample weights, \(_{i}()\) denote the loss function calculated based on \(i\)-th sample with model parameter \(\), \(w_{(i)}\) is the \(i\)-th coordinate of \(w\), \(w_{[i]}\) is the \(i\)-th largest component of \(w\), \(>0\) is the regularization parameter, \(w+ z\) denote to adding \( z\) on each coordinate of \(w\) where \(z(0,1)\). Note that \(R(w,):=-_{i=1}^{K}_{z}(w+ z)_{[i]}\) denote the smoothed top-\(K\) regularization. We add this regularization to make sure the summation of the top-\(K\) entries of the learned probability vector is large, such that we can confidently choose a coreset with size \(K\). The goal of employing Gaussian noise to the regularizer is for the ease of algorithm design: this Gaussian smoothing technique can make the regularizer to be smooth such that it is easier to design efficient first-order bilevel optimization solvers. Otherwise, the upper-level problem would become nonconvex and nonsmooth, and it would be difficult for algorithm design under this case.

Discussion and Comparison with Prior WorksIn this part, we illustrate how this new formulation addresses the drawbacks of the previous approaches. The work of  does not use any regularizer and regards the weight of each sample as a binary mask. This formulation needs to solve a combinatorial optimization problem and their approach of incremental subset selection is computationally expensive. The work of  relaxes the bilevel formulation in  to minimize the expected loss function over the Bernoulli distribution \(s\), i.e., \(_{s}(s)\), and develops a policy gradient solver to optimize the Bernoulli variable. Their gradient \(_{s}(s)=_{p(m|s)}L(^{*}(m))_{s} p(m|s)\) does not include the implicit gradient of \(L(^{*}(m))\) in terms of \(s\). However, \(^{*}(m)\) actually depends on the mask \(m\), and \(m\) depends on the Bernoulli variable \(s\). In contrast, our bilevel optimization computes the hypergradients for the coreset weights \(w\) (\(0 w 1\) and \(\|w\|_{1}=1\) ), which considers the dependence between \((w)\) and \(w\)3. In addition, Zhou et al.  assume that the inner loop can obtain the exact minimizer \(^{*}(m)\), which may not hold in practice. In contrast, we carefully analyze the gap between the estimated \(^{*}(w)\) and itself by our algorithm and analysis.

```
0: Dataset \(\)
0: model parameter \(_{0}\), memory \(=\{\}\)
1:for batch \(_{t}\)do
2: Compute coreset \(_{t}\) = Find-coreset(\(_{t-1}\), \(_{t}\))
3:\(\) = \(_{t}\)
4:endfor ```

**Algorithm 2** Bilevel Coreset Selection via Regularization (BCSR)

## 4 Algorithm Design

Equipped with the new formulation, the entire algorithm is presented in Algorithm 2, which calls Algorithm 3 as a subroutine. Each time the algorithm encounters a minibatch \(\), a coreset is selected within this minibatch by invoking Algorithm 3. Algorithm 3 is a first-order algorithm for solving the bilevel formulation (1). In Algorithm 3, the model parameter \(\) and the weight distribution \(w\) are updated alternatively. We first perform \(N\) steps of gradient descent steps to find a sufficiently good \(\) for the lower-level problem (lines 5-7) by the update rule:

\[_{j}^{k}=_{j}^{k-1}-_{}L(_{j}^{k-1},w_{j}),\] (2)

where \(_{j}^{k}\) denotes the model parameters at the \(j\)-th outer loop and the \(k\)-th inner loop. To update the outer variable \(w\), BCSR approximates the true gradient \((w)\) of the outer function w.r.t \(w\), which is called hypergradient . BCSR constructs a hypergradient estimator:

\[_{j}=|}_{}_{w} R(w,;)-_{w}_{}L(_{j}^{N},w_{j}) (_{}^{2}L(_{j}^{N},w_{j}))^{-1}(_{i=1}^{n} _{}_{i}(_{j}^{N})),\] (3)where \(R(w,;):=-_{i=1}^{K}(w+)_{[i]}\) and \((0,1)\). Solving the Hessian-inverse-vector product in eq. (3) is computationally intractable. We denote \(v^{*}:=(_{}^{2}L(_{j}^{N},w_{j}))^{-1}(_{i=1}^{n}_{ }_{i}(_{j}^{N}))\) in eq. (3), where \(v^{*}\) can be approximated by solving the following quadratic programming problem efficiently by \(Q\) steps of gradient descent (line 9):

\[_{v}v^{T}_{}^{2}L(_{j}^{N},w_{j})v-v^{T}_ {i=1}^{n}_{}_{i}(_{j}^{N}).\] (4)

Next, the hypergradient estimate (line 10) is computed based on the output of approximated quadratic programming. Note both model parameters and sample weights need to use warm start initialization (line 4 and line 8). Then the weight is updated and projected onto simplex (line 11):

\[_{j+1}=w_{j}-_{j},\ \ w_{j+1}=_{^{n}}( _{j+1}),\] (5)

where \(^{n}:=\{w^{n}:0 w_{(i)} 1,||w||_{1}=1\}\). In terms of other experimental hyperparameters, we allow very flexible choices of hyperparameters (e.g., \(N\), \(Q\)) as shown in our theory to achieve polynomial time complexity for finding a \(\)-stationary point.

The selected coresets for each task are stored in a memory buffer with a fixed size \(m\). There is a separated memory slot for each task with size \([m/i]\) when the task \(i\) comes. After each task \(i\), the memory slots before \(i\) will randomly remove some samples to adjust all the memory slots to \([m/i]\). That means the memory size for each task decreases as the task ID increase to maintain the total buffer size \(m\) unchanged. The same memory strategy is also used in the greedy coreset approach .

## 5 Experiments

We conduct extensive experiments under various settings, including balanced data, imbalanced data, and label-noise data. The empirical results demonstrate the effectiveness of our method in rehearsal-based continual learning.

### Experimental Setup

**Datasets** We use commonly-used datasets in the field of continual learning, including Split CIFAR-100, Permuted MNIST, Multiple Datasets, Tiny-ImageNet, and Split Food-101. We follow the experimental settings as that in prior work  and . Each dataset is processed with three approaches: balanced, imbalanced, and label-noise. Please refer to Appendix L for more details about data processing and settings.

**Baselines** We compare our algorithm BCSR with other continual learning methods based on coreset strategy, including \(k\)-means features , \(k\)-means embedding , Uniform Sampling, iCaRL , Grad Matching , Greedy Coreset , PBCS , GCR , and OCS . We also compare with non-coreset reply method, SPR , MetaSP . All algorithms are built upon episodic memory, which stores coreset selected from stream data. Then a model, such as ResNet-18 , is trained over the data from the current stream and the episodic memory.

**Metrics** Average accuracy and forgetting measure  are two primary evaluation metrics that are used in continual learning literature. AVG ACC (\(A_{T}\)) is the average accuracy tested on all tasks after finishing the task \(T\): \(A_{T}=_{i=1}^{T}a_{T,i}\), where \(a_{T,i}\) is the test accuracy of task \(i\) after training

    &  &  &  \\ Methods & \(A_{T}\) & \(FGT_{T}\) & \(A_{T}\) & \(FGT_{T}\) & \(A_{T}\) & \(FGT_{T}\) \\  K-means Features & 57.82±0.69 & 0.070±0.003 & 45.44±0.76 & 0.037±0.002 & 57.38±1.26 & 0.098±0.003 \\ K-means Embedding & 59.77±0.24 & 0.061±0.001 & 43.91±0.15 & 0.044±0.001 & 57.92±1.25 & 0.091±0.016 \\ Uniform & 58.99±0.54 & 0.074±0.004 & 44.74±0.14 & 0.033±0.007 & 58.76±1.07 & 0.087±0.006 \\ iCaRL & 60.74±0.09 & **0.44±0.026** & 44.42±0.04 & 0.042±0.019 & 59.70±0.70 & 0.071±0.010 \\ Grad Matching & 59.17±0.38 & 0.067±0.03 & 45.44±0.64 & 0.038±0.001 & 59.58±0.28 & 0.073±0.008 \\ SPR & 59.56±0.73 & 0.143±0.64 & 44.45±0.55 & 0.086±0.023 & 58.74±0.63 & 0.073±0.010 \\ MetaSP & 60.14±0.25 & 0.056±0.230 & 0.34±0.36 & 0.079±0.04 & 57.43±0.54 & 0.086±0.007 \\ Greedy Coreset & 59.39±0.16 & 0.066±0.017 & 43.80±0.01 & 0.039±0.007 & 58.22±0.16 & 0.066±0.001 \\ GCR & 58.73±0.43 & 0.073±0.31 & 44.48±0.05 & 0.035±0.005 & 58.72±0.63 & 0.081±0.005 \\ PBCS & 55.64±2.26 & 0.062±0.001 & 39.87±1.12 & 0.076±0.01 & 56.93±0.14 & 0.100±0.003 \\ OCS & 52.57±0.37 & 0.088±0.001 & 46.54±0.34 & 0.022±0.003 & 51.77±0.81 & 0.103±0.007 \\ BCSR & **61.60±0.14** & 0.051±0.015 & **47.30±0.57** & **0.022±0.005** & **60.70±0.08** & **0.059±0.013** \\   

Table 1: Experiment results on Split CIFAR-100

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

To verify the effectiveness of our bilevel optimizer, we compare the loss curve with Greedy Coreset that uses NTK. The result is presented in Figure 3. There are a number of stream mini-batches in each task. The bilevel optimizer trains over each mini-batch, where the upper loss is plotted in the figure. In the experiment, we plot the loss value for every \(5\) mini-batches. Within each task, the loss from BCSR gradually decreases with slight fluctuations, and it increases only when encountering a new task. In contrast, the loss value of the Greedy Coreset approach always stays large. It indicates that our bilevel optimizer is more effective than the Greedy Coreset approach.

**Effectiveness of the regularizer.** In our algorithm, the bilevel formulation has a smooth top-\(K\) loss as a regularizer in the objective function to promote the probability distribution to have \(K\) nonzero entries. The goal is to make sure that the summation of top-\(K\) entries of the learned probability vector is large, which increases confidence in the coreset selection. The hyperparameter \(\) is used to balance the cross-entropy loss and the regularization term. We explore the effects on the performance with different \(\), and list the results in Table 6.

This ablation experiment is performed on our framework BCSR, and average accuracy and forgetting on three balanced benchmarks are reported. When \(=0.10\), BCSR can reach the best performance (The highest AVG ACC and lowest FGT) on Split CIFAR-100 and Permuted MNIST. While on Multiple Datasets, BCSR performs the best when \(=1.0\). This dataset contains more categories, and hence it is more challenging to select the appropriate representative samples. In this case, larger \(\) would emphasize more on maximizing the top-\(K\) probability and help select better representative samples. In addition, we observe that \(\) set as a too large or too small value will damage the performance, which is in line with the observations in standard regularized empirical risk minimization problems such as overfitting and underfitting. Please refer to Appendix H for further analysis.

## 6 Theoretical Analysis

In this section, we provide a theoretical analysis for our proposed method. In particular, we establish convergence rate for our algorithm BCSR.

**Theorem 1**.: _Suppose standard assumptions hold in bilevel optimization (see Assumptions 1, 2 and 3 in Appendix M). Choose parameters \(,,\) and \(N\) such that \((1+)(1-)^{N}(1+}{}) 1-\), where \(r=^{2}}{(+L)^{2}}\) and \(C_{Q}=+^{2}Q^{2} M+ QL\). Furthermore, choose the stepsize \(\) such that \(6^{2}L^{2}<\) and \(}\), where the constant \(\) is given by Equation (16) in Appendix A.

    &  &  \\ \(\) & \(A_{T}\) & \(FGT_{T}\) & \(A_{T}\) & \(FG_{T}\) & \(A_{T}\) & \(FGT_{T}\) \\ 
0.00 & 60.43\(\)0.15 & 0.064\(\)0.005 & 54.20\(\)0.53 & 0.1164\(\)0.030 & 55.71\(\)0.32 & 0.0554\(\)0.003 \\
0.01 & 60.56\(\)2.05 & 0.074\(\)0.016 & 55.09\(\)2.94 & 0.0974\(\)0.020 & 55.39\(\)0.95 & 0.0654\(\)0.012 \\
0.10 & **61.04\(\)0.53** & **0.063\(\)0.007** & **57.20\(\)0.68** & **0.064\(\)0.01** & 55.69\(\)0.40 & 0.057\(\)0.002 \\
1.00 & 59.43\(\)1.20 & 0.072\(\)0.008 & 55.43\(\)5.3 & 0.018\(\)0.019 & **58.18\(\)0.77** & **0.0464\(\)0.006** \\
5.00 & 58.80\(\)1.58 & 0.084\(\)0.017 & 53.134\(\)2.74 & 0.120\(\)0.021 & 56.67\(\)0.78 & 0.051\(\)0.007 \\
10.00 & 59.53\(\)0.42 & 0.075\(\)0.012 & 53.54\(\)2.02 & 0.118\(\)0.029 & 55.57\(\)1.05 & 0.060\(\)0.010 \\   

Table 6: The impact of different values of \(\)

Figure 3: The upper loss of bilevel optimization. Each distinct spike means the arrival of a new task.

_Then, we have the following convergence result._

\[_{j=0}^{J-1}\|G_{j}\|^{2}}{ J}+|}+}{ J}.\]

_where \(G_{j}:=(w_{j}-_{^{n}}(w_{j}-(w_{ j})))\) denote the generalized projected gradient, \(D_{}:=(w_{0})-_{w}(w)>0\), \(D_{0}=\|_{0}^{0}-_{0}^{*}\|^{2}+\|v_{0}^{-}v_{0}^{*}\|^{2}\), \(L_{}\) is the smoothness parameter of the total objective \((w)\) whose form is proved in Appendix A._

Theorem 1 provides a general convergence result for the proposed bilevel algorithm, which allows for a very flexible selection of subloop lengths \(N\) and \(Q\) as long as the inequality \((1+)(1-)^{N}(1+}{}) 1-\) holds given proper stepsizes \(,,\). For example, in most of our experiments, the choice of \(N=1\) and \(Q=3\) works the best. Then, in this case, we further specify the parameters in Theorem 1, and provide the following corollary.

**Corollary 1**.: _Under the same setting as in Theorem 1, choose \(N=1,Q=3\) and set \(=\), \(}{4608L^{2}}\) and \(\). Then, to make sure an \(\)-accurate stationary point, i.e., \(_{j=0}^{J-1}\|G_{j}\|^{2}^{2}\), the number of iterations is \((^{-2})\), each using \(||=(^{-2})\) of samples from the standard Gaussian distribution \((0,1)\)._

Corollary 1 shows that the proposed bilevel algorithm converges to an \(\)-accurate stationary point using only \((^{-2})\) iterations and \((^{-2})\) samples drawn from \((0,1)\) per iteration. Note the the large batch size \(||=(^{-2})\) is necessary here to guarantee a convergence rate of \((1/T)\), which matches the results in solving single-level constrained nonconvex problems . In addition, it is computationally tractable because sampling from a known Gaussian distribution is easy and cheap.

## 7 Conclusion

In this paper, we advance the state-of-the-art bilevel coreset selection in continual learning. We first introduce a new bilevel formulation with smoothed top-\(K\) regularization and then design an efficient bilevel optimizer as a solver. We conduct extensive experiments in continual learning benchmark datasets to demonstrate the effectiveness of our proposed approach. We also show that the bilevel optimizer can efficiently find \(\)-stationary point with \(O(1/^{4})\) computational complexity, which matches the best complexity of projected SGD for a single-level problem.