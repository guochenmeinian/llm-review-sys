# Private Algorithms for Stochastic Saddle Points and Variational Inequalities: Beyond Euclidean Geometry

**Raef Bassily**

Department of Computer Science & Engineering

Translational Data Analytics Institute (TDAI)

The Ohio State University

bassily.1@osu.edu

**Cristobal Guzman**

Inst. for Mathematical and Comput. Eng.

Fac. de Matematicas and Esc. de Ingenieria

Pontificia Universidad Catolica de Chile

crguzmanp@uc.cl

**Michael Menart**

Department of Computer Science & Engineering

The Ohio State University *

Footnote *: This work was done while M. Menart was at The Ohio State University.

Department of Computer Science, University of Toronto

Vector Institute

menart.2@osu.edu

**Abstract**

In this work, we conduct a systematic study of stochastic saddle point problems (SSP) and stochastic variational inequalities (SVI) under the constraint of \((,)\)-differential privacy (DP) in both Euclidean and non-Euclidean setups. We first consider Lipschitz convex-concave SSPs in the \(_{p}/_{q}\) setup, \(p,q\). That is, we consider the case where the primal problem has an \(_{p}\)-setup (i.e., the primal parameter is constrained to an \(_{p}\) bounded domain and the loss is \(_{p}\)-Lipschitz with respect to the primal parameter) and the dual problem has an \(_{q}\) setup. Here, we obtain a bound of \(}+}{n}\) on the strong SP-gap, where \(n\) is the number of samples and \(d\) is the dimension. This rate is nearly optimal for any \(p,q\). Without additional assumptions, such as smoothness or linearity requirements, prior work under DP has only obtained this rate when \(p=q=2\) (i.e., only in the Euclidean setup). Further, existing algorithms have each only been shown to work for specific settings of \(p\) and \(q\) and under certain assumptions on the loss and the feasible set, whereas we provide a general algorithm for DP SSPs whenever \(p,q\). Our result is obtained via a novel analysis of the recursive regularization algorithm. In particular, we develop new tools for analyzing generalization, which may be of independent interest. Next, we turn our attention towards SVIs with a monotone, bounded and Lipschitz operator and consider \(_{p}\)-setups, \(p\). Here, we provide the first analysis which obtains a bound on the strong VI-gap of \(}+}{n}\). For \(p-1=(1)\), this rate is near optimal due to existing lower bounds. To obtain this result, we develop a modified version of recursive regularization. Our analysis builds on the techniques we develop for SSPs as well as employing additional novel components which handle difficulties arising from adapting the recursive regularization framework to SVIs.

Introduction

Stochastic saddle point problems (SSP), are an increasingly important part of the machine learning toolkit. These problems model optimization settings with an inherent min-max structure, and for this reason are also referred to as stochastic min-max optimization problems. Concretely, the goal is to find an approximate solution of the following problem defined over a convex-concave loss,

\[_{w}_{}F_{}(w,):= _{x}[f(w,;x)]},\] (1)

where \(\) is an unknown distribution for which we have access to an i.i.d. sample \(S\). Problems of this kind have important applications in stochastic optimization , federated learning , distributionally robust learning , reinforcement learning , and algorithmic fairness .

Closely related to saddle point problems are stochastic variational inequalities (SVIs). Given a monotone operator, \(G_{}(z):=}_{x}[g(z;x)]\), the objective is to approximate the point \(z^{*}\), where

\[ G_{}(z^{*}),z^{*}-z 0,\ \  z.\] (2)

Stochastic saddle point problems can be easily related to variational inequalities by observing that the _saddle operator_ (an operator closely related to the gradient) of a convex-concave function is monotone. While SSPs and SVIs are closely related, it can be the case that a problem which can be formulated as a monotone SVI is not easily cast as a convex-concave SSP .

Parallel to the above, the problem of _privacy_ has become increasingly important in the big data era. In this regard, the notion of differential privacy has arisen as the premier standard. Stochastic optimization problems are a natural target for privacy concerns due to the fact that they are frequently formulated using a dataset of (potentially sensitive) individual data records. For many such problems, the constraint of differential privacy necessitates fundamentally new rates and techniques, and as such the formal characterization of these problems is an important task.

Thus far, work on differentially private SSPs and SVIs has focused primarily on Euclidean settings. However, a number of important, including many of those referenced at the start of this section, are naturally formulated in other geometries. Prior to this work, the optimal utility rate for DP SSPs was known only in Euclidean and polyhedral settings. For SVIs, the best achievable utility was unknown in any geometry (including Euclidean), at least under canonical utility measures. In this work, we provide the first systematic study of SSPs and SVIs in general geometries. The new analysis tools we develop lead to optimal rates for a number of these important setups.

### Contributions

In this work, we provide the first systematic study of stochastic saddle point problems and variational inequalities in both Euclidean and non-Euclidean geometries. Our first results pertain to stochastic saddle point problems where the primal problem has an \(_{p}\)-setup and the dual problem has an \(_{q}\)-setup, where \(p,q\). Here we assume the convex-concave loss is Lipschitz. We generalize the recursive regularization framework developed in previous works to more handle non-Euclidean geometries . At the heart of this extension is a fundamentally new analysis of the generalization properties of this algorithm. The issue of generalization has in fact been a key issue at the heart of many other works studying SSPs , as the presence of a supremum in the strong SP-gap accuracy measure (see Eqn. (4)) breaks more traditional generalization techniques. In contrast to prior work, our generalization technique works by avoiding entirely any generalization bound for the strong gap itself. Rather, we introduce a new accuracy measure which, when used in conjunction with the recursive regularization algorithm, eventually translates into a strong gap guarantee. Our technique stands in particular contrast to , which is thus far the only work in the DP literature to obtain optimal strong SP-gap rates in the Euclidean setting and also uses recursive regularization. However, their technique fundamentally relies on a McDiarmids style concentration bound that is worse by a \(poly(d)\) factor in non-Euclidean setups such as the \(_{1}\) setting . Using these new techniques, we provide the first analysis which obtains the near optimal rate of \((}+}{n})\) on the strong SP-gap for any \(p,q\). Our algorithm achieves this rate in \(\{^{1.5}}{},n^{3/2}\} \) number of gradient evaluations. We note that the near optimality of this rate is established by lower bounds for DP stochastic convex optimization, which is a special case of DP SSPs . Previously, comparable rates on the strong gap had only been obtained in the case where \(p=q=2\) or under strong additional assumptions .

Next, we consider DP stochastic variational inequalities with a monotone, bounded, and Lipschitz operator. We adapt the recursive regularization framework even further and again leverage a novel generalization analysis. Here, we obtain the rate \((}+}{n})\) on the strong VI-gap (see Eqn. (5)) in the \(_{p}\)-setting, \(p\). Our algorithm again achieves this rate in \(\{^{1.5}}{},n^{3/2}\} \) number of gradient evaluations. This is the first result to obtain the near optimal convergence rate on the strong VI-gap for \(p-1=(1)\), which notably includes the Euclidean case. The corresponding lower bound for \(p=2\) was established in , and we provide a simple extension of their technique to the case where \(p-1=(1)\). See Appendix E. Finally, for the setting \(p=2\), we show that our rate can be achieved in a near linear number of gradient evaluations by leveraging acceleration techniques for Lipschitz and strongly monotone variational inequalities.

### Related Work

Differentially private stochastic optimization now has a broad body of work spanning over a decade . Such work has rigorously characterized the problem of stochastic convex optimization in a variety of geometries. In \(_{p}\) setups, for \(p\), it is now known that the optimal rate is \((}+}{n})\) for such problems . It has also been shown that additional improvements are possible in the \(_{1}\) setting under smoothness assumptions. The study of stochastic saddle point problems under differential privacy is much less developed, but has nonetheless attracted a surge of recent interest . Without privacy, optimal \(O(1/)\) guarantees on the strong SP-gap have long been known . With privacy, the (near) optimal \(}+}{n}\) rate was obtained only recently, and then only in the Euclidean setting . In fact, work on DP-SSPs has focused largely on the case where \(p=q=2\), despite the fact that important problems are naturally formulated in other geometries. In particular, the case where \(q=1\) has important applications in distributionally robust optimization, federated learning, and algorithmic fairness. In this regard, the works  have recently studied DP SSPs with \(q=1\). The work  studies the \(_{1}/_{1}\) setting when the loss is additionally assumed to be smooth and the constraint set is polyhedral; they achieved the rate \(}+}\). We note that smoothness is fundamentally necessary in achieving this dimension independent rate, as otherwise existing lower bounds of \(}+}{n}\) hold for such problems . The work  studied the problem of differentially private worst-group risk minimization, which is closely related to DP-SSPs in the \(_{1}/_{2}\) setting, but requires the loss to have a specific linear structure with respect to the dual parameter. For \(_{1}/_{2}\) saddle point problems having this structure, their result implies a rate of \(O}+}{n}\) on the strong gap.

Work on SVIs is less developed. Non-privately, the optimal strong VI-gap rate of \(O(})\) was established in  (although related techniques trace back to ). Work on differentially private variational inequalities is limited to the work . In the Euclidean setup, this work achieved a rate of \(}+}{n^{2/3}}\) on the strong VI-gap under DP.

## 2 Preliminaries

In this section, we detail preliminaries for stochastic saddle point problems and differential privacy. Both SSPs and SVIs share a similar structure, which we detail first. Throughout, we use \([w,]\) to denote the concatenation of the vectors \(w\) and \(\). For a function \(f\), we let \( f\) denote an arbitrary subgradient selection of \(f\). Finally, we let \((U)\) denote the uniform distribution over the set \(U\).

Stochastic Monotone Operators.Let \(\) be some abstract data domain and let \(S^{n}\) for \(n>0\) and \(\) some unknown distribution over \(\). Let \(\|\|\) be some norm and \(\|\|_{*}\) its dual. We consider some compact convex parameter space \(^{d}\) of diameter \(B\) with respect to \(\|\|\). Let \(^{d}_{\|\|}(r)\) denote the \(d\)-dimensional ball of radius \(r>0\) w.r.t. \(\|\|\) centered on zero. We assume there exists\(L>0\) such that \(g:^{d}_{\|:\|_{}}(L)\) is a bounded operator and that for any \(x\), \(g(;x)\) is monotone. That is, \( z_{1},z_{2}\) it holds that \( g(z_{1};x)-g(z_{2};x),z_{1}-z_{2} 0\). We define the empirical and population operators as \(G_{S}(z)=_{i=1}^{n}g(z;x_{i})\) and \(G_{}(z)=}_{x}[g(z;x)]\).

Stochastic Saddle Point Problems.SSPs have the following structure in addition to the above. Let \(d_{w},d_{} 0\) such that \(d_{w}+d_{}=d\). We assume \(\) is the product of the convex compact sets \(^{d_{w}}\) and \(^{d_{}}\) equipped with norms \(\|\|_{w}\) and \(\|\|_{}\) and having diameters \(B_{w}\) and \(B_{}\) respectively. Then \(=\). We let \(\|[w,]\|=^{2}+\|\|_{}^{2}}\), and thus the diameter of \(\) satisfies \(B^{2}+B_{}^{2}}\); note the geometric mean of two norms is always a norm.

In SSPs, we consider the case where the monotone operator is the _saddle operator_ of a convex-concave loss function \(f:\). The saddle operator is defined as \(g(w,;x)=[_{w}f(w,;x),\,-_{}f(w,;x)]\) and is always monotone if \(f\) is convex-concave. We also define the corresponding population loss and empirical loss functions as \(F_{}(w,)=}_{x}[f(w, ;x)]\) and \(F_{S}(w,)=_{x S}f(w,;x)\) respectively. The boundedness assumption on \(g\) means \(f\) is \(L\)-Lipschitz. Concretely, \( w_{1},w_{2}\) and \(_{1},_{2}\):

\[|f(w_{1},_{1};x)-f(w_{2},_{2};x )| L\|[w_{1},_{1}]-[w_{2},_{2}]\|\] (3)

Under such assumptions, a solution for problem (1) always exists , and is referred to as the _saddle point_. Further, given an SSP (1), we will denote a saddle point as \([w^{*},^{*}]\).

The utility of an approximation to the saddle point is characterized by the strong SP-gap. Given a (randomized) algorithm \(\) with output \([_{w}(S),_{}(S)]\), this is defined as

\[_{}() = }_{,S}[_{} \{F_{}(_{w}(S),)\}-_{w }\{F_{}(w,_{}(S))\}].\] (4)

For notational convenience, we define the following function which is closely related to the SP-gap, \(}_{}(,)=_{ }\{F_{}(,)\}-_{w} \{F_{}(w,)\}\). Usefully, this function is known to be Lipschitz whenever \(f\) is Lipschitz.

**Fact 1**.: _() If \(f\) is \(L\)-Lipschitz then \(}_{}\) is \(L\)-Lipschitz._

Finally, we define \(_{p}/_{q}\) saddle point problems as those which, in addition to the above, also have the following structure. Assume \(\|\|_{w}=\|\|_{p}\) and \(\|\|_{}=\|\|_{q}\) and that \(\) and \(\) have diameters bounded by \(B_{w}\) and \(B_{}\) with respect to \(\|\|_{p}\) and \(\|\|_{q}\) respectively. We assume that for any \(x\) that \(f(,;x)\) is \(L_{w}\) Lipschitz in its first parameter w.r.t. \(\|\|_{p}\) and \(L_{}\) Lipschitz in its second parameter w.r.t. \(\|\|_{q}\). Note this implies an overall Lipschitz parameter, as per Eqn. (3), of \(L^{2}+L_{}^{2}}\).

Stochastic Variational Inequalities.For SVIs, in addition to the assumption that the monotone operator is \(L\)-bounded, we will also assume it is \(\)-Lipschitz. The objective for stochastic variational inequalities is to find an approximation of the (population) equilibrium point \(z^{*}\), where \(z^{*}\) is characterized by Eqn. (2). We refer to such a solution as an equilibrium point. For approximate solutions, the quality of the approximation is characterized by the strong VI-gap:

\[_{}() = }_{,S}[_{z} \{ G_{}(z),(S)-z\}].\] (5)

Note that it is not true in general that the VI-gap bounds the SP-gap even when the monotone operator in question is the saddle operator of some convex-concave loss (see Fact 3 in Appendix A). However, in such a case it is true that the equilibrium point of the of the SVI is the saddle point of the corresponding SSP.

We say that an operator \(g\) is \(\)-strongly monotone if for any \(z_{1},z_{2}\) that \( g(z_{1};x)-g(z_{2};x),z_{1}-z_{2}\|z_{1}-z_{2}\|\).

Stability.Important to our analysis will be the notion of uniform stability .

**Definition 1**.: _A randomized algorithm \(:^{n}\) satisfies \(\)-uniform argument stability (UAS) if for any pair of adjacent datasets \(S,S^{}^{n}\) it holds that \(}_{}[\|(S)-(S^{}) \|]\)._The notion of strong convexity is often important in analyzing stability. A function \(:\) is \(\)-strongly convex w.r.t. \(\|\|\) if for any \(z\), \(z^{}\) one has \((z)-(z^{})(z^{}),z-z^{}+ \|z-z^{}\|^{2}\). We call a function \(F:\)\(\)-strongly-convex/strongly-concave (SC/SC) if for any \(\) and \(w\) the functions \(F(,)\) and \(-F(w,)\) are \(\)-strongly-convex.

Notably, adding a SC/SC regularizer leads to stability properties. The following fact results from a more general result for regularized SVIs; see Lemma 5 in Appendix A.

**Lemma 1**.: _Let \(:\) be \(\)-strongly-convex/strongly-concave w.r.t. \(\|\|_{w}\) and \(\|\|_{}\). Then the algorithm which returns the saddle point of \((w,)_{z S}f(w,;z)+(w,)\) is \(()\)-UAS._

Differential Privacy (DP) .An algorithm \(\) is \((,)\)-differentially private if for all datasets \(S\) and \(S^{}\) differing in one data point and all events \(\) in the range of the \(\), we have, \(((S)) e^{} ((S^{}))+\).

### Example of Non-Euclidean SSP

One important example of non-Euclidean SSPs arises from the problem of minimizing the worst case risk over multiple populations. This problem has arisen in group distributionally robust optimization to name just one of many applications . Let \(f:\) and \(\) have a standard \(_{p}\) setup. Consider \(k\) distributions, \(_{1},,_{k}\), and the goal of selecting a model \(w\) which guarantees the lowest worst-case risk for the \(k\) distributions above:

\[_{w}_{j[k]}_{x_{j}_{j}}[f(w; x_{j})]=_{w}_{}_{j=1}^{k}^{(j)} _{x_{j}_{j}}[f(w;x_{j})],\]

where here \(\) denotes the standard \(k\)-dimensional simplex, and the equality above holds by the maximum principle for convex functions . Given that the feasible set for \(\) is a simplex, it is natural to endow this space with the \(_{1}\)-geometry. We thus end up with a SSP problem in \(_{p}/_{1}\) setup.

## 3 A New Analysis for Recursive Regularization

```
0: Dataset \(S^{n}\), loss function \(f\), subroutine \(_{}\), regularization parameter \(}\) (where \(\|\|_{w}^{2}\) and \(\|\|_{}^{2}\) are \(\) strongly convex), constraint set diameter \(B\), Lipschitz constant \(L\).
1: Let \(n^{}=n/_{2}(n)\), and \(T=_{2}()\).
2: Let \(S_{1},...,S_{T}\) be a disjoint partition of \(S\) with each \(S_{t}\) of size \(n^{}\) (_which is always possible due to the condition on \(\)_)
3: Let \([_{0},_{0}]\) be any point in \(\)
4: Define function \((w,,x) f^{(1)}(w,;x)=f(w,;x)+2\|w- _{0}\|_{w}^{2}-2\|-_{0}\|_{}^ {2}\)
5:for\(t=1\) to \(T\)do
6:\([_{t},_{t}]=_{}(S_{t},f^{(t)},[ _{t-1},_{t-1}],})\)
7: Define \((w,,x) f^{(t+1)}(w,;x)=f^{(t)}(w,;x)+2^{t+1} \|w-_{t}\|_{w}^{2}-2^{t+1}\|-_ {t}\|_{}^{2}\)
8:endfor
9:Output:\([_{T},_{T}]\) ```

**Algorithm 1** Recursive Regularization: \(_{}\)

In this section, we present our modified recursive regularization algorithm, first developed in  and extended to Euclidean SSPs in . We then discuss the key components of our analysis needed to obtain our results for non-Euclidean geometries. We conclude the section by applying our general result for recursive regularization to DP \(_{p}/_{q}\)-SSPs.

Algorithm Overview.As in , our recursive regularization implementation, Algorithm 1, solves a series of regularized saddle point problems defined by \(f^{(1)},...,f^{(T)}\). The saddle point problem defined in each round of Algorithm 1 is solved using some empirical subroutine, \(_{emp}\). This subroutine takes as input a subset of the dataset, \(S_{t}\), the regularized loss function for that round, \(f^{(t)}\), a starting point, \([_{t-1},_{t-1}]\), and an upper bound on the expected distance to the empirical saddle point of the problem defined by \(S_{t}\) and \(f^{(t)}\). The exact implementation of \(_{emp}\), Algorithm 3, will be discussed in the next section. Here, we focus on the guarantees of Recursive Regularization given that \(_{emp}\) satisfies a certain accuracy condition. At each round, the empirical subroutine \(_{}\) is required to find a point, \([_{t},_{t}]\), which is close (under \(\|\|\)) to the _empirical_ saddle point. Because the scale of regularization doubles each round, this task becomes easier each round. Specifically,  observed that implementations of \(_{}\) which satisfy the notion of relative accuracy succeed at finding such points.

**Definition 2** (\(\)-relative accuracy).: _Given a dataset \(S^{}^{n^{}}\), loss function \(f^{}\), and an initial point \([w^{},^{}]\), we say that \(_{}\) satisfies \(\)-relative accuracy w.r.t. the empirical saddle point \([w^{*}_{S^{}},^{*}_{S^{}}]\) of \(F^{}_{S^{}}(w,)=_{x S^{}}f^{} (w,;x)\) if, \(>0\), whenever \([\|[w^{},^{}]-[w^{*}_{S^{}},^{*} _{S^{}}]\|]\), the output \([,]\) of \(_{}\) satisfies \([F^{}_{S^{}}(,^{*}_{S^{}})-F^{ }_{S^{}}(w^{*}_{S^{}},)]\)._

In contrast to , our algorithm uses more general regularization to ensure strong-convexity/strong-concavity with respect to the appropriate norm.

Guarantees of Recursive Regularization.Our general result for recursive regularization is stated as follows, and its full proof is given in Appendix B.2.

**Theorem 1**.: _Let \(_{}\) satisfy \(\)-relative accuracy for any \((5L)\)-Lipschitz loss function and dataset of size \(n^{}=\) and assume \(\|\|_{w}^{2}\) and \(\|\|_{}^{2}\) are \(\)-strongly convex under \(\|\|_{w}\) and \(\|\|_{}\) respectively. Then Algorithm 1, run with \(_{}\) as a subroutine and \(=(^{2}+}{}})\), satisfies_

\[_{}(_{})=OB ^{2}(n)+^{3/2}(n)}{}.\]

The similarity of this result to [1, Theorem 5] and our exposition thus far belies the difficulty of adapting their result to non-Euclidean setups. The key challenge addressed by the analysis of  was that of _generalization_. In this regard, their key insight was to use McDiarmid style concentration bounds to show that the _empirical_ saddle point obtains non-trivial guarantees on the strong gap. However, such concentration results critically rely on the fact that the underlying norm is Euclidean. A generalization of this concentration to, for example, the \(_{1}\) setup, necessarily incurs an additional \(\) factor . Thus, a fundamentally new analysis is needed. One should also note that the squared norms \(\|\|_{w}^{2}\) may _not_ be strongly convex for certain norms, such as \(\|\|_{1}\). Regardless, in some such cases, we can still leverage this result by modifying the underlying problem, as we will show in Section 4.

Key Proof Ideas.We circumvent the above issues by providing a fundamentally new generalization analysis for the intermediate iterates of recursive regularization. Specifically, our analysis avoids entirely any analysis of the strong gap at intermediate stages of the algorithm. Instead, we introduce two new functions, which are similar in nature to the quantity used in the definition of relative accuracy, but are taken with respect to the _population_ saddle point. For \(t[T]\), define \(F^{(t)}_{}(w,;x):=}_{x} f^{(t)}(w,;x)\) and let \([w^{*}_{t},^{*}_{t}]\) be its saddle point; define \(F^{(t)}_{S}:=}_{x S_{t}}f^{(t)}(w,;x)\). We are interested in the functions,

\[H^{(t)}_{}([w,]):=F^{(t)}_{}(w,^{*}_{t})-F^{( t)}_{}(w^{*}_{t},) H^{(t)}_{S}(w,):=F^{(t)}_{S }(w,^{*}_{t})-F^{(t)}_{S}(w^{*}_{t},).\] (6)

Notably, the strong-convexity/strong-concavity of \(F^{(t)}_{}\) means that a bound on \(H^{(t)}_{}([w,])\) yields a bound on \(\|[w,]-[w^{*}_{t},^{*}_{t}]\|\). Ultimately, finding a point sufficiently close to \([w^{*}_{t},^{*}_{t}]\) at each round is all recursive regularization needs to succeed. The question then, is how to obtain guarantees on \(H^{(t)}_{}\). We accomplish this via a stability-implies-generalization argument.

**Lemma 2**.: _Let \(f:\) be \(L\)-Lipschitz. Let \([w^{*},^{*}]\) be the population saddle point. For any \(x\) define \(h([w,];x)=f(w,^{*};x)-f(w^{*},;x)\). For \(S^{n}\), let \(H_{S}(z)=_{x S}h(z;x)\) and \(H_{}(z)=}_{x}[h(z;x)]\). Then for any \(\)-UAS algorithm, \(\), one has \(}_{S,A}[H_{}((S))-H_{S}( (S))] 2 L\)._The proof relies on two main observations. First, it is easy to see that because \(f\) is Lipschitz, then \(h\) is also Lipschitz. Then, because \(H_{}\) and \(H_{S}\) can be written as the expectation of \(f\) w.r.t. \(z\) and \(z(S)\) respectively, we can apply standard stability-implies-generalization results to \(h\) to obtain the claimed result . We provide a full proof in Appendix B.1. This analysis bypasses difficulties of working directly with \(}_{}\) experienced by  and other works since, in general, there is _no_ function \(h\) such that \(}_{}(z)=}_{x} [h(z;x)]\). Note also it is important that we have defined \(h(z;x)\) w.r.t. to the data independent point \([w^{*}_{t},^{*}_{t}]\). Were \([w^{*}_{t},^{*}_{t}]\) to depend on \(S\), this result would not hold.

Because \(H^{(t)}_{S}\) uses the _population_ saddle point in its definition, it may not be immediately clear how one could first minimize \(H^{(t)}_{S}\). Direct access to \(H^{(t)}_{S}\) is in fact not possible without knowledge of \([w^{*}_{t},^{*}_{t}]\), which the algorithm does not have. In this regard, we observe that algorithms which minimize the _empirical_ gap are powerful enough to minimize \(H^{(t)}_{S}\), even without knowledge of \([w^{*}_{t},^{*}_{t}]\), since

\[H^{(t)}_{S}(w,)=F^{(t)}_{S}(w,^{*}_{t})-F^{(t)}_{S}(w^{*}_{t}, )_{w^{},^{}}\{F^{(t)}_{S}(w,^{ })-F^{(t)}_{S}(w^{},)\}.\]

Particular to our analysis, we will leverage the fact that the exact empirical saddle point is \(()\)-stable and has an empirical gap of \(0\).

## 4 Optimal Rates for Private \(_{p}/_{q}\) Saddle Point Problems

Setup.In this section, we apply Theorem 1 to obtain results for \(_{p}/_{q}\) saddle point problems. In order to do this, we will apply recursive regularization using norms slightly different than the \(_{p}\) and \(_{q}\) norms. Specifically, to solve an \(_{p}/_{q}\) SSP, we define \(=\{p,1+\}\) and \(=\{q,1+\}\) and will apply recursive regularization with \(\|\|_{w}=}\|\|_{}\) and \(\|\|_{}=}\|\|_{}\). We also have \(\|\|_{}\|\|_{1} d^{1-}\|\|_{} 2 \|\|_{}\). Thus, under these norms we have diameter bound \(B^{2}=1\) and Lipschitz constant \(L^{2} 4B^{2}_{w}L^{2}_{w}+4B^{2}_{}L^{2}_{}\). Further the strong convexity assumption needed by Theorem 1 is satisfied with \(=\{^{-1}},-1}\}\). This is because for any \(p>1\), \(\|\|_{p}^{2}\) is \((p-1)\)-strongly convex w.r.t. \(\|\|_{p}\).

Private Algorithm Satisfying Relative Accuracy.To apply Theorem 1, we must construct an algorithm satisfying relative accuracy and \((,)\)-DP. For this, we use the stochastic mirror prox algorithm of , Algorithm 2. This algorithm will also have application in our analysis of SVIs later on.

```
0: Learning rate \(\), Operator oracle \(\), Initial point \(z_{0}\), Regularization function \(\) minimized at \(z_{0}\), Iterations \(T\)
1:for\(t=1 T\)do
2:\(_{t}=_{u}\{(u)+ (z_{t-1})-(z_{t-1}),u\}\)
3:\(z_{t}=_{u}\{(u)+( {z}_{t})-(z_{t-1}),u\}\)
4:endfor
5:SSP output:\(_{t=1}^{T}z_{t}\)
6:SVI output:\(z_{t^{*}}\) for\(t^{*}([T])\) ```

**Algorithm 2** Stochastic Mirror Prox

This algorithm takes as input a stochastic oracle for the saddle operator of \(f\), \(\), and a strongly convex regularizer, \(\). We leverage this algorithm by constructing a differentially private version of the operator oracle \(\), and taking as output the average iterate \(=_{t=1}^{T}z_{t}\). We make the oracle private by adding Gaussian noise to minibatch estimates of the saddle operator. It is then easy to show the whole algorithm is private by composition results and the post processing properties of differential privacy. We defer these details to Appendix C.2, and here state the final relative accuracy bound.

**Lemma 3**.: _Under the setup described above, there exists an \((,)\)-DP algorithm which satisfies \(\)-relative accuracy with parameter \(=O^{2}L_{w}^{2}+B_{}^{2}L_{}^{ 2})}}}{n}+}\) and runs in \(On^{2}^{1.5}}{^{2}(n)}},n^{3/2}}{^{3/2}(n)}} \) number of gradient evaluations, where \(=1+\{p<2 q<2\}(d)\)._

Main Result for DP \(_{p}/_{q}\) SSPs.Applying now the result of recursive regularization, Theorem 1, we obtain the optimal rate for \(_{p}/_{q}\) SSPs (up to logarithmic factors). Recall under our chosen norm that \(B 1\) and \(L^{2} 4B_{w}^{2}L_{w}^{2}+4B_{}^{2}L_{}^{2}\). Further, the privacy of Algorithm 1 follows from the privacy of \(_{}\) and the parallel composition and post processing properties of differential privacy.

**Corollary 1**.: _There exists an Algorithm, \(\), which is \((,)\)-DP, has number of gradient evaluations bounded by \(On^{2}^{1.5}}{(n)}},n^{3/2}}{}} \), and satisfies (up to \((n)\) factors),_

\[_{}}()=(^{2.5} ^{2}L_{w}^{2}+B_{}^{2}L_{}^{2}}(}}{n}+})).\]

_(\(\) is at most \((d)\).)_

Note that in the \(_{2}/_{2}\)-setting \(=1\) and the above exactly recovers the result of . We further recall that the near optimality of this result is established by existing lower bounds for stochastic minimization, which is a special case of SSPs .

## 5 Extension to Variational Inequalities

In this section, we start by discussing the modifications that must be made to the recursive regularization algorithm to handle the more general structure of SVIs. We then discuss key ideas in the analysis and how to apply the algorithm to SVIs in the \(_{p}\) setting. We recall that we here assume the operator \(g\) is monotone, \(L\)-bounded and \(\)-Lipschitz.

### Recursive Regularization Algorithm for SVIs

Algorithm 3 bears many similarities to Algorithm 1. Most notable among the differences is that we here regularize with a strongly monotone operator \(\) instead of a strongly-convex/strongly-concave function. In our eventual application we will use \(=(\|\|^{2})\), but strictly we only require that \(\) satisfies the following.

**Assumption 1**.: _For \(>0\), \(:^{d}\) is \(\)-strongly monotone w.r.t. \(\|\|\) and satisfy \(\|(z)\|_{*}\|z\|\) for all \(z\)._

When \(=(\|\|^{2})\), the second half of the condition is always guaranteed by the properties of the dual norm (see Fact 2). When \(\) satisfies Assumption 1, we obtain the following guarantee.

**Theorem 2**.: _Let \(_{}\) satisfy \(\)-relative stationarity for any \((5L)\)-bounded monotone operator and dataset of size \(n^{}=\). Let \(\) satisfy Assumption 1. Then Algorithm 1, run with \(_{}\) as a subroutine and \(=(^{3}+}{ }})\), satisfies_

\[_{}(_{})=O(n)B^{3}+(n)B( B+L)^{2}}{}.\]

The full proof is in Appendix B.2. We discuss the key ideas in the following subsection.

### Analysis Idea

Unfortunately, the analysis used for stochastic saddle point problems does not easily extend to variational inequalities due to the fact that the VI-gap and SP-gap behave in fundamentally different ways. Even though SSPs are a special case of SVIs (when the operator in question is the saddle operator of the loss function), a bound on the VI-gap does not imply a bound on the SSP-gap. Further, a natural attempt to extend the notion of relative accuracy (Definition 3) to SVIs by asking \(_{}\) to bound \( G_{}(z^{*}_{S}),(S)-z^{*}_{S}\) does not work, because such a term does not yield an upper bound on the distance \(\|(S)-z^{*}_{S}\|\). A similar problem holds for generalization measures in Eqn. (6).

A New Empirical Accuracy Measure.Motivated by the above issues, we introduce a new relative accuracy measure for our analysis of SVIs. Importantly, this notion will allow us to bound the distance between the output of \(_{}\) and the empirical equilibrium point of the strongly monotone operator created at each round of the recursive regularzation algorithm.

**Definition 3** (\(\)-relative stationarity).: _Given a dataset \(S^{}^{n^{}}\), operator \(g^{}\), and an initial point \(z^{}\), we say that \(_{}\) satisfies \(\)-relative stationarity w.r.t. to the empirical equilibrium \(z^{*}_{S^{}}\) of \(G_{S^{}}(z)=}_{x S^{}}g^{}(z;x)\), if, \(>0\), whenever \([\|z^{}-z^{*}_{S^{}}\|]\), the output \(\) of \(_{}\) satisfies \([ G(),-z^{*}_{S^{}}] \)._

Modified Generalization Measure.The generalization measure we use can be modified in a similar fashion.

**Lemma 4**.: _Let \(g(z;x)=g_{1}(z;x)+g_{2}(z)\) such that \(g_{1}:^{d}\) is \(L\)-bounded and \(\)-Lipschitz with respect to \(z\) and \(g_{2}:^{d}\) is any (data independent) operator. Let \(z^{*}\) be its population equilibrium point. For any \(x\), define \(h(z;x)= g(z;x),z-z^{*}\). For \(S^{n}\), let \(H_{S}(z)=_{x S}h(z;x)\) and \(H_{}(z)=}_{x}[h(z;x)]\). Then for any \(\)-UAS algorithm, \(\), one has \(}_{S,A}[H_{}((S))-H_{S}( (S))]( B+L)\)._

The proof is similar to that of Lemma 2, but must account for additional complications. First, the function \(h\) may not be Lipschitz if the regularizer, represented by \(g_{2}\) above, is not Lipschitz. This happens, for example, in the \(_{1}\) setting. Thus, we must decompose \(h\) in the stability-implies-generalization analysis and handle the non-Lipschitz, but data-independent, term \(g_{2}\) separately. Then, the Lipschitzness of the remainder is established using that fact that \(g_{1}\) is both bounded and Lipschitz. The full proof is in Appendix D.1.

### Application to DP variational inequalities in the \(_{p}\) setting

In order to apply Theorem 2 to SVIs with an \(_{p}\) setup, we pick \(\|\|=\|z\|_{}\) where \(=\{p,1+\}\)2. We will use the regularizer,

\[(z)=(\|z\|_{}^{2}).\] (7)

Note the above operator is uniquely defined since \(>1\), and thus \(\|\|_{}^{2}\) is differentiable . This choice of \(\) satisfies Assumption 1 with parameter \(=-1}\). To see this, first observe thatfor any \(>1\), \(\|\|_{}^{2}\) is \((-1)\)-strongly convex w.r.t. \(\|\|_{}\) and the gradient operator of a differentiable \(\)-strongly convex function is \(\)-strongly monotone. Second, for any norm and it's dual \(\|(\|z\|^{2})\|_{*}\|z\|\) for all \(z\) (see Fact 2 in Appendix A).

To obtain relative stationarity guarantees, we again apply Algorithm 2 with a differentially private oracle for the empirical operator \(G_{S}\). The proof falls out of our existing analysis for stochastic mirror prox given in Appendix C.2. Specifically, Theorem 4 in the case where \(\) is the empty set implies there exists an \((,)\)-DP implementation of mirror prox which satisfies \(\)-relative stationarity with

\[=OBL \{p<2\}(d))}}{n}+BL}{} .\]

We here highlight one notable difference that arises with the algorithm. Typically, after running an algorithm like stochastic mirror prox for \(t\) iterations, one obtains a bound on the quantity \([_{t=1}^{T} G_{S}(z_{t}),z_{t}-z ]\). Analysis then proceeds to bound the (empirical) VI-gap of the average iterate by leveraging monotonicity of the operator. However, this application of monotonicity does not help for the purposes of relative stationarity. For this reason, we instead select an iterate uniformly at random. We note this step is nonstandard as such a selection does not necessarily yield any bound on the (empirical) VI-gap.

Main Result for \(_{p}\) DP SVIs.Using Algorithm 3 and the implementation of \(_{}\) described above, we ultimately obtain the following result as a corollary of Theorem 2 and Theorem 4. Recall under our choice of norm we have diameter bound \(1\) and operator bound \(BL\). Further, we can leverage existing lower bounds to show that this rate is near optimal; more details are available in Appendix E.

**Corollary 2**.: _There exists an Algorithm, \(\), which is \((,)\)-DP, has gradient evaluations bounded by \(On^{2}^{1.5}}{(n)}},n^{3/2}}{}} \), and satisfies (up to \((n)\) factors)_

\[}()=^{3.5}BL}}{n}+} .\] (8)

_where \(=\}-1}\) is at most \((d)\) and \(=1+\{p<2\}(d)\)._

Near Linear Time Algorithm for the \(_{2}\) Setting.Because we assume the operator is Lipschitz, in the \(_{2}\) setting, we can leverage existing accelerated optimization techniques to achieve a near linear time version of \(_{}\), in a similar fashion to . Specifically, using the accelerated SVRG algorithm of  and Gaussian noise it is possible to obtain the rate in Eqn. (8) (with \(==1\)) in \(O(n+ n(n/))\) gradient evaluations. We provide full details in Appendix D.3.