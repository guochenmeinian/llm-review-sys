# SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark

Sithursan Sivasubramaniam\({}^{*}\), Cedric Osei-Akoto\({}^{*}\), Yi Zhang,

Kurt Stockinger, Jonathan Furst\({}^{}\)

Zurich University of Applied Sciences, Switzerland

{sivassit,oseiaced}@students.zhaw.ch, {zhay,stog,fues}@zhaw.ch

 Equal contribution.\(\) Corresponding author.

###### Abstract

Electronic health records (EHRs) are stored in various database systems with different database models on heterogeneous storage architectures, such as relational databases, document stores, or graph databases. These different database models have a big impact on query complexity and performance. While this has been a known fact in database research, its implications for the growing number of Text-to-Query systems have surprisingly not been investigated so far. In this paper, we present SM3-Text-to-Query, the first multi-model medical Text-to-Query benchmark based on synthetic patient data from Synthea, following the SNOMED-CT taxonomy--a widely used knowledge graph ontology covering medical terminology. SM3-Text-to-Query provides data representations for relational databases (PostgreSQL), document stores (MongoDB), and graph databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically and manually develop 408 template questions, which we augment to construct a benchmark of 10K diverse natural language question/query pairs for these four query languages (40K pairs overall). On our dataset, we evaluate several common in-context-learning (ICL) approaches for a set of representative closed and open-source LLMs. Our evaluation sheds light on the trade-offs between database models and query languages for different ICL strategies and LLMs. Last, SM3-Text-to-Query is easily extendable to additional query languages or real, standard-based patient databases.

## 1 Introduction

The health sector is being increasingly digitalized, with data stored in electronic health records (EHR) . In practice, those records can be kept in various forms and systems: (i) traditional relational databases such as PostgreSQL or Oracle implement the _relational data model_ where data is organized into relations (tables) that are collections of tuples (rows). Users access data through the declarative query language SQL; (ii) popular document databases, such as MongoDB, model data as a collection of documents in the _document data model_, providing data access through specialized languages such as the MongoDB Query Language (MQL); (iii) graph databases (triple stores) model data as property graphs (e.g., Neo4j) or semantic RDF graphs (e.g., Ontotext GraphDB), providing interfaces through the query languages Cypher and SPARQL, respectively.

While the relational model and the SQL query language are still the primary choice for EHRs , there has been an increased interest in document and graph database models due to their schema flexibility and natural capacity to interconnect data sources across data silos . E.g., AICCELERATE, a recent large-scale European pilot project on digital hospitals, uses a graph modelto connect various data sources, including traditional EHRs, other hospital data, wearables, and IoT devices . Moreover, RDF graph databases that use SPARQL as the primary query language are widely used in life sciences, with medicine rapidly catching up .

The choice of database and the underlying core data model (relational, document, graph) has a large impact on read/write performance and query complexity. For example, the graph model naturally represents many-to-many relationships, such as connections between patients, doctors, treatments, and medical conditions, whereas relational databases require potentially expensive join operations and complex queries. Document databases have only rudimentary support for many-to-many relationships and aim at scenarios where data is not highly interconnected and stored in collections of documents with a flexible schema . Figure 1 shows an example of a single user question together with the corresponding query statements in four query languages. While these differences have been a known fact in database research and industry, _its implications for the growing number of Text-to-Query systems have surprisingly not been investigated so far._

Text-to-Query systems have seen a recent growth in the number of developed methods and new high scores, mainly due to the transformer architecture and advances in Large Language Models (LLMs) . The idea is compelling: Instead of writing database queries, users express their intends in natural language and a Text-to-Query system with access to the underlying database translates them into valid query statements (e.g., SQL or SPARQL). This is especially relevant in a digital health context in which the domain experts (e.g., nurses, doctors, or admin staff) cannot be expected to write complex queries and currently rely on pre-defined rule conversion systems that limit the potential questions that can be asked .

Existing Text-to-Query datasets and benchmarks have usually been focusing on single database models and query languages. E.g., Spider , WikiSQL , BIRD , ScienceBenchmark , Statbot.swiss  and EHRSQL  (Text-to-SQL) and LC-Quad 2  and Spider4SPARQL , for Text-to-SPARQL, also called Knowledge Graph Question Answering (KGQA). Only recently, in FootballDB , have there been initial attempts to evaluate the data model impact inside a single database system (with different schemas for the same data). Further, there has been initial works across two database models and query languages. E.g., a recent comparison of SQL and SPARQL based on the MIMICSQL dataset  finds a 34-point accuracy difference .

However, _no existing work in and outside the medical domain enables the evaluation of Text-to-Query across multiple core database models and query languages_. This is the goal of our dataset and benchmark. In this paper, we present _SM3-Text-to-Query, a first Multi-Model Medical Text-to-Query Benchmark based on synthetic patient data_. SM3-Text-to-Query provides the following key features:

Figure 1: Differences across query languages and database systems for the same user request.

* **Standard-based and privacy-preserving.** SM3-Text-to-Query has been carefully constructed from Synthea , a synthetic patient data simulator. Thus, there are no privacy implications in the published dataset. In our data schemas, we follow the SNOMED-CT taxonomy , a commonly used medical knowledge graph ontology that is widely applied across institutions and countries, enabling interoperability of health data. _This ensures the wide impact of the benchmark on real-world health use cases._
* **Three database models--four database systems and query languages.** Involving database experts in the process, we design four data representations (schemas) in PostgreSQL, MongoDB, Neo4j, and RDF (GraphDB), and create transformations for them from the Synthea output. These databases represent three core database models: relational, document, and graph model. The databases with _the same data content_ can be accessed through four different query languages: SQL, MQL, Cypher, and SPARQL. SM3-Text-to-Query is, to our knowledge, the first benchmark with these features. _Our chosen database systems and query languages constitute a wide representation of the most popular systems according to DB-Engines Ranking ._
* **Systematic and expandable question generation.** We systematically and manually create a set of \(408\) template questions covering the major entities and properties of the Synthea data. These template questions are then automatically enhanced and enriched to result in a benchmark set of 10K natural language/query pairs for each query language. The enrichment is performed via parameterizable sampling methods to retrieve, for instance, patient names or allergies from the underlying database. Our method is easily extendable through the addition of new templates (e.g., different languages, different questions, paraphrasing) or through plugging in real patient databases modeled according to SNOMED-CT. _This ensures the benchmark is future-proof and can be adjusted to other use scenarios._

## 2 Related Work

In this section, we review related works for Text-to-Query systems with a focus on (i) medical data and (ii) generally relevant benchmarks.

**Medical focus.** MIMICSQL  is derived from the MIMIC-III database , containing 10K unique questions tailored to medical quality assurance tasks. To avoid potential limitations, such as fixed question structures, MIMICSQL underwent a filtering and paraphrasing process performed by expert freelancers. MIMIC-SPARQL  builds on the framework of MIMICSQL  and customizes its question templates to query a modified schema with SPARQL. With a similar structure to MIMICSQL, it provides 10K unique questions tailored to medical QA tasks. Last, EHRSQL  provides a benchmark for text-to-SQL tasks with a focus on electronic health records (EHR). It is based on MIMIC-III and eICU databases, while the 230 question templates are derived from user surveys. Based on these 230 templates, EHRSQL generates 24K questions/query pairs for SQL.

**General benchmarks.** The WikiSQL  dataset is a well-known general Text-to-SQL benchmark that comprises over 80K text/SQL pairs. What makes this dataset noteworthy is the wide distribution of queries over 24,241 tables. Spider  is considered one of the most popular cross-domain text-to-SQL datasets and consists of 10,181 questions with 5,693 unique SQL queries on 200 databases. KaggleDBQA  builds on large-scale datasets such as Spider and WikiSQL to provide a cross-domain dataset with domain-specific data types. BIRD  is a comprehensive resource for question answering (QA) that includes 12,751 unique questions from various repositories such as Kaggle, CTU Prague, and Open Tables and covers 37 subject areas. BIRD targets real-world applications by including complex examples from 95 large databases totaling 33.4 GB. ScienceBenchmark  presents three real-world, domain-specific text-to-SQL datasets. In comparison to other datasets, it reflects the high importance of domain-specific benchmark datasets for real-world text-to-SQL tasks. Last, FootballDB  investigates different database schemas and their impact on Text-to-SQL systems inside a single database. The questions are derived from a real deployment with end users.

Compared to these works, the main novelty of SM3-Text-to-Query is that it is, to the best of our knowledge, the first dataset and benchmark that allows for the evaluation of Text-to-Query systems across three core database models (relational, graph, document) and four query languages (SQL, SPARQL, Cypher, MQL). FootballDB  is comparable in terms of analyzing the schema dependency of Text-to-SQL systems inside a single database. However, it only targets SQL, a single database model (relational), and query language (SQL). For our template-based approach, we take inspiration from EHRSQL . We complement their idea with synthetic data generation following a widely used medical standard (SNOMED ). This makes our benchmark relevant to digital health scenarios worldwide, where databases follow the SNOMED medical naming taxonomy.

## 3 SM3-Text-to-Query Benchmark Construction

Our SM3-Text-to-Query benchmark construction consists of two main steps. First, we construct the database based on synthetic medical data in four data models (Figure 2). Second, we implement a template-based text/query-pair construction approach. The dataset was created over a period of more than a year in the context of a project with health professionals from two university hospitals, medical doctors, nurses, data scientists, and computer scientists who have been working in the respective fields for 5+ years. We constructed the question templates to cover all SNOMED CT entities. The database queries were mainly written by two undergraduates and one PhD student with a computer science and database background and verified by two faculty members.

### Database Construction

#### 3.1.1 Synthetic Patient Data with Synthea

As much of this data only reaches its full potential when it is interoperable across organizations, such as hospitals, insurance providers, and specialists, there has been a move toward standardization in healthcare. Here, SNOMED Clinical Terms (CT) is considered to be the most comprehensive, multilingual clinical healthcare terminology in the world . While there exists a range of clinical EHR datasets such as MIMIC-III  and eICU , they are based on de-identified data from single countries (eICU) or even just single medical centers (MIMIC-III) and do not follow the SNOMED CT taxonomy.

To construct the SM3-Text-to-Query benchmark, we, therefore, choose to build on synthetic but standardized data that we generate through Synthea . Synthea is an open-source, synthetic patient generator that models the medical history of synthetic patients and their electronic health records while being compatible with SNOMED CT. Its generated data includes 18 classes representing various aspects of healthcare, such as allergies, care plans, and medications, and is available in CSV format, FHIR, C-CDA, and CPCDS formats . Synthea employs top-down and bottom-up approaches to generate structured synthetic EHRs throughout a patient's life. E.g., the simulation incorporates models for the top ten reasons for primary care visits and chronic conditions responsible for the most years of life lost in the United States, based on US Census Bureau, Centers for Disease

Figure 2: **Database construction from Synthetic Patient Data**. Synthea uses clinical care maps and statistics to build models of disease progression and treatment, encoded as state transition machines. The synthetic world population is seeded with census data demographics and configuration options, enabling the creation of realistic patient data in 18 classes, which we export as CSVs. We implement custom Extract Transform and Load (ETL) pipelines to transform these CSVs to four database systems and models.

Control and Prevention, and National Institutes of Health reports. Further, international populations can be simulated through provided metadata and configuration files for currently 25 countries .

#### 3.1.2 Data Transformation to different Databases

Based on the Synthea output, we define data schemas/ontologies and implement Extract, Transform and Load (ETL) pipelines for our chosen databases: PostgreSQL (SQL), MongoDB (MQL), Neo4j (Cypher), GraphDB (RDF). For PostgreSQL, we define appropriate data types and consistency constraints (mainly primary and foreign keys). MongoDB, as a document database, does not enforce a strict schema as relational databases. Here, we define a JSON schema following a tree structure with four top-level collections (patients, organizations, providers, payers) and the remaining entities being embedded in these collections. We connect documents across collections through ID references ($lookup operator as equivalent to Join). For Neo4J, we implement ETL by following the guidelines provided by Neo4j Solutions , adapting it due to missing classes and connections in the original configuration files. For RDF, we extend Synthea-RDF  to cover all Synthea attributes and automate the conversion of data from CSV files into RDF as Terse RDF Triple Language (TTL). All four data models can be found in Appendix A.6 and in the supplement material.

### Text/Query-Pairs Construction

To construct a dataset of text/query-pairs, we follow the established template-based approach  in which a set of template questions is augmented with values to scale the dataset without extensive manual efforts. Together with the standard-based Synthea data (SNOMED CT taxonomy), our generation process has the following advantages:

* **Coverage and diversity through Synthea data generation.** Through the use of templates, the benchmark dataset can be automatically adjusted to different Synthea datasets (e.g., for different patient populations). Each template is tagged with the relevant Synthea entities (e.g., patient, claim). Based on this structure, our method allows for the construction of datasets that only cover a subset of entities (e.g., only focusing on an insurance database).
* **Reduced bias in machine learning methods of the task.** By filling the query templates with parameterizable values, various biases of text-to-query methods can be exploited. Thus, we can avoid the respective LLMs overfitting.
* **Standardized evaluation over different systems.** The creation of standardized templates is possible through the implementation of the SNOMED terminology in the Synthea dataset. The benchmark dataset leverages SNOMED attributes, i.e., standardized medical terminology, for the evaluation of queries over different systems and database models. The same template questions can easily be combined with real-patient data following SNOMED medical terminology.

Overall, we create 408 template questions (see supplement material for a full list and Appendix A.4 for an example) in a structured way that is guided by the goal to cover all 18 entity types created by Synthea. The template questions include WH\({}^{*}\) and non-WH questions, factual questions, linking questions, summarization questions, and cause-effect questions. We tag each template question with its related entities and question type.

Last, following previous work , we define 10 non-answerable medical and 5 non-medical questions. These are questions that cannot be answered from data stored in the respective databases but would require additional information. For instance, _What is the marital status of patient Max Miller?_

For each question template, we manually develop the corresponding query in SQL, SPARQL, Cypher and MQL. The queries are then verified by a second expert for correctness. For scaling the template questions, we augment them by automatically inserting values such as IDs, descriptions of diseases, and patient names queried from the database. This data augmentation step is fully configurable and can be used to generate enriched and linguistically diverse text/query pairs for arbitrary Synthea databases and in-the-wild databases following the SNOMED CT standard.

[MISSING_PAGE_FAIL:6]

and SPARQL queries have similar complexity in terms of token count. SPARQL even includes more joins and traversals than existing benchmarks, while MQL has more nesting.

## 5 Baseline Experimental Evaluation

The goal of our experiments is to evaluate how well large language models (LLMs) perform in translating natural language questions into four different query languages provided by our novel benchmark. We restrict ourselves to LLMs as these models dominate the leader boards of popular Text-to-Query benchmarks such as Spider .

### Experimental Setup

For our baseline experiments, we select a set of four common open and closed-sourced LLMs and implement the same in-context learning (ICL) prompting strategies across our four query languages. Here, we follow standard practices in terms of task instruction formulation and the inclusion of schema/ontology information as well as few-shot examples. We take inspiration from Nan et al. who propose general design strategies for enhancing Text-to-SQL capabilities in LLMs , widely adopted in recent applications. Further, Chang and Fosler-Lussier demonstrate the importance of including database schema and content . Liu and Tan suggest using notes and annotations to mitigate unexpected behaviors of LLMs , improving accuracy. Drawing upon these strategies, we developed various prompt templates tailored to the requirements of our experimental settings. The detailed templates applied in the experiments are listed in Appendix A.5.

As open-source LLMs, we select Meta Llama3-8b and Llama3-70b (instruction-tuned variants) . As closed-source LLMs, we select Google Gemini 1.0 Pro  and OpenAI GPT-3.5-turbo-0125 . The closed-sourced models are run via their respective APIs. We run Llama3-8b locally on a single NVIDIA A100 GPU. For Llama3-70b, we use the cloud-hosted model provided by Groq .

We define three prompts with schema information (_w/schema 0-shot_, _w/schema 1-shot_ and _w/schema 5-shot_) and two prompts without schema information (_w/o schema 1-shot_ and _w/o schema 5-shot_). Due to the size of the ontology for SPARQL, we implement a smaller version in which the ontology is summarized as a JSON object with all contained classes, objects, and data properties. Likewise, for MQL and Cypher, we provide the encoded MongoDB document schema and Neo4J graph schema, respectively. The shots are selected using stratified random sampling by considering the categories of the original question templates to ensure a diverse selection. For the prompts that include examples (1-shot/ 5-shot), we perform five runs with different sampling (for further details, see Appendix A.5).

Execution Accuracy (EA).We apply _exact execution accuracy_ (EA), also known as result matching  as our main accuracy metric. EA denotes the fraction of questions within the evaluation set, where the outcomes of both the predicted and ground-truth queries yield identical results relative to the total number of queries. Given two sets, i.e., the reference set \(R\), produced by the execution of the \(n\) ground-truth SQL queries \(Y_{n}\), and the corresponding result set denoted as \(\) obtained from the execution of the predicted SQL queries \(_{n}\), EX can be computed by Equation 1.

\[EA=^{N}(r_{n},_{n})}{N}\] (1)

where \(r_{n} R_{n}\), \(_{n}_{n}\), and \(\) is the indicator function, defined as:

\[(r_{n},_{n})=1&r_{n}=_{n}\\ 0&\] (2)

    & **EHRSQ** & **MIMICSQL** & **SM3\({}_{}\)** & **SM3\({}_{}\)** & **SM3\({}_{}\)** & **SM3\({}_{}\)** \\ 
**Tokens** & **32.63** & 19.45 & 7.28 & 28.56 & 31.35 & 9.93 \\
**Keywords** & **11.74** & 6.38 & 2.43 & 5.08 & 7.41 & 4.06 \\
**Joins/Traversals** & 0.33 & 0.64 & 0.62 & 0.04 & **4.00** & 0.31 \\
**Nesting Depth** & 1.37 & 0.77 & 2.14 & **3.77** & 1.15 & 0.15 \\   

Table 2: SM3-Text-to-Query query statistic compared to recent medical Text-to-SQL datasets.

### Text-to-Query Accuracy

We first evaluate Text-to-Query accuracy for the different prompting strategies, LLMs, and query languages. Table 3 depicts the Execution Accuracy (EA) without schema information (two left-most columns), while the three right-most columns contain results for experiments with schema. The \(\) represents the standard deviation. Numbers are in %. We observe the following four key insights:

* _Schema information helps for all query languages, but not equally._ As expected, the accuracy of w/ schema experiments is higher than that of their w/o schema counterparts. Especially with 1-shot, the models cannot generate correct queries in the majority of cases. However, surprisingly, schema information has a much larger impact on the performance for SQL, Cypher, and MQL (more than doubles the performance for 5-shot compared to w/o schema) than for SPARQL (only slightly higher or equal performance). This indicates that LLMs may have encountered the SNOMED CT ontology and vocabulary during their pre-training phase, as these are standardized and widely published on the web, whereas the specific schemas for SQL, Cypher, and MQL databases are private to each implementation and thus novel to the model, making explicit schema information more crucial for these query languages.
* _Adding examples improves accuracy through ICL for all LLMs and query languages, however, the rate of improvement varies greatly across query languages._ For SQL--the most popular query language--the larger LLMs already achieve \( 40\%\) (w/schema 0-shot) and only improve by \( 10\) points with 5-shots (\( 25\%\) relative improvement). For the "more exotic" query languages (SPARQL, MQL, and partly Cypher), LLMs are often unable to generate a correct query with only the schema information. E.g., for SPARQL, 0-shot is \(<4\%\), while 5-shot can reach up to \(30\%\) (10-fold relative improvement). This indicates again that the model is already proficient in the SQL query language, whereas for SPARQL (and to a smaller extent Cypher and MQL), the model is truly benefiting from ICL by learning how to formulate more correct queries from the provided fixed few-shot examples (even though the examples might not directly be related to the asked question).
* _LLMs exhibit mostly consistent performance patterns across query languages._ Observing w/schema 5-shot results, Llama3-70b achieves the best results for all query languages. GPT-3.5 and Llama3-70b share the 2nd and 3rd place, while the smallest LLM Llama3-8b achieves always the lowest accuracy. Further, some results show a large standard deviation, indicating that the different few-shot example compositions for each run have a large performance impact. To further investigate the impact of few-shot sampling, we explore an advanced similarity-based sampling strategy in Section 5.4. An overall even higher standard deviation can be observed for MQL. We trace this additional variance to inconsistent output variations in LLMs (see also Section 6).
* _LLMs have varying levels of knowledge across different query languages._ We suspect that this can be traced back to their training data. A large resource of such training data has been Stack Overflow . When we search Stack Overflow for tags (indicated with []) related to our four query languages, we get the following numbers (15.08.2024): [SQL]: 673K posts; [SPARQL]: 6K posts; [MongoDB, MQL]: 176K posts; [Cypher, Neo4JJ]: 33K posts. Relating the number of posts to our "w/ schema 0-shot" results (we want to leave the impact of few-shots ICL out of this), we see that SQL performs best (best model: 47.05% ), Cypher and MQL perform average (best models: 34.45% and 21.55%), while SPARQL performs worst (best model: 3.3%). These results correlate to the post frequency on Stack Overflow and support results by  that find a statistically significant impact on the correctness of LLM answers based on question popularity and recency. An exception to this pattern is MQL as it is under-performing Cypher. We suspect that this has to do with the fact that Cypher queries contain much fewer tokens and language keywords than MQL (only 25% of tokens and 50% of keywords, see Figure 3).

### Per-category Results

Next, we analyze the performance on a per-category level based on the entity-tagged template questions. For that, we look at the results for _w/ schema 0-shot_ and _w/ schema 5-shot_ to observe the impact of ICL through few-shot examples for our 19 question categories, our four query languages, and four LLMs. Figure 4 (top) shows the execution accuracy based on the w/ schema 0-shot results, while Figure 4 (bottom) shows the mean results for w/ schema 5-shot experiments.

We observe a clear difference between high and low-resource query languages. Despite the available schema, correct SPARQL and MQL queries can mostly not be generated for all LLMs without few-shot examples (top, 3rd, and 4th columns). For these low-resource languages, performance improves substantially with ICL. We also observe differences that can be traced to model size and potential training data. Llama3-8b, the smallest model, struggles even with examples to produce correct SPARQL queries. Both Llama models seem to encode more knowledge about MQL than the other LLMs (highest schema 0-shot results). For MQL, we see the highest performance for questions related to top-level entities (i.e., not nested objects), such as _patients_, _organizations_, _providers_. For non-answerable questions, we observe that non-medical questions have a higher accuracy than medical ones. The capability of an LLM to recognize unanswerable questions varies across query languages, even with the same prompt instruction.

### Similarity-based few-shot sample selection

Last, we implement a similarity-based approach using a BM25  retriever that, on a per-question basis, retrieves the five most similar question-query pairs from the training data. We use these

Figure 4: Execution Accuracy (EA) for our 19 different question categories for _w/ schema 0-shot_ (top) and _w/ schema 5-shot_ (bottom).

    &  &  \\   & **w/o schema 1-shot** & **w/o schema 5-shot** & **w/ schema 0-shot** & **w/ schema 1-shot** & **w/ schema 5-shot** \\  &  \\  Llama3-8b & 4.20 (\(\)5.6) & 10.81 (\(\)9.87) & 22.55 & 23.27 (\(\)1.05) & 27.49 (\(\)1.57) \\ Gemini 1.0 Pro & 4.47 (\(\)4.88) & 21.65 (\(\)11.10) & 38.60 & 38.37 (\(\)3.31) & 49.32 (\(\)3.63) \\ GPT 3.5 & 1.45 (\(\)0.99) & 11.71 (\(\)12.77) & 42.20 & 48.92 (\(\)6.72) & 56.30 (\(\)2.36) \\ Llama3-70b & 7.35 (\(\)7.59) & 20.14 (\(\)13.14) & 47.05 & 51.06 (\(\)1.75) & 57.50 (\(\)2.91) \\   \\  Llama3-8b & 3.09 (\(\)2.70) & 4.18 (\(\)9.4) & 0.05 & 1.51 (\(\)1.92) & 4.27 (\(\)8.92) \\ Gemini 1.0 Pro & 3.23 (\(\)1.95) & 11.99 (\(\)7.87) & 2.85 & 7.76 (\(\)4.65) & 26.32 (\(\)5.60) \\ GPT 3.5 & 6.95 (\(\)5.48) & 25.32 (\(\)4.57) & 3.30 & 7.88 (\(\)4.78) & 23.58 (\(\)8.09) \\ Llama3-70b & 7.37 (\(\)4.46) & 27.14 (\(\)2.63) & 1.00 & 10.26 (\(\)6.89) & 30.49 (\(\)1.82) \\   \\  Llama3-8b & 9.43 (\(\)1.12) & 19.64 (\(\)3.35) & 2.75 & 15.31 (\(\)11.28) & 34.89 (\(\)5.34) \\ Gemini 1.0 Pro & 13.80 (\(\)1.67) & 22.91 (\(\)1.38) & 23.45 & 39.74 (\(\)2.99) & 53.84 (\(\)4.09) \\ GPT 3.5 & 10.37 (\(\)4.84) & 18.08 (\(\)1.05) & 16.35 & 29.87 (\(\)3.44) & 41.12 (\(\)2.85) \\ Llama3-70b & 16.04 (\(\)2.40) & 25.25 (\(\)1.50) & 34.45 & 43.06 (\(\)4.57) & 57.07 (\(\)4.41) \\   \\  Llama3-8b & 2.64 (\(\)3.35) & 4.62 (\(\)6.56) & 9.45 & 6.71 (\(\)6.55) & 11.33 (\(\)15.06) \\ Gemini 1.0 Pro & 5.25 (\(\)2.47) & 13.25 (\(\)3.25) & 3.40 & 18.53 (\(\)1.67) & 30.65 (\(\)7.19) \\ GPT 3.5 & 1.49 (\(\)3.30) & 5.36 (\(\)5.17) & 3.50 & 26.26 (\(\)1.64) & 35.06 (\(\)18.74) \\ Llama3-70b & 8.86 (\(\)2.09) & 17.91 (\(\)4.52) & 21.55 & 33.83 (\(\)8.54) & 40.35 (\(\)17.03) \\   

Table 3: Execution Accuracy of different LLMs **without** and **with schema** information for **test data**question pairs as few-shot examples instead of the fixed few-shots used before. We use the w/ schema prompt with GPT-3.5. This greatly improves results to up to 88.55% for SQL (see Table 4), which is in line with related results for the EHRSQL dataset in EHRXQA . As an end-to-end comparison, EHRXQA achieves an execution accuracy of 92.9%, an improvement by 62.9 points from their fixed-shot strategy. This is consistent with our results, where SQL execution accuracy improves by 32.25 points to 88.55%. However, we can also see that even with a state-of-the-art ICL method, SPARQL, Cypher, and MQL cannot reach the same performance as SQL (MQL has the highest score with 78.70%). This indicates the need for more research, such as developing better ICL methods and potentially schema encodings. Overall, it reinforces the motivation and strengthens the necessity of our work in terms of creating a new multi-model Text-to-Query benchmark to work on these problems and to extend the research scope from "Text-to-SQL" to multi-model "Text-to-Query".

## 6 Discussion and Limitations

While SM3-Text-to-Query is the first benchmark across four different query languages, we note several limitations, some of which provide the potential for further research.

First, compared to other datasets , our question templates were only created with guidance from health professionals, but not directly formulated by them. Our dataset is synthetic, based on simulated patient data, with the benefit of flexibility and no privacy issues. In the future, we plan to extend our question set through crowd-sourcing as part of an ongoing Swiss digital health project .

Second, SM3 currently only contains English questions and database values. We believe that the addition of multilingual questions and databases could be a valuable extension to our benchmark.

Third, while our questions cover all main entities in the dataset, their corresponding queries might be too easy in some query languages (e.g., SQL and Cypher require, on average, less than 10 tokens; see Figure 3). There is the potential to include temporal templates as in  to increase query complexity.

Last, we experience that LLMs exhibit large output variations for the same prompt and their generations can be inconsistent across query languages. We implemented extensive data-cleaning logic to extract the predicted query from the LLM output. These outputs varied across models but also across languages: while GPT-3.5 follows instructions well for SQL, it does not for MQL, despite the same structured prompt (see Appendix A.3 for examples of encountered issues). Further research should focus on prompt optimization across database models, potentially using LLMs as optimizers .

**Potential negative societal impacts.** There are no direct negative societal impacts associated with our dataset. It will help researchers and people in industry to improve their Text-to-Query systems, thereby democratizing data access to wider user groups.

## 7 Conclusion

This paper provides, to the best of our knowledge, the first multi-model Text-to-Query dataset and benchmark that allows for the evaluation of Text-to-Query systems across three core database models (relational, graph, document) and four query languages (SQL, SPARQL, Cypher, MQL). Our dataset is based on synthetic medical data generated through Synthea , follows an international medical ontology standard (SNOMED ), and can be easily extended through further template questions or by exchanging the synthetic data through standard-conform real patient data. SM3-Text-to-Query will be essential to develop and test the next generation of Text-to-Query systems that appear with increasing frequency thanks to the progress in transformer-based large language models. All our code and data are available at https://github.com/jf87/SM3-Text-to-Query.

  
**Query Language** & **EA in \%** & **Improvement to fixed 5-shot** \\  SQL (PostgreSQL) & 88.55 & +32.25 \\ SPARQL (GraphDB) & 75.75 & +52.17 \\ Cypher (Neo4j) & 78.30 & +37.18 \\ MQL (MongoDB) & 78.70 & +43.64 \\   

Table 4: Advanced In-Context Learning (ICL) Method: BM25-based few-shot selection (5-shot) determined by question similarity to the training data.