# Self-supervised Learning to Discover Physical Objects and Predict Their Interactions from Raw Videos

Self-supervised Learning to Discover Physical Objects and Predict Their Interactions from Raw Videos

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The ability to discover objects from raw videos and to predict their future dynamics is crucial for achieving general intelligence. While existing methods accomplish these two tasks separately, i.e., learning object segmentation with fixed dynamics or learning dynamics with known system states, we explore the feasibility of jointly accomplishing the two together in a self-supervised setting for physical environments. Critically, we show on real video datasets that learning object dynamics improves the accuracy of discovering dynamical objects.

## 1 Introduction

Cognitive science researchers have studied how humans understand both scenes and events since the 1970s . Inspired by these studies, AI researchers have been striving to build intelligence systems with similar abilities . Most recent work pursues these two objectives _separately_, e.g., supervised and unsupervised object discovery  and learning physics and dynamics from data .

Meanwhile, inspired by cognitive science research about how infants can develop their perceptual system and learn the physical world simultaneously in a self-supervised fashion by observing and interacting with moving objects , recent studies hypothesize that such joint learning of object discovery and dynamics should also be feasible for machines. In particular, recent progress on object discovery from motion, e.g., , shows that the _existence_ of dynamics prediction, even when the dynamical models are primitive, improves the accuracy of object discovery.

In parallel, machine learning for physics has achieved significant progress in recent years, with applications to physical property prediction , protein or material generation , particle-based simulation , among many others. Notably, neural ODE  and its successor  have demonstrated strong capabilities of neural networks in approximating dynamical systems. In most settings, however, the states of physical objects are assumed to be given, with only a few studies, e.g., , attempted to learn state of objects from video.

In this work, we show that the accuracy of object discovery in physical environments can be further improved when the dynamical model is trainable and represents a hypothesis space that covers the ground truth dynamics, and on the other hand, an incorrect assumption about dynamics may result in faulty segmentation of objects. As shown in Figure 1, our model is based on a factorized generative model for object discovery and a trainable neural ODE for dynamics prediction . The component linking the object discovery and the dynamical model is a state encoder, which maps a time sequence of object masks to object states such as position, orientation, and their time derivatives. Unlikeprevious studies where the dynamics is fixed, our model introduces the challenge of jointly learning for object discovery and for dynamics prediction.

The key contributions of the paper are as follows:

* We present effective learning architecture, loss, and algorithm for solving the challenge posed by the joint learning task.
* We empirically test our model on two video datasets: real-world double pendulum, and real-world 3D block tower falling. We show that through joint learning of object discovery and dynamical model, our method outperforms recent object segmentation methods that use factorized generative model [7; 41] or primitive dynamics [18; 53]. The learned dynamical model can also predict the movement of objects in long-term.

## 2 Method

By integrating a trainable dynamical model into an object discovery framework, our model jointly learns object masks and predicts object interactions. As shown in Figure 1, The learning framework is composed of: (1) a mask encoder that encodes an input video frame into object masks using attention modules, (2) a component variational autoencoder (VAE) that encodes the concatenated image and object masks into object-wise latent representations, which can be decoded back into an image and reconstructed masks, (3) a prefixed state encoder that computes the center of mass, orientations, and their time derivatives for each masked object, (4) a dynamical model that evolves states along time.

**The mask encoder.** Let a video with \(T\) time frames be \(=\{I^{0},...,I^{T}\}\), where \(I^{t}^{H W 3}\) is an RGB image with height \(H\) and width \(W\). A mask encoder, denoted by \(f_{}()\) with trainable parameters \(\), encodes an image into one background mask and \(C\) object masks: \(f_{}(I^{t})=^{t}\{m_{0}^{t},m_{1}^{t},...,m_{C}^{t}\}\), where \(m_{c}^{t}^{H W}\) and \(m_{0}^{t}\) represents the background mask. Since masks should cover all pixels in the scene, the sum of all masks is 1: \(_{c=0}^{C}m_{c}^{t}=J_{H,W}\). Let \(q_{c}\) represent the area unexplored until iteration \(c\). To discover objects in the scene, we adopt the method in . The attention module \(_{}\) recurrently discovers objects through

\[m_{c}=q_{c-1}(_{}(I,q_{c-1})),q_{c}=q_{c-1}(1-_{ }(I,q_{c-1})),\; c=1,...,C,q_{0}=.\] (1)

**The component VAE.** For the \(c^{th}\) mask, the encoder encodes the image \(I\) to a latent posterior distribution, denoted as \(p_{}(z_{c}|I,m_{c})\). The latent vector \(z_{c}\) for each mask \(m_{c}\) is decoded back to both the image likelihood \(p_{}(I_{c}|z_{c})\) and the mask prediction likelihood \(p_{}(d_{c}|z_{c})\). The reconstructed image is a summation over all channels \(I=_{c=0}^{C}m_{c}I_{c}\). \(\) and \(\) are trainable component VAE encoder and decoder parameters.

**The state encoder.** computes the state (\(x_{c}^{t}\)) based on each object mask (\(m_{c}^{t}\)): \(f_{s}(m_{c}^{t})=x_{c}^{t}\). The state is composed of the center of mass \(p_{c}^{t}^{2}\), velocity \(_{c}^{t}^{2}\), orientation \(r_{c}^{t}\), and angular velocity \(_{c}^{t}\) of each object. Therefore \(x_{c}^{t}^{6}\). In our implementation, the state encoder first extracts pixel coordinates of an object based on its mask and then computes the state from these

Figure 1: Our framework consists of four components: mask encoder, component VAE, state encoder, and dynamical model.

coordinates. The collection of coordinates \(l_{c}^{t}\) is computed by an element-wise multiplication of mask \(m_{c}^{t}\) with a 2D coordinate grid \(g[-1,1]^{H W 2}\). The center of mass \(p_{c}^{t}\) is retrieved as the mean of \(l_{c}^{t}\), and the orientation \(r_{c}^{t}\) as the direction of the principle axis of \(l_{c}^{t}\) through differentiable singular value decomposition. The time derivatives \(_{c}^{t}\) and \(_{c}^{t}\) are computed by a finite difference using the position and orientation of the current and the previous time steps: \(_{c}^{t}=p_{c}^{t}-p_{c}^{t-1}\), \(_{c}^{t}=r_{c}^{t}-r_{c}^{t-1}\). Note that the state encoder is a non-trainable differentiable program, which is able to backpropagate gradients from the dynamical model back to the mask encoder.

**The dynamical model.** The dynamical model \(f_{}\) predicts future states given the current state: \(x^{t+ t}=f_{}(x^{t}, t)\), where \(\) are trainable model parameters and \( t\) is a time span. The state \(x^{t}\) is concatenated by states of each mask, denoted as \(x^{t}=[x_{1}^{t},...,x_{C}^{t}]^{C 6}\). \(f_{}\) is composed of a neural ODE: \(^{t}=f_{ode}(x^{t}, t)\), and a differentiable ODE solver (e.g., Euler or Runge-Kutta):

\[}=f_{}(x^{t}, t)=(f_{ode},x^{ t},t,t+ t).\] (2)

Future object masks can be predicted by applying affine transformations using the predicted states:

\[^{t+1}}=(^{t}},^{t+1}}, ^{t}}),^{0}}=m_{c}^{0},^{0}}=x_{c}^ {0}, c 1,\] (3)

where \(\) is a differentiable affine transformation given rotation and translation .

**Training losses.** In a nutshell, the training loss consists of (1) a time-independent reconstruction loss regarding the mask encoder and the VAE, and (2) a time-dependent dynamics loss that is dependent on both the mask encoder and the dynamical model. The reconstruction loss includes a standard VAE loss and a KL regularization. The VAE loss has two terms: the first term is the negative log-likelihood of the generated image distribution, denoted as \(_{}=-_{c=0}^{C}m_{c}p_{}(I|z_{c})\); the second term is the KL divergence of the learned latent distribution from the prior, denoted as \(_{}=KL(p_{}(z_{c}|I,m_{c})||p(z))\), where the prior follows a standard normal distribution: \(p(z)=(0,1)\). The KL regularization loss is the KL divergence of the encoded mask distribution from the decoded mask prediction distribution, denoted as \(_{,}=KL(p_{}(d_{c}|I)||p_{}(m_{c}|z_{c})\). Together, the reconstruction loss is:

\[_{recon}=_{,,}_{}+ _{,}+_{,}\] (4)

The dynamics loss is composed of a state loss and a mask loss. The state loss measures the difference between the state encoded from an image and the state predicted by the dynamical model using past encoded states. Through preliminary experiments, we notice that the state loss alone may lead to a trivial solution during training convergence, where states are both encoded and predicted as being constant, therefore minimizing the state loss without learning the actual dynamics. To avoid this, we introduce an additional mask loss that measures the difference between the masks encoded from the image and those evolved by the dynamical model. Thus, the performance of dynamics prediction is measured in both the state and the mask spaces. Together, the dynamics loss is:

\[_{dynamics}=_{,}_{t=1}^{T}(\|}-x^{t}\|_{2}+_{c=1}^{C}\|^{t}}-m_{c}^{ t}\|_{2}).\] (5)

The overall training loss is a weighted sum of the reconstruction, dynamics, and regularization loss:

\[_{total}=_{recon}+_{dynamics}.\] (6)

## 3 Experiments

**Experiment settings.** We conduct experiments on video datasets of two physical environments: a video-recorded double pendulum dataset, and a video-recorded 3D block tower dataset. The double-pendulum dataset is video recorded from actual experiments and shared by . The 3D block tower dataset, introduced in , provides a collection of videos showcasing block stacks that may or may not fall. The dataset comprises 516 videos, each featuring 2 to 4 blocks of various colors. To quantify the object discovery performance, we employ the intersection over union (IoU) metric, which compares the encoded masks and ground truth segmentation.

**Real-world video recording of double pendulum.** We compare our method against baselines in Figure 2(a). Our method performs the best, with only a few pixels on the edge of the blue pendulum being mistakenly grouped with the gray pendulum, as shown in Figure 2(b). We note that while our model achieves low dynamics prediction error in the state space (Figure 2(c)), it has limited understanding of geometric relations of objects (Figure 2(d)), leaving room for improvement.

**3D Real-world Block Tower.** To compute 3D states from 2D masks, we first extract 2D states from our state encoder and then project them to 3D using the back-projection model pretrained by . Since the number of objects in the scene can vary, we choose to measure the detection performance of models as well as the object segmentation IoU for evaluation.

We compare our method with baselines in Figure 3. We observe that Monet tends to group objects with similar colors together, such as the blue and green blocks, and occasionally misclassifies light or dark regions as part of the background. Podnet exhibits good object detection performance but encounters challenges in object discovery, as shown in Figure 3. Podnet struggles with accurately delineating the boundary between the green and blue blocks. In the second row, it misidentifies a shadow as an object rather than perceiving it as part of the background. Additionally, in the third row, it fails to detect a portion of the yellow block. In comparison, our model achieves more accurate object detection and object discovery. This improvement highlights the effectiveness of incorporating a trainable nonlinear dynamical model into the segmentation framework.

## 4 Conclusion

In this work, we present a model that decomposes images into multiple objects and predicts the dynamics of these objects. We show that ill-posed assumptions of dynamics may result in false object discovery. Our model with trainable nonlinear dynamics is capable of accurately discovering objects while predicting their future movements. For future work, we envision an extension to interactions among non-rigid objects that require both explicit and implicit state encoding for time-variant shape and color changes (e.g., cell migration).

Figure 3: The quantitative and qualitative object discovery on the block tower dataset.

Figure 2: The quantitative and qualitative object discovery and dynamical prediction result on the double pendulum dataset. (a) The quantitative object discovery result; (b) The qualitative object discovery result; (c) The state loss over time; (d) The qualitative dynamical prediction results.