# LibAMM: Empirical Insights into Approximate Computing for Accelerating Matrix Multiplication

Xianzhi Zeng\({}^{1,2}\)1  Wenchao Jiang\({}^{3}\)  Shuhao Zhang\({}^{1}\)

\({}^{1}\) National Engineering Research Center for Big DataTechnology and System

Services Computing Technology and System Lab

Cluster and Grid Computing Lab

School of Computer Science and Technology

Huazhong University of Science and Technology, Wuhan, 430074, China

\({}^{2}\) Nanyang Technological University

\({}^{3}\) Singapore University of Technology and Design

shuhao_zhang@hust.edu.cn

###### Abstract

Matrix multiplication (MM) is pivotal in fields from deep learning to scientific computing, driving the quest for improved computational efficiency. Accelerating MM encompasses strategies like complexity reduction, parallel and distributed computing, hardware acceleration, and approximate computing techniques, namely AMM algorithms. Amidst growing concerns over the resource demands of large language models (LLMs), AMM has garnered renewed focus. However, understanding the nuances that govern AMM's effectiveness remains incomplete. This study delves into AMM by examining algorithmic strategies, operational specifics, dataset characteristics, and their application in real-world tasks. Through comprehensive testing across diverse datasets and scenarios, we analyze how these factors affect AMM's performance, uncovering that the selection of AMM approaches significantly influences the balance between efficiency and accuracy, with factors like memory access playing a pivotal role. Additionally, dataset attributes are shown to be vital for the success of AMM in applications. Our results advocate for tailored algorithmic approaches and careful strategy selection to enhance AMM's effectiveness. To aid in the practical application and ongoing research of AMM, we introduce LibAMM --a toolkit offering a wide range of AMM algorithms, benchmarks, and tools for experiment management. LibAMM aims to facilitate research and application in AMM, guiding future developments towards more adaptive and context-aware computational solutions.

## 1 Introduction

Matrix multiplication (MM) is essential across computational domains, particularly in machine learning and scientific simulations. While efforts to improve MM's performance and scalability  underscore its importance, MM can dominate up to 90% of processing time in some applications, posing a significant bottleneck. Approximate matrix multiplication (AMM)  offers a solution by trading exact accuracy for increased efficiency in contexts where absolute precision is not critical, such as machine learning inference from VGG-like models  to GPT-3 LLMs . One may further combine AMM with advances in algorithmic design , parallel and distributed computing , and hardware technology  to mitigate the computational demands of traditional MM, highlighting its potential to revolutionize MM-intensive applications.

The advancement of AMM methods has brought forth a range of techniques designed to meet various computational challenges. These techniques strategically utilize approximations to enhance matrix operation efficiency [13; 16]. Central to AMM is the concept of simplifying computations by modulating calculation detail, prioritizing matrix elements vital to the outcome and downplaying lesser ones [34; 2]. This approach embodies a crucial trade-off: it lowers computational load in exchange for reduced accuracy, a compromise finely adjusted through parameter \(\). AMM encompasses three principal approximation strategies: pruning-based methods that cut superfluous calculations for better efficiency [21; 13; 17], extraction-based techniques that identify and leverage key elements or patterns to streamline computations [36; 29; 2; 23; 6; 25], and hybrid approaches that merge these methods [22; 34; 9; 31].

Despite the progress in AMM, a comprehensive and impartial comparison remains elusive, often leading to confusing and sometimes contradictory advice for algorithm selection and task-specific modifications. This confusion stems from several factors: First, the appeal of AMM across various downstream tasks, each favoring a different optimal approach, results in varied conclusions. For example, Adelman et al.  favor pruning-based AMM for machine learning training, whereas Blalock et al.  recommend extraction-based AMM for inference tasks. Second, the evaluation of some AMM algorithms lacks breadth over representative workloads. Mroueh et al.'s  assertion that extraction-based AMM yields only minor inaccuracies is primarily based on synthetic datasets, which may not reflect real-world distribution complexities. Third, inconsistencies in AMM implementation and the standards for baselines raise further issues. The mixture of just-in-time (JIT) and static compilation methods [1; 6; 29] complicates fair performance evaluations among AMM implementations. Additionally, the use of manually coded nested loop MM as a baseline by some studies [36; 25; 16] diverges from modern data science practices, which often employ optimizations like cache-aware data loading and SIMD instructions .

To address these gaps, our study delves into a comprehensive empirical analysis of AMM techniques across key algorithmic dimensions. We structure our investigation around three main axes: (a) We conduct a unified static compilation analysis of twelve AMM algorithms alongside two benchmark MM baselines to measure their performance and efficiency. (b) We leverage eight real-world datasets from a spectrum of disciplines to test the versatility and robustness of AMM strategies. (c) We examine four statistics and machine learning applications to assess AMM's practical utility in various downstream tasks. Our methodology intentionally avoids optimizations that cater to specific algorithms or hardware setups, focusing instead on principles with broad applicability [25; 16]. This approach aims to provide a wide-ranging and insightful examination of the AMM domain, highlighting the diverse factors critical for progress in the field. In our study, we discovered insights crucial for advancing MM acceleration via approximation. Key takeaways from our experiments include:

* Among three approximation strategies, **pruning-based and hybrid AMM is notably more beneficial** than extraction-based AMM (Section 3.1).
* For all evaluated AMM, **minimizing memory overhead is the key** to practical performance benefits, especially when further leveraging hardware accelerations like GPUs (Section 3.2).
* For all evaluated AMM, the **accuracy greatly depends on dataset attributes** like value skewness and non-zero distribution, requiring improvements of the error bound (Section 3.3).
* In downstream tasks where approximate computing is allowed, **pruning-based and hybrid AMM significantly outperform extraction-based** AMM. Specifically, they are superior in both reducing processing latency and conducting the task-aligned approximation with less error (Section 3.4).

To support and inspire ongoing and future research, we introduce LibAMM, a framework within the PyTorch ecosystem. LibAMM is offered as an open-source tool at [https://github.com/intellistream/LibAMM](https://github.com/intellistream/LibAMM). It aggregates prevalent AMM algorithms, benchmark datasets, and scripts to easily reproduce our experimental results.

## 2 Preliminary

### Problem Formulation

Let \(A^{M K}\) and \(B^{K N}\) denote two matrices intended for multiplication (denoted as \(MM()\)), aiming to calculate the product \(C=MM(A,B)^{M N}\). With AMM, the objective shifts towards computing an approximation \(^{M N}\), which seeks to balance computational efficiency with the fidelity of the approximation to \(C\). Efforts in AMM focus on developing an algorithm \(AMM()\)that takes \(A\) and \(B\) as inputs and applies approximation techniques to produce \(\) with reduced computational demand. The result of this process is expressed as \(=AMM(A,B)^{M N}\), with each element \(_{ij}\) signifying the approximated value corresponding to the element \(c_{ij}\) in the traditional product \(C\). The critical performance metrics for \(\) are _Processing Latency (\(\), \(\) Error (\(\))_, and _Approximation Impact Factor_ (\( E\)). \(l\) indicates the time efficiency of \(\), by measuring the time from \(A\) and \(B\) are presented to \(\) is eventually produced. \(\), calculated by \(-C\|_{F}}{\|C\|_{F}}\), measures the _frobenius normalized accuracy_ of \(\) to standard \(\). The \( E\), calculated through \(E_{MM}-E_{AMM}\), contrasts the downstream application error when employing \(\) (\(E_{MM}\)) against that when using \(\) (\(E_{AMM}\)). This metric effectively measures the impact of \(\) on prediction accuracy within downstream tasks, capturing both the potential benefits and drawbacks.

### Existing \(\) Revisit

Based on how to handle the matrix information, we broadly classify \(\) algorithms into three categories: pruning-based, extraction-based, and hybrid. We summarize representative \(\) algorithms and the \(\) baselines in Table 1, more related work is discussed in Section 4.

#### 2.2.1 Pruning-based \(\)

Pruning-based \(\) emphasizes the selective removal of redundant information from matrices. This approach can be implemented at two levels of granularity: bit-wise pruning and element-wise pruning. Bit-wise pruning involves compressing each matrix element by using fewer binary bits. A notable example is \(8\), which quantizes 32-bit floating-point elements into 8-bit signed integers. Conversely, element-wise pruning keeps the entire binary representation for certain elements and completely discards others. Techniques for achieving this include CRS (column row sampling) and count sketching mechanisms used in CS (count sketch-based \(\)). Pruning-based \(\) algorithms are lightweight, but also reliant on the nature of the underlying data.

#### 2.2.2 Extraction-based \(\)

Extraction-based \(\) focuses on identifying and utilizing higher-level characteristics from matrix elements. It preserves these characteristics in intermediate structures, which are then used to facilitate \(\) computations. This approach is favored because operations on these intermediate structures are significantly faster than traditional \(\) processes. Various matrix attributes serve to accomplish this goal. Frequent Direction-based methods, such as CoOFD, iteratively isolate significant singular values directly from the input matrices. In contrast, the BlockLRA algorithm applies singular value decomposition to separate blocks within the matrices, offering a balance between precision and computational efficiency compared to comprehensive singular value decomposition as in CoOFD. FastJLT employs a different tactic by extracting Johnson-Lindenstrauss (JL) embedding properties of the matrices through Walsh-Hadamard transformations. Beyond leveraging intermediate matrix structures, incorporating a codebook that catalogues K-nearest neighbor (KNN) centroids of matrix rows or columns also facilitates effective information extraction for \(\). This technique is implemented in VQ (vector quantization) and PQ (product quantization), with PQ typically

  _Category_ & _Algorithm Name_ & _Descriptions_ \\   Purning- \\ based \(\) \\  } & INTR  & Purning 32-bit into 8-bit \\   & CRS  & Purning elements by sampling \\   & CS  & Purning elements by sketching \\   Extraction- \\ based \(\) \\  } & CoOFD  & Extracting singular value, for entire matrices \\   & BlockLRA  & Extracting singular value, for blocks \\   & FastJLT  & Extracting JL embeddings \\   & VQ  & Extracting KNN centroids \\   & PQ  & Similar to VQ, more efficient codebook \\   & RHP  & Randomized JL embeddings extraction \\   & SMP-PCA  & Similar to RIP, scaling values for higher accuracy \\   & WeightedCR  & Extract the weight information during sampling \\   Baseline \(\) \\  } & ToGoFW  & Extract the median and select the optimal after sketching \\   & NLMM  & The manual, brute-force, nested loop implementation of \(\) \\   & LTMM  & LibTorch’s optimized implementation of \(\) \\  

Table 1: \(\) algorithms and \(\) baselines investigatedachieving faster processing times for larger matrices by optimizing the number of KNN centroids through the Cartesian product of subspaces. While extraction-based methods theoretically maintain higher accuracy by preserving essential information during feature extraction, the computational and memory demands of this extraction process may surpass those of standard MM.

#### 2.2.3 Hybrid AMM

Hybrid AMM methods combine pruning and extraction techniques to balance processing latency with accuracy. These methods aim to accelerate feature extraction while allowing for selective information pruning to enhance efficiency. For instance, RIP and SMP-PCA introduce randomness to the Johnson-Lindenstrauss (JL) embeddings extraction process, which is inherently deterministic in approaches like FastJLT, achieving a speed boost. SMP-PCA distinguishes itself by incorporating an additional value scaling step post-randomly pruned JL transform, which theoretically yields higher accuracy compared to RIP. Other hybrid AMM strategies focus on refining element-wise pruning through targeted feature extraction. For example, WeightedCR leverages extracted weight information for more informed sampling, presenting a significant improvement over the indiscriminate sampling seen in CRS. Similarly, the TugOfWar method identifies the optimal sketch from a set of random sketches, prioritizing the retention of information over the more arbitrary selection found in CS. By leveraging the strengths of both pruning and extraction, hybrid AMM methods stand to deliver superior performance, striking an optimal balance between speed and precision.

### Tuning Knob \(\) of AMM Algorithms

The tuning parameter \(\) plays a crucial role in managing the trade-off between computational efficiency (\(l\)) and accuracy (\(\))in AMM algorithms. This section elucidates the influence of \(\) across a spectrum of AMM methodologies, noting that \(\) has no bearing on INT8, LTMM, and NLMM. The latter two algorithms, being precise MM implementations, and INT8, although an AMM algorithm, do not utilize this adjustable parameter.

VQ and PQ showcase an innovative approach by adjusting the number of K-nearest neighbor (KNN) centroids relative to the row count of the input matrix \(A\) through \(\). Unlike other AMM methods that rely on intermediate matrices, these algorithms employ codebooks for feature encapsulation, linking each KNN centroid to a unique code within the codebook. This adjustment effectively moderates the codebook's capacity and computational overhead, striking a balance between latency and precision.

For feature extraction-focused algorithms like CoOFD and FastJLT, \(\) is instrumental in determining the dimensionality of the feature space extracted in relation to input matrix \(A\)'s column volume. By converting singular values or JL embeddings of \(A,B\) into intermediate matrices, the size of which is configurable through \(\), these algorithms propose a trade-off: smaller intermediate structures facilitate quicker computations at the expense of accuracy, while larger configurations promise enhanced accuracy at the cost of increased computational time. This principle is similarly applicable to hybrid AMM approaches such as RIP and SMP-PCA, which incorporate randomized optimizations in the feature extraction phase to improve efficiency.

In the context of BlockLRA, \(\) significantly impacts the sizing of the feature extraction matrix in proportion to the eigenspace of \(A,B\), diverging from the full matrix consideration to a block-wise feature extraction perspective. This nuanced application of \(\) alters the dimensionality of intermediate matrices, thereby modifying computational dynamics.

Lastly, for CRS, CS, WeightedCR, and TugOfWar, the parameter \(\) governs the proportion of matrix elements preserved after pruning, employing sampling or sketching techniques to forgo computations on specific elements within \(A,B\). The allocation determined by \(\) serves to represent the characteristics of the majority pruned, leveraging statistical properties such as mean or variance to approximate the impact of excluded elements.

## 3 Empirical Studies

In the following section, we first introduce experimental configurations and then present the results.

**Evaluation Configurations.** Our experimental framework is meticulously designed to ensure a comprehensive and equitable assessment of AMM algorithms across a spectrum of real-world and synthetic datasets. To this end, we delineate our evaluation methodology under two principal components: the datasets employed and the detailed implementation nuances of AMM algorithms. In the evaluation, we set \(\) as \(10\%\) if not otherwise specified.

**Datasets.** The core of our benchmark suite is derived from MatrixMarket , encapsulating a diverse array of **real-world workloads** including but not limited to _ECO_, _DWAVE_, _AST_, _UTM_, _RDB_, _ZENOS_, _QCD_, and _BUS_, detailed in Table 2. These datasets are preprocessed to normalize matrix elements within the range of \(-1\) to \(+1\), covering a wide breadth of applications from economic modeling (_ECO_) to power flow analysis (_BUS_). Specifically, we linearly align the maximum value to 1 and minimum value to -1, and we leave the more complicated and application-specific normalization such as L2 Normalization  for future works. Complementing these real-world datasets, we also generate **synthetic workloads** using LibTorch functionalities like _torch::rand_.

**Downstream Tasks.** We examine a suite of downstream tasks where the integration of AMM is particularly appealing. These tasks include _Principal Component Analysis_ (PCA) , _Machine Learning Training_ and _Inference_ phases, and _Unitary Transformation_. Detailed descriptions of those applications are presented in Appendix A. Note that, while some pruning-based AMM methods under element quantization like INT8 can seamlessly integrate into complex models, such as transformer-based large language models , incorporating other AMM strategies (e.g., CRS or SMP-PCA) into these advanced models poses a significant challenge. As such, we follow  for training and  for inference, and leave more intricate models in future research.

Table 3 presents an analysis of the datasets utilized, highlighting the proportion of latency associated with AMM-replaceable MM operations within these downstream tasks. By integrating these tasks into the LibTorch ecosystem, we capitalize on its LTMM functionality for MM operations. The machine learning training and inference procedures, in particular, engage the PyTorch frontend. We bind PyTorch calls to static compilation, thus isolating the impacts of JIT on the execution of AMM or MM.

**Implementation.** We unify the implementation of the examined AMM algorithms into one C++ codebase, using static compilation to guarantee consistency across experiments. We use the IEEE 754 32-bit floating-point (FP32) format for representing matrix elements, and take the advantage of LibTorch C++ API , thereby inheriting AVX-512 instructions of FP32 from LibTorch. While certain AMM algorithms might benefit from algorithm-specific optimizations--like Bernoulli sampling probabilities in CRS or the _MADDNESS_ hash function in PQ--we exclude these from our primary evaluation. This exclusion is to avoid bias introduced by optimizations that rely on assumptions not universally applicable, aiming for evaluations that are as inclusive and applicable as possible. We focus on in-memory MM and AMM, and leave out-of-memory case  or disk-memory corporation  for future works.

**Deployment.** Our evaluation primarily unfolds on a Silver 4310 processor, with both MM and AMM seamlessly adapting to parallel and distributed computing through a straightforward block partition approach . This directs our focus towards single-threaded evaluation. Notably, we include an experiment to explore AMM's performance in a parallelized context, utilizing an I7-13700K CPU and an RTX A6000 GPU, to assess its potential on parallel hardware architectures.

### Algorithmic Strategies

We first investigate how different algorithmic strategies affect the effectiveness of AMM, concerning both processing latency and accuracy, as summarized in Table 4.

 
**Name** & **Application Field** & **Size** \\  _ECO_ & Economics & \(2077 200\) \\  _DWAVE_ & Integrated Circuit & \(512 512\) \\  _AST_ & Astrophysics & \(765 765\) \\  _UTM_ & Plasma Physics & \(1700 1700\) \\  _RDB_ & Chemical Engineering & \(2948 2948\) \\  _ZENOS_ & Air Traffic & \(2873 2873\) \\  _QCD_ & Quantum Physics & \(3072 3072\) \\  _BUS_ & Land Traffic & \(4929 10595\) \\  

Table 2: Real-world Workloads for Comparing AMM Algorithms

[MISSING_PAGE_FAIL:6]

**Our results highlighted a complex performance trade-off landscape.** Generally, increasing \(\) leads to lower \(\) for most algorithms, except CS, but at the expense of increased \(l\). This trade-off between minimizing error and managing latency varies significantly among algorithms. For instance, CRS can reduce \(l\) by up to \(60\%\) by tolerating an \(\) rise from \(0.02\) to \(0.28\). In contrast, BlockLRA consistently keeps \(\) below \(0.01\) and indicates a narrower trade-off scope. Interestingly, CS deviates from the expected trend of reduced \(\) with higher \(\), attributed to its relatively loose error bound.

### Operational Specifics

We examined the impact of operational specifics on AMM performance, categorizing processor operations into _memory operations_ and _computing operations_, using PAPI  to trace processor cycles during AMM. Our analysis on a Silver 4310 processor identified cycles affected by memory operations as 1) _Mem Stall_, 2) _L1D Stall_, 3) _L2 Stall_, and 4) _L3 Stall_. Additionally, we noted 5) _Computing Stall_, caused by pending computing operations, and 6) _Useful_ cycles, where the processor efficiently executes without memory or computing stalls. Importantly, stalls are only recorded when operations exceed their expected completion time , with on-time operations classified as _Useful_.

Observation 4. Cache and memory stalls significantly impact all AMM algorithms, particularly in larger datasets, with memory stalls been a key issue.

In Table 5, we delve into the scalability of AMM algorithms across a spectrum of dataset sizes, including _ECO_, _UTM_, and _BUS_. A key finding is the pronounced impact of cache and memory stalls on all AMM algorithms and MM baselines, particularly in larger datasets, where memory stalls emerge as a critical bottleneck. For instance, FastJLT sees nearly all its processor cycles consumed by memory stalls in the _BUS_ dataset, underscoring the heavy data demands of matrix operations.

**Memory access and the resulting cache performance issues scale with dataset size**, evident in the stark increase from \(21.91\%\) of affected cycles in _ECO_ to \(99.99\%\) in _BUS_ for SMP-PCA. Memory stalls notably surpass cache stalls in magnitude across most algorithms, such as FastJLT on _BUS_, due to LibTorch's cache optimizations aligning well with algorithms that utilize contiguous data structures. Conversely, algorithms like CS and VQ face greater challenges due to their access patterns to disjoint data structures. Furthermore, computational challenges compound for algorithms like CoOFD and PQ in large datasets, with a significant portion of their processing time hampered by computational stalls--evident in the substantial delays faced by these algorithms on _BUS_. These observations suggest that while memory stalls are a universal bottleneck, especially in larger datasets, computational efficiency remains a critical consideration for certain AMM algorithms.

Figure 3 further reveals **the escalation of memory stall cycles with data size**, aligning with increases in processing latency (\(l\)) as demonstrated in Figure 1(a). The surge in memory stalls, particularly observed in LTMM with over \(71\) growth from \(1000\) to \(2500\) rows, highlights the critical point of memory bandwidth utilization. Pruning-based and hybrid AMM algorithms, notably SMP-PCA and RIP, excel in mitigating this memory stall growth, outperforming even LTMM by maintaining stall cycles within manageable limits up to \(50000\) rows. This contrasts with extraction-based algorithms, which exhibit higher memory stall cycles due to their more intensive memory usage.

Observation 5. Adapting AMM to GPUs shows promise but is limited by data transfer costs, highlighting the need for optimized hardware-software integration.

   &  Algorithm \\  } & 

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

feature extraction does not align with the task's principal data characteristics, leading to elevated \( E\). For example, these algorithms significantly increased \( E\) in PCA due to mismatches in feature representation. The impact of AmM on machine learning training and inference highlights the importance of semantic preservation, which is often overlooked by current approximation strategies. In training, CRS's \( E\) escalated from \(0.05\) at a 500-D hidden layer to \(0.08\) at 2000-D, indicating a greater risk of information loss with larger matrices. Similarly, in inference, extraction-based methods (CoOFD, BlockLRA, VQ, and PQ) failed to preserve semantic details, with CoOFD's \( E\) reaching \(0.68\). These findings underscore that AmM's numerical approximation focus can lead to semantic information loss, particularly in consecutive applications or nuanced interpretation tasks. However, SMP-PCA's resilience to error amplification in unitary transformations signals a potential for AmM strategies to achieve numerical and semantic fidelity.

## 4 Related Work

While AmM has been theoretically well-explored [21; 25; 13; 1; 9; 31; 29; 2; 22; 7; 17; 34; 36; 16; 6; 23], there exists a gap in benchmarks that evaluate these algorithms' performance in diverse real-world applications, particularly emphasizing accuracy alongside computational efficiency. Prior research has illustrated AmM's potential in specific tasks; however, these analyses often lack a holistic evaluation that considers various performance metrics [14; 34]. Our contribution aims to bridge this gap by offering an in-depth comparison of AmM algorithms across a range of practical tasks, thereby providing insights into their applicability and efficiency in real-world scenarios. Our approach sets a new precedent in the study of AmM by amalgamating an extensive array of algorithms and applications, moving beyond traditional theoretical assessments to explore their practical utility. By adopting a comprehensive benchmarking strategy, we illuminate the strengths and limitations of AmM, facilitating a richer understanding of its potential for enhancing computational processes. This endeavor not only aids in theoretical exploration but also enhances practical applications, marking a significant step forward in the evolution of AmM research.

## 5 Conclusion

This study sheds light on the impactful nuances of AmM algorithms within real-world applications. Our takeaways and inspirations for future works can be summarized as follows.

**Summary of Observations.** There are three major findings throughout this study. First, pruning-based (e.g., INT8, CRS) and hybrid (e.g., RIP, SMP-PCA) AmM outperform extraction-based AmM, especially in various downstream tasks **(O1, O2, O8)**. Second, memory overhead is a common bottleneck, while some AmM (e.g., CRS, SMP-PCA) do succeed in optimizing memory access, the high cost of data transfer still limits border applications of AmM **(O4, O5)**. Third, the tradeoff space between accuracy and efficiency is wide for some AmM (e.g., CRS, SMP-PCA) yet narrow for others (e.g., BlockLRA). It is further challenged by data distributions, and the existing error bound of AmM is not strong enough under severe skewness or bias of data distribution **(O3, O6, O7)**.

**Impacts and Future Directions.** Our empirical insights offer a more comprehensive understanding of the strengths and limitations in current AmM to data science communities. Future work on AmM could further reduce the memory overhead while strengthening the error bound guarantees. We also envision a versatile and robust software-hardware co-design to better incorporate AmM with orthogonal optimizations, i.e., algorithmic design , parallel and distributed computing , and hardware technology , to better cater to the evolving demands of diverse computational landscapes.