# Maximum State Entropy Exploration using Predecessor and Successor Representations

Arnav Kumar Jain

Mila-Quebec AI Institute

Universite de Montreal

&Lucas Lehnert

Fundamental AI Research at Meta

&Irina Rish

Mila-Quebec AI Institute

Universite de Montreal

&Glen Berseth

Mila-Quebec AI Institute

Universite de Montreal

Correspondence to <arnav-kumar.jain@mila.quebec>

###### Abstract

Animals have a developed ability to explore that aids them in important tasks such as locating food, exploring for shelter, and finding misplaced items. These exploration skills necessarily track where they have been so that they can plan for finding items with relative efficiency. Contemporary exploration algorithms often learn a less efficient exploration strategy because they either condition only on the current state or simply rely on making random open-loop exploratory moves. In this work, we propose \(\)-Learning, a method to learn efficient exploratory policies by conditioning on past episodic experience to make the next exploratory move. Specifically, \(\)-Learning learns an exploration policy that maximizes the entropy of the state visitation distribution of a single trajectory. Furthermore, we demonstrate how variants of the predecessor representation and successor representations can be combined to predict the state visitation entropy. Our experiments demonstrate the efficacy of \(\)-Learning to strategically explore the environment and maximize the state coverage with limited samples.

## 1 Introduction

Animals and humans are very efficient at exploration compared to their data-hungry algorithms counterparts . For instance, when misplacing an item, a person will methodically search through the environment to locate the lost item. To efficiently explore, an intelligent agent must consider past interactions to decide what to explore next and avoid re-visiting previously encountered locations to find rewarding states as fast as possible. Consequently, the agent needs to reason over potentially long interaction sequences, a space that grows exponentially with the sequence length. Here, assuming that all the information the agent needs to act is contained in the current state is impossible .

In this paper, we present \(\)-Learning, an algorithm to learn an exploration policy that methodically searches through a task. This is accomplished by maximizing the entropy of the state visitation distribution of a single finite-length trajectory. This focus on optimizing the state visitation distribution of a single trajectory distinguishes our approach from prior work on exploration methods that maximize the entropy of the state visitation distribution . For example Hazan et al.  focus on learning a Markovian policy--a decision-making strategy that is only conditionedon the current state and does not consider which states have been explored before. A Markovian policy constrains the agent in its ability to express different exploration policies and typically results in randomizing at uncertain states to maximize the state entropy objective. While this will lead to uniformly covering the state space in the limit, such behaviors are not favorable for real-world tasks where the agent needs to maximize state coverage with limited number of interactions.

Figure 1 presents a didactic example to illustrate how an intelligent agent can learn to efficiently explore a \(4 4\) grid. In this example, the agent transitions between different grid cells by selecting one of four actions: _up_, _down_, _left_, and _right_. To explore optimally, the agent would select actions that maximize the entropy of the visited state distribution of the entire trajectory. Suppose the agent started its trajectory in the top left corner of the grid (shown in Figure 1(a)) and has moved to the right twice and made one downward step (indicated by red arrows). At this point, the agent has to decide between one of the four actions to further explore the grid. For example, it could move _left_ and follow the green trajectory as outlined in Figure 1(b). This path would be optimal in this example because every state is visited exactly once and not multiple times. However, the _top_ action would lead to a sub-optimal trajectory as the agent would visit the previous state. To mitigate sub-optimal exploration, an intelligent agent must keep track of visited states to avoid revisiting states. Although the _right_ action will lead to a novel state in the next step, the overall behavior will be sub-optimal as the agent will have to visit a state twice to explore the entire grid (depicted in Figures 1(c) and 1(d)). This further requires an agent to carefully plan and account for the states that would follow after taking an action.

In this work, we propose \(\)-Learning, an algorithm to compute an exploration strategy that methodically explores within a single finite-length trajectory--as illustrated in Figure 1(b). \(\)-Learning maintains two state representations: a predecessor representation  to encode past state visitation frequencies and a Successor Representation (SR)  to predict future state visitation frequencies. The two representations are used to evaluate at every time step the decision that leads to covering all states as uniformly as possible. Specifically, for every potential action the agent can take, the SR is combined with the predecessor representation to predict the state visitation distribution for the current trajectory. Then, the action that results in the highest entropy of this state visitation distribution is selected for exploration. Furthermore, this exploration policy can be deterministic and does not randomize to achieve its maximum state entropy objective.

To summarize, the contributions of this work are as follows: Firstly, we propose a mechanism to combine successor  and predecessor  representations for maximizing the entropy of the state visitation distribution of a finite-length trajectory. To the best of our knowledge, this is the first work using the two representations to optimize the state visitation distribution and learn an efficient exploration policy. Secondly, we introduce \(\)-Learning, a method that utilizes the combination of two representations to learn deterministic and non-Markovian exploration policies for the finite-sample regime. Thirdly, we discuss how \(\)-Learning optimizes the entropy-based objective function for both finite and (uncountably) infinite action spaces. In Section 5 we demonstrate through empirical experiments that \(\)-Learning achieves optimal coverage within a single finite-length trajectory. Moreover, the visualizations presented in Section 5 demonstrate that \(\)-Learning learns

Figure 1: Consider a \(4 4\) gridworld for illustration. (a) The agent starts at the top left corner and takes a few actions(red arrows show the trace), (b) an optimal trajectory that covers the grid (green arrows) and can visit all the states without visiting any state twice, (c) a sub-optimal trajectory where agent visits a previously observed state in the last step and not able to visit all the states with the limited steps, (d) another sub-optimal trajectory where agent visits an observed state at an earlier step.

an exploration policy that maneuvers through the state space to efficiently explore a task while minimizing the number of times the same state is revisited.

## 2 Related Work

The domain of exploration in Reinforcement Learning (RL) focuses on discovering an agent's environment via intrinsic motivation to accelerate learning optimal policies. Many existing exploration methods seek novelty by using prediction errors [45; 9; 54; 57] or pseudo-counts [58; 6; 39]. However, such methods add an intrinsic reward signal to improve sample efficiency in a single task setting. They do not explicitly learn a policy that is designed to efficiently explore a task. In contrast, we present a method for explicitly learning an efficient exploration strategy by maximizing the entropy of a single trajectory's state visitation distribution. Efficient exploration algorithms can be used to improve learning efficiency in application domains such as Meta RL [16; 70; 48; 37], Continual RL [27; 33], and Unsupervised RL . For example, in Meta RL an agent needs to first explore to identify which of the previously observed tasks it is in before the agent can start exploiting rewards in the current task. In this context, VariBAD  maintains a belief over which task the agent is in given the observed interactions. While Zintgraf et al. argue that a Bayes-optimal policy implements an efficient exploration strategy, we propose a method that explicitly learns an efficient exploration policy, resulting in discovering rewarding states more efficiently than VariBAD (Section 5).

The core idea behind \(\)-Learning is the use of the predecessor and successor representations to predict the state visitation distribution induced by a non-Markovian policy for a single finite-length trajectory. Instead of using the successor representation for transfer, lifelong learning, or learning one representation that solve a set of tasks [3; 69; 4; 8; 38; 56; 22; 5; 32; 34; 1; 62], we use the successor representation to estimate the state visitation distribution and maximize its entropy. By using the successor representation in this way, the \(\)-Learning does not rely on density models [23; 31], an explicit transition model [61; 41], or non-parametric estimators such as k-NN . In the following sections we will discuss how \(\)-Learning learns a deterministic exploration policy and does not rely on randomization techniques [42; 31] or mixing multiple policies to manipulate the state visitation distribution [31; 23]. Moreover, Mutti et al.  provide a theoretical analysis proving that efficient (zero regret) exploration is possible with a deterministic non-Markovian policy but computing such a policy is NP-hard. In this context, \(\)-Learning is to our knowledge the first algorithm for computing such an efficient exploration policy.

## 3 Maximum state entropy exploration

We formalize the exploration task as a Controlled Markov Process (CMP), a quadruple \(=,,p,\) consisting of a (finite) state space \(\), a (finite) action space \(\), a transition function \(p\) specifying transition probabilities with \(p(s,a,s^{})=(s^{}|s,a)\), and a start state distribution \(\) specifying probability of starting a trajectory at state \(s\) with \((s)\). A trajectory is a sequence \(=(s_{1},a_{1},...,a_{h-1},s_{h})\) of some length \(h\) that can be simulated in a CMP. A policy \(\) specifies a probability distribution or density function over the action space that is sampled when selecting actions and simulating a trajectory in a CMP. This policy definition include deterministic policies: For discrete action spaces a specific action is selected with probability one and for uncountably infinite action spaces the policy models a delta-dirac density function. In algorithms such as Q-learning , the policy is conditioned on the task state and specifies the probabilities of selecting a particular action. However, as illustrated in Figure 1, the past trajectory (red arrows) determines which next action leads to the best exploratory trajectory. Consequently, we consider policies that are functions of trajectories rather than just states.

The state visitation distribution of a trajectory \(_{h}=(s_{1},a_{1},...,a_{h-1},s_{h})\) of length \(h\) can be formally expressed in a probability vector by first encoding every state \(s_{t}\) as a one-hot bit vector \(_{s_{t}}\). Using this one-hot encoding, the \(h\)-step state visitation probability vector of the trajectory \(_{h}\) can computed by marginalizing across the time steps:

\[_{,}=_{t=1}^{h}(t)_{s_{t}},\] (1)where \(:\) is the _discount function_ (we denote the set of positive integers with \(\)), such that \(_{t=1}^{h}(t)=1\). Using the normalization in the discount function is necessary as it ensures that \(_{,}\) is a probability vector. We note that this use of a discount function is distinct from using a discount factor in common RL algorithms such as Q-learning but using a discount function is necessary as we will elaborate in the following section.The expected state visitation distribution for a policy \(\) can be obtained by generating multiple trajectories using \(\) and computing the average across them to get the expected state visitation distribution, denoted by \(_{}[_{,}]\).

An optimal exploration policy would achieve a similar visitation frequency for each state, as illustrated in Figure 1(b) where the optimal trajectory traverses every state once within the first 15 steps. For this trajectory the vector \(_{,}\) would encode a uniform probability vector, given \((t)=\) for any \(t\). In fact, an optimal exploration policy \(^{*}\) maximizes the entropy of this probability vector and solves the optimization problem

\[^{*}_{}H_{}[_{ ,}]\] (2)

where the expectation is computed across trajectories that are simulated in a CMP and follow the policy \(\).3 In the remainder of the paper, we will show how optimizing this objective leads to the uniform sweeping behavior illustrated in Figure 1 and the agent learns to maximize the entropy of the state visitation distribution in a single finite length trajectory. In the following section, we describe how \(\)-Learning optimizes the objective in Equation 2.

## 4 \(\)-Learning

To learn an efficient exploration policy, we need to estimate the state visitation history and predict the distribution over future states. Consider a trajectory \(=(s_{1},a_{1},...,s_{T-1},a_{T-1},s_{T},...a_{h-1},s_{h})\). At an intermediary step \(T\), we denote the \(T-1\)-step prefix with \(_{T-1}=(s_{1},a_{1},...,s_{T-1})\) and the suffix starting at step \(T\) with \(_{T:}=(s_{T},a_{T}...,a_{h-1},s_{h})\). Using this sub-trajectory notation, the discounted state visitation distribution in Equation 1 can be written as

\[_{,}=_{t=1}^{T-1}(t)_{s_{t }}+_{t=T}^{h}(t)_{s_{t}}.\] (3)

Assuming the scenario presented in Section 3, suppose the agent has followed the trajectory \(_{:T}\) until time step \(T\). At this time step, the agent needs to decide which action \(a_{T}\) leads to covering the state space as uniformly as possible and maximizes the entropy of the state visitation distribution. The expected state visitation distribution for a policy \(\) can be expressed by conditioning on the trace \(_{:T}\) and a potential action \(a_{T}\):

\[_{,}_{,} _{:T},a_{T} =_{,}_{t=1}^{T-1}(t) {e}_{s_{t}}+_{t=T}^{h}(t)_{s_{t}}_{:T},a_{T} \] (4) \[=^{T-1}(t)_{s_{t}}}_{= (_{:T-1})}+_{_{T+1},} _{t=T}^{h}(t)_{s_{t}}_{:T},a_{T}} _{=^{}(_{:T},a_{T})},\] (5)

where the vector \((_{:T-1})\) is a variant of the predecessor representation [64; 2] and the vector \(^{}(_{:T},a_{T})\) is a variant of the successor representation (SR) . Splitting the expected state visitation distribution into a vector \(\) and \(^{}\) as outlined in Equation 5 is possible because we are assuming a discount function \(\) as defined in Section 3. At time step \(T\), the two representations can be added together to estimate the expected state visitation probability vector. Simulating the proposed algorithm is analogous to effectively drawing Monte-Carlo samples from the expectation at different steps \(T\) to learn a SR and predict the expected visitation frequencies of \(_{,}\).

The predecessor representation vector \((_{:T-1})\) can still be estimated incrementally similarly to the eligibility trace in TD(\(\)) algorithm  (but with a different weighting scheme that uses the discount function \(\)). While the definition of the vector \((_{:T})\) is similar to the definition of eligibility traces [60, Chapter 12], we do not use the predecessor trace for multi-step TD updates to learn more efficiently. Instead, the vector \((_{:T-1})\) estimates the visitation frequencies of past states--the predecessor states--to decide which states to explore next.

While the predecessor representation can be maintained using an update rule because the observed states are known, predicting future state visitation frequencies is more challenging. A potential solution is to exhaustively search through all possible sequences of trajectories starting from the current state. This is computationally infeasible and requires a dynamics model of the environment. Moreover, such a model is not always available, and learning them is prone to errors that compound for longer horizons . To this end, we learn a variant of the successor representation (SR), which predicts the expected frequencies of visiting future or successor states under a policy . In contrast to previous methods which learn successor representation (SR) conditioned on the current state , \(\)-Learning conditions the SR on the entire history of states, given by

\[^{}(_{:T},a_{T})=_{_{T+1},}[_{t=T}^ {h}(t)_{s_{t}}_{:T},a_{T}].\] (6)

Conditioning the SR on the trajectory \(_{:T}\) is necessary because policy \(\) is also conditioned on \(_{:T}\) and therefore the visitation frequencies of future states depend on \(_{:T}\). Moreover, the expectation evaluates all possible trajectories after taking action \(a_{T}\) at time \(T\) and following policy \(\) afterward. We discuss in Appendix C how the SR vectors are approximated using a recurrent neural network.

We saw in Equation 5 that the predecessor representation and successor representation can be combined to predict the state visitation distribution for a policy \(\) and a trajectory-prefix \(_{:T}\). \(\)-Learning uses the estimated state visitation distribution to compute the entropy term in the objective defined in Equation 2. Specifically, the utility function \(Q_{}\) approximates the entropy of the state visitation distribution for an action \(a_{T}\) at every time step. By defining

\[Q_{}(_{:T},a_{T})=H((_{:T-1})+^{} (_{:T},a_{T})),\] (7)

the action that leads to the highest state visitation entropy is assigned the highest utility value. Notably, the proposed Q-function differs from prior methods using the SR, as we neither factorize the reward function  nor use the SR for learning a state abstraction . Optimizing the exploration Q-function stated in Equation 7 is challenging as it depends on the SR that itself depends on the policy \(\) which changes during learning. Furthermore, Guo et al.  show that the Shannon-entropy based objective is difficult to directly optimize using gradient-based methods (due to the log term inside an expectation) . In contrast, we outline in the following paragraphs how the entropy objective in Equation 7 can be directly optimized using either a Q-learning style method or a method based on the Deterministic Policy Gradient  framework for finite and infinite action spaces, respectively.

Finite action space frameworkSince the predecessor representation is fixed for a given trajectory \(_{:T}\), optimizing the Q-function defined in Equation 7 boils down to predicting the optimal SR for a given history \(_{:T}\). Similar to prior Successor Feature learning methods , we approximate the SR with a parameterized and differentiable function \(}\) and use a loss based on a one-step temporal difference error. Given an approximation \(}\), the SR prediction target is obtained by the current state embedding and SR of the optimal action at the next step:

\[(_{:T+1},a^{}_{T+1})=_{s_{T}}+(T+1)}(_{:T+1},a^{}_{T+1}),\] (8)

where \(_{:T+1}\) is obtained by adding action \(a_{T}\) and the received next state \(s_{T+1}\) to the trajectory \(_{:T}\). Analogous to Q-Learning, the optimal action at the next step is specified by

\[a^{}_{T+1}=_{a}Q_{}(_{:T+1},a).\] (9)

Being greedy with respect to these entropy values to estimate the target leads to improving the policy \(\) which in turn finds the SR for the optimal policy. (Appendix A presents a convergence analysis of this method in a dynamic programming setting.) Then, the function \(}\) is optimized using gradient descent on the loss function \(_{SR}\), given by

\[_{SR}=||}(_{:T},a_{T})-(_{:T+1},a^{ }_{T+1})||^{2},\] (10)

where gradients are not propagated through the target \((_{:T+1},a^{}_{T+1})\). Finally, the optimal policy selects actions greedily with respect to the \(Q_{}\) function. Algorithm 2 describes the training procedure for the proposed variant for finite action spaces.

Infinite action space frameworkDirectly obtaining gradient estimates of objective defined in Equation 2 is challenging because of the expectation term in the non-linear logarithmic term. Previous approaches have used alternative optimization methods [31; 47] or resorted to a simpler noise-contrastive objective function . In contrast with prior algorithms, we derived an \(\)-Learning variant for infinite action spaces that optimizes an actor-critic architecture using policy gradients to maximise the maximum state entropy objective. The agent uses an actor-critic architecture where actor and critic networks are conditioned on the history of visited states. The actor \(_{}()\) is parameterized with a parameter vector \(\) and is a deterministic map from a trajectory to an action. The critic predicts the SR to estimate the utility function conditioned on a given trajectory and action. Similar to the finite action space variant, the predecessor representation is fixed for a given trajectory and the network has to predict SR \(_{}(,a)\) for a given trajectory \(\) and action \(a\). Here, the target value of SR to update the critic is specified by the action obtained using the policy \(a^{}_{T+1}=_{}(_{:T+1})\), given by

\[=_{s_{T}}+(T+1)_{}( _{:T+1},a^{}_{T+1}).\] (11)

The critic is trained with the same loss function \(_{SR}\) as defined in Equation 10, where the gradients are not propagated through the target. The actor is optimized to maximize the estimated utility function (Equation 2). Since the actor is deterministic, policy gradients are computed using an adaptation of the deterministic policy gradient theorem . Because the actor network has no dependency on predecessor trace (which depends on the observed states only), gradients for the actor parameters are obtained by applying chain rule leading to the following gradient of Equation 7 (please refer to Proposition 2 for more details on the derivation):

\[_{}J(_{})=_{}_{i}z_{i}_{ }_{}()_{a}_{i}(,a)_{a=_{ }()},\] (12)

where \(z_{i}=-[(_{:-1})_{i}+(,_{ }())_{i}]-1\) is the multiplicative factor for state \(i\), and depends on the expected probability of visiting a state. The factor \(z_{i}\) can take values between between -1 and \(\), with positive values of high magnitude for states with low visitation probability and negative values for state with high probability of visitation. Thus, the factor \(z_{i}\) scales the policy gradients to maximize the entropy of the state visitation distribution. In Algorithm 3 we outline the training procedure for the infinite action space framework.

## 5 Experiments

To analyze and test if \(\)-Learning learns an efficient exploration policy, we evaluate the proposed method on a set of discrete and continuous control tasks. In these experiments, we are recording a set of different performance measures to access if the resulting exploration policies do in fact maximize the entropy of the state visitation distribution and if most states are explored by \(\)-Learning. The following results demonstrate that by maintaining a predecessor representation and conditioning the SR on the simulated trajectory prefix, the \(\)-Learning agent learns a deterministic exploration policy that minimizes the number of interactions needed to visit all states. In addition, we expand our method to continuous control tasks and demonstrate how \(\)-Learning can efficiently explore in complex domains with infinite action space. Our method is ideal for searching out rewards in difficult sparse reward environments. We compare \(\)-Learning, which learns to explore an environment as efficiently as possible, to recent meta-RL methods  that aim to learn how to optimally explore an environment to infer the rewarding or goal state. Lastly, we show the applicability of \(\)-Learning on standard RL tasks where the extrinsic rewards are sparse and present how the proposed method can be combined with existing algorithms.

**Environments**: We experiment with different tasks with both finite and infinite action spaces. The **ChainMDP** and **RiverSwim** is a six-state chain where the transitions are deterministic or stochastic, respectively. In these tasks a Markovian policy cannot cover the state space uniformly because the agent has to pace back and forth along the chain, visiting the same state multiple times. For the RiverSwim environment, a non-stationary policy, a policy that is a function of the time step, cannot optimally cover all states because non-determinism in the transitions can place the agent into different states at random. Furthermore, we include the \(5 5\)**grid world** example used in Figure 1. We also test \(\)-Learning on two harder exploration tasks--the **TwoRooms** and **FourRooms** domains, which are challenging because it is not possible to obtain exact uniform visitation distribution due to the wall structure. For continuous control tasks, we evaluate on Reacher and Pusher tasks, where the agent has a robotic-arm with multiple joints. The task is to maximize the entropy over the locations covered by the fingertip of the robotic-arm. Appendix F provides more details on the environments and the hyper-parameters are reported in Appendix G.

**Prior Methods**: To our knowledge, existing work focusses on learning Markovian exploration policies . We use MaxEnt  as a baseline agent for our experiments because this method optimizes a similar maximum entropy objective as \(\)-Learning--with the difference that MaxEnt learns a Markovian policy and resorts to randomization to obtain a close to uniform state visitation distribution. We have also compared with MEPOL  on continuous control tasks which leans a Markovian policy and uses kNN-based estimators to compute the entropy. A comparison with SMM  is skipped because the method optimizes a similar maximum entropy objective with a Markovian policy and cannot express the same exploration behaviour as \(\)-Learning.

**Evaluation Metrics**: _Entropy_ measures a method's ability to have similar visitation frequency for each state in the state space. This signifies the gap between the observed state visitation distribution and the optimal distribution that maximizes the entropy term. The Entropy metric is computed using the objective defined in Equation 2 over a single trajectory generated by the agent. A constant discount factor of \((t)=\) is used to obtain the state visitation distribution during evaluation. An agent can maximize this measure without actually exploring all states of an environment--a desirable property for RL where rewards may be sparse and hidden in complex to-reach states. We record the _state coverage_ metric which represents the fraction of states in the environment visited by the agent at least once within a trajectory. Lastly, we want agents to explore the state space efficiently. For example, an optimal agent can sweep through the gridworld presented in Figure 1 with a search completion time of \(15\) steps ( Figure 1(b) shows an optimal trajectory). The _search completion time_ metric measures the steps taken to discover each state in the environment. All results report the mean performance computed over 5 random seeds with 95% confidence intervals shading.

**Quantitative Results**: Figure 2 presents the results obtained for \(\)-Learning and MaxEnt . Compared to MaxEnt, which learns a Markovian policy, \(\)-Learning achieves 20-50% higher entropy. This indicates that the MaxEnt algorithm by learning a Markovian and stochastic policy was randomizing at certain states which lead to sub-optimal behaviors. The performance gain was more prominent in grid-based environments because the MaxEnt agent was visiting some states more frequently than others, which are harder to explore efficiently. Furthermore, high _entropy_ values suggest that the agent visits states with similar frequency in the environment and does not get stuck at a particular state. We attribute this behavior of \(\)-Learning to the proposed Q-function that picks action to visit different states and maximize the objective.

Figure 2: Comparison of \(\)-Learning and MaxEnt  on three metrics: _Entropy_ (top row) of state visitation distribution, _State Coverage_ (middle row) representing the fraction of state space visited, and _Search Completion Time_ (bottom row) denoting steps taken to cover the state space.

Figure 2 also shows that \(\)-Learning achieves optimal state coverage across environments exemplifying that \(\)-Learning while maximizing the entropic measure also learns to cover the state space within a single trajectory. However, the baseline MaxEnt was not able to discover all the states in the environment. MaxEnt was unable to visit all the states in ChainMDP and RiverSwim environments with trajectory length of \(20\) and \(50\), respectively. Moreover, the state coverage of MaxEnt was around **50-60%** on the harder TwoRooms and FourRooms tasks, where the agent has to navigate between different rooms and is required to remember the order of visiting different rooms. These results reveal that Markovian policy limits an agent's ability to maximize the state coverage in a task. The proposed method also outperformed the baseline on the _search completion time_ metric across all environments. Notably, on ChainMDP, the \(5 5\) gridworld, and TwoRooms environments, \(\)-Learning converged within 500 episodes. However, \(\)-Learning did not achieve optimal _search exploration time_ on the FourRooms environment as it missed a spot in a room and resorted to it later in the episode.

To further understand the gains offered by \(\)-Learning, we visualized the state visitation distributions on a single trajectory (in Figure 3(a)). On the TwoRooms environment, \(\)-Learning had similar density on the states in both the rooms, where the density is more around the center. This is because the agent was sweeping across rooms alternatively. \(\)-Learning showed a better-visited state distribution on the FourRooms environment with more distributed density across states. However, MaxEnt was not visiting all states and also visited a few states more frequently than others, elucidating the lower performance on entropy and state coverage. We further visualized the learned SR to see if \(\)-Learning learns a SR for the optimal exploration policy through generalized policy improvement . For this analysis, we sampled a trajectory on \(5 5\) gridworld. Figure 3(b) reports the heatmaps of the learned SR vector for each action at different steps in the trajectory. We observe that the SR vector for each action has lower density on the states already observed in the trace. This exemplifies that the learned SR captures the history of the visited states that further aids in taking actions to maximize the entropy of state visitation distribution. We also study if MaxEnt can show similar gains when trained with a recurrent policy (Appendix I.1) and compared the agents when evaluated across multiple trajectories (Appendix I.2).

**Continuous Control tasks**: The efficacy of \(\)-Learning is further tested on environments with infinite action space. Figure 4(a) reports the Entropy and State Coverage metric on Reacher and Pusher environments, where \(\)-Learning outperformed the baseline MaxEnt on both metrics. The gains are more significant on the Pusher environment which is a harder task because of multiple hinges in the robotic-arm. The proposed method \(\)-Learning achieves close to **90%** coverage in both environments, whereas the MaxEnt had only close to **50%** and **40%** coverage on Reacher and Pusher environments, respectively. In Figure 4(b), heatmaps of the state visitation density for a single trajectory shows that \(\)-Learning has more uniformly distributed density compared to MaxEnt. The Pusher environment has high density at the top-right corner of the grid denoting the time taken by the agent to move the fingertip to other locations from the starting state. Notably, the proposed method \(\)-Learning has lower density at the starting state and we believe that conditioning on the history of states is guiding the agent to move the robotic-arm to other locations to maximize the entropy over the state space. In Appendix I.5, a visualization of a rolled-out trajectory generated using \(\)-Learning is

Figure 3: (a) Heatmap of state visitation distribution by unrolling a trajectory using MaxEnt (left) and \(\)-Learning (right) on TwoRooms and FourRooms environments. (b) Visualization of learned SR of each action (denoted with \((.)\)) at time steps \(T=7,14\) for a trajectory using \(\)-Learning on \(5 5\) grid. \((s,a)\) denotes the state (black) and action taken by the agent (direction of white arrow), \(\) is the predecessor representation till time \(T\) (higher values have darker shade)

presented showing that the agent learns to efficiently maneuver the fingertip of the robotic-arm to different locations in the environment.

**Comparison with Meta-RL**: A question central to Meta-RL [16; 70; 37] is the ability to quickly explore a task and find rewarding states in complex tasks where the rewards are sparse. In this context, Zintgraf et al.  present the VariBAD method, which maintains a belief over different tasks to infer the optimal policy--leading to efficient exploration behaviour that enables the agent to discover rewarding states quickly. Similar to VariBAD, the predecessor representation \(\) in \(\)-Learning keeps track of which states have been explored and which states are not explored. Figure 4(c) compares the exploration behaviour of \(\)-Learning to VariBAD: In terms of the State Coverage and Goal Search Time metric, \(\)-Learning outperforms VariBAD significantly because \(\)-Learning is designed to optimize the entropy of the state visitation frequencies of a single trajectory instead of performing Bayes-adaptive inference across a task space. We refer the reader to Appendix H for more details on this experiment.

\(\)**-Learning in Sparse Reward Tasks**: Sparse reward environments pose a challenge where an agent has to discover the reward function by visiting different regions of the state space. Through this experiment, we demonstrate how an agent can leverage \(\)-Learning as an exploratory bonus and improve its efficiency in such tasks. The experiment is conducted on Sparse Mountain-Car environment where the agent receives a positive reward upon reaching the goal position. This is hard because the agent needs to plan to visit different positions in the environment. We have used TD3  as the baseline algorithm for this task. To compare with leading exploration methods, we add TD3 combined with Count-Based  and First-Occupancy based  to the baselines. It is to be noted that we have used these methods as episodic bonuses as described in . We also propose a variant of TD3 equipped with the proposed method and call it TD3-\(\)-Learning, where we learn two critics to estimate the sum of extrinsic rewards and the entropy of state visitation distribution (Equation 7). Appendix J describes the experimental setup in more detail and Algorithm 4 provides the pseudocode of the proposed variant. The evaluation is done across two metrics- Average Return and Average Episode Length denoting the steps taken to reach the goal state. Figure 5 presents the results where the proposed method outperforms the baseline algorithms and is sample-efficient at learning the task. TD3 combined with First-Occupancy based intrinsic reward performed better than Count-Based bonus but still takes twice number of steps as the proposed method to solve the task. This shows that the proposed method can improve sample efficiency in standard RL tasks, especially with sparse

Figure 4: (a) Comparison of \(\)-Learning with Random policy, MaxEnt , and MEPOL  on Reacher and Pusher tasks (b) Heatmaps of visitation distribution of MaxEnt (top) and \(\)-Learning (bottom), (c) comparison with VariBAD  on State Coverage and Goal Search Time metrics.

Figure 5: Results on Sparse Mountain Car environment.

rewards. We leave scaling the proposed method with leading architectures [21; 25; 53] to more challenging tasks for future research.

## 6 Discussion

To explore efficiently, an intelligent agent needs to consider past episodic experiences to decide on the next exploratory action. We demonstrate how the predecessor representation--an encoding of past state visitations--can be combined with the successor representation--a prediction of future state visitations--to learn efficient exploration policies that maximize the state-visitation-distribution entropy. Across a set of different environments, we illustrate how \(\)-Learning consistently reasons across different trajectories to explore near optimally--a task that is NP-hard .

To the best of our knowledge, \(\)-Learning is the first algorithm that combines predecessor and successor representations to estimate the state visitation distribution. Furthermore, \(\)-Learning learns a non-Markovian policy and can therefore express exploration behavior not afforded by existing methods [23; 31; 42; 18]. To further increase the applicability of \(\)-Learning, one interesting direction of future research is to extend \(\)-Learning to POMDP environments where states are either partially observable or complex such as images. This is challenging because the agent has to learn an embedding of state observations that capture only the relevant components of the state space to maximize the entropy. We believe a promising approach would be to leverage the idea of Successor Measures (SM) [62; 63; 15] which have shown promising results when scaled to high-dimensional inputs like images. Furthermore, the presented approach can be also used for designing other algorithms that control the state visitation distribution. An application is goal-conditioned RL, where the agents need to minimize the KL divergence between visitation distribution of policy and goal-distribution [31; 47]. Another application is Safe RL  where agents receive a penalty upon visiting unsafe states to avoid observing them.

We study reinforcement learning, which aims to enable autonomous agents to acquire search behaviors. This study of developing exploration behaviors in reinforcement learning is guided by a fundamental curiosity about the nature of autonomous learning; it has a number of potential practical applications and broad implications. First, autonomous exploration for pre-training in general, can enable autonomous agents to acquire useful skills with less human intervention and effort, potentially improving the feasibility of learning-enabled robotic systems. Second, the practical applications that we illustrate, such as applications to continuous environments, can accelerate reinforcement learning in certain settings. Specific to our method, finite length entropy maximization may also in the future offer a useful tool for search and rescue, by equipping agents with an objective that causes them to explore a space systematically to locate lost items. However, these types of reinforcement learning methods also have a number of uncertain broad implications: agents that explore the environment and attempt to acquire open-ended skills may carry out unexpected or unwanted behaviors, and would require suitable safety mechanisms of their own during training.