# Smoothie: Label Free Language Model Routing

Neel Guha &Mayee F. Chen1Trevor ChowIshan S. KhareChristopher Re

Stanford University, Department of Computer Science

{nguha, mfchen, tmychow, iskhare, chrismre}@stanford.edu

Equal contribution.

###### Abstract

Large language models (LLMs) are increasingly used in applications where LLM inputs may span many different tasks. Recent work has found that the choice of LLM is consequential, and different LLMs may be good for different input samples. Prior approaches have thus explored how engineers might select an LLM to use for each sample (i.e. _routing_). While existing routing methods mostly require training auxiliary models on human-annotated data, our work explores whether it is possible to perform _unsupervised_ routing. We propose Smoothie, a weak supervision-inspired routing approach that requires no labeled data. Given a set of outputs from different LLMs, Smoothie constructs a latent variable graphical model over embedding representations of observable LLM outputs and unknown "true" outputs. Using this graphical model, we estimate sample-dependent quality scores for each LLM, and route each sample to the LLM with the highest corresponding score. We find that Smoothie's LLM quality-scores correlate with ground-truth model quality (correctly identifying the optimal model on 9/14 tasks), and that Smoothie outperforms baselines for routing by up to 10 points accuracy.

## 1 Introduction

Large language models (LLMs) are increasingly being deployed in _multi-capability_ regimes where data inputs may span a diverse range of tasks, each of which requires different capabilities . For instance, an LLM-powered chatbot may be asked to write code, answer questions about different domains, summarize documents, perform extraction, and more [3; 8; 14; 30]. One challenge is that while engineers often have access to numerous pre-trained LLMs (i.e., through Huggingface or various APIs), they do not know which LLM is optimal for each possible user input . Because the quality of generations can significantly vary across LLMs, choosing the right LLM for each input sample is important for ensuring high task performance .

Recent work has explored various ways to utilize ensembles of pretrained LLMs in multi-capability settings, by (1) collecting a diverse pool of LLMs and (2) identifying which LLM to _route_ each sample to [55; 86]. However, the majority of existing approaches require labeled data; engineers typically either (1) train an auxiliary model using labeled data to rank or predict which LLM each sample should be routed to [41; 79], or (2) directly use labeled data to determine which LLM is the best on average . As a result, engineers designing routing protocols face the practical difficulty of constructing labeled datasets.

Given a candidate pool of LLMs and an unlabeled test dataset, this paper explores how to best select LLM outputs for each sample in an entirely unsupervised manner--without labeled data, or models trained on labeled data. To make progress in addressing this question, we face two technical challenges:

* **Unknown LLM quality**: The first challenge is estimating the quality of each LLM. Access to labeled data allows engineers to identify higher performing LLMs by measuring the accuracy/quality of LLM outputs. In this paper, we study the question of how to estimate quality _without_ labeled validation data.
* **Sample-conditional generator performance**: The second challenge is determining how to select the best LLM for each individual test sample. LLM outputs can vary in quality over different samples, which could render population-level estimates of LLM quality misleading.

In this work, we propose Smoothie, a method for routing samples to LLMs in a label-free manner (Figure 1). Below, we describe how Smoothie addresses the two challenges described above.

* **Quality estimation**: Using the LLM outputs for each test sample as "voters," Smoothie estimates the quality of each generator using methods from Weak Supervision (WS). Concretely, Smoothie constructs a latent variable graphical model over observable LLM outputs and an unknown true output. By modeling the embedding vector difference between each LLM output and the true output as a multivariate Gaussian, we can derive a closed-form estimator adapted from  for learning LLM quality scores efficiently.
* **Conditioning**: We condition these quality estimates to be particular to a given test sample by only using the nearest neighbors of a test sample in the training data as inputs to the estimator (i.e., kernel smoothing). We then route each test sample to the LLM with the highest quality score estimate on that sample. We call the version of Smoothie that produces quality estimates using all available test data Smoothie-Global, and we call the version that uses a sample's nearest neighbors Smoothie-Local.

We empirically evaluate Smoothie in three stages.

* **LLM selection:** First, we assess Smoothie-Global's ability to identify--from an ensemble of mixed quality LLMs--the optimal LLM for a given task overall. On traditional generation tasks such as summarization, reading comprehension, and data-to-text generation, we find that Smoothie-Global's learned LLM quality-weights correlate with actual LLM performance (\(=0.72\))), and on the AlpacaEval benchmark, Smoothie-Global identifies the best-performing instruction model 70% of the time . The highest quality LLM identified by Smoothie-Global--all computed without labeled data--can beat random-selection by up to 15 points win-rate on AlpacaEval, and by up to 8 points on SQuAD.
* **Routing:** Second, we study whether Smoothie-Local's sample-conditional scoring mechanism allows it to _route_ samples in mixed-task datasets to higher-performing LLMs (i.e., the multi-capability regime). We find that Smoothie-Local can improve the quality of produced generations by up to 7 points accuracy over Smoothie-Global, and that Smoothie-Local outperforms baseline unsupervised routing methods by up to 10 points accuracy and supervised routing methods by up to 5.0 points accuracy.
* **Prompt selection:** Finally, we assess whether Smoothie's quality-estimation mechanism can be applied to select the optimal prompt template in a candidate pool while using a fixed LLM. We find that Smoothie-Global can outperform other prompt selection approaches by up to 18 points, allowing a 410M parameter model to match the performance of 6.9B parameter model.

Figure 1: For a given input \(x\), Smoothie estimates the quality of every LLM ensembleâ€™s generation, and uses this quality weight to route \(x\) to a single LLM.

Related Work

We provide an abbreviated related work, with a full treatment in Appendix C.

**Routing** Routing has been classically utilized in Mixture-of-Experts models [25; 37; 42; 82], which involve jointly training a set of models as well as a router. Recently, routing mechanisms have been used at inference time to decide which pre-trained LLM to use for a given sample . Some approaches involve training an auxiliary model using labeled training data to either score or rank the performance of each LLM on each test sample [38; 74]. Others do not involve training a model but instead use nearest neighbor methods, selecting the LLM that does the best on a test sample's labeled neighbors [48; 86]. In contrast, Smoothie does not require any labels.

**Ensembling** Ensembling is another way of utilizing a pool of LLMs. Existing work has primarily focused on ensembling outputs for classification tasks [2; 68; 98]. Ensembling generative outputs typically requires training an auxiliary model , combining or switching among outputs when decoding [36; 83], or averaging in weight space .

**Prompt selection** In addition to selecting the best LLM for a sample, prior works have studied how to select the best prompt or in-context examples. While the simplest approach is to use a held-out labeled dataset , there are also retrieval-based approaches to selecting the best in-context examples , as well as approaches based on mutual information  and probability-based measures , although the latter two are limited to classification.

**Weak supervision** Smoothie utilizes statistical techniques inspired by weak supervision, which programmatically generate labels for an unlabeled dataset by aggregating the predictions of several weak "voters" via a latent variable graphical model [71; 73]. Weak supervision has mostly been studied in classification settings [72; 26] but more recently has been extended to tasks such as learning rankings and manifolds [94; 85]. We derive our estimation procedure from the Gaussian model in , applying it to LLM embeddings and the routing setting.

## 3 Preliminaries

### Problem setup

Let \(\) be the token vocabulary space, and let \(}=\) be the space of all vocabulary sequences. We consider a generative task with input text \(x}\) and reference output text \(y}\). We have a candidate pool of \(m\) LLMs, \(G=\{g_{1},,g_{m}\}\), where each \(g_{i}:\) produces a generative output sequence \(g_{i}(x)\) for a given input text sequence \(x\). We are given an unlabeled test dataset \(_{}=\{x_{i}\}_{i=1}^{n}\), where the ground-truth reference outputs are _unknown_.

Our goal is to route each sample \(x_{}\) to one of the LLMs in \(G\). Specifically, we wish to construct a router \(:^{m}\) that selects the LLM that yields the highest quality generation on \(x\) for each test sample \(x\), without any labeled data.

### Graphical model

We present a probabilistic graphical model (see Figure 1 (center)) that describes how the LLM outputs, \(g_{1}(x),,g_{m}(x)\), are related to a true output \(y\) in terms of each LLM's quality on a given input \(x\), which we call \(_{i}(x)\), corresponding to each \(g_{i}(x)\). Let \(z_{g_{0}}:}^{d}\) map from a sequence of tokens to a \(d\)-dimensional embedding using a common model \(g_{0}\) such as SentenceBERT . Define \(_{i}(x):=z_{g_{0}}([x,g_{i}(x)])\) to be the observable embedding of \(x\) and the LLM output, and define \(z^{}(x):=z_{g_{0}}([x,y])\) to be the latent ground-truth embedding of \(x\) and reference output \(y\). Similar to the approach in , we model the distribution over embedding vectors, \((z^{}(x),_{1}(x),,_{m}(x)|x)\) as

\[(z^{}(x),_{1}(x),,_{m}(x)|x)= _{i=1}^{m}-_{i}(x)||_{i}(x)-z^{}(x)||^{2}\] (1)

where \(Z\) is the log partition function and the \(_{i}(x)\)s--the LLM quality scores--are canonical parameters of the graphical model. Intuitively, our model captures LLM quality by supposing that if \(g_{i}\) is of high quality and \(_{i}(x)\) is very large, then it should be unlikely for the LLM output to be very different from the true output in terms of Euclidean distance in embedding space. Conversely, if \(_{i}(x)\) is small, we assign larger probability to the setting where \(_{i}(x)\) and \(z^{}(x)\) differ significantly. Finally, note that this graphical model corresponds to a multivariate Gaussian. That is, the vector \([_{1}(x)-z^{}(x),,_{m}(x)-z^{}(x)]^{dm}\) is Gaussian with mean \(=\) and a diagonal covariance matrix \(^{dm dm}\) with \(_{jj}=(x)}\). Intuitively, this means that the average difference vector between each \(_{i}\) and \(z^{}(x)\) is centered, with its magnitude inversely proportional to the LLM score \(_{i}(x)\) and independent of other LLMs. Given this probabilistic graphical model, our goal is to learn each quality score \(_{i}(x)\) from the unlabeled test dataset and use these for improved routing.

## 4 Method

Given an unlabeled test dataset \(_{}\) and a pool of LLMs \(G\), Smoothie consists of two steps:

1. **Estimation**: The LLM quality scores \(_{1}(x),,_{m}(x)\) are learned for each \(x_{}\) (Section 4.1, Algorithm 1).
2. **Routing**: The LLM with the highest scores is selected, and its output is used as our final prediction for \(x\) (Section 4.2).

We describe each step in the following sections.

### LLM score estimation

We describe how to estimate each \(_{i}(x)\)s in the graphical model in (1) using only unlabeled data from \(_{}\). Then, we describe how the LLM score estimate can be instantiated to be sample-conditional.

Computing \(_{i}(x)\)Below, we state a simple property arising from the fact that (1) corresponds to a multivariate Gaussian with a diagonal covariance matrix.

**Proposition 1**: _[_85_]_ _For any \(i,j[m]\), it follows from the graphical model in (1) that_

\[[\|_{i}(x)-_{j}(x)\|^{2}]=[ \|_{i}(x)-z^{}(x)\|^{2}]+[\|_{j}(x)-z ^{}(x)\|^{2}].\] (2)

The proof is in Appendix D and relies on the fact that off-diagonal entries of \(\) are \(0\). Note that the left hand side of the equation is observable while the two expectations on the right are unknown. We can apply this equation to pairs of LLM embeddings over a triplet of \(_{i},_{j},_{k}\) to form a system of three equations with three unknown expectations. Solving, we have

\[[\|_{i}(x)-z^{}(x)\|^{2}]= _{ij}(x)+_{ik}(x)-_{jk}(x)\ (i,j,k)[m],\] (3)where \(_{ij}(x)=[\|_{i}(x)-_{j}(x)\|^{2}]\). Since (1) is a multivariate Gaussian with \(_{jj}=(x)}\), we can write \(_{i}(x)\) as the following function of \([\|_{i}(x)-z^{}(x)\|^{2}]\):

\[[\|_{i}(x)-z^{}(x)\|^{2}]=_{ j=1}^{d}[(_{i,j}(x)-z_{j}^{}(x))^{2}]=_{j=1}^{d }(_{i,j}(x)-z_{j}^{}(x))=(x)},\] (4)

where \(_{i,j}(x)\) and \(z_{j}^{}(x)\) are the \(j\)th indices of the embeddings \(_{i}(x)\) and \(z^{}(x)\) respectively. Therefore, we can write \(_{i}^{jk}(x)=(x)+_{ik}(x)-_{jk}(x)}\), where each \(_{ij}(x)\) can be estimated using the LLM outputs on \(_{}\), and in practice in Algorithm 1 we estimate \(_{i}(x)\) by averaging \(_{i}^{jk}(x)\) over all \(\) pairs of \(j,k i\).

**Sample-conditional estimation of \(_{i}(x)\)** Note that the expectation in \(_{ij}(x)=[\|_{i}(x)-_{j}(x)\|]\) is over the randomness in \(_{i}(x),_{j}(x)\) conditioned on a fixed point \(x\). However, we only have one sample per \(x\). One simple approach is to use the entire dataset to estimate \(_{i}(x)\), i.e., \(_{ij}(x)=_{x^{}_{}} \|_{i}(x^{})-_{j}(x^{})\|^{2}\). We denote this as Smoothie-Global. However, in Smoothie-Global each \(_{i}(x)\) for \(i[m]\) is a constant over the entire \(_{}\). Therefore, we use nearest neighbor kernel smoothing to estimate each \(_{ij}(x)\) in a sample-dependent manner, an approach we call Smoothie-Local. Concretely, for \(x_{}\), define \(_{n_{0}}(x)_{}\) as the \(n_{0}<n\) nearest neighbors of \(x\) (excluding \(x\) itself) in \(f_{0}\)'s embedding space. Then, we construct \(_{ij}(x)=}_{x^{}_{n_{0}}(x)} \|_{i}(x^{})-_{j}(x^{})\|^{2}\), and do the same for \(_{ik}(x),_{jk}(x)\) to get a sample-conditional estimate of \(_{i}(x)\). The procedure for estimating \(_{i}(x)\) in Smoothie-Local is outlined in Algorithm 1.

### Routing

Once we have estimates of \(_{i}(x)\) for each of the \(m\) generators by using Algorithm 1, we can construct our \(()\) function. We define \((,x)=g_{i}\) where \(i=\{_{1}(x),,_{m}(x)\}\), which selects the highest scoring LLM for input \(x\) based on \(_{i}(x)\). We apply this on \(_{}\) to determine the best LLM for each input sample.

## 5 Results

We empirically analyze Smoothie-Global and Smoothie-Local, focusing on four questions:

1. How well does Smoothie-Global recover ground-truth LLM rankings over samples belonging to the same task (Section 5.1)?
2. In multi-task datasets, how well can Smoothie-Local perform unsupervised-routing, by identifying the best LLM for each sample (Section 5.2)?
3. Can Smoothie-Global and Smoothie-Local be applied to select from or route between different prompts (Section 5.3)?
4. How does Smoothie-Global and Smoothie-Local's performance change as a function of different algorithmic choices (Section 5.4)?

### Single-Task LLM Scoring

SetupWe begin by evaluating whether Smoothie-Global can accurately learn the relative performance of different LLMs on a single task-dataset. We study three categories of tasks. First, we consider 7 datasets corresponding to commonly-studied natural language generation (NLG) tasks : CNN/DailyMail and XSum (summarization), SQuAD (reading comprehension), TriviaQA (factual recall), E2E and WebNLG (data-to-text generation), and LegalBench's Definition Extraction (text extraction) . We report Rouge2 for summarization and data-to-text generation tasks and accuracy for all others. For all tasks other than Definition Extraction we evaluate Smoothie-Global on a 1000 sample subset.2 For these tasks, we consider two ensemblesof LLMs at different size points. At the 3B size point, our ensemble consists of Pythia-2.8B , Gemma-2B , Incite-3B , and Dolly-3B . At the 7B size point, our ensemble consists of Llama-2 , Mistral , Vicuna , Gemma-7B , and Nous Capybara . We manually write a single prompt template for each task, and all model generations rely on this template.

Second, we consider two instruction-following benchmarks: AlpacaEval and MixInstruct . For AlpacaEval, we rely on responses accessible via the online leaderboard.3 We identify 10 LLMs (each from a different base family), and download these models' responses to the AlpacaEval instructions. We conduct 10 different simulations, where in each simulation we randomly select 5 LLMs from our pool to function as an ensemble. Reported win-rates use the standard GPT-4 references. For MixInstruct, we use generations from an ensemble of 11 different LLMs originally studied in . Following , we measure generation quality using a ChatGPT-based rank.

Finally, we consider a more "reasoning-intensive" task, GSM8K . We consider an ensemble of three models: Gemma-7B, Phi-2 , and Lema-7b . We prompt each model to provide a chain-of-though reasoning , and apply Smoothie to these generations.

For all datasets, we apply Smoothie-Global using SentenceBERT (all-mpnet-base-v2) embeddings of generations .

ResultsWe first measure how frequently the highest-weighted LLM according to Smoothie-Global corresponds to the best-performing LLM in the ensemble. We observe that Smoothie-Global selects the best-performing LLM for 4/7 tasks on the 3B ensemble, and for 5/7 tasks on the 7B ensemble (Figure 8). On AlpacaEval, Smoothie-Global selects the best-performing LLM by win-rate for 8/10 ensembles, and the best performing LLM by length-controlled win-rate for 7/10 ensembles. On MixInstruct and GSM8K, Smoothie-Global again identifies the best-performing LLM in the ensemble.

Second, we measure how well Smoothie-Global captures quality differences between LLMs in the ensemble, by computing the Spearman's rank correlation coefficient between \(_{i}\) and ground truth quality scores ensemble models. Overall, we find that Smoothie-Global's learned weights approximate the relative ordering of model quality well. On the NLG tasks Smoothie-Global we measure an average correlation coefficient (across both ensembles and seven tasks) of 0.72. Figure 2(a) visually depicts the distribution of task coefficients--on only one ensemble/dataset pair is there a correlation coefficient \( 0\). On MixInstruct, we observe a correlation coefficient of 0.94, and on AlpacaEval, we observe a correlation coefficient of 0.46.

Finally, we measure how the performance of the LLM selected by Smoothie compares to other selection algorithms. We first compare Smoothie-Global to an unsupervised random baseline (Random), which would select a random model from the ensemble. We reported the _expected_ performance of this method, which is equivalent to taking the average performance of the ensemble. We also compare Smoothie-Global to a labeled baseline which simulates selecting an LLM on the basis of a small amount of validation data  (Best-on-Val). We sample a small labeled validation set (50 samples) and select the LLM that performs the best on this set. To account for sampling variation, we repeat this with 10 random draws and report the average performance. Because AlpacaEval has no training split and MixInstruct has no labeled data, we only compare Smoothie-Global to Random on those datasets.

Table 1 provides results for the seven NLG tasks. We find that Smoothie-Global outperforms the unsupervised Random baseline on 6/7 tasks for the 3B ensemble and on 7/7 tasks for the 7B ensemble. Smoothie-Global outperforms Random by up to 7pts (on tasks measured by rouge2), and by up to 12pts (on tasks measured by accuracy). We also observe that Smoothie-Global is frequently competitive with and even outperforms the Best-on-Val baseline, which uses labeled data. Smoothie-Global outperforms Best-on-Val on 4/7 tasks for the 3B ensemble, and 5/7 tasks for the 7B ensemble. On GSM8K, Smoothie-Global achieves a solve-rate of 37.5% (matching Best-on-Val, while Random achieves a solve-rate of 28.3% (Table 11).

Smoothie-Global also outperforms the Random baseline on the instruction-following datasets. On MixInstruct, Smoothie-Global achieves a ChatGPT-rank (\(\)) of \(3.91\), while Random achieves a ChatGPT-rank of \(5.95\) (Table 10). On AlpacaEval, Smoothie-Global outperforms Random on all but one trial (across both win-rate and length-controlled win-rate). Smoothie-Globaloutperforms Random by an average of 15pt win-rate, and up to 27pts. Figure 2(b) and Figure 2(c) visualize this distribution.

### Multi-task Routing

SetupWe next assess whether Smoothie-Local's sample-conditional scoring mechanism allows it to route samples to LLMs in the multi-capability regime. We construct two mixed-task distributions by combining existing datasets. The first distribution corresponds to tasks measured by accuracy, and contains SQuAD, TriviaQA, and Definition Extraction. We refer to this as Distr-Acc. The second distribution corresponds to tasks measured by Rouge2, and contains CNN/DailyMail, XSum, Web NLG, and E2E. We refer to this as Distr-Rouge2. For each mixed-task dataset, we report the metric averaged across all tasks. We compare to three baselines.

* Random: A random-selection baseline which returns a generation from a random LLM in the ensemble. Though naive, prior work has found this to be a strong method in practice . We run 10 trials and report the mean of this approach to account for variance.
* Labeled-kNN: A labeled data-based KNN baseline. For this, we sample \(50\) labeled samples from a separate hold-out set (\(_{}\)), and measure the performance of each candidate LLM on this set. For a given test sample \(x\), we identify the \(20\) most semantically similar instances in \(_{}\) (using SentenceBERT embeddings ), and route \(x\) to the highest performing LLM over this subset. We note that the Labeled-KNN baseline is derived from routing methods in [48; 86].
* PairRM: A reward model from  which accepts an instruction and multiple generations as input, scores each generations suitability for the instruction, and returns the predicted best generation. PairRM is a labeled-data method which  trained on collected preference data.

In addition, we also compare the best individual model in the ensemble (Best-Model), and Smoothie-Global. For both mixed-task datasets, we run Smoothie-Local with SentenceBERT

    & & CNN & Def. Ext. & E2E & SQuAD & TriviaQA & WebNLG & XSum \\   & Random & 12.9 & 52.4 & 27.3 & 59.6 & 32.7 & 23.4 & 4.5 \\  & Smoothie-Global & **14.3** & **61.5** & **31.8** & 60.7 & 32.1 & **30.7** & 4.5 \\  & Best-on-Val & 13.0 & 60.5 & 31.1 & **66.4** & **38.7** & 30.3 & **5.3** \\   & Random & 13.7 & 58.5 & 35.3 & 67.9 & 59.3 & 44.1 & 6.9 \\  & Smoothie-Global & **14.5** & **70.9** & **36.9** & **76.2** & **68.3** & 45.9 & **8.4** \\   & Best-on-Val & **14.5** & 69.4 & 36.7 & 74.0 & 65.8 & **48.3** & 8.3 \\   

Table 1: Comparing Smoothie-Global to baseline methods on different ensembles across NLG datasets. Underlined values are the best performing _unsupervised_ methods. Bold values are the best performing _overall_ methods. We report rouge2 scores for CNN, XSum, WebNLG, and E2E, and accuracy for the rest. All metrics are scaled to 0-100.

Figure 2: **(a) Spearmanâ€™s rank correlation coefficient between Smoothie-Global weights and ground-truth LLM performance for 3B and 7B ensembles across NLG tasks. (b) Smoothie-Globalâ€™s improvement over Random by win-rate on AlpacaEval. (c)Smoothie-Globalâ€™s improvement over Random by length-controlled win-rate on AlpacaEval.**

embeddings, and the sample-conditional version of Smoothie-Local estimates \(_{i}(x)\) using a neighborhood size \(n_{0}=1\).

Results for the 3B and 7B ensembles over Distr-Acc and Distr-Rouge2 are provided in Table 2. We find that Smoothie-Local outperforms all baselines across both data distributions, for both ensembles. Though Smoothie-Local requires no labels, it still outperforms labeled data baselines like Labeled-kNN and PairRM. We observe a substantial gap between Smoothie-Local and Smoothie-Global, which indicates that Smoothie-Local's sample-specific scoring mechanism provides performance improvements.

Notably, we see that Smoothie-Local substantially betters Best-Model, indicating that Smoothie-Local's routing mechanism is offering a performance improvement over a strategy which merely selects the best LLM on average. We study this in greater detail by examining the relative rank of the LLM selected by Smoothie-Local for each sam

    &  &  \\ 
**Method** & Distr-Acc & Distr-Rouge2 & Distr-Acc & Distr-Rouge2 \\  Random & 48.7 & 17.0 & 65.4 & 25.0 \\  PairRM & 53.9 & 19.0 & 71.8 & 25.5 \\  Labeled-kNN & 51.0 & 16.8 & 71.7 & 26.2 \\  Best-Model & 52.3 & 18.1 & 73.2 & 26.4 \\  Smoothie-Global & 51.3 & 18.1 & 66.5 & 26.1 \\  Smoothie-Local & **58.7** & **20.2** & **75.0** & **26.9** \\   

Table 2: Comparing Smoothie-Local to baseline methods on the 3B and 7B ensembles for multi-task distributions. Distr-Acc and Distr-Rouge2 are measured with accuracy and rouge2 respectively. Bold values indicate the best performing method for each dataset and model size. Metrics are scaled to 0-100.

Figure 3: On Distr-Acc and Distr-Rouge2, we measure how frequently Smoothie-Local selects the \(i\)-th best generation across the ensemble, for both the 3B and 7B ensembles.

Distr-Acc and Distr-Rouge2, we rank the quality of each LLM's generation according to standard-competition ranking (i.e., "1-2-2-4" ranking). We then count how frequently Smoothie-Local selects the rank-\(i\) generation across each distribution for each ensemble. We visualize results in Figure 3. As the visualizations demonstrate, Smoothie-Local consistently selects the best or second-best generation from within the ensemble.

### Prompt Selection

Third, we study whether Smoothie-Local and Smoothie-Global can be generalized to other settings where engineers have a candidate pool of text generators of unknown quality, and must select one of them to use for some application. In particular, we focus on the setting where an engineer has access to multiple prompt templates for a given generation task, and must select which prompt-templates' generation to use as the final output . Unlike above, we assume the engineer only has access to one LLM. We study Smoothie-Local and Smoothie-Global in this regime using the NLG tasks from Section 5.1. For each task, we manually write between 3 and 5 prompt templates, varying the wording of instructions and the choice of in-context samples. We analyze Smoothie applied to two models at different size points: Falcon (1B)  and Llama-2 (7B) .

Table 3 provides the results. Overall, we find that Smoothie-Global selects the optimal prompt 2/7 times for Falcon-1B, and 3/7 times for Llama-2. Smoothie-Local and Smoothie-Global consistently outperform Random-on 6/7 tasks for Falcon-1b and 6/7 tasks for Llama-2. On 7 task/model combinations, one of either Smoothie-Global or Smoothie-Local matches or outperforms a labeled baseline. To better contextualize performance improvements from Smoothie-Global, we also compare to the improvement that accompanies increasing model size. Following a common practice in recent work, we can quantify the extent to which Smoothie-Global allows smaller models to match or exceed the performance of larger models [2; 29]. In Figure 4 (Appendix E), we compare Random and Smoothie-Global on models from the Pythia suite at four sizes: 410M, 1B, 2.8B, and 6.9B parameters . We observe that Smoothie-Global substantially improves performance--on E2E, Smoothie-Global enables a 410M parameter model to outperform a 6.9B parameter model.

### Ablations

Finally, we conduct ablations to examine different aspects of Smoothie-Global and Smoothie-Local: improving its efficiency, adjusting the neighborhood size, varying the choice of embedding model, and using different LLM ensembles.

**Improving efficiency** First, we explain Smoothie's current efficiency properties. To estimate the Smoothie weights for routing, we use a simple closed-form procedure that does not require any SGD or training, as described in Algorithm 1. As a result, Smoothie weights on the entire dataset can be computed in seconds--for the 7B ensemble, Smoothie-Local on the multi-task datasets takes

    & & CNN & Def. Ext. & E2E & SQuAD & TriviaQA & WebNLG & XSum \\   & Random & 7.1 & 60.3 & 27.8 & 47.3 & 22.0 & 29.2 & 4.7 \\  & Smoothie-Global & 7.9 & 62.2 & **31.6** & **53.3** & **31.4** & 28.3 & 6.4 \\  & Smoothie-Local & 8.0 & **69.2** & 31.5 & **53.3** & 27.4 & 30.8 & 6.0 \\  & Best-on-Val & **8.4** & 64.2 & 31.0 & 52.7 & **31.4** & **32.5** & **6.7** \\   & Random & 7.3 & 47.8 & 31.6 & 54.0 & 45.9 & 45.5 & 11.2 \\  & Smoothie-Global & 6.9 & **64.6** & **37.6** & 61.4 & **68.7** & 48.5 & 12.8 \\   & Smoothie-Local & 9.5 & 59.3 & 33.6 & 63.1 & 61.3 & 48.0 & 12.7 \\   & Best-on-Val & **11.8** & **64.6** & 35.0 & **66.1** & **68.7** & **48.7** & **13.0** \\   

Table 3: Comparing Smoothie-Global and Smoothie-Local to baseline methods in the prompt-selection setting. Underlined values are the best performing _unsupervised_ methods. Bold values are the best performing _overall_ methods. We report rouge2 scores for CNN, XSum, WebNLG, and E2E, and accuracy for the rest. All metrics are scaled to 0-100.

2.14 seconds per 1000 samples, and Smoothie-Global on the single-task datasets takes under 0.03 seconds per 1000 samples. Moreover, Smoothie does not require any ground-truth annotations; however, all \(m\) model generations per test sample are needed as input to the algorithm. That is, we need \(n m\) generations for a \(_{}\) of size \(n\) samples.

Fortunately, the need for computing all model generations per test sample can be removed with a small algorithm tweak, making Smoothie even more efficient and its runtime independent of \(n\). Suppose we have a held-out set of \(n_{}\) train samples with precomputed generations from the models in the ensemble. For each test sample, we retrieve the most similar train samples, learn the Smoothie weights for the sample using the corresponding train sample generations, and return the model with the highest Smoothie weight (i.e., in line 5 in Algorithm 1, KNN is now over a held-out training dataset). This approach, which we call Smoothie-Train, selects the model for a test sample without needing model generations for that sample. Only \(n_{} m\) generations are needed, regardless of how large the test dataset \(n\) is.

We study the NLG tasks, using \(n_{}=250\) samples. In Table 7 (Appendix E), we evaluate a version of Smoothie-Global-Train) and observe that it matches Smoothie-Global on 12/14 model-dataset pairs, and performs worse on the remaining 2/14 pairs. We also evaluate Smoothie-Local-Train, on Distr-Acc and Distr-Rouge2 (Table 8) using a neighborhood of size \(n_{0}=20\). We find here that while Smoothie-Local-Train underperforms Smoothie-Local on both the 3B and 7B ensemble for both Distr-Acc and Distr-Rouge2, it still outperforms Random and remains competitive with supervised baselines.

**Neighborhood size** We study the impact of \(n_{0}\), and consider Smoothie-Local's performance for \(n_{0}\). Figure 5 provides performance over Distr-Acc and Figure 6 provides performance over Distr-Rouge2. Overall, we find that Smoothie-Local's performance steadily degrades as \(n_{0}\) increases, and is highest when \(n_{0}=1\).

**Choice of embeddings** We study how the choice of embeddings affects Smoothie-Global performance (Table 9). Specifically, we compare the performance of Smoothie-Local using Sentence-Bert embeddings (all-mpnet-base-v2)  to BGE embeddings (bge-small-en-v1.5) . We observe that Smoothie-Local appears robust to different embeddings--Smoothie-Local with BGE embeddings still outperforms other labeled and unlabeled baselines. Interestingly, we observe that certain embedding models appear to yield better performance over certain distribution/ensemble combinations. For instance, Smoothie-Local with SentenceBERT embeddings outperforms Smoothie-Local with BGE embeddings on Distr-Acc for the 3B ensemble and Distr-Rouge2 for the 7B ensemble, while performing worse on Distr-Rouge2 for the 3B ensemble and Distr-Acc for the 7B ensemble.

**Different ensembles** Finally, we consider whether Smoothie-Global can generalize to a wider array of ensembles (Figure 7). We combine the LLMs contained in the 3B and 7B ensembles into a single pool, and sample \(50\) distinct ensembles ranging in size from 4-7 LLMs. For each of the \(7\) NLG tasks, we evaluate Smoothie-Global's ability to identify the best model from within each ensemble. Across these \(350\) settings, we find that Smoothie-Global identifies the best model in \(211\) of them (\(60.2\)% of the time), and one of the two best models in 292 of them (\(83\)% of the time).

## 6 Conclusion

In this paper we study and propose an algorithm for learning label-free routers for generative tasks. We validate our approach across a variety of evaluation regimes, finding it consistently beats other unsupervised approaches and often matches/exceeds supervised approaches.

**Limitations** We discuss several of Smoothie's limitations. First, its multivariate Gaussian graphical model currently uses a diagonal covariance matrix. This assumes independent error vectors for each generation, though Smoothie could be extended to account for dependencies [72; 93]. Additionally, Smoothie optimizes only for performance without considering cost tradeoffs between large and small models. Finally, its reliance on embeddings may capture only certain aspects of semantic similarity. Other embedding models and additional heuristics could be used to create richer input features for Smoothie.

Acknowledgements

We thank the Microsoft Accelerate Foundation Models Research Program for providing portions of the compute used for the results in this paper. We thank Gautam Machiraju, Jon Saad-Falcon, Krista Opsahl-Ong, Sabri Eyuboglu, Jordan Juravsky, Vishnu Sarukkai, Ben Spector, Eric Nguyen, Jerry Liu, Chris Fifty, Avanika Narayan, and Michael Zhang for their helpful feedback and discussion.

We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF2247015 (Hardware-Aware), CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under Nos. W911NF-23-2-0184 (Long-context) and W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under Nos. N000142312633 (Deep Signal Processing); Stanford HAI under No. 247183; NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Meta, Google, and VMWare. NG is supported by the Stanford Interdisciplinary Graduate Fellowship (SIGF). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.