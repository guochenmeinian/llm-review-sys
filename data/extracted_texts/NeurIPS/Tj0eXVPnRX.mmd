# Loss Dynamics of Temporal Difference Reinforcement Learning

Blake Bordelon, Paul Masset, Henry Kuo & Cengiz Pehlevan

John Paulson School of Engineering and Applied Sciences,

Center for Brain Science,

Kempner Institute for the Study of Natural & Artificial Intelligence,

Harvard University

Cambridge MA, 02138

blake_bordelon@g.harvard.edu, cpehlevan@g.harvard.edu

###### Abstract

Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent dynamics. We study how learning dynamics and plateaus depend on feature structure, learning rate, discount factor, and reward function. We then analyze how strategies like learning rate annealing and reward shaping can favorably alter learning dynamics and plateaus. To conclude, our work introduces new tools to open a new direction towards developing a theory of learning dynamics in reinforcement learning.

## 1 Introduction

Reinforcement learning (RL) is a general paradigm which allows agents to learn from experience the relative value of states in their environment and to take actions that maximize long term rewards . RL algorithms have been successfully applied in a number of real world scenarios such as strategic games like backgammon and Go, autonomous vehicles, and fine tuning language models .

Despite these empirical successes, a theoretical understanding of the learning dynamics and inductive biases of RL algorithms is currently lacking . A large fraction of the theoretical work has focused on proving convergence and deriving bounds both in the asymptotic  and non-asymptotic  limits, but do not provide a full picture of the evolution of the learning dynamics.

A desired feature of a candidate theory is to characterize the influence of function approximation to RL dynamics and its performance. Early versions of RL operated in a tabular setting, similar to dynamic programming , where all the states in the environment could be mapped one-to-one to a specific value and policy. In large and complex environments, it is not possible to enumerate all the states in the environment necessitating the use of function approximation for the target value and policy functions. Indeed, the recent success of many RL algorithms relies on deep reinforcementlearning architectures that combine an RL architecture with deep neural networks to build effective value estimators and policy networks .

One difficulty in analysing these algorithms compared to supervised learning settings is that the distribution of the data received at each time-step is not stationary. This non-stationarity arises from two principal sources: First, whether in an episodic or continuous setting, states visited within a learning trajectory are dependent on the recent past. Trajectories might be randomly sampled but points within a trajectory are correlated. Second, when the policy is updated it also changes the distribution of future visited states.

Here, we will focus on the first form of non-stationarity when learning a value function in the context of _policy evaluation_ using a classical RL algorithm, temporal difference (TD) learning . We develop a theory of learning dynamics for RL in this setting in a high dimensional asymptotic limit with a focus on understanding the role of linear function approximation from a set of nonlinear and static features. In particular, we leverage ideas from recent work in application of statistical physics to machine learning theory to perform an average over the possible sequences of features encountered during learning. Our contributions are as follows:

* We introduce concepts from statistical physics, including a path integral approach to describe dynamics [21; 22; 23; 24; 25] and the Gaussian equivalence assumption [26; 27; 28; 29], to derive a theory of learning dynamics in TD learning (SS3) in an online setting. We provide an analytical formula for the typical case learning curve for TD learning.
* We show that our theory predicts scaling of the learning convergence speed and performance plateaus with parameters of the problem including task-feature alignment , learning rate, discount factor or batch size (SS4 and SS5). Task-feature alignment is a metric that quantifies how features allow fast or slow learning for a given task.
* We show our theory can be used to understand and guide design principles when choosing meta-parameters. Specifically, we show that we can use our theory to infer optimal schedules of learning rate annealing and the effects of reward shaping (SS5 and SS6).

## 2 Problem Setup and Related Works

### Problem Setup

We consider a set of states denoted by \(s\), possibly continuous, and a fixed policy \(\) which generates a distribution over actions given the state. The state dynamics are defined by a distribution \(p()\) over trajectories through state space \(=\{s_{1},s_{2},...,s_{T}\}\). Note that state transitions do not have to be Markovian, but each trajectory is i.i.d. sampled from \(p()\). We consider trajectories of length \(T\). Each state is represented by an \(N\)-dimensional feature vector \((s)^{N}\), so that trajectory generates a collection of feature vectors \(\{(s_{t})\}_{t=1}^{T}\). The rewards are generated by a reward function \(R(s)\) which depends on the state. (In general, the features and rewards can depend on action as well: transition dynamics are still fixed as the policy is fixed, but variance over rewards at a given state may need to be modeled, see Appendix B.5).

At any time, we are interested in characterizing the _value function_ associated with a state, which measures the expected discounted sum of future rewards when starting in state \(s_{0}\)

\[V(s_{0})=R(s_{0})+_{t 1}_{s_{t}|s_{0}}^{t}R(s_{t})=R(s_ {0})+_{s_{1}|s_{0}}V(s_{1}).\] (1)

We use linear function approximation to learn the value function \((s)=(s)\). Similar to kernel learning , the features \(\) should be high dimensional so that they can express a large set of possible value functions.

We study TD learning dynamics given this setup. At each step of the TD iteration, we sample a batch of \(B\) independent trajectories from the distribution and compute the TD update

\[_{n+1} =_{n}+}{TB}_{=1}^{B}_{t=1}^{T} _{n}^{}(t)(s_{n}^{}(t)),\] \[_{n}^{}(t)  R(s_{n}^{}(t))+(s_{n}^{}(t+1))-(s_{n }^{}(t)).\] (2)We operate in a online batch regime as the trajectories in each batch are resampled at each iteration. This is distinct from an offline setting where the batches would be resampled from a finite-sized buffer . Convergence considerations for infinite-batch online TD learning width different types of features \(\) are outlined in Appendix A. The specific form for the TD-error \(_{n}^{}(t)\) depends on the precise variant of TD learning that is used. Here, we will focus on TD(0) but our approach can be extended to other TD learning rules and definitions of the return function. We see that the iterates \(_{n}\) will form a stochastic process as each sequence of states in an episode \(\{s_{n}^{}(t)\}\) are drawn randomly from \(p()\). In general, we allow the learning rate \(_{n}\) to depend on iteration, an important point we will revisit later. The distribution of features \(\{(s_{n}^{}(t))\}\) over random trajectories \(\) is in general quite complicated, depending on the details of the state transitions and the nonlinear feature maps, which motivates the following question:

**Question: _How can the stochastic dynamics of temporal difference learning be characterized for complicated trajectory distributions \(p()\) and feature maps \((s)\)?_**

To address this question, in this work, we provide an analysis of TD learning that explicitly models the statistics of stochastic semi-gradient updates to \(_{n}\). Our framework is based on a Gaussian equivalence ansatz for TD learning and high dimensional mean field theory which predicts the statistics of TD errors \(_{n}^{}(t)\) and the weight iterates \(_{n}\). The theory reveals a rich set of phenomena including plateaus unique to SGD noise in TD learning which can be ameliorated with learning rate annealing.

### Related Works

The dynamics of TD learning have been notoriously difficult to analyse. Unlike supervised learning settings, sampled states are correlated across a trajectory and the algorithms involve bootstrapping: using estimates of the value function for future states in the temporal difference update . Some prior works study the least-square TD learning rule, which solves, at each step \(n\) of the algorithm, a linear system for the instantaneous best fit to \(n\) samples . Alternatively, many works focus on the online SGD version of TD learning, where incremental updates are made to the parameters at each step, using fresh samples. This is the setting of our work. The focus of this literature has initially been to prove convergence and bounds on asymptotic behavior . More recently, progress has been made in deriving bounds in the non-asymptotic regime. Initial work assumed that data samples were _i.i.d._ and recent work has extended those approaches to Markovian noise . The majority of these proofs use the ODE-like method for stochastic approximation , which corresponds to a limit of the stochastic semi-gradient dynamics where the effects of mini-batch noise are neglected. This is also known as the "mean-path" dynamics of TD learning and will correspond to the infinite batch limit of our theory. Furthermore, many of these methods require the use of iterative averaging of the learned value function, whereas we study the final iterate convergence. The approach we take here differs from many of these results as our goal is not to provide bounds on worst-case behavior but instead to provide a full description of the dynamics of the typical case scenario during learning.

Our approach also highlights the importance of the structure of the representations in controlling the dynamics of learning. This had been long been recognized in reinforcement learning and previous works proposed to improve feature representations to improve algorithmic performance . This line of work has shown the importance of the relative smoothness of the representations and target functions in the ODE limit of TD dynamics . Similarly, several methods have been proposed to empirically learn a better shaping function . In _policy learning_ it has also been recognized that using a gradient aligned to the statistics of the tasks, such as the natural gradient  can greatly speed up convergence . Our work does not explore such feature learning per se but could be used as a diagnostic tool to analyse how representations impact learning speed.

We adopt the perspective of statistical physics, by working with a simplified feature distribution which captures the learning dynamics and solving the theory in a high-dimensional limit . We derive TD reinforcement learning curves from a mean field theory formalism which is exact for infinite dimensional features and batch size. Similar calculations for supervised learning on Gaussian data have been shown to provide an accurate description of high dimensional dynamics . Further, even when data is not actually Gaussian, several algorithms, such as kernel or random-features regression, exhibit universality in their loss behavior, enabling analysis of the learning curve with a simpler Gaussian proxy . We exploit this idea in the TD learning setting to some success. We note that Gaussian equivalence or universality is not a panacea, and in many cases the Gaussian proxy can fail to capture important machine learning phenomena .

Theoretical Results for Online TD Learning

### Computation of Learning Curves

We develop a dynamical mean field theory (DMFT) formalism can be utilized to compute the learning curves. We provide the full derivation of the DMFT in Appendix B. This computation consists of tracking the moment generating function for the iterates \(_{n}\) over the trajectories of randomly sampled features \(\{_{}^{n}(t)\}_{t=1}^{T}\). In an appropriate high dimensional asymptotic limit, the results of our theory can be summarized as the following proposition.

**Proposition 3.1**.: _Let \(N,B\) with \(B/N=(1)\) and episode length \(T=(1)\). Let the ground truth reward function be \(R(s)=_{R}(s)\) and value function \(V(s)=_{TD}(s)\) in the basis of our features. Define matrices_

\[}_{t}(t,t),}_{+}_{t}(t,t+1),}-}_{+},\] (3)

_and assume that the features are such that matrix \(\) is of extensive rank in \(N\). Then the typical value estimation error \(_{n}=(V(s)-_{n}(s))^{2}_ {s}\) after \(n\) steps has the form_

\[_{n} =}_{n},\] (4) \[_{n+1} =(-)_{n}(-)^{}+}{^{2}T^{2}}_{tt^{}}Q_{n}(t,t^{})(t,t ^{})\] (5) \[Q_{n}(t,t^{}) =(_{R}-_{n})^{}(t,t^{})(_{R}-_{n})+ (_{R}-_{n})^{}(t,t^{}+1)_{n}\] \[+_{n}^{}(t +1,t^{})(_{R}-_{n})+}{N} _{n}^{}(t+1,t^{}+1)_{n},\] (6)

_where \(=B/N\) and \(Q_{n}(t,t^{})=_{n}(t)_{n}(t^{})\) is the correlation of randomly sampled TD-errors at episodic times \(t,t^{}\) and iteration \(n\). The average over weights \(\) denotes a Gaussian average whose moments are related to \(_{n}\). The correlation function \(Q_{n}(t,t^{})\) depends on \(_{n}\) and the average weights \(_{n}\); we provide its full formula in Appendix B.3, equation (B.17)._

Proof.: The full derivation is in Appendix B. At a high level, we track the moment generating function of the iterates \(_{n}\) over random draws of features \(\{_{n}^{}(t)\}\), \(Z[\{_{n}\}]=_{\{_{n}^{}(t)\}}(i_{n} _{n}_{n})q(S[ q,\{_{n}\}])\) where \(S\) is a \((1)\) action and \(q\) are a set of order parameters of the theory which include the following overlaps \(C_{n}(t,t^{})=_{n}^{}(t,t^{})_{n}\) and \(Q_{n}(t,t^{})=_{=1}^{B}_{n}^{}(t)_{n}^{}( t^{})\). In this high dimension \(N,B\) limit with \(B/N=(1)\) and episode length \(T=(1)\), the order parameters can be obtained from saddle point integration, which requires solving \(=0\). This procedure results in a deterministic learning curve given in equations (4),(5),(6) even though the realization of sampled states are disordered. The TD-error variables \(_{n}(t)\) become mean zero Gaussians and the \(\{_{n}\}\) also follow a Gaussian distribution with mean and variance determined by the order parameters. 

Before we explore the predictions of this theory, we first make a few remarks about this result.

_Remark 1_.: Though the theory is technically derived for large batch size \(B\), we will show that it provides an accurate description of the loss trajectory even for batches as small as \(B=1\). An alternative formulation in terms of recursive averaging reveals transparently which approximations lead to the same result as the mean field theory (Appendix B.6).

_Remark 2_.: The case where the reward function and/or the value function are inexpressible by the features \(\) can also be handled within this framework. In this case, the unlearnable components of the value function act as additional noise which limits performance . These can also be handled by our theory, see Appendix A.

_Remark 3_.: The limit where \(=0\) recovers known results in online supervised learning with stochastic gradient methods . In this limit, the dynamics will converge to zero loss provided the model features are sufficiently rich to represent the true value function.

_Remark 4_.: The TD learner with perfect coverage (infinite batch size) at each step will converge to the ground truth \(_{TD}=(}-}_{+})^{-1}}_{R}\) (see Appendix A).

_Remark 5_.: \(_{n}\) is equivalently defined as \(_{n}=(-_{TD})(-_{TD})^{} _{\{_{n^{}}^{}\}_{n^{}<n}}\), which measures deviation from the fixed point of gradient flow (vanishing learning rate) dynamics \(_{TD}\) over random sets of sampled episodes (Appendix B).

### Gaussian Approximation

The theory presented in Section 3.1 relies on an approximation of the feature distribution as Gaussian. Similar approximations have been successfully utilized in high dimensional regression problems even when the true features are non-Gaussian . We note that an exact, non-asymptotic theory for non-Gaussian features can be provided which closes under knowledge of the fourth cumulants of the features as we show in Appendix D, though this theory is especially cumbersome to analyze or evaluate compared to the theory of Section 3.1. Concretely, Proposition 3.1 relies on the following.

**Gaussian Feature Assumption**.: _The learning curves for a TD learner with high dimensional features \(\{(s_{t})\}_{t=1}^{T}\) over random \(\) are well approximated by the learning curves of a TD learner trained with Gaussian features \(_{G}(,+^{})\) with matching mean and correlations_

\[(t)=(s_{t})_{ p()}\,(t,t^{})=(s_{t})(s_{t^{ }})^{}_{ p()}.\] (7)

_where averages are taken over sequences of states \(\{s(t)\} p()\)._

One interpretation of this ansatz is that the dependence of the learning curve on higher order cumulants of the features is negligible in high dimensional feature spaces under the square loss. This

Figure 1: An illustration of our theory for TD learning. (a) A diffusion process in a 2D grid world generates many possible trajectories through state space. Each colored line is a different trajectory. Reward function is shown in red, with darker red indicating higher reward. (b) When combined with nonlinear place cell feature representation, the state transitions generate a distribution over observed features \(\{(s_{t})\}\). (c) The value error associated with TD learning for a bump reward function on the true features generated from a single set of MDP trajectories (blue) is compared to training on sampled Gaussian vectors \(\{_{t}\}\) with matching within-episode covariance structure. These single runs of TD learning on either set of features are consistent with the typical case theory (black dashed). (d) The structure of the features alters learning dynamics. We consider, for simplicity, altering the bandwidth (BW) of the place cell features. (e) Varying place cell BW changes the dynamics for both large batch (\(B=30\)) and (f) small batch (\(B=3\)) TD learning. There is an optimal BW for a given step size. Small batch stochastic semi-gradient noise is more severe.

approximation has been shown to provide an accurate description on realistic supervised learning settings with non-Gaussian data with the square loss in prior works [26; 27; 29; 30; 55; 58]. As shown in these works, for standard supervised learning, even highly non-Gaussian features \(\{(s_{t})\}\) have least squares learning curves which are only sensitive to the first two cumulants of the distribution. We do not aim to provide a rigorous proof of this ansatz for TD learning but instead compute the learning curve implied by this assumption and compare to experiments on simple Markov Decision Processes (MDPs). The benefit of this hypothesis in the RL setting is that it abstracts away details of transitions in the state space and instead deals with the correlations of sampled features through time.

To illustrate an example of the Gaussian Equivalence idea, in Figure 1, we consider an MDP which is defined by diffusion through a 2-dimensional (2D) state space (Figure 1(a)). We choose the features \((s)\) to be a collection of localized 2D Radial Basis Function (RBF) bumps which tile the 2D space, similarly to the "place cell" neurons found in the mammalian hippocampus [60; 61] (Figure 1(b)). The feature map is parameterized by the bandwidth of individual "place cells". In Figure 1(c), we show the value error learning curve as a function of the number of steps \(n\) (blue) and compare the value estimation error of the MDP with a Gaussian distribution for \((t)\) with matching first and second moments (orange). Lastly, we plot the theoretical prediction of our theory (described in Section 3), which is computed under the Gaussian equivalence ansatz (black dashed). We see a remarkable match of the three curves. The equivalence can be used to predict the speed of TD learning for different features, such as place cells with varying bandwidth as we illustrated in Figure 1 (d)-(f). In Figure 1 (e) and (f), we plot the loss trajectories for a single run of TD for each feature set. We observe that bandwidth affects both the learning dynamics and the asymptotic error with an optimal bandwidth at any step. One of our goals will be to elucidate the role of feature quality in learning dynamics. While the large batch dynamics are approximately self-averaging, as shown by the fact that single runs of TD learning coincide with our theoretical typical case theory curves, there is significant semi-gradient variance in the value error at small batch sizes. While we expect Gaussian equivalence to hold for high dimensional features, in low dimensions non-Gaussian effects can significantly alter the learning curves as we show in Appendix D.1. However, for high dimensional features, the equivalence holds for many other feature distributions such as polynomial and fourier features (Appendix E).

## 4 Spectral Perspective on Hard Reward Functions

Our theory can provide some insights into the structure of tasks which can be learned easily and which require more sampled trajectories to estimate based on spectral decompositions of the feature covariances. We note that similar spectral arguments have been given in the ODE-limit  and are intimately related to the source conditions used in recent work to identify power-law rates in the large batch regime .

To build our argument, we diagonalize the matrix \(=}-}_{+}\), obtaining \(_{k}=_{k}_{k}\), noting that eigenvalues \(_{k}\) can be complex. We then expand the TD solution in this basis \(_{TD}=_{k}w_{k}_{k}\).

Figure 2: Reward functions and dynamics which lead to value functions with high spectral alignment to the features can be learned more quickly than those that do not. (a) A sparse and dense reward function in a 2D spatial navigation task can illustrate this effect. (b) The cumulative power distribution \(C(k)\) defined from the spectral decomposition of \(=}-}_{+}\). Concretely we let \(_{k}=_{k}_{k}\) with \(_{k}\) ordered by real part and \(_{TD}=_{k}w_{k}_{k}\). In the \(B\) limit the task which has rapidly rising \(C(k)=w_{}^{2}}{_{}w_{}^{2}}\) will converge more quickly than the task with slowly rising \(C(k)\). (c) Indeed, for large batch regime (\(B=20\)) the value error decreases more rapidly for \(R_{2}\) than for \(R_{1}\).

The theory predicts that, the average learned weights will be \(_{n}=_{k}|1-_{k}|^{n}e^{i_{k}n}w_{k}_{k}\), where \(||\) is complex modulus and \(_{k}=(1-_{k})\). We can therefore order the modes by their convergence timescales \(|1-_{k}|\). Given this ordering of timescales, we can order the modes \(k\) from those with smallest to largest timescales. Given this ordering, we see that tasks can be learned efficiently are those with most of the norm of \(_{k}\) in the modes with small timescales. We quantify how well aligned a task is to a given feature representation by computing a cumulative power distribution for the target weights \(C(k)=w_{}^{2}}{_{}w_{}^{2}}\). If this quantity rises rapidly with \(k\) then the task can be learned from a small number of samples .

We consider again, the setting of Figure 1, the 2D exploration MDP but now contrast two different reward functions. In Figure 2 we show that this spectral decomposition can account for the gaps in loss for a place cell code in learning a sparse or dense reward function (Figure 2(a)). As expected the cumulative power rises more rapidly for the dense reward function \(R_{2}(s)\) (Figure 2(b)). As a consequence, the value error converges to zero more rapidly than for the sparse rewards.

## 5 Stochastic Semi-Gradient Learning Plateaus and Annealing Strategies

The stochastic noise from TD learning has striking qualitative differences from SGD noise in the standard supervised case. In standard supervised learning (such as \(=0\) version of this theory), the stochastic gradient noise does not prevent the model from fitting the target function with zero error provided the features are sufficiently rich to represent the target function. However, this is not the case in TD learning, where the predicted value \((s)\) is bootstrapped using the model's weights \(_{n}\) at each iteration \(n\). This leads to asymptotic plateaus in learning curves. Our theory can predict these plateaus and their scaling whose proof is given in Appendix B.7.

**Proposition 5.1**.: _Our theoretical learning curves exhibit a fixed point for the value error dynamics for finite \(B\) and non-zero \(\) and \(\). For small \(}{B}\), we deduce that \(\) satisfies a self-consistent

Figure 3: Finite batch size, discount factor and learning rate all contribute to a stochastic semi-gradient plateau in the TD dynamics. The features are generated from a synthetic power law covariance with exponential temporal autocorrelation (see Appendix G). Dashed black lines are theory. In general, for fixed learning rate \(\), the plateau scales as \((^{2}B^{-1})\). (a) Larger batch sizes \(B\) reduce SGD noise and leads to a lower plateau in the reducible value error for a decoupled power-law feature model. (b) Larger discount factor \(\) and (c) larger learning rate \(\) lead to higher SGD plateau floor. (d) An annealing strategy \(_{n}_{0}n^{-}\) for \(>0\) can allow one to avoid the plateau. For slow annealing (small \(\)), the error scales as \(_{n}(n^{-})\). (e) The value error as a function of the learning rate annealing exponent \(\) defined by \(_{n}=_{0}n^{-}\). For this task, the optimal exponent balances the scale of the asymptote with the rate of convergence.

asymptotic scaling of the form \(=(}{B})\) implying an asymptotic value error scaling of \(}( }{B})\)._

In Figure 3, we demonstrate that our theory predicts the plateaus and their scaling as a function of finite batch size \(B\) (Figure 3(a)), non-zero discount factor \(>0\) (Figure 3(b)) and non-negligible learning rate (Figure 3(c)).

A strategy used in the literature to increase rates of convergence and improve asymptotic behavior is adaptation of the learning learning through an annealing schedule [1; 62; 63; 16]. To overcome this plateau in the loss, we consider annealing the learning rate \(_{n}\) with iteration \(n\). In Figure 3(d), we show the effect of annealing the learning rate as a power law \(_{n}=_{0}n^{-}\) for some non-negative exponent \(\). For \(=0\) the learning rate is constant and a fixed plateau is reached. For small nonzero \(\), such as \(=0.2\), the value error is, after an initial transient, always near its instantaneous fixed point plateau so the loss scales linearly with the learning rate, giving the asymptotic rate \(_{n}(n^{-})\). For large \(\), the learning rate decreases very quickly and the plateau is never reached. Our approach can be used to find an optimal annealing exponent \(\) and in Figure 3(e), we show that the optimal annealing exponent balances these effects and is well predicted by our theory.

## 6 Reward Shaping

Another strategy to improve the learning dynamics in reinforcement learning algorithms is reward shaping . In standard supervised learning, the goal is to directly approximate the target objective given a cost function. However, in reinforcement learning, the objective is not to estimate rewards at each state directly but the discounted sum of future rewards, the value function. Importantly, many different reward schedules can lead to identical value functions. Reward shaping exploits this symmetry to speed up learning by altering the structure of TD updates and SGD noise. Here, we provide a theoretical description of the changes in the learning dynamics due to reward shaping which suggests they can be understood through a change of the alignment between the original rewards and the reshaped rewards in the space of the features used to represent the states.

The original ideas around reward shaping were inspired by work in experimental psychology and were closer to what is now studied as curriculum learning [65; 66; 67]. Reward shaping as currently used in reinforcement learning directly changes the reward function by adding a potential-based shaping function \(F\) such that \(F(s_{t},a,s_{t+1})=(s_{t+1})-(s_{t})\). In each step of the algorithm we feed

Figure 4: The theory can be used to understand how reward shaping decisions alter temporal difference learning dynamics. (a) A visualization of possible reward shaping potentials \((s)=_{}(s)\) strategies in feature space. Probability density level curves for the features are depicted in blue. Reshaping with \(_{}=_{TD}\) for scale factor \(\) merely changes the scale of weights which must be recovered (gold) and does not change timescales of TD dynamics. (b) The value error dynamics for the scale based reward shaping for the features in Figure 3. On the other hand, rotation based reward shaping where \(_{}\) is not parallel to \(_{V}\) (red) leads to a potentially helpful mixture of timescales if the new target vector is more aligned with feature dimensions with high variance (purple). In (c), we plot loss curves for rotation angle \(\) between the original mode \(_{V}\) and the top eigenvector of the feature covariance matrix \(}\). Dashed black lines are theory.

the following _reshaped rewards_\(\) to the TD learner

\[(s_{t})=R(s_{t})-(s_{t+1})&t=0\\ R(s_{t})+(s_{t})-(s_{t+1})&t>0.\] (8)

We note that this transformation simply offsets the target value function by \((s)\) as the series above telescopes with a cancellation of \((s_{t})\) between the \(t-1\) and \(t\)-th terms  (see Appendix C). However, the dynamics of TD learning with these reshaped rewards \(\) is quite distinct from the dynamics with original rewards \(R\). Here, we study the case where we can express \((s)\) as a linear function of our features: \((s)=(s)_{}\). This leads to a change in the dynamics for \(_{n}\) and \(_{n}\) that we describe in the Appendix C.

In Figure 4, we illustrate the possible benefits of reward shaping. We explore two types of reward shaping. First, a scale based reward shaping where \(_{}\) is parallel to the target TD weights \(_{TD}\). This merely changes the overall scale of the weights needed to converge in the dynamics, leading to similar timescales and an identical plateau for TD learning as we show in Figure 4 (b). On the other hand, reward shaping which rotates the fixed point of the TD dynamics into directions of higher feature variance can improve timescales of convergence. In Figure 4 (c), we show an example where we vary the angle \(\) of the shaped-TD fixed point (see also Appendix C).

## 7 TD Learning Plateaus in More Realistic Settings

In this section, we test if some of the phenomena observed in our theory and experiments also hold in more realistic settings. We perform TD learning with Fourier features to evaluate a pre-trained policy on MountainCar-v0. As expected, we see that the value error plateaus to an error level determined by both the learning rate (Figure 4(a)) and batch size (Figure 4(b)) due to semigradient noise.

We show that the plateaus obey the predicted scalings of \(( B^{-1})\) in Appendix F.

## 8 Discussion

Our work presents a new approach using concepts from statistical physics to derive average-case learning curve for _policy evaluation_ in TD-learning. However, it is only a first step towards a new theory of learning dynamics in reinforcement learning.

One major limitation of the present work is that it concerns linear function approximation where the features representing states/actions are fixed throughout learning. This limit can apply to neural networks in the "lazy" regime of training [68; 69], however it cannot account for neural networks that adapt their internal representations to the structure of the reward function. This differs from the setting of most practical algorithms, including in deep reinforcement learning, that specifically adapt their representations.

Figure 5: Policy evaluation in MountainCar-v0 environment. The policy was learned with tabular \(\)-greedy Q-learning (see Appendix F for details). (a) Value error curves for different \(\) when \(B=1\). (b) Value error curves for different \(B\) with \(=0.1\). Shaded area denotes 95% confidence interval over 10 seeds.

Our theory provides a description of learning dynamics through a set of iterative equations (Proposition 3.1). In Figure 1 we evaluate these dynamics for a simple MDP but although the predicted dynamics present an excellent fit to the empirical simulations, the iterative equations can be difficult to interpret and computationally expensive to evaluate in a larger network and more realistic tasks. Nevertheless, our equations can be used to derive some scaling between key parameters of the algorithm for example by studying their fixed points as in Proposition 5.1.

Here, we considered the simplest form of temporal difference learning, batched online TD(0). In future work, it will be important to further characterize the behavior for online TD(0) with batch size \(B=1\) and to expand our approach to TD(\(\)) and other return distributions. Similarly, expanding our theory to the offline setting, in which the buffer of resampled trajectories would be of finite size, could provide an understanding of how the interactions between parameters govern convergence and divergence .

Another limitation of our work is that we only considered the setting of _policy evaluation_ with a fixed policy. The goal of an RL agent is to learn how to act in the work and not merely to represent the value is its states. Unlike in supervised learning, the changes in the value function affect the policy but in many of RL algorithms, for example in _actor-critic_ architecture, there is a separation of the _policy evaluation_ (critic) and the _policy learning_ (actor) . Such algorithms estimate the value associated with state/action pairs under a given policy and then use this information to make beneficial updates to the policy, usually with the value and policy functions approximated by separate neural networks. In this paper, we only treated the first part of this process. Recently, a related approach has been used to analyse the dynamics of _policy learning_ in an "RL perceptron" setup . A full theory of reinforcement learning combining _policy evaluation_ and _policy learning_ remains difficult due to the interaction between the two processes, but combining these approaches would be fruitful. One promising direction is in settings where the timescales of the two processes are different , such as when _policy learning_ occurring at a much slower rate which is often the case in practice.

Beyond developing a theory of learning dynamics in reinforcement learning, the approach could be used in neuroscience to understand how neural representation of space or value can shape the learning dynamics at the behavioral level. Ideas from reinforcement learning have been extremely influential to understand phenomena observed in neuroscience and have been mapped directly onto specific brain circuits . The place cells of the hippocampus  exhibit localized tuning as the example in Figure 1 and together with grid cells in enthorinal cortex are thought to be crucial for navigation in spatial and cognitive spaces and their tuning is shaped by experience . Our theory specifically link the structure of representations, policy and reward to learning rates, which can all be experimentally measured simultaneously and could shed on light on how the spectral properties of representations govern learning and navigation , similarly to how the mean field theories we have used here can explain learning of sensory features . Future work could straightforwardly extend this DMFT formalism to deal with replay of sampled experiences during TD learning  at the cost of tracking correlations of weight updates across iterations of the algorithm .

To summarize, our work provide a new promising direction towards a theory of learning dynamics in reinforcement learning in artificial and biological agents.