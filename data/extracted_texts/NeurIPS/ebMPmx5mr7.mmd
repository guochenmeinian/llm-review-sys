# Semantic HELM: A Human-Readable Memory

for Reinforcement Learning

 Fabian Paischer \({}^{1}\), Thomas Adler \({}^{1}\), Markus Hofmarcher \({}^{2}\), Sepp Hochreiter \({}^{1}\)

\({}^{1}\) ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning,

\({}^{2}\) JKU LIT SAL eSPML Lab, Institute for Machine Learning,

Johannes Kepler University, Linz, Austria

paischer@ml.jku.at

###### Abstract

Reinforcement learning agents deployed in the real world often have to cope with partially observable environments. Therefore, most agents employ memory mechanisms to approximate the state of the environment. Recently, there have been impressive success stories in mastering partially observable environments, mostly in the realm of computer games like Dota 2, StarCraft II, or MineCraft. However, existing methods lack interpretability in the sense that it is not comprehensible for humans what the agent stores in its memory. In this regard, we propose a novel memory mechanism that represents past events in human language. Our method uses CLIP to associate visual inputs with language tokens. Then we feed these tokens to a pretrained language model that serves the agent as memory and provides it with a coherent and human-readable representation of the past. We train our memory mechanism on a set of partially observable environments and find that it excels on tasks that require a memory component, while mostly attaining performance on-par with strong baselines on tasks that do not. On a challenging continuous recognition task, where memorizing the past is crucial, our memory mechanism converges two orders of magnitude faster than prior methods. Since our memory mechanism is human-readable, we can peek at an agent's memory and check whether crucial pieces of information have been stored. This significantly enhances troubleshooting and paves the way toward more interpretable agents.

## 1 Introduction

In reinforcement learning (RL) an agent interacts with an environment and learns from feedback provided in the form of a reward function. In many applications, especially in real-world scenarios, the true state of the environment is not directly accessible to the agent, but rather approximated via observations that reveal mere parts of it. In such environments, the capability to approximate the true state by virtue of an agent's perception is crucial (Astrom, 1964; Kaelbling et al., 1998). To this end, many approaches track events that occurred in the past. The brute-force strategy is to simply store all past observations. However, this is often infeasible and it is much more efficient to store more abstract representations of the history. Thus, many RL algorithms use memory mechanisms such as LSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017) to compress sequences of high-dimensional observations. This has led to impressive successes mostly in the realm of mastering computer games on a human or even super-human level. Some examples are Dota 2 (Berner et al., 2019), StarCraft II (Vinyals et al., 2019), or MineCraft (Baker et al., 2022; Patil et al., 2022).

Most state-of-the-art methods dealing with partial observability in RL employ a memory mechanism that is not intelligible for humans. In this regard, we draw inspiration from the semantic memorypresent in humans (Yee et al., 2017) and propose to represent past events in human language. Humans memorize abstract concepts rather than every detail of information they encountered in the past (Richards and Frankland, 2017; Bowman and Zeithamova, 2018). Their ability to abstract is heavily influenced by the exposure to language in early childhood (Waxman and Markov, 1995). Further, humans use language on a daily basis to abstract and pass on information. Therefore, language is a natural choice as a representation for compounding information and has the key advantage of being human-readable. This enables analyzing if critical pieces of information have entered the memory or not. Based on this data, it becomes clear which parts of the system require refinement. Moreover, natural language has been shown to be effective as a compressed representation of past events in RL (Paischer et al., 2022).

Our proposed method, Semantic HELM (SHELM), leverages pre-trained foundation models to construct a memory mechanism that does not require any training. We use CLIP (Radford et al., 2021) to associate visual inputs with language tokens. Thereby, the vocabulary of the CLIP language encoder serves as a semantic database of concepts from which we retrieve the closest tokens to a given observation. These tokens are passed to a pretrained language model that serves as memory and provides the agent with a coherent representation of the past.

We illustrate the benefits of a human-readable and semantic memory in partially observable RL problems. First, we conduct a qualitative analysis on whether a CLIP vision encoder is capable of extracting semantics out of synthetic environments. Then, we test SHELM on a set of partially observable 2D MiniGrid (Chevalier-Boisvert et al., 2018), and 3D MiniWorld (Chevalier-Boisvert, 2018) environments. We find that even though these environments are only partially observable, they often do not necessitate a memory mechanism as they are solvable by a memory-less policy. The MiniGrid-Memory task, however, clearly requires memory and SHELM reaches state-of-the-art performance. On more realistic 3D environments such as Avalon (Albrecht et al., 2022) and Psychlab (Leibo et al., 2018), SHELM successfully assesses the semantics of visual observations. In turn, it requires approximately 100 times fewer interaction steps than prior methods on Psychlab's continuous recognition task that explicitly evaluates for memory capacity. On Avalon, we find that the addition of our semantic memory performs on-par with the current state-of-the-art for a limited interaction budget, while adding interpretability to the memory.

## 2 Methods

Our goal is to express visual observations in language space such that the resulting representations become comprehensible for humans. To this end, we instantiate a mapping from images to text in the form of pretrained components which are entirely frozen during training. This way the available computational resources are invested into performance rather than interpretability. Before we describe SHELM, we briefly review the HELM framework, which serves as a starting point for our work.

### Background

HELM (Paischer et al., 2022) was proposed as a framework for RL in partially observable environments. It utilizes a pretrained language model (LM) as memory mechanism that compresses past

Figure 1: We add a semantic and human-readable memory to an agent to tackle partially observable RL tasks. We map visual observations \(_{t}\) to the language domain via CLIP retrieval. The memory component, a pretrained language encoder, operates on text only and compresses a history of tokens into a vector \(_{t}\). The agent takes an action \(_{t}\) based on the current observation \(_{t}\) and the compressed history \(_{t}\).

observations. To map environment observations \(_{t}^{n}\) to the LM, it introduces the FrozenHopfield (FH) mechanism, which performs a randomized attention over pretrained token embeddings \(=(_{1},,_{k})^{}^{k m}\) of the LM, where \(k\) is the vocabulary size and \(m\) is the embedding dimension. Let \(^{m n}\) be a random matrix with entries sampled independently from \((0,n/m)\). The FH mechanism performs

\[_{t}=^{}(_{t}),\] (1)

where \(\) is a scaling factor that controls the dispersion of \(_{t}\) within the convex hull of the token embeddings. This corresponds to a spatial compression of observations to a mixture of tokens in the LM embedding space. Since \(\) is random, the association of observations \(_{t}\) with token embeddings \(_{i}\) is arbitrary, i.e., not meaningful. That is, the FH mechanism does not preserve semantics. For temporal compression, HELM leverages a pretrained LM. At time \(t\), HELM obtains a compressed history representation by

\[_{t}=(_{t-1},_{t})\] (2)

where \(_{t}\) represents the context cached in the memory register of the LM up to timestep \(t\).

More recent work has shown that the FH mechanism is prone to representation collapse if observations are visually similar to each other (Paischer et al., 2022, c). They propose a new method, namely HELMv2, which substitutes the random mapping with a pretrained CLIP encoder. Subsequently, they adopt a batch normalization layer (Ioffe and Szegedy, 2015) with fixed shifting and scaling parameters to transfer the image embedding to the language space. Consequently, HELMv2 computes the inputs to the LM as

\[_{t}=_{=_{E},= _{E}}(_{}(_{t})),\] (3)

where \(_{}\) denotes the CLIP vision model and \(_{E}\) and \(_{E}\) denote mean and standard deviation of the embedded vocabulary \(\). This effectively fits the statistics of the image embeddings to those of the LM embeddings. Since the embedding spaces of CLIP and the LM were trained independently they are not semantically aligned. Therefore, also HELMv2 fails to preserve semantics of observations and, consequently, the memory mechanism of HELMv2 is not human-readable.

### Semantic HELM

Semantic HELM (SHELM) inherits the high-level architecture from HELM but introduces some changes to the memory module. Similarly to HELMv2, we also use CLIP to embed environment observations. However, we replace the batch normalization layer of HELMv2 with a token-retrieval mechanism. We retrieve tokens that are similar to an observation in CLIP space and pass them to the LM in the form of text so they can be regarded as textual descriptions of environment observations.

Figure 2: Architecture of SHELM. We compile a semantic database \(\) by encoding prompt-augmented tokens from the overlapping vocabularies of CLIP and the LM **(a)**. Given an observation \(_{t}\) we retrieve the top-\(k\) embeddings present in \(\) and select their corresponding text tokens **(b)**. These tokens are passed to the LM which represents the memory module of SHELM **(c)**. \(_{t-1}\) represents the memory cache of the LM which tracks past tokens.

In a first step, we determine the overlap of the token vocabularies of CLIP and the LM. This is necessary to (i) to control for the number of tokens the LM receives per observation, and (ii) to avoid loss of information due to different tokenizers used by CLIP and the LM. Thereby, we obtain a set of tokens \(\). Since CLIP was pretrained on image-caption pairs, we augment each token \(v\) with a set of pre-defined prompts \(=\{p_{1},p_{2},\}\)1. The contents of \(\) are hyperparameters of our method and can be designed to specifically target certain aspects of the different environments. We embed a token \(v\) in the CLIP output space by computing the average embedding of its prompt augmentations. That is, we define the function

\[(v)=|}_{p}_ {}((p,v)).\] (4)

We do this for every \(v\), which results in a set \(\) of CLIP-embedded tokens

\[=\{(v)\}_{v}.\] (5)

We denote by \(^{k}\) an extension of the \(\) operator that returns the subset of the \(k\) largest elements in a set. For each observation we retrieve the \(k\) most similar tokens in terms of cosine similarity by

\[^{*}=_{}^{k}(, _{}(_{t})),\] (6)

where

\[(,)=^{}}{\|\|\|\|}.\] (7)

Note that \(k=|^{*}|\) is another hyperparameter of our method, namely the number of tokens that represent an observation. Effectively, \(k\) controls the degree of compression in the memory.

Finally, we embed single tokens \(\) corresponding to the set of tokens in \(^{*}\) in the LM embedding space and pass them to the LM. In this manner, we provide the LM with a textual representation of the current observation. While HELM and HELMv2 also leverage the latent structure of a pre-trained LM, they do not explicitly represent the observations in the form of text, thus, do not provide a human-readable form of past observations. Another improvement over HELMv2 is that SHELM removes the restriction of HELMv2 that the embedding spaces of CLIP and LM must have the same dimensionality. In turn, any CLIP-like encoder can be used as semantic extractor for SHELM. Figure 2 shows an illustration of the methodology of SHELM.

## 3 Experiments

First, we investigate in Section 3.1 whether CLIP can extract semantics of artificial scenes. Next, we train SHELM on four different environments, namely MiniGrid (Chevalier-Boisvert et al., 2018), MiniWorld (Chevalier-Boisvert, 2018), Avalon (Albrecht et al., 2022), and Psychlab (Leibo et al., 2018). We compare SHELM to HELMv2, LSTM (a recurrent baseline based on the LSTM architecture), and the popular Dreamerv2 (Hafner et al., 2021) and Dreamerv3 (Hafner et al., 2023). We show that a semantic memory boosts performance in environments that are both heavily dependent on memory and photorealistic. Finally, in Section 3.6 we perform ablation studies on the benefit of semantics and the trade-off between interpretability and performance.

We train all HELM variants and LSTM with Proximal Policy Optimization (PPO,Schulman et al., 2017) on RGB observations. Following Paischer et al. (2022), we instantiate the LM with a pretrained TransformerXL (TrXL, Dai et al., 2019) model. For training Dreamerv2 and Dreamerv3, we use the respective codebases23 and train on RGB observations. We report results via IQM (Agarwal et al., 2021) and 95% bootstrapped confidence intervals (CIs) unless mentioned otherwise. We follow (Colas et al., 2019) and perform a Welch's t-test with a reduced significance level of \(=0.025\) at the end of training to test for statistical significance. We elaborate on the architectural design and hyperparameter sweeps in Appendix F.

### Extracting semantics of virtual scenes

First, we analyse whether CLIP vision encoders are able to extract semantics from artificial scenes that are typically encountered in RL environments. We compare the two smallest ResNet (He et al., 2016) and ViT (Dosovitskiy et al., 2021) architectures, namely RN50 and ViT-B/16, since those add the least amount of computational overhead to SHELM. In this regard, we look at observations sampled via a random policy, provide them to the CLIP vision encoder and retrieve the closest tokens in the CLIP embedding space as explained in Section 2.2. We find that the retrieval of tokens strongly varies between vision encoder architectures for MiniWorld and Avalon (see Figure 9 and Figure 8 in the appendix). The differences are especially prominent for MiniWorld environments, where the ViT-B/16 encoder recognizes shapes and colors, whereas RN50 ranks entirely unrelated concepts highest. More photorealistic observations from the Avalon benchmark show a similar, but less pronounced trend. We also observe a strong bias toward abstract tokens such as _screenshot_, _biome_, or _render_. We alleviate the retrieval of these tokens by including them in our prompts for retrieval. Therefore, instead of using the prompt "An image of a <tok>", we consider prompts such as "A screenshot of a <tok>", or "A bione containing a <tok>", where <tok> stands for a token in \(\). We only use prompts for Avalon and Psychlab, since the effect of this prompting scheme was negligible on the other environments. Table 1 in the appendix features the full list of prompts for the two environments. We also add an analysis on using off-the-shelf captioning engines, such as BLIP-2 (Li et al., 2023) in Appendix B. We find that, while BLIP-2 correctly recognizes shapes, colors, and objects, it lacks accuracy in their compositionality. Based on this analysis we use the ViT-B/16 encoder in combination with our designed prompts as semantics extractor for SHELM.

### MiniGrid

We compare all methods on a set of six partially observable grid-world environments as in as (Paischer et al., 2022). Additionally, we train on the MiniGrid-MemoryS11-v0 environment, which we refer to as Memory. The Memory task requires the agent to match the object in the starting room to one of the two objects at the end of a corridor. The agent's view is restricted so it cannot observe both objects at the same time. Therefore, it needs to memorize the object in the starting room. If the agent chooses the wrong object at the end of the corridor, it receives no reward and the episode ends.

Figure 3 (left and middle, respectively) shows the results across the six MiniGrid environments and the Memory environment. On the MiniGrid environments, Dreamerv3 excels and converges much faster than any other method. However, final performance is approximately equal for Dreamerv3, HELMv2, and SHELM. Interestingly, although all MiniGrid environments are partially observable, they are solvable with a memory-less policy. In the Memory environment, SHELM performs on par with Dreamerv3 but exhibits faster convergence and more stable training. Figure 11 visualizes the tokens passed to the memory of SHELM after sampling episodes from a trained policy. SHELM primarily maps the ball to the token _miner_, and the key to the token _narrow_. Although the retrieved tokens do not represent the semantically correct objects in the observations, they are still stored as two different language tokens in the memory. This enables SHELM to discriminate between them and enables faster learning as mirrored in the learning curves. The LSTM baseline suffers from poor

Figure 3: **Left: Accumulated reward for different methods on six MiniGrid environments, Middle: on the MiniGrid-Memory task, Right: on eight MiniWorld tasks. We report IQM and 95% CIs across 30 seeds for each method.**sample efficiency and does not learn to utilize its memory within the budget of 2 M interaction steps. LSTM requires approximately 5 M steps to solve the task.

### MiniWorld

The MiniWorld benchmark suite provides a set of minimalistic 3D environments. In line with Paischer et al. (2022), we select eight different tasks from MiniWorld and train our method and all baselines on those. The tasks comprise collecting and placing objects, finding objects, or navigating through mazes. A detailed description of each task can be found in Appendix A. Prior work on the MiniWorld benchmark mostly used single environments to evaluate specialized approaches (Liu et al., 2021; Venuto et al., 2019; Zha et al., 2021; Khetarpal et al., 2020), or handcraft novel environments (Sahni et al., 2019; Hutsebaut-Buysse et al., 2020). To the best of our knowledge, HELMv2 is the only method that was evaluated on all eight tasks and has been state of the art so far (Paischer et al., 2022).

Figure 3 (right) shows the results for all methods. Interestingly, SHELM reaches performance on par with HELMv2, even though semantics can be extracted from the 3D observations to a certain degree (as explained in Section 3.1). Further, Dreamerv2 outperforms both and reaches state-of-the-art performance. Surprisingly, Dreamerv3 attains a significantly lower score and performs on par with HELM and the memory-less PPO. This might be due to suboptimal hyperparameters, even though Hafner et al. (2023) claim that the choice of hyperparameters transfers well across environments. LSTM again suffers from poor sample efficiency reaching the lowest scores out of all compared methods. Finally, we again observe that PPO can in principle solve all tasks, which yields further evidence that partial observability does not automatically imply necessity for memory. Our results suggest that a memory mechanism can result in enhanced sample efficiency (SHELM vs PPO). However, memory is not imperative to solve these tasks.

### Avalon

Avalon is an open-ended 3D survival environment consisting of 16 different tasks. The aim for the agent is to survive as long as possible by defending against predators, hunting animals, and eating food in order to restore energy. An episode ends if the agent has no energy left. This can happen if the agent receives environmental damage (e.g. from falling), is being killed by a predator, or does not eat frequently. The agent receives a dense reward as the difference in energy between consecutive timesteps. Additionally, it receives a delayed reward upon successful completion of a task, e.g., eating food. The observation space are RGBD images as well as proprioceptive input that comprise, e.g., the agent's energy. We adopt the same architecture and training strategy as Albrecht et al. (2022) for all HELM variants. Specifically, we add the history compression branch to the baseline PPO agent and train on all 16 tasks including their difficulty curriculum. The history compression branch is only based on RGB images and does not receive the proprioceptive state. We also train Dreamerv2, and a memory-less baseline (PPO), which is identical to the PPO baseline in Albrecht et al. (2022). Due to computational constraints and since we observed superior performance of Dreamerv2 on 3D environments, we neglect Dreamerv3 and LSTM and train on a limited budget of 10 M interaction steps. We elaborate on training details and hyperparameter selection in Appendix F. The final performance of an agent is measured in terms of mean human normalized scores on a curated set of 1000 test worlds.

Figure 4: **Left: Mean and standard deviation of human normalized score across all tasks on the Avalon test worlds after 10 M timesteps. Right: Two sample episodes and their corresponding token mappings for episodes sampled from Avalon with a random policy.**

The results are shown in Figure 4, left. For detailed results per task see Table 2 in the appendix. SHELM and Dreamerv2 yield the highest performance on average after 10 M interaction steps. However, the difference to the memory-less PPO is not statistically significant. To further investigate this finding we train PPO for 50 M interaction steps and compare it to the baseline results reported in Albrecht et al. (2022) in Appendix D. Indeed, we find that our memory-less baseline attains scores on-par with memory-based approaches trained for 50 M interaction steps. This yields further evidence that Avalon does not necessitate the addition of a memory mechanism. Finally, we can glance at SHELM's memory to identify failure cases. We show observations of two episodes and their token correspondences for SHELM in Figure 4, right. The observations are mostly mapped to semantically meaningful tokens that represent parts in the image observations. However, we also observe some cases where CLIP retrieves tokens that are semantically related, but not correct, which we show in Figure 12 in the appendix.

### PsychLab

The Psychlab environment suite (Leibo et al., 2018) consists of 8 complex tasks and was designed to isolate various cognitive faculties of RL agents including vision and memory. The continuous recognition (CR) task of Psychlab was specifically designed to target the memory capacity of an agent. In this task the agent is placed in front of a monitor which displays a stream of objects. The agent then needs to identify whether it has already seen a particular object by swiping either left or right. If the agent correctly identifies an object it receives a reward of +1. A policy that always swipes in one direction achieves a reward of around 30, which we refer to as the random baseline. Every reward higher than that indicates that an agent actually utilizes its memory. This task is ideal to evaluate the memory capacity of an agent, since episodes last for about 2000 timesteps and usually require remembering up to 50 different objects.

We train all methods for 10 M interaction steps on the CR task (Figure 5). We neglect the memoryless baseline because this task is unsolvable without memory. SHELM indeed learns to effectively utilize its memory within 10 M interaction steps and significantly outperforms all competitors. Other approaches require interaction steps in the range of billions until convergence (Parisotto et al., 2020; Fortunato et al., 2019). These works report human normalized scores which are not publicly available, therefore we cannot compare SHELM to those. HELMv2 is the only competitor that attains a performance better than random, but reaches significantly lower scores than SHELM. Surprisingly, both variants of Dreamer do not exceed performance of the random baseline. Finally, we inspect the memory of SHELM and show the token mappings that are passed on to the memory module for some sample observations in Figure 5. In most cases SHELM assigns semantically correct tokens to the displayed objects. However, we also show some cases where the token retrieval of SHELM conflates different objects in Figure 13. We find that this can mostly be attributed to the low resolution of observations. SHELM can recover from these failure cases when using higher resolutions (see Figure 14).

Figure 5: Performance for all methods on CR task from Psychlab. We report IQM and 95% CIs across 5 seeds (**left**). Observation and corresponding tokens for SHELM on CR environment (**right**).

### Ablation studies

Are semantics important?We slightly alter the HELMv2 implementation from (Paischer et al., 2022, 2022) by retrieving the closest tokens in the language space after their frozen batch normalization layer, and finally passing those to the LM. This setting is similar to SHELM in that the LM receives tokens in the form of text. However, semantics are not preserved since visual features are merely shifted to the language space. We call this setting HELMv2-Ret and train it on the Memory environment (see Figure 6). We find that if the mapping from observation to language is arbitrary, the performance decreases drastically.

Is it important to learn task-specific features?SHELM would be even more interpretable if not only the memory module but also the branch processing the current observation (CNN in Figure 2) could utilize language tokens. Therefore, we substitute the CNN with a CLIP vision encoder (SHELM-CLIP). An important consequence of this methodological change is that even features from the current observation can be interpreted by our retrieval mechanism. However, Figure 6 suggests that it is vital to learn task-specific features.

Should the history branch operate on task-specific features?To answer this question we substitute the CLIP encoder in the history branch with the CNN encoder learned at the current timestep (SHELM-CNN). An immediate drawback of that is the loss of readability, since we do not represent the observations in text form anymore. Further, we again observe a drastic drop in performance. The reason for this is that the task-specific features must be learned before they actually provide any benefit to the history branch. Thus, we prefer to keep abstract semantic features in text form since that does not require any training and has the additional benefit of being human-readable.

Is a pretrained language encoder required?We replace the LM with an LSTM operating on the text tokens (SHELM-LSTM). This setting resulted in performance equivalent to randomly choosing an object at the end of the corridor. However, we believe that after longer training SHELM-LSTM can eventually learn to solve the task, since the LSTM baseline also solved the task after longer training. Thus, the simpler and more sample efficient method is to maintain the frozen pretrained encoder instead of learning a compressed history representation.

We show additional results for an ablation study where we exchange the ViT-B/16 vision encoder with a RN50 in Appendix E. SHELM appears to be quite sensitive to the selection of vision encoder, which corroborates our qualitative findings in Section 3.1.

## 4 Related work

RL and partial observabilityRL with incomplete state information can necessitate a memory for storing the history of observations an agent encountered. However, storing the entire history is often infeasible. History Compression tries to answer the question of what information to store in a stream of observations (Schmidhuber, 1992; Schmidhuber et al., 1993; Zenke et al., 2017; Kirkpatrick et al., 2016; Schwarz et al., 2018; Ruvolo & Eaton, 2013). A plethora of prior works have used history compression to tackle credit assignment (Arjona-Medina et al., 2019; Patil et al., 2022; Widrich et al., 2021; Holzleitner et al., 2021), and partial observability (Hausknecht & Stone, 2015; Vinyals et al., 2019; Berner et al., 2019; Pleines et al., 2022). The memory maintained by an agent can either

Figure 6: Ablation study on the effect of semantics, the influence of task-specific features in the history and the current timestep, and the importance of the pretrained LM. We report IQM and 95% CIs across 30 seeds on the MiniGrid-Memory task.

be external (Hill et al., 2021; Wayne et al., 2018), or directly integrated into the feature extraction pipeline via the network architecture. An orthogonal approach for history compression is training recurrent dynamics models (Ha and Schmidhuber, 2018; Pasukonis et al., 2022; Hafner et al., 2020, 2021). We believe language is very well suited as medium for compression to summarize past events, as suggested by Paischer et al. (2022).

Language in RLLanguage provides useful abstractions for RL. These abstractions can be leveraged to represent high-level skills (Sharma et al., 2022; Jiang et al., 2019; Jacob et al., 2021). LMs have been used to improve exploration in text-based environments (Yao et al., 2020), or in visual environments via a language oracle (Mu et al., 2022). Pretrained vision-language models (VLMs) provide abstract embedding spaces that can be used for exploration in visual environments (Tam et al., 2022). We leverage VLMs to spatially compress visual observations to language tokens. Furthermore, pretrained LMs were leveraged for (i) initializing policies in text-based environments (Li et al., 2022), (ii) grounding to various environments (Andreas et al., 2018; Huang et al., 2023; Carta et al., 2023; Hill et al., 2021), (iii) sequence modeling in the offline RL setup (Reid et al., 2022), and (iv) generating high-level plans (Huang et al., 2022, 2022; Wang et al., 2023; Singh et al., 2022; Ichter et al., 2022; Liang et al., 2022; Dasgupta et al., 2023; Du et al., 2023; Shah et al., 2022; Ahn et al., 2022; Zeng et al., 2022). Other works train agents to generate descriptions of virtual scenes (Yan et al., 2022), or thought processes of humans (Hu and Clune, 2023). Additionally, language has been used for augmenting the reward function (Wang et al., 2019; Bahdanau et al., 2019; Goyal et al., 2019; Carta et al., 2022; Kwon et al., 2023), or learning a dynamics model (Zhong et al., 2022, 2021, 2020; Wu et al., 2023). To manage a language-based memory, Park et al. (2023) stores a record of an agent's memory which comprises different levels of abstraction. Importantly, all agent-environment interactions in their work are scripted, while our agent enables a memory based on language for visual inputs.

Language for interpretabilityIn the realm of supervised learning, a plethora of prior works had used language as human-interpretable medium to explain classification decisions in computer vision (Hendricks et al., 2016, 2018; Park et al., 2018; Zellers et al., 2019; Hernandez et al., 2022), or in natural language processing (Andreas and Klein, 2017; Zaidan and Eisner, 2008; Camburu et al., 2018; Rajani et al., 2019; Narang et al., 2020). Interpretability methods in RL are scarce and usually follow a post-hoc approach (Puiutta and Veith, 2020). Intrinsically interpretable models are designed to be inherently interpretable even during training time and are preferable over post-hoc approaches (Rudin et al., 2021). They often restrict the complexity of the model class, which in turn results in reduced performance of the agent. Therefore, (Glanois et al., 2021) propose to adopt a modular approach to interpretability. To this end, our work focuses on the memory module. This enables us to provide some extent of intrinsic interpretability while exceeding performance of existing (non-interpretable) methods on tasks that necessitate memory.

Foundation modelsThe advent of the Transformer architecture (Vaswani et al., 2017) gave rise to foundation models (FMs, Bommasani et al., 2021), such as GPT-3 (Brown et al., 2020). As shown by Petroni et al. (2019); Talmor et al. (2020); Kassner et al. (2020); Mahowald et al. (2023), pretrained LMs can learn abstract symbolic rules and show sparks of reasoning. We leverage their abstraction capabilities for history compression in RL. Further, vision FMs have been demonstrated to be well adaptable to foreign domains (Adler et al., 2020; Evci et al., 2022; Ostapenko et al., 2022; Parisi et al., 2022). Our approach combines language-based FMs with vision-language models, such as CLIP (Radford et al., 2021) or ALIGN (Jia et al., 2021). We use CLIP to obtain language tokens that semantically correspond to concepts present in synthetic environments.

## 5 Limitations

Token-level abstractionOne potential shortcoming of our method is that our retrieval is based on single tokens. However, we have shown that for environments where one or a few objects are important, this is sufficient. Further, our approach is very flexible and the semantic database can easily be augmented with more detailed descriptions of objects and their relationships. We aim to investigate more in this direction in future work.

Wall clock timeA current limitation of SHELM is wall clock time. The rollout phase is particularly expensive since each timestep needs to be propagated through the LM. Despite that, it is still more efficient than, e.g., Dreamerv2. This is due to the fact that the memory mechanism is kept frozen and need not be updated durint the training phase. A potential solution for decreasing the complexity of the rollout phase would be distillation of the LM into smaller actor networks as in (Parisotto and Salakhutdinov, 2021).

Modality gapOur semantic mapping is limited by the inherent ability of CLIP vision encoders to extract semantically meaningful features in synthetic environments. However, CLIP suffers from the modality gap (Liang et al., 2022), i.e., a mis-alignment between image and text embedding spaces. In the future, we aim at incorporating methods that mitigate the modality gap (Furst et al., 2022; Ouali et al., 2023).

Distribution shiftWe use a pretrained CLIP model to retrieve tokens that are similar to visual observations. However, CLIP was pretrained on large-scale data crawled from the web. We have shown that it can still extract semantics of synthetic environments when those are sufficiently photorealistic, i.e., MiniWorld, Avalon, or Psychlab. Further, prompting can enhance the quality of the retrieval. For environments such as MiniGrid, the retrieval yields tokens that do not correspond to aspects in the image. However, Gupta et al. (2022) has shown that CLIP can handle task-specific texts for MiniGrid, thus, we would like to investigate the effect of augmenting our semantic database with such texts in the future.

## 6 Conclusion

In many real-world scenarios an agent requires a memory mechanism to deal with partial observability. Current memory mechanisms in RL act as a black box where it is not comprehensible for humans what pieces of information were stored. To solve this problem, we proposed a new method called Semantic HELM that represents past events in form of human-readable language by leveraging pretrained vision-language models. We showed compelling evidence that even for synthetic environments our memory mechanism can extract semantics from visual observations. Further, SHELM outperforms strong baselines on photorealistic memory-dependent environments through its human-readable memory module. In cases where SHELM fails to extract semantics from observations, we can investigate the cause by inspection of the memory module. Even in such cases, SHELM mostly performs on-par with other memory-based approaches.

We believe that we can further enhance our method by generating full captions from history observations instead of only a small number of words. This could enable (i) a long-term memory that textually summarizes all captions of corresponding observations, (ii) a potential for planning in form of text from visual observations similar to Patel et al. (2023), and (iii) modeling dynamics of an environment in language space.