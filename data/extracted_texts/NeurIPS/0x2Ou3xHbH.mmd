# On the Convergence of No-Regret Learning Dynamics in Time-Varying Games

Ioannis Anagnostides

Carnegie Mellon University

ianagnos@cs.cmu.edu

Ioannis Panageas

University of California Irvine

ipanagea@ics.uci.edu

Gabriele Farina

MIT

gfarina@mit.edu

Tuomas Sandholm

Carnegie Mellon University

Strategic Machine, Inc.

Strategy Robot, Inc.

Optimized Markets, Inc.

sandholm@cs.cmu.edu

###### Abstract

Most of the literature on learning in games has focused on the restrictive setting where the underlying repeated game does not change over time. Much less is known about the convergence of no-regret learning algorithms in dynamic multiagent settings. In this paper, we characterize the convergence of _optimistic gradient descent (OGD)_ in time-varying games. Our framework yields sharp convergence bounds for the equilibrium gap of OGD in zero-sum games parameterized on natural variation measures of the sequence of games, subsuming known results for static games. Furthermore, we establish improved _second-order_ variation bounds under strong convexity-concavity, as long as each game is repeated multiple times. Our results also extend to time-varying _general-sum_ multi-player games via a bilinear formulation of correlated equilibria, which has novel implications for meta-learning and for obtaining refined variation-dependent regret bounds, addressing questions left open in prior papers. Finally, we leverage our framework to also provide new insights on dynamic regret guarantees in static games.

## 1 Introduction

Most of the classical results in the literature on learning in games--exemplified by, among others, the work of Hart and Mas-Colell , Foster and Vohra , Freund and Schapire --rest on the assumption that the underlying repeated game remains invariant. Yet, in many paradigmatic learning environments that is unrealistic . One such class is settings where the underlying game is actually changing, such as routing problems on the internet , online advertising auctions , and dynamic mechanism design . Another such class consists of settings in which many similar games need to be solved . For example, one may want to solve variations of a game for the purpose of sensitivity analysis with respect to the modeling assumptions used to construct the game model.

Despite the considerable interest in such dynamic multiagent environments, much less is known about the convergence of _no-regret learning_ algorithms in time-varying games. No-regret dynamics are natural learning algorithms that have desirable convergence properties in static settings. Also, the state-of-the-art algorithms for finding minimax equilibria in two-player zero-sum games are based on advanced forms of no-regret dynamics . Indeed, all the superhuman milestones in poker have used them in the equilibrium-finding module of their architectures .

In this paper, we seek to fill this knowledge gap by understanding properties of no-regret dynamics in time-varying games. In particular, we primarily investigate the convergence of _optimistic gradient descent (OGD)_ in time-varying games. Unlike traditional no-regret learning algorithms, such as (online) gradient descent, OGD has been recently shown to exhibit _last-iterate_ convergence in _static_ (two-player) zero-sum games . For the more challenging scenario where the underlying game can vary in every round, a fundamental question arises: _Under what conditions on the sequence of games does OGD approximate (with high probability) the sequence of Nash equilibria?_

### Our results

In this paper, we develop a framework that enables us to characterize the convergence of OGD, and generalizations thereof, in a number of fundamental time-varying multiagent settings.

Bilinear saddle-point problemsFirst, building on the work of Zhang et al. , we identify natural variation measures on the sequence of games whose sublinear growth guarantees that _almost all_ iterates of OGD under a constant learning rate are approximate Nash equilibria in time-varying (two-player) zero-sum games (Corollary 3.4). More precisely, in Theorem 3.3 we derive a sharp non-asymptotic characterization of the equilibrium gap of OGD as a function of the _minimal first-order_ variation of _approximate_ Nash equilibria and the _second-order_ variation of the payoff matrices. We stress that our variation measure that depends on the deviation of approximate Nash equilibria of the games can be arbitrarily smaller than the one based on (even the least varying) sequence of exact Nash equilibria (Proposition 3.2), thereby significantly sharpening the measure considered by Zhang et al. . It is also a compelling property, in light of the multiplicity of Nash equilibria, that the variation of the Nash equilibria is measured in terms of the most favorable--_i.e._, one that minimizes the variation--such sequence.

From a technical standpoint, our analysis revolves around a connection between the convergence of OGD in time-varying games and _dynamic regret_. In particular, the first key observation is that _dynamic regret cannot be too negative under any sequence of approximate Nash equilibria_ (Property 3.1); this was first observed by Zhang et al.  under a sequence of _exact_ Nash equilibria. We also generalize this property by connecting it to the admission of a _minimax theorem_ (Property A.4), as well as the so-called _MVI property_ in the more general context of time-varying variational inequalities (VIs); this will enable us to extend our scope to certain multi-player games such as _polymatrix zero-sum_ (Theorem A.16). By combining Property 3.1 with a dynamic _RVU bound_, we obtain a variation-dependent bound for the second-order path length of OGD under a constant learning rate in time-varying zero-sum games. In turn, this leads to our first main result, Theorem 3.3, discussed above. In the special case of static games, our theorem reduces to a tight \(T^{-1/2}\) rate. We also instantiate our results to the special setting of _meta-learning_, where each game is repeated multiple times.

Strongly convex-concave gamesMoreover, for strongly convex-concave time-varying games, we obtain a refined _second-order_ variation bound on the sequence of Nash equilibria, as long as each game is repeated multiple times (Theorem 3.5); this is inspired by an improved second-order bound for dynamic regret under analogous conditions due to Zhang et al. . As a byproduct of our techniques, we point out that _any_ no-regret learners are approaching a Nash equilibrium under strong convexity-concavity (Proposition 3.6; _cf._). Those results apply even in non-strongly convex-concave settings by suitably trading off the magnitude of a regularizer that makes the game strongly convex-concave. This offers significant gains in the meta-learning setting as well.

General-sum multi-player gamesNext, we extend our results to time-varying general-sum multi-player games by expressing _correlated equilibria_ via a bilinear saddle-point problem (BSPP); while this formulation goes back to the early work of Hart and Schmeidler , it is rarely employed in the literature on learning in games. By leveraging a structural property of the underlying BSPP, we manage to obtain convergence bounds parameterized on the variation of the correlated equilibria (Theorem 3.9). To illustrate the power of our framework, we immediately recover natural and algorithm-independent similarity measures for the meta-learning setting (Proposition A.14) even in general games (Corollary A.25), thereby addressing an open question of Harris et al. . Our techniques also imply new per-player regret bounds in zero-sum and general-sum games (Corollaries A.12 and A.26), the latter addressing a question left open by Zhang et al. , albeit under a different learning paradigm (Section 3.3 contains a discussion on this point). We further parameterize the convergence of (vanilla) gradient descent in time-varying _potential_ games in terms of the deviation of the potential functions (Theorem 3.7).

Finally, building on our techniques in time-varying games, we investigate the best dynamic-regret guarantees possible in _static_ games (see also the work of Cai and Zheng ). We first note that instances of optimistic mirror descent guarantee \(O()\) dynamic per-player regret (Proposition 3.10), matching the known rate of (online) gradient descent but for the significantly weaker notion of _external_ regret. We further point out that \(O( T)\) dynamic regret is attainable, but in a stronger two-point feedback model. In stark contrast, even obtaining sublinear dynamic regret for each player is precluded in general-sum games (Proposition 3.12). This motivates studying a relaxation of dynamic regret that constrains the number of switches in the comparator, for which we derive accelerates rates in general games (Theorem 3.13) by leveraging the techniques of Syrgkanis et al.  in conjunction with the dynamic RVU bound. Interesting, this relaxation of dynamic regret gives rise to a natural refinement of coarse correlated equilibria, recently investigated by Crippa et al. .

### Further related work

Even in static (two-player) zero-sum games, the pointwise convergence of no-regret learning algorithms is a tenuous affair. Indeed, traditional learning dynamics within the no-regret framework, such as (online) mirror descent, may even diverge away from the equilibrium; _e.g._, see . Notwithstanding the lack of pointwise convergence, the _empirical frequency_ of no-regret learners is well-known to approach the set of Nash equilibria in zero-sum games , and the set of coarse correlated equilibria in general-sum games --a standard relaxation of the Nash equilibrium . Unfortunately, those classical results are of little use beyond static games, thereby offering a crucial impetus for investigating iterate-convergence in games with a time-varying component--a ubiquitous theme in many practical scenarios of interest . One concrete and quite broad example worth mentioning here revolves around the _steering_ problem , where a mediator dynamically modifies the utilities of the game via nonnegative and vanishing payments so as to guide no-regret learners to desirable equilibria. In particular, when the payment function varies over time, the steering problem can be naturally phrased as a time-varying zero-sum game .

In this context, there has been a considerable effort endeavoring to extend the scope of traditional game-theoretic results to the time-varying setting, approached from a variety of different standpoints in prior work . In particular, our techniques in Section 3.1 are strongly related to the ones developed by Zhang et al. , but our primary focus is different: Zhang et al.  were mainly interested in obtaining variation-dependent regret bounds, while our results revolve around iterate-convergence of OGD (under a constant learning rate) to Nash equilibria. Minimizing regret and approaching Nash equilibria are two inherently distinct problems, although connections have emerged , and are further cultivated in this paper. Moreover, in an independent work Feng et al.  established a surprising separation between the behavior of optimistic GDA and the extra-gradient method in time-varying unconstrained bilinear games; it is open whether such a discrepancy persists in the constrained setting as well. We also point out the concurrent work of Yan et al.  that focuses on improved guarantees for strongly monotone games; the results we obtain in Section 3.2 are complementary to theirs.

Another closely related direction is on _meta-learning_ in games , where each game can be repeated for multiple iterations. Such considerations are motivated in part by a number of use-cases in which many "similar" games--or multiple game variations--ought to be solved , such as Poker with different stack-sizes. While the meta-learning problem is a special case of our general setting, our results are strong enough to have new implications for meta-learning in games, even though the algorithms considered herein are not tailored to operate in that setting.

## 2 Preliminaries

NotationWe let \(\{1,2,,\}\) be the set of natural numbers. For a number \(p\), we let \([\![p]\!]\{1,,p\}\). For a vector \(^{d}\), we use \(\|\|_{2}\) to represent its Euclidean norm; we also overload that notation so that \(\|\|_{2}\) denotes the spectral norm when the argument is a matrix.

For a two-player zero-sum game, we denote by \(^{d_{x}}\) and \(^{d_{y}}\) the strategy sets of the two players--namely, Player \(x\) and Player \(y\), respectively--where \(d_{x},d_{y}\) represent the corresponding dimensions. It is assumed that \(\) and \(\) are nonempty convex and compact sets. For example, in the special case where \(^{d_{x}}\) and \(^{d_{y}}\)--each set corresponds to a probability simplex--the game is said to be in _normal form_. Further, we denote by \(D_{}\) the \(_{2}\)-diameter of \(\), and by \(\|\|_{2}\) the maximum \(_{2}\)-norm attained by a point in \(\). We will always assume that the strategy sets remain invariant, while the payoff matrix can change in each round. For notational convenience, we will denote by \((,)\) the concatenation of \(\) and \(\), and by \(\) the Cartesian product of \(\) and \(\). In general \(n-\)player games, we instead use subscripts indexed by \(i n\) to specify quantities related to a player. Superscripts are typically reserved to identify the time index. Finally, to simplify the exposition, we often use the \(O()\) notation to suppress time-independent parameters of the problem.

Dynamic regretWe operate in the usual online learning setting under full feedback. Namely, at every time \(t\) the learner decides on a strategy \(^{(t)}\), and then subsequently observes a utility function \(,^{(t)}_{}\), for \(^{(t)}_{x}^{d_{x}}\). A strong performance benchmark in this online setting is _dynamic regret_, defined for a time horizon \(T\) as follows:

\[^{(T)}_{x}(s^{(T)}_{x})_{t=1}^{T}^{(t, )}-^{(t)},^{(t)}_{x}, \]

where \(s^{(T)}_{x}(^{(1,)},,^{(T,)})^{T}\) above is the sequence of _comparators_; by setting \(^{(1,)}=^{(2,)}==^{(T,)}\) in (1) we recover the standard notion of _(external) regret_ (denoted simply by \(^{(T)}_{x}\)), which is commonly used to establish convergence of the time-average strategies in static two-player zero-sum games . On the other hand, the more general notion of dynamic regret, introduced in (1), has been extensively used in more dynamic environments; _e.g._, [94; 92; 53; 18; 51; 54; 62]. We also define \(^{(T)}_{x}_{s^{(T)}_{x}^{T}}^{(T)}_{x}(s^{(T)}_{x})\). While ensuring \(o(T)\) dynamic regret is clearly hopeless in a truly adversarial environment, Section 3.4 reveals that non-trivial guarantees are possible when learning in zero-sum games (see also ).

Optimistic gradient descent_Optimistic gradient descent (OGD)_[20; 72] is a no-regret algorithm defined with the following update rule:

\[^{(t)} _{}(}^{(t)}+^{(t )}_{x}),\] (OGD) \[}^{(t+1)} _{}(}^{(t)}+^{(t )}_{x}).\]

Here, \(>0\) is the _learning rate_; \(}^{(1)}*{arg\,min}_{}\| }\|_{2}^{2}\) represents the initialization of OGD; \(^{(t)}_{x}^{d_{x}}\) is the _prediction vector_ at time \(t\), and it is set as \(^{(t)}_{x}^{(t-1)}_{x}\) when \(t 2\), and \(^{(1)}_{x}_{d_{x}}\); and finally, \(_{}()\) represents the Euclidean projection to the set \(\), which is well-defined, and can be further computed efficiently for structured sets, such as the probability simplex. For our purposes, we will posit access to a projection oracle for the set \(\), in which case the update rule (OGD) is indeed efficiently implementable.

In a multi-player \(n\)-player game, each player \(i n\) is associated with a utility function \(u_{i}:_{i=1}^{n}_{i}\). We recall the following central definition .

**Definition 2.1** (Approximate Nash equilibrium).: A joint strategy profile \((^{}_{1},,^{}_{n})_{i=1}^{n}_{i}\) is an \(\)-approximate Nash equilibrium (NE), for an \( 0\), if for any player \(i n\) and any possible deviation \(^{}_{i}_{i}\) it holds that \(u_{i}(^{}_{1},,^{}_{i},,^{}_{n} ) u_{i}(^{}_{1},,^{}_{i},,^{ }_{n})-\).

## 3 Convergence in time-varying games

In this section, we formalize our results regarding convergence in time-varying games. We organize this section as follows: First, in Section 3.1, we study the convergence of OGD in time-varying bilinear saddle-point problems (BSPPs) and beyond, culminating in the non-asymptotic characterization of Theorem 3.3; Section 3.2 formalizes our improvements under strong convexity-concavity; we then extend our scope to time-varying multi-player general-sum and potential games in Section 3.3; and finally, Section 3.4 is concerned with dynamic regret guarantees in static games.

### Bilinear saddle-point problems

We first study an online learning setting wherein two players interact in a sequence of time-varying BSPPs . We assume that in every repetition \(t T\) the players select a pair of strategies \((^{(t)},^{(t)})\). Then, Player \(x\) receives the utility \(_{x}^{(t)}-^{(t)}^{(t)}^{d_{x}}\), where \(^{(t)}^{d_{x} d_{y}}\) represents the payoff matrix at the \(t\)-th repetition; similarly, Player \(y\) receives the utility \(_{y}^{(t)}(^{(t)})^{}^{(t)}^{d _{y}}\). We also extend our scope to a more general class of time-varying problems, as we formalize in Section 3.1.1. The proofs of this subsection are in Appendix A.1.

We commence by pointing out a crucial property: by selecting a sequence of approximate Nash equilibria as the comparators, the _sum_ of the players' dynamic regrets _cannot be too negative_:

**Property 3.1**.: _Suppose that \(^{(t,)}=(^{(t,)},^{(t,)})\) is an \(^{(t)}\)-approximate Nash equilibrium of the \(t\)-th game. Then, for \(_{x}^{(T)}=(^{(t,)})_{1 t T}\) and \(_{y}^{(T)}=(^{(t,)})_{1 t T}\),_

\[_{x}^{(T)}(_{x}^{(T)})+_{y}^{(T)}(_{y}^{ (T)})-2_{t=1}^{T}^{(t)}.\]

A special case of this property was previously noted by Zhang et al.  by considering a sequence of _exact_ Nash equilibria; the approximate version stated above leads to a significant improvement in the convergence bounds, as we shall see in the sequel. In fact, as we note in Property A.4, Property 3.1 applies even in certain (time-varying) nonconvex-nonconcave min-max optimization problems, and it is a consequence of the _minimax theorem_. Property 3.1 also holds for time-varying variational inequalities (VIs) that satisfy the so-called _MVI property_, as we elaborate further in Section 3.1.1.

Now, building on the work of Zhang et al. , let us introduce some natural measures of the games' variation. The first-order variation of the Nash equilibria was defined by Zhang et al.  for \(T 2\) as \(_{}^{(T)}_{^{(t,)}^ {(t,)},\, t T}_{t=1}^{T-1}\|^{(t +1,)}-^{(t,)}\|_{2}\), where \(^{(t,)}\) is the (nonempty) set of Nash equilibria of the \(t\)-th game. We recall that there can be a multiplicity of Nash equilibria ; as such, a compelling feature of the above variation measure is that it depends on the most favorable sequence of Nash equilibria--one that minimizes the first-order variation.

It is important, however, to come to terms with the well-known fact that Nash equilibria can change abruptly even under a "small" perturbation in the payoff matrix (see Example A.5), which is an important caveat of the variation \(_{}^{(T)}\). To address this, and in accordance with our more general Property 3.1, we consider a more favorable variation measure, defined as

\[_{-}^{(T)}\{_{t=1}^{T-1}\| ^{(t+1,)}-^{(t,)}\|_{2}+C_{t=1}^{T}^{(t)} \}, \]

for a sufficiently large parameter \(C>0\) (see (28)); the infimum above is subject to \(^{(t)}_{ 0}\) and \(^{(t,)}_{^{(t)}}^{(t,)}\) for all \(t T\), where \(_{^{(t)}}^{(t,)}\) is the set of \(^{(t)}\)-approximate NE. It is evident that \(_{-}^{(T)}_{}^{(T)}\) since one can take \(^{(1)}==^{(T)}=0\); in fact, \(_{-}^{(T)}\) can be arbitrarily smaller:

**Proposition 3.2**.: _For any \(T 4\), there is a sequence of \(T\) games such that \(_{}^{(T)}\) while \(_{-}^{(T)}\), for any \(>0\)._

This shows that \(_{-}^{(T)}\) is a more compelling variation measure compared to \(_{}^{(T)}\). It is worth noting here that Zhang et al.  also considered the variation measure \(_{}^{(T)}_{t=1}^{T}\|^{(t)}-}\|_{2}\), where \(}\) is the average payoff matrix and \(\|\|_{2}\) denotes the spectral norm.1 It is in fact not hard to construct instances such that \(_{-}^{(T)}\{_{}^{( T)},_{}^{(T)}\}\) (Proposition A.6).

Moreover, we also recall another quantity that captures the variation of the payoff matrices: \(_{}^{(T)}_{t=1}^{T-1}\|^{(t+1)}- ^{(t)}\|_{2}^{2}\). Unlike \(_{}^{(T)}\), the variation measure \(_{}^{(T)}\) depends on the _second-order_ variation (of the payoff matrices), which could translate to a lower-order impact compared to \(_{}^{(T)}\)(see, _e.g._, Corollary A.9). We stress that while our convergence bounds will be parameterized based on \(^{(T)}_{-}\) and \(^{(T)}_{}\), the underlying algorithm--namely OGD--will remain oblivious to those variation measures. We only make the mild assumption that players know in advance the value of \(L_{1 t T}\|^{(t)}\|_{2}\), so that they can tune the learning rate \(\) appropriately.

We are now ready to state the main result of this subsection. Below, we use the notation \(^{(t)}(^{(t)})\) to represent the Nash equilibrium gap of \((^{(t)},^{(t)})=^{(t)}\) at the \(t\)-th game. More precisely, \(^{(t)}(^{(t)})\{^{(t)}_{x}(^ {(t)}),^{(t)}_{y}(^{(t)})\}\), where \(^{(t)}_{x}(^{(t)})_{}\{ ,^{(t)}_{x}\}-^{(t)},^{(t)}_{x}\) is the best response gap of Player \(x\) (and analogously for Player \(y\)).

**Theorem 3.3** (Detailed version in Theorem A.11).: _Suppose that both players employ OGD with learning rate \(=\) in a sequence of time-varying BSPPs, where \(L_{1 t T}\|^{(t)}\|_{2}\). Then, \(_{t=1}^{T}(^{(t)}(^{(t)}))^{2}=O(1+ ^{(T)}_{-}+^{(T)}_{})\), where \((^{(t)})_{1 t T}\) is the sequence of joint strategy profiles produced by OGD._

The proof of this theorem is based on upper bounding the _second-order path length_ of OGD, \(_{t=1}^{T}(\|^{(t)}-}^{(t)}\|_{2}^{2}+\|^{(t)}- }^{(t+1)}\|_{2}^{2})\), as a function of the variation measures \(^{(T)}_{-}\) and \(^{(T)}_{}\). This is shown via a _dynamic RVU_ bound  (Lemmas A.1 and A.2) in conjunction with Property 3.1.

It is worth noting that when the deviation of the payoff matrices is controlled by the deviation of the players' strategies, in the sense that \(_{t=1}^{T-1}\|^{(t+1)}-^{(t)}\|_{2}^{2} W^{2}_{ t=1}^{T-1}\|^{(t+1)}-^{(t)}\|_{2}^{2}\) for some parameter \(W_{>0}\), the variation measure \(^{(T)}_{}\) in Theorem 3.3 can be entirely eliminated; see Corollary A.9. The same in fact applies under an improved prediction mechanism (see Remark A.13); while that prediction is not implementable in the online learning setting, it can be used, for example, when the sequence of games is known in advance (as is the case in certain applications).

We next state some immediate consequences of Theorem 3.3. (Item 2 below follows from Theorem 3.3 by Jensen's inequality.)

**Corollary 3.4**.: _In the setting of Theorem 3.3,_

1. _If at least a_ \(\)_-fraction of the iterates have_ \(>0\) _NE gap,_ \(^{2} O((1+^{(T)}_{- }+^{(T)}_{}))\)_._
2. _The average NE gap is bounded as_ \(O((1+^{(T)}_{-}+ ^{(T)}_{})})\)_._

In particular, in terms of asymptotic implications, if \(_{T+}^{(T)}_{-}}{T},_{T+ }^{(T)}_{-}}{T}=0\), then (i) for any \(>0\) the fraction of iterates of OGD with at least an \(\) Nash equilibrium gap converges to \(0\); and (ii) the average Nash equilibrium gap of the iterates of OGD converges to \(0\).

In the special case where \(^{(T)}_{-},^{(T)}_{}=O(1)\), Theorem 3.3 recovers the \(T^{-1/2}\) iterate-convergence rate of OGD in static bilinear saddle-point problems.

Meta-learningOur results also have immediate applications in the _meta-learning_ setting. More precisely, meta-learning in games is a special case of time-varying games which consists of a sequence of \(H\) separate games, each of which is repeated for \(m\) consecutive rounds, so that \(T m H\). The central goal in meta-learning is to obtain convergence bounds parameterized by the _similarity_ of the games; identifying suitable similarity metrics is a central question in that line of work.

In this context, we highlight that Theorem 3.3 shown above readily provides a meta-learning guarantee parameterized by the following notion of similarity between the Nash equilibria: \(_{^{(h,)}^{(h,)}, h[H]}_{h=1}^{ H-1}\|^{(h+1,)}-^{(h,)}\|_{2}\), where \(^{(h,)}\) is the set of Nash equilibria of the \(h\)-th game in the meta-learning sequence,2 as well as the similarity of the payoff matrices--corresponding to the term \(^{(T)}_{}\). In fact, under a suitable prediction--the one used by Harris et al. --the dependence on \(^{(T)}_{}\) can be entirely removed; see Proposition A.14 for our formal result. A compelling aspect of our meta-learning guarantee is that the considered algorithm is oblivious to the boundaries of the meta-learning. It is also worth noting that our similarity metric can be arbitrarily smaller than the one considered by Harris et al.  (Proposition A.15). We further provide some novel results on meta-learning in general-sum games in Section 3.3.

#### 3.1.1 Beyond bilinear saddle-point problems

As we alluded to earlier, our approach can be generalized beyond time-varying bilinear saddle-point problems to a more general class of time-varying _variational inequality (VIs)_ as follows. Let \(F^{(t)}:\) be the (single-valued) operator of the VI problem at time \(t\). \(F^{(t)}\) is said to satisfy the _MVI property_ if there exists a point \(^{(t,)}\) such that \(-^{(t,)},F^{(t)}()  0\) for any \(\). For example, in the special case of a bilinear saddle-point problem we have that \(F:(,)( ,-^{})\), and the MVI property is satisfied by virtue of Von Neumann's minimax theorem. It is direct to see that Property 3.1 applies to any time-varying sequence of VIs with respect to \((^{(t,)})_{1 t T}\) as long as every operator in the sequence \((F^{(1)},,F^{(T)})\) satisfies the MVI property. (Even more broadly, it would suffice if almost all operators in the sequence--in that their fraction converges to \(1\) as \(T+\)--satisfy the MVI property.) This observation enables extending Theorem 3.3 to a more general class of problems. As a concrete example, we provide a generalization of Theorem 3.3 (Theorem A.16) to _polymatrix zero-sum_ games  in Appendix A.1.7. By contrast, in problems where the MVI property fails it appears that a much different approach is called for.

We finally point out that our framework could have certain implications for solving (static) general VIs, as we discuss in Appendix A.1.8.

### Strongly convex-concave games

In this subsection, we show that under additional structure we can significantly improve the variation measures established in Theorem 3.3. More precisely, we first assume that each objective function \(f(,)\) is \(\)-strongly convex with respect to \(\) and \(\)-strongly concave with respect to \(\). Our second assumption is that each game is played for _multiple_ rounds \(m\), instead of only a single round; this is akin to the meta-learning setting. The key insight is that as long as \(m\) is large enough, \(m=(1/)\), those two assumptions suffice to obtain a _second-order_ variation bound in terms of the sequence of Nash equilibria, \(_{}^{(H)}_{h=1}^{H-1}\|^{(h+1,)}-^{(h,)}\|_{2}^{2}\), where \(^{(h,)}\) is a Nash equilibrium of the \(h\)-th game. This significantly refines the result of Theorem 3.3, and is inspired by the improved dynamic regret bounds obtained by Zhang et al. . Below we sketch the key ideas of the improvement; full proofs are deferred to Appendix A.2.

In this setting, it is assumed that Player \(x\) obtains the utility \(_{x}^{(t)}-_{}f^{(t)}( {x}^{(t)},^{(t)})\) at every time \(t[\![T]\!]\), while its regret will be denoted by \(_{,y}^{(T)}\); similar notation applies for Player \(y\). The first observation is that, focusing on a single (static) game, under strong convexity-concavity the sum of the players' regrets are _strongly nonnegative_ (Lemma A.18):

\[_{,x}^{(m)}(^{})+ _{,y}^{(m)}(^{})_{t=1}^{m}\|^{(t)}-^{}\|_{2}^{2}, \]

for any Nash equilibrium \(^{}\) of the game. In turn, this can be cast in terms of dynamic regret over the sequence of the \(h\) games (Lemma A.19). Next, combining those dynamic-regret lower bounds with a suitable RVU-type property leads to a refined second-order path length bound as long as \(m=(1/)\), which in turn leads to our main result below. Before we present its statement, let us introduce the following measure of variation of the gradients: \(_{ f}^{(H)}_{h=1}^{H-1}_{ }\|F^{(h+1)}()-F^{(h)}()\|_{2}^{2}\), where let \(F:(,)(_{ }f(,),-_{}f( ,))\). This variation measure is analogous to \(_{}^{(T)}\) we introduced earlier for time-varying BSPPs.

**Theorem 3.5** (Detailed version in Theorem A.21).: _Let \(f^{(h)}:\) be a \(\)-strongly convex-concave and \(L\)-smooth function, for all \(h[\![H]\!]\). Suppose further that both players employ OGD with learning rate \(=\{,\}\) for \(T\) repetitions, where \(T=m H\) and \(m\). Then, \(_{t=1}^{T}(^{(t)}(^{(t)}))^{ 2}=O(1+_{}^{(H)}+_{ f}^{(H)})\)._Our techniques also imply the improved regret bounds \(^{(T)}_{,x},^{(T)}_{,y}=O (^{(H)}_{}+^{(H)}_{ f}})\), under suitable tuning of the learning rate (see Corollary A.22).

There is another immediate but important implication of (3): _any_ no-regret algorithm in a (static) strongly convex-concave setting ought to be approaching the Nash equilibrium;3 in contrast, this property is spectacularly false in (general) monotone settings .

**Proposition 3.6**.: _Let \(f:\) be a \(\)-strongly convex-concave function. If players incur regrets such that \(^{(T)}_{,x}+^{(T)}_{,y} CT^{1-}\), for some parameters \(C>0\) and \((0,1]\), then for any \(>0\) and \(T>(})^{1/}\) there is a pair of strategies \(^{(t)}\) such that \(\|^{(t)}-^{}\|_{2}\), where \(^{}\) is a Nash equilibrium._

The insights of this subsection are also of interest in general monotone settings by incorporating a strongly convex regularizer; tuning its magnitude allows us to trade off between a better approximation and the benefits of strong convexity-concavity revealed in this subsection.

### General-sum multi-player games

Next, we turn our attention to general-sum multi-player games. For simplicity, in this subsection we posit that the game is represented in normal form, so that each player \(i[\![n]\!]\) has a finite set of available actions \(_{i}\), and \(_{i}(_{i})\). The proofs of this subsection are included in Appendix A.3.

Potential gamesFirst, we study the convergence of (online) gradient descent (GD) in time-varying _potential games_ (see Definition A.23 for the formal description); we recall that unlike two-player zero-sum games, gradient descent is known to approach Nash equilibria in potential games. In our time-varying setup, it is assumed that each round \(t[\![T]\!]\) corresponds to a different potential game described with a potential function \(^{(t)}\). We further let \(d:(,^{})_{_{i=1}^{n} _{i}}(()-^{}())\), so that \(^{(T)}_{}_{t=1}^{T-1}d(^{(t)},^{(t+1)})\); we call attention to the fact that \(d(,)\) is not symmetric. Analogously to Theorem 3.3, we use \(^{(t)}(^{(t)})_{ 0}\) to represent the NE gap of the joint strategy profile \(^{(t)}(^{(t)}_{1},, ^{(t)}_{n})\) at the \(t\)-th game.

**Theorem 3.7**.: _Suppose that each player employs (online) GD with a sufficiently small learning rate in a sequence of time-varying potential games. Then, \(_{t=1}^{T}(^{(t)}(^{(t)}))^{ 2}=O(_{}+^{(T)}_{})\), where \(_{}\) is such that \(|^{(t)}()|_{}\)._

We refer to Appendix B for some illustrative experiments related to Theorem 3.7.

General gamesUnfortunately, unlike the settings considered thus far, computing Nash equilibria in general games is computationally hard even under a crude approximation \(=(1)\)[25; 19; 30]. Instead, learning algorithms are known to converge--in a time-average sense--to relaxations of the Nash equilibrium, known as _(coarse) correlated equilibria_. For our purposes, we will employ a bilinear formulation of correlated equilibria, which dates back to the seminal work of Hart and Schmeidler  (see also [84, Chapter 12] for an excellent exposition). This will allow us to translate the results of Section 3.1 to general multi-player games.

Specifically, correlated equilibria4 can be phrased via a game between the \(n\) players and a _mediator_, an additional agent. At a high level, the mediator is endeavoring to identify a correlated strategy \((_{i=1}^{n}_{i })\) for which no player has an incentive to deviate from the recommendation. In contrast, the players are trying to optimally deviate so as to maximize their own utility. More precisely, there exist matrices \(_{1},,_{n}\), with each matrix \(_{i}\) depending solely on the utility of Player \(i\), for which the bilinear saddle-point problem can be expressed as

\[_{}_{(_{1},,}_{n})_{i=1}^{n}_{i}}_{i=1}^{n} ^{}_{i}}_{i}, \]where \(}_{i}\) above is a suitable set of strategies; we elaborate more on this formulation in Appendix A.3. This zero-sum game has the property that there exists a strategy \(^{}\) such that \(_{}_{i}}_{i}}(^{})^{}_{i}}_{i} 0\), for any player \(i[\![n]\!]\), which is precisely a correlated equilibrium.

Before we proceed, it is important to note that the learning paradigm considered here deviates from the traditional one in that orchestrating the protocol requires an additional learning agent, resulting in a less decentralized protocol. Yet, the dynamics induced by solving (4) via online algorithms remain _uncoupled_, in the sense that each player obtains feedback--corresponding to the deviation benefit--that depends solely on its own utility.

Now in the time-varying setting, the matrices \(_{1},,_{n}\) that capture the players' utilities can change in each repetition. Crucially, we show that the structure of the induced bilinear problem (4) is such that there is a sequence of correlated equilibria that guarantee nonnegative dynamic regret; this refines Property 3.1 in that only one player's strategies suffice to guarantee nonnegativity, even if the strategy of the other player remains invariant. Below, we denote by \(^{(T)}_{}\) the dynamic regret of Player min in (4), and by \(^{(T)}_{i}\) the regret of each player \(i[\![n]\!]\) up to time \(T\), so that the regret of Player max in (4) can be expressed as \(_{i=1}^{n}^{(T)}_{i}\).

**Property 3.8**.: _Suppose that \(^{(t,)}\) is a correlated equilibrium of the game at any time \(t[\![T]\!]\). Then, \(^{(T)}_{}(^{(1,)},,^{(T,)} )+_{i=1}^{n}^{(T)}_{i} 0\)._

As a result, this enables us to apply Theorem 3.3 parameterized on (i) the variation of the CE \(^{(T)}_{}:=_{^{(t,)}^{(t,)},  t[\![T]\!]}_{t=1}^{T-1}\|^{(t+1,)}-^{(t, )}\|_{2}\), where \(^{(t,)}\) denotes the set of CE of the \(t\)-th game, and (ii) the variation in the players' utilities \(^{(T)}_{}_{i=1}^{n}_{t=1}^{T-1}\| ^{(t+1)}_{i}-^{(t)}_{i}\|_{2}^{2}\); below, we denote by \(^{(t)}(^{(t)})\) the CE gap of \(^{(t)}\) at the \(t\)-th game.

**Theorem 3.9**.: _Suppose that each player employs OGD in a sequence of time-varying BSPPs (4) with a sufficiently small learning rate. Then, \(_{t=1}^{T}(^{(t)}(^{(t)}))^{2}=O(1+ ^{(T)}_{}+^{(T)}_{})\)._

There are further interesting implications of our framework that are worth highlighting. First, we obtain meta-learning guarantees for general games that depend on the (algorithm-independent) similarity of the correlated equilibria (Corollary A.25); that was left as an open question by Harris et al. , where instead algorithm-dependent similarity metrics were derived. Further, by applying Corollary A.12, we derive natural variation-dependent per-player regret bounds in general games (Corollary A.26); this addresses a question left by Zhang et al. , albeit under a different learning paradigm. We suspect that obtaining such results--parameterized on the variation of the CE--are not possible without the presence of the additional agent, as in (4).

### Dynamic regret bounds in static games

Finally, in this subsection we switch gears by investigating dynamic regret guarantees when learning in static games. The proofs of this subsection are included in Appendix A.4.

First, we point out that while traditional no-regret learning algorithms guarantee \(O()\)_external_ regret, instances of OMD--a generalization of OGD; see (5) in Appendix A--in fact guarantee \(O()\)_dynamic_ regret in two-player zero-sum games, which is a much stronger performance measure:

**Proposition 3.10**.: _Suppose that both players in a (static) two-player zero-sum game employ OMD with a smooth regularizer. Then, \(^{(T)}_{x},^{(T)}_{y}=O()\)._

In proof, the dynamic regret of each player under OMD with a smooth regularizer can be bounded by the _first-order_ path length of that player's strategies, which in turn can be bounded by \(O()\) given that the second-order path length is \(O(1)\) (Theorem A.7). In fact, Theorem A.7 readily extends Proposition 3.10 to time-varying zero-sum games as well, implying that \(^{(T)}_{x},^{(T)}_{y}=O(^{(T )}_{-}+^{(T)}_{})})\).

A question that arises from Proposition 3.10 is whether the \(O()\) guarantee for dynamic regret of OMD can be improved in the online learning setting. Below, we point out a significant improvement to \(O( T)\), but under a stronger two-point feedback model;5 namely, we posit that in every round each player can select an additional auxiliary strategy, and each player then gets to additionally observe the utility corresponding to the auxiliary strategies. Notably, this is akin to how the _extra-gradient method_ works  (also _cf_. [72, Section 4.2] for multi-point feedback models in the bandit setting).

**Observation 3.11**.: _Under two-point feedback, there exist learning algorithms that guarantee \(_{x}^{(T)},_{y}^{(T)}=O( T)\) in two-player zero-sum games._

In particular, it suffices for each player to employ OMD, but with the twist that the first strategy in each round is the _time-average_ of OMD; the auxiliary strategy is the standard output of OMD. Then, the dynamic regret of each player will grow as \(O(_{t 1}^{T})=O( T)\) since the duality gap of the average strategies is decreasing with a rate of \(T^{-1}\). It is an interesting question whether the bound of Observation 3.11 can be improved to \(O(1)\); we conjecture that there is a lower bound of \(( T)\).

General-sum gamesIn stark contrast, no (computationally efficient) sublinear dynamic regret guarantees are possible in general-sum games:

**Proposition 3.12**.: _Any polynomial-time algorithm incurs \(_{i=1}^{n}_{i}^{(T)} CT\) for any polynomial \(T\), even if \(n=2\) and \(C>0\) is an absolute constant, unless ETH for \(\) is false ._

Indeed, this follows immediately since computing a Nash equilibrium to \(O(1)\) precision in two-player games requires superoylnomial time . As such, Proposition 3.12 applies beyond the online learning setting. This motivates considering a relaxation of dynamic regret, wherein the sequence of comparators is subject to the constraint \(_{t=1}^{T-1}\{_{i}^{(t+1,)}_{i}^{(t,)} \} K-1\), for some parameter \(K\); this will be referred to as \(K\)-\(_{i}^{(T)}\). Naturally, external regret coincides with \(K\)-\(_{i}^{(T)}\) under \(K=1\). In this context, we employ Lemma A.1 to bound \(K\)-\(^{(T)}\) under OGD:

**Theorem 3.13** (Precise version in Theorem A.28).: _Suppose that all \(n\) players employ OGD with a suitable learning rate in an \(L\)-smooth game. Then, for any \(K\),_

1. \(_{i=1}^{n}K\)_-_\(_{i}^{(T)}=O(KLD_{}^{2})\)_;_
2. \(K\)_-_\(_{i}^{(T)}=O(K^{3/4}T^{1/4}n^{1/4}L^{1/2}D_{_{i}}^{3/2})\)_, for any player_ \(i[\![n]\!]\)_._

One question that arises here is whether the per-player bound of \(O(K^{3/4}T^{1/4})\) (Item 2) can be improved to \((K)\), where \(()\) hides logarithmic factors. The main challenge is that, even for \(K=1\), all known methods that obtain \((1)\) rely on non-smooth regularizers that violate the preconditions of Lemma A.2--our dynamic RVU bound beyond (squared) Euclidean regularization; yet, we point out that it is possible under a slightly stronger feedback model (Remark A.29). We finally highlight that the game-theoretic significance of the solution concept that arises as the limit point of no-regret learners when \(K\)-\(=o(T)\) was recently investigated by Crippa et al. .

## 4 Conclusions and future work

In this paper, we developed a framework for characterizing iterate-convergence of no-regret learning algorithms--primarily optimistic gradient descent (OGD)--in time-varying games. There are many promising avenues for future research. Besides closing the obvious gaps we highlighted in Section 3.4, it is important to characterize the behavior of no-regret learning algorithms in other fundamental time-varying multiagent settings, such as Stackelberg (security) games . Moreover, our results operate in the full-feedback model where each player receives feedback on all possible actions. Extending the scope of our framework to capture partial-feedback models as well is another interesting direction for future work.