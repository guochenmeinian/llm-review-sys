# Revisiting Differentially Private ReLU Regression

Meng Ding

University at Buffalo

&Mingxi Lei

University at Buffalo

&Liyang Zhu

KAUST

Shaowei Wang

Guangzhou University

&Di Wang

KAUST

&Jinhui Xu

University at Buffalo

Correspondence to: Di Wang <di.wang@kaust.edu.sa>, Jinhui Xu <jinhui@buffalo.edu>

###### Abstract

As one of the most fundamental non-convex learning problems, ReLU regression under differential privacy (DP) constraints, especially in high-dimensional settings, remains a challenging area in privacy-preserving machine learning. Existing results are limited to the assumptions of bounded norm \(\|\|_{2} 1\), which becomes stringent and strong with increasing data dimensionality. In this work, we revisit the problem of DP ReLU regression in overparameterized regimes. We propose two innovative algorithms, DP-GLMtron and DP-TAGLMtron, that outperform the conventional DPSGD. DP-GLMtron is based on a generalized linear model perceptron approach, integrating adaptive clipping and Gaussian mechanism for enhanced privacy. To overcome the constraints of small privacy budgets in DP-GLMtron, represented by \(()\) where \(N\) is the sample size, we introduce DP-TAGLMtron, which utilizes a tree aggregation protocol to balance privacy and utility effectively, showing that DP-TAGLMtron achieves comparable performance with only an additional factor of \(O( N)\) in the utility upper bound. Moreover, our theoretical analysis extends beyond Gaussian-like data distributions to settings with eigenvalue decay, showing how data distribution impacts learning in high dimensions. Notably, our findings suggest that the utility bound could be independent of the dimension \(d\), even when \(d N\). Experiments on synthetic and real-world datasets also validate our results.

## 1 Introduction

Protecting individual privacy in data analysis has emerged as a critical concern. In light of the vast quantities of personal and sensitive information involved, traditional methods of ensuring privacy are encountering significant challenges. Differential Privacy (DP), introduced by , has gained widespread recognition as a method for preserving privacy by adding a controlled amount of random noise to the data or query responses, thereby effectively concealing the details of any individual.

Differentially Private Stochastic Optimization (DP-SO) and its empirical form, Differentially Private Empirical Risk Minimization (DP-ERM), are foundational models extensively studied in the DP community. Although there is a substantial body of research on DP-SO and DP-ERM with convex loss functions , the understanding of nonconvex optimization in a DP context remains limited. Recent efforts have developed private algorithms with established proven utility bounds for differentially private nonconvex optimization . However, most of the current work employs the gradient norm of the population risk function as a measurement instead of excess population risk, commonly used in convex scenarios. Recently,  comprehensively studied different instances of non-convex learning, such as generalized linearmodels, ReLU regression, and multi-layer neural networks. However, for each problem, they imposed different strong assumptions, which left a big room to further understanding DP non-convex learning. In this paper, we consider the most fundamental one, the ReLU (Rectified Linear Unit) regression model.

As one of the most fundamental non-convex models, ReLU regression stands out due to its widespread use and effectiveness in deep learning. Despite its prevalence in industry, theoretical exploration of ReLU regression has been relatively limited.  studies the DP ReLU regression problem in both well-specified and misspecified settings. However, the problem is still far from well-understood. By and large, several challenges need to be addressed. Their analysis, for instance, heavily relies on strong assumptions about bounded norms of feature vectors and labels, i.e. \(\|\|_{2} O(1)\) and \(|y| O(1)\), which does not hold even for normal Gaussian distributions. Secondly, while they provide a utility upper bound of \((}+\{}{(Ne)^{1/2}},}\})\) with sample size \(N\) and dimension \(d\), their dimension-independent results heavily rely on their data assumption, leading to a dimension-dependent upper bound when considering Bernoulli or \(O(1)\)-subGaussian data where \(\|\|_{2} O()\) with high probability. That means when \(d N\), i.e., when in the overparameterization regime, previous results become trivial.

In this paper, we revisit the DP ReLU regression problem in the overparameterized regime. Specifically, we provide a general analysis under the well-specified setting and propose two novel algorithms (DP-GLMtron and DP-TAGLMtron). Our contributions are three-fold, see Table 1 for details.

**1)** We propose an innovative algorithm, DP-GLMtron, built upon the Generalized Linear Model Perceptron (GLM-tron) algorithm of . Specifically, DP-GLMtron operates by making a single pass over the dataset and processes one data point at a time by integrating appropriate clipping and adding Gaussian noise to maintain privacy. Additionally, we provide an excess population risk upper bound of \((}}{N}+}^{}}{N^{ 2}^{2}})\) where \(D_{}\) (\(D_{}^{}\)) is the (private) effective dimension. It is noticed that \(D_{}\) and \(D_{}^{}\) are defined in a way that does not directly rely on the dimension \(d\). Furthermore, our results are applicable not only to Gaussian-like data, typically considered in the traditional differential privacy community, but also to data distributions that exhibit either polynomial or exponential decay. In these cases, we demonstrate that the utility bound is independent of the dimension \(d\), which marks a significant departure from the traditional one (see more discussions in Remark 4). Finally, we also show that our analysis on DP-GLMtron is almost tight by providing a lower bound of \((}}{N}+}^{} }{N^{2}^{2}})\).

**2)** A significant issue with DP-GLMtron is its upper bound only holds with a small privacy budget \(=(})\) (for further details, see Remark 3), which is due to privacy amplification via shuffling. To tackle this limitation, we propose DP-TAGLMtron, which utilizes the tree aggregation protocol to introduce noise into the sum of mini-batch gradients instead of using privacy amplification. Our analysis reveals that the utility upper bound of DP-TAGLMtron is closely comparable to that of DP-GLMtron only with an additional factor of \(O( N)\). This finding indicates that DP-TAGLMtron can offer similar performance benefits while potentially being applicable in settings with larger privacy budgets.

**3)** We conducted experiments with both synthetic data and real data on the ReLU regression model across various algorithms and privacy budgets. Specifically, in the first part, synthetic data was generated under two eigenvalue decay scenarios, \(_{i} i^{-2}\) and \(_{i} i^{-3}\). The results revealed that faster eigenvalue decay consistently resulted in lower excess risk compared to slower decay, indicating that it may suffer less from dimensionality with respect to utility. Additionally, in both synthetic and real data, DP-GLMtron and DP-TAGLMtron outperformed DP-SGD and DP-FTRL , especially as the sample size increased, highlighting the effectiveness of our proposed methods.

Due to space limitations, we have included the algorithms and proofs in the appendix.

## 2 Related Work

**Private Nonconvex Optimization** Previous research focusing on the utility of DP-SO/DP-ERM with convex loss functions predominantly employed excess population risk as the metric of utility. In contrast, for non-convex scenarios, utility assessment can be broadly categorized into three types: first-order stationary-based, second-order stationary-based, and excess population risk. The first-order stationary-based method [43; 53; 34; 8; 52; 50] involves evaluating the \(_{2}\)-norm of the gradient of the population risk function. This method, however, encounters several challenges. For example,  shows that the gradient norm tends to approach zero as the sample size increases indefinitely, while there exists no assurance that a differentially private estimator will converge to or even be near any significant local minimum. The Second-order stationary-based method [42; 45] employs the norm of the gradient and the Hessian matrix minimal eigenvalue of the population risk function. However, this approach is particularly suited to specific scenarios where any second-order stationary point is a local minimum of the problem, and all the local minima are the global minimum, such as matrix completion and dictionary learning. The third approach involves directly employing the excess population risk as the measure of utility [33; 43] and our work aligns with this direction. However, as we mentioned these bounds either need strong assumptions or become trivial when \(d N\).

**Private Generalized Linear Models** DP-SO and DP-ERM with generalized linear loss (DP-GLL) and generalized linear models (DP-GLM) have gained significant attention in recent years. The key developments in this area can be regarded as three aspects: 1) For convex loss functions, an early study on DP-GLL  revealed that, assuming features have bounded \(l_{2}\) norm, the error bound can reach \((})\), contrasting with the general convex DP-ERM bound of \(O(}{N})\). A subsequent study  found that in constrained cases, the error bound depends on the Gaussian width of the constraint set.  improved the bound to \(O(}{N})\) under the assumption that the data space is a bounded set, where \(\) is the rank of the expectation of the data matrix. 2) For constrained DP-GLM,  explored various scenarios, including smooth/non-smooth losses and losses in \(_{p}\) space for \(1 p 2\).  provided the optimal rates of DP-GLM in unconstrained settings, identifying different rates depending on the nature of the loss function (smooth and non-negative but not necessarily Lipschitz, or Lipschitz). 3) For non-convex losses, [34; 7] provided dimension-independent bounds for the gradient \(_{2}\) norm of the population risk function. As a specific instance of GLM, this work focuses on the overparameterized regime with sub-Gaussian data rather than bounded data.

## 3 Preliminaries

**Notations**: In this paper, we adhere to a consistent notation style for clarity. We use boldface lower letters such as \(,\) for vectors, and boldface capital letters (e.g. \(,\)) for matrices. Let \(\|\|_{2}\) denote the spectral norm of \(\). For two matrices \(\) and \(\) of appropriate dimension, their inner product is defined as \(,:=(^{})\). For a positive semi-definite (PSD) matrix \(\) and a vector \(\) of appropriate dimension, we write \(\|\|_{}^{2}:=^{}\). The outer product is denoted by \(\).

Given a dataset \(D=\{(_{i},y_{i})\}_{i=1}^{N}\) with each data point has a feature vector \(_{i}^{d}\) and a response variable \(y_{i}\). In this paper, we assume that \(\{(_{i},y_{i})\}_{i=1}^{N}\) are i.i.d. sampled from a ReLU regression model, i.e., each \((_{i},y_{i})\) is a realization of the ReLU regression model \(y=(^{}_{*})+z\), where \(():=\{,0\}\) is the Rectified Linear Unit \(()\); \(z\) is a zero mean randomized noise; \(_{*}^{d}\) is the optimal model parameter. Our goal is to output a model \(_{}\) that maintains privacy while minimizing the excess population risk, i.e., \((_{})-(_{*})\), where

\[()=_{(,y)}[ ((^{})-y)^{2}].\] (1)

In this paper, we will focus on classic Differential Privacy to ensure privacy.

   Method & Upper Bound & Lower Bound & Privacy Constraints & Data Assumptions \\  DP-PGD  & \((\{}{(N)^{1/2}},}\}+\) & - & - & \(\|\|_{2} 1\) \\  & \(}\) & \(}\) & \(}\) & \(}\) & \\
**DP-GLMron** & \((}{N}+}^{}}{N^{2} ^{2}})\) & \((}}{N}+}^{}}{N^{ 2}^{2}})\) & \(=O(})\) & - \\
**DP-TAGLMtron** & \((}}{N}+}^{}+D_{ }}{N^{2}^{2}})\) & - & - & - \\   

Table 1: Comparison of our work with related studies on \((,)\)-DP ReLU regression in the statistical estimation setting. Here, \(N\) represents the sample size, \(d\) refers to the dimension and \(D_{,\,}\), \(D_{}^{}\) are the (private) effective dimensions. It is noticed that \(D_{}\) and \(D_{}^{}\) are defined in a way that does not directly rely on the dimension \(d\). Instead, they segment the feature space into an effective subspace.For further discussion, please see Remark 4.

**Definition 3.1** (Differential Privacy ).: Given a data universe \(\), we say that two datasets \(D,D^{}\) are neighbors if they differ by only one element, which is denoted as \(D D^{}\). A randomized algorithm \(\) is \((,)\)-differentially private (DP) if for all adjacent datasets \(D,D^{}\) and for all events \(S\) in the output space of \(\), we have \(((D) S) e^{}((D^{}) S)+\).

In the following, we will introduce some definitions and assumptions related to the model.

**Definition 3.2** (Well-specified Condition).: Assume that there exists a parameter \(_{*}^{d}\) such that \([y]=(^{}_{*})\), and the variance of the model noise can be denoted by \(^{2}:=[(y-(^{}_{*})) ^{2}].\)

In the literature, the well-specified setting is also extensively referred to as the "noisy teacher" setting  or the well-structured noise mode . It has been widely studied previously .

**Assumption 3.3** (Data Covariance).: Define \(:=[^{}]\) as the expected data covariance matrix.

Denote the eigen decomposition of the data covariance by \(=_{i}_{i}_{i}_{i}^{}\), where \((_{i})_{i 1}\) are eigenvalues in a nonincreasing order and \((_{i})_{i 1}\) are the corresponding eigenvectors. Define \(_{k_{1}:k_{2}}\) as \(_{k_{1}:k_{2}}:=_{k_{1}<i k_{2}}_{i}_{i} _{i}^{}\), and allow \(k_{2}=\) to imply that \(_{k:}=_{i>k}_{i}_{i}_{i}^{}\).

In the case of a Gaussian distribution, the data covariance matrix reduces to an identity matrix scaled by variance.

**Assumption 3.4** (Fourth moment conditions).: Define that the fourth moment of \(\) as \(\):

1. There exists a constant \(>0\) such that for any Positive Semi-Definite (PSD) matrix \(\), the following holds: \[[^{}^{}] ().\]
2. There exists a constant \(>0\), such that for every PSD matrix \(\), the following holds: \[[^{}^{}] -( ).\]

**Remark 1**.: For Gaussian distribution, it can be verified that Assumption 3.4 holds with \(=3\) and \(=1\). Furthermore, the assumption of sub-Gaussianity in data, specifically that \(^{-}\) is a sub-Gaussian random vector, as posited in previous work  for private linear regression model, can be shown to imply Assumption 3.4 (A) . It is important to note that Assumption 3.4 (B) is specifically utilized in our analysis for establishing the lower bound only.

**Definition 3.5** (\((,C_{2},a,b)\)-Tail).: Let both \(\) and \(\) exist and are finite. A random vector \(\) satisfies \((,C_{2},a,b)\)-Tail if the the following holds:

* \( a>0\) s.t. with probability \( 1-b\), \[\|\|_{2}^{2}[\|\|_{2}^{2}]^{2a}(1/b),\] (2)
* We have, \[_{,\|\|=1}[((, |^{2}}{C_{2}^{2}\|\|_{2}})^{1/2a})] 1,\] That is, for any fixed \(\), with probability \( 1-b\) : \[(,)^{2} C_{2}^{2}\|\|_{2}\| \|^{2}^{2a}(1/b).\]

Both conditions above provide Gaussian-like tail bounds determining the resilience of the dataset. Note that Definition 3.5 is widely used in the recent literature on DP analysis for the sub-Gaussian type of data such as . In this work, we will assume each sample \(\) satisfying \((,C_{2},a,b)\)-Tail and the distribution of the inherent noise \(z\) satisfies \((^{2},C_{2},a,b)\)-Tail, which could capture more statistical properties beyond expectations. Additionally, according to Assumption 3.4 (A), it holds that \(\|\|_{2}^{2}\,()^{2a}(1/b)\) in Equation (2).

**Assumption 3.6** (Symmetricity conditions).: Assume that for every \(,^{d}\), it holds that:

\[[^{}[^{} >0,^{}>0]] =[^{}[^{} <0,^{}<0]]\] \[[(^{})^{2} ^{}[^{}>0,^{} >0]] =[(^{})^{2}^{ }[^{}<0,^{} <0]].\]

**Remark 2**.: Here we require that both the second and fourth moments of \(\) remain symmetric. It can be shown that Assumption 3.6 is satisfied when \(\) and \(-\) follow the same distribution, which can apply to symmetric sub-Gaussian such as symmetric Bernoulli or Gaussian distributions.

## 4 Proposed DP-GLMtron Algorithm

Before presenting the analysis of DP ReLU regression problem, we first introduce our novel algorithm: DP-GLMtron (Algorithm 1), which is inspired by and built upon the Generalized Linear Model Perceptron (GLM-tron) algorithm of .

The fundamental distinction between SGD and GLMtron lies in their respective update rules. Specifically, it takes the following rules:

\[&_{t}= _{t-1}-((_{t}^{} _{t-1})-y_{t})_{t}[_{t}^{}_{t -1}>0]\\ &_{t}=_{t-1}- ((_{t}^{}_{t-1})-y_{t}) _{t}.\] (3)

where the algorithm commence from an initial point \(_{0}\) and iterate from \(t=0\) to \(t=N\) with stepsize \(\). In contrast to the update rule of SGD, GLMtron diverges by changing the derivative of the ReLU function in its update mechanism. Typically, in neural network training, the gradient of the activation function, such as ReLU, is a critical component of the backpropagation process. The exclusion of this derivative in GLMtron's framework not only simplifies the computational process but also enhances efficiency. Our motivation for developing the DP-GLMtron algorithm stems from observations regarding the limitations of DP-SGD, as it sometimes fails to reach the optimal solution and can struggle with convergence under conditions of high noise, often settling at a saddle point instead (as illustrated in Figure 4). This issue is particularly pronounced when considering a low privacy budget. Furthermore,  demonstrates that this specific omission plays a significant role in GLMtron's ability to efficiently identify a predictor that closely approximates the optimal solution.

Building upon these foundations, we now present the detailed implementation of our proposed DP-GLMtron. For Algorithm 1, we begin with a random permutation of the dataset, ensuring that each data point is treated independently and uniformly. Also, such shuffling can amplify privacy. Subsequently, the algorithm performs a single pass over the dataset. Throughout this pass, we need to determine the clipping threshold before undergoing iterative updates for \(\). Establishing an appropriate clipping threshold is crucial for balancing the trade-off between utility and privacy. If the clipping threshold is set excessively low, it results in loss of gradient information, consequently leading to high bias . Consequently, we propose an advanced Algorithm 2 based on the residual estimator in  to achieve an adaptive clipping  by utilizing a small batch of data points.

Recall that the clipping term is a product of the residual, \(((_{t}^{}_{t})-_{t})\), and the covariate, \(_{t}^{}\). Based on Definition 3.5 and Assumption 3.4, we understand that \(\|_{t}\|_{2}^{2}()^{2 a}(1/b)\) with a probability of at least \(1-b\). Thus, it is sufficient to get an upper bound of \(|(_{t}^{}_{t})-_{t}|\). To bound this, we consider three cases, \()\)\(_{t}^{}_{t}>0,_{t}^{}_{s} >0\); \()\)\(_{t}^{}_{t}<0,_{t}^{}_{s} >0\); \()\)\(_{t}^{}_{t}>0,_{t}^{}_{s} <0\), corresponding to \(d_{t}^{1}\), \(d_{t}^{2}\), and \(d_{t}^{3}\) in step 2 of Algorithm 2. It is noticed that the inherent model noise is not considered when initially calculating the residual for the third case at step 2, while this noise is taken into account at step 15. Moreover, when considering a special case that \(_{t}^{}_{t}<0,_{t}^{}_{s}<0\), the clipping threshold is integrated into the third scenario. For each residual, we first partition the dataset into \(K\) batches, compute an estimate for each batch, and form a histogram with over those \(K\) estimates. We then apply a private histogram mechanism with geometrically increasing bin sizes. The bin with the most estimates is used, ensuring a constant factor

Figure 1: Training trajectories of DP-SGD and DP-GLMtron on a 2D noiseless ReLU regression with symmetric Bernoulli data.

approximation of the distance to the optimum. Finally, we take the maximum among the three private residual estimates.

Upon determining the clipping threshold \(_{t}\), it is subsequently incorporated into the existing clipping list \(\). Following this, Gaussian noise with a standard deviation of \(2f_{t}\) is introduced to the clipped \(_{t}\) and then the weight vector \(_{t+1}\) will be updated. Finally, the algorithm culminates by computing the average of the iterates.

**Theorem 4.1** (Privacy Guarantee).: _Suppose DP-GLMron is applied to \(N\) input samples with a noise multiplier set to \(f=O(})\). Under these conditions, the algorithm satisfies \((,)\)-differential privacy for \(=O(})\), with \(^{}=}\), \(^{}=\) in Algorithm 2._

**Remark 3**.: For general data distributions, DP-GLMron necessitates a more complex approach to clipping decisions. To this end, we incorporate Algorithm 2, which uses the magnitude of the residual along with privacy parameters \(^{}\) and \(^{}\), as a means to estimate the private threshold. Additionally, we take only one pass over random shuffling data for Algorithm 1, which is a well-known method of privacy amplification. Hence, we need to add noise \(O(}{})\), where \(_{t}\) is the clipping threshold and \(\) must be restricted to \(1/\), to satisfy the requirement of  and to ensure differential privacy.

**Theorem 4.2** (Utility Guarantee Upper).: _Consider a dataset \(D=\{(_{i},y_{i})\}_{i=1}^{N}\), where each \(_{i}\) is drawn from a distribution \(\) satisfying \((,C_{2},a,b)\)-Tail and the distribution of the inherent noise \(z\) satisfies \((^{2},C_{2},a,b)\)-Tail. Given Assumptions 3.4 and 3.6, if we set a step size \(()^{2}(N)}\), a noise multiplier \(f=O(})\), estimating data size \(m=()}{^{}})\), then with a probability of \(1-1/N\), the output of Algorithm 1 will satisfy:_

\[[L(}_{N})]-L(_{*}) ++,\]

_where the bias, variance, and private terms are upper bounded by_

\[ N^{2}}\|_{0}-_{ *}\|_{_{0}^{-1}}^{2}+\|_{0}-_{*}\|_{_ {k^{*},}}^{2}+(\|_{0}-_{*}\|_{ _{0},k^{*}}{N}+_{k^{*},}}^{2}) D_{ }\] \[ }{N} D_{},f^{2}}{N} D_{}^{}\]

_where the effective dimensions are given by_

\[D_{}=k^{*}+^{2}}{4}_{i>k^{*}}_{i}^{2},  D_{}^{}=_{i<k^{*}}_{i}^{-1}+ ^{2}}{4}_{i>k^{*}}_{i}\]

_with \(k^{*}=\{k:_{k}\}\) and \(=\{_{t}\}_{t=0}^{T-1}\)._

**Remark 4**.: The utility bound is typically decomposed into three terms: the bias term, which is associated with the initial value of the model; the variance term, stemming from the inherent noise intrinsic to the model itself; and the privacy term, introduced by the privacy-preserving mechanism. Notably, the results contain two crucial factors: the effective dimension \(D_{}\) and the private effective dimension \(D_{}^{}\), which can be regarded as the function of data covariance matrix's eigenvalues.

Technical Understanding Under Simplified CasesTo provide a more intuitive explanation, we explore two simplified scenarios. **1)**\(\|\|_{2} 1\): In this scenario, where the norm of the data is constrained, the trace of the covariance matrix \(() 1\). This condition indicates that the sum of the eigenvalues, \(_{i}\), is less than 1. Given that the eigenvalues are ordered non-increasingly (refer to Assumption 3.3), the majority of these eigenvalues must be very small. Assuming the step size \(\) is sufficiently small and scaled with the square root of the data size, \(\), the optimal number of iterations \(k^{*}\) must be confined within a scope proportional to \(\). If not, the inequality \(() k^{*} 1\) would contradict the data assumptions. Furthermore, the clipping threshold, which depends on \(()\), remains a constant term, and the private effective dimension \(D_{}^{}\) is bounded above by \(N\). Therefore, the total utility is \((}+(^{-1})}{N^ {2}^{2}})\). **2)** sub-Gaussian tail: In this case, the eigenvalue of the covariance matrix corresponds to the variance of sub-Gaussian data. Both the effective dimension, \(D_{}\), and the private effective dimension, \(D_{}^{}\), effectively reduce to the feature dimension \(d\). Moreover, the clipping threshold contributes a factor of \(d\), leading to a total utility bound of \((}+}{N^{2}^{2}})\).

The Role of Eigenvalue in High-dimensional SettingExisting work in the DP community primarily relies on data assumptions satisfying Gaussian-like distributions, as discussed in Remark 1, which reduces the data covariance matrix to an identity matrix scaled by variance. Such an assumption may cause the case to be overlooked when the spectrum exhibits decay. Theorem 4.2 offers a novel perspective by elucidating the interplay between the feature space and the utility bound. Notice the crucial cut-off index \(k^{*}=\{k:_{k}\}\), which separates the entire space into a \(k^{*}\) dimensional subspace. It enables us to obtain a bound that does not directly depend on the feature dimension \(d\) in the overparameterized regime. To achieve a diminishing bound, it is necessary for \(k^{*}\) to be \(o(N)\) and for the tail summation of eigenvalues to be \(o(1/N)\), which indicates that it is possible to achieve improved results without suffering the dimension \(d\) in the overparameterized regime.

To better understand the effective dimension, we offer examples of data spectra where the excess risk diminishes under certain conditions.

**Corollary 4.3**.: _Given the same assumptions in Theorem 4.2 and suppose \(\|_{0}-^{*}\|_{2}\) is bounded, it holds that:_

1. _If the spectrum of_ \(\) _satisfies_ \(_{k}=k^{-r}\) _for some_ \(r>1\)_, then with probability at least_ \(1-1/N\)_, the utility is upper bounded by_ \((N^{-}(1+(^{-2}N^{})^{ {r}{1+2r}}))\)_._
2. _If the spectrum of_ \(\) _satisfies_ \(_{k}=e^{-k}\)_, then with probability at least_ \(1-1/N\)_, the utility is upper bounded by_ \((N^{-1}(1+(^{-2}N)^{}))\)_._

**Remark 5**.: Corollary 4.3 indicates that utility bound in the overparameterized setting can still vanish if the spectrum exhibits either polynomial or exponential decay. Specifically, a constant privacy budget \(=O(1)\) can yield utility bounds of \((N^{-r/(1+2r)})\) for polynomial decay and \(}(N^{-1/2})\) for exponential decay. Notably, faster eigenvalue decay (e.g. \(r=2,3\)) consistently resulted in lower excess risk compared to slower decay, indicating that it may suffer less from dimensionality with respect to utility. This insight cannot be derived when considering only traditional Gaussian-like data. Our experiment in Section 6 also validates the conclusion. Moreover, increasing the privacy budget to \(=O(N^{r/(2+2r)})\) for polynomial decay and \(=O(N^{1/2})\) for exponential decay improves utility bounds to \(}(N^{-r/(1+r)})\) and \(}(N^{-1})\), respectively, matching non-private and linear regression scenario results [49; 48].

In the following, we show that our previous analysis for Algorithm 1 is almost tight.

**Theorem 4.4** (Utility Guarantee Lower).: _Consider a dataset \(D=\{(_{i},y_{i})\}_{i=1}^{N}\), where each \(_{i}\) is drawn from a distribution \(\) satisfying \((,C_{2},a,b)\)-Tail and the distribution of the inherent noise \(z\) satisfies \((^{2},C_{2},a,b)\)-Tail. Given Assumptions 3.4 and 3.6, if we set the same parameters as in Theorem 4.2, then with a probability of \(1-1/N\), the output of Algorithm 1 will satisfy:_

\[[L(}_{N})]-L(_{*})+ +,\]

_where_

\[ N^{2}}\|_{0}-_{ *}\|_{_{0:_{i}}^{-1}}^{2}+\|_{0}-_{*} \|_{_{^{*},}}^{2}+(\|_{0}- _{*}\|_{_{0:^{*}}}{N^{n}_{1}}+_ {^{*},}}^{2}) D_{}\] \[ }{N} D_{},^{2}f^{2}}{N} D_{}^{}\]

_where the effective dimensions, \(k^{*}_{m}\) are defined same as in Theorem 4.2 and denotes \(=\{_{t}\}_{t=0}^{T-1}\)._

**Remark 6**.: Similar to the Theorem 4.2, the lower bound stated here consists of three terms: the bias term, the variance term, and the privacy term. It is noticed that Theorem 4.4 provides the algorithmic lower bound Algorithm 1, applicable in both under-parameterized and over-parameterized settings. Notably, our lower bound aligns with the upper bound, differing only by absolute constants.

## 5 Advanced DP-TAGLMtron

A pivotal concern with Algorithm 1 DP-GLMtron lies in its primary applicability in the setting where the privacy budget \(\) is small, which could potentially limit its practical utility (see Remark 3 for more details). To address this challenge, we introduce DP-TAGLMtron (Algorithm 3) in this section, which is designed to attain an improved trade-off between privacy and utility.

In Algorithm 3, the process starts with setting the initial weights. For each iteration \(t\), a private residual is determined by Algorithm 2 on the estimating samples, current weights, and privacy parameters. Subsequently, the clipping bound is computed as the product of the private residual and the covariate, analogous to the previous case. Once this clipping bound \(_{t}\) for the current iteration is computed, it is added to the existing clipping list \(\). With \(_{t}\) in place, the clipped "gradient" \((_{t})\) is calculated. The maximum value from this list is selected as the clipping threshold \(\) for our next step. The "gradient" \(_{t}\) is then forwarded to the differentially private tree-aggregation mechanism with noise multiplier \(f\) and the maximal clipping threshold \(\).

It is noticed that our proposed algorithm, DP-TAGLMtron, does not depend on privacy amplification. Instead, it primarily involves employing tree aggregation  to introduce noise into the sum of mini-batch gradients. At the beginning of training, we construct a binary tree consisting of \((2^{_{2}N+1}-1)\) nodes, corresponding to \(N\) leaves. Each leaf node is assigned a label \(_{i}\), and the value of each internal node represents the sum of its child nodes. In each iteration, the tree receives a vector \(_{t}\), derived from the current data pair \((_{t},y_{t})\), which is used to update the tree, incorporating the new node. The final output, denoted as \(^{}_{ t}\), is obtained by aggregating the gradients accumulated in the binary tree and adding Gaussian noise to this aggregate. As a result, we can understand that \(^{}_{ t}\) is the private estimation of \(_{i=0}^{t}_{i}\) generated by tree aggregation protocol as follows:

\[^{}_{ t}=_{i=0}^{t}_{i}+_{t}, _{i}=_{_{t}}(_{i}(( _{i}^{}_{i})-y_{i})),\]

and \(_{t}\) is a combination of at most \(O(_{2}t)\) Gaussian. Therefore, each \(_{i}\) influences only its ancestor nodes, affecting at most \( t+1\) nodes. By introducing Gaussian noise with a standard deviation of \(O( t/)\) to each node, the entire tree achieves \((,)\)-differential privacy.

When receiving the output of Algorithm 4, the optimization objective at step \(9\) is to determine the weight parameter \(\). This objective function is composed of two key components: the first term, \((^{}_{ t},)\), represents the inner product of \(^{}_{ t}\) and the weight parameter \(\). The second term, \(\|\|_{2}^{2}\), introduces a regularization mechanism, where \(\) controls the balance between fitting the training data and constraining the magnitude of the parameters to prevent overfitting. One common method to find the optimal solution for this optimization problem is to take the derivative of the objective function and set it to zero. Therefore, for the given problem at step 9, we have the following update:

\[_{t+1}-^{}_{ t}=-(_ {i=0}^{t}_{i}+_{t}).\]

It should be noted that in Algorithm 1, if the initialization is set to \(_{0}=\), then the update rule of \(\) can be considered equivalent to the one presented here, provided that the variance of the noise and the clipping threshold are disregarded. Upon completing all iterations, the final model weights are determined by averaging all the updated weights from each iteration.

**Theorem 5.1** (Privacy Guarantee).: _Suppose DP-GLMtron is applied to \(N\) input samples with a noise multiplier set to \(f=}{}\). Under these conditions, the Algorithm 3 satisfies \((,)\)-differential privacy, with \(^{}=}\) and \(^{}=\) in Algorithm 2._

**Remark 7**.: Differing from Algorithm 1, the update rule of DP-TAGLMtron mainly utilizes the DP-Tree-Aggregation mechanism, which involves aggregating the "gradients" collected in a binary tree and then adding Gaussian noise to this aggregate. Based on , Algorithm 4 achieves \((,)\)-differential privacy with appropriate noise multiplier \(f=}{}\), ensuring that Algorithm 3 also upholds \((,)\)-DP under similar settings with \(^{},^{}\) as Algorithm 2.

**Theorem 5.2** (Utility Guarantee).: _Consider a dataset \(D=\{(_{i},y_{i})\}_{i=1}^{N}\), where each \(_{i}\) is drawn from a distribution \(\) satisfying \((,C_{2},a,b)\)-Tail and the distribution of the inherent noise \(z\) satisfies \((^{2},C_{2},a,b)\)-Tail. Given Assumptions 3.4 and 3.6 hold, if we set a stepsize \(<1/(())\), a noise multiplier \(f=}{}\), then with a probability of \(1-1/N\), the output of Algorithm 3 will satisfy:_

\[[L(}_{N})]-L(_{*})+ +,\]_where_

\[ N^{2}}\|_{0}- _{*}\|_{_{0}^{2}}^{2}+\|_{0}- _{*}\|_{_{k^{*}}}^{2}+(\|_{0}-_{*}\|_{_{}^{2} }+_{k^{*}}}^{2}) D_{}\] \[ }{N} D_{},_{2}N}{N}(_{2}N( )+^{2})(D_{}+D_{}^{})\]

_where the effective dimensions and \(k^{*}\) are defined same as in Theorem 4.2._

**Remark 8**.: It is noteworthy that in Theorem 5.2, both the bias term and the variance term are consistent with those in Theorem 4.2. This alignment extends to the non-private results, with the only differences being in the absolute constants. Additionally, the private term in Theorem 5.2 closely aligns with that in Theorem 4.2, with the primary difference being a factor of \(_{2}N\). It is intuitive that the tree aggregation mechanism utilized in DP-TAGLMtron indicates that the noise added for privacy is derived from a combination of at most \(_{2}N+1\) and at least one random Gaussian vector. This aspect suggests that DP-TAGLMtron diverges from the upper bound of DP-GLMtron at most by a factor of \(_{2}N\) and adheres to the same lower bound as DP-GLMtron with a different noise variance. Moreover, in contrast to Algorithm 1, DP-TAGLMtron does not necessitate \(=O(1/)\) to maintain differential privacy, allowing a broader range of privacy budget settings in terms of flexibility and applicability.

## 6 Empirical Simulation

In this section, we first conduct experiments with synthetic data to validate the theoretical insights related to the role of eigenvalues in a high-dimensional setting. Additionally, due to space constraints, we present a real-data experiment on the MNIST dataset in Appendix B to demonstrate the performance of our proposed method.

**Experiment Setup.** The experiments were designed with varying privacy budgets \(()\) set at 0.05, 0.2, and 0.5 with \(=}\) to observe the impact of differential privacy constraints on the learning process. We generated the data with two eigenvalue decay scenarios, \(_{i} i^{-2},i^{-3}\), simulating different distributions of feature importance. The dimensionality of the data was fixed at 1,024 to maintain consistency across trials. The learning rate was initially set to \(10^{-2}\), w

Figure 2: Comparative Performance of Various Differential Privacy Algorithms Across Different Data Spectrum and Privacy Budgets. This figure illustrates the performance of four differential privacy algorithms—DP-SGD, DP-GLMtron, DP-TAGLMtron, and DP-FTRL—on generated Bernoulli distributed data, comparing excess risk across different sample sizes and privacy budgets.

the sample size, which varied from 50 to 550 and increased in steps to 100. We utilized a ReLU regression model with noise to be trained using the previously mentioned algorithms, integrating privacy-preserving noise adjustments. The algorithms underwent a single iteration over generated data. The primary metric for evaluation was the average excess risk across varying dataset sizes and \(\) values. For each experiment, we also consider DP-SGD  and DP-FTRL  as baselines.

**Empirical Findings.** Compared with Figures (2a-2c), which illustrate data with eigenvalue decay \(_{i} i^{-2}\), Figures (2f-2e), which illustrate data with eigenvalue decay \(_{i} i^{-3}\) exhibit the lower excess risk under the same training algorithms and conditions regardless of privacy budget. This indicates that fast eigenvalue decay suffers less from the dimensionality with respect to utility, thereby validating our results in Section 4. Additionally, results in both data with eigenvalue decay \(_{i} i^{-2}\) and data with eigenvalue decay \(_{i} i^{-3}\) show that DP-GLMtron and DP-TAGLMtron consistently outperform DP-SGD and DP-FTRL regarding excess risk, particularly as the sample size increased. Moreover, it is noticed that the DP-SGD and DP-FTRL exhibit poor convergence performance even with comparatively large data sizes. Finally, in both figures, increasing the privacy budget from \(=0.05\) to \(=0.5\) resulted in lower excess risks for both algorithms. This observation confirms the anticipated trade-off between privacy and learning performance, with a looser privacy constraint leading to more accurate models.

## 7 Conclusions

In conclusion, our study significantly advances the field of differential privacy in ReLU regression problems. Our first proposed algorithm, DP-GLMtron, provides an excess population risk upper bound of \((}}{N}+}^{}}{N^ {4}^{2}})\), offering a novel perspective on utility decomposition and highlighting the impact of eigenvalues in overparameterized settings. Our results apply not only to Gaussian-like data, typically considered in traditional differential privacy studies, but also to data distributions with polynomial or exponential decay. To address limitations with small privacy budgets, we developed DP-TAGLMtron, which offers performance comparable to DP-GLMtron but supports broader privacy budgets. Empirical results from both synthetic and real data experiments demonstrate that DPGLMtron and DP-TAGLMtron consistently outperform DP-SGD and DP-FTRL.

## Impact Statements

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

## Limitations

While this study advances previous work by relaxing certain assumptions and providing both synthetic and real-data validations of the theoretical findings, the analysis is specifically confined to the ReLU regression problem. Consequently, it remains uncertain whether these conclusions can be generalized to other nonconvex optimization scenarios. Future research could explore whether similar patterns hold across a broader range of nonconvex problems.