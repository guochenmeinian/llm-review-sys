# Momentum Provably Improves Error Feedback!

Ilyas Fatkhullin

ETH AI Center & ETH Zurich & Alexander Tyurin

KAUST\({}^{*}\) &Peter Richtarik

KAUST

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia.

###### Abstract

Due to the high communication overhead when training machine learning models in a distributed environment, modern algorithms invariably rely on lossy communication compression. However, when untreated, the errors caused by compression propagate, and can lead to severely unstable behavior, including exponential divergence. Almost a decade ago, Seide et al. (2014) proposed an error feedback (EF) mechanism, which we refer to as EF14, as an immensely effective heuristic for mitigating this issue. However, despite steady algorithmic and theoretical advances in the EF field in the last decade, our understanding is far from complete. In this work we address one of the most pressing issues. In particular, in the canonical nonconvex setting, all known variants of EF rely on very large batch sizes to converge, which can be prohibitive in practice. We propose a surprisingly simple fix which removes this issue both theoretically, and in practice: the application of Polyak's momentum to the latest incarnation of EF due to Richtarik et al. (2021) known as EF21. Our algorithm, for which we coin the name EF21-SGDM, improves the communication and sample complexities of previous error feedback algorithms under standard smoothness and bounded variance assumptions, and does not require any further strong assumptions such as bounded gradient dissimilarity. Moreover, we propose a double momentum version of our method that improves the complexities even further. Our proof seems to be novel even when compression is removed from the method, and as such, our proof technique is of independent interest in the study of nonconvex stochastic optimization enriched with Polyak's momentum.

## 1 Introduction

Since the practical utility of modern machine learning models crucially depends on our ability to train them on large quantities of training data, it is imperative to perform the training in a distributed storage and compute environment. In federated learning (FL) (Konecny et al., 2016; Kairouz, 2019), for example, data is naturally stored in a distributed fashion across a large number of clients (who capture and own the data in the first place), and the goal is to train a single machine learning model from the wealth of all this distributed data, in a private fashion, directly on their devices.

**1.1 Formalism.** We consider the problem of collaborative training of a single model by several clients in a data-parallel fashion. In particular, we aim to solve the _distributed nonconvex stochastic optimization problem_

\[_{x^{d}}[f(x):=_{i=1}^{n}f_{i}(x) ], f_{i}(x):=_{_{i}_{i}}[f_{i}(x, _{i})], i=1,,n,\] (1)

where \(n\) is the number of clients, \(x^{d}\) represents the parameters of the model we wish to train, and \(f_{i}(x)\) is the (typically nonconvex) loss of model parameterized by the vector \(x\) on the data \(_{i}\) owned by client \(i\). Unlike most works in federated learning, we do not assume the datasets to be similar, i.e., we allow the distributions \(_{1},,_{n}\) to be arbitrarily different.

We are interested in the fundamental problem of finding an approximately stationary point of \(f\) in expectation, i.e., we wish to find a (possibly random) vector \(^{d}\) such that \([\| f()\|]\). In order to solve this problem, we assume that the \(n\) clients communicate via an orchestrating server. Typically, the role of the server is to first perform aggregation of the messages obtained from the workers, and to subsequently broadcast the aggregated information back to the workers. Following an implicit assumption made in virtually all theoretically-focused papers on communication-efficient training, we also assume that the speed of client-to-workers broadcast is so fast (compared to speed of workers-to-client communication) that the cost associated with broadcast can be neglected2.

**1.2 Aiming for communication and computation efficiency at the same time.** In our work, we pay attention to two key aspects of efficient distributed training--_communication cost_ and _computation cost_ for finding an approximate stationary point \(\). The former refers to the number of bits that need to be communicated by the workers to the server, and the latter refers to the number of stochastic gradients that need to be sampled by each client. The rest of the paper can be summarized as follows: _We pick one of the most popular communication-efficient gradient-type methods (the \(21\) method of Richtarik et al.  - the latest variant of error feedback pioneered by Seide et al. ) and modify it in a way which provably preserves its communication complexity, but massively improves its computation/sample complexity, both theoretically and in practice._

## 2 Communication Compression, Error Feedback, and Sample Complexity

Communication compression techniques such as _quantization_[Alistarh et al., 2017, Horvath et al., 2019a] and _sparsification_[Seide et al., 2014, Beznosikov et al., 2020] are known to be immensely powerful for reducing the communication footprint of gradient-type3 methods. Arguably the most studied, versatile and practically useful class of compression mappings are _contractive_ compressors.

**Definition 1** (Contractive compressors).: _We say that a (possibly randomized) mapping \(:^{d}^{d}\) is a contractive compression operator if there exists a constant \(0< 1\) such that_

\[[\|(x)-x\|^{2}](1-) \|x\|^{2}, x^{d}.\] (2)

Inequality (2) is satisfied by a vast array of compressors considered in the literature, including numerous variants of sparsification operators [Alistarh et al., 2018, Stich et al., 2018], quantization operators [Alistarh et al., 2017, Horvath et al., 2019a], and low-rank approximation [Vogels et al., 2019, Safaryan et al., 2022] and more [Beznosikov et al., 2020, Safaryan et al., 2021]. The canonical examples are i) the \(K\) sparsifier, which preserves the \(K\) largest components of \(x\) in magnitude and sets all remaining coordinates to zero [Stich et al., 2018], and ii) the (scaled) \(K\) sparsifier, which preserves a subset of \(K\) components of \(x\) chosen uniformly at random and sets all remaining coordinates to zero [Beznosikov et al., 2020]. In both cases, (2) is satisfied with \(=}{{d}}\).

### Brief history of error-feedback

When greedy contractive compressors, such as \(K\), are used in a direct way to compress the local gradients in distributed gradient descent (GD), the resulting method may diverge exponentially, even on strongly convex quadratics [Beznosikov et al., 2020]. Empirically, instability caused by such a naive application of greedy compressors was observed much earlier, and a fix was proposed in the form of the _error feedback_ (EF) mechanism by Seide et al. , which we henceforth call \(14\) or \(14\)-\(\) (in the stochastic case).4 To the best of our knowledge, the best _sample complexity_ of \(14\)-\(\) for finding a stationary point in the distributed nonconvex setting is given by Koloskova et al. : after \((G^{-1}^{-3}+^{2}n^{-1}^{-4})\) samples5, \(14\)-\(\) finds a point \(x\) such that \([\| f(x)\|]\), where \(\) is the contraction parameter (see Definition 1). However, such an analysis has two important deficiencies. First, in the deterministic case (when exact gradients are computable by each node), the analysis only gives the suboptimal \((^{-3})\)_iteration complexity_, which is suboptimal compared to vanilla (i.e., non-compressed) gradient descent, whose iteration complexity is \((^{-2})\). Second, their analysis relies heavily on additional strong assumptions, such as the _bounded gradient_ (BG) assumption, \([\| f_{i}(x,_{i})\|^{2}] G^{2}\) for all \(x^{d}\), \(i[n]\), \(_{i}_{i}\), or the bounded gradient similarity (BGS) assumption, \(_{i=1}^{n}\| f_{i}(x)- f(x)\|^{2} G^{2}\) for all \(x^{d}\). Such assumptions are restrictive and sometimes even unrealistic. In particular, both BG and BGS might not hold even in the case of convex quadratic functions.6 Moreover, it was recently shown that nonconvex analysis of stochastic gradient methods using a BG assumption may hide an exponential dependence on the smoothness constant in the complexity (Yang et al., 2023).

In 2021, these issues were _partially_ resolved by Richtarik et al. (2021), who propose a modification of the EF mechanism, which they call EF21. They address both deficiencies of the original EF14 method: i) they removed the BG/BGS assumptions, and improved the iteration complexity to \((^{-2})\) in the full gradient regime. Subsequently, the EF21 method was modified in several directions, e.g., extended to bidirectional compression, variance reduction and proximal setup (Fatkhullin et al., 2021), generalized from contractive to three-point compressors (Richtarik et al., 2022) and adaptive compressors (Makarenko et al., 2022), modified from dual (gradient) to primal (model) compression (Gruntkowska et al., 2022) and from centralized to decentralized setting (Zhao et al., 2022). For further work, we refer to (Wang et al., 2022; Dorfman et al., 2023; Islamov et al., 2022).

### Key issue: error feedback has an unhealthy appetite for samples!

Unfortunately, the current theory of EF21 with _stochastic gradients_ has weak sample complexity guarantees. In particular, Fatkhullin et al. (2021) extended the EF21-GD method, which is the basic variant of EF21 using full gradient at the clients, to EF21-SGD, which uses a "large minibatch" of stochastic gradients instead. They obtained \((}+}{^{2} ^{4}})\) sample complexity for their method. Later, Zhao et al. (2022) improved this result slightly7 to \((}+}{^{2} ^{4}})\), shaving off one \(\) in the stochastic term. However, it is easy to notice several issues in these results, which generally feature the fundamental challenge of combining biased gradient methods with stochastic gradients.

\(\)**Mega-batches.** These works require all clients to sample "mega-batches" of stochastic gradients/datapoints in each iteration, of order \((^{-2}),\) in order to control the variance coming from stochastic gradients. In Figure 1, we find that, in fact, a batch-free (i.e., with mini-batch size \(B=1\))

Figure 1: Divergence of EF21-SGD on the quadratic function \(f(x)=\|x\|^{2},\,x^{2}\), using the Top1 compressor. See the proof of Theorem 1 for details on the construction of the noise \(\); we use \(=1,B=1\). The starting point is \(x^{0}=(0,-\,0.01)^{}\). Unlike EF21-SGD, our method EF21-SGD does not suffer from divergence and is stable near optimum. Figure 0(b) shows that when increasing the number of nodes \(n\), EF21-SGD applied with \(B=1\) does not improve, and, moreover, diverges from the optimum even faster. All experiments use constant parameters \(==}{{}}=10^{-3}\); see Figure 4 for diminishing parameters. Each method is run \(10\) times and the plot shows the median performance alongside the \(25\%\) and \(75\%\) quantiles.

version of EF21-SGD diverges even on a very simple quadratic function. We also observe a similar behavior when a small batch \(B>1\) is applied. This implies that there is a fundamental flaw in the EF21-SGD method itself, rather "just" a problem of the theoretical analysis. While mega-batch methods are common in optimization literature, smaller batches are often preferred whenever they "work". For example, the time/cost required to obtain such a large number of samples at each iteration might be unreasonably large compared to the communication time, which is already reduced using compression. Moreover, when dealing with medical data, large batches might simply be unavailable (Rieke et al., 2020). In certain applications, such as federated reinforcement learning (RL) or multi-agent RL, it is often intractable to sample more than one trajectory of the environment in order to form a gradient estimator (Mitra et al., 2023; Doan et al., 2019; Jin et al., 2022; Khodadadian et al., 2022). Further, a method using a mega-batch at each iteration effectively follows the gradient descent (GD) dynamics instead of the dynamics of (mini-batch) SGD, which may hinder the training and generalization performance of such algorithms since it is both empirically (Keskar et al., 2017; Kleinberg et al., 2018) and theoretically (Kale et al., 2021) observed that mini-batch SGD is superior to mega-batch SGD or GD in a number of machine learning tasks.

\(\)**Dependence on \(\).** The total sample complexity results derived by Fatkhullin et al. (2021); Zhao et al. (2022) suffer from poor dependence on the contraction parameter \(\). Typically, EF methods are used with the Top\(K\) sparsifier, which only communicates \(K\) largest entries in magnitude. In this case, \(=}{{d}}\), and the stochastic part of sample complexity scales quadratically with dimension.

\(\)**No improvement with \(n\).** The stochastic term in the sample complexity of EF21-SGD does _not_ improve when increasing the number of nodes. However, the opposite behavior is typically desired, and is present in several latest non-EF methods based on _unbiased_ compressors, such as MARINA (Gorbunov et al., 2021) and DASHA(Tyurin and Richtarik, 2022). We are not aware of any distributed algorithms utilizing the Top\(K\) compressor achieving linear speedup in \(n\) in the stochastic term without relying on restrictive BG or BGS assumptions.

These observations motivate our work with the following central questions:

_Can we design a batch-free distributed SGD method utilizing contractive communication compression (such as Top\(K\)) without relying on restrictive BG/BGS assumptions? Is it possible to improve over the current state-of-the-art \((^{-1}^{-2}+^{2}^{-2}^ {-4})\) sample complexity under the standard smoothness and bounded variance assumptions?_

We answer both questions in the affirmative by incorporating a momentum update into EF21-SGD.

### Mysterious effectiveness of momentum in nonconvex optimization

An immensely popular modification of SGD (and its distributed variants) is the use of _momentum_. This technique, initially inspired by the developments in convex optimization (Polyak, 1964), is often applied in machine learning for stabilizing convergence and speeding up the training. In particular, momentum is an important part of an immensely popular and empirically successful line of adaptive methods for deep learning, including ADAM(Kingma and Ba, 2015) and a plethora of variants. The classical SGD method with Polyak (i.e., heavy ball) momentum (SGD) reads:

\[x^{t+1}=x^{t}- v^{t}, v^{t+1}=(1-)v^{t}+ f(x^{t+1}, ^{t+1}),\] (3)

where \(>0\) is a learning rate and \(>0\) is the momentum parameter.

We provide a concise walk through the key theoretical developments in the analysis of SGDM in stochastic nonconvex optimization in Appendix A; and only mention the most relevant works here. The most closely related works to ours are (Mishchenko et al., 2019), (Xie et al., 2020), and (Fatkhullin et al., 2021), which analyze momentum together with communication compression. The analysis in (Mishchenko et al., 2019; Xie et al., 2020) requires BG/BGS assumption, and does not provide any theoretical improvement over the variants without momentum. Finally, the analysis of Fatkhullin et al. (2021) is only established for deterministic case, and it is unclear if its extension to stochastic case can bring any convergence improvement over EF21-SGD. Recently, several other works attempt to explain the benefit of momentum (Plattner, 2022); some consider structured nonconvex problems (Wang and Abernethy, 2021), and others focus on generalization (Jelassi and Li, 2022).

[MISSING_PAGE_FAIL:5]

_i.e., \(\| f_{i}(x)- f_{i}(y)\| L_{i}\|x-y\|\) for all \(i[n],\,x,y^{d}\). We denote \(^{2}:=_{i=1}^{n}L_{i}^{2}\). Moreover, we assume that \(f\) is lower bounded, i.e., \(f^{*}:=_{x^{d}}f(x)>-\)._

**Assumption 2** (Bounded variance (BV)).: _There exists \(>0\) such that_

\[[\| f_{i}(x,_{i})- f_{i}(x)\|^{2} ]^{2}, x^{d}, i[n],\] (4)

_where \(_{i}_{i}\) are i.i.d. random samples for each \(i[n]\)._

### A deeper dive into the issues \(\) has with stochastic gradients

As remarked before, the current analysis of \(\) in the stochastic setting requires each client to sample a mega-batch in each iteration, and it is not clear how to avoid this. In order to understand this phenomenon, we propose to step back and examine an "idealized" version of \(\)-\(\), which we call \(\)-\(\)-ideal, defined by the update rules (5a) + (5aa):

\[x^{t+1} =x^{t}- g^{t}, g^{t} =_{i=1}^{n}g_{i}^{t}\] (5a) \[\)-\(\)-ideal: \[g_{i}^{t+1} = f_{i}(x^{t+1}) +( f_{i}(x^{t+1},_{i}^{t+1})- f_{i }(x^{t+1})),\] (5aa) \[\)-\(\): \[g_{i}^{t+1} = g_{i}^{t} +( f_{i}(x^{t+1},_{i}^{t+1})- g_ {i}^{t}).\] (5ab)

Compared to \(\)-\(\), given by (5a) + (5ab), we replace the previous state \(g_{i}^{t}\) by the _exact gradient_ at the current iteration. Since \(\)-\(\) heavily relies on the approximation \(g_{i}^{t} f_{i}(x^{t+1})\), and according to the proof of convergence of \(\)-\(\), such discrepancy tends to zero as \(t\), this change can only improve the method. While we admit this is a conceptual algorithm only (it does not lead to any communication or sample complexity reduction in practice)8, it serves us well to illustrate the drawbacks of \(\)-\(\). We now establish the following negative result for \(\)-\(\)-ideal.

**Theorem 1**.: _Let \(L\), \(>0\), \(0<}{{L}}\) and \(n=1\). There exists a convex, \(L\)-smooth function \(f:^{2}\), a contractive compressor \(()\) satisfying Definition 1, and an unbiased stochastic gradient with bounded variance \(^{2}\) such that if the method \(\)-\(\)-\(\) ((5a) + (5aa)) is run with step-size \(\), then for all \(T 0\) and for all \(x^{0}\{(0,x^{0}_{(2)})^{}^{2}\,|\,x^{0}_{(2)}<0\},\) we have_

\[[\| f(x^{T})\|^{2}] \{^{2},\| f(x^{0})\|^{2}\}.\]

_Fix \(0<}{{}}\) and \(x^{0}=(0,-1)^{}\). Additionally assume that \(n 1\) and the variance of unbiased stochastic gradient is controlled by \(}}{{B}}\) for some \(B 1\). If \(B<}{60^{2}}\), then we have \([\| f(x^{T})\|]>\) for all \(T 0\)._

The above theorem implies that the method (5a), (5aa), does not converge with small batch-size (e.g., equal to one) for any fixed step-size choice.9 Moreover, in distributed setting with \(n\) nodes, a mini-batch of order \(B=(}}{{^{2}}})\) is required for convergence. Notice that this batch-size is independent of \(n\), which further implies that a linear speedup in the number of nodes \(n\) cannot be achieved for this method. While we only prove these negative results for an "idealized" version of \(\)-\(\) rather than for the method itself, in Figures 0(a) and 3(a), we empirically verify that \(\)-\(\) also suffers from a similar divergence on the same problem instance provided in the proof of Theorem 1. Additionally, Figures 0(b) and 3(b) illustrate that the situation does not improve for \(\)-\(\) when increasing \(n\).

### Momentum for avoiding mega-batches

Let us now focus on the single node setting10 and try to fix the divergence issue shown above. As we can learn from Theorem 1, the key reason for non-convergence of \(\)-\(\) is that even if the state vector \(g^{t}\) sufficiently approximates the current gradient, i.e., \(g^{t} f(x^{t+1})\), the design of this method cannot guarantee that the quantity \(\|g^{t}- f(x^{t})\|^{2}\|( f(x^{ t},^{t})- f(x^{t}))\|^{2}\) is small enough. Indeed, the last term above can be bounded by \(2(2-)^{2}\) in expectation, but it is not sufficient as formally illustrated in Theorem 1. To fix this problem, we propose to modify our "idealized" EF21-SGD-ideal method so that the compressed difference can be controlled and made arbitrarily small, which leads us to another (more advanced) conceptual algorithm,

\[& v^{t+1}= f(x^{t+1})+( f(x^{t+1},^{t+1})- f (x^{t+1})),\\ & g^{t+1}= f(x^{t+1})+(v^{t+1}- f(x^ {t+1})).\] (6)

In this method, instead of using \(v^{t+1}= f(x^{t+1},^{t+1})\) as in EF21-SGD-ideal, we introduce a correction, which allows to control variance of the difference \( f(x^{t+1},^{t+1})- f(x^{t+1})\). This allows us to derive the following convergence result. Let \(_{0}:=f(x^{0})-f^{*}\).

**Proposition 1**.: _Let Assumptions 1, 2 hold, and let \(\) satisfy Definition 1. Let \(g^{0}=0\) and the step-size in method (5a), (6) be set as \(}{{L}}\). Let \(^{T}\) be sampled uniformly at random from the iterates of the method. Then for any \(>0\) after \(T\) iterations, we have \([\| f(^{T})\|^{2}]}{ T}+4^{2}^{2}\)._

Notice that if \(=1\), then algorithm EF21-SGD-ideal (5a), (6) reduces to EF21-SGD-ideal method (5a), (5aa), and this result shows that the lower bound for the batch-size established in Theorem 1 is tight, i.e., \(B=(}}{{^{2}}})\) is necessary and sufficient11 for convergence. For \(<1\), the above theorem suggests that using a small enough parameter \(\), the variance term can be completely eliminated. This observation motivates us to design a practical variant of this method. Similarly to the design of EF21 mechanism (from EF21-SGD-ideal), we propose to do this by replacing the exact gradients \( f(x^{t+1})\) by state vectors \(v^{t}\) and \(g^{t}\) as follows:

\[ v^{t+1}&=v^{t}+( f(x^{t+1},^{t+1})-v^{t}),\\ g^{t+1}&=g^{t}+(v^{t+1}-g^{t}) \] (7)

**Theorem 2**.: _Let Assumptions 1, 2 hold, and let \(\) satisfy Definition 1. Let method (5a), (7) be run with \(g^{0}=v^{0}= f(x^{0})\), and \(^{T}\) be sampled uniformly at random from the iterates of the method. Then for all \((0,1]\) with \(_{0}=\{,\},\) we have \([\| f(^{T})\|^{2}] (}{ T}+^{2})\). The choice \(=\{1,(}{^{2}T})^{1/2}\}\), \(=_{0}\) results in \([\| f(^{T})\|^{2}] }{ T}+^{2}}{T} ^{1/2}\)._

Compared to Proposition 1, where \(\) can be made arbitrarily small, Theorem 2 suggests that there is a trade-off for the choice of \((0,1]\) in algorithm (5a), (7). The above theorem implies that in single node setting EF21-SGD has \((}+}{^{4}})\) sample complexity. For \(=1\), this result matches with the sample complexity of SGD and is known to be unimprovable under Assumptions 1 and 2 (Arjevani et al., 2019). Moreover, when \(=1\), our sample complexity matches with previous analysis of momentum methods in (Liu et al., 2020) and (Defazio, 2021). However, even in this single node (\(n=1\)), uncompressed (\(=1\)) setting our analysis is different from the previous work, in particular, our choice of momentum parameter and the Lyapunov function are different, see Appendix A and J. For \(<1\), the above result matches with sample complexity of EF14-SGD (single node setting) (Stich and Karimireddy, 2019), which was recently shown to be optimal (Huang et al., 2022) for biased compressors satisfying Definition 1. However, notice that the extension of the analysis by Stich and Karimireddy (2019) for EF14-SGD to distributed setting meets additional challenges and it is unclear whether it is possible without imposing additional BG or BGS assumptions as in (Koloskova et al., 2020). We revisit this analysis in Appendix K to showcase the difficulty of removing BG/BGS. In the following we will demonstrate the benefit of our EF21-SGD method by extending it to distributed setting without imposing any additional assumptions.

### Distributed stochastic error feedback with momentum

Now we are ready to present a distributed variant of EF21-SGDM, see Algorithm 1. Letting \(_{t}:=f(x^{t})-f^{*}\), our convergence analysis of this method relies on the monotonicity of the following Lyapunov function:

\[_{t}:=_{t}+_{i=1}^{n}\|g_{ i}^{t}-v_{i}^{t}\|^{2}+n}_{i=1}^{n} \|v_{i}^{t}- f_{i}(x^{t})\|^{2}+\| _{i=1}^{n}(v_{i}^{t}- f_{i}(x^{t}))\|^{2}.\] (8)\(\) **Convergence of \(SGDM}\) with contractive compressors.** We obtain the following result:

**Theorem 3**.: _Let Assumptions 1 and 2 hold. Let \(^{T}\) be sampled uniformly at random from the \(T\) iterates of the method. Let \(SGDM}\) (Algorithm 1) be run with a contractive compressor. For all \((0,1]\) and \(B_{} 1\), with \(\{,\},\) we have_

\[[\| f(^{T})\|^{2}] (}{ T}+^{2}}{^{2}}+ {^{2}^{2}}{}+}{n}),\] (9)

_where \(_{0}\) is given by (8). Choosing the batch size \(B_{}=}{L_{0}}\), and stepsize \(=\{,\}\), and momentum \(=\{1,(^{2}}{^{2}T})^{1/4},(}{^{2}T})^{1/3},(n}{^{2}T})^{1/2},B_{}} }{}\},\)12 we get_

\[[\| f(^{T})\|^{2}]  (_{0}}{ T}+(^{2/3}}{^{2/3}T})^{3/4}+( }{})^{2/3}+(^{2}}{nT} )^{1/2}).\]

**Remark 1**.: _Note that using large initial batch size \(B_{}>1\) is not necessary for convergence of \(SGDM}\). If we set \(B_{}=1\), the above theorem still holds by replacing \(_{0}\) with \(_{0}\)._

**Remark 2**.: _In the single node setting (\(n=1\)), the above result recovers the statement of Theorem 2 (with the same choice of parameters) since by Young's inequality \((^{2/3}}{^{2/3}T})^{3/4}}{ T}+(^{2}} {T})^{1/2}\), \((}{})^{2/3}}{ T}+(^{2}}{T}) ^{1/2}\), and \(=L\)._

\(\) **Recovering previous rates in case of full gradients.** Compared to the iteration complexity \((}G}{ c^{3}})\) of \(\), our result, summarized in

**Corollary 1**.: _If \(=0\), then \([\| f(^{T})\|]\) after \(T=(}{ c^{2}})\) iterations._

is better by an order of magnitude, and does not require the BG assumption. The result of Corollary 1 is the same as for \(\) method , and \(HB}\) method . Notice, however, that even in this deterministic setting (\(=0\)) \(SGDM}\) method is different from \(\) and \(HB}\): while the original \(\) does not use momentum, \(HB}\) method incorporates momentum on the server side to update \(x^{t}\), which is different from our Algorithm 1, where momentum is applied by each node. This iteration complexity \((})\) is optimal in both \(\) and \(\). The matching lower bound was recently established by Huang et al.  for smooth nonconvex optimization in the class of centralized, zero-respecting algorithms with contractive compressors.

\(\) **Comparison to previous work.** Our sample complexity13 in

\(\) **Comparison to previous work.** Our sample complexity13 in

\(\) **Comparison to previous work.** Our sample complexity13 in

\(\) **Comparison to previous work.** Our sample complexity13 in

\(\) **Comparison to previous work.** Our sample complexity13 in

**Corollary 2**.: \([\| f(^{T})\|]\) _after \(T=(}{^{2}}+}{ ^{2/3}c^{8/3}}+c^{3}}+}{n ^{4}})\) iterations._strictly improves over the complexity \((}}{^{2}}+} ^{2}}{n^{4}})\) of EF14-SGD by Koloskova et al. (2020), even in case when \(G<+\). Notice that it always holds that \( G\). If we assume that \(G\), our three first terms in the complexity improve the first term from Koloskova et al. (2020) by the factor of \(}{{}}\), \((}{{}}\!/\!)^{1/3}\), or \(^{1/2}\). Compared to the BEER algorithm of Zhao et al. (2022), with sample complexity \((}}{}+} ^{2}}{^{2}^{4}})\), the result of Corollary 2 is strictly better in terms of \(\), \(n\), and the smoothness constants.14 In addition, we remove the large batch requirement for convergence compared to (Fatkullin et al., 2021; Zhao et al., 2022). Moreover, notice that Corollary 2 implies that EF21-SGD achieves asymptotically optimal sample complexity \((}{n^{4}})\) in the regime \( 0\).

### Further improvement using _double_ momentum!

Unfortunately, in the non-asymptotic regime, our sample complexity does not match with the lower bound in all problem parameters simultanously due to the middle term \(}{^{2/3}^{8/3}}+^{3}}\), which can potentially dominate over \(}{n^{4}}\) term for large enough \(n\) and \(\), and small enough \(\) and \(\). We propose a _double-momentum_ method, which can further improve the middle term in the sample complexity of EF21-SGD. We replace the momentum estimator \(v_{i}^{t}\) in line 6 of Algorithm 1 by the following two-step momentum update

\[ v_{i}^{t+1}=(1-)v_{i}^{t}+ f_{i}(x^ {t+1},_{i}^{t+1}), u_{i}^{t+1}=(1-)u_{i}^{t}+ v_{i}^{t+1}.\] (10)

We formally present this method in Algorithm 3 in Appendix G. Compared to EF21-SGDM (Algorithm 1), the only change is that instead of compressing \(v_{i}^{t}-g_{i}^{t}\), in EF21-SGD2M, we compress \(u_{i}^{t}-g_{i}^{t}\), where \(u_{i}\) is a two step (double) momentum estimator. The intuition behind this modification is that a double momentum estimator \(u_{i}^{t}\) has richer "memory" of the past gradients compared to \(v_{i}^{t}\). Notice that for each node, EF21-SGD2M requires to save \(3\) vectors (\(v_{i}^{t}\), \(u_{i}^{t}\), \(g_{i}^{t}\)) instead of \(2\) in EF21-SGDM (\(v_{i}^{t}\), \(g_{i}^{t}\)) and EF14-SGD (\(e_{i}^{t}\), \(g_{i}^{t}\)).15 When interacting with biased compression operator \(()\), such effect becomes crucial in improving the sample complexity. For EF21-SGD2M, we derive

**Corollary 3**.: _Let \(v_{i}^{t}\) in Algorithm 1 be replaced by \(u_{i}^{t}\) given by (10) (Algorithm 3 in Appendix G). Then with appropriate choice of \(\) and \(\) (given in Theorem 5), we have \([\| f(^{T})\|]\) after \(T=(_{9}}{^{2}}+^{2/3}}{^{7/3}^{8/3}}+ ^{2}}{n^{4}})\) iterations._

## 4 Experiments

We consider a nonconvex logistic regression problem: \(f_{i}(x_{1},,x_{c})=-_{j=1}^{m}((a_{ij}^{}x_ {y_{ij}})/_{y=1}^{c}(a_{ij}^{}x_{y}))\) with a nonconvex regularizer \(h(x_{1},,x_{c})=_{y=1}^{c}_{k=1}^{l}[x_{y}]_{k}^{2}/(1+[x_ {y}]_{k}^{2})\) with \(=10^{-3},\) where \(x_{1},,x_{c}^{l}\), \([]_{k}\) is an indexing operation of a vector, \(c 2\) is the number of classes, \(l\) is the number of features, \(m\) is the size of a dataset, \(a_{ij}^{l}\) and \(y_{ij}\{1,,c\}\) are features and labels. The datasets used are _MNIST_ (with \(l=784\), \(m=60\,000\), \(c=10\)) and _real-sim_ (with \(l=20\,958\), \(m=72\,309\), \(c=2\)) (LeCun et al., 2010; Chang and Lin, 2011). The dimension of the problem is \(d=(l+1)c\), i.e., \(d=7\,850\) for _MNIST_ and \(d=41\,918\) for _real-sim_. In each experiment, we show relations between the total number of transmitted coordinates and gradient/function values. The stochastic gradients in each algorithm are replaced by a mini-batch estimator \(_{j=1}^{B} f_{i}(x,_{ij})\) with the same \(B 1\) in each plot. Notice that all methods (except for NEOLITHIC)16 calculate the same number of samples at each communication round, thus the dependence on the number of samples used will be qualitatively the same. In all algorithms, the step sizes are fine-tuned from a set \(\{2^{k}\,|\,k[-20,20]\}\) and the Top\(K\) compressor is used to compress information from the nodes to the master. For EF21-SGDM, we fix momentum parameter \(=0.1\) in all experiments. Prior to that, we tuned \(\{0.01,0.1\}\) on the independent dataset _w8a_ (with \(l=300\), \(m=49\,749\), \(c=2\)). We omit BEER method from the plots since it showed worse performance than EF21-SGD in all runs.

### Experiment 1: increasing batch-size

In this experiment, we use _MNIST_ dataset and fix the number of transmitted coordinates to \(K=10\) (thus \(}{{d}} 10^{-3}\)), and set \(n=10\). Figure 2 shows convergence plots for \(B\{1\),\(32\),\(128\}\). EF21-SGDM and its double momentum version EF21-SGD2M have fast convergence and show a significant improvement when increasing batch-size compared to EF14-SGD. In contrast, EF21-SGD suffers from poor performance for small \(B\), which confirms our observations in previous sections. NEOLITHHC has order times slower convergence rate to the fact that it sends \(}{{K}}\) compressed vectors in each iteration, while other methods send only one.

### Experiment 2: improving convergence with \(n\)

This experiment uses _real-sim_ dataset, \(K=100\) (thus \(}{{d}} 2 10^{-3}\)), and with \(B=128 m\). We vary the number of nodes within \(n\{1,10,100\}\), see Figure 3. In this case, EF21-SGDM and EF21-SGD2M have much faster convergence compared to other methods for all \(n\). Moreover, the proposed algorithms show a significant improvement when \(n\) increases. We also observe that on this task, EF21-SGD2M performs slightly worse than EF21-SGDM for \(n=10,100\), but it is still much faster than other other methods.

In Section C, we present extra simulations with different parameters for above experiments. Additionally, we inlclude experiemnts on simple quadratic problems and perform training of larger image recognition models. In all cases, EF21-SGDM and EF21-SGD2M outperform other algorithms.