# Template-free Articulated Gaussian Splatting for Real-time Reposable Dynamic View Synthesis

Diwen Wan\({}^{1}\)   Yuxiang Wang\({}^{1}\)   Ruijie Lu\({}^{1}\)   Gang Zeng\({}^{1}\)

\({}^{1}\)National Key Laboratory of General Artificial Intelligence,

School of IST, Peking University, China

{wan,yuxiang123}@stu.pku.edu.cn  {jason_lu,zeng}@pku.edu.cn

###### Abstract

While novel view synthesis for dynamic scenes has made significant progress, capturing skeleton models of objects and re-posing them remains a challenging task. To tackle this problem, in this paper, we propose a novel approach to automatically discover the associated skeleton model for dynamic objects from videos without the need for object-specific templates. Our approach utilizes 3D Gaussian Splatting and superpoints to reconstruct dynamic objects. Treating superpoints as rigid parts, we can discover the underlying skeleton model through intuitive cues and optimize it using the kinematic model. Besides, an adaptive control strategy is applied to avoid the emergence of redundant superpoints. Extensive experiments demonstrate the effectiveness and efficiency of our method in obtaining re-posable 3D objects. Not only can our approach achieve excellent visual fidelity, but it also allows for the real-time rendering of high-resolution images. Please visit our project page for more results: https://dnvtmf.github.io/SK_GS/.

## 1 Introduction

Novel view synthesis for 3D scenes is important for many domains including virtual/augmented/mixed reality, game or movie productions. In recent years, Neural Radiance Fields (NeRF)  have witnessed significant advances in both static and dynamic scenes. Among them, 3D Gaussian Splatting (3D-GS)  proposed a novel point-based representation, and is capable of real-time rendering while ensuring the quality of generated images, bringing new insights to more complex task scenarios.

Although visually compelling results and fast rendering speed have been achieved in reconstructing a dynamic scene, current methods mainly focus on replaying the motion in the video, which means it just renders novel view images within the given time range, making it hard to explicitly repose or control the movement of individual objects in the scene. For some specific categories such as the human body or human head, one main approach is to leverage the category-specific prior knowledge such as templates to support the manipulation of reconstructed objects. However, it is hard for these methods to generalize to large-scale in-the-wild scenes or human-made articulated objects.

Some template-free methods attempt to address these challenges by building reposable models from videos. Watch-It-Move (WIM)  leverages ellipsoids, an explicit representation, to coarsely model 3D objects, and then estimate the residual by a neural network. The underlying intuition is that one or more ellipsoids can represent a functional part. By observing the motion of parts from multi-view videos, WIM can learn both the appearance and structure of articulated objects. However, the reconstruction results of WIM are of low visual quality and the training and rendering speed is slow. Apart from WIM, Articulated Point NeRF (AP-NeRF)  samples feature point cloud from a pre-trained dynamic NeRF model (TiNeuVox ) and initializes the skeleton tree using the medial axis transform algorithm. By combining linear blend skinning (LBS) and point-based rendering ,AP-NeRF jointly optimizes dynamic NeRF and skeletal model from videos. Compared to WIM, AP-NeRF achieves higher visual fidelity while significantly reducing the training time. However, AP-NeRF cannot achieve real-time rendering, which is still far from practical application.

In this paper, we target class-agnostic novel view synthesis of reposable models without the need for a template or pose annotations, while achieving real-time rendering. To enable fast rendering speed, we opt to represent the 3D object as 3D Gaussian Splatting. To be specific, we first reconstruct the 3D dynamic model using 3D Gaussians and superpoints, where each superpoint binds Gaussians with similar motions together. These superpoints will later be treated as the parts of an object. Afterward, a skeleton model is discovered leveraging some intuitive cues under the guidance of superpoint motions from the video. Finally, we jointly optimize the skeleton model and pose parameters to match the motions of the training videos. During the optimization process of object reconstruction, we will inevitably generate a lot of redundant superpoints to fit the complex motion. To simplify the skeleton model and avoid overfitting, we employ an adaptive control strategy and regularization losses to reduce the number of superpoints. Our contributions can be summarized as follows:

* We propose a novel method based on 3D Gaussians and superpoints for modeling appearance, skeleton model, and motion of articulated dynamic objects from videos. Our approach can automatically discover the skeleton model without any category-specific prior knowledge.
* We effectively learn and control superpoints by employing an adaptive control strategy and regularization losses.
* We demonstrate excellent novel view synthesis quality while achieving real-time rendering on various datasets.

## 2 Related Works

### Static and Dynamic Neural Radiance Fields

In recent years, we have witnessed significant progress in the field of novel view synthesis empowered by Neural Radiance Fields. While vanilla NeRF  manages to synthesize photorealistic images for any viewpoint using MLPs, subsequent works have explored various representations such as 4D tensors , hash encodings , or other well-designed data structures [9; 10] to improve rendering quality and speed. More recently, a novel framework 3D Gaussian Splatting  has received widespread attention for its ability to synthesize high-fidelity images for complex scenes in real-time.

Meanwhile, many research works challenge the hypothesis of a static scene in NeRFs and attempt to synthesize novel-view images of a dynamic scene at an arbitrary time from a 2D video, which is a more challenging task since the correspondence between different frames is non-trivial. One line of research works [11; 12; 13] directly represents the dynamic scene with an additional time dimension or a time-dependent interpolation in a latent space. Another line of work [14; 15; 16; 17; 18] represents the dynamic scene as a static canonical 3D scene along with its deformation fields. While one main bottleneck of synthesizing a dynamic scene is speed, some works [19; 20; 21; 22] propose to extend 3D Gaussian Splatting into 4D to mitigate the problem. Though being able to recover a high-fidelity scene, this method cannot directly support editing and reposing objects within it. In this work, we leverage 3D Gaussian Splatting as the representation for faster rendering speed.

### Object Reposing

It's impractical to directly repose the deformation fields of dynamic NeRFs due to the complexity of high-dimension. Therefore, utilizing parametric templates based on object priors to represent deformation is adopted in many research works. The classes of parametric templates range from human faces [23; 24], and bodies [25; 26] to non-human objects like animals . With the help of skeleton-based LBS and 3D or 2D annotations, these parametric templates are capable of representing articulate human heads [28; 29; 30; 31; 32; 33] and bodies [34; 35; 36; 37; 38; 39; 40; 41]. Though these template-based reposing methods can synthesize high-fidelity images, they are restricted to certain object classes and mainly deal with rigid motions, not to mention the time-consuming process of annotations.

To alleviate the excessive reliance on domain-specific skeletal models, methods based on retrieval from database  or adaptation from a generic graph [43; 44] are adopted. However, these methods are still of relatively low flexibility and diversity. Another line of work attempts to learn a more general template-free object representation by 3D shape recovery [3; 4; 45]. WIM  proposes to jointly learn a surface representation and LBS model for articulation without any supervision or prior knowledge of the structure. However, the reposing images are of low visual quality and the required training time is considerably long. AP-NeRF  achieves a much faster training speed by leveraging a point-based NeRF representation, but cannot support real-time rendering as well.

## 3 Methods

Our goal is to reconstruct a reposable articulated object with real-time rendering speed from videos. The pipeline of proposed method is illustrated in Fig. 1. We represent the appearance of the articulated object as 3D Gaussians in the canonical space while aggregating 3D Gaussians with similar motion into superpoints, which can be treated as rigid parts. It is noteworthy that we apply a time-variant 6 DoF transformation matrix to model the motion of the object. Based on these superpoints, we leverage several intuitive observations to guide the discovery of the skeleton model, which includes both joints and skeletons. Since the observations hold for most objects, our method does not require a category-specific template or pose annotations. To reduce the redundant superpoint, we propose an adaptive control strategy to densify, prune, and merge superpoints during the training process.

### Preliminaries: 3D Gaussian Splitting

3D Gaussian Splitting (3D-GS)  use a set of 3D Gaussians to represent a 3D scene. Each Gaussian \(G_{i}\): \(\{_{i},_{i},_{i},_{i},_{i}\}\) is associated with a position \(_{i}\), a rotation matrix \(_{i}\) which is parameterized by a quaternion \(_{i}\), a scaling matrix \(_{i}\) which is parameterized by a 3D vector \(_{i}\), opacity \(_{i}\) and spherical harmonics (SH) coefficients \(_{i}\). Therefore, the anisotropic 3D covariance matrix of Gaussian \(G_{i}\) is defined as \(_{i}=_{i}_{i}_{i}^{}_{ i}^{}\), which is positive semi-definite matrix.

To render images, 3D-GS employs EWA Splatting algorithm  to project a 3D Gaussian with center \(_{i}\) and covariance \(_{i}\) to 2D image space, and the projection can be approximated as a 2D Gaussian with center \(_{i}^{}\) and covariance \(_{i}^{}\). Let \(\), \(\) be the viewing transformation and projection matrix, \(_{i}^{}\) and \(_{i}^{}\) are computed as

\[_{i}^{}=(_{i})/(_ {i})_{z},_{i}^{}=_{i} ^{}^{},\] (1)

Figure 1: The pipeline of proposed approach. Our approach follows a two-stage training strategy. In the first stage (_i.e_. _dynamic_ stage), we learn the 3D Gaussians and superpoints to reconstruct the appearance. Each superpoint is associated with a rigid part, and the adaptive control strategy is used to control the count. After finishing the training of _dynamic_ stage, we can discover the skeleton model based on superpoints. After we finish the second stage (_i.e_., _kinematic_ stage), we can obtain an articulated model based on the kinematic model.

where \(\) is the Jacobian of the projective transformation. Therefore, the final opacity of a 3D Gaussian at pixel coordinate \(\) is

\[_{i}=_{i}(-(-_{i}^{})^{}_{i}^{{}^{}-1}(-_{i}^{}))\] (2)

After sorting Gaussians by depth, the color at \(\) can be computed by volume rendering:

\[I=_{i=1}^{N}(_{i}_{i}_{j=1}^{i-1}(1-_{j}))\] (3)

where RGB color \(_{i}\) is evaluated by SH with coefficients \(_{i}\) and view direction.

Given multi-view images with known camera poses, 3D-GS optimizes a static 3D scene by minimizing the following loss function:

\[_{rgb}=(1-)_{1}(I,I_{gt})+_{ }(I,I_{gt})\] (4)

where \(=0.2\), and \(I_{gt}\) is the ground truth. Besides, 3D-GS is initialized from from random point cloud or SfM sparse point cloud, and an adaptive density adjustment strategy is applied to control the number of Gaussians.

### Dynamic Stage

To reconstruct an articulated object, we build the deformation based on superpoints and the linear blend skinning (LBS) , while the canonical model is represented by 3D-GS.

The superpoints \(=\{_{j}^{3}\}_{j=1}^{M}\) are associated with a set of 3D Gaussians, and can be used to represent the object's rigid parts. For timestamp \(t\), we directly use deformable field \(\) to learn time-variant 6 DoF transformation \([_{j}^{t},_{j}^{t}]()\) of superpoint \(_{j}\) as:

\[:(_{j},t)(_{j}^{t},_{j}^{t}),\] (5)

where \(_{j}^{t}()\) is the local rotation matrix and \(_{j}^{t}^{3}\) is the translation vector. Then, LBS is employed to derive the motion of each Gaussian by interpolating the transformations for their neighboring superpoints:

\[_{i}^{t}=_{j_{i}}w_{ij}(_{j}^{t}_{ i}+_{j}^{t}),_{i}^{t}=(_{j_{i}}w_{ij}_{j} ^{t})_{i},\] (6)

where \(_{j}^{t}^{4}\) is the quaternion representation for matrix \(_{j}^{t}\), and \(\) is the production of quaternions. \(_{i}\) denotes the \(K\)-nearest superpoints of Gaussian \(G_{i}\). \(w_{ij}\) is the LBS weights between Gaussian \(G_{i}\) and superpoint \(_{j}\), which can be computed as:

\[w_{ij}=_{ij})}{_{k_{i}}(_ {ik})},\] (7)

where \(^{N M}\) is a learnable parameter.

While keeping other attributes (_i.e._, \(_{i},_{i},_{i}\)) of Gaussians the same as canonical space, we can render the image at timestamp \(t\) following Eq. 3.

### Discovery of Skeleton Model

Treating each superpoint as a rigid part of the articulated object, we can discover the skeleton model (_i.e._, the 3D joints and the connection between joints) based on the motion of superpoints. Similar to WIM , there are some observations to help us discover the underlying skeleton. First, if there is a joint between two superpoints \(_{a}\) and \(_{b}\), the position of \(_{a}\) is more likely close to the position of \(_{b}\). Second, when the relative pose between two parts changes, the joint between the two parts is relatively unchanged. Lastly, two connected parts can be merged if they maintain the same relative pose throughout the whole sequence.

Let \(_{ab}^{3}\) be the position of underlying joint between superpoints \(_{a}\) and \(_{b}\), and \(_{ab}^{t}()\) is the relative rotation matrix between two superpoints at time \(t\). The relative transform between \(_{a}\) and \(_{b}\) can be either represented by the global transform or the rotation of the joint, that is:

\[_{r}^{t}&_{r}^{t}\\ &1=_{b}^{t}&_{b}^{t}\\ &1^{-1}_{a}^{t}&_{a}^{t} \\ &1=_{ab}^{t}&_{ab}-_{ab}^{t}_{ab}\\ &1\] (8)where \(_{r}^{t}=(_{b}^{t})^{-1}_{a}^{t}=_{ab}^{t} ()\) and \(_{r}^{t}^{3}\) are the relative rotation matrix and translation vector between two superpoints respectively. Considering two joints \(_{ab}\) and \(_{ba}\) between \(_{a}\) and \(_{b}\) should be the same, we compute following distance \(d_{ab}\) for every superpoint pair \((a,b)\):

\[d_{ab}=_{t}\|_{r}-(_{ab}-_{ab}^{t}_{ab})\|_{2}^ {2}+_{d}\|_{ab}-_{ba}\|_{2}^{2}\] (9)

where \(_{d}=1\) is the hyper-parameter. To prevent the distance from changing too quickly, we smooth it across training iterations:

\[_{ab}(+1)=(1-)_{ab}()+ d_{ab} (),\] (10)

where \(=0.1\) is the momentum, and \(\) is the training iteration.

Similar to WIM , we discover the structure \(\) of joints based on the distance \(_{ab}\) by the minimum spanning tree algorithm. We first select all pairs \((a,b)\) if superpoint \(_{b}\) is \(K^{}\)-nearest neighborhood for superpoint \(_{a}\) and sort the list of \(_{ab}\) for those pairs in ascending order. We initialize \(\) as an empty set. We pick pair \((a,b)\) from the lowest distance to the highest distance, and add this pair to \(\) while there is no path between \(a\) and \(b\). After finishing the procedure, we obtain the final object structure \(\), which is an acyclic graph, _i.e._, a tree. We choose the node whose length of the longest path from itself to any other node is the shortest as the root node. If there is more than one candidate node, we randomly choose one as the root.

### Kinematic Stage

After discovering the skeleton model, we optimize the skeleton model and fine-tune 3D Gaussians by using the kinematic model. Specifically, we first predict time-variant rotations \(}_{k}()\) for each joint \(_{k}\) by using an deformable field \(\):

\[:(_{k},t)}_{k}\] (11)

Then we forward-warp the superpoint \(_{j}\) from the canonical space to the observation space of timestamp \(t\) via the kinematic model. The local transformation matrix \(}_{k}^{t}()\) of each joint \(k\) is defined by a rotation \(_{k}^{t}\) around its parent joint \(_{k}\). Consequently, the final transformation of each superpoint \(_{j}\) can be computed as a linear combination of bone transformation:

\[_{j}^{t}=_{j}^{t}&_{j}^{t}\\ &1=_{root}^{t}_{k_{j}} }_{k}^{t}}_{k}^{t}=}_{k}^{t}&_{k}-}_{k}^{t}_{k}\\ &1,\] (12)

where \(_{j}\) is the list of ancestor of superpoint \(j\) in skeleton model. \(_{root}^{t}\) is global transformation of root. Same as Sec. 3.2, we use LBS to derive the motion of each Gaussian and render images.

### Adaptive Control of Superpoints

We use the farthest point sampling algorithm to sample \(M\) Gaussians to initialize the superpoints. Simply making superpoints learnable is not enough to model complex motion patterns. More importantly, we wish to simplify the skeleton after training to ease pose editing by reducing the number of superpoints. Following 3D-GS  and SC-GS , we develop an adaptive control strategy to prune, densify, and merge superpoints.

**Prune:** To determine whether a superpoint \(_{j}\) should be pruned, we calculate its overall impact \(W_{j}=_{i}_{j}}w_{ij}\), where \(}_{j}=\{i j_{i}\}\) is the set of Gaussians whose \(K\) nearest neighbors include superpoints \(_{j}\). When \(W_{j}<_{prune}\), we prune this superpoint as it is of little contribution to the motion of 3D Gaussians.

**Densify:** Two aspects determining whether a superpoint should be split into two superpoints. On one hand, we clone a superpoint when its impact \(W_{j}\) is greater than a threshold \(_{clone}\), indicating there is a great amount of Gaussians associated with this superpoint, and cloning such superpoints helps model fine motion. On the other hand, we calculate the weighted Gaussians gradient norm of superpoint \(j\) as:

\[g_{j}=_{i}_{j}}}{_{k }_{j}}w_{kj}}\|}{_{i}}\|_{2}^{2},\] (13)where \(\) is the loss function, which demonstrated in Appendix A. We clone the superpoint \(_{j}\) if \(g_{j}\) is greater than the threshold \(_{grad}\).

**Merge:** We merge the superpoints that should belong to the same rigid part. To determine which superpoints should be merged, we first calculate the transformations \(_{j}^{t}3\) for all superpoints at all training timestamps. Then, for each pair \((a,b)\) of superpoints, we calculate the average relative transformations:

\[D_{a,b}=}_{t}\|(_{b}^{t}{}^{-1}_{a}^ {t})\|,\] (14)

where \(N_{t}\) is the number of train timestamps, \(()\) denotes the operation of converting a rigid transformation matrix to a Lie algebra. A small \(D_{a,b}\) indicates two superpoints have similar motion patterns. Therefore, we merge two superpoints \(_{a}\) and \(_{b}\) when \(D_{a,b}<_{merge}\) and \(_{b}\) is the \(K^{}\)-nearest superpoints of \(_{a}\).

## 4 Experiments

In this section, we present the evaluation of our approach, which achieves excellent view-synthesis quality and real-time rendering speed. We also evaluate the contribution of each component through an ablation study. Additionally, we demonstrate the class-agnostic reposing capability. Please refer to our project https://dnvtmf.github.io/SK_GS/ for video visualization.

### Datasets and Evaluation Metrics

To ensure fair comparison with previous work, we choose the same datasets and configurations as AP-NeRF. Specifically, we choose three multi-view video datasets. First, the _D-NeRF_ dataset is a sparse multi-view synthesis dataset, which includes 5 humanoids, 2 other articulated objects, and a multi-component scene. Each scene contains 50-200 frames. We choose 6 of 8 scenes 1 The second dataset, _Robots_,contains 7 topologically varied robots with multi-view synthetic video. We use 18 views for training and 2 views for evaluation. The third dataset, _ZJU-MoCap_, is commonly used

   Method & PSNR\(\) & SSIM\(\) & LPIPS\(\) & FPS\(\) & resolution \\  WIM  & 29.11 & 0.9664 & 0.0350 & 0.11 & \(512 512\) \\ AP-NeRF  & 32.45 & 0.9784 & 0.0202 & 0.89 & \(512 512\) \\ Ours & 34.34 & 0.9809 & 0.0187 & 137.76 & \(512 512\) \\   

Table 2: Quantitative comparison of novel view synthesis on the _Robots_ dataset.

   Method & Skeleton & PSNR\(\) & SSIM\(\) & LPIPS\(\) & FPS\(\) & resolution & Opt. Time \\  D-NeRF  & No & 30.48 & 0.9683 & 0.0450 & 1 & \(400 400\) & 20.0 hours \\ TiNeuVox-B  & No & 32.60 & 0.9783 & 0.0383 & 0.82 & \(400 400\) & 28.0 mins \\ Hexplane  & No & 29.81 & 0.9683 & 0.0400 & 1.37 & \(400 400\) & 11.5 mins \\ K-Plane hybrid  & No & 31.02 & 0.9717 & 0.0495 & 0.52 & \(400 400\) & 52.0 mins \\ 
4D-GS  & No & 34.39 & 0.9830 & 0.0190 & 141.37 & \(800 800\) & 20.0 mins \\ SP-GS  & No & 37.55 & 0.9884 & 0.0137 & 234.83 & \(800 800\) & 52.3 mins \\ D-3D-GS  & No & 40.11 & 0.9918 & 0.0120 & 42.10 & \(800 800\) & 66.0 mins \\ SC-GS  & No & 42.98 & 0.9955 & 0.0028 & 123.04 & \(400 400\) & 53.3 mins \\  WIM  & Yes & 25.21 & 0.9383 & 0.0700 & 0.16 & \(400 400\) & 11 hours \\ AP-NeRF  & Yes & 30.91 & 0.9700 & 0.0350 & 1.33 & \(400 400\) & 150. mins \\ Ours & Yes & 38.80 & 0.9870 & 0.0095 & 103.98 & \(800 800\) & 90.6 mins \\ Ours & Yes & 39.23 & 0.9890 & 0.0070 & 110.90 & \(400 400\) & 92.5 mins \\   

Table 1: Quality comparison of novel view synthesis for the _D-NeRF_ dataset.

for dynamic human reconstruction. Following WIM  and AP-NeRF , we evaluate 5 sequences with 6 training views for each sequence. We use three metrics to evaluate the image quality of the novel view, _i.e_., peak signal-to-noise ratio (PSNR), structural similarity (SSIM) , and learned perceptual image patch similarity (LPIPS) .

### Implementation Details

We implement our framework using PyTorch. The number of superpoints is initialized as 512. For both deformable field \(\) and \(\), we adopt the architecture of NeRF, _i.e_., 8-layers MLP where each layer employs 256-dimensional hidden fully connected layer and ReLU activation function. We also employ positional encoding for the input coordinates and time. For optimization, we employ the Adam optimizer and use the different learning rate decay schedules for each component: the learning rate about 3D Gaussians is the same as 3D-GS, while the learning rate of other components undergoes exponential decay, ranging from 1e-3 to 1e-5. We conducted all experiments on a single NVIDIA Tesla V100 (32GB). More implementation details are shown in Appendix A.

### Baselines

We mainly compare our method to state-of-the-art template-free articulated methods for view synthesis, _i.e_. WIM  and AP-NeRF . Besides, we also compare our method with NeRF-based and 3D-GS-based non-articulated methods. D-NeRF  extends NeRF to dynamic scenes by warping a static NeRF. TiNeuVox  improves the visual quality and training speed by using voxel grids.

Figure 2: Qualitative comparison on _D-NeRF_ datasets.

HexPlane  and K-Planes  accelerate NeRF by decomposing the space-time volume into several planes. Similar to D-NeRF, Deformable-3D-GS  extends static 3D-GS to the temporal domain. 4D-GS  accelerate Deformable-3D-GS by decomposing neural voxel encoding algorithm inspired by HexPlane. Similar to ours, SP-GS  and SC-GS  employ the superpoints/control points to reconstruct dynamic scenes. However, SP-GS and SC-GS can not extract skeleton from reconstructed model.

### Comparisons on Synthetic Dataset

In our experiments, we benchmarked our method against several baselines using the _D-NeRF_ dataset and _Robots_ datasets. The quantitative comparison results, presented in Tab. 1, demonstrate the superior performance of our approach in terms of both rendering speed and visual quality. Specifically, our method significantly outperforms WIM and AP-NeRF not only in visual quality but also in rendering speed. Compared to 3D-GS based dynamic scenes reconstruction models, our method not only have similar performance, but also discover the skeleton and can repose the object to generate a novel pose. Specifically, the rendering quality of ours is higher than 4D-GS  and SP-GS , and lower than D-3D-GS  and SC-GS . Our method also can achieve real-time rendering (>100 FPS), which is near to the rendering speed of SC-GS . Fig. 2 provides the qualitative comparisons of _D-NeRF_ dataset, which demonstrates the advantages of our method over related methods. We also provide results for _Robots_ datasets, quantitatively in Tab. 2 and qualitatively in Fig. 3. It is also clear that our approach is capable of producing high-fidelity novel views with real-time rendering speed. Per-scene results are shown in Appendix B.

### Comparison on Real-world Dataset

In Tab. 3 and Fig. 4, we compare our method to WIM and AP-NeRF in the _ZJU-MoCap_ dataset. We observe that both methods can recover the 3D shape and skeleton models. However, imperfections in the camera calibrations (see Supplement F of ), lead to lower visual quality in our results compared to WIM and AP-NeRF. With respect to rendering speed, our approach achieves up to 198.23 FPS. In stark contrast, the rendering speed of WIM and AP-NeRF is extremely slow.

Figure 4: Qualitative comparison for the _ZJU-MoCap_ dataset.

Figure 3: Qualitative comparison for the _Robots_ dataset.

### Reposing

In Fig. 5, we show our model allows free changes poses and generates animating video by smoothly interpolating the skeletons posing between user-defined poses. Video examples can be found on our project webpage.

### Ablation Study

We use superpoints to model the parts and motion of the object. The adaptive control of superpoints is the key to reducing the number of superpoints. Fig. 6 (a), (b) and (c) intuitively illustrate the impact of this control strategy. Without the control strategy, the distribution of superpoints becomes uneven, with sparse representation in the arm region, which negatively impacts motion modeling. The merge process in the control strategy significantly reduces the number of superpoints (97 vs 608), while the superpoints are more distributed in the motion area. Besides, as illustrated in Fig. 6 (d), \(_{{{arg}}}\) (in Appendix A) also plays an important role in controlling the density of superpoints. See Appendix C for more ablation studies.

   Method & PSNR\(\) & SSIM\(\) & LPIPS\(\) & FPS\(\) & resolution \\  WIM & 31.08 & 0.963 & 0.053 & 0.12 & \(512 512\) \\ AP-NeRF & 29.60 & 0.958 & 0.063 & 1.31 & \(512 512\) \\ ours & 29.11 & 0.961 & 0.063 & 198.23 & \(512 512\) \\   

Table 3: Quantitative comparison for the _ZIU-MoCap_ dataset.

Figure 5: Reposing using skeleton. Interpolation from canonical to novel pose.

Figure 6: We visualize the rendering results of (a) our full method, (b) our method without adaptive control, (c) our method without merge superpoints, (d) our method without \(_{{{arg}}}\) (see Appendix A). #sp denotes the number of superpoints. The blue points denotes superpoints.

Discuss

### Limitations

We have demonstrated that our approach can achieve real-time rendering, state-of-the-art visual quality, and straightforward repposing capability by skeleton and kinematic models. However, there are some limitations to our approach. Firstly, similar to WIM and AP-NeRF, the learned skeleton model of our approach is restricted to the kinematic motion space exhibited in the input video. Therefore, the skeleton model may have significant differences from the actual one, and extrapolation to generate arbitrary unseen poses may cause errors. Secondly, our approach has similar limitations as other 3D-GS based methods for dynamic scenes. Specifically, the datasets with inaccurate camera poses will lead to reconstruction failures, and large motion or long-term sequences can also result in failures. Lastly, the paper focuses on building the kinematic model for one articulated object. Exporting build kinematic models for multi-component objects or complex scenes that contain multiple objects remains an opportunity for future research. Additionally, extending this approach to motion capture is an interesting research direction.

### Broader Impacts

Although our approach is universal, it is also suitable for rending novel views and poses for humans. Therefore, we acknowledge that our approach can potentially be used to generate fake images or videos. We firmly oppose the use of our research for disseminating false information or damaging reputations.

### Conclusion

We have developed a new method for real-time rendering of articulated models for high-quality novel view synthesis. Without any template or annotations, our approach can reconstruct a kinematic model from multi-view videos. With state-of-the-art visual quality and real-time rendering speed, our work represents a significant step towards the development of low-cost animatable 3D objects for use in movies, games, and education.

#### Acknowledgements

This work is supported by the Sichuan Science and Technology Program (2023YFSY0008), China Tower-Peking University Joint Laboratory of Intelligent Society and Space Governance, National Natural Science Foundation of China (61632003, 61375022, 61403005), Grant SCITLAB-30001 of Intelligent Terminal Key Laboratory of SiChuan Province, Beijing Advanced Innovation Center for Intelligent Robots and Systems (2018IRS11), and PEKSenseTime Joint Laboratory of Machine Vision.