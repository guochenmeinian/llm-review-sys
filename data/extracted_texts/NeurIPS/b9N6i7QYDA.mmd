# Lightspeed Black-box Bayesian Optimization via

Local Score Matching

 Yakun Wang

University of Bristol

yakun.wang@bristol.ac.uk

&Sherman Khoo

University of Bristol

sherman.khoo@bristol.ac.uk

&Song Liu

University of Bristol

song.liu@bristol.ac.uk

###### Abstract

Bayesian Optimization (BO) is a powerful tool for tackling optimization problems involving limited black-box function evaluations. However, it suffers from high computational complexity and struggles to scale efficiently on high-dimensional problems when fitting a Gaussian process surrogate model. We address these issues by proposing a fast acquisition function maximization procedure. We leverage the fact that Probability Improvement (PI) acquisition function can be seen as a likelihood function whose score can be estimated through a simple linear regression problem called local score matching. This enables fast gradient-based optimization of the acquisition function, and a competitive BO procedure which performs similarly to that of computationally expensive neural networks.

## 1 Introduction

Black-box optimization [1; 2] seeks to identify the optimum (maximum in this paper) of a function \(g()\) whose closed-form expression and gradient information are unknown, with as little computational resources as possible. Bayesian optimization (BO, ) is particularly effective for this task. It utilizes a probabilistic surrogate, typically a Gaussian Process (GP, ), to sequentially select new evaluation points based on the mean function and quantify uncertainty through the covariance function. Although BO can achieve relatively good accuracy with only a few function evaluations, the cost of each new proposal evaluation scales as \((n^{3})\) with the number of function evaluations \(n\), which becomes the dominant factor for overall cost in the optimization process.

To reduce computational expenses, a new paradigm called _Bayesian optimization via density ratio estimation_ (BORE, [4; 16]) was introduced. BORE reformulates the improvement-based acquisition function (e.g. probability improvement, PI, expected improvement, EI) as a problem of estimating the density ratio [15; 22] between two distributions of **x** conditioned on whether the corresponding function value \(y\) exceeds a certain threshold \(\). Song et al.  further generalize BORE to likelihood-free Bayesian optimization (LFBO), where the acquisition function can be constructed in terms of more complex utility functions through variational representation. This approach significantly reduces the computational complexity from \((n^{3})\) to \((n)\). However, the acquisition function itself in both BORE and LFBO still remains demanding to optimize due to:

* The involvement of neural networks typically incurs extra cumbersome numerical optimization.
* Potential overfitting in the density ratio estimate, particularly in high dimensions.

Motivated by these challenges, we present a new approach that maximizes the acquisition function by gradient ascent. The key observation is that the PI acquisition function is a likelihood function whose gradient could be estimated using score matching. Under mild regularity conditions, we utilize a simple regression model to learn the score without training any neural network. Moreover, our method maintains the computational complexity at \((n)\).

## 2 Background

Consider the global optimization problem for a black-box function \(f\) over a compact search space \(^{d}\):

\[^{*}=*{arg\,max}_{}f(),\] (1)

where the expression and gradient of the objective function \(f:\) are unknown. We can only access \(f\) through a set of noisy observations \(_{N}=\{(x_{n},y_{n})\}_{n=1}^{N}\), where the evaluations \(y=g(f();)\) are corrupted by noise \(\).

By assigning a probabilistic surrogate model (typically analytical probabilities e.g. GP ) to the objective function \(f\), the acquisition function \((;_{N},)\) is defined as the expected value of the utility function \(u(,y,)\) under the posterior predictive \(p(y|,_{N})\) of \(f\) attained from GP regression :

\[(;):=_{p(y|,_{N})}[u( ,y,)],\] (2)

where \(\) is a hyperparameter that balances exploration and exploitation. BO selects the candidate solutions by maximizing acquisition \(_{t+1}=*{arg\,max}_{}(;)\). In GP-based BO , most acquisition functions have closed-form solutions that are easy to optimize . Nevertheless, the complexity of the GP regression is \((n^{3})\) which limits scalability to large datasets, posing significant computational challenges in high-dimensional settings.

We focus on a particular utility function, the Probability of Improvement function (PI): \(u(,y,):=(y- 0)\), where \(\) is a threshold of the function value. By definition, the acquisition function is

\[(;)=_{p(y|,_{N})}[(y- 0)]=p(y|,_{N}).\]

This acquisition function measures the probability that a new candidate point **x** yields an observation \(y\) greater than the threshold \(\). \(\) is typically selected as the current maximum observed function value: \(=_{n}y_{n}\) in our observation \(_{N}\).

## 3 Methodology

First, let \(z\) denote a binary variable:

\[z:=(y)=0,&y<\\ 1,&y,\] (3)

Then, the PI acquisition function can be rewritten as \((;)=p(z=1|)\). Without any probabilistic surrogate model to the objective function \(f\), we propose to _directly_ maximize the PI acquisition function by performing _gradient ascent_ :

\[_{k}=_{k-1}+_{} p(z=1|) |_{=_{k-1}}.\] (4)

### Matching the score locally

We model \(_{} p(z=1|)|_{=_{k-1}}\) using a vector-valued function \((z):\{0,1\}^{d}\):

\[(z):=_{0}^{d},&z= 0\\ _{1}^{d},&z=1.\] (5)

We leverage the score matching techniques [8; 18] to learn the score of the acquisition function \(_{} p(z=1|)\) through a least squares regression:

\[_{_{1}} J()=_{p(z,|_{k-1})} [\|_{} p(z|)-(z) \|^{2}],\] (6)where \(_{k-1}\) is from the previous value of **x** in the gradient ascent algorithm. The joint probability is factorized as \(p(z,|_{k-1})=p(z|)q(|_{k-1})\)1, where \(q(|_{k-1})\) is a _proposal distribution_. It samples candidate **x** around \(_{k-1}\).

Since the target score \(_{} p(z=1|)\) in the objective (6) is unknown, we cannot directly minimize \(J()\). The following theorem provides a closed-form solution of (6) and forms the basis of our algorithm:

**Theorem 1**.: _Assuming that the probability \(p(z|)\) and the proposal distribution \(q(|_{k-1})\) are differentiable. The objective (6) has the optimal solution:_

\[_{1}^{*}=_{p(|z=1,_{k-1})} [_{} p(z=1|)]=-_{p( |z=1,_{k-1})}[_{x} q(|_{k-1}) ],\] (7)

_where the second equality follows from integration by parts under mild assumptions._

Note that the minimizer \(_{1}^{*}\) in Eq. (7) is a conditional expectation of the desired score \(_{} p(z=1|)|_{=_{k-1}}\), implying that it is a biased estimator. The following result ensures that it is asymptotically unbiased given an appropriate choice of the proposal distribution.

**Corollary 1**.: _Let the proposal distribution \(q(|_{k-1})\) be a normal distribution \(N(|_{k-1},^{2}I_{d})\). Then \(_{1}^{*}=_{p(|z=1,_{k-1})}[^{- 2}(-_{k-1})]\) and_

\[_{ 0}_{1}^{*}=_{} p(z=1| )|_{=_{k-1}}.\] (8)

Corollary 1 shows the solution to the objective (6) is tractable and, when \(\) goes to zero, provides an asymptotically unbiased estimate of the desired score. The proofs of Theorem 1 and Corollary 1 can be found in Appendix A.

### Gradient ascent algorithm and finite sample estimator

We summarize our gradient ascent algorithm in Algorithm 1 and defer the full BO algorithm together with the complexity analysis to Appendix B. We now detail the computation of \(_{1}}\) using \(_{k}^{}\), i.e., paired function evaluations and their input values. According to Corollary 1, \(_{1}^{*}= p(|z=1,_{k-1})[^{-2}( -_{k-1})]d\). Once the threshold \(\) is determined, we can approximate the above integral using a Monte Carlo estimator:

\[_{1}}= (^{(m),1}-_{k-1})]}{_{m=1}^{M}z^{(m)}},&_{m= 1}^{M}z^{(m)}>0\\ 0,&_{m=1}^{M}z^{(m)}=0,\] (9)

where \(^{(m),1}\) is short for \(^{(m)}|z^{(m)}=1\). If \(z^{(m)}=0\) for all \(1 m M\) we simply set \(_{1}}=0\). Intuitively, \(_{1}}\) can be interpreted as an average of \((-_{k-1})/^{2}\) using _local information_ that is \(\{(^{(m)},z^{(m)})\}_{m=1}^{M}\) sampled within a neighborhood of \(_{k-1}\) with radius determined by \(\).

## 4 Experiments

Since we estimate the gradient of the acquisition function, we can resort to first-order optimizers such as ADAM  and RMSProp  to maximize the acquisition function.

Note that Corollary 1 suggests that \(\) should be small near the end of gradient ascent. However, since \(\) controls the local survey region as well, we propose a linear annealing schedule to balance this trade-off.

\[_{t}^{2}=_{0}^{2}(0,1-)\]

We quantitatively evaluate the performance of our method by benchmarking against some standard black-box optimization problems, fixing a function evaluation budget of \(250\). Following LFBO , we report the _immediate regret_ as a metric, which measures the distance of the current best function evaluation from the optimum function evaluation. The shaded regions represent the mean plus and minus one standard deviation across 10 different seeds. We compare against the neural-network based LFBO approach, which despite using a different acquisition function, is a similar approach to our work that does not require inference of a surrogate model. We also provide the results using a random-search algorithm as a reference. From these results, we observe that our local score matching method (SM) is broadly competitive with the LFBO method. It outperforms LFBO in higher dimensions in the Rosenbrock experiment when the neural network estimation in LFBO is likely to struggle. In this work, we focus on preliminary investigation of LSM method via synthetic datasets with mild dimensions. Experiments on high-dimensional, real-world datasets are important future works.

## 5 Discussions and Future Works

We propose an optimization scheme that maximizes the PI acquisition function bypassing the GP regression restriction. Despite the promising performance of our method, there are still some limitations. As the gradient information is myopic, our algorithm may not achieve the global optimum

Figure 1: Immediate regret for the Rosenbrock function, repeated over 10 different seeds

Figure 2: Immediate regret for the Rastrigin function, repeated over 10 different seedsof the acquisition function. In addition, our method is restricted to PI acquisition function since our score matching objective can only be used to estimate the gradient of a likelihood function. Without a likelihood expression, other acquisition functions cannot be readily estimated through our procedure. An interesting avenue for future work would be to generalize our work to a broader class of acquisition functions. Finally, our method requires additional evaluations of the blackbox function for estimating the gradient of the utility function at each gradient iteration.

We also notice that our algorithm is very similar to Covariance Matrix Adaptation - Evolutionary Strategy (CMA-ES, ). Further investigations into the relationship between these two methods could be a promising direction.