# UniControl: A Unified Diffusion Model for

Controllable Visual Generation In the Wild

 Can Qin

Shu Zhang

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Ning Yu

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Yihao Feng

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Xinyi Yang

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Yingbo Zhou

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Huan Wang

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Juan Carlos Niebles

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Caiming Xiong

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Silvio Savarese

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Stefano Ermon

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Yun Fu

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

Ran Xu

Salesforce AI Research, \({}^{}\)Northeastern University, \({}^{}\)Stanford Univeristy,

qin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,

{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,

cxiong, ssavarese, ran.xu}@salesforce.com

###### Abstract

Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions, we augment pretrained text-to-image diffusion models and introduce a task-aware HyperNet to modulate the diffusion models, enabling the adaptation to different C2I tasks simultaneously. Trained on nine unique C2I tasks, UniControl demonstrates impressive zero-shot generation abilities with unseen visual conditions. Experimental results show that UniControl often surpasses the performance of single-task-controlled methods of comparable model sizes. This control versatility positions UniControl as a significant advancement in the realm of controllable visual generation. 1

## 1 Introduction

Generative foundation models are revolutionizing the ways that humans and AI interact in natural language processing (NLP) , computer vision (CV) , audio processing (AP) , and robotic controls , to name a few. In NLP, generative foundation models such as InstructGPT or GPT-4, achieve excellent performance on a wide range of tasks, _e.g.,_ question answering, summarization, text generation, or machine translation within a _single-unified_ model. Such multi-tasking ability is one of the most appealing characteristics of generative foundation models. Furthermore, generative foundation models can also perform zero-shot or few-shot learning on unseen tasks .

For generative models in vision domains , such multi-tasking ability is less clear. Stable Diffusion Model (SDM)  has established itself as the major cornerstone for text-conditioned image generation. However, while text descriptions provide a very flexible way to control the generated images, their ability to provide pixel-level precision for spatial, structural, or geometric controls is
Figure 1: UniControl is trained with multiple tasks with a unified model, and it further demonstrates promising capability in zero-shot tasks generalization with visual example results shown above.

language and various visual conditions. Naturally, UniControl can perform multi-tasking and can encode visual conditions from different tasks into a universal representation space, seeking a common representation structure among tasks. The unified design of UniControl allows us to enjoy the advantages of improved training and inference efficiency, as well as enhanced controllable generation. On the one hand, the model size of UniControl does not significantly increase as the number of tasks scales up. On the other hand, UniControl derives advantages from the inherent connections between different visual conditions [_e.g._, 23, 24, 25]. These relationships, such as depth and segmentation mapping, leverage shared geometric information to enhance the controllable generation quality.

The unified controllable generation ability of UniControl relies on two novel designed modules, a _mixture of expert (MOE)-style adapter_ and a _task-aware HyperNet_. The MOE-style adapter can learn necessary low-level feature maps from various visual conditions, allowing UniControl to capture unique information from different visual conditions. The task-aware HyperNet, which takes the task instruction as natural language prompt inputs, and outputs a task-aware embedding. The output embeddings can be incorporated to modulate ControlNet  for task-aware visual condition controls, where each task corresponds to a particular format of visual condition. As a result, the task-aware HyperNet allows UniControl to learn meta-knowledge across various tasks, and obtain abilities to generalize to unseen tasks. As Tab. 1, UniControl has significantly compressed the model size compared with its direct baseline, _i.e._, Multi-ControlNet, by unifying nine tasks into **ONE** model.

To obtain multi-tasking and zero-shot learning abilities, we pre-train UniControl on nine distinct tasks across five categories: **1)**_edges_ (Canny, HED, User Sketch); **2)**_region-wise maps_ (Segmentation Maps, Bounding Boxes); **3)**_skeletons_ (Human Pose Skeletons); **4)**_geometric maps_ Depth, Surface Normal); **5)**_editing_ (Image Outpainting). We build **MultiGen-20M** dataset, comprising over 20 million high-quality triplets of original images, language prompts, and visual conditions for all the tasks. Then UniControl is trained for over 5,000 GPU hours on NVIDIA A100-40G hardware that is comparable with the overall training cost of different ControlNets. Moreover, UniControl exhibits a remarkable capacity for zero-shot adaptation to new tasks, highlighting its potential for deployment in real-world applications. Our contributions are summarized below:

\(\) We present UniControl, a unified model capable of handling various visual conditions for the controllable visual generation.

\(\) We collect a new dataset for multi-condition visual generation with more than 20 million image-text-condition triplets over nine distinct tasks across five categories.

\(\) We conduct extensive experiments to demonstrate that the unified model UniControl outperforms each single-task controlled image generation, thanks to learning the intrinsic relationships between different visual conditions.

\(\) UniControl shows the ability to adapt to unseen tasks in a zero-shot manner, highlighting its versatility and potential for widespread adoption in the wild.

## 2 Related Works

**Diffusion-based Generative Models.** Diffusion models were initially introduced in  that yield favorable outcomes for generating images . Improvements have been made through various training and sampling techniques such as score-based diffusion , Denoising Diffusion Probabilistic Model (DDPM) , and Denoising Diffusion Implicit Model (DDIM) , When training U-Net denoisers  with high-resolution images, researchers involve speed-up techniques including pyramids , multiple stages , or latent representations . In particular, UniControl leverages Stable Diffusion Models (SDM)  as the base model to perform multi-tasking.

**Text-to-Image Diffusion.** Diffusion models emerge to set up a cutting-edge performance in text-to-image generation tasks , by cross-attending U-Net denoiser in diffusion generators with CLIP  or T5-pretrained  text embeddings. GLIDE  is another example of a text-guided diffusion model that supports image generation and editing. UniControl and closely related

    & **Stable Diffusion** & **ControlNet** & **MoE-Adapter** & **TaskHyperNet** & **Total** \\ 
**UniControl** & 1065.7M & 361M & 0.06M & 12.7M & **1.44B** \\
**Multi-ControlNet** & 1065.7M & 361M \(\) 9 & - & - & 4.32B \\   

Table 1: Architecture and Model Size (\(\#\)Params): UniControl _vs._ Multi-ControlNetControlNet  are both built upon previous works on diffusion-based text-to-image generation .  introduces the compositional conditions to guide visual generation.

**Image-to-Image Translation.** Image-to-image (I2I) translation task was initially proposed in Pix2Pix , focusing on learning a mapping between images in different domains. Recently, diffusion-based approaches [38; 39; 21] set up the new state of the art results. Recent diffusion-based image editing methods show outstanding performances without requiring paired data, _e.g.,_ SDEdit , prompt-to-prompt , Edict . Other image editing examples include various diffusion bridges and flows [43; 44; 45; 46; 47], classifier guidance  based methods for colorization, super-resolution , inpainting , and _etc_. ControlNet  takes both visual and text conditions and achieves new state-of-the-art controllable image generation. Our proposed UniControl unifies various visual conditions of ControlNet, and is capable of performing zero-shot learning on newly unseen tasks. Concurrently, Prompt Diffusion  introduces visual prompt  from image inpainting to controllable diffusion models, which requires two additional image pairs as the in-context example for both training and inference. By contrast, UniControl takes only a single visual condition while still capable of both multi-tasking and zero-shot learning.

## 3 UniControl

In this section, we describe the training and the model design of our unified controllable diffusion model **UniControl**. Specifically, we first provide the problem setup and training objectives in Sec. 3.1, and then show the novel network design of UniControl in Sec. 3.2. Finally, we explain how to perform zero-shot image generation with the trained UniControl in Sec. 3.3.

### Training Setup

Different from the previous generative models such as Stable Diffusion Models (SDM)  or ControlNet , where the image generation conditions are _single_ language prompt, or _single_ type of visual condition such as _canny_, UniControl is required to take a wide range of visual conditions from different tasks, as well as the language prompt.

To achieve this, we reformulate the training conditions and target pairs for UniControl. Specifically, suppose we have a dataset consisting of \(K\) tasks : \(:=\{_{1}_{K}\}\), and for each task training set \(_{k}\), denote the training pairs by \(([c_{},c_{}],_{c},)\), with \(c_{}\) being the task instruction that indicates the task type, \(c_{}\) being the language prompt describing the target image, \(_{c}\) being the visual conditions, and \(\) being the target image. With the additional task instruction, UniControl can differentiate visual conditions from different tasks. A concrete training example pair is the following:

where the task is to _translate the canny edge to real images following language prompt_. With the induced training pairs \((,[c_{},c_{}],_{c})\), we define the training loss for task \(k\) following LDM :

\[^{k}():=_{z,e,t,c_{},c_{}, _{c}}[\|-_{}(z_{t},t,c_{},c_{},_{c})\|_{2}^{2}]\,,\ ([c_{},c_{}],_{c},) _{k}\,,\]

where \(t\) represents the time step, \(z_{t}\) is the noise-corrupted latent tensor at time step \(t\), \(z_{0}=E()\), and \(\) is the trainable parameters of UniControl. We also apply classifier-free guidance  to randomly drop 30% text prompts to enhance the controllability of input visual conditions. We train UniControl uninformly on the \(K\) tasks. To be more specific, we first randomly select a task \(k\) and sample a mini-match from \(_{k}\), and optimize \(\) with the calculated loss \(^{k}()\).

### Model Design

Since our unified model UniControl needs to achieve superior performance on a set of diverse tasks, it is necessary to ensure the network design enjoys the following properties: **1)** The model can overcome the misalignment of low-level features from different tasks; **2)** The model can learn meta-knowledge across tasks, and adapt to each task effectively.

The first property can ensure that UniControl can learn necessary and unique information from all tasks. For instance, if UniControl takes the segmentation map as the visual condition, the model might ignore the 3D information. As a result, the feature map learned may not be suitable for the task that takes the depth map images as visual condition. The second property would allow the model to learn the shared knowledge across tasks, as well as the differences among them.

We introduce two novel designed modules, _MOE-style adapter_ and _task-aware HyperNet_, that allows UniControl enjoys the above two properties. An overview of the model design for UniControl is in Fig. 2. We describe the detailed designs of these modules below.

MOE-Style Adapter.Inspired by the design of Mixture-of-Experts (MOEs) , we devise a group of convolution modules to serve as the adapter for UniControl to capture features of various low-level visual conditions. Precisely, the designed adapter module can be expressed as

\[_{}(_{c}^{k}):=_{i=1}^{K} (i==k)_{}^{(i)}_{}^ {(i)}(_{c}^{k})\,,\]

where \(()\) is the indicator function, \(_{c}^{k}\) is the conditioned image from task \(k\), and \(_{}^{(i)},_{}^{(i)}\) are the convolution layers of the \(i\)-th module of the adapter. We remove the weights of the original MOEs since our designed adapter is required to differentiate various visual conditions. Meanwhile, naive MOE modules can not explicitly distinguish different visual conditions when the weights are learnable. Moreover, such task-specific MOE adapters facilitate the zero-shot tasks with explicit retrieval of the adapters of highly related pre-training tasks. Besides, the number of parameters for each convolution module is approximately 70K, which is computationally efficient.

Figure 2: This figure shows our proposed UniControl method. To accommodate diverse tasks, we’ve designed a Mixture of Experts (MOE) Adapter, containing roughly \(\)70K \(\#\)params for each task, and a Task-aware HyperNet (\(\)12M \(\#\)params) to modulate \(N\) (i.e., 7) zero-conv layers. This structure allows for multi-task functionality within a singular model, significantly reducing the model size compared to an equivalent stack of single-task models, each with around 1.4B \(\#\)params.

Task-Aware HyperNet.The task-aware HyperNet modulates the zero-convolution modules of ControlNet  with the task instruction condition \(c_{}\). As shown in Figure 2, our hyperNet first projects the task instruction \(c_{}\) into task embedding with the help of CLIPText encoder. Then similar in spirit of style modulation in StyleGAN2 , we inject the task embedding into the trainable copy of ControlNet, by multiplying the task embedding to each zero-conv layer. In specific, the length of the embedding is the same as the number of input channels of the zero-conv layer, and each element scalar in the embedding is multiplied to the convolution kernel per input channel. We also show that our newly designed task-aware HyperNet can also efficiently learn from training instances and task supervision following a similar analysis as in ControlNet .

### Task Generalization Ability

With the comprehensive pretraining on the MultiGen-20M dataset, UniControl exhibits zero-shot capabilities on tasks that were not encountered during its training, suggesting that Unicontrol possesses the ability to transcend in-domain distributions for broader generalization. We demonstrate the zero-shot ability of UniControl in the following two scenarios:

Hybrid Tasks Generalization.As shown in the left side of Fig. 3, We consider two different visual conditions as the input of UniControl, a hybrid combination of segmentation maps and human skeletons, and augment specific keywords "background" and "foreground" into the text prompts. Besides, we rewrite the hybrid task instruction as a blend of instructions of the combined two tasks such as "segmentation map and human skeleton to image".

Zero-Shot New Tasks Generalization.As shown in the right side of Fig. 3, UniControl needs to generate controllable images on a newly unseen visual condition. To achieve this, estimating the task weights based on the relationship between unseen and seen pre-trained tasks is essential. The task weights can be estimated by either manual assignment or calculating the similarity score of task instructions in the embedding space. The example result in Fig. 5 (d) is generated by our manually assigned MOE weights as "depth: 0.6, seg: 0.3, canny: 0.1" for colorization. The MOE-style adapter can be linearly assembled with the estimated task weights to extract shallow features from the newly unseen visual condition.

## 4 Experiments

We empirically evaluate the effectiveness and robustness of UniControl. We conduct a series of comprehensive experiments across various conditions and tasks, utilizing diverse datasets to challenge the model's adaptability and versatility. Experimental setup, methodologies, and results analysis are provided in the subsequent sections.

### Experiment Setup

Implementation.The UniControl is illustrated as Fig. 2 with Stable Diffusion, ControlNet, MOE Adapter, and Task-aware HyperNet consisting \(\)1.5B parameters. MOE Adapter consists of parallel convolutional modules, each of which corresponds to one task. The task-aware HyperNet inputs the CLIP text embedding  of task instructions and outputs the task embeddings to modulate the weights of zero-conv kernels. We implement our model upon the ControlNet. We take the AdamW  as the optimizer based on PyTorch Lightning . The learning rate is assigned as 1\( 10^{-5}\). Our full-version UniControl model is trained on 16 Nvidia-A100 GPUs with the batch size of 4, requiring \(\) 5, 000 GPU hours. We have also applied Safety-Checker as safeguards of results.

Figure 3: Illustration of MOE’s behaviors under zero-shot scenarios. The left part shows the capacity of the MOE to generalize to hybrid task conditions, achieved through the integration of outputs from two pertinent convolution layers. The right part illustrates the ability of the MOE-style adapter to generalize to unseen tasks, facilitated by the aggregation of pre-trained tasks using estimated weights.

**Data Collection.** Since the training set of ControlNet is currently unavailable, we initiate our own data collection process from scratch and name it as **MultiGen-20M**. We use a subset of Laion-Aesthetics-V2  with aesthetics ratings over six, excluding low-resolution images smaller than 512. This yields approximately 2.8 million image-text pairs. Subsequently, we process this dataset for nine distinct tasks across five categories (edges, regions, skeletons, geometric maps, real images):

* **Canny (2.8M)**: Utilize the Canny edge detector  with randomized thresholds.
* **HED (2.8M)**: Deploy the Holistically-nested edge detection  for robust boundary determination.
* **Depth (2.8M)**: Employ the Midas  for monocular depth estimation.
* **Normal (2.8M)**: Use the depth estimation results from the depth task to estimate scene or object surface normals.
* **Segmentation (2.8M)**: Implement the Uniformer  model, pre-trained on the ADE20K  dataset, to generate segmentation maps across 150 classes.
* **Object Bounding Box (874K)**: Utilize YOLO V4  pre-trained on the COCO  dataset for bounding box labelling across 80 object classes.
* **Human Skeleton (1.3M)**: Employ the pre-trained Openpose  model to generate human skeleton labels from source images.
* **Image Outpainting (2.8M)**: Create boundary masks for source images with random masking percentages from 20% to 80%.

Further processings are carried out on HED maps using Gaussian filtering and binary thresholding to simulate user sketching. Overall, we amass over 20 million image-prompt-condition triplets. Task instructions were naturally derived from the respective conditions, with each task corresponding to a specific instruction, such as "canny edge to image" for the Canny task. We maintain a one-to-one correspondence between tasks and instructions without introducing variance to ensure stability during training. We have additionally collected a testing dataset for evaluation with 100-300 image-condition-prompt triplets for each task. The source data is collected from Laion and COCO. **We will open-source our training and testing data to contribute to the community.**

**Benchmark Models.** The most straightforward comparison for UniControl comes from task-specific ControlNet models. Six tasks overlap with those presented in ControlNet, so their official models are chosen as baselines for these tasks. For fair comparison, we re-implement the ControlNet model (single task) using our collected data. Our unified multi-task UniControl is compared against these

Figure 4: Visual comparison between official or re-implemented task-specific ControlNet and our proposed model. The example data is collected from our testing set sampled from COCO and Laion.

task-aware models for each task. We apply default sampler as DDIM  with guidance weight 9 and steps 50. All single-task models used for comparison are trained by 100K iterations and our multi-task model is trained around 900K with similar iterations for each task to ensure fairness. The efficiency and compact design of our proposed model are evident in its construction. The total size of UniControl is around 1.5B \(\#\)params and a single task ControlNet+SDM takes 1.4B. In order to achieve the same nine-task functionality, a single-task strategy would require the ensemble of a SDM with nine task-specific ControlNet models, amounting to approximately 4.3B \(\#\)params in total.

### Visual Comparison

We visually compare different tasks (Canny, HED, Depth, Normal, Segmentation, Openpose, Bounding Box, and Outpainting) in Fig. 4. Our method consistently outperforms the baseline ControlNet model. This superiority is in terms of both visual quality and alignment with conditions or prompts.

For the Canny task, the results generated by our model exhibit a higher degree of detail preservation and visual consistency. The outputs of UniControl maintain a faithful reproduction of the edge information (_i.e.,_ round table) compared to ControlNet. In the HED task, our model effectively captures the robust boundaries, leading to visually appealing images with clear and sharp edge transitions, whereas ControlNet results appear to be non-factual. Moreover, our model demonstrate a more subtle understanding of 3D geometrical guidance of depth maps and surface normals than ControlNet. The depth map conditions produce visibly more accurate outputs. In the Normal task, our model faithfully reproduces the normal surface information (_i.e.,_ ski pole), leading to more realistic

Figure 5: (a)-(b): Example results of UniControl over hybrid (unseen combination) conditions with key words “background” and ”foreground” attached in prompts. (c)-(e): Example results of UniControl on three unseen tasks (deblurring, colorization, inpainting).

Figure 6: User study between our method and official ControlNet checkpoints on six tasks. Our method outperforms ControlNet on all tasks.

and visually superior outputs. During the Segmentation, Openpose, and Object Bounding Box tasks, the produced images generated by our model are better aligned with the given conditions than that by ControlNet, ensuring a higher fidelity to the input prompts. For example, the re-implemented ControlNet-BBox misunderstands "a woman near a statue", whereas our outputs exhibit a high degree of accuracy and detail. In the Outpainting task, our model demonstrates its superiority by generating reasonable images with smooth transitions and natural-looking textures. It outperforms the ControlNet model, which produces less coherent results - "a bear missing one leg". This visual comparison underscores the strength and versatility of our approach across a diverse set of tasks.

### Quantitative Evaluation

User Study.We compare the performance of our method with both the released ControlNet model and the re-implemented single-task ControlNet on our training set. As shown in Fig. 6, our approach consistently outperforms the alternatives in all cases. In the HED-to-image generation task, our method significantly surpasses ControlNet. This superiority is even more pronounced in the depth and normal surface to image generation tasks, where users overwhelmingly favor our method, demonstrating its ability to handle complex geometric interpretations. When compared to the re-implemented single-task model, Fig. 7 reveals that our approach maintains a smaller advantage, yet it still demonstrates its benefits by effectively discerning image regions to guide content generation. Even in the challenging outpainting task, our model outperforms the baseline, highlighting its robustness and capacity to generalize.

Image Perceptual Metric.We evaluate the distance between our output and the ground truth image. As we aim to obtain a structural similar image to the ground truth image, we adopt the perceptual metric in , where a lower value indicates more similar images. As shown in Tab. 2, UniControl outperforms ControlNet on five tasks, and obtains the same image distance to ControlNet on Segmentation.

Frechet Inception Distance (FID).We've further conducted quantitative analysis with FID  to include more classic single-task-controlled methods such as GLIGEN  and T2I-adapter . With a collection of over 2,000 test samples sourced from Laion and COCO, we've assessed a wide range of tasks covering edges (Canny, HED), regions (Seg), skeletons (Pose), and geometric maps (Depth, Normal). The Tab. 3 demonstrates that our UniControl consistently surpasses the baseline methods across the majority of tasks. Notably, UniControl achieves this while maintaining a more compact and efficient architecture than its counterparts.

Ablation Study.We've conducted an ablation study, specifically focusing on the MoE-Style Adapter and TaskHyperNet in Tab. 4 with FID scores reported as the previous part. It is noticeable that the full-version UniControl (MoE-Style Adapter + TaskHyperNet) significantly outperforms the ablations which demonstrates the superiority of proposed MoE-Style Adapter and TaskHyperNet.

    & **Canny \(\)** & **HED \(\)** & **Normal \(\)** & **Depth \(\)** & **Pose \(\)** & **Segmentation \(\)** \\ 
**UniControl** & **0.546** & **0.466** & **0.623** & **0.654** & **0.741** & **0.693** \\
**ControlNet** & 0.577 & 0.582 & 0.778 & 0.700 & 0.747 & **0.693** \\   

Table 2: Image Perceptual Distance

Figure 7: User study between our multi-task model (Ours-multi) and single task model (Ours-single) on eight tasks. Our method outperforms baselines on most of tasks, and achieves big performance gains on tasks of seg-to-image and outpainting-to-image. Moreover, the p-value of voting Ours-multi in all cases is computed as **0.0028** that is statistically significant according to the criteria of \(<\)0.05.

### Zero-shot Generalization

We further showcase the surprising capabilities of our method to undertake the zero-shot challenge of hybrid conditions combination and unseen tasks generalization.

**Hybrid Tasks Combination.** This involves generating results from two distinct conditions simultaneously. Our model's zero-shot ability is tested with combinations such as depth and human skeleton or segmentation map and human skeleton. The results are shown in Fig. 5 (a)-(b). When the background is conditioned on a depth map, the model effectively portrays the intricate 3D structure of the scene, while maintaining the skeletal structure of the human subject. Similarly, when the model is presented with a combination of a segmentation map and human skeleton, the output skillfully retains the structural details of the subject, while adhering to the segmentation boundaries. These examples illustrate our model's adaptability and robustness, highlighting its ability to handle complex hybrid tasks without any prior explicit training.

**Unseen Tasks Generalization.** To evaluate the zero-shot ability to generalize to unseen tasks such as gray image colorization, image deblurring, and image inpainting, we conduct the case analysis in Fig. 5 (c)-(e). The model skillfully handles the unseen tasks, producing compelling results. This capability is deeply rooted in the shared attributes and implicit correlations among pre-training and new tasks, allowing our model to adapt seamlessly. For instance, the colorization task leverages the model's understanding of image structures from the segmentation task and depth estimation task, while deblurring and inpainting tasks benefit from the model's familiarity with edge detection and outpainting ones.

## 5 Conclusion and Discussion

We introduce UniControl, a novel unified model for incorporating a wide range of conditions into the generation process of diffusion models. UniControl has been designed to be adaptable to various tasks through the employment of two key components: a Mixture-of-Experts (MOE) style adapter and a task-aware HyperNet. The experimental results have showcased the model's robust performance and adaptability across different tasks and conditions, demonstrating its potential for handling complex text-to-image generation tasks.

**Limitation and Broader Impact.** While UniControl demonstrates impressive performance, it still inherits the limitation of diffusion-based image generation models. Specifically, it is limited by our training data, which is obtained from a subset of the Laion-Aesthetics datasets. We observe that there is a data bias in this dataset. Although we have performed keywords and image based data filtering methods, we are aware that the model may generate biased or low-fidelity output. Our model is also limited when high-quality human output is desired. UniControl could be improved if better open-source datasets are available to block the creation of biased, toxic, sexualized, or other harmful content. We hope our work can motivate researchers to develop visual generative foundation models.

  
**MoE-Adapter** & **TaskHyperNet** & **Canny \(\)** & **HED \(\)** & **Depth \(\)** & **Normal \(\)** & **Seg \(\)** & **Pose \(\)** & **Avg \(\)** \\  \(\) & \(\) & 27.2 & 29.0 & 27.6 & 28.8 & 29.1 & 30.2 & 28.7 \\ ✓ & \(\) & 24.5 & 26.1 & 23.7 & 24.8 & 26.9 & 28.3 & 25.7 \\ ✓ & ✓ & **22.9** & **23.6** & **21.3** & **23.4** & **25.5** & **27.4** & **24.0** \\   

Table 4: Ablation Study (FID)

    & **Canny \(\)** & **HED \(\)** & **Depth \(\)** & **Normal \(\)** & **Seg \(\)** & **Pose \(\)** \\ 
**GLIGEN** & 24.9 & 27.8 & 25.8 & 27.7 & - & - \\
**T2I-Adapter** & 23.6 & - & 25.4 & - & 27.1 & 28.9 \\
**ControlNet** & **22.7** & 25.1 & 25.5 & 28.4 & 26.7 & 28.8 \\
**UnControl** & 22.9 & **23.6** & **21.3** & **23.4** & **25.5** & **27.4** \\   

Table 3: Quantitative Comparison (FID)