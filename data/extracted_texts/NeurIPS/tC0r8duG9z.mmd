# On the Power of SVD in the Stochastic Block Model

Xinyu Mao

University of Southern California

xinyumao@usc.edu &Jiapeng Zhang

University of Southern California

jiapeng2@usc.edu

###### Abstract

A popular heuristic method for improving clustering results is to apply dimensionality reduction before running clustering algorithms. It has been observed that spectral-based dimensionality reduction tools, such as PCA or SVD, improve the performance of clustering algorithms in many applications. This phenomenon indicates that spectral method not only serves as a dimensionality reduction tool, but also contributes to the clustering procedure in some sense. It is an interesting question to understand the behavior of spectral steps in clustering problems.

As an initial step in this direction, this paper studies the power of vanilla-SVD algorithm in the stochastic block model (SBM). We show that, in the symmetric setting, vanilla-SVD algorithm recovers all clusters correctly. This result answers an open question posed by Van Vu (Combinatorics Probability and Computing, 2018) in the symmetric setting.

## 1 Introduction

Clustering is a fundamental task in machine learning, with applications in many fields, such as biology, data mining, and statistical physics. Given a set of objects, the goal is to partition them into _clusters_ according to their similarities. Objects and known relations can be represented in various ways. In most cases, objects are represented as vectors in \(^{d}\), forming a data set \(^{d}\); each coordinate is called a feature, whose value is directly derived from raw data.

In many applications, the number of features is very large. It has been observed that the performance of classical clustering algorithms such as K-means may be worse on high-dimensional datasets. Some people call this phenomenon _curse of dimensionality in machine learning_. A popular heuristic method to address this issue is to apply dimensionality reduction before clustering. Among tools for dimensionality reduction, it is noted in practice that spectral methods such as _principal component analysis_ (PCA) and _singular value decomposition_ (SVD) significantly improve clustering results, e.g., .

A natural question arises: _why do spectral methods help to cluster high-dimensional datasets?_ Some practitioners believe one reason is that the spectral method filters some noise from the high-dimensional data . Simultaneously, many theory works also (partially) support this explanation . With this explanation in mind, people analyzed the behavior of spectral-based algorithms with noise perturbation. Based on these analyses, many algorithms were proposed to recover clusters in probabilistic generative models. Among them, a well-studied model is the _signal-plus-noise model_.

Signal-Plus-Noise modelIn this model, we assume that each observed sample \(_{i}\) has the form \(_{i}=v_{i}+e_{i}\), where \(v_{i}\) is a ground-truth vector and \(e_{i}\) is a random noise vector. For any two sample vectors \(_{i},_{j}\), if they are from the same cluster, their corresponding ground-truth vectors are identical, i.e., \(v_{i}=v_{j}\). Signal-plus-noise model is very general; it has plentiful variants with different types of ground-truth vectors and noise distribution. In this paper, we focus on an important instance known asthe _stochastic block model_ (SBM). Though SBM is not as broad as general signal-plus-noise model, it usually serves as a _benchmark_ for clustering and provides _preliminary intuition_ about random graphs.

Stochastic block modelThe SBM is first introduced by  and is widely used as a theoretical benchmark for graph clustering algorithms. In the paper, we focus on the _symmetric version of stochastic block model_ (SSBM), described as follows. Given a set of \(n\) vertices \(V\), we uniformly partition them into \(k\) disjoint sets (clusters), denoted by \(V_{1},,V_{k}\). Based on this partition, a random (undirected) graph \(=(V,E)\) is sampled in the following way: for all pairs of vertices \(u,v V\), an edge \(\{u,v\}\) is added independently with probability \(p\), if \(u,v V_{}\) for some \(\); otherwise, an edge \(\{u,v\}\) is added independently with probability \(q\).

We usually assume that \(p>q\). The task is to recover the hidden partition \(V_{1},,V_{k}\) from the random graph \(\). We denote this model as \((V,n,k,p,q)\).

SBM as a signal-plus-noise modelThough SBM was originally designed for graph clustering, we view it as a special form of vector clustering. Namely, given the adjacency matrix of a graph \(\{0,1\}^{V V}\), the columns of \(\) form a set of \(n=|V|\) vectors. To see that SBM fits into the signal-plus-noise model, note that in the SBM, the adjacency matrix \(\{0,1\}^{V V}\) can be viewed as a fixed matrix \(G\) plus a random noise, i.e., \(=G+E\), where \(G}}{{=}}[]\) is the mean and \(E\) is a zero-mean random matrix. More precisely, in the case of SSBM,

\[G_{uv}=p&u,v V_{},\\ q&, E_{uv}=1-G_{uv}&G_{uv},\\ -G_{uv}&1-G_{uv},\]

where the random variables \(\{E_{uv}:u v\}\) are independent and \(E_{vu}=E_{uv}\) for all \(u,v V\). 1

### Motivations: Analyzing Vanilla Spectral Algorithms

Since the seminal work by McSherry , many spectral-based algorithms have been proposed and studied in the SBM  and even more general signal-plus-noise models . These algorithms are largely based on the spectral analysis of random matrices. The purpose of designing and analyzing such algorithms is twofold.

Understanding the limitation of spectral-based algorithmsSBM is specified by parameters, such as \(n,k,p,q\) in the symmetric case. Clustering is usually getting harder for larger \(k\) and smaller gap \((p-q)\). Many existing works aim to understand in which _regimes_ of these parameters it is possible to recover the hidden partition. In this regard, the state-of-the-art bound is given by Vu . Concretely,  proved that, in the symmetric setting, there is an algorithm that recovers all clusters if \(n C k(+}{p-q})^{2}\), where \(^{2}}}{{=}}\{p(1-p),q(1-q)\}\) and \(C\) is a constant.

Understanding spectral-based algorithms in practiceBesides analyzing spectral algorithms in theory, the other purpose, which is _the primary purpose of this paper_, is to explain the usefulness of such algorithms in practice. Indeed, as we mentioned before, many spectral-based algorithms, as observed in practice, can filter the noise and address the curse of dimensionality . Some representative algorithms are PCA and SVD. Furthermore, it has been observed that spectral algorithms used in practice, such as PCA or SVD, are usually _very simple_: they just project data points into some lower-dimension subspace, and no extra steps are conducted.

In stark contrast, most of the aforementioned theoretical algorithms have pre-processing or post-processing steps. For example, the idea in  is that one first applies SVD, and then runs a variant of K-means to clean up the clustering; the main algorithm in  partitions the graph into several parts and uses these parts in different ways. As noted in , these extra steps are only for the _purpose of theoretical analysis_: From the perspective of algorithm design, these extra steps appear redundant. Later on,  coined the phrase _vanilla spectral algorithms_ to describe spectral algorithms that do not include any additional steps. Both  and  conjecturedthat vanilla spectral algorithms are themselves good clustering algorithms. In practice, this is a widely-used heuristic; however, in theory, the analysis of vanilla spectral algorithms is not satisfactory due to the lack of techniques for analysis. We refer to  for a detailed discussion on barriers of the current analysis.

Why do we study vanilla algorithms?Our main focus is particularly on vanilla spectral algorithms for two reasons:

1. Vanilla spectral algorithms are the most popular in practice--no extra steps are widely used. Plus, their performance seems good enough. The lack of theoretical analysis is mostly due to technical obstacles.
2. A vanilla spectral algorithm is often simple and is not specifically designed for theoretical models such as SBM. In contrast, some complicated algorithms use extra steps which are designed for SBM. These steps made the analysis of SBM go through (as commented by ). Meanwhile, these extra steps exploit specific structures and may cause 'overfittings' on SBM, which makes these algorithms less powerful in practice.

The main purpose of this paper is to _theoretically understand_ the power of _practically successful_ vanilla spectral algorithms. To this end, we study SBM as a preliminary demonstration. We _do not_ aim to design algorithms for SBM that outperforms existing algorithms.

### Our Results

The contribution of this paper is twofold. On the one hand, we show that vanilla algorithms (alg. 1) is indeed a clustering algorithm in the SSBM for a wide range of parameters, breaking previous barrier on analyzing on only constant number of clusters. On the other hand, we provide a novel analysis on matrix perturbation with random noise. We discuss more details on this part in Section 1.4.

Recall that parameters of SBM is specified by \((V,n,k,p,q)\), where \(n=|V|\). Let \(^{2}=\{p(1-p),q(1-q)\}\). Our main result is stated below.

**Theorem 1.1**.: _There exists a constant \(C>0\) with the following property. In the model \((V,n,k,p,q)\), if \(^{2} C n/n\) and \(n C k(^{6}n+}{p-q})^{2}\), then alg. 1 recovers all clusters with probability \(1-O(n^{-1})\)._

Here we describe the vanilla-SVD algorithm in more detail. Algorithms in  share a common idea: they both use SVD-based methods to find a clear-cut vector representation of vertices. That is, every node \(v V\) is associated with a vector \((v)\), and we say a vector representation \(\) is _clear-cut_ if the following holds for some threshold \(\): if \(u,v V_{}\) for some \(\), then \(\|(u)-(v)\|/4\); otherwise, \(\|(u)-(v)\|\).

Once a clear-cut representation is found, the clustering task is easy: If the parameters \(n,k,p,q\) are all known, we can calculate \(\) and simply decide whether two vertices are in the same cluster based on their distance; in the case where \(\) is unknown, we need one more step. 2 Following , we denote by \(\) an algorithm that recovers the partition from a clear-cut representation. One natural representation is obtained by SVD as follows. Let \(\{0,1\}^{V V}\) be the adjacent matrix of the input graph, and let \(P_{_{k}}\) be the orthogonal projection matrix onto the space spanned by the first \(k\) eigenvectors of \(\). Then set \((u)}}{{=}}P_{_{k}}_{u}\), where \(_{u}\) is the column index by \(u V\). This yields alg. 1, the vanilla-SVD algorithm.

### Comparison with Existing Analysis for Vanilla Spectral Algorithms in the SBM

To the best of our knowledge, there are very few works on the analysis of vanilla spectral algorithms . All of them only apply to the case of \(k=O(1)\). In this work, we obtain the first analysis for general parameters \(n,k,p,q\), in the symmetric SBM setting.

```
1Input: adjacent matrix \(\{0,1\}^{V V}\)
2Output: a partition of \(V\) ```

1. Compute \((u)}}{{=}}P_{_{u}}_{u}\) for each \(u V\).
2. Run ClusterByDistance with representation \(\). ```

**Algorithm 1**Vanilla-SVD algorithm for graph clustering

Davis-Kahan approachesTo study spectral algorithms in signal-plus-noise models, a key step is to understand how random noise perturbs the eigenvectors of a matrix. A commonly-used technical ingredient is the Davis-Kahan \(\) theorem (or its variant). However, this type of approach faces two challenges in the SBM.

* Davis-Kahan leads to _worst-case perturbation_ bounds. For perturbations caused by random noises, such as signal-plus-noise models, Davis-Kahan \(\) theorem is sometimes _suboptimal_.
* These \(\) theorems only lead to bound on \(2\)-norm. However, in the SBM analysis, we may need \((2)\)-norm bounds. See  for more discussions.

Previous works such as  mainly followed this approach. They proposed some novel ideas to (partially) address these two challenges, but only apply to the case of \(k=O(1)\). In contrast, our approach, following the power-iteration-based analysis proposed by , completely avoids Davis-Kahan \(\) theorem and can handle the case of \(k=(1)\).

Comparison with Inspired by power iteration methods, Mukherjee and Zhang  proposed a new approach to analyze the perturbation of random matrices. The idea is to approximate the eigenvectors of a matrix by its power. In fact, this method has been widely used in practice as a fast algorithm to approximate eigenvectors. However, there are two limitations of .

* Their analysis requires a nice structure of the mean matrix, i.e., all large eigenvalues are more or less the same.
* Their algorithm is not vanilla as it has a 'centering step'. Moreover, their algorithm requires the knowledge of parameters \(p,q,k\), and particularly, the centering step alone requires the knowledge of \(q\). In comparison, we only need to know \(k\); further, we can also guess \(k\) (by checking the number of large eigenvalues) and then make alg. 1 fully parameter-free.

To overcome these limitations, we introduce a novel 'polynomial approximation + entrywise analysis' method, which makes this analysis more robust and requires less structure. More details will be discussed in Section 1.4.

Regarding parameters in Theorem 1.1The difference between the parameters in our results and those in Vu's paper is that we replaced \(\) by \(^{6}n\). If \(p<0.9\), \(\) and \(\) are equal up to constant, so our bound is essentially the same as  except for the \(^{6}n\) factor. We believe the extra \(^{6}n\) factor can be improved by future works. This term stems from the new concentration inequality we used as a black box. A refined analysis of this concentration inequality may remove this factor. Here are two example settings of parameters that satisfy the conditions in Theorem 1.1:

* \(q=(),p=(n}{})\);
* \(p-q=(1)\) and \(k=O(}{^{6}n})\).

### Proof Outline and Technical Contributions

Let \(s_{u}\) denote the size of the cluster to which \(u\) belongs. Assume for now that all \(V_{i}\)'s are of size roughly \(n/k\). Indeed, this happens with high probability inasmuch as the partition is uniformly sampled.

Our goal is to show that there exists some threshold \(>0\) such that for every \(u,v V\): if \(u,v V_{}\) for some \(\), then \(\|P_{_{u}}_{u}-P_{_{u}}_{v} \|/4\); otherwise, \(\|P_{_{u}}_{u}-P_{_{u}}_{v} \|\). Write \((u)}}{{=}}\|P_{_ {u}}_{u}-G_{u}\|\). Then \(\|\|P_{_{u}}_{u}-P_{_{u}}_{v}\|-\|G_{v}-G_{u}\|\|(u)+ (v)\). Note that \(\|G_{v}-G_{u}\|=0\) if \(u,v V_{}\) for some \(\), otherwise, \(\|G_{v}-G_{u}\|=(p-q)+s_{v}}>(p-q)\). Therefore, setting \(}\), it suffices to show that \(}\) for every \(u V\).

We decompose \((u)\) into two terms:

\[(u)\|P_{_{k}}(_{u}-G_{u})\|+ \|(P_{_{k}}-I)G_{u}\|= _{k}}E_{u}\|}_{^{*}}+ _{k}}-I)G_{u}\|}_{^{*}}.\] (1)

We shall bound the two terms from above separately. Intuitively, the noise term is small means \(P_{_{k}}\) reduces the noise, while the deviation term is small means \(P_{_{k}}\) preserves the data.

Upper bound of the noise termIt is known that \(P_{_{k}^{}}}\) (resp., \(P_{G_{k}}\)) can be write as a polynomial of \(\) (resp., \(G\)). By Weyl's inequality, the eigenvalues of \(\) are not too far from those of \(G\). Therefore, in our case, one can find a simple polynomial \(\) which only depends on \(G\), such that \(()\) (resp., \((G)\)) is a good approximation of \(P_{_{k}}\) (resp., \(P_{G_{k}}\)); this is formalized in Lemma 3.2. Then we have the following decomposition: \(\|P_{_{k}}E_{u}\| 2\|()E_{u} \| 2\|(G)E_{u}\|+2\|(()- (G))E_{u}\|,\) where the first inequality follows from Lemma 3.2, which roughly says \(()\) is a good approximation of \(P_{_{k}}\).

1. The first term, \(\|(G)E_{u}\|\), is small with high probability. To see this, we use Lemma 3.2 again: \(\|(G)E_{u}\|\|P_{G_{k}}E_{u}\|\). According to a known result (c.f. Proposition 2.4), \(\|P_{G_{k}}E_{u}\|\) is small with high probability, largely because the projection \(P_{G_{k}}\) and the vector \(E_{u}\) are independent.
2. The second term is the tricky part, and we draw on an entrywise analysis. Namely, we study every entry of \((()-(G))E_{u}\), using the new inequality from . See Lemma 3.3 for more details.

The upper bound for the noise term is encapsulated in Lemma 3.4.

Upper bound of the deviation termThe following argument is reminiscent of . Say \(u V_{}\). Note that \(G_{}=} G_{u}\) where \(_{}=}} 1_{V_{}}\) is the normalized characteristic vector of \(V_{}\) (i.e., \(1_{V_{}}(v)=1 v V_{}\)). It follows that

\[\|(P_{_{k}}-I)G\|_{2}\|(P_{_{k}}-I) \|_{2}+\|(P_{_{k}}-I)E\|_{2}\| G-\|_{2}+\|(P_{_{k}}-I)E\|_{2} 2\|E\|_{2},\]

where the second inequality holds because \(P_{_{k}}\) is the best \(k\)-rank approximation of \(\) and \((G)=k\), and in the third inequality, we use \(\|(P_{_{k}}-I)\|_{2} 1\), as \(P_{_{k}}\) is a projection matrix. Therefore,

\[\|(P_{_{k}}-I)G_{u}\|=}}\|(P_{ _{k}}-I)G_{u}\|}}\|(P_{ _{k}}-I)G\|_{2}}{}}.\] (2)

A typical result in random matrix theory (c.f. Proposition 2.3) states that with high probability, \(\|E\|_{2}=O()\). Combining Equation (2) and \(s_{u} n/k\), we get \(\|(P_{_{k}}-I)G_{u}\|=O()\). And by our assumption on \(n\), we have \(=o((p-q)n/k)=o()\).

Technical contributionThe major novelty of our analysis is using the polynomial \(\).  used a centering step to make the mean matrix nicely structured, while in our analysis, we used polynomial approximation to address this issue. Another difference is that in , the centering step appears explicitly in the algorithm. By contrast, our polynomial approximation only appears in the analysis -- the algorithm is vanilla.

As a byproduct, we developed new techniques for studying _eigenspace perturbation_, a typical topic in random matrix theory. Our high-level idea is "polynomial approximation + entrywise analysis". That is, we reduce the analysis of eigenspace perturbation to the analysis of a simple polynomial (of matrix) under perturbation. We have more tools to deal with the latter.

### Discussion and Future Directions

In this paper, we studied the behavior of vanilla-SVD in the SSBM, a benchmark signal-plus-noise model widely studied in random matrix theory. We showed that vanilla-SVD indeed filters noise in the SSBM. In fact, our analysis technique, 'polynomial approximation + entrywise analysis', is not very limited to SSBM. A direct and interesting question yet to be answered is: Can our method be extended to prove that vanilla-SVD works in the general SBM where partitions are not uniformly sampled and edges appear with different probabilities? Moreover, our method may be useful for analyzing some other realistic, probabilistic models such as the factor model -- a model which has been widely used in economics and model portfolio theory.

In the long term, it would be very interesting to understand the behavior of vanilla spectral algorithms on real data: 1) Why does it succeed in some applications? 2) How could we fix it if it has failed in other cases? A deeper understanding of vanilla spectral algorithms will provide guidelines for using them in many machine learning tasks.

## 2 Preliminaries

NotationsLet \(_{n}\) denote the \(n\)-dimensional vector whose entries are all 1's, and let \(J_{n}\) be the \(n n\) matrix whose entries are all 1's. Let \(s_{u}\) denote the size of the cluster to which \(u\) belongs. For a matrix \(A\), \(A[i]\) denotes the row of \(A\) indexed by \(i\), and \(A_{i}\) denotes the column indexed by \(i\); \(_{i}(A)\) is the \(i\)-th largest eigenvalue of \(A\); let \(P_{A_{k}}\) denote the orthogonal projection matrix onto the space spanned by the first \(k\) eigenvectors of \(A\). For a vector \(x^{n}\), \(\|x\|}}{{=}}^{2 }++x_{n}^{2}}\) denotes the Euclidean norm.

**Definition 2.1** (Matrix operator norms).: Let \(A^{n n}\). Define \(\|A\|_{2}}}{{=}}_{ \|x\|=1}\|Ax\|\) and \(\|A\|_{2}}}{{=}}_{x:\|x\|=1}\|Ax\|_{}\).

**Proposition 2.1** (e.g., ).: _For all matrices \(A,B^{n n}\), it holds that (1) \(\|A\|_{2}=_{i[n]}\|A[i]\|\); (2) \(\|AB\|_{2}\|A\|_{2 }\|B\|_{2}\)._

**Proposition 2.2** (Weyl's inequality).: _For all \(A,E^{n n}\), we have \(|_{i}(A)-_{i}(A+E)|\|E\|_{2}\)._

**Proposition 2.3** (Norm of a random matrix ).: _There is a constant \(C_{0}>0\). Let \(E\) be a symmetric matrix whose upper diagonal entries \(e_{ij}\) are independent random variables where \(e_{ij}=1-p_{ij}\) or \(-p_{ij}\) with probabilities \(p_{ij}\) and \(1-p_{ij}\) respectively, where \(p_{ij}\). Let \(^{2}:=_{ij}\{p_{ij}(1-p_{ij})\}\). If \(^{2} C_{0} n/n\), then \([\|E\|_{2} C_{0} n^{1/2}] n^{-3}\)._

**Proposition 2.4** (Projection of a random vector, lemma 2.1 in ).: _There exists a constant \(C_{1}\) such that the following holds. Let \(X=(_{1},,_{n})\) be a random vector in \(^{n}\) whose coordinates \(_{i}\) are independent random variables with mean 0 and variance at most \(^{2} 1\). Assume furthermore that the \(_{i}\) are bounded by 1 in absolute value. Let \(H\) be a subspace of dimension \(d\) and let \(_{H}\) be the length of the orthogonal projection of \(\) onto \(H\). Then \([_{H}X+C_{1}] n^{-3}\)._

**Proposition 2.5**.: _For \(a\) and \(r\), if \(|a-1|<\), then \(|a^{r}-1| 2r\)._

## 3 Analysis of Vanilla SVD Algorithm

Write \(s_{i}}}{{=}}|V_{i}|\). We say the partition \(V_{1},,V_{k}\) is _balanced_ if \((1-) s_{i}(1+), i[k].\) By Chernoff bound, the partition \(V_{1},,V_{k}\) is balanced with probability at least \(1-n^{-1}\); hence, we assume that the partition is balanced in the following argument. Since \(^{2} C n/n\), the event \(\|E\|=O()\) holds with high probability (see Proposition 2.3).

Recall the decomposition into deviation term and noise term in Equation (1). We first state our upper bound of the deviation term, which readily follows from the argument in Section 1.4, and the complete proof is in Appendix B.

**Lemma 3.1** (Upper bound of deviation term).: _Let \(C_{0}\) be the constant in Proposition 2.3. If the partition is balanced and \(n 10^{4} C_{0}^{2}^{2}}{(p-q)^{2}}\), then with probability at least \(1-n^{-3}\) we have \(\|(P_{_{k}}-I)G_{u}\| 0.04(p-q), u V.\)_

Section 3.1 and Section 3.2 lead to an upper bound of the noise term, and Section 3.3 is the proof of main theorem.

### An Approximation of \(P_{G_{k}}\) and \(P_{_{k}}\)

In order to give some intuition on the choice of \(\), we first analyze the spectrum of \(G\), and the result is summed up in Theorem 3.1.

The eigenvalues of \(G\)Note that \(G=H+q_{n}_{n}^{}\), where \(H=((p-q)J_{s_{1}}&&&\\ &(p-q)J_{s_{2}}&&\\ &&&\\ &&&(p-q)J_{s_{k}})\).

Without loss of generality, assume that \(s_{1} s_{2} s_{k}\). It is easy to see that the eigenvalues of \(H\) are \((p-q)s_{1},,(p-q)s_{k},0\). Viewing \(G\) as a rank-one perturbation of \(H\), we have the following theorem that characterizes eigenvalues of \(G\). Its proof, in Appendix C, readily follows from a theorem in , which studies eigenvalues under rank-one perturbation.

**Theorem 3.1**.: _Write \(s_{i}}}{{=}}|V_{i}|\) and assume that \(s_{1} s_{2} s_{k}\). Define \(_{i}}}{{=}}_{i}(G)-(p-q)s_{i}\), then (1) \(_{i} 0\) and \(_{i=1}^{k}_{i}=nq\); (2) \(_{1}(G) nq+(p-q)\), and hence \(_{i=2}^{k}_{i}(p-q)(s_{1}-)\)._

The choice of the polynomial \(\)Let \(}}{{=}}(p-q)\), and let \((t)\) be the quadratic polynomial such that \((_{1}(G))=()=1,(0)=0\), i.e., \((t)}}{{=}}-(G)}(t -_{1}(G))(t-)+1}}{{=}}At^{2}+Bt,\) where \(A=-(G)},B=(G)}+\). Finally, let \((t)}}{{=}}((t))^{r}\) where \(r}}{{=}} n\).

Here we give some intuition for the choice of \(\). Let \(=_{i=1}^{n}_{i}v_{i}v_{i}^{}\) be the spectral decomposition of \(\). Then \(()=_{i=1}^{n}(_{i})v_{i}v_{i}^ {},P_{_{k}}=_{i=1}^{k}{v_{i}v^{}}.\) The spectral decomposition of \(()-P_{_{k}}\) is \(()-P_{_{k}}=_{i=1}^{k}((_{i})-1)v_{i}v^{}+_{i=k+1}^{n}(_{i})v_{i}v^{}\). Hence,

\[\|()-P_{_{k}}\|_{2}=\{|( _{1})-1|,,|(_{k})-1|,| (_{k+1})|,,|(_{n})|\}.\] (3)

Recall that \(_{i}-_{i}(G)\) is bounded by Weyl's inequality. Plus, when the partition is balanced, Theorem 3.1 shows that the eigenvalues of \(G\) is nicely distributed: Except for \(_{1}(G)\), other eigenvalues are all close to \(\). Hence, our choice of \(\) makes \(\|()-P_{_{k}}\|_{2}\) small, and thus \(()\) is a good approximation of \(P_{_{k}}\). Formally, we have the following lemma.

**Lemma 3.2** (Polynomial approximation).: _Assume that the partition is balanced and \(n 10^{4} C_{0}^{2} p n}{(p-q)^{2}}\), where \(C_{0}\) is the constant in Proposition 2.3. Then with probability at least \(1-n^{-3}\), it holds that for all \(x^{n}\), and \(\|P_{G_{k}}x\|\|P_{G_{k}}x\| +\|x\|/n^{ n}\), and \(\|P_{G_{k}}x\|\|(G)x\|\|P_{G_{k}}x\|.\)_

Proof.: Let \(G=_{i=1}^{k}_{i}u_{i}u_{i}^{}\)(resp., \(=_{i=1}^{n}_{i}v_{i}v_{i}^{}\)) be the spectral decomposition of \(G\) (resp., \(\)). We shall use the following claim.

**Claim 3.1**.: _The following holds with probability \(1-n^{-3}\) (over the choice of \(E\)): for every \(i[k]\), \(|(_{i})-1|<,|(_{i}) -1|<\); and for every \(i=k+1,,n\), \(|(_{i})|<n^{- n}\)._Fix \(x^{n}\). On the one hand, \((_{i}), i[k]\), and hence

\[\|()x\|^{2}=_{i=1}^{n}(_{i})^{2} x,v_{i}^{2}_{i=1}^{k}(_{i})^{2} x,v_{i}^{2}_{i=1}^{k} x,v_{ i}^{2}=\|P_{_{k}}x\|^{2},\]

which means \(\|()x\|\|P_{_{k}}x\|\). On the other hand,

\[\|()x\|^{2}=_{i=1}^{n}(_{i})^{2} x,v_{i}^{2}_{i=1}^{k}()^{2} x,v_{i}^{2}+_{i=k+1}^{n} ^{2}}{n^{2 n}}\|P_{_{k}}x \|^{2}+}{n^{2 n}}.\]

Since \(+\), we have \(\|()x\|\|P_{_{k}}x \|+}}\). This establishes the first part.

Note that \(\|(G)x\|=^{k}(_{i})^{2} x,u_{i}^{2}}\) and we also have \((_{i}), i[k]\), and thus similar argument goes for \(G\). This finishes the proof of Lemma 3.2.

It remains to prove Claim 3.1. The claim readily follows from the choice of \(\) and the fact that \(_{i},_{i}\) are close. A complete proof can be found in Appendix C. 

### The Upper Bound of the Noise Term

According to Equation (1), in order to derive an upper bound of \(\|P_{G_{k}}E_{u}\|\), it remains to bound \(\|(()-(G))E_{u}\|\) from above. This is done by the following lemma.

**Lemma 3.3**.: _Let \(C_{0}\) be the constant in Proposition 2.3. Assume that the partition is balanced and \(n(100+C_{0})^{2} p^{12}n}{(p-q)^{2}}\). For every \(u V\), it holds that_

\[}_{E}[\|(()- (G))E_{u}\| C_{2}(^{2}n)+) ] 1-O(n^{-2}),\]

_where \(C_{2}}}{{=}}7 10^{6}\) is a constant._

Combining Lemma 3.2 and Proposition 2.4, we get an upper bound of the noise term:

**Lemma 3.4** (Upper bound of noise term).: _Let \(C_{0}\) be the constant in Proposition 2.3. Assume that \(n(100+C_{0})^{2} p^{12}n}{(p-q)^{2}}\). Then with probability at least \(1-O(n^{-1})\), we have \(\|P_{_{k}}E_{u}\| C_{3}(^{2}n+)\) for all \(u V\), where \(C_{3}\) is a constant._

The proof of Lemma 3.3 is deferred to Section 4. We use it to prove Lemma 3.4 here.

Proof of Lemma 3.4.: It follows from Lemma 3.2 that

\[\|P_{_{k}}E_{u}\| 2\|( )E_{u}\|  2\|(()-(G))E_{u} \|+2\|(G)E_{u}\|\] \[ 2\|(()-(G))E_{u} \|+3\|P_{G_{k}}E_{u}\|.\]

By Proposition 2.4, with high probability at least \(1-n^{-1}\), \(\|P_{G_{k}}E_{u}\|\) is bounded by \(+C_{1}\), where \(C_{1}\) is a universal constant. Meanwhile, by Lemma 3.3 and union bound over all \(u\), with probability at least \(1-O(n^{-1})\), \(\|(()-(G))E_{u}\| 7 10^{6}( ^{2}n+1/ n)\) for every \(u V\). Therefore, with probability \(1-O(n^{-1})\), it holds that \(\|P_{_{k}}E_{u}\| 1.4 10^{7}^{2}n+3 +3C_{1}\) for all \(u V\). Setting \(C_{3}}}{{=}}(1.4 10^{7}+3+3C_{1})\), we have the desired result.

### Putting It Together

Now we are well-equipped to prove Theorem 1.1.

Proof of Theorem 1.1.: Let \(C}}{{=}}(100+100C_{0}+100C_{3})^{2}\), where \(C_{0},C_{3}\) are the constants in Proposition 2.3 and Lemma 3.4. By our assumption on \(n\), we have \((p-q)>100C_{3}(^{6}n+)\). It is easy to verify \(n\) satisfies the conditions in Lemma 3.4 and Lemma 3.1.

Write \(}}{{=}}0.8(p-q)\). We aim to show that for every \(u,v V\): if \(u,v V_{}\) for some \(\), then \(\|P_{_{u}}_{u}-P_{_{h}}_{v} \|/4\); otherwise, \(\|P_{_{h}}_{u}-P_{_{h}}_{v} \|\). Then by calling ClusterByDistance, alg. 1 recovers all large clusters correctly.

Let \((u)}}{{=}}\|P_{_ {h}}_{u}-G_{u}\|\). According to the argument in Section 1.4, it suffices to show that \((u) 0.1(p-q)\) for all \(u V\). We further decompose \((u)\) into noise term and deviation term, i.e., \((u)(u)+(u)\), where \((u)}}{{=}}\|P_{}E_{u}\|\) and \((u)}}{{=}}\|(P_{}-I)G_{u}\|\). By Lemma 3.4 and Lemma 3.1, with probability at least \(1-O(n^{-1})\), the following hold for all \(u V\): (1) \((u) C_{3}(^{2}n+) 0.01(p-q) \); (2) \((u) 0.04(p-q)\). Therefore, with probability at least \(1-O(n^{-1})\), we indeed have \((u) 0.1(p-q), u V\). This completes the proof. 

## 4 Proof of Lemma 3.3: Entrywise Analysis

This section is dedicated to proving Lemma 3.3.

Since both \((()-(G))\) and \(E\) are symmetric, we have \(\|(()-(G))E_{u}\|\|E(( )-(G))\|_{2}.\) The high-level idea is to write \(E(()-(G))\) as a sum of matrices, where each matrix is of the form \(E^{t}SQ\) such that \(\|Q\|_{2}=O(1)\). This way, we have \(\|E^{t}SQ\|_{2}\|E^{t}S\|_{2}  O(1)\), and \(\|E^{t}S\|_{2}\) is bounded by a lemma from .

Let \(D}}{{=}}()-(G)=A(EG+GE+ E^{2})+BE\) and write \(F}}{{=}}(G),}}{{=}}()\). Then

\[()-(G)=()^{r}-(G)^{r} =(F+D)^{r}-F^{r}\] \[=D+F^{r-2}D++FD^{ r-2}}_{}}{{=}}M}+D^{r-1},\]

where the last step is a decomposition based on the first location of \(D\) in the product terms. And

\[D^{r-1}=D(D+F)^{r-1}=D^{r}+^{r-2}+D^{2}F ^{r-3}++D^{r-1}F}_{}}{{=}}M ^{}}.\]

That is, \(E(()-(G))=EM+ED^{r}+EM^{}\). We bound the three terms respectively.

Here we first list some definitions and estimations of the quantities involved.

* According to Proposition 2.3, with probability at least \(1-n^{-3}\), we have \(\|E\|_{2} C_{0}\), where \(C_{0}\) is a constant. In the following argument, we always assume this holds.
* \(}}{{=}}(p-q)n/k\). By our assumption on \(n\), we have \((100+C_{0})^{6}n\).
* \(A=-(G)},B=((G)}+),r= n\); \(_{1}(G)>\), and thus \(B,|A|}\).
* By Claim 3.1, \(\|F\|_{2} 1+,\|\|_{2}  1+\). By Proposition 2.5, \(\|F\|_{2}^{t},\|\|_{2}^{t} 2, t n\).

**Upper bound of \(\|EM\|_{2}\)** Note that \(\|EF^{t}D\|_{2}\|EF\|_{2 }\|F\|_{2}^{t-1}\|D\|_{2}\), and \(\|F\|_{2}^{t-1} 2\) for all \(t r\). Moreover, \(\|D\|_{2}|A|(2\|E\|_{2}\|G\|_ {2}+\|E\|_{2}^{2})+B\|E\|_{2} 3}{}+}+ 4}{}  4(^{6}n)^{-1}<n}\). And the following lemma gives an upper bound of \(\|EF\|_{2}\).

**Lemma 4.1**.: \(_{E}[\|EF\|_{2} 10(+ )] 1-2n^{-2}\)_._

Therefore, by union bound, we have the following holds with probability at least \(1-n^{-1}\):

\[\|EM\|_{2} r 10(+ ) 2n}+1)}{ n}.\] (4)

**Upper bound of \(\|ED^{r}\|_{2}\)** Since \(\|D\|_{2}<n}\), we have

\[\|ED^{r}\|_{2}\|E\|_{2 }\|D\|_{2}^{r}(^{3}n)^{-  n}<.\] (5)

**Lemma 4.2** (Upper bound of \(\|EM^{}\|_{2}\)).: _With probability \(1-O(n^{-2})\) (over the choice of \(E\)), we have \(\|EM^{}\|_{2} 6C_{2}^{2}n\), where \(C_{2}=10^{6}\) is a constant._

Finally, combining Equation (4), Equation (5), and the above lemma, we conclude that with probability at least \(1-O(n^{-2})\),

\[\|E(()-(G))\|_{2} +1)}{ n}++6C_{2}^{2}n 7C_{2} (^{2}n+).\]

This establishes Lemma 3.3.

Proofs of Lemma 4.1 and Lemma 4.2 are deferred to Appendix D.