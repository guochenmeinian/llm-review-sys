# Constructing Semantics-Aware Adversarial Examples with a Probabilistic Perspective

Andi Zhang

Computer Laboratory

University of Cambridge

az381@cantab.ac.uk

&Mingtian Zhang

Centre for Artificial Intelligence

University College London

m.zhang@cs.ucl.ac.uk

&Damon Wischik

Computer Laboratory

University of Cambridge

djw1005@cam.ac.uk

Corresponding Author.

###### Abstract

We propose a probabilistic perspective on adversarial examples, allowing us to embed subjective understanding of semantics as a distribution into the process of generating adversarial examples, in a principled manner. Despite significant pixel-level modifications compared to traditional adversarial attacks, our method preserves the overall semantics of the image, making the changes difficult for humans to detect. This extensive pixel-level modification enhances our method's ability to deceive classifiers designed to defend against adversarial attacks. Our empirical findings indicate that the proposed methods achieve higher success rates in circumventing adversarial defense mechanisms, while remaining difficult for human observers to detect. Code can be found at https://github.com/andiac/AdvPP.

## 1 Introduction

The purpose of generating adversarial examples is to deceive a classifier (which we refer to as victim classifier) by making minimal changes to the original data's semantic meaning. In image classification, most existing adversarial techniques ensure the preservation of adversarial example semantics by limiting their geometric distance (\(_{p}\) distance) from the original image . While these methods can deceive classifiers using minimal geometric perturbations, they are not as successful in black-box attack scenarios. Furthermore, the recent surge in adversarial defense methods  primarily targets geometric-based attacks, gradually reducing their effectiveness. In response to these challenges, unrestricted adversarial attacks are gaining traction as a potential solution. These methods employ more natural alterations, moving away from the small \(_{p}\) norm perturbations typical of traditional approaches. This shift towards unrestricted modifications offers a more practical approach to adversarial attacks.

In this paper, we introduce a probabilistic perspective for adversarial examples. Through this innovative lens, both the victim classifier and geometric constraints are regarded as distinct distributions: the victim distribution and the distance distribution. Adversarial examples naturally arise as samples from the product of these distributions.

This probabilistic perspective offers an opportunity to transform traditional geometric-based constraints into semantic constraints. Traditional geometric constraints, when viewed through thisprobabilistic lens, manifest as simple distance distributions - for instance, the squared \(_{2}\) norm constraint naturally maps to a Gaussian distribution centered at the original image. This reveals that conventional \(_{2}\) squared constraints implicitly define semantic similarity through a Gaussian distribution. Leveraging recent advances in probabilistic generative models (PGMs), we propose replacing this Gaussian distribution (distance distribution) with a fitted PGM. This substitution introduces a data-driven approach to defining semantic similarity, effectively transforming geometric constraints into semantic ones. To demonstrate the practical implementation of our conceptual framework, we present two approaches for constructing PGMs that capture semantic similarity:

* Our first method injects subjective semantic understanding by defining semantic-preserving transformations for the original image. We train a PGM to model the distribution of these transformed images, thereby learning the manifold of semantically equivalent variations.
* Our second method leverages the semantic knowledge embedded in pre-trained PGMs. By fine-tuning a PGM on the original image, we create a localized distribution that capture image-specific semantic variations, representing the semantic distance distribution around the original image.

These approaches can also be combined in practice. By employing appropriate PGMs as distance distributions, our method generates adversarial examples that appear more natural despite substantial geometric modifications (Figure 1). These adversarial examples demonstrate improved transferability in black-box scenarios and higher success rates against adversarial defenses.

## 2 Preliminaries

In this section, we present essential concepts related to adversarial attacks, energy-based models, and diffusion models. For detailed information on the training and sampling processes of these models, please refer to Appendix B.

### Adversarial Examples

The notion of adversarial examples was first introduced by . Let's assume we have a classifier \(C:^{n}\), where \(n\) represents the dimension of the input space and \(\) denotes the label space. Given an image \(x_{}^{n}\) and a target label \(y_{}\), the optimization problem for finding an adversarial instance \(x_{}\) for \(x_{}\) can be formulated as follows:

\[(x_{},x_{})C(x_{})=y_{}x_{}^{n}\]

Here, \(\) is a distance metric employed to assess the difference between the original and perturbed images. This distance metric typically relies on geometric distance, which can be represented by \(_{1}\), \(_{2}\), or \(_{}\) norms.

Figure 1: Adversarial examples generated by our method. Left: MNIST examples where we injected the subjective semantic understanding that scaling, translation, and distortion preserve digit meaning. The adversarial examples maintain digit interpretability while applying these transformations (see Figure 3 for comparison with other methods). Right: Adversarial example of a hamster image, leveraging semantic knowledge from pre-trained diffusion models. Despite substantial pixel modifications, the image remains natural-looking (see Figure 4 for comparison with other methods).

However, solving this problem is challenging.  propose a relaxation of the problem: Let \((x_{},y_{}):=c_{1}\,(x_{},x_{ })+c_{2}\,f(x_{},y_{})\), the optimization problem is

\[\ (x_{},y_{})x_{}^{n}\] (1)

where \(c_{1}\), \(c_{2}\) are constants, and \(f\) is an objective function closely tied to the classifier's prediction. For example, in , \(f\) is the cross-entropy loss function, indicating a misclassified direction of the classifier, while  suggest several different choices for \(f\).  recommend solving (1) using box-constrained L-BFGS.

### Energy-Based Models (EBMs)

An Energy-based Model (EBM) [17; 11] involves a non-linear regression function, represented by \(E_{}\), with a parameter \(\). This function is known as the energy function. Given a data point, \(x\), the probability density function (PDF) is given by:

\[p_{}(x)=(x))}{Z_{}}\] (2)

where \(Z_{}=(-E_{}(x))x\) is the normalizing constant that ensures the PDF integrates to \(1\).

### Diffusion Models

Starting with data \(x_{0}\), we define a diffusion process (also known as the forward process) using a specific variance schedule denoted by \(_{1},,_{T}\). This process is mathematically represented as:

\[q(x_{1:T}|x_{0})=_{t=1}^{T}q(x_{t}|x_{t-1}),\]

where each step is defined by

\[q(x_{t}|x_{t-1}):=(x_{t};}\,x_{t-1},_{t}I)\]

where \(\) is the pdf of Gaussian distributions. In this formula, the variance schedule \(_{t}(0,1)\) is selected to ensure that the distribution of \(x_{T}\) is a standard normal distribution, \(q(x_{T})=(x_{T};0,I)\). For convenience, we also define the notation \(_{t}:=1-_{t}\) for each \(t\), and \(_{t}:=_{s=1}^{t}_{s}\). By using the property of Gaussian distribution, we have

\[q(x_{t}|x_{0})=(x_{t};_{t}}\,x_{0},(1-_{t})I)\] (3)

The reverse process, known as the denoising process and denoted by \(q(x_{t-1}|x_{t})\), cannot be analytically derived. Thus, we use a parametric model, represented as \(p_{}(x_{t-1}|x_{t}):=(x_{t-1};_{}(x_{t},t),_{ }(x_{t},t))\), to estimate \(q(x_{t-1}|x_{t})\). In practice, \(_{}\) and \(_{}\) are implemented using a UNet architecture , which takes as input a noisy image at its corresponding timestep. For simplicity, within the rest of this paper, we will abbreviate \(_{}(x_{t},t)\) and \(_{}(x_{t},t)\) as \(_{}(x_{t})\) and \(_{}(x_{t})\) respectively.

## 3 A Probabilistic Perspective on Adversarial Examples

We propose a probabilistic perspective where adversarial examples are sampled from an adversarial distribution, denoted as \(p_{}\). This distribution can be conceptualized as a product of expert distributions :

\[p_{}(x_{}|x_{},y_{}) p_{}(x_{}|y_{})\,p_{}(x_{}|x_{ })\] (4)

where \(p_{}\) is defined as the 'victim distribution', which is based on the victim classifier and the target class \(y_{}\). \(p_{}\), on the other hand, denotes the distance distribution, where a high value of \(p_{}\) indicates a significant similarity between \(x_{}\) and \(x_{}\).

The subsequent theorem demonstrates the compatibility of our probabilistic approach with the conventional optimization problem for generating adversarial examples:

**Theorem 1**.: _Given the condition that \(p_{}(x_{}|y_{})(-c_{2}\,f(x_{},y_{}))\) and \(p_{}(x_{}|x_{})(-c_{1}\,(x_ {},x_{}))\), the samples drawn from \(p_{}\) will exhibit the same distribution as the adversarial examples derived from applying the box-constrained Langevin Monte Carlo method to the optimization problem delineated in equation (1)._The proof of the theorem can be found in Appendix A. Within the context of our discussion, we initially define \(p_{}\) and \(p_{}\) to have the same form as described in the theorem. Given this formulation, we can conveniently generate samples from \(p_{}\), \(p_{}\), and \(p_{}\) using LMC. Detailed procedures are provided in Section 5.1. As we delve further into this paper, we may explore alternative formulations for these components.

The Victim Distribution\(p_{}\) is dependent on the victim classifier. As suggested by , \(f\) could be the cross-entropy loss of the classifier. We can sample from this distribution using Langevin dynamics. Figure 2(a) presents samples drawn from \(p_{}\) when the victim classifier is subjected to standard training, exhibiting somewhat indistinct shapes of the digits. This implies that the classifier has learned the semantics of the digits to a certain degree, but not thoroughly. In contrast, Figure 2(b) displays samples drawn from \(p_{}\) when the victim classifier undergoes adversarial training. In this scenario, the shapes of the digits are clearly discernible. This observation suggests that we can obtain meaningful samples from adversarially trained classifiers, indicating that such classifiers depend more on semantics, which corresponds to the fact that an adversarially trained classifier is more difficult to attack. A similar observation concerning the generation of images from an adversarially trained classifier has been reported by .

The Distance Distribution\(p_{}\) relies on \((x_{},x_{})\), representing the distance between \(x_{}\) and \(x_{}\). By its nature, samples that are closer to \(x_{}\) may yield a higher \(p_{}\), which is consistent with the objective of generating adversarial samples. For example, if \(\) represents the square of the \(_{2}\) norm, then \(p_{}\) becomes a Gaussian distribution with a mean of \(x_{}\) and a variance determined by \(c_{1}\). Figure 2 (c) portrays samples drawn from \(p_{}\) when \(\) is the square of the \(_{2}\) distance and \(c_{1}\) is relatively large. The samples closely resemble the original images, \(x_{}\)s. This is attributed to the fact that each sample converges near the Gaussian distribution's mean, which corresponds to the \(x_{}\)s.

The Product of the DistributionsSamples drawn from \(p_{}\) tend to be concentrated in the regions of high density resulting from the product of \(p_{}\) and \(p_{}\). As is discussed, a robust victim classifier possesses generative capabilities. This means the high-density regions of \(p_{}\) are inclined to generate images that embody the semantics of the target class. Conversely, the dense regions of \(p_{}\) tend to produce images reflecting the semantics of the original image. If these high-density regions of both \(p_{}\) and \(p_{}\) intersect, then samples from \(p_{}\) may encapsulate the semantics of both the target class and the original image. As depicted in Figure 2 (e), the generated samples exhibit traces of both the target class and the original image. From our probabilistic perspective, the tendency of the generated adversarial samples to semantically resemble the target class stems from the generative ability of the victim distribution. Therefore, it is crucial to construct appropriate \(p_{}\) and \(p_{}\) distributions to minimize any unnatural overlap between the original image and the target class. We will focus on achieving this throughout the remainder of this paper.

## 4 Semantics-aware Adversarial Examples

Under the probabilistic perspective, the distance distribution \(p_{}\) is not necessarily based on a explicitly defined distance \(\). Instead, the primary role of \(p_{}\) is to ensure that samples generated from \(p_{}(|x_{})\) closely resemble the original data point \(x_{}\). With this concept in mind, we gain the flexibility to define \(p_{}\) in various ways. In this work, we construct \(p_{}\) using a probabilistic generative model (PGM). By utilizing this data-driven distance distribution and choosing a proper victim distribution, we can generate adversarial examples that exhibit more natural transformations in terms of semantics. These are referred to as semantics-aware adversarial examples. Moving forward, given that the distance \(\) is implicitly learned within \(p_{}\), we set \(c_{1}=1\) for simplicity, and henceforth, use \(c\) to denote \(c_{2}\).

### Data-driven Distance Distributions

We present two methods to develop the distribution \(p_{}(|x_{})\) centered on \(x_{}\): The first relies on a subjective understanding of semantic similarity, while the second leverages the semantic generalization capabilities of contemporary PGMs.

Semantics-Invariant Data AugmentationConsider \(\), a set of transformations we subjectively believe to maintain the semantics of \(x_{}\). We train a PGM on the dataset \(\{t_{1}(x_{}),t_{2}(x_{}),\}\), where each \(t_{i}\) is a sample from \(\), thereby shaping the distribution \(p_{}\). Through \(\), individuals can incorporate their personal interpretation of semantics into \(p_{}\). For instance, if one considers that scaling, rotation and TPS distortion (Appendix D.1) do not alter an image's semantics, these transformations are included in \(\).

Fine-Tuning Pretrained PGMsContemporary PGMs demonstrate remarkable semantic generalization capabilities when fine-tuned on a single object or image [32; 20]. Leveraging this trait, we propose fine-tuning the PGM on the given image \(x_{}\). The distribution of the fine-tuned model then closely aligns with \(x_{}\), while still facilitating robust semantic generalization.

### Victim Distributions

The victim distribution \(p_{}(c\,f(x_{},y_{}))\) is influenced by the choice of function \(f\). Let \(g_{}:^{n}^{||}\) be a classifier that produces logits as output with \(\) representing the neural network parameters, \(n\) denoting the dimensions of the input, and \(\) being the set of labels (the output of \(g_{}\) are logits).  suggested using cross-entropy as the function \(f\), which can be expressed as

\[f_{}(x,y_{}):=-g_{}(x)[y_{}]+_{y} (g_{}(x)[y])=-(g_{}(x))[y_{}]\]

where \(\) denotes the softmax function.

 explored and compared multiple options for \(f\). They found that, empirically, the most efficient choice of their proposed \(f\)s is:

\[f_{}(x,y_{}):=(_{y y_{}}g_{}(x)[y ]-g_{}(x)[y_{}],0).\]

In this study, we employ \(f_{}\) for the MNIST dataset and \(f_{}\) for the ImageNet dataset. A detailed discussion on this choice is provided in Section 8.1.

## 5 Concrete PGM Implementations

Fundamentally, any probabilistic generative model (PGM) is capable of fitting the distance distribution \(p_{}\). However, for efficient sampling from \(p_{}\), which is the multiplication of \(p_{}\) and \(p_{}\) as introduced in (4), we recommend employing sampling techniques based on the score \(s= p_{}(x_{}|x_{},y_{})\). Energy-based models and diffusion models are particularly effective in providing these scores. Therefore, in this study, we utilize these models to represent \(p_{}\).

### Generating Adversarial Examples Using Energy-Based Models

The distance distribution \(p_{}(|x_{})\) can be modeled using energy-based models (EBMs). For a given \(x_{}\), we train or fine-tune an EBM in the vicinity of \(x_{}\) to represent this distance distribution. Let \(E_{}\) denote the energy in the EBM. Consequently, the adversarial distribution is expressed as:

\[p_{}(x_{}|x_{},y_{}) e^{-cf(x_{ },y_{})}e^{-E_{}(x_{})}\]

and the corresponding score is:

\[_{x_{}} p_{}(x_{}|x_{},y_{ })=-c_{x_{}}f(x_{},y_{})-_ {x_{}}E_{}(x_{})\]

Utilizing Langevin dynamics (Appendix B.1), we can sample from this adversarial distribution. The process is detailed in Algorithm 1 (Appendix C).

### Generating Adversarial Examples Using Diffusion Models

To enhance generation quality and enable the creation of higher resolution images, we frame the construction of the adversarial distribution within a diffusion model context. Given an original image \(x_{}\) and a target class \(y_{}\), we employ a diffusion model \(p_{_{}}(x_{t-1}|x_{t}):=(x_{t-1};_{_{ }}(x_{t}),_{_{}}(x_{t}))\) at time step \(t-1\), where \(_{}\) denotes the parameters of the model when trained or fine-tuned on \(x_{}\). This diffusion model is used to approximate \(p_{}(|x_{})\). For simplicity, we will refer to \(_{}\) as \(\) throughout this paper, assuming no confusion arises from this notation. Then, letting \(x_{0}=x_{}\), the adversarial distribution is formulated as:

\[p_{}(x_{0}|x_{},y_{}) p_{}(x_{0 }|y_{})p_{}(x_{0}|x_{})=p_{}(x_{0}|y_{ }) p(x_{T})_{t=1}^{T}p_{}(x_{t-1}|x_{t})dx_{1 T}\]

where \(p(x_{T})\) is \(N(0,I)\) and the victim distribution \(p_{}(x|y_{})(-cf(x,y_{}))\). However, sampling \(x_{0}\) in this form is challenging in the denoising order of diffusion models. Therefore, we incorporate the \(p_{}\) term within the product:

\[ p(x_{T})_{t=1}^{T}p_{}(x_{0}|y_{})^{1/T}p_{ }(x_{t-1}|x_{t})dx_{1 T}\]

As each denoising step cannot predict \(x_{0}\) when sampling \(x_{t-1}\), we employ Tweedie's approach [12; 22; 16] to estimate \(x_{0}\) given \(x_{t}\):

\[_{0|t}=_{t}}}(x_{t}-_{t }}\,_{}(x_{t}))\] (5)

with \(_{}(x_{t})\) obtainable through the reparametrization trick from :

\[_{}(x_{t})=_{t}}}{_{t}}(x_{t}- }\,_{}(x_{t}))\] (6)

This leads to a practical expression for the adversarial distribution:

\[ p(x_{T})_{t=1}^{T}p_{}(_{0|t}|y_{})^{1/T}p _{}(x_{t-1}|x_{t})dx_{1 T}\]

In each denoising step, we sample \(x_{t-1}\) from the distribution \(p_{}(_{0|t}|y_{})^{1/T}p_{}(x_{t-1}|x_{t})\). The following theorem demonstrates that this distribution approximates a Gaussian distribution:

**Theorem 2**.: _Let \(p_{}(x|y_{})(-cf(x,y_{}))\) and \(p_{}(x_{t-1}|x_{t})=(x_{t-1};_{}(x_{t}),_{ }(x_{t}))\), we have_

\[p_{}(_{0|t}|y_{})^{1/T}p_{}(x_{t-1}|x_{t}) (x_{t-1};_{}(x_{t})+_{}(x_{t })g,_{}(x_{t}))\]

_where \(g=-_{x_{t-1}}f(_{0|t},y_{})|_{x_{t-1}=_{}(x_{t})}\) and \(_{0|t}\) is defined in Equation (5)._

For the proof, refer to Appendix A. Building upon Theorem 2, and assuming \(_{x_{t-1}}f(_{0|t},y_{})|_{x_{t-1}=_{}(x_{t})} _{x_{t-1}}f(_{0|t},y_{})|_{x_{t-1}=x_{t}}\), in line with the assumption made by , we introduce Algorithm 2 (Appendix C). This algorithm is designed to sample from the adversarial distribution \(p_{}\) as formulated within the context of diffusion models.

## 6 Experiments

### Mnist

SettingWe use an energy-based model (EBM) to model the distance distribution \(p_{}(|x_{})\) for a given original image \(x_{}\). This EBM is specifically trained on a set of transformations of \(x_{}\), denoted as \(\{t_{1}(x_{}),t_{2}(x_{}),\}\), where each \(t_{i}\) represents a sample from the transformation distribution \(\). This distribution includes a variety of transformations such as translations, rotations, and TPS (Thin Plate Spline) transformations (Appendix D.1). Examples of these transformed MNIST images are showcased in Figure 2 (d). To produce high-quality adversarial examples for MNIST, we employ rejection sampling and sample refinement techniques, as detailed in Appendix D.

For the victim distribution \(p_{}\), we choose the adversarially trained Madrynet as our victim (surrogate) classifier. We use \(f_{}\) to represent the function \(f\) in the victim distribution, as detailed in Section 4.2.

We benchmark our method against several approaches: PGD , ProbCW (which employs a Gaussian distribution for \(p_{}\) and \(f_{}\) for \(p_{}\)), and stAdv .

Quantitative resultWe select 20 images from each class in the MNIST test set as the original images. For each image, we generate one adversarial example for each target class, excluding the image's true class. This yields a total of \(20 10 9=1800\) adversarial examples for each method. The parameters of each method are adjusted to ensure approximately 90% of the adversarial examples accurately reflect the original concept of \(x_{}\). The effectiveness of our adversarial examples is evaluated against the adversarially trained Madrynet under white-box conditions, with results displayed in the 'MadryNet Adv' row of Table 1. Additionally, we task 5 human annotators with classifying these adversarial examples, considering an example to be successfully deceptive if the annotator identifies its original class. The annotators' success rates are shown in the 'Human Anno.' row of Table 1. We also assess the transferability and the success rate of the examples against defensive methods, with these outcomes detailed in Table 1. Notably, the term 'Certified Def' denotes the defense method introduced by .

Table 1 demonstrates that our proposed method achieves a higher success rate in white-box scenarios and greater transferability to other classifiers and defense methods, all while preserving the meaning of the original image. The white-box success rate of our method reaches 100% due to the implementation of rejection sampling, as introduced in Appendix D.2.

Qualitative resultUnlike the quantitative experiment, here we adjust the parameters so that the vast majority of examples can just barely deceive the classifier. The adversarial examples thus generated

    & PGD & ProbCW & stAdv & OURS \\  Human Anno. & 88.4 & 89.3 & 90.1 & **92.6** \\ 
**White-box** &  & & & \\ MadryNet Adv & 25.3 & 30.2 & 29.4 & **100.0** \\ 
**Transferability** &  & & & \\ MadryNet noAdv & 15.1 & 17.4 & 16.3 & **61.4** \\ Resnet noAdv & 10.2 & 10.9 & 12.5 & **24.3** \\ 
**Adv. Defence** &  & & & \\ Resnet Adv & 7.2 & 8.8 & 11.5 & **18.5** \\ Certified Def & 10.7 & 12.3 & 20.8 & **39.2** \\   

Table 1: Success rate (%) of the methods on MNIST.

Figure 3: Comparative visual analysis of PGD, Prob CW, StAdv, and our proposed method applied to MNIST. The surrogate classifier used is MadryNet with adversarial training. Images are framed with a green border to indicate a successful white-box attack, whereas a red border signifies a failed attack.

are displayed in Figure 3. From this figure, it is evident that the PGD method significantly alters the original image's meaning, indicating an inability to preserve the original content. ProbCW and StAdv perform somewhat better, yet they falter, especially when '0' and '1' are the original digits: for '0', the roundness is compromised; for '1', most adversarial examples take on the form of the target class. Furthermore, ProbCW examples exhibit noticeable overlapping shadows, and the StAdv samples clearly show signs of tampering. In contrast, our method maintains the integrity of the original image's meaning the most effectively.

### Imagenet

SettingWe employ a diffusion model that has been fine-tuned on \(x_{}\) to approximate the distance distribution \(p_{}(|x_{})\). Specifically, we start with a pre-trained diffusion model \(p_{}(x_{t-1}|x_{t})\), and then we fine-tune it on a given \(x_{}\), as introduced in section 4.1. For the victim distribution, we choose ResNet50 as the surrogate classifier and utilize \(f_{}\), the cross-entropy function for \(f\).

To evaluate our method's performance, we compare it with several existing approaches: ACE , ColorFool , cAdv  and NCF . Our method is evaluated across three hyperparameter

    & NCF & cAdv & ACE & ColorFool & OURS (\(c=5\)) & OURS (\(c=10\)) & OURS (\(c=20\)) \\  Human Anno. & 2.6 & 16.9 & 11.2 & 6.7 & **31.2** & 28.3 & 25.4 \\ 
**White-box** & & & & & & & \\ Resnet 50 & **94.2** & 93.3 & 91.2 & 90.4 & 84.5 & 88.4 & 91.3 \\ 
**Transferability** & & & & & & & \\ VGG19 & **83.7** & 71.0 & 73.5 & 72.8 & 70.8 & 74.4 & 79.7 \\ ResNet 152 & **71.8** & 61.1 & 55.4 & 54.9 & 57.3 & 64.2 & 67.0 \\ DenseNet 161 & **63.9** & 54.0 & 45.1 & 41.3 & 45.0 & 52.4 & 55.3 \\ Inception V3 & **72.4** & 60.1 & 57.5 & 57.4 & 58.1 & 64.1 & 66.9 \\ EfficientNet B7 & **72.9** & 58.0 & 56.3 & 62.6 & 60.4 & 61.6 & 66.0 \\ 
**Adversarial Defence** & & & & & & & \\ Inception V3 Adv & **61.1** & 48.9 & 40.3 & 41.9 & 43.4 & 47.2 & 51.4 \\ EfficientNet B7 Adv & **50.3** & 42.9 & 34.7 & 36.1 & 37.7 & 40.4 & 44.2 \\ Ensemble IncRes V2 & **53.3** & 45.2 & 36.6 & 35.6 & 39.7 & 42.7 & 46.5 \\  Average & **66.2** & 55.2 & 49.9 & 50.3 & 51.6 & 55.9 & 59.6 \\   

Table 2: Success rate (%) of the methods on Imagenet.

Figure 4: Comparative visual analysis of NCF, cAdv, ACE, ColorFool and our proposed method applied to Imagenet. The surrogate classifier used is ResNet50. For additional examples, refer to Appendix I.

configurations: \(c=5\), \(c=10\), and \(c=20\). We test the transferability of these methods on Inception V3 , EfficientNet B7 , VGG19 , Resnet 152  and DenseNet 161 . We also list the attack success rate on the adversarial defence methods such as adversarially trained Inception V3 , adversarially trained EfficientNet  and ensemble adversarial Inception ResNet v2 .

Quantitative ResultsWe randomly select 1,000 non-human images from the ImageNet dataset to serve as original images \(x_{}\), adhering to the ethical guidelines outlined in Section 8.4. For each method, we then generate one untargeted adversarial example per original image. As with the MNIST experiment, Table 2 presents the quantitative results for the ImageNet dataset. In this context, human annotators were presented with pairs consisting of an original image and its corresponding adversarial example and were asked to identify the computer-modified photo. A case is considered successful if the annotator mistakenly identifies the original image as the manipulated one. Therefore, a 'Human Anno.' success rate around 50% suggests that the adversarial examples are indistinguishable from the original images by human observers.

The data in Table 2 places our proposed method second in terms of transferability across different classifiers and defense methods. Note that the 'Average' line is the average of the transferability lines and the adversarial defence lines. Drawing on data from Table 2, Figure 5 is plotted, illustrating that our proposed method not only secures a relatively high attack success rate but also remains minimally detectable to human observers. It's important to mention that while NCF achieves the highest attack success rate in many instances, it is also easily detectable by humans. This observation is supported by the human annotation success rate and further evidenced by our qualitative comparison in Figure 4.

Qualitative ResultFigure 4 displays adversarial examples generated by our method compared with those from alternative methods, under the same parameters used in the quantitative analysis. The images reveal that other methods tend to produce significant color changes to the original image, rendering the alterations easily recognizable by humans. This observation is corroborated by the 'Human Anno.' row in Table 2. Meanwhile, adversarial examples from our method are more subtle and the alterations are less detectable by humans.

## 7 Related Work

The term 'unrestricted adversarial attack' refers to adversarial attacks that are not confined by geometrical constraints. Unlike traditional attacks that focus on minimal perturbations within a strict geometric framework, unrestricted attacks often induce significant changes in geometric distance while preserving the semantics of the original image. These methods encompass attacks based on spatial transformations [46; 1], manipulations within the color space [19; 54], the addition of texture , and color transformations guided by segmentation [35; 51]. Notably,  introduced a concept also termed 'unrestricted adversarial attack'; however, in their context, 'unrestricted' signifies that the attack is not limited by the presence of an original image but rather by an original class.

Recent works [7; 50; 6; 5] incorporate adversarial attack gradients into the image editing process and utilize contemporary probabilistic generative models - diffusion models - to create semantic-aware adversarial examples. Our approach is distinct. While all methods involve some form of combining attack gradients with generative gradients, our method is principled, derived from the original optimization problem introduced in Equation (1). Moreover, we introduce a novel probabilistic perspective on adversarial attacks for the first time.

Figure 5: Average attack success rate across the blackbox transferability and defence methods v.s. human annotation success rate illustrated in Table 2.

Discussions

### Contrasting MNIST and ImageNet Experiments

Targeted vs. Untargeted AttacksThe MNIST dataset, comprising only 10 classes, allows us to perform targeted attack experiments efficiently. However, ImageNet, with its extensive set of 1,000 classes, presents practical challenges for conducting targeted attacks on each class individually. Consequently, we assess untargeted attack performance, aligning with methodologies in other studies.

Data DiversityAdversarially trained networks like MadryNet for MNIST are difficult to fool, primarily due to the limited diversity among handwritten digits. As illustrated in Figure 2 (b), the classifier can nearly memorize the contours of each digit, given its impressive generative capabilities for such data. In attacking this classifier, we carefully selected the \(f_{}\) method for the victim distribution to reduce the influence of the target class's'shadow.' In contrast, for ImageNet, the vast diversity and the relatively weaker generative ability of the victim classifier allow for the use of \(f_{}\), facilitating higher confidence in target class recognition by the victim classifier.

### Defending This Attack

Adversarial training operates on the principle: 'If I know the form of adversarial examples in advance, I can use them for data augmentation during training.' Thus, the success of adversarial training largely depends on foreknowledge of the attack form. Our method bypasses adversarially trained classifiers because the'semantic-preserving perturbation' we employ is unforeseen by the classifier designers - they use conventional adversarial examples for training.

Conversely, if designers anticipate attacks from our algorithm, they could incorporate examples generated by our method into their training process - essentially, a new form of adversarial training.

This scenario transforms adversarial attacks and defenses into a game of Rock-Paper-Scissors, where anticipating the type of attack becomes crucial. One might consider training a classifier using all known types of attacks. However, expanding the training data too far from the original distribution typically leads to decreased performance on the original classification task, which is undesirable . We believe that investigating the trade-off between this 'generalized' adversarial training and accuracy on the original task represents a promising avenue for future research.

### Limitations

Training or fine-tuning a model for each original image \(x_{}\) is time-consuming. Recent advancements, such as faster fine-tuning methods [20; 48], offer potential solutions to mitigate this issue. We see promise in these developments and consider their application an avenue for future research.

### Ethical Considerations in User Studies

As mentioned by , the ImageNet dataset contains elements that may be pornographic or violate personal privacy. To mitigate the exposure of human annotators in our experiments (see Section 6) to such sensitive content, we avoid selecting any images that depict humans for our original images \(x_{}\).

## 9 Conclusion

This paper offers a probabilistic perspective on adversarial examples, illustrating a seamless transition from 'geometrically restricted adversarial attacks' to 'unrestricted adversarial attacks.' Building upon this perspective, we introduce two specific implementations for generating adversarial examples using EBMs and diffusion models. Our empirical results demonstrate that these proposed methods yield superior transferability and success rates against adversarial defense mechanisms, while also being minimally detectable by human observers.