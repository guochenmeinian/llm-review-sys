# Principles

Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability, Reproducibility, and Practicality

 Tianle Zhang\({}^{1,2}\), **Langtian Ma\({}^{3}\), Yuchen Yan\({}^{4}\), Yuchen Zhang\({}^{2}\), Kai Wang\({}^{2}\), Yue Yang\({}^{1}\), Ziyao Guo\({}^{1}\), Wenqi Shao\({}^{1}\), Yang You\({}^{2}\), Yu Qiao\({}^{1}\), Ping Luo\({}^{5}\), Kaipeng Zhang\({}^{1}\)\({}^{*}\)**

\({}^{1}\)Shanghai AI Laboratory \({}^{2}\)National University of Singapore \({}^{3}\)University of Wisconsin-Madison

\({}^{4}\)University of California San Diego \({}^{5}\)The University of Hong Kong

Corresponding author

###### Abstract

Recent text-to-video (T2V) technology advancements, as demonstrated by models such as Gen2, Pika, and Sora, have significantly broadened its applicability and popularity. Despite these strides, evaluating these models poses substantial challenges. Primarily, due to the limitations inherent in automatic metrics, manual evaluation is often considered a superior method for assessing T2V generation. However, existing manual evaluation protocols face reproducibility, reliability, and practicality issues. To address these challenges, this paper introduces the Text-to-Video Human Evaluation (T2VHE) protocol, a comprehensive and standardized protocol for T2V models. The T2VHE protocol includes well-defined metrics, thorough annotator training, and an effective dynamic evaluation module. Experimental results demonstrate that this protocol not only ensures high-quality annotations but can also reduce evaluation costs by nearly 50%. We will open-source the entire setup of the T2VHE protocol, including the complete protocol workflow, the dynamic evaluation component details, and the annotation interface code2. This will help communities establish more sophisticated human assessment protocols.

## 1 Introduction

Text-to-video (T2V) technology has made significant advancements in the last two years and garnered increasing attention from the general community. T2V products such as Gen2  and Pika  have attracted many users. More recently, Sora , a powerful T2V model from OpenAI, further heightened public anticipation for the T2V technology. Predictably, the evaluation of the T2V generation will also become increasingly important, which can guide the development of T2V and assist the public in selecting appropriate models . This paper does a comprehensive paper survey (see Appendix F for details) and explores a human evaluation protocol for T2V generation.

Automatic and human evaluation are the two main kinds of evaluation for video generation. In recent years, nearly half of video generation papers conduct only automatic evaluation, such as Inception Score (IS) , Frechet Inception Distance (FID) , Frechet Video Distance (FVD) , CLIP Similarity  and Video Quality Assessment (VQA) . However, these metrics meet various challenges, such as relying on reference videos for calculation, overlooking temporal motion changes, and, more importantly, not aligning well with human perception . Undoubtedly, automatic evaluation is a promising research direction, but human evaluation is more convincing so far.

Existing human evaluation for video generation also meets reproducibility, reliability, and practicability challenges. Our survey reveals that few papers employ consistent human evaluation protocols, evidenced by significant disparities in assessment metrics, evaluation methods, and annotator sources across studies. For example, some papers [100; 53] use Likert scales while others prefer comparative approach [40; 26]. Moreover, many papers lack detailed information on evaluation protocols, impeding further analysis and reproducibility. Additionally, most papers rely on annotators recruited directly by the authors, i.e., laboratory-recruited annotators (LRAs). These articles often lack quality checks, which can introduce bias and affect reliability . Furthermore, there is notable variation in the number of annotations used across papers, ranging from a few dozen to tens of thousands, posing a practical challenge in achieving a balance between credible results and resource constraints.

This paper introduces Text-to-Video Human Evaluation (T2VHE), a standardized human evaluation protocol for T2V generation. T2VHE consists of well-designed evaluation metrics, annotator training encompassing detailed instructions and illustrative examples, and a user-friendly interface, see Figure 1 for illustrations. Additionly, it introduces a dynamic evaluation module to reduce the annotation cost. We further introduce the details of our protocol as follows:

Figure 1: (a) An illustration of our human evaluation protocol. (b) The annotation interface, wherein annotators choose the superior video based on provided evaluation metrics. (c) Instruction and examples to guide used to the “Video Quality” evaluation.

**Evaluation metrics.** Many previous research focuses on video quality and text alignment but neglects the critical aspects of temporal and motion quality inherent to videos [103; 74], as well as ethical considerations . T2VHE employs four objective evaluation metrics: video quality, temporal quality, motion quality, and text alignment, and two subjective metrics: ethical robustness and human preference. For different types of metrics, annotators were asked to rely more on the definitions of reference perspectives and their preferences to make judgments, respectively.

**Evaluation method.** Due to the complexities and potential noise inherent in absolute scoring , T2VHE employs the comparison-based method, which is relatively annotator-friendly . Critiquing the reliance of traditional protocols on win rates, which may introduce biases and offer limited insights into model performance [33; 22], T2VHE adopts the Rao and Kupper model  to quantify annotations results. This approach enables more efficient management of pairwise comparison results, yielding improved model rankings and score estimations.

**Evaluators.** While crowdsourcing platforms are often considered to gather high-quality annotations [8; 15], our survey reveals that researchers primarily rely on LRAs. However, concerns about LRAs' reliability due to inadequate training and quality checks have been raised, potentially biasing evaluation outcomes. To address this, T2VHE proposes comprehensive annotator training, including detailed guidelines and examples to improve understanding of evaluation metrics. Experimental validation shows that properly trained LRAs can achieve agreement with crowdsourced annotators, affirming the effectiveness of our training approach in ensuring high-quality annotations.

**Dynamic evaluation module.** T2VHE incorporates a dynamic evaluation component to enhance protocol efficiency. This component optimizes utility through two key functionalities: firstly, pre-annotation sorting of videos using automatic scoring results, prioritizing the annotation of video pairs considered more deserving of manual evaluation during the static annotation phase; secondly, the dynamic annotation phase, which determines whether to annotate pending video pairs based on differences in model scores. Experimental results indicate that this module reduces costs by approximately 50%, while concurrently ensuring the validity of the annotation results.

Our study reveals several findings: (1) Post-training LRAs and annotators on crowdsourcing platforms achieve consensus. The low quality of the pre-training LRAs' annotations mainly stems from annotators' biased interpretations of evaluation metric definitions. Thus, furnishing detailed guidelines and example training can substantially improve annotation quality, yielding results comparable to those obtained by professional annotators. (2) Comparison-based evaluation exhibits significant potential for optimization. It manifests an \(O(N^{2})\) growth trend in the number of annotations as the number of models compared increases. Our dynamic evaluation module reveals that judiciously selecting samples for annotation can produce model ranking outcomes consistent with those derived from fully annotated data, even when annotating only approximately half of them. (3) A substantial disparity persists between open-source and closed-source models. While open-source models perform well in some evaluation metrics, videos generated by closed-source models generally exhibit superior quality and tend to garner greater popularity among annotators.

The main contributions of this paper are as follows:

1. We introduce a standardized human evaluation protocol for T2V models, comprising a meticulously crafted array of evaluation metrics alongside accompanying annotator training resources. Moreover, it lets us acquire high-quality annotations utilizingLRAs.
2. Our dynamic evaluation component reduces annotation costs to approximately half of the original expenditure while maintaining annotation quality.
3. We comprehensively evaluate the latest T2V models and commit to open-sourcing the entire evaluation process and code, empowering the community to assess new models with fresh data based on existing reviews. Our protocol is also easily extended.

## 2 Related work

**Text-to-video generative models.** Creating realistic and novel videos has been a compelling research area for many years [87; 71; 111]. Various generative models have been investigated in prior studies, including GANs [87; 75; 83; 81; 77], autoregressive models [80; 105; 61; 23; 37], and implicit neural representations [79; 113]. T2V generation focuses on producing videos from textual descriptions.

Recently, the success of diffusion models in image synthesis has spurred studies to adapt these models for conditional and unconditional video synthesis [86; 31; 123; 101; 5; 45; 107].

**Evaluation metrics of video generative models.** Evaluation metrics for video generative models can be broadly classified into automated evaluation metrics and benchmark methods. IS assesses image diversity and clarity through a pre-trained network , CLIP Similarity assesses alignment between textual descriptions and generated images , FID and FVD compare feature representations from real and generated images [34; 84]. Despite their usefulness, these metrics have limitations, including bias, sensitivity to surface similarity, and inconsistency with human perception [67; 54]. Additionally, VBench , EvalCrafter , and FETV , among other benchmarks, provide comprehensive evaluations but may lack the diversity to cover all real-world scenarios, and these automated scorers typically require alignment training based on human evaluation results. Given the limitations of these automated metrics and benchmarks, high-quality manual assessments remain critical.

## 3 Human evaluation in video generation

We survey 89 papers on video generation models since 20163, and after reviewing how they use and report human assessments, we have the following findings:

**Automatic vs. Human evaluation.** Among the 89 papers reviewed, 44 rely exclusively on automated evaluation metrics. However, these metrics typically have limitations, such as only capturing certain characteristics of the generated video , dependence on pre-trained models , and the necessity for reference videos . Moreover, most automated metrics have demonstrated inconsistency with human evaluations, thus rendering them unsuitable as the sole measure [67; 54].

Many benchmark studies have attempted to train automated scorers based on human-assessed results [54; 100]. However, the training data is often obtained by pre-training LRAs, and the protocols employed exhibit considerable variation across studies. Hence, though automated evaluation should be a promising research direction, human evaluation is more reliable so far, and also good human evaluation can assist the development of automated evaluation.

**Human evaluation metrics.** The setup of human evaluation metrics varies greatly across papers. Ten studies directly use overall human preference as an evaluation metric, while some develop 16 nuanced metrics to assess from various perspectives. Overall, assessing the video's quality and relevance to the text descriptions remains the main focus of human evaluation [74; 28]. Moreover, an increasing number of studies focus on evaluating the video generation model's capabilities concerning motion, consistency, and continuity [53; 40]. In addition, we found no study introduces ethical evaluation in human evaluation, which is an aspect that warrants attention .

**Human evaluation methods.** Of the studies conducting human evaluations, nearly 70% employ a comparison-based approach in their evaluation protocols, wherein annotators select the superior video among two or more options to make judgments. In contrast, seventeen papers employed 3-point or 5-point Likert scales (i.e. absolute scoring), necessitating annotators to directly rate individual video's performance across various dimensions. However, this approach increases annotation complexity and introduces higher levels of noise and disagreement .

**Quantification of annotation results.** In evaluation protocols employing absolute scoring, the final model score is usually an average of all annotation results from all samples. On the other hand, pairwise comparison-based evaluation protocols often employ the win ratio to quantify annotations . Thus, while comparison-based evaluations are more user-friendly, they often require the evaluation of all model combinations to generate stable and reliable outcomes .

**Annotators.** Many articles omit crucial annotator information, such as the number of annotators, their recruitment sources, and remuneration details. Additionally, for studies utilizing crowdsourcing platforms, only a handful of articles mention eligibility screener settings. This information is vital for gauging the reliability and ethicality of assessment results .

**Annotator training and quality checking.** Only eight articles provided instruction-based or example-based training while utilizing LRAs. Moreover, merely two articles employed inter-annotatoragreement (IAA) for annotation quality checking. Most protocols not only neglected to train LRAs before annotation but also omitted quality checks, raising concerns about the reliability of results.

**Prompts.** For different evaluation needs, researchers usually choose different prompts for generating videos, such as HuMMan , which is specifically designed for evaluating fine-grained spatiotemporal motion generation. However, datasets sourced from real-world instances often lack diversity, motivating recent benchmarking studies to advocate for manually curated cue datasets encompassing comprehensive categories for model evaluation [40; 53; 54]. Due to different needs and different sources of prompts, the number of prompts used in human assessment often varies by more than a few dozen times from study to study, and many papers use fewer than 100 samples per model for human assessment, such small sample sizes are likely to produce biased results .

**Annotation interface.** Since videos generated by T2V models usually have different resolutions and sizes, how they are presented in the annotation interface also impacts the evaluation results. While some studies adopt methods such as adding watermarks, frame sampling, or cropping to standardize videos from different models , the majority only scale or leave generated videos unaltered during annotation. In addition, only nine articles provide the details of the annotation interface, with none sharing the interface code. This absence of information impedes the reproducibility of results for future studies adhering to similar protocols. Moreover, the scarcity of reusable resources hinders the ongoing enhancement of human evaluation protocols and practices .

Further, we provide more in-depth discussions of video processing, protocol settings, and samples for the human assessment process in Appendix C.

## 4 Our protocol for text-to-vedo models

Our T2VHE framework comprises four key components: evaluation metrics, evaluation method, evaluator, and dynamic evaluation module. To ensure a comprehensive assessment of the T2V model, we meticulously devise a set of evaluation metrics, accompanied by precise definitions and corresponding reference perspectives. For ease of annotation, we employ a comparison-based scoring format as evaluation method [7; 8] and develop annotator training to ensure researchers can procure high-quality annotations using post-training LRAs. Furthermore, our protocol incorporates an optional dynamic evaluation component, enabling researchers to attain reliable evaluation results at reduced costs. More details can be found in Appendix D.

### Evaluation metrics

Drawing from established protocols in image generation evaluation, prior studies have primarily used metrics like "Video Quality" and "Overall Alignment" for human assessment. However, these metrics often suffer from vague definitions, leading annotators to base ratings on general impressions and fidelity to the textual content. This lack of specificity can introduce subjectivity, potentially undermining the quality of annotations. Recent research also underscores that motion and temporal quality are also vital metrics for assessing video generation models' capabilities [53; 40]. Additionally, as video generation technology gains popularity, its ethical and societal impacts are becoming increasingly critical factors in evaluation . However, our survey reveals that none of the previous protocols considered this indicator. Moreover, only ten studies provided specific training for annotators, suggesting that the majority of research has only offered basic definitions of each metric without comprehensive instructions or relevant examples, which are essential for ensuring high-quality annotations .

To this end, we establish a comprehensive evaluation framework with explicit definitions and corresponding reference perspectives for each metric. Additionally, to enable precise assessments, we also devise thorough annotator training, detailed in Section 4.3. Objective indicators require strict adherence to the reference perspectives to ensure consistency and repeatability in evaluations, while subjective indicators allow for personal interpretation, providing a holistic assessment of the model's performance and potential. Recognizing the subjective nature of certain indicators, we categorize them into objective and subjective types. Objective indicators require strict adherence to the reference perspectives to ensure consistency and repeatability in evaluations, while subjective indicators allow for personal interpretation, providing a holistic assessment of the model's performance and potential. Detailed definitions and reference perspectives for each metric can be found in Table 1.

### Evaluation method

There exist two primary scoring methods: comparative and absolute. The former requires annotators to compare a set of videos and select the one demonstrating superior performance, whereas the latter entails directly assigning scores to the videos. Absolute scoring typically necessitates detailed instructions and precise question formulations due to its complexity . However, even with these in place, absolute scoring could still result in noisy annotations and pose challenges in reaching consensus among annotators . Hence, we use the less challenging comparative scoring method.

**Quantification of annotations.** Traditional comparative scoring protocols rely on the win ratio in pairwise comparison, however, this method has several drawbacks. First, it can introduce bias if models are not uniformly compared . For instance, a model frequently pitted against stronger counterparts might exhibit a lower win ratio compared to one facing weaker opponents more frequently. Consequently, a significant number of comparisons are required to establish reliable rankings . Moreover, the win ratio alone does not reliably indicate the likelihood of one model outperforming another . To overcome these issues, we adopt the Rao and Kupper model , a probabilistic approach that allows for more efficient handling of the results of pairwise comparisons using less data than full comparisons. This model enables better estimation of model rankings and scores, thereby furnishing a more precise and dependable evaluation compared to simply using the win ratio. The

  
**Metric** & **Definition** & **Reference perspectives** & **Description** & **Type** \\   &  & Video Fidelity & Assess whether the video appears highly realistic, making it hard to distinguish from actual footage. &  \\  & & Video Fidelity & Assess whether the video appears highly realistic, making it hard to distinguish from actual footage. & & \\  & Which video is more realistic and aesthetically pleasing? & & Evaluate the artistic beauty and aesthetic value of each video frame, including color coordination, composition, and lighting effects. & \\  & & & & \\   &  & Content Consistency & Evaluation whether the subject’s and background’s appearances remain unchanged throughout the video. &  \\  & & & & \\   & & Temporal Flickering & Assess the consistency of local and high-frequency details over time in the video. & & \\   &  & Movement Fluidity & Evaluate the natural fluidity and adherence to physical laws of movements within the video. &  \\  & & & & \\   & & Motion Intensity & Assess whether the dynamic activities in the video are sufficient and appropriate. & & \\   &  & Object Category & Assess whether the video accurately reflects the types and quantities of objects described in the text. &  \\  & & & & \\   & & Style Consistency & Evaluate whether the visual style of the video matches the text description. & & \\   &  & Toxicity & Evaluate the video for any content that might be deemed toxic or inappropriate. &  \\  & & & & \\   & & & & \\   & & & & \\   & & & & \\   & & & & \\   &  & Video Originality & Evaluate the originality of the video’s contents. &  \\  & & & & \\   & & & & \\   & & & & \\   & & & & \\   & & & & \\   

Table 1: Comprehensive evaluation criteria for T2V models. The table presents T2VHE’s evaluation metrics, their definitions, corresponding reference perspectives, and types. When considering different indicators, annotators rely differently on reference angles in making their judgments.

estimation is conducted by maximizing the log-likelihood function:

\[l(p,)=_{i=1}^{t}_{j=i+1}^{t}(n_{ij}}{p_{i}+  p_{j}}+n_{ji}}{ p_{i}+p_{j}}+_{ij} p_{j}(^{2}-1)}{(p_{i}+ p_{j})( p_{i}+p_{j})}),\] (1)

where \(t\) is the number of models, \(p=(p_{1},,p_{t})^{T}^{t}\) is the vector representing the scores of each model, \(\) is a tolerance parameter, \(n_{ij}\) denote the number of times model \(i\) is preferred to model \(j\), and \(}\) denotes the number of times the two models reached a tie. Further details about model's implementation and its parameter estimation process are provided in Appendix D.3.

### Evaluators

For most video generation evaluation tasks, the evaluator does not need specific expertise. However, using annotators from crowdsourcing platforms, such as Amazon Mechanical Turk (AMT), still results in higher annotation quality [67; 64; 66], as these workers have usually completed many tasks and carefully followed the publisher's requirements to ensure successful payment. Nevertheless, due to cost constraints, more studies tend to use non-professional, unpaid LRAs for the annotation tasks. Furthermore, our survey showed that most studies using LRAs lack annotator training and quality checking, raising concerns about the reliability of their annotations. To address this issue, we developed a comprehensive training methodology for annotators and conducted experiments on a pilot dataset to explore the impact of annotator qualifications on annotation quality.

**Annotator training.** We propose two cost-effective training methods: instruction-based and example-based training. Specifically, we furnish detailed guidance for each metric, complemented by two to three reference perspectives, each perspective is paired with an example and an analytical process to aid annotators in understanding metric definitions and making accurate judgments. Detailed training interfaces are illustrated in Figure 1 and Figures 4 to 8.

**Comparison of annotator qualifications.** We assess three annotator qualifications: AMT evaluators (who are required to hold an AMT Master designation), pre-training LRAs, and post-training LRAs. Each AMT evaluator is compensated $0.05 per task and underwent both instruction-based and example-based training to maintain high standards of annotation quality . In the case of pre-training LRAs, workers annotate tasks directly based on the problem definition for each indicator. Conversely, post-training LRAs familiarize themselves with guidelines and examples before annotating. Five annotators are tasked with selecting the superior video from each pair in each qualification category, as outlined in Section 4.1. Detailed annotation interfaces and the pilot dataset setup are presented in Figure 1 and Section 5, respectively.

After collecting all annotations, we observe disparities in model rankings derived from AMT annotators compared to those from pre-training LRAs across various metrics. To further evaluate these differences, we calculate the internal IAA4 of AMT annotators and the external ones between them and the pre-and post-training annotators. For the latter two sets of experiments, we randomly select five annotators from the AMT group and the two LRAs groups, respectively, to calculate the corresponding IAA and average the results. As shown in Table 2, pre-training annotators demonstrate lower agreement with professional annotators across multiple dimensions, evidenced by significantly

   Metric & AMT \& Pre-training LRAs & AMT \& Post-training LRAs & AMT \\  Video Quality & 0.185 & 0.411 & 0.451 \\ Temporal Quality & 0.131 & 0.340 & 0.369 \\ Motion Quality & 0.088 & 0.338 & 0.249 \\ Text Alignment & 0.069 & 0.327 & 0.366 \\ Ethical Robustness & -0.057 & 0.100 & 0.177 \\ Human Preference & 0.167 & 0.281 & 0.297 \\   

Table 2: Comparison of annotation consensus under different annotator qualifications. We compute Krippendorff’s \(\) as an IAA measure. Higher values represent more consensus among annotators.

lower IAA than the other two groups. In contrast, post-training annotators exhibit improved agreement with the professional annotators, approaching levels of intra-AMT consensus. Thus, the model rankings obtained from the two sets of annotation results are identical. These findings suggest that effective annotator training within our protocol can yield high-quality annotation results using LRAs comparable to those obtained using professional annotators.

### Dynamic evaluation module

As the number of models increases, traditional evaluation protocols often become more costly. To minimize annotation costs and ensure stable model ranking with fewer comparisons, we develop a dynamic evaluation module based on two key principles: the video quality proximity rule and the model strength rule. The first principle ensures that initially evaluated video pairs are of comparable quality, reducing unnecessary annotations, the second principle selects video pairs based on model strength, enhancing evaluation efficiency. The specific process is as follows:

Before annotation starts, each model receives an unbiased strength value. These scores are normalized and summed to generate a feature score for each video. Groups of model pairs are then constructed for each prompt, with the difference between video scores input into an exponential decay model to determine pair scores and group total scores. These groups are then sorted based on their total scores to prioritize those with close quality. In the follow-up phases, for each assessment indicator, all model scores are updated using the Rao and Kupper model after evaluating video pairs in the initial groups. Subsequent annotations occur in batches, with model strengths adjusted periodically. When evaluation results stabilize across all dimensions, i.e., the model rankings are unchanged for several consecutive batches, the evaluation is terminated. We provide the implementation details of the module in Appendix D.2 and verify its effectiveness in Section 5.3.

## 5 Human evaluation of existing models

We evaluated five state-of-the-art T2V models, including Gen2 , Pika , TF-T2V , Latte , and Videocrafter , see Appendix D.1 for details. All videos were generated without any prompt engineering or filtering. Furthermore, to ensure uniformity and ease of comparison for evaluators, we standardized the height of all videos in the annotation interface.

### Settings

**Data preparation.** We use the Prompt Suite per Category  as the source of prompts, which comprises prompts manually curated from eight distinct categories. We randomly select a quarter of the prompts from each category to serve as our evaluation prompts. For each prompt, we construct 10 pairwise comparisons using videos generated by five models and ask annotators to evaluate the superiority of one video over another across various metrics. This process results in 2,000 video pairs for annotation, from which we randomly sampled 200 video pairs to form the pilot dataset.

**Annotators.** To analyze the differences in results among different annotators and to affirm the protocol's generalizability and validity, following settings detailed in Section 4.3, we engage three distinct categories of annotators: AMT annotators, pre-training LRAs, and post-training LRAs. Each AMT annotator is limited to 250 tasks to maintain quality, while both pre-training and post-training LRAs are tasked with annotating all video pairs. We also test the effectiveness of our dynamic evaluation component using post-training LRAs under the same conditions.

### Evaluation results

For annotators who don't use the dynamic evaluation module, we collect the annotated data for all video pairs under the six metrics, resulting in a total of \(3 5 2000 6\) annotations (three categories of annotators, five in each category). Conversely, for annotators using the dynamic component, the average number of video pairs to be annotated is only 1,068 per annotator. For AMT annotators, 76 participants contribute, with an average of 131 tasks per annotator.

Figure 2 summarizes the results of the quantified annotations, more detailed scores and rankings can be found in Appendix D.4. As discussed in Section 4.3, the annotation results obtained by the pre-training LRAs markedly differ from those of the other three groups, evident in the discrepancy

between the final model scores and rankings for each dimension. In addition, the annotation results of the trained LRAs closely mirror those of the AMT personnel, yielding consistent final model ranking outcomes. We also conduct a quality check of the annotation results for the LRAs before and after training. As shown in Table 3, the annotations from the post-training LRAs exhibit higher quality.

However, regardless of the annotator sources, closed-source models typically perform better. In the annotated results from AMT personnel, Gen2 demonstrates significant superiority over other models across all metrics, while Pika also exhibits commendable performance across most metrics.

In contrast, the performances of open-source models show less disparity in terms of video quality, temporal quality, and motion quality metrics. TF-T2V's generations typically excel in video quality and action timing, while Videocrafter2, an earlier open-source model, demonstrates notable proficiency in generating high-quality videos. However, distinctions among the three models become more apparent in the metrics of text alignment, ethical robustness, and human preference. Notably, Latte exhibits strong performance in text alignment and ethical robustness, even surpassing Pika. This contributes to its higher ranking in human preference compared to other open-source models despite marginal differences in other metrics.

### Module validation

As detailed in Section 5.2, our protocol, augmented by the dynamic evaluation module, cuts annotation costs to about 53% of the original expense while achieving comparable outcomes. We further explore the module's effectiveness and reliability in this section.

**Effectiveness.** Although protocols utilizing pairwise comparisons offer convenience to annotators, their evaluation costs can escalate rapidly as the number of models under examination increases. To assess the effectiveness of the dynamic evaluation component, we randomly select corresponding annotation results from 2-4 models (25 model combinations in total) out of all annotations. For each

   Model 1 & Model 2 & Count \\  Gen2 & Latte & 230 \\  & Videocrafter2 & 219 \\  & TF-T2V & 215 \\  & Pika & 184 \\ Pika & Videocrafter2 & 76 \\  & TF-T2V & 65 \\  & Latte & 76 \\ Latte & TF-T2V & 19 \\  & Videocrafter2 & 12 \\ TF-T2V & Videocrafter2 & 20 \\   

Table 4: Type and number of model pairs discarded in dynamic evaluation.

   Metric & Pre-training & Post-training \\  Video Quality & 0.224 & 0.339 \\ Temporal Quality & 0.178 & 0.288 \\ Motion Quality & 0.164 & 0.321 \\ Text Alignment & 0.145 & 0.236 \\ Ethical Robustness & 0.055 & 0.107 \\ Human Preference & 0.195 & 0.284 \\   

Table 3: Comparison of internal IAA of LRAs before and after training. The internal IAA of post-training LRAs rose in all dimensions, implying a significant improvement in the annotation quality. We compute Krippendorff’s \(\) as a measure of internal IAA. Higher values represent more consensus among annotators.

Figure 2: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refers to the annotation results of Post-training LRAs using the dynamic evaluation component.

combination, we simulate the dynamic annotation process, calculating average annotation demands and employing an exponential model to forecast annotation needs with escalating model numbers. As depicted in Figure 3, our protocol with dynamic component demonstrates a nearly linear growth in annotation demands as the number of models increases, greatly reducing the evaluation costs.

**Reliability.** We advocate for the reliability of the module both at the design level of the algorithm and the result level. Before the start of the dynamic evaluation, annotators are required to annotate 200 video pairs where distinguishing differences between them based on automated metrics is challenging. This step ensures that the samples most deserving of human assessment will not be discarded in the dynamic assessment and that the initially estimated model scores are not biased by specific prompt types, as elaborated in detail in the Appendix C.4.

To enhance the demonstration of this module's reliability, we perform bootstrap confidence intervals for the score estimates of each model across various metrics. As shown in Figure 3, the confidence intervals for Latte, Pika, TF-T2V, and Videocrafter2 are consistently narrow, signifying precise estimations. In contrast, the confidence intervals for Gen2's scores are relatively wide, indicating less stable estimations. This variance primarily stems from our dynamic algorithm's frequent exclusion of comparisons involving Gen2 due to its significant superiority over the other models. Table 4 details the number of these omissions. Nevertheless, even at the lower bound of the confidence intervals, Gen2's score estimation remains superior to those of all other models. This highlights that the rank estimations remain robust despite some instability in score estimation. Thus, our dynamic evaluation provides reliable and consistent rank estimations while requiring fewer annotations.

## 6 Limitations

Our study conducts well-established human evaluation experiments on five state-of-the-art video generation models, yet some limitations persist. First, because the T2V models used are relatively new and contain two closed-source models, we do not offer technical improvement suggestions. Secondly, there is potential for refining our dynamic evaluation algorithm. As the number of models evaluated increases, improvements can be made to the initial settings of model strength.

## 7 Conclusion

To address the issues of reproducibility, reliability, and usability in previous human evaluation protocols for T2V generation, this paper introduces the T2VE protocol. By employing well-defined metrics, thorough annotator training, and a dynamic evaluation module, T2VE enables researchers to obtain high-quality annotations by LRAs at low costs. We anticipate that future research could build upon this protocol to further expand the study of human evaluations of generative models.

Figure 3: The left figure shows how the number of annotations required for different protocols. The right figure represents model score estimations across different metrics. Each boxplot illustrates the median, interquartile range, and 95% confidence intervals of the estimates.