# On the Scalability of Certified Adversarial Robustness with Generated Data

Thomas Altstidl\({}^{1}\) David Dobre\({}^{3}\) Arthur Kosmala\({}^{5}\) Bjorn Eskofier\({}^{1,2}\)

Gauthier Gidel\({}^{3,4}\) Leo Schwinn\({}^{5}\)

\({}^{1}\) Machine Learning and Data Analytics Lab, FAU Erlangen Nurnberg, Germany

\({}^{2}\) Institute of AI for Health, Helmholtz Zentrum Munchen, Germany

\({}^{3}\) Mila, Universite de Montreal, Canada \({}^{4}\) Canada CIFAR AI Chair

\({}^{5}\) Data Analytics and Machine Learning, Technische Universitat Munchen, Germany

{thomas.r.altstidl,bjoern.eskofier}@fau.de

{david-a.dobre,gidelgau}@mila.quebec {a.kosmala,l.schwinn}@tum.de

###### Abstract

Certified defenses against adversarial attacks offer formal guarantees on the robustness of a model, making them more reliable than empirical methods such as adversarial training, whose effectiveness is often later reduced by unseen attacks. Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption. Gowal et al. and Wang et al. have shown that generating additional training data using state-of-the-art diffusion models can considerably improve the robustness of adversarial training. In this work, we demonstrate that a similar approach can substantially improve deterministic certified defenses but also reveal notable differences in the scaling behavior between certified and empirical methods. In addition, we provide a list of recommendations to scale the robustness of certified training approaches. Our approach achieves state-of-the-art deterministic robustness certificates on CIFAR-10 for the \(_{2}\) (\(=36/255\)) and \(_{}\) (\(=8/255\)) threat models, outperforming the previous results by \(+3.95\) and \(+1.39\) percentage points, respectively. Furthermore, we report similar improvements for CIFAR-100.

## 1 Introduction

Deep learning models have been successfully applied for a variety of different applications. However, it is widely recognized that the vulnerability of neural networks to adversarial examples  remains an open problem and hinders their adoption in safety-critical domains. Prior research on improving the robustness of neural networks against adversarial examples can be broadly classified into empirical  and certified approaches .

Adversarial training is currently the most prominent empirical robustification method . Here, the training data of neural networks is augmented with adversarial examples, improving the robustness against attacks at inference time. Recent work has demonstrated that adversarial training can be considerably improved using synthetically generated data, even without training the generative model with external data . Nevertheless, empirical robustness has repeatedly been shown to be ineffective against more sophisticated attacks developed in subsequent work .

In contrast to empirical methods, certified approaches yield robustness guarantees given a predefined threat model, most often based on the \(_{1}\), \(_{2}\), or \(_{}\) norm. As a result, these methods provide reliable protection against future attacks. Nevertheless, the robustness guarantees achieved by certificationmethods are generally substantially lower than the robustness obtained by empirical defenses for the same threat model [3; 4; 8].

In this work, we aim to analyze how well certified robustness scales when utilizing additional data generated by diffusion models during the model training, a recipe that has previously proven successful for empirical robustness. Only Hu et al.  have already used generated data, and thus a broader in-depth review of the factors influencing scalability is to date missing. In our empirical study, we analyze models trained to be robust against \(_{}\) and models trained to be robust against \(_{2}\) norm attacks. The proposed approach improves robustness for both the \((_{},=8/255)\) and \((_{2},=36/255)\) threat models on CIFAR-10, improving upon the previous results in the literature by \(3.95\%p\) and \(1.39\%p\) (percentage points), respectively. In most experiments, the increase in certified accuracy is accompanied by an increase in accuracy on clean data, where we observe improvements by up to \(4.83\%p\). Figure 1 summarizes the improvements compared to the previous state-of-the-art with respect to clean and certified accuracy on CIFAR-10. Further experiments show that the same approach considerably improves certified accuracy on CIFAR-100 as well.

Moreover, we conduct ablations to evaluate the impact of different design choices, including regularization, the number of training epochs, the optimization schedule, and the optimal balance between real and generated data. We summarize the most important insights of this empirical study in a list of recommendations that can be followed to more accurately compare and improve the robustness of deterministic certified defenses. Lastly, we discern crucial differences in the scaling behavior between empirical and certified methods. All code used to produce the results and figures in this paper will be released on GitHub after publication.

## 2 Related Work

Empirical Robustness.Adversarial training was first introduced by Goodfellow et al. . The authors employed the single-step Fast Gradient Sign Method (FGSM) to craft adversarial examples during training and thereby robustify the model against these attacks. Later research by Madry et al.  demonstrated that single-step adversarial training does not yield considerable robustness against multi-step attacks. They showed that using the multi-step Projected Gradient Descent (PGD) attack during training successfully improves the robustness of neural networks at test time, even against strong attacks. Subsequent work proposed improvements to the loss function, the adversarial attack used during training, and better trade-offs between clean and certified accuracy [11; 12].

Certified Robustness.Unlike empirical methods, certified methods yield robustness guarantees, thereby eliminating possible vulnerabilities to future attacks. Certification methods can be broadly classified into two methodologically distinct groups, namely probabilistic and deterministic methods. Probabilistic methods aim to approximate smooth classifiers using Monte Carlo sampling and noise injection . A given sample is verified as robust with a certain probability depending on the noise magnitude and number of Monte Carlo samples. To obtain a tight verification bound, probabilistic methods need to perform a substantial amount of sampling procedures (forward passes)

Figure 1: Certified and clean accuracy of top-ranked models on CIFAR-10 taken from the SoK Certified Robustness for Deep Neural Networks  leaderboard. By using data generated by an elucidating diffusion model (EDM), accuracy significantly improves for four different models and two different norms (\(_{}\) and \(_{2}\)). Grey arrows indicate improvements stemming from this data augmentation.

for each sample, considerably increasing the computational overhead in practice. Contrary to probabilistic methods, deterministic approaches do not entail considerable computational overhead during inference.

Convex bound propagation [13; 14; 15; 16] is a group of deterministic methods that leverages interval arithmetic and linear programming to track how perturbations in the input space transform through each layer, effectively constructing an outer envelope that contains all possible network outputs for inputs within the specified perturbation region. Our initial results scaling the work by Palma et al. , provided in Appendix H, show that this yields severely deteriorated performance when training with additional generated data and is thus a poor candidate for scaling certified robustness.

Our main focus is hence devoted to deterministic approaches that bound the Lipschitz constant of each neural network layer to be small (generally smaller or equal to \(1\)) for a predefined \(_{p}\) norm [17; 18]. The Lipschitz constant of the whole network is bounded by the multiplication of the Lipschitz constants of the individual layers . Given a network's upper bound of the Lipschitz constant, a robustness guarantee can be trivially obtained by computing the distance between the highest two logits in the output space.

Diffusion Models.More recently, diffusion models have superseded generative adversarial networks (GANs) as the preferred method for image generation . Denoising diffusion probabilistic models (DDPM)  can generate high-quality samples on CIFAR-10  with an FID score of \(3.17\), a common measure of image quality. Since then, other variants have been proposed [23; 24]. By further analyzing the design space of these models , elucidating diffusion models (EDMs) achieve a current state-of-the-art FID score of \(1.79\) on CIFAR-10. With additional discriminator guidance , the quality of these EDM-generated images are reported to reach an FID score of 1.64, the best score reported in literature for CIFAR-10 at the time of writing.

Improving Empirical Robustness with Auxiliary Data.Hendrycks et al.  showed that utilizing additional data from external datasets during adversarial training can improve empirical adversarial robustness. Gowal et al.  extended this approach to synthetically generated data from generative models only trained on the source dataset. Recently, Wang et al.  showed that leveraging the latest advances in diffusion models further improves empirical adversarial robustness.

In this work, we investigate if leveraging data generated with state-of-the-art diffusion models can also improve certified robustness against adversarial attacks and analyze how certified training approaches can be scaled optimally.

## 3 Experiment Setup

Given the recent improvements in adversarial training using additional data generated by diffusion models, we devise a set of experiments to investigate whether this also transfers to certified robustness. We focus on deterministic methods as probabilistic methods entail a tremendous computational overhead during inference time and do not achieve considerable robustness for the \(_{}\) norm yet . All our experiments are done on a single Nvidia A100 graphics card (40GB of VRAM) without distributed training.

### Dataset and Threat Models

We perform experiments on CIFAR-10 and CIFAR-100 , for which EDM-generated data is readily available and a wealth of previous robustness research exists . Our experiments and ablation studies focus on CIFAR-10. We refrain from experiments on larger datasets like ImageNet  as robustness guarantees achieved by deterministic methods for these datasets are still comparatively low, and only  support it at the time of writing. We perform experiments on two common threat models, specifically \((_{},=8/255)\) and \((_{2},=36/255)\) adversaries. We do not consider the \(_{1}\) threat model, as only smoothing-based approaches achieve considerable robustness for this threat model at the time of writing. For our experiments, we select the two best architectures from the popular certified robustness leaderboard introduced by Li et al.  for both the \(_{2}\) (GlorotNet  and LOT ) and \(_{}\) threat models (SortNet  and \(_{}\)-dist Net ). In total, we perform experiments on architectures from four different papers.

### Generated Auxiliary Data

To explore the effectiveness of augmenting the original CIFAR-10 and CIFAR-100  datasets with generated data, we adjust the data loader of each model to use a fraction of generated data and original data in every epoch. We use the same generated data used by Wang et al. , which was produced by an EDM trained only on the train set of CIFAR-10. In a preliminary experiment, we found the generated-to-real ratio to be optimal when \(30\)% of training images are real and \(70\)% are generated in every epoch during training, matching the ratio used by Wang et al. . We performed experiments with \(50,000\) (50k), \(100,000\) (100k), \(200,000\) (200k), \(500,000\) (500k), \(1\) million (1m), \(5\) million (5m) and \(10\) million (10m) generated images. Wang et al.  sub-sampled the 1m images from 5m images choosing only the \(20\)% most confidently classified images according to a pretrained WRN-28-10 model. In contrast, we naively sub-sample the 1m images from the 5m image dataset to avoid potential selection bias by the classifier used to select the data. Moreover, using the same selection process for all datasets should allow us to assess better the effect of the amount of generated data on the final robustness.

### Hyperparameters

With additional data, it is also expected that both model size and the number of training epochs can be further scaled to improve clean accuracy and robustness. We thus perform experiments on the influence of model depth and the number of epochs on clean and certified accuracy. For some models, we investigate further techniques that add learning capacity. Concretely, for SortNet  we also experiment with models that do not employ dropout, and for LOT  we adjust the learning rate scheduler to cosine annealing .

## 4 Results

In the following, we first summarize the effect of using additional generated data on the achievable certified and clean accuracy. Furthermore, we ablate the effect of other design choices on the certified robustness, such as the number of training epochs, model size, the amount of additional synthetic data and other hyperparameters. Lastly, we summarize our findings and provide a list of recommendations to scale certified robustness effectively.

### Improving Certification Approaches with Generated Data

Across all four reference models and all two threat models, we find that the inclusion of generated data can improve certified accuracy. In most cases, clean accuracy is considerably improved as well. An overview of our new state-of-the-art results in comparison with existing related work is given in Figure 1. For the \((_{},=8/255)\) threat model on CIFAR-10 we can increase the robustness of the existing SortNet  to \(41.78\%\), an improvement of \(1.39\) percentage points. For the \((_{2},=36/255)\) model we achieve a certified accuracy of \(69.05\%\) using LOT , a substantial increase of \(3.95\) points compared to the best result previously reported in the literature . In almost all cases this

    & Ep- &  w/o auxiliary \\  } &  \\   & & Clean & Cert. & Clean & Cert. & \(|_{gen}|\) \\  \)-dist} & 800 & 57.34 & 34.25 & 61.04 & 36.98 & 1m \\  & 1600 & 57.19 & 34.00 & 62.02 & 37.53 & 1m \\  SortNet & 3000 & 53.38 & **39.72** & 53.29 & 41.32 & 10m \\ \(=.85\) & 6000 & 53.36 & 39.05 & 52.41 & 40.70 & 1m \\  SortNet & 3000 & 56.09 & 37.44 & 54.36 & 41.71 & 5m \\ \(=.00\) & 6000 & 54.81 & 36.50 & 54.75 & **41.78** & 10m \\   

Table 1: Clean and certified test accuracy (%) on **CIFAR-10** (\(_{},=8/255\)) for \(_{}\)-dist Net and SortNet with dropout rate \(\). Bold highlights the best model with and without auxiliary data. \(|_{gen}|\) denotes the number of EDM-generated images at which highest accuracy was achieved.

improvement coincides with an increase in clean accuracy. One exception is SortNet, where the clean accuracy slightly decreases from \(54.84\%\) to \(54.75\%\).

Full results are given in Tab. 1 for both SortNet and \(_{}\)-dist Net, as well as Tab. 2 for LOT and Tab. 3 for GloroNet. We find that by removing the dropout from SortNet we can improve certified accuracy from \(41.32\%\) to \(41.78\%\) when using auxiliary data. However, the same leads to a drop from \(39.72\%\) to \(37.44\%\) with only the original data, reinforcing the notion that the additional data acts as a good regularizer. Similarly, for LOT, cosine annealing is superior by a margin of up to \(1.12\) points compared to a multi-step scheduler, indicating that the model can make better use of its capacity when trained with auxiliary data. For full results using the multi-step scheduler, we refer to App. B, Tab. 2 - all results discussed in subsequent sections refer to those obtained with cosine annealing.

Adding auxiliary data improves certified robustness on CIFAR-100 as well, as demonstrated in Tab. 4. We restrict our evaluation to the most effective models from the analysis on CIFAR-10 and do not further scale the number of training epochs. The most substantial improvements on CIFAR-100 are obtained for the SortNet model, where certified accuracy increases by \(8.08\) percentage points from \(9.2\%\) to \(17.28\%\), and for GloroNet, where certified accuracy increases by \(2.49\) percentage points from \(36.41\%\) to \(38.9\%\).

### Sensitivity Analysis

Scaling the Amount of Auxiliary Data.The main goal of this work was to evaluate the influence of additional synthetic data during training on the achievable certified accuracy of deterministic certification methods. To this end, we analyze how different amounts of additional data affect the final certified robustness. We evaluate the saturation by training with with 50k, 100k, 200k, 500k, 1m, 5m, and 10m auxiliary generated images. All models are trained for at least twice the amount of epochs that were used in the original papers to ensure saturation in terms of training time. As seen in Fig. 2, improvements in certified robustness beyond 1m are mostly negligible. This behavior is also largely independent of model size, which stands in contrast to prior results reported for adversarial training .

Scaling the Model Size.We performed experiments on several different model sizes to investigate possible correlations between the benefit of additional training data and the model capacity. The \(_{}\)-based models are largely constructed out of fully connected layers. As a result, the computational effort when scaling these models increases quadratically. As experiments on the \(_{}\)-based models proved to be computationally too expensive we refrain from scaling these models and focus instead on \(_{2}\)-based models.

Tables 1 to 3 demonstrate that scaling the model size can increase the certified robustness for both LOT and GloroNet. Shown are the best improvements when adding 1m, 5m, or 10m auxiliary data when compared to the same model trained without any auxiliary data - referred to as the _base model_ from here on. The highest gains are for medium models with LOT and for large models with

Figure 2: Influence of total CIFAR-10 dataset size \(|_{orig}|+|_{gen}|\) (number of original and generated images) on certified accuracy. \(\) is the dropout rate of SortNet. All models were trained with a large number of epochs, i.e., 1600 for \(_{}\)-dist Net, 6000 for SortNet, 600 for LOT, and 2400 for GloroNet. Accuracy generally improves little beyond 1m generated images.

GloroNet. For GloroNet in particular we observe that model size becomes more important the longer the model is trained, with all models trained for the default \(800\) epochs showing similar gains.

Scaling the Number Training Epochs.As larger models and additional training data may require longer model training to achieve optimal results we increased the number of training epochs compared to the original configurations for all tested models1. For the \(_{}\)-based models in Tab. 1 we see that by doubling the number of epochs we can further improve certified robustness when regularization is removed. A similar picture arises for the \(_{2}\)-based models, presented in Tabs. 2 and 3. Here, we find that an increase in the number of epochs yields a significant improvement regardless of model size. Overall, this parameter had the strongest impact in combination with auxiliary data and all best certified accuracies are achieved at their respective maximum number of epochs, with the only exception being SortNet with dropout.

### Relationship between Generalization Gap and Certified Robustness

The individual improvements in certified robustness vary considerably between different model and data configurations in our experiments. One possible explanation for these differences may be that the generalization gap of the respective models trained without auxiliary data - i.e., the _base models_ - are different, leading to different gains when closing this generalization gap. To investigate this, we

   Size & Ep- &  &  \\   & ochs & Clean & Cert. & Clean & Cert. & \(|_{gen}|\) \\   & 200 & 76.60 & 63.45 & 79.22 & 66.17 & 10m \\  & 400 & 76.50 & 63.48 & 80.02 & 67.42 & 5m \\  & 600 & 76.75 & 63.81 & 80.59 & 68.07 & 10m \\   & 200 & 76.92 & 63.38 & 79.59 & 66.95 & 5m \\  & 400 & 76.41 & 63.93 & 80.53 & 68.17 & 5m \\  & 600 & 76.49 & 63.63 & 80.98 & 68.66 & 5m \\   & 200 & 77.21 & **64.53** & 79.91 & 67.58 & 10m \\  & 400 & 77.00 & 64.47 & 80.80 & 68.66 & 5m \\   & 600 & 76.62 & 64.39 & 81.42 & **69.05** & 5m \\   

Table 2: Clean and certified test accuracy (%) on **CIFAR-10** (\(_{2},=36/255\)) for LOT. Bold highlights the best model with and without auxiliary data. \(|_{gen}|\) denotes the number of EDM-generated images at which highest accuracy was achieved.

   Size & Ep- &  &  \\   & ochs & Clean & Cert. & Clean & Cert. & \(|_{gen}|\) \\   & 800 & 76.51 & 63.44 & 77.42 & 65.57 & 5m \\  & 1600 & 76.95 & 63.79 & 78.38 & 66.48 & 10m \\  & 2400 & 77.56 & 64.14 & 78.54 & 66.60 & 10m \\   & 800 & 77.22 & 64.33 & 77.90 & 66.23 & 10m \\  & 1600 & 77.91 & 64.68 & 78.74 & 66.78 & 1m \\  & 2400 & 78.28 & 64.79 & 78.89 & 67.07 & 1m \\   & 800 & 77.73 & 64.81 & 77.95 & 66.59 & 5m \\  & 1600 & 77.77 & 65.06 & 79.18 & 67.31 & 10m \\  & 2400 & 78.41 & 64.87 & 79.43 & 67.56 & 10m \\   & 800 & 77.94 & 65.09 & 79.13 & 67.06 & 10m \\  & 1600 & 78.99 & 65.16 & 79.81 & 67.67 & 1m \\   & 2400 & 79.33 & **65.21** & 80.28 & **68.12** & 5m \\   

Table 3: Clean and certified test accuracy (%) on **CIFAR-10** (\(_{2},=36/255\)) for GloroNet. Bold highlights the best model with and without auxiliary data. \(|_{gen}|\) denotes the number of EDM-generated images at which highest accuracy was achieved.

correlate the difference in generalization gap with the improvement in certified accuracy obtained when adding auxiliary data. Here, generalization gap refers to the difference between the train and test accuracy on clean data for the best epoch. Figure 3a demonstrates a considerable correlation between the decrease in generalization gap compared to the base model trained with no auxiliary data and the improvement in certified robustness for models with auxiliary data. Moreover, we perform a line fit between the generalization gap and robustness improvement for all the analyzed models. Surprisingly the slope of the different lines is similar for most models, except \(_{}\)-dist Net, indicating that robustness gains can be predicted once the offset of the line is known for unseen models. However, the offset of the different lines depends on the base model considered. Generalization gaps for each auxiliary dataset size \(|_{gen}|\) in Fig. 3b also correlate well with scaling curves in Fig. 2.

These results are in line with certified robustness gains achieved for SortNet with and without dropout shown in Tab. 1. Here, the certified robustness of the SortNet model trained without auxiliary data and for \(3000\) epochs decreases when using less regularization by removing dropout from \(39.72\%\) to \(37.44\%\). At the same time, the generalization gap of the two models increases from \(4.16\%\) to \(12.89\%\), respectively, as an effect of removing dropout. However, once additional synthetic data is used, removing dropout actually improves the certified robustness to up to \(41.71\%\) by nearly two points. Here, increasing the generalization gap by removing dropout had a positive effect on the final certified robustness, which fits observations in Fig. 3a.

### Ratio of Generated and Real Data

In every training epoch, we use a proportion of synthetic and real images and keep the total amount of images the same as the size of the original training set. The default configuration throughout our experiments, and the one also used by Wang et al. , is to use 30% real images and 70% generated images in each batch. Figure 3c illustrates how using different proportions for generated and real data affects the certified robustness of the \(_{}\)-dist Net and LOT-S architectures. We see that at ratios of 60% generated data the clean accuracy saturates, and with 70% the certified accuracy saturates. Notably, in all cases the accuracy when only training with generated images was higher than when only training with real images, indicating that it may be possible to fully train these models on only generated data in the future.

### Certification Radius Distribution

To examine the underlying factors contributing to the observed increase in robustness when using auxiliary data, we conduct an analysis of the certification radius distribution for the SortNet (without dropout) and LOT-L architectures. Figure 4 displays the number of images on the \(y\)-axis with a certification radius equal to or above a specific value, shown on the \(x\)-axis. Curves are plotted for the

Figure 3: **a** Correlation between generalization gap and certified accuracy improvement. Generalization gap measures difference between training and testing accuracy. \(\) refers to difference between base model and model trained with auxiliary data. **b** Generalization gaps by amount of auxiliary data \(|_{gen}|\) for all models trained with maximal epoch count. **c** Clean and certified accuracy (%) for different ratios of generated and real data for \(_{}\)-dist Net and LOT-S. Here, a generated-to-original ratio of 70 means 70% of each batch is generated data and the remaining 30% is real data.

best models trained with and without auxiliary data. Additionally, curves for correct and incorrect classifications are displayed separately.

We observe no considerable differences between the distribution of certification radii for the LOT model obtained with or without auxiliary data. Nevertheless, models trained with auxiliary data show slightly higher robustness for correctly classified samples and lower robustness for misclassified samples on average. The SortNet architecture exhibits considerably higher robustness radii for both correct and incorrect classifications when using auxiliary data. Here, differences in certified robustness do not seem to come from a better generalization ability on clean data but from larger certification radii on unseen data. On the other hand, the LOT architecture shows similar certification radii but considerably better generalization on clean data. A more detailed analysis is given in App. G, Fig. 3. Both models show considerable over-robustness for a considerable fraction of the test set, where the certification radius is well beyond the certification goal \(\).

### Takeaways for Scaling Certified Robustness

Differences in certified robustness between distinct defense approaches are often marginal and even small improvements over prior work may be relevant. Here, we summarize the most important takeaways from the empirical study presented in this work on how to scale the robustness of deterministic certified models.

* **Scale the number of training epochs.** Among all investigated hyperparameters, we found the number of training epochs had the most consistent effect on certified accuracy when training with auxiliary data. Based on our experiments, we expect the amount of auxiliary data to not matter as long as it is sufficient for closing the generalization gap.
* **Increase your model capacity.** When using auxiliary data, a large generalization gap between training and testing accuracies is less of an issue and, based on our results, indicative of untapped performance improvements that can be leveraged (see Sec. 4.3). This means model capacity can be scaled with little fear of overfitting. Our experiments show that reducing the amount of regularization, using better optimizers, and increasing the model size all improve certified accuracy when using auxiliary data without leading to large generalization gaps.
* for example, contrary to results reported in the original papers, our results indicate that LOT may actually be superior to GloroNet.
* **Benchmark with generated auxiliary data.** As the proposed approach does not entail a computational overhead for the same amount of training epochs, we recommend future work to compare their approaches using auxiliary data and ensure that models are trained till convergence. Differentiating between approaches that use auxiliary data and those that only utilize the original dataset may be helpful for future benchmarks. Similar approaches have been adopted in the empirical robustness domain .

Figure 4: Cumulative distribution of certification radii for the best \(_{}\)-based model, Sortnet w/o dropout, and the best \(_{2}\)-based model, LOT-L. Note how for SortNet images exhibit overall smaller certification radii for both correct and incorrect classes, yet clean accuracy slightly decreases.

* **Decrease over-robustness.** While not related to the usage of auxiliary data, our evaluation in Fig. 4 indicates that a considerable number of samples are noticeably more resistant to adversarial attacks than what was intended during training. Future research may consider using smaller certification objectives for samples that already demonstrate considerable robustness, an approach that has already been shown to be successful in adversarial training .

## 5 Comparison to Empirical Robustness

While we observe many of the same trends as Gowal et al.  and Wang et al. , i.e., that larger models and more epochs generally help, we also note a few crucial differences.

* **Amount of training data.** Scaling beyond one million CIFAR-10 generated images did not further improve certified accuracy, regardless of model size. This is different to scaling behavior reported previously for adversarial training, with, e.g., Gowal et al.  reporting an improvement of \(+2.65\) and \(+2.52\%\) when scaling their WRN-23-10 and WRN-70-16 models, respectively, from one million to 100 million generated images. One hypothesis would be that this is due to the consistency property of the Glivenko-Cantelli class, mentioned by Bethune et al. in Section 5.1 , due to which Lipschitz-1 neural networks' training loss converges to the testing loss for increasingly large datasets. In contrast, adversarial training can be interpreted as a data-dependent operator norm regularization  and seems to require more data samples to close the generalization gap. More recent work suggests that the Bayes error may limit the achievable accuracy of both deterministic  and probabilistic  certified robustness, in line with our results.
* **Overfitting between best and last epoch.** In adversarial training, prior work finds that the difference between the best and the last epoch becomes increasingly smaller with larger amounts of auxiliary data . In contrast, we observe no such effect for certifiably robust models (see App. E, Fig. 1), highlighting another difference in scaling behavior between empirical and certified robustness.
* **Optimal generated-to-original ratio.** Previous research regarding the optimal generated-to-original ratio for adversarial training observes a drop-off when using only generated data. Both Gowal et al.  (Fig. 5) and Wang et al.  (App. B, Fig. 3) suggest accuracy decreases again significantly beyond 70% generated data when using diffusion models. As evident in Fig. 3c this is not as pronounced for certifiably robust models.

Together, these results suggest that for certifiably robust models it may be possible to determine a sufficient amount of generated data beforehand for any given dataset - contrary to what is the case for adversarial training, where more is always better. Moreover, it indicates that certified robustness is considerably harder to scale than empirical robustness, as once saturation with respect to data has been achieved, further gains can only be attained by better algorithms and increased model size. Future experiments should explore concrete scaling laws for certified and empirical adversarial robustness, which was out-of-scope of this paper. Deriving and verifying theoretical properties of certifiably robust models, such as those already known on graphs , also remains an exciting topic for further research.

   Architecture &  &  \\   & Clean & Cert. & Clean & Cert. & \(|_{gen}|\) \\  \(_{}\)-dist Net & 25.99 & 9.36 & 27.73 & **10.47** & 10m \\ SortNet \(=.00\) & 24.93 & 9.20 & 27.58 & **17.28** & 1m \\ LOT-L & 46.60 & 32.92 & 50.68 & **36.56** & 5m \\ GloroNet-L & 51.57 & 36.41 & 51.71 & **38.90** & 10m \\   

Table 4: Clean and certified test accuracy (%) on **CIFAR-100** for both (\(_{},=8/255\)) and (\(_{2},=36/255\)) threat models. Bold highlights the best overall model for each architecture.

## 6 Limitations

Despite our best efforts and extensive experiments, some limitations remain. Our focus is largely on deterministic Lipschitz-bound methods, which incidentally were used in the two best-performing models for the \((_{},=8/255)\) and \((_{2},=36/255)\) attacks on CIFAR-10 at the time the experiments were performed . Other approaches for certified robustness, including probabilistic ones and other deterministic methods, stay largely unexplored except for our experiments on convex bound propagation in Appendix H. The inherently high computational complexity of the evaluated models [9; 17; 18; 27] also inhibited us from running each configuration multiple times, as even a single run already results in an overall compute time of around five thousand GPU hours. Finally, we made the conscious choice to not perform experiments on ImageNet as only one of the evaluated models, that by Hu et al. , supported this dataset and thus no meaningful comparison could be made.

## 7 Conclusion

We show that deterministic certified robustness can be improved by up to \(5.28\%p\) when additional generated data from a diffusion model is used during training. This is true across four different architectures and two different threat models, \((_{},=8/255)\) and \((_{2},=36/255)\), on CIFAR-10, where we report improved certified accuracies of \(41.78\%\) and \(69.05\%\), respectively. For \(_{}\) we are thus able to achieve a new state-of-the-art, while Hu et al.  report a slightly better accuracy of \(70.1\%\) for the \(_{2}\) threat model using data generated by DDPM. In addition, we show the data scaling also improves certified accuracy on CIFAR-100 substantially.

We find that the highest gains can be achieved for models where the generalization gap, i.e., the difference between training and testing accuracy, is high for the original model. When augmenting with generated data, the generalization gap is mostly eliminated across all models when a sufficient amount of additional images are used. As the generalization gap gets smaller, removing regularization techniques, such as dropout, and switching to learning rate schedulers aimed at better convergence yields additional improvements. We also note that increasing the number of epochs had the greatest impact when paired with generated data. Lastly, we observe that a considerable number of samples are noticeably more resistant to adversarial attacks than required by the \(\)-bound.

## Author Contributions

T.A. and L.S. came up with the original idea of using generated data for certified robustness. T.A. implemented the code adjustments, ran the experiments, performed the analysis, prepared all figures/tables, and wrote the manuscript. D.D. supported some experiments with additional compute. All authors contributed to the interpretation of the results and to the editing of the manuscript.