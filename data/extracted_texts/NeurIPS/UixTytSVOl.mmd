# Cross-modal Representation Flattening for

Multi-modal Domain Generalization

 Yunfeng Fan\({}^{1}\), Wenchao Xu\({}^{1,}\)1, Haohao Wang\({}^{2}\), Song Guo\({}^{3}\)

\({}^{1}\)Department of Computing, The Hong Kong Polytechnic University,

\({}^{2}\)School of Computer Science and Technology, Huazhong University of Science and Technology,

\({}^{3}\)Hong Kong University of Science and Technology

yunfeng.fan@connect.polyu.hk, wenchao.xu@polyu.edu.hk,

hz_wang@hust.edu.cn, songguo@cse.ust.hk

###### Abstract

Multi-modal domain generalization (MMDG) requires that models trained on multi-modal source domains can generalize to unseen target distributions with the same modality set. Sharpness-aware minimization (SAM) is an effective technique for traditional uni-modal domain generalization (DG), however, with limited improvement in MMDG. In this paper, we identify that _modality competition_ and _discrepant uni-modal flatness_ are two main factors that restrict multi-modal generalization. To overcome these challenges, we propose to construct consistent flat loss regions and enhance knowledge exploitation for each modality via cross-modal knowledge transfer. Firstly, we turn to the optimization on representation-space loss landscapes instead of traditional parameter space, which allows us to build connections between modalities directly. Then, we introduce a novel method to flatten the high-loss region between minima from different modalities by interpolating mixed multi-modal representations. We implement this method by distilling and optimizing generalizable interpolated representations and assigning distinct weights for each modality considering their divergent generalization capabilities. Extensive experiments are performed on two benchmark datasets, EPIC-Kitchens and Human-Animal-Cartoon (HAC), with various modality combinations, demonstrating the effectiveness of our method under multi-source and single-source settings. Our code is open-sourced 2.

## 1 Introduction

Domain generalization (DG) aims to equip models with the ability to perform robustly across unseen domains when trained only on several source domains, thereby enhancing their adaptability and utility in real-world scenarios, such as autonomous driving , medical health , person re-identification  and brain-computer interface . Methods on how to deal with domain shift have been extensively proposed in the literature, including domain alignment , meta-learning , data augmentation  and ensemble learning . Despite the remarkable achievements of DG in recent years, most of research still focuses on uni-modal data. The emergence of various multi-modal datasets and the requirement to complete a variety of multi-modal tasks highlight the need to address multi-modal domain generalization (MMDG) problems.

Due to the complementary information that exists between modalities, MMDG aims to exploit generalization capabilities from each modality simultaneously. According to , the generalization capability of deep neural networks (DNNs) is closely related to their flatness of minima on losslandscape (as shown in Fig. 1 (a)), which motivates penalizing sharpness  and rewarding flatness . Sharpness-aware minimization (SAM)  and its variants  have been proposed to seek flatter minima and achieve better generalization across domains. Despite their success on uni-modal scenarios, in this paper, we argue that they are not compatible well in MMDG since the distinct properties between modalities pose two challenges (more details can be found in Sec.3.2). (1) **Modality competition**: according to , multiple modalities will compete with each other during joint training, leading to inadequate knowledge exploitation for each modality , i.e, larger minima of loss as shown in Fig. 1 (b), and consequently worse generalization. (2) **Discrepant uni-modal flatness**: the generalization gap between modalities makes it hard to find their flat minima simultaneously, resulting in multi-modal networks incapable of utilizing generalization capabilities from all modalities, as illustrated in Fig. 1 (c). Hence, existing methods can not fully exploit the generalization potential of each modality, which inevitably leads to sub-optimal solutions for MMDG.

To overcome these challenges, we propose to construct consistent flat loss regions and enhance knowledge exploitation for each modality via cross-modal knowledge transfer. Traditional SAM-based methods are analyzed on parameter space. However, due to the heterogeneity between modalities, their parameter spaces could be extremely different (e.g., different model structures and parameter numbers), making it challenging to represent their correlation. Instead, we turn to optimization on representation-space loss landscape  as representations of different modalities can be mapped into a shared space, so that we can build their connections directly. Based on this, we propose a novel **C**ross-**M**odal **R**epresentation **F**lattening (CMRF) method to achieve consistent representation flat minima. As shown in Fig. 1 (d), we construct the interpolations by mixing paired multi-modal representations and then optimize them to flatten the high-loss regions between minima from different modalities. Specifically, we obtain more stable and generalizable cross-modal interpolations from moving averaged teacher model and then employ feature distillation to regularize the learning of each modality. The interpolations between modalities bring their flat regions closer, alleviating their flatness discrepancy. Moreover, the cross-modal knowledge transfer also helps to promote each modality and alleviate their competition. Our contributions can be summarized as:

* To the best of our knowledge, we are the first to extend the uni-modal flatness analysis to MMDG, and empirically attribute the reasons for limited MMDG performance to two problems: modality competition and discrepant uni-modal flatness.
* We construct shared representation space instead of parameter space to build connections between modalities directly and propose to flatten high-loss representation regions between modalities by interpolating mixed multi-modal representations and performing knowledge distillation to regularize the learning of each modality.
* Extensive experiments verify the effectiveness and superiority of our framework on two benchmark datasets of EPIC-Kitchens and Human-Animal-Cartoon (HAC) under various modalities combinations on both multi- and single-source MMDG.

Figure 1: (a) Flat minima on loss landscape generalize better than sharp minima with domain shift. (b) Multi-modal joint training leads to larger loss for each modality compared with independent uni-modal training. (c) The flat minima between modalities are usually inconsistent, making it hard to obtain flat minima for each modality simultaneously in a multi-modal network. (d) We optimize the cross-modal interpolations on representation-space loss landscape to get consistent flat region.

Related Work

**Flat Minimum of Loss Landscape for DG.** Domain generalization refers to the ability of models to perform well on new, unseen domains that are dissimilar with domains they were trained on. Numerous methods have been proposed to tackle the domain shift, while one type among them is to search for flat minima in loss landscapes [18; 24; 19]. Jiang _et al._ conducted comprehensive measures and found that a sharpness-based measure has highest correlation with generalization. Based on that, Foret _et al._ proposed sharpness-aware minimization (SAM) to seek parameters that lie in neighborhoods with uniformly low loss via perturbed gradients, while Wang _et al._ further proposed to align the gradient directions between the empirical risk and the perturbed loss. Moreover, average weights during training has also shown to yield flatter minima , which motivates more elegant average methods such as SWAD  and EoA . In this paper, we try to optimize consistent flat minima for different modalities in representation-space loss landscapes instead of traditional parameter space.

**Multi-modal DG.** Although uni-modal DG has been extensively studied in recent years, the research on MMDG is severely insufficient, while only few works have been done. Planamente _et al._ proposed RNA-Net to balance audio and video feature norms via a relative norm alignment loss. Dong _et al._ proposed a unified framework to achieve domain generalization in various multi-modal scenarios including multi-source, uni-source, and modality missing DG. In this paper, we extend the uni-modal flatness analysis to MMDG and address two particular problems in multi-modal scenarios.

**Mixup.** Mixup  is a data augmentation technique introduced to improve the generalization performance of models. Traditional mixup and its variant CutMix  are performed on input data, while Verma _et al._ further introduced Manifold Mixup that mixes the representations in each layer to produce smoother decision boundaries. However, Manifold Mixup and its variants [32; 33] are designed for uni-modal data, and only few works are on multi-modal scenarios [34; 35]. STEMM  aims to align speech and text features by mixing them, but is limited with its architecture-specific design. Oh _et al._ introduced \(m^{2}\)-Mix aiming at generating hard negative samples by mixing image and text embeddings to fine-tuning CLIP. Compared with them, our mixed multi-modal representations has no architecture restrictions and are used as teacher signals to guide various modalities to learn consistent flat minima.

## 3 Method

### Preliminaries

We follow the definition of multi-modal domain generalization problem as in . In MMDG, we are given \(D\) source domains for training \(_{train}=\{^{i}|i=1,,D\}\), where \(^{i}=\{(_{j}^{i},y_{j}^{i})\}_{j =1}^{n_{i}} P_{XY}^{i}\) denotes the \(i\)-th domain with \(n_{i}\) data instances sampled from a joint distribution of input samples and output labels \(P_{XY}^{i}\). \(X\) and \(Y\) represent the corresponding random variables. Each input instance \(_{j}^{i}=\{(_{j}^{i})_{k}|k=1,,M \}\) consists of \(M\) different modalities and \(y_{j}^{i}\) denotes corresponding label, where \(\) and \(\) represent input and output space. The joint distributions in \(_{train}\) are different from each other: \(P_{XY}^{i} P_{XY}^{j},1 i j D\). Now, with an unseen test domain \(_{test}\) with \(M\) modalities that cannot be accessed during training and \(P_{XY}^{test} P_{XY}^{i}\) for \(i\{1,,D\}\), the goal of MMDG is to learn a robust and generalizable predictive function \(f:\) based on \(D\) training domains to achieve a minimum prediction error on \(_{test}\):

\[_{f}\,_{(,y)_{test}}[ (f(),y)]\] (1)

where \(\) is the expectation and \((,)\) is the loss function, e.g., cross-entropy loss for multi-modal classification tasks. In this paper, we use \(=\{_{1},,_{M}\}\) to denote the parameters of the neural network \(f\), where \(_{i}\) indicates the parameters for \(i\)-th modality. Therefore, the training loss over all training domains \(_{train}\) is defined as follows:

\[(;_{train})=^{D}n_{ i}}_{i=1}^{D}_{j=1}^{n_{i}}(f(_{j}^{i}; ),y_{j}^{i})\] (2)The empirical risk minimization (ERM) of Eq. 2 tends to converge to sharp minima and SAM  is proposed to seek flatter minima on loss landscape with the following optimization:

\[_{}\,(+;_{train}),\,\,\,\, (;_{train})}{\|(; _{train})\|}.\] (3)

where \(\) is a predefined constant controlling the radius of the neighborhood.

### MMDG Analysis

MMDG aims to comprehensively exploit the generalization capabilities from each modality to learn more robust and generalized models. However, the generalization behavior of each modality in multi-modal networks has not been well explored. Here, we analyze the behavior of each modality and find the challenges for generalizable multi-modal networks.

**Modality competition leads to larger minima.** As demonstrated in Tab. 1, we compare naive joint training and SAM about their uni- and multi-modal performance. SAM can clearly improve generalization on both uni-modal and multi-modal training. However, the uni-modal generalization from multi-modal trained network is worse than uni-modal trained network, whether or not SAM is applied (e.g, 56.65% vs. 58.73% without SAM and 58.80% vs. 61.68% with SAM on EPIC-Kitchens video). This phenomenon can be explained by modality competition [20; 36] that modalities in joint training compete with each other, making each modality under-explored. Our empirical results show that it not only degrades in-domain performance for each modality as discussed in [37; 38], but also weakens their out-of-domain generalization, resulting in larger minima of loss as shown in Fig. 1 (b).

**Generalization gap results in discrepant uni-modal flatness.** We observe that applying SAM can only improve generalization of better modality in multi-modal network but has marginal benefit or even harm on weak modality (e.g., video generalization is improved from 56.65% to 58.80% on EPIC-Kitchens while the number of audio drops from 38.62% to 37.77%). According to , the better modality will dominate multi-modal gradients. Hence, in Eq. 3, the gradient perturbation \(\) in SAM could also be dominated by the better modality, which means this optimization on multi-modal network tends to search for flatter regions for modality with better generalization but ignores other weak modalities. This suggests that conventional uni-modal SAM-based methods cannot find the coexisting flat minima for each modality due to their generalization gap, leading to discrepant flatness and consequently under-utilization of generalization from all modalities, as shown in Fig. 1 (c). More results with other modality combinations can be found in Sec. 4.2 and Appendix. B.

### Cross-Modal Representation Flattening

Based on the analyses above, in this paper, we aim to 1) accomplish consistent flat minima for all modalities in multi-modal network and 2) alleviate the competition between modalities to utilize their generalization comprehensively. Considering the correlation and complementary information between modalities, we propose to leverage cross-modal knowledge transfer to enhance MMDG.

**Representation-space loss landscape.** Previous analysis of loss landscapes usually happens on parameter space [19; 39]. However, the network structures and sizes for different modalities are

    &  &  \\   & Video & Audio & Video-Audio & Video & Audio & Video-Audio \\  Uni-video & 58.73 & - & - & 68.07 & - & - \\ Uni-audio & - & 40.04 & - & - & 32.81 & - \\ Uni-video-SAM & **61.68** & - & - & 69.58 & - & - \\ Uni-audio-SAM & - & 42.65 & - & - & **35.84** & - \\  Base & 56.65 & 38.62 & 59.63 & 67.60 & 31.24 & 63.11 \\ SAM & 58.80 & 37.77 & 61.19 & 68.46 & 31.56 & 64.72 \\ CMRF (ours) & 60.66 & **43.13** & **63.91** & **70.54** & 34.86 & **71.91** \\   

Table 1: MMDG analysis on EPIC-Kitchens and HAC with video and audio data. ‘Base’ denotes the naive multi-modal joint training without any domain generalization strategies. ‘Uni-video’ and ‘Uni-audio’ means training only with uni-modal data. ‘Video’, ‘Audio’ and ‘Video-Audio’ denote testing with uni-modal and multi-modal data. Results are averaged by using each domain as target.

commonly different, leading to disparate parameter spaces. This makes it difficult to catch correlations between modalities and produce consistent flat loss regions in parameter space. Inspired by  that introduces representation-space loss landscape, we turn to analyze loss landscapes of different modalities in representation space. Specifically, given a data point \(_{j}^{i}=\{(_{j}^{i})_{k}|k=1,,M\}\), feature extractors are usually applied to transform input data into features with different dimensions:

\[(_{j}^{i})_{k}=g_{k}((_{j}^{i} )_{k})^{d_{k}}\] (4)

where \(g_{k}\) is feature extractor for \(k\)-th modality, \(d_{k}\) is feature dimension size and \( k l,d_{k} d_{l}\). In this paper, we use a projector \(Proj_{k}()\) for \(k\)-th modality that maps its features into a shared representation space for all modalities with the same dimension \(d\) (omit superscript and subscript of domain and instance index for simplicity):

\[_{k}=Proj_{k}(_{k})^{ d},\ k\{1,,M\}\] (5)

Given that each point in the representation space corresponds to a specific loss value, it is feasible to construct a landscape that maps each representation point to its associated loss value (e.g., horizontal axis indicates representation and vertical axis indicates loss in Fig. 1 (d)). After training, each representation extracted from each training sample can be viewed as a minimum. And we can judge whether a representation minimum is flat or sharp according to its neighboring loss distribution. In the shared representation loss landscape, we can build connections between different modalities directly.

**Cross-modal representation interpolation.** As discussed in Sec. 3.2, the discrepant uni-modal flatness severely impedes the utilization of generalization capability from each modality. The conclusion also applies to representation-space loss landscape since better modality still dominates gradients of representations, which optimizes weak modalities at sharp regions. Therefore, to obtain flat minima for various modalities simultaneously, we aim to flatten the high-loss regions between minima from different modalities. Given the paired multi-modal representations \(_{k}\) and \(_{l}\), \(k l\), we construct interpolated representations between them by cross-modal representation mixup:

\[_{k,l}=_{k}+(1-) _{l}\] (6)

where \(\) is mixing ratio. If the loss of mixed representations can be optimized to lower values, we would get a flatter region between modalities, as demonstrated in Fig. 1 (d). However, according to , directly optimization on mixed representations requires mixup at multiple eligible layers to be effective. It is impractical in multi-modal scenarios because representations of each layer for different modalities are generally at different scales, converting all them into a shared space is costly. In this paper, we propose a simple yet effective method that distills the knowledge from mixed representations to each modality and then optimize the learned representations. Firstly, we perform

Figure 2: The overall framework of our method. The projectors map features with different dimensions to the same representation space. The teacher model is moving averaged from online model and generates cross-modal mixed representations as interpolations to distill the student representations. Uni-modal classifier is used to lower the loss of distilled features for each modality and a contrastive loss aims to alleviate gap between modalities. Only the online student model back propagates gradients. **The teacher model is used for evaluation finally.**

simple moving average (SMA)  for the online updated network \(_{k}\) of each modality to establish the teacher network \(_{k}^{t}\), which can produce more stable and generalizable representations:

\[_{k}^{t}=\{_{k}^{t},&\ t t_{0} \\ }{t-t_{0}+1}\ _{k}^{t-1}++1}_{k}^{t},& .\] (7)

where \(_{k}^{t}\) is the online model's state at iteration \(t\) of \(k\)-th modality. \(t_{0}\) is the start iteration for SMA. Hence, the representation from teacher network is denoted as \(}_{k}\) and the mixed representation of Eq. 6 should be rewritten as:

\[}_{k,l}=}_{k}+(1-)}_{l},\  Beta(,)\] (8)

where \(\) is a hyperparameter in Beta distribution. Considering the semantic gap between modalities, we let **interpolation closer** to \(k\)-th modality act as its teacher signal, so distillation loss should be:

\[_{dis}^{k}=_{l=1,l k}^{M}\| _{k}-}_{k,l}\|_{2}^{2},&>0.5\\ _{dis}^{l}=_{k=1,k l}^{M}\|_{l}- }_{k,l}\|_{2}^{2},&<0.5\] (9)

Then, we assign specific classifier for each modality before \(Proj_{k}()\) to online models and optimize the features by classification loss \(_{cls}^{k}\). **The combination \(_{dis}^{k}+_{cls}^{k}\) flattens the neighboring representation-space loss landscape of \(k\)-th modality to other modalities.** Further, we employ a multi-modal supervised contrastive loss on shared representation space, which can help to narrow the gap between modalities and make it conducive to flatten the region between them. For a random batch \(\) with \(M B\) uni-modal samples, we let \(i\) as the index of a uni-modal instance in the batch, and define \(P(i)\) as the set of uni-modal samples that have the same label with \(i\) (except itself). The supervised contrastive loss can be written as (notably, subscript here does not denote modality index but the index of each sample):

\[_{con}=_{i}-_{p P(i)}_{i}_{p}/ )}{_{a\{i\}}( _{i}_{a}/)}\] (10)

where \(^{+}\) is the temperature parameter.

**Adaptive weight.** As demonstrated in Tab. 1, the generalization capabilities between modalities may have significant gaps, so we propose to assign stronger flattening weights to better modalities. We compare the uni-modal validation accuracy from teacher model (calculated by the moving averaged uni-modal classifier) as a rough estimate of the difference in generalization ability between modalities (the performance of different modalities on in-domain validation set can generally reflect their strength in generalization capability, as shown in Appendix. B). The distillation loss can be modified as:

\[_{dis}^{k}=_{l=1,l k}^{M}_{k,l}\| {z}_{k}-}_{k,l}\|_{2}^{2},\ _{k,l}=\{1&_{k}/_{l}>\\ 0.5&_{k}/_{l}.\] (11)

where \(_{k}\) denotes the validation accuracy of \(k\)-th modality by teacher model, \(\) is a hyperparameter (default 1.2 in this paper). In this way, the teacher signal with stronger generalization ability is applied with a larger distillation weight. Finally, we can get our final loss as follows:

\[=_{cls}+_{k=1}^{M}_{1}_{cls}^{k}+ _{k=1}^{M}_{2}_{dis}^{k}+_{3}_{con}\] (12)

where \(_{cls}\) is the multi-modal classification loss, and \(_{1}\), \(_{2}\) and \(_{3}\) are hyperparameters to control the strength of each loss. Finally, we use teacher model for evaluation as it averages learned knowledge from student for better generalization.

## 4 Experiments

### Experimental Setting

**Dataset and implementation details.** We utilize two benchmark datasets, EPIC-Kitchens  and Human-Animal-Cartoon (HAC) , both of them have video, optical flow, and audio data. Threedistinct domains for EPIC-Kitchens are D1, D2, and D3 and for HAC are humans (H), animals (A), and cartoon figures (C). Our experiment setup follow . Training details including model structures, hyperparameters, and experimental environment can be found in Appendix. A.

**Baselines.** We compare our CMRF with seven different baselines that can be divided into four groups: 1) Base, naive multi-modal joint training without any domain generalization strategies, 2) SAM  and SAGM , searching for flat minima in parameter loss landscapes, 3) SWAD  and EoA , ensemble-based methods for flat minima, and 4) RNA-Net  and SimMMDG , domain generalization methods specifically designed for MMDG. SAM, SAGM, SWAD and EoA are initially designed for uni-modal DG and we extent them into MMDG. For all methods, we follow  and select the model with best validation (in-domain) accuracy to evaluate generalization on test (out-of-domain) data. We report the Top-1 accuracy for all results.

    &  &  &  \\  Method & Video & Audio & Flow & D2, D3 \(\) D1 & D1, D3 \(\) D2 & D1, D2 \(\) D3 & _Avg_ & A, C \(\) H & H, C \(\) A & H, A \(\) C & _Avg_ \\  Base & ✓ & ✓ & 54.94 & 62.26 & 61.70 & 59.63 & 69.92 & 69.32 & 50.09 & 63.11 \\ SAM  & ✓ & ✓ & 55.86 & 63.33 & 64.37 & 61.19 & 64.49 & 76.70 & 52.96 & 64.72 \\ SAGM  & ✓ & ✓ & 56.81 & 65.10 & 65.33 & 62.08 & 71.17 & 72.05 & 55.38 & 66.20 \\ SWAD  & ✓ & ✓ & 55.63 & 63.74 & 63.55 & 60.97 & 70.72 & 72.94 & 53.45 & 65.70 \\ EoA  & ✓ & ✓ & 55.63 & 64.93 & 64.68 & 61.75 & 69.20 & 77.27 & **58.71** & 68.39 \\ RNA-Net  & ✓ & ✓ & 55.37 & 64.20 & 62.25 & 60.61 & 67.45 & 68.32 & 54.78 & 63.52 \\ SimMMDG  & ✓ & ✓ & **57.24** & 65.07 & 63.55 & 61.95 & 72.75 & 76.14 & 54.59 & 67.83 \\ CMRF (ours) & ✓ & ✓ & 56.55 & **68.13** & **67.04** & **63.91** & **76.45** & **82.39** & 56.88 & **71.91** \\  Base & ✓ & ✓ & 55.86 & 67.47 & 59.34 & 60.89 & 72.83 & 77.84 & 43.58 & 64.75 \\ SAM  & ✓ & ✓ & 58.85 & 67.33 & 63.96 & 63.38 & 74.27 & 78.98 & 46.79 & 66.68 \\ SAGM  & ✓ & ✓ & 57.64 & 66.70 & 64.67 & 63.00 & 76.78 & 75.10 & 45.80 & 65.89 \\ SWAD  & ✓ & ✓ & 59.79 & 67.33 & 62.47 & 63.20 & 75.82 & 78.33 & 51.90 & 68.68 \\ EoA  & ✓ & ✓ & 62.99 & **68.89** & 63.76 & 65.21 & 74.45 & 80.06 & 53.13 & 69.42 \\ RNA-Net  & ✓ & ✓ & 54.21 & 64.80 & 59.31 & 59.44 & 74.56 & 75.39 & 44.90 & 64.95 \\ SimMMDG  & ✓ & ✓ & 57.03 & 66.67 & 63.86 & 62.82 & 77.90 & 78.98 & **57.80** & 71.56 \\ CMRF (ours) & ✓ & ✓ & **65.28** & 67.87 & **64.89** & **66.01** & **81.16** & **81.25** & 55.50 & **72.64** \\  Base & ✓ & ✓ & 49.42 & 55.60 & 54.41 & 53.14 & 52.89 & 55.11 & 40.92 & 49.64 \\ SAM  & ✓ & ✓ & 54.48 & 59.87 & 57.90 & 57.42 & 54.71 & 59.66 & 47.21 & 53.86 \\ SAGM  & ✓ & ✓ & 55.76 & 61.32 & 60.28 & 59.11 & 55.90 & 61.03 & 47.48 & 54.80 \\ SWAD  & ✓ & ✓ & 51.32 & 61.74 & 61.05 & 58.84 & 54.71 & 59.76 & 52.00 & 55.49 \\ EoA  & ✓ & ✓ & 52.41 & 60.67 & 61.81 & 58.30 & 55.43 & 58.97 & 52.29 & 55.56 \\ RNA-Net  & ✓ & ✓ & 50.89 & 54.24 & 55.90 & 53.68 & 53.11 & 59.32 & 43.82 & 52.08 \\ SimMMDG  & ✓ & ✓ & 55.86 & 64.60 & 59.34 & 59.35 & 57.88 & 60.79 & 48.62 & 55.76 \\ CMRF (ours) & ✓ & ✓ & **57.24** & **64.94** & **66.12** & **62.76** & **59.06** & **61.79** & **55.04** & **58.49** \\  Base & ✓ & ✓ & 54.71 & 67.20 & 61.70 & 61.20 & 70.29 & 71.25 & 53.75 & 65.07 \\ SAM  & ✓ & ✓ & 56.78 & 65.20 & 62.22 & 61.40 & 75.36 & 73.68 & 57.34 & 68.79 \\ SAGM  & ✓ & ✓ & 57.76 & 67.12 & 61.78 & 62.22 & 76.56 & 75.48 & 56.92 & 69.65 \\ SWAD  & ✓ & ✓ & ✓ & 55.84 & 68.21 & 64.90 & 62.98 & 75.78 & 74.95 & 58.02 & 69.58 \\ EoA  & ✓ & ✓ & ✓ & 57.93 & 68.53 & 68.78 & 65.08 & 76.09 & 76.95 & 57.19 & 70.08 \\ RNA-Net  & ✓ & ✓ & 56.25 & 63.47 & 59.72 & 59.81 & 71.89 & 70.88 & 54.58 & 65.78 \\ SimMMDG  & ✓ & ✓ & ✓ & **62.08** & 66.13 & 64.40 & 64.20 & 76.27 & 77.70 & 56.42 & 70.13 \\ CMRF (ours) & ✓ & ✓ & ✓ & 61.84 & **70.13** & **70.12** & **67.36** & **78.26** & **79.54** & **60.09** & **72.44** \\   

Table 2: Multi-modal **multi-source** DG with different modalities on EPIC-Kitchens and HAC datasets.

    &  &  \\   &  &  &  &  &  &  \\ 
**Method** & Target: & D2 & D3 & D1 & D3 & D1 & D2 & _Avg_ & A & C & H & C & H & A & _Avg_ \\  Base & 56.80 & 53.08 & 47.36 & 59.65 & 55.63 & 56.93 & 54.91 & 64.20 & 39.45 & 64.85 & 52.29 & 57.97 & 65.90 & 57.44 \\ SAM  & 54.40 & 55.24 & 49.65 & 61.4

[MISSING_PAGE_FAIL:8]

generalization of each modality via mitigating modality competition and flattening representation loss landscape between modalities. In Appendix B, we show the alleviated competition under in-domain performance and flatter region with perturbations. As for baselines, SAM and SimMMDG only enhance the generalization of better modality and EoA just achieves marginal uni-modal improvement, which means they can not utilize the generalization capability of all modalities comprehensively. Detailed results for each test domain and more results on HAC dataset are shown in Appendix. B.

### Ablation Studies

**Ablation on each design.** Our CMRF contains five main modules: distillation loss \(^{k}_{dis}\), uni-modal classification loss \(^{k}_{cls}\), multi-modal supervised contrastive loss \(_{con}\), adaptive weight, and SMA for teacher model. We conduct extensive ablation experiments to verify the effectiveness of each proposed module on EPIC-Kitchens with video-audio data under multi-source domain generalization setting. The results are illustrated in Tab. 5. Only applying distillation loss or uni-modal classification loss improves slightly and their combination leads to noticeable increase, highlighting the importance of flattening representation loss landscape between modalities for domain generalization. However, it does not guarantee steady improvement, e.g., the accuracy decreases from 54.94% to 52.75% in D2, D3 \(\) D1 setting. Multi-modal supervised contrastive loss can enhance the average generalization by a small margin. Adaptive weight and using SMA network as teacher can both improve MMDG by a large margin, suggesting that it is necessary to emphasize the more generalized modality and obtain more stable distillation signals. Finally, combining all of them achieves the best results for multi-modal domain generalization, hence, each of them is indispensable.

**Ablation on interpolations.** In this paper, we mix multi-modal representations in the random ratio generated from Beta distribution as teacher signals, and choose interpolations closer to current modality for distillation, as in Eq. 9. We conduct experiments by using different forms of teacher signals to verify our method's effectiveness, as presented in Tab. 6. For \(k\)-th modality, we set \(\) to 1, 0, 0.5 for self-modal distillation, cross-modal distillation, and distillation with fixed mixing ratio. Since self-modal distillation can enhance learning for each modality via more generalizable signals, it achieves great performance next to ours. The heterogeneous knowledge between modalities makes cross-mode distillation worse. Fixed mixing ratio only locates one interpolation while our random ratio covers all possible points, resulting in our better performance.

**Comparison with methods designed for modality competition.** Here, we conduct experiments with three baselines Gradient Blending , OGM-GE , and PMR  for modality competition as we attribute it as one challenge for MMDG. We not only report out-of-domain test accuracy but also in-domain validation results, as shown in Tab. 7. We can see that these methods can actually promote their performance on multi-modal validation set since they mitigate the competition. However, they tend to locate at sharp minima and the generalization gap between modalities still makes it hard to build consistent flat minima for different modalities. Hence, their performance increase on test set is limited, while our method achieves significant improvement on both validation and test sets.

    & Validation & Test \\  Base & 91.41 & 63.11 \\ Grad Blending  & 92.70 & 66.82 \\ OGM-GE  & 93.67 & 64.33 \\ PMR  & **94.90** & 65.24 \\ CMRF & 93.21 & **71.91** \\   

Table 7: The average results compared with methods designed for modality competition on HAC with video and audio data under multi-source DG.

Figure 3: Parameter sensitivity analysis on HAC with video and audio data under A, C \(\) H.

**Parameter sensitivity.** Fig. 3 shows the results of different values on loss weights \(_{1}\), \(_{2}\), and \(_{3}\). Since our method uses the moving averaged teacher model for evaluation, it is insensitive to hyperparameters.

## 5 Conclusion

In this paper, we analyze the behavior of multi-modal domain generalization and find that modality competition and discrepant uni-modal flatness restrict the generalization capability of multi-modal network. To address these challenges, we propose cross-modal representation flattening (CMRF) to construct consistent flat regions in a shared representation-space loss landscape. Our method builds interpolations by mixing multi-modal representations from moving averaged teacher model and use feature distillation to optimize the high-loss regions between modalities. Our extensive experiments on two benchmark datasets demonstrate the effectiveness of our method to promote multi-modal domain generalization, as well as uni-modal domain generalization in multi-modal network.

**Limitations.** Currently, we need to test on validation set to estimate generalization of each modality for Eq. 11, which can be time-consuming with the scale increase of validation set. In future work, we can add low-frequency noise as in  for domain shifting to evaluate the generalization.