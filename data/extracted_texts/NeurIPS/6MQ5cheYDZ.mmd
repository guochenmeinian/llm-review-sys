# Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees

Sharan Vaswani

Simon Fraser University

vaswani.sharan@gmail.com

&Amirreza Kazemi

Simon Fraser University

aka208@sfu.ca

&Reza Babanezhad

Samsung - SAIT AI Lab, Montreal

babanezhad@gmail.com

&Nicolas Le Roux

Microsoft Research, Mila

nicolas@le-roux.name

###### Abstract

Actor-critic (AC) methods are widely used in reinforcement learning (RL), and benefit from the flexibility of using any policy gradient method as the actor and value-based method as the critic. The critic is usually trained by minimizing the TD error, an objective that is potentially decorrelated with the true goal of achieving a high reward with the actor. We address this mismatch by designing a joint objective for training the actor and critic in a _decision-aware_ fashion. We use the proposed objective to design a generic, AC algorithm that can easily handle any function approximation. We explicitly characterize the conditions under which the resulting algorithm guarantees monotonic policy improvement, regardless of the choice of the policy and critic parameterization. Instantiating the generic algorithm results in an actor that involves maximizing a sequence of surrogate functions (similar to TRPO, PPO), and a critic that involves minimizing a closely connected objective. Using simple bandit examples, we provably establish the benefit of the proposed critic objective over the standard squared error. Finally, we empirically demonstrate the benefit of our decision-aware actor-critic framework on simple RL problems.

## 1 Introduction

Reinforcement learning (RL) is a framework for solving problems involving sequential decision-making under uncertainty, and has found applications in games , robot manipulation tasks  and clinical trials . RL algorithms aim to learn a policy that maximizes the long-term return by interacting with the environment. Policy gradient (PG) methods  are an important class of algorithms that can easily handle function approximation and structured state-action spaces, making them widely used in practice. PG methods assume a differentiable parameterization of the policy and directly optimize the return with respect to the policy parameters. Typically, a policy's return is estimated by using Monte-Carlo samples obtained via environment interactions . Since the environment is stochastic, this approach results in high variance in the estimated return, leading to higher sample-complexity (number of environment interactions required to learn a good policy). Actor-critic (AC) methods  alleviate this issue by using value-based approaches  in conjunction with PG methods, and have been empirically successful . In AC algorithms, a value-based method ("critic") is used to approximate a policy's estimated value, and a PG method ("actor") uses this estimate to improve the policy towards obtaining higher returns.

Though AC methods have the flexibility of using any method to independently train the actor and critic, it is unclear how to train the two components _jointly_ in order to learn good policies. For example, the critic is typically trained via temporal difference (TD) learning and its objective is to minimize the value estimation error across all states and actions. For large real-world Markov decision processes (MDPs), it is intractable to estimate the values across all states and actions, andalgorithms resort to function approximation schemes. In this setting, the critic should focus its limited model capacity to correctly estimate the state-action values that have the largest impact on improving the actor's policy. This idea of explicitly training each component of the RL system to help the agent take actions that result in higher returns is referred to as _decision-aware RL_. Decision-aware RL [17; 16; 1; 10; 13; 14; 32] has mainly focused on model-based approaches that aim to learn a model of the environment, for example, the rewards and transition dynamics in an MDP. In this setting, decision-aware RL aims to model relevant parts of the world that are important for inferring a good policy. This is achieved by (i) designing objectives that are aware of the current policy [1; 14] or its value [17; 16], (ii) differentiating through the transition dynamics to learn models that result in good action-value functions  or (iii) simultaneously learning value functions and models that are consistent [51; 40; 35]. In the model-free setting, decision-aware RL aims to train the actor and critic cooperatively in order to optimize the same objective that results in near-optimal policies. In particular, Dai et al.  use the linear programming formulation of MDPs and define a joint saddle-point objective (minimization w.r.t. the critic and maximization w.r.t. the actor). The use of function approximation makes the resulting optimization problem non-convex non-concave leading to training instabilities and necessitating the use of heuristics. Recently, Dong et al.  used stochastic gradient descent-ascent to optimize this saddle-point objective and, under certain assumptions on the problem, proved that the resulting policy converges to a stationary point of the value function. Similar to Dong et al. , we study a decision-aware AC method with function approximation and equipped with theoretical guarantees on its performance. In particular, we make the following contributions.

**Joint objective for training the actor and critic**: Following Vaswani et al. , we distinguish between a policy's _functional representation_ (sufficient statistics that define a policy) and its _parameterization_ (the specific model used to realize these sufficient statistics in practice). For example, a policy can be represented by its state-action occupancy measure, and we can use a neural network parameterization to model this measure in practice (refer to Sec. 2 for more examples). In Sec. 3.2, we exploit a smoothness property of the return and design a lower-bound (Prop. 1) on the return of an arbitrary policy. Importantly, the lower bound depends on both the actor and critic, and immediately implies a joint objective for training the two components (minimization w.r.t the critic and maximization w.r.t the actor). Unlike Dai et al. , Dong et al. , the proposed objective works for **any** policy representation - the policy could be represented as conditional distributions over actions for each state or a deterministic mapping from states to actions . Another advantage of working in the functional space is that our lower bound does not depend on the parameterization of either the actor or the critic. Moreover, unlike Dai et al. , Dong et al. , our framework does not need to model the distribution over states, and hence results in a more efficient algorithm. We note that our framework can be used for other applications where gradient computation is expensive or has large variance , and hence requires a model of the gradient (e.g., variational inference).

**Generic actor-critic algorithm**: In Sec. 3.2, we use our joint objective to design a generic decision-aware AC algorithm. The resulting algorithm (Algorithm 1) can be instantiated with any functional representation of the policy, and can handle any policy or critic parameterization. Similar to Vaswani et al. , the actor update involves optimizing a _surrogate function_ that depends on the current policy, and consequently supports _off-policy updates_, i.e. similar to common PG methods such as TRPO , PPO , the algorithm can update the policy without requiring additional interactions with the environment. This property coupled with the use of a critic makes the resulting algorithm sample-efficient in practice. In contrast with TRPO/PPO, both the off-policy actor updates and critic updates in Algorithm 1 are designed to maximize the same lower bound on the policy return.

**Theoretical guarantees**: In Sec. 4.1, we analyze the necessary and sufficient conditions in order to guarantee monotonic policy improvement, and hence convergence to a stationary point. We emphasize that these improvement guarantees hold regardless of the policy parameterization and the quality of the critic (up to a certain threshold that we explicitly characterize). This is in contrast to existing theoretical results that focus on the tabular or linear function approximation settings or rely on highly expressive critics to minimize the critic error and achieve good performance for the actor. By exploiting the connection to inexact mirror descent (MD), we prove that Algorithm 1 is guaranteed to converge to the neighbourhood of a stationary point where the neighbourhood term depends on the decision-aware critic loss (Sec. 4.2). Along the way, we improve the theoretical guarantees for MD on general smooth, non-convex functions [15; 12]. As an additional contribution, we demonstrate a way to use the framework of Vaswani et al.  to "lift" the existing convergence rates [61; 37; 24] for the tabular setting to use off-policy updates and function approximation (Appendix D.2 and D.3). This gives rise to a simple, black-box proof technique that might be of independent interest.

**Instantiating the general AC framework**: We instantiate the framework for two policy representations - in Sec. 5.1, we represent the policy by the set of conditional distributions over actions ("direct" representation), whereas in Sec. 5.2, we represent the policy by using the logits corresponding to a softmax representation of these conditional distributions ("softmax" representation). In both cases, we instantiate the generic lower-bound (Propositions 4, 6), completely specifying the actor and critic objectives in Algorithm 1. Importantly, unlike the standard-critic objective that depends on the squared difference of the value functions, the proposed decision-aware critic loss (i) depends on the policy representation - it involves the state-action value functions for the direct representation and depends on the advantage functions for the softmax representation, and (ii) penalizes the under-estimation and over-estimation of these quantities in an asymmetric manner. For both representations, we consider simple bandit examples (Propositions 5, 7) which show that minimizing the decision-aware critic loss results in convergence to the optimal policy, whereas minimizing variants of the squared loss do not. In App. B, we consider a third policy representation involving stochastic value gradients  for continuous control, and instantiate our decision-aware actor-critic framework in this case.

**Experimental evaluation**: Finally, in Sec. 6, we consider simple RL environments and benchmark Algorithm 1 for both the direct and softmax representations with a linear policy and critic parameterization. We compare the actor performance when using the squared critic loss vs the proposed critic loss, and demonstrate the empirical benefit of our decision-aware actor-critic framework.

## 2 Problem Formulation

We consider an infinite-horizon discounted Markov decision process (MDP)  defined by the tuple \(,,,r,,\) where \(\) is the set of states, \(\) is the action set, \(:_{}\) is the transition probability function, \(_{}\) is the initial distribution of states, \(r:\) is the reward function and \([0,1)\) is the discount factor. For state \(s\), a policy \(\) induces a distribution \(p^{}(|s)\) over actions. It also induces a measure \(d^{}\) over states such that \(d^{}(s)=(1-)_{=0}^{}^{}(s_{}= s s_{0},a_{} p^{}(|s_{}))\). Similarly, we define \(^{}\) as the measure over state-action pairs induced by policy \(\), implying that \(^{}(s,a)=d^{}(s)\,p^{}(a|s)\) and \(d^{}(s)=_{a}^{}(s,a)\). The _action-value function_ corresponding to policy \(\) is denoted by \(Q^{}:\) such that \(Q^{}(s,a):=[_{=0}^{}^{}r(s_{},a_{ })]\) where \(s_{0}=s,a_{0}=a\) and for \( 0\), \(s_{+1}(|s_{},a_{})\) and \(a_{+1} p^{}(|s_{+1})\). The _value function_ of a stationary policy \(\) for the start state equal to \(s\) is defined as \(J_{s}():=_{a p^{}(|s)}[Q^{}(s,a)]\) and we define \(J():=_{s}[J_{s}()]\). For a state-action pair \((s,a)\), the _advantage function_ corresponding to policy \(\) is given by \(A^{}(s,a):=Q^{}(s,a)-J_{s}()\). Given a set of feasible policies \(\), the objective is to compute the policy that maximizes \(J()\).

**Functional representation vs Policy Parameterization:** Similar to the policy optimization framework of Vaswani et al. , we differentiate between a policy's functional representation and its parameterization. The _functional representation_ of a policy \(\) defines its sufficient statistics, for example, we may represent a policy via the set of distributions \(p^{}(|s)_{A}\) for state \(s\). We will refer to this as the _direct representation_. The same policy can have multiple functional representations, for example, since \(p^{}(|s)\) is a probability distribution, one can write \(p^{}(a|s)=(z^{}(s,a))/_{a^{}}(z^{}(s,a^{}))\), and represent \(\) by the set of logits \(z^{}(s,a)\) for each \((s,a)\) pair. We will refer to this as the _softmax representation_. On the other hand, the _policy parameterization_ is determined by a _model_ (with parameters \(\)) that realizes these statistics. For example, we could use a neural-network to parameterize the logits corresponding to the policy's softmax representation, rewriting \(z^{}(s,a)=z^{}(s,a|)\) where the model is implicit in the \(z^{}(s,a|)\) notation. As another example, the _tabular parameterization_ corresponds to having a parameter for each state-action pair [61; 37]. The policy parameterization thus defines the set \(\) of realizable policies that can be expressed with the parametric model at hand. Note that the policy parameterization can be chosen independently of its functional representation. Next, we recap the framework in Vaswani et al.  and generalize it to the actor-critic setting.

## 3 Methodology

We describe functional mirror ascent in Sec. 3.1, and use it to design a general decision-aware actor-critic framework and corresponding algorithm in Sec. 3.2.

### Functional Mirror Ascent for Policy Gradient (FMAPG) framework

For a given functional representation, Vaswani et al.  update the policy by _functional mirror ascent_ and project the updated policy onto the set \(\) determined by the policy parameterization.

Functional mirror ascent is an iterative algorithm whose update at iteration \(t\{0,1,,T-1\}\) is given as: \(_{t+1}=*{arg\,max}_{}[,\,_{ }J(_{t})-\,D_{}(,_{t})]\) where \(_{t}\) is the policy (expressed as its functional representation) at iteration \(t\), \(\) is the step-size in the functional space and \(D_{}\) is the Bregman divergence (induced by the mirror map \(\)) between the representation of policies \(\) and \(_{t}\). The FMAPG framework casts the projection step onto \(\) as an unconstrained optimization w.r.t the parameters \(^{n}\) of a _surrogate function_: \(_{t+1}=*{arg\,max}_{t}():=( ),\,_{}J((_{t}))-D_{}(( ),(_{t}))\). Here, \(()\) refers to the parametric form of the policy where the choice of the parametric model is implicit in the \(()\) notation. The policy at iteration \(t\) is thus expressed as \((_{t})\), whereas the updated policy is given by \(_{t+1}=(_{t+1})\). The surrogate function is non-concave in general and can be approximately maximized using a gradient-based method, resulting in a nested loop algorithm. Importantly, the inner-loop (optimization of \(_{t}()\)) updates the policy parameters (and hence the policy), but does not involve recomputing \(_{}J()\). Consequently, these policy updates do not require interacting with the environment and are thus _off-policy_. This is a desirable trait for designing sample-efficient PG algorithms and is shared by methods such as TRPO  and PPO .

With the appropriate choice of \(\) and \(\), the FMAPG framework guarantees monotonic policy improvement for any number of inner-loops and policy parameterization. A shortcoming of this framework is that it requires access to the exact gradient \(_{}J()\). When using the direct or softmax representations, computing \(_{}J()\) involves computing either the action-value \(Q^{}\) or the advantage \(A^{}\) function respectively. In complex real-world environments where the rewards and/or the transition dynamics are unknown, these quantities can only be estimated. For example, \(Q^{}\) can be estimated using Monte-Carlo sampling by rolling out trajectories using policy \(\) resulting in large variance, and consequently higher sample complexity. Moreover, for large MDPs, function approximation is typically used to estimate the \(Q\) function, and the resulting aliasing makes it impossible to compute it exactly in practice. This makes the FMAPG framework impractical in real-world scenarios. Next, we generalize FMAPG to handle inexact gradients and subsequently design an actor-critic framework.

### Generalizing FMAPG to Actor-Critic

To generalize the FMAPG framework, we first prove the following proposition in App. C.

**Proposition 1**.: For any policy representations \(\) and \(^{}\), any strictly convex mirror map \(\), and any gradient estimator \(\), for \(c>0\) and \(\) such that \(J+\) is convex in \(\),

\[J() J(^{})+(^{}),-^{} -(+)D_{}(,^{} )-\,D_{^{*}}(^{})-c[ J(^{ })-(^{})],\,(^{})\]

where \(^{*}\) is the Fenchel conjugate of \(\) and \(D_{^{*}}\) is the Bregman divergence induced by \(^{*}\).

The above proposition is a statement about the relative smoothness  of \(J\) (w.r.t \(D_{}\)) in the functional space. Here, the brown term is the linearization of \(J\) around \(^{}\), but involves \((^{})\) which can be **any** estimate of the gradient at \(^{}\). The red term quantifies the distance between the representations of policies \(\) and \(^{}\) in terms of \(D_{}(,^{})\), whereas the blue term characterizes the penalty for an inaccurate estimate of \(_{}J(^{})\) and depends on \(\). We emphasize that Prop. 1 can be used for **any** continuous optimization problem that requires a model of the gradient, e.g., in variational inference which uses an approximate posterior in lieu of the true one.

For policy optimization with FMAPG, \(_{}J()\) involves the action-value or advantage function for the direct or softmax functional representations respectively (see Sec. 5 for details), and the gradient estimation error is equal to the error in these functions. Since these quantities are estimated by the critic, we refer to the blue term as the _critic error_. In order to use Prop. 1, at iteration \(t\) of FMAPG, we set \(^{}=_{t}\) and include the policy parameterization, resulting in **inequality (I)**:

\[J()-J(_{t})_{t},()-_{t}- (+)D_{}((),_{t})-\,D_{^{*}}(_{t})-c[ J(_{t})-_{t}], (_{t}),\]

where \(_{t}:=(_{t})\). We see that in order to obtain a policy \(\) that maximizes the policy improvement \(J()-J(_{t})\) and hence the LHS, we should maximize the RHS i.e. (i) learn \(_{t}\) to minimize the blue term (equal to the critic objective) and (ii) compute \(\) that maximizes the green term (equal to the functional mirror ascent update at iteration \(t\)). Using a second-order Taylor series expansion of \(D_{^{*}}\) (Prop. 14), we see that as \(c\) decreases, the critic error decreases, whereas the \((+)D_{}(,_{t})\) term increases. Consequently, we interpret the scalar \(c\) as a trade-off parameter that relates the critic error to the permissible movement in the functional mirror ascent update.

Hence, _both the actor and critic objectives are coupled through Prop. 1 and both components of the RL system should be jointly trained in order to maximize policy improvement_. We refer to the resulting framework as _decision-aware actor-critic_ and present its pseudo-code in Algorithm 1.

```
1Input: \(\) (choice of functional representation), \(_{0}\) (initial policy parameters), \(_{(-1)}\) (initial critic parameters), \(T\) (AC iterations), \(m_{a}\) (actor inner-loops), \(m_{c}\) (critic inner-loops), \(\) (functional step-size for actor), \(c\) (trade-off parameter), \(_{a}\) (parametric step-size for actor), \(_{c}\) (parametric step-size for critic) Initialization: \(_{0}=(_{0})\)for\(t 0\)to\(T-1\)do
2 Estimate \(}J(_{t})\) and form \(_{t}():=\,D_{^{*}}(_{t}) -c\,[\,}J(_{t})-_{t}()],(_ {t})\) Initialize inner-loop: \(_{0}=_{t-1}\)for\(k 0\)to\(m_{c}-1\)do
3\(_{k+1}=_{k}-_{}\,_{}\,_{t}( _{k})\) /* Critic Updates */
4\(_{t}=_{m_{c}};_{t}=_{t}(_{t})\) Form \(_{t}():=_{t},()-_{t}- (+)D_{}((),_{t})\) Initialize inner-loop: \(_{0}=_{t}\)for\(k 0\)to\(m_{a}-1\)do
5\(_{k+1}=_{k}+_{}\,_{}\,_{t}(_{k})\) /* Off-policy actor updates */ \(_{t+1}=_{m_{a}};_{t+1}=(_{t+1})\) Return \(_{T}=(_{T})\) ```

**Algorithm 1**Generic actor-critic algorithm

Unlike Wu et al. , Konda and Tsitsiklis , Algorithm 1 does not update the actor and critic in a two time-scale setting (one environment interaction and update to the critic followed by an actor update), but rather performs multiple steps to update the critic, then uses the critic to perform multiple steps to update the actor [2; 61]. At iteration \(t\) of Algorithm 1, \(_{t}\) (the gradient estimate at \(_{t}\)) is parameterized by \(\) and the parametric model for the critic is implicit in the \(_{t}()\) notation. The algorithm interacts with the environment, uses these interactions to form the estimate \(}J(_{t})\) and construct the critic loss function \(_{t}()\). For the direct or softmax representations, \(}J(_{t})\) corresponds to the empirical estimates of the action-value or advantage functions respectively. In practice, these quantities can be estimated using Monte-Carlo rollouts or bootstrapping. Given these estimates, the critic is trained (using \(m_{c}\) inner-loops) to minimize \(_{t}()\) and obtain \(_{t}\) (Lines 5-8). Line 9 uses \(_{t}\) to construct the surrogate function \(_{t}()\) for the actor and depends on the policy parameterization. The inner-loop (Lines 10 - 13) involves maximizing \(_{t}()\) and corresponds to \(m_{a}\) off-policy updates. Next, we establish theoretical guarantees on the performance of Algorithm 1.

## 4 Theoretical Guarantees

We first establish the necessary and sufficient conditions to guarantee monotonic policy improvement in the presence of critic error (Sec. 4.1). In Sec. 4.2, we prove that Algorithm 1 is guaranteed to converge to the neighbourhood (that depends on the critic error) of a stationary point.

### Conditions for monotonic policy improvement

According to **inequality (I)**, to guarantee monotonic policy improvement at iteration \(t\), one must find a \((,c)\) pair to guarantee that the RHS of **(I)** is positive. In Prop. 2 (proved in App. D), we derive the conditions on the critic error to ensure that it possible to find such an \((,c)\) pair.

**Proposition 2**.: For any policy representation and any policy or critic parameterization, there exists a \((,c)\) pair that makes the RHS of **inequality (I)** strictly positive, and hence guarantees monotonic policy improvement (\(J(_{t+1})>J(_{t})\)), if and only if

\[ b_{t},_{t}^{}b_{t}>[ J(_{t})- _{t}],[_{}^{2}(_{t})]^{-1}\,[ J(_{t})-_ {t}]\;,\]

where \(b_{t}^{n}:=_{s}_{a}[ {g}_{t}]_{s,}_{}[(_{t})]_{s,a}\), \(_{t}^{n n}:=_{}(_{t})^{} \,_{}^{2}(_{t})\,_{}(_{t})\) and \(_{t}^{}\) denotes the pseudo-inverse of \(_{t}\). For the special case of the tabular policy parameterization, the above condition becomes equal to,

\[_{t},[_{}^{2}(_{t})]^{-1}_{t}> [ J(_{t})-_{t}],[_{}^{2}(_{t})]^{-1}[  J(_{t})-_{t}]\;.\]For the Euclidean mirror map with the tabular policy parameterization, this condition becomes equal to \(\|_{t}\|_{2}^{2}>\| J(_{t})-_{t}\|_{2}^{2}\) meaning that the relative error in estimating \( J(_{t})\) needs to be less than \(1\). For a general mirror map, the relative error is measured in a different norm induced by the mirror map. The above proposition also quantifies the scenario when the critic error is too large to guarantee policy improvement. In this case, the algorithm should either improve the critic by better optimization or by using a more expressive model, or resort to using sufficiently many (high-variance) Monte-Carlo samples as in REINFORCE . Finally, we see that the impact of a smaller function class for the actor is a potentially lower value for \( b_{t},_{t}^{}b_{t}\), making it more difficult to satisfy the condition. _The improvement guarantee in Prop. 2 holds regardless of the policy representation and parameterization of the policy or critic._ This is in contrast to existing theoretical results  that focus on either the tabular or linear function approximation setting for the policy and/or critic, or rely on using expressive models to minimize the critic error and achieve good performance for the actor. Moreover, this result only depends on the magnitude of the critic loss (after updates), irrespective of the optimizer, step-size or other factors influencing the critic optimization. The actor and critic are coupled via the threshold (on the critic loss) required to guarantee policy improvement.

### Convergence of Algorithm 1

Prop. 2 holds when the critic error is small. We now analyze the convergence of Algorithm 1 for an arbitrary critic error. Define \(_{t+1}:=_{}_{t}()\), \(_{t+1}=(_{t+1})=_{}\{ {g}_{t},-_{t}-(+)D_{}(,_{t})\}\). Note that \(_{t+1}\) is the iterate obtained by using the inexact mirror ascent (MA) update (because it does not use the true gradient \(_{}J(_{t})\)) starting from \(_{t}\), and that the inner-loop (Lines 10-13) of Algorithm 1 approximates this update. This connection allows us to prove the following guarantee (see App. D.1 for details) for Algorithm 1.

**Proposition 3**.: For any policy representation and mirror map \(\) such that (i) \(J+\) is convex in \(\), any policy parameterization such that (ii) \(_{t}()\) is smooth w.r.t \(\) and satisfies the Polyak-Lojasiewicz (PL) condition, for \(c>0\), after \(T\) iterations of Algorithm 1 we have that,

\[[(_{R+1},_{R})}{^{2}}] \,[)-J(_{0})}_{}+_{t=0}^{T-1}(D_{^{*}}( (_{t})-c\,_{t},(_{t}))}_{}+[e_{t}]}_{})]\]

where \(_{t}:= J(_{t})-_{t}\), \(:=+\), \(\) is a random variable chosen uniformly from \(\{0,1,2, T-1\}\) and \(e_{t}()})\) is the projection error (onto \(\)) at iteration \(t\).

Prop. 3 shows that Algorithm 1 converges to the neighbourhood of a stationary point of \(J\) for an arbitrary critic error. The LHS of the above expression is a measure of sub-optimality similar to the one used in the analysis of stochastic mirror descent . For the Euclidean mirror map, the LHS becomes equal to \(\|_{}J(_{R})\|_{2}^{2}\), the standard characterization of a stationary point. Term (i) on the RHS is the initial sub-optimality, whereas Term (ii) is equal to the critic error and can be further decomposed into variance and bias terms. The variance decreases as the number of samples used to train the critic (Line 4 in Algorithm 1) increases. The bias can be decomposed into an optimization error (that decreases as \(m_{c}\) increases) and a function approximation error (that decreases as we use more expressive models for the critic). Finally, Term (iii) is the projection (onto \(\)) error, is equal to zero for the tabular policy parameterization, and decreases as \(m_{a}\) increases. Hence, the performance of Algorithm 1 improves as we increase both \(m_{a}\) and \(m_{c}\).

In Sec. 5, we specify the step-size \(\) such that Assumption (i) is satisfied for both the direct and softmax representations. Assumption (ii) is satisfied when using a linear and, in some cases, a neural network policy parameterization . For the above proposition to hold, we require that step-sizes \(_{c}\) and \(_{a}\) in Algorithm 1 be set according to the smoothness of critic (\(_{t}()\)) and actor (\(_{t}()\)) objectives respectively. These choice of step-sizes guarantee ascent for the actor objective, and descent for the critic objective (refer to the proof of Prop. 3 in App. D for details). In practice, we set both step-sizes using an Armijo line-search, and refer the reader to App. F for details. Since Algorithm 1 does not update the actor and critic in a two time-scale setting, unlike , the relative scales of the step-sizes and the number of inner iterations (\(m_{a}\), \(m_{c}\)) do not affect the algorithm's performance.

In contrast to Prop. 3, Dong et al.  prove that their proposed algorithm results in an \(O(}{{T}})\) convergence to the stationary point (not the neighbourhood). However, they make a strong unjustified assumption that the minimization problem w.r.t the parameters modeling the policy and distribution over states is jointly PL. Compared to [2; 61; 36] that focus on proving convergence to the neighbourhood of the optimal value function, but bound the critic error in the \(_{2}\) or \(_{}\) norm, we focus on proving convergence to the (neighbourhood) of a stationary point, but define the critic loss in a decision-aware manner that depends on \(D_{^{*}}\). Since Algorithm 1 is not a two time-scale algorithm, unlike Konda and Tsitsiklis , Wu et al. , the proof of Prop. 3 does not require analyzing coupled recursions between the actor and critic. Furthermore, the guarantees in Prop. 3 are independent of how the critic loss is minimized. We could use any policy evaluation method in order to estimate the value function. Hence, unlike the standard two time-scale analyses, we do not make assumptions about the mixing time of the underlying Markov chain.

Compared to the existing theoretical work on general (not decision-aware) AC methods [62; 60; 8; 28; 22; 30; 18; 41; 9] that prove convergence for the tabular or linear function approximation settings, (a) our theoretical results require fewer assumptions on the function approximation. For instance, Prop. 2 holds for _any_ actor or critic parameterization (including complex neural networks), while the guarantees in Prop. 3 hold for any critic parameterization, but require that \(_{t}\), the surrogate function for the actor satisfy smoothness and gradient domination properties. (b) On the other hand, since our analysis does not explicitly model how the critic error is minimized, we can only converge to the neighbourhood of a stationary point. This is in contrast to the existing two time-scale analyses that jointly analyze the actor and critic, and show convergence to a stationary point . (c) Finally, we note that the proposed algorithm supports off-policy updates i.e. the actor can re-use the value estimates from the critic to update the policy multiple times (corresponding to Lines 10-13 in Algorithm 1). This is in contrast to existing theoretically principled actor-critic methods that require interacting with the environment and gathering new data after each policy update. Hence, compared to the existing literature on AC methods, Algorithm 1 is more practical, has weaker theoretical guarantees but requires fewer assumptions on the function approximation.

## 5 Instantiating the generic actor-critic framework

We now instantiate Algorithm 1 for the direct (Sec. 5.1) and softmax (Sec. 5.2) representation.

### Direct representation

Recall that for the direct functional representation, policy \(\) is represented by the set of distributions \(p^{}(|s)\) over actions for each state \(s\). Using the policy gradient theorem , \([_{}J()]_{s,a}=d^{}(s)\,Q^{}(s,a)\). Similar to [57; 61], we use a weighted (across states) negative entropy mirror map implying that \(D_{}(p^{},p^{^{}})=_{s}d^{_{t}}(s)\,D_{ }(p^{}(|s),p^{^{}}(|s))\) where \((p^{}(|s))=-_{a}p^{}(a|s)\,(p^{}(a|s))\) and hence, \(D_{}(p^{}(|s),p^{^{}}(|s))=(p^{}(| s)||p^{^{}}(|s))\). We now instantiate **inequality (I)** in Sec. 3.2 in the proposition below (see App. E for the derivation).

**Proposition 4**.: For the direct representation and negative entropy mirror map, \(c>0\), \(}{2\,|A|}\),

\[J()-J(_{t}) C+_{s d^{_{t}}}[ _{a p^{_{t}}(|s)}[(a|s)}{p^{_{t}}( a|s)}\,(^{_{t}}(s,a)-(+)\, ((a|s)}{p^{_{t}}(a|s)}))]]\] \[-_{s d^{_{t}}}[_{a p^{_{t }}(|s)}[Q^{_{t}}(s,a)-^{_{t}}(s,a)]+ \,(_{a p^{_{t}}(|s)}[(-c[Q^ {_{t}}(s,a)-^{_{t}}(s,a)]])])\]

where \(C\) is a constant and \(^{_{t}}\) is the estimate of the action-value function for policy \(_{t}\).

For incorporating policy (with parameters \(\)) and critic (with parameters \(\)) parameterization, we note that \(p^{}(|s)=p^{}(|s,)\) and \(^{}(s,a)=Q^{}(s,a|)\) where the model is implicit in the notation. Using the reasoning in Sec. 3.2 with Prop. 4 immediately gives us the actor and critic objectives (\(_{t}()\) and \(L_{t}()\) respectively) at iteration \(t\) and completely instantiates Algorithm 1. Observe that the critic error is asymmetric and penalizes the under/over-estimation of the \(Q^{}\) function differently. This is different from the standard squared critic loss: \(E_{s d^{_{t}}}_{a p^{_{t}}(|s)}[Q^{_{t}}(s,a)-Q^{_{t}}(s,a|)]^{2}\) that does not take into account the sign of the misestimation.

To demonstrate the effectiveness of the proposed critic loss, we consider a two-armed bandit example in Prop. 5 (see App. E for details) with deterministic rewards (there is no variance due to sampling), use the direct representation and tabular parameterization for the policy, linear function approximation for the critic and compare minimizing the standard squared loss vs the decision-aware loss in Prop. 4.

**Proposition 5**.: Consider a two-armed bandit example with deterministic rewards where arm \(1\) is optimal and has a reward \(r_{1}=Q_{1}=2\) whereas arm \(2\) has reward \(r_{2}=Q_{2}=1\). Consider using linear function approximation to estimate the \(Q\) function i.e. \(=x\,\) where \(\) is the parameter to be learned and \(x\) is the feature of the corresponding arm. Let \(x_{1}=-2\) and \(x_{2}=1\) implying that \(_{1}()=-2\) and \(_{2}()=\). Let \(p_{t}\) be the probability of pulling the optimal arm at iteration \(t\) and consider minimizing two alternative objectives to estimate \(\):

(1) Squared loss: \(_{t}^{(1)}:=\{}{2}[_{1}()-Q_{1 }]^{2}+}{2}[_{2}()-Q_{2}]^{2}\}\).

(2) Decision-aware critic loss: \(_{t}^{(2)}=_{t}():=p_{t}[Q_{1}-_{1 }()]+(1-p_{t})[Q_{2}-_{2}()]+ \,(p_{t}\,(-c[Q_{1}-_{1}()]+(1-p_{t} )\,(-c[Q_{2}-_{2}()]))]\).

For \(p_{0}<\), minimizing the squared loss results in convergence to the sub-optimal action, while minimizing the decision-aware loss (for \(c,p_{0}>0\)) results in convergence to the optimal action.

Hence, minimizing the decision-aware critic loss results in a better, more well-informed estimate of \(\) which when coupled with the actor update results in convergence to the optimal arm. For this simple example, at every iteration \(t\), \(_{t}(_{t}^{(2)})=0\), while the standard squared loss is non-zero at \(_{t}^{(1)}\), though we use the same linear function approximation model in both cases. In Prop. 16, we prove that for a 2-arm bandit with deterministic rewards and linear critic parameterization, minimizing the decision-aware critic loss will always result in convergence to the optimal arm.

### Softmax representation

Recall that for the softmax functional representation, policy \(\) is represented by the logits \(z^{}(s,a)\) for each \(s\) and \(a\) such that \(p^{}(a|s)=(s,a)}}{_{a^{}}^{2^{}(s,a^{ })}}\). Using the policy gradient theorem, \([_{}J()]_{s,a}=d^{}(s)\,A^{}(s,a)\,p^{}(a|s)\) where \(A^{}\) is the advantage function. Similar to Vaswani et al. , we use a weighted (across states) log-sum-exp mirror map implying that \(D_{}(z,z^{})=_{s}d^{_{t}}(s)\,D_{}(z(s, ),z^{}(s,))\) where \((z(s,))=(_{a}(z(s,a)))\) and hence, \(D_{}(z(s,),z^{}(s,))=(p^{^{}}(|s),p ^{}(|s))\) (see Lemma 11 for a derivation). We now instantiate **inequality (I)** in Sec. 3.2 in the proposition below (see App. E for the derivation).

**Proposition 6**.: For the softmax representation and log-sum-exp mirror map, \(c>0\), \( 1-\),

\[J()-J(_{t}) _{s d^{_{t}}}\,_{a p^{_{t} (|s)}}[(^{_{t}}(s,a)++) \,((a|s)}{p^{_{t}}(a|s)})]\] \[-\,_{s d^{_{t}}}_{a p^{ _{t}(|s)}}[(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a )])\,(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a) ])]\]

where \(^{_{t}}\) is the estimate of the advantage function for policy \(_{t}\).

For incorporating policy (with parameters \(\)) and critic (with parameters \(\)) parameterization, we note that \(p^{}(a|s)=(s,a|))}{_{a^{}}(z^{}(s, a^{}|))}\) and \(^{}(s,a)=A^{}(s,a|)\) where the model is implicit in the notation. Using the reasoning in Sec. 3.2 with Prop. 6 immediately gives us the actor and critic objectives (\(_{t}()\) and \(L_{t}()\) respectively) at iteration \(t\) and completely instantiates Algorithm 1. Similar to the direct representation, observe that \(_{t}\) is asymmetric and penalizes the under/over-estimation of the advantage function differently.

To demonstrate the effectiveness of the proposed critic loss, we construct a two-armed bandit example in Prop. 7 below (see App. E for details), use the softmax representation and tabular parameterization for the policy and consider a discrete hypothesis class (with two hypotheses) as the model for the critic. We compare minimizing the squared loss on the advantage: \(E_{s d^{_{t}}}_{a p^{_{t}}(|s)}[A^{_{t}}( s,a)-A^{_{t}}(s,a|)]^{2}\) with minimizing the decision-aware loss. We see that minimizing the decision-aware critic loss can distinguish between the two hypotheses and choose the correct hypothesis resulting in convergence to the optimal action.

**Proposition 7**.: Consider a two-armed bandit example and define \(p\) as the probability of pulling arm \(1\). Given \(p\), let the advantage of arm \(1\) be equal to \(A_{1}:=>0\), while that of arm \(2\) is \(A_{2}:=-<0\) implying that arm \(1\) is optimal. For \((,1)\), consider approximating the advantage of the two arms using a function approximation model with two hypotheses that depend on \(p\): \(_{0}:_{1}=+\,,_{2}=- \,(+)\) and \(_{1}:_{1}=-\,(-p)\,,_{2}=-\,(-\, (-p))\) where sgn is the signum function. If \(p_{t}\) is the probability of pulling arm 1 at iteration \(t\), consider minimizing two alternative loss functions to choose the hypothesis \(_{t}\):

(1) Squared loss: \(_{t}=*{arg\,min}_{\{_{0},_{1}\}} \{}{2}[A_{1}-_{1}]^{2}+}{2} [A_{2}-_{2}]^{2}\}\).

(2) Decision-aware critic loss with \(c=1\): \(_{t}=*{arg\,min}_{\{_{0},_{1}\}}\)

\(\{p_{t}(1-[A_{1}-_{1}])\,(1-[A_{1} -_{1}])+(1-p_{t})(1-[A_{2}-_{2}] )\,(1-[A_{2}-_{2}])\}\).

For \(p_{0}\), the squared loss cannot distinguish between \(_{0}\) and \(_{1}\), and depending on how ties are broken, minimizing it can result in convergence to the sub-optimal action. On the other hand, minimizing the divergence loss (for any \(p_{0}>0\)) results in convergence to the optimal arm.

In Prop. 13 in App. E, we study the softmax representation with the Euclidean mirror map and instantiate **inequality (I)** for this case. Finally, in App. B, we instantiate our actor-critic framework to handle stochastic value gradients used for learning continuous control policies . In the next section, we consider simple RL environments to empirically benchmark Algorithm 1.

## 6 Experiments

We demonstrate the benefit of the decision-aware framework over the standard AC algorithm where the critic is trained by minimizing the squared error. We instantiate Algorithm 1 for the direct and softmax representations, and evaluate the performance on two grid-world environments, namely Cliff World  and Frozen Lake  (see App. F for details). We compare the performance of three AC algorithms that have the same actor, but differ in the objective function used to train the critic.

**Critic Optimization**: For the direct and softmax representations, the critic's objective is to estimate the action-value (\(Q\)) and advantage (\(A\)) functions respectively. We use a linear parameterization for the \(Q\) function implying that for each policy \(\), \(Q^{}(s,a|)=,(s,a)\), where \((s,a)^{d}\) are features obtained via tile-coding [53, Ch. 9]. We vary the dimension \(d\{80,60,40\}\) of the tile-coding features to vary the expressivity of the critic. Given the knowledge of \(p^{}\) and the estimate \(Q^{}(s,a|)\), the estimated advantage can be obtained as: \(A^{}(s,a|)=Q^{}(s,a|)-_{a}p^{}(a|s)\,Q^{}(s,a|)\). We consider two ways to estimate the \(Q\) function for training the critic: (a) using the known MDP to exactly compute the \(Q\) values and (b) estimating the \(Q\) function using Monte-Carlo (MC) rollouts. There are three sources of error for an insufficient critic - the bias due to limited model capacity, the optimization error due to an insufficient number of inner iterations (\(m_{c}\)) and the variance due to Monte-Carlo sampling. We use a large value of \(m_{c}\) and sufficiently large

Figure 1: Comparison of decision-aware, Adv-MSE and MSE loss functions using a linear actor and linear (with three different dimensions) critic in the Cliff World environment for direct and softmax policy representations. For \(d=80\) (corresponding to an expressive critic), all algorithms have the same performance. For \(d=40\) and \(d=60,\) MSE does not have monotonic improvement and converges to a sub-optimal policy. Adv-MSE almost always reaches the optimal policy. Compared to the Adv-MSE and MSE, minimizing the decision-aware loss always results in convergence to the optimal policy at a faster rate, especially when using a less expressive critic (\(d=40\)).

Monte-Carlo rollouts to control the optimization error and variance respectively (see App. F). This ensures that the bias dominates, and enables us to isolate the effect of the form of the critic loss.

We evaluate the performance of the decision-aware loss defined for the direct (Prop. 4) and softmax representations (Prop. 6). For both representations, we minimize the corresponding objective at each iteration \(t\) (Lines 6-8 in Algorithm 1) using gradient descent with the step-size \(_{c}\) determined by the Armijo line-search . We use a grid-search to tune the trade-off parameter \(c\), and propose an alternative albeit conservative method to estimate \(c\) in App. F. We compare against two baselines (see App. F for implementation details) - (i) the standard squared loss on the \(Q\) functions (referred to as MSE in the plots) defined in Prop. 5 and (ii) squared loss on \(A\) function (referred to as Adv-MSE in the plots) defined in Prop. 7. We note that the Adv-MSE loss corresponds to a second-order Taylor series expansion of the decision-aware loss (see Prop. 14 for details), and is similar to the loss in Pan et al. . Recall that the critic error consists of the variance when using MC samples (equal to zero when we exactly compute the \(Q\) function) and the bias because of the critic optimization error (controlled since the critic objective is convex) and error due to the limited expressivity of the linear function approximation (decreases as \(d\) increases). Since our objective is to study the effect of the critic loss and its interaction with function approximation, we do not use bootstrapping to estimate the \(Q^{}\) since it would result in a confounding bias term.

**Actor Optimization**: For all algorithms, we use the same actor objective defined for the direct (Prop. 4) and softmax representations (Prop. 6). We consider both the tabular and linear policy paramterization for the actor. For the linear function approximation, we use the same tile-coded features and set \(n=60\) for both environments. We update the policy parameters at each iteration \(t\) in the off-policy inner-loop (Lines 11-13 in Algorithm 1) using Armijo line-search to set \(_{a}\). For details about the derivatives and closed-form solutions for the actor objective, refer to  and App. F. We use a grid-search to tune \(\), and compare different values. Our experimental setup 1 enables us to isolate the effect of the critic loss, without non-convexity or optimization issues acting as confounders.

**Results**: For each environment, we conduct four experiments that depend on (a) whether we use MC samples or the true dynamics to estimate the \(Q\) function, and (b) on the policy parameterization. We only show the plot corresponding to using the true dynamics for estimating the \(Q\) function and linear policy paramterization, and defer the remaining plots to App. G. For all experiments, we report the mean and \(95\%\) confidence interval of \(J()\) averaged across \(5\) runs. In the main paper, we only include \(2\) values of \(\{0.01,0.1\}\) and vary \(d\{40,60,80\}\), and defer the complete figure with a broader range of \(\) and \(d\) to App. G. For this experiment, \(c\) is tuned to \(0.01\) and we include a sensitivity (of \(J()\) to \(c\)) plot in App. G. From Fig. 1, we see that (i) with a sufficiently expressive critic (\(d=80\)), all algorithms reach the optimal policy at nearly the same rate. (ii) as we decrease the critic capacity, minimizing the MSE loss does not result in monotonic improvement and converges to a sub-optimal policy, (iii) minimizing the Adv-MSE usually results in convergence to the optimal policy, whereas (iv) minimizing the decision-aware loss results in convergence to better policies at a faster rate, and is more beneficial when using a less-expressive critic (corresponding to \(d=40\)). We obtain similar results for the tabular policy parameterization or when using sampling to estimate the \(Q\) function (see App. G for additional results).

## 7 Discussion

We designed a generic decision-aware actor-critic framework where the actor and critic are trained cooperatively to optimize a joint objective. Our framework can be used with any policy representation and easily handle general policy and critic parameterization, while preserving theoretical guarantees. Instantiating the framework resulted in an actor that supports off-policy updates, and a corresponding critic loss that can be minimized using first-order optimization. We demonstrated the benefit of our framework both theoretically and empirically. We note that Algorithm 1 can be directly used with any complex actor/critic parameterization in order to generalize across states/actions. The theoretical guarantees of Prop. 2 would still hold. From a practical perspective, w.r.t tuning hyperparameters, \(\) does not depend on the actor/critic parameterization. On the other hand, \(_{a}\) and \(_{c}\) are set adaptively using an Armijo line-search that only requires the smoothness of actor/critic objectives, and does not depend on their convexity. However, Algorithm 1 does require tuning hyper-parameter \(c\), and we will aim to investigate automatic adaptive ways to set it. In the future, we aim to benchmark Algorithm 1 for complex deep RL environments. Finally, we aim to broaden the scope our framework to applications such as variational inference.

[MISSING_PAGE_FAIL:11]

*  Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Single-timescale actor-critic provably finds globally optimal policy. _arXiv preprint arXiv:2008.00483_, 2020.
*  Gianluca Gorni. Conjugation and second-order properties of convex functions. _Journal of Mathematical Analysis and Applications_, 158(2):293-315, 1991.
*  Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. _arXiv preprint arXiv:1801.01290_, 2018.
*  Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In _Advances in Neural Information Processing Systems_, pages 2944-2952, 2015.
*  Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic. _SIAM Journal on Optimization_, 33(1):147-180, 2023.
*  Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In _International conference on machine learning_, pages 2961-2970. PMLR, 2019.
*  Emmeran Johnson, Ciara Pike-Burke, and Patrick Rebeschini. Optimal convergence rate for exact policy mirror descent in discounted markov decision processes. _arXiv preprint arXiv:2302.11381_, 2023.
*  Sham Kakade. A natural policy gradient. In _NIPS_, volume 14, pages 1531-1538, 2001.
*  Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In _International Conference on Machine Learning (ICML)_, pages 267-274, 2002.
*  Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-lojasiewicz condition. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16_, pages 795-811. Springer, 2016.
*  Sajad Khodadadian, Thinh T Doan, Justin Romberg, and Siva Theja Maguluri. Finite sample analysis of two-time-scale natural actor-critic algorithm. _IEEE Transactions on Automatic Control_, 2022.
*  Vijay Konda and John Tsitsiklis. Actor-critic algorithms. _Advances in neural information processing systems_, 12, 1999.
*  Harshat Kumar, Alec Koppel, and Alejandro Ribeiro. On the sample complexity of actor-critic method for reinforcement learning with function approximation. _Machine Learning_, pages 1-35, 2023.
*  Jonathan Wilder Lavington, Sharan Vaswani, Reza Babanezhad, Mark Schmidt, and Nicolas Le Roux. Target-based surrogates for stochastic optimization. _arXiv preprint arXiv:2302.02607_, 2023.
*  Chongchong Li, Yue Wang, Wei Chen, Yuting Liu, Zhi-Ming Ma, and Tie-Yan Liu. Gradient information matters in policy optimization by back-propagating through model. In _International Conference on Learning Representations_, 2021.
*  Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. _Applied and Computational Harmonic Analysis_, 59:85-116, 2022.
*  Haihao Lu, Robert M Freund, and Yurii Nesterov. Relatively smooth convex optimization by first-order methods, and applications. _SIAM Journal on Optimization_, 28(1):333-354, 2018.
*  Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. _arXiv preprint arXiv:1807.03858_, 2018.

*  Jincheng Mei, Chenjun Xiao, Ruitong Huang, Dale Schuurmans, and Martin Muller. On principled entropy exploration in policy optimization. In _IJCAI_, pages 3130-3136, 2019.
*  Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. _arXiv preprint arXiv:2005.06392_, 2020.
*  Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.
*  Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. _The Journal of Machine Learning Research_, 21(1):5183-5244, 2020.
*  Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. _Advances in neural information processing systems_, 30, 2017.
*  Alex Olshevsky and Bahman Gharesifard. A small gain analysis of single timescale actor critic. _arXiv preprint arXiv:2203.02591_, 2022.
*  Hsiao-Ru Pan, Nico Gurtler, Alexander Neitz, and Bernhard Scholkopf. Direct advantage estimation. _Advances in Neural Information Processing Systems_, 35:11869-11880, 2022.
*  Jan Peters, Sethu Vijayakumar, and Stefan Schaal. Natural actor-critic. In _Machine Learning: ECML 2005: 16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005. Proceedings 16_, pages 280-291. Springer, 2005.
*  Martin L. Puterman. _Markov Decision Processes: Discrete Stochastic Dynamic Programming_. John Wiley & Sons, Inc., USA, 1994.
*  Andrew J Schaefer, Matthew D Bailey, Steven M Shechter, and Mark S Roberts. Modeling medical treatment using Markov decision processes. In _Operations research and health care_, pages 593-612. Springer, 2005.
*  John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International Conference on Machine Learning (ICML)_, pages 1889-1897, 2015.
*  John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning. _arXiv preprint arXiv:1704.06440_, 2017.
*  John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _CoRR_, abs/1707.06347, 2017.
*  David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. _Journal of Machine Learning Research_, 2014.
*  David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _Nature_, 529(7587):484, 2016.
*  David Silver, Hado Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, et al. The predictron: End-to-end learning and planning. In _International Conference on Machine Learning_, pages 3191-3199. PMLR, 2017.
*  Richard S Sutton. Learning to predict by the methods of temporal differences. _Machine learning_, 3(1):9-44, 1988.
*  Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. MIT Press, 2 edition, 2018.

*  Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 1057-1063, 2000.
*  Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. _arXiv preprint arXiv:1804.10332_, 2018.
*  Manan Tomar, Lior Shani, Yonathan Efroni, and Mohammad Ghavamzadeh. Mirror descent policy optimization. _arXiv preprint arXiv:2005.09814_, 2020.
*  Sharan Vaswani, Olivier Bachem, Simone Totaro, Robert Muller, Shivam Garg, Matthieu Geist, Marlos C Machado, Pablo Samuel Castro, and Nicolas Le Roux. A general class of surrogate functions for stable and efficient reinforcement learning. _arXiv preprint arXiv:2108.05828_, 2021.
*  Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.
*  Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine learning_, 8(3-4):229-256, 1992.
*  Yue Frank Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite-time analysis of two time-scale actor-critic methods. _Advances in Neural Information Processing Systems_, 33:17617-17628, 2020.
*  Lin Xiao. On the convergence rates of policy gradient methods. _Journal of Machine Learning Research_, 23(282):1-36, 2022.
*  Tengyu Xu, Zhe Wang, and Yingbin Liang. Non-asymptotic convergence analysis of two time-scale (natural) actor-critic algorithms. _arXiv preprint arXiv:2005.03557_, 2020.
*  Rui Yuan, Robert M Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policy gradient. In _International Conference on Artificial Intelligence and Statistics_, pages 3332-3380. PMLR, 2022.
*  Andy Zeng, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. Tossingbot: Learning to throw arbitrary objects with residual physics. _IEEE Transactions on Robotics_, 36(4):1307-1319, 2020.
*  Siqi Zhang and Niao He. On the convergence rate of stochastic mirror descent for nonsmooth nonconvex optimization. _arXiv preprint arXiv:1806.04781_, 2018.

**Supplementary material**

## Appendix A Definitions

* **[S]**: We define the solution set \(^{*}\) for a function \(f\) as \(^{*}:=\{x^{*}|x^{*}_{x(f)}f(x)\}\).
* **[Convexity]**. A differentiable function \(f\) is convex iff for all \(v\) and \(w\) in \((f)\) \[f(v) f(w)+ f(w),\,v-w.\] (Convexity)
* **[Lipschitz continuity]**. A differentiable function \(f\) is \(G\)-Lipschitz continuous, meaning that for all \(v\) and \(w\) and constant \(G>\), \[|f(v)-f(w)| G\,\|v-w\|\| f(v)\| G\,.\] (Lipschitz Continuity)
* **[Smoothness]**. A differentiable function \(f\) is \(L\)-smooth, meaning that for all \(v\) and \(w\) and some constant \(L>0\) \[f(v) f(w)+ f(w),\,v-w+\,\|v-w\|_{2}^{2}\.\] (Smoothness)
* **[Polyak-Lojasiewicz inequality]**. A differentiable function \(f\) satisfies the Polyak-Lojasiewicz (PL) inequality if there exists a constant \(_{p}>0\) s.t. for all \(v\), \[_{p}(f(v)-f^{*})\| f(v)\|_{2}^{2}\,,\] (PL) where \(f^{*}\) is the optimal function value i.e. \(f^{*}:=f(x^{*})\) for \(x^{*}^{*}\).
* **[Restricted Secant Inequality]**. A differentiable function \(f\) satisfies the Restricted Secant Inequality (RSI) inequality if there exists a constant \(_{r}>0\) that for all \(v\) \[ f(v),\,v-v_{p}_{r}\|v-v_{p}\|_{2}^{2}\,,\] (RSI) where \(v_{p}\) is the projection of \(v\) onto \(^{*}\).
* **[Bregman divergence]**. For a strictly-convex, differentiable function \(\), we define the Bregman divergence induced by \(\) (known as the mirror map) as: \[D_{}(w,v):=(w)-(v)-(v),\,w-v.\] (Bregman divergence)
* **[Relative smoothness]**. A function \(f\) is \(\)-relatively smooth w.r.t. \(D_{}\) iff \(f+\) is convex. Furthermore, if \(f\) is \(\)-relatively smooth w.r.t. \(\), then, \(|f(w)-f(v)- f(v),\,w-v|\,D_{}(w,v)\).
* **[Mirror Ascent]**. Optimizing \(_{x}f(x)\) using mirror ascent (MA), if \(x_{t}\) is the current iterate, then the update at iteration \(t\{0,1,,T-1\}\) with a step-size \(_{t}\) and mirror map \(\) is given as: \[x_{t+1}:=*{arg\,max}_{x}\{ f(x_{ t}),\,x-}D_{}(x,x_{t})\},\] (MD update)The above update can be formulated into two steps Bubeck [7, Chapter 4] as follows:

\[y_{t+1} :=()^{-1}((x_{t})+_{t} f(x_{t}))\] (Move in dual space) \[x_{t+1} :=*{arg\,min}_{x}\{D_{}(x,y_{ t+1})\}\] (Projection step)

## Appendix B Extension to stochastic value gradients

In Sec. 5, we have seen alternative ways to represent a policy's conditional distributions over actions \(p^{}(|s)\) for each state \(s\). On the other hand, stochastic value gradients  represent a policy by a set of actions. Formally, if \(\) are random variables drawn from a fixed distribution \(\), then policy \(\) is a deterministic map from \(\). This corresponds to the functional representation of the policy, and is particularly helpful for continuous control, i.e. when the action-space is continuous. The action \(a\) chosen by \(\) in state \(s\), when fixing the random variable \(=\), is represented as \((s,)\), and the value function for policy \(\) is given as:

\[J()=_{s}d^{}(s)_{}\,r(s,(s, ))\,d\] (1)

and Silver et al.  showed that \(=d^{}(s)_{a}Q^{} (s,a)_{a=(s,)}\). In order to characterize the dependence on the policy parameterization, we note that \((s,)=(s,,)\) where \(\) are the model parameters. For a fixed \(\), we will use a Euclidean mirror map implying that \(D_{}(,^{})=_{s}d^{_{}}(s)D_{ }((s,),^{}(s,)\) and choose \(((s,))=\,\|^{}(s,)\|_{2} ^{2}\) implying that \(D_{}((s,),^{}(s,)=\,[(s, )-^{}(s,)]^{2}\). In order to instantiate the generic lower bound in Prop. 1 at iteration \(t\), we prove the following proposition in App. E.

**Proposition 8**.: For the stochastic value gradient representation and Euclidean mirror map, \(c>0\), \(\) such that \(J+\) is convex in \(\).

\[J()-J(_{t})  C+_{s d^{_{t}}}_{ }[_{a}Q^{_{t}}(s,a)_{a=_{t}(s,)}\,(s, )-\,(+)\,[_ {t}(s,)-(s,)]^{2}]\] \[-\,_{s d^{_{t}}}_{ }\,[_{a}Q^{_{t}}(s,a)_{a=_{t}(s, )}-Q^{_{t}}}(s,a)_{a=_{t}(s,) }]^{2}\]

where \(C\) is a constant and \(Q^{_{t}}}(s,a)_{a=_{t}(s,)}\) is the estimate of the action-value gradients for policy \(\) at state \(s\) and \(a=_{t}(s,)\).

For incorporating policy (with parameters \(\)) and critic (with parameters \(\)) parameterization, we note that \((s,)=(s,|)\) and \(Q^{_{t}}}(s,a)_{a=_{t}(s,)}=_{a}Q^ {_{t}}(s,a|)_{a=_{t}(s,,_{t})}\) where the model is implicit in the notation. Using the reasoning in Sec. 3.2 with Prop. 8 immediately gives us the actor and critic objectives (\(_{t}()\) and \(L_{t}()\) respectively) at iteration \(t\) and completely instantiates Algorithm 1. The actor objective is similar to Eq (15) of Silver et al. , with the easier to compute \(Q^{_{t}}\) instead of \(Q^{}\), whereas the critic objective is similar to the one used in existing work on policy-aware model-based RL for continuous control .

Proofs for Sec. 3

**Proposition 1**.: For any policy representations \(\) and \(^{}\), any strictly convex mirror map \(\), and any gradient estimator \(\), for \(c>0\) and \(\) such that \(J+\) is convex in \(\),

\[J() J(^{})+(^{}),-^{} -(+)D_{}(,^{})- \,D_{^{*}}(^{})-c[ J(^{ })-(^{})],(^{})\]

where \(^{*}\) is the Fenchel conjugate of \(\) and \(D_{^{*}}\) is the Bregman divergence induced by \(^{*}\).

Proof.: For any \(\) such that \(J+\) is convex, we use Lemma 2 to form the following lower-bound,

\[J()  J(^{})+ J(^{}),(-^{ })-\,D_{}(,^{})\] \[=J(^{})+(^{}),(-^{} )+ J(^{})-(^{}),(-^{ })-\,D_{}(,^{})\]

Defining \(:= J(^{})-(^{})\), and assuming that \(c\,\) is small enough to satisfy the requirement for Lemma 1, we use Lemma 1 with \(x=\), \(y=\) and \(y^{}=^{}\).

\[=J(^{})+(^{}),(-^{}) -\,D_{}(,^{})-\,[D_{}(, ^{})+D_{^{*}}((^{})-c,(^{ }))]\] \[ J() J(^{})+(^{})^{}(- ^{})-(+)D_{}(,^{ })-D_{^{*}}(^{})-c[ J(^{ })-(^{})],(^{})\]

**Lemma 1** (Bregman Fenchel-Young).: _Let \(x^{*}\), \(y\), \(y^{}\). Then, for sufficiently small \(c>0\) and \(x\) s.t. \(()^{-1}[(y^{})-c\,x]\), we have_

\[ y-y^{},x-D_{}(y,y^{})+D _{^{*}}((y^{})-c\,x,(y^{}))\;.\] (2)

_For a fixed \(y^{}\), this inequality is tight for \(y=_{v} x,v-y^{}+D_{}(v,y) }\)._

Proof.: Define \(f(y):= x,y-y^{}+D_{}(y,y^{})\). If \(y^{*}= f(y)\), then,

\[ f(y^{*}) =0(y^{*})=(y^{})-cx\] \[y^{*} =()^{-1}[(y^{})-cx] y^{*}= ^{*}[(y^{})-cx]\]

Note that according to our assumption, \(y^{*}\). For any \(y\),

\[f(y) f(y^{*})= x,y^{*}-y^{}+D_{}(y^{*}, y^{})\] (3)

In order to simplify \(D_{}(y^{*},y^{})\), we will use the definition of \(^{*}(z)\). In particular, for any \(y\),

\[(y) =_{z}\,[ z,y-^{*}(z)]; z^{*}= *{arg\,max}_{z}\,[ z,y-^{*}(z)] y= ^{*}(z^{*}) z^{*}=(y)\] \[(y) =(y),y-^{*}((y))\] (4) \[D_{}(y^{*},y^{}) =(y^{*})-(y^{})-(y^{}),y^{*}- y^{}\] \[=[(y^{*}),y^{}-^{*}( (y^{*}))]-(y^{})-(y^{}),y^{*}-y^{}\]

Let us focus on the first term and simplify it,

\[(y^{*}),y^{*}-^{*}((y^{*})) =(^{*}[(y^{})-cx] ),^{*}[(y^{})-cx]-^{*}( (^{*}[(y^{})-cx]))\] \[=[(y^{})-cx],^{*}[(y^ {})-cx]-^{*}([(y^{})-cx])\;\;(z))=z$)}\]Using the above relations,

\[D_{}(y^{*},y^{}) =[(y^{})-cx],^{*}[(y^{ })-cx]-^{*}([(y^{})-cx])-(y^{})\] \[-(y^{}),^{*}[(y^{ })-cx]-y^{}\] \[=(y^{}),^{*}[(y^{ })-cx]-c x,^{*}[(y^{})-cx]\] \[-^{*}([(y^{})-cx])-(y^{})- (y^{}),^{*}[(y^{})-cx]-y^{}\] \[ D_{}(y^{*},y^{}) =-c x,^{*}[(y^{})-cx]- ^{*}([(y^{})-cx])-(y^{})+(y^{ }),y^{}\]

Using the above simplification with Eq. (3),

\[f(y)  x,y^{*}-y^{}+[-c x,^{*}[(y^{})-cx]-^{*}([(y^{ })-cx])-(y^{})+(y^{}),y^{}]\] \[= x,y^{*}-y^{}- x,^{*}[ (y^{})-cx]-[^{*}([(y^{ })-cx])+(y^{})-(y^{}),y]\] \[=- x,y^{}+ x,^{*}[ (y^{})-cx]- x,^{*}[(y^{})-cx ]-[^{*}([(y^{})-cx])+(y^{ })-(y^{}),y^{}]\] \[=- x,y^{}-[^{*}([ (y^{})-cx])+(y^{})-(y^{}),y^{ }]\]

Using Eq. (4), \((y^{})=(y^{}),y^{}-^{*}( (y^{}))(y^{})-(y^{} ),y^{}=-^{*}((y^{}))\),

\[ f(y)- x,y^{}-[^{*}([ (y^{})-cx])-^{*}((y^{}))]=-[c x,y+^{*}([(y^{})-cx])-^{*}( (y^{}))]\]

Using the definition of \(f(y)\),

\[ x,y-y^{}+D_{}(y,y^{}) -D_{}^{*}((y^{})-cx,(y^{ }))\] \[ x,y-y^{} -[D_{}(y,y^{})+D_{}^{*}( (y^{})-cx,(y^{}))]\]

**Lemma 2**.: _If \(J+\) is convex, then, \(J()\) is \(\)-relatively smooth w.r.t to \(D_{}\), and satisfies the following inequality,_

\[J() J(^{})+_{}J(^{}),-^{ }-\,D_{}(,^{})\]

Proof.: If \(J+\) is convex,

\[(J+)() (J+)(^{})+ -^{},_{}(J+)(^{})\] \[ J()  J(^{})+-^{},_{}J(^{ })-\,D_{}(,^{})\]

Proofs for Sec. 4

**Proposition 2**.: For any policy representation and any policy or critic parameterization, there exists a \((,c)\) pair that makes the RHS of **inequality (I)** strictly positive, and hence guarantees monotonic policy improvement (\(J(_{t+1})>J(_{t})\)), if and only if

\[ b_{t},_{t}^{}b_{t}>[ J(_{t})- _{t}],[_{}^{2}(_{t})]^{-1}[ J(_{t})-_{t}],\]

where \(b_{t}^{n}:=_{s}_{a}[ {g}_{t}]_{s,a}_{}[(_{t})]_{s,a}\), \(_{t}^{n n}:=_{}(_{t})^{ { T}}_{}^{2}(_{t})\,_{}(_{t})\) and \(_{t}^{}\) denotes the pseudo-inverse of \(_{t}\). For the special case of the tabular policy parameterization, the above condition becomes equal to,

\[_{t},[_{}^{2}(_{t})]^{-1}_{t}> [ J(_{t})-_{t}],[_{}^{2}(_{t})]^{-1} [ J(_{t})-_{t}].\]

Proof.: As a warmup, let us first consider the tabular parameterization where \(()=^{SA}\). In this case, the lower-bound in Prop. 1 is equal to,

\[J()-J(_{t})_{t},-_{t}-(+)D_{}(,_{t})-\,D_{^{ }}(_{t})-c[ J(_{t})- _{t}],(_{t})\]

We shall do a second-order Taylor expansion of the critic objective (blue term) in \(c\) around 0 and a second-order Taylor expansion of the actor objective (green term) around \(=_{t}\). Defining \(:= J(_{t})-_{t}\),

\[=_{t},-_{t}-\,( +)\,(-_{t})^{}[ ^{2}(_{t})](-_{t})-\, ,^{2}^{}((_{t}))\,+o(c)+o(\| -_{t}\|_{2}^{2})\,,\] (Using Prop. 14 )

where \(o(c)\) and \(o(\|-_{t}\|_{2}^{2})\) consist of the higher order terms in the Taylor series expansion. A necessary and sufficient condition for monotonic improvement is equivalent to finding a \((,c)\) such that RHS is positive. As \(c\) tends to 0, the \(\) maximizing the RHS is

\[^{}=_{t}+[^{2}(_{t}) ]^{}_{t}\]

With this choice,

\[ =\,+}\,_{t},[^{2}(_{t})]^{-1}_{t}-\,,^{2}^{}((_{t}))\,+o (c)\|_{2}^{2})$ is subsumed by $o(c)$)}\] \[=\,_{t},[^{2}(_{t}) ]^{-1}_{t}-\,,^{2}^{ }((_{t}))\,+\,( {1}{+}-c)_{t},[^{2} (_{t})]^{-1}_{t}}_{o(c)}+o(c)\] \[=\,_{t},[^{2}(_{t}) ]^{-1}_{t}-\,,^{2}^{ }((_{t}))\,+o(c)\]

If \(_{t},[^{2}(_{t})]^{-1}_{t} >,^{2}^{}((_{t}))\,\), i.e. there exists an \(>0\) s.t. \(_{t},[^{2}(_{t})]^{-1}_{t} =,^{2}^{}((_{t}))\, +\), then, \(=+o(c)\). For any fixed \(>0\), since \(o(c)/c 0\) as \(c 0\), there exists a neighbourhood \((0,c_{})\) around zero such that for all \(c\) in this neighbourhood, \(o(c)/c>-\) and hence \(o(c)>- c\). Setting \(=\), there is a \(c\) such that

\[>>0\]

Hence, there exists a \(c(0,\{,c_{}\})\) such that the RHS is positive, and is hence sufficient to guarantee monotonic policy improvement.

On the other hand, if \(_{t},[^{2}(_{t})]^{-1}_{t} <,^{2}^{}((_{t}))\,\), i.e. there exists an \(>0\) s.t. \(_{t},[^{2}(_{t})]^{-1}_{t} =,^{2}^{}((_{t}))\,-\), then, \(=+o(c)\) which can be negative and hence monotonic improvement can not be guaranteed. Hence, \(_{t},[^{2}(_{t})]^{-1}_{t} >,^{2}^{}((_{t}))\,\) is a necessary and sufficient condition for improvement.

Let us now consider the more general case, and define \(m=SA\), \(_{t}^{m 1}\), \(()^{m 1}\) is a function of \(^{n 1}\). Rewriting Prop. 1,

\[J()-J(_{t})_{t},()-(_{t})-( +)D_{}((),(_{t}))- \,D_{^{}}(_{t})-c[ J(_{t})- _{t}],(_{t})\]As before, we shall do a second-order Taylor expansion of the critic objective (blue term) in \(c\) around \(0\) and a second-order Taylor expansion of the actor objective (green term) around \(=_{t}\). Defining \(:= J(_{t})-_{t}\). From Prop. 14, we know that,

\[\,D_{^{*}}\!((_{t})-c[ J(_{t})-_{t}],(_{t}))=,^{2}^{*}( (_{t}))\,+o(c)\]

In order to calculate the second-order Taylor series expansion of the actor objective, we define \(_{}(_{t})^{m n}\) as the Jacobian of the \(:\) map, and use \(_{}[(_{t})]_{i}^{1 n}\) for \(i[m]\) to refer to row \(i\)

\[_{t},()-(_{t})=_{i=1}^{ m}_{t}]_{i}}_{1 1}[( _{t})]_{i}}_{1 n})}_{n 1}+ \,)}_{1 n}[_{i=1}^{m} _{t}]_{i}}_{1 1}^{2}[( _{t})]_{i}}_{n n}])}_{n 1}+ o(\|-_{t}\|_{2}^{2})\]

where \(o(\|-_{t}\|_{2}^{2})\) consist of the higher order terms in the Taylor series expansion. For expanding the divergence term, note that \(D_{}((),(_{t}))=(())-((_{t})) -((_{t})),()-(_{t})\)

\[(())-((_{t})) =(_{t})^{}}_{1 m} (_{t})}_{m n})}_{n 1}\] \[+\,)^{}}_{1 n} \,[(_{t})^{}}_{n m} ^{2}(_{t})}_{m m}.(_{t})}_{m n}+_{i=1}^{m}(_{t})]_{i}}_{1 1}\,^{2}[(_{t})]_{i}}_{n  n}]\,)}_{n 1}+ o(\|-_{t}\|_{2}^{2})\]

\[((_{t})),()-(_{t})= _{i=1}^{m})]_{i}}_{1 1}(_{t})]_{i}}_{1 n}\,)}_{n 1}+ \,)}_{1 n}[_{i=1}^{m} )]_{i}}_{1 1}\,^{2}[(_{t})]_{i}}_{n n}])}_{n 1}+ o(\|-_{t}\|_{2}^{2})\]

Putting everything together,

\[=(_{i=1}^{m}\,_{t}]_{i}}_{1  1}\,[(_{t})]_{i}}_{1 n}) \,)}_{n 1}\] \[+\,)^{}}_{1 n} \,[_{i=1}^{m}\,(_{t}]_{i}}_{1 1}\, ^{2}[(_{t})]_{i}}_{n n})-( +)\,(_{t})^ {}}_{n m}\,^{2}(_{t})}_{m m} \,(_{t})}_{m n}]\,(}{n 1})\] \[-\,,^{2}^{*}((_{t }))\,+o(\|-_{t}\|_{2}^{2})+o(c)\]

Defining \(b_{t}:=_{i=1}^{m}\,[_{t}]_{i}\,_{}[(_{t})]_{i}\) and \(H_{t}:=_{}(_{t})^{}\,_{}^{2}(_{t})\, _{}(_{t})-+)} \,_{i=1}^{m}\,([_{t}]_{i}\,_{}^{2}[(_{t}) ]_{i})\)

\[= b_{t},-_{t}+\, (+)\,(-_{t}),H_{t} \,(-_{t})-\,,^{2}^{*}( (_{t}))\,+o(\|-_{t}\|_{2}^{2})+o(c)\]

As a sanity check, it can be verified that if \(()=\), \(H_{t}=(+)^{2}(_{t})\) and \(b_{t}=_{t}\), and we recover the tabular result above. Notice that \((-_{t}),+ )}\,_{i=1}^{m}\,([_{t}]_{i}\,_{}^{2}[( _{t})]_{i})(-_{t})\) is \(o(c)\) as \(c\) goes to zero. Subsuming this term in \(o(c)\),

\[= b_{t},-_{t}+\, (+)\,(-_{t}), _{t}\,(-_{t})-\,,^{2} ^{*}((_{t}))\,+o(\|-_{t}\|_{2}^{2})+o(c)\]

where \(_{t}:=_{}(_{t})^{}\,_{}^{2}(_{t })\,_{}(_{t})\). As before, a necessary and sufficient condition for monotonic improvement is equivalent to finding a \((,c)\) such that RHS is positive. As \(c\) tends to \(0\), the \(\) maximizing the RHS is

\[^{*}=_{t}+[_{t}]^{ }b_{t}\]

With this choice,

\[=\,\,+}\, b _{t},[_{t}]^{}b_{t}-\, ,^{2}^{*}((_{t}))\,+o(c) (o(\|-_{t}\|_{2}^{2})o(c))\]As in the tabular case, since \(+\) is \(o(c)\), we can subsume it, and we get that,

\[= b_{t},[_{t}]^{}b_{ t}-,^{2}^{*}(( _{t}))\,+o(c)\]

Using the same reasoning as in the tabular case above, we can prove that

\[ b_{t},[_{t}]^{}b_{t}> ,^{2}^{*}((_{t}))\,\]

is a necessary and sufficient condition for monotonic policy improvement. Finally, we use Gorni [19, Eq (1.5)] which shows that \(^{2}^{*}((_{t}))=[_{}^{2}(_{t})]^{-1}\) and complete the proof.

### Proof of Prop. 3

The following proposition shows the convergence of inexact mirror ascent in the functional space.

**Proposition 9**.: Assuming that (i) \(J+\,\) is convex in \(\), for a constant \(c>0\), after \(T\) iterations of mirror ascent with \(}=+\) we have

\[(_{+1},_{ })}{^{2}}\,[J(^{*})-J(_{0}) ]+_{t=0}^{T-1}D_{^{*}}(_{t})-c[ J(_{t})-(_{t})],(_{t})]\]

where \(=^{}/2\) and \(\) is picked uniformly random from \(\{0,1,2, T-1\}\).

Proof.: We divide the mirror ascent (MA) update into two steps:

\[(_{t+1})=(_{t})+^{ }_{t}(_{t})(_{t})=_{t}}[(_{t+1})-(_{t})]\] \[_{t+1}=*{arg\,min}_{}D_{}(, _{t+1}).\]

We denote the above update as \(_{t+1}=(_{t})\). Using Prop. 1 with \(=_{t+1}\), \(^{}=_{t}\),

\[J(_{t+1})  J(_{t})+(_{t})^{}(_{t+1 }-_{t})-(+)D_{}(_{t+1 },_{t})-D_{^{*}}(_{t})-c[ J(_{t})-(_{t})],(_{t})}_{:=_{t}^{c}}\] \[ J(_{t})+_{t}}\{D_{}( _{t+1},_{t})+D_{}(_{t},_{t+1})-D_{ }(_{t+1},_{t+1})\}-(+ )D_{}(_{t+1},_{t})-_{t}^{c}\] (using Lemma 4) \[=J(_{t})+_{t}} (_{t},_{t+1})-D_{}(_{t+1},_{t+1})\}}_{:= A}+(_{t}}--)D_{}( _{t+1},_{t})-_{t}^{c}\] \[ J(_{t})+(_{t}}--)D_{}(_{t+1},_{t})-_{t}^{c}\] ( \[A 0\] since \[_{t+1}\] is the projection of \[_{t+1}\] onto \[\] ) \[ J(_{t})++ )}_{:=}D_{}(_{t+1},_{t})-_{t }^{c}\] ( \[1/^{}_{t}=2/+2/c\] )Recursing for \(T\) iterations and dividing by \(1/\), picking \(\) uniformly random from \(\{0,1,2, T-1\}\) and taking expectation we get

\[(_{+1},_{ })}{^{2}} =T}_{t=0}^{T-1}D_{}(_ {t+1},_{t})\] \[[J(_{T})-J(_{0})]}{ T}+^{T-1}_{t}^{c}}{T}\] \[)-J(_{0})]}{ T}+^{T-1} _{t}^{c}}{T}\] \[=\,[J(^{*})-J(_{0})]+ _{t=0}^{T-1}D_{^{*}}(_{t})-c[ J (_{t})-(_{t})],(_{t})]\]

Compared to Dragomir et al. , D'Orazio et al.  that analyze stochastic mirror ascent in the smooth, non-convex setting, our analysis ensures that the (i) sub-optimality gap (the LHS in the above proposition) is always positive, and (ii) uses a different notion of variance that depends on \(D_{^{*}}\).

Similar to Vaswani et al. , we assume that each \(\) is parameterized by \(\). In Algorithm 1, we run gradient ascent (GA) on \(_{t}()\) to compute \(_{t+1}=(_{t+1})\) and interpret the inner loop of Algorithm 1 as an approximation to the projection step in the mirror ascent update. We note that \(_{t}()\) does not have any additional randomness and is a deterministic function w.r.t \(\). Note that \(_{t+1}=\) where \(_{t}=(_{t})\). Assuming that \(_{t}()\) is smooth, and satisfies the PL condition , we get the following convergence guarantee for Algorithm 1.

**Proposition 3**.: For any policy representation and mirror map \(\) such that (i) \(J+\) is convex in \(\), any policy parameterization such that (ii) \(_{t}()\) is smooth w.r.t \(\) and satisfies the Polyak-Lojasiewicz (PL) condition, for \(c>0\), after \(T\) iterations of Algorithm 1 we have that,

\[[(_{+1},_{ })}{^{2}}][)-J(_{0})}_{}+_{t=0}^{T-1}(D_{^{*}} (_{t})-c\,_{t},(_{t})}_{}+[e_{t}]}_{})]\]

where \(_{t}:= J(_{t})-_{t}\), \(:=+\), \(\) is a random variable chosen uniformly from \(\{0,1,2, T-1\}\) and \(e_{t}()})\) is the projection error (onto \(\)) at iteration \(t\).

Proof.: For this proof, we define the following notation:

\[_{t}:=(_{t})\] \[_{t}():=J(_{t})+(_{t})^{}(()- _{t})-(+)D_{}((),_{t})\] \[_{t+1}:=_{t}()\] \[_{t+1}:=(_{t+1})=*{arg\,max} _{}\{}(_{t}),-_{t}-}D_{}(,_{t})\}=(_{t})\] \[$)}\] \[_{t+1}:=(_{t}(),_{t},m_{a})\] \[_{t+1}:=(_{t+1})\,,\]

where \((_{t}(),_{t},m_{a})\) means running GradientAscent for \(m_{a}\) iterations on \(_{t}()\) with the initialization equal to \(_{t}\). Since we assume that \(_{t}\) satisfies the \(PL\)-condition w.r.t. \(\) for all \(t\), based on the results from Karimi et al. , we get

\[_{t}(_{t+1})-_{t}(_{t+1})(- c_{2}m_{a})\,(_{t}(_{t+1})-_{t}(_{t}))}_{:=e_{t}}\]where \(c_{1},\,c_{2}\) are problem-dependent constants related to the smoothness and curvature of \(_{t}\), and \(e_{t}\) is the approximation error diminishes as we increase the value of \(m_{a}\). Following the same steps as before,

\[J(_{t+1})  J(_{t})+(_{t})^{}((_{t+1})-_{t}) -(+)D_{}((_{t+1}),_{t})- _{t}\] (Using Prop. 1) \[ J(_{t})+(_{t})^{}((_{t+1})- _{t})-(+)D_{}((_{t+1 }),_{t})-_{t}\] (Using the above bound for GA) \[=J(_{t})+(_{t})^{}(_{t+1}-_{t})- (+)D_{}(_{t+1},_{t})-e_{t }-_{t}\] \[ J(_{t})+^{}}( _{t+1})-(_{t}),_{t+1}-_{t}-( +)D_{}(_{t+1},_{t})-_{t }^{c}-e_{t}\] (Using the MA update) \[ J(_{t})+^{}}\{D_{}(_{t+1},_{t})+D_{}(_{t},_{t+1})-D_{}(_{t+ 1},_{t+1})\}-(+)D_{} (_{t+1},_{t})-_{t}^{c}-e_{t}\] (using Lemma 4) \[=J(_{t})+^{}}(_{t},_{t+1})-D_{}(_{t+1},_{t+1})\} }_{:=A}+(^{}}--)D_{ }(_{t+1},_{t})-_{t}^{c}-e_{t}\] \[ J(_{t})+(^{}}- -)D_{}(_{t+1},_{t})-_{t}^{c}-e_{t}\] ( \[ J(_{t})++ )}_{:=}D_{}(_{t+1},_{t})-_{t}^{c} -e_{t}\] ( setting \[_{t}^{}\] s.t. \[1/_{t}^{} 2/+2/c\] )

Recusing for \(T\) iterations and dividing by \(1/\), picking \(\) uniformly random from \(\{0,1,2, T-1\}\) and taking expectation we get

\[(_{+1},_{})}{^{2}} =T}_{t=0}^{T-1}D_{}(_ {t+1},_{t})\] \[[J(_{T})-J(_{0})]}{ T}+^{T-1}_{t}^{c}}{ T}+^{T-1}_{t}}{ T}\] \[)-J(_{0})]}{ T}+^{ T-1}_{t}^{c}}{ T}+^{T-1} _{t}}{ T}\] \[(_{+1},_{ })}{^{2}} [J(^{*})-J(_{0})]+_ {t=0}^{T-1}D_{^{*}}(_{t})-c[ J(_{t} )-(_{t})],(_{t})+_{t=0}^{T-1}e_{t}]\]

Note that it is possible to incorporate a sampling error (in the distribution \(d^{}\) across states) for the actor update in Algorithm 1. This corresponds to an additional error in calculating \(D_{}\), and we can use the techniques from Lavington et al.  to characterize the convergence in this case.

[MISSING_PAGE_FAIL:24]

Now we state the main proposition of this part.

**Proposition 10** (Convergence of tabular MDP with Lifting).: Assume that (i) \(_{t}()\) is smooth and satisfies RSI condition, (ii) \(()\) is \(L_{}\)-Lipschitz continuous, (iii) the bias is bounded for all \(t\) i.e. \(\|_{t+1}-_{t+1}\|_{2}^{2} b_{t}^{2}\), (iv) \(\|Q^{}(s,)\| q\) for all \(\) and \(s\). By setting \(_{t}^{}\) as in Eq. (5) and running Gradient Ascent for \(m\) iterations to maximize \(_{t}\) we have

\[\|J(^{*})-J(_{T})\|_{}^{T}\|J( ^{*})-J(_{0})\|_{}+_{t=1}^{T}^{-t}(c_{t}+ [e_{t}+b_{t}])\]

where \(^{*}\) is the optimal policy, \({^{*}}^{s}\) refers to the optimal action in state \(s\). Here, \(e_{t}=O((-m))\) is the approximation error.

Proof.: This proof is mainly based on the proof of Theorem 3 of Johnson et al. . Using Lemma 3 and the fact that \(^{*} 0\), we have \( Q^{_{t}}(s,),\ _{t+1}^{s} Q^{_{t+1}}(s,),\ _{t+1}^{s}=J_{s}(_{t+1})\). Using this inequality we get,

\[ Q^{_{t}}(s,),\ ^{*s}-_{t+1}^{s}  Q^{_{t}}(s,),\ ^{*s}-J_{s}(_{t+1})\] \[= Q^{_{t}}(s,)-Q^{^{*}}(s,),\ ^{*s}+ Q^{^{*}}(s,),\ ^{*s}-J_{s}(_{t+1})\] \[-\|Q^{_{t}}(s,)-Q^{^{*}}(s,)\|_{ }+J_{s}(^{*})-J_{s}(_{t+1})\] (Holder's inequality) \[-\|J(_{t})-J(^{*})\|_{}+J_{s}( ^{*})-J_{s}(_{t+1})\]

The last inequality is from the definition of \(Q\) and \(J\) as follows. For any action \(a\),

\[Q^{_{t}}(s,a)-Q^{^{*}}(s,a) =_{s^{}}P(s^{}|s,a)[J_{s^{}}( _{t})-J_{s^{}}(^{*})]\] \[_{s^{}}P(s^{}|s,a)\|J(_{t})-J( ^{*})\|_{}\] \[\|J(_{t})-J(^{*})\|_{}\]

From the above inequality,

\[-\|J(_{t})-J(^{*})\|_{}+J_{s}(^{ *})-J_{s}(_{t+1})  Q^{_{t}}(s,),\ ^{*s}-_{t+1}^{s}\] \[ Q^{_{t}}(s,),\ p_{t}^{s}-_{t+1 }^{s}\] (For any \[p_{t}^{s}_{t}^{s}\] ) \[(p_{t}^{s},{_{t}}^{s})-D_{}(p_{t}^{s}, _{t+1}^{s})-D_{}(_{t+1}^{s},{_{t}}^{s})}{_{t}^{ }}\] (Using Lemma 5 with \[d=Q^{_{t}}(s,),\ y=_{t+1}^{s},\ x=p_{t}^{s}\] ) \[(p_{t}^{s},{_{t}}^{s})}{_{t}^{}}\] \[-\|J(_{t})-J(^{*})\|_{}+J_{s }(^{*})-J_{s}(_{t+1}) _{p_{t}^{s}_{t}^{s}}(p_{t}^{s},{_{t}} ^{s})}{_{t}^{}} c_{t}\] (Based on the definition of \[^{}\] in Eq. ( 5 ) \[-\|J(_{t})-J(^{*})\|_{}+J_{s ^{}}(^{*})-J_{s^{}}(_{t+1})  c_{t}+J_{s^{}}(_{t+1})-J_{s^{}}(_{t+1})\] (Since \[s\] is an arbitrary state, changing \[s=s^{}\] for convenience) \[=c_{t}+_{s}d^{_{t+1}}(s) Q^{ _{t+1}}(s,),\ _{t+1}^{s}-{_{t+1}}^{s}\] (Using performance difference lemma 6 with the starting state equal to \[s^{}\]) \[ c_{t}+_{s}d^{_{t+1}}(s)\|Q^ {_{t+1}}(s,)\|\|_{t+1}^{s}-{_{t+1}}^{s}\|\] (Cauchy Schwartz) \[ c_{t}+_{s}d^{_{t+1}}(s)\| _{t+1}^{s}-{_{t+1}}^{s}\|\] \[ c_{t}+_{s}d^{_{t+1}}(s)[ \|_{t+1}^{s}-_{t+1}^{s}\|+\|_{t+1}^{ s}-{_{t+1}}^{s}\|]\] \[ c_{t}+(e_{t}+b_{t})\]Since the above equation is true for all \(s^{}\) we have:

\[\|J(^{*})-J(_{t+1})\|_{}\|J(_{t})-J(^{ *})\|_{}+c_{t}+(e_{t}+b_{t})\]

Recursing for \(T\) iterations we get:

\[\|J(^{*})-J(_{T})\|_{}^{T}\|J( ^{*})-J(_{0})\|_{}+_{t=1}^{T}^{-t}(c_{t}+[e_{t}+b_{t}])\]

We can control the approximation error \(e_{t}\) by using a larger \(m\). The bias term \(b_{t}\) can be small if our function approximation model is expressive enough. \(c_{t}\) is an arbitrary value and if we set \(c_{t}=^{t}c\) for some constant \(c>0\), then \(_{t=1}^{T}^{-t}(c_{t})=Tc\) and therefore \(^{T}Tc\) can diminish linearly. The above analysis relied on the knowledge of the true \(Q\) functions, but can be easily extended to using inexact estimates of \(Q^{}\) by using the techniques developed in .

### Exact setting with lifting trick (Softmax representation)

In the softmax representation in the tabular MDP, we consider the case that \(\) is parameterized with parameter \(^{n}\). In this setting \(\) is the Euclidean norm. Using Prop. 1, for \(\) such that \(J+\) is convex we have for a given \(_{t}\),

\[J()  J(_{t})+ J(_{t}),-_{t}-\,D_{}(,_{t})\] \[=)+ J(_{t}),-_{t} -\,\|-_{t}\|_{2}^{2}}_{:=h()} (.)=\|.\|_{2}^{2}\]

If we maximize \(h()\) w.r.t. \(\) we get

\[_{t+1}=*{arg\,max}_{}\{h()\}_{t+ 1}=_{t}+_{}J(_{t})\]

Mei et al. [37, Lemma 8] proves that \(J()\) satisfies a gradient domination condition w.r.t the softmax representation. In particular, if \(a^{*}(s)\) is the optimal action in state \(s\) and \(:=_{}p^{*}(a^{*}(s)|s)}{\|}{d^{}}\|_{}}\), they prove that for all \(\),

\[\|_{}J()\|[J(^{*})-J()]\]

Consider optimization in the parameter space where \(_{t}():=J(_{t})+ J((_{t}),()-( _{t}))-\,D_{}((),(_{t}))\).

\[_{t+1} :=*{arg\,max}_{}_{t}()\] \[_{t+1} =(_{t+1})\] \[_{t+1} :=(_{t},_{t},m)\] \[_{t+1} =(_{t+1})\]

GradientAscent\((_{t}(),_{t},m)\) means that we run gradient ascent for \(m\) iterations to maximize \(_{t}\) with \(_{t}\) as the initial value. Assuming that \(_{t}\) is Lipschitz smooth w.r.t. \(\) and satisfies the Polyak-Lojasiewicz (PL) condition, we use the gradient ascent property for PL functions  to obtain,

\[h(_{t+1})-h(_{t+1})=_{t}(_{t+1})-(_{ t+1}):=O((-m))}_{}\]

**Proposition 11** (Convergence of softmax+tabular setting with Lifting).: Assume (i) \(J+\) is convex, (ii) \(J\) satisfies gradient domination property above with \(>0\), (iii) \(_{t}()\) is Lipschitz smooth and satisfies PL condition, (iv) \(|h(_{t+1})-h(_{t+1})| b_{t}\) for all \(t\). Then after running Gradient Ascent for \(m\) iterations to maximize \(_{t}\) we have

\[_{t[T-1]}[J(^{*})-J(_{t})])-J (_{0})+_{t=0}^{T-1}[e_{t}+b_{t}]}{ T}}\]

where \(:=}{2}\) and \(e_{t}\) is the approximation error at iteration \(t\) and \([T-1]:=\{0,1,2, T-1\}\).

Proof.: Since \(J+\) is convex,

\[J(_{t+1})  h(_{t+1})=J(_{t})+ J(_{t}),_{t+1}- _{t}-\ \|_{t+1}-_{t}\|_{2}^{2}\] \[ h(_{t+1})-e_{t}\] (Using the GA bound from above) \[ J(_{t})+\|_{}J(_{t})\|_ {2}^{2}-e_{t}-b_{t}\] (Since \[_{t+1}=_{t}+_{}J(_{t})\] ) \[ J(_{t})+}{2}[J(^{*})-J(_{t}) ]^{2}-e_{t}-b_{t}\] (Using gradient domination of \[J\] ) \[ J(^{*})-J(_{t+1}) )-J(_{t})}_{:=_{t}}- }{2}}_{:=}[J(^{*})-J(_{t}) ]^{2}+e_{t}+b_{t}\] \[_{t+1} _{t}-_{t}^{2}+e_{t}+b_{t}\] \[_{t}^{2} _{t}-_{t+1}+e_{t}+b_{t}\]

Summing up for \(T\) iterations and dividing both sides by \(T\)

\[_{t[T-1]}_{t}^{2} _{t=0}^{T-1}_{t}^{2}\] \[[_{0}-_{T+1}]+ _{t=0}^{T-1}[e_{t}+b_{t}][_{0}]+_{t=0}^{T-1}[e_{t}+b_{t}]\] \[_{t[T-1]}_{t}+ _{t=0}^{T-1}[e_{t}+b_{t}]}{ T}}\]

The above analysis relied on the knowledge of the exact gradient \( J()\), but can be easily extended to using inexact estimates of the gradient by using the techniques developed in .

### Helper Lemmas

**Lemma 4** (3-Point Bregman Property).: _For \(x,y,z\),_

\[(z)-(y),z-x=D_{}(x,z)+D_{}(z,y)-D_{ }(x,y)\]

**Lemma 5** (3-Point Descent Lemma for Mirror Ascent).: _For any \(z\) rint dom \(\), and a vector \(d\), let_

\[y=*{arg\,max}_{x}\{ d,x-D_{}(x,z)\}.\]

_Then \(y\) rint dom \(\) and for any \(x\)_

\[ d,y-x[D_{}(y,z)+D_{}(x,y)-D_{ }(x,z)]\]

**Lemma 6** (Performance Difference Lemma ).: _For any \(\), \(^{}\),_

\[J()-J(^{})=\,_{s d^{}}[  Q^{^{}}(s,),p^{}(|s)-p^{^{}}(|s) ]\]

## Appendix E Proofs for Sec. 5

**Proposition 12** (State-wise lower bound).: For (i) any representation \(\) that is separable across states i.e. there exists \(^{s} R^{A}\) such that \(_{s,a}=[^{s}]_{a}\), (ii) any strictly convex mirror map \(\) that induces a Bregman divergence that is separable across states i.e. \(D_{}(,^{})=_{s}d^{}(s)\,D_{}(^{s},^{ s})\), (iii) any \(\) such that \(J+\) is convex, if (iv) \( J()\) is separable across states i.e. \([ J()]_{s,a}=d^{}(s)\,[_{^{s}}J()_{a}\) where \(_{^{s}}J()^{A}\), then (v) for any separable (across states) gradient estimator \(\) i.e. \([()]_{s,a}=d^{}(s)\,[^{s}()]_{a}\) where \(^{s}()^{A}\), and \(c(0,)^{S}\),

\[J() J(_{t})+(_{t}),(-_{t})-_{s}d^{ _{t}}(s)\,(+})\,D_{}(^{s}, _{t}{}^{s})-_{s}}(s)\,D_{^{*}}\,((_{t }{}^{s})-c_{s}\,_{t}^{s},(_{t}{}^{s}))}{c_{s}}\]

Proof.: Using condition (iii) of the proposition with Lemma 2,

\[J()  J(_{t})+ J(_{t}),-_{t}- {1}{}\,D_{}(,_{t})\] \[=J(_{t})+(_{t}),-_{t}+  J(_{t})-(_{t}),-_{t}-\,D_{ }(,_{t})\]

Using conditions (iv) and (v), we know that \([ J(_{t})]_{s,a}=d^{_{t}}(s)\,[_{^{s}}J(_{t})]_{a}\) and \([(_{t})]_{s,a}=d^{_{t}}(s)\,[^{s}(_{t})]_{a}\). Defining \(_{t}^{s}:=_{^{s}}J(_{t})-^{s}(_{t}) ^{A}\). Using conditions (i) and (ii), we can rewrite the lower-bound as follows,

\[J()  J(_{t})+(_{t}),(-_{t})+ _{s}d^{_{t}}(s)\,_{t}^{s},^{s}-_{t}{}^{s}-\,_{s}d^{_{t}}(s)\,D_{}(^{s},_{t}{}^{s})\] \[=J(_{t})+(_{t}),-_{t}+_{s}d^ {_{t}}(s)\,[_{t}^{s},^{s}-_{t}{}^{s}-\,D_{}(^{s},_{t}{}^{s})]\]

Using Lemma 1 with \(x=_{t}^{s}\), \(y=^{s}\) and \(y^{}=_{t}{}^{s}\),

\[ J(_{t})+(_{t}),-_{t}-_{s}d^{_{t} }(s)\,[}\,((_{t}{}^{s})-c_{s}\,_{t}^{ s},(_{t}{}^{s}))}{c_{s}}+(+} )\,D_{}(^{s},_{t}{}^{s})]\] \[J()  J(_{t})+(_{t}),-_{t}-_{s}d^ {_{t}}(s)\,(+})\,D_{}(^{s},_ {t}{}^{s})-_{s}}(s)\,D_{^{*}}\,((_{t}{}^{ s})-c_{s}\,_{t}^{s},(_{t}{}^{s}))}{c_{s}}\]

**Proposition 4**.: For the direct representation and negative entropy mirror map, \(c>0\), \(}{2\,|A|}\),

\[J()-J(_{t}) C+_{s d^{_{t}}}\,[ _{a p^{_{t}}(|s)}\,[(a|s)}{p^{_{t}} (a|s)}\,(^{_{t}}(s,a)-(+) \,((a|s)}{p^{_{t}}(a|s)}))]]\] \[-_{s d^{_{t}}}\,[_{a p^{_{t }}(|s)}\,[Q^{_{t}}(s,a)-^{_{t}}(s,a)]+\, (_{a p^{_{t}}(|s)}\,[(-e\,[Q^{_{t}}( s,a)-^{_{t}}(s,a)])])]\]

where \(C\) is a constant and \(^{_{t}}\) is the estimate of the action-value function for policy \(_{t}\).

Proof.: For the direct representation, \(_{s,a}=p^{}(a|s)\). Using the policy gradient theorem, \([_{}J()]_{s,a}=d^{}(s)\,Q^{}(s,a)\). We choose \(()\) such that \([()]_{s,a}=d^{}(s)\,^{}(s,a)\) as the estimated gradient. Using Vaswani et al. [57, Proposition 2], \(J+\) is convex for \(}{2\,|A|}\). Defining \(_{t}^{s}:=_{^{s}}J(_{t})-^{s}(_{t})=Q^{_{t}}(s, )-^{_{t}}(s,)^{A}\), and using Prop. 12 with \(c_{s}=c\) for all \(s\),

\[J() J(_{t})+(_{t}),-_{t}-_{s}d^{ _{t}}(s)\,(+)\,D_{}(^{s},_{t}{ }^{s})-_{s}}(s)\,D_{^{*}}\,((_{t}{}^{s})-c \,_{t}^{s},(_{t}{}^{s}))}{c}\]Since \((^{s})=(p^{}(|s))=_{a}p^{}(a|s)(p^{}(a|s))\), using Lemma 10, \(D_{}(^{s},_{t}{}^{s})=(p^{}(|s)||p^{_{t}}( |s))\). Hence,

\[J()  J(_{t})+_{s}d^{_{t}}(s)_{a}^{_{t}}(s,a)\,[p^{}(a|s)-p^{_{t}}(a|s)]-(+)\, _{s}d^{_{t}}(s)(p^{}(|s)||p^{_{t}}(|s))\] \[-_{s}}(s)\,D_{^{*}}((_{ t}{}^{s})-c\,_{t}^{s},(_{t}{}^{s}))}{c}\]

Using Lemma 7 to simplify the last term,

\[_{s}}(s)\,D_{^{*}}((_{ t}{}^{s})-c\,_{t}^{s},(_{t}{}^{s}))}{c}\] \[=[_{s}d^{_{t}}(s)[c\, p^{_ {t}}(|s),_{t}^{s}+(_{a}p^{_{t}}(a|s)\,(- c\,_{t}^{s}[a]))]]\] \[=_{s}d^{_{t}}(s)\,[_{a}p^{_{t}}(a|s)\,[Q^{_ {t}}(s,a)-^{_{t}}(s,a)]+(_{a}p^{_{t}}(a|s) \,(-c\,[Q^{_{t}}(s,a)-^{_{t}}(s,a)]))]\]

Putting everything together,

\[J() J(_{t})+_{s}d^{_{t}}(s)_{a}^{ _{t}}(s,a)\,[p^{}(a|s)-p^{_{t}}(a|s)]-(+)\,_{s}d^{_{t}}(s)(p^{}(|s)||p^{_{t}}( |s))\] \[-[_{s}d^{_{t}}(s)\,[_{a}p^{_{t}}(a|s)\,[Q ^{_{t}}(s,a)-^{_{t}}(s,a)]+\,(_{a}p^{_ {t}}(a|s)\,(-c[Q^{_{t}}(s,a)-^{_{t}}(s,a)])) ]]\] \[=J(_{t})-_{s d^{_{t}}}[ _{a p^{_{t}}(|s)}[^{_{t}}(s,a)]]}_{:=-C}+ _{s d^{_{t}}}[_{a p^{_{t}}(|s)} [^{_{t}}(s,a)-(+)( (a|s)}{p^{_{t}}(a|s)})]]\] \[-[_{s}d^{_{t}}(s)\,[_{a}p^{_{t}}(a|s)\,[Q ^{_{t}}(s,a)-^{_{t}}(s,a)]+\,(_{a}p^{_ {t}}(a|s)\,(-c[Q^{_{t}}(s,a)-^{_{t}}(s,a)])) ]]\] \[J()  J(_{t})+C+_{s d^{_{t}}}[_ {a p^{_{t}}(|s)}[(a|s)}{p^{_{t}}(a|s)}\, (^{_{t}}(s,a)-(+)\, ((a|s)}{p^{_{t}}(a|s)}))]]\] \[-_{s d^{_{t}}}[_{a p^{_{ t}}(|s)}\,[Q^{_{t}}(s,a)-^{_{t}}(s,a)]+\,( _{a p^{_{t}}(|s)}[(-c\,[Q^{_{t}}(s,a)- ^{_{t}}(s,a)])])]\]

**Proposition 6**.: For the softmax representation and log-sum-exp mirror map, \(c>0\), \( 1-\),

\[J()-J(_{t}) _{s d^{_{t}}}\,_{a p^{_{t}}( |s)}[(^{_{t}}(s,a)++)\, ((a|s)}{p^{_{t}}(a|s)})]\] \[-\,_{s d^{_{t}}}_{a p^{ _{t}}(|s)}[(1-c\,[A^{_{t}}(s,a)-^{_{t}}(s,a)] )\,(1-c\,[A^{_{t}}(s,a)-^{_{t}}(s,a)])]\]

where \(^{_{t}}\) is the estimate of the advantage function for policy \(_{t}\).

Proof.: For the softmax representation, \(_{s,a}=z(s,a)\) s.t. \(p^{}(a|s)=}(z(s,a^{}))}\). Using the policy gradient theorem, \([_{}J()]_{s,a}=d^{}(s)\,p^{}(a|s)\,A^{}(s,a)\). We choose \(()\) such that \([()]_{s,a}=d^{}(s)\,p^{}(a|s)\,^{}(s,a)\) as the estimated gradient. Using Vaswani et al. [57, Proposition 3], \(J+\) is convex for \( 1-\). Define \(_{s}^{A}\) such that \(_{t}^{s}[a]:=_{^{s}}J(_{t})-^{*}(_{t})=p^{_{t}}(a|s) \,[A^{_{t}}(s,a)-^{_{t}}(s,a)]\). Using Prop. 12 with \(c_{s}=c\) for all \(s\),

\[J() J(_{t})+(_{t}),-_{t}-_{s}d^{ _{t}}(s)\,(+)\,D_{}(^{s},{_{t }}^{s})-_{s}}(s)\,D_{^{*}}((_{t}{}^{s} )-c\,_{t}^{s},(_{t}{}^{s}))}{c}\]Since \((^{s})=(z(s,))=(_{a}(z(s,a)))\), using Lemma 11, \(D_{}(^{s},{_{t}}^{s})=(p^{_{t}}(|s))[|p^{}(|s))\) where \(p^{}(a|s)=}(z(s,a^{}))}\) and \(p^{_{t}}(a|s)=(s,a))}{_{a^{}}(z_{t}(s,a^{ }))}\). Hence, the above bound can be simplified as,

\[J()  J(_{t})+_{s}d^{_{t}}(s)_{a}^{_{t}}(s, a)\,p^{_{t}}(a|s)[z(s,a)-z_{t}(s,a)]-(+ )_{s}d^{_{t}}(s)\,(p^{_{t}}(|s)||p^{}(| s))\] \[-_{s}}(s)\,D_{^{*}}(({_ {t}}^{s})-c\,_{t}^{s},({_{t}}^{s}))}{c}\]

Using Lemma 8 to simplify the last term,

\[_{s}}(s)\,D_{^{*}}(({_ {t}}^{s})-c\,_{t}^{s},({_{t}}^{s}))}{c}\] \[=[_{s}d^{_{t}}(s)[_{a}(p^{ _{t}}(a|s)-c\,_{t}^{s}[a])\,(}(a|s)-c \,_{t}^{s}[a]}{p^{_{t}}(a|s)})]]\] \[=[_{s}d^{_{t}}(s)[_{a}(p^{ _{t}}(a|s)-c\,[p^{_{t}}(a|s)[A^{_{t}}(s,a)-^{_{t}} (s,a)]])\,(}(a|s)-c\,[p^{_{ t}}(a|s)[A^{_{t}}(s,a)-^{_{t}}(s,a)]]}{p^{_{t}}(a|s) })]\] \[=[_{s}d^{_{t}}(s)[_{a}p^{_{t}} (a|s)[(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a)] )\,(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a)] )]]]\]

Putting everything together,

\[J()  J(_{t})+_{s}d^{_{t}}(s)_{a}^{_{t}}(s,a)\,p^{_{t}}(a|s)[z(s,a)-z_{t}(s,a)]-(+)_{s}d^{_{t}}(s)\,(p^{_{t}}(|s)||p^{}( |s))\] \[-[_{s}d^{_{t}}(s)[_{a}p^{_{t}} (a|s)[(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a)] )\,(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a)] )]]]\] \[=J(_{t})+_{s}d^{_{t}}(s)_{a}[p^{_{t}}(a|s) \,^{_{t}}(s,a)[z(s,a)-z_{t}(s,a)]-(+ )p^{_{t}}(a|s)\,(}(a|s)}{p^{ }(a|s)})]\] \[-_{s d^{_{t}}_{a p^{_ {t}}(|s)}}[(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a) ])\,(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a) ])]\]

Let us focus on simplifying \(_{a}[p^{_{t}}(a|s)\,^{_{t}}(s,a)[z(s,a)-z_{t}(s,a) ]]\) for a fixed \(s\). Note that \(_{a}p^{_{t}}(a|s)\,^{_{t}}(s,a)=0(_{a^{ }}(z(s,a^{})))\,_{a}p^{_{t}}(a|s)\,^{_{ t}}(s,a)=0\).

\[_{a}[p^{_{t}}(a|s)\,^{_{t}}(s,a)\,z(s,a) ]=_{a}[p^{_{t}}(a|s)\,^{_{t}}(s,a)\,(z(s,a)- (_{a^{}}(z(s,a^{}))))]\] \[=_{a}[p^{_{t}}(a|s)\,^{_{t}}(s,a)\,( ((z(s,a))-(_{a^{}}(z(s,a^{}))) ))]\] \[=_{a}[p^{_{t}}(a|s)\,^{_{t}}(s,a)\, (}(z(s,a^{}))}) ]=_{a p^{_{t}}(|s)}[^{_{t}}(s,a)\, (p^{}(a|s))]\]

Similarly, simplifying \(_{a}[p^{_{t}}(a|s)\,^{_{t}}(s,a)\,z_{t}(s,a)]\)

\[_{a}[p^{_{t}}(a|s)\,^{_{t}}(s,a)\,z_{t}(s,a) ]=E_{a p^{_{t}}(|s)}[^{_{t}}(s,a)\,(p^{ _{t}}(a|s))]\] \[_{a}[p^{_{t}}(a|s)\,^{_{t}}(s,a)[ z(s,a)-z_{t}(s,a)]]=_{a p^{_{t}}(|s)}[^{_{t}}(s,a) \,((a|s)}{p^{_{t}}(a|s)})]\]Using the above relations,

\[J()  J(_{t})+_{s}d^{_{t}}(s)\,_{a p^{_{t} }(|s)}[^{_{t}}(s,a)\,((a|s)}{p^{_{ t}}(a|s)})-(+)( }(a|s)}{p^{}(a|s)})]\] \[-_{s d^{_{t}}}_{a p^{ _{t}}(|s)}[(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a) ])\,(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a) ])]\] \[=J(_{t})+_{s d^{_{t}}}\,_{a p^{ _{t}}(|s)}[(^{_{t}}(s,a)++)\,((a|s)}{p^{_{t}}(a|s)})]\] \[-_{s d^{_{t}}}_{a p^{ _{t}}(|s)}[(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a) ])\,(1-c[A^{_{t}}(s,a)-^{_{t}}(s,a) ])].\]

**Proposition 8**.: For the stochastic value gradient representation and Euclidean mirror map, \(c>0\), \(\) such that \(J+\) is convex in \(\).

\[J()-J(_{t})  C+_{s d^{_{t}}}_{c}[ Q^{_{t}}}(s,a)_{a=_{t}(s,)}\,(s, )-\,(+)\,[_ {t}(s,)-(s,)]^{2}]\] \[-\,_{s d^{_{t}}}_{ }\,[_{a}Q^{_{t}}(s,a)_{a=_{t}(s,)}- Q^{_{t}}}(s,a)_{a=_{t}(s,)}]^ {2}\]

where \(C\) is a constant and \(Q^{_{t}}}(s,a)_{a=_{t}(s,)}\) is the estimate of the action-value gradients for policy \(\) at state \(s\) and \(a=_{t}(s,)\).

Proof.: For stochastic value gradients with a fixed \(\), \(=d^{}(s)_{a}Q^{}(s,a)_{a=(s,)}\). We choose \(()\) such that \([()]_{s,a}=d^{}(s)Q^{}}(s,a)_{a=( s,)}\). Define \(_{t}^{s}^{A}\) such that \(_{t}^{s}[a]:=_{a}Q^{_{t}}(s,a)_{a=_{t}(s,)}- Q^{_{t}}}(s,a)_{a=_{t}(s,)}\). Using Prop. 12 with \(c_{s}=c\) for all \(s\),

\[J()  J(_{t})+_{}_{s} d^{_{t}}(s)\,Q^{_{t}}}(s,a)_{a=_{t}(s, )}[(s,)-_{t}(s,)]-_{s}d^{ _{t}}(s)\,(+)\,D_{}(^{s},{_{ t}}^{s})\] \[-_{s}}(s)\,D_{^{*}}(({_{ t}}^{s})-c\,_{t}^{s},({_{t}}^{s}))}{c}\]

For a fixed \(\), since \((^{s})=((s,))=[(s,)]^{2}\), \(D_{}(^{s},{_{t}}^{s})=[(s,)-_{t}(s, )]^{2}\). Hence,

\[J()  J(_{t})+_{}_{s} d^{_{t}}(s)\,Q^{_{t}}}(s,a)_{a=_{t}(s, )}[(s,)-_{t}(s,)]- \,(+)\,_{s}d^{_{t}}(s)\,[(s, )-_{t}(s,)]^{2}\] \[-_{s}}(s)\,D_{^{*}}(({_{ t}}^{s})-c\,_{t}^{s},({_{t}}^{s}))}{c}\]

Simplifying the last term, since \(((s,))=[(s,)]^{2}\),

\[}(({_{t}}^{s})-c\,_{t}^{s},( {_{t}}^{s}))}{c}=\,[_{t}^{s}]^{2}=\, [_{a}Q^{_{t}}(s,a)_{a=_{t}(s,)}-Q^{_{t}}}(s,a)_{a=_{t}(s,)}]^{2}\]Putting everything together,

\[J()  J(_{t})+_{}[_{s}d^{ _{t}}(s)\,Q^{_{t}}}(s,a)_{a=_{t}(s,) }\,(s,)-d^{_{t}}(s)\,_{a}Q^{_{t }}(s,a)_{a=_{t}(s,)}\,_{t}(s,)]}_{:=-C}]\] \[-_{}[\,(+)\,_{s}d^{_{t}}(s)\,[(s,)-_{t} (s,)]^{2}-\,_{s}d^{_{t}}(s)\,[_{a}Q^{_ {t}}(s,a)_{a=_{t}(s,)}-Q^{_{t}}}(s,a) _{a=_{t}(s,)}]^{2}]\] \[J()  J(_{t})+C+_{}[_{s d^{_{t}}}\,[Q^{_{t}}}(s,a)_{a=_{t}(s, )}\,(s,)-\,(+ )\,[(s,)-_{t}(s,)]^{2}]]\] \[-\,_{}[_{s  d^{_{t}}}[_{a}Q^{_{t}}(s,a)_{a=_{t}(s, )}-Q^{_{t}}}(s,a)_{a=_{t}(s,)}]^ {2}]\]

**Proposition 13**.: For the softmax representation and Euclidean mirror map, \(c>0\), \(}{8}\) then

\[J()  J(_{t})+C+_{s d^{_{t}}(s)}[_{a p^{_{t}}(.|s)}[^{_{t}}(s,a)\,z(s,a)]- (+)||z(s,)-z_{t}(s,)||^{2}]\] \[-\,_{s d^{_{t}}}_{a p^{ _{t}}(.|s)}[A^{_{t}}(s,a)-^{_{t}}(s,a)]^{2}\]

where \(C\) is a constant and \(^{}\) is the estimate of advantage function for policy \(_{t}\).

Proof.: For the softmax representation, \(_{s,a}=z(s,a)\) s.t. \(p^{}(a|s)=}(z(s,a^{}))}\). Using the policy gradient theorem, \([_{}J()]_{s,a}=d^{}(s)\,p^{}(a|s)\,A^{}(s,a)\). We choose \(()\) such that \([()]_{s,a}=d^{}(s)\,p^{}(a|s)\,^{}(s,a)\) as the estimated gradient. Define \(_{s}^{A}\) such that \(_{t}^{s}[a]:=_{^{s}}J(_{t})-^{s}(_{t})=p^{_{t}} (a|s)\,[A^{_{t}}(s,a)-^{_{t}}(s,a)]\). Using Mei et al. [37, Lemma 7], \(J+\) is convex for \( 1-\). Using Prop. 12 with \(c_{s}=c\) for all \(s\),

\[J()  J(_{t})+(_{t}),-_{t}-( +)\,_{s}d^{_{t}}(s)\,D_{}(^{s}, {_{t}}^{s})-_{s}}(s)\,D_{^{*}}(({ _{t}}^{s})-c\,_{t}^{s},({_{t}}^{s}))}{c}\]

Since \((^{s})=(z(s,))=_{a}[z_{s,a}]^{2}\), \(D_{}(^{s},{_{t}}^{s})=\|z(s,)-z_{t}(s,) \|_{2}^{2}\). Hence,

\[J()  J(_{t})+_{s}d^{_{t}}(s)_{a}^{_{t}}( s,a)\,p^{_{t}}(a|s)\,[z(s,a)-z_{t}(s,a)]-\,(+ )\,_{s}d^{_{t}}(s)\,\|z(s,)-z_{t}(s,) \|_{2}^{2}\] \[-_{s}}(s)\,D_{^{*}}(({_{t} }^{s})-c\,_{t}^{s},({_{t}}^{s}))}{c}\]

Simplifying the last term, since \((z(,a))=[z(s,a)]^{2}\),

\[d^{_{t}}(s)\,D_{^{*}}(({_{t }}^{s})-c\,_{t}^{s},({_{t}}^{s}))}{c}=\, _{s}d^{_{t}}(s)\,_{a}[_{t}^{s}(a)]^{2}\] \[=\,_{s}d^{_{t}}(s)\,_{a}p^{_{t}}(a|s)^{2} [A^{_{t}}(s,a)-^{_{t}}(s,a)]^{2}\] \[\,_{s}d^{_{t}}(s)\,_{a}p^{_{t}}(a|s) \,[A^{_{t}}(s,a)-^{_{t}}(s,a)]^{2}\] (Since \[p^{_{t}}(a|s) 1\]Putting everything together,

\[J()  J(_{t})+_{s}d^{_{t}}(s)_{a}^{_{t}}(s,a) \,p^{_{t}}(a|s)\,[z(s,a)-z_{t}(s,a)]-(+)_{s}d^{_{t}}(s)\,\|z(s,)-z_{t}(s,)\|_{2} ^{2}\] \[-_{s}d^{_{t}}(s)\,_{a}p^{_{t}}(a|s)\, [A^{_{t}}(s,a)-^{_{t}}(s,a)]^{2}\] \[=J(_{t})-d^{_{t}}(s)_{a}^{ _{t}}(s,a)\,p^{_{t}}(a|s)\,z_{t}(s,a)}_{:=-C}\] \[+_{s}d^{_{t}}(s)[_{a}^{_{t}}(s,a)\,p^{ _{t}}(a|s)\,z(s,a)-\,(+)\, \|z(s,)-z_{t}(s,)\|_{2}^{2}]\] \[-_{s}d^{_{t}}(s)\,_{a}p^{_{t}}(a|s)\, [A^{_{t}}(s,a)-^{_{t}}(s,a)]^{2}\] \[=J(_{t})+C+_{s d^{_{t}}}[_{a  p^{_{t}}(|s)}[^{_{t}}(s,a)\,z(s,a)]- {2}\,(+)\,\|z(s,)-z_{t}(s, )\|_{2}^{2}]\] \[-\,_{s d^{_{t}}}_{a p^{ _{t}}(|s)}[A^{_{t}}(s,a)-^{_{t}}(s,a)]^{2}\]

**Proposition 14**.: For both the direct (with the negative-entropy mirror map) and softmax representations (with the log-sum-exp mirror map), for a fixed state \(s\), if \(^{A}:=_{^{s}}J(_{t})-^{s}(_{t})\), the second-order Taylor expansion of \(f(c)=D_{^{*}}(({_{t}}^{s})-c,({_{t}}^{s}))\) around \(c=0\) is equal to

\[f(c)}{2}\,_{a}p^{_{t}}(a|s)[A(s,a)-(s,a)]^{2}\,.\]

Proof.: \[f(c) =D_{^{*}}(({_{t}}^{s})-c,({_{ t}}^{s})) f(0)=D_{}^{*}(({_{t}}^{s}),({_{t}}^{s}))=0\] \[f(c) =D_{^{*}}(({_{t}}^{s})-c,({_{ t}}^{s}))=^{*}(({_{t}}^{s})-c)-^{*}(({_{t}}^{s }))-^{*}(({_{t}}^{s})),({_{t}}^{s} )-c-({_{t}}^{s})\] \[ f^{}(c) =^{*}(({_{t}}^{s})-c),- +}^{s}, f^{}(0)= _{t}{}^{s},-+}^{s},=0\] \[f^{}(c) =,^{2}^{*}(({_{t}}^{s})-c ) f^{}(0)=,^{2}^ {*}(({_{t}}^{s}))\,.\]

By the second-order Taylor series expansion of \(f(c)\) around \(c=0\),

\[f(c) f(0)+f^{}(0)(c-0)+(0)(c-0)^{2}}{2}= }{2}\,,^{2}^{*}(({_{t}}^{s}))\,\]

Let us first consider the softmax case with the log-sum-exp mirror map, where \(^{s}=z(s,)\) and \((z(s,))=(_{a}(z(s,a)))\), \(^{*}(p^{}(|s))=_{a}p^{}(a|s)(p^{}(a|s))\). Since the negative entropy and log-sum-exp are Fenchel conjugates (see Lemma 9), \((z_{t}(s,))=p^{_{t}}(|s)\). Hence, we need to compute \(^{2}^{*}(p^{_{t}}(|s))\).

\[^{*}(p^{_{t}}(|s))=1+(p^{_{t}}(|s)); ^{2}^{*}(p^{_{t}}(|s))=(}{{p^ {_{t}}(|s)}})\]

For the softmax representation, using the policy gradient theorem, \([]_{a}=p^{_{t}}(a|s)[A(s,a)-(s,a)]\) and hence,

\[,^{2}^{*}(({_{t}}))\,=_{a} p^{_{t}}(a|s)[A(s,a)-(s,a)]^{2}\,.\]

Hence, for the softmax representation, the second-order Taylor series expansion around \(c=0\) is equal to,

\[f(c)}{2}\,_{a}p^{_{t}}(a|s)[A(s,a)-(s,a)]^{2}\,.\]

Now let us consider the direct case, where \(^{s}=p^{}(|s)\), \((p^{}(|s))=_{a}p^{}(a|s)(p^{}(a|s))\), \(^{*}(z(s,))=(_{a}(z(s,a)))\). Since the negative entropy and log-sum-exp are Fenchel conjugates (see Lemma 9), \((p^{_{t}}(|s))=(_{a}p^{_{t}}(a|s))\).

\(z_{t}(s,)\). Hence, we need to compute \(^{2}^{*}(z_{t}(s,))\).

\[[^{*}(z_{t}(s,))]_{a} =(s,a)}{_{a^{}}(z_{t}(s,a^{}))} =p^{_{t}}(a|s);[^{2}^{*}(z_{t}(s,))]_{a,a}=p^{_{ t}}(a|s)-[p^{_{t}}(a|s)]^{2}\] \[[^{2}^{*}(z_{t}(s,))]_{a,a^{}} =-p^{_{t}}(a|s)\,p^{_{t}}(a^{}|s)^{2} ^{*}(z_{t}(s,))=(p^{_{t}}(|s))-p^{_{t}}(|s )[p^{_{t}}(|s)]^{}.\]

For the direct representation, using the policy gradient theorem, \([]_{a}=Q^{_{t}}(s,a)-^{_{t}}(s,a)\) and hence,

\[,^{2}^{*}((_{t}))\, =[Q^{_{t}}(s,)-^{_{t}}(s,)]^{}\, [(p^{_{t}}(|s))-p^{_{t}}(|s)[p^{_{t}}( |s)]^{}]\,[Q^{_{t}}(s,)-^{_{t}}(s,)]\] \[=_{a}p^{_{t}}(a|s)[Q^{_{t}}(s,a)-^{_{t}} (s,a)]^{2}-[ p^{_{t}}(a|s),Q^{_{t}}(s,)- {Q}^{_{t}}(s,)]^{2}\] \[=_{a}p^{_{t}}(a|s)[Q^{_{t}}(s,a)-^{_{t}} (s,a)]^{2}-[J_{s}(_{t})-_{s}(_{t})]^{2}\] (where \[_{s}(_{t})\] is the estimated value function for starting state \[s\] )

Hence, for the direct representation, the second-order Taylor series expansion around \(c=0\) is equal to,

\[f(c) }{2}\,[_{a}p^{_{t}}(a|s)[Q^{ _{t}}(s,a)-^{_{t}}(s,a)]^{2}-[J_{s}(_{t})-_ {s}(_{t})]^{2}]\] \[=}{2}\,[_{a}p^{_{t}}(a|s)[Q^{_{t}} (s,a)-^{_{t}}(s,a)]^{2}-2[J_{s}(_{t})-_{s}(_ {t})]^{2}_{a}p^{_{t}}(a|s)+[J_{s}(_{t})-_{s}(_ {t})]^{2}_{a}p^{_{t}}(a|s)]\] \[=}{2}\,[_{a}p^{_{t}}(a|s)[Q^{_{t}} (s,a)-^{_{t}}(s,a)]^{2}-2[J_{s}(_{t})-_{s}(_ {t})]_{a}p^{_{t}}(a|s)[Q^{_{t}}(s,a)-^{_{t}}(s,a)]\] \[+[J_{s}(_{t})-_{s}(_{t})]^{2}_{a}p^{ _{t}}(a|s)]\] \[=}{2}\,[_{a}p^{_{t}}(a|s)[Q^{_{t}} (s,a)-^{_{t}}(s,a)]^{2}-2[J_{s}(_{t})-_{s}(_ {t})][Q^{_{t}}(s,a)-^{_{t}}(s,a)]+[J_{s}( _{t})-_{s}(_{t})]^{2}]\] \[=}{2}\,[_{a}p^{_{t}}(a|s)([Q^{ _{t}}(s,a)-^{_{t}}(s,a)]-[J_{s}(_{t})-_{s}( _{t})])^{2}]\] \[=}{2}\,[_{a}p^{_{t}}(a|s)[A^{_{t}} (s,a)-^{_{t}}(s,a)]^{2}]\]

### Bandit examples to demonstrate the benefit of the decision-aware loss

**Proposition 15** (Detailed version of Prop. 5).: Consider a two-armed bandit example with deterministic rewards where arm \(1\) is optimal and has a reward \(r_{1}=Q_{1}=2\) whereas arm \(2\) has reward \(r_{2}=Q_{2}=1\). Using a linear parameterization for the critic, \(Q\) function is estimated as: \(=x\,\) where \(\) is the parameter to be learned and \(x\) is the feature of the corresponding arm. Let \(x_{1}=-2\) and \(x_{2}=1\) implying that \(_{1}()=-2\) and \(_{2}()=\). Let \(p_{t}\) be the probability of pulling the optimal arm at iteration \(t\), and consider minimizing two alternative objectives to estimate \(\):

(1) Squared loss: \(_{1}^{(1)}:=*{arg\,min}\{}{2}[_{1}()-Q_{1}]^{2}+}{2}[_{2}()-Q_{2 }]^{2}\}\).

(2) Decision-aware critic loss: \(_{2}^{(2)}:=*{arg\,min}_{t}():=p_{t} [Q_{1}-_{1}()]+(1-p_{t})[Q_{2}-_{2}()]+\,(p_{t}\,(-c[Q_{1}-_{1}( )]+(1-p_{t})\,(-c[Q_{2}-_{2}( )]))]\).

Using the tabular parameterization for the actor, the policy update at iteration \(t\) is given by: \(p_{t+1}=\,(_{1})}{p_{t}\,\,(_{1})+ (1-p_{t})\,(_{2})}\), where \(\) is the functional step-size for the actor. For \(p_{0}<\), minimizing the squared loss results in convergence to the sub-optimal action, while minimizing the decision-aware loss (for \(c,p_{0}>0\)) results in convergence to the optimal action.

Proof.: Note that \(_{1}()-Q_{1}=-2(+1)\) and \(_{2}()-Q_{2}=-1\). Calculating \(^{(1)}\) for a general policy s.t. the probability of pulling the optimal arm equal to \(p\),

\[() =[_{1}()-Q_{1}]^{2}+\,[_{2}()-Q_{2}]^{2}=[4p\,(+1)^{2}+ (1-p)\,(-1)^{2}]\] \[_{}() =4p\,(+1)+(1-p)\,(-1)\]

Setting the gradient to zero,

\[^{(1)}=\]

Calculating \(^{(2)}\) for a general policy s.t. the probability of pulling the optimal arm equal to \(p\),

\[L_{t}()=2p\,(+1)-(1-p)\,(-1)+(p\, (-2c\,(+1))+(1-p)\,(c\,(-1)))\] \[_{}L_{t}()=(3p-1)+_{ }[(p\,(-2c\,(+1))+(1-p)\,(c\,(-1)))]\]

Setting the gradient to zero,

\[_{}[(p\,(-2c\,(+1))+(1-p)\,(c\,( -1)))]=(1-3p)\,c\]

Define \(A:=(-2c\,(+1))\) and \(B:=(c\,(-1))\))

\[=(1-3p)\,c {p\,A+(1-p)\,B}=1^{(2)}=.\]

Now, let us consider the actor update,

\[p_{t+1}=\,(_{1})}{p_{t}\,\,(_{1})+(1- p_{t})\,\,(_{2})}}{p_{t}}=+(1-p_{t}) \,(\,(_{2}-_{1}))}\]

Since arm \(1\) is optimal, if \(}{p_{t}}<1\) for all \(t\), the algorithm will converge to the sub-optimal arm. This happens when \(+(1-p_{t})\,(\,(_{2}-_{1}))}<1 _{2}-_{1}>0>0\). Hence, for any \(\) and any iteration \(t\), if \(_{t}>0\), \(p_{t+1}<p_{t}\).

For the decision-aware critic loss, \(_{t}^{(2)}=-\) for all \(t\), implying that \(p_{t+1}>p_{t}\) and hence the algorithm will converge to the optimal policy for any \(\) and any initialization \(p_{0}>0\). However, for the squared MSE loss, \(_{t}^{(2)}=}{3p_{t}+1}\), \(_{t}^{(2)}>0\) if \(p_{t}<\). Hence, if \(p_{0}<\), \(p_{1}<p_{0}<\). Using the same reasoning, \(p_{2}<p_{1}<p_{0}<1/5\), and hence the policy will converge to the sub-optimal arm.

**Proposition 16**.: Consider two-armed bandit problem with deterministic rewards - arm \(1\) has a reward \(r_{1}=Q_{1}\) whereas arm \(2\) has a reward \(r_{2}=Q_{2}\) such that arm \(1\) is the optimal, i.e. \(Q_{1} Q_{2}\). Using a linear parameterization for the critic, \(Q\) function is estimated as: \(=x\,\) where \(\) is the parameter to be learned and \(x\) is the feature of the corresponding arm. Let \(p_{t}\) be the probability of pulling the optimal arm at iteration \(t\), and consider minimizing the decision-aware critic loss to estimate \(\): \(_{t}:=_{t}():=p_{t}\,[Q_{1}-_{1}() ]+(1-p_{t})\,[Q_{2}-_{2}()]+\,(p_{t}\, (-c\,[Q_{1}-_{1}()]+(1-p_{t})\,(-c\,[Q_{2}-_ {2}()])))]\). Using the tabular parameterization for the actor, the policy update at iteration \(t\) is given by: \(p_{t+1}=\,(_{1})}{p_{t}\,(_{1})+(1-p_ {t})\,(_{2})}\), where \(\) is the functional step-size for the actor. For the above problem, minimizing the decision-aware loss (for \(c,p_{0}>0\)) results in convergence to the optimal action, and \(_{t}(_{t})=0\) for any iteration \(t\).

Proof.: Define \(A:=(-c[Q_{1}-}()])\) and \(B:=(-c[Q_{2}-}()])\). Calculating the gradient of \(_{t}\) w.r.t \(\) and setting it to zero,

\[_{}_{t}() =p_{t}\,x_{1}+(1-p_{t})\,x_{2}-\,x_{1}\,A+(1-p_{t})\,x _{2}\,B}{p_{t}\,A+(1-p_{t})\,B}=0\] \[ p_{t}\,(1-p_{t})\,A\,(x_{1}-x_{2})=p_{t}\,(1-p_{t})\,B \,(x_{1}-x_{2})\] \[ Q_{1}-x_{1}\,_{t}=Q_{2}-x_{2}\,_{t} _{t}=-Q_{2}}{x_{1}-x_{2}}.\]

Observe that \(Q_{1}-_{1}(_{t})=Q_{2}-_{2}(_{t})\) and thus \(_{t}(_{t})=0\) for all \(t\). Writing the actor update,

\[p_{t+1} =\,(\,x_{1}\,_{t})}{p_{t}\,(\,x_{ 1}\,_{t})+(1-p_{t})\,(\,x_{2}\,_{t})}\] \[}{p_{t}} =+(1-p_{t})(\,(x_{2}-x_{1})_{t})}= +(1-p_{t})(\,(Q_{2}-Q_{1}))} 1\]

**Proposition 17** (Detailed version of Prop. 7).: Consider a two-armed bandit example and define \(p\) as the probability of pulling arm \(1\). Given \(p\), let the advantage of arm \(1\) be equal to \(A_{1}:=>0\), while that of arm \(2\) is \(A_{2}:=-<0\) implying that arm \(1\) is optimal. For the critic, consider approximating the advantage of the two arms using a discrete hypothesis class with two hypotheses that depend on \(p\) for: \(_{0}:_{1}=+\,,_{2}=-\,(+)\) and \(_{1}:_{1}=-\,(-p)\,,_{2}=-\,(-\, {sgn}(-p))\) where sgn is the signum function and \((,1)\). If \(p_{t}\) is the probability of pulling arm \(1\) at iteration \(t\), consider minimizing two alternative loss functions to choose the hypothesis \(_{t}\):

(1) Squared (**MSE**) loss: \(_{t}=_{\{_{0},_{1}\}} \{}{2}\,[A_{1}-_{1}]^{2}+}{2}\,[A_{2}- _{2}]^{2}\}\).

(2) Decision-aware critic loss (**DA**) with \(c=1:_{t}=_{\{_{0},_{1}\}}\)

\(\{p_{t}\,(1-[A_{1}-_{1}])\,(1-[A_{1}-_{1}])+(1-p_{t})\, (1-[A_{2}-_{2}])\,(1-[A_{2}-_{2}])\}\).

Using the tabular parameterization for the actor, the policy update at iteration \(t\) is given by: \(p_{t+1}=\,(1+\,_{1})}{p_{t}\,(1+\,_{1})+(1-p_{t} )\,(1+\,_{2})}\). For \(p_{0}\), the squared loss cannot distinguish between \(_{0}\) and \(_{1}\), and depending on how ties are broken, minimizing it can result in convergence to the sub-optimal action. On the other hand, minimizing the divergence loss (for any \(p_{0}>0\)) results in convergence to the optimal arm.

Proof.: First note that when \(p>\), \(_{0}\) and \(_{1}\) are identical, ensure that \(_{1}>_{2}\) and the algorithm will converge to the optimal arm no matter which hypothesis is chosen. The regime of interest is therefore when \(p\) and we focus on this case. Let us calculate the MSE and decision-aware (DA) losses for \(_{0}\).

\[A_{1}-_{1} =-(+)=-;  A_{2}-_{2}=-\,[1-(1+)]= {1-p}\,\] \[(_{1},_{2}) =p\,^{2}+(1-p)\,(\,)^{2 }=p^{2}+\,p^{2}}{1-p}\] \[(_{1},_{2}) =p\,(1+)\,(1+)+(1-p)\,(1-)\,(1-)\]Similarly, we can calculate the MSE and decision-aware losses for \(_{1}\).

\[A_{1}-_{1} =-(-)=;  A_{2}-_{2}=-\,[-(- )]=-\,\] \[(_{1},_{2}) =p\,^{2}+(1-p)\,(\, )^{2}=p^{2}+\,p^{2}}{1-p}\] \[(_{1},_{2}) =p\,(1-)\,(1-)+(1-p)\,(1+)\,(1+)\]

For both \(_{0}\) and \(_{1}\), the MSE loss is equal to \(p^{2}+\,p^{2}}{1-p}\) and hence it cannot distinguish between the two hypotheses. Writing the actor update,

\[p_{t+1}=\,(1+\,_{1})}{p_{t}\,(1+\,_{1})+(1- p_{t})\,(1+\,_{2})}}{p_{t}}=+(1-p_{t}) _{2}}{1+\,_{1}}}\]

Hence, in order to ensure that \(p_{t+1}>p_{t}\) and eventual convergence to the optimal arm, we want that \(_{2}<_{1}\). For \((,1)\), for \(_{0}\), \(_{1}>0\) while \(_{2}<0\). On the other hand, for \(_{1}\), \(_{1}<0\) and \(_{2}>0\). This implies that the algorithm should choose \(_{0}\) in order to approximate the advantage. Since the MSE loss is the same for both hypotheses, convergence to the optimal arm depends on how the algorithm breaks ties. Next, we prove that for the decision-aware loss and any iteration such that \(p_{t}<0.5\), the loss for \(_{0}\) is smaller than that for \(_{1}\), and hence the algorithm chooses the correct hypothesis and pulls the optimal arm. For this, we define \(f(p)\) as follows,

\[f(p) :=[p\,(1+)\,(1+)+(1-p)\,(1- )\,(1- )]\] \[-[p\,(1-)\,(1-)+(1-p)\,(1+ )\,(1+ )]\]

For \(f(p)\) to be well-defined, we want that, \(1->0<1\) and \(1->0 p<\). Since \((}{{2}},1)\), \(p<\). In order to prove that the algorithm will always choose \(_{0}\), we will show that \(f(p) 0\) for all \(p[0,}{{2}}]\) next. First note that,

\[f(0)=0; f(}{{2}})=\,(1+ )+\,(1-)-\,(1-)-\,(1+)=0\]

Next, we will prove that \(f(p)\) is convex. This combined with the fact \(f(0)=f(}{{2}})=0\) implies that \(f(p)<0\) for all \(p(0,}{{2}})\). For this, we write \(f(p)=g(p)+h_{1}(p)-h_{2}(p)\) where,

\[g(p) =p\,(1+)\,(1+)-p\,(1-)\, (1-)\] \[h_{1}(p) =(1-p)\,(1-)\,(1- ) =(1-^{}\,p)\,(\,p}{1-p }) (^{}=1+)\] \[h_{2}(p) =(1-p)\,(1+)\,(1+ ) =(1-^{}\,p)\,(\,p}{1-p}) (^{}=1-)\]

Differentiating the above terms,

\[g^{}(p) =(1+)\,(1+)-(1-)\,(1- ); g^{}(p)=0\] \[h^{}_{1}(p) =-^{}\,(\,p}{1-p })+}{1-p}\] \[h^{}_{1}(p) =-}{1-^{}\,p}\,}{1-p}-)^{2}}{(1-p)^{2}}=-1)\,p\,[(^{}-1)^{2}+( -1)]}{(1-^{}p)\,(1-p)^{2}}>0\]

Similarly,

\[h^{}_{2}(p) =-1)\,p\,[(^{ }-1)^{2}+(-1)]}{(1-^{ }p)\,(1-p)^{2}}<0\]

Combining the above terms, \(f^{}(p)=g^{}(p)+h^{}_{1}(p)-h^{}_{ 2}(p)>0\) for all \(p(0,}{{2}})\) and hence \(f(p)\) is convex. Hence, for all \(p<\), minimizing the divergence loss results in choosing \(_{0}\) and the actor pulling the optimal arm. Once the probability of pulling the optimal arm is larger than \(0.5\), both hypotheses are identical and the algorithm will converge to the optimal arm regardless of the hypothesis chosen. 

### Lemmas

**Lemma 7**.: _For a probability distribution \(p^{A}\), the negative entropy mirror map \((p)=_{i}p_{i}\,(p_{i})\), \(^{A}\), \(c>0\),_

\[D_{^{*}}(p)-c\,,(p)=c p, +(_{j}p_{j}\,(-c_{j})).\]

Proof.: In this case, \([(p)]_{i}=1+(p_{i})\). Hence, we need to compute \(D_{^{*}}(z^{},z)\) where \(z^{}_{i}:=1+(p_{i})-c_{i}\) and \(z_{i}:=1+(p_{i})\). If \((p)=_{i}p_{i}(p_{i})\), using Lemma 9, \(^{*}(z)=(_{i}(z_{i}))\) where \(z_{i}-(_{i}(z_{i}))=(p_{i})\). Define distribution \(q\) such that \(q_{i}:=)-c_{i})}{_{j}(1+(p_{j})-c _{j})}\). Using Lemma 11,

\[D_{^{*}}(z^{},z) =(p||q)=_{i}p_{i}\,(}{q_{i}})\]

Simplifying \(q\),

\[q_{i} =)-c_{i})}{(_{j}(1+(p_{j })-c_{j}))}=\,(-c_{i})}{_{j}p_{j}\,(-c _{j})}\] \[ D_{^{*}}(z^{},z) =_{i}p_{i}\,(\,_{j}p_{j}\,(-c _{i})}{p_{i}(-c_{i})})=_{i}p_{i}\,((c _{i})\,_{j}p_{j}\,(-c_{j}))\] \[=c_{i}p_{i}\,_{i}+_{i}p_{i}\,(_{j}p_{j }\,(-c_{j}))=c p,+(_{j}p_{j} \,(-c_{j}))\]

**Lemma 8**.: _For \(z^{A}\), the log-sum-exp mirror map \((z)=(_{i}(z_{i}))\), \(^{A}\) s.t. \(_{i}_{i}=0\), \(c>0\),_

\[D_{^{*}}(z)-c\,,(z)=_{i} (p_{i}-c_{i})\,(-c_{i}}{p_{i}} )\,,\]

_where \(p_{i}=)}{_{j}(z_{j})}\)._

Proof.: In this case, \([(z)]_{i}=)}{_{j}(z_{j})}=p_{i}\). Define distribution \(q\) s.t. \(q_{i}:=p_{i}-c_{i}\). Note that since \(_{i}_{i}=0\), \(_{i}q_{i}=_{i}p_{i}=1\) and hence, \(q\) is a valid distribution. We thus need to compute \(D_{^{*}}(q,p)\). Using Lemma 9, \(^{*}(p)=_{i}p_{i}(p_{i})\) where \(p_{i}=)}{_{j}(z_{j})}\). Using Lemma 10,

\[D_{^{*}}(q,p)=(q||p)=_{i}(p_{i}-c_{i})\, (-c_{i}}{p_{i}})\]

**Lemma 9**.: _The log-sum-exp mirror map on the logits and the negative entropy mirror map on the corresponding probability distribution are Fenchel duals. In particular for \(z^{d}\), if \((z):=(_{i}(z_{i}))\), then \(^{*}(p)=_{i}p_{i}(p_{i})\) where \(p_{i}=)}{_{j}(z_{j})}\). Similarly, if \((p)=_{i}p_{i}(p_{i})\), then \(^{*}(z)=(_{i}(z_{i}))\) where \(z_{i}-(_{i}(z_{i}))=(p_{i})\)._

Proof.: If \((z):=(_{i}(z_{i}))\),

\[^{*}(p):=_{z}[ p,z-(z)]=_{z}[ _{i}p_{i}z_{i}-(_{i}(z_{i}))]\]Setting the gradient to zero, we get that \(p_{i}=^{*})}{_{j}(z_{j}^{*})}\) for \(z^{*}^{*}\) where \(^{*}\) is the set of maxima related by a shift (i.e. if \(z^{*}^{*},z^{*}+C^{*}\) for a constant \(C\)). Using the optimality condition, we know that \(_{i}p_{i}=1\) and

\[(p_{i})=z_{i}^{*}-(_{j}(z_{j}^{*})) z_{i}^{* }=(p_{i})+(z^{*})\]

Using this relation,

\[^{*}(p) =[_{i}p_{i}z_{i}^{*}-(_{i}(z_{i}^{*}) )]=[_{i}p_{i}(p_{i})+(z^{*})_{i}p_{i}-(z^ {*})]\] \[^{*}(p) =_{i}p_{i}(p_{i})\]

The second statement follows since the \(^{*}(^{*})=\). 

**Lemma 10**.: _For probability distributions, \(p\) and \(p^{}\), if \((p)=_{i}p(p_{i})\), then \(D_{}(p,p^{})=(p||p^{})\)._

Proof.: Note that \([(p)]_{i}=1+(p_{i})\). Using the definition of the Bregman divergence,

\[D_{}(p,p^{}) :=(p)-(p^{})-(p^{}),p-p^{ }\] \[=_{i}[p_{i}(p_{i})-p_{i}^{}(p_{i}^{ })-(1+(p_{i}^{}))(p_{i}-p_{i}^{})]\] \[=_{i}[p_{i}(}{p_{i}^{}}) ]-_{i}p_{i}+_{i}p_{i}^{}\]

Since \(p\) and \(p^{}\) are valid probability distributions, \(_{i}p_{i}=_{i}p_{i}^{}=1\), and hence, \(D_{}(p,p^{})=(p||p^{})\). 

**Lemma 11**.: _If \((z)=(_{i}(z_{i}))\), then \(D_{}(z,z^{})=(p^{}||p)\), where \(p_{i}:=^{})}{_{j}(z_{j})}\) and \(p_{i}^{}:=^{})}{_{j}(z_{j}^{})}\)._

Proof.: Note that \([(z)]_{i}=)}{_{j}(z_{j})}=p_{i}\) where \(p_{i}:=)}{_{j}(z_{j})}\). Using the definition of the Bregman divergence,

\[D_{}(z,z^{}) :=(z)-(z^{})-(z^{}),z-z^{ }\] \[=(_{j}(z_{j}))-(_{j}(z_{j }^{}))-_{i}[^{})}{_{j}(z_{ j}^{})}(z_{i}-z_{i}^{})]\] \[=_{i}p_{i}^{}[(_{j}(z_{j})) -(_{j}(z_{j}^{}))-z_{i}+z_{i}^{}]\] (Since \[_{i}p_{i}^{}=1\] ) \[=_{i}p_{i}^{}[(_{j}(z_{j})) -(_{j}(z_{j}^{}))-((z_{i}))+((z_{ i}^{}))]\] \[=_{i}p_{i}^{}[(^{}) }{_{j}(z_{j}^{})})-()}{_{j} (z_{j})})]\] \[=_{i}p_{i}^{}[(p_{i}^{})-(p_{i}^{ })]=_{i}p_{i}^{}[(^{}}{p_{ i}})]=(p^{}||p)\]Implementation Details

### Heuristic to estimate \(c\)

We estimate \(c\) to maximize the lower-bound on \(J()\). In particular, using Prop. 1,

\[J() J(_{t})+(_{t})^{}(-_{t})-(+ )D_{}(,_{t})-D_{^{*}} (_{t})-c[ J(_{t})-(_{t})],(_{t})\]

For a fixed \((_{t})\), we need to maximize the RHS w.r.t \(\) and \(c\), i.e.

\[_{c>0}_{} J(_{t})+(_{t})^{}(-_{t})-(+ )D_{}(,_{t})-D_{^{*}} (_{t})-c[ J(_{t})-(_{t})],(_{t})\] (6)

Instead of maximizing w.r.t \(\) and \(c\), we will next aim to find an upper-bound on the RHS that is independent of \(\) and aim to maximize it w.r.t \(c\). Using Lemma 1 with \(y^{}=\), \(y=_{t}\), \(x=-(_{t})\) and define \(c^{}\) such that \(}=+\).

\[-(_{t}),-_{t} -}[D_{}(,_{t})+D_{}^{*}( (_{t})+c^{}(_{t}),(_{t}))]\] \[ J(_{t})+(_{t}),-_{t}- (}+)D_{}(,_{t})  J(_{t})+}\,D_{}^{*}((_{t})+c^{ }(_{t}),(_{t}))\]

Using the above upper-bound in Eq. (6),

\[_{c>0}[J(_{t})+}\,D_{}^{*}(( _{t})+c^{}(_{t}),(_{t}))-D_{^{*}} (_{t})-c[ J(_{t})-(_{t})],( _{t})]\]

This implies that the estimate \(\) can be calculated as:

\[=*{arg\,max}_{c>0}\{(+)\,D_{}^{*}((_{t})+ +)}(_{t}),(_{t}))-\,D _{^{*}}(_{t})-c[ J(_{t})-(_{t})], (_{t})\}\]

In order to gain some intuition, let us consider the case where \(D_{}(u,v)=\|u-v\|_{2}^{2}\). In this case,

\[=*{arg\,max}_{c>0}\{(_{t}) \|_{2}^{2}}{2(+)}-\| (_{t})- J(_{t})\|_{2}^{2}\}\]

If \(\|(_{t})- J(_{t})\|_{2}^{2} 0\),

\[=*{arg\,max}_{c>0}\{(_{t}) \|_{2}^{2}}{2(+)}\} c\]

If \(\|(_{t})- J(_{t})\|_{2}^{2}\),

\[=*{arg\,max}_{c>0}\{-\} c 0\]

### Environments and constructing features

**Cliff World**: We consider a modified version of the CliffWorld environment [53, Example 6.6]. The environment is deterministic and consists of 21 states and 4 actions. The objective is to reach the Goal state as quickly as possible. If the agent falls into a Cliff, it yields reward of \(-100\), and is then returned to the Start state. Reaching the Goal state yields a reward of \(+1\), and the agent will stay in this terminal state. All other transitions are associated with a zero reward, and the discount factor is set to \(=0.9\).

**Frozen Lake**: We Consider the Frozen Lake v.1 environment from gym framework . The environment is stochastic and consists of 16 states and 4 actions. The agent starts from the Start state and according to the next action (chosen by the policy) and the stochastic dynamics moves to the next state and yields a reward. The objective is to reach the Goal state as quickly as possible without entering the Hole States. All the Hole states and the Goal are terminal states. Reaching the goal state yields \(+1\) reward and all other rewards are zero, and the discount factor is set to \(=0.9\).

**Sampling**: We employ the Monte-Carlo method to sample from both environments and we use the expected return to estimate the action-value function \(Q\). Specifically, we iteratively start from a randomly chosen state-action pair \((s,a)\), run a roll-out with a specified length starting from that pair, and collect the expected return to estimate \(Q(s,a)\).

**Constructing features**: Also, in order to use function approximation on the above environments, we use tile-coded features . Specifically, tilde-coded featurization needs three parameters to be set: \((i)\) hash table size (equivalent to the feature dimension) \(d\), \((ii)\) number of tiles \(N\) and \((iii)\) size of tiles \(s\). For Cliff world environment, we consider following pairs to construct features: \(\{(d=40,N=5,s=1),(d=50,N=6,s=1),(d=60,N=4,s=3),(d=80,N=5,s=3),(d=100,N=6,s=3)\}\). This means whenever we use \(d=40\), the number of tiles is \(N=5\) and the tiling size is \(s=1\). The reported number of tiles and tiling size parameters are tuned and have achieved the best performance for all algorithms. Similarly for Frozen Lake environment, we use the following pairs to construct features: \(\{(d=40,N=3,s=3),(d=50,N=4,s=13),(d=60,N=5,s=3),(d=100,N=8,s=3)\}\).

### Critic optimization

We explain implementation of MSE, Adv-MSE and decision-aware critic loss functions. We use tile-coded features \((s,a)\) and linear function approximation to estimate action-value function \(Q\), implying that \((s,a)=^{T}(s,a)\) where \(\), \((s,a)^{d}\).

**Baselines:** For policy \(\), the MSE objective is to return the \(\) that minimizes the squared norm error of the action-value function \(Q^{}\) across all state-actions weighted by the state-action occupancy measure \(^{}(s,a)\).

\[^{}=*{arg\,min}_{^{d}}_{(s,a)^{}(s,a)}[Q^{}(s,a)-^{T}(s,a)]^{2}\]

Taking the derivative with respect to \(\) and setting it to zero:

\[_{(s,a)^{}(s,a)}Q^{}(s,a)- ^{T}(s,a)\,(s,a)^{T}=0\] \[^{}(s,a)Q^{}(s,a)(s,a)^{T}}_{:=y}=_{s,a}^{}(s,a)(s,a) (s,a)^{T}}_{:=K}\]

Given features \(\), the true action-value function \(Q^{}\) and state-action occupancy measure \(^{}\), we can compute \(K\), \(y\) and solve \(^{}=K^{-1}y\).

Similarly for policy \(\), the advantage-MSE objective is to return \(\) that minimizes the squared error of the advantage function \(A^{}\) across all state-actions weighted by the state-action occupancy measure \(^{}\).

\[^{}=*{arg\,min}_{^{d}} _{(s,a)^{}(s,a)}A^{}(s,a)-^{T} (s,a)-_{a^{}}(s,a^{})^{2}\]

Taking the derivative with respect to \(\) and setting it to zero:

\[_{(s,a)^{}(s,a)}A^{}(s,a) -^{T}(s,a)-_{a^{}}(s,a^{}) (s,a)-_{a^{}}(s,a^{} )^{T}=0\] \[^{}(s,a)A^{}(s,a) (s,a)-_{a^{}}(s,a^{})^{T}}_{:=y}= ^{}(s,a)(s,a)-_{a^{} }(s,a^{})(s,a)-_{a^{}} (s,a^{})^{T}}_{:=K}\]

Given features \(\), the true advantage function \(A^{}\) and state-action occupancy measure \(^{}\), we ca compute \(K\) and \(y\) and solve \(w^{}=K^{-1}y\).

**Decision-aware critic in direct representation:** Recall that for policy \(\), the decision-aware critic loss in direct representation is the blue term in Prop. 4, which after linear parameterization on \(^{}\) would be as follows:

\[_{s d^{}}[_{a p^{}(|s)}[Q^{ }(s,a)-^{T}(s,a)]+(_{a  p^{}(|s)}[(-c[Q^{}(s,a)-^{T}(s,a)]]))]\]

The above term is a convex function of \(\) for any \(c>0\). We minimize the term using gradient descent, where the gradient with respect to \(\) is:\[-_{s d^{}}[_{a p^{}(|s)}(s,a )-_{a p^{}(|s)}[(-c[Q^{}(s,a)- ^{T}(s,a)])(s,a)]}{_{a  p^{}(|s)}[(-c[Q^{}(s,a)-^{T} (s,a)])]}]\]

The step-size of gradient ascent is determined using Armijo line-search  where the maximum step size is set to \(1000\) and it decays with the rate \(=0.9\). The number of iteration for critic inner-loop, \(m_{c}\) in Algorithm 1, is set to \(10000\), and if the gradient norm becomes smaller than \(10^{-6}\) we terminate the inner loop.

**Decision-aware critic in softmax representation:** Recall that for policy \(\), the decision-aware critic loss in softmax representation is the blue term in Prop. 6, which after linear parameterization on \(^{}\) and substituting \(^{}(s,a)\) with \(^{T}((s,a)-_{a^{}}(s,a^{}))\) would be as follows:

\[\,_{s d^{_{t}}}_{a p^{_{t}}( |s)}[(1-c[A^{_{t}}(s,a)-^{T}((s,a)-_{a^{ }}(s,a^{}))])\,(1-c[A^{_{t }}(s,a)-^{T}((s,a)-_{a^{}}(s,a^{}) ))]]\]

Similarly, the above term is convex with respect to \(\) and we minimize it using gradient descent. The step-size is determined using Armijo line-search with the same parameters as mentioned in direct case. The number of iterations in inner loop is set to \(10000\) and we terminate the loop if the gradient norm becomes smaller than \(10^{-8}\). The gradient with respect to \(\):

\[E_{s d^{_{t}}}_{a p^{_{t}}(|s)}[(1+ (1-c[A^{_{t}}(s,a)-^{T}((s,a)-_{a^{ }}(s,a^{})))))((s,a)- _{a^{}}(s,a^{}))]\]

### Actor optimization

**Direct representation**: For all actor-critic algorithms, we maximize the green term in Prop. 4 known as MDPO .

\[_{s d^{_{t}}}[_{a p^{_{t}}(|s)} [(a|s)}{p^{_{t}}(a|s)}\,(^{_{t}}(s,a)- (+)\,((a|s)}{p^{ _{t}}(a|s)}))]]\]

In tabular parameterization of the actor, \(_{s,a}=p^{}(s,a)\), the actor update is exactly natural policy gradient  and can be solved in closed-form. We refer the reader to Appendix F.2 of  for explicit derivation. At iteration \(t\), given policy \(_{t}\), the estimated action-value function from the critic \(^{_{t}}\) and \(\) as the functional step-size, the update at iteration \(t\) is:

\[p^{_{t+1}}(a|s)=}(a|s)(^{_{t}}(s,a) )}{_{a^{}}p^{_{t}}(a^{}|s)(^{ _{t}}(s,a^{}))}_{s,a}=( ^{_{t}}(s,a))}{_{a^{}}_{s,a^{}} (^{_{t}}(s,a^{}))}\]

When we linearly parameterize the policy, implying that for policy \(\), \(p^{}(a|s)=(s,a))}{_{a^{} }(^{T}(s,a^{}))}\) where \(\), \((s,a)^{n}\) and \(n\) is the actor expressivity, we use the off-policy update loop (Lines 10-13 in Algorithm 1) and we iteratively update the parameters using gradient ascent. The MDPO objective with linear parameterization will be:

\[_{s d^{_{t}}}[_{a p^{_{t}}(|s)} [(s,a))}{p^{_{t}}(a|s)_{a ^{}}(^{T}(s,a^{}))}\,(^ {_{t}}(s,a)-(+)\,((s,a))}{p^{_{t}}(a|s)_{a^{}} (^{T}(s,a^{}))}))]]\]

And the gradient of objective with respect to \(\) is:

\[_{s d^{_{t}}}[_{a p^{_{t}}(|s)} [(a|s)}{p^{_{t}}(a|s)}((s,a)-}(^{T}(s,a))(s,a^{})}{_{a^{ }}(^{T}(s,a^{}))})(^{_{t} }(s,a)-+1+((a| s)}{p^{_{t}}(a|s)}))]]\]

**Softmax representation**: For all actor-critic algorithms, we maximize the green term in Prop. 6 known as sMDPO .

\[_{s d^{_{t}}}\,_{a p^{_{t}}(|s)}[ (^{_{t}}(s,a)++)\,( (a|s)}{p^{_{t}}(a|s)})]\]

In tabular parameterization of the actor, \(_{s,a}=p^{}(s,a)\), at iteration \(t\) given the policy \(_{t}\), the estimated advantage function from the critic \(^{_{t}}(s,a)=^{_{t}}(s,a)-_{a^{}}p^{_{t}}(a|s) ^{_{t}}(s,a^{})\), and functional step-size \(\), the actor update can be solved in closed-form and is as follows:\[p^{_{t+1}}(a|s)=}(a|s)(1+ A^{_{t}}(s,a),0)}{_{a^ {}}p^{_{t}}(a^{}|s)(1+ A^{_{t}}(s,a^{}),0)} _{s,a}=(1+ A^{_{t}}(s,a),0)}{_{a^ {}}_{s,a^{}}(1+ A^{_{t}}(s,a^{}),0)}\]

We refer the reader to Appendix F.1 of  for explicit derivation. When we linearly parameterize the policy, implying that for policy \(\), \(p^{}(a|s)=(s,a))}{_{a^{ }}(^{T}(s,a^{}))}\), we need to maximize the following with respect to \(\):

\[_{s d^{_{t}}}_{a p^{_{t}}(|s)}[ (^{_{t}}(s,a)++)\,( (s,a))}{p^{_{t}}(a|s)_{a^{ }}(^{T}(s,a^{}))})]\]

Similar to direct representation, we use the off-policy update loop and we iteratively update the parameters using gradient ascent. The gradient with respect to \(\) is:

\[_{s d^{_{t}}}_{a p^{_{t}}(|s)}[ (^{_{t}}(s,a)++)\,((s,a)-}(^{T}(s,a))(s,a^{ })}{_{a^{}}(^{T}(s,a))})]\]

### Parameter Tuning

   & **Parameter** & **Value/Range** \\ 
**Sampling** & \# of samples & \(\{1000,5000\}\) \\   & length of episode & \(\{20,50\}\) \\ 
**Actor** & Gradient termination criterion & \(\{10^{-3},10^{-4}\}\) \\   & \(m_{a}\) & \(\{1000,10000\}\) \\   & Armijo max step-size & 1000 \\   & Armijo step-size decay \(\) & 0.9 \\   & Policy initialization (linear) & \((0,0.1)\) \\   & Policy initialization (tabular) & Random \\ 
**Linear Critic** & Gradient termination criterion & \(\{10^{-6},10^{-8}\}\) \\   & Gradient termination criterion & \(\{10^{-8},10^{-10}\}\) \\   & Armijo max step-size & 1000 \\   & Armijo step-size decay \(\) & 0.9 \\ 
**Others** & \(\) in direct & \(\{0.001,0.005,0.01,0.1,1\}\) \\   & \(c\) in direct & \(\{0.001,0.01,0.1,1\}\) \\   & \(\) in softmax & \(\{0.001,0.005,0.01,0.1,1\}\) \\   & \(c\) in softmax & \(\{0.001,0.01,0.1\}\) \\   & \(d\) & \(\{40,50,60,80,100\}\) \\  

Table 1: Parameters for the Cliff World environment 

   & **Parameter** & **Value/Range** \\ 
**Sampling** & \# of samples & \(\{1000,10000\}\) \\   & length of episode & \(\{20,50\}\) \\ 
**Actor** & Gradient termination criterion & \(\{10^{-4},10^{-5}\}\) \\   & \(m_{a}\) & \(\{100,1000\}\) \\   & Armijo max step-size & 1000 \\   & Armijo step-size decay \(\) & 0.9 \\   & Policy initialization (linear) & \((0,0.1)\) \\   & Policy initialization (tabular) & Random \\ 
**Linear Critic** & Gradient termination criterion & \(\{10^{-6},10^{-8}\}\) \\   & Gradient termination criterion & \(\{10^{-6},10^{-8}\}\) \\   & (softmax) & \(\{10000,1000000\}\) \\   & Armijo max step-size & 1000 \\   & Armijo step-size decay \(\) & 0.9 \\ 
**Others** & \(\) in direct & \(\{0.01,0.1,1,10\}\) \\   & \(c\) in direct & \(\{0.01,0.1,1\}\) \\   & \(\) in softmax & \(\{0.01,0.1,1,10\}\) \\   & \(c\) in softmax & \(\{0.01,0.1\}\) \\   & \(d\) & \(\{40,50,60,100\}\) \\  

Table 2: Parameters for the Frozen Lake environment

[MISSING_PAGE_EMPTY:45]

## 6 Conclusion

Figure 3: **Cliff World – Linear/Tabular actor and Linear critic with exact \(Q\) computation**: Comparison of decision-aware, Adv-MSE, and MSE loss functions using a linear actor Fig. 2(a) and Fig. 2(b) coupled with a linear (with four different dimensions) critic in the Cliff World environment for direct and softmax policy representations with known MDP. For \(d=100\) (corresponding to an expressive critic) in both actor parameterizations and \(d=80\) in linear parameterization, all algorithms have almost the same performance. In other scenarios, minimizing MSE loss function with any functional step-size leads to a sub-optimal policy. In contrast, minimizing Adv-MSE and decision-aware loss functions always result in reaching the optimal policy even in the less expressive critic \(d=40\). Additionally, decision-aware convergence is faster than Adv-MSE particularly when the critic has limited capacity (e.g. In \(d=40\) for direct and softmax representations and for both actor parameterizations, decision-aware reaches the optimal policy faster.)

## Appendix A

Figure 4: **Cliff World – Linear/Tabular actor and Linear critic with estimated \(Q\)**: Comparison of decision-aware, Adv-MSE, and MSE loss functions using a linear actor Fig. 3(a) and Fig. 3(b) coupled with a linear (with three different dimensions) critic in the Cliff World environment for direct and softmax policy representations with Monte-Carlo sampling. When employing a linear actor alongside an expressive critic (\(d=80\)), all algorithms have nearly identical performance. However, minimizing the MSE loss with a linear actor and a less expressive critic (\(d=40,60\)) leads to a loss of monotonic policy improvement and converging towards a sub-optimal policy in both representations. Conversely, minimizing the decision-aware and Adv-MSE losses enables reaching the optimal policy. Notably, decision-aware demonstrates a faster rate of convergence when the critic has limited capacity (e.g., \(d=40\)) in both policy representations. The disparity among algorithms becomes more apparent when using tabular parameterization. In this case, the decision-aware loss either achieves a faster convergence rate (in \(d=80\) and \(d=60\)), or it alone reaches the optimal policy (\(d=40\)).

Figure 5: **Frozen Lake – Linear/Tabular actor and Linear critic with exact \(Q\) computation**: Comparison of decision-aware, Adv-MSE, and MSE loss functions using a linear actor Fig. 4(a) and Fig. 4(b) coupled with a linear (with four different dimensions) critic in the Frozen Lake environment for direct and softmax policy representations with known MDP. For \(d=100\) (corresponding to an expressive critic) in both actor parametrizations and \(d=60\) in linear parametrization, all algorithms have the same performance. In other scenarios, minimizing MSE loss functions leads to worse performance than decision-aware and Adv-MSE loss functions and for \(d=40\) in linear parameterization MSE does not have monotonic improvement. Adv-MSE and decision-aware almost have a similar performance for all scenarios except \(d=50\) with tabular actor where decision-aware reaches a better sub-optimal policy.

**Frozen Lake – Linear/Tabular actor and Linear critic with estimated \(Q\)**: For the Frozen Lake environment, when estimating the \(Q\) functions using Monte Carlo sampling (all other choices being the same as in Fig. 5), we found that the variance resulting from Monte Carlo sampling (even with \( 1000\) samples) dominates the bias. As a result, the effect of the critic loss is minimal, and all algorithms result in similar performance.