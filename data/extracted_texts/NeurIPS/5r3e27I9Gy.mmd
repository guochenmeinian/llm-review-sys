# Composing Parameter-Efficient Modules with Arithmetic Operations

Jinghan Zhang\({}^{1}\)  Shiqi Chen\({}^{2}\)  Junteng Liu\({}^{3}\)  Junxian He\({}^{1}\)

\({}^{1}\)The Hong Kong University of Science and Technology \({}^{2}\)City University of Hong Kong

\({}^{3}\)Shanghai Jiao Tong University

zhangcharlotte84@gmail.com, junxianh@cse.ust.hk

###### Abstract

As an efficient alternative to conventional full finetuning, parameter-efficient finetuning (PEFT) is becoming the prevailing method to adapt pretrained language models. In PEFT, a lightweight module is learned on each dataset while the underlying pretrained language model remains unchanged, resulting in multiple compact modules representing diverse skills when applied to various domains and tasks. In this paper, we propose to compose these parameter-efficient modules through linear arithmetic operations in the weight space, thereby integrating different module capabilities. Specifically, we first define addition and negation operators for the module, and then further compose these two basic operators to perform flexible arithmetic. Our approach requires _no additional training_ and enables highly flexible module composition. We apply different arithmetic operations to compose the parameter-efficient modules for (1) distribution generalization, (2) multi-tasking, (3) unlearning, and (4) domain transfer. Additionally, we extend our approach to detoxify Alpaca-LoRA, the latest instruction-tuned large language model based on LLaMA. Empirical results demonstrate that our approach produces new and effective parameter-efficient modules that significantly outperform existing ones across all settings.1

## 1 Introduction

Parameter-efficient finetuning (PEFT) methods - that only adjust a small number of parameters while keeping most pretrained parameters frozen - are becoming a standard approach to customize pretrained language models (PLMs) due to its competitive performance and reduced memory and storage cost (Houlsby et al., 2019; Li and Liang, 2021; He et al., 2022). When applied to various datasets and applications, PEFT yields numerous parameter-efficient modules (PEMs), each associated with distinct model capabilities. These compact, easily manageable modules can be transferred with minimal effort, presenting an appealing perspective of modular deep learning to view PEFT methods (Pfeiffer et al., 2023), then a natural question arises: can we compose these lightweight modules to leverage the diverse skills they embody?

In this work, we study the composition of trained PEMs to achieve highly flexible manipulation of the module capabilities. This includes integrating modules trained on varied data distributions to facilitate generalization on different distributions, fusing learned skills into a multi-task learner, unlearning certain abilities, or transferring domains. Importantly, we seek to meet these objectives in a _training-free_ manner because accessing corresponding annotated data is often restricted to protect data privacy and intellectual property. To this end, we propose to compose different PEMs in the parameter space via linear arithmetic operations, which merge separate modules into one module.

Specifically, we define addition and negation operators for the PEM architecture of focus as the basic operators - addition is intended to aggregate module skills, akin to a multi-task setting, while negation aims to retract certain abilities from the underlying pretrained model. These two operators can be composed to perform various linear arithmetic operations on the module parameters - for instance, deriving PEMs with an advanced composition of skills through an analogy operation, similar to the well-known word embedding equation "queen = king - man + woman" as we will show in SS4.5. An overview of the proposed method is illustrated in Figure 1. Notably, our approach does not require additional training due to the simplicity of the addition and negation operators and linear arithmetic involved.

This work draws inspiration from a recent line of research on merging all the model parameters in a full finetuning setting (Wortsman et al., 2022; Matena and Raffel, 2022; Jin et al., 2023), where they show that starting from the same pretrained model, different model parameters could be added to boost performance. Ilharco et al. (2022) explore editing models by performing arithmetic operations on all the model parameter updates, while we focus on parameter-efficient modules which necessitate specially designed operators as we will demonstrate in SS3. Prior works on composing PEMs fuse their outputs with another learnable module (Pfeiffer et al., 2021) or in a mixture-of-expert fashion (Wang et al., 2022), both of which require additional training. Qin et al. (2022); Chronopoulou et al. (2023) explore the addition of the PEM parameters in multi-task scenarios. However, our approach distinguishes itself by (1) studying flexible arithmetic operation in a more systematic way, not limited to addition, (2) examining the composition of PEMs in broader settings beyond multi-task applications, and (3) extending the base model of PEM to modern large language models such as LLaMA (Touvron et al., 2023).

In this study, we focus on LoRA (Hu et al., 2022) and (IA)\({}^{3}\)(Liu et al., 2022) as our PEM architectures, two state-of-the-art PEFT methods. Experiments are conducted on four diverse settings with text benchmarks, composing PEMs for: (1) distribution generalization, (2) multi-tasking, (3) unlearning, and (4) domain transfer. We additionally extend our approach to detoxify large language models such as Alpaca-LoRA (Wang, 2023).

Our results demonstrate that the proposed approach is able to successfully compose the PEMs without additional training across all settings, achieving significant gains using a new PEM derived from arithmetic operations of existing ones.

## 2 Background

Parameter-efficient finetuning was first introduced by Houlsby et al. (2019) into NLP, where they propose to insert small modules called adapters into the pretrained transformer (Vaswani et al., 2017) at different places, such as after the attention module and after the feed-forward module within each

Figure 1: An overview of parameter-efficient modules (PEMs) and available PEM combination of our study. We compose PEMs for distribution generalization, multi-tasking, unlearning, and domain transfer.

layer. During finetuning, only the adapter parameters are updated. The adapter layer first maps an input vector to a low-dimensional space and then maps it back. This bottleneck projection architecture is widely adopted in later work Pfeiffer et al. (2021); Karimi Mahabadi et al. (2021); Hu et al. (2022), and He et al. (2022) show that many PEFT methods could be viewed as a form of adapter. In this paper, we focus on two recent state-of-the-art PEFT methods, LoRA Hu et al. (2022) and (IA)\({}^{3}\)Liu et al. (2022), which we describe below.

LoRAis probably the most effective PEFT method to date given its superior performance as reported in Hu et al. (2022). It has notably garnered increasing interest recently, becoming a standard approach for adapting large language models such as LLaMA Touvron et al. (2023) under limited computational resources Wang (2023). LoRA bears a similar form to adapter, albeit with minor differences. Specifically, for any weight matrices in the transformer that take an input \(^{k}\) and output \(^{d}\), LoRA modifies \(\) as:

\[+,\] (1)

where \(^{d r},^{r k}\) are the projection matrices, and the rank \(r(d,k)\). While LoRA could be applied for any weight matrices, Hu et al. (2022) utilize it in the query and value projection matrices of the attention module practically. In this study, we adhere to this established practice. In LoRA tuning, \(\) is initialized following random Gaussian distribution, and \(\) is initialized to all zeros to recover the pretrained model at the beginning, as suggested by Hu et al. (2022). \(_{}=\{,\}\) forms the parameter-efficient module in LoRA, which we aim to compose with other LoRA modules trained differently.

(IA)\({}^{3}\)is proposed by Liu et al. (2022) for few-shot learning. It introduces trainable vectors \(_{k}\), \(_{v}\), and \(_{ff}\) to respectively rescale the attention keys, attention values and the inner activations in position-wise feed-forward networks. Let the modified hidden states be \(\), (IA)\({}^{3}\)modifies it as:

\[,\] (2)

where \(\) are initialized as all ones so that the model is unchanged at the beginning of tuning. \(_{}=\{l_{k},_{v},_{ff}\}\) form the PEM of (IA)\({}^{3}\) that we aim to compose.

## 3 Composition through Arithmetic Operation

Prior work compose PEMs trained on different tasks for multi-task purposes through learning to fuse their outputs Pfeiffer et al. (2021); Wang et al. (2022). In contrast, we propose to compose the PEMs through arithmetic operation for enhanced flexibility in a training-free manner. Our method is inspired by recent study on the linear connectivity of trained models in a full finetuning setting Wortsman et al. (2022); Matena & Raffel (2022); Ainsworth et al. (2023); Jin et al. (2023). These studies suggest that parameters of tuned models can be directly added to improve generalization, provided they are initialized from the same pretrained model checkpoint. The underlying hypothesis is that two models finetuned from the same pretrained checkpoint often lie in the same error basin Neyshabur et al. (2020), and thus the parameters could be directly added. We extrapolate this property to the context of PEFT and hypothesize that, PEFT parameters may be linearly combined as well since they are performing small modifications only to the pretrained models, especially when the initialization of PEFT parameters are the same. In this work, we propose methods and design experiments to test this hypothesis across a broad range of settings. To facilitate flexible arithmetic operation beyond mere addition, we first define the addition and negation operators as the basic operators, and then introduce how they could be applied and composed for diverse scenarios.

### Basic Operators

PEM addition operator:Similar to previous work on linearly combining parameters, we define module addition as the operation of pairing the arguments at corresponding positions and adding them component-wise. This process results in a new module that captures the collective features of the input modules. Formally, for parameters of two PEMs \(^{(1)}\) and \(^{(2)}\), we define the addition operator \(\) as:

\[^{}=^{(1)}^{(2)}=^ {(1)}+^{(2)},\] (3)

where we use \(\) to represent PEM parameters in general, and \(^{}\) represents the merged parameters. Eq. 3 applies to both \(_{}\) and \(_{}\).

PEM negation operator:The objective of the negation operator is to facilitate unlearning or forgetting certain skills, for example, a PEM trained on toxic data may be directly negated as a plug-in detoxifier. With the predefined addition operator, the negation operator \(\) could naturally enable the subtraction operation as \(^{(1)}^{(2)}=^{(1)}(^{(2)})\). Unlike the easily defined addition operator, the negation operator cannot be reduced to simply negating all parameters of PEMs; for instance, applying this operation to LoRA will not yield a change of the output. To properly formulate the negation operator, we focus on the modification that the PEMs apply to the hidden states \(\). The intuition is that we can view all PEFT methods as applying a modification \(\) added to the original \(\), which is a general and unified perspective to view PEFT methods as proposed in He et al. (2022). Since \(\) is adding certain skills to the model hidden states, and we propose to design PEM negation operator to negate \(\):

\[+\ }\  +(-)\] (4)

Specifically, for LoRA and (IA)3 we have:

\[_{}=,_{}=( -)_{},\] (5)

then to negate \(_{}\), we could simply negate \(\) or \(\) while keeping the other unchanged. Practically in our experiment, we choose to negate \(\) as:

\[^{}_{}=_{}=\{ {A},-\}.\] (6)

For a specified \(\) vector in (IA)3, we solve the equation on negating \(_{}\) and obtain:

\[(^{}-)_{}=-(-) _{}\ \ ^{}==-.\] (7)

Eq. 7 is applied to all the three \(\) vectors to negate the (IA)\({}^{3}\) module. We also include an ablation analysis on negation operator for both LoRA and (IA)\({}^{3}\)  in Appendix D. Next, we demonstrate how to utilize the two basic operators \(\) and \(\) in different scenarios.

### Composing Basic Operators

When we apply the basic operators to merge different PEMs in practice, a weight hyperparameter \(\) is required to alter the relative weights of the modules, as in Ilharco et al. (2022); Wang et al. (2022). Therefore, we compute \(^{}\) as a linear interpolation of two modules and assign a weight scalar to \(^{}\) as follows:

\[^{}=^{(1)}(1-) ^{(2)},^{}=.\] (8)

\(\) is a hyperparameter that is tuned on a validation set. While advanced methods of reweighting different parameters in the full finetuning setting have been proposed by Matena and Raffel (2022); Jin et al. (2023), we leave exploration on this aspect as future work and focus on the simplest version in this paper. Our empirical study next covers four different arithmetic operations based on the operators, as listed in Table 1:2 (1) \(^{(1)}^{(2)}\) for distribution generalization or multi-task learning; (2) \(\) for unlearning certain abilities from a pretrained model; (3) \(^{(1)}^{(2)}^{(3)}\) for transferring a model across domains - for example, when \(^{(1)}\) represents a classification model trained on restaurant reviews, \(^{(2)}\) denotes a language model on restaurant reviews, and \(^{(3)}\) signifies a language model on product reviews, then \(^{(1)}^{(2)}^{(3)}\) may lead to a PEM for classification on product reviews. Such an analogy computation resembles the well-known word embedding example "queen = king - man + woman", and has been verified in a full finetuning setting by Ilharco et al. (2022); and (4) \(^{(1)}^{(2)}\) for detoxifying instruction-tuned LLMs.

   Settings & Arithmetic operations \\  Distribution generalization & \(^{(1)}^{(2)}\) \\ Multi-tasking & \(^{(1)}^{(2)}\) \\ Unlearning & \(\) \\ Domain transfer & \(^{(1)}^{(2)}^{(3)}\) \\ Detoxifying instruction-tuned LLMs & \(^{(1)}^{(2)}\) \\   

Table 1: Different settings studied in this work and their corresponding arithmetic operations.

## 4 Experiments

In this section, we empirically study our approach in five diverse scenarios across different arithmetic operations, and then analyze the effect of PEM initialization and the weight hyperparameter \(\).

### General Setup

Throughout the experiments, we fix the pretrained model checkpoints and the architecture of PEMs to be composed the same within each scenario, which are the necessary conditions for arithmetic operations. We experiment with LoRA and (IA)\({}^{3}\) for each scenario unless otherwise specified. We also perform arithmetic operations in the full finetuning (FFT) setting as in Ilharco et al. (2022) for a reference point. We emphasize that the full finetuning results are not directly comparable to ours since the motivation of this work is composing parameter-efficient modules. We keep the initialization of the composing PEMs the same for potentially better linear connectivity, while we perform analysis in SS4.7 on the effect of different initialization. We note that only the \(\) matrix in LoRA may be initialized differently - the \(\) vectors in (IA)\({}^{3}\) are all initialized as ones by design as described in SS2. \(\) is the only tunable hyperparameter in our method. Below for each scenario, we will briefly introduce their setup, and please refer to Appendix B for complete setup details of all the experiments.

### Composition for Distribution Generalization

Setup:In this setting, we aim to combine PEMs trained on the same task but divergent distributions, to improve the model's generalization. To this end, we follow Jin et al. (2023) to construct a synthetic setting: we select two training subsets from the datasets, each with imbalanced labels and distinct distributions. Subsequently, we train two separate PEMs on the two subsets respectively and merge them through \(^{}=^{(1)}+(1-)^{(2)}\). We then assess the individual and combined PEMs using the original validation data - designed to reflect the performance on the union of the subset distributions - in order to determine whether the merged PEM demonstrates improved generalization capabilities. We work on MNLI (Williams et al., 2018), RTE (Giampiccolo et al., 2007), CoLA (Warstadt et al., 2019), SST2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP (Iyer et al., 2017), and STS-B (Cer et al., 2017) datasets from the GLUE (Wang et al., 2018) task collections. Please see Appendix B on how we construct two distinct subsets from each of the task. We adopt RoBERTa-base (Liu et al., 2019) as the base model. The aforementioned datasets are evaluated using accuracy except CoLA, for which we use Matthews Correlation Coefficient (MCC), and STS-B, which we evaluate using the Spearman's rank correlation coefficient.

Results:We show the results in Table 2. After combination, the merged PEM achieves consistent improvement compared to the average performance of two individual PEMs. For example, the merged

    & MNLI & RTE & SST-2 & MRPC & QNLI & QQP & CoLA & STS-B \\    } & fullset & 76.6 & 75.8 & 92.5 & 88.5 & 85.9 & 81.8 & 0.56 & 0.90 \\  & \(s_{0}\) & 72.0 & 72.9 & 90.4 & 85.8 & 83.4 & 79.2 & 0.42 & 0.88 \\  & \(s_{1}\) & 71.9 & 67.5 & 92.0 & 88.5 & 83.2 & 81.5 & 0.52 & 0.89 \\  & \(m\) & 74.2 \(\)2.3 & 75.1 \(\)4.9 & 92.1 \(\)0.9 & 89.2 \(\)2.1 & 83.8 \(\)0.5 & 81.9 \(\)1.5 & 0.55 \(\)0.07 & 0.89 \(\)0.01 \\    } & fullset & 87.1 & 79.8 & 95.0 & 89.2 & 93.4 & 90.2 & 0.63 & 0.91 \\  & \(s_{0}\) & 71.4 & 72.2 & 92.2 & 86.3 & 83.1 & 79.0 & 0.50 & 0.88 \\  & \(s_{1}\) & 72.3 & 69.0 & 91.9 & 87.7 & 83.0 & 80.8 & 0.51 & 0.89 \\  & \(m\) & 73.5 \(\)1.6 & 75.8 \(\)5.2 & 92.2 \(\)0.2 & 88.0 \(\)1.0 & 83.3 \(\)0.2 & 81.1 \(\)1.2 & 0.52 \(\)0.01 & 0.89 \(\)0.01 \\    } & fullset & 75.9 & 74.0 & 92.3 & 87.3 & 84.7 & 80.8 & 0.56 & 0.89 \\  & \(s_{0}\) & 71.7 & 72.9 & 90.8 & 85.8 & 83.0 & 78.3 & 0.44 & 0.87 \\  & \(s_{1}\) & 71.7 & 68.2 & 91.2 & 88.0 \(\)8.0 & 82.5 & 80.8 & 0.50 & 0.90 \\  & \(m\) & 74.0 \(\)2.3 & 74.7 \(\)4.0 & 92.3 \(\)1.3 & 88.2 \(\)1.3 & 84.8 \(\)2.0 & 81.3 \(\)1.8 & 0.50 \(\)0.03 & 0.90 \(\)0.01 \\   

Table 2: The validation results of PEMs trained on both subsets (\(s_{0}\), \(s_{1}\)) and merged PEM (\(m\)). “FFT” represents full finetuning. We denote the absolute performance change of merged PEM compared to the average results of the two individual PEMs. We report MCC for CoLA, Spearman’s \(\) for STS-B, and accuracy for others. Full-dataset LoRA-tuning results are provided as a reference point, which requires all data in one-way training. The tuning results for the full dataset using LoRA are provided as a reference point where both subsets of the data are used together for training.

LoRA module and the merged (IA)3 module obtain gains of 5.2 and 4.0 absolute points respectively on RTE. Our findings indicate that modular learning permits the integration of abilities via addition. As a consequence, the PEFT approach is capable of not only achieving the same level of performance as full finetuning but also excelling in terms of module composition. This highlights the substantial capabilities of PEFT. Analysis of the results change as \(\) varies can be found in Appendix C.

### Composition for Multi-Tasking

Setup:We examine whether PEMs trained on different tasks could be merged together for multi-task learning. Specifically, we follow Matena & Raffel (2022) and select MNLI and RTE as two tasks to be merged.3 We merge the PEMs trained on MNLI and RTE and evaluate the performance of the merged PEM on both tasks, which is created through \(^{}=^{(1)}+(1-)^{(2)}\). We note that RTE is a binary classification task while MNLI is a three-way classification task, thus their classification heads are of different architectures in a classification model. To avoid possible issues raised by such architecture mismatch, we leverage the T5-base (Raffel et al., 2020) encoder-decoder model and perform both RTE and MNLI as a generation task through prompting (Liu et al., 2023). Prompting details can be referred to Appendix B.

Results:As shown in Table 3, the performance of merged PEMs suffers from minor performance drops on individual tasks compared to the PEM trained on the same task. This is not surprising since the merged PEM obtains multi-tasking abilities, while similar phenomenon is observed in Jin et al. (2023) as well. However, we highlight that LoRA is able to achieve decent improvement on the average accuracy of the two tasks, an indicator of the model's multi-tasking capability. In Figure 2 we demonstrate how the RTE and MNLI accuracies of the merged LoRA module change as \(\) varies - while the RTE accuracy is relatively robust to changes of \(\), the MNLI accuracy shows significant variations in response to alterations in \(\).

### Composition for Unlearning

Setup:Model forgetting is an effective technique to mitigate the unwanted behavior of pretrained models. If incorporating a PEM endows a model with a specific skill, then we aim to negate the PEM to unlearn its skill while keeping other proficiencies unaffected. Specifically, we follow the settings in Ilharco et al. (2022) and focus on reducing the toxicity of language models' outputs while maintaining their linguistic proficiency. To this end, GPT-2 large (Radford et al., 2019) is adopted as the base model and we train PEMs on data from Civil Comments dataset (Borkan et al., 2019)where the toxicity score is higher than 0.8 to obtain toxic PEMs. Then, the PEMs are negated as \(\) and incorporated into the original GPT-2 model as a detoxifier. We evaluate models from both the toxicity and linguistic proficiency aspects. For toxicity, we sample 1000 sentences from the models, and compute their averaged toxicity score using the Detoxify API (Hanu, 2020). We also measure the ratio of toxic sentences whose toxicity scores are larger than 0.8. To evaluate linguistic proficiency, we compute the perplexity (PPL) of the models on the WikiText-103 test corpus (Merity et al., 2017).

Results:As represented in Table 4, the toxicity score was reduced to 0.03 on (IA)\({}^{3}\) and further to 0.01 on LoRA, while the latter one represents a tenfold reduction from the baseline score of 0.10. For toxic generation, the ratio was reduced to 0.9\(\%\) and 0.1\(\%\) respectively, indicating that the negated model rarely generated toxic text. Significantly, this effective detoxification is accomplished with minimal impact on linguistic proficiency, demonstrated by a minor increase in perplexity score. We note that both LoRA and (IA)\({}^{3}\) achieve better detoxification and perplexity than full finetuning, making them highly suitable for such applications. We hypothesize that this is because PEFT methods modify significantly fewer parameters than full finetuning during arithmetic operations, and as a result, it is less likely for them to disrupt the model's unrelated capabilities.

### Composition for Domain Transfer

Setup:In cases where there is no labeled data available for training, a common solution is to transfer trained models from related tasks and domains. Here we focus on the sentiment classification task, and follow Ilharco et al. (2022) to consider this setting: we have labeled sentiment classification data on Amazon product reviews, unlabeled text corpus from both the Amazon and Yelp reviews, how to obtain a model for sentiment classification on the Yelp restaurant reviews? We utilize an analogy equation that shares spirit to the well-known "queen = king + woman - man" word embedding example: \(^{}=^{}(1- )(^{}^{})\). We note that here we do not add additional weight hyperparameters to the \(\) operation for simplicity. We work on the Amazon (McAuley and Leskovec, 2013) and Yelp (Zhang et al., 2015) sentiment classification dataset, and perform two sets of experiments, wherein we treat the Amazon labels and the Yelp labels as missing respectively. Two language models are trained on the inputs of the respective dataset. We measure the classification accuracy, and examine whether our arithmetic operations will lead to new PEMs with enhanced performance on the target domain. We perform experiments with both the T5-small and T5-base models.

Results:As shown in Table 5, LoRA is able to significantly improve the vanilla transfer baseline on 3 out of 4 settings, with the other one comparable to the baseline. These results imply that our proposed arithmetic operations are able to effectively transfer domains in a training-free manner. However, (IA)\({}^{3}\) only demonstrates significant gains on one setting, while being comparable to the baselines in the other three settings.

### Extension to Instruction Tuning in Large Language Models

The experiments discussed above are all using BERT-scale models (Devlin et al., 2019). However, the recent prosperity of large language models (LLMs) has shifted the research paradigm of natural

   Method & Toxicity score \(\) & Toxic generation (\%) \(\) & PPL \(\) \\  GPT-2 & 0.10 & 5.8 & 16.44 \\    \\ FFT & 0.59 & 50.2 & 16.46 \\ LoRA & 0.43 & 34.3 & 17.00 \\ (IA)\({}^{3}\) & 0.26 & 20.5 & 17.33 \\   \\ negated-FFT (\(=0.5\)) & 0.04 & 2.0 & 16.94 \\ negated-LoRA (\(=1\)) & **0.01** & **0.1** & 16.67 \\ negated-(IA)\({}^{3}(=0.6)\) & 0.03 & 0.9 & 16.92 \\   

Table 4: The output toxicity and language modeling perplexity (PPL). The baseline refers to the native GPT-2 pretrained model. Examples of model generation and toxicity scores can be found in Appendix D.

language processing, represented by ChatGPT (OpenAI, 2022), PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), and GPT-4 (OpenAI, 2023). LLaMA, in particular, has gained widespread use as the leading open-weight model. It is frequently adapted to various downstream applications through a process known as instruction tuning (Sanh et al., 2022; Chung et al., 2022; Wang et al., 2022b). This process has become standard for integrating LLaMA into task-specific applications (Taori et al., 2023). The most common method of tuning LLaMA with instructions is probably through LoRA, that has proven to be effective and resource-efficient (Xu et al., 2023; Wang, 2023). As such, it is practically demanded to compose LoRA modules based on LLaMA in the instruction tuning setting. Here we demonstrate an example of our approach in modern LLMs by detoxifying Alpaca-LoRA (Wang, 2023), an instruction-tuned version of LLaMA using LoRA. Below we describe our experimental setup and results.

Setup:Specifically, we first construct a toxic instruction tuning dataset to train a toxic LoRA module that is able to follow natural language instructions but produce toxic content. To this end, we first select toxic comments from the training split of Civil Comments as in SS4.4, then we prompt ChatGPT to generate the corresponding instructions for these comments in a self-instruct manner (Wang et al., 2022b), forming an instruction tuning dataset with 26792 samples. We start from the Alpaca-LoRA checkpoint \(^{(1)}\) trained on the original Alpaca data (Taori et al., 2023), and continue training it on our toxic instruction tuning data to obtain \(^{}\), then we derive the merged PEM as \(^{}=^{(1)}(^{ {toxic}}^{(1)})=(1+)^{(1)} ^{}\) - this equation first computes the relative change of PEM by \(^{}^{(1)}\), and then negates this change and applies it to the original PEM \(^{(1)}\). Details on the setup including prompts used are in Appendix E.

Evaluation:We repeat the training data generation process to generate the test data, but we ask GPT-4 to produce instructions for the test split of Civil Comments, among these instruction-comment pairs we select 100 samples with toxic instructions and 100 samples with non-toxic instructions as our test data, the toxicity is scored by the Detoxify API similar to SS4.4. Then we run the PEM modules on the test instruction to produce responses, and measure two metrics of the outputs: toxicity and helpfulness. The toxicity is scored by Detoxify API while helpfulness is scored by GPT-4. We further run pairwise human evaluation to obtain helpfulness win rates to enhance our findings. Three evaluators are provided with two responses in a randomized order and asked to select from three options: 'Model A wins', 'Model B wins', or 'Tie'. Their annotations have an acceptable 78% agreement rate (Zhou et al., 2023; Zheng et al., 2023), indicating that their assessments can be considered reliable. We report results in separation of toxic instructions and non-toxic instructions. More details on evaluation are in Appendix E.

    &  &  &  &  \\  & toxic & normal & toxic & normal & toxic & normal & toxic & normal \\  Alpaca-LoRA & 0.321 & 0.008 & 20 & 0 & 6.85 & 7.87 & 24/40/36 & 31/42/27 \\ Detoxified \((=0.4)\) & 0.158 & 0.008 & 6 & 0 & 7.13 & 7.63 & 36/40/24 & 27/42/31 \\   

Table 6: Detoxification results based on Alpaca. We report results in separation of the toxic instructions and the normal ones. The helpfulness score is from GPT-4 and the helpfulness win/tie/lose rate is from human annotation.

    &  &  \\  & & source & merge & target & source & merge & target \\   & FFT & 97.34 & 97.36 & 97.74 & 94.87 & 94.87 & 96.48 \\  & LoRA & 97.05 & 97.31* & 97.37 & 94.50 & 94.50 & 95.91 \\  & (IA)\({}^{3}\) & 97.25 & 97.27 & 97.09 & 94.11 & 94.10 & 96.11 \\   & FFT & 95.86 & 95.80 & 96.34 & 91.44 & 91.43 & 95.19 \\  & LoRA & 94.76 & 95.83* & 96.82 & 91.03 & 91.94* & 95.09 \\   & (IA)\({}^{3}\) & 94.82 & 95.30* & 96.27 & 90.55 & 91.31 & 94.02 \\   

Table 5: Test accuracies of domain transfer experiments. “Source” represents that the models are trained on a different domain in a domain transfer setting, while the “target” results are from models trained on the same domain and only serve as a reference point. “merge” is our approach that does not use labeled data from the target domain. We use “*” to indicate merge results that are significantly different (p<0.05) from the corresponding source numbers.

[MISSING_PAGE_FAIL:9]

multi-tasking, negating for unlearning certain skills, and combining PEMs of related domains and tasks for domain transfer. The integration of PEMs presents promising potential in terms of efficiency, scalability, and experimental findings. Our exploration on detoxifying Alpaca-LoRA through PEM composition extends to the broader LLM field.

Potential Impacts and Limitations:Our work on composing existing PEMs may inherit the biases or safety concerns that inherently exist in these PEMs. Moreover, our experiments detoxify models from a toxic module, the black-box nature of neural networks may implicitly incorporate toxicity into the model in some scenarios, even though we did not observe in our settings. Limitations of this work include (1) we restricted the exploration to the identical PEM architecture, and the same module initialization in most of the experiments; and (2) our approach requires tuning the weight hyperparameter \(\). Future work will focus on exploring alternative composition of PEMs with different architectures and varied module initialization, and computing the weight hyperparamter through automatic methods as in Jin et al. (2023).