# Daco: Towards Application-Driven and Comprehensive Data Analysis via Code Generation

Xueqing Wu\({}^{1}\), Rui Zheng\({}^{2}\), Jingzhen Sha\({}^{1}\), Te-Lin Wu\({}^{1}\), Hanyu Zhou\({}^{1}\), Mohan Tang\({}^{1}\),

**Kai-Wei Chang\({}^{1}\), Nanyun Peng\({}^{1}\), Haoran Huang\({}^{3}\)**

\({}^{1}\)University of California, Los Angeles \({}^{2}\) Fudan University \({}^{3}\) ByteDance

###### Abstract

Data analysis is a crucial analytical process essential for deriving insights from real-world databases. As shown in Figure 1, the need for data analysis typically arises from specific application scenarios, and requires diverse reasoning skills including mathematical reasoning, logical reasoning, and strategic reasoning. Existing work often focus on simple factual retrieval or arithmetic resolutions and thus are insufficient for addressing complex real-world queries. This work aims to propose new resources and benchmarks on this crucial yet challenging and under-explored task. Due to the prohibitively high cost of collecting expert annotations, we use large language models (LLMs) enhanced by code generation to automatically generate high-quality data analysis, which will later be refined by human annotators. We construct the **Daco dataset**, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) \( 2k\) automatically generated query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. Experiments show that while LLMs like GPT-4 exhibit promising data analysis capabilities, they are still evaluated as less helpful than human-written analysis on 58.1% cases. Leveraging our weak supervision data, we experiment with various fine-tuning methods, including supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Our trained model outperforms existing baselines for table question answering, and RLHF further boosts the helpfulness of generated analysis on 58.5% cases. Data and code are released at https://github.com/shirley-wu/daco.

## 1 Introduction

Data analysis is the process of systematically applying statistical and logical reasoning to comprehend data and derive insights. Existing literature on table question answering has investigated answering queries about information given by structural data (_e_.\(g\)., tables) . However, they either focus straightforward factual retrieval or short-form arithmetic resolutions over retrieved entities while real-world data analysis can involve more complex analytical processes.

Take the scenario in Figure 1 as an example: a user is investigating potential age discrimination of a shop. To effectively answer queries such as this one, a chain of mathematical and logical reasoning and interacting with the data is required. For instance, _finding 1_ is inferred from the membership data ('member' table) through mathematical and analytical reasoning, while _finding and suggestion 2_ are derived from both'member' and 'happy_hour_member' tables through mathematical and strategic reasoning. These rigorous quantitative analyses eventually conclude the opposite to the user's hypothesis. As valuable as the conclusive suggestions such comprehensive analysis can bring, the extensive labor-efforts, hinted by these examples, can hinder the efficiency of gaining intelligencefrom the data in a competitive business environment. It is thus imperative to devise a system that is able to automate the aforementioned data analysis process.

To this end, we introduce a new dataset for this challenging task, Daco, **d**ata analysis via **code** generation. Daco is constructed from a set of diverse real-world databases associated with curated user queries. In light of the previously described labor-intensive challenge, we propose to leverage LLMs with a multi-turn chained prompts to automatically curate the analytical answers for each query. Specifically, our designed framework employs the code generation capabilities of GPT-4  for automating the statistical analysis, interleaved with its ability to interpret the obtained quantitative results. The Daco dataset contains \(440\) databases and \(1,942\) associated user queries, where each query is annotated with an average of \(3.3\) coding steps and \(1.9k\) lines of code during intermediate steps and a final output of \( 10\) bullet points. This resource can be used for both model fine-tuning and evaluation. To provide a refined benchmarking resource, we curate a high-quality test set through comprehensive human annotations on a subset of 100 samples. Detailed statistics are in Table 1.

We evaluate three types of methods on this new dataset: (1) existing methods designed for table QA tasks, (2) prompt-based LLMs, and (3) fine-tuned LLMs. We use **helpfulness** as our main evaluation metric, assessed through pair-wise comparison. We observe that proprietary LLMs such as GPT-4 demonstrate strong data analysis capabilities and significantly outperform table QA models. However, they still fall short of human performance by 58.1% in pair-wise evaluations. Regarding fine-tuned LLMs, via supervised fine-tuning (SFT) using automatically generated annotations that include both code generation trajectories and the final answers. Inspired by the recent success of reinforcement learning from human feedback (RLHF) [29; 35; 4; 5; 48; 38], we employ RLHF to further align the SFT models with human preferences towards _helpful_ data analysis. Our SFT model exhibits promising data analyais capabilities and outperforms table QA baselines despite lagging behind proprietary LLMs. On top of SFT, RLHF further enhances the output helpfulness in 58.5% of cases.

In summary, our contributions are as follows: (1) We explore the challenging task of data analysis, where we construct the Daco dataset with our proposed multi-turn prompting technique on a diverse set of real-world databases. (2) We curate a human-refined evaluation set for benchmarking models. (3) We evaluate a diverse set of models on this challenging dataset, including existing models designed for table QA, prompt-based LLMs, and fine-tuned LLMs with both SFT and RLHF. (4) Our dataset and code are made publicly available at https://github.com/shirley-wu/daco.

## 2 Task Formulation

As shown in Figure 1, the input to our task consists of a database \(\) and a query \(\), where the database

Figure 1: **Task overview. Given a user query driven by an application scenario, a data analysis system should produce an answer containing findings and suggestions based on the database. This requires the system to perform mathematical, logical and domain-specific reasoning, which can be done through invoking external tools such as Python libraries. In this example, _finding 1_ is inferred from analyzing age distribution within the membership data (‘member’ table) through mathematical reasoning and analytical reasoning. _Finding 2_ is inferred by comparing the ages of the happy hours participants (using ‘member’ and ‘happy_hour_member’ tables) through mathematical reasoning, and _suggestion 2_ is further derived by relating the data to coffee shop business setting through strategic reasoning.**

\(\) is a relational database containing multiple named tables. The output \(\) is formatted as two lists: findings and suggestions.

Inspired by recent work on tool usage and LLM agent [40; 41; 23], we allow the LLM to invoke tools for multiple turns before producing the final output \(\). At the \(i\)-th turn, the _action_\(_{i}\) is generated by the LLM, and the _observation_\(_{i}\) is produced by the environment after executing the corresponding action \(_{i}\). While the action \(\) can involve various tools such as SQL executor or search engine, we utilize Python due to its extensive mathematical libraries and programming flexibility.

To evaluate the quality of generated data analysis \(\), we use **helpfulness** as the primary metric. Motivated by literature in the data analysis field , we define helpfulness as: (1) relevance to the query, (2) effective and insightful data interpretation, and (3) diversity in terms of analysis perspectives. We evaluate helpfulness through **pairwise comparison** following common approach [29; 38; 45]. Given two analyses generated by two different systems, the annotator (either human or simulated by LLM) selects the more helpful one based on our defined criteria. The winning rate of each system is reported as helpfulness score. To obtain a comparable set of numbers for all models, we report the winning rate of each model against the annotations, so a score of 50 would indicate the model generations are perceived as helpful as annotations.

## 3 Dataset Construction

We construct our Daco dataset through four stages: (1) database collection, (2) query collection, (3) automatic annotation collection, and (4) human refinement. The workflow is shown in Figure 2(a).

**Database collection.** We collect databases from two sources: Spider  and Kaggle (https://www.kaggle.com/datasets). There are 157 databases collected from Spider, which originally come from university databases, DatabaseAnswers and Wikipedia. We then crawl and filter 5,830 databases from Kaggle. Despite the initial filtering, the quality of these databases remains low, often containing noisy or unintelligible data. Therefore, we conduct a manual inspection and thereby select a subset of 314 clean and interpretable databases to build our dataset. To maintain the diversity of the resulting database set, 157 of the databases are deliberately chosen from the long tail of the topic distribution. We employ BERTopic  to model the topic distribution, which produces in total 160 topics. We take its least frequent 80 topics as the long tail, which covers 26.79% of the total databases.

**Query collection.** We generate 10 queries for each database by prompting ChatGPT to first assume the role of a database stakeholder and then generate an application-driven query based on the role. To ensure the quality of the query, we perform a manual filtering to the machine generated queries. Specifically, we remove queries that are not driven by real-world applications or cannot be answered by the given reference database. We train a group of 6 annotators to perform such a filtering process. As a result, there are about 42% of the queries removed, where the removal agreement achieves a 0.62 cohen kappa score. After the aforementioned processes, we obtain in total 2,664 queries. Examples of databases and the automatically generated queries are shown in Figure 8.

**Automatic annotation collection.** As shown in Figure 2(b), we design a pipeline that leverages the code generation capability of LLMs to automate the answer annotation for our Daco dataset. Based on the database and the query, we instruct the LLM to perform data analysis in multiple turns. At each turn, the LLM will produce a python code snippet and take its execution outputs as evidences to reason over and support its follow-up interpretation. After each turn, we prompt the model to decide whether the analysis is sufficiently comprehensive; if deemed sufficient, it terminates the coding turns and produces the final answer. With this pipeline, we instruct GPT-4 to automatically generate all the answer annotations to each query of our dataset, for both the immediate code and the final analysis answering the queries. To improve the quality of such automatically constructed annotations, we additionally allow GPT-4 to correct its own mistakes when its generated code leads to run-time or syntax error, where only the corrected codes are kept. In total, we obtain 1.9k valid query-answer pairs, each with roughly 3.3 intermediate coding steps.

**Human refinement.** The annotated analyses thus far have been algorithmically generated, where their actual quality are to be further verified. We thus curate a human-refined subset containing 100 densely human-annotated query-answer pairs as illustrated in Figure 2(c). For each query, we sample 
Figure 2: An overview of Daco construction.

3 different analysis candidates using the previously described automated method with GPT-4. We ask the annotators to evaluate the quality of each machine generated bullet point and categorize each point into one of _not helpful_, _borderline helpful_, or _very helpful_. The highest quality points from the three candidates were combined and refined into a final gold-standard analysis. Concretely, the annotators should first combine all _very helpful_ points, remove duplicate points, reorder the points to maintain coherence, and make necessary textual edits for fluency. Suppose the number of bullet points are lower than our pre-defined lowest threshold (3 bullet points per answer), the annotators should select additional bullet points ranked as _borderline helpful_ to augment the answer. we ask a group of 3 internal members to perform refinement. The agreement accuracy of the refinement process (candidate point selection) is 0.83 and the Cohen's Kappa is 0.67.

## 4 Data Statistics

Daco dataset includes training, development and test sets with annotations generated by GPT-4. Furthermore, Daco features a human-refined testing subset. To differentiate the two test sets, we denote the automatically annotated set as Test\({}^{A}\) and the human-refined set as Test\({}^{H}\). Detailed statistics are in Table 1.

**Database statistics.** In total, Daco comprises 440 databases, each of which contains on average 2.3 tables. To better visualize the _major topic_ distribution of this selected subset, we again use BERTopic but group these databases into 10 topics. The keywords for top 5 topics are shown in Figure 3. The leading topic (topic 1) is associated with business setting and consists of 46.52% of the dataset. The remaining nine topics exhibit a relatively even distribution, covering a broad range of domains, including sports (topic 2), healthcare (topic 3), weather (topic 4), and education (topic 5).

**Query statistics.** In average, each database is accompanied by 4.4 different user queries. We show the top 15 verbs and their top 3 direct noun objectives in Figure 4. The queries demonstrate a notable level of diversity. The most common type of queries is to request analysis (such as "analyze data" and "identify pattern"), followed by queries aiming to make decisions (such as "determine strategy" and "make decision"). To quantatively verify the diversity of input queries, we measure the overlap between queries over the same database using cosine similarity of Sentence-BERT  embeddings. Based on manual inspection of various cases, we categorize the overlap between query pairs into _low_, _medium_, and _high_ at thresholds of 0.5 and 0.8. Representative examples are shown in Table 2. We find that 45.6% of generated query pairs have low similarity, 52.4% have medium similarity, and only 2.0% are highly similar. The small percentage of highly similar pairs suggests high diversity of input queries.

**Annotation statistics.** Figure 5 shows the distribution of the top 10 API functions invoked in the generated code. Excluding the trivial print function, the most frequent APIs are pandas APIs including table manipulation (e.g. groupby and merge) and mathematical computation (e.g. mean

   & Train & Dev & Test\({}^{A}\) & Test\({}^{H}\) & Total \\  _Input Statistics_ & & & & & \\  \# db & 353 & 22 & 65 & 17 & 440 \\ \# queries & 1558 & 100 & 284 & 100 & 1942 \\  _Annotation Statistics_ & & & & & \\  \# bullets & 14.8\(k\) & 996 & 2728 & 980 & 19.5\(k\) \\ \# tokens & 575\(k\) & 36.6\(k\) & 106\(k\) & 42.3\(k\) & 760\(k\) \\ \# code steps & 5086 & 346 & 948 & - & 6380 \\ \# code lines & 3.0\(M\) & 208\(k\) & 555\(k\) & - & 3.7\(M\) \\      & Med. & Max & Min \\   \\  \# tables & 1 & 15 & 1 \\ \# columns & 15 & 96 & 3 \\ \# rows & 572 & 67.2\(k\) & 4 \\   \\  \# findings & 5 & 8 & 3 \\ \# suggestions & 5 & 8 & 3 \\ \# tokens & 397 & 864 & 202 \\   
   & Med. & Max & Min \\   \\  \# tables & 1 & 15 & 1 \\ \# columns & 15 & 96 & 3 \\ \# rows & 572 & 67.2\(k\) & 4 \\   \\  \# findings & 5 & 8 & 3 \\ \# suggestions & 5 & 8 & 3 \\ \# tokens & 397 & 864 & 202 \\  

Table 1: **Statistics of Daco dataset. Left:** We report the size of each data split, including the number of databases (# db), the number of queries (# queries), the number of bullet points (# bullets) and tokens (# tokens) in output answer y, and the number of code steps and code lines in intermediate coding steps a. Train, Dev and Test\({}^{A}\) sets are automatically generated with GPT-4, while Test\({}^{H}\) is the human refined subset and only contains the final output answer. **Right:** We report more fine-grained statistics regarding the input databases and output answers. For all databases in Daco, we report the number of tables, columns and rows within each database. The numbers of rows and columns for each database are calculated by summing across all tables within the database. For output answers in Test\({}^{H}\), we report the number of findings, suggestions and tokens in each answer.

Figure 3: **Domain distribution of databases in Daco dataset.** Upper section: distribution of the 10 topics of databases, showing a long-tail effect. Lower section: keywords of the top five topics explaining the topics’ content.

Figure 4: **Distribution of queries in Daco dataset.** We display the top 15 verbs and their top 3 direct noun objectives, demonstrating the query diversity.

  Overlap & \% & Representative Examples \\  Low & 45.6 & _Query 1:_ As an advertising executive, I want to select the channels for targeted ad placements. \\  & & _Query 2:_ As a platform developer, I want to analyze user behavior and preferences to optimize the user experience. \\  Medium & 52.4 & _Query 1:_ As a farmer, I want to determine the suitable fruit varieties to grow on my farm. \\  & & _Query 2:_ As a fruit reporter, I want to identify the fruits that meet export standards and have a longer shelf life. \\  High & 2.0 & _Query 1:_ As a consultant for honey market, I want to study the honey production trend to recommend business strategies for my clients. \\  & & _Query 2:_ As a curious analyst, I want to study the production trend to understand the US honey industry. \\  

Table 2: Percentage and representative examples of query pairs with low, medium and high overlap. In the representative examples, repetitive information and distinctive information are highlighted.

and sort_values). Figure 6 and 7 displays the human rating for each bullet points in Test\({}^{A}\) and Test\({}^{H}\) collected during the human refinement stage. While automatic annotations significantly lag behind manually refined ones, they still offer valuable knowledge that can be leveraged through supervised fine-tuning, as demonstrated in the experiments section. For manually refined annotations, mentioned in Section 3, the agreement accuracy is 0.83 and Cohen's Kappa is 0.67, verifying their quality. An example of human refinement is shown in Figure 2.

## 5 Experiments

### Experimental Settings

**Table QA baselines.** We experiment with two models designed for table QA tasks, TAPAS  and TAPEX . TAPAS is a BERT-style model pre-trained to select relevant information from a table based on user query. For our dataset, we first use TAPAS to select relevant information and then use ChatGPT to interpret the selected information. TAPEX is a encoder-to-decoder model pre-trained on table and SQL data. We fine-tune TAPEX with GPT-4-generated annotations.

**Prompt-based LLMs.** We evaluate the performance of ChatGPT and GPT-4 when enhanced by code generation. For comparison, we also experiment with a baseline counterpart that does not include code generation and instead directly takes raw table content as input.

**Fine-tuned LLMs.** With the answer annotation generated by GPT-4, we train a 6B CodeGeeX2-6B  model through SFT, RLHF, and fine-grained RLHF (denoted as FG-RLHF) .

We begin by training the model with SFT, using either all annotations including code generation or purely the final answer annotations. The former SFT model is further refined through RLHF with an end goal of optimizing the final answer **y**'s helpfulness. We model the helpness of final answer with an _answer RM_\(R_{a}\). Concretely, \(R_{a}\) is trained on pairwise comparison data of output bullet points annotated by ChatGPT. To encourage diversity, we additionally impose repetition penalty by subtracting a similarity score between different bullet points from the final reward.

However, we observe that RLHF struggles to provide useful supervision signal to intermediate code generation since its reward is only applied to the final answer. Recent work has shown that providing dense reward for the intermediate steps can alleviate this issue [18; 38]. Thus, we propose to use fine-grained RLHF , where we introduce two novel reward models to provide dense reward signals for code generation: _contribution RM_\(R_{c}\) and _regularization RM_\(R_{r}\). \(R_{c}\) encourages code generation that better contributes to the final answer at each step, while \(R_{r}\) helps prevent reward hacking. Concretely, we compute the similarity \(Sim(,_{i})\) between final answer **y** and the \(i\)-th step observation \(_{i}\) to measure the contribution of generated code action \(_{i}\). Using \(Sim(,_{i})\), we can rank the contribution of different steps, and then use the comparison pairs \((_{i},_{j})\) to train \(R_{c}\). However, this heuristic training of \(R_{c}\) may lead to the well-studied reward misspecification issue termed reward hacking . To mitigate this issue, we propose to regularize such behavior with \(R_{r}\). Given the misspecified reward model \(R_{c}\), we first train an RL model until its generations start to collapse to certain patterns, then we train \(R_{r}\) to detect such patterns and thus assign lower scores.

**Evaluation.** The main metric we use is **pairwise comparison of helpfulness** as in Section 2. We use three LLM evaluators, GPT-4o mini, Claude 3.5 Sonnet, and Llama 3 8B , as well as trained human annotators for the evaluation. We additionally report BLEU score, entailment score, and helpfulness evaluation for each individual bullet point. These metrics cannot holistically measure the analysis helpfulness, but can provide complementary insights for analyzing model performance. For entailment, we use an off-the-shelf NLI model to compute the probability that the model generation is entailed by the annotation. For point-wise evaluation, we ask the annotator to assign a score chosen from 0 (_not helpful_), 1 (_borderline helpful_) and 2 (_very helpful_) to each bullet point using the same standard as in human refinement of test set. Our human annotation achieves high agreement of 0.62 Cohen's kappa for pairwise comparison of helpfulness, and 0.65 Cohen's kappa for point-wise helpfulness evaluation.

### Results

The main results are in Table 3. The key conclusions are as follows:

**Proprietary LLMs demonstrate strong data analysis capabilities but still lag behind human performance.** Proprietary LLMs, especially GPT-4, perform the best and significantly outperform other models on almost all metrics, particularly when enhanced with code generation. However, pairwise comparisons between model generation and human-refined annotations (Test\({}^{H}\)) show that even the best model only wins 41.88% of the time, indicating a gap in generating helpful data analysis.

**The fine-tuned models show promising data analysis capabilities, demonstrating the usefulness of our weak supervision data.** By training with automatically generated annotations, SFT with code generation achieves a reasonable helpfulness score and outperforms the TAPEX baseline. Though RLHF negatively affects performance due to sparsity of reward signals, FG-RLHF with our dense reward models significantly improves the performance, outperforming SFT by 7 points in helpfulness. Despite the difference in model size, FG-RLHF outperforms ChatGPT without code generation in helpfulness and entailment metrics. Human evaluation shows a 57.72% win rate of FG-RLHF over SFT, as seen in Table 4. Our qualitative analysis indicates that FG-RLHF better focuses on user queries, while SFT tends to display generic statistics less relevant to user queries.

**The fine-tuned models show promising data analysis capabilities, demonstrating the usefulness of our weak supervision data.** By training with automatically generated annotations, SFT with code generation achieves a reasonable helpfulness score and outperforms the TAPEX baseline. Though RLHF negatively affect the performance due to the sparsity of reward signal, we see that FG-RLHF with our designed dense reward models significantly boosts the generation helpfulness, and outperforms SFT by 7 points on helpfulness, thus better aligning with human preference. Despite the difference in model size, FG-RLHF outperforms ChatGPT w/o code generation on helpfulness and entailment metrics. Human evaluation demonstrates a 57.72 win rate of FG-RLHF over SFT as in Table 4. Our qualitative analysis shows that FG-RLHF better focuses on user query, while SFT tends to display generic statistics that are less relevant to user query.

We analyze the behavior of two reward models to better understand their effects on FG-RLHF. The _contribution RM_ favors API calls that extract important information from tabular data but is vulnerable

   &  &  \\  & Human & LLM & wise \\  GPT-4 code gen _v.s._ & **66.41** & **70.07** & **1.45** \\ GPT-4 w/o code gen & 33.59 & 29.93 & 1.36 \\  FG-RLHF _v.s._ & **57.72** & **58.49** & **1.42** \\ SFT & 42.28 & 41.51 & 1.30 \\   
   &  \\  API & Corr. & API & Corr. \\  print & 44.24 & to\_datetime & -18.96 \\ largest & 20.06 & insull & -17.76 \\ mean & 14.56 & describe & -12.02 \\ sort\_values & 12.23 & merge & -10.83 \\  

Table 4: **Human evaluation. We report human-rated and LLM-rated helpfulness pairwise comparison of two pairs of models: GPT-4 with v.s. without code generation, and FG-RLHF v.s. SFT. We also report point-wise evaluation scores scaled into \(0 2\) rated by human annotators.**

   & & & & ^{A}\)} & ^{H}\)} \\  & Method & \# para. & Code gen & Help. & Entail. & BLEU & Help. & Entail. & BLEU \\  _TableQA_ & TAPAS & 337M & ✗ & **19.19** & 1.96 & 11.62 & **16.50** & **3.67** & 9.73 \\ _Baselines_ & TAPEX & 406M & ✗ & 15.08 & **3.34** & **14.60** & 9.00 & 3.50 & **13.81** \\   & ChatGPT & 20B\({}^{}\) & ✗ & 19.31 & 3.06 & 13.22 & 13.50 & 2.07 & 13.51 \\  & GPT-4 & 175B\({}^{}\) & ✗ & 30.43 & 3.35 & 14.90 & 20.50 & **4.36** & 13.71 \\  & ChatGPT & 20B\({}^{}\) & ✓ & 26.51 & 2.74 & 14.22 & 21.38 & 2.59 & 14.51 \\  & GPT-4 & 175B\({}^{}\) & ✓ & **50.79** & **4.59** & **17.77** & **43.92** & 3.26 & **17.54** \\   & SFT & 6B & ✗ & 18.96 & 2.30 & 14.47 & 11.33 & 2.65 & 13.63 \\  & SFT & 6B & ✓ & 13.73 & 2.15 & **14.88** & 9.83 & 4.47 & **14.60** \\   & RLHF & 6B & ✓ & 10.64 & 3.18 & 12.66 & 7.51 & 3.13 & 11.46 \\   & FG-RLHF & 6B & ✓ & **19.42** & **3.65** & 13.13 & **12.50** & **5.98** & 11.80 \\  

Table 3: **Main results. We report helpfulness (Help.), entailment (Entail.), and BLEU on both automatically annotated test set (Test\({}^{A}\)) and human curated test set (Test\({}^{H}\)). The helpfulness score is the average of scores evaluated by GPT-4o mini, Claude 3.5 Sonnet, and Llama 3 8B. Highest numbers in each section are highlighted in bold. We also report the number of parameters (# para.) of each model. \(\): For ChatGPT and GPT-4, we report the number of parameters based on our best estimation.**to reward hacking. As in Table 5, the functions rewarded most are related to extracting significant features (nlargest, sort_values), aggregating results (mean), and displaying specific information (print). In contrast, the least rewarded functions involve displaying generic statistics (describe) and wrangling data (merge, to_datetime, is_null) since they cannot directly contribute to the user query. However, the concerningly high correlation between print function and contribution RM scores indicates the policy may exploit the correlation to lack reward, which can be mitigated by employing the regularization RM.

**Evaluation on external test sets.** We further evaluate our fine-tuned models on two external test sets: (1) InfiAgent-DA benchmark that focuses on complex but not application-driven data analysis , and (2) free-form table question answering dataset FeTaQA . We find that FG-RLHF improves the accuracy over SFT on InfiAgent-DA (14.61 v.s. 12.92), especially over questions about summary statistics (14.86 v.s. 10.80) and correlation analysis (21.57 v.s. 14.86), which aligns with our evaluation results on FG-RLHF dataset. On FeTaQA, FG-RLHF retains similar performance (6.35 Rouge-L, 80.74 BERTScore) compared to SFT (6.39 Rouge-L, 80.68 BERTScore) since FG-RLHF is not specifically trained to enhance information lookup capabilities.

## 6 Related Work

**Table Analysis.** Early work in table question answering (table QA) targets simple questions that requires table lookup and cell aggregations . Later benchmarks further require free-form answer generation , multi-hop reasoning  and mathematical reasoning . Despite the similar formulation between our task and existing table QA work, their focus are different: most existing table QA datasets focus on obtaining specific information, our data analysis queries can be complex and requires query decomposition and reasoning. Some concurrent work further targets comprehensive table analysis such as correlation analysis and causal reasoning . The main difference between this work to the concurrent work is our focus on addressing application-driven and complex user queries, which requires more reasoning skills.

**Code Generation.** Code generation benchmarks have been proposed for general-purpose programming , math problems , and data science scenario . Similar to our work, some recent work allows the language model to interact with a code execution environment and receive execution outputs as feedback . The most relevant work is  that also addresses data analysis via code generation. Given a data analysis query, they use GPT-4 to first generate code and then provide an interpretation of the execution results. While their analysis queries are still relatively simple, this is an early exploration aiming at automating data analysis.

**LLM Agent.** The notable capability of LLMs in reasoning and planning inspires many work to use them as intelligent _agents_ for complex tasks like Minecraft gaming , robotics control , and web browsing . In this work, our code generation pipeline can be considered as an adaptation of ReAct , where the LLM iteratively generates code as actions and reads execution results as observations. Although we have not yet explored more advanced agent designs, such as tool-crafting agents  or self-reflection agents , these approaches could be adapted to our task by redefining the action space as code generation and the observation space as execution results. We leave this exploration for future work.

## 7 Conclusion

In this work, we propose a novel and challenging data analysis task, which involves decomposing user query into multiple perspectives, grounding each perspective to the input data and performing logical and mathematical reasoning. To support this task, we build the Daco dataset containing large-scale annotations automatically generated by GPT-4 and a small but high-quality test set with human curated annotations. We evaluate three types models on our dataset: table QA models, prompt-based proprietary LLMs, and open LLMs fine-tuned on our automatically collected annotations. While GPT-4 consistently performs the best, the fine-tuned models achieves reasonably good helpfulness with much less computation. On top of the SFT model, we further show that fine-grained RLHF can be employed to boost helpfulness perceived by humans.