# LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding

Haoning Wu Dongxu Li Bei Chen Junnan Li

Rhymes AI

Corresponding Author. Dataset and leaderboard at: https://longvideobench.github.io

###### Abstract

Large multimodal models (LMMs) are processing increasingly longer and richer inputs. Albeit the progress, few public benchmark is available to measure such development. To mitigate this gap, we introduce **LongVideoBench**, a question-answering benchmark that features video-language interleaved inputs up to an hour long. Our benchmark includes 3,763 varying-length web-collected videos with their subtitles across diverse themes, designed to comprehensively evaluate LMMs on long-term multimodal understanding. To achieve this, we interpret the primary challenge as to accurately _retrieve_ and _reason over_ detailed multimodal information from long inputs. As such, we formulate a novel video question-answering task termed _referring reasoning_. Specifically, as part of the question, it contains a _referring query_ that references related video contexts, called _referred context_. The model is then required to reason over relevant video details from the referred context. Following the paradigm of referring reasoning, we curate 6,678 human-annotated multiple-choice questions in 17 fine-grained categories, establishing one of the most comprehensive benchmarks for long-form video understanding. Evaluations suggest that the LongVideoBench presents significant challenges even for the most advanced proprietary models (_e.g._ GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their open-source counterparts show an even larger performance gap. In addition, our results indicate that model performance on the benchmark improves only when they are capable of processing more frames, positioning LongVideoBench as a valuable benchmark for evaluating future-generation long-context LMMs.

Figure 1: **(Left)** LongVideoBench features _referring reasoning_ questions, with a _referring query_ that references particular video contexts (_i.e. referred context_) to answer questions about. **(Right)** For top-tier models, LongVideoBench requires more input frames to obtain better performance.

Introduction

Recent foundation models are processing inputs of longer contexts, with a growth from 2K tokens as in LLaMA (Touvron et al., 2023), to 128K as in GPT-4 (OpenAI, 2024a) and further into millions in models like Gemini-1.5-Pro (Team, 2024). To measure such development, most benchmarks focus on text-only inputs (Hsieh et al., 2024; Wang et al., 2024; gkamradt, 2024), while those for long multimodal context remain lacking. In this regard, the task of understanding long-duration videos, such as those extending up to hours, is considered a promising testbed. However, existing video benchmarks exhibit strong single-frame bias. Namely, their results do not improve even models can process more frames. This longstanding issue has continued to be a pain in the neck for video understanding, making evaluation of long-context multimodal inputs a significant challenge.

To address this challenge, this work introduces **LongVideoBench**, a video understanding benchmark that measures the progress of LMMs in processing hour-long subtitled videos. In contrary to findings from previous benchmarks, we observe consistent performance improvements when an LMM is capable of processing a larger number of frames (Fig. 1 (b)). To achieve this, we begin by identifying two capabilities essential for long-context multimodal understanding. First, akin to the needle in a haystack (NIAH) evaluation for text LLMs (gkamradt, 2024), effective LMMs must be adept at perceiving specific multimodal details in response to user queries, a task that becomes harder with longer input lengths. Second, in addition to recalling specified elements, the model must be able to relate them and reason about them coherently and contextually. This challenges the model to interpret and integrate large volumes of multimodal information meaningfully.

To effectively evaluate these abilities, we design _referring reasoning_ (Fig. 1 (a)) as the foundation task for our benchmark. In particular, this task initially introduces a _referring query_. It references particular video contexts, which are termed the _referred context_. Subsequently, the model is presented with a question related to this referred context. This question tests the model's multimodal understanding capabilities, such as visual perception and relational reasoning. To achieve good performance in referring reasoning, models have to interpret the referring query and accurately recall the referred context from the long-context inputs. In addition, they need to perform complex multimodal reasoning. These challenges are closely aligned with the required capabilities as outlined previously.

Following the task of referring reasoning, the LongVideoBench contains 6,678 multiple-choice questions on 3,763 videos. These videos are diverse in their themes, including movies, news, life and knowledge, covering 4 progressive duration groups: _8-15 seconds_, _15-60 seconds_, _3-10 minutes_, and _15-60 minutes_, making LongVideoBench widely relevant for real-world video applications. Videos are also accompanied with original or transcribed subtitles, which challenges the model to understand long-context interleaved multimodal inputs.

We incorporate _perception_ and _relation_ questions in the benchmark. Specifically, perception questions require the model to perceive visually on an individual referred video scene, such as to recognize objects, attributes and events. In contrast, relation questions require the model to associate multiple scenes within the referred context, and answer questions about their temporal ordering, attribute change or to track referred objects. These questions are further divided into 17 fine-grained categories, with human-annotated choices, covering a wide range of video understanding tasks.

Our contributions are summarized in three-fold:

1. We introduce LongVideoBench (Tab. 1), a multi-choice question-answering benchmark for long-context multimodal video understanding. Our benchmark consists of 6,678 human-crafted comprehensive questions posed on vary-length videos up to an hour long on diverse themes, widely relevant for video understanding applications in the wild.
2. We propose the task of _referring reasoning_ to effectively address the longstanding issue of single frame bias in video understanding metrics. As a result, models have to be capable of processing effectively more frames, longer multimodal inputs to improve performance. This requirement distinguishes LongVideoBench from existing video benchmarks;
3. We evaluate comprehensively the proprietary and open-source models to understand their long-context multimodal modeling capabilities. Our results demonstrate significant challenges posed by LongVideoBench. In addition, the evaluation results show intriguing insights into deficiencies of existing models, thereby offering valuable directions for future research on multimodal long-context understanding.

## 2 The _Referring Reasoning_ Task

In this section, we first identify the primary challenges for multimodal long-context understanding. To reflect these challenges, we further define _referring reasoning_, the foundational task for LongVideoBench. We introduce its general task scheme and specific categories as follows.

**Challenges for the LongVideoBench.** Similar to challenges identified in text-only long-context benchmarks , the LongVideoBench designs question-answering tasks to reflect the following two major difficulties in understanding long videos:

First, **retrieving details** from long videos. Existing studies  notice that LLMs or LMMs often struggle to extract specific details from long sequences. To accurately assess this capability in the domain of long videos, the tasks in LongVideoBench demand a focus on granular details such as _objects, events_, or _attributes_, rather than a summary or topic overview.

Second, **reasoning contextual relations** in long videos. According to Hsieh et al. , beyond mere retrieval, it is significantly challenging for models to reason about the relationships among extensive inputs. Questions in LongVideoBench are therefore designed to compel LMMs to analyze the interconnections among diverse content within a long video to derive the correct answer.

**General Scheme for _Referring Reasoning_.** To effectively measure model performance against aforementioned challenges, we establish the _referring reasoning_ task as the fundamental paradigm for LongVideoBench. Each question begins by describing a _referring query_, pinpointing one or multiple moments from the video. These video moments, composed of frames and subtitles, are denoted as _referred context_. A specific question body follows the referring query, which requires the model to reason over the referred context to deduct the answer. We employ the multiple-choice question format, where several distracting options are provided alongside the correct answer option.

**Two Levels: Perception _and_ Relation.** We divide _referring reasoning_ questions into two levels. In **(L1) Perception**, the referring query references a single moment of the video. Then, a question body is posed to ask about the visual perception of a specific concept in the referred moment, such as object, action, or event. (L1) questions mainly challenge models on locating the referred context from the long inputs and understand its visual information. In **(L2) Relation**, the referred context spans across multiple moments of the video. These moments are either related with a specific sequential

 
**Benchmark** & _Labels_ & _#Eval Videos_ & _#Eval QAs_ & _Avg Duration (s)_ & _Theme Category_ & _Interleaved?_ \\  MSVD-QA  & Auto & 520 & 13,157 & 10 & Everyday Life & ✗ \\ MSSVT-QA  & Auto & 2,990 & 72,821 & 15 & Everyday Life & ✗ \\ ActivityNet-QA  & Human & 800 & 8,000 & 180 & Everyday Life & ✗ \\ NeXT-QA  & Human & 1,000 & 8,564 & 44 & Everyday Life & ✗ \\ MVBench  & Auto & 4,000 & 4,000 & 16 & Life, Human Action, Movie & ✗ \\ EggSchema  & Auto & 5,031 & 5,031(r) & 180 & Life, Human Action & ✗ \\ MovieChat-1k  & Human & 130 & 1,950 & 500 & Movie & ✗ \\  LongVideoBench **(ours)** & Human & 3,763 & 6,678(r) & 473 & Life, Movie, Knowledge, News & ✓ \\  

Table 1: The LongVideoBench and popular benchmarks for video LMMs. The \({}^{(HT)}\) denotes the benchmarks split test sets with hidden answers to avoid contamination.

 
**Level** & **Task** &  &  & **Code** & \# \\   & Scene-segmented Event & a scene & an event that happens in **Q** & S2D & 410 \\  & Scene-segmented Object Existence & a scene & an object that exists in **Q** & S2D & 403 \\  & Scene-segmented Object Attribute & a scene & an attribute of \(_{2}\) in \(_{1}\) & S2A & 403 \\
**Creption** & Event-segmented Object & an event & an object that participates **Q** & E2D & 393 \\ (L1, 2304) & Object-segmented Event & an object & an event which **Q** & **Q** appears & O2E & 401 \\  & Text-segmented Event & a subtletic & an event concurrent with **Q** & T2E & 398 \\  & Text-segmented Object Existence & a subtletic & an object that exists while **Q** & T2D & 387 \\  & Text-segmented Object Attribute & a subtle & “at-an object & “u & an attribute of \(_{2}\) while \(_{1}\) & TAA & 402 \\  & Event before/Agte Event & an event that happens before/Agte Event & S2E & 406 \\  & Object before/Agte Event & an object & an object that appears before/Agte & Q & O3D & 394 \\  & Sequence of scenes & multiple scenes & the sequential order among **Q** & SSS & 398 \\  & Scene-segmented Object Tracking & a scene & “at-an object” & another scene that \(_{1}\) appears & S0S & 381 \\
**Relation** & Scene-segmented Object Attribute Change & two scenes & “at-an object” & attribute change of \(_{1}\) from \(_{1}\) to \(_{2}\) & SAA & 375 \\
**(L2, 374)** & Event before/Agte Text & a subtletic & an event that happens before/Agte & Q & TSE & 401 \\  & Object before/Agte Text & a subtle & an object that appears before/Agte & Q & TSO & 391 \\  & Text-segmented Object Attribute Change & a scene & “at-an object” & subtitle at \(_{3}\)’s appearance other than \(_{1}\) & TOS & 380 \\  & Text-segmented Object Attribute Change & two subtleles & “at-an object” & attribute change of \(_{3}\) from \(_{1}\) to \(_{2}\) & SAA & 348 \\  

Table 2: Definition of 17 categories of _referring reasoning_ questions in the LongVideoBench.

order (before/after/concurrent) or containing the same concept (_e.g._ the same object appears in these moments). The question is then posed regarding the relations of the moments, and answering these questions require models to not only locate the referred moments, but further reason over their relations. This makes (L2) questions in general more challenging than (L1) questions.

**17 Finer-grained Question Categories.** We further subdivide the two levels of questions into 17 finer-grained categories, dividing based on the type of referring query and the type of target answer. As listed in Tab. 2, given interleaved multimodal inputs, the referring query could either be describing a scene, an event, or an object from the video frames, or be narrating a sentence or a phrase from the text subtitles. The target answer typically is about a visual concept (an event, object, or attribute) from one of the referred moments, with two exceptions: the Sequence of Scenes (SSS) category requires to answer the correct sequential order of multiple (\(>3\)) scenes in the video, and the Text-referred Object Tracking (TOS) requires to answer the specific subtitle while a given object appears.

## 3 Dataset Construction

In this section, we discuss the dataset construction for the LongVideoBench. We first define the category and duration groups of videos (Sec. 3.1), then we introduce the process of collecting and creating interleaved video-subtitle data (Sec. 3.2), lastly we elaborate on the human annotation process to collect high-quality referring questions and answers for LongVideoBench (Sec. 3.3).

### Groups of Videos

**Progressive Duration Groups.** In LongVideoBench, we aim to not only evaluate LMMs on ultra-long videos, but analyze how their ability changes from short videos (_about 10s_) to long (_hour-long_). In light of this, we propose to collect videos in four progressive duration groups, as listed in Tab. 3.1. The first two groups contain shorter videos of length _(8s, 15s]_ and _(15s, 60s]_, whereas the latter two duration groups contain long videos of length _(180s, 600s]_ and _(900s, 3600s]_. The four groups not

Figure 2: Examples of 17 categories of _referring reasoning_ questions in the LongVideoBench.

only cover the duration ranges of existing video understanding benchmarks, but also provide a unique hour-long subset to further expand the video length beyond existing benchmarks.

**Category Groups.** Existing LMM benchmarks for long videos typically focus on a specific category of videos, _e.g._ egocentric videos , or movies . In comparison, LongVideoBench is a more comprehensive benchmark that covers diverse categories of contents. The videos in LongVideoBench are collected from 99 different channels for landscape videos and 20 channels for portrait videos, in the 10 following categories: Movie Recaps (_MR_); three life-related categories: Travel Guides (_LT_), Life Vlogs (_LV_), Cooking/Recipes (_LC_); News Programs (_NP_); and five knowledge-related categories: Art (_KA_), History (_KH_), Geography (_KG_), STEM (_KS_), Computer Science (_KC_). As listed in Tab. 4, LongVideoBench includes a sufficient number of videos from all 10 category groups, spanning over a diverse distribution.

### Video and Subtitle Collection

The video and subtitle collection process is illustrated in Fig. 3. First, all videos with at least 720P resolution from the 119 channels are downloaded. After downloading the videos, for the source platforms that provide transcribed subtitles, we remove the videos without transcribed subtitles or with non-English subtitles. For those videos without provided transcribed subtitles, we employ Whisper-V3-Large  to generate subtitles for them. These videos are further sampled to cover different topics uniformly. Finally, we evaluate their video quality via Q-Align  and remove especially low-quality videos to ensure that all videos have scores \(>0.25\) (in range \(\)). Remaining videos are further manually filtered by annotators (in Sec. 3.3) to the final 3,763 videos.

Subtitles are important for multimodal video understanding, as they provide vital text information from human speech and reduce ambiguity from pure visual scenes. Aligning with the way humans watch videos with subtitles, in LongVideoBench, we require LMMs to receive the text subtitles simultaneously with concurrent frames. To achieve this, we define the **interleaved multimodal input** format to feed videos and subtitles together into LMMs as temporally-aligned multimodal sequences. Specifically, a chunk of subtitle will be inserted in-between the two frames before and after the mid-timestamp of the subtitle.

### Annotating Questions and Answers

We conduct the annotation process in a well-controlled lab environment with experienced annotators. Before annotation, we conduct a special training to all annotators for them to understand the requirements of each specific question category. During the annotation process, the subtitles are appended at the bottom of the video with aligned timestamps, and displayed to annotators. The annotator is required to watch the full video before starting the annotation, and is allowed to drag back to to any specific timestamps after full watching. We collect one question per video for videos longer than 60 seconds, two questions for videos in _(180s, 600s)_, three questions for _(900s, 3600s)_. The annotator also needs to provide 3-4 distracting answer options that are relevant to the question and the video.

   &  &  &  &  \\  &  &  &  &  &  &  &  &  \\ Source Platform & _LS_ & _PT_ & _LS_ & _PT_ & _LS_ & _PT_ & _LS_ & _PT_ & _LS_ & _LS_ & _LS_ & _LS_ & _LS_ & _LS_ \\  _GChannels_ & 18 & 4 & 10 & 6 & 7 & 5 & 8 & 5 & 12 & 5 & 8 & 7 & 10 & 7 \\ _#Downloaded Videos_ & 7679 & 1106 & 2230 & 2532 & 1009 & 1891 & 2731 & 4706 & 24002 & 2010 & 2900 & 1280 & 1350 & 335 \\ _#Aumatated Videos_ & 352 & 160 & 343 & 173 & 338 & 179 & 336 & 203 & 329 & 327 & 336 & 330 & 200 & 160 \\  

Table 4: Statistics of videos in LongVideoBench, by duration groups and video layouts.

Figure 3: Video collection for LongVideoBench, ensuring all videos have subtitles.

   &  &  &  &  \\   & Landscape &  & Landscape &  & Landscape &  & Landscape & Landscape & Landscape \\   & _Duration_ & _Videos_ & _Duration_ & _Videos_ & _Duration_ & _Videos_ & _Duration_ & _Videos_ & _Duration_ & _Videos_ \\   & 11.06 & 546 & 11.93 & 338 & 33.88 & 551 & 38.59 & 374 & 389 & 986 & 1408 & 966 \\  

Table 3: Statistics of videos in LongVideoBench, by duration groups and video layouts.

We further introduce two additional annotation requirements to ensure high-quality _referred reasoning_ questions: 1) We explicitly require annotators to include and highlight the referred query in all questions2; 2) To ensure that the referred context uniformly span over the video, we ask annotators to explicitly label the frame index for all referred moments in each question. This additional requirement further facilitates our in-depth study of long-context understanding abilities for LMMs with respect to the relative token-wise distance between the question and the referred context.

To control the annotation quality, each video is passed through three annotators: 1) The primary annotator, whose duty is to provide annotations and filter out videos that are not available for annotation (still frames, incomplete subtitles); 2) The examiner, who examines whether the annotated question is in the correct question category, and whether the annotation requirements are all met; 3) The reviser, to revise the annotations labeled as incorrect by examiners. The examiner and reviser have identified 20% of annotations to be problematic and revised them, which significantly improved the quality of the LongVideoBench.

As we require all questions to include the question body itself as well as a referring query, the average question length is as long as **43.53** words, ensuring that the referred context is clearly depicted in the question without introducing ambiguity. The average length of an answer is 8.28 words.

## 4 Evaluation of LongVideoBench

### Models and Evaluation Strategies

**Participating LMMs.** We include in total 22 LMMs for evaluation. The main participants are long-context LMMs, including four proprietary models: GPT-4o (gpt-4o-0513), Gemini-1.5-Pro (gemini-1.5-pro-0514), GPT-4-Turbo (gpt-4-turbo-0409), and Gemini-1.5-Flash (gemini-1.5-flash-0514), and four state-of-the-art open-sourced long-context LMMs: Phi-3-Vision-Instruct (_128K_), Idefics2 (_32K_), Mantis-Idefics2 (_32K_), and Mantis-BakLLaVA (_32K_). All these models above support interleaved video-language inputs. We also evaluate 9 representative video-specific LMMs, and 6 image LMMs that support \(\)8 images.

**Validation and Test Subsets.** We split the LongVideoBench into two subsets, the _validation set_ (752 videos, 1337 MCQs), and the _test set_ (3011 videos, 5341 MCQs). We use the _validation set_ to analyze the performance of LMMs under different settings. Afterwards, we pick the optimal setting for each LMM to report their performance on test set leaderboard.

### Main Results

In Tab. 5 and Tab. 6, we analyze the performance of six long-context LMMs under different settings on the val set of LongVideoBench. Our evaluation brings several important findings, as follows:

**1) _LMMs have to understand long inputs for better results._** As shown in Tab. 5 (a), (b), (c) and (d), all four proprietary models, especially more advanced GPT-4o and Gemini-Pro, have shown significant improvements while increasing their input length, in particular for long videos. For videos longer than 180 seconds, GPT-4o and Gemini-1.5-Pro can improve more than **10%** by increasing input length from 16 to 256 frames. In contrast, on EgoSchema, Gemini-1.5-Pro only improves 2.5% from 16 to 150 frames. This validates the effectiveness of LongVideoBench as a longstanding challenging benchmark for models to evaluate their long-context multimodal understanding abilities.

**2) _Open-source models lag significantly behind._** Different from proprietary models, open-source LMMs are unable to improve their results by inputting more than 16 frames. Idefics2 and Mantis-Idefics2, as shown in Tab. 5 (e) and (g), even face a severe degradation on accuracy with 64 input frames, before they have reached their context length limits.

**3) _Longer videos are more challenging._** As in Tab. 5, all six models show the lowest accuracy on the longest _(900,3600)_ group, followed by the _(180,600)_ group, and then the shorter-video groups. These results pose LongVideoBench as a meaningful and challenging benchmark for LMMs to test their video understanding abilities.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

2023b, Liu et al., 2023a, Ye et al., 2023] have shown competitive performance on many traditional short-video understanding tasks [Xu et al., 2017, Yu et al., 2019].

For longer videos, recent research explores methods like compressing video frames to fewer tokens to manage hour-long content within LMMs [Li et al., 2023c], and incorporating memory banks into standard LMM architectures [Song et al., 2023, He et al., 2024, Tan et al., 2024]. Leading models, both open-source (_e.g.,_ LWM [Liu et al., 2024a], Phi-3-128K [Abdin et al., 2024]) and proprietary (_e.g.,_ GPT-4o [OpenAI, 2024a], Gemini-1.5-Pro [Team, 2024]), now support context lengths over 128K tokens, allowing detailed video analysis. However, robust benchmarks for long-duration video understanding are lacking, with GPT-4o assessed only on 3-minute videos [Yu et al., 2019, Mangalam et al., 2023] and Gemini-1.5-Pro on an in-house benchmark. To advance LMM capabilities in understanding longer videos, we introduce LongVideoBench, a comprehensive benchmark for evaluating LMMs across various video durations and distributions.

**Benchmarks for Video LMMs.** Traditionally, video LMMs are evaluated on classical video QA datasets like MSVD-QA, MSRVTT-QA [Xu et al., 2017], and ActivityNet-QA [Yu et al., 2019], which primarily evaluate video LMMs through global-summary questions. However, it has been demonstrated that these benchmarks are addressable by a few key frames. For a focused assessment of temporal comprehension, NeXT-QA [Xiao et al., 2021] and MVBench [Wang et al., 2023] serve to measure temporal dynamics over short clips, with average durations of _44s_ and _16s_, respectively. Long-duration video understanding is targeted by benchmarks like EgoSchema [Mangalam et al., 2023], which involves multi-choice questions on _3-minute-long_ egocentric videos, and MovieChat-1K [Song et al., 2023], focused on _10-minute-long_ movie clips. These long-video benchmarks often limit their scope to videos on specific themes and still include a large proportion of summary questions solvable with limited frames. To address these gaps and enhance evaluation of detailed multimodal reasoning over longer videos, we introduce the LongVideoBench, a comprehensive benchmark focusing on referring reasoning questions that by-design requires dense input frames to solve, encompassing diverse video topics and varying lengths up to hour long.

## 6 Conclusion

This work introduces LongVideoBench, a comprehensive benchmark that evaluates Large Multimodal Models (LMMs) in understanding hour-long subtitled videos in diverse themes. The benchmark introduces referring reasoning questions, a novel video question-answering paradigm that addresses the longstanding issue of single frame bias in existing video understanding benchmarks. Evaluation results demonstrate that LongVideoBench presents significant challenges for both proprietary and open-source LMMs in their long-context multimodal capabilities. In addition, the benchmark results provide valuable insights on the deficiencies of existing models, making it a valuable asset to understand the current multimodal model landscape and to guide the future explorations.

Figure 4: Accuracy of proprietary and open-source LMMs _w.r.t._ referring query depth and video duration. All models perform worse when the referred moment is closer to video start or middle video. Please refer to Appendix Sec. C for respective visualizations on rest 15 models.