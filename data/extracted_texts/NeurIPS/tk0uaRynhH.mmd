# Dynamic Conditional Optimal Transport through Simulation-Free Flows

Gavin Kerrigan

Department of Computer Science

University of California, Irvine

gavin.k@uci.edu &Giosue Migliorini

Department of Statistics

University of California, Irvine

gmiglior@uci.edu &Padhraic Smyth

Department of Computer Science

University of California, Irvine

smyth@ics.uci.edu

###### Abstract

We study the geometry of conditional optimal transport (COT) and prove a dynamic formulation which generalizes the Benamou-Brenier Theorem. Equipped with these tools, we propose a simulation-free flow-based method for conditional generative modeling. Our method couples an arbitrary source distribution to a specified target distribution through a triangular COT plan, and a conditional generative model is obtained by approximating the geodesic path of measures induced by this COT plan. Our theory and methods are applicable in infinite-dimensional settings, making them well suited for a wide class of Bayesian inverse problems. Empirically, we demonstrate that our method is competitive on several challenging conditional generation tasks, including an infinite-dimensional inverse problem.

## 1 Introduction

Many fundamental tasks in machine learning and statistics may be posed as modeling a conditional distribution \((u y)\) where obtaining an analytical representation of \((u y)\) is impractical. While sampling-based approaches such as Markov Chain Monte Carlo (MCMC) methods are useful, they suffer from several limitations. First, MCMC requires numerous likelihood evaluations, rendering it prohibitively expensive in scientific and engineering applications where the likelihood is determined by an expensive numerical simulator. Second, MCMC must be run away for every observation \(y\), which is impractical in applications such as Bayesian inverse problems (Dashti and Stuart, 2013) and generative modeling (Mirza and Osindero, 2014). These limitations motivate the need for a _likelihood-free_(Cranmer et al., 2020) and _amortized_(Amos et al., 2023) approach. While methods like ABC (Beaumont, 2010) and variational inference (Blei et al., 2017) partially address these challenges, they are difficult to scale to high dimensions or have limited flexibility.

Recently, generative models such as normalizing flows (Papamakarios et al., 2019, 2021), GANs (Ramesh et al., 2022), and diffusion models (Sharrock et al., 2022) have shown promise in amortized and likelihood-free inference. These models may be viewed in the framework of _measure transport_(Baptista et al., 2020), where samples \(u(u)\) from a tractable source distribution are transformed by a mapping \(T(y,u)\) such that the transformed samples are approximately distributed as \((u y)\). One way to achieve this is through _triangular_ mappings (Baptista et al., 2020; Spantini et al., 2022), where a joint source distribution \((y,u)\) is transformed by a mapping of the form \(T:(y,u)(T_{Y}(y),T_{U}(y,u))\). Under suitable assumptions, if \(T\) transforms the source \((y,u)\) into the target \((y,u)\), then \(T_{U}(y,-)\) couples the conditionals \((u y)\) and \((u y)\).

Typically, such a map \(T\) is not unique (Wang et al., 2023), and a natural idea is thus to regularize the transport and search for an admissible mapping that is in some sense optimal. In other words, learning a conditional sampler may be phrased as finding a conditional optimal transport (COT) map. While there exists some work on learning COT maps, these approaches often rely on a difficult adversarial optimization problem (Baptista et al., 2020; Hosseini et al., 2023; Bunne et al., 2022; Ray et al., 2022) or simulating from the model during training (Baptista et al., 2023; Wang et al., 2023). In this work, we propose a conditional generative model for likelihood-free inference based on a dynamic formulation of conditional optimal transport. Specifically, our contributions are as follows:

1. We develop a general theoretical framework for dynamic conditional optimal transport in separable Hilbert spaces. Our framework is applicable in infinite-dimensional spaces, enabling applications in function space Bayesian inference. In Section 4, we study the _conditional Wasserstein space_\(_{p}^{}(Y U)\) and show that this space admits constant speed geodesics between any two measures. In Section 5, we characterize the absolutely continuous curves of measures in \(_{p}^{}(Y U)\) via the continuity equation and _triangular_ vector fields. As a consequence, we obtain conditional generalizations of the McCann interpolants (McCann, 1997) and the Benamou-Brenier Theorem (Benamou and Brenier, 2000).
2. In Section 6, we propose COT flow matching (COT-FM), a simulation-free flow-based model for conditional generation. This model directly leverages our theoretical framework, where we learn to model a path of measures interpolating between an arbitrary source and target distribution via a geodesic in the conditional Wasserstein space.
3. In Section 7, we demonstrate our method on several challenging conditional generation tasks. We apply our method to two Bayesian inverse problems - one arising from the Lotka-Volterra dynamical system, and an infinite-dimensional problem arising from the Darcy Flow PDE. Our method shows competitive performance against recent COT methods.

## 2 Related Work

Conditional Optimal Transport.Conditional Optimal Transport (COT) remains relatively under-explored in both machine learning and related fields. Recent approaches learn static COT maps via input convex networks (Bunne et al., 2022; Wang et al., 2023) or normalizing flows (Wang et al., 2023). In addition, there have been a number of heuristic approaches to conditional simulation through W-GANs (Sajiadi et al., 2017; Adler and Oktem, 2018; Kim et al., 2022, 2023), for which Chemseddine et al. (2023) provide a rigorous basis. Closely related to our work are those which employ triangular plans (Carlier et al., 2016; Trigila and Tabak, 2016), which have been modeled through GANs in Euclidean spaces (Baptista et al., 2020) and function spaces (Hosseini et al., 2023). In contrast, we use a novel _dynamic_ formulation of COT, which we model through a generalization of flow matching (Lipman et al., 2022; Albergo et al., 2023; Liu et al., 2022). This allows us to use flexible architectures while avoiding the difficulties of training GANs (Arora et al., 2018).

Simulation-Free Continuous Normalizing Flows.Flow matching (Lipman et al., 2022) (and the closely related stochastic interpolants (Albergo et al., 2023) and rectified flows (Liu et al., 2022)) are a class of methods for building continuous-time normalizing flows in a simulation-free manner. Notably, these works do not approximate an optimal transport between the source and target measures. Pooladian et al. (2023) and Tong et al. (2023) propose instead to couple the source and target distributions via optimal transport, leading to marginally optimal paths. In this work, we study an extension of these techniques for conditional generation.

While some works (Davtyan et al., 2023; Gebhard et al., 2023; Isobe et al., 2024; Wildberger et al., 2024) have applied flow matching for conditional generation, these approaches do not employ COT. Notably, these approaches are limited to the finite-dimensional setting, whereas our method adds to the growing literature on function-space generative models (Hosseini et al., 2023; Kerrigan et al., 2023; Kerrigan et al., 2024; Lim et al., 2023; Franzese et al., 2024). Concurrent work by Chemseddine et al. (2024) develops the foundation of dynamic COT with applications to flow matching, and concurrent work by Barboni et al. (2024) develop the theory of dynamic COT for the purposes of studying infinitely deep ResNets. However, these works focus on the Euclidean setting, whereas our methods are applicable in general separable Hilbert spaces.

## 3 Background and Notation

Let \(X,X^{}\) represent arbitrary separable Hilbert spaces, equipped with the Borel \(\)-algebra. We use \((X)\) to represent the space of Borel probability measures on \(X\), and \(_{p}(X)(X)\) to represent the subspace of measures having finite \(p\)th moment. If \((X)\) is a probability measure on \(X\) and \(T:X X^{}\) is measurable, then the pushforward measure \(T_{\#}(-)=(T^{-1}(-))\) is a probability measure on \(X^{}\). Maps of the form e.g. \(^{X}:X X^{} X\) represent the canonical projection.

We assume that we have two separable Hilbert spaces of interest. The first, \(Y\), is a space of observations, and the second, \(U\), is a space of unknowns. These spaces may be of infinite dimensions, but a case of practical interest is when \(Y\) and \(U\) are finite dimensional Euclidean spaces. We will consider the product space \(Y U\), equipped with the canonical inner product obtained via the sum of the inner products on \(Y\) and \(U\), under which the space \(Y U\) is also a separable Hilbert space. Let \((Y U)\) be a joint probability measure. The measures \(^{Y}_{\#}(Y)\) and \(^{U}_{\#}(U)\) obtained via projection are the _marginals_ of \(\). We use \(^{y}(U)\) to represent the measure obtained by conditioning \(\) on the value \(y Y\). By the disintegration theorem (Bogachev and Raus, 2007, Chapter 10), such conditional measures exist and are essentially unique, in the sense that there exists a Borel set \(E Y\) with \(^{Y}_{\#}(E)=0\), and the \(^{y}\) are unique for \(y E\).

### Static Conditional Optimal Transport

In conditional optimal transport, we are given a target measure \((Y U)\) and some source measure \((U)\). We seek a transport map \(T:Y U U\) such that, for any given \(y Y\), the mapping \(T(y,-):U U\) transforms the source distribution \(\) into the conditional distribution \(^{y}\), i.e., \(T(y,-)_{\#}=^{y}\). In a sense, \(T\) can be thought of as a collection of transport maps indexed by \(y Y\). If such a \(T\) were available, by drawing samples \(u_{0}\) and transforming them, one would obtain samples \(T(y,u_{0})^{y}\). Solving this transport problem for each fixed \(y\) is expensive at best, or impossible when only has a single (or no) samples \((y,u)\) for any given \(y\). Thus, one must leverage information across different observations \(y\). To that end, recent work has focused on the notion of _triangular mappings_\(T:Y U Y U\)(Hosseini et al., 2023, Baptista et al., 2020) of the form \(T(y,u)=(T_{Y}(y),T_{U}(T_{Y}(y),u))\) for some \(T_{Y}:Y Y\) and \(T_{U}:Y U U\). Triangular mappings are of interest as they allow us to obtain conditional couplings from joint couplings.

**Proposition 1** (Theorem 2.4(Baptista et al., 2020), Prop. 2.3(Hosseini et al., 2023)): _Suppose \(,(Y U)\) and \(T:Y U Y U\) is triangular. If \(T_{\#}=\), then \(T_{U}(T_{Y}(y),-)_{\#}^{y}=^{T_{Y}(y)}\) for \(^{Y}_{\#}\)-almost every \(y\)._

In many scenarios of practical interest, the source measure \(\) and the target measure \(\) have the same \(Y\)-marginals. We will henceforth make this assumption, and use \(=^{Y}_{\#}=^{Y}_{\#}\) to represent this marginal. In this case, we may take \(T_{Y}\) to be the identity mapping, so that the conclusion of Proposition 1 simplifies to \(T_{U}(y,-)_{\#}^{y}=^{y}\) for \(\)-almost every \(y\). We note that in situations where such an assumption does not hold, one may simply preprocess the source measure \(\) via an invertible mapping \(T_{Y}\) satisfying \([T_{Y}]_{\#}[^{Y}_{\#}]=^{Y}_{\#}\)(Hosseini et al., 2023, Prop 3.2).

Given a source and target measures \(,^{}(Y U)\) and a cost function \(c:(Y U)^{2}\), the _conditional Monge problem_ seeks to find a triangular mapping solving

\[_{T}\{_{Y U}c(y,u,T(y,u))\,(y,u) T_{\#} =,T:(y,u)(y,T_{U}(y,u))\}.\] (1)

The conditional Monge problem also admits a relaxation under which one only considers couplings whose \(Y\)-components are almost surely equal. To that end, for \(,^{}_{p}(Y U)\) we define the set of _triangular couplings_\(_{Y}(,)\) to be the couplings of \(\) and \(\) that almost surely fix the \(Y\)-components,

\[_{Y}(,)=\{((Y U)^{2} )^{1,2}_{\#}=,^{3,4}_{\#}=,^{1,3}_{\#}=( I,I)_{\#}\}.\] (2)

In other words, a triangular coupling \(_{Y}(,)\) has samples \((y_{0},u_{0},y_{1},u_{1})\) such that \(y_{0}=y_{1}\) almost surely. The _conditional Kantorovich problem_ seeks a triangular coupling solving

\[_{}\{_{(Y U)^{2}}c(y_{0},u_{0},y_{1},u_{1})\, (y_{0},u_{0},y_{1},u_{1})_{Y}(,)\}.\] (3)Hosseini et al. (2023) prove the existence of minimizers to the conditional Kantorovich and Monge problems under very general assumptions. Moreover, optimal couplings to the conditional Kantorovich problem induce optimal couplings for \(\)-almost every conditional measure. We refer to Appendix B and Hosseini et al. (2023) for further details.

## 4 Conditional Wasserstein Space

Motivated by our discussion on triangular transport maps, we introduce the conditional Wasserstein spaces, consisting of joint measures with finite \(p\)th moments and having fixed \(Y\)-marginals \(\). Interestingly, Gigli (2008, Chapter 4) studies the same space for the purposes of constructing geometric tangent spaces in the usual Wasserstein space.

**Definition 1** (Conditional Wasserstein Space):

_Suppose \((Y)\) is given and \(1 p<\). The conditional \(p\)-Wasserstein space is_

\[_{p}^{}(Y U)=\{_{p}(Y U) _{\#}^{Y}=\}.\] (4)

We now equip \(_{p}^{}(Y U)\) with a metric \(W_{p}^{}\), the conditional Wasserstein distance. Intuitively, the conditional Wasserstein distance measures the usual Wasserstein distance between all of the conditional distributions in expectation under the fixed \(Y\)-marginal \(\).

**Definition 2** (Conditional \(p\)-Wasserstein Distance):

_Suppose \(,_{p}^{}(Y U)\) and \(1 p<\). The function \(W_{p}^{}:_{p}^{}(Y U)_{p}^{}(Y U )\),_

\[W_{p}^{}(,)=(_{y}[W_{p}^{p}(^{y}, ^{y})])^{1/p}=(_{Y}W_{p}^{p}(^{y},^{y})\,(y))^{1/p}\] (5)

_is the conditional \(p\)-Wasserstein distance. \(W_{p}\) is the usual Wasserstein distance for measures on \(U\)._

By Jensen's inequality we have \(W_{p}^{}(,)_{y}[W_{p}(^{y},^{y})]\). In the following, we show that the conditional Wasserstein distance is a well-defined metric as well as a few other metric properties.

**Proposition 2** (Some Properties of \(W_{p}^{}\)):

_Let \(1 p<\)._

* \(W_{p}^{}\) _is well-defined, finite, and equals the minimal conditional Kantorovich cost._
* \(W_{p}^{}\) _is a metric on the space_ \(_{p}^{}(Y U)\)_._
* _There does not exist_ \(C>0\) _such that_ \(W_{p}^{}(,) CW_{p}(,)\) _for all_ \(,_{p}^{}(Y U)\)_._
* _For all_ \(,_{p}^{}(Y U)\)_,_ \(W_{p}(_{\#}^{U},_{\#}^{U}) W_{p}^{}(,)\) _and_ \(W_{p}(,) W_{p}^{}(,)\)_._

Proposition 2(c, d) together shows that one should expect the topology generated by \(W_{p}^{}\) to be stronger than the unconditional distance \(W_{p}\). Here, we note that Gigli (2008) and Chemseddine et al. (2023) previously showed that \(W_{p}^{}\) is a metric through an equivalence with restricted couplings. Our approach builds on the results of Hosseini et al. (2023) and is somewhat more direct, and hence our proofs may be of independent interest.

For the sake of concreteness, we include an example where the conditional \(2\)-Wasserstein distance may be explicitly computed. Here, the necessary calculations follows from the fact that the conditional distributions of a multivariate are again Gaussian, and Gaussian distributions admit a closed-form expression for the usual unconditional \(2\)-Wasserstein distance.

Example: Gaussian Measures.Suppose \(Y=U=\) and that \(,_{p}^{}(Y U)\) are Gaussians

\[=(0,I)=(0,1&\\ &1)||<1.\] (6)

It follows that \(=_{\#}^{Y}=_{\#}^{Y}=(0,1)\). As \(^{y},^{y}\) are Gaussians, their \(W_{2}\) distance admits a closed form and we can directly compute the expectation in Equation (5) to obtain \(W_{2}^{,2}(,)=2(1-})\). This is zero if and only if \(=0\), i.e. \(=\). However, \(_{\#}^{U}=_{\#}^{U}=(0,1)\) and \(W_{2}(_{\#}^{U},_{\#}^{U})=0\) regardless of \(\). Moreover, the unconditional distance is \(W_{2}^{2}(,)=2(2--)\), from which it is easy to verify that \(W_{2}(,) W_{2}^{}(,)\). See Appendix C for a similar derivation which applies to arbitrary Gaussians.

Conditional Wasserstein Space as a Geodesic Space.We now turn our attention to the geodesics in \(_{p}^{}(Y U)\). In particular, we show that there exists a constant speed geodesic between any two measures in \(_{p}^{}(Y U)\), generalizing a similar result in the unconditional setting (Santambrogio, 2015, Theorem 5.27). Moreover, we show that under suitable regularity assumptions, solutions to the conditional Monge problem (1) induce constant speed geodesics. Our motivation for studying geodesics in \(_{p}^{}(Y U)\) is practical - in Section 6, we show how one can model geodesics in \(_{p}^{}(Y U)\) in order to obtain a conditional flow-based model whose paths are easy to integrate.

A _curve_ is a continuous function \(_{}:I_{p}^{}(Y U)\) where \(I=(a,b)\) is any open interval of finite length. A curve is _absolutely continuous_ if there exists \(m L^{1}((a,b))\) such that

\[W_{p}^{}(_{s},_{t})_{s}^{t}m()\,  a<s t<b.\] (7)

If \((_{t})\) is an absolutely continuous curve, then its metric derivative

\[|^{}|(t)=_{s t}^{}(_{s},_{t})}{| s-t|}\] (8)

exists for almost every \(t(a,b)\), and, moreover, we almost surely have \(|^{}|(t) m(t)\) pointwise for any \(m\) satisfying Equation (7) (Ambrosio et al., 2005, Theorem 1.1.2). A curve \((_{t})\) is called a _constant speed geodesic_ if for all \(a<s t<b\), we have \(W_{p}^{}(_{s},_{t})=|t-s|W_{p}^{}(_{a},_{b})\). It is straightforward to show that every constant speed geodesic is absolutely continuous.

**Theorem 1** (\(_{p}^{}(Y U)\) is a Geodesic Space)

_For any \(,_{p}^{}(Y U)\), there exists a constant speed geodesic between \(\) and \(\)._

When an optimal triangular coupling \(^{*}_{Y}(,)\) is induced by an injective triangular map \(T^{}\), we may recover a constant speed geodesic in \(_{p}^{}(Y U)\), generalizing the McCann interpolant (McCann, 1997) to the conditional setting. We refer to Proposition 4 for sufficient conditions on \(,\) under which such a \(T^{}\) exists. Informally, samples from \((y_{0},u_{0})\) flow in a straight path at a constant speed to their destination \(T^{}(y_{0},u_{0})\).

**Theorem 2** (Conditional McCann Interpolants)

_Fix \(,_{p}^{}(Y U)\). Suppose \(T^{}(y,u)=(y,T^{}_{}(y,u))\) is an injective triangular map solving the conditional Monge problem (1). Define the maps \(T_{t}:Y U Y U\) for \(0 t 1\) via \(T_{t}=(1-t)I+tT^{}\), and define the curve of measures \(_{t}=[T_{t}]_{\#}_{p}^{}(Y U)\). Then,_

* \((_{t})\) _is absolutely continuous and a constant speed geodesic between_ \(,\)__
* _The vector field_ \(v_{t}(T^{}_{t}(y,u))=(0,T^{}_{}(y,u)-u)\) _generates the path_ \(_{t}\)_, in the sense that_ \((_{t},v_{t})\) _solve the continuity equation (_9_)._

## 5 Conditional Benamou-Brenier Theorem

In this section, we prove a characterization of the absolutely continuous curves in \(_{p}^{}(Y U)\). As a corollary, we obtain a conditional generalization of the Benamou-Brenier Theorem (Benamou and Brenier, 2000), giving us a dynamic characterization of the conditional Wasserstein distance. Roughly speaking, all such curves are generated by a vector field on \(Y U\) which has zero velocity in the \(Y\) component. This is natural, as all measures in \(_{p}^{}(Y U)\) have a fixed \(Y\)-marginal \(\). Such a vector field can be informally seen as tangent to a curve of measures, and is the dynamic analogue of the triangular maps discussed in Section 3. More formally, given an open interval \(I\), a time-dependent Borel vector field \(v:I Y U Y U\) is said to be _triangular_ if there exists a Borel vector field \(v^{U}:I Y U U\) such that \(v_{t}(y,u)=0,v^{U}_{t}(y,u)\).

Continuity Equation.We introduce some necessary background which allows us to link vector fields to curves of measures. The _continuity equation_\(_{t}_{t}+(v_{t}_{t})=0\) describes the evolution of a measure \(_{t}\) which flows along a given vector field \(v_{t}\)(Ambrosio et al., 2005, Chapter 8). This equation must be understood in the sense of distributions, i.e. for every \(\) in a space of test functions,

\[_{I}_{Y U}(_{t}(y,u,t)+ v_{t}(y,u), _{y,u}(y,u,t))\,_{t}(y,u)\,t=0.\] (9)We consider cylindrical test functions \((Y U I)\), i.e. of the form \((y,u,t)=(^{d}(y,u),t)\) where \(^{d}:Y U^{d}\) maps \((y,u)((y,u),e_{1},,(y,u),e_{d})\) and \(\{e_{1},e_{2},,e_{d}\}\) is any orthonormal family in \(Y U\). In the finite dimensional setting, one may take \( C_{c}^{}(Y U)\) to be smooth and compactly supported (Ambrosio et al., 2005, Remark 8.1.1).

In Appendix E, we prove Lemma 1, which is key in proving Theorem 4 below. Informally, Lemma 1 states that if the weak continuity equation (9) is satisfied for a joint distribution and triangular vector field, then the continuity equation is also satisfied for the corresponding conditional distributions and \(U\) components of the vector field.

**Lemma 1** (Triangular Vector Fields Preserve Conditionals): _Suppose \(v_{t}(y,u)=(0,v_{t}^{U}(y,u))\) is triangular and that \((_{t})_{p}^{}(Y U)\) is a path of measures such that \((v_{t},_{t})\) satisfy the continuity equation in the sense of distributions. Then, it follows that for \(\)-almost every \(y Y\), we have \(_{t}_{t}^{y}+(v_{t}^{U}(y,-)_{t}^{y})=0\)._

We note that having \(v_{t}\) be triangular is sufficient, but certainly not necessary, for the conditional continuity equation to almost surely hold. For instance, the vector field in \(^{d}\) that rotates \((0,I)\) about the origin is not triangular yet preserves all conditional distributions.

Absolutely Continuous Curves.In this section, we state our characterization of absolutely continuous curves in \(_{p}^{}(Y U)\). Informally, given such a curve, Theorem 3 provides us with a triangular vector field which generates the curve, in the sense that the pair solve the continuity equation.

**Theorem 3** (Absolutely Continuous Curves in \(_{p}^{}(Y U)\)): _Let \(I\) be an open interval, and suppose \(_{t}:I_{p}^{}(Y U)\) is an absolutely continuous in the \(W_{p}^{}\) metric with \(|^{}|(t) L^{1}(I)\). Then, there exists a Borel vector field \(v_{t}(y,u)\) such that_

* \(v_{t}\) _is triangular_
* \(v_{t} L^{p}(_{t},Y U)\) _and_ \(\|v_{t}\|_{L^{p}(_{t},Y U)}|^{}|(t)\) _for a.e._ \(t\)__
* \((v_{t},_{t})\) _solve the continuity equation in the sense of distributions._

Conversely, we show in Theorem 4 that if the pair \((_{t},v_{t})\) solve the continuity equation and \(v_{t}\) is triangular, then the curve \((_{t})\) is absolutely continuous and \(|^{}|(t)\|v_{t}\|_{L^{p}(_{t},Y U)}\). The main technique of this result is to study the collection of _conditional_ continuity equations (which is feasible by Lemma 1) and to apply the converse of Ambrosio et al. (2005, Theorem 8.3.1). In this setting, the infinite-dimensional result is obtained via a finite-dimensional approximation argument.

**Theorem 4** (Continuous Curves Generated by Triangular Vector Fields): _Suppose that \(_{t}:I_{p}^{}(Y U)\) is narrowly continuous and \((v_{t})\) is a triangular vector field such that \((_{t},v_{t})\) solve the continuity equation with \(\|v_{t}\|_{L^{p}(_{t},Y U)} L^{1}(I)\). Then, \(_{t}:I_{p}^{}(Y U)\) is absolutely continuous in the \(W_{p}^{}\) metric and \(|^{}|(t)\|v_{t}\|_{L^{p}(,Y U)}\) for almost every \(t\)._

As a corollary of Theorem 3 and Theorem 4, we obtain a conditional version of the Benamou-Brenier theorem (Benamou and Brenier, 2000). Once we have our characterization of absolutely continuous curves provided by these theorems, the proof of Theorem 5 largely follows the unconditional case (see e.g. Ambrosio et al. (2005, Chapter 8)), but we include it for the sake of completeness.

**Theorem 5** (Conditional Benamou-Brenier): _Let \(1<p<\). For any \(,_{p}^{}(Y U)\), we have_

\[W_{p}^{p,}(,)=_{(_{t},v_{t})}\{_{0}^{1}\|v_{t}\| _{L^{p}(_{t})}^{p}\,\,t\,|\,(v_{t},_{t})), $_{0}=,_{1}=$, and $v_{t}$ is triangular}\}.\]

## 6 COT Flow Matching

We have thus far seen that the COT problem (3) admits a dynamic formulation by Theorem 5, where one may take the underlying vector fields to be triangular. We use these results to design a principled model for conditional generation based on flow matching (Lipman et al., 2022, Albergo et al., 2023, Liu et al., 2022, Tong et al., 2023, Pooladian et al., 2023). We hereafter use the squared-distance cost (i.e. \(p=2\)).

Flow Matching.We assume that we have access to samples \(z_{0}=(y_{0},u_{0})(y_{0},u_{0})_{p}^{}(Y U)\) from a source measure, and samples \(z_{1}=(y_{1},u_{1})(y_{1},u_{1})_{p}^{}(Y U)\) from a target measure. Let \(z=(z_{0},z_{1})(z_{0},z_{1})(,)\) be any coupling of the source and target measure. We specify a collection of measures and vector fields on \(Y U\) via

\[_{t}(y,u z)=(y,u tz_{1}+(1-t)\,z_{0},C)  v_{t}(y,u z)=z_{1}-z_{0}\] (10)

where \(C\) is any trace-class covariance operator (Da Prato and Zabczyk, 2014). As is standard in flow matching (Lipman et al., 2022; Kerrigan et al., 2024), we obtain from Equations (10) a marginal measure \(_{t}(y,u)\) and vector field \(v_{t}(y,u)\) satisfying the continuity equation via

\[_{t}(y,u)=_{(Y U)^{2}}_{t}(y,u z)\,( z) v_{t}(y,u)=_{(Y U)^{2}}v_{t}(y,u z) _{t}(y,u z)}{_{t}(y,u)}\,(z).\] (11)

This marginal path \((_{t})_{t=0}^{1}\) interpolates between the source measure (\(t=0\)) and a smoothed version of the target measure (\(t=1\)). To transform source samples from \(\) into target samples from \(\), we seek to learn the intractable vector field \(v_{t}(y,u)\) with a model \(v^{}(t,y,u)\) by minimizing the loss1

\[()=_{t,(z),_{t}(y,u|z)}\|v^{}(t,y,u)-v_{t}(y,u z)\|^{2}\] (12)

which has the same \(\)-gradient as the MSE loss to the true vector field \(u_{t}(y,u)\)(Tong et al., 2023).

COT Flow Matching.In the preceding section, \((z)\) may be an arbitrary coupling between \(\) and \(\). Motivated by Proposition 1, we will choose \(\) to be a COT coupling. Under sufficient regularity conditions (see Appendix B), this COT plan will be induced by a triangular map. In turn, Theorem 2 gives us that this triangular map is generated by a triangular vector field of the form (10). Thus, we parametrize our model \(u^{}\) to also be triangular. Moreover, we recover the optimal dynamic transport given in Theorem 5 as \((C) 0\) by a pointwise application of (Tong et al., 2023, Proposition 3.4).

Given a collection of samples \(\{z_{0}^{i},z_{1}^{i}\}_{i=1}^{n}\) drawn from \(\) and \(\), we approximate a conditional optimal coupling \(\) using standard numerical techniques with the cost function \(c_{e}(y_{0},u_{0},y_{1},u_{1})=|y_{1}-y_{0}|^{2}+|u_{1}-u_{0}|^{2}\) for some \(0< 1\). Intuitively, such a cost penalizes mass transfer along the \(Y\) dimension, which is precisely the constraint sought in the COT problem (3). As \( 0\), we recover the true optimal triangular map (Carlier et al., 2010; Hosseini et al., 2023). The COT coupling

Figure 1: Samples from the ground-truth joint target distribution and the various models. Samples from COT-FM more closely match the ground-truth distribution than the baselines. In the final column, we plot conditional KDEs for samples drawn conditioned on the \(y\) value indicated by the dashed horizontal line. See Appendix F for a larger figure and additional results.

can either be precomputed for small datasets or computed on each minibatch drawn during training. While the use of minibatches is a computational necessity, we find that surprisingly small batch sizes still yields accurate approximations of the true COT mapping using our COT-FM method. See Appendix G.

After training, we obtain a learned triangular vector field \(v^{}(t,y,u)\). Given an arbitrary fixed \(y Y\), we may approximately sample from the target \((u y)\) by sampling \(u_{0}(u_{0} y)\) and numerically solving the corresponding flow equation \(_{t}(y,u_{t})=v^{}(t,y,u_{t})\) with initial condition \((y,u_{0})\).

Source Measure.Our framework is agnostic to the choice of source measure \(\), allowing for flexibility in the modeling process. The main requirement is that the \(Y\)-marginals of the source \(\) and target \(\) must match. In some scenarios, this is trivially satisfied. If one is interested in using a source distribution which is simply random noise, one may take \((y_{0},u_{0})=_{\#}^{Y}(y_{0})_{U}(u_{0})\) to be the product of two independent distributions where \(_{U}\) is arbitrary, e.g. Gaussian noise.

## 7 Experiments

We now illustrate our methodology (COT-FM) on a variety of conditional simulation tasks. We compare our method against several competitive baselines, namely PCP-Map (Wang et al., 2023), COT-Flow (Wang et al., 2023), and WaMGAN (Hosseini et al., 2023). These baselines are chosen as they reflect current state-of-the-art approaches to learning COT maps. We additionally compare against flow matching (Lipman et al., 2022; Wildberger et al., 2024) without COT, i.e. where the coupling between the source and target measures is the independent coupling \((z_{0},z_{1})=\). This baseline serves as an ablation for the COT component of our model.

Overall, our method (COT-FM) typically outperforms these baselines across the diverse and challenging set of tasks we consider. We find that PCP-Map (Wang et al., 2023) is a strong baseline, but we emphasize that this model relies on the use of an input-convex neural network (Amos et al., 2017) and it is hence unclear how to adapt this method to e.g. images. Appendix F contains further details and results for all of our experiments.2

2D Synthetic Data.We first consider synthetic distributions where \(Y=U=\). Our source measure is taken to be the independent product \((y,u)=_{\#}^{Y}(0,1)\). We plot ground-truth joint distributions and samples for two datasets in Figure 1. See Appendix F for additional results. Samples from our method (COT-FM) closely match those from the ground-truth distribution, whereas samples from PCP-Map and COT-Flow (Wang et al., 2023) can produce samples in regions of zero support under the ground-truth distribution. In Table 1, we provide a quantitative analysis, where we measure the \(W_{2}\) and MMD distances between the generated and ground-truth joint distributions. This is motivated by Proposition 1, as triangular maps which couple the joint distributions necessarily couple the conditional distributions. Our method outperforms the baselines across all metrics.

    &  &  &  &  \\  & \(W_{2}\) (1e-2) & MMD (1e-3) & \(W_{2}\) (1e-2) & MMD (1e-3) & \(W_{2}\) (1e-2) & MMD (1e-3) & \(W_{2}\) (1e-2) & MMD (1e-3) \\  PCP-Map & \(6.27 0.81\) & \(0.21 0.13\) & \(8.44 1.09\) & \(0.22 0.10\) & \(6.19 0.43\) & \( 0.17\) & \(5.35 0.93\) & \(0.16 0.13\) \\ COT-Flow & \(8.20 0.49\) & \(0.26 0.16\) & \(18.49 2.22\) & \(1.32 0.79\) & \(10.04 1.69\) & \(0.24 0.22\) & \(6.47 0.69\) & \(0.19 0.19\) \\ FM & \(8.81 0.58\) & \(0.24 0.20\) & \(15.55 0.77\) & \(1.85 0.22\) & \(7.03 0.17\) & \(0.45 0.11\) & \(8.18 0.34\) & \(0.58 0.09\) \\ COT-FM (Ours) & \( 1.00\) & \( 0.13\) & \( 1.41\) & \( 0.10\) & \( 0.43\) & \( 0.04\) & \( 1.26\) & \( 0.19\) \\   

Table 1: Distances between the ground-truth and generated joint distributions for the 2D datasets. Our method (COT-FM) obtains lower distances than the considered baselines. Average results \(\) one standard deviation are reported across five test sets, with the lowest average distance in bold.

    & \(W_{2}\) (1e-2) & MMD (1e-3) \\  PCP-Map & \(5.04 0.05\) & \(2.67 2.1\) \\ COT-Flow & \(4.86 1.1\) & \( 0.50\) \\ FM & \(11.41 0.26\) & \(2.65 0.14\) \\ COT-FM (Ours) & \( 0.06\) & \(0.95 0.03\) \\   

Table 2: Statistical distances between MCMC and posterior samples \(u(u y)\) for each method on the LV dataset. Average results \(\) one standard deviation reported across five test sets.

Lotka-Volterra (LV) Dynamical System.Here we estimate parameters of the Lotka-Volterra (LV) model given only noisy observations of its solution. The LV model has parameters \(u=(,,,)^{4}_{ 0}\) and a pair of coupled nonlinear ODEs of the form

\[p_{1}(t)}{t}= p_{1}- p_{1}p_{2}p_{2}(t)}{t}=- p_{2}+ p_{1}p_{2}\] (13)

whose solution \(p(t)=(p_{1}(t),p_{2}(t))^{2}_{ 0}\) represents the number of prey and predator species at time \(t[0,T]\). Following Alfonso et al. (2023), we assume \(p(0)=(30,1)\) and that \((u)(m,0.5I)\) with \(m=(-0.125,-3,-0.125,-3)\). Given parameters \(u^{4}_{ 0}\), we simulate Equation (13) for \(t\{0,2,,20\}\) to obtain a solution \(z(u)^{22}_{ 0}\). An observation \(y^{22}_{ 0}\) is obtained by the addition of log-normal noise, i.e. \((y)((z(u),0.1I)\). We thus may simulate many \((y,u)\) pairs from the target measure for training.

As a benchmark, we follow the settings of Alfonso et al. (2023) and choose parameters \(u=(0.83,0.041,1.08,0.04)\) to generate a single observation \(y\) as described above. In Figure 2, we plot a histogram of \(10,000\) samples from the posterior \((u y)\) of COT-FM.

Since the ground-truth posterior is intractable, we compare against differential evolution Metropolis MCMC (Braak, 2006). Samples from our method qualitatively resemble those from MCMC, and the posterior mode is typically close to the true unknown \(u\) (shown in red). Our method is quantitatively closest to the MCMC samples in the \(W_{2}\) metric, and competitive in the MMD metric (Table 2).

Darcy Flow Inverse Problem.Here we consider an infinite-dimensional Bayesian inverse problem from the 2D Darcy flow PDE. The setting is adapted from Hosseini et al. (2023). We opt to compare against WaMGAN (Hosseini et al., 2023), as this is currently the only other extant amortized function-space COT method, and FFM (Kerrigan et al., 2023) as a function-space flow matching ablation.

The Darcy flow PDE is an elliptic equation on a smooth domain \(^{d}\) which relates a permeability field \((u)\), a pressure field \(\), and a source term \(f\) via \(-(u)=f\) on \(\) subject to \(=0\) on \(\). Our goal is to recover the permeability \(u\) from noisy measurements \(y\) of the pressure \(\). Both the unknown \(u\) and observations \(y\) are functions and thus infinite-dimensional. To define our target measure, we specify a prior \((u)=(0,C)\) with a Matern kernel \(C\) of lengthscale \(=1/2\) and \(=3/2\). Given \(u(u)\), the Darcy flow PDE is solved numerically \(\). Our goal is to recover the permeability \(u\) from noisy measurements \(y\) of the pressure \(\). Both the unknown \(u\) and observations \(y\) are functions and thus infinite-dimensional. To define our target measure, we specify a prior \((u)=(0,C)\) with a Matern kernel \(C\) of lengthscale \(=1/2\) and \(=3/2\). Given \(u(u)\), the Darcy flow PDE is solved numerically (Alnaes et al., 2015) to obtain a solution \((u)\) observed at some finite but arbitrary number of points \(\{x_{1},,x_{n}\}^{2}\). An observation \(y(u)\) is obtained by adding Gaussian noise to each observation, i.e. \(y(u)((u),^{2}I)\) where \(=2.5 10^{-2}\). We implement all models via a Fourier Neural Operator (Li et al., 2020), allowing us to work with arbitrary discretizations, as required by the functional nature of this problem.

We provide an illustration in Figure 3. As the true posterior is intractable, we compare against preconditioned Crank-Nicolson (pCN) (Cotter et al., 2013), a function-space MCMC method. In Figure 3, we plot the mean posteriors obtained from the various methods. Qualitatively, both COT-FM and FFM are good approximations to pCN, while WaMGAN has visual artifacts. However, the MSE

    & MSE (1e-2) & CRPS (1e-2) \\  WaMGAN & \(6.55 0.07\) & \(18.75 0.10\) \\ FFM & \(7.30 0.07\) & \( 0.06\) \\ COT-FFM (Ours) & \( 0.08\) & \(15.56 0.08\) \\   

Table 3: Predictive performance of the generated samples on the Darcy flow inverse problem. Average result \(\) one standard deviation obtained on 5 test sets of 5,000 samples each.

Figure 2: Sample KDEs on the Lotka-Volterra inverse problem. The red lines denote the true parameter values.

between our method and the pCN mean is lower than that of FFM. Table 3 provides a quantitative comparison between the methods on a test set of 5,000 samples, where we measure MSE and CRPS (Hersbach, 2000). We compare the ensemble mean of 10 samples against the true \(u\) value as running pCN for each observation is prohibitively expensive. COT-FM outperforms FFM and WaMGAN in terms of MSE and is on-par with FFM in terms of CRPS. See Appendix F for further details.

## 8 Conclusion

We analyze conditional optimal transport from a geometric and dynamical point of view. Our analysis culminates in the characterization of absolutely continuous curves of measures in a conditional Wasserstein space, resulting in a conditional analog of the Benamou-Brenier Theorem.

We use these result to build on the framework of triangular transport and flow matching to develop simulation-free methods for conditional generative models. Our methods are applicable across a wide class of problems, and we demonstrate our methodology on several challenging inverse problems.

Limitations and Broader Impacts.A limitation COT-FM is that computing the full COT plan can be expensive for large datasets, necessitating the use of minibatch approximations potentially resulting in sub-optimal plans. While this approximation does not limit the practical applicability of our method, an interesting challenge is to characterize the precise relationship between this minibatch approximation and the full COT plan. Moreover, computing the COT plan incurs a small additional computational cost compared to standard flow matching. As with all generative models, a potential negative impact is the potential for disinformation through generated samples being purported as real.