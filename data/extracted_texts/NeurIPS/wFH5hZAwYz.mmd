# Sharp Calibrated Gaussian Processes

Alexandre Capone

Technical University of Munich

alexandre.capone@tum.de &Sandra Hirche

Technical University of Munich

hirc@cit.tum.de &Geoff Pleiss

University of British Columbia

Vector Institute

geoff.pleiss@stat.ubc.ca

###### Abstract

While Gaussian processes are a mainstay for various engineering and scientific applications, the uncertainty estimates don't satisfy frequentist guarantees and can be miscalibrated in practice. State-of-the-art approaches for designing calibrated models rely on inflating the Gaussian process posterior variance, which yields confidence intervals that are potentially too coarse. To remedy this, we present a calibration approach that generates predictive quantiles using a computation inspired by the vanilla Gaussian process posterior variance but using a different set of hyperparameters chosen to satisfy an empirical calibration constraint. This results in a calibration approach that is considerably more flexible than existing approaches, which we optimize to yield tight predictive quantiles. Our approach is shown to yield a calibrated model under reasonable assumptions. Furthermore, it outperforms existing approaches in sharpness when employed for calibrated regression.

## 1 Introduction

Gaussian process (GP) regression offers an ambitious proposition: by conditioning a model on measurement data, we are provided with a Gaussian probability distribution for the unseen data. Assuming that the posterior probability distribution holds, we can then directly calibrate our model using the inverse error function. Though the distribution of unseen data seldom follows the Gaussian prior distribution, and the GP generally does not adapt adequately to the observed distributions after being conditioned on the data, GPs have become one of the most powerful and established regression techniques. Besides having found widespread use in machine learning (Deisenroth et al., 2015; Srinivas et al., 2012), their good generalization properties have motivated applications in the fields of control (Kocijan, 2016), astrophysics (Roberts et al., 2013) and chemistry (Deringer et al., 2021), to name a few. Furthermore, the Bayesian paradigm offers a powerful tool to analyze the theoretical properties of different regression techniques (Srinivas et al., 2012; Capone et al., 2022).

In this paper, we present a novel approach to obtaining sharp calibrated Gaussian processes, i.e., Gaussian processes that provide concentrated predictive distributions that accurately match the observed data. Instead of computing confidence intervals by inflating the Gaussian process posterior variance, our approach discards it and computes a new quantity inspired by the computation of the posterior variance, where all hyperparameters are chosen in a way that results in both accurate and sharp calibration. In other words, we train two separate Gaussian processes: one for the predictive mean and one for obtaining predictive quantiles, which is exclusively used for calibration purposes. By doing so, we reach considerably more flexibility than existing calibration approaches, which enables us to additionally optimize the sharpness of the calibration. Our approach outperforms severalstate-of-the-art calibration approaches in terms of sharpness while still yielding similar calibration performance. Furthermore, it is competitive compared to a neural network-based method in sharpness without sacrificing calibration performance.

Notation.We use \(_{+}\) to denote the non-negative real numbers. Boldface lowercase/uppercase characters denote vectors/matrices. For two vectors \(\) and \(^{}\) in \(^{d}\), we employ the notation \(^{}\) to denote componentwise inequality, i.e., \(a_{i} a_{i}^{}\), \(i=1,,d\). For a square matrix \(\), we use \(||\) to denote its determinant, and \([]_{ij}\) to denote the entry corresponding to the \(i\)-th row and \(j\)-th column.

## 2 Related Work

Calibration of Classification Models.There has been extensive work on obtaining calibrated models in the domain of classification. While there are many methods that do not employ post-processing, we only focus here on methods that employ some form of post-processing. Most forms of post-processing-based calibration for classification fall into the category of conformal methods (Vovk et al., 2005), which, given an input, aim to produce sets of labels that contain the true label with a pre-specified probability. Arguably the two most common forms of calibration are isotonic regression (Niculescu-Mizil and Caruana, 2005) and Platt scaling (Platt et al., 1999). In Niculescu-Mizil and Caruana (2005), Platt scaling and isotonic regression are analyzed extensively for different types of predictive models. In Guo et al. (2017), a modified form of Platt scaling for modern classification neural networks is proposed.

Calibration of Regression Models.Though initially developed for classification, conformal calibration has been extended to regression settings. In Lakshminarayanan et al. (2017), a calibration approach was proposed for deep ensembles. Gal et al. (2017) propose a dropout-based technique for calibrating deep neural networks. However, these approaches require changing the regressor, potentially deteriorating its predictive performance. It should be noted that Bayesian neural networks (MacKay, 1995), while being able to provide credible sets for the output, fully trust the posterior, resulting in a naively calibrated model that seldom reflects the data's distribution. As a remedy for this, Kuleshov et al. (2018) present a recalibration approach that scales a model's predictive quantiles to satisfy the observed data's distribution. In Vovk et al. (2020), a similar approach is presented, where interpolation between scaling factors is randomized, and a theoretical analysis is provided. An extension of both Kuleshov et al. (2018) and Vovk et al. (2020) and other recalibration is proposed in Marx et al. (2022), along with corresponding theoretical guarantees. While these methods have been shown to yield well-calibrated models, the resulting predictive quantiles are potentially much too crude, resulting in predictions that perform poorly in terms of sharpness, i.e., the corresponding confidence intervals will overestimate the model error by a very large margin. To remedy this, Song et al. (2019) and Kuleshov and Deshpande (2022) propose optimizing the parameters of a recalibration model by obtaining calibration on a distribution level. However, while Song et al. (2019) relies on complex approximations and provides no theoretical guarantees, Kuleshov and Deshpande (2022) does not allow to optimize for sharpness directly, and calibration is only guaranteed asymptotically as the number of data grows.

## 3 Problem Statement

Consider a compact input space \(^{d}\), and output space \(\), and an unknown data distribution \(\) on \(\). Consider a model conditioned on training data \(_{}\) that, for every \(\) and confidence level \(\), returns a base prediction \(_{_{}}()\) and an additive cut-point term \(_{}_{_{}}(,)\), where \(_{_{}}(,) 0\) and \(_{}\) is potentially negative, such that \(_{_{}}()+_{}_{_{ }}(,)\) corresponds to a predictive \(\)-quantile. The model is then said to be calibrated if

\[_{,y}y-_{_{}}() _{}_{}(,)=\] (1)

holds for every \(\). Furthermore, the calibrated model is also said to be sharp if the corresponding predictive distributions are concentrated (Gneiting et al., 2007), i.e., if the centered confidence intervals induced by the predictive quantiles

\[|_{}_{_{}}(,)-_{1- }_{_{}}(1-,)|\] (2)are as small as possible for every \(\). Our goal is to find a sharply calibrated model based on GP regression.

## 4 Gaussian Process Regression

In this section, we briefly review GP regression, with particular focus on the choice and influence of hyperparameters.

A GP is formally defined as a collection of random variables, any finite subset of which is jointly Gaussian (Rasmussen Williams, 2006). It is fully specified by a prior mean function, which we set to zero without loss of generality, and a hyperparameter-dependent covariance function, called kernel \(k:\), where \(\) denotes the hyperparameter space. The core concept behind GP regression lies in assuming that any finite number of measurements of an unknown function \(f:\) at arbitrary inputs \(_{1},,_{N}\) are jointly Gaussian with mean zero and covariance \(()\), where

\[[()]_{ij}=k(,_{i},_{j})\]

consists of kernel evaluations at the test inputs.

Given a set of noisy observations \(=\{_{i},y_{i}\}_{i=1}^{N}\), where \(y_{i} f(_{i})+_{i}\) and \(_{i}(0,_{0}^{2})\) is iid Gaussian measurement noise, we can condition the GP on them to obtain the posterior distribution. The posterior distribution for a new observation \(y^{*}\) at an arbitrary input \(^{*}\) is again Gaussian distributed, with mean and variance

\[_{_{u}}(,^{*})= ()(()+_{0}^{2})^{-1},\] (3a) \[_{_{u}}^{2}(,^{*})= k(,^{*},^{*})-()(( )+_{0}^{2})^{-1}()+_{0}^{2},\] (3b)

with \(()(k(,^{*},_{1}),,k( ,^{*},_{N}))\), \(=(y_{1},,y_{N})\), and \(\) denoting the identity matrix. In this paper, we restrict ourselves to kernels \(k(,,)\) that yield a posterior variance that is monotonically increasing with respect to the hyperparameters \(\), as specified in the following assumption.

**Assumption 4.1**.: The posterior variance \(_{_{u}}^{2}(,^{*})\) is a continuous function of \(\). Furthermore, for all hyperparameters \(,^{}\) with \(^{}\), it holds that \(_{_{u}}^{2}(,^{*})_{_{ u}}^{2}(^{},^{*})\).

Assumption 4.1 holds trivially for the signal variance of a kernel. Moreover, it holds for any hyperparameters that lead to a monotonous increase in the Fourier transform, which is the case, e.g., for the inverse lengthscale of stationary kernels up to a multiplicative factor corresponding to the ratio of the lengthscales (Capone et al., 2022). Furthermore, several results that employ the so-called fill-distance indicate that Assumption 4.1 holds for the inverse lengthscale of a broad class of stationary kernels as the lengthscale becomes very large or very small (Wendland, 2004). In our experiments, we observed that Assumption 4.1 was never violated for the inverse lengthscale of the squared-exponential kernel. Assumption 4.1 will be leveraged to define a cumulative density function by also changing the hyperparameters corresponding to the posterior covariance, as opposed to simply scaling it.

Arguably one of the most challenging aspects of GP regression lies in the choice of hyperparameters \(\), as they ultimately determine various characteristics of the posterior, e.g., smoothness and amplitude. In practice, the most common way of choosing \(\) is by maximizing the log marginal likelihood

\[ p(|,)=-|()+_ {0}^{2}|-(2)-^{}( ()+_{0}^{2})^{-1}.\] (4)

In terms of _posterior mean_ quality, i.e., predictive performance of \(_{_{u}}(,^{*})\), choosing the hyperparameters in this manner is often the most promising option, since it seeks a trade-off between model complexity and data fit, and has repeatedly been shown to yield a satisfactory mean square error when applied to test data (Rasmussen Williams, 2006). However, even when employing log-likelihood maximization, the posterior GP distribution is seldom well calibrated, and, in practice, the data often has a significantly different distribution. As a result, quantiles obtained with a purely Bayesian approach are either too confident or too conservative in practice (Capone et al., 2022; Fong Holmes, 2020). Furthermore, the restrictions imposed by the GP prior often produce a posterior variance \(^{2}(,^{*})\) that is grossly conservative. See Figure 1 for an illustration.

## 5 Proposed Approach

In this section, we present our approach to obtaining calibrated GPs by discarding the posterior variance and obtaining alternative predictive quantiles using a quantity inspired by the posterior variance and new hyperparameters. As we will demonstrate, our approach has numerous advantages. First, by varying the hyperparameters used to obtain predictive quantiles, the resulting quantiles are sharper than what can be obtained simply by multiplying the posterior variance with an appropriate constant. Secondly, by exploiting the monotonicity of the hyperparameters, our approach can be used to obtain intervals for multiple confidence levels \(\) in a very efficient manner. Finally, our method is backed by tight theoretical guarantees, obtained by exploiting its connection to conformal prediction.

### Sharp Calibrated GP for Single Confidence Level \(\)

In the following, we describe how to obtain a sharply calibrated GP for a fixed desired calibration level \(\). Instead of scaling the GP posterior variance to meet the desired calibration level \(\), we propose computing predictive quantiles by training a new quantity similar to the posterior variance but with new hyperparameters. In other words, we discard the posterior variance of the first GP and replace it by a quantity that corresponds to the posterior variance of a different GP, which we train separately. This way, we allow for more degrees of freedom during calibration. We then leverage this additional freedom to minimize the distance of the quantiles to the predictive mean, which yields a sharply calibrated model.

We assume to have a data set \(\), which we split into training data \(_{}\) and calibration (holdout) data \(_{}\), with \(=_{}_{}\). The training data \(_{}\) will be used to compute the posterior, whereas \(_{}\) will be used to calibrate the model. Note that while not splitting the data might be reasonable when using other types of regression models, e.g., Bayesian or ensemble neural networks, splitting the data in the case of GPs is beneficial for providing accurate quantiles for data out of distribution. This is because, for many commonly used kernels, the GP posterior distribution is considerably more concentrated for test points close to the training data \(_{}\) than for those far away from \(_{}\). Since we wish to obtain a model that is calibrated for data both close and far away from the training data, we take this into account during training by splitting the data. We also assume to have a _predictive_ mean \(_{_{}}(^{R},)\), corresponding to GP posterior mean function, where the regressor hyperparameters \(^{R}\) were obtained, e.g., via log-likelihood maximization, as discussed in Section 4. However, note that any other way of choosing the posterior mean hyperparameters \(^{R}\) is permitted.

Given \(_{_{}}(^{R},)\) and \(=_{}_{}\), we then follow the convention of other recalibration approaches (Kuleshov et al., 2018; Marx et al., 2022) and aim to obtain, for an arbitrary \(0 1\), a scalar \(_{}\) and vector of hyperparameters \(_{}\), such that the corresponding predictive quantile

Figure 1: Confidence interval of \(95\%\) (shaded regions) obtained with purely Bayesian approach (a), where the inverse error function is employed to compute \(_{}\), and our approach (b). Solid lines represent the predictive mean. The confidence interval obtained with the Bayesian approach is not only grossly overconfident (contains less than \(80\%\) of the total data) but also partially extremely loose, exhibiting unnecessarily large confidence intervals far away from the data. By contrast, our approach is both accurate and tight.

\(_{}_{_{}}(_{},)\) contains \(\) times the total amount of data points. For this reason, we henceforth refer to \(_{}\) as _calibration hyperparameters_.

In order to obtain a sharply calibrated GP model, ideally we would like to choose \(_{}\) and \(_{}\) such that they minimize the expected length of the centered intervals (2) subject to calibration. However, this optimization problem is hard to solve. Hence, we instead attempt to improve model sharpness by concentrating the predictive distribution around the predictive mean \(_{_{}}(^{R},)\), i.e., by minimizing the deviation of the quantiles from \(_{_{}}(^{R},)\). This corresponds to solving the optimization problem

\[_{_{}\\ _{}}&_{i=1}^{N_{ }}_{}^{2}_{_{}}^{2}(_{},_{}^{i})\\ &_{i=1}^{N_{}}_{  0}( y_{}^{i}-_{}_{_{}} (_{},_{}^{i}))}{N_{}+1}=\] (5)

where \(\{_{}^{i},y_{}^{i}\}_{ {cal}}\) are samples from the calibration data set, \( y_{}^{i} y_{}^{i}-_{_{} }(^{R},_{}^{i})\) corresponds to the difference between predicted and measured output, and \(N_{}=|_{}|\) to the number of data points used for calibration.

_Remark 5.1_.: Note that we employ \(N_{}+1\) in the denominator in (5) instead of \(N_{}\). Though this choice makes little difference in practice, we require it for theoretical guarantees.

The equality constraint in (5) is generally infeasible, e.g., if \(N_{}=2\) and \(=0.5\), and is discontinuous, making it hard to solve. As it turns out, this can be easily remedied without any detriment to sharpness or calibration, and the problem can be rendered considerably easier to solve by substituting the equality constraint in (5) with

\[_{}=q_{}(,_{_{}}^{-1} {}_{}),\] (6)

where \(q_{}(,_{_{}}^{-1}_ {})\) is a monotonically increasing piecewise linear function1

\[_{_{}}^{-1}_{}=( }^{1}}{_{_{}}( _{},_{}^{1})},...,}^{N_ {}}}{_{_{}}(_{},_{}^{N_{}})})^{}\]

 correspond to the z-scores of the data under the calibration standard deviation \(_{_{}}(_{},)\). The original problem (5) then becomes

\[_{}_{i=1}^{N_{}}[q_{ }(,_{_{}}^{-1}_{}) _{_{}}(_{},_{}^{i} )]^{2},\] (7)

which is considerably easier to solve due to the lack of constraints, and enables us to use gradient-based approaches, since \(q_{}(,_{_{}}^{-1}_ {})\) is differentiable with respect to \(_{_{}}(_{},_{}^{i})\).

_Remark 5.2_.: The choice of interpolant (6) is due to its simplicity. However, other forms of monotone interpolation are also possible. For high data sizes, the choice of interpolant becomes of little relevance, since we only perform small interpolation steps.

### Calibrated GP for Arbitrary Confidence Level \(\)

While (7) is useful for obtaining a sharply calibrated model for a single confidence level \(\), solving (7) multiple times whenever we want to obtain sharply calibrated models for different confidence levels \(\) can be time-consuming. Furthermore, interpolating between any two arbitrary solutions of (7) won't necessarily yield a result close to the desired calibration. Fortunately, we can leverage Assumption 4.1 to show that interpolating between two solutions of (7) will yield a result close to the desired calibration, provided that we interpolate between two strictly increasing or decreasing sets of hyperparameters. Formally, this is achieved by solving (7) \(N_{}+1\) times to obtain \(_{0},_{_{1}},,_{_{N}}\) and \(_{0},_{_{1}},,_{_{N}}\), subject to two additional constraints. First, the calibration scaling parameters \(_{}\) must be monotonically increasing with \(\), i.e.,

\[_{_{i}}_{_{j}}_{i}<_{j},\]

and the calibration hyperparameters \(_{}\) must be decreasing with \(\) if \(_{}\) is negative, and increasing if \(_{}\) is positive, i.e.,

\[_{_{i}} _{_{j}},_{i}<_{j}\ _{_{i}} 0,\] \[_{_{i}} _{_{j}},_{i}<_{j}\ _{_{j}} 0.\]

In other words, the entries of \(_{}\) are strictly decreasing with \(_{}\) up until the sign of \(_{}\) switches, after which they are increasing. The reason why we impose these restrictions is that we can then confidently interpolate between any values of \(_{}\) and \(_{}\), since Assumption 4.1 implies that the quantile stipulated by \(_{}_{_{}}(,)\) is monotonically increasing with \(\). We then train simple piecewise linear interpolation models \(:\) and \(}:\), such that \((_{i})=_{_{i}}\) and \(}(_{i})=_{_{i}}\), with the additional constraint that \(}()\) reaches a minimum whenever \(()=0\), which can be potentially achieved by adding an artificial vector of training hyperparameters \(_{}\) for computing \(}()\). Note, however, that any other form of monotone interpolation is also acceptable for obtaining \(()\) and \(}()\). The procedure is summarized in Algorithm 1.

``` Input: kernel \(k(,,)\), predictor \(_{_{}}(^{R},)\), calibration data \(_{}\), confidence levels \(_{1},..._{N}\) for\(i=1\)to\(M\)do  Compute \(_{_{1}},_{_{1}},...,_{_{N_{}}}, _{_{N_{}}}\) by solving (6) and (7) subject to \[_{_{i}} _{_{j}},_{i}<_{j} \ _{_{i}} 0,\] \[_{_{i}} _{_{j}},_{i}<_{j} \ _{_{j}} 0.\] endfor  Fit a continuous, monotonically increasing interpolation model \(()\) and a continuous model \(}()\) using the training data \(\{_{i},_{_{i}},_{_{i}}\}_{i=1,...,N}\) Output: \((),}()\) ```

**Algorithm 1** Training Calibration Hyperparameters for Arbitrary Confidence Level

_Remark 5.3_.: While lengthscale constraints of the form \(_{_{j}}_{_{j+1}}\) can be easily enforced when solving (7) by substituting \(\) with \(_{_{j}}+\) and minimizing over the logarithm of \(\), the constraint \(_{_{j}}_{_{j+1}}\) is not enforced in (7). However, in practice we were often able to find local minima of (7) that satisfy this requirement.

We can then easily show that our approach achieves an arbitrary calibration level as the amount of data grows, provided that we choose the confidence levels \(\) accordingly.

**Theorem 5.4**.: _Let \(y\) be absolutely continuous conditioned on \(\), let \(_{_{}}(^{R},)\) be a posterior GP mean, and let \(_{_{}}(,)\) be a GP posterior variance conditioned on \(_{}\). Then, for any calibration data set \(_{}=\{^{i}_{},y^{i}_{}\}\), choose_

\[_{1}=}+1},_{2}= }+1},\ ,_{N_{}}=}}{N_{}+1},\]

_and let \(()\) and \(}()\) be interpolation models obtained with Algorithm 1 and confidence levels \(_{1},...,_{N_{}}\). Then_

\[_{,y}y-_{_{}}(}(),)()_{_{}}(}(),)[-}+1},+}+1}].\]

Proof.: The proof can be found in the supplementary material. 

Note that Theorem 5.4 also implies that a single set of calibration parameters \(_{}\) and \(_{}\) obtained by solving (7), since we can substitute \(()=_{}\) and \(}()=_{}\) into (8).

## 6 Discussion

Computational Complexity.Much like hyperparameter optimization for standard GPs (Rasmussen & Williams, 2006), the major driver behind the computational complexity in our approach stems from the need to invert the covariance matrix, an operation that scales cubically with the amount of data. In order to alleviate the computational cost of our approach, we can resort to different tools that improve scalability (Liu et al., 2020). One approach consists of employing only a subset of the training data \(_{}\) to choose the calibration hyperparameters \(_{}\), and then the full data set to choose \(_{}\). While this potentially leads to a loss in sharpness compared to when using the full data set, it still guarantees a calibrated model. Our technique is also readily applicable to sparse GPs (Snelson & Ghahramani, 2005; Titsias, 2009), as Assumption 4.1 typically still holds. This option is also explored in numerical experiments, in Section 7. Moreover, in many settings a specific level of calibration is often required, as opposed to several different ones, e.g., in stochastic model predictive control, where chance constraints corresponding to a fixed risk have to be satisfied (Mesbah, 2016). In such settings, we potentially only have to train a single vector of calibration hyperparameters \(_{}\), which reduces computational cost.

Initialization and Solution.The choice of initial hyperparameters can affect the optimization results considerably, and choosing a good hyperparameter initialization can be challenging, as is true when choosing the hyperparameters for the predictive mean. While this can be partially addressed by employing random restarts, we can also reuse trained models for similar calibration levels, since it is reasonable to expect that only small changes to the calibration hyperparameters are required to achieve a slight increase or decrease in confidence level. Furthermore, we can also simplify the problem by considering only a scaled version of the regression hyperparameters \(^{R}\) to compute \(_{}\), which would reduce the optimization problem to a line search.

## 7 Experiments

In this section, we apply and analyze our approach using a toy data set and different regression benchmark data sets from the UCI repository. In the supplementary material, we also compare our

Figure 2: Centered 99 \(\%\) confidence intervals (shaded regions) obtained with our method, vanilla GPs recalibrated using the approaches of Kuleshov et al. (2018) and Vovk et al. (2020), a naive vanilla GP, the point-predictor (variance-free) approach proposed in Marx et al. (2022), and a naive fully Bayesian GP. Solid lines represent the predictive mean, crosses represent data used to train the base model, and squares represent calibration (holdout) data. Our approach yields a model that is both well-calibrated and sharp. This is because of the added flexibility that comes from being able to also change the lengthscale to design a calibrated model.

approach to that of Capone et al. (2022) when used to obtain uniform error bounds and apply our method to two different Bayesian optimization problems.

The goal is for our approach to obtain a sharp calibrated regression model for each data set in the calibrated regression experiments. We test our approach on various data sets and compare it to the state-of-the-art recalibration approaches by Kuleshov et al. (2018) and Vovk et al. (2020), the point predictor (posterior variance-free) approach proposed in Marx et al. (2022), as well as the check-score-based approach of Kuleshov and Deshpande (2022). The technique proposed by Kuleshov et al. (2018) essentially multiplies the vanilla posterior standard deviation \(_{_{u}}(^{R},)\) with the recalibrated z-score, such that the confidence level observed on the calibration data matches that of the desired confidence level. Vovk et al. (2020) employ a similar approach, except that random interpolation is employed to compute new scaling values. The point predictor-based method proposed in Marx et al. (2022) discards the posterior standard deviation \(_{_{v}}(^{R},)\) and computes a constant scalar that is added to the predictive mean and used to compute quantiles everywhere within the input space. The method of Kuleshov and Deshpande (2022) trains a neural network using a quantile loss, which takes base quantiles as inputs and returns new, recalibrated quantiles. As a recalibrator for the method of Kuleshov and Deshpande (2022), we employ the same neural network architecture suggested in their paper, trained over \(200\) epochs, with additional pretraining over \(2000\) epochs using a single dataset for the UCI experiments. In all experiments except kin8nm and Facebook comment volume 2, we employ standard GPs with automatic relevance determination squared-exponential (ARD-SE) kernels and zero prior mean as base models, trained using log-likelihood maximization. For the kin8nm and Facebook comment volume 2 datasets, we employ sparse GPs (Titsias, 2009) with zero prior mean, ARD-SE kernels, and \(300\) inducing points.

### Toy Data Set

The first regression data set corresponds to a one-dimensional synthetic data set, where the results can be easily displayed visually. The main purpose of this section is to give an intuition as to how our approach computes confidence intervals compared to other techniques. We investigate the performance of our approach and compare it to other methods when employed to compute centered \(99\%\) confidence intervals. We observe that the confidence intervals obtained with our approach peak less strongly far away from the data while being tight near the data compared to all other approaches except the one of Kuleshov and Deshpande (2022). This is because we allow the lengthscale to change to obtain a calibrated model. In contrast, all other methods except that of Kuleshov and Deshpande (2022) scale the standard GP posterior variance without changing hyperparameters. The method of Kuleshov and Deshpande (2022), which uses a neural network as a recalibrator, offers additional flexibility, resulting in sharper confidence intervals. However, calibration is not explicitly enforced during training, resulting in poor calibration. The results are depicted in Figure 2.

### Benchmark Data Sets

We now experiment with seven different regression data sets from the UCI repository, two containing over eight thousand data points and requiring sparse GP approximations. The training/calibration/test split is \(0.6\), \(0.2\), and \(0.2\) for all data sets except the Facebook comment volume 2 data set, which contains over \(80000\) data points, and where the split is \(0.08\), \(0.02\), and \(0.9\). For the approach of Kuleshov and Deshpande (2022), we follow the steps in their paper and limit the calibration data size to \(500\).

We assess performance by employing diagnostic tools commonly used to assess calibration and sharpness (Kuleshov et al., 2018; Marx et al., 2022; Gneiting et al., 2007)). The score used to quantify calibration is the calibration error (Kuleshov et al., 2018), given by

\[(_{_{u}}(^{R},),,_{ _{u}}(,))=_{j=1}^{m}(p_{j}-_{j} )^{2},\] (8)

where \(p_{j}\) corresponds to the \(j\)-th desired confidence level, chosen, e.g., evenly spaced between \(0\) and \(1\), and \(_{j}\) is the observed confidence level, i.e.,

\[_{j}=^{*}\ \  y_{t}^{*}_{p_{j }}_{_{u}}(_{p_{j}}_{t}^{*}),t=1,...,T\} |}{T}.\] (9)Here the superscript \(*\) denotes test inputs and outputs, \(T\) denotes the total number of test points, and \( y_{t}^{*}_{_{i}}(^{R},_{t}^{*})-y _{t}^{*}\). We employ \(m=21\) evenly spaced values between \(0\) and \(1\) for \(p_{j}\). To measure sharpness, we employ the average length of the \(95\%\) confidence interval, the average standard deviation of the predictive distribution, and the average negative log-likelihood of the predictions (Gneiting et al., 2007; Marx et al., 2022). Note that since every model outputs a quantile for any desired calibration level, the corresponding negative log-likelihood and average standard deviation are well specified. These are computed by employing the cumulative distribution function, obtained by inverting the quantile function specified by each model.

We carried out each experiment \(100\) times and report the resulting average expected calibration error, standard deviation, negative log-likelihood, and length of the centered 95% confidence intervals in Table 1. Our approach performs best or marginally worse than all other calibration approaches regarding expected calibration error. This is to be expected from Theorem 5.4. Furthermore, it outperforms all approaches except that of Kuleshov and Deshpande (2022) in sharpness. However, the improved sharpness of the method of Kuleshov and Deshpande (2022) comes at the expense of calibration.

   Data set & Metric & Ours & RK & RV & RM & NN & B \\   & ECE & 0.003 & **0.0029** & **0.0029** & **0.0029** & 0.0056 & 0.041 \\  & STD & **0.16** & 0.31 & 0.3 & 0.33 & 0.22 & 1.9 \\  & NLL & 0.21 & 0.39 & 0.4 & 0.42 & **-0.24** & 1.6 \\  & 95\% CI & 0.76 & 1.4 & 1.4 & 1.4 & **0.73** & 7.4 \\   & ECE & 0.0044 & **0.0043** & 0.0044 & **0.0043** & 0.0081 & 0.039 \\  & STD & 0.16 & 0.5 & 0.47 & 0.5 & **0.14** & 2.8 \\  & NLL & 0.26 & 0.68 & 0.69 & 0.68 & **-2** & 2 \\  & 95\% CI & 0.76 & 2.3 & 2.3 & 2.3 & **0.3** & 11 \\   & ECE & 0.0036 & **0.0035** & **0.0035** & **0.0035** & 0.0053 & 0.044 \\  & STD & **0.13** & 0.38 & 0.38 & 0.38 & 0.29 & 2.8 \\  & NLL & 0.032 & 0.63 & 0.64 & 0.63 & **0.02** & 2 \\  & 95\% CI & **0.6** & 1.7 & 1.8 & 1.7 & 0.96 & 11 \\   & ECE & **0.00047** & **0.00047** & **0.00047** & **0.00047** & 0.0067 & 0.0058 \\  & STD & **0.54** & 1 & 1 & 0.88 & 0.72 & 1.4 \\   & NLL & 1.2 & 1.3 & 1.3 & 1.3 & **-0.36** & 1.4 \\   & 95\% CI & **2.1** & 3.8 & 3.8 & 3.9 & 2.8 & 5.4 \\   & ECE & 0.00071 & **0.00064** & **0.00064** & **0.00064** & 0.00076 & 0.032 \\  & STD & **0.25** & 0.64 & 0.64 & 0.64 & 0.57 & 2.8 \\   & NLL & **0.72** & 1.1 & 1.1 & 1.1 & 0.85 & 2 \\   & 95\% CI & **0.93** & 2.5 & 2.5 & 2.5 & 2.1 & 11 \\   & ECE & **0.00016** & **0.00016** & **0.00016** & **0.00016** & 0.00053 & 0.028 \\  & STD & **0.074** & 0.12 & 0.12 & 0.12 & 0.098 & 0.4 \\   & NLL & -0.54 & -0.65 & -0.65 & -0.63 & **-0.76** & 0.1 \\   & 95\% CI & **0.26** & 0.47 & 0.47 & 0.48 & 0.44 & 1.6 \\   & ECE & 0.00044 & **0.00043** & **0.00043** & 0.00045 & 0.0089 & 0.044 \\   & STD & **0.068** & 0.18 & 0.18 & 0.18 & 0.18 & 1.2 \\   & NLL & 3.6 & -1.3 & -1.3 & -1.2 & **-2.3** & 1.2 \\   & 95\% CI & **0.6** & 1.7 & 1.7 & 1.7 & 3.4 & 4.6 \\   

Table 1: Expected calibration error and sharpness of different methods over \(100\) repetitions per experiment. We report the expected calibration error (ECE), the average predictive standard deviation (STD), negative log-likelihood (NLL) and 95% confidence interval width (95% CI) obtained with our approach, vanilla GPs recalibrated using the methods of Kuleshov et al. (2018) (RK) and Vovk et al. (2020) (RV), the variance-free approach proposed in Marx et al. (2022) (RM), and the neural network-based recalibrator of Kuleshov and Deshpande (2022) (NN). We additionally report performance for the base model (B), which corresponds to a vanilla GP without the holdout data. Lower is better for all metrics. In all experiments except the Facebook2 dataset, our method is sharpest compared to all other methods except that of Kuleshov and Deshpande (2022). However, Kuleshov and Deshpande (2022) performs more poorly in terms of expected calibration error.

Conclusion

We have presented a calibration method for Gaussian process regression that leverages the monotonicity properties of the kernel hyperparameters to obtain sharp calibrated models. We show that, under reasonable assumptions, our method yields an accurately calibrated model as the size of data used for calibration increases. When applied to different regression benchmark data sets, our approach was shown to be competitive in sharpness compared to state-of-the-artcalibration methods without sacrificing calibration performance. It is worth stressing that, though the tools presented here emerge naturally from a Gaussian process setting, we do not require our predictor to be a Gaussian process to obtain theoretical guarantees. In future work, we aim to leverage similar monotonicity characteristics to get sharply calibrated models using tools different from Gaussian processes. Furthermore, we aim to experiment with inducing variables as hyperparameters when optimizing the models for sharpness.