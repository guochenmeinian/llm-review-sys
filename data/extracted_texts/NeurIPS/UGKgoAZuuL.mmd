# Bridging Inter-task Gap of Continual Self-supervised Learning with External Data

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent research on Self-Supervised Learning (SSL) has demonstrated its ability to extract high-quality representations from unlabeled samples. However, in continual learning scenarios where training data arrives sequentially, SSL's performance tends to deteriorate. This study focuses on Continual Contrastive Self-Supervised Learning (CCSSL) and highlights that the absence of contrastive learning on inter-task data, due to the unavailability of historical samples, leads to a significant drop in performance. To tackle this issue, we introduce a simple and effective method called BGE, which **B**ridges the inter-task **G**ap of CCSSL using **E**xternal data from publicly available datasets. BGE enables the contrastive learning of each task data with external data, allowing relationships between them to be passed along the tasks, thereby facilitating _implicit_ inter-task data comparisons. To overcome the limitation of the external data selection and maintain its effectiveness, we further propose the One-Propose-One algorithm to collect more relevant and diverse high-quality samples from the chosen external data while filtering out distractions from the out-of-distribution data. Experiments show that BGE can generate better discriminative representation in CCSSL, especially for inter-task data, and improve classification results with various external data compositions. Additionally, the proposed method can be seamlessly integrated into existing continual learning methods yielding significant performance improvement.

## 1 Introduction

In recent years, deep neural networks [13; 22; 35] have achieved great success, but plenty of works are under the assumption that all data are available simultaneously for training. In practical scenarios, acquiring the entire dataset at once is often challenging due to data being constantly updated. In this case, training the network continually suffers from catastrophic forgetting , meaning that the network severely forgets old task knowledge after learning the new one. Hence, continual learning investigates methods to train networks incrementally while mitigating catastrophic forgetting.

Although continual learning has been widely studied and numerous effective methods [32; 36; 40] have been proposed, most existing research remains focused on supervised learning, with Continual Contrastive Self-Supervised Learning (CCSSL) receiving relatively little attention. However, studying CCSSL is equally significant.

To prevent catastrophic forgetting, prior CCSSL works CaSSLe , PFR , and POCON  use knowledge distillation, while CPPF  incorporates prototype clustering. In this paper, we highlight an important but generally overlooked issue in these works: _Comparisons of inter-task data are absent_. Specifically, a widely accepted opinion in continual learning is that if the sum of each task's loss is minimized, then continual learning's performance reaches its upper bound: _joint learning_. However, in CCSSL, even if each task's loss is minimized, there is still a gap between jointlearning. Because joint learning requires any sample pair in the entire dataset to participate in the contrastive loss computation. In contrast, in continual learning, inter-task data are unavailable to each other, meaning this aspect of the contrastive loss is never computed and optimized. This omission increases the likelihood of inter-task class confusion, as illustrated in Figure 1 Right, despite classes from four different tasks having distinctly different semantics, they still show confusion in prior methods Fine-tune and CaSSLe . In contrast, our method and joint training consider inter-task comparisons and can better distinguish them.

Since we could not directly use data from other tasks for inter-task comparisons, we would like to compensate for these comparisons with the help of external data. Some prior works  have explored using external data for continual learning. GD  and ZSCL  use external data for distillation to stabilize the feature space, while requiring extensive external data and high computational costs. ST  employs external data as additional training data, but as a supervised method, it requires pseudo-labels, making it less robust to out-of-distribution (OOD) data. Tang et al.  enhance exemplar diversity with external data. Existing methods focus on using external data in supervised learning, but given that CCSSL does not require labels for training, we propose using external data in CCSSL, which avoids the need for pseudo-labels and is more generalizable and robust to OOD data. Besides, our motivation is to improve feature space by compensating for absent comparisons rather than merely stabilizing it, and it does not require extensive external data.

In summary, we propose incorporating publicly available external data into training to compensate for the absent inter-task comparisons, as shown in Figure 1 Left. When the external dataset is sufficiently large, it is reasonable to assume a high probability that some external data share similar features with the task data, even if they are in different classes. By incorporating these high-quality external data into CCSSL, the data from each task can be compared with them. enables the inter-data relationship to be passed along the tasks, thereby constructing implicit inter-task comparisons. Further, considering that external data in open-world scenarios may contain extensive OOD data that is not beneficial for task training, we propose the One-Propose-One (OPO) sampling algorithm, to sample high-quality external data that are relevant to tasks and sufficiently diverse without any hyperparameters.

Experiments demonstrate that BGE can be seamlessly integrated into existing methods, resulting in significant performance improvement. We also point out that although it may seem unsurprising that network performance improves with more training data, this improvement is not due to richer input features, because when we add equal external data into joint training, the performance doesn't improve even sometimes decreases. Instead, BGE compensates for the absent comparisons caused by

Figure 1: **Left: Overview of our method BGE. In typical CCSSL methods, the inter-task data pairs are incomparable. We employ an external dataset to complement these missing comparisons, effectively bridging the inter-task gap. Right: t-SNE  visualization of four classes belonging to different tasks in continual learning. Compared to prior methods Fine-tune and CaSSLe , we make the inter-task data more separable.**

inter-task data unavailability, which is much more meaningful in continual learning. Our contributions can be summarized as follows:

* We point out that existing methods overlook the issue of inter-task data comparisons, and propose BGE to incorporate external data into training to address this gap.
* We propose the One-Propose-One (OPO) sampling algorithm to sample external data that are relevant to tasks and sufficiently diverse, while also filtering out OOD data that are not beneficial for learning.
* Experiments show that BGE can be seamlessly integrated into existing CCSSL methods and consistently yields significant improvement.

## 2 Related work

Self-Supervised Learning (SSL)SSL trains the network without the need for supervised signals. One of the prominent branches is contrastive learning [5; 8; 9; 10; 21; 23; 53]. The objective of contrastive learning can be roughly explained as reducing the distance between positive pairs while enlarging it between negative pairs. SimCLR  simply follows this objective but requires a large batch size. MoCo [10; 23] introduces a momentum encoder and a negative sample dictionary to solve this problem. SwAV  and Barlow Twins  introduces prototype comparisons and cross-decorrelation loss, respectively. Then BYOL  and SimSiam  can conduct contrastive learning without negative samples. However, all these methods assume that a large dataset is available for pre-training, which is often impractical in real-world scenarios where data acquisition is incremental. Therefore, we research a continual method, which is more practical.

Since no labeling requirement, incorporating external data into SSL is straightforward. Prior long-tailed SSL works [3; 28] leverage external data to balance head and tail classes. Instead, we extend the exploration to continual learning, aiming to use external data to compensate for the absent inter-task comparisons while further preventing catastrophic forgetting.

Continual learningContinual learning allows the network to learn from sequentially arriving data and prevent catastrophic forgetting. Existing continual learning methods can be categorized into three groups, which are 1) Regularization-based methods [1; 29; 32; 34; 50; 54] add additional regularization constraints such as knowledge distillation [14; 32; 50] or limiting important parameters update [1; 29; 34; 54] to network training. 2) Replay-based methods [4; 26; 40; 43; 55] save few representative data from old tasks called exemplars to recover the distribution of old data when the new task is trained. 3) Architecture-based methods [15; 36; 37; 41; 51], which adjust the architecture or parameters of the network during each task training. Currently, most continual learning methods still focus on supervised learning. While some of them [6; 33; 44] draw on the idea of contrastive learning, there are still few works consider continual learning without any supervision. Among them, CaSSLe , PFR, and POCON use distillation, and CPPF adds clustering to form a more complete framework. Sy-CON  also reveals the distinction between CCSSL and joint training, but it only additionally passes current task data into the old network to get more diverse intra-task negative features, which still fails to provide effective inter-task comparisons. Thus it underperforms in most contrastive learning frameworks. Compared to them, we introduce external data to facilitate implicit inter-task comparisons to solve the problem of absent inter-task comparisons.

## 3 Proposed method

### Preliminary

Contrastive Self-Supervised Learning (CSSL)In Self-Supervised Learning (SSL), the dataset \(D\) contains only \(n\) image inputs \(\{x_{1},x_{2},...,x_{n}\}\) without labels. SSL trains a network \(f_{}\) parameterized by \(\) to map these inputs to embeddings \(\{z_{1},z_{2},...,z_{n}\}\). Many well-known SSL works [5; 8; 21; 23; 53] use contrastive learning framework. In contrastive learning, a random augmentation function \(A\) is pre-designed. Given an input \(x\), two augmented views \((x_{a},x_{b})\) are obtained by applying \(A\) twice. Subsequently, embeddings \(z_{a}=f_{}(x_{a})\) and \(z_{b}=f_{}(x_{b})\) are passed through a projector \(h_{^{}}\) parameterized by \(^{}\) to get \(z^{}_{a}=h_{^{}}(z_{a})\), \(z^{}_{b}=h_{^{}}(z_{b})\), which are involved in \(_{SSL}\). In essence,\(_{SSL}\) expects the network to output similar embeddings for two views of the same input (i.e. positive pair), while ensuring that embeddings from views of different inputs (i.e. negative pair) are dissimilar.

Continual CSSL (CCSSL)In CCSSL setting, The overall dataset \(D\) is divided into multiple tasks. Assuming that \(T\) tasks \(\{_{1},_{2},...,_{T}\}\) are to be learned, \(D\) can be divided into \(\{D_{1},D_{2},...,D_{T}\}\), where \(D_{i} D_{j}=, i,j\{1:T\}\). Also as SSL, for each task \(_{t}\), \(D_{t}\) is only composed of \(n_{t}\) images \(\{x_{1},x_{2},...,x_{n_{t}}\}\) without labels. Continual learning requires the network to learn knowledge as each task's data arrives sequentially, with dataset \(D_{i}\) only available at \(_{i}\). The optimization objective is to continually train the network parameter \(\) to satisfy every task, which is defined as:

\[_{t=1}^{T}_{(x_{a},x_{b}) A(D_{t}) }_{SSL}(h_{^{}}(f_{}(x_{a})),h_{^{}} (f_{}(x_{b})))\] (1)

### Revising and improving CCSSL via external data

Typical contrastive learning paradigms [8; 23; 53] can be generalized as reducing distances between positive pairs and enlarging them between negative pairs on feature hyperspheres. Adjusting the interrelationships of sample pairs in this way enables the network to effectively represent features [27; 49]. However, in CCSSL, the data is divided by tasks. During the learning process of task \(_{t}\), data from other tasks are unavailable. This prevents adequate tuning of inter-sample relationships, resulting in suboptimal network training. We identify two reasons for this suboptimality: **1) The network rapidly forgets knowledge about old data due to catastrophic forgetting**, so their features cannot be well extracted in subsequent tasks. **2) Insufficient learning about each task occurs** because data from one task cannot act as negative samples for another task. While prior works address problem 1 through techniques like distillation [16; 18; 19] and clustering , problem 2 remains underexplored. However, we argue that this is unreasonable, and solving problem 2 is equally important.

Prior works [20; 32] widely agree that in the ideal case, continual learning can perform up to joint learning, wherein no forgetting occurs and each task reaches optimality. However, in CSSL, even if no forgetting occurs, there is still an optimization gap between continual and joint learning due to the absence of inter-task data comparisons in the training objective. Unlike supervised learning which guides the network through labels, CSSL relies on data interactions for network learning. When data is incomplete, the training objective also becomes incomplete. For better comprehension, we can decompose the joint training contrastive loss into two terms as in Eq. 2, representing the comparisons of intra-task and inter-task data, denoted as \(_{intra}\) and \(_{inter}\), respectively. \(_{intra}\) is the training objective of the conventional CCSSL, also referred to as \(_{continual}\). However, for input \(x D_{t}\) in task \(_{t}\), negative samples come exclusively from \(D_{t}\) rather than the overall dataset \(D\), making direct comparisons between inter-task data infeasible. Consequently, \(_{inter}\) can not be computed and optimized in continual learning forever, resulting in a \(_{inter}\) gap between \(_{continual}\) and \(_{joint}\).

\[_{joint}&=_{t =1}^{T}_{(x_{a},x_{b}) A(D_{t})}_{ SSL}(h_{^{}}(f_{}(x_{a})),h_{ ^{}}(f_{}(x_{b})))}^{}\\ &+_{)}{e_{a} A(D _{t})}}_{SSL}(h_{^{}}(f_{}(x_{a} )),h_{^{}}(f_{}(x_{b})) )}_{_{inter}}\] (2)

We argue that the lack of optimization for \(_{inter}\) leads to confusion between inter-task data. Figure 1 Right compares the t-SNE visualizations of features from 4 CIFAR100 classes under joint and 10 tasks continual training (4 classes belong to different tasks during continual training). Compared to the joint-trained network, the continually trained network shows poor clustering and severe class boundary confusion. More experiments about inter-task confusion can be found at Appendix A.2.1. Despite CaSSLe  employing distillation to consolidate old knowledge, the issue of inter-task class boundary confusion remains. To address the overlooked problem of \(_{inter}\), a straightforward idea is to save exemplars for each task. However, this may raise serious privacy concerns. We therefore explore an alternative method to optimize \(_{inter}\) without exemplars and protect the discriminative class boundaries. Figure 0(c) shows the feature distribution of our method, with all 4 inter-task classes better distinguished, and the overall distribution closer to joint training.

To compensate for \(_{inter}\), bridging the gap of inter-task comparisons is essential. This requires introducing additional comparisons into each task, implying extra data incorporation. Under the constraints of continual learning, simultaneous access to data from multiple tasks is infeasible. Therefore, the idea emerges to incorporate publicly available external data into CCSSL to address the lack of inter-task comparisons. Each task's data can be directly compared with external data, enabling relationships between data to be passed along the task sequence. Moreover, using external data better protects privacy, and the costs of obtaining unlabeled data from public data sources are extremely low. We thus propose our method BGE, meaning **B**ridging the inter-task comparison **G**ap with **E**xternal data, as shown in Figure 1 Left. BGE incorporates external data into each task's training except the first one, and resamples part of them after each task using our sampling algorithm ( detailed in Section 3.3). This external data acts as a bridge for inter-task comparisons, constructing implicit comparisons for inter-task data. For task \(_{t}\), with \(D_{e}^{t-1}\) as the external data sampled after task \(_{t-1}\), the training objective is defined as:

\[_{t}=_{(x_{a},x_{b}) A(D_{t} D_{e}^{t-1} )}_{SSL}(h_{^{}}(f_{}(x_{a} )),h_{^{}}(f_{}(x_{b})))\] (3)

Incorporating external data aligns the optimization objective of continual learning more closely with Eq. 2, enhancing the mutual understanding of inter-task classes.

### One-Propose-One (OPO) sampling

While abundant external data features generally cover in-task data comprehensively, incorporating all external data into continual learning is impractical due to computational constraints. Additionally, open-world external data may include substantial task-irrelevant out-of-distribution (OOD) data, which is unhelpful for training. Therefore, a sampling algorithm is needed to select high-quality external data. We observe that \(_{inter}\) includes comparisons of current task data \(D_{t}\) with both old task data \(D_{1:t-1}\) and future task data \(D_{t+1:T}\). So sampled external data should ideally proxy for both old and future task data. To represent old data, sampled data should have similar features to them, while representing future data requires imaginative sampling. Therefore, our sampling algorithm is based on both proximity and diversity considerations, and integrates these two aspects into a single objective without any hyperparameters. We noted that prior sampling algorithms [3; 28] for long-tailed learning also consider proximity and diversity, but they require hyperparameters selection.

We measure proximity using the cosine distance between sample features. On the other hand, prior work  indicates that to avoid collapse, contrastive learning methods tend to map all inputs to a uniform distribution within the feature hypersphere (i.e. uniformity). Thus we assume that the entire distribution of the current task data approximately covers the hypersphere, ensuring diversity. Based on the above, we propose a sampling algorithm called _One-Propose-One (OPO)_ as depicted in Algorithm 1. After training each task \(_{t}\), OPO constructs the external dataset \(D_{e}^{t}\), which is then incorporated in training task \(_{t+1}\). Specifically, OPO considers that each in-task sample can equally propose an external sample with the closest feature distance to itself and has not been proposed. Given the current task budget \(K_{t}\), we collect all proposed samples as a candidate set \(D_{c}\), and select the \(K_{t}\) minimum distance samples to be added to the external dataset \(D_{e}^{t}\). We follow iCaRL 's exemplar update algorithm, maintaining an equal budget for each task within the total budget \(K\). OPO ensures proximity and diversity without hyperparameters, maintaining similarity to old data and adequate coverage of future data features.

## 4 Experiments

### Experimental setup

Dataset setupWe conduct experiments with the following datasets: 1) **CIFAR100**, which contains 100 classes, each with 500 train images and 100 test images. Each image is 32\(\)32 pixels. We follow the class incremental learning setting to split the classes equally by the number of tasks. Experiments are conducted under 4 tasks and 10 tasks settings, wherein each task contains 25 classes and 10 classes. 2) **ImageNet100**, which consists of 100 classes selected from ImageNet , with a total of 130K images of 224\(\)224 pixels. It is equally split under 5 tasks and 10 tasks settings.

External dataset setupFor CIFAR100, the selected external datasets include **CIFAR10**, **Places365\({}_{test}\)** (the test set of Places365 ) and **ImageNet-R**, among them, Places365\({}_{test}\) and ImageNet-R are OOD for CIFAR100. CIFAR10 contains 50,000 images with 32\(\)32 pixels in 10 classes. Places365 is a scene recognition dataset with its test set containing 328,500 images of various scenes. ImageNet-R contains 24,000 images featuring art, cartoons, and other styles. We resize both Places365\({}_{test}\) and ImageNet-R to 32\(\)32 pixels. We consider three compositions of external datasets, **CIFAR** (CIFAR10), **CP** (CIFAR10+Places365\({}_{test}\)) and **CPI** (CIFAR10+Places365\({}_{test}\)+ImageNet-R)

For ImageNet100, the external datasets include **ImageNet900**, **Places365** and **DomainNet**. ImageNet900 is all data in ImageNet excluding ImageNet100, totaling 1.1 million images. Places365 contains 1.8 million images, and DomainNet contains 0.6 million images of 6 domains. They are also used here as OOD data. All data are 224\(\)224 pixels. We consider three compositions of external datasets, **IN** (ImageNet-900), **INP** (ImageNet900+Places365) and **IND** (ImageNet900+DomainNet).

BaselinesWe compare the original performance of existing exemplar-free CCSSL methods to their performance when with BGE. The methods we compare include 1) **Fine-Tune (FT)**: Sequentially training the network with data from each task without additional prevention of catastrophic forgetting. 2) **CaSSLe**: Introducing a distillation loss between the current model and the old model in the form of contrastive loss. 3) **PFR**: Addressing catastrophic forgetting based on functional regularization . We slightly optimized its network structure and training procedure.

Training and evaluation setupUnless specified otherwise, all experiments employ Barlow Twins  as the contrastive learning framework and Resnet18  as the backbone. The sampling budget is uniformly set at 10K. For evaluation, we follow [16; 18; 19] to report the linear evaluation accuracy of the final network across all classes as the evaluation metric. For other setups see Appendix A.1.

### Results

Performance improvement on prior methodsWe compare the performance improvement BGE yields to the base methods when using different external data compositions. Table 1 shows that on CIFAR100, BGE can consistently and significantly improve base methods. It is worth noting that as the number of tasks increases, BGE yields even greater improvement, with improvement of 1.5%-3.5% for 4 tasks and 2.5%-7% for 10 tasks. This is also in line with our motivation, as an increasing number of tasks results in more missing inter-task data comparisons.

Moreover, across different external dataset compositions, we observe that CIFAR yields the most significant improvement. This is attributed to the CIFAR10 dataset best matches the distribution of CIFAR100, thereby offering highly relevant features, even if their classes do not intersect. When incorporating datasets like Places365 or ImageNet-R, which are OOD for CIFAR100, the improvement decreases. Thanks to our OPO sampling algorithm can well resist the harm of OOD data (detailed in Section 4.3). On ImageNet100, the performance improvement is shown in Table 2, showcasing a similar improvement regularity to that observed on CIFAR100. BGE achieves 1.5%-4% improvement for 5 tasks and 5%-7.5% improvement for 10 tasks. More experiments see Appendix A.2.7.

We also emphasize that although it might seem intuitive that network performance would improve with richer data because of richer features, BGE yielded improvement does not simply stem from using more data. In Table 1 and Table 2, we incorporate an equal amount of external data into joint training. However, the results do not improve, and may even decrease when the external data contains OOD samples. We believe this is because incorporating irrelevant external data into the training process causes the model to allocate some capacity to learning these unrelated data, thereby weakening its focus on the in-task data. Hence, the learning of external data can not directly contribute to the learning of in-task data.

Long task sequence experimentsWe conduct experiments with 100 tasks on CIFAR100, which means one task only contains one class, to verify the effectiveness of BGE on long task sequences. We set the sampling budget to 1000. Figure 2 shows the performance of different base methods with or without BGE as the learned tasks increase. On one hand, BGE improves the final network performance, especially evident in FT and PFR. On the other hand, the network's performance increases even more rapidly with BGE, indicating that the network's generalization ability to unseen

    &  &  &  \\   & 4tasks & 10tasks & 4tasks & 10tasks & 4tasks & 10tasks \\  FT & 56.19 & 49.36 & 56.19 & 49.36 & 56.19 & 49.36 \\ FT+_BGE_ & 59.49(+3.30) & 56.62(+7.26) & 58.69(+2.50) & 55.14(+5.78) & 58.71(+2.52) & 55.74(+6.38) \\  CaSSLe  & 60.04 & 53.89 & 60.04 & 53.89 & 60.04 & 53.89 \\ CaSSLe+_BGE_ & 62.38(+2.34) & 58.14(+4.25) & 61.72(+1.68) & 56.92(+3.03) & 61.51(+1.47) & 56.36(+2.47) \\  PFR  & 60.92 & 55.57 & 60.92 & 55.57 & 60.92 & 55.57 \\ PFR+_BGE_ & 64.37(+3.45) & 61.02(+5.45) & 63.15(+2.23) & 60.31(+4.74) & 62.88(+1.96) & 59.99(+4.42) \\    \\  Joint & 68.09 & 68.09 & 68.09 & 68.09 \\ Joint+_ED_ & 68.15(+0.06) & 67.11(-0.98) & 68.19(+0.10) \\   

Table 1: Comparison of BGE’s performance improvement on CIFAR100. CIFAR, CP, and CPI are different external dataset compositions. Performance was evaluated by linear evaluation accuracy of the final network. We equally divided classes into 4 tasks and 10 tasks. BGE consistently improves base methods across different external dataset compositions. As for Joint training, ED represents adding equivalent external data, which does not improve the performance.

    &  &  &  \\   & 5tasks & 10tasks & 5tasks & 10tasks & 5tasks & 10tasks \\  FT & 64.02 & 56.72 & 64.02 & 56.72 & 64.02 & 56.72 \\ FT+_BGE_ & 68.20(+4.18) & 64.16(+7.44) & 67.84(+3.82) & 64.08(+7.36) & 69.06(+5.04) & 65.00(+8.28) \\  CaSSLe  & 70.02 & 60.68 & 70.02 & 60.68 & 70.02 & 60.68 \\ CaSSLe+_BGE_ & 72.46(+2.44) & 66.80(+6.12) & 71.44(+1.42) & 65.94(+5.26) & 72.68(+2.66) & 67.10(+6.42) \\  PFR  & 70.14 & 63.12 & 70.14 & 63.12 & 70.14 & 63.12 \\ PFR+_BGE_ & 72.52(+2.38) & 69.28(+6.16) & 72.94(+2.80) & 68.40(+5.28) & 72.60(+2.46) & 68.94(+5.82) \\   \\  Joint & 80.44 & 80.44 & 80.44 & 80.44 \\ Joint+_ED_ & 80.24(-0.20) & 79.70(-0.74) & 78.88(-1.56) \\   

Table 2: Performance improvement yielded by BGE on ImageNet100. IN, INP, and IND are different external dataset compositions. ED represents adding equivalent external data in joint training.

[MISSING_PAGE_EMPTY:8]

more information, and fewer images will satisfy the proximity. 2) With a larger quantity of external data, there are more potentially high-quality data, facilitating better sampling.

Effect of additional positive and negative pairsWe further investigate whether additional positive or negative pairs provided by BGE contribute more to performance improvement. We conduct experiments based on CaSSLe  on the CIFAR100 4 tasks setting. Because this experiment requires explicitly calculating the loss incurred by each positive and negative pair, we convert the framework to SimCLR . We masked the additional positive or negative pairs in Table 4. The results show that both types of pairs improve performance individually, and negative pairs yield more significant improvement, supporting our emphasis that the impact of absent inter-task comparisons is severe but neglected. But positive pairs also yield performance improvement, which is because high-quality external data have feature intersections with in-task data, proving that external data can prevent catastrophic forgetting as well. With the synergistic effect of both, the improvement reaches the highest.

Experiments with only OOD external dataIn the experiments presented in Table 1 and Table 2, all external data contain some amount of ID data. To assess BGE's performance without any ID data in the external dataset, we conduct experiments on CIFAR100 4 tasks based on PFR, as shown in Table 5. The external dataset is only composed of ImageNet-R or Places365\({}_{test}\). In joint training, these data are detrimental. While in continual training, BGE consistently improves the base method by nearly 2%, regardless of the composition of OOD data used. It indicates that the performance improvement from BGE does not only come from imitating in-task data features, but also from introducing similar additional comparisons into each task itself, which is beneficial for constructing implicit inter-task comparisons. Even if the external data has few recognizable similar features to the in-task data, the network can still try its best to mine valuable knowledge from external data to compensate for inter-task comparisons.

BGE with more types of datasetsWe validate the effectiveness of BGE across more aspects of external datasets. Table 6 presents the results when using GenImage , a dataset of generated images; CC3M , a dataset sourced from the Internet; and CUB200 , a fine-grained bird dataset as external dataset. Experiments with GenImage and CC3M demonstrate BGE's effectiveness with both model-generated and real-world Internet data, demonstrating its practical value. Since CUB200 is fine-grained and lacking in diversity, it is extremely unfriendly to BGE, yet BGE can still improve the base method.

## 5 Conclusion

In this paper, we address a commonly overlooked but severe issue in Continual Contrastive Self-Supervised Learning (CCSSL): the lack of inter-task comparisons. To tackle this, we propose our method BGE to incorporate external data into training, bridging the inter-task gap and facilitating implicit inter-task data comparisons. We also design the One-Propose-One sampling algorithm to select high-quality external data and filter out irrelevant OOD data. BGE can be seamlessly integrated into existing methods and yield significant improvement.

  External datasets & Acc \\  N/A & 60.92 \\ GenImage  & 64.37 \\ CC3M  & 63.53 \\ CUB200  & 62.42 \\  

Table 6: Performance of BGE when choosing more types of datasets.

   Negative & Positive & Acc \\   & & & 52.79 \\  & ✓ & 53.40 \\ ✓ & & & 55.61 \\ ✓ & ✓ & 56.21 \\   

Table 4: Comparison of additional positive and negative pairs’ effects.

    &  &  &  &  \\  ImageNet-R & & & & & \\  ✓ & & 60.92 & 62.85(+1.93) & 68.09 & 68.03(-0.06) \\  & ✓ & 60.92 & 62.81(+1.89) & 68.09 & 67.75(-0.34) \\ ✓ & ✓ & 60.92 & 62.88(+1.96) & 68.09 & 67.15(-0.94) \\   

Table 5: Effectiveness of BGE when external data are totally OOD.