# Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models

Geon Yeong Park\({}^{1*}\) Jeongsol Kim\({}^{1*}\) Beomsu Kim\({}^{2}\) Sang Wan Lee\({}^{1,2,3}\) Jong Chul Ye\({}^{2}\)

\({}^{1}\)Bio and Brain Engineering, \({}^{2}\)Kim Jaechul Graduate School of AI, \({}^{3}\)Brain and Cognitive Sciences Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea

\(*\), \(\): Co-first and Co-corresponding authors

{pky3436, jeongsol, beomsu.kim, sangwan, jong.ye}@kaist.ac.kr

###### Abstract

Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework for adaptive context control by modeling the posterior of context vectors. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing. Code: https://github.com/EnergyAttention/Energy-Based-CrossAttention.

## 1 Introduction

Diffusion models (DMs) have made significant advances in controllable multi-modal generation tasks , including text-to-image synthesis. The recent success of text-to-image diffusion models, e.g., Stable Diffusion , Imagen , etc., is mainly attributed to the combination of high-fidelity DMs with high-performance large language models (LMs).

Although text-to-image DMs have shown revolutionary progress, recent studies have shown that the current state-of-the-art models often suffer from semantic misalignment problems, where the generated images do not accurately represent the intended semantic contents of the text prompts. For example,  discovered the catastrophic neglect problem, where one or more of the concepts of the prompt are neglected in generated images. Moreover, for a multi-modal inpainting task with text and mask guidance,  found that the text-to-image DMs may often fail to fill in the masked region precisely following the text prompt.

Therefore, this work focuses on obtaining a harmonized pair of latent image representations and text embeddings, i.e., context vectors, to generate semantically aligned images. In order to mitigate the misalignment, instead of leveraging fixed context vectors, we aim to establish an _adaptive_ context by modeling the posterior of the context, i.e. \(p()\). Note that this is a significant departure from the previous methods which only model \(p()\) with _frozen_ context vectors encoded by the pretrained textual encoder.

Specifically, we introduce a novel energy-based Bayesian framework, namely energy-based cross-attention (EBCA), which approximates maximum a posteriori probability (MAP) estimates of context vectors given observed latent representations. Specifically, to model \(p()\), we first consider analogous \(p(K_{l}|Q_{t,l})\) in the latent space of the intermediate cross-attention layer of a denoising auto-encoder, i.e., cross-attention space. Here, \(K_{l}\) and \(Q_{t,l}\) correspond to the key and query in \(l\)-th layer at time \(t\) that encode the context from the text and the image representation, respectively. Inspired by the energy-based perspective of attention mechanism , we then formulate energy functions \(E(K_{l};Q_{t,l})\) in each cross-attention space to model the posterior. Finally, we create a correspondence between the context and representation by minimizing these parameterized energy functions. More specifically, by obtaining the gradient of the log posterior of context vectors, a nested hierarchy of energy functions can be implicitly minimized by cascading the updated contexts to the subsequent cross-attention layer (Figure 2).

Moreover, our energy-based perspective of cross-attention also allows zero-shot compositional generation due to the inherent compositionality of Energy-Based Models (EBMs). This involves the convenient integration of multiple distributions, each defined by the energy function of a specific text embedding. In practical terms, this amalgamation can be executed as a straightforward linear combination of cross-attention outputs that correspond to all selected editing prompts.

We demonstrate the effectiveness of the proposed EBM framework in various text-to-image generative scenarios, including multi-concept generation, text-guided inpainting, and compositional generation. The proposed method is training-free, easy to implement, and can potentially be integrated into most of the existing text-to-image DMs.

## 2 Preliminaries

### Diffusion Model

Diffusion models [36; 13; 37; 18; 16] aims to generate samples from the Gaussian noise by iterative denoising processes. Given clean data \(_{0} p_{data}(_{0})\), diffusion models define the forward sampling from \(p(_{t}|_{0})\) as \(_{t}=}_{0}+}_{t}\), where \(_{t}(0,)\), \(t\). Here, for the Denoising Diffusion Probabilistic Model (DDPM) , the noise schedule \(_{t}\) is an increasing sequence of \(t\), with \(_{t}:=_{i=1}^{t}_{t},\,_{t}:=1-_{t}\). The goal of diffusion model training is to obtain a neural network \(_{^{*}}\) that satisfies

\[^{*}=*{argmin}_{}_{_{t} p (_{t}|_{0}),_{0} p_{data}(_{0}), (0,)}[\|_{}(_{t},t)-\|_{2}^{2}],\] (1)

Figure 1: The energy-based cross-attention improves the semantic alignment between given text and the generated sample. The proposed method could be leveraged for multiple applications without additional training.

so that the reverse sampling from \(q(_{t-1}|_{t},_{^{*}}(_{t},t))\) is achieved by

\[_{t-1}=}}_{t}-}{ }}_{^{*}}(_{t},t)+_{t} _{t},\] (2)

where \(_{t}(0,)\) and \(^{2}\) is variance which is set as \(\) in DDPM. With iterative process, we can sample the image \(_{0}\) from initial sample \(_{T}(0,)\).

Since diffusion models require the iterative sampling on high dimensional space, they are computationally expansive and time consuming. To mitigate this limitation, Latent Diffusion Model (LDM)  has proposed diffusion processes on the compressed latent space using pre-trained auto-encoder. Furthermore, by introducing language model-based cross-attention to diffusion model's U-Net neural backbone , LDM enables token-based conditioning method such as text-to-image.

### Energy-based Perspective of Attention Mechanism

**Definition.** Given an \(N\)-dimensional vector \(\), its \(\) and \(\) are defined as

\[(,):=^{-1}(_{i=1}^{N}(v_{i})), ():=(-(,1)).\]

where \(v_{i}\) denotes the \(i\)-th element of \(\). For a given a matrix \(\), its \(_{i}()\) and \(_{i}()\) are understood as taking the corresponding operation along the \(i\)-th dimension of \(\). So, for instance, \(i=1\), \(_{i}()\) consist of column vectors that sum to \(1\).

**Energy model.** Following the success of the attention mechanism, several studies have focused on establishing its theoretical foundation. One promising avenue is interpreting the attention mechanism using the Energy-Based Model (EBM) framework. This line of research begins with recent research on modern Hopfield networks [30; 14], which gradually builds up to the self-attention mechanism of the Transformer model.

The Hopfield network is a dense associative memory model that aims to associate an input with its most similar pattern. Specifically, it constructs an energy function to model an energy landscape that contains basins of attraction around desired patterns. Recently, modern Hopfield networks [30; 6; 19] has introduced a new family of energy functions, which aim to improve its pattern storage capacity or make it compatible with continuous embeddings. To this end,  proposed the following energy function of a state pattern (query) \(^{d}\) parameterized by \(N\)-stored (key) patterns \(=[_{1},_{N}]^{d N}\) and \(>0\):

\[E(;)=^{T}-( ^{T},).\] (3)

Intuitively, the first term ensures the query remains finite, while the second term measures the individual alignment of the query with every stored pattern. Based on the Concave-Convex-Procedure (CCCP) [43; 30] derives the update rule for a state pattern \(}\) and time \(t\) as follows:

**Proposition 1** ().: _Define the update rule \(f:^{d}^{d}\) as follows:_

\[^{}=f()=\,(^{T} )\] (4)

_Then, the update rule decreases the loss (3) and converges globally. In another word, for \(^{(t+1)}=f(^{(t)})\), the energy \(E(^{(t)}) E(^{*})\) for \(t\) and a fixed point \(^{*}\)._

Figure 2: Comparison between the Stable-Diffusion and the proposed method. (**A**) The Stable-Diffusion uses fixed context embedding encoded by pre-trained CLIP. (**B**) The proposed method allows adaptive context embedding through energy-based context update (EBCU) and energy-based composition of queries (EBCQ).

Note that (4) is equivalent to a gradient descent update to minimize (3) with a step size \(=1\):

\[-_{}(;)=-(-(^ {T})).\] (5)

**Connection to the attention of the transformer.** Remarkably, this implicit energy minimization is closely related to the attention mechanism as shown in . To see this, suppose that \(_{i},_{i}^{d}\) is given as stored (key) and state (query) patterns, respectively. Let \(_{K},_{Q}^{d d_{H}}\) represent linear maps for \(_{i}\) and \(_{i}\), respectively. Then, we introduce \(_{i}=_{K}^{T}_{i}^{d_{H}}\) and \(_{i}=_{Q}^{T}_{i}^{d_{H}}\). Let \(=(_{1},,_{N})^{T}^{N d}\), and \(=(_{1},,_{S})^{T}^{S d}\). We define \(^{T}==_{K}^{N d_{H}}\), and \(=_{Q}^{S d_{H}}\). By plugging \(^{T}=\) and \(=_{i}\) into (4) for all \(i\), we obtain that: \(^{T}=^{T}_{1}(^{T}) ^{d_{H} S}\). By taking the transpose, we obtain \(=_{2}(^{T})\). Let \(^{N d_{V}}\) denote the value matrix as \(=_{K}_{V}=_{V}\), which will replace the \(\) outside of \(_{2}\). Then, we finally obtain

\[^{new}=_{2}(^{T}),\] (6)

which is exactly the transformer attention with \(=1/}\). This connection affords us the insightful theoretical ground of the attention mechanism: the update step of the attention mechanism in a Transformer layer acts as an inner-loop optimization step, minimizing an implicit energy function that is determined by queries, keys, and values.

## 3 Energy-based Cross-Attention

Recall that our objective is to generate semantically correct images by harmonizing latent (U-Net) representations and context vectors within the denoising autoencoder. To achieve this, we propose a simple but effective Bayesian framework, namely energy-based cross-attention (EBCA). Specifically, we perform test-time optimization of context vectors within the cross-attention spaces of a denoising autoencoder to implicitly minimize a specially designed energy function. Note that this is a significant departure from the conventional text-guided diffusion models which have leveraged _fixed_ context vectors embedded by pre-trained CLIP  to all cross-attention layers.

### Energy-based Bayesian Context Update

**Energy function.** Our focus is on the cross-attention space of a time-dependent denoising auto-encoder, utilizing the conventional U-Net neural architecture. Here, we refer to latent representations as the representations of intermediate layers in U-Net unless otherwise specified. Let \(L\) be the number of cross-attention layers. For each \(l\)-th layer at time \(t\), we define the queries matrix \(_{t,l}^{P_{t}^{2} d_{l}}\), and the keys and values matrices \(_{l}^{N d_{l}}\) and \(_{l}^{N d_{l}}\), respectively. Here, \(P_{l}\) represents the spatial dimension of latent representations in the \(l\)-th layer. Given context vectors \(^{N d_{c}}\), we map \(_{l}\) and \(_{l}\) with \(_{K,l},_{V,l}^{d_{c} d_{l}}\), such that \(_{l}=_{K,l}\) and \(_{l}=_{V,l}\). In the following, time \(t\) and layer index \(l\) are omitted in notations \(_{t,l}\) and \(_{l}\) for simplicity.

The main goal is to obtain a maximum a posteriori probability (MAP) estimate of context vectors \(\) given a set of observed latent representations of queries \(_{t,l}\). First, based on the energy functions (8) and (9), the posterior distribution of \(\) can be defined by using Bayes' rule: \(p()=p()p()/p()\), where (8) and (9) are leveraged to model the distribution \(p()\) and \(p()\), respectively. Then, in order to obtain a MAP estimation of \(\), we approximate the posterior inference using the gradient of the log posterior. The gradient can be estimated as follows:

\[_{} p()=_{} p()+_{} p()=-_{}(;)+_{}().\] (7)

Motivated by the energy function in (3), we first introduce a new conditional energy function w.r.t \(\) as follows:

\[(;)=(^{T})-_{i=1}^{N}(_{i}^{T},),\] (8)

where \(_{i}\) denotes the \(i\)-th row vector of \(\), \( 0\), and \(()=_{i=1}^{N}A_{i,i}\) for a given \(^{N N}\). Intuitively, \((_{i}^{T},)\) term takes a smooth maximum alignment between latent representations \(_{j},j=1,,P^{2}\) and a given text embedding \(_{i}\). Let \(j^{*}=*{arg\,max}_{j}_{j}_{i}^{T}\). Then, the implicit minimization of \(-*{logsumexp}\) term encourages each \(_{i}\) to be semantically well-aligned with a corresponding spatial token representation \(_{j^{*}}\). In turn, this facilitates the retrieval and incorporation of semantic information encoded by \(_{i}\) into the spatial region of \(_{j^{*}}\).

The \(*{diag}(^{T})\) term in (8) serves as a regularizer that constrains the energy of each context vector \(_{i}\), preventing it from exploding during the maximization of \(*{logsumexp}(_{i}^{T},)\), thereby ensuring that no single context vector excessively dominates the forward attention path. In this regard, we also propose a new \(\)-independent prior energy function \(*{E}()\) given by:

\[*{E}()=*{logsumexp} *{diag}(^{T}),1=_{i=1}^{N} _{i}_{i}^{T}.\] (9)

Instead of penalizing each norm uniformly, it primarily regularizes the smooth maximum of \(\|_{i}\|\) which improves the stability in implicit energy minimization. We empirically observed that the proposed prior energy \(*{E}()\) serves as a better regularizer compared to the original \(*{diag}(^{T})\) term (related analysis in (12) and appendix E).

Although our energy function is built based on the energy function, in contrast to (3), the proposed one is explicitly formulated for implicit energy minimization w.r.t _keys_\(\) and the associated _context vectors_\(\), which is different from the theoretical settings in Section 2.2 w.r.t \(\). It is worth noting that the two energy functions are designed to serve different purposes and are orthogonal in their application.

**MAP estimation.** Based on the proposed energy functions, we derive the update rule of key \(\) and context \(\) following (7):

**Theorem 1**.: _For the energy functions (8) and (9), the gradient of the log posterior is given by:_

\[_{} p()=*{softmax}_{2} ^{T}-+} *{softmax}*{diag}(^{ T}),\] (10)

_where \(}()\) is a vector-to-diagonal-matrix operator._

_Then, by using the chain rule the update rule of context vectors \(\) is derived as follows:_

\[_{n+1}=_{n}+*{softmax}_{2} ^{T}-+} *{softmax}*{diag}(^{ T})_{K}^{T},\] (11)

_where \(>0\) is a step size._

In practice, we empirically observed that the nonzero \(\) in (8) often leads to an over-penalization of contexts, which can ultimately cause some contexts to vanish. To address this, we set \(=0\). Moreover, we found it beneficial to assign distinct step sizes \(_{}\) and \(_{}\) as follows:

\[_{n+1}=_{n}+_{}_{2}^{T}_{K}^{T}}_{}-_{}} *{softmax}*{diag}(^{ T})_{K}^{T}}_{},\] (12)

where the first and second terms are named as _attention_ and _regularization_ term, respectively.

Our theoretical observations offer valuable insights into implicit energy minimization by modifying the forward path of cross-attention. Inspired by these observations, we have transplanted the energy-based cross-attention layers to pre-trained text-to-image diffusion models. We first illustrate the challenges associated with adopting EBCA in a deep denoising auto-encoder and subsequently demonstrate how to overcome them in practice.

If a single recurrent Transformer block were available, a single energy function would be minimized for a given cross-attention layer by recurrently passing the updated context \(_{n+1}\). However, in practical applications, there are multiple layer- and time-dependent energy functions in conventional deep denoising auto-encoder, which makes it infeasible to minimize each energy function individually. To address this challenge, we implicitly minimize a nested hierarchy of energy functions in a single forward pass based on our theoretical observations. More details are followed.

**Algorithms.** At a given time step \(t\), we initialize the context vectors \(_{1,t}\) with text embeddings \(_{CLIP}\) obtained from a pre-trained CLIP. Then, within the \(n\)-th cross-attention layer with \(_{n,t}\) (\(1 n L\)), we compute the updated context vectors \(_{n+1,t}\) using the gradients in Theorem 1 and (12). We then _cascade_\(_{n+1,t}\) to the next \((n+1)\)-th cross-attention layer. We do not forward the final context \(_{L+1,t}\) at time \(t\) to time \(t+1\), as the distributions of \(_{t}\) and \(_{t+1}\) can be significantly different due to the reverse step of the diffusion model. Instead, we reinitialize \(_{1,t+1}\) with \(_{CLIP}\). The pseudo-code for the proposed framework is provided in appendix B.

The sequence of energy-based cross-attention layers is designed to minimize _nested_ energy functions. While a single-layer update in a single step may not lead to optimal convergence, the proposed layer-cascade context optimization can synergistically improve the quality of context vectors. Notably, our proposed method does not incur additional computational costs in practice since the gradient term in Theorem 1 relies on the existing keys, queries, and attention maps in the cross-attention layer, which can be obtained for free in the forward path.

### Energy-based Composition of Queries

In addition to the above key advantages, the cross-attention space EBMs shed new light on the zero-shot compositional generation. Recent studies [8; 10] have demonstrated that the underlying distributions of EBMs can be combined to represent different compositions of concepts. For example, given a data point \(\) and independent concept vectors \(_{1},,_{n}\), the posterior likelihood of \(\) given a conjunction of specific concepts is equivalent to the product of the likelihood of each individual concept as follows:

\[p(_{1},,_{n})=_{i}p(_{i})  e^{-_{i}(_{i})},\] (13)

where each \((_{i})\) represent independently trained EBM with concept code \(_{i}\). While it is an appealing solution to the controllable generation, it is notoriously difficult to train EBMs [9; 27] in a way to make themselves scalable to high-resolution image generation. Instead of directly training EBMs in pixel space, we leverage the cross-attention space EBMs and the generative power of state-of-the-art pre-trained DMs to achieve high-fidelity compositional generations.

More specifically, assume that we have _main_ context vectors \(_{1}\) embedded from a main prompt, e.g. "a castle next to a river", and a set of independent _editing_ context vectors, \(=\{_{2},,_{M}\}\), each embedded from different editorial prompt, e.g. "monet style", "boat on a river", etc. Then, we define the keys \(_{l,s}\) for context \(s\) within a cross-attention layer of index \(l\) as \(_{l,s}=_{s}_{K,l}, s\{1,2,,M\}\). The index \(l\) will be omitted for notational simplicity. Then, for a given key \(_{s}\), we consider the energy function in cross-attention space w.r.t _queries_\(\):

\[(;_{s})=(^{T})-_{i=1}^{P^{2}}(_{s}_{i}^{T}, ),\] (14)

which is directly motivated by (3). We then introduce the compositional energy function \(}\), for the concept conjunction of \(\) as in (13) and the updated rule as done in (6):

\[}(;\{_{s}\}_{s=1}^ {M})&=_{s=1}^{M}(;_{s}) \\ &=(^{T})- _{s=1}^{M}_{i=1}^{P^{2}}(_{s}_{i}^ {T},),\] (15)

\[_{TF}^{new} =_{s=1}^{M}_{s}_{2} _{TF}_{TF,s}^{T}_{TF},\] (16)

where \(_{TF}\), \(_{TF,s}\) and \(_{TF}\) directly follows the definition in (6) and the degree and direction of \(s\)-th composition can be controlled for each concept individually by setting the scalar \(_{s}\), with \(_{s}<0\) for concept negation . Note that this is exactly a linear combination of transformer cross-attention outputs from different contexts with \(=1/}\). We refer to this process as the Energy-based Composition of Queries (EBCQ).

This update rule implies that the compositional energy can be minimized implicitly through modifications to the forward path of EBCA, thereby guiding the integration of semantic information conveyed by editorial prompts. Notably, this is a significant departure from the existing energy-based compositional methods [8; 26; 9]. Specifically, no training or fine-tuning is required. Instead, cross-attention outputs of the main and editorial prompts are simply obtained in parallel, averaged, and propagated to the next layer. Moreover, the introduction of EBMs in cross-attention space is orthogonal to , which conducts compositional generation by treating a pre-trained diffusion model \(_{^{*}}\) itself as an implicitly parameterized EBM.

## 4 Experimental Results

To verify our claim of energy minimization via modified cross-attention, we conduct various experiments in text-guided image generation to verify semantic alignment, namely (1) multi-concept generation [22; 11], (2) text-guided image inpainting [23; 41], and (3) compositional generation [12; 1; 28] which includes real and synthetic image editing. In this work, while the first and third applications address similar challenges, we categorize them separately based on the implementation derived from the energy-based interpretation. Specifically, the Energy-based Bayesian Context Update (EBCU) is applied to every task, and the Energy-based Composition of Queries (EBCQ) is additionally leveraged in the third task. Note that all applications have been done without additional training.

**Setup.** The proposed framework can be widely mounted into many state-of-the-art text-to-image DMs due to its unique functionality of context updates and cross-attention map compositions. Here, we verify the effectiveness of energy-based cross-attention with Stable Diffusion (SD) . The SD is an LDM that is pre-trained on a subset of the large-scale image-language pair dataset, LAION-5B  followed by the subsequent fine-tuning on the LAION-aesthetic dataset. For the text embedding, we use a pre-trained CLIP  model following the Imagen . The pre-trained SD is under the creativeML OpenRAIL license. Detailed experimental settings are provided in the appendix.

### Analysis on Energy during the sampling

We perform a comprehensive analysis on (8) and (9) during the forward path through the modified cross-attention, which offers insights into the energy behavior for real applications. Specifically, we examine the energy dynamics involved in the multi-concept image generation that is straightforward and could be readily applied to other applications with minimal effort. We record computed energy values along layers and sampling steps 30 times with a fixed text prompt and then display them in Figure 3 with the generated samples, where red and green lines denote the energy involved with the Stable-Diffusion (SD) and the proposed method, respectively. For each sampling step block that is alternately shaded, the energy values are plotted through 16 cross-attention layers.

Across all layers and sampling steps, the energy associated with the proposed method remains consistently lower than that of the SD. This is in line with the semantic alignment between intermediate denoised estimates and the given text prompt. In both cases of the SD and the proposed method, \(E(;)\) decreases over sampling steps. This implies that the iterative refinement of the updated query carried over to subsequent sampling steps, resulting in a better match to the given context. Note that the proposed method even achieves lower energy with the EBCU. More analyses are provided in the appendix E including the ablation study.

Figure 3: Energy analysis for multi-concept image generation. The right graph displays \(E(Q;K)\) across 16 cross-attention layers within each sampling step. The red and green lines correspond to the Stable Diffusion and ours, respectively. Mean and standard deviation are calculated and displayed. The bottom plot shows the cumulative sum of the posterior energy difference between Stable Diffusion and the proposed method, and intermediate denoised estimates are displayed together.

### Multi-Concept Generation

We empirically demonstrate that the proposed framework alleviates the catastrophic neglect and attribute binding problems defined in the existing literature . As shown in Figures 3 and 4, the EBCU effectively mitigates these problems by promoting attention to all relevant concepts in the prompt. Specifically, the regularization term introduced by the prior energy prevents a single context from dominating the attention, while the attention term updates the context vectors in the direction of input queries, improving semantic alignment and facilitating the retrieval of related information.

### Text-guided Image Inpainting

In addition, we have evaluated the efficacy of the proposed energy-based EBCU on a text-guided inpainting task. Although existing state-of-the-art DMs such as DALLE and SD can be employed for inpainting by exploiting the ground-truth unmasked area , they usually require computationally expensive fine-tuning and tailored data augmentation strategies [23; 41]. In contrast, as shown in Figure 5, our proposed energy-based framework significantly enhances the quality of inpainting without any fine-tuning. Specifically, we incorporate the energy-based cross-attention into the Stable-Repaint (SR) and Stable-Inpaint (SI) models, which can struggle to inpaint multiple objects (e.g., birthday hat and bow tie) or unlikely combinations of foreground and background objects (e.g., Teddy bear on the Starry Night painting). In contrast, the proposed approach accurately fills in semantically relevant objects within the target mask region.

### Compositional Generation

We demonstrate that the proposed framework improves the controllability and compositionality of DMs, which is a significant challenge for generative models. To assess this, we split the task into synthetic and real image editing.

Figure 4: Multiconcept generation comparison. Each row indicates the Stable Diffusion, the Structured Diffusion, and the proposed method applied to the Stable Diffusion, respectively. Generated samples in the same column used the same text prompt and the same random seed.

Figure 5: Text-guided inpainting comparison. The masked region is conditionally generated by given text-prompt positioned on the left-end of each row.

**Synthetic image editing.** Text-to-image DMs provide impressive results, but it is difficult to generate images perfectly aligned with user intentions . Although modifying prompts can guide generation, it can also introduce unintended changes in the generated content. Our results, shown in Figure 5, demonstrate that the proposed method can edit image contents while maintaining the original-prompt-related identity of the generated images thanks to the EBCU, which continuously harmonizes the latent representations and text prompts. While composable-diff  can generate compositions, they often fail to preserve the original properties of images, such as the color in the second row, or fail to compose the desired concept, such as the boat in the first row.

**Real image editing.** We demonstrate that the proposed framework can also edit real images, which is especially challenging as existing methods often dramatically alter input content or introduce unexpected variations. To do this, we integrate our framework with DDIM inversion , which inverts images with meaningful text prompts into the domain of pre-trained diffusion models. First, we use the image captioning network BLIP  to automatically caption the interested image, following . Next, we obtain the inverted noise latents of the input image using Diffusion Pivotal Inversion . Then, we apply EBCQ and EBCU while denoising the inverted latents. The optimized unconditional textual embedding vector is also targeted for additional EBCUs. The results in Figure 6 demonstrate that our method achieves better editing performance by avoiding undesired changes.

Figure 6: Image editing comparison. The left three columns show examples of synthetic image editing, while the right four columns show examples of real image editing. To ensure a fair comparison, the same editing prompt, positioned on the left-end of each row, is given to all methods being compared.

Figure 7: Real image editing with multiple editing prompts. Given a real image at the left-end of each row, the proposed method allows robust image editing with different prompts over each sample.

**Quantitative comparisons.** We further conducted a comparative analysis of the proposed framework against several state-of-the-art diffusion-based image editing methods [24; 12; 25; 28]. For our evaluations, we focus on two image-to-image translation tasks. The first one is an animal transition task, which translates (1) cat \(\) dog, (2) horse \(\) zebra, and (3) adding glasses to cat input images (cat \(\) cat with glasses). The second one is a human transition task, which translates (1) woman \(\) man, (2) woman \(\) woman with glasses, and (3) woman \(\) man with glasses. Source images are retrieved from the LAION 5B dataset  and CelebA-HQ . Motivated by [39; 28], we measure CLIP Accuracy and DINO-ViT structure distance. Experimental details are provided in the appendix E including explanations of baselines, datasets, metrics, and hyperparameter configurations.

Table 1 and 2 in the appendix show that the proposed energy-based framework gets a high CLIP-Acc while having low Structure Dist. It implies that the proposed framework can perform the best edit while still retaining the structure of the original input image. While DDIM + word swap records remarkably high CLIP-Acc in horse \(\) zebra task, Figure 8 shows that such improvements are based on unintended changes in the overall structure. More visualizations are provided in the appendix E.

## 5 Conclusion, Limitations and Societal Impacts

**Conclusion.** In this work, we formulated the cross-attention with an energy perspective and proposed the EBCU, a modified cross-attention that could implicitly minimize the energy in the latent space. Furthermore, we proposed EBCQ which is inspired by energy-based formulation of multiple text composition. The proposed method is versatile as shown by multiple applications, theory-grounded, easy to implement, and computationally almost free.

**Limitations and Societal Impacts.** The framework presented in this study generates images based on user intentions, which raises concerns regarding potential misuse for creating deepfakes or other forms of disinformation. It is crucial to ensure that these methods are implemented ethically and regulated appropriately. Additionally, while the framework performs well across various tasks, it requires pre-trained deep models, rendering it challenging to apply to out-of-domain datasets, such as medical images.

    &  &  &  \\   & CLIP Acc (\(\)) & Dist (\(\)) & CLIP Acc (\(\)) & Dist (\(\)) & CLIP Acc (\(\)) & Dist (\(\)) \\  SDEdit  + word swap & 71.2\(\%\) & 0.081 & 92.2\(\%\) & 0.105 & 34.0\(\%\) & 0.082 \\ DDIM + word swap & 72.0\(\%\) & 0.087 & **94.0**\(\%\) & 0.123 & 37.6\(\%\) & 0.085 \\ prompt-to-prompt  & 66.0\(\%\) & 0.080 & 18.4\(\%\) & 0.095 & 69.6\(\%\) & 0.081 \\ pix2pix-zero  & 92.4\(\%\) & 0.044 & 75.2\(\%\) & 0.066 & 71.2\(\%\) & **0.028** \\ Stable Diffusion + ours & **93.7\(\%\)** & **0.040** & 90.4\(\%\) & **0.061** & **81.1**\(\%\) & 0.052 \\   

Table 1: (Animal transition) Comparison to state-of-the-art diffusion-based editing methods. Dist for DINO-ViT Structure distance. Baseline results are from .

Figure 8: Real image editing on (**a**) animal transition and (**b**) human transition tasks.

#### Acknowledgments

This research was supported by the KAIST Key Research Institute (Interdisciplinary Research Group) Project, National Research Foundation of Korea under Grant NRF-2020R1A2B5B03001980, Field-oriented Technology Development Project for Customs Administration through National Research Foundation of Korea(NRF) funded by the Ministry of Science & ICT and Korea Customs Service(** NRF-2021M31I1A1097938**), Korea Medical Device Development Fund grant funded by the Korea government (the Ministry of Science and ICT, the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) (Project Number: 1711137899, KMDF_PR_20200901_0015), Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program(KAIST)), Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2021-0-02068, Artificial Intelligence Innovation Hub), Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. RS-2023-00233251, System3 reinforcement learning with high-level brain functions), and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2019M3E5D2A01066267).