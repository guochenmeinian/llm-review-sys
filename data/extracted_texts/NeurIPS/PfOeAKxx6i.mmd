# Algebraic Positional Encodings

Konstantinos Kogkalidis\({}^{1,2}\)

kokos.kogkalidis@aalto.fi

Jean-Philippe Bernardy\({}^{3,4}\)

jean-philippe.bernardy@gu.se

Vikas Garg\({}^{1,5}\)

vgarg@csail.mit.edu

###### Abstract

We introduce a novel positional encoding strategy for Transformer-style models, addressing the shortcomings of existing, often _ad hoc_, approaches. Our framework implements a flexible mapping from the algebraic specification of a domain to a positional encoding scheme, where positions are interpreted as orthogonal operators. This design preserves the structural properties of the source domain, thereby ensuring that the end-model upholds them. The framework can accommodate various structures, including sequences, grids and trees, but also their compositions. We conduct a series of experiments demonstrating the practical applicability of our method. Our results suggest performance on par with or surpassing the current state of the art, without hyper-parameter optimizations or "task search" of any kind. Code is available through https://aalto-quml.github.io/ape/.

## 1 Introduction

Attention-based models inheriting from the Transformer  have become ubiquitous in neural computation, supplanting the go-to models of the last decade and driving a continuous stream of breakthroughs across diverse domains. Their success is perhaps at odds with the Transformer's structural lenience. Its key building block, dot-product attention, is by default unable to perceive and utilize the structure and arrangement of the input/output tokens being processed. To address this limitation, a plethora of works have sought to endow Transformers with appropriate inductive biases. The default strategy is to adjust token representations via so-called _positional encodings_; vector operations that hint at the structure being modeled. Nonetheless, most positional encoding schemes to date are either empirically motivated, or tailored to specific tasks. This renders their theoretical evaluation challenging, and hinders any prospects of a unifying framework.

In this study, we seek to fill this gap with a theory-first approach. Through the lens of group theory, we scrutinize some of the most commonly targeted data structures, and express them by means of inductive definitions that reveal and explicate their structural properties. Leveraging this analysis, our modeling strategy invokes a homomorphic interpretation that maps each domain into **algebraic positional encodings** (ape): attention-compatible vector operations parameterizing (subgroups of) the orthogonal group. In the sequential context, algebraic positional encodings streamline the widely adopted rotary encodings of Su et al. , while also offering clear theoretical insights on their success. More importantly, algebraic positional encodings naturally extend to non-sequential domains, such as \(\)-ary trees and multidimensional regular grids, paving the way for a simple and elegant methodology for interpretable and domain-general structurally-refined Transformers. We carry out an experimental evaluation in settings that allow for reproducible and statistically sound conclusions. Across the tasks considered, algebraic positional encodings consistently and significantly outperform strong baselines at an aggregate level, providing initial but compelling evidence that they constitute not just a _sensible meta-theory_ for positional encodings, but also an _actionable alternative_ to the current state of the art.

## SS2 Background

We introduce the problem (SS2.1) and the vocabulary of the solution (SS2.2).

SS3 Theory

We provide an algebraic characterization of positions in the context of different ambient structures. We frame algebraic positional encodings as structure-preserving semantic interpretations, and present reference implementations. Concretely:

SS3.1_Sequences_ are an isomorphism of the free group \( 1\) (_i.e._, the integers, \(\)), and can be interpreted as a single generator subgroup of the orthogonal group \(O(d)\).

SS3.2_Rotary Positional Encodings_ correspond to a (quite literally) special case of this interpretation: \(SO(d)\).

SS3.3_k-ary Trees_ are an isomorphism of the finitely generated group \( 1,2\), and can be interpreted as a finitely generated subgroup of \(O(d)\).

SS3.4_Regular Grids_ are the group direct sum of multiple sequences. They can be interpreted as the matrix direct sum of their components' interpretations.

SS3.5_Extensions_ can be obtained in multiple directions.

SS4 Practice

We carry out fair and replicable experiments across all three structures analyzed, namely _Sequence Transduction_ (SS4.1), _Tree Transduction_ (SS4.2) and _Image Recognition_ (SS4.3), and find that algebraic positional encodings consistently match or outperform alternative schemes in the tasks considered (SS4.4).

SS5 Related Work

We position our work in the context of the broader literature.

SS6 Limitations

We close with a brief discussion of possible limitations and potential future improvements.

## 2 Background

### The Problem with Dot-Product Attention

All transformer variants employ some variation of the multi-head scaled dot-product attention mechanism of Vaswani et al. (2017). For each attention head, the dot-product attention between queries \(^{m d}\) and keys \(^{n d}\) is defined as:

\[(,):=_{(n)} ^{(q)})(^{(k)})^{}}{}\,^{(v)}\] (1)

In equation (1), matrices \(^{(q)},^{(k)},^{(v)}:^{d d}\) enact linear functions, applied point-wise (broadcasted) across all \(m\) and \(n\) entries of \(\) and \(\). The dot-product term \((^{(q)})(^{(k)})^{}\) contains unnormalized attention scores in the Cartesian product of queries and keys. Unmodified, dot-product attention is permutation _invariant_ with respect to its second argument; that is, for any arbitrary permutation \(p_{n}_{n}\):

\[(,)(,p_{n}())\] (2)

Unless one is dealing with orderless structures like multisets or fully connected graphs, this property is generally undesirable. The lack of structural biases is traditionally counteracted by the component-wise addition of unidimensional periodic signals of varying frequencies. These, however, often prove inadequate in data-scarce domains, where extensive pretraining is impossible, and structure-rich domains, where a sequence-of-tokens projection is too radical of a simplification.

### Recap on Group Theory

To address this issue, we propose an algebraic treatment of positional encodings, based on principles lent from group theory. For the sake of convenience and accessibility, we provide a brief recap of the notions of interest here. A _group_\(G\) consists of a set of _elements_ and a _binary operation_ (_\(\_\).) satisfying four fundamental laws:

* The group is _closed_ under the the group operation. For all \(a\), \(b\) in \(G\), \(a b\) is also in \(G\).
* The group operation is _associative_. For all \(a\), \(b\), \(c\) in \(G\), \((a b) c=a(b c)\).
* The group operation has an _identity_ element \(e\), such that for all \(a\) in \(G\), \(a e=e a=a\).
* Each group member has an _inverse_. For all \(a\) in \(G\), there exists some element \(\) such that \(a=a=e\), where \(e\) is the identity element.

  
**\(@sectionsign\)2 Background** \\   

Table 1: A summary of this paper.

A group is characterized as _finite_ or _infinite_ depending on the number of elements it has. If all elements of a group \(G\) can be expressed as a combination of a subset \(S\) of the group elements (combined by means of the group operation, applied either on the elements themselves or on their inverses), we write \(G= S\). We say that \(G\) is _generated_ by \(S\), and we call the elements of \(S\) the _generators_ of \(G\). A group with a single generator is called _cyclic_.

## 3 The Algebra(s) of Positions

Our objective is to establish a framework that offers general and extensible _semantics_ for positions across various structures - what we commonly encounter in the literature as _positional encodings_. Most existing proposals adopt a rather parochial stance, relying on maneuvers or heuristics tailored to specific applications and driven, predominantly, by extensive empirical investigations. As such, they fall short with respect to accommodating or reflecting the properties of the underlying structure. We follow a different approach. We adopt Montague's perspective, succinctly paraphrased as:

"_syntax is an algebra, semantics is an algebra, and meaning is a homomorphism between them_" (Janssen, 2014).

We begin by noting that "positions" do not exist in isolation, but only in the context of some underlying ambient structure. We contend that reasonable positional encodings (_semantics_) may only be reliably obtained by taking into account exactly this structure, its formation rules and properties (_syntax_), and then applying an appropriate interpretation (_meaning_). This is _not_ just an academic exercise: a careful syntactic specification is a prerequisite if we aim for semantics that adhere to certain properties, which is arguably preferable to searching for these properties in the wild.

### Sequences

SyntaxWe start from the simplest structure, and incidentally also the most standard one: the sequence. The full range of positions a token can occupy within a sequence coincides exactly with the naturals, \(\). Relative paths \(\) between any two positions can then be seen as the integers, \(\), with positive (resp. negative) numbers denoting forward (resp. backward) offsets. Using this insight, it is handy to inspect how the standard inductive definition of the integers provides the building blocks for path formation. We start with two constants: the empty path (0), which relates any given point to itself, and the unit path (1), which relates any point to its immediate next. We may compose simple paths into complex ones with the aid of a binary operation \(+_{}\). This already suffices to specify all forward offsets. In order to construct backward offsets, we need a unary operation \((-)_{}\), such that \(-\,\) denotes the inverse of \(\). We can summarize the above by the grammar:

\[:=\,+_{}\,\,-\] (3)

For this to make sense, the operations must be _coherent_; that is, all ways to start from point \(_{1}\) and end up in point \(_{2}\) should be equivalent, even if apparently distinct. The needed equivalences exactly correspond to the group laws, with closure internalized by the inductive definition of (3):

\[(_{1}+_{}_{2})+_{}_{3} =_{1}+_{}(_{2}+_{}_{3})\] (L1) \[+_{} ==+_{}\] (L2) \[+_{}(-) =\] (L3)

The (unsurprising) insight here is that paths in a sequence form a free group, generated by a single generator (1) - the uniqueness of the generator exceptionally also makes the group abelian (_i.e._, commutative). For convenience, we adopt the notational shorthand \(^{p}\), where:

\[^{p}:=}+_{} }_{p}&p 0\\ }+_{}(-\,)}_{-p}&p<0 \] (4)

SemanticsThe syntactic specifications of the previous paragraph impose constraints on the candidate semantic targets. Among these candidates, we isolate and focus on \(\), the subgroup of the orthogonal group \(O(d)\) that is generated by a single orthogonal matrix \(\). This semantics is not only sound1 with respect to the structure under scrutiny, but also a familiar object in machine learning literature (Arjovsky et al., 2016; Bernardy and Lappin, 2022, _inter alia_). Note that for \(\), the group axioms are obtained for free from the orthogonal group, and the additional requirement of commutativity is again satisfied by the uniqueness of the generator.

To illustrate the correspondence between the two structures (and at risk of being pedantic), we spell out the homomorphism \(\), which maps paths \(\) to elements of \(\), and path operations to operations on orthogonal matrices of size \(d\). For the primitives, we have \(:=_{d}\) and \(:=\). Path composition amounts to matrix multiplication, _i.e._, \(_{1}+_{2}:=_{1}_{2}\), while path inversion corresponds to matrix transposition, _i.e._, \(-:=^{-1}^{}\). The fact that orthogonal matrices form a group under multiplication is folklore; one can easily verify that the group laws hold also for the semantics.2

ImplementationIn practice, we have \(^{p}^{p}\); a norm-preserving bilinear form \(^{d}^{d}\) which can be used to mediate the dot-product between a query \(q\) and a key \(k\) offset by a relative distance of \(p\). The representation of all paths up to length \(p\) can thus be implemented as a matrix collection \([^{0},,^{p}]\), which can asymptotically be obtained using \((_{2}(p))\) matrix products (of exponentially larger matrices), and taking up the storage space equivalent of \((pd^{2})\) floats. Transposed, the same matrices also serve to represent backwards paths \([^{-p},,^{0}]\). Storing the representations of all relative paths between queries and keys in a tensor \(:^{m n d d}\), we may then substitute the dot-product term of equation (1) for the tensor contraction:

\[_{,}_{m}^{(q)}_{}_{mn }_{n}^{(k)}_{}\] (5)

Albeit transparent, this reduction strategy is computationally unappealing due to the doubly quadratic nature of \(\). We can do better by noting that \(_{mn}\) is (definitionally) equal to:

\[_{mn}=_{}^{(X)}_{m}^{(Y)}_ {n}\] (6)

where \(^{(X)}\) and \(^{(Y)}\) are the matrices containing representations for the _absolute_ positions of the entries in \(\) and \(\), respectively. Concretely, a single relative representation is built by composing the _inverted_ representation of the source with the representation of the target. Intuitively, each query follows the path that takes it _back_ to the origin, which then allows it to directly combine with each forward-offset key; see Figure 1a for a visual example. This insight allows us to keep the memory footprint of equation (1) unchanged, replacing expression (5) with:

\[_{,,,,}_{m}^{(q)}_{ }^{(X)}_{m}^{(Y)}_{n}_{n }^{(k)}_{}\] (7)

This version decomposes the tensor contraction into two matrix multiplications, essentially transforming (rotating or reflecting) the entries of \(\) and \(\) independently according to their positions.

### Intermezzo: Equivalence with rope

The story so far should be reminiscent of the rotary positional encoding scheme of Su et al. (2023, rope). Not unlike our approach, rope substitutes the vanilla dot-product for a position-dependent bilinear form. Underlying the form is a \(d d\)-dimensional matrix \(\) with a block-diagonal structure, where each \(2 2\)-sized block corresponds to a rotation matrix that acts on a \(2\)-dimensional subspace of \(^{d}\). These independent rotations are parameterized by a (fixed) set of base angles \(:=[_{1},,_{d/2}]\). To incorporate position-dependence, _i.e._, for a query/key pair at a relative distance of \(p\), the base angles are multiplied by \(p\), effectively altering the rotations applied.

At first glance, rotary encodings appear to be under-parameterized, and thus strictly weaker than orthogonal ones. However, any orthogonal matrix \( O(d)\) admits a canonical form \(=^{}\), where \(\) is an orthogonal change of basis, and \(\) is block-diagonal, with the \(2 2\)-sized blocks being, once again, \(2-\)dimensional rotation matrices (Murnaghan and Wintner, 1931)3. Owing to the orthogonality of \(\), raising \(\) to its \(p\)th power is equal to \(^{p}^{}\) (_i.e._, it leaves the change of basis unaffected). In turn, raising \(\) to its \(p\)th power is equivalent to simply multiplying the rotation angles of its blocks by \(p\). Finally, given the linearity of the transformations \(^{(q)}\) and \(^{(k)}\), their compositions with \(\) are also linear. By identifying \(\) with rope's \(\), we can then see that, for any given collection of angles \(\), ape and rope coincide under the substitutions:

\[^{(q)}_{}=^{(q)} ^{(k)}_{}=^{(k)}\] (8)

In practical terms, and uniquely for the sequential case, ape _is equivalent to a trainable version of rope, where the rotation angles \(\) may vary and be optimized during training.4_

Which of the two parameterizations is preferable is debatable. On the one hand, ape's formulation is FLOP-optimized (being just matrix multiplications), and obviates the need for backpropagating through trigonometric functions (which are periodic, non-monotonic, and prone to gradient instabilities). On the other hand, rope's diagonalized form gives access to a memory-efficient contraction that does away with the matrix multiplications of expression (7) altogether; we direct the interested reader to Su et al. (2023, Section 3.4.2) for a reference implementation.5

In either case, the equivalence between the two is confined to the _sequential_ setup; we will now move on to generalize our strategy to other, _previously inaccessible_, structures.

### Trees

SyntaxIn the previous section, we characterized the structure of relative paths on a sequence as the free group with one generator, and uncovered a (practically) isomorphic interpretation in the subgroup of orthogonal matrices with a single generator. Upon closer inspection, we note that a sequence can be viewed as a special case of the more general structure of \(\)-ary branching trees, where the branching factor \(\) just so happens to be \(1\). Denoting the more general case as \(_{}\), we must first extend the set of primitives to include all branching options, \(1,2,:_{}\). Each primitive now denotes a choice of branch (except for \(0\), which is again the empty path). Paths now form a free group with \(\) distinct generators. The presence of multiple generators means that commutativity no longer holds; \(1+_{}\) 2 is distinct from \(2+_{}\) 1 (the former prescribes a descent down branch 1 then branch 2, whereas the latter prescribes a descent down branch 2 then branch 1). Inversion is as before: for every path from each local root to some descendant down the line, there is also an inverse path from that descendant up to its ancestor. Perhaps more interestingly, upwards and downwards paths can be joined, allowing the precise specification of relative paths between any two nodes, even when the two do not share a single line of descent (think nephews, aunts and all other sorts of distant relatives, see Figure 0(b) for an example). Adjusting grammar (3) accordingly, we have:

\[_{}:=0\ |\ 1\ |\ 2\ |\ |\ \ |\ _{}+ _{}\ _{}\ |\ -_{}\] (9)

with laws L1, L2 and L3 still in effect.

SemanticsThe interpretation follows along the same lines as before. This time around, however, we cannot make do with a single orthogonal matrix \(\) - we need a collection of \(\) matrices, one for each branch option. As a consequence, the semantic target is now \(_{1},_{2},_{}\). Note that the target is no longer commutative (exactly in alignment with the source).

ImplementationFor a tree structure of depth \(\) and branching factor \(\), let \(\) denote the number of _unique_ absolute positions occupied (upper bound by \(^{}\) in the case of a complete tree). Their representations can be computed in \(\) steps of parallel matrix-matrix multiplications and a memory cost of \( d^{2}\), as follows. First, we can build up a collection of all unique absolute paths, each represented as a (right-padded) word of length \(\) from the vocabulary of primitives. Their corresponding representations constitute a tensor of size \( d d\), initialized as \(\) identity matrices. We can then iterate across these words in parallel, one primitive per step (_i.e._, depth) \(t\), selecting all words that take the same branching direction at the current depth, and right-multiplying their representations by the corresponding orthogonal generator. Finally, absolute paths can be composed into relative ones using the modified dot-product attention of expression (7), just like before.

### Grids

The generalization from sequences to trees rests on the observation that a sequence is a tree with a deficit of choices. An altogether different axis of generalization can be obtained by recalling that composite groups can be constructed by joining together two or more elementary groups. Moreover, if it just so happens that the original groups were abelian, then so is their composition; in that case, we call the composite a _group direct sum_. This construction provides access to an extension from sequences to multidimensional regular grids.

For the sake of simplicity and without loss of generality, we consider a standard instance of a two-dimensional grid: an image. An image is a collection of pixels (or pixel patches) that inhabit a coordinate system \((h,w)\). Each of \(h\) and \(w\) is the product of grammar (3), inheriting all path-related notions discussed earlier. Since \(\) is an abelian group, the coordinate system also constitutes an abelian group \(^{2}:=\). The new group and inversion operations are \(+^{2}\) and \((-)_{^{2}}\), and denote the act of joining and inverting two-dimensional paths, respectively. Both are canonically defined component-wise, on the basis of their one-dimensional counterparts:

\[(x,y)+\!^{2}\,(z,w) :=(x+\,y,z+\!\,w)\] (10) \[-(x,y) :=(-\,x,-\,y)\] (11)

with \(^{2}:=(0,0)\) as the new neutral element. Intuitively, \(+^{2}\) corresponds to vector addition, and \((-)_{^{2}}\) to a reflection about the origin with respect to both axes.

SemanticsThe specifications above allow us to reuse the notions from Section 3.1 in order to interpret the components and operations of \(^{2}\). What is left unspecified is the interpretation of the group elements themselves; that is, we have yet to explicate what an object of \([]\) looks like. The quest is a short one; the notion of a direct sum carries over to matrices, and is defined as:

\[:=&\\ &\] (12)

From this, we get the (rather straightforward) interpretation \((_{1},_{2})_{1}_{2}\).

ImplementationIn practice, we now split the vector space in two independent parts. The first part is modulated by orthogonal matrices from \(\), and the second part by orthogonal matrices from \(\). For a query \(q\) and a key \(k\) that reside at a relative distance of \((h,w)\), their attention score is computed as \(q(^{h}^{w})k\) - see Figure 1 for an illustration. Each axis contributes an additive but separable factor to the attention score, forcing the model to learn contextual alignments between token pairs on the basis of their coordinate-wise distances. Not much else is different: we can still compute all matrices in parallel, temporally bound by a logarithmic complexity of \(_{2}((h,w))\) and \((h,w)()^{2}\) storage space, given a grid of size \((h,w)\). Subquadratic memory complexity can once more be achieved by virtue of diagonalization, just as in the sequential case.

### Variants & Extensions

The structures that we have seen so far are not the only ones that our methodology can tackle - in fact, many other group-like structures are amenable to similar interpretations. We sketch out some enticing examples below.

Absolute PositionsOur analysis has so far focused on paths _relative_ to positions. Fixing the point of origin allows a straightforward simplification to _absolute_ positions. The new structure is that of a _monoid_: there's no longer an inversion, and laws L1 and L2 only are now in effect. The framework remains largely unchanged: one can still use subgroups of matrices to represent positions, except this time applying them on either the queries or the keys (rather than both).

Periodic DomainsUnder addition, the integers form an _infinite_ cyclic group. An interesting twist would be to consider the positional encodings of _finite_ cyclic groups instead. Such structures are not uncommon; in chemistry, for instance, a benzene molecule comprises six carbon atoms arranged in a ring. The semantics of such a structure would need to be of a matching period; that is, we would need a generator \(\) such that \(^{6}=\). Such a parameterization is straightforward; we simply need to fix the orthogonal matrix so as to have it implement rotations at angle-multiples of \(/3\).

Time Series & SubsamplingOur sequential case analysis assumed a dense sequence with a uniform sampling rate. However, our strategy also applies to any series, even if sparsely sampled, as long as the sampling rate is quantized (_i.e._, a multiple of some constant step). That is, positional indices (and their representations) do not need to match the placement of tokens in the sequence.

Composite GroupsThe direct sum interpretation of Section 3.4 is applicable for arbitrary groups that can be described as products, commutative or otherwise. This allows the representation of positional encodings for several other kinds of composite structures that can be concocted using the same principles, such as sequences of trees, trees of grids, etc.

Beyond Dot-Product AttentionThroughout the previous sections, we have adopted a dot-product formulation for the attention weight function. Nonetheless, ape can be readily integrated into any other attention mechanism, such as linear (Katharopoulos et al., 2020), cluster (Vyas et al., 2020) and "softmax-free" (Lu et al., 2021) variants, _inter alia_.

## 4 Experiments

To assess the viability of our approach, we conduct a series of experiments across a range of tasks, in setups that allow for replicable and reliable comparisons with alternatives. When using ape, we follow Wu et al. (2021) in scaling the dot-product score between two tokens at a distance of \(p\) (_i.e._, \(p\) steps away) by \(p^{c}\); here, we set \(c:=0.98\). This serves to stabilize training by introducing a locality bias (or long-distance decay) factor. For the sake of parameter compression, we share the orthogonal matrices between the different encoder/decoder layers, but use a distinct matrix (or collection of matrices) per head. To isolate and quantify the effect of initialization, we report results on two

Figure 1: Example paths and their interpretations across the structures examined.

different initialization strategies: one where the orthogonal operators are set to mimic rope rotations (default), and one where they are set to be close to the identity (no init). Similarly, to isolate and quantify the effect of trainability when comparing to rope, we report results over both fixed (frozen) and trainable (tuned) rotation angles.

We provide an extensive account of our experimental setups in Appendix B.

### Sequence Transduction

Machine TranslationFirst, we follow Vaswani et al. (2017) in training a Transformer\({}_{}\) model on machine translation over wmt14 en\(\)deBojar et al. (2014).

To provide a comprehensive comparison, we pit our proposed methodology against standard positional encoding schemes from the literature: the vanilla _Sinusoidal_ encodings of Vaswani et al. (2017), the _Absolute_ encodings of Gehring et al. (2017), the _Relative_ encodings of Shaw et al. (2018) and the _Rotary_ encodings of Su et al. (2023). To ensure a fair comparison, we allow all models the exact same budgets (both memory and time).

Synthetic TasksWe further examine three standard sequence transduction tasks: sequence copying, sequence reversal, and sequence repetition. These are meant to directly assess each model's capacity for algorithmic induction, in setups where explicit position-based addressing, both absolute and relative, is required.

### Tree Transduction

Next, we consider four algorithmic transduction tasks on binary branching trees: tree copying, recursive tree rotation up to a fixpoint, algebraic reduction of C\({}_{3}\) expressions, and self-referential tree manipulation; see Appendix B for details.

In addition to previous sequential baselines, we compare our model to the encodings of Shiv and Quirk (2019, _Tree-SQ_). For all four tasks, we experiment with both breadth-first and depth-first decoding.

### Image Recognition

Finaly, we train a Compact Convolutional TransformerHassani et al. (2021) on cifar-10Krizhevsky et al. (2009).

Typically, attention-based architectures for vision rely on additive positional encoding schemes, applied on the image prior to it being sequentialized (row-by-row flattened). Here, we compare fixedWang and Liu (2019); _Sinusoidal 2D_] and parametricGehring et al. (2017); _Absolute_] variants of the above against both the sequential and the grid-structured versions of our scheme.

### Results

We repeat each experiment three times, varying the seeds used for weight initialization and optimization, but fixing the data across repetitions. We report means and 95% CIs in Table 2. We highlight each category's best (in green), and underline scores where the CI spans the mean of the respective best.

At the macro level and consistently across modalities, domain-appropriate algebraic interpretations match or surpass strong and specialized baselines - without _any_ hyper-parameter tuning or search. Specifically, across the 13 setups considered, ape is the uncontested top performer in 8, ranks among the best in 3, and falls within the confidence margin of the top performer in one. Exceptionally, in the breadth-first version of the tree-copy task, tree algebraic encodings are surpassed by a handful of sequential alternatives; this is no surprise, since in this case the tree structure is practically a task-irrelevant syntactic confound. Perhaps more surprisingly, in the breadth-first version of the tree-manipulation task, tree algebraic encodings are surpassed only by their non-initialized, sequential version; an anomaly likely due to a single repetition with an unusually low perplexity score.

We also note three general trends. First, initializing ape to match rope frequency bands at the start of training consistently and significantly improves performance, possibly because rope rotary primitives have undergone empirical tuning for stability and performance. Second, given identical initialization, a sequential ape generally outperforms a trainable rope, despite their theoretical equivalence. This might be due to the difficulty of optimizing periodic signals (_i.e._, rope's trigonometric functions) compared to ape's (orthogonal) matrix multiplications. Third, a frozen rope performs comparably to a randomly initialized ape in most tasks considered, suggesting that adjusting rotoreflection angles during training is not necessarily better than adjusting rotation planes while keeping the angles fixed. Contrary to all the above, a frozen rope weakly outperforms both a tunable rope and an initialized ape in the neural machine translation task; likely an artifact of attention overfitting to specific positional patterns.

## 5 Related Work

Dense attention is by now a foundational component of various problem- and domain-general architectures. Combined with its structural indifference, this underscores the pressing need for learning strategies capable of injecting structural biases directly at the representation level. As such, positional encodings have garnered significant community attention in recent years - too much, in fact, to permit an exhaustive enumeration here. An extensive survey and meta-review is provided by Dufter et al. (2022) who group and rank these works on the basis of several criteria. Our work presents a universal, intuitive and formally grounded recipe that meets _all_ these criteria: it is _trainable_, amenable to problem-specific and data-driven tuning; _reference-adjustable_, allowing both absolute and relative positional specifications; _unbounded_, capable of representing enumerably infinite positions irrespective of model instantiation and/or the targeted data size; _contextual_, implementing a dynamic effect that varies depending on token content; _effective_, consistently matching or surpassing

Table 2: Experimental results and baselines across the tasks considered.

baselines in the tasks considered; and, finally, _efficient_, exhibiting generally favorable asymptotic complexities.

We must point out that the concept of positional encodings as sequence homomorphisms has already been hinted at, first by Wang et al. (2020) and later by Su et al. (2023), even if not explicitly formulated as such. Despite approaching the problem from different angles, both approaches interpret positions as multiplicative, norm-preserving (rotation-like) operations. Our proposal expands upon these two, first in providing a proper algebraic framing of the problem, and second in extending the interpretation from rotations around the axes to rotations and reflections about arbitrary planes. In the case of a single generator matrix (_i.e._, sequences), this difference turns to be non-essential, being practically neutralized by the Transformer's trainable weights. This no longer holds, however, in the case of multiple generator matrices (_i.e._, grids or trees), where each generator should be able to rotate and reflect different sets of planes. In that sense, algebraic positional encodings offer an appealing unifying perspective of a multidimensional generalization to the aforementioned rotation-based frameworks. This sentiment is shared by Lim et al. (2023) who, in parallel to our work, similarly advocate for positional encodings as group homomorphisms, there framed as irreducible group representations. Modulo presentation, the two approaches are variations on a common theme; theirs is technically concerned with post-hoc representation of symmetries and equivariances at a per-datum scale, whereas ours focuses on the interpretation of domain signatures at the dataset scale.

More generally, algebraic manipulations are not uncommon in modern machine learning literature. The recognition of abstract algebra as a practical tool for imposing structural well-behavedness has led to its increased adoption as the go-to recipe for structure-informed neural architectures, largely obsoleting the inefficient and _ad hoc_ augmentation routines of the past. This line of work can be traced back to the group equivariant convolutions of Cohen and Welling (2016), which have by now bloomed into a field of their own; see Weiler et al. (2023) for an up-to-date overview.

## 6 Limitations

We recognize weaknesses and limitations across three fronts. On the _theoretical_ front, we have limited our scope to simple inductive groups, consciously ignoring potential interpretations of more complex constructions. We defer this to future work. On the _empirical_ front, having to recompute positional encodings once per batch increases a model's temporal complexity during training. While this is barely noticeable in sequential and grid constructions, which scale logarithmically, it becomes evident when dealing with complete trees, which scale linearly and require explicit for-loops. On the _epistemic_ front, we conducted a limited set of experiments, focusing primarily on replicability and fairness. We leave more exhaustive empirical comparisons on practical downstream tasks to future work or interested parties.

## 7 Conclusion

We have presented a theoretically motivated approach towards constructing positional encodings for a variety of structures. Without any significant modification or overhead, our methodology can capture sequences and their (multi-dimensional as well as multi-branching) generalizations. In doing so, it reconciles powerful but structurally oblivious models with their missing inductive biases, permitting structure-aware architectural refinements across a range of tasks and setups (see also Kogkalidis et al. (2024) for parallel work employing the methodology in a neurosymbolic representation learning setup). Beyond that, our approach grants full control over how these biases are to be implemented, while also being amenable to adjustments and extensions. Our work indicates that generality and extensibility are not _in spite of_, but rather _due to_ structural discipline and abstraction. We perceive it as an important step towards data-efficient, general, and transparent models of neural computation.