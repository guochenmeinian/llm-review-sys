# Black-box Backdoor Defense via

Zero-shot Image Purification

 Yucheng Shi\({}^{1}\) Mengnan Du\({}^{2}\) Xuansheng Wu\({}^{1}\) Zihan Guan\({}^{1}\) Jin Sun\({}^{1}\) Ninghao Liu\({}^{1}\)

\({}^{1}\)School of Computing, University of Georgia

\({}^{2}\)Department of Data Science, New Jersey Institute of Technology

{yucheng.shi, xuansheng.wu, zihan.guan, jinsun, ninghao.liu}@uga.edu

mengnan.du@njit.edu

Corresponding Author

###### Abstract

Backdoor attacks inject poisoned samples into the training data, resulting in the misclassification of the poisoned input during a model's deployment. Defending against such attacks is challenging, especially for real-world black-box models where only query access is permitted. In this paper, we propose a novel defense framework against backdoor attacks through Zero-shot Image Purification (ZIP). Our framework can be applied to poisoned models without requiring internal information about the model or any prior knowledge of the clean/poisoned samples. Our defense framework involves two steps. First, we apply a linear transformation (e.g., blurring) on the poisoned image to destroy the backdoor pattern. Then, we use a pre-trained diffusion model to recover the missing semantic information removed by the transformation. In particular, we design a new reverse process by using the transformed image to guide the generation of high-fidelity purified images, which works in zero-shot settings. We evaluate our ZIP framework on multiple datasets with different types of attacks. Experimental results demonstrate the superiority of our ZIP framework compared to state-of-the-art backdoor defense baselines. We believe that our results will provide valuable insights for future defense methods for black-box models. Our code is available at https://github.com/sycny/ZIP.

## 1 Introduction

Machine learning has been increasingly integrated into real-world applications such as healthcare , finance  and computer vision . Despite great success, machine learning models are susceptible to adversaries such as backdoor attacks [15; 8; 43; 54; 17], which compromises model security and reliability. Backdoor attacks can manipulate a model's behavior by injecting malicious samples into the training data or altering the model's weights. Although many defense strategies have been proposed to mitigate backdoor attacks, most of them require access to the model's internal structure and poisoned training data . Deploying these defenses is challenging in real-world black-box scenarios, where defenders do not have access to verify or audit the inner workings of the model [18; 19]. For example, many developers and end-users now prefer using machine learning as a service (MLaaS), relying on models provided by third-party vendors for their applications. However, these models may contain backdoors, and due to copyright concerns, the services typically operate in a black-box setting with query-only access. In such scenarios, detecting and mitigating backdoor attacks is very difficult.

Currently, there are only a few backdoor defense methods that work in black-box settings. They can be categorized into two types: detecting-based and purification-based. Detecting-based methods [65;18; 31; 12; 56] can detect the poisoned sample but could not remove the poisoned patterns. These methods are not applicable when critical poisoned samples must be used by downstream classification models. Purification-based methods can address this problem since they aim to retrieve clean images given the poisoned images. However, state-of-the-art purification approaches rely on masking and reconstructing the poisoned area [42; 53]. It can only protect against patch-based attacks. Other purification-based methods that employ image transformations for defense have the potential to defend against more sophisticated attacks but may result in a reduction in classification accuracy due to the loss of semantic information .

To overcome these challenges, we propose a novel framework to defend against attacks through _Zero-shot Image Purification_ (ZIP). We define _"purification"_ as the process of maximizing the retention of important semantic information while eliminating the trigger pattern. With this goal, we preserve the classification accuracy while breaking the connection between trigger patterns and poisoned labels. We also define _"zero-shot"_ as the ability to defend against various attacks without relying on prior knowledge of attack methods. In other words, our approach does not require access to any clean or poisoned image samples (patch or non-patch based), and could be applied directly to unseen attack scenarios. This setting is crucial because real-world users usually have limited information, while new threats continue to emerge. Our proposed framework contains two main stages: we first utilize image transformation to destruct the trigger pattern, and then leverage an off-the-shelf, pre-trained diffusion generative model to restore the semantic information. Our defense strategy is based on the motivation that the semantic information in a poisoned image (e.g., faces, cars, or buildings) constitutes the majority of the data and typically falls within the training data distribution of a pre-trained image generation model. In contrast, engineered trigger patterns (e.g., mosaic patches and colorful noise) are subtle and unlikely to exist in the pre-training datasets [32; 34]. Since the diffusion model learns to sample images from the training distribution , purified images generated from the diffusion model will retain only their semantic information while eliminating the trigger patterns. As a result, our purification approach can effectively defend against various attacks while maintaining high-fidelity in the restored images.

Our main contributions are summarized as follows. (1) We develop a novel defense framework that can be applied to black-box models without requiring any internal information about the model. Our method is versatile and easy to use without retraining. (2) The proposed framework is designed for zero-shot settings, which do not require any prior knowledge of the clean or poisoned images. This feature relieves end-users from the need to collect samples, thus enhancing the framework's applicability. (3) Our defense framework achieves good classification accuracy on the purified images, which were originally poisoned samples, even when using an attacked model as the classifier. This improvement further enhances the framework's effectiveness and usability.

## 2 Preliminaries

### Problem Definition

This paper addresses the backdoor defense problem in the context of image classification tasks. The goal of image classification is to learn a function \(f_{}()\) that maps input images \(\) to their correct labels \(y\), where \(\) denotes the image space, \(\) denotes the label space, and \(\) represents model parameters. Typical backdoor attacks include poisoning training data and perturbing model weights, and we use \(f_{}^{attack}\) to denote the model that has been attacked. During inference, an attacker can take a clean sample \(\) and manipulate it to create a backdoor sample \(^{P}\), e.g., adding the trigger pattern \(\) to make \(^{P}=+\). The backdoored model will misclassify \(^{P}\) as the target label \(y^{target}=f_{}^{attack}(^{P}) y\). Our **threat model** is in a challenging black-box setting, where defenders can only query the poisoned model and have no access to the model's internal parameters or training datasets. The attackers can modify model components or any other necessary information to implement their attacks. We formally define our backdoor defense problem as below.

**Problem 1**.: _Image Purification for Backdoor Defense. Our defense is implemented in the model inference stage. Let \(f_{}^{attack}\) denote the attacked model, whose parameters are not accessible. Given a poisoned image \(^{P}\), our goal is to obtain a purified image \(^{*}\) from \(^{P}\) by removing the effect of trigger \(\). The purified image should be classified as the same category as the original clean image \(\), i.e., \(f_{}(^{*})=f_{}() f_{}(^{ P})\)._

### Diffusion Models

We leverage the reverse process of diffusion models to purify images. The denoising diffusion probabilistic model (DDPM)  is a powerful generative model for generating high-quality images. It has two processes: a forward process and a reverse process. In the forward process, the model iteratively adds noise to an input image \(_{0}\) until it becomes random Gaussian noises \(_{T}\); in the reverse process, the model iteratively removes the added noise from \(_{T}\) to recover the noise-free image \(_{0}\). More details can be found in the DDPM paper .

**Forward Process:** A noise-free image \(_{0}\) is transformed to a noisy image \(_{t}\) with controlled noise. Specifically, Gaussian noise \(\) is gradually added to image \(_{0}\) in \(T\) steps based on a variance schedule \(_{t}\) such that \(q(_{t}_{t-1}):=(_{t};}_{t-1},_{t})\), where \(q\) denotes the posterior probability of \(_{t}\) conditioned on \(_{t-1}\). A nice property of this process is that the \(t\)-th step noisy image \(_{t}\) could be directly generated by:

\[q(_{t}_{0})=(_{t };_{t}}_{0},(1-_{t}) ),\ \ _{t}=_{t}}_{0}+_{t}},\] (1)

where \((0,)\), \(_{t}=1-_{t}\), and \(_{t}=_{i=1}^{t}_{i}\).

**Reverse Process:** The noisy input image \(_{T}\) is transformed into a noise-free output image \(_{0}\) over time steps. In each step, the diffusion model takes the current image state \(_{t}\) as input and produces the previous state \(_{t-1}\). We aim to obtain clean images \(_{0}\) by iteratively sampling \(_{t-1}\) from \(p(_{t-1}|_{t},_{0})\):

\[_{t-1}_{t-1}}_{t}}{1-_{t}}_{0}+}(1-_{t-1} )}{1-_{t}}_{t}+_{t},\ \ \ (0,),\ \ \ _{t}^{2}=_{t-1}}{1-_{t}}_{t}.\] (2)

Based on Equation 1, the original clean image \(_{0}\) could be approximated based on the \(t\)-th step observation \(_{t}\) as: \(_{0 t}=_{t}}}(_{t}-_{t}}_{t})\), where \(_{t}\) denotes the estimation of the real \(\) in step \(t\). In each step \(t\), DDPM utilizes a neural network \(g_{}()\) to predict the noise \(_{t}\), i.e., \(_{t}=g_{}(_{t},t)\). With this estimation, we can convert Equation 2 into the following form as reverse process:

\[_{t-1}_{t}}}(_{t}- }{_{t}}}_{t})+_{t} .\] (3)

## 3 Zero-shot Image Purification (ZIP)

### Overview of Proposed Framework

Our defense framework consists of two primary stages: (1) destruct poisoned images through image transformation, and (2) recover images using a diffusion model, as depicted in Figure 1. The first stage is based on the observation that the integrity of trigger patterns is crucial for backdoor attacks to deceive the model. Thus, destructing the trigger pattern would significantly reduce the effectiveness

Figure 1: The proposed **ZIP** backdoor defense framework. In Stage 1, we use a linear transformation such as blurring to destruct the trigger pattern in poisoned image \(^{P}\). In Stage 2, we design a guided diffusion process to generate the purified image \(_{0}\) with a pre-trained diffusion model. Finally, in \(_{0}\), the semantic information from \(\) is kept while the trigger pattern is destroyed.

of backdoor attacks [34; 46]. On the other hand, a strong transformation may destroy both the trigger pattern and semantic information. To address this, we introduce the second stage to recover the semantic information through the reverse process of diffusion models. However, traditional diffusion models (e.g., DDPM ) or image restoration models (e.g., DDRM  and DDNM ) can not generate high-fidelity and clean images due to a lack of direct control over the generated output.

To bridge the gap, in the following section, we first propose an image generation constraint based on the image transformation in Section 3.2. We then apply this constraint to guide the image reverse process of the diffusion model. In addition, we discuss the guide adaptation when applied in the zero-shot settings, along with its theoretical justification in Section 3.3. Finally, we introduce our efforts to improve the inference speed in Section 3.4.

### Image Transformation and Decomposition

In the first stage, we apply image transformation to destruct potential trigger patterns, such as using average pooling to blur the poisoned images. Formally, we denote the transformation as a linear operator \(\), and let the transformed image be \(^{A}=^{P}=(+)\). Previous research [34; 47] has used the transformed image directly as the purified result. However, such approaches may result in poor classification accuracy due to the loss of fidelity induced by \(\).

To recover the lost information, an intuitive way is to apply an image generative model, e.g., the diffusion model, to yield a purified image. However, the vanilla diffusion model generates images from random Gaussian noise and lacks control over the fidelity of generated images. Thus, we propose a constraint to guide the generation process of diffusion models to recover high-fidelity images. Specifically, for an ideally purified image \(_{0}\), it should satisfy \(_{0}=\), so we have:

\[(_{0}+)=(+)= ^{A}.\] (4)

Based on the RND theory [48; 58], it is possible to decompose an image \(\) into two parts using a linear operator \(\) (e.g., average pooling) and its pseudo-inverse \(^{}\) (e.g., upsampling) that satisfies \(^{}=\). The decomposition is expressed as \(=^{}+(- ^{})\), where the former part denotes the observable information in the range-space, and the latter part denotes the information in the null-space removed by transformation1. Bringing this decomposition to Equation 4, we have:

\[(_{0}+)=^{}(_{0}+ )+(-^{})(_{0}+).\] (5)

This equation derives a constraint for image purification to restore the original \(\) as below:

\[_{0}=^{}^{A}-^{} +(-^{}) _{0},\] (6)

accordingly, the ideally purified image could be decomposed into three parts. The first two parts are in the range space: the observable information stored in the transformed image \(^{}^{A}\), and the intractable information embedded in the transformed trigger pattern \(^{}\); the last part is in the null-space and is unobservable as it is removed by the transformation. To restore the lost information in the null-space, we utilize the observable information in the range-space as references.

### Image Purification with Diffusion Model

#### 3.3.1 Reverse Process Conditioned on Poisoned Images

Our proposed image purification is based on the reverse process of the diffusion model, which takes Gaussian noise \(_{T}\) as input and generates a noise-free image \(_{0}\). We use \(_{t}\) to denote the image at time step \(t\) in the reverse process of diffusion. To generate high-fidelity images, we propose a **rectified estimation** of \(_{t}\) as \(_{t}^{}\), so that it produces \(_{0 t}\) that satisfies the decomposition constraint in Equation 6. The \(_{t}^{}\) is computed using the following equation, which is derived from Equation 1 and 6. The detailed proof is in the Supplementary Material A.

\[_{t}^{}=_{t}}^{} ^{A}-_{t}}^{}+(-^{})_{t}+^{} _{t}}_{t},\] (7)where \(_{t}\) denotes the estimated noise, which is calculated using the pre-trained diffusion model \(g_{}\): \(_{t}=g_{}(}_{t},t)\). Then, we modify the original reverse process in Equation 3 to accommodate this rectified estimation. The modified reverse process is expressed as:

\[}_{t-1}}}(} {}^{}}^{A}-}}^{ }}}+(}-}^{ }})}_{t}+}^{}}}_{t}-}{_{t}}}_{t})+_{t},\] (8)

where \((0,})\) denotes Gaussian noise and we have \(_{t}^{2}=_{t-1}}{1-_{t}}_{t}\).

#### 3.3.2 Adapting Reverse Process to the Zero-shot Setting

In this subsection, we show how our reverse process design can be effectively applied in the zero-shot setting, where the trigger pattern \(}\) is unknown to defenders. We propose to omit the intractable term \(_{t}}}^{}}}\) in Equation 7, and approximate \(}_{t}^{}\) with \(}}_{t}\) to obtain a new rectified estimation:

\[}}_{t}=_{t}}}^{} {}^{A}+(}-}^{}}) }_{t}+}^{}}_{t}}_{t}.\] (9)

The intractable term \(_{t}}}^{}}}\) can be omitted due to the following reasons:

* The value of \(_{t}}\) at the beginning of the reverse process is very small, making \(_{t}}}^{}}}\) negligible compared to other terms. We provide an improved approximation in Section 3.3.3 for later stages when \(_{t}}\) increases.
* Backdoor attacks are generally stealthy or use more negligible patterns compared to the original images since the attacker wants to minimize the impact on the model's accuracy on legitimate data . Thus, the destructed pattern \(}}\) is usually negligible compared to \(}^{A}=}(}+})\).
* The effect of \(}^{}}}\) is further reduced by selecting appropriate image transformations. Most backdoor attacks are characterized by severe high-frequency artifacts . Therefore, transformations such as average pooling can remove the high-frequency information in \(}\).

Nevertheless, approximation in an iterative process such as diffusion can be risky because errors from the previous step can accumulate rapidly (e.g., exponentially) and lead to significant inaccuracies. Our method addresses this issue by ensuring the approximation error between each step is well-bounded theoretically. As a result, the final recovered image \(}}_{0}\) preserves the essential information of the original image, while the trigger pattern undergoes a transformation and is likely to be destroyed.

**Lemma 3.1**.: _Suppose the estimated noise output by \(g_{}()\) is Gaussian. Given \(g_{}(}_{t},t)=_{t}\), we have \(g_{}((}_{t}+_{t}}}^{ }}}),t)=_{t}+_{t}^{}\), where \(_{t},_{t}^{}\) are also Gaussian._

The error \(_{t}^{}\) in Lemma 3.1 is much smaller than the real estimated noise \(_{t}\). This is because the trigger pattern \(_{t}}}^{}}}\), which is weighted by \(_{t}}\), is relatively subtle compared with the intermediate image \(}_{t}\). Additionally, the attacker-engineered trigger pattern \(}\) is unlikely to be present within the natural image distribution learned by the pre-trained diffusion model. Therefore, it is unlikely to activate the model \(g_{}()\) and generate significant output.

**Theorem 3.2**.: _Suppose that \(g_{}((}_{t}+_{t}}}^{ }}}))=_{t}+_{t}^{}\). We define the error at step \(t\) between \(}}_{t}\) and \((}_{t}+_{t}}}^{}}})\) as \(_{t}^{}\), i.e., \(_{t}^{}=}}_{t}-(}_{t}+ _{t}}}^{}}})\). Let \(_{t}=}_{t}^{}\), we have the following bound on its norm: \(\|_{t}\|_{t})_{t+1}}}{ _{t+1}}}\|}\|\|_{t+1}^{}\|\)._

This theorem means that our proposed approximation introduces only limited approximation error at each time step after considering the error from the previous step. When paired with a common linear transform \(}\) like average pooling, its norm is relatively small . Additionally, at the initial steps of the reverse process, the term \(\|_{t+1}^{}\|\) is also small according to Lemma 3.1, and we have \(_{t})_{t+1}}}{_{t+1} }}_{t+1}+_{t}}{2}\), which is also a small value. Collectively, the error from our approximation is well bounded by a small number. According to this theorem, the final purified image exhibits the following property.

**Corollary 3.2.1**.: _When \(t=0\), we have \(}}_{0}=}_{0}+}^{}}}+_{0}^{}\), where \(}_{0}^{}=}\)._

According to Corollary 3.2.1, our final purified image contains three parts: the ideally purified image \(}_{0}\), the altered trigger pattern \(}^{}}}\), and an approximation error \(_{0}^{}\). Therefore, compared with the original poisoned image \(}^{P}=}+}\) or the transformed image \(}^{A}=}(}+})\), our purified image \(}}_{0}\) reduces the effect of the trigger pattern \(}\) and preserves the high fidelity of the image. This is critical for achieving good performance in the downstream image classification task. The proofs can be found in Supplementary Material A.

#### 3.3.3 Improving ZIP with Theoretical Insights

Based on the above theoretical analysis, we propose two further improvements to our guided image purification framework to enhance its effectiveness, which include (1) adding a confidence score to the rectified estimation, and (2) introducing multiple transformations.

**Confidence Score.** The effectiveness of our framework depends on the confidence of the proposed rectified estimation. As the reverse process proceeds, the \(_{t}}\) value increases, making it difficult for \(_{t}}^{}\) to be neglected. Simply omitting this pattern would weaken the confidence in our proposed estimation.

To address this issue, we re-formulate the rectified estimation at step \(t\) as: \(}_{t}=(1-_{t}^{})}_{t}+ {}_{t}^{}_{t}\), where \(_{t}\) denotes the result of a pure diffusion model (e.g., DDPM) at step \(t+1\), and \( 0\) is a hyper-parameter. Since \(0<_{t}<1\), when \(=0\), we obtain a pure reverse diffusion process without any rectification; when \(=+\), the reverse process reduces to ZIP. A nice property of \(}_{t}\) is that, \(_{t}\) increases as the reverse process process (\(t\) decreases), making the contribution of \(_{t}\) larger. This is reasonable because the intermediate image becomes increasingly informative over time, reducing the reliance on \(}_{t}\) in reverse diffusion. We set \(\) value to achieve desirable fidelity scores (e.g., PSNR [29; 58]) for the resultant purified images. Finally, our revised reverse process is defined as:

\[_{t-1}_{t}}}( }_{t}-}{_{t}}}_{t} )+_{t}\,.\] (10)

**Multiple Transformations.** To improve the effectiveness of our approach in removing the poisoning effect, we propose including multiple transformations to more effectively destroy the unknown trigger pattern in a zero-shot setting. Specifically, given \(N\) transformations, the intermediate image \(_{t}\) and estimated noise \(_{t}\) should all satisfy:

\[}_{t}^{n}=_{t}}_{n}^{} ^{A_{n}}+(-_{n}^{}_{n})_{t}+_{n}^{}_{n}_{t}} _{t},\ \ n=1,2,...,N.\] (11)

Finally, we have \(}_{t}=(}_{t}^{1}+}_{t }^{2}+...+}_{t}^{N})\). We provide our proposed algorithm in Algorithm 1. We focus on two types of transformation pairs in this paper: blurring, represented by \(_{1}^{}_{1}\), and gray-scale conversion, represented by \(_{2}^{}_{2}\).

### Purification Speed-up

The purification speed is crucial when performing defense at inference time. Our framework leverages pre-trained diffusion models, and conducts purification on each test image. Hence, we propose several techniques to speed up purification and reduce costs.

#### 3.4.1 Diffusion Model Inference Speed-up

Algorithm 1 requires a large number of steps to generate a single sample. Each step involves computing the estimated noise and diffusion process, which can be computationally expensive. To address this issue, we leverage the power of the denoising diffusion implicit model (DDIM)  to improve the inference speed of our reverse process as below:

\[_{t-1}_{t-1}}}_{0|t}+ _{t-1}-_{t}^{2}}_{t}+_{t },\] (12)

where \(}_{0|t}\) is also estimated image based on \(}_{t}\), and we have \(}_{0|t}=_{t}}}(}_ {t}-_{t}}_{t})\). By using this speed-up inference method, we can obtain a high-fidelity purified image by sampling only a few steps instead of conducting sampling in thousands of steps. The modified algorithm based on DDIM is provided in the Supplementary Material D.

```
0: Test image for purification \(^{r}\); linear transformation \(_{1},_{2},...,_{n}\) and their pseudo-inverse \(_{1}^{},_{2}^{},...,_{n}^{}\); diffusion model \(g\); hyperparameter \(\).
0:\(_{t}^{n}=_{n}^{r},\ n=0,1,...,N\)
1:\(^{T}(,)\)
2:for\(t=T,T-1,...,1\)do
3:\((,)\) if \(t>1\), else \(=\)
4:\(_{t}=g_{}(_{t},t)\)
5:for\(n=1,2,...,N\)do
6:\(}_{t}^{n}=_{t}}_{1}^{} ^{A_{n}}+(-_{n}^{}_{n}) _{t}+_{n}^{}_{n}_{t} }_{t}\)
7:endfor
8:\(}_{t}=(}_{t}^{1}+}_{t }^{2}+,...,+}_{t}^{N})\)
9:\(}_{t}=(1-_{t}^{})}_{t}+ {}_{t}^{}_{t}\)
10:\(_{t-1}_{t}}}(}_ {t}-}{_{t}}}_{t})+ _{t}\)
11:endfor
12:return\(_{0}\) ```

**Algorithm 1** Zero-shot Image Purification

Algorithm 1 requires a large number of steps to generate a single sample. Each step involves computing the estimated noise and diffusion process, which can be computationally expensive. To address this issue, we leverage the power of the denoising diffusion implicit model (DDIM)  to improve the inference speed of our reverse process as below:

\[_{t-1}_{t-1}}}_{0|t}+ _{t-1}-_{t}^{2}}_{t}+_{t },\] (13)

where \(}_{0|t}\) is also estimated image based on \(}_{t}\), and we have \(}_{0|t}=_{t}}}(}_ {t}-_{t}}_{t})\). By using this speed-up inference method, we can obtain a high-fidelity purified image by sampling only a few steps instead of conducting sampling in thousands of steps. The modified algorithm based on DDIM is provided in the Supplementary Material D.

#### 3.4.2 Framework Speed-up

**Batch Data Speed-up.** We introduce an acceleration method for purification, when images can be processed in batch during inference. For instance, if the pre-trained model accepts images of size \(L L\) as input, and we wish to purify \(l l\) images, we can first combine \((L/l)^{2}\) images into a single \(L L\) image by tiling. We then apply our purification method to the tiled image, and split the purified image back to the original images afterwards. Larger images could be resized into \(l l\) to apply this method. In this way, our defense approach can handle images of various sizes and significantly improve the purification speed.

**Streaming Data Speed-up.** In the streaming data scenarios, defenders may only have access to a single test image at a time. In such cases, we can use a fast zero-shot detection method such as  to quickly assess whether the image is likely to be poisoned. If the detection result indicates a potential poisoning, we can apply our purification method to remove the trigger pattern. This approach allows us to quickly and effectively defend against poisoned images without applying our purification on every image, thus saving the average processing time.

## 4 Experiments

### Experimental Settings

For defense evaluation, we conducted experiments on three types of backdoor attacks: BadNet , Attack in the physical world (PhysicalBA) , and Blended . The first two attacks are representatives of patch-based attacks, while the last one represents a non-patch-based backdoor attack. To configure these attack algorithms, we follow the benchmark setting in  and use the provided codes. We evaluate the effectiveness of our defense framework ZIP on three datasets: CIFAR-10 , GTSRB , and Imagenette . The poisoned classification network is based on ResNet-34 . Specifically, for the CIFAR-10 dataset, we apply both blur and gray-scale conversion as linear transformations. For the GTSRB and Imagenette datasets, we solely apply blur as the linear transformation. The additional experiment details are in the Supplementary Material E.

Defense methods available for black-box purification in zero-shot are rare. To fairly assess the effectiveness of our proposed method, we conduct a comparative evaluation against two baseline approaches: ShrinkPad  and image blurring (referred to as Blur). The former is a state-of-the-art image transformation-based defense method that can work on black-box models in the zero-shot setting. The latter uses blurred images \(_{1}^{}^{A_{1}}\) as purified images. We apply defense methods to all test samples, and then evaluate the output using a poisoned classification network. In addition to the clean accuracy (CA) and attack success rate (ASR) metrics for assessing defense effectiveness, we introduce a new metric called poisoned accuracy (PA). It measures the classification performance of the purified poisoned samples. A higher value of PA indicates that the purified poisoned samples are more likely to be correctly classified, even when using an attacked classification model.

### Qualitative Results of Purification

We conduct qualitative case studies (see Figure 2) of our method in purifying poisoned images created by the Blended and BadNet attacks. We show examples of poisoned images \(^{P}\) and purified image \(_{0}\), where the trigger pattern is clearly visible in the poisoned images but has been altered/removed in the purified images. For comparison, we also show the blurred image \(_{1}^{}^{A_{1}}\) and the grayscale images \(_{2}^{}^{A_{2}}\). The transformed images can destruct the trigger pattern, but they also alter a lot of semantic information. These results demonstrate the effectiveness of ZIP in removing the effect of trigger patterns from images while maintaining semantic information. More qualitative results for different attacks are in the Supplementary Material B.

### Quantitative Results of Defense

We conduct quantitative experiments where the results are in Table 1. Our observations are as follows: **(1)** Our method effectively defends against different backdoor attacks. This is because the transformations in our framework, which are independent of specific attacks, break the connection between backdoor triggers and backdoor labels. **(2)** Our method achieves overall better classificationaccuracy (CA) compared to baseline methods because our formulation successfully recovers semantic information to ensure accurate downstream classification. For example, on the Imagenette dataset, ZIP reduces the ASR of the BadNet attack from 94.53% (no defense) to just 7.55%, with only a 0.94% drop in CA. Similarly, on the GTSRB dataset, our approach reduces the success rate of the PhysicalBA attack from 100% (no defense) to 6.57%, with only a 1.39% drop in CA. **(3)** Our method outperforms the baselines in poisoned accuracy (PA), indicating that poisoned samples can still be used for classification even when using an attacked black-box classifier. It demonstrates the robustness and usability of ZIP in real-world scenarios. **(4)** ShrinkPad performs poorly on the PhysicalBA attack, which is consistent with the findings in . This is because PhysicalBA is specifically designed to evade defense methods such as ShrinkPad . On the other hand, our method successfully defends against this attack, demonstrating its superior performance. We include additional experiment results of defenses in Supplementary Material H.

    &  &  &  &  &  \\  & & CA \(\) & ASR \(\) & PA \(\) & CA \(\) & ASR \(\) & PA \(\) & CA \(\) & ASR \(\) & PA \(\) & CA \(\) & ASR \(\) & PA \(\) \\   & None & 80.15 & — & — & — & — & — & — & — & — & — & — \\  & BadNet & 82.31 & 99.98 & 10.00 & 62.89 & 9.34 & 63.12 & 88.19 & 21.78 & 53.47 & **78.97** & **5.53** & **79.10** \\  & Blended & 80.26 & 99.96 & 10.03 & 85.97 & **2.28** & 40.22 & 55.91 & 3.04 & 49.91 & **72.62** & 7.75 & **57.98** \\  & PhysicalBA & 85.30 & 98.73 & 11.20 & **82.84** & 90.50 & 18.37 & 41.84 & **1.09** & 41.37 & 80.10 & 4.33 & **80.33** \\  & Average & 82.62 & 99.56 & 10.41 & 68.23 & 34.04 & 40.57 & 51.98 & 66.44 & 82.55 & **77.23** & **5.87** & **72.47** \\   & None & 96.95 & — & — & — & — & — & — & — & — & — & — \\  & BadNet & 96.53 & 99.99 & 5.70 & 78.33 & **5.81** & 78.82 & 95.98 & 7.33 & 95.11 & **96.18** & 6.19 & **96.03** \\  & Blended & 96.58 & 99.89 & 5.79 & 76.76 & 10.54 & 56.41 & 93.68 & 61.07 & 73.91 & **95.74** & **8.53** & **81.27** \\  & PhysicalBA & 96.83 & 100.00 & 5.70 & **97.41** & 100.00 & 5.70 & 91.00 & **5.53** & 90.53 & 95.44 & 6.57 & **94.91** \\  & Average & 96.65 & 99.96 & 5.73 & 84.17 & 38.78 & 46.98 & 93.55 & 7.98 & 86.52 & **95.79** & **7.10** & **90.74** \\   & None & 84.58 & — & — & — & — & — & — & — & — & — \\  & BadNet & 84.99 & 94.53 & 14.98 & 14.98 & 35.86 & 70.72 & 81.47 & 16.45 & 79.94 & **84.05** & **7.55** & **83.97** \\   & Blended & 86.14 & 99.85 & 10.19 & 74.06 & 20.63 & 36.10 & 78.95 & 79.41 & 25.57 & **81.42** & **8.35** & **78.36** \\   & PhysicalBA & 90.67 & 72.94 & 34.29 & **90.21** & 96.81 & 13.07 & 84.84 & 32.40 & 74.87 & 87.26 & **10.91** & **86.54** \\   & Average & 87.27 & 89.11 & 19.82 & 78.50 & 42.00 & 39.96 & 81.75 & 42.75 & 60.13 & **84.24** & **8.94** & **82.96** \\   

Table 1: The clean accuracy (CA %), the attack success rate (ASR %), and the poisoned accuracy (PA %) of defense methods against different backdoor attacks. _None_ means no attacks are applied.

Figure 2: Qualitative analysis of purification. (Row 1-2: Blended; Row 3-4: BadNets. More qualitative results are listed in Supplementary Material B)

### Ablation Studies

#### 4.4.1 Evaluation on Transformations

Comparison of Different Transformations.Image transformation plays a crucial role in our defense. We evaluate the effectiveness of ZIP on the Imagenette dataset using four distinct types of transformations: Blur\({}_{2}\), Blur\({}_{4}\), Blur\({}_{8}\), and Grayscale (the results are in Table 2). The subscript of "Blur" represents the kernel size of average pooling, where a larger number indicates a stronger transformation. We have the following observations: (1) The blur operation is more effective than grayscale conversion in reducing the ASR while maintaining good CA and PA. (2) Generally, stronger transformations are more effective in destructing the trigger pattern, resulting in a lower ASR. However, they also destroy more semantic information, leading to lower classification accuracy.

Comparison to Masking and Reconstruction.Linear transformations are more effective than masking in removing poisoned effects, particularly in cases like the Blended attack where the backdoor pattern is distributed across the image. We compare ZIP with a recently proposed purification method BDMAE  on CIFAR-10.

BDMAE first identifies the trigger region on a test image, masks the region, and then uses a masked autoencoder to restore the image. To ensure a fair comparison, we apply the defense stage of BDMAE to our benchmarks using its official code. Table 3 shows that BDMAE effectively defends against the BadNet attack, but it fails to defend against the Blended attack. This is because the trigger pattern in Blended attacks is not located in a local region, making it difficult for BDMAE to identify an appropriate mask. In contrast, our ZIP framework does not make assumptions about trigger patterns thus successfully defends against both attacks.

#### 4.4.2 Evaluation on Enhanced Attacks

We consider a more challenging scenario, where we assume the attacker is aware of our defense and has access to the purified backdoor images, which are then used as enhanced poisoned images to attack a classifier. To demonstrate the effectiveness of our proposed method in defending against such enhanced attacks, we apply our method to BadNet and Blended as examples and choose blurring and grayscale conversion together as the transformations of ZIP. Other settings remain the same as the original attack, and more details about the settings are in the Supplementary Material F.1.

During the inference stage, we continue to use the original trigger pattern for attacks. From the experimental results in Table 4, we observe that the original trigger pattern no longer triggers the attack, but if the purification (Blur+Grayscale) we use has the same transformation as the attacker, the attacks can still be triggered. However, by switching to a different transformation, such as solely Blur or Grayscale, the attacks can be effectively mitigated. It is important to note that in practical scenarios, the attacker may not know which transformation the defender is using, therefore the defender should consider using diverse transformations to enhance the defense ability.

   Attack & Original Trigger & ShrinkPad & Blur & ZIP (Blur+Grayscale) & ZIP (Blur) & ZIP (Grayscale) \\  BadNets & 82.36/24.00/76.05 & 69.32/10.34/70.44 & 80.58/31.41/70.70 & 58.01/79.89/28.02 & 82.39/18.47/77.29 & 63.26/73.52/32.91 \\ Blended & 85.42/21.83/48.30 & 75.10/11.71/47.13 & 78.47/63.61/37.04 & 75.79/75.21/33.01 & 81.63/28.83/51.32 & 73.80/40.71/36.22 \\   

Table 4: Defense against enhanced attacks (CA\(\) / ASR\(\) / PA\(\)).

   Attack & Blur\({}_{2}\) & Blur\({}_{4}\) & Blur\({}_{8}\) & Grayscale \\  BadNets & 84.05/7.55/83.97 & 82.77/7.15/82.39 & 72.38/13.41/72.50 & 70.52/10.79/68.61 \\ Blended & 84.05/26.11/63.46 & 81.42/8.35/78.36 & 71.03/7.69/67.13 & 79.79/99.40/10.59 \\ PhysicalBA & 90.24/22.88/79.89 & 87.26/10.91/86.54 & 76.58/19.43/76.43 & 76.66/17.51/74.87 \\   

Table 2: ZIP with different transformations (CA\(\) / ASR\(\) / PA\(\)).

   Attack & No Defense & BDMAE & ZIP \\  BadNet & 82.31/99.98 & 81.44/1.12 & 78.97/5.53 \\ Blended & 80.26/99.96 & 78.57/99.88 & 72.62/7.75 \\   

Table 3: Comparison to BDMAE (CA\(\) / ASR\(\)).

#### 4.4.3 Evaluation on Purification Speed

We analyze the computational costs of classification and purification before and after applying speed-up strategies. Figure 3 presents the average time required per image in CIFAR-10. Our defense method's efficiency can be enhanced with the introduction of our proposed tiling and detection-based strategies, resulting in a 33\(\) and 39\(\) speedup, respectively. The results also show that ZIP can complete the purification step faster than the classification step under various scenarios, highlighting its efficiency. In addition, our method is three times faster than another inference time purification model BDMAE . More details about the speed-up settings are provided in the Supplementary Material F.2.

## 5 Related Work: Backdoor Defense

In this section, we briefly review backdoor defenses here and provide a more detailed discussion in the Appendix G. Existing backdoor defense methods are mainly designed for white-box models [32; 61]. However, these methods [47; 10] often require access to model parameters or original training data, which is not always feasible in real-world scenarios. To address this challenge, existing black-box defense methods are proposed and can be roughly divided into backdoor detection and backdoor purification. Detection models [18; 19; 65; 12; 38; 14; 56; 16] aim to identify and reject any detected poisoned images for further inference as a defense mechanism. However, this approach can limit the usefulness of these methods in practical settings where users expect results for all of their test samples. On the other hand, backdoor purification methods aim to remove the poison effect from the image to defend against attacks. Some methods [42; 53] in this category involve masking the potentially poisoned region and then reconstructing the masked image to obtain a poison-free image. However, these strategies may fail when the trigger patterns are distributed throughout the image, rather than in a specific patch-based location [1; 43; 68; 37]. Another approach  involves applying strong image transformations to the test image to destruct the trigger pattern. While such methods can defend against more advanced attacks, they typically result in a decrease in classification accuracy.

## 6 Conclusion

We propose a novel framework called ZIP for defending against backdoor attacks in black-box settings. Our method involves applying strong transformations to the poisoned image to destroy the trigger pattern. It then leverages a pre-trained diffusion model to recover the removed semantic information while maintaining the fidelity of the purified images. The experiments demonstrate the effectiveness of ZIP in defending against various backdoor attacks, without requiring model internal information or any training samples. ZIP also enables end-users to utilize full test samples, even when using an attacked classification model. Some future directions include designing black-box defense for other data domains and exploring other types of diffusion models.