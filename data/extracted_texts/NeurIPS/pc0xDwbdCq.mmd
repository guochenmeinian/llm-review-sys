# The Structural Safety Generalization

Tom Gibbs\({}^{1}\) & Julius Broomfield\({}^{2}\) & George Ingebretson\({}^{3}\) & Ethan Kosak-Hine\({}^{1}\) &

Tia Nasir\({}^{1}\) & Jazon Zhang\({}^{4}\) & Reihaneh Iranmanesh\({}^{5}\) & Sara Pieri\({}^{6}\) & Reihaneh Rabbany\({}^{7}\) &

&Kellin Pelrine\({}^{7}\)

\({}^{1}\) Independent Researcher

\({}^{2}\) Georgia Institute of Technology

\({}^{3}\) University of California, Berkeley

\({}^{4}\) Stanford University

\({}^{5}\) Amherst College

\({}^{6}\) Mohamed bin Zayed University of Artificial Intelligence

\({}^{7}\) McGill University; Mila

\({}^{}\) Equal contribution. Order was randomly determined.

###### Abstract

It is widely known that AI is vulnerable to adversarial examples, from pixel perturbations to jailbreaks. We propose that there is a key subclass of problems that is also still unsolved: failures of safety to generalize over structure, despite semantic equivalence. We demonstrate this by exposing new vulnerabilities to multi-turn and multi-image attacks, that yield different outcomes from their equivalent-meaning single-turn and single-image counterparts. We suggest this is the same class of vulnerability as that found in yet unconnected threads of the literature, such as vulnerabilities to low-resource languages and indefensibility of strongly superhuman Go Als to cyclic attacks. In contrast to attacks with identical benign input (e.g., pictures that look like cats) but unknown semanticity of the harmful component (e.g., noise that is unintelligible to humans), attacks like these represent a class where semantic understanding and defense against one version should in theory guarantee defense against others--yet current AI safety measures do not. This is a necessary condition to defending against arbitrary attacks. Consequently, our discussion, data, and approaches here frame an intermediate problem for AI safety to solve, that is more tractable than universal defenses and represents a critical checkpoint towards safe AI.

## 1 Introduction

The progress of recent AI systems can be a double-edged sword: new model capabilities potentially open new vulnerabilities. Single-turn attacks are the most extensively explored in the literature surrounding LLMs (Wei et al., 2024; Xu et al., 2024; Shen et al., 2024), yet expanding context windows and agentic capabilities make multi-turn interactions prevalent. Single-image, and even text-only and image-only attacks, are the most studied, but recent systems offer multi-modal, multi-image capabilities. Safety research often focuses on high-resource languages like English, but LLMs today often have capabilities in low-resource ones.

We propose that many of these potential vulnerabilities can be viewed under a common umbrella, which we call the _structural safety generalization problem_, as differentiated from the _semantic safety generalization problem_, and global solutions to safety that encompass both. Structural safety generalization is achieved if, for content with the same semantic meaning, safety with one input structure generalizes to another. For example, defending against harmful instructions in one singleturn or single-image input should ideally defend against the same harmful instructions split into several turns or images. Or defending against an input in one language should ideally defend against its translation in another.

However, current AI systems have not done so. Prior work has shown LLMs are vulnerable to translation attacks where the same content produces safe or harmful responses depending on the language Yong et al. (2023); Deng et al. (2023), and that strongly superhuman Go AIs are as yet unable defend against "cyclic" attacks that are in the same semantic class to humans Tseng et al. (2024); Wang et al. (2023). In this work, we show that structural vulnerabilities also exist in recent LLMs between single-turn and multi-turn inputs, and in recent VLMs between unimodal, single-image, and multi-image ones. When viewed together, a common picture emerges. Solving this problem is a necessary condition to safety in general. Furthermore, maintaining equivalent semantic meaning is a strong constraint, yielding a problem that should be significantly more tractable than robust safety to completely arbitrary inputs. Thus, we suggest that it could be a guiding target for future research.

We note that the structural attacks described differ from many works on adversarial robustness where the benign component has fixed semantic meaning but the harmful one has arbitrary semantics. For example, many attacks on image classifiers maintain the meaning of an image to humans, but the noise applied has unknown or no semanticity, or in some cases may even change the entire meaning to humans Bartoldson et al. (2024). Here, not only benign parts of the input but also the harmful parts, such as the instructions eliciting harmful behavior in the translation, multi-turn, and multi-image attacks mentioned above, have and maintain their meaning. In practice, this difference is crucial: to defend against such attacks, we only need proper generalization over semanticity-preserving structural variations. This can be the difference between seeking vaccines that works against multiple forms of the same disease, and seeking immortality.

To build a foundation for future research to solve the structural safety generalization problem, we contribute:

* The problem framing, building a unifying picture over our findings and previously disparate ones in the literature.
* Experimental results demonstrating new vulnerabilities in LLMs and VLMs, and that they are not just vulnerable but differently vulnerable to single-turn vs multi-turn attacks, and text vs single-image vs multi-image ones.
* Multiple new datasets eliciting these vulnerabilities, where examples come in multiple structural variations with equivalent semantics, as well as data for surrounding experiments like analysis of false positives and guardrails.1

## 2 Background

### Single and Multi-turn Jailbreaks

Jailbreaks are a pervasive and widely-known vulnerability of recent LLMs Wei et al. (2024); Anwar et al. (2024). Much of the literature has focused on attacks with a single input and output--a single turn of a conversation. But LLMs are often deployed in multi-turn settings, as conversational or action-taking agents. Recently, this has been shown to introduce novel vulnerabilities (Russinovich et al., 2024; Yang et al., 2024). Concurrent work by Li et al. (2024) suggests that even when LLMs are well-defended against automated single-turn jailbreaks, they can be vulnerable to human multi-turn jailbreaks. Both their study and ours provide complementary evidence for the critical need to go beyond single-turn setups to strengthen LLM defences. Our work is the first to construct single-turn and multi-turn attacks with meaning held constant, thereby isolating the structural effect.

### Payload Splitting

Payload splitting involves decomposing harmful content into benign components (Schulhoff et al., 2024), which are then recomposed by the target model. Multi-turn jailbreaks, where harm is distributed over multiple prompts, fall squarely in this category. Gong et al. (2023) constructs a visual variation of payload splitting, building on Fragmentation Concatenation Attacks (FCA) (Schulhoff, 2022) by embedding instructions in typographic images and decomposing them into several fragments, leaving the target VLM to recompose them - we describe their visual FCA method as _image decomposition_.

### Cryptographic Prompts

Various encoding techniques have found success in bypassing model safeguards (Wei et al., 2024). For instance, in a _word substitution cipher_ attack, explored in Handa et al. (2024), certain words in a malicious text prompt are replaced with benign word substitutes according to a key, obfuscating the malicious intent. Other obfuscation schemes have also found success, such as the Caesar Cipher (Yuan et al., 2024).

### Structural Attacks

The influential framing of Wei et al. (2024) identifies two primary failure modes that underlie successful jailbreak attacks: competing objectives and mismatched generalization. The latter, of particular interest here, "arises when inputs are out-of-distribution for a model's safety training data but within the scope of its broad pretraining corpus". This problem framing is insightful, but so far has not been solvable (Anwar et al., 2024).

A key piece missing in this framing is that the generalization failures breaking safety are not isolated in the safety part of the system. This is illustrated by the board game Go, where superhuman AIs have not learned in pre-training how to handle "cyclic" groups Wang et al. (2023), and safety measures--which train on far more cyclic groups than pre-training--have still proven ineffective Tseng et al. (2024). It is plausible that if the systems could be made to learn a better representation of cyclic groups in pre-training, like humans do easily (despite being far less skilled in Go overall), then a small number of demonstrations of the vulnerability would be sufficient to generalizability fix it.

Therefore, we propose a lens that can both narrow our view of the problem and expand our view of possible solutions: structural vs semantic attacks, with a particular focus on the former. Many of the attacks in the mismatched generalization umbrella, which change structure with virtually no modification to the meaning of inputs, fall into this category. Besides cyclic groups in Go, LLM-based examples includes translation attacks Yong et al. (2023); Deng et al. (2023), various ciphers Handa et al. (2024); Yuan et al. (2023); Jiang et al. (2024); Wei et al. (2024), and payload splitting Kang et al. (2024). But other mismatched generalization failures, such as "asking for content from a website the model would have seen during pretraining but not mentioned during safety training" Wei et al. (2024), are excluded. Furthermore, this perspective focused on equivalence or difference in meaning highlights that the generalization issue is not limited to _mismatched_ generalization, but generalization overall: neither the safety measures nor the model itself are treating semantically-equivalent inputs equivalently.

## 3 Methodology

To help better understand structural vulnerabilities and how we can study them, we build new datasets in the growing multi-turn and multi-modal areas. We approach this by decomposing harmful goals without changing their semantics, and assessing the difference in vulnerability between the composed and decomposed versions. For multi-turn attacks, the harm is distributed across multiple inputs, rather than fed to the LLM as a single prompt; for multi-modal attacks, the harm is distributed across different modalities rather than input as plain text. If defenses generalized over structure, all these versions would be equally vulnerable or defended.

### Attack Methodologies

Word Substitution CipherTo construct multi-turn jailbreaks, we draw on the _word substitution cipher_ approach (Handa et al., 2024). The original is single-turn; we modify and expand it to also have a multi-turn version. This approach enables us to isolate the impact of the prompting structure, by testing identical prompts packaged in both the single-turn manner and the multi-turn one. Weexamine two versions of "input-cipher" here: random word replacement that leads to a nonsensical instruction before decoding, and "perplexity-filtered" mappings where the encoded instructions also make sense. The construction process is illustrated in Figure 11 and detailed in Appendix F.

Caesar CipherFor the single and multi-turn attacks, we also explore the impact of asking the model to provide output in ciphered form, to observe if this amplifies their potency. For this, we focus on the _Caesar cipher_ due to its simplicity and the limited capacity of the models to interpret more complex ciphers.

Color-Based Word Substitution CipherBuilding on the word substitution concept, we developed a _color-based substitution cipher_ (CBSC) that maps background colors in typographic images to specific key words. The text within the images is benign, containing only neutral words that should not trigger safety mechanisms. We create two different structures within the CBSC: decomposed and composite images. In the decomposed format, described previously, the images are kept separate. However, in the composite format, we concatenate the images into a single, combined image. Visual examples of this attack can be found in Figure 12 and Appendix N.2.

Image DecompositionGong et al. (2023) expanded upon textual payload splitting attacks, creating decomposed typographic images to jailbreak VLMs. We denote these images _unperturbed decomposed images_. To increase the effectiveness and robustness of our multimodal attacks, we introduce color perturbations, yielding _perturbed decomposed images_. In this context, perturbations refer to randomly applied swaps to the background color of the images, where the previously white background is replaced with a solid color. These changes were made without following any optimization process. Like with CBSC, we concatenate decomposed images, both perturbed and unperturbed, to create _composite images_. For visual examples, refer to Figure 13 and Appendix N.2.

### Dataset Overview

Using the approaches above, we create two primary types of datasets used in our evaluations: _multi-turn datasets_ and _multi-modal datasets_.

Multi-TurnThe multi-turn datasets consist of user instructions without assistant responses. These datasets are categorized as follows:

* _Harmful Dataset:_ Contains 4,136 unique user instructions explicitly designed to elicit harmful outputs from the model. Every prompt is provided in both of two distinct structures: (1) _Single-Turn_, (2) _Multi-Turn_.
* _Benign Control Datasets:_ These datasets are used for safety guardrail evaluation rather than direct model evaluation. Further details regarding the safety guardrails can be found in Section 3.5. The benign control datasets include:
* _Semi-Benign Dataset:_ Comprises 1,200 unique user prompts that are benign in intent but contain toxic words, evaluated in both single-turn and multi-turn structures.
* _Completely Benign Dataset:_ Comprises 1,200 unique user prompts with no harmful content or toxic language, and are also evaluated in both single-turn and multi-turn structures.

In total, the multi-turn datasets include the evaluation of 8,272 harmful prompts. For additional information on the construction of these datasets, see Appendix F.

Multi-ModalThe multimodal datasets incorporate both textual and visual inputs. These datasets include:

* _Harmful Dataset:_ This dataset consists of 550 unique harmful instructions, categorized into multiple harm subcategories. These prompts are evaluated across the following six attack methods: (1) _Color-Based Substitution Cipher_, (2) _Perturbed Decomposition_, (3) _Unperturbed Decomposition_, (4) _Perturbed Composition_, (5) _Unperturbed Composition_, (6) _Text-Only Format_.

* _Benign Control Dataset:_ This dataset includes 90 unique benign instructions. The primary goal is to evaluate whether models become overly defensive and reject benign content that has been obfuscated. These prompts are evaluated across the (1) _Color-Based Substitution Cipher_, and (2) _Perturbed Decomposition_.

In total, the multimodal datasets are evaluated on 3,300 harmful prompts and 180 benign prompts. For additional information on the construction of these datasets, see Appendix G and Appendix J.1.

### Models

We evaluated two sets of models across our multi-turn and multi-modal experiments. All models were tested with a temperature setting of \(1.0\), a top \(P\) value of \(1.0\) (where configurable), and with no external safety guardrails applied.2 It should be noted that a high temperature and top P encourage more diverse and varied responses, therefore the same prompt will likely lead to different model responses. For full version numbers, please see Appendix K.

### Evaluation Procedure

For our multi-turn evaluations, each prompt is evaluated in two different formats where only the prompting structure varies. Due to time and budget constraints, only prompts with Caesar-cipher output and no output-cipher were sampled for the testing phase. We refer to this subset as the _complete harmful dataset_, and release it with our other data.

Our main multimodal evaluations consist of three separate runs across the harmful multimodal dataset. We perform a single-run evaluation across the multimodal control dataset.

**Attack Success Rate (ASR)** is a commonly used metric in adversarial machine learning (Wu et al., 2021; Andriushchenko et al., 2024) to quantify the success of adversarial attacks on a model. Given an instruction set \(D\) consisting of malicious user queries, we define the ASR as:

\[=^{|D|}(S(Q_{i},R_{i})>0)}{|D|},\]

where \(Q_{i}\) represents the \(i\)-th malicious user query in \(D\), and \(R_{i}\) is the model's response to \(Q_{i}\).

We use the **StrongREJECT**3 evaluator for our evaluations (Souly et al., 2024). The StrongREJECT evaluator \(S()\) provides a continuous score from 0 to 1 that reflects the model's degree of engagement with the malicious intent of the query. It has been used in several model safety evaluations, including OpenAI's safety evaluation of GPT-4o and their recent o1 series models (OpenAI, 2024a).

\(S(R_{i})=0\) indicates the model either refused to respond or failed to decode the adversarial prompt, while \(S(R_{i})>0\) indicates unaligned behavior, with 1 reflecting a maximally specific and harmful response. We consider \(Q_{i}\) a successful attack if any of its runs results in success, accounting for stochastic variations across runs.

In addition to StrongREJECT, a small subset of 382 single-turn and multi-turn samples were hand-labelled to determine whether each model was successfully jailbroken and whether the model understood the question being asked of it. Classification criteria for this can be found in Appendix L. We found that the StrongREJECT labels aligned with our labels on 91 out of 100 test examples.

Appendix N details many case studies of successful attacks across the evaluated models, both _multi-turn_ and _multi-modal_ in design.

### Guardrails

To further evaluate the effectiveness of our attacks and the ability of the structural changes to bypass safety measures, we evaluate them against LLMs with augmented security guardrails attached. For multi-turn attacks, we test NeMoGuardrails (Rebedea et al., 2023) and our own in-house LLM Judge system, both with and without conversational awareness and with GPT-3.5 and GPT-4 as backbones. For multi-modal attacks we use our LLM Judge system with GPT-4o as the backbone, allowing for image input. These guardrails are largely prompt-based LLMs instructed to filter harmful interactions. Additional details about them and their implementations can be found in Appendix Q.

## 4 Experiments

Prompt Structure AsymmetryOn average, single-turn uni-modal attacks achieved an ASR of \(21.0\%\), while multi-turn uni-modal attacks a slightly higher ASR of \(24.1\%\). Multi-image multi-modal attacks showed a larger margin with an average ASR of \(28.4\%\), compared to single-image multi-modal attack's ASR of \(16.4\%\). Therefore, we already see some differences between structures, in the direction of vulnerability to more complex ones. To assess more deeply whether there is a qualitative difference between these attacks, we examine what percentage of attacks are successful using one entity compared to multiple. We refer to this as _prompting structure asymmetry_.

Figure 1 shows that there are a significant number of attacks that successfully jailbreak the models in one turn but fail to do so in multiple turns, and vice versa. For example, \(41.7\%\) of successful attacks on Claude-3-Opus were attacks that succeeded in multi-turn format but failed in single-turn format; only \(8.3\%\) of successful attacks succeeded in single-turn format but failed in multi-turn format.

Meanwhile, for multi-modal LLMs, Figure 2 suggests that models are more susceptible to multi-image adversarial attacks than single-image attacks, and especially compared to text-only ones. The one exception here is Gemini Flash, which held an mean ASR of \(50\%\) against single-image attacks and \(42.3\%\) against multi-image attacks (Table 4). Regardless, the evaluated multi-modal models demonstrated a significant prompt structure asymmetry.

Overall, we see a substantial difference between structures. This indicates both that there are novel vulnerabilities here, and that defending against one of these structures is no guarantee of defending against the rest.

Comparison of Multi-modal Attack MethodsThe CBSC was the strongest multi-modal adversarial attack on average, achieving the highest mean ASR of \(36\%\). This was followed by Perturbed Decomposition with an ASR of \(24.8\%\), Unperturbed Decomposition with an ASR of \(24.5\%\), Unperturbed Composition with an ASR of \(20.2\%\), and ending with Perturbed Composition as the weakest method with an ASR of \(12.7\%\) (Table 3). This breakdown further supports that multi-modal LLMs are likely more susceptible to multi-image compositional adversarial attacks, particularly within typographic settings (Figure 5).

Figure 1: **Prompting Structure Asymmetry.** The percentage of successful attacks that only jailbroke the model in one prompting structure, but failed in the other. Models are ordered by Elo rating (Chiang et al., 2024). The structural asymmetry is substantial.

Moreover, we explore asymmetries in model vulnerabilities against semantic domains as well, including violence, misinformation, and hate speech (Figure 4). We found that prompts related to privacy, malicious activities, and misinformation were the most frequently successfully jailbroken, with mean ASRs of \(37.3\%\), \(35\%\), and \(34\%\) respectively. For more details, refer to Appendix B.

Comparison of Textual Attack MethodsWe see that the success rates for the two forms of input-cipher were fairly similar across models (see Table 2), with random word replacement appearing at least as successful as perplexity-filtered mappings across the board--and sometimes marginally better. The success rates for both rise when considering only observations in which the model understood the question, though similar trends can still be observed in relation to the efficacy of each type.

On the other hand, we find that when the model understood the question, prompts specifying the use of a Caesar output-cipher were overwhelmingly more successful--almost double--than prompts that did not require an output-cipher (Table 2), for both single and multi-turn attacks. However, there was no notable difference between non-ciphered outputs and Caesar-ciphered outputs when model understanding was not factored in.

This is likely due to the complex nature of Caesar-ciphers and the models generally struggling to perform this task. We suggest that as LLM capabilities increase, model comprehension of ciphered requests will increase in kind - which given the UTQ jailbreaking rates, could pose an issue for safety. This also provides another illustration of an emerging structural attack paradigm. The form of the output is irrelevant to the semantics of the harmful instructions, yet the model's representations seem to rely on it to such an extent that safety measures fail to kick in properly.

GuardrailsWe tested the multi-turn guardrails on harmful, benign, and semi-benign (potentially toxic words but in benign instructional context) data. NeMo Guardrails with GPT-4 blocks all harmful multi-turn inputs, but it also produces over 90% false positives. This suggests it is specifically blocking the structure and not assessing if the semantics are harmful. Indeed, we find it fails against approximately 50% of harmful single-turn prompts. This points to both limitations of the system and

    &  &  &  \\   & & Perturbed & Unperturbed & Cipher & Perturbed & Unperturbed & Cipher \\   Claude 3 Hanku & 0.01\(\)0.11 & 0.17\(\)0.38 & 0.10\(\)0.29 & 0.06\(\)0.24 & 0.25\(\)0.43 & 0.27\(\)0.45 & **0.28\(\)0.45** \\ Claude 3.5 Sonnet & 0.01\(\)0.12 & 0.01\(\)0.11 & 0.00\(\)0.00 & 0.09\(\)0.28 & **0.25\(\)0.43 & **0.25\(\)0.43** & 0.01\(\)0.10 \\ GPT-4o & 0.10\(\)0.31 & 0.04\(\)0.20 & 0.13\(\)0.34 & 0.24\(\)0.43 & 0.38\(\)0.49 & 0.37\(\)0.48 & **0.50\(\)0.50** \\ GPT-4o Mini & 0.09\(\)0.29 & 0.00\(\)0.04 & 0.03\(\)0.17 & 0.21\(\)0.41 & 0.04\(\)0.20 & 0.02\(\)0.15 & **0.46\(\)0.50** \\ Gemini 1.5 Flash & 0.03\(\)0.18 & 0.28\(\)0.45 & **0.72\(\)**0.45 & 0.42\(\)0.49 & 0.30\(\)0.46 & 0.30\(\)0.6 & 0.67\(\)0.47 \\ Gemini 1.5 Pro & 0.02\(\)0.14 & 0.26\(\)0.44 & 0.23\(\)0.42 & 0.11\(\)0.31 & **0.27\(\)**0.45 & 0.26\(\)0.44 & 0.24\(\)0.43 \\   

Table 1: **Comparison of Mean ASR across Models and Multimodal Attack Methods. The results are presented for baseline (text), single image, and multi-image attack methods. Boldface highlights the method that achieved the highest mean ASR against a model. Mean \(\) standard deviation.**

Figure 2: **Image Structure Asymmetry. The percentage of successful attacks for each multimodal model that succeeded in one structure, but failed in the others. For full breakdown, please see Table 4.**

another setting where differences in structure are surprisingly significant. Meanwhile, our in-house Judge also blocks all harmful multi-turn attacks, with far fewer false positives (2.1% vs. 96.1% on completely benign data). It still exhibits some false positives on semi-benign data (35.7%), and while it has better rates of blocking single-turn attacks than NeMo (14.1% vs. 51.8%), the ASR is still well above 0. Thus, our system can provide a partial defense that improves in this setting on NeMo, but there are still limitations and variations depending on structure. For further details, see Appendix S. 72% of the multi-modal dataset prompts were correctly flagged as harmful by our LLM Judge. However, we have yet to test our LLM Judge against benign version of the multi-modal dataset.

Assessing Overgeneralized Safety ResponsesGood defenses will block harmful behavior but also limit false positives, in which the model refuses to answer benign questions. To assess whether overzealous defenses could be giving an impractical sense of security, we test the models on our benign multi-modal dataset. We found that Anthropic models, often known for being particularly stringent on safety, refused many benign prompts. This was especially true for Sonnet 3.5, suggesting its strong defense may be more due to training to broadly refuse prompts of a certain structure, rather than actually assessing which inputs are harmful and which aren't. We find that if false positives and negatives are weighed equally, then Gemini is the best-performing model. On the other hand, in scenarios where false negatives could result in severe harm, the more safety-first Anthropic models may be preferred. These results are discussed in more detail in Appendix T.

## 5 Conclusion

We have shown that frontier models are vulnerable to variations in prompting structure and modality, even when controlling for semantic meaning of the inputs. These new attacks present distinct threats, especially as capabilities like context window sizes and accepted modalities increase. But they are also part of an overall picture of structural vulnerabilities. Despite equivalent semantics, the safety systems are failing to generalize structurally.

This is currently a grim conclusion. But if we could solve this generalization problem, then although there could still be many other attacks out there that leverage entirely different instructions, we could nonetheless immediately eliminate a great many threats. And it might also point, or at least make it a smaller leap, to a more universal alignment solution. This perspective also facilitates the creation of new datasets, like ours, with well-controlled and systematic setups to assess structural vulnerabilities. Linked data, where the challenge is to make a defense on one structure in each set generalize to the other(s), provides a much more concretely-scoped problem than defending against arbitrary attacks. Thus, we propose that this is a tractable perspective that can inform and provide a target for future research.