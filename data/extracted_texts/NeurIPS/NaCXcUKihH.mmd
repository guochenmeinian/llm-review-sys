# Towards a theory of how the structure of language is acquired by deep neural networks

Francesco Cagnetta

Institute of Physics

Ecole Polytechnique Federale de Lausanne

francesco.cagnetta@epfl.ch

&Matthieu Wyart

Institute of Physics

Ecole Polytechnique Federale de Lausanne

matthieu.wyart@epfl.ch

###### Abstract

How much data is required to learn the structure of a language via next-token prediction? We study this question for synthetic datasets generated via a Probabilistic Context-Free Grammar (PCFG)--a tree-like generative model that captures many of the hierarchical structures found in natural languages. We determine token-token correlations analytically in our model and show that they can be used to build a representation of the grammar's hidden variables, the longer the range the deeper the variable. In addition, a finite training set limits the resolution of correlations to an effective range, whose size grows with that of the training set. As a result, a Language Model trained with increasingly many examples can build a deeper representation of the grammar's structure, thus reaching good performance despite the high dimensionality of the problem. We conjecture that the relationship between training set size and effective range of correlations holds beyond our synthetic datasets. In particular, our conjecture predicts how the scaling law for the test loss behaviour with training set size depends on the length of the context window, which we confirm empirically in Shakespeare's plays and Wikipedia articles.

## 1 Introduction

Two central foci of linguistics are the language structure and how humans acquire it. Formal language theory, for instance, describes languages with hierarchical generative models of grammar, classified in different levels of complexity . In this context, the 'poverty of the stimulus' argument --stating that the data children receive is insufficient to uniquely determine the grammatical structure of their language--led to the hypothesis that linguistic faculties are largely innate. By contrast, statistical learning theory  posits that the statistics of the input data can be used to deduce the language structure. This assumption is supported by empirical evidence concerning a broad range of tasks, including word segmentation  and reconstruction of the hierarchical phrase structure .

Large Language Models (LLMs) offer an interesting perspective on the subject. For instance, the success of LLMs trained for next-token prediction  establishes that a language can be acquired from examples alone--albeit with a training set much larger than what humans are exposed to. Furthermore, empirical studies of LLMs' representations showed that they learn a hierarchy of contextual information, including notions of linguistics such as word classes and syntactic structure . Recent studies have begun revealing the inner workings of LLMs by using synthetic data generated via context-free grammars , determining, in particular, the algorithm that these models follow when predicting the next token. However, there is no consensus on the mechanisms behind language _acquisition_ by LLMs . As a result, empirical phenomena such as the _scaling_ of the test loss with dataset size and number of parameters  and the _emergence_ of specific skills at certain scales  remain unexplained. In this work, we use hierarchical generative models of data to describe how the structure of a language is learnt as the training set grows.

### Our contributions

We consider synthetic datasets generated via the Random Hierarchy Model (RHM) , an ensemble of probabilistic context-free grammars (PCFGs). The RHM generates sequences of tokens by applying randomly chosen production rules to a hierarchy of hidden variables that live on the nodes of a tree with fixed geometry.

* We characterise analytically the power-law decay of the correlations between tokens with their distance. We then show that, because of this decay, a finite training set size \(P\) limits the resolution of correlations to an effective context window, whose size \(t^{*}\) increases with \(P\).
* Building on previous works on classification, we argue that deep learning models trained on next-token prediction can use measurable correlations to represent the hidden variables of the PCFG, with larger \(P\) allowing the representation of deeper hidden variables.
* Combining these results, we predict a sequence of sample complexities where the emergence of a deeper data structure representation leads to a jump in test loss. We empirically validate this for deep transformers and CNNs. Notably, the sample complexities are polynomial in the effective context size \(t^{*}\), avoiding the curse of dimensionality.
* We conjecture that the relationship between training set size, correlations and effective context window holds beyond our data model, and we test it by training deep transformers on collections of Shakespeare's lines and Wikipedia articles. In particular, we find that the test loss decay levels off at a characteristic training set size that depends on the length of the context window and can be measured from token-token correlations.

### Additional related works

Fixed-tree hierarchical generative models have been introduced to study phylogeny , then used in supervised learning [22; 23; 20; 24] and score-based diffusion [25; 26]. In particular,  introduced a sequential clustering algorithm that reveals the importance of correlations between the input features and the labels for supervised learning. The RHM of  provides a framework to show how features-label correlations emerge from the generative model and can be used by deep networks to represent the hidden hierarchical structure of the data. Here we extend this result to self-supervised learning, where the relevant correlations are those between the different input features.

PCFGs can in principle generate sequences with token correlations that decay as a power of their distance . When the production rule probabilities are random [28; 29], these probabilities must follow a broad distribution for the data to retain information about the generative process. Learning a PCFG from examples is a longstanding problem of theoretical linguistics . While some PCFG classes are learnable using distributional information , the sample complexity is unknown. In the context of deep learning, PCFGs have been used to study how trained transformers encode the grammar's structure [13; 14]. , in particular, showed that the operations performed by BERT-like transformers resemble well-known algorithms for grammatical inference, and proved that, for PCFG data, these algorithms are optimal solutions of the masked language modelling objective. However, when the training data is compatible with both a PCFG and a non-hierarchical generative model, neither recurrent language models  nor transformer  consistently prefer the hierarchical explanation. In addition, none of these works study the learning process.

Empirical work on the learning dynamics of Long Short-Term Memories showed that short-range dependencies are learnt first, then used as a foundation for forming longer-range dependencies . Our work introduces a theoretical framework to explain this hierarchical inductive bias, focusing on the learning curves of deep learning architectures. Shortly after our submission,  unveiled another form of hierarchical inductive bias in the training dynamics of transformers, whereby many-body interactions among tokens are learnt in the order of the interaction's degree.

## 2 Notation and setup

This work focuses on the pretraining phase of language models, aimed at building an approximation of the data distribution via unlabelled examples [8; 9]. Let us define a text datum, or sentence, as a sequence \(\!=\!(x_{1},,x_{d})\) of \(d\) tokens belonging to a finite vocabulary \(\). Denoting with \(v\) thevocabulary size, each token \(x_{i}\) is represented as a \(v\)-dimensional one-hot vector \((x_{i,})_{=1,,v}\)1:

\[x_{i,}=\{1,&$-th element of $$},\\ 0,&..\] (1)

A dataset, or _corpus_, consists of a probability distribution over sequences, which measures the frequency at which a given combination of tokens appears within the text. Assuming that all sequences have length \(d\), the data distribution is a joint probability over \(d\)-dimensional sequences with elements in \(\), \(P_{}():=\{X_{1}=x_{1},,X_{d}=x_{d}\}.\) The specifics of the approximation of \(P_{}\) depend on the training objective. In Masked Language Modelling, for instance, a random fraction of tokens is masked, i.e. replaced with a fixed token \(x_{}\), and the model is tasked with predicting their value . Autoregressive language models, instead, are trained to predict the \(i\)-th token of a sequence based on all the previous ones . Here we consider a simplified setup where the last token of the sequence is masked and the model is trained to predict it. In other words, the model takes the _context window_\((x_{1},,x_{d-1})\) as input and outputs a parametric approximation \(p_{}\) of the conditional probability of the last token,

\[p_{}(x_{d}|x_{1},,x_{d-1})\{X_{d}=x_{d}|X_{1 }=x_{1},,X_{d-1}=x_{d-1}\},\] (2)

obtained by updating the parameters \(\) via gradient descent on the empirical cross-entropy,

\[(_{P})=-_{_{P}} (p_{}(x_{d}|x_{1},,x_{d-1})),\] (3)

where \(_{P}\) is a set of \(P\) training examples drawn from \(P_{}\). Numerical experiments are performed in PyTorch , with the code available at https://github.com/fracagnetta/random-hierarchy-model. Details of the machine learning models, training hyperparameters and computer resources are presented in App. A.

### Hierarchical generative models

To model the hierarchical structure of sentences, we consider synthetic datasets generated via a probabilistic context-free grammar (PCFG) . PCFGs are collections of symbols and rules that prescribe how to generate sequences. In particular, the PCFGs we consider consist of

* \(L\) finite vocabularies of hidden (nonterminal) symbols \((_{})_{=1,,L}\);
* A finite vocabulary of observable (terminal) symbols \(\!\!_{0}\);
* \(L\) sets of _production rules_ describing how one symbols of \(_{}\) generates a tuple of symbols of \(_{-1}\), for \(\!=\!1,,L\).

Production rules take the form

\[^{()}_{1}^{(-1)},,_{s_{}}^{(-1)}, {for $^{()}_{},_{i}^{(-1)}_{-1}$},\] (4)

for some integer size \(s_{}\!\!1\). The left panel of Fig. 1 shows an example of the generative process, represented as a tree: pick (uniformly at random) a level-\(3\) symbol (root) and one of the production rule having that symbol on the left-hand side (also uniformly at random), replace the symbol with the right-hand side of the production rules (first generation), then repeat the process until left with only terminal symbols (leaves). The resulting datum is a sequence in \((_{0})^{d}\), with \(d\!=\!_{}s_{}\). Assuming a finite number of production rules emanating from each nonterminal symbol, this model generates a finite number of \(d\)-dimensional sequences. Since the probabilities of the level-\(L\) symbol and the production rules are uniform, the data distribution \(P_{}\) is uniform over the generated sequences.

The Random Hierarchy Model (RHM) of  is an ensemble of such generative models, obtained by prescribing a probability distribution over production rules. In particular, the \(\)-th set of production rules is chosen uniformly at random between all the _unambiguous_ sets of rules in the form of Eq. 4. Unambiguity means that each \(s_{}\)-tuple of level-\((-1)\) symbols can be generated by one level-\(\) symbol at most. The uniform probability and unambiguity assumptions are not satisfied in a generic natural language, but they allow us to characterise quantitatively the effects of the hierarchical structure. We will further assume, to ease notation, that all the vocabularies \(_{}\) have the same size \(v\) and that the size of the production rules is homogeneous, i.e. \(s_{}\!=\!s\) for all \(\). We further assume that each nonterminal appears as the left-hand side of exactly \(m\) production rules, i.e. the hidden symbols have \(m\) equivalent low-level representations. Since there are \(v^{s}\) distinct low-level representations and each of the \(v\) high-level symbols is assigned \(m\), unambiguity requires \(m\!\!v^{s-1}\).

## 3 Correlations, training set size and effective context window

Given a dataset of \(d\)-dimensional sequences of tokens in \(\), we measure correlations via the token co-occurrences matrix, 2

\[C_{i,j}(,)\{X_{i}=,X_{j}=\}-\{X_{i}=\}\{X_{j}=\},\] (5)

where \(\) and \(\) are arbitrary elements of the vocabulary \(\) and \(\) refers to the data distribution \(P_{}\). Since the masked token is always the last in our setup, it is convenient to set \(j=d\) and write \(C_{i,d}\) as a function of the distance \(t\!=\!|i-d|\) between the \(i\)-th and the masked token. Taking the root mean square over the vocabulary yields the _correlation function_,

\[(t)(v^{-2}_{,}(C_{d-t,d} (,))^{2})^{1/2},\] (6)

which measures the typical dependency between tokens as a function of their distance \(t\). For RHM data with \(m\!=\!v^{s-1}\), \(P_{}\) is uniform over all the possible sequences of tokens in \(\) and there are no correlations. If, instead, \(m\!<\!v^{s-1}\), the correlations strength depends on the distance. Fig. 1 shows an example for RHM data with \(L\!=\!4\), \(s\!=\!2\), \(v\!=\!32\) and \(m\!=\!8\).

Correlations decay with distance.The stepwise decay of \((t)\) mirrors the tree structure of the generative model. The masked token has the highest correlations with those belonging to the same \(s\)-tuple, as they were all generated by the same level-\(1\) symbol (as in the blue box of Fig. 1, left). The second highest is with the tokens generated by the same level-\(2\) symbol (orange box in the figure), and so on until the root. Formally, with \(\!=\!1,,L\) denoting the height of the lowest common ancestor (LCA) of the \(d\)-th and \((d-t)\)-th tokens,

\[(t)\!=\!^{()} t\!=\!s^{-1},, s^{}-1;^{(1)}>^{(2)}>>^{(L)}.\] (7)

These \(L\) plateau values can be determined analytically in the large \(v\) limit by approximating the variance over the vocabulary entries \(\) and \(\) on the right-hand side of Eq. 7 with the variance over realisations of the RHM. Denoting the average over such realisations with \(.\),

\[^{()}=((C^{()}(,))^{2} )^{1/2})}{v^{3}m^{2-1}}},\] (8)

Figure 1: **Left:** Example of data generation according to the RHM, with depth \(L\!=\!3\) and branching factor \(s\!=\!2\). Starting from the root with \(\!=\!3\) and following the arrows, each level-\(\) symbol is replaced with a pair of lower-level symbols, down to the leaves with \(\!=\!0\). **Right:** Empirical (coloured) and analytical (black dashed) correlation functions of RHM data, with \(L\!=\!3\), \(s\!=\!2\), \(v\!=\!32\) and \(m\!=\!8\). The stepwise decay mirrors the tree structure of the generative model. Empirical estimates obtained from \(P\) examples initially follow the true correlation function, but then saturate due to the sampling noise (coloured dashed). As a result, a finite training set only allows for measuring correlations with the tokens up to a certain distance \(t^{*}(P)\). Graphically, \(t^{*}(P)\) corresponds to the highest value of \(t\) where the empirical estimate matches the true correlation (e.g. 1 for the orange and green curves, \(3\) for the red curve).

where the rightmost equality is exact asymptotically in \(v\) and \(m\). Eq. 8 is derived in detail in App. D, confirmed empirically in the right panel of Fig. 1 and can be given a simple interpretation in terms of the sample size required for the empirical measurement of correlations, as discussed in the following paragraph. In addition, notice that, upon replacing \(s^{}\) with \(t\), the \(m^{-}\) dependence on \(\) is approximated by a power-law decay \((t) t^{-}\), with \(= m/ s\).

Saturation due to finite training set.When measuring the correlation function from a finite sample \(_{P}\) of \(P\) data, there is an additional contribution due to the sampling noise. The scenario is illustrated in Fig. 1, right: the empirical estimates \(_{P}(t)\), shown as coloured lines for different values of \(P\), begin by following the descent of the true correlation function \((t)\), shown as a black dashed line. However, empirical estimates saturate when approaching the sampling noise size \((v^{2}P)^{-1/2}\), as proved in App. E and shown as dashed coloured lines in Fig. 1, right. Combining the saturation with the values of the steps, we deduce that a finite training set allows for the resolution of correlations up to distance \(t^{*}=s^{^{*}}-1\) such that

\[^{(^{*})}>(v^{2}P)^{-1/2}>^{(^{*}+1)}.\] (9)

Eq. 9 suggests that a language model trained with \(P\) examples can only extract information from the tokens within distance \(t^{*}(P)\) from the last. In other words, a finite training set is equivalent to an _effective context window_ of size \(t^{*}(P)\). If \( t^{-}\), then \(t^{*}(P) P^{1/2}\). Alternatively, setting \(^{()}=(v^{2}P)^{-1/2}\) yields a sequence of thresholds \(P_{}\) for the resolution of correlations of increasing range. From Eq. 8, \(P_{} vm^{2-1}\), which has a simple interpretation as the number of choices in the generative process to determine two tokens at a distance \(t[s^{-1},s^{})\): \(v\) choices for the level-\(\) LCA, \(m\) for the first production rule and \(m^{2}\) (\(m\) per branch) for each of the remaining \(-1\) generations.

## 4 Self-supervised learning of the Random Hierarchy Model

We now show how the correlations can be translated into a prediction of sample complexities that allow for a sequence of increasingly accurate approximations of the masked token probability, based on reconstructing the hidden variables of the generative tree. We then test these predictions in numerical experiments with deep networks.

### Prediction of the sequence of performance steps and sample complexities

Loss steps.Due to the structure of the data, there is a natural sequence of \(L\) increasingly accurate approximations of the last token probability in Eq. 2. For all \(=1,,L\), these approximations are

Figure 2: **Left:** Learning curves of depth-\(3\) transformers trained on RHM data with \(L=3\), \(s=2\), \(v=32\) and \(m=8\) (blue) or \(11\) (orange, both are averaged over \(8\) independent realisations of the dataset and initialisations of the network), displaying a stepwise behaviour analogous to the correlation function. The vertical dashed lines mark the characteristic training set sizes \(P_{k}\) at which the correlation with tokens at distances up to \(t=s^{k}-1\) emerge from the sampling noise. Horizontal dashed lines represent (upper bounds on) the cross-entropy of the probability of the last token conditioned on the previous \(s^{k}-1\), suggesting that the steps correspond to the model learning a progressively larger sub-tree of the data structure. **Right:** Learning curves of transformers for \(m=8\) and different sizes \(t\) of the context window. The saturation of the loss decay due to the finite context window highlights that the decay is entirely due to the ability to leverage a larger portion of the context window.

realised by conditioning the probability of the last token on the previous \(s^{}-1\). These approximations amount to using an effective context window of size \(t_{}\!=\!s^{}\!-1\). The effective context windows consist of the leaves of the subtree generated by the level-\(\) hidden symbol above the last token, as illustrated by the coloured boxes of Fig. 1, left. The resulting cross-entropy loss is given by

\[_{} =_{ P_{}}[- \{X_{d}|X_{d-s^{}+1}=x_{d-s+1},,X_{d-1}=x_{d-1}\}]\] \[=_{ P_{}}[ N(x_{d-s^{ }+1},,x_{d-1})],\] (10)

where \(N(x_{d-s^{}+1},,x_{d-1})\) denotes the number of possible values of the masked token depending on the effective context window. For \(\!=\!0\), there is no restriction on the masked token value and this number equals \(v\)--the vocabulary size. For \(\!=\!1\), we can determine the average \(_{1}[N(x_{d-s+1},,x_{d-1})]\) as follows. For each \(s\)-tuple \((x_{d-s+1},,x_{d})\) there is at least one value of the mask compatible with the other \(s-1\) symbols, i.e. \(x_{d}\) itself. In addition, each of the remaining \(v-1\) values \(_{d} x_{d}\) has a probability \(f\) of being compatible with the context, coinciding with the probability that the \(s\)-tuple \((x_{d-s+1},,_{d})\) is compatible with the production rules. This probability is given by \((mv-1)\), i.e. the number of \(s\)-tuples compatible with the production rules except \((x_{d-s+1},,x_{d})\), over \((v^{s}-1)\), i.e. the total number of \(s\)-tuples except \((x_{d-s+1},,x_{d})\). Therefore, \(_{1}=1+(v-1)f=1+(v-1)(mv-1)/(v^{s}-1)\). For \(\!>\!1\), the average number \(_{}\) of symbols compatible with the context can be determined iteratively. The level-\(\) symbol generating the whole \(s^{}\)-tuple can take any of the \(v\) values, but the level-\((-1)\) symbol below it is now restricted to \(_{1}\) values. By the previous argument, \(_{}=1+(v-1)(m_{-1}-1)/(v^{s}-1)\). Due to the concavity of the logarithm, we can bound the test loss of Eq. 10 with \(}_{}=_{}\), i.e., after solving the recurrence relation and introducing the fraction of compatible \(s\)-tuples \(f\!=\!m/v^{s-1}\).

\[}_{} =-v}{v^{s}-1-m(v-1)}+-mv)(v-1) }{v^{s}-1-m(v-1)}(-1})^{}\] \[[v,m 1]{v,m 1}+vf^{} ,\] (11)

Naive strategy.The simplest strategy to estimate \(\{X_{d}|X_{d-s^{}+1}=x_{d-s^{}+1},,X_{d-1}=x_{d-1}\}\) is to count the empirical occurences of the \(s^{}\)-dimensional subsequences of the input in the training set--the so-called \(n\)-gram language model with \(n\!=\!s^{}\). This estimation requires the training set to contain all the distinct subsequences of size \(s^{}\). Following the generative process, each of these subsequences occurs with probability given by that of the corresponding LCA symbol (the root of the subtree encased by the corresponding coloured box in Fig. 1, left), times the probability of the production rules that generate the subsequence from the LCA, \(m^{-1} m^{-s} m^{-(s^{}-1)}\!=\!m^{-(s^{}-1)/(s- 1)}\). The latter is exponentially small in the effective context length \(t_{}\!=\!s^{}-1\), hence the required sample size is exponentially large in \(t_{}\).

Efficient strategy leveraging the hidden variables.Using the hidden variables results in a much lower sample complexity. Indeed, due to the tree structure of PCFGs, the value of the last token is conditionally independent of most of the observable tokens when the hidden variables are given. For instance, looking at the tree in Fig. 1, left, the probability of the last token is independent of the pair \((_{5}^{(0)},_{6}^{(0)})\) if the parent level-\(1\) variable \(_{3}^{(1)}\) is given. In general, fixing a hidden symbol splits the tree into an _inside_ (the subtree rooted at the hidden symbol) and an _outside_ (the rest of the tree) that are conditionally independent. As a result, the minimal set of variables that the \(s^{}\)-gram probability depends on consist of \(s-1\) observable tokens (those in the same patch as the last token) and \((s-1)(-1)\) hidden variables (\((s-1)\) for each level below the LCA of the context window). The probability of any such set of variables is given by the LCA probability times \(m^{-}\). The resulting sample complexity grows exponentially with \(\), or as a power of the effective context length \(t_{}\).

Reconstruction of the hidden variables.We now argue that, as shown  in the context of classification, the hidden variables can be represented via the correlations between tokens. Consider, for instance, the pair \((_{5}^{(0)},_{6}^{(0)})\) in Fig. 1, left. Because of the aforementioned conditional independence, the correlation between any such pair and the last token depends only on the level-\(1\) hidden variable \(_{3}^{(1)}\). Thus, pairs displaying the same correlations can be grouped as descendants of the same hidden variable. This strategy requires enough training data to resolve correlations between the masked token and the adjacent \(s\)-tuples of observable tokens. As shown in App. F, replacing an observable token with a whole \(s\)-tuple reduces correlation plateaus and sampling noise by the same factor. Therefore, the condition for the resolution of correlations with the nearest \(s\)-tuples is given by Eq. 9 with \(\!=\!2\), implying \(P\!>\!P_{2}\!=\!vm^{3}\). By iterating this argument we get a sequence of sample complexities \(P_{}\) that allow for resolving correlations between the masked token and \(s\)-tuples up to distance \(t\!=\!s^{}-1\),

\[P_{}=(v^{2}^{()})^{-1}=vm^{2-1}(1-} )^{-1}.\] (12)

For instance, in the case illustrated in Fig. 1, left, the correlations of the pairs \((_{1}^{(0)},_{2}^{(0)})\) and \((_{3}^{(0)},_{4}^{(0)})\) with the masked token can be used to reconstruct the pair of hidden symbols \((_{1}^{(1)},_{2}^{(1)})\). The hidden symbols have a higher correlation with the masked token than their children. Hence, as in the case of classification , a training set large enough to resolve correlations between observable and masked tokens also allows for resolving correlations of the masked token with the hidden symbols. These correlations yield a representation of higher-level hidden symbols (e.g. \(_{1}^{(2)}\) for \((_{1}^{(1)},_{2}^{(1)})\) in the figure), which, in turn, enables the reconstruction of \(\{X_{d}|X_{d-s^{}+1}=x_{d-s+1},,X_{d-1}=x_{d-1}\}\) via the efficient strategy. As \(\) increases, the sample complexity of Eq. 12 grow faster than \(m^{}\), but still polynomially in the effective context length \(t_{}\).

Scaling law of the RHM.After solving Eq. 12 for \(\) as a function of \(P\), we can use Eq. 11 to derive the _scaling law_ for the behaviour of the loss steps as a function of the training set size \(P\). Neglecting all the factors that do not depend on \(\), Eq. 12 implies \( P/(2 m)\). Thus, from Eq. 11,

\[}(P)+})}.\] (13)

Notice that \(f\!<\!1\), thus \( f\!<\!0\). Therefore, Eq. 11 implies an early logarithmic decay as long as \(| f| P 2 m\). For larger \(P\), the expansion \( x\) recovers the ubiquitous power-law decay \(P^{-}\), with exponent \(\!= f/(2 m)\). Notice that the power-law scaling is caused by the sequence of steps associated with the emergence of the hidden variables representation. Therefore, this picture unifies the emergence and scaling paradigms.

### Comparison with empirical learning curves

Fig. 2, left, compares the learning curves of deep transformers (details of the architectures in subsection A.2) with the sample complexities \(P_{}\) of Eq. 12 (vertical dashed lines in the figure) and the test loss upper bounds \(}_{}\) of Eq. 11 (horizontal dashed lines), showing good qualitative agreement. Additional experiments that support the quantitative scaling of the sample complexities \(P_{1}\) and \(P_{2}\) with \(m\) are shown in App. G. Fig. 2, right, shows the learning curves of models trained on a reduced context window. In this setting, our description correctly predicts the saturation of the loss due to the finite context window size \(t\): with \(t\!=\!s^{}-1\), the model can only learn the level-\(\) hidden variable above the masked token, thus follow only the first \(\) of the \(L\) steps of Eq. 11.

Let us remark that, as shown in App. G, the learning curves are qualitatively similar for CNNs, despite a noticeable quantitative dependence on architecture and context size \(t\). These differences are not captured by the analysis of subsection 4.1, although, in some cases, they can be rationalised using results from the theory of shallow neural networks. We discuss these aspects in detail in App. G.

### Emergence of hierarchical representations of the data structure

We now study the hidden representations of models trained on RHM data to show that, as the training set size increases, these representations encode for deeper hidden variables. More specifically, we show that certain representations depend only on specific, high-level hidden variables of a datum's tree structure, thus becoming insensitive to the entire subtree emanating from this hidden variable. For the sake of interpretability, we consider deep convolutional networks (CNNs) with architecture matched to the data structure, represented schematically in the graphs on the right of Fig. 3 (further details in subsection A.1). To probe representations we introduce two sets of transformations. Given a datum and the associated tree ( Fig. 1, left), consider the \(i\)-th level-\(\) symbol \(_{i}^{()}\): \(_{,i}\) replaces it with another one randomly chosen from the vocabulary, whereas \(_{,i}\) resets the choice of the production rule emanating from \(_{i}^{()}\). Both transformations alter the subtree originating from \(_{i}^{()}\) (e.g. the subtree within the orange box of Fig. 2, left for \(\!=\!2\) and \(i\!=\!2\)), affecting \(s^{}\) observable tokens. However, \(_{,i}\) preserves the hidden symbols that generated the subtree. Therefore, a hidden representation that encodes only the \(i\)-th level-\(\) hidden symbol will be invariant to \(_{,i}\) but not to \(_{,i}\).

We define hidden representations \(h_{}(x)\) (hidden nodes of the network's graphs in Fig. 3) as the sequence of pre-activations in a given layer \(\) (depth of the node in the tree), standardised over the dataset (i.e. centred around the mean and scaled by the standard deviation). For CNNs, representations carry a spatial index \(j\!=\!1,,s^{L-}\) (horizontal position of the node within the layer) and a channel index. We measure the sensitivity to \(\) or \(\) via the cosine similarity between original and transformed representations, i.e.

\[r_{,i}(h)=_{x P_{}}[h_{^{},j}(x)  h_{^{},j}(_{,i}x)],s_{,i}(h)=_{x P_{}}[h_{^{},j}(x) h_{^{},j}( _{,i}x)],\] (14)

where the \(\) symbol denotes the scalar product over the channels. In order to leave the masked token unaltered, we always apply the transformations to the penultimate hidden symbol of the level, i.e. \(i\!=\!s^{L-}-1\). Hence, from now on, we omit the spatial index \(i\). The left column of Fig. 3 reports the ratio \(r_{}/s_{}\) for the hidden representations of a deep CNN trained on RHM data. Each row refers to the level of the data transformations. The group of observable tokens affected by the transformation is highlighted by horizontal square brackets in the right panels. The drop of \(r_{}/s_{}\) from \( 1\) to \( 0\) signals that a representation depends on the corresponding level-\(\) hidden variable, but not on the other variables in the associated subtree. 3 These drops occur at the same training set sizes \(P_{}\) as the

Figure 3: Relative sensitivity \(r_{}/s_{}\) of the representation of trained depth-4 CNNs (sketched on the right panels) for input transformations (the affected tokens are indicated by the black horizontal segments on the right panels) corresponding to resetting the production rule emanating from a given level-\(\) variable (\(=1,2,3\) for top, centre and bottom), as a function of training set size \(P\). Colours represent the layer of the representation, as indicated in the key and by the squares on the right panels. The CNNs are trained on RHM data with \(L\!=\!4\), \(s\!=\!2\), \(v\!=\!16\), \(m\!=\!4\). Vertical dashed lines mark the sample complexities \(P_{}\) of Eq. 12. The drop of the curves from \( 1\) to \( 0\) around \(P_{}\) signals that the trained representations only encode for the relevant level-\(\) symbol when \(P\!>\!P_{}\).

test loss steps, highlighted in the figures with vertical dashed lines. This result confirms that, as \(P\) increases, trained models learn a deeper representation of the tree structure of the data.

## 5 Conjecture and test on real language data

We conjecture that the relationship between training set size, correlations and effective context window holds beyond our synthetic dataset.

**Conjecture:**_"If the token correlation function decays with the token distance, then a language model trained to predict the next token from a training set of \(P\) examples can only extract relevant information from an effective context window of \(P\)-dependent size \(t^{*}(P)\)."_

We test this conjecture in two datasets: a selection of lines from Shakespeare's plays  and a collection of articles from English Wikipedia . For both datasets we adopt a character-level tokenisation, resulting in over \(10^{6}\) tokens. We then extract sequences of \(t\) consecutive tokens and train BERT-like deep transformers in the setup of section 2--further details of architecture and training are in subsection A.3. The results of our test are reported in Fig. 4 for Shakespeare and Fig. 5 of App. B for Wikipedia. First, with a large context window, the test loss follows the empirical scaling law \( P^{-}\) (top left panel). However, the learning curve levels off at some characteristic scale \(P\) that grows with the size \(t\) of the context window. This phenomenon can be explained via the correlation function, which decays as a power of the distance \((t) t^{-}\), with \( 1.4\)4 (top right panel). Empirical estimates \(_{P}(t)\) saturate when reaching the sampling noise scale \( P^{-1/2}\): following the analysis of section 3, this behaviour results in an effective context window size \(t^{*}(P)\), given by the

Figure 4: **Top, Left:** Test losses of \(3\)-layers transformers trained on \((t+1)\)-characters blocks of the tiny-Shakespeare dataset  (\(t\) as in the key). The saturation of the loss to some \(t\)-dependent value indicates that performance improves with \(P\) because the model can use information from a larger context window. **Top, Right:** Empirical estimates \(_{P}(t)\) for different training set sizes \(P\) as in the key. The curves initially follow the true correlation \((t)\) (black dashed), but then saturate due to the sampling noise (coloured dashed). **Bottom, Right:** The empirical curves \(_{P}(t)\) collapse when rescaling correlations by the sampling noise size \(P^{-1/2}\) and \(t\) by the characteristic distance \(t^{*}(P) P^{1/z}\), with \(z 2.8\). **Bottom, Left:** As predicted by our conjecture, the losses collapse when rescaled according to Eq. 16 with the same \(z\) as the correlation functions.

value of \(t\) where the correlation function \((t) t^{-}\) intersects the sampling noise scale \( P^{-1/2}\),

\[(t^{*})^{-} P^{-1/2} t^{*}(P) P^{1/z},z=2 2.8.\] (15)

As a result, the empirical correlation functions with varying \(P\) collapse when rescaling \(\) by the sampling noise and the distance by \(t^{*}(P)\) (bottom right panel).

By inverting \(t^{*}(P)\) we get a characteristic training set size \(P^{*}(t)\) where the training set allows for resolving correlations at all distances \(t^{}\!<\!t\), \(P^{*}(t) t^{z}\). Paired with the empirical power-law scaling with \(P\), this result leads to the following context-dependent scaling hypothesis for the test loss:

\[(P,t)=t^{- z}f(}),\] (16)

with \(f(x) x^{-}\) for \(x\!\!1\) and constant for \(x\!\!1\). In particular, Eq. 16 implies that the behaviour of the empirical correlation functions predicts the saturation of the loss decay. The collapse reported in the bottom left panels of Fig. 4 and Fig. 5 quantitatively confirms Eq. 16 and our previous conjecture.

## 6 Conclusions

We proposed a conceptual framework for understanding the performance-vs.-data scaling laws of language models trained for next-token prediction. In our picture, increasing the number of data allows for the resolution of a longer range of correlations. These correlations, in turn, can be exploited to build a hierarchical representation of the data structure, the longer the range the deeper the representation. For our synthetic hierarchical data, the emergence of deeper representation results in a series of steps in the next-token prediction performance. These steps conspire to determine the scaling law, whose exponent depends on the dataset structure. This scenario is consistent with the empirical phenomenology of language models, including both the emergence of skills at specific training set sizes [18; 43; 44; 45] and the steady improvement of overall performance . To the best of our knowledge, this is the first theoretical description of scaling laws in a setting where learning data features is crucial, whereas previous works focused on kernel limits [46; 47; 48; 49; 50].

Furthermore, our analysis predicts a fundamental relationship between the effective context window captured by a language model trained with a finite training set and the decay of token-token correlations, which we confirmed empirically on two examples of text data. This finding suggests that the exponents entering scaling laws are influenced by the intrinsic properties of the data. On the one hand, our predictions can be tested on state-of-the-art LLMs trained on larger datasets. On the other hand, our framework can be extended to shed light on other aspects of scaling laws of high practical relevance, such as the role of the number of parameters and the behaviour of performance when the model size and the number of data are optimised under a fixed compute budget.

**Limitations.** Our hierarchical model of data is limited by the context-free structure of the rules, which describes most, but not all, of the syntactic forms observed in natural languages . Understanding the role of context-sensitive structures in language acquisition is a promising avenue for future research. In addition, the RHM assumes a fixed geometry of the data tree and the uniform probability and unambiguity of the production rules. These assumptions are not satisfied by real text data and are responsible for the stepwise behaviour of correlations in our model. Relaxing these constraints while keeping the large-scale, power-law decay of correlations with the distance, which is indeed observed in real data, could broaden the scope of our conceptual framework. On the technical side, there is no proof of the connection between the strategy illustrated in subsection 4.1 and the sample complexity of deep neural networks trained with gradient descent and its variants. Such a proof would require a formal description of the dynamics of deep networks trained on hierarchical data, which is beyond the scope of the present paper. This description would also capture the discrepancies between different architectures presented in App. G, making it a valuable direction for future work.