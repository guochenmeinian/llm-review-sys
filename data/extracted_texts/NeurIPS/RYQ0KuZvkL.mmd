# Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning

Adhyyan Narang

University of Washington

adhyyan@uw.edu

&Andrew Wagenmaker

University of California, Berkeley

ajwagen@berkeley.edu

&Lillian J. Ratliff

University of Washington

ratliffl@uw.edu

&Kevin Jamieson

University of Washington

jamieson@cs.washington.edu

###### Abstract

In this paper, we study the non-asymptotic sample complexity for the pure exploration problem in contextual bandits and tabular reinforcement learning (RL): identifying an \(\)-optimal policy from a set of policies \(\) with high probability. Existing work in bandits has shown that it is possible to identify the best policy by estimating only the _difference_ between the behaviors of individual policies- which can be substantially cheaper than estimating the behavior of each policy directly --yet the best-known complexities in RL fail to take advantage of this, and instead estimate the behavior of each policy directly. Does it suffice to estimate only the differences in the behaviors of policies in RL? We answer this question positively for contextual bandits, but in the negative for tabular RL, showing a separation between contextual bandits and RL. However, inspired by this, we show that it _almost_ suffices to estimate only the differences in RL: if we can estimate the behavior of a _single_ reference policy, it suffices to only estimate how any other policy deviates from this reference policy. We develop an algorithm which instantiates this principle and obtains, to the best of our knowledge, the tightest known bound on the sample complexity of tabular RL.

## 1 Introduction

Online platforms, such as AirBnB, often try to improve their services by A/B testing different marketing strategies. Based on the inventory, their strategy could include emphasizing local listings versus tourist destinations, providing discounts for longer stays, or de-prioritizing homes that have low ratings. In order to choose the best strategy, the standard approach would be to apply each strategy sequentially and measure outcomes. However, recognize that the choice of strategy (policy) affects the future inventory (state) of the platform. This complex interaction between different strategies makes it difficult to estimate the impact of any strategy, if it were to be applied independently. To address this, we can model the platform as an Markov Decision Process (MDP) with an observed state [17; 15] and a finite set of policies \(\) corresponding to possible strategies. We wish to collect data by playing _exploratory_ actions which will enable us to estimate the true value of each policy \(\), and identify the best policy from \(\) as quickly as possible.

In addition to A/B testing, similar challenges arise in complex medical trials, learning robot policies to pack totes, and autonomous navigation in unfamiliar environments. All of these problems can be formally modeled as the PAC (Probably Approximately Correct) policy identification problem in reinforcement learning (RL). An algorithm is said to be \((,)\)-PAC if, given a set of policies \(\), it returns a policy \(\) that performs within \(\) of the optimal policy in \(\), with probability \(1-\). Thegoal is to satisfy this condition whilst minimizing the number of interactions with the environment (the _sample complexity_).

Traditionally, prior work has aimed to obtain _minimax_ or _worst-case_ guarantees for this problem--guarantees that hold across _all_ environments within a problem class. Such worst-case guarantees typically scale with the "size" of the environment, for example, scaling as \(((S,A,H)/^{2})\), for environments with \(S\) states, \(A\) actions, horizon \(H\). While guarantees of this form quantify which classes of problems are efficiently learnable, they fail to characterize the difficulty of particular problem instances--producing the same complexity on both "easy" and "hard" problems that share the same "size". This is not simply a failure of analysis--recent work has shown that algorithms that achieve the minimax-optimal rate could be very suboptimal on particular problem instances . Motivated by this, a variety of recent work has sought to obtain _instance-dependent_ complexity measures that capture the hardness of learning each particular problem instance. However, despite progress in this direction, the question of the _optimal_ instance-dependent complexity has remained elusive, even in tabular settings.

Towards achieving instance-optimality in RL, the key question is: _what aspects_ of a given environment must be learned, in order to choose a near-optimal policy? In the simpler bandit setting, this question has been settled by showing that it is sufficient to learn the _differences_ between values of actions rather than learning the value of each individual action: it is only important whether a given action's value is greater or lesser than that of other actions. This observation can yield significant improvements in sample efficiency . Precisely, the best-known complexity measures in the bandit setting scale as:

\[_{_{}}_{}-^{}\|_{ (_{})^{-1}}^{2}}{()^{2}},\] (1.1)

where \(^{}\) is the feature vector of action \(\), \(^{}\) the feature vector of the optimal action, \(()\) is the suboptimality of action \(\). Here, \((_{})\) are the covariates induced by \(_{}\), our distribution of exploratory actions. The denominator of this expression measures the performance gap between action \(\) and the optimal action. The numerator measures the variance of the estimated (from data collected by \(_{}\)) difference in values between \((,^{})\). The max over actions follows because to choose the best action, we have to rule out every sub-optimal action from the set of candidates \(\); the infimum optimizes over data collection strategies.

In contrast, in RL, instead of estimating the difference between policy values _directly_, the best known algorithms simply estimate the value of each individual policy _separately_ and then take the difference. This obtains instance-dependent complexities which scale as follows :

\[_{h=1}^{H}_{_{}}_{}^{}\|_ {_{h}(_{})^{-1}}^{2}+\|_{h}^{}\|_{_{h}( _{})^{-1}}^{2}}{()^{2}}\] (1.2)

where \(_{h}^{}\) is the state-action visitation of policy \(\) at step \(h\). Since now the difference is calculated _after_ estimation, the variance of the difference is the sum of the individual variances of the estimates of each policy, captured in the numerator of (1.2). Comparing the numerator of (1.2) to that of (1.1) begs the question: in RL can we estimate the _difference_ of policies directly to reduce the sample complexity of RL?

To motivate why this distinction is important, consider the tabular MDP example of Figure 1. In this example, the agent starts in state \(s_{1}\), takes one of three actions, and then transitions to one of states \(s_{2},s_{3},s_{4}\). Consider the policy set \(=\{_{1},_{2}\}\), where \(_{1}\) always plays action \(a_{1}\), and \(_{2}\) is identical, except plays actions \(a_{2}\) in the red states. If \(_{h}^{_{i}}_{S}\) denotes the state-action visitations of policy \(_{i}\) at time \(h=1,2\), then we see that \(_{1}^{_{1}}=_{1}^{_{2}}\) since \(_{1}\) and \(_{2}\) agree on the action in \(s_{1}\). But \(_{2}^{_{1}}_{2}^{_{2}}\) as their actions differ on the red states.

Since these red states will be reached with probability at most \(3\), the norm of the _difference_

\[\|_{2}^{}-_{2}^{}\|_{_{2}(_{})^{-1}}^{2} =_{s,a}^{}(s,a)-_{2}^{}(s,a))^{2}}{_{2}^{ _{}}(s,a)}\]

is significantly less than the sum of the individual norms

\[\|_{2}^{}\|_{_{2}(_{})^{-1}}^{2}+\|_{2}^{ }\|_{_{2}(_{})^{-1}}^{2}=_{s,a}^{}( s,a)^{2}+^{}(s,a)^{2}}{_{2}^{_{}}(s,a)}.\]Intuitively, to minimize differences \(_{}\) can explore just states \(s_{3},s_{4}\) where the policies differ, whereas minimizing the individual norms requires wasting lots of energy in state \(s_{2}\) where the two policies and the difference is zero. Formally:

**Proposition 1**.: _On the MDP and policy set \(\) from Figure 1, we have that_

\[_{_{}}_{}\|_{2}^{}\|_{_{2}(_{})^{ -1}}^{2} 1_{_{}}_{}\|_{2}^{ }-_{2}^{}\|_{_{2}(_{})^{-1}}^{2} 15^{2}.\]

Proposition 1 shows that indeed, the complexity of the form Equation (1.1) (generalized to RL) in terms of differences could be significantly tighter than Equation (1.2); in this case, it is a factor of \(^{2}\) better. But achieving a sample complexity that depends on the differences requires more than just a better analysis: it requires a new estimator and an algorithm to exploit it.

Contributions.In this work, we aim to understand whether such a complexity is achievable in RL. Letting \(_{}\) denote the generalization of (1.1) to the RL case--that is, (1.2) but with \(\|_{h}^{}\|_{_{h}(_{})^{-1}}^{2}\) replaced by \(\|_{h}^{}-_{h}^{}\|_{_{h}(_{})^{-1}}^{2}\), our contributions are as follows:

1. In the Tabular RL case,  recently showed that \(_{}\) is a lower bound on the sample complexity of RL by characterizing the difficulty of learning the unknown reward function; however, they did not resolve whether it is achievable when the state-transitions are unknown as well. We provide a lower bound which demonstrates that \((_{})\) is _not_ sufficient for learning with state transitions.
2. We provide an algorithm Perp, which first learns the behavior a particular reference policy \(\), and then estimates the difference in behavior between \(\) and every other policy \(\), rather than estimating the behavior of each \(\) directly.
3. In the case of tabular RL, we show that Perp obtains a complexity that scales with \((_{})\), in addition to an extra term which measures the cost of learning the behavior of the reference policy \(\). We argue that this additional term is critical to achieving instance-optimal guarantees in RL, and that Perp leads to improved complexities over existing work.
4. In the contextual bandit setting, we provide an upper bound that scales (up to lower order terms) as \((_{})\) for the _unknown-context_ distribution case. This matches the lower bound from  for the known context distribution case, thus showing that \(_{}\) is necessary and sufficient in contextual bandits even when the context distribution is unknown. Hence, we observe a qualitative information-theoretic separation between contextual bandits and RL.

The key insight from our work is that it does not suffice to _only_ learn the differences between policy values in RL, but it _almost_ suffices to--if we can learn how a single policy behaves, it suffices to learn the difference between this policy and every other policy.

## 2 Related Work

The reinforcement learning literature is vast, and here we focus on results in tabular RL and instance-dependent guarantees in RL.

Figure 1: A motivating example for differences. The rewards for all actions other than the ones specified in the figure are \(0\). Define policy set \(=\{_{1},_{2}\}\) so that \(_{1}\) always plays \(a_{1}\), whereas \(_{2}\) plays \(a_{1}\) on green states but \(a_{2}\) on red states. The difference of their state-action visitation probabilities is only non-zero in states \(s_{3},s_{4}\) and are just \(O()\) apart.

**Minimax Guarantees Tabular RL.** Finite-time minimax-style results on policy identification in tabular MDPs go back to at least the late 90s and early 2000s [24; 26; 25; 8; 21]. This early work was built upon and refined by a variety of other works over the following decade [38; 4; 34; 39], leading up to works such as [28; 9], which establish sample complexity bounds of \((S^{2}A(H)/^{2})\). More recently, [10; 33; 11] have proposed algorithms which achieve the optimal dependence of \((SA(H)/^{2})\), with [11; 33] also achieving the optimal \(H\) dependence. The question of regret minimization is intimately related to that of policy identification--any low-regret algorithm can be used to obtain a near-optimal policy via an online-to-batch conversion . Early examples of low-regret algorithms in tabular MDPs are [3; 4; 5; 48], with more recent works removing the horizon dependence or achieving the optimal lower-order terms as well [50; 51]. Recently, [6; 7] provide minimax guarantees in the multi-task RL setting as well.

**Instance-Dependence in RL.** While the problem of obtaining worst-case optimal guarantees in tabular RL is nearly closed, we are only beginning to understand what types of instance-dependent guarantees are possible. In the setting of regret minimization, [35; 14] achieve instance-optimal regret for tabular RL asymptotically. Simchowitz and Jamieson  show that standard optimistic algorithms achieve regret bounded as \((_{s,a,h}(s,a)})\), a result later refined by [47; 12]. In settings of RL with linear function approximation, several works achieve instance-dependent regret guarantees [18; 44]. Recently, Wagenmaker and Foster  achieved finite-time guarantees on instance-optimal regret in general decision-making settings, a setting encompassing much of RL.

On the policy identification side, early works obtaining instance-dependent guarantees for tabular MDPs include [49; 20; 31; 32], but they all exhibit shortcomings such as requiring access to a generative model or lacking finite-time results. The work of Wagenmaker et al.  achieves a finite-time instance-dependent guarantee for tabular RL, introducing a new notion of complexity, the _gap-visitation complexity_. In the special case of deterministic, tabular MDPs, Tirinzoni et al.  show matching finite-time instance-dependent upper and lower bounds. For RL with linear function approximation, [42; 43] achieve instance-dependent guarantees on policy identification, in particular, the complexity given in (1.2), and propose an algorithm, Pedel, which directly inspires our algorithmic approach. On the lower bound side, Al-Marjani et al.  show that \(_{}\) is necessary for tabular RL, but fail to close the aforementioned gap between \(_{}\) and (1.2). We will show instead that this gap is real and both the lower bound of Al-Marjani et al.  and upper bound of Wagenmaker and Jamieson  are loose.

Several works on linear and contextual bandits are also relevant. In the seminal work,  posed the best-arm identification problem for linear bandits and beautifully argued--without proof--that estimating differences were crucial and that (1.1) ought to be the true sample complexity of the problem. Over time, this conjecture was affirmed and generalized [16; 13; 22]. This improved understanding of pure-exploration directly led to instance-dependent optimal linear bandit algorithms for regret [29; 27]. More recently, contextual bandits have also been given a similar treatment [40; 30].

## 3 Preliminaries and Problem Setting

Let \(\|x\|_{}^{2}=x^{} x\) for any \((x,)\). We let \(_{}\) denote the probability measure induced by playing policy \(\) in our MDP.

Tabular Markov Decision Processes.We study episodic, finite-horizon, time inhomogenous and tabular Markov Decision Processes (MDPs), denoted by the tuple \((,,H,\{P_{h}\}_{h=1}^{H},\{_{h}\}_{h=1}^{H})\) where the state space \(\) and action space \(\) are finite, \(H\) is the horizon, \(P_{h}^{S SA}\) denote the transition matrix at stage \(h\) where \([P_{h}]_{s^{},sa}=(s_{h+1}=s^{}|s_{h}=s,a_{h}=a)\), and \(_{h}(s,a)_{}\) denote the distribution over reward at stage \(h\) when the state of the system is \(s\) and action \(a\) is chosen. Let \(r_{h}(s,a)\) be the expectation of a reward drawn from \(_{h}(s,a)\). We assume that every episode starts in state \(s_{1}\), and that \(_{h}\) and \(P_{h}\) are initially unknown and must be estimated over time.

Let \(=\{_{h}\}_{h=1}^{H}\) denote a policy mapping states to actions, so that \(_{h}(s)_{}\) denotes the distribution over actions for the policy at \((s,h)\); when the policy is deterministic, \(_{h}(s)\) outputs a single action. An episode begins in state \(s_{1}\), the agent takes action \(a_{1}_{1}(s_{1})\) and receives reward \(R_{1}_{1}(s_{1},a_{1})\) with expectation \(r_{1}(s_{1},a_{1})\); the environment transitions to state \(s_{2} P_{h}(s_{1},a_{1})\). The process repeats until timestep \(H\), at which point the episode ends and the agent returns to state \(s_{1}\). Let \(V_{h}^{}(s)=_{}[_{h^{}=h}^{H}r_{h^{}}(s_{h^{ }},a_{h^{}})|s_{h}=s]\), \(V_{0}^{}\) the total expected reward, \(V_{0}^{}:=V_{1}^{}(s_{0})\)and \(Q_{h}^{}(s,a)=_{}[_{h^{}=h}^{H}r_{h^{}}(s_{h^{ }},a_{h^{}})|s_{h}=s,a_{h}=a]\) the amount of reward we expect to collect if we are in state \(s\) at step \(h\), play action \(a\) and then play policy \(\) for the remainder of the episode. Note that we can understand these functions as \(S\) and \(SA\)-dimensional vectors respectively. We use \(V^{}=V_{0}^{}\) when clear from context.

We call \(w_{h}^{}_{S}\) the _state visitation vector_ at step \(h\) for policy \(\), so that \(w_{h}^{}(s)\) captures the probability that policy \(\) would land in state \(s\) at step \(h\) during an episode. Let \(_{h}^{SA S}\) denote the policy matrix for policy \(\), that maps states to state-actions as follows

\[[_{h}]_{(s,a),s^{}}=(s=s^{})[_{h}(s)]_{a}.\]

Denote \(_{h}^{}_{SA}\) as \(_{h}^{}:=_{h}w_{h}^{}\) as the _state-action visitation vector:_\(_{h}^{}(s,a)\) measures the the probability that policy \(\) would land in state \(s\) and play action \(a\) at step \(h\) during an episode. From these definitions, it follows that \([P_{h}_{h}^{}]_{s}=[P_{h}_{h}w_{h}^{}]_{s}=w_{h+1}^{}(s)\). For policy \(\), denote the covariance matrix at timestep \(h\) as \(_{h}()=_{s,a}_{h}^{}(s,a)_{(s,a)}_{(s,a)}^{}\).

\((,)\)-PAC Best Policy Identification.For a collection of policies \(\), define \(^{}:=_{}V^{}\) as the optimal policy, \(V^{}\) its value, and \(_{h}^{}\) as its state-action visitation vector. Let \(_{}:=_{\{^{}\}}V^{}-V^{}\) in the case when \(^{}\) is unique, and otherwise \(_{}:=0\). Define \(():=\{V^{}-V^{},_{}\}\). Given \( 0\), \((0,1)\) an algorithm is said to be \((,)\)-PAC if at a stopping time \(\) of its choosing, it returns a policy \(\) which satisfies \(()\) with probability \(1-\). Our goal is to obtain an \((,)\)-PAC algorithm that minimizes \(\). A fundamental complexity measure used throughout this work is defined as

\[_{}:=_{h=1}^{H}_{_{}}_{}^{}-_{h}^{}\|^{2}_{_{h}(_{})^{-1}}} {\{^{2},()^{2}\}}\|_{h}^{}- _{h}^{}\|^{2}_{_{h}(_{})^{-1}}:=_{s,a}^{}(s,a)-_{h}^{}(s,a))^{2}}{_{h}^{_{} }(s,a)}\]

where the infimum is over all exploration policies \(_{}\) (not necessarily just those in \(\)). Recall that for \(=0\),  showed any \((,)\)-PAC algorithm satisfies \([]_{}()\).

## 4 What is the Sample Complexity of Tabular RL?

In this section, we seek to understand the complexity of tabular RL. We start by showing that \(_{}\) is not sufficient. We have the following result.

**Lemma 1**.: _For the MDP \(\) and policy set \(\) from Figure 1,_

1. \(_{h=1}^{H}_{_{}}_{}^{} -_{h}^{}\|^{2}_{_{h}(_{})^{-1}}}{\{^{2},()^{2}\}} 15,\)__
2. _Any_ \((,)\)_-PAC algorithm must collect at least_ \(^{}[]\)_. samples._

Where does the additional complexity arise on the instance of Figure 1? As described in the introduction, \(_{1}\) and \(_{2}\) differ only on the red states, and a complexity scaling as \(_{}\) quantifies only the difficulty of distinguishing \(\{_{1},_{2}\}\) on these states. Note that on this example \(_{1}\) plays the optimal action in state \(s_{3}\) and a suboptimal action in state \(s_{4}\), and \(_{2}\) plays a suboptimal action in \(s_{3}\) and the optimal action in \(s_{4}\). The total reward of policy \(_{1}\) is therefore equal to the reward achieved at state \(s_{3}\) times the probability it reaches state \(s_{3}\), and the total reward of policy \(_{2}\) is the reward achieved at state \(s_{4}\) times the probability it reaches state \(s_{4}\). Here, \(_{}\) would quantify the difficulty of learning the reward achieved at each state. However, it fails to quantify _the probability of reaching each state_, since this depends on the behavior at step 1, not step 2.

Thus, on this example, to determine whether \(_{1}\) or \(_{2}\) is optimal, we must pay some additional complexity to learn the outgoing transitions from the initial state, giving rise to the lower bound in Lemma 1. Inspecting the lower bound of , one realizes that the construction of this lower bound only quantifies the cost of learning the reward distributions \(\{_{h}\}_{h}\) and _not_ the state transition matrices \(\{P_{h}\}_{h}\). On examples such as Figure 1, this lower bound then does not quantify the cost of learning the probability of visiting each state, which we've argued is necessary. We therefore conclude that, while \(_{}\) may be enough for learning the rewards, it is _not_ sufficient for solving the full tabular RL problem. Our main algorithm builds on this intuition, and, in addition to estimating the rewards, aims to estimate where policies visit as efficiently as possible.

### Main Result

If \(_{}\) is not achievable as the sample complexity for Tabular RL, what is the best that we can do? In this section, we answer this question with our sample complexity bound; we later describe the algorithmic insights that enable us to achieve this result in the following section. First, for any \(,\), we define

\[U(,):=_{h=1}^{H}\!_{s_{h} w_{h}^{}}[(Q_{h}^{ }(s_{h},_{h}(s_{h}))-Q_{h}^{}(s_{h},_{h}(s_{h})))^{2}].\] (4.1)

Now, we state our main result.

**Theorem 1**.: _There exists an algorithm (Algorithm 1) which, with probability at least \(1-2\), finds an \(\)-optimal policy and terminates after collecting at most_

\[_{h=1}^{H}_{_{}}_{}\|_{h}^{ }-_{h}^{}\|^{2}_{_{h}(_{})^{-1}}}{\{ ^{2},()^{2}\}}^{2}+_{})}{\{^{2},()^{2}\}}}{}+}}{\{^{}, _{}^{}\}}\]

_episodes, for \(C_{}:=(S,A,H, 1/,,||),:=C })}\) and \(:=}\)._

Theorem 1 shows that, up to terms lower-order in \(\) and \(_{}\), \(_{}\) is almost sufficient, if we are willing to pay for an additional term scaling as \(U(,^{})/()^{2}\). Recognize the similarity of this term to the that from the performance difference lemma: if there were no square inside the expectation, the quantity \(U(,^{})\) would be equal to \(()\). However, the square may change the scaling in some instances. Below, Lemma 2 shows that there exist settings where the complexity of Theorem 1 could be significantly tighter than Equation (1.2), the complexity achieved by the Pedel algorithm of . We revisit the instance from Figure 1 to show this; recall from Lemma 1 that the first term from Theorem 1 is a universal constant for this instance.

**Lemma 2**.: _On MDP \(\) and policy set \(\) from Figure 1, we have:_

1. \(_{})}{\{^{2},()^{2} \}}=,\)__
2. \(_{h=1}^{H}_{_{}}_{}^{} \|^{2}_{_{h}(_{})^{-1}}}{\{^{2},()^ {2}\}}}.\)__

Furthermore, the complexity of Theorem 1 is never worse than Equation (1.2).

**Lemma 3**.: _For any MDP instance and policy set \(\), we have that_

\[_{h=1}^{H}_{_{}}_{}^{}-_{h}^{}\|^{2}_{_{h}(_{})^{-1}}}{ \{^{2},()^{2}\}},)}{\{ ^{2},()^{2}\}}}_{h=1}^{H}_{_{}}_ {}^{}\|^{2}_{_{h}(_{})^{-1}} }{\{^{2},()^{2}\}}.\]

We briefly remark on the lower-order term for Theorem 1, \(}}{\{^{5/3},_{}^{5/3}\}}\). Note that for small \(\) or \(_{}\), this term will be dominated by the leading-order terms, which scale with \(\{^{-2},_{}^{-2}\}\). While we make no claims on the tightness of this term, we note that recent work has shown that some lower-order terms are necessary for achieving instance-optimality .

### The Main Algorithmic Insight: The Reduced-Variance Difference Estimator

In this section, we describe how we can estimate the difference between the values of policies directly, and provide intuition for why this results in the two main terms in Theorem 1. Fix any reference policy \(\) and logging policy \(\) (neither are necessarily in \(\)). Here \(\) can be thought of as playing the role of \(_{}\). Or, we can consider the A/B testing scenario from the introduction, where a policy \(\) is taking random actions and one wishes to perform off-policy estimation over some set of policies \(\)[17; 15]. For any \(s\), we define

\[_{h}^{}(s):=w_{h}^{}(s)-w_{h}^{}(s)\]

as the difference in state-visitations of policy \(\) from reference policy \(\), and \(_{h}^{}^{S}\) as the vectorization of \(_{h}^{}(s^{})\).

Policy selection rule.First, we describe our procedure of data collection and estimation. We collect \(K_{}\) trajectories from \(\) and \(K_{}\) trajectories from \(\), and let \(\{_{h}^{}(s)\}_{s,h}\) denote the empirical state visitations from playing \(\). From the data collected by playing \(\), we construct estimates \(\{_{h}(s^{}|s,a)\}_{s,a,s^{},h}\) of the transition matrices. Note that \(_{h}^{}(s)\) simply counts visitations, so that \([(_{h}^{}(s)-w_{h}^{}(s))^{2}]^{} (s)}{K_{}}\) for all \(h,s\). Define estimated state visitations for policy \(\) in terms of deviations from \(\) as \(_{h}^{}:=_{h}^{}+_{h}^{}\). Here, \(_{h}^{}\) is defined recursively as:

\[_{h+1}^{}:=_{h}_{h}_{h }^{}+_{h}(_{h}-}_{h})_{h}^{}\]

Then, assuming, for simplicity, that rewards are known, we recommend the following policy:

\[=_{}^{}^{}:=_{h=1}^{H} r_{h},_{h}_{h}^{}- r_{h},(}_{h}-_{h}) _{h}^{}\]

Sufficient condition for \(\)-optimality.Here, we show that if

\[,|^{}-D^{}|\{,()\}\] (4.2)

then \(\) is \(\)-optimal. First, write the difference between values of policies \(\) and \(\) as:

\[ D^{}:=V_{0}^{}-V_{0}^{}& =_{h=1}^{H} r_{h},_{h}w_{h}^{}-_ {h=1}^{H} r_{h},}_{h}w_{h}^{}\\ &=_{h=1}^{H} r_{h},_{h}_{h}^{} - r_{h},(}_{h}-_{h})w_{h}^{}.\] (4.3)

Then, it is easy to verify that if \(|^{}-D^{}| 1/3\ ()\), then \(^{^{*}}-^{} 0\); hence, \(\). Hence, under Condition (4.2), either \(=^{}\) or or \(|^{}-D^{}|\). In the first case, clearly \(\) is \(\)-optimal. In the second case, we can add and subtract terms to write

\[V^{}-V^{}|D^{^{*}}-^{^{*}}|+ {D}^{^{*}}-^{}+|^{}-D^{ }|+^{^{*}}-^{}.\]

The last inequality follows since \(\) maximizes \(^{}\). Hence, \(\) would be \(\)-optimal in this case as well.

Sample complexity.Now, we characterize how many samples must be collected from \(\) and \(\) in order to meet Condition (4.2). After dropping some lower-order terms and unrolling the recursion (see Section A for details), we observe that

\[_{h+1}^{}-_{h+1}^{}& (_{h}-P_{h})(_{h}^{}-_{h}^{})+P_{h}( _{h}-}_{h})(_{h}^{}-w_{h}^{ })+P_{h}_{h}(_{h}^{}-_{h}^{})\\ &=_{k=0}^{h}_{j=k+1}^{H}P_{j}_{j} (_{k}-P_{k})(_{k}^{}-_{k}^{})+P_{k}( _{k}-}_{k})(_{k}^{}-w_{h}^{ }).\]

After manipulating this expression a bit more, we observe that

\[_{h=1}^{H} r_{h},_{h}(_{h}^{}-_{h}^ {})=_{k=0}^{H-1} V_{k+1}^{},(_{k}-P_{k})( _{k}^{}-_{k}^{})+P_{k}(_{k}-}_{k})( _{k}^{}-w_{k}^{})\]

Recognizing \(Q_{h}^{}=r_{h}+P_{h}^{}V_{h+1}^{}\),

\[|^{}&-D^{}|=|_ {h=1}^{H} r_{h},_{h}(_{h}^{}-_{h}^{ })+ r_{h},(_{h}-}_{h})(_{h}^{ }-w_{h}^{})|\\ &=|_{h=0}^{H-1} V_{h+1}^{},(_{h}-P_{h })(_{h}^{}-_{h}^{})+ r_{h}+P_{h}^{}V_{h+1} ^{},(_{h}-}_{h})(_{h}^{}-w_{h}^{ })|\]

We can bound this as:

\[&_{h=0}^{H-1}_{s,a}^{}(s,a)-_{h}^{}(s,a))^{2}}{K_{}_{h}(s,a)}}+ {_{h=0}^{H-1}_{s}Q_{h}^{}(s,_{h}(s))-Q_{h}^{}(s,_{h}(s))^{2}^{}(s)}{K_{}}}\\ &=_{h=0}^{H-1}^{}-_{h}^{ }\|_{_{h}()^{-1}}^{2}}{K_{}}}+ )}{K_{}}}.\]Here, we applied Bernstein's inequality and observed that \(_{s^{}}V_{h+1}^{}(s^{})^{2}P_{h}(s^{}|s,a) H^{2}\). Now, we have that if

\[K_{}_{}_{h=0}^{H-1}\|_{h}^{}-_ {h}^{}\|_{_{h}()^{-1}}^{2}}{\{^{2},( )^{2}\}} K_{}_{})}{\{^{2},()^{2}\}}\] (4.4)

then Condition (4.2) holds. Notice that up to \(H\) and \(()\) factors, this is precisely the sample complexity of Theorem 1 if we set \(=^{}\) and minimize over all logging/exploration policies \(/_{}\). Note that, if \(\) denotes the average reward collected from rolling out \(\)\(K_{}\) times, then \(|-V_{0}^{}|}{K_{}}}\) by Hoeffding's inequality. Thus, one could use \(^{}=^{}+\) as an effective off-policy estimator. Likewise, \(^{}-^{^{}}\) is an effective estimator for \(V_{0}^{}-V_{0}^{^{}}\).

This calculation (elaborated on in Appendix A) suggests that our analysis is tight, and clearly illustrates that the \(U(,)\) term arises due to estimating the behavior of the reference policy \(w_{h}^{}\). The \(U(,)\) term is, to the best of our knowledge, novel in the literature. More precisely, this term corresponds to the cost of estimating where \(\) visits, if our goal is to estimate the difference in value between policy \(\) and \(\). If, for a given state, the actions taken by \(\) and \(\) achieve the same long-term reward, then it is not critical that the frequency with which \(\) visits this state is estimated, as it does not affect the difference in values between \(\) and \(\); if the actions take by \(\) and \(\) do achieve different long-term reward at \(s\), then we must estimate the behavior of each policy at this state. This is reflected by the term inside the expectation of \(U(,)\); this will be \(0\) in the former case, and scale with the difference between long-term action reward in the latter case.

Additionally, note that if we had offline data from _some_ policy \(\), that had been played for a long time, so that \(K_{}\), then we would only incur the \(K_{}\) term; this is precisely \(_{}\), but with \(^{}\) replaced with our reference policy \(\) in the numerator.

## 5 Achieving Theorem 1: Perp Algorithm

While the above section provides intuition for where the terms in Theorem 1 come from, it does not lead to a practical algorithm. This is because the desired number of samples in Equation (4.4) are in terms of unknown quantities: \(\{\|_{h}^{}-_{h}^{}\|_{_{h}()^{-1}}^{2},( ),U(,)\}\), which depend on our unknown environment variables \(_{h},P_{h}\); hence, we would not know how many samples to collect. In this section, we propose an algorithm that will proceed in rounds, successively improving our estimates of these quantities. Define

\[_{,h}(,^{}):=}_{^{}, }[(_{,h}^{}(s_{h},_{h}(s))-_{,h}^{ }(s_{h},^{}_{h}(s)))^{2}],\] (5.1)

where \(}_{^{},}\) denotes the expectation induced playing policy \(^{}\) on the MDP with transitions \(_{,h}\), and \(_{,h}^{}\) denotes the \(Q\)-function of policy \(\) on this same MDP. To compute \(_{,h}\), we use the standard estimator: \(_{,h}(s^{} s,a)=(s,a,s^{})}{N_ {,h}(s,a)}\) for \(N_{,h}(s,a)\) and \(N_{,h}(s,a,s^{})\) the visitation counts in \(_{,h}^{}\). We set \(_{,h}(s^{} s,a)=()\) if \(N_{,h}(s,a)=0\). The analogous estimator is used to estimate \(_{,h}\). The quantity \(_{h}^{}-_{h}^{}\) is estimated as in the previous section: \((_{h}-}_{,h})_{,h}^{}+_{h}_{,h}^{}\).

Algorithm 1 proceeds in epochs. It begins with a policy set \(_{1}\), which contains all policies of interest, \(\). It then gradually begins to refine this policy set, seeking to estimate the _difference_ in values between policies in the set up to tolerance \(_{}=2^{-}\). To achieve this, it instantiates the intuition above. First, it chooses a reference policy \(_{}\), then running this estimate a sufficient number of times to estimate \(w_{h}^{_{}}\). Given this estimate, it then seeks to estimate \(_{h}^{}\) for each \(\) in the active set of policies, \(_{}\), by collecting data covering the directions \((_{h}-}_{,h})_{,h}^{}+_ {h}_{,h}^{}\) for all \(_{}\). To efficiently collect this covering data, on line 1, we run a data collection procedure first developed in . Finally, after estimating each \(_{h}^{}\), it estimates the differences between policy values as in (4.3), and eliminates suboptimal policies.

The computational complexity of Perp is poly \((S,A,H,1/,||,(1/))\). The primary contributor to the computational complexity is the the use of the Franke-Wolfe algorithm for experiment design in the OptCov subroutine. Lemma 37 from Wagenmaker and Pacchiano  shows that the number of iterations of the Franke-Wolfe algorithm is bounded polynomially in the problem parameters,and from the definition of this procedure given in Wagenmaker and Pacchiano , we see that each iteration of Franke-Wolfe has computational complexity polynomial in problem parameters. We omit several technical details from Algorithm 1 for simplicity, but present the full definition in Algorithm 2.

```
1:tolerance \(\), confidence \(\), policies \(\)
2:\(_{1}\), \(_{0}\) arbitrary transition matrix
3:for\(=1,2,3,,_{2}\)do
4: Set \(_{} 2^{-}\)
5: // Compute new reference policy
6: Compute \(_{-1,h}(,^{})\) as in (5.1) for all \((,^{})_{}\)
7: Choose \(_{}_{_{}}_{_{}}_{ h=1}^{H}_{-1,h}(,)\)
8: Collect the following number of episodes from \(_{}\) and store in dataset \(_{}^{}\) \[_{}=_{_{}}c_ {-1}(,_{})}{_{}^{2}}^{2}\|_{}\|}{}\]
9: Compute \(\{_{,h}^{}(s)\}_{h=1}^{H}\) using empirical state visitation frequencies in \(_{}^{}\)
10: // Estimate Policy Differences
11: Initialize \(_{1}^{} 0\)
12:for\(h=1,,H\)do
13: Run OptCov (Algorithm 3) to collect dataset \(_{,h}^{}\) such that: \[_{_{}}\|(_{h}-}_{,h})_{ ,h}^{}+_{h}_{,h}^{}\|_{_{ ,h}^{-1}}^{2}_{}^{2}/H^{4}_{}^{2} _{,h}=_{(s,a)_{,h}^{}}e_{sa} e_{sa}^{}\] and \(_{}(|_{}|/})\)
14: Use \(_{,h}^{}\) to compute \(_{,h}(s^{}|s,a)\) and \(_{,h}\)
15: Compute \(_{,h+1}^{}_{,h}(_{h}- }_{,h})_{,h}^{}+_{,h} _{h}_{,h}^{})\)
16:endfor
17:// Eliminate suboptimal policies
18: Compute \(_{_{}}()_{h}_{, h},_{h}_{,h}+_{h}_{ ,h},(_{h}-}_{,h})_{,h}^{}\)
19: Update \(_{+1}=_{}\{_{}:_{^{}} {D}_{_{}}(^{})-_{_{}}()>8 _{}\ \}\)
20:if\(|_{+1}|=1\)thenreturn\(_{+1}\)
21:endfor
22:return any \(_{+1}\) ```

**Algorithm 1**Perp: Policy Elimination with Reference Policy (informal)

## 6 When is Sufficient?

Our results so far show that \(_{}\) is not in general sufficient for tabular RL. In this section, we consider several special cases where it _is_ sufficient.

Tabular Contextual Bandits.The tabular contextual bandit setting is the special case of the RL setting with \(H=1\) and where the initial action does not affect the next-state transition. Theorem 2.2 of Li et al.  show that if the rewards distributions \((s,a)\) are Gaussian for each \((s,a)\), where here \(s\) denotes the context, any \((0,)\)-PAC algorithm requires at least \(_{}\) samples. Crucially, however, they assume that the context distribution--in this case corresponding to the initial transition \(P_{1}\)--is known. Their algorithm makes explicit use of this fact, using this to estimate the value of \(^{}\). The following result shows that knowing the context distribution is not critical--we can achieve a complexity of \((_{})\) without this prior knowledge.

**Corollary 1**.: _For the setting of tabular contextual bandits, there exists an algorithm such that with probability at least \(1-2\), as long as \(\) contains only deterministic policies, it finds an \(\)-optimalpolicy and terminates after collecting at most the following number of samples:_

\[_{_{}}_{}-^{}\|^{2}_{ _{(_{})^{-1}}}}{\{^{2},()^{2}\}} ^{2}}+} }{\{^{5/3},_{}^{5/3}\}},\]

_for \(C_{}=(||,A, 1/, 1/(_{ }),||)\) and \(=C })}\)._

The theorem is proved in Appendix D, and follows from the application of our algorithm Perp to the contextual bandit problem. The key intuition behind this result is that, in the contextual case:

\[U(,)=_{s P_{1}}[(r_{1}(s,_{1}(s))-r_{1}(s,_{1}(s))^{2}]_{s P_{1}}[\{_{1}(s)_{1}(s)\}].\]

It is then possible to show that, since \(_{}\) only has choices of which actions are taken (and cannot affect the context distribution), this can be further bounded by \(_{_{}}\|^{}-^{}\|^{2}_{(_{ })^{-1}}\). This is not true in the full MDP case, where our choice of exploration policy in \(_{}\) could make \(_{_{}}\|^{}-^{}\|^{2}_{(_{ })^{-1}}\) significantly smaller than \(U(,)\) (as is the case in Lemma 2). Hence, we observe that the cost of learning the contexts is dominated by that of learning the rewards in the case of contextual bandits. This is the opposite of tabular RL, where our complexity from Theorem 1 is unchanged (as seen in Section 4.2) even if we knew the reward distribution. This shows that there is a distinct separation between instance-optimal learning in tabular RL vs contextual bandits.

MDPs with Action-Independent Transitions.In the special case of MDPs where the transitions do not depend on the actions selected, the complexity simplifies to \((_{})\). Note that this exactly matches (up to lower order terms) the lower bound from .

**Corollary 2**.: _Assume that all \(P_{h}\) are such that \(P_{h}(s^{}|s,a)=P_{h}(s^{}|s,a^{})\) for all \((a,a^{})\). Then, with probability at least \(1-2\), Perp (Algorithm 2) finds an \(\)-optimal policy and terminates after collecting at most the following number of episodes:_

\[_{h=1}^{H}_{_{}}_{}_{ h}-^{}_{h}\|^{2}_{_{h}(_{})^{-1}}}{\{^{2}, ()^{2}\}} H^{4}^{2}+}}{\{ ^{5/3},_{}^{5/3}\}}\]

_for \(C_{},\) as defined in Theorem 1._

The intuition for Corollary 2 is similar to that of Corollary 1, and proved in Appendix E.

## 7 Discussion

In this paper, we performed a fine-grained study of the instance-dependent complexity of tabular RL. We proposed a new off-policy estimator that estimates the value relative to a reference policy. We leveraged this insight to close the instance-dependent contextual bandits problem and obtained the tightest known upper bound for tabular MDPs.

**Limitations and Future work** One limitation of the present work is that Perp, in it's current form, would be too computationally expensive to run for most practical applications; enumerating the policy set \(\) is often intractable, but works in contextual bandits have avoided this issue by only relying on argmax oracles over this set ; an interesting direction of future work would be to extend this technique to tabular RL. Extending the results from this paper to obtain refined instance-dependent bounds for linear MDPs and general function approximation is an exciting direction as well.

The new estimator and its improved sample complexity raise additional theoretical questions. Our upper bound has unfortunate low order terms; can these be removed? Can one show that \()}{(()^{2},^{2})}\) is unavoidable for all MDPs in general, thereby matching our upper bound? As discussed above, a few works have proven gap-dependent regret upper bounds, but we are unaware of any matching lower bounds besides over restricted classes of MDPs; can our estimator involving the differences result in even tighter instance-dependent regret bounds for MDPs?