# Non-Stationary Bandits with Auto-Regressive Temporal Dependency

Qinyi Chen

Operations Research Center

Massachusetts Institute of Technology

qinyc@mit.edu

&Negin Golrezaei

Sloan School of Management

Massachusetts Institute of Technology

golrezae@mit.edu

&Djallel Bouneffouf

IBM Research

djallel.bouneffouf@ibm.com

###### Abstract

Traditional multi-armed bandit (MAB) frameworks, predominantly examined under stochastic or adversarial settings, often overlook the temporal dynamics inherent in many real-world applications such as recommendation systems and online advertising. This paper introduces a novel non-stationary MAB framework that captures the temporal structure of these real-world dynamics through an auto-regressive (AR) reward structure. We propose an algorithm that integrates two key mechanisms: (i) an alternation mechanism adept at leveraging temporal dependencies to dynamically balance exploration and exploitation, and (ii) a restarting mechanism designed to discard out-of-date information. Our algorithm achieves a regret upper bound that nearly matches the lower bound, with regret measured against a robust dynamic benchmark. Finally, via a real-world case study on tourism demand prediction, we demonstrate both the efficacy of our algorithm and the broader applicability of our techniques to more complex, rapidly evolving time series.

## 1 Introduction

The multi-armed bandit (MAB) framework  is commonly used to study online decision-making under uncertainty. It is primarily examined under either the stochastic setting , where arms have fixed unknown reward distributions, or the adversarial setting , where an adversary determines the reward distributions that can change arbitrarily over time. However, neither setting accurately represents real-world decision-making problems (see examples in dynamic pricing , online advertising , and online auctions ), where reward distributions change over time following intrinsic temporal structures. These structures often exhibit frequent variations and temporal dependencies, making them challenging to approximate using stationary distributions.

This motivates us to consider a non-stationary bandits setting involving certain temporal structure, which captures the real-world characteristics. Specifically, we are interested in temporal structures with linear amount of changes, which are distinct from the infrequent, limited changes typically handled by change-point detection  or seasonality analysis . Such frequent changes are commonly observed in applications such as financial data  and click-through rates in online advertising , where the data experience lots of volatility within a short time. Traditional bandits algorithms, such as Thompson sampling or UCB, may explore too aggressively in response to these drastic changes, leading to suboptimal performance . That being said, the observed time series in these scenarios typically exhibit certain temporal dependencies between past and present, which can be leveraged to improve decision-making.

To encapsulate the main characteristics of real-world time series, we relax the previously restrictive assumptions on the reward distributions and study a non-stationary MAB problem, where the rewardof each arm evolves over time following an auto-regressive (AR) temporal structure. The AR model is a popular time series model for predicting outcomes in applications such as sales , advertising  and marketing . It simultaneously captures frequent changes (i.e., linear amount of changes over the time horizon) and temporal dependencies, both of which impact the quality of online decision-making. Although real-world data cannot be perfectly represented by an AR model, it has been shown that the AR parameters accurately capture temporal dependencies and can serve as a useful proxy . The non-stationary bandits framework with an AR reward structure can effectively model a wide range of real-world applications, including:

(1) **recommendation systems**, where MAB can determine the best product/contents to display to users, while the AR model captures the evolution of user preferences and demand over time  (see Section 7 for a related case study on tourism demand prediction);

(2) **online advertising**, where MAB adjusts budget allocation for ad campaigns, and the AR model can capture the evolution of click-through rates of ads. The amount of changes are usually linear in time, making the AR structure a potential modeling choice ;

(3) **financial portfolio management**, where MAB can determine the investment allocation, and the AR model can capture the evolution of returns for each investment/asset .

When making online decisions in the presence of frequent changes, traditional methods that are designed for stationary or adversarial settings can be ineffective, due to two main challenges.

1. Balancing exploration and exploitation becomes more challenging, as complete exploration is often unattainable. Continuous exploration is crucial, but there is also a risk of over-exploration. Therefore, finding the right balance becomes essential in navigating this tradeoff effectively.
2. Frequent changes in the environment can significantly diminish the value of past learnings. Failing to promptly leverage the knowledge we have gained can lead to a swift deterioration in our confidence levels regarding the accuracy of our reward estimates.

The goal of this work is to show, via our non-stationary bandits framework with an AR reward structure, that _effectively leveraging knowledge of temporal structure can address these challenges_. In our approach, we would use our knowledge or estimates of the temporal dependency, measured by the AR parameter. Since non-stationary bandits with temporal structures exhibiting such real-world characteristics have been scarcely explored in the literature, we consider the AR structure as a suitable starting point for showcasing key ideas and techniques. We anticipate that this work will inspire future research on bandits with alternative temporal structures with similar features.

### Main Contributions

We present an algorithm for the non-stationary AR bandits, called AR2, which stands for "Alternating and Restarting" algorithm for non-stationary "AR" bandits. AR2 features two mechanisms: (i) an _alternation_ mechanism that handles the first challenge above by enforcing exploitation at least every other round, and switch to exploration only when we discover an arm with potential, instead of simply adopting the principle of "optimism in the fact of uncertainty" which can lead to over-exploration. (ii) a _restarting_ mechanism that handles the second challenge by discarding unnecessary information when our confidence level deteriorates significantly, which balances a second tradeoff known as "remembering"-and-"forgetting"; this tradeoff is crucial even for time series with limited changes . Overall, our work addresses an important gap in the literature by studying non-stationary bandits with temporal structure that exhibit frequent changes and temporal dependency. While the AR structure does not capture all temporal structures in the real world, we believe that our high-level messages and techniques will guide future research on bandits with other temporal structures.

In evaluating the performance of our algorithm, we employ the concept of _per-round steady state dynamic regret_ (Definition 2.1). This metric serves as a robust dynamic benchmark that competes with the best arm in each round. It surpasses the static benchmark used in stochastic and adversarial MAB, making it a considerably stronger measure of performance. We provide an upper bound on the regret of AR2 (Theorem 5.2), for which the analysis is rather intricate (Section 6), and show that it almost matches the regret lower bound (Theorem 3.1). Our lower bound result also characterizes the challenges embedded in AR temporal processes, as it shows the per-round dynamic regret does not go to zero as \(T\) increases, implying that any algorithm needs to keep exploring over time.

Finally, we conduct a real-world case study on tourism demand prediction  in Section 7, confirming the superiority of AR2 compared to benchmark algorithms. There, we also show that the techniques and high-level ideas of our algorithm can be readily extended to handle more complicated, rapidly changing temporal structure (e.g., general AR-\(p\) processes) while still achieving good performance. Our case study is complemented by synthetic experiments in Appendix A which show the strength of AR2 against a number of benchmarks designed for stationary and non-stationary settings.

### Related Work

Our work, which focuses on MAB with an AR temporal structure, contributes to the non-stationary MAB literature that draws attention in recent years. Most related works on non-stationary bandits focused on either the _rested bandits_ or _restless bandits_. In rested bandits, only the expected reward of the arm that is being pulled will change, while in restless bandits, the expected reward of each arm changes at each round according to a known yet arbitrary stochastic process, regardless of whether the arm is pulled. Recently, there has also been a third stream of literature that studies non-stationary bandits where the changes in reward distributions depend on both which arm is pulled and the time period that passes by since its last pull (see, e.g., [32; 13]). The non-stationary bandit problem that we study belongs to the restless setting, since the reward distributions change at each round based on AR processes, independent of our actions. Restless bandits, however, is known to be difficult and intractable , and many work thus focus on studying its approximation  or relaxation .

One line of closely related work on restless bandits studies non-stationary MAB problems with limited amount of changes. These works either assume piecewise stationary rewards (see [25; 37; 12]), or impose a variation budget on the total amount of changes (see ). They differ from our setting in that they rely on the amount of changes in the environment to be sublinear in \(T\), while in AR processes, changes are more rapid and linear in \(T\). Another line of relevant research on restless bandits assumes different types of temporal structures for the reward distributions. See, e.g., [38; 41; 50; 40] for finite-state Markovian processes,  for stationary \(\)-mixing processes and  for Brownian processes, which is a special case of an AR model but does not experience the exponential decay in the correlation between the past and the future. Recently,  proposed a predictive sampling algorithm that can also be applied to AR-1 bandits; their main focus, however, is to show the algorithm's superiority over the traditional Thompson sampling algorithm in non-stationary environments.

Similar to the second line of works above, our work also aims to exploit the temporal structure of the reward distributions to devise well-performing learning algorithms. However, the AR process is much more unpredictable than the temporal structures studied previously, due to its infinite state space and the fast decay in the values of the past observations. We are thus prompted to take a different approach that is designed specifically for adapting to the rapid changes attributed to the AR process.

## 2 Preliminaries

**Expected rewards.** Consider a non-stationary MAB problem with \(k\) arms over \(T\) rounds. The state (expected reward) of arm \(i[k]\) at any round \(t[T]\) is denoted by \(r_{i}(t)\), where \(r_{i}(t)[-,+]\). Conditioned on the state \(r_{i}(t)\), the realized reward of arm \(i\) at round \(t\), denoted by \(R_{i}(t)\), is given by \(R_{i}(t)=r_{i}(t)+_{i}(t)\,,\) where \(_{i}(t) N(0,)\) is independent across arms and rounds and we call \((0,1)\) the stochastic rate of change. More generally, if each arm \(i[k]\) has a different, unknown stochastic rate of change \(_{i}\), as long as we have an upper bound \(_{i}_{i}\), all of our theoretical results naturally carries over with the same dependency on \(\).

**Evolution of expected rewards.** The expected reward of each arm \(i[k]\), \(r_{i}(t)\), evolves over time. In our model, we assume that \(r_{i}(t)\) undergoes an independent AR-1 process:

\[r_{i}(t+1)=(r_{i}(t)+_{i}(t))= R_{i}(t)\,,\]

or equivalently, \(R_{i}(t+1)= R_{i}(t)+_{i}(t+1)\), where \((0,1)\) is the parameter of the AR-1 model and can be used as a proxy to measure temporal correlation over time. As \(\) increases, the temporal correlation increases. As commonly seen in the bandit literature (e.g., ), we make the rewards bounded by truncating \(r_{i}(t+1)\) when its absolute value exceeds some finite boundary value \(>0\). That is, \(r_{i}(t+1)=(r_{i}(t)+_{i}(t))\,,\) where \((y)=\{\{y,-\},\}\).

Here, we assume that the AR parameter \(\) is known to the decision maker, a common assumption in literature [10; 27; 48], and often justifiable in real-world scenarios where sufficient historical data enables accurate fitting of AR models (see Section 7). This assumption is made to simplify the analysis. However, in Section 8, we would show that this assumption can be relaxed, and the AR parameter can be effectively estimated via maximum likelihood estimation.

We also assume here that all arms share the same AR parameter \(\). As our numerical studies in Appendix A and Section 8 suggest, this assumption can be relaxed too. There, we generalize our model by considering heterogeneous AR parameters \(_{i}\) for each arm \(i[k]\), and our algorithm maintains its performance. Nonetheless, it is worth noting that the homogeneity of AR parameter can be justified in some applications. One example can be seen in our case study in Section 7, where a travel agency offers vacation packages to travelers originating from a particular origin and headingto a specific destination. In this scenario, the demand is modeled as a general AR-\(4\) process, with the AR parameters being consistent across all arms. This is because the temporal correlation here is primarily determined by exogenous factors related to the travel route, such as popular attractions and the availability of flights between the two locations, hence impacting all arms in the same way.

**Feedback structure and goal.** Our goal is to design an algorithm that obtains high reward, against a strong dynamic benchmark that in every round \(t\), pulls an arm that has the highest expected reward. This benchmark is much more demanding than the time-invariant benchmark used in traditional MAB literature. Obtaining good performance against the dynamic benchmark is particularly challenging as in any round \(t[T]\), only the reward of the pulled arms can be observed. Not being able to observe the reward of unpulled arms in dynamic and time-varying environments introduces _missing values_ in the AR process associated with arms. This, in turn, deteriorates the prediction quality of the future expected rewards and the performance of any algorithm that relies on such predictions (see  for a work that studies the impact of missing values on the prediction quality in AR processes).

We now formally define how we measure the performance of any non-anticipating algorithm.

**Definition 2.1** (Dynamic steady state regret).: _Let \(\) be a non-anticipating algorithm that, at each round \(t[T]\), pulls arm \(I_{t}\) based on the history \(\{I_{1},R_{1},,I_{t-1},R_{t-1}\}\), where \(R_{t^{}}=R_{I_{t^{}}}(t^{})\) is the observed reward at round \(t^{}<t\). The dynamic regret of \(\) at round \(t\) is defined as_

\[_{}(t)=r^{}(t)-r_{I_{t}}(t),\]

_where \(r^{}(t)=_{i[k]}\{r_{i}(t)\}\) is the maximum expected reward at round \(t\). Furthermore, the per-round steady state regret of algorithm \(\) is defined as_

\[}_{}=_{T}[ _{t=1}^{T}_{}(t)].\]

We remark that in Definition 2.1, our per-round regret is defined asymptotically mainly because we would like to focus on evaluating the steady state performance of our algorithm (i.e., under the steady state distribution).1 All of our theoretical analyses for regret lower bound (Section 3) and upper bounds (Section 5) are also performed under the steady state distribution.

In this work, we would like to design an algorithm with a small per-round steady state regret, where the per-round regret is a function of the stochastic rate of change (\(\)) and the temporal correlation (\(\)).

## 3 Regret Lower Bound

We now provide a lower bound on the best achievable dynamic per-round steady state regret.

**Theorem 3.1** (Regret Lower Bound).: _Consider a non-stationary MAB problem with \(k\) arms, where the expected reward of each arm evolves as an independent AR-1 process with parameter \(\), stochastic rate of change \(\) and truncating boundaries \([-,]\). Then the per-round steady state regret of any algorithm \(\) is at least \((g(k,,))\), where \(g(k,,)\) is the probability that two best arms are within \(\) distance of each other at any given round, under the steady state distribution. See the expression of this probability in Equation (16) in Appendix F._

The proof of Theorem 3.1, provided in Appendix F, builds on the following idea: even if algorithm \(\) has access to all past information at round \(t\), due to stochastic noise \(_{i}(t)\), \(\) will pull a sub-optimal arm with constant probability at round \(t+1\). We show that the probability of the two best arms' expected rewards being within \(\) of each other can be expressed as \(g(k,,)\) (see (16)).2 If the two best arms are \(\) close, \(\) will incur \(()\) regret with constant probability at round \(t+1\).

In the extreme case when \(\) is close to one, the steady state distribution of \(r_{i}(t)\) can be approximated with a uniform distribution within the boundaries and two probability masses at the boundaries (see Appendix D). Using the uniform approximation, one can show that \(g(k,,)=(k)\) (see discussion in Appendix F). This then yields a regret lower bound of order \((k^{2}^{2})\).

Theorem 3.1 implies that under our setup, the best achievable per-round regret with respect to a strong dynamic benchmark does not converge to zero, which differs from stationary or adversarial

**Input:** AR Parameter \(\), stochastic rate of change \(\), epoch size \(_{}\), parameter \(c_{0}\).

1. Set the epoch index \(s=1\), parameter \(c_{1}=24c_{0}\).
2. Repeat while \(s T/_{}\): 1. Initialization: Set \(t_{0}=(s-1)_{}\). Set the initial triggered set \(=\). Pull each arm \(i[k]\) at round \(t_{0}+i\) and set \(_{i}=t_{0}+i\). Set estimates \(_{i}(t_{0}+k+1)=^{k-i}( R_{i}(t_{0}+i))\). 2. Repeat for \(t=t_{0}+(k+1),,\{t_{0}+_{},T\}\) * Update the identity of the superior arm and its estimated reward \[i_{}(t)=\{I_{t-1}&_{I_{t-1}}(t) _{I_{t-2}}(t)\\ I_{t-2}&_{I_{t-1}}(t)<_{I_{t-2}}(t) ._{}(t)=_{i_{}(t)}(t)\,.\] (1)
3. Trigger arms with potential: For \(i\{i_{}(t)\}\), trigger arm \(i\) if \[_{}(t)-_{i}(t) c_{1}-^{2(t-_{i}+1)})/(1-^{2})}.\] (2) If triggered, add \(i\) to \(\) and set \(_{i}^{}=t\).
4. Alternate between exploration and exploitation: * If \(t\) is odd and \(\), pull a triggered arm with the earliest triggering time: \(I_{t}=_{j}_{j}^{}\,.\) * Otherwise, pull the superior arm \(I_{t}=i_{}(t)\). * Receive a reward \(R_{I_{t}}(t)\), and set \(_{I_{t}}=t\). * Maintain Estimates: Set \(_{I_{t}}(t+1)=( R_{I_{t}}(t))\) and set \(_{i}(t+1)=_{i}(t)\) for \(i I_{t}\). 3. Set \(s=s+1\).

**Algorithm 1** Alternating and Restarting algorithm for non-stationary AR bandits (AR2)

bandits with a time-invariant benchmark where algorithms like UCB for stationary bandits and Exp3 for adversarial bandits can achieve zero per-round regret as \(T\) approaches infinity. In our setting, each arm undergoes linear amount of changes, making it challenging for any algorithm to adapt to the changing environment in time. Our result aligns with the lower bound from , which states that when the total variation of expected rewards is \(O(T)\), the regret also grows linearly.

## 4 Algorithm Ar2

In this section, we present our algorithm, called AR2 (Alternating and Restarting algorithm for non-stationary "AR" bandits). Algorithm 1 outlines the workings of AR2, which operates in epochs of a fixed length. Within each epoch, AR2 maintains and updates estimates of the expected rewards for each arm \(i[k]\). It alternates between exploitation and exploration steps based on these estimates. During exploitation, AR2 plays a "superior arm" expected to yield high rewards, while in exploration, it selects a "triggered arm" that hasn't been pulled recently but has high potential. At the end of an epoch, the algorithm restarts. As alluded earlier, AR2 effectively balances two inherent tradeoffs in non-stationary MAB problems with AR reward structure. Firstly, it addresses the exploration-exploitation tradeoff by alternating between exploiting the superior arm and exploring the triggered arm within each epoch. Secondly, it handles the tradeoff between "remembering"-and-"forgetting", a commonly considered tradeoff in non-stationary environments [10; 11], via searching.

**Maintaining estimates of expected reward of arms.** For any round \(t\) and arm \(i[k]\), let \(_{i}(t)\) be the last round before \(t\) (including \(t\)) at which arm \(i\) is pulled, and let \(_{i}^{}(t)\) be the next round after \(t\) (excluding \(t\)) at which arm \(i\) is pulled. Let \(_{i}(t)=_{i}^{}(t)-_{i}(t)\) be the gap between two consecutive pulls of arm \(i\) around \(t\). Define \(_{i}(t)\) as the estimate of the reward of arm \(i\) at \(t\) based on the most recent observed reward of arm \(i\) (i.e., \(R_{i}(_{i}(t))\)). Via recursive updates (see Step (b)), AR2 maintains the following estimate of expected reward for each arm:

\[_{i}(t)=^{t-_{i}(t)-1}_{i}(_{i}(t)+1)= ^{t-_{i}(t)-1}\!( R_{i}(_{i}(t))).\]

**Superior arms.** The superior arm at round \(t\), denoted by \(i_{}(t)\), is one of the two most recently pulled arms, i.e., \(I_{t-1}\) or \(I_{t-2}\), that has the higher estimated reward (see (1)). We further define \(_{}(t)=_{i_{}(t)}(t)\) as the estimated reward of the superior arm at round \(t\). We remark that here, for simplicity of analysis, our definition of the superior arm only considers the two most recently pulled arms. We can in fact set the superior arm to be the one with the highest estimated reward among the \(m\) most recently pulled arms for any constant \(m 2\). A similar theoretical analysis will then yield the same theoretical guarantee, as shown in Theorem 5.2. We further verify the robustness of AR2 to the choice of \(m\) in our numerical studies, where we consider all arms as potential candidates for the superior arm, and AR2 maintains its competitive performance (see Appendix A).

**Triggered arms.** In order to adapt to changes, AR2 identifies and keeps track of a set of arms with potential that are not pulled recently. We refer to these arms as _triggered arms_ and denote their associated set by \(\). In a round \(t\), an arm \(i i_{}(t)\) gets _triggered_ if the triggering criteria in (2) is satisfied. We call the earliest such time \(t>_{i}(t)\) as the _triggering time_ of arm \(i\) and denote it as \(_{i}^{}(t)\). If arm \(i\) is triggered, it is added to the triggered set \(\). When arm \(i\) gets pulled or is chosen as the superior arm, it is removed from \(\). The right-hand side of the triggering criteria is a confidence bound constructed based on Hoeffding's Inequality (see details in Section 6 and Appendix H).

Note that at the exploration step, AR2 would only pull a triggered arm if the triggered set \(\) is non-empty. This inherently adjusts the rate of exploration in our alternation mechanism. In a slowly-changing environment (e.g., with small \(\)), the triggered set may not always include arms needing exploration, thus allowing focused exploitation of the superior arm; in a fast-changing environment, the rate of exploration can be as high as the rate of exploitation.

## 5 Regret of Algorithm Ar2

Our algorithm AR2 works for AR-1 model with any choice of parameter \((0,1)\). However, it turns out that the problem is less challenging when the future is weakly correlated with the past (i.e., when \(\) is small). Theorem 5.1 states that _any_ algorithm--including both a naive approach that continuously pulls the same arm throughout the horizon and our algorithm AR2--can achieve near-optimal performance when \(\) is too small. This is because with small \(\), (i) for any arm \(i[k]\), the steady state distribution of its expected reward \(r_{i}(t)\) concentrates around zero with high probability, and (ii) our observations of past rewards quickly deteriorate in their value of providing insights on the evolution of expected rewards in the future.

**Theorem 5.1**.: _Any non-anticipating algorithm \(\) incurs per-round steady state regret of at most \(O(}},2)\)._

In Figure 1, we compare the order of the regret upper bound in Theorem 5.1 with the order of the regret lower bound in Theorem 3.1.3 Observe that \( 0.5\) serves as a rough threshold value such that when \((0,)\), the orders of lower and upper bounds almost match each other, which suggests limited room for improvement. (That being said, our numerical studies in Appendix A in fact show that AR2 still outperforms other benchmarks even when \(\) is small.) On the other hand, as \( 1\), the gap between lower and upper bounds start to expand, implying that the problem becomes more difficult and simplistic approaches such as the naive algorithm would no longer produce satisfying performance.

In the rest of the discussion, we thereby focus on the more challenging regime (i.e., \([,1)\)), and establish a regret upper bound for AR2 when \([,1)\).

**Theorem 5.2**.: _Let \(()(()/ +1)\), and let \(\) be the AR2 algorithm that gets restarted in each epoch of length \(_{}= k^{-3}^{-3}\). If \([,1)\) and \(k()\), the per-round steady state regret of \(\) satisfies \(}_{} O(c_{0}^{2}^{2}^{2}k^{3 }(c_{0}))\,,\) where \(c_{0}=}+2(4k)}\)._

Recall that if \(\) is close to one, the regret lower bound can be characterized as \((k^{2}^{2})\). This shows that our algorithm is optimal in terms of AR parameter \(\) and stochastic rate of change \(\) (up to logarithmic factors). In Appendix E.3, we further highlight the significance of Theorem 5.2 by illustrating the evolution of upper and lower bounds, as well as the per-round regret attained by AR2, at different values of \(\) and \(\). We show that Theorem 5.2 well characterizes the performance of AR2 under different settings.

We remark that in terms of the dependency on \(k\), Theorem 5.2 requires \(k()\) mainly for the rigor of theoretical analysis, and this bound loosens as \(\) approaches one (for example, when \(=0.95\)

Figure 1: Comparison of the orders of the regret upper bound in Theorem 5.1 and the lower bound in Theorem 3.1, obtained with \(k=5,=0.2,C=0.4\). (Similar plots can be obtained for different values of \(k\) and \(\).)\(()=20.\)4 Our numerical studies in Appendix A also reveal that (i) AR2 maintains its competitive performance even when the number of arms \(k\) exceeds \(()\); and (ii) its per-round regret grows modestly with \(k\). These suggest that the assumption/dependency on \(k\) for our upper bound are artifacts of our analysis, rather than an intrinsic property of our algorithm.

## 6 Proof of Theorem 5.2

The proof of Theorem 5.2 proceeds in four steps. Step 1 introduces _distributed regret_, a novel notion that distributes instantaneous regret over subsequent rounds, enabling us to rephrase per-round steady state regret. Step 2 defines a high-probability _good event_ crucial for analyzing distributed regret. In Step 3, we leverage the good event to establish an upper bound for each round's distributed regret. Finally, Step 4 aggregates these regrets across rounds and epochs, establishing the per-round steady state regret of AR2 against the dynamic benchmark.

**Step 1: Distributed regrets.** Let \( r_{i}(t)=r^{}(t)-r_{i}(t)\) be the instantaneous regret from pulling arm \(i\) at round \(t\), where \(r^{}(t)=_{i[k]}r_{i}(t)\) is the maximum expected reward at round \(t\). Consider the expected per-round regret: \(}_{}=_{t=1} ^{T}_{}(t)=_{t=1}^{T}  r_{I_{t}}(t)\,,\) where \( r_{I_{t}}(t)=r^{}(t)-r_{I_{t}}(t)\) is the instantaneous regret incurred at round \(t\). In the following, we show that we can distribute this instantaneous regret over subsequent rounds.

Note that for any \(i[k]\), let \(_{i}^{(j)}\) denote the round at which arm \(i\) gets pulled for the \(j\)th time within an epoch. Then during the period between two consecutive pulls \(_{i}^{(j)} t<_{i}^{(j+1)}\), the only regret incurred from pulling arm \(i\) is the instantaneous regret \( r_{i}(_{i}^{(j)})\). By previous definition, we also have \(_{i}(t)=_{i}^{(j)}\), \(_{i}^{}(t)=_{i}^{(j+1)}\) and \(_{i}(t)=_{i}^{(j+1)}-_{i}^{(j)}\) for all \(_{i}^{(j)} t<_{i}^{(j+1)}\). Observe that we can decompose \( r_{i}(_{i}^{(j)})\) as follows, which then leads to the notion of _distributed regret_:

\[ r_{i}(_{i}^{(j)})=_{t=_{i}^{(j)}}^{_{i}^{(j+1)}-1} (_{i}^{(j)})^{2(t-_{i}^{(j)})}}{1+ ^{2}++^{2(_{i}^{(j+1)}-1-_{i}^{(j)})}}=_{t =_{i}^{(j)}}^{_{i+1}^{(j)}-1}(_{i}(t)) ^{2(t-_{i}(t))}}{1+^{2}++^{2(_{i}(t)-1) }}\,.\]

**Definition 6.1** (Distributed regret).: _The distributed regret of arm \(i\) at round \(t\) is defined as_

\[D_{i}(t)=((_{i}(t))}{1+^{2}++^{2 (_{i}(t)-1)}})^{2(t-_{i}(t))}\] (3)

Note that \(D_{i}(t)=0\) if arm \(i\) is the best arm at round \(_{i}(t)\). Now, let \(T_{i}\) be the total number of rounds we pull arm \(i\) in the horizon, we can rewrite the expected per-round regret as

\[}_{}=_{i=1}^{k}_{j=1}^{T_ {i}} r_{i}(_{i}^{(j)})=_{i=1 }^{k}_{i=1}^{T}D_{i}(t)=_{i=1}^{k} _{s=1}^{S}_{t=(s-1)_{}+1}^{s_{}} [D_{i}(t)]\,,\] (4)

where the second equality follows from the Definition 6.1 and the third equality follows from that AR2 proceeds in epochs of length \(_{}\), and \(S=T/_{}\) is the total number of epochs.

**Step 2: Good event and its implications.** Before proceeding, we first define a good event \((t)\) at round \(t\), which would help simplify our analysis. In principle, we say that a good event \((t)\) happens at round \(t\) if the noises within the epoch including \(t\) are not too large in magnitude. Recall from Section 2 that \(r_{i}(t)\) follows an AR-1 process with truncating boundaries: \(r_{i}(t+1)=((r_{i}(t)+_{i}(t)))\), where \((y)=\{\{y,-\},\}\). Hence, we need to first define a new noise term that shows the influence of the truncating boundary.

\[_{i}(t)=\{_{i}(t) {if }\;(r_{i}(t)+_{i}(t))[-,]\\ [(r_{i}(t)+_{i}(t))-  r_{i}(t)].\] (5)In particular, the new noise \(_{i}(t)\) satisfies the following recursive relationship: \(r_{i}(t+1)= r_{i}(t)+_{i}(t)\). We now formally define the good event \((t)\), which states that for any sub-interval \([t_{0},t_{1}]\) within the epoch including \(t\), the weighted sum of \(_{i}(t)\) satisfies a concentration inequality.

**Definition 6.2** (Good event at round \(t\)).: _We say that the good event \((t)\) occurs at round \(t\) if for every \(i[k]\), and for every sub-interval \([t_{0},t_{1}]\), where \(t-_{} t_{0}<t_{1} t+_{}\), the weighted sum of the noises \(_{i}(t)\) satisfies \(|_{t=t_{0}}^{t_{1}-1}^{t_{1}-t}_{i}(t)|< c_{0}-^{2(t_{1}-t_{0}+1)})/(1-^{2})}\,,\) where \(c_{0}=}+2(4k)}\)._

By building a connection between \(_{i}(t)\) and \(_{i}(t)\) and Hoeffding's inequality, we show Lemma 6.3, which confirms that the good event \((t)\) happens _with high probability_.

**Lemma 6.3**.: _For any \(t[T]\), we have \([^{c}(t)]()^{2}\)._

**Step 3: Distributed regret analysis.** For a given round \(t\) in the \(s\)-th epoch, we can bound its expected distributed regret by first decomposing it into two terms: \([D_{i}(t)]=[D_{i}(t)_{^{c}(t)}]+ [D_{i}(t)_{(t)}],\) where \(_{}\) is the indicator function of event \(\). We can bound the first term by applying Lemma 6.3

\[[D_{i}(t)_{^{c}(t)}] 2[ ^{c}(t)] 2()^{2}=O(^{2}^{2}).\] (6)

To bound the second term, we rely on the implications of the good event \((t)\) (presented in Appendix H.1) to show the following.

**Lemma 6.4**.: _Suppose that we apply AR2 to the non-stationary MAB problem with \([,1)\) and and \(k()\), for every arm \(i[k]\) and some round \(_{i}(t) t<_{i}^{}(t)\), where \(_{i}(t)\) and \(_{i}^{}(t)\) are two consecutive rounds at which \(i\) gets pulled, we have \([D_{i}(t)_{(t)}] O(c_{0}^{2}^{2} ^{2}k^{2}(c_{0}))\)._

The proof of Lemma 6.4 is deferred to Appendix H.2. There, we provide a bound for the nominator and denominator of \(D_{i}(t)\) respectively, conditioning on the good event \((t)\) and the value of \( r_{i}(t)\). In the proof, we critically use the implications of the good event (see Appendix H.1). Now, combining (6) and Lemma 6.4, for any round \(t\) within the \(s\)th epoch such that \(_{i}(t) t<_{i}^{}(t)\), we have

\[[D_{i}(t)]=[D_{i}(t)_{^{c}(t)}]+ [D_{i}(t)_{(t)}] O(c_{0}^{2}^{2} ^{2}k^{2}(c_{0})).\] (7)

**Step 4: Summing the distributed regrets.** Given Equations (4), (7) and \(_{}= k^{-3}^{-3}\), summing the expected distributed regrets first within an epoch, and then along the horizon yields

\[}_{}=_{i=1}^{k}_{s=1}^{S }_{t=(s-1)_{}+1}^{s_{}}[D_{i}(t)] _{i=1}^{k}S[O(^{2}())_{ }+2]=O(k^{2}()),\]

where the additional \(2\) term comes from the initialization round and the last round we pull arm \(i\) within an epoch. This concludes the proof if we plug in \(=c_{0}\). \(\)

## 7 A Real-World Case Study on Tourism Demand Prediction

In this section, we numerically demonstrate the efficacy of AR2 via a real-world case study based on a international Tourism Demand dataset for Australia . In this case study, we act as a travel agency that needs to determine which vacation package to offer, where the demand for each vacation package is highly dependent on the tourism demand during each quarter. Our case study is further complemented by a number of synthetic experiments in Appendix A, where we compare AR2 against various benchmarks designed for both stationary and non-stationary settings and again show the superior performance of AR2 in adapting to the rapidly changing environment.

**Dataset and setup.** The international tourism demand dataset , obtained from the Australian Bureau of Statistics, records the number of individual tourist arrivals to Australia from Hong Kong during each quarter between the years 1975-1989. The authors of  have fitted an AR model to the logarithms of quarterly tourist arrivals, which results in an AR-\(4\) model with a trend component5: \(r_{i}(t)=-0.01+0.32R_{i}(t-2)+0.6R_{i}(t-4)\,.\) Note that here, the number of tourist arrivals is strongly correlated with the number of arrivals four quarters before, which is likely due to seasonal patterns. The time series analysis in  also supports our assumption in Section 2 that the ARparameters are known. In this dataset, as well as many others from the real world, decision-makers are likely to have access to historical data, which enables them to fit AR processes with good precision.

Given the AR model, we consider a recommendation setting where the travel agencies have \(k=5\) vacation packages (arms) to offer to the tourists from Hong Kong, and wish to dynamically feature the package with the highest demand (reward) at each round. For each arm, we simulate its reward sequence by randomly drawing the initial reward from a uniform distribution in \(\) and simulate \(r_{i}(t)\) for a total of \(T=200\) rounds, based on the AR-4 model above. We take the stochastic rate of change to be \(=0.1\) for all arms.

**Extension of AR2 to AR-\(p\) processes.** We extend our algorithm AR2 to handle MAB problems with rewards modeled using general AR-\(p\) processes with trends; see Algorithm 2 in Appendix B.2. At a high level, the extension of AR2 uses similar techniques as described in Section 4. The key difference is that, for each arm \(i[k]\), it maintains not only an estimate \(_{i}(t)\) of the arm's reward at round \(t\), but also an estimate \(_{i}(t)\) that captures the associated estimation error. These estimates are dynamically updated based on the structure of the AR-\(p\) process with trends. The extended algorithm, akin to Algorithm 1, (i) selects the arm with the highest estimated reward \(_{i}(t)\) and (ii) triggers arms with potential, where the confidence bound in the triggering criteria now depends on the estimate of the error \(_{i}(t)\). For a comprehensive discussion of our extension, please refer to Appendix B.2.

**Results.** We compare the performance of AR2 against the two most competitive benchmarks (\(\)-greedy and a modified UCB algorithm) that we identified in synthetic experiments; see Appendix A for comparisons with the other benchmarks. The comparison is shown in Table 1. It can be seen that AR2 stands out in terms of both the regret6 and the number of times it pulls the optimal arm. In this case study, while the \(\)-greedy algorithm frequently selects the optimal arm, it still accumulates high regret due to its purely random exploration, which can lead to the selection of arms with low rewards. Similarly, the modified UCB algorithm, which leverages the knowledge of the temporal structure in its confidence bound (see description in Appendix C), doesn't perform well. Due to the rapidly changing nature of the AR-\(p\) process, modified UCB over-explores. AR2, on the other hand, prove to be effective even amidst the rapid changes introduced by more complex time series.

## 8 Extension on Learning the AR Parameter

In the previous sections, we have assumed full knowledge of the AR parameter, which as discussed in Sections 2 and 7, when we have access to past data. This section proposes an algorithm extension for when the AR parameter needs estimation. We introduce a maximum likelihood method and numerically evaluate the performance of AR2, which show that AR2 still remains competitive despite noise in the estimated AR parameter.

**A Maximum Likelihood Estimator.** If the decision maker does not have prior knowledge of the AR parameter \(\), one can learn the AR parameter via Maximum Likelihood Estimation (MLE). In the first \(T_{}\) rounds of the time horizon, where \(T_{}=O(T^{})\) for some \((0,1)\), we pull one arm \(i[k]\) consecutively for a fixed arm \(i\) and observe rewards \(R_{i}(1),,R_{i}(T_{})\). We then define the maximum likelihood estimator \(=_{(0,1)}()\,,\) where \(()\) is the negative of the log-likelihood function defined in (60) of Appendix I. Observe that here we cannot directly apply linear regression for estimating \(\) because the existence of truncating boundaries would lead to a biased estimator. MLE, on the other hand, remains robust even when the expected reward of our arm hits the boundary. Since the number of rounds used for estimation \(T_{}\) scales sublinearly with \(T\), the regret incurred within the first \(T_{}\) rounds would not impact our per-round regret upper bound. If we have heterogeneous AR parameters \(_{i}\) for arms \(i[k]\) (as in Appendix A), one can simply pull each arm \(i[k]\) consecutively for \(T_{}\) rounds and perform MLE for each \(_{i}\).

The following proposition quantifies the amount of noise in the estimated AR parameter.

**Proposition 8.1**.: _Let \(=_{(0,1)}()\) be the estimated AR parameter. Then, for \(>0\), with probability at least \(1-2/T_{}^{}-2(-T_{}V^{2}/2^{2})\), there exists constants \(L_{1},L_{2}\) such that_

   Algorithm & normalized regret & \# optimal arms \\  AR2 & 0.26 (0.14) & 142.04 (15.71) \\ UCB-mod & 0.60 (0.27) & 106.62 (7.12) \\ \(\)-greedy & 0.38 (0.16) & 133.83 (12.47) \\   

Table 1: Performance comparison.

\(|-|(4L_{1})/(VL_{2})}/T_{}}\,,\) where \(V\) is the variance of the observed reward in the steady state distribution, which is independent of our algorithm or the time horizon \(T\)._

We plot the normalized regret incurred by AR2, \(\)-greedy and mod-UCB using the estimated AR parameters, obtained through MLE with \(T_{}=\{25,50,100\}\). We observe that whenever the AR parameters \(_{i}\) are small or large, the performance of AR2 outcompetes the other two benchmarks. In particular, AR2 appears to be robust to the noises in the estimated AR parameter, with performance close to what we would otherwise obtain with the accurate AR parameters (see Table 2 in Appendix A).

## 9 Conclusion and Future Directions

In this paper, we studied a non-stationary MAB problem with an AR structure, which captures the rapid changes commonly observed in real-world dynamics. Our proposed algorithm, AR2, leverages our knowledge or estimate of the temporal dependency to effectively handle the challenges associated with the fast-changing environment, and our techniques can be potentially adapted to more complex temporal series. As the realm of non-stationary bandits with rewards governed by temporal structures remains largely unexplored, there are several exciting avenues for future research. One intriguing direction is to incorporate seasonality into our framework, exploring models such as seasonal ARIMA with both long-term and short-term changes. Additionally, building on the promising numerical results in Section 8, it would be interesting to theoretically characterize the performance of our algorithm when estimation of the AR parameter and online-decision making are performed simultaneously.