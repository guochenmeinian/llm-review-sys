# Differentially Private Decoupled Graph Convolutions

for Multigranular Topology Protection

 Eli Chien

UIUC & GaTech

ichien3@illinois.edu

ichien6@gatech.edu

&Wei-Ning Chen

Stanford University

vnchen@stanford.edu

&Chao Pan

UIUC

chaopan2@illinois.edu

&Pan Li

GaTech

panli@gatech.edu

&Ayfer Ozgur

Stanford University

aozgur@stanford.edu

&Olgica Milenkovic

UIUC

milenkov@illinois.edu

Equal contribution.

###### Abstract

Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs. Our code is publicly available2.

## 1 Introduction

Graph learning methods, such as Graph Neural Networks (GNNs) , are indispensable learning tools due to the ubiquity of graph-structured data and their importance in solving real-world problemsarising in recommendation systems , bioinformatics  and fraud detection . Graph datasets, which typically comprise records of users (i.e., node attributes) and their interaction patterns (i.e., graph topology), often contain sensitive information. For example, financial graph datasets contain sensitive financial records and transfer logs between accounts . Online review system graphs comprise information about customer identities and co-purchase records .

Given the sensitive nature of graph datasets, it is paramount to ensure that learned models do not reveal information about user attributes or interactions. Unprotected learning models can inadvertently leak information about the training data even if the data itself is not disclosed to the public . Differential privacy (DP) has been used as _the_ gold standard for rigorous quantification of "privacy leakage" of a learning algorithm, as it ensures that the output of a training algorithm remains indistinguishable from that of "adjacent" datasets . Classical DP approaches focus on DP guarantees for model weights (i.e., DP-SGD ) when only node attributes are present. This suffices to ensure DP of model predictions for standard classification settings when no graph structure is available. In such settings, the prediction of a user is merely a function of model weights and its own attributes. This assumption does not hold for graph learning methods in general, since graph convolution, by coupling node attributes and topology information, also leverages information from neighboring users at the prediction stage. Furthermore, in practice, there are usually different privacy level requirements for node attributes and graph topology. For example, customer identities can be more sensitive compared to co-purchase records in an online review system. A fine-grained trade-off between graph topology privacy, node attribute privacy, and GNN utility is a necessary consideration overlooked by prior literature. These require rethinking how to achieve adequate DP for GNNs.

Our contributions.We perform a formal analysis of Graph Differential Privacy (GDP), ensuring that both the GNN weights and node predictions are DP. The key idea of GDP is to protect the privacy of all nodes at the prediction step except those nodes whose labels are to be predicted, as users who want to know their own predictions must have access to their own data. While this requirement sounds straightforward, it leads to challenges in the analysis due to the interaction between the graph structure and node attributes. To account for different degrees of graph topology and node attribute privacy, we introduce the notion of \(k\)-neighbor-level adjacency which generalizes the notion of graph dataset adjacency (see Figure 1 (a-c)). It not only unifies previous edge- and node-level adjacency

Figure 1: Top: (a) Illustration of a training graph dataset. In the example, the graph involves \(6\) nodes and does not contain self-loops. Nodes \(5\) and \(6\) are left unlabeled in the training dataset \(\). (b) An illustration of our novel notion of \(k\)-neighbor-level graph dataset adjacency, with/without node attributes privacy. Red colors indicate entries that are to be replaced in the adjacent dataset \(^{}\) with respect to a node \(r\). (c) All possible combinations of graph topology and node attribute privacy requirements under our \(k\)-neighbor definition. Bold letters indicate settings not covered by prior literature. Bottom: Illustration of a (d) standard graph convolution and (e) our decoupled graph convolution design. For decoupled graph convolution, we concatenate the node embedding \(\) with node feature \(\) to obtain the final prediction. See Figure 2 for a more detailed description of the DPDGC model. Note that the required noise for standard graph convolution is independent of \(k\), and hence cannot leverage the intrinsic privacy-utility trade-off in \(k\)-neighbor level adjacency.

definitions [15; 16] but also allows for different granularities of privacy protection for the graph topology. Our notion of \(k\)-neighbor-level adjacency can be used to establish the trade-offs among graph topology privacy, node attribute privacy, and model utility.

The GDP analysis to follow demonstrates that the standard graph convolution operation has two fundamental drawbacks. First, it can be shown that the required DP noise level for a standard graph convolution _does not_ decay even when there are no privacy constraints on the graph topology. Hence, the standard graph convolution design fails to exploit the fine-grained trade-off pertaining to graph topology privacy. Second, the required DP noise variance for standard graph convolutions grows at least linearly with the maximum node degree , which leads to suboptimal utility. To mitigate these drawbacks, we propose the Differentially Private Decoupled Graph Convolution (DPDGC) design that provably enables the aforementioned trade-off and makes DP noise variance _independent_ of the maximum node degree. Our key idea is to prevent direct neighborhood aggregation of (potentially transformed) node features so that the GDP guarantee can be improved via the DP composition theorem (see Figure 1 (e)). This insight sheds new light on the benefits of decoupled graph convolutions, and may lead to further advances in GDP-aware designs. We conclude by demonstrating excellent privacy-utility trade-offs of DPDGC for different GDP settings via extensive experiments on seven node classification benchmarking datasets and synthetic datasets generated using the contextual stochastic block model (cSBM) [18; 19].

Missing proofs and details are relegated to the Appendix due to space limitations.

## 2 Related works

**DP for neural networks and classical graph privacy analysis.** Providing rigorous privacy guarantees for ML methods is a problem of significant interest [20; 21; 22; 23], where DP gradient descent algorithms and their variants were studied in . On the other hand, classical graph analysis with DP guarantees has been extensively studied previously. For example, problems of releasing graphs or their statistics with DP guarantees were examined in  and . In a different setting,  investigated the problem of estimating degree distributions with DP guarantees, while  studied DP PageRank algorithms. The interested reader is referred to the survey  for a more comprehensive list of references.

**DP-GNNs.** Several attempts were made to establish formal GNN privacy guarantees via DP.  propose a strategy achieving edge DP by privatizing the graph structure (i.e., perturbing the topology) before feeding it into GNNs.  describe a node DP approach for training general GNNs via extensions of DP-SGD, while  suggests to combine DP PageRank with DP-SGD GNN training. These approaches can only guarantee DP model weights and fail to provide DP model predictions.  study GNNs for graph classification instead of node classification.  propose node-level private GNNs via the Private Aggregation of Teacher Ensembles framework , which is a different setting than ours.  are the first to point out the importance of ensuring DP of GNN predictions, and to propose the GAP model for this purpose. Still, their work did not include a formal GDP analysis nor a study of the benefits of decoupled graph convolution designs.

## 3 Preliminaries

**Notation.** We reserve bold-font capital letters (e.g., \(\)) for matrices and bold-font lowercase letters (e.g., \(\)) for vectors. We use \(_{i}\) to denote the \(i^{th}\) row of \(\), \(_{ r}\) to denote the submatrix of \(\) excluding its \(r^{th}\) row and \(_{ij}\) to denote the entry of \(\) in the \(i^{th}\) row and \(j^{th}\) column. Furthermore, \(=(,)\) stands for a directed graph with node set \(=[n]\) of size \(n\) and edge set \(\). For simplicity, we assume that the graphs do not have self-loops. Standardly, we use \(\) to denote the corresponding adjacency matrix. Without loss of generality, we also assume a labeling such that the first \(m\) nodes are labeled (\([m]\)) and the remaining \([n][m]\) nodes are subjects of our prediction. The feature matrix is denoted by \(^{n F}\), where \(F\) stands for the feature vector dimension. For a \(C\)-class node classification problem, the training labels are summarized in \(\{0,1\}^{m C}\), where each row of \(\) is a one-hot vector. We reserve \(\|\|\) for the \(_{2}\) norm and \(\|\|_{F}\) for the Frobenius norm. We use \(\|\) for concatenation. Throughout the paper, we let \((v;)\) stand for the (graph) learning algorithm that leverages \(\) to generate label predictions for a node \(v\) in \([n][m]\). We assume the graph model outputs both its learned weights \(\) and its label prediction \(}_{v}\) (i.e., \((}_{v},)=(v;)\)).

**Node classification.** We focus on the transductive node classification problem. The training data is of the form \(=(,,),\) and this information is reused at the inference stage. Due to data reusing, it is important to specify which node \(v\) is subject to prediction in the graph learning mechanism \((v;)\). Requiring that all of \(\) is private inevitably leads to noninformative predictions. It is therefore reasonable to only protect the information of \(\) pertaining to the node that is not subject to prediction. We refer interested readers to Appendix I for discussion on how our analysis generalizes to inductive settings.

### Differential Privacy (DP)

We start with a formal definition of \((,)\)-differential privacy (DP) .

**Definition 3.1** (Differential Privacy).: _For \(, 0\), a randomized algorithm \(\) satisfies a \((,)\)-DP condition if for all adjacent datasets \(,^{}\) that differ in one record, and all \(\) in the range of \(\),_

\[(()) e^{} ((^{}))+.\]

Note that the standard DP definition does not depend on the choice of nodes to be predicted. Yet, such a dependence is critical for the graph learning setting and constitutes the key difference between Definition 3.1 and our definition of GDP in Section 4. We also make use of Renyi DP in order to facilitate privacy accounting. A given \((,)\)-Renyi DP guarantee can be convert to a \((,)\)-DP guarantee via the conversion lemma [34; 35; 36] (see also Appendix F).

**Definition 3.2** (Renyi Differential Privacy).: _Consider a randomized algorithm \(\) that takes \(\) as its input. The algorithm \(\) is said to be \((,)\)-Renyi DP if for every pair of adjacent datasets \(\) and \(^{}\), one has \(D_{}(()||(^{})),\) where \(D_{}(||)\) denotes the Renyi divergence of order \(\):_

\[D_{}(X||Y)=(_{x Q}[( )^{}]),X PY Q.\]

## 4 Graph Differential Privacy

We provide next a formal definition of Graph Differential Privacy (GDP). Recall that in this case keeping the entire training data \(\) private (as in standard DP definition, i.e., Definition 3.1) is problematic since it inevitably leads to noninformative model predictions. This follows since in such a setting the prediction \(}_{v}\) fully depends on \(\). This is unlike the case for standard classification where the test label predictions are formed via access to additional test data that is not subject to privacy constraints. Our key idea is to protect the privacy of all but the node \(v\) being predicted, as users who query their own predictions should clearly have access to their own features and neighbors.

We start by describing our notion of \(k\)-neighbor-level adjacent graph datasets. Throughout the remainder of the paper, we use \(r\) to denote the replaced node.

**Definition 4.1** (\(k\)-neighbor-level adjacency).: _Two graph datasets \(\) and \(^{}\) are said to be \(k\)-neighbor-level adjacent, which is denoted by \(}}{}^{}\), if \(^{}\) can be obtained by replacing: 1) the \(r^{th}\) node feature \(_{r}\) with \(_{r}^{}^{d}\), 2) the \(r^{th}\) node label \(_{r}\) with \(_{r}^{}\{0,1\}^{C}\), for \(r[m]\), and 3) replacing \(k\) entries in \(r^{th}\) row and column of \(\) respectively excluding \(_{rr}\)._

Our Definition 4.1 unifies previous graph dataset adjacency notions such as edge- and node-level adjacency. For example, if we drop 1), 2) (i.e., keep the node attributes unchanged), and set \(k=1\) for replacement in a row only, our Definition 4.1 recovers that of edge-level adjacency . If \(k=n\), we recover the definition of node-level adjacency [15; 17; 38; 37]. Edge-level adjacency may be too weak of a privacy concept since it does not protect the privacy of node attributes. At the same time, node-level adjacency may also be too restrictive since it allows arbitrary replacement of an _entire_ node neighborhood. In practice, there are cases where node features and labels carry more sensitive information when compared to the graph topology. It is desirable to allow practitioners to decide the granularity of privacy for the graph structure \(\) while maintaining the privacy of \(\) and \(\). This motivates our new and extended \(k\)-neighbor-level adjacency definition. Note that the parameter \(k\) serves as a new graph-specific privacy parameter similar to, but independent of, \(\) and \(\) in DP. Our \(k\)-neighbor definition can shed light on how a private graph learning design relies on the privacy of \(\) controlled by the parameter \(k\) and is discussed in more detail in Section 5. We also provide adetailed discussion on the privacy meaning of \(k\) in Appendix J. In practice, the parameter \(k\) also offers a unique trade-off between utility and graph structure privacy while preserving the same privacy guarantees of the node features \(\) and labels \(\) (i.e., it is independent of the DP parameters \(,\)).

We now provide our formal definition of Graph Differential Privacy (GDP). Unless otherwise specified, we use the superscript \({}^{}\) to refer to entities with respect to the adjacent dataset \(^{}\).

**Definition 4.2** (Graph Differential Privacy).: _A graph model \(\) is said to be edge (node, \(k\)-neighbor) \((,)\)-GDP if for any \(v[n][m]\), for all \(}{{}}^{}\) (\(}{{}}^{}\), \(}}{{}}^{}\)), such that \(r v\), one has: \(D_{}((v;)||(v;^{}))\), where \(r\) is the index of the replaced node in the dataset pair \(,^{}\)._

The key difference between our definition and that of standard DP is that instead of requiring the Renyi divergence bound to hold for all possible \((,^{})\) pairs, we only require it to hold for pairs for which the replaced node \(r\) does not require prediction (i.e., \(r v\)). This is crucial since in this case, the Renyi divergence has to be bounded for _different sets of adjacent graph dataset pairs that depend on \(v\)_. At a high level, Definition 4.2 ensures that even if an adversary obtains the trained weights and the predictions of node \(v\), it cannot infer information about the remaining nodes.

## 5 Graph learning methods with GDP guarantees

We first perform the GDP analysis for GAP , the state-of-the-art DP-GNN with standard graph convolution design in Section 5.1. In the same section, issues with standard graph convolutions under \(k\)-neighbor GDP settings are discussed as well. We then proceed to introduce Differentially Private Decoupled Graph Convolution (DPDGC), a model with the decoupled graph convolution design that resolves all issues with GDP guarantees. DPDGC is motivated by the LINKX model  which offers excellent performance on heterophilic graphs in a non-private setting. These models are depicted in Figure 2, with the pseudocodes available in Appendix L. All missing formal statements and proofs are relegated to Appendix B- E. We also defer the analysis of the simper edge GDP scenario to Appendix G and formal GDP guarantees to Appendix H.

### GDP guarantees of GAP and issues of standard graph convolution

**GAP training and inference.** We first describe the training process of GAP depicted in Figure 2. GAP first pretrains the node feature encoder DP-MLP\({}_{X}\) separately from the DP-optimizer. The row-normalized node embedding \(^{(0)}\) is generated and cached after the pretraining of DP-MLP\({}_{X}\). Then the privatized \(L\) multi-hop results \(\{^{(l)}\}_{l=0}^{L}\) are generated by applying the PMA module . The intermediate node embedding \(\) constructed by the concatenation of \(\{^{(l)}\}_{l=0}^{L}\) is then cached.

Figure 2: Illustration of the GAP and DPDGC (top) architectures and their corresponding information flow (bottom). Green modules indicate DP-MLPs trained with a DP-optimizer . Blue modules are non-trainable modules. We use red frames to point to designs with DP guarantees (i.e., DP-Emb and PMA ). Trainable weights are denoted by \(^{(A)}\) and \(\) for the DP-Emb module. The black dashed arrow indicates modules that are pretrained separately and the outputs are cached.

Finally, a node classifier DP-MLP\({}_{f}\) is trained with input \(\). At the inference stage, node predictions are obtained by the cached embedding \(\) with the trained DP-MLP\({}_{f}\).

**Node and \(k\)-neighbor GDP.** We assume that the out-degree (column-sum) of \(\) is bounded from above by \(D\). Note that prior works  also require this assumption. To meet this constraint in practice, preprocessing of the form of graph subsampling is needed, which causes additional data distortion. The first step of proving tight GDP guarantees for GAP is to ensure the cached embedding \(\) to be DP, _except for the replaced node_\(r\). We start with describing the PMA  module:

\[^{(l)}^{n h};^{(l+1)}=(^{(l)}+),\] (1)

where \(^{n h}\) is a Gaussian noise matrix whose entries are i.i.d. zero mean Gaussian random variables with standard deviation \(s\).

**Theorem 5.1**.: _For any \(>1\) and \(^{}\) (or \(}{}^{}\)), assume the trained parameter \(^{(X)}\) of DP-MLP\({}_{X}\) in GAP satisfies \(D_{}(^{(X)}\|^{(X)^{}})_{1}\) and that both \(\), \(^{}\) have out-degree bounded by \(D\). Let the replaced node index be \(r\) and let \(_{ r}\) be the matrix \(\) with the \(r^{th}\) row excluded. Then the embedding \(\) in GAP satisfies \(D_{}(_{ r}\|_{ r}^{}) _{1}+}\)._

Sketch of the proof:We start by showing that for any \(^{}\), \(D_{}(_{ r}^{(1)}\|_{ r}^{(1)^{ }})_{1}+}\), which is done by examining the sensitivity of \([^{(0)}]_{ r}\). For simplicity, we abbreviate \(^{(0)}\) to \(\). Note that \(\|[]_{ r}-[^{ }^{}]_{ r}\|_{F}^{2}=_{i[n] \{r\}}\|[]_{i}-[^{ }^{}]_{i}\|^{2}.\) We find that there are three cases of \(i\) that contribute to a nonzero norm in the summation. Let \(N(r)\) and \(N^{}(r)\) denote the out-neighbor node sets of \(r\) with respect to \(\) and \(^{}\), respectively (i.e., \(N(r)=\{i:_{ir}=1\}\)). The three cases are: (1) \(i N(r) N^{}(r)\), (2) \(i N^{}(r) N(r)\), and (3) \(i N(r) N^{}(r)\). For case (1) and (2), \(\|[]_{i}-[^{}^{}]_{i}\|=1\) due to \(\) and \(^{}\) being row-normalized. For case (3), we have \(\|[]_{i}-[^{}^{}]_{i}\|=\|_{r}-^{}_{r} \| 2\). Since the out-degree is upper bounded by \(D\), we know that \((|N(r)|,|N^{}(r)|) D\). The worst case arises for \(|N(r) N^{}(r)|=D\) (i.e., shares common neighbors). Thus the sensitivity equals \(2\) (i.e., \(D\) of case (3)), which leads to the term \(}\) in the divergence bound. By applying DP composition theorem  and the assumption on \(^{(X)}\), we can prove that \(D_{}(_{ r}^{(1)}\|_{ r}^{(1)^{ }})_{1}+}\). For the \(L\)-hop result \(_{ r}\), one can apply DP composition theorem as part of an induction. For the case of \(k\)-neighbor-level adjacency, the worst case scenario still arises for \(|N(r) N^{}(r)|=D\) for any \(k 0\). Thus, the same result holds for the case \(k\)-neighbor-level adjacency for all \(k 0\). Note that the above analysis/result is tight, with the worst case arising when \(_{r}=-^{}_{r}\) and \((|N(r)|,|N^{}(r)|)=D\).

Regarding the assumption on \(^{(X)}\), it can be met by invoking a standard DP-optimizer result , where \(_{1}\) depends on the noise multiplier of the DP-optimizer. By using further the DP composition theorem and standard DP-optimizer results, we can conclude that the weights of the DP-MLP\({}_{f}\) module are also DP. At the inference stage, since our GDP definition requires that \(r v\) (i.e., nodes to be predicted are not subject to replacement in adjacent graph dataset pairs), one only needs to ensure that \(_{ r}\) - instead of the entire \(\) - to be DP. This establishes the node and \(k\)-neighbor GDP guarantees for GAP (Corollary H.5).

Surprisingly, the resulting divergence bound does not depend on the parameter \(k\) for the \(k\)-neighbor GDP setting. It implies that the privacy noise scale \(s\) required by GAP is the same even when \(k\) is much smaller than \(D\), or even equal to zero. At first, this result seems counter-intuitive but can be understood as follows. By replacing \(_{r}\) with an all-zeros row, it becomes impossible to get information about the \(r^{th}\) row and column of \(\) through \([]_{ r}\). Therefore, DP-GNNs based on standard graph convolution designs such as GAP cannot exploit the intrinsic privacy-utility trade-offs induced by \(k\)-neighbor-level adjacency constraints. Furthermore, the resulting divergence bound grows linearly to the value of the maximum degree \(D\). Consequently, one has to preprocess the graph so that the maximum degree is upper-bounded by a pre-defined value \(D\), which inevitably causes graph information distortion.

Based on our analysis, we observe that the issue arises from the graph convolution operation \(\), where both the graph structure (topology) \(\) and transformed node feature \(\) change simultaneously to \(^{}\) and \(^{}\) on \(^{}\). As a result, rows corresponding to case (3) contribute \(2\) to the norm bound, which indicates greater privacy leakage (sensitivity). This motivates us to decouple the graph convolution so that there are no products of \(\) and \(\) to work with, which motivates introducing our DPDGC model discussed in the next section.

**Remark 5.2**.: _While the proof of Theorem 5.1 is mainly inspired by the proof in , it has several technical differences. First, the analysis in  asserts that the **entire \(\)** is DP (see Lemma 3 in ). In contrast, we only ensure that \(_{ r}\) is DP. This is crucial as \(\|[]_{r}-[^{}^{}]_{r}\|=2D\) in the worst-case sensitivity analysis, which leads to \(O(D^{2})\) in the divergence bound. Also, while we leverage standard DP composition theorems  in the sketch of proof similar to  for the sake of simplicity, we argue in Appendix B that it is more appropriate to use our novel generalized adaptive composition theorem (Theorem B.1) for a rigorous GDP analysis._

### GDP guarantees of DPDGC and benefits of decoupled graph convolution

**DPDGC training and inference.** We first describe the training process of DPDGC depicted in Figure 2. We first pretrain the DP-Emb module separately and freeze its weights \((^{(A)},)\). Then we generate the intermediate embedding \(\) and cache it. Finally, we use \((,,)\) to train the remaining modules in an end-to-end fashion. At the inference stage, the node prediction is obtained by \((,)\) and the trained weights. See the pseudocode in Appendix L for further details.

**Node \(k\)-neighbor GDP guarantees.** For DPDGC, we only need the bounded out-degree assumption for node GDP but not \(k\)-neighbor GDP. The key idea of the GDP guarantee proof is to ensure the cached embedding \(\) to be DP, _except for the replaced node_\(r\). We start by introducing the DP-Emb module in DPDGC, which guarantees both the model weight \((^{(A)},)\) and \(_{ r}\) to be DP:

\[}^{(A)}=(^{(A) }+)=^{(A)}++,\] (2)

where \(^{(A)}^{n h}\) and \(^{h}\) are learnable weights. Here, \(^{h C}\) is a random but fixed projection head of hidden dimension \(h\), \(()\) is some nonlinear activation function, and \(^{n h}\) is a Gaussian noise matrix whose entries are i.i.d. zero mean Gaussian random variables with standard deviation \(s\). Importantly, we constraint \(^{(A)}\) to be row-normalized, which is critical in proving \(_{ r}\) DP. In what follows, we focus on privatizing \(_{ r}\), and the GDP guarantee for the overall model then follows by applying DP composition theorem .

**Theorem 5.3**.: _For any \(>1\) and \(^{}\), assume that \(D_{}((^{(A)},)||(^{(A)},)^{ })_{1}\) and both \(,\,^{}\) have out-degree bounded by \(D\). Let the replaced node index be \(r\) and let \(_{ r}\) be the matrix \(\) with the \(r^{th}\) row excluded. Then the embedding \(\) in DPDGC satisfies \(D_{}(_{ r}||_{ r}^{}) _{1}+}\)._

**Theorem 5.4**.: _For any \(}{}^{}\), assume that \(D_{}((^{(A)},)||(^{(A)},)^{ })_{1}\) and both \(\). Let the replaced node index be \(r\) and let \(_{ r}\) be the matrix \(\) with the \(r^{th}\) row excluded. For any \(>1\), the embedding \(\) in DPDGC satisfies \(D_{}(_{ r}||_{ r}^{}) _{1}+}\)._

Sketch of the proof:For any \(^{}\) (\(}{}^{}\)), we examine \(\|[^{(A)}]_{i}-[^{} ^{(A)}]_{i}\|,\ i[n]\{r\}\). The worst case row equals \(1\) for cases (1) and (2) (defined in the proof of Theorem 5.1) since \(^{(A)}\) is row-normalized. The main difference to the proof in Section 5.1 is the case (3), where \(\|[^{(A)}]_{i}-[^{} ^{(A)}]_{i}\|=0\), since \(_{i}=_{i}^{}\) for \(i N(r) N^{}(r)\). As a result, the worst case arises for \(|N(r) N^{}(r)|=\) and there are at most \(2D\) (\(k\)) rows of cases (1) and (2). As a result, the sensitivity of \(_{ r}\) is \(\) (\(\)) which leads to the term \(}(})\) in the divergence bound. Once more, our sensitivity bound can be shown to be tight, with the worst case as described above.

The key improvement, when compared to GAP, arises from using \(^{(A)}\) instead of \(\). In the case of \(^{(A)}\) being DP, the sensitivity analysis only requires one to consider the difference between \(^{(A)}\) and \(^{}^{(A)}\) of two adjacent graph datasets, \(\) and \(^{}\). In contrast, for GAP, the sensitivity analysis of \(\) requires taking _both_\(\) and \(\) into account since both can vary when the underlying dataset changes. Note that even if DP-MLP\({}_{X}\) is trained via a DP-optimizer, \(\) is not DP since it is dependent on the weights of DP-MLP\({}_{X}\) and the node feature \(\).

The assumption on \((^{(A)},)\) can be met by applying a standard DP-optimizer with a group size  of \(D+1\) (\(k+1\)), where \(_{1}\) depends on the noise multiplier of the DP-optimizer. This follows from the fact that the out-degree is bounded by \(D\) for node GDP or from the definition of \(k\)-neighbor-level adjacency, since replacing one node can affect at most \(D\) (\(k\)) neighbors. By further applying the DPcomposition theorem and the standard DP-optimizer result, we arrive at the node (\(k\)-neighbor) GDP guarantees for DPDGC (see Corollary H.2, H.3).

Compared to GAP, DPDGC requires significantly lower DP noise whenever \(k<D\). It can thus ensure a graph topology privacy-utility trade-off that is not possible with GAP. Furthermore, the divergence bound for DPDGC is independent of the maximum degree within the \(k\)-neighbor-level adjacency setting. Hence, DPDGC does not require preprocessing the graph, which alleviates the issue of added graph distortion.

## 6 Experiments

We test graph learning models that can achieve GDP guarantees under various settings, including nonprivate, edge-level, \(k\)-neighbor-level (\(N_{k}\)) for \(k\{1,5,25\}\), and node-level. Note that all node GDP methods require the bounded out-degree constraint. We follow  to subsample the graph so as to satisfy this constraint.

**Methods.** In addition to DPDGC and GAP introduced in Section 5, we also test (DP-)MLP and several DP-GNN baselines that can achieve GDP guarantees, including RandEdge+SAGE  and DP-SAGE  for edge and node GDP, respectively. The RandEdge+SAGE approach privatizes the adjacency matrix directly via randomized response  and feeds it to GraphSAGE , a standard GNN backbone. The DP-SAGE approach trains the GraphSAGE model with the strategy proposed in  so that the weights are DP. To further ensure that the predictions are DP as well, we follow the strategy of  to add perturbations directly at the output layer during inference. For each GDP setting, we specify \(\) to indicate that all methods satisfy GDP with privacy budget \((,)\) according to Lemma F.1. Note that \(\) is set to be smaller than either \(}\) or \(}\), depending on the GDP setting. Addition experimental details are deferred to Appendix K.

**Datasets.** We test \(7\) benchmark datasets available from either Pytorch Geometric library  or prior works. These datasets include the social network **Facebook**, citation networks **Cora** and **Pubmed**, Amazon co-purchase networks **Photo** and **Computers**, and Wikipedia networks **Squirrel** and **Chameleon**. The edge density equals \(2||/(n(n-1))\), while the formal definition of the homophily measure is relegated to Appendix K.

**Results.** We examine the performance of each model under different GDP settings, including the nonprivate case. The results are summarized in Table 2. For the edge GDP setting, we observe that DPDGC achieves the best performance on heterophilic datasets, while on par with GAP on most homophilic datasets. Surprisingly, for the node GDP setting, we find that DP-MLP achieves the best performance on four datasets. This indicates that for some datasets, the benefits of graph information cannot compensate for the utility loss induced by privacy noise that protects the graph information (for all tested private graph learning methods). As a result, achieving node GDP effectively is highly challenging and highlights the importance of investigating the \(k\)-neighbor GDP setting. For the \(k\)-neighbor GDP setting, we can see that indeed DPDGC has a much better utility for \(k=1\) and the performance gradually decreases as \(k\) grows. The required privacy noise scale is the same for GAP and DP-MLP for any \(k\): this phenomenon is discussed in more depth in Section 5. DPDGC starts to outperform DP-MLP on Photo and Computers for sufficiently small \(k\), which demonstrates the unique utility-topology privacy trade-offs of our method that cannot be achieved via models with standard graph convolution design such as GAP. However, DPDGC still underperforms compared to DP-MLP on Cora and Pubmed.

**Experiments on synthetic contextual stochastic block models (cSBMs).** In order to test our conjecture that DP-MLP can only outperform DPDGC when the graph information is "too weak," we conduct an experiment involving cSBMs , following a setup similar to that reported in .

    & Squirrel & Chameleon & Facebook & Pubmed & Computers & Cora & Photo \\  nodes & 5,201 & 2,277 & 26.406 & 19,717 & 13,471 & 2,708 & 7,535 \\ edges & 216,933 & 36,051 & 21,17,924 & 88,648 & 491,722 & 10,556 & 238,162 \\ features & 2,089 & 2,325 & 501 & 500 & 767 & 1,433 & 745 \\ classes & 5 & 5 & 6 & 3 & 10 & 7 & 8 \\ edge density & 0.0160 & 0.0139 & 0.0061 & 0.0055 & 0.0054 & 0.0029 & 0.0084 \\ homophily & 0.0254 & 0.0620 & 0.3687 & 0.6641 & 0.7002 & 0.7657 & 0.7722 \\   

Table 1: Dataset statistics. Datasets are sorted by homophily.

[MISSING_PAGE_FAIL:9]

Conclusions

We performed an analysis of a novel notion of Graph Differential Privacy (GDP), specifically tailored to graph learning settings. Our analysis established theoretical privacy guarantees for both model weights and predictions. In addition, to offer multigranular protection for the graph topology, we introduced the concept of \(k\)-neighbor-level adjacency, which is a relaxation of standard node-level adjacency. This allows for controlling the strength of privacy protection for node neighborhood information via a parameter \(k\). The supporting GDP approach ensured a flexible trade-off between utility and topology privacy for graph learning methods. The GDP analysis also revealed that standard graph convolution designs failed to offer this trade-off. To provably mitigate the problem associated with standard convolutions, we introduced Differentially Private Decoupled Graph Convolution (DPDGC), a model that comes with GDP guarantees. Extensive experiments conducted on seven node classification benchmark datasets and synthetic cSBM datasets demonstrated the superior privacy-utility trade-offs offered by DPDGC when compared to existing DP-GNNs that rely on standard graph convolution designs.