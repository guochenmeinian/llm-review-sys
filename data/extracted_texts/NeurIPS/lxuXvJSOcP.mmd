# Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection

Gyusam Chang\({}^{1}\)1 &Jiwon Lee\({}^{2}\)1 &Donghyun Kim\({}^{1}\) &Jinkyu Kim\({}^{1}\)

**Dongwook Lee\({}^{2}\) &Daehyun Ji\({}^{2}\) &Sujin Jang\({}^{2}\)2 &Sangpil Kim\({}^{1}\)2 \({}^{1}\)**

\({}^{1}\)Korea University

\({}^{2}\)Samsung Advanced Institute of Technology

{gsjang95, d_kim, jinkyukim, spk7}@korea.ac.kr

{ji1.lee, dw12.lee, derek.ji, s.steve.jang}@samsung.com

These authors contributed equally.Corresponding authors.

###### Abstract

Recent advances in 3D object detection leveraging multi-view cameras have demonstrated their practical and economical value in various challenging vision tasks. However, typical supervised learning approaches face challenges in achieving satisfactory adaptation toward unseen and unlabeled target datasets (_i.e._, direct transfer) due to the inevitable geometric misalignment between the source and target domains. In practice, we also encounter constraints on resources for training models and collecting annotations for the successful deployment of 3D object detectors. In this paper, we propose Unified Domain Generalization and Adaptation (UDGA), a practical solution to mitigate those drawbacks. We first propose Multi-view Overlap Depth Constraint that leverages the strong association between multi-view, significantly alleviating geometric gaps due to perspective view changes. Then, we present a Label-Efficient Domain Adaptation approach to handle unfamiliar targets with significantly fewer amounts of labels (_i.e._, \(1\%\) and \(5\%\)), while preserving well-defined source knowledge for training efficiency. Overall, UDGA framework enables stable detection performance in both source and target domains, effectively bridging inevitable domain gaps, while demanding fewer annotations. We demonstrate the robustness of UDGA with large-scale benchmarks: nuScenes, Lyft, and Waymo, where our framework outperforms the current state-of-the-art methods.

## 1 Introduction

3D Object Detection (3DOD) is a pivotal computer vision task in various real-world applications such as autonomous driving and robotics. Recent progress in 3DOD  have showcased remarkable advancements, primarily due to the large-scale benchmark datasets  and the introduction of multiple computer vision sensors (_e.g._, LiDAR, multi-view cameras, and RADAR). Among these, camera-based multi-view 3DOD  has drawn significant attention for its cost-efficiency and rich semantic information. However, a significant challenge remains largely unexplored: accurately detecting the location and category of objects in the presence of distributional shifts between the source and target domains (_i.e._, data distributional gaps between the training and the testing datasets).

To successfully develop and deploy Multi-view 3DOD models, we need to solve two practical problems: (1) the geometric distributional shift across different sensor configurations, and (2) the limited amount of resources (_e.g._, insufficient computing resources, expensive data annotations). The first problem poses a challenge in learning transferable knowledge for robust generalization in noveldomains. The second issue inevitably requires efficient utilization of computing resources for training and inference, as well as label-efficient development of 3DOD models in practice. To tackle these practical problems, we introduce a **U**nified **D**omain **G**eneralization and **A**daptation (UDGA) strategy, which addresses a series of domain shift problems (_i.e._, learning domain generalizable features significantly improves the quality of parameter- and label-efficient few-shot domain adaptation).

Prior studies aim to learn domain-agnostic knowledge alleviating domain shifts from drastic view changes in cross-domain environments. DG-BEV  disentangles the camera intrinsic parameters and trains the network with a domain discriminator for view-invariant feature learning. Similarly, PD-BEV  renders implicit foreground volumes and suppresses the perspective bias leveraging semantic supervision. However, these approaches struggle to capture optimal representations, highlighting that there is still room for improvements in novel target domains (_i.e._, up to -50.8\(\%\) Closed Gap compared to Oracle). To tackle these drawbacks, we first advocate a Multi-view Overlap Depth Constraint that leverages occluded regions between adjacent views, which serve as notable triangular clues to guarantee geometric consistency. This approach effectively addresses perspective differences between cross-domain environments by directly penalising the corresponding depth between adjacent views, and shows considerable generalization capacity (up to +75.8\(\%\) Closed Gap compared to DT).

Nevertheless, the development of algorithms running on edge devices (_i.e._, autonomous vehicles) faces the challenge of limited resources, which requires efficient utilization of computing systems. To resolve these challenges, we carefully design a _go-to_ strategy, Label-Efficient Domain Adaptation, that bridges two different domains with cost-effective transfer learning. Precisely, motivated by Parameter-Efficient Fine-Tuning (PEFT) [16; 17; 18], we focus on smooth adaptation to target domains by fully exploiting well-defined source knowledge. Specifically, leveraging plug-and-play extra parameters, we substantially adapt to target domains while retaining information from the source domain (+14\(\%\) Average gain compared to DT+Full FT as shown in Fig. 1). As a result, we note that UDGA practically expand base models, efficiently boosting overall capacity under limited resources.

Given landmark datasets in 3DOD, nuScenes , Lyft  and Waymo , we validate the effectiveness of our UDGA framework for the camera-based multi-view 3DOD task. Notably, we achieve state-of-the-art performance in cross-domain environments and demonstrate the component-wise effectiveness through ablation studies. To summarize, our main contributions are as follows:

* We introduce the Unified Domain Generalization and Adaptation (UDGA) framework, which aims to learn generalizable geometric features and improve resource efficiency for enhanced practicality in addressing distributional shift alignments.
* We advocate depth-scale consistency across multi-view images to effectively address 3D geometric misalignment problems. To this end, we leverage the corresponding triangular cues between adjacent views to seamlessly bridge the domain gap.
* We present a label- and parameter-efficient domain adaptation method, which requires fewer annotations and fine-tuning parameters while preserving source-domain knowledge.
* We demonstrate the effectiveness of UDGA on multiple challenging cross-domain benchmarks (_i.e._, Lyft \(\) nuScenes, nuScenes \(\) Lyft, and Waymo \(\) nuScenes). The results show that UDGA achieves a new state-of-the-art performance in Multi-view 3DOD.

Figure 1: Comparison of performance in both source and target domains (Tab. 6). Here, “Average” (orange dots) refers to mean NDS in both the source and target domains. We draw comparisons with prior methods CAM-Conv , DG-BEV  and PD-BEV  offering an empirical lower and upper bounds, DT and Oracle. Note that we only use 5\(\%\) of the target label for Domain Adaptation.

Related Work

### Multi-view 3D Object Detection

3D object detection [4; 19; 1; 20; 21; 22; 23; 24; 25; 26] is a fundamental aspect of computer vision tasks in the real world. Especially, Multi-view 3D Object Detection leveraging Bird's Eye View (BEV) representations [11; 12; 8] have rapidly expanded. We observe that this paradigm is divided into two categories: (i) LSS-based [27; 11; 12], and (ii) Query-based [8; 28; 10]. The former adopts explicit methods leveraging depth estimation network, and the latter concentrates on implicit methods utilizing the attention mechanism of Transformer . Recently, these methods [9; 30; 31] significantly benefit from improved geometric understanding leveraging temporal inputs. Also, methods [32; 33; 34; 35] that directly guide the model using the LiDAR teacher model significantly encourage BEV spatial details. In particular, this approach is being adopted to gradually replace LiDAR in real-world scenarios; however, it still suffers from poor generalizability due to drastic domain shifts (_e.g._, weather, country, and sensor). To mitigate these issues, we present a novel paradigm, Unsupervised Domain Generalization and Adaptation (UDGA), that effectively addresses geometric issues leveraging multi-view triangular clues and smoothly bridge differenet domains without forgetting previously learned knowledge.

### Bridging the Domain Gap for 3D Object Detection

Due to the expensive cost of sophisticated sensor configurations and accurate 3D annotations for autonomous driving scenes, existing works strive to generalize 3D perception models in various data distributions. Specifically, they often fail to address the covariate shift between the training and test splits. To bridge the domain gap, existing approaches have introduced noteworthy solutions as below.

**LiDAR-based.** Wang _et al._ introduced Statistical Normalization (SN) to mitigate the differences in object size distribution across various datasets. ST3D  leveraged domain knowledge through random object scale augmentation, and their self-training pipeline refined the pseudo-labels. SPG  aims to capture the spatial shape, generating the missing points. 3D-CoCo  contrastively adjust the domain boundary between source and target to extract robust features. LiDAR Distillation  generates pseudo sparse point sets in spherical coordinates and aligns the knowledge between source and pseudo target. STAL3D  effectively extended ST3D by incorporating adversarial learning. DTS  randomly re-sample the beam and aim to capture the cross-density between student and teacher models. CMDA  aim to learn rich-semantic knowledge from camera BEV features and adversarially guide seen sources and unseen targets, achieving state-of-the-art UDA capacity.

**Camera-based.** While various groundbreaking methods based on LiDAR have been researched, camera-based approaches are still limited. Due to the elaborate 2D-3D alignment, not only are LiDAR-based approaches not directly applicable, but conventional 2D visual approaches [43; 44; 45; 46] cannot be adopted either. To mitigate these issues, STMono3D  self-supervise the monocular 3D detection network in a teacher-student manner. DG-BEV  adversarially guide the network from perspective augmented multi-view images. PD-BEV  explicitly supervise models by the RenderNet with pseudo labels. However, camera domain generalization methods cannot meet the performance required for the safety, struggling to address the practical domain shift in the perspective change. To narrow the gap, we introduce a Unified Domain Generalization and Adaptation (UDGA) framework that effectively promotes depth-scale consistency by leveraging occluded clues between adjacent views and then seamlessly transfers the model's potential along with a few novel labels.

### Parameter Efficient Fine-Tuning

Recent NLP works fully benefit from general-purpose Large-language Models (LLM). Additionally, they have proposed Parameter-Efficient Fine-Tuning (PEFT) [17; 16; 48; 49; 50] to effectively transfer LLM power to various downstream tasks. Specifically, PEFT preserves and exploits previously learned universal information, fine-tuning only additional parameters with a few downstream labels. This paradigm enables to notably reduce extensive computational resources, and large amounts of task-specific data and also effectively address challenging domain shifts in various downstream tasks as reported by . Inspired by this motivation, to address drastic perspective shifts between source and target domains, we design Label-Efficient Domain Adaptation that fully transfers generalized source potentials to target domains by fine-tuning only our extra modules with few-shot target data.

## 3 Methodology

### Preliminary

Multi-view 3D Object Detection is a fundamental computer vision task that involves safely localizing and categorizing objects in a 3D space exploiting 2D visual information from multiple camera views. Especially, recent landmark Multi-view 3D Object Detection models [8; 10; 9; 11; 33] are formulated as follow; \((Y,(V(I,K,T))\), where \(Y\) represents the size \((l,w,h)\), centerness \((cx,cy,cz)\), and rotation \(\) of each 3D object. Also, \(I=\{i_{1},i_{2},...,i_{n}\}^{N H W 3}\), \(K\), and \(T=[R|]\) denotes multi-view images, intrinsic and extrinsic parameters. Specifically, these models, which fully benefit from view transformation modules \(\), encode 2D visual features alongside the 3D spatial environment into a bird's eye view (BEV) representation. First, these works adopts explicit methods (BEV view transformation \(\) as shown in Eq. 1) exploiting depth estimation network. Subsequently, Detector Head modules \(\) supervises BEV features with 3D labels \(Y\) in a three-dimensional manner.

\[(I,K,T)=(F_{2d} D,K,T),\] (1)

### Domain Shifts in Multi-view 3D Object Detection

In this section, we analyze and report _de facto_ domain shift problems arising in the Autonomous Driving system. As shown in 3.1, recent works adopt camera parameters \(K\) and \(T\) as extra inputs in addition to multi-view image \(I\). As reported by , assuming that the conditional distribution of outputs for given inputs, is the same across domains, it is explained that shifts in the domain distribution are caused by inconsistent marginal distributions of inputs. To mitigate these issues, recent generalization approaches [14; 53; 47; 13; 54] often focus on covariate shift in geometric feature representation mainly due to optical changes (_i.e._, Focal length, Field-of-View, and pixel size).

This is the only part of a story. We experience drastic performance drops (up to -54\(\%\) / -67\(\%\) performance drop in NDS and mAP, respectively, as shown in Fig 2 (b)) from non-intrinsic factors (_i.e._, only extrinsic shifts). Especially, we capture a phenomenon wherein the actual depth scale from an ego-vehicle's visual sensor to an object (Fig 2 (a) red boxes) varies depending on the sensor's installation location. Followed by Pythagorean theorem, as the height difference \( h\) increases, the depth scale difference \( d\) also increases accordingly. Note that this is not limited to height solely; any shifts in deployment translation (_e.g._, along the x, y, or z axis) lead to changes in actual depth scale. As a result, perspective view differences significantly hinder the model's three-dimensional geometric understanding by causing depth inconsistency. To address above drawbacks, we introduce a novel penalising strategy that effectively boost depth consistency in various camera geometry shifts.

Figure 2: (a) An illustration of multi-view installation translation difference. The first (_i.e._, source) and second (_i.e._, target) rows are two perspective views of the same scene captured from different installation points. The translation gap between these views is substantial, approximately 30\(\%\). (b) Source trained network shows poor perception capability in target domain, primarily due to extrinsic shifts. In \(\)Height, mAP and NDS have dropped up to -67\(\%\) compared to source. Note that we simulate the camera extrinsic shift leveraging CARLA  (refer to Appendix A for further details).

### Multi-view Overlap Depth Constraint

**Motivation.** Recently, previous efforts [55; 14; 54; 56] augment multi-view images to generalize challenging perspective view gaps. However, these strategies suffer from poor generalizability in cross-domain scenarios, primarily due to the underestimated extent of view change between different sensor deployments as reported in section 3.2. To alleviate perspective gaps, we introduce Multi-view Overlap Depth Constraint, effectively encouraging perspective view-invariant learning. Here, we start from three key assumptions: First, perspective shifts between adjacent cameras in multi-view modalities are non-trivial and varied, closely akin to those observed in cross-domains (_e.g._, nuScenes \(\) Lyft). Second, visual odometry techniques such as Structure from Motion (SfM) and Simultaneous Localization and Mapping (SLAM) often benefit from improved depth consistency through relationships between adjacent views (_e.g._, relative pose estimation). Third, in multi-view modalities, overlap regions serve as strong geometric triangular clues, seamlessly bridging between adjacent views. However, under conditions where camera parameters are input, off-the-shelf pose estimation [57; 58; 59; 60; 61] leads to ambiguity in learning precise geometry. To mitigate these issues, we introduce a novel depth constraint (Fig. 3 (i)) with overlap regions between adjacent cameras.

**Approach.** To achieve generalized BEV extraction, we directly constrain depth estimation network from adjacent overlap regions between multi-view cameras. Also, we advocate that multi-frame image inputs substantially complement geometric understanding in dynamic scenes with speedy translation and rotation shifts. To this end, we formulate corresponding depth \(D^{*}\) leveraging spatial and temporal adjacent views. First, we calculate overlap transformation matrices \(T_{i j}\) from Eq. 2.

\[D^{*}_{i j}p^{*}_{i j} K_{j}(T^{-1}_{j})T_{i}(K^{-1}_{i})D_{i}p_{i},\] (2)

where \(K\) and \(T\) are the intrinsic and extrinsic camera parameters. \(p^{*}_{i j}\) and \(p_{i}\) denote corresponding pixels between adjacent views and \(D\) represent depth prediction. Then, we directly penalise unmatched corresponding depth \(D^{*}\) to smoothly induce perspective-agnostic learning as follow Eq. 3

\[_{ov}=_{(i,j)}d(D_{j},D^{*}_{i j}),\] (3)

where \(d\) represents Euclidean Distance. Also, we observe that the photometric reprojection error significantly alleviate relative geometric ambiguity. Especially, slow convergence may occur mainly

Figure 3: An overview of our proposed methodologies. Our proposed methods comprise two major parts: (i) Multi-view Overlap Depth Constraint and (ii) Label-Efficient Domain Adaptation (LEDA). In addition, our framework employs two phases (_i.e._, pre-training, and then fine-tuning). Note that we adopt our proposed depth constraint in both phases, and LEDA only in the fine-tuning phase.

due to incorrect relationships in small overlap region (about 30\(\%\) of full resolution). To mitigates these concern, we effectively boost elaborate 2D matching, formulating \(_{p}\) as follow Eq. 4:

\[_{p}=_{(i,j)}pe(I_{j} K_{j},P_{j},I_{j} K_{j },T_{i j},P_{i j}^{*})),\] (4)

where \(P\) represents point clouds generated by \(D\), and \(pe\) is photometric error by SSIM . Also, \(\) denotes bilinear sampling on RGB images. Concretely, we take two advantages leveraging \(_{p}\) in _narrow occluded regions_; First, \(_{p}\) effectively mitigates the triangular misalignment. Second, \(_{p}\) potentially supports insufficiently scaled \(_{ov}\). Ultimately, we alleviate perspective view gaps by directly constraining the corresponding depth and the photometric matching between adjacent views.

### Label-Efficient Domain Adaptation

**Motivation.** There exist practical challenges in developing and deploying multi-view 3D object detectors for safety-critical self-driving vehicles. Each vehicle and each sensor requires its own model that can successfully operate in various conditions (_e.g._, dynamic weather, location, and time). Furthermore, while collecting large-scale labels in diverse environments is highly recommended, it is extremely expensive, inefficient and time-consuming. Among those, we are particularly motivated to tackle the following: (i) Stable performance, (ii) Efficiency of training, (iii) Preventing catastrophic forgetting, and (iv) Minimizing labeling cost. To satisfy these practical requirements, we carefully design an efficient and effective learning strategy, Label-Efficient Domain Adaptation (LEDA) that seamlessly transferring and preserving their own potentials leveraging a few annotated labels.

**Approach.** In this paper, we propose Label-Efficient Domain Adaptation, a novel strategy to seamlessly bridge domain gaps leveraging a small amount of target data. To this end, we add extra parameters \(\) consisting of bottleneck structures (_i.e._, projection down \(_{down}\) and up \(_{up}\) layers).

\[(x)=_{up}((_{down}(BN(x)))),\] (5)

where \(\) and \(BN\) indicates activation function and batch normalization. We parallelly build \(\) alongside pre-trained operation blocks \(\) (_e.g._, convolution, and linear block) in Fig. 3 (ii) and Eq. 6;

\[y=(x)+(x),\] (6)

Firstly, we feed \(x\) into \(_{down}\) to compress its shape to \([H/r,W/r]\), where \(r\) is the rescale ratio, and then utilize \(_{up}\) to restore it to \([H,W]\). Secondly, we fuse each outputs from \(\), and Adapter by exploiting skip-connections that directly link between the downsampling and upsampling paths. By doing so, these extensible modules allow to capture high-resolution spatial details while reducing network and computational complexity. Plus, it notes worthy that they are initialized by a near-identity function to preserve previously updated weights. Finally, our frameworks lead to stable recognition in both source and target domains, incrementally adapting without forgetting pre-trained knowledge.

### Optimization Objective

In this section, we optimize our proposed framework UDGA using the total loss function \(_{total}\) (as shown in Eq. 7) during both phases (_i.e._, pre-train and fine-tune). \(_{det}\) denotes the detection task loss.

\[_{total}=_{det}_{det}+_{ov}_{ ov}+_{p}_{p},\] (7)

where we grid-search \(_{det}\), \(_{ov}\) and \(_{p}\) to harmonize \(_{det}\), \(_{ov}\) and \(_{p}\). Specifically, \(_{total}\) supervises \(\) during generalization and \(\) during adaptation, respectively. As a result, these strategies enable efficient learning of optimal representations in target domains while preserving pre-trained ones.

## 4 Experimental Results

In this section, we showcase the overall performance of our methodologies on landmark datasets for 3D Object Detection: Waymo , Lyft , and nuScenes . The three datasets have different specifications; thus, we convert them to a unified detection range and coordinates for accurate comparison. We also adopt only seven parameters to achieve consistent training results under the same conditions: the location of centerness \((x,y,z)\), the size of box \((l,w,h)\), and heading angle \(\). Additionally, we summarize 3D Object Detection datasets and implementation details in Appendix A.

### Evaluation Metric

In this paper, following DG-BEV  evaluation details, we adopt the alternative metric \(^{}\) (as shown in Eq. 8) that aggregates mean Average Precision (mAP), mean Average Translation Error (mATE), mean Average Scale Error (mASE), and mean Average Orientation Error (mAOE).

\[^{}=[3+_{} (1-(1,))]\] (8)

We reconstruct the unified category for Unified Domain Generalization and Adaptation as follows: the 'car' for nuScenes and Lyft, and the'vehicle' for Waymo. Furthermore, we only validate performance in the range of \(x\), \(y\) axis from -50m to 50m. Note that we offer an empirical lower bound _Direct Transfer_ (_i.e._, directly evaluating the model pre-trained in the source domain only), and an empirical upper bound _Oracle_ (_i.e._, evaluating the model fully supervised in the target domain). We report **Full F.T.** (_i.e._, fine-tuning all parameters from the pre-trained source model) and **Adapter** (_i.e._, parameter efficient fine-tuning without our proposed depth constraint methods from the pre-trained source model) Furthermore, we formulate **Closed Gap**-representing the hypothetical closed gap by

\[=_{}-_{}}{_{}-_{}} 100\%.\] (9)

### Experiment Results

**Performance Comparison in Domain Generalization.** As shown in Tab. 1, we showcase four challenging generalization scenarios, and quantitatively compare our proposed methodology with existing state-of-the-art methods, which include CAM-Conv , Single-DGOD , DG-BEV , and PD-BEV . Here, we observe that these methods still struggle to fully pilot geometric shifts from perspective changes in cross-domain scenarios. Importantly, in Lyft \(\) nuScenes, existing methods suffer from the orientation error mainly due to significantly different ground truth directions (_i.e._, only recovering 0.198 mAOE). In nuScenes \(\) Waymo (_i.e._, one of the most challenging

   Task & Method & NDS\({}^{}\)\(\) & mAP\(\) & mATE\(\) & mASE\(\) & mAOE\(\) & Closed Gap\(\) \\   & _Oracle_ & 0.587 & 0.475 & 0.577 & 0.177 & 0.147 & \\   & _Direct Transfer_ & 0.213 & 0.102 & 1.143 & 0.239 & 0.789 & \\  & CAM-Convs  & 0.181 & 0.098 & 1.198 & 0.209 & 1.064 & -8.6\(\%\) \\  & Single-DGOD  & 0.198 & 0.105 & 1.166 & 0.222 & 0.905 & -4.0\(\%\) \\  & DG-BEV  & 0.374 & 0.268 & 0.764 & 0.205 & 0.591 & +43.0\(\%\) \\  & PD-BEV  & 0.344 & 0.263 & **0.746** & 0.186 & 0.790 & +35.0\(\%\) \\   & Ours & **0.421** & **0.281** & 0.759 & **0.183** & **0.377** & **+55.6\(\%\)** \\   & _Oracle_ & 0.684 & 0.602 & 0.471 & 0.152 & 0.078 & \\   & _Direct Transfer_ & 0.296 & 0.112 & 0.997 & 0.176 & 0.389 & \\   & CAM-Convs & 0.316 & 0.145 & 0.999 & 0.173 & 0.368 & +5.2\(\%\) \\   & Single-DGOD & 0.332 & 0.159 & 0.949 & 0.174 & 0.358 & +9.3\(\%\) \\   & DG-BEV & 0.437 & 0.287 & 0.771 & 0.170 & 0.302 & +36.3\(\%\) \\   & PD-BEV & 0.458 & 0.304 & 0.709 & 0.169 & 0.289 & +41.8\(\%\) \\   & Ours & **0.487** & **0.324** & **0.709** & **0.162** & **0.180** & **+49.2\(\%\)** \\   & _Oracle_ & 0.587 & 0.475 & 0.577 & 0.177 & 0.147 & \\   & _Direct Transfer_ & 0.133 & 0.032 & 1.305 & 0.768 & 0.532 & \\   & CAM-Convs & 0.215 & 0.038 & 1.308 & 0.316 & 0.506 & +18.1\(\%\) \\   & Single-DGOD & 0.007 & 0.014 & 1.000 & 1.000 & 1.000 & -27.8\(\%\) \\   & DO-BEV & 0.472 & 0.303 & 0.689 & **0.218** & 0.171 & +74.7\(\%\) \\   & Ours & **0.477** & **0.326** & **0.684** & 0.263 & **0.168** & **+75.8\(\%\)** \\   & _Oracle_ & 0.649 & 0.552 & 0.528 & 0.148 & 0.085 & \\   & _Direct Transfer_ & 0.178 & 0.040 & 1.303 & 0.265 & 0.790 & \\    & CAM-Convs & 0.185 & 0.045 & 1.301 & 0.253 & 0.773 & +1.5\(\%\) \\    & Single-DGOD & 0.164 & 0.034 & 1.305 & 0.262 & 0.855 & -3.0\(\%\) \\    & DG-BEV & 0.415 & 0.297 & 0.822 & **0.216** & 0.372 & +50.3\(\%\) \\    & Ours & **0.459** & **0.349** & **0.754** & 0.289 & **0.250** & **+59.7\(\%\)** \\   

Table 1: Comparison of Domain Generalization performance with existing SOTA techniques. The **bold** values indicate the best performance. Note that all methods are evaluated on ‘car’ category.

scenarios due to the rear camera drop), previous approaches still show a significant gap compared to _Oracle_ (_i.e._, -49.7\(\%\) Closed Gap). In this paper, our novel depth constraint notably addresses these issues, outperforming existing SOTAs (especially, up to +4.7\(\%\) NDS and +12.6\(\%\) Closed Gap better than DG-BEV in Lyft \(\) nuScenes). Especially, leveraging triangular clues to explicitly supervise occluded depth contributes significantly to improving geometric consistency compared to prior approaches . Overall, we demonstrate that our novel approaches significantly enhance perspective-invariance, featuring strong association in occluded regions between multi-views.

**Performance Comparison in UDGA.** In Tab. 2, we show that our proposed Unified Domain Generalization and Adaptation performance compared with various PEFT approaches (_i.e._, SSF , and Adapter ). SSF directly scale and shift the deep features extracted by pre-trained operation blocks, leveraging additional normalization parameters. Adapter represents sole module performance without our proposed constraint; Adapter-B, and Adapter-S denotes base, and small version, respectively.

Existing PEFT paradigms benefit from fine-tuning only extra parameters, retaining previously updated weights. However, we observe that these paradigms do not successfully adapt to the covariate shifts originated by challenging geometric differences as reported in section 3.2. More specifically, SSF and Adapter-S, which exploit a small number of parameters, begin to capture transferable representations and then marginally adapt at the 10\(\%\) data split. Also, Adapter-B leveraging 21.3M parameters provide poor adaptation capability (_i.e._, inferior to Scratch and Full FT in Lyft \(\) nuScenes 100\(\%\)).

However, our proposed strategy seamlessly adapt to target domains in 1\(\%\), and 5\(\%\), effectively bridge perspective gaps. Furthermore, our proposed strategy show superior performance gain (outperforming Scratch in Lyft \(\) nuScenes 50\(\%\), and Full FT in both Lyft \(\) nuScenes, and nuScenes \(\) Lyft 100\(\%\)), effectively adapting to novel targets. It is noteworthy that the most effective adaptation is achieved by updating extra parameters (less than 20\(\%\) of the total), which demonstrates the practicality and efficiency of our novel UDGA strategy as shown in Fig. 4. In addition, unlike Full FT, it proves that our UDGA framework stably adapts to the target without forgetting previously learned knowledge as

    &  &  & ^{+}\)\(\) mAP\(\)} \\   & & & \(1\%\) & \(5\%\) & \(10\%\) & \(25\%\) & \(50\%\) & \(100\%\) \\   & _Oracle_ & 51.7M & — & — & — & — & — & 0.587 / 0.475 \\   & Full FT FT & 51.7M & 0.476 / 0.369 & 0.515 / 0.434 & 0.547 / 0.434 & 0.577 / 0.464 & 0.590 / 0.483 & 0.610 / 0.506 \\  & SSF  & 1M & 0.245 / 0.079 & 0.294 / 0.112 & 0.360 / 0.256 & 0.374 / 0.266 & 0.421 / 0.327 & 0.439 / 0.275 \\  & Adapter-B & 21.3M & 0.465 / 0.283 & 0.481 / 0.365 & 0.511 / 0.384 & 0.558 / 0.444 & 0.569 / 0.460 & 0.581 / 0.473 \\  & Adapter-S & 8.8M & 0.326 / 0.134 & 0.372 / 0.161 & 0.444 / 0.255 & 0.465 / 0.283 & 0.509 / 0.390 & 0.538 / 0.443 \\   & Ours & 8.8M & **0.526** / **0.404** & **0.563** / **0.444** & **0.573** / **0.457** & **0.592** / **0.481** & **0.609** / **0.510** & **0.614** / **0.507** \\   & _Oracle_ & 51.7M & — & — & — & — & — & — & 0.684 / 0.602 \\   & Full FT FT & 51.7M & 0.531 / 0.390 & 0.594 / 0.473 & 0.623 / 0.513 & 0.650 / 0.549 & 0.678 / 0.587 & 0.700 / 0.615 \\    & SSF & 1M & 0.316 / 0.115 & 0.355 / 0.145 & 0.386 / 0.185 & 0.420 / 0.230 & 0.447 / 0.269 & 0.470 / 0.300 \\   & Adapter-B & 21.3M & 0.499 / 0.328 & 0.556 / 0.465 & 0.584 / 0.475 & 0.633 / 0.532 & 0.670 / 0.564 & 0.684 / 0.596 \\   & Outs & 8.8M & **0.578** / **0.462** & **0.613** / **0.506** & **0.638** / **0.537** & **0.665** / **0.572** & **0.675** / **0.586** & **0.706** / **0.626** \\   

Table 2: Comparison of UDGA performance on BEVDepth with various PEFT modules, SSF , and Adapter . We construct six different target data splits from 1\(\%\) to 100\(\%\). Additionally, \(\#\) Params denote the number of parameters for training. Note that — represents _’Do not support’_.

Figure 4: Performance relative to training parameters. The Domain Generalization task is represented in blue, while the Domain Adaptation task is divided into two stages: 1\(\%\) in gray and 100\(\%\) in red.

[MISSING_PAGE_EMPTY:9]

### Qualitative Analysis

To qualitatively analyze the effectiveness of Multi-view Overlap Depth Constraint, we present additional visualized results in Fig. 5. For accurate comparison, we conduct binary masking leveraging given sparse depth ground truths. In middle row, BEVDepth fail to perceive hard samples (_e.g._, far distant and occluded objects) in yellow boxes, mainly due to different extent of deformation relative to perspective as reported in section 3.2. We aim to tackle this problem, explicitly bridging adjacent views in various dynamic scenes. Precisely, in bottom row, we showcase distinguishable results in yellow boxes, capturing semantic details from various view deformation. As as results, we qualitatively demonstrate that our proposed method effectively encourage depth consistency and detection robustness, significantly improving geometric understanding in cross-domain scenarios.

## 5 Conclusion

**Limitations.** While our work significantly improves the adaptability of 3D object detection, it cannot guarantee seamless adaptation due to several limitations, including: (1) The performance does not match that of 3D object detection models using LiDAR point clouds. (2) Our Multi-view Overlap Depth Constraint relies on the presence of overlapping regions between images. (3) Achieving fully domain-agnostic approaches without any target labels remains challenging. As a result, it is essential to incorporate a fallback plan when deploying the framework in safety-critical real-world scenarios.

**Summary.** Multi-View 3DOD models often face challenges in expanding appropriately to unfamiliar datasets due to inevitable domain shifts (_i.e._, changes in the distribution of data between the training and testing phases). Especially, the limited resource (_e.g._, excessive computational overhead and taxing expensive and taxing data cost) leads to hinder the successful deployment of Multi-View 3DOD. To mitigate above drawbacks, we carefully design Unified Domain Generalization and Adaptation (UDGA), a practical solution for developing Multi-View 3DOD. We first introduce Multi-view Overlap Depth Constraint that advocates strong triangular clues between adjacent views, significantly bridging perspective gaps. Additionally, we present a Label-Efficient Domain Adaptation approach that enables practical adaptation to novel targets with largely limited labels (_i.e._, \(1\%\) and \(5\%\)) without forgetting well-aligned source potential. Our UDGA paradigm efficiently fine-tune additional parameters leveraging significantly fewer annotations by effectively transferring from the source to target domain. In summary, our extensive experiments in various landmark datasets(_e.g._, nuScenes, Lyft and Waymo) show that our novel paradigm, UDGA, provide a practical solution, outperforming current state-of-the-art models on Multi-view 3D object detection.

Figure 5: Qualitative depth visualizations of front view lineups in Lyft. The top row illustrates sparse depth ground truths projected from LiDAR point clouds. The middle and bottom rows are the qualitative results of BEVDepth and Ours, respectively. Yellow boxes highlight the improved depth.