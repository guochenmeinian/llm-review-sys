# A Consistency-Aware Spot-Guided Transformer for Versatile and Hierarchical Point Cloud Registration

Renlang Huang   Yufan Tang   Jiming Chen   Liang Li

College of Control Science and Engineering

Zhejiang University, Hangzhou 310027, China

{renlanghuang,tyfan,cjm,liang.li}@zju.edu.cn

Corresponding author

###### Abstract

Deep learning-based feature matching has shown great superiority for point cloud registration in the absence of pose priors. Although coarse-to-fine matching approaches are prevalent, the coarse matching of existing methods is typically sparse and loose without consideration of geometric consistency, which makes the subsequent fine matching rely on ineffective optimal transport and hypothesis-and-selection methods for consistency. Therefore, these methods are neither efficient nor scalable for real-time applications such as odometry in robotics. To address these issues, we design a consistency-aware spot-guided Transformer (CAST), which incorporates a spot-guided cross-attention module to avoid interfering with irrelevant areas, and a consistency-aware self-attention module to enhance matching capabilities with geometrically consistent correspondences. Furthermore, a lightweight fine matching module for both sparse keypoints and dense features can estimate the transformation accurately. Extensive experiments on both outdoor LiDAR point cloud datasets and indoor RGBD point cloud datasets demonstrate that our method achieves _state-of-the-art_ accuracy, efficiency, and robustness. Our code is available at [https://github.com/RenlangHuang/CAST](https://github.com/RenlangHuang/CAST).

## 1 Introduction

Point cloud registration is a fundamental yet crucial task for a variety of 3D vision and robotic applications, such as simultaneous localization and mapping (SLAM) , object pose estimation  and structure from motion (SfM) . Aiming at aligning two partially overlapped point clouds, the typical approach involves a two-stage pipeline: data association which establishes reliable point correspondences, and pose estimation. However, establishing these correspondences has been challenging due to the noisy, irregular, non-uniform, and textureless nature of 3D point clouds.

Feature matching has long been the mainstream of data association without pose priors. Extensive research has made advances in distinctive local feature representations, ranging from hand-crafted descriptors  to recent learning-based descriptors . Although the emerging learning-based descriptors significantly improve the reliability of correspondences, the inlier ratio still falls short of what is required for robust and efficient pose estimation. Recently, coarse-to-fine matching is a thriving framework for 2D-2D , 3D-3D , and even 2D-3D  data association. It has been a consensus that Transformers stacked by alternate self-attention and cross-attention modules are effective for coarse matching, which are inspired by human visual processes. Typically, humans may first scan through the point clouds to identify and match salient landmarks across different point clouds reliably. For less salient points, the geometric relationships between them and those salient landmarks would be utilized to revisit their potential correspondences. The correspondences will eventually be established for the entire point cloud after several iterations of this process.

Unfortunately, existing coarse matching approaches tend to be sparse and loose without consideration of geometric consistency. An important reason for looseness is that global cross-attention inevitably attends to similar yet irrelevant areas, resulting in misleading feature aggregation and consequent inconsistent correspondences that undermine both robustness and accuracy. As a result, the hypothesis-and-selection pipeline such as RANSAC  is commonly used for outlier rejection, which is typically inaccurate and inefficient, especially for numerous samples with low inlier ratio. Furthermore, the sparsity necessitates the use of complicated fine matching such as optimal transport-based algorithms to establish reliable dense correspondences. Due to iterative dense matrix operations for patch-to-patch correspondences established by coarse matching, these fine matching methods are neither efficient nor scalable for real-time large-scale applications such as odometry.

To this end, we attempt to design an efficient and scalable coarse-to-fine matching network based on consistency-aware semi-dense coarse correspondences. Inspired by ASTR  for 2D feature matching, we leverage local consistency to direct the cross-attention of each point exclusively to corresponding patches of its confident neighbors, which is referred to as spot-guided cross-attention. Unlike , we propose a novel consistency-aware matching confidence criterion to sample reliable neighbors based on both feature similarity and geometric compatibility. Additionally, we design a consistency-aware self-attention module to enhance the distinctiveness of coarse feature representations via aggregation with salient nodes from the compatibility graph. Notably, both spot-guided cross-attention and consistency-aware self-attention are efficient sparse attention mechanisms.

For scalability to real-time applications such as odometry with pose priors, we propose a lightweight fine matching module allowing independent deployment without coarse matching. The scalability is credited to flexible point-to-patch local matching instead of optimal transport heavily relying on patch-to-patch correspondences. In addition, our fine matching adopts a sparse-to-dense registration pipeline, benefiting from the efficiency of sparse keypoint matching and the accuracy of dense registration. Furthermore, an efficient compatibility graph embedding module is leveraged for outlier rejection as a substitute for inefficient hypothesis-and-selection pipelines.

In summary, our main contributions are as follows:

* A consistency-aware spot-guided Transformer (CAST) with multi-scale feature fusion for much tighter coarse matching with a focus on geometric consistency.
* A spot-guided cross-attention module with a consistency-aware matching confidence criterion that can maintain local consistency without interfering with irrelevant areas.
* A consistency-aware self-attention module based on sparse sampling from the compatibility graph to enhance global consistency during feature aggregation.
* A lightweight and scalable sparse-to-dense matching module involving both sparse keypoints and dense features to achieve lower registration errors without optimal transport and hypothesis-and-selection pipelines.

## 2 Related Work

3D Feature Descriptors.Feature matching plays a crucial role in point cloud registration, enabling the establishment of reliable correspondences without pose priors. Early methods use hand-crafted descriptors based on signatures  or histograms [4; 5] to represent local geometric features. Recently, learning-based 3D descriptors have showcased greater performance than hand-crafted ones, which are usually trained in a self-supervised manner by maximizing the similarity between descriptors of true correspondences and minimizing the similarity otherwise. 3DMatch  and PerfectMatch  leverage 3D CNNs to learn local patch-wise descriptors from 3D patches converted into voxels of truncated distance function (TDF) values and smoothed density value (SDV) representations, respectively. PPFNet  extracts global context-aware patch-wise descriptors based on PointNet . FCGF  employs a sparse 3D convolutional encoder-decoder network for dense descriptor learning. SpinNet  proposes a 3D cylindrical convolution network to extract rotation-invariant patch-wise descriptors. Predator  utilizes graph convolution and cross-attention to enhance the descriptors and predict the overlapping regions for robust performance in low overlap scenarios.

3D Keypoint Detectors.Detection-based methods have been widely studied in image matching but less developed for 3D point clouds. Existing 3D keypoint detectors are mainly hand-crafted, which extract salient points based on unique geometric features such as specific curvatures  or principal directions . However, they suffer from noisy, sparse, and non-uniform real-world point clouds with large-scale transformations. Recent advances include learning-based detectors such as USIP  that predicts repeatable keypoints by minimizing a probabilistic chamfer loss, and HRegNet  that further utilizes weighted farthest point sampling to select sparse keypoints from the predicted ones for hierarchical registration. 3DFeat-Net  extracts patch-wise descriptors with saliency scores for keypoint selection in a weakly supervised manner by minimizing a weighted feature alignment triplet loss. D3Feat  adopts a fully convolutional network to predict point-wise descriptors with hand-crafted saliency scores by minimizing a self-supervised detection loss.

3D Correspondence Learning.DCP  predicts soft correspondences from learned features and estimates the pose by a differential SVD layer. IDAM  designs iterative distance-aware similarity matrix convolution for iterative pairwise matching and pose estimation. Recently, coarse-to-fine correspondence learning has been regarded as a promising approach. The pioneering work CoFiNet  exploits a group of self-attention and cross-attention for coarse feature matching and the optimal transport for fine matching. GeoTransformer  proposes a geometric structure embedding for self-attention, and the local-to-global registration (LGR) for consistent pose estimation. RoITr  improves the coarse-to-fine framework with a rotation-invariant point cloud Transformer based on point pair features, while PEAL  and DiffusionPCR  use overlap priors and diffusion models for iterative feature matching, respectively. For outlier rejection, RANSAC  remains popular despite its inefficiency. DGR  predicts correspondence-wise confidence scores via a 6D convolutional network, while PointDSC  designs a consistency-guided non-local feature embedding to sample consistent correspondences for neural spectral matching and pose estimation.

## 3 Method

In this section, we present the proposed consistency-aware spot-guided Transformer (CAST) with a lightweight sparse-to-dense fine matching module for accurate and efficient point cloud registration.

### Overview

Given two partially overlapped point clouds \(=\{_{i}^{3},i=1,,M\}\) and \(=\{_{j}^{3},j=1,,N\}\), the point cloud registration problem can be formulated as solving the optimal rigid transformation between \(,\) by minimizing the weighted sum of point-to-point errors of a predicted correspondence set \(\) with a confidence weight \(w_{k}\) for each correspondence \((_{k},_{k})\):

\[_{,}_{(_{k},_{k})}w_{k}\|_{k}+-_{k}\|_{2}^{2}, \]

where \( SO(3)\) and \(^{3}\) are the rotation and the translation between \(\) and \(\), respectively.

As depicted in Figure 1, CAST follows a coarse-to-fine feature matching and registration architecture, including a feature pyramid network, a consistency-aware spot-guided attention-based coarse matching module, and a sparse-to-dense fine matching module. We first utilize a KPConv-based fully convolutional network  to extract multi-scale features. We denote feature maps of the decoder with the size of \(1/k\) as \(^{1/k}=\{_{X}^{1/k},_{Y}^{1/k}\}\), which correspond to nodes \(^{1/k}\) and \(^{1/k}\) down-sampled from \(,\), respectively. For coarse matching, we first adopt an efficient linear cross-attention  module to enhance \(^{1/4}\). Then both _semi-dense features_\(^{1/4}\) and _coarse features_\(^{1/8}\) are fed into a consistency-aware spot-guided attention-based coarse matching module to improve the feature distinctiveness. The similarity matrix \(^{M^{} N^{}}\) between these enhanced semi-dense features \(}_{X}^{M^{} D},}_{Y} ^{N^{} D}\) is computed based on inner product: \(=}_{X}}_{Y}^{}\). Furthermore, we fed \(}_{X}\) and \(}_{Y}\) into a point-wise MLP to predict the overlap scores, which encode the likelihood of a node having a correspondence. We perform dual-softmax on \(\) to obtain the final matching scores:

\[_{ij}=_{i}^{X}_{j}^{Y}\}}{}(_{kj})_{i}\}}{}(_{ik})_{j}, \]

where \(_{i}^{X}\) and \(_{j}^{Y}\) are predicted overlap scores of the \(i\)-th node of \(^{1/4}\) and the \(j\)-th node of \(^{1/4}\), respectively. We use the mutual nearest neighbor scheme to select confident coarse correspondences.

For efficient fine matching, we extract a keypoint from the neighborhood of each semi-dense node in \(^{1/4}\), and predict its virtual correspondence in \(^{1/4}\) based on the lightweight single-head attention. Then we utilize compatibility graph embedding to predict the confidence of these keypoint correspondences as weights in Eq. 1 for initial pose estimation. Finally, a lightweight local attention module for dense points \(^{1/2}\) and \(^{1/2}\) predicts dense correspondences to refine the pose.

### Consistency-Aware Spot-Guided Attention

To tackle the sparsity and looseness of coarse matching, we focus on feature aggregation among semi-dense features \(^{1/4}\) leveraging both local and global geometric consistency. To be specific, the self-attention only attends to salient nodes sampled from a global compatibility graph, while the cross-attention only attends to nodes sampled based on local consistency, which are referred to as _consistency-aware self-attention_ and _spot-guided cross-attention_, respectively.

Preliminaries.Transformers stacked by alternate self-attention and cross-attention have showcased advanced performance in coarse feature matching. When \(D\)-dimensional features \(_{A}\) attends to \(_{B}\), the output of vanilla attention is formulated as:

\[}_{A}=(}_{A} _{Q}(_{B}_{K})^{})_{B }_{V}, \]

where \(_{Q},_{K},_{V}\) are learnable linear transformations to generate queries, keys, and values. When \(_{A},_{B}\) related to coordinates \(_{A},_{B}\) are from the same point cloud, it becomes self-attention that requires positional encoding to embed spatial information. To encode the 3D relative positions, we equip the rotary positional embedding \(}()\) with learnable weights \(_{1},,_{D/2}^{1 3}\):

\[}()=(_{1})&&\\ &&\\ &&(_{D/2}),( )=&-\\ &,^{3}. \]

When applying \(}()\) to vanilla self-attention, the output is formulated as:

\[}_{A}=(}_{A} _{Q}}(_{A})(_{B}_{K} }(_{B}))^{})_{B}_{V}. \]

Architecture.As both spot-guided cross-attention and consistency-aware self-attention are sparse attention lacking of abundant global context, we propose to enhance the semi-dense features via multi-scale feature fusion with coarse features. Hence, the architecture of our coarse matching module is designed as a sequence of blocks for attention-based multi-scale feature aggregation. For each

Figure 1: Overview of CAST. The feature pyramid network down-samples the point clouds and learns features in multiple resolutions. The coarse matching module extracts consistency-aware semi-dense correspondences via a group of alternate consistency-aware self-attention modules and spot-guided cross-attention modules with multi-scale feature fusion. Finally, the fine matching module predicts correspondences for both sparse keypoints and dense features and estimates the transformation.

block with both semi-dense features \(^{1/4}\) and coarse features \(^{1/8}\) as inputs, we first feed \(^{1/8}\) into a self-attention module (Eq. 5) and a cross-attention module (Eq. 3). Then \(^{1/4}\) and \(^{1/8}\) are fused into each other based on nearest up-sampling and distance-based interpolated down-sampling :

\[}^{1/4} =^{1/4}+(( ^{1/8})), \] \[}^{1/8} =^{1/8}+(( ^{1/4})).\]

Finally, \(}^{1/4}\) is fed into a consistency-aware self-attention module and a spot-guided cross-attention module at the end of each block. Before these sparse attention modules, we need to match the semi-dense features and evaluate the geometric consistency as a clue to select sparse yet instructive tokens. Given semi-dense features \(}_{X}^{(l)},}_{Y}^{(l)}\) in the \(l\)-th block, the matching score is formulated as:

\[_{ij}^{(l)}=\}}{}( _{kj}^{(l)})_{i}\}}{}(_{ik}^{(l)})_{j},\;^{(l)}=}_{X}^{(l)}( }_{Y}^{(l)})^{}. \]

Then the correspondence of each node can be obtained as the node from another point cloud with the highest matching score, forming a correspondence set \(^{(l)}=\{(_{i}^{},_{i}^{} ):_{i}^{}^{1/4},_{i}^{} ^{1/4}\}\). An insight about the consistency among correspondences is that the distance between two points is invariant after transformation. Hence, geometric compatibility is adopted as a simple yet effective measure of consistency , which is based on the length difference between pairwise line segments. Given a pre-defined threshold \(_{c}\), the pair-wise geometric compatibility of \(^{(l)}\) is formulated as:

\[_{ij}=[1-d_{ij}^{2}/_{c}^{2}]^{+},d_{ij}=|\|_{i}^{}-_{j}^{}\|_{2}-\|_{i}^{ }-_{j}^{}\|_{2}|. \]

The compatibility matrix \(_{c}=[_{ij}]_{M^{} N^{}}\), is also considered as the adjacency matrix of a weighted undirected graph known as the compatibility graph, where each vertex is a pair of correspondence and the edge connectivity corresponds to the compatibility between two correspondences. Intuitively, we adopt the generalized degree of a pair of correspondence in the graph as a measure of global consistency, which quantifies the connectivity of a vertex as the sum of edge weights connected to it.

Consistency-Aware Self-Attention.Intuitively, the correspondences of less salient nodes can be effectively located based on the geometric relationships between them and the salient ones. Hence, compared with global self-attention that attends to all nodes, attending to only salient nodes is more efficient and effective to encode the geometric context for matching. We propose the consistency-aware self-attention that samples sparse salient nodes to be attended to based on both geometric consistency and feature similarity. Given the correspondence set \(^{(l)}\) with a compatibility graph, we perform two-stage sampling by ranking the generalized degrees and matching scores, respectively. The first-stage graph sampling using generalized degrees can obtain sufficient consistent correspondences as proposals. The second-stage sampling based on matching scores can further obtain sparse salient nodes from these proposals. Finally, semi-dense features \(}_{X}^{(l)},}_{Y}^{(l)}\) only attend to features of salient nodes from the same point cloud for feature aggregation according to Eq. 5.

Figure 2: Illustration of consistency-aware self-attention and spot-guided cross-attention (Left), as well as visualization of the global cross-attention and spot-guided cross-attention (Right). For the left part, the green nodes are query nodes, while the red ones with correct correspondences (green dot lines) are reliable neighbors, and the blue one with a false correspondence (red dot line) is an unreliable neighbor. The self-attention (black lines) only attends to salient nodes while the cross-attention (black lines) only attends to spots (nodes within black circles).

Spot-Guided Cross-Attention.As shown in Figure 2, global cross-attention tends to aggregate features from many irrelevant regions with similar patterns, leading to false correspondences. Inspired by local consistency that the correspondences of adjacent 3D points remain close to each other, we design the spot-guided cross-attention as depicted in Figure 2. For each node \(_{i}^{S}\) such that \((_{i}^{S},_{i}^{S})^{(l)}\), we select a subset \(_{s}(_{i}^{S})\) from its neighborhood \((_{i}^{S})\) as seeds, and construct a region of interest for it as \((_{i}^{S})=_{_{i}^{S}_{s}( _{i}^{S})}(_{k}^{S})\), namely its _spot_. \(_{s}(_{i}^{S})\) selects \(_{i}^{S}\) and only its neighbors with reliable correspondences. We propose a consistency-aware matching confidence criterion to rank the neighbors, which is formulated as the product of the matching score and the normalized generalized degree in the compatibility graph. This criterion incorporates feature similarity and geometric consistency to properly measure the reliability of correspondences for seed selection. Finally, semi-dense features attend to their spots for feature aggregation according to Eq. 3. Under the guarantee of local consistency, the spots are likely to cover the true correspondences, providing guidance for feature aggregation without interfering with irrelevant areas.

### Sparse-to-Dense Fine Matching

Given a coarse correspondence set \(}=\{(_{j}^{S},_{j}^{S}):_{j}^{S }^{1/4},_{j}^{S}^{1/4}\}\) selected as mutual nearest neighbors from the final coarse matching scores (Eq. 2), we propose a lightweight sparse-to-dense fine matching module for hierarchical pose estimation without optimal transport, maintaining scalability and efficiency. For sparse matching, we first search \(k\)-nearest neighbors (kNN) of semi-dense nodes \(^{1/4}\) among dense points \(^{1/2}\) to group patches, then we use an attentive keypoint detector  to predict a repeatable keypoint with a descriptor from each patch. Each keypoint of point cloud \(\) is assigned to its nearest node, and each node \(_{j}^{S}^{1/4}\) with a correspondence in \(}\) groups a patch \((_{j}^{S})\) of keypoints via kNN. Then, a keypoint \(_{i}\) assigned to \(_{j}^{S}\) will correspond to the patch \((_{j}^{S})\), forming a pair of keypoint-to-patch correspondence. Finally, we utilize a shared single-head attention layer for each keypoint-to-patch correspondence to predict virtual correspondences for keypoints. Denote the descriptor of \(_{i}\) as \(d_{i}^{X}\), the virtual correspondence \(}_{i}\) with feature \(_{i}^{Y}\) is predicted from keypoints \(_{i_{1}},,_{i_{k}}\) with top-\(k\) descriptor similarity in \((_{j}^{S})\) as:

\[}_{i}=_{j=1}^{k}(d_{i}^{X}}_{Q}(d_{i_{j}}^{Y}}_{K})^{})_{i_{k} },_{i}^{Y}=_{j=1}^{k}(d_{i}^{X}}_ {Q}(d_{i_{j}}^{Y}}_{K})^{})d_{i_{k}}^{Y}, \]

where \(d_{i_{1}}^{Y},,d_{i_{k}}^{Y}\) are descriptors of \(_{i_{1}},,_{i_{k}}\), and \(}_{Q}\) and \(}_{K}\) are learnable weights. Inspired by PointDSC , we construct a compatibility graph \(\) (Eq. 8) of sparse keypoint correspondences \(\{(_{i},}_{i})\}\) for spatial consistency filtering via compatibility graph embedding:

\[^{(l+1)}=(}}^{(l)} _{Q}^{(l)}(^{(l)}_{K}^{(l)})^{} )^{(l)}_{V}^{(l)},\ _{i}^{(0)}=([ _{i},d_{i}^{X},}_{i},_{i}^{Y}]), \]

where \(^{(l)}\) is the correspondence-wise embedding of the \(l\)-th layer with learnable weights \(_{Q}^{(l)}\), \(_{K}^{(l)}\), \(_{V}^{(l)}\). Finally, the embedding is fed into an MLP to classify if a correspondence is an inlier. The predicted inlier confidences serve as the weights of keypoint correspondences for pose estimation formulated as Eq. 1, which can be analytically solved by weighted Kabsch algorithm . It is noteworthy that the above process is really lightweight and scalable to large-scale registration tasks.

After aligning two point clouds based on sparse matching, we propose to refine the transformation based on dense matching. We still utilize local attention (Eq. 9) to predict the correspondences of dense points \(^{1/2}\) from its neighbors in \(^{1/2}\) within a radius \(R_{d}\), and we simply set the confidence weight of a correspondence with a distance \(d\) as \(w=[1-d/R_{d}]^{+}\). By solving Eq. 1 again with both sparse and dense correspondences, we can achieve more accurate pose estimation efficiently.

### Loss Functions

Our loss function needs to supervise four modules, _i.e._, keypoint detection, coarse matching, keypoint matching, and dense registration. For keypoint detection, we utilize the probabilistic chamfer loss \(_{p}\) to minimize the distances between the closest keypoints from the source and target point clouds after alignment under the ground-truth transformation. Please refer to  for details.

Coarse Matching.Given the ground-truth coarse correspondence set \(\) with an overlap ratio \(o_{ij}\) for each correspondence \((i,j)\), we propose a spot matching loss \(_{s}\) and a coarse matching loss \(_{c}\) formulated as weighted cross entropy losses to supervise the layer-wise coarse matching scores \(^{(l)}\) (\(l=1,2,,L\)) and the final coarse matching scores \(\), respectively:

\[_{s}=-_{l=1}^{L}}o_ {ij}}_{(i,j)}o_{ij}^{(l)}_{ij}, \]

\[_{c}=-}o_{ij}}_{(i,j) }o_{ij}_{ij}-_{X}|}_{k _{X}}(1-_{k}^{X})-_{Y}|}_{k _{Y}}(1-_{k}^{Y}), \]

where \(_{X}\) and \(_{Y}\) are sets of semi-dense nodes in point clouds \(\) and \(\) without correspondences, respectively. Two nodes are considered as a pair of coarse correspondence only when their ground-truth overlap ratio is greater than 0. Assuming that the patch centered at a point \(p^{3}\) is a spherical neighborhood of radius \(r\), the overlapping ratio \(o_{ij}\) of patches centered at \(p_{i}^{S}\) and \(q_{j}^{S}\) with ground-truth translation \(^{3}\) and rotation \( SO(3)\) can be calculated by:

\[o_{ij}=^{r}(r^{2}-h^{2})dh}{4 r^{3}/3}=1-+ }{16r^{3}},\ d=\{\|p_{i}+-q_{i}\|,2r\}. \]

Keypoint Matching.As our keypoint matching module follows a three-stage pipeline including similarity calculation, correspondence prediction, and consistency filtering, it is reasonable to supervise these stages with three losses, respectively. Only valid keypoint-to-patch correspondences are supervised during training, _i.e._, the distance of the keypoint \(x\) and its closest point \(p_{x}\) in the patch \(C_{x}\) is less than a pre-defined threshold \(R_{p}>0\), and the points whose distances from \(x\) are greater than a pre-defined threshold \(R_{n} R_{p}\) form a non-empty set \(N_{x} C_{x}\). We formulate the keypoint matching loss \(_{f}\) as an InfoNCE loss  with symmetric learnable weights \(W\), which aims at maximizing the similarity between descriptors \(d_{x}\) and \(d_{p_{x}}\) of true correspondences \((x,p_{x})\) and minimizing the similarity between descriptors \(d_{x}\) and \(d_{n_{x}}\) of false correspondences \((x,n_{x}),n_{x} N_{x}\).

\[_{f}=-_{(x,p_{x},N_{x})}[^{ }Wd_{p_{x}})}{(d_{x}^{}Wd_{p_{x}})+_{n_{x} N_{ x}}(d_{x}^{}Wd_{n_{x}})}]. \]

For correspondence prediction, we adopt a \(L_{2}\) loss \(_{k}=_{(x,)}\|x+-\|_{2}\) for the predicted correspondences \(\) of keypoints \(x\) from all valid keypoint-to-patch correspondences \((x,C_{x})\). For consistency filtering, we simply utilize a binary entropy loss \(_{i}\) to supervise the confidence scores of all keypoint correspondences. The binary ground-truth label of a keypoint correspondence \((x,)\) is \(1\) if and only if it is an inlier, _i.e._, \(\|x+-\|_{2}\) is less than a threshold \(R_{f}>0\).

Dense Registration.Given the translation \(}\) and rotation \(}\) estimated by dense registration, we adopt a translation loss \(_{t}=\|}-\|_{2}\) and a rotation loss \(_{r}=\|}^{}-_{3 3 }\|_{F}\) for supervision.

Finally, we formulate our loss as \(=_{p}+_{s}_{s}+_{c} _{c}+_{f}_{f}+_{k}_{k}+_{i} _{i}+_{t}_{t}+_{r}_{r}\), where \(_{c},_{s},_{f},_{k},_{i},_{t}, _{r}\) are balancing weights.

## 4 Experiments

In this section, we evaluate our method on both outdoor LiDAR point cloud datasets KITTI , nuScenes , and the indoor RGBD point cloud dataset 3DMatch . Our network is trained using an AdamW  optimizer with a batch size of 1, an initial learning rate of 1e-4, and a weight decay of 1e-4. The step scheduler decrease the learning rate to 90% every five steps, with gradients clipped at a norm of 0.5 during back propagation. Despite the complexity of the loss function, only one stage is needed for training. Our model is trained on an NVIDIA RTX 3090 GPU with an Intel Xeon CPU @2.90GHZ for 5, 40, and 3 epochs on 3DMatch, KITTI, and nuScenes, respectively, and we set \(_{f}=_{i}=1,_{r}=20,_{t}=5,_{s}=0.1\), and \(_{c}=0.2,_{k}=1\) for KITTI and nuScenes, \(_{c}=1,_{k}=10\) for 3DMatch.

### Outdoor Scenarios: KITTI and NuScenes

KITTI  is a popular benchmark for autonomous driving. Following , we use sequences 0 to 5 for training, 6 to 7 for validation, and 8 to 10 for testing, and select only point cloud pairs at least 10m away from each other with ICP-refined  GPS localization results as ground truth. NuScenes  is another large-scale outdoor autonomous driving benchmark including 850 scenes for training and validation and 150 for testing. Following , we select each LiDAR keyframe with the second keyframe after it as a pair of point clouds. We use three metrics for evaluation : _relative translation error_ (RTE), _relative rotation error_ (RRE), and _registration recall_ (RR).

Our results on KITTI are detailed quantitatively in Table 1 and qualitatively in Figure 3. Table 1 shows that the proposed CAST outperforms various learning-based methods, including descriptor-based , coarse-to-fine correspondence-based  including the latest ones with iterative matching , a recent graph-based  and an end-to-end  baselines. Specifically, CAST achieves a RR of 100.0% and the lowest RTE of 2.5cm, which is 60.3% improvement over the _state-of-the-art_ DiffusionPCR , highlighting its superior robustness and accuracy.

As for RRE, CAST slightly underperforms some coarse-to-fine methods , primarily due to numerical errors in SVD-based pose estimation that usually produces non-orthonormal rotation matrices. RRE is set as 0 when \((}^{})>3\), a condition met frequently across all methods. Consequently, geodesic distance-based RRE may not accurately reflect the actual performance.

   Model & Publication & RTE (cm) & RRE (\({}^{}\)) & RR (\%) \\ 
3DFeat-Net & ECCV 2018  & 25.9 & 0.25 & 96.0 \\ FCGF & ICCV 2019  & 9.5 & 0.30 & 96.6 \\ D3Feat & CVPR 2020  & 7.2 & 0.30 & 99.8 \\ SpinNet & CVPR 2021  & 9.9 & 0.47 & 99.1 \\ Predator & CVPR 2021  & 6.8 & 0.27 & 99.8 \\ CoFiNet & NeurIPS 2021  & 8.2 & 0.41 & 99.8 \\ GeoTransformer & CVPR 2022  & 6.8 & 0.24 & 99.8 \\ OIF-Net & NeurIPS 2022  & 6.5 & **0.23** & 99.8 \\ PEAL & CVPR 2023  & 6.8 & **0.23** & 99.8 \\ DiffusionPCR & CVPR 2024  & 6.3 & **0.23** & 99.8 \\ MAC & CVPR 2023  & 8.5 & 0.40 & 99.5 \\ RegFormer & ICCV 2023  & 8.4 & 0.24 & 99.8 \\ CAST & & **2.5** & 0.27 & **100.0** \\   

Table 1: Registration performance on KITTI odometry dataset.

Figure 3: Qualitative registration results on KITTI dataset. We show three examples in three columns. The first two rows present the raw point clouds and highlight the 3D keypoints with low uncertainty in red. Our keypoints are typically located in sharp corners and edges of buildings, pillars, and vehicles. The third row shows the predicted sparse keypoint correspondences with high scores, while the last row presents the aligned point clouds after pose estimation. Although a few outliers colored in red have not been filtered out, their distances are acceptable for accurate registration.

For a more challenging LiDAR benchmark nuScenes, we compare CAST with both traditional [42; 45; 15] and learning-based algorithms [26; 27; 46; 31; 24] in Table 2. We do not include the coarse-to-fine methods since none have been trained or tested on nuScenes. Most of the results are borrowed from  while HRgeNet  is re-evaluated with their open source codes. Our method achieves the lowest translation error of 0.12m and the lowest rotation error of 0.20\({}^{}\) while maintaining the best RR of 99.9%, showcasing _state-of-the-art_ robustness and accuracy.

### Indoor Scenarios: 3DMatch and 3DLoMatch

Our approach is also evaluated on indoor benchmarks 3DMatch  and 3DLoMatch , which consist of point cloud pairs with overlaps \(>\)30% and \(10\% 30\%\), respectively. In Table 3, we use registration recall  as our evaluation metric, and test the runtime of all methods in Pytorch implementation with 5000 points on our device with an Intel CPU i7-12800HX@2.30GHZ and an NVIDIA RTX 3080Ti GPU for fairness, except  in Tensorflow implementation and [13; 29; 30] using the results reported in their papers [13; 30] due to the absence of source codes. Our method along with other sparse matching baselines [47; 48] directly uses all points for evaluation. To enhance the robustness in low overlapping cases, our method is combined with RANSAC estimating an initial pose from only 250 coarse correspondences to reject the outliers during fine matching.

On the 3DMatch benchmark, our method achieves _state-of-the-art_ RR of 95.2%. On the more challenging 3DLoMatch, CAST achieves a high RR of 75.1%, outperforming all descriptors and non-iterative correspondence-based methods [47; 11; 12; 28; 48; 49] except OIF-Net  using more than 1000 sampled points. As our method typically detects about 1000 sparse keypoints and establishes less than 250 keypoint correspondences on 3DLoMatch, it is fair to compare CAST with other methods using only 250 sample points. However, CAST outperforms the _state-of-the-art_ non-iterative correspondence-based methods OIF-Net  using less than 1000 points. Notably, our method achieves such superior performance only with the lowest runtime, while RANSAC remains efficient due to our high inlier ratio. Although PEAL  and DiffusionPCR  show higher RR on 3DLoMatch, their iterative feature matching with overlap priors is extremely time-consuming (10 times of ours), while PEAL even requires extra information from 2D images.

   Method & RTE (m) & RRE (\({}^{}\)) & RR (\%) \\  Point-to-Point ICP  & 0.25 & 0.25 & 18.8 \\ Point-to-Plane ICP  & 0.15 & 0.21 & 36.8 \\ FGR  & 0.71 & 1.01 & 32.2 \\ RANSAC  & 0.21 & 0.74 & 60.9 \\ DCP  & 1.09 & 2.07 & 56.8 \\ IDM  & 0.47 & 0.79 & 88.0 \\ FMR  & 0.60 & 1.61 & 92.1 \\ DGR  & 0.21 & 0.48 & 98.4 \\ HRgeNet  & 0.18 & 0.45 & **99.9** \\ CAST & **0.12** & **0.20** & **99.9** \\   

Table 2: Registration performance on nuScenes.

    &  &  &  \\   &  &  &  \\  &  & 5000 & 2500 & 1000 & 500 & 250 & 5000 & 2500 & 1000 & 500 & 250 & All \\    } & PerfectMatch  & 78.4 & 76.2 & 71.4 & 67.6 & 50.8 & 33.0 & 29.0 & 23.3 & 17.0 & 11.0 & - \\  & FCGF  & 85.1 & 84.7 & 83.3 & 81.6 & 71.4 & 40.1 & 41.7 & 38.2 & 35.4 & 26.8 & 0.271 \\  & D3Feat  & 81.6 & 84.5 & 83.4 & 82.4 & 77.9 & 37.2 & 42.7 & 46.9 & 43.8 & 39.1 & 0.289 \\  & SpinNet  & 88.6 & 86.6 & 85.5 & 83.5 & 70.2 & 59.8 & 54.9 & 48.3 & 39.8 & 26.8 & 90.804 \\  & YOHO  & 90.8 & 90.3 & 89.1 & 88.6 & 84.5 & 65.2 & 65.5 & 63.2 & 56.5 & 48.0 & 13.529 \\  & Predator  & 89.0 & 89.9 & 90.6 & 88.5 & 86.6 & 59.8 & 61.2 & 62.4 & 60.8 & 58.1 & 0.759 \\    } & REGTR  &  &  &  \\  & CoFiNet  & 89.3 & 88.9 & 88.4 & 87.4 & 87.0 & 67.5 & 66.2 & 64.2 & 63.1 & 61.0 & 0.306 \\  & GeoTransformer  & 92.0 & 91.8 & 91.8 & 91.4 & 91.2 & 75.0 & 74.8 & 74.2 & 74.1 & 73.5 & 0.192 \\  & OIF-Net  & 92.4 & 91.9 & 91.8 & 92.1 & 91.2 & 76.1 & 75.4 & 75.1 & 74.4 & 73.6 & 0.555 \\  & RoITr  & 91.9 & 91.7 & 91.8 & 91.4 & 91.0 & 74.7 & 74.8 & 74.8 & 74.2 & 73.6 & 0.457 \\  & PEAL  & 94.4 & 94.1 & 94.1 & 93.9 & 93.4 & 79.2 & 79.0 & 78.8 & 78.5 & 77.7 & 2.074 \\  & BUFFER  &  &  &  \\  & SIRA-PCR  & 93.6 & 93.9 & 93.9 & 92.7 & 92.4 & 73.5 & 73.9 & 73.0 & 73.4 & 71.1 & 0.291 \\  & DiffusionPCR  & 94.4 & 94.3 & 94.5 & 94.0 & 93.9 & **80.0** & **80.4** & **79.2** & **78.8** & **78.8** & 1.964 \\  & CAST &  &  &  \\   

Table 3: Evaluation results on indoor RGBD point cloud datasets.

### Ablation Studies

We select indoor datasets for ablation studies of coarse matching as they are more challenging. Here we evaluate the RR over the whole dataset rather than the average RR of eight sequences reported in Table 3, which is more reasonable for a dataset with significant variances of sequence lengths. Besides, we assess two extra metrics to directly measure the performance of coarse matching: _patch inlier ratio_ (PIR), the fraction of patch matches with actual overlap; and _patch matching recall_ (PMR), the fraction of point cloud pairs with PIR above 20%. Results from five experiments in Table 4 demonstrate the effects of the proposed multi-scale feature fusion (MS), spot-guided cross-attention (SG), consistency-aware self-attention (CA), and the overlap head for overlap score prediction (OV). The first experiment ablating CA and replacing SG with linear cross-attention, suffers performance degradation in all metrics due to inconsistency. The second experiment improves all metrics based on SG, while the last one achieves the best performance via CA, showcasing their effectiveness. Figure 2 visualizes the vanilla global cross-attention and our spot-guided cross-attention. Instead of interacting with many similar yet irrelevant regions for misleading feature aggregation, SG can effectively select instructive areas to attend to according to local consistency. Compared to the last experiment, the third one verifies the effectiveness of multi-scale feature fusion, while the fourth one demonstrates the necessity of overlap prediction.

Additionally, we conducted five ablation studies on KITTI for a better understanding of our fine matching, since pose errors are better metrics to reflect accuracy. The second experiment using only sparse keypoints for registration highlights the effectiveness of dense registration, while the third one shows the effect of learnable dense correspondences compared to nearest neighbors. The last three experiments report the performance of sparse registration by ablating the keypoint detector, the learnable sparse correspondences, and the compatibility graph embedding, each demonstrating their necessity for accuracy. Despite these variations, all studies maintain a 100% RR, showing the robustness of coarse matching.

## 5 Conclusion

In this paper, we present a novel consistency-aware spot-guided Transformer to achieve compact and consistent coarse matching for point cloud registration. At the coarse matching stage, our consistency-aware self-attention enhances the feature representations with sparse sampling from the geometric compatibility graph. Additionally, our spot-guided cross-attention leverages local consistency to guide the cross-attention to confident spots without interfering with irrelevant areas. Based on these semi-dense and consistent coarse correspondences, a lightweight and scalable sparse-to-dense fine matching module empowered by local attention can achieve accurate pose estimation without optimal transport or hypothesis-and-selection pipelines. Our method has showcased _state-of-the-art_ accuracy, robustness, and efficiency for point cloud registration across different 3D sensors and scenarios, which paves the way for large-scale real-time applications such as SLAM.

    & MS & SG & CA & OV &  &  \\  & & & & & PIR (\%) & PMR (\%) & RR (\%) & PIR (\%) & PMR (\%) & RR (\%) \\ 
1 & ✓ & & & & ✓ & 77.56 & 95.87 & 94.45 & 40.82 & 70.58 & 72.07 \\
2 & ✓ & & ✓ & & ✓ & 77.95 & 96.61 & 94.92 & 42.55 & 72.77 & 74.57 \\
3 & & & ✓ & ✓ & ✓ & 69.58 & 96.67 & 94.14 & 32.59 & 65.02 & 73.00 \\
4 & ✓ & ✓ & ✓ & & 73.56 & **97.17** & 95.07 & 35.25 & 68.33 & 74.91 \\
5 & ✓ & ✓ & ✓ & ✓ & **79.79** & **97.17** & **96.01** & **44.41** & **75.24** & **76.59** \\   

Table 4: Ablation studies of coarse matching modules on indoor datasets.

    & RTE (cm) & RRE (\(@math@degree\)) & RR (\%) \\  ours & **2.51** & **0.27** & **100.00** \\ ours w/o dense registration & 3.13 & 0.30 & **100.00** \\ ours w/o virtual dense corr. & 2.85 & 0.28 & **100.00** \\ ours w/o keypoint detection & 3.58 & 0.30 & **100.00** \\ ours w/o virtual sparse corr. & 3.25 & 0.30 & **100.00** \\ ours w/o graph embedding & 5.01 & 0.30 & **100.00** \\   

Table 5: Ablation studies of fine matching on KITTI.