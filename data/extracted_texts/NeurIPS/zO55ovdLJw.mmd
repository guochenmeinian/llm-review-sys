# Deep Correlated Prompting for Visual Recognition

with Missing Modalities

 Lianyu Hu, Tongkai Shi, Wei Feng, Fanhua Shang, Liang Wan

College of Intelligence and Computing, Tianjin University

{hly2021,stk,wfeng,fhshang,lwan}@tju.edu.cn

https://github.com/hulianyuyyy/Deep_Correlated_Prompting

Corresponding author

###### Abstract

Large-scale multimodal models have shown excellent performance over a series of tasks powered by the large corpus of paired multimodal training data. Generally, they are always assumed to receive modality-complete inputs. However, this simple assumption may not always hold in the real world due to privacy constraints or collection difficulty, where models pretrained on modality-complete data easily demonstrate degraded performance on missing-modality cases. To handle this issue, we refer to prompt learning to adapt large pretrained multimodal models to handle missing-modality scenarios by regarding different missing cases as different types of input. Instead of only prepending independent prompts to the intermediate layers, we present to leverage the correlations between prompts and input features and excavate the relationships between different layers of prompts to carefully design the instructions. We also incorporate the complementary semantics of different modalities to guide the prompting design for each modality. Extensive experiments on three commonly-used datasets consistently demonstrate the superiority of our method compared to the previous approaches upon different missing scenarios. Plentiful ablations are further given to show the generalizability and reliability of our method upon different modality-missing ratios and types.

## 1 Introduction

Our human beings typically perceive information of multiple modalities such as visual, linguistic and audio signals to understand the world, where different signals drawn from various perspectives are inherently complementary. Thus, modeling and coordinating multimodal information is of great value for large-scale models to reason about real-world scenarios. Recently, multimodal models [34; 1; 22; 29] have developed fast powered by the large collection of multimodal data pairs and evolution of model architectures (e.g., Transformer ), which demonstrate promising performance across a series of downstream tasks, such as cross-model retrieval [8; 18; 29; 9], image captioning [24; 1] and image/video generation [23; 22]. Supported by their large capacity and general knowledge acquired by training upon web-scale data, these models have been more and more applied to daily work in our life (e.g., GPT-4 is used by millions of people).

However, there exist two major concerns that may hinder these impressive models from broader applications. First, a common assumption of previous methods is that the input data is modality-complete, which may not always hold in the real world due to privacy considerations, collection difficulty and security issues [27; 19]. When an input modality is missing in general real-world conditions, the performance of these models usually degrades a lot (regardless of training or testing settings) , which is easily influenced by the input completeness. Second, these large models are usually parameter-abundant [1; 6; 21] and require heavy computations [30; 45] to pretrain andfinetune on downstream tasks, whose computational demands may not always be available in most real-world applications due to limited computing resources. It's necessary to develop a new method to efficiently adapt these powerful methods to perform robustly against missing-modality scenarios.

Previous works [27; 42; 32; 28; 19] of multimodal learning have considerably explored the missing-modality issues. Earlier works [27; 42; 32; 28] mostly reconstruct the absent information of missing modalities or use other modalities to augment the missing modalities. MMP  has first introduced prompt learning to handle missing-modality scenarios by regarding different missing cases as different types of input. The multimodal backbone is kept frozen and only the newly introduced prompts are updated in the fine-tuning process, thus only incurring a few extra computations. However, MMP  simply inserts independent prompts into each layer, and overlooks the relationships among prompts and input features. The prompts across different layers lack cooperation to aggregate beneficial information to well guide the model predictions. The fixed prompts are used for different input samples which fail to consider the characteristics of various inputs. The complementary multimodal information of different input modalities is overlooked in the fine-tuning process.

To better adapt large multimodal models to missing-modality scenarios, we introduce deep correlated prompting (DCP) by capturing different types of correlations between prompts and input features. Specifically, to leverage the hierarchical semantics of different layers, we propose correlated prompts by perceiving beneficial information from preceding layers to instruct the features of the current layer. We further propose to dynamically generate the prompts according to the input features to better fit the characteristics of different inputs. To leverage the complementary information of multimodal inputs, we decompose the prompts into modal-common and modal-specific parts to guide each encoder to focus on its unique features. The proposed prompts are concatenated and prepended to the input and intermediate features of the multimodal backbone to instruct the model to alleviate the performance drop caused by the missing modality. The multimodal backbone keeps frozen during training and only the learnable prompts are tuned, thus offering high training efficiency. Extensive experiments on three widely-used datasets including MM-IMDB , UPMC Food-101  and Hateful Memes  demonstrate consistently superior performance compared to other methods across all benchmarks, which verify the effectiveness of our proposed method. Ablation studies are further given to verify the generalizability and reliability of our method upon different missing-modality types and ratios.

## 2 Related Work

### Missing-Modality for Multimodal Learning.

Earlier works of missing modalities for multimodal learning mostly generate the missing modality based on other modalities , or align features of latent representations for multiple modalities to help recognition [38; 15]. Recently, multimodal transformers [1; 22; 23] emerge as an effective tool to model information from various modalities and process them into a robust representation, which have shown impressive performance over a series of tasks [8; 18; 29; 9; 24; 11; 12]. However, these methods typically assume that the inputs are modality-complete, which may not always hold in real-world scenarios. When a modality is missing, these methods usually demonstrate degraded accuracy and lead to unstable performance .

To deal with missing modalities in multimodal transformers, MMIN  predicts the intermediate features of the missing modality based on other available modalities, by learning a common multimodal representation. SMIL  proposes a Bayesian meta-learning framework to estimate the latent features of the modality-incomplete data. It further explores the effects faced with severe modality-incomplete samples (e.g., 90% missing ratio). Ma et al.  test the robustness of multimodal transformers to missing modalities and their missing types, and propose a multitask optimization framework to improve it via modality fusion. Zeng et al.  propose a tag-assisted transformer Encoder network to handle the problem of missing uncertain modalities. ShaoSpec  employs a shared head across different tasks to aggregate information from various input samples, which complements the features for missing modalities. However, these methods mostly assume that a fixed modality is missing, which is known in advance. Besides, these methods still require updating most parameters of the model, which consume heavy computations in downstream tasks. MMP  first introduced missing-aware prompts to handle scenarios with missing modalities with minimal extra computational costs. However, it simply inserts independent prompts into the intermediate features of the multimodal backbone, without considering the relationships between prompts of different layers as well as the correlations between prompts and input features. In contrast, we carefully design the prompts by exploring the correlations between prompts and input features, and achieves much superior performance across all missing-modality scenarios upon three commonly-used datasets.

### Prompt Learning

Prompt Learning is first explored in neural language processing (NLP), which adopts a "prompt" to modify the input text to instruct the pre-trained model for downstream tasks. Earlier works [6; 14] usually adopt manually designed prompts to improve the generalizability of large models over downstream tasks. Later, prompt tuning methods [26; 20; 25] emerge by prepending learnable prompts to the input features in the training phase to automate the optimization process. Recently, prompt learning is also introduced into computer vision tasks [4; 13; 37] and multimodal learning tasks [36; 44; 35; 16; 43]. CoOp  is first proposed to insert learnable soft prompts besides input images to adapt vision-language models to various vision tasks. CoCoOp  generates an image-conditional prompt to utilize the power of input features. ProGrad  only updates the prompts whose gradients are aligned to the "general knowledge" generated by the original prompts. KgCoOp  tries to align the output embeddings of the text encoder with those of the pretrained CLIP to preserve beneficial information. MaPLe  projects deep learnable soft prompts into both the image and text branches to enable prompt collaboration. DualPrompt  extends the prompts to learn different task information conditionally in continual learning. DePT  decouples base-specific knowledge from feature channels into an isolated feature space during prompt tuning. These works have demonstrated the effectiveness of prompt learning to adapt large-scale vision-language models for downstream tasks with minimal extra computation costs. In this paper, we introduce prompt learning into multimodal models to increase their robustness to missing-modality scenarios, via attaching different types of prompts according to various missing cases.

## 3 Method

### Overall framework

**Problem definition.** We first provide a succinct overview of the missing-modality scenario addressed in this paper. Without loss of generalizability, we explore multimodal inputs with \(M=2\) modalities \(m_{1}\) and \(m_{2}\) (e.g., text and image), which is naturally extensible to more modalities. Specifically, with a multimodal dataset \(D\) = {\(D^{c}\), \(D^{m_{1}}\), \(D^{m_{2}}\)}, we denote \(D^{c}\)={\(x^{m_{1}}\), \(x^{m_{2}}\), \(y\)} as the modality-complete case, where \(x\) denotes the input and \(y\) represents the label. We let \(D^{m_{1}}\)={\(x^{m_{1}}\), \(y\)} and \(D^{m_{2}}\)={\(x^{m_{2}}\), \(y\)} denote the modality-incomplete cases, wherein one modality is absent (e.g., missing text and missing image). Some examples of modality-complete and modality-incomplete inputs are depicted in the lower part of Fig. 1.

**Recap MMP .** MMP  first introduced missing-aware prompts to handle missing-modality cases. Specifically, it assigns \(2^{M}-1\) types of prompts for tasks involving \(M\) modalities (e.g., 3 prompts for vision-language tasks, including one prompt for modality-complete case, one prompt for image-only case, and one prompt for text-only case). Given an input sample, it first selects the prompts corresponding to the missing case and then prepends the prompts to the input and intermediate features of the multimodal backbone, instructing the pretrained model to perform prediction. In this procedure, only the learnable prompts are updated and the multimodal backbone keeps frozen.

### Overall framework

Though MMP  has achieved notable progress in improving the robustness of multimodal models upon missing-modality cases by incurring minimal computational costs, it only assigns independent prompts to the input and intermediate features, which (1) fails to consider the relationships between prompts of different layers, and (2) lacks the correlations between prompts and input features, and (3) overlooks the complementarity of multimodal inputs. To better adapt the pretrained multimodal model for missing-modality scenarios, we propose to design three types of missing-aware prompts by capturing the relationships between prompts and inputs. First, we generate missing-aware prompts by leveraging the correlations of preceding prompts across multiple layers and various modalities. Second, we dynamically generate the prompts for each input sample to fit its characteristics. Third,we decompose multimodal prompts into modal-common parts and modal-specific parts, which fuses beneficial information from other modalities and enables each encoder to focus its unique features.

Fig. 1 shows the framework overview of our proposed method. Specifically, without loss of generalizability, we adopt the widespread two-stream multimodal method CLIP  as our backbone. Given the input text and image, we first employ pretrained text and image embedding layers from the pretrained CLIP  to convert them into token sequences. For each input sample, we select the corresponding prompt \(P_{m}^{T}\) and \(P_{m}^{I}\) for the text encoder and image encoder, respectively, given the type of missing modality with \(m\{c,m_{1},m_{2}\}\). The prompt for each encoder is composed of three types of missing-aware prompts including the correlated prompts \(P_{m}^{T,R}\) (\(P_{m}^{I,R}\)), dynamic prompts \(P_{m}^{T,D}\) (\(P_{m}^{I,D}\)) and modal-common prompts \(P_{m}^{T,C}\) (\(P_{m}^{I,C}\)). We concatenate the missing-aware prompts and prepend them to the input tokens \(x^{m_{1}}\) (\(x^{m_{2}}\)) as a whole sequence to process. Finally, we concatenate the task-related token of both encoders as the final output representation, and pass it through a fully-connected layer for task prediction. In this procedure, only the parameters of the fully-connected layer and the newly introduced deep correlated prompts are updated in the training procedure, and the backbone (i.e., the text embedding layer, image embedding layer, text encoder and image encoder) is kept frozen. The illustration of our proposed prompts is given in Fig. 2. We next introduce our prompting designs in detail.

### Deep Correlated Prompt Learning

#### 3.3.1 Correlated Prompts

MMP  append independent prompts to the input and intermediate features of the multimodal backbone to guide model predictions. Though it could theoretically provide enough guidance for features in each layer, the prompts across each layer lack synergy, which fails to cooperate with the representations of each layer that various semantics. We argue that prompts across consecutive layers should be closely correlated to receive necessary semantics from preceding layers to instruct

Figure 1: The overview of our proposed framework. We first select the prompt \(P_{m}^{T}\) and \(P_{m}^{I}\) with \(m\{c,m_{1},m_{2}\}\) for the text encoder and image encoder according to the missing case (e.g., complete, text-only, image-only) of the multimodal inputs (\(x^{m1}\), \(x^{m2}\)). The prompt \(P_{m}^{T}\) (\(P_{m}^{I}\)) is composed of three types of missing-aware prompts including the correlated prompts \(P_{m}^{T,R}\) (\(P_{m}^{I,R}\)), dynamic prompts \(P_{m}^{I,D}\) (\(P_{m}^{I,D}\)) and modal-common prompts \(P_{m}^{T,C}\) (\(P_{m}^{I,C}\)). Then we prepend the prompts to the inputs and intermediate features of both encoders to instruct the model to fit the missing case. Finally, we concatenate the task-related token of both encoders as the final representation, and pass it through a fully-connected layer for class prediction. In the whole procedure, only the fully-connected (fc) layer and deep correlated prompts are updated while others keep frozen.

the current layer to handle missing modalities. Thus, we propose to generate the prompts of the current layer based on the observation of the preceding prompts. Besides, the features of different modalities usually contain complementary information. Leveraging the complementary information from various modalities could further help the model predictions. We propose to incorporate the beneficial information from multimodal inputs to help guide the outputs of each modality.

Specifically, we prepend the prompts to the input features and the intermediate features of the multimodal backbone up to a depth of \(J\). Taking the image encoder as an example, the calculation process of the \(i_{th}\) (\(i[0,,J-1]\)) layer \(_{i}\) could be expressed as:

\[[\_,X_{i}^{I}]=_{i}([P_{m,i-1}^{I,R},X_{i-1}^{I}]).\] (1)

Here, \(X_{i}^{I}\) is the features of the \(i_{th}\) layer in the image encoder and \(P_{m,i-1}^{I,R}\) is the newly introduced correlated prompts of the \((i-1)_{th}\) layer. After the \(J_{th}\) layer, the correlated prompt is retained from the previous layer, and the calculation process for the \(i_{th}\) (\(i[J,,N-1]\)) layer \(_{i}\) can be presented as:

\[[P_{m,i}^{I,R},X_{i}]=_{i}([P_{m,i-1}^{I,R},X_{i-1}^{I}])\] (2)

where \(N\) denotes the length of all layers.

To leverage the correlations of prompts between different layers, we generate the prompts of the \(i_{th}\) (\(i[1,,J-1]\)) layer \(_{i}\) by observing prompts of its preceding layer \(_{i-1}\) to inject beneficial information, as:

\[P_{m,i}^{I,R}=_{i-1}^{I}(P_{m,i-1}^{I,R})\] (3)

where \(_{i-1}^{I,R}()\) denotes the prompt generation function for the \((i-1)_{th}\) layer in the image encoder. To decrease the required parameters and add non-linearity to \(()\), we adopt \(()\) as a bottleneck

Figure 2: (1) Baseline, which simply uses fixed image encoder and text encoder and only finetunes the classifier to handle downstream tasks. (2) MMP, which inserts independent prompts at each layer to guide the model to handle missing-modality cases. (3) Correlated prompts, which generate the prompts of the next layer based on the prompts of both modalities in the current layer to enable cooperation of prompts from both modalities. (4) Dynamic prompts, which dynamically computes the prompts based on different input features to better guide the behavior of the model, avoiding using fixed prompts for different inputs. (5) Modal-common prompts, which store the shared information across different modalities and facilitate the model to encode modal-specific information to better handle the missing scenarios in each modality.

MLP with the GELU activation  in between, followed by a LayerNorm (LN) function  as:

\[()=(((()))).\] (4)

The intermediate feature dimension of \(()\) is \(r\) (usually \(r=\)) times of its input channel dimension. For the correlated prompts at the input level, we leave it randomly initialized without the generation procedure.

To leverage complementary information from different modalities, we incorporate prompts from both encoders to fuse their distinct semantics for instructions. Taking the image encoder as an example, we generate the prompts of the \(i_{th}\) (\(i[1,,J-1]\)) layer \(_{i}\) based on the prompts of the layer \(_{i-1}\) in the image encoder and the prompts of layer \(_{i-1}\) in the text encoder as:

\[P^{I}_{m,i}=^{I}_{i-1}((P^{I}_{m,i-1},P^{T}_{m,i-1})).\] (5)

Here, Concat denotes the concatenation operation. Instead of leaving prompts uncorrelated across different modalities, our design improves mutual synergy between different modalities to help adapt to various missing scenarios.

#### 3.3.2 Dynamic prompts

Different input samples usually contain information of various semantics. Using fixed prompts for different inputs may not well fit their distinct features, and thus can't offer enough guidance for the model to fit different missing-modality scenarios. Thus, we propose to dynamically generate the prompts based on the input features to adjust the instructions to fit the characteristics of different inputs.

Specifically, taking the image encoder as an example, we generate the dynamic prompts \(P^{I,D}_{m}\) based on the input features \(X^{I}_{0}\) as:

\[P^{I,D}_{m}=^{I}(X^{I}_{0})\] (6)

where \(^{I}\) denotes the dynamic prompt generation function for the image encoder. To deal with the varying length of tokens for different inputs, we instantiate \(^{I}\) as a self-attention layer , whose architecture can be expressed as :

\[()=(((()))).\] (7)

Here, MHA is the multi-head attention mechanism in transformers . We set the number of heads as 1 for simplicity. The dynamic prompts are only inserted at the input level. For the subsequent layers, the prompts are retained from the previous layers and updated together with intermediate features following Eq. 2.

#### 3.3.3 Modal-common prompts

Multimodal inputs usually contain both modal-specific information and modal-common information across different modalities. By disentangling the modal-common features from the modal-specific features, each modality could aggregate beneficial information from the shared features across various input modalities and build more powerful representations based on its unique characteristics. We decompose the multimodal missing-aware prompts into modal-common parts and modal-specific parts to store common characteristics across different modalities and model-specific features for each modality, respectively.

Specifically, we introduce a modal-common prompt \(P^{C}_{m}\) for multimodal inputs to embody the mutual features across different modalities, and accordingly encourage the proposed correlated prompts and dynamic prompts to embed modal-specific instructions. To cooperate with the features of different modalities, we project the modal-common prompt \(P^{C}_{m}\) to the image and text space to obtain \(P^{T,C}_{m}\) and \(P^{I,C}_{m}\) via projection layers, respectively, as:

\[P^{T,C}_{m} =^{T}(P^{C}_{m})\] (8) \[P^{I,C}_{m} =^{I}(P^{C}_{m}).\]

Here, \(^{T}\) and \(^{I}\) are the projection layers for the text modality and the image modality, respectively, which are both instantiated as a MLP with the intermediate feature reduction factor \(r\)=16. We only insert modal-common prompts at the input level and leave the prompt in the intermediate layers reserved and updated following Eq. 2.

Experiments

### Experimental Setup

**Datasets.** We follow the previous works [19; 27] to evaluate our methods:

MM-IMdb  is currently the largest publicly available multimodal dataset for genre prediction on movie genre classification. It is notated with both image and text modalities for 25959 movies. As a movie might have several genres, the genre prediction task is thus a multi-label classification task.

UPMC Food-101  is a large multimedia dataset consisting of noisy image-text pairs collected from Google Image Search with 101 food categories. It has identical categories to the largest publicly available ETHZ Food-101 dataset , used to classify the categories of different foods.

Hateful Memes  is a multimodal dataset for hateful meme detection (image + text) that contains 10,000+ new multimodal examples created by Facebook AI. To prevent the model from relying on a single modality, it is constructed to make unimodal models more likely to fail.

**Implementation details.** We use CLIP  as our multimodal backbone, with ViT-B/16  as the image encoder. For the image input, we follow CLIP  to resize input images into 224\(\)224. The patch size is set as 16 for the multimodal transformer. For the text modality, we use the tokenizer from pretrained CLIP to tokenize the text input. The maximum length of text inputs is 77. We freeze all the parameters of both the image encoder and text encoder, and only tune the parameters of deep correlated prompts and the fc layer (for task target). We set the length \(L_{p}\) of learnable prompts as 36 and prepend them to the features of \(M=6\) layers. We use Adam optimizer with initial learning rate of 1e-2 and weight decay 2e-2. The learning rate is warmed up for 10% of the total training steps and then is decayed linearly to zero. We perform our experiments with batch size of 4 on a 3090 GPU. For the missing modality, we stop feeding the inputs into the corresponding encoder and use a zero-filled tensor as the output instead.

**Metrics.** We use corresponding proper metrics for each dataset to evaluate our method. For MM-IMdb , we adopt F1-Macro to measure the multi-label classification performance. For UPMC Food-101 , we employ the top-1 classification accuracy to evaluate the recognition performance. For Hateful Memes , we use Area Underthe Receiver Operating Characteristic Curve (AUROC).

**Setting of Missing Modality.** In this paper, we focus on the general realistic scenarios in real life, where any modality may appear in both training and testing phases. To stimulate this condition, we follow MMP  to define the missing rate \(\) as the proportion of modality-incomplete data to the entire dataset. For vision-language tasks, there exist three types of missing cases: missing-both (text and image), missing-text and missing-image. For the missing-both case, the training and testing data are composed of \(\) text-only data, \(\) image-only data and (1-\(\)) modality-complete data. For the missing-text and missing-image cases, the training and testing data are composed of \(\) image-only (text-only) data and (1-\(\)) modality-complete data. This definition could be naturally extended to data with more modalities by using \((\ -2})\) modality-incomplete data for each missing case and (1-\(\)) complete data. In our experiments, we use \(\)=\(70\%\) by default.

### Experimental Results

**Effectiveness.** We first verify the effectiveness of our proposed components across different missing cases including missing-image, missing-text and missing-both. We conduct the experiments upon the MM-IMdb  dataset with missing rates ranging from 0% to 100%. The methods used for comparison include (1) baseline, which directly sets the features as zeros when a modality is missing; (2) Ours (A), which only equips the correlated prompts; (3) Ours (B), which equips both the correlated prompts and the dynamic prompts; (4) Ours, which uses all the three proposed prompts. The only difference between our method and the baseline is inserting learnable prompts at each layer which only bring few extra parameters. The results are shown in Fig. 3.

It's first noticed that compared to the baseline, all our three variants could notably promote the performance across all missing rates for various missing-modality cases, demonstrating strong robustness to different missing-modality scenarios. Using the correlated prompts boosts the performance most, and equipping the dynamic prompts or modal-common prompts offers a similar performance boost. Using all three proposed prompts achieves the best performance, which verifies the effectiveness of 

[MISSING_PAGE_FAIL:8]

projection performs better than using a fc for projection. Using different channel reduction factors \(r\) including 4, 8 and 16 achieves similar performance. We thus use a MLP with \(r\)=16 by default.

**Generalizability.** We conduct experiments to verify the generalizability of our method when trained and tested upon different missing cases. We evaluate models trained on _missing-both_ cases or _missing a specific modality_, and test them on the missing-both, missing-image and missing text cases in Fig. 4(a), (b) and (c), respectively. It's first noticed that all our variants outperform the baseline by a large margin across various missing cases and missing rates. Models trained with a certain modality missing (e.g., missing-text, missing-image), usually perform slightly better than models trained upon missing-both cases when tested with a certain modality missing. Models trained with missing both modalities perform robustly to cases with any missing type (e.g. missing-both, missing-text and missing-image). We further observe the model trained with higher missing rates (e.g., 70%), is more robust to high-rate missing cases (70%-100%) during testing than models trained with lower missing rates (e.g., 30% and 50%). It is concluded that models trained on missing-both cases are robust to various missing cases, which demonstrate strong robustness compared to training with one modality.

**Comparison with other methods.** We compare our method with recent approaches upon three commonly-used datasets, i.e., MM-IMDb , UPMC Food-101  and Hateful Memes , to verify its effectiveness upon different missing-modality scenarios. We include the following methods for comparison (1) baseline, which directly drops the features when a modality is missing; (2) CoOp , which only prepends prompts at the input level; (3) MMP , which inserts independent prompts for the input and intermediate features of the multimodal backbone; (4) MaPLe , which generates the prompts in the image encoder based on those of the text encoder; (5) DePT , which decouples base-specific knowledge from feature channels into an isolated feature space during prompt tuning. MMP  is originally based on the ViLT backbone and we reimplement it following the same protocol as ours. We compare these methods across three different missing rates including \(\)=50%, \(\)=70% and \(\)=90% upon three various missing-modality cases including missing-image, missing-text and missing-both in Table. 4. Our method largely outperforms other methods on three datasets across different missing rates, which verifies its robustness to different missing scenarios. Compared to MMP , our method exceeds it over all missing cases by a large margin, which demonstrates the superiority of leveraging the correlations between prompts and input features.

An interesting observation is that on most datasets (e.g., MM-IMDb  and Food101 ), missing text input usually has higher effects on the performance than missing image input. We figure that texts are more crucial for the tasks by providing more detailed explanations and precise captions than images on these two datasets. Instead, on the Hateful Memes dataset , it's observed that missing image input influences the results more than missing text or missing both. This may reflect the different focus when constructing various datasets.

**Efficiency.** Compared with only finetuning the last fc layer to fit downstream tasks, our deep correlated prompts only introduce extra 4.0M parameters, which just consume 2.4% of the entire model (151M), but could notably improve the performance by a large margin over different missing cases and missing rates.

Figure 4: Ablations on the generalizability to different testing scenarios across various missing rates on the val set of MM-IMDb dataset . (a) All models are trained on _missing-both_ cases, and evaluated on _missing-both_ cases with different missing rates. (b) Models are trained on _missing-both_ or _missing-image_ cases, and evaluated on _missing-image_ cases with different missing rates. (c) Models are trained on _missing-both_ or _missing-text_ cases, and evaluated on _missing-text_ cases with different missing rates.

**Performance v.s. MMP when owning comparable parameters.** We compare our method with MMP by allowing them to own comparable parameters in Tab. 5. Specially, either when we decrease the required parameters of our method to Table 5: Performance v.s. MMP when owning comparable parameters on the val set of MM-IMdb .

**Limitations and broader impacts.** The limitations include (1) we only test the effectiveness upon two commonly-used two-stream multimodal models, and don't apply our method to other popular multimodal models. (2) we only include two modalities in the experiments, and will incorporate more modalities in the future. The broader impacts include (1) The missing-modality cases happen at times in real life. This paper proposes a novel method by adapting large multimodal models towards missing-modality scenarios, which increases the robustness of large multimodal models. (2) Our methods can notably decrease the required computations compared to previous methods upon missing-modality learning, which achieves a better accuracy-computation trade-off in real life.

## 5 Conclusion

In this paper, we tackle two main challenges in multimodal learning, including (1) any modality may be missing in any learning phase (e.g., training, testing or both), and (2) how to efficiently adapt large-scale multimodal transformers to fit missing-modality cases. We propose deep correlated prompting by leveraging the correlations between prompts of different layers and the relationships between prompts and input features. Results on three diverse datasets across various missing types and missing ratios verify the effectiveness of our proposed method.