# Disentangling Interpretable Factors with Supervised Independent Subspace Principal Component Analysis

Disentangling Interpretable Factors with Supervised Independent Subspace Principal Component Analysis

Jiayu Su\({}^{1,2,5}\) David A. Knowles\({}^{2,4,5}\) Raul Rabadan\({}^{1,2,3}\)

\({}^{1}\)Program for Mathematical Genomics; \({}^{2}\)Department of Systems Biology, Columbia University

\({}^{3}\)Department of Biomedical Informatics, Columbia University

\({}^{4}\)Department of Computer Science, Columbia University

\({}^{5}\)New York Genome Center

\({}\)Correspondence to {js5756, rr2579}@cumc.columbia.edu, dak2173@columbia.edu

###### Abstract

The success of machine learning models relies heavily on effectively representing high-dimensional data. However, ensuring data representations capture human-understandable concepts remains difficult, often requiring the incorporation of prior knowledge and decomposition of data into multiple subspaces. Traditional linear methods fall short in modeling more than one space, while more expressive deep learning approaches lack interpretability. Here, we introduce _Supervised Independent Subspace Principal Component Analysis (sisPCA)_, a PCA extension designed for multi-subspace learning. Leveraging the Hilbert-Schmidt Independence Criterion (HSIC), sisPCA incorporates supervision and simultaneously ensures subspace disentanglement. We demonstrate sisPCA's connections with autoencoders and regularized linear regression and showcase its ability to identify and separate hidden data structures through extensive applications, including breast cancer diagnosis from image features, learning aging-associated DNA methylation changes, and single-cell analysis of malaria infection. Our results reveal distinct functional pathways associated with malaria colonization, underscoring the essentiality of explainable representation in high-dimensional data analysis.

## 1 Introduction

High-dimensional data generated by complex biological mechanisms encapsulate an ensemble of patterns. A prime example is single-cell RNA sequencing (scRNA-seq) data (Fig. 1). These datasets measure the expression of tens of thousands of genes across potentially millions of cells, creating rich tapestries woven from interacting cellular pathways, gene dynamics, cell states, and inherent measurement noise. To unravel these patterns and reveal hidden relationships, it is necessary to decompose the data into meaningful, lower-dimensional subspaces.

Linear representation learning methods, such as Principal Component Analysis (PCA)  and Independent Component Analysis (ICA) , extract latent spaces from data using explainable linear transformations. These widely employed unsupervised tools learn a single latent space or a union of one-dimensional subspaces. Independent Subspace Analysis (ISA) extends ICA by extracting multidimensional components as independent subspaces . Yet, the unsupervised nature of these methods precludes knowledge integration, restricting their utility and sometimes even identifiability . In the context of Fig. 1, the representation learned without supervision fails to separate temporal variability from technical batch effects.

Conversely, recent advancements in deep generative models, especially semi-supervised approaches , have shown promise in disentangling diverse latent spaces and retaining relevant information under supervision. However, challenges remain in ensuring subspace independence within a variational autoencoder (VAE). While models like \(\)-VAE , HCV[Lopez et al., 2018] and biolord [Piran et al., 2024] attempt to address this, inference for deep generative models remains challenging and the learned representations are not interpretable.

To bridge the gap, we propose Supervised Independent Subspace Principal Component Analysis (sisPCA)1, an innovative method extending PCA to _multiple subspaces_. By incorporating the Hilbert-Schmidt Independence Criterion (HSIC), sisPCA effectively decomposes data into explainable independent subspaces that align with target supervision. It thus reconciles the simplicity and clarity of linear methods with the nuanced multi-space modeling capabilities of advanced generative models. In summary, our contributions include:

* A multi-subspace extension of PCA for disentangling linear latent subspaces in high-dimensional data. We additionally show that supervising subspaces with a linear target kernel can be conceptualized as linear regression regularized akin to Zellner's g-prior.
* An efficient eigendecomposition-based alternating optimization algorithm to compute sisPCA subspaces. The learning process with linear kernels resembles matrix factorization, which may potentially benefit from desirable local geometric properties (Conjecture 3.1).
* Demonstrated effectiveness and interpretability of sisPCA in various applications. This includes identifying diagnostic image features for breast cancer, dissecting aging signatures in human DNA methylation data, and unraveling time-independent transcriptomic changes in mouse liver cells upon malaria infection.

## 2 Background

### Hilbert-Schmidt Independence Criterion (HSIC)

The Hilbert-Schmidt Independence Criterion (HSIC) serves as a methodology for testing the independence of two random variables \(X\) and \(Y\)[Gretton et al., 2005a]. It operates by embedding probability distributions into reproducing kernel Hilbert spaces (RKHS) and quantifying independence by distance between the joint distribution and the product of its marginals. Specifically, HSIC builds on the cross-covariance operator \(C_{XY}:\), which is a generalization of the cross-covariance matrix to infinite-dimensional RKHS (i.e., the feature spaces of \(X\) and \(Y\) after transformation),

\[C_{XY}:=_{XY}[(X)(Y)]-_{X}[(X)] _{Y}[(Y)].\]

Here \(:=(\{(X)\})\) and \(:=(\{(Y)\})\) are the RKHSs with feature maps \(\) and \(\) for \(X\) and \(Y\) respectively. The tensor product operator \(f g\) maps \(\) to \(\), such that \((f g)h:=f g,h_{}\) for all \(h\). The HSIC is then defined as,

\[HSIC(X,Y;,):=||C_{XY}||_{HS}^{2}:=_{i,j} C_{ xy}v_{i},u_{j}_{}\]

Figure 1: Example scRNA-seq dataset from Afriat et al. . Each dot represents the gene expression vector \(^{8,203}\) of a cell, visualized in 2D and colored by cell properties \(\{Y_{m}\}\). Variability in the dataset \(X\) arises from multiple sources: (left to right) temporal dynamics of infection, technical batch effects, and cell quality. Incorporating supervisory information \(Y\), such as time points, allows for the extraction of patterns in distinct subspaces \(\{Z_{m}\}\) that correspond to different sources of variability. Moreover, the linear mapping \(\{U_{m}:X Z_{m}\}\) directly quantifies the relationship between gene expression and the property of interest, enabling discoveries such as the identification of genes underlying the persistent defense against infection. The disentanglement is particularly important to ensure minimal confounding effects. See Section 4.4 for details.

where \(v_{i}\) and \(u_{j}\) are the orthogonal bases of \(\) and \(\). \(X\) and \(Y\) are independent if and only if \(HSIC(X,Y;,)=0\). In practice, given a finite sample \(\{(x_{i},y_{i})\}_{i=1}^{n}\) from the joint distribution \(P_{X,Y}\), the empirical HSIC can be computed as,

\[HSIC_{n}(X,Y)=}tr(K_{X}HL_{Y}H)\]

where \(K_{X}\) and \(L_{Y}\) are matrices of kernel evaluations over the samples \(\{x_{i}\}_{i=1}^{n}\) in \(\) and \(\{y_{i}\}_{i=1}^{n}\) in \(\), and \(H\) is the centering matrix defined as \(H=I_{n}-_{n}_{n}^{}\), with \(I_{n}\) being the identity matrix.

### Related work

The HSIC, as a non-parametric criterion for independence, has been effectively integrated into numerous representation learning models, especially for disentangling complex data structures. One of the earliest applications of HSIC is ICA (Comon, 1994), where the goal is to recover unmixed and statistically independent sources. Gretton et al. (2005a) showed that minimizing HSIC via gradient descent outperforms specialized linear ICA algorithms. Alternatively, HSIC can also be maximized to encode specific information in learning tasks. In supervised PCA (sPCA), HSIC is deployed to guide the identification of principal subspaces with maximum dependence on target variables (Barshan et al., 2011). More recently, Ma et al. (2020) used HSIC as a supervised learning objective for deep neural networks to bypass back-propagation, and Li et al. (2021) subsequently extend it to the context of self-supervised learning.

Our primary interest lies in extending these models to identify _multiple subspaces_, each representing independent, meaningful signatures. In this direction, Cao et al. (2015) suggested the inclusion of HSIC as a diversity term in multi-view subspace clustering, encouraging representations from different views to capture complementary information. Building on a similar idea, Lopez et al. (2018) incorporated HSIC-based regularization into VAE architectures to promote subspace separation. However, there has yet to be a linear multi-space model for more interpretable data decomposition.

The presented work is also related to contrastive representation learning, as introduced in Abid et al. (2018) and Abid and Zou (2019). Along this line of research, recent studies have also applied HSIC to regularize contrastive VAE subspaces (Tu et al., 2024; Qiu et al., 2023). See Appendix A for more discussions on connections and differences.

## 3 The sisPCA model

We introduce _Supervised Independent Subspace Principal Component Analysis (sisPCA)_, a linear model for disentangling independent data variation (Fig. 2). The model's linearity ensures explicit interpretability and enables regression-based extensions such as sparse feature selection. We formally discuss the connection between sisPCA and regularized linear regression in Section 3.2.

### Problem formulation

As motivated in Fig. 1, given a dataset \(\{X_{i}^{p}\}_{i=1}^{n}\) with \(n\) observations and \(p\) features and \(m\) associated target variables \(\{Y_{i}^{m}\}_{i=1}^{n}\), we aim to find \(m\) separate subspace representations of

Figure 2: Overview of sisPCA and its relationship with other PCA models.

the data \(\{\{_{i}^{j}^{d_{j}}\}_{j=1}^{m}\}_{i=1}^{n}\) with latent dimensions \(\{d_{j}\}_{j=1}^{m}\). Each subspace should maximize dependence with one target variable while minimizing dependence with other subspaces. For simplicity, we assume Euclidean space, \(\{X_{i}\}:=X^{n p}\) and \(\{Y_{i}\}:=Y^{n m}\). Other types of data can be easily handled using appropriate kernels, which will be discussed later. The linear projection to the \(j\)-th subspace \(_{j}:^{p}^{d_{j}}\) is represented by the matrix \(U_{j}^{p d_{j}}\), with \(Z_{j}:=XU_{j}^{n d_{j}}\) being the data representation in this \(j\)-th subspace. Similar to the concept of PCA loading, the projection \(U_{j}\) depicts linear combinations of original features in \(X\) and thus can be directly interpreted as feature importance scores.

Our overall objective is to find the set of subspace projections \(\{U_{1},...,U_{m}\}\) that solves

\[*{argmax}_{U_{1},...,U_{m}}_{j=1}^{m}(XU_{j},Y_{ j})-_{j=1}^{m}_{i>j}^{m}(XU_{i},XU_{j}),\]

under some constraints. Here \(\) is a measure of dependence and \(\) penalizes overlapping subspaces. Using HSIC as the dependence measure, \(\) solves the constrained optimization

\[*{argmax}_{U_{1},...,U_{m}}_{j=1}^{m}tr(K_{Z_{j} }HK_{Y_{j}}H)-_{j=1}^{m}_{i>j}^{m}tr(K_{Z_{i}}HK_{Z_{j}}H)\] (1) \[\ U_{j}^{T}U_{j}=I,\  j\{1,...,m\},\]

where \(K_{Z_{j}}\) and \(K_{Y_{j}}\) are kernels defined on the \(j\)-th subspace \(_{j}\) and the \(j\)-th target variable \(_{j}\), respect. This formulation differs from kernel PCA, where in the first term of (1) the kernel is defined over \(Z\) rather than \(X\) (the kernel extension of \(\) will be discussed later).

In the special case where \(K_{Z_{j}}:=Z_{j}Z_{j}^{T}\) is linear for all subspaces, the optimization becomes

\[*{argmax}_{U_{1},...,U_{m}}\ _{j=1}^{m}tr(XU_{j}U_{j}^{T}X^{T}HK_{Y_{j}}H)- _{j=1}^{m}_{i>j}^{m}tr(XU_{i}U_{i}^{T}X^{T}HXU_{j}U_{j}^{T}X^{T }H)\] (2) \[\ U_{j}^{T}U_{j}=I,\  j\{1,...,m\}.\]

The first term is the supervised PCA objective . We can thus view the above formulation, termed \(\)-linear, as an extension of supervised PCA to multiple subspaces with additional regularization for subspace independence (Fig. 2). It comes with an appealing property:

_Remark 3.1_.: _Maximizing the \(\)-linear objective (2) is equivalent to minimizing the reconstruction error of a linear autoencoder plus regularization. See Appendix B._

We now examine the second HSIC term in (2). Consider two subspaces \(Z_{u}:=XU^{n d_{u}}\) and \(Z_{v}:=XV^{n d_{v}}\) with centered \(X\). The HSIC regularization is

\[tr(XUU^{T}X^{T}XVV^{T}X^{T})=tr(Z_{u}Z_{u}^{T}Z_{v}Z_{v}^{T})=||Z_{u}^{T}Z_{v} ||_{F}^{2} 0.\]

This term equals zero if and only if \(Z_{u}\) and \(Z_{v}\) are orthogonal. While not convex in \(U\) and \(V\) jointly, the coupling of \(U\) and \(V\) solely through the matrix product \(Z_{u}^{T}Z_{v}\) indicates a well-behaved local geometry, as explored by Sun and Luo (2016). Indeed, it allows \(\)-linear to benefit from theoretical insights on matrix factorization. For example, Ge et al. (2017) showed that for a quadratic function \(f\) over the matrix \(U^{T}V\) -- in our context \(f=||Z_{u}^{T}Z_{v}||_{F}^{2}\) -- all local minima are also globally optimal under mild conditions achievable through proper regularization.

Returning to (2), we see that spurious local optima may only emerge from subspace imbalance in the first symmetry-breaking supervision term, where some subspaces may contribute more to the overall objective. This leads to the following conjecture on the optimization landscape of (2):

**Conjecture 3.1** (informal): _The \(\)-linear objective (2) has no spurious local optima under balanced supervision, which is achievable through proper regularization; With unbalanced supervision, the global optima can still be recovered using local search algorithms following simple initialization based on the relative supervision strength of each subspace._

Intuitively, the balanced supervision condition is to ensure that the local optima induced by subspace symmetry and interchangeability are also global optima. For more discussions on the optimization landscape, the balance condition, and Conjecture 3.1, see Appendix C.

We now consider an iterative optimization approach to solve (2).

_Remark 3.2_.: _The optimization problem in (2) can be solved via alternating optimization, where each iteration has an analytical update for each subspace._

Using basic matrix algebra, the objective of (2) simplifies to,

\[_{j=1}^{m}\!tr(U_{j}^{T}X^{T}HK_{Y_{j}}HXU_{j})-_{j=1}^ {m}_{i j}^{m}tr(K_{Z_{i}}HK_{Z_{j}}H)=_{j=1}^{m}tr(U_{j}^{T}X^{T} _{j}XU_{j})\]

where \(_{j}:=H(K_{Y_{j}}-_{i=1,i j}^{m}K_{Z_{i}})H\) and \(K_{Z_{i}}:=Z_{i}Z_{i}^{T}=XU_{i}U_{i}^{T}X^{T}\). Given the set \(\{U_{i j}\}_{i}\), \(U_{j}\) can be updated by maximizing \(tr(U_{j}^{T}X^{T}_{j}XU_{j})\), leading to the update \(U_{j}^{(t+1)} Q_{d_{j}}\), where \(Q_{d_{j}}\) are the columns of \(Q\) corresponding to the \(d_{j}\) largest eigenvalues from the eigendecomposition \(X^{T}_{j}X:=Q Q^{T}\).

The full optimization process is outlined in Algorithm 1, Appendix B. This procedure guarantees convergence to an optimum as the objective is bounded and non-decreasing in every iteration. We further implement an initialization step to find the path (subspace update order) towards the global optimum as proposed in Conjecture 3.1. Briefly, we compare subspace contributions to the supervision loss and prioritize updates for subspaces under stronger supervision.

While convenient, a zero HSIC regularization loss with a linear kernel in (2) does not guarantee independent subspaces. For strict independence, the subspace kernel \(K_{Z}\) in (1) needs to be universal (e.g., Gaussian). We refer to this as \(\) and solve it using gradient descent (Algorithm 2, Appendix D). The naive implementation with a Gaussian kernel has complexity \(O(n^{3})\) in contrast to \(O(n^{2})\) with a linear kernel. See Appendix D for performance difference discussions.

Both \(\) and \(\) are linear methods where \(\{U_{m}\}\) measures direct contribution of original features to each subspace. Nevertheless, the \(\) framework can be easily extended to incorporate nonlinear feature interactions, analogous to kernel PCA. See Appendix E.

### Kernel selection for different target variables

The use of kernel independence measure in (1) allows \(\) to accommodate various data types through flexible kernel choices for \(K_{X}\) (data), \(K_{Z}\) (latent subspace), and \(K_{Y}\) (target).

For categorical variables \(Y=Y_{j}^{\,n}\) (e.g., cancer types), we use the Dirac delta kernel,

\[K_{Y}(i,j)=_{Y_{i}=Y_{j}}.\]

It is also possible to use other general graph kernels for categorical variables with an intrinsic hierarchical structure, e.g., subtypes and stages (Smola and Kondor, 2003).

For continuous variables \(Y^{n}\), we use the linear kernel

\[K_{Y}=YY^{T}.\]

When \(Y^{n d}\) is multivariate, the kernel is the sum of per-dimension kernels \(K_{Y}=_{i=1}^{d}Y_{:i}Y_{:i}^{\,T}\).

_Remark 3.3_.: _Maximizing the \(\) objective (2) with linear kernels on the target space is equivalent to performing regularized regression against the target. See Appendix B._

In a nutshell, we show that \(\) can be viewed as approximating the target \(Y\) with \(Z=Xu\). The particular regularization on \(u\) corresponds to a zero-mean multivariate Gaussian prior, which is related to Zellner's g-prior in Bayesian regression.

### Learning an unknown subspace without supervision

In practical applications, a common goal would be to recover both subspaces linked to known attributes (supervised) and to unknown attributes (unsupervised) simultaneously. In \(\), this is achieved by setting the target kernel for the unknown subspace to the identity matrix (\(K_{Y}=I\)), corresponding to unsupervised PCA (Fig. 2). Section 4.1 provides an example of this process.

However, the absence of external supervision introduces a potential identifiability issue. As indicated in Appendix B eq. 4 each supervised subspace is driven by two forces to (1) align with the target and (2) capture major variations in the data. This dual objective can lead to scenarios where unknown attributes, ideally retained in the residual unsupervised subspace, being inadvertently presented in supervised subspaces. Fig. 8 in Appendix D gives an example where \(\) fails.

## 4 Applications

Unless otherwise specified, in the following sections sisPCA refers to sisPCA-linear, where the objective (2) is solved using Algorithm 1. For baseline comparisons, we consider linear models including PCA and sPCA. While non-linear VAE counterparts such as HCV (Lopez et al., 2018) are included for quantitative performance benchmarking in Section 4.4, they are not considered for interpretability analyses due to their inherent complexity. The rationale and details for baseline selection are provided in Appendix F.

### Recovering supervised and unsupervised subspaces in simulated data

We first consider learning latent subspaces associated with known and unknown attributes using simulated data. The dataset reflects a ground truth 6-dimensional latent space, comprising three distinct 2D subspaces (Fig. 2(a)): S1 with two Gaussian distributions, S2 with a noisy 2D grid and S3 with a ring structure. The defining manifold characteristics \(\) of S3 remain unknown to the model, representing the unsupervised component. These subspaces were concatenated and linearly projected to 20-dimensions using a \(6 20\) matrix with entries uniformly distributed on \(\).

Both unsupervised PCA (targeting S3) and supervised PCA (S1 and S2) subspaces capture structures heavily influenced by S2 (Fig. 2(b) and Fig. 11 in Appendix H), due to S2's pronounced variations. In contrast, sisPCA markedly improves the disentanglement of these subspaces, especially the unsupervised S3 (Fig. 2(c)). Despite S2's dominant influence, sisPCA isolates the effects of each subspace, resulting in clearer separation of the three independent signals. The two supervised subspaces S1 and S2 are distinctly characterized by patterns exclusively associated with the supervision attributes. In the unsupervised subspace S3, although Principal Component 2 (PC2) picks up some categorical information from S1, sisPCA successfully uncovers the underlying circular structure.

### Learning diagnostic subspaces from breast cancer image features

We apply sisPCA to the Kaggle Breast Cancer Wisconsin Data2 to demonstrate its utility in data compression and feature extraction. The dataset contains 569 samples with 30 summary features from breast mass imaging. Our goals are to (1) learn compressed subspaces for predicting disease status ('Malignant' or 'Benign', agnostic during training), and (2) understand the relationship between original features and how they contribute to the learned representation and diagnosis potential.

Figure 3: Example application of recovering a latent space with three subspaces (rows in panel a) embedded in a high-dimensional space. The first two subspaces (rows) of sPCA (panel b) and sisPCA (panel c) are supervised by the corresponding target variables.

The diagnosis label, invisible to all models, is used to measure subspace quality via the mean silhouette score \(_{i}s(i)=_{i}(b(i)-a(i))/\{a(i),b(i)\}\), where \(a(i)\) and \(b(i)\) are mean intra-cluster and nearest-cluster distances for sample \(i\). Higher scores indicate larger diagnostic potential. In addition, we use the Geodesic Grassmann distance \(d(Z_{i},Z_{j})=(_{n=1}^{k}_{n}^{2})^{1/2}\) to measure subspace separateness, where \(k=\{ Z_{i}, Z_{j}\}\) and \(\{_{n}\}\) the principal angles [Miao and Ben-Israel, 1992]. Higher scores indicate better disentanglement. Inputs for all model are zero-centered and variance-standardized.

In the PCA space, samples are well-separated based on diagnosis along PC1 (Fig. 3(a)).'symmetry_mean' and 'radius_mean' are the top two features negatively contributing to PC1, motivating us to construct separate subspaces to reflect nuclei size (using 'radius_mean' and 'radius_sd' as targets) and shape (using'symmetry_mean' and'symmetry_sd' as targets). The remaining 26 features are projected onto these subspaces using SPCA (Fig. 3(b)) and sisPCA (Fig. 3(c)). In sPCA, both subspaces better explain diagnosis status than PCA but remain highly entangled (Grassmann distance: \(1.493\), Pearson correlation of PC2 loading: 0.850). However, with sisPCA's explicit disentanglement, the symmetry subspace loses its predictive power as the two spaces separate further (Grassmann distance: \(2.710\)). sisPCA subspaces are constructed from distinct feature sets (PC2 loading correlation \(-0.203\)), with 'area' and 'perimeter' contributing more to the radius subspace and 'compactness' and'smoothness' to the symmetry one. The radius subspace also gains additional predictive power by repulsing further from the symmetry space (Silhouette scores: \(0.516\) in sisPCA, \(0.470\) in sPCA).

Our sisPCA results suggest that cell nuclear size is more informative for breast cancer diagnosis than nuclear shape. We confirm this by measuring directly the predictive potential of target variables (Silhouette scores: 0.457 for 'radius_mean' and 'radius_sd', 0.092 for'symmetry_mean' and'symmetry_sd'). Our conclusion also aligns with previous clinical observations [Kashyap et al., 2018]. In contrast, PCA and sPCA, while capable to extract new diagnostic features (PC1), cannot faithfully capture feature relationships without disentanglement and potentially overestimate symmetry-related features' relevance in malignancy.

### Separating aging-dependent DNA methylation changes from tumorigenic signatures

Tumorigenesis and aging are two intricately linked biological processes, resulting in cancer omics data that often display patterns of both. DNA methylation (DNAm) exemplifies this complexity, undergoing genome-wide alterations during aging while also exhibiting cancer-specific changes in particular regions, presumably silencing tumor suppressor genes or activating oncogenes.

#### 4.3.1 Problem and dataset description

The Cancer Genome Atlas (TCGA)3 offers a comprehensive collection of DNAm datasets from patients with various cancer types. In TCGA DNAm data, methylation status is probed across genomic locations using the Illumina Infinium 450K array and quantified as beta values ranging from 0 (unmethylated) to 1 (methylated). The resulting data matrix \(X\) presents challenges due to the use of three different probe types and highly correlated features, typically requiring careful preprocessing. For illustration purpose, we use a downsampled dataset comprising the first 5,000 non-constant and non-NA CpG sites from 9,725 TCGA tumor samples across 33 cancer types.

Figure 4: Feature extraction on the breast cancer dataset. The two top PC1 contributors in PCA (panel a) are used as supervisions to construct the ’radius’ and ’symmetry’ subspaces (panel b and c).

Our goal is to disentangle tumorigenic signatures from age-dependent methylation dynamics in this pan-cancer DNAm data. Traditional methylation analyses often employ regression-based methods to learn site-specific statistics, later aggregated by gene or high-level genomic annotations (Bock, 2012). However, these approaches may suffer from high dimensionality and multi-collinearity due to potential redundancy in CpG methylation activity. We propose using sisPCA to address these limitations, which allows us to (1) learn compressed, low-dimensional representations that retain biological information, and (2) minimize confounding factors not controlled in simple regression models by enforcing disentanglement.

Specifically, we aim to learn two subspaces: one aligning with chronological age (CA, aging subspace, supervised with a linear kernel) and another with TCGA cancer categories (cancer subspace, supervised with a delta kernel). We evaluate the quality of learned representations using information density measured by the Silhouette score and subspace separateness by the Grassmann distance. For the rank-one aging subspace (\((K_{Y})=(YY^{T})=1\)), we also measure information density using the maximum absolute Spearman correlation, \(_{d[1,d_{j}]}\{|(Z_{j}^{(d)},)|\}\), between CA and each axis of the subspace \(Z_{j}^{n d_{j}}\).

#### 4.3.2 Quantitative performance of subspace quality

We first validate the disentanglement effect of formulation (2). Unlike models such as Lopez et al. (2018) that use a Gaussian kernel, sisPCA minimizes the HSIC regularization with a linear kernel. This approach trades strict statistical guarantees for improved computational efficiency (detailed in Appendix D). Our experiments demonstrate that as \(\) increases, the two subspaces show increasing divergence (Table 1). Notably, while HSIC-Gaussian is not explicitly optimized, it decreases in tandem with HSIC-Linear. The generally larger values of HSIC-linear potentially offer advantages in optimization and help mitigate numerical rounding errors.

Furthermore, the separation of aging and cancer subspaces leads to a moderate increase in target information density and a decrease in confounding information (Table 2). However, stronger regularization does not always equate to better representations. This is partly due to the inherent coupling between aging and tumorigenesis. Efforts to remove aging signals inevitably result in information loss on cancer type. We discuss the tuning of \(\) more generally in Appendix G.

Disentangling infection-induced changes in the mouse single-cell atlas of the _Plasmodium liver stage_

Malaria, transmitted by mosquitoes carrying the _Plasmodium_ parasite, involves a critical liver stage where the parasite colonizes and replicates within host hepatocytes. This section examines the intricate host-parasite interactions at single-cell resolution during this stage.

    & &  \\   & PCA & \(=0\) (sPCA) & \(=1\) & \(=10\) \\  HSIC-Linear (in the objective of sisPCA) & 481.6 & 189.7 & 2.0e-4 & **2.4e-05** \\ HSIC-Gaussian & 1.1e-2 & 6.5e-3 & 7.0e-4 & **7.0e-4** \\ Grassmann distance & 0 & 3.09 & 4.97 & **4.97** \\   

Table 1: Separateness of the aging and cancer subspaces inferred by sisPCA.

    & & &  \\   & Subspace & PCA & \(=0\) & \(=1\) & \(=10\) \\  Maximum Spearman correlation with age & age & 0.213 & 0.278 & 0.286 & **0.294** \\  & cancer & 0.213 & 0.233 & **0.103** & 0.115 \\ Silhouette score with cancer type & age & 0.074 & -0.183 & -0.221 & **-0.230** \\  & cancer & 0.074 & 0.106 & **0.107** & 0.097 \\   

Table 2: Information density in each sisPCA subspace.

#### 4.4.1 Problem and dataset description

We analyze scRNA-seq data of mouse hepatocytes from Afriat et al. (2022)4. Our goal is to distinguish genes directly involved in parasite harboring from those associated with broader temporal changes post-infection. The processed dataset comprises gene expression profiles of 19,053 cells collected at five post-infection time points (2, 12, 24, 30, and 36h) and from control mice. Infection status was determined based on GFP expression linked to malaria. We keep the top 2,000 highly variable genes and use normalized expression as model inputs. Time points are treated as discrete categories to account for potential nonlinear dynamics. See Appendix F for full experiment details.

#### 4.4.2 Learning the infection subspace associated with parasite encapsulation

UMAP visualizations of PCA, sPCA, and sisPCA subspaces show that while PCA primarily captures temporal variations (Fig. 5a), both sPCA and sisPCA successfully differentiate between infected and uninfected hepatocytes in their infection subspaces. However, sPCA's infection space still exhibits significant temporal effects, suggesting uncontrolled confounding effects (Fig. 12b in Appendix H). sisPCA effectively eliminates this intermingling, yielding cleaner representations where relevant biological information is further enriched in the corresponding spaces (Fig. 5b).

Comparisons with non-linear VAE counterparts (Fig. 5c and Fig. 12, full model description in Appendix F) and quantitative evaluations (Table 3) demonstrate that our HSIC-based supervision formulation (1) achieves performance comparable to neural network predictors. Notably, sisPCA outperforms HSIC-constrained supervised VAE (hsVAE)(Lopez et al., 2018) in separating infected and uninfected cells. Indeed, hsVAE's performance in the infection subspace is so poor that even under supervision, the representation contains near-random information (Fig. 5c). To address this gap, we developed hsVAE-sc by incorporating additional domain-specific knowledge (See Appendix F, and Fig. 12d in Appendix H). This model learns a much improved infection space (Silhouette score: 0.233) while maintaining high distinguishability in the temporal space (Silhouette score: 0.634). The improvement is likely due to the fact that parasite encapsulation can induce a slight increase in total RNA counts; by using unnormalized count-level data and explicitly modeling the library size, the model captures this additional information and thus produces enhanced results.

While visualizations and quantitative metrics provide useful estimates, they may not fully capture the biological relevance of the representations. For instance, a subspace with two point masses perfectly separating cells by infection status would have the highest information density, yet offer little new biological insight. Advantageously, linear models like sisPCA are inherently interpretable. Using the learned sisPCA projection \(U\) as feature importance scores, we rank genes based on their PC contributions. Chemokine genes, including _Cxcl10_, emerge as top positive contributors to infection-PC1, which is elevated in infected cells. This aligns with their established role as acute

Figure 5: UMAP visualizations of scRNA-seq data. Each column shows a different learned subspace: (a) PCA, (b) sisPCA-infection and sisPCA-time, and (c) hsVAE-infection and hsVAE-time. See Fig. 12 for other models. Cells are colored by either infection status (top row) or post-infection time (bottom row). In an optimal pair of subspaces, each property (infection status or time) should be more distinguishable in its corresponding subspace while showing less separation in the other.

phase response markers. Gene Ontology (GO) enrichment analysis of genes with significant PC1 loading scores reveals that infection leads to reduced fatty acid metabolism and enhanced stress and defense responses, consistent with known _Plasmodium_ harboring effects. These results remain highly consistent across a wide range of hyperparameters, demonstrating the robustness of our approach. See Appendix G for full examination on the effect of \(\).

## 5 Discussion

This study presents sisPCA, a novel extension of PCA for disentangling multiple subspaces. We showcase its capability in interpretable analyses for learning complex biological patterns, such as aging dynamics in methylation and transcriptomic changes during _Plasmodium_ infection. To enhance usability, we have implemented an automatic hyperparameter tuning pipeline for \(\) using grid search and spectral clustering, similar to contrastive PCA [Abid et al., 2018] (Appendix G).

Still, sisPCA has several limitations: **Linearity constraints:** The linear nature of sisPCA may miss non-linear feature interactions, potentially underperforming on more complicated datasets. While nonlinear extensions are possible (Appendix E), they come at the cost of reduced computational efficiency and interpretability. **Linear kernel HSIC:** The HSIC-linear regularization, while computationally convenient, does not guarantee complete subspace independence. However, minimizing HSIC-linear tends to reduce HSIC with a Gaussian kernel (Table 1), which suggests that the issue is less of a concern in practice. **Subspace identifiability:** Our formulation (1) relies on external supervision to differentiate subspaces, which could lead to identifiability issues if the supervisions are too similar, or when one subspace is unsupervised.

Despite these limitations, sisPCA's ability to provide interpretable, disentangled representations of complex biological data makes it a valuable addition to the toolkit of biologists working with high-dimensional datasets. We envision future applications on larger-scale omics datasets and potential new biomedical discoveries.

## 6 Acknowledgements

We thank Bianca Dumitirascu for early discussions and anonymous reviewers for their valuable comments. This work was funded by the National Institutes of Health, National Cancer Institute (R35CA253126, U01CA261822, and U01CA243073 to R. R.) and the Edward P. Evans Center for MDS at Columbia University (to J. S. and R. R.).

    & &  &  \\   & Subspace & PCA & sPCA & **sisPCA** & VAE & supVAE & hsVAE \\  Grassmann distance & & 0 & 3.771 & **4.824** & 0 & 3.571 & 3.598 \\ Silhouette - infection & infection & -0.014 & 0.207 & **0.235** & -0.036 & 0.041 & -0.015 \\  & time & -0.014 & -0.075 & **-0.097** & -0.036 & -0.087 & -0.069 \\ Silhouette - time point & infection & 0.311 & **-0.029** & **-0.028** & 0.296 & 0.052 & 0.015 \\  & time & 0.311 & 0.348 & 0.355 & 0.296 & 0.479 & **0.582** \\   

Table 3: Quantitative evaluation of subspace representation quality.

Figure 6: GO biological process enrichment results of top genes contributing to the sisPCA-infection subspace. Genes are ranked by their PC1 loading and are grouped by effect direction.