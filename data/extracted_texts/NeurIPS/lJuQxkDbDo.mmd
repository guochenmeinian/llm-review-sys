# DisenGCD: A Meta Multigraph-assisted Disentangled Graph Learning Framework for Cognitive Diagnosis

Shangshang Yang\({}^{1,2}\) Mingyang Chen\({}^{1,3}\) Ziwen Wang\({}^{1}\) Xiaoshan Yu\({}^{1}\)

Panpan Zhang\({}^{1}\) Haiping Ma\({}^{1,3}\)

Corresponding author.

Xingyi Zhang\({}^{1,3}\)

\({}^{1}\)Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education,

Anhui University, Hefei, Anhui 230601, P. R. China

\({}^{2}\) Anhui Province Key Laboratory of Intelligent Computing and Applications,

\({}^{3}\)Department of Information Materials and Intelligent Sensing Laboratory of Anhui Province

{yangshang0308, wzw12sir, yxsleo, zppan55, xyzhanghust}@gmail.com

q22201127@stu.ahu.edu.cn hpma@ahu.edu.cn

###### Abstract

Existing graph learning-based cognitive diagnosis (CD) methods have made relatively good results, but their student, exercise, and concept representations are learned and exchanged in an implicit unified graph, which makes the interaction-agnostic exercise and concept representations be learned poorly, failing to provide high robustness against noise in students' interactions. Besides, lower-order exercise latent representations obtained in shallow layers are not well explored when learning the student representation. To tackle the issues, this paper suggests a meta multigraph-assisted disentangled graph learning framework for CD (DisenGCD), which learns three types of representations on three disentangled graphs: student-exercise-concept interaction, exercise-concept relation, and concept dependency graphs, respectively. Specifically, the latter two graphs are first disentangled from the interaction graph. Then, the student representation is learned from the interaction graph by a devised meta multigraph learning module; multiple learnable propagation paths in this module enable current student representation to access lower-order exercise latent representations, which can lead to more effective nad robust student representations learned; the exercise and concept representations are learned on the relation and dependency graphs by graph attention modules. Finally, a novel diagnostic function is devised to handle three disentangled representations for prediction. Experiments show better performance and robustness of DisenGCD than state-of-the-art CD methods and demonstrate the effectiveness of the disentangled learning framework and meta multigraph module. The source code is available at https://github.com/BIMK/Intelligent-Education/tree/main/DisenGCD.

## 1 Introduction

In the realm of intelligent education , cognitive diagnosis (CD) plays a crucial role in estimating students' mastery/proficiency on each knowledge concept , which mainly models the exercising process of students by predicting students' responses based on their response records/logsand the exercise-concept relations. As shown in Figure 1, student Bob completed five exercises \(\{e_{1},e_{2},e_{3},e_{4},e_{5}\}\) and got corresponding responses, and his diagnosis result can be obtained through CD based on his records and the plotted relations between exercises and concepts. With students' diagnosis results, many intelligent education tasks can be benefited, such as exercise assembly , course recommendation [34; 45], student testing , and targeted training , and remedial instruction .

In recent years, many cognitive diagnosis models (CDMs) have been suggested based on various techniques, generally classified into two types. The first type of CDMs is based on the theory of educational psychology, and the representatives include DINA , (IRT) , and MIRT . Considering that the simple diagnostic functions of this type of models fail to model complex student interactions, recently, artificial intelligence researchers have tried to employ neural networks (NNs) to enhance and invent novel CDMs. Therefore, the second type of CDMs can be further divided into two sub-types: focusing on inventing novel diagnostic functions and enhancing the input representations of CDMs, respectively. The representatives of the first sub-type of CDMs include the well-known NCD , KSCD , and so on [44; 16; 12; 36; 38]. For the second sub-type of CDMs, many NN techniques are employed, including hierarchical attention NNs for ECD , long short-term memory networks for DIRT , graph neural networks (GNNs) for RCD , SCD , and GraphCDM .

Among existing CDMs, GNN-based ones exhibit significantly better performance than others, which is attributed to the rich information propagation and exchange brought by GNNs. These CDMs generally learn the student, exercise, and concept representations implicitly or explicitly on a unified graph [7; 28], which makes three types of representations be exchanged and aggregated fully even if they are distant regarding relations, thus generating better representations and providing better performance. However, learning exercise and concept representations should be student-interaction-agnostic , while the above CDMs' learning for exercise and concept representations is easily affected by students' interactions, and the representations will be learned poorly especially when there exists noise in students' interactions. In short, existing GNN-based CDMs fail to provide high robustness against student interaction noise. In addition, these CDMs do not explore the use of lower-order exercise latent representations in shallow layers for learning the student representation due to the intrinsic defect of their GNNs. The learned student representations could be more robust and not sensitive to student interaction data change.

Therefore, we propose a meta multigraph-assisted disentangled graph learning framework for CD (DisenGCD) to learn robust representations against interaction noise. The contributions include

(1) The disentangled graph learning framework DisenGCD learns three types of representations on three disentangled graphs. Specifically, the student representation is learned on the student-exercise-concept interaction graph; the exercise and concept representations are learned on two disentangled graphs: the exercise-concept relation graph and the concept dependency graph. By doing so, the latter two learned representations are interaction-agnostic and robust against student interaction noise.

(2) To make the best of lower-order exercise latent representations for learning the robust student representation, a modified meta multigraph module containing multiple learnable propagation paths is used, where propagation paths enable current student latent representation to access and use lower-order exercise latent representations. The exercise and concept representations are learned through common graph attention networks (GAT) on relation and dependency graphs, respectively. Finally, a novel diagnostic function is devised to handle three learned representations for prediction.

(3) Extensive experiments show the superiority of the proposed DisenGCD to state-of-the-art (SOTA) models regarding performance and robustness, and the effectiveness of the disentangled graph learning framework and the devised meta multigraph module is validated.

Figure 1: Cognitive diagnosis based on a student’s response records and exercise-concept relations.

Related Work

**Related Work on Cognitive Diagnosis**. The above has given the classification of CDMs, and we will briefly introduce typical CDMs, especially GNN-based ones. As representatives in educational psychology, IRT  (MIRT ) utilizes single (multiple) variable(s) to denote student's ability with logistic function for prediction. For NN-based representatives, NCD  and KSCD  fed student, exercise, and concept vectors to an IRT-like NN as the diagnostic function for prediction; NAS-GCD  is similar to NCD, but its diagnostic function is automatically obtained.

For GNN-based CDMs, their focus is on obtaining enhanced representations. For example, RCD  learns the student representation along the relation of adjacent exercise nodes, learns the exercise representation along the relation of adjacent student and concept nodes, and learns the concept representation along the relation of adjacent exercise and concept nodes. Since each node's information can be propagated to any node, learning regarding three relations can be seen as learning on an implicit unified graph; Similarly, SCD  takes the same aggregation manner along the first two relations of RCD for learning student and exercise representations for contrastive learning to mitigate long-tail problems. Despite the success of GNN-based CDMs, their learning manners of exercise and concept representations are not robust against student interaction noise, because the representations are learned together with student interactions in a unified graph, where the noise will prevent the representations from being learned well.

**Related Work on Meta Graph/Multigraph**. As can be seen, existing GNN-based CDMs intuitively adopt classical GNNs (GAT  or GCN) to learn three types of representations. These CDMs update the student representation by only using the exercise latent representation in the previous layer yet not exploring the use of lower-order exercise latent representations.

Compared to traditional GNNs, meta graph-based GNNs can make the target type of nodes access lower-order latent representations of their adjacent nodes. The meta graph is a directed acyclic graph built for a GNN, each node stores each layer's output (latent representations) of the GNN, and each edge between two nodes could be one of multiple types of propagation paths. The representatives include DiffMG  and GEMS . The meta multigraph is the same as the meta graph, but its each edge could hold more than one type of propagation path, where the representative is PMMM . To make the best of low-order exercise latent representations for learning effective and robust student representations, this paper adopts the idea of meta multigraph and devises a modified meta multigraph learning module for the updating of the student interaction graph.

**Related Work on Disentangling Graph Representation Learning**. Learning potential representations of disentangling in complex graphs to achieve model robustness and interpretability has been a hot topic in recent years. Researchers have put forward many disentanglement approaches (e.g., DisenGCN , DisenHAN , DGCF , DCCF , DcRec , etc.) to address this challenge. For example, in DisenHAN , the authors utilized disentangled representation learning to account for the influence of each factor in an item. They achieved this by mapping the representation into different spatial dimensions and aggregating item information from various edge types within the graph neural network to extract features from different aspects; DcRec  disentangles the network into a user-item domain and a user-user social domain, generating two views through data augmentation and ultimately obtaining a more robust representation via contrastive learning.

Despite many approaches suggested, they were primarily applied to bipartite graphs to learn different representations from different perspectives, for learning more comprehensive representations. While this paper aims to leverage disentanglement learning to mitigate the influence of the interaction noise in the interaction graph, and thus we proposed a meta multigraph-assisted disentangled graph cognitive diagnostic framework to learn three types of representations on three disentangled graphs. By doing so, the influence of the noise on exercise and concept learning can be well alleviated.

## 3 Problem Formulation

For easy understanding, two tables are created to describe all notations utilized in this paper including notations for disentangled graphs and notations for the meta multigraph, summarized in Table 4 and Table 5. Due to the page limit, the two tables are included in **Appendix**A.1.

### Disentangled Graph

This paper only employs the student-exercise-concept interaction graph \(}\) for students' representation learning, and disentangles two graphs from \(}\) for the remaining two types of representation learning, which are the exercise-concept relation graph \(}\), and the concept dependency graph \(}\).

**Student-Exercise-Concept Interaction Graph.** With students' response records \(\), exercise-concept relation matrix, and concept dependency matrix, the interaction graph \(}\) can be represented as \(}=\{,\}\). The node set \(=S E C\) is the union of the student set \(S\), exercise set \(E\), and concept set \(C\), while the edge set \(=_{se}_{ec}_{cc}\) contains three types of relations: \(rse_{ij} R_{se}\) represents the student \(s_{i} S\) answered exercise \(e_{j} E\), \(rec_{jk} R_{ec}\) denotes the exercise \(e_{j}\) contains concept \(c_{k} C\), and \(rcc_{km} R_{cc}\) denotes concept \(c_{k}\) relies on concept \(c_{m}\).

**Disentangled Relation Graph.** To avoid the impact of students' interactions \(_{se}\) on exercise representation learning, the exercise-concept relation graph \(}\) is disentangled from \(}\), denoted as \(}=}/\{S,_{se}\}=\{E C, _{ec}_{cc}\}\).

**Disentangled Dependency Graph.** Similarly, a concept dependency graph \(}\) is further disentangled from \(}\) to learn the concept representation without the influence of interactions \(_{se}\) and exercises' relation \(_{ec}\), which is represented by \(}=}/\{E,_{ec}\}=\{C,_{cc}\}\).

### Problem Statement

For the cognitive diagnosis task in an intelligent education online platform, there are usually three sets of items: a set of \(N\) students \(S=\{s_{1},s_{2},,s_{N}\}\), a set of \(M\) exercises \(E=\{e_{1},e_{2},,e_{M}\}\), and a set of \(K\) knowledge concepts (concept for short) \(C=\{c_{1},c_{2},,c_{K}\}\). Besides, there commonly exists two matrices: exercise-concept relation matrix \(Q=(Q_{jk}\{0,1\})^{M K}\) (Q-matrix) and concept dependency matrix \(D=(Q_{km}\{0,1\})^{K K}\), to show the relationship of exercises to concepts and concepts to concepts, respectively. \(Q_{jk}=1\) denotes the concept \(c_{k}\) is not included in the exercise \(e_{j}\) and \(Q_{jk}=0\) otherwise; similarly, \(D_{km}=1\) denotes concept \(c_{k}\) relies on concept \(c_{m}\) and \(D_{km}=0\) otherwise. All students' exercising reponse logs are denoted by \(=\{(s_{i},e_{j},r_{ij})|s_{i} S,e_{j} E,r_{ij}\{0,1\}\}\), where \(r_{ij}\) refers to the response/answer of student \(s_{i}\) on exercise \(e_{j}\). \(r_{ij}=1\) means the answer is correct and \(r_{ij}=0\) otherwise.

The CD based on disentangled graphs is defined as follows: **Given**: students' response logs \(\) and three disentangled graphs: interaction graph \(}\), relation graph \(}\), and dependency graph \(}\); **Goal**: revealing students' proficiency on concepts by predicting students' responses through NNs.

## 4 Method

For better understanding, Figure 2 presents the overall architecture of the proposed DisenGCD, where three learning modules are used for learning three types of representations on three disentangled graphs and the diagnostic function makes the final prediction based on the learned representations. Specifically, these three learning modules are: (1) a meta multigraph-based student learning module, (2) a GAT-based exercise learning module, and (3) a GAT-based concept learning module. In each learning module, the corresponding graph's embedding is randomly first initialized. Then, the meta multigraph-based student learning module is employed to learn the student representation \(_{i}}\) based on interaction graph \(}\); the GAT-based exercise learning module is used to learn the exercise representation \(_{j}}\) based on relation graph \(}\); while the concept representation \(_{k}}\) is learned by the GAT-based concept learning module on dependency graph \(}\) or the naive embedding if \(}\) is unavailable. Finally, a devised novel diagnostic function receives three learned representations \(_{i}}\), \(_{j}}\), and \(_{k}}\) to get the prediction \(}\) of student \(s_{i}\) on exercise \(e_{j}\).

Note that the first module employs a modified meta multigraph aggregator to learn student representations. Compared to traditional graph learning, the module contains multiple learnable propagation paths, which enable the student latent representation to be learned currently to access and use lower-order exercise latent representations, leading to a more effective and robust student representation.

### The Meta Multigraph-based Student Learning Module

Based on interaction graph \(}\), this module is responsible for: learning the optimal meta multigraph structure in \(}\), i.e., optimal multiple propagation paths, and updating student nodes' representations based on the learned meta multigraph structure to get the representation \(_{i}}^{1 d}\).

To start with, all nodes (including students, exercises, and concepts) need to be mapped into a \(d\)-dimensional hidden space. The student embedding \(_{i}^{1 d}\) for student \(s_{i}\), exercise embedding \(_{j}^{I}^{1 d}\) for exercise \(e_{j}\), and concept embedding \(_{k}^{I}^{1 d}\) for concept \(c_{k}\) can be obtained by

\[_{i}=_{i}^{S} W_{S}^{I},_{j}^{I}=_{j}^{E} W_{E}^{I},_{k}^{I}=_{k}^{C} W_{C}^{I},W_ {S}^{I}^{N d},W_{E}^{I}^{M d},W_{C}^{I} ^{K d}.\] (1)

\(_{i}^{S}\{0,1\}^{1 N}\), \(_{j}^{E}\{0,1\}^{1 M}\), and \(_{k}^{C}\{0,1\}^{1 K}\) are three one-hot vectors for student \(s_{i}\), exercise \(e_{j}\), concept \(c_{k}\), respectively. \(W_{S}^{I}\), \(W_{E}^{I}\), and \(W_{C}^{I}\) are three learnable parameter matrices.

#### 4.1.1 The Meta Multigraph Aggregator

To learn more useful representations from the above initial embedding, we construct a meta multigraph \(=\{,\}\) to specify a set of propagation paths for all nodes' aggregation in graph learning.

As shown in Figure 2, the meta multigraph is actually a directed acyclic graph containing \(P\) hyper-nodes: \(=\{^{(1)},,^{(p)},,^{(P)}\}\). \(^{(p)}=\{^{p}^{N d},^{p} ^{M d},^{p}^{K d}\}\) is a set of latent representations stored in \(p\)-th hyper-node for all students, exercises, and concepts, whose \(i\)-th row \(_{i}^{p}^{1 d}\), \(j\)-th row \(_{j}^{p}^{1 d}\), and \(k\)-th row \(_{k}^{p}^{1 d}\) are the \(p\)-th latent representations of student \(s_{i}\), exercise \(e_{j}\), and concept \(c_{k}\), respectively. Especially when \(p\)=1, \(_{i}^{p}\), \(_{j}^{p}\), and \(_{k}^{p}\) equal initial embedding \(_{i}\), \(_{j}^{I}\), and \(_{k}^{I}\).

Each edge \(AP_{uv}\) between two hyper-nodes \(H^{(u)}\) and \(H^{(v)}\) contains multiple types (\(|HR|\)) of propagation paths, which can be denoted as \(AP_{uv}=\{(hr_{a},hr_{a}^{wei})|1 a|HR|\}\). Here \(hr_{a}\) is \(a\)-th type of propagation path in candidate path set \(HR\), and \(hr_{a}^{wei}\) represents the weight of \(hr_{a}\). This paper adopts \(|HR|\)=7 types of propagation paths as the candidate paths for \(HR\): 1) the students to exercises path \(A_{se}\), 2) the exercises to students path \(A_{es}\), 3) the exercises to concepts path \(A_{ek}\), 4) the concepts to exercises path \(A_{ke}\), 5) the concepts to concepts path \(A_{kk}\), 6) the identity path \(I\), 7) the zero path \(zero\).

Figure 2: Overview of the proposed DisenGCD: the disentangled graph learning framework is composed of three learning modules: two GAT-based learning modules and a meta multigraph-based learning module, where the latter’s details are shown in the green part.

Here, path \(A_{se}\) means updating the exercise latent representation along the propagation path of student nodes to exercise nodes, and other paths hold similar meanings. As a result, \(HR=\{A_{se},A_{es},A_{ek},A_{ke},A_{kk},I,zero\}\), and the edge set \(\) in \(\) can be denoted by \(=\{AP_{uv}|1 u<v P\}\), containing \(|HR|*[P*(P-1)/2]\) propagation paths. With above propagation paths, \(^{(p)}\) can be updated by

\[^{(p)}=\{f(_{up},^{(u)})|\;1 u<p\},\] (2)

where \(f()\) is the aggregation function of GCN. \(f(_{up},^{(u)})\) refers to that obtaining the latent representations \((_{i}^{p},_{j}^{p},_{k}^{p})\) in hyper-node \(^{(p)}\) based on all latent representations stored in previous hyper-nodes (i.e., \(^{(1)}\) to \(^{(p-1)}\)). It can be seen that the updating process of \(^{(p)}\) replies on latent representations in multiple hyper-nodes and multiple propagation paths, which compose **the modified meta multigraph aggregator** together.

For better understanding, here is an example: when \(p\)=3, \(AP_{13}=\{zero\}\) and \(AP_{23}=\{A_{es},I\}\), the updating of three latent representations \(_{i}^{3}\), \(_{j}^{3}\), and \(_{k}^{3}\) is denoted as

\[AP_{13}:\{_{i}^{3}(13)0*_{i}^{1},_{j}^{3}(13)0*_{j}^{1},_{k}^{3}(13) 0*_{k}^{1}.\] (3) \[AP_{23}:._{i}^{3}(23)=Up(_{i}^{2},_{j N_{s_{i}}}Mess(_{i}^{2},e_{j}^{2}))\\ _{j}^{3}(23)=_{j}^{2},_{k}^{3}(23)=_{k }^{2}.\] \[_{i}^{3}=_{i}^{3}(13)+_{i}^{3}(23), \;_{j}^{3}=_{j}^{3}(13)+_{j}^{3}(23),\;_{k}^{3}=_{k}^{3}(13)+_{k}^{3}(23)\]

The first equation is to update three latent representations according to \(AP_{13}\), while the second is to get the updating based on \(AP_{23}\). The final latent representations are obtained by summing up both updated ones. Here \(Up()\) and \(Mess()\) are common update and message-passing functions.

#### 4.1.2 Routing Strategy

To find suitable propagation paths, threshold \(^{(u,v)}\) is created for each pair of hyper-nodes \((u,v)\):

\[^{(u,v)}=(Softmax(AP_{uv}))+(1-)(Softmax(AP _{uv})),\;.\] (4)

\(Softmax(AP_{uv}))\) normalizes weights of each type of propagation path regarding their \(hr_{a}^{wei}\) values. By doing so, the propagation paths of each pair of hyper-nodes will remain if their \(hr_{a}^{wei}\) values are greater than the corresponding threshold. Thus the updating process of \(^{(p)}\) can be rewritten as

\[^{(p)} =\{f(A_{up},^{(u)})||\;1 u<p\}\] (5) \[A_{up} =\{(hr_{a},hr_{a}^{wei})|hr_{a}^{wei}^{(u,p)}, hr_ {a} AP_{up}\}.\]

Finally, the learned student representation \(^{P}\) in \(^{P}\) are used for the diagnosis, i.e, \(_{i}^{P}\) is used as \(_{i}}\).

### The GAT-based Exercise Learning Module and GAT-based Concept Learning Module

**GAT-based Exercise Learning Module**. This module is responsible for learning the exercise representation \(_{j}}^{1 d}\) on the relation graph \(}\) via a \(L\)-layer GAT network . Firstly, the embedding of exercises and concepts in \(}\) is obtained in the manner same as Eq.(1) through two learnable matrices \(W_{E}^{R}^{M d}\) and \(W_{C}^{R}^{K d}\), i.e, \(_{j}^{R}=_{j}^{E} W_{E}^{R},_{k}^{R}= _{k}^{C} W_{C}^{R}\).

Afterward, the GAT neural network is applied to aggregate neighbor information to learn the exercise representation. The aggregation process of \(l\)-th layer (\(1 l L\)) can be represented as

\[_{j}^{R(l)} =_{k N_{s_{j}}}_{j(k)}^{R(l)}_{k}^{R(l-1)}+ _{j}^{R(l-1)},\] (6) \[_{k}^{R(l)} =_{j N_{c_{k}}^{c_{k}}}_{k(l)}^{R(l)}_{j}^{ R(l-1)}+_{m N_{c_{k}}^{c_{k}}}_{(m)}^{R(l)}_{m}^{R(l-1)}+ _{k}^{R(l-1)}.\]

The first equation is to aggregate the information of exercise \(e_{j}\)'s neighbors \(N_{e_{j}}\) to get its \(l\)-th layer's latent representation \(_{j}^{R(l)}^{1 d}\), while the second is to update the \(l\)-th layer's concept latent representation \(_{k}^{R(l)}^{1 d}\) from its exercise neighbors \(N_{c_{k}}^{ec}\) and concept neighbors \(N_{c_{k}}^{ec}\). \(_{j}^{R(l)}\)\(_{k}^{R(l)}\), and \(_{k}^{R(l)}\) are the \(l\)-th layer's attention matrices for exercise \(e_{j}\)'s concept neighbors, concept \(c_{k}\)'s exercise neighbors, and \(c_{k}\)'s concept neighbors. They can be obtained in the same manner, and the \(k\)-th row of \(_{j}^{R(l)}\) can be obtained by

\[_{j(k)}^{R(l)}=(F_{ec}([_{j}^{R(l-1)},_{k}^{R(l-1)}])), k N_{e_{j}}.\] (7)

\([]\) is the concatenation and \(F_{ec}()\) is a fully connected (FC) layer mapping \(2 d\) vectors to scalars.

The latent representation \(_{j}^{R(0)}\) and \(_{k}^{R(0)}\) refer to \(_{j}^{R}\) and \(_{k}^{R}\), and the \(L\)-th layer output \(_{j}^{R(L)}\) is used as \(_{j}}\). By disentangling the interaction data, the learned exercise representation \(_{j}}\) will be more robust against interaction noise.

**GAT-based Concept Learning Module**. Similarly, this module obtains the concept representation \(_{k}}^{1 d}\) by applying a \(L\)-layer GAT network to the dependency graph \(}\). After obtaining the initial embedding \(_{k}^{D}=_{k}^{C} W_{C}^{D}\) by a learnable matrix \(W_{C}^{D}^{K d}\), this module updates the \(l\)-th layer's latent representation \(_{k}^{D(l)}\) by \(_{k}^{D(l)}=_{m N_{c_{k}}^{ec}}_{k(l)}^{D(l)}_{m(l)}^{D(l-1)}+_{k}^{D(l-1)}\). \(_{k}^{D(l)}\) denotes the attention matrix, which can be computed as same as \(_{}()\).

Here \(_{k}^{D(0)}\) is equal to \(_{k}^{D}\), and the \(L\)-th layer output \(_{k}^{D(l)}\) is used as \(_{k}}\). If graph \(}\) is unavailable, this module will directly take the initial embedding \(_{k}^{D}\) as \(_{k}}\). By further disentangling, the learned representation \(_{k}}\) may be robust against noise in student interactions to some extent.

### The Diagnosis Module

To effectively handle the obtained three types of representations, a novel diagnostic function is proposed to predict the response \(}\) of student \(s_{i}\) got on exercise \(e_{j}\) as follows:

\[_{simi}=(F_{simi}(_{si} _{ej})),_{si} =F_{si}(_{i}}+_{k}}), _{ej}=F_{ej}(_{j}}+_{k}})\] (8) \[} =( Q_{k}_{simi})/ Q_{k}\]

where \(F_{si}()\), \(F_{ej}()\), and \(F_{simi}()\) are three FC layers mapping a \(d\)-dimensional vector to another one, and \(Q_{k}\) is a binary vector in the \(k\)-th row of Q-matrix.

Here \(_{si}^{1 d}\) can be seen as the student's mastery of each knowledge concept; while the obtaining of \(_{ej}^{1 d}\) aims to get the exercise difficulty of each concept; \(_{simi}\) is to measure the similarity between \(_{si}\) and \(_{ej}\) via a dot-product followed by an FC layer and a Sigmoid function \(()\), where a higher similarity value in each bit represents a higher mastery on each concept, further indicating a higher probability of answering the related exercises; the last equation follows the idea of NCD to compute the overall mastery averaged over all concepts contained in exercise \(e_{j}\).

We can see that the proposed diagnostic function has as high interpretability as NCD, IRT, and MIRT.

**Model Optimization.** With the above modules, the proposed DisenGCD are trained by solving the following bilevel optimization problem through Adam :

\[_{}_{val}(D_{val}|^{*}(), ),\ \ \ ^{*}()=_{}_{ train}(D_{train}|,),\] (9)

where \(_{val}()\) and \(_{train}()\) denote the loss on validation dataset \(D_{val}\) and training dataset \(D_{train}\). \(\) denotes all model parameters, and \(\) denotes the weights of learnable propagation paths \(\) in meta multigraph \(MG\). The cross-entropy loss  is used for \(_{val}\) and \(_{train}\).

## 5 Experiments

This section answers the following questions: **RQ1**: How about the performance of DisenGCD compared to SOTA CDMs? **RQ2**: How about the DisenGCD's robustness against noise and the disentangled learning framework's effectiveness in DisenGCD? **RQ3**: How about the effectiveness of the devised meta multigraph learning module? **RQ4**: How does the learned meta multigraph on target datasets look like, and how about their generalization on other datasets? In addition, **more experiments to validate the proposed DisenGCD's effectiveness are in the Appendix.

[MISSING_PAGE_FAIL:8]

the performance change of DisenGCD and RCD is much smaller than NCD and ECD, which signifies DisenGCD and RCD are more robust to different-sparsity data. To validate its sparsity superiority, **more experiments** are summarized in **Appendix B.1**.

### Effectiveness of Disentangled Learning Framework of DisenGCD (RQ2)

To investigate DisenGCD's robustness against interaction noise, we conducted robust experiments on the ASSISTments dataset under the ratio of 60%/10%/30%, where a certain amount of noise interactions were added to each student in the training and validation datasets. Figure (a)a presents the ACC and AUC of DisenGCD and RCD under noise data of different percentages. As the noise data increases, the performance leading of DisenGCD over RCD becomes more significant, which reaches maximal when noise data of 50% was added. That demonstrates DisenGCD is more robust to student noise interactions than RCD, attributed to the disentangled learning framework of DisenGCD. Similar observations can be drawn from **more experiments on other two datasets in Appendix B.2**.

To further analyze the framework effectiveness, four variants of DisenGCD were created: _DisenGCD(I)_ refers to learning three representations only on the interaction graph \(}\); _DisenGCD(Is+Rec)_ refers to learning student representation on \(}\), but learning exercise and concept ones on the relation graph \(}\); _DisenGCD(Ise+Rc)_ refers to learning student and exercise representations on \(}\) but learning concept one on \(}\); while _DisenGCD(Isc+Re)_ learns student and concept representations on \(}\) but learns exercise one on \(}\). Table 3 compares the performance of RCD, DisenGCD, and its four variants on ASSISTments. As can be seen, the comparison between _DisenGCD(I)_ and other variants indicates learning three representations in two disentangled graphs is more effective than in one unified graph, especially with textitDisenGCD(Is+Rec); the comparison between _DisenGCD(Is+Rec)_ and DisenGCD indicates learning three representations in three disentangled graphs is more effective, further validating the above conclusion. Finally, we can conclude that the proposed disentangled learning framework is effective in enhancing DisenGCD's performance and robustness.

### Effectiveness of Meta Multigraph Learning Module in DisenGCD (RQ3)

In Table 3, both _DisenGCD(I)_ and RCD learn three types of representations in one unified graph, but _DisenGCD(I)_ utilizes the meta multigraph aggregator. Therefore, the comparison between _DisenGCD(I)_ and RCD proves the devised meta multigraph learning module is effective in improving the model performance to some extent.

To further validate this, we created three variants of DisenGCD: _DisenGCD(naive)_ refers to the meta multigraph module replaced by the naive embedding (i.e., \(}}\) equal to \(_{i}\) in Eq.(1)); _Dis

Figure 3: (a): Performance of RCD and DisenGCD under different noises. (b): Effectiveness of the meta multigraph learning module.

    &  &  & _DisenGCD_ & _DisenGCD_ & _DisenGCD_ & \\  & & & _(Is+Rec)_ & _(Ise+Rc)_ & _(Isc+Re)_ & DisenGCD \\  ACC\(\) & 0.7291 & 0.7331 & 0.7321 & 0.7301 & 0.7333 & **0.7335** \\ RMSE\(\) & 0.4262 & 0.4235 & 0.4259 & 0.4235 & 0.4231 & **0.4219** \\ AUC\(\) & 0.7663 & 0.7678 & 0.7701 & 0.7678 & 0.7685 & **0.7723** \\   

Table 3: Performance of DisenGCD, RCD, and its four variants on ASSISTments dataset.

_enGCD(mp)_ refers to predefining the meta multigraph module's propagation paths according to HAN ; _DisenGCD(mg)_ refers to this module's meta multigraph replaced by meta graph. Figure 3b presents the AUC and ACC values of DisenGDCN and its variants on two datasets under the ratio of 60%/10%/30%. As can be seen, the devised meta multigraph learning module enables DisenGDCN to hold significantly better performance than the naive embedding-based variant. The comparison results between DisenGCD and _DisenGCD(mg)_ as well as _DisenGCD(mp)_ validates the effectiveness of using meta multigraph and the automatically learned propagation paths (i.e., learned meta multigraph). Thus, the effectiveness of the devised meta multigraph module can be validated.

### Visualization and Effectiveness of Learned Meta Multigraph (RQ4)

The comparison between _DisenGCD(mp)_ and DisenGCD has revealed the effectiveness of the learned meta multigraph on the target dataset. Therefore, an intuitive doubt naturally emerged: Is the learned meta multigraph still effective on other datasets? Before solving this, Figure 4a gives the structure visualization of two learned meta multigraphs. We can see two learned meta multigraphs hold two distinct structures. That may be because two datasets are a bit different regarding the relations between exercises and concepts, where exercises in Math contain only one concept while exercises in ASSISTments may contain more than one concept. Thus, another doubt naturally emerged: Is the learned meta multigraph on the target dataset ineffective on a different type of dataset?

To solve the doubts, we applied six learned meta multigraphs (A1, A2, A3, M1, M2, and M3) to the SLP. A1-A3 and M1-M3 were learned by DisenGCD three times on ASSISTments and Math. SLP dataset is similar to Math, whose exercises only contain one concept. Figure 4b summarizes the results of RCD and six DisenGCD variants that utilize six given meta multigraphs. As can be seen, DisenGD's performance under M1-M3 is promising and better than RCD, while DisenGCD under A1-A3 performed poorly. The observation can answer the above doubts to some extent: meta multigraphs learned by DisenGCD may be effective when the datasets to be applied are similar to target datasets.

In addition to the above four experiments, **more experiments** were executed to validate the devised diagnostic functions, analyze the DisenGCD's parameter sensitivity, analyze its execution efficiency, and investigate the effectiveness of the employed GAT modules in **Appendix B.3**, **Appendix B.4**, and **Appendix B.6**.

## 6 Conclusion

This paper proposed a meta multigraph-assisted disentangled graph learning framework for CD, called DisenGCD. The proposed DisenGCD learned student, exercise, and concept representations on three disentangled graphs, respectively. It devised a meta multigraph module to learn student representation and employed two common GAT modules to learn exercise and concept representations. Compared to SOTA CDMs on three datasets, the proposed DisenGCD exhibited highly better performance and showed high robustness against interaction noise.

Figure 4: (a): Visualization of learned meta multigraph on two datasets. (b): Generalization validation of six learned meta multigraphs.