# FedMeKI: A Benchmark for Scaling Medical Foundation Models via Federated Knowledge Injection

Jiaqi Wang\({}^{1}\)  Xiaochen Wang\({}^{1}\)  Lingjuan Lyu\({}^{2}\)  Jinghui Chen\({}^{1}\)  Fenglong Ma\({}^{1}\)

\({}^{1}\)Pennsylvania State University, \({}^{2}\)Sony AI

{jqwang, xcwang, jzc5917, fenglong}@psu.edu,lingjuan.lv@sony.com

https://github.com/psudslab/FEDMEKI

The first two authors contributed equally to this work.

###### Abstract

This study introduces the Federated Medical Knowledge Injection (FedMeKI) platform, a new benchmark designed to address the unique challenges of integrating medical knowledge into foundation models under privacy constraints. By leveraging a cross-silo federated learning approach, FedMeKI circumvents the issues associated with centralized data collection, which is often prohibited under health regulations like the Health Insurance Portability and Accountability Act (HIPAA) in the USA. The platform is meticulously designed to handle multi-site, multi-modal, and multi-task medical data, which includes 7 medical modalities, including images, signals, texts, laboratory test results, vital signs, input variables, and output variables. The curated dataset to validate FedMeKI covers 8 medical tasks, including 6 classification tasks (lung opacity detection, COVID-19 detection, electrocardiogram (ECG) abnormal detection, mortality prediction, sepsis prediction, and enlarged cardiomediastinum detection) and 2 generation tasks (medical visual question answering (MedVQA) and ECG noise clarification). This comprehensive dataset is partitioned across several clients to facilitate the decentralized training process under 16 benchmark approaches. FedMeKI not only preserves data privacy but also enhances the capability of medical foundation models by allowing them to learn from a broader spectrum of medical knowledge without direct data exposure, thereby setting a new benchmark in the application of foundation models within the healthcare sector.

## 1 Introduction

Foundation models have revolutionized various domains by demonstrating powerful capabilities in handling different modalities and tasks. Models such as GPT-3  and LLaMA  have shown exceptional performance across a wide range of applications, including natural language processing, image classification, and multimodal reasoning. The primary reason for their success is their exposure to vast amounts of training data, enabling them to acquire a deep understanding of diverse domains. Leveraging this extensive data allows foundation models to generalize effectively and perform well across various tasks, making them invaluable in fields like healthcare, finance, and education. Furthermore, the scale of their training enables these models to capture nuanced relationships within data, enhancing their ability to perform high-level reasoning and decision-making. Consequently, foundation models serve as robust baselines and starting points for more specialized AI applications, fostering innovation and accelerating advancements across numerous domains.

In the medical domain, there have been attempts to develop medical foundation models that replicate the success seen in general domains [3; 4; 5]. However, the limited availability of public medicaldata restricts the ability to train medical foundation models from scratch. To address this challenge, researchers have proposed fine-tuning general foundation models with medical data to customize medical foundation models. For instance, PMC-LLaMA  fine-tunes LLaMA with 4.8 million biomedical academic papers and 30,000 medical books. Similarly, LLaVA-Med  fine-tunes LLaVA  with biomedical image-text pairs extracted from PMC-15M . Although existing medical foundation models have achieved superior performance on various domain-specific tasks, their scalability remains limited due to the current fine-tuning methods.

As previously discussed, most medical foundation models require fine-tuning existing general domain foundation models in a centralized training manner. However, due to the sensitivity and privacy issues of medical data, such centralized fine-tuning is unrealistic in real-world healthcare settings. Health regulations, such as the Health Insurance Portability and Accountability Act (HIPAA) in the USA, prohibit the collection and central storage of patient data for model training. In practice, medical data are stored at individual health institutions or hospitals and cannot typically be shared with others. Therefore, a more practical and realistic solution is to collaboratively inject medical knowledge learned from private client data into foundation models in a federated manner.

**A New Task.** To achieve this goal, we introduce a new task to scale existing medical foundation models, named **Fed**erated **M**edical **K**nowledge **I**njection into foundation models (FedMeKI). In this task, each client stores a set of private multi-modal, multi-task medical datasets, while the server hosts a medical foundation model. The objective is to inject client medical knowledge into the foundation model without sharing their private data. This new task presents several unique challenges compared to existing medical foundation model fine-tuning methods.

_C1 - Data Fine-tuning vs. Parameter Adaptation._ This new task prohibits the sharing of private data among clients. To extract medical knowledge from these clients, a straightforward solution is to treat the learned client model parameters as a new format of medical knowledge, which will be uploaded to the server for knowledge injection. However, the foundation model deployed on the server has different network structures from the client models, making it impossible to perform averaging operations like FedAvg . The challenge here is to adapt client model parameters to the foundation model.

_C2 - Task-specific Fine-tuning vs. Scalable Fine-tuning._ Existing medical foundation models can only handle task-specific downstream tasks. For instance, LLaVA-Med is fine-tuned for medical vision question answering (VQA) tasks, including VQA-RAD , SLAKE , and PathVQA . Similarly, PMC-LLaMA can only handle tasks that use text inputs, including PubMedQA , MedMCQA , and USMLE . In addition to medical images and text, complex medical data include other commonly used modalities, such as medical signals and lab results, which existing medical foundation models often miss. Therefore, this new task is crucial for enabling the simultaneous fine-tuning of medical foundation models with diverse modalities.

**A Comprehensive Medical Dataset.** To address the aforementioned challenges and benchmark this new task, we first curated a new multi-site, multi-modal, multi-task dataset. This dataset covers **eight diverse medical tasks**: lung opacity detection , COVID-19 detection , ECG abnormal detection , mortality prediction , sepsis prediction , enlarged cardiometabolicinum detection , MedVQA , and signal noise clarification . These tasks span **seven medical modalities**: medical images, medical texts, medical signals, laboratory test results, vital signs, input variables, and output variables, extracted from **seven publicly available datasets** (RSNA , COVQU , PTB-XL , MIMIC-III , CheXpert , VQA-RAD , and ECG-QA ). We divided the tasks in our dataset into training tasks and validation tasks. The training tasks aim to inject modality-level knowledge into medical foundation models, while the validation tasks evaluate the ability of zero-shot inference for the knowledge-injected medical foundation models. The data is distributed to several clients, following a cross-silo federated learning setting similar to FLamby , due to the typically small size of medical datasets.

**A Novel Federated Knowledge Injection Platform.** We have developed a new FedMeKI platform to address this new task with the curated dataset, as shown in Figure 1. Specifically, the platform is equipped with the functionalities of multi-modal multi-task data preprocessing, multi-site data partition, multi-modal multi-task client training, and medical foundation model federated scaling. Besides, it implements 16 methods as benchmarks to evaluate the platform, including traditional federated learning, federated learning with fine-tuning, and federated learning with foundation model scaling. To sum up, the contributions of this work are fourfold:* We investigate an important and practical task in the medical domain, aiming to inject medical knowledge into medical foundation models in a cross-silo federated manner, thereby scaling the capability of medical foundation models while ensuring privacy.
* We curate a new dataset from seven publicly available data sources, which covers eight diverse medical tasks (single-modal and multi-modal classification and generation tasks) with seven medical modalities.
* We build an open-source federated medical knowledge injection platform FedMeKI for benchmarking this new task with the curated dataset. The FedMeKI platform can be easily scaled with new medical tasks and integrates different federated learning algorithms.
* We implement 16 different approaches as benchmark baselines to validate the FedMeKI platform in two scenarios: four training task evaluations to evaluate its task-specific capabilities and four validation task evaluations to assess its ability for zero-shot inference.

## 2 Related Work

**Federated Learning with Medical Data.** Medical data containing highly sensitive patient information is rigorously protected by various regulations and laws, making centralized access and processing impractical for machine learning model training. Federated learning [10; 25; 26; 27], a distributed paradigm, enables participants to train machine learning models without exchanging data. This approach has been extensively applied in medical tasks using different types of medical data, such as electronic health records (EHRs) [28; 29; 30; 31] and medical imaging [32; 33; 34]. There are a range of applications of federated learning in healthcare, encompassing disease prediction [35; 36; 37; 38], medical image classification [39; 40; 41], and segmentation [42; 43]. Additionally, several surveys have reviewed related advancements [44; 45; 46; 38]. To date, only one benchmark  has investigated the application of federated learning specifically to medical data. Notably, **no research** has yet explored the scalability of medical foundation models within a federated framework.

**Medical Foundation Models.** Foundation models, characterized by their extensive parameters and vast training datasets, have demonstrated remarkable capabilities across various domains [2; 47; 48; 49; 50]. In the realm of healthcare [51; 52], these models are increasingly prevalent. Thirunavukarasu et al. (2023)  discuss the potential of large language models (LLMs) in clinical settings, highlighting their effectiveness in healthcare applications. Moor et al. (2023)  introduce the concept of a generalist medical AI, designed to handle diverse tasks using multimodal medical data. Additionally, specialized medical foundation models have been developed for targeted applications such as disease detection using retinal images , cancer imaging biomarker identification , echocardiogram interpretation , medical image segmentation , and precision oncology . Despite these advancements, there remains **a gap in research** concerning the development of datasets and benchmarks that enable medical foundation models to integrate and leverage medical knowledge from distributed data sources.

Figure 1: Overview of our proposed FedMeKI platform.

**Federated Fine-tuning with Foundation Models.** To achieve better performance in specific tasks, fine-tuning foundation models (FMs) with task-specific data is essential. FL facilitates this fine-tuning process by allowing the use of locally stored data through distributed computational resources . Existing related research can be categorized into full tuning [59; 60], partial tuning [61; 62; 63], and parameter-efficient fine-tuning (PEFT) [64; 65]. In , each client has a foundation model and exchanges the adapters with the server in each communication round. The server conducts the basic FedAvg on the adapter and sends it back to the clients. Similarly, FedPETuning  provides a PEFT approach on pre-trained language models via sharing part of the client models in FL. The aforementioned studies typically require clients to possess FMs, with the aim of mutual benefits. In contrast, our approach places the medical FM on the server side, representing a more practical setting. Moreover, our objective is to enable clients to collaboratively contribute to **scaling the capability** of the medical FM models without accessing local data.

## 3 The FedMeKI Platform

As shown in Figure 1(a), the designed FedMeKI platform consists of several clients \(\{C_{1},,C_{n}\}\) and a server \(S\). Each client \(C_{n}\) trains a specific model \(_{n}\) using private data \(_{n}\), which can be treated as the knowledge representation of the client. The trained client models \(\{_{1},,_{N}\}\) will be uploaded to the server. After receiving the client models, the server will inject the aggregated medical knowledge representation by \(_{s}\) into the medical foundation model \(\) using the public data \(_{p}\). The updated global model \(_{s}\) will be distributed to each client again for the learning of the next communication round until convergence.

### Client Deployment

The goal of FedMeKI is to inject medical knowledge learned from private multi-modal multi-task data \(_{n}\) into the foundation model \(\). We deploy a basic client model \(_{n}\) to handle the multi-modal multi-task data to achieve this goal.

**Modality-specific Encoders.** Although we have five training tasks for each client, some tasks share the same modality. For example, both ECG abnormal detection  and ECGQA  tasks have the signal ECG modality. To avoid the redundancy of modality encoders and learn shared features across tasks, we propose to deploy modality-specific encoders. The details of these encoders are shown in Appendix Section N. Let \((_{n}^{i},_{n}^{i})_{n}\) denote a training sample. Only the task-associated encoders will generate outputs, and the output of an encoder is denoted as \(_{n}^{m}(_{n}^{i})\) (\(m[1,M]\)), where \(M\) is the number of unique modalities. We finally obtain the task-specific representation of each data sample \(_{n}^{i}\) by concatenating outputs from task-associated encoders.

**Task-specific Decoders.** Each task has a unique decoder \(_{n}^{t}(_{n}^{i})\) to generate the outcome and we use cross-entropy as the loss. The details of each task-specific decoder are shown in Appendix of Section N.

**Federated Optimization.** The ground truth \(_{n}^{i}\) will be used to optimize the client model \(_{n}\) with the cross-entropy loss for all training tasks. Since there are several ways to conduct federated learning, we use FedAvg  and FedProx  as examples to demonstrate how FedMeKI works in this study.

\(\)**FedAvg** aims to collaboratively train each client separately and upload their model parameters \(\{_{1},,_{N}\}\) directly to the server.

\(\)**FedProx** is developed based on FedAvg but added an \(L_{2}\) regularization term on each local loss function as follows

\[_{_{n}}_{n}(_{n};_{s})=_{n}(_{n})+||_{n}-_{s}||^{2},\] (1)

where \(_{s}\) is the global model, \(_{n}()\) is the client loss function, and \(\) is a hyperparameter. The learned client parameters \(\{_{1},,_{N}\}\) will be uploaded to the server. Since the designed FedMeKI platform is general, we can use any FedAvg-style approaches, including _personalized FL_ methods [67; 68], _differential privacy-based FL_ methods [69; 70], and _adaptive FL_ methods [71; 72; 73].

### Server Deployment

We deploy a model aggregator on the server to aggregate client models \(\{_{1},,_{N}\}\) and a LLaVA-style module to inject medical knowledge with the help of public data.

**Client Model Aggregation.** We still follow FedAvg-style approaches to obtain the aggregated global model \(_{s}\) using the averaging of all client models, i.e., \(_{s}=_{n=1}^{N}_{n}\).

**Scaling Medical Foundation Model \(\).** We deploy a medical foundation model \(\) on the server. Note that it can be _any of the existing medical foundation models_, such as MedVInT  and ChatDoctor . The current platform uses MMedLM-22  as \(\), which is a pretrained language model for medicine and achieves state-of-the-art performance on several tasks. MMedLM-2 can only take text as the input. Our goal is to enable \(\) to work on tasks with other modalities.

To this end, we follow the LLaVA's fine-tuning style to generate the representation \(_{p}^{j}\) of a public data sample \((_{p}^{j},_{p}^{j})_{p}\) using the encoder of \(_{s}\) first. We then align \(_{p}^{j}\) with the task prompt representation \(_{k}\) using a linear layer, i.e., \(_{p}^{j}=(_{p}^{j})\), where \(_{k}=_{}(_{k})\), \(_{}()\) denotes the embedding layer of \(\), and \(_{k}\) is the \(k\)-th task's prompt. The concatenation of \(_{p}^{j}\) and \(_{k}\) is subsequently fed into \(\) to generate the output \(}_{p}^{j}\). Finally, the parameters are optimized by the ground truth \(_{p}^{j}\). Note that all parameters of \(\) are fixed during the optimization, and only the encoder of \(_{s}\) will be updated. The updated \(_{s}\) is then sent to all clients again for updates in the next communication round until FedMeKI converges.

## 4 The FedMeKI Dataset Suite

Since we propose a new research task, **no** existing datasets are suitable for evaluation. We curated a new dataset from publicly available medical sources to address this, comprising two types of tasks: training and validation. The **training tasks** are used to scale the medical foundation model and to evaluate its task-specific capabilities. The **validation tasks** are _independent_ of the training tasks and are used to assess the ability of the scaled medical foundation model in zero-shot inference.

### Training Tasks

To inject medical knowledge into the foundation model \(\), as shown in Section 3.1, we need to train tasks to cover as many medical modalities as possible. In this benchmark, we choose 4 commonly used classification tasks covering 6 medical modalities. Note that we do not use any tasks with the text modality since the medical foundation model \(\) has the superior capability to handle texts.

(1) Lung Opacity Detection  is an unimodal classification task aiming at predicting lung opacity from chest X-ray **images**. The data are provided by the RSNA Pneumonia Detection Challenge 2018 . Medical practitioners at the Society for Thoracic Radiology and MD.ai provide the annotations, i.e., ground truth labels. The original medical images are found in the chest X-ray database . The data details are in Appendix Section F.

(2) COVID-19 Detection requires the model to determine whether an X-ray **image** indicates COVID-19 symptoms, testing the model's understanding of medical images. We utilize the COVQU dataset  for this task. The details of this task can be found in Appendix Section G.

(3) ECG Abnormal Detection aims to determine whether an electrocardiogram (ECG) **signal** exhibits abnormal patterns or not. This is an unimodal binary classification task, where the data are sourced from an existing ECG database , consisting of 12-lead ECGs of 10-second length. The data details are in Appendix Section H.

(4) Mortality Prediction involves using various data points and a classification or predictive model to estimate the likelihood of a patient's survival or death during their stay in the Intensive Care Unit (ICU). We extract the data from MIMIC-III using the ICU-oriented preprocessing pipeline . Following , we extract 48 dynamic features, including **vital signs** (7 variables) and **laboratory tests** (39 variables), with **two more variables** that measure input (_fraction of inspired oxygen_) and output (_urine_). The data details are in Appendix Section I.

### Validation Tasks

Using the training tasks, we can inject various medical knowledge into the foundation model \(\) by inserting an aggregated encoder learned from federated clients into \(\). We use four new tasks to evaluate the generalization ability of the federated scaled \(\) learned by the FedMeKI platform with 2 classification tasks and 2 generation tasks.

(5) Enlarged Cardiomediastinum Detection  aims to determine the likelihood of an enlarged cardiomediastinum using medical **images** from clinical assessments. This task evaluates the model's ability to interpret radiographic data. Further details of this task can be found in Appendix Section J.

(6) Sepsis Prediction aims to predict the probability of sepsis occurring during ICU stays, examining the model's ability to comprehend diverse **clinical features**, which are the same as those extracted for the mortality prediction task from the MIMIC-III database using the preprocessing pipeline . The details of this task can be found in Appendix Section K.

(7) Medical Visual Question Answering (MedVQA) aims to use both **visual images** and **textual questions** as inputs to generate the answers. This task tests the model's ability to align text and image modalities in the medical domain. We use the VQA-RAD dataset in this work . The details of this task can be found in Appendix Section L.

(8) Signal Noise Clarification is another generative task that focuses on accurately describing noise in ECG **signals** with the corresponding **textual questions**, where the data are extracted from an existing ECG question answering dataset . The signals are in 12 channels, lasting 10 seconds, similar to the ECG Abnormal Detection task. The data details are in Appendix Section M.

### Data Partition

The **training tasks** have two roles. The first role is to inject the medical knowledge in the training tasks into the foundation model \(\). The second one is to evaluate the performance of these training tasks on the scaled \(\). Thus, for each training task, we divide the data into four parts in a ratio of 7:1:1:1, where 70% data \(^{tra}_{tr}\) are the real training data that will be evenly distributed to \(N\) clients, 10% data as the public data \(^{tra}_{p}\) that will be put on the server, another 10% data as the development data \(^{tra}_{d}\) that are preserved on the server to guide the model training, and the remaining 10% data \(^{tra}_{te}\) as the testing data for training tasks. The **validation tasks** aim to evaluate the capability of zero-shot inference. For validation tasks with numerous samples in the test set, we randomly choose several data samples \(^{val}_{te}\) for the testing. Details of these datasets' split are available in Table 1.

## 5 Benchmark

### Approaches & Evaluation Metrics

We use the following approaches as benchmarks for the evaluation of **training tasks**, which will be evaluated with the training data of the training tasks, i.e., \(^{tra}_{tr}\). Our evaluation focuses on two scenarios: single-task and multi-task evaluations. Note that the original medical foundation model MMedLM-2, which can only input text data, cannot work on all these tasks.

    &  &  &  &  &  &  \\  & & (5) Clients) & (Server) & (Server) & (Server) \\   & Lung Opacity Detection & 18,406 & 12,880 & 1,849 & 1,841 & 1,836 \\   & COVID-19 Detection & 13,808 & 9,665 & 1,380 & 1,380 & 1,383 \\   & ECG Abnormal Detection & 21,797 & 15,259 & 2,179 & 2,180 & 2,179 \\   & Mortality Prediction & 38,129 & 26,690 & 3,812 & 3,812 & 3,813 \\    & Enlarged Cardiomediastinum Detection & 234 & ✗ & ✗ & ✗ & 234 \\   & Sepsis Prediction & 1,000 & ✗ & ✗ & ✗ & 1,000 \\    & MedVQA & 1,000 & ✗ & ✗ & ✗ & 1,000 \\    & Signal Noise Clarification & 1,000 & ✗ & ✗ & ✗ & 1,000 \\   

Table 1: Details of data split, where we deploy 5 clients on the FedMeKI platform.

**Eight Single-task Evaluation Benchmarks.** Single-task evaluation aims to validate the generalization ability of FedMeKI on tasks with specific modalities. We use the following approaches as benchmark baselines: (1) Traditional Federated Learning (TFL). We use two representative federated learning models as benchmark baselines: FedAvg  and FedProx . For each task, we use the corresponding task data to train an FL model \(_{s}\) or \(_{s}\). We use the aggregated global model to evaluate the performance. (2) Federated Learning with Global Fine-tuning (FL+GF). Since the server stores a small set of public data \(^{tra}_{p}\), the traditional models can conduct the fine-tuning using \(^{tra}_{p}\) for the aggregated global models. These approaches are denoted as \(^{+}_{s}\) and \(^{+}_{s}\). (3) Federated Learning with LLM Fine-tuning (FL+LLM). To further enhance the learning ability of traditional federated learning approaches, we allow them to fine-tune with the LLM. In particular, the encoder of each aggregated model will be used first to generate the representation of the public data. The representation is then concatenated with the representation of LLM to generate the output. We denote these LLM fine-tuning approaches as \(^{}_{s}\) and \(^{}_{s}\). Besides, we can obtain the aggregated models from \(^{}_{s}\) and \(^{}_{s}\) on the server as traditional FL approaches, denoted as \(^{*}_{s}\) and \(^{*}_{s}\).

**Eight Multi-task Evaluation Benchmarks.** The final goal of the designed FedMeKI platform is to evaluate the multi-site, multi-modal, multi-task medical knowledge injection. Since MMedLM-2 can only handle single modality inputs, we do not consider baselines of directly using MMedLM-2 in this evaluation. (1) TFL. We still employ FedAvg  and FedProx  but use a multi-modal multi-task encoder for each client model as described in Section 3.1. These two approaches are denoted as \(_{m}\) and \(_{m}\). (2) FL+GF. We can also fine-tune the aggregated model on the server using the public data \(^{tra}_{p}\) at each communication round. We use \(^{+}_{m}\) and \(^{+}_{m}\) to denote the fine-tuned approaches. (3) FL+LLM. We use \(^{}_{m}\) and \(^{}_{m}\) to denote the federated fine-tuned approaches, which are the full version of solutions deployed on the proposed FedMeKI platform. Except for the fine-tuned medical foundation models, we can also obtain an aggregated global model, denoted as \(^{*}_{m}\) or \(^{*}_{m}\), similar to traditional FL.

The details of all these 16 benchmark approaches can be found in Appendix Section N.

**Low-resource Evaluation Benchmarks.** We have four **validation tasks** with diverse modalities. Without federated scaling of the original medical foundation model, MMedLM cannot handle these three tasks. Thus, we use the scaled medical foundation models, including \(^{}_{m}\), and \(^{}_{m}\) to evaluate the three validation tasks with zero-shot inference on \(^{val}_{te}\).

**Evaluation Metrics.** We use accuracy, precision, recall, and F1 as the evaluation metrics for the classification tasks, and BLEU, ROUGE, and METEOR are used to evaluate the generation tasks. The higher, the better.

### Benchmark Results

#### 5.2.1 Evaluation Results of Training Tasks

**Single-task Benchmarks.** Table 2 shows the results of the single-task benchmarks. We can observe that the existing medical foundation model MMedLM-2 cannot handle these tasks. However, after

    &  &  &  &  \\  & & & **FedAvg** & **FedAvg** & **FedAvg** & **FedProx** & **FedProx** & **FedProx** \\   & Accuracy & ✗ & 95.86 & 94.44 & 96.02 & 89.42 & 95.70 & 96.08 & 95.70 & 91.23 \\  & Precision & ✗ & 97.40 & 93.81 & 96.70 & 84.69 & 97.49 & 97.11 & 95.23 & 87.76 \\  & Recall & ✗ & 94.01 & 95.58 & 95.58 & 97.16 & 94.11 & 95.27 & 96.53 & 96.52 \\  & F1 & ✗ & 93.1 & 94.69 & 96.14 & 90.50 & 95.77 & 96.18 & 95.87 & 91.93 \\   & Accuracy & ✗ & 99.35 & 99.48 & 99.28 & 93.94 & 99.13 & 99.42 & 99.13 & 84.16 \\  & Precision & ✗ & 99.71 & 99.70 & 100.00 & 93.59 & 99.71 & 99.42 & 99.71 & 77.27 \\  & Recall & ✗ & 97.72 & 94.30 & 97.15 & 74.92 & 96.87 & 98.29 & 96.87 & 53.27 \\  & F1 & ✗ & 98.71 & 96.93 & 98.75 & 98.15 & 98.21 & 98.95 & 98.27 & 63.07 \\   & Accuracy & ✗ & 67.68 & 66.83 & 87.86 & 43.15 & 79.41 & 80.51 & 57.77 & 45.25 \\  & Precision & ✗ & 69.13 & 80.65 & 89.56 & 86.97 & 89.04 & 89.06 & 87.34 & 60.85 \\  & Recall & ✗ & 80.78 & 56.24 & 31.61 & 11.22 & 73.88 & 76.00 & 32.47 & 17.80 \\  & F1 & ✗ & 74.50 & 66.27 & 46.72 & 18.74 & 80.75 & 82.01 & 47.34 & 27.55 \\   Mortality Prediction \\  } & Accuracy & ✗ & 91.98 & 91.66 & 91.61 & 84.11 & 91.98 & 90.12 & 91.61 & 82.41 \\  & Precision & ✗ & 70.00 & 52.86 & 58.33 & 16.35 & 71.05 & 36.45 & 58.33 & 13.87 \\   & Recall & ✗ & 8.70 & 11.42 & 12.7 & 21.43 & 8.39 & 22.98 & 2.17 & 16.64 \\   & F1 & ✗ & 15.47 & 18.88 & 4.19 & 18.55 & 15.00 & 28.19 & 4.19 & 15.13 \\   

Table 2: Benchmark performance of single-task evaluation for training tasks.

scaling it with private medical data on the designed FedMeKI platform, the scaled models FedAvg\({}_{s}^{}\) and FedProx\({}_{s}^{}\) can work for these training tasks. These comparisons demonstrate that the FedMeKI platform effectively achieves the goal of medical knowledge injection.

We can also observe that the federated scaled medical foundation models, FedAvg\({}_{s}^{}\) and FedProx\({}_{s}^{}\), still perform worse on the four training tasks than traditional federated learning approaches, FedAvg\({}_{s}\) and FedProx\({}_{s}\), and their scaled version FedAvg\({}_{s}^{+}\) and FedProx\({}_{s}^{+}\). This is reasonable since they are specifically designed for federated learning, and the aggregated global models do not contain any "noisy knowledge" injected by the medical foundation model MMedLM-2. However, comparing their performance is not the goal of this work. We aim to enable the medical foundation model to handle tasks with diverse medical modalities.

FedAvg\({}_{s}^{*}\) and FedProx\({}_{s}^{}\) are the byproducts of FedAvg\({}_{s}^{}\) and FedProx\({}_{s}^{}\). Their performance is comparable to that of federated learning approaches on two image classification tasks but worse on the other two tasks. This may be because these two tasks are easier than ECG abnormal detection and mortality prediction tasks, and the medical foundation model can also be quickly adapted to these easy tasks.

Multi-task Benchmarks.Although we train multiple tasks with a designed multi-modal multi-task encoder, two of these tasks (ECG abnormal detection and mortality prediction) do not share overlapped modalities, leading to the same performance as single-task training as shown in Table 2. Thus, we do not list them in Table 3. We can observe that for the two image classification tasks, both foundation models, FedAvg\({}_{m}^{}\) and FedProx\({}_{m}^{}\), significantly improve their performance compared with single-task benchmarks, FedAvg\({}_{s}^{}\) and FedProx\({}_{s}^{}\). These results clearly demonstrate the importance and necessity of training multiple medical tasks together when injecting medical knowledge into foundation models.

#### 5.2.2 Evaluation Results of Validation Tasks

Low-resource Benchmarks.A primary goal of training foundation models is to boost the performance of multiple downstream tasks, especially for zero-shot inference. To achieve this goal, we test the scaled medical foundation models in the previous experiment with four tasks. The enlarged cardiomediastinum detection task is similar to the lung opacity prediction task, as both take radiological images as input. Also, the sepsis prediction task is similar to the mortality prediction task in training, sharing the same feature space. However, the MedVQA and signal noise clarification tasks are new since they combine two modalities, which were not trained during the training. Thus, the two generation tasks are much harder than the two classification ones.

From the results shown in Table 4, we can observe that the knowledge-injected medical foundation models have the ability to deal with new tasks. Although the performance of the two generation-based tasks still has significant room for improvement, the designed platform at least can work for such tasks compared to the original medical foundation model MMedLM-2. Therefore, these results still demonstrate the utility of our benchmark for federated medical knowledge injection.

## 6 Discussion

Summary of Key Findings.In this study, we aimed to create a benchmark for federated medical knowledge injection into medical foundation models. To achieve this, we curated a comprehensive

    &  &  & _{s}^{}\)**} & _{s}^{}\)**} \\   & & **FedAvg\({}_{m}\)** & **FedAvg\({}_{s}^{}\)** & **FedAvg\({}_{s}^{}\)** & **FedProx\({}_{s}^{}\)** & **FedProx\({}_{s}^{}\)** & **FedProx\({}_{s}^{}\)** & **FedProx\({}_{s}^{}\)** \\   Lang Opacity \\ Detection \\  } & Accuracy & ✗ & 95.42 & 94.23 & 94.83 & 95.98 & 94.77 & 96.24 & 96.41 & 93.13 \\  & Precision & ✗ & 99.66 & 93.51 & 93.68 & 98.22 & 99.54 & 97.22 & 97.63 & 93.74 \\  & Recall & ✗ & 91.48 & 95.48 & 96.64 & 92.95 & 90.33 & 95.48 & 95.37 & 92.95 \\  & F1 & ✗ & 95.39 & 94.48 & 95.13 & 95.51 & 94.71 & 96.34 & 96.49 & 93.35 \\   COVID-19 \\ Detection \\  } & Accuracy & ✗ & 99.06 & 98.99 & 99.28 & 98.34 & 99.06 & 99.20 & 98.99 & 86.11 \\  & Precision & ✗ & 99.42 & 98.56 & 99.14 & 96.07 & 99.13 & 98.85 & 98.56 & 65.09 \\  & Recall & ✗ & 96.87 & 97.44 & 98.01 & 97.44 & 97.15 & 98.01 & 97.44 & 97.72 \\  & F1 & ✗ & 98.12 & 97.99 & 98.57 & 96.75 & 98.13 & 98.43 & 97.99 & 78.13 \\   

Table 3: Benchmark performance of multi-task evaluation for training tasks. Note that the performance of **ECG Abnormal Detection** and **Mortality Prediction** is the same as that shown in Table 2 since the modalities of these two tasks are non-overlapped with others.

dataset for evaluation and implemented 16 benchmark baselines. Our enhanced foundation models demonstrated the capability to handle new tasks involving new medical modalities, showcasing the potential of this approach. However, the performance of these new foundation models was observed to be lower compared to traditional federated learning models.

**Implications of the Study.** Our findings have several important implications for the field of medical AI. Firstly, the ability of the enhanced foundation models to adapt to new medical modalities without the need for retraining from scratch highlights the potential for more efficient and scalable AI systems in healthcare. This capability can lead to significant time and resource savings, particularly in rapidly evolving medical fields. Secondly, federated learning ensures data privacy and security, which is paramount in handling sensitive medical data. The creation of a curated dataset and implementation of 16 benchmark baselines provide a robust framework for evaluating the effectiveness of federated medical knowledge injection, setting a standard for future research in this area.

**Limitations.** Our study has several limitations. The primary limitation is the observed performance trade-off when injecting medical knowledge into the foundation models. Moreover, the performance of zero-shot evaluation is still unsatisfactory. Additionally, the diversity and quality of the data available from multiple clients could impact the learning outcomes. Federated learning introduces challenges related to communication overhead and synchronization across clients, which might affect the overall efficiency and effectiveness of the learning process.

**Future Research Directions.** Future research should focus on optimizing the training algorithms to better handle the increased complexity introduced by the injection of medical knowledge. Exploring advanced federated learning techniques, such as personalized federated learning or federated transfer learning, could potentially enhance performance. Additionally, investigating more efficient communication protocols and strategies to manage data heterogeneity across clients would be beneficial. Expanding the study to include a wider variety of medical modalities and tasks could further validate the versatility and robustness of the proposed approach. Moreover, continually refining the curated dataset and updating the benchmark baselines will be crucial for ongoing evaluation and improvement.

## 7 Conclusion

Our study demonstrates the potential of injecting medical knowledge into foundation models within a federated learning framework. While there are challenges related to performance optimization, the enhanced adaptability and scalability of these models represent a promising direction for future medical AI research. By addressing the current limitations and exploring advanced learning techniques, we can further improve the efficacy and application of these innovative models in healthcare. Our curated dataset and benchmark baselines provide a solid foundation for continued research and development in this area.

**Task (Modalities)** & **Metric** & **MMedLM-2** & **FedAvg\({}_{m}^{F}\)** & **FedProx\({}_{m}^{F}\)** \\   & Accuracy & ✗ & 58.54 & 57.26 \\  & Precision & ✗ & 53.33 & 52.57 \\  & Recall & ✗ & 88.07 & 84.40 \\  & F1 & ✗ & 66.04 & 64.78 \\   Sepsis Prediction (48 clinical features) \\  } & Accuracy & ✗ & 39.00 & 39.80 \\  & Precision & ✗ & 2.61 & 3.57 \\  & Recall & ✗ & 55.17 & 75.86 \\  & F1 & ✗ & 4.98 & 6.81 \\   MedVQA (medical image + text) \\  } & BLEU & ✗ & 1.20 & 1.20 \\  & ROUGE & ✗ & 2.43 & 3.42 \\  & METEOR & ✗ & 1.07 & 2.83 \\   Signal Noise Clarification \\ (signal + text) \\  } & BLEU & ✗ & 0.06 & 0.04 \\  & ROUGE & ✗ & 0.29 & 0.23 \\   & METEOR & ✗ & 1.88 & 0.63 \\   

Table 4: Zero-shot evaluation for validation tasks.