# Differentiable sorting for censored time-to-event data.

Andre Vauvelle\({}^{12}\), Benjamin Wild\({}^{3}\)1, Roland Eils\({}^{3}\), Spiros Denaxas\({}^{1}\)

University College London\({}^{1}\), BenevolentAI\({}^{2}\), Berlin Institute of Health\({}^{3}\)

{andre.vauvelle.19,s.denaxas}@ucl.ac.uk

{benjamin.wild, roland.eils}@bih-charite.de

Equal contribution

###### Abstract

Survival analysis is a crucial semi-supervised task in machine learning with significant real-world applications, especially in healthcare. The most common approach to survival analysis, Cox's partial likelihood, can be interpreted as a ranking model optimized on a lower bound of the concordance index. We follow these connections further, with listwise ranking losses that allow for a relaxation of the pairwise independence assumption. Given the inherent transitivity of ranking, we explore differentiable sorting networks as a means to introduce a stronger transitive inductive bias during optimization. Despite their potential, current differentiable sorting methods cannot account for censoring, a crucial aspect of many real-world datasets. We propose a novel method, Diffsurv, to overcome this limitation by extending differentiable sorting methods to handle censored tasks. Diffsurv predicts matrices of _possible_ permutations that accommodate the label uncertainty introduced by censored samples. Our experiments reveal that Diffsurv outperforms established baselines in various simulated and real-world risk prediction scenarios. Furthermore, we demonstrate the algorithmic advantages of Diffsurv by presenting a novel method for top-k risk prediction that surpasses current methods. In conclusion, Diffsurv not only provides a novel framework for survival analysis through differentiable sorting, but also significantly impacts real-world applications by improving risk stratification and offering a methodological foundation for developing predictive models in healthcare and beyond.

## 1 Introduction

Survival analysis plays a pivotal role in many realworld machine learning applications, spanning fields such as reliability engineering, marketing, and insurance, with a particularly significant impact in healthcare. The goal of survival analysis is to predict the time until the occurrence of an event of interest, such as death, based on a set of covariates. In clinical studies, these include demographic variables such as sex and age, but may also encompass more complex data modalities such as medical images.

The concept of censoring is a distinguishing characteristic that sets survival analysis apart from conventional machine learning approaches. Particularly prevalent in observational datasets, it refers to situations where event times remain unobserved because a patient might not have undergone the event by the time of data collection. This can be due to a variety of reasons such as the study period ending before all events of interest have occurred or subjects leaving the study.

Overlooking censoring can skew predictions towards the censoring event, rather than the event of interest. This bias becomes particularly noticeable when the study's endpoint can be inferred from the observed covariates - age being a notable example. In such cases, the predicted event times are likelyto biased towards the censoring event time, thereby neglecting the actual event of interest (Kavamme and Borgan, 2019).

The Cox Proportional Hazards model is widely used for handling censored data in survival analysis (Cox, 1972). The model optimizes a partial likelihood function over ranked data, considering only the order of events, not their exact time of occurrence. As such, Cox's partial likelihood serves as a ranking loss, learning from the order of patients based on their hazard of experiencing an event, not their exact survival time.

Raykar et al. (2007) showed that Cox's partial likelihood (CPL) and ranking losses can be directly equated, with both providing lower bounds to the concordance index, the primary evaluation metric used in survival analysis. Both losses are foundational to many survival deep learning methodologies like DeepSurv Katzman et al. (2018) and DeepHit Lee et al. (2018).

However, this relation operates under the assumption of pairwise independence. This simplification, while practical, can de-emphasize the transitive properties inherent in survival data. As shown by Goldstein and Langholz (1992), larger risk set sizes can lead to more efficient estimators, suggesting potential benefits in considering listwise ranking losses Cao et al. (2007). These losses optimize over lists of values rather than individual pairs, thereby better capturing the transitive dynamics of the data. Despite the similarities, listwise losses have remained largely unexplored within the field of survival analysis. This could be partly due to an uncertainty around how to handle censoring.

We propose a new approach that takes advantage of recent developments in continuous relaxations of sorting operations, allowing end-to-end training of neural networks with ordering supervision (Grover et al., 2019; Blondel et al., 2020; Petersen et al., 2021). This method incorporates a sorting algorithm into the network architecture, where the order of the samples is known, but their exact values are unsupervised. With this, we introduce _Diffsurv_, an extension of differentiable sorting methods that enables end-to-end training of survival ranking models with censored data.

Briefly, our contributions are summarised:

* Our primary contribution is the extension of differentiable sorting methods to account for censoring by introducing the concept of possible permutation matrices. (Section 3.1)
* We empirically demonstrate that our new differentiable sorting method matches or improves risk ranking performance across multiple semi-simulated and real-worlds censored datasets. (Section 4)
* We investigate the role of transitivity in survival analysis and show, through experiments with semi-simulated data, that differentiable sorting networks can benefit from this inherent property of the data. (Section 4)
* We demonstrate that differentiable sorting of censored data enables the development of new methods with practical applications, using the example of end-to-end learning for top-k risk stratification. (Section 3.2)

## 2 Survival Analysis and its Relation to Ranking

A dataset with censored event times is summarized as \(=\{t_{i},_{i},_{i}\}_{i=1}^{N}\), where \(N\) is the total number of patients. For a patient \(i\), the time-to-event \(t_{i}\) is the minimum of the true survival time \(t_{i}^{*}\) and the censoring time \(c_{i}^{*}\), with \(_{i}\) indicating whether an event (\(t_{i}^{*} c_{i}^{*}\), \(_{i}=1\)) or censoring (\(t_{i}^{*}>c_{i}^{*}\), \(_{i}=0\)) was observed. Covariates are \(_{i}^{d}\) representing a \(d\)-dimensional vector but the methods discussed here also generalise to higher dimensional tensors such as image data.

The widely-used method for addressing censoring in survival analysis is the Cox Partial Likelihood (CPL) model, introduced by Cox (1972). The CPL is designed to maximize the following general form:

\[():=_{i:_{i}=1}(_{i})}{ _{j:t_{j}>t_{i}}f_{}(_{j})}, \]where \(f_{}\) is the hazard function, a score prediction function estimating the probability of an event at a particular time, given input features \(_{i}\). The product only includes uncensored patients, whereas the denominator term also includes censored patients with \(t_{j}>t_{i}\).

Reflecting the structure of survival data, the Cox Partial Likelihood (CPL) model compares individuals still "at risk" at each time point, similar to a nested case-control study. This directly shapes the likelihood equation in CPL, with the numerator representing the hazard function for the event-experiencing individual, and the denominator summing over all individuals still at risk.

Extensions of the Cox model, like (Katzman et al., 2018) and (Kvamme et al., 2019), have modified \(f_{}=(_{i})\) to relax the linear covariate interaction and proportional hazards assumptions. Introducing neural networks \(h_{}\) to handle the non-linearity, adjusting \(f_{}\) to be \(f_{}=(h_{}(_{i}))\), and to manage non-proportional hazards, they set \(f_{}=(h_{}(_{i},t_{i}))\).

### Pairwise Independence

Both of these previous works note that the risk set \(=\{j:t_{j}>t_{i}\}\) is intractable for deep learning applications as it considers all comparable patients. To mitigate memory constraints, we can sample a fixed-size risk set, denoted as \(}\), such that \(|}|=n<N\). Kvamme et al. (2019) go further, arguing it is reasonable to take a constant sample size of 1 and include the individual \(i\) in the risk set (such that \(n=2\)). This leads to the simplified loss of the form

\[()=_{i:_{i}=1}(_{i})}{f_{ }(_{i})+f_{}(_{J(i)})},J(i) \{i\}. \]

Further, take the mean log partial likelihood to be

\[=}_{i:_{i}=1}(1+[h_{}(_ {J(i)})-h_{}(_{i})]),J(i)\{i\}, \]

where \(n_{e}\) is the number of non-censored events. In this simplified form, it can be seen that the partial likelihood only considers the pairwise relative ordering or ranking of survival times.

The concordance index or c-index Harrell et al. (1982) is a commonly used as an evaluation for survival analysis methods and is a generalization of the Area Under the Receiver Operating Characteristic Curve (AUROC) that handles right-censored data. It is defined as

\[:=_{i:_{i}=1}(f(_{i})< f(_{j})),j\{i\}. \]

Raykar et al. (2007) first showed that the Cox's partial likelihood is approximately equivalent to maximizing the concordance index or c-index and that closer bounds can be found by minimizing the

Figure 1: Differentiable Sorting for Censored Time-to-Event Data. Inputs, in this case SVHN images, are transformed into scalar values through a neural network. A differentiable permutation matrix, \(\), is computed using sorting networks. The model can be optimized for downstream tasks, such as risk stratification and top-k highest risk prediction, by using the matrix \(_{p}\) of possible permutations based on the observed events and censoring.

general ranking loss, with acceptable pairs \(=\{(i,j):_{i}=1 t_{j}>t_{i}\}\) and

\[:=|}_{(i,j)}(f_{ }(_{i})-f_{}(_{j})), \]

where \(\) is a function that relaxes the non-differentiable \(1\) of the c-index. From Equation 3, it can be seen that \(:x-(1+(-x))=((x))\). Here, we have shown that the simplifications to the partial likelihood made by Kvamme et al. (2019) are equivalent to using the log-sigmoid ranking loss.

The key difference between ranking and partial likelihood losses comes when considering the assumption that it is reasonable to take a constant sample size of 2 (one pair in the risk set) in the partial likelihood. This effectively introduces the assumption that each pair (i, j) is independent of any other pair. However, this assumption seems puzzling given the inherent transitivity of ranking (if \(i>j\) and \(j>k\) then \(i>k\)).

### Listwise Ranking and Differentiable Sorting Networks

(Cao et al., 2007) first proposed the notion of a _listwise_ ranking loss, arguing its benefits in situations where the entire order of items is of importance. This approach treats the ranking problem as a permutation problem, with the aim of learning a model that can provide the optimal permutation of an entire list of items, rather than considering independent pairs. Indeed, this idea is closely related to the partial likelihood from Cox's original work. As pointed out by Wang and Yang (2022), the Top-1 probability method originally proposed by Cao et al. (2007) and extended to ListMLE by Xia et al. (2008) takes the same form as CPL.

More recent works closely related to listwise ranking have begun to explore combining traditional sorting algorithms with differentiable sorting functions (Petersen et al., 2021). Sorting networks are a family of sorting algorithms that consist of two basic components: wires and conditional swaps (Batcher, 1968; Knuth, 1998). Wires carry values to be compared at conditional swaps. If one value is bigger than the other then the values carried forward are swapped around. This allows construction of _sorting networks_ that can provably sort a list of values. Conditional swaps are exactly the min and max operators that ensure that with inputs \(\{a,b\}\) and outputs \(a^{*} b^{*}\), \(a^{*}=(a,b)\) and \(b^{*}=(a,b)\). Examples of odd even and bitonic networks are shown in Appendix C.

In order to train models based on ordering information alone, differences between predicted and true orderings must be backpropagated through sorting algorithms. However, they often require the use of non-differentiable \(\) and \(\) operators. These are analogous to the non-differentiable indicator function that was discussed earlier in the c-index equation 4. Differentiable sorting methods, similar to ranking losses, rely on approximating these operators with smooth alternatives (Grover et al., 2019).

Petersen et al. (2021) propose combining traditional sorting networks and differentiable sorting functions. Consider that when \(a\) asymptotically approaches \(b\), the transition point where it surpasses \(b\) is non-continuous and therefore non-differentiable. Just as previously shown in the ranking loss, such operations can be made differentiable using the logistic relaxation

\[_{}(a,b)=a(b-a)+b(a-b)_{}(a,b)=a(a-b)+b(b-a). \]

If an inverse temperature parameter \(>0\) is introduced such that \(:x}\), then as \(\) the functions tend to the exact \(\) and \(\) functions. Other relaxations of the step function can also be considered, Petersen et al. (2022) show that the Cauchy distribution preserves monotonicity which is desirable for optimization. Given this, we use the Cauchy distribution as our relaxation for all experiments, where \(:x( x)+\).

For an input list to be ordered, each layer of the sorting network can be considered an independent permutation matrix \(_{l}\) with elements given by

\[P_{l,ii}=P_{l,jj}=(a_{j}-a_{i})P_{l,ij}=P_{l,ji}=1-(a_{j}-a_{i}), \]

where \(a\) signifies intermediate values being compared. The first layer is input with \(z_{i}=h_{}(_{i})\), each vector of covariates or images being processed independently by the same neural network. The indices being compared at each layer are determined by the sorting network and the final predicted probability matrix is the product of each layer of sorting operations,

\[=(_{l=1}^{n}_{l}^{})^{}. \]

Where \(\), is the final doubly-stochastic permutation matrix, doubly-stochastic meaning that the rows and columns both sum to 1. It is possible to interpret each element \(P_{ij}\) of the predicted permutation matrix as the predicted probability of permuting from a randomly assigned rank \(i\) to a true rank \(j\). Finally, we can define a loss by minimizing the cross-entropy between the ground truth orders represented by true permutation matrix \(\) and predicted permutation matrix \(\) as

\[:=_{c=1}^{n}((_{c}, _{c})), \]

where \(_{c}\) and \(_{c}\) denote the c-th columns of their respective matrices.

### Differentiable Sorting Networks Relation to Ranking and Partial Likelihood

It is possible to directly relate differentiable sorting networks with ranking losses and partial likelihood. Expanding out the cross entropy loss, we find

\[=_{c=1}^{n}(_{i=1}^{n}q_{ic}(p_{ic}) ), \]

where \(q_{ic}=1\) only when \(i\) is the true rank otherwise \(0\). Each \(p_{ic}\) is always a function of the difference in pairs of inputs \(x_{i}\) and \(x_{j}\). This is complicated by the products of intermediate values \(a\) introduced by the sorting network but denoted as

\[p_{ic}=_{(a_{i},a_{j})_{l}:l=1}^{n}(a_{i}-a_{j}) \]

where \(_{l}\) to denotes the set of comparisons to be made at each layer of the sorting network. With \(n=2\) and \(=1\), a sorting network only requires a single relaxed conditional swap and the loss returns to the same recognisable log-sigmoid ranking loss in Equation 5, and Cox negative log partial likelihood in Equation 3.

## 3 Methods

### Diffsurv: Handling Censoring with Possible Permutation Matrices

For risk sets of size 2, given proper case-control sampling, it will always be possible to define a single ground truth permutation matrix \(\). However, when venturing to higher risk set sizes, differentiable sorting methods can no longer handle censoring since there is not a single ground truth permutation matrix \(\). We cannot determine the exact rank of patients who are censored before another who experienced an event. It is only possible to know the range of possible ranks to which a patient should belong. In Figure 2, we provide an illustration demonstrating the possible ranks for a number of censored and uncensored events.

Though we no longer have access to a single permutation matrix, we may instead consider the set of all possible permutation matrices, \(=\{_{1},_{1},,_{}\}\). In the best case, all values are uncensored and \(||=1\) and in the worse case, when all patients are censored \(||=n!\). Our primary contribution is to extend differentiable sorting methods to censored ranks by discriminating between possible and impossible permutations.

We introduce a more computationally tractable representation of \(\) by defining the _possible permutation matrix_, \(_{p}\), which is the element-wise maximum of every permutation in \(\),

\[_{pij}=\{_{1ij},_{2ij},,_{ ij}\}. \]

For survival analysis, it is possible to determine \(_{p}\) in linear time given a sorted list of event times \(t_{i}\) and event indicators \(_{i}\). We will consider higher ranks to correspond with a smaller time-to-event. Let us consider two scenarios:1. For a right-censored individual \(i\) (i.e., \(_{i}=0\)), the possible ranks must be lower than the ranks of preceding uncensored events. We can express the set of possible ranks \(_{i}\) as: \[_{i}=\{r r<(j), j:_{j}=1,t_{j}<t_{i}\}\] (13) This set includes all ranks \(r\) that are less than the rank of any uncensored patient \(j\) with an event time \(t_{j}\) preceding the censoring time \(t_{i}\) of the individual \(i\).
2. For an uncensored individual \(i\) (i.e., \(_{i}=1\)), the possible rank must be lower than all preceding uncensored events and higher than the ranks of all subsequent events. We can define the set of possible ranks as: \[_{i}=\{r r<(j), j:_{j}=1,t_{j}<t_{i}\} \{r r>(j), j:t_{j}>t_{i}\}\] (14) This set includes all ranks \(r\) that satisfy both conditions: being less than the rank of any preceding uncensored patient \(j\) with \(t_{j}<t_{i}\) and greater than the rank of any subsequent patient \(j\) with \(t_{j}>t_{i}\).

With these observations, it's straightforward to construct \(_{p}\). If it's feasible for patient \(i\) to permute to rank \(j\), i. e. \(j_{i}\), then \(_{pij}=1\), otherwise \(_{pij}=0\). See Figure 2 (c) for a visual representation of \(_{p}\).

Given the possible permutation matrix \(_{p}\) and the predicted permutation matrix \(\), the vector of probabilities \(\) of a value being ranked within the set of possible ranks can be computed. Although the ground truth probabilities are unknown, the range of possible ranks is known, and the model can be optimized to maximize the sum of the predicted probabilities of all possible ranks for each sample. Noted here as the column-sum of the element-wise product \(\), between \(_{p}\) and \(\).

\[=_{j=1}^{n}(_{p})_{i,j}. \]

The binary cross-entropy loss can then be easily applied

\[=_{i=1}^{n}-y_{i}(p_{i})-(1-y_{i})(1-p_{i}) \]

where \(y_{i}\) indicates whether set of predicted ranks is possible or impossible.

Equation 15 accounts for the potential challenges of incorporating right-censored samples. The binary cross-entropy remains identical whether the model predicts uniform probability for all possible ranks or concentrates the probability mass on a single rank possible rank.

The introduction of the possible permutation matrix can be used in conjunction with any differentiable sorting method that outputs a doubly-stochastic permutation matrix. This includes methods such

Figure 2: For an example case (**a**) with two events (\(\), \(e_{1}\) and \(e_{5}\)) and multiple censored samples (\(\), \(c_{1},c_{3},c_{4},c_{6},c_{7}\)) the uncertainty in the possible permuted rankings (**b**) due to censoring is taken into account to derive the possible permutation matrix \(_{p}\) (**c**).

as SinkhornSort from Cuturi et al. (2019). Though, in this paper, we will restrict our focus to the discussed differentiable sorting networks. We refer to the use of differentiable sorting networks and the possible permutation matrix as _Diffsurv_.

### Top-K risk prediction

Finally, we demonstrate how the algorithmic supervision of sorting algorithms enables the development of novel methods in survival analysis, using the example of top-k risk prediction. In practical settings, it is often not necessary to rank all samples correctly. Rather, it is essential to identify the samples with the highest risk, such as by a healthcare provider, to prioritize care and interventions.

With Diffsurv, top-k risk prediction is straightforward to implement by modifying the loss such that the negative log-likelihood of predicted top-k ranks in \(\) is maximised for individuals with a possible permutation to _any_ top-k rank according to \(_{p}\).

First, let's denote \(_{k}\) as the set of values with a possible permutation to a top-k rank, derived from the ground truth possible permutation matrix \(_{p}\):

\[_{k}=\{i|_{j=1}^{k}_{pij}>0\} \]

Importantly, due to the uncertainty introduced by censoring, the set of individuals with a possible permutation to a top k rank \(_{k}\) can be arbitrarily large. For example, in case all individuals are censored, \(_{k}\) is the set of all individuals. Then, the top-k loss is described as:

\[_{}=-_{i_{k}}(_{j=1}^{k }_{ij}). \]

This loss is minimized when the model correctly predicts a top-k rank for the indices in \(_{k}\). This represents the individuals with possible permutations to the top-k highest risk ranks. Importantly, this loss function is optimized for the identification of potential top-k high-risk individuals, without considering the specific order within these top-k ranks. To establish a baseline for comparison with Diffsurv's top-k risk prediction, we also introduce two variants of the Cox Partial Likelihood method. In the first variant, we adjust the likelihood term so that the product considers only the set of patients who have a potential permutation to a top-k rank, according to the matrix of possible permutations \(_{p}\):

\[_{}=_{i:i_{k}}(_{i})}{_{j:t_{j}>t_{i}}f_{}(_{j})} \]

In the second variant, we further limit the set of patients to those who have both experienced an event and have a possible permutation to a top-k rank:

\[_{}=_{i:_{i}=1 i_{k}} (_{i})}{_{j:T_{j}>T_{i}}f_{}(_{j})}. \]

Note that the denominator term is unchanged in both variants and considers only comparable pairs and includes censored patients \(T_{j}>Ti\). Evaluation of top-k risk prediction is also complicated by the uncertainty due to censoring. For Diffsurv and both variants of the Cox Partial Likelihood, we can first define the set of individuals predicted to be within the top-k highest risk:

\[_{k}=\{i|(f_{}(x_{i})) k\} \]

We can then define the fraction of how many of these individuals are in the set of possible top-k highest ranks \(_{k}\) to evaluate the top-k risk prediction performance:

\[=_{k}_{k}|}{|_{ k}|} \]

## 4 Experiments

In our experiments, we aim to assess the performance of _Diffsurv_ and compare it against the conventional Cox Partial Likelihood (CPL) methods. Initially, we focus on confirming the importance of taking a listwise approach and evaluating the ability of differentiable sorting networks to better capture the inherent transitivity in semi-simulated data. Subsequently, we extend our analysis to compare Diffsurv and its top-k extension across multiple publicly available real-world datasets.

**Baselines**: We compare _Diffsurv_ primarily against Cox's Partial Likelihood, using the Ranked List implementation from pycox (Kvamme et al., 2019). We include Efron and Breslow estimates of CPL from Yang et al. (2022) for survSVHN. For smaller datasets, we add non-deep learning baselines: Lifelines' Cox Regression (Davidson-Pilon, 2019) and sksurv's Random Survival Forests (Polsterl, 2020). We do not compare with DeepHit (Lee et al., 2018) since we do not model non-proportional hazards. For an extended discussion, see Appendix A.

**Network Architectures**: For both CPL and Diffsurv, we use a fixed neural network architecture depending on the dataset. Small datasets utilize a single-layer neural network, survSVHN uses a ConvNet architecture as in Petersen et al. (2021), and MIMIC IV CXR uses EfficientNet-B0 (Tan and Le, 2020).

**Training and Evaluation**: We employ AdamW for optimization. Validation approach varies: for smaller datasets, we apply nested 5-fold cross-validation, while for imaging datasets we use train:val:test splits. We performed hyperparameter tuning for learning rate, weight decay, batch size, and risk set size. In the case of imaging datasets, we maintained fixed values for learning rate and weight decay. As in Petersen et al. (2021), we determine steepness as a function of the risk set size \(n\), \(=2n\) for odd-even and \(=(_{2}n)(1+_{2}n)\) for bitonic. The type of sorting network can either be bitonic or odd-even and is determined during hyperparameter tuning. Further details on the experimental setup, including compute time, are provided in Appendix B and at [https://github.com/andre-vauvelle/diffsurv](https://github.com/andre-vauvelle/diffsurv).

**Semi-synthetic survSVHN**: Based on the Street View House Numbers (SVHN) dataset (Netzer et al., 2011), we simulate survival times akin to survMNIST Polsterl (2019). The increased complexity of SVHN over MNIST offers a testbed which is better able to discern the performance differences between methods. Each house number parameterizes a beta-exponential time function for survival times. Risk parameters or hazards \(_{i}\) are calculated as the logarithm of house numbers, standardized and scaled for a mean survival time of 30. We introduce censoring by randomly selecting 30% of house numbers and replacing true times with values sampled uniformly between \((0,t_{i}]\) (See Figure 3).

We can examine the implications of inherent transitivity within the data. Instead of parameterizing a time function based on unique hazards derived from house numbers, we group \(_{i}\) into distinct hazard quantiles. Each quantile encompasses a set of house numbers associated with a similar hazard level. We then calculate the transitivity ratio, defined as \(}{\#}\), where a sampled triplet is considered transitive if (\(_{i}>_{j}>_{k}\)).

This methodology provides us with a means to control the degree of transitivity in our data. At one extreme, we might categorize data into only two groups, representing the lower and upper halves of house numbers, which results in a transitivity ratio of 0. At the other extreme, each house number could constitute its own unique category (indicated by \(\)), leading to high transitivity.

Our results, summarized in Table 1, align with the expectations laid out in Section 2.3: both Diffsurv and CPL methods perform similarly when the risk set size is at its minimum (\(n=2\)). However, with the expansion of the risk set size, the performance of the two methods diverges, with Diffsurv consistently outperforming CPL. Table 2 sheds light on a potential reason for this divergence. As

Figure 3: Visual abstract of the survSVHN dataset.

the number of quantiles is increased, thereby enhancing the degree of transitivity within the data, Diffsurv-based methods start to surpass CPL methods. This finding underscores the role of transitivity in survival data and validates Diffsurv's effectiveness in encapsulating this inherent property. Despite the strong performance of Diffsurv, the C-index for the ground truth risks is 0.980, which is still far above 0.943 for Diffsurv, highlighting the challenging nature of the survSVHN dataset.

**Real-world datasets**: We assess our methods on several public datasets: Four small, popular real-world survival datasets (FLCHAIN, NWTCO, SUPPORT, METABRIC) (Kvamme et al., 2019) and the MIMIC IV Chest X-Ray dataset (CXR) with death as the event (Johnson et al., 2019). Further details in Appendix B.1.

The results presented in Table 3 demonstrate that Diffsurv achieves equal to or better performance on all datasets analyzed. Additionally, when Diffsurv is optimized for predicting the top 10% of highest-risk individuals, it matches or outperforms Cox's partial likelihood on the real-world datasets.

## 5 Conclusion

Diffsurv introduces a new perspective in survival analysis with censored data, highlighting the relations between survival analysis and the listwise ranking. Our experiments show the effectiveness of differentiable sorting methods for improving survival analysis predictions. Notably, Diffsurv matches or surpasses the performance of the established CPL methods across all examined datasets.

Crucially, Diffsurv sheds light on the importance of transitivity in ranking and survival data, revealing that methods sensitive to this inherent property, such as Diffsurv, show improved performance over those that are not. This insight underscores the value of a listwise approach in dealing with survival data and encourages further exploration for methods that promote a transitive inductive bias.

Moreover, Diffsurv provides a foundation for the development of innovative methods, including the top-k risk stratification method introduced in this work. Beyond survival analysis, the introduction of the possible permutations carries potential for other tasks that involve ranking based on limited order information. The utilization of specialized sorting networks, such as splitter selection networks as in Petersen et al. (2022b), could further leverage partial order information.

Though promising, this work is not without limitations. Future research could focus on extending its applicability to non-proportional hazards and understanding the impact of ties. Moreover, investigating how well it can recover survival functions using approaches like Breslow's estimator and evaluating with Brier scores would provide valuable insights into its potential and limitations.

   Number of Quantiles & 2 & 4 & 8 & 16 & 32 & 64 & 128 & \(\) \\ Transitivity Ratio &.0 &.374 &.657 &.819 &.908 &.954 &.975 &.991 \\  Diffsurv: Bitonic &.643 &.803 &.882 &.922 &.933 &.939 &.939 &.939 \\ Diffsurv: Odd Even &.646 &.802 &.883 & **.923** & **.935** & **.939** & **.940** & **.941** \\  CPL: Ranked List &.651 &.803 &.880 &.909 &.916 &.920 &.921 &.920 \\ CPL: Efron &.647 &.801 &.871 &.898 &.904 &.905 &.909 &.908 \\ CPL: Breslow &.648 &.801 &.871 &.898 &.904 &.909 &.907 &.910 \\   

Table 2: Results for training on survSVHN while increasing transitivity. Metric is c-index. Mean performance over 3 trails with different seeds. **Bold** indicates significant improvement (t-test, \(p 0.01\)). Restricted to a fixed batch size and risk set size of 32.

   Risk set size & 2\({}^{}\) & 4 & 8 & 16 & 32 \\  Diffsurv &.918 (.003) & **.934** (.002) & **.940** (.001) & **.943** (.002) & **.941** (.002) \\ Cox Partial Likelihood &.913 (.002) &.925 (.002) &.931 (.002) &.933 (.002) &.930 (.003) \\   

Table 1: Results for training on survSVHN with increasing risk set size. Mean (standard deviation) c-index over 5 trails with different seeds. \({}^{}\) When \(n=2\) both methods are equivalent to the ranking loss to up continuous relaxation of swap operation.

Furthermore, it is important to note that Diffsurv is a survival ranking method and thus can not be used to directly estimate the expected duration until an event occurs.

Overall, Diffsurv constitutes a meaningful advancement in survival analysis, showcasing its significant potential for enhancing risk prediction in real-world use cases. It not only demonstrates promising performance improvements, but also introduces new directions for future research, thereby making a valuable contribution to the field.

## 6 Acknowledgements

We extend our gratitude to Felix Peterson, Leon Sixt, and Samuel Holt for their insightful discussions and invaluable feedback, which significantly contributed to the quality of this work. We also thank the anonymous reviewers for their thoughtful feedback and fruitful discussions, which have been instrumental in enhancing the manuscript.

    & FLCHAIN & NWTCO & SUPPORT & METABRIC & MIMIC IV CXR & survSVHN \\  Size & 6,524 & 4,028 & 8,873 & 1,904 & 377,110 & 248,823 \\ Censored Proportion & 69.9\% & 85.8\% & 32.0\% & 42.1\% & 60.9\% & 30.0\% \\  
**Survival** & & & & & & \\  Cox Regression &.750 (.083) &.692 (.021) &.598 (.010) &.628 (.013) & - & - \\ Random Survival Forest &.789 (.011) &.691 (.024) &.614 (.009) &.641 (.012) & - & - \\ Cox Partial Likelihood &.794 (.013) &.709 (.015) &.642 (.006) &.698 (.011) &.760 (.002) &.933 (.002) \\ Diffsurv & -.793 (.009) &.703 (.026) &.645 (.002) &.684 (.011) & **.763** (.001) & **.943** (.002) \\  
**Top 10\% prediction** & & & & & & \\  Cox Partial Likelihood &.460 (.013) &.390 (.068) &.280 (.023) &.249 (.065) &.390 (.010) & - \\ CPL-TopK (Variant I) &.469 (.007) &.413 (.061) &.479 (.016) &.527 (.083) &.408 (.008) & - \\ CPL-TopK (Variant II) &.460 (.009) &.413 (.054) &.479 (.035) &.487 (.058) &.406 (.006) & - \\ Diffsurv &.452 (.011) &.395 (.082) &.296 (.015) &.331 (.102) &.412 (.002) & - \\ Diffsurv-TopK & **.482** (.019) & **.421** (.065) & **.508** (.027) &.533 (.092) &.412 (.009) & - \\   

Table 3: Results for real-world and semi-synthetic datasets. Mean (standard deviation). Survival metric is c-index, Top 10% metric is top-k-score. **Bold** indicates significant improvement (t-test, \(p 0.01\)).