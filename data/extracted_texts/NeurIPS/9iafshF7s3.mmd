# Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation

Yun Xing\({}^{1}\) Jian Kang\({}^{1}\) Aoran Xiao\({}^{1}\) Jiahao Nie\({}^{1}\) Ling Shao\({}^{2}\) Shijian Lu\({}^{1}\)

\({}^{1}\) Nanyang Technological University

\({}^{2}\) UCAS-Terminus AI Lab, UCAS, China

Corresponding Author.

###### Abstract

Vision-Language Pre-training has demonstrated its remarkable zero-shot recognition ability and potential to learn generalizable visual representations from language supervision. Taking a step ahead, language-supervised semantic segmentation enables spatial localization of textual inputs by learning pixel grouping solely from image-text pairs. Nevertheless, the state-of-the-art suffers from clear _semantic gaps_ between visual and textual modality: plenty of visual concepts appeared in images are missing in their paired captions. Such semantic misalignment circulates in pre-training, leading to inferior zero-shot performance in dense predictions due to insufficient visual concepts captured in textual representations. To close such _semantic gap_, we propose Concept Curation (CoCu), a pipeline that leverages CLIP to compensate for the missing semantics. For each image-text pair, we establish a _concept archive_ that maintains potential visually-matched concepts with our proposed _vision-driven expansion_ and _text-to-vision-guided ranking_. Relevant concepts can thus be identified via _cluster-guided sampling_ and fed into pre-training, thereby bridging the gap between visual and textual semantics. Extensive experiments over a broad suite of 8 segmentation benchmarks show that CoCu achieves superb zero-shot transfer performance and greatly boosts language-supervised segmentation baseline by a large margin, suggesting the value of bridging _semantic gap_ in pre-training data. Code is available at https://github.com/xing0047/rewrite.

## 1 Introduction

Vision-Language Pre-training , which aims to learn visual representations directly from natural language supervision, has endowed existing recognition systems with superior generality and open-vocabulary understanding capability. As a representative, CLIP  performs contrastive language-image pre-training on 400M web-crawled image-text pairs, whereby the learnt models may effortlessly transfer to a wide spectrum of classification tasks in a zero-shot manner. Motivated by the breakthrough, recent studies  extend the supervision paradigm to semantic segmentation, enabling spatial localization of textual queries in images and pixel grouping with solely supervision from image-text pairs. Distinct from conventional semantic segmentation, the language-supervised paradigm obviates the need for costly manual pixel-level annotation and enables million-level pre-training scale with much less effort.

Despite the progresses  in language-supervised semantic segmentation, the pre-training stage still suffers heavily from clear _semantic gap_ between visual and textual modality. In image-text pairs used for pre-training, it is ubiquitous that visual concepts appeared in images are missing in the corresponding textual captions. This happens largely because captions merely describe salient concepts that are worthy of mention , while naturally forgo full semantic coverage of images(Fig. 1 (a)). Under the presence of clear cross-modal _semantic gap_ in image-text pairs, the pre-training stage of language-supervised segmentation is found to be harder to converge, leading to inferior zero-shot performance on downstream tasks (more details are elaborated in Section 4.3).

This work explores to bridge _semantic gaps_ in language-supervised semantic segmentation. For each image in the pre-training data, the goal is to recover the missing visual concepts in its paired caption for more comprehensive image-text modeling. With the rich vision-language correlations in off-the-shelf foundation models such as CLIP , a straight solution is to retrieve the missing concepts from the text captions of pre-training data. However, such retrieved captions suffer from the _semantic bias_ illustrated in Fig. 1 (b) (e.g., "person" recovered but "grass" and "sky" still missing). The root cause lies with the original text captions in foundation model pre-training, which only capture salient concepts appeared in the paired images. Hence, the retrieved captions and concepts still suffer from clear cross-modal _semantic gap_.

We propose Concept Curation (CoCu), a novel pipeline that side-steps the negative effect by _semantic bias_2 while exploiting vision-language foundation models for semantic segmentation. CoCu consists of three sequential stages: 1) _vision-driven expansion_ that constructs _concept archive_ via cross-image retrieval; 2) _text-to-vision-guided ranking_ that scores the retrieved concepts according to their assigned relevancies; and 3) _cluster-guided sampling_ that exploits semantic diversity beyond the relevancy scores for concept ranking (Fig. 1 (c)). We perform pre-training from scratch on the segmentation backbone of  and evaluate zero-shot transfer over 8 widely adopted segmentation benchmarks. The experiments show that the proposed CoCu improves the baseline as well as the state-of-the-art consistently by large margins, indicating the necessity of closing the _semantic gap_ and the effectiveness of our designs in concept archiving and concept ranking.

In summary, the contributions of this work are three-fold. _First_, we identify the issue of _semantic gap_ in language-supervised semantic segmentation, and demonstrate the effectiveness of mining more relevant visual concepts for segmentation model pre-training. _Second_, we design Concept Curation (CoCu), a novel pipeline that constructs _concept archives_ to expand and identify relevant

Figure 1: Cross-modal _semantic gap_ is prevalent in web-crawled image-text pairs. As in (a), the caption text often captures certain salient visual concepts only in the paired image but misses many others (i.e., ‘_person_’, ‘_grass_’, and ‘_sky_’) that are also useful in image-text modeling. Leveraging CLIP , more useful visual concepts could be captured via image-to-text retrieval, but the retrieved captions usually suffer from the _semantic bias_ as in (b) (i.e., ‘_person_’ recovered but ‘_grass_’ and ‘_sky_’ still missing). Our proposed Concept Curation (CoCu) bridges the cross-modal _semantic gap_ effectively by _vision-driven expansion_, _text-to-vision-guided ranking_ and _cluster-guided sampling_ while avoiding the negative effect by _semantic bias_, as illustrated in (c). Best viewed in color.

visual concepts from pre-training data, mitigating the _semantic gap_ between textual and visual modalities effectly. _Third_, extensive experiments show that the proposed pipeline achieves superior zero-shot transfer performance and outperforms the state-of-the-art across 8 segmentation benchmarks consistently by large margins.

## 2 Related Works

**Semantic Segmentation.** Partitioning an image into semantic regions, also known as semantic segmentation, has been widely studied due to myriad real-world applications such as video surveillance and autonomous driving. It has been explored along different directions, e.g., by designing different network architectures [29; 9; 11; 42], constructing benchmarks of diverse scenes and categories [12; 50; 5; 17], etc. However, collecting per-category dense annotations for supervised training is notoriously labor-intensive, which impedes the upscaling of semantic vocabulary greatly. Different from conventional semantic segmentation, segmentation from language supervision [43; 28; 7; 32; 35; 44] relieves the burden of mask annotation by leveraging image-text pairs available on the Internet. Beyond that, it can handle arbitrary new semantics thanks to the language supervision paradigm, making it feasible to learn generalizable segmentation models.

**Vision-Language Pre-training.** Recently, Vision-Language Pre-training has become a predominant trend by learning visual representations from natural language supervision [34; 21; 24; 47; 1; 31; 10; 49]. By matching billion-scale image-text pairs via contrast, the learnt representations can be seamlessly transferred to various downstream classification tasks in a zero-shot manner. As a representative, CLIP  can match the performance of supervised baselines on ImageNet , meanwhile obtain competitive performance over plethora of downstream tasks without accessing any target data. The same learning paradigm has recently been explored for the task of semantic segmentation by hierarchical grouping , supervision mining [35; 44; 7; 32], etc. Nevertheless, state-of-the-art language-supervised segmentation is held back by the cross-modal _semantic gap_ between textual and visual pre-training data. Instead of relaxing the strict one-to-one correspondence in vanilla contrastive learning , we mitigate the _semantic gap_ by automated curation of relevant visual concepts through concept expanding and concept ranking.

**Open-Vocabulary Semantic Segmentation.** Open-Vocabulary Semantic Segmentation has been studied extensively and most existing work can be broadly grouped into three categories. **Mix Supervision:** the first category follows a zero-shot manner [4; 23] which aims to segment new classes by learning from densely annotated seen classes in the pre-training data. Recently, several studies [18; 22; 46; 19; 26; 52; 45; 48] introduce language supervision to enhance the generality of the learnt zero-shot models. These approaches require no data annotation from new classes, but still rely on dense annotations from seen classes during the pre-training stage. **No Supervision**: the second category follows a training-free approach [51; 38; 37] which explores the segmentation potential of frozen vision-language models (VLMs) [51; 38; 37] to predict segmentation masks. However, most VLMs are trained with image-level supervision which restricts their capability on pixel/region-level predictions in semantic segmentation. **Language Supervision:** the third category follows a pure-language-supervision paradigm [43; 28; 32; 35; 7; 44] which aims to learn pixel grouping from solely image-text pairs. Our work follows the third approach. Different from existing studies, we identify the _semantic gap_ in pre-training image and text data and design concept curation that mitigates the _semantic gap_ with clearly improved semantic segmentation performance, more details to be described in the ensuing subsections.

## 3 Methodology

With clear _semantic gaps_ between visual and textual concepts in pre-training data as illustrated in Fig. 1 (a), one naive solution is to employ given image as query to retrieve related captions and derive missing textual concepts as described in Sec. 3.2. However, the naive solution suffers from clear _semantic bias_ as most VLM-retrieved captions contain salient concepts only as illustrated in Fig. 1 (b). We thus further design _vision-guided expansion_, _text-to-image-guided ranking_ and _cluster-guided sampling_ for better mitigation of the _semantic gap_ as presented in Fig. 1 (c) and Sec. 3.3.

### Revisiting GroupViT

**Segmentation Architecture.** We use GroupViT  as the segmentation backbone for pre-training. Assume a batch of image-text pairs \(\{(x^{I},x^{T})\}_{i=1}^{B}\), where \(x^{I}\) amd \(x^{T}\) denotes an image and its paired caption, respectively. For the vision flow, a grouping-aware transformer \(_{s}^{I}\) encodes image \(x^{I}\) as G segment tokens \(z_{}^{I}=\{z_{_{g}}^{I},g=1,...,G\}^{G  d}\), where each segment token \(z_{_{g}}^{I}^{d}\) encodes an arbitrary-shaped region in image \(x^{I}\).

**Image-Caption Contrastive Loss.** To perform pre-training, the segment tokens \(Z_{}^{I}\) are merged via average pooling, producing a global representation \(z_{}^{I}^{d}\) that captures all the visual concepts appeared in image \(x^{I}\). Meanwhile, the paired caption \(x^{T}\) is encoded to \(z^{T}^{d}\) by a text encoder \(^{T}\). The visual embedding \(z_{}^{I}\) and textual embedding \(z^{T}\) are mapped to the same space by separate linear projectors. The segmentator is then learnt from language supervision by the standard contrastive objective InfoNCE , which is defined as:

\[_{I T} =-_{i=1}^{B}^{I} z_{i}^{T}/ )}{_{j=1}^{B}(z_{i}^{I} z_{j}^{T}/)}\] (1) \[_{T I} =-_{i=1}^{B}^{T} z_{i}^{I}/ )}{_{j=1}^{B}(z_{i}^{T} z_{j}^{I}/)}\] (2)

where \(\) is a learnable parameter initialized with 0.07  and the \(z^{I} z^{T}\) computes the cross-modal cosine similarity.

**Multi-Label Loss**. Beyond learning segmentation from raw caption \(x^{T}\), GroupViT  further introduces \(L\) extra text labels \(\{x^{T_{l}},l=1,...,L\}\) by prompting the extracted concepts \(\{c_{l},l=1,...,L\}\) with handcrafted templates  (e.g., "a photo of a {concept}"). The \(L\) text labels are fed to the same text encoder \(^{T}\) to obtain textual representations of \(\{z^{T_{l}},l=1,...,L\}\). The language supervision by multi-label loss is thus defined as:

\[_{I\{T_{l}\}_{l=1}^{L}}=-_{i=1}^{B} ^{L}(z_{i}^{I} z_{i}^{T_{l}}/)}{_{l=1}^{L} _{j=1}^{B}(z_{i}^{I} z_{j}^{T_{l}}/)}\] (3)

\[_{\{T_{l}\}_{l=1}^{L} I}=-_{l=1}^{L} _{i=1}^{B}^{T_{l}} z_{i}^{I}/)}{_{j=1}^{B }(z_{i}^{T_{l}} z_{j}^{I}/)}\] (4)

The overall training objective of learning segmentation from language supervision in  is defined as:

\[=_{I T}+_{\{T_{l}\}_{l=1}^{L}  I}\] (5)

**Discussion.** We use the exact same training objective as in GroupViT  to learn segmentation from language supervision. For each pre-training image \(x^{I}\), the _multi-label loss_ enhances the contrastive learning  with \(L\) extra positive pairs and \(L(B-1)\) extra negative pairs (\(B\) denotes batch size used in pre-training). However, we highlight that the simple concept prompting does not expand the textual concepts much. The cross-modal _semantic gap_ still exists and circulates in pre-training, which holds back the training convergence and degrades the zero-shot transfer.

### Naive Solution

**Caption Curation.** The web-crawled image-text pairs are often noisy with imprecise and even irrelevant text descriptions . In addition, many visual concepts (especially those inconspicuous in the background) in images are often missing in the corresponding text descriptions. Both factors lead to clear _semantic gaps_ between web-crawled images and texts. With super-rich image-text correlations in pre-trained VLMs such as CLIP , a straight solution, which we term by _caption curation_, is to apply \(x^{I}\) as query to retrieve \(L\) extra captions \(\{x^{T_{l}},l=1,...,L\}\) from pre-training data. The _semantic gaps_ between visual and textual modality could thus be mitigated by identifying relevant concepts from the retrieved captions.

**Semantic Bias.** Though _caption curation_ expands visual concepts effectively, retrieved captions often suffer from clear _semantic bias_: VLMs tend to retrieve salient concepts but miss many inconspicuous ones that are also useful in image description. Consequently, visual concepts \(C^{I}=\{c_{l},\;l=1,...,M^{I}\}\) appeared in an image \(x^{I}\) are usually clearly more than textual concepts \(C^{T}=\{c_{l},\;l=1,...,M^{T}\}\) extracted from \(\{x^{T},x^{T_{1}},...,x^{T_{L}}\}\) (i.e., \(M^{I}>M^{T}\)). The root cause of the _semantic bias_ lies with the loose correlation between the visual and textual pre-training data of VLMs, where most captions just capture partial visual concepts appeared in the paired images . The _semantic bias_ thus impedes convergence and effectiveness of language-supervised training without language supervision available for those non-described image regions.

### Concept Curation

To bridge semantic gaps in image-text pairs, we propose **Concept Curation (CoCu)** to rewrite caption semantics with the help of a pre-trained vision-language model. Consequently, **CoCu** finds more concept candidates that are aligned to images and compensate for missing semantics in captions. In pre-training, a multi-modal segmentor matches images and visual-enriched captions by contrastive objectives mentioned in Sec. 3.1, encoding better vision-language alignment in its representations. Details of CoCu are described as below.

**Vision-driven Expansion.** For an image-text pair \((x^{I},\;x^{T})\), the goal of the vision-driven expansion is to build an archive of textual concepts \(C^{T}=\{c_{m},\;m=1,...,M\}\) that are potentially matched with \(x^{I}\) as illustrated in Fig. 2. Instead of acquiring \(C^{T}\) via direct text retrieval as in _caption curation_, we resort to cross-image retrieval to achieve the expansion. Concretely, \(N\) image-text pairs \(P=\{(x^{I}_{i},\;x^{T}_{i}),\;i=1,...,N\}\) are automatically selected from the pre-training data, where \(\{x^{I}_{i},\;i=1,...,N\}\) are N captioned images whose visual features match the best with that of \(x^{I}\) (all encoded by CLIP). \(C^{T}\) can thus be derived by extracting textual concepts from the captions

Figure 2: Illustration of _vision-driven expansion_ (above) and _text-to-image-guided ranking_ (below) in CoCu. To compensate for missing semantics, _vision-driven expansion_ establishes an archive of potential matched concepts through image-to-image retrieval, while _text-to-vision-guided ranking_ scores retrieved concepts based on assigned relevancy. The textual concepts can later be identified in pre-training by sampling. In the figure, images with a blue border \(\) are retrieved via expanded concepts (marked as blue) using their paired captions, while images with a red border \(\) represent images for curation (as anchor). Best viewed in color.

\(\{x_{i}^{T},\ i=1,...,N\}\) of the \(N\) best matched images. Compared with the _caption curation_ that retrieves \(L\) descriptions \(\{x_{i}^{T},\ i=1,...,L\}\), _vision-driven expansion_ exploits all visual information in images instead of biased caption texts (mostly describes salient visual concepts only), which helps restore more relevant textual concepts. In addition, it builds an extra image set \(\{x_{i}^{I},\ i=1,...,N\}\) that plays a pivotal role in the upcoming stages.

**Text-to-Vision-Guided Ranking.** For each textual concept \(c_{m}\) in the concept archive \(C^{T}\), we assign a score \(s_{c_{m}}\) to represent its relevancy to image \(x^{I}\). A naive solution to \(p_{m}\) is to compute the cosine similarity between the visual representation of \(x^{I}\) and textual representation of \(t_{m}\) encoded by CLIP , which is simply defined as:

\[s_{c_{m}}^{a}=f(x^{I},\ t_{m})\] (6)

where \(t_{m}\) is derived from visual concept \(c_{m}\) via prompt engineering [34; 43]. Note direct text retrieval could easily get biased towards salient visual concepts here, imposing low relevance scores for other text concepts in _concept archive_. Beyond \(f(x^{I},\ t_{m})\), we also design a non-biased metric to capture the relevancy between image \(x^{I}\) and concept \(c_{m}\). Specifically, for the \(N\) retrieved image-text pairs \(P\), we first extract a subset \(P_{G}\) (blue box/text in Fig. 2 (below)), whose caption of each image-text pair contains the textual concept \(c_{m}\). The non-biased metric is thus defined with \(x^{I}\) (red box) and image-text pairs \(P_{G}=\{(x_{i}^{I},x_{i}^{I}),\ i=1,...,N^{}\}\) as follows:

\[s_{c_{m}}^{b}=)f(t_{m},x^{I})}{f(t_{m},x^{I})+_{i=1}^{N ^{}}f(t_{m},x_{i}^{I})}\] (7)

The given term functions as follows: 1) lower relevancy between image \(x^{I}\) and irrelevant concepts (e.g., 'horse' in Fig. 2); 2) enhance relevancy between \(x^{I}\) and inconspicuous concepts (e.g., 'grass'). Instead of computing relevancies of \(x^{I}\) to all textual concepts in a single run, we consider one \(c_{m}\) at a time and measure its relevancy by comparing \(f(x^{I},\ t_{m})\) with \(\{f(x_{i}^{I},\ t_{m}),\ i=1,...,N^{}\}\). The idea behind this is simple: 1) comparing the responses of images \(\{x^{I},x_{1}^{I},...,x_{N^{}}^{I}\}\) to the same textual concept \(c_{m}\); 2) high relevancy is given if response of \(x^{I}\) to textual concept \(c_{m}\) is comparably high and vice versa. Take the visual concept 'grass' in Fig. 2 (below) as an example. The \(t_{m}\) ('a photo of grass') causes \(x^{I}\) to rank considerably higher than image captioned with \(c_{m}\) ('grass'). In this case, we should be fairly confident to pass the concept \(c_{m}\) to \(x^{I}\). In conclusion, the relevancy score is simply defined as:

\[s_{c_{m}}=s_{c_{m}}^{a}\ +\ s_{c_{m}}^{b}\] (8)

we perform ranking according to computed relevancies \(\{s_{c_{m}},\ m=1,...,M\}\), which represents chances identified by later sampling.

**Cluster-guided Sampling.** The pre-training can thus be empowered by including the expanded and ranked textual concepts which as selected by sampling \(L\) textual concepts according to their computed relevancies as in . However, selection with relevancy alone is often short of semantic diversity in the selected text concepts. Instead of directly selecting \(L\) concepts from the ranked archive \(C^{T}\), we partition \(C^{T}\) into \(L\) semantic clusters based on their textual representations and sample one textual concept from each semantic cluster. The _cluster-guided sampling_ has two clear benefits: 1) it includes more diverse semantics in each single training step; 2) it keeps good consistency with the expression of visual concepts, more details to be discussed in Sec. 4.4 and appendix.

## 4 Experiments

### Experimental Setup

**Training Detail**. We follow the prior study  and conduct pre-training on three publicly available image-text datasets: CC3M (C3) , CC12M (C12) , YFCC14M (Y14) . For fair comparison, we use the same GroupViT  as the visual encoder, which is built upon ViT-S backbone [14; 40] and learnt from scratch. We set the global batch size for contrastive learning as 1,024 and use 4 Tesla V100 GPUs to carry out pre-training for all experiments. Consistent with , we set the initial learning rate to 0.0016. The pre-training undergoes 30 epochs, with a linear warmup for the first 2 epochs and a cosine schedule for the remaining epochs. \(L\) is set to 3. In our ablations and discussions, we report the performance of models pre-trained on CC3M.

**Implementation of curation**.The curation pipeline utilizes clip-retrieval , a utility that enables efficient computation of CLIP embeddings and fast indexing for retrieval. We employ the CLIP ViT-B/16  model for image/text inference and concept curation. For efficient semantic searching, we build indexing systems using autofaiss 3. It is worth mentioning that alternative systems can also be used for implementing the curation process.

**Evaluation**. We benchmark zero-shot transfer performance of CoCu on the validation splits of eight different datasets that cover a myriad of scenes and category sets, including Pascal VOC , Pascal Context , COCO , ImageNet-S-50, ImageNet-S-300 , COCO Stuff , Cityscapes , and ADE20K . For the first five datasets, we follow  and evaluate foreground classes by thresholding the similarity between visual and textual embeddings. For other datasets, we evaluate both foreground and background classes. More details are given in the appendix.

### Comparison with the state-of-the-art

We first benchmark CoCu with state-of-the-art zero-shot methods [51; 43] and evaluate its effectiveness. Specifically, we follow prior work  and pre-train CoCu over the combination of C3, C12, and Y14 datasets. Tab. 1 reports zero-shot segmentation results. Besides GroupViT as the baseline method, we also compare the advanced MaskCLIP ), which directly leverages the frozen CLIP model for segmentation prediction without pre-training. In addition, for a comprehensive comparison, we list the performance of other advanced methods including 1) fully-supervised method  that provides Oracle's performance, 2) self-supervised methods [20; 6] that pre-train models with unlabeled data and fine-tuning models over segmentation datasets. Detailed implementations of the comparing methods could be found in the appendix.

As shown in Tab. 1, MaskCLIP achieves limited segmentation performance, primarily due to CLIP being trained with image-level supervision and thus falling short in precise pixel-level predictions. GroupViT achieves better performance than MaskCLIP, but still limited by insufficient supervision from language side in pre-training. On the contrary, our CoCu achieves the best segmentation performance over all eight benchmarks, surpassing GroupViT by large margins on average. This indicates the necessity of bridging _semantic gaps_ in language-supervised semantic segmentation and the effectiveness of our design.

We further evaluate the robustness of CoCu with different pre-train data. Spcifically, we pre-train GroupViT and CoCu over CC3M and CC12M, respectively. We also sub-sample half of image-text pairs from CC12M (denoted as C12\({}^{*}\)) for pre-training. Tab. 2 shows the experimental results. We can observe consistent yet significant performance gains on eight benchmarks. The improvement by bridging _semantic gap_ is thus robust and not affected by pre-training size.

  
**Method** & **Pretrain Data** & **Supervision** & **LC** & **BS** & **Backbone** & **PVOC** & **PCON** & **COCO** & **NS0** & **IN00** & **CITY** & **ADE** & **STUF** & **AVG** \\  DeT  & IN-1K & full & - & VIT-S & 53.0 & 35.9 & - & - & - & - & - & - & - \\  MoCo  & IN-1K & self & - & - & 34.3 & 21.3 & - & - & - & - & - & - & - \\ DINO  & IN-1K & self & - & - & 39.1 & 20.4 & - & - & - & - & - & - & - \\ MoCo  & C12Y4 & self & - & - & - & 36.1 & 23.0 & - & - & - & - & - & - & - \\ DINO  & C12Y14 & self & - & - & 37.6 & 22.8 & - & - & - & - & - & - & - \\  MaskCLIP  & - & N.A & ✓ & - & ResNet-50 & 41.5 & 18.5 & 10.5 & 13.8 & 7.9 & 18.8 & 8.3 & 10.2 & 15.0 \\ MaskCLIP  & - & N.A & ✓ & - & VIT-B/16 & 49.5 & 21.7 & 13.6 & 25.9 & 11.7 & 19.8 & 9.5 & 12.5 & 20.5 \\  GroupViT  & C3,C12,Y14 & text & & 4,096 & VIT-S & **52.4** & 22.3 & **24.3** & 44.3 & 23.5 & 15.8 & 10.4 & 13.0 & 25.7 \\ GroupViT  & C3,C12,Y14 & text & & 1,024 & VIT-S & 43.8 & 19.3 & 19.6 & 37.8 & 17.2 & 17.2 & 10.4 & 13.6 & 22.4 \\ CoCo (ours) & C3,C12,Y14 & text & & 1,024 & VIT-S & 49.7 & 22.8 & 22.0 & 46.7 & 24.7 & 21.9 & 12.0 & 14.9 & 26.8 \\  GroupViT  & C3,C12,Y14 & text & ✓ & 1,024 & VIT-S & 45.4 & 19.9 & 20.3 & 39.2 & 17.7 & 17.6 & 10.6 & 13.9 & 23.1 \\ CoCo (ours) & C3,C12,Y14 & text & ✓ & 1,024 & VIT-S & 51.4 & **23.6** & 22.7 & **45.8** & **25.5** & **22.1** & **12.3** & **15.2** & **27.7** \\   

Table 1: **Performance of different zero-shot methods for semantic segmentation. Abbreviations of benchmarks, from left to right: Pascal VOC , Pascal Context , Microsoft COCO , ImageNet-S , Cityscapes , and ADE20K . BS denotes pre-training batch size, while LC represents local consistency  in mask prediction. \(\) denotes our re-implementation. CoCu consistently achieves the best performance across all benchmarks.**

### CoCu helps convergence

**Loss Curve.** In Figure 3 (a), we compare the pre-training loss curves of GroupViT and our proposed method CoCu. We can see that CoCu exhibits a notably faster convergence rate, primarily attributed to the inclusion of curated semantic concepts for each image, resulting in more effective contrastive learning. Additionally, CoCu achieves a lower minimum loss by extracting significantly richer language concepts from image data. This enriches the training process by incorporating more identified image regions and ultimately learning representations that better align with the training data.

**Qualitative Comparison.** We also present qualitative results that demonstrate the effectiveness of CoCu. In Figure 3 (b), we show binary segmentation results of GroupViT (first row) and CoCu (second row) over an example image with the caption "a red fox drinking water." Our focus is on the concept of "grass," which is missing in the caption. We compare the visual responses of models trained using these two methods at different checkpoints. Both methods improve progressively during training. However, GroupViT fails to correctly localize the region of "grass" due to the lack of direct supervision from the language side. In contrast, CoCu bridges the semantic gap by accurately capturing and localizing "grass," encoding it in representations during pre-training. Consequently, it achieves significantly better segmentation results under zero-shot context.

Figure 4 displays the activation maps of GroupViT and CoCu for different concepts as text inputs that do not appear in the corresponding captions. These maps further demonstrate the superiority of CoCu in language-supervised learning. In all presented images, GroupViT incorrectly activates corresponding regions based on the given text inputs (e.g., activating the "sky" region with a text input of "person" for the first image). In contrast, CoCu enables the segmentor to have the highest activations on visually relevant regions indicated by the text. This suggests that segmentors derived from our method have a better capability to discriminate various visual concepts. More convergence results can be found in the appendix.

  
**Method** & 
 **Pertual** \\ **Data** \\  & **FOCU (8b)** & **COU (8d)** & **FNQ (5k)** & **FNQ (8e)** & **FNQ (80k)** & **CITY (19)** & **ADE (35k)** & **FIST (37k)** & **Avg** \\  GroupViT\({}^{}\) & C12 & 15.5 & 10.4 & 6.5 & 10.2 & 2.9 & 8.1 & 4.4 & 7.7 & 8.2 \\ CoCu & C3 & 10.6 (+15.1) & 13.3 (+15.0) & 10.8 (+1.2) & 10.3 (+1.5) & 13.3 (+4.7) & 8.2 (+19.5) & 6.1 (+1.7) & 8.5 (+0.9) & 12.1 (+1.9) \\  GroupViT\({}^{}\) & C127 & 20.9 & 13.3 & 13.2 & 129 & 27.9 & 12.4 & 10.7 & 5.6 & 86 & 15.5 \\ CoCu & C127 & 34.1 (+1.49) & 16.4 (+3.15) & 17.0 (+4.15) & 33.0 (+5.15) & 17.3 (+4.9) & 11.8 (+1.1) & 8.1 (+2.9) & 9.5 (+0.9) & 18.4 (+2.9) \\  GroupViT\({}^{}\) & C127 & 36.8 & 15.9 & 16.2 & 31.5 & 14.0 & 12.4 & 7.0 & 10.5 & 8.2 \\ CoCu & C127 & 38.1 (+1.06) & 19.2 (+3.15) & 39.1 (+1.9) & 35.8 (+2.9) & 18.5 (+4.9) & 14.7 (+2.9) & 5.9 (+2.9) & 11.1 (+0.9) & 23.0 (+2.9) \\  GroupViT\({}^{}\) & C12 & 37.5 & 18.0 & 18.3 & 35.7 & 16.8 & 13.5 & 9.1 & 13.1 & 20.2 \\ CoCu & C12 & 40.9 (+3.16) & 21.2 (+3.25) & 20.3 (+9.2) & 40.0 (+3.95) & 19.4 (+2.8) & 15.0 (+1.5) & 11.1 (+2.0) & 13.6 (+0.9) & 22.7 (+2.5) \\   

Table 2: **Zero-shot semantic segmentation performance with different pre-training data.** CoCu consistently outperforms the baseline method GroupViT across all benchmarks, demonstrating its effectiveness in bridging _semantic gaps_ and achieving significant improvements.

Figure 3: **CoCu enhances training convergence.** (a) The training loss curves of GroupViT and CoCu demonstrate that CoCu significantly accelerates pre-training convergence. (b) CoCu achieves superior binary segmentation results (second row) compared to GroupViT (first row) for the concept of ”grass,” which is missing in the caption, using an example image captioned as ”a red fox drinking water.” Best viewed in color.

### Analysis

**Ablation Study.** We further assess the effectiveness of each module in CoCu, which includes _vision-driven expansion_, _text-to-image-guided ranking_ and _cluster-guided sampling_. Specifically, we pre-train five models with the combination of these modules or their alternative strategies, namely: 1) Baseline model of GroupViT, which is pre-trained without involving concept curation. 2) Model #1, which utilizes language-driven expansion, naive ranking, and naive sampling (Caption Curation in Sec. 3.2). 3) Model #2, which replaces language-driven expansion with vision-driven expansion on top of Model #1. 4) Model #3, which incorporates text-to-image-guided ranking on top of Model #2. And 4) the full CoCu Model #4, which combines vision-driven expansion, text-to-image-guided ranking, and cluster-guided sampling in pre-training. We report the average segmentation performance of these models across the eight datasets used previously (as shown in Table 1 and Table 2). Detailed illustrations of implementations are provided in appendix.

As Tab. 3 shows, the simplest strategy of _language-driven_ in model #1 improves average mIoU by \(1.7\%\), which comes from stronger vision-language correlation in pre-training data enhanced by direct text retrieval. Next, replacing direct text retrieval with _vision-driven expansion_ in model #2 brings an additional performance boost, highlighting its significance in capturing unbiased semantics. Furthermore, incorporating _text-to-vision-guided ranking_ in Model #3 brings another noticeable performance gain, underscoring the importance of measuring concept-to-image relevancy. Finally, we upgrade the sampling strategy from the naive one that solely relies on relevancy to _cluster-guided sampling_, and build model #4 with the full CoCu, which provides more diverse semantic information in each pre-training step, ultimately leading to the best zero-shot transfer performance for semantic segmentation.

**Zero-Shot Classification.** In addition to its application in zero-shot segmentation, CoCu can also be used to improve zero-shot classification. Following the previous study , we evaluate CoCu and compare it with GroupViT on the ImageNet-1K dataset . As shown in Table 4, CoCu exhibits significant performance gains over GroupViT, demonstrating its superiority in bridging semantic gaps across tasks and achieving improved zero-shot classification results.

   & Pre-training &  \\   & data & Acc@1(\%) & Acc@5(\%) \\  GroupViT & C12 & 34.9 & 63.3 \\ CoCu (ours) & C12 & **38.4** (4.5 \(\)) & **68.6** (5.3 \(\)) \\  GroupViT & C3,C12,Y14 & 36.8 & 66.8 \\ CoCu (ours) & C3,C12,Y14 & **43.0** (6.2 \(\)) & **73.7** (6.9 \(\)) \\  

Table 4: **Zero-shot classification on ImageNet-1K**. Acc@1 and Acc@5 denote top-1 and top-5 accuracy, respectively.

Figure 4: **Visualization of activation heatmaps.** GroupViT fails to activate on corresponding visual regions for concepts not represented in captions, while CoCu exhibits significantly better localization. High activation is shown as red, and low activation is displayed as blue. Best viewed in color.

    &  & Ranking & Sampling & Average \\   & _lang-driven_ & _vision-driven_ & _naive text-to-vision-guided_ & _naive cluster-guided_ & \\  Baseline  & & & & & \(8.2\) \\
#1 & ✓ & ✓ & ✓ & ✓ & \(9.9\) (\(1.7\)\(\)) \\
#2 & & ✓ & ✓ & ✓ & \(10.3\) (\(2.1\)\(\)) \\
#3 & & ✓ & ✓ & ✓ & \(12.4\) (\(4.2\)\(\)) \\
#4 (_Full_ CoCu) & & ✓ & ✓ & ✓ & \(13.1\) (\(4.9\)\(\)) \\   

Table 3: **Ablation study of CoCu.** We conduct an ablation study on each designed modules. Zero-shot transfer performance on semantic segmentation results are reported, averaged across eight evaluation datasets. “Naïve ranking” refers to solely using cosine similarity between visual and textual representations (encoded by CLIP) as concept-to-image relevancy. “Naïve sampling” denotes selecting textual concepts based solely on relevancy before pre-training.

Conclusion

In this paper, we identify the issue of _semantic gap_ in language-supervised semantic segmentation and explore how to bridge _semantic gaps_ effectively. To achieve this, we design Concept Curation, a novel pipeline that resolves the issue by three consecutive stages: _vision-driven expansion_, _text-to-vision-guided ranking_ and _cluster-guided sampling_. Extensive experiments demonstrate the superiority of our method for boosting language-supervised semantic segmentation across a bundle of pre-training sets and evaluation benchmarks. Looking ahead, we hope to extend the idea of concept curation to other computer vision tasks, including object detection and instance segmentation.