# Inflationary Flows: Calibrated Bayesian Inference

with Diffusion-Based Models

 Daniela de Albuquerque

Department of Electrical &

Computer Engineering

School of Medicine

Duke University

Durham, NC 27708

daniela.de.albuquerque@duke.edu &John Pearson

Department of Neurobiology

Department of Electrical &

Computer Engineering

Center for Cognitive Neuroscience

Duke University

Durham, NC 27708

john.pearson@duke.edu

###### Abstract

Beyond estimating parameters of interest from data, one of the key goals of statistical inference is to properly quantify uncertainty in these estimates. In Bayesian inference, this uncertainty is provided by the posterior distribution, the computation of which typically involves an intractable high-dimensional integral. Among available approximation methods, sampling-based approaches come with strong theoretical guarantees but scale poorly to large problems, while variational approaches scale well but offer few theoretical guarantees. In particular, variational methods are known to produce overconfident estimates of posterior uncertainty and are typically non-identifiable, with many latent variable configurations generating equivalent predictions. Here, we address these challenges by showing how diffusion-based models (DBMs), which have recently produced state-of-the-art performance in generative modeling tasks, can be repurposed for performing calibrated, identifiable Bayesian inference. By exploiting a previously established connection between the stochastic and probability flow ordinary differential equations (pfODEs) underlying DBMs, we derive a class of models, _inflationary flows,_ that uniquely and deterministically map high-dimensional data to a lower-dimensional Gaussian distribution via ODE integration. This map is both invertible and neighborhood-preserving, with controllable numerical error, with the result that uncertainties in the data are correctly propagated to the latent space. We demonstrate how such maps can be learned via standard DBM training using a novel noise schedule and are effective at both preserving and reducing intrinsic data dimensionality. The result is a class of highly expressive generative models, uniquely defined on a low-dimensional latent space, that afford principled Bayesian inference.

## 1 Introduction

In many fields of science, the aim of statistical inference is not only to estimate model parameters of interest from data but to quantify the _uncertainty_ in these estimates. In Bayesian inference, for data \(\) generated from latent parameters \(\) via a model \(p(|)\), this information is encapsulated in the posterior distribution \(p(|)\), computation of which requires evaluation of the often intractable normalizing integral \(p()= p(,)\,d\). Where accurate uncertainty estimation is required, the gold standard remains sampling-based Markov Chain Monte Carlo (MCMC) methods, which are guaranteed (asymptotically) to produce exact samples from the posterior distribution . However, MCMC methods can be computationally costly and do not readily scale either to large or high-dimensional data sets.

Alternatively, methods based on variational inference (VI) attempt to approximate posterior distributions by optimization, minimizing some measure of divergence between the true posterior and a parameterized set of distributions \(q_{}(|)\). For example, methods like the variational autoencoder (VAE) [3; 4] minimize the Kullback-Leibler (KL) divergence between true and approximate posteriors, producing bidirectional mappings between data and latent spaces. In vanilla VAEs, posterior uncertainty estimates are typically overconfident due to minimization of the reverse (mode-seeking) KL divergence [5; 6]. While some lines of work have sought to mitigate this posterior mismatch problem by utilizing different divergences 7-10, VAEs still tend to produce blurry data reconstructions and non-unique latent spaces without additional assumptions [11; 12; 13].

By contrast, normalizing flow (NF) models [14; 15] work by applying a series of bijective transformations to a simple base distribution (usually uniform or Gaussian) to deterministically convert samples to a desired target distribution. While NFs have been successfully used for posterior approximation [16; 17; 18; 19; 20] and produce higher-quality samples, the requirement that the Jacobian of each transformation be simple to compute often requires a high number of transformations and, traditionally, these transformations do not alter the the dimensionality of their inputs, resulting in latent spaces with thousands of dimensions. More recent lines of work on _injective flow_ models 21-25 address this limitation by allowing practitioners to use flows to learn lower dimensional manifolds from data, but most compression-capable flow models still fail to reach high generative performance on key benchmark image datasets (cf. ).

More recently, diffusion-based models (DBMs) [26; 27; 28; 29; 30; 31; 32; 33] have been shown to achieve state-of-the-art results in several generative tasks, including image, sound, and text-to-image generation. These models work by stipulating a fixed forward noising process (e.g., a forward stochastic differential equation (SDE)), wherein Gaussian noise is incrementally added to samples of the target data distribution until all information in the original data is degraded. To generate samples from the target distribution, one then needs to simulate the reverse de-noising process (reverse SDE ) which requires knowledge of the score of the intermediate "noised" transitional densities. Estimation of this score function across multiple noise levels is the key component of DBM model training, typically using a de-noising score matching objective [35; 28; 30]. Yet, despite their excellent performance as _generative_ models, DBMs, unlike VAEs or flows, do not readily lend themselves to _inference_. In particular, because DBMs use a _diffusion_ process to transform the data distribution, they fail to preserve local structure in the data (**Figure 1**), and uncertainty under this mapping is high at its endpoint because of continuous noise injection and resultant mixing. Moreover, because the final distribution--Gaussian white noise of the same dimension--must have _higher_ entropy than the original data, there is no data compression.

Finally, emerging work on _flow matching_ models [36; 37; 38; 39; 40; 41; 42] has achieved impressive generative performance on several benchmark image datasets. Such models utilize simple _conditional_ distribution families to learn a vector field capable of transporting points between two pre-specified densities. These are closely related to the _probability flow ODE (pfODE)_ view of DBMs, and, in fact, have been shown to be equivalent to such models for specific choices of "interpolant" functions and conditional distributions. Despite their exceptional generative performance and deterministic nature, existing flow matching approaches do not allow for compression and, therefore, do not allow practitioners to infer a lower dimensional latent space from data.

Thus, despite tremendous improvements in sample quality, modern generative models do not lend themselves to one of the key modeling goals in scientific applications: calibrated Bayesian inference. Note that while many works focus on _predictive_ calibration, how well the inferred marginal \(p()\) matches real data [43; 44; 45; 46; 47], our focus here is on _posterior calibration_, how well \(q(|)\) matches the true posterior \(p(|)\). We address this challenge by demonstrating how a novel DBM variant that we call _inflationary flows_ can, in fact, produce calibrated Bayesian inference in this sense.

**Specifically, our contributions are: First,** focusing on the case of _unconditional_ generative models, we show how a previously established link between the SDE defining diffusion models and the probability flow ODE (pfODE) that gives rise to the same Fokker-Planck equation  can be used to define a _unique, deterministic_ map between the original data and an asymptotically Gaussian distribution. This map is bidirectional, preserves local neighborhoods, and has controllable numerical error, making it suitable for rigorous uncertainty quantification. **Second,** we define two classes of flows that correspond to novel noise injection schedules in the forward SDE of the diffusion model. The first of these preserves a measure of dimensionality, the participation ratio (PR) , based on second-order data statistics, preventing an effective _increase_ in data dimensionality with added noise, while the second flow _reduces_ PR, providing _data compression_. We demonstrate experimentally that inflationary flows indeed preserve local neighborhood structure, allowing for sampling-based uncertainty estimation, and that these models continue to provide high-quality generation under compression, even from latent spaces reduced to as little as 0.03% of the nominal data dimensionality. As a result, inflationary flows offer excellent generative performance while affording data compression and accurate uncertainty estimation for scientific applications.

## 2 Three views of diffusion-based models

As with standard DBMs, we assume a data distribution \(p_{0}()=p_{}()\) at time \(t=0\), transformed via a forward noising process defined by the stochastic differential equation (e.g., 26; 28):

\[=(,t)t+(,t),\] (1)

with most DBMs assuming linear drift (\(=f(t)\)) and isotropic noise (\(=(t)\)) that monotonically increases over time . As a result, for \(_{0}^{T}(T)t_{data}\), \(p_{T}()\) becomes essentially indistinguishable from an isotropic Gaussian (**Figure 1, left**). DBMs work by learning an approximation to the reverse SDE [34; 28; 29; 30; 50],

\[=\{(,t)-[( ,t)(,t)^{}]-(,t) (,t)^{}_{} p_{t}()\} t+(,t)},\] (2)

where \(}\) is time-reversed Brownian motion. In practice, this requires approximating the score function \(_{} p_{t}()\) by incrementally adding noise according to the schedule \((t)\) of the forward process and then requiring that denoising by (2) match the original sample. The fully trained model then generates samples from the target distribution by starting with \(_{T}(,^{2}(T))\) and integrating (2) in reversed time.

As previously shown, this diffusive process gives rise to a series of marginal distributions \(p_{t}()\) satisfying a Fokker-Planck equation (**Figure 1, middle**) [30; 49],

\[_{t}p_{t}()=-_{i}_{i}[f_{i}(,t)p_{t}( )]+_{ij}_{i}_{j}[_{k}G_{ik} (,t)G_{jk}(,t)p_{t}()],\] (3)

Figure 1: **SDE-ODE Duality of diffusion-based models**. The forward (noising) SDE defining the DBM (**left**) gives rise to a sequence of marginal probability densities whose temporal evolution is described by a Fokker-Planck equation (FPE, **middle**). But this correspondence is not unique: the probability flow ODE (pfODE, **right**) gives rise to the _same_ FPE. That is, while both the SDE and the pfODE possess the same marginals, the former is noisy and mixing while the latter is deterministic and neighborhood-preserving. Both models require knowledge of the score function \(_{} p_{t}()\), which can learned by training either model.

where \(_{i}}\). In the "variance preserving" noise schedule of , (3) has as its stationary solution an isotropic Gaussian distribution. This "distributional" perspective views the forward process as a means of transforming the data into an easy-to-sample form (as with normalizing flows) and the reverse process as a means of data generation.

However, in addition to the SDE and FPE perspectives, Song et al.  also showed that (3) is satisfied by the marginals of a different process with no noise term, the so-called _probability flow ODE_ (pfODE):

\[=\{(,t)-[ (,t)(,t)^{}]-(,t)(,t)^{}_{} p_{t}( )\}t.\] (4)

Unlike (1), this process is deterministic, and data points evolve smoothly (**Figure 1, right**), resulting in a flow that preserves local neighborhoods. Moreover, the pfODE is uniquely defined by \((,t)\), \((,t)\), and the score function. This connection between the marginals satisfying the SDEs of diffusion processes and _deterministic flows_ described by an equivalent ODE has also been recently explored in the context of flow matching models [36; 37; 38; 39; 40; 41; 42], a connection on which we elaborate in **Section 7**.

In the following sections, we show how this pfODE, constructed using a score function estimated by training the corresponding DBM, can be used to map points from \(p_{}()\) to a compressed latent space in a manner that affords accurate uncertainty quantification.

## 3 Inflationary flows

As argued above, the probability flow ODE offers a means of deterministically transforming an arbitrary data distribution into a simpler form via a score function learnable through DBM training. Here, we introduce a specialized class of pfODEs, _inflationary flows_, that follow from an intuitive picture of local dynamics and asymptotically give rise to stationary Gaussian solutions of (3).

We begin by considering a sequence of marginal transformations in which points in the original data distribution are convolved with Gaussians of increasingly larger covariance \((t)\):

\[p_{t}()=p_{0}()(;, (t)).\] (5)

It is straightforward to show (**Appendix A.1**) that this class of time-varying densities satisfies (3) when \(=\) and \(^{}=}\). This can be viewed as a process of deterministically "inflating" each point in the data set, or equivalently as smoothing the underlying data distribution on ever coarser scales, similar to denoising approaches to DBMs [51; 52]. Eventually, if the smoothing kernel grows much larger than \(_{0}\), the covariance in the original data, total covariance \((t)_{0}+(t)(t)\), \(p_{t}()(,(t))\), and all information has been removed from the original distribution. However, because it is numerically inconvenient for the variance of the asymptotic distribution \(p_{}()\) to grow much larger than that of the data, we follow previous work in adding a time-dependent coordinate rescaling \(}(t)=(t)(t)\)[30; 49], which results in an asymptotic solution \(p_{}()=(, ^{})\) of the corresponding Fokker-Planck equation when \(}=}\) and \(}^{}+ }^{}=\) (**Appendix A.2**). Together, these assumptions give rise to the pfODE (**Appendix A.3**):

\[}}{t}=(t)(- }(t)_{} p_{t}() )+(}(t)^{-1}(t))},\] (6)

where the score function is evaluated at \(=^{-1}}\). Notably, (6) is equivalent to the general pfODE form given in  in the case both \((t)\) and \((t)\) are isotropic (**Appendix A.4**), with \((t)\) playing the role of injected noise and \((t)\) the role of the scale schedule. In the following sections, we will show how to choose both of these in ways that either preserve or reduce intrinsic data dimensionality.

### Dimension-preserving flows

In standard DBMs, the final form of the distribution \(p_{T}()\) approximates an isotropic Gaussian distribution, typically with unit variance. As a result, these models _increase_ the effective dimensionality of the data, which may begin as a low-dimensional manifold embedded within \(^{d}\). Thus, even maintaining intrinsic data dimensionality requires both a definition of dimensionality and a choice of flow that preserves this dimension. In this work, we consider a particularly simple measure of dimensionality, the participation ratio (PR), first introduced by Gao et al. :

\[()=()^{2}}{( ^{2})}=_{i}^{2})^{2}}{_{i}_{i}^{ 4}}\] (7)

where \(\) is the covariance of the data with eigenvalues \(\{_{i}^{2}\}\). PR is invariant to linear transforms of the data, depends only on second-order statistics, is 1 when \(\) is rank-1, and is equal to the nominal dimensionality \(d\) when \(_{d d}\). In **Appendix C.1** we report this value for several benchmark image datasets, confirming that in all cases, PR is substantially lower than the nominal data dimensionality.

To construct flows that preserve this measure of dimension, following (5), we write total variance as \((t)=(^{2}(t))=(t)+ _{0}\), where \(_{0}\) is the original data covariance and \((t)\) is our time-dependent smoothing kernel. Moreover, we will choose \((t)\) to be diagonal in the eigenbasis of \(_{0}\) and work in that basis, in which case \((t)=(}(t))\) and we have (**Appendix A.6**):

\[=0(-(})}}{_{k}_{k}^{2}}) }=0.\] (8)

The simplest solution to this constraint is a proportional inflation, \(}{t}(})=}\), along with a rescaling along each principal axis:

\[C_{jj}(t)=_{j}^{2}(t)-_{0j}^{2}=_{0j}^{2}(e^{ t}-1) A _{jj}(t)=}{_{j}(t)}=}{_{0j}}e^{- t/2}.\] (9)

As with other flow models based on physical processes like diffusion  or electrostatics [53; 54], our use of the term _inflationary flows_ for these choices is inspired by cosmology, where a similar process of rapid expansion exponentially suppresses local fluctuations in background radiation density . However, as a result of our coordinate rescaling, the effective covariance \(}=^{}=(A_{0j}^{2})\) remains constant (so \(^{2}}=\) trivially), and the additional conditions of **Appendix A.2** are satisfied, such that \((,})\) is a stationary solution of the relevant rescaled Fokker-Planck equation. As **Figure 2** shows, these choices result in a version of (6) that smoothly maps nonlinear manifolds to Gaussians and can be integrated in reverse to generate samples of the original data.

### Dimension-reducing flows

In the previous section, we saw that isotropic inflation preserves intrinsic data dimensionality as measured by PR. Here, we generalize and consider _anisotropic_ inflation at different rates along each of the eigenvectors of \(\): \(}{t}(})= }\). In addition, we denote \(g_{*}()\), so that the

Figure 2: **Dimension-preserving flows for toy datasets.** Numerical simulations of dimension-preserving flows for five sample toy datasets. Left sequences of sub-panels show results for integrating the pfODE forward in time (inflation); right sub-panels show results of integrating the same system backwards in time (generation) (**Appendix B.3**). Simulations were conducted with score approximations obtained from neural networks trained on each respective toy dataset (**Appendix B.4.1**).

fastest inflation rate is \( g_{*}\). Then, if we take \(g_{i}=g_{*}\) for \(i\{i_{1},i_{2}, i_{K}\}\) and \(g_{i}<g_{*}\) for the other dimensions,

\[((t))=_{0i}^{2}e^{(g_{i}-g_{*}) t })^{2}}{_{i}(_{0i}^{2}e^{(g_{i}-g_{*}) t})^{2}}[t ]{}^{K}_{0i_{k}}^{2})^{2}}{_{j=1}^{K}_{ 0i_{j}}^{4}}\] (10)

which is the dimension that would be achieved by simply truncating the original covariance matrix in a manner set by our choice of \(\). Here, unlike in (9), we do not aim for rescaling to compensate for expansion along each dimension, since that would undo the effect of differential inflation rates. Instead, we choose a single global rescaling factor \((t) A_{0}(- g_{*}t/2)\), leading to a Gaussian asymptotic solution with the original data covariance in dimensions \(i\{i_{1},i_{2}, i_{K}\}\).

Two additional features of this class of flows are worth noting: First, the final scale ratio of preserved to shrunken dimensions for finite integration times \(T\) is governed by the quantity \(e^{(g_{*}-g_{i})T}\) in (10). For good compression, we want this number to be very large, but as we show in **Appendix A.4**, this corresponds to a maximum injected noise of order \(e^{(g_{*}-g_{i})T/2}\) in the equivalent DBM. That is, the compression one can achieve with inflationary flows is constrained by the range of noise levels over which the score function can be accurately estimated, and this is quite limited in typical models. Second, despite the appearance given by (10), the corresponding flow _is not_ simply a linear projection to the top \(K\) principal components: though higher PCs are selectively removed by dimension-reducing flows via exponential shrinkage, individual particles are repelled by _local_ density as captured by the score function (6), and this term couples different dimensions even when \(\) and \(\) are diagonal. Thus, the final positions of particles in the retained dimensions depend on their initial positions in the full space, producing a nonlinear map (**Figure 3**).

## 4 Score function approximation from DBMs

Having chosen inflation and rescaling schedules, the last component needed for the pfODE (6) is the score function \((,t)_{} p_{t}()\). Our strategy will be to exploit the correspondence described above between diffusion models (1) and pfODEs (4) that give rise to the same marginals (3). That is, we will learn an approximation to \((,t)\) by fitting the DBM corresponding to our desired pfODE, since both make use of the same score function.

Briefly, in line with previous work on DBMs , we train neural networks to estimate a de-noised version, \((,(t))\), of a noise-corrupted data sample \(\) given noise level \((t)\) (cf. **Appendix A.4** for the correspondence between \((t)\) and noise). That is, we model \(_{}(,(t))\) using a neural network and train it by minimizing a standard \(L_{2}\) de-noising error:

\[_{}_{( ,(t))}\|(+;(t))- \|_{2}^{2}\] (11)

De-noised outputs can then be used to compute the desired score term using \(_{} p(,(t))=^{-1}(t)( (;(t))-)\)[30; 49]. Moreover, as in , we also adopt a series of preconditioning

Figure 3: **Dimension-reducing flows for toy datasets.** Numerical simulations of dimension-reducing flows for the same five datasets as in **Figure 2**. For 2D datasets, we showcase reduction from two to one dimension, while 3D datasets are reduced to two dimensions. Colors and layouts are the same as in **Figure 2**, with scores again estimated using neural networks trained on each example. Additional results showcasing (1) similar flows further compressing two-dimensional manifolds embedded in \(D=3\) space, and (2) effects of adopting different scaling schemes for target data are given in **Appendices C.2.2** and **C.2.3**, respectively.

factors aimed at making training with the above \(L_{2}\) loss and our noising scheme more amenable to gradient descent techniques (**Appendix B.1**).

## 5 Calibrated uncertainty estimates from inflationary flows

Several previous lines of work  have focused on assessing how well model-predicted marginals \(p()\) match real data (i.e., the _predictive_ calibration case). Though we do compare our models' predictive calibration performance against existing injective flow models (**Table 3**), here we are primarily focused on quantifying error in unconditional posterior inference. That is, we are interested in quantifying the mismatch between inferred posteriors \(q(|)\) and true posteriors \(p(|)\), especially in contexts where the true generative model is unknown and must be learned from data. This is by far the most common scenario in modern generative models like VAEs, flows, and GANs.

As with other implicit models, our inflationary flows provide a deterministic link between complex data and simplified distributions with tractable sampling properties. This mapping requires integrating the pfODE (6) for a given choice of \((t)\) and \((t)\) and an estimate of the score function of the original data. As a result, sampling-based estimates of uncertainty are trivial to compute: given a prior \(()\) over the data (e.g., a Gaussian ball centered on a particular example \(_{0}\)), this can be transformed into an uncertainty on the dimension-reduced space by sampling \(\{_{j}\}()\) and integrating (6) forward to generate samples from \(\!p(_{T}|_{0})(_{0})\,_{0}\). As with MCMC, these samples can be used to construct either estimates of the posterior or credible intervals. Moreover, because the pfODE is unique given \(\), \(\), and the score, the model is _identifiable_ when conditioned on these choices.

The only potential source of error, apart from Monte Carlo error, in the above procedure arises from the fact that the score function used in (6) is only an _estimate_ of the true score. To assess whether integrating noisy estimates of the score could produce errant posterior samples, we conducted the experiment showcased in **Figure 4A** (**Appendix B.6**). Briefly, we constructed a Gaussian Mixture Model (GMM) prior with three pre-specified components (**Appendix B.6**) from which we drew samples of \(\), integrating backwards in time using our trained pfODE networks to construct corresponding observed data points \(\). We then utilized Hamiltonian Monte Carlo (HMC) 1, 56-58 to obtain posterior samples for the GMM component weights. As shown in **Figure 4B, C**, the resulting posterior correctly covers the original ground-truth values, suggesting that numerical errors in score estimates, at least in this simplified scenario, do not appreciably accumulate. This is likely because, empirically, score estimates do not appear to be strongly auto-correlated in time (**Appendix C.3**),

Figure 4: **Calibration experiments. To assess error in our posterior model estimates, we used Hamiltonian Monte Carlo (HMC) to perform inference in one of our toy datasets (2D circles). Drawing samples from a 3-component Gaussian Mixture Model (GMM) prior, we integrated the generative process backward in time to obtain corresponding data space samples (\(\), components shown in orange, blue, and purple). We then used HMC to obtain posterior samples from the posterior distribution over the weights of the GMM components. (\(\), \(\)) Kernel density estimates from the joint posterior samples over the mixture distribution weights in the dimension-preserving and dimension-reducing cases. Dashed vertical and horizontal lines indicate posterior means for each component. Reference ground-truth weights were \(=[0.5,0.25,0.25]\).**

[MISSING_PAGE_FAIL:8]

## 7 Discussion

Here, we have proposed a new type of implicit probabilistic model based on the probability flow ODE (pfODE) in which it is possible to perform calibrated, identifiable Bayesian inference on a reduced-dimension latent space via sampling and integration. To do so, we have leveraged a correspondence between pfODEs and diffusion-based models by means of their associated Fokker-Planck equations, and we have demonstrated that such models continue to produce high-quality generated samples even when latent spaces are as little as 0.03% of the nominal data dimension. More importantly, the uniqueness and controllable error of the generative process make these models an attractive approach in cases where accurate uncertainty estimates are required.

**Limitations:** One limitation of our model is its reliance on the participation ratio (7) as a measure of dimensionality. Because PR relies only on second-order statistics and our proposals (9) are formulated in the data eigenbasis, our method tends to favor the top principal components of the data when reducing dimension. However, as noted above, this is not simply a truncation to the lowest principal components, since dimensions still mix via coupling to the score function in (6). Nonetheless, solutions to the condition (8) that preserve (or reduce) more complex dimensionality measures might lead to even stronger compressions for curved manifolds (**Appendix C.2.2**), and more sophisticated choices for noise and rescaling schedules in (6) might lead to compressions that do not simply remove information along fixed axes, more similar to . That is, we believe much more interesting classes of flows are possible. A second limitation is that mentioned in **Section 3.2** and in our experiments: our schedule requires training DBMs over much larger ranges of noise than are typically used, and this results in noticeable tradeoffs in compression performance as the inflation gap and number of preserved dimensions are varied.

  
**Dimensions** & **IG** & **FID** & **MSE** \\  \(2\) & 1.02 & \(11.95 0.06\) & \(1.55 0.21\) \\ \(2\) & 1.10 & \(13.98 0.13\) & \(1.35 0.08\) \\ \(2\) & 1.25 & \(17.84 0.15\) & \(1.65 0.09\) \\ \(2\) & 1.35 & \(34.68 0.37\) & \(1.19 0.18\) \\ \(2\) & 1.50 & \(107.64 0.43\) & \(0.11 0.02\) \\   

Table 2: FID and round-trip MSE (mean \( 2\)) for AFHQv2 at varying Inflation Gaps (IG)

Figure 5: **Generation and round-trip experiments for AFHQv2 at IG=1.02 and varying number of preserved dimensions. Top row: Generated samples for select flow schedules (PR-Preserving (PRP), PR-Reducing to 2D (\( 0.07\%\)), 30D(\( 1\%\)), and 307D(\( 10\%\)), at 1.02 IG. Bottom row: Results for round-trip experiments under same schedules. Leftmost columns are original samples, middle columns are samples mapped to Gaussian latent spaces, and rightmost columns are recovered samples.**

**Related work:** This work draws on several related lines of research, including work on using DBMs as likelihood estimation machines [50; 67; 31], relations with normalizing flows and hierarchical VAEs [67; 33; 68], _injective flow_ models [21; 22; 23; 24; 25], and generative flow networks . By contrast, our focus is on the use of DBMs to learn score functions estimates for implicit probabilistic models, with the ultimate goal of performing accurate posterior inference. In this way, it is also closely related to work on denoising models [51; 52; 66; 70] that cast that process in terms of statistical inference and to models that use DBMs for de-blurring and in-painting [71; 72]. However, this work is distinct from several models that use reversal of deterministic transforms to train generative models [73; 74; 75; 76]. Whereas those models work by removing information from each sample \(\), our proposal relies critically on adjusting the local density of samples with respect to one another, moving the marginal distribution toward a Gaussian.

Our work is also similar to methods that use DBMs to construct samplers for unnormalized distributions [77; 78; 79; 80; 81]. Whereas we begin with samples from the target distribution and aim to learn latent representations, those studies start with a pre-specified form for the target distribution and aim to generate samples. Other groups have also leveraged sequential Monte Carlo (SMC) techniques to construct new types of denoising diffusion samplers for, e.g., conditional generation [82; 83; 84]. While our goals are distinct, we believe that the highly simplified Gaussian distribution of our latent spaces may potentially render joint and conditional generation more tractable in future models. Finally, while many prior studies have considered compressed representations for diffusion models [85; 86; 87; 88], typically in an encoder-decoder framework, the focus there has been on generative quality, not inference. Along these lines, the most closely related to our work here is , which considered diffusion along linear subspaces as a means of improving sample quality in DBMs, though there again, the focus was on improving generation and computational efficiency, not statistical inference.

Yet another line of work closely related to ours is the emerging literature on _flow matching_[36; 37; 38; 90] models, which utilize a simple, time-differentiable, "interpolant" function to specify _conditional_ families of distributions that continuously map between specified initial and final densities. That is, the interpolant functions define flows that map samples from a base distribution \(_{0}()\) to samples from a target distribution \(_{1}()\). Typically, these approaches rely on a simple quadratic objective that attempts to match the _conditional_ flow field, which can be computed in closed form without needing to integrate the corresponding ODE. As shown in **Appendix A.5**, the pfODEs obtained using our proposed scaling and noising schedules are _equivalent_ to the ODEs obtained by using the "Gaussian paths formulation" from  when the latter are generalized to full covariance matrices. As a result, our models are amenable to training using flow-matching techniques, suggesting that faster training and inference schemes may be possible through leveraging connections between flow matching and optimal transport [40; 41; 38; 42]

**Broader impacts:** Works like this one that focus on improving generative models risk contributing to an increasingly dangerous set of tools capable of creating misleading, exploitative, or plagiarized content. While this work does not seek to improve the quality of data generation, it does propose a set of models that feature more informative latent representations of data, which could potentially be leveraged to those ends. However, this latent data organization may also help to mitigate certain types of content generation by selectively removing, prohibiting, or flagging regions of the compressed space corresponding to harmful or dangerous content. We believe this is a promising line of research that, if developed further, might help address privacy and security concerns raised by generative models.

  
**Dimensions Preserved** & **IFs (IG=1.02)** & **M-Flow** & **RFs** & **CMFs** \\  \(30\) & **23.3** & 541.2 & 544.0 & 532.6 \\ \(40\) & **24.3** & 535.7 & 481.3 & 444.6 \\ \(62\) & **23.2** & 280.9 & 280.8 & 287.9 \\   

Table 3: FID score comparison with injective flows for CIFAR-10