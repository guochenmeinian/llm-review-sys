# ContinuAR: Continuous Autoregression For

Infinite-Fidelity Fusion

 Wei W. Xing

School of Mathematics and Statistics, University of Sheffield

Hicks Building, Hounsfield Rd, Sheffield, UK, S3 7RH

w.xing@sheffield.ac.uk

&Yuxin Wang

Department of Statistics and Data Science

National University of Singapore

21 Lower Kent Ridge Road, Singapore, 119077.

yuxinwang@u.nus.edu &Zheng Xing

Graphics& Computing Department

Rockchip Electronics Co., Ltd

Fuzhou, China, 350003.

zheng.xing@rock-chips.com

Corresponding author.

###### Abstract

Multi-fidelity fusion has become an important surrogate technique, which provides insights into expensive computer simulations and effectively improves decision-making, e.g., optimization, with less computational cost. Multi-fidelity fusion is much more computationally efficient compared to traditional single-fidelity surrogates. Despite the fast advancement of multi-fidelity fusion techniques, they lack a systematic framework to make use of the fidelity indicator, deal with high-dimensional and arbitrary data structure, and scale well to infinite-fidelity problems. In this work, we first generalize the popular autoregression (AR) to derive a novel linear fidelity differential equation (FiDE), paving the way to tractable infinite-fidelity fusion. We generalize FiDE to a high-dimensional system, which also provides a unifying framework to seemly bridge the gap between many multi- and single-fidelity GP-based models. We then propose ContinuAR, a rank-1 approximation solution to FiDEs, which is tractable to train, compatible with arbitrary multi-fidelity data structure, linearly scalable to the output dimension, and most importantly, delivers consistent SOTA performance with a significant margin over the baseline methods. Compared to the SOTA infinite-fidelity fusion, IFC, ContinuAR achieves up to 4x improvement in accuracy and 62,500x speedup in training time.

## 1 Introduction

Contemporary scientific and engineering endeavors depend significantly on analyzing highly complex systems, where the repeated execution of large-scale differential equation numerical simulations, often with intricate interconnections, is essential. For instance, in the design of a system-on-chip (SoC), more than 80% of the design time is spent on analysis based on different types of simulations, e.g., timing analysis and yield optimization. Such an intense computational demand prompts the use of a data-driven surrogate model, which essentially acts as a functional approximation for the input-output mapping of a simulation. To enhance convergence efficiency, especially in scenarios involving recurrent simulations, such as those encountered in Bayesian optimization (BO)  and uncertainty quantification (UQ) .

Traditional surrogates are trained on many high-fidelity simulation results, which are still computationally expensive to generate. To make high-fidelity predictions  while further reducing the computational burden, it is a possible way to combine low-fidelity results. More specifically, we can run low-fidelity simulations based on simplified equations (e.g., reducing the levels of physical detail) or coarse solver setups (e.g., using a coarse mesh, a large time step, a lower order of approximating basis, and a higher error tolerance) to generate cheap but inaccurate results, which offer a chance to train a rough model with low cost. The multi-fidelity fusion techniques then improve such a rough model by using only a few high-fidelity simulation samples. In total, the computational cost is dramatically reduced. Multi-fidelity fusion, owing to its efficiency, has garnered growing interest in BO [4; 5], UQ , and surrogate modeling .

While many state-of-the-art (SOTA) fusion approaches have been rapidly developing, most focus on improving the model accuracy or scalability for large-scale problems. They normally assume a small number of fidelities (say, less than five), and a particular fidelity data structure (i.e., the output space is well aligned and the high-fidelity samples' corresponding inputs must form a subset of the low-fidelity inputs). However, in practice, a natural multi-fidelity problem can be much more complicated. For instance, the data does not admit any particular structure. Furthermore, the number of fidelities is in general countably infinite, very few works actually utilize the fact that the fidelity levels are implicitly quantified by a continuous variable, e.g., the number of nodes in a mesh and there are infinite number of fidelities. These limitations seriously hinder the applications of multi-fidelity-based methods, e.g., multi-fidelity BO and multi-fidelity Monte Carlo.

Recently, Li et al.  propose the first infinite-fidelity fusion, IFC, which utilizes NeuralODE  to resolve these challenges. Despite its success, IFC is difficult to train and scale poorly to high-dimensional problems. To make a further step towards practicality while preserving tractability and accuracy, we propose the first tractable infinite-fidelity fusion framework, linear fidelity differential equations (FiDEs), and its rank-1 solution, ContinuAR, to deliver a powerful yet tractable fusion model. The novelty of this work is as follows,

1. We propose the first linear fidelity differential equation (FiDE) and its general solution, paving the way to tractable infinite-fidelity fusion.
2. We extend FiDE to FiDEs, which handle the common high-dimensional simulation problems. Furthermore, FiDEs bridge the gap between multi-fidelity and single-fidelity surrogates and serve as a unifying framework for many existing GP-based surrogates, revealing some future directions for classic surrogate models (a.k.a emulators) with multi-fidelity fusion.
3. We propose ContinAR, a tractable and efficient rank-1 solution to FiDEs. It is scalable to infinite fidelity, capable of handling high-dimension problems, compatible with arbitrary multi-fidelity data structure, and delivering SOTA accuracy with low computational cost.

## 2 Background

### Statement of the problem

Provided with multi-fidelity dataset \(^{i}\), where \(i=0,,T\), each set comprising entries \(\{t_{i},_{n}^{(i)},_{n}^{(i)}\}_{n=1}^{N^{i}}\), with \(t_{i}\) representing the fidelity indicator (e.g., the number of nodes in a mesh generation), where a larger \(t\) indicates a more accurate solution; \(^{(i)}^{Q}\) denotes the system inputs (e.g., a vector containing parameters found in the system of equations or initial-boundary conditions for a simulation), where \(Q\) is the dimensionality for \(\); \(^{(i)}^{D}\) indicates the vectorized outputs associated with \(\), where \(D\) is the dimensionality for \(\); \(T\) is the total number of fidelities. In general, higher fidelity simulations, being closer to the ground truth and more costly to acquire, result in a limited number of available samples, i.e., \(N^{0}>N^{1}>>N^{T}\). In most works, e.g., [10; 11; 12], the system inputs of higher-fidelity are chosen to be the subset of the lower-fidelity, i.e., \(^{T}^{2}^{1}\). We call this the subset structure for a multi-fidelity dataset, as opposed to arbitrary data structures, which we will resolve in Section 4. Our objective is to estimate the function \(^{(T)}()\) based on observations at various fidelities \(\{^{i}\}_{i=1}^{T}\).

### Autoregression (AR)

The classic AR model  only considers the scalar problem (i.e., \(y^{(i)}\)) and imposes a Markov property for its multi-fidelity formulation. Considering that \(t_{0}\) and \(t_{T}\) are indicators (e.g., time steps) for the low- and high-fidelity data, respectively, AR defines

\[y^{(T)}()= y^{(0)}()+u(),\] (1)where \(\) is a factor transferring knowledge from the low-fidelity in a linear fashion, while \(u()\) aims to encapsulate the remaining information. Assuming a zero mean Gaussian process (GP) prior  for \(y^{(0)}()\) and \(u()\), denoted as \(y^{(0)}()(0,k^{0}(,^{}))\) and \(u()(0,k^{u}(,^{}))\), the high-fidelity function similarly adheres to a GP. This results in an elegant joint GP applicable to the combined observations \(=[^{(0)};^{(T)}]\),

\[(^{(0)}\\ ^{(T)})( ,&_{0}^{(0)}&\:_{0}^{(0T)}\\ _{0}^{(T)}&^{2}_{0}^{(T)}+_{u}^{(T)} )\] (2)

where \([_{0}^{(0)}]_{ij}=k^{0}(_{i}^{(0)},_{j}^{(0)})\); \([_{0}^{(0T)}]_{ij}=k^{0}(_{i}^{(0)},_{j}^{(T)})\); \([_{0}^{(T)0}]_{ij}=k^{0}(_{i}^{(T)},_{j}^{(0)})\); \([_{0}^{(T)}]_{ij}=k^{0}(_{i}^{(T)},_{j}^{(T)})\); \([_{0}^{(T)}]_{ij}=k^{u}(_{i}^{(T)},_{j}^{(T)})\). We can see that the joint likelihood is still Gaussian, which admits a tractable solution for model training as in a standard GP. Furthermore, the predictive posterior is also a standard GP posterior, which can effectively utilize low- and high-fidelity data. Moreover, by decomposing the likelihood and predictive posterior into two separate components, Le Gratiet  managed to reduce the complexity from \(O((N^{0}+N^{T})^{3})\) to \(O((N^{0})^{3}+(N^{T})^{3})\) using a subset data structure, where \(^{T}^{0}\).

### Fidelity Differential Equation

In scientific numerical simulations, the fidelity can be determined by quite different factors, e.g., using simplified/complete equations to describe the target system or implementing a dense/sparse mesh to discretize the domain. For problems where the fidelity factor cannot be easily quantified, the classic multi-fidelity methods, e.g., AR, and other methods should work just fine. However, it is more frequent to find that the fidelity indicators have valuable information in them. For instance, the fidelity is often indicated by the number of elements in a finite element simulation or time steps in a forward/backward Euler timing scheme. In these cases, we should treat the fidelity indicators as continuous variables and utilize the information they carry to boost the predictive accuracy. To this end, Li et al.  propose a general formulation of infinite-fidelity models,

\[y(,t)/t=(,t,y(,t)).\] (3)

Although this formulation is general enough to cover all multi-fidelity models, it lacks an insightful interpretation--it is not clear how to design the function \((,t,y(,t))\). A clever workaround is to use a neural network to approximate \((,t,y(,t))\), which introduces many challenges such as 1) requiring a large amount of training data, 2) expensive computational cost for backpropagation even with adjoint method, and 3) instability as we can see how easy a simple nonlinear system can lead to chaotic behaviors (e.g., the Lorenz system ).

## 3 Proposed Method

### Linear Fidelity Differential Equation

To remedy the over-general formulation of Eq. (3), we first revisit the classic AR and reveal its connection to a linear ODE's forward Euler solution. Rewrite Eq. (1) as

\[,t_{T})-y(,t_{0})}{t_{T}-t_{0}}=_{0}y( ,t_{0})+u(,t_{0}),\] (4)

and take the limit \((t_{T}-t_{0}) 0\), we derive Proposition 1 (see Appendix B for detailed derivations).

**Proposition 1**.: _The linear fidelity differential equation (FiDE):_

\[(,t)=(t)y(,t)+u(,t).\] (5)

If we take a forward Euler solution to solve Eq. (5) discretized at each fidelity, we recover the classic AR in Eq. (1). Let us stick to the continuous formulation and utilize calculus techniques , we can derive the general solution to Eq. (5),

\[y(,t)=e^{-B(t)}(^{t}e^{B()}u(,) +C()),\] (6)

where \(C()\) is a function of \(\), and \(B(t)=^{t}()\) is the antiderivative of \((t)\). In order to design \((t)\) such that the solution converges to the ground truth as \(t\), a detailed stability analysis is necessitated. General stability considerations for systems of this kind are often complex and may warrant the application of Lyapunov's direct method. For the sake of maintaining a tractable model, we make the simplifying assumption that \((t)=\) is a constant value. Provided that \(>0\), our system is guaranteed to converge to what we define as the "ultra-high-fidelity ground truth" as \(t\). Setting \(t_{0}\) as the lowest-fidelity indicator, we derive a general solution (see Appendix C)

\[y(,t)=y(,t_{0})e^{-(t-t_{0})}+_{t_{0}}^{t}e^{- (t-)}u(,).\] (7)

To design a subtle model with enough flexibility while trying to maintain tractability, we place zero mean GP priors for \((,t_{0})\) and \(u(,t)\) (with all data being normalized), i.e.,

\[(,t_{0})(0,k^{0}(,^{})),  u(,t)(0,k^{u}(,t,^{ },t^{})).\] (8)

Similarly to the latent force model , the resulting general solution \(y(,t)\) is a also GP with zero mean and covariance with the analytical form:

\[[y(,t),y(^{},t^{})]=k^{0}( ,^{})e^{-(t-t_{0})}+_{t_{0}}^{t}e^{-( t-)}_{t_{0}}^{t^{}}e^{-(t^{}-^{})}k^{u}( ,,^{},^{})^{} .\] (9)

Alvarez et al.  demonstrate that a simple ODE system, devoid of \(\), yields a nonstationary output covariance with a closed-form solution when employing a squared-exponential (SE) kernel for \(u(t)\). This result, however, is not directly applicable to the FiDE due to the inherent \(\) dependency.

To improve model flexibility, we enable the usage of arbitrary kernels, e.g., deep kernel , by implementing a Monte-Carlo integration and reparameterization trick  to conduct efficient backpropagation for model training--the integral approximated by \(_{i,j}^{M}e^{-(t_{i}+t_{j}-2t_{0})}k^{u}(,_ {i},^{},_{j})\), where \(t_{i}\) and \(t_{j}\) are \(M\) are random samples from \([t_{0},t]\) and \([t_{0},t^{}]\), respectively.

### FiDEs: Multi-Variate Extension For High-dimensional Problems

In practice, the outputs of a simulation are generally high-dimensional , i.e., \(^{(t)}^{D}\). We hereby generalize Eq. (5) for a more general formulation that describes a multi-variate system:

**Proposition 2**.: _The linear fidelity differential equations (FiDEs) for a multi-variate system:_

\[}(,t)+(t)(,t)=(,t),\] (10)

_where \((t)^{D D}\) and \(^{D R}\) are affine transformations; \((,t)^{R}\) is a source function._

We can derive a general solution for Eq. (10) with some calculus as model design guideline (see Appendix D). For instance, for a constant \(\), all eigenvalues of \(\) must have positive real parts to keep the system stable. To derive an efficient model, we define a constant diagonal matrix \(\) and derive a tractable general solution

\[y_{d}(,t)=C_{d}()e^{-B_{d}t}+_{=1}^{R}S_{d,} _{d}(u_{r}(,t)),\] (11)

\[_{d}(u_{r}(,t))=_{t_{0}}^{t}\!e ^{-B_{d}(t-)}u_{r}(,),\] (12)

with \(B_{d}\) being the \(d\)-diagonal element of \(\). Due to the linearity of the integral in \(_{d}(u_{r}(,t))\), we can derive the output correlation with an analytical form:

\[[y_{d}(,t),y_{d^{}}(^{},t^{} )]=e^{-(t-t_{0})}k^{0}(,^{})H_{d,d^{}}+ _{=1}^{R}S_{d,}S_{d^{},}[_{d }(u_{r}(,t)),_{d^{}}(u_{r}(^{},t^{}))],\] (13)

where

\[[_{d}(u_{r}(,t)),_ {d^{}}(u_{r}(^{},t^{}))]=_{t _{0}}^{t}e^{-B_{d}(t-)}_{t_{0}}^{t^{}}e^{-B_{d^{}}(t^{ }-^{})}k^{u_{r}}(,,^{},^{ })^{}.\] (14)

We recognize that this model is a generalization of semiparametric latent factor model (SLFM) , which is a special case by setting \(t=t_{0}\) or \(B_{d}=0\) to consider a single-fidelity problem.

### ContinuAR: A Rank-1 Solution to FiDEs

A special yet practical case of SLFM is the intrinsic model of coregionalization (IMC, also a rank-1 approximation to LMC ), where all \(u_{r}(,t)\) share the same kernel function, i.e.,

\[(,t)(,^{u}(,t,^{},t^{})).\] (15)

Here \(\) means the Kronecker product. Similar IMC assumptions are made in ResGP, a popular modification of AR for high-dimensional output fusion , which shows promising results in many physics applications. Based on their conclusions and our intention to keep our model simple and efficient, we define \(=\) following our solution to FiDE and place an IMC model for the lowest-fidelity model, i.e., \((,t_{0})(,k^{0}(,^{}))\), where \(\) is the output correlations. Substituting the new \((,t)\) and \(\) back into Eq. (13), we get the output correlation \(k^{y}(,t,^{},t^{})=[ (,t),(^{},t^{})]=\)

\[e^{-(t-t_{0})}k^{0}(,^{})+ _{t_{0}}^{t}e^{-(t-)}_{t_{0}}^{t^{}}e^{-(t^{} -^{})}k^{u}(,,^{},^{}) ^{}^{},\] (16)

which uniquely defines a rank-1 solution to FiDEs. We call it ContinuAR.

**Lemma 1**.: _Autokregability in ContinAR: the particular values of the spatial correlation matrix \(\) and \(^{}\) do not matter in the predictive mean as they will be canceled out._

The proof is given in Appendix E for clarity by basically deriving the predictive mean of ContinAR. Since the predictive mean is the main concern in high-dimensional problems [8; 12], we simply set \(=^{}=\) to significantly improve model efficiency without introducing any additional error in the mean predictions. The computational complexity w.r.t the output dimension is reduced from \((D^{3})\) to \((D)\) and the memory consumption is reduced from \((D^{2})\) to \((1)\).

## 4 Efficient Training and Inference Through Subset Decomposition

We have implicitly marginalized out the underlying function \((,t)\) based on its GP prior and derived the output covariance of Eq. (16). Given a set of observations \(=[^{(0)};^{(1)};;^{(T)}]\), we have the joint distribution

\[(}^{(0)}\\ \\ }^{(T)})(^{(00)}&&^{(0T)}\\ ,&&&\\ ^{(T0)}&&^{(TT)}),\] (17)

where \(}^{(0)}=(^{(0)})\) is the vectorization; \([^{(kl)}]_{ij}=k^{y}(_{i},t_{k},_{j},t_{l})\) is the shorthand notation of output correlation Eq. (16). For a small number of total training data of all fidelity, we can simply opt for a maximum likelihood estimation (MLE) for the joint likelihood

\[=-}^{}^{-1}}-||-(2),\] (18)

where \(\) is the whole covariance matrix in Eq. (17) and \(}=[^{(0)},,^{(T)}]^{}\). However, this approach will soon become invalid, particularly in a multi-fidelity scenario where we expect many low-fidelity simulations.

Since the integration of Eq. (16) can be done by parts, \(^{(kl)}\) admits an additive structure exactly as in AR. We can follow Gratiet and Cannamela  to decompose the joint likelihood Eq. (18) into independent components provided that corresponding inputs strictly follow a subset structure, i.e., \(^{T}^{2}^{1}\). For problems with only a small number of fidelity (a small \(T\)), the subset structure may not be too difficult to satisfy. However, for the infinite (countable) fidelity setting, such a requirement is not practical. Here, we derive a decomposition by introducing virtual observations \(}\) for each fidelity such that \(^{(T)}\) satisfies the subset requirement for the completed set \(\{^{(T-1)},}^{(T-1)}\}\). \(}^{(T)}\) is the part of \(^{(T)}\) that forms the subset of \(^{(T-1)}\) (with a selection formulation \(}^{(T)}=^{(T)}^{(T-1)}\), where \(^{(T-1)}\) corresponds to the previous-fidelity outputs \(^{(T-1)}\)). The joint likelihood can then be decomposed,

\[=  p(^{(0:T)})= p(^{(0:T-1)}, ^{(T)})\] \[=  p(^{(0:T-1)})+[p(^{(T)}| ^{(T-1)},}^{(T-1)})\ p(}^{(T-1)}| ^{(T-1)})]d}^{(T-1)}\] \[=  p(^{(0:T-1)})-}{2}(2)-|}_{a}^{(TT)}|-[(_{a}^{(T)})^{}(}_{a}^{(TT)})^{-1} _{a}^{(T)}],\] (19)

where

\[}_{a}^{(TT)}=_{a}^{(TT)}+}^{(T)} }^{(T)}(}^{(T)})^{},\] (20)

is the updated additive kernel with the uncertainty of the predictive variance \(}^{(T)}\) of the virtual points, whose computation details are given later in Section 4.1;

\[[_{a}^{(TT)}]_{ij}=[^{(TT)}]_{ij}-[^{(T-1,T-1)}] _{ij}\] (21)is the additive/residual part of the kernel from \((T-1)\) to \(T\);

\[_{a}^{(T)}=(}^{(T)}\\ }^{(T)})-e^{-_{T}}( }^{(T-1)}\\ }^{(T-1)})\] (22)

is the additive part for the completed outputs from \((T-1)\) to \(T\); \(_{T}\) is the time interval between \(t_{T-1}\) and \(t_{T}\); \( p(^{(0:T-1)})\) is the log likelihood for the previous \(T\) fidelities, which is obtained by calling Eq. (19) recursively. The detailed derivation is preserved in Appendix I due to the space limitation. Through the decomposition of Eq. (19), the computation complexity is reduced from \(((D_{i=0}^{T}N^{t_{i}})^{3})\) to \((D_{i=1}^{T}(N^{t_{i}}+N^{t_{i-1}}-|^{(t_{i})} ^{(t_{i-1})}|_{n})^{3})\). Here, \(||_{n}\) indicates the number of samples. Furthermore, when data at a new fidelity is obtained, we only need to modify the joint likelihood slightly by adding new blocks, which will be handy for active learning or Bayesian optimization. Model training is conducted easily by maximizing the joint likelihood Eq. (19) with respect to the hyperparameters using gradient-based optimizations.

### Predictive Posterior

Since the joint model (17) is a Gaussian, the predictive posterior for the highest fidelity can be derived as in standard GP with a large covariance matrix that requires inversion for once. Similar to the derivation of an efficient joint likelihood in the previous section, we drive an efficient predictive posterior \((_{*},T)(}(_{*},T),(_{*},T))\) (see Appendix H),

\[}(_{*},T)& =e^{-_{T}}}(_{*},T-1) +(_{a*}^{(TT)})^{}(_{a}^{(TT)})^{-1} _{a}^{(T)}\\ (_{*},T)&=e^{-2_ {T}}(_{*},T-1)+}^{(T)}+ ^{(T)}}^{(T)}(^{(T)})^{ }\] (23)

where

\[}^{(t_{T})}&= (}_{a*}^{(TT)})^{}(}_{a}^{(TT)})^{-1}}_{a*}^{(TT)},}^{(t_{T})}=(}_{a*}^{(TT)} )^{}(}_{a}^{(TT)})^{-1}} _{a*}^{(TT)},\\ ^{(t_{T})}&=[_{a*}^{(T,T)}(_{n}^{(T)})^{}}_{a*}^{(T-1,T-1 )}(}_{a}^{(T-1,T-1)})^{-1}]_{m}^{( T)},\] (24)

with two selection matrixes that follow:

\[}^{(t_{T})}=(_{m}^{(T)})^{}[^{(T-1)},}^{(T)}],^{(t_{T})}=( _{n}^{(T)})^{}[^{(T-1)},}^{(T)}].\] (25)

In these equations, \([_{a*}^{(TT)}]_{j}=k^{y}(_{*},T,_{j},T)-k^{y}( _{*},T-1,_{j},T-1)\) for \(_{j}\{^{(t_{T})}\}\) is the kernel additional part between \(T\) and \(T-1\) for \(_{*}\) and \(^{(t_{T})}\); \(}^{(t_{T})}\) is the predictive variance based on \(\{^{(T-1)},}^{(T)}\}\) of \(T\) fidelity model; \([_{a*}^{(TT)}]_{j}=k^{y}(_{*},T,_{j},T)-k^{y}( _{*},T-1,_{j},T-1)\) for \(_{j}\{}^{(t_{T})}\}\) is the kernel additional part between \(T\) and \(T-1\) for \(_{*}\) and \(}^{(t_{T})}\); \(}^{(t_{T})}\) is the predictive variance based on \(\{}^{(T)}\}\).

## 5 Related Work

As stated, FiDEs serve as a unifying framework that unifies many SOTA multi-fidelity and single-fidelity surrogates. We made connections to some popular models in Table 1.

**Multi-Variate GPs** are commonly used surrogates that can be recovered by setting \(t=Const\) or \(T=0\) for the FiDEs, which gives us the fundamental semiparametric latent factor models (SLFM) . SLFM is a simplified Linear model of coregionalization (LMC) [23; 29]. Based on SLFM, intrinsic model of coregionalization (IMC) reduces the computational complexity by setting a rank-1 approximation \(()(,^{u}(, ^{}))\); HOGP  improves IMC by letting \([^{}]_{ij}=k^{d}(_{i},_{j})\) with tensor decomposition, where \(\) are latent vectors. Higdon et al.  further simplify SLFM using singular value decomposition (SVD) to obtain \(\). For a full review of the GP-based multi-variate surrogates, the readers are referred to  for an excellent review. To overcome the fixed bases of SLFM, GP regression network (GPRN [30; 25; 31]) introduces another GP to model a flexible \(\).

   Model & Assumptions under FIDEs \\  SLFM  & \(T=0\); \\ IMC  & \(T=0\); \(()(,^{u}(, ^{}))\) \\ HOGP  & \(T=0\); \(\); \(\); \(|^{}|_{j}=k^{d}(_{i},_{j})\) \\ GPRN  & \(T=0\); \( GP()\) \\  AR  & \(D=1\); \((t)=(t)\). \\ ResGP  & \(T=1\); \((t)=0\); \(\), \(\) \\  NAR  & \((t)=0\); \((}(,t)) GP(, ,t_{i}),=\) \\ MF-BNN  & \((t)=0\), \((}(,t)) BN(, ,t_{i}),=\) \\ SC  & \((t)=0\), \((}(,t)) PCE(,t_{i}), =\) \\ DC  & \((t)=0\); \((}(,t)) GP(, ,t_{i})\) \\  \(=ResPCA()\) or \(_{i}=PCA(^{(i)})\) \\  IFC  & \((t)=0\), \((,t), NN(,,t),= \) \\ ContinuaN & \((t)=,=\) \\  

Table 1: FiDEs unifying GP surrogates

**Tractable Fusion** is the name we give to multi-fidelity models where the joint outputs form a GP. AR  is the most fundamental and tractable solution to FiDEs for a univariate problem, i.e., \(D=1\), which allows it to take a (discrete) t-dependent \((t)=(t_{i})\) without leading to an intractable solution. To deal with high-dimensional problems, ResGP  avoids the difficulty involved with \((t)\) by setting it to zero and uses a conditional independent GP for \(u(,t_{i})\) (equivalent to \(R=1,S=I\)).

**Intractable Fusion** refers to methods where the joint output is no longer a GP. Normally, they assume \((t)=0\) to avoid the integral and replace the antiderivative \(((,t))\) in Eq. (13) with a regression model that takes \(\) (at the previous fidelity) as model input. For instance, the popular non-linear AR (NAR)  uses \(GP(,,t_{i})\) to replace \(((,t))\); Deep coregionalization (DC ) extends NAR with a residual PCA to capture \(\); Wang et al.  further introduce a fidelity variating \((t_{i})\) to increase model flexibility at the cost of significant growth in the number of model parameters and a few simplifications in the approximated inference; Li et al.  take the advances of recent advancement of deep learning neural network (NN) and place a Bayesian neural network (BNN) to replace \(((,t))\); A similar idea is proposed by Meng and Karniadakis  who add a physics regularization layer to a deep NN; Li et al.  propose a Bayesian network approach to multi-fidelity fusion with active learning techniques for efficiency improvement. To account for the missing uncertainty propagation in intractable fusion methods, Cutajar et al.  use approximation inference at the price of overfitting and scalability to high-dimensional problems; In the UQ community, multi-fidelity fusion has been implemented using stochastic collocation (SC) , which essentially uses a polynomial chaos expansion (PCE) to approximate \(((,t))\) under FiDEs. All the above methods do not generalize to infinite-fidelity problems. The seminal work IFC  uses a NeuralODE to solve general FiDEs, leading to a great challenge in model training.

**Algorithm.** The solutions of FiDEs are closely related to the latent force models (LFMs) [16; 35], where they focus on dynamic processes without \(\). For tractable fusion, Le Gratiet  improves the efficiency of AR by decomposing the likelihood functions based on a subset multi-fidelity data structure, which is relaxed through our non-subset decomposition in Section 4.

## 6 Experiment

To assess ContinuAR, we compare it with (1) AR , (2) ResGP , (3) MF-BNN2, and (4) IFC3, which are the most closely related SOTA methods for high-dimensional multi-fidelity fusion, particularly with infinite fidelities. Note that AR and ResGP are modification versions according to  instead of their original versions that cannot deal with non-subset or high-dimensional problems. ContinuAR, AR, and ResGP are implemented using Pytorch. All GP-based methods use the RBF kernel for a fair comparison. MF-BNN and IFC are conducted using default settings from their source codes. As presented in IFC's original paper, there are two variations of IFC, one with deep learning (IFC-ODE) and the other one with Gaussian process ODE (IFC-GPODE), which shows better results. Thus, we show the results of IFC-GPODE in our experiments. All GP-based models are trained with 200 iterations whereas MF-BNN and IFC with 1000 iterations to reach convergence. All experiments are run on a workstation with an AMD 5950x CPU, Nvidia RTX3090 GPU, and 32 GB RAM. All experiments are repeated five times with different random seeds, and the mean performance and its standard deviation are reported.

### Multi-Fidelity Fusion for Benchmark PDEs

We first assess ContinuAR in three canonical PDE simulation benchmark problems, namely, Heat, Burgers', and Poisson's equations as in [28; 37; 38; 39], which produces high-dimensional spatial/spatial-temporal fields as model outputs. The multi-fidelity results are generated by solving the corresponding PDEs using finite difference with mesh nodes of \(\{4^{2},8^{2},16^{2},32^{2},64^{2}\}\), which is also used as the fidelity index \(t\) for ContinuAR and IFC. We use interpolation to upscale the lower fidelity fields and record them at the high-fidelity grid nodes to provide uniform output across different fidelity. Equation parameters in the PDEs and parameterized initial or boundary conditions represent the corresponding inputs. Please refer to Appendix J for comprehensive details of simulation setups.

IFC has an extremely high computational cost which scales poorly to output dimension (see Table 1 for experiments with \(N^{0}=32\) and \(=0.5\)). For Poisson's equation, considering the 5x more iterations required, IFC requires about 62,500x more training time than ContinuAR on a CPU. Itwill take \(32\) days just to finish the Poisson experiment in our experimental setup. Thus, we do not consider IFC a practical method for high-dimensional problems because its training cost is almost as expensive as running a high-fidelity simulation; we only apply it to relatively low-dimensional problems, namely, Heat equation and the later real-world application experiments.

**Classic Subset Assessment**. We follow the classic experiment setup where the training samples are consistently increased; the lower-fidelity data forms a superset of the higher-fidelity data for this experiment. To deliver concrete results, the high-fidelity training samples are reduced at rate \(\), i.e., \((|^{(t_{i+1})}|_{n})=|^{(t_{i})}|_{n}\), while the removed samples are randomly selected. For each experiment, we gradually increase the number of lowest-fidelity training data \(N^{0}\) from 32 to 128 and calculate predictive accuracy (RMSE). The statistical result for five repeated experiments (with different random seeds) is demonstrated in Fig. 2. The superiority of IFC and ContinuAR indeed highlight the benefits of harnessing the useful information hidden in the fidelity indicators \(t_{i}\). All method benefits from a larger \(\); MF-BNN is unstable due to its model complexity. ContinuAR outperforms the competitors with a large margin consistently in call cases (with up to 6.8x improvement). Averaging over all experiments, ContinuAR achieves 3.5x, 2.5x, and 8.7x accuracy improvements over the best competitor for Heat, Burger's, and Poisson's equation, respectively. Testing error against training time for Heat \(N^{0}=32\) and \(=0.5\) is shown in Fig. 1, where ContinuAR achieves 2.8x and 603x improvement in RMSE and training time. Detailed mean error fields (see Appendix for the computational details) are also provided in Fig. 4, which clearly reveals the superiority of ContinuAR by producing minimal red regions (high error) and maximal blue regions (low error).

**Non-subset Assessment**. We then assess the compatibility of ContinuAR for non-subset training data, which is inevitable in many-fidelity problems. The setup is similar to subset assessment with the same decreasing rate \(\), except that the training data for each fidelity is randomly selected without forcing a subset structure. The results are reported in Fig. 3. The results are consistent with the subset assessment, and ContinuAR outperforms the competitors with a large margin (with up to 3x improvements), whereas MF-BNN performs poorly as usual. Averaging over all experiments, ContinuAR achieves 1.9x, 1.6x, and 1.5x accuracy improvements over the best competitor for Heat, Burger's, and Poisson's equation, respectively. The mean error fields are demonstrated in Fig. 4, which draws the same conclusion as the subset assessment.

    & IFC & Ours & AR & ResGP & MF-BNN \\  & (CPU/GPU) & & & & \\  Heat & 3.0/3.6 & 0.016 & 0.012 & 0.010 & 0.014 \\ Burgers & 66.9/63.9 & 0.010 & 0.009 & 0.024 & 0.026 \\ Poisson & 137/67.1 & 0.011 & 0.001 & 0.001 & 0.025 \\   

Table 2: Training time (seconds) per iteration Figure 1: Testing RMSE against training time for Heat equation.

Figure 2: Subset Evaluation with \(=0.5\) (top row) and \(=0.75\) (bottom row): RMSE against number of training samples \(N^{0}\) for Heat (left), Burger’s (middle), and Poisson’s (right) equation.

### Multi-Fidelity Fusion for Real-World Applications

Next, we assess ContinuAR with real-world applications. Particularly, we look at two practical simulation applications: topology optimization (TopOP) and Plasmonic nanoparticle arrays (PNA) simulation, both of which are known for their high computational cost and render the need for multi-fidelity fusion. The detailed problem descriptions and simulation setups are given in Appendix J. The TopOP data contains five fidelity solutions of dimensionality of \(1600\) based on simulations on a mesh concatenating \(\{64,256,576,1024,1600\}\) nodes, whereas the PNA data has five-fidelity outputs of two dimensions.

The same subset and non-subset assessments are conducted, and the results are reported in Fig. 5. We can see that the advantage of ContinuAR is clear but less pronounced due to the complexity of the data. Averaging over all experiments, ContinuAR achieves 1.1x and 1.4x improvements over the best competitor in TopOP and PNA, respectively. Although taking averagely 192s for training (140x more than ResGP and AR) for PNA, IFC only shows small improvements over ResGP and AR in non-subset settings, but not in the subset settings. In contrast, the training time of ours is only 5.7s (4.4x more than ResGP and AR) and the improvements are significant in all cases. Detailed mean error field in Fig. 4 shows a significant reduction in red region volume for ContinuAR.

Figure 4: Average RMSE fields of the subset (left four columns) and non-subset (right four columns) evaluation with \(=0.5\) and 128 \(N^{0}\) training samples for Heat equation (1st row), Poisson’s equation (2nd row), Burger’s equation (3rd row), and TopOP (4th row).

Figure 3: Non-Subset Evaluation with \(=0.5\) (top row) and \(=0.75\) (bottom row): RMSE against number of training samples \(N^{0}\) for Heat (left), Burger’s (middle), and Poisson’s (right) equation.

**Cost Evaluation.** Finally, to assess ContinuAR in a more realistic setting, we relax the requirement that low-fidelity samples are more than high-fidelity samples and randomly pick samples for each fidelity (ensuring that each fidelity has a minimal of four samples) for each models. We do not test under different seeds because the randomness of setting is already sufficient to assess the robustness of the models. The computational cost for generating the training data against accuracy results for all assessment datasets are reported in Fig. 6. The superiority of ContinuAR over the competitors is consistent with previous experiments. The average accuracy improvements over all settings of ContinuAR over the best competitors is 4.3x, 9.3x, 2.7x, 1.2x, and 1.3x for Heat equation, Burger's equation, Poisson's equation, TopOP, and PNA, respectively.

## 7 Conclusions

In this work, we propose a unifying framework FiDEs to pave the way for tractable infinite-fidelity fusion and a novel solution ContinuAR, which shows a significant improvement over the SOTA competitors. We expect FiDEs and ContinuAR can lead to the development of surrogate-assisted systems to deliver cheaper, more efficient, and eco-friendly scientific computation. The limitation of this work includes the rank-1 assumptions and further examination in highly-complex real systems.

Figure 5: Subset (first row) and Non-Subset (second row) Evaluation.

Figure 6: Cost Evaluation for Heat equation, Burger’s equation, Poisson’s equation, TopOP, and PNA.