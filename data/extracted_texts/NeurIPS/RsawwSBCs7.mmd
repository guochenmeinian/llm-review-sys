# OTTER: Effortless Label Distribution Adaptation of Zero-shot Models

Changho Shin, Jitian Zhao, Sonia Cromp, Harit Vishwakarma, Frederic Sala

Department of Computer Sciences

University of Wisconsin-Madison

{cshin23, jzhao326, cromp, hvishwakarma, fsala}@wisc.edu

###### Abstract

Popular zero-shot models suffer due to artifacts inherited from pretraining. One particularly detrimental issue, caused by unbalanced web-scale pretraining data, is _mismatched label distribution_. Existing approaches that seek to repair the label distribution are not suitable in zero-shot settings, as they have mismatching requirements, such as needing access to labeled downstream task data or knowledge of the true label balance in the pretraining distribution. We sidestep these challenges and introduce a simple and lightweight approach to adjust pretrained model predictions via optimal transport. Our technique requires only an _estimate_ of the label distribution of a downstream task. Theoretically, we characterize the improvement produced by our procedure under certain mild conditions and provide bounds on the error caused by misspecification. Empirically, we validate our method in a wide array of zero-shot image and text classification tasks, improving accuracy by 4.8% and 15.9% on average, and beating baselines like prior matching--often by significant margins--in 17 out of 21 datasets.

## 1 Introduction

Zero-shot models are popular but struggle with biases inherited from their large pretraining datasets [21; 59; 2]. In particular, zero-shot classification is strongly biased by _the label distribution_ of the pretraining task. When the label distribution of the downstream task differs from pretraining, the performance of zero-shot classifiers suffers greatly. For example, Figure 1 illustrates the effects of mismatched distributions on a pet image classification task. Two CLIP models (RN50, and ViT-B/16) produce biased predictions on the Abyssinian and Persian classes. Furthermore, datasets with a large number of classes, such as ImageNet, may contain both extremely common and very rare classes, resulting in an outsized probability that a zero-shot model will predict some classes over others. As a result, even large models intended for use in zero-shot settings, such as CLIP , naturally have a label distribution mismatch between pretraining data and downstream tasks.

Existing methods that seek to address label distribution _make strong assumptions or have expensive requirements_. For example, to fine-tune a model, we must obtain a labeled fine-tuning dataset of adequate size, then obtain the time and compute to further train the model. To perform label shift adap

Figure 1: Label distribution mismatch example in zero-shot classification. In the Oxford-IIIT-Pet dataset, the ground-truth labels are uniformly distributed, while zero-shot models exhibit biased predictions toward certain classes. This bias is influenced by the distribution of labels in the pretraining task.

tation techniques, we must know the true label distribution of the pretraining distribution--difficult to impossible for real-world tasks.

Can we deal with label distribution mismatch _without additional training or access to ground-truth downstream task information_? While seemingly challenging, one cause for optimism is the observation that zero-shot models still give relatively high prediction probabilities for correct classes, though classes common in pretraining tend to have relatively inflated scores overall. Intuitively, the model has already learned to identify examples of its downstream classes (and so does not require further training) and is already impacted by the pretraining label distribution (and so does not need access to the ground-truth pretraining label distribution). Instead, the model's prediction probabilities must be adjusted based on an estimated downstream label distribution specification.

To perform this label distribution adjustment, we view zero-shot learning through the lens of optimal transport (OT) and develop a technique called **OTTER (Optimal Transport AdapER)**. This OT-based approach offers a systematic way to rebalance predicted labels: data points are transported to optimal downstream classes, minimizing the overall cost in accordance with the estimated the downstream label distribution specifications.

Theoretically, we show that optimal transport given the true label distribution of the downstream can recover the Bayes-optimal classifier under mild conditions. Additionally, we provide error bounds on our adaptation method for misspecification. We provide synthetic experiments validating our theoretical claims. In real-world data settings, we validate our method on a wide variety of image and text classification tasks, showing 4.8% and 15.5% accuracy improvement on average in image and text zero-shot classification tasks, respectively. Finally, we evaluate our method in _few-shot adaptation_ scenarios, where OTTER provides further improvements when combined with linear probing. Our contributions include:

* OTTER, an algorithm to deal with label distribution mismatch at inference time via optimal transport,
* Theoretical results showing the effectiveness of our method, including the ability to recover the Bayes-optimal classifier and a sensitivity analysis with respect to the label distribution specification estimation,
* Extensive empirical results on zero-shot classification for text and image datasets, showing accuracy improvements of up to 25%,
* Experimental results demonstrating the applicability of OTTER to few-shot settings, showing accuracy improvements of up to 15%, even with noisy label distribution specifications,
* Extensions of OTTER to leverage label hierarchy information or relax the batched prediction requirement and
* Application to LLM selection bias mitigation.

## 2 Background and Problem Formulation

Before presenting OTTER and providing our results, we introduce some crucial background and describe the problem setting formally.

### Background

We briefly describe zero-shot models, the technical tool we use (optimal transport), along with other techniques that seek to address shifts. We have extended related work, including in-depth characterizations and comparisons with related methods, in Appendix B.

Zero-shot Models.Zero-shot classification, popularized by models such as CLIP , is a powerful paradigm that enables prediction on downstream tasks without additional fine-tuning. Image, language, and multimodal models have been increasingly employed for zero-shot prediction [64; 36]. These models undergo extensive pretraining on massive datasets with concept and label spaces that may be very different from those of downstream applications.

Optimal Transport.Optimal Transport (OT) is a framework for matching two probability distributions [48; 53]. We predominantly consider optimal transport between empirical discrete measures. Suppose that we are given points \(x_{1},x_{2},,x_{n}\) and \(y_{1},,y_{K}\), a source measure \(\) defined by \(=_{i=1}^{n}w_{i}_{x_{i}}\), and a target measure given by \(=_{j=1}^{K}p_{j}_{j}\), where \(w_{i},p_{j}\) are positive values such that \(_{i=1}^{n}w_{i}=1\), \(_{j=1}^{K}p_{j}=1\). Suppose also that \(_{x}\) is a Dirac delta function at \(x\), i.e. \(_{x}(x^{})=1\) if \(x^{}=x\), and \(_{x}(x^{})=0\) otherwise. Given a cost matrix \(C^{n K}\), the Monge-Kantorovich formulation of optimal transport is to find a minimal cost transport plan \(\) such that

\[=*{arg\,min}_{(,)},C,\]

where \((,)=\{^{n K}_{+}|=, ^{T}=\}\).

Distribution and Label Shifts.Distribution shift refers to the discrepancy between a source distribution \(P_{s}\) on which the model is trained, and a target distribution \(P_{t}\) on which the model is deployed. Distribution shift often degrades trained model performance on target tasks. Label shift is a specific type of distribution shift such that \(P_{s}(Y) P_{t}(Y)\) and the data generation process is fixed -- in other words, the conditional distributions of the inputs are the same: \(P_{s}(X|Y)=P_{t}(X|Y)\). Techniques such as importance sampling [35; 6; 24], recalibration  and domain adaptation  are commonly used to mitigate the effects of label shift. Unfortunately, these methods assume access to source distribution data, whereas zero-shot models' pretraining data is inaccessible (often proprietary or blocked for privacy reasons). Thus, adapting zero-shot models to new label distributions poses challenges unmet by these pre-existing methods.

### Problem Formulation

Let \(=\{x_{1},x_{2},,x_{n}\}\) be an inference dataset with \(x_{i}\). Furthermore, let \(=\{y_{1},y_{2},,y_{n}\}\) be the true labels of the \(K\)-class classification dataset, such that \(y_{i}=[K]\), are sampled according to the downstream label distribution \(=(p_{1},p_{2},,p_{K})\).

Let \(s_{}(x,j):=P_{}(Y=j|X=x)\) be a pretrained classification model constrained to the downstream label space. During pretraining, \(s_{}\) has been biased to the source label distribution \(^{s}\). We wish to offset such label distribution bias with a label distribution specification \(\) for the target distribution. \(\) is expected to be closer to the true label distribution of the downstream task. Given a label distribution specification, our goal is to rebalance predictions so that the predicted label distribution follows the label distribution specification.

```
1:Input: Input \(=\{x_{1},,x_{n}\}\), label distribution specification \((p_{1},,p_{K})\), cost matrix \(C^{n K}\)
2:Define input marginal \(=\), prediction marginal \(=(p_{1},,p_{K})\)
3:Run optimal transport and obtain transport plan \(\) s.t. \(=*{arg\,min}_{(,)},C\).
4:Get modified classification outputs \(_{i}=*{arg\,max}_{j[K]}_{i,j}\). Return \(\{_{i}\}_{i[n]}\) ```

**Algorithm 1**OTTER

## 3 Proposed Framework

We propose OTTER (Optimal TransportT adaptER), an optimal transport-based label distribution adaptation approach. Our goal is to have the \(n\) input data points allocated to \(K\) classes match a given label distribution \(\), where \(_{j=1}^{K}_{j}=1,_{j} 0\). Specifically, we want to classify \(n_{1}\) points as the first class, \(n_{2}\) points as the second, and so on. However, there are many such allocations, and it is not a priori clear which one should be selected. We propose formulating an optimal transport problem that selects the allocation minimizing a particular cost:

\[=*{arg\,min}_{(,)},C,\]

where \((,)=\{^{n K}_{+}|=,^{T}=\}\), \(=\) and \(C\) is the cost (loss) matrix such that \(C_{ij}\) represents a loss when we classify \(x_{i}\) as class \(j\). This procedure is described in Algorithm 1. Note that this procedure naturally matches the given label distribution specification \(\).

We wish to use Algorithm 1 for zero-shot classification given the pretrained model \(s_{}\). To do so, we must select a cost function and produce \(C_{ij}\). An ideal choice of such a function is \(C_{ij}=- P_{t}(Y=j|X=i)\) such that optimal transport minimizes the negative log posterior under constraints. However, the target distribution \(P_{t}\) is unknown. Instead, we replace the posterior with the classifier scores \(s_{}(x_{i},j)\). We highlight that this choice of cost matrix is an natural extension of zero-shot classification under the label distribution constraint. We prove this claim in the next section. First, we show a toy example that shows how OTTER improves zero-shot accuracy.

Example.To illustrate the benefits of OTTER, consider the following example for binary classification. We have two data points, \(X=\{x_{1},x_{2}\}\) with \(Y=\{1,2\}\), and true label distribution \(=(,)\). Suppose that the zero-shot model's prediction scores are \(s_{1}=(0.4,0.6)\) and \(s_{2}=(0.1,0.9)\).

Traditional classification yields \(_{1}=2,_{2}=2\), producing a 50% error rate. However, given the cost matrix \(C\) derived from the prediction score matrix

\[S=0.4&0.6\\ 0.1&0.9,C=- 0.4&- 0.6\\ - 0.1&- 0.9,\]

along with \(=(0.5,0.5)\) and \(=(0.5,0.5)\), the optimal transport procedure discovers the transport map \(=0.5&0.0\\ 0.0&0.5\), yielding \(_{1}=1,_{2}=2\). This corrects the original zero-shot prediction error.

Extension to Online PredictionsWhile OTTER offers efficient label distribution adaptation, it requires a batched set of inference data points, making online predictions challenging. To address this, we introduce R-OTTER (Reweighting OTTER), which learns a reweighting factor--an estimate of \(P_{t}(Y)/P_{s}(Y)\)--using OTTER's predictions \(y_{}\) on a validation set. Once learned, these reweighting parameters can be applied to online predictions by adjusting logit probability scores. We use a reweighting formulation equivalent to logit adjustment in . The reweighted probability scores of \(P_{}\), with a reweighting vector \(r^{K}\), are defined as:

\[P_{,r}(Y=j|X=x)=P_{}(Y=j|X=x)}{_{j^{ }=1}^{K}r_{j^{}}P_{}(Y=j^{}|X=x)}.\]

The parameter \(r\) is learned using cross-entropy loss on \(y_{}\). We expect R-OTTER to perform comparably to OTTER, with the additional advantage of not requiring OTTER to be run over the entire dataset. In the following section, we provide a theoretical result showing that \(r^{*}=P_{t}(Y)/P_{s}(Y)\) is an optimal reweighting parameter when learned from \(y_{}\) as pseudolabels, effectively addressing label shift.

## 4 Theoretical Results

In practical scenarios, label distribution specifications are frequently subject to noise, and prediction probabilities may not be well-calibrated. To understand the impact of these factors, we examine how errors in label distribution specification and calibration influence the transport plan. Our theoretical analysis yields following findings: (a) OTTER can recover the Bayes-optimal classifier in the label shift setting, (b) for a noisy cost matrix with the noisy label distribution specification setup, the suboptimality can be bounded by the deviation of cost matrix and label distribution, and (c) R-OTTER effectively addresses label shift when learned from \(y_{}\) as pseudolabels.

Classification as optimal transport.Prior to discussing the main theoretical results, we demonstrate that standard classification--expressed as \(_{i}=*{arg\,max}_{j[K]}P_{}(Y=j|X=x_{i})\)--can be interpreted as a (trivial) solution derived from optimal transport.

**Theorem 4.1**.: _Let \(^{ZS}_{j}=_{i=1}^{n}[_{i}^{ZS}=j]\), where \(_{i}^{ZS}=*{arg\,max}_{j^{}[K]}P_{}(Y=j^{ }|X=x_{i})\). Then, given \(C_{ij}=- P_{}(Y=j|X=x_{i})\),_

\[=*{arg\,min}_{(,^{ZS})}< ,C>,\] \[_{i}^{OT}=*{arg\,max}_{j[K]}_{ij}.\]

_Assuming there are no ties in scores, i.e. \(P_{}(Y=j|X=x_{i}) P_{}(Y=j^{}|X=x_{i})\), for all \(j j^{}\), the OTTER predictions are equivalent zero-shot predictions, i.e. \(_{i}^{OT}=_{i}^{ZS}\) for all \(i[n]\)._

This theorem has the following implications. First, it suggests that the predictions will remain unchanged if we set \(=^{ZS}\). Second, Bayes-optimal classifiers can be derived through optimal transport, using a (true) cost matrix defined as \(C_{ij}^{*}=- P_{t}(Y=j|X=x_{i})\), coupled with the true label distribution \(^{*}\).

Our analysis begins with the label shift setup, which is a commonly-studied type of distribution shift--as well as a prominent issue when applying zero-shot models. We demonstrate that when the label distribution is correctly specified, optimal transport preserves Bayes-optimal classifier predictions under label shift. Next, we consider general perturbations in label distribution and cost matrix as well as their impact on the resulting solutions.

### Label Shift Invariance

In this setting, we assume features follow the same conditional distribution across source and target distributions, i.e. \(P_{s}(X|Y)=P_{t}(X|Y)\). Furthermore, we suppose that the prediction scores are accurately calibrated in the training dataset, such that \(s_{}(x,j)=P_{s}(Y=j|X=x)\). For zero-shot models, we often lack access to \(P_{s}\). This is a typical scenario in zero-shot model applications: after training on large-scale corpora, we use the pretrained model without the source dataset.

For a given downstream task with the target label distribution \(^{*}=P_{t}(Y)\), one standard approach to achieve the Bayes-optimal classifier for the target distribution is to reweight the score function outputs using the ratio \((Y=j)}{P_{s}(Y=j)}\). This adjustment leads to:

\[_{}(x,j)=s_{}(x,j)(Y=j)}{P_{s}(Y=j)} P _{t}(X=x|Y=j) P_{t}(Y=j) P_{t}(Y=j|X=x).\]

This reweighted score function aligns with the target distribution, thus correcting label shift.

Although reweighting the score function is a popular solution, it faces an important obstacle when applied to zero-shot models like CLIP, where the source distribution \(P_{s}(Y)\) is typically unknown. We show that OTTER successfully induces a Bayes classifier for the target distribution, represented as \(f_{t}(x)=*{arg\,max}_{j[K]}P_{t}(Y=j|X=x)\), without requiring access to \(P_{s}(Y)\). This capability is particularly significant for zero-shot models, enabling them to adapt to target distributions effectively, even in the absence of explicit knowledge of the source distribution.

Now, we show that optimal transport can be an effective tool to correct label shift.

**Theorem 4.2**.: _Suppose the pretrained model is well-calibrated for the source distribution,_

\[P_{}(Y=j|X=x_{i})=P_{s}(Y=j|X=x_{i})\]

_and there is no tie probability, for all \(j j^{},i[n]\)_

\[P_{}(Y=j|X=x_{i}) P_{}(Y=j^{}|X=x_{i}).\]

_Denote the Bayes optimal predictions in the target distribution as \(_{i}^{*}=*{arg\,max}_{j[K]} P_{t}(Y=j|X=x_{i})\). Then_ OTTER _predictions \(=(,^{*},C)\) are the same as Bayes optimal predictions \(^{*}\)._

That is, OTTER recovers a Bayes classifier in the target distribution without access to the source distribution, given the target distribution and a well-calibrated model for the source dataset.

### General Perturbation Sensitivity

In practical applications, calibration error could extend beyond noise in the elements of the cost matrix. A key source of error is label distribution estimation error. Hence, we address a more general setting, examining the impact of simultaneous perturbations in the label distribution and cost matrix of the transport plan. Our result applies techniques from perturbation theory for linear programming.

We rewrite our optimal transport problem \(_{(,)},C\) as a linear programming problem. Let \(\) and \(C\) be the transport plan and cost matrix respectively. Matrix \(G\) and vector \(g\) are used to denote the row and column constraints on \(\) to form a feasible plan which transports distribution from \(\) to \(\).

\[H:=_{n}^{T}_{K}\\ _{n}_{K}^{T},G=H\\ -H,g=\\ \\ -\\ -.\]

Then, we have the equivalent linear programming problem,

\[\{_{i,j}C_{i,j}_{i,j}|G() g, 0 \}.\] (1)We adapt a theorem from Robinson  with our optimal transport problem notation.

**Theorem 4.3**.: _Let the primal linear programming problem be defined as in equation (1), and its dual problem be \(\{w^{T}g|w^{T}G(C)^{T},w 0\}\). Suppose perturbed cost matrix is \(=C+_{C}\), the perturbed class distribution \(=+_{}\), such that \(=g+_{g}\) where_

\[_{g}=0\\ -\\ 0\\ -+.\]

_Assume that primal and dual problems are solvable. Denote the original solutions as \(,w\) and perturbed solutions as \(\) and \(\). Then,_

\[\|-\|_{F}^{2}^{2}(\|_{}\|_{2}^{2}+\|[ {vec}(_{C})]_{+}\|_{2}^{2}+\|(C)^{T}()-g^{ T}\|_{2}^{2})-\|w-\|_{2}^{2}\]

_, where \(1 p\) and \(\) is a Hoffman constant that only relates to the original problem ._

Ignoring the constant and the subtraction part, the upper bound can be decomposed into three components,

* \(_{}\): noise (or the estimation error) of the target balance,
* \([(_{C})]_{+}\): noise (or the calibration error) of the cost matrix,
* \((C)^{T}()-g^{T}\): the suboptimality of perturbed solution \(\).

Theorem 4.3 implies that the deviation from perturbed solution to true solution is bounded by the magnitude of perturbations and suboptimality of the perturbed solution. From this result, we can expect prediction accuracy to deteriorate with perturbations in the label distribution and calibration.

### Effectiveness of R-OTTER

We provide a theoretical result showing that R-OTTER can learn an optimal parameter by learning reweighting parameters from \(_{}\) as pseudolabels, and produce identical predictions to OTTER once the optimal parameter is obtained in the label shift setup.

**Theorem 4.4**.: _Under the same assumptions as in Theorem 4.2, the parameter \(r^{*}=P_{t}(Y)/P_{s}(Y)\) is optimal when learning with \(y_{}\) as pseudolabels._

The proof is provided in Appendix D.6.

## 5 Experiments

The primary objective of our experiments is to (1) validate that OTTER improves zero-shot model performance when given accurate label distribution estimates and (2) investigate its sensitivity to perturbations. In experiments on real datasets (Section 5.1), we confirm that OTTER can improve zero-shot classification significantly in a variety of settings. In synthetic experiments (Section 5.2), we validate our theoretical claims--label shift invariance and sensitivity to perturbation in a fully controllable setting. Additionally, we show that OTTER can be combined with label distribution estimation methods in the few-shot learning setting (Section 5.3). Next, we show the empirical results for H-OTTER that leverages label hierarchy (Section 5.4) and R-OTTER that mitigates the batched prediction requirement (Section 5.5). Finally, we we apply OTTER to mitigate LLM selection bias (Section 5.6). Our code is available at https://github.com/SprocketLab/OTTER.

### Real Data Experiments

We hypothesize that the model performance can improve significantly when the label distribution specification is exact.

**Setup and Procedure.** We used 17 image classification datasets and 4 text classification datasets. We employed CLIP  for image zero-shot classification, and BERT . A comprehensive list and details of experiments can be found in Appendix E.

**Baseline.** We adopt Prior Matching (PM)  as a baseline. It optimizes score weighting parameters to align with the label distribution specification. A detailed explanation of Prior Matching is given in Appendix C. It is worth noting that the performance of _Prior Matching is highly sensitive to hyperparameters such as temperature and learning rate_. Optimal hyperparameters may vary across different datasets. We selected hyperparameters through grid search, by evaluating their performance on a validation set, consisting of 10 labeled examples per class. In contrast, we highlight that OTTER is tuning-free.

**Results.** Table 1 shows the image classification results with CLIP (ViT-B/16) and the text classification results with BERT. Notably, OTTER _demonstrates a 4.8% and 15.5% enhancement on average in image and text zero-shot classification, respectively_. While Prior Matching shows competitive performance when accurately tuned, it often struggles. We found that hyperparameter tuning fails in the class-imbalanced datasets such as Caltech256, SUN397, ImageNet-r (Appendix E, Table 7). This suggests that the hyperparameter selection process necessitates a validation set label distribution similar to the target distribution--rendering it unusable in zero-shot scenarios. More details and additional experiment results -- including the sensitivity study on the label distribution specification error, computation time, and combination with other prompting methods -- are provided in Appendix E.3.

### Synthetic Experiments

We hypothesize OTTER is invariant to label shift under the conditions in Theorem 4.2. We also investigate the sensitivity to perturbations of the cost matrix and the label distribution.

**Setup and Procedure.** We simulate label shift in logistic regression. Suppose \(X|Y=0(-1,1)\) and \(X|Y=1(1,1)\). Training data is sampled from a mixture of Gaussians \(X_{s}_{0}^{s}(-1,1)+_{1}^{s}(1,1)\) such that \(P_{s}(Y=0)=_{0}^{s},P_{s}(Y=1)=_{1}^{s}\), \(_{0}^{s}+_{1}^{s}=1\). Similarly, we sample the test data from \(X_{t}_{0}^{t}(-1,1)+_{1}^{t}(1,1)\). We fix the training set label distribution as \(_{0}^{s}=0.1,_{1}^{s}=0.9\) and vary test set label distribution \(^{t}\) to simulate label shift. We train a logistic regression model with 10,000 samples from the source distribution, and test the model with 10,000 samples from the target distribution. A Bayes-optimal classifier in the target distribution is given by \(f_{Bayes}(x)=[x(^{t}}{_{1}^{t}}+1)]\). The naive classifier is defined as the maximizer of the predicted score. The OTTER predictions are produced with Algorithm 1, with the cost matrix \(C_{ij}=- P_{}(Y=j|X=x_{i})\) and the label distribution specification \(^{t}\), where \(P_{}(Y|X)\) represents the logistic regression model scores.

We separately investigate perturbed prediction score matrix and perturbed label distribution specification's impact on the prediction accuracy. For perturbed prediction scores, we fix the label distribution to be the true one, and add noise \((0,^{2})\) of varying levels \(\) to the predicted score \(P_{}(Y=1|X)\). For the perturbed label distribution specification, we fix the prediction score to be true scores and add noise \(\): \(=^{t}+(,-)\). We use these perturbed variants to obtain perturbed solutions and compare with ground-truth solution.

**Results.** Figure 2 illustrates how accuracy changes with label shift when the predicted score is perturbed and when label distribution specification is perturbed. We observe that the naive classifier

   & Zero-shot Prior Matching & OTTER & & Zero-shot & Prior Matching & OTTER \\  CIFAR10 & 88.3 & 91.3 (\( 0.0\)) & **91.7** & Oxford-IIIT-Pet & 83.8 & 82.0 (\( 0.3\)) & **88.8** \\ CIFAR100 & 63.8 & 64.1 (\( 2.7\)) & **67.9** & Stanford-Cars & 55.7 & 39.8 (\( 2.6\)) & **59.7** \\ Caltech101 & 79.8 & 59.3 (\( 15.4\)) & **88.7** & STL10 & 98.0 & 98.4 (\( 0.0\)) & **98.6** \\ Caltech256 & 79.8 & 9.5 (\( 1.5\)) & **87.0** & SUN397 & 47.1 & 6.7 (\( 1.6\)) & **54.1** \\ Country211 & 19.8 & 19.0 (\( 0.1\)) & **21.1** & CUB & 46.0 & 40.4 (\( 0.0\)) & **50.4** \\ DTD & 39.0 & 42.1 (\( 0.1\)) & **44.4** & ImageNet & 60.2 & 53.6 (\( 0.1\)) & **62.9** \\ EUROSAT & 32.9 & 41.6 (\( 0.8\)) & **42.0** & ImageNet-r & 68.9 & 16.7 (\( 3.5\)) & **72.4** \\ Flowers102 & 64.0 & 54.0 (\( 14.1\)) & **70.8** & ImageNet-Sketch & 39.8 & 36.5 (\( 0.4\)) & **44.5** \\ Food101 & 85.6 & 86.8 (\( 3.1\)) & **89.9** & & & \\  Amazon review & 74.0 & 58.8 (\( 46.4\)) & **91.7** & GenderBias & 84.1 & 41.4 (\( 39.6\)) & **91.9** \\ CivilComments & 48.4 & 57.2 (\( 37.7\)) & **81.4** & HateXplain & 30.9 & 31.3 (\( 3.3\)) & **34.3** \\  

Table 1: Accuracy (%) in zero-shot image classification (ViT-B/16) and text classification (BERT). We use the true label distribution as the label distribution specification. The numbers in parenthesis of Prior Matching represent the standard deviation of 10 different samplings of the validation set. OTTER produces improvements nearly across-the-board, with an average lift 4.9% in image classification and 15.5% in text classification, outperforming a powerful baseline, prior matching in almost all cases.

deteriorates as the total variation distance between source and target distributions increases. It indicates that naive classifier is sensitive to label shift. However, without perturbation, OTTER _remains unaffected by the label distribution shift_, which validates our invariance result in Section 4.

In the case of confidence prediction perturbation, both the naive classifier and OTTER have accuracy decreasing as perturbation level increases. For simplicity, we omitted the naive classifier's performances under different levels of noise as adding zero-mean noise does not alter its accuracy significantly. We observe that OTTER has better performance than the naive method when significant label shift exists. Similarly, for label distribution perturbation, we observe as the noise level \(\) increases, OTTER's accuracy downgrades--but still yields better performance when label shift is severe.

Our experimental results suggest simply using prediction scores for zero-shot classification leads to inaccurate predictions under label shift, while OTTER is robust to label shift when no perturbations are present. Perturbations in both predicted score and label distribution specification downgrades the predicted accuracy, as expected, but OTTER still yields better results than the naive baseline.

### Few-shot adaptation with label distribution estimation

We anticipate that OTTER can be used in few-shot learning when combined with label distribution estimation methods. We expect OTTER can improve zero-shot classification if the label distribution estimation error is sufficiently small. Also, we expect OTTER can improve linear probing, which is one of standard approaches for few-shot learning.

**Setup and Procedure.** We use the same datasets as the previous experiment. We consider a 10-shot learning setting: 10 labeled samples per class are given. Note that labeled samples have uniform label distribution, while the label distribution in the target distribution _may not be uniform_. This setting requires the use of label distribution estimation methods used in label shift adaptation [35; 6; 24]. We estimate the target label

Figure 4: Ablation on the number of samples in few-shot learning. In (a), We can observe that BBSE estimation get more precise as the number of samples increases. Following this, OTTER gets better accuracy in (b). Additionally, OTTER consistently improves linear probing when combined.

Figure 2: Synthetic experiment results. X-axis represents total variation distance between the source and the target distribution, describing label shift severity. Y-axis represents prediction accuracy. Curves represent different methods and noise levels. Our approaches dramatically outperform the baseline at higher mismatch levels.

distribution with Black Box Shift Estimation (BBSE) . BBSE estimates the target balance using confusion matrix, under the label shift assumption. For detailed explanation, refer to Appendix C.

**Results.** Table 2 shows the image and text zero-shot classification results with the label distribution estimation via BBSE and linear probing. The image classification results show that OTTER can yield mild improvement over linear probing, even with the label distribution estimation errors. Figure 4 shows that accuracy improvement is consistent across the number of samples used for linear probing. In text classification, we found OTTER improves zero-shot text classifications where the number of classes is small (\(K=2\) or \(3\)). While it shows a relatively high variance due to the small sample size (20 \(\) 30), the average accuracy improves significantly over zero-shot classification. More detailed analysis regarding label distribution estimation error and the number of samples is provided in Appendix E.4.

### Zero-shot prediction improvement with class hierarchy

We hypothesize incorporating class hierarchy information can enhance few-shot label distribution estimation and thus improve zero-shot predictions.

**Setup and Procedure.** We use a subset of CIFAR100 data with WordNet hierarchy. Specifically, we take 'fish' and 'tree' as superclasses and have 5 subclasses in each of them. We suppose we can access 10 labeled samples per each subclass. We first apply OTTER with the superlevel label distribution estimation and make pseudo-labels of superlevel class in the test set. Using them, we estimate the sublevel label distribution and use OTTER.

**Results.** Table 3 presents the results. As anticipated, we note an enhancement in accuracy when compared to the naive implementation of OTTER. Specifically, we observe a significant improvement in accuracy for RN50, RN101, and ViT-B/16,

 Dataset & ZS & ZS BBSE+PM & ZS BBSE+OT & LP & LP & BBSE+PM & LP \\  CIFAR10 & 88.3 & 72.7 & 87.5 & **90.2** & 89.8 & 90.0 \\ CIFAR100 & **63.8** & 3.2 & 59.1 & 58.3 & 24.4 & 60.5 \\ Caltech101 & 79.8 & 32.5 & 80.7 & **91.5** & 87.5 & 91.4 \\ Caltech256 & 79.8 & 6.0 & 80.3 & 84.5 & 58.4 & **85.4** \\ Country211 & **19.8** & 1.5 & 15.9 & 12.4 & 9.2 & 13.2 \\ DTD & 39.0 & 3.2 & 31.2 & 58.6 & 49.0 & **59.3** \\ EUROSAT & 32.9 & 19.2 & 34.0 & 74.6 & 71.6 & **75.9** \\ Flowers102 & 64.0 & 40.3 & 60.8 & 89.0 & 87.8 & **90.2** \\ Food101 & **85.6** & 15.3 & 82.3 & 79.1 & 60.6 & 79.8 \\ Oxford-IIII-Pet & **83.8** & 43.3 & 71.4 & 75.7 & 72.0 & 75.6 \\ Stanford-Cars & 55.7 & 2.3 & 51.7 & 64.5 & 65.4 & **66.3** \\ STL10 & **98.0** & 97.4 & 96.9 & 97.7 & 97.5 & 97.6 \\ SUN397 & **47.1** & 6.9 & 25.6 & 0.2 & 0.2 & 0.2 \\ cub & 46.0 & 3.3 & 45.5 & 72.2 & 63.3 & **75.6** \\ ImageNet & **60.2** & 0.8 & 57.7 & 56.8 & 53.6 & 59.8 \\ ImageNet-r & **68.9** & 1.7 & 63.3 & 54.9 & 47.6 & 57.1 \\ ImageNet-Sketch & 39.8 & 0.8 & 40.4 & 43.4 & 37.9 & **48.3** \\  Amazon & 74.0 & 47.9 & **89.1** & 71.3 & 66.9 & 71.3 \\ CivilComments & 48.3 & **69.1** & 55.8 & 53.8 & 45.5 & 53.8 \\ Gender & 84.0 & 57.0 & **87.8** & 78.0 & 71.2 & 78.5 \\ HateXplain & 30.4 & 34.4 & **35.2** & 32.8 & 32.7 & 32.3 \\  

Table 2: Accuracy (%) with OTTER combined with class balance estimation. ZS BBSE denotes BBSE label distribution estimation based on zero-shot prediction scores, and LP BBSE denotes BBSE label distribution estimation based on linear probing prediction scores. We report the mean of 10 different random samplings of the validation set. OTTER produces moderate improvements when combined with linear probing in image classification tasks. In text classification tasks, OTTER significantly improves accuracy, up to 15.1%, even with noisy label distribution estimation.

  & OTTER & H-OTTER \\  RN50 & 38.5 (\( 4.9\)) & **43.6 (\( 3.1\))** \\ RN101 & 39.9 (\( 6.9\)) & **44.8 (\( 5.1\))** \\ ViT-B/32 & 59.0 (\( 3.1\)) & **59.3 (\( 2.9\))** \\ ViT-B/16 & 54.6 (\( 8.3\)) & **58.2 (\( 3.6\))** \\ ViT-L/14 & **71.3 (\( 3.9\))** & 69.4 (\( 5.2\)) \\  

Table 3: Accuracy (%) with hierarchical OTTER (H-OTTER). (H-OTTER) yields additional improvements over OTTER, up to 5.1%, using the hierarchy information of labels.

which we attribute primarily to the reduction in label distribution estimation error. Further details are provided in Appendix E.5.

### Effectiveness of R-OTTER

We show that R-OTTER provides a performance comparable to that of OTTER empirically.

Setup and Procedure.We use the identical setup for Section 5.1. R-OTTER learns reweighting parameters in validation set using \(y_{}\). Note that the validation set is not required to be labeled since \(y_{}\) is used as pseudolabels in the validation set.

Results.Although R-OTTER is suboptimal compared to OTTER due to generalization issues, it still provides label distribution correction, improving accuracy over zero-shot predictions. We also provide synthetic experiments for R-OTTER in Appendix E.6.

### Mitigating selection bias in LLM multiple-choice questions

Selection bias is the tendency of LLMs to favor certain prefix tokens in multiple-choice questions [69; 17; 12; 65]. We demonstrate that OTTER can effectively mitigate selection bias by randomly shuffling the options and enforcing a uniform class balance in OTTER.

Setup and Procedure.Our experimental setup follows Zheng et al. , utilizing the MMLU , ARC-Challenge , and CommonsenseQA (CSQA)  datasets. We test with llama-2(-chat)-7/13B  and vicuna-v1.3-7/13B  models, treating the probabilities of each option token (A/B/C/D) as prediction probabilities. OTTER is applied under the assumption of a uniform distribution. We use 0-shot predictions and evaluate performance using accuracy and the standard deviation of recalls (RStd) as metrics.

Results.Table 5 presents the experimental results. OTTER significantly reduces selection bias, enhancing accuracy by up to 10.6% and lowering RStd by as much as 25.7%.

## 6 Conclusion

While zero-shot models have been successful, pretraining using Internet-scale datasets yields artifacts that may harm downstream tasks. In this paper, we identify the bias in class balance, and provide a simple but powerful solution using optimal transport. Theoretically, we describe how OT can fix label distribution mismatch and its sensitivity to perturbations. Empirically, we validated our approach's ability to improve zero-shot classification accuracy, mitigating label distribution mismatch in zero-shot models. We believe our method can expedite the deployment of zero-shot classification, reducing the necessity of finetuning for downstream tasks.

   &  &  &  \\   &  &  &  &  &  &  \\
**Model** & Acc. (\(\)) & RStd (\(\)) & Acc. & RStd & Acc. & RStd & Acc. & RStd & Acc. & RStd \\  Lllama-2-7b & 36.0 & 27.4 & **45.5** & **1.7** & 31.9 & 28.4 & **42.7** & **3.8** & 36.1 & 22.5 & **40.0** & **0.9** \\ Lllama-2-13b & **62.9** & 6.0 & 62.8 & **1.5** & 57.0 & 10.2 & **58.1** & **2.0** & 51.1 & 6.9 & **51.8** & **1.3** \\ Lllama-2-7b-ctit & 36.5 & 2.4 & **57.4** & **73.3** & 56.5 & 15.2 & **60.4** & **3.3** & 45.9 & 43.7 & **46.6** & **0.5** \\ Lllama-2-13b-bat & 64.4 & 13.7 & **66.2** & **2.5** & 64.0 & 9.8 & **66.4** & **1.8** & 52.3 & 13.9 & **53.8** & **1.0** \\ Vicuna-7b & 33.3 & 8.6 & **54.1** & **2.3** & 58.9 & 8.5 & **57.6** & **1.0** & 46.6\({}^{-}\) & 5.8 & **46.9** & **0.3** \\ vicuna-13b & 62.9 & 8.3 & **63.3** & **2.4** & 63.4 & 12.9 & **64.5** & **3.1** & 50.5 & 9.9 & **50.7** & **1.1** \\  

Table 4: Accuracy (%) of naive zero-shot, OTTER, and R-OTTER in zero-shot image classification (ViT-B/16)

   &  &  \\  CIFAR10 & 88.3 & 91.7 & 88.4 & Oxford-IIIT-Pet & 83.8 & 88.8 & 85.7 \\ CIFAR100 & 63.8 & 67.9 & 65.3 & Stanford-Cars & 55.7 & 59.7 & 51.0 \\ Caltech101 & 79.8 & 88.7 & 88.2 & STL10 & 98.0 & 98.6 & 98.1 \\ Caltech256 & 79.8 & 87.0 & 79.6 & SUN397 & 47.1 & 54.1 & 46.6 \\ Country211 & 19.8 & 21.1 & 19.5 & CUB & 46.0 & 50.4 & 44.4 \\ DTD & 39.0 & 44.4 & 44.0 & ImageNet & 60.2 & 62.9 & 59.8 \\ EUROSAT & 32.9 & 42.0 & 39.3 & ImageNet-r & 68.8 & 72.4 & 68.5 \\ Flowers102 & 64.0 & 70.8 & 69.7 & ImageNet-Sketch & 39.8 & 44.5 & 39.3 \\ Food101 & 85.6 & 89.9 & 88.5 & & & \\  

Table 4: Accuracy (%) of naive zero-shot, OTTER, and R-OTTER in zero-shot image classification (ViT-B/16)

#### Acknowledgments

We are grateful for the support of the NSF under CCF2106707 (Program Synthesis for Weak Supervision) and the Wisconsin Alumni Research Foundation (WARF). Jitian Zhao gratefully acknowledge support from the IFDS at UW-Madison and NSF through TRIPODS grant 2023239 for their support. We thank Nick Roberts for his valuable discussions and insights.