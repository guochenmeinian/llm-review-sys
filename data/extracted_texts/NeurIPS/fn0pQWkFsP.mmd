# "Using the {schema} schema and {sub_category} sub-category, solve the following problem: {question}"

SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation

 Prakhar Dixit

Department of Computer Science

University of Maryland Baltimore County

pdixit1@umbc.edu

&Tim Oates

Department of Computer Science

University of Maryland Baltimore County

oates@cs.umbc.edu

###### Abstract

Many students struggle with math word problems (MWPs), often finding it difficult to identify key information and select the appropriate mathematical operations. Schema-based instruction (SBI) is an evidence-based strategy that helps students categorize problems based on their structure, improving problem-solving accuracy. Building on this, we propose a Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) framework that incorporates a large language model (LLM). Our approach emphasizes step-by-step reasoning by leveraging schemas to guide solution generation. We evaluate its performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo, and introduce a "reasoning score" metric to assess solution quality. Our findings suggest that SBI-RAG enhances reasoning clarity and facilitates a more structured problem-solving process potentially providing educational benefits for students.

## 1 Introduction

Proficiency in solving math word problems (MWPs) is not only measured by students' ability to arrive at the correct solution but also by their capacity to follow a structured, step-by-step reasoning process . This approach is vital for developing critical thinking and mathematical reasoning abilities, which are essential for tackling complex word problems effectively . Unfortunately, many students struggle with word problems, often failing to identify key information or select the appropriate operations despite understanding the underlying mathematical concepts. This difficulty is a significant barrier to academic success, as highlighted by a survey from the EdWeek Research Center, which reported that nearly 50% of students can read a word problem's text but fail to grasp the mathematical question being asked . Consequently, poor problem-solving skills in MWPs can lead to academic challenges and even failure in school.

Word problems, as a core component of the mathematics curriculum, serve an important function by fostering logical analysis, mental abilities, and creative thinking. Educators and researchers have explored various methods to improve students' proficiency in solving these problems. One such method is Schema-Based Instruction (SBI) , an evidence-based approach widely used in the field of Mathematics that helps students classify word problems based on their underlying structure or schema. SBI has been shown to enhance students' ability to identify relevant information and apply the appropriate mathematical operations for problem-solving. In addition to educational approaches like SBI, Intelligent Tutoring Systems (ITSs) have emerged as valuable tools in addressing challenges associated with MWPs. ITSs leverage artificial intelligence (AI) and interactive interfaces to provide personalized, step-by-step guidance. Examples of ITSs designed for word problem-solving include AnimalWatch , MathCAL , PAT (Pump Algebra Tutor)  and HINTS . These systems have proven effective in supporting learners by offering feedback, hints, and individualized learning paths. However, many ITSs rely on rule-based algorithms and lack the transformative potential of more recent AI advancements, like those in Natural Language Processing (NLP)  and the development of Large Language Models (LLMs), such as ChatGPT , LLaMA 2  and Gemini .

LLMs exhibit emergent abilities, such as understanding linguistic nuances, making logical inferences, and decomposing tasks into simpler steps, which can be harnessed to scaffold students' learning in math-related tasks . However, while LLMs like GPT-4 and others can generate intermediate steps through approaches like Chain-of-Thought (CoT) prompting , this happens primarily at the prompting level and requires the user to have knowledge of how to effectively structure the thoughts. CoT prompting is highly dependent on precise prompt engineering; poorly designed or unclear prompts can result in irrelevant or inefficient reasoning steps. Additionally, CoT prompting can sometimes generate illogical chains of thought, exposing gaps in the reasoning process . Creating effective CoT prompts can be time-consuming and complex, particularly for advanced tasks.

In this paper, we propose a Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) framework incorporating a Large Language Model (LLM) to assist in solving MWPs. Our system first utilizes a schema classifier, trained on DistilBERT , to predict the appropriate schema \(S_{i}\) for a given problem \(P\). The identified schema is then used to generate a schema-specific prompt, which retrieves relevant context from a pre-defined document set. The retrieved context, schema, and problem are passed to an LLM (Ollama Llama 3.1), which generates a detailed, step-by-step solution.

Our findings suggest that the schema-guided RAG approach facilitates a more structured problem-solving process, which we believe will lead to improved reasoning and deeper student understanding of MWPs. This framework also forms a pathway for future work, where we can incorporate feedback from teachers and students to refine and adapt the system further, thereby improving reasoning and critical thinking skills. By leveraging this system in classroom settings, we can iteratively enhance its effectiveness as both a teaching and learning tool. In summary, our contributions include: a schema classifier trained to predict the relevant schema type and subcategory given a math word problem; structured prompt generation based on the predicted schema/subcategory that uses RAG to include schema-relevant content; a new evaluation metric (the step-by-step reasoning score) to evaluate the quality of the LLM's reasoning steps; and an LLM-as-a-judge evaluation .

## 2 Approach

As seen in Figure 1, our approach is divided into four main parts: 1) Schema Classifier, 2) Prompt Creation, 3) Context Retrieval, and 4) Answer and Response Generation. The training and dataset details are described in Appendix C and D, respectively.

A schema is a structured framework that represents a generalized method for solving a specific type of problem . In the context of MWPs, schemas help categorize problems based on their underlying structure, making it easier to determine which mathematical operations to use . For example, MWPs can often be grouped into two major schemas: Additive and Multiplicative .

Each schema can be further divided into sub-categories. For instance, the Additive schema can include Additive Change (where a value is increased or decreased), Additive Difference (problems that focus on the difference between two values), and Additive Total (where two or more quantities are combined to get a total) . Similarly, the Multiplicative schema can include Multiplicative Comparison (where one quantity is compared to another using multiplication), Multiplicative Equal Groups (where the total is divided into equal parts), and Multiplicative Ratios/Proportions (problems that involve finding ratios or proportional relationships) .

These schemas provide a structured framework for problem-solving, helping both the language model and learners identify the type of problem and apply the appropriate operations.

**Building a Schema Classifier:** We develop a schema classifier that performs supervised learning to predict the relevant schema (\(S_{i}\)) and sub-category (\(S_{ci}\)) for a given problem (\(P\)). This classifier is built using a DistilBERT model, which has been fine-tuned on a custom dataset of schema-based instruction problems.

Each problem in the dataset is labelled with its associated schema and sub-category, helping the classifier learn the relationships between different types of word problems and their corresponding schemas. Specifically, the input problem is tokenized and processed by the DistilBERT model, which then outputs the most suitable schema (\(S_{i}\)) and sub-category (\(S_{ci}\)).

The schema classifier is essential because it ensures that the correct problem-solving framework is applied to each problem. This step forms the foundation for schema-driven problem solving, guiding the language model to select the appropriate reasoning method.

**Prompt Creation:** Once the schema classifier has predicted the relevant schema (\(S_{i}\)) and sub-category (\(S_{ci}\)), the next stage is prompt creation. This involves generating a structured prompt that instructs the system on how to apply the identified schema to solve the problem. The generated prompt is formulated as follows:This prompt guides the language model by ensuring that the problem-solving approach adheres to the appropriate schema.

**Context Retrieval**: After the schema-specific prompt is created, relevant context is retrieved to support the problem-solving process. This retrieval is done using a Retrieval-Augmented Generation (RAG) framework , which combines document retrieval with prompt-based generation.

The generated prompt is embedded, and a cosine similarity search  is performed within a vector store to identify the most relevant documents . The vector store contains documents that serve as knowledge resources for the problem-solving process. These documents include explanations of various problem-solving strategies, examples of similar problems, and definitions of key mathematical concepts. For instance, a document might explain how to apply the Additive Change schema or provide sample problems involving ratios and proportions.

The document store is particularly useful because it supplies the model with additional knowledge that helps ensure the problem is solved in a structured, context-driven manner. The retrieved documents are ranked based on their similarity to the prompt, ensuring that the most relevant information is used to enhance the problem-solving process .

**Answer and Response Generation:** In the final stage, the retrieved context, problem, and schema-specific prompt are passed to the Llama 3.1 LLM  for generating the answer. The input is structured by combining the context, schema, and problem, allowing the model to produce a step-by-step solution that incorporates schema-driven reasoning. This ensures that each part of the problem-solving process is addressed in a structured manner, guiding the learner through the solution transparently. The response incorporates relevant contextual information, ensuring that the generated solution is both accurate and aligned with the instructional methodology. This schema-informed and context-enhanced process improves the transparency and effectiveness of the solution.

## 3 Evaluation

For evaluating the utility of our approach, we focus on the step-by-step reasoning provided by the generated responses, rather than solely on accuracy. Our goal is to ensure that the reasoning process is clear, logical and follows schema-driven methodologies, which helps improve understanding in solving MWPs . To address this, we introduce a new metric, the reasoning score, to measure the quality of the reasoning in the generated solutions. We also evaluate the performance of our schema classifier and analyze both the training and validation losses, ensuring that it generalizes well to unseen data. We also make use of the LLM-as-a-Judge approach  to get feedback and evaluate our response from LLMs like GPT-4 and GPT-3.5 Turbo. This approach is a scalable and explainable method for approximating human preferences , which are otherwise costly to obtain.

Figure 1: Illustration of SBI-RAG Architecture

All experiments were run using Google's Colab environment with an NVIDIA L4 GPU. More details on the evaluation and metrics used are given in Appendices D, E, F, and G.

**Schema Classifier Results:** The schema classifier was trained to identify two schema categories and three sub-categories: Additive Change, Additive Difference, Additive Total, Multiplicative Comparison, Multiplicative Equal Groups, and Multiplicative Ratios/Proportions. As seen in Figure 2 and Figure 3, it achieved high precision, recall, and F1 scores, with an overall accuracy of 97%. The training and validation losses show consistent convergence, indicating effective learning without overfitting, ensuring reliable schema predictions across various problem types.

**Reasoning Evaluation** We evaluated the reasoning quality by comparing responses generated using our Schema-Based RAG approach against responses from LLMs like GPT-4 and GPT-3.5 Turbo. The responses generated by our system, which incorporates schema-based reasoning, achieved higher scores in reasoning quality. Specifically, the best reasoning scores for SBI-RAG, GPT-4, and GPT-3.5 Turbo were 0.588, 0.491, and 0.290, respectively. Paired sample t-tests showed that the differences between the SBI-RAG and the GPT models were significantly different at the 0.05 level (see Appendix E). These results suggest that schema-based reasoning can enhance the overall quality of reasoning, particularly in educational contexts, when compared to responses generated by LLMs alone.

**LLM-as-a-Judge Results:** We implemented the LLM-as-a-Judge approach  to evaluate the quality of reasoning in the responses generated by both our Schema-Based RAG system and the baseline LLMs. This method allows for an objective, scalable evaluation by approximating human judgment through the use of LLMs. Our LLM-as-a-Judge process involves scoring responses based on clarity, logical progression, and completeness. Results showed that the Schema-Based RAG approach consistently outperformed GPT-4 and GPT-3.5 Turbo in terms of reasoning quality.For more details refer to Appendix G.

## 4 Conclusion

Despite the promising results of our Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) framework for improving math word problem reasoning, some limitations exist. This study relies on the LLM-as-a-Judge method, lacking direct human evaluation from educators or students, which would provide more informative feedback. The success of the RAG framework hinges on the relevance and quality of retrieved documents, which may vary and impact the generated solutions. The evaluation focuses on arithmetic word problems (GSM8K). More complex problem datasets are needed to assess the framework's generalizability. Finally, extending the framework to different subjects or educational levels may present challenges, requiring further adaptation. These limitations highlight areas for future research, particularly in improving schema coverage, expanding dataset diversity, and incorporating human evaluations.

In conclusion, we proposed a Schema-Based Retrieval-Augmented Generation framework that enhances reasoning and understanding in solving math word problems. Our approach, combining schema-based instruction with large language models, outperformed existing LLM responses in

Figure 3: Training and validation losses for the schema classifier

Figure 2: Confusion matrix for the schema classifier

quality and step-by-step reasoning. This framework provides a strong foundation for improving problem-solving in education, with future work focused on refining the system with user feedback. Additionally, this work could have applications in enhancing the reasoning capabilities of LLMs themselves.