# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

et al., 2020) or Transformer (Peebles and Xie, 2023)), different training datasets, and different input resolutions (e.g. 64, 256, 512). Through extensive experiments, we demonstrate that all the existing methods we tested (Liang and Wu, 2023; Zheng et al., 2023; Shan et al., 2023; Xue et al., 2023; Chen et al., 2024; Salman et al., 2023; Liang et al., 2023), targeting to attack LDMs, fail to generate effective adv-samples for PDMs. Moreover, we conduct adaptive attacks for PDMs, applying strategies like gradient averaging and attacking the intermediate features, where all attacks cannot effectively effect reverse diffusion process as fooling LDMs. This implies that PDMs are more adversarial robust than we think.

Building on this insight that PDMs are strongly robust against adversarial perturbations, we further propose PDM-Pure, a universal purifier that can effectively remove the protective perturbations of different scales (e.g. Mist-v2 (Zheng et al., 2023) and Glaze (Shan et al., 2023)) based on PDMs trained on large datasets. Through extensive experiments, we demonstrate that PDM-Pure achieves way better performance than all baseline methods.

To summarize, the pixel is a barrier to adversarial attack (Figure 1); the diffusion process in the pixel space makes PDMs much more robust than LDMs. This property of PDMs also makes real protection against the misusage of diffusion models difficult since: (1) no existing attacks have proven effective in attacking PDMs, which means no protection can be achieved by fooling a PDM, (2) all the existing protections against LDMs can be easily purified using a strong PDM. Our contributions are listed below.

1. We observe that most existing works on adversarial examples for protection focus on LDMs. Adversarial attacks against PDMs are **largely overlooked** in this field.
2. We fill in the gap in the literature by conducting extensive experiments on various LDMs and PDMs. We discover that all the existing methods **fail** to attack the PDMs, indicating that PDMs are much more adversarially robust than LDMs.
3. Based on this novel insight, we propose a simple yet effective framework termed PDM-Pure that applies strong PDMs as **a universal purifier** to remove attack-agnostic adversarial perturbations, easily bypassing almost all existing protective methods.

## 2 Related Works

Adversarial Examples for DMsAdversarial samples (Goodfellow et al., 2014; Carlini and Wagner, 2017; Shan et al., 2023) are clean samples perturbed by an imperceptible small noise that can fool the deep neural networks into making wrong decisions. Under the white-box settings, gradient-based methods are widely used to generate adv-samples. Among them, the projected gradient descent (PGD) algorithm (Madry et al., 2018) is one of the most effective methods. Recent works (Liang et al., 2023; Salman et al., 2023) show that it is also easy to find adv-samples for diffusion models (AdvDM) with a proper loss to attack the denoising process, the perturbed image can fool the diffusion models

Figure 1: Overview: (a) Recent protection approaches based on adversarial perturbation against latent diffusion models (LDMs) cannot be used in pixel-space diffusion models (PDMs); The underlying reason is that the encoder of the Latent Diffusion Model (LDM) amplifies the perturbations, causing the inputs to the denoiser to have significantly different distributions. In contrast, the inputs of the PDM maintain large overlap, showing robustness. (b) Strong PDM can be used as a universal purifier to effectively remove the protective perturbation generated by existing protection methods. (Best viewed with zoom-in on computer)

to generate chaotic images when operating diffusion-based mimicry. Furthermore, many improved algorithms (Zheng et al., 2023; Chen et al., 2024; Xue et al., 2023) have been proposed to generate better AdvDM samples. However, to our best knowledge, all the AdvDM methods listed above are used on LDMs, and those for the PDMs are rarely explored.

Adversarial Perturbation as ProtectionAdversarial perturbation against DMs turns out to be an effective method to safeguard images against unauthorized editing (Liang et al., 2023; Shan et al., 2023; Salman et al., 2023; Xue et al., 2023; Zheng et al., 2023; Chen et al., 2024; Ahn et al., 2024; Liu et al., 2023). It has found applications (e.g., Glaze (Shan et al., 2023) and Mist (Zheng et al., 2023; Liang and Wu, 2023)) for individual artists to protect their creations. SDS-attack (Xue et al., 2023) further investigates the mechanism behind the attack and proposes some tools to make the protection more effective. However, they are limited to protecting LDMs only. In addition, some works (Zhao et al., 2023; Sandoval-Segura et al., 2023) find that these protective perturbations can be purified. For instance, GrIDPure (Zhao et al., 2023) find that DiffPure (Nie et al., 2022) can be used to purify the adversarial patterns, but they did not realize that the reason behind this is the robustness of PDMs.

## 3 Preliminaries

Generative Diffusion ModelsThe generative diffusion model (Ho et al., 2020; Song et al., 2020) is one type of generative model, and it has demonstrated remarkable generative capability in numerous fields such as image (Rombach et al., 2022; Balaji et al., 2022), 3D (Poole et al., 2023; Lin et al., 2022), video (Ho et al., 2022; Singer et al., 2022), story (Pan et al., 2022; Rahman et al., 2023) and music (Mittal et al., 2021; Huang et al., 2023) generation. Diffusion models, like other generative models, are parametrized models \(p_{}(_{0})\) that can estimate an unknown distribution \(q(x_{0})\). For image generation tasks, \(q(x_{0})\) is the distribution of real images.

There are two processes involved in a diffusion model, a forward diffusion process and a reverse denoising process. The forward diffusion process progressively injects noise into the clean image, and the \(t\)-th step diffusion is formulated as \(q(x_{t} x_{n-1})=(x_{t};}x_{t-1},\,_{t} )\). Accumulating the noise, we have \(q_{t}(x_{t} x_{0})=(x_{t};}\,x_{t-1},\,(1- _{t}))\). Here \(_{t}\) growing from \(0\) to \(1\) are pre-defined values, \(_{t}=1-_{t}\), and \(_{t}=_{i=1}^{t}_{s}\). Finally, \(x_{T}\) will become approximately an isotropic Gaussian random variable when \(_{t} 0\).

Reversely, \(p_{}(_{t-1}|_{t})\) can generate samples from Gaussian \(_{T}(0,)\), where \(p_{}\) be re-parameterized by learning a noise estimator \(_{}\), the training loss is \(_{t,x_{0},}|(t)||_{}(x_{t},t)- ||^{2}]\) weighted by \((t)\), where \(\) is the noise used to diffuse \(x_{0}\) following \(q_{t}(x_{t}|x_{0})\). Finally, by iteratively applying \(p_{}(_{t-1}|_{t})\), we can sample realistic images following \(p_{}(_{0})\).

Figure 2: **PDMs Cannot be Attacked as LDMs**: LDMs can be easily fooled by running PGD to fool the denoising loss, but PDMs cannot be easily fooled. DiT (Peebles and Xie, 2023) and SD (Rombach et al., 2022) are LDMs, GD (Dhariwal and Nichol, 2021) AND IF-Stage-II (Shonenkov et al.) are PDMs (Best viewed with zoom-in)

Since the above diffusion process operates directly in the pixel space, we call such diffusion models Pixel-Space Diffusion Models (PDMs). Another popular choice is to move the diffusion process into the latent space to make it more scalable, resulting in the Latent Diffusion Models (LDMs) (Rombach et al., 2022). More specifically, LDMs first use an encoder \(_{}\) parameterized by \(\) to encode \(x_{0}\) into a latent variable \(z_{0}=_{}(x_{0})\). The denoising diffusion process is the same as PDMs. At the end of the denoising process, \(_{0}\) can be projected back to the pixel space using decoder \(_{}\) parameterized by \(\) as \(_{0}=_{}(_{0})\).

Adversarial Examples for Diffusion ModelsRecent works (Salman et al., 2023; Liang et al., 2023) find that adding small perturbations to clean images will make the diffusion models perform badly in noise prediction, and further generate chaotic results in tasks like image editing and customized generation. The adversarial perturbations for LDMs can be generated by optimizing the Monte-Carlo-based adversarial loss:

\[_{adv}(x)=_{t,e}_{z_{}_{t}(_{}(x))}\|_{}(z_{t},t)-\|_{2}^{2}.\] (1)

Other encoder-based losses (Shan et al., 2023; Liang and Wu, 2023; Zheng et al., 2023; Xue et al., 2023) further enhance the attack to make it more effective. With the carefully designed adversarial loss, we can run Projected Gradient Descent (PGD) (Madry et al., 2018) with \(_{}\) budget \(\) to generate adversarial perturbations:

\[x^{k+1}=_{B_{}(x^{0},)}[x^{k}+\, _{x^{k}}_{adv}(x^{k})]\] (2)

In the above equation, \(_{B_{}(x^{0},)}()\) is the projection operator on the \(_{}\) ball, where \(x^{0}\) is the clean image to be perturbed. We use superscript \(x^{k}\) to represent the iterations of the PGD and subscript \(x_{t}\) for the diffusion steps.

## 4 Rethink Adversarial Examples for Diffusion Models

### Diffusion Models Demonstrate Strong Adversarial Robustness

While there are many approaches that adopt adversarial perturbation to fool diffusion models, most of them focus only on latent diffusion models due to the wide impact of the Stable Diffusion; no attempts have been made to attack PDMs. This lack of investigation may mislead us to conclude that diffusion models, like most deep neural networks, are vulnerable to adversarial perturbations, and that the algorithms used in LDMs can be transferred to PDMs by simply applying the same adversarial loss in the pixel space formulated as: \(_{adv}(x)=_{t,e}_{x_{t} q_{t}(x)}\| _{}(x_{t},t)-\|_{2}^{2}\).

However, we show through experiments that PDMs are robust against this form of attack (Figure 2), which means all the existing attacks against diffusion models are, in fact, special cases of attacks against the LDMs only. We conduct extensive experiments on popular LDMs and PDMs structures including Diffusion Transformer (DiT), Guided Diffusion (GD), Stable Diffusion (SD), and Deep-Floyd (IF), and demonstrate in Table 2 that only the LDMs can be attacked and PDMs are not that susceptible to adversarial perturbations: for PDMs, the image quality does not significantly decrease due to the perturbation both visually and quantitatively. More details and analysis can be found in the experiment section.

Prior to this study, there may have been a prevailing belief that diffusion models could be easily deceived. However, our research reveals an important distinction: it is the LDMs that exhibit vulnerability, while the PDMs demonstrate significantly higher adversarial robustness.

### Adaptive Attacks for Pixel-space Diffusion Models

To further test the robustness of pixel-space diffusion models, we move forward by designing more adaptive attacks for PDMs. We adopt some design code from (Tramer et al., 2020) to craft adaptive attacks. We first divide the attacks into two categories (C1): attack the full pipeline, which is an end-to-end attack for the targeted editing pipeline. (C2): use diffusion loss as the objective, which follows Equation 1.

Then we try other tricks e.g. apply Expectation over Transformation (EOT) (Athalye et al., 2018), use targeted attack, and latent attack (attacking the intermediate layers). We collect the following attacks to test the robustness of Guided Diffusion (GD), including:

* Attack (1) / (2): (C1) with / without EoT
* Attack (3) / (4): (C2) with targeted / untargeted loss without EoT
* Attack (5) / (6): The above two attacks with EoT
* Attack (7) / (8): Latent attack / Latent attack+ in (Shih et al., 2024)

Attacks (1)-(6) are largely ineffective against PDMs, suggesting that end-to-end or Expectation over Transformation (EoT) attacks are unlikely to yield better results. As demonstrated in Figure 3, all crafted perturbations fail to induce chaotic generation outcomes in PDMs.

Recent work by (Shih et al., 2024) introduces latent attacks that can effectively deceive diffusion models. The core idea is to target the intermediate layers of the U-Net architecture in Guided Diffusion (GD). While this type of attack appears capable of misleading the PDM to edit the object as something different (see Figure 4), it suffers from two major limitations: The perturbation magnitude is excessively large, with \(_{}>150/255\). As a result, the appearance of the objects is significantly altered and further degraded by added Gaussian noise. Consequently, the diffusion model will to blind to correctly identify the object. For instance, as shown in the last block of Figure 4), when large Gaussian noise is introduced, the diffusion model mistakenly identifies the chicken as a turtle. Additionally, such latent attacks are ineffective when the editing strength is low, indicating that the attack mechanism heavily relies on the magnitude of noise applied. In contrast, attacks against Latent Diffusion Models (LDMs) can remain effective even with small perturbation steps, as they are capable of crafting strong adversarial attacks despite limited noise being added.

Figure 4: **Latent Attacks for PDMs**: (Shih et al., 2024) proposes to attack the intermediate feature of denoiser, and use a additional encoder-decode to regularize the perturbation. This kind of attack need large perturbation \(_{}>150/255\), and it barely work for small editing steps.

Figure 3: **Crafting Adaptive Attacks for PDMs**: PDM should robustness against end-to-end attacks and sampling based attacks, for EoT settings. We use the images in (Zheng et al., 2023) as the targeted image in the pixel space.

### Latent Diffusion Model is Vulnerable Because of the Encoder

The previous two sections demonstrate that PDMs exhibit significantly stronger empirical robustness compared to LDMs. Rather than providing a theoretical proof of the robustness of the diffusion process in pixel space (which is challenging to establish for DNN-based systems), we offer an intuitive explanation for why PDMs exhibit greater resilience.

The vulnerability of the LDMs is caused by the vulnerability of the latent space (Xue et al., 2023), meaning that although we may set budgets for perturbations in the pixel space, the perturbations in the latent space can be large. In (Xue et al., 2023), the authors show statistics of perturbations in the latent space over the perturbations in the pixel space and this value \(|}{|x-x^{}|}\) can be as large as \(10\), making the inputs into the denoiser (\(z_{t}=q_{t}(z),z_{t}^{}=q_{t}(z^{})\)) have smaller overlap (Figure 1 Middle). In contrast, the inputs into PDMs (\(x_{t}=q_{t}(x),x_{t}^{}=q_{t}(x^{})\)) will still have large overlap, since \(x\) and \(x^{}\) are close to each other due to the limited attack budget.

If we decompose the attacks on LDMs into two categories: (a) attacking the encoder and (b) attacking the diffusion model. We observe that the former is due to the encoder's adversarial vulnerability, while the latter results from a significant domain shift. Essentially, the input changes so drastically that it diverges from the distribution of the training environment, leading to reduced performance and robustness.

Almost all the copyright protection perturbations (Shan et al., 2023; Liang and Wu, 2023; Zheng et al., 2023) are based on the insight that it is easy to craft adversarial examples to fool the diffusion models. We need to rethink the adversarial samples of diffusion models since there are a lot of PDMs that cannot be attacked easily. Next, we show that PDMs can be utilized to purify all adversarial patterns generated by existing methods in Section 5. This new landscape poses new challenges to ensure the security and robustness of diffusion-based copyright protection techniques.

## 5 PDM-Pure: PDM as a Strong Universal Purifier

Since PDM is robust to adversarial perturbations, a natural idea emerges: we can utilize PDMs as a universal purification network. This approach could potentially eliminate any adversarial patterns without knowing the nature of the attacks. We term this framework **PDM-Pure**, which is a general framework to deal with all the perturbations nowadays. To fully harness the capabilities of PDM-Pure, we need to fulfill two basic requirements: (1) The perturbation shows out-of-distribution pattern as reflected in existing works on adversarial purification/attacks using diffusion models (Nie et al., 2022; Xue et al., 2024) (2) The PDM being used is strong enough to represent \(p(x_{0})\), which can be largely determined by the dataset they are trained on.

It is **effortless** to design a PDM-Pure. The key idea behind this method is to run SDEdit in the pixel space. Given any strong pixel-space diffusion model, we add a small noise to the protected images and run the denoising process (Figure 5), and then the adversarial pattern should be removed. The key idea of PDM-Pure is simple. In practice, we need to adjust the pipeline to fit the resolution of the PDMs being used.

In the main paper, we adopt DeepFloyd-IF (Shonenkov et al., 2020), the strongest pixel-space diffusion models nowadays as purifier. We conduct experiments on purifying protected images sized \(512 512\). For images with a larger resolution, purifying in the resolution of \(256 256\) may lose information. In

Figure 5: **PDM-Pure is Easy to Design: (a) PDM-Pure applies SDEdit Meng et al. (2021) in the pixel space: it first runs forward diffusion with a small step \(t^{*}\) and then runs denoising process. (b) We adapt the framework to DeepFloyd-IF Shonenkov et al., one of the strongest PDMs.**Appendix J we show PDM-Pure can also applied to purify patches of high-resolution inputs, removing widely used protections like Glaze on artworks. More details about the how we run DeepFloyd-IF as the purification pipeline are in the Appendix H.

## 6 Experiments

In this section, we conduct experiments on various attacking methods and various models to support the following two conclusions:

* **(C1)**: PDMs are much more adversarial robust than LDMs, and PDMs can not be effectively attacked using all the existing attacks for LDMs.
* **(C2)**: PDMs can be applied to effectively purify all of the existing protective perturbations. Our PDM-Pure based on DeepFloyd-IF shows state-of-the-art purification power.

details about the models and metrics used in this paper are in Section C in the Appendix.

### (C1) Diffusion Denoising Process is More Robust Than We Think

In Table 2, we attack different LDMs and PDMs with one of the most popular adversarial loss (Zheng et al., 2023) in Equation 1, which can be interpreted as fooling the denoiser using a Monte-Carlo-based loss. Given the attacked samples, we test the SDEdit results on the attacked samples, which can be generally used to test whether the samples are adversarial for the diffusion model or not. We use FID-score (Heusel et al., 2017), SSIM (Wang et al., 2004), LPIPS (Zhang et al., 2018), and IA-Score (Kumari et al., 2023) to measure the quality of the attack. If the quality of generated images decreases a lot compared with editing the clean images, then the attack is successful. We found that for all LDMs, attacks using adversarial loss successfully provide protection. However, for all PDMs, the adversarial attacks do not work. This phenomenon occurs across all scales of perturbation. For example, when, the FID of LDMs increased by over 100, while the FID of PDMs remained nearly unchanged. We also show some visualizations in Figure 2, which illustrates that the perturbation will affect the LDMs but not the PDMs.

To further investigate how robust PDM is, we test other advanced attacking methods, including the End-to-End Diffusion Attacks (E2E-Photoguard) proposed in (Salman et al., 2023) and the Improved Targeted Attack (ITA) proposed in (Zheng et al., 2023). Though the End-to-End attack is usually impractical to run, it shows the strongest performance to attack LDMs. We find that both attacks are not successful in PDM settings. We show attacked samples and edited samples in Figure 2, 3, 4 as well as the Appendix I. In conclusion, existing adversarial attack methods for diffusion models can only work for the LDMs, and PDMs are more robust than we think.

   Methods & AdvDM & AdvDM(-) & SDS(-) & SDS(+) & SDST & Photoguard & Mist & Mist-v2 \\  Before Protection & 166 & 166 & 166 & 166 & 166 & 166 & 166 & 166 \\ After Protection & 297 & 221 & 231 & 299 & 322 & 375 & 372 & 370 \\  Crop-Resize & 210 & 271 & 228 & 217 & 280 & 295 & 289 & 288 \\ JPEG & 296 & 222 & 229 & 297 & 320 & 359 & 351 & 348 \\ Adv-Clean & 243 & 201 & 204 & 244 & 243 & 266 & 282 & 270 \\ LDM-Pure & 300 & 251 & 235 & 300 & 350 & 385 & 380 & 375 \\ GrIDPure & 200 & 182 & 195 & 200 & 210 & 220 & 230 & 210 \\ PDM-Pure (ours) & **161** & **170** & **165** & **159** & **179** & **175** & **178** & **170** \\   

Table 1: **Quantitative Measurement of Different Purification Methods in Different Scale (FID-score)**: We compute the FID-score of editing purified images over the clean dataset. PDM-Pure is the strongest to remove all the tested protection, under strong protection with \(=16\). GrIDPure Zhao et al. (2023) can also do reasonable protection, but the performance is limited because the PDM they used is not strong enough.

### (C2) PDM-Pure: A Universal Purifier that is Simple yet Effective

PDM-Pure is simple: basically, we just run SDEdit to purify the protected image in the pixel space. Given our assumption that PDMs are quite robust, we can use PDMs trained on large-scale datasets as a universal black-box purifier. We follow the model pipeline introduced in Section 5 and purify images protected by various methods in Table 1.

PDM-Pure is effective: from Table 1 we can see that the purification will remove adversarial patterns for all the protection methods we tested, largely decreasing the FID score for the SDEdit task. Also, we test the protected images and purified images in more tasks including Image Inpainting (Song et al., 2020), Textual-Inversion (Gal et al., 2022), and LoRA customization (Hu et al., 2021). We show purification results fir inpainting in Figure 12, and purification results for LoRA in Figure 7. We show more results in Figure 16 in the appendix.

Both qualitative and quantitative results show that the purified images are no more adversarial and can be effectively edited or imitated in different tasks without any obstruction.

Also, PDM-Pure shows SOTA results compared with previous purification methods, including some simple purifiers based on compression and filtering like Adv-Clean, crop-and-resize, JPEG Compression, and SDEdit-based methods like GrIDPure (Zhao et al., 2023), which uses patchified SDEdit with a GD (Dhariwal and Nichol, 2021). We also add LDM-Pure as a baseline to show that LDMs can not be used to purify the protected images. For GrIDPure, we use Guided-Diffusion trained on ImageNet to run patchied purification. All the experiments are conducted on the datasets collected in (Xue et al., 2023) under the resolution of \(512 512\). Results for higher resolutions are presented in Appendix \(J\). We also test the ablation of timeteps used for PDM-Pure in Appendix Appendix K, from which we can see \(t*\) around \(0.15\) works well.We also find that PDM-Pure works better for cartoon pictures with larger plain color patches. For pictures with high details like oil paintings, it will lose some detail; however, generally the art style can still be well learned by LoRA from the attacker's perspective (e.g. Claude Monet-style in Appendix Figure **?**).

## 7 Conclusions and Future Directions

In this paper, we present novel insights that while many studies demonstrate the ease of finding adversarial samples for Latent Diffusion Models (LDMs), Pixel Diffusion Models (PDMs) exhibit far greater adversarial robustness than previously assumed. We are the first to investigate the adversarial samples for PDMs, revealing a surprising discovery that existing attacks fail to fool PDMs. Leveraging this insight, we propose utilizing strong PDMs as universal purifiers, resulting in PDM-Pure, a simple yet effective framework that can generate protective perturbations in a black-box manner.

Pixel is a barrier for us to do real protection against adversarial attacks. Since PDMs are quite robust, they cannot be easily attacked. PDMs can even be used to purify the protective perturbations, challenging the current assumption for the safe protection of generative diffusion models. We advocate rethinking the problem of adversarial samples for generative diffusion models and unauthorized image protection based on it. More rigorous studies need to be conducted to better understand the mechanism behind the robustness of PDMs. Furthermore, we can utilize it as a new structure for many other tasks

Figure 6: **PDM-Pure makes the Protected Images no more Protected**: PDM can help effectively remove adversarial pattern to bypass the protection for LDMs, here we show example on in-painting with SDS protection proposed in (Xue et al., 2023). We put more results on more attacks and more examples in the Appendix Figure 16.