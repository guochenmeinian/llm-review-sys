# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

process and instead only have access to a set of prior observations of inputs and associated objective values; this problem can often be referred to as offline _model-based optimization_ (**MBO**) (Trabucco et al., 2021; Mashkaria et al., 2023). While one may naively attempt to learn a surrogate black-box model from the prior observations that approximates the true oracle objective function, such models can suffer from overestimation errors, yielding falsely promising objective estimates for inputs not contained in the offline dataset. As a result, offline optimization against the surrogate objective may yield low-scoring candidate designs according to the true oracle objective function--a key limitation of traditional policy optimization techniques in the offline setting (**Fig. 1**).

In this work, we propose a novel offline MBO algorithm that leverages source critic models to optimize a surrogate objective while simultaneously remaining in-distribution when compared against a reference offline dataset. In this setting, an optimizer is rewarded for proposing optima that are "similar" to reference data points, thereby minimizing overestimation error and allowing for more robust oracle function optimization in the offline setting. Inspired by recent work on generative adversarial networks (Goodfellow et al., 2014), we quantify design similarity by proposing a novel method that regularizes a surrogate objective model using a source critic actor, which we call _adaptive source critic regularization_ (**aSCR**). We show how our algorithm can be readily leveraged with optimization methods such as Bayesian optimization (BO) and first-order methods.

**Contributions:** Our contributions are as follows: (1) We propose a novel approach for MBO that formulates the task as a constrained primal optimization problem, and we show how this framework can be used to solve for the optimal tradeoff between naively optimizing against the surrogate model and staying in-distribution relative to the offline dataset. (2) We introduce a computationally tractable method--which we call adaptive source critic regularization (aSCR)--to implement this framework with two popular optimization methods: Bayesian optimization and gradient ascent. (3) We show that compared to prior methods, our proposed algorithm with Bayesian optimization empirically achieves the highest rank of **3.8** (second best is 5.5) on top-1 design evaluation, and highest rank of **3.0** (second best is 4.6) on top-128 design evaluation across a variety of tasks spanning multiple scientific domains.

## 2 Related Work

Leveraging source critic model feedback for adversarial training of neural networks was popularized by works such as Goodfellow et al. (2014), where a generator and adversarial discriminator "play" a zero-sum minimax game to train a generative model. However, such discriminators often suffer from mode collapse and training instability in practice. To overcome these limitations, Arjovsky et al. (2017) introduced the Wasserstein generative adversarial net (WGAN), which instead utilizes a source critic that learns to approximate the 1-Wasserstein distance between the generated and training distributions. However, WGANs and similar networks primarily aim to generate samples that look in-distribution from a latent space prior, rather than optimize against an objective function. In our work, we adapt WGAN-inspired source critic models for Wasserstein distance estimation.

Separately in the field of optimization, Brookes et al. (2019) introduced a method for conditioning by adaptive sampling (CbAS) that learns a density model of the input space that is gradually adapted towards the optimal solution. However, such prior works have focused on solving low-dimensional, online optimization tasks (Hansen and Ostermeier, 1996; Brookes et al., 2019). More recently, Trabucco et al. (2021) introduced conservative objective models (**COM**) specifically for _offline_ optimization tasks; however, their method requires directly modifying the parameters of the surrogate function

Figure 1: Naive offline model-based optimization (MBO) (Trabucco et al., 2021), which optimizes against a learned surrogate model \(f_{}\) trained on a fixed dataset \(_{n}=\{(_{i},y_{i})\}_{i=1}^{n}\) (shaded region) without access to the true oracle \(f\), often yields candidate designs \(^{*}\) (i.e., diamond) that score poorly using the true oracle (i.e., cross). Our method (aSCR) constrains optimization trajectories to avoid these extrapolated points, instead proposing ‘in-distribution’ designs (i.e., star).

during optimization, which is not always feasible for any general task. Mashkaria et al. (2023) proposed Black-box Optimization Networks (**BONET**) to learn the dynamics of optimization trajectories using a causally masked transformer model, and Krishnamoorthy et al. (2023) introduced Denoising Diffusion Optimization Models (**DDOM**) to learn the generative process via a diffusion model. Furthermore, Yu et al. (2021) and Chen et al. (2022) describe Robust Model Adaptation (**RoMA**) and Bidirectional learning via infinite-width networks (**BDI**), respectively. RoMA regularizes the gradient of surrogate objective models by enforcing a local smoothness prior at the observed inputs, and BDI learns bidirectional mappings between low- and high- scoring candidates. Finally, Nguyen et al. (2023) introduce Experiment Pretrained Transformers (**ExPT**) to learn a general model for optimization using unsupervised methods. While these recent works and others propose promising algorithms for offline optimization tasks, they are often evaluated using expensive oracle query budgets that are often not achievable in practice--especially for potentially dangerous tasks such as patient care and other high-stakes applications.

## 3 Background

### Offline Model-Based Optimization

In many real-world domains, we often seek to optimize an _oracle_ objective function \(f()\) over a space of design candidates \(\) to solve for \(^{*}=_{}f()\). Examples of such problems include optimizing certain desirable properties of molecules in molecular design (Guimaraes et al., 2017; Brown et al., 2019; Maus et al., 2022), and estimating the optimal therapeutic intervention for patient care in clinical medicine (Kim and Bastani, 2021; Berrevoets et al., 2022; Xu and Bastani, 2023). In practice, however, the true objective function \(f\) may be costly to compute or even entirely unknown, making it difficult to query in optimizing \(f()\). Instead, it is often more feasible to obtain access to a reference labeled dataset of observations from nature \(_{n}=\{(_{1},y_{1}),,(_{n},y_{n})\}\) where \(y_{i}=f(_{i})\). Optimization methods may use a variety of different strategies to leverage \(_{n}\) in the offline setting (Mashkaria et al., 2023; Krishnamoorthy et al., 2023; Chen et al., 2022); one common approach used by Trabucco et al. (2021) and others is to learn a regressor model \(f_{}\) parametrized by

\[^{*}=_{}\ _{(_{i},y_{i}) _{n}}||f_{}(_{i})-y_{i}||^{2}\] (1)

as a _surrogate model_ for the true oracle objective \(f()\). Rather than querying the oracle \(f\) as in the online setting, we can instead solve the related optimization problem

\[^{*}=_{}f_{}()\] (2)

with the hope that optimizing \(f_{}\) will also lead to desirable oracle values of \(f\) as well. Solving (2) is one instantiation of offline **model-based optimization (MBO)** for which a number of techniques have been developed, such as gradient ascent and Bayesian optimization (**BO**) (Sun et al., 2020).

Of note, it is difficult to guarantee the reliability of the model's predictions for \(_{n}\) that are almost certainly encountered in the optimization trajectory. Thus, naively optimizing the surrogate objective \(f_{}\) can result in "optima" that are low-scoring according to the oracle objective \(f\).

### Optimization Over Latent Spaces

In certain cases, the search space \(\) for an optimization task may be discretized over a finite set of structured inputs, such as amino acids for protein sequences or atomic building blocks for molecules. However, many historical optimization algorithms do not generalize well to these settings for a number of different reasons, such as the lack of gradients with respect to the input designs to guide the optimization trajectory. Instead of directly optimizing over \(\), recent work leverages deep variational autoencoders (VAEs) to first map the input space into a continuous, (often) lower dimensional latent space \(\) and then performing optimization over \(\) instead (Tripp et al., 2020; Deshwal and Doppa, 2021; Maus et al., 2022). A VAE is composed of a two components: (1) an encoder with parameters \(\) that learns an approximated posterior distribution \(q_{}(z|)\) for \(,z\); and (2) a decoder with parameters \(\) that learns the conditional likelihood distribution \(p_{}(|z)\)(Kingma and Welling, 2013). The encoder and decoder are co-trained to maximize the evidence lower bound (ELBO)

\[=_{z q_{}}[ p_{}(|z) ]-D_{}[q_{}(z|)\ ||\ p_{}(z)]\] (3)

where \(D_{}\) is the Kullback-Leibler (KL) divergence and \(p_{}(z)\) is the prior distribution. A common choice is to set \(p_{}=(0,I)\) (i.e., the standard normal distribution). Optimization can then be performed over the continuous _latent space_\(\) of the VAE to propose 'latent space designs' that can be readily decoded using the decoder \(\) back into the original input space.

One such optimization method over VAE latent spaces is **Bayesian optimization (BO)**, a sample-efficient framework for solving expensive black-box optimization problems (Mockus, 1982; Osborne et al., 2009; Snoek et al., 2012). While the utility of BO has primarily been explored for expensive-to-evaluate black-box functions in prior literature, recent work has shown that BO also outperforms baseline optimization methods in offline tasks involving models that are relatively inexpensive to evaluate, such as the neural network surrogates used in model-based optimization (MBO). Multiple prior works have shown that BO and related methods consistently outperform both first-order gradient-based and stochastic evolutionary methods (Eriksson et al., 2019; Maus et al., 2022; Hvarfner et al., 2024; Eriksson and Jankowiak, 2021; Astudillo and Frazier, 2019).

### Wasserstein Metric Between Probability Distributions

The Wasserstein distance is a distance metric between any two probability distributions, and is closely related to problems in optimal transport. We define the \(p=1\) Wasserstein distance between a reference distribution \(P\) and a generated distribution \(Q\) using distance metric \(d(,)\) as

\[W_{1}(P,Q)=_{(P,Q)}_{(z^{},z)}d( z^{},z)\] (4)

where \(\) is the set of all couplings between \(P\) and \(Q\). For empirical distributions where \(p_{n}\) (\(q_{n}\)) is based on \(n\) observations \(\{z^{}_{j}\}_{j=1}^{n}\) (\(\{z_{i}\}_{i=1}^{n}\)), (4) can be simplified to

\[W_{1}(p_{n},q_{n})=_{}_{i=1}^{n}||z^{}_{( i)}-z_{i}||\] (5)

where the infimum is over all permutations \(\) of \(n\) elements. Leveraging the Kantorovich-Rubinstein duality theorem (Kantorovich and Rubinstein, 1958), (5) can be equivalently written as

\[W_{1}(p_{n},q_{n})=_{||c||_{L} K}[_{z^{ } P}[c(z^{})]-_{z Q}[c(z)]]\] (6)

where \(c(z)\) is a _source critic_ and \(||c||_{L}\) is the Lipschitz norm of \(c(z)\). In the Wasserstein GAN (WGAN) model proposed by Arjovsky et al. (2017), a generative network and source critic are co-trained in a minimax game where the generator (critic) seeks to minimize (maximize) the Wasserstein distance \(W_{1}\) between the training and generated distributions. Such an optimization schema enables the generator policy to learn the distribution of training samples from nature.

## 4 A Framework for Generative Adversarial Optimization

In this section we describe our proposed framework for generative adversarial model-based optimization using **adaptive source critic regularization (aSCR)**. Our method uses a \(K\)-Lipschitz source critic model to dynamically regularize the optimization objective to avoid extrapolation against the proxy surrogate model \(f_{}\) in offline MBO.

### Constrained Optimization Formulation

In offline generative optimization, we aim to optimize against a surrogate objective function \(f_{}\). In order to ensure that we are achieving reliable estimates of the true, unknown oracle objective, we can add a regularization penalty to keep generated samples "similar" to those from the training dataset of \(f_{}\) according to an adversarial source critic trained to discriminate between generated and offline samples. That is, in contrast to (2), aSCR instead considers a closely related _constrained_ problem

\[_{z}&-f_{ }(z)\\ &_{z^{} P}[c^{*}(z^{ })]-c^{*}(z) 0\] (7)

over some configuration space \(^{d}\), and where we define \(c^{*}\) as a source critic model that maximizes \(_{z^{} P}[c^{*}(z^{})]-_{z Q}[c^{*}(z)]\) over all \(K\)-Lipschitz functions as in (6). We can think of \(_{z^{} P}[c^{*}(z^{})]-c^{*}(z)\) as the contribution of a particular generated datum \(z\) to the overall \(p=1\)Wasserstein distance between the generated candidate (\(Q\)) and reference (\(P\)) distributions of designs as in (6). In practice, we model \(c^{*}\) as a fully connected neural net. Intuitively, the imposed constraint restricts the feasible search space to designs that score at least as in-distribution as the average sample in the offline dataset according to the source critic. Therefore, \(c^{*}\) acts as an adversarial model to regularize the optimization policy. Of note, this additional constraint in (7) may be highly non-convex for general \(c^{*}\), and so it is often impractical to directly apply (7) to any arbitrary MBO policy.

### Dual Formulation

To solve this implementation problem, we instead look to reformulate (7) in its dual space by first considering the Lagrangian \(\) of our constrained problem:

\[(z;)=-f_{}(z)+[_{z^{ } P}[c^{*}(z^{})]-c^{*}(z)]\] (8)

where \( 0\) is the Lagrange multiplier associated with the constraint in (7). We can equivalently think of \(\) as a hyperparameter that controls the relative strength of the source critic-penalty term: \(=0\) equates to naively optimizing the surrogate objective, while \( 1\) asymptotically approaches a WGAN-like optimization policy. Minimizing \(\) thus minimizes a relative sum of \(-f_{}\) and the Wasserstein distance contribution from any particular generated datum \(z\) with relative weighting dictated by the hyperparameter \(\). From duality, minimizing \(\) over \(z\) and simultaneously maximizing over \(_{+}\) is equivalent to the original constrained problem in (7).

The challenge now is in determining this optimal value of \(\): if \(\) is too small, then the objective estimates may be unreliable; if \(\) is too large, then the optimization trajectory may be unable to adequately explore the input space. Prior work by Trabucco et al. (2021) has previously explored the idea of formulating offline optimization problems as a similarly regularized Lagrangian (albeit with a separate regularization constraint), although their method tunes a static hyperparameter by hand. In contrast, aSCR treats \(\) as a dynamic parameter that adapts to the optimization trajectory in real time.

### Computing the Lagrange Multiplier \(\)

Continuing with our dual formulation of (7), the Lagrange dual function \(g()\) is defined as \(g()=_{z^{n}}(z;)\). The \(z=\) that minimizes the Lagrangian in the definition of \(g\) is evidently a function of \(\). To show this, we use the first-order condition that \(_{z}=0\) at \(z=\). Per (8), we have

\[_{z}(;)=-_{z}f_{}()- _{z}c^{*}()=0\] (9)

In general, solving (9) for \(\) is computationally intractable--especially in high-dimensional problems. Instead, we can approximate \(\) by relaxing the condition in (9) according to

\[()=*{argmin}_{z^{n}}|| -_{z}f_{}(z)-_{z}c^{*}(z)||^{2}\] (10)

Our key insight is that although minimizing the loss term in (10) is not practical when the feasible set is naively uniform over \(^{n}\), we can instead choose to focus our attention on latent space coordinates with high associated probability according to the VAE prior distribution \(p_{}(z)\). This is because in optimization problems acting over the latent space of any variational autoencoder, the majority of the encoded information content is embedded according to \(p_{}(z)\) due to the Kullback-Leibler (KL) divergence contribution to VAE training. Put simply, the encoder distribution \(q_{}(z|)\) is trained so that \(D_{}[q_{}(z|)||p_{}(z))]\) is optimized as a regularization term in (3). We argue that it is thus sufficient enough to approximate \(()\) using a Monte Carlo sampling schema with random samples \(_{N}=(z_{1},z_{2},,z_{N}) p_{}(z)\):

\[()*{argmin}_{_{N} p_{}(z)}||-_{z}f_{}(z)-_{z}c^{*}(z) ||^{2}.\] (11)

We can now concretely write an approximation of the Lagrange dual problem of (7):

\[*{maximize} g()=-f_{}()+[_{z^{ } P}[c^{*}(z^{})]-c^{*}()]\] (12) subject to \[ 0\]

where \(\) is as in (11). Defining the surrogate variable \(\) such that \(=\), we can rewrite (12) as

\[*{maximize} -(1-)f_{}()+[_{z^{ } P}[c^{*}(z^{})]-c^{*}()]\] (13) subject to \[0<1\]In practice, we discretize the search space for \(\) to 200 evenly spaced points between \(0\) and \(1\) inclusive. From weak duality, finding the optimal solution to (12) provides a lower bound on the optimal solution to the primal problem in (7). **Algorithm 1** can now be used to choose the optimal \(\) (and hence \(\)) adaptively during offline optimization: we refer to our method as **Adaptive SCR (aSCR)**.

### Overall Algorithm

Using Adaptive SCR, we now have a proposed method for dynamically computing \(\) (and hence the Lagrange multiplier \(\)) of the constrained optimization problem in (7). Importantly, aSCR can be integrated with any standard function optimization method by optimizing the Lagrangian objective in (8) over the candidate design space as opposed to the original unconstrained objective \(f_{}\). We refer to this algorithm as _Generative Adversarial Model-Based Optimization_ (GAMBO). To evaluate aSCR empirically, we instantiate two flavors of GAMBO: (1) **G**enerative **A**dversarial **B**ayesian **O**ptimization (**GABO**, **Algorithm 2**); and (2) **G**enerative **A**dversarial **G**radient **A**scent (**GAGA**).2

We implement GABO using a quasi-expected improvement (qEI) acquisition function, iterative sampling budget of \(T=32\), sampling batch size of \(b=64\), and GAGA using a step size of \(=0.05\), \(T=128\), and \(b=16\). Of note, the optimization objective using aSCR is time-varying and causally linked to past observations made during the optimization process via intermittent training of the source critic \(c\). Prior works from Nyikosa et al. (2018) and Aglietti et al. (2022) have examined optimization against dynamic objective functions, although have either entirely disregarded causal relationships between variables or only examined causality between inputs as opposed to inputs and the objective. We leave such methods for future work given that aSCR works well in practice.

## 5 Experimental Evaluation

### Datasets and Tasks

To evaluate our proposed algorithm, we focus on a set of eight tasks spanning multiple domains with publicly available datasets in the field of offline model-based optimization. (1) The **Branin** function is a well-known synthetic benchmark function where the task is to maximize the two-dimensional Branin function \(f_{br}:[-5,10]\). (2) The **LogP** task is a well-studied optimization problem (Zhou et al., 2019; Chen et al., 2021; Flam-Shepherd et al., 2022) where we search over candidate molecules to maximize the penalized water-octanol partition coefficient (logP) score, which is an approximate measure of a molecule's hydrophobicity (Ertl and Schuffenhauer, 2009) that also rewards structures that can be synthesized easily and feature minimal ring structures. We use the publicly available Guacamol benchmarking dataset from Brown et al. (2019) to implement this task.

Tasks (3) - (7) are derived from Design-Bench, a publicly available set of MBO benchmarking tasks (Trabucco et al., 2022): (3) **TF-Bind-8** aims to maximize the transcription factor binding efficiency of an 8-base-pair DNA sequence (Barrera et al., 2016); (4) **GFP** the green fluorescence of a 237-amino-acid protein sequence (Brookes et al., 2019; Rao et al., 2019); (5) **UTR** the gene expression from a 50-base-pair 5'UTR DNA sequence (Sample et al., 2019; Angermueller et al., 2020); (6) **ChEMBL** the mean corpuscular hemoglobin concentration (MCHC) biological response of a molecule using an offline dataset collected from the ChEMBL assay \(\)(Gaulton et al., 2012); and (7) **D'Kitty** the morphological structure of the D'Kitty robot (Ahn et al., 2020).

Finally, (8) the **Warfarin** task uses the dataset of patients on warfarin medication from Consortium (2009) to estimate the optimal dose of warfarin given clinical and pharmacogenetic patient data. Of note, in contrast to tasks (1) - (7) and other traditional MBO tasks in prior literature (Trabucco et al., 2022), the Warfarin task is novel in that only a subset of the input design dimensions may be optimized over (i.e., warfarin dose) while the others remain fixed as conditioning variables (i.e., patient covariates). Such a task can therefore be thought of as _conditional_ model-based optimization.

### Policy Optimization and Evaluation

For all experiments, the surrogate objective model \(f_{}\) is a fully connected net with two hidden layers of size 2048 and LeakyReLU activations. \(f_{}\) takes as input a VAE-encoded latent space datum and returns the predicted objective function value as output. The VAE encoder and decoder backbone architectures vary by MBO task and are detailed in **Supplementary Table A1**. Following Gomez-Bombarelli et al. (2018) and Maus et al. (2022), we co-train the VAE and surrogate objective models together using an Adam optimizer with a learning rate of \(3 10^{-4}\) for all tasks. For the optimization tasks over continuous design spaces (i.e., Branin, Warfarin, and D'Kitty), we fix the VAE encoder and decoders as the identity functions, such that the latent and input spaces are equivalent.

The source critic agent \(c\) in (7) is implemented as a fully connected net with two hidden layers with sizes equal to four (one) times the number of input dimensions for the first (second) layer. To constrain the Lipschitz norm of \(c\) as in (6), we clamp the weights of the model between [-0.01, 0.01] after each optimization step as done by Arjovsky et al. (2017). The model is trained using gradient descent with a learning rate of 0.001 to maximize the Wasserstein distance between the dataset and generated candidates in the VAE latent space.

During optimization, both GABO and GAGA alternate between sampling new designs and training the source critic actor \(c(z)\) until there is no improvement to the Wasserstein distance \(W_{1}\) according to \(c\) after 100 consecutive weight updates. We find that training \(c\) every \(n_{}=4\) sampling steps is a good choice across all tasks assessed, similar to prior work Arjovsky et al. (2017).

All MBO methods were evaluated using a fixed surrogate query budget of 2048. We focus on two evaluation metrics: 100th percentile (1) top \(k=1\); and (2) top \(k=128\) oracle score. The top \(k=128\) evaluation metric is commonly reported in prior offline MBO literature (Mashkaria et al., 2023; Trabucco et al., 2021; Yu et al., 2021); the top \(k=1\) metric better accounts for the limited oracle query budget of the real-world tasks in which offline MBO would be of use. In both settings, an optimizer selects the top \(k\) design that minimize the Lagrangian function value in (8) from the 2048 assessed designs to evaluate using the true oracle function, and the maximum score of those \(k\) designs is reported across 10 random seeds.

We evaluate both GABO and GAGA against a number of pre-existing baseline algorithms on one internal cluster with 8 NVIDIA RTX A6000 GPUs. We include vanilla Bayesian Optimization (**BO**-qEI) and gradient ascent (**Grad.)** in our evaluation to assess the utility of our proposed aSCR algorithm. Furthermore, we evaluate limited-memory BFGS (**L-BFGS**) Liu & Nocedal (1989), **CMA-ES** Hansen & Ostermeier (1996), and simulated annealing (**Anneal**) Kirkpatrick et al. (1983). We also compare our method against the **TukBO**-qEI (Eriksson et al., 2019), **COM**(Trabucco et al.,2021), **RoMA**(Yu et al., 2021), **BDI**(Chen et al., 2022), **DDOM**(Krishnamoorthy et al., 2023), **BONET**(Mashkaria et al., 2023), **ExPT**(Nguyen et al., 2023), **ROMO**(Chen et al., 2023), and **BootGen**(Kim et al., 2023). Because BootGen is proposed by Kim et al. (2023) as an optimization method specifically for biological sequence design, we only assess this baseline method on the five relevant tasks in our evaluation suite.

**Conditional MBO Tasks.** To our knowledge, prior work in conditional model-based optimization is limited, and so previously reported algorithms are not equipped to solve such tasks out-of-the-box. Chen et al. (2023) explore such tasks in their work, but primarily focus on conditional tasks that are built by arbitrarily fixing certain design dimensions from unconstrained problems, which are not representative of true conditional optimization problems in the real world. In our work, we introduce the Warfarin task to assess methods on their ability to design an optimal therapeutic drug regiment _conditioned_ on a fixed patient state and lab values. To assess existing methods on this task, we implement conditional proxies of all baselines employing a first-order optimization schema via _partial_ gradient ascent to only update the warfarin dose dimension while leaving the patient attribute conditional dimensions unchanged. Conditional BO-based methods are implemented by fitting separate Gaussian processes for each patient. In conditional DDOOM, we exchange the algorithm's diffusion model-based backbone with a _conditional_ score-based diffusion model (Gu et al., 2023).

Of note, the BONET algorithm (Mashkaria et al., 2023) requires multiple observations for any given patient to construct synthetic optimization trajectories. However, the key challenge in conditional MBO is that each condition (i.e., patient) has _no_ past observations (i.e., warfarin doses), and instead relies on learning from offline datasets constructed from different permutations of condition values As a result, the BONET algorithm is unable to be evaluated on conditional MBO tasks.

### Main Results

Scoring of one-shot optimization candidates is shown in **Table 1**. Across all eight assessed tasks spanning a wide range of scientific domains, GABO with our aSCR algorithm achieved the best average rank of **3.8** when compared to other existing methods (next best is 5.5). Furthermore, GABO was able to propose top \(k=1\) candidate designs that outperform the best design in the pre-existing offline dataset for 6 of the 8 tasks-greater than any of the other methods assessed. If a larger oracle evaluation budget is available (i.e., \(k=128\)), GABO with aSCR performs even better, achieving the best average rank of **3.0** (next best is 4.6). GABO is also the best algorithm on 3 of the 8 tasks and second best on 2 tasks according to this evaluation metric. Altogether, our results suggest that GABO is a promising method for proposing optimal design candidates in offline MBO.

Importantly, our aSCR algorithm improves upon both the naive BO-qEI and Grad. Ascent parent optimizers assessed. GABO outperforms both baseline BO-based optimization methods in our evaluation suite: BO (TuRBO) only achieves a rank of 8.8 (9.0) on the top \(k=1\) evaluation metric and a rank of 6.6 (7.4) on the top \(k=128\) metric. Similarly, MAGA scores an average rank of 7.4 (7.6) on the top \(k=1\) (\(k=128\)) evaluation metric; by leveraging aSCR, GGA outperforms its base parent optimizer (Grad. Ascent), which only achieves an average rank of 9.0 and 11.0 on the same two evaluation metrics, respectively. Our results show that using aSCR to adaptively penalize the objective of two popular optimization methods can improve their offline performance.

**Qualitative Evaluation: Penalized LogP Task.** We evaluate GABO against naive BO-qEI for the **LogP** task by inspecting the three-dimensional chemical structures of the top-scoring candidate molecules. As a general principle, molecules that are associated with high Penalized LogP scores are hydrophobic with minimal ring structures and therefore often feature long hydrocarbon backbones (Ertl and Schuffenhauer, 2009). In **Figure 2**, we see that BO-qEI using the unconstrained surrogate objective generates a candidate molecule of hydrogen and carbon atoms. However, the proposed candidate includes two rings in its structure, resulting in a suboptimal oracle Penalized LogP score.

We hypothesize that this may be due to a lack of ring-containing example molecules in the offline dataset, as only 6.7% (2.7%) of observed molecules contain at least one (two) carbon ring(s). As a result, the surrogate objective model estimator returns more inaccurate Penalized LogP estimates for input ring-containing structures (surrogate model root mean squared error (RMSE) = 25.5 for offline dataset molecules with at least 2 rings; RMSE = 16.5 for those with at least 1 ring; and RMSE = 4.6 for those with at least 0 rings), leading to sub-par BO-qEI optimization performance as the unconstrained algorithm extrapolates against the surrogate to find "optimal" molecules that 

[MISSING_PAGE_FAIL:9]

Of note, the top designs found across different constant values of \(\) can be very similar for certain tasks. This reflects the inherent challenge in developing task-agnostic methods for policy regularization--if the magnitudes of the unconstrained objective and regularization function vastly differ, then constant values of \(\) may over- or under- constrain the objective. Adaptive SCR overcomes this problem by dynamically setting \(\) as an implicit function of prior observations.

## 6 Conclusion

We propose **adaptive source critic regularization (aSCR)** to solve the problem of off-distribution objective evaluation in offline MBO. When leveraged with vanilla Bayesian optimization, aSCR outperforms baseline methods to achieve an average rank of **3.8** (**3.0**) in one-shot \(k=1\) (few-shot \(k=128\)) oracle evaluation, and most consistently proposes designs better than the offline dataset.

**Limitations.** One limitation of aSCR is that our algorithm requires preexisting knowledge of the prior distribution over the input space in order to be computationally tractable. While we have focused our experimental evaluation on tasks amenable to imposed latent space priors, further work is needed to adapt aSCR to any arbitrary configuration space. Future work may also extend aSCR to improve parent optimization methods more sophisticated than BO-qEI and Gradient Ascent explored herein.

**Impact Statement.** Offline policy optimization methods, such as those discussed in this work, have the potential to benefit society. Such examples may include helping develop more effective drugs and individualizing patient therapies. However, as with any real-world algorithm, these methods can also be leveraged to generate potentially harmful design candidates. Careful oversight by domain experts and researchers is required to ensure that the contributions proposed herein are used for social good.

  
**Top-12** & **Branin** & **LogP** & **TF-Bind-8*** & **GFP*** & **UTR*** & **ChEMBL*** & **D’Kitty** & **Warfarin** & **Rank** \\  \(D\) (best) & -13.0 & 11.3 & 0.439 & 3.53 & 7.12 & 0.61 & 0.88 & -0.19 \(\) 1.96 & — \\  \(=0.0\) & -**0.4 \(\) 0.0** & 135.3 \(\) 16.0 & 0.942 \(\) 0.025 & 2.26 \(\) 103 & 8.26 \(\) 0.09 & 0.67 \(\) 0.00 & 0.72 \(\) 0.00 & 0.93 \(\) 0.11 & 4.3 \\ \(=0.2\) & **-0.4 \(\) 0.1** & 1218 \(\) 20.6 & 0.952 \(\) 0.09 & 3.01 \(\) 104 & 8.20 \(\) 0.10 & 0.67 \(\) 0.01 & 0.72 \(\) 0.00 & **1.00 \(\) 0.00** & 4.8 \\ \(=0.5\) & **-0.4 \(\) 0.0** & 1277 \(\) 23.1 & 0.944 \(\) 0.040 & 3.49 \(\) 0.09 & 8.29 \(\) 0.08 & 0.67 \(\) 0.01 & 0.72 \(\) 0.00 & **1.00 \(\) 0.00** & 2.9 \\ \(=0.8\) & **-0.4 \(\) 0.0** & 1045 \(\) 31.8 & 0.953 \(\) 0.036 & **3.74 \(\) 0.00** & 8.38 \(\) 0.11 & 0.67 \(\) 0.02 & 0.72 \(\) 0.00 & **1.00 \(\) 0.00** & 3.4 \\ \(=1.0\) & -2.2 \(\) 1.4 & **1423 \(\) 2.41** & 0.906 \(\) 0.061 & **3.74 \(\) 0.00** & **8.54 \(\) 0.08** & 0.68 \(\) 0.01 & 0.72 \(\) 0.00 & 0.99 \(\) 0.04 & 3.4 \\ 
**aSCR** & \(\) 0.5 \(\) 0.1 & 1221 \(\) 20.6 & **0.954 \(\) 0.025** & **3.74 \(\) 0.00** & 8.36 \(\) 0.08 & **0.70 \(\) 0.01** & 0.72 \(\) 0.00 & **1.00 \(\) 0.03** & **2.4** \\   

Table 3: **GABO Adaptive SCR Ablation Study** One-shot (\(k=1\)) and few-shot (\(k=128\)) oracle evaluations averaged across 10 random seeds reported as mean \(\) standard deviation. \(\) (best) reports the top oracle value in the task dataset.

Figure 2: **Penalized LogP Score Maximization Sample Candidate Designs (Left)** The molecule with the highest penalized LogP score of 11.3 in the offline dataset. Separately, we show the 100th percentile candidate molecules according to the surrogate objective generated from (Middle) vanilla BO-qEI and (Right) GABO. Teal- (white-) colored atoms are carbon (hydrogen). Non-hydrocarbon atoms are underlined in the SMILES (Weininger, 1988) string representations of the molecules.

## Funding Disclosure and Acknowledgements

The authors thank Pratik Chaudhari at the University of Pennsylvania and the anonymous NeurIPS peer reviewers for their thoughtful comments, feedback, and discussion regarding this work. MSY is supported by NIH F30 MD020264. YZ and JRG are supported by NSF award IIS-2145644. JCG is supported by NIH R01 EB031722. OB is supported by NSF Award CCF-1917852.