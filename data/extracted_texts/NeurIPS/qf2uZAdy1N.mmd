# Reinforcement Learning under Latent Dynamics:

Toward Statistical and Algorithmic Modularity

 Philip Amortila

philipa4@illinois.edu

&Dylan J. Foster

dylanfoster@microsoft.com

&Nan Jiang

nanjiang@illinois.edu

&Akshay Krishnamurthy

akshaykr@microsoft.com

&Zakaria Mhammedi

mhammedi@google.com

The full (author-recommended) version of this paper can be found at: https://arxiv.org/pdf/2410.17904.

[MISSING_PAGE_FAIL:2]

This property, which we refer to as _algorithmic modularity_, enables modular, greatly simplified algorithm design, allowing one to use an arbitrary base algorithm for the base MDP class to solve the corresponding latent-dynamics problem. Algorithmic modularity is a stronger property than mere statistical modularity, and thus is subject to our statistical lower bound. Accordingly, we consider two settings that sidestep the lower bound through additional feedback and modeling assumptions. Our first algorithmic result considers _hindsight observability_, where latent states are revealed during training, but not at deployment (Theorem 4.1). Our second considers stronger function approximation conditions that enable the estimation of _self-predictive latent models_ through representation learning (Theorem A.1). Both results are _fully modular_: they transform _any_ sample-efficient algorithm for the base MDP class into a sample-efficient algorithm for the latent-dynamics setting. Thus, they constitute the first _general-purpose_ algorithms for RL under latent dynamics.

Together, we believe our results can serve as a foundation for further development of practical, general-purpose algorithms for RL under latent dynamics. To this end, we highlight a number of fascinating and challenging open problems for future research (Section 5).

## 2 Reinforcement Learning under General Latent Dynamics

In this section we formally introduce our framework, _reinforcement learning under general latent dynamics._

MDP preliminaries.We consider an episodic finite-horizon online reinforcement learning setting. With \(H\) denoting the horizon, a Markov decision process (MDP) \(M^{}=,,\{P_{h}^{}\}_{h=0}^{H},\{R_{h} ^{}\}_{h=1}^{H},H}\) consists of a state space \(\), an action space \(\), a reward distribution \(R_{h}^{}:()\) (with expectation \(r_{h}^{}(x,a)\)), and a transition kernel \(P_{h}^{}:()\) (with the convention that \(P_{0}^{}()\) is the initial state distribution).3

At the beginning of the episode, the learner selects a randomized, non-stationary _policy_\(=(_{1},,_{H})\), where \(_{h}:()\); we let \(_{}\) denote the set of all such policies. The episode evolves through the following process; beginning from \(x_{1} P_{0}^{}()\), the MDP generates a trajectory \((x_{1},a_{1},r_{1}),,(x_{H},a_{H},r_{H})\) via \(a_{h}_{h}(x_{h})\), \(r_{h} R_{h}^{}(x_{h},a_{h})\), and \(x_{h+1} P_{h}^{}( x_{h},a_{h})\). We let \(^{{ M}^{},}\) denote the law under this process, and let \(^{{ M}^{},}\) denote the corresponding expectation, and likewise let \(^{{ M},}\) and \(^{{ M}^{},}\) denote the analogous laws and expectations in another MDP \(M\). We assume that \(_{h=1}^{H}r_{h}\) almost surely for any trajectory in \(M^{}\).

For a policy \(\) and MDP \(M\), the expected reward for \(\) is given by \(J^{{ M}}()^{{ M},}_{h=1}^{H}r_{h}\), and the value functions are given by \(V_{h}^{{ M},}(x)^{{ M},}_{h^{ }=h}^{H}r_{h^{}} x_{h}=x\), and \(Q_{h}^{{ M},}(x,a)^{{ M},} _{h^{}=h}^{H}r_{h^{}} x_{h}=x,a_{h}=a\). We let \(_{{ M}}=\{_{{ M},h}\}_{h=1}^{H}\) denote an optimal deterministic policy of \(M\), which maximizes \(V^{{ M},}\) (over \(\)) at all states (and in particular, satisfies \(_{{ M}}_{_{}}J^{{ M}}()\)), and write \(Q^{{ M},} Q^{{ M},_{{ M}}}\). For \(f:\), we write \(_{f}(x)_{a}f(x,a)\) as well as \(V_{f}(x)=_{a}f(x,a)\). For MDP \(M\), horizon \(h[H]\), and \(g:\), we let \(_{h}^{{ M}}\) denote the Bellman (optimality) operator defined via \([_{h}^{{ M}}g](x,a)=^{{ M}}[r_{h}+g(x_{h+1}) x_{h}=x,a_{h}=a],\) and we overload notation by letting \([_{h}^{{ M}}f](x,a)=[_{h}^{{ M}}V_{f}](x,a)\). We also let \(_{h}^{{ M},}\) denote the Bellman _evaluation_ operator defined via \([_{h}^{{ M},}f](x)=^{{ M}} r_{h}+_{a^{}_{h+1}( x_{h+1})}[f(x_{h+1},a^{ })] x_{h}=x,a_{h}=a,\) for any \(_{}\). We define the _occupancy measures_ for layer \(h\) via \(d_{h}^{{ M},}(x)=^{{ M},}[x_{h}=x]\) and \(d_{h}^{{ M},}(x,a)=^{{ M},}[x_{h}=x,a_{h}=a]\).

Online reinforcement learning.In online reinforcement learning, the learning algorithm Alg repeatedly interacts with an unknown MDP \(M^{}\) by executing a policy and observing the resulting trajectory. After \(T\) rounds of interaction, the algorithm outputs a final policy \(\), with the goal of minimizing their _risk_, defined via

\[(T,,M^{}) J^{{ M}^{}}(_{{ M}^{}})-J^{{ M}^{}}().\] (1)Framework: Reinforcement learning under general latent dynamics.In _reinforcement learning under general latent dynamics_, we consider MDPs \(M^{*}\) where the dynamics are governed by the evolution of an unobserved latent state \(s_{h}\), while the agent observes and acts on _observations_\(x_{h}\) generated from these latent states. Formally, a _latent-dynamics MDP_ consists of two ingredients: a _base MDP_\(M_{}=\{,,\{P_{,h}\}_{h=0}^{H},\{R_{ ,h}\}_{h=1}^{H},H\}\) defined over a _latent state space_\(\), and a _decodable emission process_\(:=\{_{h}:()\}_{h=1}^{H}\), which maps each latent state to a distribution over observations. The former is an arbitrary MDP defined over \(\), while the latter is defined as follows.

**Definition 2.1** (Emission process).: _An emission process is any function \(:=\{_{h}:()\}_{h=1}^{H}\), and is said to be decodable if_

\[ h, s^{} s:\,_{h} (s)\,_{h}(s^{})=..\] (2)

_When \(=\{_{h}\}_{h=1}^{H}\) is decodable, we let \(^{-1}:=\{_{h}^{-1}:\}_{h=1}^{H}\) denote the associated decoder._

With this, we can formally introduce the notion of a latent-dynamics MDP.

**Definition 2.2** (Latent-dynamics MDP).: _For a base MDP\(M_{}=\{,,\{P_{,h}\}_{h=0}^{H},\{R_{ ,h}\}_{h=1}^{H},H\}\), and a decodable emission process \(\), the latent-dynamics MDP\(\! M_{},\!:=, ,\{P_{,h}\}_{h=0}^{H},\{R_{,h}\}_{h=1}^{H},H}\) is defined as the MDP where the latent dynamics evolve based on the agent's action \(a_{h}\) via the process \(s_{h+1} P_{,h}(s_{h},a_{h})\) and \(r_{h} R_{,h}(s_{h},a_{h})\). The latent state is not observed directly, and instead the agent observes \(x_{h}\) generated by the emission process \(x_{h}_{h+1}(s_{h})\).4_

Note that under these dynamics, the decoder \(^{-1}\) associated with \(\) ensures that \(_{h}^{-1}(x_{h})=s_{h}\) almost surely for all \(h[H]\). That is, the latent states can be uniquely decoded from the observations. To emphasize the distinction between the latent-dynamics MDP \(\! M_{},\!\) (which operates on the observable state space \(\)) and the MDP \(M_{}\) (which operates on the latent state space \(\)), we refer to the latter as a _base MDP_ rather than, for example, a "latent MDP", and apply a similar convention to other latent objects whenever possible.5

Departing from prior work, we do not place any inherent restrictions on the base MDP, and in particular do not assume that the latent space is small (i.e., tabular). Rather, we aim to understand--in a unified fashion--what structural assumptions on the base MDP \(M_{}\) are required to enable learnability under latent dynamics. To this end, it will be useful to considers specific _classes_ (i.e., subsets) of base MDPs \(_{}\) and the classes of latent-dynamics MDPs they induce.

**Definition 2.3** (Latent-dynamics MDP class).: _Given a set of base MDPs\(_{}\) and a set of decoders \(\{\}\), we let_

\[\! M_{},\!:=\{\! M_{ },\!:M_{}_{}, ,\ ^{-1}\}\] (3)

_denote the class of induced latent-dynamics MDPs._

Stated another way, \(\! M_{},\!\) is the set of all latent-dynamics MDPs\(\! M_{},\!\) where (i) the base MDP \(M_{}\) lies in \(_{}\), and (ii), the emission process \(\) is decodable, with the corresponding decoder belonging to \(\). The class \(_{}\) represents our prior knowledge about the underlying MDP \(M_{}\); concrete classes considered in prior work include tabular MDPs , linear dynamical systems , and factored MDPs . In particular, the class \(_{}\) may itself warrant using function approximation. At the same time, the class \(\) represents our prior knowledge or inductive bias about the emission process, enabling representation learning. In what follows, we investigate what conditions on \(_{}\) make the induced class \(\! M_{},\!\) tractable, both statistically (statistical modularity; Section 3) and via reduction (algorithmic modularity; Section 4).

## 3 Statistical Modularity: Positive and Negative Results

This section presents our main statistical results. We begin by formally defining the notion of statistical modularity introduced in Section 1, present our main impossibility result (lower bound) and its implications (Section 3.2), then give positive results for the general class of _pushforward-coverable_ MDPs (Section 3.3).

[MISSING_PAGE_FAIL:5]

**Intuition for lower bound.** The intuition behind the lower bound in Theorem 3.1 is as follows: the unobserved latent state space consists of \(N=||\) binary trees (indexed from \(1\) to \(N\)), each with \(N\) leaf nodes. The starting distribution is uniform over the roots of the \(N\) trees, and the agent receives a reward of \(1\) if and only if they navigate to the leaf node that corresponds to the index of their current tree. The observed state space is identical to the latent state space, but the emission process shifts the index of the tree by an amount which is unknown to the agent. Despite the base MDP being known and the decoder class satisfying realizability, the agent requires near-exhaustive search to identify the value of the shift and recover a near-optimal policy.

A taxonomy of statistical modularity.As a corollary, we prove that many (but not all) well-studied function approximation settings do not admit statistical modularity by embedding them into the lower bound construction of Theorem 3.1 (as well as a variant of the result, Theorem E.1). Our results are summarized in Figure 1. Our impossibility results highlight the following phenomenon: many MDP classes \(_{}\) that place structural assumptions via the value functions (e.g., MDPs with linear-\(Q^{}/V^{}\) or MDPs with a Bellman complete value function class of bounded eluder dimension ) become intractable under latent dynamics. Intuitively, this is because it is not possible to take advantage of structure in value functions without learning a good representation, and, simultaneously, these assumptions are too weak by themselves to enable learning such a representation. Meanwhile, MDP classes \(_{}\) that place structural assumptions on the transition distribution (e.g., MDPs with low state occupancy complexity  or low-rank MDPs ) are sometimes (but not always) tractable under latent dynamics.8

We point to Appendix E.2 for background on all the settings in Figure 1 and proofs that they are (or are not) statistically modular. We remark that it is fairly straightforward to embed most of the MDP classes of Figure 1 into the construction of Theorem 3.1 since it only uses only a single base MDP \(M_{}\), and we expect that many other base MDP classes can similarly be shown to be intractable. However, proving the _positive_ results in Figure 1 requires establishing several new results showing that certain base classes are tractable under latent dynamics; most notably, we next discuss the case of _pushforward coverability_.

### Upper bounds: Pushforward-coverable MDPs are statistically modular

Our main postive result concerning statistical modularity is to highlight _pushforward coverability_--a strengthened version of the _coverability_ parameter introduced in Xie et al. --as a general structural parameter that enables sample-efficient reinforcement learning under latent dynamics.

[MISSING_PAGE_FAIL:7]

Setup and O2L meta-algorithm.For the results in this section, we denote the (unknown) latent-dynamics MDP of interest by \(M^{*}_{}:=\! M^{*}_{},^{*}\!\!\), and use \(^{}:=(^{})^{-1}\) to denote the true decoder. The O2L meta-algorithm (Algorithm 1) learns a near-optimal policy for \(M^{}_{}\) by alternating between performing representation learning and executing a black-box "base" RL algorithm (designed for the base MDP) on the learned representation; this approach is inspired by empirical methods that blend representation learning and RL in the latent space (e.g., ).

```
1:input: Epochs \(T\), episodes \(K\), decoder set \(\), rep. learning oracle RepLearn, base alg. \(_{}\).
2:for\(t=1,2,,T\)do
3:RepLearn chooses a representation \(^{(t)}:\) based on data collected so far.
4: Initialize new instance of \(_{}\).
5:for\(k=1,2,,K\)do//ALG1at plays \(K\) rounds in the "\(^{(t)}\)-compressed dynamics."
6:\(_{}\) chooses policy \(^{(t,k)}_{}:[H]()\).
7:\(\ _{}^{(t)}\) to collect trajectory \(\{x^{(t,k)}_{h},a^{(t,k)}_{h},r^{(t,k)}_{h}\}_{h=1}^{H}\).
8:\(\ _{}\) with compressed trajectory \(\{^{(t)}_{h}(x^{(t,k)}_{h}),a^{(t,k)}_{h},r^{(t,k)}_{h}\}_{h=1}^ {H}\).
9:endfor
10:\(_{}\) returns final policy \(^{(t)}:[H]()\), deploy \(^{(t)}^{(t)}\) to collect one trajectory.
11:endfor
12:return\(=(^{(1)}^{(1)}, ,^{(T)}^{(T)})\). ```

**Algorithm 1**O2L: Observable-to-Latent Reduction

Concretely, the algorithm takes as input a _representation learning oracle_RepLearn and a _base RL algorithm_Alg\(_{}\) that operates in the latent space. In each _epoch_\(t[T]\), RepLearn produces a new representation \(^{(t)}:\) based on data observed so far (potentially using additional side information, which we will elaborate on in the sequel). Then, the reduction involves \(_{}\), using \(^{(t)}\) to simulate access to the true latent states. In particular, \(_{}\) runs for \(K\) episodes, where at each episode \(k\): (i) \(_{}\) produces a latent policy \(_{^{(t,k)}}:[H]()\), (ii) the latent policy is transformed into an observation-level policy via composition with \(^{(t)}\), i.e. \(_{^{(t,k)}}^{(t)}\), which is then deployed to produce a trajectory \(\{x^{(t,k)}_{h},a^{(t,k)}_{h},r^{(t,k)}_{h}\}_{h=1}^{H}\), and (iii) the trajectory is _compressed through_\(^{(t)}\) and used to update \(_{}\) via \(\{^{(t)}_{h},(x^{(t,k)}_{h},a^{(t,k)}_{h},r^{(t,k)}_{h}\}_{h=1} ^{H}\) (cf. Line 8 of Algorithm 1).9 After the \(K\) rounds conclude, \(_{}\) produces a final latent policy \(^{(t)}_{}:[H]()\). The final policy \(\) chosen by the O2L algorithm is a uniform mixture of \(^{(t)}_{}^{(t)}\) over all the epochs.

The central assumption behind O2L is that the base algorithm \(_{}\) can achieve low-risk in the underlying base MDP \(M^{}_{}\)_if given access to the true latent states_\(s_{h}=^{}(x_{h})\). Beyond this assumption, we require that the representation learning oracle RepLearn can learn a sufficiently high-quality representation. In our applications, this will be made possible by assuming access to a realizable decoder class \(\) and two distinct assumptions: _hindsight observability_ (Section 4.1) and conditions enabling _self-predictive representation learning_ (Section 4.2). We will show that under these conditions, we can instantiate a representation learning oracle such that O2L inherits the sample complexity guarantee for \(_{}\), thereby achieving algorithmic modularity.

### Algorithmic modularity via hindsight observability

Our first algorithmic result bypasses the hardness in Section 3 by considering the setting of _hindsight observability_, which has garnered recent interest in the context of POMDPs . Here, we assume that at training time (but not during deployment), the algorithm has access to additional feedback in the form of the true latent states, which are revealed at the end of each episode.

**Assumption 4.1** (Hindsight Observability ).: _The latent states \((^{*}_{1}(x_{1}),,^{*}_{H}(x_{H}))\) are revealed to the learner after each episode \((x_{1},a_{1},r_{1},,x_{H},a_{H},r_{H})\) concludes._

We emphasize that in the hindsight observability framework, the learner must still execute _observation-space policies_\(_{}:[H]()\), as the latent states are only revealed _at the end of each episode_. Under hindsight observability, we can instantiate the representation learning oracle in O2L so that the reduction achieves low risk for _any choice of black-box base algorithm_\(_{ 1at}\). In particular, we make use of _online_ classification oracles, which use the revealed latent states to achieve low classification loss with respect to \(^{}\) under adaptively generated data. We first state a guarantee based on generic classification oracles, then instantiate it to give a concrete end-to-end sample complexity bound.

Formally, at each step \(t\), the online classification oracle, denoted via \(_{}\), is given the states and hindsight observations collected so far and produces a deterministic estimate \(^{{}^{(t)}}=_{}(\{x_{b}^{(t)}, _{h}^{(t)}(x_{b}^{(t)})\}_{i<t,h H})\) for the true decoder \(^{}\). We measure the regret of the oracle via the \(0/1\) loss for classification:

\[_{}(T)_{t=1}^{T}_{h=1}^{H}_{^{{}^{(t)}} p^{{}^{(t)}}}\,^{^{{}^{(t)}}} \{_{h}^{{}^{(t)}}(x_{h})_{h}^{}(x_{h}) \},\]

where \(p^{{}^{(t)}}\) represents a randomization distribution over the policy \(^{{}^{(t)}}\). Our reduction succeeds under the assumption that the oracle has low expected regret.

**Assumption 4.2**.: _For any (possibly adaptive) sequence \(^{{}^{(t)}}\), with \(^{{}^{(t)}} p^{{}^{(t)}}\), the online classification oracle \(_{}\) has expected regret bounded by_

\[[_{}(T)]_{}(T),\]

_where \(_{}(T)\) is a known upper bound._

We apply such an oracle within O2L as follows: at the end of each iteration \(t[T]\) in O2L, we sample \(k[K]\) uniformly, and update the classification oracle with the trajectory \((x_{1}^{{}^{(t,k)}},a_{1}^{{}^{(t,k)}},r_{1}^{{}^{(t,k)}}),,(x_{H}^{{}^ {(t,k)}}a_{B}^{{}^{(t,k)}},r_{B}^{{}^{(t,k)}})\); see the proof of Theorem 4.1 for details. We let \(_{}(TK)\) denote the risk of the O2L reduction when run for \(T\) epochs of \(K\) episodes, and we let \(_{}(K)[(K,_{  1at},M_{ 1at}^{})]\) denote the expected risk of \(_{ 1at}\) when executed on \(M_{ 1at}^{}\) with access to the true latent states \(s_{h}=^{}(x_{h})\) for \(K\) episodes.

**Theorem 4.1** (Risk bound for O2L under hindsight observability).: _Let \(_{ 1at}\) be a base algorithm with base risk\(_{}(K)\), and \(_{}\) a representation learning oracle satisfying Assumption 4.2. Then Algorithm 1, with inputs \(T,K,\), \(_{}\), and \(_{ 1at}\), has expected risk_

\[[_{}(TK)]_{}(K)+ _{}(T).\]

This result shows that we can achieve sublinear risk under latent dynamics as long as (i) the base algorithm achieves sublinear risk \(_{}(K)\) given access to the true latent states, and (ii) the classification oracle achieves sublinear regret \(_{}(T)\). Notably, the result is fully modular, meaning we require no explicit conditions on the latent dynamics or the base algorithm, and is computationally efficient whenever the base algorithm and classification oracle are efficient.

To make Theorem 4.1 concrete, we next provide a representation learning oracle (ExpWeights.Dr; Algorithm 3 in Appendix G.1) based on a derandomization of the classical exponential weights mechanism, which satisfies Assumption 4.2 with \(_{} H||\) whenever it has access to a class \(\) that satisfies decoder realizability.

**Lemma 4.1** (Online classification via ExpWeights.Dr).: _Under decoder realizability \((^{})\), ExpWeights.Dr (Algorithm 3) satisfies Assumption 4.2 with10_

\[_{}(T)=}(H||).\]

Instantiating Theorem 4.1 with the above representation learning oracle, we obtain the following algorithmic modularity result.

**Corollary 4.1** (Algorithmic modularity under hindsight observability).: _For any base algorithm \(_{ 1at}\), under decoder realizability \((^{})\), O2L with inputs \(T,K,,,\) and \(_{ 1at}\) achieves_

\[[_{}(TK)]_{}(K)+ .\]

_Consequently, for any \(_{ 1at}\), setting \(T KH||/_{}(K)\) achieves \([_{}(TK)]_{}(K)\) with a number of trajectories \(TK=}K^{2}H||/_{}(K)\)._Beyond achieving algorithmic modularity, this result shows that under hindsight observability, we can achieve strong statistical modularity (modulo possible \(H\) factors) for _every_ base MDP class \(_{1}\), an important result in its own right.11 As an example, suppose that \(_{}(K)=(K^{-1/2})\), which is satisfied by many standard algorithms of interest [17; 18; 19; 20; 21]. Then, setting \(T\) according to Corollary 4.1 obtains an expected risk bound of \(\) using \((H||/^{})\) trajectories.

**Remark 4.1** (Online versus offline oracles).: Theorem 4.1 critically uses that assumption that \(_{}\) satisfies an _online_ classification error bound to handle the fact that data is generated adaptively based on the estimators \(^{(1)},,^{(T)}\) it produces, which is by now a relatively standard technique in the design of interactive decision making algorithms [14; 15; 16]. We note that under coverability and other exploration conditions, online oracles for classification can be directly obtained from _offline_ (i.e. supervised) classification oracles [17; 18; 19].

### Algorithmic modularity via self-predictive estimation

We complement the above results by studying the general online RL setting _without_ hindsight observations. To address this more challenging setting, we design an _optimistic self-predictive estimation_ objective (Eq. (7)), which learns a representation by jointly fitting a decoder together with a latent model. We prove that any representation learning oracle that attains low regret with respect to this objective can be used in O2L to obtain observable-to-latent reductions for any low-risk base algorithm \(_{1}\) (for a formal statement, see Theorem A.1). We provide a (computationally inefficient) estimator (SelfPredict.Opt; Algorithm 4 in Appendix H.1) which we show attains low optimistic self-regret under certain statistical conditions (namely, coverability of the base MDP and a function approximation condition enabling us to express the self-prediction target as a latent model, see Lemma A.1 for a formal statement), thereby obtaining an end-to-end reduction for the general online RL setting. For lack of space, these results are deferred to Appendix A.

## 5 Discussion

Our work initiates the study of statistical and algorithmic modularity for reinforcement learning under general latent dynamics. Our positive and negative results serve as a first step toward a unified theory for reinforcement learning in the presence of high-dimensional observations. To this end, we close with some important future directions and open problems.

Statistical modularity.Can we obtain a unified characterization for the statistical complexity of RL under latent dynamics with a given class of base MDPs \(_{1}\)? Our results in Section 3 suggest that this will require new tools that go beyond existing notions of statistical complexity. Toward resolving this problem, concrete questions that are not yet understood include: (i) Is coverability  (as opposed to pushforward coverability) sufficient for learnability under latent dynamics? (ii) Is the _Exogenous Block MDP_ problem [17; 18]--a special case of our general framework--statistically tractable? Lastly, are there additional types of feedback that are weaker than hindsight observability, yet suffice to bypass the hardness results in Section 3?

Algorithmic modularity.Can we derive a unified representation learning objective that enables algorithmic modularity whenever statistical modularity is possible? Ideally, such an objective would be computationally tractable. Alternatively, can we show that algorithmic modularity fundamentally requires stronger modeling assumptions than statistical modularity? Toward addressing the problems above, a first step might be to understand: (i) What are the minimal statistical assumptions under which we can minimize the self-predictive objective in Section 4.2? (ii) How can we encourage finding good representations via self-prediction beyond the use of optimism over the base (latent) models; and (iii) when can we minimize self-prediction in a computationally efficient fashion?

#### Acknowledgements

Nan Jiang acknowledges funding support from NSF IIS-2112471, NSF CAREER IIS-2141781, Google Scholar Award, and Sloan Fellowship.