# HENASY: Learning to Assemble Scene-Entities for Interpretable Egocentric Video-Language Model

Khoa Vo  Thinh Phan  Kashu Yamazaki  Minh Tran  Ngan Le

AICV Lab, University of Arkansas, Fayetteville, USA

{khoavoho,thinhp,kyamazak,minht,thile}@uark.edu

###### Abstract

Current video-language models (VLMs) rely extensively on instance-level alignment between video and language modalities, which presents two major limitations: (1) visual reasoning disobeys the natural perception that humans do in first-person perspective, leading to a lack of reasoning interpretation; and (2) learning is limited in capturing inherent fine-grained relationships between two modalities.

In this paper, we take an inspiration from human perception and explore a compositional approach for egocentric video representation. We introduce _HENASY (Hierarchical ENtities ASesembl1Y)_, which includes a spatiotemporal token grouping mechanism to explicitly assemble dynamically evolving scene entities through time and model their relationship for video representation. By leveraging compositional structure understanding, HENASY possesses strong interpretability via visual grounding with free-form text queries. We further explore a suite of multi-grained contrastive losses to facilitate entity-centric understandings. This comprises three alignment types: video-narration, noun-entity, verb-entities alignments.

Our method demonstrates strong interpretability in both quantitative and qualitative experiments; while maintaining competitive performances on five downstream tasks via zero-shot transfer or as video/text representation, including video/text retrieval, action recognition, multi-choice query, natural language query, and moments query.

Project page: [https://uark-aicv.github.io/HENASY](https://uark-aicv.github.io/HENASY)

## 1 Introduction

Recent advancements in technology and hardware devices for augmented reality (AR) have fueled hopes for virtual assistant applications that can provide users a wide range of assistance, such as real-time procedural instructions, moments retrieval, and interactive learning experiences, all through egocentric video streams of similar perspective with user. Publicly available massive-scale egocentric datasets such as Ego4D  and Epic Kitchens-100 , providing suites of egocentric tasks, have further sparked even more interest within the research community.

Video-language models (VLMs) have currently become a _de-facto_ approach to egocentric video understanding. By learning robust visual-language representations from video-caption pairs , VLMs can be applied flexibly to a wide range of downstream tasks, either through zero-shot transfer or as modality encoders. Existing state-of-the-art (SOTA) VLMs for egocentric videos  exhibit remarkable performances by following CLIP-like  dual-encoder architecture. During training, these models generally learn through the _instance-level_ alignment  between pairs of video and caption representations (Fig. 1(a)).

However, videos consist of complex and dynamic interactions among arbitrary entities, which cannot be effectively captured by simple _instance-level_ alignment alone. In fact, a caption contains textual elements that concisely capture video entities. For examples, nouns indicate entity occurrences ,while verb phrases convey motion information  in the video. To fully capture these fine-grained alignments, a VLM will perform more effectively if it: (1) understand videos in a bottom-up manner, where semantically similar patches form entities, and relationships between entities construct the video representation; and (2) explicitly model fine-grained relationships between video entities and nouns/verbs to comprehensively capture appearance/motion information, respectively.

Human perception aligns closely with the above requirements. We perceive the dynamic surroundings in a compositional manner , where distinct entities emerge from smaller parts that combine to form a whole. Each entity maintains spatial and temporal coherence and interacts with others only when in close proximity. Understanding the compositional structure of the surroundings enables us to intrinsically comprehend and memorize information, while also allowing us to provide _interpretations_ of our decision-making process, which is absent in current egocentric VLMs.

Inspired by such observation, we propose _HENASY: Hierarchical **EN**tities **AS**Semb**I**Y_ framework (pronounced heh-nuh-see), which follows compositional understanding as in Fig 1(b). Concretely, HENASY comprises three key components: _(i) Local Entity Encoder_, a hierarchical transformer-based encoder that learns to assemble dynamic scene-entities from video patches via our proposed spatiotemporal token grouping mechanism, which is an enhanced version from slot-based groupings in stationary images ; _(ii) Global Encoder_, a pre-trained video representation module that perceives the input video at a global level; and _(iii) Entity-Aware Decoder_, which models the internal interactions among scene entities and their relationship with the global features, thereby enriching the entity-centric video representation extraction. Furthermore, HENASY is able to perform visual grounding to obtain dynamic segmentations corresponding to either entity or activity with the produced entity embeddings and their attention maps as a side product of its local entity encoder, showing promising interpretation via dynamic saliency maps across frames (Fig. 1(c)).

Developing an effective model necessitates a strong network architecture and well-defined objectives. With the proposed HENASY architecture, instance-level contrastive loss only handles global alignment, failing to address dynamic entity alignment. Hence, we introduce multi-grained contrastive losses to optimize HENASY for both entity- and video-level representations using narration alone. Specifically, HENASY is trained with three types of alignment: video-narration, noun-entity, and verb-entities. While the first two employ instance-level contrastive loss and model object occurrences via narration's nouns, respectively, verb-entity alignment is newly introduced. It aims to incorporate activity/motion information from narration's verb phrases into entities using a _one-to-many strategy_, which emphasizes the alignment of a verb phrase to the most semantically relevant entities. Additionally, we propose a new _projection loss_ that employs detected hand/object boxes  to ensure segmentation masks tightly cover respective entities, enhancing HENASY's interpretative robustness.

We are the first to demonstrate the value of compositional perception approach for egocentric video understanding. Our experiments show that by tasking our proposed _local entity encoder_ to assemble dynamic entities, video representations are effectively improved to outperform current VLMs in a wide range of benchmarks, including video retrieval (EgoMCQ  & EpicKitchen-MIR ),

Figure 1: **Problem Overview.****(a)** Current VLMs  rely on instance-level contrastive learning between video & narration. HelpingHands  implicitly induces object occurrence information into video features at final layer of video encoder. **(b)** Our proposed (_HENASY_) aims to assemble dynamic entities from video patches via _local entity encoder_, while _entity-aware decoder_ captures interactions between entities and global context to form comprehensive video. HENASY is trained with suite of multi-grained contrastive alignments to enforce visual representations entity-level upto video-level. **(c)** By such compositional approach, HENASY is the first VLM that shows strong interpretability via visual grounding with both appearance/motion query types.

activity recognition (EpicKitchen-CLS & EGTEA ) via zero-shot transfer. Furthermore, temporal localization models [14; 15] equipped with HENASY video/text features can achieve state-of-the-art performances in episodic memory tasks of EgoNLQ and EgoMQ . Finally, HENASY possesses strong interpretability that is quantitatively and qualitatively superior to current VLMs.

## 2 Related Works

**Video-Language Pre-Trained Models.** Pre-training VLMs on a large-scale dataset of video-text pairs and deploying them in downstream tasks has now become a standard practice. Transformer-powered pre-trained VLMs [16; 17; 18; 19; 3; 6; 5; 4; 9] have accomplished superior results on a wide range of tasks, i.e., text-to-video retrieval, action recognition, or events localization. VLMs can be divided into two common categories, i.e., unified- and dual-encoder. The former models [16; 18; 20] fuse multimodal via cross-attention and can be trained with proxy tasks of masked language modeling [21; 22] or masked frame modeling [18; 16]. The latter models [23; 3; 5; 6; 9] employ separate encoders for video and text, trained jointly via contrastive learning [8; 3].

Recently, several VLMs [4; 17; 9] employ fine-grained information from captions by decomposing them to capture object/activity through nouns/verbs, respectively. However, these models do not fully exploit fine-grained learning within the video encoder itself, and true granular-level alignment between modalities remains unexplored. _In our work, we explicitly model visual content as dynamic entities, capturing their interactions to form a comprehensive video representation. Additionally, our proposed method is trained with multi-grained objectives, ranging from video-text, noun-entity, to verb-entities pairs._

**Interpretable Video Representation.** There have been efforts to enhance the interpretability of video representations [24; 25; 26; 27], these typically involved factorizing videos into entities and environmental contexts, and utilizing adaptive attention mechanisms  to selectively focus on relevant entities. Such mechanisms enable interpretation through their selection module, highlighting only the primary entities contributing to the final predictions. However, these approaches often require entities to be pre-detected by external models, which is restricted to a predefined set of objects. Additionally, these works have focused on models specifically designed for individual tasks, e.g., temporal action detection, video dense captioning. _In contrast, our work introduces an end-to-end method capable of learning to form entities without an off-the-shelf detector. Moreover, we aim to develop a robust video-language model (VLM) that is versatile across a variety of tasks._

**Interpretation via Object Discovery.** Recent years have seen a growing body of research in end-to-end learning of object discovery, which learns to decompose an image or a video into distinct objects without direct supervision. Slot-based methods such as IODINE  and Slot Attention  utilize mixture-model likelihoods  to tackle this challenge, demonstrating promising performance through evaluations on synthetic images with simple objects. Subsequently, GroupViT  and ODIN  incorporate slot attention with contrastive learning and achieved notable success in identifying semantic groupings on natural in-the-wild images. However, these models are not capable of modeling dynamic objects in videos domain. To mitigate this problem, SaVI++  proposes a workaround technique, which requires groundtruth depth information in a reconstruction objective to bootstrap object discovery training. In our work, we enhance slot-based grouping mechanisms introduced in GroupViT  to model temporal coherency of dynamic objects in videos. _Different from , HENASY does not require any extra data further than color video sequences. Instead, HENASY utilizes learned patch features of pre-trained global encoder to bootstrap several early layers of its local entity encoder for entities grouping via a cross-attention mechanism._

## 3 Preliminaries

**Video-language representation learning** aims to learn a common latent space to represent video and text. A training dataset for this task comprises of \(N\) tuples \(\{_{i},_{i}\}_{i=1}^{N}\), where \(_{i}\) denotes a short sequence of RGB frames, and \(_{i}\) is a free-form text sentence that describes visual content.

**Dual-encoder architecture** is a common paradigm that current SOTA VLMs [4; 5; 3] employ for the above task, which consists of (a) a visual encoder \(f\) mapping the input video \(_{i}\) to a visual embedding feature \(_{i}=f(_{i})\), and (b) a language encoder \(g\) mapping the text \(_{i}\) to a linguistic embedding feature \(_{i}=g(_{i})\).

**Contrastive-based losses** are common objective for video-language representation. Given a batch of \(B\) normalized video-text embedding pairs \(i=\{}_{i}=_{i}}}{{|_{i}|}},}_{i}=_{i}}}{{|_{i}|}}\}\), a contrastive-based loss pulls embeddings of aligned (positive) pairs close in feature space, while pushing embeddings of misaligned (negative) pairs away. We adopt _EgoNCE_ variation  of contrastive loss as one of our training objectives because of its effective approach in identifying positive and negative pairs. Specifically, each sample \(i B\) is associated to a set of positives \(P_{i}\) constructed by comparing nouns and verbs across all texts. Additionally, for each sample \(i\), a hard negative \(i^{}\) is sampled from a temporally adjacent segment within the same video, expanding our batch to \(\). For a more in-depth discussion on the strategy used for sample selection, refer to . Herein, the video-to-text objective is succinctly expressed as:

\[_{ego}^{v2t}=}_{i} }(}^{T}}_{p}/)}{ _{n B}(}^{T}}_{n}/)+(}^{T}}_{n^{}}/)} \]

The objective of text-to-video \(_{ego}^{t2v}\) is derived from Eq. 1 by inverting \(v\) and \(t\), and EgoNCE loss is a summation of both directions \(_{ego}=_{ego}^{v2t}+_{ego}^{t2v}\).

## 4 Henasy

We present the HENASY framework (Fig. 2) for egocentric video-language modeling. HENASY is a compositional video understanding approach featuring a dual-encoder architecture, designed to explore an interpretable, entity-based visual representation. Specifically, besides typically capturing global features via global encoder (Sec. 4.1), our video encoder also assembles dynamic scene entities from video patches via _local entity encoder_ (Sec. 4.2), then _entity-aware decoder_ (Sec. 4.3) models their intra-connections as well as inter-connections with global features to form a comprehensive video representation. Our objective is to develop an interpretable reasoning process that robustly supports decision-making, while allowing visual grounding with text queries. To achieve this target, it requires not only an effective network design, but also a suite of multi-grained contrastive learning (Sec. 4.4) to enforce both entity-level and video-level representations.

For readability, we denote five types of tokens as follows: \(\) for video patch tokens, \(\) for learnable video tokens, \(\) for learnable group tokens, \(\) for segment tokens, and \(\) for entity tokens.

### Global Encoder

Global encoder provides global visual information of the entire input video. We adopt the pre-trained TimeSFormer  to capture the global visual context of the entire input video. Particularly, we follow the protocol of  to decompose the given input video sequence \(_{i}^{T 3 H W}\) with \(T\) RGB frames or resolution \(H W\) into \(T K\) non-overlapping patches of resolution \(P P\), where \(K=HW/P^{2}\). Then, every patch is linearly projected by a 2D convolutional layer, forming video patch tokens \(^{TK D}\) (with \(TK=T K\)) representing embedding features at every temporal and spatial location, where \(D\) is hidden dimension. TimeSFormer processes the video patch tokens \(\) with an additional learnable video tokens \(^{1 D}\) through a stack of divided space-time attention (_DST_) blocks, which is described as follows: \([^{l+1};^{l+1}]=([^{l};^{l }])\) from the previous stage into larger segments. As a result, local entity encoder forms scene entity tokens that depict individual entities, which dynamically evolve across video frames.

While following GroupViT  to directly process input video patch tokens could be an option, we find doing that diminishes performance. Additionally, the tokens grouping mechanism from  is not capable of modeling dynamic entities in videos domain. To address these challenges, we introduce a _bootstrapping stage_, which couples itself with early layers of the global encoder throughcross-attention to group video patches into initial entities' segments. Furthermore, to capture dynamic entities in video, we introduce _temporal-aware grouping_ mechanism.

**Bootstrapping Stage.** Bootstrapping stage consists of \(S_{1}\) consecutive cross-attention layers, which takes a set of learnable group tokens \(_{boot}^{l}^{G D}\) as initial queries (\(G\) is the initial number of tokens). At each cross-attention layer \(l\) starting from 0, the queries aggregate information from the patch tokens \(^{l}\) at corresponding layer of global encoder:

\[_{boot}^{l+1}=^{l}(_{boot}^{l}, ^{l}),l=0,..,S_{1}-1 \]

At the final layer of bootstrapping stage, we obtain \(_{boot}^{l}\). This is then associated with patch tokens \(^{l}\) from the corresponding layer of global encoder within _temporal-aware grouping_ block (TAG) to group patches into larger segment tokens:

\[^{l}=(_{boot}^{l},^{l})l=S_{1} \]

Herein, \((,)\) merges key tokens \(\) together based on their similarities with query \(\), while preserving the temporal dimension of key tokens (discussed in detail later). As a result, we obtain new segment tokens \(^{l}^{TG D}\), which is then utilized as inputs to the _entity grouping stage_.

**Entity Grouping Stage.** From this point, local entity encoder is decoupled from the global encoder and is trained to merge these input segments \(\) into complete scene entities \(\). At this stage, a new set of learnable group tokens \(_{entity}^{l}^{E D}\) is introduced, which aims to relate segment tokens with similar semantics into an individual scene entity. It is important to note that maintaining consistent temporal dynamics is required at each stage. Therefore, we adopt \(S_{2}\) DST blocks  to propagate information mutually between learnable group tokens and segment tokens: \([_{entity}^{l+1};^{l+1}]=([_{entity }^{l};^{l}])\), for \(l=S_{1},..,S_{1}+S_{2}-1\).

After the final layer, segment tokens \(^{l}\) are grouped to generate intermediate entity tokens, i.e., \(}^{l}=(_{entity}^{l},^{l}) ^{TE D}\), where \(l=S_{1}+S_{2}\). Then, to enable interactions between scene entities and across temporal dimension, we apply a stack of \(S_{3}\) DST blocks to all entity tokens: \(}^{l+1}=(}^{l})\), for \(l=S_{1}+S_{2},..,S_{1}+S_{2}+S_{3}-1\).

We observed that segment tokens \(\) and entity tokens \(\) facilitate temporal consistencies within the temporal attention of a DST block. Unlike in TSF , where tokens are spatially limited within a patch, these tokens can evolve freely across frames, enhancing the flexibility of the time-attention mechanism in a DST block. To obtain spatio-temporal entity embeddings, we apply temporally

Figure 2: **Overview of the HENASY framework for video-language modeling. Left:** HENASY features a dual-encoder architecture with a compositional video understanding approach. The local entity encoder assembles dynamic scene entities from video patches, while the global encoder provides contextual features. These are combined in the entity-aware decoder to create an interpretable video representation. **Right:** HENASY is supported by a suite of multi-grained contrastive learning to enforce both entity-level and video-level representations.

average pooling on entity tokens of the final layer: \(=(^{l})\), where \(^{E D}\) and \(l=S_{1}+S_{2}+S_{3}\).

Temporal-Aware Grouping (TAG).As aforementioned, TAG is employed at the final layers of bootstrapping stage and entity grouping stage, to merge semantically similar tokens (i.e., \(\) or \(\)) into a larger segment while preserving the temporal dimension. Generally, this mechanism takes a set tokens \(^{T D}\) ( \(\) can be either \(\) or \(\)) as inputs and a set of learnable group tokens \(_{q}^{Q D}\) as queries. We re-shape \(\) into 3-dimension \(^{T I D}\) and evaluate the similarity between \(_{q}\) and \(\).

It firstly evaluates similarity between each group token and every input token, forming a 3D similarity matrix \(^{T Q I}\). Then, an assignment matrix \(}\{0,1\}^{T Q I}\) is computed, assigning each input token to the most relevant group based on similarity scores. Finally, it performs per-frame groupings, merging the input tokens of the same group together, forming a set of new tokens \(^{T Q D}\) representing larger segments. We formalize the computation of every new group as follows:

\[=(_{q},)\ ;_{t,i}= _{t,i}(_{q},)=(_{q})_{i}+^{I}}_{t,i,j}_{t,j}}{_{j=1}^{I}}_{t,i,j}} \]

Afterwards, we re-shape \(\) to \(^{TQ D}\) as output of TAG.

The computation of similarity matrix \(\) and assignment matrix \(}\) are detailed in Appendix B.1. Additionally, the derivation of saliency maps for interpreting assignments between video patches and entities is explained Appendix B.2.

### Entity-Aware Decoder

We seek to propagate entity-level features \(^{l}\) from the local entity encoder to the final video embedding for a comprehensive video representation. For this purpose, we introduce _entity-aware decoder_, which is illustrated in Fig. 3. Entity-aware decoder includes a stack of hybrid-attention blocks to refine the interactions between entities and video patches, and render the video embedding. At a block \(b_{dec}\), it first performs cross-attention with entity tokens as queries and patch tokens as keys, values. Then, self-attention followed by a multi-layer perceptron (MLP) is applied over the output:

\[}^{b_{dec}} =(^{b_{dec}},^{l}) \] \[^{b_{dec}+1} =(}^{b_{dec} })\]

Eventually, we obtain the video representation, dubbed as entity-aware video embedding \(\), by averaging the final outputs of entity tokens:

\[=(^{b_{dec}}) \]

### Multi-grained Contrastive Learning

Beside video-narration contrastive loss [8; 3], which captures coarse-grained semantic alignment between the video and narration, we introduce two finer-grained contrastive losses: noun-entity contrastive loss (NEC) and verb-entities contrastive loss (VEC), which focuses on inducing visual appearance and motion cues directly to the composed entities. We also utilize projection loss, leveraging object boxes from an off-the-shelf detector  as a weak supervision to encourage the generated entity masks tightly conforming to the corresponding entity, promoting robust interpretability of our proposed model.

Noun-Entity Contrastive Loss (NEC).From the groundtruth narration, we obtain \(N_{n}\) nouns and their embeddings \(^{N_{n} D}\) via the text encoder. Following , we compute a similarity matrix between noun embeddings and entity embeddings. Every noun is matched with an entity token having highest similarity score via Hungarian matching. Following this, we construct a noun-entity contrastive loss using the InfoNCE , where positive pairs consist of the matched noun embedding \(_{p}\) and entity embedding \(_{p}\). The contrast is defined over the embeddings \(^{}_{j}\) of all nouns in the dataset taxonomy dictionary \(\):

\[_{NEC}=-}_{p=1}^{N_{n}} _{p}^{T}_{p}/)}{_{j}(_{p}^{T} ^{}_{j}/)} \]

Figure 3: Illustration of entity-aware decoder.

**Verb-Entities Contrastive Loss** is a new loss term that instills motion information directly into entity tokens from narration's verb phrases. As suggested in  that LLMs are superior to classical methods such as part-of-speech tagging in retrieving verb phrases, we use a Llama-2  to obtain \(N_{v}\) verb phrases from an input narration. Given that a verb phrase describes an activity involving several scene entities, we introduce _weighted many-to-one alignment_ strategy to prioritize the most relevant entity-verb alignments. Firstly, let \(_{i}^{D}\) be one of the embedded verb phrases, we obtain a Softmax-normalized similarity scores between \(_{i}\) and every entity \(_{j}\): \(s(_{i},_{j})=_{i}_{j}^{T}}{ _{k}^{k}_{i}_{k}^{T}}\). Then, we re-weight entities by the computed weight and obtain a weighted average of entities representation: \(^{avg}=_{j}^{E}s(_{i},_{j})_{j}\). Here, \(^{avg}\) re-weights each entity based on its relevancy with verb phrase \(_{i}\). Finally, we compute contrastive loss between this paired representations:

\[_{VEC}=-}_{p=1}^{N_{v}}_{p}^{avg})^{T}_{p}/)}{_{j B}(( _{p}^{avg})^{T}_{j}/)} \]

where we utilize batch formation technique from egocentric contrastive loss  to form negatives set in \(_{VEC}\).

**Projection Loss** operates on each individual frame of the input video, utilizing an external object detector  to identify bounding boxes \(b=\{b_{i}^{4}\}_{i=1}^{N_{b}}\) of scene entities. Let \(=\{_{i}(0,1)^{H W}\}_{i=1}^{E}\) be the predicted saliency maps of scene entities (see Appendix B.2 for saliency maps formulation), Hungarian matching pairs each detected box \(b_{i}\) with the predicted mask \(_{i}\) having the highest IoU.

Designing a differentiable loss function that guides the predicted mask \(_{j}\) by groundtruth box \(b_{j}\) is quite challenging. To address this, we utilize an axis projection function  to minimize the discrepancy of vertical and horizontal projections of \(b_{j}\) and \(_{j}\) on two axes. This ensures that the smallest box encompassing \(_{j}\) matches with \(b_{j}\). Concretely, \(b_{j}\) is firstly converted to binary mask format \(}_{j}\{0,1\}^{H W}\) where pixels inside \(b_{j}\) is assigned by 1 and 0 otherwise. Then, a projection loss is defined as follows:

\[_{proj}=}_{j=1}^{N_{b}}_{dice} _{y}(_{j}),_{y}(_{j})+_{ dice}_{x}(_{j}),_{x}(_{j})) \]

where \(_{dice}\) is a Dice loss function , \(_{y}()\) and \(_{x}()\) are max-project operators along \(y\)-axis and \(x\)-axis of the frame, respectively.

**Total Optimization.** Overall, our model is optimized with a weighted sum of EgoNCE loss over video-text pairs and three objectives stated above:

\[=_{ego}^{v2t}+_{ego}^{t2v}+_{1} _{NEC}+_{2}_{VEC}+_{3}_{proj} \]

where \(_{1}\), \(_{2}\), and \(_{3}\) balance contributions of different loss terms.

## 5 Experiments

### Training and Implementation Details

**Architecture.** We use video clip inputs of size \(224 224\), text inputs are tokenized and processed by a 12-layer Transformer following . We employ TimeSFormer  Base (TSF-B) for the global encoder. In the local entity encoder, all layers share a hidden dimension \(D=512\). Bootstrapping stage includes \(S_{1}=6\) cross-attention layers with 64 group tokens. Entity grouping stage consists of \(S_{2}=3\) DST blocks with 8 group tokens, followed by \(S_{3}=3\) DST blocks. Entity-aware decoder is a stack of 3 hybrid-attention blocks.

**Training.** HENASY is trained on EgoClip , which contains 3.8M clip-narration pairs covering a sub-set of 2,927 video hours from Ego4D . For each video clip, we uniformly sample 4 frames. We employ the pre-extracted narration's nouns and pre-detected hand and object bounding boxes from  for NEC loss and projection loss, respectively. For verb phrases, we employ Llama-2  with a prompt as discussed in Appendix C. The loss weights in Eq. 10 are set as: \(_{1}=0.5,_{2}=0.5,_{3}=1.0\). We train HENASY on two A6000 GPUs, in 5 epochs with AdamW optimizer  at fixed learning rate of \(3e-5\), and with batch size of 128. We initialize global encoder and text encoder with pretrained model provided from , but freeze them in the entire training process.

### Benchmarks and Evaluation Protocols

**Ego4D benchmarks .** Ego4D is the largest publicly available egocentric video dataset, featuring 3,670 hours of daily-life activity video for a wide range of benchmarks. We evaluate on three tasks:

* _EgoMCQ_: A multi-choice questions task to select the correct video clip from 5 candidates for each query. Accuracy is evaluated in intra-/inter-video (candidates from the same/different video).
* _EgoNLO_: A sub-task in episodic memory involving localizing video intervals that answer a given a free-form text query. Evaluation metrics include Recall@\(K\) for mIoU thresholds \(\), where \(K\{1,5\}\) and \(\{0.3,0.5\}\).
* _EgoMQ_: Also a sub-task of episodic memory, it involves identifying and categorizing action instances from 110 activity classes. Evaluation metrics are recalls (at mIoU=0.5) and mean Average Precision (mAP).

**EpicKitchens 100 benchmarks **: This dataset focuses on indoor and kitchen activities with 100 hours of video. We evaluate two tasks:

* _EK100-MIR_: A multi-instance retrieval task evaluating video and narration matching in both T\(\)V and V\(\)T. Metrics are mAP and normalized Discounted Cumulative Gain (nDCG).
* _EK100-CLS_: A action recognition task classifying videos into 300 noun classes or 97 verb classes. Metrics are Top-1 and Top-5 accuracy.

**EGTEA benchmark **: This dataset contains 28 hours of video with 106 classes. We evaluate fine-grained cooking action recognition _EGTEA_ and report Top-1 and mean accuracies.

**Evaluation Protocols.** We evaluate our model using three protocols:

* _Zero-Shot Transfer_ assesses generalization on unseen data and tasks without extra tuning. We conduct zero-shot evaluation EgoMCQ, EK100-MIR, EK100-CLS, and EGTEA.
* _Visual & Textual Representation_ is evaluated through EgoNLO and EgoMC, where we use our pre-trained model as a visual/textual feature extractor. Following , we train downstream models (VSLNet  for EgoNLO, VSGN  for EgoMC) with pre-computed features.
* _Vision-Language Grounding._ We evaluate local entity understanding and interpretation via qualitative results on EgoCLIP . We illustrate the saliency maps produced by our model and compare it with bounding boxes from .

    &  &  &  &  \\  & Inter & Intra &  &  &  &  &  &  \\  & & V-T & T-V & Avg & V-T & T-V & Avg & Acc & Acc & Acc & Acc \\  EgoVLP  & 90.6 & 57.2 & 26.0 & 20.6 & 23.3 & 28.8 & 27.0 & 27.9 & - & - & 17.6 & - \\ EgoVLPv2  & 91.0 & 60.9 & - & - & 26.7 & - & - & 29.1 & - & - & - & - \\ LaViLa  & 93.8 & 59.9 & 35.1 & 26.6 & 30.9 & 33.7 & 30.4 & 32.0 & 16.4 & 34.4 & 35.5 & 28.9 \\ HelpingHands*  & 93.2 & 58.8 & **35.6** & 26.8 & 31.2 & **34.7** & **31.7** & **33.2** & - & - & 35.3 & 29.4 \\ 
**Ours** & **94.1** & **61.3** & **35.5** & **27.1** & **31.3** & 34.6 & **31.7** & **33.2** & **19.5** & **38.2** & **35.9** & **29.6** \\   

Table 1: Comparison on the zero-shot transfer over EgoMCQ, EK100-MIR, EK100-CLS, and EGTEA. HelpingHands* refers to our re-produced results with TSF-B backbone using provided source code .

    &  &  \\   &  &  &  &  \\  & R1 & R5 & R1 & R5 & & & & \\  SlowFast  & 5.5 & 10.7 & 3.1 & 6.6 & 25.2 & 46.2 & 6.0 \\ EgoVLP  & 10.8 & 18.8 & 6.8 & 13.5 & **30.1** & **52.0** & 11.4 \\ LaViLa(B)  & 10.5 & 19.1 & 6.7 & 13.6 & 27.4 & 49.0 & 11.3 \\ HelpingHands*  & 11.2 & 20.4 & 6.9 & **14.7** & 27.5 & 49.0 & 11.7 \\
**Ours** & **11.5** & **21.5** & **7.0** & **14.7** & 28.3 & 51.0 & 12.4 \\   

Table 2: Comparison on the visual & textual representation over EgoNLO and EgoMQ. Grey indicates result we obtained using provided pre-trained checkpoint that.

### Main Results

**Comparison in Zero-shot Transfer.** In Table 1, to ensure fairness, we re-train HelpingHands  using their official codebase with TSF-B backbone and the same pre-trained weights as ours, provided by LaViLa . Our model consistently outperforms previous SOTA, achieving 3.1% improvement in top-1 accuracy on EK100-CLS, 0.5% and 0.3% increase in intra- and inter-video accuracy on EgoMCQ, and 0.5% improvement in mean accuracy on EGTEA. It also competes competitively with HelpingHands in the video/text retrieval EK100-MIR. Overall, our method demonstrates strong performance for zero-shot transfer across multiple benchmarks.

**Comparison in Visual & Textual Representation.** In Table 2, our method outperforms prior SOTA models across all metrics in EgoNLQ by adequate gaps. In EgoMQ, HENASY shows comparable performance, particularly excelling in mAP where it surpasses SOTA by 1%. This highlights HENASY's effectiveness when being applied to downstream models for features extraction.

**Vision-Language Grounding** We include qualitative experiment (Fig. 4) to compare with HelpingHands , which, in our knowledge, is the only VLM including weak visual grounding capacity via bounding boxes. As we can see, HENASY provides stronger interpretation with saliency maps reflecting dynamically evolving regions that most related to both appearance and motion queries. Furthermore, HelpingHands cannot correctly perform grounding with verb phrases (e.g., "scrolling the phone"), therefore, we show the bounding box of noun instead (e.g., "phone").

### Ablation Studies

**Losses:** We investigate various combinations of multi-grained loss components and report results in Table 3. We found that HENASY trained only with instance-level loss \(_{ego}\) yields 1-2% lower across all benchmarks compared to full loss setting. Besides, \(_{VEC}\) contributes slightly more to performance gains in EgoMCQ and EK100-MIR, compared to \(_{NEC}\). Finally, \(_{proj}\) shows a slight improvement of the overall performance.

**Impact of Entity-Aware Decoder:** We evaluate the influence of the entire-aware (EA) decoder on zero-shot tasks in the first two rows of Table 4. In the first experiment (labeled as 'w/ avg. pool'), the proposed EA decoder is omitted, and entity tokens are merely processed using average pooling to generate the video representation. This configuration results in a notable decline in performance across all benchmarks. In the second experiment (labeled as 'w/

Figure 4: **Vision-Language Grounding.** Qualitative comparisons with HelpingHands  on EgoCLIP . **Left:** comparison with a noun query obtained from narration and the pseudo-groundtruth boxes detected by  for reference. **Right:** verb phrase in the narration is used for comparison, as verb phrase cannot be captured by , we do not include pseudo boxes.

    &  &  &  &  \\ \(_{ego}\) & \(_{NEC}\) & \(_{VEC}\) & \(_{proj}\) & Inter & Intra &  Avg \\ mAP \\  &  Avg \\ nDCG \\  &  Top-1 \\ Acc \\  & 
 Top-5 \\ Acc \\  \\   &  &  &  & 93.4 & 58.4 & 30.8 & 32.7 & 18.2 & 36.8 \\  & & & & 93.6 & 59.9 & 30.9 & 32.8 & 19.1 & 37.5 \\  & & & & 93.7 & 59.7 & 31.1 & 32.9 & 18.9 & 37.3 \\  & & & & 93.2 & 58.5 & 30.8 & 32.6 & 18.5 & 37.0 \\  & & & & 94.0 & 61.1 & 31.3 & 33.0 & 19.3 & 37.7 \\  & & & & & 94.1 & 61.3 & 31.3 & 33.1 & 19.3 & 38.2 \\   

Table 3: Ablation results on multi-grained losses.

SA Dec.'), video patch tokens are combined with entity tokens and then input into a self-attention decoder, which has the same dimensions as our EA decoder. This setup leads to a performance decrease of approximately 2% across benchmarks compared to our proposed decoder ('complete settings').

These ablation studies show that modeling interactions between global features and entity embeddings plays a critical role, and the proposed design of entity-aware decoder significantly enhances overall model performance.

**Impact of Bootstrapping Stage:** We report the effect of bootstrapping stage in the third row of Table 4 (labeled as w/o bootstrap), where we remove bootstrapping stage by directly processing video patch tokens. The performance degrades by 1% across all benchmarks, showing the effectiveness of this design choice.

**Computational and Memory Costs:** We compare our method with HelpingHands  in Table 5. Our model is slightly more expensive but quite competitive in terms of memory requirements, the number of parameters and GFLOPs. Importantly, our inference time is 3 times faster than that of the HelpingHands. This superior running time of HENASY compared to HelpingHands can be attributed to HelpingHands' utilization of an autoregressive decoder, which reduces parallel computations and makes it less efficient despite its lower computational cost.

## 6 Conclusions

In this work, we explored the Hierarchical Entities Assembly framework, dubbed HENASY, which is designed to improve video representation of previous vision-language models by addressing their limitations in fine-grained modeling. Our model explicitly captures the dynamic interactions between visual entities to form a comprehensive video representation. Our experiments showed that HENASY outperforms existing SOTA methods across challenging egocentric video understanding benchmarks like EgoMCQ, EK100-MIR, EK100-CLS, EgoNLQ, and EgoMQ in both zero-shot transfer and feature extraction settings, while also demonstrating strong interpretation capabilities. Despite these strengths, there are several opportunities for future work to improve our model further.

**Limitations and Future Works** Although our focus has been on tasks utilizing ViT encoders for a variety of benchmarks, we believe it is important to extend HENASY to generative tasks such as video generation (e.g., stable diffusion) or to handle long-form videos. While HENASY can provide interpretability by focusing on relevant scene entities for both objects and actions, it is still limited in explicitly showing the interactions between scene entities. This necessitates the development of a dynamic scene graph, which remains an open question due to the unavailability of data.