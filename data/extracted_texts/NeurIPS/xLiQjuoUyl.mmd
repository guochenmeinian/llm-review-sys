# Winning Tickets from Random Initialization:

Aligning Masks for Sparse Training

 Rohan Jain\({}^{*1}\), Mohammed Adnan\({}^{*1,3}\), Ekansh Sharma\({}^{2,3}\), Yani Ioannou\({}^{1}\)

\({}^{1}\)University of Calgary \({}^{2}\)University of Toronto \({}^{3}\)Vector Institute for AI

{rohan.jain1,adnan.ahmad,yani.ioannou}@ucalgary.ca,ekansh@cs.toronto.edu

Equal contribution.

###### Abstract

The Lottery Ticket Hypothesis (LTH) suggests that there exists a sparse _winning ticket_ mask and weights that achieves the same generalization performance as the dense model while using much fewer parameters. LTH achieves this by iteratively sparsifying and re-training within the pruned solution basin. This procedure is expensive, and a winning ticket's sparsity mask does not generalize to other weight initializations. Recent work has suggested that Deep Neural Networks (DNNs) trained from random initialization find solutions within the same basin modulo weight symmetry, and proposed a method to align trained models within the same basins. We propose permuting the winning ticket mask to align with the new optimization basin when performing sparse training from a different random initialization than the one used to derive the pruned mask. Using this permuted mask, we show it is possible to significantly increase the generalization performance of sparse training from random initialization with the permuted mask as compared to sparse training naively using the non-permuted mask.

## 1 Introduction

In recent years, foundation models have achieved state-of-the-art results for different tasks. However, such an exponential increase in the size of state-of-the-art models requires a similarly exponential increase in the memory and computational costs required to train, store and use these models -- decreasing the accessibility of these models for researchers and practitioners alike. Seminal works have demonstrated that large models can be pruned after training with minimal loss in accuracy . While model pruning makes inference more efficient, it does not reduce the computational cost of training the model. Motivated by training a sparse model from a random initialization, the Lottery Ticket Hypothesis (LTH) proposes to solve the sparse training problem by reusing the same initialization as used to train the pruned models. On very small models, training from such an initialization maintains the generalization performance of the pruned model and demonstrates that training with a highly sparse mask is possible . In practice however, when training even modestly-sized models, _weight rewinding_ is necessary -- requiring significantly more compute than dense training alone.

Our hypothesis is that in order to reuse the LTH mask for different random initialization, the winning ticket mask obtained from LTH needs to be permuted such that it aligns with the optimization basin associated with this new initialization. We illustrate this intuition in Fig. 1. To empirically validate our hypothesis, we obtain a sparse mask using iterative magnitude pruning (IMP) on model \(A\) (from Fig. 1) and show that given a permutation that aligns the optimization basin of model \(A\) and a new random initialization, the mask can be reused. The sparse model (with the permuted mask) can be trained to match the generalization performance of the LTH solution.

## 2 Background & Related Work

Weight Symmetry.The process of training DNN's requires optimizing a non-linear function over a non-convex loss landscape consisting of numerous local minima, narrow ravines, plateaus, saddle points and _basins_. [4; 5; 7; 15; 26; 28; 37]. Despite non-convex optimization problems being NP-hard , the nature of first order stochastic optimizers such as SGD  have been theoretically proven to be highly effective in optimizing DNN's [18; 21] and in practice as well. Empirical evidence suggests that when training independent NN's using SGD, with different batch orders and initializations, the resulting training trajectories often exhibit remarkable similarities [1; 36]. With the rise of building larger NN's, this puzzling phenomenon has been attributed to overparameterization, which is responsible for creating numerous minima in the loss landscape, resulting in multiple different functions which fit the data similarly [23; 27]. However, as early as the 1990s, Hecht-Nielsen  demonstrated that neural networks are _permutation invariant_ possessing a weight-symmetrical property, where swapping any two neurons within a hidden layer does not alter the underlying function being computed. In other words, the permuted network remains functionally equivalent to its original configuration. Hence, the existence of permutation symmetries in the loss landscape contribute to its non-convexity, as it creates copies of global minima at different points in weight space [3; 8; 14].

Linear Mode Connectivity _modulo_ Permutation.Typically, linearly interpolating between the weights of two independently trained networks usually results in a higher loss/0-1 error compared to the two endpoints. Entezari et al.  conjectured, that independently obtained SGD solutions have no error barrier if one accounts for the permutation symmetries. Building on this conjecture, several algorithms have been developed to address permutation invariance by aligning trained networks to the same optimization basin [1; 22; 34; 35]. Ainsworth et al.  demonstrate that DNN's trained from random initialization find solutions within the same basin modulo permutation symmetry. They proposed three algorithms to permute the units of one model to align it with a reference model, enabling the permuted model to exhibit LMC (i.e. reduced loss barrier) with the reference model. Benzing et al.  use a permutation found after training to exhibit LMC between networks at initialization. The use of activation matching for model alignment was originally introduced by Li et al.  to ensure models learn similar representations when performing the same task. A rigorous study from Sharma et al.  introduced a notion of _simultaneous weak linear connectivity_ where a permutation, \(\) aligning two networks also simultaneously aligns two larger fully trained networks throughout the entire SGD trajectory and the same \(\) also aligns successive iterations of independently sparsified networks found via IMP. This work also provided early evidence towards re-usability of sparse masks.

Figure 1: **Weight Symmetry and the Sparse Training Problem**. A model with a single layer and only two parameters, \(=(w_{0},w_{1})\), operating on a single input scale \(x_{0}\) has the weight symmetry in the 2D loss landscape as illustrated above. In (a) the original dense model, \(_{A}\), is trained from a random dense initialization, \(_{A}^{t=0}\) to a dense solution, \(_{A}^{t=T}\), which is then pruned using weight magnitude resulting in the mask \(_{A}=(1,0)\). In (b), naively using the same mask to train a model, B, from a different random initialization will likely result in the initialization being far from a good solution. Permuting the mask to match the (symmetric) basin in which the new initialization is in will enable sparse training.

Lottery Ticket Hypothesis.Neural network pruning is a highly effective method of reducing the parameters in a trained dense neural network, pruning as many as 85-95% of weights while not significantly affecting generalization performance [16; 17]. The LTH proposes to solve the sparse training problem by re-using the same initialization as used to train the pruned models. For very small models, training from such an initialization maintains the generalization performance of the pruned model, and demonstrates that training with a highly sparse mask is possible . In practice however, subsequent work has shown that when training modestly-sized models requires using _weight rewinding_ -- requiring significantly more compute than dense training alone. Furthermore, recent work has shown that the LTH effectively re-learns the pruned solution . To make any practical use of sparse training, finding methods of sparse training from random initialization may be necessary to both find new solutions, and realize any efficiency gains in training.

## 3 Method

Motivation.In this work, we try to understand _why LTH masks fail to transfer to a new random initialization_. Our hypothesis is that the loss basin corresponding to the LTH mask is not aligned with the new random initialization as shown in Fig. 1. Since the sparse mask is not in alignment with the basin for the new random initialization, sparse training does not work well; therefore, aligning the LTH mask with new random initialization will improve sparse training and enable the transfer of LTH masks to random initializations.

Aligning Masks via Weight Symmetry.Ainsworth et al.  showed the permutation symmetries of the weight space can be leveraged to align the basin of two models trained from different random initializations. In their approach, the authors utilize activation matching to align the activations of two models. By permuting the parameters of the second model, they maximize the correlation between the activations of the first and second models. This method fits within the framework of solving a linear assignment problem (LAP), enabling efficient computation. In our experiments, we train two dense models, \(_{A}^{t=0}\) and \(_{B}^{t=0}\), to convergence and then use activation matching (implemented by Jordan et al. ) to find the permutation mapping \(\), such that the activations of \((_{A}^{t=T})\) and \(_{B}^{t=T}\) are aligned. Mask \(_{A}\), obtained using IMP is also permuted with the same permutation map \(\). The intuition is that the permuted mask is aligned with the loss basin of model \(_{B}^{t=T}\) and thus can be optimized easily (refer to Fig. 2). We denote training with the permuted mask, \((_{A})\) as _permuted_ and with the non-permuted mask, \(_{A}\) as _naive_. More details in Appendix A.5.

Sparse Training.For evaluating the transferability of LTH masks, we use a new random initialization \(_{B}^{t=0}\) and sparse masks \(_{A}\) and \((_{A})\) for sparse training the naive and permuted solution respectfully. We also evaluate the LTH baseline, i.e., training model \(_{A}^{t=0}\) with mask \(_{A}\). Since LTH requires weight rewinding to an earlier point in training, we also use a rewound checkpoint from epoch \(t\!=\!k\!\!T\) for both the baselines and permuted solution.

## 4 Results

To validate our hypothesis, we trained ResNet20  and VGG11  models on CIFAR-10/100 datasets  (details in Appendix A.1). We used the same set of hyper-parameters for training different sparse baselines and our permuted solution (details in Appendix A.1).

ResNet.We trained ResNet20 on CIFAR-10/100 datasets. As shown in Figs. 4 and 5, consistent across varying width and different datasets both LTH and permuted solution improve as the rewind point increases in contrast to the naive solution, which does not improve on increasing the rewind point. We observed that naive performance saturates after \(k\!\!50\) and does not yield further improvement. Since it is more difficult to train models with higher sparsity, the gap between naive and permuted solutions increases as sparsity increases, as shown in Figs. 3(d), 3(h) and 3(l). The improved performance of permuted solution supports our hypothesis and shows that misalignment of LTH masks and loss basin corresponding to new random initialization may be the reason why LTH masks do not transfer to different initializations. We also show accuracy vs sparsity plots for \(k\!=\!\{10,\!25,\!50,\!100\}\) (details in Appendix A.4); as sparsity increases, the gap between permuted and naive solution increases for all rewind points (see Appendix A.3). Results for CIFAR-100 dataset is shown in Fig. 6 in Appendix A.3.

Vgg11.We utilize the modified VGG11 architecture implemented by Jordan et al.  trained on CIFAR-10 (details in Appendix A.1). We observe that for a moderate sparsity Fig. 2(a), the permuted and naive solution are relatively similar and steadily increasing together as we increase the rewind points with the permuted solution consistently taking the slight edge over naive. As sparsity increases in Fig. 2(b), a significant gap begins to emerge between the permuted and naive solutions. As the rewind point increases, the permuted solution gradually improves and approaches the performance of LTH, while the naive solution significantly plateaus for \(k 20\) and performance subsides.

Effect of width multiplier.The activation matching algorithm proposed by Ainsworth et al.  does not find the global optimum; rather, it uses a greedy search to explore a restricted solution space. The resulting permutation mapping aligns well in practice, especially for wider models . We also examine how increasing width multipliers affect aligning ResNets in weight space. As illustrated in Fig. 4, the permuted solutions increasingly match the LTH solution as the model width expands. This finding is notable, as Ainsworth et al.  also observed improved weight matching in wider models, an effect further validated by Sharma et al.  for activation matching. This trend may also help explain why the VGG results in Fig. 3--an exceptionally over-parameterized model for CIFAR-10--are so close to the LTH baseline. If the hypothesis by Ainsworth et al.  holds--neural network loss landscapes nearly contain a single solution basin modulo weight symmetry--then with an ideal permutation mapping, the permuted solution would match the LTH solution. However, our experiments still seem to corroborate our hypothesis and may provide insights into why LTH does not transfer well to new initializations.

Figure 3: **VGG11\(\{1\}\)/CIFAR-10.** Test accuracy of sparse network solutions at increasing rewound points at various sparsity levels. The dashed (**-** ) line shows the dense model accuracy. **Note**, we do not include the plots for sparsity = {0.95, 0.97}, because the naive solution performs poorly yielding consistent metrics of test accuracy \(=10\%\) and test loss \(_{}(10)\) throughout all rewind points.

Figure 2: The overall framework of the training procedure, beginning with two distinct dense random weight initializations, \(_{A}^{t=0}\), \(_{B}^{t=0}\) sampled from a normal distribution, \(\). The sparse training problem attempts to train the random initialization, \(_{B}^{t=0}\) using the naive mask \(_{A}\), found by pruning a dense trained model, \(_{A}^{t=T}\). However, this results in poor generalization performance . We propose to instead train \(_{B}^{t=k}\) at some rewound epoch \(k\), equipped with a _permated_ mask \((_{A})\). We show that this achieves more comparable generalization to the pruned model/trained LTH solution, \(_{A}^{t=T}_{A}\). More details in Appendix A.5.

## 5 Conclusion

Sparse training and the Lottery Ticket Hypothesis (LTH) have gained significant traction in recent years. In this work, we seek to deepen insights into sparse training from random initialization and the LTH by leveraging permutation invariance in DNNs. Our empirical findings across various models and datasets support the hypothesis that misalignment between the mask and loss basin prevents effective use of LTH masks with new initializations. One limitation is that activation matching is weaker for narrow models; future work will explore more efficient matching algorithms.

## 6 Acknowledgements

We acknowledge the support of Alberta Innovates (ALLRP-577350-22, ALLRP-222301502), the Natural Sciences and Engineering Research Council of Canada (RGPIN-2022-03120, DGECR-2022-00358), and Defence Research and Development Canada (DGDND-2022-03120). This research was enabled in part by support provided by the Digital Research Alliance of Canada (alliancecan.ca).

Figure 4: **ResNet20\(\{1,4,8\}\)/CIFAR-10**. The top, middle, and bottom rows correspond to widths of 1, 4, and 8, respectively. The effect of the rewind points on the test accuracy for different sparsities is shown. As the sparsity and rewind epoch increase, the gap between training from a random initialization with the permuted mask and the LTH/dense baseline (dashed line) decreases, unlike training with a non-permuted mask (naive).

Figure 5: **ResNet20\(\{8\}\)/CIFAR-100**. Trained ResNet20 with a width of 8 on CIFAR-100. Our results show that, as sparsity increases, the gap between the permuted and naive solutions increase and the permuted solution gradually approaches the LTH baseline. The dashed (- -) line shows the dense model accuracy.