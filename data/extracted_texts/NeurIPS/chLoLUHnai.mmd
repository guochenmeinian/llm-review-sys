# Large Stepsize Gradient Descent for

Non-Homogeneous Two-Layer Networks:

Margin Improvement and Fast Optimization

 Yuhang Cai\({}^{1}\)  Jingfeng Wu\({}^{1}\)  Song Mei\({}^{1}\)  Michael Lindsey\({}^{1,2}\)  Peter L. Bartlett\({}^{1,3}\)

\({}^{1}\)UC Berkeley \({}^{2}\)Lawrence Berkeley National Laboratory \({}^{3}\)Google DeepMind

{willcai,uuujf,songmei,lindsey,peter}@berkeley.edu

###### Abstract

The typical training of neural networks using large stepsize gradient descent (GD) under the logistic loss often involves two distinct phases, where the empirical risk oscillates in the first phase but decreases monotonically in the second phase. We investigate this phenomenon in two-layer networks that satisfy a near-homogeneity condition. We show that the second phase begins once the empirical risk falls below a certain threshold, dependent on the stepsize. Additionally, we show that the normalized margin grows nearly monotonically in the second phase, demonstrating an implicit bias of GD in training non-homogeneous predictors. If the dataset is linearly separable and the derivative of the activation function is bounded away from zero, we show that the average empirical risk decreases, implying that the first phase must stop in finite steps. Finally, we demonstrate that by choosing a suitably large stepsize, GD that undergoes this phase transition is more efficient than GD that monotonically decreases the risk. Our analysis applies to networks of any width, beyond the well-known neural tangent kernel and mean-field regimes.

## 1 Introduction

Neural networks are mostly optimized by _gradient descent_ (GD) or its variants. Understanding the behavior of GD is one of the key challenges in deep learning theory. However, there is a nonnegligible discrepancy between the GD setups in theory and in practice. In theory, GD is mostly analyzed with relatively small stepsizes such that its dynamics are close to the continuous _gradient flow_ dynamics, although a few exceptions will be discussed later. However, in practice, GD is often used with a relatively large stepsize, with behaviors significantly deviating from that of small stepsize GD or gradient flow. Specifically, notice that small stepsize GD (hence also gradient flow) induces monotonically decreasing empirical risk, but in practice, good optimization and generalization performance is usually achieved when the stepsize is large and the empirical risk oscillates [see Wu and Ma, 2018, Cohen et al., 2020, for example]. Therefore, it is unclear which of the theoretical insights drawn from analyzing small stepsize GD apply to large stepsize GD used practically.

The behavior of small stepsize GD is relatively well understood. For instance, classical optimization theory suggests that GD minimizes convex and \(L\)-smooth functions if the stepsize \(\) is well below \(2/L\), with a convergence rate of \((1/(t))\), where \(t\) is the number of steps [Nesterov, 2018]. More recently, Soudry et al. , Ji and Telgarsky  show an _implicit bias_ of small stepsize GD in logistic regression with separable data, where the direction of the GD iterates converges to the max-margin direction. Subsequent works extend their implicit bias theory from linear model to _homogenous_ networks [Lyu and Li, 2020, Chizat and Bach, 2020, Ji and Telgarsky, 2020]. These theoretical results all assume the stepsize of GD is small (and even infinitesimal) such that the empirical risk decreases monotonically and, therefore cannot be directly applied to large stepsize GD used in practice.

More recently, large stepsize GD that induces oscillatory risk has been analyzed in simplified setups (see Ahn et al., 2023; Zhu et al., 2022; Kreisler et al., 2023; Chen and Bruna, 2023; Wang et al., 2022; Wu et al., 2023; Wu et al., 2024, for an incomplete list of references). In particular, in logistic regression with linearly separable data, Wu et al.  showed that the implicit bias of GD (that maximizes the margin) holds not only for small stepsizes (Soudry et al., 2018; Ji and Telgarsky, 2018) but also for an arbitrarily large stepsize. In the same problem, Wu et al.  further showed that large stepsize GD that undergoes risk oscillation can achieve an \(}(1/t^{2})\) empirical risk, whereas small stepsize GD that monotonically decreases the empirical risk must suffer from a \((1/t)\) empirical risk. Nonetheless, these theories of large stepsize GD are limited to relatively simple setups such as linear models. The theory of large stepsize GD for nonlinear networks is underdeveloped.

This work fills the gap by providing an analysis of large stepsize GD for nonlinear networks. In the following, we set up our problem formally and summarize our contributions.

Setup.Consider a binary classification dataset \((_{i},y_{i})_{i=1}^{n}\), where \(_{i}^{d}\) is a feature vector and \(y_{i}\{ 1\}\) is a binary label. For simplicity, we assume \(\|_{i}\| 1\) for all \(i\) throughout the paper. For a predictor \(f\), the empirical risk under logistic loss is defined as

\[L():=_{i=1}^{n}(y_{i}f(; _{i})),(t):=(1+e^{-t}). \]

Here, the predictor \(f(;\,\,):^{d}\) is parameterized by trainable parameters \(\) and is assumed to be continuously differentiable with respect to \(\). The predictor is initialized from \(_{0}\) and then trained by _gradient descent_ (GD) with a constant stepsize \(>0\), that is,

\[_{t+1}:=_{t}- L(_{t}), t 0.\] (GD)

We are interested in a nonlinear predictor \(f\) and a large stepsize \(\). A notable example in our theory is two-layer networks with Lipschitz, smooth, and nearly homogenous activations (see (2)). Note that minimizing \(L()\) is a non-convex problem in general.

Observation.Empirically, large stepsize GD often undergoes a phase transition, where the empirical risk defined in (1) oscillates in the first phase but decreases monotonically in the second phase (see empirical evidence in Appendix A in (Cohen et al., 2020) and a formal proof in (Wu et al., 2024) for linear predictors). This is illustrated in Figure 1. We follow Wu et al.  and call the two phases the _edge of stability_ (EoS) phase [name coined by Cohen et al., 2020] and the _stable_ phase, respectively.

Figure 1: The behavior of (GD) for optimizing a non-homogenous four-layer MLP with GELU activation function on a subset of CIFAR-10 dataset. We randomly sample \(6,000\) data with labels “airplane” and “automobile” from CIFAR-10 dataset. The normalized margin is defined as \(_{i[]}y_{i}f(_{t};_{i})/\|_{t}\|^{4}\), which is close to (3). The blue curves correspond to GD with a large stepsize \(=0.2\), where the empirical risk oscillates in the first phase but decreases monotonically in the second phase. The orange curves correspond to GD with a small stepsize \(=0.005\), where the empirical risk decreases monotonically. Furthermore, Figure 1(b) suggests the normalized margins of both two curves increase and converge in the stable phases. Finally, Figure 1(c) suggests that large stepsize achieves a better test accuracy, consistent with larger-scale learning experiment (Hoffer et al., 2017; Goyal et al., 2017). More details can be found in Section 5.

Contributions.We prove the following results for large stepsize GD for training nonlinear predictors under logistic loss.

1. For Lipschitz and smooth predictor \(f\) trained by GD with stepsize \(\), we show that as long as the empirical risk is below a threshold depending on \(\), GD monotonically decreases the empirical risk (see Theorem 2.2). This result extends the stable phase result in Wu et al. (2024) from linear predictors to nonlinear predictors, demonstrating the generality of the existence of a stable phase.
2. Assuming that GD enters the stable phase, if in addition the predator has a bounded homogenous error (see Assumption 1C), we show that the normalized margin induced by GD, \(_{i}y_{i}f(_{t};_{i})/\|_{t}\|\), nearly monotonically increases (see Theorem 2.2). To the best of our knowledge, this is the first characterization of implicit bias of GD for _non-homogenous_ predictors. In particular, our theory covers two-layer networks with commonly used activations functions (which are often non-homogenous) that cannot be covered by existing results (Lyu and Li, 2020; Ji and Telgarsky, 2020; Chizat and Bach, 2020).
3. Under additional technical assumptions (the dataset is linearly separable and the derivative of the activation function is bounded away from zero), we show that the initial EoS phase must stop in \(()\) steps and GD transits to the stable phase afterwards. Furthermore, by choosing a suitably large stepsize, GD achieves a \(}(1/t^{2})\) empirical risk after \(t\) steps. In comparison, GD that converges monotonically incurs an \((1/t)\) risk. This result indicates an optimization benefit of using large stepsize and generalizes the results in (Wu et al., 2024) from linear predictors to neural networks.

## 2 Stable Phase and Margin Improvement

In this section, we present our results for the stable phase of large stepsize GD in training nonlinear predictors. Specifically, our results apply to nonlinear predictors that are Lipschitz, smooth, and nearly homogeneous, as described by the following assumption.

**Assumption 1** (Model conditions).: _Consider a predictor \(f(;_{i})\), where \(_{i}\) is one of the feature vectors in the training set._

1. _Lipschitzness__. Assume there exists_ \(>0\) _such that for every_ \(\)_,_ \(_{i}\|_{}f(;_{i})\|\)_._
2. _Smoothness__. Assume there exists_ \(>0\) _such that for all_ \(,\)_,_ \[\| f(;_{i})- f(;_{i})\| \|-\|, i=1,,n.\]
3. _Near homogeneity__. Assume there exists_ \(>0\) _such that for every_ \(\)_,_ \[|f(;_{i})-_{}f(;_{i}),|, i=1,,n.\]

Assumptions 1A and 1B are commonly used conditions in the optimization literature. Note that Assumption 1B implies continuous differentiability, thus ruling out networks with ReLU activation function. The continuous differentiability is only used in our current stable phase analysis. We conjecture it can be relaxed using subdifferentiability (Lyu and Li, 2020) for allowing ReLU networks.

If \(=0\), then Assumption 1C requires the predictor to be exactly \(1\)-homogenous. Our Assumption 1C allows the predictor to have a bounded homogenous error. It is clear that linear predictors \(f(;):=^{}\) satisfy Assumption 1 with \(=_{i}\|_{i}\| 1\), \(=0\), and \(=0\). Another notable example is _two-layer networks_ given by

\[f(;):=_{j=1}^{m}a_{j}(^{ }^{(j)}),^{(j)}^{d}, j=1,,m, \]

where we assume \(a_{j}\{ 1\}\) are fixed and \(^{md}\), the stack of \((^{(j)})_{j=1}^{m}\), are the trainable parameters. We define two-layer networks with the mean-field scaling (Song et al., 2018; Chizat and Bach, 2020; Chen et al., 2022; Suzuki et al., 2023). However, our results hold for any width. The effect of rescaling the model will be discussed in Section 4. The following example shows that Assumption 1 covers two-layer networks with many commonly used activations \(()\). The proof is provided in Appendix F.1.

**Example 2.1** (Two-layer networks).: _Two-layer networks defined in (2) with the following activation functions satisfy Assumption 1 with the described constants:_

* _GELU._ \((x):=1+(x/)\) _with_ \(=e^{-1/2}/\)_,_ \(=2/m\)_, and_ \(=(+e^{-1/2})/\)_._
* _Softplus._ \((x):=(1+e^{x})\) _with_ \(= 2\)_,_ \(=1/m\)_, and_ \(=1/\)_._
* _SiLU._ \((x):=x/(1+e^{-x})\) _with_ \(=1\)_,_ \(=4/m\)_, and_ \(=2/\)_._
* _Huberized ReLU_ _(_Chatterji et al._,_ 2021_)__. For a fixed_ \(h>0\)_,_ \[(x):=0&x<0,\\ }{2h}&0 x h,\\ x-&x>h,\] _with_ \(=h/2\)_,_ \(=1/(hm)\)_, and_ \(=1/\)_._

Margin for nearly homogenous predictors.For a nearly homogenous predictor \(f(;)\) (see Assumption 1C), we define its _normalized margin_ (or _margin_ for simplicity) as

\[():=y_{i}f(;_{i })}{\|\|}. \]

A large normalized margin \(()\) guarantees the prediction of each sample is away from the decision boundary. The normalized margin (3) is introduced by Lyu and Li (2020) for homogenous predictors. However, we show that the same notion is also well-defined for non-homogenous predictors that satisfy Assumption 1C. The next theorem gives sufficient conditions for large stepsize GD to enter the stable phase in training non-homogenous predictors and characterizes the increase of the normalized margin. The proof of Theorem 2.2 is deferred to Appendix A.

**Theorem 2.2** (Stable phase and margin improvement).: _Consider (GD) with stepsize \(\) on a predictor \(f(;)\) that satisfies Assumptions 1A and 1B. If there exists \(r 0\) such that_

\[L(_{r})(2^{2}+)}, \]

_then GD is in the stable phase for \(t r\), that is, \((L(_{t}))_{t r}\) decreases monotonically. If additionally the predictor satisfies Assumption 1C and there exists \(s 0\) such that_

\[L(_{s})2n},\ (4^{2}+2)}}, \]

_we have the following for \(t s\):_

* _Risk convergence._ \(L(_{t})=(1/t)\)_._
* _Parameter increase._ \(\|_{t+1}\|\|_{t}\|\) _and_ \(\|_{t}\|=((t))\)_._
* _Margin improvement._ _There exists a modified margin function_ \(^{c}()\) _such that_
* \(^{c}(_{t})\) _is increasing and bounded._
* \(^{c}(_{t})\) _is a multiplicative approximiator of_ \((_{t})\)_, that is, there exists_ \(c>0\) _such that_ \[^{c}(_{t})(_{t})1+ {c}{(1/L(_{t}))}^{c}(_{t}), t s.\] _As a direct consequence,_ \(_{t}(_{t})=_{t}^{c}( _{t})\)_._

Theorem 2.2 shows that for an arbitrarily large stepsize \(\), GD must enter the stable phase if the empirical risk falls below a threshold depending on \(\) given by (4). Furthermore, for nearly homogenous predictors, Theorem 2.2 shows that under a stronger risk threshold condition (5), the risk must converge at a \((1/t)\) rate and that the normalized margin nearly monotonically increases. This demonstrates an implicit bias of GD, even when used with a large stepsize and the trained predictor is non-homogenous.

Limitations.The stable phase conditions in Theorem 2.2 require GD to enter a sublevel set of the empirical risk. However, such a sublevel set might be empty. For instance, let \(f(;)\) be a two-layer network (2) with sigmoid activation. Notice that the predictor is uniformly bounded, \(|f(;)| 1\), so we have

\[L()=_{i=1}^{n}(1+e^{-y_{i}f(; _{i})})(1+e^{-1}).\]

On the other hand, we can also verify that Assumption 1C is satisfied by \(f(;)\) with \(=1\) but no smaller \(\). Therefore (5) cannot be satisfied. In general, the sublevel set given by the right-hand side of Assumption 1C is non-empty if

\[_{i}y_{i}f( ;_{i}).\]

The above condition requires the data can be separated arbitrarily well by some predictor within the hypothesis class. This condition is general and covers (sufficiently large) two-layer networks (2) with many commonly used activations such as GeLU and SiLU. Moreover, although two-layer networks with sigmoid activation violate this condition, they can be modified by adding a leakage to the sigmoid to satisfy the condition. Furthermore, this condition can be satisfied for some nonlinear problems like XOR (or \(k\)-parity problems) since they can be realized by two-layer networks.

In the next section, we will provide sufficient conditions such that large stepsize GD will enter the stable phases characterized by (4) or (5).

Comparisons to existing works.Our Theorem 2.2 makes several important extensions compared to existing results (Wu et al., 2024; Lyu and Li, 2020; Chizat and Bach, 2020; Ji and Telgarsky, 2020). First, Theorem 2.2 suggests that the stable phase happens for general nonlinear predictors such as two-layer networks, while the work by Wu et al. (2024) only studied the stable phase for linear predictors. Second, the margin improvement is only known for small (and even infinitesimal) stepsize GD and homogenous predictors (Lyu and Li, 2020; Chizat and Bach, 2020; Ji and Telgarsky, 2020), while we extend this to non-homogenous networks. To the best of our knowledge, Theorem 2.2 is the first implicit bias result covering large stepsize GD and non-homogenous predictors.

From a technical perspective, our proof uses techniques introduced by Lyu and Li (2020) for analyzing homogenous predictors. Our main innovation is the construction of new auxiliary margin functions that can deal with errors caused by large stepsize and non-homogeneity. More details are discussed in Appendix A.3.

## 3 Edge of Stability Phase

Our stable phase results in Theorem 2.2 require the risk to be below a certain threshold (see (4) and (5)). In this section, we show that the risk can indeed be below the required threshold, even when GD is used with large stepsize. Recall that minimizing the empirical risk with a nonlinear predictor is non-convex, therefore solving it by GD is hard in general. We make additional technical assumptions to conquer the challenges caused by non-convexity. We conjecture that these technical assumptions are not necessary and can be relaxed.

We focus on two-layer networks (2). We make the following assumptions on the activation function.

**Assumption 2** (Activation function conditions).: _In the two-layer network (2), let the activation function \(:\) be continuously differentiable. Moreover,_

1. _Derivative condition__. Assume there exists_ \(0<<1\) _such that_ \(|^{}(z)| 1\)_._
2. _Smoothness__. Assume there exists_ \(>0\) _such that for all_ \(x,y\)_,_ \(|^{}(x)-^{}(y)||x-y|\)_._
3. _Near homogeneity__. Assume there exists_ \(>0\) _such that for every_ \(z\)_,_ \(|(z)-^{}(z)z|\)_._

Recall that \(_{i}\|_{i}\| 1\). One can then check by direct computation that, under Assumption 2, two-layer networks (2) satisfy Assumption 1 with \(=1/\), \(=/m\), and \(=\).

Assumptions 2B and 2C cover many commonly used activation functions. In Assumption 2A, we assume \(|^{}(z)| 1\). This is just for the simplicity of presentation and our results can be easily generalized to allow \(|^{}(z)| C\) for a constant \(C>0\). The other condition in Assumption 2A, \(|^{}(z)|\), however, is non-trivial. This condition is widely used in literature (see Brutzkus et al., 2018; Frei et al., 2021, and references thereafter) to facilitate GD analysis. Technically, this condition guarantees that each neuron in the two-layer network (2) will always receive a non-trivial gradient in the GD update; otherwise, neurons may be frozen during the GD update. Furthermore, commonly used activation functions can be combined with an identity map to satisfy Assumption 2A. This is formalized in the following example. The proof is provided in Appendix F.2.

**Example 3.1** (Leaky activation functions).: _Fix \(0.5 c<1\)._

* _Let_ \(\) _be GELU, Softplus, or SilU in Example_ 2.1_, then its modification_ \((x):=cx+(1-c)/4(x)\) _satisfies Assumption_ 2 _with_ \(=1\)_,_ \(=0.25\)_, and_ \(=1\)_. In particular, the modification of softplus can be viewed as a_ smoothed _leaky ReLU._
* _Let_ \(\) _be the Huberized ReLU in Example_ 2.1_, then its modification_ \((x):=cx+(1-c)/4(x)\) _satisfies Assumption_ 2 _with_ \(=h/2\)_,_ \(=0.5\)_, and_ \(=1/4h\)_._
* _The "leaky" tanh,_ \((x):=cx+(1-c)(x)\)_, and the "leaky" sigmoid,_ \((x):=cx+c/(1+e^{-x})\)_, both satisfy Assumption_ 2 _with_ \(=1,=0.5\) _and_ \(=1\)_._

For the technical difficulty of non-convex optimization, we also need to assume a linearly separable dataset to conduct our EoS phase analysis.

**Assumption 3** (Linear separability).: _Assume there is a margin \(>0\) and a unit vector \(_{*}\) such that \(y_{i}_{i}^{}_{*}\) for every \(i=1,,n\)._

Assumption 3 serves as a sufficient condition for two-layer neural networks, regardless of width, to reach the initial bound of the stable phase under large stepsizes. We remark that our stable phase results do not need this assumption.

The following theorem shows that when GD is used with large stepsizes, the average risk must decrease even though the risk may oscillate locally.

**Theorem 3.2** (The EoS phase for two-layer networks).: _Under Assumption 3, consider (GD) on two-layer networks (2) that satisfy Assumptions 2A and 2C. Denote the stepsize by \(:=m\), where \(m\) is the network width and \(\) can be arbitrarily large. Then for every \(t>0\), we have_

\[_{k=0}^{t-1}L(_{k})(^{2}  t)/^{2}+8^{2}/^{2}+^{2}}{^{2} t}+_{0}\|^{2}}{m t}=( t)+ ^{2}}{ t}.\]

Theorem 3.2 suggests that the average risk of training two-layer networks decreases even when GD is used with large stepsize. Consequently, the risk thresholds (4) and (5) for GD to enter the stable phase must be satisfied after a finite number of steps. This will be discussed in depth in the next section.

Compared to Theorem 1 in (Wu et al., 2024), Theorem 3.2 extends their EoS phase bound from linear predictors to two-layer networks.

## 4 Phase Transition and Fast Optimization

For two-layer networks trained by large stepsize GD, Theorem 3.2 shows that the average risk must decrease over time. Combining this with Theorem 2.2, GD must enter the stable phase in finite steps, and the loss must converge while the normalized margin must improve.

However, a direct application of Theorem 3.2 only leads to a suboptimal bound on the phase transition time. Motivated by Wu et al. (2024), we establish the following sharp bound on the phase transition time by tracking the gradient potential (see Lemma C.3). The proof is deferred to Appendix C.

**Theorem 4.1** (Phase transition and stable phase for two-layer networks).: _Under Assumption 3, consider (GD) on two-layer networks (2) that satisfy Assumption 2. Clearly, the two-layer networks also satisfy Assumption 1 with \(=1/\), \(=/m\), and \(=\). Denote the stepsize by \(:=m\), where \(m\) is the network width and \(>0\) can be arbitrarily large._* _Phase transition time__. There exists_ \(s\) _such that (_5_) in Theorem_ 2.2 _holds, where_ \[:=}c_{1},c_{2}n,e, +c_{1}n}{}+c_{1}n}{},+c_{1}n)}{ }_{0}\|}{}},\] _where_ \(c_{1}:=4e^{+2}\) _and_ \(c_{2}:=(8+4)\)_. Therefore_ (GD) _is in the stable phase from_ \(s\) _onwards._
* _Explicit risk bound in the stable phase__. We have_ \((L(_{t}))_{t s}\) _monotonically decreases and_ \[L(_{t})^{2}(t-s)}, t s.\]

Theorems 2.2, 3.2 and 4.1 together characterize the behaviors of large stepsize GD in training two-layer networks. Specifically, large stepsize GD may induce an oscillatory risk in the beginning; but the averaged empirical risk must decrease (Theorem 3.2). After the empirical risk falls below a certain stepsize-dependent threshold, GD enters the stable phase, where the risk decreases monotonically (Theorem 4.1). Finally, the normalized margin (3) induced by GD increases nearly monotonically as GD stays in the stable phase (Theorem 2.2).

Our intuition behind the phase transition phenomenon is as follows. The initial EoS phase occurs when gGD oscillates within a steep valley, transitioning to a stable phase once it navigates into a flatter valley. We believe this insight generalizes to broader nonlinear models. Moreover, our theory of large step sizes aligns with the celebrated flat minima intuition (Keskar et al., 2016).

Fast optimization.Our bounds for two-layer networks are comparable to those for linear predictors shown by Wu et al. (2024). Specifically, when used with a larger stepsize, GD achieves a faster optimization in the stable phase but stays longer in the EoS phase. Choosing a suitably large stepsize that balances the steps in EoS and stable phases, we obtain an _accelerated_ empirical risk in the following corollary. The proof is included in Appendix C.2.

**Corollary 4.2** (Acceleration of large stepsize).: _Under the same setup as in Theorem 4.1, consider (GD) with a given budget of \(T\) steps such that_

\[T^{2}}c_{1}n,\;4c_{2}^{ 2},\;\|_{0}\|}{}},\]

_where \(c_{1}:=4e^{+2}\) and \(c_{2}:=(8+4)\) are as in Theorem 4.1. Then for stepsize \(:= m\), where_

\[:=^{2}}{256(1+4)c_{2}}T,\]

_we have \( T/2\) and_

Theorem 4.1 and Corollary 4.2 extend Theorem 1 and Corollary 2 in Wu et al. (2024) from linear predictors to two-layer networks. Another notable difference is that we obtain a sharper stable phase bound (and thus a better acceleration bound) compared to theirs, where we remove a logarithmic factor through a more careful analysis.

Corollary 4.2 suggests an accelerated risk bound of \((1/T^{2})\) by choosing a large stepsize that balances EoS and stable phases. We also show the following lower bound, showing that such acceleration is impossible if (GD) does not enter the EoS phase. The proof is included in Appendix C.3.

**Theorem 4.3** (Lower bound in the classical regime).: _Consider (GD) with initialization \(_{0}=0\) and stepsize \(>0\) for a two-layer network (2) satisfying Assumption 2. Suppose the training set is given by_

\[_{1}=(,}),_{2}=(,- {1-^{2}}/2), y_{1}=y_{2}=1, 0<<0.1.\]

_It is clear that \((_{i},y_{i})_{i=1,2}\) satisfy Assumption 3. If \((L(_{t}))_{t 0}\) is non-increasing, then_

\[L(_{t}) c_{0}/t, t 1\]

_where \(c_{0}>0\) is a function of \((,,_{1},_{2},,,)\) but is independent of \(t\) and \(\)._Effect of model rescaling.We conclude this section by discussing the impact of rescaling the model. Specifically, we replace the two-layer network in the mean-field scaling (2) by the following

\[f(;):=b_{j=1}^{m}a_{j}(^{ }^{(j)}),\]

and evaluate the impact of the scaling factor \(b\) on our results. By choosing the optimal stepsize that balances the EoS and stable phases as in Corollary 4.2, we optimize the risk bound obtained by GD with a fixed budget of \(T\) steps and get the following bound. Detailed derivations are deferred to Appendix D.

\[L(_{T})=(1/T^{2})&b 1,\\ (b^{-3}/T^{2})&b<1.\]

This suggests that as long as \(b 1\), we get the same acceleration effect. In particular, the mean-field scaling \(b=1\)(Song et al., 2018; Chizat and Bach, 2020) and the _neural tangent kernel_ (NTK) scaling \(b=\)(Du et al., 2018; Jacot et al., 2018) give the same acceleration effect. An NTK analysis of large stepsize is included in (Wu et al., 2024) and their conclusion is consistent with ours. Finally, we remark that our analysis holds for any width \(m\) and uses techniques different from the mean-field or NTK methods. However, our acceleration analysis only allows linearly separable datasets.

## 5 Experiments

We conduct three sets of experiments to validate our theoretical insights. In the first set, we use a subset of the CIFAR-10 dataset (Krizhevsky et al., 2009), which includes 6,000 randomly selected samples from the "airplane" and "automobile" classes. Our model is a multilayer perceptron (MLP) with four trainable layers and GELU activation functions, with a hidden dimension of 200 for each hidden layer. The MLP is trained using gradient descent with random initialization, as described in (GD). The results are shown in Figures 1(a) to 1(c).

In the second set of experiments, we consider an XOR dataset consisting of four samples:

\[_{1}=(-1,-1),y_{1}=1;\ _{2}=(1,1),y_{2}=1;\ _{3}=(1,-1),y_{3}=-1;\ _{4}=(-1,1),y_{4}=-1.\]

The above XOR dataset is not linearly separable. We test (GD) with different stepsizes on a two-layer network (2) with the leaky softplus activation (see Example 3.1 with \(c=0.5\)). The network width is \(m=20\). The initialization is random. The results are presented in Figures 2(a) to 2(c).

In the third set of experiments, we consider the same task as in the first set of experiments, but we test (GD) with different stepsizes on a two-layer network (2) with the softplus activation. The network width is \(m=40\). The initialization is random. The results are presented in Figures 2(d) to 2(f).

Margin improvement.Figures 1(b), 2(c) and 2(f) show that the normalized margin nearly monotonically increases once gradient descent (GD) enters the stable phase, regardless of step size. This observation aligns with our theoretical findings in Theorem 2.2.

Fast optimization.From Figures 1(a), 2(a) and 2(d), we observe that after GD enters the stable phase, a larger stepsize consistently leads to a smaller empirical risk compared to the smaller stepsizes, which is consistent with our Theorem 4.1 and Corollary 4.2. Besides, Figures 2(b) and 2(e) suggest that, asymptotically, GD converges at a rate of \((1/(t))=(1/( t))\) (The width of networks is fixed), which verifies the sharpness of our stable phase bound in Theorem 4.1.

Margin of individual neurons.It is important to note that while the normalized margin behaves as expected, the margin for individual neurons may not increase and can remain negative, even when the dataset is linearly separable. A detailed example illustrating this is provided in Appendix E.

## 6 Related Works

In this section, we discuss related papers.

Small stepsize and implicit bias.For logistic regression on linearly separable data, Soudry et al. (2018), Ji and Telgarsky (2018) showed that the direction of small stepsize GD converges to the max-margin solution. Their results were later extended by Gunasekar et al. (2017, 2018), Nacson et al. (2019, 2019), Ji et al. (2021), Lyu and Li (2020), Ji and Telgarsky (2020), Chizat and Bach (2020), Chatterji et al. (2021), Kunin et al. (2022) to other algorithms and non-linear models. However, in all of their analysis, the stepsize of GD needs to be small such that the empirical risk decreases monotonically. In contrast, our focus is GD with a large stepsize that induces non-monotonic risk.

Two papers (Nacson et al., 2019, Kunin et al., 2022) studied margin maximization theory for a special form of non-homogenous models. Specifically, when viewed in terms of different subsets of the trainable parameters, the model is homogeneous, although the order of homogeneity may vary. Compared to their setting, our non-homogenous models only require a bounded homogenous error (see Assumption 1C). Therefore, our theory can cover two-layer networks (2) with non-homogeneous activations such as GELU and SiLU that cannot be covered by (Nacson et al., 2019, Kunin et al., 2022).

Large stepsize and EoS.In practice, large stepsizes are often preferred when using GD to train neural networks to achieve effective optimization and generalization performance (see Wu and Ma, 2018, Cohen et al., 2020, Barrett and Dherin, 2020, and references therein). In such scenarios, the empirical risk often oscillates in the beginning. This phenomenon is named _edge of stability_ (EoS) by Cohen et al. (2020). The theory of EoS is mainly studied in relatively simplified cases such as one- or two-dimensional functions (Zhu et al., 2022, Chen and Bruna, 2023, Ahn et al., 2022, Kreisler et al., 2023, Wang et al., 2023), linear model (Wu et al., 2023, 2024), matrix factorization (Wang et al., 2022, Chen and Bruna, 2023), scale-invariant networks (Lyu et al., 2022), linear networks under MSE loss (Ren et al., 2024, Andriushchenko et al., 2023), for an incomplete list of references. Compared to them, we focus on a more practical setup of training two-layer non-linear networks with large stepsize GD. There are some general theories of EoS subject to subtle assumptions (for

Figure 2: Behavior of (GD) for two-layer networks (2) with leaky softplus activation function (see Example 3.1 with \(c=0.5\)). We consider an XOR dataset and a subset of CIFAR-10 dataset. In both cases, we observe that (1) GD with a large stepsize achieves a faster optimization compared to GD with a small stepsize, (2) the asymptotic convergence rate of the empirical risk is \((1/(t))\), and (3) in the stable phase, the normalized margin (nearly) monotonically increases. These observations are consistent with our theoretical understanding of large stepsize GD. More details about the experiments are explained in Section 5.

example, Kong and Tao, 2020, Ahn et al., 2022, Ma et al., 2022, Damian et al., 2022, Wang et al., 2022b, Lu et al., 2023], which are not directly comparable to ours.

In what follows, we make a detailed discussion about papers that directly motivate our work [Lyu and Li, 2020, Ji and Telgarsky, 2020, Chatterji et al., 2021, Wu et al., 2024].

Comparison with Lyu and Li , Ji and Telgarsky .Both results in [Lyu and Li, 2020, Ji and Telgarsky, 2020] focused on \(L\)-homogenous networks. Specifically, Lyu and Li  showed that a modified version of normalized margin (see (3)) induced by GD with small stepsize (such that the risk decreases monotonically) increases, with limiting points of \(\{_{t}/\|_{t}\|\}_{t=1}^{}\) converging to KKT points of a margin-maximization problem. Under additional o-minimal conditions, Ji and Telgarsky  showed that gradient flow converges in direction. Our work is different from theirs in two aspects. First, we allow GD with a large stepsize that may cause risk oscillation. Second, our theory covers non-homogenous predictors, which include two-layer networks with many commonly used activation functions beyond the scope of [Lyu and Li, 2020, Ji and Telgarsky, 2020]. Compared to Lyu and Li , Ji and Telgarsky , we only show the improvement of the margin, and our theory is limited to nearly 1-homogenous predictors (Assumption 2C). It remains open to show directional convergence and to extend our near \(1\)-homogenity condition to a "near \(L\)-homogeneity" condition for a general \(L\).

Comparison with Chatterji et al. .The work by Chatterji et al.  studies the convergence of GD in training deep networks under logistic loss. Their results are related to ours as we both consider networks with nearly homogeneous activations and we both have a stable phase analysis (although this is not explicitly mentioned in their paper). However, our results are significantly different from theirs. Specifically, in our notation, they require the homogenous error \(\) (see Assumption 2C) to be smaller than \(((1/L(_{s}))/\|_{s}\|)( (_{s}))\), where \(s\) is the time for GD to enter the stable phase. Note that the margin when GD enters the stable phase could be arbitrarily small. In comparison, we only require the homogenous error to be bounded by a constant. As a consequence, we can handle many commonly used activation functions (see Example 2.1) while they can only handle the Huberized ReLU with a small \(h\) in Example 2.1. Moreover, they require the stepsize \(\) to be smaller than \((/\|_{s}\|^{8})\), thus they only allow very small stepsize. In contrast, we allow \(\) to be arbitrarily large.

Comparison with Wu et al. .The works by Wu et al.  directly motivate our paper. In particular, for logistic regression on linearly separable data, Wu et al.  showed margin maximization of GD with large stepsize and Wu et al.  showed fast optimization of GD with large stepsize. Our work can be viewed as an extension of [Wu et al., 2023, 2024] from linear predictors to non-linear predictors such as two-layer networks. Besides, our results for margin improvement and convergence within the stable phase (Theorem 2.2) hold for the general dataset, while their results strongly rely on the linear separability of the dataset.

## 7 Conclusion

We provide a theory of large stepsize gradient descent (GD) for training non-homogeneous predictors such as two-layer networks using the logistic loss function. Our analysis explains the empirical observations: large stepsize GD often reveals two distinct phases in the training process, where the empirical risk oscillates in the beginning but decreases monotonically subsequently. We show that the phase transition happens because the average empirical risk decreases despite the risk oscillation. In addition, we show that large stepsize GD improves the normalized margin in the long run, which extends the existing implicit bias theory for homogenous predictors to non-homogenous predictors. Finally, we show that large stepsize GD, by entering the initial oscillatory phase, achieves acceleration when minimizing the empirical risk.