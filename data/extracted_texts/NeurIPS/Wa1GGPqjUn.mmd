# Online learning of long-range dependencies

Nicolas Zucchet\({}^{*}\), Robert Meier\({}^{*}\), Simon Schug\({}^{*}\)

Asier Mujika, Joao Sacramento

Equal contribution.

###### Abstract

Online learning holds the promise of enabling efficient long-term credit assignment in recurrent neural networks. However, current algorithms fall short of offline backpropagation by either not being scalable or failing to learn long-range dependencies. Here we present a high-performance online learning algorithm that merely doubles the memory and computational requirements of a single inference pass. We achieve this by leveraging independent recurrent modules in multi-layer networks, an architectural motif that has recently been shown to be particularly powerful. Experiments on synthetic memory problems and on the challenging long-range arena benchmark suite reveal that our algorithm performs competitively, establishing a new standard for what can be achieved through online learning. This ability to learn long-range dependencies offers a new perspective on learning in the brain and opens a promising avenue in neuromorphic computing.

## 1 Introduction

How can the connections between neurons in a neural network be adjusted to improve behavior? This question, known as the credit assignment problem, is central in both neuroscience  and machine learning , owing to its fundamental importance for elucidating learning mechanisms in the brain and constructing intelligent artificial systems. However, the complex and nonlinear nature of neural network processing makes the precise allocation of credit an intricate task.

Deep learning provides a compelling solution to the credit assignment problem via gradient descent, which refines network parameters along the locally most promising direction. For networks processing temporal sequences, gradient computation is made possible by backpropagation-through-time [BPTT; 2; 3; 4]. BPTT stores and revisits neural activity in reverse-time order to understand how infinitesimal changes to neural activity, and thus to network parameters, would have impacted the objective function. One important drawback of this algorithm is its requirement to store the entire activity trajectory in memory, which constrains the sequence length for exact gradient computation, impairing the learning of long-term interactions. This constraint becomes a critical bottleneck when working within memory-limited systems, such as neuromorphic hardware  and presumably the brain .

Alternatives to BPTT for gradient computation do exist. One such approach, forward-mode differentiation [7; 8], involves computing gradients online as the input sequence is processed, by keeping track of the sensitivity of neural activity with respect to each of the network parameters. This marks a qualitative departure from BPTT, as it prepares for all potential future trajectories simultaneously; by contrast, BPTT focuses on improving the activity of a past trajectory. Importantly, the memory footprint of this approach does not depend on sequence length. Still, it remains intractable for real-world applications and likely infeasible in the brain due to its cubic memory scaling and quartic computational complexity in the number of neurons. Recent research focused on approximationstrategies to make online gradient estimation more tractable for general-purpose recurrent networks [9; 10; 11; 12; 13; 14; 15; 16; 17; 18]. Our work takes a fundamentally different approach: instead of tailoring the learning algorithm to the neural network architecture, we fix the learning algorithm and seek an architecture that makes it tractable.

We build upon recent advances in linear state space models, a class of recurrent neural networks (RNNs) employing linear recurrent blocks [19; 20; 21; 22; 23]. These blocks are stacked and interconnected through nonlinear networks. The key insights from this line of research are that linear recurrent connections simplify temporal credit assignment and enable parallel temporal processing, while nonlinearities between recurrent blocks ensure that network expressiveness remains comparable to that of densely-connected nonlinear RNNs. Much, if not all, of the state-of-the-art performance of those models on long-range temporal tasks  can be maintained by transitioning from real-valued to complex-valued neural activities [21; 22; 23], and restricting the recurrent connectivity matrix to be diagonal. Recurrent neurons within a given layer are now independent of each other. This greatly improves the tractability of online gradient estimation, as the recurrent parameters of a given neuron do not impact other neurons. We leverage this property to achieve exact online gradient computation within a single layer with as little as twice the memory and compute requirements needed for inference. Further, we demonstrate how this leads to improved gradient estimation compared to existing online learning algorithms when recurrent layers are stacked.

This paper is organized as follows. We start by briefly reviewing existing gradient-based online learning methods in Section 2.1. Next, we introduce the concept of independent recurrent modules, showing how some of the recent high-performance models mentioned above fit in this framework in Section 2.2. Deriving our learning rule requires complex differentiation; we give a concise overview of those tools in Section 2.3. In Section 3, we detail our online learning algorithm that combines exact differentiation within a layer of recurrent independent modules with spatial backpropagation across layers. Finally, in Section 4, we analyze our algorithm and relevant baselines on a synthetic copy task and show that it can learn sequential tasks with sequence lengths up to over \(4000\) steps.

## 2 Background

### Online gradient-based RNN learning

We study gradient-based learning of recurrent neural networks, which process input data \(x_{1},,x_{T}\) sequentially while maintaining an internal (hidden) state \(h_{t}\). The objective of learning is to minimize a cumulative loss \(L()=_{t=1}^{T}L_{t}()\) which measures performance on a task at hand as a function of network parameters \(\). The standard algorithm for computing the gradient \( L()\) is the offline backpropagation-through-time method, which requires storing the entire input \(x_{1:T}\), loss \(L_{1:T}\) and internal activity \(h_{1:T}\) sequences, and then revisiting them proceeding backwards in time. Here, we focus on online algorithms which carry the information needed to compute or estimate \( L_{t}()\) forward in time. This enables simultaneous processing of inputs and learning for RNNs, without storing past data and network states. In principle, online algorithms can learn arbitrarily long temporal dependencies as well as seamlessly handle sequences of arbitrary length \(T\).

The classical alternative to BPTT for forward-in-time gradient computation is known as real-time recurrent learning [RTRL; 7] in the context of RNNs2, a method which has its roots in control theory . While RTRL enables online gradient-based learning, it requires storing \(_{}h_{t}\) in memory and updating it as the RNN processes its inputs. The size of this auxiliary variable is \((n^{3})\), where \(n=|h|\) is the number of hidden units in the RNN. For comparison, the memory requirements of BPTT are \((nT)\). In practice, this precludes the usage of RTRL for all but the smallest of models.

There has been much effort in developing memory-efficient alternatives to RTRL. We now briefly discuss these prior efforts while referring to a recent review by Marschall et al.  for a more detailed treatment. We divide prior work into three broad categories. One class of algorithms relies on neglecting terms in \(_{}h_{t}\) to reduce its size, thereby creating a biased gradient estimator. Typically this requires introducing crude approximations, which allow going from \((n^{3})\) to a tractable \((n^{2})\) size. Despite such approximations, in many cases, performance still holds in non-trivial tasks [13; 15; 16]. At the end of this spectrum sits instantaneous (spatial) backpropagation, which neglects all temporal dependencies in the hidden state when approximating the gradient. A second class of algorithms relies on stochastic estimation; this allows retaining unbiased gradient estimates, at the expense of introducing variance [9; 10; 11; 17; 18]. Finally, a third class of methods introduces gradient models [critics; 27; 28; 29] to produce gradient estimates online. The critics themselves are then either trained separately offline, making such methods hybrid on/offline; or fully online, using temporal difference techniques . We note that despite their widespread use in reinforcement learning, it is not yet well understood whether temporal difference methods can reliably and efficiently improve the performance of a gradient critic on real-world RNN learning problems.

As noted by Irie et al. , the RTRL literature mostly focuses on single-layer recurrent networks and remains scarce for deeper networks. Recently, Javed et al.  developed a greedy learning algorithm where a growing network is progressively frozen and trained one layer at a time. Existing approximations such as [13; 15] do not prescribe how to learn the parameters of remote layers, and the multi-layer case is not considered in the respective papers. Introducing a powerful RTRL algorithm that scales to networks of multiple layers is the main algorithmic contribution of this paper. This property is of great empirical relevance given the power of depth to learn the temporal structure [e.g. 19; 23].

### Linear recurrent units and independent recurrent modules

Instead of developing approximate, general-purpose forward-mode differentiation methods, our goal shifts towards seeking an expressive architecture allowing exact, tractable online gradient calculation. We propose that networks with linear recurrent units [LRU; 23] and, more generally networks with independent recurrent modules, are particularly well-suited for this purpose.

A linear recurrent unit, depicted in Figure 1, is defined as

\[h_{t+1}= h_{t}+Bx_{t+1},\ \ \ y_{t}=[Ch_{t}]+Dx_{t},\] (1)

with \(\) the element-wise product. Here, \(x_{t}^{H}\) represents the input received by the LRU at time \(t\), \(h_{t}^{N}\) denotes its internal state, and \(y_{t}^{H}\) its output. The parameters of the unit include \(^{N}\), \(B^{N H}\), \(C^{H N}\) and \(D^{H H}\). The version of the LRU we use in our experiments includes an element-wise normalization factor for the input \(Bx\) and uses an exponential parametrization of \(\) for network stability. We omit these details in the main text for conciseness; see Appendix A.1 for more details.

LRUs differ from traditional recurrent layers in deep learning: they have linear neural dynamics, a complex-valued state \(h_{t}\), and a diagonal connectivity pattern. Orvieto et al.  found that the absence of temporal linearity in networks that stack those units through nonlinear connections (see Fig. 1) does not alter expressivity and eases gradient-based learning, a notoriously difficult process for nonlinear RNNs [33; 34]. In addition, the diagonal structure of the recurrence matrix provides several benefits over fully connected ones. First, it affords an easy way to control the eigenvalues of the Jacobian of the system and thus ensure that neural dynamics remain stable. Due to the linearity, it also enables processing the input sequence in parallel , significantly accelerating the training of such models on modern computers . Importantly, despite its diagonal parametrization, the LRU remains functionally equivalent to a linear recurrent layer with dense recurrence matrix \(A\), as \(A\) can be approximated as accurately as needed by a complex-diagonalizable matrix.

Each complex neuron in an LRU is an _independent recurrent module_, meaning its current state does not impact the dynamics of other modules. This property greatly simplifies online credit assignment (see Section 3). We focus on this specific architecture due to its simplicity and great empirical performance, but our theoretical insights also apply to networks of independent recurrent modules with low-dimensional state vectors per module.

### A primer on complex differentiation

The use of complex-valued networks, such as the LRU, and hence complex differentiation remains relatively scarce. In the following, we provide a concise review of the tools of complex differentiation integral to the derivation of our online learning rule. We use \(f\) and \(g\) to denote complex-valued functions that take the complex variable \(z\) as input.

The Wirtinger derivatives of \(f\) are defined through

\[f}{z}:=(f}{ [z]}-if}{[z]}),\;f}{}:=(f}{[z]}+if}{[z]})\!.\] (2)

Using them for complex differentiation allows using similar calculus rules as for real functions. Note that we use the row convention for derivatives, that is \(_{z}f\) is a row vector of size \(|z|\). The following formula holds in general \(_{z}f}=_{}\).

The complex derivative of a complex function is similar to a \(2 2\) real-valued matrix as both \(_{z}f\) and \(_{}f\) are necessary to characterize it. Yet, there exists a subclass of functions, called holomorphic functions for which it can be reduced to a \(2\) dimensional real-valued vector, leading to a more compact representation of derivatives. A continuous function \(f\) is _holomorphic_ if it satisfies the Cauchy-Riemann equations

\[[f]}{[z]}= [f]}{[z]}\;\;\;\; [f]}{[z]}=-[f]}{ [z]},\;\;\;f}{ }=0.\] (3)

Any affine function, as well as the composition of two holomorphic functions, are themselves holomorphic.

The chain rule of complex differentiation, crucial for automatic differentiation, is

\[(f g)}{z}=f}{g} {g}{z}+f}{}}{z}.\] (4)

When either \(f\) or \(g\) is holomorphic, the second term vanishes as \(d_{}f=0\) or \(_{z}=_{}g}=0\). When \(f\) is a real-valued function, it can be optimized through gradient descent by iteratively updating its input variable z through \( z-_{[z]}f^{}-i_{[z ]}f^{}=-2_{}f^{}\).

## 3 Online learning of networks of independent recurrent modules

Based on the foundations laid down in the preceding section we now derive our online gradient-based learning algorithm for multi-layer networks of independent recurrent modules. We first focus on a single layer, demonstrating that exact forward-mode differentiation is tractable. This insight then guides the derivation of our rule for multi-layer networks.

Figure 1: (Left) Overview of the class of neural networks we consider in this paper. We stack layers of independent recurrent modules (IRMs), augmented with layer norm  and gated linear units [GLU; 37]. Light red indicates instantaneous spatial processing, dark red temporal processing. When we examine networks with fully connected recurrent layers, only the dark red block is modified. The linear recurrent unit is the instantiation of a layer of IRMs we use in our experiments. (Right) Overview of our learning rule. As an input sequence is processed, hidden states \(h_{t}\) and their sensitivities \(e_{t}\) to the parameters are updated. Learning ensues by combining the sensitivities \(e_{t}\) with spatially backpropagated error signals. No information flows in reverse time; our rule is fully online.

### Single-layer networks

We focus on parameters \(\) that influence the hidden states; computing the gradient of any other parameter does not require temporal credit assignment. For the LRU, we have \(=\{,B\}\). Recall that \(L()=_{t=1}^{T}L_{t}(y_{t}())\) denotes the loss function that measures how good the outputs \(y_{1:T}\) of a network parametrized by \(\) are. Its derivative \(_{}L()\) can be calculated using forward-mode (complex) differentiation:

\[L}{}=_{t=1}^{T}}h_{t}}{}+ }}}{}=_{t=1}^{T}}{ h_{t}}h_{t}}{}.\] (5)

As mentioned in Section 2.3, the last equality holds as \(h_{t+1}\) is a holomorphic function of \(h_{t}\) and of \(\), hence \(h_{t}\) is a holomorphic function of \(\) by recursive composition, and \(L\) only directly depends on \(h_{t}\) through \(L_{t}\). The term \(_{t}:=_{h_{t}}L^{}\) that is here equal to \(_{h_{t}}L_{t}^{}\) can easily be computed by spatial backpropagation, as the output \(y_{t}\) at time \(t\) only depends on the current hidden state \(h_{t}\). We are left with computing the sensitivities \(_{}h_{t}\) of the states to the parameters.

Independent recurrent modules do not influence each other. The parameters \(_{i}\) that directly influence the state \(h_{t,i}\) of module \(i\) never impact the state \(h_{t^{},j}\) of another module. As a consequence, the number of non-zero entries of the sensitivity \(_{}h_{t}\) grows linearly with the size of \(\) whenever the number of recurrent neurons within each module is fixed. Applying this to the LRU, \(_{}h_{t}\) is entirely characterized by \(e_{t}^{}:=(_{}h_{t,i})_{i}\) and \(e_{t}^{B}:=(_{B_{ji}}h_{t,j})_{ji}\). Differentiating Equation 1 using the product rule gives the corresponding updates:

\[e_{t+1}^{}= e_{t}^{}+h_{t},\ \ e_{t+1}^{B}=( )e_{t}^{B}+1x_{t+1}^{},\] (6)

with \(1\) a vector of size \(|h|\) filled with ones. More detail on the derivation of Equation 6 and on how to efficiently simulate this update are given in Appendix A.2. Keeping track of those quantities only requires considering an additional hidden state of size \(||\).3 Finally, the \(\) and \(B\) updates can be obtained by following the gradient, as calculated in Equation 5:

\[_{t=1}^{T}_{t} e_{t}^{},\ \  B_{t=1}^{T}(_{t})e_{t}^{B}.\] (7)

Interestingly, all the \(e\)-updates are local to the neuron or synapse in question, and no approximations were required to accomplish this. This feature makes the algorithm particularly promising for neuroscience and neuromorphic engineering, where localized computation is highly desirable. The parameter update for \(\) and \(B\) is also fully local, as it combines a parameter-specific sensitivity, sometimes considered as an eligibility trace , and a postsynaptic error term.

The idea that element-wise recurrence simplifies RTRL precedes our work. It can be found in early work by Mozer  and Gori et al. , and has been revisited recently [32; 31]. In this paper, we extend this insight to complex numbers and thus do not lose expressivity, unlike previous work. We also note that some approximations to RTRL such as e-prop  or SnAp-1  end up being exact when applied to networks with independent recurrent modules.

### Multi-layer networks

The derivation in the last section presumes that the loss \(L\) only directly depends on \(h_{t}\) through \(L_{t}\). This assumption no longer holds when layers are stacked, which is crucial to the expressiveness of the model. In the following, we explain how we can extend our rule to the multilayer case. Let us consider layer \(l\) of the network where we aim to compute the gradient of the loss \(L\) with respect to its parameters \(^{l}\). The sensitivity \(_{^{l}}h_{t}^{l}\) can be computed as before, assuming independent recurrent modules, as \(^{l}\) does not influence the behavior of the inputs it receives from previous layers. Hence, we are left with computing \(_{t}^{l}=_{h_{t}^{l}}L^{}\). The simplification \(_{t}^{l}=_{h_{t}^{l}}L_{t}^{}\) we made in the previous section still holds for the last layer but is violated for the other layers. This is because \(L\), taken as function of the hidden states \(h^{l}\) of layer \(l\), now has an internal memory through the subsequent recurrent layers. The hidden state \(h_{t}^{l}\) at time \(t\) will thus directly affect all future losses \(L_{t^{}}\) for \(t^{} t\).

As a consequence, one has to resort to backpropagation-through-time to compute \(_{h_{t}^{l}}L^{}\) exactly, which breaks causality and rules out the possibility of learning online. To circumvent this issue, we approximate the error signal each layer receives by \(_{t}^{l}_{h_{t}^{l}}L_{t}^{}\) so that it can be computed instantaneously with spatial backpropagation. We emphasize that the only source of approximation of this algorithm is the one above. Given that there is no approximation for the last layer, we will always compute the exact gradient for that layer.

We summarize our learning rule in Figure 1. It prescribes augmenting the hidden state of each recurrent layer \(l\) with the sensitivity \(e^{l}\). For each input/output sample \((x_{t},y_{t}^{})\), we first update the full entire hidden state \((h_{t},e_{t})\) using the previous one \((h_{t-1},e_{t-1})\) and current input \(x_{t}\). We then spatially backpropagate the error signal obtained at the last layer by comparing the prediction of the network to its desired value \(y_{t}^{}\). Finally, we combine \(e_{t}^{l}\) and \(_{t}^{l}\) available at each layer using Equation 7 to compute the current update. So far, we have only described how to update parameters that directly influence the hidden neurons of the recurrent layer. We update the rest of the parameters with the gradient estimate obtained with spatial backpropagation. Importantly, the size of the extended hidden state is upper bounded by the number of neurons plus the number of parameters and is in practice much smaller.

Next, we aim to understand the factors that may degrade the quality of the gradient estimate as a result of the approximation \(_{t}^{l}_{h_{t}}L_{t}^{}\) which introduces bias in the gradient estimate. In the LRU, neural activities, and thus error signals, are processed temporally through dynamics similar to the one of Equation 1. When the norm of each \(_{i}\) approaches one, neural activity preserves past information, and correspondingly, error signals backpropagated over time contain more information about the future. This suggests that our approximation becomes less accurate as the distribution of \(|_{i}|\) narrows around 1, since it discards future error information. Moreover, \(\) worsens as it is backpropagated through more layers. At each layer, backpropagation mixes errors from the next layer and the future state of the layer. Since we neglect future information, only part of the error signal is backpropagated, resulting in a less accurate approximation. We delve deeper into these two approximation sources in a memory task in Section 4.1.

## 4 Experiments

In the following, we analyze the properties of our proposed online learning method empirically and explore how independent recurrent modules aid learning of long-range dependencies. To this end, we first conduct an extensive analysis on the copy task , a well-established test bed to study temporal credit assignment in recurrent neural networks. Comparisons to truncated online versions of BPTT and to an extension of SnAp-1 to deep recurrent networks, reveal that independent recurrent modules are generally beneficial for learning long-range dependencies, as they excel in both the online and the offline setting. Finally, we evaluate our method on three tasks of the Long Range Arena : a sequential version of CIFAR , ListOps and IMDB , scaling online learning to sequence lengths of over \(4000\) time steps and to deep recurrent neural networks. For additional experimental details and hyperparameter configurations, we refer to Appendix B.

### Understanding the approximations behind online learning in networks of LRUs

In this first series of experiments, we investigate the approximation introduced by our online algorithm in detail. We recall that our learning rule only approximates the full gradient when the network has two recurrent layers or more, as it ignores the temporal component of backpropagated error signals. Therefore, we expect that the learning signal becomes less accurate as we increase network depth and shift the eigenvalue distribution towards a norm of \(1\). To explore the effects of our approximation in a controlled setting, we consider a copy task [40; 10; 15; 18] in which the network observes a length-\(20\) sequence of \(7\)-bit patterns that it must recall when presented with an output token. Intuitively, temporal credit assignment is critical in this task to, among other things, convert forgetting neurons (\(|| 1\)) into perfect memory units (\(|| 1\)) that can solve the task. To ensure that these perfect memory units are scarce at the beginning of training, necessitating learning to create some, we initialize \(\) uniformly at random in the complex unit disk and set the number of recurrent units per layer to \(N=64\). The default architecture used in this section has four layers.

As remarked in Section 3.2, the updates prescribed by our learning rule match the gradient exactly for all parameters in the last LRU layer. We confirm that empirically for a network of one layer in Figure 2.A. While the approximation quality deteriorates with increasing depth (Fig. 2.A), alignment remains high, noticeably better than for all baseline online learning rules (Fig. 2.E). Moreover, despite alignment decreasing with depth, performance enhances significantly (Fig. 2.B). BPTT exhibits similar improvements with depth, suggesting that this is likely due to the enhanced capabilities of the model. Our rule can learn useful hierarchical temporal representations online whereas baseline methods, \(1\)-step truncated and spatial backpropagation, which ignore most of temporal dependencies, fail (c.f. Fig. 2.F). Additionally, we found that, despite its bias, our learning rule can decrease the loss to 0 when training a 4 layers network on a simpler memory task for long enough.

Exact error terms are backpropagated through recurrent units by weighting the current prediction error by \(1\) and the future error by \(\). In order to maintain an online algorithm, we ignore this dependency on the future. We expect alignment with the gradient to decrease as the distribution of \(||\) shifts towards \(1\). To test that, we initialize the \(\) uniformly at random in the complex ring of radius \([||_{},1]\). Interestingly, alterations in the initial eigenvalue distribution only slightly affect estimation quality in the beginning of training (Fig. 2.C). The key factor seems to be the degradation associated with learning progress, rather than degradation due to larger eigenvalues. Smaller initial \(||_{}\) values slow down training, as more perfect memory neurons have to be recruited, but all initializations eventually lead to solving the task (Fig. 2.D).

### Independent recurrent modules improve online learning performance

After dissecting the learning dynamics of our algorithm, we next show the importance of the independence of recurrent modules for online gradient-based learning. To this end, we compare linear recurrent units to densely-connected linear recurrent layers which do not have independent recurrent modules. To make the comparison fair, we ensure all models have the same number of parameters and recurrent neurons. In addition to the baselines we considered in the previous section, we include an extended version of the SnAp-1 algorithm that combines spatially backpropagated errors with the SnAp-1 sensitivity approximation. This algorithm reduces to ours when applied to networks of independent recurrent modules. Therefore, it enables us to isolate the impact of the independent recurrent module design on online gradient-based learning. We report final training losses in Table 1 and refer the reader to Appendix B for experimental details.

Figure 2: Impact of depth of the network (left), eigenvalues of the network (middle), and type of approximation on the quality of online learning (right). The cosine similarity measures the alignment between the estimated gradient (with our learning rule for all panels, and with spatial backpropagation (Spat.) and \(1\)-step truncated backpropagation (Trunc.) for panels E and F) and the true gradient, computed with backpropagation-through-time (BP). It is computed per layer and then averaged across layers to make a quantitative comparison possible. The encoder and decoder alignments do not influence this metric. This task is solved (\(100\%\) accuracy) for losses lower than \(0.05\) and \(70\%\) accuracy roughly corresponds to a loss of \(0.5\). See Section 4.1 for more details.

Our findings confirm the benefits of independent recurrent modules for online learning, in particular for multi-layer networks. To demonstrate that, we first compare our algorithm on the LRU architecture with the SnAp-1 algorithm applied to an equivalent linear recurrent network. The diagonal approximation of the sensitivity tensor in SnAp-1 introduces an additional bias when learning linear RNNs. We found that this additional bias hurts performance: when moving from offline BPTT to online training, the performance drop is significantly higher for linear RNNs. Interestingly, sensitivity approximation does not bring any performance gain, in this setting, compared to the cruder approximations that are 1-step truncated BPTT and spatial backpropagation.

Additionally, we run experiments on another RNN architecture, the GRU , to better understand the impact of depth in online and offline RNN training, and to confirm the importance of element-wise recurrence for online learning. In the single layer case, consistent with Menick et al. , we find that the SnAp-1 approximation performs competitively with offline BPTT. However, it suffers from depth in contrast to BPTT that benefits from it. This result highlights the importance of depth in this memory task, as well as the difficulty learning over depth poses for existing online learning algorithms.

### Scaling online learning to the long-range arena benchmark

While the approximations typically employed to enable online learning prohibit scaling to tasks with extended temporal structure, the results from our previous section have demonstrated the potential of independent linear units for online learning of long-range dependencies. We therefore move to tasks from the challenging long-range arena benchmark  specifically designed to evaluate this ability. Transformers excel in almost any benchmark today. However, they perform surprisingly subpar in this setting  in which deep state-space models  and LRUs  achieve impressive results.

We run experiments on three tasks, sequential CIFAR, IMDB and ListOps. In sequential CIFAR, the network receives the \(32 32\) image as a pixel sequence and has to perform a classification task. In line with Orvieto et al. , we use the colored version of SCIFAR instead of the grayscale version

   Layer & LRU & Linear RNN & GRU & GRU \\ Number layers & 4 & 4 & 1 & 4 \\  Spatial & \(4.66 10^{-1}\) & \(6.20 10^{-1}\) & \(6.26 10^{-1}\) & \(6.55 10^{-1}\) \\ Truncated & \(2.62 10^{-1}\) & \(5.81 10^{-1}\) & \(6.20 10^{-1}\) & \(6.49 10^{-1}\) \\ Ours / SnAp-1 & \(8.44 10^{-3}\) & \(5.82 10^{-1}\) & \(3.16 10^{-1}\) & \(3.27 10^{-1}\) \\  BPTT & \(7.59 10^{-6}\) & \(1.07 10^{-4}\) & \(2.61 10^{-1}\) & \(1.94 10^{-1}\) \\   

Table 1: Comparison of final training losses of different online learning algorithms on the copy task of Section 4.2. The independent recurrent modules design improves online learning performance. Performance greatly degrades when the LRU is replaced with a dense recurrent matrix (Linear RNN). Comparison with the SnAp-1 algorithm applied to the GRU architecture highlights that online learning of multilayer networks is difficult without element-wise recurrence. Results are averaged over 5 seeds.

   Method & sCIFAR & IMDB & ListOps \\  Spatial & \(58.20 0.70\) & \(83.50 0.20\) & \(32.02 0.27\) \\ Trunc. & \(60.01 1.26\) & \(84.04 0.47\) & \(31.88 0.59\) \\ Ours & \(79.59 1.01\) & \(86.48 0.41\) & \(37.62 0.68\) \\  BPTT & \(83.40 1.54\) & \(87.69 0.39\) & \(39.75 0.17\) \\    
   SCIFAR \\  \(50.63 0.23\) \\ \(50.53 0.43\) \\ \(63.71 0.33\) \\  \(65.23 0.56\) \\   

Table 2: Test accuracy on three tasks of the LRA benchmark  for spatial backpropagation, 1-step truncated backpropagation, our algorithm, and full backpropagation through-time. While we always use per time step local losses during training, we accumulate logits over the sequence during inference. We report the mean and std. for three seeds each.

   Layer & LRU & Linear RNN & GRU & GRU \\ Number layers & 4 & 4 & 1 & 4 \\  Spatial & \(4.66 10^{-1}\) & \(6.20 10^{-1}\) & \(6.26 10^{-1}\) & \(6.55 10^{-1}\) \\ Truncated & \(2.62 10^{-1}\) & \(5.81 10^{-1}\) & \(6.20 10^{-1}\) & \(6.49 10^{-1}\) \\ Ours / SnAp-1 & \(8.44 10^{-3}\) & \(5.82 10^{-1}\) & \(3.16 10^{-1}\) & \(3.27 10^{-1}\) \\  BPTT & \(7.59 10^{-6}\) & \(1.07 10^{-4}\) & \(2.61 10^{-1}\) & \(1.94 10^{-1}\) \\   

Table 3: Test accuracy of a linear RNN on the CIFAR task. Instead of our learning rule, we apply the SnAp-1 learning rule extended to the multilayer case, as described in Section 4.2.

originally proposed. In the IMDB task, the network is given a text encode in bytes of length at most \(4000\), and has to perform binary classification. In ListOps, the input is a sequence of numbers, brackets and operators like Max which the model needs to evaluate to determine a classification target in the range from \(1\) to \(10\). We do not use the three other tasks of the LRA benchmark: the performance gap between different models is usually small in the Retrieval task (c.f. ) and, in our preliminary experiments, we could not reach above chance performance in the PathFinder tasks with BPTT and the modifications we made to make the loss causal, as described in the next paragraph.

In order to make the sequence models employed on this benchmark compatible with the causality requirement in online learning, we remove the time pooling operation during training and consider a local loss term at every time step instead. During inference, we then evaluate our models using the average of the last layer logits in time which respects causality. Moreover, we replace batch normalization with layer normalization to avoid sending batch statistics backwards in time and consider smaller models to lower the computational burden for online learning. For further experimental details, please refer to Appendix B.

We report results comparing online learning to spatial backpropagation, truncated BPTT and the BPTT upper bound in Table 2. Our online learning algorithm outperforms other online learning approximations, significantly reducing the gap towards BPTT. As in the last section, replacing the LRU with a linear RNN layer in the CIFAR experiment leads to worth online learning performance, c.f. Table 3, providing further evidence for the effectiveness of independent recurrent modules for capturing long-term dependencies.

## 5 Discussion

We have demonstrated that long-range dependencies can be learned online, allowing recurrent neural networks to reach strong performance on a set of tasks from the long-range arena benchmark. Moreover, a detailed analysis of a memory problem revealed that our method significantly outperforms both spatial (online) backpropagation as well as prior approaches based on approximate real-time recurrent learning, coming close to full backpropagation-through-time. These findings may inform the design of new neuromorphic hardware with on-chip learning capabilities, an application where approximate real-time recurrent learning is garnering significant attention .

While most prior related work focused on developing generic gradient approximation schemes, we asked which architecture would simplify online gradient computations. In high-level terms, our philosophy draws from seminal work on long short-term memory networks [LSTMs; 40] or neural Turing machines , which established the importance of architecture design for the success of gradient descent. We build on this insight, moving to the harder setting of online learning. This led us to consider networks built of recurrent independent modules: decoupled units with low-dimensional state vectors, for which exact real-time recurrent learning is cheap. Importantly, this design underlies recent models such as deep linear recurrent units  and members of the HiPPO family  which achieve strong performance in a wide array of challenging problems, including language modeling at scale  and the long-range arena benchmark .

We conclude by noting that modularity, the overarching principle behind our approach, is at the very heart of the influential columnar hypothesis in neuroscience . This hypothesis states that the architecture of the neocortex is modular, with the cortical column as an elementary (or canonical, ) building block one level of abstraction above neurons. We thus speculate that modularity could be a key neural network design principle discovered by evolution, that considerably simplifies the temporal credit assignment problem. This is inline with our finding that a modular architecture enables learning complicated temporal dependencies through simple local temporal credit assignment mechanisms, letting spatial backpropagation take care of assigning credit over the network hierarchy. We stress this point because numerous biological implementations and alternatives for spatial backpropagation have been proposed [e.g., 53, 54, 55, 56, 57, 58, 59, 60, 61, 62], while essentially none exist yet for backpropagation-through-time . Our findings provide a starting point for understanding how the brain deals with the fundamental problem of learning the temporal structure behind its sensory inputs.