# DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models

Tao Yang\({}^{1}\)

Work done during internships at Microsoft Research Asia.

 Yuwang Wang\({}^{2}\)

Corresponding authors: wang-yuwang@mail.tsinghua.edu.cn; mnzheng@mail.xjtu.edu.cn.

Yan Lu\({}^{3}\)

Work done by International Workshop on Neural Information Processing Systems (NeurIPS 2023).

Nanning Zheng\({}^{1}\)

Corresponding authors: wang-yuwang@mail.tsinghua.edu.cn; mnzheng@mail.xjtu.edu.cn.

###### Abstract

Targeting to understand the underlying explainable factors behind observations and modeling the conditional generation process on these factors, we connect disentangled representation learning to Diffusion Probabilistic Models (DPMs) to take advantage of the remarkable modeling ability of DPMs. We propose a new task, disentanglement of (DPMs): given a pre-trained DPM, without any annotations of the factors, the task is to automatically discover the inherent factors behind the observations and disentangle the gradient fields of DPM into sub-gradient fields, each conditioned on the representation of each discovered factor. With disentangled DPMs, those inherent factors can be automatically discovered, explicitly represented, and clearly injected into the diffusion process via the sub-gradient fields. To tackle this task, we devise an unsupervised approach named DisDiff, achieving disentangled representation learning in the framework of DPMs. Extensive experiments on synthetic and real-world datasets demonstrate the effectiveness of DisDiff.

## 1 Introduction

As one of the most successful generative models, diffusion probabilistic models (DPMs) achieve remarkable performance in image synthesis. They use a series of probabilistic distributions to corrupt images in the forward process and train a sequence of probabilistic models converging to image distribution to reverse the forward process. Despite the remarkable success of DPM in tasks such as image generation , text-to-images , and image editing , little attention has been paid to representation learning  based on DPM. Diff-AE  and PDAE  are recently proposed methods for representation learning by reconstructing the images in the DPM framework. However, the learned latent representation can only be interpreted relying on an extra pre-trained with predefined semantics linear classifier. In this paper, we will refer DPMs exclusively to the Denoising Diffusion Probabilistic Models (DDPMs) .

On the other hand, disentangled representation learning  aims to learn the representation of the underlying explainable factors behind the observed data and is thought to be one of the possible ways for AI to understand the world fundamentally. Different factors correspond to different kinds of image variations, respectively and independently. Most of the methods learn the disentangled representationbased on generative models, such as VAE [16; 5; 23] and GAN . The VAE-based methods have an inherent trade-off between the disentangling ability and generating quality [16; 5; 23]. The GAN-based methods suffer from the problem of reconstruction due to the difficulty of gan-inversion .

In this paper, we connect DPM to disentangled representation learning and propose a new task: the disentanglement of DPM. Given a pre-trained DPM model, the goal of disentanglement of DPM is to learn disentangled representations for the underlying factors in an unsupervised manner, and learn the corresponding disentangled conditional sub-gradient fields, with each conditioned on the representation of each discovered factor, as shown in Figure 1.

The benefits of the disentanglement of DPM are two-fold: \((i)\) It enables totally unsupervised controlling of images by automatically discovering the inherent semantic factors behind the image data. These factors help to extend the DPM conditions information from human-defined ones such as annotations /image-text pairs , or supervised pre-trained models  such as CLIP . One can also flexibly sample partial conditions on the part of the information introduced by the superposition of the sub-gradient field, which is novel in existing DPM works. \((ii)\) DPM has remarkable performance on image generation quality and is naturally friendly for the inverse problem, e.g., the inversion of DDIM , PDAE. Compared to VAE (trade-off between the disentangling ability and generating quality) or GAN (problem of gan-inversion), DPM is a better framework for disentangled representation learning. Besides, as Locatello et al.  points out, other inductive biases should be proposed except for total correlation. DPM makes adopting constraints from all different timesteps possible as a new type of inductive bias. Further, as Srivastava et al.  points out, and the data information includes: factorized and non-factorized. DPM has the ability to sample non-factorized (non-conditioned) information , which is naturally fitting for disentanglement.

To address the task of disentangling the DPM, we further propose an unsupervised solution for the disentanglement of a pre-trained DPM named DisDiff. DisDiff adopts an encoder to learn the disentangled presentation for each factor and a decoder to learn the corresponding disentangled conditional sub-gradient fields. We further propose a novel Disentangling Loss to make the encoded representation satisfy the disentanglement requirement and reconstruct the input image.

Our main contributions can be summarized as follows:

* We present a new task: disentanglement of DPM, disentangling a DPM into several disentangled sub-gradient fields, which can improve the interpretability of DPM.
* We build an unsupervised framework for the disentanglement of DPM, DisDiff, which learns a disentangled representation and a disentangled gradient field for each factor.
* We propose enforcing constraints on representation through the diffusion model's classifier guidance and score-based conditioning trick.

## 2 Related Works

**Diffusion Probabilistic Models** DPMs have achieved comparable or superior image generation quality [38; 40; 19; 41; 20] than GAN . Diffusion-based image editing has drawn much attention, and there are mainly two categories of works. Firstly, image-guided works edit an image by mixing the latent variables of DPM and the input image [7; 31; 32]. However, using images to specify the attributes for editing may cause ambiguity, as pointed out by Kwon et al. . Secondly, the classifier-guided works [9; 1; 28] edit images by utilizing the gradient of an extra classifier. These methods require calculating the gradient, which is costly. Meanwhile, these methods require annotations or models pre-trained with labeled data. In this paper, we propose DisDiff to edit the image in an unsupervised way. On the other hand, little attention has been paid to representation learning in the literature on the diffusion model. Two related works are Diff-ae  and PDAE . Diff-ae  proposes a diffusion-based auto-encoder for image reconstruction. PDAE  uses a pre-trained DPM to build an auto-encoder for image reconstruction. However, the latent representation learned by these two works does not respond explicitly to the underlying factors of the dataset.

**Disentangled Representation Learning** Bengio et al.  introduced disentangled representation learning. The target of disentangled representation learning is to discover the underlying explanatory factors of the observed data. The disentangled representation is defined as each dimension of the representation corresponding to an independent factor. Based on such a definition, some VAE-basedworks achieve disentanglement [5; 23; 16; 3] only by the constraints on probabilistic distributions of representations. Locatello et al.  point out the identifiable problem by proving that only these constraints are insufficient for disentanglement and that extra inductive bias is required. For example, Yang et al.  propose to use symmetry properties modeled by group theory as inductive bias. Most of the methods of disentanglement are based on VAE. Some works based on GAN, including leveraging a pre-trained generative model . Our DisDiff introduces the constraint of all time steps during the diffusion process as a new type of inductive bias. Furthermore, DPM can sample non-factorized (non-conditioned) information , naturally fitting for disentanglement. In this way, we achieve disentanglement for DPMs.

## 3 Background

### Diffusion Probabilistic Models (DPM)

We take DDPM  as an example. DDPM adopts a sequence of fixed variance distributions \(q(x_{t}|x_{t-1})\) as the forward process to collapse the image distribution \(p(x_{0})\) to \((0,I)\). These distributions are

\[q(x_{t}|x_{t-1})=(x_{t};}x_{t-1},_{t}I).\] (1)

We can then sample \(x_{t}\) using the following formula \(x_{t}(x_{t};}x_{0},(1-_{t}))\), where \(_{t}=1-_{t}\) and \(_{t}=_{t=1}^{t}_{t}\), i.e., \(x_{t}=_{t}}x_{0}+_{t}}\). The reverse process is fitted by using a sequence of Gaussian distributions parameterized by \(\):

\[p_{}(x_{t-1}|x_{t})=(x_{t};_{}(x_{t},t),_{t}I),\] (2)

where \(_{}(x_{t},t)\) is parameterized by an Unet \(_{}(x_{t},t)\), it is trained by minimizing the variational upper bound of negative log-likelihood through:

\[_{}=}_{x_{0},t,}\|- _{}(x_{t},t)\|.\] (3)

### Representation learning from DPMs

The classifier-guided method  uses the gradient of a pre-trained classifier, \(_{x_{t}} p(y|x_{t})\), to impose a condition on a pre-trained DPM and obtain a new conditional DPM: \((x_{t};_{}(x_{t},t)+_{t}_{x_{t}} p(y|x_{t }),_{t})\). Based on the classifier-guided sampling method, PDAE  proposes an approach for pre-trained DPM by incorporating an auto-encoder. Specifically, given a pre-trained DPM, PDAE introduces an encoder \(E_{}\) to derive the representation by equation \(z=E_{}(x_{0})\). They use a decoder estimator \(G_{}(x_{t},z,t)\) to simulate the gradient field \(_{x_{t}} p(z|x_{t})\) for reconstructing the input image.

By this means, they create a new conditional DPM by assembling the unconditional DPM as the decoder estimator. Similar to regular DPM \((x_{t};_{}(x_{t},t)+_{t}G_{}(x_{t},z,t), _{t})\), we can use the following objective to train encoder \(E_{}\) and the decoder estimator \(G_{}\):

\[_{}=}_{x_{0},t,}\|- _{}(x_{t},t)+}_{t}}}{_{t }}_{t}G_{}(x_{t},z,t)\|.\] (4)

## 4 Method

In this section, we initially present the formulation of the proposed task, Disentangling DPMs, in Section 4.1. Subsequently, an overview of DisDiff is provided in Section 4.2. We then elaborate on the detailed implementation of the proposed Disentangling Loss in Section 4.3, followed by a discussion on balancing it with the reconstruction loss in Section 4.4.

### Disentanglement of DPM

We assume that dataset \(=\{x_{0}|x_{0} p(x_{0})\}\) is generated by \(N\) underlying ground truth factors \(f^{c}\), where each \(c=\{1,,N\}\), with the data distribution \(p(x_{0})\). This implies that the underlying factors condition each sample. Moreover, each factor \(f^{c}\) follows the distribution \(p(f^{c})\), where \(p(f^{c})\) denotes the distribution of factor \(c\). Using the Shapes3D dataset as an example, the underlying concept factors include background color, floor color, object color, object shape, object scale, and pose. We illustrate three factors (background color, floor color, and object color) in Figure 1 (c). The conditional distributions of the factors are \(\{p(x_{0}|f^{c})|c\}\), each of which can be shown as a curved surface in Figure 1 (b). The relation between the conditional distribution and data distribution can be formulated as: The conditional distributions of the factors, denoted as \(\{p(x_{0}|f^{c})|c\}\), can each be represented as a curved surface in Figure 1 (b). The relationship between the conditional distribution and the data distribution can be expressed as follows:

\[p(x_{0})= p(x_{0}|f^{c})p(f^{c})df^{c}= p(x_{0}|f^{1},,f^{N})p(f^{1}) p(f^{N})df^{1} df^{N},\] (5)

this relationship holds true because \(f^{1},,f^{N}\) and \(x_{0}\) form a v-structure in Probabilistic Graphical Models (PGM), as Figure 1 (a) depicts. A DPM learns a model \(_{}(x_{t},t)\), parameterized by \(\), to predict the noise added to a sample \(x_{t}\). This can then be utilized to derive a score function: \(_{x_{t}} p(x_{t})=1/-1}_{}(x_{t},t)\). Following  and using Bayes' Rule, we can express the score function of the conditional distribution as:

\[_{x_{t}} p(x_{t}|f^{c})=_{x_{t}} p(f^{c}|x_{t})+_{x _{t}} p(x_{t}),\] (6)

By employing the score function of \(p(x_{t}|f^{c})\), we can sample data conditioned on the factor \(f^{c}\), as demonstrated in Figure 1 (b). Furthermore, the equation above can be extended to one conditioned on a subset \(\) by substituting \(f^{c}\) with \(\). The goal of disentangling a DPM is to model \(_{x_{t}} p(x_{t}|f^{c})\) for each factor \(c\). Based on the equation above, we can learn \(_{x_{t}} p(f^{c}|x_{t})\) for a pre-trained DPM instead, corresponding to the arrows pointing towards the curve surface in Figure 1 (b). However, in an unsupervised setting, \(f^{c}\) is unknown. Fortunately, we can employ disentangled representation learning to obtain a set of representations \(\{z^{c}|c=1,,N\}\). There are two requirements for these representations: \((i)\) they must encompass all information of \(x_{0}\), which we refer to as the _completeness_ requirement, and \((ii)\) there is a bijection, \(z^{c} f^{c}\), must exist for each factor \(c\), which we designate as the _disentanglement_ requirement. Utilizing these representations, the gradient \(_{x_{t}} p(z^{c}|x_{t})\) can serve as an approximation for \(_{x_{t}} p(f^{c}|x_{t})\). In this paper, we propose a method named DisDiff as a solution for the disentanglement of a DPM.

### Overview of DisDiff

The overview framework of DisDiff is illustrated in Figure 2. Given a pre-trained unconditional DPM on dataset \(\) with factors \(\), e.g., a DDPM model with parameters \(\), \(p_{}(x_{t-1}|x_{t})=(x_{t-1};_{}(x_{t},t),_{t})\), our target is to disentangle the DPM in an unsupervised manner. Given an input \(x_{0}\), for each factor \(c\), our goal is to learn the disentangled representation \(z^{c}\) simultaneously and its corresponding disentangled gradient field \(_{x_{t}} p(z^{c}|x_{t})\). Specifically, for each representation \(z^{c}\), we employ an encoder \(E_{}\) with learnable parameters \(\) to obtain the

Figure 1: Illustration of disentanglement of DPMs. (a) is the diagram of Probabilistic Graphical Models (PGM). (b) is the diagram of image space. (c) is the demonstration of sampled images in (b). Surface indicates the conditional data distribution of a single factor \(p(x|f^{c})\). Different colors correspond to various factors. Here, we show three factors: object color, background color, and floor color. Arrows are gradient fields \(_{x_{t}} p(f^{c}|x_{t})\) modeled by using a decoder \(G^{c}_{}(x_{t},t,z^{c})\). The black points are the sampled images shown in (b).

representations as follows: \(E_{}(x_{0})=\{E_{}^{1}(x_{0}),E_{}^{2}(x_{0}),,E_{}^{N}(x_{ 0})\}=\{z^{1},z^{2},,z^{N}\}\). As the random variables \(z^{1},,z^{N}\) and \(x_{0}\) form a common cause structure, it is evident that \(z^{1},,z^{N}\) are independent when conditioned on \(x_{0}\). Additionally, this property also holds when conditioning on \(\). We subsequently formulate the gradient field, denoted as \(_{x_{t}} p(z^{}|x_{t})\), with \(z^{}=\{z^{c}|c\}\), conditioned on a factor subset \(\), as follows:

\[_{x_{t}} p(z^{}|x_{t})=_{c S}_{x_{t}} p (z^{c}|x_{t}).\] (7)

Adopting the approach from [9; 49], we formulate the conditional reverse process as a Gaussian distribution, represented as \(p_{}(x_{t-1}|x_{t},z^{})\). In conjunction with Eq. 7, this distribution exhibits a shifted mean, which can be described as follows:

\[(x_{t-1};_{}(x_{t},t)+_{t}_{c} _{x_{t}} p(z^{c}|x_{t}),_{t}).\] (8)

Directly employing \(_{x_{t}} p(z^{c}|x_{t})\) introduces computational complexity and complicates the diffusion model training pipeline, as discussed in . Therefore, we utilize a network \(G_{}(x_{t},z^{c},t)\), with \(c\) and parameterized by \(\), to estimate the gradient fields \(_{x_{t}} p(z^{}|x_{t})\). To achieve this goal, we first use \(_{c}G_{}(x_{t},z^{c},t)\) to estimate \(_{x_{t}} p(z^{}|x_{t})\) based on Eq. 7. Therefore, we adopt the same loss of PDAE  but replace the gradient field with the summation of \(G_{}(x_{t},z^{c},t)\) as

\[_{r}=}_{x_{0},t,}\|-_{ }(x_{t},t)+}_{t}}}{_{t}} _{t}_{c}G_{}(x_{t},z^{c},t)\|.\] (9)

The aforementioned equation implies that \(x_{0}\) can be reconstructed utilizing all disentangled representations when \(=\), thereby satisfying the _completeness_ requirement of disentanglement. Secondly, in the following, we fulfill the _disentanglement_ requirement and the approximation of gradient field \(_{x_{t}} p(z^{c}|x_{t})\) with decoder \(G_{}(x_{t},z^{c},t)\). In the next section, we introduce a disentangling loss to address the remaining two conditions.

### Disentangling Loss

In this section, we present the Disentangling Loss to address the _disentanglement_ requirement and ensure that \(G_{}(x_{t},z^{c},t)\) serves as an approximation of \(_{x_{t}} p(z^{c}|x_{t})\). Given the difficulty in identifying sufficient conditions for disentanglement in an unsupervised setting, we follow the previous works in disentangled representation literature, which propose necessary conditions that are effective in practice for achieving disentanglement. For instance, the group constraint in  and maximizing mutual information in [27; 45] serve as examples. We propose minimizing the mutual information between \(z^{c}\) and \(z^{k}\), where \(k c\), as a necessary condition for disentangling a DPM.

Subsequently, we convert the objective of minimizing mutual information into a set of constraints applied to the representation space. We denote \(_{0}\) is sampled from the pre-trained unconditioned

Figure 2: Illustration of DisDiff. (a) Grey networks indicate the pre-trained Unet of DPM \(_{}(x_{t},t)\). Image \(x_{0}\) is first encoded to representations \(\{z^{1},z^{2}, z^{N}\}\) of different factors by encoder \(E_{}\) (\(N=3\) in the figure), which are then decoded by decoder \(G_{}\) to obtain the gradient field of the corresponding factor. We can sample the image under the corresponding condition with the obtained gradient field. (b) We first sample a factor \(c\) and decode the representation \(z^{c}\) to obtain the gradient field of the corresponding factor, which allows us to obtain the predicted \(x_{0}\) of that factor. At the same time, we can obtain the predicted \(_{0}\) of the original pre-trained DPM. We then encode the images into two different representations and calculate the disentangling loss based on them.

DPM using \(_{}(x_{t},t)\), \(^{c}_{0}\) is conditioned on \(z^{c}\) and sampled using \(p_{}(x_{t-1}|x_{t},z^{c})\). Then we can extract the representations from the samples \(_{0}\) and \(^{c}_{0}\) with \(E_{}\) as \(^{k}=E^{k}_{}(_{0})\) and \(^{k|c}=E^{k}_{}(^{c}_{0})\), respectively. According to Proposition 1, we can minimize the proposed upper bound (estimator) to minimize the mutual information.

**Proposition 1**.: _For a PGM in Figure 1 (a), we can minimize the upper bound (estimator) for the mutual information between \(z^{c}\) and \(z^{k}\) (where \(k c\)) by the following objective:_

\[}_{k,c,x_{0},^{c}_{0}}\|^{k|c}-^{k }\|-\|^{k|c}-z^{k}\|+\|^{c|c}-z^{c}\|\] (10)

The proof is presented in Appendix K. We have primarily drawn upon the proofs in CLUB  and VAE  as key literature sources to prove our claims. As outlined below, we can partition the previously mentioned objectives into two distinct loss function components.

**Invariant Loss**. In order to minimize the first part, \(\|^{k|c}-^{k}\|\), where \(k c\), we need to minimize \(\|^{k|c}-^{k}\|\), requiring that the \(k\)-th (\(k c,k\)) representation remains unchanged. We calculate the distance scalar between their \(k\)-th representation as follows:

\[d_{k}=\|^{k|c}-^{k}\|.\] (11)

We can represent the distance between each representation using a distance vector \(d=[d_{1},d_{2},,d_{C}]\). As the distances are unbounded and their direct optimization is unstable, we employ the CrossEntropy loss3 to identify the index \(c\), which minimizes the distances at other indexes. The invariant loss \(_{in}\) is formulated as follows:

\[_{in}=}_{c,_{0},^{c}_{0}}[Cross Entropy(d,c)].\] (12)

**Variant Loss**. In order to minimize the second part, \(\|^{c|c}-z^{c}\|-\|^{k|c}-z^{k}\|\), in Eq. 10. Similarly, we implement the second part in CrossEntropy loss to maximize the distances at indexes \(k\) but minimize the distance at index \(c\), similar to Eq. 12. Specifically, we calculate the distance scalar of the representations as shown below:

\[d^{n}_{k} =\|^{k}-z^{k}\|,\] (13) \[d^{p}_{k} =\|^{k|c}-z^{k}\|,\]

We adopt a cross-entropy loss to achieve the subjective by minimizing the variant loss \(_{va}\), we denote \([d^{n}_{1},d^{n}_{2},,d^{n}_{N}]\) as \(d^{n}\) and \([d^{p}_{1},d^{n}_{2},,d^{p}_{N}]\) as \(d^{p}\).

\[_{va}=}_{c,x_{0},_{0},^{c}_{0}}[Cross Entropy(d^{n}-d^{p},c)],\] (14)

However, sampling \(^{c}_{0}\) from \(p_{}(x_{t-1}|x_{t},z^{c})\) is not efficient in every training step. Therefore, we follow  to use a score-based conditioning trick  for fast and approximated sampling. We obtain an approximator \(_{}\) of conditional sampling for different options of \(\) by substituting the gradient field with decoder \(G_{}\):

\[_{}(x_{t},z^{},t)=_{}(x_{t},t)-_{c }_{t}}G_{}(x_{t},z^{c},t).\] (15)

To improve the training speed, one can achieve the denoised result by computing the posterior expectation following  using Tweedie's Formula:

\[^{}_{0}=[x_{0}|x_{t},S]=-_{t}}_{}(x_{t},z^{},t)}{_{t}}}.\] (16)

### Total Loss

The degree of condition dependence for generated data varies among different time steps in the diffusion model . For different time steps, we should use different weights for Disentangling Loss. Considering that the difference between the inputs of the encoder can reflect such changes in condition. We thus propose using the MSE distance between the inputs of the encoder as the weight coefficient:

\[_{d}=\|_{0}-^{c}_{0}\|^{2},_{a}= _{r}+_{d}(_{in}+_{va}).\] (17)

where \(\) is a hyper-parameter. We stop the gradient of \(_{0}\) and \(^{c}_{0}\) for the weight coefficient \(_{d}\).

## 5 Experiments

### Experimental Setup

**Implementation Details**. \(x_{0}\) can be a sample in an image space or a latent space of images. We take pre-trained DDIM as the DPM (DisDiff-IM) for image diffusion. For latent diffusion, we can take the pre-trained KL-version latent diffusion model (LDM) or VQ-version LDM as DPM (DisDiff-KL and DisDiff-VQ). For details of network \(G_{}\), we follow Zhang et al.  to use the extended Group Normalization  by applying the scaling & shifting operation twice. The difference is that we use learn-able position embedding to indicate \(c\):

\[AdaGN(h,t,z^{c})=z^{c}_{s}(t^{c}_{s}GN(h)+t^{c}_{b})+z^{c}_{b},\] (18)

where \(GN\) denotes group normalization, and \([t^{c}_{s},t^{c}_{b}],[z^{c}_{s},z^{c}_{b}]\) are obtained from a linear projection: \(z^{c}_{s},z^{c}_{b}=linearProj(z^{c})\), \(t^{c}_{s},t^{c}_{b}=linearProj([t,p^{c}])\). \(p^{c}\) is the learnable positional embedding. \(h\) is the feature map of Unet. For more details, please refer to Appendix A and B.

**Datasets** To evaluate disentanglement, we follow Ren et al.  to use popular public datasets: Shapes3D , a dataset of 3D shapes. MPI3D , a 3D dataset recorded in a controlled environment, and Cars3D , a dataset of CAD models generated by color renderings. All experiments are conducted on 64x64 image resolution, the same as the literature. For real-world datasets, we conduct our experiments on CelebA .

**Baselines & Metrics** We compare the performance with VAE-based and GAN-based baselines. Our experiment is conducted following DisCo. All of the baselines and our method use the same dimension of the representation vectors, which is 32, as referred to in DisCo. Specifically, the VAE-based models include FactorVAE  and \(\)-TCVAE . The GAN-based baselines include InfoGANCR , GANspace (GS) , LatentDiscovery (LD)  and DisCo . Considering the influence of performance on the random seed. We have ten runs for each method. We use two representative metrics: the FactorVAE score  and the DCI . However, since \(\{z^{c}|c\}\) are vector-wise representations, we follow Du et al.  to perform PCA as post-processing on the representation before evaluation.

    &  &  &  \\   & FactorVAE score & DCI & FactorVAE score & DCI & FactorVAE score & DCI \\   \\  FactorVAE & \(0.906 0.052\) & \(0.161 0.019\) & \(0.840 0.066\) & \(0.611 0.082\) & \(0.152 0.025\) & \(0.240 0.051\) \\ \(\)-TCVAE & \(0.855 0.082\) & \(0.140 0.019\) & \(0.873 0.074\) & \(0.613 0.114\) & \(0.179 0.017\) & \(0.237 0.056\) \\   \\  InfoGAN-CR & \(0.411 0.013\) & \(0.020 0.011\) & \(0.587 0.058\) & \(0.478 0.055\) & \(0.439 0.061\) & \(0.241 0.075\) \\   \\  LD & \(0.852 0.039\) & \(0.216 0.072\) & \(0.805 0.064\) & \(0.380 0.062\) & \(0.391 0.039\) & \(0.196 0.038\) \\ GS & \(0.932 0.018\) & \(0.209 0.031\) & \(0.788 0.091\) & \(0.284 0.034\) & \(0.465 0.036\) & \(0.229 0.042\) \\ DisCo & \(0.855 0.074\) & \(\) & \(0.877 0.031\) & \(0.708 0.048\) & \(0.371 0.030\) & \(0.292 0.024\) \\   \\  DisDiff-VQ (Ours) & \(\) & \(0.232 0.019\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Comparisons of disentanglement on the FactorVAE score and DCI disentanglement metrics (mean \(\) std, higher is better). DisDiff achieves state-of-the-art performance with a large margin in almost all the cases compared to all baselines, especially on the MPI3D dataset.

   Method & FactorVAE score & DCI \\  DisDiff-IM & \(0.783\) & \(0.655\) \\ DisDiff-KL & \(0.837\) & \(0.660\) \\ DisDiff-VQ & \(0.902\) & \(0.723\) \\  DisDiff-VQ wo \(_{in}\) & \(0.782\) & \(0.538\) \\ DisDiff-VQ wo \(_{va}\) & \(0.810\) & \(0.620\) \\ DisDiff-VQ wo \(_{dis}\) & \(0.653\) & \(0.414\) \\ no detach & \(0.324\) & \(0.026\) \\  constant weighting & \(0.679\) & \(0.426\) \\ loss weighting & \(0.678\) & \(0.465\) \\  attention condition & \(0.824\) & \(0.591\) \\ wo pos embedding & \(0.854\) & \(0.678\) \\ wo orth embedding & \(0.807\) & \(0.610\) \\  latent number \(N\) = 6 & \(0.865\) & \(0.654\) \\ latent number \(N\) = 10 & \(0.902\) & \(0.723\) \\   

Table 2: Ablation study of DisDiff on image encoder, components, batchsize and token numbers on Shapes3D.

### Main Results

We conduct the following experiments to verify the disentanglement ability of the proposed DisDiff model. We regard the learned representation \(z^{c}\) as the disentangled one and use the popular metrics in disentangled representation literature for evaluation. The quantitative comparison results of disentanglement under different metrics are shown in Table 1. The table shows that DisDiff outperforms the baselines, demonstrating the model's superior disentanglement ability. Compared with the VAE-based methods, since these methods suffer from the trade-off between generation and disentanglement , DisDiff does not. As for the GAN-based methods, the disentanglement is learned by exploring the latent space of GAN. Therefore, the performance is limited by the latent space of GAN. DisDiff leverages the gradient field of data space to learn disentanglement and does not have such limitations. In addition, DisDiff resolves the disentanglement problem into 1000 sub-problems under different time steps, which reduces the difficulty.

We also incorporate the metric for real-world data into this paper. We have adopted the TAD and FID metrics from  to measure the disentanglement and generation capabilities of DisDiff, respectively. As shown in Table 3, DisDiff still outperforms new baselines in comparison. It is worth mentioning that InfDiffusion  has made significant improvements in both disentanglement and generation capabilities on CelebA compared to previous baselines. This has resulted in DisDiff's advantage over the baselines on CelebA being less pronounced than on Shapes3D.

The results are taken from Table 2 in , except for the results of DisDiff. For TAD, we follow the method used in  to compute the metric. We also evaluate the metric for 5-folds. For FID, we follow Wang et al.  to compute the metric with five random sample sets of 10,000 images. Specifically, we use the official implementation of  to compute TAD, and the official GitHub repository of  to compute FID.

Figure 3: The qualitative results on Shapes3D and CelebA. The source (SRC) images provide the representations of the generated image. The target (TRT) image provides the representation for swapping. Other images are generated by swapping the representation of the corresponding factor. The learned factors on Shapes3D are orientation (Oren), wall color (Wall), floor color (Floor), object color (Color), and object shape (Shape). For more visualizations, please refer to Appendix C-H.

  Model & TAD & FID \\  \(\)-VAE & \(0.088 0.043\) & \(99.8 2.4\) \\ InfoVAE & \(0.000 0.000\) & \(77.8 1.6\) \\ Diffuse & \(0.155 0.010\) & \(22.7 2.1\) \\ InfoDiffusion & \(0.299 0.006\) & \(23.6 1.3\) \\  DisDiff (ours) & \(\) & \(\) \\  

Table 3: Comparisons of disentanglement on Real-world dataset CelebA with TAD and FID metrics (mean \(\) std). DisDiff still achieves state-of-the-art performance compared to all baselines.

### Qualitative Results

In order to analyze the disentanglement of DisDiff qualitatively, we swap the representation \(z^{c}\) of two images one by one and sample the image conditioned on the swapped representation. We follow LDM-VQ to sample images in \(200\) steps. For the popular dataset of disentanglement literature, we take Shapes3D as an example. As Figure 3 shows, DisDiff successfully learned pure factors. Compared with the VAE-based methods, DisDiff has better image quality. Since there are no ground truth factors for the real-world dataset, we take CelebA as an example. As demonstrated in Figure 3, DisDiff also achieves good disentanglement on real-world datasets. Please note that, unlike DisCo , DisDiff can reconstruct the input image, which is unavailable for DisCo. Inspired by Kwon et al. , we generalize DisDiff to complex large-natural datasets FFHQ and CelebA-HQ. The experiment results are given in Appendix E-H.

### Ablation Study

In order to analyze the effectiveness of the proposed parts of DisDiff, we design an ablation study from the following five aspects: DPM type, Disentangling Loss, loss weighting, condition type, and latent number. We take Shapes3D as the dataset to conduct these ablation studies.

**DPM type** The decomposition of the gradient field of the diffusion model derives the disentanglement of DisDiff. Therefore, the diffusion space influences the performance of DisDiff. We take Shapes3D as an example. It is hard for the model to learn shape and scale in image space, but it is much easier in the latent space of the auto-encoder. Therefore, we compare the performance of DisDiff with different diffusion types: image diffusion model, e.g., DDIM (DisDiff-IM), KL-version latent diffusion model (DisDiff-KL), and VQ-version latent diffusion model, e.g., VQ-LDM (DisDiff-VQ). As shown in Table 2, the LDM-version DisDiff outperforms the image-version DisDiff as expected.

**Disentangling Loss** Disentangling loss comprises two parts: Invariant loss \(_{in}\) and Variant loss \(_{va}\). To verify the effectiveness of each part, we train DisDiff-VQ without it. \(_{in}\) encourages in-variance of representation not being sampled, which means that the sampled factor will not affect the representation of other factors (\(z^{k},k c\) of generated \(^{c}_{0}\)). On the other hand, \(_{va}\) encourages the representation of the sampled factor (\(z^{c}\) of generated \(^{c}_{0}\)) to be close to the corresponding one of \(x_{0}\). As shown in Table 2, mainly \(_{in}\) promotes the disentanglement, and \(_{va}\) further constrains the model and improves the performance. Note that the disentangling loss is optimized w.r.t. \(G_{}\) but not \(E^{c}_{}\). If the loss is optimized on both modules, as shown in Table 2, DisDiff fails to achieve disentanglement. The reason could be that the disentangling loss influenced the training of the encoder and failed to do reconstruction.

**Loss weighting** As introduced, considering that the condition varies among time steps, we adopt the difference of the encoder as the weight coefficient. In this section, we explore other options to verify its effectiveness. We offer two different weighting types: constant weighting and loss weighting. The

Figure 4: The partial condition sampling on Shapes3D and CelebA. The target (TRT) image provides the representation of partial sampling images. Images in each row are generated by imposing a single gradient field of the corresponding factor on the pre-trained DPM. DisDiff samples image condition on only a single factor. The sampled image has a fixed factor, e.g., the images in the third row have the same background color as the target one. The conditioned factors on Shapes3D are orientation (Oren), wall color (Wall), floor color (Floor), object color (Color), and object shape (Shape).

first type is the transitional way of weighting. The second one is to balance the Distangling Loss and diffusion loss. From Table 2, these two types of weighting hurt the performance to a different extent.

**Condition type & Latent number** DisDiff follows PDAE  and Dhariwal and Nichol  to adopt AdaGN for injecting the condition. However, there is another option in the literature: cross-attention. As shown in Table 2, cross-attention hurt the performance but not much. The reason may be that the condition is only a single token, which limits the ability of attention layers. We use learnable orthogonal positional embedding to indicate different factors. As shown in Table 2, no matter whether no positional embedding (wo pos embedding) or traditional learnable positional embedding (wo orth embedding) hurt the performance. The reason is that the orthogonal embedding is always different from each other in all training steps. The number of latent is an important hyper-parameter set in advance. As shown in Table 2, if the latent number is greater than the number of factors in the dataset, the latent number only has limited influence on the performance.

### Partial Condition Sampling

As discussed in Section 4.2, DisDiff can partially sample conditions on the part of the factors. Specifically, we can use Eq. 15 to sample image condition on factors set \(\). We take Shapes3D as an example when DisDiff sampling images conditioned on the background color is red. We obtain images of the background color red and other factors randomly sampled. Figure 4 shows that DisDiff can condition individual factors on Shapes3D. In addition, DisDiff also has such ability on the real-world dataset (CelebA) in Figure 4. DisDiff is capable of sampling information exclusively to conditions.

## 6 Limitations

Our method is completely unsupervised, and without any guidance, the learned disentangled representations on natural image sets may not be easily interpretable by humans. We can leverage models like CLIP as guidance to improve interpretability. Since DisDiff is a diffusion-based method, compared to the VAE-based and GAN-based, the generation speed is slow, which is also a common problem for DPM-based methods. The potential negative societal impacts are malicious uses.

## 7 Conclusion

In this paper, we demonstrate a new task: disentanglement of DPM. By disentangling a DPM into several disentangled gradient fields, we can improve the interpretability of DPM. To solve the task, we build an unsupervised diffusion-based disentanglement framework named DisDiff. DisDiff learns a disentangled representation of the input image in the diffusion process. In addition, DisDiff learns a disentangled gradient field for each factor, which brings the following new properties for disentanglement literature. DisDiff adopted disentangling constraints on all different timesteps, a new inductive bias. Except for image editing, with the disentangled DPM, we can also sample partial conditions on the part of the information by superpositioning the sub-gradient field. Applying DisDiff to more general conditioned DPM is a direction worth exploring for future work. Besides, utilizing the proposed disentangling method to pre-trained conditional DPM makes it more flexible.