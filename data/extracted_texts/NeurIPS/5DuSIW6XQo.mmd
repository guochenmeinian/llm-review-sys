# Benchmarking Self-Supervised Video Representation Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Self-supervised learning is an effective way for label-free model pre-training, especially in the video domain where labeling is expensive. Existing self-supervised works in the video domain use varying experimental setups to demonstrate their effectiveness and comparison across approaches becomes challenging with no standard benchmark. In this work, we first provide a benchmark that enables a comparison of existing approaches on the same ground. Next, we study five different aspects of self-supervised learning important for videos; 1) dataset size, 2) complexity, 3) data distribution, 4) data noise, and, 5) feature analysis. To facilitate this study, we focus on six different methods along with six different network architectures and perform an extensive set of experiments on five different datasets with an evaluation of two different downstream tasks. We present several interesting insights from this study which span across different properties of pretraining and target datasets, pretext-tasks, and model architectures among others. Furthermore, we extend these findings to Video Foundation models (ViFMs). Finally, we put some of these insights to the real test and propose an approach that requires a limited amount of training data and outperforms existing state-of-the-art approaches which use 10x pretraining data. We believe this work will pave the way for researchers to a better understanding of self-supervised representation learning in videos.

## 1 Introduction

Deep learning models require a large amount of labeled data for their training. Obtaining annotations at large-scale needs a lot of effort and it becomes even more challenging as we shift from image to video domain. There are several interesting directions focusing on this issue such as domain adaptation , knowledge distillation , semi-supervised learning , self-supervision  and weakly-supervised learning , which attempts to rely on the knowledge learned from existing source datasets and transfer to new target datasets with minimal labels. Among these approaches, self-supervised learning use pretext task as supervisory signal and does not require any labels on source datasets which makes it more favorable.

In recent years, we have seen great progress in self-supervised learning (SSL) in video domain [75; 32; 78; 69; 49; 10]. More recently, the focus is more towards context-based learning which involves modifying input data such that to derive a classification [73; 13; 75; 32], reconstruction [78; 10] or generative [67; 58; 24; 63; 46] signal which can be used as a learning objective. The main focus of these works is designing a pretext task that is computationally inexpensive and which provides a strong supervisory signal such that the model learns meaningful _spatio-temporal_ features.

Despite this great progress, it is non-trivial to compare these approaches against each other due to a lack of standard protocols. These methods are evaluated under different conditions and there is no standard benchmark to evaluate the fair effectiveness of these methods. A recent study [(62)] attempts to take a step towards this direction, but it is mainly focused on downstream learning, without exploring the self-supervision aspect which is one of the main goals in our study. In this work, we present a benchmark where important self-supervised pre-training parameters are kept consistent across methods for a fair comparison. With the help of this benchmark, we study several critical aspects which are important for self-supervised learning; _1) effect of pretraining dataset size, 2) task complexity, 3) generalization under distribution shift, 4) robustness against data noise, 5) properties of learned features._ Fig. 1 provides an overview.

The proposed benchmark includes a large-scale assessment of context-based representative self-supervised methods for video representation learning. We analyze two different factors: 1) _learning objective_ which includes _contrastive_ vs _non-contrastive_, and 2) _data transformation_ that comprises three categories namely, _spatial_, _temporal_, and _spatio-temporal_. We study six different pretext tasks with six different models and perform our experiments on five different action recognition datasets and evaluate these approaches on two different downstream tasks, action recognition, and video retrieval. Furthermore, we extend the study to recently developed video foundation models.

We observe some interesting insights in this benchmark; 1) Contrastive tasks are fast learners but are less robust against data noise, 2) there is no direct relation that increase in pretext task complexity leads to better understanding of spatio-temporal representation learning, 3) _temporal_ based pretext tasks are more difficult to solve than _spatial_ and _spatio-temporal_, 4) spatio-temporal task can solve the pretext task independent of data distribution shifts, and finally, 5) we empirically show that these pretext tasks learn complementary features across factors such as model architecture, dataset distributions, dataset size, and pretext task. Our contributions are threefold:

* We present a benchmark for self-supervised video representation learning to compare different pretext tasks under a similar experimental setup.
* We perform extensive analysis on 5 important factors for self-supervised learning in videos; 1) dataset size, 2) task complexity, 3) distribution shift, 4) data noise, and, 5) feature analysis.
* Finally, we put some of our insights from this study to test and propose a simple approach that outperforms existing state-of-the-art methods on video action recognition with a limited

Figure 1: **Overview of proposed benchmark.** We study five different aspects in this benchmark study. Starting from left, 1) we show the analysis of _effect of dataset size vs training time_. As the dataset size increases, variation in performance decreases even with longer training time, 2) We show the effect of _task complexity_ (C1, C2, C3 - Different complexities). Bottom figure shows use case of how complexity increases for the RotNet task, and, top figure shows how the performance varies for the R21D network, 3) With different _data distribution shifts_, the third sub-figure shows the impact of _target_ data distribution on the _source_ data, 4) We look into another data distribution shift due to introduction of noise. We see how _non-contrastive_ tasks are more robust than _contrastive_ ones even with increasing levels of severity of noise. The bottom part shows an example for each type of noise. Clips are provided in supplementary, and, 5) Finally, we further analyze whether the features learn _orthogonal_ information. In this sub-figure, we show that using different architectures as teachers can substantially improve performance even in a low-data regime.

amount of pretraining data. Additionally, based on our findings, we put down a set-up recipe for future self-supervised learning algorithms to build upon.

## 2 Related Work

Self-supervised learningThere are several works in the domain of self-supervised learning for video representation learning [(31; 55)]. These approaches can be grouped into two main categories on the basis of pretext task: 1) context-based [(34; 71; 3; 19; 73; 61; 76; 13; 30; 69; 49; 10; 16; 23; 50)], and 2) cross-modal [(48; 53; 1)]. Cross-modal approaches use multiple modalities such as audio, video, optical flow, and camera positions, and rely on consistencies across these modalities. Context-based learning exploits data transformations to derive supervisory signals for training the model. Context-based pretraining tasks have evolved a lot in the past few years. Our work explores the domain of how much variation in learned representations under different transformations. In contrast to other approaches, context-based approaches exploit the spatial and temporal information independently by several transformations [(43; 19; 75; 7; 49; 69)]. Recent works have started to transform the spatial and temporal domain together [(34; 42; 61; 78; 10)]. Incorporating multiple modalities improves performance, but, it's not available for all datasets, especially large-scale datasets. In this work, we restrict our focus to single-modality (RGB) approaches.

Self-supervised benchmarkingThere are some prior efforts focusing on benchmarking self-supervised learning in the image domain. In [(21)], the authors provide a detailed analysis of image-based self-supervised learning approaches and study how dataset size scaling affects the learned representations. Similarly in [(35)], the authors analyze how different model architectures play a role in visual self-supervised learning. In both these works, the authors did not focus on the importance of various pretext tasks themselves but only showed how certain pretext tasks can be improved. Therefore, their main focus was on downstream tasks rather than pretext learning. We, on the other hand, study different pretext tasks and analyze how various aspects affect feature learning. Moreover, these works are focused on the image domain, whereas we focus on the video domain. In recent work, [(18)], a study was performed to better understand unsupervised learning in the video domain. It explored the use of several pre-text tasks from the image domain and applied them to videos. We are not merely focusing on down-stream tasks and our attention is on the self-supervised aspect which includes factors such as data subset size, task complexity, dataset distribution, and noise robustness.

## 3 Self-Supervised Configurations

We first describe the pretext tasks used in our study along with their categorization followed by details of this benchmark including network architectures, datasets, downstream tasks and evaluations.

### Tasks categorization

We analyze two different aspects of video pretext tasks: 1) transformations applied to data, and 2) learning objectives. Data transformations include, _spatial-based (S)_, _temporal-based (T)_ and _spatiotemporal (ST)_. _Spatial_ transformations include reshuffling of spatial patches, temporal consistent data augmentation, or rotation of images/patches. _Temporal_ tasks involve permutation classification of frames/clip, order verification, clips sampling at different paces, or, contrastive learning from temporal triplets. _Spatio-temporal_ tasks include those in which we modify both of these parameters simultaneously. This includes dilated sampling and simultaneous frame reconstruction, shuffling spatial and temporal domains, or, speed prediction, and contrastive visual features. Learning objectives can be either _contrastive_ [(11)] or _non-contrastive_ such as [(78)].

Following this categorization, we select at least two representative pretext tasks from each _transformation_ category, one _contrastive_ and one _non-contrastive_. We study the following pretext tasks: RotNet (Rot) [(32)], Video Clip Order Prediction (VCOP) [(75)], Playback Rate Prediction (PRP) [(78)], Spatiotemporal Contrastive Video Representation Learning (CVRL) [(49)], Temporal DiscriminativeLearning (TDL) (69) and Relative Speed Perception network (RSPNet) (10). The description of tasks are provided in the supplementary (Section C).

### Benchmark details

This section standardizes the conditions used by our benchmark to compare different pretext tasks. Further explanation for using these conditions are outlined in the supplementary.

_Datasets:_ We experiment with two different dataset types, 1) where appearance is more important, and 2) where time is more important. For appearance based, we use Kinetics-400 (33), UCF101 (57), and HMDB51 (38), where appearance is more important (recognize activity with a single frame) than temporal aspect, and for temporal aspect, we use Something Something-V2 (22) and Diving48 (39), where temporal information plays a significant role (require few frames to recognize activity). More details are in the supplementary.

_Spatio-temporal architectures:_ We consider three different network capacities, 1) small-capacity, 2) medium-capacity, and large-capacity. For small capacity networks, we use ShuffleNet V1 2.0X (79), whereas for medium capacity we focus on R(2+1)D (65) (R21D). We do not include large capacity networks in our main benchmark in the interest of computational efficiency; additional results for such a model, VideoSwin (41) is shown in the supplementary.

_Downstream tasks:_ We show results and analysis on two different downstream tasks - _action recognition_ and _clip retrieval_. These two tasks are the most prominent in the field of self-supervised learning in videos. Full finetuning is performed as opposed to linear probing to adapt models.

_Evaluation and Analysis:_ We use top-1 accuracy for action recognition and top-K for Clip retrieval. For robustness performance, we calculate the relative robustness score \((R_{s})\) using original accuracy on clean test set \((A_{c})\) and perturbed accuracy on noisy test set\((A_{p})\) as \(R_{s}=-A_{p}}{A_{s}}\). Centered Kernel alignment (CKA) (44) maps illustrates model behaviours. More details in supplementary.

## 4 Benchmark Analysis

In this section, we perform analysis across the following five aspects:

_Effect of pretraining dataset size:_ In self-supervised learning, a natural question to ask is whether dataset size plays any role in the performance of downstream tasks. It is important to study if the increase in the size of the pretraining dataset will proportionally reciprocate in performance improvement. Also, a general trend is to train models for a very long duration at the pre-training stage. We investigate if the longer duration actually impacts the gain in performance. We look across different stages of training for multiple architectures and across different pretext tasks.

_Impact of task complexity:_ Some of the existing works show that increasing complexity leads to better representation learning, and if the complexity is decreased, the network will optimize to suboptimal solutions. We analyze this aspect in more detail with several tasks and different models.

_Effect of data distribution:_ Existing self-supervised methods perform evaluations on K400 and UCF101 datasets. Both these datasets fall into the same visual category with heavy appearance bias. However, we divert our attention towards datasets where the temporal dimension plays an important role such as SSv2 and Diving48.

_Robustness of SSL tasks:_ We study the robustness qualities of SSL methods against data noise (26). We analyze which factors play a key role in robustness of these methods against such domain shifts.

_Feature analysis:_ Finally, we look into feature space and analyze whether the learned representations are complementary in nature when models are trained under different protocols.

### Effect of dataset-size

We first analyze the effects of pre-training data size variation. The network trains on four subsets of the K400: 10k, 30k, 50k, and 100k. The number of videos per class is the same. The smaller pre-training dataset is a subset of the bigger pre-training dataset size (i.e. \(10k 30k\) and so on). We look into three aspects regarding _dependence on pre-train subset size:_ a) behavior of different pretext tasks with the increase in pre-train dataset subset, b) performance across the different capacity of backbones, and, c) the effect of training time across different pretext tasks.

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

from both source datasets and outperforms the teacher. Student network outperforms standalone spatio-temporal network performance in both contrastive and non-contrastive domains.

**Inference:** (i) _Knowledge can be distilled from different architectures for a given subset size (Fig. 3 (a))_, (ii) _Knowledge from different source datasets brings in complementary information (Fig. 3 (c))_, and (iii) _Orthogonal features are learned across different categories of pretext tasks (Fig. 3 (d))_.

## 5 Lessons Learned

With all the analysis along studied axes, we learned a few lessons in-between these axes such as: (i) Contrastive tasks are fast learners but are also most susceptible to noise. (ii) An increase in dataset size or complexity does not help smaller models in learning better spatio-temporal features but these features are more robust to noise. (iii) Temporal tasks are relatively more difficult to learn since looking at the correlation between time of training, increase in dataset size, and complexity, the performance gain is minimal in each of this axis. It means this category of tasks is actually difficult to solve. (iv) Spatio-temporal pretext tasks improve with the increase in complexity and dataset size (if the model permits), and their behavior to learn better spatio-temporal features is independent of data distribution. Using these lessons, we further do more analysis in feature space. From there, we observe within an axis of comparison how models learn orthogonal information. Based on those observations, we analyze if we can push the performance for downstream tasks. We look into two downstream tasks: action classification and clip retrieval.

Figure 4: **Knowledge distillation using teachers trained on multiple subset sizes on RSPNet. Student: ShuffleNet a) UCF101 and b) HMDB51. Here T1 is Teacher-1 (shufflenet) and T2 is teacher-2 (R21D). Top@$_5 Clip Retrieval - R21D on c) UCF101 and d) HMDB51, pre-trained on K400 and SSV2 - 30k subset.**

Figure 3: **Feature analysis overview. This figure shows how KD as a tool is beneficial across multiple scenarios. Brief details for each setup (Left to right): (A) _Effect of dataset size:_ Teachers (T1 and T2) are different architectures for a single subset. Student model (ST-Shuffle) CKA maps shows it learns complementary information especially for 30k. (B) _Task Complexity:_ Teachers are multiple complexities across the same task. (C1, C2, C3 - different complexities as teachers.) We observe in most of the scenarios, Student (ST) networks outperforms all teacher models which proves learning of orthogonal information from multiple teachers. (C) _Out-of-Distribution:_ Models from different _source_ datasets are teachers. Student model (ST) outperforms both teachers trained on two different datasets. (D) _Pretext Tasks:_ Spatial and temporal task networks are teachers, and, student model (ST) learnt from two different categories of pretext tasks - spatial and temporal incorporate knowledge from both and outperforms both of the teachers for both contrastive and non-contrastive.**

Clip retrievalFor this downstream task, we generate feature vectors using pretrained weights. The nearest neighbor is found by measuring the cosine distance between test and train feature vectors. We show analysis on UCF101 and HMDB51, with different source data distributions, K400 and SSv2.

_Observations:_ Spatio-temporal task still outperform other categories independent of _source_ data distribution similar to what we observe earlier. Contrastive learns better _appearance_ features during the pre-training stage given both downstream datasets are _appearance_ based. Temporal tasks have almost similar performance pre-trained on either of the _source_ datasets, which shows even with an appearance-based dataset as a pre-train dataset, the task is not focusing much on spatial features.

Action ClassificationFor this task, the model is finetuned end-to-end on downstream datasets, on UCF101 and HMDB51. In Table 5, we obtain our best performing model via knowledge distillation discussed in previous section and we show our model outperforms previous state-of-the-art approaches.

_Observations:_ With only 30k videos compared to 200k+ videos used by other pretext tasks, we show that our model outperforms by a good margin on UCF101 against single and multi-modal approaches. We got competitive results on HMDB51 with a score of 51.5%.

### Surprising Findings

We have multiple inferences from different axes of analysis. However, to club a few which are new and helpful for video self-supervised community, we list down those here:

_Dataset size and Training time Dependency:_ Against the conventional belief that a lot of training data is a _must_ to achieve the best performance, we demonstrate that beyond a certain amount of training data, additional data provides diminishing returns for SSL in terms of performance improvement. This finding has significant implications, as it allows for a substantial reduction in the training data and there is almost a 10x reduction in training time which is particularly advantageous in computationally demanding video processing tasks. Furthermore, we show how KD as a tool, outperforms the original approach (100% data) using almost 90% less data further optimizing resource utilization by 80%.

_Robustness to real-world noise:_ To our surprise, contrastive tasks are more susceptible to noise than non-contrastive. A smaller network tends to be more robust in some scenarios than a bigger network. We believe these findings are _novel and not known_ to the community as there is no existing study exploring these aspects and are helpful where robustness is necessary for real-world deployment.

   Approach & Venue & NxW/H & Backbone & Pre-training & UCF101 & HMDB51 \\ 
**Generative** &  &  &  &  \\  VIMPACE (60) &  &  & ViT-L & HTM & 92.7 & 65.9 \\ VideoMLE (63) & NeurIPS’22 & 16x224 & ViT-B & K400 & 91.3 & 62.6 \\ MME (59) & CVPR’23 & 16x224 & ViT-B & K400 & 96.5 & 78.0 \\ DVD (70) & CVPR’23 & 16x224 & ViT-B & INIK’4K00 & 97.0 & 76.4 \\ EVEREST (28) &  &  & ViT-B & - & 93.4 & 68.1 \\ SCE (14) & WACV’23 & 16x224 & ResNet3D-50 & K400 & 95.3 & 74.7 \\ 
**Context** &  &  &  &  \\  PacePred (73) & ECCV’20 & 16x112 & R21D-18 & K400 & 77.1 & 36.6 \\ TempTrans (30) & ECCV’20 & 16x112 & R3D-18 & K400 & 79.3 & 49.8 \\ STS (68) & TPAMI-21 & 16x112 & R21D-18 & K400 & 77.8 & 40.5 \\ VideoMoCo (46) & CVPR’21 & 16x112 & R21D-18 & K400 & 78.7 & 49.2 \\ RSPNet (10) & AAAI’21 & 16x112 & R21D-18 & K400 & 81.1 & 44.6 \\ TaCo (6) &  & R21D-18 & K400 & 81.8 & 46.0 \\ TCRR(13) & CVI’U2’22 & 16x112 & R21D-18 & K400 & 88.2 & 60.0 \\ CVRL (49) & CVPR’21 & 32x224 & R21D-18 & K400 & 92.9 & 67.9 \\ TransRank (16) & CVPR’22 & 16x112 & R21D-18 & K200 & 87.8 & 60.1 \\ 
**Multi-Modal** &  &  &  &  \\  AVTS (37) & NeurIPS’18 & 25x224 & I3D & K400 & 83.7 & 53.0 \\ GDT (47) \(\) & - & 32x112 & R21D & IG65M & 95.2 & 72.8 \\ XDC (4) & NeurIPS’20 & 32x224 & R21D & K400 & 84.2 & 47.1 \\  Ours \({}^{*}\) & - & 16x112 & R21D-18 & K400-30k & 97.3 & 51.5 \\   

Table 5: **Comparison with previous approaches** pre-trained on K400. Ours ( \({}^{*}\) best performing) is RSPNet pretrained on 30k subset of K400. \({}^{}\) - Different pre-training data. (%)

#### Complementary knowledge:

Improvement in performance with KD from different data distributions and categories of tasks brings out a recipe for a new SSL task. This involves utilizing a multi-teacher multi-student setup, where each teacher specializes in spatial and temporal tasks and is trained on a mixture of data sources. Our analysis indicates this would provide a strong learning scenario.

#### 5.2 Recommendations

Looking into several factors, here we provide a few recommendations to set up the recipe for SSL: 1) _Training speed:_ If training time is a concern, contrastive tasks can help in reducing the pretraining time, but they could be less robust against data noise. 2) _Data distribution:_ It is always better to use a spatio-temporal pretext task irrespective of the data distribution. However, if that is not an option, the pretext task should always be aligned with the nature of the pretraining dataset. 3) _Model capacity:_ If model capacity is limited, there is no benefit of increasing pretraining dataset size and using complex pretext tasks. 4) _Robustness:_ If best performance is the goal, we should use a non-contrastive as opposed to a contrastive pretext task. 5) _Performance:_ Pretext tasks learn complementary features across model architectures, pretraining datasets, pretext tasks, and tasks complexity, therefore, this complementary knowledge can be distilled to obtain strong spatio-temporal features.

#### Extension of findings to Video Foundation Models (ViFMs)

In this section, we extend the study to ViFMs (Tables 6 and 7). We select both image-based [2; 45; 51] which are image foundation models extended to videos and video-based [80; 72] which are trained from scratch on videos. ViFMs are all trained with contrastive pretraining objective. More details about architectures are in supplementary.

#### Dataset size:

An increase in dataset size or complexity does not help smaller models in learning better spatio-temporal features (Table 6). ViCLIP and LanguageBind, despite using a significantly larger pretraining dataset, performs worse than models pretrained on the smaller Kinetics-400 dataset; A simple increase in the number of frames is outperforms models trained on larger datasets.

#### Complementary knowledge:

Improvement in performance in the case of KD from different ViFMs brings out a recipe for training a new foundational model. This involves utilizing a multi-teacher multi-student setup, where each teacher is a ViFM pretrained differently in terms of data sources, multi-stage pretraining, and pretraining objective. Our analysis (Table 7) indicates this would provide a powerful learning scenario.

## 6 Conclusion

In this study, we explore different parameters for self-supervised learning in the video domain. We set a benchmark which provides an intuitive task categorization and enables a better comparison of different pretext tasks. Such an analysis has never been explored for video understanding to the best of our knowledge. We presented several interesting insights which will open up new directions for the research community. We also demonstrate the usefulness of some of these insights where we obtain state-of-the-art performance on video action recognition using merely a 10% pretraining dataset when compared with existing methods. We believe this benchmark study will help the research community better understand self-supervised learning in the video domain.

   ViFM & Type. Pretraining Data Frames \(\) Rate & Accuracy \\  ViF-CLIP  & 1 & K-400 & 32 x 2 \\ X-CLIP  & 1 & K-400 & 8 x 8 71.6 \\ EZ-CLIP  & 1 & K-400 & 8 x 8 70.5 \\ ViCILP  & V & InterVid-10M & 8 x 8 8 75.5 \\ LanguageBind  & V & VIDAL-10M & 8 x 8 8 69.9 \\    
   ViFM & X-CLIP & ViF-CLIP & EZ-CLIP & ViCILP LanguageBind \\  X-CLIP & X & 83.2 & 88.7 & 88.2 & 87.6 \\ ViF-CLIP & X & x & 88.0 & 86.6 & 86.6 \\ EZ-CLIP & X & x & x & 85.0 & 86.9 \\ VCLIP & X & x & x & x & 85.4 \\ LanguageBind & X & x & x & x & x \\   

Table 6: **Analysis on ViFMs. Zero-shot classification accuracy on UCF-101. I:Image, V: Video.**

   ViFM & X-CLIP & ViF-CLIP & EZ-CLIP & ViCILP LanguageBind \\  X-CLIP & X & 83.2 & 88.7 & 88.2 & 87.6 \\ ViF-CLIP & X & x & 88.0 & 86.6 & 86.6 \\ EZ-CLIP & X & x & x & 85.0 & 86.9 \\ VCLIP & X & x & x & x & 85.4 \\ LanguageBind & X & x & x & x & x \\    
   ViFM & X-CLIP & ViF-CLIP & EZ-CLIP & ViCILP LanguageBind \\  X-CLIP & X & 83.2 & 88.7 & 88.2 & 87.6 \\ ViF-CLIP & X & x & 88.0 & 86.6 & 86.6 \\ EZ-CLIP & X & x & x & 85.0 & 86.9 \\ VCLIP & X & x & x & x & 85.4 \\ LanguageBind & X & x & x & x & x \\   

Table 7: **Knowledge Distillation between different ViFM pairs as teachers, and R21D as the student.**