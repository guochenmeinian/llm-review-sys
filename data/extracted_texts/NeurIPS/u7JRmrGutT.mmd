# Graph Edit Distance with General Costs

Using Neural Set Divergence

 Eeshaan Jain &Indradyumna Roy1\({}^{}\)

Saswat Meher\({}^{}\) Soumen Chakrabarti\({}^{}\) Abir De\({}^{}\)

\({}^{}\)EPFL \({}^{}\)IIT Bombay

eeshaan.jain@epfl.ch

{saswatmeher,soumen,indraroy15,abir}@cse.iitb.ac.in

Equal contribution. Eeshaan Jain did this work while at IIT Bombay.

38th Conference on Neural Information Processing Systems (NeurIPS 2024).

###### Abstract

Graph Edit Distance (GED) measures the (dis-)similarity between two given graphs, in terms of the minimum-cost edit sequence that transforms one graph to the other. However, the exact computation of GED is NP-Hard, which has recently motivated the design of neural methods for GED estimation. However, they do not explicitly account for edit operations with different costs. In response, we propose GraphEdX, a neural GED estimator that can work with general costs specified for the four edit operations, _viz._, edge deletion, edge addition, node deletion and node addition. We first present GED as a quadratic assignment problem (QAP) that incorporates these four costs. Then, we represent each graph as a set of node and edge embeddings and use them to design a family of neural set divergence surrogates. We replace the QAP terms corresponding to each operation with their surrogates. Computing such neural set divergence require aligning nodes and edges of the two graphs. We learn these alignments using a Gumbel-Sinkhorn permutation generator, additionally ensuring that the node and edge alignments are consistent with each other. Moreover, these alignments are cognizant of both the presence and absence of edges between node-pairs. Experiments on several datasets, under a variety of edit cost settings, show that GraphEdX consistently outperforms state-of-the-art methods and heuristics in terms of prediction error. The code is available at https://github.com/structlearning/GraphEdX.

## 1 Introduction

The Graph Edit Distance (GED) between a source graph, \(G\), and a target graph, \(G^{}\), quantifies the minimum cost required to transform \(G\) into a graph isomorphic to \(G^{}\). This transformation involves a sequence of edit operations, which can include node and edge insertions, deletions and substitutions. Each type of edit operation may incur a different and distinctive cost, allowing the GED framework to incorporate domain-specific knowledge. Its flexibility has led to the widespread use of GED for comparing graphs across diverse applications including graph retrieval , pattern recognition , image and video indexing  and chemoinformatics . Because costs for addition and deletion may differ, GED is not necessarily symmetric, _i.e._, \((G,G^{})(G^{},G)\). This flexibility allows GED to model a variety of graph comparison scenarios, such as finding the Maximum Common Subgraph  and checking for Subgraph Isomorphism . In general, it is hard to even approximate GED . Recent work  has leveraged graph neural networks (GNNs) to build neural models for GED computation, but many of these approaches cannot account for edit operations with different costs. Moreover, several approaches  cast GED as the Euclidean distance between graph embeddings, leading to models that are overly attuned to cost-invariant edit sequences.

### Present work

We propose a novel neural model for computing GED, designed to explicitly incorporate the various costs of edit operations. Our contributions are detailed as follows.

Neural set divergence surrogates for GEDWe formulate GED under general (non-uniform) cost as a quadratic assignment problem (QAP) with four asymmetric distance terms representing edge deletion, edge addition, node deletion and node addition. The edge-edit operations involve quadratic dependencies on a node alignment plan -- a proposed mapping of nodes from the source graph to the target graph. To avoid the the complexity of QAP , we design a family of differentiable set divergence surrogates, which can replace the QAP objective with a more benign one. In this approach, each graph is represented as a set of embeddings of nodes and node-pairs (edges or non-edges). We replace the original QAP distance terms with their corresponding set divergences, and obtain the node alignment from a differentiable alignment generator modeled using a Gumbel-Sinkhorn network. This network produces a soft node permutation matrix based on contextual node embeddings from the graph pairs, enabling the computation of the overall set divergence in a differentiable manner, which facilitates end-to-end training. Our proposed model relies on late interaction, where the interactions between the graph pairs occur only at the final layer, rather than during the embedding computation in the GNN. This supports the indexing of embedding vectors, thereby facilitating efficient retrieval through LSH [25; 24; 12], inverted index , graph based ANN [34; 37]_etc_.

Learning all node-pair representationsThe optimal sequence of edits in GED is heavily influenced by the global structure of the graphs. A perturbation in one part of the graph can have cascading effects, necessitating edits in distant areas. To capture this sensitivity to structural changes, we associate both edges as well as non-edges with suitable expressive embeddings that capture the essence of subgraphs surrounding them. Note that the embeddings for non-edges are never explicitly computed during GNN message-passing operations. They are computed only once, after the GNN has completed its usual message-passing through _existing_ edges, thereby minimizing additional computational overhead.

Node-edge consistent alignmentTo ensure edge-consistency in the learned node alignment map, we explicitly compute the node-pair alignment map from the node alignment map and then utilize this derived map to compute collective edge deletion and addition costs. More precisely, if \((u,v) G\) and \((u^{},v^{}) G^{}\) are matched, then the nodes \(u\) and \(v\) are constrained to match with \(u^{}\) and \(v^{}\) (or, \(v^{}\) and \(u^{}\)) respectively. We call our neural framework as GraphEdX.

Our experiments across several real datasets show that (1) GraphEdX outperforms several state-of-the-art methods including those that use early interaction; (2) the performance of current state-of-the-art methods improves significantly when their proposed distance measures are adjusted to reflect GED-specific distances, as in our approach.

## 2 Related work

Heuristics for Graph Edit DistanceGED was first introduced in . Bunke and Allermann  used it as a tool for non exact graph matching. Later on,  connected GED with maximum common subgraph estimation. Blumenthal  provide an excellent survey. As they suggest, combinatorial heuristics to solve GED predominantly follows three approaches: (1) Linear sum assignment problem with error-correction, which include [27; 41; 53; 55] (2) Linear programming, which predominantly uses standard tools like Gurobi, (3) Local search . However, they can be extremely time consuming, especially for a large number of graph pairs. Among them Zheng et al.  operate in our problem setting, where the cost of edits are different across the edit operations, but for the same edit operation, the cost is same across node or node pairs.

Optimal TransportIn our work, we utilize Graph Neural Networks (GNNs) to represent each graph as a set of node embeddings. This transforms the inherent Quadratic Assignment Problem (QAP) of graph matching into a Linear Sum Assignment Problem (LSAP) on the sets of node embeddings. Essentially, this requires solving an optimal transport problem in the node embedding space. The use of neural surrogates for optimal transport was first proposed by Cuturi , who introduced entropy regularization to make the optimal transport objective strictly convex and utilized Sinkhorn iterations  to obtain the transport plan. Subsequently, Mena et al.  proposed the neural Gumbel Sinkhorn network as a continuous and differentiable surrogate of a permutation matrix, which we incorporate into our model.

In various generative modeling applications, optimal transport costs are used as loss functions, such as in Wasserstein GANs [1; 3]. Computing the optimal transport plan is a significant challenge, with approaches leveraging the primal formulation [52; 33], the dual formulation with entropy regularization [17; 48; 22], or Input Convex Neural Networks (ICNNs) .

Neural graph similarity computationMost earlier works on neural graph similarity computation have focused on training with GED values as ground truth [5; 6; 19; 40; 56; 39; 54; 31], while some have used MCS as the similarity measure [6; 5]. Current neural models for GED approximation primarily follow two approaches. The first approach uses a trainable nonlinear function applied to graph embeddings to compute GED [5; 39; 6; 54; 19]. The second approach calculates GED based on the Euclidean distance in the embedding space [31; 40].

Among these models, GOTSIM  focuses solely on node insertion and deletion, and computes node alignment using a combinatorial routine that is decoupled from end-to-end training. However, their network struggles with training efficiency due to the operations on discrete values, which are not amenable to backpropagation. With the exception of GREED  and Graph Embedding Network (GEN) , most methods use early interaction or nonlinear scoring functions, limiting their adaptability to efficient indexing and retrieval pipelines.

## 3 Problem setup

NotationThe source graph is denoted by \(G=(V,E)\) and the target graph by \(G^{}=(V^{},E^{})\). Both graphs are undirected and are padded with isolated nodes to equalize the number of nodes to \(N\). The adjacency matrices for \(G\) and \(G^{}\) after padding are \(,}\{0,1\}^{N N}\). (Note that we will use \(M^{}\), not \(M^{}\), for the transpose of matrix \(M\).) The sets of padded nodes in \(G\) and \(G^{}\) are denoted by \(_{G}\) and \(_{G^{}}\) respectively. We construct \(\{0,1\}^{N}\), where \([u]=0\) if \(u_{G}\) and \(1\) otherwise (same for \(G^{}\)). The embedding of a node \(u V\) computed at propagation layer \(k\) by the GNN, is represented as \(_{k}(u)\). Edit operations, denoted by \(\), belong to one of four types, _viz._, (i) node deletion, (ii) node addition, (iii) edge deletion, (iv) edge addition. Each operation \(\) is assigned a \(\,()\). The node and node-pair alignment maps are described using (hard) permutation matrices \(\{0,1\}^{N N}\) and \(\{0,1\}^{}\) respectively. Given that the graphs are undirected, node-pair alignment need only be specified across at most \(\) pairs. When a hard permutation matrix is relaxed to a doubly-stochastic matrix, we call it a soft permutation matrix. We use \(\) and \(\) to refer to both hard and soft permutations, depending on the context. We denote \(_{N}\) as the set of all hard permutation matrices of dimension \(N\); \([N]\) as \(\{1,,N\}\) and \(\|\|_{1,1}\) to describe \(_{u,v}|[u,v]|\). For two binary variables \(c_{1},c_{2}\{0,1\}\), we denote \(J(c_{1},c_{2})\) as (\(c_{1}\) XOR \(c_{2}\)), _i.e._, \(J(c_{1},c_{2})=c_{1}c_{2}+(1-c_{1})(1-c_{2})\).

Graph edit distance with general costWe define an _edit path_ as a sequence of edit operations \(=\{_{1},_{2},\}\); and \((G,G^{})\) as the set of all possible edit paths that transform the source graph \(G\) into a graph isomorphic to the target graph \(G^{}\). Given \((G,G^{})\) and the cost associated with each operation \(\), the GED between \(G\) and \(G^{}\) is the minimum collective cost across all edit paths in \((G,G^{})\). Formally, we write [14; 7]:

\[(G,G^{})=_{=\{_{1},_{2 },\}(G,G^{})}\ _{i[||]}(_{i}).\] (1)

In this work, we assume a fixed cost for each of the four types of edit operations. Specifically, we use \(a^{}\), \(a^{}\), \(b^{}\) and \(b^{}\) to represent the costs for edge deletion, edge addition, node deletion, and node addition, respectively. These costs are not necessarily uniform, in contrast to the assumptions made in previous works [5; 31; 56; 39]. Additional discussion on GED with node substitution in presence of labels can be found in Appendix B.

Problem statementOur objective is to design a neural architecture for predicting GED under a general cost framework, where the edit costs \(a^{}\), \(a^{}\), \(b^{}\) and \(b^{}\) are not necessarily the same. During the learning stage, these four costs are specified, and remain fixed across all training instances \(=\{(G_{i},G^{}_{i},(G_{i},G^{}_{i})) \}_{i[n]}\). Note that the edit paths are not supervised. Later, given a test instance \(G,G^{}\), assuming the same four costs, the trained system has to predict \((G,G^{})\).

## 4 Proposed approach

In this section, we first present an alternative formulation of GED as described in Eq. (1), where the edit paths are induced by node alignment maps. Then, we adapt this formulation to develop GraphEdX, a neural set distance surrogate, amenable to end-to-end training. Finally, we present the network architecture of GraphEdX.

### GED computation using node alignment map

Given the padded graph pair \(G\) and \(G^{}\), deleting a node \(u V\) can be viewed as aligning node \(u\) with some padded node \(u^{}_{G^{}}\). Similarly, adding a new node \(u^{}\) to \(G\) can be seen as aligning some padded node \(u_{G}\) with node \(u^{} V^{}\). Likewise, adding an edge to \(G\) corresponds to aligning a non-edge \((u,v) E\) with an edge \((u^{},v^{}) G^{}\). Conversely, deleting an edge in \(G\) corresponds to aligning an edge \((u,v) G\) with a non-edge \((u^{},v^{}) G^{}\).

Therefore, \((G,G^{})\) can be defined in terms of a node alignment map. Let \(_{N}\) represent the set of all node alignment maps \(:[N][N]\) from \(V\) to \(V^{}\). Recall that \(_{G}[u]=0\) if \(u_{G}\) and \(1\) otherwise.

\[_{_{N}} _{u,v}(a^{}[(u,v)  E((u),(v)) E^{}]+a^{} [(u,v) E((u),(v)) E^{}])\] \[+_{u}(b^{}_{G}[u](1-_{G ^{}}[(u)])+b^{}(1-_{G}[u])_{G^{}}[(u)]).\] (2)

In the above expression, the first sum iterates over all pairs of \((u,v)[N][N]\) and the second sum iterates over \(u[N]\). Because both graphs are undirected, the fraction \(1/2\) accounts for double counting of the edges. The first and second terms quantify the cost of deleting and adding the edge \((u,v)\) from and to \(G\), respectively. The third and the fourth terms evaluate the cost of deleting and adding node \(u\) from and to \(G\), respectively.

GED as a Quadratic Assignment ProblemIn its current form, Eq. (2) cannot be immediately adapted to a differentiable surrogate. To circumvent this problem, we provide an equivalent matricized form of Eq. (2), using a hard node permutation matrix \(\) instead of the alignment map \(\). We compute the asymmetric distances between \(\) and \(P^{}}\) and combine them with weights \(a^{}\) and \(a^{}\). Notably, \((-P^{}})[u,v]\) is non-zero if the edge \((u,v) E\) is mapped to a non-edge \((u^{},v^{}) E^{}\) with \([u,u^{}]=[v,v^{}]=1\), indicating deletion of the edge \((u,v)\) from \(G\). Similarly, \((P^{}}-)[u,v]\) becomes non-zero if an edge \((u,v)\) is added to \(G\). Therefore, for the edit operations involving edges, we have:

\[[(u,v) E((u),(v)) E^{} ]=(-P^{}})[u,v],\] (3) \[[(u,v) E((u),(v)) E^{} ]=(P^{}}-)[u,v].\] (4)

Similarly, we note that \((_{G}[u]-_{G^{}}[(u)])>0\) if \(u_{G}\) and \((u)_{G^{}}\), which allows us to compute the cost of deleting the node \(u\) from \(G\). Similarly, we use \((_{G^{}}[(u)]-_{G}[u])\) to account for the addition of the node \(u\) to \(G\). Formally, we write:

\[_{G}[u](1-_{G^{}}[(u)]) =(_{G}[u]-_{G^{}}[(u )]),\] (5) \[(1-_{G}[u])_{G^{}}[(u)] =(_{G^{}}[(u)]-_{G}[ u]).\] (6)

Using Eqs. (3)-(6), we rewrite Eq. (2) as:

\[(G,G^{})= _{_{N}}\ }{2}\| (-P^{}})\|_{1,1}+}{2} \|(P^{}}-)\|_{1,1}\] \[+b^{}\|(_{G}-_{G^{}})\|_{1}+b^{}\|( _{G^{}}-_{G})\|_{1}.\] (7)

The first and the second term denote the collective costs of deletion and addition of edges, respectively. The third and the fourth terms present a matricized representation of Eqs. (5)- (6). The above problem can be viewed as a quadratic assignment problem (QAP) on graphs, given that the hard node permutation matrix \(\) has a quadratic involvement in the first two terms. Note that, in general, \((G,G^{})(G^{},G)\). However, the optimal edit paths for these two GED values, encoded by the respective node permutation matrices, are inverses of each other, as formally stated in the following proposition (proven in Appendix B).

**Proposition 1**: _Given a fixed set of values of \(b^{},b^{},a^{},a^{}\), let \(\) be an optimal node permutation matrix corresponding to \((G,G^{})\), computed using Eq. (7). Then, \(^{}=^{}\) is an optimal node permutation corresponding to \((G^{},G)\)._

Connection to different notions of graph matchingThe above expression of GED can be used to represent various notions of graph matching and similarity measures by modifying the edit costs. These include graph isomorphism, subgraph isomorphism, and maximum common edge subgraphdetection. For example, by setting all costs to one, \((G,G^{})=_{}||-^{} ^{}||_{1}+||_{G}-_{G^{}}||_{1}\), which equals zero only when \(G\) and \(G^{}\) are isomorphic. Further discussion on this topic is provided in Appendix B.

### GraphEdX model

Minimizing the objective in Eq. (7) is a challenging problem. In similar problems, recent methods have approximated the hard node permutation matrix \(\) with a soft permutation matrix obtained using Sinkhorn iterations on a neural cost matrix. However, the binary nature of the adjacency matrix and the pad indicator \(\) still impede the flow of gradients during training. To tackle this problem, we make relaxations in two key places within each term in Eq. (7), leading to our proposed GraphEdX model.

1. We replace the binary values in \(_{G},_{G^{}},\) and \(^{}\) with real values from node and node-pair embeddings: \(^{N d}\) and \(^{ D}\). These embeddings are computed using a GNN guided neural module \(_{}\) with parameter \(\). Since the graphs are undirected, \(\) gathers the embeddings of the unique node-pairs, resulting in \(\) rows instead of \(N^{2}\).
2. We substitute the hard node permutation matrix \(\) with a soft alignment matrix, generated using a differentiable alignment planner \(_{}\) with parameter \(\). Here, \(\) is a doubly stochastic matrix, with \([u,u^{}]\) indicating the "score" or "probability" of aligning \(u u^{}\). Additionally, we also compute the corresponding node-pair alignment matrix \(\).

Using these relaxations, we approximate the four edit costs in Eq. (7) with four continuous set distance surrogate functions.

\[&\|(-^{ }^{})\|_{1,1}^{}(,^{}\,|\,),\|(^{}^{ }-)\|_{1,1}^{}(,^{ }\,|\,),\\ &\|(_{G}-_{G^{ }})\|_{1}^{}(,^{} \,|\,),\|(_{G^{}}-_{G})\|_{1}^{}(,^{} \,|\,).\]

This gives us an approximated GED parameterized by \(\) and \(\).

\[_{,}(G,G^{})& =a^{}^{}(,^{}\,|\,)+a^{ }^{}(,^{}\,|\,)\\ &+b^{}^{}(,^{}\,|\,)+ b^{}^{}(,^{}\,|\,).\] (8)

Note that since \(\) and \(^{}\) contain the embeddings of each node-pair only once, there is no need to multiply \(1/2\) in the first two terms, unlike Eq. (7). Next, we propose three types of neural surrogates to approximate each of the four operations.

(1) AlignDiffGiven the node-pair embeddings \(\) and \(^{}\) for the graph pairs \(G\) and \(G^{}\), we apply the soft node-pair alignment \(\) to \(^{}\). We then define the edge edits in terms of asymmetric

Figure 1: **Top:** Example graphs \(G\) and \(G^{}\) are shown with color-coded nodes to indicate alignment corresponding to the optimal edit path transforming \(G\) to \(G^{}\). **Bottom:** GraphEdX’s GED prediction pipeline. \(G\) and \(G^{}\) are independently encoded using \(_{}\), and then padded with zero vectors to equalize sizes, resulting in contextual node representations \(,^{}^{N d}\). For each node-pair, the corresponding embeddings and edge presence information are gathered and fed into \(_{}\) to obtain \(,^{}^{N(N-1)/2 D}\). Simultaneously, \(,^{}\) are fed into \(_{}\) to obtain the soft node alignment \(\) (Eq.(16)) which constructs the node-pair alignment matrix \(^{N(N-1)/2 N(N-1)/2}\) as \([(u,v),(u^{},v^{})]=[u,u^{}][v,v^{} ]+[u,v^{}][v,u^{}]\). Finally, \(,^{},\) are used to approximate node insertion and deletion costs, while \(,^{},\) are used to approximate edge insertion and deletion costs. The four costs are summed to give the final prediction \(_{,}(G,G^{})\) (Eq.(8)).

differences between \(\) and \(}\), which serves as a replacement for the corresponding terms in Eq. (7). We write \(^{}(,}\,|\,)\) and \(^{}(,}\,|\,)\) as:

\[^{}(,}\,|\,)=\|( {R}-})\|_{1,1},^{}(,}\,|\,)=\|(}-) \|_{1,1}.\] (9)

Similarly, for the node edits, we can compute \(^{}(,}\,|\,)\) and \(^{}(,}\,|\,)\) as:

\[^{}(,}\,|\,)=\|( -})\|_{1,1},^{}(,}\,|\,)=\|(}- )\|_{1,1}.\]

**(2) Diffalign** In Eq. (9), we first aligned \(}\) using \(\) and then computed the difference from \(\). Instead, here we first computed the pairwise differences between \(}\) and \(\) for all pairs of node-pairs (\(e,e^{}\)), and then combine these differences with the corresponding alignment scores \([e,e^{}]\). We compute the edge edit surrogates \(^{}(,}\,|\,)\) and \(^{}(,}\,|\,)\) as:

\[^{}(,}\,|\,)=_{e,e^{}}\| ([e,:]-}[e^{},:])\|_ {1}[e,e^{}],\] (10)

\[^{}(,}\,|\,)=_{e,e^{}}\| (}[e^{},:]-[e,:])\|_ {1}[e,e^{}].\] (11)

Here, \(e\) and \(e^{}\) represent node-pairs, which are not necessarily edges. When the node-pair alignment matrix \(\) is a hard permutation, \(^{}\) and \(^{}\) remain the same across AlignDiff and Diffalign (as shown in Appendix B). Similar to Eqs. (10)--(11), we can compute \(^{}(,}\,|\,)=_{u,u^{}}\| ([u,:]-}[u^{},:])\|_{ 1}[u,u^{}]\) and \(^{}(,}\,|\,)=_{u,u^{}}\| (}[u^{},:]-[u,:])\|_{ 1}[u,u^{}]\).

**(3) XOR-Diffalign** As indicated by the combinatorial formulation of GED in Eq. (7), the edit cost of a particular node-pair is non-zero only when an edge is mapped to a non-edge or vice-versa. However, the surrogates for the edge edits in AlignDiff of Diffalign fail to capture this condition because they can assign non-zero costs to the pairs \((e=(u,v),e^{}=(u^{},v^{}))\) even when both \(e\) and \(e^{}\) are either edges or non-edges. To address this, we explicitly discard such pairs from the surrogates defined in Eqs. (10)-(11). This is ensured by applying a XOR operator \(,\) between the corresponding entries in the adjacency matrices, _i.e._, \([u,v]\) and \(}[u^{},v^{}]\), and then multiplying this result with the underlying term. Hence, we write:

\[^{}(,}\,|\,)=_{e=(u,v)\\ e^{}=(u^{},v^{})}J[u,v],}[u^{},v^{}]\|([e,:]- }[e^{},:])\|_{1}[e,e^{}],\] (12)

\[^{}(,}\,|\,)=_{e=(u,v)\\ e^{}=(u^{},v^{})}J[u,v],}[u^{},v^{}]\|(}[e^{},:]-[e,:])\|_{1}[e,e^{}].\] (13)

Similarly, the cost contribution for node operations arises from mapping a padded node to a non-padded node or vice versa. We account for this by multiplying \(J(_{G}[u],_{G^{}}[u^{}])\) with each term of \(^{}(,}\,|\,)\) and \(^{}(,}\,|\,)\) computed using Diffalign. Hence, we compute \(^{}(,}\,|\,)=_{u,u^{}}J(_{G}[u],_{G^{}}[u^{}])\|([u,:]-}[u^{},:])\|_{1}[u,u^{}]\) and \(^{}(,}\,|\,)=_{u,u^{}}J(_{G}[u],_{G^{}}[u^{}])\|([u^{},:]-[u,:])\|_{1}[u,u^{}]\).

**Comparison between AlignDiff, Diffalign and XOR-Diffalign** AlignDiff and Diffalign become equivalent when \(\) is a hard permutation. However, when \(\) is doubly stochastic, the above three surrogates, AlignDiff, Diffalign and XOR-Diffalign, are not equivalent. As we move from AlignDiff to Diffalign to XOR-Diffalign, we increasingly align the design to the inherent inductive biases of GED, thereby achieving a better representation of its cost structure.

Suppose we are computing the GED between two isomorphic graphs, \(G\) and \(G^{}\), with uniform costs for all edit operations. In this scenario, we ideally expect a neural network to consistently output a zero cost. Now consider a proposed soft alignment \(\) which is close to the optimal alignment. Under the AlignDiff design, the aggregated value \(_{e^{}}[e,e^{}]}[e^{},:]\) -- where \(e\) and \(e^{}\) represent two edges matched in the optimal alignment -- can accumulate over the large number of \(N(N-1)/2\) node-pairs. This aggregation leads to high values of \(||[e,:]-}[e^{},:]||_{1}\), implying that AlignDiff captures an aggregate measure of the cost incurred by spurious alignments, but cannot disentangle the effect of individual misalignments, making it difficult for AlignDiff to learn the optimal alignment.

In contrast, the Diffalign approach, which relies on pairwise differences between embeddings to explicitly guide \(\) towards the optimal alignment, significantly ameliorates this issue. For example, in the aforementioned setting of GED with uniform costs, the cost associated with each pairing\((e,e^{})\) is explicitly encoded using \(\|[e,:]-}[e^{},:]\|_{1}\), and is explicitly set to zero for pairs that are correctly aligned. Moreover, this representation allows DiffAlign to isolate the cost incurred by each misalignment, making it easier to train the model to reduce the cost of these spurious matches to zero.

However, DiffAlign does not explicitly set edge-to-edge and non-edge-to-non-edge mapping costs to zero, potentially leading to inaccurate GED estimates. XOR-DiffAlign addresses these concerns by applying a XOR of the adjacency matrices to the cost matrix, ensuring that non-zero cost is computed only when mapping an edge to a non-edge or vice versa. This resolves the issues in both AlignDiff and DiffAlign by focusing on mismatches between edges and non-edges, while disregarding redundant alignments that do not contribute to the GED.

**Amenability to indexing and approximate nearest neighbor (ANN) search.** All of the aforementioned distance surrogates are based on a late interaction paradigm, where the embeddings of \(G\) and \(G^{}\) are computed independently of each other before computing the distances \(\). This is particularly useful in the context of graph retrieval, as it allows for the corpus graph embeddings to be indexed a-priori, thereby enabling efficient retrieval of relevant graphs for new queries.

When the edit costs are uniform, our predicted GED (8) becomes symmetric with respect to \(G\) and \(G^{}\). In such cases, DiffAlign and AlignDiff yield a structure similar to the Wasserstein distance induced by \(L_{1}\) norm. This allows us to leverage ANN techniques like Quadtree or Flowtree . However, while the presence of the XOR operator \(J\) within each term in Eq. (12) - (13) of XOR-DiffAlign enhances the interaction between \(G\) and \(G^{}\), this same feature prevents XOR-DiffAlign from being cast to an ANN-amenable setup, unlike DiffAlign and AlignDiff.

### Network architecture of Embed\({}_{}\) and PermNet\({}_{}\)

In this section, we present the network architectures of the two components of GraphEdX, _viz._, Embed\({}_{}\) and PermNet\({}_{}\), as introduced in items (1) and (2) in Section 4.2. Notably, in our proposed graph representation, non-edges and edges alike are embedded as non-zero vectors. In other words, all node-pairs are endowed with non-trivial embeddings. We then explain the design approach for edge-consistent node alignment.

**Neural architecture of Embed\({}_{}\)** Embed\({}_{}\) consists of a message passing neural network \(_{}\) and a decoupled neural module \(_{}\). Given the graphs \(G,G^{}\), \(_{}\) with \(K\) propagation layers is used to iteratively compute the node embeddings \(\{_{K}(u)^{d}\,|\,u V\}\) and \(\{^{}_{K}(u)^{d}\,|\,u V^{}\}\), then collect them into \(\) and \(}\) after padding, _i.e._,

\[:=\{_{K}(u)\,|\,u[N]\}=_{}(G), }:=\{^{}_{K}(u^{})\,|\,u^{} [N]\}=_{}(G^{}).\] (14)

The optimal alignment \(\) is highly sensitive to the global structure of the graph pairs, _i.e._, \([e,e^{}]\) can significantly change when we perturb \(G\) or \(G^{}\) in regimes distant from \(e\) or \(e^{}\). Conventional representations mitigate this sensitivity while training models, by setting non-edges to zero, rendering them invariant to structural changes. To address this limitation, we utilize more expressive graph representations, where non-edges are also embedded using trainable non-zero vectors. This approach allows information to be captured from the structure around the nodes through both edges and non-edges, thereby enhancing the representational capacity of the embedding network. For each node-pair \(e=(u,v) G\) (and equivalently \((v,u)\)), and \(e^{}=(u^{},v^{}) G^{}\), the embeddings of the corresponding nodes and their connectivity status are concatenated, and then passed through an MLP to obtain the embedding vectors \((e),}(e^{})^{D}\). For \(e=(u,v) G\), we compute \((e)\) as:

\[(e)=_{}(_{K}(u)\,||\,_{K}(v)\,||\,[u, v])+_{}(_{K}(v)\,||\,_{K}(u)\,||\,[v,u]).\] (15)

We can compute \(}(e)\) in similar manner. The property \(((u,v))=((v,u))\) reflects the undirected property of graph. Finally, the vectors \((e)\) and \(}(e^{})\) are stacked into matrices \(\) and \(}\), both with dimensions \(^{ D}\). We would like to highlight that \(((u,v))\) or \(}((u^{},v^{}))\) are computed only once for all node-pairs, after the MPNN completes its final \(K\)th layer of execution. The message passing in the MPNN occurs only over edges. Therefore, this approach does not significantly increase the time complexity.

**Neural architecture of PermNet\({}_{}\)** The network PermNet\({}_{}\) provides \(\) as a soft node alignment matrix by taking the node embeddings as input, _i.e._, \(=_{}(,})\). PermNet\({}_{}\) is implemented in two steps. In the first step, we apply a neural network \(c_{}\) on both \(_{K}\) and \(^{}_{K}\), and then compute the normed difference between their outputs to construct the matrix \(\), where \([u,u^{}]=\|c_{}(_{K}(u))-c_{}( ^{}_{K}(u^{}))\|_{1}\). Next, we apply iterative Sinkhorn normalizations  on \((-/)\), to obtain a soft node alignment \(\). Therefore,

\[=([(-\|c_{}(_ {K}(u))-c_{}(_{K}^{}(u^{}))\|_{1} /)]_{(u,u^{})[N][N]}).\] (16)

Here, \(\) is a temperature hyperparameter. In a general cost setting, \(\) is typically asymmetric, so it may be desirable for \([u,u^{}]\) to be asymmetric with respect to \(\) and \(^{}\). However, as noted in Proposition 1, when we compute \((G^{},G)\), the alignment matrix \(^{}=_{}(^{},)\) should satisfy the condition that \(^{}=^{}\), where \(\) is computed from Eq. (16). The current form of \(\) supports this condition, whereas an asymmetric form might not, as shown in Appendix B.

We construct \(^{}^{}\) as follows. Each pair of nodes \((u,v)\) in \(G\) and \((u^{},v^{})\) in \(G^{}\) can be mapped in two ways, regardless of whether they are edges or non-edges: (1) node \(u u^{}\) and \(v v^{}\) which is denoted by \([u,u^{}][v,v^{}]\); (2) node \(u v^{}\) and \(v u^{}\), which is denoted by \([u,v^{}][v,u^{}]\) Combining these two scenarios, we compute the node-pair alignment matrix \(\) as: \([(u,v),(u^{},v^{})]=[u,u^{}][v,v^{}] +[u,v^{}][v,u^{}]\). This explicit formulation of \(\) from \(\) ensures mutually consistent permutation across nodes and node-pairs.

## 5 Experiments

We conduct extensive experiments on GraphEdX to showcase the effectiveness of our method across several real-world datasets, under both uniform and non-uniform cost settings for GED. Addiitional experimental results can be found in Appendix D.

### Setup

**Datasets** We experiment with seven real-world datasets: Mutagenicity (Mutag) , Ogbg-Code2 (Code2) , Ogbg-Molhiv (Molhiv) , Ogbg-Molpcba (Molpcba) , AIDS , Linux  and Yeast . For each dataset's training, test and validation sets \(_{}\), we generate \(_{}]}{2}+|_{}|\) graph pairs, considering combinations between every two graphs, including self-pairing. We calculate the exact ground truth GED using the F2 solver , implemented within GEDLIB . For GED with uniform cost setting, we set the cost values to \(b^{}=b^{}=a^{}=a^{}=1\). For GED with non-uniform cost setting, we use \(b^{}=3,b^{}=1,a^{}=2,a^{}=1\). Further details on dataset generation and statistics are presented in Appendix C. In the main paper, we present results for the first five datasets under both uniform and non-uniform cost settings for GED. Additional experiments for Linux and Yeast, as well as GED with node label substitutions, are presented in Appendix D.

**Baselines** We compare our approach with nine state-of-the-art methods. These include two variants of GMN : (1) GMN-Match and (2) GMN-Embed; (3) ISONET , (4) GREED , (5) ERIC , (6) SimGNN , (7) H2MN , (8) GraphSim  and (9) EGSC . To compute the GED, GMN-Match, GMN-Embed, and GREED use the Euclidean distance between the vector representation of two graphs. ISONET uses an asymmetric distance specifically tailored to subgraph isomorphism. H2MN is an early interaction network that utilizes higher-order node similarity through hypergraphs. ERIC, SimGNN, and EGSC leverage neural networks to calculate the distance between two graphs. Furthermore, the last three methods predict a score based on the normalized GED in the form of \((-2(G,G^{})/(|V|+|V^{}|))\). Notably, none of these baseline approaches have been designed to incorporate non-uniform edit costs into their models. To address this limitation, when working with GED under non-uniform cost setting, we include the edit costs as initial features in the graphs for all baseline models. In Appendix D.3, we compare the performance of baselines without cost features.

**Evaluation** Given a dataset \(=\{(G_{i},G_{i}^{},(G_{i},G_{i}^{ }))\}_{i[n]}\), we divide it into training, validation and test folds with a split ratio of 60:20:20. We train the models using the Mean Squared Error (MSE) between the predicted GED and the ground truth GED as the loss. For model evaluation, we calculate the Mean Squared Error (MSE) between the actual and predicted GED on the test set. For ERIC, SimGNN and EGSC, we rescale the predicted score to obtain the true (unscaled) GED as \((G,G^{})=-(|V|+|V|^{})(s)/2\). In Appendix D, we also report Kendall's Tau (KTau) to evaluate the rank correlation across different experiments.

### Results

**Selection of \(^{}(,^{}\,|\,)\) and \(^{}(,^{}\,|\,)\)** We start by comparing the performance of the nine different combinations (three for edge edits, and three for node edits) of our neural distance sur rogates from the cartesian space of Edge-{AlignDiff, DiffAlign, XOR-DiffAlign} \(\) Node-{AlignDiff, DiffAlign, XOR-DiffAlign}. Table 2 summarizes the results. We make the following observations. (1) The best combinations share the XOR-DiffAlign on the edge edit formulation, because, XOR-DiffAlign offers more inductive bias, by zeroing the edit cost of aligning an edge to edge and a non-edge to non-edge, as we discussed in Section 4.2. Consequently, one can limit the cartesian space to only three surrogates for node edits, while using XOR-DiffAlign as the fixed surrogate for edge edits. (2) There is no clear winner between DiffAlign and AlignDiff. GraphEdX is chosen from the model which has the lowest validation error, and the numbers in Table 2 are on the test set. Hence, in datasets such as AIDS under uniform cost, or Molhiv under non-uniform cost, the model chosen for GraphEdX doesn't have the best test performance.

Comparison with baselinesWe compare the performance of GraphEdX against all state-of-the-art baselines for GED with both uniform and non-uniform costs. Table 3 summarizes the results. We make the following observations. (1) GraphEdX outperforms all the baselines by a significant margin. For GED with uniform costs, this margin often goes as high as 15%. This advantage becomes even more pronounced for GED with non-uniform costs, where our method outperforms the baselines by a margin as high as 30%, as seen in Code2. (2) There is no clear second-best method. Among the baselines, EGSC and ERIC outperforms the others in two out of five datasets for both uniform and non-uniform cost settings. Also, EGSC demonstrates competitive performance in AIDS.

Impact of cost-guided GEDAmong the baselines, GMN-Match, GMN-Embed and GREED compute GED using the euclidean distance between the graph embeddings, _i.e._, \((G,G^{})=\|_{G}-_{G^{}}\|_{2}\), whereas we compute it by summing the set distance surrogates between the node and edge embedding sets. To understand the impact of our cost guided distance, we adapt it to the graph-level embeddings used by the above three baselines as follows: \((G,G^{})=+a^{}}{2}\|(_{G}-_{G^{}})\|_{1}++a^ {}}{2}\|(_{G^{}}-_{G}) \|_{1}\). Table 4 summarizes the results in

   &  &  &  \\   & & Mitag & Code2 & Molhiv & Molpecha & AIDS & Mitag & Code2 & Molhiv & Molpecha & AIDS \\  DiffAlign & DiffAlign & 0.579 & 0.740 & 0.820 & 0.778 & 0.603 & 1.205 & 2.451 & 1.855 & 1.825 & 1.417 \\ DiffAlign & AlignDiff & 0.557 & 0.742 & 0.806 & 0.779 & 0.597 & 1.211 & 1.166 & 1.887 & 1.811 & 1.319 \\ DiffAlign & XOR & 0.538 & 0.719 & 0.794 & 0.777 & 0.580 & 1.146 & 1.896 & 1.802 & 1.822 & 1.381 \\ AlignDiff & Diffusion & 0.537 & 0.513 & 0.815 & 0.773 & 0.606 & 1.155 & 1.689 & 1.874 & 1.758 & 1.391 \\ AllGDiff & AlignDiff & 0.578 & 0.929 & 0.833 & 0.773 & 0.593 & 1.338 & 1.488 & 1.903 & 1.859 & 1.326 \\ AlignDiff & XOR & 0.533 & 0.826 & 0.812 & 0.780 & 0.575 & 1.196 & 1.741 & 1.870 & 1.815 & 1.374 \\ XOR & AlignDiff & **0.492** & **0.429** & 0.788 & 0.766 & 0.555 & 1.134 & **1.478** & 1.872 & 1.742 & **1.252** \\ XOR & DiffAlign & 0.510 & 0.634 & **0.781** & 0.765 & 0.574 & 1.148 & 1.489 & 1.804 & 1.757 & 1.340 \\ XOR & XOR & 0.530 & 1.588 & 0.807 & 0.764 & **0.564** & 1.195 & 2.507 & 1.855 & 1.677 & 1.319 \\   & 0.492 & 0.429 & 0.781 & 0.764 & 0.565 & 1.134 & 1.478 & 1.804 & 1.677 & 1.252 \\  

Table 2: Prediction error measured in terms of MSE of the nine combinations of our neural set distance surrogate across five datasets on test set, for GED with uniform costs and non-uniform costs. For GED with uniform (non-uniform) costs we have \(b^{}=b^{}=a^{}=a^{}=1\) (\(b^{}=3,b^{}=1,a^{}=2,a^{}=1\).) The GraphEdX model was selected based on the lowest MSE on the validation set, and we report the results of the MSE on the test set. Green ( yellow) numbers report the best (second best) performers.

   &  &  \\   & Mutag & Code2 & Molhiv & Molpecha & AIDS & Mutag & Code2 & Molhiv & Molpecha & AIDS \\  GMN-Match  & 0.797 & 1.677 & 1.318 & 1.073 & 0.821 & 69.210 & 13.472 & 76.923 & 23.985 & 31.522 \\ GMN-Embed  & 1.032 & 1.358 & 1.859 & 1.951 & 1.044 & 72.945 & 13.425 & 78.254 & 28.437 & 33.221 \\ ISONNET  & 1.187 & 0.879 & 1.354 & 1.106 & 1.640 & 3.369 & 3.025 & 3.451 & 2.781 & 5.513 \\ GREED  & 1.398 & 1.869 & 1.708 & 1.550 & 1.004 & 68.732 & 11.095 & 78.300 & 26.057 & 34.354 \\ ERIC  & 0.719 & 1.363 & 1.165 & 0.862 & 0.731 & 1.981 & 1.267 & 3.377 & 2.057 & 1.581 \\ SimMN  & 1.471 & 2.667 & 1.609 & 1.456 & 1.455 & 4.474 & 5.212 & 4.145 & 3.465 & 4.316 \\ H2MN  & 1.278 & 7.240 & 1.521 & 1.402 & 1.114 & 3.413 & 9.435 & 3.782 & 3.396 & 3.105 \\ GraphSim  & 2.005 & 3.139 & 2.577 & 1.656 & 1.936 & 5.730 & 7.405 & 6.643 & 3.928 & 5.266 \\ EGSC  & 0.765 & 4.165 & 1.138 & 0.938 & 0.627 & 1.758 & 3.957 & 2.371 & 2.133 & 1.693 \\  GraphEdX & 0.492 & 0.429 & 0.781 & 0.764 & 0.565 & 1.134 & 1.478 & 1.804 & 1.677 & 1.252 \\  

Table 3: Prediction error measured in terms of MSE of GraphEdX and all the state-of-the-art baselines across five datasets on test set, for GED with uniform costs and non-uniform costs. For GED with uniform (non-uniform) costs we have \(b^{}=b^{}=a^{}=a^{}=1\) (\(b^{}=3,b^{}=1,a^{}=2,a^{}=1\).) GraphEdX represents the best model based on the validation set from the cartesian space of Edge-{AlignDiff, DiffAlign, XOR-DiffAlign} \(\) Node-{AlignDiff, DiffAlign}. Green ( yellow) numbers report the best (second best) performers.

terms of MSE, which shows that (1) our set-divergence-based cost guided distance reduces the MSE by a significant margin in most cases (2) the margin of improvement is more prominent with GED involving non-uniform costs, where the modeling of specific cost values is crucial (3) GraphEdX outperforms the baselines even after changing their default distance to our cost guided distance.

Performance for GED under node substitution costThe scoring function in Eq. 8 can also be extended to incorporate node label substitution cost, which has been described in Appendix B. Here, we compare the performance of our model with the baselines in terms of MSE where we include node substitution cost \(b^{}\), with cost setting as \(b^{}=b^{}=b^{}=a^{}=a^{}=1\). In Table 5, we report the results across 5 datasets equipped with node labels, passed as one-hot encoded node features. We observe that (1) our model outperforms all other baselines across all datasets by significant margin; (2) there is no clear second winner but ERIC, EGSC and ISONET performs better than the others.

Benefits of using all node-pairs representation(i) Edge-only (\(\)): where we only consider the edges that are present, resulting in \(\) being an edge-alignment matrix, and \(,}^{(|E|,|E^{}|) D}\) (ii) Edge-only (\(\)): In this variant, the embeddings of the non-edges in \(,}^{N(N-1)/2 D}\) are explicitly set to zero. in terms of MSE, which show that (1) both these sparse representations perform significantly worse compared to our method using non-trivial representations for both edges and non-edges, and (2) Edge-only (\(\)) performs better than Edge-only (\(\)). This underscores the importance of explicitly modeling trainable non-edge embeddings to capture the sensitivity of GED to global graph structure.

## 6 Conclusion

Our work introduces a novel neural model for computing GED that explicitly incorporates general costs of edit operations. By leveraging graph representations that recognize both edges and non-edges, together with the design of suitable set distance surrogates, we achieve a more robust neural surrogate for GED. Our experiments demonstrate that this approach outperforms state-of-the-art methods, especially in settings with general edit costs, providing a flexible and effective solution for a range of applications. Future work could focus on extending the GED formulation to richly-attributed graphs by modeling the structure of edit operations and the similarity of all node-pair features.

LimitationsOur neural model for GED showcases significant improvements in accuracy and flexibility for modeling edit costs. However, there are some limitations to consider. (1) While computing graph representations over \(\) node-pairs does not require additional parameters due to parameter-sharing, it does demand significant memory resources. This could pose challenges, especially with larger-sized graphs. (2) The assumption of fixed edit costs across all graph pairs within a dataset might not reflect real-world scenarios where costs vary based on domain-specific factors and subjective human relevance judgements. This calls for more specialized approaches to accurately model the impact of each edit operation, which may differ across node pairs.

AcknowledgementsIndradyumna acknowledges Qualcomm Innovation Fellowship, Abir and Soumen acknowledge grants from Amazon, Google, IBM and SERB.

   & Mutag & Code2 & Molhiv \\  Edge-only (\(\)) & 0.566 & 0.683 & 0.858 \\ Edge-only (\(\)) & 0.596 & 0.760 & 0.862 \\  GraphEdX & **0.492** & **0.429** & 0.781 \\  

Table 6: Comparison of variants of edge representation under uniform cost setting. \(\) (yellow) numbers report the best (second best) performers.

   &  &  \\   & Mutag & Code2 & Molhiv & Mutag & Code2 & Molhiv \\  GMN-Match & 0.797 & 1.677 & 1.318 & 6920 & 13.472 & 76.923 \\ GMN-Match & **0.654** & **0.960** & **1.008** & **1.592** & **2.906** & **2.162** \\  GMN-Embed & 1.032 & 1.358 & 1.592 & 2495 & 13.425 & 78.254 \\ GMN-Embed * & **1.011** & **1.179** & **1.409** & **2.368** & **3.272** & **3.413** \\  GREED * & **1.398** & 1.690 & 1.708 & 68.732 & 11.095 & 78.300 \\ GREED * & 2.133 & **8.50** & **1.644** & **2.456** & **5.429** & **3.827** \\  \(\) & **0.492** & 0.429 & 0.781 & 1.134 & 1.478 & 1.804 \\  

Table 4: Impact of cost guided distance in terms of MSE; * represents the variant of the baseline with cost-guided distance. \(\) (**bold**) shows the best among all methods (only baselines).

   & Mutag & Code2 & Molhiv & Molge2 & AIDS \\  GMN-Match & 1.057 & 5.224 & 1.388 & 1.432 & 0.868 \\ GMN-Embed & 2.159 & 4.070 & 3.523 & 4.657 & 1.818 \\ ISONET & 0.876 & 1.129 & 1.617 & 1.332 & 1.142 \\ GREED & 2.876 & 4.983 & 2.923 & 3.902 & 2.175 \\ ERIC & 0.886 & 6.323 & 1.537 & 1.278 & 1.602 \\ SimGNN & 1.160 & 5.909 & 1.888 & 2.172 & 1.418 \\ H2MN & 1.277 & 6.783 & 1.891 & 1.666 & 1.290 \\ GraphSim & 1.043 & 4.708 & 1.817 & 1.748 & 1.561 \\ EGSC & 0.776 & 8.742 & 1.723 & 1.426 & 1.270 \\  GraphEdX & 0.441 & 0.820 & 0.792 & 0.846 & 0.538 \\  

Table 5: MSE for different methods with unit node substitution cost in uniform cost setting. \(\) (yellow) show (second) best method.