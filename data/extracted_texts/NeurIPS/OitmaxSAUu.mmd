# Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars

Kaiyue Wen

Tsinghua University

wenky20@mails.tsinghua.edu.cn

&Yuchen Li

Carnegie Mellon University

yuchenl4@cs.cmu.edu

&Bingbin Liu

Carnegie Mellon University

bingbinl@cs.cmu.edu

&Andrej Risteski

Carnegie Mellon University

aristek@andrew.cmu.edu

###### Abstract

Transformer interpretability aims to understand the algorithm implemented by a learned Transformer by examining various aspects of the model, such as the weight matrices or the attention patterns. In this work, through a combination of theoretical results and carefully controlled experiments on synthetic data, we take a critical view of methods that exclusively focus on individual parts of the model, rather than consider the network as a whole. We consider a simple synthetic setup of learning a (bounded) Dyck language. Theoretically, we show that the set of models that (exactly or approximately) solve this task satisfy a structural characterization derived from ideas in formal languages (the pumping lemma). We use this characterization to show that the set of optima is qualitatively rich; in particular, the attention pattern of a single layer can be "nearly randomized", while preserving the functionality of the network. We also show via extensive experiments that these constructions are not merely a theoretical artifact: even with severe constraints to the architecture of the model, vastly different solutions can be reached via standard training. Thus, interpretability claims based on inspecting individual heads or weight matrices in the Transformer can be misleading.

## 1 Introduction

Transformer-based models power many leading approaches to natural language processing. With their growing deployment in various applications, it is increasingly essential to understand the inner working of these models. Towards addressing this, there have been great advancement in the field of interpretability presenting various types of evidence (Clark et al., 2019; Vig and Belinkov, 2019; Wiegreffe and Pinter, 2019; Nanda et al., 2023; Wang et al., 2023), some of which, however, can be misleading despite being highly intuitive (Jain and Wallace, 2019; Serrano and Smith, 2019; Rogers et al., 2020; Grimsley et al., 2020; Brunner et al., 2020; Meister et al., 2021).

In this work, we aim to understand the theoretical limitation of certain interpretability methods by characterizing the set of viable solutions. We focus on myopic interpretability methods, i.e. methods based on examining individual components only. We adopt a particular toy setup in which Transformers are trained to generate _Dyck grammars_, a classic type of formal language grammar consisting of balanced parentheses of multiple types. Dyck is a useful sandbox, as it captures properties like long-range dependency and hierarchical tree-like structure that commonly appear in natural and programming language syntax, and has been an object of interest in many theoretical studies (Hahn, 2020; Yao et al., 2021; Liu et al., 2022b, 2023). Dyck is canonically parsed usinga stack-like data structure. Such stack-like patterns (Figure 1) have been observed in the attention heads (Ebrahimi et al., 2020), which was later bolstered by mathematical analysis in Yao et al. (2021).

From a representational perspective and via explicit constructions of Transformer weights, recent work (Liu et al., 2023; Li et al., 2023) show that Transformers are sufficiently expressive to admit very different solutions that perform equally well on the training distribution. Thus, the following questions naturally arise:

* Do Transformer solutions found empirically match the theoretical constructions given in these representational results (Figure 1)? In particular, are interpretable stack-like pattern in Ebrahimi et al. (2020) the norm or the exception in practice?
* More broadly, can we understand in a principled manner the fundamental obstructions to reliably "reverse engineering" the algorithm implemented by a Transformer by looking at individual attention patterns?
* Among models that perform (near-)optimally on the training distribution, even if we cannot fully reverse engineer the algorithm implemented by the learned solutions, can we identify properties that characterize performance beyond the training distribution?

Our contributions.We first prove several theoretical results to provide evidence for why individual components (e.g. attention patterns or weights) of a Transformer should not be expected to be interpretable. In particular, we prove:

* A **perfect balance** condition (Theorem 1) on the attention pattern that is sufficient and necessary for 2-layer Transformers with a _minimal first layer_ (Assumption 1) to predict optimally on Dyck of _any_ length. We then show that this condition permits abundant _non-stack-like_ attention patterns that do not necessarily reflect any structure of the task, including _uniform_ attentions (Corollary 1).
* An **approximate balance** condition (Theorem 3), the _near-optimal_ counterpart of the condition above, for predicting on _bounded_-length Dyck. Likewise, non-stack-like attention patterns exist.
* **Indistinguishability from a single component** (Theorem 2), proved via a _Lottery Ticket Hypothesis_ style argument that any Transformer can be approximated by pruning a larger random Transformer, implying that interpretations based exclusively on local components may be unreliable.

We further accompany these theoretical findings with an extensive set of empirical investigations.

_Is standard training biased towards interpretable solutions?_ While both stack-like and non-stack like patterns can process Dyck theoretically, the inductive biases of the architecture or the optimization process may prefer one solution over the other in practice. In Section 4.1, based on a wide range of Dyck distributions and model architecture ablations, we find that Transformers that generalize near-perfectly in-distribution (and reasonably well out-of-distribution) do _not_ typically produce stack-like attention patterns, showing that the results reported in prior work (Ebrahimi et al., 2020) should not be expected from standard training.

_Do non-interpretable solutions perform well in practice?_ Our theory predicts that balanced (or even uniform) attentions suffice for good in- and out-of-distribution generalization. In Section 4.2, we

Figure 1: **Second-layer attention patterns of two-layer Transformers on Dyck: typical attention patterns do _not_ exactly match the intuitively interpretable stack-like pattern prescribed in Ebrahimi et al. (2020); Yao et al. (2021). The blue boxes indicate the locations of the last unmatched open brackets, as they would appear in a stack-like pattern. All models reach \( 97\%\) accuracy (defined in Section 4.1). In the heatmap, darker color indicates larger value.**

empirically verify that with standard training, the extent to which attentions are balanced is positively correlated with generalization performance. Moreover, we can guide Transformers to learn more balanced attention by regularizing for the balance condition, leading to better length generalization.

### Related Work

There has been a flourishing line of work on interpretability in natural language processing. Multiple "probing" tasks have been designed to extract syntactic or semantic information from the learned representations (Raganato and Tiedemann, 2018; Liu et al., 2019; Hewitt and Manning, 2019; Clark et al., 2019). However, the effectiveness of probing often intricately depend on the architecture choices and task design, and sometimes may even result in misleading conclusions (Jain and Wallace, 2019; Serrano and Smith, 2019; Rogers et al., 2020; Brunner et al., 2020; Prasanna et al., 2020; Meister et al., 2021). While these challenges do not completely invalidate existing approaches (Wiegreffe and Pinter, 2019), it does highlight the need for more rigorous understanding of interpretability.

Towards this, we choose to focus on the synthetic setup of Dyck whose solution space is easier to characterize than natural languages, allowing us to identify a set of feasible solutions. While similar representational results have been studied in prior work (Yao et al., 2021; Liu et al., 2023; Zhao et al., 2023), our work emphasizes that theoretical constructions do not resemble the solutions found in practice. Moreover, the multiplicity of valid constructions suggest that understanding Transformer solutions require analyzing the optimization process, which a number of prior work has made progress on (Jelassi et al., 2022; Li et al., 2023; Deng et al., 2023).

Finally, it is worth noting that the challenges highlighted in our work do not contradict the line of prior work that aim to improve _mechanistic interpretability_ into a trained model or the training process (Elhage et al., 2021; Olsson et al., 2022; Nanda et al., 2023; Chughtai et al., 2023; Li et al., 2023), which aim to develop circuit-level understanding of a particular model or the training process.

We defer discussion on additional related work to Appendix A.

## 2 Problem Setup

Dyck languagesA Dyck language (Schutzenberger, 1963) is generated by a context-free grammar, where the valid strings consist of balanced brackets of different types (for example, "\([()]\)" is valid but "\([()]\)" is not). Dyck\({}_{k}\) denote the Dyck language defined on \(k\) types of brackets. The alphabet of Dyck\({}_{k}\) is denoted as \([2k]\{1,2,,2k\}\), where for each type \(t[k]\), tokens \(2t-1\) and \(2t\) are a pair of corresponding open and closed brackets. Dyck languages can be recognized by a push-down automaton. For a string \(w\) and \(i j_{+}\), we use \(w_{i:j}\) to denote the substring of \(w\) between position \(i\) and position \(j\) (both ends included). For a valid prefix \(w_{1:i}\), the _grammar depth_ of \(w_{1:i}\) is defined as the depth of the stack after processing \(w_{1:i}\):

\[(w_{1:i})=\#w_{1:i}-\#w_{1:i}.\]

We overload \((w_{1:i})\) to also denote the grammar depth of the bracket at position \(i\). For example, in each pair of matching brackets, the closing bracket is one depth smaller than the open bracket. We will use \(_{i,d}\) to denote a token of type \(i[2k]\) placed at grammar depth \(d\).

We consider _bounded-depth_ Dyck languages following Yao et al. (2021). Specifically, Dyck\({}_{k,D}\) is a subset of Dyck\({}_{k}\) such that the depth of any prefix of a word is bounded by \(D\),

\[_{k,D}:=\{w_{1:n}_{k}_{i[n]}\ (w_{1:i}) D\}.\] (1)

While a bounded grammar depth might seem restrictive, it suffices to capture many practical settings. For example, the level of recursion occurring in natural languages is typically bounded by a small constant (Karlsson, 2007; Jin et al., 2018). We further define the _length-\(N\) prefix set_ of Dyck\({}_{k,D}\) as

\[_{k,D,N}=\{w_{1:N} n N,w_{N+1:n}[2k]^{n-N},s.t. \ w_{1:n}_{k,D}\}.\] (2)

Our theoretical setup uses the following data distribution \(_{q,k,D,N}\):

**Definition 1** (Dyck distribution).: _The distribution \(_{q,k,D,N}\), specified by \(q(0,1)\), is defined over Dyck\({}_{k,D,N}\) such that \( w_{1:N}_{k,D,N}\),_

\[(w_{1:N})(q/k)^{\#\{i\,|w_{i}(w_{1:i})>1\}}(1-q)^{\#\{i\,|w_{i}(w_{1:i})<D-1\}}.\] (3)That is, \(q(0,1)\) denote the probability of seeing an open bracket at the next position, except for two corner cases: 1) the next bracket has to be open if the current grammar depth is 0 (1 after seeing the open bracket); 2) the next bracket has to be closed if the current grammar depth is \(D\).

Training Objectives.Given a model \(f_{}\) parameterized by \(\), we train with a _next-token prediction_ language modeling objective on a given \(_{q,k,D,N}\). Precisely, given a loss function \(l(,)\), \(f_{}\) is trained to minimize the loss function \(_{}(;_{q,k,D,N})\) with

\[(;_{q,k,D,N})=_{w_{1:N}_ {q,k,D,N}}[_{i=1}^{N}l(f_{}(w_{1:i-1}),z(w_{i}))]\] (4)

in which \(z(w_{i})\{0,1\}^{2k}\) denotes the one-hot embedding of token \(w_{i}\). We will omit the distribution \(_{q,k,D,N}\) when it is clear from the context. We will also consider a \(_{2}\)-regularized version \(^{}()=()+^{2}}{2}\) with parameter \(>0\).

For our theory, we will consider the mean squared error as the loss function: 1

\[l:=l_{sq}(x,z_{i})=\|x-z_{i}\|_{2}^{2}.\] (5)

In our experiments, we apply the cross entropy loss following common practice.

Transformer Architecture.We consider a general formulation of Transformer in this work: the \(l\)-th layer is parameterized by \(^{(l)}:=\{W_{Q}^{(l)},W_{K}^{(l)},W_{V}^{(l)},(^{ (l)})\}\), where \(W_{K}^{(l)},W_{Q}^{(l)}^{m_{a} m}\), and \(W_{V}^{(l)}^{m m}\) are the key, query, and value matrices of the attention module; \((^{(l)})\) are parameters of a feed-forward network \(^{(l)}\), consisting of fully connected layers, (optionally) LayerNorms and residual links. Given \(X^{m N}\), the matrix of \(m\)-dimensional features on a length-\(N\) sequence, the \(l\)-th layer of a Transformer computes the function

\[f_{l}(X;^{(l)})= ^{(l)}W_{V}^{(l)}X+(W_{K}^{(l)}X)^{}(W_{Q}^{(l)}X)}_{ }+X,\] (6)

where \(\) is the column-wise softmax operation defined as \((A_{i,j}=)}{_{k=1}^{N}(A_{k,j})}\), \(\) is the causal mask matrix defined as \(_{i,j}=- 1[i>j]\) where \(\) denotes infinity. We call \((+(W_{K}^{(l)}X)^{}(W_{Q}^{(l)}X))\) the _Attention Pattern_ of the Transformer layer \(l\). \(\) represents column-wise LayerNorm operation, whose \(j_{th}\) output column is defined as

\[_{C_{LN}}(A)_{:,j}=_{}A_{:,j}}{\{\| _{}A_{:,j}\|_{2},C_{LN}\}},_{}=_{ m}-^{}.\] (7)

Here \(_{}\) denotes the projection orthogonal to the \(^{}\) subspace 2 and \(C_{LN}\) is called the normalizing constant for LayerNorm.

We will further define the _attention output_ at the \(l\)-th layer as

\[a_{l}(X;^{(l)})= W_{V}^{(l)}X+(W_{K}^{(l)}X)^{}(W_{Q} ^{(l)}X).\] (8)

When \(C_{LN}=0\), we will also consider the _unnormalized attention output_ as

\[_{l}(X;^{(l)})= W_{V}^{(l)}X+(W_{K}^{(l)}X)^{}(W_{Q} ^{(l)}X).\] (9)

where \((A)_{i,j}=(A_{i,j})\) and it holds by definition that \(_{0}(_{l}(X;^{(l)}))=_{0}(a_{l}(X; ^{(l)}))\).

An \(L\)-layer Transformer \(_{L}\) consists of a composition of \(L\) of the above layers, along with a word embedding matrix \(W_{E}^{m 2k}\) and a linear decoding head with weight \(W_{}^{2k w}\). Wheninputting a sequence of tokens into Transformer, we will append a _starting token_\(t_{}\) that is distinct from any token in the language at the beginning of the sequence. Let \(^{2k(N+1)}\) denote the one-hot embedding of a length-\(N\) sequence, then \(_{L}\) computes for \(\) as

\[()=W_{}f_{L}((f_{1}(W_{E} )))_{1:2k,(N+1)}).\] (10)

## 3 Theoretical Analyses

Many prior works have looked for intuitive interpretations of Transformer solutions by studying the attention patterns of particular heads or some individual components of a Transformer (Clark et al., 2019; Vig and Belinkov, 2019; Dar et al., 2022). However, we show in this section why this methodology can be insufficient even for the simple setting of Dyck. Namely, for Transformers that generalize well on Dyck (both in-distribution and out-of-distribution), neither attention patterns nor individual local components are guaranteed to encode structures specific for parsing Dyck. We further argue that the converse is also insufficient: when a Transformer does produce interpretable attention patterns, there could be limitations of such interpretation as well, as discussed in Appendix B. Together, our results provide theoretical evidence that careful analyses (beyond heuristics) are required when interpreting the components of a learned Transformer.

### Interpretability Requires Inspecting More Than Attention Patterns

This section focuses on Transformers with 2 layers, which are sufficient for processing Dyck (Yao et al., 2021). We will show that even under this simplified setting, attention patterns alone are not sufficient for interpretation. In fact, we will further restrict the set of 2-layer Transformers by requiring the first-layer outputs to only depend on information necessary for processing Dyck:

**Assumption 1** (Minimal First Layer).: _We consider 2-layer Transformers with a minimal first layer \(f_{1}\). That is, let \(^{2k(N+1)}\) denote the one-hot embeddings of any input sequence \(t_{},t_{1},,t_{N}[2k]\), then the \((j+1)_{th}\) column of the output \(f_{1}(W^{E})\) only depends on the type and depth of \(t_{j}\), \( j[N]\)._

Assumption 1 requires the first layer output to depend only on the bracket type and depth, disregarding any other information such as positions; one such example is given by Yao et al. (2021). The construction of a minimal first layer can vary, hence we _directly parameterize its output_ instead:

**Definition 2** (Minimal first layer embeddings).: _Given a minimal first layer, \((_{t,d})^{m}\) denotes its output embedding of \(_{t,d}\) for \(t[2k]\), \(d[D]\). \((t_{})^{m}\) is the embedding of the starting token._

It is important to note that while the minimal first layer is a strong condition, it does not weaken our results: We will show that the function class allows for a rich set of solutions, none of which are necessarily interpretable. Relaxing to more complex classes will only expand the solution set, and hence our conclusion will remain valid. See Appendix C.2 for more technical details.

#### 3.1.1 Perfect Balance Condition: Ideal Generalization of Unbounded Length

Some prior works have tried to understand the model by inspecting the attention patterns (Ebrahimi et al., 2020; Clark et al., 2019; Vig and Belinkov, 2019). However, we will show that the attention patterns alone are too flexible to be helpful, even for the restricted class of a 2-layer Transformer with a minimal first layer (Assumption 1) and even on a language as simple as Dyck. In particular, the Transformer only needs to satisfy what we call the _balanced condition_:

**Definition 3** (Balance condition).: _A 2-layer Transformer (Equation (10)) with a minimal first layer (Assumption 1 and Definition 2) is said to satisfy the balance condition, if for any \(i,j_{1},j_{2}[k]\) and \(d^{},d_{1},d_{2}[D]\),_

\[((_{2i-1,d^{}})-(_{2i,d^{}-1}))^{ }(W_{K}^{(2)})^{}W_{Q}^{(2)}((_{2j_{1},d_{1}})-( _{2j_{2},d_{2}}))=0.\] (11)

The following result shows that under minor conditions the balance condition is both necessary and sufficient:

**Theorem 1** (Perfect Balance).: _Consider a two-layer Transformer \(\) (Equation (10)) with a minimal first layer (Assumption 1) and \(C_{LN}=0\) (Equation (7)). Let \(\) denote the optimal prediction scenario, that is, when the first layer embeddings \(\{(_{i,d})\}_{d[D],i[2k]}\) (Definition 2) and second layer parameters \(^{(2)}\) satisfy_

\[:=\{(_{i,d})\}_{d[D],i[2k]},^{(2)}\}=_{ }(;_{q,k,D,N}), N,\]

_where the objective \(\) is defined in Equation (4). Then,_

* _Equation (_11_) a necessary condition of_ \(\)_, if_ \(W_{V}^{(2)}\) _satisfies_ \(_{}W_{V}^{(2)}(_{t,d}) 0, t[k],d[D]\)_._
* _Equation (_11_) is a sufficient condition of_ \(\)_, if the set of_ \(2k+1\) _encodings_ \(\{(_{2i-1,d}),(_{2i,d})\}_{i[k]}\{(t_{})\}\) _are linearly independent for any_ \(d[D]\)_, and the projection function_ \(^{(2)}\) _is a 6-layer MLP_ 3 _with_ \(O(k^{2}D^{2})\) _width._

_Remark_: Recall from Equation (7) that \(_{}\) projects to the subspace orthogonal to \(^{}\). The assumption in the necessary condition can be intuitively understood as requiring all tokens to have nonzero contributions to the prediction after the LayerNorm.

Recall that \((_{2i-1,d^{}}),(_{2i,d^{}-1})\) denote the first-layer outputs for a matching pair of brackets. Intuitively, Equation (11) says that since matching brackets should not affect future predictions, their embeddings should balance out each other. The balance condition Equation (11) is "perfect" in the sense that the theory assumes the model can minimize the loss for any length \(N\); we will see an approximate version later in Theorem 3.

Proof of the necessity of the balance condition.: The key idea is reminiscent of the pumping lemma for regular languages. For any prefix \(p\) ending with a closed bracket \(_{2j,d}\) for \(d 1\) and containing brackets of all depths in \([D]\), let \(p_{}\) be the prefix obtained by inserting \(\) pairs of \(\{_{2i-1,d^{}},_{2i,d^{}-1}\}\) for arbitrary \(i[k]\) and \(d^{}[D]\). Denote the _projection of the unnormalized attention output_ by

\[u(_{t_{1},d_{1}},_{t_{2},d_{2}}):=_{}( _{t_{1},d_{1}}^{}(W_{K}^{(2)})^{}W_{Q}^{(2)} _{t_{2},d_{2}})W_{V}^{(2)}_{t_{1},d_{ 1}}.\] (12)

We ignored the normalization in softmax above, since the attention output will be normalized directly by LayerNorm according to Equation (6).

By Equation (10), there exists a vector \(v^{m}\) such that for any \(\), the next-token logits given by Transformer \(\) are

\[(p_{})=W_{}g^{(2)}(,_{2i,d^{}-1})+u(_{2j,d},_{2i-1,d^{}}) )}{\|v+(u(_{2j,d},_{2i,d^{}-1})+u(_{2j,d}, _{2i-1,d^{}}))\|_{2}}+(_{2j,d})).\] (13)

The proof proceeds by showing a contradiction. Suppose \(u(_{2j,d},_{2i,d^{}-1})+u(_{2j,d},_{2i-1,d^{}}) 0\). Based on the continuity of the projection function and the LayerNorm Layer, we can show that \(_{}(p_{})\) depend only on grammar depths \(d,d^{}\) and types \(2j,2i-1,2i\). However, these are not sufficient to determine the next-token probability from \(p_{}\), since the latter depends on the type of the last unmatched open bracket in \(p\). This contradicts the assumption that the model can minimize the loss for any length \(N\). Hence we must have

\[u(_{2j,d},_{2i,d^{}-1})+u(_{2j,d},_{2i-1,d^{}})=0.\] (14)

Finally, as we assumed that \(_{}W_{V}^{(2)}e(_{t,d}) 0\), we conclude that

\[(e(_{2i-1,d^{}})-e(_{2i,d^{}-1} ))^{}(W_{K}^{(2)})^{}W_{Q}^{(2)}e(_{2j+1,d}) =(_{}W_{V}e(_{2i,d^{}-1}) \|_{2}}{\|_{}W_{V}e(_{2i-1,d^{}})\|_{2} }),\]

where the right hand side is independent of \(j,d\), concluding the proof for necessity. The proof of sufficiency are given in Appendix C.1.

Note that the perfect balance condition is an orthogonal consideration to interpretability. For example, even the uniform attention satisfies the condition and can solve Dyck: 4

**Corollary 1**.: _There exists a 2-layer Transformer with uniform attention and no position embedding (but with causal mask and a starting token 5 ) that generates the Dyck language of arbitrary length._

Since uniform attention patterns are hardly reflective of any structure of Dyck, Corollary 1 proves that attention patterns can be oblivious about the underlying task, violating the "faithfulness" criteria for an interpretation (Jain & Wallace, 2019). We will further show in Appendix B.1 that empirically, seemingly structured attention patterns may not accurately represent the inherent structure of the task.

_Extension to approximate balance condition_: Theorem 1 assumes the model reaches the optimal loss for Dyck prefixes of any length. However, in practice, due to finite samples and various sources of randomness, training often does not end exactly at a population optima. In this case, the condition in Theorem 1 is not precisely met. However, even for models that _approximately_ meet those conditions, we will prove that when the second-layer projection function \(g^{(2)}\) is Lipschitz, a similar condition as in Equation (14) is still necessary. Details are deferred to Appendix C.4.

### Interpretability Requires Inspecting More Than Any Single Weight Matrix

Another line of interpretability works involves inspecting the weight matrices of the model (Li et al., 2016; Dar et al., 2022; Eldan & Li, 2023). Some of the investigations are done locally, neglecting the interplay between different parts of the model. Our result in this section shows that from a representational perspective, isolating single weights can also be misleading for interpretability. For this section only, we will assume the linear head \(W_{}\) is identity for simplicity. To consider the effect of pruning, we will also extend the parameterization of LayerNorm module (Equation (7)) as

\[_{C_{LN}}[b](A)_{:,j}=b_{}A_{:,j}}{\{\| _{}A_{:,j}\|_{2},\}}+(1-b)A_{:,j},\]

which corresponds to a weighted residual branch; note that the original LayerNorm corresponds to \(_{C}\). Let \(\) denote the set of parameters of this extended parameterization.

We define the _nonstructural pruning_6 as:

**Definition 4** (Nonstructural pruning).: _Under the extended parameterization, a nonstructural pruning of a Transformer with parameter \(\) is a Transformer with the same architecture and parameter \(^{}\), so that for any weight matrix \(W\) in \(\), the corresponding matrix \(W^{}\) in \(^{}\) has \(W^{}_{i,j}\{W_{i,j},0\}\), \( i,j\)._

To measure the quality of the pruning, define the \(\)-approximation:

**Definition 5** (\(\)-approximation).: _Given two metric spaces \(A,B\) with the same metric \(\|\|\), a function \(f:A B\) is an \(\)-approximation of function \(g\) with respect to that metric, if and only if,_

\[ x A,\|f(x)-g(x)\|\|x\|.\]

The metric, unless otherwise specified, will be the \(2\)-norm for vectors and the \(1,2\)-norm for matrices:

**Definition 6**.: _The \(1,2\)-norm of a matrix \(A\) is the max row norm, i.e. \(\|A\|_{1,2}=_{i[d^{}]}\|A_{:,i}\|_{2}\)._

With these definitions, we are ready to state the main result of this section:

**Theorem 2** (Indistinguishability From a Single Component).: _Consider any \(L\)-layer Transformer \(\) (Equation (10)) with embedding dimension \(m\), attention dimension \(m_{a}\), and projection function \(\) as 2-layer ReLU MLP with width \(w\). For any \((0,1)\) and \(N^{+}\), consider a \(4L\)-layer random Transformer \(_{}\) with embedding dimension \(m_{}=O(m(Lm/))\), attention dimension \(m_{}=O(m_{a}LmL}{})\), and projection function \(_{}\) as 4-layer ReLU MLP with width \(w_{}=O(\{m,w\}L)\).__Assume that \(\|W\|_{2} 1\) for every weight matrix \(W\) in \(\), and suppose the weights are randomly sampled as \(W_{i,j} U(-1,1)\) for every \(W_{}\). Then, with probability \(1-\) over the randomness of \(_{}\), there exists a nonstructural pruning (Definition 4) of \(_{}\), denoted as \(}_{}\), which \(\)-approximates \(\) with respect to \(\|\|_{1,2}\) for any input \(X^{m N}\) satisfying \(\|\|_{1,2} 1\). 7_

**Proof sketch: connection to Lottery Tickets.** Theorem 2 can be interpreted as a lottery ticket hypothesis (Frankle and Carbin, 2018; Malach et al., 2020) for randomly initialized Transformers, which can be of independent interest. The proof repeatedly uses an extension of Theorem 1 of Pensia et al. (2020), where it 1) first prunes the \((2l-1)\)-th and \(2l\)-th layers of \(_{}\) to approximate \(^{(l)}\) for each \(l[L]\) (Lemma 6), and 2) then prunes the remaining \(2L+1\) to \(4L\)-th layers of \(_{}\) to approximate the identity function. The full proof is deferred to Appendix C.5.

Noting that the layers used to approximate the identity can appear at arbitrary depth in \(_{}\), a direct corollary of Theorem 2 is that one cannot distinguish between two functionally different Transformers by inspecting any single weight matrix only:

**Corollary 2**.: _Let \(_{1},_{2}\) and \(_{}\) follow the same definition and assumptions as \(\) and \(_{}\) in Theorem 2. Pick any weight matrix \(W\) in \(_{}\), then with probability \(1-\) over the randomness of \(_{}\), there exist two Transformers \(_{,1},_{,2}\) pruned from \(_{}\), such that \(_{,i}\)\(\)-approximate \(_{i}\), \( i\{1,2\}\), and \(_{,1}\), \(_{,2}\) coincide on the pruned versions of \(W\)._

Hence, one should be cautious when using methods based solely on individual components to interpret the overall function of a Transformer.

## 4 Experiments

Our theory in Section 3 proves the existence of abundant _non-stack-like_ attention patterns, all of which suffice for (near-)optimal generalization on Dyck. However, could it be that stack-like solutions are more frequently discovered empirically, due to potential _implicit biases_ in the architecture and the training procedure? In this section, we show there is no evidence for such implicit bias in standard training (Section 4.1). Additionally, we propose a regularization term based on the balance condition (Theorem 1), which leads to better length generalization (Section 4.2).

### Different Attention Patterns Can Be Learned To Generate Dyck

We empirically verify our theoretical findings that Dyck solutions can give rise to a variety of attention patterns, by evaluating the accuracy of predicting the last bracket of a prefix (Equation 2) given the rest of the prefix. We only consider prefixes ending with a closing bracket, so that there exists a unique correct closing bracket which a correct parser should be able to determine. The experiments in this section are based on Transformers with \(2\) layers and \(1\) head, hidden dimension \(50\) and embedding dimension \(50\), trained using Adam. Additional results for three-layer Transformers are provided in Appendix D.3. The training data consists of valid Dyck\({}_{2,4}\) sequences of length less than \(28\) generated with \(q=0.5\). When tested in-distribution, all models are able to achieve \( 97\%\) accuracy.

Variation in attention patternsFirst, as a response to (Q1), we observe that attention patterns of Transformers trained on Dyck are not always stack-like (Figure 1). In fact, the attention patterns differ even across different random initialization. Moreover, while Theorem 1 implies that position encoding is not necessary for a Transformer to generate Dyck, 8 adding the position encoding 9 does affect the attention patterns (Figures 0(c) and 0(d)).

Specifically, for 2-layer Transformers with a minimal first layer, we experiment with three different types of embeddings \(\): let \(_{t}\) denote the one-hot embedding where \(_{t}[t]=1\),

\[_{t,d} =_{(t-1)D+d}^{2kD},\] (15) \[_{t,d} =_{t}_{d}^{2k+D},\] (16) \[_{t,d} =_{t}[(_{d}),(_{d })]^{2k+2},_{d}=(d/(D+2-d)),\] (17)

where \(\) denotes vector concatenation. Equation (15) is the standard one-hot embedding for \(_{t,d}\); Equation (16) is the concatenation of one-hot embedding of types and depths. Finally, Equation (17) is the embedding constructed in Yao et al. (2021). As shown in Figure 2, the attention patterns learned by Transformers exhibit large variance between different choices of architectures and learning rates, and most learned attention patterns are not stack-like.

Quantifying the variationWe now quantify the variation in attention by comparing across multiple random initializations. We define the _attention variation_ between two attention patterns \(A_{1},A_{2}\) as \((A_{1},A_{2})=\|A_{1}-A_{2}\|_{F}^{2}\), for \(A_{1},A_{2}^{N N}\) over an length-\(N\) input sequence. We report the _average attention variation_ of each architecture based on \(40\) random initializations.

On the prefix \([[[[[[[[[[(((((()))))))))))\)10, we observe that for standard two layer training, the average attention variation is \(2.20\) with linear position embedding, and is \(2.27\) without position embedding. Both numbers are close to the random baseline value of \(2.85\)11, showing that the attention head learned by different initializations indeed tend to be very different. We also experiment with Transformer with a minimal first layer and the embedding in Equation (15), where the average variation is reduced to \(0.24\). We hypothesize that the structural constraints in this setting provide sufficiently strong inductive bias that limit the variation.

Figure 3: **Relationship Between Balance Violation and Length Generalization.** Accuracy from Transformers with minimal first layer with embedding 15, using both standard training and contrastive regularization (Equation (18)). Standard training leas to high balance violations which negatively correlate with length generalization performance. Contrastive regularization helps reduce the balance violation and improve the length generalization performance.

Figure 2: **Second-layer attention patterns of two-layer Transformers with a minimal first layer**: (a), (b) are based on embedding 15 with different learning rates, where the attention patterns show much variance as Theorem 1 predicts. (c), (d) are based on embedding 17 and 16. Different embedding functions lead to diverse attention patterns, most of which are not stack-like.

### Guiding The Transformer To Learn Balanced Attention

In our experiments, we observe that although models learned via standard training that can generalize well in distribution, the _length generalization_ performance is far from optimal. This implies that the models do not correctly identify the parsing algorithm for Dyck when learning from finite samples. A natural question is: can we guide Transformers towards correct algorithms, as evidenced by improved generalization performance on longer Dyck sequences?

In the following, we measure length generalization performance by the model accuracy on valid Dyck prefixes with length randomly sampled from \(400\) to \(500\), which corresponds to around 16 times the length of the training sequences. Inspired by results in Section 3, we propose a regularization term to encourage more balanced attentions, which leads to better length generalization.

Regularizing for balance violation improves length generalization accuracyWe denote the _balance violation_ of a Transformer as \(:=_{d,d^{},i,j}[S_{d,d^{},i,j}/P_{d,j}]\) for \(S,P\) defined in Equations (31) and (33). Theorem 1 predicts that for models with a minimal first layer, perfect length generalization requires \(\) to be zero. Inspired by this observation, we design a contrastive training objective to reduce the balance violation, which ideally would lead to improved length generalization. Specifically, let \(p_{r}\) denote a prefix of \(r\) nested pairs of brackets of for \(r U([D])\), and let \((s p_{r} s)\) denote the logits for \(s\) when \(\) takes as input the concatenation of \(p_{r}\) and \(s\). We define the _contrastive regularization term_\(R_{}(s)\) as the mean squared error between the logits of \((s)\) and \((s p_{r} s)\), taking expectation over \(r\) and \(p_{r}\):

\[_{r U([D]),p_{r}}[\|(s p_{r} s)- (s)\|_{F}^{2}].\] (18)

Following the same intuition as in the proof of Theorem 1, if the model can perfectly length-generalize, then the contrastive loss will be zero. Models trained with contrastive loss show reduced balance violation as well as improved length generalization performance, as shown in Figure 3.

## 5 Conclusion

Why interpreting individual components sometimes leads to misconceptions? Through a case study of the Dyck grammar, we provide theoretical and empirical evidence that even in this simple and well-understood setup, Transformers can implement a rich set of non-interpretable solutions. This is reflected both by diverse attention patterns and by the absence of task-specific structures in local components. Our results directly imply similar conclusions for more complex Transformer models; see Appendix C.2 for technical details. Together, this work provides definite proof that myopic interpretability, i.e. methods based on examining individual components only, are not sufficient for understanding the functionality of a trained Transformer.

Our results do not preclude that interpretable attention patterns can emerge; however, they do suggest that interpretable patterns can be infrequent. We discuss the implications for multi-head, overparameterized Transformers trained on more complex data distributions in Appendix B. Moreover, our current results pertain to the existence of solutions; an interesting next step is to study how "inductive biases" given by the synergy of the optimization algorithm and the architecture affect the solutions found.