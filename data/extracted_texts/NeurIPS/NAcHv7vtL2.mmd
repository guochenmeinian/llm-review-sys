# Scaling laws for learning with real and surrogate data

Ayush Jain\({}^{1}\)

ayush.jain@granica.ai

&Andrea Montanari\({}^{1,2}\)

andrea.montanari@granica.ai

&Eren Sasoglu\({}^{1}\)

eren.sasoglu@granica.ai

\({}^{1}\) Granica Computing Inc. -- granica.ai

\({}^{2}\)Stanford University

###### Abstract

Collecting large quantities of high-quality data can be prohibitively expensive or impractical, and a bottleneck in machine learning. One may instead augment a small set of \(n\) data points from the target distribution with data from more accessible sources, e.g. data collected under different circumstances or synthesized by generative models. We refer to such data as'surrogate data'. We study a weighted empirical risk minimization (ERM) approach for integrating surrogate data into training. We analyze mathematically this method under several classical statistical models, and validate our findings empirically on datasets from different domains. Our main findings are: \((i)\) Integrating surrogate data can significantly reduce the test error on the original distribution. Surprisingly, this can happen _even when the surrogate data is unrelated to the original ones_. We trace back this behavior to the classical Stein's paradox. \((ii)\) In order to reap the benefit of surrogate data, it is crucial to use optimally weighted ERM. \((iii)\) The test error of models trained on mixtures of real and surrogate data is approximately described by a scaling law. This scaling law can be used to predict the optimal weighting scheme, and to choose the amount of surrogate data to add.

## 1 Introduction and overview

### Motivation and formulation

Consider a standard learning setting where we are given \(n\) i.i.d. points \(_{i}\) from a target distribution \(\). Given a family of parametric models governed by the parameters' vector \(\), the goal is to find \(\) that minimizes the expected test loss \(R_{}()\) incurred by the model predictions, where expectation is taken over the distribution \(\). In many application domains, the available data \(=(_{i})_{i n}\) from the target distribution, referred to as either _real_ or _original_ data, may be difficult or expensive to acquire. One may then attempt to supplement these data with a different, cheaper source. Examples of such cheaper sources are \((i)\) publicly available datasets; \((ii)\) datasets owned by the same research group or company but acquired in different circumstances, e.g. in a different location; \((iii)\) synthetic data produced by a generative model.

We will denote the data points obtained from this source by \(_{i}^{s}\), and assume we have \(m\) of them. We assume the'surrogate' data \(^{s}=(_{i}^{s})_{i m}\) to be i.i.d. samples with distribution \(^{s}\). In general, we will not assume the distribution \(^{s}\) of synthetic data to be close to the original data distribution \(\). However we assume that these distributions are over the same domain. A number of questions arise: \((i)\) How should we use the surrogate data in training? \((ii)\) How many surrogate samples should we add to the original data? \((iii)\) Can we predict the improvement in test error achieved by adding surrogate samples to the training?A natural approach would be to add the surrogate data to the original one in the usual training procedure, and indeed many authors have explored this approach (see Section 1.3). Namely, one attempts to minimize the overall empirical risk \(^{}_{n+m}()=_{i=1}^{n}(; _{i})+_{i=1}^{m}(;_{i}^{s})\), where \((z,)\) is a train loss function.

However, a moment of reflection reveals that this approach has serious shortcomings. Consider a simple mean estimation problem, whereby \(_{i}(_{*},_{d})\), \(_{i}^{s}(_{*}^{s},_{d})\), \((;)=\|-\|^{2}\), and \(R_{}()=\|-_{*}\|^{2}\). A straightforward calculation yields that the test error of the empirical risk minimizer \(}^{}_{n+m}:=^{}_{ n+m}()\) is

\[R_{}(}^{}_{n+m})=( )^{2}\|_{*}^{s}-_{*}\|^{2}+\.\] (1)

As \(m\) increases the variance (the second term) decreases, but the bias due to the difference \(\|_{*}^{s}-_{*}\|\) increases, and the error approaches \(\|_{*}^{s}-_{*}\|^{2}\), i.e. the model will be only as good as if training only on surrogate data.

In order to overcome these limitations, we study a weighted ERM approach, and will show that the weight plays a crucial role. Namely, we consider the following regularized empirical risk:

\[_{n,m}(;):=_{i=1}^{n}( ;_{i})+_{i=1}^{m}(;_{i}^{s})+()\,,\] (2)

where \(\) is the weight of the surrogate dataset and \(:^{d}_{ 0}\) is a regularizer, e.g. a ridge \(()=\|\|_{2}^{2}\). We denote by

\[}_{n,m}():=_{}_{n,m}( {};)\] (3)

the corresponding empirical risk minimizer, and by \(R_{}(}_{n,m}())\) the corresponding test error.

For supervised learning tasks, a sample \(\) is represented as \(=(y,)\), where \(^{d}\) is covariate vector and \(y\) is response variable and \(\) parametrizes a family of models \(f(;)\) that predict the response \(y\) given covariate vector \(\). We consider losses of the form \((,)=L(y,f(;))\) and \(R_{}():=_{z}L_{}(y,f (;))\) for some functions \(L\) and \(L_{}\). We allow for the test loss \(L_{}\) to be different from the train loss \(L\), but we will omit the subscript 'test' from the risk \(R\) and the loss \(L\) whenever clear from the context.

Figure 1: IMDB and Rotten Tomatoes data and neural networks. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).

Figure 1 provides a preview of our results, for a sentiment analysis task. (Technical details provided in Section 4 and Appendix A.3). Each frame corresponds to a different combination of \(n\) and \(m\), and we report the test error of our approach as a function of the weight parameter \(\) (red circles). Solid lines report the prediction of a scaling law that will be one of the main results presented below.

We observe that the weighted ERM approach systematically achieves better test error than either training only on original data (\( 0\)) or on surrogate data (\( 1\)). Further the error for optimal \(\) is always monotone decreasing both in \(m\) and \(n\), and the approach outperforms the naive unweighted approach. This is shown more clearly in Figure 2, which also shows that the performance of unweighted ERM can degrade with more surrogate data. Also, while scaling laws typically do not capture the dependence on hyperparameters, the scaling law presented below predicts the dependence on \(\) reasonably well. This is particularly useful, because such a scaling law can be used to tune \(\) optimally and to predict the amount of surrogate data needed.

### Summary of results

We study the method outlined above both mathematically and via numerical experiments. Our mathematical results are developed in four different settings: \((i)\) The Gaussian sequence model (Section 3.1); \((ii)\) A non-parametric function estimation setting (Section 3.2); \((iii)\) Low-dimensional empirical-risk minimization (Section 3.3); \((iv)\) High dimensional ridge regression (Section 3.4);

We carry out experiments with the following data sources. \((1)\) Simulated data from linear or Gaussian mixture models: this allows us to explicitly control the distribution shift between the original and surrogate datasets, as well as check our theoretical results in a controlled setting. \((2)\) Real natural language processing (NLP) data for sentiment analysis, with the role of original dataset played by IMDB reviews and the role of surrogate datasets played respectively by Rotten Tomatoes review and Goodreads book reviews. \((3)\) Progression-free survival analysis using Lasso on TCGA PanCancer dataset with female patients data and male patients data as original and surrogate data, respectively. \((4)\) Real image classification data, with CIFAR-10 and CIFAR-100 datasets respectively playing the role of original and surrogate data. Our results support the following conclusions:

**Surrogate data improve test error.** Including surrogate data in training generally improves the test error on the original data, _even if the surrogate data distribution is far from the original one_. In agreement with the interpretation of surrogate data as a regularizer (see also Sec. 2), the improvement is generally positive, although its size depend on the data distributions.

**Tuning of \(\).** The above conclusion holds under the condition that \(\) can be tuned (nearly) optimally. For each of the theoretical settings already mentioned, we characterize this optimal value. We verify

Figure 2: Performance of unweighted vs weighted ERM approach for the setting in Figure 1

that nearly optimal \(\) can be effectively selected by minimizing the error on a validation split of the original data. An attractive alternative is to use the scaling law we discuss next.

**Scaling law.** We propose a scaling law that captures the behavior of the test error with \(n,m,\):

\[R(}_{n,m}())-R_{*}^{2}R_{}^{}()+[^{2}R_{}^{}(m)-R_{}^{ }()^{1/}+(1-)^{2}R_{}^{}(n )^{1/}]^{}.\] (4)

Here \(R_{*}\) is the minimal (Bayes) error, \(R_{}^{}(m):=R(}_{0,m}(1))-R_{*}\) is the excess test error when training on the surrogate data (and testing on original), \(R_{}^{}(n):=R(}_{n,0}(0))-R_{*}\) is the excess test error1 when training on original data (and testing on original), and \(\) is a scaling exponent as described in Section 4. The above scaling admits natural generalizations; see Section 5.

**Practical uses of the scaling law.** Given data \(\{_{i}\}_{i n}\) and a source of surrogate data, we would like to predict how much the test error can be decreased by including any number \(m\) of surrogate samples in the mix. The scaling law (4) suggests a simple approach: \((1)\) Learn models on purely original data to extract the behavior of test loss \(R(}_{n,0}(0))\).; \((2)\) Learn models on purely surrogate data to extract the behavior of \(R(}_{0,m}(1))\). (A relatively small sample is sufficient for this step.) \((3)\) Use the minimum over \(\) of Eq. (4) to predict the test error at any given pair \(n,m\).

We can further leverage the scaling law to achieve the desired error by: \((1)\) Using the scaling law to determine the number of surrogate samples needed to achieve the desired performance. \((II)\) Acquiring the surrogate samples and train the model using weighted ERM with optimal weighting predicted by scaling law.

### Related work

The use of surrogate data to enhance training has attracted increasing research effort, also because of the recent progresses in generative modeling.

This line of work has largely focused on the techniques to generate synthetic data that are well suited for training. A wide variety of methods have been demonstrated to be useful in generating data for computer vision tasks, ranging from object classification to semantic segmentation . We refer to  for a review. More recently, synthetic data have been used for training in natural language processing .

Scaling laws have been broadly successful in guiding the development of large machine learning models . We expect them to be similarly useful for integrating heterogeneous data into training. The change in scaling laws when training on synthetic data was the subject of a recent empirical study . On the other hand, no systematic attempt was made at integrating real and synthetic data.

In data augmentation , the original samples are supplemented with transformed or noisy version of the same. In contrast, we assume that surrogate data is obtained from a different source than the original one, and the surrogate samples are independent of the original samples.

The problem we consider was also studied within 'domain adaptation', a subarea of transfer learning . Among others,  establishes bounds on the generalization error of weighted ERM via uniform convergence. However these bounds do not reveal the full advantage achieved by this approach and are not precise enough to justify the scaling laws that we derive. Recent works in domain adaptation study the behavior of test error error  and its scaling laws , but only consider vanilla ERM, a special of weighted ERM considered here.

## 2 Regularization, Gaussian mean estimation, Stein paradox

The role of the parameter \(\) can be understood by considering the limit \(m\):

\[_{n,}(;)=_{i=1}^{n} (;_{i})+\,R^{s}()+()\,,\]and \(R^{s}()=_{^{s}}^{s}}( ;^{s})\) is the population risk for surrogate data. This suggests to think of the surrogate data as an additional (highly non-trivial) regularizer, with parameter \(\). This leads to a simple yet important insight: adding surrogate data to the original data is beneficial if \(\) is chosen optimally, and large \(m\) reduces statistical fluctuations in this regularizer. This contrasts with the unweighted approach whose test error in general deteriorates for large \(m\).

As a toy example, reconsider the mean estimation problem mentioned in the introduction: \(_{i}(_{*},_{d})\) and \(_{i}^{s}(_{*}^{s},_{d})\), \((;)=\|-\|^{2}\) and \(R_{}()=\|-_{*}\|^{2}\). We have \(}_{n,m}()=(1-)_{i n}_{i}/n+ _{i m}_{i}^{s}/m\). In other words, the weighted ERM shrinks the mean of the original data towards the mean of the surrogate data. For a given \(\), the resulting test errors are

\[R(}_{n,m}())=^{2}R_{}^{}()+(}{m}+}{n} )d\,, R_{}^{}()=\|_{ *}-_{*}^{s}\|^{2}\,,\] (5)

and for the optimum value \(_{*}=_{}R(}_{n,m}())\), this yields

\[R(}_{n,m}(_{*}))=(}^{ { ex}}()+d/m}{R_{}^{}()+d/m+d/n })\,.\] (6)

Note that \(1/n\) is the error of training only on original data and the prefactor is always strictly smaller than one. Hence, weighted ERM always achieves better error than training only on original data, _regardless of the distance between original and surrogate data_, although the improvement is larger for small \(R_{}^{}()\). This might seem paradoxical at first. As mentioned above, we are shrinking towards an arbitrary point given by the empirical mean of the surrogate data: how can this help?

In fact, this is a disguised version of the celebrated Stein paradox : in estimating a Gaussian mean, a procedure that shrinks the empirical mean _towards an arbitrary point_ by a carefully chosen amount outperforms the naive empirical mean. In our toy example, the naive empirical mean corresponds to estimation purely based on the original data, and we shrink it towards the mean of the surrogate data. Of course, the improvement over empirical mean is only possible if \(\) is chosen optimally. Equation (6) assumes \(=_{*}\) is chosen by an oracle that knows the value of \(R_{}^{}()\). Stein's analysis implies that in the Gaussian mean problem, \(\) can be chosen empirically as long as the dimension of \(\) is \(d 3\). In the settings we are interested in, \(\) can be chosen via cross-validation.

## 3 Theoretical results

### Gaussian sequence model

The sequence model captures the behavior of many models in non-parametric statistics while being simpler to analyze . It is also known to approximate the behavior of overparametrized linear regression . The unknown target is \(_{*}^{d}\) (with potentially \(d=\)), and we observe

\[_{i}=_{*}+\,_{i},\ i n\,,_{i}^{s} =_{*}^{s}+_{s}\,_{i}^{s},\ i m\,,\] (7)

where \(_{*}^{s}\) is also unknown, and \(_{i},_{i}^{s}(0,_{d})\) are i.i.d. We study the penalized estimator

\[}_{n,m}():=_{}\{_{i=1}^{n}\|_{i}-\|_{2}^{2}+_{ i=1}^{m}\|_{i}^{s}-\|_{2}^{2}+\|\|_{}^{2}\},\] (8)

where \(\|\|_{}^{2}=,\) and \(\) is a regularization weight matrix. We will be concerned with the expected risk

\[R_{n,m}(,)=}_{n,m}( )-_{*}^{2}}\,.\] (9)

The proof of the next result is presented in Appendix C.

**Theorem 1**.: _Let \(_{1}_{2}\) be the ordered eigenvalues of \(\), and denote by \(_{i}\) the corresponding eigenvectors. Further denote by \(_{*,>k}\), \(_{*,>k}^{s}\) the projections of \(_{*}\), \(_{*,s}\) onto \((_{i}:i>k)\), and similarly for \(_{*, k}\), \(_{*, k}^{s}\). Assume that \(_{k} k^{}\), \(>1/2\), \(\|_{*,>k}\|^{2} C_{}k^{-2}\), \(\), and let \(_{k}\) be such that (for all \(k\)): \(_{k}:=_{k}^{-1}|_{*, k}-_{*, k}^{ s},_{*, k}| C_{0}k^{-2()}\). Then the following hold:_

\[(a)\] _There exists an explicit_ \[_{*}()\] _such that, letting_ \[:=2()/(1+2())\] _,_ \[R_{n,m},_{*}()^{2}R_{}^{}()+C[(1-)^{2}}{n}+ ^{2}^{2}}{m}]^{}\,.\] (10)_._
* _If_ \(>2-1/2\)_, there exists_ \(C^{}>0\) _and there exist_ \(_{*},_{*}^{s}\) _satisfying the assumptions in point_ \((a)\)_, such that,_ \[_{}R_{n,m},^{2}R_{}^ {}()+C^{}[(1-)^{2}}{n}+ ^{2}_{*}}{m}]^{}\,.\] (11)

Note that since the theorem also implies \(R_{}^{}(m)-R_{}^{}()( _{s}^{2}/m)^{}\) and \(R_{}^{}(m)(^{2}/n)^{}\), this result confirms the scaling law (4).

### Non-parametric regression in Sobolev classes

In this section we consider the classic non-parametric regression model. We assume that \(n=Q^{d}\) for some integer \(Q 2\), and the original data \((_{i},y_{i})_{i n}\) are defined through

\[y_{i}=f_{*}(_{i})+_{i}\,,_{i}(0,^{2})\,,\] (12)

where \(_{i}\) are independent of \(_{i}\) and of each other, and \(\{_{i}\}_{i n}\) equally spaced grid points in the \(d\)-dimensional unit-cube, i.e. \(_{n}=\{/Q:\;\;[Q]^{d}\}\). Surrogate data have a similar distribution, with \(m=Q_{s}^{d}\) equally spaced points \(_{i}^{s}\) in the unit cube, and \(y_{i}^{s}=f_{*,s}(_{i}^{s})+_{i}^{s}\), where \(_{i}^{s}(0,_{s}^{2})\). We assume that \(f_{*}\) has small Sobolev norm, that is,

\[\|f_{*}\|_{r,2}^{2}_{^{d}}|f_{*}(t)|^{2}+\|f_{*}^{(r )}(t)\|^{2}t 1\,.\]

Recall that \(\|f\|_{r,2}^{2}\) is a special reproducing kernel Hilbert space (RKHS) norm: we expect some of the considerations below to generalize to other RKHS norms.

Following our general methodology, we use the estimator

\[_{n,m,}=_{f}\{_{i=1}^{n} y_{i}-f(_{i})^{2}+_{i=1}^{m}y _{i}^{s}-f(_{i}^{s})^{2}+\|f\|_{p,2}^{2}\}.\] (13)

We are interested in \(R(f)=\{(f()-f_{*}())^{2}\}\), which is the excess squared loss for a test point \((^{d})\).

In order to avoid technical burden we will carry out the analysis for a continuous model, the so-called white noise model, where we observe the function \(f\) at all points \(^{d}\), perturbed by \(d\)-dimensional white noise:

\[Y=f_{*}()\,+}B()\,,\] (14)

and similarly for \(Y^{s}\). We use an estimator that naturally generalizes (13) to the continuous case. Our results for the white noise model are as follows.

**Theorem 2**.: _Let \(=(2p 4r)/(d+(2p 4r))\). If \(r>d/4\) and \(=( K_{n,m}^{2})^{2r/(d+(2p 4r))}\), then for every \((0,1)\) there exists a constant \(C=C(d,)\) such that_

\[R(_{n,m,})(1+)^{2}R_{}^{}( )+C\{(1-)^{2}}{n}+^{2} _{*}}{m}\}^{}\] (15)

_with high probability, where \(K_{n,m}:=(1-)^{2}/n+^{2}/m\)._

**Remark 3.1**.: The white noise model (14) is known to be equivalent to the original model (12) (with deterministic equispaced designs) in the sense of Le Cam, for \(r>d/2\). While suggestive, this equivalence does not allow us to formally deduce results for the data (12), because it does not apply to the specific estimators of interest here.

With the given choice of \(\), \(r\), the derivation of (15) also implies \(R_{}^{}(m)-R_{}^{}() C ^{}(_{s}^{2}/m)^{}\), \(R_{}^{}(n) C^{}(/n)^{}\) (for the least favorable \(f\)). Hence (15) is consistent with the scaling law (4).

### Low-dimensional asymptotics

We study the estimator of Eqs. (2), (3) under the classical asymptotics \(n,m\) at \(d\) fixed. Since this type of analysis is more standard, we defer it to Appendix B. The main result of this analysis is that the scaling law (4) holds in this setting, with the classical parametric exponent \(=1\), for \([0,_{}]\) for a suitable \(_{}(0,1)\). Importantly, the interval \([0,_{}]\) includes the optimal choice of the weight \(\).

### High-dimensional linear regression

In this section, we study ridge regression in the high-dimensional regime in which the number of samples is proportional to the number of parameters. Denoting the original data by \((,)\) (with \(^{n}\) the vector of responses and \(^{n d}\) the matrix of covariates), and the surrogate data by \((^{s},^{s})\) (with \(^{s}^{m}\) and \(^{s}^{m d}\)), we minimize the regularized empirical risk

\[_{n,m}(;)=\|-\|_{2}^{2}+\|^{s}-^{s}\|_{2}^{2 }+\,\|\|_{2}^{2}\,,\] (16)

We assume a simple distribution, whereby the rows of \(\), \(^{s}\) (denoted by \(_{i}\), \(_{i}^{s}\)) are standard normal vectors and

\[=_{*}+\,,^{s}=^{s} _{*}^{s}+^{s}\,.\] (17)

for \((,^{2}_{n})\), \(^{s}(,_{s}^{2}_{m})\). Note that the two data distributions differ in the true coefficient vectors \(_{*}\) versus \(_{*}^{s}\) as well as in the noise variance. We will denote by \(}_{n,m}()\) the ridge estimator, \(}_{n,m}()=_{^{d}} _{n,m}(;)\).

The excess test error (for square loss) is given by \(R(}):=,_{*} -,}^{2}}=\|}-_{*}\|^{2}\). The next result characterizes this error in the proportional asymptotics.

**Theorem 3**.: _Consider the ridge regression estimator \(}_{n,m}()\). Let \(r:=\|_{*}\|_{2}\), \(r_{s}:=\|_{*}^{s}\|_{2}\) and \(:=^{-1}(_{*},_{*}^{s}/(\|_{*}\|_{2}\|_{*}^{s}\|_{2}))\). Assume \(n,m,d\) such that \(n/d\), \(m/d_{s}\), with \(+_{s}>1\)2. For \((.)\) defined in Appendix E.1, let_

\[^{*}(),_{}^{*}(),^{*}()=_{,_{} 0, 0}(,_{}, ;),\]

_be the unique minimizer. Then for any \(,_{0}>0\), there exist \(c>0\) such that, for all \(n\)_

\[_{[_{0},1-_{0}]}R (}_{n,m}())-_{}()  1-2\,e^{-cn}\,,\]

_where \(_{}():=(^{*}()-r)^{2}+(_{}^{*}( ))^{2}+(^{*}())^{2}\). Further, we can take \(_{0}=0\) if \(,_{s}>1\)._

**Remark 3.2** (Optimizing \(\) over the validation set).: Note that the concentration of \(R(}_{n,m}())\) around the theoretical prediction \(_{}()\) in Theorem 3 is uniform over \([_{0},1-_{0}]\). This means that we can find the optimal \(\) by computing \(}_{n,m}()\) over a grid of \(\) values, estimating \(R(}_{n,m}())\) over the validation set and choosing the optimal \(\). The uniform guarantee insures that this procedure will achieve risk \(_{}_{}()+o_{P}(1)\).

**Remark 3.3** (Relation to scaling laws).: An analysis of the equations for \((^{*},_{}^{*},^{*})\) reveals that, for large \(,_{s}\), the predicted excess risk behaves as \(_{}()=^{2}_{*,}^{*}+ ^{2}C_{1}/_{s}+(1-)^{2}C_{2}/+o(1/,1/_{s})\) (for some constants \(_{*,}^{*},C_{1},C_{2}\)). This matches the low-dimensional asymptotics and our scaling law (4) with \(=1\). In practice, we find that, for moderate \(,_{s}\), the behavior of \(_{}()\) is better approximated by a different value of \(\) (see Appendix A.)

## 4 Empirical results

In this section, we present experiments validating that the scaling law (4) is a good approximation both for simulated and real-world data. For simulated data, we select two different distributions for the original and surrogate datasets. The test and validation sets are generated from the same distribution as the original dataset. In case of real-world data, we choose two different datasets as the original and surrogate datasets. We split the original dataset into train, test, and validation sets, while all examples in the surrogate datasets are allocated solely to the train split.

For each dataset and model discussed in this section, we carry out the same experiment: \((i)\) We use models trained on original data to fit the scaling curve \(R(}_{n,0}(0))=A_{}}+B_{}}n^{- _{}}}\) and obtain \(A_{}}\) and \(_{}}\)\((ii)\) We use models trained on purely surrogate data to fit the scaling curve \(R(}_{0,m}(1))=A_{}}+B_{}}m^{-_{ }}}\) to obtain \(A_{}}\) and \(_{}}\). \((iii)\) Since assume \(R_{*}=R(}_{,0}(0))\), we let\(R_{}=A_{}\) and excess risk estimates \(R_{}^{}(n)=R(}_{n,0}(0))-A_{}\), \(R_{}^{}(m)=R(}_{0,m}(1))-A_{}\) and \(R_{}^{}()=A_{}-A_{}\), and we use \(=_{}\), the fit exponent obtained from original data); \((iv)\) For each combination of \(n,m\), we use our estimates of \(R_{}^{}(m)\), \(R_{}^{}(n)\) (as measured empirically on the test set), \(\), \(R_{}^{}()\), and \(R_{}\) to plot the predicted \(R(}_{n,m}())\) as a function of \(\) using scaling law (4). \((v)\) We then train the model using \(n\) original and \(m\) surrogate examples with weights \((1-)\) and, \(\) for the two datasets, respectively. We average the results of 10 independent runs to compare it against those predicted by the scaling law. For ridge regression, we also compare with exact high-dimensional asymptotics from Theorem 3.

_Let us emphasize that these plots probe the dependence on the hyperparameter \(\). These are much more demanding tests that the usual ones in scaling laws. We generally observe that the scaling law captures well the behavior of the test error for data mixtures. Furthermore, we perform experiments for variety of loss functions to show these scaling laws hold more widely than the theoretical settings we considered._

Binary classification with Gaussian mixture dataThis is a simple simulated setting. The original dataset consists of independent and identically distributed examples \((y_{i},_{i})^{d}\), \(d=200\), where \(y_{i}\) is uniform over \(\{+1,-1\}\), and \(_{i}_{y_{i}}(y_{i}_{},_{d})\), where \(_{}^{d}\), \(\|_{}\|=1\). Surrogate data have the same distribution, with a different unit vector \(_{,s}\). This data distribution is parametrized by \(d\) and the angle \(\) between the original and surrogate parameters, \(:=_{},}_{,s}\). We use \(=/10\) in our experiments. For each \((n,m,)\), we averaged the results over 10 independent runs.

We use two different models for classification: (1) Logistic regression; \((2)\) A one-hidden layer neural network with 32 hidden ReLU neurons. The results for both models are presented in Appendix A.1.

Linear regression with Gaussian mixture dataFor the Gaussian mixture data generation setup described above, we also perform ridge regression. The results (presented in Appendix A.2) demonstrate that classification loss and square loss often have a similar qualitative behavior as a function of weight \(\), as seen by comparing the classification loss in Figure 7 and the squared loss in Figure 11 for the same setup. Although our theoretical results do not apply directly to classification loss, we believe that our qualitative conclusions generalize. This is confirmed by the similar behavior between the two losses and the successful prediction of actual risk by scaling laws in our classification experiments.

Figure 3: CIFAR10 and CIFAR100 data. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).

Sentiment analysis in movie reviewsAs original data, we use the IMDB dataset (link) which has 25k reviews for training, each labeled as positive or negative. For validation and testing, we split the IMDB test dataset of 25k reviews into a validation set of 10k reviews and test set of 15k reviews.

We experiment with two different surrogate datasets: 1) Rotten Tomatoes dataset of movie reviews (link): these are data with different distribution but within the same domain. This dataset contains movie reviews and the corresponding sentiments, 2) Goodreads book reviews (link): these are data from a substantially different domain. This dataset has reviews and their ratings. We choose 10k reviews each with a rating of 5 and 1, and label them as positive and negative, respectively.

We convert reviews into feature vectors with \(d=884\) dimensions as explained in Appendix A.3. We use logistic regression and neural network models with the same set of parameters as in the Gaussian mixture experiments (except for the input dimension).

Results with neural nets and Rotten Tomatoes as synthetic dataset are presented in Figure 1 and the remaining results are in Appendix A.3.

Image classification with CIFAR10 and CIFAR100We use 50,000 CIFAR10 training images as original data, its 10 classes for the classification task, and test on the 10,000 CIFAR10 test images. We use 50,000 CIFAR100 training images as surrogate data. We train a 9-layer ResNet model for classification. Appendix A.4 presents details on the data pre-processing and mapping of labels. Results are shown in Figure 3. Note that CIFAR10 and CIFAR100 datasets are quite different from each other, as they have no overlap either in the images or in their label sets. Yet, the test error on training on their mixture is well predicted by the scaling law (4).

Lasso-based Cox regression on TCGA PanCancer datasetWe use the public domain TCGA pancancer dataset [GCH\({}^{+}\)20] (link), with gene expressions as covariates and progression-free survival (PFS) as response. After filtering and feature selection, we are left with 3580 female patients, which we use as original data, and 3640 male patients, which we use as surrogate data. We fit CoxPHFitter model (link) with 500 selected genes and use "1-concordance score" as our loss function. The results are shown in Figure 4. The details of pre-processing and experiment parameters3 are in Appendix A.5.

High-dimensional ridge regressionWe simulate the data distribution in Section 3.4, i.e., \(y_{i}=_{*},_{i}+_{i}\), \(i n\); \(y_{i}^{s}=_{*,s},_{i}^{s}+_{i}^{s}\), \(i m\); with \(_{i},_{i}^{s}(,_{d})\), \(_{i}(0,^{2})\), \(=1\), see appendices.

Figure 4: Lasso-based Cox regression on TCGA PanCancer dataset. Test error when trained on mixtures of original and surrogate data. Black curves: prediction from Eq. (4).

\(_{i}^{s}(0,_{s}^{2})\), and fit a simple linear model using ridge regression. The results are shown in Figure 5. In our experiments, we use \(d=500\), \(^{2}=_{s}^{2}=1\), \(\|_{*,s}\|=\|_{*,s}\|=1\) and regularization parameter \(=2^{-10}\). Under these settings, the model is parametrized by the angle \(\) between \(_{*}\) and \(_{*,s}\), where \(:=_{*},_{*,s}\). We used \(=/6\) and \(/2\) in our experiments.4

The theoretical predictions of Theorem 3 for these curves in high-dimensional asymptotics \(n,m,d\), with \(n/d\), \(m/d_{s}\) are reported as blue lines, and match remarkably well with the empirical data. The simple scaling law (4) nevertheless provides a good approximation of these (more complicated) theoretical formulas.

Note in particular that in the top row of Figure 5, we have \(_{*},_{*,s}=0\), i.e. the surrogate data are as far as possible from the original ones. Nevertheless, the induced regularization effect leads to smaller test error on the original distribution.

We observe proposed scaling law (4) predicts well the behavior of the experiments, across of the datasets above, and for most combinations of original and surrogate examples we have tested.

Finally, we emphasize that the scaling law is only an empirical approximation of reality. This is clearly illustrated by the example of ridge regression: in this case, we use Theorem 3 to precisely predict the discrepancy between precise asymptotics and scaling law, see Appendix A.6.

## 5 Discussion

We conclude by discussing two possible generalizations of the scaling law (4), and its applicability. _First,_ throughout this paper we assumed that \(R_{}^{}()=0\), namely that we can achieve the Bayes error by training on infinitely many original samples. In practice this will not hold because of the limited model complexity. Following standard scaling laws , this effect can be accounted for by an additional term \(C N^{-}\), where \(N\) is the model size (number of parameters). _Second,_ the scaling law (4) implies as special cases that \(R_{}^{}(n) A_{}n^{-}\), \(R_{}^{}(m) R_{}^{}()+A_{ }m^{-}\). In particular, the exponent \(\) is the same when training on real or surrogate data. In practice, we observe often two somewhat different exponents \(_{}_{}\). In these cases, we set \(=_{}\), and this appears to work reasonably well. However, we can imagine cases in which the difference between \(_{}\) and \(_{}\) is significant enough (4) will stop being accurate.