# Credit Attribution and Stable Compression

Roi Livni

Tel Aviv University. rlivni@tauex.tau.ac.il.

Shay Moran

Technion and Google Research. smoran@technion.ac.il.

Kobbi Nissim

Georgetown University and Google Research. kobbi.nissim@georgetown.edu.

Chirag Pabbaraju

Stanford University. cpabbara@cs.stanford.edu.

###### Abstract

Credit attribution is crucial across various fields. In academic research, proper citation acknowledges prior work and establishes original contributions. Similarly, in generative models, such as those trained on existing artworks or music, it is important to ensure that any generated content influenced by these works appropriately credits the original creators.

We study credit attribution by machine learning algorithms. We propose new definitions-relaxations of Differential Privacy-that weaken the stability guarantees for a designated subset of \(k\) datapoints. These \(k\) datapoints can be used non-stably with permission from their owners, potentially in exchange for compensation. Meanwhile, each of the remaining datapoints is guaranteed to have no significant influence on the algorithm's output.

Our framework extends well-studied notions of stability, including Differential Privacy (\(k=0\)), differentially private learning with public data (where the \(k\) public datapoints are fixed in advance), and stable sample compression (where the \(k\) datapoints are selected adaptively by the algorithm). We examine the expressive power of these stability notions within the PAC learning framework, provide a comprehensive characterization of learnability for algorithms adhering to these principles, and propose directions and questions for future research.

## 1 Introduction

Many tasks that use machine learning algorithms require proper _credit attribution_. For example, consider a model trained on scientific papers that needs to reason about facts and figures based on existing literature. Most academic literature is protected under copyright licenses such as CC-BY 4.0 which allows adapting, remixing, transforming, and to copy and redistribute in any medium or format, as long as attribution is given to the creator. In another setting, a learner generating content, such as images or music, may benefit from creating _derivative works_ from copyrighted materials without violating the creator's rights (either through proper attribution or monetary compensation, depending on the context and licensing).

The increasing use of ML algorithms and the need for greater transparency is reflected by the recently implemented EU AI Act, which mandates the disclosure of training data . However, disclosure of training data and proper attribution are not necessarily equivalent. In particular, mere transparency of the dataset does not reveal whether certain elements of certain content have been derived, nor does it provide proper attribution when particular content is heavily built upon. Therefore, there is a need to develop more nuanced notions and definitions that enable learning under the constraint that works are properly attributed. This paper focuses on this challenge, exploring theoretical models of _credit attribution_ to provide rigorous and meaningful definitions for the task.

Credit attribution is part of a much larger problem of learning under _copyright_ constraints. Copyright issues in machine learning models are becoming increasingly prominent as these models are trained on vast amounts of data, often some of which is copyrighted. Consequently, the resulting models might contain content from copyrighted data in their training sets. Previous work suggests it may be mathematically challenging to capture algorithms that protect copyright. Specifically, attempts to regulate copyright often focus on protecting against _substantial similarity_ between output content and training data by, for example, employing stable algorithms that are not sensitive to individual training points [9; 22; 24]. This is an important aspect of copyright; however, substantial similarity is only one piece of the puzzle.

Another piece of the puzzle involves the use of original elements from copyrighted works in a legally permissible manner, such as through _de minimis quotations_, transformative use, and other types of _fair uses_, such as learning and research . To fully utilize ML in many practical scenarios, it is desirable for learning models to be allowed to use original elements in a similar manner.

To address this second piece, we focus on designing algorithms that, while allowed to use and be influenced by copyrighted material, must provide proper attribution. Such models would enable users to inspect these influences and verify that they conform to legal standards, or take necessary measures (such as monetary compensation or requesting permission). Despite credit attribution being narrower in scope than copyright protection in general, even this concept may be nuanced to be captured mathematically. Therefore, we focus on formalizing a specific (but arguably basic) aspect of it - _counterfactual attribution_:

This principle asserts that any previous work that influenced the result should be credited. Counterfactually, if the creator of a work \(W\) does not acknowledge another work \(W^{}\), they should be able to produce \(W\) as if they had no knowledge of \(W^{}\).

For example, an argument based on this principle in the extreme case when \(W=W^{}\) is found in a U.S. Supreme Court opinion:

_"...a work may be original even though it closely resembles other works, so long as the similarity is fortuitous, not the result of copying. To illustrate, assume that two poets, each ignorant of the other, compose identical poems. Neither work is novel, yet both are original..."_

_-- Feist Publications, Inc. v. Rural Telephone Service Company, Inc. 499 U.S. 340 (1991)_

## 2 Definitions and Examples

In this section, we introduce the two main definitions we study.

We first recall some standard notation from learning theory and differential privacy. Let \(\) be an input data domain and \(\) denote an output space. We denote by \(^{}\) the set of all finite sequences with elements from \(\). Two sequences \(S^{},S^{}^{}\) are called neighbors if they have the same length \(|S^{}|=|S^{}|\) and there is a unique index \(i\) such that \(S^{}_{i} S^{}_{i}\). Let \(,>0\) and let \(p,q\) be probability distributions defined over the same space. We let \(p_{,}q\) denote the following relation: \(p(E)() q(E)+\) and \(q(E)() p(E)+\) for every event \(E\).

Algorithms with Credit Attribution.Consider a mechanism \(M:^{}^{}\) that, for every possible input sequence \(S=(z_{1},,z_{n})\), outputs a pair \((c,R)\), where \(c\) and \(R^{}\). Intuitively, \(R\) is the list of inputs being credited by the mechanism, and \(c\) is the model/content produced by the mechanism. Thus, we require that each data point \(z_{i} R\) is also an input data point \(z_{i} S\). For such a mechanism \(M\) and an input sequence \(S\), we let \(M(S)\) denote the probability distribution over outputs of \(M\) given \(S\) as input, where the probability is induced by the internal randomness of the mechanism. For example, if \(M\) is deterministic, then \(M(S)\) is a Dirac distribution (i.e., it assigns probability 1 to the deterministic output of \(M\) on \(S\)).

The definition below uses the following notation: for a sequence \(S=(z_{1},,z_{n})\) and an index \(i[n]\), we let \(S_{-i}\) denote the subsequence of \(S\) obtained by omitting \(z_{i}\). Let \(z_{i} S\) be a data point such that \(_{(c,R) M(S)}[z_{i} R]<1\). That is, there is a positive probability that \(z_{i}\) is not creditedby \(M\) when executed on \(S\). In this case, we let \(M(S^{-i})\) denote the distribution of \(M(S)\) conditioned on the event that \(z_{i} R\). We are now ready to present our first definition5 of counterfactual credit attribution.

**Definition 1** (Counterfactual Credit Attribution).: _Let \(,>0\). A mechanism \(M:^{}^{}\) is called an \((,)\)-counterfactual credit attributor (CCA) if for every input sequence \(S=(z_{1},,z_{n})\) and every index \(i[n]\) the following holds: either \(_{(,R) M(S)}[z_{i} R]=1\), or_

\[M(S^{-i})_{,}M(S_{-i}),\]

_where \(M(S_{-i})\) is the output distribution on the dataset \(S_{-i}=S\{z_{i}\}\), and \(M(S^{-i})\) is the output distribution on the dataset \(S\), conditioned on \(z_{i} R\)._

To emphasize, in Definition1 the conditional output distribution \(M(S^{-i})\) models the condition "_if data-point \(z_{i}\) is not credited by \(M\)_," whereas the output distribution \(M(S_{-i})\) represents the counterfactual scenario "_had the data-point \(z_{i}\) not been seen by \(M\)_."

**Example 2.1** (Stable Sample Compression ).: A mechanism \(A:^{}\) is a stable sample compression scheme of size \(k\) if for every input sequence \(S=(z_{1},,z_{n})\) there is a subsequence \((S) S\) of size \(|(S)| k\) such that \(A(S)=A(T)\) for every intermediate subsequence \((S) T S\). See Figure1 for an example.

Each stable compression scheme corresponds to an \((=0,=0)\)-CCA which credits the datapoints in \((S)\). That is, \(M(S)=(A(S),(S))\). Stable sample compression thus provides something stronger: group-counterfactuality, meaning any subset of datapoints that is not selected does not influence the output.

Definition1 not only relaxes stable sample compression, but also extends the concept of differential privacy with public data, known as semi-private learning. In semi-private learning, the learner's input includes public examples (which can be processed non-stably) and private examples (for which the algorithm must satisfy differential privacy guarantees). Semi-private learning  has been extensively studied in recent years , for example, in the context of query release , distribution learning , computational efficiency , as well as in other contexts.

**Definition 2** (Semi-Differentially Private Mechanism).: _Let \(,>0\); an \((,)\)-semi differentially private (semi-DP) mechanism is a mapping \(M:^{}^{}\) such that for every \(S_{}^{}\) and every pair of neighboring sequences \(,S^{}_{},S^{}_{}\).6_

\[M(S_{},S^{}_{})_{,}M(S _{},S^{}_{}).\]

Figure 1: Support Vector Machine (SVM) as an \((==0)\)-counterfactual credit attributor: The SVM algorithm identifies a maximum-margin separating hyperplane, which is defined by the subsample of the support vectors. Any input point which is not a support vector does not influence the output: even if it is removed from the input sample, the output hyperplane does not change.

**Remark 1**.: Any semi-DP mechanism \(M\) that uses \(k\) public points can be turned into a CCA mechanism as follows: on an input sequence \(S\), the CCA mechanism outputs \((c,R)\), where \(R=S_{ k}\), and \(c=M(S_{ k},S_{>k})\). That is, \(M\) uses the first \(k\) points in \(S\) as public data, and the rest are private.

Private learning with public data is sometimes likened to semi-supervised learning, where private data corresponds to unlabeled data and public data to labeled data. In both scenarios, the learner accesses many less informative examples (unlabeled or private) and fewer more informative examples (labeled or public). Expanding on this analogy, Definition1 is akin to active learning, where the learner adaptively chooses which data points to credit, similar to selecting which data points to label in active learning.

Semi-differential privacy (Definition2) provides stronger stability guarantees than counterfactual credit attribution (Definition1), including for the selection process. In contrast, Definition1 allows for a highly non-stable selection process (e.g., SVM). This leads us to consider a more direct hybrid of semi-DP and sample compression, suggesting the following definition:

**Definition 3** (Sample DP-Compression Scheme).: _Let \(, 0\) and \(k n\). An \((,)\) sample differentially private \((n k)\)-compression scheme is a mechanism \(M:^{n}\) which consists of two functions:_

1. _[noitemsep]_
2. _Compression: an_ \((,)\)_-DP mechanism_ \(:^{n}[n]^{k}\)_, called the compression function, and_
3. _Reconstruction: an_ \((,)\) _semi-DP mechanism_ \(:^{}^{}\) _called the reconstruction function._

_Then, for every input sequence \(S\):_

\[M(S)=(S|_{(S)},S|_{(S)}),\]

_where \(S|_{(S)}=(S_{i})_{i(S)}\) and \(S|_{(S)}=(S_{i})_{i(S)}\)._

Note that the compression function \(\) selects the indices of the compressed subsample (rather than the subsample itself, as in classical sample compression). This technical difference allows us to pose the requirement of differential privacy on the compression function \(\). Going back to the analogy with active learning, Definition2 also imposes stability of the labeling function (i.e. the function that decides which labels to query).

**Example 2.2** (Randomized Response).: We next describe a simple task which can be performed by sample DP-compression schemes, but not by semi-DP mechanisms. Imagine that the data is drawn from a distribution where each datapoint is useful with probability 0.1 and is otherwise garbage with probability 0.9. The goal is to select \(k\) datapoints while maximizing the number of useful datapoints that are selected. If we select datapoints obliviously, for example by simply taking the first \(k\) examples, we would expect that only about 10% of them will be useful. However, by using a mechanism compliant with Definition3, we can increase the proportion of useful examples.

This mechanism is based on randomized response and operates as follows: each example is independently assigned a random label in \(\{0,1\}\), where a useful example is assigned a label of 1 with probability \(p>1/2\), and each garbage example is assigned a label of 1 with probability \(1-p<1/2\). The value of \(p\) is set as a function of the privacy parameter \(\).7 Then, the compression function \(\) selects the first \(k\) indices whose label is 1. This way, the fraction of useful points among the points labeled 1 is \(=>0.1\) (the last inequality holds for \(p>1/2\)). See AppendixB for a more detailed argument.

## 3 Main Theorems

In this section, we present our main theorems that characterize the expressivity of learning rules satisfying our proposed definitions. We focus on the PAC (Probably Approximately Correct) learning model  and employ its standard definitions (explicitly provided in Section4).

**Question** (Guiding Question).: Is learnability subject to counterfactual credit attribution (Definition 1) more restricted than unconstrained learnability? Is learnability subject to sample DP-compression (Definition 3) more restricted than unconstrained learnability? How do these restrictions compare to differentially private learning?

Note that with respect to both Definition 1 and Definition 3, it is clear that if \(k\), the number of credited points, is sufficiently large, then it is possible to learn any PAC-learnable class \(\). Indeed, if \(k\) equals the PAC sample complexity of \(\), then an oblivious selection, such as the first \(k\) points, will suffice. Therefore, the above question is particularly interesting for values of \(k\) that are significantly smaller than the PAC sample complexity of \(\).

Our first theorem demonstrates that every PAC-learnable class can be learned using an \((=0,=0)\)-counterfactual credit attribution learning rule, which selects at most a logarithmic number of sample points for attribution. Remarkably, this can be achieved using the AdaBoost algorithm.

**Theorem 1** (PAC Learning with Credit Attribution = PAC Learning).: _Let \(\) be a concept class with VC dimension \(()=d<\), and let \(,\) denote the error and confidence parameters. Then, there exists an \((=0,=0)\)-CCA learning rule \(M\) that learns \(\) with sample complexity \(n=O()\), while selecting only \(k=O(d n)\) examples for attribution._

We leave as an open question whether \(k\) can be made independent of \(n\), possibly by allowing \(\) and \(\) to be positive. Note that an affirmative answer to this question might also shed light on the sample compression conjecture .

Our second theorem establishes a limitation for sample DP-compression schemes, showing that they do not offer more expressivity than differentially private PAC learning .

**Theorem 2** (Sublinear Sample DP-Compression = DP Learning).: _Every concept class \(\) satisfies exactly one of the following:_

1. \(\) _is learnable by a DP-learner._
2. _Any sample DP-compression scheme that learns_ \(\) _has size at least_ \(k=(1/)\)_._

Theorem 2 implies a stark dichotomy: either a class \(\) can be learned by a DP algorithm (equivalently, a sample DP-compression of size \(k=0\)), or it is impossible to learn it unless \(k=(1/)\). Notice that with \(k=O(d/)\), public examples are sufficient to learn without any private examples. Theorem 2 generalizes a result by [1, Theorem 4.2] who proved it in the special case of semi-DP learning. In our setting though, we need to crucially handle scenarios where the credited (or rather, public) datapoints are chosen _adaptively_ as a function of the full dataset. This is not the case in semi-DP learning, and requires us to use novel technical tools (like Theorem 3 ahead).

Thus, in the PAC setting, sample DP-compression schemes do not offer any advantage over semi-DP learners. However, Example 2 demonstrates that using sample DP-compression, it is possible to select the \(k\) points in the compression set so that the frequency of 'useful' examples among these \(k\) points is boosted.

Our next theorem addresses the limits of handpicking \(k\) points by sample DP-compression. We formalize this task as follows: given a distribution \(D\) over \(\) and an event \(E\) of 'good' points, the goal is to design a DP-compression function \(:^{n}[n]^{k}\) that maximizes the number of selected data points that belong to \(E\). That is, the goal is to maximize

\[_{i(S)}1[z_{i} E].\]

Example 2 illustrates a method that selects roughly \(() k D(E)\) points from \(E\) by an \(( 0,=0)\)-compression function. This is a factor of \(()\) better than obliviously selecting the \(k\) points, which yields \(k D(E)\) points from \(E\). Is this factor of \(()\) optimal? Can one do better, possibly by increasing \(\)? The following result shows that \(()\) is asymptotically optimal.

**Theorem 3**.: _Let \(M\) be an \((,)\) sample DP-compression scheme, let \(\) be a distribution over \(\), and let \(E\) be any event, with \(p=(E)\). For an input sample \(S=(z_{1},,z_{n})^{n}\), define \(Z=Z(S)\) as the random variable denoting the fraction of selected indices in \((S)\) whose corresponding data points belong to \(E\). That is, \(Z=_{i(S)}1[z_{i} E]\). Then,_

\[pe^{-}- n E[Z] pe^{}+ n.\] (1)

Indeed, since by convention \(=(n) 1/n\), the above theorem implies that \(()\) is asymptotically optimal. We note that Theorem3 is also key in the proof of Theorem2. We elaborate on this in Section4.2.

Generalization.Definition1 and Definition3 can also be examined from a learning theoretic perspective as notions of algorithmic stability. Algorithmic stability is particularly useful in the context of generalization because, roughly speaking, stable algorithms typically generalize well. We note in passing that this is indeed the case for Definition1 and Definition3: any learning rule adhering to either definition satisfies that its empirical error and population error are typically close. One natural way to prove this is by following the argument that shows sample compression schemes generalize. In a nutshell, the argument proceeds as follows: first, if we fix the selected \(k\)-tuple, the obtained hypothesis generalizes well. Then, we apply a union bound over all possible \(n^{k}\) choices of \(k\)-tuples from the input sample.

## 4 Technical Background and Proofs

We study our main definitions in the context of PAC learning. Concretely, we assume that the input data domain \(\) in Section2 is \(\), for an input space \(\) and label space \(\). For our purposes, \(=\{0,1\}\). Learning rules are mechanisms \(:^{*}^{*}\), where \(\) is the set of all functions mapping \(\) to \(\), denoted as \(^{}\). We say that a distribution \(\) over \(\) is _realizable_ by a hypothesis class \(^{}\) if for every finite sequence \((x_{1},y_{1}),,(x_{n},y_{n})\) drawn i.i.d from \(\), there exists some hypothesis \(h\) that satisfies \(h(x_{i})=y_{i},\  i[n]\). For any hypothesis \(h^{}\), we denote its risk with respect to a distribution \(\) by \(R_{}(h)=_{(x,y)}[h(x) y]\).

**Definition 4** (CCA PAC learning rule).: _A mechanism \(\) is a CCA PAC learning rule for a hypothesis class \(\), if \(\) satisfies Definition1, and for any distribution \(\) realizable by \(\), for any \(\), \(>0\), there exists a finite \(n=n_{}(,)\), such that with probability at least \(1-\) over a sample \(S^{n}\) and the randomness of \(\), the hypothesis \(h\) in the output \((h,S^{})\) of \(\) on \(S\) satisfies \(R_{}(h)\)._

**Definition 5** (Sample DP-Compression learning rule).: _An \((,)\) sample differentially private \((n k)\) compression scheme \(M\) learns a hypothesis class \(^{}\), if for any distribution \(\) realizable by \(\), for any \(\), \(>0\), with probability at least \(1-\) over a sample \(S^{n}\) and the randomness of \(M\), the hypothesis \(M(S)\) output by the reconstruction function in \(M\) satisfies \(R_{}(M(S))\)._

**Remark 2**.: Note that if \(k=0\) above, we recover the standard definition of an \((,,,)\)-DP PAC learner (where \(\) is the error, \(\) is the failure probability, and \(,\) are the privacy parameters) .

Upper Bound: PAC learnability implies \((==0)\)-counterfactual credit attribution learning

Our CCA learning rule crucially uses the notion of a _randomized_ stable sample compression scheme, which is a generalization of stable sample compression schemes (Example2.1) and was developed in a recent work by . We use the notation \(S^{} S\) for sequences \(S,S^{}()^{*}\) that satisfy: \(((x,y)):(x,y) S^{}(x,y) S\).

**Definition 6** (Stable Randomized Sample Compression Scheme).: _A randomized sample compression scheme \((_{},)\) for a class \(\) having failure probability \(\) comprises of a distribution \(_{}\) over (deterministic) compression functions \(:()^{*}()^{*}\) and a deterministic reconstruction function8\(:()^{*}^{}\). The compression functions \(\) in the support of \(_{}\) must satisfy_

* _For any_ \(S()^{*}\) _realizable by_ \(\)_, if_ \((S)=S^{}\)_, then_ \(S^{} S\)_._

_The reconstruction function \(\) must satisfy_

* _For any_ \(S()^{*}\) _realizable by_ \(\)_,_

\[_{_{}}[(x,y)  S:((S))(x) y].\] (2)

_A randomized sample compression scheme \((_{},)\) for \(\) is stable if for any \(S()^{*}\) realizable by \(\) and \(S^{} S\), the distribution of \((S^{})\) is the same as the distribution of \((S)\) conditioned on \((S) S^{}\). The size \(s(n)\) of the compression scheme is the supremum over \(S()^{n}\) (realizable by \(\)) and \(\) in the support of \(_{}\) of the number of distinct elements in \((S)\)._

 show that stable randomized compression schemes imply generalization.

**Lemma 4.1** (Theorem 1.2 in ).: _Let \((_{},)\) be a stable randomized compression scheme for \(\) of size \(s(n)\) and failure probability \(\). Let \(\) be any distribution over \(\) realizable by \(\). For any \(n\) and \(>2\), with probability at least \(1-\) over \(S^{n}\) and \(_{}\), it holds that_

\[R_{}(((S))) O().\]

Furthermore, they also show that there exists a stable randomized compression scheme for any hypothesis class \(\) having finite VC dimension \(d\). This compression scheme is based on a simple variant of AdaBoost (Algorithm 1 in ). The following is contained in their work:9

**Lemma 4.2** ().: _For any hypothesis class \(\) with VC dimension \(d\), there exists a stable randomized sample compression scheme (based on AdaBoost) having failure probability \(\) of size_

\[s(n)=O(d(n/)).\] (3)

We are now equipped with the necessary tools required to prove Theorem 1.

Proof of Theorem 1.: Let \(\) be any distribution realizable by \(\), and let \(S\) be a sample of size \(n\) drawn from \(^{n}\). Given \(\), fix \(=/3\). From Lemma 4.2, we know that there exists a stable randomized compression scheme \((_{},)\) for \(\) of size \(s(n)=O(d(n/))\), and failure probability \(\). Then, since \(>2\), from Lemma 4.1, we know that with probability at least \(1-\) over \(S\) and \(_{}\),

\[R_{}(((S))) O().\]

For the right-hand size above to be at most \(\), it suffices to have \(n=O()\).

Let \(\) be the learning rule, which when given a sample \(S^{n}\) as input, runs the stable randomized compression scheme from above on \(S\) to obtain \(S^{}\) of size \(k=O(d(n/))\). The learner then outputs \(((S^{}),S^{})\). By the reasoning above, \((S^{})\) has error at most \(\) with probability at least \(1-\).

It remains to argue that \(\) is a valid CCA mechanism. This follows by virtue of \((_{},)\) being a _stable_ randomized compression scheme. Namely, for any \(i\), \(S_{-i} S\), and hence by Definition 6, the distribution of \((S_{-i})\) is identical to the distribution of \((S)\) conditioned on \(S_{i}(S)\). Finally, since \(\) is a deterministic function of its argument, \(\) satisfies Definition 1 with \(==0\). 

### Lower Bound: A dichotomy for sample DP-compression

Towards proving Theorem 2, we first show that a sample DP-compression scheme for the class of _thresholds_ can be used to construct a DP learner for it. This lemma has a similar flavor to the public data reduction lemma (Lemma 4.4) in . For a set \(S=\{x_{1},,x_{m}\}\), the class of thresholds over \(S\) comprises of \(m\) functions \(h_{1},,h_{m}\) such that \(h_{i}(x_{j})=[i j],\  i,j[m]\).

**Lemma 4.3** (Reduction from DP learner to sample DP-compression scheme).: _Let \(_{m}\) be the class of thresholds over \(\{x_{1},,x_{m}\}\). Suppose there exists an \((,)\) sample DP-compression scheme \(\) that learns \(_{m}\) with error \(\) and failure probability \(=\), and has sample complexity \(n\) and compression size \(k n\). Let \(}\). Then, there exists a \((64k^{},,2,3)\)-DP learner \(\) for \(_{m-1}\), where \(_{m-1}\) is the class of thresholds over \(\{x_{1},,x_{m-1}\}\), with sample complexity \(n\)._

Proof.: Let \(\) be any distribution over \(\{x_{1},,x_{m-1}\}\{0,1\}\) realizable by \(_{m-1}\). Given a sample \(S^{n}\), the private learner \(\) does the following. First, it constructs a sample \(\), also of size \(n\), as follows. Initialize \(j=1\). For each \(i=1,2,,n\), toss a coin (independently of the data, and other coins) that lands heads with probability \(p\), for \(p\) to be appropriately chosen later. If the coin lands heads, \((i)=S(j)\), and \(j\) is incremented by \(1\). If the coin lands tails, \((i)\) is set to the designated dummy example \((x_{m},1)\). In this way, \(\) is a sample of size \(n\) drawn from the _mixture_ distribution \(=p+(1-p)_{(x_{m},1)}\), where \(_{(x_{m},1)}\) is a point mass on \((x_{m},1)\). Note that since all the thresholds in \(_{m}\) label \(x_{m}\) as \(1\), \(}\) is realizable by \(_{m}\).

The learner \(\) now invokes the sample DP-compression scheme \(}\) on \(\). If \(any\) of the \(k\) examples in the compression set constructed by \(\) is a non-dummy element, \(\) outputs a constant hypothesis that labels all points in \(\{x_{1},,x_{m-1}\}\) as \(1\). On the other hand, if all of the \(k\) examples in the compression set are dummies, then \(\) outputs the hypothesis that \(\) outputs (restricted to \(\{x_{1},,x_{m-1}\}\)).

We first claim that the output of \(\) is \((2,3)\)-private with respect to its input \(S\).

**Claim 4.4** (\(\) is private).: \(\) _is \((2,3)\)-DP._

The proof of this claim is given in Appendix A. At a high level, the privacy parameter deteriorates to \(2\) because of the two-step process of compressing \(\) to \(k\) points in an \(\)-DP way, and then obtaining an \(\)-DP learner thereafter.

Next, we claim that on average, there will be a lot of dummies in the compression set selected by \(\). This step crucially hinges on Theorem 3, where we substitute the event \(E\) in the statement of the theorem to be the event that a non-dummy element is selected (i.e., \(E\) is the support of the distribution \(\)). In particular, we get that the expected number of non-dummy elements is at most \(k\,pe^{}+ kn\), which is at most \(\), if we set \(p=}\), and use that \(k n,}\).

We can now reason about the error and failure probability parameters of \(\). Because \(\) is an \((,)\) sample DP-compression scheme that successfully learns \(_{m}\) with error \(\) and failure probability \(\), with probability at least \(1-\) over the draw of \(\) and the randomness of \(}\), the hypothesis it outputs has error at most \(\). Furthermore, since the expected number of non-dummy elements chosen in the compression set is at most \(\), Markov's inequality gives that with probability at least \(1-\) over the draw of \(\) and the randomness of \(}\), all the \(k\) examples chosen by \(\) in the compression set are dummies. By a union bound, with probability at least \(1-\) over the draw of \(\) and the randomness of \(}\), all the examples chosen to be in the compression set by \(\) are dummies _and_ the hypothesis it outputs has error (with respect to \(}\)) less than \(\).

But recall that the distribution \(}\) on \(\) is induced by the distribution \(\) on \(S\), and that whenever all the examples chosen by \(\) in the compression set are dummies, \(\) returns \(\)'s output. This implies that with probability at least \(1-\) over the draw of \(S\) from \(^{n}\) and the randomness of \(\), the hypothesis output by \(\) has error at most \(\) with respect to \(}\). But since \(}\) is a mixture distribution,

\[R_{}}((S)) p_{}( (S)),\]

and hence we have that with probability at least \(1-\), the error of \((S)\) with respect to \(\) is at most \( 64k^{}\). Thus, \(\) is a \((64k^{},,2,3)\)-DP learner for \(_{m-1}\) as required. 

We next state a lower bound on the sample complexity of DP learners for thresholds .

**Theorem 4** (Theorem 1 in ).: _Let \(_{m}\) be the class of thresholds on \(\{x_{1},,x_{m}\}\). Let \(\) be a \((,,0.1, n})\)-DP learner for \(_{m}\) with sample complexity \(n\). Then \(n(^{*}m)\)._We are now ready prove Theorem5, which shows that non-Littlestone  classes cannot be learnt by sublinear sample DP-compression schemes. Theorem2 follows from Theorem5, since classes that are DP-learnable are exactly the classes with finite Littlestone dimension .

**Theorem 5**.: _Let \(\) be a hypothesis class over \(\) that has infinite Littlestone dimension. For \(=0.05,= n}\), let \(}\) be an \((,)\) sample differentially private \((n k)\) compression scheme that learns \(\) with error \(\) and failure probability \(\). Then \(k\)._

Proof.: Because \(\) has infinite Littlestone dimension, for any \(m 1\), there exist \(\{x_{1},,x_{m}\}\) and \(_{m}\) such that \(_{m}\) is the class of thresholds over \(\{x_{1},,x_{m}\}\)[2, Theorem 3]. Now, \(}\) is an \((n k)\) sample DP-compression scheme that learns \(\); in particular, this means that \(}\) has sample complexity \(n<\), and also that \(}\) learns \(_{m}\) with the same parameters and sample complexity. By Lemma4.3, we know that there then exists a \((68k,,0.1, n})\) private learner for \(_{m}\) with sample complexity \(n\). Assume for the sake of contradiction that \(k<\). This means that there exists a \((,,0.1, n})\) private learner for \(_{m}\) with sample complexity \(n\). By Theorem4, it must be that \(n(^{*}m)\). Since we can find \(_{m}\) for any \(m 1\), this would mean that \(n=\), which is a contradiction. Thus, it must be the case that \(k\). 

### Bounded boosting of empirical measure

We prove a simplified form of Theorem3 (with slightly tighter bounds), where we consider the input to be a bit string. Theorem3 as stated in terms of a general event can be immediately obtained as a corolmcy by interpreting the bits in the string as indicators for the event (details in AppendixA).

**Lemma 4.5** (Bounded Boosting of Empirical Measure).: _Let \(:\{0,1\}^{n}[n]^{k}\) be an \((,)\)-DP selection mechanism. Let \(\) be the product distribution on \(\{0,1\}^{n}\) where each bit is set to \(1\) with probability \(p\). For \(X\), let \(Z\) denote the fraction of indices in \((X)\) at which \(X\) is 1, i.e., \(Z=_{j(X)}[X_{j}=1]\). Then, we have that_

\[}[Z]+np(1-p)}{1-p+pe^{}}.\] (4)

Proof Sketch.: Let \((X)=I=(I_{1},I_{2},,I_{k})\) be the tuple of indices selected by the DP mechanism on input \(X\). We first write \(Z=_{j=1}^{k}_{i=1}^{n}[I_{j}=i X_{i}=1]\). Thereafter, the main step of the proof uses that the mechanism is private in order to relate the _conditional_ probability \([I_{j}=i|X_{i}=1]\) to \([I_{j}=i|X_{i}=0]\) for any \(j[k]\). Concretely, observe that

\[[I_{j}=i|X_{i}=0]==0 I_{j}=i]}{[X_{i}= 0]}=^{n},x_{i}=0}[x][I_{j}=i|x]}{1-p}\] \[=^{n},x_{i}=1}[x][I_{j}=i|x][x] (e^{}[I_{j}=i|x]+)}{p}\] \[=_{x\{0,1\}^{n},x_{i}=1}[x][I_{ j}=i|x]}{p}+=e^{}[I_{j}=i|X_{i}=1]+,\]

where in the fourth inequality, we used that for \(x\) having \(x_{i}=1\), \(_{}[x^{ i}]=[x]\), and that \(\) is an \((,)\)-DP mechanism. This relation lets us express the joint probability term \([I_{j}=i X_{i}=1]\) in the expression \([Z]=_{j=1}^{k}_{i=1}^{n}[I_{j}=i X_{i}=1]\) simply in terms of \([I_{j}=i]\). Thereafter, noticing that \(_{i=1}^{n}[I_{j}=i]=1\) yields the result. The complete details are provided in AppendixA. 

## 5 Conclusion

We study two natural definitions for algorithms satisfying credit attribution. In the context of PAC learning, we provide a characterization of learnability for algorithms that respect these definitions. Our work motivates the further study of these and other related definitions for credit attribution,and opens up interesting technical directions to pursue. However, as mentioned earlier, credit attribution is only part of the much more nuanced problem of copyright protection, and hence, our definitions only capture subtleties involved in the problem in part. With further exploration, and other suitable definitions, we will hopefully be able to ensure that algorithms (especially generative models) appropriately credit the work that they draw upon.

AcknowledgementsShay Moran is a Robert J. Shillman Fellow; he acknowledges support by ISF grant 1225/20, by BSF grant 2018385, by an Azrieli Faculty Fellowship, by Israel PBC-VATAT, by the Technion Center for Machine Learning and Intelligent Systems (MLIS), and by the the European Union (ERC, GENERALIZATION, 101039692). Roi Livni is supported by an ERC grant (FOG, 101116258), as well as an ISF Grant (2188 \(\) 20). Chirag Pabbaraju is supported by Moses Charikar and Gregory Valiant's Simons Investigator Awards. Work of Kobbi Nissim was supported by NSF Grant No. CCF2217678 "DASS: Co-design of law and computer science for privacy in sociotechnical software systems" and a gift to Georgetown University.

Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.