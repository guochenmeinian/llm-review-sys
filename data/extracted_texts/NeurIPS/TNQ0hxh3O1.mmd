# Open-Vocabulary Object Detection via Language Hierarchy

Jiaxing Huang, Jingyi Zhang, Kai Jiang, Shijian Lu

College of Computing and Data Science

Nanyang Technological University, Singapore

Corresponding author

###### Abstract

Recent studies on generalizable object detection have attracted increasing attention with additional weak supervision from large-scale datasets with image-level labels. However, weakly-supervised detection learning often suffers from image-to-box label mismatch, i.e., image-level labels do not convey precise object information. We design Language Hierarchical Self-training (LHST) that introduces language hierarchy into weakly-supervised detector training for learning more generalizable detectors. LHST expands the image-level labels with language hierarchy and enables co-regularization between the expanded labels and self-training. Specifically, the expanded labels regularize self-training by providing richer supervision and mitigating the image-to-box label mismatch, while self-training allows assessing and selecting the expanded labels according to the predicted reliability. In addition, we design language hierarchical prompt generation that introduces language hierarchy into prompt generation which helps bridge the vocabulary gaps between training and testing. Extensive experiments show that the proposed techniques achieve superior generalization performance consistently across 14 widely studied object detection datasets.

## 1 Introduction

Object detection aims to locate and identify objects in images by providing basic visual information of "where and what objects are". Thanks to the recent advances of deep neural networks, it has achieved great success with various applications in autonomous driving , intelligent surveillance , wildlife tracking , etc. However, learning a generalizable object detector for various downstream tasks that have different data distributions and data vocabularies remains an open research challenge. To this end, weakly-supervised object detection (WSOD) , which allows access of large-scale image-level datasets (e.g., ImageNet-21K  with 14M images of 21K classes) with super rich data distributions and data vocabularies, has reignited new research interest under the context of learning generalizable detectors.

While exploiting WSOD to learn generalizable detectors, one typical challenge is that the provided image-level labels do not convey precise object information  and often mismatch with box-level labels. Recent methods address this challenge by designing various label-to-box assignment strategies that assign the image-level labels to the predicted top-score  or max-size  object proposals. However, the mismatch problem remains due to the restriction of the raw image-level labels . At the other end, self-training  with the detectors pre-trained with  can generate box-level pseudo labels without the restriction of image-level labels. It allows learning from more object proposals without the image-to-box label mismatch issue, but it does not benefit much from the provided image-level label supervision.

We propose to incorporate image-level supervision with self-training for learning generalizable detectors, aiming to benefit from self-training while effectively making use of image-level weak supervision. We start from a simple observation: the image-to-box label mismatch largely comes from the ambiguity in language hierarchy, e.g., the image-level label _Aquatic Mammal_ in Figure 1 can cover different object-level labels such as seals, dolphins, walruses, etc. With the above observations, we design a **D**etector with **L**anguage **H**ierarchy (DetLH) that combines language hierarchical self-training (LHST) and language hierarchical prompt generation (LHPG) for learning generalizable detectors.

LHST introduces WordNet's language hierarchy  to expand the image-level labels and accordingly enables co-regularization between the expanded labels and self-training. Specifically, the expanded labels are not all reliable though they can mitigate the image-to-box label mismatch problem by providing richer supervision. Here self-training can predict reliability scores for the expanded labels for better selection or weightage of the expanded labels. At the other end, self-training with pseudo box labels allows learning from more proposals and can circumvent the image-to-box label mismatch, but the box-level pseudo labels are usually noisy and may lead to learning degradation . Here the expanded labels provide richer and more flexible supervision which can effectively help suppress prediction noises in self-training.

LHPG helps bridge the vocabulary gaps between training and testing by introducing WordNet's language hierarchy into prompt generation process. Specifically, LHPG leverages the CLIP language encoder  to measure the embedding distances between test concepts and WordNet synsets, and then generates the prompt for a given test concept from its best matched WordNet synset. In this way, the test prompts generated by LHPG have been standardized by WordNet and are well aligned with our proposed detector that is trained with WordNet information via LHST. In another word, the combination of LHST and LHPG actually leverages WordNet as a standard and intermediate vocabulary that bridges the gaps between training and testing vocabularies, generating better prompts and leading to better detection performance on downstream applications.

The main contributions of this work are threefold. _First_, we propose language hierarchical self-training that incorporates language hierarchy with self-training for weakly-supervised object detection. _Second_, we design language hierarchical prompt generation, which introduces language hierarchy into prompt generation to bridge the vocabulary gaps between detector training and testing. _Third_, extensive experiments show that our DetLH achieves superior generalization performance consistently across 14 detection benchmarks.

## 2 Related Work

**Weakly-supervised object detection (WSOD)** aims to train object detectors using image-level supervision. Traditional WSOD methods [23; 24; 25; 26; 27] use image-level annotations only without any box annotations and thus focus on low-level proposal mining techniques [28; 29; 12; 30; 31; 32], leading to unsatisfying localization performance. **Semi-supervised WSOD**[33; 34; 35; 36; 37; 38; 39]

Figure 1: Image-level labels in large-scale datasets such as ImageNet-21k  often do not convey precise object information [17; 15] which affects while learning generalizable detectors. Recent methods tackle this issue by various label-to-box assignment strategies [12; 13; 14; 15] as in (a) but are heavily restricted by raw image-level labels and still suffer from image-to-box label mismatch . Self-training  with the detectors pre-trained with [13; 14; 15] could circumvent the label mismatch issue but the generated pseudo box labels are error-prone due to the lack of proper supervision as in (b). Our proposed LHST introduces language hierarchy to expand the image-level labels and enables co-regularization between the expanded labels and self-training which allows producing more accurate pseudo box labels in (c).

has been proposed to further improve the performance, which leverages both box-level and image-level annotated data. With better localization quality, recent methods [13; 14; 15; 40] design various label-to-box assignment strategies, such as assigning image-level labels to max-score anchors , max-score proposals  or max-size proposals . Our work belongs to semi-supervised WSOD. Different from previous methods, we tackle the image-to-box label mismatch by introducing language hierarchy into self-training.

**Large-vocabulary object detection**[41; 13; 42; 43; 44] researches on detecting thousands of categories. Most previous papers focus on tackling the long-tail issue [45; 46; 47; 48; 49; 50], e.g., by using equalization losses [51; 52], SeeSaw loss , or Federated Loss . Recent semi-supervised WSOD methods [13; 14; 15] and our work circumvent the long-tail problem by leveraging more balanced image-level datasets such as ImageNet-21K.

**Open-vocabulary object detection** focuses on detecting objects conditioned on arbitrary words (i.e., any category names). A common strategy [55; 56; 57; 58; 59] is to replace the detector's classification layer with the language embeddings of category names. Recent methods [60; 61; 62; 63; 17; 15] leverage the powerful CLIP  model by using its text embeddings [60; 61; 62; 63; 17; 15] or conducting knowledge distillation [60; 63; 17]. Similar to Detic , our work uses CLIP text embeddings as the classifier and leverages image-level annotated data instead of distilling knowledge from CLIP.

**Language hierarchy** has been widely studied for visual recognition tasks , especially for large-vocabulary visual recognition. Most existing studies [65; 66; 67] focus on image classification tasks, e.g., leveraging language hierarchy for multi-label image classification [65; 66; 67; 68; 69; 70; 71], modelling hierarchical relations among classes [68; 69] or facilitating classification training [70; 71]. Different from previous work, we introduce language hierarchy into self-training for weakly-supervised object detection.

## 3 Method

This work focuses on learning generalizable object detectors via weakly-supervised detector training , which leverages additional large-scale image-level datasets to enlarge the data distributions and data vocabularies in detector training. We first describe the task definition with training and evaluation setups. Then, we present our proposed DetLH which is detailed in two major aspects on Language Hierarchical Self-training (LHST) that introduces language hierarchy into detector training, and Language Hierarchical Prompt Generation (LHPG) that introduces language hierarchy into prompt generation.

### Task Definition

**Training setup.** The training data consists of two parts: 1) a detection dataset \(_{det}=\{(x,y_{det})_{i}\}_{i=1}^{|D_{det}|}\), where \(x\) denotes an image while \(y_{det}\) stands for the class and bounding box labels for \(x\); 2) an image classification dataset \(_{cls}=\{(x,y_{cls})_{i}\}_{i=1}^{|D_{cls}|}\) where \(y_{cls}\) denotes the image-level label (i.e., a one-hot vector) for \(x\). Given the two datasets, the goal is to learn a generalizable detection model \(F\) by jointly optimizing \(F\) over \(_{det}\) and \(_{cls}\):

\[Loss=_{(x,y_{det})_{det}}_{det}(F(x),y_{det})+ _{(x,y_{cls})_{cls}}_{weak}(F(x),y_{cls}),\] (1)

where \(_{det}()=_{rpn}()+_{reg}()+ _{cls}()\) is the fully-supervised detection loss function while \(_{rpn}()\), \(_{reg}()\), and \(_{cls}()\) denote RPN, Regression, and Classification loss functions, respectively. \(_{weak}\) is the weakly-supervised loss function to train detectors with image-level labels.

**Evaluation setup.** As the goal is to learn a generalizable detection model that works well on various unseen downstream tasks, we conduct zero-shot cross-dataset evaluation2 to assess the generalization performance of the trained detection model. Note, different domain adaptation [72; 73; 74; 75] that generally uses downstream data in training, our setup is similar to domain generalization [76; 77] that does not involve downstream data in training.

**Open-vocabulary Detector.** We modify the classification layer of the detector into an open-vocabulary format such that the detector could be tested over unseen datasets. Specifically, we replace the weights of the detector's classification layer with the fixed language embeddings encoded from class names, where the object classification could be achieved by matching the object's embedding and the fixed language embeddings. We adopt the CLIP language embeddings  as the classification weights as in [15; 60]. In this way, the modified detector could theoretically detect any target concepts on any target data. As reported in , detectors trained solely on detection datasets often exhibits constrained performance due to the small-scale training images and vocabularies. Similar to , our proposed DetLH introduces large-scale image-level datasets to enlarge the data distributions and data vocabularies in detector training, leading to more generalizable detectors and better generalization performance on various unseen datasets.

### Language Hierarchical Self-training

The proposed LHST utilizes WordNet's language hierarchy to expand the image-level labels, which enables co-regularization between the expanded image-level labels and self-training as illustrated in Figure 2.

**Overview.** For _fully supervised detector training_ over the detection dataset, we feed box-level annotated samples \((x,y_{det})_{det}\) to the detection model \(F\) and optimize \(F\) with the standard fully supervised detection loss, i.e., the first term of Eq. 1. For _weakly-supervised detector training_ over the image-level annotated dataset \((x,y_{cls})_{cls}\) shown in Figure 2, we first leverage WordNet's language hierarchy to expand the raw image-level label \(y_{cls}\) into \(y^{hier}_{image}\) (the hierarchical image-level label), and merge \(y^{hier}_{image}\) and the generated pseudo box label \(_{box}\) to acquire \(^{hier}_{box}\) (the hierarchical box-level pseudo label). Then, we optimize the detector with \((^{hier}_{box},w^{hier}_{box})\) and \((y^{hier}_{image},w^{hier}_{image})\), where \(w^{hier}_{image}\) and \(w^{hier}_{box}\) denote the predicted reliability scores of the expanded logits '1' in \(y^{hier}_{image}\) and \(y^{hier}_{box}\) and are used to weight the labels in loss calculation.

**Expanding image labels with language hierarchy.** Given image-level annotated dataset \((x,y_{cls})_{cls}\) (\(y_{cls}\) is a label vector with length \(C\) and \(C\) denotes the number of classes), we leverage WordNet's class name hierarchy  to expand \(y_{cls}\) into \(y^{hier}_{image}\) as the following:

Figure 2: The proposed language hierarchical self-training consists of two flows including Pseudo Label Generation (top box) and Training with Generated Labels (bottom box). The Pseudo Label Generation flow leverages WordNet to expand the image-level labels, and then merges the expanded image-level labels with the predicted pseudo box labels, such that the expanded image-level labels could provide richer and more flexible supervision (than the limited and rigid raw labels) to regularize the self-training which is prone to errors in pseudo labeling. In addition, as the labels expanded by WordNet (i.e., the expanded logits ‘1’ in \(y^{hier}_{image}\) and \(y^{hier}_{box}\)) are not all reliable, Pseudo Label Generation predicts reliability scores for the expanded labels to adaptively re-weight them when applying them on different images or pseudo boxes. In Training with Generated Labels, we optimize the detector with the generated image-level and box-level labels, where the image-level training could regularize the training with pseudo box-level labels as pseudo box labels vary along training iterations and are not very stable.

\[y^{hier}_{image}=(y_{cls}),\] (2)

where the function WordNet(\(\)) recursively finds all hypernyms and hyponyms of the input (i.e., the class indicated in \(y_{cls}\)) and sets their positions in the label vector \(y_{cls}\) to be '1' to expand \(y_{cls}\) into \(y^{hier}_{image}\). In this way, a single-label annotation could be expanded into a multi-label annotation within the very rich ImageNet-21K vocabulary.

**Generating pseudo box labels with predictions.** Given the image \(x_{cls}\), we feed \(x\) into the detector \(F\) to acquire the prediction as following:

\[\{p^{c}_{n}\}_{1 n N,1 c C}=F(x),\] (3)

where \(p_{n}\) is the probability vector of the predicted \(n\)-th bounding box after Softmax, and \(p^{c}_{n}\) denotes the predicted \(c\)-th category probability. Note we filter out a prediction if its max confidence score is lower than the threshold \(t\), and \(N\) denotes the number of predicted object proposals after filtering, i.e., \(max(\{p^{c}_{n}\}_{1 c C}) t, n\).

Then the pseudo category label \(_{box}=\{_{n}\}_{1 n N}\) for \(N\) boxes in image \(x\) is derived by:

\[*{arg\,max}_{_{n}}_{c=1}^{C}^{c}_{n} p^{c} _{n},\;s.t.\;_{n}^{C}, n,\] (4)

where \(_{n}=(^{(1)}_{n},^{(2)}_{n},...,^{(C)}_{n})\) is the predicted category label, and \(^{C}\) denotes a probability simplex with length \(C\).

**Merging image and pseudo box labels.** As the predicted pseudo box label \(_{box}\) is error-prone, we regularize it with the expanded image-level supervision by merging \(y^{hier}_{image}\) and \(_{box}\) as the following:

\[^{hier}_{box}(n)=_{box}(n) y^{hier}_{image}, n,\] (5)

where \(\) denotes the logical "OR" operator.

**Assessing the expanded labels.** As the labels expanded by WordNet (i.e., the expanded logits '1' in \(^{hier}_{box}=\{^{c}_{n}\}_{1 n N,1 c C}\)) are not all reliable, we predict a reliability score \(w^{c}_{n}\) for the expanded label to adaptively re-weight \(y^{c}_{n}^{hier}_{box}\) when applying it on different pseudo boxes. We measure the reliability of \(y^{c}_{n}\) with prediction \(p^{c}_{n}\), and \(w^{hier}_{box}=\{w^{c}_{n}\}_{1 n N,1 c C}\) can be derived by:

\[w^{c}_{n}=p^{c}_{n}&y^{hier}_{image}^{(c)} y^{c} _{cls}\\ 1&,\] (6)

where \(y^{hier}_{image}^{(c)} y^{(c)}_{cls}\) returns True if the \(c\)-th label logit in \(y^{hier}_{image}\) is expanded by WordNet, which also applies to \(^{hier}_{box}\) as \(^{hier}_{box}\) is expanded by mergeing it with \(y^{hier}_{image}\).

Given the prediction \(\{p^{c}_{n}\}_{1 n N,1 c C}\), the merged pseudo box label \(^{hier}_{box}=\{^{c}_{n}\}_{1 n N,1 c C}\) and its reliability score \(w^{hier}_{box}=\{w^{c}_{n}\}_{1 n N,1 c C}\), we optimize the detector \(F\) as the following:

\[_{box}(F(x))=_{n}^{N}_{c}^{C}((p^{c}_{n},y^{c}_{ n}) w^{c}_{n}),\] (7)

where BCE(\(\)) denotes the binary cross-entropy loss.

In addition, training with the predicted pseudo box labels is not very stable as pseudo box labels vary along training process. Thus, we regularize the training of \(_{box}(F(x))\) with an image-level loss defined as the following:

\[_{image}(F(x))=_{c}^{C}((p^{c}_{image},y^{hier}_{image }^{(c)}) w^{c}_{image}),\] (8)

where \(p_{image}=\{p^{c}_{image}\}_{1 c C}\) denotes the category probability predicted for the image-level proposal. \(w^{hier}_{image}=\{w^{c}_{image}\}_{1 c C}\) denotes the reliability score for the expanded logits "1" in \(y_{image}^{hier}\). Similar to Eq. 6, \(w_{image}^{c}=p_{image}^{c}\) if \(y_{image}^{hier}\)\({}^{c} y_{cls}^{(c)}\), otherwise \(w_{image}^{c}=1\). Besides, BCE\(()\) denotes the binary cross-entropy loss.

**Training objective.** The overall training objective of Language Hierarchical Self-training is defined as:

\[_{lhist}=_{(x,y_{det})_{det}}_{det}(F( x),y_{det})+_{(x,y_{cls})_{cls}}(_{box}(F(x))+ _{image}(F(x)))\] (9)

**Language Hierarchical Prompt Generation.** As the goal is to learn a generalizable detection model that works well on various downstream tasks, one typical challenge is the vocabulary gap between detector training datasets (i.e., LVIS and ImageNet-21K) and detector testing datasets (e.g., object365 or customized data). A common solution of tackling the vocabulary gaps is to conduct prompt learning  to generate proper category prompts. However, prompt learning generally requires labeled target images for additional training.

In this work, we tackle the vocabulary gaps by generating prompts with the help of WordNet, which introduces little computation overhead and does not require labeled target images and additional training. To this end, we design language hierarchical prompt generation (LHPG) that works by incorporating WordNet information into prompt generation process. Specifically, LHPG leverages CLIP language encoder  to measure the embedding distances between test concepts and WordNet synsets, and then generates the prompt for a given test concept from its best matched WordNet synset: \(V_{test}^{}=(V_{test},)\), where \(V_{test}\) denotes test vocabulary, WordNet denotes WordNet synsets, CLIP denotes CLIP language encoder and \(V_{test}^{}\) stands for the best matched WordNet synsets for the classes in \(V_{test}\). Then, we generate test prompts from \(V_{test}^{}\). As compared with \(V_{test}\), our \(V_{test}^{}\) has been standardized by WordNet and is well aligned with our proposed detector that is trained with WordNet information via LHST. In another word, the combination of LHST and LHPG makes use of WordNet as a standard and intermediate vocabulary that bridges the gaps between training and testing vocabularies, generating better prompts and leading to better detection performance on downstream applications.

## 4 Experiments

We evaluate our DetLH on 14 widely adopted detection benchmarks. We follow the zero-shot cross-dataset object detection setting proposed in [17; 15]. More details like **Dataset** and **Implementation Details** are provided in the appendix.

### Comparison with the state-of-the-art

We conduct extensive experiments to benchmark our proposed DetLH with state-of-the-art methods. We evaluate them on 14 widely studied object detection datasets to assess their zero-shot cross-dataset generalization ability. Tables 1- 5 report zero-shot cross-dataset detection results for common objects, autonomous driving, intelligent surveillance, and wildlife detection, respectively. More details are to be described in the following paragraphs.

    &  &  \\   & AP & AP50 & AP75 & APs & APm & API & AP & AP50 & AP75 & APs & APm & API \\  WSDDN  & 21.0 & 29.1 & 22.7 & 8.7 & 20.9 & 31.2 & 61.6 & 82.7 & 67.5 & 24.8 & 50.9 & 73.5 \\ YOLO9000  & 21.0 & 28.5 & 22.6 & 8.6 & 20.7 & 30.9 & 62.6 & 83.6 & 68.7 & 23.7 & 52.0 & 73.9 \\ DLWL  & 21.3 & 29.1 & 23.0 & 8.8 & 21.0 & 31.5 & 62.4 & 83.4 & 68.3 & 23.8 & 51.2 & 73.8 \\ Detic  & 21.6 & 29.4 & 23.4 & 9.0 & 21.4 & 31.9 & 62.4 & 83.3 & 68.5 & 23.7 & 51.8 & 73.9 \\
**DetLH (Ours)** & **23.6** & **32.5** & **25.5** & **9.8** & **23.5** & **35.0** & **64.4** & **86.1** & **70.8** & **25.3** & **54.1** & **75.3** \\  Dataset-specific oracles & 31.2 & - & - & - & - & - & 54.4 & 79.7 & 59.1 & 19.0 & 40.8 & 64.5 \\   

Table 1: **Zero-shot cross-dataset object detection for common objects.** All detectors are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over target datasets (i.e., Object365 and Pascal VOC with objects from common classes and scenarios) without finetuning. “Dataset-specific oracles” denote the detectors that are fully supervised which are trained by using the training data of respective datasets.

Object detection for common objects.Table 1 shows that DetLH outperforms state-of-the-art methods clearly on common object datasets Object365 and Pascal VOC. In addition, we can observe that DetLH even brings significant gains above the _dataset-specific oracle_ (i.e., the model that is fully trained on the target training data) on Pascal VOC (i.e., a small-scale dataset), showing the advantages of leveraging large-scale training data.

Object detection for autonomous driving.As shown in Table 2, our DetLH outperforms state-of-the-art methods by large margins on various autonomous driving datasets, showing that DetLH still works effectively while facing large variations in camera views from autonomous driving scenarios to the base-dataset scenarios (LVIS and ImageNet-21K), e.g., autonomous driving images are captured under very different camera views. In addition, the experimental results in Table 3 show that our DetLH brings significant performance gains against state-of-the-art methods when encountering various weather and time-of-day conditions, which demonstrates the effectiveness of DetLH while detecting objects under large noises , e.g., the images captured under different weather and time-of-day conditions may have very different styles and image quality.

Object detection for intelligent surveillance.From Table 4, we can observe that our DetLH outperforms state-of-the-art methods by clear margins on various intelligent surveillance datasets, indicating that DetLH is also tolerant to large changes in the camera lens and angles which often happen to intelligent-surveillance images that are captured under very different camera lens and angles (e.g., surveillance cameras are often with the wide-angle lens and used in high angle views).

Object detection for Wildlife.The experimental results in Table 5 show that our DetLH performs well on various wildlife detection datasets, showing that DetLH works effectively for detecting fine-grained categories that exist widely in wildlife detection datasets. The significant performance

    &  &  &  &  &  \\   & AP & AP50 & AP75 & AP & AP50 & AP75 & AP & AP50 & AP75 & AP & AP50 & AP75 & AP & AP50 & AP75 \\  WSDDN  & 11.3 & 17.6 & 11.6 & 13.1 & 19.6 & 13.3 & 25.6 & 35.3 & 31.1 & 17.1 & 31.9 & 16.0 & 16.7 & 26.1 & 18.0 \\ YOLO9000  & 12.7 & 19.7 & 13.0 & 13.1 & 19.3 & 13.0 & 29.1 & 39.4 & 35.3 & 18.6 & 33.9 & 17.7 & 18.3 & 28.1 & 19.7 \\ DLWL  & 12.9 & 20.1 & 12.9 & 13.5 & 20.0 & 13.6 & 27.8 & 38.0 & 33.6 & 16.6 & 31.1 & 15.1 & 16.7 & 26.1 & 18.0 \\ Detic  & 13.4 & 20.6 & 13.9 & 16.9 & 23.6 & 17.6 & 28.7 & 39.2 & 34.8 & 18.6 & 34.2 & 17.6 & 19.4 & 29.4 & 21.0 \\
**DetLift (Ours)** & **15.8** & **24.5** & **16.0** & **17.9** & **25.1** & **18.5** & **32.7** & **44.0** & **39.7** & **20.1** & **36.6** & **19.3** & **21.6** & **32.6** & **23.4** \\  Dataset-specific oracles & 45.2 & 63.1 & 50.8 & 40.6 & 58.6 & 43.7 & 53.1 & 70.6 & 63.5 & 33.8 & 60.4 & 35.2 & 43.2 & 63.2 & 48.3 \\   

Table 4: **Zero-shot cross-dataset object detection for intelligent surveillance. All detectors are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over surveillance datasets MIO-TCD, BAAI-VANJEE, DETRAC and UAVDT without finetuning.**

    &  &  &  &  \\   & AP & AP50 & AP75 & AP & AP50 & AP75 & AP & AP & & & & & & & & \\  WSDDN  & 28.2 & 45.4 & 27.1 & 22.3 & 34.0 & 23.3 & 17.4 & 28.9 & 17.1 & 22.6 & 36.1 & 22.4 \\ YOLO9000  & 28.8 & 46.2 & 27.4 & 22.5 & 34.6 & 23.4 & 18.3 & 30.4 & 18.0 & 23.2 & 37.0 & 22.9 \\ DLWL  & 28.6 & 45.6 & 28.1 & 22.5 & 34.7 & 23.2 & 18.3 & 30.4 & 18.0 & 23.1 & 36.9 & 23.0 \\ Detic  & 29.6 & 47.1 & 28.4 & 23.0 & 35.6 & 23.6 & 18.8 & 30.9 & 18.5 & 23.8 & 37.9 & 23.5 \\
**DetLift (Ours)** & **31.2** & **50.3** & **29.1** & **26.5** & **44.0** & **25.8** & **25.1** & **38.4** & **26.1** & **27.6** & **44.2** & **27.0** \\  Dataset-specific oracles & 43.0 & 69.0 & 42.6 & 28.1 & 45.8 & 28.5 & 44.7 & 68.2 & 47.3 & 38.6 & 61.0 & 39.5 \\   

Table 2: **Zero-shot cross-dataset object detection for autonomous driving. All detectors are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over autonomous driving datasets (i.e., Cityscapes, Vistas and SODA10M) without finetuning.**

    &  &  &  &  \\   & rainy & monay & overcast & cloudy & feggy & underlined & daytime & dumbd-dusk & night & underlined & frog & sand & mow & \\  WSDDN  & 35.0 & 33.0 & 38.3 & 41.7 & 26.7 & 46.0 & 39.1 & 35.5 & 27.9 & 50.2 & 62.6 & 55.0 & 65.6 & 42.8 \\ YOLO9000  & 34.4 & 33.6 & 39.5 & 41.8 & 31.0 & 45.4 & 39.6 & 35.9 & 28.8 & 46.6 & 60.6 & 53.9 & 64.4 & 42.7 \\ DLWL  & 34.8 & 33.4 & 38.8 & 43.8 & **40.2** & 45.2 & 40.1 & 35.1 & 28.7 & 45.0 & 62.1 & 56.1 & 63.7 & 43.6 \\ Detic  & 34.3 & 33.2 & 39.5 & 41.9 & 27.9 & 45.4 & 39.2 & 35.5 & 28.8 & 48.2 & 52.3 & 54.1 & 56.1 & 41.3 \\
**DetLift (Ours)** & **40.2** & **37.5** & **48.2** & **49.3** & 37.1 & **49.9** & **45.7** & **40.0** & **34.2** & **53.0** & **63.2** & **57.6** & **47.3** & **47.9** \\    & 52.0 & 52.5 & 56.3 & 56.3 & 21.3 & 65.4 & 57.0 & 50.4 & 48.6 & 27.7 & 56.7 & 48.4 & 26.4 & 47.6 \\   

Table 3: **Zero-shot cross-dataset object detection under different weather and time-of-day conditions (using metric AP50). All detectors are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over BDD100K and DAWN datasets without finetuning.**gains largely come from the introduction of language hierarchy into detector training and prompt generation, which helps model the hierarchical relations among parent and fine-grained subcategories and thus leads to better fine-grained object detection.

The superior detection performance of our DetLH is largely attributed to our two core designs, i.e., LHST and LHPG. LHST enables effective usage of large-scale image-level annotated images and significantly enlarges the data distribution and the data vocabulary in detector training, yielding robust performance under large cross-dataset gaps in data distribution and vocabulary. LHPG ingeniously helps mitigate the vocabulary gaps between detector training and testing. It improves the overall confidence of detection and benefits the detection as large data distribution gaps (or large data vocabulary gaps) often lead to low-confidence predictions and poor detection results.

### Ablation Studies

We perform ablation studies with Swin-B  based CenterNet2  over the large-scale Object365 dataset as shown in Table 6. As the core of our proposed DetLH, we examine how our designed LHST and LHPG contribute to the overall performance of zero-shot cross-dataset object detection. As shown in Table 6, the _baseline_ (Box-Supervised ) does not perform well as it uses box-level training data only. It can be observed that LHST outperforms the baseline clearly, showing that LHST can effectively leverage the large-scale image-level annotated dataset to significantly enlarge the data distribution and data vocabulary involved in detector training, leading to much better zero-shot cross-dataset detection performance. In addition, LHPG brings clear performance improvements in zero-shot cross-dataset detection by introducing language hierarchy into prompt generation, demonstrating the effectiveness of LHPG in mitigating the vocabulary gaps between training and testing. Moreover, the inclusion of both LHST and LHPG in the proposed DetLH performs clearly the best, indicating the complementary property of our two designs.

### Discussion

   Method & LHST & LHPG & AP50 \\  Box-Supervised  & & & 26.5 \\   & ✓ & & 31.3 \\  & & ✓ & 31.0 \\
**DetLH (Ours)** & ✓ & ✓ & **32.5** \\   

Table 6: **Ablation studies of our DetLH** with Language Hierarchical Self-training (LHST) and Language Hierarchical Prompt Generation (LHPG). The experiments are conducted with Swin-B based CenterNet2  and the detectors are evaluated on Object365 in zero-shot cross-dataset object detection setup.

    &  &  &  &  \\   & AP & AP50 & AP75 & AP & AP50 & AP75 & AP & AP50 & AP75 & AP & AP50 & AP75 \\  WSDDN  & 18.1 & 26.3 & 18.8 & **76.7** & **88.2** & **84.0** & 36.0 & 41.7 & 37.5 & 43.6 & 52.0 & 46.7 \\ YOLO9000  & 22.6 & 33.2 & 22.5 & 75.9 & 87.5 & 83.2 & 39.0 & 45.4 & 40.8 & 45.8 & 55.3 & 48.8 \\ DLWL  & 25.3 & 34.7 & 26.3 & 74.7 & 86.2 & 81.3 & 41.7 & 48.1 & 43.7 & 47.2 & 56.3 & 50.4 \\ Detic  & 27.4 & 36.7 & 92.2 & 68.9 & 80.9 & 76.4 & 41.1 & 47.7 & 42.9 & 45.8 & 55.1 & 49.5 \\
**DetLH (Ours)** & **36.2** & **49.0** & **38.3** & 74.8 & 87.2 & 81.8 & **44.3** & **51.2** & **46.3** & **51.8** & **62.5** & **55.5** \\  Dataset-specific oracles & 75.1 & 86.3 & 79.9 & 82.7 & 90.9 & 89.1 & 64.4 & 74.6 & 69.4 & 74.1 & 83.9 & 79.5 \\   

Table 5: **Zero-shot cross-dataset object detection for Wildlife Detection.** All detectors are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over wildlife datasets (i.e., Arthropod Detection, AfricanWildlife and Animals Detection) without finetuning.

   Method & LHST & LHPG & AP50 \\  Box-Supervised  & & & 26.5 \\   & ✓ & & 31.3 \\  & & ✓ & 31.0 \\
**DetLH (Ours)** & ✓ & ✓ & **32.5** \\   

Table 7: **Zero-shot cross-dataset object detection on various datasets.** Results are averaged on 14 widely studied datasets.

**Generalization across various detection tasks:** We study the generalization of our DetLH by conducting zero-shot cross-dataset object detection on 14 widely studied object detection datasets. Tables 1 - 5 show that DetLH achieves superior performance consistently across all the detection applications. Besides, Table 7 summarizes the detection results averaged on 14 datasets, showing that DetLH clearly outperforms the state-of-the-art methods.

**Generalization across various network architectures:** We study the generalization of the proposed DetLH from the perspective of network architectures. Specifically, we perform extensive evaluations with four representative network architectures, including one Transformer-based (i.e., Swin-B) and three CNN-based (i.e., ConvNeXt-T, ResNet-50 and ResNet-18). Experimental results in Table 8 show that the proposed DetLH outperforms the state-of-the-art method consistently over different network architectures.

**Parameter Studies for Language Hierarchical Self- training (LHST).** In generating pseudo box labels in LHST, we filter out a prediction if its max confidence score is lower than the threshold \(t\). We study the threshold \(t\) by changing it from \(0.65\) to \(0.85\) with a step of \(0.05\). Table 12 reports the experimental results on zero-shot transfer object detection over object365 dataset. We can observe that the detection performance is not sensitive to the threshold \(t\).

Due to the space limit, we provide more DetLH discussions and visualizations in the appendix.

## 5 Conclusion

This paper presents DetLH, a Detector with Language Hierarchy that combines language hierarchical self-training (LHST) and language hierarchical prompt generation (LHPG) for learning generalizable object detectors. LHST introduces WordNet's language hierarchy to expand the image-level labels and accordingly enables co-regularization between the expanded labels and self-training. LHPG helps mitigate the vocabulary gaps between training and testing by introducing WordNet's language hierarchy into prompt generation. Extensive experiments over multiple object detection tasks show that our DetLH achieves superior performance as compared with state-of-the-art methods. In addition, we demonstrate that DetLH works well with different network architectures such as Swin-B, ConvNeXt-T, ResNet-50, etc. Moving forward, we will explore language hierarchy to further expand the labels in an open-vocabulary manner in addition to the closed ImageNet-21K's vocabulary.

    &  &  \\   & & AP & AP50 & AP75 & APs & APm & API \\   Detle  \\ **DetLH (Ours)** \\  } &  & 21.6 & 29.4 & 23.4 & 9.0 & 21.4 & 31.9 \\  & & 23.6 & 32.5 & 25.5 & 9.8 & 23.5 & 35.0 \\   Detle  \\ **DetLH (Ours)** \\  } &  & 16.9 & 23.5 & 18.1 & 6.8 & 16.6 & 24.9 \\  & & 18.9 & 26.8 & 20.2 & 7.6 & 18.8 & 28.2 \\   Detle  \\ **DetLH (Ours)** \\  } &  & 16.2 & 22.8 & 17.5 & 6.3 & 16.2 & 24.1 \\  & & 17.7 & 25.5 & 19.0 & 6.9 & 17.9 & 26.4 \\   Detle  \\ **DetLH (Ours)** \\  } &  & 10.8 & 15.5 & 11.6 & 3.9 & 10.2 & 16.2 \\  & & 11.8 & 17.3 & 12.5 & 4.3 & 11.4 & 17.7 \\   

Table 8: **Zero-shot cross-dataset object detection with different network architectures.** All networks architectures are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over Object365 without finetuning.

   Threshold \(t\) & 0.65 & 0.70 & 0.75 & 0.80 & 0.85 \\  AP50 & 31.1 & 31.3 & 31.3 & 31.3 & 31.2 \\   

Table 9: **Parameter Studies for Language Hierarchical Self- training (LHST) on zero-shot transfer object detection over object365 dataset. We study the thresholding parameter \(t\) used in generating pseudo box labels in LHST.**