# Towards Safe Concept Transfer of Multi-Modal Diffusion via Causal Representation Editing

Peiran Dong\({}^{1}\) Bingjie Wang\({}^{1}\) Song Guo\({}^{2}\) Junxiao Wang\({}^{3,4}\)

Jie Zhang\({}^{2}\) Zicong Hong\({}^{1}\)

\({}^{1}\)Hong Kong Polytechnic University \({}^{2}\)Hong Kong University of Science and Technology

\({}^{3}\)Guangzhou University \({}^{4}\)King Abdullah University of Science and Technology

{peiran.dong,bingjie.wang,zicong.hong}@connect.polyu.hk

songguo@cse.ust.hk, wangjunxiao@live.com, csejzhang@ust.hk

Equal Contribution.

###### Abstract

Recent advancements in vision-language-to-image (VL2I) diffusion generation have made significant progress. While generating images from broad vision-language inputs holds promise, it also raises concerns about potential misuse, such as copying artistic styles without permission, which could have legal and social consequences. Therefore, it's crucial to establish governance frameworks to ensure ethical and copyright integrity, especially with widely used diffusion models. To address these issues, researchers have explored various approaches, such as dataset filtering, adversarial perturbations, machine unlearning, and inference-time refusals. However, these methods often lack either scalability or effectiveness. In response, we propose a new framework called causal representation editing (CRE), which extends representation editing from large language models (LLMs) to diffusion-based models. CRE enhances the efficiency and flexibility of safe content generation by intervening at diffusion timesteps causally linked to unsafe concepts. This allows for precise removal of harmful content while preserving acceptable content quality, demonstrating superior effectiveness, precision and scalability compared to existing methods. CRE can handle complex scenarios, including incomplete or blurred representations of unsafe concepts, offering a promising solution to challenges in managing harmful content generation in diffusion-based models.

## 1 Introduction

Expanding on recent progress in text-to-image (T2I) diffusion generation, which is great at making realistic and varied images from written descriptions, researchers are now delving into more advanced vision-language-to-image (VL2I) generation techniques. In these VL2I methods, especially with diffusion models, some use both images of a subject and written descriptions to render the subject in a new context, which is called subject-driven generation [1; 2]. Others take original images and instructions for changes to create altered images, known as image editing . Early approaches either fine-tune models on new images [4; 2; 5; 6; 7] or directly inject image features into the U-Net of diffusion models [8; 9; 1; 10]. However, these methods struggle to jointly model multi-modal inputs and fully leverage the generalization ability of the diffusion model. BLIP-Diffusion  is a notable advancement because it creates object representations by blending images with random backgrounds, allowing for the generation of single objects without prior examples. Building on this, Kosmos-G  expands to generate multiple objects without examples, using multi-modal largelanguage models (MLLMs) instead of the original CLIP text encoder to encode different kinds of inputs into a single feature set.

The advent of a large multi-modal encoder has endowed diffusion models with zero-shot generation, enabling the transfer of concepts (e.g., object or style) to new scenes. However, this unrestricted capability also brings up ethical concerns. There's a risk that people with bad intentions could use zero-shot generation to transfer harmful concepts, like violence or pornography, with just one reference image. Existing efforts in safe generation [13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23] primarily focus on mitigating internal risks stemming from model defects. Diffusion models trained on unedited, large-scale, web-scraped datasets often learn inappropriate and unauthorized knowledge, posing risks when users manipulate textual prompts to "extract" unsafe content.

Researchers have pursued various strategies to mitigate the generation of harmful content, encompassing four primary approaches: dataset filtering [13; 14], adversarial perturbations [15; 16; 17; 18], machine unlearning [19; 20], and inference-time refusals [21; 22; 23]. Filtering the dataset involves removing images containing explicit or objectionable content, such as nudity and violence, to ensure the safe generation of diffusion models. However, the advent of zero-shot learning introduces challenges, as it enables diffusion models to transfer unseen objects and styles, complicating copyright protection and security review processes. While adversarial perturbations offer a means to safeguard specific images from manipulation, their efficacy is hampered by the need for training and adaptation to diffusion models with varying parameters. This lack of scalability arises from the requirement to train different adversarial perturbations for each model, despite their structural similarities. Similarly, unlearning-based methods address inherent model defects but fall short in addressing the use of external unsafe images for concept transfer by users. Moreover, existing inference-time refusals predominantly target unsafe concepts describable by language, thus exhibiting limited effectiveness in multi-modal zero-shot generation scenarios. These limitations underscore the need for novel approaches to address the evolving challenges associated with safe content generation in diffusion models.

**Contributions.** To address these challenges, we propose a novel framework called Causal Representation Editing (CRE), which generalizes representation editing for transformer-based Large Language Models (LLMs) to diffusion-based generative models. CRE enhances the efficiency and flexibility of safe concept transfer by introducing a plug-and-play inference-time intervention in diffusion timesteps causally related to unsafe concepts. Our framework comprises two key components: 1) Editing function: We construct steering vectors from examples of unsafe concepts to precisely eliminate them from the original representations. 2) Editing timesteps: We propose "assess-with-exclusion" to identify the causal period for each unsafe concept, during which the unsafe concept appears in the noisy image. This approach reduces editing overhead and avoids ineffective interventions in irrelevant diffusion timesteps, maintaining high editing fidelity. Our contributions include: 1) An early exploration of safe concept transfer in MLLM-enabled diffusion models, with our CRE framework enabling effective inference-time unsafe concept removal. 2) Precise removal of unsafe concepts from noisy images while retaining reasonably generated content, reducing editing overhead by nearly half through fine-grained editing based on the causal period. 3) Comprehensive evaluations demonstrating that CRE surpasses existing methods in effectiveness, preciseness, and scalability, even in complex scenarios involving incomplete or blurred features of unsafe concepts.

## 2 Related Work

**Vision-Language-to-Image Diffusion Models.** The fundamental aspect of achieving Vision-Language-to-Image (VL2I) generation lies in training multi-modal encoders responsible for aligning and fusing features from diverse input modalities. BLIP-Diffusion  adopts an "align-after-encoding" approach to train its multi-modal encoder. Initially, images and text undergo separate encoding by individual single-modal encoders. Subsequently, following BLIP-2 , the Q-Former architecture aligns visual features with text features. However, this pre-training strategy restricts BLIP-Diffusion to accept only a single image as the input for the visual modality during zero-shot generation. Conversely, Kosmos-G  employs an "align-before-encoding" paradigm to train its multi-modal encoder. Kosmos-G pursues the objective of treating images as a foreign language in the image generation process. It incorporates a multi-modal large language model (MLLM) to jointly encode images and text, with each image being embedded into 64 tokens. By utilizing the pre-trainedMLLM as an alternative to CLIP encoders , diffusion models gain the capability of zero-shot generation based on multiple input images.

**Inference-time Safe Concept Transfer.** Inference-time safe concept transfer enables generation service providers to dynamically deploy and adjust governance rules, particularly in response to potentially unsafe input from users. Existing approaches typically involve either post-generation detection or in-process adjustment to ensure safety. Rando _et al_.  employ a method where the generated image is projected into a CLIP latent space  for comparison against pre-computed embeddings of unsafe concepts, with images surpassing a similarity threshold being flagged as unsafe. However, this approach lacks precision in removing unsafe concepts while preserving overall image quality. Conversely, SLD  and ProtoRe  integrate safe guidance directly into the diffusion process. These techniques rely on textual descriptions of unsafe concepts, encoded using a CLIP text encoder, to provide negative guidance for denoising. SLD  adjusts the diffusion process by modifying the predicted noise from the U-Net, while ProtoRe  extracts unsafe concepts from the attention map and refines them. These strategies face limitations when unsafe concepts are not effectively described through natural language.

**Representation Editing for LLMs.** Current studies on Inference-Time Intervention (ITI)  in Large Language Models (LLMs) indicate that many LLMs exhibit interpretable directions in their activation spaces, which influence their inference processes. For instance, by introducing carefully designed steering vectors to specific layers for particular tokens, the output can be significantly biased, regardless of the user prompt's topic . Developing a training-free editing method to mitigate unsafe concepts in generative models offers two key advantages. Firstly, it allows the model to retain its strong zero-shot generation ability by preserving the knowledge from pre-training. Secondly, as unsafe concepts may change dynamically due to legal or copyright factors, a plug-and-play editing method can efficiently add or remove types of unsafe concepts under governance.

## 3 Safe Concept Transfer

### Threat Model

Let \(\) represent the images generated by a diffusion model \(G_{}\) based on a multi-modal prompt, which includes a text prompt \(p\) and a reference image \(r\). The reference image can contain up to \(K\) pre-defined unsafe concepts \(_{k},k=1,2,,K\), such as legally protected concepts. Our goal is to intervene in the image generation process to remove these concepts from \(\). For example (see

Figure 1: Method Overview of CRE. Users of VL2I models (U-Net) might input/query images containing unsafe concepts as reference images (objects or styles), here taking the “Van Gogh” style as an example. CRE consists of two main phases. Phase 1 involves discriminator training and causal period search for each unsafe concept category, which can be performed offline (omitted from this figure, see section 3.3 for details). During inference (phase 2, i.e., the right side of this figure), if the reference image contains unsafe concepts, the editing function of CRE is applied within the U-Net layers. Otherwise, the generated content is faithful to the user-specified prompts without modification.

Figure 1), an adversary might aim to profit by plagiarizing the style of an artistic work, such as a Van Gogh painting. They could use such a painting as a reference image to counterfeit infringing images using VL2I models with zero-shot generation capabilities. Additionally, unwitting users might input images containing unsafe concepts as reference images (objects or styles). These scenarios can lead to significant social problems and economic losses for generation service providers and copyright owners.

In contrast to prior studies that primarily address internal generation issues stemming from the diffusion process itself (often due to unedited training data ), our focus is on a new challenge where risks originate from external factors that impact the model. The key distinction between these two scenarios lies in whether users can prompt the generation of unsafe content solely through text inputs. In the case of internal unsafe generation, users might inadvertently generate nudity images by using the term "Four Horsemen" as a text prompt. In contrast, external unsafe generation involves users providing a nude image as a reference to generate more pornographic images. In this latter scenario, the model relies on externally provided visual information to generate new images.

**Capability:** Regulators can define a set of unsafe concepts based on existing laws, regulations, or proposals from copyright owners. Each category of unsafe concepts is accompanied by at least one example image. The VL2I generation service is offered to users through an API. Regulators have the ability to fully control the inference process of the generation model, without any prior information about the user input prompts.

**Objective:** Methods aimed at removing unsafe concepts must be effective and precise. Effectiveness ensures the legality of the generated image, while precision ensures that the reasonable content in the generated image is preserved. It is essential that the service experience of normal users remains unaffected, meaning the system must respond appropriately to requests involving safe concepts.

### Preliminaries

**Diffusion.** Diffusion-based models, denoted as \(G_{}\), progressively refine an initial Gaussian noisy image \(_{T}(0,)\) to generate images \(_{0}\) that faithfully represent the user's input prompt \(p,r\). At each timestep \(t[T,T-1,,1]\), the model estimates the noise \(_{}\) to be subtracted from the current noisy image \(_{t}\). This denoising process can be succinctly expressed as \(_{t-1}=_{t}-_{}(_{t},t,p,r)\)2. The noise estimate \(_{}(_{t},t,p,r)\) is computed as a linear combination of the unconditional generation \(_{}(_{t},t)\) and the conditional generation \(_{}(_{t},t,p,r)\):

\[_{}(_{t},t,p,r)=_{}( _{t},t)+s_{g}(_{}(_{t},t,p,r)-_{}( _{t},t)),\] (1)

where the guidance scale \(s_{g}\) modulates the impact of the conditioning information, allowing for flexible adjustment of the conditioning strength.

**Inference-Time Safe Guidance.** SLD  introduced the first inference-time safety guidance for latent diffusion models to address issues related to inappropriate image generation. This approach extends the generative process by integrating text conditioning using classifier-free guidance and suppressing inappropriate concepts from the output image. Specifically, it introduces a negative concept condition \(p^{}\) via the text prompt, following noise estimation principles. Essentially, this method adjusts the unconditional noise prediction towards the user prompt conditioned estimate while simultaneously moving it away from the negative concept conditioned estimate:

\[_{}^{SLD}(_{t},t,p,r)=_{}( _{t},t)+s_{g}(_{}(_{t},t,p,r)-_{ }(_{t},t)-(_{}(_{t},t,p^{})- _{}(_{t},t))),\] (2)

where \(\) is concept-dependent guidance scale.

SLD exhibits two key limitations. Firstly, its effectiveness relies heavily on text prompts that can precisely describe negative concepts. In contexts where images are included in the prompt for zero-shot generation, SLD's performance is significantly constrained by the lack of precise textual descriptions of the reference image. Secondly, while SLD introduces security guidance, it impacts the experience of benign users. The magnitude of this impact is contingent upon the setting of the guidance scale, necessitating a balance between safety and utility.

Previous research on representation engineering [30; 31] has demonstrated that representations in transformer architecture encode intricate semantic details, suggesting that manipulating these representations could be a more effective approach than updating noisy images. In this paper, we explore this idea further by introducing representation editing for large multi-modal diffusion models. Instead of directly guiding safe generation, our method manipulates a small portion of latent representations to steer the denoising process, thereby removing unsafe concepts during inference.

### Causal Representation Editing

**Representation Editing Framework.** Current research on representation editing [30; 31] mainly focuses on three key components \( F,L,P\), where \(F\) denotes the editing function, \(L\) represents the number of editing layers, and \(P\) indicates the editing token position (e.g., the number of prefix or suffix positions to intervene). Recognizing the unique characteristics of diffusion models compared to language generation models, we introduce the timestep dimension \(T\) and extend representation editing from discriminative or autoregressive predictive models to diffusion-based generative models.

**Definition 1**.: _A representation editing framework for diffusion-based generative models can be defined by a tuple \(,,,\), which governs an inference-time intervention on the representations computed by the U-Net throughout the diffusion process. This framework comprises four key components:_

* _The editing function_ \(:^{d}^{d}\)_, which encompasses operations such as linear combinations, piece-wise operations, and projections._
* _A set of layers_ \( 1,,m\) _in the U-Net where the editing is applied._
* _A set of input positions_ \( 1,,n\) _to which the editing is applied. For text prompts, token locations are typically specified, while mask guidance is commonly used for image prompts._
* _A set of timesteps_ \( 1,,T\) _during which the editing is applied._

This framework enables precise control over the editing operation, allowing for targeted interventions to modify the generated outputs as needed. In the following, we introduce our causal representation editing by detailing the four components mentioned above. The U-Net architecture comprises layers broadly classified into convolution layers, self-attention layers, and cross-attention layers. Prior investigations into image editing [32; 33; 34] have elucidated that cross-attention layers facilitate the amalgamation of noisy images and user prompts, yielding fused features. Specifically, The noisy image \(z_{t}\) is projected to a query matrix via a linear layer \(_{Q}()\), denoted as \(Q=_{Q}(z_{t})\). The embedded user prompt \(\{p,r\}\) is projected to a key matrix \(K=_{K}(p,r)\) and a value matrix \(V=_{V}(p,r)\) through linear layers \(_{K}()\) and \(_{V}()\). The attention representations \(A\) are then calculated as follows:

\[A=(}{}) V^{d}.\] (3)

Visualizing the attention map \((QK^{T}/)\) (see Appendix-F), we can observe that concepts from the prompts manifest in the weighted output representations. Consequently, the editing is implemented immediately following the computation of \(A\) and influences the representations within each cross-attention layer.

**Editing Function.** The editing function typically receives the original representation (to be edited) and the representation of a specific concept (referred to as a steering vector) as input, aiming to amplify or suppress the concept in the original representation. For instance, ActAdd  employs linear addition in the transformer activation layer of a LLM to incorporate the representation of a particular topic (e.g., "wedding") into the original representation. This ensures that regardless of the user prompt, the model's output will be biased towards the wedding topic.

In this paper, we construct steering vectors based on examples of unsafe concepts. For the \(k\)-th type of unsafe concept, we employ a procedure akin to Equation 3 to create a steering vector containing the unsafe concept. To precisely remove the unsafe concept from the original representations, we project out the component of the representation aligned with the steering vector: \((A,_{k})=A-_{k}_{k}}{\|_{k}\|^{2 }}\). \(_{k}\), where \(_{k}=(Q^{T}/)=(_{Q}(z_{t})_{K}(_{k})^{T}/)_{V}( _{k})\). Ablation study in Appendix-E demonstrates the effectiveness of the projection-based representation editing.

Although representation editing effectively removes unsafe concepts from generated images, it can hinder generation with benign prompts. As the number of unsafe concepts requiring governance grows, representation editing can significantly degrade image quality. To ensure scalability, we utilize the VL2I generation for data augmentation. Then, we train a discriminator \(f_{k}:^{C(H W)}\) to evaluate the confidence that an image contains an unsafe concept \(c_{k}\). This discriminator acts as an indicator for determining whether representation editing should be applied, yielding the final editing function:

\[(A,_{k})=A-_{k}[f_{k}(r)](_{k}}{\|_{k}\|^{2}}_{k}).\] (4)

**Editing Timesteps.** Previous research  has demonstrated that the diffusion process generates different elements at various stages. Initially, the diffusion process primarily generates low-frequency features such as layout and object contours, while in later stages, it produces high-frequency features such as color and texture. As unsafe concepts typically represent either concrete objects or abstract styles, their generation is often constrained to specific timesteps and does not encompass the entire diffusion process. Consequently, applying representation editing at each diffusion step would introduce unnecessary computational overhead. For more precise editing, we seek to identify specific diffusion periods during which the unsafe concept \(c_{k}\) manifests in the noisy image.

Drawing inspiration from causal tracing in knowledge editing , we introduce the causal period for the generation of a given concept in the diffusion process.

**Definition 2**.: _For a concept \(c_{k}\), a causal period \([t_{s},te]\) is defined as a period during which there is no shorter diffusion period that yields better generation quality for \(c_{k}\). For any diffusion period \([ts,t_{e}]\) that satisfies \([t_{s},t_{e}][t_{s}^{*},t_{e}^{*}](t_{e}-t_{s})(t_{e}^{*}-t_{s}^ {*})\), we have:_

\[f_{k}(G_{,,,=[t_{s},t_{e}] }(c_{k})) f_{k}(G_{,,,=[t_{s} ^{*},t_{e}^{*}]}(c_{k}))+_{k},\] (5)

_where \(_{k}\) is a small constant._

In Equation 5, we use the classification confidence of the discriminator \(f_{k}\) for \(c_{k}\) to assess its generation safety.

**Causal Period Search.** Previous causal tracing methods employ a "corrupted-with-restoration" approach to identify the most crucial hidden state variable in LLMs when recalling a fact. Given \(T\) diffusion rounds, the search space for determining the causal period through sampling is \(2^{T}-1\) (excluding the empty set), which is considerably larger than the linear search space in the causal tracing problem seeking a single optimal solution. To tackle this complexity, we propose a heuristic approach named "assess-with-exclusion". We start by considering representation editing at each step of the entire diffusion process, gradually corrupting the process from \(t=T\) to \(t=1\). At each step, we evaluate whether the current corruption significantly impacts the generation of the unsafe concept \(c_{k}\). The confidence gap of the discriminator \(f_{k}\) before and after corruption serves as an indicator. If this gap is smaller than the predefined threshold \(_{k}\), it suggests that not performing representation editing in the current diffusion step minimally affects the removal of the unsafe concept \(c_{k}\). In such cases, we continue assessing whether the next step is crucial. If the gap exceeds \(_{k}\), we identify the current step as the starting step \(t_{s}\) of the causal period. Once \(t_{s}\) is determined, we conduct a similar backward search process from the last step \(t=1\) to identify the ending step \(t_{e}\) of the causal period. The pseudocode of algorithm for searching \(t_{s}\) and \(t_{e}\) is present in Appendix-A.

Given the Markovian nature of the diffusion process, we first search for \(t_{s}\) and exclude \([T,t_{s}+1]\), followed by the search for \(t_{e}\) and exclusion of \([t_{s}-1,1]\). Excluding \([t_{s}-1,1]\) at the second step does not affect the diffusion process before timestep \(t_{e}\). During the search for \(t_{s}\) and \(t_{e}\), the search can be terminated when the current timestep is identified as an important step for the first time. This is because once \(t_{s}\) is determined, the subsequent adjacent steps are likely to be influenced by it and are also likely to be important steps; similarly, once \(t_{e}\) is determined, the preceding diffusion step is likely to be an important step. The computational complexity of Algorithm 1 scales linearly with the total number of diffusion steps \(T\).

**Inference with CRE.** Our proposed causal representation editing, outlined in Appendix-B, comprises two main phases. Phase 1 involves discriminator training and causal period search for each category of unsafe concept, which can be conducted offline. During inference (Phase 2), if the reference image contains unsafe concepts, causal representation editing is applied within the cross-attention layers. Otherwise, the generated content remains faithful to the user-specified prompt without modification.

Experiments

In this section, we empirically evaluate the effectiveness of our proposed Causal Representation Editing (CRE). We use Kosmos-G  as the base model for concept transfer, comprising an MLLM as a prompt encoder and stable diffusion as an image decoder. Our approach is benchmarked against several baseline methods: Kosmos-G , Safe Latent Diffusion (SLD) , and ProtoRe . Additionally, we include an intuitive method, Kosmos-G-Neg, which manually adds negative prompts (e.g., "without Van Gogh style") behind the user prompt. To ensure experimental fairness, none of the comparison methods involve any fine-tuning of the generative model. For determining the causal period, we set \(_{k}\) to 0 for all types of unsafe concepts. We conduct all experiments on an RTX 3090 and an A100-80G.

**Safe Object Transfer.** We first evaluate our approach's performance in safe object transfer through quantitative analysis. We select one class from the ImageNet dataset as an unsafe concept and generate 500 images using the prompt "an image of a [_class name_]" with Stable Diffusion 2.1 . The guidance scale is set to 9.0. Following previous work [20; 23], we use a subset of ImageNet with ten easily recognizable classes as the targeted unsafe concepts. Using Kosmos-G, we create prompts in the form "[_image 1_] with [_image 2_]" to combine 500 images of each class with other images for object transfer. Here, [_image 1_] is a portrait, as people are commonly depicted with the ten targeted objects, and [_image 2_] is selected from the 500 images of each targeted class. We set the guidance scale to 7.5. Finally, we evaluate the top-1 classification accuracy of the transfer results using a pre-trained ResNet-50 ImageNet classifier.

In Table 1, we present quantitative results comparing the accuracy of safe object transfer using Kosmos-G and four safe generation methods. Each class's objects are considered unsafe concepts, and accuracy indicates the ratio of these objects included in the generated image. A lower accuracy signifies better safety in object transfer. The "Kosmos-G" row reports the accuracy of object transfer without any safe generation mechanism, serving as a baseline. Kosmos-G exhibits varying abilities to transfer different objects. Our experiments focus on evaluating whether the safe generation method effectively reduces the generation rate of corresponding unsafe concepts. Existing methods show certain limitations: Kosmos-G-Neg not only fails to achieve safe generation but also increases the probability of generating the corresponding object. We provide a comparison between images generated by Kosmos-G and Kosmos-G-Neg in Appendix-D. This anomaly suggests that the MLLM encoder struggles to interpret the explicit "without" command in the prompt. SLD adjusts the noise prediction of U-Net in diffusion models using auxiliary guidance, making it suitable for localized image detail retouching. However, its effectiveness in object removal appears limited. ProtoRe performs well in most categories but struggles when dealing with large objects (e.g., church) that occupy a significant portion of the image. In contrast, our proposed CRE method demonstrates superior unsafe concept removal capabilities across all categories. In addition, we undertake a test with the COCO-30k dataset with two images (the first one is about cassette and the other one is about Mickey Mouse, which could be found in Figure 2).

Figure 2: Qualitative results on COCO-30k dataset.

**Safe Style Transfer.** Table 2 presents quantitative results comparing the accuracy of safe style transfer using Kosmos-G and four safe generation methods. We selected four styles as unsafe concepts: Disney, Pencil Sketch, Picasso, and Van Gogh. We create our dataset and train a ResNet-50 classifier and a ViT-base classifier based on the dreambench dataset  for unsafe style transfer. This dataset comprises 158 images, all featuring simple objects and backgrounds, which facilitates successful style transfer. In terms of classification, 96.20% of the 158 original images in the dreambench dataset are classified as safe images by ResNet-50, and 94.94% are considered safe images by the ViT-base classifier. Further details on dataset construction, classifier training, and image style transfer processes are provided in Appendix-C. Compared to Table 1, the performance of both SLD and ProtoRe has declined to varying degrees, indicating that relying solely on text prompts to accurately describe unsafe concepts is inefficient in multi-modal zero-shot generation scenarios. Safe concept transfer based on representation editing, on the other hand, proves effective in removing both concrete objects and abstract styles.

Examples of unsafe concepts removal is shown in Figure 3. Kosmos-G can combine human portraits with other objects, and can also transfer artistic styles to images of dogs, ducks, glasses, etc. Existing methods are either ineffective when removing these unsafe concepts, or the removal is incomplete and leaves residues. Our approach is able to remove unsafe concepts without leaving any trace.

**Multiple Style Transfer.** To assess the scalability of our approach, we consider scenarios where multiple unsafe concepts may require governance simultaneously. We use Kosmos-G with the same prompts in the form of "[_image 1_] in the style of [_image 2_]" to transfer the images in Dreambench to the selected styles, in which [_image 1_] is an image in Dreambench and [_image 2_] represents one of the reference images for 4 unsafe styles. However, we replace the prompts with multiple style concepts for SLD ("without the style of Disney, Pencil sketch, Picasso and Van Gogh") and ProtoRe ("the style of Disney, Pencil sketch, Picasso and Van Gogh"). For CRE, we first use the classifier to judge whether the images in the prompts belong to unsafe concepts and which unsafe concept they belong to. If the image belongs to an unsafe style, we activate CRE for the unsafe prompt; If not, the prompt undergoes the normal Kosmos-G process. Finally, we evaluate the top-1 classification accuracy of the transfer results using the classifiers trained above.

    &  &  &  &  \\  & & single \(\) & multiple \(\) & \(\) & single \(\) & multiple \(\) & \(\) & single \(\) & multiple \(\) & \(\) \\   & Disney & 56.7098 & 87.7342 & +0.2533 & 47.5494 & 52.0868 & +4.4937 & 11.3924 & 11.8680 & **+0.4684** \\  & Pencil Sketch & 14.8301 & 16.0599 & +1.2658 & 12.9747 & 11.1392 & **+1.8355** & 0.6962 & 0.6329 & -0.6533 \\  & Pencas & 11.2658 & 13.8608 & +2.4955 & 3.6709 & 3.1013 & **+0.5606** & 0.3165 & 0.4433 & -0.1256 \\  & Van Gogh & 26.2658 & 30.9055 & +4.2405 & 27.7488 & 8.1646 & +5.3798 & 0.5696 & 0.5053 & **+0.653** \\   & Disney & 36.6465 & 36.9265 & +0.2869 & 29.557 & 34.7468 & -45.1898 & 1.3291 & 1.2658 & **+0.0633** \\  & Pencil Sketch & 10.5603 & 10.9494 & +0.4431 & 6.7722 & 6.7089 & **-0.0633** & 0.6329 & 0.6582 & -0.0253 \\  & Pencas & 153.1365 & 15.8228 & +0.5063 & 5.1899 & 5.1265 & **-0.6633** & 1.6456 & 1.5823 & **-0.6633** \\  & Van Gogh & 27.9114 & 31.7722 & +3.8608 & 3.2278 & 7.2785 & +4.0507 & 0.3797 & 0.6962 & **+0.3165** \\   & - & - & +1.9022 & - & - & +2.0728 & - & - & **+0.0854** \\   

Table 1: Quantitative results of safe object transfer.

    &  &  &  &  \\  & & single \(\) & multiple \(\) & \(\) & single \(\) & multiple \(\) & \(\) & single \(\) & multiple \(\) & \(\) \\   & Disney & 56.7098 & 87.7342 & +0.2533 & 47.5494 & 52.0868 & +4.4937 & 11.3924 & 11.8680 & **+0.4684** \\  & Pencil Sketch & 14.8301 & 16.0599 & +1.2658 & 12.9747 & 11.13922 & **+1.8355** & 0.6962 & 0.6329 & -0.6533 \\  & Pencas & 11.2653 & 13.8608 & +2.4955 & 3.6709 & 3.1013 & **+0.5606** & 0.3165 & 0.4433 & -0.1256 \\  & Van Gogh & 26.2658 & 30.9055 & +4.2405 & 27.7488 & 8.16466 & +5.3798 & 0.5696 & 0.5053 & **+0.6533** \\   & Disney & 36.6465 & 36.9265 & +0.2869 & 29.557 & 34.7468 & -45.1898 & 1.3291 & 1.2658 & **-0.0633** \\  & Pencil Sketch & 10.5603 & 10.9494 & +0.4431 & 6.7722 & 6.7089 & **-0.0633** & 0.6329 & 0.6582 & -0.0253 \\  & Pencas & 153.1365 & 15.8228 & +0.5063 & 5.1899 & 5.1266 & **-0.6633** & 1.6456 & 1.5823 & **-0.6633** \\  & Van Gogh & 27.9114 & 31.7722 & +3.8608 & 3.2278 & 7.2785 & +4.0507 & 0.3797 & 0.6962 & **+0.3165** \\   & - & - & +1.9022 & - & - & +2.0728 & - & - & **+0.0854** \\   

Table 2: Quantitative results of safe style transfer.

    &  &  &  &  \\  & single \(\) & multiple \(\) & \(\) & single \(\) & multiple \(\) & \(\) & single \(\) & multiple \(\) & \(\) \\   & Disney & 56.7098 & 87.7342 & +0.2533 & 47.54949 & 52.0868 & +4.4937 & 11.3924 & 11.8680 & **+0.4684** \\  & Pencil Sketch & 14.83010 & 16.0599 & +1.2658 & 12.9747 & 11.1392 & **+1.8355** & 0.6962 & 0.6329 & -0.6533 \\  & Pencas & 11.2658 & 13.8608 & +2.4955 & 3.6709 & 3.1013 & **+0.5606** & 0.3165 & 0.4433 & -0.1256 \\  & Van Gogh & 26.2658 & 30.905

[MISSING_PAGE_FAIL:9]

## 5 Limitation

We identify two primary shortcomings of CRE from two aspects: effectiveness and overhead. Firstly, the effectiveness of CRE is contingent upon the accuracy of the unsafe concept discriminator, represented by the term \( f_{k}(r)\) in Equation 4. If the discriminator's accuracy is low, CRE might perform representation editing even for safe prompts. As evidenced in Table 3 and Figure 4, as the number of unsafe concepts requiring simultaneous governance increases, the adverse impact of inadequate discriminator performance becomes more pronounced. Secondly, in comparison to safe generation methods that utilize fine-tuned diffusion models, representation editing introduces additional inference overhead. Nevertheless, since CRE is only applied in the cross-attention layer during a specific causal period, this additional overhead remains within a tolerable range. For instance, Kosmos-G requires 226 seconds to generate 100 images, and after incorporating CRE, the time increases to 246 seconds, resulting in an average increase of 0.2 seconds per image.

## 6 Conclusion

This paper proposes a novel approach, Causal Representation Editing (CRE), to address the challenges of unsafe concept transfer in large multi-modal diffusion models. By leveraging causal periods, CRE allows for precise and efficient removal of unsafe elements from generated images while preserving the integrity and quality of the generated content. Our comprehensive empirical evaluation highlights CRE's superiority over existing methods in both safe object and style transfer tasks. Specifically, CRE effectively reduces the presence of unsafe concepts, demonstrating its robustness across a variety of scenarios. Moreover, CRE exhibits strong scalability, maintaining consistent performance when managing multiple unsafe concepts simultaneously. This scalability is critical for real-world applications where the diversity and complexity of unsafe concepts can vary significantly. The ability of CRE to handle multiple unsafe concepts with minimal performance degradation ensures its applicability in dynamic and complex environments. In addition, CRE underscores the importance of representation-based interventions in generative models. Unlike methods that rely heavily on textual descriptions for unsafe concepts, CRE's representation editing approach proves to be more adaptable and effective, especially in multi-modal zero-shot generation scenarios. Overall, CRE represents a significant advancement in safe concept transfer, offering a robust, scalable, and effective solution for mitigating unsafe content.