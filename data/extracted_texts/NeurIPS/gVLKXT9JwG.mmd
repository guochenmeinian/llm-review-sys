# Global Convergence Analysis of Local SGD for

Two-layer Neural Network without

Overparameterization

Yajie Bao\({}^{1}\)   Amarda Shehu\({}^{2}\)   Mingrui Liu\({}^{2}\)

\({}^{1}\)School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, 200240

\({}^{2}\)Department of Computer Science, George Mason University, Fairfax, VA 22030

baoyajie2019stat@sjtu.edu.cn, {ashehu,mingruil}@gmu.edu

Corresponding Author.

###### Abstract

Local SGD, a cornerstone algorithm in federated learning, is widely used in training deep neural networks and shown to have strong empirical performance. A theoretical understanding of such performance on nonconvex loss landscapes is currently lacking. Analysis of the global convergence of SGD is challenging, as the noise depends on the model parameters. Indeed, many works narrow their focus to GD and rely on injecting noise to enable convergence to the local or global optimum. When expanding the focus to local SGD, existing analyses in the nonconvex case can only guarantee finding stationary points or assume the neural network is overparameterized so as to guarantee convergence to the global minimum through neural tangent kernel analysis. In this work, we provide the first global convergence analysis of the vanilla local SGD for two-layer neural networks _without overparameterization_ and _without injecting noise_, when the input data is Gaussian. The main technical ingredients of our proof are _a self-correction mechanism_ and _a new exact recursive characterization of the direction of global model parameters_. The self-correction mechanism guarantees the algorithm reaches a good region even if the initialization is in a bad region. A good (bad) region means updating the model by gradient descent will move closer to (away from) the optimal solution. The main difficulty in establishing a self-correction mechanism is to cope with the gradient dependency between two layers. To address this challenge, we divide the landscape of the objective into several regions to carefully control the interference of two layers during the correction process. As a result, we show that local SGD can correct the two layers and enter the good region in polynomial time. After that, we establish a new exact recursive characterization of the direction of global parameters, which is the key to showing convergence to the global minimum with linear speedup in the number of machines and reduced communication rounds. Experiments on synthetic data confirm theoretical results.

## 1 Introduction

Federated learning is a prevalent framework in distributed learning to significantly reduce the communication cost and effectively preserve the privacy of local clients . As the most popular algorithm in federated learning, local SGD has shown great empirical success in training deep neural networks (DNNs) . However, existing literature has not been able to fully explain or characterize the convergence of local SGD in training DNNs. Recently, extensive works are devoted to analyzing the convergence of local SGD and its variants in nonconvex optimization . However, traditional nonconvex analysis only guarantees convergence to a stationary point, and convergence to the global minimum is in general NP-hard .

Despite the NP-hardness results for nonconvex optimization, an increasing body of research tries to address structured nonconvex optimization by first-order methods with noise injection. For instance, Ge et al.  considered strict-saddle functions and showed that SGD with isotropic noise can find local minima in polynomial time. This motivated several ensuing works  on designing different first-order algorithms to improve convergence to local minima by injecting noise. Noise-injecting schemes and their variants (such as, for instance, broadening from isotropic to anisotropic noise ) were shown to help convergence to global minima for many problems that satisfy one of two conditions: (i) local minima are global minima, as in matrix completion , dictionary learning , and certain deep linear networks ; or (ii) neural networks with distributional assumption, such as two-layer neural networks with the Gaussian input . It is worth noting that there is also a rich history on noisy GD based on Langevin dynamics (LD) . For instance, recent work  proposes Exponential Family Langevin Dynamics (EFLD) to relax the Gaussian noise assumption and include noisy sign-SGD and variants of drop-out as special cases. When assuming the neural network is overparameterized, neural tangent kernel (NTK) analysis  guarantees convergence to the global minimum for local SGD . However, the NTK theory is far from sufficient, since neural networks outperform their NTK counterpart in practice  and in theory .

Despite existing global convergence analyses of first-order methods for solving nonconvex optimization problems such as neural networks, they either require explicitly injecting noise or assume overparameterization such that NTK analysis can apply. Practical federated learning algorithms such as local SGD do not inject any noise and do not belong to the NTK regime, but they can still converge to global minima. For example, McMahan et al.  shows that the local SGD algorithm can achieve around \(99\%\) accuracy when training neural networks for an image classification task. This motivates us to study the following question in this paper:

**Is it possible to formally prove that the local SGD algorithm can find global minima for two-layer neural networks without injecting noise and without overparameterization?**

In this paper, we give a positive answer to this question under Gaussian input. We are inspired by a line of work on neural network learning theory with Gaussian input  and, in particular address the distributed version of the setting in . Suppose \(N\) local machines share the following network:

\[f(,,)=_{j=1}^{k}a_{j}(_{i}^{} {w}),\]

where \(=(_{1},,_{k})^{d k}\) is the input matrix, \(^{d}\) is the weight, \(^{k}\) is the output weight, and \((x)=\{x,0\}\) denotes the ReLU activation function. A good property of this network is positive homogeneity, i.e., \(f(,c,/c)=f(,,)\) holds for any \(c>0\). We assume each entry of the input \(\) is independently sampled from a standard Gaussian distribution. Then the response is generated by a noiseless teacher network: \(y=f(,^{*},^{*})\). Without loss of generality, we further assume \(\|^{*}\|=1\). We hope to learn a student network by collaboratively minimizing the following mean square loss among \(N\) local machines:

\[L(,)=[(y-f(,, ))^{2}]=[(f(,^{*}, ^{*})-f(,,))^{2}]. \]

Obviously, \((^{*},^{*})\) is the global minimum of the objective (1) with zero loss. In particular, the loss function also has a spurious local minimum; hence, minimizing loss is a nonconvex optimization problem. Please refer to work in  for further details on the landscape of \(L(,)\).

Despite the special input distribution, to the best of our knowledge, there is currently no work demonstrating the global convergence of vanilla SGD or vanilla local SGD without overparameterization. Work in  proved that randomly initialized GD can converge to the global minimum or the local minimum with a constant probability. The initial region where GD can converge to the global minimum is also called the attraction basin, where the gradients of two layers both point in the correct directions to the ground truth. To obtain the global convergence with arbitrary initialization in the same initial region, Zhou et al.  proposed a new perturbed GD algorithm by carefully injecting noise to the weights in two layers. Although the convergence of vanilla SGD has not been exploredtheoretically, the simulation results in [13; 63] show that vanilla SGD with random initialization can converge to the global minimum with probability \(1\) when the ratio \(|^{}^{*}|/\|^{*}\|\) is large. This result motivates us to investigate the global convergence of local SGD without injecting additional noise to escape the local minimum.

In this paper, we analyze the global convergence of the vanilla local SGD for training a two-layer neural network with Gaussian input, whose initialization starts from the same initial region as in [13; 63]. Formally, our main contributions are summarized as follows:

1. We introduce a new self-correction mechanism of local SGD under a condition on \(^{*}\) (see Assumption 1): the signals from two layers can be corrected in _polynomial_ time even though the initial point comes from a bad region where the gradients point to the wrong direction of \((^{*},^{*})\). The condition also explains the simulation results in . The self-correction process is very difficult to analyze due to the mutual influence effect of two layers. To address this challenge, we utilize a novel technique by carefully dividing the landscape of the objective (1) into several regions. In each region, the negative effect from one layer to another layer can be controlled to a negligible scale. We notice that Li and Yuan  also showed the self-correction phase of SGD for the two-layer network under Gaussian input. However, the network's structure in  is different from the network studied in this paper. In addition, Li and Yuan  also required bounding the noise of stochastic gradient and so cannot handle the vanilla SGD with Gaussian noise as in our case.
2. We show the global convergence of local SGD with linear speedup, which indicates that the iteration complexity is divided by the number of machines \(N\). In addition, we also show that the communication complexity of local SGD is reduced compared with the naive parallel version of SGD which needs to communicate at every iteration. The analysis in the convergence stage is very different from the GD in Du et al. . We establish a new recursive dynamic to characterize the direction of the _global weight_ in the first layer. Moreover, the objective is not smooth, since the gradients incorporate the angle between the first layer's weight and the ground truth. Therefore, conventional analysis of local SGD for a general smooth objective [47; 54] cannot be applied in the convergence stage. Due to the inner structure of gradients under Gaussian input, we find that the discrepancy caused by the local updates can shrink as the angle decreases, which enables us to refine the bound of discrepancy to be dominated by the statistical bound of noise.
3. We conduct several simulations on the two-layer neural network to verify the theoretical results. The experiments demonstrate that local SGD indeed corrects the wrong signals from the initial point and exhibits speedup in the convergence stage, corroborating our theoretical results. The simulation results also verify that the condition imposed on \(|^{}^{*}|/\|^{*}\|\) is almost necessary to show the convergence with almost arbitrary initialization.

## 2 Related Work

Federated OptimizationThere is a wave of studies on federated optimization in different settings. In the convex optimization setting with homogeneous data, one-shot averaging was studied [65; 42; 60], where each machine solves a local optimization problem and the average happens only at the last iterate. Local SGD skips communication rounds, and the convergence analysis is shown for convex [47; 11; 34; 28; 54; 53; 32; 31] and nonconvex optimization problems [62; 24; 51; 39; 20; 56; 34; 28; 45; 59; 32]. There is a line of work which tried to compare minibatch SGD and local SGD in federated learning [54; 53]. However, these optimization algorithms only work for black-box functions and do not utilize the property of neural networks. As neural network loss landscapes are typically nonconvex, these federated optimization algorithm can only guarantee to find a stationary point instead of a global minimum.

Optimization Theory for Neural NetworksThere is a line of work studying two layer neural networks with Gaussian input [49; 12; 37; 61; 7; 19; 5]. Li and Liang  studied two layer neural networks with cross-entropy loss and showed that SGD can find the global minimum when the neural network is overparameterized. Du et al.  proved that GD can find the global minimum for two layer overparameterized neural networks under \(_{2}\) loss. These results are later extended to deep neural networks by [14; 3; 66; 2] but are not directly applicable to analyzing local SGD in the distributed setting.

Federated Learning on Neural NetworksThere is a line of work which studied federated learning algorithms on overparameterized neural networks under the NTK regime [35; 22; 10; 58; 57]. In contrast, our analysis does not fall in the NTK regime: we directly study the dynamics of local SGD over neural networks without overparametrization.

## 3 Notations and Problem Setup

Denote \(\|\|\) the Euclidean norm and \(,\) by the inner product. \(^{d-1}\) denotes the \(d\)-dimensional unit sphere and \(^{k}()\) denotes the \(k\)-dimensional ball with center zero and radius \(\). For two vectors \(,^{d}\), denote \((,)[0,]\) the angle between \(\) and \(\). The uniform distribution is denoted by \(()\). Moreover, we use \(\) to hide logarithmic factors.

We adopt the weight-normalization technique  to the first layer by re-parametrizing \(=/\|\|\), which leads to the following prediction model:

\[f(,,)=_{j=1}^{k}a_{j}_{i}^{ })}{\|\|}. \]

Given any sample \((,y)\), we denote the empirical loss by

\[(,;,y)=(y-f(,,))^{2 }=(f(,^{*},^{*})-f(,, {a}))^{2}. \]

In the distributed environment, suppose there are \(N\) local machines sharing the same teacher model \(f(,^{*},^{*})\). It means that given any local input \(^{i}\), the response is \(y^{i}=f(^{i},^{*},^{*})\). Let \(=\{t_{0},...,t_{R}\}\) be the set of synchronization time, where \(t_{0}=0\), \(t_{R}=T\) and \(t_{r+1}-t_{r}=I\) for any \(r\). The detailed procedure of local SGD is presented in Algorithm 1, where the initial point is from the same region in Du et al.  and Zhou et al. : \(_{0}^{d-1}\) and \(_{0}^{k}(|^{}^{*}|/)\). In each round, the \(i\)-th machine runs \(I\) steps of SGD using the stochastic gradients \(_{}(_{t}^{i},_{t}^{i};_{t}^{i},y_{t}^{i})\) and \(_{}(_{t}^{i},_{t}^{i};_{t}^{i},y_{t}^{i})\) computed by the local input \((_{t}^{i},y_{t}^{i})\). At the end of a round, the server aggregates local weights to obtain the global weight and then synchronizes the global weight to each machine.

```  Initialize \(_{0}^{d-1}\) and \(_{0}^{k}(^{}^{*}|}{})\). for\(r=0,,R-1\)do for\(i=1,,N\)do  Synchronization: \(_{t_{r}}^{i}_{t_{r}}\) and \(_{t_{r}}^{i}_{t_{r}}\). for\(t=t_{r},,t_{r+1}-1\)do  Sample \(_{t}^{i}\) from the standard Gaussian distribution and generate the response \(y_{t}^{i}\).  Update \(_{t+1}^{i}=_{t_{t}}^{i}-_{}(_{t}^{i}, {a}_{t}^{i};_{t}^{i},y_{t}^{i})\).  Update \(_{t+1}^{i}=_{t}^{i}-_{}(_{t}^{i}, _{t}^{i};_{t}^{i},y_{t}^{i})\). endfor endfor  Update \(_{t_{r+1}}=_{i=1}^{N}_{t_{r+1}}^{i}\) and \(_{t_{r+1}}=_{i=1}^{N}_{t+1}=_{t}-_{i=1}^{N}_{t}^{i}_{ {v}}L(_{t}^{i},_{t}^{i};_{t}^{i}),_{t+1}=_{t}-_{i=1}^{N}_{}L(_{t}^{i},_{t}^{ i};_{t}^{i}).\]

For ease of technical presentation, we denote the averaged noise terms by \(_{t}=_{i=1}^{N}_{t}^{i}_{t}^{i}\) and \(_{t}=_{i=1}^{N}_{t}^{i}\), where \(_{t}^{i}=_{}L(_{t}^{i},_{t}^{i};_{t} ^{i})-_{}L(_{t}^{i},_{t}^{i})\) and \(_{t}^{i}=_{}L(_{t}^{i},_{t}^{i}; _{t}^{i})-_{}L(_{t}^{i},_{t}^{i})\) are the local noises in stochastic gradients. In addition, for the iterates \(_{t}^{i}\) and \(_{t}\), we write \(_{t}^{i}=(_{t}^{i},^{*})\) and \(_{t}=(_{t},^{*})\), respectively.

### Exact Dynamic of Each Layer

In this subsection, we will give the exact dynamic of each layer in the training process of local SGD, which is the starting point of our analysis. Denote \(_{t}=_{t}\|}(-_{t}_{t}^{})\) by the global projection matrix, where \(_{t}=_{t}/\|_{t}\|\). The proofs of this subsection are deferred to Appendix A.1.

**Lemma 1**.: _Let \(}_{t+1}=_{t}-(_{t}_{i=1}^ {N}_{}L(_{t}^{i},_{t}^{i})+_{t})\) and \(_{t}=(}_{t},^{*})\). The first layer in local SGD satisfies that_

\[\|_{t+1}\|^{2}^{2}_{t+1}=(1-_{t}_{t})^{2}\| _{t}\|^{2}^{2}_{t}-2 M_{1,t}+^{2}M_{2,t}+H_{t}, \]

_where \(_{t}=_{i=1}^{N}^{i}(_{t}^{i })^{}^{*}}{2}_{t}^{i})^{}^{*}}{ \|_{t}\|^{2}}\), \(H_{t}=\|_{t+1}\|^{2}^{2}_{t+1}-\|}_{t+1}\|^{2}^{2 }_{t+1}\) and_

\[M_{1,t}=(_{t}-_{t}_{i=1}^{N}_{ {w}}L(_{t}^{i},_{t}^{i}))^{}(-^{*}( ^{*})^{})_{t},\;M_{2,t}=_{t}^{}(-^{*}(^{*})^{})_{t}.\]

Lemma 1 is crucial to control the angle and show the linear speedup in further analysis. For each local machine, Lemma 5.5 in  provides the dynamic of \(^{2}_{t}^{i}\), which cannot characterize the dynamic of global quantity \(_{t}\) due to the nonlinearity. Here we introduce a new _intermediate variable_\(}_{t+1}\) and find an equality (4) to show the recursive relation between \(\|_{t+1}\|^{2}^{2}_{t+1}\) and \(\|_{t}\|^{2}^{2}_{t}\) through the global quantities, such as \(M_{1,t}\) and \(M_{2,t}\). It is worthwhile noticing that \(M_{1,t}\) is the averaged noise in local SGD, whose variance is divided by the number of clients \(N\), namely _linear speedup term_. In fact, we can control the dynamic of \(_{t}\) by upper bounding the _discrepancy term_\(H_{t}\). Let us assume the last three terms in (4) are negligible and \((_{t}^{i})^{}^{*}>0\) for any \(i[N]\):

1. When \(_{t}>/2\), \(\|_{t}\|^{2}^{2}_{t}\) will continuously increase since \(_{t}_{t}<0\). It indicates that \(_{t}\) can decrease to \(/2\) if \(\|_{t}\|\) is upper bounded by a constant, which also means the first layer can be corrected and avoided converging to the spurious local minima.
2. When \(_{t}</2\), \(\|_{t}\|^{2}^{2}_{t}\) will continuously decrease to zero. It indicates that \(_{t}\) can converge to zero if \(\|_{t}\|\) is lower bounded by a constant. The initial region with \(_{0}</2\) and \(_{0}^{}^{*}>0\) is also called the attraction basin in .

Through the remarks above, we can see that the _positive_ signal of the second layer is crucial to both the self-correction of the first layer and global convergence. Next lemma presents an exact dynamic of the averaged weight in the second layer.

**Lemma 2**.: _Let \(A_{0}=|^{}^{*}|^{2}-(^{}^{*})( ^{}_{0})\) and \(g()=(-)+\). For local SGD algorithm started with the initial point \((_{0},_{0})\), the second layer satisfies that_

\[_{t}^{}^{*}= (1-)^{t}_{0}^{} ^{*}+)^{t}}{k} (1-)^{t}A_{0}\] \[+^{*}\|^{2}}{2}_{s=0}^{t-1}(1- )^{t-1-s}_{i=1}^{N}B_{s}^{i}+S(_{0:t-1}), \]

_where \(S(_{0:t-1})\) (defined in Appendix A) is the noise term involving \(_{0},,_{t-1}\) and_

\[B_{s}^{i}=g(_{s}^{i})-1+^{}^{*}|^ {2}}{\|^{*}\|^{2}}_{l=0}^{s-1}(1-)^{s -1-l}(-g(_{l}^{i})).\]Notice that the first term \(g(_{s}^{i})-1\) in \(B_{s}^{i}\) is negative whenever \(_{s}^{i}</2\), but the second term in \(B_{s}^{i}\) is always positive for \(_{s}^{i}(0,]\). In particular, \(-g(_{t}^{i})\) tends to be larger when \(_{t}^{i}\) gets closer to \(\) (i.e., \(_{t}^{i}\) drifts away from \(^{*}\)). This insight provides a possibility that local SGD can correct the signal of the second layer by itself, instead of injecting additional noise like .

Through carefully inspecting the ingredients of dynamics, we have the following roadmap to show the global convergence of local SGD: (1) For arbitrary initialization \((_{0},_{0})\) except for a measure zero set, where the angle of the first layer between initialization and the global minimum is \(\), show that \(_{t}^{}^{*}\) can turn to the positive signal in polynomial time; (2) Show that \(_{t}^{}^{*}\) can be lower bounded by a positive constant value and \(_{t}\) will decrease below \(/2\). (3) After entering the attraction basin, show that \((_{t},_{t})\) will converge to the ground truth with a linear speedup guarantee.

### Self-correction of Signals in Two Layers

In this subsection, we will show the iterates of local SGD can enter the attraction basin such that \(_{t}^{}^{*}>0\) and \(_{t}</2\) after \((^{-1})\) steps. Before that, we introduce an assumption on \(^{*}\).

**Assumption 1**.: _Define \(=|^{}^{*}|^{2}/(k\|^{*}\|^{2})\). We assume \(k 320(-1)^{2}\) and the ground truth satisfies \(|^{}^{*}|^{2})}\) and_

\[>(1-)^{-1}. \]

The conditions on \(k\) and \(|^{}^{*}|^{2}\) are imposed for technical reasons. We believe that a constant lower bound (the right-hand side of (6)) for \(\) is necessary to show the global convergence with almost arbitrary initialization, which is also verified by our simulation results in Table 1. We compute convergence probabilities of local SGD and minibatch SGD under different values of \(\). The initial points are randomly selected by \(_{0}(^{d-1})\) and \(_{0}(^{k}(|^{}^{*}|/ {k}))\) in each trial. If the convergence probability reaches 1, it means that local SGD or minibatch SGD can converge to the global minima with arbitrary initialization except for a measure zero set. When \( 1/8\), both minibatch SGD and local SGD _cannot_ converge to the global minima with probability 1.

**Theorem 1** (Self-correction of the second layer).: _For any initial point \((_{0},_{0})\) with \(_{0}[0,)\), we denote \(_{a}=\{t 0:_{t}^{}^{*}_{a}\}\) the first time, where \(_{a}=^{}^{*}|^{2}}{k(+k-1)}\). Under Assumption 1, if_

\[ck^{2}\{(+I),(+I), \}^{*}\|^{2}}{|^{}^{*}|^{2}}<1, \]

_for a sufficiently large constant \(c\), then \(_{a} O(^{-1} k)\) with probability at least \(1-\)._

This theorem completes the first step of the roadmap, the self-correction of the signal in the second layer, whose proof is given in Appendix D.1. If \(|^{}^{*}|^{2}/\|^{*}\|^{2} 1/()\), Du et al.  proved GD can converge to the spurious local minima with the initial condition \(_{0}^{}^{*}<0\) and \(_{0}>/2\). Therefore, our results do not contradict theirs because we assume \(|^{}^{*}|^{2}/\|^{*}\|^{2}\) is large. The simulations of [13; 63] show that the success probability of converging to the global optimum of SGD increases as the ratio \(|^{}^{*}|/\|^{*}\|\) increasing. Theorem 1 can potentially explain this phenomenon since the condition (6) will be satisfied eventually when \(|^{}^{*}|/\|^{*}\|\) keeps increasing.

**Proof sketch of Theorem 1.** The proof is very technical since the angle \(_{t}^{i}\) will affect the sign of \(B_{s}^{i}\) in the dynamic (5) of \(_{t}^{}^{*}\), while controlling \(_{t}^{i}\) also requires bounding the scale and controlling

[MISSING_PAGE_FAIL:7]

### Convergence with Linear Speedup

With the correction guarantees in the previous subsection, we are ready to proceed with the convergence analysis of local SGD.

**Theorem 3**.: _Suppose the initial point \((_{0},_{0})\) satisfies \(_{0}^{}^{*}_{a}/32\) and \(_{0}^{l}\). For any \(>0\), we choose \(=\|^{*}\|^{2}}}{d(dN/)}\) for some absolute constant \(c>0\). If \(I\{1,d^{1/2}}{N^{1/2}},d}{N^{4}}\}\) and \(<\{d^{-1},dk^{-2}\}\), then \((_{T},_{T})\) holds with probability at least \(1-\) where \(T=(}{N})\)._

We have the following implications about the result in Theorem 3:

1. To the best of our knowledge, this is the first convergence result with linear speedup on the number of machines \(N\) for two-layer neural networks. Besides, our convergence analysis does not rely on the overparameterization of the width of the second layer (i.e., \(k\)).
2. The dependency on \(\) matches the best-known results of local SGD for strongly convex objective in Woodworth et al. . In fact, the size of the first layer \(d\) resembles the variance of stochastic gradients \(^{2}\) in the traditional optimization literature. We can show \(_{}L(_{t},_{t}),_{t}-^{*} \|_{t}-^{*}\|^{2}-O(_{t})\|^{*}\|^{2}\). Therefore, when the first layer converges has converged (\(_{t} 0\)), \(_{t}\) can converge to \(^{*}\) like the strongly convex regime.
3. According to the condition on \(I\), we can obtain the communication complexity in the convergence stage as \[R_{}==(\{k^{4},k^{2}},}{d}\}).\] (11) When \(N\{dk^{4},^{-1}\}\), the communication complexity in this stage can be \(R_{}=(k^{4})\), which is significantly reduced compared with the iteration complexity.

We show local SGD's convergence layer by layer. Next lemma ensures that the weight of the first layer can converge to the ground truth in polynomial time.

**Lemma 4** (Convergence of the first layer).: _Under the settings in Theorem 3. Suppose the initial point satisfies \(_{0}^{l}\) and \(_{0}^{}^{*}_{a}/32\). With probability at least \(1-\), we can guarantee that \(^{2}_{t}\) holds for any \((k^{2}^{-1}) t(^{-2})\)._

It is worthwhile noticing that \((_{_{v}},_{_{v}})\) in Theorem 2 satisfies the conditions for initial point in Lemma 4. Therefore, local SGD enters the attraction basin after finishing the self-correction process. In fact, showing the complexity \((^{-1})\) is not trivial based on the traditional analysis of local SGD . The issue comes from the inner-product noise term \(M_{1,t}\) and the discrepancy term \(H_{t}\) in Lemma 1, whose enrolled summations can only be bounded by \(O()\) and \(O\{(+I)\}\) respectively at the beginning of convergence stage. Thanks to the special structure of gradient, we find that the scales of \(M_{1,t}\) and \(H_{t}\) can shrink as \(_{t}\) decreases. In light of this, we can continuously refine the bound by the following contraction: for \((K+1)T_{v} t(^{-2})\) it holds that

\[^{2}_{t}\{(}+( +I))^{1+},\}.\]

By taking \(K=O((1/))\), we can obtain the target convergence \(^{2}_{t}\) with high probability. More details can be found in Appendix E.1.

**Lemma 5** (Convergence of the second layer).: _Under the choice for \(\) and conditions for \(\) in Theorem 3. Suppose \(^{2}_{t}\) holds for any \(0 t(^{-2})\). With probability at least \(1-\), we can guarantee that \(\|_{t}-^{*}\|^{2}\) holds for any \((^{-1}) t(^{-2})\)._

The lemma stated above guarantees the convergence of the second layer after that of the first layer, whose proof is deferred to Appendix E.2. Equipped with Lemmas 4 and 5, we can prove Theorem 3 by leveraging the closed form of the objective \(L(,)\) (see Lemma 6).

The perturbed GD method in Zhou et al.  requires manually adjusting the scale of injected noises and the learning rate between the transition of two phases. In local SGD, we can use a _universal_ learning rate in the self-correction stage and convergence stage. Considering an initial point that is closer to spurious local minima (\(_{0}>^{u}\)) defined in Lemma 3, conditions (7), (8) and (9) on the learning rate can be satisfied if we choose \(I=O\{d/( N)\}\). Together with (10) and (11), if \(N\{dk^{4},^{-1}\}\), we can get the total communication complexity

\[R_{}=(}{^{3}_{0}}).\]

Therefore, our theory shows that local SGD can correct the signals and converge to the global minima with almost arbitrary initialization except for the case of initializing at the local minima (\(_{0}=\)).

## 5 Experiments

We now report some simulation results on synthetic data. We also compare the performance of two algorithms: local SGD and minibatch SGD. At each round, minibatch SGD updates the model weights by using the stochastic gradients with batch size \(NI\) in total to update the model, where each local machine computes \(I\) gradients and communicates with other machines. Minibatch SGD and local SGD have the same computation and communication structure .

In our first simulation, we set \(\|^{*}\|=\|^{*}\|=1\) and \(|^{}^{*}|=\). The results starting from three bad initial regions (i.e., \(_{0}>/2\) or \(_{0}^{}^{*} 0\)) are reported in Figure 1. As we can see, the signals of two layers can be both corrected to a good region (i.e., both \((_{t})\) and \(_{t}^{}^{*}\) go to \(1\) and the loss goes to \(0\) when \(t\) increases) even in the worst case when \(_{0}=-^{*}\) and \(_{0}>/2\). An interesting phenomenon is that local SGD can correct the signals faster than minibatch SGD. The reason is that the statistical error of stochastic gradients is not dominating in the self-correction process, so the effect of large batch size in minibatch SGD (i.e., with batch size \(NI\) for \(1\) iteration) is not as useful as smaller batch size in local SGD with more iterations (i.e., batch size \(1\) for \(I\) iterations on each machine with \(N\) machines in total). These results corroborate our theoretical analysis.

Figure 1: Converged trajectories of local SGD and minibatch SGD with different bad initial points. The dimensions of the two layers are \(k=10\) and \(d=25\). The number of skipped communication is \(I=8\). The batch size is 4. The two algorithms’ learning rates are \(=0.1\).

In the second simulation, we plot the trajectories of local SGD and minibatch SGD under different values of \(\) in Figure 2. Here we set \(^{*}=_{d}/\) and \(^{*}=(_{ k},0,,0)/\). The bad initial point is fixed as \(_{0}=(-1,0,,0)\) and \(_{0}=(-,0,,0)\), where \(_{0}>/2\) and \(_{0}^{}^{*}<0\). When \(=1/32,1/16\), we can see that minibatch SGD and local SGD converge to the local minima. When \(=1/4\), they can converge to the global minima with the same initial point. In this case, as we can see from Figure 2(c), the signals of two layers can be both corrected to a good region.

In the third simulation, we calculate the probabilities that local SGD and minibatch SGD converge to the global minimum under different values of \(\). The averaged results are taken over 100 independent repeated simulations. In each trial, we generate initial points by \(_{0}(^{d-1})\) and \(_{0}(^{k}(|^{}^{*}|/ {k}))\). The results are given in Table 1.

## 6 Conclusion

We theoretically investigate the convergence of local SGD, a cornerstone algorithm in federated learning with strong empirical performance. We demonstrate convergence to the global minimum for two-layer neural networks _without overparameterization_, _without injecting noise_, and when the input data is Gaussian. A new self-correction mechanism guarantees the algorithm reaches a good region even if the initialization is in a bad region. The landscape of the objective is divided into several regions to carefully control the interference of the two layers during the correction process. A new exact recursive characterization of the direction of global parameter provides the key to show convergence to the global minimum with linear speedup in the number of machines and reduced communication rounds. Experiments on simulated data corroborate the theoretical results. To the best of our knowledge, this work is the first to theoretically demonstrate the global convergence of the vanilla local SGD for neural networks without overparameterization.

Figure 2: Trajectories of local SGD and minibatch SGD under different values of \(\). The dimensions of the two layers are \(k=64\) and \(d=25\). The number of skipped communication is \(I=8\). The batch size is 16. The two algorithms’ learning rates are \(=0.1\).