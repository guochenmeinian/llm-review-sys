# Transformers to Predict the Applicability of Symbolic Integration Routines

Rashid Barket

Coventry University

barketr@coventry.ac.uk

&Uzma Shafiq

Coventry University

shafiqu9@coventry.ac.uk

Matthew England

Coventry University

Matthew.England@coventry.ac.uk

&Jurgen Gerhard

Maplesoft

jgerhard@maplesoft.com

###### Abstract

Symbolic integration is a fundamental problem in mathematics: we consider how machine learning may be used to optimise this task in a Computer Algebra System (CAS). We train transformers that predict whether a particular integration method will be successful, and compare against the existing human-made heuristics (called guards) that perform this task in a leading CAS. We find the transformer can outperform these guards, gaining up to 30% accuracy and 70% precision. We further show that the inference time of the transformer is inconsequential which shows that it is well-suited to include as a guard in a CAS. Furthermore, we use Layer Integrated Gradients to interpret the decisions that the transformer is making. If guided by a subject-matter expert, the technique can explain some of the predictions based on the input tokens, which can lead to further optimisations.

## 1 Introduction

Symbolic integration is well-studied, proven to be undecidable (see e.g. discussion in Chapter 22 in Gerhard and Von zur Gathen (2013)). The most widely applicable method is the Risch algorithm (Risch (1969)). However, the original algorithm does not work with special functions, and implementations of this algorithm has trouble with algebraic extensions. It is also complicated, taking more than 100 pages to explain in (Geddes et al. (1992)): no Computer Algebra System (CAS) has a full implementation. Hence, researchers in CA have designed a variety of other symbolic implementation methods.

More recently have there been attempts to use Machine Learning (ML) to perform symbolic integration. First, Lample and Charton (2020) implemented a transformer to directly integrate an integrand, outperforming several mature CASs on the generated test data. Further attempts have been made using LLMs (Noorbakhsh et al. (2021)), and by chaining explainable rules (Sharma et al. (2023)). More generally, there has been work developing a single LLM to perform a variety of mathematical reasoning tasks including (in)definite integration such as (Drori et al. (2022), Hendrycks et al. (2021)).

We are interested in improving the efficiency of indefinite integration within a CAS using ML, while still ensuring that correctness of the answer is guaranteed, through ML-based optimisation and algorithm selection. Recent work applied TreeLSTMs to show there is room for significant performance improvements (Barket et al. (2024)). We focus on the popular commercial CAS, Maple. Maple's user-level integration call is essentially a meta-algorithm: it can employ several different **methods** for symbolic integration. They are currently tried in a deterministic order until one succeeds, at which point the answer is returned without trying the other methods. A key part of thisimplementation is what we call the **guards**: code that is run prior to calling one of the methods to decide whether or not attempting to use the method is worthwhile. The reasoning behind having a guard is that some of the methods are computationally expensive: it is a waste of time to go through a complex algorithm just to find out that it fails. Figure 1 gives a high-level overview of this idea.

Not all methods have a guard, and some guards do partial work for the method making them expensive for a quick check. Appendix A gives a list of methods and their guard (should they exist). We create small ML models to predict success for methods and investigate whether these can provide guards for methods that lack them, or replace the computationally expensive guards (provided they do better). We also investigate whether interpretations of these models might inform further guard development.

## 2 Dataset

Our dataset is composed of integrable expressions in their prefix notation. The data comes from six different data generators: the FWD, BWD, and IBP generators from Lample and Charton (2020); the RISCH generation method from Barket et al. (2023); the SUB generation method from Barket et al. (2024); and the LIOUVILLE generation method from Barket et al. (2024). For more information see Appendix B.1. We obtain the labels for each integrand by recording which methods succeed (1) and which fail (0) for each expression. We note that this labelling is considerably more computationally expensive than the ML training later. The data is stored as a list where, for position \(i\) in the list, the value records if method \(i\) is a success or failure.

The data goes through several pre-processing steps to shrink the vocabulary size and to avoid having expressions of similar form over-represented in the dataset, as described in Appendix B.2. In total, there are 1.5M and 60K samples for training and testing respectively, with an equal split from each data generator. The labels do not occur equally however: some methods have a much higher rate of success than others. This is because some of the methods are only made for certain types of integrands (e.g. Elliptic) whereas others are more general-purpose (e.g. Risch). Figure 2 shows the frequency of integrating successfully over the train set from each method.

Figure 1: A high-level overview of how the indefinite integral command works in Maple to calculate \(F(x)= f(x)dx\).

Figure 2: Frequency of each method being successful on 1.5M examples from six data generators.

## 3 Training Transformers to Predict Method Success

### Experiment Setup

For each of the methods in Maple2024, we train a classifier to determine whether it is worthwhile attempting said method. We use an encoder-only transformer Vaswani2017 to predict success or failure, with architecture similar to sample and charton2020 and charton2023. Hyper-parameters and the full model architecture are discussed in Appendix C. The results are then compared to the guards implemented by human experts in Maple. In this scenario, a false positive is considered worse than a false negative. This is because trying a method and failing wastes compute time, whereas skipping an algorithm that may succeed may be okay because another method could possibly succeed (and if none do, we could always then try the guarded ones at the end). Thus, we evaluate accuracy and precision when comparing each transformer to each guard.

### Results

Table 1 shows the results for methods that do _not_ have a guard. In this case, the transformers are compared to a hypothetical guard that simply predicts a positive label every time, since the current workflow would always attempt such methods. These transformers range in accuracy from \(93\%\) to \(98\%\). Hence, there is much scope for using ML to prevent wasteful computation. An example of such a case is for Risch and Norman: both have an operation, Field, that creates the base field along with its field extensions before proceeding with the algorithm. This operation takes nearly \(20\%\) of their runtime before determining whether to run the rest of the algorithm or output fail. On a batch of 1024 examples, the mean transformer runtime was only \(0.0895\)s compared to Field which took \(0.125\)s (\(39.7\%\) longer). In such a case, the transformers can help save computation time before performing the Field operation.

Table 2 shows the results for the methods that do already have a guard. In each of these cases, we see that a transformer can beat the guards, and some by a margin of over 30%. We note that the guards for Trager and PseudoElliptic are particularly weak compared to the transformers, with much lower accuracy and precision. The transformers can avoid the false positives that the guards cannot. The number of positive samples for PseudoElliptic is scarce and future work may include an anomaly detection approach for this method. A caveat is that the inference time of the transformers is higher than the existing guards. As mentioned above, the transformers took \(8.95\)e\(-2\)s on average per sample whereas Trager, MeijerG, PseudoElliptic, and Gosper took \(1.1\)e\(-6\), \(1.4\)e\(-6\), \(4.7\)e\(-7\), and \(4.7\)e\(-7\)s respectively. The transformers take magnitudes longer to complete inference. However, we hypothesize that the huge gains in precision over these guards will help prevent a lot of wasteful

   &  &  \\   & Transformer & Guard & Transformer & Guard \\  Trager & **98.21** & \(67.55\) & **92.21** & \(15.88\) \\  MeijerG & **96.78** & \(88.72\) & **89.58** & \(57.78\) \\  PseudoElliptic & **99.28** & \(62.61\) & **62.86** & \(2.03\) \\  Gosper & **94.04** & \(92.51\) & **92.08** & \(80.21\) \\  

Table 2: Comparing each transformer to the Maple guard on the full test dataset.

   &  \\   & Transformer & Guard \\  Default & **94.86** & \(82.15\) \\  DDivides & **94.13** & \(28.18\) \\  Parts & **93.10** & \(37.05\) \\  Risch & **94.53** & \(89.35\) \\  Norman & **95.74** & \(71.67\) \\  Orering & **97.21** & \(37.88\) \\  ParallelRisch & **97.82** & \(82.73\) \\  

Table 1: Comparing each transformer to predicting positive every time on the test dataset. These methods have no guard; they always run when Mapleâ€™s algorithm reaches that specific method.

computation and so overall, Maple's integration algorithm would save time, to be confirmed in future work.

Some guards are known to be perfect at predicting incorrectness: i.e. if such a guard outputs that a method will not work, then it is a mathematical proof that it will never succeed. Although such guards will always predict the true negatives correctly, they may make mistakes on the true positives. Table 3 reports performance after we filter the data with such guards, by removing the samples the guard finds negative (the first two have a similar guard which caused them to have the same filtered dataset). Note that because these guards are perfect at predicting the negative target class, their accuracy and precision in Table 3 are exactly that of the precision on the full dataset in Table 2. We see that, unlike the guards, the transformers have good performance on both classes.

## 4 Layer Integrated Gradients to Interpret the Classifiers

We have seen that ML can optimise the work of CASs here, but we are also interested in _how_ the ML models make their decisions: both to give confidence in their results and perhaps to inform better hard-coded guards. This proved to be the case when SHAP values were used to explain the predictions of traditional ML models optimising Cylindrical Algebraic Decomposition (Pickering et al. (2024)). However, transformers are most widely used in Math-Al Zhou et al. (2024), Sharma et al. (2023), Charton (2024) and their explanation remains a challenge. Sharma et al. (2023) proposed an explainable approach for symbolic integration: computing integrals by applying rules step-by-step to form a chain from input to output. However, while this explains how the integral may be computed, it does not explain the decisions the transformers make to select these rules.

We further experiment with interpreting the transformers using Layered Integrated Gradients (LIGs) Sundararajan et al. (2017); an extension of Integrated Gradients that gives insight into different layers of a neural network. We used LIGs with the embedding layer of our model to compute the attribution score of each input token as the embedding layer converts the input tokens into a dense vector space. We used 50 steps and the default baseline as described in Kokhlikyan et al. (2020).

One interesting observation concerns the Risch method and the attribution scores found for the token of the absolute value function (abs). These always contribute negatively, usually highly so, in the observed samples which indicates that the presence of this token will adversely impact the prediction for use of this method. A visualisation for a particular example is shown in Figure 3. After discussing with domain experts, we find this a satisfying interpretation: the Risch algorithm is suited for elementary functions and the absolute value function does not fall in this category. In fact, the Risch algorithm is not proven to work when the absolute value function is included in the field defined in the algorithm (Gerhard and Von zur Gathen (2013)). Note this explanation also suggests a simple edit to improve the existing guard code!

The attribution scores for all the samples were calculated and aggregated to plot the graph in Figure 4. It can be observed that the attribution scores for abs are the most negative, further demonstrating the alignment of the interpretation and the human expert. Of course, this is just one observation for a single method and domain expertise remains crucial for validating the insight. But it points to potential for further progress through XAI tools.

Figure 3: Example sequence to depict the attribution scores corresponding to different tokens where blue is positive and red is negative. Note the strongly negative score for the abs token.

   &  &  &  \\   & & Transformer & Guard & Transformer & Guard \\  Trager & \(19287\) & \(\) & \(15.88\) & \(\) & \(15.88\) \\  PseudoElliptic & \(19287\) & \(\) & \(2.03\) & \(\) & \(2.03\) \\  Gosper & \(18929\) & \(\) & \(80.21\) & \(\) & \(80.21\) \\  

Table 3: Comparing each transformer to a perfect Maple guard on the filtered dataset.

## 5 Conclusions

In summary, we have demonstrated how we may improve the symbolic integration function in Maple without risking its mathematical correctness, by using transformers to predict when an integration method in Maple will succeed. The transformers were compared to heuristics crafted by domain experts: Table 1 shows that when a heuristic does not exist, a transformer is a suitable guard at predicting success, achieving between 93-98% accuracy. For the four methods that do have a guard, we also show that transformers make better predictions than these guards. In some cases, accuracy and precision increase significantly compared to the guards, over 30% and 70% respectively.

The models, specifically the embedding layers, were then analysed using LIGs to try to interpret the models. We demonstrated that this approach can give a satisfying explanation for some predictions (the presence of the abs token being heavily associated with a negative label for the Risch method) although we note that domain knowledge is needed to understand why LIGs produce this result. This explanation in turn suggests improvements to the human-designed guards.

### Future Work

We have shown the potential for ML optimisation of the Maple integration meta-algorithm. As next steps we will embed the transformers and evaluate the savings: measured both in number of methods tried (by trying them in order of probability of success) and in overall CPU time.

The existence of some perfect guards suggests there is scope for a hybrid approach between algebraic tools and transformers which we will also explore in future work. After that, we will seek to implement our best approach into the actual workflow of Maple's symbolic integration algorithm. We would then be able to empirically measure how much time is saved in the algorithm rather than analysing each method individually.

We will also continue to experiment with XAI tools to see if the transformers are learning the same rules as the integration method it is learning about, inspired by the insight observed above. The LIGs only offered explanations at a token level and most likely we need a tool that can also analyse the presence and prevalence of sequence and groups of tokens. While LIGs provided us with some insight, there was a need for a domain expert to validate this, and that is likely to remain the case.

Finally, we note again that in similar applications such as Pickering et al. (2024) and Coates et al. (2023), the authors were able to discover new interpretable heuristics based on the observations from the ML models. Being able to understand the predictions is interesting, but the ultimate goal would be to discover new interpretable heuristics for the methods that do not have a guard using XAI tools. This can give better insight on how we understand symbolic integration as a whole and discover new mathematical properties associated with these methods.

Figure 4: Aggregated attribution scores for all test samples containing the abs token for the Risch method. Note the abs token has a strongly negative score.