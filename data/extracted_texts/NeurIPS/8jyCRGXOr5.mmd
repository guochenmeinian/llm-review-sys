# Efficient Sketches for Training Data Attribution and Studying the Loss Landscape

Andrea Schioppa

Google DeepMind

Amsterdam, the Netherlands

arischioppa@google.com

###### Abstract

The study of modern machine learning models often necessitates storing vast quantities of gradients or Hessian vector products (HVPs). Traditional sketching methods struggle to scale under these memory constraints. We present a novel framework for scalable gradient and HVP sketching, tailored for modern hardware. We provide theoretical guarantees and demonstrate the power of our methods in applications like training data attribution, Hessian spectrum analysis, and intrinsic dimension computation for pre-trained language models. Our work sheds new light on the behavior of pre-trained language models, challenging assumptions about their intrinsic dimensionality and Hessian properties.

## 1 Introduction

**Overview** In this work, we investigate gradient and Hessian vector product (HVP) sketching to address the memory constraints inherent in applications that require the storage of numerous such vectors. Examples of such applications include training data attribution (TDA), eigenvalue estimation, and the computation of the intrinsic dimension. While previous studies on the intrinsic dimension have employed the Fastfood Transform to mitigate memory demands, we demonstrate its theoretical limitations for sketching and its persistent memory bottlenecks on modern accelerators. To resolve these issues, we propose novel sketching algorithms designed for modern hardware and underpinned by robust theoretical guarantees. Our experiments on pre-trained language models demonstrate the scalability of our methods while offering new perspectives on the intrinsic dimension and the Hessian of generative language models.

**Motivation** Training data attribution (TDA) [20; 11] and Hessian eigenvalue estimation  offer powerful insights into neural network behavior. TDA requires storing vectors of the same dimensionality \(N\) as the network's parameters for each training point, scaling linearly with dataset size (\(O(NT)\) for \(T\) training points). Similarly, numerically stable Hessian eigenvalue estimation algorithms demand repeated Hessian-vector product (HVP) computations and storage, scaling with network size and iterations (\(O(NT)\) for \(T\) iterations). These memory bottlenecks hinder the study of large-scale models; to address this, sketching  provides a compelling solution. By projecting gradients or HVPs into lower-dimensional random subspaces, sketching preserves their essential geometric properties while drastically reducing memory requirements. However, in TDA sketching has been employed with limited effectiveness. Random projections have been carried out with dense matrices, that introduce significant scaling constraints (\(O(ND)\) memory for a target dimension \(D\)) which necessitate the restriction of gradients to a subset of layers. Current TDA scaling methods require layer selection, as demonstrated with BERT in , where only 15M parameters were used out of 110M. Beyond the computational cost of dense matrices, our experiments show that layer selection introduces substantial distortion in the estimation of TDA scores (Sec. 5.2). Recent work on neural network geometry  suggests using the Fastfood Transform for gradient sketching(abbr. **FFD**) with \(O(N)\) memory; however, as the Fastfood Transform is a random feature generation algorithm, it necessitates indirect application in order to sketch gradients. This has both theoretical and practical consequences. On the one hand, our theoretical analysis reveals that **FFD** fails to fully satisfy the algorithmic requirements of sketching (Thm. 3.1). On the other hand, our experiments (Tab. 3) demonstrate that **FFD** exhibits unacceptable run-time performance on TPUs. These limitations, both theoretical and practical, underscore the need for novel sketching algorithms. To this end, we investigate new sketching paradigms optimized for modern accelerators like GPUs and TPUs. Our work makes the following contributions:

1. _Scalable Gradient Sketching for Modern Accelerators and Networks_: We introduce algorithms (**AFFD**, **AFJL**, **QK**) (Sec. 3) designed to overcome performance limitations of existing sketching techniques with modern neural networks on architectures like GPUs and TPUs. Our design analysis provides further insights into the effectiveness of our approach.
2. _Robust Theoretical Foundations_: We establish theoretical guarantees for **AFFD** and **QK** (Thms. 3.2, 3.3) for sketching and demonstrate limitations in the theoretical basis of the Fastfood Transform (Thm. 3.1). Our analysis further indicates a dimensionality reduction advantage for **AFFD** over **QK**, a finding supported by our TDA experimental results (Sec. 5.3).
3. _Algorithmic Improvements_: We propose more efficient algorithms for the intrinsic dimension estimation and Hessian eigenvalue computation (Sec. 4).

We demonstrate how our methods enable large-scale applications in training data attribution, intrinsic dimension computation, and Hessian spectra analysis with pre-trained language models. This leads to the following insights that advance the understanding of pre-trained language models:

1. _Limitations of Layer Selection_: We demonstrate that layer selection methods yield inaccurate influence score estimations in training data attribution, so their usage should be avoided (Sec. 5.2).
2. _High Intrinsic Dimension_: In contrast to assumptions drawn from classification-based studies, we demonstrate that the intrinsic dimension of LLMs can approach their full parameter count (Sec. 5.4). This challenges prevailing beliefs about the intrinsic dimensionality of these models [14; 1; 17].
3. _LLM Hessian Spectra_: Our analysis shows distinct characteristics of LLM Hessian spectra (Sec. 5.5), contrasting with conjectures based on findings related to smaller networks [9; 21; 3; 12].

**Paper organization** Sec. 2 provides a survey of relevant research in the field, contextualizing our contributions. Sec. 3 introduces our novel sketching algorithms. We begin with necessary background material, analyze design choices, and provide a step-by-step implementation tutorial in Appendix B. Sec. 4 outlines our proposed techniques for efficient intrinsic dimension search and Hessian eigenvalue computation. Sec. 5 describes our experimental setup: subsections are aligned with Sections 3 and 4 to enhance the connection between theory and empirical results

## 2 Related Work

SketchingSketching algorithms have been extensively studied (see surveys [27; 18; 15]). Our algorithms, **AFFD** and **AFJL**, draw inspiration from the seminal **FJL** algorithm  and the **FFD** approach . However, these techniques were designed before the era of modern accelerators like GPUs and TPUs. Therefore, our work revisits their design, optimizing them for modern neural networks and hardware. Theoretically, while  established **FJL** as a sketching algorithm, their proof relies on independence assumptions that do not hold in our setting. To address this, we employ more sophisticated concentration tools tailored to bi-linear forms (Thm. 3.2) and the special orthogonal group (Thm. 3.3). Recent work on PAC bounds  leveraged Kronecker-product decompositions to accelerate gradient sketching when computing intrinsic dimensionality. Our **QK** algorithm extends the concepts introduced in . Importantly, we provide a proof that **QK** is a sketching algorithm, absent in . Additionally, we demonstrate that the Kronecker structure used in  is not essential for performance gains, highlighting that the true bottleneck in **FFD** and **FJL** is memory access. For a comprehensive comparison with , please refer to Appendix C. Finally, to support our eigenvalue estimation, we leverage the theoretical guarantees outlined in .

Intrinsic dimensionThe concept of intrinsic dimension (ID) offers a valuable metric for understanding the complexity of learning tasks. Originally employed to analyze loss landscapes , intrinsic dimension has expanded into the study of language models.  demonstrated its role in explaining the generalization power of fine-tuned pre-trained language models. However, their focus was limited to classification tasks. In contrast, our work extends the analysis of ID to generative tasks. This distinction is crucial as we identify scenarios where the task's intrinsic dimension approaches the full model size, a phenomenon not typically observed in classification settings. Additionally, while **FFD** has been used for efficient language model fine-tuning , memory constraints limited their investigation of the intrinsic dimension (ID) in generative tasks to 500k; in other words, because of scalability contraints,  could only work with a target sketching dimension \(\) 500k, which prevented searching for the true value of ID as we demonstrate on a summarization task where ID approaches the model dimension. Therefore, our work overcomes this limitation, allowing us to compute the intrinsic dimension of such tasks.

Scaling up influence functionsScaling influence functions for training-data attribution remains a crucial research direction. Previous works like [8; 7] have improved index creation, retrieval speed, and Hessian estimation. However, these approaches can still be computationally demanding. Our work takes a distinct path, aiming to make influence function calculations fundamentally more efficient. Our HVP sketching methods seamlessly replace existing HVP and gradient computations in frameworks proposed within [8; 7; 20]. Furthermore, we offer eigenvector sketching to enhance methods like the Arnoldi iteration .  has proposed to use dense random projections by materializing them in chunks and on-the-fly; drawbacks of this approach are: (1) the lack of the scalability in the target sketching dimension and (2) the need of hardware-dependent custom implementations; on the other hand, our approach removes the requirement for specialized implementations (e.g., custom CUDA kernels). This flexibility enables easy integration into standard ML workflows using higher-level languages such as Jax. An orthogonal direction to scaling up via sketching is that of using surrogate models, compare .

Hessian evolution during trainingInvestigating how the Hessian evolves during training has shed light on the dynamics of deep learning, with seminal works [21; 9] offering intriguing findings. These studies suggest the progressive disappearance of negative eigenvalues and the confinement of gradient descent within a small subspace. While  developed a numerically stable Hessian analysis algorithm, its computational demands hinder its application to large-scale models over numerous iterations. Our work addresses this limitation by introducing sketching techniques to enable the efficient construction of large Krylov subspaces (e.g., \(10^{3}\)-dimensional) for models like GPT-2L (770M parameters). This advancement significantly surpasses the memory constraints of the method utilized by : a 3TB fp32 storage requirement would have been necessary for a comparable analysis using their approach. Consequently, we are uniquely positioned to rigorously examine the conjectures proposed [21; 9] within the context of pre-trained language models.

## 3 Design Principles for Efficient Sketching Algorithms

In this section, we explore a design space for more efficient sketching algorithms. To establish a foundation, we first analyze the performance bottlenecks of existing algorithms, specifically **FJL** and **FFD**, within the context of modern accelerators. This examination highlights two critical design choices: whether the gradient is sketched implicitly or explicitly, and the kind of pre-conditioner that is used. Informed by this analysis, we propose three novel algorithms: **AFFD**, **AFJL**, and **QK**. We then delve into the theoretical underpinnings of **AFFD** and **QK**, providing rigorous proofs for their guarantees. Additionally, we demonstrate that **FFD** lacks the theoretical foundation required for sketching. Relevant experimental findings are presented in Sections 5.2 and 5.3.

Dense sketches and FJLA \(D\)-dimensional sketch of the gradient of a real-valued function \(L()\) (\(^{N}\)) is a random projection of the gradient \( L^{N}\) to \(^{D}\). To ensure this projection preserves essential geometric properties, the random projection operator \(:^{N}^{D}\) must, with high probability, concentrate the norm of \((x)\) around the norm of \(x\). Mathematically, for each \(\) and \(\) there exists a large enough target dimension \(D(,)\) so that for \(D D(,)\):

\[(\|\|(x)\|_{2}-\|x\|_{2}\|x\|_{2}) . \]This concept generalizes to sketching higher-order derivatives (see Appendix C). For our purposes, consider the Hessian vector product operator \(:^{N}^{N}\) defined as \((u)=^{2}L()(u)\) A sketch of the HVP can be obtained as \(v((^{T}v))\), \(v^{D}\). This sketch defines a linear mapping \(^{D}^{D}\). While a simple **Dense Sketch** (using a random \(D N\) Gaussian matrix) ensures the norm property, it has \(O(DN)\) memory and \(O(DN^{2})\) compute requirements. The **FJL** algorithm  addresses the compute cost with a sparser projection matrix:

\[(x)=} G_{s} H_{N} B(x), \]

where: \(B^{N N}\) is a diagonal matrix with \(B_{i,i}= 1\) (random signs); \(H_{N}\) is the \(N\)-dimensional Walsh-Hadamard transform (Appendix C); \(G_{s}\) is a sparse \(^{D N}\) Gaussian matrix with (in-expectation) \((D^{2}M)\) non-zero entries (\(M\) is a parameter). The \(H_{N} B\) component preconditions sparse inputs \(x\) for which (1) might otherwise fail. Implementing \(G_{s}\) efficiently presents challenges on modern hardware and frameworks like Jax that lack native sparsity support.

FFD: Implicit Gradient sketchingThe **FFD** transform, introduced by  in the context of kernel machines, provides a computationally efficient way to approximate high-dimensional feature maps. As a random feature generator , **FFD** constructs high-dimensional random features (\(^{N}\)) from a lower-dimensional input vector (\(u^{D}\)). Given \(u^{D}\), **FFD** concatenates \(\) vectors of the form:

\[_{i}(u)=_{F} H_{D} G_{v} H_{D} B(u) (1 i), \]

where \(B\) and \(H_{D}\) are as in (2) (and are both \(D D\)-matrices), \(\) is a permutation matrix, \(G_{v}\) is a diagonal \(D D\)-matrix with i.i.d. standard Gaussian entries, and \(_{F}\) is a normalization constant (see ; for a practical implementation see the code released by ). **FFD** has the key advantage of constant memory cost, \(O(N)\), regardless of the input dimension \(D\). Since **FFD** defines a map \(^{D}^{N}\), direct sketching of a gradient is not possible. To address this,  perform what we call an _Implicit Gradient Sketch_:

\[(_{|_{0}}L)=_{|0}L(_{0}+ {FFD}()); \]

this formulation effectively applies the transpose of **FFD** to the gradient. While  establish certain properties of **FFD**, a complete proof of its suitability as a random feature generator is missing. Additionally, whether the **FFD** satisfies sketching guarantees (1), remains to be fully investigated (we will address it in Thm. 3.1).

Explicit sketches.In light of equation (4), it's natural to consider whether a direct sketch of the gradient could be achieved using a map \(:^{N}^{D}\). We define this as an _Explicit Gradient Sketch_:

\[(_{|_{0}}L)=(_{|_{0}}L). \]

This approach offers flexibility. For random feature generation methods like **FFD**, \(\) needs to be implemented as the transpose; for sketching algorithms like **FJL**, the explicit sketch can be applied directly, while the transpose would be needed for the implicit form (4). In Appendix B, we provide a Jax-based tutorial on transposing sketching algorithms. Which one is the right approach? Intuitively, implicit sketches may seem more efficient since they avoid direct gradient materialization. However, as we'll demonstrate in Section 5.3, explicit sketches surprisingly offer significant performance advantages. Table 4 quantifies the substantial wall-time reductions (approximately 70% on average) across various algorithms when using explicit sketches.

Removing the lookup bottleneck.In both **FJL** and **FFD** algorithms, multiplications by \(G_{s}\) and \(\) introduce a lookup-based memory bottleneck on modern accelerators. This hinders performance, as seen in unacceptable early TPU results (compare Tab. 3). To address this, we propose randomizing the pre-conditioner \(H_{N}\). Efficient implementations of \(H_{N}\) leverage the fact that it can be decomposed using Kronecker products; specifically, \(H_{AB}=H_{A} H_{B}\), which allows a recursive multiplication by \(H_{N}\) in \(O(N N)\)-time and \(O(N)\) storage. We exploit this by permuting rows/columns within Kronecker factors, reducing memory access costs from \(O(AB)\) to \(O(A+B)\) in the previous example. For optimal accelerator usage, we limit factors to roughly 1024 columns. Building on this, we modify (3). In the resulting **AFFD** algorithm (6), we remove \(\), use row-permuted factors in \(H_{N}^{_{1}}\), and column-permuted factors in \(H_{N}^{_{2}}\):

\[(x)=R_{D}(} H_{N}^{_{2}} G_{v} H_{N}^{ _{1}} B(x)), \]

where \(R_{D}\) denotes restriction to the first \(D\) coordinates. While all matrices are now \(N N\), efficient Hadamard implementations avoid large matrix materializations (see Appendix B for our Jax code). We further introduce **AFJL** (7), where \(H_{N}^{_{2}}\) is removed:

\[(x)=R_{D}(} G_{v} H_{N}^{_{1}} B(x)). \]

This can be seen as replacing \(G_{s}\) in **FJL** with a diagonal Gaussian matrix. Generalizations with multiple \((G_{v},H_{N}^{_{1}})\) pairs are possible but weren't explored due to the vanilla version's strong results. Empirical results on TPUs (Table 3) show the crucial impact of our changes. **AFJL** achieves a 100x wall-time reduction over **FJL**, **AFFD** a 64x reduction over **FFD**. While GPU wall-time remains similar, peak memory usage significantly improves. Appendix A highlights **FJL**'s scaling issues beyond \(D=2^{20}\) on GPUs.

Alternative pre-conditioners.To address the smoothing of sparse inputs in sketching algorithms,  introduced \(H_{N}\) in (2). The theoretical basis lies in the Heisenberg uncertainty principle, leveraging the Fourier Transform on a discrete group (see ). However, the computationally efficient Fast Fourier Transform (FFT) shares this desirable property. This raises the question of whether the FFT might yield performance gains for sketching. We find that replacing \(H_{N}\) with the FFT offers significant advantages. Experimentally, it reduces wall time by 62% on GPUs (Tab 4). Inspired by the Kronecker product structure enabling efficient \(H_{N}\) implementation, we propose another generalized pre-conditioner, \(Q\). This random \(N N\) orthogonal matrix has a Kronecker decomposition of \(K\) independent orthogonal matrices of sizes \(\{B_{i} B_{i}\}_{i=1}^{K}\), sampled according to the Haar measure on \(SO(B_{i})\). Our approach allows direct application of \(Q\) without the additional diagonal matrix \(B\), and offers a 40% wall time reduction on GPUs (Table 4). This \(Q\) pre-conditioner has the potential to unlock broader optimizations within sketching algorithm design as discussed in the next subsection.

Direct usage of the pre-conditioner \(Q\).Inspired by the design of the pre-conditioner \(Q\), we introduce a novel sketching algorithm, **QK**. This ablation explores the direct use of \(Q\) to transform the input, potentially leading to improved efficiency and memory usage. **QK** is defined as:

\[(x)=} Q(x). \]

Here, \(Q\) is a random \(D N\)-orthogonal matrix with a Kronecker product decomposition of \(K\) independent orthogonal matrices of sizes \(\{D_{i} B_{i}\}_{i=1}^{K}\). Each factor is generated by sampling from \(SO(B_{i})\) according to the Haar measure and restricting to the first \(D_{i}\) rows. Importantly, **QK** generalizes the approach of  by employing more Kronecker factors. This offers the potential for significant memory reductions (Appendix C).

Diagrams.Figure 1 illustrates **AFFD**, **AFJL** and **QK** with diagrams.

Theoretical resultsWe now delve into the theoretical underpinnings of our proposed algorithms, emphasizing the interplay between theory and experimentation. A key finding is a limitation of the **FFD** algorithm:

**Theorem 3.1**.: _There are some inputs \(x\) for which **FFD** does not satisfy the sketching property (1)._

Note that this limitation is specific to **FFD** and not inherent to implicit-mode algorithms. This distinction is important: the explicit-mode formulation allowed to simplify the theoretical analysis to prove Thm. 3.1. Next, we establish a theoretical guarantee for our **AFFD** algorithm. Compared to  there is added complexity as independence arguments cannot be used for \(G_{v}\); we thus apply the Hanson-Wright inequality for quadratic forms to prove:

**Theorem 3.2**.: _AFFD satisfies (1) with_

\[=_{1}+2(-C^{2}}}), \]for a universal constant \(C\) and for any \(_{1}>0\)._

Finally, we analyze **QK**. Its structure allows for concentration arguments on orthogonal groups, leading to:

**Theorem 3.3**.: _QK satisfies (1) with_

\[=2_{i}(-4CD_{i}((1+)^{1/K}-1)^{2}),\]

_for a universal constant \(C\)._

Crucially, the bound in Thm. 3.3 is less favorable than that of Thm. 3.2. This is due to the \(1/K\)-root, summation over sub-dimensions, and concentration depending on the \(D_{i}\). These theoretical insights support our experimental findings in Section 5.3, where **QK** requires higher target dimensions (\(D\)) to achieve performance comparable to **AFFD**. Because of space constraints the proofs are included in Appendix C.

## 4 Expanding the Utility of Sketching Algorithms

In this Section we expand the usage of sketching to other applications.

Improving the search for the intrinsic dimension.The intrinsic dimension (\(D_{int}\)) , is the minimum dimension (\(D\)) of a random subspace (\(V\)) where SGD yields at least 90% of the full model's performance on a target metric (\(\)1). Sketching algorithms, more efficient than FastFood, already accelerate the search for \(D_{int}\). Our memory-efficient algorithms enable us to investigate scenarios where \(D_{int}\) may approach the model dimension. For instance,  applied **FFD** to fine-tune generative language models but capped the target dimension (\(D\)) to 500k due to memory limits. We propose a novel search algorithm that estimates \(D_{int}\) in a single training run. Current methods rely on multiple runs across potential \(D_{int}\) values. To streamline, consider a binary search approach (assuming \(D_{int}\) is a power of 2) starting from an initial guess (\(D_{min}\)) up to the model parameter count (\(N\)). This would require \(_{2}^{2}}\) runs. Instead, we propose a single training run where \(D\) increases progressively. The heuristic: a fixed computational budget (\(c\) steps) should yield an expected improvement of at least \(\). We start with \(D=D_{min}\). If, after \(c\) steps, the target metric's improvement is less than \(\) or hasn't reached \(_{90}\), we double \(D\). This yields an estimate \(D^{*}\) within a factor of 2 of \(D_{int}\). A subsequent run with \(D=D^{*}/2\) verifies if this factor can be eliminated. See Appendix B for Python code. In Sec. 5.4, we apply this approach to pre-trained language models on classification and generative tasks. We find that \(D_{int} N\) for classification, but for the generative task, \(D_{int}\) depends on the choice of \(\) and can approach the parameter count \(N\). This finding is significant, as it

Figure 1: Diagram to illustrate our proposed sketching algorithms.

challenges prevailing assumptions about intrinsic dimensionality in such models [14; 1; 17]. These studies primarily focused on classification tasks, obtaining generalization bounds like \(O(})\) ([1, Eq. 4], [17, Sec. 3]) where \(D_{int}\) was presumed much smaller than the model dimension. Our results suggest a need for non-linear projection algorithms to achieve lower intrinsic dimensionality for generative tasks.

Scaling eigenvalue estimationInvestigating the Hessian's spectrum and eigenvectors often relies on memory-bound iterative algorithms. The Arnoldi algorithm, as used in , requires full re-orthogonalization for stability. This necessitates storing vectors as large as the model itself for each iteration: a constraint that limits the number of estimable eigenvalues (e.g., Krylov subspaces of dimension \( 90\) in ). Inspired by the theoretical work of , we propose sketching to address this memory bottleneck. With a sketching dimension of \(10^{6}\), we can readily construct 1k-dimensional Krylov subspaces for pre-trained language models like BART, Roberta, and GPT-2L. This represents a breakthrough because the method in  would demand an impractical 3TB of storage for GPT-2L alone. Our technique enables exploration of conjectures [21; 12; 9] about Hessian structure in the context of fine-tuning large language models. These conjectures are elaborated in the experimental results on eigenvalue estimation (Sec. 5.5). Crucially, we find that Hessian spectra in these models may deviate significantly from behaviors observed in smaller networks.

## 5 Experiments

To thoroughly evaluate the proposed sketching methods, we present a comprehensive set of experiments. First, we highlight the limitations of existing TDA scaling strategies (Sec. 5.2). Next, we dissect the impact of specific design choices on our sketches (Sec. 5.3). We then introduce and validate an algorithm for intrinsic dimension estimation, enabling computational savings (Sec. 5.4) and show-casing that the intrinsic dimensionality of generative tasks can be large. Finally, we apply our techniques to explore the evolution of the Hessian spectrum during pre-trained language model fine-tuning (Sec. 5.5).

### How we define the Training-Data Attribution score.

In TDA there are different ways to measure the similarity score between two examples \(x\) and \(y\). In our experiments we opt for the TDA score defined as \(_{}L(,x)_{}L(,z)\) because of its simplicity, allowing us to iterate on multiple algorithms and layer selection schemes, and being a building block for more complicated methods. While high correlation with full gradient dot products may not be the definitive measure of long-term TDA performance [19; 23], it is a practical metric in the short time range and a building block of more computationally intensive methods like TRAK . For example, in the short time range, gradient dot products correlate with loss changes and are relevant to select examples for error correction . Evaluating with the LDS from TRAK would introduce more hyper-parameters and considerable more computation: one would need at least 50 models fine-tuned on different subsets of the data; moreover, TRAK itself relies on accurate gradient sketches, as measured by dot products, as the basic building block as TRAK demonstrates that "preserving inner products to sufficient accuracy results in a gradient-descent system that approximately preserves the same evolution as the one corresponding to model re-training" [19; 2].

### Shortcomings of previous Training-Data Attribution scaling strategies.

Past work [20; 28; 8] tackles the memory bottleneck of Training-Data Attribution by calculating gradients restricted to a specific layer and potentially applying a Dense Sketch. Here, we demonstrate that layer selection distorts both influence scores and eigenvalue estimates, while Dense Sketches exhibit poor scaling characteristics. These findings align closely with the first subsection of Sec. 3. Furthermore, advice on layer selection lacks consistency:  promotes the last layer, whereas  supports using Token Embeddings in NLP tasks. We demonstrate the distortion caused by layer selection on influence scores (the inner product of two gradients). Considering \(2^{12}\) pairs of points \((x,z)\), we compute the Pearson correlation \(r\) between the TDA score \(_{}L(,x)_{}L(,z)\) estimated using a layer-specific gradient and the ground truth based on the full gradient. We adopt the setup of : a generative task fine-tuning GPT-2 on the WikiText-103 dataset (BART and zsRE results in Appendix A). **Our findings indicate the unreliability of layer selection (Table 1)**. Correlationswith ground truth rarely exceed 90% and are significantly lower for most layers. In contrast, **AFJL** achieves a 98% correlation with a compact \(D=2^{13}\). Extending the analysis to Hessian-based influence functions by looking into eigenvalue estimation emphasizes the shortcomings of layer selection. We aim to compute the top 10 eigenvalues of both the full Hessian and those restricted to a chosen layer. Even with potential differences in magnitude and location, layer selection could still be valuable if the true eigenvalues were approximated well by applying an orientation-preserving linear map \(^{1}^{1}\) to those computed for a particular layer. However, this is not the case, with the relative mean absolute error at 20% on the best layer (Table 1). Finally, **layer selection coupled with dense projections faces severe scalability limitations**. Setting \(D=2^{12}\) within the same setup highlights this issue. Limited memory forces us to divide the computation into \(D/k\) projections to \(^{k}\) where \(k\) is the smallest power of 2 enabling a dense projection without an Out-of-Memory error. For token embeddings, we find \(k=32\), whereas other layers require \(k=128\) on a V100 GPU. Projecting to \(^{k}\) takes \( 31ms\) for token embeddings, resulting in a total of \( 4s\) to project to \(^{D}\). Other layers require \( 24ms\) per projection to \(^{k}\) and \( 0.77s\) to project to \(^{D}\), see Table 2 for a per-layer breakdown. Finally, an alternative approach to dense projections is to materialize dense matrices on the fly in chunks ; this has two substantial disadvantages: (1) the runtime scales linearly with the target dimension (memory is traded off with compute), and (2) specialized kernels are necessary for efficient implementation, with unclear applicability to TPUs; we include a demonstration of these limitations in Appendix A.

### Analyzing the Impact of Design Choices

In this section, we analyze the impact of the design choices presented in Sec. 3 on both sketching quality and performance. Regarding **sketching quality**, we find that small values of \(D\) often lead to remarkably accurate reconstructions of influence scores. For TDA scores, achieving correlation \(r 0.95\) requires the following dimensions: **FJL**, **FFD**, **AFFD**: \(D=2^{10}\); **AFJL**: \(D=2^{12}\); **QK**: \(D=2^{14}\). To reach \(r 0.99\), simply increase each dimension by a factor of \(8\). While memory limitations prevented sketching HVPs with **FJL** and **FFD** for eigenvalue estimation, the other algorithms scaled well. We achieved a relative mean absolute error below 5% with the following dimensions: **AFFD**: \(D=2^{10}\); **AFJL**: \(D=2^{12}\); **QK**: \(D=2^{13}\). Note that **QK** requires larger \(D\), consistent with the theoretical comparisons in Theorems 3.3 and 3.2 (Sec. 3). Regarding **performance**, we outline here the key findings and refer the reader to Appendix A for a comprehensive analysis

  Model & layer & r & eig. err. \\  GPT-2 & Tok. Emb. & 0.16 & 0.72 \\ GPT-2 & 1 & 0.75 & 0.24 \\ GPT-2 & 2 & 0.89 & 0.31 \\ GPT-2 & 3 & 0.90 & 0.19 \\ GPT-2 & 4 & 0.89 & 0.24 \\ GPT-2 & 5 & 0.78 & 0.37 \\ GPT-2 & 6 & 0.38 & 0.40 \\  

Table 1: Layer selection results in unreliable estimates for influence scores and eigenvalue estimation. The best correlation with ground truth influence scores does not exceed \(90\%\) and is quite low for most layers; the relative error in eigenvalue prediction is always at least \(20\%\).

  Layer & GPU wall (ms) & \(k\) (max val before OOM). \\  Tok. Emb. & 31.2 & 32 \\ layer 1 & 25.1 & 128 \\ layer 2 & 24.4 & 128 \\ layer 3 & 23.9 & 128 \\ layer 4 & 23.1 & 128 \\ layer 5 & 22.4 & 128 \\ layer 6 & 21.7 & 128 \\  

Table 2: Dense projections on the layers do not scale; for each layer we report the wall time for the maximum dimension that does not result in an OOM.

across design choices. First, _removing look-ups significantly reduces wall-time on TPUs, while on GPUs it substantially lowers peak memory usage_ (reductions of 2.4x for **FJL** to **AFJL**, and 1.3x for **FFD** to **AFFD**), see Table 3. Second, _explicit sketching consistently provides substantial speed-ups_ (see Table 4); we conjecture this is due to the fact that implicit sketching results in more memory accesses. Finally,while modifying the pre-conditioner doesn't affect TPU performance, _it significantly improves GPU performance_, see Table 4 under the headings \(H_{N}\) and \(H_{N} Q\).

### Estimating the intrinsic dimension.

Our experiments evaluate the efficiency and accuracy of our intrinsic dimension estimation algorithm (presented in Sec. 4). We consider two experimental setups: classification, where we fine-tune Roberta on SNLI with accuracy as the target metric; generation, where we fine-tune BART on XSUM for text summarization, using Rouge1 and Rouge2 for evaluation. We employ three projection algorithms: **FFD**, **AFJL** (FFT-based variant), and **QK**. We first **validate algorithm consistency** by demonstrate that the algorithm produces consistent estimates of the intrinsic dimension across multiple runs; we repeat each search experiment with three random seeds obtaining estimates within a factor of 2 (Appendix B). We then **verify that the estimated \(D^{*}\) accurately represents the intrinsic dimension**\(D_{int}\) by fine-tuning the model in a \(D=D^{*}/2\)-dimensional subspace and ensuring that the target metric remains below the \(_{90}\) threshold (details in Appendix B). We point out that selecting good search parameters \((c,)\) was relatively straightforward by observing target metric improvement during full fine-tuning. Regarding **computational efficiency, our approach requires significantly fewer steps than binary or brute-force search**. For instance, the XSUM search required only twice the steps of a full fine-tuning run and the same amount of steps for the classification task. We finally look at **how the intrinsic dimension varies between classification and generation tasks.** For classification, our findings are consistent with prior work , were all algorithms yield \(D_{int}=2^{13} N\). For generation we first observe that the results are **metric-dependent**: for Rouge1, **FFD** and **QK** estimate \(D_{int}=2^{25}\) while **AFJL** yields \(D=2^{24}\); for Rouge2, \(D_{int}\) equals the full model dimension \(N 2^{27}\). In both cases, however, the intrinsic dimension is close to the full model dimension: this challenges assumptions about intrinsic dimension in the context of generative tasks. While prior studies [14; 1; 17] focused on classification with generalization bounds of the form \(O(})\), our results indicate that: **generative tasks may exhibit higher intrinsic dimension and generative models may require new non-linear projections to uncover significantly lower intrinsic dimensions**.

  Algo & GPU (V100) (\(R\)) & TPU (v2) (\(R\)) & Algo & GPU (V100) (\(R\)) & TPU (v2) (\(R\)) \\   & \)} \\ 
**FJL** & 1.96 & 1.20 & **AFFD** & 1.98 & 1.00 \\
**AFFD** & 1.81 & 2.07 & **AFJL** & 1.64 & 1.00 \\
**FFD** & 1.76 & 1.10 &  Q\)} \\ 
**AFJL** & 1.64 & 1.67 & **AFFD** & 1.45 & 1.00 \\
**QK** & 1.41 & 2.45 & **AFJL** & 1.44 & 1.00 \\  

Table 4: Speed-ups (ratio \(R\) of the slowest wall-time to the fastest one) corresponding to changing a design choice (e.g. implicit to explicit or \(H_{N}\) to the FFT.).

  Algo &  &  \\    & \(T\) (ms) & \(M\) (Gb) & \(T\) (ms) & \(M\) (Gb) \\ 
**FJL** & 123 & 6.27 & 3.27 & 3.0 \\
**AFFD** & 245 & 3.2 & 134 & 2.8 \\
**FFD** & 197 & 4.1 & 8694 & 1.4 \\
**AFJL** & 116 & 2.9 & 89 & 2.7 \\
**QK** & **32** & **1.7** & **6.1** & **1.1** \\  

Table 3: Wall-time \(T\) and peak memory usage \(M\) comparison on gradient sketches for GPT-2. Removing look-ups is crucial for TPU performance and decreasing GPU memory utilization.

### Hessian Analysis During Pre-trained Language Model Fine-Tuning

In this section, we examine the Hessian's evolution during pre-trained language model fine-tuning. Using sketching, as proposed in Section 4, we estimate the Hessian's eigenvalues and eigenvectors. We fine-tune Roberta and GPT-2L on SNLI (3 classes) and BART on XSUM, employing **AFFD**, with a target dimension \(D=2^{20}\), to construct a 1k-dimensional Krylov subspace. This substantially extends the work of  who considered smaller models and smaller subspaces (90-dimensional Krylov subspace). Spectrum estimation is performed every 1k steps, including initial steps \(\{0,10,50,100,150,200,250,500\}\). Our goal is to see if observations from previous studies with smaller networks hold in this context:

* **Obs1**: Negative eigenvalues gradually disappear during training [21; 12].
* **Obs2**: \(K-1\) outlier eigenvalues emerge for \(K\)-class classification, with the gradient aligning to their corresponding subspace [9; 21; 3]. Moreover, these outliers, which hinder optimization, stem from training without normalization .

We find that these obervations don't fully translate to pre-trained language model fine-tuning. Regarding **Obs1**, we compute the ratio (RNEG) of the absolute values of the top negative and positive eigenvalues; RNEG shows inconsistent behavior across models (Fig.2 (a)). Roberta maintains a higher RNEG, while GPT-2L and BART see it diminish over time. Regarding **Obs2**, outlier eigenvalues don't strictly adhere to the \(K-1\) rule. Roberta has more outliers (6), and the gradient-outlier alignment is less pronounced (27% Roberta, 35% GPT-2L, 8% BART) compared to smaller networks [12; 9]. See Fig. 2 (b). Moreover, outliers emerge despite layer normalization.

## 6 Conclusions and Limitations

In this work, we have dissected the theoretical and practical limitations of existing gradient sketching techniques when applied to modern neural networks and accelerators. Our analysis motivated the design of novel sketching algorithms, for which we established theoretical guarantees; additionally, we exposed limitations in the theoretical underpinnings of the Fastfood transform. These methods, along with refined intrinsic dimension estimation and Hessian eigenvalue computation, provide an efficient toolkit for model analysis. We successfully apply this toolkit to pre-trained language models, revealing the need to rethink layer-selection-based influence functions, the high intrinsic dimensionality of a generative task, and the deviation of LLMs' Hessian spectra from what observed in smaller networks. While in Sec. 5.4 we exhibit an example of a generative task with a large intrinsic dimension, we leave an in-depth study for future work. We tested the efficiency of our sketching algorithms with Transformers in Sec. 5.3, but results might vary for other model architectures.

Figure 2: **left**: ratio (RNEG) of the absolute value of the top negative to the top positive eigenvalue; **right**: ratio \(R\) of the \(n\)-th largest positive eigenvalue to the largest positive eigenvalue. We define outliers when \(R>20\%\), motivated by [12, Fig.2]. Higher-resolution versions for printing can be found in Appendix A. These results disprove conjectures on the Hessian structure, see Sec. 5.5.