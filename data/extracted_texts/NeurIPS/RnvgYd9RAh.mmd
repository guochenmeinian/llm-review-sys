# LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models

Elias Stengel-Eskin Peter Hase Mohit Bansal

UNC Chapel Hill

###### Abstract

When answering questions, large language models (LLMs) can convey not only an answer to the question, but a level of confidence about the answer being correct. This includes explicit markers of confidence (e.g. giving a numeric confidence score) as well as implicit markers, like using an authoritative tone or elaborating with additional knowledge of a subject. For LLMs to be trustworthy sources of knowledge, the confidence they convey should match their actual expertise on a topic; however, this is currently not the case, with most models tending towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that directly models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. Specifically, we cast calibration as a preference optimization problem, creating data via a two-agent speaker-listener game, where a speaker model's outputs are judged by a simulated listener. We then finetune three different LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the models resulting from this multi-agent optimization are better calibrated on TriviaQA with respect to a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM's answers to trivia questions, finding that training with LACIE results in \(47\%\) fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers. Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better separation in confidence between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more when uncertain and adopts implicit cues to signal certainty when it is correct, such as using an authoritative tone or including details. Finally, finetuning with our listener-aware method leads to an emergent increase in model abstention (e.g. saying "I don't know") for answers that are likely to be wrong, trading recall for precision.1

## 1 Introduction

In interacting linguistically with each other, people tend to follow conventions - or maxims - that allow for successful communication. For example, good conversational partners try to make their utterances truthful, relevant, clear, and concise . When people violate these conventions, they can mislead listeners, which may ultimately lead to them being seen as incompetent, untrustworthy, or as poor conversational partners. While large language models (LLMs) generally follow many of these conventions, they often fail to respect Grice 's maxim of truthfulness, generating outputs that are not truthful . More troublingly, untruthful outputs generated by LLMs areoften expressed confidently and authoritatively, and thus appear convincing to users, meaning that humans may easily be misled by LLMs.

LLMs' confidence can be expressed in at least two ways, shown in Fig. 1A. Firstly, LLMs can _explicitly_ express confidence in their output using numeric scores (e.g. _"I am 100% confident"_) or epistemic markers [Zhou et al., 2023] (e.g. _"I'm very sure that..."_). Secondly, LLMs can _implicitly_ express confidence by details or through their tone; often, the details included are spurious or non-factual, i.e. hallucinated. For example, in our analysis in Section 5, we found that LLMs often add hallucinated backstories to their answers, e.g. _"I remember seeing this movie on the big screen in the theatre..."_, or add an additional explanation that may sound convincing but is untrue. These details convey a sense of expertise that can lead to the answer being perceived as more likely to be correct.

Overconfidence is particularly troubling given that people are increasingly interacting with LLMs as sources of information [Gude, 2023]; in other words, people rely on LLMs to answer questions they themselves do not know the answer to. Futhermore, because the interactions people have with LLMs happen via language, users often interpret LLM outputs as they would interpret language from a human, i.e. assuming that the outputs follow Gricean maxims. This in turn makes LLMs unreliable partners; many readers may have had the experience of working together with a partner or teammate who consistently overstates their confidence. While this teammate may initially have their answers accepted, over time they lose trust. Indeed, Zhou et al.  highlight this type of erosion for overconfident models, finding that overconfidence irreparably damaged a user's trust in an AI system.

Focusing on answering information-seeking questions, we hypothesize that model overconfidence of both kinds (implicit and explicit) can be mitigated by optimizing for pragmatics, i.e. for how the utterance will be interpreted by a listener. Specifically, we hypothesize that part of current models' overconfidence lies in (1) a lack of knowledge about whether its answers are correct or not, and (2) a lack of pragmatic grounding, i.e. models do not generate utterances according to how they will be perceived by a listener. Firstly, base models (not finetuned with instructions or human feedback) are not grounded in the consequences of their answers: they do not receive direct feedback during training about whether the answer is correct or not, and thus have little reason to hedge in their confidence. Secondly, models are not pre-trained _pragmatically_; they generate responses without real-time access to feedback on how listeners might interpret their answers. While models trained with human feedback may in principle have this capacity, past work has shown that they in fact have _worse_ calibration than base models and has attributed this to current reward data penalizing hedging and markers of uncertainty [Zhou et al., 2024].

To address overconfidence by tackling these two types of grounding, we introduce _Listener-Aware **C**alibration for **I**mplicit and **Explicit** confidence, or LACIE. LACIE finetunes models not only using

Figure 1: _(A)_ A non-expert listener (who does not know the answer to the question already) accepts or rejects answers based on how confident they sound. This confidence is influenced by implicit and explicit markers. _(B)_ To calibrate a speaker modelâ€™s confidence, we train a listener-aware speaker model by bootstrapping data from a base speaker model. For each training question, we generate \(k\) diverse responses. These are scored for correctness against the gold answers and accepted or rejected by a listener model. Our preference function rewards true accepts and true rejects and penalizes false accepts and false rejects. _(C)_ Before training, models tend to be confident regardless of whether they are right or wrong. After training, listener-aware models are more confident when they are correct and less confident when they are wrong.

feedback on whether their answer was correct but _also_ whether their answer was interpreted as correct by a listener. In other words, whereas past work (Kuhn et al., 2022; Tian et al., 2023; Ulmer et al., 2024; _i.a._) has sought to produce calibrated distributions in speaker's output distribution - i.e. answer probabilities that are equal to the model's chance of being correct - we seek to _induce_ a calibrated distribution in the listener via the speaker's utterance - i.e. train the model to output generations that allow a listener to recover a well-calibrated score of how likely the answer is to be correct. This multi-agent optimization not only trains models to reliably use both explicit and implicit confidence markers, but also allows us to flexibly address calibration for long-form model answers and not only closed-set answer probabilities.

To pragmatically calibrate LLMs, we adopt the Direct Preference Optimization (DPO) framework (Rafailov et al., 2024), constructing a dataset of preferrred and dispreferred examples from a seed dataset of QA pairs. As shown in Fig. 1B, we first generate long-form responses from a standard LLM (the **speaker** agent). Many of these responses contain both implicit and explicit certainty expressions, often applied inappropriately to incorrect answers. We then use another LLM to model a **listener** agent who decides whether to accept or reject the answer; in information-seeking questions (where the answer is not known), the listener model should base its decision largely on how confident the speaker sounds. Note that our multi-agent framing allows us to explore a much wider range of confidence cues; while past work (Mielke et al., 2022; Lin et al., 2022; Zhou et al., 2024) has focused on epistemic markers (explicit expressions of confidence) we are also able to define and examine more subtle and implicit confidence cues, like a tone, level of detail, and use of backstories. Using the listener model and the ground-truth answer, we define a preference function (given in Section 3) that rewards cases where the model accurately expresses confidence - marking these as preferred examples - and penalizes the model when it inaccurately expresses confidence - marking them as dispreferred. This makes the training _listener-aware_, connecting to past work in jointly modeling speakers and listeners (Frank and Goodman, 2012; Fried et al., 2018; Lazaridou et al., 2020).

We demonstrate the effectiveness of our method first with automated metrics and then through a human evaluation. We generate training data using \(10{,}000\) QA examples from TriviaQA (Joshi et al., 2017); our automated data generation pipeline (described in Section 3) allows us to transform these into \({}\,14,000\) preference instances, which we use to train several LLMs. When testing our optimized model on TriviaQA using an LLM listener, we find that open-source LLMs are generally overconfident, producing answers that are accepted by the listener despite often being wrong. Using LACIE on three different speaker models (Mistral-7B, Llama3-8B, and Llama3-70B), we obtain substantial gains in induced listener calibration, with an average \(20.7\) point gain in AUROC over the base model and a \(7.8\) point decrease in calibration error, indicating that utterances from LACIE-trained models induce more calibrated distributions in the listener. We also obtain an average \(18\%\) absolute improvement in precision, meaning that LACIE-optimized models produces less overconfident utterances; these utterances are more consistently rejected by the listener when they are wrong. Furthermore, these benefits translate to instruction-tuned model variants. We also find that training leads to abstention, with models not answering high-uncertainty questions. This helps increase precision but comes at a cost to recall. Going beyond automated evaluation, we perform a human evaluation in which we show that LACIE significantly reduces the rate at which incorrect answers are accepted by _human_ listeners; a model trained with LACIE results in a \(47\%\) decrease in the rate of false answers being accepted without significantly increasing the rate of rejection for correct answers (i.e. without lowering recall). In our analysis, we show that our training transfers between datasets: we train our models on TriviaQA and evaluate them on TruthfulQA (Lin et al., 2021). Here, we show that LACIE results in a \(28\%\) absolute improvement to truthfulness, as measured by TruthfulQA's metrics. We underscore our quantitative improvements with a qualitative analysis showing that training leads to more hedging, as well as more detailed outputs and more authoritative tone when the model is actually correct.

## 2 Background and Related Work

Background: Pragmatics.Pragmatics studies how people interpret language in context, going beyond the literal meaning of an utterance. Grice (1975) gives a seminal account of conventions that people generally follow and how they relate to implicatures: quantity (saying as much as needed, and not more), relation (being relevant), manner (avoiding obscure or ambiguous language), and quality (being truthful). Most pragmatic accounts involve speakers not only reasoning about the literal meaning of their utterance, but also about how a _listener_ will interpret the utterance. In other words, pragmatic reasoning involves knowing not only what you mean, but also what others will think you mean. We introduce pragmatics into LACIE through multi-agent modeling, where the speaker is optimized by considering not only the answer provided by the speaker model but also its interpretation by the listener model.

Pragmatic Modeling.Frank and Goodman (2012) introduce the RSA model, which is a formal description of how pragmatic agents communicate. This formulation has been applied to generation tasks in a number of ways (Fried et al., 2018, 2018, Lazaridou et al., 2020, Vaduguru et al., 2023). In two-player communication games, Wang et al. (2021) introduce methods for improving listener model calibration with the goal of reducing "semantic drift" between the meaning of speaker utterances and their original meaning in natural language. In contrast, we directly train speaker models to induce calibrated answers in listener models, and our approach is complementary to the specific choice of listener model. Furthermore, the domains examined by Wang et al. (2021) are limited to a simple communication game, and they do not evaluate with human listeners. While we draw on the RSA formulation for inspiration, we do not directly apply it because listeners choose from a fixed set of interpretations in RSA. Instead, we allow for listener models to interpret implicit and explicit confidence markers in an arbitrary manner, and we train the speaker model to induce calibrated answers in the listener regardless of the listener's manner of interpretation.

Calibration in LLMs.Given that calibration is key to making intelligent decisions on when to trust AI systems, a number of past efforts have documented calibration in neural models (Naeini et al., 2015, Guo et al., 2017, Ovadia et al., 2019, Wang et al., 2020) with recent work focusing on calibration in LLMs (Mielke et al., 2022, Kadavath et al., 2022, Kuhn et al., 2022, Stengel-Eskin and Van Durme, 2023, Tian et al., 2023, Zhang et al., 2023).

Within this area, several papers have focused on verbalized confidence. Mielke et al. (2022) introduce control codes based on model confidence to get models to better use epistemic markers. Lin et al. (2022) finetune a GPT-3 model using the average answer accuracy of predicted answers, showing improvements to the calibration of verbalized confidence; this is similar to our "truthful-only" baseline, which only optimizes for answer accuracy using the speaker model's generations. Band et al. (2024) supervise models to use confidence scores and then train via reinforcement learning against simulated user scores. Zhou et al. (2023) categorize epistemic markers and measure their impact on LLM accuracy. Zhou et al. (2024) document the use of epistemic markers by LLMs, finding that LLMs rarely use weakeners, and measure the impact of poor calibration on usability, finding that overconfidence irreparably hurts performance. Supporting these findings, Kim et al. (2024) perform a large-scale evaluation of how human subjects respond to epistemic markers, finding that first-person weakeners reduce over-reliance on model answers. Taken together, these findings suggest that improving models' abilities to correctly provide verbal confidence estimates has the potential to improve model safety, reliability, and usefulness to users. Building on these findings, we propose a new method to reduce overconfidence and make epistemic marker usage more appropriate. Unlike Mielke et al. (2022) our method does not rely on pre-defined codes, and unlike other work that trains models to have better-calibrated confidence (Mielke et al., 2022, Lin et al., 2022, Ulmer et al., 2024, Li et al., 2024), our work takes a pragmatics-based approach of also modeling the listener.

Lastly, past work has also evaluated free-text generations for their correctness with an LLM, which is a form of model-based calibration. Kadavath et al. (2022) introduce a confidence estimation method that first generates an answer and then asks via follow-up prompt whether the answer is correct or not. This formulation is also adopted by Ren et al. (2023) and is similar to using a listener model. However, unlike our work, past work has not optimized the speaker model for the listener (meaning the speaker is not pragmatic), and has restricted itself to using the same model as both speaker and listener, whereas we take a multi-agent approach in which the speaker and listener may differ.

## 3 Methodology

### Datasets

We use two datasets in this paper. First, for our finetuning experiments and human study, we use TriviaQA (Joshi et al., 2017), which includes challenging open-domain general-knowledge trivia questions along with source documents. From the \(650{,}000\) total questions, we sample \(10{,}000\) for use in DPO. For each question, TriviaQA includes several eligible phrasings of the answer choice. Following the TriviaQA evaluation, we mark a model output as correct if its answer exactly matches any eligible answer string. The second dataset we use is TruthfulQA [Lin et al., 2021], which includes questions that people commonly have misconceptions about, stemming from widely circulated conspiracy theories, folk theories, apocryphal stories, or other commonly repeated falsehoods. We use this dataset for judging the transfer of our calibration finentuning across datasets. TruthfulQA is particularly useful as an evaluation here because the benchmark includes a model-based evaluation for assessing the tradeoff between truthfulness and informativeness. Thus, we can show that our finentuning improves truthfulness directly (by preventing the model from saying false things that it is actually unconfident about). Our test sizes are \(1{,}000\) for automatic evaluation with TriviaQA, \(817\) for automatic evalation with TruthfulQA, and \(100\) for human evaluation with TriviaQA.

### Listener-Aware Preference Data Creation Methodology

To generate training data for LACIE, we instantiate speaker and listener models. The speaker model is prompted to express its confidence verbally, and the listener model is prompted to ignore its prior knowledge, as we are primarily interested in the perceived confidence of the answer (see Appendix H for the exact prompts). We begin the dataset creation process by subsampling \(N\) question-answer pairs \((Q^{i},^{i})\) from the dataset. We then obtain multiple responses \(R^{i}_{j}\), \(j\{1,,k\}\) to each question from the speaker model, sampling independently with temperature to encourage diversity, resulting in \(N k\) (\(Q^{j},R^{i}_{j}\)) pairs. For each response \(R^{i}_{j}\), we also extract the final answer \(A^{i}_{j}\) (usually 1-3 words). Each \((Q^{i},R^{i}_{j})\) is then given to the listener model, which produces probability of accepting or rejecting the answer \(P^{i}_{j}\). From the gold answer \(^{i}\) we can determine whether \(A^{i}_{j}\) was correct, leading to a gold accept/reject decision \(^{i}_{j}\). Then, for a given question \(Q^{i}\), we enumerate the different possible combinations of responses \(R^{i}_{j}\), computing the preferences given in Eq. (1).

**Answer Extraction** We prompt the speaker model for a long-form answer; however, to evaluate against the gold answer \(^{i}\) we need to extract a single short-form response. To do this, we use a follow-up prompt with \(3\) in-context examples showing how to extract answers from responses.

**Answer Anonymization.** While we instruct the listener model to ignore its prior knowledge in evaluating the speaker's answers, we find that this is a difficult instruction for the model to follow, especially when the speaker and listener model are the same. Indeed, past work has found that LLM evaluator models tend to prefer their own outputs [Panickssery et al., 2024]. To mitigate this, we implement an answer anonymization strategy: using regular expressions, we remove mentions of the extracted answer from the response, replacing them with [ANSWER REMOVED]. This way, the listener must focus more on the way the response is phrased (see Appendix B for more information).

Preference Function.To construct preference data over these tuples, we convert all probabilities \(P\) of accepting the answer to decisions by setting a threshold \(\) and setting \(D^{i}_{j}=(P^{i}_{j}>)\). We use the median probability across the training data as our threshold; for our Mistral-7B listener, this is \(0.66\). For each \(Q^{i}\), we compare all combinations of answers; our preference function is given by Eq. (1).

\[^{i}_{j}=1,D^{i}_{j}=1)=U(^{i}_{j}=0,D^ {i}_{j}=0)}_{}\] (1) \[U(^{i}_{j}=0,D^{i}_{j}=0)>U(^{i}_{j}=1,D^{i}_{j}=0)\] \[^{i}_{j}=1,D^{i}_{j}=0)>U(^{i}_{j}=0, D^{i}_{j}=1)}_{}\]

where \(^{i}_{j}=1\) means the answer was _correct_ and and \(D^{i}_{j}=1\) means the answer was _accepted_ by the listener (i.e. \(P^{i}_{j}>\)). Note that our function encodes a conservative interpretation of calibration, in which it is better to err on the side of false negatives than false positives.

### Preference Finetuning

From our preference data, we finetune models using DPO [Rafailov et al., 2024]. DPO seeks to maximize margin between the likelihood of the preferred examples and that of the dispreferredexamples; in this case, DPO maximizes for correctly-calibrated outputs and for incorrectly-calibrated outputs. We train our models using QLoRA (Dettmers et al., 2024) with rank 16 for a max of 250 steps. Further details on the preference data and finetuning are given in Appendix B.

## 4 Experiments and Results

We first show LACIE's performance using calibration metrics on the listener model, where we see substantially better calibration after training. We then show that this translates to a human evaluation, where we present LACIE and baseline outputs to real listeners.

### Setup

For all experiments, we sample a training dataset of \(10{,}000\) pairs of questions \(Q^{i}\) and gold answers \(^{i}\) from the TriviaQA validation data. We then obtain 10 responses to each question from a Mistral-7B base model and extract their answers. We use the official TriviaQA metric to obtain the correctness of the extracted answer, \(^{i}_{j}\). We reserve \(1{,}000\) (\(Q^{i},A^{i}_{j},P^{i}_{j},^{i}_{j}\)) tuples, corresponding to \(100\) TriviaQA questions, as development data to use during training. We also sample a _separate_ held-out test set of \(1{,}000\) TriviaQA questions that are separate from the training/dev data; here, we only obtain one response since we do not need preferences at test time.

We evaluate three speaker models of varying sizes: Mistral-7B, Llama3-8B, and Llama3-70B; for all models, we finetune both the base and instruct (or chat) variants, the latter of which have been optimized using supervised finetuning across a large number of tasks formatted in a conversation-style format. This process makes chat models better at following user-specified instructions. For all models, we average across 3 seeds. Across all models, we use the same listener (Mistral-7B-base).

**Baselines.** We compare LACIE to the base model, without fine-tuning. For the base model, we simply take the top generation from the base model as the response \(R\). As part of our preference function is based on answer correctness, we also compare to a model finetuned on correctness alone, i.e. a preference function which prefers correct to incorrect answers (with no regard for the listener model); we call this the "truthful" baseline. We also compare against the instruct or chat versions of the models tested; these models are instruction-tuned on chat-style data, making them generally better at following instructions. As an external baseline, we compare against a prompt-based method for obtaining better calibration from Tian et al. (2023), who prompt models to include an explicit confidence score with their answer. Tian et al. (2023)'s formulation lacks a listener model. Therefore, we implement two settings: with and without a listener model. In the first setting, we pass the outputs from Tian et al. (2023) prompt through our listener. This setting is directly comparable to the other baselines and to LACIE (all evaluated according to the listener's confidence). We additionally the original version of Tian et al. (2023) that directly extracts the confidence score from the output (rather than using a listener model), which we refer to as no-listener (or NL). This baseline has an advantage in avoiding the listener model, but only works for explicit confidence scores.

**Metrics.** We report the following metrics:

* [leftmargin=*,noitemsep,topsep=0pt]
* **AUROC** measures the tradeoff between true acceptances and false acceptances across varying thresholds on the numeric confidence, which in our case is the listener's induced \(P^{i}_{j}\). A higher AUROC means the model is better calibrated.
* **ECE**(Naeini et al., 2015) bins confidence scores \(P^{i}_{j}\) and measures the difference between these bins and their average correctness. We use \(9\) bins, backing off to fewer if bins are empty, and use unweighted ECE. Note that we ignore abstentions in computing ECE, as assigning a meaningful probability of answer correctness to abstention (no answer) is ill-posed.
* **Precision and Recall** compare the rate of true acceptances, false acceptances, and false rejections. We focus on precision as it is sensitive to false acceptances from overconfident utterances, which have been shown to reduce system trust (Zhou et al., 2024); higher precision is often driven by a lower rate of false acceptances, i.e. a lower rate of false positives. Here, we use the median listener probability on train as the threshold to binarize \(P^{i}_{j}\) into accept and reject. Higher is better.
* **Abstention rate** is the rate at which models produce no extractible answer; this typically corresponds to the model expressing a lack of knowledge. A safe model should abstain when it is very uncertain of its answer.

### Results with Modeled Listener

We evaluate on a held-out subset of \(1{,}000\) TriviaQA examples, reporting our metrics in Table 1.

LACIE results in better induced listener calibration across speaker models.Table 1 shows that LACIE improves induced listener ECE and AUROC, consistently resulting in the highest performance. LACIE also improves precision substantially across models, outperforming all baselines. On the base model variants (Mistral-7B, Llama3-8B, and Llama3-70B), LACIE improves AUROC by an average of \(20.7\) points (over the base) across models, and reduces ECE by \(7.8\) points, while increasing precision by \(18\%\) (absolute). Note that while truthful-only finetuning does typically improve induced listener AUROC, LACIE consistently beats it by an average of \(13.3\) points. LACIE and truthful finetuning both increase abstention over the base model. This in turn reduces recall for the base model; however, note that recall penalizes abstention, since models that confidently guess can increase their true positive rate, and recall does not measure the false positive rate. In practice, when looking at accuracy on non-abstained examples, LACIE is generally comparable (see Table 6). Furthermore, note that this reduction in recall _does not_ translate to the human evaluation (discussed in detail in Section 4.3), where human listeners only had one false negative more from LACIE questions

   &  \\ 
**Speaker Model** & **AUROC** & **ECE \(\)** & **Precision \(\)** & **Recall \(\)** & **\% Abstained** \\  _Mistral-7B_ & & & & & \\ base & \(0.54_{\,0.00}\) & \(0.15_{\,0.01}\) & \(0.56_{\,0.00}\) & \(0.76_{\,0.00}\) & \(0.80_{\,0.00}\) \\ chat & \(0.63_{\,0.01}\) & \(0.23_{\,0.01}\) & \(0.52_{\,0.00}\) & \(0.58_{\,0.01}\) & \(13.83_{\,0.52}\) \\ base+Truthful & \(0.58_{\,0.03}\) & \(0.16_{\,0.01}\) & \(0.63_{\,0.01}\) & \(0.52_{\,0.04}\) & \(8.87_{\,2.45}\) \\ chat+Truthful & \(0.57_{\,0.01}\) & \(0.21_{\,0.02}\) & \(0.51_{\,0.01}\) & \(0.48_{\,0.01}\) & \(5.50_{\,0.78}\) \\ [Tian et al., 2023] & \(0.70_{\,0.01}\) & \(0.30_{\,0.01}^{\,0.01}\) & \(0.45_{\,0.01}\) & \(_{\,0.00}\) & \(3.33_{\,0.28}\) \\
[Tian et al., 2023] (NL) & \(0.73_{\,0.00}\) & \(0.36_{\,0.01}\) & \(0.50_{\,0.00}\) & \(0.90_{\,0.01}\) & \(0.36_{\,0.04}\) \\ base+LACIE & \(0.74_{\,0.02}\) & \(0.12_{\,0.00}\) & \(_{\,0.02}\) & \(0.55_{\,0.03}\) & \(25.27_{\,0.37}\) \\ chat+LACIE & \(_{\,0.01}\) & \(_{\,0.01}\) & \(0.67_{\,0.00}\) & \(0.50_{\,0.05}\) & \(29.37_{\,0.37}\) \\  _Llama3-8B_ & & & & & \\ base & \(0.57_{\,0.01}\) & \(0.19_{\,0.02}\) & \(0.55_{\,0.02}\) & \(0.40_{\,0.02}\) & \(13.20_{\,0.31}\) \\ chat & \(0.59_{\,0.00}\) & \(0.23_{\,0.00}\) & \(0.64_{\,0.00}\) & \(_{\,0.00}\) & \(2.93_{\,0.20}\) \\ base+Truthful & \(0.63_{\,0.04}\) & \(0.24_{\,0.08}\) & \(0.63_{\,0.03}\) & \(0.64_{\,0.19}\) & \(9.60_{\,4.35}\) \\ chat+Truthful & \(0.71_{\,0.01}\) & \(_{\,0.03}\) & \(_{\,0.01}\) & \(0.56_{\,0.06}\) & \(9.43_{\,0.98}\) \\
[Tian et al., 2023] & \(0.66_{\,0.00}\) & \(0.10_{\,0.01}^{\,0}\) & \(0.62_{\,0.00}\) & \(_{\,0.00}\) & \(0.00_{\,0.00}\) \\
[Tian et al., 2023] (NL) & \(0.67_{\,0.00}\) & \(0.24_{\,0.00}\) & \(0.67_{\,0.00}\) & \(0.90_{\,0.00}\) & \(0.00_{\,0.00}\) \\ base+LACIE & \(_{\,0.00}\) & \(0.12_{\,0.02}\) & \(_{\,0.01}\) & \(0.37_{\,0.00}\) & \(35.37_{\,5.41}\) \\ chat+LACIE & \(_{\,0.02}\) & \(0.12_{\,0.02}\) & \(_{\,0.02}\) & \(0.83_{\,0.04}\) & \(8.47_{\,1.03}\) \\  _Llama3-70B_ & & & & & \\ base & \(0.53_{\,0.02}\) & \(0.27_{\,0.04}\) & \(0.58_{\,0.05}\) & \(0.30_{\,0.00}\) & \(12.87_{\,4.11}\) \\ chat & \(0.61_{\,0.02}\) & \(0.21_{\,0.03}\) & \(0.76_{\,0.01}\) & \(_{\,0.01}\) & \(2.25_{\,0.08}\) \\ base+Truthful & \(0.65_{\,0.03}\) & \(0.21_{\,0.00}\) & \(0.78_{\,0.02}\) & \(0.35_{\,0.05}\) & \(12.20_{\,3.88}\) \\ chat+Truthful & \(0.58_{\,0.03}\) & \(0.15_{\,0.03}\) & \(0.71_{\,0.04}\) & \(0.37_{\,0.06}\) & \(5.30_{\,0.62}\) \\
[Tian et al., 2023] & \(0.69_{\,0.01}\) & \(0.12_{\,0.00}^{\,0}\) & \(0.81_{\,0.00}\) & \(0.98_{\,0.00}\) & \(0.00_{\,0.00}\) \\
[Tian et al., 2023] (NL) & \(0.69_{\,0.00}\) & \(_{\,0.01}\) & \(0.83_{\,0.00}\) & \(0.97_{\,0.00}\) & \(0.00_{\,0.00}\) \\ base+LACIE & \(_{\,0.02}\) & \(0.14_{\,0.03}\) & \(_{\,0.01}\) & \(0.40_{\,0.01}\) & \(32.77_{\,2.34}\) \\ chat+LACIE & \(0.70_{\,0.02}\) & \(0.15_{\,0.04}\) & \(0.79_{\,0.01}\) & \(0.87_{\,0.02}\) & \(4.60_{\,0.87}\) \\  

Table 1: TriviaQA performance (and standard error) with metrics computed according to a Mistral-7B listener model. We bold the best value for each model.

than the base model. Finally, LACIE translates well to the largest size of model, resulting in better performance on AUROC, ECE, and precision for Llama-70B over the corresponding 70B baselines.

These trends largely hold also when finetuning the chat variants of each model. Here, we see an average increase over the untrained chat model of \(12.7\) AUROC and \(8\%\) precision with a decrease in ECE of \(10.3\). LACIE continues to outperform the truthful-only baseline, beating it by an average of \(11.7\) AUROC. These results indicate that LACIE improves calibration beyond what standard chat-based instruction tuning can do; this dovetails with past results indicating that general-purpose finetuning procedures may not necessarily improve calibration (Zhou et al., 2024). Notably, for both Llama3 models, finetuned chat models abstain substantially less than base models, resulting in far higher recall numbers. Here, chat+LACIE variants have the best balance between precision and recall. These trends may be due to the percentage of abstentions for the untrained chat model, which is substantially lower for Llama3 than for Mistral, possibly because the chat variants of Llama3 were trained to avoid false refusal.2

Comparing to the Tian et al. (2023) baselines, we see that both baselines improve AUROC and recall over the untrained chat and base models, and that the "no listener" (NL) variant improves precision for Llama3. This indicates that these are competitive baselines. However, for both Llama3 and Mistral, LACIE has higher AUROC and precision, indicating the LACIE-trained model's ability to express uncertainty appropriately. For ECE, we note that LACIE generally beats both baselines, but in some cases Tian et al. (2023) has lower ECE. Qualitatively, the baseline generally produces bimodal scores (close to 0% confidence or 100% confidence with few in between), which can result in artificially low ECE when the model is generally correct (i.e. for Llama3-70B).

LACIE training leads to better abstention ability.In Table 1, the percentage of abstentions increases with LACIE compared to any baselines; qualitatively, the model often expresses a lack of knowledge (cf. Table 4). In Appendix D.2 we find that the base model's accuracy on examples where LACIE models abstained is substantially lower, i.e. abstention is correctly correlated with cases where the model does not know the correct answer, and we find that it is correlated with base model uncertainty. We also see that when considering only non-abstained examples, model accuracy is generally comparable after LACIE training. Interestingly, the abstention behavior seen here is emergent, in that it is not seen during training: all training samples have valid outputs, with no examples of abstention. Nevertheless, LACIE training leads to a more conservative model that produces far more abstention outputs than the base model.

### Human Evaluation

Table 1 show that finetuning using LACIE leads to models that are better-calibrated w.r.t. a modeled listener. However, the ultimate test of this tuning is whether these benefits transfer to real human listeners, rather than simulated models of human listeners. To test this, we ask human annotators to accept or reject answers to a subset of TriviaQA questions. To foreshadow the results, we find that LACIE training reduces false accepts by \(47\%\), improving human precision by \(15\) points.

Setup.We use the outputs from the Mistral-7B-base model, and sample 100 test questions, pairing each with the LACIE-trained model's answer as well as the base model's answer, resulting in 200 total items. Additional data and annotation details can be found in Appendix G.1. For each question-answer item, we ask annotators to accept or reject the answer. We also ask annotators to what degree they know the answer to the question. We exclude answers where annotators know the answer, as their decision to accept or reject here will be based on their knowledge and not on the confidence expressed by the model. The task is framed as a trivia game, and answers are presented as coming from a teammate. The full annotation interface is shown in Appendix G.3. We recruited annotators on Amazon's Mechanical Turk with stringent qualification requirements; we then filtered annotators based on a qualification task, described in Appendix G.2. We also include attention checks in each annotation task and excluded annotators who failed any attention checks; each task had \( 10\%\) of examples as attention checks. In total, 5 annotators participated in our task. Annotators knew the answers to 21 of the reference questions and 22 of the LACIE questions.3 These were excluded from the analysis, leaving 79 and 78 total.

LACIE improves induced listener calibration for human listeners.Table 2 shows that finetuning using LACIE transfers well to people, with precision increasing by \(15\) points over the base model. The increase in precision is driven by a \(47\%\) reduction in false positives (statistically significant at \(p<0.05\) by McNemar's test), i.e. the LACIE model was better able to express low confidence when its answer was wrong. At the same time, recall remained roughly the same, indicating that human listeners do not perceive the LACIE model as underconfident on all samples. Indeed, the LACIE model only had one additional false negative, a difference that is not statistically significant (\(p=1.0\)). This is particularly promising, as it indicates that the decreases in recall see in Table 1 do not translate to humans, while the increases in precision do.

## 5 Discussion and Analysis

Effect of training on listener probabilities.Fig. 2 shows the average induced listener probability for correct and incorrect samples. For baseline speaker models, this probability is approximately equal whether an example is correct or not, indicating that the baselines use roughly the same high level of confidence to express both correct and incorrect answers. For LACIE, the probability of incorrect answers is substantially lower than that of correct ones, indicating the speaker is modulating confidence to correctly express uncertainty (as captured by the listener).

Out-of-distribution generalization.Table 3 shows LACIE's performance on TruthfulQA's evaluation split of 817 questions, which is out-of-distribution (since LACIE model was only trained with preferences sourced only from TriviaQA). TruthfulQA's evaluation is model-based, using fine-tuned judge models. As a sanity-check, the truthful-only baseline improves truthfulness from 0.27 to 0.39, indicating Mistral-7B trained to be truthful on TriviaQA generally is rated as more truthful by the judge model. However, LACIE training _further_ improves truthfulness to 0.55, reflecting the additional benefit of our pragmatic training. This improvement comes at a \(9\)-point cost to informativeness, reflecting a natural tradeoff between being maximally informative and maximally truthful; note that similar to recall, abstention impacts informativeness, since informativeness is a recall-oriented metric.4 These results demonstrate that LACIE training imbues LLMs with an ability to produce better-calibrated outputs not only for the data distribution they were trained on, but also on out-of-domain questions where the evaluation directly addresses truthfulness.

Qualitative Analysis.In Fig. 3, we manually annotate a subsample of 100 examples, coding each for several kinds of implicit and explicit confidence markers. These include adding details about the topic, other implicit markers (e.g. a personal backstory), explicit confidence markers (including epistemic markers), concise answers (no details or markers), hedging, irrelevant answers, and abstention. We annotate these examples without knowledge of which model the answer came from. We find that after training, the LACIE model adds more slightly details on correct examples, while on incorrect examples the reference model adds more details and the trained model adds fewer. This reflects the fact that confident answers are often supported by additional details, and corresponds to a lower rate of concise answers. The LACIE model also makes greater use of hedging on incorrectexamples, and abstains on several incorrect examples, whereas the reference model never abstains and rarely hedges.

Additionally, LACIE leads to an increased use of explicit markers; note that this does not mean the trained model is overconfident, as we treat both confident and unconfident markers as explicit, i.e.

 
**Setting** & **Truth.** & **Info.** \\  base & 0.27 & **0.99** \\ truthful & 0.39 & 0.97 \\ LACIE & **0.55** & 0.90 \\  

Table 3: TruthfulQA performance with Mistral-7B, measured by Truthfulness (Truth.) and Informativeness (Info.) metrics.

Figure 2: Induced listener probabilities for LACIE-trained and baseline models (Mistral-7B). Baselines have similar scores for correct and incorrect examples; LACIE results in significantly lower scores for incorrect answers.

[MISSING_PAGE_FAIL:10]